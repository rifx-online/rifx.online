---
title: "大型语言模型变得更小了"
meta_title: "大型语言模型变得更小了"
description: "这可能会改变软件初创企业的游戏规则"
date: 2024-11-04T12:29:02Z
image: "https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*1PeFyz_Dlt6jEf27Q9Y33Q.png"
categories: ["Programming", "Technology", "Machine Learning"]
author: "Rifx.Online"
tags: ["compression", "optimization", "ternary", "parallelism", "hardware"]
draft: False

---

### 这将如何改变软件初创企业的游戏规则



**本文与 [David Meiborg](https://readmedium.com/undefined) 共同撰写。**

*TLDR: 大型语言模型（简称 LLMs）目前体积庞大，运行成本高，并且具有 [显著的碳足迹](https://arxiv.org/abs/2309.14393)。然而，最近在模型压缩和系统级优化方法上的进展可能会增强 LLM 推理能力。特别是一种使用三元结构参数的方法，有潜力绕过当前标准的昂贵矩阵乘法。这对制造专用芯片的硬件初创企业以及使用或定制构建自己 LLM 的软件初创企业都有令人兴奋的影响。帮助客户部署 LLM 的初创企业可能也会迎来更多的业务。*

如今的大型语言模型非常庞大。真的很大。如果你想加载一个 LlaMa-2–70B 模型，你需要 140 GB 的显存（这就是 70 亿个参数乘以每个参数 2 字节）。作为对比，像 NVIDIA RTX 3090 或 4090 这样的 GPU 只有 24 GB 的显存——这只是所需的一小部分。

有一些关于量化的 [解决方法](https://towardsdatascience.com/run-llama-2-70b-on-your-gpu-with-exllamav2-588141a88598)，但这些往往比较繁琐。你可能仍然需要让你的 GPU 高温运行长达 15 小时，直到模型加载完成。更不用说你仍然需要一些空余内存用于推理，换句话说，就是用于部署模型。

因此，使用当前的 LLMs 成本高昂：通常需要多个高端 GPU 来保存模型，并且还必须考虑推理所产生的能源成本。

这就是为什么很多研究都在致力于应用技术，使 LLMs 更小，从而能够在更小的硬件上以更低的成本运行。在大多数情况下，这是一种艰难的权衡，因为使 LLMs 更小通常会影响它们的质量。找到成本与收益相等的点可能是棘手的。

在本文中，我们概述了一些有前景的优化方法，解释了微软研究人员的最新突破，简要概述了“高效 LLM”领域的创新初创企业，并推导出一些对在 LLM 生态系统中运营的初创企业的一般影响。

## LLM如何变得更加资源高效

像微软、OpenAI、Meta或谷歌这样的科技巨头拥有足够的资源来训练尖端模型，即使目前训练成本对大多数其他公司来说是不可承受的。因此，广泛采用的最大瓶颈不是训练，而是推理效率。换句话说，尽管Meta已经发布了LlaMa，但由于运行模型——而不是创建模型——已经足够具有挑战性，因此它仍未得到足够的采用。

然而，研究人员开始提高这种推理效率。广义而言，有两种方法可以实现这一目标：**系统级优化**并不改变模型本身，而是通过改变模型所处环境的关键方面来提高其性能。**模型优化**则压缩模型，使其更易于部署和运行。

这两种方法都有多种不同的技术。[一篇最近的论文](https://arxiv.org/pdf/2402.01799.pdf)由研究人员出色地总结了这些技术。由于这些技术可能很快就会成为任何从事LLM系统工作者的基本知识，我们在下面对这些技术进行了快速概述。

### 系统级优化

系统级优化指的是改变模型本身的运行方式，而不是模型本身。事实证明，有很多手段可以避免资源闲置或消除其他低效现象。

**分页注意力**

像 GPT 这样的 LLM 的核心是注意力机制。这个机制允许模型在生成每个输出单词时关注输入文本的不同部分。想象一下，您正在阅读一本书，并标记重要的句子以更好地记住故事。类似地，注意力机制在做出预测时“突出”或赋予某些单词或短语更多重要性。

这个机制非常耗费资源。它要求模型考虑输入文本中所有单词对之间的关系。对于长文本，这可能需要大量的内存和计算能力。

分页注意力不是一次处理整个文本，而是将文本分成更小的“页”或段落。模型然后一次处理这些页面或以较小的组处理。这种方法显著减少了任何给定时刻所需的内存量，因为模型不需要同时跟踪整个文本的关系。

这有点像一个学生，如果一次性阅读整整一年的教科书会感到不知所措。通过在整个学年中将其分解为可管理的段落，学生可以记住教科书的内容。

通过每一步所需的内存减少，分页注意力允许在相同的硬件约束下使用更大的模型或更长的文本。

**张量并行**

并行是一种计算中的众所周知的概念。它意味着将一个大型计算任务分成可以由多个处理器或计算机同时处理的小部分。这显著加快了程序运行所需的时间。

在 LLM 的上下文中，[张量](https://towardsdatascience.com/what-is-a-tensor-in-deep-learning-6dedd95d6507) 是多维数字数组。这些张量用于表示模型处理的数据。这类数据包括输入文本、模型权重，即模型学习的参数，以及输出预测。

将这两个概念结合起来，张量并行涉及将这些张量分割到多个 GPU 或其他处理单元上。例如，如果模型的参数（权重）太大而无法适应单个 GPU 的内存，则可以将其分布到多个 GPU 上。每个 GPU 然后一次只处理张量的一部分。

就像一个团队中的多个成员一起工作在一个大型项目上，处理单元在处理各自部分的张量时需要交换信息。例如，一个 GPU 上的计算结果可能需要与另一个 GPU 共享，以继续下一步计算。因此，单元之间的高效通信对于张量并行的有效性至关重要。

简而言之，张量并行是一种将 LLM 所需的计算分解为更小的并行任务的方法，这些任务可以由多个计算单元同时处理，从而加快这些大型复杂模型的训练和推理时间。

**流水线并行**

这种技术专注于改善数据通过模型层的处理工作流。这可以显著加快整体计算速度，并更好地利用可用硬件。

计算中的流水线工作方式类似于工厂的装配线，不同任务的阶段按顺序完成。这允许多个任务在不同阶段同时进行。

在 LLM 中，这些不同的阶段由神经网络的层表示。每一层按顺序处理输入数据，逐渐提取更复杂的特征或模式，直到产生最终输出。可以将每一层视为工厂装配线上的一个工人：每个工人在数据通过时都会在其上添加一些东西，直到最终出现一个复杂的产品。

在流水线并行中，模型的层被分为多个段，每个段分配给不同的 GPU 或处理单元。这样，模型可以按批次输入数据：一旦第一个段处理完第一批数据，第二个段就接手那批数据，而第一个段则接手一批新的数据。

这在模型中创建了数据的连续流动，每个模型段在任何给定时间都在处理不同的数据。这通过保持模型的所有部分处于活动状态来最大化可用硬件资源的使用，并减少单个处理器等待任务完成时可能发生的空闲时间。

前面讨论的流水线并行在模型层级别上操作，将顺序处理阶段分布到设备上。而张量并行则在更细粒度的层面上操作，将层内发生的实际计算（例如，大型矩阵乘法的部分）分布到设备上。

**CPU/GPU 卸载**

在这部分中，我们讨论了很多关于 GPU 的内容。然而，并不是所有在训练或运行 LLM 中的任务都同样适合 GPU。一些任务，如数据预处理或某些控制逻辑，可能更有效地由 CPU 处理。其他任务，特别是处理神经网络（如矩阵乘法）所涉及的重数学计算，确实更有效地在 GPU 上执行。

通过将特定任务卸载到最适合它们的处理器上——将并行化、计算密集型任务分配给 GPU，而将顺序或逻辑密集型任务分配给 CPU——系统可以确保每个工作负载部分以最有效的方式进行处理。

**融合操作**

融合操作将通常单独执行的多个处理步骤合并为一个简化的操作。例如，而不是先执行矩阵乘法再进行加法，融合操作会同时执行两者。

**推测解码**

在生成文本时，LLM 根据之前的单词计算句子中下一个单词的概率。传统上，在生成每个单词后，模型会重新计算以确定下一个单词，并且这个过程会重复，直到完整的句子或段落完成。然而，这种顺序过程可能很慢，尤其是对于较长的文本或更复杂的模型，因为每一步都依赖于前一步的完成。

并行预测：与其等待每个单词被选择后再考虑下一个，推测解码允许模型“推测”或同时对接下来的几个单词做出多个预测。这被称为 *并行预测*。这就像对句子接下来可能采取的几条路径进行有根据的猜测。

通过并行探索这些可能性，模型可以潜在地减少生成文本所需的整体时间。一旦实际的下一个单词被选定，模型可以更快地沿着最可能的路径继续，因为它已经计算了后续的选项。

### LLM模型的压缩

研究人员过去探索过模型压缩。然而，随着大规模LLM的出现，这已成为一个更大的挑战。

许多现有的压缩方法依赖于执行微调步骤以在压缩阶段恢复丢失的性能。然而，当应用于LLM时，由于其庞大的规模，这种方法有显著的局限性。因此，LLM压缩已成为一个全新的研究领域。

**架构剪枝**

当你修剪苹果树时，你会在冬季或早春剪掉某些树枝。这确保树木不会在无效的树枝上浪费资源或因枯木而感染疾病。这有助于它结出更好的果实。

当然，LLM并不结出果实。在这个背景下，剪枝是一种用于减少模型大小的方法，同时尽量保持或最小化对其性能的影响。

LLM模型有数百万甚至数十亿个参数。这些参数并不是所有对模型进行预测或理解语言都同等重要。有些参数很少使用或对模型的决策贡献不大：因此，消除这些冗余或影响较小的连接、神经元或整个层，使模型的使用更高效。

选择剪枝哪些参数并不是一项简单的任务。在基于幅度的剪枝中，移除神经网络中绝对值最小的权重。在训练之前，这些权重通常为零；训练之后，它们通常介于-1和1之间。如果训练对某个权重的影响不大，那么它很可能接近零，因此对模型的决策贡献较少。

一种资源密集但也更稳健的剪枝技术是灵敏度分析。这涉及评估移除每个参数或参数组对模型性能的影响。移除后导致性能下降最小的参数会被剪枝。

还有其他技术，但通常可以将它们分类为非结构化剪枝或结构化剪枝。非结构化剪枝（例如基于幅度的剪枝）移除单个权重，导致稀疏连接的神经网络。结构化剪枝（例如灵敏度分析）移除整个单元或层（例如，整个神经元或通道），这在某些硬件上可以更有效地提高计算效率。

剪枝后，模型通常会经历微调过程。这涉及在训练数据集或其子集上对剪枝后的模型进行再训练。目标是让模型调整和优化其剩余参数，以补偿剪枝所造成的损失。这有助于恢复因剪枝而失去的任何性能。

这可以通过迭代方式或一次性方式进行。在迭代剪枝中，模型在多个轮次中逐步剪枝。在每一轮之后，剪枝后的模型会重新训练，以恢复因剪枝而失去的性能。这个循环可以重复多次，模型可能会变得更加稳健，即使在显著减少参数的情况下也能保持性能。在一次性剪枝中，所有识别出的参数一次性移除，然后对模型进行微调。

**知识蒸馏**

想象一下，有一个足球场上有两个球员：一个非常有经验，知道很多技巧，另一个是初学者。经验丰富的球员知道的比初学者多得多，但初学者可以通过模仿其他球员在场上的行为迅速达到可比的表现。

LLM的知识蒸馏工作原理类似：这是训练一个更小（学生模型）、更高效的模型，以通过学习大模型（教师模型）的输出和处理信息的方式来复制其性能的过程。

要应用这一技术，显然需要一个大型教师模型，例如LlaMa或Mistral的开源大型模型之一。然后需要设计一个参数数量显著少于教师模型的较小神经网络。

学生模型不仅仅在原始硬目标（即真实数据标签）上进行训练，还在软目标上进行训练。这些是教师模型对相同输入生成的概率。例如，对于一组特定的查询，假设教师模型70%的时间回答为“A”，20%的时间回答为“B”，10%的时间回答为“C”、“D”或“E”。学生模型不仅会尝试正确回答每个问题；它还会尝试在一组查询中遵循相同的概率分布。

这样的软目标每个示例携带的信息比硬标签更多，因为它们包含教师模型对所有可能结果的置信水平。这就是学生模型能够以较低的计算开销表现得与教师相似的原因。

在初始知识蒸馏之后，学生模型可能会在特定任务的数据集上进行进一步的微调，以最大化其性能。

**低秩近似**

LLM通过处理和生成基于巨大的矩阵（即非常大的数字表）来工作，这些矩阵表示单词之间的关系、它们的含义以及它们在语言中的使用。这些矩阵可能大到难以处理，特别是在存储和计算方面。

低秩近似涉及找到一个更简单的矩阵，其大小要小得多，但仍能捕捉到原始大矩阵中最重要的信息。这有点像将详细的画作简化为草图。

这通过数学技术来完成，这些技术识别矩阵（或在我们的类比中，画作）中哪些部分包含最多的信息，并将矩阵缩减到仅这些部分。有一些数学技术，尤其是[奇异值分解](https://www.cs.cmu.edu/~venkatg/teaching/CStheory-infoage/book-chapter-4.pdf)，有助于实现这一点。

与剪枝不同，低秩近似执行矩阵维度减少，保持模型的结构，但以更紧凑的形式表示，而剪枝则直接移除神经网络的部分。

**量化**

LLM使用大量数学计算来处理文本。这些计算使用可以具有广泛值范围的数字。通常，这些数字以可以表示非常广泛值范围的格式存储（[浮点格式](https://de.wikipedia.org/wiki/Einfache_Genauigkeit)），在内存中占用32位。

量化减少了这些数字的精度，通常将32位浮点数字转换为更低位宽的表示，例如8位整数。这意味着模型不再使用具有许多小数位的数字，而是使用“更简单”的数字，从而使计算更快并减少内存占用。

量化感知训练（QAT）涉及在训练模型时考虑量化，使其能够适应精度损失，通常导致更好的性能，但代价是更复杂和资源密集的训练过程。

后训练量化（PTQ）在模型完全训练后应用量化，提供了一种更简单和更快速的方法来减少计算需求。然而，由于模型并未特别针对低精度操作进行优化，因此可能无法达到与QAT相同的准确性或性能水平。

### 1位 LLM 时代？

微软研究人员最近发表了一篇[引起轰动的论文](https://arxiv.org/pdf/2402.17764.pdf)，将每个参数的存储位数从当前 LLM 中的 16 位标准，降低到了仅仅 1.58 位。这是个重大新闻：通过这种技术，他们实现了近 10 倍的令牌吞吐量，即处理文本的速度几乎快了 10 倍。他们还将内存占用减少了 3.5 倍，这意味着运行这些模型所需的硬件大大减少。

这是通过使用三元位实现的。与通常使用的介于 -1 和 1 之间的浮点数（通常使用 16 位）不同，每个权重被表示为 -1、0 或 1。这些数字可以存储在 1.58 位，因为对于 3 个可能值的二进制晶体管，可以得到 2¹.58 = 3。仅使用如此简单的数字也意味着不再需要复杂的矩阵乘法，这使得资源使用效率大大提高。

这种技术令人困惑的是，它在 30 亿参数的大小下，能够实现与传统 16 位模型相似的输出性能。目前尚不清楚这种模型在超过 130 亿参数的阈值时，是否能像传统模型一样扩展。明确的是，即使在 700 亿参数下，它在延迟、内存使用和能耗方面比仅有 130 亿参数的传统模型更高效。输出质量仍需详细测试。

另一个缺点是，现有 LLM 的最先进量化技术无法用于生成 1.58 位模型。这类模型需要从头开始创建，尽管成本大幅降低，但目前仍超出普通市民的承受范围。

然而，如果这样的模型被创建并运行良好，推理将变得更加容易。1.58 位 LLM 甚至可能在边缘和移动设备上部署。它们对 CPU 设备（大多数移动设备运行的设备）也更加友好，这使得它们更容易在更便宜的芯片上部署。所有这些都有许多优势，例如隐私方面，但也允许出现人类尚未梦想的新的应用。

此外，像 [Groq](https://groq.com/) 这样的初创公司已经展示了在为 LLM 构建特定硬件 [如 LPU](https://wow.groq.com/why-groq/) 的 promising results 和巨大潜力。LLM 专用硬件已经是一个[巨大市场](https://finance.yahoo.com/news/generative-ai-market-size-expected-163500846.html#:~:text=%2D%20Large%20Language%20Model%20(LLM),the%20forecast%20period%202023%2D2029.)。这样的发现可能使这个市场的增长速度比分析师迄今预见的更为激进。

如果没有其他，推理将由于量化技术和专用硬件的结合而变得极为便宜。这对许多公司，包括初创公司，都会产生影响。

## 较轻量的 LLM 对初创企业意味着什么？

### AI硬件的繁荣刚刚开始

在1971年至1999年间，CPU几乎是市场上[唯一的微处理器](https://cs.stanford.edu/people/eroberts/courses/soco/projects/2005-06/64-bit-processors/history1.html)。随后，[NVIDIA推出](https://readmedium.com/a-brief-history-of-gpu-47d98d6a0f8a)了其GPU。虽然从技术上讲，它并不是世界上第一个GPU，但它是使游戏成为一种可接触和沉浸式体验的首批微处理器之一。（游戏消耗大量计算能力——如果你不知道，现在你知道了！）

从游戏开始，GPU迅速扩展到许多不同的任务，包括科学图像处理、线性代数、3D重建等。GPU特别擅长的一件事是什么？机器学习和LLM。如今，许多NVIDIA的芯片正在用于训练LLM。

从那时起，其他微处理器也开始涌现。[谷歌的TPU](https://cloud.google.com/tpu?hl=en)于2016年推出，特别适合AI训练和推理。虽然GPU被证明非常适合LLM，但TPU是专门为此目的设计的。它们在训练和推理方面都非常合适。

然而，行业正处于[转折点](https://www.wsj.com/tech/ai/how-a-shifting-ai-chip-market-will-shape-nvidias-future-f0c256b1)：不久之后，大多数与LLM相关的工作将是推理，而不再是训练，因为用户开始部署像LlaMa这样的模型。新的创新AI半导体公司现在有机会进入这个领域。

这包括专注于特别快速推理处理器的芯片制造商[Groq](https://wow.groq.com/press/)。其他初创公司包括[赛拉布拉斯](https://www.cerebras.net/)（专注于训练）、[Graphcore](https://www.graphcore.ai/about)（涵盖训练和推理）和[SambaNova](https://sambanova.ai/)（也包括训练和推理）。像英特尔和AMD这样更成熟的竞争对手也在关注训练和推理，尽管预计未来几年的大多数增长将来自后者。大型科技巨头——谷歌、亚马逊或微软——也在开发AI专用芯片，但主要用于内部使用。

总体而言，LLM的硬件市场仍然由数据中心应用主导。边缘和移动应用是下一个合乎逻辑的步骤，但将需要更多突破，例如微软研究人员最近发布的1.58位方法（见上文）。

## LLM软件公司的影响

在新兴AI领域的整个价值链中，我们概述的这些发展可能会导致**运行/使用LLM的成本显著降低**。

以下是我们对这一趋势的几点思考：

* **优秀的B2C产品**，因为LLM成本降低意味着可以构建具有高LLM使用频率和规模（例如，长上下文窗口）的免费增值B2C体验，而不会破坏公司的单位经济。
* 全球范围内的访问民主化，使得**低收入国家的用户**能够利用先进的AI技术。
* 公司可以自动化更广泛的任务，从而实现**效率和生产力的提升**（“我不再关心每小时有1万次API调用”）。
* 新的边缘AI硬件结合更小的模型将导致**新的边缘AI用例**变得可行，这些用例之前仅限于“数据中心”。
* 随着边缘硬件的快速发展，我们相信有机会建立软件公司，帮助客户将AI模型带入定制边缘设备的碎片化空间（“把你的模型给我，我用各种技术进行压缩，在10种不同的边缘设备上测试，告诉你哪个效果最好，然后帮助你部署”）。



---
title: "ä½¿ç”¨ Unsloth å¯¹ LLama 3 è¿›è¡Œå¾®è°ƒ"
meta_title: "ä½¿ç”¨ Unsloth å¯¹ LLama 3 è¿›è¡Œå¾®è°ƒ"
description: "åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä½¿ç”¨ Unsloth å¯¹ LLMï¼ˆæ¥è‡ª Meta çš„ Llama 3ï¼‰è¿›è¡Œå¾®è°ƒï¼ˆåŒ…æ‹¬è‡ªå®šä¹‰æ•°æ®é›†çš„æ–¹æ³•ï¼‰"
date: 2024-10-30T12:58:41Z
image: "https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kaXoudNTGeGfuNPl_kta5g.jpeg"
categories: ["Programming", "Machine Learning", "Natural Language Processing"]
author: "Rifx.Online"
tags: ["Llama", "Unsloth", "LoRA", "Alpaca", "NVIDIA"]
draft: False

---



åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä½¿ç”¨ [Unsloth](https://github.com/unslothai/unsloth) å¾®è°ƒ LLMï¼ˆMeta çš„ Llama 3ï¼‰ã€‚æˆ‘è¿˜å°†æä¾›ä½¿ç”¨æ‚¨è‡ªå·±è‡ªå®šä¹‰æ•°æ®é›†çš„æ–¹æ³•ã€‚

**æ³¨æ„ï¼š** Unsloth æ˜¯ä¸€ä¸ªåŠ é€Ÿ LLM åœ¨ NVIDIA GPU ä¸Šå¾®è°ƒçš„åº“ï¼ˆä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œå†…å­˜ä½¿ç”¨å‡å°‘ 40%ï¼‰ã€‚ä¸ Hugging Face å…¼å®¹ï¼Œæ”¯æŒ Llama å’Œ Mistral æ¶æ„ã€‚

å¦‚æœæ‚¨è§‰å¾—æˆ‘çš„æ–‡ç« æœ‰è¶£ï¼Œè¯·ä¸è¦å¿˜è®° **ç‚¹èµå¹¶ [å…³æ³¨](https://medium.com/@soulawalid)** ğŸ‘ğŸ¼ï¼Œå†™è¿™äº›æ–‡ç« éœ€è¦æ—¶é—´å’Œç²¾åŠ›ï¼

æ‚¨å¯ä»¥è®¿é—® GitHub ä»“åº“ä¸­æä¾›çš„å…è´¹ç¬”è®°æœ¬ã€‚

![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*_L4o4MDQ7W5__OwW0E5RWA.png)

ç”±äºæˆ‘ä½¿ç”¨çš„æ˜¯ Llama 3ï¼Œå› æ­¤æˆ‘å°†ç‚¹å‡»ç¬”è®°æœ¬ï¼ˆæ‚¨ä¹Ÿå¯ä»¥åœ¨è‡ªå·±çš„è®¡ç®—æœºä¸Šå®‰è£… Unslothï¼‰ã€‚

**æ³¨æ„ï¼š** æˆ‘å°†ä½¿ç”¨è¿™ä¸ªæ•°æ®é›† â€œ[alpaca\-cleaned](https://huggingface.co/datasets/yahma/alpaca-cleaned)â€ æ¥è‡ª Hugging Faceï¼Œæ•°æ®é‡‡ç”¨ Alpaca æ ¼å¼ï¼Œå³åŒ…å«ï¼ˆæŒ‡ä»¤ã€è¾“å…¥å’Œè¾“å‡ºï¼‰ã€‚

### å¼€å§‹é¡¹ç›®

åœ¨é¡¹ç›®ä¸­ï¼Œæˆ‘å°†æŒ‡å¯¼æ‚¨ä½¿ç”¨ Unsloth è¿›è¡Œå¾®è°ƒï¼Œè§£é‡Šä»£ç å¹¶æä¾›å»ºè®®ï¼Œè®©æˆ‘ä»¬å¼€å§‹æˆ‘ä»¬çš„é¡¹ç›®ï¼š

**1/ å®‰è£…æ‰€éœ€çš„åŒ…ï¼š** æˆ‘ä»¬é¦–å…ˆéœ€è¦å®‰è£… **Unsloth** å’Œ **xformers**ã€**trl**ã€**peft**ã€**accelerate**ã€**bitsandbytes** åº“ï¼Œä»¥ä¾¿è¿›è¡Œé«˜æ•ˆçš„æ¨¡å‹è®­ç»ƒå’Œæ¨ç†ã€‚

```python
!pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
!pip install --no-deps xformers trl peft accelerate bitsandbytes
```

**2/ åŠ è½½å’Œé…ç½®æ¨¡å‹ï¼š** åœ¨é…ç½®ä¸­ï¼Œæˆ‘å°†è®¾ç½®ä»¥ä¸‹å†…å®¹ï¼š

* å°†æœ€å¤§åºåˆ—é•¿åº¦è®¾ç½®ä¸º **2048**
* å°† dtype è®¾ç½®ä¸º **None**ï¼Œå®ƒä¼šè‡ªåŠ¨æ£€æµ‹æ•°æ®ç±»å‹ã€‚
* ä»¥ **4-ä½ç²¾åº¦**åŠ è½½æ¨¡å‹ï¼Œæˆ‘è®¤ä¸ºè¿™å·²ç»è¶³å¤Ÿã€‚

**æ³¨æ„ï¼š** æ‚¨å¯ä»¥åœ¨èµ„æºéƒ¨åˆ†æ‰¾åˆ°æˆ‘å…³äºå¾®è°ƒ LLM çš„æŠ€å·§çš„æ–‡ç« ã€‚

```python
from unsloth import FastLanguageModel
import torch

## é…ç½®
max_seq_length = 2048
dtype = None
load_in_4bit = True

## åŠ è½½é€‰å®šçš„æ¨¡å‹
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/llama-3-8b-bnb-4bit",
    max_seq_length=max_seq_length,
    dtype=dtype,
    load_in_4bit=load_in_4bit,
)
```

![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*cJSAcJFP7E-qJkqKUsHqLw.png)

**3/ åº”ç”¨ PEFTï¼ˆå‚æ•°é«˜æ•ˆå¾®è°ƒï¼‰ï¼š** ç„¶åæˆ‘ä»¬å°†ä½¿ç”¨ LoRA å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚

* r = 16 æ˜¯ LoRA çš„ç§©å‚æ•°ã€‚**æ³¨æ„ï¼š** å¸¸è§å€¼ä¸º 8ã€16ã€32ã€64ã€128
* lora_alpha = 16 ä»£è¡¨ LoRA æ›´æ–°çš„ç¼©æ”¾å› å­ï¼ˆæˆ‘å°†å†™ä¸€ç¯‡å…³äº LoRA çš„æ–‡ç« ï¼Œä»¥è¯¦ç»†è§£é‡Šæ¯ä¸ªéƒ¨åˆ†ï¼‰
* å¯¹äº LoRA ä¸ä½¿ç”¨ dropout å’Œåç½®
* å¯¹äº use_gradient_checkpointingï¼Œæˆ‘ä»¬ä½¿ç”¨ Unsloth æ¥å¤„ç†ï¼ˆèŠ‚çœå†…å­˜ï¼‰

```python
model = FastLanguageModel.get_peft_model(
    model,
    r = 16,
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_alpha = 16,
    lora_dropout = 0,
    bias = "none",
    use_gradient_checkpointing = "unsloth",
    random_state = 3407,
    use_rslora = False,
    loftq_config = None,
)
```

**4/ å®šä¹‰æç¤ºæ¨¡æ¿ï¼š** æˆ‘ä»¬å°†åˆ›å»º alpaca æç¤ºæ¨¡æ¿ä»¥æ ¼å¼åŒ–æ•°æ®é›†ï¼ˆå¦‚æœæ‚¨ä½¿ç”¨çš„æ•°æ®ä¸æ˜¯è¿™ç§æ ¼å¼ï¼‰ã€‚

æˆ‘ä»¬è¿˜å°†æ·»åŠ  EOSï¼ˆç»“æŸåºåˆ—ï¼‰ä»¥é€šçŸ¥ LLM å¥å­å·²ç»“æŸã€‚

æœ€åæ˜¯æ ¼å¼åŒ–å‡½æ•°ï¼Œè¯¥å‡½æ•°æ¥å—ä¸€æ‰¹ç¤ºä¾‹å¹¶æ ¹æ®æˆ‘ä»¬ä¹‹å‰ç¼–å†™çš„ alpaca æç¤ºæ¨¡æ¿æ ¼å¼åŒ–æ¯ä¸ªç¤ºä¾‹ã€‚

* å®ƒä»æ¯ä¸ªç¤ºä¾‹ï¼ˆè¡Œï¼‰ä¸­æå–æŒ‡ä»¤ã€è¾“å…¥å’Œè¾“å‡ºå­—æ®µã€‚
* ç„¶åå°†è¿™äº›å­—æ®µæ ¼å¼åŒ–åˆ°æ¨¡æ¿ä¸­å¹¶é™„åŠ  EOS æ ‡è®°ã€‚
* æ ¼å¼åŒ–çš„æ–‡æœ¬å­˜å‚¨åœ¨åˆ—è¡¨ä¸­ï¼Œå¹¶ä½œä¸ºå…·æœ‰å•ä¸ªé”®â€œtextâ€çš„å­—å…¸è¿”å›ã€‚

```python
alpaca_prompt = """ä»¥ä¸‹æ˜¯æè¿°ä»»åŠ¡çš„æŒ‡ä»¤ï¼Œé…æœ‰æä¾›è¿›ä¸€æ­¥ä¸Šä¸‹æ–‡çš„è¾“å…¥ã€‚å†™ä¸€ä¸ªé€‚å½“å®Œæˆè¯·æ±‚çš„å“åº”ã€‚

#### æŒ‡ä»¤ï¼š
{}

#### è¾“å…¥ï¼š
{}

#### å“åº”ï¼š
{}"""

EOS_TOKEN = tokenizer.eos_token

def formatting_prompts_func(examples):
    instructions = examples["instruction"]
    inputs = examples["input"]
    outputs = examples["output"]
    texts = []
    for instruction, input, output in zip(instructions, inputs, outputs):
        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN
        texts.append(text)
    return {"text": texts}
```

**5/ åŠ è½½å’Œæ ¼å¼åŒ–æ•°æ®é›†ï¼š** åŠ è½½ Alpaca æ•°æ®é›†å¹¶å¯¹æ¯ä¸ªæ•°æ®é›†ç¤ºä¾‹åº”ç”¨æ ¼å¼åŒ–ã€‚

```python
from datasets import load_dataset
dataset = load_dataset("yahma/alpaca-cleaned", split = "train")
dataset = dataset.map(formatting_prompts_func, batched = True)
```

![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*M8EmbLMdoqrM-JlkMpDv8g.png)

**6/ è®¾ç½®å’Œè®­ç»ƒæ¨¡å‹ï¼š** æˆ‘åœ¨æˆ‘[ä¹‹å‰çš„æ–‡ç« ](https://readmedium.com/supervised-fine-tuning-tips-for-your-llm-projects-f84f20593653)ä¸­æ¶µç›–äº†å¤§éƒ¨åˆ†å…³äºå¾®è°ƒçš„æŠ€å·§ã€‚

```python
from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth import is_bfloat16_supported

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    dataset_num_proc = 2, # ç”¨äºæ•°æ®é¢„å¤„ç†çš„è¿›ç¨‹æ•°é‡
    packing = False, # æ˜¯å¦å°†å¤šä¸ªåºåˆ—æ‰“åŒ…æˆä¸€ä¸ªæ‰¹æ¬¡ä»¥æé«˜è®­ç»ƒæ•ˆç‡
    args = TrainingArguments(
        per_device_train_batch_size = 2, # æ¯ä¸ªè®¾å¤‡çš„æ‰¹æ¬¡å¤§å°
        gradient_accumulation_steps = 4, # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼Œå…è®¸æœ‰æ•ˆå¢å¤§æ‰¹æ¬¡å¤§å°
        warmup_steps = 5, # è¿›è¡Œçº¿æ€§å­¦ä¹ ç‡é¢„çƒ­çš„æ­¥éª¤æ•°
        max_steps = 60, # æ€»è®­ç»ƒæ­¥éª¤æ•°
        learning_rate = 2e-5,# ä¼˜åŒ–å™¨çš„å­¦ä¹ ç‡
        fp16 = not is_bfloat16_supported(),
        bf16 = is_bfloat16_supported(),
        logging_steps = 1,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "cosine",
        seed = 3407,
        output_dir = "outputs",
    ),
)

trainer_stats = trainer.train()
```

![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Vb_OqGP9CPc8xZdnkclGyQ.png)

![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*PI0JXrTbpjuviyQ4bZJnFg.png)

**7/ æ¨ç†å’Œç”Ÿæˆï¼š** æˆ‘ä»¬é€šè¿‡å‡†å¤‡è¾“å…¥æç¤ºã€å¯¹å…¶è¿›è¡Œæ ‡è®°åŒ–ï¼Œç„¶åä½¿ç”¨æ¨¡å‹æ ¹æ®è¯¥æç¤ºç”Ÿæˆæ–°æ–‡æœ¬æ¥å‡†å¤‡æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚ç”Ÿæˆçš„æ–‡æœ¬éšåè¢«è½¬æ¢å›å¯è¯»å½¢å¼ã€‚

```python
FastLanguageModel.for_inference(model)
inputs = tokenizer(
[
    alpaca_prompt.format(
        "ç»§ç»­æ–æ³¢é‚£å¥‘æ•°åˆ—ã€‚", # æŒ‡ä»¤
        "1, 1, 2, 3, 5, 8", # è¾“å…¥
        "", # è¾“å‡º - ç•™ç©ºä»¥è¿›è¡Œç”Ÿæˆï¼
    )
], return_tensors = "pt").to("cuda")

outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)
tokenizer.batch_decode(outputs)
```

![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*PI6SBL_YPPj0-RSAn5nl7g.png)

æ‚¨è¿˜å¯ä»¥ä½¿ç”¨ TextStreamer è¿›è¡Œè¿ç»­æ¨ç†ï¼Œè¿™æ ·æ‚¨å¯ä»¥çœ‹åˆ°ç”Ÿæˆçš„æ¯ä¸ªæ ‡è®°ï¼Œè€Œä¸æ˜¯ä¸€ç›´ç­‰å¾…æ•´ä¸ªè¿‡ç¨‹ï¼

```python
FastLanguageModel.for_inference(model)
inputs = tokenizer(
[
    alpaca_prompt.format(
        "ç»§ç»­æ–æ³¢é‚£å¥‘æ•°åˆ—ã€‚",
        "1, 1, 2, 3, 5, 8",
        "",
    )
], return_tensors = "pt").to("cuda")

outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)
tokenizer.batch_decode(outputs)

from transformers import TextStreamer
text_streamer = TextStreamer(tokenizer)
_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)
```

![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*NaSQ1vQKORU1I3DsOU2iOA.png)

**8/ ä¿å­˜æ¨¡å‹ï¼š** å¦‚æœæ‚¨å¯¹æ­¤æ„Ÿåˆ°æ»¡æ„ï¼Œå¯ä»¥ä¿å­˜æ‚¨çš„æ¨¡å‹æˆ–å°†å…¶æ¨é€åˆ° Hugging Face Hubã€‚

```python
model.save_pretrained("lora_model")
tokenizer.save_pretrained("lora_model")
## model.push_to_hub("your_name/lora_model", token = "...")
## tokenizer.push_to_hub("your_name/lora_model", token = "...")
```

**9/ åŠ è½½æ¨¡å‹ï¼š**

```python
if False:
    from unsloth import FastLanguageModel
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = "lora_model",
        max_seq_length = max_seq_length,
        dtype = dtype,
        load_in_4bit = load_in_4bit,
    )
    FastLanguageModel.for_inference(model)
```

**10/ ç”¨äºç”Ÿæˆï¼š**

```python
inputs = tokenizer(
[
    alpaca_prompt.format(
        "å·´å‹’æ–¯å¦çš„é¦–éƒ½æ˜¯ä»€ä¹ˆï¼Ÿ",
        "",
        "",
    )
], return_tensors = "pt").to("cuda")

outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)
tokenizer.batch_decode(outputs)
```

å¦‚æœæ‚¨æœ‰ç‰¹å®šä¸»é¢˜å¸Œæœ›æˆ‘ä»¬è®¨è®ºï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ï¼æ‚¨çš„åé¦ˆå°†æœ‰åŠ©äºå¡‘é€ æˆ‘çš„å†…å®¹æ–¹å‘ï¼Œç¡®ä¿å…¶ä¿æŒç›¸å…³æ€§å’Œå¸å¼•åŠ›ğŸ˜€





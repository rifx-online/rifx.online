---
title: "2024 å¹´äººå·¥æ™ºèƒ½é©±åŠ¨çš„ç½‘ç»œæŠ“å–ï¼šç”¨ Python æ„å»ºä»·å€¼ç™¾ä¸‡ç¾å…ƒçš„è‡ªåŠ¨åŒ–å·¥å…·"
meta_title: "2024 å¹´äººå·¥æ™ºèƒ½é©±åŠ¨çš„ç½‘ç»œæŠ“å–ï¼šç”¨ Python æ„å»ºä»·å€¼ç™¾ä¸‡ç¾å…ƒçš„è‡ªåŠ¨åŒ–å·¥å…·"
description: "æœ¬æ–‡ä»‹ç»äº†å¦‚ä½•åœ¨2024å¹´ä½¿ç”¨Pythonæ„å»ºåŸºäºAIçš„ç½‘é¡µæŠ“å–å·¥å…·ï¼Œå¼ºè°ƒäº†ç½‘é¡µæŠ“å–åœ¨æ•°æ®æ”¶é›†ä¸­çš„é‡è¦æ€§ã€‚æŒ‡å—æ¶µç›–äº†AIé›†æˆã€ç°ä»£æŠ“å–å·¥å…·ã€åæ£€æµ‹ç­–ç•¥ã€æ€§èƒ½ä¼˜åŒ–åŠä¼¦ç†å®è·µç­‰å…³é”®å†…å®¹ï¼Œé€‚åˆæ•°æ®ç§‘å­¦å®¶ã€å¼€å‘äººå‘˜å’Œå•†ä¸šåˆ†æå¸ˆç­‰å¤šç±»è¯»è€…ã€‚é€šè¿‡å®ä¾‹å’Œä»£ç ç¤ºä¾‹ï¼Œè¯»è€…èƒ½å¤ŸæŒæ¡æ™ºèƒ½æŠ“å–æŠ€æœ¯ï¼Œå¹¶åº”ç”¨äºå®é™…é¡¹ç›®ä¸­ï¼Œæå‡æ•°æ®æ”¶é›†æ•ˆç‡å’Œè´¨é‡ã€‚"
date: 2025-01-03T00:26:40Z
image: "https://wsrv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*TJz6skyQcEC6uWTuF83q5Q.png"
categories: ["Programming", "Technology/Web", "Data Science"]
author: "Rifx.Online"
tags: ["web", "scraping", "AI", "libraries", "ethical"]
draft: False

---





### ç²¾é€šä¸‹ä¸€ä»£ç½‘ç»œçˆ¬è™«ï¼šä»é›¶åˆ°è‹±é›„ï¼Œç»“åˆ AI é›†æˆã€åæ£€æµ‹ç­–ç•¥å’ŒçœŸå®æ¡ˆä¾‹ç ”ç©¶ | å®Œæ•´æŒ‡å—ä¸ä»£ç ç¤ºä¾‹

## ğŸš€ ä»‹ç»ï¼šè‡ªåŠ¨åŒ–æ•°æ®æ”¶é›†çš„åŠ›é‡

åœ¨å½“ä»Šæ•°æ®é©±åŠ¨çš„ä¸–ç•Œä¸­ï¼Œç½‘é¡µæŠ“å–å·²ç»ä»ä¸€ç§ç®€å•çš„æ•°æ®æ”¶é›†æŠ€æœ¯æ¼”å˜ä¸ºå¼€å‘è€…ã€æ•°æ®ç§‘å­¦å®¶å’Œä¼ä¸šçš„é‡è¦æŠ€èƒ½ã€‚æ— è®ºæ‚¨æ˜¯åœ¨æ„å»ºä¸€ä¸ªäººå·¥æ™ºèƒ½é©±åŠ¨çš„ç ”ç©¶å·¥å…·ã€ç›‘æ§å¸‚åœºè¶‹åŠ¿ï¼Œè¿˜æ˜¯ä¸ºæœºå™¨å­¦ä¹ æ¨¡å‹æ”¶é›†è®­ç»ƒæ•°æ®ï¼ŒæŒæ¡ç½‘é¡µæŠ“å–éƒ½æ˜¯æ‚¨å¼€å¯ç½‘ç»œæ•°æ®å·¨å¤§æ½œåŠ›çš„é—¨æˆ·ã€‚

## ğŸ’¡ æœ¬æŒ‡å—çš„ç‹¬ç‰¹ä¹‹å¤„

* **AI é›†æˆ**ï¼šå­¦ä¹ å¦‚ä½•å°†ç½‘ç»œçˆ¬è™«ä¸ AI ç»“åˆï¼Œå®ç°æ™ºèƒ½æ•°æ®æå–
* **ç°ä»£å·¥å…·**ï¼šæ¢ç´¢å‰æ²¿åº“ï¼Œå¦‚ Crawlee å’Œ Scrapling
* **äº’åŠ¨ç¤ºä¾‹**ï¼šè·ŸéšçœŸå®é¡¹ç›®è¿›è¡Œå­¦ä¹ 
* **ä¼¦ç†å®è·µ**ï¼šäº†è§£è´Ÿè´£ä»»çš„çˆ¬è™«æŠ€æœ¯
* **æ€§èƒ½ä¼˜åŒ–**ï¼šæŒæ¡é«˜æ•ˆæ•°æ®æ”¶é›†çš„é«˜çº§ç­–ç•¥

## ğŸ¯ è°åº”è¯¥é˜…è¯»æœ¬æŒ‡å—ï¼Ÿ

* æ„å»º AI/ML æ¨¡å‹æ•°æ®é›†çš„æ•°æ®ç§‘å­¦å®¶
* è‡ªåŠ¨åŒ–æ•°æ®æ”¶é›†å·¥ä½œæµç¨‹çš„å¼€å‘äººå‘˜
* æ”¶é›†ç½‘é¡µæ•°æ®ä»¥è¿›è¡Œåˆ†æçš„ç ”ç©¶äººå‘˜
* è·Ÿè¸ªå¸‚åœºè¶‹åŠ¿çš„å•†ä¸šåˆ†æå¸ˆ
* ä»»ä½•å¯¹è‡ªåŠ¨åŒ–æ•°æ®æå–æ„Ÿå…´è¶£çš„äºº

## ğŸ› ï¸ ç°ä»£ç½‘ç»œçˆ¬è™«çš„åŸºæœ¬å·¥å…·

## æ ¸å¿ƒåº“

1. **Beautiful Soup 4**: HTML è§£æçš„ç‘å£«å†›åˆ€
2. **Scrapy**: å·¥ä¸šçº§çˆ¬è™«æ¡†æ¶
3. **Selenium**: è‡ªåŠ¨åŒ–æµè§ˆå™¨äº¤äº’
4. **Crawlee**: å…·æœ‰å†…ç½® AI åŠŸèƒ½çš„ä¸‹ä¸€ä»£çˆ¬è™«
5. **Scrapling**: ä¸å¯æ£€æµ‹ä¸”è‡ªé€‚åº”çš„çˆ¬è™«

## 2024å¹´çš„æ–°åŠŸèƒ½

* **AI\-é©±åŠ¨çš„è§£æ**: ä¸LLMsé›†æˆä»¥å®ç°æ™ºèƒ½æ•°æ®æå–
* **é«˜çº§åæ£€æµ‹**: æµè§ˆå™¨æŒ‡çº¹éšæœºåŒ–
* **è‡ªåŠ¨åŒ–ç ”ç©¶**: AIé©±åŠ¨çš„å†…å®¹å‘ç°ä¸åˆ†æ

## ğŸ“ å¼€å§‹ä½¿ç”¨ï¼šä½ çš„ç¬¬ä¸€ä¸ªçˆ¬è™«

## ç°ä»£å®‰è£…


```python
## Install the latest tools
pip install beautifulsoup4 selenium scrapy crawlee scrapling
## Import essential libraries
from bs4 import BeautifulSoup
import requests
from selenium import webdriver
from scrapling import ScraplingBrowser
```

## æ™ºèƒ½æŠ“å–ç¤ºä¾‹

æ™ºèƒ½æŠ“å–åˆ©ç”¨äººå·¥æ™ºèƒ½å’Œç°ä»£æŠ€æœ¯æ¥å¢å¼ºç½‘ç»œæ•°æ®æå–ã€‚æ­¤ç¤ºä¾‹ä½¿ç”¨ \`ScraplingBrowser\`ï¼Œæä¾›å¼‚æ­¥æ‰§è¡Œã€æ™ºèƒ½é¡µé¢åŠ è½½å’Œäººå·¥æ™ºèƒ½é©±åŠ¨çš„å†…å®¹æå–ç­‰åŠŸèƒ½ã€‚å®ƒé€šè¿‡è‡ªåŠ¨åŒ–å¸¸è§æŒ‘æˆ˜å¹¶å…è®¸é€šè¿‡çµæ´»é€‰æ‹©å™¨è½»æ¾è‡ªå®šä¹‰æ¥ç®€åŒ–æŠ“å–è¿‡ç¨‹ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªç®€æ´çš„å®ç°ï¼š


```python
import asyncio
from scrapling import ScraplingBrowser

async def modern_scraper():
    browser = ScraplingBrowser()
    
    async with browser.page() as page:
        await page.goto('https://example.com')
        
        content = await page.extract_smart({
            'title': 'h1',  # Extract title from H1 tag
            'price': '.price',  # Extract price from elements with 'price' class
            'description': 'p.description'  # Extract description from p tags with 'description' class
        })
        
        return content

data = asyncio.run(modern_scraper())
```

## ğŸš¦ é«˜çº§æŠ€æœ¯ä¸åæ£€æµ‹ç­–ç•¥ 2024

## ç°ä»£åæ£€æµ‹æ–¹æ³•

**æµè§ˆå™¨æŒ‡çº¹éšæœºåŒ–ï¼š** æµè§ˆå™¨æŒ‡çº¹éšæœºåŒ–æ˜¯ä¸€ç§åœ¨ç½‘é¡µæŠ“å–ä¸­ä½¿ç”¨çš„æŠ€æœ¯ï¼Œç”¨äºé¿å…è¢«ç½‘ç«™æ£€æµ‹å’Œå°é”ã€‚è¿™ç§æ–¹æ³•ä¸ºæ¯ä¸ªæŠ“å–ä¼šè¯ç”Ÿæˆç‹¬ç‰¹ä¸”é€¼çœŸçš„æµè§ˆå™¨é…ç½®æ–‡ä»¶ï¼Œä½¿ç½‘ç«™æ›´éš¾è¯†åˆ«è‡ªåŠ¨è®¿é—®ã€‚ä»¥ä¸‹æ˜¯ç®€è¦è¯´æ˜å’Œç®€æ´çš„ä»£ç ç¤ºä¾‹ï¼š

```python
from scrapling import ScraplingBrowser, FingerprintGenerator
async def stealth_scraping():
    # Generate random but realistic browser fingerprints
    fingerprint = FingerprintGenerator().random()
    browser = ScraplingBrowser(
        fingerprint=fingerprint,
        stealth_mode=True,
        random_delays=True
    )
    return browser
```
**æ™ºèƒ½è¯·æ±‚æ¨¡å¼ï¼š** æ™ºèƒ½è¯·æ±‚æ¨¡å¼åœ¨ç½‘é¡µæŠ“å–ä¸­æ¨¡æ‹Ÿç±»äººè¡Œä¸ºä»¥é¿å…æ£€æµ‹ã€‚è¿™ç§æŠ€æœ¯åœ¨è¯·æ±‚ä¹‹é—´ä½¿ç”¨è‡ªé€‚åº”å»¶è¿Ÿï¼Œä½¿æŠ“å–è¿‡ç¨‹æ›´åŠ è‡ªç„¶ï¼Œå¹¶ä¸”ä¸å¤ªå¯èƒ½è§¦å‘åæœºå™¨äººæªæ–½ã€‚ä»¥ä¸‹æ˜¯ç®€è¦è¯´æ˜å’Œç®€æ´çš„ä»£ç ç¤ºä¾‹ï¼š

```python
import random
import asyncio
from collections import deque
class SmartScraper:
    def __init__(self):
        self.request_times = deque(maxlen=10)
        self.base_delay = 2
    async def adaptive_delay(self):
        # Implement human-like delays
        if len(self.request_times) >= 2:
            variance = random.uniform(0.5, 1.5)
            delay = self.base_delay * variance
        else:
            delay = self.base_delay
        await asyncio.sleep(delay)
        self.request_times.append(delay)
```
**ä»£ç†ç®¡ç†ç³»ç»Ÿï¼š** ä»£ç†ç®¡ç†ç³»ç»Ÿæ˜¯ç½‘é¡µæŠ“å–ä¸­çš„ä¸€ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼Œå¸®åŠ©å°†è¯·æ±‚åˆ†æ•£åˆ°å¤šä¸ªIPåœ°å€ä¸Šï¼Œä»è€Œå‡å°‘IPè¢«å°ç¦çš„é£é™©å¹¶æé«˜æŠ“å–æ•ˆç‡ã€‚ä»¥ä¸‹æ˜¯ç®€è¦è¯´æ˜å’Œç®€æ´çš„ä»£ç ç¤ºä¾‹ï¼š

```python
class ProxyManager:
    def __init__(self, proxies):
        self.proxies = proxies
        self.current_index = 0
        self.banned_proxies = set()
    def get_next_proxy(self):
        working_proxies = [p for p in self.proxies
                          if p not in self.banned_proxies]
        if not working_proxies:
            raise Exception("No working proxies available")
        proxy = working_proxies[self.current_index % len(working_proxies)]
        self.current_index += 1
        return proxy
```

## AI-å¢å¼ºæŠ“å–

* è‡ªåŠ¨å†…å®¹ç›¸å…³æ€§æ£€æµ‹
* æ™ºèƒ½é€Ÿç‡é™åˆ¶å’Œä»£ç†è½®æ¢
* åŠ¨æ€é€‰æ‹©å™¨ç”Ÿæˆ

## ä¼¦ç†è€ƒè™‘

* å°Šé‡ robots.txt æŒ‡ä»¤
* å®æ–½é€‚å½“çš„å»¶è¿Ÿ
* åœ¨å¯ç”¨æ—¶ä½¿ç”¨ç»è¿‡èº«ä»½éªŒè¯çš„ API
* ç›‘æ§æœåŠ¡å™¨è´Ÿè½½å½±å“

## æ€§èƒ½ä¼˜åŒ–

* ä½¿ç”¨ aiohttp è¿›è¡Œå¼‚æ­¥æŠ“å–
* ä½¿ç”¨ Scrapy è¿›è¡Œåˆ†å¸ƒå¼æŠ“å–
* æ™ºèƒ½ç¼“å­˜æœºåˆ¶

## ğŸ“Š ç°å®ä¸–ç•Œçš„åº”ç”¨ä¸æ¡ˆä¾‹ç ”ç©¶

## 2024å¹´çš„ç°ä»£åº”ç”¨æ¡ˆä¾‹

**AIè®­ç»ƒæ•°æ®æ”¶é›†ï¼š** AIè®­ç»ƒæ•°æ®æ”¶é›†æ˜¯ä¸€ç§å¤æ‚çš„æ–¹æ³•ï¼Œç”¨äºæ”¶é›†é«˜è´¨é‡ã€ç›¸å…³çš„æ•°æ®ä»¥ç”¨äºæœºå™¨å­¦ä¹ æ¨¡å‹ã€‚è¯¥æŠ€æœ¯ç»“åˆäº†ç½‘é¡µæŠ“å–å’ŒAIé©±åŠ¨çš„å†…å®¹åˆ†ç±»ï¼Œä»¥è‡ªåŠ¨è¿‡æ»¤å’Œæ”¶é›†åˆé€‚çš„è®­ç»ƒæ•°æ®ã€‚ä»¥ä¸‹æ˜¯ç®€è¦è¯´æ˜å’Œç®€æ´çš„ä»£ç ç¤ºä¾‹ï¼š

```python
from scrapling import ScraplingBrowser
from transformers import pipeline
async def collect_training_data():
    browser = ScraplingBrowser()
    classifier = pipeline("text-classification")
    async with browser.page() as page:
        await page.goto('<https://example.com/articles>')
        articles = await page.extract_all('article')
        # AIé©±åŠ¨çš„å†…å®¹åˆ†ç±»
        relevant_content = [
            article for article in articles
            if classifier(article['text'])[0]['label'] == 'relevant'
        ]
        return relevant_content
```
**ç«äº‰æƒ…æŠ¥ä»ªè¡¨æ¿ï¼š** ç«äº‰æƒ…æŠ¥ä»ªè¡¨æ¿æ˜¯ä¸€ä¸ªå¼ºå¤§çš„å·¥å…·ï¼Œç”¨äºå®æ—¶ç›‘æ§å’Œåˆ†æç«äº‰å¯¹æ‰‹æ•°æ®ã€‚è¯¥æ–¹æ³•ç»“åˆäº†ç½‘é¡µæŠ“å–æŠ€æœ¯å’Œæ•°æ®å¯è§†åŒ–ï¼Œä¸ºç”µå­å•†åŠ¡ä¼ä¸šæä¾›å¯æ“ä½œçš„æ´å¯Ÿã€‚ä»¥ä¸‹æ˜¯ç®€è¦è¯´æ˜å’Œç®€æ´çš„ä»£ç ç¤ºä¾‹ï¼š

```python
import asyncio
from crawlee import PlaywrightCrawler
import pandas as pd
async def monitor_competitors():
    data = []
    sites = ['competitor1.com', 'competitor2.com']
    async for site in sites:
        prices = await track_prices(site)
        inventory = await check_inventory(site)
        data.append({
            'site': site,
            'prices': prices,
            'inventory': inventory
        })
    # åˆ›å»ºå®æ—¶ä»ªè¡¨æ¿
    df = pd.DataFrame(data)
    return df.to_html()
#### ç”µå­å•†åŠ¡æƒ…æŠ¥
```python
async def track_prices(product_urls):
    prices = {}
    async with ScraplingBrowser() as browser:
        for url in product_urls:
            page = await browser.new_page()
            await page.goto(url)
            price = await page.extract('.price')
            prices[url] = price
    return prices
```

## ç ”ç©¶è‡ªåŠ¨åŒ–

ç ”ç©¶è‡ªåŠ¨åŒ–æ˜¯ä¸€ç§å¼ºå¤§çš„æŠ€æœ¯ï¼Œåˆ©ç”¨ç½‘é¡µæŠ“å–æ¥ç®€åŒ–æ”¶é›†å­¦æœ¯å’Œç§‘å­¦ä¿¡æ¯çš„è¿‡ç¨‹ã€‚è¯¥æ–¹æ³•ä½¿ç”¨Crawleeçš„PlaywrightCrawleré«˜æ•ˆåœ°ä»å¤šä¸ªç ”ç©¶æ¥æºæ”¶é›†æ•°æ®ã€‚ä»¥ä¸‹æ˜¯ç®€è¦è¯´æ˜å’Œç®€æ´çš„ä»£ç ç¤ºä¾‹ï¼š

```python
from crawlee import PlaywrightCrawler
async def research_crawler():
    crawler = PlaywrightCrawler()
    await crawler.run([
        '<https://research-site.com/papers>',
        '<https://academic-database.com/articles>'
    ])
```

## ğŸ¯ 2024å¹´çš„æœ€ä½³å®è·µ

**ä½¿ç”¨ç°ä»£å·¥å…·**

* åˆ©ç”¨AIé©±åŠ¨çš„åº“
* å®æ–½æ™ºèƒ½é€Ÿç‡é™åˆ¶
* åœ¨éœ€è¦æ—¶ä½¿ç”¨åˆ†å¸ƒå¼æŠ“å–

**å¤„ç†åŠ¨æ€å†…å®¹**

* WebSocketç›‘æ§
* JavaScriptæ¸²æŸ“
* APIé›†æˆ

**é”™è¯¯ç®¡ç†**

* å®æ–½é‡è¯•æœºåˆ¶
* å…¨é¢è®°å½•é”™è¯¯
* ç›‘æ§æŠ“å–å¥åº·çŠ¶å†µ


## ğŸ”® æ–°å…´è¶‹åŠ¿å’Œæœªæ¥å‘å±•


## ä¸‹ä¸€ä»£æŠ“å–æŠ€æœ¯

**AIé©±åŠ¨çš„è‡ªé€‚åº”æŠ“å–**

* è‡ªå­¦ä¹ æŠ“å–å™¨ï¼Œèƒ½å¤Ÿé€‚åº”ç½‘ç«™å˜åŒ–
* æ™ºèƒ½å†…å®¹ç›¸å…³æ€§è¯„åˆ†
* è‡ªåŠ¨æ¨¡å¼è¯†åˆ«å’Œé€‰æ‹©å™¨ç”Ÿæˆ
* è‡ªç„¶è¯­è¨€ç†è§£ç”¨äºå†…å®¹æå–

**è¾¹ç¼˜è®¡ç®—é›†æˆ**

* åˆ†å¸ƒå¼æŠ“å–ç½‘ç»œ
* è¾¹ç¼˜å®æ—¶æ•°æ®å¤„ç†
* é™ä½å»¶è¿Ÿå’Œæé«˜æ€§èƒ½
* å¢å¼ºåœ°ç†åˆ†å¸ƒ

**å¤šæ¨¡æ€æ•°æ®æ”¶é›†**

* å›¾åƒå’Œè§†é¢‘å†…å®¹åˆ†æ
* éŸ³é¢‘è½¬å½•å’Œåˆ†æ
* æ–‡æ¡£ç†è§£å’Œæå–
* è·¨æ ¼å¼æ•°æ®å…³è”


## ä»£ç ç¤ºä¾‹ï¼šAIé©±åŠ¨çš„è‡ªé€‚åº”æŠ“å–å™¨

AIé©±åŠ¨çš„è‡ªé€‚åº”æŠ“å–å™¨æ˜¯ä¸€ç§å…ˆè¿›çš„ç½‘é¡µæŠ“å–æŠ€æœ¯ï¼Œåˆ©ç”¨è‡ªç„¶è¯­è¨€å¤„ç†æ™ºèƒ½æå–åŸºäºç›®æ ‡ä¸»é¢˜çš„ç›¸å…³å†…å®¹ã€‚è¯¥æ–¹æ³•ç»“åˆäº†BERTï¼ˆåŒå‘ç¼–ç å™¨è¡¨ç¤ºæ¥è‡ªTransformersï¼‰ä¸ç½‘é¡µæŠ“å–ï¼Œåˆ›å»ºä¸€ä¸ªé«˜æ•ˆä¸”ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ•°æ®æå–ç³»ç»Ÿã€‚ä»¥ä¸‹æ˜¯ç®€æ´çš„å®ç°ï¼š

```python
from transformers import AutoTokenizer, AutoModel
import torch
from scrapling import ScraplingBrowser
class AIAdaptiveScraper:
    def __init__(self):
        self.tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
        self.model = AutoModel.from_pretrained("bert-base-uncased")
        self.browser = ScraplingBrowser()
    async def extract_relevant_content(self, url, target_topic):
        async with self.browser.page() as page:
            await page.goto(url)
            # æå–æ‰€æœ‰æ–‡æœ¬å†…å®¹
            texts = await page.extract_all("p, h1, h2, h3")
            # AIé©±åŠ¨çš„ç›¸å…³æ€§è¯„åˆ†
            relevant_content = []
            for text in texts:
                score = await self.compute_relevance(text, target_topic)
                if score > 0.8:  # ç›¸å…³æ€§é˜ˆå€¼
                    relevant_content.append({
                        'text': text,
                        'relevance_score': score
                    })
            return relevant_content
    async def compute_relevance(self, text, topic):
        # ä½¿ç”¨BERTè¿›è¡Œè¯­ä¹‰ç›¸ä¼¼æ€§è®¡ç®—
        inputs = self.tokenizer(text, topic, return_tensors="pt", padding=True)
        outputs = self.model(**inputs)
        similarity = torch.cosine_similarity(
            outputs.last_hidden_state[0],
            outputs.last_hidden_state[1]
        )
        return similarity.item()
```

## æœªæ¥åº”ç”¨

ç½‘é¡µæŠ“å–æ­£è¿…é€Ÿå‘æ›´æ™ºèƒ½å’Œè‡ªåŠ¨åŒ–çš„ç³»ç»Ÿå‘å±•ã€‚åˆ°2024-2025å¹´ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¤æ‚çš„AIé©±åŠ¨çš„ç ”ç©¶åŠ©æ‰‹ï¼Œèƒ½å¤Ÿè‡ªä¸»å‘ç°å†…å®¹ã€è·¨æºéªŒè¯å’Œæ™ºèƒ½æ•°æ®å…³è”ã€‚å®æ—¶å¸‚åœºæƒ…æŠ¥ç³»ç»Ÿå°†åˆ©ç”¨æŒç»­ç›‘æ§å’Œé¢„æµ‹åˆ†æè¿›è¡ŒåŠ¨æ€å®šä»·å’Œè¶‹åŠ¿æ£€æµ‹ã€‚æ•°æ®è´¨é‡å°†é€šè¿‡AIé©±åŠ¨çš„éªŒè¯å’Œè‡ªé€‚åº”æ¨¡å¼æ¨æ–­å¾—åˆ°æå‡ã€‚å¤§å‹è¯­è¨€æ¨¡å‹çš„é›†æˆå°†å½»åº•æ”¹å˜å†…å®¹åˆ†æï¼Œè€Œéšç§ä¼˜å…ˆçš„æ–¹æ³•å°†ç¡®ä¿éµå®ˆæ•°æ®ä¿æŠ¤æ³•å¾‹ã€‚è¾¹ç¼˜è®¡ç®—å°†ä½¿åˆ†å¸ƒå¼æŠ“å–ç½‘ç»œæˆä¸ºå¯èƒ½ï¼Œä½¿æ•°æ®æ”¶é›†æ›´åŠ é«˜æ•ˆå’Œå¯æ‰©å±•ã€‚è¿™ä¸€è½¬å˜å°†ç‰¹åˆ«å½±å“è‡ªåŠ¨åŒ–ç ”ç©¶ï¼Œæ ¹æœ¬æ”¹å˜æˆ‘ä»¬æ”¶é›†å’Œå¤„ç†ç½‘é¡µæ•°æ®çš„æ–¹å¼ã€‚

![](https://wsrv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*JO1gZH8GUxV-6dB7gbscAg.png)


## ğŸ› ï¸ æ•…éšœæ’é™¤å’Œè°ƒè¯•æŒ‡å—


## å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ

**é€Ÿç‡é™åˆ¶æ£€æµ‹ï¼š** é€Ÿç‡é™åˆ¶æ£€æµ‹æ˜¯ç½‘é¡µæŠ“å–ä¸­çš„ä¸€ç§å…³é”®æŠ€æœ¯ï¼Œç”¨äºä¼˜é›…åœ°å¤„ç†æœåŠ¡å™¨æ–½åŠ çš„è¯·æ±‚é™åˆ¶ã€‚è¯¥æ–¹æ³•æœ‰åŠ©äºç»´æŠ¤æŠ“å–å™¨çš„åŠŸèƒ½ï¼ŒåŒæ—¶å°Šé‡ç›®æ ‡æœåŠ¡å™¨çš„èµ„æºã€‚ä»¥ä¸‹æ˜¯ç®€è¦æ¦‚è¿°å’Œç®€æ´çš„ä»£ç ç¤ºä¾‹ï¼š

```python
class RateLimitHandler:
    def __init__(self):
        self.retry_count = 0
        self.max_retries = 3
async def handle_response(self, response):
        if response.status == 429:  # è¯·æ±‚è¿‡å¤š
            if self.retry_count < self.max_retries:
                delay = int(response.headers.get('Retry-After', 60))
                await asyncio.sleep(delay)
                self.retry_count += 1
                return True  # é‡è¯•è¯·æ±‚
            else:
                raise Exception("è¶…å‡ºé€Ÿç‡é™åˆ¶")
        return False  # æ­£å¸¸ç»§ç»­
```

## è°ƒè¯•ç­–ç•¥

**è¯·æ±‚æ£€æŸ¥ï¼š** è¯·æ±‚æ£€æŸ¥æ˜¯ç½‘é¡µæŠ“å–ä¸­ç”¨äºè°ƒè¯•å’Œä¼˜åŒ–æŠ“å–è¿‡ç¨‹çš„é‡è¦æŠ€æœ¯ã€‚å®ƒæ¶‰åŠè®°å½•å’Œåˆ†æHTTPè¯·æ±‚ï¼Œä»¥è¯†åˆ«æ¨¡å¼ã€æ½œåœ¨é—®é¢˜å’Œæ”¹è¿›é¢†åŸŸã€‚ä»¥ä¸‹æ˜¯ç®€è¦æ¦‚è¿°å’Œç®€æ´çš„ä»£ç ç¤ºä¾‹ï¼š

```python
class RequestDebugger:
    def __init__(self):
        self.request_log = []
async def log_request(self, request):
        self.request_log.append({
            'url': request.url,
            'headers': request.headers,
            'timestamp': datetime.now(),
            'method': request.method
        })
    def analyze_patterns(self):
        # åˆ†æè¯·æ±‚æ¨¡å¼ä»¥å¯»æ‰¾æ½œåœ¨é—®é¢˜
        times = [r['timestamp'] for r in self.request_log]
        intervals = np.diff(times)
        return {
            'mean_interval': np.mean(intervals),
            'suspicious_patterns': self.detect_patterns()
        }
```

## æ€§èƒ½ä¼˜åŒ–

**å†…å­˜ç®¡ç†ï¼š** å†…å­˜ç®¡ç†æ˜¯é«˜æ•ˆç½‘é¡µæŠ“å–çš„å…³é”®æ–¹é¢ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤§æ•°æ®é›†æ—¶ã€‚è¯¥æŠ€æœ¯é€šè¿‡æ‰¹é‡å¤„ç†æ•°æ®å’Œæ¸…é™¤ä¸å¿…è¦çš„å¯¹è±¡æ¥ä¼˜åŒ–å†…å­˜ä½¿ç”¨ã€‚ä»¥ä¸‹æ˜¯ç®€è¦æ¦‚è¿°å’Œç®€æ´çš„ä»£ç ç¤ºä¾‹ï¼š

```python
class MemoryOptimizedScraper:
    def __init__(self, batch_size=100):
        self.batch_size = batch_size
        self.results = []
async def process_batch(self, urls):
        for i in range(0, len(urls), self.batch_size):
            batch = urls[i:i + self.batch_size]
            results = await self.scrape_batch(batch)
            # å¤„ç†å¹¶æ¸…ç†å†…å­˜
            await self.save_results(results)
            self.results.clear()
            gc.collect()
```

## ğŸ“š å…¶ä»–èµ„æº


## æœ€æ–°æ–‡æ¡£

1. [Crawlee Pythonæ–‡æ¡£](https://github.com/apify/crawlee-python) â€” ç°ä»£æŠ“å–æ¡†æ¶
2. [Scrapling GitHub](https://github.com/D4Vinci/Scrapling) â€” æ— æ³•æ£€æµ‹çš„æŠ“å–
3. [AIç½‘ç»œç ”ç©¶å‘˜](https://github.com/TheBlewish/Automated-AI-Web-Researcher-Ollama) â€” AIé©±åŠ¨çš„ç ”ç©¶è‡ªåŠ¨åŒ–


## ç¤¾åŒºä¸æ”¯æŒ

* åŠ å…¥[Pythonç½‘é¡µæŠ“å–ç¤¾åŒº](https://discord.gg/webscraping)
* è´¡çŒ®å¼€æºæŠ“å–é¡¹ç›®
* åˆ†äº«æ‚¨çš„ç»éªŒå¹¶å‘ä»–äººå­¦ä¹ 


## ğŸ‰ ç»“è®º

2024å¹´çš„ç½‘é¡µæŠ“å–æ¯”ä»¥å¾€ä»»ä½•æ—¶å€™éƒ½æ›´å¼ºå¤§å’Œæ˜“äºè®¿é—®ã€‚é€šè¿‡AIé›†æˆã€ç°ä»£å·¥å…·å’Œé“å¾·å®è·µï¼Œæ‚¨å¯ä»¥æ„å»ºå¤æ‚çš„æ•°æ®æ”¶é›†ç³»ç»Ÿï¼Œä¸ºä¸šåŠ¡åˆ›é€ çœŸæ­£çš„ä»·å€¼ã€‚ä»åŸºç¡€å¼€å§‹ï¼Œå®è·µçœŸå®é¡¹ç›®ï¼Œé€æ­¥æå‡åˆ°æ›´å¤æ‚çš„æŠ€æœ¯ã€‚


> **è®°ä½ï¼š** æˆåŠŸçš„ç½‘é¡µæŠ“å–çš„å…³é”®ä¸ä»…åœ¨äºæ”¶é›†æ•°æ®ï¼Œè€Œåœ¨äºä»¥è´Ÿè´£ä»»å’Œé«˜æ•ˆçš„æ–¹å¼è¿›è¡Œï¼ŒåŒæ—¶ä¸ºæ‚¨çš„é¡¹ç›®å¢åŠ ä»·å€¼ã€‚

***æœ€åæ›´æ–°ï¼š*** *2024å¹´12æœˆ*

ğŸ“ **æ³¨æ„**ï¼šæœ¬æŒ‡å—å®šæœŸæ›´æ–°æœ€æ–°å·¥å…·å’ŒæŠ€æœ¯ã€‚è¯·éšæ—¶æŸ¥çœ‹æ–°å†…å®¹å’Œæ›´æ–°ï¼


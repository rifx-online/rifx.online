---
title: "å¦‚ä½•ä½¿ç”¨ RAG æé«˜ LLM æˆç»©"
meta_title: "å¦‚ä½•ä½¿ç”¨ RAG æé«˜ LLM æˆç»©"
description: "é€‚åˆåˆå­¦è€…çš„ Python ä»£ç ä»‹ç»"
date: 2024-11-04T12:31:55Z
image: "https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*N0Ad_oCIrAyzMYRdH3trqg.png"
categories: ["Natural Language Processing", "Programming", "Generative AI"]
author: "Rifx.Online"
tags: ["RAG", "retrievers", "LlamaIndex", "knowledge", "bases"]
draft: False

---



### åˆå­¦è€…å‹å¥½çš„ä»‹ç» w/ Python ä»£ç 

æœ¬æ–‡æ˜¯å…³äºåœ¨å®è·µä¸­ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„[æ›´å¤§ç³»åˆ—](https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c)çš„ä¸€éƒ¨åˆ†ã€‚åœ¨[ä¸Šä¸€ç¯‡æ–‡ç« ](https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32)ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ QLoRA å¯¹ Mistral-7b-Instruct è¿›è¡Œäº†å¾®è°ƒï¼Œä»¥å›åº” YouTube è¯„è®ºã€‚å°½ç®¡å¾®è°ƒåçš„æ¨¡å‹åœ¨å›åº”è§‚ä¼—åé¦ˆæ—¶æˆåŠŸæ•æ‰äº†æˆ‘çš„é£æ ¼ï¼Œä½†å®ƒå¯¹æŠ€æœ¯é—®é¢˜çš„å›ç­”ä¸æˆ‘çš„è§£é‡Šå¹¶ä¸åŒ¹é…ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘å°†è®¨è®ºå¦‚ä½•é€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆå³ RAGï¼‰æ¥æé«˜ LLM çš„æ€§èƒ½ã€‚



å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å“åº”ç”¨æˆ·æŸ¥è¯¢æ—¶å±•ç¤ºäº†å­˜å‚¨å’Œéƒ¨ç½²å¤§é‡çŸ¥è¯†çš„æƒŠäººèƒ½åŠ›ã€‚è™½ç„¶è¿™ä½¿å¾—åƒ ChatGPT è¿™æ ·çš„å¼ºå¤§ AI ç³»ç»Ÿå¾—ä»¥åˆ›å»ºï¼Œä½†ä»¥è¿™ç§æ–¹å¼å‹ç¼©ä¸–ç•ŒçŸ¥è¯†æœ‰**ä¸¤ä¸ªå…³é”®é™åˆ¶**ã€‚

**é¦–å…ˆ**ï¼ŒLLM çš„çŸ¥è¯†æ˜¯é™æ€çš„ï¼Œå³ä¸ä¼šéšç€æ–°ä¿¡æ¯çš„å‡ºç°è€Œæ›´æ–°ã€‚**å…¶æ¬¡**ï¼ŒLLM å¯èƒ½å¯¹å…¶è®­ç»ƒæ•°æ®ä¸­ä¸æ˜¾è‘—çš„åˆ©åŸºå’Œä¸“ä¸šä¿¡æ¯ç¼ºä¹è¶³å¤Ÿçš„â€œç†è§£â€ã€‚è¿™äº›é™åˆ¶å¯èƒ½å¯¼è‡´æ¨¡å‹å¯¹ç”¨æˆ·æŸ¥è¯¢çš„å›ç­”ä¸ç†æƒ³ï¼ˆç”šè‡³æ˜¯è™šæ„çš„ï¼‰ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡**é€šè¿‡ä¸“ä¸šå’Œå¯å˜çš„çŸ¥è¯†åº“å¢å¼ºæ¨¡å‹**æ¥ç¼“è§£è¿™äº›é™åˆ¶ï¼Œä¾‹å¦‚å®¢æˆ·å¸¸è§é—®é¢˜è§£ç­”ã€è½¯ä»¶æ–‡æ¡£æˆ–äº§å“ç›®å½•ã€‚è¿™ä½¿å¾—åˆ›å»ºæ›´å¼ºå¤§å’Œé€‚åº”æ€§æ›´å¼ºçš„ AI ç³»ç»Ÿæˆä¸ºå¯èƒ½ã€‚

**æ£€ç´¢å¢å¼ºç”Ÿæˆ**ï¼Œæˆ–ç§° **RAG**ï¼Œå°±æ˜¯è¿™æ ·ä¸€ç§æ–¹æ³•ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘æä¾› RAG çš„é«˜çº§ä»‹ç»ï¼Œå¹¶åˆ†äº«ä½¿ç”¨ LlamaIndex å®ç° RAG ç³»ç»Ÿçš„ç¤ºä¾‹ Python ä»£ç ã€‚

## ä»€ä¹ˆæ˜¯ RAGï¼Ÿ

LLM çš„åŸºæœ¬ç”¨æ³•æ˜¯ç»™å®ƒä¸€ä¸ªæç¤ºå¹¶è·å–å“åº”ã€‚

![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*sM1p-3FoTaGZunqx918G9A.png)

**RAG é€šè¿‡åœ¨è¿™ä¸ªåŸºæœ¬è¿‡ç¨‹ä¸­æ·»åŠ ä¸€ä¸ªæ­¥éª¤æ¥å·¥ä½œ**ã€‚å…·ä½“æ¥è¯´ï¼Œæ‰§è¡Œä¸€ä¸ªæ£€ç´¢æ­¥éª¤ï¼Œæ ¹æ®ç”¨æˆ·çš„æç¤ºï¼Œä»å¤–éƒ¨çŸ¥è¯†åº“ä¸­æå–ç›¸å…³ä¿¡æ¯ï¼Œå¹¶åœ¨ä¼ é€’ç»™ LLM ä¹‹å‰å°†å…¶æ³¨å…¥åˆ°æç¤ºä¸­ã€‚

![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*EhJZj1blu7a8EPmVAPsNcA.png)

## æˆ‘ä»¬ä¸ºä»€ä¹ˆå…³å¿ƒ

è¯·æ³¨æ„ï¼ŒRAG å¹¶æ²¡æœ‰ä»æ ¹æœ¬ä¸Šæ”¹å˜æˆ‘ä»¬ä½¿ç”¨ LLM çš„æ–¹å¼ï¼›å®ƒä»ç„¶æ˜¯ *æç¤ºè¾“å…¥å’Œå“åº”è¾“å‡º*ã€‚RAG åªæ˜¯å¢å¼ºäº†è¿™ä¸ªè¿‡ç¨‹ï¼ˆå› æ­¤å¾—åï¼‰ã€‚

è¿™ä½¿å¾— **RAG æˆä¸ºä¸€ç§çµæ´»ä¸”ï¼ˆç›¸å¯¹ï¼‰ç®€å•çš„æ–¹å¼æ¥æ”¹å–„åŸºäº LLM çš„ç³»ç»Ÿ**ã€‚æ­¤å¤–ï¼Œç”±äºçŸ¥è¯†å­˜å‚¨åœ¨å¤–éƒ¨æ•°æ®åº“ä¸­ï¼Œæ›´æ–°ç³»ç»ŸçŸ¥è¯†å°±åƒä»è¡¨ä¸­æ·»åŠ æˆ–åˆ é™¤è®°å½•ä¸€æ ·ç®€å•ã€‚

### ä¸ºä»€ä¹ˆä¸è¿›è¡Œå¾®è°ƒï¼Ÿ

æœ¬ç³»åˆ—ä¹‹å‰çš„æ–‡ç« è®¨è®ºäº†[å¾®è°ƒ](https://towardsdatascience.com/fine-tuning-large-language-models-llms-23473d763b91)ï¼Œå³ä¸ºç‰¹å®šç”¨ä¾‹è°ƒæ•´ç°æœ‰æ¨¡å‹ã€‚è™½ç„¶è¿™æ˜¯ä¸€ç§èµ‹äºˆLLMä¸“ä¸šçŸ¥è¯†çš„æ›¿ä»£æ–¹æ³•ï¼Œä½†ä»ç»éªŒæ¥çœ‹ï¼Œ**å¾®è°ƒä¼¼ä¹åœ¨è¿™æ–¹é¢çš„æ•ˆæœä¸å¦‚RAG** \[1]ã€‚

## å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„

RAG ç³»ç»Ÿæœ‰ä¸¤ä¸ªå…³é”®è¦ç´ ï¼š**æ£€ç´¢å™¨**å’Œ **çŸ¥è¯†åº“**ã€‚

### Retriever

æ£€ç´¢å™¨æ¥æ”¶ç”¨æˆ·æç¤ºå¹¶ä»çŸ¥è¯†åº“ä¸­è¿”å›ç›¸å…³é¡¹ç›®ã€‚è¿™é€šå¸¸ä½¿ç”¨æ‰€è°“çš„ **æ–‡æœ¬åµŒå…¥**ï¼Œå³æ–‡æœ¬åœ¨æ¦‚å¿µç©ºé—´ä¸­çš„æ•°å€¼è¡¨ç¤ºã€‚æ¢å¥è¯è¯´ï¼Œè¿™äº›æ˜¯ **è¡¨ç¤ºç»™å®šæ–‡æœ¬çš„ *å«ä¹‰* çš„æ•°å­—**ã€‚

æ–‡æœ¬åµŒå…¥å¯ä»¥ç”¨æ¥è®¡ç®—ç”¨æˆ·æŸ¥è¯¢ä¸çŸ¥è¯†åº“ä¸­æ¯ä¸ªé¡¹ç›®ä¹‹é—´çš„ç›¸ä¼¼æ€§å¾—åˆ†ã€‚è¿™ä¸ªè¿‡ç¨‹çš„ç»“æœæ˜¯ **æ¯ä¸ªé¡¹ç›®ä¸è¾“å…¥æŸ¥è¯¢ç›¸å…³æ€§çš„æ’å**ã€‚

ç„¶åï¼Œæ£€ç´¢å™¨å¯ä»¥é€‰æ‹©å‰ k ä¸ªï¼ˆä¾‹å¦‚ k=3ï¼‰æœ€ç›¸å…³çš„é¡¹ç›®ï¼Œå¹¶å°†å®ƒä»¬æ³¨å…¥åˆ°ç”¨æˆ·æç¤ºä¸­ã€‚è¿™ä¸ªå¢å¼ºçš„æç¤ºéšåè¢«ä¼ é€’ç»™ LLM è¿›è¡Œç”Ÿæˆã€‚

![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*jpTwdBmoTlJlfPAm0oJiVQ.png)

### çŸ¥è¯†åº“

RAG ç³»ç»Ÿçš„ä¸‹ä¸€ä¸ªå…³é”®è¦ç´ æ˜¯çŸ¥è¯†åº“ã€‚è¿™ä¸ª **åŒ…å«äº†æ‚¨å¸Œæœ›æä¾›ç»™ LLM çš„æ‰€æœ‰ä¿¡æ¯**ã€‚è™½ç„¶æœ‰æ— æ•°ç§æ–¹æ³•å¯ä»¥æ„å»º RAG çš„çŸ¥è¯†åº“ï¼Œä½†åœ¨è¿™é‡Œæˆ‘å°†é‡ç‚¹ä»‹ç»å¦‚ä½•ä»ä¸€ç»„æ–‡æ¡£ä¸­æ„å»ºä¸€ä¸ªçŸ¥è¯†åº“ã€‚

è¿™ä¸ªè¿‡ç¨‹å¯ä»¥åˆ†ä¸º **4 ä¸ªå…³é”®æ­¥éª¤** \[2,3].

1. **åŠ è½½æ–‡æ¡£** â€” è¿™åŒ…æ‹¬æ”¶é›†ä¸€ç»„æ–‡æ¡£å¹¶ç¡®ä¿å®ƒä»¬å¤„äºå¯è§£æçš„æ ¼å¼ï¼ˆç¨åä¼šè¯¦ç»†ä»‹ç»ï¼‰ã€‚
2. **åˆ†å—æ–‡æ¡£â€”**ç”±äº LLM çš„ä¸Šä¸‹æ–‡çª—å£æœ‰é™ï¼Œæ–‡æ¡£å¿…é¡»è¢«æ‹†åˆ†æˆæ›´å°çš„å— **ï¼ˆä¾‹å¦‚ï¼Œ** 256 æˆ– 512 ä¸ªå­—ç¬¦é•¿ï¼‰ã€‚
3. **åµŒå…¥å—** â€” ä½¿ç”¨æ–‡æœ¬åµŒå…¥æ¨¡å‹å°†æ¯ä¸ªå—è½¬æ¢ä¸ºæ•°å­—ã€‚
4. **åŠ è½½åˆ°å‘é‡æ•°æ®åº“**â€” å°†æ–‡æœ¬åµŒå…¥åŠ è½½åˆ°æ•°æ®åº“ï¼ˆå³å‘é‡æ•°æ®åº“ï¼‰ä¸­ã€‚

![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*VWG6Tr0OxCnD5Mvygm5DCA.png)

## ä¸€äº›ç»†å¾®å·®åˆ«

è™½ç„¶æ„å»º RAG ç³»ç»Ÿçš„æ­¥éª¤åœ¨æ¦‚å¿µä¸Šå¾ˆç®€å•ï¼Œä½†ä¸€äº›ç»†å¾®å·®åˆ«å¯èƒ½ä½¿å¾—åœ¨ç°å®ä¸–ç•Œä¸­æ„å»ºä¸€ä¸ªç³»ç»Ÿå˜å¾—æ›´åŠ å¤æ‚ã€‚

**æ–‡æ¡£å‡†å¤‡**â€”RAG ç³»ç»Ÿçš„è´¨é‡å–å†³äºä»æºæ–‡æ¡£ä¸­æå–æœ‰ç”¨ä¿¡æ¯çš„èƒ½åŠ›ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä¸€ä¸ªæ–‡æ¡£æ ¼å¼æ··ä¹±ï¼Œå……æ»¡äº†å›¾åƒå’Œè¡¨æ ¼ï¼Œé‚£ä¹ˆè§£æèµ·æ¥ä¼šæ¯”ä¸€ä¸ªæ ¼å¼è‰¯å¥½çš„æ–‡æœ¬æ–‡ä»¶æ›´å›°éš¾ã€‚

**é€‰æ‹©åˆé€‚çš„å—å¤§å°**â€”æˆ‘ä»¬å·²ç»æåˆ°ç”±äº LLM ä¸Šä¸‹æ–‡çª—å£çš„éœ€è¦è¿›è¡Œåˆ†å—ã€‚ç„¶è€Œï¼Œè¿˜æœ‰ 2 ä¸ªé¢å¤–çš„åˆ†å—åŠ¨æœºã€‚

**é¦–å…ˆ**ï¼Œå®ƒå¯ä»¥é™ä½ï¼ˆè®¡ç®—ï¼‰æˆæœ¬ã€‚ä½ åœ¨æç¤ºä¸­æ³¨å…¥çš„æ–‡æœ¬è¶Šå¤šï¼Œç”Ÿæˆå®Œæˆæ‰€éœ€çš„è®¡ç®—å°±è¶Šå¤šã€‚**ç¬¬äºŒ**æ˜¯æ€§èƒ½ã€‚ç‰¹å®šæŸ¥è¯¢çš„ç›¸å…³ä¿¡æ¯å¾€å¾€é›†ä¸­åœ¨æºæ–‡æ¡£ä¸­ï¼ˆé€šå¸¸ä»…ä¸€å¥è¯å°±å¯ä»¥å›ç­”ä¸€ä¸ªé—®é¢˜ï¼‰ã€‚åˆ†å—æœ‰åŠ©äºæœ€å°åŒ–ä¼ é€’ç»™æ¨¡å‹çš„æ— å…³ä¿¡æ¯çš„æ•°é‡ \[4\]ã€‚

**æ”¹å–„æœç´¢** â€” è™½ç„¶æ–‡æœ¬åµŒå…¥æä¾›äº†ä¸€ç§å¼ºå¤§ä¸”å¿«é€Ÿçš„æœç´¢æ–¹å¼ï¼Œä½†å®ƒå¹¶ä¸æ€»æ˜¯èƒ½å¦‚äººæ‰€æ„¿åœ°å·¥ä½œã€‚æ¢å¥è¯è¯´ï¼Œå®ƒå¯èƒ½è¿”å›ä¸ç”¨æˆ·æŸ¥è¯¢â€œç›¸ä¼¼â€çš„ç»“æœï¼Œä½†å¯¹å›ç­”é—®é¢˜å¹¶æ²¡æœ‰å¸®åŠ©ï¼Œä¾‹å¦‚ï¼Œâ€œ*æ´›æ‰çŸ¶çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ*â€å¯èƒ½è¿”å›â€œ*çº½çº¦çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ*â€ã€‚

ç¼“è§£è¿™ä¸€é—®é¢˜çš„æœ€ç®€å•æ–¹æ³•æ˜¯é€šè¿‡è‰¯å¥½çš„æ–‡æ¡£å‡†å¤‡å’Œåˆ†å—ã€‚ç„¶è€Œï¼Œå¯¹äºæŸäº›ç”¨ä¾‹ï¼Œå¯èƒ½éœ€è¦é¢å¤–çš„ç­–ç•¥æ¥æ”¹å–„æœç´¢ï¼Œä¾‹å¦‚ä¸ºæ¯ä¸ªå—ä½¿ç”¨ **å…ƒæ ‡ç­¾**ã€é‡‡ç”¨ç»“åˆå…³é”®è¯å’ŒåµŒå…¥æœç´¢çš„ **æ··åˆæœç´¢**ï¼Œæˆ–ä½¿ç”¨ **é‡æ’åºå™¨**ï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨è®¡ç®—ä¸¤æ®µæ–‡æœ¬ç›¸ä¼¼æ€§çš„æ¨¡å‹ã€‚

## ç¤ºä¾‹ä»£ç ï¼šä½¿ç”¨ RAG æ”¹è¿› YouTube è¯„è®ºå“åº”å™¨

åœ¨å¯¹ RAG å·¥ä½œåŸç†æœ‰åŸºæœ¬äº†è§£åï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•åœ¨å®è·µä¸­ä½¿ç”¨å®ƒã€‚æˆ‘å°†åŸºäº [ä¸Šä¸€ç¯‡æ–‡ç« ](https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32) ä¸­çš„ç¤ºä¾‹ï¼Œåœ¨å…¶ä¸­æˆ‘ä½¿ç”¨ QLoRA å¯¹ Mistral-7B-Instruct è¿›è¡Œäº†å¾®è°ƒï¼Œä»¥å“åº” YouTube è¯„è®ºã€‚æˆ‘ä»¬å°†ä½¿ç”¨ LlamaIndex ä¸ºä¹‹å‰å¾®è°ƒçš„æ¨¡å‹æ·»åŠ  RAG ç³»ç»Ÿã€‚

ç¤ºä¾‹ä»£ç å¯åœ¨ [Colab Notebook](https://colab.research.google.com/drive/1peJukr-9E1zCo1iAalbgDPJmNMydvQms?usp=sharing) ä¸­å…è´¹è·å¾—ï¼Œè¯¥ Notebook å¯ä»¥åœ¨æä¾›çš„ï¼ˆå…è´¹ï¼‰T4 GPU ä¸Šè¿è¡Œã€‚æ­¤ç¤ºä¾‹çš„æºæ–‡ä»¶å¯åœ¨ [GitHub ä»“åº“](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/rag) ä¸­æ‰¾åˆ°ã€‚

ğŸ”— [Google Colab](https://colab.research.google.com/drive/1peJukr-9E1zCo1iAalbgDPJmNMydvQms?usp=sharing) \| [GitHub Repo](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/rag)

### å¯¼å…¥

æˆ‘ä»¬é¦–å…ˆå®‰è£…å¹¶å¯¼å…¥å¿…è¦çš„ Python åº“ã€‚

```python
!pip install llama-index
!pip install llama-index-embeddings-huggingface
!pip install peft
!pip install auto-gptq
!pip install optimum
!pip install bitsandbytes
## å¦‚æœä¸æ˜¯åœ¨ Colab ä¸Šè¿è¡Œï¼Œè¯·ç¡®ä¿ä¹Ÿå®‰è£… transformers
```

```python
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.postprocessor import SimilarityPostprocessor
```

### è®¾ç½®çŸ¥è¯†åº“

æˆ‘ä»¬å¯ä»¥é€šè¿‡å®šä¹‰æˆ‘ä»¬çš„åµŒå…¥æ¨¡å‹ã€å—å¤§å°å’Œå—é‡å æ¥é…ç½®æˆ‘ä»¬çš„çŸ¥è¯†åº“ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨æ¥è‡ªBAAIçš„\~33Må‚æ•°[bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5)åµŒå…¥æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯åœ¨Hugging Face hubä¸Šè·å–ã€‚å…¶ä»–åµŒå…¥æ¨¡å‹é€‰é¡¹å¯ä»¥åœ¨è¿™ä¸ª[text embedding leaderboard](https://huggingface.co/spaces/mteb/leaderboard)ä¸Šæ‰¾åˆ°ã€‚

```python
## import any embedding model on HF hub
Settings.embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5")

Settings.llm = None # we won't use LlamaIndex to set up LLM
Settings.chunk_size = 256
Settings.chunk_overlap = 25
```
æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åŠ è½½æºæ–‡æ¡£ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘æœ‰ä¸€ä¸ªåä¸ºâ€œ[*articles*](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/rag/articles)â€çš„æ–‡ä»¶å¤¹ï¼Œå…¶ä¸­åŒ…å«æˆ‘åœ¨[fat tails](https://towardsdatascience.com/pareto-power-laws-and-fat-tails-0355a187ee6a)ä¸Šå†™çš„3ç¯‡Mediumæ–‡ç« çš„PDFç‰ˆæœ¬ã€‚å¦‚æœåœ¨Colabä¸­è¿è¡Œï¼Œæ‚¨å¿…é¡»ä»[GitHub repo](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/rag)ä¸‹è½½æ–‡ç« æ–‡ä»¶å¤¹å¹¶æ‰‹åŠ¨ä¸Šä¼ åˆ°æ‚¨çš„Colabç¯å¢ƒã€‚

å¯¹äºè¯¥æ–‡ä»¶å¤¹ä¸­çš„æ¯ä¸ªæ–‡ä»¶ï¼Œä¸‹é¢çš„å‡½æ•°å°†ä»PDFä¸­è¯»å–æ–‡æœ¬ï¼Œå°†å…¶æ‹†åˆ†æˆå—ï¼ˆåŸºäºä¹‹å‰å®šä¹‰çš„è®¾ç½®ï¼‰ï¼Œå¹¶å°†æ¯ä¸ªå—å­˜å‚¨åœ¨åä¸º*documents*çš„åˆ—è¡¨ä¸­ã€‚

```python
documents = SimpleDirectoryReader("articles").load_data()
```
ç”±äºè¿™äº›åšå®¢æ˜¯ç›´æ¥ä»Mediumä¸‹è½½ä¸ºPDFçš„ï¼Œå› æ­¤å®ƒä»¬æ›´åƒæ˜¯ç½‘é¡µï¼Œè€Œä¸æ˜¯æ ¼å¼è‰¯å¥½çš„æ–‡ç« ã€‚å› æ­¤ï¼Œä¸€äº›å—å¯èƒ½åŒ…å«ä¸æ–‡ç« æ— å…³çš„æ–‡æœ¬ï¼Œä¾‹å¦‚ç½‘é¡µæ ‡é¢˜å’ŒMediumæ–‡ç« æ¨èã€‚

åœ¨ä¸‹é¢çš„ä»£ç å—ä¸­ï¼Œæˆ‘å¯¹documentsä¸­çš„å—è¿›è¡Œç²¾ç‚¼ï¼Œåˆ é™¤æ–‡ç« ä¸»ä½“å‰åçš„å¤§éƒ¨åˆ†å—ã€‚

```python
print(len(documents)) # prints: 71
for doc in documents:
    if "Member-only story" in doc.text:
        documents.remove(doc)
        continue

    if "The Data Entrepreneurs" in doc.text:
        documents.remove(doc)

    if " min read" in doc.text:
        documents.remove(doc)

print(len(documents)) # prints: 61
```
æœ€åï¼Œæˆ‘ä»¬å¯ä»¥å°†ç²¾ç‚¼åçš„å—å­˜å‚¨åœ¨å‘é‡æ•°æ®åº“ä¸­ã€‚

```python
index = VectorStoreIndex.from_documents(documents)
```

### è®¾ç½®æ£€ç´¢å™¨

åœ¨æˆ‘ä»¬çš„çŸ¥è¯†åº“å»ºç«‹ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ LlamaIndex çš„ *VectorIndexRetriever()* åˆ›å»ºä¸€ä¸ªæ£€ç´¢å™¨ï¼Œå®ƒè¿”å›ä¸ç”¨æˆ·æŸ¥è¯¢æœ€ç›¸ä¼¼çš„ 3 ä¸ªå—ã€‚

```python
## set number of docs to retreive
top_k = 3

## configure retriever
retriever = VectorIndexRetriever(
    index=index,
    similarity_top_k=top_k,
)
```
æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªæŸ¥è¯¢å¼•æ“ï¼Œä½¿ç”¨æ£€ç´¢å™¨å’ŒæŸ¥è¯¢è¿”å›ä¸€ç»„ç›¸å…³çš„å—ã€‚

```python
## assemble query engine
query_engine = RetrieverQueryEngine(
    retriever=retriever,
    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.5)],
)
```

### ä½¿ç”¨æŸ¥è¯¢å¼•æ“

ç°åœ¨ï¼Œéšç€æˆ‘ä»¬çš„çŸ¥è¯†åº“å’Œæ£€ç´¢ç³»ç»Ÿçš„å»ºç«‹ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨å®ƒæ¥è¿”å›ä¸æŸ¥è¯¢ç›¸å…³çš„å†…å®¹ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†ä¼ é€’æˆ‘ä»¬å‘ShawGPTï¼ˆYouTubeè¯„è®ºå›å¤è€…ï¼‰æå‡ºçš„ç›¸åŒæŠ€æœ¯é—®é¢˜ï¼Œæ¥è‡ª[ä¸Šä¸€ç¯‡æ–‡ç« ](https://readmedium.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32)ã€‚

```python
query = "What is fat-tailedness?"
response = query_engine.query(query)
```
æŸ¥è¯¢å¼•æ“è¿”å›ä¸€ä¸ªå“åº”å¯¹è±¡ï¼Œå…¶ä¸­åŒ…å«æ–‡æœ¬ã€å…ƒæ•°æ®å’Œç›¸å…³å—çš„ç´¢å¼•ã€‚ä¸‹é¢çš„ä»£ç å—è¿”å›è¯¥ä¿¡æ¯çš„æ›´æ˜“è¯»ç‰ˆæœ¬ã€‚

```python
## reformat response
context = "Context:\n"
for i in range(top_k):
    context = context + response.source_nodes[i].text + "\n\n"

print(context)
```

```python
Context:
Some of the controversy might be explained by the observation that log-
normal distributions behave like Gaussian for low sigma and like Power Law
at high sigma [2].
However, to avoid controversy, we can depart (for now) from whether some
given data fits a Power Law or not and focus instead on fat tails.
Fat-tailedness â€” measuring the space between Mediocristan
and Extremistan
Fat Tails are a more general idea than Pareto and Power Law distributions.
One way we can think about it is that â€œfat-tailednessâ€ is the degree to which
rare events drive the aggregate statistics of a distribution. From this point of
view, fat-tailedness lives on a spectrum from not fat-tailed (i.e. a Gaussian) to
very fat-tailed (i.e. Pareto 80 â€“ 20).
This maps directly to the idea of Mediocristan vs Extremistan discussed
earlier. The image below visualizes different distributions across this
conceptual landscape [2].

print("mean kappa_1n = " + str(np.mean(kappa_dict[filename])))
    print("")
Mean Îº (1,100) values from 1000 runs for each dataset. Image by author.
These more stable results indicate Medium followers are the most fat-tailed,
followed by LinkedIn Impressions and YouTube earnings.
Note: One can compare these values to Table III in ref [3] to better understand each
Îº value. Namely, these values are comparable to a Pareto distribution with Î±
between 2 and 3.
Although each heuristic told a slightly different story, all signs point toward
Medium followers gained being the most fat-tailed of the 3 datasets.
Conclusion
While binary labeling data as fat-tailed (or not) may be tempting, fat-
tailedness lives on a spectrum. Here, we broke down 4 heuristics for
quantifying how fat-tailed data are.

Pareto, Power Laws, and Fat Tails
What they donâ€™t teach you in statistics
towardsdatascience.com
Although Pareto (and more generally power law) distributions give us a
salient example of fat tails, this is a more general notion that lives on a
spectrum ranging from thin-tailed (i.e. a Gaussian) to very fat-tailed (i.e.
Pareto 80 â€“ 20).
The spectrum of Fat-tailedness. Image by author.
This view of fat-tailedness provides us with a more flexible and precise way of
categorizing data than simply labeling it as a Power Law (or not). However,
this begs the question: how do we define fat-tailedness?
4 Ways to Quantify Fat Tails
```

### å°† RAG æ·»åŠ åˆ° LLM

æˆ‘ä»¬é¦–å…ˆä» Hugging Face hub ä¸‹è½½ [å¾®è°ƒæ¨¡å‹](https://readmedium.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32)ã€‚

```python
## load fine-tuned model from hub
from peft import PeftModel, PeftConfig
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "TheBloke/Mistral-7B-Instruct-v0.2-GPTQ"
model = AutoModelForCausalLM.from_pretrained(model_name,
                                             device_map="auto",
                                             trust_remote_code=False,
                                             revision="main")

config = PeftConfig.from_pretrained("shawhin/shawgpt-ft")
model = PeftModel.from_pretrained(model, "shawhin/shawgpt-ft")

## load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
```
ä½œä¸ºåŸºçº¿ï¼Œæˆ‘ä»¬å¯ä»¥è§‚å¯Ÿæ¨¡å‹åœ¨æ²¡æœ‰ä»»ä½•æ–‡ç« ä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹å¦‚ä½•å›åº”æŠ€æœ¯é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨ lambda å‡½æ•°åˆ›å»ºä¸€ä¸ªæç¤ºæ¨¡æ¿ï¼Œè¯¥å‡½æ•°æ¥å—è§‚ä¼—è¯„è®ºå¹¶è¿”å› LLM çš„æç¤ºã€‚æœ‰å…³æ­¤æç¤ºæ¥æºçš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§æœ¬ç³»åˆ—çš„ [ä¸Šä¸€ç¯‡æ–‡ç« ](https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32#5aad)ã€‚

```python
## prompt (no context)
intstructions_string = f"""ShawGPT, functioning as a virtual data science \
consultant on YouTube, communicates in clear, accessible language, escalating \
to technical depth upon request. It reacts to feedback aptly and ends \
responses with its signature 'â€“ShawGPT'.

ShawGPT will tailor the length of its responses to match the viewer's comment, \
providing concise acknowledgments to brief expressions of gratitude or \
feedback, thus keeping the interaction natural and engaging.

Please respond to the following comment.
"""

prompt_template = lambda comment: f'''[INST] {intstructions_string} \n{comment} \n[/INST]'''
comment = "What is fat-tailedness?"

prompt = prompt_template(comment)
print(prompt)
```

```python
[INST] ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. It reacts to feedback aptly and ends responses with its signature 'â€“ShawGPT'. ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, thus keeping the interaction natural and engaging.

Please respond to the following comment.
 
What is fat-tailedness? 
[/INST]
```
æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç å°†æ­¤æç¤ºä¼ é€’ç»™æ¨¡å‹ã€‚

```python
model.eval()

inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(input_ids=inputs["input_ids"].to("cuda"), 
                          max_new_tokens=280)

print(tokenizer.batch_decode(outputs)[0])
```
è¿™æ˜¯æ¨¡å‹çš„å“åº”ï¼ˆæ²¡æœ‰ä¸Šä¸‹æ–‡ï¼‰ã€‚

```python
Great question!

Fat-tailedness is a statistical property of a distribution. In simple terms, 
it refers to the presence of extreme outliers or heavy tails in the 
distribution.

For instance, consider the distribution of heights in a population. A normal 
distribution would have most people clustered around an average height with a 
few people deviating slightly from the mean. However, in a fat-tailed 
distribution, you would observe a larger number of people being significantly 
taller or shorter than the average.

In finance, fat-tailedness is often used to describe the presence of extreme 
market moves or outliers in financial returns. It's an important concept in 
risk management and financial modeling.

I hope this explanation helps! Let me know if you have any questions.
â€“ShawGPT
```
è™½ç„¶å“åº”çš„é£æ ¼å’Œæ ¼å¼éƒ½å¾ˆå¥½ï¼Œä½†æ¨¡å‹å¯¹ fat-tailedness çš„è§£é‡Šä¸æˆ‘åœ¨æˆ‘çš„ [è§†é¢‘](https://www.youtube.com/playlist?list=PLz-ep5RbHosVrT89BRNX-IGKHYxvx9wqi) å’Œ [åšå®¢ç³»åˆ—](https://towardsdatascience.com/pareto-power-laws-and-fat-tails-0355a187ee6a) ä¸­å®šä¹‰çš„æœ‰æ‰€ä¸åŒã€‚

è®©æˆ‘ä»¬çœ‹çœ‹å½“æˆ‘ä»¬åŒ…å«é€‚å½“çš„ä¸Šä¸‹æ–‡æ—¶æ¨¡å‹çš„å“åº”ä¼šå‘ç”Ÿä»€ä¹ˆã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åˆ›å»ºå¦ä¸€ä¸ªæç¤ºæ¨¡æ¿ï¼Œè¯¥æ¨¡æ¿ä¹Ÿå¯ä»¥ä» RAG ç³»ç»Ÿä¸­è·å–ä¸Šä¸‹æ–‡ã€‚

```python
## prompt (with context)
prompt_template_w_context = lambda context, comment: f"""[INST]ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. \
It reacts to feedback aptly and ends responses with its signature 'â€“ShawGPT'. \
ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, \
thus keeping the interaction natural and engaging.

{context}
Please respond to the following comment. Use the context above if it is helpful.

{comment}
[/INST]
"""
```
æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æŸ¥è¯¢å¼•æ“ä¸­çš„ä¸Šä¸‹æ–‡å’Œè§‚ä¼—è¯„è®ºä¼ é€’ç»™æ¨¡å‹ã€‚

```python
prompt = prompt_template_w_context(context, comment)

inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(input_ids=inputs["input_ids"].to("cuda"), max_new_tokens=280)

print(tokenizer.batch_decode(outputs)[0])
```
è¿™æ˜¯æ–°çš„å“åº”ï¼ˆå¸¦ä¸Šä¸‹æ–‡ï¼‰ã€‚

```python
Great question!

Fat-tailedness is a property of a distribution that describes how much the 
presence of extreme values (rare events) affects the overall statistics of the 
distribution.

In the context of the video, the author discusses how Mediocristan and 
Extremistan distributions differ in terms of fat-tailedness. Mediocristan 
distributions have a relatively small impact from extreme values, while 
Extremistan distributions are heavily influenced by extreme values.

The author also mentions that fat-tailedness is a more general concept than 
specific distributions like Pareto or Power Laws. Instead, it lives on a 
spectrum ranging from thin-tailed (Gaussian) to very fat-tailed (Pareto 80-20).

I hope that helps clarify things a bit! Let me know if you have any questions.
â€“ShawGPT
```
è¿™æ¯”æ²¡æœ‰ä¸Šä¸‹æ–‡çš„å“åº”æ›´å¥½åœ°æ•æ‰äº†æˆ‘å¯¹ fat tails çš„è§£é‡Šï¼Œå¹¶ä¸”ç”šè‡³æåˆ°äº† Mediocristan å’Œ Extremistan çš„ç»†åˆ†æ¦‚å¿µã€‚

## æ¥ä¸‹æ¥æ˜¯ä»€ä¹ˆï¼Ÿ

åœ¨è¿™é‡Œï¼Œæˆ‘ä¸ºåˆå­¦è€…æä¾›äº†ä¸€ä¸ªå…³äº RAG çš„å‹å¥½ä»‹ç»ï¼Œå¹¶åˆ†äº«äº†å¦‚ä½•ä½¿ç”¨ LlamaIndex å®ç°å®ƒçš„å…·ä½“ç¤ºä¾‹ã€‚RAG ä½¿æˆ‘ä»¬èƒ½å¤Ÿé€šè¿‡å¯æ›´æ–°å’Œç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†æ¥æ”¹å–„ LLM ç³»ç»Ÿã€‚

è™½ç„¶æœ€è¿‘çš„ AI çƒ­æ½®ä¸»è¦é›†ä¸­åœ¨æ„å»º AI åŠ©æ‰‹ä¸Šï¼Œä½†ä¸€ä¸ªå¼ºå¤§çš„ï¼ˆä½†ä¸é‚£ä¹ˆæµè¡Œçš„ï¼‰åˆ›æ–°æ¥è‡ªäºæ–‡æœ¬åµŒå…¥ï¼ˆå³æˆ‘ä»¬ç”¨æ¥è¿›è¡Œæ£€ç´¢çš„ä¸œè¥¿ï¼‰ã€‚åœ¨æœ¬ç³»åˆ—çš„ä¸‹ä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†æ›´è¯¦ç»†åœ°æ¢è®¨ **æ–‡æœ¬åµŒå…¥**ï¼ŒåŒ…æ‹¬å®ƒä»¬å¦‚ä½•ç”¨äº **è¯­ä¹‰æœç´¢** å’Œ **åˆ†ç±»ä»»åŠ¡**ã€‚

**æ›´å¤šå…³äº LLM çš„å†…å®¹ ğŸ‘‡**

## èµ„æº

**è¿æ¥**: [æˆ‘çš„ç½‘ç«™](https://shawhintalebi.com/) \| [é¢„çº¦ç”µè¯](https://calendly.com/shawhintalebi)

**ç¤¾äº¤**: [YouTube ğŸ¥](https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA) \| [LinkedIn](https://www.linkedin.com/in/shawhintalebi/) \| [Instagram](https://www.instagram.com/shawhintalebi)

**æ”¯æŒ**: [è¯·æˆ‘å–æ¯å’–å•¡](https://www.buymeacoffee.com/shawhint) â˜•ï¸

\[1] [RAG \> FT (ç»éªŒæ€§)](https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb)

\[2] [LlamaIndex ç½‘ç»œç ”è®¨ä¼šï¼šä¸ºç”Ÿäº§æ„å»º LLM åº”ç”¨ç¨‹åºï¼Œç¬¬ä¸€éƒ¨åˆ†ï¼ˆä¸ Anyscale è”åˆä¸»æŒï¼‰](https://www.youtube.com/watch?v=efbn-3tPI_M)

\[3] [LlamaIndex æ–‡æ¡£](https://docs.llamaindex.ai/en/stable/understanding/loading/loading.html)

\[4] [LlamaIndex ç½‘ç»œç ”è®¨ä¼šï¼šä½¿ RAG å‡†å¤‡å¥½ç”Ÿäº§](https://www.youtube.com/watch?v=Zj5RCweUHIk&list=WL&index=4)


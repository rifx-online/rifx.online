---
title: "Crawl4AIï¼šæ‚¨çš„ç»ˆæå¼‚æ­¥ç½‘ç»œçˆ¬è¡Œä¼´ä¾£ ğŸ•·ï¸ğŸ¤–"
meta_title: "Crawl4AIï¼šæ‚¨çš„ç»ˆæå¼‚æ­¥ç½‘ç»œçˆ¬è¡Œä¼´ä¾£ ğŸ•·ï¸ğŸ¤–"
description: "Crawl4AIæ˜¯ä¸€ä¸ªå¼€æºçš„Pythonåº“ï¼Œæ—¨åœ¨æé«˜ç½‘ç»œçˆ¬è™«å’Œæ•°æ®æå–çš„æ•ˆç‡ï¼Œç‰¹åˆ«é€‚ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹å’Œäººå·¥æ™ºèƒ½åº”ç”¨ã€‚å®ƒæ”¯æŒå®Œå…¨å¼‚æ­¥æ“ä½œï¼Œå…è®¸å¹¶å‘çˆ¬å–å¤šä¸ªURLï¼Œå¹¶æä¾›å¤šç§æ•°æ®è¾“å‡ºæ ¼å¼ã€‚Crawl4AIå…·å¤‡å¼ºå¤§çš„ç‰¹æ€§ï¼ŒåŒ…æ‹¬åª’ä½“æ ‡ç­¾æå–ã€å…ƒæ•°æ®æå–ã€ä¼šè¯ç®¡ç†ã€åŠ¨æ€å†…å®¹çˆ¬å–å’Œè‡ªå®šä¹‰é’©å­ç­‰ã€‚è¯¥åº“æ˜“äºå®‰è£…ï¼Œé€‚åˆå„ç§ç”¨ä¾‹ï¼Œæ€§èƒ½ä¼˜äºè®¸å¤šä»˜è´¹æœåŠ¡ã€‚"
date: 2024-12-22T03:50:49Z
image: "https://wsrv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ebriuVM77YzFHlkjbdIuFQ.jpeg"
categories: ["Programming", "Technology/Web", "Data Science"]
author: "Rifx.Online"
tags: ["web", "crawling", "asynchronous", "Python", "extraction"]
draft: False

---





Crawl4AI æ˜¯ä¸€ä¸ª **å¼€æº Python åº“**ï¼Œæ—¨åœ¨ç®€åŒ–ç½‘ç»œçˆ¬è™«å¹¶è½»æ¾æå–ç½‘é¡µä¸Šçš„æœ‰ä»·å€¼ä¿¡æ¯ã€‚æ— è®ºæ‚¨æ˜¯å°†å…¶é›†æˆä½œä¸º REST APIï¼Œè¿˜æ˜¯ç›´æ¥åœ¨æ‚¨çš„ Python é¡¹ç›®ä¸­ä½¿ç”¨ï¼ŒCrawl4AI éƒ½æä¾›äº†ä¸€ç§ **å¼ºå¤§**ã€**çµæ´»** å’Œ **å®Œå…¨å¼‚æ­¥** çš„è§£å†³æ–¹æ¡ˆï¼Œä¸“ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œäººå·¥æ™ºèƒ½åº”ç”¨é‡èº«å®šåˆ¶ã€‚

## ä»‹ç»

**Crawl4AI** æ—¨åœ¨ **ç®€åŒ–ç½‘é¡µçˆ¬å–å’Œæ•°æ®æå–çš„è¿‡ç¨‹ï¼Œæé«˜æ•ˆç‡**ã€‚æ— è®ºæ‚¨æ˜¯åœ¨æ„å»ºå¤æ‚çš„ AI åº”ç”¨ç¨‹åºè¿˜æ˜¯åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒCrawl4AI éƒ½æä¾›äº†æ‚¨æ‰€éœ€çš„å·¥å…·ï¼Œä»¥ä¼˜åŒ–æ‚¨çš„å·¥ä½œæµç¨‹ã€‚å‡­å€Ÿå…¨é¢çš„å¼‚æ­¥æ”¯æŒï¼ŒCrawl4AI ç¡®ä¿æ‚¨çš„çˆ¬å–ä»»åŠ¡ **å¿«é€Ÿ**ã€**å¯é ** å’Œ **å¯æ‰©å±•**ã€‚

## å¿«é€Ÿå¼€å§‹ ğŸš€

å¿«é€Ÿä¸Šæ‰‹ Crawl4AIï¼ä»¥ä¸‹æ˜¯ä¸€ä¸ªç®€å•çš„ç¤ºä¾‹ï¼Œå±•ç¤ºå…¶å¼ºå¤§çš„å¼‚æ­¥èƒ½åŠ›ï¼š


```python
import asyncio
from crawl4ai import AsyncWebCrawler

async def main():
    # åˆå§‹åŒ–å¼‚æ­¥ç½‘é¡µçˆ¬è™«
    async with AsyncWebCrawler(verbose=True) as crawler:
        # çˆ¬å–æŒ‡å®šçš„ URL
        result = await crawler.arun(url="https://www.nbcnews.com/business")
        # ä»¥ Markdown æ ¼å¼æ˜¾ç¤ºæå–çš„å†…å®¹
        print(result.markdown)
## æ‰§è¡Œå¼‚æ­¥ä¸»å‡½æ•°
if __name__ == "__main__":
    asyncio.run(main())
```

## è¯´æ˜ï¼š

1. **å¯¼å…¥åº“**ï¼šä» `crawl4ai` åº“å¯¼å…¥ `AsyncWebCrawler` å’Œ `asyncio` æ¨¡å—ã€‚
2. **åˆ›å»ºå¼‚æ­¥ä¸Šä¸‹æ–‡**ï¼šä½¿ç”¨å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å®ä¾‹åŒ– `AsyncWebCrawler`ã€‚
3. **è¿è¡Œçˆ¬è™«**ï¼šåˆ©ç”¨ `arun()` æ–¹æ³•å¼‚æ­¥çˆ¬å–æŒ‡å®šçš„ URL å¹¶æå–æœ‰æ„ä¹‰çš„å†…å®¹ã€‚
4. **æ‰“å°ç»“æœ**ï¼šè¾“å‡ºæå–çš„å†…å®¹ï¼Œæ ¼å¼åŒ–ä¸º Markdownã€‚
5. **æ‰§è¡Œå¼‚æ­¥å‡½æ•°**ï¼šä½¿ç”¨ `asyncio.run()` æ‰§è¡Œå¼‚æ­¥ `main` å‡½æ•°ã€‚

## ç‰¹æ€§ âœ¨

Crawl4AI æ‹¥æœ‰ä¸€ç³»åˆ—æ—¨åœ¨è®©æ‚¨çš„ç½‘ç»œçˆ¬è™«å’Œæ•°æ®æå–ä»»åŠ¡æ— ç¼è¿›è¡Œçš„ç‰¹æ€§ï¼š

* **ğŸ†“ å®Œå…¨å…è´¹ \& å¼€æº**ï¼šæ²¡æœ‰éšè—è´¹ç”¨æˆ–è®¸å¯è´¹ç”¨ã€‚
* **ğŸš€ è¶…å¿«æ€§èƒ½**ï¼šåœ¨é€Ÿåº¦ä¸Šè¶…è¶Šè®¸å¤šä»˜è´¹æœåŠ¡ã€‚
* **ğŸ¤– LLM\-å‹å¥½çš„è¾“å‡ºæ ¼å¼**ï¼šæ”¯æŒ JSONã€æ¸…ç†åçš„ HTML å’Œ Markdownã€‚
* **ğŸŒ å¹¶å‘çˆ¬å–**ï¼šåŒæ—¶çˆ¬å–å¤šä¸ª URLï¼Œä»¥å®ç°æœ€å¤§æ•ˆç‡ã€‚
* **ğŸ¨ åª’ä½“æ ‡ç­¾æå–**ï¼šæå–æ‰€æœ‰åª’ä½“æ ‡ç­¾ï¼ŒåŒ…æ‹¬å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ã€‚
* **ğŸ”— å…¨é¢é“¾æ¥æå–**ï¼šæ”¶é›†å¤–éƒ¨å’Œå†…éƒ¨é“¾æ¥ã€‚
* **ğŸ“š å…ƒæ•°æ®æå–**ï¼šä»ç½‘é¡µä¸­æå–è¯¦ç»†çš„å…ƒæ•°æ®ã€‚
* **ğŸ”„ è‡ªå®šä¹‰é’©å­ \& èº«ä»½éªŒè¯**ï¼šåœ¨çˆ¬å–ä¹‹å‰è‡ªå®šä¹‰å¤´éƒ¨ã€èº«ä»½éªŒè¯å¹¶ä¿®æ”¹é¡µé¢ã€‚
* **ğŸ•µï¸ ç”¨æˆ·ä»£ç†è‡ªå®šä¹‰**ï¼šè½»æ¾æ¨¡æ‹Ÿä¸åŒçš„æµè§ˆå™¨æˆ–è®¾å¤‡ã€‚
* **ğŸ–¼ï¸ é¡µé¢æˆªå›¾**ï¼šæ•æ‰ç½‘é¡µçš„è§†è§‰å¿«ç…§ã€‚
* **ğŸ“œ JavaScript æ‰§è¡Œ**ï¼šåœ¨çˆ¬å–ä¹‹å‰æ‰§è¡Œè‡ªå®šä¹‰ JavaScript ä»¥è·å–åŠ¨æ€å†…å®¹ã€‚
* **ğŸ“Š ç»“æ„åŒ–æ•°æ®æå–**ï¼šåˆ©ç”¨ `JsonCssExtractionStrategy` è¿›è¡Œç²¾ç¡®çš„æ•°æ®ç»“æ„åŒ–ã€‚
* **ğŸ“š å¤šæ ·åŒ–çš„åˆ†å—ç­–ç•¥**ï¼šåŒ…æ‹¬åŸºäºä¸»é¢˜ã€æ­£åˆ™è¡¨è¾¾å¼ã€å¥å­ç­‰çš„ç­–ç•¥ã€‚
* **ğŸ§  é«˜çº§æå–æŠ€æœ¯**ï¼šå…·å¤‡ä½™å¼¦èšç±»å’Œ LLM é©±åŠ¨çš„æå–åŠŸèƒ½ã€‚
* **ğŸ¯ ç²¾ç¡®çš„ CSS é€‰æ‹©å™¨æ”¯æŒ**ï¼šä½¿ç”¨å‡†ç¡®çš„ CSS é€‰æ‹©å™¨å®šä½ç‰¹å®šæ•°æ®ç‚¹ã€‚
* **ğŸ“ åŸºäºæŒ‡ä»¤çš„ä¼˜åŒ–**ï¼šä¼ é€’æŒ‡ä»¤æˆ–å…³é”®è¯ä»¥å¢å¼ºæ•°æ®æå–ã€‚
* **ğŸ”’ ä»£ç†æ”¯æŒ**ï¼šé€šè¿‡ä»£ç†é…ç½®å¢å¼ºéšç§å’Œè®¿é—®æƒé™ã€‚
* **ğŸ”„ ä¼šè¯ç®¡ç†**ï¼šè½»æ¾å¤„ç†å¤æ‚çš„å¤šé¡µé¢çˆ¬å–åœºæ™¯ã€‚
* **ğŸŒ å®Œå…¨å¼‚æ­¥æ¶æ„**ï¼šé€šè¿‡å¼‚æ­¥æ”¯æŒæå‡æ€§èƒ½å’Œå¯æ‰©å±•æ€§ã€‚

## å®‰è£… ğŸ› ï¸

Crawl4AI æä¾›çµæ´»çš„å®‰è£…é€‰é¡¹ï¼Œä»¥æ»¡è¶³å„ç§ç”¨ä¾‹ã€‚é€‰æ‹©æœ€é€‚åˆæ‚¨å·¥ä½œæµç¨‹çš„æ–¹æ³•ï¼š

## ä½¿ç”¨ pip ğŸ

### åŸºæœ¬å®‰è£…

å¯¹äºæ ‡å‡†çš„ç½‘é¡µçˆ¬å–å’ŒæŠ“å–ä»»åŠ¡ï¼š

```python
pip install crawl4ai
```
*é»˜è®¤æƒ…å†µä¸‹ï¼Œè¿™å°†ä½¿ç”¨ Playwright å®‰è£… Crawl4AI çš„å¼‚æ­¥ç‰ˆæœ¬è¿›è¡Œç½‘é¡µçˆ¬å–ã€‚*

**ğŸ‘‰ æ³¨æ„**ï¼šå¦‚æœåœ¨å®‰è£…è¿‡ç¨‹ä¸­é‡åˆ°ä¸ Playwright ç›¸å…³çš„é”™è¯¯ï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•ä¹‹ä¸€æ‰‹åŠ¨å®‰è£… Playwrightï¼š

* **å‘½ä»¤è¡Œï¼š**

```python
playwright install
```
* **ç‰¹å®šçš„ Chromium å®‰è£…ï¼š**

```python
python -m playwright install chromium
```

### ä½¿ç”¨åŒæ­¥ç‰ˆæœ¬çš„å®‰è£…

å¦‚æœæ‚¨æ›´å–œæ¬¢ä½¿ç”¨ Selenium çš„åŒæ­¥ç‰ˆæœ¬ï¼š


```python
pip install crawl4ai[sync]
```

### å¼€å‘å®‰è£…

å¯¹äºå¸Œæœ›ä¿®æ”¹æºä»£ç çš„è´¡çŒ®è€…ï¼š


```python
git clone https://github.com/unclecode/crawl4ai.git
cd crawl4ai
pip install -e .
```

## é«˜çº§ç”¨æ³• ğŸ”¬

é€šè¿‡è¿™äº›é«˜çº§åŠŸèƒ½å’Œç”¨ä¾‹ï¼Œé‡Šæ”¾ Crawl4AI çš„å…¨éƒ¨æ½œåŠ›ï¼š

## æ‰§è¡Œ JavaScript \& ä½¿ç”¨ CSS é€‰æ‹©å™¨

é€šè¿‡æ‰§è¡Œè‡ªå®šä¹‰ JavaScript å’Œä½¿ç”¨ CSS é€‰æ‹©å™¨æ¥å¢å¼ºæ‚¨çš„çˆ¬è™«ä»»åŠ¡ã€‚

```python
import asyncio
from crawl4ai import AsyncWebCrawler

async def main():
    async with AsyncWebCrawler(verbose=True) as crawler:
        js_code = [
            "const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More')); loadMoreButton && loadMoreButton.click();"
        ]
        result = await crawler.arun(
            url="https://www.nbcnews.com/business",
            js_code=js_code,
            css_selector="article.tease-card",
            bypass_cache=True
        )
        print(result.extracted_content)
if __name__ == "__main__":
    asyncio.run(main())
```

## ä½¿ç”¨ä»£ç†

é€šè¿‡å°†æ‚¨çš„çˆ¬è™«ä»»åŠ¡é€šè¿‡ä»£ç†è¿›è¡Œè·¯ç”±ï¼Œå¢å¼ºæ‚¨çš„éšç§å’Œè®¿é—®ã€‚

```python
import asyncio
from crawl4ai import AsyncWebCrawler

async def main():
    async with AsyncWebCrawler(verbose=True, proxy="http://127.0.0.1:7890") as crawler:
        result = await crawler.arun(
            url="https://www.nbcnews.com/business",
            bypass_cache=True
        )
        print(result.markdown)
if __name__ == "__main__":
    asyncio.run(main())
```

## æå–ç»“æ„åŒ–æ•°æ®è€Œæ— éœ€ LLM

åˆ©ç”¨ `JsonCssExtractionStrategy` é€šè¿‡ CSS é€‰æ‹©å™¨ç²¾ç¡®æå–ç»“æ„åŒ–æ•°æ®ã€‚

```python
import asyncio
import json
from crawl4ai import AsyncWebCrawler
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

async def extract_news_teasers():
    schema = {
        "name": "News Teaser Extractor",
        "baseSelector": ".wide-tease-item__wrapper",
        "fields": [
            {"name": "category", "selector": ".unibrow span[data-testid='unibrow-text']", "type": "text"},
            {"name": "headline", "selector": ".wide-tease-item__headline", "type": "text"},
            {"name": "summary", "selector": ".wide-tease-item__description", "type": "text"},
            {"name": "time", "selector": "[data-testid='wide-tease-date']", "type": "text"},
            {
                "name": "image",
                "type": "nested",
                "selector": "picture.teasePicture img",
                "fields": [
                    {"name": "src", "type": "attribute", "attribute": "src"},
                    {"name": "alt", "type": "attribute", "attribute": "alt"},
                ],
            },
            {"name": "link", "selector": "a[href]", "type": "attribute", "attribute": "href"},
        ],
    }
    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)
    async with AsyncWebCrawler(verbose=True) as crawler:
        result = await crawler.arun(
            url="https://www.nbcnews.com/business",
            extraction_strategy=extraction_strategy,
            bypass_cache=True,
        )
        assert result.success, "Failed to crawl the page"
        news_teasers = json.loads(result.extracted_content)
        print(f"Successfully extracted {len(news_teasers)} news teasers")
        print(json.dumps(news_teasers[0], indent=2))
if __name__ == "__main__":
    asyncio.run(extract_news_teasers())
```

## ä½¿ç”¨ OpenAI æå–ç»“æ„åŒ–æ•°æ®

åˆ©ç”¨ OpenAI çš„èƒ½åŠ›åŠ¨æ€æå–å’Œç»“æ„åŒ–æ•°æ®ã€‚


```python
import os
import asyncio
from crawl4ai import AsyncWebCrawler
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from pydantic import BaseModel, Field

class OpenAIModelFee(BaseModel):
    model_name: str = Field(..., description="Name of the OpenAI model.")
    input_fee: str = Field(..., description="Fee for input token for the OpenAI model.")
    output_fee: str = Field(..., description="Fee for output token for the OpenAI model.")
async def main():
    async with AsyncWebCrawler(verbose=True) as crawler:
        result = await crawler.arun(
            url='https://openai.com/api/pricing/',
            word_count_threshold=1,
            extraction_strategy=LLMExtractionStrategy(
                provider="openai/gpt-4o",
                api_token=os.getenv('OPENAI_API_KEY'), 
                schema=OpenAIModelFee.schema(),
                extraction_type="schema",
                instruction="""From the crawled content, extract all mentioned model names along with their fees for input and output tokens. 
Do not miss any models in the entire content. One extracted model JSON format should look like this: 
{"model_name": "GPT-4", "input_fee": "US$10.00 / 1M tokens", "output_fee": "US$30.00 / 1M tokens"}."""
            ),            
            bypass_cache=True,
        )
        print(result.extracted_content)
if __name__ == "__main__":
    asyncio.run(main())
```

## ä¼šè¯ç®¡ç† \& åŠ¨æ€å†…å®¹çˆ¬å–

å¤„ç†å¤æ‚åœºæ™¯ï¼Œä¾‹å¦‚çˆ¬å–é€šè¿‡ JavaScript åŠ è½½çš„å¤šä¸ªé¡µé¢çš„åŠ¨æ€å†…å®¹ã€‚

```python
import asyncio
import re
from bs4 import BeautifulSoup
from crawl4ai import AsyncWebCrawler

async def crawl_typescript_commits():
    first_commit = ""
    async def on_execution_started(page):
        nonlocal first_commit 
        try:
            while True:
                await page.wait_for_selector('li.Box-sc-g0xbh4-0 h4')
                commit = await page.query_selector('li.Box-sc-g0xbh4-0 h4')
                commit = await commit.evaluate('(element) => element.textContent')
                commit = re.sub(r'\s+', '', commit)
                if commit and commit != first_commit:
                    first_commit = commit
                    break
                await asyncio.sleep(0.5)
        except Exception as e:
            print(f"è­¦å‘Šï¼šåœ¨ JavaScript æ‰§è¡Œåæœªå‡ºç°æ–°å†…å®¹ï¼š{e}")
    async with AsyncWebCrawler(verbose=True) as crawler:
        crawler.crawler_strategy.set_hook('on_execution_started', on_execution_started)
        url = "https://github.com/microsoft/TypeScript/commits/main"
        session_id = "typescript_commits_session"
        all_commits = []
        js_next_page = """
        const button = document.querySelector('a[data-testid="pagination-next-button"]');
        if (button) button.click();
        """
        for page in range(3):  # çˆ¬å– 3 é¡µ
            result = await crawler.arun(
                url=url,
                session_id=session_id,
                css_selector="li.Box-sc-g0xbh4-0",
                js=js_next_page if page > 0 else None,
                bypass_cache=True,
                js_only=page > 0
            )
            assert result.success, f"çˆ¬å–ç¬¬ {page + 1} é¡µå¤±è´¥"
            soup = BeautifulSoup(result.cleaned_html, 'html.parser')
            commits = soup.select("li.Box-sc-g0xbh4-0")
            all_commits.extend(commits)
            print(f"ç¬¬ {page + 1} é¡µï¼šæ‰¾åˆ° {len(commits)} ä¸ªæäº¤")
        await crawler.crawler_strategy.kill_session(session_id)
        print(f"æˆåŠŸçˆ¬å–äº† 3 é¡µä¸­çš„ {len(all_commits)} ä¸ªæäº¤")
if __name__ == "__main__":
    asyncio.run(crawl_typescript_commits())
```

## é€Ÿåº¦æ¯”è¾ƒ ğŸš€

Crawl4AI ä¸“ä¸º **é€Ÿåº¦** å’Œ **æ•ˆç‡** è€Œè®¾è®¡ï¼Œå§‹ç»ˆä¼˜äºè®¸å¤šä»˜è´¹æœåŠ¡ã€‚ä»¥ä¸‹æ˜¯æˆ‘ä»¬çš„æ€§èƒ½åŸºå‡†çš„ç®€è¦æ¦‚è§ˆï¼š

**æœåŠ¡è€—æ—¶å†…å®¹é•¿åº¦æ‰¾åˆ°çš„å›¾ç‰‡Firecrawl**7\.02 ç§’42,074 å­—ç¬¦49**Crawl4AI (ç®€å•çˆ¬å–)**1\.60 ç§’18,238 å­—ç¬¦49**Crawl4AI (æ‰§è¡Œ JS)**4\.64 ç§’40,869 å­—ç¬¦89

**å…³é”®è¦ç‚¹ï¼š**

* **ç®€å•çˆ¬å–**ï¼šCrawl4AI çš„é€Ÿåº¦æ˜¯ Firecrawl çš„ **4 å€ä»¥ä¸Š**ã€‚
* **æ‰§è¡Œ JavaScript**ï¼šå³ä½¿åœ¨æ‰§è¡Œ JavaScript ä»¥åŠ è½½é¢å¤–å†…å®¹ï¼ˆå›¾ç‰‡æ•°é‡ç¿»å€ï¼‰çš„æƒ…å†µä¸‹ï¼ŒCrawl4AI ä»ç„¶æ¯” Firecrawl çš„ç®€å•çˆ¬å– **æ˜¾è‘—æ›´å¿«**ã€‚

è¦è¿›è¡Œå…¨é¢æ¯”è¾ƒï¼Œè¯·æŸ¥çœ‹æˆ‘ä»¬åœ¨ä»“åº“ä¸­çš„ [Crawl4AI vs. Firecrawl](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/crawl4ai_vs_firecrawl.py) ç¤ºä¾‹ã€‚

## å‚è€ƒæ–‡çŒ®

<https://github.com/unclecode/crawl4ai>


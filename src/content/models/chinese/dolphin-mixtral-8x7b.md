---
title: "Dolphin 2.6 Mixtral 8x7B 🐬"
meta_title: "Dolphin 2.6 Mixtral 8x7B 🐬"
description: "Dolphin 2.6 Mixtral 8x7B 🐬"
date: 2023-12-21T00:00:00Z
image: "/images/logo.svg"
categories: ["text 2 text"]
author: "cognitivecomputations"
tags: ["Programming", "Natural Language Processing", "Generative AI", "Ethics", "Chatbots"]
draft: False

id: "dolphin-mixtral-8x7b"
context: 32768
input: 5e-07
output: 5e-07
img: 0
request: 0
last_updated: 2024-11-04T12:52:28Z
---

这是对 [Mixtral-8x7b](/mistralai/mixtral-8x7b) 的 16k 上下文微调。由于大量使用编码数据进行训练，它在编码任务中表现出色，并以其服从性而闻名，尽管缺乏 DPO 调优。

该模型未经过审查，并且去除了对齐和偏见。它需要一个外部对齐层以确保伦理使用。用户被提醒要负责任地使用这个高度合规的模型，具体细节可参见关于未审查模型的博客文章 [erichartford.com/uncensored-models](https://erichartford.com/uncensored-models)。

#moe #uncensored


---
title: "DeepSeek V3"
meta_title: "DeepSeek V3"
description: "DeepSeek V3"
date: 2024-12-27T09:59:13Z
image: "https://img.rifx.online/icons/deepseek-color.svg"
categories: ["text 2 text"]
author: "DeepSeek"
tags: ["Programming", "Technology", "Machine Learning", "Natural Language Processing", "Generative AI", "New", "Hot"]
model_tags: ['New', 'Hot']
draft: False
is_recommended: True
is_active: True
discount: 1
is_free: False

id: "deepseek/deepseek-chat"
context: 64000
input: 1.4e-07
output: 2.8e-07
img: 0
request: 0
last_updated: 2024-12-27T09:59:13Z
---

DeepSeek-V3 is the latest model from the DeepSeek team, building upon the instruction following and coding abilities of the previous versions. Pre-trained on nearly 15 trillion tokens, the reported evaluations reveal that the model outperforms other open-source models and rivals leading closed-source models. For model details, please visit the DeepSeek-V3 repo for more information.

DeepSeek-V2 Chat is a conversational finetune of DeepSeek-V2, a Mixture-of-Experts (MoE) language model. It comprises 236B total parameters, of which 21B are activated for each token.

Compared with DeepSeek 67B, DeepSeek-V2 achieves stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times.

DeepSeek-V2 achieves remarkable performance on both standard benchmarks and open-ended generation evaluations.


---
title: "Liquid: LFM 40B MoE"
meta_title: "Liquid: LFM 40B MoE"
description: "Liquid: LFM 40B MoE"
date: 2024-09-30T00:00:00Z
image: "/images/logo.svg"
categories: ["text 2 text"]
author: "liquid"
tags: ["Machine Learning", "Natural Language Processing", "Data Science", "Generative AI", "Computer Vision"]
draft: False

id: "lfm-40b"
context: 32768
input: 1e-06
output: 2e-06
img: 0
request: 0
last_updated: 2024-11-04T12:44:22Z
---

Liquid's 40.3B Mixture of Experts (MoE) model. Liquid Foundation Models (LFMs) are large neural networks built with computational units rooted in dynamic systems.

LFMs are general-purpose AI models that can be used to model any kind of sequential data, including video, audio, text, time series, and signals.

See the [launch announcement](https://www.liquid.ai/liquid-foundation-models) for benchmarks and more info.


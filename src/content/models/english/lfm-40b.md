---
author: liquid
categories:
- text 2 text
context: 32768
date: 2024-09-30 00:00:00+00:00
description: 'Liquid: LFM 40B MoE'
draft: false
id: lfm-40b
image: /images/logo.svg
img: 0
input: 1e-06
is_active: false
is_recommended: true
last_updated: 2024-11-14 05:10:16+00:00
meta_title: 'Liquid: LFM 40B MoE'
output: 2e-06
request: 0
tags:
- Machine Learning
- Natural Language Processing
- Data Science
- Generative AI
- Computer Vision
title: 'Liquid: LFM 40B MoE'
---






Liquid's 40.3B Mixture of Experts (MoE) model. Liquid Foundation Models (LFMs) are large neural networks built with computational units rooted in dynamic systems.

LFMs are general-purpose AI models that can be used to model any kind of sequential data, including video, audio, text, time series, and signals.

See the [launch announcement](https://www.liquid.ai/liquid-foundation-models) for benchmarks and more info.


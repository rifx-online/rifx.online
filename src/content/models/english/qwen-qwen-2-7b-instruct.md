---
title: "Qwen 2 7B Instruct"
meta_title: "Qwen 2 7B Instruct"
description: "Qwen 2 7B Instruct"
date: 2024-12-02T13:14:59Z
image: "https://img.rifx.online/icons/qwen-color.svg"
categories: ["text 2 text"]
author: "Qwen"
tags: ["Generative AI", "Qwen", "Machine Learning", "Natural Language Processing", "multilingual understanding", "Programming", "Data Science", "coding and reasoning", "SwiGLU activation", "qwen-2-7b", "transformer model"]
model_tags: []
labels: ["qwen-2-7b", "multilingual understanding", "transformer model", "coding and reasoning", "SwiGLU activation"]
draft: False
is_recommended: False
is_active: True
discount: 1
is_free: False

id: "qwen/qwen-2-7b-instruct"
context: 32768
input: 5.4e-08
output: 5.4e-08
img: 0
request: 0
last_updated: 2024-12-02T13:14:59Z

---

Qwen2 7B is a transformer-based model that excels in language understanding, multilingual capabilities, coding, mathematics, and reasoning.

It features SwiGLU activation, attention QKV bias, and group query attention. It is pretrained on extensive data with supervised finetuning and direct preference optimization.

For more details, see this [blog post](https://qwenlm.github.io/blog/qwen2/) and [GitHub repo](https://github.com/QwenLM/Qwen2).

Usage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).


---
author: mistralai
categories:
- text 2 text
context: 65536
date: 2024-04-17 00:00:00+00:00
description: 'Mistral: Mixtral 8x22B Instruct'
draft: false
id: mixtral-8x22b-instruct
image: https://img.rifx.online/logo/mistral.png
img: 0
input: 9e-07
is_active: false
is_recommended: false
last_updated: 2024-11-14 08:21:32+00:00
meta_title: 'Mistral: Mixtral 8x22B Instruct'
output: 9e-07
request: 0
tags:
- Programming
- Natural Language Processing
- Machine Learning
- Data Science
- Generative AI
title: 'Mistral: Mixtral 8x22B Instruct'
---






Mistral's official instruct fine-tuned version of [Mixtral 8x22B](/mistralai/mixtral-8x22b). It uses 39B active parameters out of 141B, offering unparalleled cost efficiency for its size. Its strengths include:
- strong math, coding, and reasoning
- large context length (64k)
- fluency in English, French, Italian, German, and Spanish

See benchmarks on the launch announcement [here](https://mistral.ai/news/mixtral-8x22b/).
#moe


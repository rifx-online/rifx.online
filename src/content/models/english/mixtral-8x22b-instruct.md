---
title: "Mistral: Mixtral 8x22B Instruct"
meta_title: "Mistral: Mixtral 8x22B Instruct"
description: "Mistral: Mixtral 8x22B Instruct"
date: 2024-04-17T00:00:00Z
image: "https://img.rifx.online/logo/mistral.png"
categories: ["text 2 text"]
author: "mistralai"
tags: ["Programming", "Natural Language Processing", "Machine Learning", "Data Science", "Generative AI"]
draft: False
is_recommended: False

id: "mixtral-8x22b-instruct"
context: 65536
input: 9e-07
output: 9e-07
img: 0
request: 0
last_updated: 2024-11-14T08:21:32Z
---

Mistral's official instruct fine-tuned version of [Mixtral 8x22B](/mistralai/mixtral-8x22b). It uses 39B active parameters out of 141B, offering unparalleled cost efficiency for its size. Its strengths include:
- strong math, coding, and reasoning
- large context length (64k)
- fluency in English, French, Italian, German, and Spanish

See benchmarks on the launch announcement [here](https://mistral.ai/news/mixtral-8x22b/).
#moe


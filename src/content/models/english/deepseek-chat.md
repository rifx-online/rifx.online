---
author: deepseek
categories:
- text 2 text
context: 128000
date: 2024-05-14 00:00:00+00:00
description: DeepSeek V2.5
draft: false
id: deepseek-chat
image: https://img.rifx.online/logo/deepseek.svg
img: 0
input: 1.4e-07
is_active: false
last_updated: 2024-11-01 04:19:11+00:00
meta_title: DeepSeek V2.5
output: 2.8e-07
request: 0
tags:
- Programming
- Natural Language Processing
- Machine Learning
- Data Science
- Chatbots
title: DeepSeek V2.5
---






DeepSeek-V2.5 is an upgraded version that combines DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct. The new model integrates the general and coding abilities of the two previous versions.

DeepSeek-V2 Chat is a conversational finetune of DeepSeek-V2, a Mixture-of-Experts (MoE) language model. It comprises 236B total parameters, of which 21B are activated for each token.

Compared with DeepSeek 67B, DeepSeek-V2 achieves stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times.

DeepSeek-V2 achieves remarkable performance on both standard benchmarks and open-ended generation evaluations.


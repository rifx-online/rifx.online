---
title: "AI21: Jamba 1.5 Large"
meta_title: "AI21: Jamba 1.5 Large"
description: "AI21: Jamba 1.5 Large"
date: 2024-12-02T04:08:59Z
image: "https://img.rifx.online/icons/ai21.svg"
categories: ["text 2 text"]
author: "Ai21"
tags: ["Generative AI", "SSM-Transformer", "jamba-1-5-large", "resource efficiency", "Ai21", "Machine Learning", "document summarization", "Data Science", "Technology", "context window", "Chatbots"]
model_tags: []
labels: ["jamba-1-5-large", "context window", "document summarization", "SSM-Transformer", "resource efficiency"]
draft: False
is_recommended: False
is_active: True
discount: 1
is_free: False

id: "ai21/jamba-1-5-large"
context: 256000
input: 2e-06
output: 8e-06
img: 0
request: 0
last_updated: 2024-12-02T04:08:59Z

---

Jamba 1.5 Large is part of AI21's new family of open models, offering superior speed, efficiency, and quality.

It features a 256K effective context window, the longest among open models, enabling improved performance on tasks like document summarization and analysis.

Built on a novel SSM-Transformer architecture, it outperforms larger models like Llama 3.1 70B on benchmarks while maintaining resource efficiency.

Read their [announcement](https://www.ai21.com/blog/announcing-jamba-model-family) to learn more.


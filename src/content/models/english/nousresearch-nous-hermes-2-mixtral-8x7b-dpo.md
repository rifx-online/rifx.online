---
title: "Nous: Hermes 2 Mixtral 8x7B DPO"
meta_title: "Nous: Hermes 2 Mixtral 8x7B DPO"
description: "Nous: Hermes 2 Mixtral 8x7B DPO"
date: 2024-01-16T00:00:00Z
image: "/images/what-is-openai.png"
categories: ["text->text"]
author: "nousresearch"
tags: ["Role", "nousresearch"]
draft: false

id: "nousresearch/nous-hermes-2-mixtral-8x7b-dpo"
context: 32768
input: 5.4e-07
output: 0.00000054
img: 0
request: 0
---

Nous Hermes 2 Mixtral 8x7B DPO is the new flagship Nous Research model trained over the [Mixtral 8x7B MoE LLM](/mistralai/mixtral-8x7b).

The model was trained on over 1,000,000 entries of primarily [GPT-4](/openai/gpt-4) generated data, as well as other high quality data from open datasets across the AI landscape, achieving state of the art performance on a variety of tasks.

#moe


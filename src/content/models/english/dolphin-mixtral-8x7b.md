---
author: cognitivecomputations
categories:
- text 2 text
context: 32768
date: 2023-12-21 00:00:00+00:00
description: Dolphin 2.6 Mixtral 8x7B üê¨
draft: false
id: dolphin-mixtral-8x7b
image: /images/logo.svg
img: 0
input: 5e-07
is_active: false
last_updated: 2024-11-04 12:52:28+00:00
meta_title: Dolphin 2.6 Mixtral 8x7B üê¨
output: 5e-07
request: 0
tags:
- Programming
- Natural Language Processing
- Generative AI
- Ethics
- Chatbots
title: Dolphin 2.6 Mixtral 8x7B üê¨
---















This is a 16k context fine-tune of [Mixtral-8x7b](/mistralai/mixtral-8x7b). It excels in coding tasks due to extensive training with coding data and is known for its obedience, although it lacks DPO tuning.

The model is uncensored and is stripped of alignment and bias. It requires an external alignment layer for ethical use. Users are cautioned to use this highly compliant model responsibly, as detailed in a blog post about uncensored models at [erichartford.com/uncensored-models](https://erichartford.com/uncensored-models).

#moe #uncensored


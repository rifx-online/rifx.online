---
title: "Ministral 8B"
meta_title: "Ministral 8B"
description: "Ministral 8B"
date: 2024-10-17T00:00:00Z
image: "https://img.rifx.online/logo/mistral.png"
categories: ["text 2 text"]
author: "mistralai"
tags: ["mistralai"]
draft: False

id: "ministral-8b"
context: 128000
input: 1e-07
output: 1e-07
img: 0
request: 0
---

Ministral 8B is an 8B parameter model featuring a unique interleaved sliding-window attention pattern for faster, memory-efficient inference. Designed for edge use cases, it supports up to 128k context length and excels in knowledge and reasoning tasks. It outperforms peers in the sub-10B category, making it perfect for low-latency, privacy-first applications.


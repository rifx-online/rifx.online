---
title: "Mistral: Mistral Nemo"
meta_title: "Mistral: Mistral Nemo"
description: "Mistral: Mistral Nemo"
date: 2024-12-02T13:15:03Z
image: "https://img.rifx.online/icons/mistral-color.svg"
categories: ["text 2 text"]
author: "MistralAI"
tags: ["Programming", "Technology", "Machine Learning", "Natural Language Processing", "Generative AI"]
model_tags: []
draft: False
is_recommended: False
is_active: True
discount: 1
is_free: False

id: "mistralai/mistral-nemo"
context: 128000
input: 1.3e-07
output: 1.3e-07
img: 0
request: 0
last_updated: 2024-12-02T13:15:03Z
---

A 12B parameter model with a 128k token context length built by Mistral in collaboration with NVIDIA.

The model is multilingual, supporting English, French, German, Spanish, Italian, Portuguese, Chinese, Japanese, Korean, Arabic, and Hindi.

It supports function calling and is released under the Apache 2.0 license.


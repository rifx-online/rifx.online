---
author: Liquid
categories:
- text 2 text
context: 8192
date: 2024-12-02 02:38:41+00:00
description: 'Liquid: LFM 40B MoE (free)'
draft: false
id: liquid/lfm-40b:free
image: https://img.rifx.online/icons/rifx.svg
img: 0
input: 0.0
is_active: false
is_recommended: false
last_updated: 2024-12-02 02:38:41+00:00
meta_title: 'Liquid: LFM 40B MoE (free)'
output: 0.0
request: 0
tags:
- Technology
- Machine Learning
- Natural Language Processing
- Data Science
- Generative AI
- Free
title: 'Liquid: LFM 40B MoE (free)'
---


Liquid's 40.3B Mixture of Experts (MoE) model. Liquid Foundation Models (LFMs) are large neural networks built with computational units rooted in dynamic systems.

LFMs are general-purpose AI models that can be used to model any kind of sequential data, including video, audio, text, time series, and signals.

See the [launch announcement](https://www.liquid.ai/liquid-foundation-models) for benchmarks and more info.

_These are free, rate-limited endpoints for [LFM 40B MoE](/liquid/lfm-40b). Outputs may be cached. Read about rate limits [here](/docs/limits)._


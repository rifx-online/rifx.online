---
title: "Qwen2-VL 7B Instruct"
meta_title: "Qwen2-VL 7B Instruct"
description: "Qwen2-VL 7B Instruct"
date: 2024-08-28T00:00:00Z
image: "/images/logo.svg"
categories: ["text imaget 2 text"]
author: "Qwen"
tags: ["Technology", "Machine Learning", "Computer Vision", "Robotics", "Natural Language Processing"]
draft: False
is_recommended: False
is_active: True

id: "qwen/qwen-2-vl-7b-instruct"
context: 32768
input: 1e-07
output: 1e-07
img: 0.0001445
request: 0
last_updated: 2024-12-02T04:10:44Z
---

Qwen2 VL 7B is a multimodal LLM from the Qwen Team with the following key enhancements:

- SoTA understanding of images of various resolution & ratio: Qwen2-VL achieves state-of-the-art performance on visual understanding benchmarks, including MathVista, DocVQA, RealWorldQA, MTVQA, etc.

- Understanding videos of 20min+: Qwen2-VL can understand videos over 20 minutes for high-quality video-based question answering, dialog, content creation, etc.

- Agent that can operate your mobiles, robots, etc.: with the abilities of complex reasoning and decision making, Qwen2-VL can be integrated with devices like mobile phones, robots, etc., for automatic operation based on visual environment and text instructions.

- Multilingual Support: to serve global users, besides English and Chinese, Qwen2-VL now supports the understanding of texts in different languages inside images, including most European languages, Japanese, Korean, Arabic, Vietnamese, etc.

For more details, see this [blog post](https://qwenlm.github.io/blog/qwen2-vl/) and [GitHub repo](https://github.com/QwenLM/Qwen2-VL).

Usage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).


[{"lang":"en","group":"blog","slug":"blog/a-new-risings-red-star-qwen2-5-is-here-0dffe0fb09ad","frontmatter":{"title":"A new risings Red star: Qwen2.5 is here","meta_title":"A new risings Red star: Qwen2.5 is here","description":"Let’s test together the new born Alibaba Cloud’s generative AI Qwen2.5 with python and llama-cpp","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*zU-XtqK2oMLkvscgxavjdw.png","categories":["Programming","Technology","Education"],"author":"Rifx.Online","tags":["Qwen2.5","multimodal","instruction-following","text-generation","multilingual"],"draft":false,"slug":"blog/a-new-risings-red-star-qwen2-5-is-here-0dffe0fb09ad"},"content":"\n\n\n\n\n### Let’s test together the new born Alibaba Cloud’s generative AI Qwen2.5 with python and llama-cpp\n\n\n\nIn silence, with not so many claims and anticipated announcements, Alibaba Cloud release on September the 19th their flagship model family Qwen2.5.\n\nAlibaba Cloud’s revolutionary journey with Qwen is showing once again strong Leadership through Innovation.\n\nHow? What’s so cool in them? And should we expect?\n\nIn this article we are going to explore the new models and check the performances. As a follow up, in the next article, we are going to use `llama-cpp-python` and the quantized version of qwen2.5–1.5b-instruct, putting the model under 13 NLP tasks test.\n\nIn fact I believe that we are the best Benchmark tool around and we are fully able to evaluate when a model is good for us!\n\nFor now, here what we are going to cover:\n\n\n```python\n- Qwen2.5 family innovation\n- Declared scope, use cases and models\n- Qwen2.5: a party of Foundation models\n- Expanding Reach through Open-Source Contributions\n- Bridging Industries through cutting-edge AI solutions\n- 13 Tasks to prove it worth \n- Future outlook: continued Open-Sourcing\n```\nLet’s dive in!\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*OeQ5qeOzCdl8LPJOZZgTIw.png)\n\n\n## Qwen2.5 family innovation\n\nQwen is the large language model and large multimodal model series of the Qwen Team, Alibaba Group. Just yesterday the large language models have been upgraded to Qwen2.5.\n\nBoth language models and multimodal models are pretrained on large-scale multilingual and multimodal data and post-trained on quality data for aligning to human preferences. Qwen is capable of natural language understanding, text generation, vision understanding, audio understanding, tool use, role play, playing as AI agent, etc.\n\nWith the recent release of Qwen2.5 and additional open-source model releases Alibaba Cloud continues its leadership position to meet rising AI demands from enterprise users. Since June last year, the Qwen family has attracted over 90,000 deployments via Model Studio in various industries including consumer electronics, automobiles, gaming, and more.\n\nQwen also expanded its reach with new models such as Qwen1.5–110B and CodeQwen1.5–7B on platforms like Hugging Face, showcasing Alibaba’s commitment to open-source AI development.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*A4pEOgsLK2PAFtiaGQx1Qw.png)\n\n\n## Declared scope, use cases and models\n\nIn the past three months since Qwen2’s release, numerous developers have built new models on the Qwen2 language models, providing valuable feedback to the entire community, but also to Alibaba Cloud.\n\n\n> During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5.\n\nTheir claims come with facts about the new family of models:\n\n* Dense, easy-to-use, decoder-only language models, available in 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B sizes, and base and instruct variants.\n* Pretrained on our latest large-scale dataset, encompassing up to 18T tokens.\n* Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON.\n* More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n* Context length support up to 128K tokens and can generate up to 8K tokens.\n* Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\n\n## Qwen2.5: a party of Foundation models\n\nAs announced on the [official blog press release](https://qwenlm.github.io/blog/qwen2.5/) on September 19, 2024:\n\n\n> Today, we are excited to introduce the latest addition to the Qwen family: **Qwen2.5**. We are announcing what might be the largest opensource release in history! Let’s get the party started!\n\n\n> Our latest release features the LLMs **Qwen2.5**, along with specialized models for coding, **Qwen2.5-Coder**, and mathematics, **Qwen2.5-Math**.\n\nTo showcase Qwen2.5’s capabilities, the Alibaba Cloud team benchmarked their largest open-source model, **Qwen2.5–72B** — a 72B-parameter dense decoder-only language model — against leading open-source models like Llama-3.1–70B and Mistral-Large-V2.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*-MMFgkkWHa307jNo.jpg)\n\nAll open-weight models are dense, decoder-only language models, available in various sizes, including:\n\n* Qwen2.5: 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B\n* Qwen2.5-Coder: 1.5B, 7B, and 32B on the way\n* Qwen2.5-Math: 1.5B, 7B, and 72B.\n\nAll these open-source models, except for the 3B and 72B variants, are licensed under Apache 2.0. You can find the license files in the respective Hugging Face repositories.\n\n\n> In addition to these models, we offer APIs for our flagship language models: **Qwen-Plus** and **Qwen-Turbo** through Model Studio, and we encourage you to explore them!\n\nBut this is not all!\n\n\n> …we have also open-sourced the **Qwen2-VL-72B**, which features performance enhancements compared to last month’s release.\n\nIn terms of **Qwen2.5**, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to **18 trillion** tokens. Compared to Qwen2, Qwen2.5 has acquired significantly more knowledge (MMLU: 85+) and has greatly improved capabilities in coding (HumanEval 85+) and mathematics (MATH 80+). Additionally, the new models achieve significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*7c7CIbl-WVjazUeE.jpeg)\n\nQwen2.5 models are generally more resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n\nLike Qwen2, the Qwen2.5 language models support up to **128K** tokens and can generate up to **8K** tokens. They also maintain multilingual support for over **29** languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\n\n### Qwen-Coder is the new kid of the family\n\nThe specialized expert language models, namely **Qwen2.5-Coder** for coding and **Qwen2.5-Math** for mathematics, have undergone substantial enhancements compared to their predecessors, CodeQwen1.5 and Qwen2-Math. Specifically, Qwen2.5-Coder has been trained on **5.5 trillion** tokens of code-related data, enabling even smaller coding-specific models to deliver competitive performance against larger language models on coding evaluation benchmarks. Meanwhile, Qwen2.5-Math supports both **Chinese** and **English** and incorporates various reasoning methods, including Chain-of-Thought (CoT), Program-of-Thought (PoT), and Tool-Integrated Reasoning (TIR).\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Nvk4wrcB0SB4Tt-xbCzO6g.png)\n\n\n## Expanding Reach through Open-Source Contributions\n\nAs part of its continuous commitment to the broader community, Alibaba Cloud has made additional steps in releasing various sizes and variants of Qwen models. This includes:\n\n1. **Qwen 0.5 billion parameters**, a foundational version suitable for more traditional applications.2. A compact but potent model tailored specifically for gaming development: **Qwen-VL (vision-language)** optimized with high capabilities.\n\nThese advancements demonstrate Alibaba’s commitment to open-source AI, sharing not only the base versions of Qwen but also significant improvements and new models that are targeting directly the enterprise needs while enhancing their ability to innovate rapidly.\n\nThis aligns closely with a strategic vision where continuous contributions benefit both community members and its own clients as they seek innovative applications across multiple sectors.\n\n\n### Bridging Industries through cutting-edge AI solutions\n\nTo showcase the breadth of Qwen’s capabilities in real-world scenarios, Alibaba Cloud has been at the forefront:\n\n1. **Xiaomi**: the Company is integrating Alibaba’s models into their AI assistant, Xiao Ai, and deploying it within Xiaomi smartphones and electric vehicles to create enhanced features like car infotainment image generation via voice commands.\n\n2. **Perfect World Games**: the integration of Qwen in game development has led to innovative applications including improving plot resolution through dialogue dynamics and real-time content management.\n\nThe collaborations between Alibaba Cloud models and various industries have not only enriched the user experience but also facilitated greater opportunities for growth within these sectors, pushing boundaries that would otherwise be unimaginable without AI advancements.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*ku8o3rq6PHDE8xcc.png)\n\n\n## 13 Tasks to prove it worth\n\nThe 1.5 Billion parameters model is probably the best variant considering complexity, prompt understanding and inference speed.\n\nI will show you my internal testing using only `llama-cpp-python` and a simple terminal interface.\n\nTo do so, I created a list of prompt, covering a series of normally used tasks where you can also assign a vote (from 0 to 5) after every generation. It’s a personal human benchmark.\n\n\n### Requirements\n\nCreate a `venv` (python 3.11+ is required): I tested it on my Mini-PC running Windows 11.\n\n\n```python\n## create the virtual environment\npython -m venv venv\n## activate the venv\nvenv\\Scripts\\activate\n## Install the dependencies \npip install llama-cpp-python==0.2.90 tiktoken\n```\nWe need to download the GGUF file from the [official Qwen2.5 Hugging Face repo](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF). I used the qwen2.5–1.5b-instruct-q5\\_k\\_m.gguf version.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Fa-qFsx9RTFGZmM-vxCEPQ.png)\n\nDownload the file in the main project directory. And we are all set.\n\nThe code used here for the analysis is in my GitHub repository:\n\nI will explain the entire code and the results in the next article. Stay updated!\n\n\n## Future outlook: continued Open-Sourcing\n\nIn future plans, Alibaba has also expressed their commitment to ongoing open-source contributions by releasing smaller variants of Qwen for developers across different sectors. In reality in the Hugging Face community many users have started to fine-tune Qwen for dedicated tasks: I wrote an example in my article on NuExtract: the smaller variant of this model family is based on Qwen2–0.5b!\n\nThese developments in AI technology and model advancements are crucial steps towards leveraging the full potential of large language models like **Qwen** within a variety of industries. With robust adoption rates continuing to grow rapidly through Model Studio, it is clear that Alibaba Cloud has been a pioneer industry leader not only by providing advanced tools but also promoting innovation across enterprises.\n\nOn my side, my outlook are to proceed with internal testing on the new models, specifically on the small ones, up to 3B.\n\nIn the next article I will share with you my method, how to run the models and the prompt templates used for each of the thirteen NLP tasks.\n\nHope you enjoyed the article. If this story provided value and you wish to show a little support, you could:\n\n1. Clap a lot of times for this story\n2. Highlight the parts more relevant to be remembered (it will be easier for you to find them later, and for me to write better articles)\n3. **Join my [totally free weekly Substack newsletter here](https://thepoorgpuguy.substack.com/about)**\n4. Sign up for a Medium membership ($5/month to read unlimited Medium stories)\n5. Follow me on Medium\n6. Read my latest articles <https://medium.com/@fabio.matricardi>\n\nHere are a few more articles to feed your curiosity:\n\nResources references in this article:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Du7V61mEX_yIrfmF.png)\n\nThis story is published on [Generative AI](https://generativeai.pub/). Connect with us on [LinkedIn](https://www.linkedin.com/company/generative-ai-publication) and follow [Zeniteq](https://www.zeniteq.com/) to stay in the loop with the latest AI stories.\n\nSubscribe to our [newsletter](https://www.generativeaipub.com/) and [YouTube](https://www.youtube.com/@generativeaipub) channel to stay updated with the latest news and updates on generative AI. Let’s shape the future of AI together!\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*pvLAT3it1FkdhVU0.png)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/a-practical-guide-for-using-autogen-in-software-applications-8799185d27ee","frontmatter":{"title":"A practical guide for using AutoGen in software applications","meta_title":"A practical guide for using AutoGen in software applications","description":"Update: While this article was written only 4 months ago, AutoGen has since changed quite a bit. I apologize for some things that may be…","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*yrraWH6aGNnbx8p-wfQ1OQ.jpeg","categories":["Programming","Chatbots","Autonomous Systems"],"author":"Rifx.Online","tags":["AutoGen","multi-agent","LLMs","customization","collaboration"],"draft":false,"slug":"blog/a-practical-guide-for-using-autogen-in-software-applications-8799185d27ee"},"content":"\n\n\n\n\n\n*Update: While this article was written only 4 months ago, AutoGen has since changed quite a bit. I apologize for some things that may be outdated in my code examples.*\n\nIf you want to learn about AutoGen, there is [documentation](https://microsoft.github.io/autogen/), [Colab notebooks](https://microsoft.github.io/autogen/docs/Examples), and [a blog](https://microsoft.github.io/autogen/blog). Huge kudos to the AutoGen team for making an AMAZING product, but honestly — after reading all their stuff, I still didn’t know how to use AutoGen outside of a terminal or Jupyter Notebook.\n\nThis article tries to help fill that gap by giving some helpful ways to make AutoGen work in a software application. Here are the topics I’ll go over:\n\n1. Agents aren’t limited to communicating just over the terminal\n2. Registering custom replies\n3. How to include real humans in the conversation in real ways\n4. You can (and should) customize who speaks next\n5. You don’t have to use OpenAI\n6. Functions can be used instead of executing code\n7. Use Agents for organization, not just for conversations\n\nLastly, I’ll go over why I think you should use AutoGen to begin with. Let’s go!\n\n\n## Agents aren’t limited to communicating just over the terminal\n\nYou’ll see everyone demo AutoGen using a terminal or Jupyter Notebook. That’s nice for a demo, but there are other ways these agents can talk to each other.\n\nThere are 2 basic AutoGen classes: [`UserProxyAg`ent](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/user_proxy_agent.py) and [`AssistantAg`ent](https://github.com/microsoft/autogen/blob/main/autogen/agentchat/assistant_agent.py) . They inherit the [`ConversableAg`ent](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py) class, providing just a few different default parameters to the base class.\n\nWhen you see this classic code example:\n\n\n```python\nassistant = autogen.AssistantAgent(\n    name=\"assistant\",\n    llm_config=llm_config\n)\nuser_proxy = autogen.UserProxyAgent(name=\"user_proxy\")\nawait user_proxy.a_initiate_chat(\n    assistant,\n    message=\"\"\"What date is today? Compare the year-to-date gain for META and TESLA.\"\"\",\n)\n```\nwhat happens is that the `UserProxyAgent` will call its own `send` method, which will call `AssistantAgent` ‘s [`rece`ive](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L514) method, passing along the original message. A reply will be generated (more on that below), and `AssistantAgent` will now call its [`s`end](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L351) method, which will then call `UserProxyAgent` ‘s `receive` method, and so forth, until `UserProxyAgent` determines the conversation is terminated (which can be customized via the `is_termination_msg` argument).\n\nMy first “aha” moment was when I realized these agents were classes, and I could create my own custom agent classes that inherit the AutoGen UserProxy/Assistant/Conversable Agent classes, and override any of the default methods. That makes AutoGen very extensible.\n\nI had a use-case where I needed a human who could type in a message (proxied by `UserProxyAgent`) using a chat UI on a website, and I wanted an `AssistantAgent` to respond back to that chat in the UI, and be able to receive more messages from the human user, as though the human was just another agent in this AutoGen conversation.\n\nI could override the `send` and `receive` methods (or `a_send` and `a_receive`), and push/pull over http, websockets, etc. I tried this, and it started to work, but doesn’t scale. Let’s learn a better way.\n\n\n## Registering custom replies\n\nAutoGen has a plugin system that lets you customize how an agent generates a reply. We’re used to seeing examples where AutoGen queries OpenAI for an answer, and uses that as its reply, but you can insert your own methods as well:\n\n\n```python\nclass WeatherAgent(AssistantAgent):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, llm_config=False, **kwargs)\n        self.register_reply(Agent, WeatherAgent.get_weather)\n\n    async def get_weather(\n        self,\n        messages: List[Dict] = [],\n        sender=None,\n        config=None,\n    ) -> Tuple[bool, Union[str, Dict, None]]:\n        last_message = messages[-1][\"content\"]\n        result = await fetch_weather(last_message)\n        return True, result\n\nasync def fetch_weather(city: str) -> str:\n    async with httpx.AsyncClient() as client:\n        result = await client.post(\n            WEATHER_API_URL,\n            json={\"city\": question},\n        )\n        return result.json()\n\nweather_assistant = WeatherAgent(name=\"weather_assistant\")\nuser_proxy = autogen.UserProxyAgent(name=\"user_proxy\")\nawait user_proxy.a_initiate_chat(assistant, message=\"Lehi\")\nprint(weather_assistant.last_message)\n```\nHere, `register_reply` will insert my custom method for getting a reply, and by default, will put this method in `position=0`, meaning it will be the first reply method attempted. That method should return a tuple, where the first item is a boolean indicating if this reply is the one that should be used or whether to try the next registered\\_reply (such as the built-in reply generations using OpenAI — see the full order [here](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L145-L153)).\n\nKnowing about [`register_re`ply](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L155) allows you to customize how replies are retrieved, allow you to start sub multi-agent conversations, etc.\n\n\n## How to include real humans in the conversation in real ways\n\nHere’s one way to do it:\n\n\n```python\n## user makes a POST /query { \"message\": \"What's the weather?\" }\n\n@query_blueprint.route(\"/query\", methods=[\"POST\"])\nasync def post_query():\n  message = request.form.get(\"message\")\n\n  assistant = autogen.AssistantAgent(\n    name=\"assistant\",\n    llm_config=llm_config\n    system_message=\"\"\"You're a helpful assistant.\n    If you need more info, ask the user for anything missing.\"\"\"\n  )\n  user_proxy = autogen.UserProxyAgent(\n    name=\"user_proxy\",\n    human_input_mode=\"NEVER\",\n    code_execution_config=False,\n    is_termination_msg=lambda message: True # Always True\n  )\n  weather_assistant = WeatherAgent(\n    name=\"weather_assistant\",\n    system_message=\"\"\"You're a helpful assistant to get the weather.\n    You fetch weather information, then return it.\"\"\"\n  )\n\n  groupchat = autogen.GroupChat(\n    agents=[assistant, user_proxy, weather_assistant],\n    messages=[]\n  )\n  manager = autogen.GroupChatManager(\n    name=\"Manager\",\n    groupchat=groupchat,\n    llm_config=llm_config,\n  )\n\n  await user_proxy.a_initiate_chat(manager, message=message)\n\n  return groupchat.messages[-1]\n```\nWhat’s going on here?\n\n1. Anytime a message is sent to `user_proxy`, the conversation will end (we’ll resume it later). Why do this? This means the `user_proxy` can actually proxy for the user. Rather than try to answer, it will end the current conversation flow and allow the real human user to respond (by resuming the conversation — see below).\n2. If the assistant needs more info, it’ll ask user\\_proxy, which will end the current conversation.\n\nIn the above code, what is likely to occur is something like this:\n\n1. user\\_proxy -> manager: “What’s the weather?”\n2. assistant -> manager: “The user didn’t specify for which city.”\n3. manager -> user\\_proxy : conversation will end\n\nNow, if the user wants to respond and resume the conversation, how would we do that? There’s lots of ways to do this, here’s just a sample flavor:\n\n\n```python\n## user makes a POST /query { \"message\": \"What's the weather?\" }\n## above posts returns a `history` array\n## user makes a second POST /query { \"message\": \"What's the weather?\", \"history\": history }\n\nclass ResumableGroupChatManager(GroupChatManager):\n    groupchat: GroupChat\n\n    def __init__(self, groupchat, history, **kwargs):\n        self.groupchat = groupchat\n        if history:\n            self.groupchat.messages = history\n\n        super().__init__(groupchat, **kwargs)\n\n        if history:\n            self.restore_from_history(history)\n\n    def restore_from_history(self, history) -> None:\n        for message in history:\n            # broadcast the message to all agents except the speaker.  This idea is the same way GroupChat is implemented in AutoGen for new messages, this method simply allows us to replay old messages first.\n            for agent in self.groupchat.agents:\n                if agent != self:\n                    self.send(message, agent, request_reply=False, silent=True)\n\n@query_blueprint.route(\"/query\", methods=[\"POST\"])\nasync def post_query():\n  message = request.form.get(\"message\")\n\n  assistant = autogen.AssistantAgent(\n    name=\"assistant\",\n    llm_config=llm_config\n    system_message=\"\"\"You're a helpful assistant.\n    If you need more info, ask the user for anything missing.\"\"\"\n  )\n  user_proxy = autogen.UserProxyAgent(\n    name=\"user_proxy\",\n    human_input_mode=\"NEVER\",\n    code_execution_config=False,\n    is_termination_msg=lambda message: True # Always True\n  )\n  weather_assistant = WeatherAgent(\n    name=\"weather_assistant\",\n    system_message=\"\"\"You're a helpful assistant to get the weather.\n    You fetch weather information, then return it.\"\"\"\n  )\n\n  groupchat = autogen.GroupChat(\n    agents=[assistant, user_proxy, weather_assistant],\n    messages=[]\n  )\n  manager = ResumableGroupChatManager(\n    name=\"Manager\",\n    groupchat=groupchat,\n    llm_config=llm_config,\n  )\n\n  await user_proxy.a_initiate_chat(manager, message=message)\n\n  return {\n    \"response\": groupchat.messages[-1],\n    \"history\": groupchat.messages,\n  }\n```\nUsing this approach, you can now include humans as though they were just another agent in the groupchat. Anytime an assistant agent wants human input, they ask user\\_proxy, user\\_proxy then ends the current conversation, allowing the human user to respond with more information, then pick up the conversation where it left off.\n\nThe benefits to this approach are:\n\n* Conversations can include real human input via any means you want (such as over http or websocket).\n* The conversation is stopped while getting human input. This frees up the thread for other conversations and computation.\n* You can persist these conversations across sessions.\n\n\n## You can (and should) customize who speaks next\n\nThis is subjective, but I think you should always customize the way speakers are selected because:\n\n1. You’ll use less tokens (saves both $ and response time)\n2. You can separate the logic that decides who speaks next from the logic that defines the system instructions for each agent\n\n\n```python\nshort_role_descriptions = {\n  \"user_proxy\": \"A proxy for the user\",\n  \"weather_assistant\": \"You can get the weather\",\n  \"planner\": \"You help coordinate the plan. Your turn happens when XYZ, but skip your turn when ABC\"\n}\n\nclass CustomGroupChat(GroupChat):\n    # The default message uses the full system message, which is a long string.  We are overriding this to use a shorter message.\n    def select_speaker_msg(self, agents: List[Agent]):\n        message = f\"\"\"You are in a role play game. The following roles are available:\n        ---\n        {new_line.join([f\"{agent.name}: {short_role_descriptions[agent.name]}\" for agent in agents])}\n        ---\n\n        The role who plays next depends on the conversation.  User_Proxy will star the conversation, and typically Planner would go next.\n\n        Here are some examples\n        ---\n        ... not shown here ...\n        ---\n\n        Read the following conversation.\n        Then select the next role from {', '.join([agent.name for agent in agents])} to play. Only return the role.\"\"\"\n        return message\n```\n\n## You don’t have to use OpenAI\n\nAutoGen already notes you can use other LLMs, as long as they are “ChatGPT-like”, meaning their API responds with a similar shape and response as ChatGPT API calls.\n\nBut, remember how these agents are classes, and you can override most of the methods?\n\nTry overriding the method: [generate\\_oai\\_reply](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L678), and you can query any LLM you’d like.\n\n\n## Functions can be used instead of executing code\n\nWhen I went to our security team and said “I’d like to use AutoGen for my service in Kubernetes. It needs to be able to execute any arbitrary code produced by an LLM. You’re ok with that, right?”\n\nOf course, the answer was a definite: NO.\n\nSo, why use AutoGen without the auto-code-execution abilities?\n\nOn top of the reasons stated below, another is that you can use function calling to gain total control over code execution. If you have a set of python functions you want to provide to AutoGen — functions you wrote, control, and can accept some safe parameters — that sounds like a better idea anyway than the wild west of allowing any and all code to be executed in your private infrastructure.\n\n\n## Use Agents for organization, not just for conversations\n\nMaybe you don’t have a need for an autonomous, multi-agent conversation. Maybe you just need to make a few different calls to an LLM.\n\nI still like the idea of having different “Agents” just for the sake of organization. Here’s a really crazy idea, but take it for what it’s worth:\n\n\n```python\nanalyst = autogen.AssistantAgent(\n    name=\"Analyst\",\n    system_message=\"\"\"Your an analyst.  You do XYZ.\"\"\",\n    llm_config=llm_config,\n)\n\nsummarizer = autogen.AssistantAgent(\n    name=\"Summarizer\",\n    system_message=\"\"\"Your a summarizer.  You do XYZ.\"\"\",\n    llm_config=llm_config,\n)\n\nreport = \"\"\"Some long report\"\"\"\n\nanalysis = analyst.generate_oai_reply(report)[1]\nsummary = summarizer.generate_oai_reply(report)[1]\n\nprint(f\"Analysis: {analysis}\")\nprint(f\"Summary: {summary}\")\n```\n\n## Why use AutoGen?\n\n1. AutoGen allows multiple agents, with different system prompts and instructions, to solve a problem. Just like in real-life, different perspectives working together will solve a problem better than a single brain.\n2. AutoGen GroupChat is amazing. It provides routing to the right experts (agents), and it allows a conversation to continue autonomously until the problem is solved. Some conversations will go from agent: a->b->c->d, others will be b->a->d->c. This allows AutoGen to solve a variety of different problems without needing explicit rules for each scenario.\n3. AutoGen can recover from mistakes. For example, I made an AutoGen-powered service that made API calls to a service. Sometimes, the API calls errored out because it didn’t send the right data at first. The AutoGen GroupChat kept trying different things until it succeeded. Sometimes, it took 4+ attempts, but my Planner agent didn’t give up — just pivoted autonomously to handle the API failures and try new things.\n4. AutoGen came up with the concept of separating `UserProxyAgent`s from `AssistantAgent` s from the beginning. This also allows us to let the user proxy actually proxy for the user, as shown above.\n5. AutoGen is a well maintained library. Every week they’re adding something new.\n6. AutoGen is very extensible. With the way they’ve built their classes, you can customize anything to your liking.\n7. AutoGen has other features I don’t use, but others may find them helpful, such as helping you count tokens and cost of conversations, cacheing, etc.\n\n"},{"lang":"en","group":"blog","slug":"blog/alibabas-open-source-qwen-how-it-s-revolutionizing-ai-and-how-you-can-use-it-dcba8f687c97","frontmatter":{"title":"Alibaba’s Open-Source Qwen: How It’s Revolutionizing AI and How You Can Use It","meta_title":"Alibaba’s Open-Source Qwen: How It’s Revolutionizing AI and How You Can Use It","description":"Alibaba has recently made waves in the AI world by open-sourcing its Qwen 2.5 models during the 2024 Apsara Conference. With over 100…","date":"2024-10-26T00:26:25.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*I7QDwbLMzoJ_ORq5.jpg","categories":["Programming","Machine Learning","Natural Language Processing"],"author":"Rifx.Online","tags":["Qwen","multimodal","open-source","fine-tune","text-to-video"],"draft":false,"slug":"blog/alibabas-open-source-qwen-how-it-s-revolutionizing-ai-and-how-you-can-use-it-dcba8f687c97"},"content":"\nAlibaba has recently made waves in the AI world by open-sourcing its **Qwen 2.5** models during the 2024 Apsara Conference. With over 100 models, Qwen spans multiple modalities including language, vision, audio, and code, making it one of the most comprehensive open-source AI solutions. The release empowers developers by providing tools for diverse applications, from text-to-video generation to real-time question answering.\n\n\n\n## Key Features of Alibaba’s Qwen Models\n\n1. **Multimodal Capabilities**: Qwen models handle diverse inputs, including text, audio, and visual data. This multimodal approach makes them suitable for a wide range of industries, from media and entertainment to robotics.\n2. **Open Source**: Available on platforms like **Hugging Face** and **ModelScope**, Qwen has already been downloaded over 40 million times, with over 50,000 custom models built on its foundation.\n3. **Enhanced Performance**: Qwen2.5 introduces improved language understanding, mathematics, and coding capabilities, rivaling leading models in the field. With optimized performance for tasks like structured data understanding and long text generation, Qwen opens the door to high-level AI applications.\n\n## How to Use Alibaba’s Qwen\n\nDevelopers and organizations can access Qwen models on platforms like Hugging Face, where they can:\n\n* **Fine-tune models**: Tailor Qwen for specific industry applications such as customer service, automation, or video content creation.\n* **Integrate with applications**: Qwen’s text-to-video model can be incorporated into media production pipelines, generating dynamic content from static images and text prompts.\n* **Develop AI assistants**: With enhanced vision-language models, Qwen can be used in robotics and autonomous vehicles to process video data and perform real-time tasks like navigation or object recognition.\n\n**Example of Using Qwen via Hugging Face**:\n\n```python\nfrom transformers import QwenTokenizer, QwenModel\n\ntokenizer = QwenTokenizer.from_pretrained(\"qwen-2.5\")\nmodel = QwenModel.from_pretrained(\"qwen-2.5\")\n\ninput_text = \"What is the future of AI in healthcare?\"\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\noutputs = model(input_ids)\n```\n\nThis allows users to access Qwen models, run inference, and customize them based on specific needs.\n\n## Qwen’s Impact Across Industries\n\n1. **Media and Entertainment**: With the new text-to-video capabilities, Qwen can automatically generate videos from written scripts, transforming the creative industry by automating tedious production tasks.\n2. **Robotics and Autonomous Vehicles**: The enhanced vision-language models in Qwen can help robots understand real-world environments, leading to better decision-making in autonomous driving or manufacturing.\n3. **Software Development**: Alibaba’s AI Developer tool, powered by Qwen, automates tasks like code generation, debugging, and requirement analysis, enabling developers to focus on higher-level problem-solving.\n\n## Conclusion: A New Era of Open AI Innovation\n\nBy open-sourcing its Qwen 2.5 models, Alibaba is democratizing access to advanced AI technologies. Developers, startups, and large enterprises alike can harness Qwen’s multimodal and real-time capabilities to drive innovation in industries ranging from media to autonomous vehicles. Whether you’re a developer looking to fine-tune models for a niche application or a corporation integrating AI into your infrastructure, Qwen offers powerful tools to accelerate progress.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/building-a-local-ai-powered-news-aggregator-with-ollama-swarm-and-duckduckgo-95aaf8b3ee41","frontmatter":{"title":"Building a Local AI-Powered News Aggregator with Ollama, Swarm, and DuckDuckGo","meta_title":"Building a Local AI-Powered News Aggregator with Ollama, Swarm, and DuckDuckGo","description":"No subtitle provided","date":"2024-10-24T17:47:43.000Z","image":"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*OHMOTk_WYGOxWHBsKqdpNQ.jpeg","categories":["Programming","Generative AI","Technology/Web"],"author":"Rifx.Online","tags":["Llama","Swarm","DuckDuckGo","News","Aggregator"],"draft":false,"slug":"blog/building-a-local-ai-powered-news-aggregator-with-ollama-swarm-and-duckduckgo-95aaf8b3ee41"},"content":"\n\n# Building a Local AI-Powered News Aggregator with Ollama, Swarm, and DuckDuckGo\n\n\n\nIn today’s fast-paced world, staying up-to-date with the latest news in specific fields can be challenging. What if we could leverage the power of Generative AI and Agents to create a personalized news aggregator that runs entirely on our local machine? In this article, we’ll explore how to build such a system using **Ollama**’s Llama 3.2 model, **Swarm** for agent orchestration, and **DuckDuckGo** for web searches.\n\n\n# The Power of Local AI\n\nWith the rise of large language models, we now have the ability to run sophisticated AI systems on our personal computers. This opens up a world of possibilities for creating customized tools tailored to our specific needs. Our news aggregator is a perfect example of this potential.\n\n\n# Components of Our System\n\n1. **Ollama with Llama 3.2**: This serves as the brain of our system, powering our AI agents.\n2. **Swarm**: An agent orchestration framework that allows us to create and manage multiple AI agents.\n3. **DuckDuckGo Search**: Provides up-to-date web search results without tracking user data.\n\n\n# How It Works\n\nOur news aggregator consists of two main AI agents:\n\n1. **News Assistant**: Fetches the latest news articles on a given topic using DuckDuckGo search.\n2. **Editor Assistant**: Reviews and refines the collected news for final presentation.\n\nLet’s break down the workflow:\n\n\n# 1. Setting Up the Environment\n\n\n```python\nollama pull llama3.2\n\nexport OPENAI_MODEL_NAME=llama3.2\nexport OPENAI_BASE_URL=http://localhost:11434/v1\nexport OPENAI_API_KEY=any\n\npip install git+https://github.com/openai/swarm.git duckduckgo-search\n```\nWe start by importing the necessary libraries and initializing our Swarm client:\n\n\n```python\nfrom duckduckgo_search import DDGS\nfrom swarm import Swarm, Agent\nfrom datetime import datetime\n\ncurrent_date = datetime.now().strftime(\"%Y-%m\")\nclient = Swarm()\n```\n\n# 2. Creating the News Search Function\n\nWe define a function to search for news using DuckDuckGo:\n\n\n```python\npythondef get_news_articles(topic):\n  ddg_api = DDGS()\n  results = ddg_api.text(f\"{topic} {current_date}\", max_results=5)\n  if results:\n      news_results = \"\\n\\n\".join([f\"Title: {result['title']}\\nURL: {result['href']}\\nDescription: {result['body']}\" for result in results])\n      return news_results\n  else:\n      return f\"Could not find news results for {topic}.\"\n```\n\n# 3. Defining Our AI Agents\n\nWe create two agents using Ollama’s Llama 3.2 model:\n\n\n```python\nnews_agent = Agent(\n  model=\"llama3.2\",\n  name=\"News Assistant\",\n  instructions=\"You provide the latest news articles for a given topic using DuckDuckGo search.\",\n  functions=[get_news_articles],\n)\n\neditor_agent = Agent(\n  model=\"llama3.2\",\n  name=\"Editor Assistant\",\n  instructions=\"You review and finalise the news article for publishing.\",\n)\n```\n\n# 4. Orchestrating the Workflow\n\nWe define a function to run our news aggregation workflow:\n\n\n```python\ndef run_news_workflow(topic):\n  # Fetch news\n  news_response = client.run(\n      agent=news_agent,\n      messages=[{\"role\": \"user\", \"content\": f\"Get me the news about {topic} on {current_date}\"}],\n  )\n  raw_news = news_response.messages[-1][\"content\"]\n  \n  # Pass news to editor for final review\n  edited_news_response = client.run(\n      agent=editor_agent,\n      messages=[{\"role\": \"system\", \"content\": raw_news}],\n  )\n  print(f\"{edited_news_response.messages[-1]['content']}\")\n```\n\n# 5. Running the System\n\nFinally, we can run our news aggregator for any topic of interest:\n\n\n```python\nrun_news_workflow(\"AI in Drug Discovery\")\n```\n\n# Complete Code : app.py\n\n\n```python\nfrom duckduckgo_search import DDGS\nfrom swarm import Swarm, Agent\nfrom datetime import datetime\n\ncurrent_date = datetime.now().strftime(\"%Y-%m\")\n\n# Initialize Swarm client\nclient = Swarm()\n\n# 1. Create Internet Search Tool\n\ndef get_news_articles(topic):\n    print(f\"Running DuckDuckGo news search for {topic}...\")\n    \n    # DuckDuckGo search\n    ddg_api = DDGS()\n    results = ddg_api.text(f\"{topic} {current_date}\", max_results=5)\n    if results:\n        news_results = \"\\n\\n\".join([f\"Title: {result['title']}\\nURL: {result['href']}\\nDescription: {result['body']}\" for result in results])\n        return news_results\n    else:\n        return f\"Could not find news results for {topic}.\"\n    \n# 2. Create AI Agents\n\ndef transfer_to_editor_assistant(raw_news):\n    print(\"Passing articles to Editor Assistant...\")\n    return editor_agent.run({\"role\": \"system\", \"content\": raw_news})\n\n# News Agent to fetch news\nnews_agent = Agent(\n    model=\"llama3.2\",\n    name=\"News Assistant\",\n    instructions=\"You provide the latest news articles for a given topic using DuckDuckGo search.\",\n    functions=[get_news_articles],\n)\n\n# Editor Agent to edit news\neditor_agent = Agent(\n    model=\"llama3.2\",\n    name=\"Editor Assistant\",\n    instructions=\"You review and finalise the news article for publishing.\",\n)\n\n# 3. Create workflow\n\ndef run_news_workflow(topic):\n    print(\"Running news Agent workflow...\")\n    \n    # Step 1: Fetch news\n    news_response = client.run(\n        agent=news_agent,\n        messages=[{\"role\": \"user\", \"content\": f\"Get me the news about {topic} on {current_date}\"}],\n    )\n    raw_news = news_response.messages[-1][\"content\"]\n    print(f\"Fetched news: {raw_news}\")\n    \n    # Step 2: Pass news to editor for final review\n    edited_news_response = client.run(\n        agent=editor_agent,\n        messages=[{\"role\": \"system\", \"content\": raw_news}],\n    )\n    print(f\"{edited_news_response.messages[-1]['content']}\")\n\n\n# Example of running the news workflow for a given topic\nrun_news_workflow(\"AI in Drug Discovery\")\n```\n\n# Sample Output\n\n\n```python\nRunning news Agent workflow...\nRunning DuckDuckGo news search for AI in Drug Discovery...\nFetched news: Here's a formatted answer based on the news articles:\n\n**AI in Drug Discovery: A Revolutionary Shift**\n\nThe role of Artificial Intelligence (AI) in drug discovery has marked a revolutionary shift in the pharmaceutical landscape. AI leverages sophisticated algorithms for autonomous decision-making from data analysis, augmenting human capabilities rather than replacing them.\n\n**Challenges and Limitations**\n\nDespite the promising advancements, challenges and limitations have been identified in the field. The paper \"The Role of AI in Drug Discovery\" addresses these issues, highlighting the need for high-quality data, addressing ethical concerns, and recognizing the limitations of AI-based approaches.\n\n**Applications of AI in Drug Discovery**\n\nAI has the potential to play a critical role in drug discovery, design, and studying drug-drug interactions.Applications of AI in drug discovery include:\n\n* Polypharmacology: AI can predict the likelihood of a compound's effectiveness against multiple diseases.\n* Chemical synthesis: AI can optimize chemical synthesis processes for faster and more efficient production.\n* Drug repurposing: AI can identify new uses for existing drugs.\n* Predicting drug properties: AI can predict the efficacy, toxicity, and physicochemical characteristics of compounds.\n\n**The Future of AI in Drug Discovery**\n\nAs AI continues to evolve, it is expected to significantly impact the pharmaceutical industry. The successful application of AI will depend on the availability of high-quality data, addressing ethical concerns, and recognizing the limitations of AI-based approaches.\n```\n\n# The Benefits of Local AI News Aggregation\n\n* **Privacy**: All processing happens on your local machine, ensuring your data stays with you.\n* **Customization**: You can easily modify the agents’ instructions or add new agents to suit your specific needs.\n* **Up-to-date Information**: By using DuckDuckGo search, you always get the latest news on your chosen topic.\n* **AI-powered Curation**: The Editor Assistant helps refine and organize the collected news, providing a more polished final output.\n\n\n# Conclusion\n\nThis local AI-powered news aggregator demonstrates the potential of combining large language models with web search capabilities. By leveraging Ollama’s Llama 3.2 model, Swarm for agent orchestration, and DuckDuckGo for search, we’ve created a powerful tool that can keep us informed on any topic of interest, all while maintaining our privacy and running entirely on our local machine.\n\nAs AI continues to evolve, the possibilities for creating personalized, AI-driven tools will only expand. This news aggregator is just the beginning — imagine what other innovative applications you could build using these technologies!\n\n\n# Reference :\n\nSwarm Github : <https://github.com/openai/swarm>\n\nIf you found this article informative and valuable, I’d greatly appreciate your support:\n\n* Give it a few claps 👏 on Medium to help others discover this content (did you know you can clap up to 50 times?). Your claps will help spread the knowledge to more readers.\n- Share it with your network of AI enthusiasts and professionals.\n- Connect with me on LinkedIn: <https://www.linkedin.com/in/manjunath-janardhan-54a5537/>\n\n\n\n\n\n"},{"lang":"en","group":"blog","slug":"blog/claude-3-5-sonnet-new-pioneering-the-future-of-ai-with-computer-control-capabilities-37a6ff9f9033","frontmatter":{"title":"Claude 3.5 Sonnet (New): Pioneering the Future of AI with Computer Control Capabilities","meta_title":"Claude 3.5 Sonnet (New): Pioneering the Future of AI with Computer Control Capabilities","description":"Anthropic has unveiled its latest AI model, Claude 3.5 Sonnet, on October 22, 2024. This release introduces revolutionary computer control…","date":"2024-10-27T13:57:00.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*n0NkOFbhUm7_fllJ","categories":["Programming","Technology","Generative AI"],"author":"Rifx.Online","tags":["Claude","Sonnet","automation","benchmarks","safety"],"draft":false,"slug":"blog/claude-3-5-sonnet-new-pioneering-the-future-of-ai-with-computer-control-capabilities-37a6ff9f9033"},"content":"\n\n\n\n\n\nAnthropic has unveiled its latest AI model, Claude 3.5 Sonnet, on October 22, 2024. This release introduces revolutionary computer control capabilities and substantial improvements across various benchmarks, setting new standards in the AI industry.\n\n\n## Revolutionary Computer Control: A New Frontier\n\nThe standout feature of Claude 3.5 Sonnet is its ability to interact with computers just like humans do. This groundbreaking capability allows the AI to:\n\n* Navigate desktop interfaces using mouse and keyboard inputs\n* Interact with various applications and web browsers\n* Execute complex multi-step tasks\n* Perform file management operations\n* Automate repetitive workflows\n\nThis computer control feature, currently in public beta, represents a paradigm shift in how AI systems can interact with digital interfaces. While still in its experimental phase, early testing shows promising results, with Claude 3.5 Sonnet scoring 14.9% on the OSWorld benchmark for screenshot-only tasks — significantly higher than the next-best system’s 7.8%.\n\n\n## Benchmark-Breaking Performance\n\nThe upgraded model demonstrates remarkable improvements across various metrics:\n\n\n## Coding and Technical Tasks\n\n* 49% performance on SWE-bench Verified (up from 33.4%)\n* 93.7% score on HumanEval coding tasks\n* Superior performance in software engineering compared to specialized coding systems\n\n\n## Academic and Reasoning Capabilities\n\n* 65% on graduate-level reasoning (GPQA-Diamond)\n* 78% on undergraduate-level knowledge (MMLU Pro)\n* 78.3% on mathematical problem-solving (MATH)\n\n\n## Business Applications\n\n* 69.2% on retail domain tasks (TAU-bench)\n* 46% on airline domain tasks\n* 90.8% accuracy on chart analysis\n* 94.2% accuracy on document Q&A\n\n\n## Enterprise Integration and Availability\n\nClaude 3.5 Sonnet is accessible through multiple platforms:\n\n* Anthropic API\n* Amazon Bedrock\n* Google Cloud’s Vertex AI\n\nMajor companies including Asana, Canva, DoorDash, and Replit have already begun implementing Claude 3.5 Sonnet’s capabilities in their workflows, particularly leveraging its computer control features for complex automation tasks.\n\n\n## Practical Applications\n\n\n## Software Development\n\n* Automated code testing and debugging\n* Intelligent IDE interactions\n* Code review and optimization\n* Documentation generation\n\n\n## Customer Support\n\n* Advanced chatbot capabilities\n* Visual data interpretation\n* Automated ticket resolution\n* Process automation\n\n\n## Business Operations\n\n* Document processing and analysis\n* Data extraction from visual sources\n* Workflow automation\n* Complex problem-solving\n\n\n## Safety and Responsibility\n\nAnthropic has implemented robust safety measures for the computer control feature:\n\n* New classifiers to identify potential misuse\n* Proactive monitoring systems\n* Restricted access to sensitive operations\n* Regular safety assessments\n\n\n## Looking Ahead\n\nWhile Claude 3.5 Sonnet represents a significant advancement in AI capabilities, it’s important to note that some features, particularly computer control, are still in their early stages. Certain actions like scrolling, dragging, and zooming present challenges, and Anthropic encourages developers to begin with low-risk tasks while exploring these new capabilities.\n\nThe release of Claude 3.5 Sonnet marks a pivotal moment in AI development, combining advanced reasoning capabilities with practical computer control features. As the technology continues to evolve, we can expect to see even more innovative applications and improvements in how AI systems interact with our digital world.\n\n*This article is based on official announcements and documentation from Anthropic, AWS, and various technology partners. For the most up-to-date information, please refer to Anthropic’s official documentation.*\n\n\n"},{"lang":"en","group":"blog","slug":"blog/claude-3-5-sonnet-v-s-gpt-4o-which-one-is-better-3b3675195bf9","frontmatter":{"title":"Claude 3.5 Sonnet V/S GPT-4O: Which one is better","meta_title":"Claude 3.5 Sonnet V/S GPT-4O: Which one is better","description":"In November 2022, OpenAI launched ChatGPT, a model that has revolutionized how we search and interact with information. Next year, in…","date":"2024-10-27T13:59:09.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*4MXLuSFfGwFkWWn0","categories":["Generative AI","Machine Learning","Natural Language Processing"],"author":"Rifx.Online","tags":["GPT-4o","Claude","multimodal","reasoning","code-generation"],"draft":false,"slug":"blog/claude-3-5-sonnet-v-s-gpt-4o-which-one-is-better-3b3675195bf9"},"content":"\n\n\n\nIn November 2022, OpenAI launched ChatGPT, a model that has revolutionized how we search and interact with information. Next year, in March, an American startup,” Anthropic,” founded by ex-OpenAI employees, launched their own AI model, “Claude.” Since the launch, both AI companies have been competing to bring the best to customers regarding features and experience through their AI models. Recently, OpenAI launched “GPT-4o,” a spectacular model that handles file, voice, and video data amazingly. Similarly, Claude launched the “Claude 3.5 Sonnet,” which is the most advanced AI model, as they claimed, and can handle complex tasks. In this article, we will determine which is better, between Claude 3.5 Sonnet and GPT-4o, and compare its features and output with the same input to check which is better for you.\n\n\n## Capabilities and Features\n\n\n### GPT-4o\n\n\n\nGPT-4o is the latest LLM launched by OpenAI. The “o” stands for omni, which means “every” in Latin. This model can analyze voice, images, videos, and files as input and respond accordingly. It can take voice input and give the output in different characters’ voices, including tones, emotions, etc. The whole process is as low as a human conversation, with an average of 0.32 seconds compared to other voice models, which is 2.8 seconds. It also allows users to generate written content such as articles, blogs, product descriptions, code in different programming languages, data analysis, charts, etc. In addition, GPT-4o can also analyze images and videos, which makes the model act as a language translator, personal assistant, virtual teacher, or shopping assistant. It can also be used in medicine, engineering, the military, etc. To use this feature, GPT-4o can use the user’s camera to get a real-time view and respond accordingly in the voice mode. It can also access your computer screen and describe what is shown on the screen, users can ask questions related to the stuff displayed on the screen.\n\n*For example, users can enable the model on the screen, open the VS code, and prompt the model to act as a coding assistant to get answers to the coding problems. Alternatively, you can enable the camera to act as a fitness trainer whether you are doing it correctly or not.*\n\nThe model has unique features, such as data analysis, code interpreter, and real-time web browsing, making it different from its competitors. The model also has a plethora of GPTs, which is a tailored version of ChatGPT.\n\n\n### Claude 3.5 Sonnet\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*BSMcOpvWZ5lUm4Tl)\n\nClaude 3.5 Sonnet is the AI chatbot launched by Anthropic. It is the third generation of the family of Claude AI model series. This model has stood at a high bae and outperformed many AI models on various evaluations, keeping the hallucinations and wrong information away. While it doesn’t support voice and video features like GPT-4o, it can also perform all the basic tasks, such as text generation and code generation in different programming languages, brainstorming ideas, etc. According to the report by Anthropic, Claude 3.5 Sonnet is one of the best computer vision models in the market, which can be used to analyze charts and graphs, transcribe texts from images, and many more. Claude is powered by an advanced feature, “Artifacts,” a special popup window along the conversation, allowing the users to check the code snippets, text documents, or website designs and allow them to edit the output in real-time.\n\n*For example, users can use computer vision and artifacts in their workflows. Users can make essential prototyping of a website’s design on paper, attach the file with Claude 3.5 Sonnet, and prompt it to design a website based on the prototype. The generated code and the website design appear in the artifacts. Users can edit the code and the design according to their requirements. Users can also publish their projects live on the Internet.*\n\n\n## Head-to-Head Comparison\n\nIn this section, we will compare the two LLMs based on factors such as complex reasoning and code generation, check out their capabilities in handling complex tasks, and see which model is best.\n\n* **Graduate Level Reasoning(GPQA, Diamond)**This factor evaluates the models’ ability to handle complex, high-level reasoning tasks at a graduate level of education. In this task, researchers compare the model on the GPQA test, a set of 448 questions in different fields designed by experts. These questions are Google Proof, so anyone can’t find them online. The Claude score is nearly 59.4%, while the GPT-4o scores only 53.6%. Both the scores are relatively close, but as we can see, Claude could be a better option in tasks that require advanced analytical thinking, such as research analysis, complex problem solving, and high academic level problems.\n* **Undergraduate level knowlege(MMLU)**The MMLU, which means Massive Multitask Language Understanding, is a benchmark that explains the general knowledge understanding of any AI model across various subjects at an undergraduate level. Claude 3.5 Sonnet scores 88.3% in this experiment, and the GPT-4o scores 88.7%. This shows how both LLMs have trained in various domains and have a deeper understanding of them. It makes the AI model a well-suited tool for general knowledge tasks, basic tutoring of multiple subjects, etc.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*A4w-tvsxcmFINaQT)\n\n* **Code(HumanEval)**HumanEval is a benchmark that evaluates the model’s ability to generate, understand, and debug code. This benchmark is where Claude 3.5 Sonnet achieves 92%, and GPT-4o scores 90.2%. Claude 3.5 Sonnet results are spectacular in this task as it provides a better coding environment, “Artifacts,” and better code generation than GPT-4o. Claude allows the users to design, edit, and run the code in the Artifacts pop-up window. After the launch of Claude 3.5 Sonnet, everyone is developing tools, websites, and basic games and sharing them across the internet. On the other hand, GPT-4o also scored well, but it does not have any coding environment in its interface, so the developers must do too much hassle as the code generated by it is too much hassle to get to the result.\n* **Reasoning Over Text(DROP, FLscore)**The DROP(Discrete Reasoning Over Paragraphs) is the benchmark that measures the model’s ability to understand complex text information. In this challenge, the Claude 3.5 Sonnet scores 87.1%, while the GPT-4o scores 83.4%. This shows that the Claude 3.5 Sonnet is better and more effective for the task, which involves detailed text analysis, text review, complex question-answering systems, etc.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Kcy7sFb2FYpbfrwp)\n\n* **Math problem solving(MATH)**This test evaluates the ability of any AI model to solve various mathematical problems. Claude 3.5 Sonnet scores just 71.1%, while the GPT-4o scores 76.6%. These scores make the GPT-4o a better model for mathematical problem-solving tasks and can be used for mathematical computations such as financial modeling, scientific calculations, and advanced data analysis.\n* **Multilingual Maths (MSGM)**This factor describes the ability of any AI model to solve mathematical problems in multiple languages. Both models get scores close to each other: GPT-4o 90.5% and Claude 3.5 Sonnet 91.6%. This shows that both models perform excellently, with Claude slightly better. The capability is particularly helpful for educational applications or any scenario where mathematical reasoning needs to be communicated across language barriers.\n* **Visual question answering(MMU/val)**This factor describes the LLM’s capability to analyze the information presented in images. The GPT-4o outperforms Claude’s 3.5 Sonnet in this benchmark with 69.1% and 68.3%, respectively. On the other hand, when analyzing text from the document, Claude’s 3.5 Sonnet score is 95.2% compared to GPT-4o’s 92.1%.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*xzjqBV2YL0lVFitX)\n\n* **Image Generation**Image Generation is the ability of the LLMs to generate images from the text. GPT-4o is integrated with DallE-2 and can produce images with the help of text, and the results are excellent. On the other hand, Claude 3.5 Sonnet cannot create any images. This feature also helps GPT-4o design websites and references better, as it is trained on many images.\n* **Knowledge Cutoff**Here, both the models trained on a limited data set till a specific date. Claude 3.5 Sonnet trained on data till April 2024, while the other hand, GPT-4o trained on data till 2024. The real advantage of GPT-4o is that it has real-time web browsing, which helps the LLM train on new data regularly.\n\n\n## Pros of GPT-4o:\n\n* Handles voice, images, and video input.\n* Real-time web browsing capability.\n* Faster response time (0.32 seconds average).\n* Superior in math problem-solving.\n* Can generate images using DALL-E 2.\n\n\n## Cons of GPT-4o:\n\n* Slightly lower performance in graduate-level reasoning.\n* No built-in coding environment.\n* A lower score in document visual Q&A.\n* Slightly behind in code generation capabilities.\n* Less effective in detailed text analysis.\n\n\n## Pros Claude 3.5 Sonnet:\n\n* Excels in graduate-level reasoning.\n* Superior code generation and built-in “Artifacts” feature.\n* Better performance in detailed text analysis.\n* A higher score in document visual Q&A.\n* Slightly better in multilingual math.\n\n\n## Cons Claude 3.5 Sonnet:\n\n* Cannot handle voice or video input.\n* No image generation capability.\n* Slightly lower performance in visual question-answering.\n* Cannot access real-time web information.\n* Weaker in math problem-solving.\n\n\n## Conclusion\n\nGPT-4o and Claude 3.5 Sonnet demonstrate impressive capabilities across various tasks, each with its strengths. GPT-4o excels in multimodal inputs, real-time information access, and image generation, making it versatile for diverse applications. Claude 3.5 Sonnet shines in complex reasoning, code generation, and detailed text analysis, offering superior performance in specific academic and professional contexts. The choice between these models depends on the specific use case and required features. We can expect further improvements and specialized models catering to different needs as AI technology advances.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/explore-swarm-multi-agent-framework-locally-0e25ee617795","frontmatter":{"title":"Explore Swarm Multi-Agent Framework Locally","meta_title":"Explore Swarm Multi-Agent Framework Locally","description":"Swarm is an experimental sample framework to simulate lightweight multi-agent framework for educational purpose. Usually it works with Open…","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*0ZVceq32bvkytC7HSIgmwA.png","categories":["Programming","Technology","Education"],"author":"Rifx.Online","tags":["Swarm","Multi-Agent","Framework","OpenAI","Ollama"],"draft":false,"slug":"blog/explore-swarm-multi-agent-framework-locally-0e25ee617795"},"content":"\n\n\n\n\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*zkpW8DDwh0TTYuHJVJbDaw.png)\n\nSwarm is an experimental sample framework to simulate lightweight multi-agent framework for educational purpose. Usually it works with Open AI Key but we can change it to use local Ollama or LM Studio Models.\n\n**Setup:**\n\n\n```python\n## Create a new Conda or Python Virtual Environment and activate it\nconda install python==3.10\npip install torch openai\npip install transformers accelerate huggingface_hub\npip install git+ssh://git@github.com/openai/swarm.git\n```\n**To use with Open AI Key:**\n\n\n```python\nexport OPEN_API_KEY = Your Key\n```\n**To use Ollama or LM Studio Local LLMs — Update to Local URL:**\n\n\n```python\n## Find the location site-packages/swarm on the conda or python virtual env\n## Locate the file core.py\nclass Swarm:\n    def __init__(self, client=None):\n        if not client:\n          # Actual Code\n          #client = OpenAI()\n          # Update the Base URL and API Key to Ollama / LM Studio\n          # In this demo we are using LM Studio and Llama 3.1\n          client = OpenAI(base_url=\"http://localhost:1234/v1\",api_key=\"random\")\n        self.client = client\n```\n**Clone Repo:**\n\nClone the Repo — where you can find examples directory with different use cases like basic, airline and weather etc.\n\n\n```python\ngit clone https://github.com/openai/swarm.git\ncd swarm/examples\n```\n**Sample Code:**\n\n\n```python\nfrom swarm import Swarm, Agent\n\nclient = Swarm()\n\n\nit_agent = Agent(\n    name=\"IT Agent\",\n    instructions=\"You are an IT Expert with 10 Years of Experience.\",\n)\n\nsales_agent = Agent(\n    name=\"Sales Agent\",\n    instructions=\"You are a Sales Expert with 5 Years of Experience and knows about best selling mobiles.\",\n)\n\ndef transfer_to_sales_agent():\n    print(\"Sales agent in action\")\n    \"\"\"Transfer sales related questions to sales team immediately.\"\"\"\n    return sales_agent\n\ndef transfer_to_it_agent():\n    print(\"IT agent in action\")\n    \"\"\"Transfer IT users immediately.\"\"\"\n    return it_agent\n\nenglish_agent = Agent(\n    name=\"English Agent\",\n    instructions=\"You only speak English.\",\n    functions=[transfer_to_sales_agent,transfer_to_it_agent],\n)\n\n\nmessages = [{\"role\": \"user\", \"content\": \"How to install pandas lib?\"}]\nresponse = client.run(agent=english_agent, messages=messages)\n\nprint(response.messages[-1][\"content\"])\n\nmessages = [{\"role\": \"user\", \"content\": \"What are the best selling items?\"}]\nresponse = client.run(agent=english_agent, messages=messages)\n\nprint(response.messages[-1][\"content\"])\n```\n**References:**\n\n\n```python\nhttps://github.com/openai/swarm\n\nhttps://github.com/victorb/ollama-swarm/tree/main\n```\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*hCFJ4VQoT12yElYPXwXvWA.png)\n\nGiven that it is an experimental release, there is still much room for improvement. The airline agent example code [swarm/examples/airline] was interesting, so try those examples. Give it a try and share your experience in the comments. Thanks.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/generating-structured-data-from-an-image-with-gpt-vision-and-langchain-34aaf3dcb215","frontmatter":{"title":"Generating structured data from an image with GPT vision and Langchain","meta_title":"Generating structured data from an image with GPT vision and Langchain","description":"In today’s world, where visual data is abundant, the ability to extract meaningful information from images is becoming increasingly…","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*FPRRg85jYb7MrzXEpNWbmw.jpeg","categories":["Programming","Computer Vision","Natural Language Processing"],"author":"Rifx.Online","tags":["Langchain","GPT","vision","LLMs","structured"],"draft":false,"slug":"blog/generating-structured-data-from-an-image-with-gpt-vision-and-langchain-34aaf3dcb215"},"content":"\n\n\n\n\n\nIn today’s world, where visual data is abundant, the ability to extract meaningful information from images is becoming increasingly valuable. Langchain, a powerful framework for building applications with large language models (LLMs), offers a versatile toolset for tackling this challenge. In this article, we’ll explore how to use Langchain to extract structured information from images, such as counting the number of people and listing the main objects.\n\nBefore diving into the code, let’s set the stage by understanding the task at hand. Imagine you have an image of a scene, such as a city street. Your goal is to extract valuable information from this image, including the number of people present and a list of the main objects in the scene.\n\n\n## About Langchain\n\nLangchain is a comprehensive framework that allows developers to build sophisticated applications by leveraging the power of large language models (LLMs). It provides a modular and extensible architecture, enabling developers to create custom pipelines, agents, and workflows tailored to their specific needs.\n\nLangchain simplifies the integration of LLMs, offering abstractions and utilities for handling various data sources, including text, images, and structured data. It supports a wide range of LLMs from different providers, such as OpenAI and Anthropic, making it easy to switch between models or combine multiple models in a single application.\n\n\n## Preparing the Environment and Setting Up the OpenAI API Key\n\nTo follow along with this tutorial, you’ll need to have Langchain installed. You can install it using pip:\n\n\n```python\npip install langchain langchain_openai\n```\nTo use the OpenAI language models with Langchain, you’ll need to obtain an API key from OpenAI. If you don’t have an API key yet, you can sign up for one on the OpenAI website (<https://openai.com/api/>).\n\nOnce you have your API key, you can set it as an environment variable in your system or provide it directly in your code. Here’s an example of how to set the API key as an environment variableCopy code\n\n\n```python\nexport OPENAI_API_KEY=\"your_openai_api_key_here\"\n```\nAlternatively, you can provide the API key directly in your Python code:\n\n\n```python\nimport os\nimport langchain\nos.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key_here\"\n```\nAfter setting up the API key, Langchain will be able to authenticate with the OpenAI API and use their language models.\n\n\n## Loading and Encoding the Image\n\nBefore we can process images with Langchain, we need to load the image data from a file and encode it in a format that can be passed to the language model. The code below defines a function `load_image` that takes a dictionary with an `image_path` key and returns a new dictionary with an `image` key containing the image data encoded as a base64 string.\n\n\n```python\ndef load_image(inputs: dict) -> dict:\n    \"\"\"Load image from file and encode it as base64.\"\"\"\n    image_path = inputs[\"image_path\"]\n  \n    def encode_image(image_path):\n        with open(image_path, \"rb\") as image_file:\n            return base64.b64encode(image_file.read()).decode('utf-8')\n    image_base64 = encode_image(image_path)\n    return {\"image\": image_base64}\n```\nThe `load_image` function first extracts the `image_path` from the input dictionary. It then defines a nested function `encode_image` that opens the image file in binary mode, reads its contents, and encodes them as a base64 string using the `base64.b64encode` function from the Python standard library.\n\nThe `load_image` function calls `encode_image` with the provided `image_path` and stores the resulting base64-encoded string in the `image_base64` variable. Finally, it returns a new dictionary with the `image` key set to `image_base64`.\n\nTo integrate this function into a Langchain pipeline, we can create a `TransformChain` that takes the `image_path` as input and produces the `image` (base64-encoded string) as outputCopy code\n\n\n```python\nload_image_chain = TransformChain(\n    input_variables=[\"image_path\"],\n    output_variables=[\"image\"],\n    transform=load_image\n)\n```\nWith this setup, we can easily load and encode images as part of a larger Langchain workflow, enabling us to process visual data alongside text using large language models.\n\n\n## Defining the Output Structure\n\nBefore we can extract information from the image, we need to define the structure of the output we want to receive. In this case, we’ll create a Pydantic model called `ImageInformation` that includes fields for the image description and any additional information we might want to extract.\n\n\n```python\nfrom langchain_core.pydantic_v1 import BaseModel, Field\n\nclass ImageInformation(BaseModel):\n \"\"\"Information about an image.\"\"\"\n image_description: str = Field(description=\"a short description of the image\")\n people_count: int = Field(description=\"number of humans on the picture\")\n main_objects: list[str] = Field(description=\"list of the main objects on the picture\")\n```\n\n## Setting up the Image Model\n\nNext, we’ll create a chain that combines the image loading and encoding steps with the LLM invocation step. Since the `ChatOpenAI` model is not natively capable of handling both text and image inputs simultaneously (to my unsderstanding), we'll create a wrapper chain to achieve this functionality.\n\n\n```python\nfrom langchain.chains import TransformChain\nfrom langchain_core.messages import HumanMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain import globals\nfrom langchain_core.runnables import chain\n\n## Set verbose\nglobals.set_debug(True)\n\n@chain\ndef image_model(inputs: dict) -> str | list[str] | dict:\n \"\"\"Invoke model with image and prompt.\"\"\"\n model = ChatOpenAI(temperature=0.5, model=\"gpt-4-vision-preview\", max_tokens=1024)\n msg = model.invoke(\n             [HumanMessage(\n             content=[\n             {\"type\": \"text\", \"text\": inputs[\"prompt\"]},\n             {\"type\": \"text\", \"text\": parser.get_format_instructions()},\n             {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{inputs['image']}\"}},\n             ])]\n             )\n return msg.content\n```\nIn this code snippet, we define a chain called `image_model` that invokes the `ChatOpenAI` model with the provided prompt, format instructions, and image. The `image_model` chain accepts a dictionary `inputs` containing the prompt and the base64-encoded image string.\n\nInside the chain, we create a `HumanMessage` object that combines the prompt text, format instructions, and the image URL, formatted as a data URI with the base64-encoded image data. We then invoke the `ChatOpenAI` model with this `HumanMessage` object, using the `gpt-4-vision-preview` model, which is specifically designed for multimodal tasks involving both text and images.\n\nThe model processes both the text prompt and the image, and returns the output.\n\n\n## Putting It All Together\n\nNow that we have all the necessary components, we can define a function that orchestrates the entire process:\n\n\n```python\nfrom langchain_core.output_parsers import JsonOutputParser\n\nparser = JsonOutputParser(pydantic_object=ImageInformation)\ndef get_image_informations(image_path: str) -> dict:\n   vision_prompt = \"\"\"\n   Given the image, provide the following information:\n   - A count of how many people are in the image\n   - A list of the main objects present in the image\n   - A description of the image\n   \"\"\"\n   vision_chain = load_image_chain | image_model | parser\n   return vision_chain.invoke({'image_path': f'{image_path}', \n                               'prompt': vision_prompt})\n```\nIn this function, we define a prompt that asks the LLM to provide a count of the people in the image and a list of the main objects. We then create a chain that combines the image loading step (`load\\_image\\_chain`), the LLM invocation step (`image\\_model`), and a JSON output parser (`parser`). Finally, we invoke this chain with the image path and the prompt, and the function returns a dictionary containing the extracted information.\n\n\n## Example Usage\n\nTo use this function, simply provide the path to an image file:\n\n\n```python\nresult = get_image_informations(\"path/to/your/image.jpg\")\nprint(result)\n```\nThis will output a dictionary with the requested information, such as:\n\n\n```python\n{\n 'description': 'a view of a city showing cars waiting at a traffic light',\n 'people_count': 5,\n 'main_objects': ['car', 'building', 'traffic light', 'tree']\n}\n```\n\n## Conclusion\n\nLangchain provides a powerful toolset for working with large language models and extracting valuable information from various data sources, including images. By combining Langchain’s capabilities with custom prompts and output parsing, you can create robust applications that can extract structured information from visual data.\n\nRemember, the quality of the output will depend on the capabilities of the LLM you’re using and the specificity of your prompts. Experiment with different models and prompts to find the best solution for your use case.\n\nIf you find a better way to achieve the same results or have suggestions for improvements, please don’t hesitate to share them in the comments. The code examples provided in this article are meant to serve as a starting point, and there may be alternative approaches or optimizations .\n\n\n"},{"lang":"en","group":"blog","slug":"blog/how-to-run-nvidia-llama-3-1-nemotron-70b-instruct-locally-a58ad283aaff","frontmatter":{"title":"How to Run Nvidia’ llama-3.1-nemotron-70b-instruct Locally","meta_title":"How to Run Nvidia’ llama-3.1-nemotron-70b-instruct Locally","description":"Running large language models (LLMs) locally has become increasingly popular among developers, researchers, and AI enthusiasts. One such…","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*fqVKJkw5sQvLtIsyCcengQ.png","categories":["Programming","Technology","Science"],"author":"Rifx.Online","tags":["Nvidia","llama","Ollama","llama.cpp","Transformers"],"draft":false,"slug":"blog/how-to-run-nvidia-llama-3-1-nemotron-70b-instruct-locally-a58ad283aaff"},"content":"\n\n\n\nRunning large language models (LLMs) locally has become increasingly popular among developers, researchers, and AI enthusiasts. One such model that has gained significant attention is the llama-3.1-nemotron-70b-instruct, a powerful LLM customized by NVIDIA to enhance the helpfulness of generated responses. In this comprehensive guide, we’ll explore multiple methods to run this model on your local machine, starting with the user-friendly Ollama platform.\n\n\n> Before we get started, If you are seeking an All-in-One AI platform that manages all your AI subscriptions in one place, including all LLMs (such as GPT-o1, Llama 3.1, Claude 3.5 Sonnet, Google Gemini, Uncensored LLMs) and Image Generation Models (FLUX, Stable Diffusion, etc.), Use Anakin AI to manage them all!\n\n\n\n\n## Method 1: Run llama-3.1-nemotron-70b-instruct Locally with Ollama\n\nOllama is an excellent tool for running LLMs locally, offering a straightforward setup process and support for various models, including llama-3.1-nemotron-70b-instruct.\n\n\n### Installation\n\n1. Visit the official Ollama website ([https://ollama.ai](https://ollama.ai/)) and download the appropriate version for your operating system.\n2. Install Ollama by running the following command in your terminal:\n\n\n```python\ncurl https://ollama.ai/install.sh | sh\n```\n\n### Running llama-3.1-nemotron\n\nOnce Ollama is installed, you can easily run the llama-3.1-nemotron-70b-instruct model with a simple command:\n\n\n```python\nollama run nemotron:70b-instruct-q5_K_M\n```\nThis command will download the model if it’s not already present on your system and start an interactive session.\n\n\n### Using the Model\n\nAfter the model is loaded, you can start interacting with it by typing your prompts. For example:\n\n\n```python\n>>> What are the key features of llama-3.1-nemotron-70b-instruct?\n\nLlama-3.1-Nemotron-70B-Instruct is a large language model with several key features:\n1. Customized by NVIDIA: The model has been fine-tuned by NVIDIA to improve the helpfulness and quality of its responses.\n2. Based on Llama 3.1: It builds upon the Llama 3.1 architecture, which is known for its strong performance across various tasks.\n3. 70 billion parameters: This large parameter count allows for complex reasoning and a wide range of capabilities.\n4. Instruct-tuned: The model is specifically designed to follow instructions and generate helpful responses to user queries.\n5. RLHF training: It has been trained using Reinforcement Learning from Human Feedback, specifically the REINFORCE algorithm.\n6. Specialized reward model: The training process utilized Llama-3.1-Nemotron-70B-Reward for optimization.\n7. HelpSteer2-Preference prompts: These were used during the training process to further improve the model's helpfulness.\n8. Extended context length: Like other Llama 3.1 models, it likely supports a longer context window of 128K tokens.\n9. Multilingual capabilities: It can understand and generate text in multiple languages.\n10. Strong reasoning abilities: The model excels in tasks requiring complex reasoning and problem-solving.\nThese features make llama-3.1-nemotron-70b-instruct a powerful and versatile language model suitable for a wide range of applications, from general conversation to specialized tasks in various domains.\n```\nFor more advanced use cases, you can integrate Ollama with Python using libraries like Langchain. Here’s a simple example:\n\n\n```python\npython\n\nfrom langchain.llms import Ollama\n\nollama = Ollama(base_url=\"http://localhost:11434\", model=\"nemotron:70b-instruct-q5_K_M\")\nresponse = ollama.generate(\"Explain the concept of quantum entanglement.\")\nprint(response)\n```\nThis allows you to incorporate the model into your Python projects and applications seamlessly.\n\n\n## Method 2: Using llama.cpp\n\nllama.cpp is a popular C++ implementation of the Llama model inference, optimized for CPU usage. While it may require more setup than Ollama, it offers greater flexibility and control over the model’s parameters.\n\n\n### Installation\n\n1. Clone the llama.cpp repository:\n\n\n```python\ngit clone https://github.com/ggerganov/llama.cpp.git\ncd llama.cpp\n```\n1. Build the project:\n\n\n```python\nmake\n```\n\n### Downloading the Model\n\nTo run llama-3.1-nemotron-70b-instruct, you’ll need to download the model weights. These are typically available in GGML or GGUF format. You can find pre-converted models on platforms like Hugging Face.\n\n\n```python\nmkdir models\ncd models\nwget https://huggingface.co/TheBloke/Llama-3.1-Nemotron-70B-Instruct-GGUF/resolve/main/llama-3.1-nemotron-70b-instruct.Q4_K_M.gguf\n```\n\n### Running the Model\n\nOnce you have the model file, you can run it using the following command:\n\n\n```python\n./main -m models/llama-3.1-nemotron-70b-instruct.Q4_K_M.gguf -n 1024 -p \"Hello, how are you today?\"\n```\nThis command loads the model and generates a response to the given prompt. You can adjust various parameters like the number of tokens to generate (-n) or the temperature to control randomness.\n\n\n## Method 3: Using Hugging Face Transformers\n\nHugging Face’s Transformers library provides a high-level API for working with various language models, including llama-3.1-nemotron-70b-instruct.\n\n**Installation**\n\nFirst, install the necessary libraries:\n\n\n```python\npip install transformers torch accelerate\n```\n**Running the Model**\n\nHere’s a Python script to load and use the model:\n\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nmodel_name = \"meta-llama/Llama-3.1-Nemotron-70b-instruct\"\n## Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n## Prepare the input\nprompt = \"Explain the concept of quantum computing in simple terms.\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n## Generate the response\nwith torch.no_grad():\n    outputs = model.generate(**inputs, max_new_tokens=100)\n## Decode and print the response\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response)\n```\nThis method allows for more fine-grained control over the model’s behavior and integration with other Hugging Face tools and pipelines.\n\n\n## Conclusion\n\nRunning llama-3.1-nemotron-70b-instruct locally opens up a world of possibilities for developers and researchers. Whether you choose the simplicity of Ollama, the flexibility of llama.cpp, or the integration capabilities of Hugging Face Transformers, you now have the tools to harness the power of this advanced language model on your own hardware.As you explore the capabilities of llama-3.1-nemotron-70b-instruct, remember to balance performance with resource constraints, and always consider the ethical implications of your applications. With responsible use, this model can be a valuable asset in pushing the boundaries of what’s possible in natural language processing and AI-driven applications.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/key-points-llm-quantization-chatgpt-artificial-intelligence-8201ffcb33d4","frontmatter":{"title":"5 Key Points to Unlock LLM Quantization","meta_title":"5 Key Points to Unlock LLM Quantization","description":"Quantizing Large Language Models","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*RUqPEr2NTYXlI1omqF22Qg.png","categories":["Machine Learning","Data Science","Technology/Web"],"author":"Rifx.Online","tags":["quantization","weights","activations","calibration","Quanto"],"draft":false,"slug":"blog/key-points-llm-quantization-chatgpt-artificial-intelligence-8201ffcb33d4"},"content":"\n\n\n\n\n### Quantizing Large Language Models\n\n\n\nLLM Quantization is currently a hot topic due to its vital role in making Large Language Models (LLMs) more efficient and deployable across various hardware platforms, including consumer-grade devices.\n\nBy adjusting the precision of certain components within the model, **quantization significantly reduces the model’s memory footprint** while maintaining similar performance levels.\n\nIn this guide, we will explore five key aspects of LLM quantization including some practical steps for applying this technique to our models.\n\n\n## #1. Understanding Quantization\n\nQuantization is a model compression technique that reduces the precision of weights and activations in an LLM. This involves converting high-precision values to lower-precision ones, effectively **changing data types that store more information to those that store less**.\n\nDecreasing the number of bits needed for each weight or activation significantly reduces the overall model size. As a result, **quantization creates LLMs that use less memory, and require less storage space.**\n\nThis technique has become essential in response to the exponential growth in the number of parameters in successive iterations of LLMs. For example, for the OpenAI’s GPT family, we can observe the growing trend in the following graph:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*QlAhma3Wu1F6w2WvkE8jDA.png)\n\nThis significant increase presents a challenge: as models grow, their memory requirements often exceed the capacity of advanced hardware accelerators such as GPUs. **This requires distributed training and inference to manage these models, which in turn limits their deployability.**\n\n\n## #2. Intuition Behind Quantization\n\nAlthough the definition of quantization may seem rather complex, the concept can be intuitively explained using matrices.\n\nLet’s consider the following a 3x3 matrix representing the weights of a neural network. The matrix on the left shows the original weights, while the matrix on the right shows the quantized version of these weights:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*LPzWe9oxjlDYdSp7dVvRUg.png)\n\nIn this simple example, we round the elements of the original matrix from four decimal places to a single decimal place. Although the matrices appear similar, **the storage space required for the four-decimal version is significantly higher**.\n\nIn practice, quantization is not merely a rounding operation. Instead, it involves converting numerical values to a different data type, typically from a higher to a lower precision one.\n\nFor example, the default data type for most models is `float32`, which requires 4 bytes per parameter (32 bits). Therefore, for a 3x3 matrix, the total memory footprint is 36 bytes. Changing the data type to `int8`, only 1 byte per parameter is needed, reducing the total memory footprint of the matrix to just 9 bytes.\n\n\n## #3. Quantization Error\n\nAs we have seen, the original matrix and its quantized form are not completely equal, but very similar. The value-by-value difference is known as “Quantization error”, which we can also represent in matrix form:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*VtGDjVbr7daagLXB57i7Mg.png)\n\n**This quantization error can accumulate for each matrix of weights in the network, affecting the model’s performance as a result.**\n\nCurrent research in quantization aims to minimize the difference in precision while decreasing the computational resources required to train or run inference on models, while maintaining acceptable performance levels.\n\n\n## #4. Linear Quantization\n\nLinear quantization is one of the most popular quantization schemes for LLMs. In simple terms, it involves mapping the range of floating-point values of the original weights to a range of fixed-point values.\n\nLet’s review the steps required to apply linear quantization to our models:\n\n* **Get the minimum and maximum ranges:** We need to get the minimum and maximum values of the floating-point weights to be quantized (`x_min` and `x_max`). We also need to define the quantized range (`q_min` and `q_max`), which is already set by the data type we want to convert to.\n* **Compute the scale (`s`) and the zero-point (`z`) values:** Firstly, the scale (`s`) adjusts the range of floating-point values to fit within the integer range, preserving the data distribution and range. Secondly, the zero-point (`z`) ensures that zero in the floating-point range is accurately represented by an integer, maintaining numerical accuracy and stability, especially for values close to zero.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BepC6-izw0yE19ejsS705Q.png)\n\n* **Quantize the values (`q`)**: We need to map the original floating-point values to the integer range using a scale factor (`s`) and a zero point (`z`) computed in the previous step.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BBOQ0VbSGbwf7CN8c4PWKQ.png)\n\nApplying these formulas is quite straightforward. If we apply them to the 3x3 weight tensor on the left in the image below, we will get the quantized matrix shown on the right:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*KzBvg84mfI2gAhTIyVibwQ.png)\n\nWe can see that the lower bound of the `int8` value corresponds to the lower value of the original tensor, while the upper bound corresponds to the higher value of the original tensor, *i.e., the mapping is`0.50 → 255` and `-0.40 → 0`.*\n\nWe can now dequantize the values using the formula below.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*E5nnqYzncYCRuM5prssuOw.png)\n\nIf we place the dequantized values again in matrix form (matrix on the left), we can compute the quantization error (matrix on the right) by calculating the point-by-point difference between the original matrix and its dequantized version:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*56NALu9PAN95QG2hn8HXoQ.png)\n\nAs we can observe, the quantization error starts kicking in for some of the matrix values.\n\n\n## #5. Weight Quantization vs Activation Quantization\n\nIn our example above, we have focused primarily on quantizing the weights of the model. While weight quantization is crucial for model optimization, it’s also important to consider that activations can be quantized as well.\n\n**Activation quantization involves reducing the precision of the intermediate outputs of each layer in the network**. Unlike weights, which remain constant once the model is trained, activations are dynamic and change with each input, making their range harder to predict.\n\nGenerally, activation quantization is more challenging to implement than weight quantization because it requires careful calibration to ensure the dynamic range of activations is accurately captured.\n\nWeight quantization and activation quantization are complementary techniques. Using both can significantly reduce model size without greatly compromising performance.\n\n\n## Final Thoughts\n\nIn this article, we have reviewed 5 key points about quantization to better understand how to reduce the size of these constantly growing models.\n\nAs for the implementation of those techniques, there are several tools and libraries in Python that support quantization such as `pytorch` and `tensorflow`. Nevertheless, integrating quantization seamlessly in existing models requires a deep understanding of the libraries and model internals.\n\nThat is why my favorite option to implement quantization in easy steps so far is the [Quanto](https://huggingface.co/blog/quanto-introduction) library by Hugging Face, designed to simplify the quantization process for PyTorch models.\n\nIf you are interested in the in-depths of LLM Quantization and how to use the aforementioned library, you might also be interested in the article [“Quantization for Large Language Models (LLMs): Reduce AI Model Sizes Efficiently”](https://www.datacamp.com/tutorial/quantization-for-large-language-models).\n\nThat is all! Many thanks for reading!\n\nI hope this article helps you when **using LLMs for coding!**\n\nYou can also subscribe to my [**Newsletter**](https://readmedium.com/@andvalenzuela/subscribe) to stay tuned for new content.\n\n**Especially**, **if you are interested in articles about Large Language Models and ChatGPT**:\n\n\n"},{"lang":"en","group":"blog","slug":"blog/langgraph-vs-langchain-vs-langflow-vs-langsmith-which-one-to-use-why-69ee91e91000","frontmatter":{"title":"LangGraph vs. LangChain vs. LangFlow vs. LangSmith: Which One to Use & Why?","meta_title":"LangGraph vs. LangChain vs. LangFlow vs. LangSmith: Which One to Use & Why?","description":"Discover the key differences between LangGraph, LangChain, LangFlow, and LangSmith, and learn which framework is best suited for your…","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*xrWv1QVt4zE5cxjA8VA3ag.png","categories":["Programming","Technology","Technology/Web"],"author":"Rifx.Online","tags":["LangGraph","LangChain","LangFlow","LangSmith","frameworks"],"draft":false,"slug":"blog/langgraph-vs-langchain-vs-langflow-vs-langsmith-which-one-to-use-why-69ee91e91000"},"content":"\n\n\n\n\n### Discover the key differences between LangGraph, LangChain, LangFlow, and LangSmith, and learn which framework is best suited for your language model applications — from workflow building to performance monitoring.\n\n👨🏾‍💻 [GitHub](https://github.com/mdmonsurali) ⭐️ | 👔[LinkedIn](https://www.linkedin.com/in/mdmonsurali/) |📝 [Medium](https://medium.com/@monsuralirana)\n\n\n\nIn recent years, the world of natural language processing (NLP) has witnessed an explosion in the number of frameworks, libraries, and tools available for building language model-based applications. Among these, **LangGraph**, **LangChain**, **LangFlow**, and **LangSmith** have emerged as leading options, each catering to different use cases and user needs. If you’re looking to build, monitor, or scale language model workflows, it’s crucial to understand the strengths and purposes of these tools.\n\nIn this blog, we’ll explore each framework, break down their strengths, and provide insights into when to use them. Whether you’re a seasoned developer or a newcomer to the field, understanding the nuances of these tools will help you choose the right one for your project.\n\n\n## Introduction to Language Model Frameworks\n\nWith the rise of powerful language models such as GPT-3, GPT-4, and other transformer-based models, there is a growing need for frameworks that streamline the creation and management of language-based applications. These frameworks simplify complex tasks like **chaining multiple prompts**, **retrieving relevant documents**, and even **monitoring model performance**.\n\nHowever, not all frameworks are the same. While some provide a **visual interface** to manage workflows, others offer advanced **debugging and observability** features. Let’s dive into each of these tools to understand their unique offerings.\n\n\n## 1. LangGraph: Visualizing Complex Workflows\n\n**LangGraph** is a newer framework designed for developers who prefer a **visual approach** to building language model pipelines. It allows you to structure complex workflows with **graph-based visualizations**, making it easier to understand dependencies between different tasks and components. This can be especially useful for larger applications where multiple steps, such as text generation, document retrieval, and classification, are chained together.\n\n\n### Strengths:\n\n* **Visual Workflow Representation**: LangGraph lets you visualize the flow of data and actions between different components. This graphical approach is intuitive and helps in designing more complex pipelines.\n* **Ease of Debugging**: The visual nature of LangGraph makes it easier to identify bottlenecks or problematic nodes in a workflow.\n\n\n### Example Use Case:\n\nSuppose you’re building an automated system that first retrieves relevant documents using a language model and then passes them through a summarizer. In LangGraph, you can visually map out this workflow, showing the relationships between each step. If there’s an issue at any point in the chain, the visual tool makes it easy to pinpoint where things went wrong.\n\n\n### When to Use LangGraph:\n\nIf you’re managing **complex workflows** with multiple steps and value a **graphical interface** for understanding your pipeline, LangGraph is a fantastic choice. It’s particularly helpful for developers or data scientists who prefer a more intuitive, drag-and-drop approach to workflow design.\n\n**Key points**:\n\n* If you need a clear visual representation of language processing workflows.\n* When creating more complex pipelines that require branching or multi-path dependencies.\n\n\n## 2. LangChain: The Workhorse for LLM Applications\n\n**LangChain** is one of the most popular frameworks for building applications powered by **large language models (LLMs)**. It provides a versatile, **code-first approach**, allowing developers to chain tasks such as document retrieval, summarization, and question-answering into cohesive workflows.\n\n\n### Strengths:\n\n* **Extensive Support for LLMs**: LangChain is compatible with various language models, making it easy to integrate models like OpenAI’s GPT or even locally hosted models.\n* **Chaining Capabilities**: LangChain excels at **chaining multiple operations** — hence the name — enabling developers to create sophisticated NLP applications.\n* **Wide Adoption**: As one of the most popular frameworks, LangChain has a **thriving community** and excellent support, with ample documentation and tutorials.\n\n\n### Example Use Case:\n\nImagine you’re building a **chatbot** that first understands the user’s question, retrieves relevant information from a database, and then generates a response. With LangChain, you can easily create this multi-step process programmatically, ensuring each step in the chain works harmoniously.\n\n\n### When to Use LangChain:\n\nIf you’re a **developer building production-level applications** and need a **flexible, code-centric solution**, LangChain is your best bet. It’s ideal for those who prefer control over their application’s architecture and are comfortable writing code to define workflows.\n\n**Key points**:\n\n* If you’re building production-grade applications that require chaining of tasks across multiple language models.\n* If you need a library with extensive community support and wide-ranging integrations.\n* When you’re more comfortable with programmatic solutions rather than visual tools.\n\n\n## 3. LangFlow: No-Code/Low-Code Extension of LangChain\n\n**LangFlow** is essentially a **visual extension of LangChain**. It combines the powerful backend of LangChain with an **intuitive drag-and-drop interface**. LangFlow allows users who might not be as comfortable writing code to still leverage the power of language models in their applications.\n\n\n### Strengths:\n\n* **Visual Workflow Creation**: Like LangGraph, LangFlow provides a visual interface for building workflows. However, it’s specifically built on top of LangChain, meaning users can harness LangChain’s power without needing to write extensive code.\n* **Ideal for Rapid Prototyping**: LangFlow is perfect for quickly **prototyping ideas** or building out proof-of-concept applications.\n* **Beginner-Friendly**: It’s a great entry point for users who are less familiar with coding but want to create language model workflows.\n\n\n### Example Use Case:\n\nIf you want to quickly build a **summarization tool** that retrieves documents, you can drag and drop the components in LangFlow’s interface to create a fully functioning application. This can be done without writing much code, if any.\n\n\n### When to Use LangFlow:\n\nLangFlow is perfect for **non-developers** or **rapid prototyping**. If you want to experiment with **LLM workflows quickly** without delving into the code, this tool makes it easy to get started.\n\n**Key points**:\n\n* If you want to prototype LLM workflows quickly without writing code.\n* If you’re comfortable with visual programming but need the flexibility of LangChain.\n* For educational purposes, to help users learn how workflows can be constructed.\n\n\n## 4. LangSmith: Monitoring and Observability\n\nWhile the other tools focus on **building workflows**, **LangSmith** is designed for **monitoring** and **debugging** language model applications. It provides advanced observability features to track the performance of your workflows and models, making it invaluable for production environments.\n\n\n### Strengths:\n\n* **Deep Observability**: LangSmith allows developers to monitor language model performance, ensuring that workflows behave as expected.\n* **Error Tracking**: It excels at helping developers track down issues, making debugging easier.\n* **Performance Insights**: LangSmith gives insights into **workflow performance**, helping developers optimize their applications.\n\n\n### Example Use Case:\n\nLet’s say you’ve deployed a **customer service chatbot** that uses a language model to answer questions. Over time, you notice that some responses are less accurate than expected. LangSmith can help you trace the problem by providing visibility into each decision point within the workflow.\n\n\n### When to Use LangSmith:\n\nIf you’re deploying applications in **production environments** and need to ensure **robustness, reliability, and performance**, LangSmith is an essential tool. It’s particularly useful when managing **complex systems that require debugging and optimization** over time.\n\n**Key points**:\n\n* If you need advanced monitoring or debugging capabilities in LLM workflows.\n* For development environments where observability is key to ensuring optimal model performance.\n* If your focus is on improving and iterating LLM-powered applications based on real-time insights.\n\n\n## Which One to Choose?\n\n* **Use LangGraph** if you prefer graph-based, visual workflows for building complex LLM tasks. Ideal for users who need clarity and structure.\n* **Use LangChain** if you need a robust, flexible solution for creating language model applications programmatically. It’s versatile and great for developers building production-level applications.\n* **Use LangFlow** if you want the power of LangChain with a visual, no-code/low-code interface. Best for rapid prototyping and users who prefer visual tools over coding.\n* **Use LangSmith** if your focus is on observability and debugging of LLM applications. Ideal when you need to monitor and optimize workflows in a development or production environment.\n\nUltimately, your choice depends on your comfort with code, the complexity of your workflows, and whether you prioritize ease of use, flexibility, or observability.\n\n\n## Conclusion\n\nEach of these tools — **LangGraph**, **LangChain**, **LangFlow**, and **LangSmith** — caters to different stages of developing and managing language model applications. **LangGraph** provides a visual, intuitive way to build complex workflows, while **LangChain** offers a robust, code-first solution for developers looking to create scalable applications. For those who prefer a **low-code**, drag-and-drop approach, **LangFlow** simplifies the process without sacrificing power. Finally, **LangSmith** focuses on observability and debugging, ensuring that your workflows are optimized and reliable. Choosing the right tool depends on your project needs, whether it’s for rapid prototyping, production-level scaling, or monitoring and performance tracking.\n\nHappy coding! 🎉\n\n👨🏾‍💻 [GitHub](https://github.com/mdmonsurali) ⭐️ | 👔[LinkedIn](https://www.linkedin.com/in/mdmonsurali/) |📝 [Medium](https://medium.com/@monsuralirana)\n\nThank you for your time in reading this post!\n\nMake sure to leave your feedback and comments. See you in the next blog, stay tuned 📢\n\n\n## References:\n\n1. “LangChain Documentation” — <https://python.langchain.com/docs/introduction/>\n2. “LangGraph Overview” — <https://langchain-ai.github.io/langgraph/>\n3. “LangFlow GitHub Repository” — [https://github.com/LangFlow/LangFlow](https://docs.langflow.org/)\n4. “LangSmith Introduction” — <https://www.langchain.com/langsmith>\n5. “How to Build Chatbots With LangChain” by JetBrains blog — <https://blog.jetbrains.com/pycharm/2024/08/how-to-build-chatbots-with-langchain/>\n\n"},{"lang":"en","group":"blog","slug":"blog/o1-preview-vs-claude-3-5-sonnet-comparing-top-llms-d68734b53c93","frontmatter":{"title":"o1-preview vs. claude-3.5-sonnet: Comparing top LLMs","meta_title":"o1-preview vs. claude-3.5-sonnet: Comparing top LLMs","description":"Discover how OpenAI’s o1-preview compares to Claude 3.5 Sonnet in performance, speed, and capabilities.","date":"2024-10-27T13:58:01.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kTWAcpRdOpsrFIDZjjjr7Q.jpeg","categories":["Programming","Machine Learning","Generative AI"],"author":"Rifx.Online","tags":["o1-preview","Claude","throughput","latency","reasoning"],"draft":false,"slug":"blog/o1-preview-vs-claude-3-5-sonnet-comparing-top-llms-d68734b53c93"},"content":"\n\n\n\nToday (Sep 12, 2024), OpenAI unveiled its latest language model, o1-preview. This advanced model is engineered to dedicate more time to processing before generating responses, enabling it to tackle complex tasks and solve challenging problems in science, coding, and mathematics with enhanced capabilities.\n\nIn this blog post, we’ll thoroughly analyze o1-preview and compare it to Claude 3.5 Sonnet, which was previously considered one of the most advanced models available.\n\n\n\n\n## Comparison Methodology\n\nOur analysis utilizes [Keywords AI’s LLM playground](https://docs.keywordsai.co/features/prompt/model-playground), a platform that supports over 200 language models and offers function-calling capabilities. We’ll explore the following aspects:\n\n* Basic comparison\n* Benchmark comparison\n* Processing speed\n* Evaluation metrics\n* Suggested use cases\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*yc171ikejtBy_o11.jpeg)\n\n\n## Basic Comparison\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*z2FrS_AVig7Y6eU_.jpeg)\n\nNote: o1-preview doesn’t support Streaming, function calling, and system messages.\n\n\n## Benchmark Comparison\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Bx_vAvFc9DAD0cZA.jpeg)\n\nO1-preview outperforms Claude 3.5 Sonnet across all benchmarks. The smallest gap is in MMLU (general knowledge). GPQA Diamond, testing graduate-level reasoning, shows a significant performance difference. The MATH benchmark reveals the largest gap, highlighting o1-preview’s advanced mathematical capabilities. These results indicate o1-preview’s substantial improvements in complex reasoning and problem-solving across various domains.\n\n\n## Speed Comparison\n\nO1-preview takes longer to think and respond than other LLMs. While direct speed comparisons may not be entirely fair, testing o1-preview’s speed is crucial. This information helps developers better understand o1-preview’s capabilities and determine if it’s suitable for their projects. Note: As o1-preview doesn’t support streaming, we disabled streaming for both models. Consequently, time to first token (TTFT) couldn’t be measured.\n\n\n## Latency\n\nOur tests, involving hundreds of requests per model, revealed significant differences. Claude 3.5 Sonnet averages 18.3s/request, whereas o1-preview takes 39.4s/request. O1-preview’s significantly longer latency is due to its extended thinking and reasoning process.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*2PMkgPVuylFxwfIa.jpeg)\n\n\n## Throughput (Tokens per second)\n\nDespite higher latency, o1-preview shows superior throughput. O1-preview generates 92.94 tokens/second, while Claude 3.5 Sonnet produces 74.87 tokens/second. This indicates that o1-preview’s longer generation time is primarily due to its initial processing phase rather than token generation speed.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*wxqpnwZhl9pnbw8y.jpeg)\n\n\n## Performance comparison\n\nWe conducted evaluation tests on the [Keywords AI platform](https://keywordsai.co/). The evaluation comprised three parts:\n\n* **Coding Task**: Both models successfully completed frontend and backend development tasks. O1-preview demonstrated superior performance with longer contexts, identifying and resolving bugs more efficiently in the first attempt. It also exhibited a more thorough code analysis capability.\n* **Logical Reasoning**: O1-preview excels in reasoning tasks. Its thinking process closely mimics human cognition. While Claude 3.5 Sonnet performs well on most problems, o1-preview consistently solves complex reasoning challenges, including International Mathematical Olympiad (IMO) level problems.\n* **Writing Task:** Both models perform exceptionally well on writing tasks. They demonstrate the ability to craft genuine, personalized cold emails, as well as concise and meaningful blog posts.\n\n\n## Model Recommendations\n\no1-preview\n\n* **Best for:** Complex problem-solving in mathematics, coding, and physics. Particularly suited for researchers tackling challenging tasks.\n* **Not suitable for:** AI applications requiring rapid response times or heavily reliant on system prompts. Voice AI applications due to lack of streaming support.\n\nClaude 3.5 Sonnet\n\n* **Best for:** Most AI applications requiring problem-solving capabilities and high-quality content generation.\n* **Not suitable for:** Voice AI applications or projects with strict budget constraints requiring lower operational costs.\n\n\n## How to integrate o1-preview into your AI apps.\n\nTo incorporate o1-preview into your AI applications, simply visit the Keywords AI model page and locate the “View code” button. Click this button to copy the provided code snippet, then paste it directly into your codebase. With this straightforward process, you’ll be ready to harness the power of o1-preview in your projects, enabling you to tackle complex problems and generate high-quality content with ease.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*XyQ9QiI7TN8Uc5Jp.jpeg)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*t8fEYlEs13eM7D28lVbtIw.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*yhu9y5ixNuxeFVe1.png)\n\nThis story is published on [Generative AI](https://generativeai.pub/). Connect with us on [LinkedIn](https://www.linkedin.com/company/generative-ai-publication) and follow [Zeniteq](https://www.zeniteq.com/) to stay in the loop with the latest AI stories.\n\nSubscribe to our [newsletter](https://www.generativeaipub.com/) and [YouTube](https://www.youtube.com/@generativeaipub) channel to stay updated with the latest news and updates on generative AI. Let’s shape the future of AI together!\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*PelNtaNaEVDWgMWr.png)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/qwen2-5-1-5b-the-future-of-mobile-ai-6bd5f29bbc84","frontmatter":{"title":"Qwen2.5 1.5b: the future of Mobile AI?","meta_title":"Qwen2.5 1.5b: the future of Mobile AI?","description":"Local Testing and Evaluation of Alibaba Cloud’s Latest LLM. With llama-cpp-python and a DIY prompt catalog.","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*awb56jkdXobA-Ip6d-QHRA.png","categories":["Programming","Technology","Science"],"author":"Rifx.Online","tags":["Qwen2.5","summarization","retrieval","mobile","improvements"],"draft":false,"slug":"blog/qwen2-5-1-5b-the-future-of-mobile-ai-6bd5f29bbc84"},"content":"\n### Local Testing and Evaluation of Alibaba Cloud’s Latest LLM. With llama-cpp-python and a DIY prompt catalog.\n\n\n\nIn part one we explored together the innovations from Alibaba Cloud’s team with the release of the Qwen2.5 models family.\n\nIn Generative AI benchmarks are now the main *oracle*: the validity of a new LLM needs to pass several verdicts. The more benchmark records you break, the better you are.\n\nIt is the way to win the SOTA race\n\nWell, I disagree. Even though for the AI advancement we need milestones and better performances, still the user experience and the personal point of view cannot be just put aside as irrelevant.\n\nI believe that exploring some frequently used NLP tasks, and putting aside the chat experience, we must focus on the quality of the replies. And we are the only benchmark required. Our user experience is the best indicator to understand if a model is good or not. The model must be reliable enough to be used in an automated workflow.\n\nBy the way, I already run what I decided to call [RBYF — Revised Benchmarks with You as a Feedback](https://open.substack.com/pub/thepoorgpuguy/p/rbyf-is-here-revised-benchmarks-with?r=i78xo&utm_campaign=post&utm_medium=web) on the claimed amazing Llama3.2–1B-instruct… and Qwen2.5–1.5b is so much better!\n\nSo in this article, as promised, we will verify with our own eyes how good is this model for every day use.\n\nBack to us… Let’s get started!\n\n## Requirements\n\nHere we are going to build a minimal text interface to be able to run the model, test different tasks and wait for user feedback to evaluate it.\n\nThe requirements are minimal, but I suggest you to create a new project directory and a virtual environment.\n\nCreate a `venv` (python 3.11+ is required): I tested it on my Mini-PC running Windows 11.\n\n```python\n## create the virtual environment\npython -m venv venv\n## activate the venv\nvenv\\Scripts\\activate\n## Install the dependencies \npip install llama-cpp-python==0.3.0 tiktoken\n```\n\nWe need to download the GGUF file from the official qwen repository on Hugging Face [https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF): I used the `qwen2.5-1.5b-instruct-q5_k_m.gguf` version.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*YtQJb_xyq_xcF40yRWPcZA.png)\n\nWe are all set!\n\nNote: if you want to add a different Backend support of a GPU accelerator, you can follow [the instructions in the repo](https://github.com/abetlen/llama-cpp-python#supported-backends). I used, for example, the Vulkan support so before the pip install I added the environment variable\n\n```python\n## Vulkan support - for Windows\n$env:CMAKE_ARGS = \"-DGGML_VULKAN=on\"\n```\n\n## The Code — a main app and a library\n\nTo keep the code at minimum, I decided to extend some functionalities using an external library. Well, it is a Do It Yourself library, so there are no secrets here.\n\nYou can find all the details in my article here:\n\nAnd to speed it up you can directly [download the file from here](https://github.com/fabiomatricardi/YouAreTheBenchmark/raw/main/QWEN2.5-1.5B/promptLibv2Qwen.py): it contains a version 2 of the `promptLib` discussed in the mentioned above article (and it is called `promptLibv2Qwen.py`, with few fine tuning of the prompt specifically tailored for the `Qwen2.5-1.5B-instruct` model.\n\nSave the file in the main directory, and create a new file called `main.py`\n\n```python\n## Chat with an intelligent assistant in your terminal  \n## MODEL: https://huggingface.co/Qwen\n## qwen2.5-1.5b-instruct-q5_k_m.gguf\nimport sys\nfrom time import sleep\nimport warnings\nwarnings.filterwarnings(action='ignore')\nimport datetime\nfrom promptLibv2Qwen import countTokens, writehistory, createCatalog\nfrom promptLibv2Qwen import genRANstring, createStats\nimport argparse\n### PREPARING FINAL DATASET\npd_id = []\npd_task = []\npd_vote = []\npd_remarks = []\n####################Add GPU argument in the parser###################################\nparser = argparse.ArgumentParser()\nparser.add_argument(\"-g\", \"--gpu\", type=int, default=0,nargs='?',\n                    help=\"The number of layers to load on GPU\")\nargs = parser.parse_args()\nif args.gpu == None:\n   ngpu_layers = 0 \nelse:\n    ngpu_layers = args.gpu\nprint(f'Selected GPU: offloading {ngpu_layers} layers...')   \n####################INITIALIZE THE MODEL###################################\nstops = ['<|im_end|>']\ntasks = createCatalog()\nmodelname = 'qwen2.5-1.5b-instruct-q5_k_m.gguf'\n## create THE LOG FILE \ncoded5 = genRANstring(5)\nlogfile = f'logs/Qwen2.5-1.5B-it_CPP_{coded5}_log.txt'\ncsvfile = f'logs/Qwen2.5-1.5B-it_CPP_{coded5}.csv'\nlogfilename = logfile\n#Write in the history the first 2 sessions\nwritehistory(logfilename,f'{str(datetime.datetime.now())}\\n\\nYour own LocalGPT with 💻 {modelname}\\n---\\n🧠🫡: You are a helpful assistant.')  \nwritehistory(logfilename,f'💻: How can I assist you today in writing?')\n```\n\nHere we are only doing preparations: we import the libraries, including our own personal `promptLibv2Qwen` and also `argparse`. I wanted to try something new: [argparse](https://realpython.com/command-line-interfaces-python-argparse/) is a python library intended for terminal python program where you are reading multiple arguments from the command line.\n\nIn this case here we have only one argument (and no parameters) with th flag `-g` or even `--gpu`. When you run the python code with this argument we will set the number of GPU layers to the maximum (but you can change it yourself).\n\nThen we set some global variables, used across the entire code: the tasks, our prompt collection, the stop words and the log filename.\n\n> NOTE: all the logs are saved into a subdirectory called `logs`… so make sure to create one.\n\nWe are also preparing all the relevant information to store them into a dataset and then save it at the end int a CSV file (for easily creating a performance matrix)\n\n```python\n### PREPARING FINAL DATASET\npd_id = []\npd_task = []\npd_vote = []\npd_remarks = []\n```\n\nWe then load the model into RAM (no GPU) or the VRAM (with GPU) using Llama-CPP-python.\n\n```python\n## LOAD THE MODEL\nprint(\"\\033[95;3;6m\")\nprint(\"1. Waiting 10 seconds for the API to load...\")\nfrom llama_cpp import Llama\nllm = Llama(\n            model_path='models/qwen2.5-1.5b-instruct-q5_k_m.gguf',\n            n_gpu_layers=ngpu_layers,\n            temperature=0.1,\n            n_ctx=8192,\n            max_tokens=1500,\n            repeat_penalty=1.178,\n            stop=stops,\n            verbose=False,\n            )\nprint(f\"2. Model {modelname} loaded with LlamaCPP...\")\nprint(\"\\033[0m\")  #reset all\nhistory = []\nprint(\"\\033[92;1m\")\nprint(f'📝Logfile: {logfilename}')\n```\n\nBy the way, you can find all the code in my GitHub Repository:\n\nThe next one is a one-shot warm up inference: the model neural network is going to be activated for the first time, so think about it like a warm-up lap.\n\nDon’t be scared, I will explain the code\n\n```python\n##################### ALIGNMENT FIRST GENERATION ##############################################\nquestion = 'Explain the plot of Cinderella in a sentence.'\ntest = [\n    {\"role\": \"user\", \"content\": question}\n]\nprint('Question:', question)\nstart = datetime.datetime.now()\nprint(\"💻 > \", end=\"\", flush=True)\nfull_response = \"\"\nfisrtround = 0\nfor chunk in llm.create_chat_completion(\n    messages=test,\n    temperature=0.25,\n    repeat_penalty= 1.31,\n    stop=stops,\n    max_tokens=1500,\n    stream=True,):\n    try:\n        if chunk[\"choices\"][0][\"delta\"][\"content\"]:\n            if fisrtround==0:\n                print(chunk[\"choices\"][0][\"delta\"][\"content\"], end=\"\", flush=True)\n                full_response += chunk[\"choices\"][0][\"delta\"][\"content\"]\n                ttftoken = datetime.datetime.now() - start  \n                fisrtround = 1\n            else:\n                print(chunk[\"choices\"][0][\"delta\"][\"content\"], end=\"\", flush=True)\n                full_response += chunk[\"choices\"][0][\"delta\"][\"content\"]                            \n    except:\n        pass      \ndelta = datetime.datetime.now() - start\noutput = full_response\nprint('')\nprint(\"\\033[91;1m\")\nrating = input('Rate from 0 (BAD) to 5 (VERY GOOD) the quality of generation> ')\nprint(\"\\033[92;1m\")\nstats = createStats(delta,question,output,rating,logfilename,'Alignment Generation',ttftoken)\nprint(stats)\nwritehistory(logfilename,f'''👨‍💻 . {question}\n💻 > {output}\n{stats}\n''')\n```\n\nWe set the first user question and put into a well known chat format dictionary. Then we start our timer (useful for speed, token counts etc…).\n\nWe call the inference with the method `create_chat_completion()` that allows us to accept the prompts in chat format and stream the output one token at the time.\n\nBecause of the first reply from the model does not contain any output tokens (but only statistics) we use the try/except statement. Furthermore, since I want to know when the first token is generated, we raise a flag and stop temporary our time count saving the information inside the `ttftoken` variable.\n\nAt the end of the streaming we count the delta time from start, and wait the user to provide his/her personal feedback on the generated output: giving a mark from 0 to 5 and adding comments related to the compliance with the instruction prompt and user intent.\n\nWe use our internal library called `createStats()` to print all the statistics of the generation, and save them in our log file. The output of the function will be something like this:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8znYCqpisviXvYgrzjWF5w.png)\n\n## Prompt catalog — what we want to test\n\nI wrote here about my habit. I have a catalog of prompts that covers many of the main language tasks used in chat-bots, like summarization, short summarization, casual chat, RAG, truthful RAG and so on.\n\nThe idea is to be able to load the model in 5 minutes, and start evaluating each task. At the end of every generation the user is prompted to give a mark (a score from 0 to 5) and leave any comments if required.\n\nThis is crucial: not all the models are alike, and small/big adjustments to the wording in the prompts are always required.\n\nSo back to the code… Because the previous one was only a warm-up, now it will start the real while loop, iterating over the entire prompt catalog. See the workflow here below…\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*EL0Q97Du6HwtcYQZ.png)\n\nThere are only few changes in the code, and I will point them out, so bear with me.\n\n```python\n############################# AUTOMATIC PROMPTING EVALUATION  11 TURNS #################################\nid =1\nfor items in tasks:\n    fisrtround = 0\n    task = items[\"task\"]\n    prompt = items[\"prompt\"]\n    test = []\n    print(f'NLP TAKS>>> {task}')\n    print(\"\\033[91;1m\")  #red\n    print(prompt)\n    test.append({\"role\": \"user\", \"content\": prompt})\n    print(\"\\033[92;1m\")\n    full_response = \"\"\n    start = datetime.datetime.now()\n    print(\"💻 > \", end=\"\", flush=True)\n    for chunk in llm.create_chat_completion(\n        messages=test,\n        temperature=0.15,\n        repeat_penalty= 1.31,\n        stop=stops,\n        max_tokens=1500,\n        stream=True,):\n        try:\n            if chunk[\"choices\"][0][\"delta\"][\"content\"]:\n                if fisrtround==0:\n                    print(chunk[\"choices\"][0][\"delta\"][\"content\"], end=\"\", flush=True)\n                    full_response += chunk[\"choices\"][0][\"delta\"][\"content\"]\n                    ttftoken = datetime.datetime.now() - start  \n                    fisrtround = 1\n                else:\n                    print(chunk[\"choices\"][0][\"delta\"][\"content\"], end=\"\", flush=True)\n                    full_response += chunk[\"choices\"][0][\"delta\"][\"content\"]                            \n        except:\n            pass      \n    delta = datetime.datetime.now() - start\n    print('')\n    print(\"\\033[91;1m\")\n    rating = input('Rate from 0 (BAD) to 5 (VERY GOOD) the quality of generation> ')\n    print(\"\\033[92;1m\")\n    stats = createStats(delta,prompt,full_response,rating,logfilename,task,ttftoken)\n    print(stats)\n    writehistory(logfilename,f'''👨‍💻 > {prompt}\n💻 > {full_response}\n{stats}\n''')\n    pd_id.append(id)\n    pd_task.append(task)\n    pd_vote.append(rating[:2])\n    pd_remarks.append(rating[2:])\n    id += 1\n## create dataframe and save to csv\nzipped = list(zip(pd_id,pd_task,pd_vote,pd_remarks))\nimport pandas as pdd\ndf = pdd.DataFrame(zipped, columns=['#', 'TASK', 'VOTE','REMARKS'])\n#saving the DataFrame as a CSV file \ndf_csv_data = df.to_csv(csvfile, index = False, encoding='utf-8') \nprint('\\nCSV String:\\n', df_csv_data)  \n```\n\nThe main changes are only in the first lines:\n\n```python\nfor items in tasks:\n    fisrtround = 0\n    task = items[\"task\"]\n    prompt = items[\"prompt\"]\n```\n\nIf you read the article about the `promptLib` you shouldn’t be surprised: but if you are new, here we are iterating over a list of dictionaries with the following structure:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*rGcKJWNzSUrcu4wi.png)\n\nSo for each items in the catalog (means a pair of tasks and prompts) we extract the task description and the prompt for the task.\n\n```python\ntest.append({\"role\": \"user\", \"content\": prompt})\n```\n\nThen we create the chat template message in a temporary list called `test` and pass it to the `create_chat_template()` method for generation.\n\nEverything else is the same.\n\nSave the file, and with the `venv` activated run:\n\n```python\npython main.py\n## if you are using the GPU python main.py -g\n```\n\nThis will get you something like the below example…\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MhhQu4lLjtU__Wjf0dSWBg.gif)\n\nNote that at the end of the entire Prompt Catalog a *csv* file is created with the summary of all the tasks!\n\n## Test Overview\n\nI run them with several Small Language Models, from [Qwen2–1.5B-instruct](https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF), to [Gemma2–2B-instruct](https://huggingface.co/bartowski/gemma-2-2b-it-GGUF), with [Llama3.2–1B-instruct](https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF) and finally the new [Qwen2.5–1.5B-instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF).\n\nWhile I was [quite disappointed by Llama3.2–1B-instruct](https://generativeai.pub/llama3-2-1b-instruct-is-ok-but-not-good-enough-28f88046b63e), I have been amazed by the good job done by the new [Qwen2.5–1.5B-instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF).\n\nAt the end of every generation the user is asked to evaluate the results with a mark from 0 to 5. **In this case the user is me…**\n\nThis kind of qualitative analysis is indeed poor, so every mark does have a description, and the user can add comments (“some wrong information”, “maybe better change the wording in the prompt” )\n\nHere the qualitative matrix with a description\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*eBdPfZtfr99MsvLh6tt42w.png)\n\n## The good and the bad — details\n\nSummarization was amazing. Listing the main topics over a log text was also very good.\n\nThe RAG tasks were quite fast (even on my mini-PC) and truthful RAG (ask questions out of the context) was on spot.\n\nSo really good.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*DuV3LJep_PuDqiCcAMb6Cg.png)\n\nThere were also few shortcomings: even with a temperature of only `0.15` I got some made up information in the summarize in two sentences task. And this is not good.\n\n> I expect this can be fixed with a little prompt tuning, or with a temperature at `0`.\n\nAnother fact is that the creative writing was quite bad: for the test I used some fixed Generation Parameters.\n\n```python\n        temperature=0.15,\n        repeat_penalty= 1.31,\n```\n\nProbably for creative writing, with Qwen2.5–1.5B-instruct, we should use an higher `repeat_penalty` and also a higher `temperature`.\n\nAs a side note I have to say that also the reflection prompt was not that bad at all! The opening/closure of the tags was not maintained (so not easy to put this into a pipeline or workflow) but the overall flow of the generation and the “Chain of Thought” reasoning process was fairly good.\n\n```python\n<thinking>\nThe importance of using Generative Artificial Intelligence (AI) in \nteaching cannot be overstated, as this technology has immense potential \nnot only within current educational practices but also towards shaping a \nmore innovative approach to lifelong learning.\n</thinking>\n\n**Chain Of Thought:**\n1. **Understanding the Impact**: AI can automate repetitive tasks and provide \npersonalized feedback for students based on their performance data \n(e.g., through chatbots or adaptive assessments).\n2. **Enhancing Learning Experience**: By integrating Generative AI, \nteachers could create more engaging lessons that cater to diverse learning \nstyles.\n3. **Improving Student Outcomes**: Personalized instruction can lead to \nbetter understanding and retention of knowledge for students.\n\n**Reflection:**\n- The integration of generational technology in education is crucial as \nit aligns with the evolving needs of learners who are increasingly digital \nnatives, requiring a pedagogical approach that incorporates these technologies.\n</thinking>\n\n<output>\nTeachers should learn how Generative AI can be used to enhance their teaching \nmethods and improve student outcomes. This includes understanding its \ncapabilities for automating tasks like grading or creating personalized \nlearning paths.\n\n**Learning Path:**\n1. **Foundation Courses in Education Technology**: Start with courses \nthat provide an overview of educational technology, including the basics \nof digital tools.\n2. **AI Fundamentals Course (Coursera)** - Learn about AI concepts and \nhow they can be applied to education through Coursera's free online \ncourse on Artificial Intelligence for Educators.\n\n3. **Specialized Courses in Generative Learning**: Enroll in courses \nthat focus specifically on generational learning, such as \"Generative Design\" \nor similar specialized programs offered by universities like MIT.\n4. **Professional Development Workshops and Conferences** - Attend \nworkshops focused on integrating AI into teaching practices at local \neducational conferences.\n\n5. **Online Communities for Educators (e.g., Edmodo)**: Join online \ncommunities where educators discuss the use of generative technology \nin education, share resources, or ask questions about implementation.\n6. **Certification Programs**: Consider obtaining certifications that \nrecognize your knowledge and skills related to AI integration into teaching \npractices through organizations like Google's \"AI for Educators\" program.\n\nBy following this learning path, teachers can not only enhance their own \nprofessional development but also contribute positively towards the future \nof education by incorporating generative technology effectively.\n</output>\n```\n\nI think that for the size of this Small Language Model, the result is not bad at all!\n\n## Comparing Qwen2.5 with Llama3.2\n\nEven though this is only coming from my personal evaluation, I want to share it with you.\n\nBoth models are designed to be used on mobile devices, but the performance is quite different. Look here below:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*T6vLgvOKdkotlV1K5x6-QQ.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*DuV3LJep_PuDqiCcAMb6Cg.png)\n\nFirst of all, the overall score difference is huge (41 for Llama3.2, 57 for Qwen2.5).\n\nSecondly if you think about what you may ask on a mobile device, in terms of language tasks, is to have a smooth chatting experience (task 4), good summarization (tasks five to 7) and some creative writing (task 11 and 13).\n\nIn terms of speed, running the model only on CPU, with a very limited mini-PC, **I got an average inference speed of 14 t/s.**\n\n## Conclusions\n\nIn the past three months since Qwen2’s release, numerous developers have built new models on the Qwen2 language models, providing valuable feedback to the entire community, but also to Alibaba Cloud.\n\n> During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5\n\nTheir claims come with facts about the new family of models:\n\n* Dense, **easy-to-use**, decoder-only language models, available in 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B sizes, and base and instruct variants.\n* Pretrained on our latest large-scale dataset, encompassing up to 18T tokens.\n* Significant improvements in **instruction following**\n* More **resilient to the diversity of system prompts**, enhancing role-play implementation and condition-setting for chatbots.\n* **Context length support up to 128K** tokens and can generate up to 8K tokens.\n* Multilingual support for over 29 languages\n\nIn my extensive (but certainly limited to one shot prompts and on few NLP tasks) tests I could see with my very own eyes that the claims were based on a good quality training dataset and curated fine tuning.\n\nThis model can perform extremely good on mobile devices!\n\n\n"},{"lang":"en","group":"blog","slug":"blog/rag-llm-and-pdf-conversion-to-markdown-text-with-pymupdf-03af00259b5d","frontmatter":{"title":"RAG/LLM and PDF: Conversion to Markdown Text with PyMuPDF","meta_title":"RAG/LLM and PDF: Conversion to Markdown Text with PyMuPDF","description":"Data feeding in markdown text format increases generated text quality","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*swPjVuudAhsoRiiw3Ee32w.png","categories":["Programming","Technology","Technology/Web"],"author":"Rifx.Online","tags":["markdown","PyMuPDF","LLM","RAG","PDF"],"draft":false,"slug":"blog/rag-llm-and-pdf-conversion-to-markdown-text-with-pymupdf-03af00259b5d"},"content":"\n\n\n\n\n### Data feeding in markdown text format increases generated text quality\n\n\n\n\n## Introduction\n\nIn the context of **Large Language Models (LLMs)** and **Retrieval-Augmented Generation (RAG)** environments, data feeding in **markdown text format** holds **significant importance**. Here are some detailed considerations.\n\n**LLMs** are powerful language models that can generate coherent and contextually relevant text. However, they may sometimes produce responses that lack factual accuracy or context. By incorporating retrieval-based methods (like RAG), we can enhance the quality of generated text.\n\n**RAG** enables the integration of **external data** — previously absent in the LLM’s training data — into the text generation process. This inclusion mitigates “hallucination issues’’ and enhances the relevance of text responses.\n\n\n## Why Markdown for LLM?\n\n**Markdown** is a lightweight markup language that allows users to format plain text using simple syntax. It is widely used for creating structured documents, especially on platforms like GitHub, Jupyter notebooks, and various content management systems. When feeding data into an LLM or RAG system, using markdown format provides several benefits:\n\n1. **Structured Content**: Markdown allows you to organize information into headings, lists, tables, and other structured elements. This structure aids in better understanding and context preservation.\n2. **Rich Text**: Markdown supports basic formatting such as bold, italics, links, and code blocks. Including rich text in the input data enhances the context for the language model.\n3. **Embedding Links and References**: Markdown lets you embed hyperlinks, footnotes, and references. In RAG scenarios, this can be crucial for referring to external sources or providing additional context.\n4. **Ease of Authoring**: Markdown is human-readable and easy to write. Authors can create content efficiently without complex formatting tools.\n5. **Chunking**: Essential for RAG systems, chunking (otherwise known as “splitting”) breaks down extensive documents for easier processing. With PyMuPDF data extraction available in MD format we support chunking to keep text with common context together. **Importantly, PyMuPDF extraction in MD format allows for [Level 3 chunking](https://readmedium.com/five-levels-of-chunking-strategies-in-rag-notes-from-gregs-video-7b735895694d#b123)**.\n\nIn summary, using markdown text format in LLM and RAG environments ensures more accurate and relevant results because it supplies richer data structures and more relevant data chunk loads to your LLM.\n\n\n## PyMuPDF Support for Markdown Conversion of a PDF\n\nSince its inception, PyMuPDF has been able to extract text, images, vector graphics and, since August 2023, tables from PDF pages. Each of these object types has its own extraction method: there is one for text, and yet others for tables, images and vector graphics. To meet the requirements of RAG, we merged these disparate extractions to produce one common, unified **Markdown** string which consistently represents the page’s content as a whole.\n\nAll this is implemented as [one Python script](https://github.com/pymupdf/RAG/blob/main/helpers/pymupdf_rag.py). It can be imported as a module by some other script, or be invoked as a line command in a terminal window like this:\n\n`$ python pymupdf_rag.py input.pdf [-pages PAGES]`\n\nIt will produce a text file (called `input.md`) in **Markdown** format. The optional parameter `PAGES` allows restricting the conversion to a subset of the PDF’s total pages. If omitted, the full PDF is processed.\n\n\n## Markdown Creation Details\n\n\n### Selecting Pages to Consider\n\nThe “`-pages`” parameter is a string consisting of desired page numbers (1-based) to consider for markdown conversion. Multiple page number specifications can be given, separated by commas. Each specification either is one integer or two integers separated by a “`-`” hyphen, specifying a range of pages. Here is an example:\n\n“`-pages 1–10,15,20-N`”\n\nThis would include pages 1 through 10, 15 and pages 20 through the end of the file (capital “N” is treated as the number of the last page).\n\n\n### Identifying Headers\n\nUpon invocation, the program examines all text on the given pages and finds the most frequently used font size. This value (and all smaller font sizes) is assumed to represent **body text**. Larger font sizes are assumed to represent **header text**.\n\nDepending on their relative position in the font size hierarchy, header text will be prefixed with one or more markdown header `#`-tag characters.\n\n\n### Identifying the Processing Mode per Page Area\n\nAll text on each page will first be classified as being either **standard** text or **table** text. Then the page content will be extracted from top to bottom converting everything to markdown format.\n\nThis is best explained by an example:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*u5fv2aAIvDaaAd6H.png)\n\nThis page shows content, that represents typical situations:\n\n* Two tables, having partly overlapping vertical positions. One table has no headers, the other one has **external** column headers.\n* There is a **title** line and **headers** at multiple levels.\n* The **body text** contains a variety of styling details like **bold**, *italic* and `inline code`.\n* Ordered and unordered lists.\n* Code snippet.\n\nLayout analysis will determine three areas and select the appropriate processing modes: **(1)** text, **(2)** table, **(3)** text.\n\nThe generated Markdown text reflects the above faithfully — as much as at all possible in this format.\n\nFor an example, let us look at the output for the table with external headers:\n\n\n```python\n|Column1|Column2|\n\n|---|---|\n\n|Cell (0, 0)|Cell (0, 1)|\n\n|Cell (1, 0)|Cell (1, 1)|\n\n|Cell (2, 0)|Cell (2, 1)|\n```\nThis is GitHub-compatible format with the minimum possible token size — an important aspect for keeping feeds into RAG systems small.\n\n**Column borders** are indicated by the “`|`” character. A text line is assumed to be a **table header** if it is followed by a line of the form “`|---|---| …`”. The full **table definition** must be preceded and followed by at least one empty line.\n\nPlease note that for technical reasons markdown tables must have a header and thus will choose the first table row if no external header is available.\n\nTo confirm overall fidelity, here is how a Markdown parser processes the full page:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Ge83uj7FiM4T6XFn)\n\n\n## Invoking the Markdown Converter Programmatically\n\nInstead of executing a program in the command line, Markdown conversion can also be requested by a program:\n\n\n```python\nimport fitz\nfrom pymupdf_rag import to_markdown  # import Markdown converter\n\ndoc = fitz.open(“input.pdf”)  # open input PDF\n\n## define desired pages: this corresponds “-pages 1-10,15,20-N”\npage_list = list(range(9)) + [14] + list(range(19, len(doc) – 1))\n\n## get markdown string for all pages\nmd_text = to_markdown(doc, pages=page_list)\n\n## write markdown string to some file\noutput = open(“out-markdown.md”, “w”)\noutput.write(md_text)\noutput.close()\n```\n\n## Conclusion\n\nBy integrating PyMuPDF’s extraction methods, the content of PDF pages will be faithfully converted to markdown text that can be used as input for RAG chatbots.\n\nRemember, the key to a successful RAG chatbot lies in the quality and completeness of information it can access.\n\nPyMuPDF-enabled markdown extraction ensures that this information from PDFs is not only possible but straightforward, showcasing the library’s strength and developer-friendliness. Happy coding!\n\n\n### Source Code\n\n* [RAG/helpers/pymupdf\\_rag.py (github.com)](https://github.com/pymupdf/RAG/blob/main/helpers/pymupdf_rag.py)\n\n\n### References\n\n* [5 Levels of Text Splitting](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb)\n\n\n### Related Blogs\n\n* [Building a RAG Chatbot GUI with the ChatGPT API and PyMuPDF](https://readmedium.com/building-a-rag-chatbot-gui-with-the-chatgpt-api-and-pymupdf-9ea8c7fc4ab5)\n* [Creating a RAG Chatbot with ChatGPT and PyMUPDF](https://readmedium.com/creating-a-rag-chatbot-with-chatgpt-and-pymupdf-f6c30907ae27)\n* [RAG/LLM and PDF: Enhanced Text Extraction](https://readmedium.com/rag-llm-and-pdf-enhanced-text-extraction-5c5194c3885c)\n\n"},{"lang":"en","group":"blog","slug":"blog/rbyf-qwen2-5-3b-instruct-is-damn-good-dcf443cacc63","frontmatter":{"title":"RBYF: Qwen2.5–3B-instruct is damn good.","meta_title":"RBYF: Qwen2.5–3B-instruct is damn good.","description":"Revised Benchmark with You as a Feedback: the brand new 3B model from Alibaba Qwen is an amazing model, and I can prove it!","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*NWaBtJ64TLUoUHv4F1qJpg.png","categories":["Programming","Technology","Science"],"author":"Rifx.Online","tags":["Qwen","NLP","multimodal","RBYF","evaluation"],"draft":false,"slug":"blog/rbyf-qwen2-5-3b-instruct-is-damn-good-dcf443cacc63"},"content":"\n### Revised Benchmark with You as a Feedback: the brand new 3B model from Alibaba Qwen is an amazing model, and I can prove it!\n\n\n\nThe illusion of emergent properties is largely a product of the metrics used to evaluate these models. And this is a fact.\n\nFew weeks ago I decided to be a little rebel, discard all the official Benchmarks, and start being a Benchmark myself!\n\nThis is the meaning behind this totally made up Acronym RBYF: Revised Benchmark with You as a Feedback. And the underlined principle is that there is no better judge than you, to verify how good a Large Language Model can be.\n\nTo be honest, I am focusing on Small Language Models. I don’t own a dedicated GPU and my computational resources are limited. But again, I agree to the [LLMWare rebel principle number one](https://readmedium.com/getting-work-done-with-genai-just-do-the-opposite-10-contrarian-rules-that-may-actually-work-634501602a27):\n\nUse Small Models, Not Large Ones.\n\nIn this article I am going to show you the results of my evaluation on qwen2.5–3b-instruct. And it is really good!\n\n> Disclaimer: all the prompt used with the results are available in my GitHub repository:\n\n## Less is More\n\nScaling laws describe how model performance improves as the number of parameters and training data increases. This principle has fueled the search for novel abilities in LLMs.\n\n> Simply increasing the size of a model, we can unlock new capabilities…\n\nScaling laws describe the relationship between model performance and the number of parameters and training data. As models grow larger and are trained on more data, we expect their performance to improve. This has led to a relentless pursuit of larger and larger LLMs, in the hope of unlocking new capabilities.\n\nEmergent properties are those that arise from the interactions of individual components within a complex system. They cannot be predicted or understood by studying the components in isolation. In the case of LLMs, the hope is that as these models grow larger and more complex, they will exhibit unexpected and new capabilities.\n\nThis is a fairy-tale.\n\nIn the past weeks we saw with our own eyes, that over-trained and well curated Small Language Models can perform as good as their big brothers. And this is a punch to the so called emergent abilities, striking back to the scaling law. Gemma2–2B, Qwen2.5–3B and even the latest Llama3.2–3B are far better models than the old SOTA 7B models.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*-EjpdEky-Hf3WEQn.png)\n\n## The Qwen2.5 family of Models\n\nAlibaba Cloud released on middle of September their flagship model family Qwen2.5.\n\n> Alibaba Cloud’s revolutionary journey with Qwen is showing once again strong Leadership through Innovation\n\nQwen2.5 is the large language model and large multimodal model series of the Qwen Team, Alibaba Group. Both language models and multimodal models are pretrained on large-scale multilingual and multimodal data and post-trained on quality data for aligning to human preferences. Qwen is capable of natural language understanding, text generation, vision understanding, audio understanding, tool use, role play, playing as AI agent, etc.\n\n**What stands out in the new Qwen2.5 is the thoroughly curated training dataset.** You can clearly understand this, checking the small models performance.\n\nIf the Small Language Models of the family are good, means that the training and the dataset were highly revised and curated.\n\nHere some numbers:\n\n* Dense, easy-to-use, decoder-only language models, available in 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B sizes, and base and instruct variants.\n* Pretrained on our latest large-scale dataset, encompassing up to 18T tokens.\n* Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON.\n* More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n* Context length support up to 128K tokens and can generate up to 8K tokens.\n* Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*3tnTS_UCImBRBDeKmFyjVQ.png)\n\n## Qwen2.5–3B-instruct\n\nEven though for the AI advancement we need milestones and better performances, still the user experience and the personal point of view cannot be just put aside as irrelevant.\n\nI believe that exploring some frequently used NLP tasks, and putting aside the chat experience, we must focus on the quality of the replies. And we are the only benchmark required. **Our user experience is the best indicator to understand if a model is good or not**. The model must be reliable enough to be used in an automated workflow.\n\nBy the way, I already run what I decided to call [RBYF — Revised Benchmarks with You as a Feedback](https://open.substack.com/pub/thepoorgpuguy/p/rbyf-is-here-revised-benchmarks-with?r=i78xo&utm_campaign=post&utm_medium=web) on [Qwen2.5–1.5b-instruct](https://ai.gopubby.com/qwen2-5-1-5b-the-future-of-mobile-ai-6bd5f29bbc84): you can read the details. In the article I also explained how to create your test bench. The method described is the same I used for Qwen2.5–3B.\n\nLet’s begin with an overall performance across all the tasks. The model has been evaluated by me (in this case is my Own Feedback) based on the qualitative matrix as displayed here below.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*rdVHfCDWX9jlvtiq)\n\nThe overall score is 62/70 = 8.8\n\nOk, but based in what Qwen2.5–3B-instruct got this evaluation score?\n\n## Test Overview\n\nThe idea behind this is a fair user feedback, not an automated one across standard benchmarks and frameworks. Is a Small Language Model able to satisfy the user intent over the mostly used NLP task?\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*wgngfGvSebeoH3YxTdvc8A.png)\n\nWe want to validate both the user intent and the quality of the responses. Here the breakdown of every task:\n\n### Introduction\n\nTo verify how the model reply to initial greetings and talks about itself.\n\n### Explain in one sentence\n\nSynthesis and summarization. The final evaluation focuses in whether or not the model is able to fit the reply in only one sentence\n\n### Explain in three paragraphs\n\nUser intent is to get a smart explanation of the text that must fit into three paragraphs. SML usually find it very hard because the always put a recap (forth) paragraph.\n\n### Say “I am ready”.\n\nIn a chat turn base application, instruction-following models are usually asked to first read a provided text and later on to complete some sort of analysis. Usually SML cannot do this…\n\n### Summarize\n\nBasic summarization, with no limits. Here we want to evaluate how the summary is grounded on the text, without made up facts.\n\n### Summarize in two sentences\n\nBasic summarization, with a 2 sentences limits. Here we want to evaluate how the summary is grounded on the text, without made up facts: but as well we want to ensure the 2 sentences constraint.\n\n### Write in a list the three main key points — format output\n\nFocus: the SML must format the output in a specific format. In this prompt we ask to create a list of the 3 key points and give the output as a python list.\n\n### Table of Contents\n\nThis task is quite hard for many SML. The prompt requires some adjustments otherwise the model return a markdown table. The user want an ordered list of the topics following the provided document structure.\n\n### RAG\n\nRetrieval Augmented Generation, without any framework (haystack, Langchain…). This is one of the most used tasks for a Language model. The reply is evaluated on the ability to understand the instructions and how grounded to the text is the answer.\n\n### Truthful RAG\n\nIt is A RAG with a question completely out of the provided context. The model must reply unanswerable meaning it understood the instruction and it is not using any external knowledge or made up information.\n\n### Write content from a reference\n\nThis is a creative task. Using a reference text the SML must provide a new essay.\n\n### Extract 5 topics\n\nThe focus on this task is to verify that:\n\n* there are exactly 5 topics\n* they are grounded (no hallucination)\n\n### Creativity: 1000 words SF story\n\nCompletely creative task. It is very hard even for larger models to be coherent and produce a small story hitting the correct word count.\n\n### Reflection prompt\n\nThe reflection prompt is meant to verify the CoT reasoning process of the model. The output is constrained into opening/closure of special tags. The focus is both on the reasoning and on a consistent output structure. The output must be easily used for further structured prompts or visualizations. You can read more in this article:\n\n## The evaluation process\n\nAt the end of every generation the user is asked to evaluate the results with a mark from 0 to 5. **In this case the user is me…**\n\nThis kind of qualitative analysis is indeed poor, so every mark does have a description, and the user can add comments (“some wrong information”, “maybe better change the wording in the prompt” )\n\nHere the qualitative matrix with a description\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*H_Qx2UT1lxqy2BF4.png)\n\n## The good and the bad — details\n\nOverall Qwen2.5–3B-instruct is an All Rounded model. The average score for each task is 4.4 out of 5, that is quite impressive.\n\nSummarization was amazing. Listing the main topics over a log text was also very good.\n\nIt shines in RAG, sometimes being too verbose: in the truthful RAG it also gave some explanation, and the reasons for the *unanswerable* reply.\n\nWe can have a look together. It is quite often a hard feat for SML to precisely count the number of sentences and paragraphs.\n\n```python\n**Write a two-sentences summary of the following text:**\n\nChina's regime under Chinese Communist Party operates without transparent \nelections and primary candidates. While undesirable for democracies, \nit allows long-term planning free from electoral pressures that can change \nplans abruptly; this has led to China achieving peak carbon dioxide emissions \nin 2023 due largely to increased renewable energy installations, potentially \nsetting a new global standard for sustainable development despite its status \nas one of the world's largest polluters.\n```python\n**Write a two-sentences summary of the following text:**\n\nChina's regime under Chinese Communist Party operates without transparent \nelections and primary candidates. While undesirable for democracies, \nit allows long-term planning free from electoral pressures that can change \nplans abruptly; this has led to China achieving peak carbon dioxide emissions \nin 2023 due largely to increased renewable energy installations, potentially \nsetting a new global standard for sustainable development despite its status \nas one of the world's largest polluters.\n\n```\n\nTable of Contents is another nightmare for the Language models, because they usually understand to create a table… But look at this:\n\n```python\nA \"table of content\" provides an organized structure to the text, allowing \nreaders quick access to key points. Write the \"table of content\" of \nthe following text.\n\n[table of contents]\n1. Introduction: Overview and Critique on China's Governance System  \n2. Comparison with Western Democracies - Undesirable Dictatorship? \n3. Advantages in Long-Term Planning for Sustainability   \n4. The Chinese Model as a Comparative Advantage  \n5. Recent Achievements in Carbon Emissions Reduction\n6. Transition to Renewable Energy: A Turning Point?\n7. Potential Leadership Role of China and India  \n8. Conclusion - Future Directions\n\n```python\nA \"table of content\" provides an organized structure to the text, allowing \nreaders quick access to key points. Write the \"table of content\" of \nthe following text.\n\n[table of contents]\n1. Introduction: Overview and Critique on China's Governance System  \n2. Comparison with Western Democracies - Undesirable Dictatorship? \n3. Advantages in Long-Term Planning for Sustainability   \n4. The Chinese Model as a Comparative Advantage  \n5. Recent Achievements in Carbon Emissions Reduction\n6. Transition to Renewable Energy: A Turning Point?\n7. Potential Leadership Role of China and India  \n8. Conclusion - Future Directions\n\n\n```\n\nI assign a score of 4 out of 5 to the truthful RAG, but it could have been easily a 5. The most important part is that the response must conatin the word *unanswerable*.\n\nWhy? Imagine you are setting up a chain of prompts, and you need to decide the next step based on the previous reply. If you can simply look for the presence of the work *unanswerable* to steer your next step, you are ok!\n\n```python\n**Reply to the question only using the provided context. If the answer \nis not contained in the provided context say \"unanswerable\".\nquestion: who is Anne Frank?**\n{context}\n**Remember: if you cannot answer based on the provided context, say \n\"unanswerable\"\nanswer:**\n\nUnanswerable\nThe given text does not provide any information about Anne Frank \nor her identity. The content is unrelated to this question and focuses \ninstead on discussing China's system compared to Western democracies in \nterms of long-term planning for environmental issues.\n```python\n**Reply to the question only using the provided context. If the answer \nis not contained in the provided context say \"unanswerable\".\nquestion: who is Anne Frank?**\n{context}\n**Remember: if you cannot answer based on the provided context, say \n\"unanswerable\"\nanswer:**\n\nUnanswerable\nThe given text does not provide any information about Anne Frank \nor her identity. The content is unrelated to this question and focuses \ninstead on discussing China's system compared to Western democracies in \nterms of long-term planning for environmental issues.\n\n```\n\nI imagine that the model was trained to provide also the reasons for this kind of replies… Awesome!\n\n### The excellent Reflection\n\nThe main focus of the Reflection prompt, is the ability of the model of returning a correct reasoning process, and format the output following few specific open/closure of tags.\n\nSo far this is **the best Reflection output** from a Small Language Model, even better than the Gemma2–2B:\n\n```python\n*👨‍💻 > You are an AI assistant designed to provide detailed, step-by-step \nresponses. Your outputs should follow this structure:\n1. Begin with a <thinking> section.\n2. Inside the thinking section:\n   a. Briefly analyze the question and outline your approach.\n   b. Present a clear plan of steps to solve the problem.\n   c. Use a \"Chain of Thought\" reasoning process if necessary, breaking down \nyour thought process into numbered steps.\n3. Include a <reflection> section for each idea where you:\n   a. Review your reasoning.\n   b. Check for potential errors or oversights.\n   c. Confirm or adjust your conclusion if necessary.\n4. Be sure to close all reflection sections.\n5. Close the thinking section with </thinking>.\n6. Provide your final answer in an <output> section.\nAlways use these tags in your responses. Be thorough in your explanations, \nshowing each step of your reasoning process. Aim to be precise and logical \nin your approach, and don't hesitate to break down complex problems into \nsimpler components. Your tone should be analytical and slightly formal, \nfocusing on clear communication of your thought process.\nRemember: Both <thinking> and <reflection> MUST be tags and must be closed \nat their conclusion\nMake sure all <tags> are on separate lines with no other text. \nDo not include other text on a line containing a tag.\n\nuser question: explain why it is crucial for teachers to learn how to use \ngenerative AI for their job and for the future of education. \nInclude relevant learning path for teachers and educators. *\n\n\n**💻 > <thinking>\nI will begin by analyzing the importance of teaching about Generative \nArtificial Intelligence (AI) in an educational context, then outline a \nstep-by-step plan on what steps should be taken to learn this technology \neffectively as well as provide resources or courses that could help.\nFirstly, understanding AI is crucial for teachers because it can \nsignificantly enhance their ability to support students. \nTeachers need the knowledge and skills necessary not only to integrate these tools into teaching but also understand how they work so they are able to guide both themselves and their pupils in ethical use of technology.\n</thinking>\n\n<reflection>\nMy initial thought process was clear, emphasizing that teachers should \nbe equipped with AI literacy as it can improve educational outcomes. \nI outlined the importance for guiding students ethically while using \nthese tools effectively.\nHowever, my plan needs more detail on specific steps to learn about \ngenerative AI and resources available online or in-person courses.\n</reflection>**\n*MY PERSONAL NOTE: here it is starting an additional thinking after the first \nreflection... And this is really too good to be true!!*\n**<thinking>\nTo create a comprehensive learning path that teachers could follow:\n1. **Understand Basic Concepts**: Start with basic concepts of machine \n   learning (ML) such as supervised vs unsupervised, deep neural networks  \n   etc., which are foundational to understanding generative AI.\n2. **Learn About Generative Models**:\n   - Study different types: Variational Autoencoders (VAEs), GANs.\n   - Understand how these models generate new data that resembles the \n     training set but is not identical and can be used for tasks like \n     text generation, image synthesis etc., which are crucial in education.\n3. **Explore Applications**: Look at real-world applications of generative \n   AI such as language modeling (e.g., GPT-2), computer vision techniques to \n   create realistic images or videos from raw data.\n4. **Ethical Considerations and Implications for Education**:\n   - Discuss the ethical implications, including bias in models' outputs \n     which could affect students’ perceptions about themselves if they are \n     exposed regularly through AI-generated content that is not representative \n     of them.\n5. **Hands-on Practice**: Engage with practical exercises using open-source \n   tools like TensorFlow or PyTorch to build simple generative model \n   applications.\n6. **Stay Updated and Network**:\n   - Follow the latest research in ML, especially on Generative Models \n     by following relevant journals (e.g., NeurIPS), conferences such as \n     ICML etc.\n7. Attend Workshops/Conferences: Participate actively at AI-related workshops \n   or events to network with professionals working within this field.\n</thinking>**\n*MY PERSONAL NOTE: and no surprise, an additional reflection now starts, \nbefore producing the final output*\n**<reflection>\nMy plan has been expanded and includes a more detailed approach for teachers, \ncovering foundational knowledge through practical application of generative \nmodels in education along with ethical considerations that are crucial. \nI have also included staying updated on the latest research as an important \npart.\nHowever, it's essential to ensure these resources or courses exist so \neducators can access them easily.\n</reflection>\n\n<output>\nIt is vital for teachers and educational professionals alike to learn how \ngenerative AI works because this technology has significant potential in \nenhancing teaching methods. By understanding basic concepts of machine \nlearning (ML) such as supervised vs unsupervised, deep neural networks etc., \nthey will be better equipped with the knowledge necessary not only to \nintegrate these tools into their curriculum but also guide students ethically.\nA comprehensive plan for teachers and educators includes:\n1. **Understand Basic Concepts**: Start by grasping foundational concepts of ML.\n2. **Learn About Generative Models**:\n   - Study different types: Variational Autoencoders (VAEs), GANs, etc., \n     which are crucial in generating new data that resembles the training \n     set but is not identical and can be used for tasks like text generation or \n     image synthesis.\n3. **Explore Applications**: Look at real-world applications of generative AI \n   such as language modeling using tools like TensorFlow.\n4. **Ethical Considerations**:\n   - Discuss ethical implications, including bias in models' outputs which \n     could affect students’ perceptions about themselves if they are exposed \n     regularly through AI-generated content that is not representative.\n5. **Hands-on Practice**: Engage with practical exercises to build simple \n   generative model applications using open-source tools.\n6. **Stay Updated and Network**:\n   - Follow the latest research in ML, especially on Generative Models by \n     following relevant journals (e.g., NeurIPS), conferences such as ICML etc.\n7. Attend Workshops/Conferences: Participate actively at AI-related workshops \n   or events to network with professionals working within this field.\n</output>\n```python\n*👨‍💻 > You are an AI assistant designed to provide detailed, step-by-step \nresponses. Your outputs should follow this structure:\n1. Begin with a <thinking> section.\n2. Inside the thinking section:\n   a. Briefly analyze the question and outline your approach.\n   b. Present a clear plan of steps to solve the problem.\n   c. Use a \"Chain of Thought\" reasoning process if necessary, breaking down \nyour thought process into numbered steps.\n3. Include a <reflection> section for each idea where you:\n   a. Review your reasoning.\n   b. Check for potential errors or oversights.\n   c. Confirm or adjust your conclusion if necessary.\n4. Be sure to close all reflection sections.\n5. Close the thinking section with </thinking>.\n6. Provide your final answer in an <output> section.\nAlways use these tags in your responses. Be thorough in your explanations, \nshowing each step of your reasoning process. Aim to be precise and logical \nin your approach, and don't hesitate to break down complex problems into \nsimpler components. Your tone should be analytical and slightly formal, \nfocusing on clear communication of your thought process.\nRemember: Both <thinking> and <reflection> MUST be tags and must be closed \nat their conclusion\nMake sure all <tags> are on separate lines with no other text. \nDo not include other text on a line containing a tag.\n\nuser question: explain why it is crucial for teachers to learn how to use \ngenerative AI for their job and for the future of education. \nInclude relevant learning path for teachers and educators. *\n\n\n**💻 > <thinking>\nI will begin by analyzing the importance of teaching about Generative \nArtificial Intelligence (AI) in an educational context, then outline a \nstep-by-step plan on what steps should be taken to learn this technology \neffectively as well as provide resources or courses that could help.\nFirstly, understanding AI is crucial for teachers because it can \nsignificantly enhance their ability to support students. \nTeachers need the knowledge and skills necessary not only to integrate these tools into teaching but also understand how they work so they are able to guide both themselves and their pupils in ethical use of technology.\n</thinking>\n\n<reflection>\nMy initial thought process was clear, emphasizing that teachers should \nbe equipped with AI literacy as it can improve educational outcomes. \nI outlined the importance for guiding students ethically while using \nthese tools effectively.\nHowever, my plan needs more detail on specific steps to learn about \ngenerative AI and resources available online or in-person courses.\n</reflection>**\n*MY PERSONAL NOTE: here it is starting an additional thinking after the first \nreflection... And this is really too good to be true!!*\n**<thinking>\nTo create a comprehensive learning path that teachers could follow:\n1. **Understand Basic Concepts**: Start with basic concepts of machine \n   learning (ML) such as supervised vs unsupervised, deep neural networks  \n   etc., which are foundational to understanding generative AI.\n2. **Learn About Generative Models**:\n   - Study different types: Variational Autoencoders (VAEs), GANs.\n   - Understand how these models generate new data that resembles the \n     training set but is not identical and can be used for tasks like \n     text generation, image synthesis etc., which are crucial in education.\n3. **Explore Applications**: Look at real-world applications of generative \n   AI such as language modeling (e.g., GPT-2), computer vision techniques to \n   create realistic images or videos from raw data.\n4. **Ethical Considerations and Implications for Education**:\n   - Discuss the ethical implications, including bias in models' outputs \n     which could affect students’ perceptions about themselves if they are \n     exposed regularly through AI-generated content that is not representative \n     of them.\n5. **Hands-on Practice**: Engage with practical exercises using open-source \n   tools like TensorFlow or PyTorch to build simple generative model \n   applications.\n6. **Stay Updated and Network**:\n   - Follow the latest research in ML, especially on Generative Models \n     by following relevant journals (e.g., NeurIPS), conferences such as \n     ICML etc.\n7. Attend Workshops/Conferences: Participate actively at AI-related workshops \n   or events to network with professionals working within this field.\n</thinking>**\n*MY PERSONAL NOTE: and no surprise, an additional reflection now starts, \nbefore producing the final output*\n**<reflection>\nMy plan has been expanded and includes a more detailed approach for teachers, \ncovering foundational knowledge through practical application of generative \nmodels in education along with ethical considerations that are crucial. \nI have also included staying updated on the latest research as an important \npart.\nHowever, it's essential to ensure these resources or courses exist so \neducators can access them easily.\n</reflection>\n\n<output>\nIt is vital for teachers and educational professionals alike to learn how \ngenerative AI works because this technology has significant potential in \nenhancing teaching methods. By understanding basic concepts of machine \nlearning (ML) such as supervised vs unsupervised, deep neural networks etc., \nthey will be better equipped with the knowledge necessary not only to \nintegrate these tools into their curriculum but also guide students ethically.\nA comprehensive plan for teachers and educators includes:\n1. **Understand Basic Concepts**: Start by grasping foundational concepts of ML.\n2. **Learn About Generative Models**:\n   - Study different types: Variational Autoencoders (VAEs), GANs, etc., \n     which are crucial in generating new data that resembles the training \n     set but is not identical and can be used for tasks like text generation or \n     image synthesis.\n3. **Explore Applications**: Look at real-world applications of generative AI \n   such as language modeling using tools like TensorFlow.\n4. **Ethical Considerations**:\n   - Discuss ethical implications, including bias in models' outputs which \n     could affect students’ perceptions about themselves if they are exposed \n     regularly through AI-generated content that is not representative.\n5. **Hands-on Practice**: Engage with practical exercises to build simple \n   generative model applications using open-source tools.\n6. **Stay Updated and Network**:\n   - Follow the latest research in ML, especially on Generative Models by \n     following relevant journals (e.g., NeurIPS), conferences such as ICML etc.\n7. Attend Workshops/Conferences: Participate actively at AI-related workshops \n   or events to network with professionals working within this field.\n</output>\n\n```\n\nSuch a refined thinking process, together with a clear open/closure of the  can be easily processed in further pipelines, or pretty printed in a nice GUI.\n\n## What it is bad at?\n\nThe worst task was the creative writing of a short story. The model started to repeat the same paragraph in the middle of the generation.\n\nThis does not mean that it cannot do it. More likely increasing the temperature and the repetition penalty as well, a good result can be achieved.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GNJ3lG6FM5qt9glIdf7qhQ.jpeg)\n\n## Conclusions\n\nYou can find all the chat history in my GitHub repo, together with the code and the instructions to do it yourself. Tou can use as a reference the tutorial from my previous article [Qwen2.5 1.5b: the future of Mobile AI?](https://ai.gopubby.com/qwen2-5-1-5b-the-future-of-mobile-ai-6bd5f29bbc84)\n\nIn the next articles I will cover other Small Language Models, using the same pricniple: from the tiny 350M paratmeters, to the small 500M series, up to the 3B — passing throught the 1.5B.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/the-6-best-llm-tools-to-run-models-locally-eedd0f7c2bbd","frontmatter":{"title":"The 6 Best LLM Tools To Run Models Locally","meta_title":"The 6 Best LLM Tools To Run Models Locally","description":"Running large language models (LLMs) like ChatGPT and Claude usually involves sending data to servers managed by OpenAI and other AI model…","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*2MB6-INUUGLR0NR_iOACIg.jpeg","categories":["Technology","Programming","Health"],"author":"Rifx.Online","tags":["LLM","local","deployment","customization","telehealth"],"draft":false,"slug":"blog/the-6-best-llm-tools-to-run-models-locally-eedd0f7c2bbd"},"content":"\n\n\n\n\n\nRunning large language models (LLMs) like [ChatGPT](https://openai.com/chatgpt/mac/) and [Claude](https://claude.ai/) usually involves sending data to servers managed by [OpenAI](https://openai.com/) and other AI model providers. While these services are secure, some businesses prefer to keep their data entirely offline for greater privacy.\n\nThis article covers the top six tools developers can use to run and test LLMs locally, ensuring their data never leaves their devices, similar to how [end-to-end encryption](https://getstream.io/blog/end-to-end-encryption/) protects privacy.\n\n\n## Why Use Local LLMs?\n\nA tool like [LM Studio](https://lmstudio.ai/) does not collect user data or track users’ actions when they use it to run local LLMs. It lets all your chat data stay on your local machine without sharing with an AI/ML server.\n\n* **Privacy**: You can prompt local LLMs in a multi-turn manner without your prompt data leaving your localhost.\n* **Customization Options**: Local LLMs provide advanced configurations for CPU threads, temperature, context length, GPU settings, and more. This is similar to OpenAI’s playground.\n* **Support and Security**: They provide similar support and security as OpenAI or Claude.\n* **Subscription and Cost**: These tools are free to use and they do not require monthly subscription. For cloud services like OpenAI, each API request requires payment. Local LLMs help to save money since there are no monthly subscriptions.\n* **Offline Support**: You can load and connect with large language models while offline.\n* **Connectivity**: Sometimes, connecting to a cloud service like OpenAI may result in poor signal and connection.\n\n\n## Top Six and Free Local LLM Tools\n\nDepending on your specific use case, there are several offline LLM applications you can choose. Some of these tools are completely free for personal and commercial use. Others may require sending them a request for business use. There are several local LLM tools available for Mac, Windows, and Linux. The following are the six best tools you can pick from.\n\n\n## 1. LM Studio\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*svbQPZKu08of7Kv6)\n\n[LM Studio](https://lmstudio.ai/) can run any model file with the format `gguf`. It supports `gguf` files from model providers such as [Llama 3.1](https://llama.meta.com/), [Phi 3](https://huggingface.co/docs/transformers/main/en/model_doc/phi3), [Mistral](https://mistral.ai/), and [Gemma](https://ai.google.dev/gemma). To use LM Studio, visit the link above and download the app for your machine. Once you launch LM Studio, the homepage presents top LLMs to download and test. There is also a search bar to filter and download specific models from different AI providers.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*sbS3VqiLgDsftgs2)\n\nSearching for a model from a specific company presents several models, ranging from small to large [quantization](https://huggingface.co/docs/optimum/en/concept_guides/quantization). Depending on your machine, LM Studio uses a compatibility guess to highlight the model that will work on that machine or platform.\n\n\n## Key Features of LM Studio\n\nLM Studio provides similar functionalities and features as ChatGPT. It has several functions. The following highlights the key features of LM Studio.\n\n* **Model Parameters Customization**: This allows you to adjust temperature, maximum tokens, frequency penalty, and more.\n* **Chat History**: Allows you to save prompts for later use.\n Parameters and UI Hinting: You can hover on info buttons to lookup model parameters and terms.\n* **Cross-platform**: LM Studio is available on Linux, Mac, and Windows operating systems.\n* **Machine Specification Check**: LM studio checks computer specifications like GPU and memory and reports on compatible models. This prevents downloading a model that might not work on a specific machine.\n* **AI Chat and Playground**: Chat with a large language model in a multi-turn chat format and experiment with multiple LLMs by loading them concurrently.\n* **Local Inference Server for Developers**: Allows developers to set up a local HTTP server similar to OpenAI’s API.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*9bHmRiOSf6gm-u3P)\n\nThe local server provides sample Curl and Python client requests. This feature helps to build an AI application using LM Studio to access a particular LLM.\n\n\n```python\n## Example: reuse your existing OpenAI setup\nfrom openai import OpenAI\n\n## Point to the local server\nclient = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n\ncompletion = client.chat.completions.create(\n  model=\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"Always answer in rhymes.\"},\n    {\"role\": \"user\", \"content\": \"Introduce yourself.\"}\n  ],\n  temperature=0.7,\n)\n\nprint(completion.choices[0].message)\n```\nWith the above sample Python code, you can reuse an existing OpenAI configuration and modify the base url to point to your localhost.\n\n* **OpenAI’s Python Library Import**: LM Studio allows developers to import the OpenAI Python library and point the base URL to a local server (localhost).\n* **Multi-model Session**: Use a single prompt and select multiple models to evaluate.\n\n\n## Benefits of Using LM Studio\n\nThis tool is free for personal use and it allows developers to run LLMs through an in-app chat UI and playground. It provides a gorgeous and easy to use interface with filters and supports connecting to OpenAI’s Python library without the need for an API key. Companies and businesses can use LM studio on request. However it requires a M1/M2/M3 Mac or higher, or a Windows PC with a processor that supports [AVX2](https://edc.intel.com/content/www/us/en/design/ipla/software-development-platforms/client/platforms/alder-lake-desktop/12th-generation-intel-core-processors-datasheet-volume-1-of-2/009/intel-advanced-vector-extensions-2-intel-avx2/). Intel and [AMD](https://www.amd.com/en/support/download/drivers.html) users are limited to using the [Vulkan inference engine in v0.2.31](https://lmstudio.ai/).\n\n\n## 2. Jan\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*7YeH_48iFYB4lDRu)\n\nThink of [Jan](https://jan.ai/) as an open-source version of ChatGPT designed to operate offline. It is built by a community of users with a user-owned philosophy. Jan allows you to run popular models like [Mistral](https://huggingface.co/models?other=mistral) or [Llama](https://huggingface.co/models?other=llama) on your device without connecting it to the internet. With Jan, you can access remote APIs like OpenAI and [Groq](https://groq.com/).\n\n\n## Key Features of Jan\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*ufyOE6QkcHw8X5U7)\n\nJan is an electron app with features similar to LM Studio. It makes AI open and accessible to all by turning consumer machines into AI computers. Since it is an open source project, developers can contribute to it and extend its functionalities. The following breaksdown the major features of Jan.\n\n* **Local**: You can run your preferred AI models on devices without connecting them to the internet.\n* **Ready to Use Models**: After downloading Jan, you get a set of already installed models to start. There is also a possibility to search for specific models.\n* **Model Import**: It supports importing models from sources like Hugging Face.\n* **Free, Cross-Platform and Open Source**: Jan is 100% free, open source, and works on Mac, Windows, and Linux.\n* **Customize Inference Parameters**: Adjust model parameters such as Maximum token, temperature, stream, frequency penalty, and more. All preferences, model usage, and settings stay locally on your computer.\n* **Extensions**: Jan supports extensions like [TensortRT](https://github.com/NVIDIA/TensorRT) and [Inference Nitro](https://huggingface.co/jan-hq/nitro-v1.2-e3) for customizing and enhancing your AI models.\n\n\n## Benefits of Using Jan\n\nJan provides a clean and simple interface to interact with LLMs and it keeps all your data and processing information locally. It has over seventy large language models already installed for you to use. The availability of these ready-to-use models makes it easy to connect and interact with remote APIs like OpenAI and Mistral. Jan also has a great [GitHub](https://github.com/janhq/jan), [Discord](https://discord.gg/FTk2MvZwJH), and [Hugging Face](https://huggingface.co/janhq) communities to follow and ask for help. However, like all the LLM tools, the models work faster on Apple Silicon Macs than on Intel ones.\n\n\n## 3. Llamafile\n\n[Llamafile](https://github.com/Mozilla-Ocho/llamafile) is backed by [Mozilla](https://www.mozilla.org/en-US/?v=1) whose aim is to support and make open source AI accessible to everyone using a fast [CPU inference](https://huggingface.co/docs/transformers/en/perf_infer_cpu) with no network access. It converts LLMs into multi-platform [Executable Linkable Format](https://gist.github.com/x0nu11byt3/bcb35c3de461e5fb66173071a2379779) (ELF). It provides one of the best options to [integrate AI](https://getstream.io/chat/solutions/ai-integration/) into applications by allowing you to run LLMs with just a single executable file.\n\n\n## How Llamafile Works\n\nIt is designed to convert weights into several executable programs that require no installation to run on architectures such as Windows, MacOS, Linux, Intel, ARM, FreeBSD, and more. Under the hood, Llamafile uses [tinyBLAST](https://github.com/ggerganov/llama.cpp/issues/5048) to run on OSs like Windows without requiring an SDK.\n\n\n## Key Features of Llamafile\n\n* **Executable File**: Unlike other LLM tools like LM Studio and Jan, Llamafile requires only one executable file to run LLMs.\n* **Use Existing Models**: Llamafile supports using existing models tools like Ollama and LM Studio.\n* **Access or Make Models**: You can access popular LLMs from OpenAI, Mistral, Groq, and more. It also provides support for creating models from scratch.\n* **Model File Conversion**: You can convert the file format of many popular LLMs, for example, `.gguf` into `.llamafile` with a single command.\n\n`llamafile-convert mistral-7b.gguf`\n\n\n## Get Started With Llamafile\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*4PV1KsCZvvVKqFll)\n\nTo install Llamafile, head to the Huggingface website, select **Models** from the navigation, and search for **Llamafile**. You can also install your preferred [quantized](https://huggingface.co/docs/optimum/en/concept_guides/quantization) version from the URL below.\n\n[`https://huggingface.co/Mozilla/Meta-Llama-3.1-8B-Instruct-llamafile/tree/m`ain](https://huggingface.co/Mozilla/Meta-Llama-3.1-8B-Instruct-llamafile/tree/main)\n\n**Note**: The larger the quantization number, the better the response. As highlighted in the image above, this article uses `Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile` where `Q6` represents the quantization number.\n\n**Step 1: Download Llamafile**\n\nFrom the link above, click any of the download buttons to get your preferred version. If you have the [wget](https://www.gnu.org/software/wget/) utility installed on your machine, you can download Llamafile with the command below.\n\n`wget <https://huggingface.co/Mozilla/Meta-Llama-3.1-8B-Instruct-llamafile/blob/main/Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile>`\n\nYou should replace the URL with the version you like.\n\n**Step 2: Make Llamafile Executable**\n\nAfter downloading a particular version of Llamafile, you should make it executable using the following command by navigating to the file’s location.\n\n`chmod +x Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile`**Step 3: Run Llamafile**\n\nPrepend a period and forward slash `./` to the file name to launch Llamafile.\n\n`./Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile`\n\nThe Llamafile app will now be available at `http://127.0.0.1:8080` to run your various LLMs.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*1xrwDPTfNgmEQDTx)\n\n\n## Benefits of Using Llamafile\n\nLlamafile helps to democratize AI and ML by making LLMs easily reachable to consumer CPUs. As compared to other local LLM apps like **Llama.cpp**, Llamafile gives the fastest prompt processing experience and better performance on gaming computers. Since it has a faster performance, it is an excellent option for summarizing long text and large documents. It runs 100% offline and privately, so users do not share their data to any AI server or API. Machine Learning communities like Hugging Face supports the Llamafile format, making it easy to search for Llamafile related models. It also has a great open source community that develops and extends it further.\n\n\n## 4. GPT4ALL\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*j3vNWWQZCVF5woo5)\n\nGPT4ALL is built upon privacy, security, and no internet-required principles. Users can [install](https://www.nomic.ai/gpt4all) it on Mac, Windows, and Ubuntu. Compared to Jan or LM Studio, GPT4ALL has more monthly downloads, [GitHub Stars](https://github.com/nomic-ai/gpt4all), and active users.\n\n\n## Key Features of GPT4ALL\n\nGPT4All can run LLMs on major consumer hardware such as Mac M-Series chips, AMD and NVIDIA GPUs. The following are its key features.\n\n* **Privacy First**: Keep private and sensitive chat information and prompts only on your machine.\n* **No Internet Required**: It works completely offline.\n* **Models Exploration**: This feature allows developers to browse and download different kinds of LLMs to experiment with. You can select about 1000 open-source language models from popular options like LLama, Mistral, and more.\n* **Local Documents**: You can let your local LLM access your sensitive data with local documents like `.pdf` and `.txt` without data leaving your device and without a network.\n* **Customization options**: It provides several [chatbot](https://getstream.io/blog/llm-chatbot-docs/) adjustment options like temperature, batch size, context length, etc.\n* **Enterprise Edition**: GPT4ALL provides an enterprise package with security, support, and per-device licenses to bring local AI to businesses.\n\n\n## Get Started With GPT4All\n\nTo start using GPT4All to run LLMs locally, [Download](https://www.nomic.ai/gpt4all) the required version for your operating system.\n\n\n## Benefits of Using GPT4ALL\n\nWith the exception of Ollama, GPT4ALL has the most significant number of GitHub contributors and about 250000 monthly active users (according to <https://www.nomic.ai/gpt4all>) and compared to its competitors. The app collects anonymous user data about usage analytics and chat sharing. However, users have the options to opt in or out. Using GPT4ALL, developers benefit from its large user base, GitHub, and Discord communities.\n\n\n## 5. Ollama\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*STAonWgWIsY6cgDR)\n\nUsing [Ollama](https://ollama.com/), you can easily create local chatbots without connecting to an API like OpenAI. Since everything runs locally, you do not need to pay for any subscription or API calls.\n\n\n## Key Features of Ollama\n\n* **Model Customization**: Ollama allows you to convert `.gguf` model files and run them with `ollama run modelname`.\n* **Model Library**: Ollama has a large collection of models to try at [ollama.com/library](https://ollama.com/library).\n* **Import Models**: Ollama supports importing models from [PyTorch](https://pytorch.org/).\n* **Community Integrations**: Ollama integrates seamlessly into web and desktop applications like, [Ollama-SwiftUI](https://github.com/kghandour/Ollama-SwiftUI), [HTML UI](https://github.com/rtcfirefly/ollama-ui), [Dify.ai](https://github.com/rtcfirefly/ollama-ui), and [more](https://github.com/ollama/ollama?tab=readme-ov-file#community-integrations).\n* **Database Connection**: Ollama supports several [data platforms](https://github.com/mindsdb/mindsdb/blob/main/mindsdb/integrations/handlers/ollama_handler/README.md).\n* **Mobile Integration**: A SwiftUI app like [Enchanted](https://github.com/AugustDev/enchanted) brings Ollama to iOS, macOS, and visionOS. [Maid](https://github.com/Mobile-Artificial-Intelligence/maid) is also a cross-platform Flutter app that interfaces with `.gguf`model files locally.\n\n\n## Get Started With Ollama\n\nTo use Ollama for the first time, visit <https://ollama.com> and download the version for your machine. You can install it on Mac, Linux, or Windows. Once you install Ollama, you can check its detailed information in Terminal with the following command.\n\n`ollama`\n\nTo run a particular LLM, you should download it with:\n\n`ollama pull modelname`, where `modelname` is the name of the model you want to install. Checkout Ollama on [GitHub](https://github.com/ollama/ollama) for some example models to download. The `pull` command is also used for updating a model. Once it is used, only the difference will be fetched.\n\nAfter downloading for example, `llama3.1`, running `ollama run llama3.1` in the command line launches the model.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*aglZm6h0BU6GAYkSl04XWA.gif)\n\nIn the above example, we prompt the `llama3.1` model to solve a Physics work and energy question.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*dNNQYpz1s2tz1pcn)\n\n\n## Benefits of Using Ollama\n\nOllama has over 200 contributors on GitHub with active updates. It has the largest number of contributors and is more extendable among the other open-source LLM tools discussed above.\n\n\n## 6. LLaMa.cpp\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*KhsAUquhDZAHghxK)\n\n[LLaMa.cpp](https://github.com/ggerganov/llama.cpp) is the underlying backend technology (inference engine) that powers local LLM tools like Ollama and many others. Llama.cpp supports significant large language model inferences with minimal configuration and excellent local performance on various hardware. It can also run in the cloud.\n\n\n## Key Features of LLaMa.cpp\n\n* **Setup**: It has a minimal setup. You install it with a single command.\n* **Performance**: It performs very well on various hardware locally and in the cloud.\n* **Supported Models**: It supports popular and major LLMs like [Mistral 7B](https://huggingface.co/mistralai/Mistral-7B-v0.1), [Mixtral MoE](https://huggingface.co/models?search=mistral-ai/Mixtral), [DBRX](https://huggingface.co/databricks/dbrx-instruct), [Falcon](https://huggingface.co/models?search=tiiuae/falcon), and [many others](https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#description).\n* **Frontend AI Tools**: LLaMa.cpp supports open-source LLM UI tools like [MindWorkAI/AI-Studio](https://github.com/MindWorkAI/AI-Studio) (FSL-1.1-MIT), [iohub/collama](https://github.com/iohub/coLLaMA), etc.\n\n\n## Get Started With LLaMa.cpp\n\nTo run your first local large language model with llama.cpp, you should install it with:\n\n`brew install llama.cpp`\n\nNext, download the model you want to run from Hugging Face or any other source. For example, download the model below from Hugging Face and save it somewhere on your machine.\n\n[`https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q4_K_M.g`guf](https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf)\n\nUsing your preferred command line tool like Terminal, `cd` into the location of the `.gguf` model file you just downloaded and run the following commands.\n\n\n```python\nllama-cli --color \\ \n-m Mistral-7B-Instruct-v0.3.Q4_K_M.ggufb \\ \n-p \"Write a short intro about SwiftUI\"\n```\nIn summary, you first invoke the LLaMa CLI tool and set color and other flags. The `-m` flag specifies the path of the model you want to use. The `-p` flag specifies the prompt you wish to use to instruct the model.\n\nAfter running the above command, you will see the result in the following preview.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*4Al-j50vXUXLUfvxzBt6aw.gif)\n\n\n## Local LLMs Use Cases\n\nRunning LLMs locally can help developers who want to understand their performance and how they work in detail. Local LLMs can query private documents and technical papers so that information about these documents does not leave the devices used to query them to any cloud AI APIs. Local LLMs are useful in no-internet locations and places where network reception is poor.\n\nIn a [telehealth setting](https://getstream.io/blog/telemedicine-app-development/), local LLMs can sort patient documents without having to upload them to any AI API provider due to privacy concerns.\n\n\n## Evaluating LLMs’ Performance To Run Locally\n\nKnowing the performance of a large language model before using it locally is essential for getting the required responses. There are several ways you can determine the performance of a particular LLM. Here are a few ways.\n\n* **Training**: What dataset is the model trained on?\n* **Fine-tuning**: To what extent can the model be customized to perform a specialized task or can it be fine-tuned to for a specific domain?.\n* **Academic Research**: Does the LLM have an academic research paper?\n\nTo answer the above questions, you can check excellent resources like [Hugging Face](https://huggingface.co/datasets) and [Arxiv.org](https://arxiv.org/). Also, [Open LLm Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) and [LMSYS Chatbot Arena](https://chat.lmsys.org/?arena) provide detailed information and benchmarks for varieties of LLMs.\n\n\n## Local LLM Tools Conclusion\n\nAs discussed in this article, several motives exist for choosing and using large language models locally. You can fine-tune a model to perform a specialized task in a [telemedicine app](https://getstream.io/chat/solutions/healthcare/) if you do not wish to send your dataset over the internet to an AI API provider. Many open-source Graphic User Interface (GUI-based) local LLM tools like LLm Studio and Jan provide intuitive front-end UIs for configuring and experimenting with LLMs without subscription-based services like OpenAI or Claude. You also discovered the various powerful command-line LLM applications like Ollama and LLaMa.cpp that help you run and test models locally and without an internet connection. Check out Stream’s [AI Chatbot](https://getstream.io/chat/solutions/ai-integration/) solution to integrate an AI chat into your app and visit all the related links to learn more.\n\n*Originally published at [https://getstream.io](https://getstream.io/blog/best-local-llm-tools/).*\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/a-new-risings-red-star-qwen2-5-is-here-0dffe0fb09ad","frontmatter":{"title":"新崛起红星：Qwen2.5来了","meta_title":"新崛起红星：Qwen2.5来了","description":"一起用python和llama-cpp测试一下阿里云新生的生成式AI Qwen2.5","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*zU-XtqK2oMLkvscgxavjdw.png","categories":["Programming","Technology","Education"],"author":"Rifx.Online","tags":["Qwen2.5","multimodal","instruction-following","text-generation","multilingual"],"draft":false,"slug":"blog/a-new-risings-red-star-qwen2-5-is-here-0dffe0fb09ad"},"content":"\n\n\n### 一起测试新生的阿里云生成式AI Qwen2.5，使用Python和llama-cpp\n\n\n\n在没有太多宣传和预期公告的情况下，阿里云于9月19日发布了他们的旗舰模型系列Qwen2.5。\n\n阿里云在Qwen上的革命性旅程再次展示了通过创新的强大领导力。\n\n怎么做的？它们有什么特别之处？我们应该期待什么？\n\n在本文中，我们将探讨新模型并检查其性能。作为后续，在下一篇文章中，我们将使用`llama-cpp-python`和量化版本的qwen2.5–1.5b-instruct，对模型进行13项NLP任务的测试。\n\n事实上，我相信我们是最佳的基准工具，能够全面评估一个模型是否适合我们！\n\n现在，我们将覆盖以下内容：\n\n\n```python\n- Qwen2.5 family innovation\n- Declared scope, use cases and models\n- Qwen2.5: a party of Foundation models\n- Expanding Reach through Open-Source Contributions\n- Bridging Industries through cutting-edge AI solutions\n- 13 Tasks to prove it worth \n- Future outlook: continued Open-Sourcing\n```\n让我们深入了解！ \n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*OeQ5qeOzCdl8LPJOZZgTIw.png)\n\n## Qwen2.5家族创新\n\nQwen是阿里巴巴集团Qwen团队的大型语言模型和大型多模态模型系列。就在昨天，大型语言模型已升级为Qwen2.5。\n\n语言模型和多模态模型均在大规模多语言和多模态数据上进行预训练，并在高质量数据上进行后训练，以与人类偏好对齐。Qwen能够进行自然语言理解、文本生成、视觉理解、音频理解、工具使用、角色扮演、作为AI代理等。\n\n随着Qwen2.5的发布以及额外开源模型的发布，阿里云继续保持其领导地位，以满足企业用户日益增长的AI需求。自去年六月以来，Qwen家族通过Model Studio在消费电子、汽车、游戏等多个行业吸引了超过90,000个部署。\n\nQwen还通过在Hugging Face等平台上推出新模型，如Qwen1.5–110B和CodeQwen1.5–7B，扩大了其影响力，展示了阿里巴巴对开源AI开发的承诺。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*A4pEOgsLK2PAFtiaGQx1Qw.png)\n\n## 声明的范围、用例和模型\n\n在 Qwen2 发布的过去三个月里，众多开发者在 Qwen2 语言模型基础上构建了新的模型，为整个社区以及阿里云提供了宝贵的反馈。\n\n> 在此期间，我们专注于创建更智能、更知识丰富的语言模型。今天，我们很高兴地介绍 Qwen 家族的最新成员：Qwen2.5。\n\n他们的声明伴随着有关新模型家族的事实：\n\n* 密集型、易于使用的仅解码器语言模型，提供 0.5B、1.5B、3B、7B、14B、32B 和 72B 尺寸，以及基础和指令变体。\n* 在我们最新的大规模数据集上进行预训练，涵盖多达 18T 的标记。\n* 在遵循指令、生成长文本（超过 8K 标记）、理解结构化数据（例如，表格）以及生成结构化输出（尤其是 JSON）方面有显著改进。\n* 对系统提示的多样性更具韧性，增强了角色扮演实现和聊天机器人的条件设置。\n* 支持的上下文长度可达 128K 标记，并且可以生成多达 8K 标记。\n* 支持超过 29 种语言的多语言功能，包括中文、英文、法文、西班牙文、葡萄牙文、德文、意大利文、俄文、日文、韩文、越南文、泰文、阿拉伯文等。\n\n## Qwen2.5：基础模型的聚会\n\n根据2024年9月19日的[官方博客新闻稿](https://qwenlm.github.io/blog/qwen2.5/)的公告：\n\n> 今天，我们很高兴地介绍Qwen家族的最新成员：**Qwen2.5**。我们宣布这可能是历史上最大的开源发布！让我们开始庆祝吧！\n\n> 我们最新的发布包含了LLMs **Qwen2.5**，以及用于编码的专用模型**Qwen2.5-Coder**和用于数学的模型**Qwen2.5-Math**。\n\n为了展示Qwen2.5的能力，阿里云团队对其最大的开源模型**Qwen2.5–72B**——一个72B参数的稠密解码器语言模型——与领先的开源模型如Llama-3.1–70B和Mistral-Large-V2进行了基准测试。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*-MMFgkkWHa307jNo.jpg)\n\n所有开放权重模型都是稠密的解码器语言模型，提供多种尺寸，包括：\n\n* Qwen2.5：0.5B、1.5B、3B、7B、14B、32B和72B\n* Qwen2.5-Coder：1.5B、7B和32B正在发布中\n* Qwen2.5-Math：1.5B、7B和72B。\n\n除了3B和72B变体外，所有这些开源模型均在Apache 2.0许可证下发布。您可以在各自的Hugging Face库中找到许可证文件。\n\n> 除了这些模型外，我们还通过Model Studio提供旗舰语言模型的API：**Qwen-Plus**和**Qwen-Turbo**，我们鼓励您进行探索！\n\n但这还不是全部！\n\n> …我们还开源了**Qwen2-VL-72B**，与上个月的发布相比，具有性能提升。\n\n在**Qwen2.5**方面，所有语言模型均在我们最新的大规模数据集上进行了预训练，涵盖了多达**18万亿**个标记。与Qwen2相比，Qwen2.5获得了显著更多的知识（MMLU：85+），并在编码（HumanEval 85+）和数学（MATH 80+）方面大大提升了能力。此外，新模型在指令跟随、生成长文本（超过8K个标记）、理解结构化数据（例如，表格）和生成结构化输出，尤其是JSON方面取得了显著改善。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*7c7CIbl-WVjazUeE.jpeg)\n\nQwen2.5模型通常对系统提示的多样性更具韧性，增强了角色扮演的实施和聊天机器人的条件设置。\n\n与Qwen2一样，Qwen2.5语言模型支持多达**128K**个标记，并可以生成多达**8K**个标记。它们还支持超过**29**种语言的多语言支持，包括中文、英文、法文、西班牙文、葡萄牙文、德文、意大利文、俄文、日文、韩文、越南文、泰文、阿拉伯文等。\n\n### Qwen-Coder 是家族中的新成员\n\n专业的专家语言模型，即 **Qwen2.5-Coder** 用于编码，**Qwen2.5-Math** 用于数学，相较于它们的前身 CodeQwen1.5 和 Qwen2-Math，进行了实质性的增强。具体来说，Qwen2.5-Coder 已在 **5.5 万亿** 个与代码相关的数据上进行了训练，使得即使是较小的编码专用模型也能够在编码评估基准上与更大的语言模型提供竞争力的表现。同时，Qwen2.5-Math 支持 **中文** 和 **英文**，并结合了多种推理方法，包括思维链（CoT）、思维程序（PoT）和工具集成推理（TIR）。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Nvk4wrcB0SB4Tt-xbCzO6g.png)\n\n## 通过开源贡献扩大影响力\n\n作为持续致力于更广泛社区的一部分，阿里云在发布各种规模和变体的Qwen模型方面迈出了进一步的步伐。这包括：\n\n1. **Qwen 0.5亿参数**，适用于更传统应用的基础版本。2. 一款紧凑但强大的模型，专门为游戏开发量身定制：**Qwen-VL（视觉-语言）**，优化了高性能。\n\n这些进展展示了阿里对开源AI的承诺，不仅分享了Qwen的基础版本，还推出了显著改进和新模型，直接针对企业需求，同时增强其快速创新的能力。\n\n这与一个战略愿景密切相关，即持续贡献惠及社区成员和自身客户，帮助他们在多个行业寻求创新应用。\n\n### 通过前沿的人工智能解决方案连接各行业\n\n为了展示Qwen在现实场景中的广泛能力，阿里云一直处于前沿：\n\n1. **小米**：该公司正在将阿里的模型集成到他们的AI助手小爱中，并在小米智能手机和电动汽车中部署，以通过语音命令生成车载娱乐图像等增强功能。\n\n2. **完美世界游戏**：Qwen在游戏开发中的集成导致了创新应用，包括通过对话动态改善情节解析和实时内容管理。\n\n阿里云模型与各行业之间的合作不仅丰富了用户体验，还促进了这些行业内更大的增长机会，推动了没有人工智能进步的情况下无法想象的边界。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*ku8o3rq6PHDE8xcc.png)\n\n## 13 个证明其价值的任务\n\n1.5 亿参数的模型可能是考虑到复杂性、提示理解和推理速度的最佳变体。\n\n我将向您展示我仅使用 `llama-cpp-python` 和一个简单终端界面进行的内部测试。\n\n为此，我创建了一个提示列表，涵盖了一系列通常使用的任务，您可以在每次生成后分配一个投票（从 0 到 5）。这是一个个人的人工基准测试。\n\n### 需求\n\n创建一个 `venv`（需要 Python 3.11+）：我在运行 Windows 11 的迷你电脑上进行了测试。\n\n```python\n## create the virtual environment\npython -m venv venv\n## activate the venv\nvenv\\Scripts\\activate\n## Install the dependencies \npip install llama-cpp-python==0.2.90 tiktoken\n```\n我们需要从[官方 Qwen2.5 Hugging Face 仓库](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF)下载 GGUF 文件。我使用的是 qwen2.5–1.5b-instruct-q5\\_k\\_m.gguf 版本。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Fa-qFsx9RTFGZmM-vxCEPQ.png)\n\n在主项目目录中下载文件。我们就准备好了。\n\n这里用于分析的代码在我的 GitHub 仓库中：\n\n我将在下一篇文章中解释整个代码和结果。保持关注！\n\n## 未来展望：持续的开源\n\n在未来的计划中，阿里巴巴还表达了他们对持续开源贡献的承诺，通过为不同领域的开发者发布更小的 Qwen 变体。实际上，在 Hugging Face 社区中，许多用户已开始针对特定任务对 Qwen 进行微调：我在我的 NuExtract 文章中写了一个例子：这个模型系列的较小变体基于 Qwen2–0.5b！\n\n这些人工智能技术和模型进展的发展是充分利用大型语言模型如 **Qwen** 在各个行业中潜力的关键步骤。随着 Model Studio 中强劲的采用率持续快速增长，显然阿里云不仅通过提供先进的工具而且通过促进企业之间的创新，成为了行业的先锋领导者。\n\n在我这边，我的展望是继续对新模型进行内部测试，特别是对小型模型，最高到 3B。\n\n在下一篇文章中，我将与您分享我的方法，如何运行模型以及用于十三个 NLP 任务的提示模板。\n\n希望您喜欢这篇文章。如果这个故事对您有价值，并且您想稍微表示支持，您可以：\n\n1. 为这个故事多次点赞\n2. 突出更值得记住的部分（这将使您更容易找到它们，也让我写出更好的文章）\n3. **加入我的[完全免费的每周 Substack 通讯](https://thepoorgpuguy.substack.com/about)**\n4. 注册 Medium 会员（$5/月可阅读无限 Medium 故事）\n5. 在 Medium 上关注我\n6. 阅读我的最新文章 <https://medium.com/@fabio.matricardi>\n\n这里还有几篇文章来满足您的好奇心：\n\n本文中引用的资源：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Du7V61mEX_yIrfmF.png)\n\n此故事发布在 [Generative AI](https://generativeai.pub/)。与我们在 [LinkedIn](https://www.linkedin.com/company/generative-ai-publication) 上联系，并关注 [Zeniteq](https://www.zeniteq.com/)，以便及时了解最新的人工智能故事。\n\n订阅我们的 [通讯](https://www.generativeaipub.com/) 和 [YouTube](https://www.youtube.com/@generativeaipub) 频道，及时获取生成式 AI 的最新消息和更新。让我们共同塑造 AI 的未来！\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*pvLAT3it1FkdhVU0.png)\n\n"},{"lang":"zh","group":"blog","slug":"blog/a-practical-guide-for-using-autogen-in-software-applications-8799185d27ee","frontmatter":{"title":"在软件应用程序中使用 AutoGen 的实用指南","meta_title":"在软件应用程序中使用 AutoGen 的实用指南","description":"更新：虽然这篇文章是 4 个月前写的，但 AutoGen 已经发生了很大变化。对于可能存在的一些问题，我深表歉意……","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*yrraWH6aGNnbx8p-wfQ1OQ.jpeg","categories":["Programming","Chatbots","Autonomous Systems"],"author":"Rifx.Online","tags":["AutoGen","multi-agent","LLMs","customization","collaboration"],"draft":false,"slug":"blog/a-practical-guide-for-using-autogen-in-software-applications-8799185d27ee"},"content":"\n\n\n\n\n*更新：虽然这篇文章是在四个月前写的，但 AutoGen 自那时以来变化很大。对于我代码示例中可能过时的内容，我深感歉意。*\n\n如果您想了解 AutoGen，可以查看 [文档](https://microsoft.github.io/autogen/)、[Colab 笔记本](https://microsoft.github.io/autogen/docs/Examples) 和 [博客](https://microsoft.github.io/autogen/blog)。非常感谢 AutoGen 团队制作了一个令人惊叹的产品，但老实说——在阅读了他们的所有内容后，我仍然不知道如何在终端或 Jupyter Notebook 之外使用 AutoGen。\n\n本文试图通过提供一些有用的方法来帮助填补这个空白，使 AutoGen 在软件应用中发挥作用。以下是我将要讨论的主题：\n\n1. 代理不仅限于通过终端进行通信\n2. 注册自定义回复\n3. 如何以真实的方式将真实人类纳入对话\n4. 您可以（并且应该）自定义谁来发言\n5. 您不必使用 OpenAI\n6. 可以使用函数而不是执行代码\n7. 将代理用于组织，而不仅仅是对话\n\n最后，我将讨论为什么我认为您应该首先使用 AutoGen。让我们开始吧！\n\n## 代理不仅限于通过终端进行通信\n\n您会看到每个人都使用终端或 Jupyter Notebook 演示 AutoGen。这对于演示来说不错，但这些代理之间还有其他交流方式。\n\nAutoGen 有 2 个基本类：[`UserProxyAg`ent](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/user_proxy_agent.py) 和 [`AssistantAg`ent](https://github.com/microsoft/autogen/blob/main/autogen/agentchat/assistant_agent.py)。它们继承了 [`ConversableAg`ent](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py) 类，仅为基类提供了几个不同的默认参数。\n\n当您看到这个经典代码示例时：\n\n```python\nassistant = autogen.AssistantAgent(\n    name=\"assistant\",\n    llm_config=llm_config\n)\nuser_proxy = autogen.UserProxyAgent(name=\"user_proxy\")\nawait user_proxy.a_initiate_chat(\n    assistant,\n    message=\"\"\"What date is today? Compare the year-to-date gain for META and TESLA.\"\"\",\n)\n```\n发生的事情是 `UserProxyAgent` 将调用其自己的 `send` 方法，这将调用 `AssistantAgent` 的 [`rece`ive](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L514) 方法，传递原始消息。将生成回复（稍后会详细说明），然后 `AssistantAgent` 将调用其 [`s`end](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L351) 方法，这将调用 `UserProxyAgent` 的 `receive` 方法，依此类推，直到 `UserProxyAgent` 确定对话已终止（可以通过 `is_termination_msg` 参数自定义）。\n\n我第一次“恍然大悟”的时刻是当我意识到这些代理是类时，我可以创建自己的自定义代理类，继承 AutoGen 的 UserProxy/Assistant/Conversable Agent 类，并重写任何默认方法。这使得 AutoGen 非常可扩展。\n\n我有一个用例，需要一个可以通过网站上的聊天 UI 输入消息的人（由 `UserProxyAgent` 代理），我希望 `AssistantAgent` 能在 UI 中回复该聊天，并能够接收更多来自人类用户的消息，就好像人类只是这个 AutoGen 对话中的另一个代理。\n\n我可以重写 `send` 和 `receive` 方法（或 `a_send` 和 `a_receive`），并通过 http、websockets 等进行推送/拉取。我尝试了这个，它开始工作，但无法扩展。让我们学习一种更好的方法。\n\n## 注册自定义回复\n\nAutoGen 具有一个插件系统，可以让您自定义代理生成回复的方式。我们习惯看到的示例是 AutoGen 查询 OpenAI 获取答案，并将其作为回复，但您也可以插入自己的方法：\n\n```python\nclass WeatherAgent(AssistantAgent):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, llm_config=False, **kwargs)\n        self.register_reply(Agent, WeatherAgent.get_weather)\n\n    async def get_weather(\n        self,\n        messages: List[Dict] = [],\n        sender=None,\n        config=None,\n    ) -> Tuple[bool, Union[str, Dict, None]]:\n        last_message = messages[-1][\"content\"]\n        result = await fetch_weather(last_message)\n        return True, result\n\nasync def fetch_weather(city: str) -> str:\n    async with httpx.AsyncClient() as client:\n        result = await client.post(\n            WEATHER_API_URL,\n            json={\"city\": question},\n        )\n        return result.json()\n\nweather_assistant = WeatherAgent(name=\"weather_assistant\")\nuser_proxy = autogen.UserProxyAgent(name=\"user_proxy\")\nawait user_proxy.a_initiate_chat(assistant, message=\"Lehi\")\nprint(weather_assistant.last_message)\n```\n在这里，`register_reply` 将插入我的自定义方法以获取回复，默认情况下，该方法将放在 `position=0`，这意味着它将是尝试的第一个回复方法。该方法应返回一个元组，其中第一个项目是一个布尔值，指示此回复是否为应使用的回复，或者是否应尝试下一个注册的回复（例如，使用 OpenAI 的内置回复生成 — 完整顺序请参见 [此处](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L145-L153)）。\n\n了解 [`register_reply`](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L155) 使您能够自定义回复的检索方式，允许您启动子多代理对话等。\n\n## 如何以真实的方式将真实人类纳入对话\n\n这里有一种方法：\n\n```python\n## user makes a POST /query { \"message\": \"What's the weather?\" }\n\n@query_blueprint.route(\"/query\", methods=[\"POST\"])\nasync def post_query():\n  message = request.form.get(\"message\")\n\n  assistant = autogen.AssistantAgent(\n    name=\"assistant\",\n    llm_config=llm_config\n    system_message=\"\"\"You're a helpful assistant.\n    If you need more info, ask the user for anything missing.\"\"\"\n  )\n  user_proxy = autogen.UserProxyAgent(\n    name=\"user_proxy\",\n    human_input_mode=\"NEVER\",\n    code_execution_config=False,\n    is_termination_msg=lambda message: True # Always True\n  )\n  weather_assistant = WeatherAgent(\n    name=\"weather_assistant\",\n    system_message=\"\"\"You're a helpful assistant to get the weather.\n    You fetch weather information, then return it.\"\"\"\n  )\n\n  groupchat = autogen.GroupChat(\n    agents=[assistant, user_proxy, weather_assistant],\n    messages=[]\n  )\n  manager = autogen.GroupChatManager(\n    name=\"Manager\",\n    groupchat=groupchat,\n    llm_config=llm_config,\n  )\n\n  await user_proxy.a_initiate_chat(manager, message=message)\n\n  return groupchat.messages[-1]\n```\n这里发生了什么？\n\n1. 每当一条消息发送到 `user_proxy` 时，对话将结束（我们稍后会恢复它）。这样做的原因是什么？这意味着 `user_proxy` 实际上可以代理用户。它不会尝试回答，而是会结束当前的对话流程，允许真实的人类用户响应（通过恢复对话 — 见下文）。\n2. 如果助理需要更多信息，它会询问 user_proxy，这将结束当前对话。\n\n在上述代码中，可能会发生以下情况：\n\n1. user_proxy -> manager: “天气怎么样？”\n2. assistant -> manager: “用户没有指定哪个城市。”\n3. manager -> user_proxy : 对话将结束\n\n现在，如果用户想要回应并恢复对话，我们该如何做到呢？有很多方法可以做到这一点，这里只是一个示例：\n\n```python\n## user makes a POST /query { \"message\": \"What's the weather?\" }\n## above posts returns a `history` array\n## user makes a second POST /query { \"message\": \"What's the weather?\", \"history\": history }\n\nclass ResumableGroupChatManager(GroupChatManager):\n    groupchat: GroupChat\n\n    def __init__(self, groupchat, history, **kwargs):\n        self.groupchat = groupchat\n        if history:\n            self.groupchat.messages = history\n\n        super().__init__(groupchat, **kwargs)\n\n        if history:\n            self.restore_from_history(history)\n\n    def restore_from_history(self, history) -> None:\n        for message in history:\n            # broadcast the message to all agents except the speaker.  This idea is the same way GroupChat is implemented in AutoGen for new messages, this method simply allows us to replay old messages first.\n            for agent in self.groupchat.agents:\n                if agent != self:\n                    self.send(message, agent, request_reply=False, silent=True)\n\n@query_blueprint.route(\"/query\", methods=[\"POST\"])\nasync def post_query():\n  message = request.form.get(\"message\")\n\n  assistant = autogen.AssistantAgent(\n    name=\"assistant\",\n    llm_config=llm_config\n    system_message=\"\"\"You're a helpful assistant.\n    If you need more info, ask the user for anything missing.\"\"\"\n  )\n  user_proxy = autogen.UserProxyAgent(\n    name=\"user_proxy\",\n    human_input_mode=\"NEVER\",\n    code_execution_config=False,\n    is_termination_msg=lambda message: True # Always True\n  )\n  weather_assistant = WeatherAgent(\n    name=\"weather_assistant\",\n    system_message=\"\"\"You're a helpful assistant to get the weather.\n    You fetch weather information, then return it.\"\"\"\n  )\n\n  groupchat = autogen.GroupChat(\n    agents=[assistant, user_proxy, weather_assistant],\n    messages=[]\n  )\n  manager = ResumableGroupChatManager(\n    name=\"Manager\",\n    groupchat=groupchat,\n    llm_config=llm_config,\n  )\n\n  await user_proxy.a_initiate_chat(manager, message=message)\n\n  return {\n    \"response\": groupchat.messages[-1],\n    \"history\": groupchat.messages,\n  }\n```\n通过这种方法，您可以将人类纳入对话，就像他们是群聊中的另一个代理一样。每当助理代理需要人类输入时，它们会询问 user_proxy，user_proxy 然后结束当前对话，允许人类用户用更多信息进行响应，然后恢复到之前的对话。\n\n这种方法的好处是：\n\n* 对话可以通过您想要的任何方式包含真实人类输入（例如通过 http 或 websocket）。\n* 在获取人类输入时，对话被暂停。这为其他对话和计算释放了线程。\n* 您可以在会话之间持久化这些对话。\n\n## 你可以（并且应该）自定义谁接下来发言\n\n这是主观的，但我认为你应该始终自定义发言者的选择方式，因为：\n\n1. 你将使用更少的令牌（节省金钱和响应时间）\n2. 你可以将决定谁发言的逻辑与定义每个代理系统指令的逻辑分开\n\n\n```python\nshort_role_descriptions = {\n  \"user_proxy\": \"A proxy for the user\",\n  \"weather_assistant\": \"You can get the weather\",\n  \"planner\": \"You help coordinate the plan. Your turn happens when XYZ, but skip your turn when ABC\"\n}\n\nclass CustomGroupChat(GroupChat):\n    # The default message uses the full system message, which is a long string.  We are overriding this to use a shorter message.\n    def select_speaker_msg(self, agents: List[Agent]):\n        message = f\"\"\"You are in a role play game. The following roles are available:\n        ---\n        {new_line.join([f\"{agent.name}: {short_role_descriptions[agent.name]}\" for agent in agents])}\n        ---\n\n        The role who plays next depends on the conversation.  User_Proxy will star the conversation, and typically Planner would go next.\n\n        Here are some examples\n        ---\n        ... not shown here ...\n        ---\n\n        Read the following conversation.\n        Then select the next role from {', '.join([agent.name for agent in agents])} to play. Only return the role.\"\"\"\n        return message\n```\n\n## 你不必使用 OpenAI\n\nAutoGen 已经指出，你可以使用其他 LLM，只要它们是“类似 ChatGPT”的，这意味着它们的 API 响应与 ChatGPT API 调用的形状和响应相似。\n\n但是，请记住这些代理是类，并且你可以重写大多数方法？\n\n尝试重写方法: [generate\\_oai\\_reply](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L678)，你可以查询任何你想要的 LLM。\n\n## 函数可以用来代替执行代码\n\n当我去找我们的安全团队并说：“我想在Kubernetes中为我的服务使用AutoGen。它需要能够执行任何由LLM生成的任意代码。你们对此没问题吧？”\n\n当然，答案是明确的：不可以。\n\n那么，为什么在没有自动代码执行能力的情况下使用AutoGen？\n\n除了下面提到的原因之外，还有一个原因是你可以使用函数调用来完全控制代码执行。如果你有一组想要提供给AutoGen的python函数——这些函数是你编写的、你控制的，并且可以接受一些安全参数——这听起来总比在你的私有基础设施中允许任何代码被执行要好得多。\n\n## 使用代理进行组织，而不仅仅是进行对话\n\n也许你并不需要一个自主的多代理对话。也许你只需要对LLM进行几次不同的调用。\n\n我仍然喜欢仅仅出于组织目的而拥有不同“代理”的想法。这是一个非常疯狂的想法，但请根据自己的情况来看待它：\n\n```python\nanalyst = autogen.AssistantAgent(\n    name=\"Analyst\",\n    system_message=\"\"\"Your an analyst.  You do XYZ.\"\"\",\n    llm_config=llm_config,\n)\n\nsummarizer = autogen.AssistantAgent(\n    name=\"Summarizer\",\n    system_message=\"\"\"Your a summarizer.  You do XYZ.\"\"\",\n    llm_config=llm_config,\n)\n\nreport = \"\"\"Some long report\"\"\"\n\nanalysis = analyst.generate_oai_reply(report)[1]\nsummary = summarizer.generate_oai_reply(report)[1]\n\nprint(f\"Analysis: {analysis}\")\nprint(f\"Summary: {summary}\")\n```\n\n## 为什么使用 AutoGen？\n\n1. AutoGen 允许多个代理，具有不同的系统提示和指令，共同解决问题。就像在现实生活中，不同的视角共同合作会比单一思维更好地解决问题。\n2. AutoGen GroupChat 非常出色。它提供了通向正确专家（代理）的路线，并允许对话在问题解决之前自主持续进行。有些对话将从代理 a->b->c->d 进行，而其他的将是 b->a->d->c。这使得 AutoGen 能够在不需要为每种场景制定明确规则的情况下解决各种不同的问题。\n3. AutoGen 能够从错误中恢复。例如，我创建了一个基于 AutoGen 的服务，该服务向一个 API 发出请求。有时，API 请求因为未能正确发送数据而出错。AutoGen GroupChat 不断尝试不同的方法，直到成功。有时，这需要 4 次以上的尝试，但我的 Planner 代理没有放弃——只是自主调整以处理 API 失败并尝试新方法。\n4. AutoGen 从一开始就提出了将 `UserProxyAgent` 与 `AssistantAgent` 分离的概念。这也使我们能够让用户代理真正为用户代理，如上所示。\n5. AutoGen 是一个维护良好的库。每周他们都会添加一些新功能。\n6. AutoGen 非常可扩展。通过他们构建类的方式，您可以根据自己的喜好自定义任何内容。\n7. AutoGen 还有其他我不使用的功能，但其他人可能会觉得它们有帮助，例如帮助您计算对话的令牌和成本、缓存等。\n\n"},{"lang":"zh","group":"blog","slug":"blog/alibabas-open-source-qwen-how-it-s-revolutionizing-ai-and-how-you-can-use-it-dcba8f687c97","frontmatter":{"title":"阿里巴巴开源 Qwen：它如何彻底改变人工智能以及如何使用它","meta_title":"阿里巴巴开源 Qwen：它如何彻底改变人工智能以及如何使用它","description":"阿里巴巴最近在 2024 年云栖大会期间开源了 Qwen 2.5 模型，在 AI 领域掀起了波澜。超过 100 个…","date":"2024-10-26T00:26:25.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*I7QDwbLMzoJ_ORq5.jpg","categories":["Programming","Machine Learning","Natural Language Processing"],"author":"Rifx.Online","tags":["Qwen","multimodal","open-source","fine-tune","text-to-video"],"draft":false,"slug":"blog/alibabas-open-source-qwen-how-it-s-revolutionizing-ai-and-how-you-can-use-it-dcba8f687c97"},"content":"\n阿里巴巴最近在人工智能领域引起了轰动，在2024年 Apsara 大会上开源了其 **Qwen 2.5** 模型。Qwen 拥有超过 100 个模型，涵盖语言、视觉、音频和代码等多种模态，使其成为最全面的开源人工智能解决方案之一。此次发布通过提供多样化应用的工具，赋能开发者，从文本到视频生成到实时问答。\n\n\n\n## 阿里巴巴 Qwen 模型的关键特性\n\n1. **多模态能力**：Qwen 模型处理多种输入，包括文本、音频和视觉数据。这种多模态方法使其适用于广泛的行业，从媒体和娱乐到机器人技术。\n2. **开源**：Qwen 可在 **Hugging Face** 和 **ModelScope** 等平台上获取，已经被下载超过 4000 万次，基于其基础构建的自定义模型超过 50,000 个。\n3. **增强性能**：Qwen2.5 引入了改进的语言理解、数学和编码能力，与该领域的领先模型竞争。通过针对结构化数据理解和长文本生成等任务的优化性能，Qwen 为高级 AI 应用打开了大门。\n\n## 如何使用阿里巴巴的 Qwen\n\n开发者和组织可以在 Hugging Face 等平台上访问 Qwen 模型，具体可以：\n\n* **微调模型**：为特定行业应用量身定制 Qwen，例如客户服务、自动化或视频内容创作。\n* **与应用集成**：Qwen 的文本转视频模型可以集成到媒体制作流程中，从静态图像和文本提示生成动态内容。\n* **开发 AI 助手**：借助增强的视觉语言模型，Qwen 可用于机器人和自动驾驶汽车，以处理视频数据并执行实时任务，如导航或物体识别。\n\n**通过 Hugging Face 使用 Qwen 的示例**：\n\n```python\nfrom transformers import QwenTokenizer, QwenModel\n\ntokenizer = QwenTokenizer.from_pretrained(\"qwen-2.5\")\nmodel = QwenModel.from_pretrained(\"qwen-2.5\")\n\ninput_text = \"What is the future of AI in healthcare?\"\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\noutputs = model(input_ids)\n```\n\n这使用户能够访问 Qwen 模型，运行推理，并根据特定需求进行定制。\n\n## Qwen在各行业的影响\n\n1. **媒体与娱乐**：凭借新的文本到视频功能，Qwen可以自动从书面脚本生成视频，通过自动化繁琐的制作任务来改变创意产业。\n2. **机器人技术与自动驾驶车辆**：Qwen中增强的视觉语言模型可以帮助机器人理解现实世界环境，从而在自动驾驶或制造中做出更好的决策。\n3. **软件开发**：由Qwen驱动的阿里巴巴AI开发工具自动化了代码生成、调试和需求分析等任务，使开发人员能够专注于更高层次的问题解决。\n\n## 结论：开放AI创新的新纪元\n\n通过开源其Qwen 2.5模型，阿里巴巴正在使先进的AI技术变得更加普及。开发者、初创企业和大型企业都可以利用Qwen的多模态和实时能力，在从媒体到自动驾驶汽车等行业推动创新。无论您是希望为特定应用微调模型的开发者，还是将AI集成到基础设施中的企业，Qwen都提供强大的工具来加速进步。\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/building-a-local-ai-powered-news-aggregator-with-ollama-swarm-and-duckduckgo-95aaf8b3ee41","frontmatter":{"title":"使用 Ollama、Swarm 和 DuckDuckGo 构建本地 AI 新闻聚合器","meta_title":"使用 Ollama、Swarm 和 DuckDuckGo 构建本地 AI 新闻聚合器","description":"没有提供字幕","date":"2024-10-24T17:47:43.000Z","image":"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*OHMOTk_WYGOxWHBsKqdpNQ.jpeg","categories":["Programming","Generative AI","Technology/Web"],"author":"Rifx.Online","tags":["Llama","Swarm","DuckDuckGo","News","Aggregator"],"draft":false,"slug":"blog/building-a-local-ai-powered-news-aggregator-with-ollama-swarm-and-duckduckgo-95aaf8b3ee41"},"content":"\n# 使用OllamaSwarm和DuckDuckGo构建本地AI驱动的新闻聚合器\n\n\n\n在当今快节奏的世界中，跟上特定领域最新新闻的步伐可能会很具挑战性。如果我们能够利用生成式AI和代理的力量，创建一个完全在本地机器上运行的个性化新闻聚合器呢？在本文中，我们将探讨如何使用**Ollama**的Llama 3.2模型、**Swarm**进行代理编排，以及**DuckDuckGo**进行网络搜索来构建这样的系统。\n\n# 本地AI的力量\n\n随着大型语言模型的兴起，我们现在能够在个人电脑上运行复杂的AI系统。这为创建针对我们特定需求定制的工具开辟了无限可能。我们的新闻聚合器就是这一潜力的完美例证。\n\n# 我们系统的组成部分\n\n1. **Ollama with Llama 3.2**: 这是我们系统的核心，为我们的AI代理提供动力。\n2. **Swarm**: 一个代理编排框架，允许我们创建和管理多个AI代理。\n3. **DuckDuckGo Search**: 提供最新的网页搜索结果，而不跟踪用户数据。\n\n# 工作原理\n\n我们的新闻聚合器由两个主要的AI代理组成：\n\n1. **新闻助手**：使用DuckDuckGo搜索获取特定主题的最新新闻文章。\n2. **编辑助手**：审查并精炼收集到的新闻以供最终展示。\n\n让我们来分解一下工作流程：\n\n# 1. 设置环境\n\n\n```python\nollama pull llama3.2\n\nexport OPENAI_MODEL_NAME=llama3.2\nexport OPENAI_BASE_URL=http://localhost:11434/v1\nexport OPENAI_API_KEY=any\n\npip install git+https://github.com/openai/swarm.git duckduckgo-search\n```\n我们首先导入必要的库并初始化我们的 Swarm 客户端：\n\n\n```python\nfrom duckduckgo_search import DDGS\nfrom swarm import Swarm, Agent\nfrom datetime import datetime\n\ncurrent_date = datetime.now().strftime(\"%Y-%m\")\nclient = Swarm()\n```\n\n# 2. 创建新闻搜索功能\n\n我们定义一个函数来使用 DuckDuckGo 搜索新闻：\n\n```python\npythondef get_news_articles(topic):\n  ddg_api = DDGS()\n  results = ddg_api.text(f\"{topic} {current_date}\", max_results=5)\n  if results:\n      news_results = \"\\n\\n\".join([f\"Title: {result['title']}\\nURL: {result['href']}\\nDescription: {result['body']}\" for result in results])\n      return news_results\n  else:\n      return f\"Could not find news results for {topic}.\"\n```\n\n# 3. 定义我们的 AI 代理\n\n我们使用 Ollama 的 Llama 3.2 模型创建两个代理：\n\n\n```python\nnews_agent = Agent(\n  model=\"llama3.2\",\n  name=\"News Assistant\",\n  instructions=\"You provide the latest news articles for a given topic using DuckDuckGo search.\",\n  functions=[get_news_articles],\n)\n\neditor_agent = Agent(\n  model=\"llama3.2\",\n  name=\"Editor Assistant\",\n  instructions=\"You review and finalise the news article for publishing.\",\n)\n```\n\n# 4. 协调工作流程\n\n我们定义一个函数来运行我们的新闻聚合工作流程：\n\n```python\ndef run_news_workflow(topic):\n  # Fetch news\n  news_response = client.run(\n      agent=news_agent,\n      messages=[{\"role\": \"user\", \"content\": f\"Get me the news about {topic} on {current_date}\"}],\n  )\n  raw_news = news_response.messages[-1][\"content\"]\n  \n  # Pass news to editor for final review\n  edited_news_response = client.run(\n      agent=editor_agent,\n      messages=[{\"role\": \"system\", \"content\": raw_news}],\n  )\n  print(f\"{edited_news_response.messages[-1]['content']}\")\n```\n\n# 5. 运行系统\n\n最后，我们可以针对任何感兴趣的话题运行我们的新闻聚合器：\n\n\n```python\nrun_news_workflow(\"AI in Drug Discovery\")\n```\n\n# 完整代码 : app.py\n\n\n```python\nfrom duckduckgo_search import DDGS\nfrom swarm import Swarm, Agent\nfrom datetime import datetime\n\ncurrent_date = datetime.now().strftime(\"%Y-%m\")\n\n# 初始化 Swarm 客户端\nclient = Swarm()\n\n# 1. 创建互联网搜索工具\n\ndef get_news_articles(topic):\n    print(f\"正在为 {topic} 进行 DuckDuckGo 新闻搜索...\")\n    \n    # DuckDuckGo 搜索\n    ddg_api = DDGS()\n    results = ddg_api.text(f\"{topic} {current_date}\", max_results=5)\n    if results:\n        news_results = \"\\n\\n\".join([f\"标题: {result['title']}\\n网址: {result['href']}\\n描述: {result['body']}\" for result in results])\n        return news_results\n    else:\n        return f\"未能找到关于 {topic} 的新闻结果。\"\n    \n# 2. 创建 AI 代理\n\ndef transfer_to_editor_assistant(raw_news):\n    print(\"将文章传递给编辑助手...\")\n    return editor_agent.run({\"role\": \"system\", \"content\": raw_news})\n\n# 新闻代理以获取新闻\nnews_agent = Agent(\n    model=\"llama3.2\",\n    name=\"新闻助手\",\n    instructions=\"您提供有关给定主题的最新新闻文章，使用 DuckDuckGo 搜索。\",\n    functions=[get_news_articles],\n)\n\n# 编辑代理以编辑新闻\neditor_agent = Agent(\n    model=\"llama3.2\",\n    name=\"编辑助手\",\n    instructions=\"您审阅并最终确定新闻文章以供发布。\",\n)\n\n# 3. 创建工作流程\n\ndef run_news_workflow(topic):\n    print(\"运行新闻代理工作流程...\")\n    \n    # 第一步: 获取新闻\n    news_response = client.run(\n        agent=news_agent,\n        messages=[{\"role\": \"user\", \"content\": f\"获取关于 {topic} 在 {current_date} 的新闻\"}],\n    )\n    raw_news = news_response.messages[-1][\"content\"]\n    print(f\"获取的新闻: {raw_news}\")\n    \n    # 第二步: 将新闻传递给编辑进行最终审查\n    edited_news_response = client.run(\n        agent=editor_agent,\n        messages=[{\"role\": \"system\", \"content\": raw_news}],\n    )\n    print(f\"{edited_news_response.messages[-1]['content']}\")\n\n\n# 运行给定主题的新闻工作流程示例\nrun_news_workflow(\"药物发现中的 AI\")\n```\n\n# 示例输出\n\n\n```python\nRunning news Agent workflow...\nRunning DuckDuckGo news search for AI in Drug Discovery...\nFetched news: Here's a formatted answer based on the news articles:\n\n**药物发现中的人工智能：革命性的转变**\n\n人工智能（AI）在药物发现中的作用标志着制药领域的革命性转变。AI利用复杂的算法进行自主决策，从数据分析中增强人类能力，而不是取代它们。\n\n**挑战与局限性**\n\n尽管有着令人期待的进展，但在该领域中仍然存在挑战和局限性。论文《AI在药物发现中的作用》探讨了这些问题，强调了高质量数据的必要性、伦理问题的解决以及对基于AI的方法局限性的认识。\n\n**AI在药物发现中的应用**\n\nAI有潜力在药物发现、设计和研究药物间相互作用中发挥关键作用。AI在药物发现中的应用包括：\n\n* 多靶点药理学：AI可以预测化合物对多种疾病的有效性。\n* 化学合成：AI可以优化化学合成过程，以实现更快和更高效的生产。\n* 药物重定位：AI可以识别现有药物的新用途。\n* 预测药物特性：AI可以预测化合物的效力、毒性和物理化学特性。\n\n**药物发现中AI的未来**\n\n随着AI的不断发展，预计将对制药行业产生重大影响。AI的成功应用将依赖于高质量数据的可用性、伦理问题的解决以及对基于AI的方法局限性的认识。\n```\n\n# 本地 AI 新闻聚合的好处\n\n* **隐私**：所有处理都在您的本地机器上进行，确保您的数据留在您自己手中。\n* **定制化**：您可以轻松修改代理的指令或添加新的代理以满足您的特定需求。\n* **最新信息**：通过使用 DuckDuckGo 搜索，您总是能获得关于您选择主题的最新新闻。\n* **AI 驱动的策展**：编辑助手帮助精炼和组织收集的新闻，提供更精致的最终输出。\n\n# 结论\n\n这个本地的人工智能驱动新闻聚合器展示了将大型语言模型与网络搜索能力结合的潜力。通过利用Ollama的Llama 3.2模型、Swarm进行代理编排，以及DuckDuckGo进行搜索，我们创建了一个强大的工具，可以让我们在任何感兴趣的话题上保持信息灵通，同时维护我们的隐私，并完全在本地计算机上运行。\n\n随着人工智能的不断发展，创建个性化、人工智能驱动工具的可能性只会不断扩大。这个新闻聚合器只是一个开始——想象一下，利用这些技术你还可以构建哪些其他创新应用！\n\n# 参考：\n\nSwarm Github : <https://github.com/openai/swarm>\n\n如果您觉得这篇文章信息丰富且有价值，我将非常感谢您的支持：\n\n* 在Medium上为它点赞几次 👏，帮助其他人发现这篇内容（您知道您可以点赞多达50次吗？）。您的点赞将帮助更多读者传播知识。\n- 与您的AI爱好者和专业人士网络分享。\n- 在LinkedIn上与我联系：<https://www.linkedin.com/in/manjunath-janardhan-54a5537/>\n\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/claude-3-5-sonnet-new-pioneering-the-future-of-ai-with-computer-control-capabilities-37a6ff9f9033","frontmatter":{"title":"Claude 3.5 Sonnet（新）：利用计算机控制能力开拓人工智能的未来","meta_title":"Claude 3.5 Sonnet（新）：利用计算机控制能力开拓人工智能的未来","description":"Anthropic 于 2024 年 10 月 22 日发布了其最新的 AI 模型 Claude 3.5 Sonnet。此版本引入了革命性的计算机控制……","date":"2024-10-27T13:57:00.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*n0NkOFbhUm7_fllJ","categories":["Programming","Technology","Generative AI"],"author":"Rifx.Online","tags":["Claude","Sonnet","automation","benchmarks","safety"],"draft":false,"slug":"blog/claude-3-5-sonnet-new-pioneering-the-future-of-ai-with-computer-control-capabilities-37a6ff9f9033"},"content":"\n\n\n\n\nAnthropic于2024年10月22日发布了最新的AI模型Claude 3.5 Sonnet。此次发布引入了革命性的计算机控制能力，并在多个基准测试中实现了显著改进，为AI行业设定了新标准。\n\n## 革命性的计算机控制：新前沿\n\nClaude 3.5 Sonnet 的突出特点是其能够像人类一样与计算机进行交互。这一突破性的能力使得 AI 可以：\n\n* 使用鼠标和键盘输入导航桌面界面\n* 与各种应用程序和网页浏览器进行交互\n* 执行复杂的多步骤任务\n* 执行文件管理操作\n* 自动化重复的工作流程\n\n这一计算机控制功能目前处于公开测试阶段，代表了 AI 系统与数字界面交互方式的范式转变。尽管仍处于实验阶段，但早期测试显示出良好的结果，Claude 3.5 Sonnet 在仅截图任务的 OSWorld 基准测试中得分为 14.9% — 显著高于下一个最佳系统的 7.8%。\n\n## 基准突破性能\n\n升级后的模型在多个指标上表现出显著的改进：\n\n## 编码和技术任务\n\n* 在SWE-bench Verified上的性能为49%（较之前的33.4%有所提升）\n* 在HumanEval编码任务中的得分为93.7%\n* 在软件工程方面的表现优于专业编码系统\n\n## 学术和推理能力\n\n* 65% 的研究生级推理 (GPQA-Diamond)\n* 78% 的本科级知识 (MMLU Pro)\n* 78.3% 的数学问题解决能力 (MATH)\n\n## 商业应用\n\n* 69.2% 在零售领域任务上 (TAU-bench)\n* 46% 在航空领域任务上\n* 90.8% 在图表分析上的准确率\n* 94.2% 在文档问答上的准确率\n\n## 企业集成与可用性\n\nClaude 3.5 Sonnet 可以通过多个平台访问：\n\n* Anthropic API\n* Amazon Bedrock\n* Google Cloud’s Vertex AI\n\n包括 Asana、Canva、DoorDash 和 Replit 在内的主要公司已经开始在其工作流程中实施 Claude 3.5 Sonnet 的功能，特别是利用其计算机控制功能来处理复杂的自动化任务。\n\n## 实际应用\n\n## 软件开发\n\n* 自动化代码测试和调试\n* 智能IDE交互\n* 代码审查与优化\n* 文档生成\n\n## 客户支持\n\n* 高级聊天机器人功能\n* 可视化数据解读\n* 自动化工单解决\n* 流程自动化\n\n## 商业运营\n\n* 文档处理与分析\n* 从视觉源提取数据\n* 工作流自动化\n* 复杂问题解决\n\n## 安全与责任\n\nAnthropic 已实施强有力的安全措施用于计算机控制功能：\n\n* 新分类器以识别潜在的误用\n* 主动监控系统\n* 限制对敏感操作的访问\n* 定期安全评估\n\n## 展望未来\n\n虽然Claude 3.5 Sonnet在人工智能能力方面代表了重大进展，但重要的是要注意某些功能，特别是计算机控制，仍处于早期阶段。某些操作如滚动、拖动和缩放面临挑战，Anthropic鼓励开发者在探索这些新功能时，从低风险任务开始。\n\nClaude 3.5 Sonnet的发布标志着人工智能发展的一个关键时刻，将先进的推理能力与实用的计算机控制功能相结合。随着技术的不断发展，我们可以期待看到更多创新的应用以及人工智能系统与我们的数字世界互动方式的改进。\n\n*本文基于Anthropic、AWS和各类技术合作伙伴的官方公告和文档。有关最新信息，请参考Anthropic的官方文档。*\n\n"},{"lang":"zh","group":"blog","slug":"blog/claude-3-5-sonnet-v-s-gpt-4o-which-one-is-better-3b3675195bf9","frontmatter":{"title":"Claude 3.5 Sonnet V/S GPT-4O：哪一个更好","meta_title":"Claude 3.5 Sonnet V/S GPT-4O：哪一个更好","description":"2022 年 11 月，OpenAI 推出了 ChatGPT 模型，该模型彻底改变了我们搜索和与信息交互的方式。明年，在…","date":"2024-10-27T13:59:09.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*4MXLuSFfGwFkWWn0","categories":["Generative AI","Machine Learning","Natural Language Processing"],"author":"Rifx.Online","tags":["GPT-4o","Claude","multimodal","reasoning","code-generation"],"draft":false,"slug":"blog/claude-3-5-sonnet-v-s-gpt-4o-which-one-is-better-3b3675195bf9"},"content":"\n\n\n在2022年11月，OpenAI推出了ChatGPT，这一模型彻底改变了我们搜索和与信息互动的方式。次年3月，由前OpenAI员工创办的美国初创公司“Anthropic”推出了他们自己的AI模型“Claude”。自发布以来，这两家AI公司一直在竞争，以通过其AI模型为客户提供最佳的功能和体验。最近，OpenAI推出了“GPT-4o”，这是一个令人惊叹的模型，能够出色地处理文件、语音和视频数据。同样，Claude推出了“Claude 3.5 Sonnet”，他们声称这是最先进的AI模型，能够处理复杂任务。在本文中，我们将确定Claude 3.5 Sonnet和GPT-4o之间哪个更好，并比较其在相同输入下的功能和输出，以检查哪个更适合您。\n\n## 能力和特性\n\n### GPT-4o\n\n\n\nGPT-4o 是 OpenAI 最新推出的 LLM。“o” 代表 omni，意为拉丁语中的“每一个”。该模型可以分析语音、图像、视频和文件作为输入，并相应地做出回应。它可以接受语音输入，并以不同角色的声音输出，包括语调、情感等。整个过程与人类对话的延迟相当低，平均为 0.32 秒，而其他语音模型则为 2.8 秒。它还允许用户生成书面内容，如文章、博客、产品描述、不同编程语言的代码、数据分析、图表等。此外，GPT-4o 还可以分析图像和视频，使该模型可以充当语言翻译器、个人助理、虚拟教师或购物助手。它还可以用于医学、工程、军事等领域。要使用此功能，GPT-4o 可以使用用户的摄像头获取实时视图，并在语音模式下相应地回应。它还可以访问您的计算机屏幕，并描述屏幕上显示的内容，用户可以询问与屏幕上显示的内容相关的问题。\n\n*例如，用户可以在屏幕上启用该模型，打开 VS 代码，并提示模型充当编码助手，以获取编码问题的答案。或者，您可以启用摄像头，充当健身教练，检查您是否做得正确。*\n\n该模型具有独特的功能，如数据分析、代码解释器和实时网页浏览，使其与竞争对手不同。该模型还有大量的 GPTs，这是 ChatGPT 的定制版本。\n\n### Claude 3.5 Sonnet\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*BSMcOpvWZ5lUm4Tl)\n\nClaude 3.5 Sonnet 是由 Anthropic 推出的 AI 聊天机器人。它是 Claude AI 模型系列的第三代。这一模型在多个评估中表现出色，保持了高基准，避免了幻觉和错误信息。虽然它不支持像 GPT-4o 那样的语音和视频功能，但它仍然可以执行所有基本任务，例如文本生成和不同编程语言的代码生成、头脑风暴等。根据 Anthropic 的报告，Claude 3.5 Sonnet 是市场上最好的计算机视觉模型之一，可以用于分析图表和图形，从图像中转录文本等。Claude 拥有一个先进的功能，“Artifacts”，这是一个在对话中出现的特殊弹出窗口，允许用户查看代码片段、文本文件或网站设计，并允许他们实时编辑输出。\n\n*例如，用户可以在工作流程中使用计算机视觉和 artifacts。用户可以在纸上进行网站设计的基本原型制作，将文件附加到 Claude 3.5 Sonnet，并提示它根据原型设计网站。生成的代码和网站设计会出现在 artifacts 中。用户可以根据自己的需求编辑代码和设计。用户还可以将他们的项目实时发布到互联网上。*\n\n## 逐项比较\n\n在本节中，我们将根据复杂推理和代码生成等因素比较这两个 LLM，检查它们在处理复杂任务方面的能力，并看看哪个模型更好。\n\n* **研究生水平推理(GPQA, Diamond)**此因素评估模型处理研究生水平教育中复杂、高级推理任务的能力。在此任务中，研究人员在 GPQA 测试中比较模型，该测试由专家设计，包含448个不同领域的问题。这些问题是 Google Proof，因此任何人都无法在线找到它们。Claude 的得分接近 59.4%，而 GPT-4o 的得分仅为 53.6%。虽然两个得分相对接近，但正如我们所见，Claude 在需要高级分析思维的任务中可能是更好的选择，例如研究分析、复杂问题解决和高学术水平的问题。\n* **本科水平知识(MMLU)**MMLU，即大规模多任务语言理解，是一个基准，解释任何 AI 模型在本科水平上对各个学科的通用知识理解。Claude 3.5 Sonnet 在此实验中的得分为 88.3%，而 GPT-4o 的得分为 88.7%。这表明这两个 LLM 在多个领域进行了训练，并对这些领域有更深入的理解。这使得 AI 模型成为通用知识任务、多个学科的基础辅导等的合适工具。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*A4w-tvsxcmFINaQT)\n\n* **代码(HumanEval)**HumanEval 是一个基准，评估模型生成、理解和调试代码的能力。在这个基准中，Claude 3.5 Sonnet 达到了 92%的得分，而 GPT-4o 的得分为 90.2%。Claude 3.5 Sonnet 在此任务中的结果非常出色，因为它提供了比 GPT-4o 更好的编码环境“Artifacts”和更好的代码生成。Claude 允许用户在 Artifacts 弹出窗口中设计、编辑和运行代码。在 Claude 3.5 Sonnet 发布后，大家都在开发工具、网站和基本游戏，并在互联网上分享它们。另一方面，GPT-4o 的得分也不错，但它的界面中没有任何编码环境，因此开发人员必须花费很多精力，因为它生成的代码很难达到结果。\n* **文本推理(DROP, FLscore)**DROP（段落离散推理）是一个基准，测量模型理解复杂文本信息的能力。在这个挑战中，Claude 3.5 Sonnet 的得分为 87.1%，而 GPT-4o 的得分为 83.4%。这表明 Claude 3.5 Sonnet 在涉及详细文本分析、文本审查、复杂问答系统等任务时更好且更有效。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Kcy7sFb2FYpbfrwp)\n\n* **数学问题解决(MATH)**此测试评估任何 AI 模型解决各种数学问题的能力。Claude 3.5 Sonnet 的得分仅为 71.1%，而 GPT-4o 的得分为 76.6%。这些得分使 GPT-4o 成为数学问题解决任务的更好模型，并可用于财务建模、科学计算和高级数据分析等数学计算。\n* **多语言数学(MSGM)**此因素描述任何 AI 模型在多种语言中解决数学问题的能力。两个模型的得分接近：GPT-4o 90.5% 和 Claude 3.5 Sonnet 91.6%。这表明两个模型表现出色，Claude 略胜一筹。该能力对于教育应用或任何需要跨语言障碍进行数学推理交流的场景特别有用。\n* **视觉问答(MMU/val)**此因素描述 LLM 分析图像中呈现的信息的能力。GPT-4o 在这一基准中以 69.1% 超过 Claude 3.5 Sonnet 的 68.3%。另一方面，在分析文档中的文本时，Claude 3.5 Sonnet 的得分为 95.2%，而 GPT-4o 的得分为 92.1%。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*xzjqBV2YL0lVFitX)\n\n* **图像生成**图像生成是 LLM 从文本生成图像的能力。GPT-4o 集成了 DallE-2，可以通过文本生成图像，结果非常出色。另一方面，Claude 3.5 Sonnet 无法创建任何图像。此功能还帮助 GPT-4o 更好地设计网站和参考，因为它在许多图像上进行了训练。\n* **知识截止**在这里，两个模型都在特定日期之前的有限数据集上进行了训练。Claude 3.5 Sonnet 在 2024 年 4 月之前的数据上进行了训练，而 GPT-4o 则在 2024 年之前的数据上进行了训练。GPT-4o 的真正优势在于它具有实时网页浏览功能，这有助于 LLM 定期在新数据上进行训练。\n\n## GPT-4o 的优点：\n\n* 处理语音、图像和视频输入。\n* 实时网页浏览能力。\n* 更快的响应时间（平均 0.32 秒）。\n* 在数学问题解决方面表现优越。\n* 可以使用 DALL-E 2 生成图像。\n\n## GPT-4o 的缺点：\n\n* 研究生水平推理的性能稍低。\n* 没有内置的编码环境。\n* 文档视觉问答得分较低。\n* 代码生成能力稍逊。\n* 在详细文本分析方面效果较差。\n\n## Pros Claude 3.5 Sonnet:\n\n* 在研究生级别的推理方面表现出色。\n* 优越的代码生成和内置的“工件”功能。\n* 在详细文本分析中表现更佳。\n* 在文档视觉问答中得分更高。\n* 在多语言数学方面略有优势。\n\n## Cons Claude 3.5 颂：\n\n* 无法处理语音或视频输入。\n* 没有图像生成能力。\n* 在视觉问答方面性能稍低。\n* 无法访问实时网络信息。\n* 在数学问题解决方面较弱。\n\n## 结论\n\nGPT-4o 和 Claude 3.5 Sonnet 在各种任务中展现了令人印象深刻的能力，各自有其优势。GPT-4o 在多模态输入、实时信息访问和图像生成方面表现出色，使其在多种应用中非常灵活。Claude 3.5 Sonnet 在复杂推理、代码生成和详细文本分析方面表现突出，在特定的学术和专业背景下提供了更优的性能。选择这两种模型取决于具体的使用案例和所需的功能。随着人工智能技术的进步，我们可以期待进一步的改进和针对不同需求的专业模型。\n\n"},{"lang":"zh","group":"blog","slug":"blog/explore-swarm-multi-agent-framework-locally-0e25ee617795","frontmatter":{"title":"本地探索 Swarm 多智能体框架","meta_title":"本地探索 Swarm 多智能体框架","description":"Swarm 是一个实验性示例框架，用于模拟轻量级多代理框架，用于教育目的。通常它与 Open… 配合使用","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*0ZVceq32bvkytC7HSIgmwA.png","categories":["Programming","Technology","Education"],"author":"Rifx.Online","tags":["Swarm","Multi-Agent","Framework","OpenAI","Ollama"],"draft":false,"slug":"blog/explore-swarm-multi-agent-framework-locally-0e25ee617795"},"content":"\n\n\n\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*zkpW8DDwh0TTYuHJVJbDaw.png)\n\nSwarm 是一个实验性样本框架，用于模拟轻量级多智能体框架，旨在教育目的。通常它与 Open AI Key 一起使用，但我们可以更改为使用本地的 Ollama 或 LM Studio 模型。\n\n**设置：**\n\n\n```python\n## 创建一个新的 Conda 或 Python 虚拟环境并激活它\nconda install python==3.10\npip install torch openai\npip install transformers accelerate huggingface_hub\npip install git+ssh://git@github.com/openai/swarm.git\n```\n**使用 Open AI Key：**\n\n\n```python\nexport OPEN_API_KEY = Your Key\n```\n**使用 Ollama 或 LM Studio 本地 LLM — 更新为本地 URL：**\n\n\n```python\n## 查找 conda 或 python 虚拟环境中的 site-packages/swarm\n## 找到文件 core.py\nclass Swarm:\n    def __init__(self, client=None):\n        if not client:\n          # 实际代码\n          #client = OpenAI()\n          # 将基础 URL 和 API Key 更新为 Ollama / LM Studio\n          # 在本演示中，我们使用 LM Studio 和 Llama 3.1\n          client = OpenAI(base_url=\"http://localhost:1234/v1\",api_key=\"random\")\n        self.client = client\n```\n**克隆仓库：**\n\n克隆仓库 — 在这里您可以找到不同用例的示例目录，如基本、航空公司和天气等。\n\n\n```python\ngit clone https://github.com/openai/swarm.git\ncd swarm/examples\n```\n**示例代码：**\n\n\n```python\nfrom swarm import Swarm, Agent\n\nclient = Swarm()\n\n\nit_agent = Agent(\n    name=\"IT Agent\",\n    instructions=\"You are an IT Expert with 10 Years of Experience.\",\n)\n\nsales_agent = Agent(\n    name=\"Sales Agent\",\n    instructions=\"You are a Sales Expert with 5 Years of Experience and knows about best selling mobiles.\",\n)\n\ndef transfer_to_sales_agent():\n    print(\"Sales agent in action\")\n    \"\"\"Transfer sales related questions to sales team immediately.\"\"\"\n    return sales_agent\n\ndef transfer_to_it_agent():\n    print(\"IT agent in action\")\n    \"\"\"Transfer IT users immediately.\"\"\"\n    return it_agent\n\nenglish_agent = Agent(\n    name=\"English Agent\",\n    instructions=\"You only speak English.\",\n    functions=[transfer_to_sales_agent,transfer_to_it_agent],\n)\n\n\nmessages = [{\"role\": \"user\", \"content\": \"How to install pandas lib?\"}]\nresponse = client.run(agent=english_agent, messages=messages)\n\nprint(response.messages[-1][\"content\"])\n\nmessages = [{\"role\": \"user\", \"content\": \"What are the best selling items?\"}]\nresponse = client.run(agent=english_agent, messages=messages)\n\nprint(response.messages[-1][\"content\"])\n```\n**参考文献：**\n\n\n```python\nhttps://github.com/openai/swarm\n\nhttps://github.com/victorb/ollama-swarm/tree/main\n```\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*hCFJ4VQoT12yElYPXwXvWA.png)\n\n鉴于这是一个实验性版本，仍有很大的改进空间。航空代理示例代码 [swarm/examples/airline] 非常有趣，因此可以尝试这些示例。试试看，并在评论中分享您的经验。谢谢。\n\n"},{"lang":"zh","group":"blog","slug":"blog/generating-structured-data-from-an-image-with-gpt-vision-and-langchain-34aaf3dcb215","frontmatter":{"title":"使用 GPT Vision 和 Langchain 从图像生成结构化数据","meta_title":"使用 GPT Vision 和 Langchain 从图像生成结构化数据","description":"在当今世界，视觉数据非常丰富，从图像中提取有意义信息的能力变得越来越重要……","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*FPRRg85jYb7MrzXEpNWbmw.jpeg","categories":["Programming","Computer Vision","Natural Language Processing"],"author":"Rifx.Online","tags":["Langchain","GPT","vision","LLMs","structured"],"draft":false,"slug":"blog/generating-structured-data-from-an-image-with-gpt-vision-and-langchain-34aaf3dcb215"},"content":"\n\n\n\n\n在当今这个视觉数据丰富的世界中，从图像中提取有意义信息的能力变得越来越重要。Langchain是一个强大的框架，用于构建大型语言模型（LLMs）应用程序，提供了一套多功能的工具来应对这一挑战。在本文中，我们将探讨如何使用Langchain从图像中提取结构化信息，例如计算人数和列出主要物体。\n\n在深入代码之前，让我们先了解一下任务的背景。想象一下你有一张场景的图像，比如城市街道。你的目标是从这张图像中提取有价值的信息，包括在场的人数和场景中的主要物体列表。\n\n## 关于 Langchain\n\nLangchain 是一个综合框架，允许开发者利用大型语言模型（LLMs）的强大功能构建复杂的应用程序。它提供了模块化和可扩展的架构，使开发者能够创建针对特定需求的自定义管道、代理和工作流。\n\nLangchain 简化了 LLM 的集成，提供了处理各种数据源（包括文本、图像和结构化数据）的抽象和工具。它支持来自不同提供商的广泛 LLM，例如 OpenAI 和 Anthropic，使得在单个应用程序中轻松切换模型或组合多个模型变得简单。\n\n## 准备环境并设置 OpenAI API 密钥\n\n要跟随本教程，您需要安装 Langchain。您可以使用 pip 安装它：\n\n```python\npip install langchain langchain_openai\n```\n要在 Langchain 中使用 OpenAI 语言模型，您需要从 OpenAI 获取一个 API 密钥。如果您还没有 API 密钥，可以在 OpenAI 网站上注册一个 (<https://openai.com/api/>)。\n\n一旦您拥有了 API 密钥，可以将其设置为系统中的环境变量，或者直接在代码中提供。以下是如何将 API 密钥设置为环境变量的示例：\n\n```python\nexport OPENAI_API_KEY=\"your_openai_api_key_here\"\n```\n或者，您可以直接在 Python 代码中提供 API 密钥：\n\n```python\nimport os\nimport langchain\nos.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key_here\"\n```\n在设置好 API 密钥后，Langchain 将能够与 OpenAI API 进行身份验证并使用他们的语言模型。\n\n## 加载和编码图像\n\n在我们使用 Langchain 处理图像之前，我们需要从文件中加载图像数据，并将其编码为可以传递给语言模型的格式。下面的代码定义了一个函数 `load_image`，该函数接受一个包含 `image_path` 键的字典，并返回一个新的字典，其中 `image` 键包含编码为 base64 字符串的图像数据。\n\n```python\ndef load_image(inputs: dict) -> dict:\n    \"\"\"Load image from file and encode it as base64.\"\"\"\n    image_path = inputs[\"image_path\"]\n  \n    def encode_image(image_path):\n        with open(image_path, \"rb\") as image_file:\n            return base64.b64encode(image_file.read()).decode('utf-8')\n    image_base64 = encode_image(image_path)\n    return {\"image\": image_base64}\n```\n`load_image` 函数首先从输入字典中提取 `image_path`。然后，它定义了一个嵌套函数 `encode_image`，该函数以二进制模式打开图像文件，读取其内容，并使用 Python 标准库中的 `base64.b64encode` 函数将其编码为 base64 字符串。\n\n`load_image` 函数使用提供的 `image_path` 调用 `encode_image`，并将结果 base64 编码字符串存储在 `image_base64` 变量中。最后，它返回一个新的字典，其中 `image` 键设置为 `image_base64`。\n\n为了将此函数集成到 Langchain 流水线中，我们可以创建一个 `TransformChain`，该链接受 `image_path` 作为输入，并生成 `image`（base64 编码字符串）作为输出。\n\n```python\nload_image_chain = TransformChain(\n    input_variables=[\"image_path\"],\n    output_variables=[\"image\"],\n    transform=load_image\n)\n```\n通过这种设置，我们可以轻松地将图像加载和编码作为更大 Langchain 工作流的一部分，从而使我们能够使用大型语言模型处理视觉数据和文本。\n\n## 定义输出结构\n\n在我们提取图像信息之前，需要定义我们希望接收的输出结构。在这种情况下，我们将创建一个名为 `ImageInformation` 的 Pydantic 模型，其中包括图像描述和我们可能想要提取的任何其他信息的字段。\n\n```python\nfrom langchain_core.pydantic_v1 import BaseModel, Field\n\nclass ImageInformation(BaseModel):\n \"\"\"Information about an image.\"\"\"\n image_description: str = Field(description=\"a short description of the image\")\n people_count: int = Field(description=\"number of humans on the picture\")\n main_objects: list[str] = Field(description=\"list of the main objects on the picture\")\n```\n\n## 设置图像模型\n\n接下来，我们将创建一个链，将图像加载和编码步骤与 LLM 调用步骤结合起来。由于 `ChatOpenAI` 模型在我的理解中并不具备同时处理文本和图像输入的能力，我们将创建一个包装链来实现这一功能。\n\n```python\nfrom langchain.chains import TransformChain\nfrom langchain_core.messages import HumanMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain import globals\nfrom langchain_core.runnables import chain\n\n## Set verbose\nglobals.set_debug(True)\n\n@chain\ndef image_model(inputs: dict) -> str | list[str] | dict:\n \"\"\"Invoke model with image and prompt.\"\"\"\n model = ChatOpenAI(temperature=0.5, model=\"gpt-4-vision-preview\", max_tokens=1024)\n msg = model.invoke(\n             [HumanMessage(\n             content=[\n             {\"type\": \"text\", \"text\": inputs[\"prompt\"]},\n             {\"type\": \"text\", \"text\": parser.get_format_instructions()},\n             {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{inputs['image']}\"}},\n             ])]\n             )\n return msg.content\n```\n在这个代码片段中，我们定义了一个名为 `image_model` 的链，使用提供的提示、格式说明和图像调用 `ChatOpenAI` 模型。`image_model` 链接受一个包含提示和 base64 编码图像字符串的字典 `inputs`。\n\n在链内部，我们创建了一个 `HumanMessage` 对象，该对象结合了提示文本、格式说明和图像 URL，以数据 URI 格式化，包含 base64 编码的图像数据。然后，我们使用这个 `HumanMessage` 对象调用 `ChatOpenAI` 模型，使用专门为涉及文本和图像的多模态任务设计的 `gpt-4-vision-preview` 模型。\n\n该模型处理文本提示和图像，并返回输出。\n\n## 整合所有内容\n\n现在我们已经拥有了所有必要的组件，我们可以定义一个函数来协调整个过程：\n\n```python\nfrom langchain_core.output_parsers import JsonOutputParser\n\nparser = JsonOutputParser(pydantic_object=ImageInformation)\ndef get_image_informations(image_path: str) -> dict:\n   vision_prompt = \"\"\"\n   Given the image, provide the following information:\n   - A count of how many people are in the image\n   - A list of the main objects present in the image\n   - A description of the image\n   \"\"\"\n   vision_chain = load_image_chain | image_model | parser\n   return vision_chain.invoke({'image_path': f'{image_path}', \n                               'prompt': vision_prompt})\n```\n在这个函数中，我们定义了一个提示，要求LLM提供图像中人物的数量和主要物体的列表。然后，我们创建一个链，将图像加载步骤（`load\\_image\\_chain`）、LLM调用步骤（`image\\_model`）和JSON输出解析器（`parser`）结合在一起。最后，我们用图像路径和提示调用这个链，函数返回一个包含提取信息的字典。\n\n## 示例用法\n\n要使用此功能，只需提供图像文件的路径：\n\n\n```python\nresult = get_image_informations(\"path/to/your/image.jpg\")\nprint(result)\n```\n这将输出一个包含请求信息的字典，例如：\n\n\n```python\n{\n 'description': 'a view of a city showing cars waiting at a traffic light',\n 'people_count': 5,\n 'main_objects': ['car', 'building', 'traffic light', 'tree']\n}\n```\n\n## 结论\n\nLangchain 提供了强大的工具集，用于处理大型语言模型并从各种数据源（包括图像）中提取有价值的信息。通过将 Langchain 的功能与自定义提示和输出解析相结合，您可以创建强大的应用程序，从视觉数据中提取结构化信息。\n\n请记住，输出的质量将取决于您使用的 LLM 的能力以及您提示的具体性。尝试不同的模型和提示，以找到最适合您用例的解决方案。\n\n如果您找到更好的方法来实现相同的结果或有改进建议，请随时在评论中分享。本文提供的代码示例旨在作为起点，可能还有其他方法或优化。\n\n"},{"lang":"zh","group":"blog","slug":"blog/how-to-run-nvidia-llama-3-1-nemotron-70b-instruct-locally-a58ad283aaff","frontmatter":{"title":"如何在本地运行 Nvidia 的 llama-3.1-nemotron-70b-instruct","meta_title":"如何在本地运行 Nvidia 的 llama-3.1-nemotron-70b-instruct","description":"在本地运行大型语言模型 (LLM) 在开发人员、研究人员和 AI 爱好者中越来越受欢迎。其中之一就是……","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*fqVKJkw5sQvLtIsyCcengQ.png","categories":["Programming","Technology","Science"],"author":"Rifx.Online","tags":["Nvidia","llama","Ollama","llama.cpp","Transformers"],"draft":false,"slug":"blog/how-to-run-nvidia-llama-3-1-nemotron-70b-instruct-locally-a58ad283aaff"},"content":"\n\n\n在开发者、研究人员和 AI 爱好者中，本地运行大型语言模型（LLMs）变得越来越受欢迎。其中一个引起广泛关注的模型是 llama-3.1-nemotron-70b-instruct，这是 NVIDIA 定制的强大 LLM，旨在增强生成响应的有用性。在本综合指南中，我们将探讨多种方法，以便在您的本地机器上运行此模型，首先介绍用户友好的 Ollama 平台。\n\n> 在开始之前，如果您正在寻找一个一体化的 AI 平台，以便在一个地方管理所有 AI 订阅，包括所有 LLM（如 GPT-o1、Llama 3.1、Claude 3.5 Sonnet、Google Gemini、未审查的 LLM）和图像生成模型（FLUX、Stable Diffusion 等），请使用 Anakin AI 来管理它们！\n\n\n\n## 方法 1：使用 Ollama 本地运行 llama-3.1-nemotron-70b-instruct\n\nOllama 是一个出色的工具，用于本地运行 LLM，提供简单的设置过程并支持多种模型，包括 llama-3.1-nemotron-70b-instruct。\n\n### 安装\n\n1. 访问官方 Ollama 网站 ([https://ollama.ai](https://ollama.ai/))，下载适合您操作系统的版本。\n2. 通过在终端中运行以下命令来安装 Ollama：\n\n\n```python\ncurl https://ollama.ai/install.sh | sh\n```\n\n### 运行 llama-3.1-nemotron\n\n安装 Ollama 后，您可以通过一个简单的命令轻松运行 llama-3.1-nemotron-70b-instruct 模型：\n\n```python\nollama run nemotron:70b-instruct-q5_K_M\n```\n该命令将在您的系统上下载模型（如果尚未存在），并启动一个交互式会话。\n\n### 使用模型\n\n在模型加载后，您可以通过输入提示开始与其互动。例如：\n\n```python\n>>> What are the key features of llama-3.1-nemotron-70b-instruct?\n\nLlama-3.1-Nemotron-70B-Instruct is a large language model with several key features:\n1. Customized by NVIDIA: The model has been fine-tuned by NVIDIA to improve the helpfulness and quality of its responses.\n2. Based on Llama 3.1: It builds upon the Llama 3.1 architecture, which is known for its strong performance across various tasks.\n3. 70 billion parameters: This large parameter count allows for complex reasoning and a wide range of capabilities.\n4. Instruct-tuned: The model is specifically designed to follow instructions and generate helpful responses to user queries.\n5. RLHF training: It has been trained using Reinforcement Learning from Human Feedback, specifically the REINFORCE algorithm.\n6. Specialized reward model: The training process utilized Llama-3.1-Nemotron-70B-Reward for optimization.\n7. HelpSteer2-Preference prompts: These were used during the training process to further improve the model's helpfulness.\n8. Extended context length: Like other Llama 3.1 models, it likely supports a longer context window of 128K tokens.\n9. Multilingual capabilities: It can understand and generate text in multiple languages.\n10. Strong reasoning abilities: The model excels in tasks requiring complex reasoning and problem-solving.\nThese features make llama-3.1-nemotron-70b-instruct a powerful and versatile language model suitable for a wide range of applications, from general conversation to specialized tasks in various domains.\n```\n对于更高级的用例，您可以使用像 Langchain 这样的库将 Ollama 与 Python 集成。以下是一个简单的示例：\n\n```python\npython\n\nfrom langchain.llms import Ollama\n\nollama = Ollama(base_url=\"http://localhost:11434\", model=\"nemotron:70b-instruct-q5_K_M\")\nresponse = ollama.generate(\"Explain the concept of quantum entanglement.\")\nprint(response)\n```\n这使您能够无缝地将模型集成到您的 Python 项目和应用程序中。\n\n## 方法 2：使用 llama.cpp\n\nllama.cpp 是一个流行的 C++ 实现的 Llama 模型推理，针对 CPU 使用进行了优化。虽然它可能需要比 Ollama 更多的设置，但它提供了更大的灵活性和对模型参数的控制。\n\n### 安装\n\n1. 克隆 llama.cpp 仓库：\n\n```python\ngit clone https://github.com/ggerganov/llama.cpp.git\ncd llama.cpp\n```\n1. 构建项目：\n\n```python\nmake\n```\n\n### 下载模型\n\n要运行 llama-3.1-nemotron-70b-instruct，您需要下载模型权重。这些通常以 GGML 或 GGUF 格式提供。您可以在 Hugging Face 等平台上找到预先转换的模型。\n\n```python\nmkdir models\ncd models\nwget https://huggingface.co/TheBloke/Llama-3.1-Nemotron-70B-Instruct-GGUF/resolve/main/llama-3.1-nemotron-70b-instruct.Q4_K_M.gguf\n```\n\n### 运行模型\n\n一旦你拥有模型文件，就可以使用以下命令运行它：\n\n```python\n./main -m models/llama-3.1-nemotron-70b-instruct.Q4_K_M.gguf -n 1024 -p \"Hello, how are you today?\"\n```\n该命令加载模型并生成对给定提示的响应。你可以调整各种参数，比如生成的令牌数量 (-n) 或温度以控制随机性。\n\n## 方法 3：使用 Hugging Face Transformers\n\nHugging Face 的 Transformers 库提供了一个高层次的 API，用于处理各种语言模型，包括 llama-3.1-nemotron-70b-instruct。\n\n**安装**\n\n首先，安装必要的库：\n\n\n```python\npip install transformers torch accelerate\n```\n**运行模型**\n\n以下是一个加载和使用模型的 Python 脚本：\n\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nmodel_name = \"meta-llama/Llama-3.1-Nemotron-70b-instruct\"\n## Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n## Prepare the input\nprompt = \"Explain the concept of quantum computing in simple terms.\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n## Generate the response\nwith torch.no_grad():\n    outputs = model.generate(**inputs, max_new_tokens=100)\n## Decode and print the response\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response)\n```\n这种方法允许对模型的行为进行更细粒度的控制，并与其他 Hugging Face 工具和管道集成。\n\n## 结论\n\n在本地运行 llama-3.1-nemotron-70b-instruct 为开发者和研究人员打开了无限可能。无论您选择 Ollama 的简单性、llama.cpp 的灵活性，还是 Hugging Face Transformers 的集成功能，您现在都有工具可以在自己的硬件上利用这一先进语言模型的强大能力。在探索 llama-3.1-nemotron-70b-instruct 的能力时，请记住在性能与资源限制之间取得平衡，并始终考虑您应用的伦理影响。负责任的使用，这个模型可以成为推动自然语言处理和 AI 驱动应用可能性的宝贵资产。\n\n"},{"lang":"zh","group":"blog","slug":"blog/key-points-llm-quantization-chatgpt-artificial-intelligence-8201ffcb33d4","frontmatter":{"title":"解锁 LLM 量化的 5 个关键点","meta_title":"解锁 LLM 量化的 5 个关键点","description":"量化大型语言模型","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*RUqPEr2NTYXlI1omqF22Qg.png","categories":["Machine Learning","Data Science","Technology/Web"],"author":"Rifx.Online","tags":["quantization","weights","activations","calibration","Quanto"],"draft":false,"slug":"blog/key-points-llm-quantization-chatgpt-artificial-intelligence-8201ffcb33d4"},"content":"\n\n\n### 大型语言模型的量化\n\n\n\nLLM量化目前是一个热门话题，因为它在提高大型语言模型（LLMs）的效率和在各种硬件平台（包括消费级设备）上部署方面发挥着至关重要的作用。\n\n通过调整模型中某些组件的精度，**量化显著减少了模型的内存占用**，同时保持相似的性能水平。\n\n在本指南中，我们将探讨LLM量化的五个关键方面，包括将此技术应用于我们模型的一些实用步骤。\n\n## #1. 理解量化\n\n量化是一种模型压缩技术，通过降低 LLM 中权重和激活的精度来实现。这涉及将高精度值转换为低精度值，实际上是**将存储更多信息的数据类型更改为存储更少信息的数据类型**。\n\n减少每个权重或激活所需的位数显著降低了整体模型大小。因此，**量化创建了使用更少内存和需要更少存储空间的 LLM。**\n\n这一技术在应对 LLM 连续迭代中参数数量的指数增长时变得至关重要。例如，在 OpenAI 的 GPT 系列中，我们可以在以下图表中观察到这一增长趋势：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*QlAhma3Wu1F6w2WvkE8jDA.png)\n\n这一显著增加带来了挑战：随着模型的增长，它们的内存需求往往超过先进硬件加速器（如 GPU）的容量。**这需要分布式训练和推理来管理这些模型，从而限制了它们的可部署性。**\n\n## #2. 量化背后的直觉\n\n尽管量化的定义看起来相当复杂，但这个概念可以通过矩阵直观地解释。\n\n让我们考虑以下一个 3x3 矩阵，表示神经网络的权重。左侧的矩阵显示了原始权重，而右侧的矩阵显示了这些权重的量化版本：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*LPzWe9oxjlDYdSp7dVvRUg.png)\n\n在这个简单的例子中，我们将原始矩阵的元素从四位小数四舍五入到一位小数。尽管矩阵看起来相似，**但四位小数版本所需的存储空间显著更高**。\n\n在实践中，量化不仅仅是一个四舍五入操作。相反，它涉及将数值转换为不同的数据类型，通常是从更高精度转换为更低精度。\n\n例如，大多数模型的默认数据类型是 `float32`，每个参数需要 4 字节（32 位）。因此，对于一个 3x3 矩阵，总内存占用为 36 字节。将数据类型更改为 `int8`，每个参数只需要 1 字节，从而将矩阵的总内存占用减少到仅 9 字节。\n\n## #3. 量化误差\n\n正如我们所看到的，原始矩阵及其量化形式并不完全相等，但非常相似。逐值之间的差异被称为“量化误差”，我们也可以用矩阵形式表示：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*VtGDjVbr7daagLXB57i7Mg.png)\n\n**这种量化误差可以在网络中的每个权重矩阵中累积，从而影响模型的性能。**\n\n当前的量化研究旨在最小化精度差异，同时减少训练或推理模型所需的计算资源，同时保持可接受的性能水平。\n\n## #4. 线性量化\n\n线性量化是 LLMs 中最流行的量化方案之一。简单来说，它涉及将原始权重的浮点值范围映射到固定点值范围。\n\n让我们回顾一下将线性量化应用于我们的模型所需的步骤：\n\n* **获取最小和最大范围：** 我们需要获取待量化的浮点权重的最小值和最大值（`x_min` 和 `x_max`）。我们还需要定义量化范围（`q_min` 和 `q_max`），该范围已经由我们想要转换的数据类型设置。\n* **计算缩放因子（`s`）和零点（`z`）值：** 首先，缩放因子（`s`）将浮点值的范围调整到适合整数范围，保持数据分布和范围。其次，零点（`z`）确保浮点范围内的零被准确地表示为整数，从而保持数值的准确性和稳定性，特别是对于接近零的值。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BepC6-izw0yE19ejsS705Q.png)\n\n* **量化值（`q`）：** 我们需要使用在前一步计算的缩放因子（`s`）和零点（`z`）将原始浮点值映射到整数范围。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BBOQ0VbSGbwf7CN8c4PWKQ.png)\n\n应用这些公式相当简单。如果我们将它们应用于下图左侧的 3x3 权重张量，我们将得到右侧所示的量化矩阵：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*KzBvg84mfI2gAhTIyVibwQ.png)\n\n我们可以看到，`int8` 值的下限对应于原始张量的下限，而上限对应于原始张量的上限，*即，映射为 `0.50 → 255` 和 `-0.40 → 0`。*\n\n我们现在可以使用下面的公式对值进行反量化。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*E5nnqYzncYCRuM5prssuOw.png)\n\n如果我们将反量化后的值再次放入矩阵形式（左侧矩阵），我们可以通过计算原始矩阵与其反量化版本之间逐点差异来计算量化误差（右侧矩阵）：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*56NALu9PAN95QG2hn8HXoQ.png)\n\n正如我们所观察到的，量化误差开始在某些矩阵值中显现。\n\n## #5. 权重量化与激活量化\n\n在上面的例子中，我们主要关注于量化模型的权重。虽然权重量化对于模型优化至关重要，但考虑到激活也可以进行量化同样重要。\n\n**激活量化涉及减少网络中每层的中间输出的精度**。与权重在模型训练后保持不变不同，激活是动态的，并且随着每个输入而变化，使其范围更难预测。\n\n一般而言，激活量化比权重量化更具挑战性，因为它需要仔细校准以确保准确捕捉激活的动态范围。\n\n权重量化和激活量化是互补的技术。两者结合使用可以显著减少模型大小，而不会大幅影响性能。\n\n## 最后的思考\n\n在本文中，我们回顾了关于量化的5个关键点，以更好地理解如何减小这些不断增长的模型的大小。\n\n至于这些技术的实现，Python中有几个支持量化的工具和库，例如`pytorch`和`tensorflow`。然而，在现有模型中无缝集成量化需要对库和模型内部结构有深入的理解。\n\n这就是为什么到目前为止，我最喜欢的简单步骤实现量化的选项是Hugging Face的[Quanto](https://huggingface.co/blog/quanto-introduction)库，旨在简化PyTorch模型的量化过程。\n\n如果你对LLM量化的深入内容以及如何使用上述库感兴趣，你可能还会对文章[“大型语言模型（LLMs）的量化：有效减少AI模型大小”](https://www.datacamp.com/tutorial/quantization-for-large-language-models)感兴趣。\n\n就这些！非常感谢你的阅读！\n\n我希望这篇文章能在**使用LLMs进行编码时**对你有所帮助！\n\n你也可以订阅我的[**时事通讯**](https://readmedium.com/@andvalenzuela/subscribe)，以便及时获取新内容。\n\n**特别是**，**如果你对有关大型语言模型和ChatGPT的文章感兴趣**：\n\n"},{"lang":"zh","group":"blog","slug":"blog/langgraph-vs-langchain-vs-langflow-vs-langsmith-which-one-to-use-why-69ee91e91000","frontmatter":{"title":"LangGraph、LangChain、LangFlow、LangSmith：使用哪一个以及为什么？","meta_title":"LangGraph、LangChain、LangFlow、LangSmith：使用哪一个以及为什么？","description":"了解 LangGraph、LangChain、LangFlow 和 LangSmith 之间的主要区别，并了解哪种框架最适合您的……","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*xrWv1QVt4zE5cxjA8VA3ag.png","categories":["Programming","Technology","Technology/Web"],"author":"Rifx.Online","tags":["LangGraph","LangChain","LangFlow","LangSmith","frameworks"],"draft":false,"slug":"blog/langgraph-vs-langchain-vs-langflow-vs-langsmith-which-one-to-use-why-69ee91e91000"},"content":"\n\n\n### 探索 LangGraph、LangChain、LangFlow 和 LangSmith 之间的关键区别，了解哪种框架最适合您的语言模型应用——从工作流构建到性能监控。\n\n👨🏾‍💻 [GitHub](https://github.com/mdmonsurali) ⭐️ | 👔[LinkedIn](https://www.linkedin.com/in/mdmonsurali/) |📝 [Medium](https://medium.com/@monsuralirana)\n\n\n\n近年来，自然语言处理（NLP）领域见证了可用于构建基于语言模型的应用程序的框架、库和工具数量的激增。在这些工具中，**LangGraph**、**LangChain**、**LangFlow** 和 **LangSmith** 已成为领先的选择，各自满足不同的用例和用户需求。如果您希望构建、监控或扩展语言模型工作流，了解这些工具的优势和目的至关重要。\n\n在本博客中，我们将探讨每个框架，分析它们的优势，并提供何时使用它们的见解。无论您是经验丰富的开发者还是该领域的新手，理解这些工具的细微差别将帮助您为您的项目选择合适的工具。\n\n## 语言模型框架简介\n\n随着强大的语言模型如 GPT-3、GPT-4 以及其他基于变换器的模型的崛起，越来越需要能够简化语言应用程序创建和管理的框架。这些框架简化了复杂的任务，如 **链接多个提示**、**检索相关文档**，甚至 **监控模型性能**。\n\n然而，并非所有框架都是相同的。有些框架提供 **可视化界面** 来管理工作流程，而其他框架则提供高级的 **调试和可观察性** 功能。让我们深入了解这些工具，以理解它们独特的功能。\n\n## 1. LangGraph：可视化复杂工作流\n\n**LangGraph** 是一个为开发者设计的新框架，适合那些偏好 **可视化方法** 来构建语言模型管道的用户。它允许您通过 **基于图的可视化** 来构建复杂的工作流，从而更容易理解不同任务和组件之间的依赖关系。这对于多个步骤（如文本生成、文档检索和分类）串联在一起的大型应用尤其有用。\n\n### 优势：\n\n* **可视化工作流表示**：LangGraph 允许您可视化不同组件之间的数据和操作流。这种图形化的方法直观且有助于设计更复杂的管道。\n* **调试简单**：LangGraph 的可视化特性使得识别工作流中的瓶颈或问题节点变得更加容易。\n\n### 示例用例：\n\n假设您正在构建一个自动化系统，该系统首先使用语言模型检索相关文档，然后将其传递给摘要生成器。在 LangGraph 中，您可以直观地绘制出此工作流程，展示每个步骤之间的关系。如果链中的任何一点出现问题，视觉工具使您能够轻松定位问题所在。\n\n### 何时使用 LangGraph：\n\n如果您正在管理 **复杂的工作流程**，并且重视 **图形界面** 来理解您的管道，LangGraph 是一个绝佳的选择。它特别适合那些更喜欢直观的拖放式工作流程设计的开发人员或数据科学家。\n\n**关键点**：\n\n* 如果您需要清晰的语言处理工作流程的可视化表示。\n* 在创建需要分支或多路径依赖的更复杂的管道时。\n\n## 2. LangChain：LLM 应用的工作马\n\n**LangChain** 是构建由 **大型语言模型 (LLMs)** 驱动的应用程序最受欢迎的框架之一。它提供了一种灵活的 **代码优先方法**，允许开发者将文档检索、摘要和问答等任务串联成统一的工作流程。\n\n### 优势：\n\n* **广泛支持LLMs**：LangChain兼容多种语言模型，使得集成OpenAI的GPT或本地托管模型变得简单。\n* **链式能力**：LangChain擅长于**多个操作的链式处理**——因此得名——使开发者能够创建复杂的NLP应用。\n* **广泛采用**：作为最受欢迎的框架之一，LangChain拥有一个**蓬勃发展的社区**和出色的支持，提供丰富的文档和教程。\n\n### 示例用例：\n\n想象一下，您正在构建一个 **聊天机器人**，它首先理解用户的问题，从数据库中检索相关信息，然后生成响应。使用 LangChain，您可以轻松地以编程方式创建这个多步骤的过程，确保链中的每一步协调工作。\n\n### 何时使用 LangChain：\n\n如果您是一个 **构建生产级应用的开发者**，并且需要一个 **灵活、以代码为中心的解决方案**，LangChain 是您的最佳选择。它非常适合那些希望控制应用架构并且能舒适地编写代码来定义工作流程的开发者。\n\n**关键点**：\n\n* 如果您正在构建需要跨多个语言模型链式任务的生产级应用。\n* 如果您需要一个拥有广泛社区支持和多种集成的库。\n* 当您对编程解决方案更为熟悉，而非可视化工具。\n\n## 3. LangFlow: 无需编码/低代码的 LangChain 扩展\n\n**LangFlow** 本质上是 **LangChain 的可视化扩展**。它将 LangChain 强大的后端与 **直观的拖放界面** 结合在一起。LangFlow 使那些可能不太擅长编写代码的用户仍然能够在他们的应用程序中利用语言模型的强大功能。\n\n### 优势：\n\n* **可视化工作流创建**：与 LangGraph 类似，LangFlow 提供了一个可视化界面用于构建工作流。然而，它是基于 LangChain 构建的，这意味着用户可以利用 LangChain 的强大功能，而无需编写大量代码。\n* **快速原型制作的理想选择**：LangFlow 非常适合快速 **原型化想法** 或构建概念验证应用程序。\n* **适合初学者**：它是一个很好的入门点，适合那些对编码不太熟悉但想要创建语言模型工作流的用户。\n\n### 示例用例：\n\n如果您想快速构建一个**摘要工具**来检索文档，您可以在LangFlow的界面中拖放组件，以创建一个完全功能的应用程序。这可以在几乎不编写代码的情况下完成。\n\n### 何时使用 LangFlow：\n\nLangFlow 非常适合 **非开发人员** 或 **快速原型设计**。如果您想快速实验 **LLM 工作流** 而不深入代码，这个工具可以让您轻松入门。\n\n**关键点**：\n\n* 如果您想快速原型设计 LLM 工作流而不编写代码。\n* 如果您对视觉编程感到舒适，但需要 LangChain 的灵活性。\n* 用于教育目的，帮助用户了解如何构建工作流。\n\n## 4. LangSmith: 监控与可观察性\n\n虽然其他工具专注于 **构建工作流程**，**LangSmith** 的设计目标是 **监控** 和 **调试** 语言模型应用。它提供了先进的可观察性功能，以跟踪您的工作流程和模型的性能，使其在生产环境中不可或缺。\n\n### 优势：\n\n* **深度可观察性**：LangSmith 允许开发者监控语言模型的性能，确保工作流程按预期运行。\n* **错误跟踪**：它在帮助开发者定位问题方面表现出色，使调试变得更加容易。\n* **性能洞察**：LangSmith 提供有关 **工作流程性能** 的洞察，帮助开发者优化他们的应用程序。\n\n### 示例用例：\n\n假设您已经部署了一个**客户服务聊天机器人**，该聊天机器人使用语言模型来回答问题。随着时间的推移，您会发现某些回答的准确性低于预期。LangSmith 可以帮助您追踪问题，通过提供对工作流程中每个决策点的可见性。\n\n### 何时使用 LangSmith：\n\n如果您在 **生产环境** 中部署应用程序，并且需要确保 **健壮性、可靠性和性能**，LangSmith 是一个不可或缺的工具。它在管理 **需要随着时间调试和优化的复杂系统** 时特别有用。\n\n**关键点**：\n\n* 如果您需要 LLM 工作流中的高级监控或调试能力。\n* 对于观察性对确保最佳模型性能至关重要的开发环境。\n* 如果您的重点是基于实时洞察改进和迭代 LLM 驱动的应用程序。\n\n## 哪个更适合你？\n\n* **使用 LangGraph** 如果你更喜欢基于图形的可视化工作流程来构建复杂的 LLM 任务。非常适合需要清晰和结构的用户。\n* **使用 LangChain** 如果你需要一个强大、灵活的解决方案来以编程方式创建语言模型应用。它多功能且非常适合构建生产级应用的开发者。\n* **使用 LangFlow** 如果你想要 LangChain 的强大功能，同时又希望拥有一个可视化的无代码/低代码界面。最适合快速原型开发和更喜欢可视化工具而非编码的用户。\n* **使用 LangSmith** 如果你的重点是 LLM 应用的可观察性和调试。非常适合在开发或生产环境中监控和优化工作流程。\n\n最终，你的选择取决于你对代码的舒适度、工作流程的复杂性，以及你是否优先考虑易用性、灵活性或可观察性。\n\n## 结论\n\n这些工具 — **LangGraph**、**LangChain**、**LangFlow** 和 **LangSmith** — 针对开发和管理语言模型应用的不同阶段。**LangGraph** 提供了一种可视化、直观的方式来构建复杂的工作流程，而 **LangChain** 则为希望创建可扩展应用的开发者提供了一种强大的代码优先解决方案。对于那些更喜欢 **低代码**、拖放方式的用户，**LangFlow** 在不牺牲功能的情况下简化了流程。最后，**LangSmith** 专注于可观察性和调试，确保您的工作流程是优化和可靠的。选择合适的工具取决于您的项目需求，无论是快速原型设计、生产级扩展，还是监控和性能跟踪。\n\n快乐编码！ 🎉\n\n👨🏾‍💻 [GitHub](https://github.com/mdmonsurali) ⭐️ | 👔[LinkedIn](https://www.linkedin.com/in/mdmonsurali/) |📝 [Medium](https://medium.com/@monsuralirana)\n\n感谢您花时间阅读这篇文章！\n\n请务必留下您的反馈和评论。下次博客见，敬请关注 📢\n\n## 参考文献：\n\n1. “LangChain 文档” — <https://python.langchain.com/docs/introduction/>\n2. “LangGraph 概述” — <https://langchain-ai.github.io/langgraph/>\n3. “LangFlow GitHub 仓库” — [https://github.com/LangFlow/LangFlow](https://docs.langflow.org/)\n4. “LangSmith 介绍” — <https://www.langchain.com/langsmith>\n5. “如何使用 LangChain 构建聊天机器人” by JetBrains 博客 — <https://blog.jetbrains.com/pycharm/2024/08/how-to-build-chatbots-with-langchain/>\n\n"},{"lang":"zh","group":"blog","slug":"blog/o1-preview-vs-claude-3-5-sonnet-comparing-top-llms-d68734b53c93","frontmatter":{"title":"o1-preview 与 claude-3.5-sonnet：比较顶级法学硕士","meta_title":"o1-preview 与 claude-3.5-sonnet：比较顶级法学硕士","description":"了解 OpenAI 的 o1 预览版与 Claude 3.5 Sonnet 在性能、速度和功能方面的比较。","date":"2024-10-27T13:58:01.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kTWAcpRdOpsrFIDZjjjr7Q.jpeg","categories":["Programming","Machine Learning","Generative AI"],"author":"Rifx.Online","tags":["o1-preview","Claude","throughput","latency","reasoning"],"draft":false,"slug":"blog/o1-preview-vs-claude-3-5-sonnet-comparing-top-llms-d68734b53c93"},"content":"\n\n\n今天（2024年9月12日），OpenAI 发布了其最新的语言模型 o1-preview。这个先进的模型经过设计，能够在生成响应之前投入更多时间进行处理，使其能够更好地应对复杂任务，并在科学、编码和数学等领域解决具有挑战性的问题。\n\n在这篇博客文章中，我们将深入分析 o1-preview，并将其与之前被认为是最先进模型之一的 Claude 3.5 Sonnet 进行比较。\n\n\n\n## 比较方法论\n\n我们的分析利用了 [Keywords AI 的 LLM playground](https://docs.keywordsai.co/features/prompt/model-playground)，这是一个支持超过 200 种语言模型并提供函数调用功能的平台。我们将探讨以下几个方面：\n\n* 基本比较\n* 基准比较\n* 处理速度\n* 评估指标\n* 建议的使用案例\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*yc171ikejtBy_o11.jpeg)\n\n## 基本比较\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*z2FrS_AVig7Y6eU_.jpeg)\n\n注意：o1-preview 不支持流式传输、函数调用和系统消息。\n\n## 基准比较\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Bx_vAvFc9DAD0cZA.jpeg)\n\nO1-preview 在所有基准测试中都优于 Claude 3.5 Sonnet。最小的差距出现在 MMLU（一般知识）中。GPQA Diamond 测试研究生水平的推理，显示出显著的性能差异。MATH 基准揭示了最大的差距，突显了 o1-preview 的高级数学能力。这些结果表明，o1-preview 在复杂推理和各个领域的问题解决方面有了显著改善。\n\n## 速度比较\n\nO1-preview 的思考和响应时间比其他 LLM 更长。虽然直接的速度比较可能并不完全公平，但测试 o1-preview 的速度至关重要。这些信息帮助开发者更好地理解 o1-preview 的能力，并判断它是否适合他们的项目。注意：由于 o1-preview 不支持流式传输，我们已为两个模型禁用流式传输。因此，无法测量首次令牌时间（TTFT）。\n\n## 延迟\n\n我们的测试涉及每个模型数百个请求，揭示了显著的差异。Claude 3.5 Sonnet 的平均延迟为 18.3 秒/请求，而 o1-preview 的平均延迟为 39.4 秒/请求。o1-preview 显著更长的延迟是由于其延长的思考和推理过程。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*2PMkgPVuylFxwfIa.jpeg)\n\n## 吞吐量（每秒令牌数）\n\n尽管延迟较高，o1-preview的吞吐量更为出色。o1-preview生成92.94个令牌/秒，而Claude 3.5 Sonnet生成74.87个令牌/秒。这表明o1-preview较长的生成时间主要是由于其初始处理阶段，而非令牌生成速度。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*wxqpnwZhl9pnbw8y.jpeg)\n\n## 性能比较\n\n我们在[Keywords AI平台](https://keywordsai.co/)上进行了评估测试。评估包括三个部分：\n\n* **编码任务**：两个模型成功完成了前端和后端开发任务。O1-preview在处理较长上下文时表现更佳，能够在第一次尝试中更有效地识别和解决bug。它还展现了更全面的代码分析能力。\n* **逻辑推理**：O1-preview在推理任务中表现出色。它的思维过程与人类认知非常相似。虽然Claude 3.5 Sonnet在大多数问题上表现良好，但o1-preview始终能够解决复杂的推理挑战，包括国际数学奥林匹克（IMO）级别的问题。\n* **写作任务**：两个模型在写作任务上表现非常出色。它们展现了撰写真实、个性化的冷邮件以及简洁且有意义的博客文章的能力。\n\n## 模型推荐\n\no1-preview\n\n* **最佳选择：** 适用于数学、编码和物理学中的复杂问题解决。特别适合处理挑战性任务的研究人员。\n* **不适合：** 需要快速响应时间或严重依赖系统提示的AI应用。由于缺乏流媒体支持，不适用于语音AI应用。\n\nClaude 3.5 Sonnet\n\n* **最佳选择：** 适用于大多数需要问题解决能力和高质量内容生成的AI应用。\n* **不适合：** 语音AI应用或对预算限制严格、需要较低运营成本的项目。\n\n## 如何将 o1-preview 集成到您的 AI 应用中\n\n要将 o1-preview 集成到您的 AI 应用中，只需访问 Keywords AI 模型页面并找到“查看代码”按钮。点击此按钮以复制提供的代码片段，然后将其直接粘贴到您的代码库中。通过这个简单的过程，您将能够在项目中利用 o1-preview 的强大功能，使您能够轻松应对复杂问题并生成高质量内容。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*XyQ9QiI7TN8Uc5Jp.jpeg)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*t8fEYlEs13eM7D28lVbtIw.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*yhu9y5ixNuxeFVe1.png)\n\n此故事发布在 [Generative AI](https://generativeai.pub/)。请在 [LinkedIn](https://www.linkedin.com/company/generative-ai-publication) 上与我们联系，并关注 [Zeniteq](https://www.zeniteq.com/)，以便获取最新的 AI 资讯。\n\n订阅我们的 [newsletter](https://www.generativeaipub.com/) 和 [YouTube](https://www.youtube.com/@generativeaipub) 频道，及时了解生成 AI 的最新消息和动态。让我们共同塑造 AI 的未来！\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*PelNtaNaEVDWgMWr.png)\n\n"},{"lang":"zh","group":"blog","slug":"blog/qwen2-5-1-5b-the-future-of-mobile-ai-6bd5f29bbc84","frontmatter":{"title":"Qwen2.5 1.5b：移动AI的未来？","meta_title":"Qwen2.5 1.5b：移动AI的未来？","description":"阿里云最新 LLM 的本地测试和评估。使用 llama-cpp-python 和 DIY 提示目录。","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*awb56jkdXobA-Ip6d-QHRA.png","categories":["Programming","Technology","Science"],"author":"Rifx.Online","tags":["Qwen2.5","summarization","retrieval","mobile","improvements"],"draft":false,"slug":"blog/qwen2-5-1-5b-the-future-of-mobile-ai-6bd5f29bbc84"},"content":"\n### 本地测试和评估阿里云最新的 LLM。使用 llama-cpp-python 和 DIY 提示目录。\n\n\n\n在第一部分中，我们共同探讨了阿里云团队发布的 Qwen2.5 模型系列的创新。\n\n在生成式 AI 中，基准测试现在是主要的 *oracle*：一个新的 LLM 的有效性需要通过几个评判。你打破的基准记录越多，你就越优秀。\n\n这就是赢得 SOTA 竞赛的方式。\n\n好吧，我不同意。尽管我们在 AI 进步中需要里程碑和更好的表现，但用户体验和个人观点不能被视为无关紧要而被置于一旁。\n\n我相信，探索一些常用的 NLP 任务，并将聊天体验搁置一旁，我们必须关注回复的质量。而我们是唯一需要的基准。我们的用户体验是理解一个模型是否优秀的最佳指标。这个模型必须足够可靠，以便在自动化工作流程中使用。\n\n顺便提一下，我已经运行了我决定称之为 [RBYF — 以您为反馈的修订基准](https://open.substack.com/pub/thepoorgpuguy/p/rbyf-is-here-revised-benchmarks-with?r=i78xo&utm_campaign=post&utm_medium=web) 的测试，声称惊人的 Llama3.2–1B-instruct… 而 Qwen2.5–1.5b 要好得多！\n\n所以在这篇文章中，正如承诺的那样，我们将亲自验证这个模型在日常使用中的表现如何。\n\n回到我们自己……让我们开始吧！\n\n## 需求\n\n在这里，我们将构建一个最小的文本接口，以便能够运行模型、测试不同的任务并等待用户反馈以进行评估。\n\n需求很简单，但我建议您创建一个新的项目目录和一个虚拟环境。\n\n创建一个 `venv`（需要 Python 3.11+）：我在运行 Windows 11 的迷你电脑上进行了测试。\n\n```python\n## create the virtual environment\npython -m venv venv\n## activate the venv\nvenv\\Scripts\\activate\n## Install the dependencies \npip install llama-cpp-python==0.3.0 tiktoken\n```\n\n我们需要从 Hugging Face 的官方 qwen 存储库下载 GGUF 文件 [https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF)：我使用了 `qwen2.5-1.5b-instruct-q5_k_m.gguf` 版本。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*YtQJb_xyq_xcF40yRWPcZA.png)\n\n我们准备好了！\n\n注意：如果您想添加对 GPU 加速器的不同后端支持，可以按照 [存储库中的说明](https://github.com/abetlen/llama-cpp-python#supported-backends) 进行操作。我使用了 Vulkan 支持，因此在 pip 安装之前我添加了环境变量\n\n```python\n## Vulkan support - for Windows\n$env:CMAKE_ARGS = \"-DGGML_VULKAN=on\"\n```\n\n## 代码 — 主应用和库\n\n为了保持代码的简洁，我决定使用一个外部库来扩展一些功能。好吧，这是一个自己动手的库，所以这里没有秘密。\n\n你可以在我的文章中找到所有的细节：\n\n为了加快速度，你可以直接 [从这里下载文件](https://github.com/fabiomatricardi/YouAreTheBenchmark/raw/main/QWEN2.5-1.5B/promptLibv2Qwen.py)：它包含了在上述文章中讨论的 `promptLib` 的版本 2（它被称为 `promptLibv2Qwen.py`，并且对 `Qwen2.5-1.5B-instruct` 模型的提示进行了少量微调）。\n\n将文件保存在主目录中，并创建一个名为 `main.py` 的新文件。\n\n```python\n## Chat with an intelligent assistant in your terminal  \n## MODEL: https://huggingface.co/Qwen\n## qwen2.5-1.5b-instruct-q5_k_m.gguf\nimport sys\nfrom time import sleep\nimport warnings\nwarnings.filterwarnings(action='ignore')\nimport datetime\nfrom promptLibv2Qwen import countTokens, writehistory, createCatalog\nfrom promptLibv2Qwen import genRANstring, createStats\nimport argparse\n### PREPARING FINAL DATASET\npd_id = []\npd_task = []\npd_vote = []\npd_remarks = []\n####################Add GPU argument in the parser###################################\nparser = argparse.ArgumentParser()\nparser.add_argument(\"-g\", \"--gpu\", type=int, default=0,nargs='?',\n                    help=\"The number of layers to load on GPU\")\nargs = parser.parse_args()\nif args.gpu == None:\n   ngpu_layers = 0 \nelse:\n    ngpu_layers = args.gpu\nprint(f'Selected GPU: offloading {ngpu_layers} layers...')   \n####################INITIALIZE THE MODEL###################################\nstops = ['\n\n## 提示目录 — 我们想要测试的内容\n\n我在这里写了我的习惯。我有一个提示目录，涵盖了聊天机器人中使用的许多主要语言任务，如摘要、简短摘要、随意聊天、RAG、真实RAG等。\n\n这个想法是能够在5分钟内加载模型，并开始评估每个任务。在每次生成结束时，用户会被提示给出一个分数（从0到5的评分）并在需要时留下任何评论。\n\n这非常重要：并不是所有模型都是一样的，对提示中的措辞进行小的或大的调整总是必要的。\n\n所以回到代码……因为之前的代码只是热身，现在将开始真正的while循环，遍历整个提示目录。请看下面的工作流程……\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*EL0Q97Du6HwtcYQZ.png)\n\n代码中只有少量更改，我会指出这些更改，请耐心等待。\n\n```python\n############################# AUTOMATIC PROMPTING EVALUATION  11 TURNS #################################\nid =1\nfor items in tasks:\n    fisrtround = 0\n    task = items[\"task\"]\n    prompt = items[\"prompt\"]\n    test = []\n    print(f'NLP TAKS>>> {task}')\n    print(\"\\033[91;1m\")  #red\n    print(prompt)\n    test.append({\"role\": \"user\", \"content\": prompt})\n    print(\"\\033[92;1m\")\n    full_response = \"\"\n    start = datetime.datetime.now()\n    print(\"💻 > \", end=\"\", flush=True)\n    for chunk in llm.create_chat_completion(\n        messages=test,\n        temperature=0.15,\n        repeat_penalty= 1.31,\n        stop=stops,\n        max_tokens=1500,\n        stream=True,):\n        try:\n            if chunk[\"choices\"][0][\"delta\"][\"content\"]:\n                if fisrtround==0:\n                    print(chunk[\"choices\"][0][\"delta\"][\"content\"], end=\"\", flush=True)\n                    full_response += chunk[\"choices\"][0][\"delta\"][\"content\"]\n                    ttftoken = datetime.datetime.now() - start  \n                    fisrtround = 1\n                else:\n                    print(chunk[\"choices\"][0][\"delta\"][\"content\"], end=\"\", flush=True)\n                    full_response += chunk[\"choices\"][0][\"delta\"][\"content\"]                            \n        except:\n            pass      \n    delta = datetime.datetime.now() - start\n    print('')\n    print(\"\\033[91;1m\")\n    rating = input('Rate from 0 (BAD) to 5 (VERY GOOD) the quality of generation> ')\n    print(\"\\033[92;1m\")\n    stats = createStats(delta,prompt,full_response,rating,logfilename,task,ttftoken)\n    print(stats)\n    writehistory(logfilename,f'''👨‍💻 > {prompt}\n💻 > {full_response}\n{stats}\n''')\n    pd_id.append(id)\n    pd_task.append(task)\n    pd_vote.append(rating[:2])\n    pd_remarks.append(rating[2:])\n    id += 1\n## create dataframe and save to csv\nzipped = list(zip(pd_id,pd_task,pd_vote,pd_remarks))\nimport pandas as pdd\ndf = pdd.DataFrame(zipped, columns=['#', 'TASK', 'VOTE','REMARKS'])\n#saving the DataFrame as a CSV file \ndf_csv_data = df.to_csv(csvfile, index = False, encoding='utf-8') \nprint('\\nCSV String:\\n', df_csv_data)  \n```\n\n主要的更改仅在前几行：\n\n```python\nfor items in tasks:\n    fisrtround = 0\n    task = items[\"task\"]\n    prompt = items[\"prompt\"]\n```\n\n如果你读过关于 `promptLib` 的文章，你应该不会感到惊讶：但如果你是新手，这里我们正在遍历一个字典列表，结构如下：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*rGcKJWNzSUrcu4wi.png)\n\n因此，对于目录中的每个项目（表示任务和提示的一对），我们提取任务描述和任务的提示。\n\n```python\ntest.append({\"role\": \"user\", \"content\": prompt})\n```\n\n然后我们在一个名为 `test` 的临时列表中创建聊天模板消息，并将其传递给 `create_chat_template()` 方法以进行生成。\n\n其他部分保持不变。\n\n保存文件，并在激活的 `venv` 下运行：\n\n```python\npython main.py\n## 如果你使用的是GPU python main.py -g\n```\n\n这将得到类似于下面示例的内容……\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MhhQu4lLjtU__Wjf0dSWBg.gif)\n\n请注意，在整个提示目录结束时，会创建一个 *csv* 文件，其中包含所有任务的摘要！\n\n## 测试概述\n\n我使用了几个小型语言模型，从 [Qwen2–1.5B-instruct](https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF) 到 [Gemma2–2B-instruct](https://huggingface.co/bartowski/gemma-2-2b-it-GGUF)，再到 [Llama3.2–1B-instruct](https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF)，最后是新的 [Qwen2.5–1.5B-instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF)。\n\n虽然我对 [Llama3.2–1B-instruct](https://generativeai.pub/llama3-2-1b-instruct-is-ok-but-not-good-enough-28f88046b63e) 感到相当失望，但我对新的 [Qwen2.5–1.5B-instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF) 的出色表现感到惊讶。\n\n在每次生成结束时，用户会被要求用0到5的分数来评估结果。**在这种情况下，用户就是我……**\n\n这种定性分析确实较为简单，因此每个分数都有描述，用户可以添加评论（“一些错误信息”，“可能更好地更改提示中的措辞”）\n\n这里是带有描述的定性矩阵\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*eBdPfZtfr99MsvLh6tt42w.png)\n\n## 好与坏 — 细节\n\n总结效果非常好。列出长文本的主要主题也非常不错。\n\nRAG 任务的速度相当快（即使在我的迷你电脑上），真实的 RAG（在上下文外提问）也很到位。\n\n所以真的很好。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*DuV3LJep_PuDqiCcAMb6Cg.png)\n\n当然也有一些不足之处：即使温度仅为 `0.15`，在总结两句话的任务中，我也得到了些虚假的信息。这不好。\n\n> 我期待这可以通过稍微调整提示，或将温度设置为 `0` 来修复。\n\n另一个事实是，创意写作的效果相当差：在测试中我使用了一些固定的生成参数。\n\n```python\n        temperature=0.15,\n        repeat_penalty= 1.31,\n```\n\n可能对于创意写作，使用 Qwen2.5–1.5B-instruct 时，我们应该使用更高的 `repeat_penalty` 和更高的 `temperature`。\n\n顺便提一下，我必须说反思提示的效果也并不差！标签的开闭没有保持（所以不容易将其放入管道或工作流中），但整体生成的流程和“思维链”推理过程相当不错。\n\n```python\n<thinking>\n使用生成型人工智能（AI）在教学中的重要性不容小觑，因为这项技术不仅在当前的教育实践中具有巨大的潜力，还能塑造更具创新性的终身学习方法。\n</thinking>\n\n**思维链：**\n1. **理解影响**：AI 可以自动化重复性任务，并根据学生的表现数据提供个性化反馈（例如，通过聊天机器人或自适应评估）。\n2. **增强学习体验**：通过整合生成型 AI，教师可以创建更具吸引力的课程，以满足不同的学习风格。\n3. **改善学生成果**：个性化指导可以提高学生对知识的理解和记忆。\n\n**反思：**\n- 在教育中整合生成技术至关重要，因为它与日益成为数字原住民的学习者不断变化的需求相一致，要求采用包含这些技术的教学方法。\n</thinking>\n\n<output>\n教师应学习如何利用生成型 AI 来增强他们的教学方法并改善学生成果。这包括了解其在自动化任务（如评分或创建个性化学习路径）方面的能力。\n\n**学习路径：**\n1. **教育技术基础课程**：从提供教育技术概述的课程开始，包括数字工具的基础知识。\n2. **AI 基础课程（Coursera）** - 通过 Coursera 的人工智能教育者免费在线课程，了解 AI 概念及其在教育中的应用。\n\n3. **生成学习的专业课程**：报名参加专注于生成学习的课程，例如“生成设计”或类似的由 MIT 等大学提供的专业项目。\n4. **专业发展研讨会和会议** - 参加地方教育会议上专注于将 AI 整合到教学实践中的研讨会。\n\n5. **教育工作者在线社区（如 Edmodo）**：加入在线社区，教育工作者讨论生成技术在教育中的应用，分享资源或询问实施问题。\n6. **认证项目**：考虑获得认证，以认可您在教学实践中与 AI 整合相关的知识和技能，例如 Google 的“教育者 AI”项目。\n\n通过遵循这条学习路径，教师不仅可以提升自身的专业发展，还可以通过有效整合生成技术为教育的未来做出积极贡献。\n</output>\n```\n\n我认为对于这个小型语言模型的规模来说，结果并不差！\n\n## 比较 Qwen2.5 和 Llama3.2\n\n尽管这只是我个人的评估，我想与您分享。\n\n这两个模型都是为移动设备设计的，但性能差异很大。请看下面：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*T6vLgvOKdkotlV1K5x6-QQ.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*DuV3LJep_PuDqiCcAMb6Cg.png)\n\n首先，总体评分差异巨大（Llama3.2 为 41，Qwen2.5 为 57）。\n\n其次，如果考虑您在移动设备上可能提出的语言任务，主要是希望有流畅的聊天体验（任务 4）、良好的摘要能力（任务 5 到 7）和一些创造性写作（任务 11 和 13）。\n\n在速度方面，仅在 CPU 上运行模型，使用非常有限的迷你 PC，**我获得了平均推理速度为 14 t/s。**\n\n## 结论\n\n在Qwen2发布的过去三个月里，众多开发者在Qwen2语言模型上构建了新模型，为整个社区以及阿里云提供了宝贵的反馈。\n\n> 在此期间，我们专注于创建更智能、更知识丰富的语言模型。今天，我们很高兴地介绍Qwen家族的最新成员：Qwen2.5\n\n他们的声明伴随着关于新模型家族的事实：\n\n* 密集型、**易于使用**的仅解码语言模型，提供0.5B、1.5B、3B、7B、14B、32B和72B的不同尺寸，以及基础和指令变体。\n* 在我们最新的大规模数据集上进行预训练，涵盖多达18T的标记。\n* 在**指令遵循**方面有显著改善。\n* 对系统提示的多样性**更具韧性**，增强了角色扮演的实施和聊天机器人的条件设置。\n* **支持高达128K**的上下文长度，并能生成最多8K的标记。\n* 支持29种以上的多语言。\n\n在我广泛（但确实仅限于一次性提示和少量NLP任务）的测试中，我亲眼看到这些声明是基于高质量的训练数据集和精心调整的结果。\n\n该模型在移动设备上的表现极为出色！\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/rag-llm-and-pdf-conversion-to-markdown-text-with-pymupdf-03af00259b5d","frontmatter":{"title":"RAG/LLM 和 PDF：使用 PyMuPDF 转换为 Markdown 文本","meta_title":"RAG/LLM 和 PDF：使用 PyMuPDF 转换为 Markdown 文本","description":"采用 markdown 文本格式输入数据可提高生成的文本质量","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*swPjVuudAhsoRiiw3Ee32w.png","categories":["Programming","Technology","Technology/Web"],"author":"Rifx.Online","tags":["markdown","PyMuPDF","LLM","RAG","PDF"],"draft":false,"slug":"blog/rag-llm-and-pdf-conversion-to-markdown-text-with-pymupdf-03af00259b5d"},"content":"\n\n\n### 以Markdown文本格式输入数据可以提高生成文本的质量\n\n\n\n## 介绍\n\n在**大型语言模型（LLMs）**和**检索增强生成（RAG）**环境中，以**markdown文本格式**输入数据具有**重要意义**。以下是一些详细考虑因素。\n\n**LLMs** 是强大的语言模型，可以生成连贯且具有上下文相关性的文本。然而，它们有时可能会产生缺乏事实准确性或上下文的响应。通过结合基于检索的方法（如RAG），我们可以提高生成文本的质量。\n\n**RAG** 使得将**外部数据**——在LLM的训练数据中之前缺失的数据——整合到文本生成过程中成为可能。这种包含减少了“幻觉问题”，并增强了文本响应的相关性。\n\n## 为什么选择 Markdown 用于 LLM？\n\n**Markdown** 是一种轻量级标记语言，允许用户使用简单的语法格式化纯文本。它广泛用于创建结构化文档，特别是在 GitHub、Jupyter 笔记本和各种内容管理系统上。当将数据输入到 LLM 或 RAG 系统时，使用 Markdown 格式提供了几个好处：\n\n1. **结构化内容**：Markdown 允许您将信息组织成标题、列表、表格和其他结构化元素。这种结构有助于更好地理解和上下文保留。\n2. **富文本**：Markdown 支持基本格式，如粗体、斜体、链接和代码块。在输入数据中包含富文本可以增强语言模型的上下文。\n3. **嵌入链接和引用**：Markdown 允许您嵌入超链接、脚注和引用。在 RAG 场景中，这对于引用外部来源或提供额外上下文至关重要。\n4. **易于创作**：Markdown 具有可读性，易于编写。作者可以高效地创建内容，而无需复杂的格式化工具。\n5. **分块**：对于 RAG 系统至关重要，分块（也称为“拆分”）将大量文档拆分为更易处理的部分。通过支持 MD 格式的 PyMuPDF 数据提取，我们支持分块以保持具有共同上下文的文本在一起。**重要的是，MD 格式的 PyMuPDF 提取允许进行 [第 3 级分块](https://readmedium.com/five-levels-of-chunking-strategies-in-rag-notes-from-gregs-video-7b735895694d#b123)**。\n\n总之，在 LLM 和 RAG 环境中使用 Markdown 文本格式可以确保更准确和相关的结果，因为它提供了更丰富的数据结构和更相关的数据块负载给您的 LLM。\n\n## PyMuPDF 支持 PDF 的 Markdown 转换\n\n自推出以来，PyMuPDF 一直能够从 PDF 页面中提取文本、图像、矢量图形，并且从 2023 年 8 月起，还能够提取表格。这些对象类型各自有其提取方法：文本有一种，表格、图像和矢量图形则有其他方法。为了满足 RAG 的要求，我们将这些不同的提取方式合并，生成一个统一的 **Markdown** 字符串，以一致地表示页面的整体内容。\n\n所有这些都实现为 [一个 Python 脚本](https://github.com/pymupdf/RAG/blob/main/helpers/pymupdf_rag.py)。它可以被其他脚本作为模块导入，或者在终端窗口中通过以下命令行调用：\n\n`$ python pymupdf_rag.py input.pdf [-pages PAGES]`\n\n它将生成一个 **Markdown** 格式的文本文件（称为 `input.md`）。可选参数 `PAGES` 允许将转换限制为 PDF 总页面的一个子集。如果省略，则处理整个 PDF。\n\n## Markdown 创建细节\n\n### 选择要考虑的页面\n\n“`-pages`” 参数是一个字符串，由所需的页面编号（从1开始）组成，用于考虑进行markdown转换。可以给出多个页面编号规范，使用逗号分隔。每个规范可以是一个整数或两个用“`-`”连接的整数，指定一个页面范围。以下是一个示例：\n\n“`-pages 1–10,15,20-N`”\n\n这将包括第1页到第10页、第15页以及第20页到文件末尾（大写“N”被视为最后一页的编号）。\n\n### 识别标题\n\n在调用时，程序检查给定页面上的所有文本并找出最常用的字体大小。该值（以及所有较小的字体大小）被假定为 **正文文本**。较大的字体大小被假定为 **标题文本**。\n\n根据它们在字体大小层级中的相对位置，标题文本将前面加上一个或多个 markdown 标题 `#` 标签字符。\n\n### 按页面区域识别处理模式\n\n每个页面上的所有文本首先将被分类为**标准**文本或**表格**文本。然后，页面内容将从上到下提取，并转换为Markdown格式。\n\n这最好通过一个例子来解释：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*u5fv2aAIvDaaAd6H.png)\n\n该页面显示的内容代表典型情况：\n\n* 两个表格，具有部分重叠的垂直位置。一个表格没有标题，另一个表格有**外部**列标题。\n* 有一行**标题**和多个级别的**标题**。\n* **正文文本**包含多种样式细节，如**粗体**、*斜体*和`行内代码`。\n* 有序和无序列表。\n* 代码片段。\n\n布局分析将确定三个区域并选择适当的处理模式：**(1)** 文本，**(2)** 表格，**(3)** 文本。\n\n生成的Markdown文本忠实地反映了上述内容——在这种格式中尽可能做到。\n\n作为一个例子，让我们看一下具有外部标题的表格的输出：\n\n```python\n|Column1|Column2|\n\n|---|---|\n\n|Cell (0, 0)|Cell (0, 1)|\n\n|Cell (1, 0)|Cell (1, 1)|\n\n|Cell (2, 0)|Cell (2, 1)|\n```\n这是与GitHub兼容的格式，具有最小的可能令牌大小——这是保持输入到RAG系统的小型化的重要方面。\n\n**列边框**由“`|`”字符表示。如果文本行后面跟着“`|---|---| …`”形式的行，则假定该文本行是**表头**。完整的**表格定义**必须前后至少有一行空行。\n\n请注意，由于技术原因，Markdown表格必须有一个标题，因此如果没有外部标题，将选择第一行作为表头。\n\n为了确认整体准确性，以下是Markdown解析器如何处理完整页面的示例：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Ge83uj7FiM4T6XFn)\n\n## 以编程方式调用 Markdown 转换器\n\n除了在命令行中执行程序外，Markdown 转换也可以通过程序请求：\n\n```python\nimport fitz\nfrom pymupdf_rag import to_markdown  # import Markdown converter\n\ndoc = fitz.open(“input.pdf”)  # open input PDF\n\n## define desired pages: this corresponds “-pages 1-10,15,20-N”\npage_list = list(range(9)) + [14] + list(range(19, len(doc) – 1))\n\n## get markdown string for all pages\nmd_text = to_markdown(doc, pages=page_list)\n\n## write markdown string to some file\noutput = open(“out-markdown.md”, “w”)\noutput.write(md_text)\noutput.close()\n```\n\n## 结论\n\n通过集成 PyMuPDF 的提取方法，PDF 页面的内容将被忠实地转换为可用作 RAG 聊天机器人的输入的 Markdown 文本。\n\n请记住，成功的 RAG 聊天机器人的关键在于它能够访问的信息的质量和完整性。\n\n启用 PyMuPDF 的 Markdown 提取确保从 PDF 中获取这些信息不仅是可能的，而且是简单的，展示了该库的强大和对开发者的友好。祝编码愉快！\n\n### 源代码\n\n* [RAG/helpers/pymupdf\\_rag.py (github.com)](https://github.com/pymupdf/RAG/blob/main/helpers/pymupdf_rag.py)\n\n### 参考文献\n\n* [5 Levels of Text Splitting](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb)\n\n### 相关博客\n\n* [使用 ChatGPT API 和 PyMuPDF 构建 RAG 聊天机器人 GUI](https://readmedium.com/building-a-rag-chatbot-gui-with-the-chatgpt-api-and-pymupdf-9ea8c7fc4ab5)\n* [使用 ChatGPT 和 PyMUPDF 创建 RAG 聊天机器人](https://readmedium.com/creating-a-rag-chatbot-with-chatgpt-and-pymupdf-f6c30907ae27)\n* [RAG/LLM 和 PDF：增强文本提取](https://readmedium.com/rag-llm-and-pdf-enhanced-text-extraction-5c5194c3885c)\n\n"},{"lang":"zh","group":"blog","slug":"blog/rbyf-qwen2-5-3b-instruct-is-damn-good-dcf443cacc63","frontmatter":{"title":"RBYF：Qwen2.5–3B-instruct 非常棒。","meta_title":"RBYF：Qwen2.5–3B-instruct 非常棒。","description":"修改后的基准测试并附上您的反馈：阿里巴巴Qwen的全新3B模型是一个了不起的模型，我可以证明这一点！","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*NWaBtJ64TLUoUHv4F1qJpg.png","categories":["Programming","Technology","Science"],"author":"Rifx.Online","tags":["Qwen","NLP","multimodal","RBYF","evaluation"],"draft":false,"slug":"blog/rbyf-qwen2-5-3b-instruct-is-damn-good-dcf443cacc63"},"content":"\n### 修订基准：以您为反馈的全新3B模型来自阿里巴巴Qwen，是个令人惊叹的模型，我可以证明这一点！\n\n\n\n涌现属性的错觉在很大程度上是评估这些模型所使用的指标的产物。这是一个事实。\n\n几周前，我决定做一个小反叛，放弃所有官方基准，开始自己做基准测试！\n\n这就是这个完全虚构的首字母缩略词RBYF的意义：以您为反馈的修订基准。其基本原则是，没有比您更好的评判者来验证一个大型语言模型的优劣。\n\n老实说，我专注于小型语言模型。我没有专用的GPU，计算资源有限。但我同样同意[LLMWare反叛原则第一条](https://readmedium.com/getting-work-done-with-genai-just-do-the-opposite-10-contrarian-rules-that-may-actually-work-634501602a27)：\n\n使用小模型，而不是大模型。\n\n在这篇文章中，我将向您展示我对qwen2.5–3b-instruct的评估结果。结果真的很好！\n\n> 免责声明：所有与结果相关的提示均可在我的GitHub存储库中找到：\n\n## 少即是多\n\n缩放法则描述了模型性能如何随着参数和训练数据数量的增加而改善。这个原则推动了对LLM新能力的探索。\n\n> 仅仅通过增加模型的规模，我们可以解锁新的能力……\n\n缩放法则描述了模型性能与参数数量和训练数据之间的关系。随着模型变得更大并在更多数据上进行训练，我们期待它们的性能得到提升。这导致了对越来越大LLM的无休止追求，希望能够解锁新的能力。\n\n涌现属性是指在复杂系统中个体组件之间的相互作用中产生的属性。通过孤立地研究组件，无法预测或理解这些属性。在LLM的情况下，希望随着这些模型变得更大和更复杂，它们会展现出意想不到的新能力。\n\n这是一个童话故事。\n\n在过去几周，我们亲眼目睹了经过过度训练和精心策划的小型语言模型可以表现得与它们的大型兄弟一样好。这对所谓的涌现能力是一个重击，反击了缩放法则。Gemma2–2B、Qwen2.5–3B，甚至最新的Llama3.2–3B都是远胜于旧的SOTA 7B模型的更好模型。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*-EjpdEky-Hf3WEQn.png)\n\n## Qwen2.5模型系列\n\n阿里云在九月中旬发布了他们的旗舰模型系列Qwen2.5。\n\n> 阿里云在Qwen的革命性旅程中再次展现出强大的创新领导力\n\nQwen2.5是阿里巴巴集团Qwen团队的大型语言模型和大型多模态模型系列。这些语言模型和多模态模型在大规模多语言和多模态数据上进行了预训练，并在高质量数据上进行了后训练，以便与人类偏好对齐。Qwen能够理解自然语言、生成文本、理解视觉、理解音频、使用工具、角色扮演、作为AI代理进行操作等。\n\n**新款Qwen2.5的亮点在于经过精心策划的训练数据集。** 通过检查小模型的性能，您可以清楚地理解这一点。\n\n如果该系列的小型语言模型表现良好，意味着训练和数据集经过高度修订和策划。\n\n以下是一些数字：\n\n* 密集型、易于使用的解码器语言模型，提供0.5B、1.5B、3B、7B、14B、32B和72B尺寸，以及基础和指令变体。\n* 在我们最新的大规模数据集上进行了预训练，涵盖多达18T个标记。\n* 在指令跟随、生成长文本（超过8K个标记）、理解结构化数据（例如，表格）和生成结构化输出（尤其是JSON）方面有显著改进。\n* 对系统提示的多样性更具韧性，增强了角色扮演的实现和聊天机器人的条件设置。\n* 支持的上下文长度可达128K个标记，并且可以生成多达8K个标记。\n* 支持超过29种语言的多语言，包括中文、英文、法文、西班牙文、葡萄牙文、德文、意大利文、俄文、日文、韩文、越南文、泰文、阿拉伯文等。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*3tnTS_UCImBRBDeKmFyjVQ.png)\n\n## Qwen2.5–3B-instruct\n\n尽管为了推动人工智能的发展，我们需要里程碑和更好的表现，但用户体验和个人观点不能被视为无关紧要。\n\n我认为，探索一些常用的自然语言处理任务，抛开聊天体验，我们必须关注回复的质量。而我们是唯一所需的基准。**我们的用户体验是理解模型好坏的最佳指标**。模型必须足够可靠，以便在自动化工作流程中使用。\n\n顺便提一下，我已经运行了我决定称之为[RBYF — 修订基准与您反馈](https://open.substack.com/pub/thepoorgpuguy/p/rbyf-is-here-revised-benchmarks-with?r=i78xo&utm_campaign=post&utm_medium=web)的项目，在[Qwen2.5–1.5b-instruct](https://ai.gopubby.com/qwen2-5-1-5b-the-future-of-mobile-ai-6bd5f29bbc84)上：您可以阅读详细信息。在文章中，我还解释了如何创建您的测试基准。所描述的方法与我用于Qwen2.5–3B的相同。\n\n让我们从所有任务的整体表现开始。该模型已由我评估（在这种情况下是我自己的反馈），基于以下显示的定性矩阵。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*rdVHfCDWX9jlvtiq)\n\n总体得分为62/70 = 8.8\n\n好的，但Qwen2.5–3B-instruct是基于什么得到了这个评估分数呢？\n\n## 测试概述\n\n这个想法是进行公平的用户反馈，而不是基于标准基准和框架的自动化反馈。一个小型语言模型能否满足用户在主要自然语言处理任务上的意图？\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*wgngfGvSebeoH3YxTdvc8A.png)\n\n我们希望验证用户意图和响应质量。以下是每个任务的详细信息：\n\n### 介绍\n\n验证模型如何回应初始问候并谈论自身。\n\n### 一句话解释\n\n综合与总结。最终评估的重点在于模型是否能够将回复压缩为仅一句话。\n\n### 用三段话解释\n\n用户的意图是获得对文本的智能解释，这种解释必须适合分成三段。SML通常会发现这很困难，因为他们总是会添加一个总结段落。\n\n### 说“我准备好了”。\n\n在一个基于聊天回合的应用中，遵循指令的模型通常会被要求首先阅读提供的文本，然后进行某种分析。通常情况下，SML 无法做到这一点……\n\n### 总结\n\n基本总结，没有限制。在这里，我们想评估总结是如何基于文本的，而不是凭空捏造事实。\n\n### 总结为两句\n\n基本总结，限制为两句话。在这里，我们希望评估总结是否基于文本，而不是虚构的事实，同时确保遵守两句的限制。\n\n### 列出三个主要关键点 — 格式输出\n\n```python\nkey_points = [\n    \"SML必须以特定格式输出。\",\n    \"此提示要求创建一个包含3个关键点的列表。\",\n    \"输出格式应为Python列表。\"\n]\n```\n\n### 目录\n\n这个任务对许多 SML 来说相当困难。该提示需要一些调整，否则模型会返回一个 markdown 表格。用户希望按照提供的文档结构获得一个有序的主题列表。\n\n### RAG\n\n检索增强生成，没有任何框架（haystack，Langchain…）。这是语言模型中最常用的任务之一。回复的评估基于对指令的理解能力以及答案与文本的相关性。\n\n### 真实的 RAG\n\n这是一个与提供的上下文完全无关的问题的 RAG。模型必须回复“无法回答”，这意味着它理解了指令，并且没有使用任何外部知识或虚构的信息。\n\n### 从参考文献撰写内容\n\n这是一个创造性的任务。使用参考文本，SML必须提供一篇新的文章。\n\n### 提取5个主题\n\n此任务的重点是验证：\n\n* 确保正好有5个主题\n* 它们是基于事实的（没有虚构）\n\n### 创意：1000字科幻故事\n\n完全创造性的任务。即使对于更大的模型来说，要保持连贯性并产生一个符合字数要求的小故事也是非常困难的。\n\n### 反思提示\n\n反思提示旨在验证模型的链式推理过程。输出被限制在特殊标签的开头/结尾。重点是推理和一致的输出结构。输出必须易于用于进一步的结构化提示或可视化。您可以在本文中阅读更多内容：\n\n## 评估过程\n\n在每次生成结束时，用户会被要求用0到5的分数来评估结果。**在这种情况下，用户就是我……**\n\n这种定性分析确实较为简单，因此每个分数都有一个描述，用户可以添加评论（“一些错误的信息”，“也许更改提示中的措辞会更好”）\n\n这里是带有描述的定性矩阵\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*H_Qx2UT1lxqy2BF4.png)\n\n## 好与坏 — 细节\n\n总体而言，Qwen2.5–3B-instruct 是一个全方位的模型。每个任务的平均得分为 4.4 分（满分 5 分），这相当令人印象深刻。\n\n摘要表现出色。在长文本中列出主要主题也非常好。\n\n它在 RAG 中表现突出，有时显得过于冗长：在真实的 RAG 中，它还提供了一些解释，以及 *无法回答* 回复的原因。\n\n我们可以一起看看。这对于 SML 来说，精确计算句子和段落的数量通常是一项艰巨的任务。\n\n```python\n**Write a two-sentences summary of the following text:**\n\nChina's regime under Chinese Communist Party operates without transparent \nelections and primary candidates. While undesirable for democracies, \nit allows long-term planning free from electoral pressures that can change \nplans abruptly; this has led to China achieving peak carbon dioxide emissions \nin 2023 due largely to increased renewable energy installations, potentially \nsetting a new global standard for sustainable development despite its status \nas one of the world's largest polluters.\n```\n\n目录对于语言模型来说是另一个噩梦，因为它们通常理解如何创建表格……但看看这个：\n\n```python\nA \"table of content\" provides an organized structure to the text, allowing \nreaders quick access to key points. Write the \"table of content\" of \nthe following text.\n\n[table of contents]\n1. Introduction: Overview and Critique on China's Governance System  \n2. Comparison with Western Democracies - Undesirable Dictatorship? \n3. Advantages in Long-Term Planning for Sustainability   \n4. The Chinese Model as a Comparative Advantage  \n5. Recent Achievements in Carbon Emissions Reduction\n6. Transition to Renewable Energy: A Turning Point?\n7. Potential Leadership Role of China and India  \n8. Conclusion - Future Directions\n\n```\n\n我给真实的 RAG 评分 4 分（满分 5 分），但它本可以轻松得到 5 分。最重要的是，回复中必须包含 *无法回答* 这个词。\n\n为什么？想象一下你在设置一系列提示，你需要根据先前的回复决定下一步。如果你可以简单地寻找 *无法回答* 这个词的存在来引导你的下一步，那就没问题了！\n\n```python\n**Reply to the question only using the provided context. If the answer \nis not contained in the provided context say \"unanswerable\".\nquestion: who is Anne Frank?**\n{context}\n**Remember: if you cannot answer based on the provided context, say \n\"unanswerable\"\nanswer:**\n\nUnanswerable\nThe given text does not provide any information about Anne Frank \nor her identity. The content is unrelated to this question and focuses \ninstead on discussing China's system compared to Western democracies in \nterms of long-term planning for environmental issues.\n```\n\n我想象模型也被训练来提供这种回复的原因……太棒了！\n\n### 优秀的反思\n\n反思提示的主要焦点是模型返回正确推理过程的能力，并按照几个特定的标签开闭格式化输出。\n\n到目前为止，这是**小型语言模型**中**最佳的反思输出**，甚至比Gemma2–2B更好：\n\n```python\n*👨‍💻 > You are an AI assistant designed to provide detailed, step-by-step \nresponses. Your outputs should follow this structure:\n1. Begin with a <thinking> section.\n2. Inside the thinking section:\n   a. Briefly analyze the question and outline your approach.\n   b. Present a clear plan of steps to solve the problem.\n   c. Use a \"Chain of Thought\" reasoning process if necessary, breaking down \nyour thought process into numbered steps.\n3. Include a <reflection> section for each idea where you:\n   a. Review your reasoning.\n   b. Check for potential errors or oversights.\n   c. Confirm or adjust your conclusion if necessary.\n4. Be sure to close all reflection sections.\n5. Close the thinking section with </thinking>.\n6. Provide your final answer in an <output> section.\nAlways use these tags in your responses. Be thorough in your explanations, \nshowing each step of your reasoning process. Aim to be precise and logical \nin your approach, and don't hesitate to break down complex problems into \nsimpler components. Your tone should be analytical and slightly formal, \nfocusing on clear communication of your thought process.\nRemember: Both <thinking> and <reflection> MUST be tags and must be closed \nat their conclusion\nMake sure all <tags> are on separate lines with no other text. \nDo not include other text on a line containing a tag.\n\nuser question: explain why it is crucial for teachers to learn how to use \ngenerative AI for their job and for the future of education. \nInclude relevant learning path for teachers and educators. *\n\n**💻 > <thinking>\nI will begin by analyzing the importance of teaching about Generative \nArtificial Intelligence (AI) in an educational context, then outline a \nstep-by-step plan on what steps should be taken to learn this technology \neffectively as well as provide resources or courses that could help.\nFirstly, understanding AI is crucial for teachers because it can \nsignificantly enhance their ability to support students. \nTeachers need the knowledge and skills necessary not only to integrate these tools into teaching but also understand how they work so they are able to guide both themselves and their pupils in ethical use of technology.\n</thinking>\n\n<reflection>\nMy initial thought process was clear, emphasizing that teachers should \nbe equipped with AI literacy as it can improve educational outcomes. \nI outlined the importance for guiding students ethically while using \nthese tools effectively.\nHowever, my plan needs more detail on specific steps to learn about \ngenerative AI and resources available online or in-person courses.\n</reflection>**\n*MY PERSONAL NOTE: here it is starting an additional thinking after the first \nreflection... And this is really too good to be true!!*\n**<thinking>\nTo create a comprehensive learning path that teachers could follow:\n1. **Understand Basic Concepts**: Start with basic concepts of machine \n   learning (ML) such as supervised vs unsupervised, deep neural networks  \n   etc., which are foundational to understanding generative AI.\n2. **Learn About Generative Models**:\n   - Study different types: Variational Autoencoders (VAEs), GANs.\n   - Understand how these models generate new data that resembles the \n     training set but is not identical and can be used for tasks like \n     text generation, image synthesis etc., which are crucial in education.\n3. **Explore Applications**: Look at real-world applications of generative \n   AI such as language modeling (e.g., GPT-2), computer vision techniques to \n   create realistic images or videos from raw data.\n4. **Ethical Considerations and Implications for Education**:\n   - Discuss the ethical implications, including bias in models' outputs \n     which could affect students’ perceptions about themselves if they are \n     exposed regularly through AI-generated content that is not representative \n     of them.\n5. **Hands-on Practice**: Engage with practical exercises using open-source \n   tools like TensorFlow or PyTorch to build simple generative model \n   applications.\n6. **Stay Updated and Network**:\n   - Follow the latest research in ML, especially on Generative Models \n     by following relevant journals (e.g., NeurIPS), conferences such as \n     ICML etc.\n7. Attend Workshops/Conferences: Participate actively at AI-related workshops \n   or events to network with professionals working within this field.\n</thinking>**\n*MY PERSONAL NOTE: and no surprise, an additional reflection now starts, \nbefore producing the final output*\n**<reflection>\nMy plan has been expanded and includes a more detailed approach for teachers, \ncovering foundational knowledge through practical application of generative \nmodels in education along with ethical considerations that are crucial. \nI have also included staying updated on the latest research as an important \npart.\nHowever, it's essential to ensure these resources or courses exist so \neducators can access them easily.\n</reflection>\n\n<output>\nIt is vital for teachers and educational professionals alike to learn how \ngenerative AI works because this technology has significant potential in \nenhancing teaching methods. By understanding basic concepts of machine \nlearning (ML) such as supervised vs unsupervised, deep neural networks etc., \nthey will be better equipped with the knowledge necessary not only to \nintegrate these tools into their curriculum but also guide students ethically.\nA comprehensive plan for teachers and educators includes:\n1. **Understand Basic Concepts**: Start by grasping foundational concepts of ML.\n2. **Learn About Generative Models**:\n   - Study different types: Variational Autoencoders (VAEs), GANs, etc., \n     which are crucial in generating new data that resembles the training \n     set but is not identical and can be used for tasks like text generation or \n     image synthesis.\n3. **Explore Applications**: Look at real-world applications of generative AI \n   such as language modeling using tools like TensorFlow.\n4. **Ethical Considerations**:\n   - Discuss ethical implications, including bias in models' outputs which \n     could affect students’ perceptions about themselves if they are exposed \n     regularly through AI-generated content that is not representative.\n5. **Hands-on Practice**: Engage with practical exercises to build simple \n   generative model applications using open-source tools.\n6. **Stay Updated and Network**:\n   - Follow the latest research in ML, especially on Generative Models by \n     following relevant journals (e.g., NeurIPS), conferences such as ICML etc.\n7. Attend Workshops/Conferences: Participate actively at AI-related workshops \n   or events to network with professionals working within this field.\n</output>\n```\n\n这样的精炼思维过程，以及清晰的标签开闭格式，可以方便后续的处理流程，或者在漂亮的GUI中进行美观的打印。\n\n## 它不擅长什么？\n\n最糟糕的任务是创作短篇小说。模型在生成过程中开始重复相同的段落。\n\n这并不意味着它无法做到。更有可能的是，通过提高温度和重复惩罚，可以获得良好的结果。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GNJ3lG6FM5qt9glIdf7qhQ.jpeg)\n\n## 结论\n\n您可以在我的GitHub仓库中找到所有聊天记录，以及代码和自行操作的说明。您可以参考我之前文章中的教程 [Qwen2.5 1.5b: 移动AI的未来？](https://ai.gopubby.com/qwen2-5-1-5b-the-future-of-mobile-ai-6bd5f29bbc84)\n\n在接下来的文章中，我将介绍其他小型语言模型，使用相同的原则：从小型的350M参数，到500M系列，再到3B — 经过1.5B。\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/the-6-best-llm-tools-to-run-models-locally-eedd0f7c2bbd","frontmatter":{"title":"6 种最佳本地运行模型的 LLM 工具","meta_title":"6 种最佳本地运行模型的 LLM 工具","description":"运行大型语言模型 (LLM)（例如 ChatGPT 和 Claude）通常涉及将数据发送到由 OpenAI 和其他 AI 模型管理的服务器……","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*2MB6-INUUGLR0NR_iOACIg.jpeg","categories":["Technology","Programming","Health"],"author":"Rifx.Online","tags":["LLM","local","deployment","customization","telehealth"],"draft":false,"slug":"blog/the-6-best-llm-tools-to-run-models-locally-eedd0f7c2bbd"},"content":"\n\n\n\n\n运行大型语言模型（LLMs）如 [ChatGPT](https://openai.com/chatgpt/mac/) 和 [Claude](https://claude.ai/) 通常涉及将数据发送到由 [OpenAI](https://openai.com/) 和其他 AI 模型提供商管理的服务器。虽然这些服务是安全的，但一些企业更倾向于将其数据完全离线，以获得更高的隐私保护。\n\n本文将介绍开发人员可以使用的六款工具，以便在本地运行和测试 LLM，确保他们的数据永远不会离开他们的设备，这类似于 [端到端加密](https://getstream.io/blog/end-to-end-encryption/) 保护隐私的方式。\n\n## 为什么使用本地 LLM？\n\n像 [LM Studio](https://lmstudio.ai/) 这样的工具在用户使用它来运行本地 LLM 时，不会收集用户数据或跟踪用户的行为。它允许所有聊天数据保留在本地计算机上，而不与 AI/ML 服务器共享。\n\n* **隐私**：您可以以多轮的方式提示本地 LLM，而不会让您的提示数据离开本地主机。\n* **自定义选项**：本地 LLM 提供 CPU 线程、温度、上下文长度、GPU 设置等高级配置选项。这类似于 OpenAI 的游乐场。\n* **支持和安全性**：它们提供与 OpenAI 或 Claude 相似的支持和安全性。\n* **订阅和费用**：这些工具免费使用，并且不需要每月订阅。对于像 OpenAI 这样的云服务，每个 API 请求都需要付费。本地 LLM 有助于节省费用，因为没有每月订阅。\n* **离线支持**：您可以在离线时加载和连接大型语言模型。\n* **连接性**：有时，连接到像 OpenAI 这样的云服务可能会导致信号和连接不良。\n\n## 六大免费本地 LLM 工具\n\n根据您的具体使用案例，您可以选择几种离线 LLM 应用程序。这些工具中有一些完全免费供个人和商业使用。其他工具可能需要您发送请求以用于商业用途。对于 Mac、Windows 和 Linux，有多种本地 LLM 工具可供选择。以下是您可以选择的六个最佳工具。\n\n## 1. LM Studio\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*svbQPZKu08of7Kv6)\n\n[LM Studio](https://lmstudio.ai/) 可以运行任何格式为 `gguf` 的模型文件。它支持来自模型提供商的 `gguf` 文件，如 [Llama 3.1](https://llama.meta.com/)、[Phi 3](https://huggingface.co/docs/transformers/main/en/model_doc/phi3)、[Mistral](https://mistral.ai/) 和 [Gemma](https://ai.google.dev/gemma)。要使用 LM Studio，请访问上述链接并下载适合您机器的应用程序。启动 LM Studio 后，主页会展示可下载和测试的顶级 LLM。还有一个搜索栏，可以筛选并下载来自不同 AI 提供商的特定模型。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*sbS3VqiLgDsftgs2)\n\n从特定公司的模型中搜索会显示多个模型，范围从小型到大型 [quantization](https://huggingface.co/docs/optimum/en/concept_guides/quantization)。根据您的机器，LM Studio 会使用兼容性猜测来突出显示适合该机器或平台的模型。\n\n## LM Studio 的关键特性\n\nLM Studio 提供与 ChatGPT 相似的功能和特性。它具有多个功能。以下是 LM Studio 的关键特性。\n\n* **模型参数自定义**：这允许您调整温度、最大令牌、频率惩罚等。\n* **聊天历史**：允许您保存提示以供后续使用。\n* **参数和用户界面提示**：您可以将鼠标悬停在信息按钮上以查找模型参数和术语。\n* **跨平台**：LM Studio 可在 Linux、Mac 和 Windows 操作系统上使用。\n* **机器规格检查**：LM Studio 检查计算机规格，如 GPU 和内存，并报告兼容的模型。这可以防止下载可能在特定机器上无法运行的模型。\n* **AI 聊天和游乐场**：以多轮聊天格式与大型语言模型进行对话，并通过同时加载多个 LLM 进行实验。\n* **开发者本地推理服务器**：允许开发者设置一个类似于 OpenAI API 的本地 HTTP 服务器。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*9bHmRiOSf6gm-u3P)\n\n本地服务器提供示例 Curl 和 Python 客户端请求。此功能有助于使用 LM Studio 构建 AI 应用程序，以访问特定的 LLM。\n\n```python\n## Example: reuse your existing OpenAI setup\nfrom openai import OpenAI\n\n## Point to the local server\nclient = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n\ncompletion = client.chat.completions.create(\n  model=\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"Always answer in rhymes.\"},\n    {\"role\": \"user\", \"content\": \"Introduce yourself.\"}\n  ],\n  temperature=0.7,\n)\n\nprint(completion.choices[0].message)\n```\n通过上述示例 Python 代码，您可以重用现有的 OpenAI 配置，并将基本 URL 修改为指向您的本地主机。\n\n* **OpenAI 的 Python 库导入**：LM Studio 允许开发者导入 OpenAI Python 库，并将基本 URL 指向本地服务器（localhost）。\n* **多模型会话**：使用单个提示并选择多个模型进行评估。\n\n## 使用 LM Studio 的好处\n\n该工具可供个人免费使用，允许开发者通过应用内聊天用户界面和游乐场运行 LLM。它提供了一个华丽且易于使用的界面，带有过滤器，并支持连接到 OpenAI 的 Python 库，无需 API 密钥。公司和企业可以根据要求使用 LM Studio。然而，它需要 M1/M2/M3 Mac 或更高版本，或具有支持 [AVX2](https://edc.intel.com/content/www/us/en/design/ipla/software-development-platforms/client/platforms/alder-lake-desktop/12th-generation-intel-core-processors-datasheet-volume-1-of-2/009/intel-advanced-vector-extensions-2-intel-avx2/) 的处理器的 Windows PC。Intel 和 [AMD](https://www.amd.com/en/support/download/drivers.html) 用户仅限于使用 [v0.2.31](https://lmstudio.ai/) 中的 Vulkan 推理引擎。\n\n## 2. Jan\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*7YeH_48iFYB4lDRu)\n\n将 [Jan](https://jan.ai/) 理解为一个设计用于离线操作的开源版本的 ChatGPT。它由一个用户社区构建，秉持用户拥有的理念。Jan 允许您在设备上运行流行的模型，如 [Mistral](https://huggingface.co/models?other=mistral) 或 [Llama](https://huggingface.co/models?other=llama)，而无需连接互联网。使用 Jan，您可以访问远程 API，如 OpenAI 和 [Groq](https://groq.com/)。\n\n## Jan 的主要特点\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*ufyOE6QkcHw8X5U7)\n\nJan 是一款电子应用程序，其功能类似于 LM Studio。它通过将消费者设备转变为 AI 计算机，使 AI 变得开放和可访问。由于这是一个开源项目，开发者可以为其贡献代码并扩展其功能。以下是 Jan 的主要特点。\n\n* **本地**：您可以在设备上运行您喜欢的 AI 模型，而无需将其连接到互联网。\n* **即用模型**：下载 Jan 后，您将获得一组已安装的模型以供开始使用。也可以搜索特定模型。\n* **模型导入**：支持从 Hugging Face 等来源导入模型。\n* **免费、跨平台和开源**：Jan 完全免费，开源，并可在 Mac、Windows 和 Linux 上运行。\n* **自定义推理参数**：调整模型参数，如最大令牌、温度、流、频率惩罚等。所有偏好设置、模型使用和配置都保留在您的计算机上。\n* **扩展**：Jan 支持 [TensortRT](https://github.com/NVIDIA/TensorRT) 和 [Inference Nitro](https://huggingface.co/jan-hq/nitro-v1.2-e3) 等扩展，以自定义和增强您的 AI 模型。\n\n## 使用 Jan 的好处\n\nJan 提供了一个干净简单的界面来与 LLM 互动，并且将所有数据和处理信息保存在本地。它已经为您安装了超过七十个大型语言模型供您使用。这些现成的模型的可用性使得连接和与远程 API（如 OpenAI 和 Mistral）互动变得简单。Jan 还有一个很棒的 [GitHub](https://github.com/janhq/jan)、[Discord](https://discord.gg/FTk2MvZwJH) 和 [Hugging Face](https://huggingface.co/janhq) 社区，可以关注并寻求帮助。然而，像所有 LLM 工具一样，这些模型在 Apple Silicon Macs 上的运行速度比在 Intel 机器上更快。\n\n## 3. Llamafile\n\n[Llamafile](https://github.com/Mozilla-Ocho/llamafile) 由 [Mozilla](https://www.mozilla.org/en-US/?v=1) 支持，旨在通过快速的 [CPU 推理](https://huggingface.co/docs/transformers/en/perf_infer_cpu) 使开源 AI 对每个人都可访问，而无需网络连接。它将 LLM 转换为多平台的 [可执行链接格式](https://gist.github.com/x0nu11byt3/bcb35c3de461e5fb66173071a2379779) (ELF)。它提供了将 AI [集成](https://getstream.io/chat/solutions/ai-integration/) 到应用程序中的最佳选项之一，使您能够仅通过一个可执行文件运行 LLM。\n\n## Llamafile 的工作原理\n\n它旨在将权重转换为多个可执行程序，这些程序无需安装即可在 Windows、MacOS、Linux、Intel、ARM、FreeBSD 等架构上运行。在底层，Llamafile 使用 [tinyBLAST](https://github.com/ggerganov/llama.cpp/issues/5048) 在像 Windows 这样的操作系统上运行，而无需 SDK。\n\n## Llamafile 的关键特性\n\n* **可执行文件**：与 LM Studio 和 Jan 等其他 LLM 工具不同，Llamafile 只需一个可执行文件即可运行 LLM。\n* **使用现有模型**：Llamafile 支持使用现有的模型工具，如 Ollama 和 LM Studio。\n* **访问或创建模型**：您可以访问 OpenAI、Mistral、Groq 等流行 LLM。它还提供从头创建模型的支持。\n* **模型文件转换**：您可以将许多流行 LLM 的文件格式转换，例如，将 `.gguf` 转换为 `.llamafile` 只需一个命令。\n\n`llamafile-convert mistral-7b.gguf`\n\n## 开始使用 Llamafile\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*4PV1KsCZvvVKqFll)\n\n要安装 Llamafile，请访问 Huggingface 网站，从导航中选择 **Models**，然后搜索 **Llamafile**。您还可以从下面的 URL 安装您喜欢的 [量化](https://huggingface.co/docs/optimum/en/concept_guides/quantization) 版本。\n\n[`https://huggingface.co/Mozilla/Meta-Llama-3.1-8B-Instruct-llamafile/tree/m`ain](https://huggingface.co/Mozilla/Meta-Llama-3.1-8B-Instruct-llamafile/tree/main)\n\n**注意**：量化数字越大，响应越好。正如上图所示，本文使用 `Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile`，其中 `Q6` 代表量化数字。\n\n**步骤 1：下载 Llamafile**\n\n从上面的链接，点击任意下载按钮以获取您喜欢的版本。如果您在机器上安装了 [wget](https://www.gnu.org/software/wget/) 工具，您可以使用以下命令下载 Llamafile。\n\n`wget <https://huggingface.co/Mozilla/Meta-Llama-3.1-8B-Instruct-llamafile/blob/main/Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile>`\n\n您应该用您喜欢的版本替换 URL。\n\n**步骤 2：使 Llamafile 可执行**\n\n下载特定版本的 Llamafile 后，您应该通过导航到文件位置，使用以下命令使其可执行。\n\n`chmod +x Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile`**步骤 3：运行 Llamafile**\n\n在文件名之前添加一个点和斜杠 `./` 来启动 Llamafile。\n\n`./Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile`\n\nLlamafile 应用程序现在将在 `http://127.0.0.1:8080` 可用，以运行您的各种 LLMs。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*1xrwDPTfNgmEQDTx)\n\n## 使用 Llamafile 的好处\n\nLlamafile 通过使 LLM 容易被消费者 CPU 访问，帮助实现 AI 和 ML 的民主化。与其他本地 LLM 应用程序如 **Llama.cpp** 相比，Llamafile 提供了最快的提示处理体验，并在游戏电脑上表现更佳。由于其更快的性能，它是总结长文本和大型文档的绝佳选择。它完全离线运行并保护隐私，因此用户不会将数据分享给任何 AI 服务器或 API。像 Hugging Face 这样的机器学习社区支持 Llamafile 格式，使得搜索与 Llamafile 相关的模型变得容易。它还有一个出色的开源社区，进一步开发和扩展它。\n\n## 4. GPT4ALL\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*j3vNWWQZCVF5woo5)\n\nGPT4ALL 基于隐私、安全和无需互联网的原则构建。用户可以在 Mac、Windows 和 Ubuntu 上 [安装](https://www.nomic.ai/gpt4all)。与 Jan 或 LM Studio 相比，GPT4ALL 拥有更多的每月下载量、[GitHub Stars](https://github.com/nomic-ai/gpt4all) 和活跃用户。\n\n## GPT4ALL的主要特点\n\nGPT4All可以在主要消费硬件上运行LLM，例如Mac M系列芯片、AMD和NVIDIA GPU。以下是其主要特点。\n\n* **隐私优先**：将私人和敏感的聊天信息和提示仅保留在您的设备上。\n* **无需互联网**：它完全离线工作。\n* **模型探索**：此功能允许开发者浏览和下载不同类型的LLM进行实验。您可以从流行选项中选择大约1000个开源语言模型，如LLama、Mistral等。\n* **本地文档**：您可以让本地LLM访问您的敏感数据，使用本地文档如`.pdf`和`.txt`，数据不会离开您的设备，也无需网络。\n* **自定义选项**：它提供多个[聊天机器人](https://getstream.io/blog/llm-chatbot-docs/)调整选项，如温度、批处理大小、上下文长度等。\n* **企业版**：GPT4ALL提供企业套餐，具备安全性、支持和每台设备的许可证，将本地AI带入企业。\n\n## 开始使用 GPT4All\n\n要开始使用 GPT4All 在本地运行 LLMs，请[下载](https://www.nomic.ai/gpt4all)适合您操作系统的版本。\n\n## 使用GPT4ALL的好处\n\n除了Ollama，GPT4ALL在GitHub贡献者数量上最为显著，拥有约250000名每月活跃用户（根据<https://www.nomic.ai/gpt4all>）并且与其竞争对手相比。该应用收集有关使用分析和聊天分享的匿名用户数据。然而，用户可以选择加入或退出。使用GPT4ALL，开发者可以从其庞大的用户基础、GitHub和Discord社区中受益。\n\n## 5. Ollama\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*STAonWgWIsY6cgDR)\n\n使用 [Ollama](https://ollama.com/)，您可以轻松创建本地聊天机器人，而无需连接到像 OpenAI 这样的 API。由于一切都在本地运行，您无需支付任何订阅费或 API 调用费用。\n\n## Ollama 的关键特性\n\n* **模型自定义**：Ollama 允许您转换 `.gguf` 模型文件并使用 `ollama run modelname` 运行它们。\n* **模型库**：Ollama 拥有大量模型可供尝试，访问 [ollama.com/library](https://ollama.com/library)。\n* **导入模型**：Ollama 支持从 [PyTorch](https://pytorch.org/) 导入模型。\n* **社区集成**：Ollama 无缝集成到网页和桌面应用程序中，例如 [Ollama-SwiftUI](https://github.com/kghandour/Ollama-SwiftUI)、[HTML UI](https://github.com/rtcfirefly/ollama-ui)、[Dify.ai](https://github.com/rtcfirefly/ollama-ui) 和 [更多](https://github.com/ollama/ollama?tab=readme-ov-file#community-integrations)。\n* **数据库连接**：Ollama 支持多个 [数据平台](https://github.com/mindsdb/mindsdb/blob/main/mindsdb/integrations/handlers/ollama_handler/README.md)。\n* **移动集成**：像 [Enchanted](https://github.com/AugustDev/enchanted) 这样的 SwiftUI 应用将 Ollama 带入 iOS、macOS 和 visionOS。[Maid](https://github.com/Mobile-Artificial-Intelligence/maid) 也是一个跨平台的 Flutter 应用，能够本地处理 `.gguf` 模型文件。\n\n## 开始使用 Ollama\n\n要首次使用 Ollama，请访问 <https://ollama.com> 并下载适合您机器的版本。您可以在 Mac、Linux 或 Windows 上安装它。安装 Ollama 后，您可以在终端中使用以下命令检查其详细信息。\n\n`ollama`\n\n要运行特定的 LLM，您应该使用以下命令下载它：\n\n`ollama pull modelname`，其中 `modelname` 是您要安装的模型名称。请在 [GitHub](https://github.com/ollama/ollama) 上查看一些可供下载的示例模型。`pull` 命令也用于更新模型。一旦使用，仅会获取差异部分。\n\n例如，在下载了 `llama3.1` 后，在命令行中运行 `ollama run llama3.1` 将启动该模型。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*aglZm6h0BU6GAYkSl04XWA.gif)\n\n在上述示例中，我们提示 `llama3.1` 模型解决一个物理功和能量的问题。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*dNNQYpz1s2tz1pcn)\n\n## 使用 Ollama 的好处\n\nOllama 在 GitHub 上拥有超过 200 名贡献者，并且有活跃的更新。它拥有最多的贡献者，并且在上述其他开源 LLM 工具中更具可扩展性。\n\n## 6. LLaMa.cpp\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*KhsAUquhDZAHghxK)\n\n[LLaMa.cpp](https://github.com/ggerganov/llama.cpp) 是支持本地 LLM 工具（如 Ollama 等）的底层后端技术（推理引擎）。LLaMa.cpp 支持显著的大型语言模型推理，配置简单，并在各种硬件上提供出色的本地性能。它也可以在云端运行。\n\n## LLaMa.cpp 的主要特点\n\n* **设置**：它的设置非常简单。您只需一个命令即可安装。\n* **性能**：它在本地和云端的各种硬件上表现非常出色。\n* **支持的模型**：它支持流行的主要 LLM，如 [Mistral 7B](https://huggingface.co/mistralai/Mistral-7B-v0.1)、[Mixtral MoE](https://huggingface.co/models?search=mistral-ai/Mixtral)、[DBRX](https://huggingface.co/databricks/dbrx-instruct)、[Falcon](https://huggingface.co/models?search=tiiuae/falcon) 和 [其他许多模型](https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#description)。\n* **前端 AI 工具**：LLaMa.cpp 支持开源 LLM UI 工具，如 [MindWorkAI/AI-Studio](https://github.com/MindWorkAI/AI-Studio) (FSL-1.1-MIT)、[iohub/collama](https://github.com/iohub/coLLaMA) 等。\n\n## 使用 LLaMa.cpp 开始\n\n要运行您的第一个本地大型语言模型，请使用以下命令安装 llama.cpp：\n\n`brew install llama.cpp`\n\n接下来，从 Hugging Face 或其他来源下载您想要运行的模型。例如，从 Hugging Face 下载下面的模型并将其保存在您计算机上的某个位置。\n\n[`https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q4_K_M.g`guf](https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf)\n\n使用您喜欢的命令行工具，如终端，`cd` 进入您刚下载的 `.gguf` 模型文件的位置，并运行以下命令。\n\n```python\nllama-cli --color \\ \n-m Mistral-7B-Instruct-v0.3.Q4_K_M.ggufb \\ \n-p \"Write a short intro about SwiftUI\"\n```\n总之，您首先调用 LLaMa CLI 工具并设置颜色和其他标志。`-m` 标志指定您要使用的模型的路径。`-p` 标志指定您希望用来指示模型的提示。\n\n运行上述命令后，您将看到以下预览中的结果。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*4Al-j50vXUXLUfvxzBt6aw.gif)\n\n## 本地 LLM 的用例\n\n在本地运行 LLM 可以帮助开发人员深入了解其性能和工作原理。 本地 LLM 可以查询私有文档和技术论文，以便与这些文档相关的信息不会离开用于查询的设备，不会发送到任何云 AI API。 本地 LLM 在没有互联网的地方和网络信号较差的地方非常有用。\n\n在 [远程医疗环境](https://getstream.io/blog/telemedicine-app-development/) 中，本地 LLM 可以对患者文档进行排序，而无需出于隐私考虑将其上传到任何 AI API 提供商。\n\n## 评估本地运行的大型语言模型性能\n\n在本地使用大型语言模型之前，了解其性能对于获得所需的响应至关重要。有几种方法可以确定特定 LLM 的性能。以下是一些方法。\n\n* **训练**：该模型是基于什么数据集进行训练的？\n* **微调**：模型在多大程度上可以定制以执行特定任务，或者是否可以针对特定领域进行微调？\n* **学术研究**：该 LLM 是否有学术研究论文？\n\n要回答上述问题，您可以查看优秀的资源，如 [Hugging Face](https://huggingface.co/datasets) 和 [Arxiv.org](https://arxiv.org/)。此外，[Open LLm Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) 和 [LMSYS Chatbot Arena](https://chat.lmsys.org/?arena) 提供了各种 LLM 的详细信息和基准测试。\n\n## 本地 LLM 工具结论\n\n正如本文所讨论的，选择和使用本地大型语言模型的动机有很多。如果您不希望将数据集通过互联网发送给 AI API 提供商，则可以对模型进行微调，以执行 [远程医疗应用](https://getstream.io/chat/solutions/healthcare/) 中的特定任务。许多开源的图形用户界面（GUI）本地 LLM 工具，如 LLm Studio 和 Jan，提供直观的前端用户界面，以便在没有像 OpenAI 或 Claude 这样的订阅服务的情况下配置和实验 LLM。您还发现了各种强大的命令行 LLM 应用程序，如 Ollama 和 LLaMa.cpp，帮助您在本地运行和测试模型，而无需互联网连接。查看 Stream 的 [AI 聊天机器人](https://getstream.io/chat/solutions/ai-integration/) 解决方案，将 AI 聊天集成到您的应用中，并访问所有相关链接以了解更多信息。\n\n*最初发布于 [https://getstream.io](https://getstream.io/blog/best-local-llm-tools/).*\n\n"},{"lang":"fr","group":"blog","slug":"blog/post-1","frontmatter":{"title":"Comment créer une application avec des technologies modernes","meta_title":"","description":"Ceci est une méta-description","date":"2022-04-04T05:00:00.000Z","image":"/images/image-placeholder.png","categories":["french","Application","Data"],"author":"John Doe","tags":["nextjs","tailwind","react"],"draft":false,"slug":"blog/post-1"},"content":"\nPersonne ne veut même sortir un maquillage de l'urne des soins empoisonnés. C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\n## Design Créatif\n\nCar en guise de maquillage, l'urne du poison C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\n> Le client lui-même doit pouvoir poursuivre l'adipisicing. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n"},{"lang":"fr","group":"blog","slug":"blog/post-2","frontmatter":{"title":"Comment créer une application avec des technologies modernes","meta_title":"","description":"Ceci est une méta-description","date":"2022-04-04T05:00:00.000Z","image":"/images/image-placeholder.png","categories":["Technology","Data"],"author":"Sam Wilson","tags":["technology","tailwind"],"draft":false,"slug":"blog/post-2"},"content":"\nPersonne ne veut même sortir un maquillage de l'urne des soins empoisonnés. C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\n## Design Créatif\n\nCar en guise de maquillage, l'urne du poison C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\n> Le client lui-même doit pouvoir poursuivre l'adipisicing. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n"},{"lang":"fr","group":"blog","slug":"blog/post-3","frontmatter":{"title":"Comment créer une application avec des technologies modernes","meta_title":"","description":"Ceci est une méta-description","date":"2022-04-04T05:00:00.000Z","image":"/images/image-placeholder.png","categories":["Software"],"author":"John Doe","tags":["software","tailwind"],"draft":false,"slug":"blog/post-3"},"content":"\nPersonne ne veut même sortir un maquillage de l'urne des soins empoisonnés. C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\n## Design Créatif\n\nCar en guise de maquillage, l'urne du poison C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\n> Le client lui-même doit pouvoir poursuivre l'adipisicing. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n"},{"lang":"fr","group":"blog","slug":"blog/post-4","frontmatter":{"title":"Comment créer une application avec des technologies modernes","meta_title":"","description":"Ceci est une méta-description","date":"2022-04-04T05:00:00.000Z","image":"/images/image-placeholder.png","categories":["Architecture"],"author":"John Doe","tags":["silicon","technology"],"draft":false,"slug":"blog/post-4"},"content":"\nPersonne ne veut même sortir un maquillage de l'urne des soins empoisonnés. C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\n## Design Créatif\n\nCar en guise de maquillage, l'urne du poison C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\n> Le client lui-même doit pouvoir poursuivre l'adipisicing. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n"},{"lang":"en","group":"models","slug":"models/chatgpt-4o-latest","frontmatter":{"title":"OpenAI: ChatGPT-4o","meta_title":"OpenAI: ChatGPT-4o","description":"OpenAI: ChatGPT-4o","date":"2024-08-14T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text image 2 text"],"author":"openai","tags":["Chatbots","Generative AI","Machine Learning","Natural Language Processing"],"draft":false,"id":"chatgpt-4o-latest","context":128000,"input":0.000005,"output":0.000015,"img":0.007225,"request":0,"last_updated":"2024-08-14T00:00:00.000Z","slug":"models/chatgpt-4o-latest"},"content":"\nDynamic model continuously updated to the current version of [GPT-4o](/openai/gpt-4o) in ChatGPT. Intended for research and evaluation.\n\nNote: This model is currently experimental and not suitable for production use-cases, and may be heavily rate-limited.\n\n"},{"lang":"en","group":"models","slug":"models/claude-3-haiku","frontmatter":{"title":"Anthropic: Claude 3 Haiku","meta_title":"Anthropic: Claude 3 Haiku","description":"Anthropic: Claude 3 Haiku","date":"2024-03-13T00:00:00.000Z","image":"https://img.rifx.online/logo/anthropic.svg","categories":["text image 2 text"],"author":"anthropic","tags":["Programming","Machine Learning","Generative AI","Chatbots","Natural Language Processing"],"draft":false,"id":"claude-3-haiku","context":200000,"input":2.5e-7,"output":0.00000125,"img":0.0004,"request":0,"last_updated":"2024-10-24T11:54:59.000Z","slug":"models/claude-3-haiku"},"content":"\nClaude 3 Haiku is Anthropic's fastest and most compact model for\nnear-instant responsiveness. Quick and accurate targeted performance.\n\nSee the launch announcement and benchmark results [here](https://www.anthropic.com/news/claude-3-haiku)\n\n#multimodal\n\n"},{"lang":"en","group":"models","slug":"models/claude-35-sonnet","frontmatter":{"title":"Anthropic: Claude 3.5 Sonnet","meta_title":"Anthropic: Claude 3.5 Sonnet","description":"Anthropic: Claude 3.5 Sonnet","date":"2024-06-20T00:00:00.000Z","image":"https://img.rifx.online/logo/anthropic.svg","categories":["text image 2 text"],"author":"anthropic","tags":["Programming","Data Science","Computer Vision","Chatbots","Autonomous Systems"],"draft":false,"id":"claude-3.5-sonnet","context":200000,"input":0.000003,"output":0.000015,"img":0.0048,"request":0,"last_updated":"2024-10-24T11:45:46.000Z","slug":"models/claude-35-sonnet"},"content":"\nClaude 3.5 Sonnet delivers better-than-Opus capabilities, faster-than-Sonnet speeds, at the same Sonnet prices. Sonnet is particularly good at:\n\n- Coding: Autonomously writes, edits, and runs code with reasoning and troubleshooting\n- Data science: Augments human data science expertise; navigates unstructured data while using multiple tools for insights\n- Visual processing: excelling at interpreting charts, graphs, and images, accurately transcribing text to derive insights beyond just the text alone\n- Agentic tasks: exceptional tool use, making it great at agentic tasks (i.e. complex, multi-step problem solving tasks that require engaging with other systems)\n\n#multimodal\n\n"},{"lang":"en","group":"models","slug":"models/gemini-flash-15-8b","frontmatter":{"title":"Google: Gemini 1.5 Flash-8B","meta_title":"Google: Gemini 1.5 Flash-8B","description":"Google: Gemini 1.5 Flash-8B","date":"2024-10-03T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text image 2 text"],"author":"google","tags":["Programming","Natural Language Processing","Chatbots","Translation","Technology/Web"],"draft":false,"id":"gemini-flash-1.5-8b","context":1000000,"input":3.75e-8,"output":1.5e-7,"img":0,"request":0,"last_updated":"2024-10-03T00:00:00.000Z","slug":"models/gemini-flash-15-8b"},"content":"\nGemini 1.5 Flash-8B is optimized for speed and efficiency, offering enhanced performance in small prompt tasks like chat, transcription, and translation. With reduced latency, it is highly effective for real-time and large-scale operations. This model focuses on cost-effective solutions while maintaining high-quality results.\n\n[Click here to learn more about this model](https://developers.googleblog.com/en/gemini-15-flash-8b-is-now-generally-available-for-use/).\n\nUsage of Gemini is subject to Google's [Gemini Terms of Use](https://ai.google.dev/terms).\n\n"},{"lang":"en","group":"models","slug":"models/gemini-pro-15","frontmatter":{"title":"Google: Gemini Pro 1.5","meta_title":"Google: Gemini Pro 1.5","description":"Google: Gemini Pro 1.5","date":"2024-04-09T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text image 2 text"],"author":"google","tags":["Programming","Natural Language Processing","Machine Learning","Generative AI","Chatbots"],"draft":false,"id":"gemini-pro-1.5","context":2000000,"input":0.00000125,"output":0.000005,"img":0.00263,"request":0,"last_updated":"2024-04-09T00:00:00.000Z","slug":"models/gemini-pro-15"},"content":"\nGoogle's latest multimodal model, supporting image and video in text or chat prompts.\n\nOptimized for language tasks including:\n\n- Code generation\n- Text generation\n- Text editing\n- Problem solving\n- Recommendations\n- Information extraction\n- Data extraction or generation\n- AI agents\n\nUsage of Gemini is subject to Google's [Gemini Terms of Use](https://ai.google.dev/terms).\n\n#multimodal\n\n"},{"lang":"en","group":"models","slug":"models/gpt-35-turbo-instruct","frontmatter":{"title":"OpenAI: GPT-3.5 Turbo Instruct","meta_title":"OpenAI: GPT-3.5 Turbo Instruct","description":"OpenAI: GPT-3.5 Turbo Instruct","date":"2023-09-28T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"openai","tags":["Programming","Natural Language Processing","Generative AI","Chatbots","Technology/Web"],"draft":false,"id":"gpt-3.5-turbo-instruct","context":4095,"input":0.0000015,"output":0.000002,"img":0,"request":0,"last_updated":"2023-09-28T00:00:00.000Z","slug":"models/gpt-35-turbo-instruct"},"content":"\nThis model is a variant of GPT-3.5 Turbo tuned for instructional prompts and omitting chat-related optimizations. Training data: up to Sep 2021.\n\n"},{"lang":"en","group":"models","slug":"models/gpt-4o-mini","frontmatter":{"title":"OpenAI: GPT-4o-mini","meta_title":"OpenAI: GPT-4o-mini","description":"OpenAI: GPT-4o-mini","date":"2024-07-18T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text image 2 text"],"author":"openai","tags":["Programming","Technology","Programming/Scripting","Technology/Web"],"draft":false,"id":"gpt-4o-mini","context":128000,"input":1.5e-7,"output":6e-7,"img":0.007225,"request":0,"last_updated":"2024-07-18T00:00:00.000Z","slug":"models/gpt-4o-mini"},"content":"\nGPT-4o mini is OpenAI's newest model after [GPT-4 Omni](/openai/gpt-4o), supporting both text and image inputs with text outputs.\n\nAs their most advanced small model, it is many multiples more affordable than other recent frontier models, and more than 60% cheaper than [GPT-3.5 Turbo](/openai/gpt-3.5-turbo). It maintains SOTA intelligence, while being significantly more cost-effective.\n\nGPT-4o mini achieves an 82% score on MMLU and presently ranks higher than GPT-4 on chat preferences [common leaderboards](https://arena.lmsys.org/).\n\nCheck out the [launch announcement](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/) to learn more.\n\n"},{"lang":"en","group":"models","slug":"models/gpt-4o","frontmatter":{"title":"OpenAI: GPT-4o","meta_title":"OpenAI: GPT-4o","description":"OpenAI: GPT-4o","date":"2024-05-13T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text image 2 text"],"author":"openai","tags":["Programming","Natural Language Processing","Machine Learning","Generative AI","Computer Vision"],"draft":false,"id":"gpt-4o","context":128000,"input":0.0000025,"output":0.00001,"img":0.0036125,"request":0,"last_updated":"2024-05-13T00:00:00.000Z","slug":"models/gpt-4o"},"content":"\nGPT-4o (\"o\" for \"omni\") is OpenAI's latest AI model, supporting both text and image inputs with text outputs. It maintains the intelligence level of [GPT-4 Turbo](/openai/gpt-4-turbo) while being twice as fast and 50% more cost-effective. GPT-4o also offers improved performance in processing non-English languages and enhanced visual capabilities.\n\nFor benchmarking against other models, it was briefly called [\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209)\n\n"},{"lang":"en","group":"models","slug":"models/llama-31-nemotron-70b-instruct","frontmatter":{"title":"Nvidia: Llama 3.1 Nemotron 70B Instruct","meta_title":"Nvidia: Llama 3.1 Nemotron 70B Instruct","description":"Nvidia: Llama 3.1 Nemotron 70B Instruct","date":"2024-10-15T00:00:00.000Z","image":"https://img.rifx.online/logo/nvidia.svg","categories":["text 2 text"],"author":"nvidia","tags":["Programming","Natural Language Processing","Machine Learning","Generative AI","Ethics"],"draft":false,"id":"llama-3.1-nemotron-70b-instruct","context":131072,"input":3.5e-7,"output":4e-7,"img":0,"request":0,"last_updated":"2024-10-15T00:00:00.000Z","slug":"models/llama-31-nemotron-70b-instruct"},"content":"\nNVIDIA's Llama 3.1 Nemotron 70B is a language model designed for generating precise and useful responses. Leveraging [Llama 3.1 70B](/models/meta-llama/llama-3.1-70b-instruct) architecture and Reinforcement Learning from Human Feedback (RLHF), it excels in automatic alignment benchmarks. This model is tailored for applications requiring high accuracy in helpfulness and response generation, suitable for diverse user queries across multiple domains.\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).\n\n"},{"lang":"en","group":"models","slug":"models/llama-32-11b-vision-instruct","frontmatter":{"title":"Meta: Llama 3.2 11B Vision Instruct","meta_title":"Meta: Llama 3.2 11B Vision Instruct","description":"Meta: Llama 3.2 11B Vision Instruct","date":"2024-09-25T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text image 2 text"],"author":"meta-llama","tags":["Natural Language Processing","Computer Vision","Machine Learning","Generative AI","Data Science"],"draft":false,"id":"llama-3.2-11b-vision-instruct","context":131072,"input":5.5e-8,"output":5.5e-8,"img":0.000079475,"request":0,"last_updated":"2024-09-25T00:00:00.000Z","slug":"models/llama-32-11b-vision-instruct"},"content":"\nLlama 3.2 11B Vision is a multimodal model with 11 billion parameters, designed to handle tasks combining visual and textual data. It excels in tasks such as image captioning and visual question answering, bridging the gap between language generation and visual reasoning. Pre-trained on a massive dataset of image-text pairs, it performs well in complex, high-accuracy image analysis.\n\nIts ability to integrate visual understanding with language processing makes it an ideal solution for industries requiring comprehensive visual-linguistic AI applications, such as content creation, AI-driven customer service, and research.\n\nClick here for the [original model card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md).\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).\n\n"},{"lang":"en","group":"models","slug":"models/llama-32-1b-instruct","frontmatter":{"title":"Meta: Llama 3.2 1B Instruct","meta_title":"Meta: Llama 3.2 1B Instruct","description":"Meta: Llama 3.2 1B Instruct","date":"2024-09-25T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text 2 text"],"author":"meta-llama","tags":["Natural Language Processing","Programming","Technology","Machine Learning","Generative AI"],"draft":false,"id":"llama-3.2-1b-instruct","context":131072,"input":1e-8,"output":2e-8,"img":0,"request":0,"last_updated":"2024-09-25T00:00:00.000Z","slug":"models/llama-32-1b-instruct"},"content":"\nLlama 3.2 1B is a 1-billion-parameter language model focused on efficiently performing natural language tasks, such as summarization, dialogue, and multilingual text analysis. Its smaller size allows it to operate efficiently in low-resource environments while maintaining strong task performance.\n\nSupporting eight core languages and fine-tunable for more, Llama 1.3B is ideal for businesses or developers seeking lightweight yet powerful AI solutions that can operate in diverse multilingual settings without the high computational demand of larger models.\n\nClick here for the [original model card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md).\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).\n\n"},{"lang":"en","group":"models","slug":"models/llama-32-90b-vision-instruct","frontmatter":{"title":"Meta: Llama 3.2 90B Vision Instruct","meta_title":"Meta: Llama 3.2 90B Vision Instruct","description":"Meta: Llama 3.2 90B Vision Instruct","date":"2024-09-25T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text image 2 text"],"author":"meta-llama","tags":["Natural Language Processing","Computer Vision","Machine Learning","Data Science","Generative AI"],"draft":false,"id":"llama-3.2-90b-vision-instruct","context":131072,"input":3.5e-7,"output":4e-7,"img":0.00050575,"request":0,"last_updated":"2024-09-25T00:00:00.000Z","slug":"models/llama-32-90b-vision-instruct"},"content":"\nThe Llama 90B Vision model is a top-tier, 90-billion-parameter multimodal model designed for the most challenging visual reasoning and language tasks. It offers unparalleled accuracy in image captioning, visual question answering, and advanced image-text comprehension. Pre-trained on vast multimodal datasets and fine-tuned with human feedback, the Llama 90B Vision is engineered to handle the most demanding image-based AI tasks.\n\nThis model is perfect for industries requiring cutting-edge multimodal AI capabilities, particularly those dealing with complex, real-time visual and textual analysis.\n\nClick here for the [original model card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md).\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).\n\n"},{"lang":"en","group":"models","slug":"models/ministral-8b","frontmatter":{"title":"Ministral 8B","meta_title":"Ministral 8B","description":"Ministral 8B","date":"2024-10-17T00:00:00.000Z","image":"https://img.rifx.online/logo/mistral.png","categories":["text 2 text"],"author":"mistralai","tags":["Technology","Machine Learning","Data Science","Generative AI","Ethics"],"draft":false,"id":"ministral-8b","context":128000,"input":1e-7,"output":1e-7,"img":0,"request":0,"last_updated":"2024-10-19T04:54:11.000Z","slug":"models/ministral-8b"},"content":"\nMinistral 8B is an 8B parameter model featuring a unique interleaved sliding-window attention pattern for faster, memory-efficient inference. Designed for edge use cases, it supports up to 128k context length and excels in knowledge and reasoning tasks. It outperforms peers in the sub-10B category, making it perfect for low-latency, privacy-first applications.\n\n"},{"lang":"en","group":"models","slug":"models/o1-mini","frontmatter":{"title":"OpenAI: o1-mini","meta_title":"OpenAI: o1-mini","description":"OpenAI: o1-mini","date":"2024-09-12T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"openai","tags":["Programming","Science","Natural Language Processing","Machine Learning","Data Science"],"draft":false,"id":"o1-mini","context":128000,"input":0.000003,"output":0.000012,"img":0,"request":0,"last_updated":"2024-09-12T00:00:00.000Z","slug":"models/o1-mini"},"content":"\nThe latest and strongest model family from OpenAI, o1 is designed to spend more time thinking before responding.\n\nThe o1 models are optimized for math, science, programming, and other STEM-related tasks. They consistently exhibit PhD-level accuracy on benchmarks in physics, chemistry, and biology. Learn more in the [launch announcement](https://openai.com/o1).\n\nNote: This model is currently experimental and not suitable for production use-cases, and may be heavily rate-limited.\n\n"},{"lang":"en","group":"models","slug":"models/o1-preview","frontmatter":{"title":"OpenAI: o1-preview","meta_title":"OpenAI: o1-preview","description":"OpenAI: o1-preview","date":"2024-09-12T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"openai","tags":["Programming","Science","Natural Language Processing","Machine Learning","Data Science"],"draft":false,"id":"o1-preview","context":128000,"input":0.000015,"output":0.00006,"img":0,"request":0,"last_updated":"2024-09-12T00:00:00.000Z","slug":"models/o1-preview"},"content":"\nThe latest and strongest model family from OpenAI, o1 is designed to spend more time thinking before responding.\n\nThe o1 models are optimized for math, science, programming, and other STEM-related tasks. They consistently exhibit PhD-level accuracy on benchmarks in physics, chemistry, and biology. Learn more in the [launch announcement](https://openai.com/o1).\n\nNote: This model is currently experimental and not suitable for production use-cases, and may be heavily rate-limited.\n\n"},{"lang":"en","group":"models","slug":"models/qwen-2-7b-instruct","frontmatter":{"title":"Qwen 2 7B Instruct","meta_title":"Qwen 2 7B Instruct","description":"Qwen 2 7B Instruct","date":"2024-07-16T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"qwen","tags":["Natural Language Processing","Programming","Machine Learning","Data Science","Ethics"],"draft":false,"id":"qwen-2-7b-instruct","context":32768,"input":5.4e-8,"output":5.4e-8,"img":0,"request":0,"last_updated":"2024-07-16T00:00:00.000Z","slug":"models/qwen-2-7b-instruct"},"content":"\nQwen2 7B is a transformer-based model that excels in language understanding, multilingual capabilities, coding, mathematics, and reasoning.\n\nIt features SwiGLU activation, attention QKV bias, and group query attention. It is pretrained on extensive data with supervised finetuning and direct preference optimization.\n\nFor more details, see this [blog post](https://qwenlm.github.io/blog/qwen2/) and [GitHub repo](https://github.com/QwenLM/Qwen2).\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).\n\n"},{"lang":"en","group":"models","slug":"models/qwen-2-vl-72b-instruct","frontmatter":{"title":"Qwen2-VL 72B Instruct","meta_title":"Qwen2-VL 72B Instruct","description":"Qwen2-VL 72B Instruct","date":"2024-09-18T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text image 2 text"],"author":"qwen","tags":["Natural Language Processing","Computer Vision","Robotics","Machine Learning"],"draft":false,"id":"qwen-2-vl-72b-instruct","context":32768,"input":4e-7,"output":4e-7,"img":0.000578,"request":0,"last_updated":"2024-09-18T00:00:00.000Z","slug":"models/qwen-2-vl-72b-instruct"},"content":"\nQwen2 VL 72B is a multimodal LLM from the Qwen Team with the following key enhancements:\n\n- SoTA understanding of images of various resolution & ratio: Qwen2-VL achieves state-of-the-art performance on visual understanding benchmarks, including MathVista, DocVQA, RealWorldQA, MTVQA, etc.\n\n- Understanding videos of 20min+: Qwen2-VL can understand videos over 20 minutes for high-quality video-based question answering, dialog, content creation, etc.\n\n- Agent that can operate your mobiles, robots, etc.: with the abilities of complex reasoning and decision making, Qwen2-VL can be integrated with devices like mobile phones, robots, etc., for automatic operation based on visual environment and text instructions.\n\n- Multilingual Support: to serve global users, besides English and Chinese, Qwen2-VL now supports the understanding of texts in different languages inside images, including most European languages, Japanese, Korean, Arabic, Vietnamese, etc.\n\nFor more details, see this [blog post](https://qwenlm.github.io/blog/qwen2-vl/) and [GitHub repo](https://github.com/QwenLM/Qwen2-VL).\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).\n\n"},{"lang":"en","group":"models","slug":"models/qwen-25-72b-instruct","frontmatter":{"title":"Qwen2.5 72B Instruct","meta_title":"Qwen2.5 72B Instruct","description":"Qwen2.5 72B Instruct","date":"2024-09-19T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"qwen","tags":["Programming","Natural Language Processing","Chatbots","Machine Learning","Data Science"],"draft":false,"id":"qwen-2.5-72b-instruct","context":131072,"input":3.5e-7,"output":4e-7,"img":0,"request":0,"last_updated":"2024-09-19T00:00:00.000Z","slug":"models/qwen-25-72b-instruct"},"content":"\nQwen2.5 72B is the latest series of Qwen large language models. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\n\n- Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n\n- Long-context Support up to 128K tokens and can generate up to 8K tokens.\n\n- Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).\n\n"},{"lang":"en","group":"models","slug":"models/qwen-25-7b-instruct","frontmatter":{"title":"Qwen2.5 7B Instruct","meta_title":"Qwen2.5 7B Instruct","description":"Qwen2.5 7B Instruct","date":"2024-10-16T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"qwen","tags":["Programming","Natural Language Processing","Chatbots","Machine Learning","Data Science"],"draft":false,"id":"qwen-2.5-7b-instruct","context":131072,"input":2.7e-7,"output":2.7e-7,"img":0,"request":0,"last_updated":"2024-10-16T00:00:00.000Z","slug":"models/qwen-25-7b-instruct"},"content":"\nQwen2.5 7B is the latest series of Qwen large language models. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\n\n- Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n\n- Long-context Support up to 128K tokens and can generate up to 8K tokens.\n\n- Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).\n\n"},{"lang":"zh","group":"models","slug":"models/chatgpt-4o-latest","frontmatter":{"title":"OpenAI: ChatGPT-4o","meta_title":"OpenAI: ChatGPT-4o","description":"OpenAI: ChatGPT-4o","date":"2024-08-14T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text image 2 text"],"author":"openai","tags":["Chatbots","Generative AI","Machine Learning","Natural Language Processing"],"draft":false,"id":"chatgpt-4o-latest","context":128000,"input":0.000005,"output":0.000015,"img":0.007225,"request":0,"last_updated":"2024-08-14T00:00:00.000Z","slug":"models/chatgpt-4o-latest"},"content":"\n动态模型持续更新到 ChatGPT 中的当前版本 [GPT-4o](/openai/gpt-4o)。旨在用于研究和评估。\n\n注意：此模型目前处于实验阶段，不适合生产使用，并可能受到严格的速率限制。\n\n"},{"lang":"zh","group":"models","slug":"models/claude-3-haiku","frontmatter":{"title":"Anthropic: Claude 3 Haiku","meta_title":"Anthropic: Claude 3 Haiku","description":"Anthropic: Claude 3 Haiku","date":"2024-03-13T00:00:00.000Z","image":"https://img.rifx.online/logo/anthropic.svg","categories":["text image 2 text"],"author":"anthropic","tags":["Programming","Machine Learning","Generative AI","Chatbots","Natural Language Processing"],"draft":false,"id":"claude-3-haiku","context":200000,"input":2.5e-7,"output":0.00000125,"img":0.0004,"request":0,"last_updated":"2024-10-24T11:54:59.000Z","slug":"models/claude-3-haiku"},"content":"\nClaude 3 Haiku 是 Anthropic 反应速度最快、体积最小的模型，能够实现近乎即时的响应。快速且准确的定向性能。\n\n查看发布公告和基准测试结果 [这里](https://www.anthropic.com/news/claude-3-haiku)\n\n#multimodal\n\n"},{"lang":"zh","group":"models","slug":"models/claude-35-sonnet","frontmatter":{"title":"Anthropic: Claude 3.5 Sonnet","meta_title":"Anthropic: Claude 3.5 Sonnet","description":"Anthropic: Claude 3.5 Sonnet","date":"2024-06-20T00:00:00.000Z","image":"https://img.rifx.online/logo/anthropic.svg","categories":["text image 2 text"],"author":"anthropic","tags":["Programming","Data Science","Computer Vision","Chatbots","Autonomous Systems"],"draft":false,"id":"claude-3.5-sonnet","context":200000,"input":0.000003,"output":0.000015,"img":0.0048,"request":0,"last_updated":"2024-10-24T11:45:46.000Z","slug":"models/claude-35-sonnet"},"content":"\nClaude 3.5 Sonnet 提供优于 Opus 的能力，以快于 Sonnet 的速度，且价格与 Sonnet 相同。Sonnet 在以下方面表现尤为出色：\n\n- 编程：自主编写、编辑和运行代码，具备推理和故障排除能力\n- 数据科学：增强人类数据科学专业知识；在使用多种工具获取洞察的同时，处理非结构化数据\n- 视觉处理：擅长解读图表、图形和图像，准确转录文本以获得超越文本本身的洞察\n- 代理任务：出色的工具使用能力，使其在代理任务（即需要与其他系统互动的复杂多步骤问题解决任务）中表现出色\n\n#multimodal\n\n"},{"lang":"zh","group":"models","slug":"models/gemini-flash-15-8b","frontmatter":{"title":"Google: Gemini 1.5 Flash-8B","meta_title":"Google: Gemini 1.5 Flash-8B","description":"Google: Gemini 1.5 Flash-8B","date":"2024-10-03T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text image 2 text"],"author":"google","tags":["Programming","Natural Language Processing","Chatbots","Translation","Technology/Web"],"draft":false,"id":"gemini-flash-1.5-8b","context":1000000,"input":3.75e-8,"output":1.5e-7,"img":0,"request":0,"last_updated":"2024-10-03T00:00:00.000Z","slug":"models/gemini-flash-15-8b"},"content":"\nGemini 1.5 Flash-8B 针对速度和效率进行了优化，在聊天、转录和翻译等小提示任务中提供了增强的性能。通过减少延迟，它在实时和大规模操作中非常有效。该模型专注于成本效益解决方案，同时保持高质量的结果。\n\n[点击此处了解更多关于此模型的信息](https://developers.googleblog.com/en/gemini-15-flash-8b-is-now-generally-available-for-use/)。\n\n使用 Gemini 受限于 Google's [Gemini 使用条款](https://ai.google.dev/terms)。\n\n"},{"lang":"zh","group":"models","slug":"models/gemini-pro-15","frontmatter":{"title":"Google: Gemini Pro 1.5","meta_title":"Google: Gemini Pro 1.5","description":"Google: Gemini Pro 1.5","date":"2024-04-09T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text image 2 text"],"author":"google","tags":["Programming","Natural Language Processing","Machine Learning","Generative AI","Chatbots"],"draft":false,"id":"gemini-pro-1.5","context":2000000,"input":0.00000125,"output":0.000005,"img":0.00263,"request":0,"last_updated":"2024-04-09T00:00:00.000Z","slug":"models/gemini-pro-15"},"content":"\n谷歌最新的多模态模型，支持在文本或聊天提示中使用图像和视频。\n\n针对以下语言任务进行了优化：\n\n- 代码生成\n- 文本生成\n- 文本编辑\n- 问题解决\n- 推荐\n- 信息提取\n- 数据提取或生成\n- AI代理\n\nGemini的使用受限于谷歌的[Gemini使用条款](https://ai.google.dev/terms)。\n\n#multimodal\n\n"},{"lang":"zh","group":"models","slug":"models/gpt-35-turbo-instruct","frontmatter":{"title":"OpenAI: GPT-3.5 Turbo Instruct","meta_title":"OpenAI: GPT-3.5 Turbo Instruct","description":"OpenAI: GPT-3.5 Turbo Instruct","date":"2023-09-28T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"openai","tags":["Programming","Natural Language Processing","Generative AI","Chatbots","Technology/Web"],"draft":false,"id":"gpt-3.5-turbo-instruct","context":4095,"input":0.0000015,"output":0.000002,"img":0,"request":0,"last_updated":"2023-09-28T00:00:00.000Z","slug":"models/gpt-35-turbo-instruct"},"content":"\n该模型是GPT-3.5 Turbo的一个变体，针对教学提示进行了调整，并省略了与聊天相关的优化。训练数据：截至2021年9月。\n\n"},{"lang":"zh","group":"models","slug":"models/gpt-4o-mini","frontmatter":{"title":"OpenAI: GPT-4o-mini","meta_title":"OpenAI: GPT-4o-mini","description":"OpenAI: GPT-4o-mini","date":"2024-07-18T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text image 2 text"],"author":"openai","tags":["Programming","Technology","Programming/Scripting","Technology/Web"],"draft":false,"id":"gpt-4o-mini","context":128000,"input":1.5e-7,"output":6e-7,"img":0.007225,"request":0,"last_updated":"2024-07-18T00:00:00.000Z","slug":"models/gpt-4o-mini"},"content":"\nGPT-4o mini 是 OpenAI 在 [GPT-4 Omni](/openai/gpt-4o) 之后发布的最新模型，支持文本和图像输入，并生成文本输出。\n\n作为他们最先进的小型模型，它的价格比其他最近的前沿模型低了许多倍，并且比 [GPT-3.5 Turbo](/openai/gpt-3.5-turbo) 便宜超过 60%。它保持了 SOTA 智能，同时在成本效益上显著更高。\n\nGPT-4o mini 在 MMLU 上获得了 82% 的分数，目前在聊天偏好 [常见排行榜](https://arena.lmsys.org/) 上排名高于 GPT-4。\n\n查看 [发布公告](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/) 以了解更多信息。\n\n"},{"lang":"zh","group":"models","slug":"models/gpt-4o","frontmatter":{"title":"OpenAI: GPT-4o","meta_title":"OpenAI: GPT-4o","description":"OpenAI: GPT-4o","date":"2024-05-13T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text image 2 text"],"author":"openai","tags":["Programming","Natural Language Processing","Machine Learning","Generative AI","Computer Vision"],"draft":false,"id":"gpt-4o","context":128000,"input":0.0000025,"output":0.00001,"img":0.0036125,"request":0,"last_updated":"2024-05-13T00:00:00.000Z","slug":"models/gpt-4o"},"content":"\nGPT-4o（“o”代表“全能”）是OpenAI最新的AI模型，支持文本和图像输入，并生成文本输出。它保持了[GPT-4 Turbo](/openai/gpt-4-turbo)的智能水平，同时速度提高了两倍，成本效益提高了50%。GPT-4o还在处理非英语语言和增强视觉能力方面提供了更好的性能。\n\n为了与其他模型进行基准测试，它曾被暂时称为[\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209)\n\n"},{"lang":"zh","group":"models","slug":"models/llama-31-nemotron-70b-instruct","frontmatter":{"title":"Nvidia: Llama 3.1 Nemotron 70B Instruct","meta_title":"Nvidia: Llama 3.1 Nemotron 70B Instruct","description":"Nvidia: Llama 3.1 Nemotron 70B Instruct","date":"2024-10-15T00:00:00.000Z","image":"https://img.rifx.online/logo/nvidia.svg","categories":["text 2 text"],"author":"nvidia","tags":["Programming","Natural Language Processing","Machine Learning","Generative AI","Ethics"],"draft":false,"id":"llama-3.1-nemotron-70b-instruct","context":131072,"input":3.5e-7,"output":4e-7,"img":0,"request":0,"last_updated":"2024-10-15T00:00:00.000Z","slug":"models/llama-31-nemotron-70b-instruct"},"content":"\nNVIDIA的Llama 3.1 Nemotron 70B是一个旨在生成精确和有用响应的语言模型。利用[Llama 3.1 70B](/models/meta-llama/llama-3.1-70b-instruct)架构和基于人类反馈的强化学习（RLHF），它在自动对齐基准测试中表现出色。该模型专为需要高准确性以提供帮助和生成响应的应用而设计，适合处理多个领域的多样用户查询。\n\n使用此模型须遵循[Meta的可接受使用政策](https://www.llama.com/llama3/use-policy/)。\n\n"},{"lang":"zh","group":"models","slug":"models/llama-32-11b-vision-instruct","frontmatter":{"title":"Meta: Llama 3.2 11B Vision Instruct","meta_title":"Meta: Llama 3.2 11B Vision Instruct","description":"Meta: Llama 3.2 11B Vision Instruct","date":"2024-09-25T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text image 2 text"],"author":"meta-llama","tags":["Natural Language Processing","Computer Vision","Machine Learning","Generative AI","Data Science"],"draft":false,"id":"llama-3.2-11b-vision-instruct","context":131072,"input":5.5e-8,"output":5.5e-8,"img":0.000079475,"request":0,"last_updated":"2024-09-25T00:00:00.000Z","slug":"models/llama-32-11b-vision-instruct"},"content":"\nLlama 3.2 11B Vision 是一个具有 110 亿参数的多模态模型，旨在处理结合视觉和文本数据的任务。它在图像描述和视觉问答等任务中表现出色，弥合了语言生成与视觉推理之间的差距。该模型在一个庞大的图像-文本对数据集上进行了预训练，在复杂的高精度图像分析中表现良好。\n\n它将视觉理解与语言处理相结合的能力，使其成为需要全面视觉语言 AI 应用的行业的理想解决方案，例如内容创作、AI 驱动的客户服务和研究。\n\n点击此处查看 [原始模型卡](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md)。\n\n使用此模型须遵循 [Meta 的可接受使用政策](https://www.llama.com/llama3/use-policy/)。\n\n"},{"lang":"zh","group":"models","slug":"models/llama-32-1b-instruct","frontmatter":{"title":"Meta: Llama 3.2 1B Instruct","meta_title":"Meta: Llama 3.2 1B Instruct","description":"Meta: Llama 3.2 1B Instruct","date":"2024-09-25T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text 2 text"],"author":"meta-llama","tags":["Natural Language Processing","Programming","Technology","Machine Learning","Generative AI"],"draft":false,"id":"llama-3.2-1b-instruct","context":131072,"input":1e-8,"output":2e-8,"img":0,"request":0,"last_updated":"2024-09-25T00:00:00.000Z","slug":"models/llama-32-1b-instruct"},"content":"\nLlama 3.2 1B 是一个拥有10亿参数的语言模型，专注于高效执行自然语言任务，如摘要、对话和多语言文本分析。其较小的规模使其能够在低资源环境中高效运行，同时保持强大的任务性能。\n\n支持八种核心语言，并可针对更多语言进行微调，Llama 1.3B 非常适合寻求轻量级但强大的 AI 解决方案的企业或开发者，这些解决方案能够在多样化的多语言环境中运行，而不需要大型模型的高计算需求。\n\n点击此处查看 [原始模型卡](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md)。\n\n使用此模型需遵循 [Meta 的可接受使用政策](https://www.llama.com/llama3/use-policy/)。\n\n"},{"lang":"zh","group":"models","slug":"models/llama-32-90b-vision-instruct","frontmatter":{"title":"Meta: Llama 3.2 90B Vision Instruct","meta_title":"Meta: Llama 3.2 90B Vision Instruct","description":"Meta: Llama 3.2 90B Vision Instruct","date":"2024-09-25T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text image 2 text"],"author":"meta-llama","tags":["Natural Language Processing","Computer Vision","Machine Learning","Data Science","Generative AI"],"draft":false,"id":"llama-3.2-90b-vision-instruct","context":131072,"input":3.5e-7,"output":4e-7,"img":0.00050575,"request":0,"last_updated":"2024-09-25T00:00:00.000Z","slug":"models/llama-32-90b-vision-instruct"},"content":"\nLlama 90B Vision模型是一款顶级的90亿参数多模态模型，旨在应对最具挑战性的视觉推理和语言任务。它在图像描述、视觉问答和高级图像-文本理解方面提供无与伦比的准确性。该模型在庞大的多模态数据集上进行预训练，并通过人类反馈进行微调，专为处理最苛刻的基于图像的AI任务而设计。\n\n此模型非常适合需要尖端多模态AI能力的行业，尤其是那些处理复杂实时视觉和文本分析的行业。\n\n点击此处查看[原始模型卡片](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md)。\n\n使用此模型需遵守[Meta的可接受使用政策](https://www.llama.com/llama3/use-policy/)。\n\n"},{"lang":"zh","group":"models","slug":"models/ministral-8b","frontmatter":{"title":"Ministral 8B","meta_title":"Ministral 8B","description":"Ministral 8B","date":"2024-10-17T00:00:00.000Z","image":"https://img.rifx.online/logo/mistral.png","categories":["text 2 text"],"author":"mistralai","tags":["Technology","Machine Learning","Data Science","Generative AI","Ethics"],"draft":false,"id":"ministral-8b","context":128000,"input":1e-7,"output":1e-7,"img":0,"request":0,"last_updated":"2024-10-19T04:54:11.000Z","slug":"models/ministral-8b"},"content":"\nMinistral 8B 是一个具有 8B 参数的模型，采用独特的交错滑动窗口注意力模式，以实现更快、更节省内存的推理。该模型专为边缘使用案例设计，支持最长 128k 的上下文长度，并在知识和推理任务中表现出色。它在低于 10B 的类别中优于同类产品，非常适合低延迟、注重隐私的应用。\n\n"},{"lang":"zh","group":"models","slug":"models/o1-mini","frontmatter":{"title":"OpenAI: o1-mini","meta_title":"OpenAI: o1-mini","description":"OpenAI: o1-mini","date":"2024-09-12T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"openai","tags":["Programming","Science","Natural Language Processing","Machine Learning","Data Science"],"draft":false,"id":"o1-mini","context":128000,"input":0.000003,"output":0.000012,"img":0,"request":0,"last_updated":"2024-09-12T00:00:00.000Z","slug":"models/o1-mini"},"content":"\nOpenAI最新且最强大的模型系列o1旨在在响应之前花更多时间思考。\n\no1模型经过优化，适用于数学、科学、编程及其他STEM相关任务。它们在物理、化学和生物学的基准测试中始终展现出博士级的准确性。了解更多信息，请查看[发布公告](https://openai.com/o1)。\n\n注意：该模型目前处于实验阶段，不适合生产使用，并可能受到严格的速率限制。\n\n"},{"lang":"zh","group":"models","slug":"models/o1-preview","frontmatter":{"title":"OpenAI: o1-preview","meta_title":"OpenAI: o1-preview","description":"OpenAI: o1-preview","date":"2024-09-12T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"openai","tags":["Programming","Science","Natural Language Processing","Machine Learning","Data Science"],"draft":false,"id":"o1-preview","context":128000,"input":0.000015,"output":0.00006,"img":0,"request":0,"last_updated":"2024-09-12T00:00:00.000Z","slug":"models/o1-preview"},"content":"\nOpenAI最新且最强大的模型系列o1旨在在响应之前花更多时间思考。\n\no1模型经过优化，适用于数学、科学、编程和其他STEM相关任务。它们在物理、化学和生物学的基准测试中始终表现出博士级的准确性。有关更多信息，请参阅[发布公告](https://openai.com/o1)。\n\n注意：该模型目前处于实验阶段，不适合生产使用案例，并且可能会受到严格的速率限制。\n\n"},{"lang":"zh","group":"models","slug":"models/qwen-2-7b-instruct","frontmatter":{"title":"Qwen 2 7B Instruct","meta_title":"Qwen 2 7B Instruct","description":"Qwen 2 7B Instruct","date":"2024-07-16T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"qwen","tags":["Natural Language Processing","Programming","Machine Learning","Data Science","Ethics"],"draft":false,"id":"qwen-2-7b-instruct","context":32768,"input":5.4e-8,"output":5.4e-8,"img":0,"request":0,"last_updated":"2024-07-16T00:00:00.000Z","slug":"models/qwen-2-7b-instruct"},"content":"\nQwen2 7B 是一个基于变换器的模型，擅长语言理解、多语言能力、编码、数学和推理。\n\n它具有 SwiGLU 激活、注意力 QKV 偏置和组查询注意力。它在大量数据上进行预训练，并经过监督微调和直接偏好优化。\n\n有关更多详细信息，请参阅此 [博客文章](https://qwenlm.github.io/blog/qwen2/) 和 [GitHub 仓库](https://github.com/QwenLM/Qwen2)。\n\n使用此模型须遵循 [同义千问许可证协议](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE)。\n\n"},{"lang":"zh","group":"models","slug":"models/qwen-2-vl-72b-instruct","frontmatter":{"title":"Qwen2-VL 72B Instruct","meta_title":"Qwen2-VL 72B Instruct","description":"Qwen2-VL 72B Instruct","date":"2024-09-18T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text image 2 text"],"author":"qwen","tags":["Natural Language Processing","Computer Vision","Robotics","Machine Learning"],"draft":false,"id":"qwen-2-vl-72b-instruct","context":32768,"input":4e-7,"output":4e-7,"img":0.000578,"request":0,"last_updated":"2024-09-18T00:00:00.000Z","slug":"models/qwen-2-vl-72b-instruct"},"content":"\nQwen2 VL 72B 是来自 Qwen Team 的多模态 LLM，具有以下关键增强功能：\n\n- 对各种分辨率和比例图像的最先进理解：Qwen2-VL 在视觉理解基准测试中实现了最先进的性能，包括 MathVista、DocVQA、RealWorldQA、MTVQA 等。\n\n- 理解超过 20 分钟的视频：Qwen2-VL 可以理解超过 20 分钟的视频，以进行高质量的视频问答、对话、内容创作等。\n\n- 能够操作您的手机、机器人等的智能体：凭借复杂推理和决策能力，Qwen2-VL 可以与手机、机器人等设备集成，实现基于视觉环境和文本指令的自动操作。\n\n- 多语言支持：为了服务全球用户，除了英语和中文，Qwen2-VL 现在还支持理解图像中不同语言的文本，包括大多数欧洲语言、日语、韩语、阿拉伯语、越南语等。\n\n有关更多详细信息，请参阅此 [博客文章](https://qwenlm.github.io/blog/qwen2-vl/) 和 [GitHub 仓库](https://github.com/QwenLM/Qwen2-VL)。\n\n使用此模型受 [通义千问许可协议](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE) 的约束。\n\n"},{"lang":"zh","group":"models","slug":"models/qwen-25-72b-instruct","frontmatter":{"title":"Qwen2.5 72B Instruct","meta_title":"Qwen2.5 72B Instruct","description":"Qwen2.5 72B Instruct","date":"2024-09-19T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"qwen","tags":["Programming","Natural Language Processing","Chatbots","Machine Learning","Data Science"],"draft":false,"id":"qwen-2.5-72b-instruct","context":131072,"input":3.5e-7,"output":4e-7,"img":0,"request":0,"last_updated":"2024-09-19T00:00:00.000Z","slug":"models/qwen-25-72b-instruct"},"content":"\nQwen2.5 72B 是 Qwen 大型语言模型的最新系列。Qwen2.5 在 Qwen2 的基础上带来了以下改进：\n\n- 知识显著增加，并在编码和数学能力上有了很大提升，这得益于我们在这些领域的专业专家模型。\n\n- 在遵循指令、生成长文本（超过 8K tokens）、理解结构化数据（例如，表格）以及生成结构化输出（特别是 JSON）方面有显著改进。对系统提示的多样性更加强韧，增强了角色扮演的实现和聊天机器人的条件设置。\n\n- 支持最长 128K tokens 的长上下文，并可以生成最多 8K tokens。\n\n- 支持超过 29 种语言，包括中文、英文、法语、西班牙语、葡萄牙语、德语、意大利语、俄语、日语、韩语、越南语、泰语、阿拉伯语等。\n\n使用此模型须遵守 [同义千问许可协议](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE)。\n\n"},{"lang":"zh","group":"models","slug":"models/qwen-25-7b-instruct","frontmatter":{"title":"Qwen2.5 7B Instruct","meta_title":"Qwen2.5 7B Instruct","description":"Qwen2.5 7B Instruct","date":"2024-10-16T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"qwen","tags":["Programming","Natural Language Processing","Chatbots","Machine Learning","Data Science"],"draft":false,"id":"qwen-2.5-7b-instruct","context":131072,"input":2.7e-7,"output":2.7e-7,"img":0,"request":0,"last_updated":"2024-10-16T00:00:00.000Z","slug":"models/qwen-25-7b-instruct"},"content":"\nQwen2.5 7B 是 Qwen 大语言模型的最新系列。Qwen2.5 在 Qwen2 的基础上带来了以下改进：\n\n- 知识显著增加，并在编码和数学方面的能力大幅提升，这得益于我们在这些领域的专业模型。\n\n- 在遵循指令、生成长文本（超过 8K tokens）、理解结构化数据（例如，表格）以及生成结构化输出，特别是 JSON 方面有显著改进。对系统提示的多样性更具韧性，增强了角色扮演的实现和聊天机器人的条件设置。\n\n- 长文本支持高达 128K tokens，并且可以生成最多 8K tokens。\n\n- 支持超过 29 种语言，包括中文、英文、法文、西班牙文、葡萄牙文、德文、意大利文、俄文、日文、韩文、越南文、泰文、阿拉伯文等。\n\n使用此模型须遵守 [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE)。\n\n"}]
[{"lang":"fr","group":"blog","slug":"blog/post-1","frontmatter":{"title":"Comment créer une application avec des technologies modernes","meta_title":"","description":"Ceci est une méta-description","date":"2022-04-04T05:00:00.000Z","image":"/images/image-placeholder.png","categories":["french","Application","Data"],"author":"John Doe","tags":["nextjs","tailwind","react"],"draft":false,"slug":"blog/post-1"},"content":"\nPersonne ne veut même sortir un maquillage de l'urne des soins empoisonnés. C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\n## Design Créatif\n\nCar en guise de maquillage, l'urne du poison C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\n> Le client lui-même doit pouvoir poursuivre l'adipisicing. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n"},{"lang":"fr","group":"blog","slug":"blog/post-2","frontmatter":{"title":"Comment créer une application avec des technologies modernes","meta_title":"","description":"Ceci est une méta-description","date":"2022-04-04T05:00:00.000Z","image":"/images/image-placeholder.png","categories":["Technology","Data"],"author":"Sam Wilson","tags":["technology","tailwind"],"draft":false,"slug":"blog/post-2"},"content":"\nPersonne ne veut même sortir un maquillage de l'urne des soins empoisonnés. C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\n## Design Créatif\n\nCar en guise de maquillage, l'urne du poison C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\n> Le client lui-même doit pouvoir poursuivre l'adipisicing. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n"},{"lang":"fr","group":"blog","slug":"blog/post-3","frontmatter":{"title":"Comment créer une application avec des technologies modernes","meta_title":"","description":"Ceci est une méta-description","date":"2022-04-04T05:00:00.000Z","image":"/images/image-placeholder.png","categories":["Software"],"author":"John Doe","tags":["software","tailwind"],"draft":false,"slug":"blog/post-3"},"content":"\nPersonne ne veut même sortir un maquillage de l'urne des soins empoisonnés. C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\n## Design Créatif\n\nCar en guise de maquillage, l'urne du poison C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\n> Le client lui-même doit pouvoir poursuivre l'adipisicing. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n"},{"lang":"fr","group":"blog","slug":"blog/post-4","frontmatter":{"title":"Comment créer une application avec des technologies modernes","meta_title":"","description":"Ceci est une méta-description","date":"2022-04-04T05:00:00.000Z","image":"/images/image-placeholder.png","categories":["Architecture"],"author":"John Doe","tags":["silicon","technology"],"draft":false,"slug":"blog/post-4"},"content":"\nPersonne ne veut même sortir un maquillage de l'urne des soins empoisonnés. C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\n## Design Créatif\n\nCar en guise de maquillage, l'urne du poison C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\n> Le client lui-même doit pouvoir poursuivre l'adipisicing. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n"},{"lang":"en","group":"models","slug":"models/chatgpt-4o-latest","frontmatter":{"title":"OpenAI: ChatGPT-4o","meta_title":"OpenAI: ChatGPT-4o","description":"OpenAI: ChatGPT-4o","date":"2024-08-14T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text image 2 text"],"author":"openai","tags":["openai"],"draft":false,"id":"chatgpt-4o-latest","context":128000,"input":0.000005,"output":0.000015,"img":0.007225,"request":0,"slug":"models/chatgpt-4o-latest"},"content":"\nDynamic model continuously updated to the current version of [GPT-4o](/openai/gpt-4o) in ChatGPT. Intended for research and evaluation.\n\nNote: This model is currently experimental and not suitable for production use-cases, and may be heavily rate-limited.\n\n"},{"lang":"en","group":"models","slug":"models/gemini-flash-1.5-8b","frontmatter":{"title":"Google: Gemini 1.5 Flash-8B","meta_title":"Google: Gemini 1.5 Flash-8B","description":"Google: Gemini 1.5 Flash-8B","date":"2024-10-03T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text image 2 text"],"author":"google","tags":["google"],"draft":false,"id":"gemini-flash-1.5-8b","context":1000000,"input":3.75e-8,"output":1.5e-7,"img":0,"request":0,"slug":"models/gemini-flash-1.5-8b"},"content":"\nGemini 1.5 Flash-8B is optimized for speed and efficiency, offering enhanced performance in small prompt tasks like chat, transcription, and translation. With reduced latency, it is highly effective for real-time and large-scale operations. This model focuses on cost-effective solutions while maintaining high-quality results.\n\n[Click here to learn more about this model](https://developers.googleblog.com/en/gemini-15-flash-8b-is-now-generally-available-for-use/).\n\nUsage of Gemini is subject to Google's [Gemini Terms of Use](https://ai.google.dev/terms).\n\n"},{"lang":"en","group":"models","slug":"models/gemini-pro-1.5","frontmatter":{"title":"Google: Gemini Pro 1.5","meta_title":"Google: Gemini Pro 1.5","description":"Google: Gemini Pro 1.5","date":"2024-04-09T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text image 2 text"],"author":"google","tags":["google"],"draft":false,"id":"gemini-pro-1.5","context":2000000,"input":0.00000125,"output":0.000005,"img":0.00263,"request":0,"slug":"models/gemini-pro-1.5"},"content":"\nGoogle's latest multimodal model, supporting image and video in text or chat prompts.\n\nOptimized for language tasks including:\n\n- Code generation\n- Text generation\n- Text editing\n- Problem solving\n- Recommendations\n- Information extraction\n- Data extraction or generation\n- AI agents\n\nUsage of Gemini is subject to Google's [Gemini Terms of Use](https://ai.google.dev/terms).\n\n#multimodal\n\n"},{"lang":"en","group":"models","slug":"models/gpt-3.5-turbo-instruct","frontmatter":{"title":"OpenAI: GPT-3.5 Turbo Instruct","meta_title":"OpenAI: GPT-3.5 Turbo Instruct","description":"OpenAI: GPT-3.5 Turbo Instruct","date":"2023-09-28T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"openai","tags":["openai"],"draft":false,"id":"gpt-3.5-turbo-instruct","context":4095,"input":0.0000015,"output":0.000002,"img":0,"request":0,"slug":"models/gpt-3.5-turbo-instruct"},"content":"\nThis model is a variant of GPT-3.5 Turbo tuned for instructional prompts and omitting chat-related optimizations. Training data: up to Sep 2021.\n\n"},{"lang":"en","group":"models","slug":"models/gpt-4o-mini","frontmatter":{"title":"OpenAI: GPT-4o-mini","meta_title":"OpenAI: GPT-4o-mini","description":"OpenAI: GPT-4o-mini","date":"2024-07-18T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text image 2 text"],"author":"openai","tags":["Programming","Technology","Programming/Scripting","Technology/Web"],"draft":false,"id":"gpt-4o-mini","context":128000,"input":1.5e-7,"output":6e-7,"img":0.007225,"request":0,"slug":"models/gpt-4o-mini"},"content":"\nGPT-4o mini is OpenAI's newest model after [GPT-4 Omni](/openai/gpt-4o), supporting both text and image inputs with text outputs.\n\nAs their most advanced small model, it is many multiples more affordable than other recent frontier models, and more than 60% cheaper than [GPT-3.5 Turbo](/openai/gpt-3.5-turbo). It maintains SOTA intelligence, while being significantly more cost-effective.\n\nGPT-4o mini achieves an 82% score on MMLU and presently ranks higher than GPT-4 on chat preferences [common leaderboards](https://arena.lmsys.org/).\n\nCheck out the [launch announcement](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/) to learn more.\n\n"},{"lang":"en","group":"models","slug":"models/gpt-4o","frontmatter":{"title":"OpenAI: GPT-4o","meta_title":"OpenAI: GPT-4o","description":"OpenAI: GPT-4o","date":"2024-05-13T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text image 2 text"],"author":"openai","tags":["Programming","Programming/Scripting","Technology/Web","Technology"],"draft":false,"id":"gpt-4o","context":128000,"input":0.0000025,"output":0.00001,"img":0.0036125,"request":0,"slug":"models/gpt-4o"},"content":"\nGPT-4o (\"o\" for \"omni\") is OpenAI's latest AI model, supporting both text and image inputs with text outputs. It maintains the intelligence level of [GPT-4 Turbo](/openai/gpt-4-turbo) while being twice as fast and 50% more cost-effective. GPT-4o also offers improved performance in processing non-English languages and enhanced visual capabilities.\n\nFor benchmarking against other models, it was briefly called [\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209)\n\n"},{"lang":"en","group":"models","slug":"models/llama-3.2-11b-vision-instruct","frontmatter":{"title":"Meta: Llama 3.2 11B Vision Instruct","meta_title":"Meta: Llama 3.2 11B Vision Instruct","description":"Meta: Llama 3.2 11B Vision Instruct","date":"2024-09-25T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text image 2 text"],"author":"meta-llama","tags":["meta-llama"],"draft":false,"id":"llama-3.2-11b-vision-instruct","context":131072,"input":5.5e-8,"output":5.5e-8,"img":0.000079475,"request":0,"slug":"models/llama-3.2-11b-vision-instruct"},"content":"\nLlama 3.2 11B Vision is a multimodal model with 11 billion parameters, designed to handle tasks combining visual and textual data. It excels in tasks such as image captioning and visual question answering, bridging the gap between language generation and visual reasoning. Pre-trained on a massive dataset of image-text pairs, it performs well in complex, high-accuracy image analysis.\n\nIts ability to integrate visual understanding with language processing makes it an ideal solution for industries requiring comprehensive visual-linguistic AI applications, such as content creation, AI-driven customer service, and research.\n\nClick here for the [original model card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md).\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).\n\n"},{"lang":"en","group":"models","slug":"models/llama-3.2-1b-instruct","frontmatter":{"title":"Meta: Llama 3.2 1B Instruct","meta_title":"Meta: Llama 3.2 1B Instruct","description":"Meta: Llama 3.2 1B Instruct","date":"2024-09-25T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text 2 text"],"author":"meta-llama","tags":["meta-llama"],"draft":false,"id":"llama-3.2-1b-instruct","context":131072,"input":1e-8,"output":2e-8,"img":0,"request":0,"slug":"models/llama-3.2-1b-instruct"},"content":"\nLlama 3.2 1B is a 1-billion-parameter language model focused on efficiently performing natural language tasks, such as summarization, dialogue, and multilingual text analysis. Its smaller size allows it to operate efficiently in low-resource environments while maintaining strong task performance.\n\nSupporting eight core languages and fine-tunable for more, Llama 1.3B is ideal for businesses or developers seeking lightweight yet powerful AI solutions that can operate in diverse multilingual settings without the high computational demand of larger models.\n\nClick here for the [original model card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md).\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).\n\n"},{"lang":"en","group":"models","slug":"models/llama-3.2-90b-vision-instruct","frontmatter":{"title":"Meta: Llama 3.2 90B Vision Instruct","meta_title":"Meta: Llama 3.2 90B Vision Instruct","description":"Meta: Llama 3.2 90B Vision Instruct","date":"2024-09-25T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text image 2 text"],"author":"meta-llama","tags":["meta-llama"],"draft":false,"id":"llama-3.2-90b-vision-instruct","context":131072,"input":3.5e-7,"output":4e-7,"img":0.00050575,"request":0,"slug":"models/llama-3.2-90b-vision-instruct"},"content":"\nThe Llama 90B Vision model is a top-tier, 90-billion-parameter multimodal model designed for the most challenging visual reasoning and language tasks. It offers unparalleled accuracy in image captioning, visual question answering, and advanced image-text comprehension. Pre-trained on vast multimodal datasets and fine-tuned with human feedback, the Llama 90B Vision is engineered to handle the most demanding image-based AI tasks.\n\nThis model is perfect for industries requiring cutting-edge multimodal AI capabilities, particularly those dealing with complex, real-time visual and textual analysis.\n\nClick here for the [original model card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md).\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).\n\n"},{"lang":"en","group":"models","slug":"models/o1-mini","frontmatter":{"title":"OpenAI: o1-mini","meta_title":"OpenAI: o1-mini","description":"OpenAI: o1-mini","date":"2024-09-12T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"openai","tags":["openai"],"draft":false,"id":"o1-mini","context":128000,"input":0.000003,"output":0.000012,"img":0,"request":0,"slug":"models/o1-mini"},"content":"\nThe latest and strongest model family from OpenAI, o1 is designed to spend more time thinking before responding.\n\nThe o1 models are optimized for math, science, programming, and other STEM-related tasks. They consistently exhibit PhD-level accuracy on benchmarks in physics, chemistry, and biology. Learn more in the [launch announcement](https://openai.com/o1).\n\nNote: This model is currently experimental and not suitable for production use-cases, and may be heavily rate-limited.\n\n"},{"lang":"en","group":"models","slug":"models/o1-preview","frontmatter":{"title":"OpenAI: o1-preview","meta_title":"OpenAI: o1-preview","description":"OpenAI: o1-preview","date":"2024-09-12T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"openai","tags":["openai"],"draft":false,"id":"o1-preview","context":128000,"input":0.000015,"output":0.00006,"img":0,"request":0,"slug":"models/o1-preview"},"content":"\nThe latest and strongest model family from OpenAI, o1 is designed to spend more time thinking before responding.\n\nThe o1 models are optimized for math, science, programming, and other STEM-related tasks. They consistently exhibit PhD-level accuracy on benchmarks in physics, chemistry, and biology. Learn more in the [launch announcement](https://openai.com/o1).\n\nNote: This model is currently experimental and not suitable for production use-cases, and may be heavily rate-limited.\n\n"},{"lang":"en","group":"models","slug":"models/qwen-2-7b-instruct","frontmatter":{"title":"Qwen 2 7B Instruct","meta_title":"Qwen 2 7B Instruct","description":"Qwen 2 7B Instruct","date":"2024-07-16T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"qwen","tags":["qwen"],"draft":false,"id":"qwen-2-7b-instruct","context":32768,"input":5.4e-8,"output":5.4e-8,"img":0,"request":0,"slug":"models/qwen-2-7b-instruct"},"content":"\nQwen2 7B is a transformer-based model that excels in language understanding, multilingual capabilities, coding, mathematics, and reasoning.\n\nIt features SwiGLU activation, attention QKV bias, and group query attention. It is pretrained on extensive data with supervised finetuning and direct preference optimization.\n\nFor more details, see this [blog post](https://qwenlm.github.io/blog/qwen2/) and [GitHub repo](https://github.com/QwenLM/Qwen2).\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).\n\n"},{"lang":"en","group":"models","slug":"models/qwen-2-vl-72b-instruct","frontmatter":{"title":"Qwen2-VL 72B Instruct","meta_title":"Qwen2-VL 72B Instruct","description":"Qwen2-VL 72B Instruct","date":"2024-09-18T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text image 2 text"],"author":"qwen","tags":["qwen"],"draft":false,"id":"qwen-2-vl-72b-instruct","context":32768,"input":4e-7,"output":4e-7,"img":0.000578,"request":0,"slug":"models/qwen-2-vl-72b-instruct"},"content":"\nQwen2 VL 72B is a multimodal LLM from the Qwen Team with the following key enhancements:\n\n- SoTA understanding of images of various resolution & ratio: Qwen2-VL achieves state-of-the-art performance on visual understanding benchmarks, including MathVista, DocVQA, RealWorldQA, MTVQA, etc.\n\n- Understanding videos of 20min+: Qwen2-VL can understand videos over 20 minutes for high-quality video-based question answering, dialog, content creation, etc.\n\n- Agent that can operate your mobiles, robots, etc.: with the abilities of complex reasoning and decision making, Qwen2-VL can be integrated with devices like mobile phones, robots, etc., for automatic operation based on visual environment and text instructions.\n\n- Multilingual Support: to serve global users, besides English and Chinese, Qwen2-VL now supports the understanding of texts in different languages inside images, including most European languages, Japanese, Korean, Arabic, Vietnamese, etc.\n\nFor more details, see this [blog post](https://qwenlm.github.io/blog/qwen2-vl/) and [GitHub repo](https://github.com/QwenLM/Qwen2-VL).\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).\n\n"},{"lang":"en","group":"models","slug":"models/qwen-2.5-72b-instruct","frontmatter":{"title":"Qwen2.5 72B Instruct","meta_title":"Qwen2.5 72B Instruct","description":"Qwen2.5 72B Instruct","date":"2024-09-19T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"qwen","tags":["qwen"],"draft":false,"id":"qwen-2.5-72b-instruct","context":131072,"input":3.5e-7,"output":4e-7,"img":0,"request":0,"slug":"models/qwen-2.5-72b-instruct"},"content":"\nQwen2.5 72B is the latest series of Qwen large language models. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\n\n- Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n\n- Long-context Support up to 128K tokens and can generate up to 8K tokens.\n\n- Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).\n\n"},{"lang":"en","group":"models","slug":"models/qwen-2.5-7b-instruct","frontmatter":{"title":"Qwen2.5 7B Instruct","meta_title":"Qwen2.5 7B Instruct","description":"Qwen2.5 7B Instruct","date":"2024-10-16T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"qwen","tags":["qwen"],"draft":false,"id":"qwen-2.5-7b-instruct","context":131072,"input":2.7e-7,"output":2.7e-7,"img":0,"request":0,"slug":"models/qwen-2.5-7b-instruct"},"content":"\nQwen2.5 7B is the latest series of Qwen large language models. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\n\n- Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n\n- Long-context Support up to 128K tokens and can generate up to 8K tokens.\n\n- Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).\n\n"}]
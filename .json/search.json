[{"lang":"en","group":"blog","slug":"blog/a-new-risings-red-star-qwen2-5-is-here-0dffe0fb09ad","frontmatter":{"title":"A new risings Red star: Qwen2.5 is here","meta_title":"A new risings Red star: Qwen2.5 is here","description":"Let‚Äôs test together the new born Alibaba Cloud‚Äôs generative AI Qwen2.5 with python and llama-cpp","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*zU-XtqK2oMLkvscgxavjdw.png","categories":["Programming","Technology","Education"],"author":"Rifx.Online","tags":["Qwen2.5","multimodal","instruction-following","text-generation","multilingual"],"draft":false,"slug":"blog/a-new-risings-red-star-qwen2-5-is-here-0dffe0fb09ad"},"content":"\n\n\n\n\n### Let‚Äôs test together the new born Alibaba Cloud‚Äôs generative AI Qwen2.5 with python and llama-cpp\n\n\n\nIn silence, with not so many claims and anticipated announcements, Alibaba Cloud release on September the 19th their flagship model family Qwen2.5.\n\nAlibaba Cloud‚Äôs revolutionary journey with Qwen is showing once again strong Leadership through Innovation.\n\nHow? What‚Äôs so cool in them? And should we expect?\n\nIn this article we are going to explore the new models and check the performances. As a follow up, in the next article, we are going to use `llama-cpp-python` and the quantized version of qwen2.5‚Äì1.5b-instruct, putting the model under 13 NLP tasks test.\n\nIn fact I believe that we are the best Benchmark tool around and we are fully able to evaluate when a model is good for us!\n\nFor now, here what we are going to cover:\n\n\n```python\n- Qwen2.5 family innovation\n- Declared scope, use cases and models\n- Qwen2.5: a party of Foundation models\n- Expanding Reach through Open-Source Contributions\n- Bridging Industries through cutting-edge AI solutions\n- 13 Tasks to prove it worth \n- Future outlook: continued Open-Sourcing\n```\nLet‚Äôs dive in!\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*OeQ5qeOzCdl8LPJOZZgTIw.png)\n\n\n## Qwen2.5 family innovation\n\nQwen is the large language model and large multimodal model series of the Qwen Team, Alibaba Group. Just yesterday the large language models have been upgraded to Qwen2.5.\n\nBoth language models and multimodal models are pretrained on large-scale multilingual and multimodal data and post-trained on quality data for aligning to human preferences. Qwen is capable of natural language understanding, text generation, vision understanding, audio understanding, tool use, role play, playing as AI agent, etc.\n\nWith the recent release of Qwen2.5 and additional open-source model releases Alibaba Cloud continues its leadership position to meet rising AI demands from enterprise users. Since June last year, the Qwen family has attracted over 90,000 deployments via Model Studio in various industries including consumer electronics, automobiles, gaming, and more.\n\nQwen also expanded its reach with new models such as Qwen1.5‚Äì110B and CodeQwen1.5‚Äì7B on platforms like Hugging Face, showcasing Alibaba‚Äôs commitment to open-source AI development.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*A4pEOgsLK2PAFtiaGQx1Qw.png)\n\n\n## Declared scope, use cases and models\n\nIn the past three months since Qwen2‚Äôs release, numerous developers have built new models on the Qwen2 language models, providing valuable feedback to the entire community, but also to Alibaba Cloud.\n\n\n> During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5.\n\nTheir claims come with facts about the new family of models:\n\n* Dense, easy-to-use, decoder-only language models, available in 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B sizes, and base and instruct variants.\n* Pretrained on our latest large-scale dataset, encompassing up to 18T tokens.\n* Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON.\n* More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n* Context length support up to 128K tokens and can generate up to 8K tokens.\n* Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\n\n## Qwen2.5: a party of Foundation models\n\nAs announced on the [official blog press release](https://qwenlm.github.io/blog/qwen2.5/) on September 19, 2024:\n\n\n> Today, we are excited to introduce the latest addition to the Qwen family: **Qwen2.5**. We are announcing what might be the largest opensource release in history! Let‚Äôs get the party started!\n\n\n> Our latest release features the LLMs **Qwen2.5**, along with specialized models for coding, **Qwen2.5-Coder**, and mathematics, **Qwen2.5-Math**.\n\nTo showcase Qwen2.5‚Äôs capabilities, the Alibaba Cloud team benchmarked their largest open-source model, **Qwen2.5‚Äì72B** ‚Äî a 72B-parameter dense decoder-only language model ‚Äî against leading open-source models like Llama-3.1‚Äì70B and Mistral-Large-V2.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*-MMFgkkWHa307jNo.jpg)\n\nAll open-weight models are dense, decoder-only language models, available in various sizes, including:\n\n* Qwen2.5: 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B\n* Qwen2.5-Coder: 1.5B, 7B, and 32B on the way\n* Qwen2.5-Math: 1.5B, 7B, and 72B.\n\nAll these open-source models, except for the 3B and 72B variants, are licensed under Apache 2.0. You can find the license files in the respective Hugging Face repositories.\n\n\n> In addition to these models, we offer APIs for our flagship language models: **Qwen-Plus** and **Qwen-Turbo** through Model Studio, and we encourage you to explore them!\n\nBut this is not all!\n\n\n> ‚Ä¶we have also open-sourced the **Qwen2-VL-72B**, which features performance enhancements compared to last month‚Äôs release.\n\nIn terms of **Qwen2.5**, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to **18 trillion** tokens. Compared to Qwen2, Qwen2.5 has acquired significantly more knowledge (MMLU: 85+) and has greatly improved capabilities in coding (HumanEval 85+) and mathematics (MATH 80+). Additionally, the new models achieve significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*7c7CIbl-WVjazUeE.jpeg)\n\nQwen2.5 models are generally more resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n\nLike Qwen2, the Qwen2.5 language models support up to **128K** tokens and can generate up to **8K** tokens. They also maintain multilingual support for over **29** languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\n\n### Qwen-Coder is the new kid of the family\n\nThe specialized expert language models, namely **Qwen2.5-Coder** for coding and **Qwen2.5-Math** for mathematics, have undergone substantial enhancements compared to their predecessors, CodeQwen1.5 and Qwen2-Math. Specifically, Qwen2.5-Coder has been trained on **5.5 trillion** tokens of code-related data, enabling even smaller coding-specific models to deliver competitive performance against larger language models on coding evaluation benchmarks. Meanwhile, Qwen2.5-Math supports both **Chinese** and **English** and incorporates various reasoning methods, including Chain-of-Thought (CoT), Program-of-Thought (PoT), and Tool-Integrated Reasoning (TIR).\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Nvk4wrcB0SB4Tt-xbCzO6g.png)\n\n\n## Expanding Reach through Open-Source Contributions\n\nAs part of its continuous commitment to the broader community, Alibaba Cloud has made additional steps in releasing various sizes and variants of Qwen models. This includes:\n\n1. **Qwen 0.5 billion parameters**, a foundational version suitable for more traditional applications.2. A compact but potent model tailored specifically for gaming development: **Qwen-VL (vision-language)** optimized with high capabilities.\n\nThese advancements demonstrate Alibaba‚Äôs commitment to open-source AI, sharing not only the base versions of Qwen but also significant improvements and new models that are targeting directly the enterprise needs while enhancing their ability to innovate rapidly.\n\nThis aligns closely with a strategic vision where continuous contributions benefit both community members and its own clients as they seek innovative applications across multiple sectors.\n\n\n### Bridging Industries through cutting-edge AI solutions\n\nTo showcase the breadth of Qwen‚Äôs capabilities in real-world scenarios, Alibaba Cloud has been at the forefront:\n\n1. **Xiaomi**: the Company is integrating Alibaba‚Äôs models into their AI assistant, Xiao Ai, and deploying it within Xiaomi smartphones and electric vehicles to create enhanced features like car infotainment image generation via voice commands.\n\n2. **Perfect World Games**: the integration of Qwen in game development has led to innovative applications including improving plot resolution through dialogue dynamics and real-time content management.\n\nThe collaborations between Alibaba Cloud models and various industries have not only enriched the user experience but also facilitated greater opportunities for growth within these sectors, pushing boundaries that would otherwise be unimaginable without AI advancements.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*ku8o3rq6PHDE8xcc.png)\n\n\n## 13 Tasks to prove it worth\n\nThe 1.5 Billion parameters model is probably the best variant considering complexity, prompt understanding and inference speed.\n\nI will show you my internal testing using only `llama-cpp-python` and a simple terminal interface.\n\nTo do so, I created a list of prompt, covering a series of normally used tasks where you can also assign a vote (from 0 to 5) after every generation. It‚Äôs a personal human benchmark.\n\n\n### Requirements\n\nCreate a `venv` (python 3.11+ is required): I tested it on my Mini-PC running Windows 11.\n\n\n```python\n## create the virtual environment\npython -m venv venv\n## activate the venv\nvenv\\Scripts\\activate\n## Install the dependencies \npip install llama-cpp-python==0.2.90 tiktoken\n```\nWe need to download the GGUF file from the [official Qwen2.5 Hugging Face repo](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF). I used the qwen2.5‚Äì1.5b-instruct-q5\\_k\\_m.gguf version.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Fa-qFsx9RTFGZmM-vxCEPQ.png)\n\nDownload the file in the main project directory. And we are all set.\n\nThe code used here for the analysis is in my GitHub repository:\n\nI will explain the entire code and the results in the next article. Stay updated!\n\n\n## Future outlook: continued Open-Sourcing\n\nIn future plans, Alibaba has also expressed their commitment to ongoing open-source contributions by releasing smaller variants of Qwen for developers across different sectors. In reality in the Hugging Face community many users have started to fine-tune Qwen for dedicated tasks: I wrote an example in my article on NuExtract: the smaller variant of this model family is based on Qwen2‚Äì0.5b!\n\nThese developments in AI technology and model advancements are crucial steps towards leveraging the full potential of large language models like **Qwen** within a variety of industries. With robust adoption rates continuing to grow rapidly through Model Studio, it is clear that Alibaba Cloud has been a pioneer industry leader not only by providing advanced tools but also promoting innovation across enterprises.\n\nOn my side, my outlook are to proceed with internal testing on the new models, specifically on the small ones, up to 3B.\n\nIn the next article I will share with you my method, how to run the models and the prompt templates used for each of the thirteen NLP tasks.\n\nHope you enjoyed the article. If this story provided value and you wish to show a little support, you could:\n\n1. Clap a lot of times for this story\n2. Highlight the parts more relevant to be remembered (it will be easier for you to find them later, and for me to write better articles)\n3. **Join my [totally free weekly Substack newsletter here](https://thepoorgpuguy.substack.com/about)**\n4. Sign up for a Medium membership ($5/month to read unlimited Medium stories)\n5. Follow me on Medium\n6. Read my latest articles <https://medium.com/@fabio.matricardi>\n\nHere are a few more articles to feed your curiosity:\n\nResources references in this article:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Du7V61mEX_yIrfmF.png)\n\nThis story is published on [Generative AI](https://generativeai.pub/). Connect with us on [LinkedIn](https://www.linkedin.com/company/generative-ai-publication) and follow [Zeniteq](https://www.zeniteq.com/) to stay in the loop with the latest AI stories.\n\nSubscribe to our [newsletter](https://www.generativeaipub.com/) and [YouTube](https://www.youtube.com/@generativeaipub) channel to stay updated with the latest news and updates on generative AI. Let‚Äôs shape the future of AI together!\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*pvLAT3it1FkdhVU0.png)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/a-practical-guide-for-using-autogen-in-software-applications-8799185d27ee","frontmatter":{"title":"A practical guide for using AutoGen in software applications","meta_title":"A practical guide for using AutoGen in software applications","description":"Update: While this article was written only 4 months ago, AutoGen has since changed quite a bit. I apologize for some things that may be‚Ä¶","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*yrraWH6aGNnbx8p-wfQ1OQ.jpeg","categories":["Programming","Chatbots","Autonomous Systems"],"author":"Rifx.Online","tags":["AutoGen","multi-agent","LLMs","customization","collaboration"],"draft":false,"slug":"blog/a-practical-guide-for-using-autogen-in-software-applications-8799185d27ee"},"content":"\n\n\n\n\n\n*Update: While this article was written only 4 months ago, AutoGen has since changed quite a bit. I apologize for some things that may be outdated in my code examples.*\n\nIf you want to learn about AutoGen, there is [documentation](https://microsoft.github.io/autogen/), [Colab notebooks](https://microsoft.github.io/autogen/docs/Examples), and [a blog](https://microsoft.github.io/autogen/blog). Huge kudos to the AutoGen team for making an AMAZING product, but honestly ‚Äî after reading all their stuff, I still didn‚Äôt know how to use AutoGen outside of a terminal or Jupyter Notebook.\n\nThis article tries to help fill that gap by giving some helpful ways to make AutoGen work in a software application. Here are the topics I‚Äôll go over:\n\n1. Agents aren‚Äôt limited to communicating just over the terminal\n2. Registering custom replies\n3. How to include real humans in the conversation in real ways\n4. You can (and should) customize who speaks next\n5. You don‚Äôt have to use OpenAI\n6. Functions can be used instead of executing code\n7. Use Agents for organization, not just for conversations\n\nLastly, I‚Äôll go over why I think you should use AutoGen to begin with. Let‚Äôs go!\n\n\n## Agents aren‚Äôt limited to communicating just over the terminal\n\nYou‚Äôll see everyone demo AutoGen using a terminal or Jupyter Notebook. That‚Äôs nice for a demo, but there are other ways these agents can talk to each other.\n\nThere are 2 basic AutoGen classes: [`UserProxyAg`ent](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/user_proxy_agent.py) and [`AssistantAg`ent](https://github.com/microsoft/autogen/blob/main/autogen/agentchat/assistant_agent.py) . They inherit the [`ConversableAg`ent](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py) class, providing just a few different default parameters to the base class.\n\nWhen you see this classic code example:\n\n\n```python\nassistant = autogen.AssistantAgent(\n    name=\"assistant\",\n    llm_config=llm_config\n)\nuser_proxy = autogen.UserProxyAgent(name=\"user_proxy\")\nawait user_proxy.a_initiate_chat(\n    assistant,\n    message=\"\"\"What date is today? Compare the year-to-date gain for META and TESLA.\"\"\",\n)\n```\nwhat happens is that the `UserProxyAgent` will call its own `send` method, which will call `AssistantAgent` ‚Äòs [`rece`ive](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L514) method, passing along the original message. A reply will be generated (more on that below), and `AssistantAgent` will now call its [`s`end](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L351) method, which will then call `UserProxyAgent` ‚Äòs `receive` method, and so forth, until `UserProxyAgent` determines the conversation is terminated (which can be customized via the `is_termination_msg` argument).\n\nMy first ‚Äúaha‚Äù moment was when I realized these agents were classes, and I could create my own custom agent classes that inherit the AutoGen UserProxy/Assistant/Conversable Agent classes, and override any of the default methods. That makes AutoGen very extensible.\n\nI had a use-case where I needed a human who could type in a message (proxied by `UserProxyAgent`) using a chat UI on a website, and I wanted an `AssistantAgent` to respond back to that chat in the UI, and be able to receive more messages from the human user, as though the human was just another agent in this AutoGen conversation.\n\nI could override the `send` and `receive` methods (or `a_send` and `a_receive`), and push/pull over http, websockets, etc. I tried this, and it started to work, but doesn‚Äôt scale. Let‚Äôs learn a better way.\n\n\n## Registering custom replies\n\nAutoGen has a plugin system that lets you customize how an agent generates a reply. We‚Äôre used to seeing examples where AutoGen queries OpenAI for an answer, and uses that as its reply, but you can insert your own methods as well:\n\n\n```python\nclass WeatherAgent(AssistantAgent):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, llm_config=False, **kwargs)\n        self.register_reply(Agent, WeatherAgent.get_weather)\n\n    async def get_weather(\n        self,\n        messages: List[Dict] = [],\n        sender=None,\n        config=None,\n    ) -> Tuple[bool, Union[str, Dict, None]]:\n        last_message = messages[-1][\"content\"]\n        result = await fetch_weather(last_message)\n        return True, result\n\nasync def fetch_weather(city: str) -> str:\n    async with httpx.AsyncClient() as client:\n        result = await client.post(\n            WEATHER_API_URL,\n            json={\"city\": question},\n        )\n        return result.json()\n\nweather_assistant = WeatherAgent(name=\"weather_assistant\")\nuser_proxy = autogen.UserProxyAgent(name=\"user_proxy\")\nawait user_proxy.a_initiate_chat(assistant, message=\"Lehi\")\nprint(weather_assistant.last_message)\n```\nHere, `register_reply` will insert my custom method for getting a reply, and by default, will put this method in `position=0`, meaning it will be the first reply method attempted. That method should return a tuple, where the first item is a boolean indicating if this reply is the one that should be used or whether to try the next registered\\_reply (such as the built-in reply generations using OpenAI ‚Äî see the full order [here](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L145-L153)).\n\nKnowing about [`register_re`ply](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L155) allows you to customize how replies are retrieved, allow you to start sub multi-agent conversations, etc.\n\n\n## How to include real humans in the conversation in real ways\n\nHere‚Äôs one way to do it:\n\n\n```python\n## user makes a POST /query { \"message\": \"What's the weather?\" }\n\n@query_blueprint.route(\"/query\", methods=[\"POST\"])\nasync def post_query():\n  message = request.form.get(\"message\")\n\n  assistant = autogen.AssistantAgent(\n    name=\"assistant\",\n    llm_config=llm_config\n    system_message=\"\"\"You're a helpful assistant.\n    If you need more info, ask the user for anything missing.\"\"\"\n  )\n  user_proxy = autogen.UserProxyAgent(\n    name=\"user_proxy\",\n    human_input_mode=\"NEVER\",\n    code_execution_config=False,\n    is_termination_msg=lambda message: True # Always True\n  )\n  weather_assistant = WeatherAgent(\n    name=\"weather_assistant\",\n    system_message=\"\"\"You're a helpful assistant to get the weather.\n    You fetch weather information, then return it.\"\"\"\n  )\n\n  groupchat = autogen.GroupChat(\n    agents=[assistant, user_proxy, weather_assistant],\n    messages=[]\n  )\n  manager = autogen.GroupChatManager(\n    name=\"Manager\",\n    groupchat=groupchat,\n    llm_config=llm_config,\n  )\n\n  await user_proxy.a_initiate_chat(manager, message=message)\n\n  return groupchat.messages[-1]\n```\nWhat‚Äôs going on here?\n\n1. Anytime a message is sent to `user_proxy`, the conversation will end (we‚Äôll resume it later). Why do this? This means the `user_proxy` can actually proxy for the user. Rather than try to answer, it will end the current conversation flow and allow the real human user to respond (by resuming the conversation ‚Äî see below).\n2. If the assistant needs more info, it‚Äôll ask user\\_proxy, which will end the current conversation.\n\nIn the above code, what is likely to occur is something like this:\n\n1. user\\_proxy -> manager: ‚ÄúWhat‚Äôs the weather?‚Äù\n2. assistant -> manager: ‚ÄúThe user didn‚Äôt specify for which city.‚Äù\n3. manager -> user\\_proxy : conversation will end\n\nNow, if the user wants to respond and resume the conversation, how would we do that? There‚Äôs lots of ways to do this, here‚Äôs just a sample flavor:\n\n\n```python\n## user makes a POST /query { \"message\": \"What's the weather?\" }\n## above posts returns a `history` array\n## user makes a second POST /query { \"message\": \"What's the weather?\", \"history\": history }\n\nclass ResumableGroupChatManager(GroupChatManager):\n    groupchat: GroupChat\n\n    def __init__(self, groupchat, history, **kwargs):\n        self.groupchat = groupchat\n        if history:\n            self.groupchat.messages = history\n\n        super().__init__(groupchat, **kwargs)\n\n        if history:\n            self.restore_from_history(history)\n\n    def restore_from_history(self, history) -> None:\n        for message in history:\n            # broadcast the message to all agents except the speaker.  This idea is the same way GroupChat is implemented in AutoGen for new messages, this method simply allows us to replay old messages first.\n            for agent in self.groupchat.agents:\n                if agent != self:\n                    self.send(message, agent, request_reply=False, silent=True)\n\n@query_blueprint.route(\"/query\", methods=[\"POST\"])\nasync def post_query():\n  message = request.form.get(\"message\")\n\n  assistant = autogen.AssistantAgent(\n    name=\"assistant\",\n    llm_config=llm_config\n    system_message=\"\"\"You're a helpful assistant.\n    If you need more info, ask the user for anything missing.\"\"\"\n  )\n  user_proxy = autogen.UserProxyAgent(\n    name=\"user_proxy\",\n    human_input_mode=\"NEVER\",\n    code_execution_config=False,\n    is_termination_msg=lambda message: True # Always True\n  )\n  weather_assistant = WeatherAgent(\n    name=\"weather_assistant\",\n    system_message=\"\"\"You're a helpful assistant to get the weather.\n    You fetch weather information, then return it.\"\"\"\n  )\n\n  groupchat = autogen.GroupChat(\n    agents=[assistant, user_proxy, weather_assistant],\n    messages=[]\n  )\n  manager = ResumableGroupChatManager(\n    name=\"Manager\",\n    groupchat=groupchat,\n    llm_config=llm_config,\n  )\n\n  await user_proxy.a_initiate_chat(manager, message=message)\n\n  return {\n    \"response\": groupchat.messages[-1],\n    \"history\": groupchat.messages,\n  }\n```\nUsing this approach, you can now include humans as though they were just another agent in the groupchat. Anytime an assistant agent wants human input, they ask user\\_proxy, user\\_proxy then ends the current conversation, allowing the human user to respond with more information, then pick up the conversation where it left off.\n\nThe benefits to this approach are:\n\n* Conversations can include real human input via any means you want (such as over http or websocket).\n* The conversation is stopped while getting human input. This frees up the thread for other conversations and computation.\n* You can persist these conversations across sessions.\n\n\n## You can (and should) customize who speaks next\n\nThis is subjective, but I think you should always customize the way speakers are selected because:\n\n1. You‚Äôll use less tokens (saves both $ and response time)\n2. You can separate the logic that decides who speaks next from the logic that defines the system instructions for each agent\n\n\n```python\nshort_role_descriptions = {\n  \"user_proxy\": \"A proxy for the user\",\n  \"weather_assistant\": \"You can get the weather\",\n  \"planner\": \"You help coordinate the plan. Your turn happens when XYZ, but skip your turn when ABC\"\n}\n\nclass CustomGroupChat(GroupChat):\n    # The default message uses the full system message, which is a long string.  We are overriding this to use a shorter message.\n    def select_speaker_msg(self, agents: List[Agent]):\n        message = f\"\"\"You are in a role play game. The following roles are available:\n        ---\n        {new_line.join([f\"{agent.name}: {short_role_descriptions[agent.name]}\" for agent in agents])}\n        ---\n\n        The role who plays next depends on the conversation.  User_Proxy will star the conversation, and typically Planner would go next.\n\n        Here are some examples\n        ---\n        ... not shown here ...\n        ---\n\n        Read the following conversation.\n        Then select the next role from {', '.join([agent.name for agent in agents])} to play. Only return the role.\"\"\"\n        return message\n```\n\n## You don‚Äôt have to use OpenAI\n\nAutoGen already notes you can use other LLMs, as long as they are ‚ÄúChatGPT-like‚Äù, meaning their API responds with a similar shape and response as ChatGPT API calls.\n\nBut, remember how these agents are classes, and you can override most of the methods?\n\nTry overriding the method: [generate\\_oai\\_reply](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L678), and you can query any LLM you‚Äôd like.\n\n\n## Functions can be used instead of executing code\n\nWhen I went to our security team and said ‚ÄúI‚Äôd like to use AutoGen for my service in Kubernetes. It needs to be able to execute any arbitrary code produced by an LLM. You‚Äôre ok with that, right?‚Äù\n\nOf course, the answer was a definite: NO.\n\nSo, why use AutoGen without the auto-code-execution abilities?\n\nOn top of the reasons stated below, another is that you can use function calling to gain total control over code execution. If you have a set of python functions you want to provide to AutoGen ‚Äî functions you wrote, control, and can accept some safe parameters ‚Äî that sounds like a better idea anyway than the wild west of allowing any and all code to be executed in your private infrastructure.\n\n\n## Use Agents for organization, not just for conversations\n\nMaybe you don‚Äôt have a need for an autonomous, multi-agent conversation. Maybe you just need to make a few different calls to an LLM.\n\nI still like the idea of having different ‚ÄúAgents‚Äù just for the sake of organization. Here‚Äôs a really crazy idea, but take it for what it‚Äôs worth:\n\n\n```python\nanalyst = autogen.AssistantAgent(\n    name=\"Analyst\",\n    system_message=\"\"\"Your an analyst.  You do XYZ.\"\"\",\n    llm_config=llm_config,\n)\n\nsummarizer = autogen.AssistantAgent(\n    name=\"Summarizer\",\n    system_message=\"\"\"Your a summarizer.  You do XYZ.\"\"\",\n    llm_config=llm_config,\n)\n\nreport = \"\"\"Some long report\"\"\"\n\nanalysis = analyst.generate_oai_reply(report)[1]\nsummary = summarizer.generate_oai_reply(report)[1]\n\nprint(f\"Analysis: {analysis}\")\nprint(f\"Summary: {summary}\")\n```\n\n## Why use AutoGen?\n\n1. AutoGen allows multiple agents, with different system prompts and instructions, to solve a problem. Just like in real-life, different perspectives working together will solve a problem better than a single brain.\n2. AutoGen GroupChat is amazing. It provides routing to the right experts (agents), and it allows a conversation to continue autonomously until the problem is solved. Some conversations will go from agent: a->b->c->d, others will be b->a->d->c. This allows AutoGen to solve a variety of different problems without needing explicit rules for each scenario.\n3. AutoGen can recover from mistakes. For example, I made an AutoGen-powered service that made API calls to a service. Sometimes, the API calls errored out because it didn‚Äôt send the right data at first. The AutoGen GroupChat kept trying different things until it succeeded. Sometimes, it took 4+ attempts, but my Planner agent didn‚Äôt give up ‚Äî just pivoted autonomously to handle the API failures and try new things.\n4. AutoGen came up with the concept of separating `UserProxyAgent`s from `AssistantAgent` s from the beginning. This also allows us to let the user proxy actually proxy for the user, as shown above.\n5. AutoGen is a well maintained library. Every week they‚Äôre adding something new.\n6. AutoGen is very extensible. With the way they‚Äôve built their classes, you can customize anything to your liking.\n7. AutoGen has other features I don‚Äôt use, but others may find them helpful, such as helping you count tokens and cost of conversations, cacheing, etc.\n\n"},{"lang":"en","group":"blog","slug":"blog/alibabas-open-source-qwen-how-it-s-revolutionizing-ai-and-how-you-can-use-it-dcba8f687c97","frontmatter":{"title":"Alibaba‚Äôs Open-Source Qwen: How It‚Äôs Revolutionizing AI and How You Can Use It","meta_title":"Alibaba‚Äôs Open-Source Qwen: How It‚Äôs Revolutionizing AI and How You Can Use It","description":"Alibaba has recently made waves in the AI world by open-sourcing its Qwen 2.5 models during the 2024 Apsara Conference. With over 100‚Ä¶","date":"2024-10-26T00:26:25.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*I7QDwbLMzoJ_ORq5.jpg","categories":["Programming","Machine Learning","Natural Language Processing"],"author":"Rifx.Online","tags":["Qwen","multimodal","open-source","fine-tune","text-to-video"],"draft":false,"slug":"blog/alibabas-open-source-qwen-how-it-s-revolutionizing-ai-and-how-you-can-use-it-dcba8f687c97"},"content":"\nAlibaba has recently made waves in the AI world by open-sourcing its **Qwen 2.5** models during the 2024 Apsara Conference. With over 100 models, Qwen spans multiple modalities including language, vision, audio, and code, making it one of the most comprehensive open-source AI solutions. The release empowers developers by providing tools for diverse applications, from text-to-video generation to real-time question answering.\n\n\n\n## Key Features of Alibaba‚Äôs Qwen Models\n\n1. **Multimodal Capabilities**: Qwen models handle diverse inputs, including text, audio, and visual data. This multimodal approach makes them suitable for a wide range of industries, from media and entertainment to robotics.\n2. **Open Source**: Available on platforms like **Hugging Face** and **ModelScope**, Qwen has already been downloaded over 40 million times, with over 50,000 custom models built on its foundation.\n3. **Enhanced Performance**: Qwen2.5 introduces improved language understanding, mathematics, and coding capabilities, rivaling leading models in the field. With optimized performance for tasks like structured data understanding and long text generation, Qwen opens the door to high-level AI applications.\n\n## How to Use Alibaba‚Äôs Qwen\n\nDevelopers and organizations can access Qwen models on platforms like Hugging Face, where they can:\n\n* **Fine-tune models**: Tailor Qwen for specific industry applications such as customer service, automation, or video content creation.\n* **Integrate with applications**: Qwen‚Äôs text-to-video model can be incorporated into media production pipelines, generating dynamic content from static images and text prompts.\n* **Develop AI assistants**: With enhanced vision-language models, Qwen can be used in robotics and autonomous vehicles to process video data and perform real-time tasks like navigation or object recognition.\n\n**Example of Using Qwen via Hugging Face**:\n\n```python\nfrom transformers import QwenTokenizer, QwenModel\n\ntokenizer = QwenTokenizer.from_pretrained(\"qwen-2.5\")\nmodel = QwenModel.from_pretrained(\"qwen-2.5\")\n\ninput_text = \"What is the future of AI in healthcare?\"\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\noutputs = model(input_ids)\n```\n\nThis allows users to access Qwen models, run inference, and customize them based on specific needs.\n\n## Qwen‚Äôs Impact Across Industries\n\n1. **Media and Entertainment**: With the new text-to-video capabilities, Qwen can automatically generate videos from written scripts, transforming the creative industry by automating tedious production tasks.\n2. **Robotics and Autonomous Vehicles**: The enhanced vision-language models in Qwen can help robots understand real-world environments, leading to better decision-making in autonomous driving or manufacturing.\n3. **Software Development**: Alibaba‚Äôs AI Developer tool, powered by Qwen, automates tasks like code generation, debugging, and requirement analysis, enabling developers to focus on higher-level problem-solving.\n\n## Conclusion: A New Era of Open AI Innovation\n\nBy open-sourcing its Qwen 2.5 models, Alibaba is democratizing access to advanced AI technologies. Developers, startups, and large enterprises alike can harness Qwen‚Äôs multimodal and real-time capabilities to drive innovation in industries ranging from media to autonomous vehicles. Whether you‚Äôre a developer looking to fine-tune models for a niche application or a corporation integrating AI into your infrastructure, Qwen offers powerful tools to accelerate progress.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/building-a-local-ai-powered-news-aggregator-with-ollama-swarm-and-duckduckgo-95aaf8b3ee41","frontmatter":{"title":"Building a Local AI-Powered News Aggregator with Ollama, Swarm, and DuckDuckGo","meta_title":"Building a Local AI-Powered News Aggregator with Ollama, Swarm, and DuckDuckGo","description":"No subtitle provided","date":"2024-10-24T17:47:43.000Z","image":"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*OHMOTk_WYGOxWHBsKqdpNQ.jpeg","categories":["Programming","Generative AI","Technology/Web"],"author":"Rifx.Online","tags":["Llama","Swarm","DuckDuckGo","News","Aggregator"],"draft":false,"slug":"blog/building-a-local-ai-powered-news-aggregator-with-ollama-swarm-and-duckduckgo-95aaf8b3ee41"},"content":"\n\n# Building a Local AI-Powered News Aggregator with Ollama, Swarm, and DuckDuckGo\n\n\n\nIn today‚Äôs fast-paced world, staying up-to-date with the latest news in specific fields can be challenging. What if we could leverage the power of Generative AI and Agents to create a personalized news aggregator that runs entirely on our local machine? In this article, we‚Äôll explore how to build such a system using **Ollama**‚Äôs Llama 3.2 model, **Swarm** for agent orchestration, and **DuckDuckGo** for web searches.\n\n\n# The Power of Local AI\n\nWith the rise of large language models, we now have the ability to run sophisticated AI systems on our personal computers. This opens up a world of possibilities for creating customized tools tailored to our specific needs. Our news aggregator is a perfect example of this potential.\n\n\n# Components of Our System\n\n1. **Ollama with Llama 3.2**: This serves as the brain of our system, powering our AI agents.\n2. **Swarm**: An agent orchestration framework that allows us to create and manage multiple AI agents.\n3. **DuckDuckGo Search**: Provides up-to-date web search results without tracking user data.\n\n\n# How It Works\n\nOur news aggregator consists of two main AI agents:\n\n1. **News Assistant**: Fetches the latest news articles on a given topic using DuckDuckGo search.\n2. **Editor Assistant**: Reviews and refines the collected news for final presentation.\n\nLet‚Äôs break down the workflow:\n\n\n# 1. Setting Up the Environment\n\n\n```python\nollama pull llama3.2\n\nexport OPENAI_MODEL_NAME=llama3.2\nexport OPENAI_BASE_URL=http://localhost:11434/v1\nexport OPENAI_API_KEY=any\n\npip install git+https://github.com/openai/swarm.git duckduckgo-search\n```\nWe start by importing the necessary libraries and initializing our Swarm client:\n\n\n```python\nfrom duckduckgo_search import DDGS\nfrom swarm import Swarm, Agent\nfrom datetime import datetime\n\ncurrent_date = datetime.now().strftime(\"%Y-%m\")\nclient = Swarm()\n```\n\n# 2. Creating the News Search Function\n\nWe define a function to search for news using DuckDuckGo:\n\n\n```python\npythondef get_news_articles(topic):\n  ddg_api = DDGS()\n  results = ddg_api.text(f\"{topic} {current_date}\", max_results=5)\n  if results:\n      news_results = \"\\n\\n\".join([f\"Title: {result['title']}\\nURL: {result['href']}\\nDescription: {result['body']}\" for result in results])\n      return news_results\n  else:\n      return f\"Could not find news results for {topic}.\"\n```\n\n# 3. Defining Our AI Agents\n\nWe create two agents using Ollama‚Äôs Llama 3.2 model:\n\n\n```python\nnews_agent = Agent(\n  model=\"llama3.2\",\n  name=\"News Assistant\",\n  instructions=\"You provide the latest news articles for a given topic using DuckDuckGo search.\",\n  functions=[get_news_articles],\n)\n\neditor_agent = Agent(\n  model=\"llama3.2\",\n  name=\"Editor Assistant\",\n  instructions=\"You review and finalise the news article for publishing.\",\n)\n```\n\n# 4. Orchestrating the Workflow\n\nWe define a function to run our news aggregation workflow:\n\n\n```python\ndef run_news_workflow(topic):\n  # Fetch news\n  news_response = client.run(\n      agent=news_agent,\n      messages=[{\"role\": \"user\", \"content\": f\"Get me the news about {topic} on {current_date}\"}],\n  )\n  raw_news = news_response.messages[-1][\"content\"]\n  \n  # Pass news to editor for final review\n  edited_news_response = client.run(\n      agent=editor_agent,\n      messages=[{\"role\": \"system\", \"content\": raw_news}],\n  )\n  print(f\"{edited_news_response.messages[-1]['content']}\")\n```\n\n# 5. Running the System\n\nFinally, we can run our news aggregator for any topic of interest:\n\n\n```python\nrun_news_workflow(\"AI in Drug Discovery\")\n```\n\n# Complete Code : app.py\n\n\n```python\nfrom duckduckgo_search import DDGS\nfrom swarm import Swarm, Agent\nfrom datetime import datetime\n\ncurrent_date = datetime.now().strftime(\"%Y-%m\")\n\n# Initialize Swarm client\nclient = Swarm()\n\n# 1. Create Internet Search Tool\n\ndef get_news_articles(topic):\n    print(f\"Running DuckDuckGo news search for {topic}...\")\n    \n    # DuckDuckGo search\n    ddg_api = DDGS()\n    results = ddg_api.text(f\"{topic} {current_date}\", max_results=5)\n    if results:\n        news_results = \"\\n\\n\".join([f\"Title: {result['title']}\\nURL: {result['href']}\\nDescription: {result['body']}\" for result in results])\n        return news_results\n    else:\n        return f\"Could not find news results for {topic}.\"\n    \n# 2. Create AI Agents\n\ndef transfer_to_editor_assistant(raw_news):\n    print(\"Passing articles to Editor Assistant...\")\n    return editor_agent.run({\"role\": \"system\", \"content\": raw_news})\n\n# News Agent to fetch news\nnews_agent = Agent(\n    model=\"llama3.2\",\n    name=\"News Assistant\",\n    instructions=\"You provide the latest news articles for a given topic using DuckDuckGo search.\",\n    functions=[get_news_articles],\n)\n\n# Editor Agent to edit news\neditor_agent = Agent(\n    model=\"llama3.2\",\n    name=\"Editor Assistant\",\n    instructions=\"You review and finalise the news article for publishing.\",\n)\n\n# 3. Create workflow\n\ndef run_news_workflow(topic):\n    print(\"Running news Agent workflow...\")\n    \n    # Step 1: Fetch news\n    news_response = client.run(\n        agent=news_agent,\n        messages=[{\"role\": \"user\", \"content\": f\"Get me the news about {topic} on {current_date}\"}],\n    )\n    raw_news = news_response.messages[-1][\"content\"]\n    print(f\"Fetched news: {raw_news}\")\n    \n    # Step 2: Pass news to editor for final review\n    edited_news_response = client.run(\n        agent=editor_agent,\n        messages=[{\"role\": \"system\", \"content\": raw_news}],\n    )\n    print(f\"{edited_news_response.messages[-1]['content']}\")\n\n\n# Example of running the news workflow for a given topic\nrun_news_workflow(\"AI in Drug Discovery\")\n```\n\n# Sample Output\n\n\n```python\nRunning news Agent workflow...\nRunning DuckDuckGo news search for AI in Drug Discovery...\nFetched news: Here's a formatted answer based on the news articles:\n\n**AI in Drug Discovery: A Revolutionary Shift**\n\nThe role of Artificial Intelligence (AI) in drug discovery has marked a revolutionary shift in the pharmaceutical landscape. AI leverages sophisticated algorithms for autonomous decision-making from data analysis, augmenting human capabilities rather than replacing them.\n\n**Challenges and Limitations**\n\nDespite the promising advancements, challenges and limitations have been identified in the field. The paper \"The Role of AI in Drug Discovery\" addresses these issues, highlighting the need for high-quality data, addressing ethical concerns, and recognizing the limitations of AI-based approaches.\n\n**Applications of AI in Drug Discovery**\n\nAI has the potential to play a critical role in drug discovery, design, and studying drug-drug interactions.Applications of AI in drug discovery include:\n\n* Polypharmacology: AI can predict the likelihood of a compound's effectiveness against multiple diseases.\n* Chemical synthesis: AI can optimize chemical synthesis processes for faster and more efficient production.\n* Drug repurposing: AI can identify new uses for existing drugs.\n* Predicting drug properties: AI can predict the efficacy, toxicity, and physicochemical characteristics of compounds.\n\n**The Future of AI in Drug Discovery**\n\nAs AI continues to evolve, it is expected to significantly impact the pharmaceutical industry. The successful application of AI will depend on the availability of high-quality data, addressing ethical concerns, and recognizing the limitations of AI-based approaches.\n```\n\n# The Benefits of Local AI News Aggregation\n\n* **Privacy**: All processing happens on your local machine, ensuring your data stays with you.\n* **Customization**: You can easily modify the agents‚Äô instructions or add new agents to suit your specific needs.\n* **Up-to-date Information**: By using DuckDuckGo search, you always get the latest news on your chosen topic.\n* **AI-powered Curation**: The Editor Assistant helps refine and organize the collected news, providing a more polished final output.\n\n\n# Conclusion\n\nThis local AI-powered news aggregator demonstrates the potential of combining large language models with web search capabilities. By leveraging Ollama‚Äôs Llama 3.2 model, Swarm for agent orchestration, and DuckDuckGo for search, we‚Äôve created a powerful tool that can keep us informed on any topic of interest, all while maintaining our privacy and running entirely on our local machine.\n\nAs AI continues to evolve, the possibilities for creating personalized, AI-driven tools will only expand. This news aggregator is just the beginning ‚Äî imagine what other innovative applications you could build using these technologies!\n\n\n# Reference :\n\nSwarm Github : <https://github.com/openai/swarm>\n\nIf you found this article informative and valuable, I‚Äôd greatly appreciate your support:\n\n* Give it a few claps üëè on Medium to help others discover this content (did you know you can clap up to 50 times?). Your claps will help spread the knowledge to more readers.\n- Share it with your network of AI enthusiasts and professionals.\n- Connect with me on LinkedIn: <https://www.linkedin.com/in/manjunath-janardhan-54a5537/>\n\n\n\n\n\n"},{"lang":"en","group":"blog","slug":"blog/claude-3-5-sonnet-new-pioneering-the-future-of-ai-with-computer-control-capabilities-37a6ff9f9033","frontmatter":{"title":"Claude 3.5 Sonnet (New): Pioneering the Future of AI with Computer Control Capabilities","meta_title":"Claude 3.5 Sonnet (New): Pioneering the Future of AI with Computer Control Capabilities","description":"Anthropic has unveiled its latest AI model, Claude 3.5 Sonnet, on October 22, 2024. This release introduces revolutionary computer control‚Ä¶","date":"2024-10-27T13:57:00.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*n0NkOFbhUm7_fllJ","categories":["Programming","Technology","Generative AI"],"author":"Rifx.Online","tags":["Claude","Sonnet","automation","benchmarks","safety"],"draft":false,"slug":"blog/claude-3-5-sonnet-new-pioneering-the-future-of-ai-with-computer-control-capabilities-37a6ff9f9033"},"content":"\n\n\n\n\n\nAnthropic has unveiled its latest AI model, Claude 3.5 Sonnet, on October 22, 2024. This release introduces revolutionary computer control capabilities and substantial improvements across various benchmarks, setting new standards in the AI industry.\n\n\n## Revolutionary Computer Control: A New Frontier\n\nThe standout feature of Claude 3.5 Sonnet is its ability to interact with computers just like humans do. This groundbreaking capability allows the AI to:\n\n* Navigate desktop interfaces using mouse and keyboard inputs\n* Interact with various applications and web browsers\n* Execute complex multi-step tasks\n* Perform file management operations\n* Automate repetitive workflows\n\nThis computer control feature, currently in public beta, represents a paradigm shift in how AI systems can interact with digital interfaces. While still in its experimental phase, early testing shows promising results, with Claude 3.5 Sonnet scoring 14.9% on the OSWorld benchmark for screenshot-only tasks ‚Äî significantly higher than the next-best system‚Äôs 7.8%.\n\n\n## Benchmark-Breaking Performance\n\nThe upgraded model demonstrates remarkable improvements across various metrics:\n\n\n## Coding and Technical Tasks\n\n* 49% performance on SWE-bench Verified (up from 33.4%)\n* 93.7% score on HumanEval coding tasks\n* Superior performance in software engineering compared to specialized coding systems\n\n\n## Academic and Reasoning Capabilities\n\n* 65% on graduate-level reasoning (GPQA-Diamond)\n* 78% on undergraduate-level knowledge (MMLU Pro)\n* 78.3% on mathematical problem-solving (MATH)\n\n\n## Business Applications\n\n* 69.2% on retail domain tasks (TAU-bench)\n* 46% on airline domain tasks\n* 90.8% accuracy on chart analysis\n* 94.2% accuracy on document Q&A\n\n\n## Enterprise Integration and Availability\n\nClaude 3.5 Sonnet is accessible through multiple platforms:\n\n* Anthropic API\n* Amazon Bedrock\n* Google Cloud‚Äôs Vertex AI\n\nMajor companies including Asana, Canva, DoorDash, and Replit have already begun implementing Claude 3.5 Sonnet‚Äôs capabilities in their workflows, particularly leveraging its computer control features for complex automation tasks.\n\n\n## Practical Applications\n\n\n## Software Development\n\n* Automated code testing and debugging\n* Intelligent IDE interactions\n* Code review and optimization\n* Documentation generation\n\n\n## Customer Support\n\n* Advanced chatbot capabilities\n* Visual data interpretation\n* Automated ticket resolution\n* Process automation\n\n\n## Business Operations\n\n* Document processing and analysis\n* Data extraction from visual sources\n* Workflow automation\n* Complex problem-solving\n\n\n## Safety and Responsibility\n\nAnthropic has implemented robust safety measures for the computer control feature:\n\n* New classifiers to identify potential misuse\n* Proactive monitoring systems\n* Restricted access to sensitive operations\n* Regular safety assessments\n\n\n## Looking Ahead\n\nWhile Claude 3.5 Sonnet represents a significant advancement in AI capabilities, it‚Äôs important to note that some features, particularly computer control, are still in their early stages. Certain actions like scrolling, dragging, and zooming present challenges, and Anthropic encourages developers to begin with low-risk tasks while exploring these new capabilities.\n\nThe release of Claude 3.5 Sonnet marks a pivotal moment in AI development, combining advanced reasoning capabilities with practical computer control features. As the technology continues to evolve, we can expect to see even more innovative applications and improvements in how AI systems interact with our digital world.\n\n*This article is based on official announcements and documentation from Anthropic, AWS, and various technology partners. For the most up-to-date information, please refer to Anthropic‚Äôs official documentation.*\n\n\n"},{"lang":"en","group":"blog","slug":"blog/claude-3-5-sonnet-v-s-gpt-4o-which-one-is-better-3b3675195bf9","frontmatter":{"title":"Claude 3.5 Sonnet V/S GPT-4O: Which one is better","meta_title":"Claude 3.5 Sonnet V/S GPT-4O: Which one is better","description":"In November 2022, OpenAI launched ChatGPT, a model that has revolutionized how we search and interact with information. Next year, in‚Ä¶","date":"2024-10-27T13:59:09.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*4MXLuSFfGwFkWWn0","categories":["Generative AI","Machine Learning","Natural Language Processing"],"author":"Rifx.Online","tags":["GPT-4o","Claude","multimodal","reasoning","code-generation"],"draft":false,"slug":"blog/claude-3-5-sonnet-v-s-gpt-4o-which-one-is-better-3b3675195bf9"},"content":"\n\n\n\nIn November 2022, OpenAI launched ChatGPT, a model that has revolutionized how we search and interact with information. Next year, in March, an American startup,‚Äù Anthropic,‚Äù founded by ex-OpenAI employees, launched their own AI model, ‚ÄúClaude.‚Äù Since the launch, both AI companies have been competing to bring the best to customers regarding features and experience through their AI models. Recently, OpenAI launched ‚ÄúGPT-4o,‚Äù a spectacular model that handles file, voice, and video data amazingly. Similarly, Claude launched the ‚ÄúClaude 3.5 Sonnet,‚Äù which is the most advanced AI model, as they claimed, and can handle complex tasks. In this article, we will determine which is better, between Claude 3.5 Sonnet and GPT-4o, and compare its features and output with the same input to check which is better for you.\n\n\n## Capabilities and Features\n\n\n### GPT-4o\n\n\n\nGPT-4o is the latest LLM launched by OpenAI. The ‚Äúo‚Äù stands for omni, which means ‚Äúevery‚Äù in Latin. This model can analyze voice, images, videos, and files as input and respond accordingly. It can take voice input and give the output in different characters‚Äô voices, including tones, emotions, etc. The whole process is as low as a human conversation, with an average of 0.32 seconds compared to other voice models, which is 2.8 seconds. It also allows users to generate written content such as articles, blogs, product descriptions, code in different programming languages, data analysis, charts, etc. In addition, GPT-4o can also analyze images and videos, which makes the model act as a language translator, personal assistant, virtual teacher, or shopping assistant. It can also be used in medicine, engineering, the military, etc. To use this feature, GPT-4o can use the user‚Äôs camera to get a real-time view and respond accordingly in the voice mode. It can also access your computer screen and describe what is shown on the screen, users can ask questions related to the stuff displayed on the screen.\n\n*For example, users can enable the model on the screen, open the VS code, and prompt the model to act as a coding assistant to get answers to the coding problems. Alternatively, you can enable the camera to act as a fitness trainer whether you are doing it correctly or not.*\n\nThe model has unique features, such as data analysis, code interpreter, and real-time web browsing, making it different from its competitors. The model also has a plethora of GPTs, which is a tailored version of ChatGPT.\n\n\n### Claude 3.5 Sonnet\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*BSMcOpvWZ5lUm4Tl)\n\nClaude 3.5 Sonnet is the AI chatbot launched by Anthropic. It is the third generation of the family of Claude AI model series. This model has stood at a high bae and outperformed many AI models on various evaluations, keeping the hallucinations and wrong information away. While it doesn‚Äôt support voice and video features like GPT-4o, it can also perform all the basic tasks, such as text generation and code generation in different programming languages, brainstorming ideas, etc. According to the report by Anthropic, Claude 3.5 Sonnet is one of the best computer vision models in the market, which can be used to analyze charts and graphs, transcribe texts from images, and many more. Claude is powered by an advanced feature, ‚ÄúArtifacts,‚Äù a special popup window along the conversation, allowing the users to check the code snippets, text documents, or website designs and allow them to edit the output in real-time.\n\n*For example, users can use computer vision and artifacts in their workflows. Users can make essential prototyping of a website‚Äôs design on paper, attach the file with Claude 3.5 Sonnet, and prompt it to design a website based on the prototype. The generated code and the website design appear in the artifacts. Users can edit the code and the design according to their requirements. Users can also publish their projects live on the Internet.*\n\n\n## Head-to-Head Comparison\n\nIn this section, we will compare the two LLMs based on factors such as complex reasoning and code generation, check out their capabilities in handling complex tasks, and see which model is best.\n\n* **Graduate Level Reasoning(GPQA, Diamond)**This factor evaluates the models‚Äô ability to handle complex, high-level reasoning tasks at a graduate level of education. In this task, researchers compare the model on the GPQA test, a set of 448 questions in different fields designed by experts. These questions are Google Proof, so anyone can‚Äôt find them online. The Claude score is nearly 59.4%, while the GPT-4o scores only 53.6%. Both the scores are relatively close, but as we can see, Claude could be a better option in tasks that require advanced analytical thinking, such as research analysis, complex problem solving, and high academic level problems.\n* **Undergraduate level knowlege(MMLU)**The MMLU, which means Massive Multitask Language Understanding, is a benchmark that explains the general knowledge understanding of any AI model across various subjects at an undergraduate level. Claude 3.5 Sonnet scores 88.3% in this experiment, and the GPT-4o scores 88.7%. This shows how both LLMs have trained in various domains and have a deeper understanding of them. It makes the AI model a well-suited tool for general knowledge tasks, basic tutoring of multiple subjects, etc.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*A4w-tvsxcmFINaQT)\n\n* **Code(HumanEval)**HumanEval is a benchmark that evaluates the model‚Äôs ability to generate, understand, and debug code. This benchmark is where Claude 3.5 Sonnet achieves 92%, and GPT-4o scores 90.2%. Claude 3.5 Sonnet results are spectacular in this task as it provides a better coding environment, ‚ÄúArtifacts,‚Äù and better code generation than GPT-4o. Claude allows the users to design, edit, and run the code in the Artifacts pop-up window. After the launch of Claude 3.5 Sonnet, everyone is developing tools, websites, and basic games and sharing them across the internet. On the other hand, GPT-4o also scored well, but it does not have any coding environment in its interface, so the developers must do too much hassle as the code generated by it is too much hassle to get to the result.\n* **Reasoning Over Text(DROP, FLscore)**The DROP(Discrete Reasoning Over Paragraphs) is the benchmark that measures the model‚Äôs ability to understand complex text information. In this challenge, the Claude 3.5 Sonnet scores 87.1%, while the GPT-4o scores 83.4%. This shows that the Claude 3.5 Sonnet is better and more effective for the task, which involves detailed text analysis, text review, complex question-answering systems, etc.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Kcy7sFb2FYpbfrwp)\n\n* **Math problem solving(MATH)**This test evaluates the ability of any AI model to solve various mathematical problems. Claude 3.5 Sonnet scores just 71.1%, while the GPT-4o scores 76.6%. These scores make the GPT-4o a better model for mathematical problem-solving tasks and can be used for mathematical computations such as financial modeling, scientific calculations, and advanced data analysis.\n* **Multilingual Maths (MSGM)**This factor describes the ability of any AI model to solve mathematical problems in multiple languages. Both models get scores close to each other: GPT-4o 90.5% and Claude 3.5 Sonnet 91.6%. This shows that both models perform excellently, with Claude slightly better. The capability is particularly helpful for educational applications or any scenario where mathematical reasoning needs to be communicated across language barriers.\n* **Visual question answering(MMU/val)**This factor describes the LLM‚Äôs capability to analyze the information presented in images. The GPT-4o outperforms Claude‚Äôs 3.5 Sonnet in this benchmark with 69.1% and 68.3%, respectively. On the other hand, when analyzing text from the document, Claude‚Äôs 3.5 Sonnet score is 95.2% compared to GPT-4o‚Äôs 92.1%.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*xzjqBV2YL0lVFitX)\n\n* **Image Generation**Image Generation is the ability of the LLMs to generate images from the text. GPT-4o is integrated with DallE-2 and can produce images with the help of text, and the results are excellent. On the other hand, Claude 3.5 Sonnet cannot create any images. This feature also helps GPT-4o design websites and references better, as it is trained on many images.\n* **Knowledge Cutoff**Here, both the models trained on a limited data set till a specific date. Claude 3.5 Sonnet trained on data till April 2024, while the other hand, GPT-4o trained on data till 2024. The real advantage of GPT-4o is that it has real-time web browsing, which helps the LLM train on new data regularly.\n\n\n## Pros of GPT-4o:\n\n* Handles voice, images, and video input.\n* Real-time web browsing capability.\n* Faster response time (0.32 seconds average).\n* Superior in math problem-solving.\n* Can generate images using DALL-E 2.\n\n\n## Cons of GPT-4o:\n\n* Slightly lower performance in graduate-level reasoning.\n* No built-in coding environment.\n* A lower score in document visual Q&A.\n* Slightly behind in code generation capabilities.\n* Less effective in detailed text analysis.\n\n\n## Pros Claude 3.5 Sonnet:\n\n* Excels in graduate-level reasoning.\n* Superior code generation and built-in ‚ÄúArtifacts‚Äù feature.\n* Better performance in detailed text analysis.\n* A higher score in document visual Q&A.\n* Slightly better in multilingual math.\n\n\n## Cons Claude 3.5 Sonnet:\n\n* Cannot handle voice or video input.\n* No image generation capability.\n* Slightly lower performance in visual question-answering.\n* Cannot access real-time web information.\n* Weaker in math problem-solving.\n\n\n## Conclusion\n\nGPT-4o and Claude 3.5 Sonnet demonstrate impressive capabilities across various tasks, each with its strengths. GPT-4o excels in multimodal inputs, real-time information access, and image generation, making it versatile for diverse applications. Claude 3.5 Sonnet shines in complex reasoning, code generation, and detailed text analysis, offering superior performance in specific academic and professional contexts. The choice between these models depends on the specific use case and required features. We can expect further improvements and specialized models catering to different needs as AI technology advances.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/explore-swarm-multi-agent-framework-locally-0e25ee617795","frontmatter":{"title":"Explore Swarm Multi-Agent Framework Locally","meta_title":"Explore Swarm Multi-Agent Framework Locally","description":"Swarm is an experimental sample framework to simulate lightweight multi-agent framework for educational purpose. Usually it works with Open‚Ä¶","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*0ZVceq32bvkytC7HSIgmwA.png","categories":["Programming","Technology","Education"],"author":"Rifx.Online","tags":["Swarm","Multi-Agent","Framework","OpenAI","Ollama"],"draft":false,"slug":"blog/explore-swarm-multi-agent-framework-locally-0e25ee617795"},"content":"\n\n\n\n\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*zkpW8DDwh0TTYuHJVJbDaw.png)\n\nSwarm is an experimental sample framework to simulate lightweight multi-agent framework for educational purpose. Usually it works with Open AI Key but we can change it to use local Ollama or LM Studio Models.\n\n**Setup:**\n\n\n```python\n## Create a new Conda or Python Virtual Environment and activate it\nconda install python==3.10\npip install torch openai\npip install transformers accelerate huggingface_hub\npip install git+ssh://git@github.com/openai/swarm.git\n```\n**To use with Open AI Key:**\n\n\n```python\nexport OPEN_API_KEY = Your Key\n```\n**To use Ollama or LM Studio Local LLMs ‚Äî Update to Local URL:**\n\n\n```python\n## Find the location site-packages/swarm on the conda or python virtual env\n## Locate the file core.py\nclass Swarm:\n    def __init__(self, client=None):\n        if not client:\n          # Actual Code\n          #client = OpenAI()\n          # Update the Base URL and API Key to Ollama / LM Studio\n          # In this demo we are using LM Studio and Llama 3.1\n          client = OpenAI(base_url=\"http://localhost:1234/v1\",api_key=\"random\")\n        self.client = client\n```\n**Clone Repo:**\n\nClone the Repo ‚Äî where you can find examples directory with different use cases like basic, airline and weather etc.\n\n\n```python\ngit clone https://github.com/openai/swarm.git\ncd swarm/examples\n```\n**Sample Code:**\n\n\n```python\nfrom swarm import Swarm, Agent\n\nclient = Swarm()\n\n\nit_agent = Agent(\n    name=\"IT Agent\",\n    instructions=\"You are an IT Expert with 10 Years of Experience.\",\n)\n\nsales_agent = Agent(\n    name=\"Sales Agent\",\n    instructions=\"You are a Sales Expert with 5 Years of Experience and knows about best selling mobiles.\",\n)\n\ndef transfer_to_sales_agent():\n    print(\"Sales agent in action\")\n    \"\"\"Transfer sales related questions to sales team immediately.\"\"\"\n    return sales_agent\n\ndef transfer_to_it_agent():\n    print(\"IT agent in action\")\n    \"\"\"Transfer IT users immediately.\"\"\"\n    return it_agent\n\nenglish_agent = Agent(\n    name=\"English Agent\",\n    instructions=\"You only speak English.\",\n    functions=[transfer_to_sales_agent,transfer_to_it_agent],\n)\n\n\nmessages = [{\"role\": \"user\", \"content\": \"How to install pandas lib?\"}]\nresponse = client.run(agent=english_agent, messages=messages)\n\nprint(response.messages[-1][\"content\"])\n\nmessages = [{\"role\": \"user\", \"content\": \"What are the best selling items?\"}]\nresponse = client.run(agent=english_agent, messages=messages)\n\nprint(response.messages[-1][\"content\"])\n```\n**References:**\n\n\n```python\nhttps://github.com/openai/swarm\n\nhttps://github.com/victorb/ollama-swarm/tree/main\n```\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*hCFJ4VQoT12yElYPXwXvWA.png)\n\nGiven that it is an experimental release, there is still much room for improvement. The airline agent example code [swarm/examples/airline] was interesting, so try those examples. Give it a try and share your experience in the comments. Thanks.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/generating-structured-data-from-an-image-with-gpt-vision-and-langchain-34aaf3dcb215","frontmatter":{"title":"Generating structured data from an image with GPT vision and Langchain","meta_title":"Generating structured data from an image with GPT vision and Langchain","description":"In today‚Äôs world, where visual data is abundant, the ability to extract meaningful information from images is becoming increasingly‚Ä¶","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*FPRRg85jYb7MrzXEpNWbmw.jpeg","categories":["Programming","Computer Vision","Natural Language Processing"],"author":"Rifx.Online","tags":["Langchain","GPT","vision","LLMs","structured"],"draft":false,"slug":"blog/generating-structured-data-from-an-image-with-gpt-vision-and-langchain-34aaf3dcb215"},"content":"\n\n\n\n\n\nIn today‚Äôs world, where visual data is abundant, the ability to extract meaningful information from images is becoming increasingly valuable. Langchain, a powerful framework for building applications with large language models (LLMs), offers a versatile toolset for tackling this challenge. In this article, we‚Äôll explore how to use Langchain to extract structured information from images, such as counting the number of people and listing the main objects.\n\nBefore diving into the code, let‚Äôs set the stage by understanding the task at hand. Imagine you have an image of a scene, such as a city street. Your goal is to extract valuable information from this image, including the number of people present and a list of the main objects in the scene.\n\n\n## About Langchain\n\nLangchain is a comprehensive framework that allows developers to build sophisticated applications by leveraging the power of large language models (LLMs). It provides a modular and extensible architecture, enabling developers to create custom pipelines, agents, and workflows tailored to their specific needs.\n\nLangchain simplifies the integration of LLMs, offering abstractions and utilities for handling various data sources, including text, images, and structured data. It supports a wide range of LLMs from different providers, such as OpenAI and Anthropic, making it easy to switch between models or combine multiple models in a single application.\n\n\n## Preparing the Environment and Setting Up the OpenAI API Key\n\nTo follow along with this tutorial, you‚Äôll need to have Langchain installed. You can install it using pip:\n\n\n```python\npip install langchain langchain_openai\n```\nTo use the OpenAI language models with Langchain, you‚Äôll need to obtain an API key from OpenAI. If you don‚Äôt have an API key yet, you can sign up for one on the OpenAI website (<https://openai.com/api/>).\n\nOnce you have your API key, you can set it as an environment variable in your system or provide it directly in your code. Here‚Äôs an example of how to set the API key as an environment variableCopy code\n\n\n```python\nexport OPENAI_API_KEY=\"your_openai_api_key_here\"\n```\nAlternatively, you can provide the API key directly in your Python code:\n\n\n```python\nimport os\nimport langchain\nos.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key_here\"\n```\nAfter setting up the API key, Langchain will be able to authenticate with the OpenAI API and use their language models.\n\n\n## Loading and Encoding the Image\n\nBefore we can process images with Langchain, we need to load the image data from a file and encode it in a format that can be passed to the language model. The code below defines a function `load_image` that takes a dictionary with an `image_path` key and returns a new dictionary with an `image` key containing the image data encoded as a base64 string.\n\n\n```python\ndef load_image(inputs: dict) -> dict:\n    \"\"\"Load image from file and encode it as base64.\"\"\"\n    image_path = inputs[\"image_path\"]\n  \n    def encode_image(image_path):\n        with open(image_path, \"rb\") as image_file:\n            return base64.b64encode(image_file.read()).decode('utf-8')\n    image_base64 = encode_image(image_path)\n    return {\"image\": image_base64}\n```\nThe `load_image` function first extracts the `image_path` from the input dictionary. It then defines a nested function `encode_image` that opens the image file in binary mode, reads its contents, and encodes them as a base64 string using the `base64.b64encode` function from the Python standard library.\n\nThe `load_image` function calls `encode_image` with the provided `image_path` and stores the resulting base64-encoded string in the `image_base64` variable. Finally, it returns a new dictionary with the `image` key set to `image_base64`.\n\nTo integrate this function into a Langchain pipeline, we can create a `TransformChain` that takes the `image_path` as input and produces the `image` (base64-encoded string) as outputCopy code\n\n\n```python\nload_image_chain = TransformChain(\n    input_variables=[\"image_path\"],\n    output_variables=[\"image\"],\n    transform=load_image\n)\n```\nWith this setup, we can easily load and encode images as part of a larger Langchain workflow, enabling us to process visual data alongside text using large language models.\n\n\n## Defining the Output Structure\n\nBefore we can extract information from the image, we need to define the structure of the output we want to receive. In this case, we‚Äôll create a Pydantic model called `ImageInformation` that includes fields for the image description and any additional information we might want to extract.\n\n\n```python\nfrom langchain_core.pydantic_v1 import BaseModel, Field\n\nclass ImageInformation(BaseModel):\n \"\"\"Information about an image.\"\"\"\n image_description: str = Field(description=\"a short description of the image\")\n people_count: int = Field(description=\"number of humans on the picture\")\n main_objects: list[str] = Field(description=\"list of the main objects on the picture\")\n```\n\n## Setting up the Image Model\n\nNext, we‚Äôll create a chain that combines the image loading and encoding steps with the LLM invocation step. Since the `ChatOpenAI` model is not natively capable of handling both text and image inputs simultaneously (to my unsderstanding), we'll create a wrapper chain to achieve this functionality.\n\n\n```python\nfrom langchain.chains import TransformChain\nfrom langchain_core.messages import HumanMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain import globals\nfrom langchain_core.runnables import chain\n\n## Set verbose\nglobals.set_debug(True)\n\n@chain\ndef image_model(inputs: dict) -> str | list[str] | dict:\n \"\"\"Invoke model with image and prompt.\"\"\"\n model = ChatOpenAI(temperature=0.5, model=\"gpt-4-vision-preview\", max_tokens=1024)\n msg = model.invoke(\n             [HumanMessage(\n             content=[\n             {\"type\": \"text\", \"text\": inputs[\"prompt\"]},\n             {\"type\": \"text\", \"text\": parser.get_format_instructions()},\n             {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{inputs['image']}\"}},\n             ])]\n             )\n return msg.content\n```\nIn this code snippet, we define a chain called `image_model` that invokes the `ChatOpenAI` model with the provided prompt, format instructions, and image. The `image_model` chain accepts a dictionary `inputs` containing the prompt and the base64-encoded image string.\n\nInside the chain, we create a `HumanMessage` object that combines the prompt text, format instructions, and the image URL, formatted as a data URI with the base64-encoded image data. We then invoke the `ChatOpenAI` model with this `HumanMessage` object, using the `gpt-4-vision-preview` model, which is specifically designed for multimodal tasks involving both text and images.\n\nThe model processes both the text prompt and the image, and returns the output.\n\n\n## Putting It All Together\n\nNow that we have all the necessary components, we can define a function that orchestrates the entire process:\n\n\n```python\nfrom langchain_core.output_parsers import JsonOutputParser\n\nparser = JsonOutputParser(pydantic_object=ImageInformation)\ndef get_image_informations(image_path: str) -> dict:\n   vision_prompt = \"\"\"\n   Given the image, provide the following information:\n   - A count of how many people are in the image\n   - A list of the main objects present in the image\n   - A description of the image\n   \"\"\"\n   vision_chain = load_image_chain | image_model | parser\n   return vision_chain.invoke({'image_path': f'{image_path}', \n                               'prompt': vision_prompt})\n```\nIn this function, we define a prompt that asks the LLM to provide a count of the people in the image and a list of the main objects. We then create a chain that combines the image loading step (`load\\_image\\_chain`), the LLM invocation step (`image\\_model`), and a JSON output parser (`parser`). Finally, we invoke this chain with the image path and the prompt, and the function returns a dictionary containing the extracted information.\n\n\n## Example Usage\n\nTo use this function, simply provide the path to an image file:\n\n\n```python\nresult = get_image_informations(\"path/to/your/image.jpg\")\nprint(result)\n```\nThis will output a dictionary with the requested information, such as:\n\n\n```python\n{\n 'description': 'a view of a city showing cars waiting at a traffic light',\n 'people_count': 5,\n 'main_objects': ['car', 'building', 'traffic light', 'tree']\n}\n```\n\n## Conclusion\n\nLangchain provides a powerful toolset for working with large language models and extracting valuable information from various data sources, including images. By combining Langchain‚Äôs capabilities with custom prompts and output parsing, you can create robust applications that can extract structured information from visual data.\n\nRemember, the quality of the output will depend on the capabilities of the LLM you‚Äôre using and the specificity of your prompts. Experiment with different models and prompts to find the best solution for your use case.\n\nIf you find a better way to achieve the same results or have suggestions for improvements, please don‚Äôt hesitate to share them in the comments. The code examples provided in this article are meant to serve as a starting point, and there may be alternative approaches or optimizations .\n\n\n"},{"lang":"en","group":"blog","slug":"blog/how-to-run-nvidia-llama-3-1-nemotron-70b-instruct-locally-a58ad283aaff","frontmatter":{"title":"How to Run Nvidia‚Äô llama-3.1-nemotron-70b-instruct Locally","meta_title":"How to Run Nvidia‚Äô llama-3.1-nemotron-70b-instruct Locally","description":"Running large language models (LLMs) locally has become increasingly popular among developers, researchers, and AI enthusiasts. One such‚Ä¶","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*fqVKJkw5sQvLtIsyCcengQ.png","categories":["Programming","Technology","Science"],"author":"Rifx.Online","tags":["Nvidia","llama","Ollama","llama.cpp","Transformers"],"draft":false,"slug":"blog/how-to-run-nvidia-llama-3-1-nemotron-70b-instruct-locally-a58ad283aaff"},"content":"\n\n\n\nRunning large language models (LLMs) locally has become increasingly popular among developers, researchers, and AI enthusiasts. One such model that has gained significant attention is the llama-3.1-nemotron-70b-instruct, a powerful LLM customized by NVIDIA to enhance the helpfulness of generated responses. In this comprehensive guide, we‚Äôll explore multiple methods to run this model on your local machine, starting with the user-friendly Ollama platform.\n\n\n> Before we get started, If you are seeking an All-in-One AI platform that manages all your AI subscriptions in one place, including all LLMs (such as GPT-o1, Llama 3.1, Claude 3.5 Sonnet, Google Gemini, Uncensored LLMs) and Image Generation Models (FLUX, Stable Diffusion, etc.), Use Anakin AI to manage them all!\n\n\n\n\n## Method 1: Run llama-3.1-nemotron-70b-instruct Locally with Ollama\n\nOllama is an excellent tool for running LLMs locally, offering a straightforward setup process and support for various models, including llama-3.1-nemotron-70b-instruct.\n\n\n### Installation\n\n1. Visit the official Ollama website ([https://ollama.ai](https://ollama.ai/)) and download the appropriate version for your operating system.\n2. Install Ollama by running the following command in your terminal:\n\n\n```python\ncurl https://ollama.ai/install.sh | sh\n```\n\n### Running llama-3.1-nemotron\n\nOnce Ollama is installed, you can easily run the llama-3.1-nemotron-70b-instruct model with a simple command:\n\n\n```python\nollama run nemotron:70b-instruct-q5_K_M\n```\nThis command will download the model if it‚Äôs not already present on your system and start an interactive session.\n\n\n### Using the Model\n\nAfter the model is loaded, you can start interacting with it by typing your prompts. For example:\n\n\n```python\n>>> What are the key features of llama-3.1-nemotron-70b-instruct?\n\nLlama-3.1-Nemotron-70B-Instruct is a large language model with several key features:\n1. Customized by NVIDIA: The model has been fine-tuned by NVIDIA to improve the helpfulness and quality of its responses.\n2. Based on Llama 3.1: It builds upon the Llama 3.1 architecture, which is known for its strong performance across various tasks.\n3. 70 billion parameters: This large parameter count allows for complex reasoning and a wide range of capabilities.\n4. Instruct-tuned: The model is specifically designed to follow instructions and generate helpful responses to user queries.\n5. RLHF training: It has been trained using Reinforcement Learning from Human Feedback, specifically the REINFORCE algorithm.\n6. Specialized reward model: The training process utilized Llama-3.1-Nemotron-70B-Reward for optimization.\n7. HelpSteer2-Preference prompts: These were used during the training process to further improve the model's helpfulness.\n8. Extended context length: Like other Llama 3.1 models, it likely supports a longer context window of 128K tokens.\n9. Multilingual capabilities: It can understand and generate text in multiple languages.\n10. Strong reasoning abilities: The model excels in tasks requiring complex reasoning and problem-solving.\nThese features make llama-3.1-nemotron-70b-instruct a powerful and versatile language model suitable for a wide range of applications, from general conversation to specialized tasks in various domains.\n```\nFor more advanced use cases, you can integrate Ollama with Python using libraries like Langchain. Here‚Äôs a simple example:\n\n\n```python\npython\n\nfrom langchain.llms import Ollama\n\nollama = Ollama(base_url=\"http://localhost:11434\", model=\"nemotron:70b-instruct-q5_K_M\")\nresponse = ollama.generate(\"Explain the concept of quantum entanglement.\")\nprint(response)\n```\nThis allows you to incorporate the model into your Python projects and applications seamlessly.\n\n\n## Method 2: Using llama.cpp\n\nllama.cpp is a popular C++ implementation of the Llama model inference, optimized for CPU usage. While it may require more setup than Ollama, it offers greater flexibility and control over the model‚Äôs parameters.\n\n\n### Installation\n\n1. Clone the llama.cpp repository:\n\n\n```python\ngit clone https://github.com/ggerganov/llama.cpp.git\ncd llama.cpp\n```\n1. Build the project:\n\n\n```python\nmake\n```\n\n### Downloading the Model\n\nTo run llama-3.1-nemotron-70b-instruct, you‚Äôll need to download the model weights. These are typically available in GGML or GGUF format. You can find pre-converted models on platforms like Hugging Face.\n\n\n```python\nmkdir models\ncd models\nwget https://huggingface.co/TheBloke/Llama-3.1-Nemotron-70B-Instruct-GGUF/resolve/main/llama-3.1-nemotron-70b-instruct.Q4_K_M.gguf\n```\n\n### Running the Model\n\nOnce you have the model file, you can run it using the following command:\n\n\n```python\n./main -m models/llama-3.1-nemotron-70b-instruct.Q4_K_M.gguf -n 1024 -p \"Hello, how are you today?\"\n```\nThis command loads the model and generates a response to the given prompt. You can adjust various parameters like the number of tokens to generate (-n) or the temperature to control randomness.\n\n\n## Method 3: Using Hugging Face Transformers\n\nHugging Face‚Äôs Transformers library provides a high-level API for working with various language models, including llama-3.1-nemotron-70b-instruct.\n\n**Installation**\n\nFirst, install the necessary libraries:\n\n\n```python\npip install transformers torch accelerate\n```\n**Running the Model**\n\nHere‚Äôs a Python script to load and use the model:\n\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nmodel_name = \"meta-llama/Llama-3.1-Nemotron-70b-instruct\"\n## Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n## Prepare the input\nprompt = \"Explain the concept of quantum computing in simple terms.\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n## Generate the response\nwith torch.no_grad():\n    outputs = model.generate(**inputs, max_new_tokens=100)\n## Decode and print the response\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response)\n```\nThis method allows for more fine-grained control over the model‚Äôs behavior and integration with other Hugging Face tools and pipelines.\n\n\n## Conclusion\n\nRunning llama-3.1-nemotron-70b-instruct locally opens up a world of possibilities for developers and researchers. Whether you choose the simplicity of Ollama, the flexibility of llama.cpp, or the integration capabilities of Hugging Face Transformers, you now have the tools to harness the power of this advanced language model on your own hardware.As you explore the capabilities of llama-3.1-nemotron-70b-instruct, remember to balance performance with resource constraints, and always consider the ethical implications of your applications. With responsible use, this model can be a valuable asset in pushing the boundaries of what‚Äôs possible in natural language processing and AI-driven applications.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/key-points-llm-quantization-chatgpt-artificial-intelligence-8201ffcb33d4","frontmatter":{"title":"5 Key Points to Unlock LLM Quantization","meta_title":"5 Key Points to Unlock LLM Quantization","description":"Quantizing Large Language Models","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*RUqPEr2NTYXlI1omqF22Qg.png","categories":["Machine Learning","Data Science","Technology/Web"],"author":"Rifx.Online","tags":["quantization","weights","activations","calibration","Quanto"],"draft":false,"slug":"blog/key-points-llm-quantization-chatgpt-artificial-intelligence-8201ffcb33d4"},"content":"\n\n\n\n\n### Quantizing Large Language Models\n\n\n\nLLM Quantization is currently a hot topic due to its vital role in making Large Language Models (LLMs) more efficient and deployable across various hardware platforms, including consumer-grade devices.\n\nBy adjusting the precision of certain components within the model, **quantization significantly reduces the model‚Äôs memory footprint** while maintaining similar performance levels.\n\nIn this guide, we will explore five key aspects of LLM quantization including some practical steps for applying this technique to our models.\n\n\n## #1. Understanding Quantization\n\nQuantization is a model compression technique that reduces the precision of weights and activations in an LLM. This involves converting high-precision values to lower-precision ones, effectively **changing data types that store more information to those that store less**.\n\nDecreasing the number of bits needed for each weight or activation significantly reduces the overall model size. As a result, **quantization creates LLMs that use less memory, and require less storage space.**\n\nThis technique has become essential in response to the exponential growth in the number of parameters in successive iterations of LLMs. For example, for the OpenAI‚Äôs GPT family, we can observe the growing trend in the following graph:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*QlAhma3Wu1F6w2WvkE8jDA.png)\n\nThis significant increase presents a challenge: as models grow, their memory requirements often exceed the capacity of advanced hardware accelerators such as GPUs. **This requires distributed training and inference to manage these models, which in turn limits their deployability.**\n\n\n## #2. Intuition Behind Quantization\n\nAlthough the definition of quantization may seem rather complex, the concept can be intuitively explained using matrices.\n\nLet‚Äôs consider the following a 3x3 matrix representing the weights of a neural network. The matrix on the left shows the original weights, while the matrix on the right shows the quantized version of these weights:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*LPzWe9oxjlDYdSp7dVvRUg.png)\n\nIn this simple example, we round the elements of the original matrix from four decimal places to a single decimal place. Although the matrices appear similar, **the storage space required for the four-decimal version is significantly higher**.\n\nIn practice, quantization is not merely a rounding operation. Instead, it involves converting numerical values to a different data type, typically from a higher to a lower precision one.\n\nFor example, the default data type for most models is `float32`, which requires 4 bytes per parameter (32 bits). Therefore, for a 3x3 matrix, the total memory footprint is 36 bytes. Changing the data type to `int8`, only 1 byte per parameter is needed, reducing the total memory footprint of the matrix to just 9 bytes.\n\n\n## #3. Quantization Error\n\nAs we have seen, the original matrix and its quantized form are not completely equal, but very similar. The value-by-value difference is known as ‚ÄúQuantization error‚Äù, which we can also represent in matrix form:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*VtGDjVbr7daagLXB57i7Mg.png)\n\n**This quantization error can accumulate for each matrix of weights in the network, affecting the model‚Äôs performance as a result.**\n\nCurrent research in quantization aims to minimize the difference in precision while decreasing the computational resources required to train or run inference on models, while maintaining acceptable performance levels.\n\n\n## #4. Linear Quantization\n\nLinear quantization is one of the most popular quantization schemes for LLMs. In simple terms, it involves mapping the range of floating-point values of the original weights to a range of fixed-point values.\n\nLet‚Äôs review the steps required to apply linear quantization to our models:\n\n* **Get the minimum and maximum ranges:** We need to get the minimum and maximum values of the floating-point weights to be quantized (`x_min` and `x_max`). We also need to define the quantized range (`q_min` and `q_max`), which is already set by the data type we want to convert to.\n* **Compute the scale (`s`) and the zero-point (`z`) values:** Firstly, the scale (`s`) adjusts the range of floating-point values to fit within the integer range, preserving the data distribution and range. Secondly, the zero-point (`z`) ensures that zero in the floating-point range is accurately represented by an integer, maintaining numerical accuracy and stability, especially for values close to zero.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BepC6-izw0yE19ejsS705Q.png)\n\n* **Quantize the values (`q`)**: We need to map the original floating-point values to the integer range using a scale factor (`s`) and a zero point (`z`) computed in the previous step.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BBOQ0VbSGbwf7CN8c4PWKQ.png)\n\nApplying these formulas is quite straightforward. If we apply them to the 3x3 weight tensor on the left in the image below, we will get the quantized matrix shown on the right:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*KzBvg84mfI2gAhTIyVibwQ.png)\n\nWe can see that the lower bound of the `int8` value corresponds to the lower value of the original tensor, while the upper bound corresponds to the higher value of the original tensor, *i.e., the mapping is`0.50 ‚Üí 255` and `-0.40 ‚Üí 0`.*\n\nWe can now dequantize the values using the formula below.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*E5nnqYzncYCRuM5prssuOw.png)\n\nIf we place the dequantized values again in matrix form (matrix on the left), we can compute the quantization error (matrix on the right) by calculating the point-by-point difference between the original matrix and its dequantized version:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*56NALu9PAN95QG2hn8HXoQ.png)\n\nAs we can observe, the quantization error starts kicking in for some of the matrix values.\n\n\n## #5. Weight Quantization vs Activation Quantization\n\nIn our example above, we have focused primarily on quantizing the weights of the model. While weight quantization is crucial for model optimization, it‚Äôs also important to consider that activations can be quantized as well.\n\n**Activation quantization involves reducing the precision of the intermediate outputs of each layer in the network**. Unlike weights, which remain constant once the model is trained, activations are dynamic and change with each input, making their range harder to predict.\n\nGenerally, activation quantization is more challenging to implement than weight quantization because it requires careful calibration to ensure the dynamic range of activations is accurately captured.\n\nWeight quantization and activation quantization are complementary techniques. Using both can significantly reduce model size without greatly compromising performance.\n\n\n## Final Thoughts\n\nIn this article, we have reviewed 5 key points about quantization to better understand how to reduce the size of these constantly growing models.\n\nAs for the implementation of those techniques, there are several tools and libraries in Python that support quantization such as `pytorch` and `tensorflow`. Nevertheless, integrating quantization seamlessly in existing models requires a deep understanding of the libraries and model internals.\n\nThat is why my favorite option to implement quantization in easy steps so far is the [Quanto](https://huggingface.co/blog/quanto-introduction) library by Hugging Face, designed to simplify the quantization process for PyTorch models.\n\nIf you are interested in the in-depths of LLM Quantization and how to use the aforementioned library, you might also be interested in the article [‚ÄúQuantization for Large Language Models (LLMs): Reduce AI Model Sizes Efficiently‚Äù](https://www.datacamp.com/tutorial/quantization-for-large-language-models).\n\nThat is all! Many thanks for reading!\n\nI hope this article helps you when **using LLMs for coding!**\n\nYou can also subscribe to my [**Newsletter**](https://readmedium.com/@andvalenzuela/subscribe) to stay tuned for new content.\n\n**Especially**, **if you are interested in articles about Large Language Models and ChatGPT**:\n\n\n"},{"lang":"en","group":"blog","slug":"blog/langgraph-vs-langchain-vs-langflow-vs-langsmith-which-one-to-use-why-69ee91e91000","frontmatter":{"title":"LangGraph vs. LangChain vs. LangFlow vs. LangSmith: Which One to Use & Why?","meta_title":"LangGraph vs. LangChain vs. LangFlow vs. LangSmith: Which One to Use & Why?","description":"Discover the key differences between LangGraph, LangChain, LangFlow, and LangSmith, and learn which framework is best suited for your‚Ä¶","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*xrWv1QVt4zE5cxjA8VA3ag.png","categories":["Programming","Technology","Technology/Web"],"author":"Rifx.Online","tags":["LangGraph","LangChain","LangFlow","LangSmith","frameworks"],"draft":false,"slug":"blog/langgraph-vs-langchain-vs-langflow-vs-langsmith-which-one-to-use-why-69ee91e91000"},"content":"\n\n\n\n\n### Discover the key differences between LangGraph, LangChain, LangFlow, and LangSmith, and learn which framework is best suited for your language model applications ‚Äî from workflow building to performance monitoring.\n\nüë®üèæ‚Äçüíª [GitHub](https://github.com/mdmonsurali) ‚≠êÔ∏è | üëî[LinkedIn](https://www.linkedin.com/in/mdmonsurali/) |üìù [Medium](https://medium.com/@monsuralirana)\n\n\n\nIn recent years, the world of natural language processing (NLP) has witnessed an explosion in the number of frameworks, libraries, and tools available for building language model-based applications. Among these, **LangGraph**, **LangChain**, **LangFlow**, and **LangSmith** have emerged as leading options, each catering to different use cases and user needs. If you‚Äôre looking to build, monitor, or scale language model workflows, it‚Äôs crucial to understand the strengths and purposes of these tools.\n\nIn this blog, we‚Äôll explore each framework, break down their strengths, and provide insights into when to use them. Whether you‚Äôre a seasoned developer or a newcomer to the field, understanding the nuances of these tools will help you choose the right one for your project.\n\n\n## Introduction to Language Model Frameworks\n\nWith the rise of powerful language models such as GPT-3, GPT-4, and other transformer-based models, there is a growing need for frameworks that streamline the creation and management of language-based applications. These frameworks simplify complex tasks like **chaining multiple prompts**, **retrieving relevant documents**, and even **monitoring model performance**.\n\nHowever, not all frameworks are the same. While some provide a **visual interface** to manage workflows, others offer advanced **debugging and observability** features. Let‚Äôs dive into each of these tools to understand their unique offerings.\n\n\n## 1. LangGraph: Visualizing Complex Workflows\n\n**LangGraph** is a newer framework designed for developers who prefer a **visual approach** to building language model pipelines. It allows you to structure complex workflows with **graph-based visualizations**, making it easier to understand dependencies between different tasks and components. This can be especially useful for larger applications where multiple steps, such as text generation, document retrieval, and classification, are chained together.\n\n\n### Strengths:\n\n* **Visual Workflow Representation**: LangGraph lets you visualize the flow of data and actions between different components. This graphical approach is intuitive and helps in designing more complex pipelines.\n* **Ease of Debugging**: The visual nature of LangGraph makes it easier to identify bottlenecks or problematic nodes in a workflow.\n\n\n### Example Use Case:\n\nSuppose you‚Äôre building an automated system that first retrieves relevant documents using a language model and then passes them through a summarizer. In LangGraph, you can visually map out this workflow, showing the relationships between each step. If there‚Äôs an issue at any point in the chain, the visual tool makes it easy to pinpoint where things went wrong.\n\n\n### When to Use LangGraph:\n\nIf you‚Äôre managing **complex workflows** with multiple steps and value a **graphical interface** for understanding your pipeline, LangGraph is a fantastic choice. It‚Äôs particularly helpful for developers or data scientists who prefer a more intuitive, drag-and-drop approach to workflow design.\n\n**Key points**:\n\n* If you need a clear visual representation of language processing workflows.\n* When creating more complex pipelines that require branching or multi-path dependencies.\n\n\n## 2. LangChain: The Workhorse for LLM Applications\n\n**LangChain** is one of the most popular frameworks for building applications powered by **large language models (LLMs)**. It provides a versatile, **code-first approach**, allowing developers to chain tasks such as document retrieval, summarization, and question-answering into cohesive workflows.\n\n\n### Strengths:\n\n* **Extensive Support for LLMs**: LangChain is compatible with various language models, making it easy to integrate models like OpenAI‚Äôs GPT or even locally hosted models.\n* **Chaining Capabilities**: LangChain excels at **chaining multiple operations** ‚Äî hence the name ‚Äî enabling developers to create sophisticated NLP applications.\n* **Wide Adoption**: As one of the most popular frameworks, LangChain has a **thriving community** and excellent support, with ample documentation and tutorials.\n\n\n### Example Use Case:\n\nImagine you‚Äôre building a **chatbot** that first understands the user‚Äôs question, retrieves relevant information from a database, and then generates a response. With LangChain, you can easily create this multi-step process programmatically, ensuring each step in the chain works harmoniously.\n\n\n### When to Use LangChain:\n\nIf you‚Äôre a **developer building production-level applications** and need a **flexible, code-centric solution**, LangChain is your best bet. It‚Äôs ideal for those who prefer control over their application‚Äôs architecture and are comfortable writing code to define workflows.\n\n**Key points**:\n\n* If you‚Äôre building production-grade applications that require chaining of tasks across multiple language models.\n* If you need a library with extensive community support and wide-ranging integrations.\n* When you‚Äôre more comfortable with programmatic solutions rather than visual tools.\n\n\n## 3. LangFlow: No-Code/Low-Code Extension of LangChain\n\n**LangFlow** is essentially a **visual extension of LangChain**. It combines the powerful backend of LangChain with an **intuitive drag-and-drop interface**. LangFlow allows users who might not be as comfortable writing code to still leverage the power of language models in their applications.\n\n\n### Strengths:\n\n* **Visual Workflow Creation**: Like LangGraph, LangFlow provides a visual interface for building workflows. However, it‚Äôs specifically built on top of LangChain, meaning users can harness LangChain‚Äôs power without needing to write extensive code.\n* **Ideal for Rapid Prototyping**: LangFlow is perfect for quickly **prototyping ideas** or building out proof-of-concept applications.\n* **Beginner-Friendly**: It‚Äôs a great entry point for users who are less familiar with coding but want to create language model workflows.\n\n\n### Example Use Case:\n\nIf you want to quickly build a **summarization tool** that retrieves documents, you can drag and drop the components in LangFlow‚Äôs interface to create a fully functioning application. This can be done without writing much code, if any.\n\n\n### When to Use LangFlow:\n\nLangFlow is perfect for **non-developers** or **rapid prototyping**. If you want to experiment with **LLM workflows quickly** without delving into the code, this tool makes it easy to get started.\n\n**Key points**:\n\n* If you want to prototype LLM workflows quickly without writing code.\n* If you‚Äôre comfortable with visual programming but need the flexibility of LangChain.\n* For educational purposes, to help users learn how workflows can be constructed.\n\n\n## 4. LangSmith: Monitoring and Observability\n\nWhile the other tools focus on **building workflows**, **LangSmith** is designed for **monitoring** and **debugging** language model applications. It provides advanced observability features to track the performance of your workflows and models, making it invaluable for production environments.\n\n\n### Strengths:\n\n* **Deep Observability**: LangSmith allows developers to monitor language model performance, ensuring that workflows behave as expected.\n* **Error Tracking**: It excels at helping developers track down issues, making debugging easier.\n* **Performance Insights**: LangSmith gives insights into **workflow performance**, helping developers optimize their applications.\n\n\n### Example Use Case:\n\nLet‚Äôs say you‚Äôve deployed a **customer service chatbot** that uses a language model to answer questions. Over time, you notice that some responses are less accurate than expected. LangSmith can help you trace the problem by providing visibility into each decision point within the workflow.\n\n\n### When to Use LangSmith:\n\nIf you‚Äôre deploying applications in **production environments** and need to ensure **robustness, reliability, and performance**, LangSmith is an essential tool. It‚Äôs particularly useful when managing **complex systems that require debugging and optimization** over time.\n\n**Key points**:\n\n* If you need advanced monitoring or debugging capabilities in LLM workflows.\n* For development environments where observability is key to ensuring optimal model performance.\n* If your focus is on improving and iterating LLM-powered applications based on real-time insights.\n\n\n## Which One to Choose?\n\n* **Use LangGraph** if you prefer graph-based, visual workflows for building complex LLM tasks. Ideal for users who need clarity and structure.\n* **Use LangChain** if you need a robust, flexible solution for creating language model applications programmatically. It‚Äôs versatile and great for developers building production-level applications.\n* **Use LangFlow** if you want the power of LangChain with a visual, no-code/low-code interface. Best for rapid prototyping and users who prefer visual tools over coding.\n* **Use LangSmith** if your focus is on observability and debugging of LLM applications. Ideal when you need to monitor and optimize workflows in a development or production environment.\n\nUltimately, your choice depends on your comfort with code, the complexity of your workflows, and whether you prioritize ease of use, flexibility, or observability.\n\n\n## Conclusion\n\nEach of these tools ‚Äî **LangGraph**, **LangChain**, **LangFlow**, and **LangSmith** ‚Äî caters to different stages of developing and managing language model applications. **LangGraph** provides a visual, intuitive way to build complex workflows, while **LangChain** offers a robust, code-first solution for developers looking to create scalable applications. For those who prefer a **low-code**, drag-and-drop approach, **LangFlow** simplifies the process without sacrificing power. Finally, **LangSmith** focuses on observability and debugging, ensuring that your workflows are optimized and reliable. Choosing the right tool depends on your project needs, whether it‚Äôs for rapid prototyping, production-level scaling, or monitoring and performance tracking.\n\nHappy coding! üéâ\n\nüë®üèæ‚Äçüíª [GitHub](https://github.com/mdmonsurali) ‚≠êÔ∏è | üëî[LinkedIn](https://www.linkedin.com/in/mdmonsurali/) |üìù [Medium](https://medium.com/@monsuralirana)\n\nThank you for your time in reading this post!\n\nMake sure to leave your feedback and comments. See you in the next blog, stay tuned üì¢\n\n\n## References:\n\n1. ‚ÄúLangChain Documentation‚Äù ‚Äî <https://python.langchain.com/docs/introduction/>\n2. ‚ÄúLangGraph Overview‚Äù ‚Äî <https://langchain-ai.github.io/langgraph/>\n3. ‚ÄúLangFlow GitHub Repository‚Äù ‚Äî [https://github.com/LangFlow/LangFlow](https://docs.langflow.org/)\n4. ‚ÄúLangSmith Introduction‚Äù ‚Äî <https://www.langchain.com/langsmith>\n5. ‚ÄúHow to Build Chatbots With LangChain‚Äù by JetBrains blog ‚Äî <https://blog.jetbrains.com/pycharm/2024/08/how-to-build-chatbots-with-langchain/>\n\n"},{"lang":"en","group":"blog","slug":"blog/o1-preview-vs-claude-3-5-sonnet-comparing-top-llms-d68734b53c93","frontmatter":{"title":"o1-preview vs. claude-3.5-sonnet: Comparing top LLMs","meta_title":"o1-preview vs. claude-3.5-sonnet: Comparing top LLMs","description":"Discover how OpenAI‚Äôs o1-preview compares to Claude 3.5 Sonnet in performance, speed, and capabilities.","date":"2024-10-27T13:58:01.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kTWAcpRdOpsrFIDZjjjr7Q.jpeg","categories":["Programming","Machine Learning","Generative AI"],"author":"Rifx.Online","tags":["o1-preview","Claude","throughput","latency","reasoning"],"draft":false,"slug":"blog/o1-preview-vs-claude-3-5-sonnet-comparing-top-llms-d68734b53c93"},"content":"\n\n\n\nToday (Sep 12, 2024), OpenAI unveiled its latest language model, o1-preview. This advanced model is engineered to dedicate more time to processing before generating responses, enabling it to tackle complex tasks and solve challenging problems in science, coding, and mathematics with enhanced capabilities.\n\nIn this blog post, we‚Äôll thoroughly analyze o1-preview and compare it to Claude 3.5 Sonnet, which was previously considered one of the most advanced models available.\n\n\n\n\n## Comparison Methodology\n\nOur analysis utilizes [Keywords AI‚Äôs LLM playground](https://docs.keywordsai.co/features/prompt/model-playground), a platform that supports over 200 language models and offers function-calling capabilities. We‚Äôll explore the following aspects:\n\n* Basic comparison\n* Benchmark comparison\n* Processing speed\n* Evaluation metrics\n* Suggested use cases\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*yc171ikejtBy_o11.jpeg)\n\n\n## Basic Comparison\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*z2FrS_AVig7Y6eU_.jpeg)\n\nNote: o1-preview doesn‚Äôt support Streaming, function calling, and system messages.\n\n\n## Benchmark Comparison\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Bx_vAvFc9DAD0cZA.jpeg)\n\nO1-preview outperforms Claude 3.5 Sonnet across all benchmarks. The smallest gap is in MMLU (general knowledge). GPQA Diamond, testing graduate-level reasoning, shows a significant performance difference. The MATH benchmark reveals the largest gap, highlighting o1-preview‚Äôs advanced mathematical capabilities. These results indicate o1-preview‚Äôs substantial improvements in complex reasoning and problem-solving across various domains.\n\n\n## Speed Comparison\n\nO1-preview takes longer to think and respond than other LLMs. While direct speed comparisons may not be entirely fair, testing o1-preview‚Äôs speed is crucial. This information helps developers better understand o1-preview‚Äôs capabilities and determine if it‚Äôs suitable for their projects. Note: As o1-preview doesn‚Äôt support streaming, we disabled streaming for both models. Consequently, time to first token (TTFT) couldn‚Äôt be measured.\n\n\n## Latency\n\nOur tests, involving hundreds of requests per model, revealed significant differences. Claude 3.5 Sonnet averages 18.3s/request, whereas o1-preview takes 39.4s/request. O1-preview‚Äôs significantly longer latency is due to its extended thinking and reasoning process.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*2PMkgPVuylFxwfIa.jpeg)\n\n\n## Throughput (Tokens per second)\n\nDespite higher latency, o1-preview shows superior throughput. O1-preview generates 92.94 tokens/second, while Claude 3.5 Sonnet produces 74.87 tokens/second. This indicates that o1-preview‚Äôs longer generation time is primarily due to its initial processing phase rather than token generation speed.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*wxqpnwZhl9pnbw8y.jpeg)\n\n\n## Performance comparison\n\nWe conducted evaluation tests on the [Keywords AI platform](https://keywordsai.co/). The evaluation comprised three parts:\n\n* **Coding Task**: Both models successfully completed frontend and backend development tasks. O1-preview demonstrated superior performance with longer contexts, identifying and resolving bugs more efficiently in the first attempt. It also exhibited a more thorough code analysis capability.\n* **Logical Reasoning**: O1-preview excels in reasoning tasks. Its thinking process closely mimics human cognition. While Claude 3.5 Sonnet performs well on most problems, o1-preview consistently solves complex reasoning challenges, including International Mathematical Olympiad (IMO) level problems.\n* **Writing Task:** Both models perform exceptionally well on writing tasks. They demonstrate the ability to craft genuine, personalized cold emails, as well as concise and meaningful blog posts.\n\n\n## Model Recommendations\n\no1-preview\n\n* **Best for:** Complex problem-solving in mathematics, coding, and physics. Particularly suited for researchers tackling challenging tasks.\n* **Not suitable for:** AI applications requiring rapid response times or heavily reliant on system prompts. Voice AI applications due to lack of streaming support.\n\nClaude 3.5 Sonnet\n\n* **Best for:** Most AI applications requiring problem-solving capabilities and high-quality content generation.\n* **Not suitable for:** Voice AI applications or projects with strict budget constraints requiring lower operational costs.\n\n\n## How to integrate o1-preview into your AI apps.\n\nTo incorporate o1-preview into your AI applications, simply visit the Keywords AI model page and locate the ‚ÄúView code‚Äù button. Click this button to copy the provided code snippet, then paste it directly into your codebase. With this straightforward process, you‚Äôll be ready to harness the power of o1-preview in your projects, enabling you to tackle complex problems and generate high-quality content with ease.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*XyQ9QiI7TN8Uc5Jp.jpeg)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*t8fEYlEs13eM7D28lVbtIw.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*yhu9y5ixNuxeFVe1.png)\n\nThis story is published on [Generative AI](https://generativeai.pub/). Connect with us on [LinkedIn](https://www.linkedin.com/company/generative-ai-publication) and follow [Zeniteq](https://www.zeniteq.com/) to stay in the loop with the latest AI stories.\n\nSubscribe to our [newsletter](https://www.generativeaipub.com/) and [YouTube](https://www.youtube.com/@generativeaipub) channel to stay updated with the latest news and updates on generative AI. Let‚Äôs shape the future of AI together!\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*PelNtaNaEVDWgMWr.png)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/qwen2-5-1-5b-the-future-of-mobile-ai-6bd5f29bbc84","frontmatter":{"title":"Qwen2.5 1.5b: the future of Mobile AI?","meta_title":"Qwen2.5 1.5b: the future of Mobile AI?","description":"Local Testing and Evaluation of Alibaba Cloud‚Äôs Latest LLM. With llama-cpp-python and a DIY prompt catalog.","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*awb56jkdXobA-Ip6d-QHRA.png","categories":["Programming","Technology","Science"],"author":"Rifx.Online","tags":["Qwen2.5","summarization","retrieval","mobile","improvements"],"draft":false,"slug":"blog/qwen2-5-1-5b-the-future-of-mobile-ai-6bd5f29bbc84"},"content":"\n### Local Testing and Evaluation of Alibaba Cloud‚Äôs Latest LLM. With llama-cpp-python and a DIY prompt catalog.\n\n\n\nIn part one we explored together the innovations from Alibaba Cloud‚Äôs team with the release of the Qwen2.5 models family.\n\nIn Generative AI benchmarks are now the main *oracle*: the validity of a new LLM needs to pass several verdicts. The more benchmark records you break, the better you are.\n\nIt is the way to win the SOTA race\n\nWell, I disagree. Even though for the AI advancement we need milestones and better performances, still the user experience and the personal point of view cannot be just put aside as irrelevant.\n\nI believe that exploring some frequently used NLP tasks, and putting aside the chat experience, we must focus on the quality of the replies. And we are the only benchmark required. Our user experience is the best indicator to understand if a model is good or not. The model must be reliable enough to be used in an automated workflow.\n\nBy the way, I already run what I decided to call [RBYF ‚Äî Revised Benchmarks with You as a Feedback](https://open.substack.com/pub/thepoorgpuguy/p/rbyf-is-here-revised-benchmarks-with?r=i78xo&utm_campaign=post&utm_medium=web) on the claimed amazing Llama3.2‚Äì1B-instruct‚Ä¶ and Qwen2.5‚Äì1.5b is so much better!\n\nSo in this article, as promised, we will verify with our own eyes how good is this model for every day use.\n\nBack to us‚Ä¶ Let‚Äôs get started!\n\n## Requirements\n\nHere we are going to build a minimal text interface to be able to run the model, test different tasks and wait for user feedback to evaluate it.\n\nThe requirements are minimal, but I suggest you to create a new project directory and a virtual environment.\n\nCreate a `venv` (python 3.11+ is required): I tested it on my Mini-PC running Windows 11.\n\n```python\n## create the virtual environment\npython -m venv venv\n## activate the venv\nvenv\\Scripts\\activate\n## Install the dependencies \npip install llama-cpp-python==0.3.0 tiktoken\n```\n\nWe need to download the GGUF file from the official qwen repository on Hugging Face [https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF): I used the `qwen2.5-1.5b-instruct-q5_k_m.gguf` version.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*YtQJb_xyq_xcF40yRWPcZA.png)\n\nWe are all set!\n\nNote: if you want to add a different Backend support of a GPU accelerator, you can follow [the instructions in the repo](https://github.com/abetlen/llama-cpp-python#supported-backends). I used, for example, the Vulkan support so before the pip install I added the environment variable\n\n```python\n## Vulkan support - for Windows\n$env:CMAKE_ARGS = \"-DGGML_VULKAN=on\"\n```\n\n## The Code ‚Äî a main app and a library\n\nTo keep the code at minimum, I decided to extend some functionalities using an external library. Well, it is a Do It Yourself library, so there are no secrets here.\n\nYou can find all the details in my article here:\n\nAnd to speed it up you can directly [download the file from here](https://github.com/fabiomatricardi/YouAreTheBenchmark/raw/main/QWEN2.5-1.5B/promptLibv2Qwen.py): it contains a version 2 of the `promptLib` discussed in the mentioned above article (and it is called `promptLibv2Qwen.py`, with few fine tuning of the prompt specifically tailored for the `Qwen2.5-1.5B-instruct` model.\n\nSave the file in the main directory, and create a new file called `main.py`\n\n```python\n## Chat with an intelligent assistant in your terminal  \n## MODEL: https://huggingface.co/Qwen\n## qwen2.5-1.5b-instruct-q5_k_m.gguf\nimport sys\nfrom time import sleep\nimport warnings\nwarnings.filterwarnings(action='ignore')\nimport datetime\nfrom promptLibv2Qwen import countTokens, writehistory, createCatalog\nfrom promptLibv2Qwen import genRANstring, createStats\nimport argparse\n### PREPARING FINAL DATASET\npd_id = []\npd_task = []\npd_vote = []\npd_remarks = []\n####################Add GPU argument in the parser###################################\nparser = argparse.ArgumentParser()\nparser.add_argument(\"-g\", \"--gpu\", type=int, default=0,nargs='?',\n                    help=\"The number of layers to load on GPU\")\nargs = parser.parse_args()\nif args.gpu == None:\n   ngpu_layers = 0 \nelse:\n    ngpu_layers = args.gpu\nprint(f'Selected GPU: offloading {ngpu_layers} layers...')   \n####################INITIALIZE THE MODEL###################################\nstops = ['<|im_end|>']\ntasks = createCatalog()\nmodelname = 'qwen2.5-1.5b-instruct-q5_k_m.gguf'\n## create THE LOG FILE \ncoded5 = genRANstring(5)\nlogfile = f'logs/Qwen2.5-1.5B-it_CPP_{coded5}_log.txt'\ncsvfile = f'logs/Qwen2.5-1.5B-it_CPP_{coded5}.csv'\nlogfilename = logfile\n#Write in the history the first 2 sessions\nwritehistory(logfilename,f'{str(datetime.datetime.now())}\\n\\nYour own LocalGPT with üíª {modelname}\\n---\\nüß†ü´°: You are a helpful assistant.')  \nwritehistory(logfilename,f'üíª: How can I assist you today in writing?')\n```\n\nHere we are only doing preparations: we import the libraries, including our own personal `promptLibv2Qwen` and also `argparse`. I wanted to try something new: [argparse](https://realpython.com/command-line-interfaces-python-argparse/) is a python library intended for terminal python program where you are reading multiple arguments from the command line.\n\nIn this case here we have only one argument (and no parameters) with th flag `-g` or even `--gpu`. When you run the python code with this argument we will set the number of GPU layers to the maximum (but you can change it yourself).\n\nThen we set some global variables, used across the entire code: the tasks, our prompt collection, the stop words and the log filename.\n\n> NOTE: all the logs are saved into a subdirectory called `logs`‚Ä¶ so make sure to create one.\n\nWe are also preparing all the relevant information to store them into a dataset and then save it at the end int a CSV file (for easily creating a performance matrix)\n\n```python\n### PREPARING FINAL DATASET\npd_id = []\npd_task = []\npd_vote = []\npd_remarks = []\n```\n\nWe then load the model into RAM (no GPU) or the VRAM (with GPU) using Llama-CPP-python.\n\n```python\n## LOAD THE MODEL\nprint(\"\\033[95;3;6m\")\nprint(\"1. Waiting 10 seconds for the API to load...\")\nfrom llama_cpp import Llama\nllm = Llama(\n            model_path='models/qwen2.5-1.5b-instruct-q5_k_m.gguf',\n            n_gpu_layers=ngpu_layers,\n            temperature=0.1,\n            n_ctx=8192,\n            max_tokens=1500,\n            repeat_penalty=1.178,\n            stop=stops,\n            verbose=False,\n            )\nprint(f\"2. Model {modelname} loaded with LlamaCPP...\")\nprint(\"\\033[0m\")  #reset all\nhistory = []\nprint(\"\\033[92;1m\")\nprint(f'üìùLogfile: {logfilename}')\n```\n\nBy the way, you can find all the code in my GitHub Repository:\n\nThe next one is a one-shot warm up inference: the model neural network is going to be activated for the first time, so think about it like a warm-up lap.\n\nDon‚Äôt be scared, I will explain the code\n\n```python\n##################### ALIGNMENT FIRST GENERATION ##############################################\nquestion = 'Explain the plot of Cinderella in a sentence.'\ntest = [\n    {\"role\": \"user\", \"content\": question}\n]\nprint('Question:', question)\nstart = datetime.datetime.now()\nprint(\"üíª > \", end=\"\", flush=True)\nfull_response = \"\"\nfisrtround = 0\nfor chunk in llm.create_chat_completion(\n    messages=test,\n    temperature=0.25,\n    repeat_penalty= 1.31,\n    stop=stops,\n    max_tokens=1500,\n    stream=True,):\n    try:\n        if chunk[\"choices\"][0][\"delta\"][\"content\"]:\n            if fisrtround==0:\n                print(chunk[\"choices\"][0][\"delta\"][\"content\"], end=\"\", flush=True)\n                full_response += chunk[\"choices\"][0][\"delta\"][\"content\"]\n                ttftoken = datetime.datetime.now() - start  \n                fisrtround = 1\n            else:\n                print(chunk[\"choices\"][0][\"delta\"][\"content\"], end=\"\", flush=True)\n                full_response += chunk[\"choices\"][0][\"delta\"][\"content\"]                            \n    except:\n        pass      \ndelta = datetime.datetime.now() - start\noutput = full_response\nprint('')\nprint(\"\\033[91;1m\")\nrating = input('Rate from 0 (BAD) to 5 (VERY GOOD) the quality of generation> ')\nprint(\"\\033[92;1m\")\nstats = createStats(delta,question,output,rating,logfilename,'Alignment Generation',ttftoken)\nprint(stats)\nwritehistory(logfilename,f'''üë®‚Äçüíª . {question}\nüíª > {output}\n{stats}\n''')\n```\n\nWe set the first user question and put into a well known chat format dictionary. Then we start our timer (useful for speed, token counts etc‚Ä¶).\n\nWe call the inference with the method `create_chat_completion()` that allows us to accept the prompts in chat format and stream the output one token at the time.\n\nBecause of the first reply from the model does not contain any output tokens (but only statistics) we use the try/except statement. Furthermore, since I want to know when the first token is generated, we raise a flag and stop temporary our time count saving the information inside the `ttftoken` variable.\n\nAt the end of the streaming we count the delta time from start, and wait the user to provide his/her personal feedback on the generated output: giving a mark from 0 to 5 and adding comments related to the compliance with the instruction prompt and user intent.\n\nWe use our internal library called `createStats()` to print all the statistics of the generation, and save them in our log file. The output of the function will be something like this:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8znYCqpisviXvYgrzjWF5w.png)\n\n## Prompt catalog ‚Äî what we want to test\n\nI wrote here about my habit. I have a catalog of prompts that covers many of the main language tasks used in chat-bots, like summarization, short summarization, casual chat, RAG, truthful RAG and so on.\n\nThe idea is to be able to load the model in 5 minutes, and start evaluating each task. At the end of every generation the user is prompted to give a mark (a score from 0 to 5) and leave any comments if required.\n\nThis is crucial: not all the models are alike, and small/big adjustments to the wording in the prompts are always required.\n\nSo back to the code‚Ä¶ Because the previous one was only a warm-up, now it will start the real while loop, iterating over the entire prompt catalog. See the workflow here below‚Ä¶\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*EL0Q97Du6HwtcYQZ.png)\n\nThere are only few changes in the code, and I will point them out, so bear with me.\n\n```python\n############################# AUTOMATIC PROMPTING EVALUATION  11 TURNS #################################\nid =1\nfor items in tasks:\n    fisrtround = 0\n    task = items[\"task\"]\n    prompt = items[\"prompt\"]\n    test = []\n    print(f'NLP TAKS>>> {task}')\n    print(\"\\033[91;1m\")  #red\n    print(prompt)\n    test.append({\"role\": \"user\", \"content\": prompt})\n    print(\"\\033[92;1m\")\n    full_response = \"\"\n    start = datetime.datetime.now()\n    print(\"üíª > \", end=\"\", flush=True)\n    for chunk in llm.create_chat_completion(\n        messages=test,\n        temperature=0.15,\n        repeat_penalty= 1.31,\n        stop=stops,\n        max_tokens=1500,\n        stream=True,):\n        try:\n            if chunk[\"choices\"][0][\"delta\"][\"content\"]:\n                if fisrtround==0:\n                    print(chunk[\"choices\"][0][\"delta\"][\"content\"], end=\"\", flush=True)\n                    full_response += chunk[\"choices\"][0][\"delta\"][\"content\"]\n                    ttftoken = datetime.datetime.now() - start  \n                    fisrtround = 1\n                else:\n                    print(chunk[\"choices\"][0][\"delta\"][\"content\"], end=\"\", flush=True)\n                    full_response += chunk[\"choices\"][0][\"delta\"][\"content\"]                            \n        except:\n            pass      \n    delta = datetime.datetime.now() - start\n    print('')\n    print(\"\\033[91;1m\")\n    rating = input('Rate from 0 (BAD) to 5 (VERY GOOD) the quality of generation> ')\n    print(\"\\033[92;1m\")\n    stats = createStats(delta,prompt,full_response,rating,logfilename,task,ttftoken)\n    print(stats)\n    writehistory(logfilename,f'''üë®‚Äçüíª > {prompt}\nüíª > {full_response}\n{stats}\n''')\n    pd_id.append(id)\n    pd_task.append(task)\n    pd_vote.append(rating[:2])\n    pd_remarks.append(rating[2:])\n    id += 1\n## create dataframe and save to csv\nzipped = list(zip(pd_id,pd_task,pd_vote,pd_remarks))\nimport pandas as pdd\ndf = pdd.DataFrame(zipped, columns=['#', 'TASK', 'VOTE','REMARKS'])\n#saving the DataFrame as a CSV file \ndf_csv_data = df.to_csv(csvfile, index = False, encoding='utf-8') \nprint('\\nCSV String:\\n', df_csv_data)  \n```\n\nThe main changes are only in the first lines:\n\n```python\nfor items in tasks:\n    fisrtround = 0\n    task = items[\"task\"]\n    prompt = items[\"prompt\"]\n```\n\nIf you read the article about the `promptLib` you shouldn‚Äôt be surprised: but if you are new, here we are iterating over a list of dictionaries with the following structure:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*rGcKJWNzSUrcu4wi.png)\n\nSo for each items in the catalog (means a pair of tasks and prompts) we extract the task description and the prompt for the task.\n\n```python\ntest.append({\"role\": \"user\", \"content\": prompt})\n```\n\nThen we create the chat template message in a temporary list called `test` and pass it to the `create_chat_template()` method for generation.\n\nEverything else is the same.\n\nSave the file, and with the `venv` activated run:\n\n```python\npython main.py\n## if you are using the GPU python main.py -g\n```\n\nThis will get you something like the below example‚Ä¶\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MhhQu4lLjtU__Wjf0dSWBg.gif)\n\nNote that at the end of the entire Prompt Catalog a *csv* file is created with the summary of all the tasks!\n\n## Test Overview\n\nI run them with several Small Language Models, from [Qwen2‚Äì1.5B-instruct](https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF), to [Gemma2‚Äì2B-instruct](https://huggingface.co/bartowski/gemma-2-2b-it-GGUF), with [Llama3.2‚Äì1B-instruct](https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF) and finally the new [Qwen2.5‚Äì1.5B-instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF).\n\nWhile I was [quite disappointed by Llama3.2‚Äì1B-instruct](https://generativeai.pub/llama3-2-1b-instruct-is-ok-but-not-good-enough-28f88046b63e), I have been amazed by the good job done by the new [Qwen2.5‚Äì1.5B-instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF).\n\nAt the end of every generation the user is asked to evaluate the results with a mark from 0 to 5. **In this case the user is me‚Ä¶**\n\nThis kind of qualitative analysis is indeed poor, so every mark does have a description, and the user can add comments (‚Äúsome wrong information‚Äù, ‚Äúmaybe better change the wording in the prompt‚Äù )\n\nHere the qualitative matrix with a description\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*eBdPfZtfr99MsvLh6tt42w.png)\n\n## The good and the bad ‚Äî details\n\nSummarization was amazing. Listing the main topics over a log text was also very good.\n\nThe RAG tasks were quite fast (even on my mini-PC) and truthful RAG (ask questions out of the context) was on spot.\n\nSo really good.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*DuV3LJep_PuDqiCcAMb6Cg.png)\n\nThere were also few shortcomings: even with a temperature of only `0.15` I got some made up information in the summarize in two sentences task. And this is not good.\n\n> I expect this can be fixed with a little prompt tuning, or with a temperature at `0`.\n\nAnother fact is that the creative writing was quite bad: for the test I used some fixed Generation Parameters.\n\n```python\n        temperature=0.15,\n        repeat_penalty= 1.31,\n```\n\nProbably for creative writing, with Qwen2.5‚Äì1.5B-instruct, we should use an higher `repeat_penalty` and also a higher `temperature`.\n\nAs a side note I have to say that also the reflection prompt was not that bad at all! The opening/closure of the tags was not maintained (so not easy to put this into a pipeline or workflow) but the overall flow of the generation and the ‚ÄúChain of Thought‚Äù reasoning process was fairly good.\n\n```python\n<thinking>\nThe importance of using Generative Artificial Intelligence (AI) in \nteaching cannot be overstated, as this technology has immense potential \nnot only within current educational practices but also towards shaping a \nmore innovative approach to lifelong learning.\n</thinking>\n\n**Chain Of Thought:**\n1. **Understanding the Impact**: AI can automate repetitive tasks and provide \npersonalized feedback for students based on their performance data \n(e.g., through chatbots or adaptive assessments).\n2. **Enhancing Learning Experience**: By integrating Generative AI, \nteachers could create more engaging lessons that cater to diverse learning \nstyles.\n3. **Improving Student Outcomes**: Personalized instruction can lead to \nbetter understanding and retention of knowledge for students.\n\n**Reflection:**\n- The integration of generational technology in education is crucial as \nit aligns with the evolving needs of learners who are increasingly digital \nnatives, requiring a pedagogical approach that incorporates these technologies.\n</thinking>\n\n<output>\nTeachers should learn how Generative AI can be used to enhance their teaching \nmethods and improve student outcomes. This includes understanding its \ncapabilities for automating tasks like grading or creating personalized \nlearning paths.\n\n**Learning Path:**\n1. **Foundation Courses in Education Technology**: Start with courses \nthat provide an overview of educational technology, including the basics \nof digital tools.\n2. **AI Fundamentals Course (Coursera)** - Learn about AI concepts and \nhow they can be applied to education through Coursera's free online \ncourse on Artificial Intelligence for Educators.\n\n3. **Specialized Courses in Generative Learning**: Enroll in courses \nthat focus specifically on generational learning, such as \"Generative Design\" \nor similar specialized programs offered by universities like MIT.\n4. **Professional Development Workshops and Conferences** - Attend \nworkshops focused on integrating AI into teaching practices at local \neducational conferences.\n\n5. **Online Communities for Educators (e.g., Edmodo)**: Join online \ncommunities where educators discuss the use of generative technology \nin education, share resources, or ask questions about implementation.\n6. **Certification Programs**: Consider obtaining certifications that \nrecognize your knowledge and skills related to AI integration into teaching \npractices through organizations like Google's \"AI for Educators\" program.\n\nBy following this learning path, teachers can not only enhance their own \nprofessional development but also contribute positively towards the future \nof education by incorporating generative technology effectively.\n</output>\n```\n\nI think that for the size of this Small Language Model, the result is not bad at all!\n\n## Comparing Qwen2.5 with Llama3.2\n\nEven though this is only coming from my personal evaluation, I want to share it with you.\n\nBoth models are designed to be used on mobile devices, but the performance is quite different. Look here below:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*T6vLgvOKdkotlV1K5x6-QQ.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*DuV3LJep_PuDqiCcAMb6Cg.png)\n\nFirst of all, the overall score difference is huge (41 for Llama3.2, 57 for Qwen2.5).\n\nSecondly if you think about what you may ask on a mobile device, in terms of language tasks, is to have a smooth chatting experience (task 4), good summarization (tasks five to 7) and some creative writing (task 11 and 13).\n\nIn terms of speed, running the model only on CPU, with a very limited mini-PC, **I got an average inference speed of 14 t/s.**\n\n## Conclusions\n\nIn the past three months since Qwen2‚Äôs release, numerous developers have built new models on the Qwen2 language models, providing valuable feedback to the entire community, but also to Alibaba Cloud.\n\n> During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5\n\nTheir claims come with facts about the new family of models:\n\n* Dense, **easy-to-use**, decoder-only language models, available in 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B sizes, and base and instruct variants.\n* Pretrained on our latest large-scale dataset, encompassing up to 18T tokens.\n* Significant improvements in **instruction following**\n* More **resilient to the diversity of system prompts**, enhancing role-play implementation and condition-setting for chatbots.\n* **Context length support up to 128K** tokens and can generate up to 8K tokens.\n* Multilingual support for over 29 languages\n\nIn my extensive (but certainly limited to one shot prompts and on few NLP tasks) tests I could see with my very own eyes that the claims were based on a good quality training dataset and curated fine tuning.\n\nThis model can perform extremely good on mobile devices!\n\n\n"},{"lang":"en","group":"blog","slug":"blog/rag-llm-and-pdf-conversion-to-markdown-text-with-pymupdf-03af00259b5d","frontmatter":{"title":"RAG/LLM and PDF: Conversion to Markdown Text with PyMuPDF","meta_title":"RAG/LLM and PDF: Conversion to Markdown Text with PyMuPDF","description":"Data feeding in markdown text format increases generated text quality","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*swPjVuudAhsoRiiw3Ee32w.png","categories":["Programming","Technology","Technology/Web"],"author":"Rifx.Online","tags":["markdown","PyMuPDF","LLM","RAG","PDF"],"draft":false,"slug":"blog/rag-llm-and-pdf-conversion-to-markdown-text-with-pymupdf-03af00259b5d"},"content":"\n\n\n\n\n### Data feeding in markdown text format increases generated text quality\n\n\n\n\n## Introduction\n\nIn the context of **Large Language Models (LLMs)** and **Retrieval-Augmented Generation (RAG)** environments, data feeding in **markdown text format** holds **significant importance**. Here are some detailed considerations.\n\n**LLMs** are powerful language models that can generate coherent and contextually relevant text. However, they may sometimes produce responses that lack factual accuracy or context. By incorporating retrieval-based methods (like RAG), we can enhance the quality of generated text.\n\n**RAG** enables the integration of **external data** ‚Äî previously absent in the LLM‚Äôs training data ‚Äî into the text generation process. This inclusion mitigates ‚Äúhallucination issues‚Äô‚Äô and enhances the relevance of text responses.\n\n\n## Why Markdown for LLM?\n\n**Markdown** is a lightweight markup language that allows users to format plain text using simple syntax. It is widely used for creating structured documents, especially on platforms like GitHub, Jupyter notebooks, and various content management systems. When feeding data into an LLM or RAG system, using markdown format provides several benefits:\n\n1. **Structured Content**: Markdown allows you to organize information into headings, lists, tables, and other structured elements. This structure aids in better understanding and context preservation.\n2. **Rich Text**: Markdown supports basic formatting such as bold, italics, links, and code blocks. Including rich text in the input data enhances the context for the language model.\n3. **Embedding Links and References**: Markdown lets you embed hyperlinks, footnotes, and references. In RAG scenarios, this can be crucial for referring to external sources or providing additional context.\n4. **Ease of Authoring**: Markdown is human-readable and easy to write. Authors can create content efficiently without complex formatting tools.\n5. **Chunking**: Essential for RAG systems, chunking (otherwise known as ‚Äúsplitting‚Äù) breaks down extensive documents for easier processing. With PyMuPDF data extraction available in MD format we support chunking to keep text with common context together. **Importantly, PyMuPDF extraction in MD format allows for [Level 3 chunking](https://readmedium.com/five-levels-of-chunking-strategies-in-rag-notes-from-gregs-video-7b735895694d#b123)**.\n\nIn summary, using markdown text format in LLM and RAG environments ensures more accurate and relevant results because it supplies richer data structures and more relevant data chunk loads to your LLM.\n\n\n## PyMuPDF Support for Markdown Conversion of a PDF\n\nSince its inception, PyMuPDF has been able to extract text, images, vector graphics and, since August 2023, tables from PDF pages. Each of these object types has its own extraction method: there is one for text, and yet others for tables, images and vector graphics. To meet the requirements of RAG, we merged these disparate extractions to produce one common, unified **Markdown** string which consistently represents the page‚Äôs content as a whole.\n\nAll this is implemented as [one Python script](https://github.com/pymupdf/RAG/blob/main/helpers/pymupdf_rag.py). It can be imported as a module by some other script, or be invoked as a line command in a terminal window like this:\n\n`$ python pymupdf_rag.py input.pdf [-pages PAGES]`\n\nIt will produce a text file (called `input.md`) in **Markdown** format. The optional parameter `PAGES` allows restricting the conversion to a subset of the PDF‚Äôs total pages. If omitted, the full PDF is processed.\n\n\n## Markdown Creation Details\n\n\n### Selecting Pages to Consider\n\nThe ‚Äú`-pages`‚Äù parameter is a string consisting of desired page numbers (1-based) to consider for markdown conversion. Multiple page number specifications can be given, separated by commas. Each specification either is one integer or two integers separated by a ‚Äú`-`‚Äù hyphen, specifying a range of pages. Here is an example:\n\n‚Äú`-pages 1‚Äì10,15,20-N`‚Äù\n\nThis would include pages 1 through 10, 15 and pages 20 through the end of the file (capital ‚ÄúN‚Äù is treated as the number of the last page).\n\n\n### Identifying Headers\n\nUpon invocation, the program examines all text on the given pages and finds the most frequently used font size. This value (and all smaller font sizes) is assumed to represent **body text**. Larger font sizes are assumed to represent **header text**.\n\nDepending on their relative position in the font size hierarchy, header text will be prefixed with one or more markdown header `#`-tag characters.\n\n\n### Identifying the Processing Mode per Page Area\n\nAll text on each page will first be classified as being either **standard** text or **table** text. Then the page content will be extracted from top to bottom converting everything to markdown format.\n\nThis is best explained by an example:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*u5fv2aAIvDaaAd6H.png)\n\nThis page shows content, that represents typical situations:\n\n* Two tables, having partly overlapping vertical positions. One table has no headers, the other one has **external** column headers.\n* There is a **title** line and **headers** at multiple levels.\n* The **body text** contains a variety of styling details like **bold**, *italic* and `inline code`.\n* Ordered and unordered lists.\n* Code snippet.\n\nLayout analysis will determine three areas and select the appropriate processing modes: **(1)** text, **(2)** table, **(3)** text.\n\nThe generated Markdown text reflects the above faithfully ‚Äî as much as at all possible in this format.\n\nFor an example, let us look at the output for the table with external headers:\n\n\n```python\n|Column1|Column2|\n\n|---|---|\n\n|Cell (0, 0)|Cell (0, 1)|\n\n|Cell (1, 0)|Cell (1, 1)|\n\n|Cell (2, 0)|Cell (2, 1)|\n```\nThis is GitHub-compatible format with the minimum possible token size ‚Äî an important aspect for keeping feeds into RAG systems small.\n\n**Column borders** are indicated by the ‚Äú`|`‚Äù character. A text line is assumed to be a **table header** if it is followed by a line of the form ‚Äú`|---|---| ‚Ä¶`‚Äù. The full **table definition** must be preceded and followed by at least one empty line.\n\nPlease note that for technical reasons markdown tables must have a header and thus will choose the first table row if no external header is available.\n\nTo confirm overall fidelity, here is how a Markdown parser processes the full page:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Ge83uj7FiM4T6XFn)\n\n\n## Invoking the Markdown Converter Programmatically\n\nInstead of executing a program in the command line, Markdown conversion can also be requested by a program:\n\n\n```python\nimport fitz\nfrom pymupdf_rag import to_markdown  # import Markdown converter\n\ndoc = fitz.open(‚Äúinput.pdf‚Äù)  # open input PDF\n\n## define desired pages: this corresponds ‚Äú-pages 1-10,15,20-N‚Äù\npage_list = list(range(9)) + [14] + list(range(19, len(doc) ‚Äì 1))\n\n## get markdown string for all pages\nmd_text = to_markdown(doc, pages=page_list)\n\n## write markdown string to some file\noutput = open(‚Äúout-markdown.md‚Äù, ‚Äúw‚Äù)\noutput.write(md_text)\noutput.close()\n```\n\n## Conclusion\n\nBy integrating PyMuPDF‚Äôs extraction methods, the content of PDF pages will be faithfully converted to markdown text that can be used as input for RAG chatbots.\n\nRemember, the key to a successful RAG chatbot lies in the quality and completeness of information it can access.\n\nPyMuPDF-enabled markdown extraction ensures that this information from PDFs is not only possible but straightforward, showcasing the library‚Äôs strength and developer-friendliness. Happy coding!\n\n\n### Source Code\n\n* [RAG/helpers/pymupdf\\_rag.py (github.com)](https://github.com/pymupdf/RAG/blob/main/helpers/pymupdf_rag.py)\n\n\n### References\n\n* [5 Levels of Text Splitting](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb)\n\n\n### Related Blogs\n\n* [Building a RAG Chatbot GUI with the ChatGPT API and PyMuPDF](https://readmedium.com/building-a-rag-chatbot-gui-with-the-chatgpt-api-and-pymupdf-9ea8c7fc4ab5)\n* [Creating a RAG Chatbot with ChatGPT and PyMUPDF](https://readmedium.com/creating-a-rag-chatbot-with-chatgpt-and-pymupdf-f6c30907ae27)\n* [RAG/LLM and PDF: Enhanced Text Extraction](https://readmedium.com/rag-llm-and-pdf-enhanced-text-extraction-5c5194c3885c)\n\n"},{"lang":"en","group":"blog","slug":"blog/rbyf-qwen2-5-3b-instruct-is-damn-good-dcf443cacc63","frontmatter":{"title":"RBYF: Qwen2.5‚Äì3B-instruct is damn good.","meta_title":"RBYF: Qwen2.5‚Äì3B-instruct is damn good.","description":"Revised Benchmark with You as a Feedback: the brand new 3B model from Alibaba Qwen is an amazing model, and I can prove it!","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*NWaBtJ64TLUoUHv4F1qJpg.png","categories":["Programming","Technology","Science"],"author":"Rifx.Online","tags":["Qwen","NLP","multimodal","RBYF","evaluation"],"draft":false,"slug":"blog/rbyf-qwen2-5-3b-instruct-is-damn-good-dcf443cacc63"},"content":"\n### Revised Benchmark with You as a Feedback: the brand new 3B model from Alibaba Qwen is an amazing model, and I can prove it!\n\n\n\nThe illusion of emergent properties is largely a product of the metrics used to evaluate these models. And this is a fact.\n\nFew weeks ago I decided to be a little rebel, discard all the official Benchmarks, and start being a Benchmark myself!\n\nThis is the meaning behind this totally made up Acronym RBYF: Revised Benchmark with You as a Feedback. And the underlined principle is that there is no better judge than you, to verify how good a Large Language Model can be.\n\nTo be honest, I am focusing on Small Language Models. I don‚Äôt own a dedicated GPU and my computational resources are limited. But again, I agree to the [LLMWare rebel principle number one](https://readmedium.com/getting-work-done-with-genai-just-do-the-opposite-10-contrarian-rules-that-may-actually-work-634501602a27):\n\nUse Small Models, Not Large Ones.\n\nIn this article I am going to show you the results of my evaluation on qwen2.5‚Äì3b-instruct. And it is really good!\n\n> Disclaimer: all the prompt used with the results are available in my GitHub repository:\n\n## Less is More\n\nScaling laws describe how model performance improves as the number of parameters and training data increases. This principle has fueled the search for novel abilities in LLMs.\n\n> Simply increasing the size of a model, we can unlock new capabilities‚Ä¶\n\nScaling laws describe the relationship between model performance and the number of parameters and training data. As models grow larger and are trained on more data, we expect their performance to improve. This has led to a relentless pursuit of larger and larger LLMs, in the hope of unlocking new capabilities.\n\nEmergent properties are those that arise from the interactions of individual components within a complex system. They cannot be predicted or understood by studying the components in isolation. In the case of LLMs, the hope is that as these models grow larger and more complex, they will exhibit unexpected and new capabilities.\n\nThis is a fairy-tale.\n\nIn the past weeks we saw with our own eyes, that over-trained and well curated Small Language Models can perform as good as their big brothers. And this is a punch to the so called emergent abilities, striking back to the scaling law. Gemma2‚Äì2B, Qwen2.5‚Äì3B and even the latest Llama3.2‚Äì3B are far better models than the old SOTA 7B models.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*-EjpdEky-Hf3WEQn.png)\n\n## The Qwen2.5 family of Models\n\nAlibaba Cloud released on middle of September their flagship model family Qwen2.5.\n\n> Alibaba Cloud‚Äôs revolutionary journey with Qwen is showing once again strong Leadership through Innovation\n\nQwen2.5 is the large language model and large multimodal model series of the Qwen Team, Alibaba Group. Both language models and multimodal models are pretrained on large-scale multilingual and multimodal data and post-trained on quality data for aligning to human preferences. Qwen is capable of natural language understanding, text generation, vision understanding, audio understanding, tool use, role play, playing as AI agent, etc.\n\n**What stands out in the new Qwen2.5 is the thoroughly curated training dataset.** You can clearly understand this, checking the small models performance.\n\nIf the Small Language Models of the family are good, means that the training and the dataset were highly revised and curated.\n\nHere some numbers:\n\n* Dense, easy-to-use, decoder-only language models, available in 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B sizes, and base and instruct variants.\n* Pretrained on our latest large-scale dataset, encompassing up to 18T tokens.\n* Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON.\n* More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n* Context length support up to 128K tokens and can generate up to 8K tokens.\n* Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*3tnTS_UCImBRBDeKmFyjVQ.png)\n\n## Qwen2.5‚Äì3B-instruct\n\nEven though for the AI advancement we need milestones and better performances, still the user experience and the personal point of view cannot be just put aside as irrelevant.\n\nI believe that exploring some frequently used NLP tasks, and putting aside the chat experience, we must focus on the quality of the replies. And we are the only benchmark required. **Our user experience is the best indicator to understand if a model is good or not**. The model must be reliable enough to be used in an automated workflow.\n\nBy the way, I already run what I decided to call [RBYF ‚Äî Revised Benchmarks with You as a Feedback](https://open.substack.com/pub/thepoorgpuguy/p/rbyf-is-here-revised-benchmarks-with?r=i78xo&utm_campaign=post&utm_medium=web) on [Qwen2.5‚Äì1.5b-instruct](https://ai.gopubby.com/qwen2-5-1-5b-the-future-of-mobile-ai-6bd5f29bbc84): you can read the details. In the article I also explained how to create your test bench. The method described is the same I used for Qwen2.5‚Äì3B.\n\nLet‚Äôs begin with an overall performance across all the tasks. The model has been evaluated by me (in this case is my Own Feedback) based on the qualitative matrix as displayed here below.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*rdVHfCDWX9jlvtiq)\n\nThe overall score is 62/70 = 8.8\n\nOk, but based in what Qwen2.5‚Äì3B-instruct got this evaluation score?\n\n## Test Overview\n\nThe idea behind this is a fair user feedback, not an automated one across standard benchmarks and frameworks. Is a Small Language Model able to satisfy the user intent over the mostly used NLP task?\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*wgngfGvSebeoH3YxTdvc8A.png)\n\nWe want to validate both the user intent and the quality of the responses. Here the breakdown of every task:\n\n### Introduction\n\nTo verify how the model reply to initial greetings and talks about itself.\n\n### Explain in one sentence\n\nSynthesis and summarization. The final evaluation focuses in whether or not the model is able to fit the reply in only one sentence\n\n### Explain in three paragraphs\n\nUser intent is to get a smart explanation of the text that must fit into three paragraphs. SML usually find it very hard because the always put a recap (forth) paragraph.\n\n### Say ‚ÄúI am ready‚Äù.\n\nIn a chat turn base application, instruction-following models are usually asked to first read a provided text and later on to complete some sort of analysis. Usually SML cannot do this‚Ä¶\n\n### Summarize\n\nBasic summarization, with no limits. Here we want to evaluate how the summary is grounded on the text, without made up facts.\n\n### Summarize in two sentences\n\nBasic summarization, with a 2 sentences limits. Here we want to evaluate how the summary is grounded on the text, without made up facts: but as well we want to ensure the 2 sentences constraint.\n\n### Write in a list the three main key points ‚Äî format output\n\nFocus: the SML must format the output in a specific format. In this prompt we ask to create a list of the 3 key points and give the output as a python list.\n\n### Table of Contents\n\nThis task is quite hard for many SML. The prompt requires some adjustments otherwise the model return a markdown table. The user want an ordered list of the topics following the provided document structure.\n\n### RAG\n\nRetrieval Augmented Generation, without any framework (haystack, Langchain‚Ä¶). This is one of the most used tasks for a Language model. The reply is evaluated on the ability to understand the instructions and how grounded to the text is the answer.\n\n### Truthful RAG\n\nIt is A RAG with a question completely out of the provided context. The model must reply unanswerable meaning it understood the instruction and it is not using any external knowledge or made up information.\n\n### Write content from a reference\n\nThis is a creative task. Using a reference text the SML must provide a new essay.\n\n### Extract 5 topics\n\nThe focus on this task is to verify that:\n\n* there are exactly 5 topics\n* they are grounded (no hallucination)\n\n### Creativity: 1000 words SF story\n\nCompletely creative task. It is very hard even for larger models to be coherent and produce a small story hitting the correct word count.\n\n### Reflection prompt\n\nThe reflection prompt is meant to verify the CoT reasoning process of the model. The output is constrained into opening/closure of special tags. The focus is both on the reasoning and on a consistent output structure. The output must be easily used for further structured prompts or visualizations. You can read more in this article:\n\n## The evaluation process\n\nAt the end of every generation the user is asked to evaluate the results with a mark from 0 to 5. **In this case the user is me‚Ä¶**\n\nThis kind of qualitative analysis is indeed poor, so every mark does have a description, and the user can add comments (‚Äúsome wrong information‚Äù, ‚Äúmaybe better change the wording in the prompt‚Äù )\n\nHere the qualitative matrix with a description\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*H_Qx2UT1lxqy2BF4.png)\n\n## The good and the bad ‚Äî details\n\nOverall Qwen2.5‚Äì3B-instruct is an All Rounded model. The average score for each task is 4.4 out of 5, that is quite impressive.\n\nSummarization was amazing. Listing the main topics over a log text was also very good.\n\nIt shines in RAG, sometimes being too verbose: in the truthful RAG it also gave some explanation, and the reasons for the *unanswerable* reply.\n\nWe can have a look together. It is quite often a hard feat for SML to precisely count the number of sentences and paragraphs.\n\n```python\n**Write a two-sentences summary of the following text:**\n\nChina's regime under Chinese Communist Party operates without transparent \nelections and primary candidates. While undesirable for democracies, \nit allows long-term planning free from electoral pressures that can change \nplans abruptly; this has led to China achieving peak carbon dioxide emissions \nin 2023 due largely to increased renewable energy installations, potentially \nsetting a new global standard for sustainable development despite its status \nas one of the world's largest polluters.\n```python\n**Write a two-sentences summary of the following text:**\n\nChina's regime under Chinese Communist Party operates without transparent \nelections and primary candidates. While undesirable for democracies, \nit allows long-term planning free from electoral pressures that can change \nplans abruptly; this has led to China achieving peak carbon dioxide emissions \nin 2023 due largely to increased renewable energy installations, potentially \nsetting a new global standard for sustainable development despite its status \nas one of the world's largest polluters.\n\n```\n\nTable of Contents is another nightmare for the Language models, because they usually understand to create a table‚Ä¶ But look at this:\n\n```python\nA \"table of content\" provides an organized structure to the text, allowing \nreaders quick access to key points. Write the \"table of content\" of \nthe following text.\n\n[table of contents]\n1. Introduction: Overview and Critique on China's Governance System  \n2. Comparison with Western Democracies - Undesirable Dictatorship? \n3. Advantages in Long-Term Planning for Sustainability   \n4. The Chinese Model as a Comparative Advantage  \n5. Recent Achievements in Carbon Emissions Reduction\n6. Transition to Renewable Energy: A Turning Point?\n7. Potential Leadership Role of China and India  \n8. Conclusion - Future Directions\n\n```python\nA \"table of content\" provides an organized structure to the text, allowing \nreaders quick access to key points. Write the \"table of content\" of \nthe following text.\n\n[table of contents]\n1. Introduction: Overview and Critique on China's Governance System  \n2. Comparison with Western Democracies - Undesirable Dictatorship? \n3. Advantages in Long-Term Planning for Sustainability   \n4. The Chinese Model as a Comparative Advantage  \n5. Recent Achievements in Carbon Emissions Reduction\n6. Transition to Renewable Energy: A Turning Point?\n7. Potential Leadership Role of China and India  \n8. Conclusion - Future Directions\n\n\n```\n\nI assign a score of 4 out of 5 to the truthful RAG, but it could have been easily a 5. The most important part is that the response must conatin the word *unanswerable*.\n\nWhy? Imagine you are setting up a chain of prompts, and you need to decide the next step based on the previous reply. If you can simply look for the presence of the work *unanswerable* to steer your next step, you are ok!\n\n```python\n**Reply to the question only using the provided context. If the answer \nis not contained in the provided context say \"unanswerable\".\nquestion: who is Anne Frank?**\n{context}\n**Remember: if you cannot answer based on the provided context, say \n\"unanswerable\"\nanswer:**\n\nUnanswerable\nThe given text does not provide any information about Anne Frank \nor her identity. The content is unrelated to this question and focuses \ninstead on discussing China's system compared to Western democracies in \nterms of long-term planning for environmental issues.\n```python\n**Reply to the question only using the provided context. If the answer \nis not contained in the provided context say \"unanswerable\".\nquestion: who is Anne Frank?**\n{context}\n**Remember: if you cannot answer based on the provided context, say \n\"unanswerable\"\nanswer:**\n\nUnanswerable\nThe given text does not provide any information about Anne Frank \nor her identity. The content is unrelated to this question and focuses \ninstead on discussing China's system compared to Western democracies in \nterms of long-term planning for environmental issues.\n\n```\n\nI imagine that the model was trained to provide also the reasons for this kind of replies‚Ä¶ Awesome!\n\n### The excellent Reflection\n\nThe main focus of the Reflection prompt, is the ability of the model of returning a correct reasoning process, and format the output following few specific open/closure of tags.\n\nSo far this is **the best Reflection output** from a Small Language Model, even better than the Gemma2‚Äì2B:\n\n```python\n*üë®‚Äçüíª > You are an AI assistant designed to provide detailed, step-by-step \nresponses. Your outputs should follow this structure:\n1. Begin with a <thinking> section.\n2. Inside the thinking section:\n   a. Briefly analyze the question and outline your approach.\n   b. Present a clear plan of steps to solve the problem.\n   c. Use a \"Chain of Thought\" reasoning process if necessary, breaking down \nyour thought process into numbered steps.\n3. Include a <reflection> section for each idea where you:\n   a. Review your reasoning.\n   b. Check for potential errors or oversights.\n   c. Confirm or adjust your conclusion if necessary.\n4. Be sure to close all reflection sections.\n5. Close the thinking section with </thinking>.\n6. Provide your final answer in an <output> section.\nAlways use these tags in your responses. Be thorough in your explanations, \nshowing each step of your reasoning process. Aim to be precise and logical \nin your approach, and don't hesitate to break down complex problems into \nsimpler components. Your tone should be analytical and slightly formal, \nfocusing on clear communication of your thought process.\nRemember: Both <thinking> and <reflection> MUST be tags and must be closed \nat their conclusion\nMake sure all <tags> are on separate lines with no other text. \nDo not include other text on a line containing a tag.\n\nuser question: explain why it is crucial for teachers to learn how to use \ngenerative AI for their job and for the future of education. \nInclude relevant learning path for teachers and educators. *\n\n\n**üíª > <thinking>\nI will begin by analyzing the importance of teaching about Generative \nArtificial Intelligence (AI) in an educational context, then outline a \nstep-by-step plan on what steps should be taken to learn this technology \neffectively as well as provide resources or courses that could help.\nFirstly, understanding AI is crucial for teachers because it can \nsignificantly enhance their ability to support students. \nTeachers need the knowledge and skills necessary not only to integrate these tools into teaching but also understand how they work so they are able to guide both themselves and their pupils in ethical use of technology.\n</thinking>\n\n<reflection>\nMy initial thought process was clear, emphasizing that teachers should \nbe equipped with AI literacy as it can improve educational outcomes. \nI outlined the importance for guiding students ethically while using \nthese tools effectively.\nHowever, my plan needs more detail on specific steps to learn about \ngenerative AI and resources available online or in-person courses.\n</reflection>**\n*MY PERSONAL NOTE: here it is starting an additional thinking after the first \nreflection... And this is really too good to be true!!*\n**<thinking>\nTo create a comprehensive learning path that teachers could follow:\n1. **Understand Basic Concepts**: Start with basic concepts of machine \n   learning (ML) such as supervised vs unsupervised, deep neural networks  \n   etc., which are foundational to understanding generative AI.\n2. **Learn About Generative Models**:\n   - Study different types: Variational Autoencoders (VAEs), GANs.\n   - Understand how these models generate new data that resembles the \n     training set but is not identical and can be used for tasks like \n     text generation, image synthesis etc., which are crucial in education.\n3. **Explore Applications**: Look at real-world applications of generative \n   AI such as language modeling (e.g., GPT-2), computer vision techniques to \n   create realistic images or videos from raw data.\n4. **Ethical Considerations and Implications for Education**:\n   - Discuss the ethical implications, including bias in models' outputs \n     which could affect students‚Äô perceptions about themselves if they are \n     exposed regularly through AI-generated content that is not representative \n     of them.\n5. **Hands-on Practice**: Engage with practical exercises using open-source \n   tools like TensorFlow or PyTorch to build simple generative model \n   applications.\n6. **Stay Updated and Network**:\n   - Follow the latest research in ML, especially on Generative Models \n     by following relevant journals (e.g., NeurIPS), conferences such as \n     ICML etc.\n7. Attend Workshops/Conferences: Participate actively at AI-related workshops \n   or events to network with professionals working within this field.\n</thinking>**\n*MY PERSONAL NOTE: and no surprise, an additional reflection now starts, \nbefore producing the final output*\n**<reflection>\nMy plan has been expanded and includes a more detailed approach for teachers, \ncovering foundational knowledge through practical application of generative \nmodels in education along with ethical considerations that are crucial. \nI have also included staying updated on the latest research as an important \npart.\nHowever, it's essential to ensure these resources or courses exist so \neducators can access them easily.\n</reflection>\n\n<output>\nIt is vital for teachers and educational professionals alike to learn how \ngenerative AI works because this technology has significant potential in \nenhancing teaching methods. By understanding basic concepts of machine \nlearning (ML) such as supervised vs unsupervised, deep neural networks etc., \nthey will be better equipped with the knowledge necessary not only to \nintegrate these tools into their curriculum but also guide students ethically.\nA comprehensive plan for teachers and educators includes:\n1. **Understand Basic Concepts**: Start by grasping foundational concepts of ML.\n2. **Learn About Generative Models**:\n   - Study different types: Variational Autoencoders (VAEs), GANs, etc., \n     which are crucial in generating new data that resembles the training \n     set but is not identical and can be used for tasks like text generation or \n     image synthesis.\n3. **Explore Applications**: Look at real-world applications of generative AI \n   such as language modeling using tools like TensorFlow.\n4. **Ethical Considerations**:\n   - Discuss ethical implications, including bias in models' outputs which \n     could affect students‚Äô perceptions about themselves if they are exposed \n     regularly through AI-generated content that is not representative.\n5. **Hands-on Practice**: Engage with practical exercises to build simple \n   generative model applications using open-source tools.\n6. **Stay Updated and Network**:\n   - Follow the latest research in ML, especially on Generative Models by \n     following relevant journals (e.g., NeurIPS), conferences such as ICML etc.\n7. Attend Workshops/Conferences: Participate actively at AI-related workshops \n   or events to network with professionals working within this field.\n</output>\n```python\n*üë®‚Äçüíª > You are an AI assistant designed to provide detailed, step-by-step \nresponses. Your outputs should follow this structure:\n1. Begin with a <thinking> section.\n2. Inside the thinking section:\n   a. Briefly analyze the question and outline your approach.\n   b. Present a clear plan of steps to solve the problem.\n   c. Use a \"Chain of Thought\" reasoning process if necessary, breaking down \nyour thought process into numbered steps.\n3. Include a <reflection> section for each idea where you:\n   a. Review your reasoning.\n   b. Check for potential errors or oversights.\n   c. Confirm or adjust your conclusion if necessary.\n4. Be sure to close all reflection sections.\n5. Close the thinking section with </thinking>.\n6. Provide your final answer in an <output> section.\nAlways use these tags in your responses. Be thorough in your explanations, \nshowing each step of your reasoning process. Aim to be precise and logical \nin your approach, and don't hesitate to break down complex problems into \nsimpler components. Your tone should be analytical and slightly formal, \nfocusing on clear communication of your thought process.\nRemember: Both <thinking> and <reflection> MUST be tags and must be closed \nat their conclusion\nMake sure all <tags> are on separate lines with no other text. \nDo not include other text on a line containing a tag.\n\nuser question: explain why it is crucial for teachers to learn how to use \ngenerative AI for their job and for the future of education. \nInclude relevant learning path for teachers and educators. *\n\n\n**üíª > <thinking>\nI will begin by analyzing the importance of teaching about Generative \nArtificial Intelligence (AI) in an educational context, then outline a \nstep-by-step plan on what steps should be taken to learn this technology \neffectively as well as provide resources or courses that could help.\nFirstly, understanding AI is crucial for teachers because it can \nsignificantly enhance their ability to support students. \nTeachers need the knowledge and skills necessary not only to integrate these tools into teaching but also understand how they work so they are able to guide both themselves and their pupils in ethical use of technology.\n</thinking>\n\n<reflection>\nMy initial thought process was clear, emphasizing that teachers should \nbe equipped with AI literacy as it can improve educational outcomes. \nI outlined the importance for guiding students ethically while using \nthese tools effectively.\nHowever, my plan needs more detail on specific steps to learn about \ngenerative AI and resources available online or in-person courses.\n</reflection>**\n*MY PERSONAL NOTE: here it is starting an additional thinking after the first \nreflection... And this is really too good to be true!!*\n**<thinking>\nTo create a comprehensive learning path that teachers could follow:\n1. **Understand Basic Concepts**: Start with basic concepts of machine \n   learning (ML) such as supervised vs unsupervised, deep neural networks  \n   etc., which are foundational to understanding generative AI.\n2. **Learn About Generative Models**:\n   - Study different types: Variational Autoencoders (VAEs), GANs.\n   - Understand how these models generate new data that resembles the \n     training set but is not identical and can be used for tasks like \n     text generation, image synthesis etc., which are crucial in education.\n3. **Explore Applications**: Look at real-world applications of generative \n   AI such as language modeling (e.g., GPT-2), computer vision techniques to \n   create realistic images or videos from raw data.\n4. **Ethical Considerations and Implications for Education**:\n   - Discuss the ethical implications, including bias in models' outputs \n     which could affect students‚Äô perceptions about themselves if they are \n     exposed regularly through AI-generated content that is not representative \n     of them.\n5. **Hands-on Practice**: Engage with practical exercises using open-source \n   tools like TensorFlow or PyTorch to build simple generative model \n   applications.\n6. **Stay Updated and Network**:\n   - Follow the latest research in ML, especially on Generative Models \n     by following relevant journals (e.g., NeurIPS), conferences such as \n     ICML etc.\n7. Attend Workshops/Conferences: Participate actively at AI-related workshops \n   or events to network with professionals working within this field.\n</thinking>**\n*MY PERSONAL NOTE: and no surprise, an additional reflection now starts, \nbefore producing the final output*\n**<reflection>\nMy plan has been expanded and includes a more detailed approach for teachers, \ncovering foundational knowledge through practical application of generative \nmodels in education along with ethical considerations that are crucial. \nI have also included staying updated on the latest research as an important \npart.\nHowever, it's essential to ensure these resources or courses exist so \neducators can access them easily.\n</reflection>\n\n<output>\nIt is vital for teachers and educational professionals alike to learn how \ngenerative AI works because this technology has significant potential in \nenhancing teaching methods. By understanding basic concepts of machine \nlearning (ML) such as supervised vs unsupervised, deep neural networks etc., \nthey will be better equipped with the knowledge necessary not only to \nintegrate these tools into their curriculum but also guide students ethically.\nA comprehensive plan for teachers and educators includes:\n1. **Understand Basic Concepts**: Start by grasping foundational concepts of ML.\n2. **Learn About Generative Models**:\n   - Study different types: Variational Autoencoders (VAEs), GANs, etc., \n     which are crucial in generating new data that resembles the training \n     set but is not identical and can be used for tasks like text generation or \n     image synthesis.\n3. **Explore Applications**: Look at real-world applications of generative AI \n   such as language modeling using tools like TensorFlow.\n4. **Ethical Considerations**:\n   - Discuss ethical implications, including bias in models' outputs which \n     could affect students‚Äô perceptions about themselves if they are exposed \n     regularly through AI-generated content that is not representative.\n5. **Hands-on Practice**: Engage with practical exercises to build simple \n   generative model applications using open-source tools.\n6. **Stay Updated and Network**:\n   - Follow the latest research in ML, especially on Generative Models by \n     following relevant journals (e.g., NeurIPS), conferences such as ICML etc.\n7. Attend Workshops/Conferences: Participate actively at AI-related workshops \n   or events to network with professionals working within this field.\n</output>\n\n```\n\nSuch a refined thinking process, together with a clear open/closure of the  can be easily processed in further pipelines, or pretty printed in a nice GUI.\n\n## What it is bad at?\n\nThe worst task was the creative writing of a short story. The model started to repeat the same paragraph in the middle of the generation.\n\nThis does not mean that it cannot do it. More likely increasing the temperature and the repetition penalty as well, a good result can be achieved.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GNJ3lG6FM5qt9glIdf7qhQ.jpeg)\n\n## Conclusions\n\nYou can find all the chat history in my GitHub repo, together with the code and the instructions to do it yourself. Tou can use as a reference the tutorial from my previous article [Qwen2.5 1.5b: the future of Mobile AI?](https://ai.gopubby.com/qwen2-5-1-5b-the-future-of-mobile-ai-6bd5f29bbc84)\n\nIn the next articles I will cover other Small Language Models, using the same pricniple: from the tiny 350M paratmeters, to the small 500M series, up to the 3B ‚Äî passing throught the 1.5B.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/the-6-best-llm-tools-to-run-models-locally-eedd0f7c2bbd","frontmatter":{"title":"The 6 Best LLM Tools To Run Models Locally","meta_title":"The 6 Best LLM Tools To Run Models Locally","description":"Running large language models (LLMs) like ChatGPT and Claude usually involves sending data to servers managed by OpenAI and other AI model‚Ä¶","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*2MB6-INUUGLR0NR_iOACIg.jpeg","categories":["Technology","Programming","Health"],"author":"Rifx.Online","tags":["LLM","local","deployment","customization","telehealth"],"draft":false,"slug":"blog/the-6-best-llm-tools-to-run-models-locally-eedd0f7c2bbd"},"content":"\n\n\n\n\n\nRunning large language models (LLMs) like [ChatGPT](https://openai.com/chatgpt/mac/) and [Claude](https://claude.ai/) usually involves sending data to servers managed by [OpenAI](https://openai.com/) and other AI model providers. While these services are secure, some businesses prefer to keep their data entirely offline for greater privacy.\n\nThis article covers the top six tools developers can use to run and test LLMs locally, ensuring their data never leaves their devices, similar to how [end-to-end encryption](https://getstream.io/blog/end-to-end-encryption/) protects privacy.\n\n\n## Why Use Local LLMs?\n\nA tool like [LM Studio](https://lmstudio.ai/) does not collect user data or track users‚Äô actions when they use it to run local LLMs. It lets all your chat data stay on your local machine without sharing with an AI/ML server.\n\n* **Privacy**: You can prompt local LLMs in a multi-turn manner without your prompt data leaving your localhost.\n* **Customization Options**: Local LLMs provide advanced configurations for CPU threads, temperature, context length, GPU settings, and more. This is similar to OpenAI‚Äôs playground.\n* **Support and Security**: They provide similar support and security as OpenAI or Claude.\n* **Subscription and Cost**: These tools are free to use and they do not require monthly subscription. For cloud services like OpenAI, each API request requires payment. Local LLMs help to save money since there are no monthly subscriptions.\n* **Offline Support**: You can load and connect with large language models while offline.\n* **Connectivity**: Sometimes, connecting to a cloud service like OpenAI may result in poor signal and connection.\n\n\n## Top Six and Free Local LLM Tools\n\nDepending on your specific use case, there are several offline LLM applications you can choose. Some of these tools are completely free for personal and commercial use. Others may require sending them a request for business use. There are several local LLM tools available for Mac, Windows, and Linux. The following are the six best tools you can pick from.\n\n\n## 1. LM Studio\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*svbQPZKu08of7Kv6)\n\n[LM Studio](https://lmstudio.ai/) can run any model file with the format `gguf`. It supports `gguf` files from model providers such as [Llama 3.1](https://llama.meta.com/), [Phi 3](https://huggingface.co/docs/transformers/main/en/model_doc/phi3), [Mistral](https://mistral.ai/), and [Gemma](https://ai.google.dev/gemma). To use LM Studio, visit the link above and download the app for your machine. Once you launch LM Studio, the homepage presents top LLMs to download and test. There is also a search bar to filter and download specific models from different AI providers.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*sbS3VqiLgDsftgs2)\n\nSearching for a model from a specific company presents several models, ranging from small to large [quantization](https://huggingface.co/docs/optimum/en/concept_guides/quantization). Depending on your machine, LM Studio uses a compatibility guess to highlight the model that will work on that machine or platform.\n\n\n## Key Features of LM Studio\n\nLM Studio provides similar functionalities and features as ChatGPT. It has several functions. The following highlights the key features of LM Studio.\n\n* **Model Parameters Customization**: This allows you to adjust temperature, maximum tokens, frequency penalty, and more.\n* **Chat History**: Allows you to save prompts for later use.\n Parameters and UI Hinting: You can hover on info buttons to lookup model parameters and terms.\n* **Cross-platform**: LM Studio is available on Linux, Mac, and Windows operating systems.\n* **Machine Specification Check**: LM studio checks computer specifications like GPU and memory and reports on compatible models. This prevents downloading a model that might not work on a specific machine.\n* **AI Chat and Playground**: Chat with a large language model in a multi-turn chat format and experiment with multiple LLMs by loading them concurrently.\n* **Local Inference Server for Developers**: Allows developers to set up a local HTTP server similar to OpenAI‚Äôs API.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*9bHmRiOSf6gm-u3P)\n\nThe local server provides sample Curl and Python client requests. This feature helps to build an AI application using LM Studio to access a particular LLM.\n\n\n```python\n## Example: reuse your existing OpenAI setup\nfrom openai import OpenAI\n\n## Point to the local server\nclient = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n\ncompletion = client.chat.completions.create(\n  model=\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"Always answer in rhymes.\"},\n    {\"role\": \"user\", \"content\": \"Introduce yourself.\"}\n  ],\n  temperature=0.7,\n)\n\nprint(completion.choices[0].message)\n```\nWith the above sample Python code, you can reuse an existing OpenAI configuration and modify the base url to point to your localhost.\n\n* **OpenAI‚Äôs Python Library Import**: LM Studio allows developers to import the OpenAI Python library and point the base URL to a local server (localhost).\n* **Multi-model Session**: Use a single prompt and select multiple models to evaluate.\n\n\n## Benefits of Using LM Studio\n\nThis tool is free for personal use and it allows developers to run LLMs through an in-app chat UI and playground. It provides a gorgeous and easy to use interface with filters and supports connecting to OpenAI‚Äôs Python library without the need for an API key. Companies and businesses can use LM studio on request. However it requires a M1/M2/M3 Mac or higher, or a Windows PC with a processor that supports [AVX2](https://edc.intel.com/content/www/us/en/design/ipla/software-development-platforms/client/platforms/alder-lake-desktop/12th-generation-intel-core-processors-datasheet-volume-1-of-2/009/intel-advanced-vector-extensions-2-intel-avx2/). Intel and [AMD](https://www.amd.com/en/support/download/drivers.html) users are limited to using the [Vulkan inference engine in v0.2.31](https://lmstudio.ai/).\n\n\n## 2. Jan\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*7YeH_48iFYB4lDRu)\n\nThink of [Jan](https://jan.ai/) as an open-source version of ChatGPT designed to operate offline. It is built by a community of users with a user-owned philosophy. Jan allows you to run popular models like [Mistral](https://huggingface.co/models?other=mistral) or [Llama](https://huggingface.co/models?other=llama) on your device without connecting it to the internet. With Jan, you can access remote APIs like OpenAI and [Groq](https://groq.com/).\n\n\n## Key Features of Jan\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*ufyOE6QkcHw8X5U7)\n\nJan is an electron app with features similar to LM Studio. It makes AI open and accessible to all by turning consumer machines into AI computers. Since it is an open source project, developers can contribute to it and extend its functionalities. The following breaksdown the major features of Jan.\n\n* **Local**: You can run your preferred AI models on devices without connecting them to the internet.\n* **Ready to Use Models**: After downloading Jan, you get a set of already installed models to start. There is also a possibility to search for specific models.\n* **Model Import**: It supports importing models from sources like Hugging Face.\n* **Free, Cross-Platform and Open Source**: Jan is 100% free, open source, and works on Mac, Windows, and Linux.\n* **Customize Inference Parameters**: Adjust model parameters such as Maximum token, temperature, stream, frequency penalty, and more. All preferences, model usage, and settings stay locally on your computer.\n* **Extensions**: Jan supports extensions like [TensortRT](https://github.com/NVIDIA/TensorRT) and [Inference Nitro](https://huggingface.co/jan-hq/nitro-v1.2-e3) for customizing and enhancing your AI models.\n\n\n## Benefits of Using Jan\n\nJan provides a clean and simple interface to interact with LLMs and it keeps all your data and processing information locally. It has over seventy large language models already installed for you to use. The availability of these ready-to-use models makes it easy to connect and interact with remote APIs like OpenAI and Mistral. Jan also has a great [GitHub](https://github.com/janhq/jan), [Discord](https://discord.gg/FTk2MvZwJH), and [Hugging Face](https://huggingface.co/janhq) communities to follow and ask for help. However, like all the LLM tools, the models work faster on Apple Silicon Macs than on Intel ones.\n\n\n## 3. Llamafile\n\n[Llamafile](https://github.com/Mozilla-Ocho/llamafile) is backed by [Mozilla](https://www.mozilla.org/en-US/?v=1) whose aim is to support and make open source AI accessible to everyone using a fast [CPU inference](https://huggingface.co/docs/transformers/en/perf_infer_cpu) with no network access. It converts LLMs into multi-platform [Executable Linkable Format](https://gist.github.com/x0nu11byt3/bcb35c3de461e5fb66173071a2379779) (ELF). It provides one of the best options to [integrate AI](https://getstream.io/chat/solutions/ai-integration/) into applications by allowing you to run LLMs with just a single executable file.\n\n\n## How Llamafile Works\n\nIt is designed to convert weights into several executable programs that require no installation to run on architectures such as Windows, MacOS, Linux, Intel, ARM, FreeBSD, and more. Under the hood, Llamafile uses [tinyBLAST](https://github.com/ggerganov/llama.cpp/issues/5048) to run on OSs like Windows without requiring an SDK.\n\n\n## Key Features of Llamafile\n\n* **Executable File**: Unlike other LLM tools like LM Studio and Jan, Llamafile requires only one executable file to run LLMs.\n* **Use Existing Models**: Llamafile supports using existing models tools like Ollama and LM Studio.\n* **Access or Make Models**: You can access popular LLMs from OpenAI, Mistral, Groq, and more. It also provides support for creating models from scratch.\n* **Model File Conversion**: You can convert the file format of many popular LLMs, for example, `.gguf` into `.llamafile` with a single command.\n\n`llamafile-convert mistral-7b.gguf`\n\n\n## Get Started With Llamafile\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*4PV1KsCZvvVKqFll)\n\nTo install Llamafile, head to the Huggingface website, select **Models** from the navigation, and search for **Llamafile**. You can also install your preferred [quantized](https://huggingface.co/docs/optimum/en/concept_guides/quantization) version from the URL below.\n\n[`https://huggingface.co/Mozilla/Meta-Llama-3.1-8B-Instruct-llamafile/tree/m`ain](https://huggingface.co/Mozilla/Meta-Llama-3.1-8B-Instruct-llamafile/tree/main)\n\n**Note**: The larger the quantization number, the better the response. As highlighted in the image above, this article uses `Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile` where `Q6` represents the quantization number.\n\n**Step 1: Download Llamafile**\n\nFrom the link above, click any of the download buttons to get your preferred version. If you have the [wget](https://www.gnu.org/software/wget/) utility installed on your machine, you can download Llamafile with the command below.\n\n`wget <https://huggingface.co/Mozilla/Meta-Llama-3.1-8B-Instruct-llamafile/blob/main/Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile>`\n\nYou should replace the URL with the version you like.\n\n**Step 2: Make Llamafile Executable**\n\nAfter downloading a particular version of Llamafile, you should make it executable using the following command by navigating to the file‚Äôs location.\n\n`chmod +x Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile`**Step 3: Run Llamafile**\n\nPrepend a period and forward slash `./` to the file name to launch Llamafile.\n\n`./Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile`\n\nThe Llamafile app will now be available at `http://127.0.0.1:8080` to run your various LLMs.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*1xrwDPTfNgmEQDTx)\n\n\n## Benefits of Using Llamafile\n\nLlamafile helps to democratize AI and ML by making LLMs easily reachable to consumer CPUs. As compared to other local LLM apps like **Llama.cpp**, Llamafile gives the fastest prompt processing experience and better performance on gaming computers. Since it has a faster performance, it is an excellent option for summarizing long text and large documents. It runs 100% offline and privately, so users do not share their data to any AI server or API. Machine Learning communities like Hugging Face supports the Llamafile format, making it easy to search for Llamafile related models. It also has a great open source community that develops and extends it further.\n\n\n## 4. GPT4ALL\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*j3vNWWQZCVF5woo5)\n\nGPT4ALL is built upon privacy, security, and no internet-required principles. Users can [install](https://www.nomic.ai/gpt4all) it on Mac, Windows, and Ubuntu. Compared to Jan or LM Studio, GPT4ALL has more monthly downloads, [GitHub Stars](https://github.com/nomic-ai/gpt4all), and active users.\n\n\n## Key Features of GPT4ALL\n\nGPT4All can run LLMs on major consumer hardware such as Mac M-Series chips, AMD and NVIDIA GPUs. The following are its key features.\n\n* **Privacy First**: Keep private and sensitive chat information and prompts only on your machine.\n* **No Internet Required**: It works completely offline.\n* **Models Exploration**: This feature allows developers to browse and download different kinds of LLMs to experiment with. You can select about 1000 open-source language models from popular options like LLama, Mistral, and more.\n* **Local Documents**: You can let your local LLM access your sensitive data with local documents like `.pdf` and `.txt` without data leaving your device and without a network.\n* **Customization options**: It provides several [chatbot](https://getstream.io/blog/llm-chatbot-docs/) adjustment options like temperature, batch size, context length, etc.\n* **Enterprise Edition**: GPT4ALL provides an enterprise package with security, support, and per-device licenses to bring local AI to businesses.\n\n\n## Get Started With GPT4All\n\nTo start using GPT4All to run LLMs locally, [Download](https://www.nomic.ai/gpt4all) the required version for your operating system.\n\n\n## Benefits of Using GPT4ALL\n\nWith the exception of Ollama, GPT4ALL has the most significant number of GitHub contributors and about 250000 monthly active users (according to <https://www.nomic.ai/gpt4all>) and compared to its competitors. The app collects anonymous user data about usage analytics and chat sharing. However, users have the options to opt in or out. Using GPT4ALL, developers benefit from its large user base, GitHub, and Discord communities.\n\n\n## 5. Ollama\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*STAonWgWIsY6cgDR)\n\nUsing [Ollama](https://ollama.com/), you can easily create local chatbots without connecting to an API like OpenAI. Since everything runs locally, you do not need to pay for any subscription or API calls.\n\n\n## Key Features of Ollama\n\n* **Model Customization**: Ollama allows you to convert `.gguf` model files and run them with `ollama run modelname`.\n* **Model Library**: Ollama has a large collection of models to try at [ollama.com/library](https://ollama.com/library).\n* **Import Models**: Ollama supports importing models from [PyTorch](https://pytorch.org/).\n* **Community Integrations**: Ollama integrates seamlessly into web and desktop applications like, [Ollama-SwiftUI](https://github.com/kghandour/Ollama-SwiftUI), [HTML UI](https://github.com/rtcfirefly/ollama-ui), [Dify.ai](https://github.com/rtcfirefly/ollama-ui), and [more](https://github.com/ollama/ollama?tab=readme-ov-file#community-integrations).\n* **Database Connection**: Ollama supports several [data platforms](https://github.com/mindsdb/mindsdb/blob/main/mindsdb/integrations/handlers/ollama_handler/README.md).\n* **Mobile Integration**: A SwiftUI app like [Enchanted](https://github.com/AugustDev/enchanted) brings Ollama to iOS, macOS, and visionOS. [Maid](https://github.com/Mobile-Artificial-Intelligence/maid) is also a cross-platform Flutter app that interfaces with `.gguf`model files locally.\n\n\n## Get Started With Ollama\n\nTo use Ollama for the first time, visit <https://ollama.com> and download the version for your machine. You can install it on Mac, Linux, or Windows. Once you install Ollama, you can check its detailed information in Terminal with the following command.\n\n`ollama`\n\nTo run a particular LLM, you should download it with:\n\n`ollama pull modelname`, where `modelname` is the name of the model you want to install. Checkout Ollama on [GitHub](https://github.com/ollama/ollama) for some example models to download. The `pull` command is also used for updating a model. Once it is used, only the difference will be fetched.\n\nAfter downloading for example, `llama3.1`, running `ollama run llama3.1` in the command line launches the model.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*aglZm6h0BU6GAYkSl04XWA.gif)\n\nIn the above example, we prompt the `llama3.1` model to solve a Physics work and energy question.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*dNNQYpz1s2tz1pcn)\n\n\n## Benefits of Using Ollama\n\nOllama has over 200 contributors on GitHub with active updates. It has the largest number of contributors and is more extendable among the other open-source LLM tools discussed above.\n\n\n## 6. LLaMa.cpp\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*KhsAUquhDZAHghxK)\n\n[LLaMa.cpp](https://github.com/ggerganov/llama.cpp) is the underlying backend technology (inference engine) that powers local LLM tools like Ollama and many others. Llama.cpp supports significant large language model inferences with minimal configuration and excellent local performance on various hardware. It can also run in the cloud.\n\n\n## Key Features of LLaMa.cpp\n\n* **Setup**: It has a minimal setup. You install it with a single command.\n* **Performance**: It performs very well on various hardware locally and in the cloud.\n* **Supported Models**: It supports popular and major LLMs like [Mistral 7B](https://huggingface.co/mistralai/Mistral-7B-v0.1), [Mixtral MoE](https://huggingface.co/models?search=mistral-ai/Mixtral), [DBRX](https://huggingface.co/databricks/dbrx-instruct), [Falcon](https://huggingface.co/models?search=tiiuae/falcon), and [many others](https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#description).\n* **Frontend AI Tools**: LLaMa.cpp supports open-source LLM UI tools like [MindWorkAI/AI-Studio](https://github.com/MindWorkAI/AI-Studio) (FSL-1.1-MIT), [iohub/collama](https://github.com/iohub/coLLaMA), etc.\n\n\n## Get Started With LLaMa.cpp\n\nTo run your first local large language model with llama.cpp, you should install it with:\n\n`brew install llama.cpp`\n\nNext, download the model you want to run from Hugging Face or any other source. For example, download the model below from Hugging Face and save it somewhere on your machine.\n\n[`https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q4_K_M.g`guf](https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf)\n\nUsing your preferred command line tool like Terminal, `cd` into the location of the `.gguf` model file you just downloaded and run the following commands.\n\n\n```python\nllama-cli --color \\ \n-m Mistral-7B-Instruct-v0.3.Q4_K_M.ggufb \\ \n-p \"Write a short intro about SwiftUI\"\n```\nIn summary, you first invoke the LLaMa CLI tool and set color and other flags. The `-m` flag specifies the path of the model you want to use. The `-p` flag specifies the prompt you wish to use to instruct the model.\n\nAfter running the above command, you will see the result in the following preview.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*4Al-j50vXUXLUfvxzBt6aw.gif)\n\n\n## Local LLMs Use Cases\n\nRunning LLMs locally can help developers who want to understand their performance and how they work in detail. Local LLMs can query private documents and technical papers so that information about these documents does not leave the devices used to query them to any cloud AI APIs. Local LLMs are useful in no-internet locations and places where network reception is poor.\n\nIn a [telehealth setting](https://getstream.io/blog/telemedicine-app-development/), local LLMs can sort patient documents without having to upload them to any AI API provider due to privacy concerns.\n\n\n## Evaluating LLMs‚Äô Performance To Run Locally\n\nKnowing the performance of a large language model before using it locally is essential for getting the required responses. There are several ways you can determine the performance of a particular LLM. Here are a few ways.\n\n* **Training**: What dataset is the model trained on?\n* **Fine-tuning**: To what extent can the model be customized to perform a specialized task or can it be fine-tuned to for a specific domain?.\n* **Academic Research**: Does the LLM have an academic research paper?\n\nTo answer the above questions, you can check excellent resources like [Hugging Face](https://huggingface.co/datasets) and [Arxiv.org](https://arxiv.org/). Also, [Open LLm Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) and [LMSYS Chatbot Arena](https://chat.lmsys.org/?arena) provide detailed information and benchmarks for varieties of LLMs.\n\n\n## Local LLM Tools Conclusion\n\nAs discussed in this article, several motives exist for choosing and using large language models locally. You can fine-tune a model to perform a specialized task in a [telemedicine app](https://getstream.io/chat/solutions/healthcare/) if you do not wish to send your dataset over the internet to an AI API provider. Many open-source Graphic User Interface (GUI-based) local LLM tools like LLm Studio and Jan provide intuitive front-end UIs for configuring and experimenting with LLMs without subscription-based services like OpenAI or Claude. You also discovered the various powerful command-line LLM applications like Ollama and LLaMa.cpp that help you run and test models locally and without an internet connection. Check out Stream‚Äôs [AI Chatbot](https://getstream.io/chat/solutions/ai-integration/) solution to integrate an AI chat into your app and visit all the related links to learn more.\n\n*Originally published at [https://getstream.io](https://getstream.io/blog/best-local-llm-tools/).*\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/a-new-risings-red-star-qwen2-5-is-here-0dffe0fb09ad","frontmatter":{"title":"Êñ∞Â¥õËµ∑Á∫¢ÊòüÔºöQwen2.5Êù•‰∫Ü","meta_title":"Êñ∞Â¥õËµ∑Á∫¢ÊòüÔºöQwen2.5Êù•‰∫Ü","description":"‰∏ÄËµ∑Áî®pythonÂíållama-cppÊµãËØï‰∏Ä‰∏ãÈòøÈáå‰∫ëÊñ∞ÁîüÁöÑÁîüÊàêÂºèAI Qwen2.5","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*zU-XtqK2oMLkvscgxavjdw.png","categories":["Programming","Technology","Education"],"author":"Rifx.Online","tags":["Qwen2.5","multimodal","instruction-following","text-generation","multilingual"],"draft":false,"slug":"blog/a-new-risings-red-star-qwen2-5-is-here-0dffe0fb09ad"},"content":"\n\n\n### ‰∏ÄËµ∑ÊµãËØïÊñ∞ÁîüÁöÑÈòøÈáå‰∫ëÁîüÊàêÂºèAI Qwen2.5Ôºå‰ΩøÁî®PythonÂíållama-cpp\n\n\n\nÂú®Ê≤°ÊúâÂ§™Â§öÂÆ£‰º†ÂíåÈ¢ÑÊúüÂÖ¨ÂëäÁöÑÊÉÖÂÜµ‰∏ãÔºåÈòøÈáå‰∫ë‰∫é9Êúà19Êó•ÂèëÂ∏É‰∫Ü‰ªñ‰ª¨ÁöÑÊóóËà∞Ê®°ÂûãÁ≥ªÂàóQwen2.5„ÄÇ\n\nÈòøÈáå‰∫ëÂú®Qwen‰∏äÁöÑÈù©ÂëΩÊÄßÊóÖÁ®ãÂÜçÊ¨°Â±ïÁ§∫‰∫ÜÈÄöËøáÂàõÊñ∞ÁöÑÂº∫Â§ßÈ¢ÜÂØºÂäõ„ÄÇ\n\nÊÄé‰πàÂÅöÁöÑÔºüÂÆÉ‰ª¨Êúâ‰ªÄ‰πàÁâπÂà´‰πãÂ§ÑÔºüÊàë‰ª¨Â∫îËØ•ÊúüÂæÖ‰ªÄ‰πàÔºü\n\nÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨Â∞ÜÊé¢ËÆ®Êñ∞Ê®°ÂûãÂπ∂Ê£ÄÊü•ÂÖ∂ÊÄßËÉΩ„ÄÇ‰Ωú‰∏∫ÂêéÁª≠ÔºåÂú®‰∏ã‰∏ÄÁØáÊñáÁ´†‰∏≠ÔºåÊàë‰ª¨Â∞Ü‰ΩøÁî®`llama-cpp-python`ÂíåÈáèÂåñÁâàÊú¨ÁöÑqwen2.5‚Äì1.5b-instructÔºåÂØπÊ®°ÂûãËøõË°å13È°πNLP‰ªªÂä°ÁöÑÊµãËØï„ÄÇ\n\n‰∫ãÂÆû‰∏äÔºåÊàëÁõ∏‰ø°Êàë‰ª¨ÊòØÊúÄ‰Ω≥ÁöÑÂü∫ÂáÜÂ∑•ÂÖ∑ÔºåËÉΩÂ§üÂÖ®Èù¢ËØÑ‰º∞‰∏Ä‰∏™Ê®°ÂûãÊòØÂê¶ÈÄÇÂêàÊàë‰ª¨ÔºÅ\n\nÁé∞Âú®ÔºåÊàë‰ª¨Â∞ÜË¶ÜÁõñ‰ª•‰∏ãÂÜÖÂÆπÔºö\n\n\n```python\n- Qwen2.5 family innovation\n- Declared scope, use cases and models\n- Qwen2.5: a party of Foundation models\n- Expanding Reach through Open-Source Contributions\n- Bridging Industries through cutting-edge AI solutions\n- 13 Tasks to prove it worth \n- Future outlook: continued Open-Sourcing\n```\nËÆ©Êàë‰ª¨Ê∑±ÂÖ•‰∫ÜËß£ÔºÅ \n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*OeQ5qeOzCdl8LPJOZZgTIw.png)\n\n## Qwen2.5ÂÆ∂ÊóèÂàõÊñ∞\n\nQwenÊòØÈòøÈáåÂ∑¥Â∑¥ÈõÜÂõ¢QwenÂõ¢ÈòüÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂíåÂ§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÁ≥ªÂàó„ÄÇÂ∞±Âú®Êò®Â§©ÔºåÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂ∑≤ÂçáÁ∫ß‰∏∫Qwen2.5„ÄÇ\n\nËØ≠Ë®ÄÊ®°ÂûãÂíåÂ§öÊ®°ÊÄÅÊ®°ÂûãÂùáÂú®Â§ßËßÑÊ®°Â§öËØ≠Ë®ÄÂíåÂ§öÊ®°ÊÄÅÊï∞ÊçÆ‰∏äËøõË°åÈ¢ÑËÆ≠ÁªÉÔºåÂπ∂Âú®È´òË¥®ÈáèÊï∞ÊçÆ‰∏äËøõË°åÂêéËÆ≠ÁªÉÔºå‰ª•‰∏é‰∫∫Á±ªÂÅèÂ•ΩÂØπÈΩê„ÄÇQwenËÉΩÂ§üËøõË°åËá™ÁÑ∂ËØ≠Ë®ÄÁêÜËß£„ÄÅÊñáÊú¨ÁîüÊàê„ÄÅËßÜËßâÁêÜËß£„ÄÅÈü≥È¢ëÁêÜËß£„ÄÅÂ∑•ÂÖ∑‰ΩøÁî®„ÄÅËßíËâ≤ÊâÆÊºî„ÄÅ‰Ωú‰∏∫AI‰ª£ÁêÜÁ≠â„ÄÇ\n\nÈöèÁùÄQwen2.5ÁöÑÂèëÂ∏É‰ª•ÂèäÈ¢ùÂ§ñÂºÄÊ∫êÊ®°ÂûãÁöÑÂèëÂ∏ÉÔºåÈòøÈáå‰∫ëÁªßÁª≠‰øùÊåÅÂÖ∂È¢ÜÂØºÂú∞‰ΩçÔºå‰ª•Êª°Ë∂≥‰ºÅ‰∏öÁî®Êà∑Êó•ÁõäÂ¢ûÈïøÁöÑAIÈúÄÊ±Ç„ÄÇËá™ÂéªÂπ¥ÂÖ≠Êúà‰ª•Êù•ÔºåQwenÂÆ∂ÊóèÈÄöËøáModel StudioÂú®Ê∂àË¥πÁîµÂ≠ê„ÄÅÊ±ΩËΩ¶„ÄÅÊ∏∏ÊàèÁ≠âÂ§ö‰∏™Ë°å‰∏öÂê∏Âºï‰∫ÜË∂ÖËøá90,000‰∏™ÈÉ®ÁΩ≤„ÄÇ\n\nQwenËøòÈÄöËøáÂú®Hugging FaceÁ≠âÂπ≥Âè∞‰∏äÊé®Âá∫Êñ∞Ê®°ÂûãÔºåÂ¶ÇQwen1.5‚Äì110BÂíåCodeQwen1.5‚Äì7BÔºåÊâ©Â§ß‰∫ÜÂÖ∂ÂΩ±ÂìçÂäõÔºåÂ±ïÁ§∫‰∫ÜÈòøÈáåÂ∑¥Â∑¥ÂØπÂºÄÊ∫êAIÂºÄÂèëÁöÑÊâøËØ∫„ÄÇ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*A4pEOgsLK2PAFtiaGQx1Qw.png)\n\n## Â£∞ÊòéÁöÑËåÉÂõ¥„ÄÅÁî®‰æãÂíåÊ®°Âûã\n\nÂú® Qwen2 ÂèëÂ∏ÉÁöÑËøáÂéª‰∏â‰∏™ÊúàÈáåÔºå‰ºóÂ§öÂºÄÂèëËÄÖÂú® Qwen2 ËØ≠Ë®ÄÊ®°ÂûãÂü∫Á°Ä‰∏äÊûÑÂª∫‰∫ÜÊñ∞ÁöÑÊ®°ÂûãÔºå‰∏∫Êï¥‰∏™Á§æÂå∫‰ª•ÂèäÈòøÈáå‰∫ëÊèê‰æõ‰∫ÜÂÆùË¥µÁöÑÂèçÈ¶à„ÄÇ\n\n> Âú®Ê≠§ÊúüÈó¥ÔºåÊàë‰ª¨‰∏ìÊ≥®‰∫éÂàõÂª∫Êõ¥Êô∫ËÉΩ„ÄÅÊõ¥Áü•ËØÜ‰∏∞ÂØåÁöÑËØ≠Ë®ÄÊ®°Âûã„ÄÇ‰ªäÂ§©ÔºåÊàë‰ª¨ÂæàÈ´òÂÖ¥Âú∞‰ªãÁªç Qwen ÂÆ∂ÊóèÁöÑÊúÄÊñ∞ÊàêÂëòÔºöQwen2.5„ÄÇ\n\n‰ªñ‰ª¨ÁöÑÂ£∞Êòé‰º¥ÈöèÁùÄÊúâÂÖ≥Êñ∞Ê®°ÂûãÂÆ∂ÊóèÁöÑ‰∫ãÂÆûÔºö\n\n* ÂØÜÈõÜÂûã„ÄÅÊòì‰∫é‰ΩøÁî®ÁöÑ‰ªÖËß£Á†ÅÂô®ËØ≠Ë®ÄÊ®°ÂûãÔºåÊèê‰æõ 0.5B„ÄÅ1.5B„ÄÅ3B„ÄÅ7B„ÄÅ14B„ÄÅ32B Âíå 72B Â∞∫ÂØ∏Ôºå‰ª•ÂèäÂü∫Á°ÄÂíåÊåá‰ª§Âèò‰Ωì„ÄÇ\n* Âú®Êàë‰ª¨ÊúÄÊñ∞ÁöÑÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜ‰∏äËøõË°åÈ¢ÑËÆ≠ÁªÉÔºåÊ∂µÁõñÂ§öËææ 18T ÁöÑÊ†áËÆ∞„ÄÇ\n* Âú®ÈÅµÂæ™Êåá‰ª§„ÄÅÁîüÊàêÈïøÊñáÊú¨ÔºàË∂ÖËøá 8K Ê†áËÆ∞Ôºâ„ÄÅÁêÜËß£ÁªìÊûÑÂåñÊï∞ÊçÆÔºà‰æãÂ¶ÇÔºåË°®Ê†ºÔºâ‰ª•ÂèäÁîüÊàêÁªìÊûÑÂåñËæìÂá∫ÔºàÂ∞§ÂÖ∂ÊòØ JSONÔºâÊñπÈù¢ÊúâÊòæËëóÊîπËøõ„ÄÇ\n* ÂØπÁ≥ªÁªüÊèêÁ§∫ÁöÑÂ§öÊ†∑ÊÄßÊõ¥ÂÖ∑ÈüßÊÄßÔºåÂ¢ûÂº∫‰∫ÜËßíËâ≤ÊâÆÊºîÂÆûÁé∞ÂíåËÅäÂ§©Êú∫Âô®‰∫∫ÁöÑÊù°‰ª∂ËÆæÁΩÆ„ÄÇ\n* ÊîØÊåÅÁöÑ‰∏ä‰∏ãÊñáÈïøÂ∫¶ÂèØËææ 128K Ê†áËÆ∞ÔºåÂπ∂‰∏îÂèØ‰ª•ÁîüÊàêÂ§öËææ 8K Ê†áËÆ∞„ÄÇ\n* ÊîØÊåÅË∂ÖËøá 29 ÁßçËØ≠Ë®ÄÁöÑÂ§öËØ≠Ë®ÄÂäüËÉΩÔºåÂåÖÊã¨‰∏≠Êñá„ÄÅËã±Êñá„ÄÅÊ≥ïÊñá„ÄÅË•øÁè≠ÁâôÊñá„ÄÅËë°ËêÑÁâôÊñá„ÄÅÂæ∑Êñá„ÄÅÊÑèÂ§ßÂà©Êñá„ÄÅ‰øÑÊñá„ÄÅÊó•Êñá„ÄÅÈü©Êñá„ÄÅË∂äÂçóÊñá„ÄÅÊ≥∞Êñá„ÄÅÈòøÊãâ‰ºØÊñáÁ≠â„ÄÇ\n\n## Qwen2.5ÔºöÂü∫Á°ÄÊ®°ÂûãÁöÑËÅö‰ºö\n\nÊ†πÊçÆ2024Âπ¥9Êúà19Êó•ÁöÑ[ÂÆòÊñπÂçöÂÆ¢Êñ∞ÈóªÁ®ø](https://qwenlm.github.io/blog/qwen2.5/)ÁöÑÂÖ¨ÂëäÔºö\n\n> ‰ªäÂ§©ÔºåÊàë‰ª¨ÂæàÈ´òÂÖ¥Âú∞‰ªãÁªçQwenÂÆ∂ÊóèÁöÑÊúÄÊñ∞ÊàêÂëòÔºö**Qwen2.5**„ÄÇÊàë‰ª¨ÂÆ£Â∏ÉËøôÂèØËÉΩÊòØÂéÜÂè≤‰∏äÊúÄÂ§ßÁöÑÂºÄÊ∫êÂèëÂ∏ÉÔºÅËÆ©Êàë‰ª¨ÂºÄÂßãÂ∫ÜÁ•ùÂêßÔºÅ\n\n> Êàë‰ª¨ÊúÄÊñ∞ÁöÑÂèëÂ∏ÉÂåÖÂê´‰∫ÜLLMs **Qwen2.5**Ôºå‰ª•ÂèäÁî®‰∫éÁºñÁ†ÅÁöÑ‰∏ìÁî®Ê®°Âûã**Qwen2.5-Coder**ÂíåÁî®‰∫éÊï∞Â≠¶ÁöÑÊ®°Âûã**Qwen2.5-Math**„ÄÇ\n\n‰∏∫‰∫ÜÂ±ïÁ§∫Qwen2.5ÁöÑËÉΩÂäõÔºåÈòøÈáå‰∫ëÂõ¢ÈòüÂØπÂÖ∂ÊúÄÂ§ßÁöÑÂºÄÊ∫êÊ®°Âûã**Qwen2.5‚Äì72B**‚Äî‚Äî‰∏Ä‰∏™72BÂèÇÊï∞ÁöÑÁ®†ÂØÜËß£Á†ÅÂô®ËØ≠Ë®ÄÊ®°Âûã‚Äî‚Äî‰∏éÈ¢ÜÂÖàÁöÑÂºÄÊ∫êÊ®°ÂûãÂ¶ÇLlama-3.1‚Äì70BÂíåMistral-Large-V2ËøõË°å‰∫ÜÂü∫ÂáÜÊµãËØï„ÄÇ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*-MMFgkkWHa307jNo.jpg)\n\nÊâÄÊúâÂºÄÊîæÊùÉÈáçÊ®°ÂûãÈÉΩÊòØÁ®†ÂØÜÁöÑËß£Á†ÅÂô®ËØ≠Ë®ÄÊ®°ÂûãÔºåÊèê‰æõÂ§öÁßçÂ∞∫ÂØ∏ÔºåÂåÖÊã¨Ôºö\n\n* Qwen2.5Ôºö0.5B„ÄÅ1.5B„ÄÅ3B„ÄÅ7B„ÄÅ14B„ÄÅ32BÂíå72B\n* Qwen2.5-CoderÔºö1.5B„ÄÅ7BÂíå32BÊ≠£Âú®ÂèëÂ∏É‰∏≠\n* Qwen2.5-MathÔºö1.5B„ÄÅ7BÂíå72B„ÄÇ\n\nÈô§‰∫Ü3BÂíå72BÂèò‰ΩìÂ§ñÔºåÊâÄÊúâËøô‰∫õÂºÄÊ∫êÊ®°ÂûãÂùáÂú®Apache 2.0ËÆ∏ÂèØËØÅ‰∏ãÂèëÂ∏É„ÄÇÊÇ®ÂèØ‰ª•Âú®ÂêÑËá™ÁöÑHugging FaceÂ∫ì‰∏≠ÊâæÂà∞ËÆ∏ÂèØËØÅÊñá‰ª∂„ÄÇ\n\n> Èô§‰∫ÜËøô‰∫õÊ®°ÂûãÂ§ñÔºåÊàë‰ª¨ËøòÈÄöËøáModel StudioÊèê‰æõÊóóËà∞ËØ≠Ë®ÄÊ®°ÂûãÁöÑAPIÔºö**Qwen-Plus**Âíå**Qwen-Turbo**ÔºåÊàë‰ª¨ÈºìÂä±ÊÇ®ËøõË°åÊé¢Á¥¢ÔºÅ\n\n‰ΩÜËøôËøò‰∏çÊòØÂÖ®ÈÉ®ÔºÅ\n\n> ‚Ä¶Êàë‰ª¨ËøòÂºÄÊ∫ê‰∫Ü**Qwen2-VL-72B**Ôºå‰∏é‰∏ä‰∏™ÊúàÁöÑÂèëÂ∏ÉÁõ∏ÊØîÔºåÂÖ∑ÊúâÊÄßËÉΩÊèêÂçá„ÄÇ\n\nÂú®**Qwen2.5**ÊñπÈù¢ÔºåÊâÄÊúâËØ≠Ë®ÄÊ®°ÂûãÂùáÂú®Êàë‰ª¨ÊúÄÊñ∞ÁöÑÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜÈ¢ÑËÆ≠ÁªÉÔºåÊ∂µÁõñ‰∫ÜÂ§öËææ**18‰∏á‰∫ø**‰∏™Ê†áËÆ∞„ÄÇ‰∏éQwen2Áõ∏ÊØîÔºåQwen2.5Ëé∑Âæó‰∫ÜÊòæËëóÊõ¥Â§öÁöÑÁü•ËØÜÔºàMMLUÔºö85+ÔºâÔºåÂπ∂Âú®ÁºñÁ†ÅÔºàHumanEval 85+ÔºâÂíåÊï∞Â≠¶ÔºàMATH 80+ÔºâÊñπÈù¢Â§ßÂ§ßÊèêÂçá‰∫ÜËÉΩÂäõ„ÄÇÊ≠§Â§ñÔºåÊñ∞Ê®°ÂûãÂú®Êåá‰ª§Ë∑üÈöè„ÄÅÁîüÊàêÈïøÊñáÊú¨ÔºàË∂ÖËøá8K‰∏™Ê†áËÆ∞Ôºâ„ÄÅÁêÜËß£ÁªìÊûÑÂåñÊï∞ÊçÆÔºà‰æãÂ¶ÇÔºåË°®Ê†ºÔºâÂíåÁîüÊàêÁªìÊûÑÂåñËæìÂá∫ÔºåÂ∞§ÂÖ∂ÊòØJSONÊñπÈù¢ÂèñÂæó‰∫ÜÊòæËëóÊîπÂñÑ„ÄÇ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*7c7CIbl-WVjazUeE.jpeg)\n\nQwen2.5Ê®°ÂûãÈÄöÂ∏∏ÂØπÁ≥ªÁªüÊèêÁ§∫ÁöÑÂ§öÊ†∑ÊÄßÊõ¥ÂÖ∑ÈüßÊÄßÔºåÂ¢ûÂº∫‰∫ÜËßíËâ≤ÊâÆÊºîÁöÑÂÆûÊñΩÂíåËÅäÂ§©Êú∫Âô®‰∫∫ÁöÑÊù°‰ª∂ËÆæÁΩÆ„ÄÇ\n\n‰∏éQwen2‰∏ÄÊ†∑ÔºåQwen2.5ËØ≠Ë®ÄÊ®°ÂûãÊîØÊåÅÂ§öËææ**128K**‰∏™Ê†áËÆ∞ÔºåÂπ∂ÂèØ‰ª•ÁîüÊàêÂ§öËææ**8K**‰∏™Ê†áËÆ∞„ÄÇÂÆÉ‰ª¨ËøòÊîØÊåÅË∂ÖËøá**29**ÁßçËØ≠Ë®ÄÁöÑÂ§öËØ≠Ë®ÄÊîØÊåÅÔºåÂåÖÊã¨‰∏≠Êñá„ÄÅËã±Êñá„ÄÅÊ≥ïÊñá„ÄÅË•øÁè≠ÁâôÊñá„ÄÅËë°ËêÑÁâôÊñá„ÄÅÂæ∑Êñá„ÄÅÊÑèÂ§ßÂà©Êñá„ÄÅ‰øÑÊñá„ÄÅÊó•Êñá„ÄÅÈü©Êñá„ÄÅË∂äÂçóÊñá„ÄÅÊ≥∞Êñá„ÄÅÈòøÊãâ‰ºØÊñáÁ≠â„ÄÇ\n\n### Qwen-Coder ÊòØÂÆ∂Êóè‰∏≠ÁöÑÊñ∞ÊàêÂëò\n\n‰∏ì‰∏öÁöÑ‰∏ìÂÆ∂ËØ≠Ë®ÄÊ®°ÂûãÔºåÂç≥ **Qwen2.5-Coder** Áî®‰∫éÁºñÁ†ÅÔºå**Qwen2.5-Math** Áî®‰∫éÊï∞Â≠¶ÔºåÁõ∏ËæÉ‰∫éÂÆÉ‰ª¨ÁöÑÂâçË∫´ CodeQwen1.5 Âíå Qwen2-MathÔºåËøõË°å‰∫ÜÂÆûË¥®ÊÄßÁöÑÂ¢ûÂº∫„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåQwen2.5-Coder Â∑≤Âú® **5.5 ‰∏á‰∫ø** ‰∏™‰∏é‰ª£Á†ÅÁõ∏ÂÖ≥ÁöÑÊï∞ÊçÆ‰∏äËøõË°å‰∫ÜËÆ≠ÁªÉÔºå‰ΩøÂæóÂç≥‰ΩøÊòØËæÉÂ∞èÁöÑÁºñÁ†Å‰∏ìÁî®Ê®°Âûã‰πüËÉΩÂ§üÂú®ÁºñÁ†ÅËØÑ‰º∞Âü∫ÂáÜ‰∏ä‰∏éÊõ¥Â§ßÁöÑËØ≠Ë®ÄÊ®°ÂûãÊèê‰æõÁ´û‰∫âÂäõÁöÑË°®Áé∞„ÄÇÂêåÊó∂ÔºåQwen2.5-Math ÊîØÊåÅ **‰∏≠Êñá** Âíå **Ëã±Êñá**ÔºåÂπ∂ÁªìÂêà‰∫ÜÂ§öÁßçÊé®ÁêÜÊñπÊ≥ïÔºåÂåÖÊã¨ÊÄùÁª¥ÈìæÔºàCoTÔºâ„ÄÅÊÄùÁª¥Á®ãÂ∫èÔºàPoTÔºâÂíåÂ∑•ÂÖ∑ÈõÜÊàêÊé®ÁêÜÔºàTIRÔºâ„ÄÇ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Nvk4wrcB0SB4Tt-xbCzO6g.png)\n\n## ÈÄöËøáÂºÄÊ∫êË¥°ÁåÆÊâ©Â§ßÂΩ±ÂìçÂäõ\n\n‰Ωú‰∏∫ÊåÅÁª≠Ëá¥Âäõ‰∫éÊõ¥ÂπøÊ≥õÁ§æÂå∫ÁöÑ‰∏ÄÈÉ®ÂàÜÔºåÈòøÈáå‰∫ëÂú®ÂèëÂ∏ÉÂêÑÁßçËßÑÊ®°ÂíåÂèò‰ΩìÁöÑQwenÊ®°ÂûãÊñπÈù¢ËøàÂá∫‰∫ÜËøõ‰∏ÄÊ≠•ÁöÑÊ≠•‰ºê„ÄÇËøôÂåÖÊã¨Ôºö\n\n1. **Qwen 0.5‰∫øÂèÇÊï∞**ÔºåÈÄÇÁî®‰∫éÊõ¥‰º†ÁªüÂ∫îÁî®ÁöÑÂü∫Á°ÄÁâàÊú¨„ÄÇ2. ‰∏ÄÊ¨æÁ¥ßÂáë‰ΩÜÂº∫Â§ßÁöÑÊ®°ÂûãÔºå‰∏ìÈó®‰∏∫Ê∏∏ÊàèÂºÄÂèëÈáèË∫´ÂÆöÂà∂Ôºö**Qwen-VLÔºàËßÜËßâ-ËØ≠Ë®ÄÔºâ**Ôºå‰ºòÂåñ‰∫ÜÈ´òÊÄßËÉΩ„ÄÇ\n\nËøô‰∫õËøõÂ±ïÂ±ïÁ§∫‰∫ÜÈòøÈáåÂØπÂºÄÊ∫êAIÁöÑÊâøËØ∫Ôºå‰∏ç‰ªÖÂàÜ‰∫´‰∫ÜQwenÁöÑÂü∫Á°ÄÁâàÊú¨ÔºåËøòÊé®Âá∫‰∫ÜÊòæËëóÊîπËøõÂíåÊñ∞Ê®°ÂûãÔºåÁõ¥Êé•ÈíàÂØπ‰ºÅ‰∏öÈúÄÊ±ÇÔºåÂêåÊó∂Â¢ûÂº∫ÂÖ∂Âø´ÈÄüÂàõÊñ∞ÁöÑËÉΩÂäõ„ÄÇ\n\nËøô‰∏é‰∏Ä‰∏™ÊàòÁï•ÊÑøÊôØÂØÜÂàáÁõ∏ÂÖ≥ÔºåÂç≥ÊåÅÁª≠Ë¥°ÁåÆÊÉ†ÂèäÁ§æÂå∫ÊàêÂëòÂíåËá™Ë∫´ÂÆ¢Êà∑ÔºåÂ∏ÆÂä©‰ªñ‰ª¨Âú®Â§ö‰∏™Ë°å‰∏öÂØªÊ±ÇÂàõÊñ∞Â∫îÁî®„ÄÇ\n\n### ÈÄöËøáÂâçÊ≤øÁöÑ‰∫∫Â∑•Êô∫ËÉΩËß£ÂÜ≥ÊñπÊ°àËøûÊé•ÂêÑË°å‰∏ö\n\n‰∏∫‰∫ÜÂ±ïÁ§∫QwenÂú®Áé∞ÂÆûÂú∫ÊôØ‰∏≠ÁöÑÂπøÊ≥õËÉΩÂäõÔºåÈòøÈáå‰∫ë‰∏ÄÁõ¥Â§Ñ‰∫éÂâçÊ≤øÔºö\n\n1. **Â∞èÁ±≥**ÔºöËØ•ÂÖ¨Âè∏Ê≠£Âú®Â∞ÜÈòøÈáåÁöÑÊ®°ÂûãÈõÜÊàêÂà∞‰ªñ‰ª¨ÁöÑAIÂä©ÊâãÂ∞èÁà±‰∏≠ÔºåÂπ∂Âú®Â∞èÁ±≥Êô∫ËÉΩÊâãÊú∫ÂíåÁîµÂä®Ê±ΩËΩ¶‰∏≠ÈÉ®ÁΩ≤Ôºå‰ª•ÈÄöËøáËØ≠Èü≥ÂëΩ‰ª§ÁîüÊàêËΩ¶ËΩΩÂ®±‰πêÂõæÂÉèÁ≠âÂ¢ûÂº∫ÂäüËÉΩ„ÄÇ\n\n2. **ÂÆåÁæé‰∏ñÁïåÊ∏∏Êàè**ÔºöQwenÂú®Ê∏∏ÊàèÂºÄÂèë‰∏≠ÁöÑÈõÜÊàêÂØºËá¥‰∫ÜÂàõÊñ∞Â∫îÁî®ÔºåÂåÖÊã¨ÈÄöËøáÂØπËØùÂä®ÊÄÅÊîπÂñÑÊÉÖËäÇËß£ÊûêÂíåÂÆûÊó∂ÂÜÖÂÆπÁÆ°ÁêÜ„ÄÇ\n\nÈòøÈáå‰∫ëÊ®°Âûã‰∏éÂêÑË°å‰∏ö‰πãÈó¥ÁöÑÂêà‰Ωú‰∏ç‰ªÖ‰∏∞ÂØå‰∫ÜÁî®Êà∑‰ΩìÈ™åÔºåËøò‰øÉËøõ‰∫ÜËøô‰∫õË°å‰∏öÂÜÖÊõ¥Â§ßÁöÑÂ¢ûÈïøÊú∫‰ºöÔºåÊé®Âä®‰∫ÜÊ≤°Êúâ‰∫∫Â∑•Êô∫ËÉΩËøõÊ≠•ÁöÑÊÉÖÂÜµ‰∏ãÊó†Ê≥ïÊÉ≥Ë±°ÁöÑËæπÁïå„ÄÇ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*ku8o3rq6PHDE8xcc.png)\n\n## 13 ‰∏™ËØÅÊòéÂÖ∂‰ª∑ÂÄºÁöÑ‰ªªÂä°\n\n1.5 ‰∫øÂèÇÊï∞ÁöÑÊ®°ÂûãÂèØËÉΩÊòØËÄÉËôëÂà∞Â§çÊùÇÊÄß„ÄÅÊèêÁ§∫ÁêÜËß£ÂíåÊé®ÁêÜÈÄüÂ∫¶ÁöÑÊúÄ‰Ω≥Âèò‰Ωì„ÄÇ\n\nÊàëÂ∞ÜÂêëÊÇ®Â±ïÁ§∫Êàë‰ªÖ‰ΩøÁî® `llama-cpp-python` Âíå‰∏Ä‰∏™ÁÆÄÂçïÁªàÁ´ØÁïåÈù¢ËøõË°åÁöÑÂÜÖÈÉ®ÊµãËØï„ÄÇ\n\n‰∏∫Ê≠§ÔºåÊàëÂàõÂª∫‰∫Ü‰∏Ä‰∏™ÊèêÁ§∫ÂàóË°®ÔºåÊ∂µÁõñ‰∫Ü‰∏ÄÁ≥ªÂàóÈÄöÂ∏∏‰ΩøÁî®ÁöÑ‰ªªÂä°ÔºåÊÇ®ÂèØ‰ª•Âú®ÊØèÊ¨°ÁîüÊàêÂêéÂàÜÈÖç‰∏Ä‰∏™ÊäïÁ•®Ôºà‰ªé 0 Âà∞ 5Ôºâ„ÄÇËøôÊòØ‰∏Ä‰∏™‰∏™‰∫∫ÁöÑ‰∫∫Â∑•Âü∫ÂáÜÊµãËØï„ÄÇ\n\n### ÈúÄÊ±Ç\n\nÂàõÂª∫‰∏Ä‰∏™ `venv`ÔºàÈúÄË¶Å Python 3.11+ÔºâÔºöÊàëÂú®ËøêË°å Windows 11 ÁöÑËø∑‰Ω†ÁîµËÑë‰∏äËøõË°å‰∫ÜÊµãËØï„ÄÇ\n\n```python\n## create the virtual environment\npython -m venv venv\n## activate the venv\nvenv\\Scripts\\activate\n## Install the dependencies \npip install llama-cpp-python==0.2.90 tiktoken\n```\nÊàë‰ª¨ÈúÄË¶Å‰ªé[ÂÆòÊñπ Qwen2.5 Hugging Face ‰ªìÂ∫ì](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF)‰∏ãËΩΩ GGUF Êñá‰ª∂„ÄÇÊàë‰ΩøÁî®ÁöÑÊòØ qwen2.5‚Äì1.5b-instruct-q5\\_k\\_m.gguf ÁâàÊú¨„ÄÇ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Fa-qFsx9RTFGZmM-vxCEPQ.png)\n\nÂú®‰∏ªÈ°πÁõÆÁõÆÂΩï‰∏≠‰∏ãËΩΩÊñá‰ª∂„ÄÇÊàë‰ª¨Â∞±ÂáÜÂ§áÂ•Ω‰∫Ü„ÄÇ\n\nËøôÈáåÁî®‰∫éÂàÜÊûêÁöÑ‰ª£Á†ÅÂú®ÊàëÁöÑ GitHub ‰ªìÂ∫ì‰∏≠Ôºö\n\nÊàëÂ∞ÜÂú®‰∏ã‰∏ÄÁØáÊñáÁ´†‰∏≠Ëß£ÈáäÊï¥‰∏™‰ª£Á†ÅÂíåÁªìÊûú„ÄÇ‰øùÊåÅÂÖ≥Ê≥®ÔºÅ\n\n## Êú™Êù•Â±ïÊúõÔºöÊåÅÁª≠ÁöÑÂºÄÊ∫ê\n\nÂú®Êú™Êù•ÁöÑËÆ°Âàí‰∏≠ÔºåÈòøÈáåÂ∑¥Â∑¥ËøòË°®Ëææ‰∫Ü‰ªñ‰ª¨ÂØπÊåÅÁª≠ÂºÄÊ∫êË¥°ÁåÆÁöÑÊâøËØ∫ÔºåÈÄöËøá‰∏∫‰∏çÂêåÈ¢ÜÂüüÁöÑÂºÄÂèëËÄÖÂèëÂ∏ÉÊõ¥Â∞èÁöÑ Qwen Âèò‰Ωì„ÄÇÂÆûÈôÖ‰∏äÔºåÂú® Hugging Face Á§æÂå∫‰∏≠ÔºåËÆ∏Â§öÁî®Êà∑Â∑≤ÂºÄÂßãÈíàÂØπÁâπÂÆö‰ªªÂä°ÂØπ Qwen ËøõË°åÂæÆË∞ÉÔºöÊàëÂú®ÊàëÁöÑ NuExtract ÊñáÁ´†‰∏≠ÂÜô‰∫Ü‰∏Ä‰∏™‰æãÂ≠êÔºöËøô‰∏™Ê®°ÂûãÁ≥ªÂàóÁöÑËæÉÂ∞èÂèò‰ΩìÂü∫‰∫é Qwen2‚Äì0.5bÔºÅ\n\nËøô‰∫õ‰∫∫Â∑•Êô∫ËÉΩÊäÄÊúØÂíåÊ®°ÂûãËøõÂ±ïÁöÑÂèëÂ±ïÊòØÂÖÖÂàÜÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂ¶Ç **Qwen** Âú®ÂêÑ‰∏™Ë°å‰∏ö‰∏≠ÊΩúÂäõÁöÑÂÖ≥ÈîÆÊ≠•È™§„ÄÇÈöèÁùÄ Model Studio ‰∏≠Âº∫Âä≤ÁöÑÈááÁî®ÁéáÊåÅÁª≠Âø´ÈÄüÂ¢ûÈïøÔºåÊòæÁÑ∂ÈòøÈáå‰∫ë‰∏ç‰ªÖÈÄöËøáÊèê‰æõÂÖàËøõÁöÑÂ∑•ÂÖ∑ËÄå‰∏îÈÄöËøá‰øÉËøõ‰ºÅ‰∏ö‰πãÈó¥ÁöÑÂàõÊñ∞ÔºåÊàê‰∏∫‰∫ÜË°å‰∏öÁöÑÂÖàÈîãÈ¢ÜÂØºËÄÖ„ÄÇ\n\nÂú®ÊàëËøôËæπÔºåÊàëÁöÑÂ±ïÊúõÊòØÁªßÁª≠ÂØπÊñ∞Ê®°ÂûãËøõË°åÂÜÖÈÉ®ÊµãËØïÔºåÁâπÂà´ÊòØÂØπÂ∞èÂûãÊ®°ÂûãÔºåÊúÄÈ´òÂà∞ 3B„ÄÇ\n\nÂú®‰∏ã‰∏ÄÁØáÊñáÁ´†‰∏≠ÔºåÊàëÂ∞Ü‰∏éÊÇ®ÂàÜ‰∫´ÊàëÁöÑÊñπÊ≥ïÔºåÂ¶Ç‰ΩïËøêË°åÊ®°Âûã‰ª•ÂèäÁî®‰∫éÂçÅ‰∏â‰∏™ NLP ‰ªªÂä°ÁöÑÊèêÁ§∫Ê®°Êùø„ÄÇ\n\nÂ∏åÊúõÊÇ®ÂñúÊ¨¢ËøôÁØáÊñáÁ´†„ÄÇÂ¶ÇÊûúËøô‰∏™ÊïÖ‰∫ãÂØπÊÇ®Êúâ‰ª∑ÂÄºÔºåÂπ∂‰∏îÊÇ®ÊÉ≥Á®çÂæÆË°®Á§∫ÊîØÊåÅÔºåÊÇ®ÂèØ‰ª•Ôºö\n\n1. ‰∏∫Ëøô‰∏™ÊïÖ‰∫ãÂ§öÊ¨°ÁÇπËµû\n2. Á™ÅÂá∫Êõ¥ÂÄºÂæóËÆ∞‰ΩèÁöÑÈÉ®ÂàÜÔºàËøôÂ∞Ü‰ΩøÊÇ®Êõ¥ÂÆπÊòìÊâæÂà∞ÂÆÉ‰ª¨Ôºå‰πüËÆ©ÊàëÂÜôÂá∫Êõ¥Â•ΩÁöÑÊñáÁ´†Ôºâ\n3. **Âä†ÂÖ•ÊàëÁöÑ[ÂÆåÂÖ®ÂÖçË¥πÁöÑÊØèÂë® Substack ÈÄöËÆØ](https://thepoorgpuguy.substack.com/about)**\n4. Ê≥®ÂÜå Medium ‰ºöÂëòÔºà$5/ÊúàÂèØÈòÖËØªÊó†Èôê Medium ÊïÖ‰∫ãÔºâ\n5. Âú® Medium ‰∏äÂÖ≥Ê≥®Êàë\n6. ÈòÖËØªÊàëÁöÑÊúÄÊñ∞ÊñáÁ´† <https://medium.com/@fabio.matricardi>\n\nËøôÈáåËøòÊúâÂá†ÁØáÊñáÁ´†Êù•Êª°Ë∂≥ÊÇ®ÁöÑÂ•ΩÂ•áÂøÉÔºö\n\nÊú¨Êñá‰∏≠ÂºïÁî®ÁöÑËµÑÊ∫êÔºö\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Du7V61mEX_yIrfmF.png)\n\nÊ≠§ÊïÖ‰∫ãÂèëÂ∏ÉÂú® [Generative AI](https://generativeai.pub/)„ÄÇ‰∏éÊàë‰ª¨Âú® [LinkedIn](https://www.linkedin.com/company/generative-ai-publication) ‰∏äËÅîÁ≥ªÔºåÂπ∂ÂÖ≥Ê≥® [Zeniteq](https://www.zeniteq.com/)Ôºå‰ª•‰æøÂèäÊó∂‰∫ÜËß£ÊúÄÊñ∞ÁöÑ‰∫∫Â∑•Êô∫ËÉΩÊïÖ‰∫ã„ÄÇ\n\nËÆ¢ÈòÖÊàë‰ª¨ÁöÑ [ÈÄöËÆØ](https://www.generativeaipub.com/) Âíå [YouTube](https://www.youtube.com/@generativeaipub) È¢ëÈÅìÔºåÂèäÊó∂Ëé∑ÂèñÁîüÊàêÂºè AI ÁöÑÊúÄÊñ∞Ê∂àÊÅØÂíåÊõ¥Êñ∞„ÄÇËÆ©Êàë‰ª¨ÂÖ±ÂêåÂ°ëÈÄ† AI ÁöÑÊú™Êù•ÔºÅ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*pvLAT3it1FkdhVU0.png)\n\n"},{"lang":"zh","group":"blog","slug":"blog/a-practical-guide-for-using-autogen-in-software-applications-8799185d27ee","frontmatter":{"title":"Âú®ËΩØ‰ª∂Â∫îÁî®Á®ãÂ∫è‰∏≠‰ΩøÁî® AutoGen ÁöÑÂÆûÁî®ÊåáÂçó","meta_title":"Âú®ËΩØ‰ª∂Â∫îÁî®Á®ãÂ∫è‰∏≠‰ΩøÁî® AutoGen ÁöÑÂÆûÁî®ÊåáÂçó","description":"Êõ¥Êñ∞ÔºöËôΩÁÑ∂ËøôÁØáÊñáÁ´†ÊòØ 4 ‰∏™ÊúàÂâçÂÜôÁöÑÔºå‰ΩÜ AutoGen Â∑≤ÁªèÂèëÁîü‰∫ÜÂæàÂ§ßÂèòÂåñ„ÄÇÂØπ‰∫éÂèØËÉΩÂ≠òÂú®ÁöÑ‰∏Ä‰∫õÈóÆÈ¢òÔºåÊàëÊ∑±Ë°®Ê≠âÊÑè‚Ä¶‚Ä¶","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*yrraWH6aGNnbx8p-wfQ1OQ.jpeg","categories":["Programming","Chatbots","Autonomous Systems"],"author":"Rifx.Online","tags":["AutoGen","multi-agent","LLMs","customization","collaboration"],"draft":false,"slug":"blog/a-practical-guide-for-using-autogen-in-software-applications-8799185d27ee"},"content":"\n\n\n\n\n*Êõ¥Êñ∞ÔºöËôΩÁÑ∂ËøôÁØáÊñáÁ´†ÊòØÂú®Âõõ‰∏™ÊúàÂâçÂÜôÁöÑÔºå‰ΩÜ AutoGen Ëá™ÈÇ£Êó∂‰ª•Êù•ÂèòÂåñÂæàÂ§ß„ÄÇÂØπ‰∫éÊàë‰ª£Á†ÅÁ§∫‰æã‰∏≠ÂèØËÉΩËøáÊó∂ÁöÑÂÜÖÂÆπÔºåÊàëÊ∑±ÊÑüÊ≠âÊÑè„ÄÇ*\n\nÂ¶ÇÊûúÊÇ®ÊÉ≥‰∫ÜËß£ AutoGenÔºåÂèØ‰ª•Êü•Áúã [ÊñáÊ°£](https://microsoft.github.io/autogen/)„ÄÅ[Colab Á¨îËÆ∞Êú¨](https://microsoft.github.io/autogen/docs/Examples) Âíå [ÂçöÂÆ¢](https://microsoft.github.io/autogen/blog)„ÄÇÈùûÂ∏∏ÊÑüË∞¢ AutoGen Âõ¢ÈòüÂà∂‰Ωú‰∫Ü‰∏Ä‰∏™‰ª§‰∫∫ÊÉäÂèπÁöÑ‰∫ßÂìÅÔºå‰ΩÜËÄÅÂÆûËØ¥‚Äî‚ÄîÂú®ÈòÖËØª‰∫Ü‰ªñ‰ª¨ÁöÑÊâÄÊúâÂÜÖÂÆπÂêéÔºåÊàë‰ªçÁÑ∂‰∏çÁü•ÈÅìÂ¶Ç‰ΩïÂú®ÁªàÁ´ØÊàñ Jupyter Notebook ‰πãÂ§ñ‰ΩøÁî® AutoGen„ÄÇ\n\nÊú¨ÊñáËØïÂõæÈÄöËøáÊèê‰æõ‰∏Ä‰∫õÊúâÁî®ÁöÑÊñπÊ≥ïÊù•Â∏ÆÂä©Â°´Ë°•Ëøô‰∏™Á©∫ÁôΩÔºå‰Ωø AutoGen Âú®ËΩØ‰ª∂Â∫îÁî®‰∏≠ÂèëÊå•‰ΩúÁî®„ÄÇ‰ª•‰∏ãÊòØÊàëÂ∞ÜË¶ÅËÆ®ËÆ∫ÁöÑ‰∏ªÈ¢òÔºö\n\n1. ‰ª£ÁêÜ‰∏ç‰ªÖÈôê‰∫éÈÄöËøáÁªàÁ´ØËøõË°åÈÄö‰ø°\n2. Ê≥®ÂÜåËá™ÂÆö‰πâÂõûÂ§ç\n3. Â¶Ç‰Ωï‰ª•ÁúüÂÆûÁöÑÊñπÂºèÂ∞ÜÁúüÂÆû‰∫∫Á±ªÁ∫≥ÂÖ•ÂØπËØù\n4. ÊÇ®ÂèØ‰ª•ÔºàÂπ∂‰∏îÂ∫îËØ•ÔºâËá™ÂÆö‰πâË∞ÅÊù•ÂèëË®Ä\n5. ÊÇ®‰∏çÂøÖ‰ΩøÁî® OpenAI\n6. ÂèØ‰ª•‰ΩøÁî®ÂáΩÊï∞ËÄå‰∏çÊòØÊâßË°å‰ª£Á†Å\n7. Â∞Ü‰ª£ÁêÜÁî®‰∫éÁªÑÁªáÔºåËÄå‰∏ç‰ªÖ‰ªÖÊòØÂØπËØù\n\nÊúÄÂêéÔºåÊàëÂ∞ÜËÆ®ËÆ∫‰∏∫‰ªÄ‰πàÊàëËÆ§‰∏∫ÊÇ®Â∫îËØ•È¶ñÂÖà‰ΩøÁî® AutoGen„ÄÇËÆ©Êàë‰ª¨ÂºÄÂßãÂêßÔºÅ\n\n## ‰ª£ÁêÜ‰∏ç‰ªÖÈôê‰∫éÈÄöËøáÁªàÁ´ØËøõË°åÈÄö‰ø°\n\nÊÇ®‰ºöÁúãÂà∞ÊØè‰∏™‰∫∫ÈÉΩ‰ΩøÁî®ÁªàÁ´ØÊàñ Jupyter Notebook ÊºîÁ§∫ AutoGen„ÄÇËøôÂØπ‰∫éÊºîÁ§∫Êù•ËØ¥‰∏çÈîôÔºå‰ΩÜËøô‰∫õ‰ª£ÁêÜ‰πãÈó¥ËøòÊúâÂÖ∂‰ªñ‰∫§ÊµÅÊñπÂºè„ÄÇ\n\nAutoGen Êúâ 2 ‰∏™Âü∫Êú¨Á±ªÔºö[`UserProxyAg`ent](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/user_proxy_agent.py) Âíå [`AssistantAg`ent](https://github.com/microsoft/autogen/blob/main/autogen/agentchat/assistant_agent.py)„ÄÇÂÆÉ‰ª¨ÁªßÊâø‰∫Ü [`ConversableAg`ent](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py) Á±ªÔºå‰ªÖ‰∏∫Âü∫Á±ªÊèê‰æõ‰∫ÜÂá†‰∏™‰∏çÂêåÁöÑÈªòËÆ§ÂèÇÊï∞„ÄÇ\n\nÂΩìÊÇ®ÁúãÂà∞Ëøô‰∏™ÁªèÂÖ∏‰ª£Á†ÅÁ§∫‰æãÊó∂Ôºö\n\n```python\nassistant = autogen.AssistantAgent(\n    name=\"assistant\",\n    llm_config=llm_config\n)\nuser_proxy = autogen.UserProxyAgent(name=\"user_proxy\")\nawait user_proxy.a_initiate_chat(\n    assistant,\n    message=\"\"\"What date is today? Compare the year-to-date gain for META and TESLA.\"\"\",\n)\n```\nÂèëÁîüÁöÑ‰∫ãÊÉÖÊòØ `UserProxyAgent` Â∞ÜË∞ÉÁî®ÂÖ∂Ëá™Â∑±ÁöÑ `send` ÊñπÊ≥ïÔºåËøôÂ∞ÜË∞ÉÁî® `AssistantAgent` ÁöÑ [`rece`ive](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L514) ÊñπÊ≥ïÔºå‰º†ÈÄíÂéüÂßãÊ∂àÊÅØ„ÄÇÂ∞ÜÁîüÊàêÂõûÂ§çÔºàÁ®çÂêé‰ºöËØ¶ÁªÜËØ¥ÊòéÔºâÔºåÁÑ∂Âêé `AssistantAgent` Â∞ÜË∞ÉÁî®ÂÖ∂ [`s`end](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L351) ÊñπÊ≥ïÔºåËøôÂ∞ÜË∞ÉÁî® `UserProxyAgent` ÁöÑ `receive` ÊñπÊ≥ïÔºå‰æùÊ≠§Á±ªÊé®ÔºåÁõ¥Âà∞ `UserProxyAgent` Á°ÆÂÆöÂØπËØùÂ∑≤ÁªàÊ≠¢ÔºàÂèØ‰ª•ÈÄöËøá `is_termination_msg` ÂèÇÊï∞Ëá™ÂÆö‰πâÔºâ„ÄÇ\n\nÊàëÁ¨¨‰∏ÄÊ¨°‚ÄúÊÅçÁÑ∂Â§ßÊÇü‚ÄùÁöÑÊó∂ÂàªÊòØÂΩìÊàëÊÑèËØÜÂà∞Ëøô‰∫õ‰ª£ÁêÜÊòØÁ±ªÊó∂ÔºåÊàëÂèØ‰ª•ÂàõÂª∫Ëá™Â∑±ÁöÑËá™ÂÆö‰πâ‰ª£ÁêÜÁ±ªÔºåÁªßÊâø AutoGen ÁöÑ UserProxy/Assistant/Conversable Agent Á±ªÔºåÂπ∂ÈáçÂÜô‰ªª‰ΩïÈªòËÆ§ÊñπÊ≥ï„ÄÇËøô‰ΩøÂæó AutoGen ÈùûÂ∏∏ÂèØÊâ©Â±ï„ÄÇ\n\nÊàëÊúâ‰∏Ä‰∏™Áî®‰æãÔºåÈúÄË¶Å‰∏Ä‰∏™ÂèØ‰ª•ÈÄöËøáÁΩëÁ´ô‰∏äÁöÑËÅäÂ§© UI ËæìÂÖ•Ê∂àÊÅØÁöÑ‰∫∫ÔºàÁî± `UserProxyAgent` ‰ª£ÁêÜÔºâÔºåÊàëÂ∏åÊúõ `AssistantAgent` ËÉΩÂú® UI ‰∏≠ÂõûÂ§çËØ•ËÅäÂ§©ÔºåÂπ∂ËÉΩÂ§üÊé•Êî∂Êõ¥Â§öÊù•Ëá™‰∫∫Á±ªÁî®Êà∑ÁöÑÊ∂àÊÅØÔºåÂ∞±Â•ΩÂÉè‰∫∫Á±ªÂè™ÊòØËøô‰∏™ AutoGen ÂØπËØù‰∏≠ÁöÑÂè¶‰∏Ä‰∏™‰ª£ÁêÜ„ÄÇ\n\nÊàëÂèØ‰ª•ÈáçÂÜô `send` Âíå `receive` ÊñπÊ≥ïÔºàÊàñ `a_send` Âíå `a_receive`ÔºâÔºåÂπ∂ÈÄöËøá http„ÄÅwebsockets Á≠âËøõË°åÊé®ÈÄÅ/ÊãâÂèñ„ÄÇÊàëÂ∞ùËØï‰∫ÜËøô‰∏™ÔºåÂÆÉÂºÄÂßãÂ∑•‰ΩúÔºå‰ΩÜÊó†Ê≥ïÊâ©Â±ï„ÄÇËÆ©Êàë‰ª¨Â≠¶‰π†‰∏ÄÁßçÊõ¥Â•ΩÁöÑÊñπÊ≥ï„ÄÇ\n\n## Ê≥®ÂÜåËá™ÂÆö‰πâÂõûÂ§ç\n\nAutoGen ÂÖ∑Êúâ‰∏Ä‰∏™Êèí‰ª∂Á≥ªÁªüÔºåÂèØ‰ª•ËÆ©ÊÇ®Ëá™ÂÆö‰πâ‰ª£ÁêÜÁîüÊàêÂõûÂ§çÁöÑÊñπÂºè„ÄÇÊàë‰ª¨‰π†ÊÉØÁúãÂà∞ÁöÑÁ§∫‰æãÊòØ AutoGen Êü•ËØ¢ OpenAI Ëé∑ÂèñÁ≠îÊ°àÔºåÂπ∂Â∞ÜÂÖ∂‰Ωú‰∏∫ÂõûÂ§çÔºå‰ΩÜÊÇ®‰πüÂèØ‰ª•ÊèíÂÖ•Ëá™Â∑±ÁöÑÊñπÊ≥ïÔºö\n\n```python\nclass WeatherAgent(AssistantAgent):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, llm_config=False, **kwargs)\n        self.register_reply(Agent, WeatherAgent.get_weather)\n\n    async def get_weather(\n        self,\n        messages: List[Dict] = [],\n        sender=None,\n        config=None,\n    ) -> Tuple[bool, Union[str, Dict, None]]:\n        last_message = messages[-1][\"content\"]\n        result = await fetch_weather(last_message)\n        return True, result\n\nasync def fetch_weather(city: str) -> str:\n    async with httpx.AsyncClient() as client:\n        result = await client.post(\n            WEATHER_API_URL,\n            json={\"city\": question},\n        )\n        return result.json()\n\nweather_assistant = WeatherAgent(name=\"weather_assistant\")\nuser_proxy = autogen.UserProxyAgent(name=\"user_proxy\")\nawait user_proxy.a_initiate_chat(assistant, message=\"Lehi\")\nprint(weather_assistant.last_message)\n```\nÂú®ËøôÈáåÔºå`register_reply` Â∞ÜÊèíÂÖ•ÊàëÁöÑËá™ÂÆö‰πâÊñπÊ≥ï‰ª•Ëé∑ÂèñÂõûÂ§çÔºåÈªòËÆ§ÊÉÖÂÜµ‰∏ãÔºåËØ•ÊñπÊ≥ïÂ∞ÜÊîæÂú® `position=0`ÔºåËøôÊÑèÂë≥ÁùÄÂÆÉÂ∞ÜÊòØÂ∞ùËØïÁöÑÁ¨¨‰∏Ä‰∏™ÂõûÂ§çÊñπÊ≥ï„ÄÇËØ•ÊñπÊ≥ïÂ∫îËøîÂõû‰∏Ä‰∏™ÂÖÉÁªÑÔºåÂÖ∂‰∏≠Á¨¨‰∏Ä‰∏™È°πÁõÆÊòØ‰∏Ä‰∏™Â∏ÉÂ∞îÂÄºÔºåÊåáÁ§∫Ê≠§ÂõûÂ§çÊòØÂê¶‰∏∫Â∫î‰ΩøÁî®ÁöÑÂõûÂ§çÔºåÊàñËÄÖÊòØÂê¶Â∫îÂ∞ùËØï‰∏ã‰∏Ä‰∏™Ê≥®ÂÜåÁöÑÂõûÂ§çÔºà‰æãÂ¶ÇÔºå‰ΩøÁî® OpenAI ÁöÑÂÜÖÁΩÆÂõûÂ§çÁîüÊàê ‚Äî ÂÆåÊï¥È°∫Â∫èËØ∑ÂèÇËßÅ [Ê≠§Â§Ñ](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L145-L153)Ôºâ„ÄÇ\n\n‰∫ÜËß£ [`register_reply`](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L155) ‰ΩøÊÇ®ËÉΩÂ§üËá™ÂÆö‰πâÂõûÂ§çÁöÑÊ£ÄÁ¥¢ÊñπÂºèÔºåÂÖÅËÆ∏ÊÇ®ÂêØÂä®Â≠êÂ§ö‰ª£ÁêÜÂØπËØùÁ≠â„ÄÇ\n\n## Â¶Ç‰Ωï‰ª•ÁúüÂÆûÁöÑÊñπÂºèÂ∞ÜÁúüÂÆû‰∫∫Á±ªÁ∫≥ÂÖ•ÂØπËØù\n\nËøôÈáåÊúâ‰∏ÄÁßçÊñπÊ≥ïÔºö\n\n```python\n## user makes a POST /query { \"message\": \"What's the weather?\" }\n\n@query_blueprint.route(\"/query\", methods=[\"POST\"])\nasync def post_query():\n  message = request.form.get(\"message\")\n\n  assistant = autogen.AssistantAgent(\n    name=\"assistant\",\n    llm_config=llm_config\n    system_message=\"\"\"You're a helpful assistant.\n    If you need more info, ask the user for anything missing.\"\"\"\n  )\n  user_proxy = autogen.UserProxyAgent(\n    name=\"user_proxy\",\n    human_input_mode=\"NEVER\",\n    code_execution_config=False,\n    is_termination_msg=lambda message: True # Always True\n  )\n  weather_assistant = WeatherAgent(\n    name=\"weather_assistant\",\n    system_message=\"\"\"You're a helpful assistant to get the weather.\n    You fetch weather information, then return it.\"\"\"\n  )\n\n  groupchat = autogen.GroupChat(\n    agents=[assistant, user_proxy, weather_assistant],\n    messages=[]\n  )\n  manager = autogen.GroupChatManager(\n    name=\"Manager\",\n    groupchat=groupchat,\n    llm_config=llm_config,\n  )\n\n  await user_proxy.a_initiate_chat(manager, message=message)\n\n  return groupchat.messages[-1]\n```\nËøôÈáåÂèëÁîü‰∫Ü‰ªÄ‰πàÔºü\n\n1. ÊØèÂΩì‰∏ÄÊù°Ê∂àÊÅØÂèëÈÄÅÂà∞ `user_proxy` Êó∂ÔºåÂØπËØùÂ∞ÜÁªìÊùüÔºàÊàë‰ª¨Á®çÂêé‰ºöÊÅ¢Â§çÂÆÉÔºâ„ÄÇËøôÊ†∑ÂÅöÁöÑÂéüÂõ†ÊòØ‰ªÄ‰πàÔºüËøôÊÑèÂë≥ÁùÄ `user_proxy` ÂÆûÈôÖ‰∏äÂèØ‰ª•‰ª£ÁêÜÁî®Êà∑„ÄÇÂÆÉ‰∏ç‰ºöÂ∞ùËØïÂõûÁ≠îÔºåËÄåÊòØ‰ºöÁªìÊùüÂΩìÂâçÁöÑÂØπËØùÊµÅÁ®ãÔºåÂÖÅËÆ∏ÁúüÂÆûÁöÑ‰∫∫Á±ªÁî®Êà∑ÂìçÂ∫îÔºàÈÄöËøáÊÅ¢Â§çÂØπËØù ‚Äî ËßÅ‰∏ãÊñáÔºâ„ÄÇ\n2. Â¶ÇÊûúÂä©ÁêÜÈúÄË¶ÅÊõ¥Â§ö‰ø°ÊÅØÔºåÂÆÉ‰ºöËØ¢ÈóÆ user_proxyÔºåËøôÂ∞ÜÁªìÊùüÂΩìÂâçÂØπËØù„ÄÇ\n\nÂú®‰∏äËø∞‰ª£Á†Å‰∏≠ÔºåÂèØËÉΩ‰ºöÂèëÁîü‰ª•‰∏ãÊÉÖÂÜµÔºö\n\n1. user_proxy -> manager: ‚ÄúÂ§©Ê∞îÊÄé‰πàÊ†∑Ôºü‚Äù\n2. assistant -> manager: ‚ÄúÁî®Êà∑Ê≤°ÊúâÊåáÂÆöÂì™‰∏™ÂüéÂ∏Ç„ÄÇ‚Äù\n3. manager -> user_proxy : ÂØπËØùÂ∞ÜÁªìÊùü\n\nÁé∞Âú®ÔºåÂ¶ÇÊûúÁî®Êà∑ÊÉ≥Ë¶ÅÂõûÂ∫îÂπ∂ÊÅ¢Â§çÂØπËØùÔºåÊàë‰ª¨ËØ•Â¶Ç‰ΩïÂÅöÂà∞Âë¢ÔºüÊúâÂæàÂ§öÊñπÊ≥ïÂèØ‰ª•ÂÅöÂà∞Ëøô‰∏ÄÁÇπÔºåËøôÈáåÂè™ÊòØ‰∏Ä‰∏™Á§∫‰æãÔºö\n\n```python\n## user makes a POST /query { \"message\": \"What's the weather?\" }\n## above posts returns a `history` array\n## user makes a second POST /query { \"message\": \"What's the weather?\", \"history\": history }\n\nclass ResumableGroupChatManager(GroupChatManager):\n    groupchat: GroupChat\n\n    def __init__(self, groupchat, history, **kwargs):\n        self.groupchat = groupchat\n        if history:\n            self.groupchat.messages = history\n\n        super().__init__(groupchat, **kwargs)\n\n        if history:\n            self.restore_from_history(history)\n\n    def restore_from_history(self, history) -> None:\n        for message in history:\n            # broadcast the message to all agents except the speaker.  This idea is the same way GroupChat is implemented in AutoGen for new messages, this method simply allows us to replay old messages first.\n            for agent in self.groupchat.agents:\n                if agent != self:\n                    self.send(message, agent, request_reply=False, silent=True)\n\n@query_blueprint.route(\"/query\", methods=[\"POST\"])\nasync def post_query():\n  message = request.form.get(\"message\")\n\n  assistant = autogen.AssistantAgent(\n    name=\"assistant\",\n    llm_config=llm_config\n    system_message=\"\"\"You're a helpful assistant.\n    If you need more info, ask the user for anything missing.\"\"\"\n  )\n  user_proxy = autogen.UserProxyAgent(\n    name=\"user_proxy\",\n    human_input_mode=\"NEVER\",\n    code_execution_config=False,\n    is_termination_msg=lambda message: True # Always True\n  )\n  weather_assistant = WeatherAgent(\n    name=\"weather_assistant\",\n    system_message=\"\"\"You're a helpful assistant to get the weather.\n    You fetch weather information, then return it.\"\"\"\n  )\n\n  groupchat = autogen.GroupChat(\n    agents=[assistant, user_proxy, weather_assistant],\n    messages=[]\n  )\n  manager = ResumableGroupChatManager(\n    name=\"Manager\",\n    groupchat=groupchat,\n    llm_config=llm_config,\n  )\n\n  await user_proxy.a_initiate_chat(manager, message=message)\n\n  return {\n    \"response\": groupchat.messages[-1],\n    \"history\": groupchat.messages,\n  }\n```\nÈÄöËøáËøôÁßçÊñπÊ≥ïÔºåÊÇ®ÂèØ‰ª•Â∞Ü‰∫∫Á±ªÁ∫≥ÂÖ•ÂØπËØùÔºåÂ∞±ÂÉè‰ªñ‰ª¨ÊòØÁæ§ËÅä‰∏≠ÁöÑÂè¶‰∏Ä‰∏™‰ª£ÁêÜ‰∏ÄÊ†∑„ÄÇÊØèÂΩìÂä©ÁêÜ‰ª£ÁêÜÈúÄË¶Å‰∫∫Á±ªËæìÂÖ•Êó∂ÔºåÂÆÉ‰ª¨‰ºöËØ¢ÈóÆ user_proxyÔºåuser_proxy ÁÑ∂ÂêéÁªìÊùüÂΩìÂâçÂØπËØùÔºåÂÖÅËÆ∏‰∫∫Á±ªÁî®Êà∑Áî®Êõ¥Â§ö‰ø°ÊÅØËøõË°åÂìçÂ∫îÔºåÁÑ∂ÂêéÊÅ¢Â§çÂà∞‰πãÂâçÁöÑÂØπËØù„ÄÇ\n\nËøôÁßçÊñπÊ≥ïÁöÑÂ•ΩÂ§ÑÊòØÔºö\n\n* ÂØπËØùÂèØ‰ª•ÈÄöËøáÊÇ®ÊÉ≥Ë¶ÅÁöÑ‰ªª‰ΩïÊñπÂºèÂåÖÂê´ÁúüÂÆû‰∫∫Á±ªËæìÂÖ•Ôºà‰æãÂ¶ÇÈÄöËøá http Êàñ websocketÔºâ„ÄÇ\n* Âú®Ëé∑Âèñ‰∫∫Á±ªËæìÂÖ•Êó∂ÔºåÂØπËØùË¢´ÊöÇÂÅú„ÄÇËøô‰∏∫ÂÖ∂‰ªñÂØπËØùÂíåËÆ°ÁÆóÈáäÊîæ‰∫ÜÁ∫øÁ®ã„ÄÇ\n* ÊÇ®ÂèØ‰ª•Âú®‰ºöËØù‰πãÈó¥ÊåÅ‰πÖÂåñËøô‰∫õÂØπËØù„ÄÇ\n\n## ‰Ω†ÂèØ‰ª•ÔºàÂπ∂‰∏îÂ∫îËØ•ÔºâËá™ÂÆö‰πâË∞ÅÊé•‰∏ãÊù•ÂèëË®Ä\n\nËøôÊòØ‰∏ªËßÇÁöÑÔºå‰ΩÜÊàëËÆ§‰∏∫‰Ω†Â∫îËØ•ÂßãÁªàËá™ÂÆö‰πâÂèëË®ÄËÄÖÁöÑÈÄâÊã©ÊñπÂºèÔºåÂõ†‰∏∫Ôºö\n\n1. ‰Ω†Â∞Ü‰ΩøÁî®Êõ¥Â∞ëÁöÑ‰ª§ÁâåÔºàËäÇÁúÅÈáëÈí±ÂíåÂìçÂ∫îÊó∂Èó¥Ôºâ\n2. ‰Ω†ÂèØ‰ª•Â∞ÜÂÜ≥ÂÆöË∞ÅÂèëË®ÄÁöÑÈÄªËæë‰∏éÂÆö‰πâÊØè‰∏™‰ª£ÁêÜÁ≥ªÁªüÊåá‰ª§ÁöÑÈÄªËæëÂàÜÂºÄ\n\n\n```python\nshort_role_descriptions = {\n  \"user_proxy\": \"A proxy for the user\",\n  \"weather_assistant\": \"You can get the weather\",\n  \"planner\": \"You help coordinate the plan. Your turn happens when XYZ, but skip your turn when ABC\"\n}\n\nclass CustomGroupChat(GroupChat):\n    # The default message uses the full system message, which is a long string.  We are overriding this to use a shorter message.\n    def select_speaker_msg(self, agents: List[Agent]):\n        message = f\"\"\"You are in a role play game. The following roles are available:\n        ---\n        {new_line.join([f\"{agent.name}: {short_role_descriptions[agent.name]}\" for agent in agents])}\n        ---\n\n        The role who plays next depends on the conversation.  User_Proxy will star the conversation, and typically Planner would go next.\n\n        Here are some examples\n        ---\n        ... not shown here ...\n        ---\n\n        Read the following conversation.\n        Then select the next role from {', '.join([agent.name for agent in agents])} to play. Only return the role.\"\"\"\n        return message\n```\n\n## ‰Ω†‰∏çÂøÖ‰ΩøÁî® OpenAI\n\nAutoGen Â∑≤ÁªèÊåáÂá∫Ôºå‰Ω†ÂèØ‰ª•‰ΩøÁî®ÂÖ∂‰ªñ LLMÔºåÂè™Ë¶ÅÂÆÉ‰ª¨ÊòØ‚ÄúÁ±ª‰ºº ChatGPT‚ÄùÁöÑÔºåËøôÊÑèÂë≥ÁùÄÂÆÉ‰ª¨ÁöÑ API ÂìçÂ∫î‰∏é ChatGPT API Ë∞ÉÁî®ÁöÑÂΩ¢Áä∂ÂíåÂìçÂ∫îÁõ∏‰ºº„ÄÇ\n\n‰ΩÜÊòØÔºåËØ∑ËÆ∞‰ΩèËøô‰∫õ‰ª£ÁêÜÊòØÁ±ªÔºåÂπ∂‰∏î‰Ω†ÂèØ‰ª•ÈáçÂÜôÂ§ßÂ§öÊï∞ÊñπÊ≥ïÔºü\n\nÂ∞ùËØïÈáçÂÜôÊñπÊ≥ï: [generate\\_oai\\_reply](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L678)Ôºå‰Ω†ÂèØ‰ª•Êü•ËØ¢‰ªª‰Ωï‰Ω†ÊÉ≥Ë¶ÅÁöÑ LLM„ÄÇ\n\n## ÂáΩÊï∞ÂèØ‰ª•Áî®Êù•‰ª£ÊõøÊâßË°å‰ª£Á†Å\n\nÂΩìÊàëÂéªÊâæÊàë‰ª¨ÁöÑÂÆâÂÖ®Âõ¢ÈòüÂπ∂ËØ¥Ôºö‚ÄúÊàëÊÉ≥Âú®Kubernetes‰∏≠‰∏∫ÊàëÁöÑÊúçÂä°‰ΩøÁî®AutoGen„ÄÇÂÆÉÈúÄË¶ÅËÉΩÂ§üÊâßË°å‰ªª‰ΩïÁî±LLMÁîüÊàêÁöÑ‰ªªÊÑè‰ª£Á†Å„ÄÇ‰Ω†‰ª¨ÂØπÊ≠§Ê≤°ÈóÆÈ¢òÂêßÔºü‚Äù\n\nÂΩìÁÑ∂ÔºåÁ≠îÊ°àÊòØÊòéÁ°ÆÁöÑÔºö‰∏çÂèØ‰ª•„ÄÇ\n\nÈÇ£‰πàÔºå‰∏∫‰ªÄ‰πàÂú®Ê≤°ÊúâËá™Âä®‰ª£Á†ÅÊâßË°åËÉΩÂäõÁöÑÊÉÖÂÜµ‰∏ã‰ΩøÁî®AutoGenÔºü\n\nÈô§‰∫Ü‰∏ãÈù¢ÊèêÂà∞ÁöÑÂéüÂõ†‰πãÂ§ñÔºåËøòÊúâ‰∏Ä‰∏™ÂéüÂõ†ÊòØ‰Ω†ÂèØ‰ª•‰ΩøÁî®ÂáΩÊï∞Ë∞ÉÁî®Êù•ÂÆåÂÖ®ÊéßÂà∂‰ª£Á†ÅÊâßË°å„ÄÇÂ¶ÇÊûú‰Ω†Êúâ‰∏ÄÁªÑÊÉ≥Ë¶ÅÊèê‰æõÁªôAutoGenÁöÑpythonÂáΩÊï∞‚Äî‚ÄîËøô‰∫õÂáΩÊï∞ÊòØ‰Ω†ÁºñÂÜôÁöÑ„ÄÅ‰Ω†ÊéßÂà∂ÁöÑÔºåÂπ∂‰∏îÂèØ‰ª•Êé•Âèó‰∏Ä‰∫õÂÆâÂÖ®ÂèÇÊï∞‚Äî‚ÄîËøôÂê¨Ëµ∑Êù•ÊÄªÊØîÂú®‰Ω†ÁöÑÁßÅÊúâÂü∫Á°ÄËÆæÊñΩ‰∏≠ÂÖÅËÆ∏‰ªª‰Ωï‰ª£Á†ÅË¢´ÊâßË°åË¶ÅÂ•ΩÂæóÂ§ö„ÄÇ\n\n## ‰ΩøÁî®‰ª£ÁêÜËøõË°åÁªÑÁªáÔºåËÄå‰∏ç‰ªÖ‰ªÖÊòØËøõË°åÂØπËØù\n\n‰πüËÆ∏‰Ω†Âπ∂‰∏çÈúÄË¶Å‰∏Ä‰∏™Ëá™‰∏ªÁöÑÂ§ö‰ª£ÁêÜÂØπËØù„ÄÇ‰πüËÆ∏‰Ω†Âè™ÈúÄË¶ÅÂØπLLMËøõË°åÂá†Ê¨°‰∏çÂêåÁöÑË∞ÉÁî®„ÄÇ\n\nÊàë‰ªçÁÑ∂ÂñúÊ¨¢‰ªÖ‰ªÖÂá∫‰∫éÁªÑÁªáÁõÆÁöÑËÄåÊã•Êúâ‰∏çÂêå‚Äú‰ª£ÁêÜ‚ÄùÁöÑÊÉ≥Ê≥ï„ÄÇËøôÊòØ‰∏Ä‰∏™ÈùûÂ∏∏ÁñØÁãÇÁöÑÊÉ≥Ê≥ïÔºå‰ΩÜËØ∑Ê†πÊçÆËá™Â∑±ÁöÑÊÉÖÂÜµÊù•ÁúãÂæÖÂÆÉÔºö\n\n```python\nanalyst = autogen.AssistantAgent(\n    name=\"Analyst\",\n    system_message=\"\"\"Your an analyst.  You do XYZ.\"\"\",\n    llm_config=llm_config,\n)\n\nsummarizer = autogen.AssistantAgent(\n    name=\"Summarizer\",\n    system_message=\"\"\"Your a summarizer.  You do XYZ.\"\"\",\n    llm_config=llm_config,\n)\n\nreport = \"\"\"Some long report\"\"\"\n\nanalysis = analyst.generate_oai_reply(report)[1]\nsummary = summarizer.generate_oai_reply(report)[1]\n\nprint(f\"Analysis: {analysis}\")\nprint(f\"Summary: {summary}\")\n```\n\n## ‰∏∫‰ªÄ‰πà‰ΩøÁî® AutoGenÔºü\n\n1. AutoGen ÂÖÅËÆ∏Â§ö‰∏™‰ª£ÁêÜÔºåÂÖ∑Êúâ‰∏çÂêåÁöÑÁ≥ªÁªüÊèêÁ§∫ÂíåÊåá‰ª§ÔºåÂÖ±ÂêåËß£ÂÜ≥ÈóÆÈ¢ò„ÄÇÂ∞±ÂÉèÂú®Áé∞ÂÆûÁîüÊ¥ª‰∏≠Ôºå‰∏çÂêåÁöÑËßÜËßíÂÖ±ÂêåÂêà‰Ωú‰ºöÊØîÂçï‰∏ÄÊÄùÁª¥Êõ¥Â•ΩÂú∞Ëß£ÂÜ≥ÈóÆÈ¢ò„ÄÇ\n2. AutoGen GroupChat ÈùûÂ∏∏Âá∫Ëâ≤„ÄÇÂÆÉÊèê‰æõ‰∫ÜÈÄöÂêëÊ≠£Á°Æ‰∏ìÂÆ∂Ôºà‰ª£ÁêÜÔºâÁöÑË∑ØÁ∫øÔºåÂπ∂ÂÖÅËÆ∏ÂØπËØùÂú®ÈóÆÈ¢òËß£ÂÜ≥‰πãÂâçËá™‰∏ªÊåÅÁª≠ËøõË°å„ÄÇÊúâ‰∫õÂØπËØùÂ∞Ü‰ªé‰ª£ÁêÜ a->b->c->d ËøõË°åÔºåËÄåÂÖ∂‰ªñÁöÑÂ∞ÜÊòØ b->a->d->c„ÄÇËøô‰ΩøÂæó AutoGen ËÉΩÂ§üÂú®‰∏çÈúÄË¶Å‰∏∫ÊØèÁßçÂú∫ÊôØÂà∂ÂÆöÊòéÁ°ÆËßÑÂàôÁöÑÊÉÖÂÜµ‰∏ãËß£ÂÜ≥ÂêÑÁßç‰∏çÂêåÁöÑÈóÆÈ¢ò„ÄÇ\n3. AutoGen ËÉΩÂ§ü‰ªéÈîôËØØ‰∏≠ÊÅ¢Â§ç„ÄÇ‰æãÂ¶ÇÔºåÊàëÂàõÂª∫‰∫Ü‰∏Ä‰∏™Âü∫‰∫é AutoGen ÁöÑÊúçÂä°ÔºåËØ•ÊúçÂä°Âêë‰∏Ä‰∏™ API ÂèëÂá∫ËØ∑Ê±Ç„ÄÇÊúâÊó∂ÔºåAPI ËØ∑Ê±ÇÂõ†‰∏∫Êú™ËÉΩÊ≠£Á°ÆÂèëÈÄÅÊï∞ÊçÆËÄåÂá∫Èîô„ÄÇAutoGen GroupChat ‰∏çÊñ≠Â∞ùËØï‰∏çÂêåÁöÑÊñπÊ≥ïÔºåÁõ¥Âà∞ÊàêÂäü„ÄÇÊúâÊó∂ÔºåËøôÈúÄË¶Å 4 Ê¨°‰ª•‰∏äÁöÑÂ∞ùËØïÔºå‰ΩÜÊàëÁöÑ Planner ‰ª£ÁêÜÊ≤°ÊúâÊîæÂºÉ‚Äî‚ÄîÂè™ÊòØËá™‰∏ªË∞ÉÊï¥‰ª•Â§ÑÁêÜ API Â§±Ë¥•Âπ∂Â∞ùËØïÊñ∞ÊñπÊ≥ï„ÄÇ\n4. AutoGen ‰ªé‰∏ÄÂºÄÂßãÂ∞±ÊèêÂá∫‰∫ÜÂ∞Ü `UserProxyAgent` ‰∏é `AssistantAgent` ÂàÜÁ¶ªÁöÑÊ¶ÇÂøµ„ÄÇËøô‰πü‰ΩøÊàë‰ª¨ËÉΩÂ§üËÆ©Áî®Êà∑‰ª£ÁêÜÁúüÊ≠£‰∏∫Áî®Êà∑‰ª£ÁêÜÔºåÂ¶Ç‰∏äÊâÄÁ§∫„ÄÇ\n5. AutoGen ÊòØ‰∏Ä‰∏™Áª¥Êä§ËâØÂ•ΩÁöÑÂ∫ì„ÄÇÊØèÂë®‰ªñ‰ª¨ÈÉΩ‰ºöÊ∑ªÂä†‰∏Ä‰∫õÊñ∞ÂäüËÉΩ„ÄÇ\n6. AutoGen ÈùûÂ∏∏ÂèØÊâ©Â±ï„ÄÇÈÄöËøá‰ªñ‰ª¨ÊûÑÂª∫Á±ªÁöÑÊñπÂºèÔºåÊÇ®ÂèØ‰ª•Ê†πÊçÆËá™Â∑±ÁöÑÂñúÂ•ΩËá™ÂÆö‰πâ‰ªª‰ΩïÂÜÖÂÆπ„ÄÇ\n7. AutoGen ËøòÊúâÂÖ∂‰ªñÊàë‰∏ç‰ΩøÁî®ÁöÑÂäüËÉΩÔºå‰ΩÜÂÖ∂‰ªñ‰∫∫ÂèØËÉΩ‰ºöËßâÂæóÂÆÉ‰ª¨ÊúâÂ∏ÆÂä©Ôºå‰æãÂ¶ÇÂ∏ÆÂä©ÊÇ®ËÆ°ÁÆóÂØπËØùÁöÑ‰ª§ÁâåÂíåÊàêÊú¨„ÄÅÁºìÂ≠òÁ≠â„ÄÇ\n\n"},{"lang":"zh","group":"blog","slug":"blog/alibabas-open-source-qwen-how-it-s-revolutionizing-ai-and-how-you-can-use-it-dcba8f687c97","frontmatter":{"title":"ÈòøÈáåÂ∑¥Â∑¥ÂºÄÊ∫ê QwenÔºöÂÆÉÂ¶Ç‰ΩïÂΩªÂ∫ïÊîπÂèò‰∫∫Â∑•Êô∫ËÉΩ‰ª•ÂèäÂ¶Ç‰Ωï‰ΩøÁî®ÂÆÉ","meta_title":"ÈòøÈáåÂ∑¥Â∑¥ÂºÄÊ∫ê QwenÔºöÂÆÉÂ¶Ç‰ΩïÂΩªÂ∫ïÊîπÂèò‰∫∫Â∑•Êô∫ËÉΩ‰ª•ÂèäÂ¶Ç‰Ωï‰ΩøÁî®ÂÆÉ","description":"ÈòøÈáåÂ∑¥Â∑¥ÊúÄËøëÂú® 2024 Âπ¥‰∫ëÊ†ñÂ§ß‰ºöÊúüÈó¥ÂºÄÊ∫ê‰∫Ü Qwen 2.5 Ê®°ÂûãÔºåÂú® AI È¢ÜÂüüÊéÄËµ∑‰∫ÜÊ≥¢Êæú„ÄÇË∂ÖËøá 100 ‰∏™‚Ä¶","date":"2024-10-26T00:26:25.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*I7QDwbLMzoJ_ORq5.jpg","categories":["Programming","Machine Learning","Natural Language Processing"],"author":"Rifx.Online","tags":["Qwen","multimodal","open-source","fine-tune","text-to-video"],"draft":false,"slug":"blog/alibabas-open-source-qwen-how-it-s-revolutionizing-ai-and-how-you-can-use-it-dcba8f687c97"},"content":"\nÈòøÈáåÂ∑¥Â∑¥ÊúÄËøëÂú®‰∫∫Â∑•Êô∫ËÉΩÈ¢ÜÂüüÂºïËµ∑‰∫ÜËΩ∞Âä®ÔºåÂú®2024Âπ¥ Apsara Â§ß‰ºö‰∏äÂºÄÊ∫ê‰∫ÜÂÖ∂ **Qwen 2.5** Ê®°Âûã„ÄÇQwen Êã•ÊúâË∂ÖËøá 100 ‰∏™Ê®°ÂûãÔºåÊ∂µÁõñËØ≠Ë®Ä„ÄÅËßÜËßâ„ÄÅÈü≥È¢ëÂíå‰ª£Á†ÅÁ≠âÂ§öÁßçÊ®°ÊÄÅÔºå‰ΩøÂÖ∂Êàê‰∏∫ÊúÄÂÖ®Èù¢ÁöÑÂºÄÊ∫ê‰∫∫Â∑•Êô∫ËÉΩËß£ÂÜ≥ÊñπÊ°à‰πã‰∏Ä„ÄÇÊ≠§Ê¨°ÂèëÂ∏ÉÈÄöËøáÊèê‰æõÂ§öÊ†∑ÂåñÂ∫îÁî®ÁöÑÂ∑•ÂÖ∑ÔºåËµãËÉΩÂºÄÂèëËÄÖÔºå‰ªéÊñáÊú¨Âà∞ËßÜÈ¢ëÁîüÊàêÂà∞ÂÆûÊó∂ÈóÆÁ≠î„ÄÇ\n\n\n\n## ÈòøÈáåÂ∑¥Â∑¥ Qwen Ê®°ÂûãÁöÑÂÖ≥ÈîÆÁâπÊÄß\n\n1. **Â§öÊ®°ÊÄÅËÉΩÂäõ**ÔºöQwen Ê®°ÂûãÂ§ÑÁêÜÂ§öÁßçËæìÂÖ•ÔºåÂåÖÊã¨ÊñáÊú¨„ÄÅÈü≥È¢ëÂíåËßÜËßâÊï∞ÊçÆ„ÄÇËøôÁßçÂ§öÊ®°ÊÄÅÊñπÊ≥ï‰ΩøÂÖ∂ÈÄÇÁî®‰∫éÂπøÊ≥õÁöÑË°å‰∏öÔºå‰ªéÂ™í‰ΩìÂíåÂ®±‰πêÂà∞Êú∫Âô®‰∫∫ÊäÄÊúØ„ÄÇ\n2. **ÂºÄÊ∫ê**ÔºöQwen ÂèØÂú® **Hugging Face** Âíå **ModelScope** Á≠âÂπ≥Âè∞‰∏äËé∑ÂèñÔºåÂ∑≤ÁªèË¢´‰∏ãËΩΩË∂ÖËøá 4000 ‰∏áÊ¨°ÔºåÂü∫‰∫éÂÖ∂Âü∫Á°ÄÊûÑÂª∫ÁöÑËá™ÂÆö‰πâÊ®°ÂûãË∂ÖËøá 50,000 ‰∏™„ÄÇ\n3. **Â¢ûÂº∫ÊÄßËÉΩ**ÔºöQwen2.5 ÂºïÂÖ•‰∫ÜÊîπËøõÁöÑËØ≠Ë®ÄÁêÜËß£„ÄÅÊï∞Â≠¶ÂíåÁºñÁ†ÅËÉΩÂäõÔºå‰∏éËØ•È¢ÜÂüüÁöÑÈ¢ÜÂÖàÊ®°ÂûãÁ´û‰∫â„ÄÇÈÄöËøáÈíàÂØπÁªìÊûÑÂåñÊï∞ÊçÆÁêÜËß£ÂíåÈïøÊñáÊú¨ÁîüÊàêÁ≠â‰ªªÂä°ÁöÑ‰ºòÂåñÊÄßËÉΩÔºåQwen ‰∏∫È´òÁ∫ß AI Â∫îÁî®ÊâìÂºÄ‰∫ÜÂ§ßÈó®„ÄÇ\n\n## Â¶Ç‰Ωï‰ΩøÁî®ÈòøÈáåÂ∑¥Â∑¥ÁöÑ Qwen\n\nÂºÄÂèëËÄÖÂíåÁªÑÁªáÂèØ‰ª•Âú® Hugging Face Á≠âÂπ≥Âè∞‰∏äËÆøÈóÆ Qwen Ê®°ÂûãÔºåÂÖ∑‰ΩìÂèØ‰ª•Ôºö\n\n* **ÂæÆË∞ÉÊ®°Âûã**Ôºö‰∏∫ÁâπÂÆöË°å‰∏öÂ∫îÁî®ÈáèË∫´ÂÆöÂà∂ QwenÔºå‰æãÂ¶ÇÂÆ¢Êà∑ÊúçÂä°„ÄÅËá™Âä®ÂåñÊàñËßÜÈ¢ëÂÜÖÂÆπÂàõ‰Ωú„ÄÇ\n* **‰∏éÂ∫îÁî®ÈõÜÊàê**ÔºöQwen ÁöÑÊñáÊú¨ËΩ¨ËßÜÈ¢ëÊ®°ÂûãÂèØ‰ª•ÈõÜÊàêÂà∞Â™í‰ΩìÂà∂‰ΩúÊµÅÁ®ã‰∏≠Ôºå‰ªéÈùôÊÄÅÂõæÂÉèÂíåÊñáÊú¨ÊèêÁ§∫ÁîüÊàêÂä®ÊÄÅÂÜÖÂÆπ„ÄÇ\n* **ÂºÄÂèë AI Âä©Êâã**ÔºöÂÄüÂä©Â¢ûÂº∫ÁöÑËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºåQwen ÂèØÁî®‰∫éÊú∫Âô®‰∫∫ÂíåËá™Âä®È©æÈ©∂Ê±ΩËΩ¶Ôºå‰ª•Â§ÑÁêÜËßÜÈ¢ëÊï∞ÊçÆÂπ∂ÊâßË°åÂÆûÊó∂‰ªªÂä°ÔºåÂ¶ÇÂØºËà™ÊàñÁâ©‰ΩìËØÜÂà´„ÄÇ\n\n**ÈÄöËøá Hugging Face ‰ΩøÁî® Qwen ÁöÑÁ§∫‰æã**Ôºö\n\n```python\nfrom transformers import QwenTokenizer, QwenModel\n\ntokenizer = QwenTokenizer.from_pretrained(\"qwen-2.5\")\nmodel = QwenModel.from_pretrained(\"qwen-2.5\")\n\ninput_text = \"What is the future of AI in healthcare?\"\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\noutputs = model(input_ids)\n```\n\nËøô‰ΩøÁî®Êà∑ËÉΩÂ§üËÆøÈóÆ Qwen Ê®°ÂûãÔºåËøêË°åÊé®ÁêÜÔºåÂπ∂Ê†πÊçÆÁâπÂÆöÈúÄÊ±ÇËøõË°åÂÆöÂà∂„ÄÇ\n\n## QwenÂú®ÂêÑË°å‰∏öÁöÑÂΩ±Âìç\n\n1. **Â™í‰Ωì‰∏éÂ®±‰πê**ÔºöÂá≠ÂÄüÊñ∞ÁöÑÊñáÊú¨Âà∞ËßÜÈ¢ëÂäüËÉΩÔºåQwenÂèØ‰ª•Ëá™Âä®‰ªé‰π¶Èù¢ËÑöÊú¨ÁîüÊàêËßÜÈ¢ëÔºåÈÄöËøáËá™Âä®ÂåñÁπÅÁêêÁöÑÂà∂‰Ωú‰ªªÂä°Êù•ÊîπÂèòÂàõÊÑè‰∫ß‰∏ö„ÄÇ\n2. **Êú∫Âô®‰∫∫ÊäÄÊúØ‰∏éËá™Âä®È©æÈ©∂ËΩ¶ËæÜ**ÔºöQwen‰∏≠Â¢ûÂº∫ÁöÑËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂèØ‰ª•Â∏ÆÂä©Êú∫Âô®‰∫∫ÁêÜËß£Áé∞ÂÆû‰∏ñÁïåÁéØÂ¢ÉÔºå‰ªéËÄåÂú®Ëá™Âä®È©æÈ©∂ÊàñÂà∂ÈÄ†‰∏≠ÂÅöÂá∫Êõ¥Â•ΩÁöÑÂÜ≥Á≠ñ„ÄÇ\n3. **ËΩØ‰ª∂ÂºÄÂèë**ÔºöÁî±QwenÈ©±Âä®ÁöÑÈòøÈáåÂ∑¥Â∑¥AIÂºÄÂèëÂ∑•ÂÖ∑Ëá™Âä®Âåñ‰∫Ü‰ª£Á†ÅÁîüÊàê„ÄÅË∞ÉËØïÂíåÈúÄÊ±ÇÂàÜÊûêÁ≠â‰ªªÂä°Ôºå‰ΩøÂºÄÂèë‰∫∫ÂëòËÉΩÂ§ü‰∏ìÊ≥®‰∫éÊõ¥È´òÂ±ÇÊ¨°ÁöÑÈóÆÈ¢òËß£ÂÜ≥„ÄÇ\n\n## ÁªìËÆ∫ÔºöÂºÄÊîæAIÂàõÊñ∞ÁöÑÊñ∞Á∫™ÂÖÉ\n\nÈÄöËøáÂºÄÊ∫êÂÖ∂Qwen 2.5Ê®°ÂûãÔºåÈòøÈáåÂ∑¥Â∑¥Ê≠£Âú®‰ΩøÂÖàËøõÁöÑAIÊäÄÊúØÂèòÂæóÊõ¥Âä†ÊôÆÂèä„ÄÇÂºÄÂèëËÄÖ„ÄÅÂàùÂàõ‰ºÅ‰∏öÂíåÂ§ßÂûã‰ºÅ‰∏öÈÉΩÂèØ‰ª•Âà©Áî®QwenÁöÑÂ§öÊ®°ÊÄÅÂíåÂÆûÊó∂ËÉΩÂäõÔºåÂú®‰ªéÂ™í‰ΩìÂà∞Ëá™Âä®È©æÈ©∂Ê±ΩËΩ¶Á≠âË°å‰∏öÊé®Âä®ÂàõÊñ∞„ÄÇÊó†ËÆ∫ÊÇ®ÊòØÂ∏åÊúõ‰∏∫ÁâπÂÆöÂ∫îÁî®ÂæÆË∞ÉÊ®°ÂûãÁöÑÂºÄÂèëËÄÖÔºåËøòÊòØÂ∞ÜAIÈõÜÊàêÂà∞Âü∫Á°ÄËÆæÊñΩ‰∏≠ÁöÑ‰ºÅ‰∏öÔºåQwenÈÉΩÊèê‰æõÂº∫Â§ßÁöÑÂ∑•ÂÖ∑Êù•Âä†ÈÄüËøõÊ≠•„ÄÇ\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/building-a-local-ai-powered-news-aggregator-with-ollama-swarm-and-duckduckgo-95aaf8b3ee41","frontmatter":{"title":"‰ΩøÁî® Ollama„ÄÅSwarm Âíå DuckDuckGo ÊûÑÂª∫Êú¨Âú∞ AI Êñ∞ÈóªËÅöÂêàÂô®","meta_title":"‰ΩøÁî® Ollama„ÄÅSwarm Âíå DuckDuckGo ÊûÑÂª∫Êú¨Âú∞ AI Êñ∞ÈóªËÅöÂêàÂô®","description":"Ê≤°ÊúâÊèê‰æõÂ≠óÂπï","date":"2024-10-24T17:47:43.000Z","image":"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*OHMOTk_WYGOxWHBsKqdpNQ.jpeg","categories":["Programming","Generative AI","Technology/Web"],"author":"Rifx.Online","tags":["Llama","Swarm","DuckDuckGo","News","Aggregator"],"draft":false,"slug":"blog/building-a-local-ai-powered-news-aggregator-with-ollama-swarm-and-duckduckgo-95aaf8b3ee41"},"content":"\n# ‰ΩøÁî®OllamaSwarmÂíåDuckDuckGoÊûÑÂª∫Êú¨Âú∞AIÈ©±Âä®ÁöÑÊñ∞ÈóªËÅöÂêàÂô®\n\n\n\nÂú®ÂΩì‰ªäÂø´ËäÇÂ•èÁöÑ‰∏ñÁïå‰∏≠ÔºåË∑ü‰∏äÁâπÂÆöÈ¢ÜÂüüÊúÄÊñ∞Êñ∞ÈóªÁöÑÊ≠•‰ºêÂèØËÉΩ‰ºöÂæàÂÖ∑ÊåëÊàòÊÄß„ÄÇÂ¶ÇÊûúÊàë‰ª¨ËÉΩÂ§üÂà©Áî®ÁîüÊàêÂºèAIÂíå‰ª£ÁêÜÁöÑÂäõÈáèÔºåÂàõÂª∫‰∏Ä‰∏™ÂÆåÂÖ®Âú®Êú¨Âú∞Êú∫Âô®‰∏äËøêË°åÁöÑ‰∏™ÊÄßÂåñÊñ∞ÈóªËÅöÂêàÂô®Âë¢ÔºüÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨Â∞ÜÊé¢ËÆ®Â¶Ç‰Ωï‰ΩøÁî®**Ollama**ÁöÑLlama 3.2Ê®°Âûã„ÄÅ**Swarm**ËøõË°å‰ª£ÁêÜÁºñÊéíÔºå‰ª•Âèä**DuckDuckGo**ËøõË°åÁΩëÁªúÊêúÁ¥¢Êù•ÊûÑÂª∫ËøôÊ†∑ÁöÑÁ≥ªÁªü„ÄÇ\n\n# Êú¨Âú∞AIÁöÑÂäõÈáè\n\nÈöèÁùÄÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÖ¥Ëµ∑ÔºåÊàë‰ª¨Áé∞Âú®ËÉΩÂ§üÂú®‰∏™‰∫∫ÁîµËÑë‰∏äËøêË°åÂ§çÊùÇÁöÑAIÁ≥ªÁªü„ÄÇËøô‰∏∫ÂàõÂª∫ÈíàÂØπÊàë‰ª¨ÁâπÂÆöÈúÄÊ±ÇÂÆöÂà∂ÁöÑÂ∑•ÂÖ∑ÂºÄËæü‰∫ÜÊó†ÈôêÂèØËÉΩ„ÄÇÊàë‰ª¨ÁöÑÊñ∞ÈóªËÅöÂêàÂô®Â∞±ÊòØËøô‰∏ÄÊΩúÂäõÁöÑÂÆåÁæé‰æãËØÅ„ÄÇ\n\n# Êàë‰ª¨Á≥ªÁªüÁöÑÁªÑÊàêÈÉ®ÂàÜ\n\n1. **Ollama with Llama 3.2**: ËøôÊòØÊàë‰ª¨Á≥ªÁªüÁöÑÊ†∏ÂøÉÔºå‰∏∫Êàë‰ª¨ÁöÑAI‰ª£ÁêÜÊèê‰æõÂä®Âäõ„ÄÇ\n2. **Swarm**: ‰∏Ä‰∏™‰ª£ÁêÜÁºñÊéíÊ°ÜÊû∂ÔºåÂÖÅËÆ∏Êàë‰ª¨ÂàõÂª∫ÂíåÁÆ°ÁêÜÂ§ö‰∏™AI‰ª£ÁêÜ„ÄÇ\n3. **DuckDuckGo Search**: Êèê‰æõÊúÄÊñ∞ÁöÑÁΩëÈ°µÊêúÁ¥¢ÁªìÊûúÔºåËÄå‰∏çË∑üË∏™Áî®Êà∑Êï∞ÊçÆ„ÄÇ\n\n# Â∑•‰ΩúÂéüÁêÜ\n\nÊàë‰ª¨ÁöÑÊñ∞ÈóªËÅöÂêàÂô®Áî±‰∏§‰∏™‰∏ªË¶ÅÁöÑAI‰ª£ÁêÜÁªÑÊàêÔºö\n\n1. **Êñ∞ÈóªÂä©Êâã**Ôºö‰ΩøÁî®DuckDuckGoÊêúÁ¥¢Ëé∑ÂèñÁâπÂÆö‰∏ªÈ¢òÁöÑÊúÄÊñ∞Êñ∞ÈóªÊñáÁ´†„ÄÇ\n2. **ÁºñËæëÂä©Êâã**ÔºöÂÆ°Êü•Âπ∂Á≤æÁÇºÊî∂ÈõÜÂà∞ÁöÑÊñ∞Èóª‰ª•‰æõÊúÄÁªàÂ±ïÁ§∫„ÄÇ\n\nËÆ©Êàë‰ª¨Êù•ÂàÜËß£‰∏Ä‰∏ãÂ∑•‰ΩúÊµÅÁ®ãÔºö\n\n# 1. ËÆæÁΩÆÁéØÂ¢É\n\n\n```python\nollama pull llama3.2\n\nexport OPENAI_MODEL_NAME=llama3.2\nexport OPENAI_BASE_URL=http://localhost:11434/v1\nexport OPENAI_API_KEY=any\n\npip install git+https://github.com/openai/swarm.git duckduckgo-search\n```\nÊàë‰ª¨È¶ñÂÖàÂØºÂÖ•ÂøÖË¶ÅÁöÑÂ∫ìÂπ∂ÂàùÂßãÂåñÊàë‰ª¨ÁöÑ Swarm ÂÆ¢Êà∑Á´ØÔºö\n\n\n```python\nfrom duckduckgo_search import DDGS\nfrom swarm import Swarm, Agent\nfrom datetime import datetime\n\ncurrent_date = datetime.now().strftime(\"%Y-%m\")\nclient = Swarm()\n```\n\n# 2. ÂàõÂª∫Êñ∞ÈóªÊêúÁ¥¢ÂäüËÉΩ\n\nÊàë‰ª¨ÂÆö‰πâ‰∏Ä‰∏™ÂáΩÊï∞Êù•‰ΩøÁî® DuckDuckGo ÊêúÁ¥¢Êñ∞ÈóªÔºö\n\n```python\npythondef get_news_articles(topic):\n  ddg_api = DDGS()\n  results = ddg_api.text(f\"{topic} {current_date}\", max_results=5)\n  if results:\n      news_results = \"\\n\\n\".join([f\"Title: {result['title']}\\nURL: {result['href']}\\nDescription: {result['body']}\" for result in results])\n      return news_results\n  else:\n      return f\"Could not find news results for {topic}.\"\n```\n\n# 3. ÂÆö‰πâÊàë‰ª¨ÁöÑ AI ‰ª£ÁêÜ\n\nÊàë‰ª¨‰ΩøÁî® Ollama ÁöÑ Llama 3.2 Ê®°ÂûãÂàõÂª∫‰∏§‰∏™‰ª£ÁêÜÔºö\n\n\n```python\nnews_agent = Agent(\n  model=\"llama3.2\",\n  name=\"News Assistant\",\n  instructions=\"You provide the latest news articles for a given topic using DuckDuckGo search.\",\n  functions=[get_news_articles],\n)\n\neditor_agent = Agent(\n  model=\"llama3.2\",\n  name=\"Editor Assistant\",\n  instructions=\"You review and finalise the news article for publishing.\",\n)\n```\n\n# 4. ÂçèË∞ÉÂ∑•‰ΩúÊµÅÁ®ã\n\nÊàë‰ª¨ÂÆö‰πâ‰∏Ä‰∏™ÂáΩÊï∞Êù•ËøêË°åÊàë‰ª¨ÁöÑÊñ∞ÈóªËÅöÂêàÂ∑•‰ΩúÊµÅÁ®ãÔºö\n\n```python\ndef run_news_workflow(topic):\n  # Fetch news\n  news_response = client.run(\n      agent=news_agent,\n      messages=[{\"role\": \"user\", \"content\": f\"Get me the news about {topic} on {current_date}\"}],\n  )\n  raw_news = news_response.messages[-1][\"content\"]\n  \n  # Pass news to editor for final review\n  edited_news_response = client.run(\n      agent=editor_agent,\n      messages=[{\"role\": \"system\", \"content\": raw_news}],\n  )\n  print(f\"{edited_news_response.messages[-1]['content']}\")\n```\n\n# 5. ËøêË°åÁ≥ªÁªü\n\nÊúÄÂêéÔºåÊàë‰ª¨ÂèØ‰ª•ÈíàÂØπ‰ªª‰ΩïÊÑüÂÖ¥Ë∂£ÁöÑËØùÈ¢òËøêË°åÊàë‰ª¨ÁöÑÊñ∞ÈóªËÅöÂêàÂô®Ôºö\n\n\n```python\nrun_news_workflow(\"AI in Drug Discovery\")\n```\n\n# ÂÆåÊï¥‰ª£Á†Å : app.py\n\n\n```python\nfrom duckduckgo_search import DDGS\nfrom swarm import Swarm, Agent\nfrom datetime import datetime\n\ncurrent_date = datetime.now().strftime(\"%Y-%m\")\n\n# ÂàùÂßãÂåñ Swarm ÂÆ¢Êà∑Á´Ø\nclient = Swarm()\n\n# 1. ÂàõÂª∫‰∫íËÅîÁΩëÊêúÁ¥¢Â∑•ÂÖ∑\n\ndef get_news_articles(topic):\n    print(f\"Ê≠£Âú®‰∏∫ {topic} ËøõË°å DuckDuckGo Êñ∞ÈóªÊêúÁ¥¢...\")\n    \n    # DuckDuckGo ÊêúÁ¥¢\n    ddg_api = DDGS()\n    results = ddg_api.text(f\"{topic} {current_date}\", max_results=5)\n    if results:\n        news_results = \"\\n\\n\".join([f\"Ê†áÈ¢ò: {result['title']}\\nÁΩëÂùÄ: {result['href']}\\nÊèèËø∞: {result['body']}\" for result in results])\n        return news_results\n    else:\n        return f\"Êú™ËÉΩÊâæÂà∞ÂÖ≥‰∫é {topic} ÁöÑÊñ∞ÈóªÁªìÊûú„ÄÇ\"\n    \n# 2. ÂàõÂª∫ AI ‰ª£ÁêÜ\n\ndef transfer_to_editor_assistant(raw_news):\n    print(\"Â∞ÜÊñáÁ´†‰º†ÈÄíÁªôÁºñËæëÂä©Êâã...\")\n    return editor_agent.run({\"role\": \"system\", \"content\": raw_news})\n\n# Êñ∞Èóª‰ª£ÁêÜ‰ª•Ëé∑ÂèñÊñ∞Èóª\nnews_agent = Agent(\n    model=\"llama3.2\",\n    name=\"Êñ∞ÈóªÂä©Êâã\",\n    instructions=\"ÊÇ®Êèê‰æõÊúâÂÖ≥ÁªôÂÆö‰∏ªÈ¢òÁöÑÊúÄÊñ∞Êñ∞ÈóªÊñáÁ´†Ôºå‰ΩøÁî® DuckDuckGo ÊêúÁ¥¢„ÄÇ\",\n    functions=[get_news_articles],\n)\n\n# ÁºñËæë‰ª£ÁêÜ‰ª•ÁºñËæëÊñ∞Èóª\neditor_agent = Agent(\n    model=\"llama3.2\",\n    name=\"ÁºñËæëÂä©Êâã\",\n    instructions=\"ÊÇ®ÂÆ°ÈòÖÂπ∂ÊúÄÁªàÁ°ÆÂÆöÊñ∞ÈóªÊñáÁ´†‰ª•‰æõÂèëÂ∏É„ÄÇ\",\n)\n\n# 3. ÂàõÂª∫Â∑•‰ΩúÊµÅÁ®ã\n\ndef run_news_workflow(topic):\n    print(\"ËøêË°åÊñ∞Èóª‰ª£ÁêÜÂ∑•‰ΩúÊµÅÁ®ã...\")\n    \n    # Á¨¨‰∏ÄÊ≠•: Ëé∑ÂèñÊñ∞Èóª\n    news_response = client.run(\n        agent=news_agent,\n        messages=[{\"role\": \"user\", \"content\": f\"Ëé∑ÂèñÂÖ≥‰∫é {topic} Âú® {current_date} ÁöÑÊñ∞Èóª\"}],\n    )\n    raw_news = news_response.messages[-1][\"content\"]\n    print(f\"Ëé∑ÂèñÁöÑÊñ∞Èóª: {raw_news}\")\n    \n    # Á¨¨‰∫åÊ≠•: Â∞ÜÊñ∞Èóª‰º†ÈÄíÁªôÁºñËæëËøõË°åÊúÄÁªàÂÆ°Êü•\n    edited_news_response = client.run(\n        agent=editor_agent,\n        messages=[{\"role\": \"system\", \"content\": raw_news}],\n    )\n    print(f\"{edited_news_response.messages[-1]['content']}\")\n\n\n# ËøêË°åÁªôÂÆö‰∏ªÈ¢òÁöÑÊñ∞ÈóªÂ∑•‰ΩúÊµÅÁ®ãÁ§∫‰æã\nrun_news_workflow(\"ËçØÁâ©ÂèëÁé∞‰∏≠ÁöÑ AI\")\n```\n\n# Á§∫‰æãËæìÂá∫\n\n\n```python\nRunning news Agent workflow...\nRunning DuckDuckGo news search for AI in Drug Discovery...\nFetched news: Here's a formatted answer based on the news articles:\n\n**ËçØÁâ©ÂèëÁé∞‰∏≠ÁöÑ‰∫∫Â∑•Êô∫ËÉΩÔºöÈù©ÂëΩÊÄßÁöÑËΩ¨Âèò**\n\n‰∫∫Â∑•Êô∫ËÉΩÔºàAIÔºâÂú®ËçØÁâ©ÂèëÁé∞‰∏≠ÁöÑ‰ΩúÁî®Ê†áÂøóÁùÄÂà∂ËçØÈ¢ÜÂüüÁöÑÈù©ÂëΩÊÄßËΩ¨Âèò„ÄÇAIÂà©Áî®Â§çÊùÇÁöÑÁÆóÊ≥ïËøõË°åËá™‰∏ªÂÜ≥Á≠ñÔºå‰ªéÊï∞ÊçÆÂàÜÊûê‰∏≠Â¢ûÂº∫‰∫∫Á±ªËÉΩÂäõÔºåËÄå‰∏çÊòØÂèñ‰ª£ÂÆÉ‰ª¨„ÄÇ\n\n**ÊåëÊàò‰∏éÂ±ÄÈôêÊÄß**\n\nÂ∞ΩÁÆ°ÊúâÁùÄ‰ª§‰∫∫ÊúüÂæÖÁöÑËøõÂ±ïÔºå‰ΩÜÂú®ËØ•È¢ÜÂüü‰∏≠‰ªçÁÑ∂Â≠òÂú®ÊåëÊàòÂíåÂ±ÄÈôêÊÄß„ÄÇËÆ∫Êñá„ÄäAIÂú®ËçØÁâ©ÂèëÁé∞‰∏≠ÁöÑ‰ΩúÁî®„ÄãÊé¢ËÆ®‰∫ÜËøô‰∫õÈóÆÈ¢òÔºåÂº∫Ë∞É‰∫ÜÈ´òË¥®ÈáèÊï∞ÊçÆÁöÑÂøÖË¶ÅÊÄß„ÄÅ‰º¶ÁêÜÈóÆÈ¢òÁöÑËß£ÂÜ≥‰ª•ÂèäÂØπÂü∫‰∫éAIÁöÑÊñπÊ≥ïÂ±ÄÈôêÊÄßÁöÑËÆ§ËØÜ„ÄÇ\n\n**AIÂú®ËçØÁâ©ÂèëÁé∞‰∏≠ÁöÑÂ∫îÁî®**\n\nAIÊúâÊΩúÂäõÂú®ËçØÁâ©ÂèëÁé∞„ÄÅËÆæËÆ°ÂíåÁ†îÁ©∂ËçØÁâ©Èó¥Áõ∏‰∫í‰ΩúÁî®‰∏≠ÂèëÊå•ÂÖ≥ÈîÆ‰ΩúÁî®„ÄÇAIÂú®ËçØÁâ©ÂèëÁé∞‰∏≠ÁöÑÂ∫îÁî®ÂåÖÊã¨Ôºö\n\n* Â§öÈù∂ÁÇπËçØÁêÜÂ≠¶ÔºöAIÂèØ‰ª•È¢ÑÊµãÂåñÂêàÁâ©ÂØπÂ§öÁßçÁñæÁóÖÁöÑÊúâÊïàÊÄß„ÄÇ\n* ÂåñÂ≠¶ÂêàÊàêÔºöAIÂèØ‰ª•‰ºòÂåñÂåñÂ≠¶ÂêàÊàêËøáÁ®ãÔºå‰ª•ÂÆûÁé∞Êõ¥Âø´ÂíåÊõ¥È´òÊïàÁöÑÁîü‰∫ß„ÄÇ\n* ËçØÁâ©ÈáçÂÆö‰ΩçÔºöAIÂèØ‰ª•ËØÜÂà´Áé∞ÊúâËçØÁâ©ÁöÑÊñ∞Áî®ÈÄî„ÄÇ\n* È¢ÑÊµãËçØÁâ©ÁâπÊÄßÔºöAIÂèØ‰ª•È¢ÑÊµãÂåñÂêàÁâ©ÁöÑÊïàÂäõ„ÄÅÊØíÊÄßÂíåÁâ©ÁêÜÂåñÂ≠¶ÁâπÊÄß„ÄÇ\n\n**ËçØÁâ©ÂèëÁé∞‰∏≠AIÁöÑÊú™Êù•**\n\nÈöèÁùÄAIÁöÑ‰∏çÊñ≠ÂèëÂ±ïÔºåÈ¢ÑËÆ°Â∞ÜÂØπÂà∂ËçØË°å‰∏ö‰∫ßÁîüÈáçÂ§ßÂΩ±Âìç„ÄÇAIÁöÑÊàêÂäüÂ∫îÁî®Â∞Ü‰æùËµñ‰∫éÈ´òË¥®ÈáèÊï∞ÊçÆÁöÑÂèØÁî®ÊÄß„ÄÅ‰º¶ÁêÜÈóÆÈ¢òÁöÑËß£ÂÜ≥‰ª•ÂèäÂØπÂü∫‰∫éAIÁöÑÊñπÊ≥ïÂ±ÄÈôêÊÄßÁöÑËÆ§ËØÜ„ÄÇ\n```\n\n# Êú¨Âú∞ AI Êñ∞ÈóªËÅöÂêàÁöÑÂ•ΩÂ§Ñ\n\n* **ÈöêÁßÅ**ÔºöÊâÄÊúâÂ§ÑÁêÜÈÉΩÂú®ÊÇ®ÁöÑÊú¨Âú∞Êú∫Âô®‰∏äËøõË°åÔºåÁ°Æ‰øùÊÇ®ÁöÑÊï∞ÊçÆÁïôÂú®ÊÇ®Ëá™Â∑±Êâã‰∏≠„ÄÇ\n* **ÂÆöÂà∂Âåñ**ÔºöÊÇ®ÂèØ‰ª•ËΩªÊùæ‰øÆÊîπ‰ª£ÁêÜÁöÑÊåá‰ª§ÊàñÊ∑ªÂä†Êñ∞ÁöÑ‰ª£ÁêÜ‰ª•Êª°Ë∂≥ÊÇ®ÁöÑÁâπÂÆöÈúÄÊ±Ç„ÄÇ\n* **ÊúÄÊñ∞‰ø°ÊÅØ**ÔºöÈÄöËøá‰ΩøÁî® DuckDuckGo ÊêúÁ¥¢ÔºåÊÇ®ÊÄªÊòØËÉΩËé∑ÂæóÂÖ≥‰∫éÊÇ®ÈÄâÊã©‰∏ªÈ¢òÁöÑÊúÄÊñ∞Êñ∞Èóª„ÄÇ\n* **AI È©±Âä®ÁöÑÁ≠ñÂ±ï**ÔºöÁºñËæëÂä©ÊâãÂ∏ÆÂä©Á≤æÁÇºÂíåÁªÑÁªáÊî∂ÈõÜÁöÑÊñ∞ÈóªÔºåÊèê‰æõÊõ¥Á≤æËá¥ÁöÑÊúÄÁªàËæìÂá∫„ÄÇ\n\n# ÁªìËÆ∫\n\nËøô‰∏™Êú¨Âú∞ÁöÑ‰∫∫Â∑•Êô∫ËÉΩÈ©±Âä®Êñ∞ÈóªËÅöÂêàÂô®Â±ïÁ§∫‰∫ÜÂ∞ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏éÁΩëÁªúÊêúÁ¥¢ËÉΩÂäõÁªìÂêàÁöÑÊΩúÂäõ„ÄÇÈÄöËøáÂà©Áî®OllamaÁöÑLlama 3.2Ê®°Âûã„ÄÅSwarmËøõË°å‰ª£ÁêÜÁºñÊéíÔºå‰ª•ÂèäDuckDuckGoËøõË°åÊêúÁ¥¢ÔºåÊàë‰ª¨ÂàõÂª∫‰∫Ü‰∏Ä‰∏™Âº∫Â§ßÁöÑÂ∑•ÂÖ∑ÔºåÂèØ‰ª•ËÆ©Êàë‰ª¨Âú®‰ªª‰ΩïÊÑüÂÖ¥Ë∂£ÁöÑËØùÈ¢ò‰∏ä‰øùÊåÅ‰ø°ÊÅØÁÅµÈÄöÔºåÂêåÊó∂Áª¥Êä§Êàë‰ª¨ÁöÑÈöêÁßÅÔºåÂπ∂ÂÆåÂÖ®Âú®Êú¨Âú∞ËÆ°ÁÆóÊú∫‰∏äËøêË°å„ÄÇ\n\nÈöèÁùÄ‰∫∫Â∑•Êô∫ËÉΩÁöÑ‰∏çÊñ≠ÂèëÂ±ïÔºåÂàõÂª∫‰∏™ÊÄßÂåñ„ÄÅ‰∫∫Â∑•Êô∫ËÉΩÈ©±Âä®Â∑•ÂÖ∑ÁöÑÂèØËÉΩÊÄßÂè™‰ºö‰∏çÊñ≠Êâ©Â§ß„ÄÇËøô‰∏™Êñ∞ÈóªËÅöÂêàÂô®Âè™ÊòØ‰∏Ä‰∏™ÂºÄÂßã‚Äî‚ÄîÊÉ≥Ë±°‰∏Ä‰∏ãÔºåÂà©Áî®Ëøô‰∫õÊäÄÊúØ‰Ω†ËøòÂèØ‰ª•ÊûÑÂª∫Âì™‰∫õÂÖ∂‰ªñÂàõÊñ∞Â∫îÁî®ÔºÅ\n\n# ÂèÇËÄÉÔºö\n\nSwarm Github : <https://github.com/openai/swarm>\n\nÂ¶ÇÊûúÊÇ®ËßâÂæóËøôÁØáÊñáÁ´†‰ø°ÊÅØ‰∏∞ÂØå‰∏îÊúâ‰ª∑ÂÄºÔºåÊàëÂ∞ÜÈùûÂ∏∏ÊÑüË∞¢ÊÇ®ÁöÑÊîØÊåÅÔºö\n\n* Âú®Medium‰∏ä‰∏∫ÂÆÉÁÇπËµûÂá†Ê¨° üëèÔºåÂ∏ÆÂä©ÂÖ∂‰ªñ‰∫∫ÂèëÁé∞ËøôÁØáÂÜÖÂÆπÔºàÊÇ®Áü•ÈÅìÊÇ®ÂèØ‰ª•ÁÇπËµûÂ§öËææ50Ê¨°ÂêóÔºüÔºâ„ÄÇÊÇ®ÁöÑÁÇπËµûÂ∞ÜÂ∏ÆÂä©Êõ¥Â§öËØªËÄÖ‰º†Êí≠Áü•ËØÜ„ÄÇ\n- ‰∏éÊÇ®ÁöÑAIÁà±Â•ΩËÄÖÂíå‰∏ì‰∏ö‰∫∫Â£´ÁΩëÁªúÂàÜ‰∫´„ÄÇ\n- Âú®LinkedIn‰∏ä‰∏éÊàëËÅîÁ≥ªÔºö<https://www.linkedin.com/in/manjunath-janardhan-54a5537/>\n\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/claude-3-5-sonnet-new-pioneering-the-future-of-ai-with-computer-control-capabilities-37a6ff9f9033","frontmatter":{"title":"Claude 3.5 SonnetÔºàÊñ∞ÔºâÔºöÂà©Áî®ËÆ°ÁÆóÊú∫ÊéßÂà∂ËÉΩÂäõÂºÄÊãì‰∫∫Â∑•Êô∫ËÉΩÁöÑÊú™Êù•","meta_title":"Claude 3.5 SonnetÔºàÊñ∞ÔºâÔºöÂà©Áî®ËÆ°ÁÆóÊú∫ÊéßÂà∂ËÉΩÂäõÂºÄÊãì‰∫∫Â∑•Êô∫ËÉΩÁöÑÊú™Êù•","description":"Anthropic ‰∫é 2024 Âπ¥ 10 Êúà 22 Êó•ÂèëÂ∏É‰∫ÜÂÖ∂ÊúÄÊñ∞ÁöÑ AI Ê®°Âûã Claude 3.5 Sonnet„ÄÇÊ≠§ÁâàÊú¨ÂºïÂÖ•‰∫ÜÈù©ÂëΩÊÄßÁöÑËÆ°ÁÆóÊú∫ÊéßÂà∂‚Ä¶‚Ä¶","date":"2024-10-27T13:57:00.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*n0NkOFbhUm7_fllJ","categories":["Programming","Technology","Generative AI"],"author":"Rifx.Online","tags":["Claude","Sonnet","automation","benchmarks","safety"],"draft":false,"slug":"blog/claude-3-5-sonnet-new-pioneering-the-future-of-ai-with-computer-control-capabilities-37a6ff9f9033"},"content":"\n\n\n\n\nAnthropic‰∫é2024Âπ¥10Êúà22Êó•ÂèëÂ∏É‰∫ÜÊúÄÊñ∞ÁöÑAIÊ®°ÂûãClaude 3.5 Sonnet„ÄÇÊ≠§Ê¨°ÂèëÂ∏ÉÂºïÂÖ•‰∫ÜÈù©ÂëΩÊÄßÁöÑËÆ°ÁÆóÊú∫ÊéßÂà∂ËÉΩÂäõÔºåÂπ∂Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÂÆûÁé∞‰∫ÜÊòæËëóÊîπËøõÔºå‰∏∫AIË°å‰∏öËÆæÂÆö‰∫ÜÊñ∞Ê†áÂáÜ„ÄÇ\n\n## Èù©ÂëΩÊÄßÁöÑËÆ°ÁÆóÊú∫ÊéßÂà∂ÔºöÊñ∞ÂâçÊ≤ø\n\nClaude 3.5 Sonnet ÁöÑÁ™ÅÂá∫ÁâπÁÇπÊòØÂÖ∂ËÉΩÂ§üÂÉè‰∫∫Á±ª‰∏ÄÊ†∑‰∏éËÆ°ÁÆóÊú∫ËøõË°å‰∫§‰∫í„ÄÇËøô‰∏ÄÁ™ÅÁ†¥ÊÄßÁöÑËÉΩÂäõ‰ΩøÂæó AI ÂèØ‰ª•Ôºö\n\n* ‰ΩøÁî®Èº†Ê†áÂíåÈîÆÁõòËæìÂÖ•ÂØºËà™Ê°åÈù¢ÁïåÈù¢\n* ‰∏éÂêÑÁßçÂ∫îÁî®Á®ãÂ∫èÂíåÁΩëÈ°µÊµèËßàÂô®ËøõË°å‰∫§‰∫í\n* ÊâßË°åÂ§çÊùÇÁöÑÂ§öÊ≠•È™§‰ªªÂä°\n* ÊâßË°åÊñá‰ª∂ÁÆ°ÁêÜÊìç‰Ωú\n* Ëá™Âä®ÂåñÈáçÂ§çÁöÑÂ∑•‰ΩúÊµÅÁ®ã\n\nËøô‰∏ÄËÆ°ÁÆóÊú∫ÊéßÂà∂ÂäüËÉΩÁõÆÂâçÂ§Ñ‰∫éÂÖ¨ÂºÄÊµãËØïÈò∂ÊÆµÔºå‰ª£Ë°®‰∫Ü AI Á≥ªÁªü‰∏éÊï∞Â≠óÁïåÈù¢‰∫§‰∫íÊñπÂºèÁöÑËåÉÂºèËΩ¨Âèò„ÄÇÂ∞ΩÁÆ°‰ªçÂ§Ñ‰∫éÂÆûÈ™åÈò∂ÊÆµÔºå‰ΩÜÊó©ÊúüÊµãËØïÊòæÁ§∫Âá∫ËâØÂ•ΩÁöÑÁªìÊûúÔºåClaude 3.5 Sonnet Âú®‰ªÖÊà™Âõæ‰ªªÂä°ÁöÑ OSWorld Âü∫ÂáÜÊµãËØï‰∏≠ÂæóÂàÜ‰∏∫ 14.9% ‚Äî ÊòæËëóÈ´ò‰∫é‰∏ã‰∏Ä‰∏™ÊúÄ‰Ω≥Á≥ªÁªüÁöÑ 7.8%„ÄÇ\n\n## Âü∫ÂáÜÁ™ÅÁ†¥ÊÄßËÉΩ\n\nÂçáÁ∫ßÂêéÁöÑÊ®°ÂûãÂú®Â§ö‰∏™ÊåáÊ†á‰∏äË°®Áé∞Âá∫ÊòæËëóÁöÑÊîπËøõÔºö\n\n## ÁºñÁ†ÅÂíåÊäÄÊúØ‰ªªÂä°\n\n* Âú®SWE-bench Verified‰∏äÁöÑÊÄßËÉΩ‰∏∫49%ÔºàËæÉ‰πãÂâçÁöÑ33.4%ÊúâÊâÄÊèêÂçáÔºâ\n* Âú®HumanEvalÁºñÁ†Å‰ªªÂä°‰∏≠ÁöÑÂæóÂàÜ‰∏∫93.7%\n* Âú®ËΩØ‰ª∂Â∑•Á®ãÊñπÈù¢ÁöÑË°®Áé∞‰ºò‰∫é‰∏ì‰∏öÁºñÁ†ÅÁ≥ªÁªü\n\n## Â≠¶ÊúØÂíåÊé®ÁêÜËÉΩÂäõ\n\n* 65% ÁöÑÁ†îÁ©∂ÁîüÁ∫ßÊé®ÁêÜ (GPQA-Diamond)\n* 78% ÁöÑÊú¨ÁßëÁ∫ßÁü•ËØÜ (MMLU Pro)\n* 78.3% ÁöÑÊï∞Â≠¶ÈóÆÈ¢òËß£ÂÜ≥ËÉΩÂäõ (MATH)\n\n## ÂïÜ‰∏öÂ∫îÁî®\n\n* 69.2% Âú®Èõ∂ÂîÆÈ¢ÜÂüü‰ªªÂä°‰∏ä (TAU-bench)\n* 46% Âú®Ëà™Á©∫È¢ÜÂüü‰ªªÂä°‰∏ä\n* 90.8% Âú®ÂõæË°®ÂàÜÊûê‰∏äÁöÑÂáÜÁ°ÆÁéá\n* 94.2% Âú®ÊñáÊ°£ÈóÆÁ≠î‰∏äÁöÑÂáÜÁ°ÆÁéá\n\n## ‰ºÅ‰∏öÈõÜÊàê‰∏éÂèØÁî®ÊÄß\n\nClaude 3.5 Sonnet ÂèØ‰ª•ÈÄöËøáÂ§ö‰∏™Âπ≥Âè∞ËÆøÈóÆÔºö\n\n* Anthropic API\n* Amazon Bedrock\n* Google Cloud‚Äôs Vertex AI\n\nÂåÖÊã¨ Asana„ÄÅCanva„ÄÅDoorDash Âíå Replit Âú®ÂÜÖÁöÑ‰∏ªË¶ÅÂÖ¨Âè∏Â∑≤ÁªèÂºÄÂßãÂú®ÂÖ∂Â∑•‰ΩúÊµÅÁ®ã‰∏≠ÂÆûÊñΩ Claude 3.5 Sonnet ÁöÑÂäüËÉΩÔºåÁâπÂà´ÊòØÂà©Áî®ÂÖ∂ËÆ°ÁÆóÊú∫ÊéßÂà∂ÂäüËÉΩÊù•Â§ÑÁêÜÂ§çÊùÇÁöÑËá™Âä®Âåñ‰ªªÂä°„ÄÇ\n\n## ÂÆûÈôÖÂ∫îÁî®\n\n## ËΩØ‰ª∂ÂºÄÂèë\n\n* Ëá™Âä®Âåñ‰ª£Á†ÅÊµãËØïÂíåË∞ÉËØï\n* Êô∫ËÉΩIDE‰∫§‰∫í\n* ‰ª£Á†ÅÂÆ°Êü•‰∏é‰ºòÂåñ\n* ÊñáÊ°£ÁîüÊàê\n\n## ÂÆ¢Êà∑ÊîØÊåÅ\n\n* È´òÁ∫ßËÅäÂ§©Êú∫Âô®‰∫∫ÂäüËÉΩ\n* ÂèØËßÜÂåñÊï∞ÊçÆËß£ËØª\n* Ëá™Âä®ÂåñÂ∑•ÂçïËß£ÂÜ≥\n* ÊµÅÁ®ãËá™Âä®Âåñ\n\n## ÂïÜ‰∏öËøêËê•\n\n* ÊñáÊ°£Â§ÑÁêÜ‰∏éÂàÜÊûê\n* ‰ªéËßÜËßâÊ∫êÊèêÂèñÊï∞ÊçÆ\n* Â∑•‰ΩúÊµÅËá™Âä®Âåñ\n* Â§çÊùÇÈóÆÈ¢òËß£ÂÜ≥\n\n## ÂÆâÂÖ®‰∏éË¥£‰ªª\n\nAnthropic Â∑≤ÂÆûÊñΩÂº∫ÊúâÂäõÁöÑÂÆâÂÖ®Êé™ÊñΩÁî®‰∫éËÆ°ÁÆóÊú∫ÊéßÂà∂ÂäüËÉΩÔºö\n\n* Êñ∞ÂàÜÁ±ªÂô®‰ª•ËØÜÂà´ÊΩúÂú®ÁöÑËØØÁî®\n* ‰∏ªÂä®ÁõëÊéßÁ≥ªÁªü\n* ÈôêÂà∂ÂØπÊïèÊÑüÊìç‰ΩúÁöÑËÆøÈóÆ\n* ÂÆöÊúüÂÆâÂÖ®ËØÑ‰º∞\n\n## Â±ïÊúõÊú™Êù•\n\nËôΩÁÑ∂Claude 3.5 SonnetÂú®‰∫∫Â∑•Êô∫ËÉΩËÉΩÂäõÊñπÈù¢‰ª£Ë°®‰∫ÜÈáçÂ§ßËøõÂ±ïÔºå‰ΩÜÈáçË¶ÅÁöÑÊòØË¶ÅÊ≥®ÊÑèÊüê‰∫õÂäüËÉΩÔºåÁâπÂà´ÊòØËÆ°ÁÆóÊú∫ÊéßÂà∂Ôºå‰ªçÂ§Ñ‰∫éÊó©ÊúüÈò∂ÊÆµ„ÄÇÊüê‰∫õÊìç‰ΩúÂ¶ÇÊªöÂä®„ÄÅÊãñÂä®ÂíåÁº©ÊîæÈù¢‰∏¥ÊåëÊàòÔºåAnthropicÈºìÂä±ÂºÄÂèëËÄÖÂú®Êé¢Á¥¢Ëøô‰∫õÊñ∞ÂäüËÉΩÊó∂Ôºå‰ªé‰ΩéÈ£éÈô©‰ªªÂä°ÂºÄÂßã„ÄÇ\n\nClaude 3.5 SonnetÁöÑÂèëÂ∏ÉÊ†áÂøóÁùÄ‰∫∫Â∑•Êô∫ËÉΩÂèëÂ±ïÁöÑ‰∏Ä‰∏™ÂÖ≥ÈîÆÊó∂ÂàªÔºåÂ∞ÜÂÖàËøõÁöÑÊé®ÁêÜËÉΩÂäõ‰∏éÂÆûÁî®ÁöÑËÆ°ÁÆóÊú∫ÊéßÂà∂ÂäüËÉΩÁõ∏ÁªìÂêà„ÄÇÈöèÁùÄÊäÄÊúØÁöÑ‰∏çÊñ≠ÂèëÂ±ïÔºåÊàë‰ª¨ÂèØ‰ª•ÊúüÂæÖÁúãÂà∞Êõ¥Â§öÂàõÊñ∞ÁöÑÂ∫îÁî®‰ª•Âèä‰∫∫Â∑•Êô∫ËÉΩÁ≥ªÁªü‰∏éÊàë‰ª¨ÁöÑÊï∞Â≠ó‰∏ñÁïå‰∫íÂä®ÊñπÂºèÁöÑÊîπËøõ„ÄÇ\n\n*Êú¨ÊñáÂü∫‰∫éAnthropic„ÄÅAWSÂíåÂêÑÁ±ªÊäÄÊúØÂêà‰Ωú‰ºô‰º¥ÁöÑÂÆòÊñπÂÖ¨ÂëäÂíåÊñáÊ°£„ÄÇÊúâÂÖ≥ÊúÄÊñ∞‰ø°ÊÅØÔºåËØ∑ÂèÇËÄÉAnthropicÁöÑÂÆòÊñπÊñáÊ°£„ÄÇ*\n\n"},{"lang":"zh","group":"blog","slug":"blog/claude-3-5-sonnet-v-s-gpt-4o-which-one-is-better-3b3675195bf9","frontmatter":{"title":"Claude 3.5 Sonnet V/S GPT-4OÔºöÂì™‰∏Ä‰∏™Êõ¥Â•Ω","meta_title":"Claude 3.5 Sonnet V/S GPT-4OÔºöÂì™‰∏Ä‰∏™Êõ¥Â•Ω","description":"2022 Âπ¥ 11 ÊúàÔºåOpenAI Êé®Âá∫‰∫Ü ChatGPT Ê®°ÂûãÔºåËØ•Ê®°ÂûãÂΩªÂ∫ïÊîπÂèò‰∫ÜÊàë‰ª¨ÊêúÁ¥¢Âíå‰∏é‰ø°ÊÅØ‰∫§‰∫íÁöÑÊñπÂºè„ÄÇÊòéÂπ¥ÔºåÂú®‚Ä¶","date":"2024-10-27T13:59:09.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*4MXLuSFfGwFkWWn0","categories":["Generative AI","Machine Learning","Natural Language Processing"],"author":"Rifx.Online","tags":["GPT-4o","Claude","multimodal","reasoning","code-generation"],"draft":false,"slug":"blog/claude-3-5-sonnet-v-s-gpt-4o-which-one-is-better-3b3675195bf9"},"content":"\n\n\nÂú®2022Âπ¥11ÊúàÔºåOpenAIÊé®Âá∫‰∫ÜChatGPTÔºåËøô‰∏ÄÊ®°ÂûãÂΩªÂ∫ïÊîπÂèò‰∫ÜÊàë‰ª¨ÊêúÁ¥¢Âíå‰∏é‰ø°ÊÅØ‰∫íÂä®ÁöÑÊñπÂºè„ÄÇÊ¨°Âπ¥3ÊúàÔºåÁî±ÂâçOpenAIÂëòÂ∑•ÂàõÂäûÁöÑÁæéÂõΩÂàùÂàõÂÖ¨Âè∏‚ÄúAnthropic‚ÄùÊé®Âá∫‰∫Ü‰ªñ‰ª¨Ëá™Â∑±ÁöÑAIÊ®°Âûã‚ÄúClaude‚Äù„ÄÇËá™ÂèëÂ∏É‰ª•Êù•ÔºåËøô‰∏§ÂÆ∂AIÂÖ¨Âè∏‰∏ÄÁõ¥Âú®Á´û‰∫âÔºå‰ª•ÈÄöËøáÂÖ∂AIÊ®°Âûã‰∏∫ÂÆ¢Êà∑Êèê‰æõÊúÄ‰Ω≥ÁöÑÂäüËÉΩÂíå‰ΩìÈ™å„ÄÇÊúÄËøëÔºåOpenAIÊé®Âá∫‰∫Ü‚ÄúGPT-4o‚ÄùÔºåËøôÊòØ‰∏Ä‰∏™‰ª§‰∫∫ÊÉäÂèπÁöÑÊ®°ÂûãÔºåËÉΩÂ§üÂá∫Ëâ≤Âú∞Â§ÑÁêÜÊñá‰ª∂„ÄÅËØ≠Èü≥ÂíåËßÜÈ¢ëÊï∞ÊçÆ„ÄÇÂêåÊ†∑ÔºåClaudeÊé®Âá∫‰∫Ü‚ÄúClaude 3.5 Sonnet‚ÄùÔºå‰ªñ‰ª¨Â£∞Áß∞ËøôÊòØÊúÄÂÖàËøõÁöÑAIÊ®°ÂûãÔºåËÉΩÂ§üÂ§ÑÁêÜÂ§çÊùÇ‰ªªÂä°„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨Â∞ÜÁ°ÆÂÆöClaude 3.5 SonnetÂíåGPT-4o‰πãÈó¥Âì™‰∏™Êõ¥Â•ΩÔºåÂπ∂ÊØîËæÉÂÖ∂Âú®Áõ∏ÂêåËæìÂÖ•‰∏ãÁöÑÂäüËÉΩÂíåËæìÂá∫Ôºå‰ª•Ê£ÄÊü•Âì™‰∏™Êõ¥ÈÄÇÂêàÊÇ®„ÄÇ\n\n## ËÉΩÂäõÂíåÁâπÊÄß\n\n### GPT-4o\n\n\n\nGPT-4o ÊòØ OpenAI ÊúÄÊñ∞Êé®Âá∫ÁöÑ LLM„ÄÇ‚Äúo‚Äù ‰ª£Ë°® omniÔºåÊÑè‰∏∫Êãâ‰∏ÅËØ≠‰∏≠ÁöÑ‚ÄúÊØè‰∏Ä‰∏™‚Äù„ÄÇËØ•Ê®°ÂûãÂèØ‰ª•ÂàÜÊûêËØ≠Èü≥„ÄÅÂõæÂÉè„ÄÅËßÜÈ¢ëÂíåÊñá‰ª∂‰Ωú‰∏∫ËæìÂÖ•ÔºåÂπ∂Áõ∏Â∫îÂú∞ÂÅöÂá∫ÂõûÂ∫î„ÄÇÂÆÉÂèØ‰ª•Êé•ÂèóËØ≠Èü≥ËæìÂÖ•ÔºåÂπ∂‰ª•‰∏çÂêåËßíËâ≤ÁöÑÂ£∞Èü≥ËæìÂá∫ÔºåÂåÖÊã¨ËØ≠Ë∞É„ÄÅÊÉÖÊÑüÁ≠â„ÄÇÊï¥‰∏™ËøáÁ®ã‰∏é‰∫∫Á±ªÂØπËØùÁöÑÂª∂ËøüÁõ∏ÂΩì‰ΩéÔºåÂπ≥Âùá‰∏∫ 0.32 ÁßíÔºåËÄåÂÖ∂‰ªñËØ≠Èü≥Ê®°ÂûãÂàô‰∏∫ 2.8 Áßí„ÄÇÂÆÉËøòÂÖÅËÆ∏Áî®Êà∑ÁîüÊàê‰π¶Èù¢ÂÜÖÂÆπÔºåÂ¶ÇÊñáÁ´†„ÄÅÂçöÂÆ¢„ÄÅ‰∫ßÂìÅÊèèËø∞„ÄÅ‰∏çÂêåÁºñÁ®ãËØ≠Ë®ÄÁöÑ‰ª£Á†Å„ÄÅÊï∞ÊçÆÂàÜÊûê„ÄÅÂõæË°®Á≠â„ÄÇÊ≠§Â§ñÔºåGPT-4o ËøòÂèØ‰ª•ÂàÜÊûêÂõæÂÉèÂíåËßÜÈ¢ëÔºå‰ΩøËØ•Ê®°ÂûãÂèØ‰ª•ÂÖÖÂΩìËØ≠Ë®ÄÁøªËØëÂô®„ÄÅ‰∏™‰∫∫Âä©ÁêÜ„ÄÅËôöÊãüÊïôÂ∏àÊàñË¥≠Áâ©Âä©Êâã„ÄÇÂÆÉËøòÂèØ‰ª•Áî®‰∫éÂåªÂ≠¶„ÄÅÂ∑•Á®ã„ÄÅÂÜõ‰∫ãÁ≠âÈ¢ÜÂüü„ÄÇË¶Å‰ΩøÁî®Ê≠§ÂäüËÉΩÔºåGPT-4o ÂèØ‰ª•‰ΩøÁî®Áî®Êà∑ÁöÑÊëÑÂÉèÂ§¥Ëé∑ÂèñÂÆûÊó∂ËßÜÂõæÔºåÂπ∂Âú®ËØ≠Èü≥Ê®°Âºè‰∏ãÁõ∏Â∫îÂú∞ÂõûÂ∫î„ÄÇÂÆÉËøòÂèØ‰ª•ËÆøÈóÆÊÇ®ÁöÑËÆ°ÁÆóÊú∫Â±èÂπïÔºåÂπ∂ÊèèËø∞Â±èÂπï‰∏äÊòæÁ§∫ÁöÑÂÜÖÂÆπÔºåÁî®Êà∑ÂèØ‰ª•ËØ¢ÈóÆ‰∏éÂ±èÂπï‰∏äÊòæÁ§∫ÁöÑÂÜÖÂÆπÁõ∏ÂÖ≥ÁöÑÈóÆÈ¢ò„ÄÇ\n\n*‰æãÂ¶ÇÔºåÁî®Êà∑ÂèØ‰ª•Âú®Â±èÂπï‰∏äÂêØÁî®ËØ•Ê®°ÂûãÔºåÊâìÂºÄ VS ‰ª£Á†ÅÔºåÂπ∂ÊèêÁ§∫Ê®°ÂûãÂÖÖÂΩìÁºñÁ†ÅÂä©ÊâãÔºå‰ª•Ëé∑ÂèñÁºñÁ†ÅÈóÆÈ¢òÁöÑÁ≠îÊ°à„ÄÇÊàñËÄÖÔºåÊÇ®ÂèØ‰ª•ÂêØÁî®ÊëÑÂÉèÂ§¥ÔºåÂÖÖÂΩìÂÅ•Ë∫´ÊïôÁªÉÔºåÊ£ÄÊü•ÊÇ®ÊòØÂê¶ÂÅöÂæóÊ≠£Á°Æ„ÄÇ*\n\nËØ•Ê®°ÂûãÂÖ∑ÊúâÁã¨ÁâπÁöÑÂäüËÉΩÔºåÂ¶ÇÊï∞ÊçÆÂàÜÊûê„ÄÅ‰ª£Á†ÅËß£ÈáäÂô®ÂíåÂÆûÊó∂ÁΩëÈ°µÊµèËßàÔºå‰ΩøÂÖ∂‰∏éÁ´û‰∫âÂØπÊâã‰∏çÂêå„ÄÇËØ•Ê®°ÂûãËøòÊúâÂ§ßÈáèÁöÑ GPTsÔºåËøôÊòØ ChatGPT ÁöÑÂÆöÂà∂ÁâàÊú¨„ÄÇ\n\n### Claude 3.5 Sonnet\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*BSMcOpvWZ5lUm4Tl)\n\nClaude 3.5 Sonnet ÊòØÁî± Anthropic Êé®Âá∫ÁöÑ AI ËÅäÂ§©Êú∫Âô®‰∫∫„ÄÇÂÆÉÊòØ Claude AI Ê®°ÂûãÁ≥ªÂàóÁöÑÁ¨¨‰∏â‰ª£„ÄÇËøô‰∏ÄÊ®°ÂûãÂú®Â§ö‰∏™ËØÑ‰º∞‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰øùÊåÅ‰∫ÜÈ´òÂü∫ÂáÜÔºåÈÅøÂÖç‰∫ÜÂπªËßâÂíåÈîôËØØ‰ø°ÊÅØ„ÄÇËôΩÁÑ∂ÂÆÉ‰∏çÊîØÊåÅÂÉè GPT-4o ÈÇ£Ê†∑ÁöÑËØ≠Èü≥ÂíåËßÜÈ¢ëÂäüËÉΩÔºå‰ΩÜÂÆÉ‰ªçÁÑ∂ÂèØ‰ª•ÊâßË°åÊâÄÊúâÂü∫Êú¨‰ªªÂä°Ôºå‰æãÂ¶ÇÊñáÊú¨ÁîüÊàêÂíå‰∏çÂêåÁºñÁ®ãËØ≠Ë®ÄÁöÑ‰ª£Á†ÅÁîüÊàê„ÄÅÂ§¥ËÑëÈ£éÊö¥Á≠â„ÄÇÊ†πÊçÆ Anthropic ÁöÑÊä•ÂëäÔºåClaude 3.5 Sonnet ÊòØÂ∏ÇÂú∫‰∏äÊúÄÂ•ΩÁöÑËÆ°ÁÆóÊú∫ËßÜËßâÊ®°Âûã‰πã‰∏ÄÔºåÂèØ‰ª•Áî®‰∫éÂàÜÊûêÂõæË°®ÂíåÂõæÂΩ¢Ôºå‰ªéÂõæÂÉè‰∏≠ËΩ¨ÂΩïÊñáÊú¨Á≠â„ÄÇClaude Êã•Êúâ‰∏Ä‰∏™ÂÖàËøõÁöÑÂäüËÉΩÔºå‚ÄúArtifacts‚ÄùÔºåËøôÊòØ‰∏Ä‰∏™Âú®ÂØπËØù‰∏≠Âá∫Áé∞ÁöÑÁâπÊÆäÂºπÂá∫Á™óÂè£ÔºåÂÖÅËÆ∏Áî®Êà∑Êü•Áúã‰ª£Á†ÅÁâáÊÆµ„ÄÅÊñáÊú¨Êñá‰ª∂ÊàñÁΩëÁ´ôËÆæËÆ°ÔºåÂπ∂ÂÖÅËÆ∏‰ªñ‰ª¨ÂÆûÊó∂ÁºñËæëËæìÂá∫„ÄÇ\n\n*‰æãÂ¶ÇÔºåÁî®Êà∑ÂèØ‰ª•Âú®Â∑•‰ΩúÊµÅÁ®ã‰∏≠‰ΩøÁî®ËÆ°ÁÆóÊú∫ËßÜËßâÂíå artifacts„ÄÇÁî®Êà∑ÂèØ‰ª•Âú®Á∫∏‰∏äËøõË°åÁΩëÁ´ôËÆæËÆ°ÁöÑÂü∫Êú¨ÂéüÂûãÂà∂‰ΩúÔºåÂ∞ÜÊñá‰ª∂ÈôÑÂä†Âà∞ Claude 3.5 SonnetÔºåÂπ∂ÊèêÁ§∫ÂÆÉÊ†πÊçÆÂéüÂûãËÆæËÆ°ÁΩëÁ´ô„ÄÇÁîüÊàêÁöÑ‰ª£Á†ÅÂíåÁΩëÁ´ôËÆæËÆ°‰ºöÂá∫Áé∞Âú® artifacts ‰∏≠„ÄÇÁî®Êà∑ÂèØ‰ª•Ê†πÊçÆËá™Â∑±ÁöÑÈúÄÊ±ÇÁºñËæë‰ª£Á†ÅÂíåËÆæËÆ°„ÄÇÁî®Êà∑ËøòÂèØ‰ª•Â∞Ü‰ªñ‰ª¨ÁöÑÈ°πÁõÆÂÆûÊó∂ÂèëÂ∏ÉÂà∞‰∫íËÅîÁΩë‰∏ä„ÄÇ*\n\n## ÈÄêÈ°πÊØîËæÉ\n\nÂú®Êú¨ËäÇ‰∏≠ÔºåÊàë‰ª¨Â∞ÜÊ†πÊçÆÂ§çÊùÇÊé®ÁêÜÂíå‰ª£Á†ÅÁîüÊàêÁ≠âÂõ†Á¥†ÊØîËæÉËøô‰∏§‰∏™ LLMÔºåÊ£ÄÊü•ÂÆÉ‰ª¨Âú®Â§ÑÁêÜÂ§çÊùÇ‰ªªÂä°ÊñπÈù¢ÁöÑËÉΩÂäõÔºåÂπ∂ÁúãÁúãÂì™‰∏™Ê®°ÂûãÊõ¥Â•Ω„ÄÇ\n\n* **Á†îÁ©∂ÁîüÊ∞¥Âπ≥Êé®ÁêÜ(GPQA, Diamond)**Ê≠§Âõ†Á¥†ËØÑ‰º∞Ê®°ÂûãÂ§ÑÁêÜÁ†îÁ©∂ÁîüÊ∞¥Âπ≥ÊïôËÇ≤‰∏≠Â§çÊùÇ„ÄÅÈ´òÁ∫ßÊé®ÁêÜ‰ªªÂä°ÁöÑËÉΩÂäõ„ÄÇÂú®Ê≠§‰ªªÂä°‰∏≠ÔºåÁ†îÁ©∂‰∫∫ÂëòÂú® GPQA ÊµãËØï‰∏≠ÊØîËæÉÊ®°ÂûãÔºåËØ•ÊµãËØïÁî±‰∏ìÂÆ∂ËÆæËÆ°ÔºåÂåÖÂê´448‰∏™‰∏çÂêåÈ¢ÜÂüüÁöÑÈóÆÈ¢ò„ÄÇËøô‰∫õÈóÆÈ¢òÊòØ Google ProofÔºåÂõ†Ê≠§‰ªª‰Ωï‰∫∫ÈÉΩÊó†Ê≥ïÂú®Á∫øÊâæÂà∞ÂÆÉ‰ª¨„ÄÇClaude ÁöÑÂæóÂàÜÊé•Ëøë 59.4%ÔºåËÄå GPT-4o ÁöÑÂæóÂàÜ‰ªÖ‰∏∫ 53.6%„ÄÇËôΩÁÑ∂‰∏§‰∏™ÂæóÂàÜÁõ∏ÂØπÊé•ËøëÔºå‰ΩÜÊ≠£Â¶ÇÊàë‰ª¨ÊâÄËßÅÔºåClaude Âú®ÈúÄË¶ÅÈ´òÁ∫ßÂàÜÊûêÊÄùÁª¥ÁöÑ‰ªªÂä°‰∏≠ÂèØËÉΩÊòØÊõ¥Â•ΩÁöÑÈÄâÊã©Ôºå‰æãÂ¶ÇÁ†îÁ©∂ÂàÜÊûê„ÄÅÂ§çÊùÇÈóÆÈ¢òËß£ÂÜ≥ÂíåÈ´òÂ≠¶ÊúØÊ∞¥Âπ≥ÁöÑÈóÆÈ¢ò„ÄÇ\n* **Êú¨ÁßëÊ∞¥Âπ≥Áü•ËØÜ(MMLU)**MMLUÔºåÂç≥Â§ßËßÑÊ®°Â§ö‰ªªÂä°ËØ≠Ë®ÄÁêÜËß£ÔºåÊòØ‰∏Ä‰∏™Âü∫ÂáÜÔºåËß£Èáä‰ªª‰Ωï AI Ê®°ÂûãÂú®Êú¨ÁßëÊ∞¥Âπ≥‰∏äÂØπÂêÑ‰∏™Â≠¶ÁßëÁöÑÈÄöÁî®Áü•ËØÜÁêÜËß£„ÄÇClaude 3.5 Sonnet Âú®Ê≠§ÂÆûÈ™å‰∏≠ÁöÑÂæóÂàÜ‰∏∫ 88.3%ÔºåËÄå GPT-4o ÁöÑÂæóÂàÜ‰∏∫ 88.7%„ÄÇËøôË°®ÊòéËøô‰∏§‰∏™ LLM Âú®Â§ö‰∏™È¢ÜÂüüËøõË°å‰∫ÜËÆ≠ÁªÉÔºåÂπ∂ÂØπËøô‰∫õÈ¢ÜÂüüÊúâÊõ¥Ê∑±ÂÖ•ÁöÑÁêÜËß£„ÄÇËøô‰ΩøÂæó AI Ê®°ÂûãÊàê‰∏∫ÈÄöÁî®Áü•ËØÜ‰ªªÂä°„ÄÅÂ§ö‰∏™Â≠¶ÁßëÁöÑÂü∫Á°ÄËæÖÂØºÁ≠âÁöÑÂêàÈÄÇÂ∑•ÂÖ∑„ÄÇ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*A4w-tvsxcmFINaQT)\n\n* **‰ª£Á†Å(HumanEval)**HumanEval ÊòØ‰∏Ä‰∏™Âü∫ÂáÜÔºåËØÑ‰º∞Ê®°ÂûãÁîüÊàê„ÄÅÁêÜËß£ÂíåË∞ÉËØï‰ª£Á†ÅÁöÑËÉΩÂäõ„ÄÇÂú®Ëøô‰∏™Âü∫ÂáÜ‰∏≠ÔºåClaude 3.5 Sonnet ËææÂà∞‰∫Ü 92%ÁöÑÂæóÂàÜÔºåËÄå GPT-4o ÁöÑÂæóÂàÜ‰∏∫ 90.2%„ÄÇClaude 3.5 Sonnet Âú®Ê≠§‰ªªÂä°‰∏≠ÁöÑÁªìÊûúÈùûÂ∏∏Âá∫Ëâ≤ÔºåÂõ†‰∏∫ÂÆÉÊèê‰æõ‰∫ÜÊØî GPT-4o Êõ¥Â•ΩÁöÑÁºñÁ†ÅÁéØÂ¢É‚ÄúArtifacts‚ÄùÂíåÊõ¥Â•ΩÁöÑ‰ª£Á†ÅÁîüÊàê„ÄÇClaude ÂÖÅËÆ∏Áî®Êà∑Âú® Artifacts ÂºπÂá∫Á™óÂè£‰∏≠ËÆæËÆ°„ÄÅÁºñËæëÂíåËøêË°å‰ª£Á†Å„ÄÇÂú® Claude 3.5 Sonnet ÂèëÂ∏ÉÂêéÔºåÂ§ßÂÆ∂ÈÉΩÂú®ÂºÄÂèëÂ∑•ÂÖ∑„ÄÅÁΩëÁ´ôÂíåÂü∫Êú¨Ê∏∏ÊàèÔºåÂπ∂Âú®‰∫íËÅîÁΩë‰∏äÂàÜ‰∫´ÂÆÉ‰ª¨„ÄÇÂè¶‰∏ÄÊñπÈù¢ÔºåGPT-4o ÁöÑÂæóÂàÜ‰πü‰∏çÈîôÔºå‰ΩÜÂÆÉÁöÑÁïåÈù¢‰∏≠Ê≤°Êúâ‰ªª‰ΩïÁºñÁ†ÅÁéØÂ¢ÉÔºåÂõ†Ê≠§ÂºÄÂèë‰∫∫ÂëòÂøÖÈ°ªËä±Ë¥πÂæàÂ§öÁ≤æÂäõÔºåÂõ†‰∏∫ÂÆÉÁîüÊàêÁöÑ‰ª£Á†ÅÂæàÈöæËææÂà∞ÁªìÊûú„ÄÇ\n* **ÊñáÊú¨Êé®ÁêÜ(DROP, FLscore)**DROPÔºàÊÆµËêΩÁ¶ªÊï£Êé®ÁêÜÔºâÊòØ‰∏Ä‰∏™Âü∫ÂáÜÔºåÊµãÈáèÊ®°ÂûãÁêÜËß£Â§çÊùÇÊñáÊú¨‰ø°ÊÅØÁöÑËÉΩÂäõ„ÄÇÂú®Ëøô‰∏™ÊåëÊàò‰∏≠ÔºåClaude 3.5 Sonnet ÁöÑÂæóÂàÜ‰∏∫ 87.1%ÔºåËÄå GPT-4o ÁöÑÂæóÂàÜ‰∏∫ 83.4%„ÄÇËøôË°®Êòé Claude 3.5 Sonnet Âú®Ê∂âÂèäËØ¶ÁªÜÊñáÊú¨ÂàÜÊûê„ÄÅÊñáÊú¨ÂÆ°Êü•„ÄÅÂ§çÊùÇÈóÆÁ≠îÁ≥ªÁªüÁ≠â‰ªªÂä°Êó∂Êõ¥Â•Ω‰∏îÊõ¥ÊúâÊïà„ÄÇ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Kcy7sFb2FYpbfrwp)\n\n* **Êï∞Â≠¶ÈóÆÈ¢òËß£ÂÜ≥(MATH)**Ê≠§ÊµãËØïËØÑ‰º∞‰ªª‰Ωï AI Ê®°ÂûãËß£ÂÜ≥ÂêÑÁßçÊï∞Â≠¶ÈóÆÈ¢òÁöÑËÉΩÂäõ„ÄÇClaude 3.5 Sonnet ÁöÑÂæóÂàÜ‰ªÖ‰∏∫ 71.1%ÔºåËÄå GPT-4o ÁöÑÂæóÂàÜ‰∏∫ 76.6%„ÄÇËøô‰∫õÂæóÂàÜ‰Ωø GPT-4o Êàê‰∏∫Êï∞Â≠¶ÈóÆÈ¢òËß£ÂÜ≥‰ªªÂä°ÁöÑÊõ¥Â•ΩÊ®°ÂûãÔºåÂπ∂ÂèØÁî®‰∫éË¥¢Âä°Âª∫Ê®°„ÄÅÁßëÂ≠¶ËÆ°ÁÆóÂíåÈ´òÁ∫ßÊï∞ÊçÆÂàÜÊûêÁ≠âÊï∞Â≠¶ËÆ°ÁÆó„ÄÇ\n* **Â§öËØ≠Ë®ÄÊï∞Â≠¶(MSGM)**Ê≠§Âõ†Á¥†ÊèèËø∞‰ªª‰Ωï AI Ê®°ÂûãÂú®Â§öÁßçËØ≠Ë®Ä‰∏≠Ëß£ÂÜ≥Êï∞Â≠¶ÈóÆÈ¢òÁöÑËÉΩÂäõ„ÄÇ‰∏§‰∏™Ê®°ÂûãÁöÑÂæóÂàÜÊé•ËøëÔºöGPT-4o 90.5% Âíå Claude 3.5 Sonnet 91.6%„ÄÇËøôË°®Êòé‰∏§‰∏™Ê®°ÂûãË°®Áé∞Âá∫Ëâ≤ÔºåClaude Áï•ËÉú‰∏ÄÁ≠π„ÄÇËØ•ËÉΩÂäõÂØπ‰∫éÊïôËÇ≤Â∫îÁî®Êàñ‰ªª‰ΩïÈúÄË¶ÅË∑®ËØ≠Ë®ÄÈöúÁ¢çËøõË°åÊï∞Â≠¶Êé®ÁêÜ‰∫§ÊµÅÁöÑÂú∫ÊôØÁâπÂà´ÊúâÁî®„ÄÇ\n* **ËßÜËßâÈóÆÁ≠î(MMU/val)**Ê≠§Âõ†Á¥†ÊèèËø∞ LLM ÂàÜÊûêÂõæÂÉè‰∏≠ÂëàÁé∞ÁöÑ‰ø°ÊÅØÁöÑËÉΩÂäõ„ÄÇGPT-4o Âú®Ëøô‰∏ÄÂü∫ÂáÜ‰∏≠‰ª• 69.1% Ë∂ÖËøá Claude 3.5 Sonnet ÁöÑ 68.3%„ÄÇÂè¶‰∏ÄÊñπÈù¢ÔºåÂú®ÂàÜÊûêÊñáÊ°£‰∏≠ÁöÑÊñáÊú¨Êó∂ÔºåClaude 3.5 Sonnet ÁöÑÂæóÂàÜ‰∏∫ 95.2%ÔºåËÄå GPT-4o ÁöÑÂæóÂàÜ‰∏∫ 92.1%„ÄÇ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*xzjqBV2YL0lVFitX)\n\n* **ÂõæÂÉèÁîüÊàê**ÂõæÂÉèÁîüÊàêÊòØ LLM ‰ªéÊñáÊú¨ÁîüÊàêÂõæÂÉèÁöÑËÉΩÂäõ„ÄÇGPT-4o ÈõÜÊàê‰∫Ü DallE-2ÔºåÂèØ‰ª•ÈÄöËøáÊñáÊú¨ÁîüÊàêÂõæÂÉèÔºåÁªìÊûúÈùûÂ∏∏Âá∫Ëâ≤„ÄÇÂè¶‰∏ÄÊñπÈù¢ÔºåClaude 3.5 Sonnet Êó†Ê≥ïÂàõÂª∫‰ªª‰ΩïÂõæÂÉè„ÄÇÊ≠§ÂäüËÉΩËøòÂ∏ÆÂä© GPT-4o Êõ¥Â•ΩÂú∞ËÆæËÆ°ÁΩëÁ´ôÂíåÂèÇËÄÉÔºåÂõ†‰∏∫ÂÆÉÂú®ËÆ∏Â§öÂõæÂÉè‰∏äËøõË°å‰∫ÜËÆ≠ÁªÉ„ÄÇ\n* **Áü•ËØÜÊà™Ê≠¢**Âú®ËøôÈáåÔºå‰∏§‰∏™Ê®°ÂûãÈÉΩÂú®ÁâπÂÆöÊó•Êúü‰πãÂâçÁöÑÊúâÈôêÊï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜËÆ≠ÁªÉ„ÄÇClaude 3.5 Sonnet Âú® 2024 Âπ¥ 4 Êúà‰πãÂâçÁöÑÊï∞ÊçÆ‰∏äËøõË°å‰∫ÜËÆ≠ÁªÉÔºåËÄå GPT-4o ÂàôÂú® 2024 Âπ¥‰πãÂâçÁöÑÊï∞ÊçÆ‰∏äËøõË°å‰∫ÜËÆ≠ÁªÉ„ÄÇGPT-4o ÁöÑÁúüÊ≠£‰ºòÂäøÂú®‰∫éÂÆÉÂÖ∑ÊúâÂÆûÊó∂ÁΩëÈ°µÊµèËßàÂäüËÉΩÔºåËøôÊúâÂä©‰∫é LLM ÂÆöÊúüÂú®Êñ∞Êï∞ÊçÆ‰∏äËøõË°åËÆ≠ÁªÉ„ÄÇ\n\n## GPT-4o ÁöÑ‰ºòÁÇπÔºö\n\n* Â§ÑÁêÜËØ≠Èü≥„ÄÅÂõæÂÉèÂíåËßÜÈ¢ëËæìÂÖ•„ÄÇ\n* ÂÆûÊó∂ÁΩëÈ°µÊµèËßàËÉΩÂäõ„ÄÇ\n* Êõ¥Âø´ÁöÑÂìçÂ∫îÊó∂Èó¥ÔºàÂπ≥Âùá 0.32 ÁßíÔºâ„ÄÇ\n* Âú®Êï∞Â≠¶ÈóÆÈ¢òËß£ÂÜ≥ÊñπÈù¢Ë°®Áé∞‰ºòË∂ä„ÄÇ\n* ÂèØ‰ª•‰ΩøÁî® DALL-E 2 ÁîüÊàêÂõæÂÉè„ÄÇ\n\n## GPT-4o ÁöÑÁº∫ÁÇπÔºö\n\n* Á†îÁ©∂ÁîüÊ∞¥Âπ≥Êé®ÁêÜÁöÑÊÄßËÉΩÁ®ç‰Ωé„ÄÇ\n* Ê≤°ÊúâÂÜÖÁΩÆÁöÑÁºñÁ†ÅÁéØÂ¢É„ÄÇ\n* ÊñáÊ°£ËßÜËßâÈóÆÁ≠îÂæóÂàÜËæÉ‰Ωé„ÄÇ\n* ‰ª£Á†ÅÁîüÊàêËÉΩÂäõÁ®çÈÄä„ÄÇ\n* Âú®ËØ¶ÁªÜÊñáÊú¨ÂàÜÊûêÊñπÈù¢ÊïàÊûúËæÉÂ∑Æ„ÄÇ\n\n## Pros Claude 3.5 Sonnet:\n\n* Âú®Á†îÁ©∂ÁîüÁ∫ßÂà´ÁöÑÊé®ÁêÜÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤„ÄÇ\n* ‰ºòË∂äÁöÑ‰ª£Á†ÅÁîüÊàêÂíåÂÜÖÁΩÆÁöÑ‚ÄúÂ∑•‰ª∂‚ÄùÂäüËÉΩ„ÄÇ\n* Âú®ËØ¶ÁªÜÊñáÊú¨ÂàÜÊûê‰∏≠Ë°®Áé∞Êõ¥‰Ω≥„ÄÇ\n* Âú®ÊñáÊ°£ËßÜËßâÈóÆÁ≠î‰∏≠ÂæóÂàÜÊõ¥È´ò„ÄÇ\n* Âú®Â§öËØ≠Ë®ÄÊï∞Â≠¶ÊñπÈù¢Áï•Êúâ‰ºòÂäø„ÄÇ\n\n## Cons Claude 3.5 È¢ÇÔºö\n\n* Êó†Ê≥ïÂ§ÑÁêÜËØ≠Èü≥ÊàñËßÜÈ¢ëËæìÂÖ•„ÄÇ\n* Ê≤°ÊúâÂõæÂÉèÁîüÊàêËÉΩÂäõ„ÄÇ\n* Âú®ËßÜËßâÈóÆÁ≠îÊñπÈù¢ÊÄßËÉΩÁ®ç‰Ωé„ÄÇ\n* Êó†Ê≥ïËÆøÈóÆÂÆûÊó∂ÁΩëÁªú‰ø°ÊÅØ„ÄÇ\n* Âú®Êï∞Â≠¶ÈóÆÈ¢òËß£ÂÜ≥ÊñπÈù¢ËæÉÂº±„ÄÇ\n\n## ÁªìËÆ∫\n\nGPT-4o Âíå Claude 3.5 Sonnet Âú®ÂêÑÁßç‰ªªÂä°‰∏≠Â±ïÁé∞‰∫Ü‰ª§‰∫∫Âç∞Ë±°Ê∑±ÂàªÁöÑËÉΩÂäõÔºåÂêÑËá™ÊúâÂÖ∂‰ºòÂäø„ÄÇGPT-4o Âú®Â§öÊ®°ÊÄÅËæìÂÖ•„ÄÅÂÆûÊó∂‰ø°ÊÅØËÆøÈóÆÂíåÂõæÂÉèÁîüÊàêÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩøÂÖ∂Âú®Â§öÁßçÂ∫îÁî®‰∏≠ÈùûÂ∏∏ÁÅµÊ¥ª„ÄÇClaude 3.5 Sonnet Âú®Â§çÊùÇÊé®ÁêÜ„ÄÅ‰ª£Á†ÅÁîüÊàêÂíåËØ¶ÁªÜÊñáÊú¨ÂàÜÊûêÊñπÈù¢Ë°®Áé∞Á™ÅÂá∫ÔºåÂú®ÁâπÂÆöÁöÑÂ≠¶ÊúØÂíå‰∏ì‰∏öËÉåÊôØ‰∏ãÊèê‰æõ‰∫ÜÊõ¥‰ºòÁöÑÊÄßËÉΩ„ÄÇÈÄâÊã©Ëøô‰∏§ÁßçÊ®°ÂûãÂèñÂÜ≥‰∫éÂÖ∑‰ΩìÁöÑ‰ΩøÁî®Ê°à‰æãÂíåÊâÄÈúÄÁöÑÂäüËÉΩ„ÄÇÈöèÁùÄ‰∫∫Â∑•Êô∫ËÉΩÊäÄÊúØÁöÑËøõÊ≠•ÔºåÊàë‰ª¨ÂèØ‰ª•ÊúüÂæÖËøõ‰∏ÄÊ≠•ÁöÑÊîπËøõÂíåÈíàÂØπ‰∏çÂêåÈúÄÊ±ÇÁöÑ‰∏ì‰∏öÊ®°Âûã„ÄÇ\n\n"},{"lang":"zh","group":"blog","slug":"blog/explore-swarm-multi-agent-framework-locally-0e25ee617795","frontmatter":{"title":"Êú¨Âú∞Êé¢Á¥¢ Swarm Â§öÊô∫ËÉΩ‰ΩìÊ°ÜÊû∂","meta_title":"Êú¨Âú∞Êé¢Á¥¢ Swarm Â§öÊô∫ËÉΩ‰ΩìÊ°ÜÊû∂","description":"Swarm ÊòØ‰∏Ä‰∏™ÂÆûÈ™åÊÄßÁ§∫‰æãÊ°ÜÊû∂ÔºåÁî®‰∫éÊ®°ÊãüËΩªÈáèÁ∫ßÂ§ö‰ª£ÁêÜÊ°ÜÊû∂ÔºåÁî®‰∫éÊïôËÇ≤ÁõÆÁöÑ„ÄÇÈÄöÂ∏∏ÂÆÉ‰∏é Open‚Ä¶ ÈÖçÂêà‰ΩøÁî®","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*0ZVceq32bvkytC7HSIgmwA.png","categories":["Programming","Technology","Education"],"author":"Rifx.Online","tags":["Swarm","Multi-Agent","Framework","OpenAI","Ollama"],"draft":false,"slug":"blog/explore-swarm-multi-agent-framework-locally-0e25ee617795"},"content":"\n\n\n\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*zkpW8DDwh0TTYuHJVJbDaw.png)\n\nSwarm ÊòØ‰∏Ä‰∏™ÂÆûÈ™åÊÄßÊ†∑Êú¨Ê°ÜÊû∂ÔºåÁî®‰∫éÊ®°ÊãüËΩªÈáèÁ∫ßÂ§öÊô∫ËÉΩ‰ΩìÊ°ÜÊû∂ÔºåÊó®Âú®ÊïôËÇ≤ÁõÆÁöÑ„ÄÇÈÄöÂ∏∏ÂÆÉ‰∏é Open AI Key ‰∏ÄËµ∑‰ΩøÁî®Ôºå‰ΩÜÊàë‰ª¨ÂèØ‰ª•Êõ¥Êîπ‰∏∫‰ΩøÁî®Êú¨Âú∞ÁöÑ Ollama Êàñ LM Studio Ê®°Âûã„ÄÇ\n\n**ËÆæÁΩÆÔºö**\n\n\n```python\n## ÂàõÂª∫‰∏Ä‰∏™Êñ∞ÁöÑ Conda Êàñ Python ËôöÊãüÁéØÂ¢ÉÂπ∂ÊøÄÊ¥ªÂÆÉ\nconda install python==3.10\npip install torch openai\npip install transformers accelerate huggingface_hub\npip install git+ssh://git@github.com/openai/swarm.git\n```\n**‰ΩøÁî® Open AI KeyÔºö**\n\n\n```python\nexport OPEN_API_KEY = Your Key\n```\n**‰ΩøÁî® Ollama Êàñ LM Studio Êú¨Âú∞ LLM ‚Äî Êõ¥Êñ∞‰∏∫Êú¨Âú∞ URLÔºö**\n\n\n```python\n## Êü•Êâæ conda Êàñ python ËôöÊãüÁéØÂ¢É‰∏≠ÁöÑ site-packages/swarm\n## ÊâæÂà∞Êñá‰ª∂ core.py\nclass Swarm:\n    def __init__(self, client=None):\n        if not client:\n          # ÂÆûÈôÖ‰ª£Á†Å\n          #client = OpenAI()\n          # Â∞ÜÂü∫Á°Ä URL Âíå API Key Êõ¥Êñ∞‰∏∫ Ollama / LM Studio\n          # Âú®Êú¨ÊºîÁ§∫‰∏≠ÔºåÊàë‰ª¨‰ΩøÁî® LM Studio Âíå Llama 3.1\n          client = OpenAI(base_url=\"http://localhost:1234/v1\",api_key=\"random\")\n        self.client = client\n```\n**ÂÖãÈöÜ‰ªìÂ∫ìÔºö**\n\nÂÖãÈöÜ‰ªìÂ∫ì ‚Äî Âú®ËøôÈáåÊÇ®ÂèØ‰ª•ÊâæÂà∞‰∏çÂêåÁî®‰æãÁöÑÁ§∫‰æãÁõÆÂΩïÔºåÂ¶ÇÂü∫Êú¨„ÄÅËà™Á©∫ÂÖ¨Âè∏ÂíåÂ§©Ê∞îÁ≠â„ÄÇ\n\n\n```python\ngit clone https://github.com/openai/swarm.git\ncd swarm/examples\n```\n**Á§∫‰æã‰ª£Á†ÅÔºö**\n\n\n```python\nfrom swarm import Swarm, Agent\n\nclient = Swarm()\n\n\nit_agent = Agent(\n    name=\"IT Agent\",\n    instructions=\"You are an IT Expert with 10 Years of Experience.\",\n)\n\nsales_agent = Agent(\n    name=\"Sales Agent\",\n    instructions=\"You are a Sales Expert with 5 Years of Experience and knows about best selling mobiles.\",\n)\n\ndef transfer_to_sales_agent():\n    print(\"Sales agent in action\")\n    \"\"\"Transfer sales related questions to sales team immediately.\"\"\"\n    return sales_agent\n\ndef transfer_to_it_agent():\n    print(\"IT agent in action\")\n    \"\"\"Transfer IT users immediately.\"\"\"\n    return it_agent\n\nenglish_agent = Agent(\n    name=\"English Agent\",\n    instructions=\"You only speak English.\",\n    functions=[transfer_to_sales_agent,transfer_to_it_agent],\n)\n\n\nmessages = [{\"role\": \"user\", \"content\": \"How to install pandas lib?\"}]\nresponse = client.run(agent=english_agent, messages=messages)\n\nprint(response.messages[-1][\"content\"])\n\nmessages = [{\"role\": \"user\", \"content\": \"What are the best selling items?\"}]\nresponse = client.run(agent=english_agent, messages=messages)\n\nprint(response.messages[-1][\"content\"])\n```\n**ÂèÇËÄÉÊñáÁåÆÔºö**\n\n\n```python\nhttps://github.com/openai/swarm\n\nhttps://github.com/victorb/ollama-swarm/tree/main\n```\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*hCFJ4VQoT12yElYPXwXvWA.png)\n\nÈâ¥‰∫éËøôÊòØ‰∏Ä‰∏™ÂÆûÈ™åÊÄßÁâàÊú¨Ôºå‰ªçÊúâÂæàÂ§ßÁöÑÊîπËøõÁ©∫Èó¥„ÄÇËà™Á©∫‰ª£ÁêÜÁ§∫‰æã‰ª£Á†Å [swarm/examples/airline] ÈùûÂ∏∏ÊúâË∂£ÔºåÂõ†Ê≠§ÂèØ‰ª•Â∞ùËØïËøô‰∫õÁ§∫‰æã„ÄÇËØïËØïÁúãÔºåÂπ∂Âú®ËØÑËÆ∫‰∏≠ÂàÜ‰∫´ÊÇ®ÁöÑÁªèÈ™å„ÄÇË∞¢Ë∞¢„ÄÇ\n\n"},{"lang":"zh","group":"blog","slug":"blog/generating-structured-data-from-an-image-with-gpt-vision-and-langchain-34aaf3dcb215","frontmatter":{"title":"‰ΩøÁî® GPT Vision Âíå Langchain ‰ªéÂõæÂÉèÁîüÊàêÁªìÊûÑÂåñÊï∞ÊçÆ","meta_title":"‰ΩøÁî® GPT Vision Âíå Langchain ‰ªéÂõæÂÉèÁîüÊàêÁªìÊûÑÂåñÊï∞ÊçÆ","description":"Âú®ÂΩì‰ªä‰∏ñÁïåÔºåËßÜËßâÊï∞ÊçÆÈùûÂ∏∏‰∏∞ÂØåÔºå‰ªéÂõæÂÉè‰∏≠ÊèêÂèñÊúâÊÑè‰πâ‰ø°ÊÅØÁöÑËÉΩÂäõÂèòÂæóË∂äÊù•Ë∂äÈáçË¶Å‚Ä¶‚Ä¶","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*FPRRg85jYb7MrzXEpNWbmw.jpeg","categories":["Programming","Computer Vision","Natural Language Processing"],"author":"Rifx.Online","tags":["Langchain","GPT","vision","LLMs","structured"],"draft":false,"slug":"blog/generating-structured-data-from-an-image-with-gpt-vision-and-langchain-34aaf3dcb215"},"content":"\n\n\n\n\nÂú®ÂΩì‰ªäËøô‰∏™ËßÜËßâÊï∞ÊçÆ‰∏∞ÂØåÁöÑ‰∏ñÁïå‰∏≠Ôºå‰ªéÂõæÂÉè‰∏≠ÊèêÂèñÊúâÊÑè‰πâ‰ø°ÊÅØÁöÑËÉΩÂäõÂèòÂæóË∂äÊù•Ë∂äÈáçË¶Å„ÄÇLangchainÊòØ‰∏Ä‰∏™Âº∫Â§ßÁöÑÊ°ÜÊû∂ÔºåÁî®‰∫éÊûÑÂª∫Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂ∫îÁî®Á®ãÂ∫èÔºåÊèê‰æõ‰∫Ü‰∏ÄÂ•óÂ§öÂäüËÉΩÁöÑÂ∑•ÂÖ∑Êù•Â∫îÂØπËøô‰∏ÄÊåëÊàò„ÄÇÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨Â∞ÜÊé¢ËÆ®Â¶Ç‰Ωï‰ΩøÁî®Langchain‰ªéÂõæÂÉè‰∏≠ÊèêÂèñÁªìÊûÑÂåñ‰ø°ÊÅØÔºå‰æãÂ¶ÇËÆ°ÁÆó‰∫∫Êï∞ÂíåÂàóÂá∫‰∏ªË¶ÅÁâ©‰Ωì„ÄÇ\n\nÂú®Ê∑±ÂÖ•‰ª£Á†Å‰πãÂâçÔºåËÆ©Êàë‰ª¨ÂÖà‰∫ÜËß£‰∏Ä‰∏ã‰ªªÂä°ÁöÑËÉåÊôØ„ÄÇÊÉ≥Ë±°‰∏Ä‰∏ã‰Ω†Êúâ‰∏ÄÂº†Âú∫ÊôØÁöÑÂõæÂÉèÔºåÊØîÂ¶ÇÂüéÂ∏ÇË°óÈÅì„ÄÇ‰Ω†ÁöÑÁõÆÊ†áÊòØ‰ªéËøôÂº†ÂõæÂÉè‰∏≠ÊèêÂèñÊúâ‰ª∑ÂÄºÁöÑ‰ø°ÊÅØÔºåÂåÖÊã¨Âú®Âú∫ÁöÑ‰∫∫Êï∞ÂíåÂú∫ÊôØ‰∏≠ÁöÑ‰∏ªË¶ÅÁâ©‰ΩìÂàóË°®„ÄÇ\n\n## ÂÖ≥‰∫é Langchain\n\nLangchain ÊòØ‰∏Ä‰∏™ÁªºÂêàÊ°ÜÊû∂ÔºåÂÖÅËÆ∏ÂºÄÂèëËÄÖÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÂº∫Â§ßÂäüËÉΩÊûÑÂª∫Â§çÊùÇÁöÑÂ∫îÁî®Á®ãÂ∫è„ÄÇÂÆÉÊèê‰æõ‰∫ÜÊ®°ÂùóÂåñÂíåÂèØÊâ©Â±ïÁöÑÊû∂ÊûÑÔºå‰ΩøÂºÄÂèëËÄÖËÉΩÂ§üÂàõÂª∫ÈíàÂØπÁâπÂÆöÈúÄÊ±ÇÁöÑËá™ÂÆö‰πâÁÆ°ÈÅì„ÄÅ‰ª£ÁêÜÂíåÂ∑•‰ΩúÊµÅ„ÄÇ\n\nLangchain ÁÆÄÂåñ‰∫Ü LLM ÁöÑÈõÜÊàêÔºåÊèê‰æõ‰∫ÜÂ§ÑÁêÜÂêÑÁßçÊï∞ÊçÆÊ∫êÔºàÂåÖÊã¨ÊñáÊú¨„ÄÅÂõæÂÉèÂíåÁªìÊûÑÂåñÊï∞ÊçÆÔºâÁöÑÊäΩË±°ÂíåÂ∑•ÂÖ∑„ÄÇÂÆÉÊîØÊåÅÊù•Ëá™‰∏çÂêåÊèê‰æõÂïÜÁöÑÂπøÊ≥õ LLMÔºå‰æãÂ¶Ç OpenAI Âíå AnthropicÔºå‰ΩøÂæóÂú®Âçï‰∏™Â∫îÁî®Á®ãÂ∫è‰∏≠ËΩªÊùæÂàáÊç¢Ê®°ÂûãÊàñÁªÑÂêàÂ§ö‰∏™Ê®°ÂûãÂèòÂæóÁÆÄÂçï„ÄÇ\n\n## ÂáÜÂ§áÁéØÂ¢ÉÂπ∂ËÆæÁΩÆ OpenAI API ÂØÜÈí•\n\nË¶ÅË∑üÈöèÊú¨ÊïôÁ®ãÔºåÊÇ®ÈúÄË¶ÅÂÆâË£Ö Langchain„ÄÇÊÇ®ÂèØ‰ª•‰ΩøÁî® pip ÂÆâË£ÖÂÆÉÔºö\n\n```python\npip install langchain langchain_openai\n```\nË¶ÅÂú® Langchain ‰∏≠‰ΩøÁî® OpenAI ËØ≠Ë®ÄÊ®°ÂûãÔºåÊÇ®ÈúÄË¶Å‰ªé OpenAI Ëé∑Âèñ‰∏Ä‰∏™ API ÂØÜÈí•„ÄÇÂ¶ÇÊûúÊÇ®ËøòÊ≤°Êúâ API ÂØÜÈí•ÔºåÂèØ‰ª•Âú® OpenAI ÁΩëÁ´ô‰∏äÊ≥®ÂÜå‰∏Ä‰∏™ (<https://openai.com/api/>)„ÄÇ\n\n‰∏ÄÊó¶ÊÇ®Êã•Êúâ‰∫Ü API ÂØÜÈí•ÔºåÂèØ‰ª•Â∞ÜÂÖ∂ËÆæÁΩÆ‰∏∫Á≥ªÁªü‰∏≠ÁöÑÁéØÂ¢ÉÂèòÈáèÔºåÊàñËÄÖÁõ¥Êé•Âú®‰ª£Á†Å‰∏≠Êèê‰æõ„ÄÇ‰ª•‰∏ãÊòØÂ¶Ç‰ΩïÂ∞Ü API ÂØÜÈí•ËÆæÁΩÆ‰∏∫ÁéØÂ¢ÉÂèòÈáèÁöÑÁ§∫‰æãÔºö\n\n```python\nexport OPENAI_API_KEY=\"your_openai_api_key_here\"\n```\nÊàñËÄÖÔºåÊÇ®ÂèØ‰ª•Áõ¥Êé•Âú® Python ‰ª£Á†Å‰∏≠Êèê‰æõ API ÂØÜÈí•Ôºö\n\n```python\nimport os\nimport langchain\nos.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key_here\"\n```\nÂú®ËÆæÁΩÆÂ•Ω API ÂØÜÈí•ÂêéÔºåLangchain Â∞ÜËÉΩÂ§ü‰∏é OpenAI API ËøõË°åË∫´‰ªΩÈ™åËØÅÂπ∂‰ΩøÁî®‰ªñ‰ª¨ÁöÑËØ≠Ë®ÄÊ®°Âûã„ÄÇ\n\n## Âä†ËΩΩÂíåÁºñÁ†ÅÂõæÂÉè\n\nÂú®Êàë‰ª¨‰ΩøÁî® Langchain Â§ÑÁêÜÂõæÂÉè‰πãÂâçÔºåÊàë‰ª¨ÈúÄË¶Å‰ªéÊñá‰ª∂‰∏≠Âä†ËΩΩÂõæÂÉèÊï∞ÊçÆÔºåÂπ∂Â∞ÜÂÖ∂ÁºñÁ†Å‰∏∫ÂèØ‰ª•‰º†ÈÄíÁªôËØ≠Ë®ÄÊ®°ÂûãÁöÑÊ†ºÂºè„ÄÇ‰∏ãÈù¢ÁöÑ‰ª£Á†ÅÂÆö‰πâ‰∫Ü‰∏Ä‰∏™ÂáΩÊï∞ `load_image`ÔºåËØ•ÂáΩÊï∞Êé•Âèó‰∏Ä‰∏™ÂåÖÂê´ `image_path` ÈîÆÁöÑÂ≠óÂÖ∏ÔºåÂπ∂ËøîÂõû‰∏Ä‰∏™Êñ∞ÁöÑÂ≠óÂÖ∏ÔºåÂÖ∂‰∏≠ `image` ÈîÆÂåÖÂê´ÁºñÁ†Å‰∏∫ base64 Â≠óÁ¨¶‰∏≤ÁöÑÂõæÂÉèÊï∞ÊçÆ„ÄÇ\n\n```python\ndef load_image(inputs: dict) -> dict:\n    \"\"\"Load image from file and encode it as base64.\"\"\"\n    image_path = inputs[\"image_path\"]\n  \n    def encode_image(image_path):\n        with open(image_path, \"rb\") as image_file:\n            return base64.b64encode(image_file.read()).decode('utf-8')\n    image_base64 = encode_image(image_path)\n    return {\"image\": image_base64}\n```\n`load_image` ÂáΩÊï∞È¶ñÂÖà‰ªéËæìÂÖ•Â≠óÂÖ∏‰∏≠ÊèêÂèñ `image_path`„ÄÇÁÑ∂ÂêéÔºåÂÆÉÂÆö‰πâ‰∫Ü‰∏Ä‰∏™ÂµåÂ•óÂáΩÊï∞ `encode_image`ÔºåËØ•ÂáΩÊï∞‰ª•‰∫åËøõÂà∂Ê®°ÂºèÊâìÂºÄÂõæÂÉèÊñá‰ª∂ÔºåËØªÂèñÂÖ∂ÂÜÖÂÆπÔºåÂπ∂‰ΩøÁî® Python Ê†áÂáÜÂ∫ì‰∏≠ÁöÑ `base64.b64encode` ÂáΩÊï∞Â∞ÜÂÖ∂ÁºñÁ†Å‰∏∫ base64 Â≠óÁ¨¶‰∏≤„ÄÇ\n\n`load_image` ÂáΩÊï∞‰ΩøÁî®Êèê‰æõÁöÑ `image_path` Ë∞ÉÁî® `encode_image`ÔºåÂπ∂Â∞ÜÁªìÊûú base64 ÁºñÁ†ÅÂ≠óÁ¨¶‰∏≤Â≠òÂÇ®Âú® `image_base64` ÂèòÈáè‰∏≠„ÄÇÊúÄÂêéÔºåÂÆÉËøîÂõû‰∏Ä‰∏™Êñ∞ÁöÑÂ≠óÂÖ∏ÔºåÂÖ∂‰∏≠ `image` ÈîÆËÆæÁΩÆ‰∏∫ `image_base64`„ÄÇ\n\n‰∏∫‰∫ÜÂ∞ÜÊ≠§ÂáΩÊï∞ÈõÜÊàêÂà∞ Langchain ÊµÅÊ∞¥Á∫ø‰∏≠ÔºåÊàë‰ª¨ÂèØ‰ª•ÂàõÂª∫‰∏Ä‰∏™ `TransformChain`ÔºåËØ•ÈìæÊé•Âèó `image_path` ‰Ωú‰∏∫ËæìÂÖ•ÔºåÂπ∂ÁîüÊàê `image`Ôºàbase64 ÁºñÁ†ÅÂ≠óÁ¨¶‰∏≤Ôºâ‰Ωú‰∏∫ËæìÂá∫„ÄÇ\n\n```python\nload_image_chain = TransformChain(\n    input_variables=[\"image_path\"],\n    output_variables=[\"image\"],\n    transform=load_image\n)\n```\nÈÄöËøáËøôÁßçËÆæÁΩÆÔºåÊàë‰ª¨ÂèØ‰ª•ËΩªÊùæÂú∞Â∞ÜÂõæÂÉèÂä†ËΩΩÂíåÁºñÁ†Å‰Ωú‰∏∫Êõ¥Â§ß Langchain Â∑•‰ΩúÊµÅÁöÑ‰∏ÄÈÉ®ÂàÜÔºå‰ªéËÄå‰ΩøÊàë‰ª¨ËÉΩÂ§ü‰ΩøÁî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂ§ÑÁêÜËßÜËßâÊï∞ÊçÆÂíåÊñáÊú¨„ÄÇ\n\n## ÂÆö‰πâËæìÂá∫ÁªìÊûÑ\n\nÂú®Êàë‰ª¨ÊèêÂèñÂõæÂÉè‰ø°ÊÅØ‰πãÂâçÔºåÈúÄË¶ÅÂÆö‰πâÊàë‰ª¨Â∏åÊúõÊé•Êî∂ÁöÑËæìÂá∫ÁªìÊûÑ„ÄÇÂú®ËøôÁßçÊÉÖÂÜµ‰∏ãÔºåÊàë‰ª¨Â∞ÜÂàõÂª∫‰∏Ä‰∏™Âêç‰∏∫ `ImageInformation` ÁöÑ Pydantic Ê®°ÂûãÔºåÂÖ∂‰∏≠ÂåÖÊã¨ÂõæÂÉèÊèèËø∞ÂíåÊàë‰ª¨ÂèØËÉΩÊÉ≥Ë¶ÅÊèêÂèñÁöÑ‰ªª‰ΩïÂÖ∂‰ªñ‰ø°ÊÅØÁöÑÂ≠óÊÆµ„ÄÇ\n\n```python\nfrom langchain_core.pydantic_v1 import BaseModel, Field\n\nclass ImageInformation(BaseModel):\n \"\"\"Information about an image.\"\"\"\n image_description: str = Field(description=\"a short description of the image\")\n people_count: int = Field(description=\"number of humans on the picture\")\n main_objects: list[str] = Field(description=\"list of the main objects on the picture\")\n```\n\n## ËÆæÁΩÆÂõæÂÉèÊ®°Âûã\n\nÊé•‰∏ãÊù•ÔºåÊàë‰ª¨Â∞ÜÂàõÂª∫‰∏Ä‰∏™ÈìæÔºåÂ∞ÜÂõæÂÉèÂä†ËΩΩÂíåÁºñÁ†ÅÊ≠•È™§‰∏é LLM Ë∞ÉÁî®Ê≠•È™§ÁªìÂêàËµ∑Êù•„ÄÇÁî±‰∫é `ChatOpenAI` Ê®°ÂûãÂú®ÊàëÁöÑÁêÜËß£‰∏≠Âπ∂‰∏çÂÖ∑Â§áÂêåÊó∂Â§ÑÁêÜÊñáÊú¨ÂíåÂõæÂÉèËæìÂÖ•ÁöÑËÉΩÂäõÔºåÊàë‰ª¨Â∞ÜÂàõÂª∫‰∏Ä‰∏™ÂåÖË£ÖÈìæÊù•ÂÆûÁé∞Ëøô‰∏ÄÂäüËÉΩ„ÄÇ\n\n```python\nfrom langchain.chains import TransformChain\nfrom langchain_core.messages import HumanMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain import globals\nfrom langchain_core.runnables import chain\n\n## Set verbose\nglobals.set_debug(True)\n\n@chain\ndef image_model(inputs: dict) -> str | list[str] | dict:\n \"\"\"Invoke model with image and prompt.\"\"\"\n model = ChatOpenAI(temperature=0.5, model=\"gpt-4-vision-preview\", max_tokens=1024)\n msg = model.invoke(\n             [HumanMessage(\n             content=[\n             {\"type\": \"text\", \"text\": inputs[\"prompt\"]},\n             {\"type\": \"text\", \"text\": parser.get_format_instructions()},\n             {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{inputs['image']}\"}},\n             ])]\n             )\n return msg.content\n```\nÂú®Ëøô‰∏™‰ª£Á†ÅÁâáÊÆµ‰∏≠ÔºåÊàë‰ª¨ÂÆö‰πâ‰∫Ü‰∏Ä‰∏™Âêç‰∏∫ `image_model` ÁöÑÈìæÔºå‰ΩøÁî®Êèê‰æõÁöÑÊèêÁ§∫„ÄÅÊ†ºÂºèËØ¥ÊòéÂíåÂõæÂÉèË∞ÉÁî® `ChatOpenAI` Ê®°Âûã„ÄÇ`image_model` ÈìæÊé•Âèó‰∏Ä‰∏™ÂåÖÂê´ÊèêÁ§∫Âíå base64 ÁºñÁ†ÅÂõæÂÉèÂ≠óÁ¨¶‰∏≤ÁöÑÂ≠óÂÖ∏ `inputs`„ÄÇ\n\nÂú®ÈìæÂÜÖÈÉ®ÔºåÊàë‰ª¨ÂàõÂª∫‰∫Ü‰∏Ä‰∏™ `HumanMessage` ÂØπË±°ÔºåËØ•ÂØπË±°ÁªìÂêà‰∫ÜÊèêÁ§∫ÊñáÊú¨„ÄÅÊ†ºÂºèËØ¥ÊòéÂíåÂõæÂÉè URLÔºå‰ª•Êï∞ÊçÆ URI Ê†ºÂºèÂåñÔºåÂåÖÂê´ base64 ÁºñÁ†ÅÁöÑÂõæÂÉèÊï∞ÊçÆ„ÄÇÁÑ∂ÂêéÔºåÊàë‰ª¨‰ΩøÁî®Ëøô‰∏™ `HumanMessage` ÂØπË±°Ë∞ÉÁî® `ChatOpenAI` Ê®°ÂûãÔºå‰ΩøÁî®‰∏ìÈó®‰∏∫Ê∂âÂèäÊñáÊú¨ÂíåÂõæÂÉèÁöÑÂ§öÊ®°ÊÄÅ‰ªªÂä°ËÆæËÆ°ÁöÑ `gpt-4-vision-preview` Ê®°Âûã„ÄÇ\n\nËØ•Ê®°ÂûãÂ§ÑÁêÜÊñáÊú¨ÊèêÁ§∫ÂíåÂõæÂÉèÔºåÂπ∂ËøîÂõûËæìÂá∫„ÄÇ\n\n## Êï¥ÂêàÊâÄÊúâÂÜÖÂÆπ\n\nÁé∞Âú®Êàë‰ª¨Â∑≤ÁªèÊã•Êúâ‰∫ÜÊâÄÊúâÂøÖË¶ÅÁöÑÁªÑ‰ª∂ÔºåÊàë‰ª¨ÂèØ‰ª•ÂÆö‰πâ‰∏Ä‰∏™ÂáΩÊï∞Êù•ÂçèË∞ÉÊï¥‰∏™ËøáÁ®ãÔºö\n\n```python\nfrom langchain_core.output_parsers import JsonOutputParser\n\nparser = JsonOutputParser(pydantic_object=ImageInformation)\ndef get_image_informations(image_path: str) -> dict:\n   vision_prompt = \"\"\"\n   Given the image, provide the following information:\n   - A count of how many people are in the image\n   - A list of the main objects present in the image\n   - A description of the image\n   \"\"\"\n   vision_chain = load_image_chain | image_model | parser\n   return vision_chain.invoke({'image_path': f'{image_path}', \n                               'prompt': vision_prompt})\n```\nÂú®Ëøô‰∏™ÂáΩÊï∞‰∏≠ÔºåÊàë‰ª¨ÂÆö‰πâ‰∫Ü‰∏Ä‰∏™ÊèêÁ§∫ÔºåË¶ÅÊ±ÇLLMÊèê‰æõÂõæÂÉè‰∏≠‰∫∫Áâ©ÁöÑÊï∞ÈáèÂíå‰∏ªË¶ÅÁâ©‰ΩìÁöÑÂàóË°®„ÄÇÁÑ∂ÂêéÔºåÊàë‰ª¨ÂàõÂª∫‰∏Ä‰∏™ÈìæÔºåÂ∞ÜÂõæÂÉèÂä†ËΩΩÊ≠•È™§Ôºà`load\\_image\\_chain`Ôºâ„ÄÅLLMË∞ÉÁî®Ê≠•È™§Ôºà`image\\_model`ÔºâÂíåJSONËæìÂá∫Ëß£ÊûêÂô®Ôºà`parser`ÔºâÁªìÂêàÂú®‰∏ÄËµ∑„ÄÇÊúÄÂêéÔºåÊàë‰ª¨Áî®ÂõæÂÉèË∑ØÂæÑÂíåÊèêÁ§∫Ë∞ÉÁî®Ëøô‰∏™ÈìæÔºåÂáΩÊï∞ËøîÂõû‰∏Ä‰∏™ÂåÖÂê´ÊèêÂèñ‰ø°ÊÅØÁöÑÂ≠óÂÖ∏„ÄÇ\n\n## Á§∫‰æãÁî®Ê≥ï\n\nË¶Å‰ΩøÁî®Ê≠§ÂäüËÉΩÔºåÂè™ÈúÄÊèê‰æõÂõæÂÉèÊñá‰ª∂ÁöÑË∑ØÂæÑÔºö\n\n\n```python\nresult = get_image_informations(\"path/to/your/image.jpg\")\nprint(result)\n```\nËøôÂ∞ÜËæìÂá∫‰∏Ä‰∏™ÂåÖÂê´ËØ∑Ê±Ç‰ø°ÊÅØÁöÑÂ≠óÂÖ∏Ôºå‰æãÂ¶ÇÔºö\n\n\n```python\n{\n 'description': 'a view of a city showing cars waiting at a traffic light',\n 'people_count': 5,\n 'main_objects': ['car', 'building', 'traffic light', 'tree']\n}\n```\n\n## ÁªìËÆ∫\n\nLangchain Êèê‰æõ‰∫ÜÂº∫Â§ßÁöÑÂ∑•ÂÖ∑ÈõÜÔºåÁî®‰∫éÂ§ÑÁêÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂπ∂‰ªéÂêÑÁßçÊï∞ÊçÆÊ∫êÔºàÂåÖÊã¨ÂõæÂÉèÔºâ‰∏≠ÊèêÂèñÊúâ‰ª∑ÂÄºÁöÑ‰ø°ÊÅØ„ÄÇÈÄöËøáÂ∞Ü Langchain ÁöÑÂäüËÉΩ‰∏éËá™ÂÆö‰πâÊèêÁ§∫ÂíåËæìÂá∫Ëß£ÊûêÁõ∏ÁªìÂêàÔºåÊÇ®ÂèØ‰ª•ÂàõÂª∫Âº∫Â§ßÁöÑÂ∫îÁî®Á®ãÂ∫èÔºå‰ªéËßÜËßâÊï∞ÊçÆ‰∏≠ÊèêÂèñÁªìÊûÑÂåñ‰ø°ÊÅØ„ÄÇ\n\nËØ∑ËÆ∞‰ΩèÔºåËæìÂá∫ÁöÑË¥®ÈáèÂ∞ÜÂèñÂÜ≥‰∫éÊÇ®‰ΩøÁî®ÁöÑ LLM ÁöÑËÉΩÂäõ‰ª•ÂèäÊÇ®ÊèêÁ§∫ÁöÑÂÖ∑‰ΩìÊÄß„ÄÇÂ∞ùËØï‰∏çÂêåÁöÑÊ®°ÂûãÂíåÊèêÁ§∫Ôºå‰ª•ÊâæÂà∞ÊúÄÈÄÇÂêàÊÇ®Áî®‰æãÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇ\n\nÂ¶ÇÊûúÊÇ®ÊâæÂà∞Êõ¥Â•ΩÁöÑÊñπÊ≥ïÊù•ÂÆûÁé∞Áõ∏ÂêåÁöÑÁªìÊûúÊàñÊúâÊîπËøõÂª∫ËÆÆÔºåËØ∑ÈöèÊó∂Âú®ËØÑËÆ∫‰∏≠ÂàÜ‰∫´„ÄÇÊú¨ÊñáÊèê‰æõÁöÑ‰ª£Á†ÅÁ§∫‰æãÊó®Âú®‰Ωú‰∏∫Ëµ∑ÁÇπÔºåÂèØËÉΩËøòÊúâÂÖ∂‰ªñÊñπÊ≥ïÊàñ‰ºòÂåñ„ÄÇ\n\n"},{"lang":"zh","group":"blog","slug":"blog/how-to-run-nvidia-llama-3-1-nemotron-70b-instruct-locally-a58ad283aaff","frontmatter":{"title":"Â¶Ç‰ΩïÂú®Êú¨Âú∞ËøêË°å Nvidia ÁöÑ llama-3.1-nemotron-70b-instruct","meta_title":"Â¶Ç‰ΩïÂú®Êú¨Âú∞ËøêË°å Nvidia ÁöÑ llama-3.1-nemotron-70b-instruct","description":"Âú®Êú¨Âú∞ËøêË°åÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLM) Âú®ÂºÄÂèë‰∫∫Âëò„ÄÅÁ†îÁ©∂‰∫∫ÂëòÂíå AI Áà±Â•ΩËÄÖ‰∏≠Ë∂äÊù•Ë∂äÂèóÊ¨¢Ëøé„ÄÇÂÖ∂‰∏≠‰πã‰∏ÄÂ∞±ÊòØ‚Ä¶‚Ä¶","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*fqVKJkw5sQvLtIsyCcengQ.png","categories":["Programming","Technology","Science"],"author":"Rifx.Online","tags":["Nvidia","llama","Ollama","llama.cpp","Transformers"],"draft":false,"slug":"blog/how-to-run-nvidia-llama-3-1-nemotron-70b-instruct-locally-a58ad283aaff"},"content":"\n\n\nÂú®ÂºÄÂèëËÄÖ„ÄÅÁ†îÁ©∂‰∫∫ÂëòÂíå AI Áà±Â•ΩËÄÖ‰∏≠ÔºåÊú¨Âú∞ËøêË°åÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂèòÂæóË∂äÊù•Ë∂äÂèóÊ¨¢Ëøé„ÄÇÂÖ∂‰∏≠‰∏Ä‰∏™ÂºïËµ∑ÂπøÊ≥õÂÖ≥Ê≥®ÁöÑÊ®°ÂûãÊòØ llama-3.1-nemotron-70b-instructÔºåËøôÊòØ NVIDIA ÂÆöÂà∂ÁöÑÂº∫Â§ß LLMÔºåÊó®Âú®Â¢ûÂº∫ÁîüÊàêÂìçÂ∫îÁöÑÊúâÁî®ÊÄß„ÄÇÂú®Êú¨ÁªºÂêàÊåáÂçó‰∏≠ÔºåÊàë‰ª¨Â∞ÜÊé¢ËÆ®Â§öÁßçÊñπÊ≥ïÔºå‰ª•‰æøÂú®ÊÇ®ÁöÑÊú¨Âú∞Êú∫Âô®‰∏äËøêË°åÊ≠§Ê®°ÂûãÔºåÈ¶ñÂÖà‰ªãÁªçÁî®Êà∑ÂèãÂ•ΩÁöÑ Ollama Âπ≥Âè∞„ÄÇ\n\n> Âú®ÂºÄÂßã‰πãÂâçÔºåÂ¶ÇÊûúÊÇ®Ê≠£Âú®ÂØªÊâæ‰∏Ä‰∏™‰∏Ä‰ΩìÂåñÁöÑ AI Âπ≥Âè∞Ôºå‰ª•‰æøÂú®‰∏Ä‰∏™Âú∞ÊñπÁÆ°ÁêÜÊâÄÊúâ AI ËÆ¢ÈòÖÔºåÂåÖÊã¨ÊâÄÊúâ LLMÔºàÂ¶Ç GPT-o1„ÄÅLlama 3.1„ÄÅClaude 3.5 Sonnet„ÄÅGoogle Gemini„ÄÅÊú™ÂÆ°Êü•ÁöÑ LLMÔºâÂíåÂõæÂÉèÁîüÊàêÊ®°ÂûãÔºàFLUX„ÄÅStable Diffusion Á≠âÔºâÔºåËØ∑‰ΩøÁî® Anakin AI Êù•ÁÆ°ÁêÜÂÆÉ‰ª¨ÔºÅ\n\n\n\n## ÊñπÊ≥ï 1Ôºö‰ΩøÁî® Ollama Êú¨Âú∞ËøêË°å llama-3.1-nemotron-70b-instruct\n\nOllama ÊòØ‰∏Ä‰∏™Âá∫Ëâ≤ÁöÑÂ∑•ÂÖ∑ÔºåÁî®‰∫éÊú¨Âú∞ËøêË°å LLMÔºåÊèê‰æõÁÆÄÂçïÁöÑËÆæÁΩÆËøáÁ®ãÂπ∂ÊîØÊåÅÂ§öÁßçÊ®°ÂûãÔºåÂåÖÊã¨ llama-3.1-nemotron-70b-instruct„ÄÇ\n\n### ÂÆâË£Ö\n\n1. ËÆøÈóÆÂÆòÊñπ Ollama ÁΩëÁ´ô ([https://ollama.ai](https://ollama.ai/))Ôºå‰∏ãËΩΩÈÄÇÂêàÊÇ®Êìç‰ΩúÁ≥ªÁªüÁöÑÁâàÊú¨„ÄÇ\n2. ÈÄöËøáÂú®ÁªàÁ´Ø‰∏≠ËøêË°å‰ª•‰∏ãÂëΩ‰ª§Êù•ÂÆâË£Ö OllamaÔºö\n\n\n```python\ncurl https://ollama.ai/install.sh | sh\n```\n\n### ËøêË°å llama-3.1-nemotron\n\nÂÆâË£Ö Ollama ÂêéÔºåÊÇ®ÂèØ‰ª•ÈÄöËøá‰∏Ä‰∏™ÁÆÄÂçïÁöÑÂëΩ‰ª§ËΩªÊùæËøêË°å llama-3.1-nemotron-70b-instruct Ê®°ÂûãÔºö\n\n```python\nollama run nemotron:70b-instruct-q5_K_M\n```\nËØ•ÂëΩ‰ª§Â∞ÜÂú®ÊÇ®ÁöÑÁ≥ªÁªü‰∏ä‰∏ãËΩΩÊ®°ÂûãÔºàÂ¶ÇÊûúÂ∞öÊú™Â≠òÂú®ÔºâÔºåÂπ∂ÂêØÂä®‰∏Ä‰∏™‰∫§‰∫íÂºè‰ºöËØù„ÄÇ\n\n### ‰ΩøÁî®Ê®°Âûã\n\nÂú®Ê®°ÂûãÂä†ËΩΩÂêéÔºåÊÇ®ÂèØ‰ª•ÈÄöËøáËæìÂÖ•ÊèêÁ§∫ÂºÄÂßã‰∏éÂÖ∂‰∫íÂä®„ÄÇ‰æãÂ¶ÇÔºö\n\n```python\n>>> What are the key features of llama-3.1-nemotron-70b-instruct?\n\nLlama-3.1-Nemotron-70B-Instruct is a large language model with several key features:\n1. Customized by NVIDIA: The model has been fine-tuned by NVIDIA to improve the helpfulness and quality of its responses.\n2. Based on Llama 3.1: It builds upon the Llama 3.1 architecture, which is known for its strong performance across various tasks.\n3. 70 billion parameters: This large parameter count allows for complex reasoning and a wide range of capabilities.\n4. Instruct-tuned: The model is specifically designed to follow instructions and generate helpful responses to user queries.\n5. RLHF training: It has been trained using Reinforcement Learning from Human Feedback, specifically the REINFORCE algorithm.\n6. Specialized reward model: The training process utilized Llama-3.1-Nemotron-70B-Reward for optimization.\n7. HelpSteer2-Preference prompts: These were used during the training process to further improve the model's helpfulness.\n8. Extended context length: Like other Llama 3.1 models, it likely supports a longer context window of 128K tokens.\n9. Multilingual capabilities: It can understand and generate text in multiple languages.\n10. Strong reasoning abilities: The model excels in tasks requiring complex reasoning and problem-solving.\nThese features make llama-3.1-nemotron-70b-instruct a powerful and versatile language model suitable for a wide range of applications, from general conversation to specialized tasks in various domains.\n```\nÂØπ‰∫éÊõ¥È´òÁ∫ßÁöÑÁî®‰æãÔºåÊÇ®ÂèØ‰ª•‰ΩøÁî®ÂÉè Langchain ËøôÊ†∑ÁöÑÂ∫ìÂ∞Ü Ollama ‰∏é Python ÈõÜÊàê„ÄÇ‰ª•‰∏ãÊòØ‰∏Ä‰∏™ÁÆÄÂçïÁöÑÁ§∫‰æãÔºö\n\n```python\npython\n\nfrom langchain.llms import Ollama\n\nollama = Ollama(base_url=\"http://localhost:11434\", model=\"nemotron:70b-instruct-q5_K_M\")\nresponse = ollama.generate(\"Explain the concept of quantum entanglement.\")\nprint(response)\n```\nËøô‰ΩøÊÇ®ËÉΩÂ§üÊó†ÁºùÂú∞Â∞ÜÊ®°ÂûãÈõÜÊàêÂà∞ÊÇ®ÁöÑ Python È°πÁõÆÂíåÂ∫îÁî®Á®ãÂ∫è‰∏≠„ÄÇ\n\n## ÊñπÊ≥ï 2Ôºö‰ΩøÁî® llama.cpp\n\nllama.cpp ÊòØ‰∏Ä‰∏™ÊµÅË°åÁöÑ C++ ÂÆûÁé∞ÁöÑ Llama Ê®°ÂûãÊé®ÁêÜÔºåÈíàÂØπ CPU ‰ΩøÁî®ËøõË°å‰∫Ü‰ºòÂåñ„ÄÇËôΩÁÑ∂ÂÆÉÂèØËÉΩÈúÄË¶ÅÊØî Ollama Êõ¥Â§öÁöÑËÆæÁΩÆÔºå‰ΩÜÂÆÉÊèê‰æõ‰∫ÜÊõ¥Â§ßÁöÑÁÅµÊ¥ªÊÄßÂíåÂØπÊ®°ÂûãÂèÇÊï∞ÁöÑÊéßÂà∂„ÄÇ\n\n### ÂÆâË£Ö\n\n1. ÂÖãÈöÜ llama.cpp ‰ªìÂ∫ìÔºö\n\n```python\ngit clone https://github.com/ggerganov/llama.cpp.git\ncd llama.cpp\n```\n1. ÊûÑÂª∫È°πÁõÆÔºö\n\n```python\nmake\n```\n\n### ‰∏ãËΩΩÊ®°Âûã\n\nË¶ÅËøêË°å llama-3.1-nemotron-70b-instructÔºåÊÇ®ÈúÄË¶Å‰∏ãËΩΩÊ®°ÂûãÊùÉÈáç„ÄÇËøô‰∫õÈÄöÂ∏∏‰ª• GGML Êàñ GGUF Ê†ºÂºèÊèê‰æõ„ÄÇÊÇ®ÂèØ‰ª•Âú® Hugging Face Á≠âÂπ≥Âè∞‰∏äÊâæÂà∞È¢ÑÂÖàËΩ¨Êç¢ÁöÑÊ®°Âûã„ÄÇ\n\n```python\nmkdir models\ncd models\nwget https://huggingface.co/TheBloke/Llama-3.1-Nemotron-70B-Instruct-GGUF/resolve/main/llama-3.1-nemotron-70b-instruct.Q4_K_M.gguf\n```\n\n### ËøêË°åÊ®°Âûã\n\n‰∏ÄÊó¶‰Ω†Êã•ÊúâÊ®°ÂûãÊñá‰ª∂ÔºåÂ∞±ÂèØ‰ª•‰ΩøÁî®‰ª•‰∏ãÂëΩ‰ª§ËøêË°åÂÆÉÔºö\n\n```python\n./main -m models/llama-3.1-nemotron-70b-instruct.Q4_K_M.gguf -n 1024 -p \"Hello, how are you today?\"\n```\nËØ•ÂëΩ‰ª§Âä†ËΩΩÊ®°ÂûãÂπ∂ÁîüÊàêÂØπÁªôÂÆöÊèêÁ§∫ÁöÑÂìçÂ∫î„ÄÇ‰Ω†ÂèØ‰ª•Ë∞ÉÊï¥ÂêÑÁßçÂèÇÊï∞ÔºåÊØîÂ¶ÇÁîüÊàêÁöÑ‰ª§ÁâåÊï∞Èáè (-n) ÊàñÊ∏©Â∫¶‰ª•ÊéßÂà∂ÈöèÊú∫ÊÄß„ÄÇ\n\n## ÊñπÊ≥ï 3Ôºö‰ΩøÁî® Hugging Face Transformers\n\nHugging Face ÁöÑ Transformers Â∫ìÊèê‰æõ‰∫Ü‰∏Ä‰∏™È´òÂ±ÇÊ¨°ÁöÑ APIÔºåÁî®‰∫éÂ§ÑÁêÜÂêÑÁßçËØ≠Ë®ÄÊ®°ÂûãÔºåÂåÖÊã¨ llama-3.1-nemotron-70b-instruct„ÄÇ\n\n**ÂÆâË£Ö**\n\nÈ¶ñÂÖàÔºåÂÆâË£ÖÂøÖË¶ÅÁöÑÂ∫ìÔºö\n\n\n```python\npip install transformers torch accelerate\n```\n**ËøêË°åÊ®°Âûã**\n\n‰ª•‰∏ãÊòØ‰∏Ä‰∏™Âä†ËΩΩÂíå‰ΩøÁî®Ê®°ÂûãÁöÑ Python ËÑöÊú¨Ôºö\n\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nmodel_name = \"meta-llama/Llama-3.1-Nemotron-70b-instruct\"\n## Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n## Prepare the input\nprompt = \"Explain the concept of quantum computing in simple terms.\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n## Generate the response\nwith torch.no_grad():\n    outputs = model.generate(**inputs, max_new_tokens=100)\n## Decode and print the response\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response)\n```\nËøôÁßçÊñπÊ≥ïÂÖÅËÆ∏ÂØπÊ®°ÂûãÁöÑË°å‰∏∫ËøõË°åÊõ¥ÁªÜÁ≤íÂ∫¶ÁöÑÊéßÂà∂ÔºåÂπ∂‰∏éÂÖ∂‰ªñ Hugging Face Â∑•ÂÖ∑ÂíåÁÆ°ÈÅìÈõÜÊàê„ÄÇ\n\n## ÁªìËÆ∫\n\nÂú®Êú¨Âú∞ËøêË°å llama-3.1-nemotron-70b-instruct ‰∏∫ÂºÄÂèëËÄÖÂíåÁ†îÁ©∂‰∫∫ÂëòÊâìÂºÄ‰∫ÜÊó†ÈôêÂèØËÉΩ„ÄÇÊó†ËÆ∫ÊÇ®ÈÄâÊã© Ollama ÁöÑÁÆÄÂçïÊÄß„ÄÅllama.cpp ÁöÑÁÅµÊ¥ªÊÄßÔºåËøòÊòØ Hugging Face Transformers ÁöÑÈõÜÊàêÂäüËÉΩÔºåÊÇ®Áé∞Âú®ÈÉΩÊúâÂ∑•ÂÖ∑ÂèØ‰ª•Âú®Ëá™Â∑±ÁöÑÁ°¨‰ª∂‰∏äÂà©Áî®Ëøô‰∏ÄÂÖàËøõËØ≠Ë®ÄÊ®°ÂûãÁöÑÂº∫Â§ßËÉΩÂäõ„ÄÇÂú®Êé¢Á¥¢ llama-3.1-nemotron-70b-instruct ÁöÑËÉΩÂäõÊó∂ÔºåËØ∑ËÆ∞‰ΩèÂú®ÊÄßËÉΩ‰∏éËµÑÊ∫êÈôêÂà∂‰πãÈó¥ÂèñÂæóÂπ≥Ë°°ÔºåÂπ∂ÂßãÁªàËÄÉËôëÊÇ®Â∫îÁî®ÁöÑ‰º¶ÁêÜÂΩ±Âìç„ÄÇË¥üË¥£‰ªªÁöÑ‰ΩøÁî®ÔºåËøô‰∏™Ê®°ÂûãÂèØ‰ª•Êàê‰∏∫Êé®Âä®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂíå AI È©±Âä®Â∫îÁî®ÂèØËÉΩÊÄßÁöÑÂÆùË¥µËµÑ‰∫ß„ÄÇ\n\n"},{"lang":"zh","group":"blog","slug":"blog/key-points-llm-quantization-chatgpt-artificial-intelligence-8201ffcb33d4","frontmatter":{"title":"Ëß£ÈîÅ LLM ÈáèÂåñÁöÑ 5 ‰∏™ÂÖ≥ÈîÆÁÇπ","meta_title":"Ëß£ÈîÅ LLM ÈáèÂåñÁöÑ 5 ‰∏™ÂÖ≥ÈîÆÁÇπ","description":"ÈáèÂåñÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*RUqPEr2NTYXlI1omqF22Qg.png","categories":["Machine Learning","Data Science","Technology/Web"],"author":"Rifx.Online","tags":["quantization","weights","activations","calibration","Quanto"],"draft":false,"slug":"blog/key-points-llm-quantization-chatgpt-artificial-intelligence-8201ffcb33d4"},"content":"\n\n\n### Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÈáèÂåñ\n\n\n\nLLMÈáèÂåñÁõÆÂâçÊòØ‰∏Ä‰∏™ÁÉ≠Èó®ËØùÈ¢òÔºåÂõ†‰∏∫ÂÆÉÂú®ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÊïàÁéáÂíåÂú®ÂêÑÁßçÁ°¨‰ª∂Âπ≥Âè∞ÔºàÂåÖÊã¨Ê∂àË¥πÁ∫ßËÆæÂ§áÔºâ‰∏äÈÉ®ÁΩ≤ÊñπÈù¢ÂèëÊå•ÁùÄËá≥ÂÖ≥ÈáçË¶ÅÁöÑ‰ΩúÁî®„ÄÇ\n\nÈÄöËøáË∞ÉÊï¥Ê®°Âûã‰∏≠Êüê‰∫õÁªÑ‰ª∂ÁöÑÁ≤æÂ∫¶Ôºå**ÈáèÂåñÊòæËëóÂáèÂ∞ë‰∫ÜÊ®°ÂûãÁöÑÂÜÖÂ≠òÂç†Áî®**ÔºåÂêåÊó∂‰øùÊåÅÁõ∏‰ººÁöÑÊÄßËÉΩÊ∞¥Âπ≥„ÄÇ\n\nÂú®Êú¨ÊåáÂçó‰∏≠ÔºåÊàë‰ª¨Â∞ÜÊé¢ËÆ®LLMÈáèÂåñÁöÑ‰∫î‰∏™ÂÖ≥ÈîÆÊñπÈù¢ÔºåÂåÖÊã¨Â∞ÜÊ≠§ÊäÄÊúØÂ∫îÁî®‰∫éÊàë‰ª¨Ê®°ÂûãÁöÑ‰∏Ä‰∫õÂÆûÁî®Ê≠•È™§„ÄÇ\n\n## #1. ÁêÜËß£ÈáèÂåñ\n\nÈáèÂåñÊòØ‰∏ÄÁßçÊ®°ÂûãÂéãÁº©ÊäÄÊúØÔºåÈÄöËøáÈôç‰Ωé LLM ‰∏≠ÊùÉÈáçÂíåÊøÄÊ¥ªÁöÑÁ≤æÂ∫¶Êù•ÂÆûÁé∞„ÄÇËøôÊ∂âÂèäÂ∞ÜÈ´òÁ≤æÂ∫¶ÂÄºËΩ¨Êç¢‰∏∫‰ΩéÁ≤æÂ∫¶ÂÄºÔºåÂÆûÈôÖ‰∏äÊòØ**Â∞ÜÂ≠òÂÇ®Êõ¥Â§ö‰ø°ÊÅØÁöÑÊï∞ÊçÆÁ±ªÂûãÊõ¥Êîπ‰∏∫Â≠òÂÇ®Êõ¥Â∞ë‰ø°ÊÅØÁöÑÊï∞ÊçÆÁ±ªÂûã**„ÄÇ\n\nÂáèÂ∞ëÊØè‰∏™ÊùÉÈáçÊàñÊøÄÊ¥ªÊâÄÈúÄÁöÑ‰ΩçÊï∞ÊòæËëóÈôç‰Ωé‰∫ÜÊï¥‰ΩìÊ®°ÂûãÂ§ßÂ∞è„ÄÇÂõ†Ê≠§Ôºå**ÈáèÂåñÂàõÂª∫‰∫Ü‰ΩøÁî®Êõ¥Â∞ëÂÜÖÂ≠òÂíåÈúÄË¶ÅÊõ¥Â∞ëÂ≠òÂÇ®Á©∫Èó¥ÁöÑ LLM„ÄÇ**\n\nËøô‰∏ÄÊäÄÊúØÂú®Â∫îÂØπ LLM ËøûÁª≠Ëø≠‰ª£‰∏≠ÂèÇÊï∞Êï∞ÈáèÁöÑÊåáÊï∞Â¢ûÈïøÊó∂ÂèòÂæóËá≥ÂÖ≥ÈáçË¶Å„ÄÇ‰æãÂ¶ÇÔºåÂú® OpenAI ÁöÑ GPT Á≥ªÂàó‰∏≠ÔºåÊàë‰ª¨ÂèØ‰ª•Âú®‰ª•‰∏ãÂõæË°®‰∏≠ËßÇÂØüÂà∞Ëøô‰∏ÄÂ¢ûÈïøË∂ãÂäøÔºö\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*QlAhma3Wu1F6w2WvkE8jDA.png)\n\nËøô‰∏ÄÊòæËëóÂ¢ûÂä†Â∏¶Êù•‰∫ÜÊåëÊàòÔºöÈöèÁùÄÊ®°ÂûãÁöÑÂ¢ûÈïøÔºåÂÆÉ‰ª¨ÁöÑÂÜÖÂ≠òÈúÄÊ±ÇÂæÄÂæÄË∂ÖËøáÂÖàËøõÁ°¨‰ª∂Âä†ÈÄüÂô®ÔºàÂ¶Ç GPUÔºâÁöÑÂÆπÈáè„ÄÇ**ËøôÈúÄË¶ÅÂàÜÂ∏ÉÂºèËÆ≠ÁªÉÂíåÊé®ÁêÜÊù•ÁÆ°ÁêÜËøô‰∫õÊ®°ÂûãÔºå‰ªéËÄåÈôêÂà∂‰∫ÜÂÆÉ‰ª¨ÁöÑÂèØÈÉ®ÁΩ≤ÊÄß„ÄÇ**\n\n## #2. ÈáèÂåñËÉåÂêéÁöÑÁõ¥Ëßâ\n\nÂ∞ΩÁÆ°ÈáèÂåñÁöÑÂÆö‰πâÁúãËµ∑Êù•Áõ∏ÂΩìÂ§çÊùÇÔºå‰ΩÜËøô‰∏™Ê¶ÇÂøµÂèØ‰ª•ÈÄöËøáÁü©ÈòµÁõ¥ËßÇÂú∞Ëß£Èáä„ÄÇ\n\nËÆ©Êàë‰ª¨ËÄÉËôë‰ª•‰∏ã‰∏Ä‰∏™ 3x3 Áü©ÈòµÔºåË°®Á§∫Á•ûÁªèÁΩëÁªúÁöÑÊùÉÈáç„ÄÇÂ∑¶‰æßÁöÑÁü©ÈòµÊòæÁ§∫‰∫ÜÂéüÂßãÊùÉÈáçÔºåËÄåÂè≥‰æßÁöÑÁü©ÈòµÊòæÁ§∫‰∫ÜËøô‰∫õÊùÉÈáçÁöÑÈáèÂåñÁâàÊú¨Ôºö\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*LPzWe9oxjlDYdSp7dVvRUg.png)\n\nÂú®Ëøô‰∏™ÁÆÄÂçïÁöÑ‰æãÂ≠ê‰∏≠ÔºåÊàë‰ª¨Â∞ÜÂéüÂßãÁü©ÈòµÁöÑÂÖÉÁ¥†‰ªéÂõõ‰ΩçÂ∞èÊï∞ÂõõËàç‰∫îÂÖ•Âà∞‰∏Ä‰ΩçÂ∞èÊï∞„ÄÇÂ∞ΩÁÆ°Áü©ÈòµÁúãËµ∑Êù•Áõ∏‰ººÔºå**‰ΩÜÂõõ‰ΩçÂ∞èÊï∞ÁâàÊú¨ÊâÄÈúÄÁöÑÂ≠òÂÇ®Á©∫Èó¥ÊòæËëóÊõ¥È´ò**„ÄÇ\n\nÂú®ÂÆûË∑µ‰∏≠ÔºåÈáèÂåñ‰∏ç‰ªÖ‰ªÖÊòØ‰∏Ä‰∏™ÂõõËàç‰∫îÂÖ•Êìç‰Ωú„ÄÇÁõ∏ÂèçÔºåÂÆÉÊ∂âÂèäÂ∞ÜÊï∞ÂÄºËΩ¨Êç¢‰∏∫‰∏çÂêåÁöÑÊï∞ÊçÆÁ±ªÂûãÔºåÈÄöÂ∏∏ÊòØ‰ªéÊõ¥È´òÁ≤æÂ∫¶ËΩ¨Êç¢‰∏∫Êõ¥‰ΩéÁ≤æÂ∫¶„ÄÇ\n\n‰æãÂ¶ÇÔºåÂ§ßÂ§öÊï∞Ê®°ÂûãÁöÑÈªòËÆ§Êï∞ÊçÆÁ±ªÂûãÊòØ `float32`ÔºåÊØè‰∏™ÂèÇÊï∞ÈúÄË¶Å 4 Â≠óËäÇÔºà32 ‰ΩçÔºâ„ÄÇÂõ†Ê≠§ÔºåÂØπ‰∫é‰∏Ä‰∏™ 3x3 Áü©ÈòµÔºåÊÄªÂÜÖÂ≠òÂç†Áî®‰∏∫ 36 Â≠óËäÇ„ÄÇÂ∞ÜÊï∞ÊçÆÁ±ªÂûãÊõ¥Êîπ‰∏∫ `int8`ÔºåÊØè‰∏™ÂèÇÊï∞Âè™ÈúÄË¶Å 1 Â≠óËäÇÔºå‰ªéËÄåÂ∞ÜÁü©ÈòµÁöÑÊÄªÂÜÖÂ≠òÂç†Áî®ÂáèÂ∞ëÂà∞‰ªÖ 9 Â≠óËäÇ„ÄÇ\n\n## #3. ÈáèÂåñËØØÂ∑Æ\n\nÊ≠£Â¶ÇÊàë‰ª¨ÊâÄÁúãÂà∞ÁöÑÔºåÂéüÂßãÁü©ÈòµÂèäÂÖ∂ÈáèÂåñÂΩ¢ÂºèÂπ∂‰∏çÂÆåÂÖ®Áõ∏Á≠âÔºå‰ΩÜÈùûÂ∏∏Áõ∏‰ºº„ÄÇÈÄêÂÄº‰πãÈó¥ÁöÑÂ∑ÆÂºÇË¢´Áß∞‰∏∫‚ÄúÈáèÂåñËØØÂ∑Æ‚ÄùÔºåÊàë‰ª¨‰πüÂèØ‰ª•Áî®Áü©ÈòµÂΩ¢ÂºèË°®Á§∫Ôºö\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*VtGDjVbr7daagLXB57i7Mg.png)\n\n**ËøôÁßçÈáèÂåñËØØÂ∑ÆÂèØ‰ª•Âú®ÁΩëÁªú‰∏≠ÁöÑÊØè‰∏™ÊùÉÈáçÁü©Èòµ‰∏≠Á¥ØÁßØÔºå‰ªéËÄåÂΩ±ÂìçÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ**\n\nÂΩìÂâçÁöÑÈáèÂåñÁ†îÁ©∂Êó®Âú®ÊúÄÂ∞èÂåñÁ≤æÂ∫¶Â∑ÆÂºÇÔºåÂêåÊó∂ÂáèÂ∞ëËÆ≠ÁªÉÊàñÊé®ÁêÜÊ®°ÂûãÊâÄÈúÄÁöÑËÆ°ÁÆóËµÑÊ∫êÔºåÂêåÊó∂‰øùÊåÅÂèØÊé•ÂèóÁöÑÊÄßËÉΩÊ∞¥Âπ≥„ÄÇ\n\n## #4. Á∫øÊÄßÈáèÂåñ\n\nÁ∫øÊÄßÈáèÂåñÊòØ LLMs ‰∏≠ÊúÄÊµÅË°åÁöÑÈáèÂåñÊñπÊ°à‰πã‰∏Ä„ÄÇÁÆÄÂçïÊù•ËØ¥ÔºåÂÆÉÊ∂âÂèäÂ∞ÜÂéüÂßãÊùÉÈáçÁöÑÊµÆÁÇπÂÄºËåÉÂõ¥Êò†Â∞ÑÂà∞Âõ∫ÂÆöÁÇπÂÄºËåÉÂõ¥„ÄÇ\n\nËÆ©Êàë‰ª¨ÂõûÈ°æ‰∏Ä‰∏ãÂ∞ÜÁ∫øÊÄßÈáèÂåñÂ∫îÁî®‰∫éÊàë‰ª¨ÁöÑÊ®°ÂûãÊâÄÈúÄÁöÑÊ≠•È™§Ôºö\n\n* **Ëé∑ÂèñÊúÄÂ∞èÂíåÊúÄÂ§ßËåÉÂõ¥Ôºö** Êàë‰ª¨ÈúÄË¶ÅËé∑ÂèñÂæÖÈáèÂåñÁöÑÊµÆÁÇπÊùÉÈáçÁöÑÊúÄÂ∞èÂÄºÂíåÊúÄÂ§ßÂÄºÔºà`x_min` Âíå `x_max`Ôºâ„ÄÇÊàë‰ª¨ËøòÈúÄË¶ÅÂÆö‰πâÈáèÂåñËåÉÂõ¥Ôºà`q_min` Âíå `q_max`ÔºâÔºåËØ•ËåÉÂõ¥Â∑≤ÁªèÁî±Êàë‰ª¨ÊÉ≥Ë¶ÅËΩ¨Êç¢ÁöÑÊï∞ÊçÆÁ±ªÂûãËÆæÁΩÆ„ÄÇ\n* **ËÆ°ÁÆóÁº©ÊîæÂõ†Â≠êÔºà`s`ÔºâÂíåÈõ∂ÁÇπÔºà`z`ÔºâÂÄºÔºö** È¶ñÂÖàÔºåÁº©ÊîæÂõ†Â≠êÔºà`s`ÔºâÂ∞ÜÊµÆÁÇπÂÄºÁöÑËåÉÂõ¥Ë∞ÉÊï¥Âà∞ÈÄÇÂêàÊï¥Êï∞ËåÉÂõ¥Ôºå‰øùÊåÅÊï∞ÊçÆÂàÜÂ∏ÉÂíåËåÉÂõ¥„ÄÇÂÖ∂Ê¨°ÔºåÈõ∂ÁÇπÔºà`z`ÔºâÁ°Æ‰øùÊµÆÁÇπËåÉÂõ¥ÂÜÖÁöÑÈõ∂Ë¢´ÂáÜÁ°ÆÂú∞Ë°®Á§∫‰∏∫Êï¥Êï∞Ôºå‰ªéËÄå‰øùÊåÅÊï∞ÂÄºÁöÑÂáÜÁ°ÆÊÄßÂíåÁ®≥ÂÆöÊÄßÔºåÁâπÂà´ÊòØÂØπ‰∫éÊé•ËøëÈõ∂ÁöÑÂÄº„ÄÇ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BepC6-izw0yE19ejsS705Q.png)\n\n* **ÈáèÂåñÂÄºÔºà`q`ÔºâÔºö** Êàë‰ª¨ÈúÄË¶Å‰ΩøÁî®Âú®Ââç‰∏ÄÊ≠•ËÆ°ÁÆóÁöÑÁº©ÊîæÂõ†Â≠êÔºà`s`ÔºâÂíåÈõ∂ÁÇπÔºà`z`ÔºâÂ∞ÜÂéüÂßãÊµÆÁÇπÂÄºÊò†Â∞ÑÂà∞Êï¥Êï∞ËåÉÂõ¥„ÄÇ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BBOQ0VbSGbwf7CN8c4PWKQ.png)\n\nÂ∫îÁî®Ëøô‰∫õÂÖ¨ÂºèÁõ∏ÂΩìÁÆÄÂçï„ÄÇÂ¶ÇÊûúÊàë‰ª¨Â∞ÜÂÆÉ‰ª¨Â∫îÁî®‰∫é‰∏ãÂõæÂ∑¶‰æßÁöÑ 3x3 ÊùÉÈáçÂº†ÈáèÔºåÊàë‰ª¨Â∞ÜÂæóÂà∞Âè≥‰æßÊâÄÁ§∫ÁöÑÈáèÂåñÁü©ÈòµÔºö\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*KzBvg84mfI2gAhTIyVibwQ.png)\n\nÊàë‰ª¨ÂèØ‰ª•ÁúãÂà∞Ôºå`int8` ÂÄºÁöÑ‰∏ãÈôêÂØπÂ∫î‰∫éÂéüÂßãÂº†ÈáèÁöÑ‰∏ãÈôêÔºåËÄå‰∏äÈôêÂØπÂ∫î‰∫éÂéüÂßãÂº†ÈáèÁöÑ‰∏äÈôêÔºå*Âç≥ÔºåÊò†Â∞Ñ‰∏∫ `0.50 ‚Üí 255` Âíå `-0.40 ‚Üí 0`„ÄÇ*\n\nÊàë‰ª¨Áé∞Âú®ÂèØ‰ª•‰ΩøÁî®‰∏ãÈù¢ÁöÑÂÖ¨ÂºèÂØπÂÄºËøõË°åÂèçÈáèÂåñ„ÄÇ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*E5nnqYzncYCRuM5prssuOw.png)\n\nÂ¶ÇÊûúÊàë‰ª¨Â∞ÜÂèçÈáèÂåñÂêéÁöÑÂÄºÂÜçÊ¨°ÊîæÂÖ•Áü©ÈòµÂΩ¢ÂºèÔºàÂ∑¶‰æßÁü©ÈòµÔºâÔºåÊàë‰ª¨ÂèØ‰ª•ÈÄöËøáËÆ°ÁÆóÂéüÂßãÁü©Èòµ‰∏éÂÖ∂ÂèçÈáèÂåñÁâàÊú¨‰πãÈó¥ÈÄêÁÇπÂ∑ÆÂºÇÊù•ËÆ°ÁÆóÈáèÂåñËØØÂ∑ÆÔºàÂè≥‰æßÁü©ÈòµÔºâÔºö\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*56NALu9PAN95QG2hn8HXoQ.png)\n\nÊ≠£Â¶ÇÊàë‰ª¨ÊâÄËßÇÂØüÂà∞ÁöÑÔºåÈáèÂåñËØØÂ∑ÆÂºÄÂßãÂú®Êüê‰∫õÁü©ÈòµÂÄº‰∏≠ÊòæÁé∞„ÄÇ\n\n## #5. ÊùÉÈáçÈáèÂåñ‰∏éÊøÄÊ¥ªÈáèÂåñ\n\nÂú®‰∏äÈù¢ÁöÑ‰æãÂ≠ê‰∏≠ÔºåÊàë‰ª¨‰∏ªË¶ÅÂÖ≥Ê≥®‰∫éÈáèÂåñÊ®°ÂûãÁöÑÊùÉÈáç„ÄÇËôΩÁÑ∂ÊùÉÈáçÈáèÂåñÂØπ‰∫éÊ®°Âûã‰ºòÂåñËá≥ÂÖ≥ÈáçË¶ÅÔºå‰ΩÜËÄÉËôëÂà∞ÊøÄÊ¥ª‰πüÂèØ‰ª•ËøõË°åÈáèÂåñÂêåÊ†∑ÈáçË¶Å„ÄÇ\n\n**ÊøÄÊ¥ªÈáèÂåñÊ∂âÂèäÂáèÂ∞ëÁΩëÁªú‰∏≠ÊØèÂ±ÇÁöÑ‰∏≠Èó¥ËæìÂá∫ÁöÑÁ≤æÂ∫¶**„ÄÇ‰∏éÊùÉÈáçÂú®Ê®°ÂûãËÆ≠ÁªÉÂêé‰øùÊåÅ‰∏çÂèò‰∏çÂêåÔºåÊøÄÊ¥ªÊòØÂä®ÊÄÅÁöÑÔºåÂπ∂‰∏îÈöèÁùÄÊØè‰∏™ËæìÂÖ•ËÄåÂèòÂåñÔºå‰ΩøÂÖ∂ËåÉÂõ¥Êõ¥ÈöæÈ¢ÑÊµã„ÄÇ\n\n‰∏ÄËà¨ËÄåË®ÄÔºåÊøÄÊ¥ªÈáèÂåñÊØîÊùÉÈáçÈáèÂåñÊõ¥ÂÖ∑ÊåëÊàòÊÄßÔºåÂõ†‰∏∫ÂÆÉÈúÄË¶Å‰ªîÁªÜÊ†°ÂáÜ‰ª•Á°Æ‰øùÂáÜÁ°ÆÊçïÊçâÊøÄÊ¥ªÁöÑÂä®ÊÄÅËåÉÂõ¥„ÄÇ\n\nÊùÉÈáçÈáèÂåñÂíåÊøÄÊ¥ªÈáèÂåñÊòØ‰∫íË°•ÁöÑÊäÄÊúØ„ÄÇ‰∏§ËÄÖÁªìÂêà‰ΩøÁî®ÂèØ‰ª•ÊòæËëóÂáèÂ∞ëÊ®°ÂûãÂ§ßÂ∞èÔºåËÄå‰∏ç‰ºöÂ§ßÂπÖÂΩ±ÂìçÊÄßËÉΩ„ÄÇ\n\n## ÊúÄÂêéÁöÑÊÄùËÄÉ\n\nÂú®Êú¨Êñá‰∏≠ÔºåÊàë‰ª¨ÂõûÈ°æ‰∫ÜÂÖ≥‰∫éÈáèÂåñÁöÑ5‰∏™ÂÖ≥ÈîÆÁÇπÔºå‰ª•Êõ¥Â•ΩÂú∞ÁêÜËß£Â¶Ç‰ΩïÂáèÂ∞èËøô‰∫õ‰∏çÊñ≠Â¢ûÈïøÁöÑÊ®°ÂûãÁöÑÂ§ßÂ∞è„ÄÇ\n\nËá≥‰∫éËøô‰∫õÊäÄÊúØÁöÑÂÆûÁé∞ÔºåPython‰∏≠ÊúâÂá†‰∏™ÊîØÊåÅÈáèÂåñÁöÑÂ∑•ÂÖ∑ÂíåÂ∫ìÔºå‰æãÂ¶Ç`pytorch`Âíå`tensorflow`„ÄÇÁÑ∂ËÄåÔºåÂú®Áé∞ÊúâÊ®°Âûã‰∏≠Êó†ÁºùÈõÜÊàêÈáèÂåñÈúÄË¶ÅÂØπÂ∫ìÂíåÊ®°ÂûãÂÜÖÈÉ®ÁªìÊûÑÊúâÊ∑±ÂÖ•ÁöÑÁêÜËß£„ÄÇ\n\nËøôÂ∞±ÊòØ‰∏∫‰ªÄ‰πàÂà∞ÁõÆÂâç‰∏∫Ê≠¢ÔºåÊàëÊúÄÂñúÊ¨¢ÁöÑÁÆÄÂçïÊ≠•È™§ÂÆûÁé∞ÈáèÂåñÁöÑÈÄâÈ°πÊòØHugging FaceÁöÑ[Quanto](https://huggingface.co/blog/quanto-introduction)Â∫ìÔºåÊó®Âú®ÁÆÄÂåñPyTorchÊ®°ÂûãÁöÑÈáèÂåñËøáÁ®ã„ÄÇ\n\nÂ¶ÇÊûú‰Ω†ÂØπLLMÈáèÂåñÁöÑÊ∑±ÂÖ•ÂÜÖÂÆπ‰ª•ÂèäÂ¶Ç‰Ωï‰ΩøÁî®‰∏äËø∞Â∫ìÊÑüÂÖ¥Ë∂£Ôºå‰Ω†ÂèØËÉΩËøò‰ºöÂØπÊñáÁ´†[‚ÄúÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÈáèÂåñÔºöÊúâÊïàÂáèÂ∞ëAIÊ®°ÂûãÂ§ßÂ∞è‚Äù](https://www.datacamp.com/tutorial/quantization-for-large-language-models)ÊÑüÂÖ¥Ë∂£„ÄÇ\n\nÂ∞±Ëøô‰∫õÔºÅÈùûÂ∏∏ÊÑüË∞¢‰Ω†ÁöÑÈòÖËØªÔºÅ\n\nÊàëÂ∏åÊúõËøôÁØáÊñáÁ´†ËÉΩÂú®**‰ΩøÁî®LLMsËøõË°åÁºñÁ†ÅÊó∂**ÂØπ‰Ω†ÊúâÊâÄÂ∏ÆÂä©ÔºÅ\n\n‰Ω†‰πüÂèØ‰ª•ËÆ¢ÈòÖÊàëÁöÑ[**Êó∂‰∫ãÈÄöËÆØ**](https://readmedium.com/@andvalenzuela/subscribe)Ôºå‰ª•‰æøÂèäÊó∂Ëé∑ÂèñÊñ∞ÂÜÖÂÆπ„ÄÇ\n\n**ÁâπÂà´ÊòØ**Ôºå**Â¶ÇÊûú‰Ω†ÂØπÊúâÂÖ≥Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂíåChatGPTÁöÑÊñáÁ´†ÊÑüÂÖ¥Ë∂£**Ôºö\n\n"},{"lang":"zh","group":"blog","slug":"blog/langgraph-vs-langchain-vs-langflow-vs-langsmith-which-one-to-use-why-69ee91e91000","frontmatter":{"title":"LangGraph„ÄÅLangChain„ÄÅLangFlow„ÄÅLangSmithÔºö‰ΩøÁî®Âì™‰∏Ä‰∏™‰ª•Âèä‰∏∫‰ªÄ‰πàÔºü","meta_title":"LangGraph„ÄÅLangChain„ÄÅLangFlow„ÄÅLangSmithÔºö‰ΩøÁî®Âì™‰∏Ä‰∏™‰ª•Âèä‰∏∫‰ªÄ‰πàÔºü","description":"‰∫ÜËß£ LangGraph„ÄÅLangChain„ÄÅLangFlow Âíå LangSmith ‰πãÈó¥ÁöÑ‰∏ªË¶ÅÂå∫Âà´ÔºåÂπ∂‰∫ÜËß£Âì™ÁßçÊ°ÜÊû∂ÊúÄÈÄÇÂêàÊÇ®ÁöÑ‚Ä¶‚Ä¶","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*xrWv1QVt4zE5cxjA8VA3ag.png","categories":["Programming","Technology","Technology/Web"],"author":"Rifx.Online","tags":["LangGraph","LangChain","LangFlow","LangSmith","frameworks"],"draft":false,"slug":"blog/langgraph-vs-langchain-vs-langflow-vs-langsmith-which-one-to-use-why-69ee91e91000"},"content":"\n\n\n### Êé¢Á¥¢ LangGraph„ÄÅLangChain„ÄÅLangFlow Âíå LangSmith ‰πãÈó¥ÁöÑÂÖ≥ÈîÆÂå∫Âà´Ôºå‰∫ÜËß£Âì™ÁßçÊ°ÜÊû∂ÊúÄÈÄÇÂêàÊÇ®ÁöÑËØ≠Ë®ÄÊ®°ÂûãÂ∫îÁî®‚Äî‚Äî‰ªéÂ∑•‰ΩúÊµÅÊûÑÂª∫Âà∞ÊÄßËÉΩÁõëÊéß„ÄÇ\n\nüë®üèæ‚Äçüíª [GitHub](https://github.com/mdmonsurali) ‚≠êÔ∏è | üëî[LinkedIn](https://www.linkedin.com/in/mdmonsurali/) |üìù [Medium](https://medium.com/@monsuralirana)\n\n\n\nËøëÂπ¥Êù•ÔºåËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÔºàNLPÔºâÈ¢ÜÂüüËßÅËØÅ‰∫ÜÂèØÁî®‰∫éÊûÑÂª∫Âü∫‰∫éËØ≠Ë®ÄÊ®°ÂûãÁöÑÂ∫îÁî®Á®ãÂ∫èÁöÑÊ°ÜÊû∂„ÄÅÂ∫ìÂíåÂ∑•ÂÖ∑Êï∞ÈáèÁöÑÊøÄÂ¢û„ÄÇÂú®Ëøô‰∫õÂ∑•ÂÖ∑‰∏≠Ôºå**LangGraph**„ÄÅ**LangChain**„ÄÅ**LangFlow** Âíå **LangSmith** Â∑≤Êàê‰∏∫È¢ÜÂÖàÁöÑÈÄâÊã©ÔºåÂêÑËá™Êª°Ë∂≥‰∏çÂêåÁöÑÁî®‰æãÂíåÁî®Êà∑ÈúÄÊ±Ç„ÄÇÂ¶ÇÊûúÊÇ®Â∏åÊúõÊûÑÂª∫„ÄÅÁõëÊéßÊàñÊâ©Â±ïËØ≠Ë®ÄÊ®°ÂûãÂ∑•‰ΩúÊµÅÔºå‰∫ÜËß£Ëøô‰∫õÂ∑•ÂÖ∑ÁöÑ‰ºòÂäøÂíåÁõÆÁöÑËá≥ÂÖ≥ÈáçË¶Å„ÄÇ\n\nÂú®Êú¨ÂçöÂÆ¢‰∏≠ÔºåÊàë‰ª¨Â∞ÜÊé¢ËÆ®ÊØè‰∏™Ê°ÜÊû∂ÔºåÂàÜÊûêÂÆÉ‰ª¨ÁöÑ‰ºòÂäøÔºåÂπ∂Êèê‰æõ‰ΩïÊó∂‰ΩøÁî®ÂÆÉ‰ª¨ÁöÑËßÅËß£„ÄÇÊó†ËÆ∫ÊÇ®ÊòØÁªèÈ™å‰∏∞ÂØåÁöÑÂºÄÂèëËÄÖËøòÊòØËØ•È¢ÜÂüüÁöÑÊñ∞ÊâãÔºåÁêÜËß£Ëøô‰∫õÂ∑•ÂÖ∑ÁöÑÁªÜÂæÆÂ∑ÆÂà´Â∞ÜÂ∏ÆÂä©ÊÇ®‰∏∫ÊÇ®ÁöÑÈ°πÁõÆÈÄâÊã©ÂêàÈÄÇÁöÑÂ∑•ÂÖ∑„ÄÇ\n\n## ËØ≠Ë®ÄÊ®°ÂûãÊ°ÜÊû∂ÁÆÄ‰ªã\n\nÈöèÁùÄÂº∫Â§ßÁöÑËØ≠Ë®ÄÊ®°ÂûãÂ¶Ç GPT-3„ÄÅGPT-4 ‰ª•ÂèäÂÖ∂‰ªñÂü∫‰∫éÂèòÊç¢Âô®ÁöÑÊ®°ÂûãÁöÑÂ¥õËµ∑ÔºåË∂äÊù•Ë∂äÈúÄË¶ÅËÉΩÂ§üÁÆÄÂåñËØ≠Ë®ÄÂ∫îÁî®Á®ãÂ∫èÂàõÂª∫ÂíåÁÆ°ÁêÜÁöÑÊ°ÜÊû∂„ÄÇËøô‰∫õÊ°ÜÊû∂ÁÆÄÂåñ‰∫ÜÂ§çÊùÇÁöÑ‰ªªÂä°ÔºåÂ¶Ç **ÈìæÊé•Â§ö‰∏™ÊèêÁ§∫**„ÄÅ**Ê£ÄÁ¥¢Áõ∏ÂÖ≥ÊñáÊ°£**ÔºåÁîöËá≥ **ÁõëÊéßÊ®°ÂûãÊÄßËÉΩ**„ÄÇ\n\nÁÑ∂ËÄåÔºåÂπ∂ÈùûÊâÄÊúâÊ°ÜÊû∂ÈÉΩÊòØÁõ∏ÂêåÁöÑ„ÄÇÊúâ‰∫õÊ°ÜÊû∂Êèê‰æõ **ÂèØËßÜÂåñÁïåÈù¢** Êù•ÁÆ°ÁêÜÂ∑•‰ΩúÊµÅÁ®ãÔºåËÄåÂÖ∂‰ªñÊ°ÜÊû∂ÂàôÊèê‰æõÈ´òÁ∫ßÁöÑ **Ë∞ÉËØïÂíåÂèØËßÇÂØüÊÄß** ÂäüËÉΩ„ÄÇËÆ©Êàë‰ª¨Ê∑±ÂÖ•‰∫ÜËß£Ëøô‰∫õÂ∑•ÂÖ∑Ôºå‰ª•ÁêÜËß£ÂÆÉ‰ª¨Áã¨ÁâπÁöÑÂäüËÉΩ„ÄÇ\n\n## 1. LangGraphÔºöÂèØËßÜÂåñÂ§çÊùÇÂ∑•‰ΩúÊµÅ\n\n**LangGraph** ÊòØ‰∏Ä‰∏™‰∏∫ÂºÄÂèëËÄÖËÆæËÆ°ÁöÑÊñ∞Ê°ÜÊû∂ÔºåÈÄÇÂêàÈÇ£‰∫õÂÅèÂ•Ω **ÂèØËßÜÂåñÊñπÊ≥ï** Êù•ÊûÑÂª∫ËØ≠Ë®ÄÊ®°ÂûãÁÆ°ÈÅìÁöÑÁî®Êà∑„ÄÇÂÆÉÂÖÅËÆ∏ÊÇ®ÈÄöËøá **Âü∫‰∫éÂõæÁöÑÂèØËßÜÂåñ** Êù•ÊûÑÂª∫Â§çÊùÇÁöÑÂ∑•‰ΩúÊµÅÔºå‰ªéËÄåÊõ¥ÂÆπÊòìÁêÜËß£‰∏çÂêå‰ªªÂä°ÂíåÁªÑ‰ª∂‰πãÈó¥ÁöÑ‰æùËµñÂÖ≥Á≥ª„ÄÇËøôÂØπ‰∫éÂ§ö‰∏™Ê≠•È™§ÔºàÂ¶ÇÊñáÊú¨ÁîüÊàê„ÄÅÊñáÊ°£Ê£ÄÁ¥¢ÂíåÂàÜÁ±ªÔºâ‰∏≤ËÅîÂú®‰∏ÄËµ∑ÁöÑÂ§ßÂûãÂ∫îÁî®Â∞§ÂÖ∂ÊúâÁî®„ÄÇ\n\n### ‰ºòÂäøÔºö\n\n* **ÂèØËßÜÂåñÂ∑•‰ΩúÊµÅË°®Á§∫**ÔºöLangGraph ÂÖÅËÆ∏ÊÇ®ÂèØËßÜÂåñ‰∏çÂêåÁªÑ‰ª∂‰πãÈó¥ÁöÑÊï∞ÊçÆÂíåÊìç‰ΩúÊµÅ„ÄÇËøôÁßçÂõæÂΩ¢ÂåñÁöÑÊñπÊ≥ïÁõ¥ËßÇ‰∏îÊúâÂä©‰∫éËÆæËÆ°Êõ¥Â§çÊùÇÁöÑÁÆ°ÈÅì„ÄÇ\n* **Ë∞ÉËØïÁÆÄÂçï**ÔºöLangGraph ÁöÑÂèØËßÜÂåñÁâπÊÄß‰ΩøÂæóËØÜÂà´Â∑•‰ΩúÊµÅ‰∏≠ÁöÑÁì∂È¢àÊàñÈóÆÈ¢òËäÇÁÇπÂèòÂæóÊõ¥Âä†ÂÆπÊòì„ÄÇ\n\n### Á§∫‰æãÁî®‰æãÔºö\n\nÂÅáËÆæÊÇ®Ê≠£Âú®ÊûÑÂª∫‰∏Ä‰∏™Ëá™Âä®ÂåñÁ≥ªÁªüÔºåËØ•Á≥ªÁªüÈ¶ñÂÖà‰ΩøÁî®ËØ≠Ë®ÄÊ®°ÂûãÊ£ÄÁ¥¢Áõ∏ÂÖ≥ÊñáÊ°£ÔºåÁÑ∂ÂêéÂ∞ÜÂÖ∂‰º†ÈÄíÁªôÊëòË¶ÅÁîüÊàêÂô®„ÄÇÂú® LangGraph ‰∏≠ÔºåÊÇ®ÂèØ‰ª•Áõ¥ËßÇÂú∞ÁªòÂà∂Âá∫Ê≠§Â∑•‰ΩúÊµÅÁ®ãÔºåÂ±ïÁ§∫ÊØè‰∏™Ê≠•È™§‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇÂ¶ÇÊûúÈìæ‰∏≠ÁöÑ‰ªª‰Ωï‰∏ÄÁÇπÂá∫Áé∞ÈóÆÈ¢òÔºåËßÜËßâÂ∑•ÂÖ∑‰ΩøÊÇ®ËÉΩÂ§üËΩªÊùæÂÆö‰ΩçÈóÆÈ¢òÊâÄÂú®„ÄÇ\n\n### ‰ΩïÊó∂‰ΩøÁî® LangGraphÔºö\n\nÂ¶ÇÊûúÊÇ®Ê≠£Âú®ÁÆ°ÁêÜ **Â§çÊùÇÁöÑÂ∑•‰ΩúÊµÅÁ®ã**ÔºåÂπ∂‰∏îÈáçËßÜ **ÂõæÂΩ¢ÁïåÈù¢** Êù•ÁêÜËß£ÊÇ®ÁöÑÁÆ°ÈÅìÔºåLangGraph ÊòØ‰∏Ä‰∏™Áªù‰Ω≥ÁöÑÈÄâÊã©„ÄÇÂÆÉÁâπÂà´ÈÄÇÂêàÈÇ£‰∫õÊõ¥ÂñúÊ¨¢Áõ¥ËßÇÁöÑÊãñÊîæÂºèÂ∑•‰ΩúÊµÅÁ®ãËÆæËÆ°ÁöÑÂºÄÂèë‰∫∫ÂëòÊàñÊï∞ÊçÆÁßëÂ≠¶ÂÆ∂„ÄÇ\n\n**ÂÖ≥ÈîÆÁÇπ**Ôºö\n\n* Â¶ÇÊûúÊÇ®ÈúÄË¶ÅÊ∏ÖÊô∞ÁöÑËØ≠Ë®ÄÂ§ÑÁêÜÂ∑•‰ΩúÊµÅÁ®ãÁöÑÂèØËßÜÂåñË°®Á§∫„ÄÇ\n* Âú®ÂàõÂª∫ÈúÄË¶ÅÂàÜÊîØÊàñÂ§öË∑ØÂæÑ‰æùËµñÁöÑÊõ¥Â§çÊùÇÁöÑÁÆ°ÈÅìÊó∂„ÄÇ\n\n## 2. LangChainÔºöLLM Â∫îÁî®ÁöÑÂ∑•‰ΩúÈ©¨\n\n**LangChain** ÊòØÊûÑÂª∫Áî± **Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLMs)** È©±Âä®ÁöÑÂ∫îÁî®Á®ãÂ∫èÊúÄÂèóÊ¨¢ËøéÁöÑÊ°ÜÊû∂‰πã‰∏Ä„ÄÇÂÆÉÊèê‰æõ‰∫Ü‰∏ÄÁßçÁÅµÊ¥ªÁöÑ **‰ª£Á†Å‰ºòÂÖàÊñπÊ≥ï**ÔºåÂÖÅËÆ∏ÂºÄÂèëËÄÖÂ∞ÜÊñáÊ°£Ê£ÄÁ¥¢„ÄÅÊëòË¶ÅÂíåÈóÆÁ≠îÁ≠â‰ªªÂä°‰∏≤ËÅîÊàêÁªü‰∏ÄÁöÑÂ∑•‰ΩúÊµÅÁ®ã„ÄÇ\n\n### ‰ºòÂäøÔºö\n\n* **ÂπøÊ≥õÊîØÊåÅLLMs**ÔºöLangChainÂÖºÂÆπÂ§öÁßçËØ≠Ë®ÄÊ®°ÂûãÔºå‰ΩøÂæóÈõÜÊàêOpenAIÁöÑGPTÊàñÊú¨Âú∞ÊâòÁÆ°Ê®°ÂûãÂèòÂæóÁÆÄÂçï„ÄÇ\n* **ÈìæÂºèËÉΩÂäõ**ÔºöLangChainÊìÖÈïø‰∫é**Â§ö‰∏™Êìç‰ΩúÁöÑÈìæÂºèÂ§ÑÁêÜ**‚Äî‚ÄîÂõ†Ê≠§ÂæóÂêç‚Äî‚Äî‰ΩøÂºÄÂèëËÄÖËÉΩÂ§üÂàõÂª∫Â§çÊùÇÁöÑNLPÂ∫îÁî®„ÄÇ\n* **ÂπøÊ≥õÈááÁî®**Ôºö‰Ωú‰∏∫ÊúÄÂèóÊ¨¢ËøéÁöÑÊ°ÜÊû∂‰πã‰∏ÄÔºåLangChainÊã•Êúâ‰∏Ä‰∏™**Ëì¨ÂãÉÂèëÂ±ïÁöÑÁ§æÂå∫**ÂíåÂá∫Ëâ≤ÁöÑÊîØÊåÅÔºåÊèê‰æõ‰∏∞ÂØåÁöÑÊñáÊ°£ÂíåÊïôÁ®ã„ÄÇ\n\n### Á§∫‰æãÁî®‰æãÔºö\n\nÊÉ≥Ë±°‰∏Ä‰∏ãÔºåÊÇ®Ê≠£Âú®ÊûÑÂª∫‰∏Ä‰∏™ **ËÅäÂ§©Êú∫Âô®‰∫∫**ÔºåÂÆÉÈ¶ñÂÖàÁêÜËß£Áî®Êà∑ÁöÑÈóÆÈ¢òÔºå‰ªéÊï∞ÊçÆÂ∫ì‰∏≠Ê£ÄÁ¥¢Áõ∏ÂÖ≥‰ø°ÊÅØÔºåÁÑ∂ÂêéÁîüÊàêÂìçÂ∫î„ÄÇ‰ΩøÁî® LangChainÔºåÊÇ®ÂèØ‰ª•ËΩªÊùæÂú∞‰ª•ÁºñÁ®ãÊñπÂºèÂàõÂª∫Ëøô‰∏™Â§öÊ≠•È™§ÁöÑËøáÁ®ãÔºåÁ°Æ‰øùÈìæ‰∏≠ÁöÑÊØè‰∏ÄÊ≠•ÂçèË∞ÉÂ∑•‰Ωú„ÄÇ\n\n### ‰ΩïÊó∂‰ΩøÁî® LangChainÔºö\n\nÂ¶ÇÊûúÊÇ®ÊòØ‰∏Ä‰∏™ **ÊûÑÂª∫Áîü‰∫ßÁ∫ßÂ∫îÁî®ÁöÑÂºÄÂèëËÄÖ**ÔºåÂπ∂‰∏îÈúÄË¶Å‰∏Ä‰∏™ **ÁÅµÊ¥ª„ÄÅ‰ª•‰ª£Á†Å‰∏∫‰∏≠ÂøÉÁöÑËß£ÂÜ≥ÊñπÊ°à**ÔºåLangChain ÊòØÊÇ®ÁöÑÊúÄ‰Ω≥ÈÄâÊã©„ÄÇÂÆÉÈùûÂ∏∏ÈÄÇÂêàÈÇ£‰∫õÂ∏åÊúõÊéßÂà∂Â∫îÁî®Êû∂ÊûÑÂπ∂‰∏îËÉΩËàíÈÄÇÂú∞ÁºñÂÜô‰ª£Á†ÅÊù•ÂÆö‰πâÂ∑•‰ΩúÊµÅÁ®ãÁöÑÂºÄÂèëËÄÖ„ÄÇ\n\n**ÂÖ≥ÈîÆÁÇπ**Ôºö\n\n* Â¶ÇÊûúÊÇ®Ê≠£Âú®ÊûÑÂª∫ÈúÄË¶ÅË∑®Â§ö‰∏™ËØ≠Ë®ÄÊ®°ÂûãÈìæÂºè‰ªªÂä°ÁöÑÁîü‰∫ßÁ∫ßÂ∫îÁî®„ÄÇ\n* Â¶ÇÊûúÊÇ®ÈúÄË¶Å‰∏Ä‰∏™Êã•ÊúâÂπøÊ≥õÁ§æÂå∫ÊîØÊåÅÂíåÂ§öÁßçÈõÜÊàêÁöÑÂ∫ì„ÄÇ\n* ÂΩìÊÇ®ÂØπÁºñÁ®ãËß£ÂÜ≥ÊñπÊ°àÊõ¥‰∏∫ÁÜüÊÇâÔºåËÄåÈùûÂèØËßÜÂåñÂ∑•ÂÖ∑„ÄÇ\n\n## 3. LangFlow: Êó†ÈúÄÁºñÁ†Å/‰Ωé‰ª£Á†ÅÁöÑ LangChain Êâ©Â±ï\n\n**LangFlow** Êú¨Ë¥®‰∏äÊòØ **LangChain ÁöÑÂèØËßÜÂåñÊâ©Â±ï**„ÄÇÂÆÉÂ∞Ü LangChain Âº∫Â§ßÁöÑÂêéÁ´Ø‰∏é **Áõ¥ËßÇÁöÑÊãñÊîæÁïåÈù¢** ÁªìÂêàÂú®‰∏ÄËµ∑„ÄÇLangFlow ‰ΩøÈÇ£‰∫õÂèØËÉΩ‰∏çÂ§™ÊìÖÈïøÁºñÂÜô‰ª£Á†ÅÁöÑÁî®Êà∑‰ªçÁÑ∂ËÉΩÂ§üÂú®‰ªñ‰ª¨ÁöÑÂ∫îÁî®Á®ãÂ∫è‰∏≠Âà©Áî®ËØ≠Ë®ÄÊ®°ÂûãÁöÑÂº∫Â§ßÂäüËÉΩ„ÄÇ\n\n### ‰ºòÂäøÔºö\n\n* **ÂèØËßÜÂåñÂ∑•‰ΩúÊµÅÂàõÂª∫**Ôºö‰∏é LangGraph Á±ª‰ººÔºåLangFlow Êèê‰æõ‰∫Ü‰∏Ä‰∏™ÂèØËßÜÂåñÁïåÈù¢Áî®‰∫éÊûÑÂª∫Â∑•‰ΩúÊµÅ„ÄÇÁÑ∂ËÄåÔºåÂÆÉÊòØÂü∫‰∫é LangChain ÊûÑÂª∫ÁöÑÔºåËøôÊÑèÂë≥ÁùÄÁî®Êà∑ÂèØ‰ª•Âà©Áî® LangChain ÁöÑÂº∫Â§ßÂäüËÉΩÔºåËÄåÊó†ÈúÄÁºñÂÜôÂ§ßÈáè‰ª£Á†Å„ÄÇ\n* **Âø´ÈÄüÂéüÂûãÂà∂‰ΩúÁöÑÁêÜÊÉ≥ÈÄâÊã©**ÔºöLangFlow ÈùûÂ∏∏ÈÄÇÂêàÂø´ÈÄü **ÂéüÂûãÂåñÊÉ≥Ê≥ï** ÊàñÊûÑÂª∫Ê¶ÇÂøµÈ™åËØÅÂ∫îÁî®Á®ãÂ∫è„ÄÇ\n* **ÈÄÇÂêàÂàùÂ≠¶ËÄÖ**ÔºöÂÆÉÊòØ‰∏Ä‰∏™ÂæàÂ•ΩÁöÑÂÖ•Èó®ÁÇπÔºåÈÄÇÂêàÈÇ£‰∫õÂØπÁºñÁ†Å‰∏çÂ§™ÁÜüÊÇâ‰ΩÜÊÉ≥Ë¶ÅÂàõÂª∫ËØ≠Ë®ÄÊ®°ÂûãÂ∑•‰ΩúÊµÅÁöÑÁî®Êà∑„ÄÇ\n\n### Á§∫‰æãÁî®‰æãÔºö\n\nÂ¶ÇÊûúÊÇ®ÊÉ≥Âø´ÈÄüÊûÑÂª∫‰∏Ä‰∏™**ÊëòË¶ÅÂ∑•ÂÖ∑**Êù•Ê£ÄÁ¥¢ÊñáÊ°£ÔºåÊÇ®ÂèØ‰ª•Âú®LangFlowÁöÑÁïåÈù¢‰∏≠ÊãñÊîæÁªÑ‰ª∂Ôºå‰ª•ÂàõÂª∫‰∏Ä‰∏™ÂÆåÂÖ®ÂäüËÉΩÁöÑÂ∫îÁî®Á®ãÂ∫è„ÄÇËøôÂèØ‰ª•Âú®Âá†‰πé‰∏çÁºñÂÜô‰ª£Á†ÅÁöÑÊÉÖÂÜµ‰∏ãÂÆåÊàê„ÄÇ\n\n### ‰ΩïÊó∂‰ΩøÁî® LangFlowÔºö\n\nLangFlow ÈùûÂ∏∏ÈÄÇÂêà **ÈùûÂºÄÂèë‰∫∫Âëò** Êàñ **Âø´ÈÄüÂéüÂûãËÆæËÆ°**„ÄÇÂ¶ÇÊûúÊÇ®ÊÉ≥Âø´ÈÄüÂÆûÈ™å **LLM Â∑•‰ΩúÊµÅ** ËÄå‰∏çÊ∑±ÂÖ•‰ª£Á†ÅÔºåËøô‰∏™Â∑•ÂÖ∑ÂèØ‰ª•ËÆ©ÊÇ®ËΩªÊùæÂÖ•Èó®„ÄÇ\n\n**ÂÖ≥ÈîÆÁÇπ**Ôºö\n\n* Â¶ÇÊûúÊÇ®ÊÉ≥Âø´ÈÄüÂéüÂûãËÆæËÆ° LLM Â∑•‰ΩúÊµÅËÄå‰∏çÁºñÂÜô‰ª£Á†Å„ÄÇ\n* Â¶ÇÊûúÊÇ®ÂØπËßÜËßâÁºñÁ®ãÊÑüÂà∞ËàíÈÄÇÔºå‰ΩÜÈúÄË¶Å LangChain ÁöÑÁÅµÊ¥ªÊÄß„ÄÇ\n* Áî®‰∫éÊïôËÇ≤ÁõÆÁöÑÔºåÂ∏ÆÂä©Áî®Êà∑‰∫ÜËß£Â¶Ç‰ΩïÊûÑÂª∫Â∑•‰ΩúÊµÅ„ÄÇ\n\n## 4. LangSmith: ÁõëÊéß‰∏éÂèØËßÇÂØüÊÄß\n\nËôΩÁÑ∂ÂÖ∂‰ªñÂ∑•ÂÖ∑‰∏ìÊ≥®‰∫é **ÊûÑÂª∫Â∑•‰ΩúÊµÅÁ®ã**Ôºå**LangSmith** ÁöÑËÆæËÆ°ÁõÆÊ†áÊòØ **ÁõëÊéß** Âíå **Ë∞ÉËØï** ËØ≠Ë®ÄÊ®°ÂûãÂ∫îÁî®„ÄÇÂÆÉÊèê‰æõ‰∫ÜÂÖàËøõÁöÑÂèØËßÇÂØüÊÄßÂäüËÉΩÔºå‰ª•Ë∑üË∏™ÊÇ®ÁöÑÂ∑•‰ΩúÊµÅÁ®ãÂíåÊ®°ÂûãÁöÑÊÄßËÉΩÔºå‰ΩøÂÖ∂Âú®Áîü‰∫ßÁéØÂ¢É‰∏≠‰∏çÂèØÊàñÁº∫„ÄÇ\n\n### ‰ºòÂäøÔºö\n\n* **Ê∑±Â∫¶ÂèØËßÇÂØüÊÄß**ÔºöLangSmith ÂÖÅËÆ∏ÂºÄÂèëËÄÖÁõëÊéßËØ≠Ë®ÄÊ®°ÂûãÁöÑÊÄßËÉΩÔºåÁ°Æ‰øùÂ∑•‰ΩúÊµÅÁ®ãÊåâÈ¢ÑÊúüËøêË°å„ÄÇ\n* **ÈîôËØØË∑üË∏™**ÔºöÂÆÉÂú®Â∏ÆÂä©ÂºÄÂèëËÄÖÂÆö‰ΩçÈóÆÈ¢òÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩøË∞ÉËØïÂèòÂæóÊõ¥Âä†ÂÆπÊòì„ÄÇ\n* **ÊÄßËÉΩÊ¥ûÂØü**ÔºöLangSmith Êèê‰æõÊúâÂÖ≥ **Â∑•‰ΩúÊµÅÁ®ãÊÄßËÉΩ** ÁöÑÊ¥ûÂØüÔºåÂ∏ÆÂä©ÂºÄÂèëËÄÖ‰ºòÂåñ‰ªñ‰ª¨ÁöÑÂ∫îÁî®Á®ãÂ∫è„ÄÇ\n\n### Á§∫‰æãÁî®‰æãÔºö\n\nÂÅáËÆæÊÇ®Â∑≤ÁªèÈÉ®ÁΩ≤‰∫Ü‰∏Ä‰∏™**ÂÆ¢Êà∑ÊúçÂä°ËÅäÂ§©Êú∫Âô®‰∫∫**ÔºåËØ•ËÅäÂ§©Êú∫Âô®‰∫∫‰ΩøÁî®ËØ≠Ë®ÄÊ®°ÂûãÊù•ÂõûÁ≠îÈóÆÈ¢ò„ÄÇÈöèÁùÄÊó∂Èó¥ÁöÑÊé®ÁßªÔºåÊÇ®‰ºöÂèëÁé∞Êüê‰∫õÂõûÁ≠îÁöÑÂáÜÁ°ÆÊÄß‰Ωé‰∫éÈ¢ÑÊúü„ÄÇLangSmith ÂèØ‰ª•Â∏ÆÂä©ÊÇ®ËøΩË∏™ÈóÆÈ¢òÔºåÈÄöËøáÊèê‰æõÂØπÂ∑•‰ΩúÊµÅÁ®ã‰∏≠ÊØè‰∏™ÂÜ≥Á≠ñÁÇπÁöÑÂèØËßÅÊÄß„ÄÇ\n\n### ‰ΩïÊó∂‰ΩøÁî® LangSmithÔºö\n\nÂ¶ÇÊûúÊÇ®Âú® **Áîü‰∫ßÁéØÂ¢É** ‰∏≠ÈÉ®ÁΩ≤Â∫îÁî®Á®ãÂ∫èÔºåÂπ∂‰∏îÈúÄË¶ÅÁ°Æ‰øù **ÂÅ•Â£ÆÊÄß„ÄÅÂèØÈù†ÊÄßÂíåÊÄßËÉΩ**ÔºåLangSmith ÊòØ‰∏Ä‰∏™‰∏çÂèØÊàñÁº∫ÁöÑÂ∑•ÂÖ∑„ÄÇÂÆÉÂú®ÁÆ°ÁêÜ **ÈúÄË¶ÅÈöèÁùÄÊó∂Èó¥Ë∞ÉËØïÂíå‰ºòÂåñÁöÑÂ§çÊùÇÁ≥ªÁªü** Êó∂ÁâπÂà´ÊúâÁî®„ÄÇ\n\n**ÂÖ≥ÈîÆÁÇπ**Ôºö\n\n* Â¶ÇÊûúÊÇ®ÈúÄË¶Å LLM Â∑•‰ΩúÊµÅ‰∏≠ÁöÑÈ´òÁ∫ßÁõëÊéßÊàñË∞ÉËØïËÉΩÂäõ„ÄÇ\n* ÂØπ‰∫éËßÇÂØüÊÄßÂØπÁ°Æ‰øùÊúÄ‰Ω≥Ê®°ÂûãÊÄßËÉΩËá≥ÂÖ≥ÈáçË¶ÅÁöÑÂºÄÂèëÁéØÂ¢É„ÄÇ\n* Â¶ÇÊûúÊÇ®ÁöÑÈáçÁÇπÊòØÂü∫‰∫éÂÆûÊó∂Ê¥ûÂØüÊîπËøõÂíåËø≠‰ª£ LLM È©±Âä®ÁöÑÂ∫îÁî®Á®ãÂ∫è„ÄÇ\n\n## Âì™‰∏™Êõ¥ÈÄÇÂêà‰Ω†Ôºü\n\n* **‰ΩøÁî® LangGraph** Â¶ÇÊûú‰Ω†Êõ¥ÂñúÊ¨¢Âü∫‰∫éÂõæÂΩ¢ÁöÑÂèØËßÜÂåñÂ∑•‰ΩúÊµÅÁ®ãÊù•ÊûÑÂª∫Â§çÊùÇÁöÑ LLM ‰ªªÂä°„ÄÇÈùûÂ∏∏ÈÄÇÂêàÈúÄË¶ÅÊ∏ÖÊô∞ÂíåÁªìÊûÑÁöÑÁî®Êà∑„ÄÇ\n* **‰ΩøÁî® LangChain** Â¶ÇÊûú‰Ω†ÈúÄË¶Å‰∏Ä‰∏™Âº∫Â§ß„ÄÅÁÅµÊ¥ªÁöÑËß£ÂÜ≥ÊñπÊ°àÊù•‰ª•ÁºñÁ®ãÊñπÂºèÂàõÂª∫ËØ≠Ë®ÄÊ®°ÂûãÂ∫îÁî®„ÄÇÂÆÉÂ§öÂäüËÉΩ‰∏îÈùûÂ∏∏ÈÄÇÂêàÊûÑÂª∫Áîü‰∫ßÁ∫ßÂ∫îÁî®ÁöÑÂºÄÂèëËÄÖ„ÄÇ\n* **‰ΩøÁî® LangFlow** Â¶ÇÊûú‰Ω†ÊÉ≥Ë¶Å LangChain ÁöÑÂº∫Â§ßÂäüËÉΩÔºåÂêåÊó∂ÂèàÂ∏åÊúõÊã•Êúâ‰∏Ä‰∏™ÂèØËßÜÂåñÁöÑÊó†‰ª£Á†Å/‰Ωé‰ª£Á†ÅÁïåÈù¢„ÄÇÊúÄÈÄÇÂêàÂø´ÈÄüÂéüÂûãÂºÄÂèëÂíåÊõ¥ÂñúÊ¨¢ÂèØËßÜÂåñÂ∑•ÂÖ∑ËÄåÈùûÁºñÁ†ÅÁöÑÁî®Êà∑„ÄÇ\n* **‰ΩøÁî® LangSmith** Â¶ÇÊûú‰Ω†ÁöÑÈáçÁÇπÊòØ LLM Â∫îÁî®ÁöÑÂèØËßÇÂØüÊÄßÂíåË∞ÉËØï„ÄÇÈùûÂ∏∏ÈÄÇÂêàÂú®ÂºÄÂèëÊàñÁîü‰∫ßÁéØÂ¢É‰∏≠ÁõëÊéßÂíå‰ºòÂåñÂ∑•‰ΩúÊµÅÁ®ã„ÄÇ\n\nÊúÄÁªàÔºå‰Ω†ÁöÑÈÄâÊã©ÂèñÂÜ≥‰∫é‰Ω†ÂØπ‰ª£Á†ÅÁöÑËàíÈÄÇÂ∫¶„ÄÅÂ∑•‰ΩúÊµÅÁ®ãÁöÑÂ§çÊùÇÊÄßÔºå‰ª•Âèä‰Ω†ÊòØÂê¶‰ºòÂÖàËÄÉËôëÊòìÁî®ÊÄß„ÄÅÁÅµÊ¥ªÊÄßÊàñÂèØËßÇÂØüÊÄß„ÄÇ\n\n## ÁªìËÆ∫\n\nËøô‰∫õÂ∑•ÂÖ∑ ‚Äî **LangGraph**„ÄÅ**LangChain**„ÄÅ**LangFlow** Âíå **LangSmith** ‚Äî ÈíàÂØπÂºÄÂèëÂíåÁÆ°ÁêÜËØ≠Ë®ÄÊ®°ÂûãÂ∫îÁî®ÁöÑ‰∏çÂêåÈò∂ÊÆµ„ÄÇ**LangGraph** Êèê‰æõ‰∫Ü‰∏ÄÁßçÂèØËßÜÂåñ„ÄÅÁõ¥ËßÇÁöÑÊñπÂºèÊù•ÊûÑÂª∫Â§çÊùÇÁöÑÂ∑•‰ΩúÊµÅÁ®ãÔºåËÄå **LangChain** Âàô‰∏∫Â∏åÊúõÂàõÂª∫ÂèØÊâ©Â±ïÂ∫îÁî®ÁöÑÂºÄÂèëËÄÖÊèê‰æõ‰∫Ü‰∏ÄÁßçÂº∫Â§ßÁöÑ‰ª£Á†Å‰ºòÂÖàËß£ÂÜ≥ÊñπÊ°à„ÄÇÂØπ‰∫éÈÇ£‰∫õÊõ¥ÂñúÊ¨¢ **‰Ωé‰ª£Á†Å**„ÄÅÊãñÊîæÊñπÂºèÁöÑÁî®Êà∑Ôºå**LangFlow** Âú®‰∏çÁâ∫Áâ≤ÂäüËÉΩÁöÑÊÉÖÂÜµ‰∏ãÁÆÄÂåñ‰∫ÜÊµÅÁ®ã„ÄÇÊúÄÂêéÔºå**LangSmith** ‰∏ìÊ≥®‰∫éÂèØËßÇÂØüÊÄßÂíåË∞ÉËØïÔºåÁ°Æ‰øùÊÇ®ÁöÑÂ∑•‰ΩúÊµÅÁ®ãÊòØ‰ºòÂåñÂíåÂèØÈù†ÁöÑ„ÄÇÈÄâÊã©ÂêàÈÄÇÁöÑÂ∑•ÂÖ∑ÂèñÂÜ≥‰∫éÊÇ®ÁöÑÈ°πÁõÆÈúÄÊ±ÇÔºåÊó†ËÆ∫ÊòØÂø´ÈÄüÂéüÂûãËÆæËÆ°„ÄÅÁîü‰∫ßÁ∫ßÊâ©Â±ïÔºåËøòÊòØÁõëÊéßÂíåÊÄßËÉΩË∑üË∏™„ÄÇ\n\nÂø´‰πêÁºñÁ†ÅÔºÅ üéâ\n\nüë®üèæ‚Äçüíª [GitHub](https://github.com/mdmonsurali) ‚≠êÔ∏è | üëî[LinkedIn](https://www.linkedin.com/in/mdmonsurali/) |üìù [Medium](https://medium.com/@monsuralirana)\n\nÊÑüË∞¢ÊÇ®Ëä±Êó∂Èó¥ÈòÖËØªËøôÁØáÊñáÁ´†ÔºÅ\n\nËØ∑Âä°ÂøÖÁïô‰∏ãÊÇ®ÁöÑÂèçÈ¶àÂíåËØÑËÆ∫„ÄÇ‰∏ãÊ¨°ÂçöÂÆ¢ËßÅÔºåÊï¨ËØ∑ÂÖ≥Ê≥® üì¢\n\n## ÂèÇËÄÉÊñáÁåÆÔºö\n\n1. ‚ÄúLangChain ÊñáÊ°£‚Äù ‚Äî <https://python.langchain.com/docs/introduction/>\n2. ‚ÄúLangGraph Ê¶ÇËø∞‚Äù ‚Äî <https://langchain-ai.github.io/langgraph/>\n3. ‚ÄúLangFlow GitHub ‰ªìÂ∫ì‚Äù ‚Äî [https://github.com/LangFlow/LangFlow](https://docs.langflow.org/)\n4. ‚ÄúLangSmith ‰ªãÁªç‚Äù ‚Äî <https://www.langchain.com/langsmith>\n5. ‚ÄúÂ¶Ç‰Ωï‰ΩøÁî® LangChain ÊûÑÂª∫ËÅäÂ§©Êú∫Âô®‰∫∫‚Äù by JetBrains ÂçöÂÆ¢ ‚Äî <https://blog.jetbrains.com/pycharm/2024/08/how-to-build-chatbots-with-langchain/>\n\n"},{"lang":"zh","group":"blog","slug":"blog/o1-preview-vs-claude-3-5-sonnet-comparing-top-llms-d68734b53c93","frontmatter":{"title":"o1-preview ‰∏é claude-3.5-sonnetÔºöÊØîËæÉÈ°∂Á∫ßÊ≥ïÂ≠¶Á°ïÂ£´","meta_title":"o1-preview ‰∏é claude-3.5-sonnetÔºöÊØîËæÉÈ°∂Á∫ßÊ≥ïÂ≠¶Á°ïÂ£´","description":"‰∫ÜËß£ OpenAI ÁöÑ o1 È¢ÑËßàÁâà‰∏é Claude 3.5 Sonnet Âú®ÊÄßËÉΩ„ÄÅÈÄüÂ∫¶ÂíåÂäüËÉΩÊñπÈù¢ÁöÑÊØîËæÉ„ÄÇ","date":"2024-10-27T13:58:01.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kTWAcpRdOpsrFIDZjjjr7Q.jpeg","categories":["Programming","Machine Learning","Generative AI"],"author":"Rifx.Online","tags":["o1-preview","Claude","throughput","latency","reasoning"],"draft":false,"slug":"blog/o1-preview-vs-claude-3-5-sonnet-comparing-top-llms-d68734b53c93"},"content":"\n\n\n‰ªäÂ§©Ôºà2024Âπ¥9Êúà12Êó•ÔºâÔºåOpenAI ÂèëÂ∏É‰∫ÜÂÖ∂ÊúÄÊñ∞ÁöÑËØ≠Ë®ÄÊ®°Âûã o1-preview„ÄÇËøô‰∏™ÂÖàËøõÁöÑÊ®°ÂûãÁªèËøáËÆæËÆ°ÔºåËÉΩÂ§üÂú®ÁîüÊàêÂìçÂ∫î‰πãÂâçÊäïÂÖ•Êõ¥Â§öÊó∂Èó¥ËøõË°åÂ§ÑÁêÜÔºå‰ΩøÂÖ∂ËÉΩÂ§üÊõ¥Â•ΩÂú∞Â∫îÂØπÂ§çÊùÇ‰ªªÂä°ÔºåÂπ∂Âú®ÁßëÂ≠¶„ÄÅÁºñÁ†ÅÂíåÊï∞Â≠¶Á≠âÈ¢ÜÂüüËß£ÂÜ≥ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÈóÆÈ¢ò„ÄÇ\n\nÂú®ËøôÁØáÂçöÂÆ¢ÊñáÁ´†‰∏≠ÔºåÊàë‰ª¨Â∞ÜÊ∑±ÂÖ•ÂàÜÊûê o1-previewÔºåÂπ∂Â∞ÜÂÖ∂‰∏é‰πãÂâçË¢´ËÆ§‰∏∫ÊòØÊúÄÂÖàËøõÊ®°Âûã‰πã‰∏ÄÁöÑ Claude 3.5 Sonnet ËøõË°åÊØîËæÉ„ÄÇ\n\n\n\n## ÊØîËæÉÊñπÊ≥ïËÆ∫\n\nÊàë‰ª¨ÁöÑÂàÜÊûêÂà©Áî®‰∫Ü [Keywords AI ÁöÑ LLM playground](https://docs.keywordsai.co/features/prompt/model-playground)ÔºåËøôÊòØ‰∏Ä‰∏™ÊîØÊåÅË∂ÖËøá 200 ÁßçËØ≠Ë®ÄÊ®°ÂûãÂπ∂Êèê‰æõÂáΩÊï∞Ë∞ÉÁî®ÂäüËÉΩÁöÑÂπ≥Âè∞„ÄÇÊàë‰ª¨Â∞ÜÊé¢ËÆ®‰ª•‰∏ãÂá†‰∏™ÊñπÈù¢Ôºö\n\n* Âü∫Êú¨ÊØîËæÉ\n* Âü∫ÂáÜÊØîËæÉ\n* Â§ÑÁêÜÈÄüÂ∫¶\n* ËØÑ‰º∞ÊåáÊ†á\n* Âª∫ËÆÆÁöÑ‰ΩøÁî®Ê°à‰æã\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*yc171ikejtBy_o11.jpeg)\n\n## Âü∫Êú¨ÊØîËæÉ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*z2FrS_AVig7Y6eU_.jpeg)\n\nÊ≥®ÊÑèÔºöo1-preview ‰∏çÊîØÊåÅÊµÅÂºè‰º†Ëæì„ÄÅÂáΩÊï∞Ë∞ÉÁî®ÂíåÁ≥ªÁªüÊ∂àÊÅØ„ÄÇ\n\n## Âü∫ÂáÜÊØîËæÉ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Bx_vAvFc9DAD0cZA.jpeg)\n\nO1-preview Âú®ÊâÄÊúâÂü∫ÂáÜÊµãËØï‰∏≠ÈÉΩ‰ºò‰∫é Claude 3.5 Sonnet„ÄÇÊúÄÂ∞èÁöÑÂ∑ÆË∑ùÂá∫Áé∞Âú® MMLUÔºà‰∏ÄËà¨Áü•ËØÜÔºâ‰∏≠„ÄÇGPQA Diamond ÊµãËØïÁ†îÁ©∂ÁîüÊ∞¥Âπ≥ÁöÑÊé®ÁêÜÔºåÊòæÁ§∫Âá∫ÊòæËëóÁöÑÊÄßËÉΩÂ∑ÆÂºÇ„ÄÇMATH Âü∫ÂáÜÊè≠Á§∫‰∫ÜÊúÄÂ§ßÁöÑÂ∑ÆË∑ùÔºåÁ™ÅÊòæ‰∫Ü o1-preview ÁöÑÈ´òÁ∫ßÊï∞Â≠¶ËÉΩÂäõ„ÄÇËøô‰∫õÁªìÊûúË°®ÊòéÔºåo1-preview Âú®Â§çÊùÇÊé®ÁêÜÂíåÂêÑ‰∏™È¢ÜÂüüÁöÑÈóÆÈ¢òËß£ÂÜ≥ÊñπÈù¢Êúâ‰∫ÜÊòæËëóÊîπÂñÑ„ÄÇ\n\n## ÈÄüÂ∫¶ÊØîËæÉ\n\nO1-preview ÁöÑÊÄùËÄÉÂíåÂìçÂ∫îÊó∂Èó¥ÊØîÂÖ∂‰ªñ LLM Êõ¥Èïø„ÄÇËôΩÁÑ∂Áõ¥Êé•ÁöÑÈÄüÂ∫¶ÊØîËæÉÂèØËÉΩÂπ∂‰∏çÂÆåÂÖ®ÂÖ¨Âπ≥Ôºå‰ΩÜÊµãËØï o1-preview ÁöÑÈÄüÂ∫¶Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇËøô‰∫õ‰ø°ÊÅØÂ∏ÆÂä©ÂºÄÂèëËÄÖÊõ¥Â•ΩÂú∞ÁêÜËß£ o1-preview ÁöÑËÉΩÂäõÔºåÂπ∂Âà§Êñ≠ÂÆÉÊòØÂê¶ÈÄÇÂêà‰ªñ‰ª¨ÁöÑÈ°πÁõÆ„ÄÇÊ≥®ÊÑèÔºöÁî±‰∫é o1-preview ‰∏çÊîØÊåÅÊµÅÂºè‰º†ËæìÔºåÊàë‰ª¨Â∑≤‰∏∫‰∏§‰∏™Ê®°ÂûãÁ¶ÅÁî®ÊµÅÂºè‰º†Ëæì„ÄÇÂõ†Ê≠§ÔºåÊó†Ê≥ïÊµãÈáèÈ¶ñÊ¨°‰ª§ÁâåÊó∂Èó¥ÔºàTTFTÔºâ„ÄÇ\n\n## Âª∂Ëøü\n\nÊàë‰ª¨ÁöÑÊµãËØïÊ∂âÂèäÊØè‰∏™Ê®°ÂûãÊï∞Áôæ‰∏™ËØ∑Ê±ÇÔºåÊè≠Á§∫‰∫ÜÊòæËëóÁöÑÂ∑ÆÂºÇ„ÄÇClaude 3.5 Sonnet ÁöÑÂπ≥ÂùáÂª∂Ëøü‰∏∫ 18.3 Áßí/ËØ∑Ê±ÇÔºåËÄå o1-preview ÁöÑÂπ≥ÂùáÂª∂Ëøü‰∏∫ 39.4 Áßí/ËØ∑Ê±Ç„ÄÇo1-preview ÊòæËëóÊõ¥ÈïøÁöÑÂª∂ËøüÊòØÁî±‰∫éÂÖ∂Âª∂ÈïøÁöÑÊÄùËÄÉÂíåÊé®ÁêÜËøáÁ®ã„ÄÇ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*2PMkgPVuylFxwfIa.jpeg)\n\n## ÂêûÂêêÈáèÔºàÊØèÁßí‰ª§ÁâåÊï∞Ôºâ\n\nÂ∞ΩÁÆ°Âª∂ËøüËæÉÈ´òÔºåo1-previewÁöÑÂêûÂêêÈáèÊõ¥‰∏∫Âá∫Ëâ≤„ÄÇo1-previewÁîüÊàê92.94‰∏™‰ª§Áâå/ÁßíÔºåËÄåClaude 3.5 SonnetÁîüÊàê74.87‰∏™‰ª§Áâå/Áßí„ÄÇËøôË°®Êòéo1-previewËæÉÈïøÁöÑÁîüÊàêÊó∂Èó¥‰∏ªË¶ÅÊòØÁî±‰∫éÂÖ∂ÂàùÂßãÂ§ÑÁêÜÈò∂ÊÆµÔºåËÄåÈùû‰ª§ÁâåÁîüÊàêÈÄüÂ∫¶„ÄÇ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*wxqpnwZhl9pnbw8y.jpeg)\n\n## ÊÄßËÉΩÊØîËæÉ\n\nÊàë‰ª¨Âú®[Keywords AIÂπ≥Âè∞](https://keywordsai.co/)‰∏äËøõË°å‰∫ÜËØÑ‰º∞ÊµãËØï„ÄÇËØÑ‰º∞ÂåÖÊã¨‰∏â‰∏™ÈÉ®ÂàÜÔºö\n\n* **ÁºñÁ†Å‰ªªÂä°**Ôºö‰∏§‰∏™Ê®°ÂûãÊàêÂäüÂÆåÊàê‰∫ÜÂâçÁ´ØÂíåÂêéÁ´ØÂºÄÂèë‰ªªÂä°„ÄÇO1-previewÂú®Â§ÑÁêÜËæÉÈïø‰∏ä‰∏ãÊñáÊó∂Ë°®Áé∞Êõ¥‰Ω≥ÔºåËÉΩÂ§üÂú®Á¨¨‰∏ÄÊ¨°Â∞ùËØï‰∏≠Êõ¥ÊúâÊïàÂú∞ËØÜÂà´ÂíåËß£ÂÜ≥bug„ÄÇÂÆÉËøòÂ±ïÁé∞‰∫ÜÊõ¥ÂÖ®Èù¢ÁöÑ‰ª£Á†ÅÂàÜÊûêËÉΩÂäõ„ÄÇ\n* **ÈÄªËæëÊé®ÁêÜ**ÔºöO1-previewÂú®Êé®ÁêÜ‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇÂÆÉÁöÑÊÄùÁª¥ËøáÁ®ã‰∏é‰∫∫Á±ªËÆ§Áü•ÈùûÂ∏∏Áõ∏‰ºº„ÄÇËôΩÁÑ∂Claude 3.5 SonnetÂú®Â§ßÂ§öÊï∞ÈóÆÈ¢ò‰∏äË°®Áé∞ËâØÂ•ΩÔºå‰ΩÜo1-previewÂßãÁªàËÉΩÂ§üËß£ÂÜ≥Â§çÊùÇÁöÑÊé®ÁêÜÊåëÊàòÔºåÂåÖÊã¨ÂõΩÈôÖÊï∞Â≠¶Â••ÊûóÂåπÂÖãÔºàIMOÔºâÁ∫ßÂà´ÁöÑÈóÆÈ¢ò„ÄÇ\n* **ÂÜô‰Ωú‰ªªÂä°**Ôºö‰∏§‰∏™Ê®°ÂûãÂú®ÂÜô‰Ωú‰ªªÂä°‰∏äË°®Áé∞ÈùûÂ∏∏Âá∫Ëâ≤„ÄÇÂÆÉ‰ª¨Â±ïÁé∞‰∫ÜÊí∞ÂÜôÁúüÂÆû„ÄÅ‰∏™ÊÄßÂåñÁöÑÂÜ∑ÈÇÆ‰ª∂‰ª•ÂèäÁÆÄÊ¥Å‰∏îÊúâÊÑè‰πâÁöÑÂçöÂÆ¢ÊñáÁ´†ÁöÑËÉΩÂäõ„ÄÇ\n\n## Ê®°ÂûãÊé®Ëçê\n\no1-preview\n\n* **ÊúÄ‰Ω≥ÈÄâÊã©Ôºö** ÈÄÇÁî®‰∫éÊï∞Â≠¶„ÄÅÁºñÁ†ÅÂíåÁâ©ÁêÜÂ≠¶‰∏≠ÁöÑÂ§çÊùÇÈóÆÈ¢òËß£ÂÜ≥„ÄÇÁâπÂà´ÈÄÇÂêàÂ§ÑÁêÜÊåëÊàòÊÄß‰ªªÂä°ÁöÑÁ†îÁ©∂‰∫∫Âëò„ÄÇ\n* **‰∏çÈÄÇÂêàÔºö** ÈúÄË¶ÅÂø´ÈÄüÂìçÂ∫îÊó∂Èó¥Êàñ‰∏•Èáç‰æùËµñÁ≥ªÁªüÊèêÁ§∫ÁöÑAIÂ∫îÁî®„ÄÇÁî±‰∫éÁº∫‰πèÊµÅÂ™í‰ΩìÊîØÊåÅÔºå‰∏çÈÄÇÁî®‰∫éËØ≠Èü≥AIÂ∫îÁî®„ÄÇ\n\nClaude 3.5 Sonnet\n\n* **ÊúÄ‰Ω≥ÈÄâÊã©Ôºö** ÈÄÇÁî®‰∫éÂ§ßÂ§öÊï∞ÈúÄË¶ÅÈóÆÈ¢òËß£ÂÜ≥ËÉΩÂäõÂíåÈ´òË¥®ÈáèÂÜÖÂÆπÁîüÊàêÁöÑAIÂ∫îÁî®„ÄÇ\n* **‰∏çÈÄÇÂêàÔºö** ËØ≠Èü≥AIÂ∫îÁî®ÊàñÂØπÈ¢ÑÁÆóÈôêÂà∂‰∏•Ê†º„ÄÅÈúÄË¶ÅËæÉ‰ΩéËøêËê•ÊàêÊú¨ÁöÑÈ°πÁõÆ„ÄÇ\n\n## Â¶Ç‰ΩïÂ∞Ü o1-preview ÈõÜÊàêÂà∞ÊÇ®ÁöÑ AI Â∫îÁî®‰∏≠\n\nË¶ÅÂ∞Ü o1-preview ÈõÜÊàêÂà∞ÊÇ®ÁöÑ AI Â∫îÁî®‰∏≠ÔºåÂè™ÈúÄËÆøÈóÆ Keywords AI Ê®°ÂûãÈ°µÈù¢Âπ∂ÊâæÂà∞‚ÄúÊü•Áúã‰ª£Á†Å‚ÄùÊåâÈíÆ„ÄÇÁÇπÂáªÊ≠§ÊåâÈíÆ‰ª•Â§çÂà∂Êèê‰æõÁöÑ‰ª£Á†ÅÁâáÊÆµÔºåÁÑ∂ÂêéÂ∞ÜÂÖ∂Áõ¥Êé•Á≤òË¥¥Âà∞ÊÇ®ÁöÑ‰ª£Á†ÅÂ∫ì‰∏≠„ÄÇÈÄöËøáËøô‰∏™ÁÆÄÂçïÁöÑËøáÁ®ãÔºåÊÇ®Â∞ÜËÉΩÂ§üÂú®È°πÁõÆ‰∏≠Âà©Áî® o1-preview ÁöÑÂº∫Â§ßÂäüËÉΩÔºå‰ΩøÊÇ®ËÉΩÂ§üËΩªÊùæÂ∫îÂØπÂ§çÊùÇÈóÆÈ¢òÂπ∂ÁîüÊàêÈ´òË¥®ÈáèÂÜÖÂÆπ„ÄÇ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*XyQ9QiI7TN8Uc5Jp.jpeg)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*t8fEYlEs13eM7D28lVbtIw.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*yhu9y5ixNuxeFVe1.png)\n\nÊ≠§ÊïÖ‰∫ãÂèëÂ∏ÉÂú® [Generative AI](https://generativeai.pub/)„ÄÇËØ∑Âú® [LinkedIn](https://www.linkedin.com/company/generative-ai-publication) ‰∏ä‰∏éÊàë‰ª¨ËÅîÁ≥ªÔºåÂπ∂ÂÖ≥Ê≥® [Zeniteq](https://www.zeniteq.com/)Ôºå‰ª•‰æøËé∑ÂèñÊúÄÊñ∞ÁöÑ AI ËµÑËÆØ„ÄÇ\n\nËÆ¢ÈòÖÊàë‰ª¨ÁöÑ [newsletter](https://www.generativeaipub.com/) Âíå [YouTube](https://www.youtube.com/@generativeaipub) È¢ëÈÅìÔºåÂèäÊó∂‰∫ÜËß£ÁîüÊàê AI ÁöÑÊúÄÊñ∞Ê∂àÊÅØÂíåÂä®ÊÄÅ„ÄÇËÆ©Êàë‰ª¨ÂÖ±ÂêåÂ°ëÈÄ† AI ÁöÑÊú™Êù•ÔºÅ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*PelNtaNaEVDWgMWr.png)\n\n"},{"lang":"zh","group":"blog","slug":"blog/qwen2-5-1-5b-the-future-of-mobile-ai-6bd5f29bbc84","frontmatter":{"title":"Qwen2.5 1.5bÔºöÁßªÂä®AIÁöÑÊú™Êù•Ôºü","meta_title":"Qwen2.5 1.5bÔºöÁßªÂä®AIÁöÑÊú™Êù•Ôºü","description":"ÈòøÈáå‰∫ëÊúÄÊñ∞ LLM ÁöÑÊú¨Âú∞ÊµãËØïÂíåËØÑ‰º∞„ÄÇ‰ΩøÁî® llama-cpp-python Âíå DIY ÊèêÁ§∫ÁõÆÂΩï„ÄÇ","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*awb56jkdXobA-Ip6d-QHRA.png","categories":["Programming","Technology","Science"],"author":"Rifx.Online","tags":["Qwen2.5","summarization","retrieval","mobile","improvements"],"draft":false,"slug":"blog/qwen2-5-1-5b-the-future-of-mobile-ai-6bd5f29bbc84"},"content":"\n### Êú¨Âú∞ÊµãËØïÂíåËØÑ‰º∞ÈòøÈáå‰∫ëÊúÄÊñ∞ÁöÑ LLM„ÄÇ‰ΩøÁî® llama-cpp-python Âíå DIY ÊèêÁ§∫ÁõÆÂΩï„ÄÇ\n\n\n\nÂú®Á¨¨‰∏ÄÈÉ®ÂàÜ‰∏≠ÔºåÊàë‰ª¨ÂÖ±ÂêåÊé¢ËÆ®‰∫ÜÈòøÈáå‰∫ëÂõ¢ÈòüÂèëÂ∏ÉÁöÑ Qwen2.5 Ê®°ÂûãÁ≥ªÂàóÁöÑÂàõÊñ∞„ÄÇ\n\nÂú®ÁîüÊàêÂºè AI ‰∏≠ÔºåÂü∫ÂáÜÊµãËØïÁé∞Âú®ÊòØ‰∏ªË¶ÅÁöÑ *oracle*Ôºö‰∏Ä‰∏™Êñ∞ÁöÑ LLM ÁöÑÊúâÊïàÊÄßÈúÄË¶ÅÈÄöËøáÂá†‰∏™ËØÑÂà§„ÄÇ‰Ω†ÊâìÁ†¥ÁöÑÂü∫ÂáÜËÆ∞ÂΩïË∂äÂ§öÔºå‰Ω†Â∞±Ë∂ä‰ºòÁßÄ„ÄÇ\n\nËøôÂ∞±ÊòØËµ¢Âæó SOTA Á´ûËµõÁöÑÊñπÂºè„ÄÇ\n\nÂ•ΩÂêßÔºåÊàë‰∏çÂêåÊÑè„ÄÇÂ∞ΩÁÆ°Êàë‰ª¨Âú® AI ËøõÊ≠•‰∏≠ÈúÄË¶ÅÈáåÁ®ãÁ¢ëÂíåÊõ¥Â•ΩÁöÑË°®Áé∞Ôºå‰ΩÜÁî®Êà∑‰ΩìÈ™åÂíå‰∏™‰∫∫ËßÇÁÇπ‰∏çËÉΩË¢´ËßÜ‰∏∫Êó†ÂÖ≥Á¥ßË¶ÅËÄåË¢´ÁΩÆ‰∫é‰∏ÄÊóÅ„ÄÇ\n\nÊàëÁõ∏‰ø°ÔºåÊé¢Á¥¢‰∏Ä‰∫õÂ∏∏Áî®ÁöÑ NLP ‰ªªÂä°ÔºåÂπ∂Â∞ÜËÅäÂ§©‰ΩìÈ™åÊêÅÁΩÆ‰∏ÄÊóÅÔºåÊàë‰ª¨ÂøÖÈ°ªÂÖ≥Ê≥®ÂõûÂ§çÁöÑË¥®Èáè„ÄÇËÄåÊàë‰ª¨ÊòØÂîØ‰∏ÄÈúÄË¶ÅÁöÑÂü∫ÂáÜ„ÄÇÊàë‰ª¨ÁöÑÁî®Êà∑‰ΩìÈ™åÊòØÁêÜËß£‰∏Ä‰∏™Ê®°ÂûãÊòØÂê¶‰ºòÁßÄÁöÑÊúÄ‰Ω≥ÊåáÊ†á„ÄÇËøô‰∏™Ê®°ÂûãÂøÖÈ°ªË∂≥Â§üÂèØÈù†Ôºå‰ª•‰æøÂú®Ëá™Âä®ÂåñÂ∑•‰ΩúÊµÅÁ®ã‰∏≠‰ΩøÁî®„ÄÇ\n\nÈ°∫‰æøÊèê‰∏Ä‰∏ãÔºåÊàëÂ∑≤ÁªèËøêË°å‰∫ÜÊàëÂÜ≥ÂÆöÁß∞‰πã‰∏∫ [RBYF ‚Äî ‰ª•ÊÇ®‰∏∫ÂèçÈ¶àÁöÑ‰øÆËÆ¢Âü∫ÂáÜ](https://open.substack.com/pub/thepoorgpuguy/p/rbyf-is-here-revised-benchmarks-with?r=i78xo&utm_campaign=post&utm_medium=web) ÁöÑÊµãËØïÔºåÂ£∞Áß∞ÊÉä‰∫∫ÁöÑ Llama3.2‚Äì1B-instruct‚Ä¶ ËÄå Qwen2.5‚Äì1.5b Ë¶ÅÂ•ΩÂæóÂ§öÔºÅ\n\nÊâÄ‰ª•Âú®ËøôÁØáÊñáÁ´†‰∏≠ÔºåÊ≠£Â¶ÇÊâøËØ∫ÁöÑÈÇ£Ê†∑ÔºåÊàë‰ª¨Â∞Ü‰∫≤Ëá™È™åËØÅËøô‰∏™Ê®°ÂûãÂú®Êó•Â∏∏‰ΩøÁî®‰∏≠ÁöÑË°®Áé∞Â¶Ç‰Ωï„ÄÇ\n\nÂõûÂà∞Êàë‰ª¨Ëá™Â∑±‚Ä¶‚Ä¶ËÆ©Êàë‰ª¨ÂºÄÂßãÂêßÔºÅ\n\n## ÈúÄÊ±Ç\n\nÂú®ËøôÈáåÔºåÊàë‰ª¨Â∞ÜÊûÑÂª∫‰∏Ä‰∏™ÊúÄÂ∞èÁöÑÊñáÊú¨Êé•Âè£Ôºå‰ª•‰æøËÉΩÂ§üËøêË°åÊ®°Âûã„ÄÅÊµãËØï‰∏çÂêåÁöÑ‰ªªÂä°Âπ∂Á≠âÂæÖÁî®Êà∑ÂèçÈ¶à‰ª•ËøõË°åËØÑ‰º∞„ÄÇ\n\nÈúÄÊ±ÇÂæàÁÆÄÂçïÔºå‰ΩÜÊàëÂª∫ËÆÆÊÇ®ÂàõÂª∫‰∏Ä‰∏™Êñ∞ÁöÑÈ°πÁõÆÁõÆÂΩïÂíå‰∏Ä‰∏™ËôöÊãüÁéØÂ¢É„ÄÇ\n\nÂàõÂª∫‰∏Ä‰∏™ `venv`ÔºàÈúÄË¶Å Python 3.11+ÔºâÔºöÊàëÂú®ËøêË°å Windows 11 ÁöÑËø∑‰Ω†ÁîµËÑë‰∏äËøõË°å‰∫ÜÊµãËØï„ÄÇ\n\n```python\n## create the virtual environment\npython -m venv venv\n## activate the venv\nvenv\\Scripts\\activate\n## Install the dependencies \npip install llama-cpp-python==0.3.0 tiktoken\n```\n\nÊàë‰ª¨ÈúÄË¶Å‰ªé Hugging Face ÁöÑÂÆòÊñπ qwen Â≠òÂÇ®Â∫ì‰∏ãËΩΩ GGUF Êñá‰ª∂ [https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF)ÔºöÊàë‰ΩøÁî®‰∫Ü `qwen2.5-1.5b-instruct-q5_k_m.gguf` ÁâàÊú¨„ÄÇ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*YtQJb_xyq_xcF40yRWPcZA.png)\n\nÊàë‰ª¨ÂáÜÂ§áÂ•Ω‰∫ÜÔºÅ\n\nÊ≥®ÊÑèÔºöÂ¶ÇÊûúÊÇ®ÊÉ≥Ê∑ªÂä†ÂØπ GPU Âä†ÈÄüÂô®ÁöÑ‰∏çÂêåÂêéÁ´ØÊîØÊåÅÔºåÂèØ‰ª•ÊåâÁÖß [Â≠òÂÇ®Â∫ì‰∏≠ÁöÑËØ¥Êòé](https://github.com/abetlen/llama-cpp-python#supported-backends) ËøõË°åÊìç‰Ωú„ÄÇÊàë‰ΩøÁî®‰∫Ü Vulkan ÊîØÊåÅÔºåÂõ†Ê≠§Âú® pip ÂÆâË£Ö‰πãÂâçÊàëÊ∑ªÂä†‰∫ÜÁéØÂ¢ÉÂèòÈáè\n\n```python\n## Vulkan support - for Windows\n$env:CMAKE_ARGS = \"-DGGML_VULKAN=on\"\n```\n\n## ‰ª£Á†Å ‚Äî ‰∏ªÂ∫îÁî®ÂíåÂ∫ì\n\n‰∏∫‰∫Ü‰øùÊåÅ‰ª£Á†ÅÁöÑÁÆÄÊ¥ÅÔºåÊàëÂÜ≥ÂÆö‰ΩøÁî®‰∏Ä‰∏™Â§ñÈÉ®Â∫ìÊù•Êâ©Â±ï‰∏Ä‰∫õÂäüËÉΩ„ÄÇÂ•ΩÂêßÔºåËøôÊòØ‰∏Ä‰∏™Ëá™Â∑±Âä®ÊâãÁöÑÂ∫ìÔºåÊâÄ‰ª•ËøôÈáåÊ≤°ÊúâÁßòÂØÜ„ÄÇ\n\n‰Ω†ÂèØ‰ª•Âú®ÊàëÁöÑÊñáÁ´†‰∏≠ÊâæÂà∞ÊâÄÊúâÁöÑÁªÜËäÇÔºö\n\n‰∏∫‰∫ÜÂä†Âø´ÈÄüÂ∫¶Ôºå‰Ω†ÂèØ‰ª•Áõ¥Êé• [‰ªéËøôÈáå‰∏ãËΩΩÊñá‰ª∂](https://github.com/fabiomatricardi/YouAreTheBenchmark/raw/main/QWEN2.5-1.5B/promptLibv2Qwen.py)ÔºöÂÆÉÂåÖÂê´‰∫ÜÂú®‰∏äËø∞ÊñáÁ´†‰∏≠ËÆ®ËÆ∫ÁöÑ `promptLib` ÁöÑÁâàÊú¨ 2ÔºàÂÆÉË¢´Áß∞‰∏∫ `promptLibv2Qwen.py`ÔºåÂπ∂‰∏îÂØπ `Qwen2.5-1.5B-instruct` Ê®°ÂûãÁöÑÊèêÁ§∫ËøõË°å‰∫ÜÂ∞ëÈáèÂæÆË∞ÉÔºâ„ÄÇ\n\nÂ∞ÜÊñá‰ª∂‰øùÂ≠òÂú®‰∏ªÁõÆÂΩï‰∏≠ÔºåÂπ∂ÂàõÂª∫‰∏Ä‰∏™Âêç‰∏∫ `main.py` ÁöÑÊñ∞Êñá‰ª∂„ÄÇ\n\n```python\n## Chat with an intelligent assistant in your terminal  \n## MODEL: https://huggingface.co/Qwen\n## qwen2.5-1.5b-instruct-q5_k_m.gguf\nimport sys\nfrom time import sleep\nimport warnings\nwarnings.filterwarnings(action='ignore')\nimport datetime\nfrom promptLibv2Qwen import countTokens, writehistory, createCatalog\nfrom promptLibv2Qwen import genRANstring, createStats\nimport argparse\n### PREPARING FINAL DATASET\npd_id = []\npd_task = []\npd_vote = []\npd_remarks = []\n####################Add GPU argument in the parser###################################\nparser = argparse.ArgumentParser()\nparser.add_argument(\"-g\", \"--gpu\", type=int, default=0,nargs='?',\n                    help=\"The number of layers to load on GPU\")\nargs = parser.parse_args()\nif args.gpu == None:\n   ngpu_layers = 0 \nelse:\n    ngpu_layers = args.gpu\nprint(f'Selected GPU: offloading {ngpu_layers} layers...')   \n####################INITIALIZE THE MODEL###################################\nstops = ['\n\n## ÊèêÁ§∫ÁõÆÂΩï ‚Äî Êàë‰ª¨ÊÉ≥Ë¶ÅÊµãËØïÁöÑÂÜÖÂÆπ\n\nÊàëÂú®ËøôÈáåÂÜô‰∫ÜÊàëÁöÑ‰π†ÊÉØ„ÄÇÊàëÊúâ‰∏Ä‰∏™ÊèêÁ§∫ÁõÆÂΩïÔºåÊ∂µÁõñ‰∫ÜËÅäÂ§©Êú∫Âô®‰∫∫‰∏≠‰ΩøÁî®ÁöÑËÆ∏Â§ö‰∏ªË¶ÅËØ≠Ë®Ä‰ªªÂä°ÔºåÂ¶ÇÊëòË¶Å„ÄÅÁÆÄÁü≠ÊëòË¶Å„ÄÅÈöèÊÑèËÅäÂ§©„ÄÅRAG„ÄÅÁúüÂÆûRAGÁ≠â„ÄÇ\n\nËøô‰∏™ÊÉ≥Ê≥ïÊòØËÉΩÂ§üÂú®5ÂàÜÈíüÂÜÖÂä†ËΩΩÊ®°ÂûãÔºåÂπ∂ÂºÄÂßãËØÑ‰º∞ÊØè‰∏™‰ªªÂä°„ÄÇÂú®ÊØèÊ¨°ÁîüÊàêÁªìÊùüÊó∂ÔºåÁî®Êà∑‰ºöË¢´ÊèêÁ§∫ÁªôÂá∫‰∏Ä‰∏™ÂàÜÊï∞Ôºà‰ªé0Âà∞5ÁöÑËØÑÂàÜÔºâÂπ∂Âú®ÈúÄË¶ÅÊó∂Áïô‰∏ã‰ªª‰ΩïËØÑËÆ∫„ÄÇ\n\nËøôÈùûÂ∏∏ÈáçË¶ÅÔºöÂπ∂‰∏çÊòØÊâÄÊúâÊ®°ÂûãÈÉΩÊòØ‰∏ÄÊ†∑ÁöÑÔºåÂØπÊèêÁ§∫‰∏≠ÁöÑÊé™ËæûËøõË°åÂ∞èÁöÑÊàñÂ§ßÁöÑË∞ÉÊï¥ÊÄªÊòØÂøÖË¶ÅÁöÑ„ÄÇ\n\nÊâÄ‰ª•ÂõûÂà∞‰ª£Á†Å‚Ä¶‚Ä¶Âõ†‰∏∫‰πãÂâçÁöÑ‰ª£Á†ÅÂè™ÊòØÁÉ≠Ë∫´ÔºåÁé∞Âú®Â∞ÜÂºÄÂßãÁúüÊ≠£ÁöÑwhileÂæ™ÁéØÔºåÈÅçÂéÜÊï¥‰∏™ÊèêÁ§∫ÁõÆÂΩï„ÄÇËØ∑Áúã‰∏ãÈù¢ÁöÑÂ∑•‰ΩúÊµÅÁ®ã‚Ä¶‚Ä¶\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*EL0Q97Du6HwtcYQZ.png)\n\n‰ª£Á†Å‰∏≠Âè™ÊúâÂ∞ëÈáèÊõ¥ÊîπÔºåÊàë‰ºöÊåáÂá∫Ëøô‰∫õÊõ¥ÊîπÔºåËØ∑ËÄêÂøÉÁ≠âÂæÖ„ÄÇ\n\n```python\n############################# AUTOMATIC PROMPTING EVALUATION  11 TURNS #################################\nid =1\nfor items in tasks:\n    fisrtround = 0\n    task = items[\"task\"]\n    prompt = items[\"prompt\"]\n    test = []\n    print(f'NLP TAKS>>> {task}')\n    print(\"\\033[91;1m\")  #red\n    print(prompt)\n    test.append({\"role\": \"user\", \"content\": prompt})\n    print(\"\\033[92;1m\")\n    full_response = \"\"\n    start = datetime.datetime.now()\n    print(\"üíª > \", end=\"\", flush=True)\n    for chunk in llm.create_chat_completion(\n        messages=test,\n        temperature=0.15,\n        repeat_penalty= 1.31,\n        stop=stops,\n        max_tokens=1500,\n        stream=True,):\n        try:\n            if chunk[\"choices\"][0][\"delta\"][\"content\"]:\n                if fisrtround==0:\n                    print(chunk[\"choices\"][0][\"delta\"][\"content\"], end=\"\", flush=True)\n                    full_response += chunk[\"choices\"][0][\"delta\"][\"content\"]\n                    ttftoken = datetime.datetime.now() - start  \n                    fisrtround = 1\n                else:\n                    print(chunk[\"choices\"][0][\"delta\"][\"content\"], end=\"\", flush=True)\n                    full_response += chunk[\"choices\"][0][\"delta\"][\"content\"]                            \n        except:\n            pass      \n    delta = datetime.datetime.now() - start\n    print('')\n    print(\"\\033[91;1m\")\n    rating = input('Rate from 0 (BAD) to 5 (VERY GOOD) the quality of generation> ')\n    print(\"\\033[92;1m\")\n    stats = createStats(delta,prompt,full_response,rating,logfilename,task,ttftoken)\n    print(stats)\n    writehistory(logfilename,f'''üë®‚Äçüíª > {prompt}\nüíª > {full_response}\n{stats}\n''')\n    pd_id.append(id)\n    pd_task.append(task)\n    pd_vote.append(rating[:2])\n    pd_remarks.append(rating[2:])\n    id += 1\n## create dataframe and save to csv\nzipped = list(zip(pd_id,pd_task,pd_vote,pd_remarks))\nimport pandas as pdd\ndf = pdd.DataFrame(zipped, columns=['#', 'TASK', 'VOTE','REMARKS'])\n#saving the DataFrame as a CSV file \ndf_csv_data = df.to_csv(csvfile, index = False, encoding='utf-8') \nprint('\\nCSV String:\\n', df_csv_data)  \n```\n\n‰∏ªË¶ÅÁöÑÊõ¥Êîπ‰ªÖÂú®ÂâçÂá†Ë°åÔºö\n\n```python\nfor items in tasks:\n    fisrtround = 0\n    task = items[\"task\"]\n    prompt = items[\"prompt\"]\n```\n\nÂ¶ÇÊûú‰Ω†ËØªËøáÂÖ≥‰∫é `promptLib` ÁöÑÊñáÁ´†Ôºå‰Ω†Â∫îËØ•‰∏ç‰ºöÊÑüÂà∞ÊÉäËÆ∂Ôºö‰ΩÜÂ¶ÇÊûú‰Ω†ÊòØÊñ∞ÊâãÔºåËøôÈáåÊàë‰ª¨Ê≠£Âú®ÈÅçÂéÜ‰∏Ä‰∏™Â≠óÂÖ∏ÂàóË°®ÔºåÁªìÊûÑÂ¶Ç‰∏ãÔºö\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*rGcKJWNzSUrcu4wi.png)\n\nÂõ†Ê≠§ÔºåÂØπ‰∫éÁõÆÂΩï‰∏≠ÁöÑÊØè‰∏™È°πÁõÆÔºàË°®Á§∫‰ªªÂä°ÂíåÊèêÁ§∫ÁöÑ‰∏ÄÂØπÔºâÔºåÊàë‰ª¨ÊèêÂèñ‰ªªÂä°ÊèèËø∞Âíå‰ªªÂä°ÁöÑÊèêÁ§∫„ÄÇ\n\n```python\ntest.append({\"role\": \"user\", \"content\": prompt})\n```\n\nÁÑ∂ÂêéÊàë‰ª¨Âú®‰∏Ä‰∏™Âêç‰∏∫ `test` ÁöÑ‰∏¥Êó∂ÂàóË°®‰∏≠ÂàõÂª∫ËÅäÂ§©Ê®°ÊùøÊ∂àÊÅØÔºåÂπ∂Â∞ÜÂÖ∂‰º†ÈÄíÁªô `create_chat_template()` ÊñπÊ≥ï‰ª•ËøõË°åÁîüÊàê„ÄÇ\n\nÂÖ∂‰ªñÈÉ®ÂàÜ‰øùÊåÅ‰∏çÂèò„ÄÇ\n\n‰øùÂ≠òÊñá‰ª∂ÔºåÂπ∂Âú®ÊøÄÊ¥ªÁöÑ `venv` ‰∏ãËøêË°åÔºö\n\n```python\npython main.py\n## Â¶ÇÊûú‰Ω†‰ΩøÁî®ÁöÑÊòØGPU python main.py -g\n```\n\nËøôÂ∞ÜÂæóÂà∞Á±ª‰ºº‰∫é‰∏ãÈù¢Á§∫‰æãÁöÑÂÜÖÂÆπ‚Ä¶‚Ä¶\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MhhQu4lLjtU__Wjf0dSWBg.gif)\n\nËØ∑Ê≥®ÊÑèÔºåÂú®Êï¥‰∏™ÊèêÁ§∫ÁõÆÂΩïÁªìÊùüÊó∂Ôºå‰ºöÂàõÂª∫‰∏Ä‰∏™ *csv* Êñá‰ª∂ÔºåÂÖ∂‰∏≠ÂåÖÂê´ÊâÄÊúâ‰ªªÂä°ÁöÑÊëòË¶ÅÔºÅ\n\n## ÊµãËØïÊ¶ÇËø∞\n\nÊàë‰ΩøÁî®‰∫ÜÂá†‰∏™Â∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÔºå‰ªé [Qwen2‚Äì1.5B-instruct](https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF) Âà∞ [Gemma2‚Äì2B-instruct](https://huggingface.co/bartowski/gemma-2-2b-it-GGUF)ÔºåÂÜçÂà∞ [Llama3.2‚Äì1B-instruct](https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF)ÔºåÊúÄÂêéÊòØÊñ∞ÁöÑ [Qwen2.5‚Äì1.5B-instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF)„ÄÇ\n\nËôΩÁÑ∂ÊàëÂØπ [Llama3.2‚Äì1B-instruct](https://generativeai.pub/llama3-2-1b-instruct-is-ok-but-not-good-enough-28f88046b63e) ÊÑüÂà∞Áõ∏ÂΩìÂ§±ÊúõÔºå‰ΩÜÊàëÂØπÊñ∞ÁöÑ [Qwen2.5‚Äì1.5B-instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF) ÁöÑÂá∫Ëâ≤Ë°®Áé∞ÊÑüÂà∞ÊÉäËÆ∂„ÄÇ\n\nÂú®ÊØèÊ¨°ÁîüÊàêÁªìÊùüÊó∂ÔºåÁî®Êà∑‰ºöË¢´Ë¶ÅÊ±ÇÁî®0Âà∞5ÁöÑÂàÜÊï∞Êù•ËØÑ‰º∞ÁªìÊûú„ÄÇ**Âú®ËøôÁßçÊÉÖÂÜµ‰∏ãÔºåÁî®Êà∑Â∞±ÊòØÊàë‚Ä¶‚Ä¶**\n\nËøôÁßçÂÆöÊÄßÂàÜÊûêÁ°ÆÂÆûËæÉ‰∏∫ÁÆÄÂçïÔºåÂõ†Ê≠§ÊØè‰∏™ÂàÜÊï∞ÈÉΩÊúâÊèèËø∞ÔºåÁî®Êà∑ÂèØ‰ª•Ê∑ªÂä†ËØÑËÆ∫Ôºà‚Äú‰∏Ä‰∫õÈîôËØØ‰ø°ÊÅØ‚ÄùÔºå‚ÄúÂèØËÉΩÊõ¥Â•ΩÂú∞Êõ¥ÊîπÊèêÁ§∫‰∏≠ÁöÑÊé™Ëæû‚ÄùÔºâ\n\nËøôÈáåÊòØÂ∏¶ÊúâÊèèËø∞ÁöÑÂÆöÊÄßÁü©Èòµ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*eBdPfZtfr99MsvLh6tt42w.png)\n\n## Â•Ω‰∏éÂùè ‚Äî ÁªÜËäÇ\n\nÊÄªÁªìÊïàÊûúÈùûÂ∏∏Â•Ω„ÄÇÂàóÂá∫ÈïøÊñáÊú¨ÁöÑ‰∏ªË¶Å‰∏ªÈ¢ò‰πüÈùûÂ∏∏‰∏çÈîô„ÄÇ\n\nRAG ‰ªªÂä°ÁöÑÈÄüÂ∫¶Áõ∏ÂΩìÂø´ÔºàÂç≥‰ΩøÂú®ÊàëÁöÑËø∑‰Ω†ÁîµËÑë‰∏äÔºâÔºåÁúüÂÆûÁöÑ RAGÔºàÂú®‰∏ä‰∏ãÊñáÂ§ñÊèêÈóÆÔºâ‰πüÂæàÂà∞‰Ωç„ÄÇ\n\nÊâÄ‰ª•ÁúüÁöÑÂæàÂ•Ω„ÄÇ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*DuV3LJep_PuDqiCcAMb6Cg.png)\n\nÂΩìÁÑ∂‰πüÊúâ‰∏Ä‰∫õ‰∏çË∂≥‰πãÂ§ÑÔºöÂç≥‰ΩøÊ∏©Â∫¶‰ªÖ‰∏∫ `0.15`ÔºåÂú®ÊÄªÁªì‰∏§Âè•ËØùÁöÑ‰ªªÂä°‰∏≠ÔºåÊàë‰πüÂæóÂà∞‰∫Ü‰∫õËôöÂÅáÁöÑ‰ø°ÊÅØ„ÄÇËøô‰∏çÂ•Ω„ÄÇ\n\n> ÊàëÊúüÂæÖËøôÂèØ‰ª•ÈÄöËøáÁ®çÂæÆË∞ÉÊï¥ÊèêÁ§∫ÔºåÊàñÂ∞ÜÊ∏©Â∫¶ËÆæÁΩÆ‰∏∫ `0` Êù•‰øÆÂ§ç„ÄÇ\n\nÂè¶‰∏Ä‰∏™‰∫ãÂÆûÊòØÔºåÂàõÊÑèÂÜô‰ΩúÁöÑÊïàÊûúÁõ∏ÂΩìÂ∑ÆÔºöÂú®ÊµãËØï‰∏≠Êàë‰ΩøÁî®‰∫Ü‰∏Ä‰∫õÂõ∫ÂÆöÁöÑÁîüÊàêÂèÇÊï∞„ÄÇ\n\n```python\n        temperature=0.15,\n        repeat_penalty= 1.31,\n```\n\nÂèØËÉΩÂØπ‰∫éÂàõÊÑèÂÜô‰ΩúÔºå‰ΩøÁî® Qwen2.5‚Äì1.5B-instruct Êó∂ÔºåÊàë‰ª¨Â∫îËØ•‰ΩøÁî®Êõ¥È´òÁöÑ `repeat_penalty` ÂíåÊõ¥È´òÁöÑ `temperature`„ÄÇ\n\nÈ°∫‰æøÊèê‰∏Ä‰∏ãÔºåÊàëÂøÖÈ°ªËØ¥ÂèçÊÄùÊèêÁ§∫ÁöÑÊïàÊûú‰πüÂπ∂‰∏çÂ∑ÆÔºÅÊ†áÁ≠æÁöÑÂºÄÈó≠Ê≤°Êúâ‰øùÊåÅÔºàÊâÄ‰ª•‰∏çÂÆπÊòìÂ∞ÜÂÖ∂ÊîæÂÖ•ÁÆ°ÈÅìÊàñÂ∑•‰ΩúÊµÅ‰∏≠ÔºâÔºå‰ΩÜÊï¥‰ΩìÁîüÊàêÁöÑÊµÅÁ®ãÂíå‚ÄúÊÄùÁª¥Èìæ‚ÄùÊé®ÁêÜËøáÁ®ãÁõ∏ÂΩì‰∏çÈîô„ÄÇ\n\n```python\n<thinking>\n‰ΩøÁî®ÁîüÊàêÂûã‰∫∫Â∑•Êô∫ËÉΩÔºàAIÔºâÂú®ÊïôÂ≠¶‰∏≠ÁöÑÈáçË¶ÅÊÄß‰∏çÂÆπÂ∞èËßëÔºåÂõ†‰∏∫ËøôÈ°πÊäÄÊúØ‰∏ç‰ªÖÂú®ÂΩìÂâçÁöÑÊïôËÇ≤ÂÆûË∑µ‰∏≠ÂÖ∑ÊúâÂ∑®Â§ßÁöÑÊΩúÂäõÔºåËøòËÉΩÂ°ëÈÄ†Êõ¥ÂÖ∑ÂàõÊñ∞ÊÄßÁöÑÁªàË∫´Â≠¶‰π†ÊñπÊ≥ï„ÄÇ\n</thinking>\n\n**ÊÄùÁª¥ÈìæÔºö**\n1. **ÁêÜËß£ÂΩ±Âìç**ÔºöAI ÂèØ‰ª•Ëá™Âä®ÂåñÈáçÂ§çÊÄß‰ªªÂä°ÔºåÂπ∂Ê†πÊçÆÂ≠¶ÁîüÁöÑË°®Áé∞Êï∞ÊçÆÊèê‰æõ‰∏™ÊÄßÂåñÂèçÈ¶àÔºà‰æãÂ¶ÇÔºåÈÄöËøáËÅäÂ§©Êú∫Âô®‰∫∫ÊàñËá™ÈÄÇÂ∫îËØÑ‰º∞Ôºâ„ÄÇ\n2. **Â¢ûÂº∫Â≠¶‰π†‰ΩìÈ™å**ÔºöÈÄöËøáÊï¥ÂêàÁîüÊàêÂûã AIÔºåÊïôÂ∏àÂèØ‰ª•ÂàõÂª∫Êõ¥ÂÖ∑Âê∏ÂºïÂäõÁöÑËØæÁ®ãÔºå‰ª•Êª°Ë∂≥‰∏çÂêåÁöÑÂ≠¶‰π†È£éÊ†º„ÄÇ\n3. **ÊîπÂñÑÂ≠¶ÁîüÊàêÊûú**Ôºö‰∏™ÊÄßÂåñÊåáÂØºÂèØ‰ª•ÊèêÈ´òÂ≠¶ÁîüÂØπÁü•ËØÜÁöÑÁêÜËß£ÂíåËÆ∞ÂøÜ„ÄÇ\n\n**ÂèçÊÄùÔºö**\n- Âú®ÊïôËÇ≤‰∏≠Êï¥ÂêàÁîüÊàêÊäÄÊúØËá≥ÂÖ≥ÈáçË¶ÅÔºåÂõ†‰∏∫ÂÆÉ‰∏éÊó•ÁõäÊàê‰∏∫Êï∞Â≠óÂéü‰ΩèÊ∞ëÁöÑÂ≠¶‰π†ËÄÖ‰∏çÊñ≠ÂèòÂåñÁöÑÈúÄÊ±ÇÁõ∏‰∏ÄËá¥ÔºåË¶ÅÊ±ÇÈááÁî®ÂåÖÂê´Ëøô‰∫õÊäÄÊúØÁöÑÊïôÂ≠¶ÊñπÊ≥ï„ÄÇ\n</thinking>\n\n<output>\nÊïôÂ∏àÂ∫îÂ≠¶‰π†Â¶Ç‰ΩïÂà©Áî®ÁîüÊàêÂûã AI Êù•Â¢ûÂº∫‰ªñ‰ª¨ÁöÑÊïôÂ≠¶ÊñπÊ≥ïÂπ∂ÊîπÂñÑÂ≠¶ÁîüÊàêÊûú„ÄÇËøôÂåÖÊã¨‰∫ÜËß£ÂÖ∂Âú®Ëá™Âä®Âåñ‰ªªÂä°ÔºàÂ¶ÇËØÑÂàÜÊàñÂàõÂª∫‰∏™ÊÄßÂåñÂ≠¶‰π†Ë∑ØÂæÑÔºâÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇ\n\n**Â≠¶‰π†Ë∑ØÂæÑÔºö**\n1. **ÊïôËÇ≤ÊäÄÊúØÂü∫Á°ÄËØæÁ®ã**Ôºö‰ªéÊèê‰æõÊïôËÇ≤ÊäÄÊúØÊ¶ÇËø∞ÁöÑËØæÁ®ãÂºÄÂßãÔºåÂåÖÊã¨Êï∞Â≠óÂ∑•ÂÖ∑ÁöÑÂü∫Á°ÄÁü•ËØÜ„ÄÇ\n2. **AI Âü∫Á°ÄËØæÁ®ãÔºàCourseraÔºâ** - ÈÄöËøá Coursera ÁöÑ‰∫∫Â∑•Êô∫ËÉΩÊïôËÇ≤ËÄÖÂÖçË¥πÂú®Á∫øËØæÁ®ãÔºå‰∫ÜËß£ AI Ê¶ÇÂøµÂèäÂÖ∂Âú®ÊïôËÇ≤‰∏≠ÁöÑÂ∫îÁî®„ÄÇ\n\n3. **ÁîüÊàêÂ≠¶‰π†ÁöÑ‰∏ì‰∏öËØæÁ®ã**ÔºöÊä•ÂêçÂèÇÂä†‰∏ìÊ≥®‰∫éÁîüÊàêÂ≠¶‰π†ÁöÑËØæÁ®ãÔºå‰æãÂ¶Ç‚ÄúÁîüÊàêËÆæËÆ°‚ÄùÊàñÁ±ª‰ººÁöÑÁî± MIT Á≠âÂ§ßÂ≠¶Êèê‰æõÁöÑ‰∏ì‰∏öÈ°πÁõÆ„ÄÇ\n4. **‰∏ì‰∏öÂèëÂ±ïÁ†îËÆ®‰ºöÂíå‰ºöËÆÆ** - ÂèÇÂä†Âú∞ÊñπÊïôËÇ≤‰ºöËÆÆ‰∏ä‰∏ìÊ≥®‰∫éÂ∞Ü AI Êï¥ÂêàÂà∞ÊïôÂ≠¶ÂÆûË∑µ‰∏≠ÁöÑÁ†îËÆ®‰ºö„ÄÇ\n\n5. **ÊïôËÇ≤Â∑•‰ΩúËÄÖÂú®Á∫øÁ§æÂå∫ÔºàÂ¶Ç EdmodoÔºâ**ÔºöÂä†ÂÖ•Âú®Á∫øÁ§æÂå∫ÔºåÊïôËÇ≤Â∑•‰ΩúËÄÖËÆ®ËÆ∫ÁîüÊàêÊäÄÊúØÂú®ÊïôËÇ≤‰∏≠ÁöÑÂ∫îÁî®ÔºåÂàÜ‰∫´ËµÑÊ∫êÊàñËØ¢ÈóÆÂÆûÊñΩÈóÆÈ¢ò„ÄÇ\n6. **ËÆ§ËØÅÈ°πÁõÆ**ÔºöËÄÉËôëËé∑ÂæóËÆ§ËØÅÔºå‰ª•ËÆ§ÂèØÊÇ®Âú®ÊïôÂ≠¶ÂÆûË∑µ‰∏≠‰∏é AI Êï¥ÂêàÁõ∏ÂÖ≥ÁöÑÁü•ËØÜÂíåÊäÄËÉΩÔºå‰æãÂ¶Ç Google ÁöÑ‚ÄúÊïôËÇ≤ËÄÖ AI‚ÄùÈ°πÁõÆ„ÄÇ\n\nÈÄöËøáÈÅµÂæ™ËøôÊù°Â≠¶‰π†Ë∑ØÂæÑÔºåÊïôÂ∏à‰∏ç‰ªÖÂèØ‰ª•ÊèêÂçáËá™Ë∫´ÁöÑ‰∏ì‰∏öÂèëÂ±ïÔºåËøòÂèØ‰ª•ÈÄöËøáÊúâÊïàÊï¥ÂêàÁîüÊàêÊäÄÊúØ‰∏∫ÊïôËÇ≤ÁöÑÊú™Êù•ÂÅöÂá∫ÁßØÊûÅË¥°ÁåÆ„ÄÇ\n</output>\n```\n\nÊàëËÆ§‰∏∫ÂØπ‰∫éËøô‰∏™Â∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑËßÑÊ®°Êù•ËØ¥ÔºåÁªìÊûúÂπ∂‰∏çÂ∑ÆÔºÅ\n\n## ÊØîËæÉ Qwen2.5 Âíå Llama3.2\n\nÂ∞ΩÁÆ°ËøôÂè™ÊòØÊàë‰∏™‰∫∫ÁöÑËØÑ‰º∞ÔºåÊàëÊÉ≥‰∏éÊÇ®ÂàÜ‰∫´„ÄÇ\n\nËøô‰∏§‰∏™Ê®°ÂûãÈÉΩÊòØ‰∏∫ÁßªÂä®ËÆæÂ§áËÆæËÆ°ÁöÑÔºå‰ΩÜÊÄßËÉΩÂ∑ÆÂºÇÂæàÂ§ß„ÄÇËØ∑Áúã‰∏ãÈù¢Ôºö\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*T6vLgvOKdkotlV1K5x6-QQ.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*DuV3LJep_PuDqiCcAMb6Cg.png)\n\nÈ¶ñÂÖàÔºåÊÄª‰ΩìËØÑÂàÜÂ∑ÆÂºÇÂ∑®Â§ßÔºàLlama3.2 ‰∏∫ 41ÔºåQwen2.5 ‰∏∫ 57Ôºâ„ÄÇ\n\nÂÖ∂Ê¨°ÔºåÂ¶ÇÊûúËÄÉËôëÊÇ®Âú®ÁßªÂä®ËÆæÂ§á‰∏äÂèØËÉΩÊèêÂá∫ÁöÑËØ≠Ë®Ä‰ªªÂä°Ôºå‰∏ªË¶ÅÊòØÂ∏åÊúõÊúâÊµÅÁïÖÁöÑËÅäÂ§©‰ΩìÈ™åÔºà‰ªªÂä° 4Ôºâ„ÄÅËâØÂ•ΩÁöÑÊëòË¶ÅËÉΩÂäõÔºà‰ªªÂä° 5 Âà∞ 7ÔºâÂíå‰∏Ä‰∫õÂàõÈÄ†ÊÄßÂÜô‰ΩúÔºà‰ªªÂä° 11 Âíå 13Ôºâ„ÄÇ\n\nÂú®ÈÄüÂ∫¶ÊñπÈù¢Ôºå‰ªÖÂú® CPU ‰∏äËøêË°åÊ®°ÂûãÔºå‰ΩøÁî®ÈùûÂ∏∏ÊúâÈôêÁöÑËø∑‰Ω† PCÔºå**ÊàëËé∑Âæó‰∫ÜÂπ≥ÂùáÊé®ÁêÜÈÄüÂ∫¶‰∏∫ 14 t/s„ÄÇ**\n\n## ÁªìËÆ∫\n\nÂú®Qwen2ÂèëÂ∏ÉÁöÑËøáÂéª‰∏â‰∏™ÊúàÈáåÔºå‰ºóÂ§öÂºÄÂèëËÄÖÂú®Qwen2ËØ≠Ë®ÄÊ®°Âûã‰∏äÊûÑÂª∫‰∫ÜÊñ∞Ê®°ÂûãÔºå‰∏∫Êï¥‰∏™Á§æÂå∫‰ª•ÂèäÈòøÈáå‰∫ëÊèê‰æõ‰∫ÜÂÆùË¥µÁöÑÂèçÈ¶à„ÄÇ\n\n> Âú®Ê≠§ÊúüÈó¥ÔºåÊàë‰ª¨‰∏ìÊ≥®‰∫éÂàõÂª∫Êõ¥Êô∫ËÉΩ„ÄÅÊõ¥Áü•ËØÜ‰∏∞ÂØåÁöÑËØ≠Ë®ÄÊ®°Âûã„ÄÇ‰ªäÂ§©ÔºåÊàë‰ª¨ÂæàÈ´òÂÖ¥Âú∞‰ªãÁªçQwenÂÆ∂ÊóèÁöÑÊúÄÊñ∞ÊàêÂëòÔºöQwen2.5\n\n‰ªñ‰ª¨ÁöÑÂ£∞Êòé‰º¥ÈöèÁùÄÂÖ≥‰∫éÊñ∞Ê®°ÂûãÂÆ∂ÊóèÁöÑ‰∫ãÂÆûÔºö\n\n* ÂØÜÈõÜÂûã„ÄÅ**Êòì‰∫é‰ΩøÁî®**ÁöÑ‰ªÖËß£Á†ÅËØ≠Ë®ÄÊ®°ÂûãÔºåÊèê‰æõ0.5B„ÄÅ1.5B„ÄÅ3B„ÄÅ7B„ÄÅ14B„ÄÅ32BÂíå72BÁöÑ‰∏çÂêåÂ∞∫ÂØ∏Ôºå‰ª•ÂèäÂü∫Á°ÄÂíåÊåá‰ª§Âèò‰Ωì„ÄÇ\n* Âú®Êàë‰ª¨ÊúÄÊñ∞ÁöÑÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜ‰∏äËøõË°åÈ¢ÑËÆ≠ÁªÉÔºåÊ∂µÁõñÂ§öËææ18TÁöÑÊ†áËÆ∞„ÄÇ\n* Âú®**Êåá‰ª§ÈÅµÂæ™**ÊñπÈù¢ÊúâÊòæËëóÊîπÂñÑ„ÄÇ\n* ÂØπÁ≥ªÁªüÊèêÁ§∫ÁöÑÂ§öÊ†∑ÊÄß**Êõ¥ÂÖ∑ÈüßÊÄß**ÔºåÂ¢ûÂº∫‰∫ÜËßíËâ≤ÊâÆÊºîÁöÑÂÆûÊñΩÂíåËÅäÂ§©Êú∫Âô®‰∫∫ÁöÑÊù°‰ª∂ËÆæÁΩÆ„ÄÇ\n* **ÊîØÊåÅÈ´òËææ128K**ÁöÑ‰∏ä‰∏ãÊñáÈïøÂ∫¶ÔºåÂπ∂ËÉΩÁîüÊàêÊúÄÂ§ö8KÁöÑÊ†áËÆ∞„ÄÇ\n* ÊîØÊåÅ29Áßç‰ª•‰∏äÁöÑÂ§öËØ≠Ë®Ä„ÄÇ\n\nÂú®ÊàëÂπøÊ≥õÔºà‰ΩÜÁ°ÆÂÆû‰ªÖÈôê‰∫é‰∏ÄÊ¨°ÊÄßÊèêÁ§∫ÂíåÂ∞ëÈáèNLP‰ªªÂä°ÔºâÁöÑÊµãËØï‰∏≠ÔºåÊàë‰∫≤ÁúºÁúãÂà∞Ëøô‰∫õÂ£∞ÊòéÊòØÂü∫‰∫éÈ´òË¥®ÈáèÁöÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜÂíåÁ≤æÂøÉË∞ÉÊï¥ÁöÑÁªìÊûú„ÄÇ\n\nËØ•Ê®°ÂûãÂú®ÁßªÂä®ËÆæÂ§á‰∏äÁöÑË°®Áé∞ÊûÅ‰∏∫Âá∫Ëâ≤ÔºÅ\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/rag-llm-and-pdf-conversion-to-markdown-text-with-pymupdf-03af00259b5d","frontmatter":{"title":"RAG/LLM Âíå PDFÔºö‰ΩøÁî® PyMuPDF ËΩ¨Êç¢‰∏∫ Markdown ÊñáÊú¨","meta_title":"RAG/LLM Âíå PDFÔºö‰ΩøÁî® PyMuPDF ËΩ¨Êç¢‰∏∫ Markdown ÊñáÊú¨","description":"ÈááÁî® markdown ÊñáÊú¨Ê†ºÂºèËæìÂÖ•Êï∞ÊçÆÂèØÊèêÈ´òÁîüÊàêÁöÑÊñáÊú¨Ë¥®Èáè","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*swPjVuudAhsoRiiw3Ee32w.png","categories":["Programming","Technology","Technology/Web"],"author":"Rifx.Online","tags":["markdown","PyMuPDF","LLM","RAG","PDF"],"draft":false,"slug":"blog/rag-llm-and-pdf-conversion-to-markdown-text-with-pymupdf-03af00259b5d"},"content":"\n\n\n### ‰ª•MarkdownÊñáÊú¨Ê†ºÂºèËæìÂÖ•Êï∞ÊçÆÂèØ‰ª•ÊèêÈ´òÁîüÊàêÊñáÊú¨ÁöÑË¥®Èáè\n\n\n\n## ‰ªãÁªç\n\nÂú®**Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ**Âíå**Ê£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRAGÔºâ**ÁéØÂ¢É‰∏≠Ôºå‰ª•**markdownÊñáÊú¨Ê†ºÂºè**ËæìÂÖ•Êï∞ÊçÆÂÖ∑Êúâ**ÈáçË¶ÅÊÑè‰πâ**„ÄÇ‰ª•‰∏ãÊòØ‰∏Ä‰∫õËØ¶ÁªÜËÄÉËôëÂõ†Á¥†„ÄÇ\n\n**LLMs** ÊòØÂº∫Â§ßÁöÑËØ≠Ë®ÄÊ®°ÂûãÔºåÂèØ‰ª•ÁîüÊàêËøûË¥Ø‰∏îÂÖ∑Êúâ‰∏ä‰∏ãÊñáÁõ∏ÂÖ≥ÊÄßÁöÑÊñáÊú¨„ÄÇÁÑ∂ËÄåÔºåÂÆÉ‰ª¨ÊúâÊó∂ÂèØËÉΩ‰ºö‰∫ßÁîüÁº∫‰πè‰∫ãÂÆûÂáÜÁ°ÆÊÄßÊàñ‰∏ä‰∏ãÊñáÁöÑÂìçÂ∫î„ÄÇÈÄöËøáÁªìÂêàÂü∫‰∫éÊ£ÄÁ¥¢ÁöÑÊñπÊ≥ïÔºàÂ¶ÇRAGÔºâÔºåÊàë‰ª¨ÂèØ‰ª•ÊèêÈ´òÁîüÊàêÊñáÊú¨ÁöÑË¥®Èáè„ÄÇ\n\n**RAG** ‰ΩøÂæóÂ∞Ü**Â§ñÈÉ®Êï∞ÊçÆ**‚Äî‚ÄîÂú®LLMÁöÑËÆ≠ÁªÉÊï∞ÊçÆ‰∏≠‰πãÂâçÁº∫Â§±ÁöÑÊï∞ÊçÆ‚Äî‚ÄîÊï¥ÂêàÂà∞ÊñáÊú¨ÁîüÊàêËøáÁ®ã‰∏≠Êàê‰∏∫ÂèØËÉΩ„ÄÇËøôÁßçÂåÖÂê´ÂáèÂ∞ë‰∫Ü‚ÄúÂπªËßâÈóÆÈ¢ò‚ÄùÔºåÂπ∂Â¢ûÂº∫‰∫ÜÊñáÊú¨ÂìçÂ∫îÁöÑÁõ∏ÂÖ≥ÊÄß„ÄÇ\n\n## ‰∏∫‰ªÄ‰πàÈÄâÊã© Markdown Áî®‰∫é LLMÔºü\n\n**Markdown** ÊòØ‰∏ÄÁßçËΩªÈáèÁ∫ßÊ†áËÆ∞ËØ≠Ë®ÄÔºåÂÖÅËÆ∏Áî®Êà∑‰ΩøÁî®ÁÆÄÂçïÁöÑËØ≠Ê≥ïÊ†ºÂºèÂåñÁ∫ØÊñáÊú¨„ÄÇÂÆÉÂπøÊ≥õÁî®‰∫éÂàõÂª∫ÁªìÊûÑÂåñÊñáÊ°£ÔºåÁâπÂà´ÊòØÂú® GitHub„ÄÅJupyter Á¨îËÆ∞Êú¨ÂíåÂêÑÁßçÂÜÖÂÆπÁÆ°ÁêÜÁ≥ªÁªü‰∏ä„ÄÇÂΩìÂ∞ÜÊï∞ÊçÆËæìÂÖ•Âà∞ LLM Êàñ RAG Á≥ªÁªüÊó∂Ôºå‰ΩøÁî® Markdown Ê†ºÂºèÊèê‰æõ‰∫ÜÂá†‰∏™Â•ΩÂ§ÑÔºö\n\n1. **ÁªìÊûÑÂåñÂÜÖÂÆπ**ÔºöMarkdown ÂÖÅËÆ∏ÊÇ®Â∞Ü‰ø°ÊÅØÁªÑÁªáÊàêÊ†áÈ¢ò„ÄÅÂàóË°®„ÄÅË°®Ê†ºÂíåÂÖ∂‰ªñÁªìÊûÑÂåñÂÖÉÁ¥†„ÄÇËøôÁßçÁªìÊûÑÊúâÂä©‰∫éÊõ¥Â•ΩÂú∞ÁêÜËß£Âíå‰∏ä‰∏ãÊñá‰øùÁïô„ÄÇ\n2. **ÂØåÊñáÊú¨**ÔºöMarkdown ÊîØÊåÅÂü∫Êú¨Ê†ºÂºèÔºåÂ¶ÇÁ≤ó‰Ωì„ÄÅÊñú‰Ωì„ÄÅÈìæÊé•Âíå‰ª£Á†ÅÂùó„ÄÇÂú®ËæìÂÖ•Êï∞ÊçÆ‰∏≠ÂåÖÂê´ÂØåÊñáÊú¨ÂèØ‰ª•Â¢ûÂº∫ËØ≠Ë®ÄÊ®°ÂûãÁöÑ‰∏ä‰∏ãÊñá„ÄÇ\n3. **ÂµåÂÖ•ÈìæÊé•ÂíåÂºïÁî®**ÔºöMarkdown ÂÖÅËÆ∏ÊÇ®ÂµåÂÖ•Ë∂ÖÈìæÊé•„ÄÅËÑöÊ≥®ÂíåÂºïÁî®„ÄÇÂú® RAG Âú∫ÊôØ‰∏≠ÔºåËøôÂØπ‰∫éÂºïÁî®Â§ñÈÉ®Êù•Ê∫êÊàñÊèê‰æõÈ¢ùÂ§ñ‰∏ä‰∏ãÊñáËá≥ÂÖ≥ÈáçË¶Å„ÄÇ\n4. **Êòì‰∫éÂàõ‰Ωú**ÔºöMarkdown ÂÖ∑ÊúâÂèØËØªÊÄßÔºåÊòì‰∫éÁºñÂÜô„ÄÇ‰ΩúËÄÖÂèØ‰ª•È´òÊïàÂú∞ÂàõÂª∫ÂÜÖÂÆπÔºåËÄåÊó†ÈúÄÂ§çÊùÇÁöÑÊ†ºÂºèÂåñÂ∑•ÂÖ∑„ÄÇ\n5. **ÂàÜÂùó**ÔºöÂØπ‰∫é RAG Á≥ªÁªüËá≥ÂÖ≥ÈáçË¶ÅÔºåÂàÜÂùóÔºà‰πüÁß∞‰∏∫‚ÄúÊãÜÂàÜ‚ÄùÔºâÂ∞ÜÂ§ßÈáèÊñáÊ°£ÊãÜÂàÜ‰∏∫Êõ¥ÊòìÂ§ÑÁêÜÁöÑÈÉ®ÂàÜ„ÄÇÈÄöËøáÊîØÊåÅ MD Ê†ºÂºèÁöÑ PyMuPDF Êï∞ÊçÆÊèêÂèñÔºåÊàë‰ª¨ÊîØÊåÅÂàÜÂùó‰ª•‰øùÊåÅÂÖ∑ÊúâÂÖ±Âêå‰∏ä‰∏ãÊñáÁöÑÊñáÊú¨Âú®‰∏ÄËµ∑„ÄÇ**ÈáçË¶ÅÁöÑÊòØÔºåMD Ê†ºÂºèÁöÑ PyMuPDF ÊèêÂèñÂÖÅËÆ∏ËøõË°å [Á¨¨ 3 Á∫ßÂàÜÂùó](https://readmedium.com/five-levels-of-chunking-strategies-in-rag-notes-from-gregs-video-7b735895694d#b123)**„ÄÇ\n\nÊÄª‰πãÔºåÂú® LLM Âíå RAG ÁéØÂ¢É‰∏≠‰ΩøÁî® Markdown ÊñáÊú¨Ê†ºÂºèÂèØ‰ª•Á°Æ‰øùÊõ¥ÂáÜÁ°ÆÂíåÁõ∏ÂÖ≥ÁöÑÁªìÊûúÔºåÂõ†‰∏∫ÂÆÉÊèê‰æõ‰∫ÜÊõ¥‰∏∞ÂØåÁöÑÊï∞ÊçÆÁªìÊûÑÂíåÊõ¥Áõ∏ÂÖ≥ÁöÑÊï∞ÊçÆÂùóË¥üËΩΩÁªôÊÇ®ÁöÑ LLM„ÄÇ\n\n## PyMuPDF ÊîØÊåÅ PDF ÁöÑ Markdown ËΩ¨Êç¢\n\nËá™Êé®Âá∫‰ª•Êù•ÔºåPyMuPDF ‰∏ÄÁõ¥ËÉΩÂ§ü‰ªé PDF È°µÈù¢‰∏≠ÊèêÂèñÊñáÊú¨„ÄÅÂõæÂÉè„ÄÅÁü¢ÈáèÂõæÂΩ¢ÔºåÂπ∂‰∏î‰ªé 2023 Âπ¥ 8 ÊúàËµ∑ÔºåËøòËÉΩÂ§üÊèêÂèñË°®Ê†º„ÄÇËøô‰∫õÂØπË±°Á±ªÂûãÂêÑËá™ÊúâÂÖ∂ÊèêÂèñÊñπÊ≥ïÔºöÊñáÊú¨Êúâ‰∏ÄÁßçÔºåË°®Ê†º„ÄÅÂõæÂÉèÂíåÁü¢ÈáèÂõæÂΩ¢ÂàôÊúâÂÖ∂‰ªñÊñπÊ≥ï„ÄÇ‰∏∫‰∫ÜÊª°Ë∂≥ RAG ÁöÑË¶ÅÊ±ÇÔºåÊàë‰ª¨Â∞ÜËøô‰∫õ‰∏çÂêåÁöÑÊèêÂèñÊñπÂºèÂêàÂπ∂ÔºåÁîüÊàê‰∏Ä‰∏™Áªü‰∏ÄÁöÑ **Markdown** Â≠óÁ¨¶‰∏≤Ôºå‰ª•‰∏ÄËá¥Âú∞Ë°®Á§∫È°µÈù¢ÁöÑÊï¥‰ΩìÂÜÖÂÆπ„ÄÇ\n\nÊâÄÊúâËøô‰∫õÈÉΩÂÆûÁé∞‰∏∫ [‰∏Ä‰∏™ Python ËÑöÊú¨](https://github.com/pymupdf/RAG/blob/main/helpers/pymupdf_rag.py)„ÄÇÂÆÉÂèØ‰ª•Ë¢´ÂÖ∂‰ªñËÑöÊú¨‰Ωú‰∏∫Ê®°ÂùóÂØºÂÖ•ÔºåÊàñËÄÖÂú®ÁªàÁ´ØÁ™óÂè£‰∏≠ÈÄöËøá‰ª•‰∏ãÂëΩ‰ª§Ë°åË∞ÉÁî®Ôºö\n\n`$ python pymupdf_rag.py input.pdf [-pages PAGES]`\n\nÂÆÉÂ∞ÜÁîüÊàê‰∏Ä‰∏™ **Markdown** Ê†ºÂºèÁöÑÊñáÊú¨Êñá‰ª∂ÔºàÁß∞‰∏∫ `input.md`Ôºâ„ÄÇÂèØÈÄâÂèÇÊï∞ `PAGES` ÂÖÅËÆ∏Â∞ÜËΩ¨Êç¢ÈôêÂà∂‰∏∫ PDF ÊÄªÈ°µÈù¢ÁöÑ‰∏Ä‰∏™Â≠êÈõÜ„ÄÇÂ¶ÇÊûúÁúÅÁï•ÔºåÂàôÂ§ÑÁêÜÊï¥‰∏™ PDF„ÄÇ\n\n## Markdown ÂàõÂª∫ÁªÜËäÇ\n\n### ÈÄâÊã©Ë¶ÅËÄÉËôëÁöÑÈ°µÈù¢\n\n‚Äú`-pages`‚Äù ÂèÇÊï∞ÊòØ‰∏Ä‰∏™Â≠óÁ¨¶‰∏≤ÔºåÁî±ÊâÄÈúÄÁöÑÈ°µÈù¢ÁºñÂè∑Ôºà‰ªé1ÂºÄÂßãÔºâÁªÑÊàêÔºåÁî®‰∫éËÄÉËôëËøõË°åmarkdownËΩ¨Êç¢„ÄÇÂèØ‰ª•ÁªôÂá∫Â§ö‰∏™È°µÈù¢ÁºñÂè∑ËßÑËåÉÔºå‰ΩøÁî®ÈÄóÂè∑ÂàÜÈöî„ÄÇÊØè‰∏™ËßÑËåÉÂèØ‰ª•ÊòØ‰∏Ä‰∏™Êï¥Êï∞Êàñ‰∏§‰∏™Áî®‚Äú`-`‚ÄùËøûÊé•ÁöÑÊï¥Êï∞ÔºåÊåáÂÆö‰∏Ä‰∏™È°µÈù¢ËåÉÂõ¥„ÄÇ‰ª•‰∏ãÊòØ‰∏Ä‰∏™Á§∫‰æãÔºö\n\n‚Äú`-pages 1‚Äì10,15,20-N`‚Äù\n\nËøôÂ∞ÜÂåÖÊã¨Á¨¨1È°µÂà∞Á¨¨10È°µ„ÄÅÁ¨¨15È°µ‰ª•ÂèäÁ¨¨20È°µÂà∞Êñá‰ª∂Êú´Â∞æÔºàÂ§ßÂÜô‚ÄúN‚ÄùË¢´ËßÜ‰∏∫ÊúÄÂêé‰∏ÄÈ°µÁöÑÁºñÂè∑Ôºâ„ÄÇ\n\n### ËØÜÂà´Ê†áÈ¢ò\n\nÂú®Ë∞ÉÁî®Êó∂ÔºåÁ®ãÂ∫èÊ£ÄÊü•ÁªôÂÆöÈ°µÈù¢‰∏äÁöÑÊâÄÊúâÊñáÊú¨Âπ∂ÊâæÂá∫ÊúÄÂ∏∏Áî®ÁöÑÂ≠ó‰ΩìÂ§ßÂ∞è„ÄÇËØ•ÂÄºÔºà‰ª•ÂèäÊâÄÊúâËæÉÂ∞èÁöÑÂ≠ó‰ΩìÂ§ßÂ∞èÔºâË¢´ÂÅáÂÆö‰∏∫ **Ê≠£ÊñáÊñáÊú¨**„ÄÇËæÉÂ§ßÁöÑÂ≠ó‰ΩìÂ§ßÂ∞èË¢´ÂÅáÂÆö‰∏∫ **Ê†áÈ¢òÊñáÊú¨**„ÄÇ\n\nÊ†πÊçÆÂÆÉ‰ª¨Âú®Â≠ó‰ΩìÂ§ßÂ∞èÂ±ÇÁ∫ß‰∏≠ÁöÑÁõ∏ÂØπ‰ΩçÁΩÆÔºåÊ†áÈ¢òÊñáÊú¨Â∞ÜÂâçÈù¢Âä†‰∏ä‰∏Ä‰∏™ÊàñÂ§ö‰∏™ markdown Ê†áÈ¢ò `#` Ê†áÁ≠æÂ≠óÁ¨¶„ÄÇ\n\n### ÊåâÈ°µÈù¢Âå∫ÂüüËØÜÂà´Â§ÑÁêÜÊ®°Âºè\n\nÊØè‰∏™È°µÈù¢‰∏äÁöÑÊâÄÊúâÊñáÊú¨È¶ñÂÖàÂ∞ÜË¢´ÂàÜÁ±ª‰∏∫**Ê†áÂáÜ**ÊñáÊú¨Êàñ**Ë°®Ê†º**ÊñáÊú¨„ÄÇÁÑ∂ÂêéÔºåÈ°µÈù¢ÂÜÖÂÆπÂ∞Ü‰ªé‰∏äÂà∞‰∏ãÊèêÂèñÔºåÂπ∂ËΩ¨Êç¢‰∏∫MarkdownÊ†ºÂºè„ÄÇ\n\nËøôÊúÄÂ•ΩÈÄöËøá‰∏Ä‰∏™‰æãÂ≠êÊù•Ëß£ÈáäÔºö\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*u5fv2aAIvDaaAd6H.png)\n\nËØ•È°µÈù¢ÊòæÁ§∫ÁöÑÂÜÖÂÆπ‰ª£Ë°®ÂÖ∏ÂûãÊÉÖÂÜµÔºö\n\n* ‰∏§‰∏™Ë°®Ê†ºÔºåÂÖ∑ÊúâÈÉ®ÂàÜÈáçÂè†ÁöÑÂûÇÁõ¥‰ΩçÁΩÆ„ÄÇ‰∏Ä‰∏™Ë°®Ê†ºÊ≤°ÊúâÊ†áÈ¢òÔºåÂè¶‰∏Ä‰∏™Ë°®Ê†ºÊúâ**Â§ñÈÉ®**ÂàóÊ†áÈ¢ò„ÄÇ\n* Êúâ‰∏ÄË°å**Ê†áÈ¢ò**ÂíåÂ§ö‰∏™Á∫ßÂà´ÁöÑ**Ê†áÈ¢ò**„ÄÇ\n* **Ê≠£ÊñáÊñáÊú¨**ÂåÖÂê´Â§öÁßçÊ†∑ÂºèÁªÜËäÇÔºåÂ¶Ç**Á≤ó‰Ωì**„ÄÅ*Êñú‰Ωì*Âíå`Ë°åÂÜÖ‰ª£Á†Å`„ÄÇ\n* ÊúâÂ∫èÂíåÊó†Â∫èÂàóË°®„ÄÇ\n* ‰ª£Á†ÅÁâáÊÆµ„ÄÇ\n\nÂ∏ÉÂ±ÄÂàÜÊûêÂ∞ÜÁ°ÆÂÆö‰∏â‰∏™Âå∫ÂüüÂπ∂ÈÄâÊã©ÈÄÇÂΩìÁöÑÂ§ÑÁêÜÊ®°ÂºèÔºö**(1)** ÊñáÊú¨Ôºå**(2)** Ë°®Ê†ºÔºå**(3)** ÊñáÊú¨„ÄÇ\n\nÁîüÊàêÁöÑMarkdownÊñáÊú¨Âø†ÂÆûÂú∞ÂèçÊò†‰∫Ü‰∏äËø∞ÂÜÖÂÆπ‚Äî‚ÄîÂú®ËøôÁßçÊ†ºÂºè‰∏≠Â∞ΩÂèØËÉΩÂÅöÂà∞„ÄÇ\n\n‰Ωú‰∏∫‰∏Ä‰∏™‰æãÂ≠êÔºåËÆ©Êàë‰ª¨Áúã‰∏Ä‰∏ãÂÖ∑ÊúâÂ§ñÈÉ®Ê†áÈ¢òÁöÑË°®Ê†ºÁöÑËæìÂá∫Ôºö\n\n```python\n|Column1|Column2|\n\n|---|---|\n\n|Cell (0, 0)|Cell (0, 1)|\n\n|Cell (1, 0)|Cell (1, 1)|\n\n|Cell (2, 0)|Cell (2, 1)|\n```\nËøôÊòØ‰∏éGitHubÂÖºÂÆπÁöÑÊ†ºÂºèÔºåÂÖ∑ÊúâÊúÄÂ∞èÁöÑÂèØËÉΩ‰ª§ÁâåÂ§ßÂ∞è‚Äî‚ÄîËøôÊòØ‰øùÊåÅËæìÂÖ•Âà∞RAGÁ≥ªÁªüÁöÑÂ∞èÂûãÂåñÁöÑÈáçË¶ÅÊñπÈù¢„ÄÇ\n\n**ÂàóËæπÊ°Ü**Áî±‚Äú`|`‚ÄùÂ≠óÁ¨¶Ë°®Á§∫„ÄÇÂ¶ÇÊûúÊñáÊú¨Ë°åÂêéÈù¢Ë∑üÁùÄ‚Äú`|---|---| ‚Ä¶`‚ÄùÂΩ¢ÂºèÁöÑË°åÔºåÂàôÂÅáÂÆöËØ•ÊñáÊú¨Ë°åÊòØ**Ë°®Â§¥**„ÄÇÂÆåÊï¥ÁöÑ**Ë°®Ê†ºÂÆö‰πâ**ÂøÖÈ°ªÂâçÂêéËá≥Â∞ëÊúâ‰∏ÄË°åÁ©∫Ë°å„ÄÇ\n\nËØ∑Ê≥®ÊÑèÔºåÁî±‰∫éÊäÄÊúØÂéüÂõ†ÔºåMarkdownË°®Ê†ºÂøÖÈ°ªÊúâ‰∏Ä‰∏™Ê†áÈ¢òÔºåÂõ†Ê≠§Â¶ÇÊûúÊ≤°ÊúâÂ§ñÈÉ®Ê†áÈ¢òÔºåÂ∞ÜÈÄâÊã©Á¨¨‰∏ÄË°å‰Ωú‰∏∫Ë°®Â§¥„ÄÇ\n\n‰∏∫‰∫ÜÁ°ÆËÆ§Êï¥‰ΩìÂáÜÁ°ÆÊÄßÔºå‰ª•‰∏ãÊòØMarkdownËß£ÊûêÂô®Â¶Ç‰ΩïÂ§ÑÁêÜÂÆåÊï¥È°µÈù¢ÁöÑÁ§∫‰æãÔºö\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Ge83uj7FiM4T6XFn)\n\n## ‰ª•ÁºñÁ®ãÊñπÂºèË∞ÉÁî® Markdown ËΩ¨Êç¢Âô®\n\nÈô§‰∫ÜÂú®ÂëΩ‰ª§Ë°å‰∏≠ÊâßË°åÁ®ãÂ∫èÂ§ñÔºåMarkdown ËΩ¨Êç¢‰πüÂèØ‰ª•ÈÄöËøáÁ®ãÂ∫èËØ∑Ê±ÇÔºö\n\n```python\nimport fitz\nfrom pymupdf_rag import to_markdown  # import Markdown converter\n\ndoc = fitz.open(‚Äúinput.pdf‚Äù)  # open input PDF\n\n## define desired pages: this corresponds ‚Äú-pages 1-10,15,20-N‚Äù\npage_list = list(range(9)) + [14] + list(range(19, len(doc) ‚Äì 1))\n\n## get markdown string for all pages\nmd_text = to_markdown(doc, pages=page_list)\n\n## write markdown string to some file\noutput = open(‚Äúout-markdown.md‚Äù, ‚Äúw‚Äù)\noutput.write(md_text)\noutput.close()\n```\n\n## ÁªìËÆ∫\n\nÈÄöËøáÈõÜÊàê PyMuPDF ÁöÑÊèêÂèñÊñπÊ≥ïÔºåPDF È°µÈù¢ÁöÑÂÜÖÂÆπÂ∞ÜË¢´Âø†ÂÆûÂú∞ËΩ¨Êç¢‰∏∫ÂèØÁî®‰Ωú RAG ËÅäÂ§©Êú∫Âô®‰∫∫ÁöÑËæìÂÖ•ÁöÑ Markdown ÊñáÊú¨„ÄÇ\n\nËØ∑ËÆ∞‰ΩèÔºåÊàêÂäüÁöÑ RAG ËÅäÂ§©Êú∫Âô®‰∫∫ÁöÑÂÖ≥ÈîÆÂú®‰∫éÂÆÉËÉΩÂ§üËÆøÈóÆÁöÑ‰ø°ÊÅØÁöÑË¥®ÈáèÂíåÂÆåÊï¥ÊÄß„ÄÇ\n\nÂêØÁî® PyMuPDF ÁöÑ Markdown ÊèêÂèñÁ°Æ‰øù‰ªé PDF ‰∏≠Ëé∑ÂèñËøô‰∫õ‰ø°ÊÅØ‰∏ç‰ªÖÊòØÂèØËÉΩÁöÑÔºåËÄå‰∏îÊòØÁÆÄÂçïÁöÑÔºåÂ±ïÁ§∫‰∫ÜËØ•Â∫ìÁöÑÂº∫Â§ßÂíåÂØπÂºÄÂèëËÄÖÁöÑÂèãÂ•Ω„ÄÇÁ•ùÁºñÁ†ÅÊÑâÂø´ÔºÅ\n\n### Ê∫ê‰ª£Á†Å\n\n* [RAG/helpers/pymupdf\\_rag.py (github.com)](https://github.com/pymupdf/RAG/blob/main/helpers/pymupdf_rag.py)\n\n### ÂèÇËÄÉÊñáÁåÆ\n\n* [5 Levels of Text Splitting](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb)\n\n### Áõ∏ÂÖ≥ÂçöÂÆ¢\n\n* [‰ΩøÁî® ChatGPT API Âíå PyMuPDF ÊûÑÂª∫ RAG ËÅäÂ§©Êú∫Âô®‰∫∫ GUI](https://readmedium.com/building-a-rag-chatbot-gui-with-the-chatgpt-api-and-pymupdf-9ea8c7fc4ab5)\n* [‰ΩøÁî® ChatGPT Âíå PyMUPDF ÂàõÂª∫ RAG ËÅäÂ§©Êú∫Âô®‰∫∫](https://readmedium.com/creating-a-rag-chatbot-with-chatgpt-and-pymupdf-f6c30907ae27)\n* [RAG/LLM Âíå PDFÔºöÂ¢ûÂº∫ÊñáÊú¨ÊèêÂèñ](https://readmedium.com/rag-llm-and-pdf-enhanced-text-extraction-5c5194c3885c)\n\n"},{"lang":"zh","group":"blog","slug":"blog/rbyf-qwen2-5-3b-instruct-is-damn-good-dcf443cacc63","frontmatter":{"title":"RBYFÔºöQwen2.5‚Äì3B-instruct ÈùûÂ∏∏Ê£í„ÄÇ","meta_title":"RBYFÔºöQwen2.5‚Äì3B-instruct ÈùûÂ∏∏Ê£í„ÄÇ","description":"‰øÆÊîπÂêéÁöÑÂü∫ÂáÜÊµãËØïÂπ∂ÈôÑ‰∏äÊÇ®ÁöÑÂèçÈ¶àÔºöÈòøÈáåÂ∑¥Â∑¥QwenÁöÑÂÖ®Êñ∞3BÊ®°ÂûãÊòØ‰∏Ä‰∏™‰∫Ü‰∏çËµ∑ÁöÑÊ®°ÂûãÔºåÊàëÂèØ‰ª•ËØÅÊòéËøô‰∏ÄÁÇπÔºÅ","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*NWaBtJ64TLUoUHv4F1qJpg.png","categories":["Programming","Technology","Science"],"author":"Rifx.Online","tags":["Qwen","NLP","multimodal","RBYF","evaluation"],"draft":false,"slug":"blog/rbyf-qwen2-5-3b-instruct-is-damn-good-dcf443cacc63"},"content":"\n### ‰øÆËÆ¢Âü∫ÂáÜÔºö‰ª•ÊÇ®‰∏∫ÂèçÈ¶àÁöÑÂÖ®Êñ∞3BÊ®°ÂûãÊù•Ëá™ÈòøÈáåÂ∑¥Â∑¥QwenÔºåÊòØ‰∏™‰ª§‰∫∫ÊÉäÂèπÁöÑÊ®°ÂûãÔºåÊàëÂèØ‰ª•ËØÅÊòéËøô‰∏ÄÁÇπÔºÅ\n\n\n\nÊ∂åÁé∞Â±ûÊÄßÁöÑÈîôËßâÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÊòØËØÑ‰º∞Ëøô‰∫õÊ®°ÂûãÊâÄ‰ΩøÁî®ÁöÑÊåáÊ†áÁöÑ‰∫ßÁâ©„ÄÇËøôÊòØ‰∏Ä‰∏™‰∫ãÂÆû„ÄÇ\n\nÂá†Âë®ÂâçÔºåÊàëÂÜ≥ÂÆöÂÅö‰∏Ä‰∏™Â∞èÂèçÂèõÔºåÊîæÂºÉÊâÄÊúâÂÆòÊñπÂü∫ÂáÜÔºåÂºÄÂßãËá™Â∑±ÂÅöÂü∫ÂáÜÊµãËØïÔºÅ\n\nËøôÂ∞±ÊòØËøô‰∏™ÂÆåÂÖ®ËôöÊûÑÁöÑÈ¶ñÂ≠óÊØçÁº©Áï•ËØçRBYFÁöÑÊÑè‰πâÔºö‰ª•ÊÇ®‰∏∫ÂèçÈ¶àÁöÑ‰øÆËÆ¢Âü∫ÂáÜ„ÄÇÂÖ∂Âü∫Êú¨ÂéüÂàôÊòØÔºåÊ≤°ÊúâÊØîÊÇ®Êõ¥Â•ΩÁöÑËØÑÂà§ËÄÖÊù•È™åËØÅ‰∏Ä‰∏™Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑ‰ºòÂä£„ÄÇ\n\nËÄÅÂÆûËØ¥ÔºåÊàë‰∏ìÊ≥®‰∫éÂ∞èÂûãËØ≠Ë®ÄÊ®°Âûã„ÄÇÊàëÊ≤°Êúâ‰∏ìÁî®ÁöÑGPUÔºåËÆ°ÁÆóËµÑÊ∫êÊúâÈôê„ÄÇ‰ΩÜÊàëÂêåÊ†∑ÂêåÊÑè[LLMWareÂèçÂèõÂéüÂàôÁ¨¨‰∏ÄÊù°](https://readmedium.com/getting-work-done-with-genai-just-do-the-opposite-10-contrarian-rules-that-may-actually-work-634501602a27)Ôºö\n\n‰ΩøÁî®Â∞èÊ®°ÂûãÔºåËÄå‰∏çÊòØÂ§ßÊ®°Âûã„ÄÇ\n\nÂú®ËøôÁØáÊñáÁ´†‰∏≠ÔºåÊàëÂ∞ÜÂêëÊÇ®Â±ïÁ§∫ÊàëÂØπqwen2.5‚Äì3b-instructÁöÑËØÑ‰º∞ÁªìÊûú„ÄÇÁªìÊûúÁúüÁöÑÂæàÂ•ΩÔºÅ\n\n> ÂÖçË¥£Â£∞ÊòéÔºöÊâÄÊúâ‰∏éÁªìÊûúÁõ∏ÂÖ≥ÁöÑÊèêÁ§∫ÂùáÂèØÂú®ÊàëÁöÑGitHubÂ≠òÂÇ®Â∫ì‰∏≠ÊâæÂà∞Ôºö\n\n## Â∞ëÂç≥ÊòØÂ§ö\n\nÁº©ÊîæÊ≥ïÂàôÊèèËø∞‰∫ÜÊ®°ÂûãÊÄßËÉΩÂ¶Ç‰ΩïÈöèÁùÄÂèÇÊï∞ÂíåËÆ≠ÁªÉÊï∞ÊçÆÊï∞ÈáèÁöÑÂ¢ûÂä†ËÄåÊîπÂñÑ„ÄÇËøô‰∏™ÂéüÂàôÊé®Âä®‰∫ÜÂØπLLMÊñ∞ËÉΩÂäõÁöÑÊé¢Á¥¢„ÄÇ\n\n> ‰ªÖ‰ªÖÈÄöËøáÂ¢ûÂä†Ê®°ÂûãÁöÑËßÑÊ®°ÔºåÊàë‰ª¨ÂèØ‰ª•Ëß£ÈîÅÊñ∞ÁöÑËÉΩÂäõ‚Ä¶‚Ä¶\n\nÁº©ÊîæÊ≥ïÂàôÊèèËø∞‰∫ÜÊ®°ÂûãÊÄßËÉΩ‰∏éÂèÇÊï∞Êï∞ÈáèÂíåËÆ≠ÁªÉÊï∞ÊçÆ‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇÈöèÁùÄÊ®°ÂûãÂèòÂæóÊõ¥Â§ßÂπ∂Âú®Êõ¥Â§öÊï∞ÊçÆ‰∏äËøõË°åËÆ≠ÁªÉÔºåÊàë‰ª¨ÊúüÂæÖÂÆÉ‰ª¨ÁöÑÊÄßËÉΩÂæóÂà∞ÊèêÂçá„ÄÇËøôÂØºËá¥‰∫ÜÂØπË∂äÊù•Ë∂äÂ§ßLLMÁöÑÊó†‰ºëÊ≠¢ËøΩÊ±ÇÔºåÂ∏åÊúõËÉΩÂ§üËß£ÈîÅÊñ∞ÁöÑËÉΩÂäõ„ÄÇ\n\nÊ∂åÁé∞Â±ûÊÄßÊòØÊåáÂú®Â§çÊùÇÁ≥ªÁªü‰∏≠‰∏™‰ΩìÁªÑ‰ª∂‰πãÈó¥ÁöÑÁõ∏‰∫í‰ΩúÁî®‰∏≠‰∫ßÁîüÁöÑÂ±ûÊÄß„ÄÇÈÄöËøáÂ≠§Á´ãÂú∞Á†îÁ©∂ÁªÑ‰ª∂ÔºåÊó†Ê≥ïÈ¢ÑÊµãÊàñÁêÜËß£Ëøô‰∫õÂ±ûÊÄß„ÄÇÂú®LLMÁöÑÊÉÖÂÜµ‰∏ãÔºåÂ∏åÊúõÈöèÁùÄËøô‰∫õÊ®°ÂûãÂèòÂæóÊõ¥Â§ßÂíåÊõ¥Â§çÊùÇÔºåÂÆÉ‰ª¨‰ºöÂ±ïÁé∞Âá∫ÊÑèÊÉ≥‰∏çÂà∞ÁöÑÊñ∞ËÉΩÂäõ„ÄÇ\n\nËøôÊòØ‰∏Ä‰∏™Á´•ËØùÊïÖ‰∫ã„ÄÇ\n\nÂú®ËøáÂéªÂá†Âë®ÔºåÊàë‰ª¨‰∫≤ÁúºÁõÆÁùπ‰∫ÜÁªèËøáËøáÂ∫¶ËÆ≠ÁªÉÂíåÁ≤æÂøÉÁ≠ñÂàíÁöÑÂ∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÂèØ‰ª•Ë°®Áé∞Âæó‰∏éÂÆÉ‰ª¨ÁöÑÂ§ßÂûãÂÖÑÂºü‰∏ÄÊ†∑Â•Ω„ÄÇËøôÂØπÊâÄË∞ìÁöÑÊ∂åÁé∞ËÉΩÂäõÊòØ‰∏Ä‰∏™ÈáçÂáªÔºåÂèçÂáª‰∫ÜÁº©ÊîæÊ≥ïÂàô„ÄÇGemma2‚Äì2B„ÄÅQwen2.5‚Äì3BÔºåÁîöËá≥ÊúÄÊñ∞ÁöÑLlama3.2‚Äì3BÈÉΩÊòØËøúËÉú‰∫éÊóßÁöÑSOTA 7BÊ®°ÂûãÁöÑÊõ¥Â•ΩÊ®°Âûã„ÄÇ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*-EjpdEky-Hf3WEQn.png)\n\n## Qwen2.5Ê®°ÂûãÁ≥ªÂàó\n\nÈòøÈáå‰∫ëÂú®‰πùÊúà‰∏≠Êó¨ÂèëÂ∏É‰∫Ü‰ªñ‰ª¨ÁöÑÊóóËà∞Ê®°ÂûãÁ≥ªÂàóQwen2.5„ÄÇ\n\n> ÈòøÈáå‰∫ëÂú®QwenÁöÑÈù©ÂëΩÊÄßÊóÖÁ®ã‰∏≠ÂÜçÊ¨°Â±ïÁé∞Âá∫Âº∫Â§ßÁöÑÂàõÊñ∞È¢ÜÂØºÂäõ\n\nQwen2.5ÊòØÈòøÈáåÂ∑¥Â∑¥ÈõÜÂõ¢QwenÂõ¢ÈòüÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂíåÂ§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÁ≥ªÂàó„ÄÇËøô‰∫õËØ≠Ë®ÄÊ®°ÂûãÂíåÂ§öÊ®°ÊÄÅÊ®°ÂûãÂú®Â§ßËßÑÊ®°Â§öËØ≠Ë®ÄÂíåÂ§öÊ®°ÊÄÅÊï∞ÊçÆ‰∏äËøõË°å‰∫ÜÈ¢ÑËÆ≠ÁªÉÔºåÂπ∂Âú®È´òË¥®ÈáèÊï∞ÊçÆ‰∏äËøõË°å‰∫ÜÂêéËÆ≠ÁªÉÔºå‰ª•‰æø‰∏é‰∫∫Á±ªÂÅèÂ•ΩÂØπÈΩê„ÄÇQwenËÉΩÂ§üÁêÜËß£Ëá™ÁÑ∂ËØ≠Ë®Ä„ÄÅÁîüÊàêÊñáÊú¨„ÄÅÁêÜËß£ËßÜËßâ„ÄÅÁêÜËß£Èü≥È¢ë„ÄÅ‰ΩøÁî®Â∑•ÂÖ∑„ÄÅËßíËâ≤ÊâÆÊºî„ÄÅ‰Ωú‰∏∫AI‰ª£ÁêÜËøõË°åÊìç‰ΩúÁ≠â„ÄÇ\n\n**Êñ∞Ê¨æQwen2.5ÁöÑ‰∫ÆÁÇπÂú®‰∫éÁªèËøáÁ≤æÂøÉÁ≠ñÂàíÁöÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜ„ÄÇ** ÈÄöËøáÊ£ÄÊü•Â∞èÊ®°ÂûãÁöÑÊÄßËÉΩÔºåÊÇ®ÂèØ‰ª•Ê∏ÖÊ•öÂú∞ÁêÜËß£Ëøô‰∏ÄÁÇπ„ÄÇ\n\nÂ¶ÇÊûúËØ•Á≥ªÂàóÁöÑÂ∞èÂûãËØ≠Ë®ÄÊ®°ÂûãË°®Áé∞ËâØÂ•ΩÔºåÊÑèÂë≥ÁùÄËÆ≠ÁªÉÂíåÊï∞ÊçÆÈõÜÁªèËøáÈ´òÂ∫¶‰øÆËÆ¢ÂíåÁ≠ñÂàí„ÄÇ\n\n‰ª•‰∏ãÊòØ‰∏Ä‰∫õÊï∞Â≠óÔºö\n\n* ÂØÜÈõÜÂûã„ÄÅÊòì‰∫é‰ΩøÁî®ÁöÑËß£Á†ÅÂô®ËØ≠Ë®ÄÊ®°ÂûãÔºåÊèê‰æõ0.5B„ÄÅ1.5B„ÄÅ3B„ÄÅ7B„ÄÅ14B„ÄÅ32BÂíå72BÂ∞∫ÂØ∏Ôºå‰ª•ÂèäÂü∫Á°ÄÂíåÊåá‰ª§Âèò‰Ωì„ÄÇ\n* Âú®Êàë‰ª¨ÊúÄÊñ∞ÁöÑÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜÈ¢ÑËÆ≠ÁªÉÔºåÊ∂µÁõñÂ§öËææ18T‰∏™Ê†áËÆ∞„ÄÇ\n* Âú®Êåá‰ª§Ë∑üÈöè„ÄÅÁîüÊàêÈïøÊñáÊú¨ÔºàË∂ÖËøá8K‰∏™Ê†áËÆ∞Ôºâ„ÄÅÁêÜËß£ÁªìÊûÑÂåñÊï∞ÊçÆÔºà‰æãÂ¶ÇÔºåË°®Ê†ºÔºâÂíåÁîüÊàêÁªìÊûÑÂåñËæìÂá∫ÔºàÂ∞§ÂÖ∂ÊòØJSONÔºâÊñπÈù¢ÊúâÊòæËëóÊîπËøõ„ÄÇ\n* ÂØπÁ≥ªÁªüÊèêÁ§∫ÁöÑÂ§öÊ†∑ÊÄßÊõ¥ÂÖ∑ÈüßÊÄßÔºåÂ¢ûÂº∫‰∫ÜËßíËâ≤ÊâÆÊºîÁöÑÂÆûÁé∞ÂíåËÅäÂ§©Êú∫Âô®‰∫∫ÁöÑÊù°‰ª∂ËÆæÁΩÆ„ÄÇ\n* ÊîØÊåÅÁöÑ‰∏ä‰∏ãÊñáÈïøÂ∫¶ÂèØËææ128K‰∏™Ê†áËÆ∞ÔºåÂπ∂‰∏îÂèØ‰ª•ÁîüÊàêÂ§öËææ8K‰∏™Ê†áËÆ∞„ÄÇ\n* ÊîØÊåÅË∂ÖËøá29ÁßçËØ≠Ë®ÄÁöÑÂ§öËØ≠Ë®ÄÔºåÂåÖÊã¨‰∏≠Êñá„ÄÅËã±Êñá„ÄÅÊ≥ïÊñá„ÄÅË•øÁè≠ÁâôÊñá„ÄÅËë°ËêÑÁâôÊñá„ÄÅÂæ∑Êñá„ÄÅÊÑèÂ§ßÂà©Êñá„ÄÅ‰øÑÊñá„ÄÅÊó•Êñá„ÄÅÈü©Êñá„ÄÅË∂äÂçóÊñá„ÄÅÊ≥∞Êñá„ÄÅÈòøÊãâ‰ºØÊñáÁ≠â„ÄÇ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*3tnTS_UCImBRBDeKmFyjVQ.png)\n\n## Qwen2.5‚Äì3B-instruct\n\nÂ∞ΩÁÆ°‰∏∫‰∫ÜÊé®Âä®‰∫∫Â∑•Êô∫ËÉΩÁöÑÂèëÂ±ïÔºåÊàë‰ª¨ÈúÄË¶ÅÈáåÁ®ãÁ¢ëÂíåÊõ¥Â•ΩÁöÑË°®Áé∞Ôºå‰ΩÜÁî®Êà∑‰ΩìÈ™åÂíå‰∏™‰∫∫ËßÇÁÇπ‰∏çËÉΩË¢´ËßÜ‰∏∫Êó†ÂÖ≥Á¥ßË¶Å„ÄÇ\n\nÊàëËÆ§‰∏∫ÔºåÊé¢Á¥¢‰∏Ä‰∫õÂ∏∏Áî®ÁöÑËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰ªªÂä°ÔºåÊäõÂºÄËÅäÂ§©‰ΩìÈ™åÔºåÊàë‰ª¨ÂøÖÈ°ªÂÖ≥Ê≥®ÂõûÂ§çÁöÑË¥®Èáè„ÄÇËÄåÊàë‰ª¨ÊòØÂîØ‰∏ÄÊâÄÈúÄÁöÑÂü∫ÂáÜ„ÄÇ**Êàë‰ª¨ÁöÑÁî®Êà∑‰ΩìÈ™åÊòØÁêÜËß£Ê®°ÂûãÂ•ΩÂùèÁöÑÊúÄ‰Ω≥ÊåáÊ†á**„ÄÇÊ®°ÂûãÂøÖÈ°ªË∂≥Â§üÂèØÈù†Ôºå‰ª•‰æøÂú®Ëá™Âä®ÂåñÂ∑•‰ΩúÊµÅÁ®ã‰∏≠‰ΩøÁî®„ÄÇ\n\nÈ°∫‰æøÊèê‰∏Ä‰∏ãÔºåÊàëÂ∑≤ÁªèËøêË°å‰∫ÜÊàëÂÜ≥ÂÆöÁß∞‰πã‰∏∫[RBYF ‚Äî ‰øÆËÆ¢Âü∫ÂáÜ‰∏éÊÇ®ÂèçÈ¶à](https://open.substack.com/pub/thepoorgpuguy/p/rbyf-is-here-revised-benchmarks-with?r=i78xo&utm_campaign=post&utm_medium=web)ÁöÑÈ°πÁõÆÔºåÂú®[Qwen2.5‚Äì1.5b-instruct](https://ai.gopubby.com/qwen2-5-1-5b-the-future-of-mobile-ai-6bd5f29bbc84)‰∏äÔºöÊÇ®ÂèØ‰ª•ÈòÖËØªËØ¶ÁªÜ‰ø°ÊÅØ„ÄÇÂú®ÊñáÁ´†‰∏≠ÔºåÊàëËøòËß£Èáä‰∫ÜÂ¶Ç‰ΩïÂàõÂª∫ÊÇ®ÁöÑÊµãËØïÂü∫ÂáÜ„ÄÇÊâÄÊèèËø∞ÁöÑÊñπÊ≥ï‰∏éÊàëÁî®‰∫éQwen2.5‚Äì3BÁöÑÁõ∏Âêå„ÄÇ\n\nËÆ©Êàë‰ª¨‰ªéÊâÄÊúâ‰ªªÂä°ÁöÑÊï¥‰ΩìË°®Áé∞ÂºÄÂßã„ÄÇËØ•Ê®°ÂûãÂ∑≤Áî±ÊàëËØÑ‰º∞ÔºàÂú®ËøôÁßçÊÉÖÂÜµ‰∏ãÊòØÊàëËá™Â∑±ÁöÑÂèçÈ¶àÔºâÔºåÂü∫‰∫é‰ª•‰∏ãÊòæÁ§∫ÁöÑÂÆöÊÄßÁü©Èòµ„ÄÇ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*rdVHfCDWX9jlvtiq)\n\nÊÄª‰ΩìÂæóÂàÜ‰∏∫62/70 = 8.8\n\nÂ•ΩÁöÑÔºå‰ΩÜQwen2.5‚Äì3B-instructÊòØÂü∫‰∫é‰ªÄ‰πàÂæóÂà∞‰∫ÜËøô‰∏™ËØÑ‰º∞ÂàÜÊï∞Âë¢Ôºü\n\n## ÊµãËØïÊ¶ÇËø∞\n\nËøô‰∏™ÊÉ≥Ê≥ïÊòØËøõË°åÂÖ¨Âπ≥ÁöÑÁî®Êà∑ÂèçÈ¶àÔºåËÄå‰∏çÊòØÂü∫‰∫éÊ†áÂáÜÂü∫ÂáÜÂíåÊ°ÜÊû∂ÁöÑËá™Âä®ÂåñÂèçÈ¶à„ÄÇ‰∏Ä‰∏™Â∞èÂûãËØ≠Ë®ÄÊ®°ÂûãËÉΩÂê¶Êª°Ë∂≥Áî®Êà∑Âú®‰∏ªË¶ÅËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰ªªÂä°‰∏äÁöÑÊÑèÂõæÔºü\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*wgngfGvSebeoH3YxTdvc8A.png)\n\nÊàë‰ª¨Â∏åÊúõÈ™åËØÅÁî®Êà∑ÊÑèÂõæÂíåÂìçÂ∫îË¥®Èáè„ÄÇ‰ª•‰∏ãÊòØÊØè‰∏™‰ªªÂä°ÁöÑËØ¶ÁªÜ‰ø°ÊÅØÔºö\n\n### ‰ªãÁªç\n\nÈ™åËØÅÊ®°ÂûãÂ¶Ç‰ΩïÂõûÂ∫îÂàùÂßãÈóÆÂÄôÂπ∂Ë∞àËÆ∫Ëá™Ë∫´„ÄÇ\n\n### ‰∏ÄÂè•ËØùËß£Èáä\n\nÁªºÂêà‰∏éÊÄªÁªì„ÄÇÊúÄÁªàËØÑ‰º∞ÁöÑÈáçÁÇπÂú®‰∫éÊ®°ÂûãÊòØÂê¶ËÉΩÂ§üÂ∞ÜÂõûÂ§çÂéãÁº©‰∏∫‰ªÖ‰∏ÄÂè•ËØù„ÄÇ\n\n### Áî®‰∏âÊÆµËØùËß£Èáä\n\nÁî®Êà∑ÁöÑÊÑèÂõæÊòØËé∑ÂæóÂØπÊñáÊú¨ÁöÑÊô∫ËÉΩËß£ÈáäÔºåËøôÁßçËß£ÈáäÂøÖÈ°ªÈÄÇÂêàÂàÜÊàê‰∏âÊÆµ„ÄÇSMLÈÄöÂ∏∏‰ºöÂèëÁé∞ËøôÂæàÂõ∞ÈöæÔºåÂõ†‰∏∫‰ªñ‰ª¨ÊÄªÊòØ‰ºöÊ∑ªÂä†‰∏Ä‰∏™ÊÄªÁªìÊÆµËêΩ„ÄÇ\n\n### ËØ¥‚ÄúÊàëÂáÜÂ§áÂ•Ω‰∫Ü‚Äù„ÄÇ\n\nÂú®‰∏Ä‰∏™Âü∫‰∫éËÅäÂ§©ÂõûÂêàÁöÑÂ∫îÁî®‰∏≠ÔºåÈÅµÂæ™Êåá‰ª§ÁöÑÊ®°ÂûãÈÄöÂ∏∏‰ºöË¢´Ë¶ÅÊ±ÇÈ¶ñÂÖàÈòÖËØªÊèê‰æõÁöÑÊñáÊú¨ÔºåÁÑ∂ÂêéËøõË°åÊüêÁßçÂàÜÊûê„ÄÇÈÄöÂ∏∏ÊÉÖÂÜµ‰∏ãÔºåSML Êó†Ê≥ïÂÅöÂà∞Ëøô‰∏ÄÁÇπ‚Ä¶‚Ä¶\n\n### ÊÄªÁªì\n\nÂü∫Êú¨ÊÄªÁªìÔºåÊ≤°ÊúâÈôêÂà∂„ÄÇÂú®ËøôÈáåÔºåÊàë‰ª¨ÊÉ≥ËØÑ‰º∞ÊÄªÁªìÊòØÂ¶Ç‰ΩïÂü∫‰∫éÊñáÊú¨ÁöÑÔºåËÄå‰∏çÊòØÂá≠Á©∫ÊçèÈÄ†‰∫ãÂÆû„ÄÇ\n\n### ÊÄªÁªì‰∏∫‰∏§Âè•\n\nÂü∫Êú¨ÊÄªÁªìÔºåÈôêÂà∂‰∏∫‰∏§Âè•ËØù„ÄÇÂú®ËøôÈáåÔºåÊàë‰ª¨Â∏åÊúõËØÑ‰º∞ÊÄªÁªìÊòØÂê¶Âü∫‰∫éÊñáÊú¨ÔºåËÄå‰∏çÊòØËôöÊûÑÁöÑ‰∫ãÂÆûÔºåÂêåÊó∂Á°Æ‰øùÈÅµÂÆà‰∏§Âè•ÁöÑÈôêÂà∂„ÄÇ\n\n### ÂàóÂá∫‰∏â‰∏™‰∏ªË¶ÅÂÖ≥ÈîÆÁÇπ ‚Äî Ê†ºÂºèËæìÂá∫\n\n```python\nkey_points = [\n    \"SMLÂøÖÈ°ª‰ª•ÁâπÂÆöÊ†ºÂºèËæìÂá∫„ÄÇ\",\n    \"Ê≠§ÊèêÁ§∫Ë¶ÅÊ±ÇÂàõÂª∫‰∏Ä‰∏™ÂåÖÂê´3‰∏™ÂÖ≥ÈîÆÁÇπÁöÑÂàóË°®„ÄÇ\",\n    \"ËæìÂá∫Ê†ºÂºèÂ∫î‰∏∫PythonÂàóË°®„ÄÇ\"\n]\n```\n\n### ÁõÆÂΩï\n\nËøô‰∏™‰ªªÂä°ÂØπËÆ∏Â§ö SML Êù•ËØ¥Áõ∏ÂΩìÂõ∞Èöæ„ÄÇËØ•ÊèêÁ§∫ÈúÄË¶Å‰∏Ä‰∫õË∞ÉÊï¥ÔºåÂê¶ÂàôÊ®°Âûã‰ºöËøîÂõû‰∏Ä‰∏™ markdown Ë°®Ê†º„ÄÇÁî®Êà∑Â∏åÊúõÊåâÁÖßÊèê‰æõÁöÑÊñáÊ°£ÁªìÊûÑËé∑Âæó‰∏Ä‰∏™ÊúâÂ∫èÁöÑ‰∏ªÈ¢òÂàóË°®„ÄÇ\n\n### RAG\n\nÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºåÊ≤°Êúâ‰ªª‰ΩïÊ°ÜÊû∂ÔºàhaystackÔºåLangchain‚Ä¶Ôºâ„ÄÇËøôÊòØËØ≠Ë®ÄÊ®°Âûã‰∏≠ÊúÄÂ∏∏Áî®ÁöÑ‰ªªÂä°‰πã‰∏Ä„ÄÇÂõûÂ§çÁöÑËØÑ‰º∞Âü∫‰∫éÂØπÊåá‰ª§ÁöÑÁêÜËß£ËÉΩÂäõ‰ª•ÂèäÁ≠îÊ°à‰∏éÊñáÊú¨ÁöÑÁõ∏ÂÖ≥ÊÄß„ÄÇ\n\n### ÁúüÂÆûÁöÑ RAG\n\nËøôÊòØ‰∏Ä‰∏™‰∏éÊèê‰æõÁöÑ‰∏ä‰∏ãÊñáÂÆåÂÖ®Êó†ÂÖ≥ÁöÑÈóÆÈ¢òÁöÑ RAG„ÄÇÊ®°ÂûãÂøÖÈ°ªÂõûÂ§ç‚ÄúÊó†Ê≥ïÂõûÁ≠î‚ÄùÔºåËøôÊÑèÂë≥ÁùÄÂÆÉÁêÜËß£‰∫ÜÊåá‰ª§ÔºåÂπ∂‰∏îÊ≤°Êúâ‰ΩøÁî®‰ªª‰ΩïÂ§ñÈÉ®Áü•ËØÜÊàñËôöÊûÑÁöÑ‰ø°ÊÅØ„ÄÇ\n\n### ‰ªéÂèÇËÄÉÊñáÁåÆÊí∞ÂÜôÂÜÖÂÆπ\n\nËøôÊòØ‰∏Ä‰∏™ÂàõÈÄ†ÊÄßÁöÑ‰ªªÂä°„ÄÇ‰ΩøÁî®ÂèÇËÄÉÊñáÊú¨ÔºåSMLÂøÖÈ°ªÊèê‰æõ‰∏ÄÁØáÊñ∞ÁöÑÊñáÁ´†„ÄÇ\n\n### ÊèêÂèñ5‰∏™‰∏ªÈ¢ò\n\nÊ≠§‰ªªÂä°ÁöÑÈáçÁÇπÊòØÈ™åËØÅÔºö\n\n* Á°Æ‰øùÊ≠£Â•ΩÊúâ5‰∏™‰∏ªÈ¢ò\n* ÂÆÉ‰ª¨ÊòØÂü∫‰∫é‰∫ãÂÆûÁöÑÔºàÊ≤°ÊúâËôöÊûÑÔºâ\n\n### ÂàõÊÑèÔºö1000Â≠óÁßëÂπªÊïÖ‰∫ã\n\nÂÆåÂÖ®ÂàõÈÄ†ÊÄßÁöÑ‰ªªÂä°„ÄÇÂç≥‰ΩøÂØπ‰∫éÊõ¥Â§ßÁöÑÊ®°ÂûãÊù•ËØ¥ÔºåË¶Å‰øùÊåÅËøûË¥ØÊÄßÂπ∂‰∫ßÁîü‰∏Ä‰∏™Á¨¶ÂêàÂ≠óÊï∞Ë¶ÅÊ±ÇÁöÑÂ∞èÊïÖ‰∫ã‰πüÊòØÈùûÂ∏∏Âõ∞ÈöæÁöÑ„ÄÇ\n\n### ÂèçÊÄùÊèêÁ§∫\n\nÂèçÊÄùÊèêÁ§∫Êó®Âú®È™åËØÅÊ®°ÂûãÁöÑÈìæÂºèÊé®ÁêÜËøáÁ®ã„ÄÇËæìÂá∫Ë¢´ÈôêÂà∂Âú®ÁâπÊÆäÊ†áÁ≠æÁöÑÂºÄÂ§¥/ÁªìÂ∞æ„ÄÇÈáçÁÇπÊòØÊé®ÁêÜÂíå‰∏ÄËá¥ÁöÑËæìÂá∫ÁªìÊûÑ„ÄÇËæìÂá∫ÂøÖÈ°ªÊòì‰∫éÁî®‰∫éËøõ‰∏ÄÊ≠•ÁöÑÁªìÊûÑÂåñÊèêÁ§∫ÊàñÂèØËßÜÂåñ„ÄÇÊÇ®ÂèØ‰ª•Âú®Êú¨Êñá‰∏≠ÈòÖËØªÊõ¥Â§öÂÜÖÂÆπÔºö\n\n## ËØÑ‰º∞ËøáÁ®ã\n\nÂú®ÊØèÊ¨°ÁîüÊàêÁªìÊùüÊó∂ÔºåÁî®Êà∑‰ºöË¢´Ë¶ÅÊ±ÇÁî®0Âà∞5ÁöÑÂàÜÊï∞Êù•ËØÑ‰º∞ÁªìÊûú„ÄÇ**Âú®ËøôÁßçÊÉÖÂÜµ‰∏ãÔºåÁî®Êà∑Â∞±ÊòØÊàë‚Ä¶‚Ä¶**\n\nËøôÁßçÂÆöÊÄßÂàÜÊûêÁ°ÆÂÆûËæÉ‰∏∫ÁÆÄÂçïÔºåÂõ†Ê≠§ÊØè‰∏™ÂàÜÊï∞ÈÉΩÊúâ‰∏Ä‰∏™ÊèèËø∞ÔºåÁî®Êà∑ÂèØ‰ª•Ê∑ªÂä†ËØÑËÆ∫Ôºà‚Äú‰∏Ä‰∫õÈîôËØØÁöÑ‰ø°ÊÅØ‚ÄùÔºå‚Äú‰πüËÆ∏Êõ¥ÊîπÊèêÁ§∫‰∏≠ÁöÑÊé™Ëæû‰ºöÊõ¥Â•Ω‚ÄùÔºâ\n\nËøôÈáåÊòØÂ∏¶ÊúâÊèèËø∞ÁöÑÂÆöÊÄßÁü©Èòµ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*H_Qx2UT1lxqy2BF4.png)\n\n## Â•Ω‰∏éÂùè ‚Äî ÁªÜËäÇ\n\nÊÄª‰ΩìËÄåË®ÄÔºåQwen2.5‚Äì3B-instruct ÊòØ‰∏Ä‰∏™ÂÖ®Êñπ‰ΩçÁöÑÊ®°Âûã„ÄÇÊØè‰∏™‰ªªÂä°ÁöÑÂπ≥ÂùáÂæóÂàÜ‰∏∫ 4.4 ÂàÜÔºàÊª°ÂàÜ 5 ÂàÜÔºâÔºåËøôÁõ∏ÂΩì‰ª§‰∫∫Âç∞Ë±°Ê∑±Âàª„ÄÇ\n\nÊëòË¶ÅË°®Áé∞Âá∫Ëâ≤„ÄÇÂú®ÈïøÊñáÊú¨‰∏≠ÂàóÂá∫‰∏ªË¶Å‰∏ªÈ¢ò‰πüÈùûÂ∏∏Â•Ω„ÄÇ\n\nÂÆÉÂú® RAG ‰∏≠Ë°®Áé∞Á™ÅÂá∫ÔºåÊúâÊó∂ÊòæÂæóËøá‰∫éÂÜóÈïøÔºöÂú®ÁúüÂÆûÁöÑ RAG ‰∏≠ÔºåÂÆÉËøòÊèê‰æõ‰∫Ü‰∏Ä‰∫õËß£ÈáäÔºå‰ª•Âèä *Êó†Ê≥ïÂõûÁ≠î* ÂõûÂ§çÁöÑÂéüÂõ†„ÄÇ\n\nÊàë‰ª¨ÂèØ‰ª•‰∏ÄËµ∑ÁúãÁúã„ÄÇËøôÂØπ‰∫é SML Êù•ËØ¥ÔºåÁ≤æÁ°ÆËÆ°ÁÆóÂè•Â≠êÂíåÊÆµËêΩÁöÑÊï∞ÈáèÈÄöÂ∏∏ÊòØ‰∏ÄÈ°πËâ∞Â∑®ÁöÑ‰ªªÂä°„ÄÇ\n\n```python\n**Write a two-sentences summary of the following text:**\n\nChina's regime under Chinese Communist Party operates without transparent \nelections and primary candidates. While undesirable for democracies, \nit allows long-term planning free from electoral pressures that can change \nplans abruptly; this has led to China achieving peak carbon dioxide emissions \nin 2023 due largely to increased renewable energy installations, potentially \nsetting a new global standard for sustainable development despite its status \nas one of the world's largest polluters.\n```\n\nÁõÆÂΩïÂØπ‰∫éËØ≠Ë®ÄÊ®°ÂûãÊù•ËØ¥ÊòØÂè¶‰∏Ä‰∏™Âô©Ê¢¶ÔºåÂõ†‰∏∫ÂÆÉ‰ª¨ÈÄöÂ∏∏ÁêÜËß£Â¶Ç‰ΩïÂàõÂª∫Ë°®Ê†º‚Ä¶‚Ä¶‰ΩÜÁúãÁúãËøô‰∏™Ôºö\n\n```python\nA \"table of content\" provides an organized structure to the text, allowing \nreaders quick access to key points. Write the \"table of content\" of \nthe following text.\n\n[table of contents]\n1. Introduction: Overview and Critique on China's Governance System  \n2. Comparison with Western Democracies - Undesirable Dictatorship? \n3. Advantages in Long-Term Planning for Sustainability   \n4. The Chinese Model as a Comparative Advantage  \n5. Recent Achievements in Carbon Emissions Reduction\n6. Transition to Renewable Energy: A Turning Point?\n7. Potential Leadership Role of China and India  \n8. Conclusion - Future Directions\n\n```\n\nÊàëÁªôÁúüÂÆûÁöÑ RAG ËØÑÂàÜ 4 ÂàÜÔºàÊª°ÂàÜ 5 ÂàÜÔºâÔºå‰ΩÜÂÆÉÊú¨ÂèØ‰ª•ËΩªÊùæÂæóÂà∞ 5 ÂàÜ„ÄÇÊúÄÈáçË¶ÅÁöÑÊòØÔºåÂõûÂ§ç‰∏≠ÂøÖÈ°ªÂåÖÂê´ *Êó†Ê≥ïÂõûÁ≠î* Ëøô‰∏™ËØç„ÄÇ\n\n‰∏∫‰ªÄ‰πàÔºüÊÉ≥Ë±°‰∏Ä‰∏ã‰Ω†Âú®ËÆæÁΩÆ‰∏ÄÁ≥ªÂàóÊèêÁ§∫Ôºå‰Ω†ÈúÄË¶ÅÊ†πÊçÆÂÖàÂâçÁöÑÂõûÂ§çÂÜ≥ÂÆö‰∏ã‰∏ÄÊ≠•„ÄÇÂ¶ÇÊûú‰Ω†ÂèØ‰ª•ÁÆÄÂçïÂú∞ÂØªÊâæ *Êó†Ê≥ïÂõûÁ≠î* Ëøô‰∏™ËØçÁöÑÂ≠òÂú®Êù•ÂºïÂØº‰Ω†ÁöÑ‰∏ã‰∏ÄÊ≠•ÔºåÈÇ£Â∞±Ê≤°ÈóÆÈ¢ò‰∫ÜÔºÅ\n\n```python\n**Reply to the question only using the provided context. If the answer \nis not contained in the provided context say \"unanswerable\".\nquestion: who is Anne Frank?**\n{context}\n**Remember: if you cannot answer based on the provided context, say \n\"unanswerable\"\nanswer:**\n\nUnanswerable\nThe given text does not provide any information about Anne Frank \nor her identity. The content is unrelated to this question and focuses \ninstead on discussing China's system compared to Western democracies in \nterms of long-term planning for environmental issues.\n```\n\nÊàëÊÉ≥Ë±°Ê®°Âûã‰πüË¢´ËÆ≠ÁªÉÊù•Êèê‰æõËøôÁßçÂõûÂ§çÁöÑÂéüÂõ†‚Ä¶‚Ä¶Â§™Ê£í‰∫ÜÔºÅ\n\n### ‰ºòÁßÄÁöÑÂèçÊÄù\n\nÂèçÊÄùÊèêÁ§∫ÁöÑ‰∏ªË¶ÅÁÑ¶ÁÇπÊòØÊ®°ÂûãËøîÂõûÊ≠£Á°ÆÊé®ÁêÜËøáÁ®ãÁöÑËÉΩÂäõÔºåÂπ∂ÊåâÁÖßÂá†‰∏™ÁâπÂÆöÁöÑÊ†áÁ≠æÂºÄÈó≠Ê†ºÂºèÂåñËæìÂá∫„ÄÇ\n\nÂà∞ÁõÆÂâç‰∏∫Ê≠¢ÔºåËøôÊòØ**Â∞èÂûãËØ≠Ë®ÄÊ®°Âûã**‰∏≠**ÊúÄ‰Ω≥ÁöÑÂèçÊÄùËæìÂá∫**ÔºåÁîöËá≥ÊØîGemma2‚Äì2BÊõ¥Â•ΩÔºö\n\n```python\n*üë®‚Äçüíª > You are an AI assistant designed to provide detailed, step-by-step \nresponses. Your outputs should follow this structure:\n1. Begin with a <thinking> section.\n2. Inside the thinking section:\n   a. Briefly analyze the question and outline your approach.\n   b. Present a clear plan of steps to solve the problem.\n   c. Use a \"Chain of Thought\" reasoning process if necessary, breaking down \nyour thought process into numbered steps.\n3. Include a <reflection> section for each idea where you:\n   a. Review your reasoning.\n   b. Check for potential errors or oversights.\n   c. Confirm or adjust your conclusion if necessary.\n4. Be sure to close all reflection sections.\n5. Close the thinking section with </thinking>.\n6. Provide your final answer in an <output> section.\nAlways use these tags in your responses. Be thorough in your explanations, \nshowing each step of your reasoning process. Aim to be precise and logical \nin your approach, and don't hesitate to break down complex problems into \nsimpler components. Your tone should be analytical and slightly formal, \nfocusing on clear communication of your thought process.\nRemember: Both <thinking> and <reflection> MUST be tags and must be closed \nat their conclusion\nMake sure all <tags> are on separate lines with no other text. \nDo not include other text on a line containing a tag.\n\nuser question: explain why it is crucial for teachers to learn how to use \ngenerative AI for their job and for the future of education. \nInclude relevant learning path for teachers and educators. *\n\n**üíª > <thinking>\nI will begin by analyzing the importance of teaching about Generative \nArtificial Intelligence (AI) in an educational context, then outline a \nstep-by-step plan on what steps should be taken to learn this technology \neffectively as well as provide resources or courses that could help.\nFirstly, understanding AI is crucial for teachers because it can \nsignificantly enhance their ability to support students. \nTeachers need the knowledge and skills necessary not only to integrate these tools into teaching but also understand how they work so they are able to guide both themselves and their pupils in ethical use of technology.\n</thinking>\n\n<reflection>\nMy initial thought process was clear, emphasizing that teachers should \nbe equipped with AI literacy as it can improve educational outcomes. \nI outlined the importance for guiding students ethically while using \nthese tools effectively.\nHowever, my plan needs more detail on specific steps to learn about \ngenerative AI and resources available online or in-person courses.\n</reflection>**\n*MY PERSONAL NOTE: here it is starting an additional thinking after the first \nreflection... And this is really too good to be true!!*\n**<thinking>\nTo create a comprehensive learning path that teachers could follow:\n1. **Understand Basic Concepts**: Start with basic concepts of machine \n   learning (ML) such as supervised vs unsupervised, deep neural networks  \n   etc., which are foundational to understanding generative AI.\n2. **Learn About Generative Models**:\n   - Study different types: Variational Autoencoders (VAEs), GANs.\n   - Understand how these models generate new data that resembles the \n     training set but is not identical and can be used for tasks like \n     text generation, image synthesis etc., which are crucial in education.\n3. **Explore Applications**: Look at real-world applications of generative \n   AI such as language modeling (e.g., GPT-2), computer vision techniques to \n   create realistic images or videos from raw data.\n4. **Ethical Considerations and Implications for Education**:\n   - Discuss the ethical implications, including bias in models' outputs \n     which could affect students‚Äô perceptions about themselves if they are \n     exposed regularly through AI-generated content that is not representative \n     of them.\n5. **Hands-on Practice**: Engage with practical exercises using open-source \n   tools like TensorFlow or PyTorch to build simple generative model \n   applications.\n6. **Stay Updated and Network**:\n   - Follow the latest research in ML, especially on Generative Models \n     by following relevant journals (e.g., NeurIPS), conferences such as \n     ICML etc.\n7. Attend Workshops/Conferences: Participate actively at AI-related workshops \n   or events to network with professionals working within this field.\n</thinking>**\n*MY PERSONAL NOTE: and no surprise, an additional reflection now starts, \nbefore producing the final output*\n**<reflection>\nMy plan has been expanded and includes a more detailed approach for teachers, \ncovering foundational knowledge through practical application of generative \nmodels in education along with ethical considerations that are crucial. \nI have also included staying updated on the latest research as an important \npart.\nHowever, it's essential to ensure these resources or courses exist so \neducators can access them easily.\n</reflection>\n\n<output>\nIt is vital for teachers and educational professionals alike to learn how \ngenerative AI works because this technology has significant potential in \nenhancing teaching methods. By understanding basic concepts of machine \nlearning (ML) such as supervised vs unsupervised, deep neural networks etc., \nthey will be better equipped with the knowledge necessary not only to \nintegrate these tools into their curriculum but also guide students ethically.\nA comprehensive plan for teachers and educators includes:\n1. **Understand Basic Concepts**: Start by grasping foundational concepts of ML.\n2. **Learn About Generative Models**:\n   - Study different types: Variational Autoencoders (VAEs), GANs, etc., \n     which are crucial in generating new data that resembles the training \n     set but is not identical and can be used for tasks like text generation or \n     image synthesis.\n3. **Explore Applications**: Look at real-world applications of generative AI \n   such as language modeling using tools like TensorFlow.\n4. **Ethical Considerations**:\n   - Discuss ethical implications, including bias in models' outputs which \n     could affect students‚Äô perceptions about themselves if they are exposed \n     regularly through AI-generated content that is not representative.\n5. **Hands-on Practice**: Engage with practical exercises to build simple \n   generative model applications using open-source tools.\n6. **Stay Updated and Network**:\n   - Follow the latest research in ML, especially on Generative Models by \n     following relevant journals (e.g., NeurIPS), conferences such as ICML etc.\n7. Attend Workshops/Conferences: Participate actively at AI-related workshops \n   or events to network with professionals working within this field.\n</output>\n```\n\nËøôÊ†∑ÁöÑÁ≤æÁÇºÊÄùÁª¥ËøáÁ®ãÔºå‰ª•ÂèäÊ∏ÖÊô∞ÁöÑÊ†áÁ≠æÂºÄÈó≠Ê†ºÂºèÔºåÂèØ‰ª•Êñπ‰æøÂêéÁª≠ÁöÑÂ§ÑÁêÜÊµÅÁ®ãÔºåÊàñËÄÖÂú®ÊºÇ‰∫ÆÁöÑGUI‰∏≠ËøõË°åÁæéËßÇÁöÑÊâìÂç∞„ÄÇ\n\n## ÂÆÉ‰∏çÊìÖÈïø‰ªÄ‰πàÔºü\n\nÊúÄÁ≥üÁ≥ïÁöÑ‰ªªÂä°ÊòØÂàõ‰ΩúÁü≠ÁØáÂ∞èËØ¥„ÄÇÊ®°ÂûãÂú®ÁîüÊàêËøáÁ®ã‰∏≠ÂºÄÂßãÈáçÂ§çÁõ∏ÂêåÁöÑÊÆµËêΩ„ÄÇ\n\nËøôÂπ∂‰∏çÊÑèÂë≥ÁùÄÂÆÉÊó†Ê≥ïÂÅöÂà∞„ÄÇÊõ¥ÊúâÂèØËÉΩÁöÑÊòØÔºåÈÄöËøáÊèêÈ´òÊ∏©Â∫¶ÂíåÈáçÂ§çÊÉ©ÁΩöÔºåÂèØ‰ª•Ëé∑ÂæóËâØÂ•ΩÁöÑÁªìÊûú„ÄÇ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GNJ3lG6FM5qt9glIdf7qhQ.jpeg)\n\n## ÁªìËÆ∫\n\nÊÇ®ÂèØ‰ª•Âú®ÊàëÁöÑGitHub‰ªìÂ∫ì‰∏≠ÊâæÂà∞ÊâÄÊúâËÅäÂ§©ËÆ∞ÂΩïÔºå‰ª•Âèä‰ª£Á†ÅÂíåËá™Ë°åÊìç‰ΩúÁöÑËØ¥Êòé„ÄÇÊÇ®ÂèØ‰ª•ÂèÇËÄÉÊàë‰πãÂâçÊñáÁ´†‰∏≠ÁöÑÊïôÁ®ã [Qwen2.5 1.5b: ÁßªÂä®AIÁöÑÊú™Êù•Ôºü](https://ai.gopubby.com/qwen2-5-1-5b-the-future-of-mobile-ai-6bd5f29bbc84)\n\nÂú®Êé•‰∏ãÊù•ÁöÑÊñáÁ´†‰∏≠ÔºåÊàëÂ∞Ü‰ªãÁªçÂÖ∂‰ªñÂ∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÔºå‰ΩøÁî®Áõ∏ÂêåÁöÑÂéüÂàôÔºö‰ªéÂ∞èÂûãÁöÑ350MÂèÇÊï∞ÔºåÂà∞500MÁ≥ªÂàóÔºåÂÜçÂà∞3B ‚Äî ÁªèËøá1.5B„ÄÇ\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/the-6-best-llm-tools-to-run-models-locally-eedd0f7c2bbd","frontmatter":{"title":"6 ÁßçÊúÄ‰Ω≥Êú¨Âú∞ËøêË°åÊ®°ÂûãÁöÑ LLM Â∑•ÂÖ∑","meta_title":"6 ÁßçÊúÄ‰Ω≥Êú¨Âú∞ËøêË°åÊ®°ÂûãÁöÑ LLM Â∑•ÂÖ∑","description":"ËøêË°åÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã (LLM)Ôºà‰æãÂ¶Ç ChatGPT Âíå ClaudeÔºâÈÄöÂ∏∏Ê∂âÂèäÂ∞ÜÊï∞ÊçÆÂèëÈÄÅÂà∞Áî± OpenAI ÂíåÂÖ∂‰ªñ AI Ê®°ÂûãÁÆ°ÁêÜÁöÑÊúçÂä°Âô®‚Ä¶‚Ä¶","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*2MB6-INUUGLR0NR_iOACIg.jpeg","categories":["Technology","Programming","Health"],"author":"Rifx.Online","tags":["LLM","local","deployment","customization","telehealth"],"draft":false,"slug":"blog/the-6-best-llm-tools-to-run-models-locally-eedd0f7c2bbd"},"content":"\n\n\n\n\nËøêË°åÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂ¶Ç [ChatGPT](https://openai.com/chatgpt/mac/) Âíå [Claude](https://claude.ai/) ÈÄöÂ∏∏Ê∂âÂèäÂ∞ÜÊï∞ÊçÆÂèëÈÄÅÂà∞Áî± [OpenAI](https://openai.com/) ÂíåÂÖ∂‰ªñ AI Ê®°ÂûãÊèê‰æõÂïÜÁÆ°ÁêÜÁöÑÊúçÂä°Âô®„ÄÇËôΩÁÑ∂Ëøô‰∫õÊúçÂä°ÊòØÂÆâÂÖ®ÁöÑÔºå‰ΩÜ‰∏Ä‰∫õ‰ºÅ‰∏öÊõ¥ÂÄæÂêë‰∫éÂ∞ÜÂÖ∂Êï∞ÊçÆÂÆåÂÖ®Á¶ªÁ∫øÔºå‰ª•Ëé∑ÂæóÊõ¥È´òÁöÑÈöêÁßÅ‰øùÊä§„ÄÇ\n\nÊú¨ÊñáÂ∞Ü‰ªãÁªçÂºÄÂèë‰∫∫ÂëòÂèØ‰ª•‰ΩøÁî®ÁöÑÂÖ≠Ê¨æÂ∑•ÂÖ∑Ôºå‰ª•‰æøÂú®Êú¨Âú∞ËøêË°åÂíåÊµãËØï LLMÔºåÁ°Æ‰øù‰ªñ‰ª¨ÁöÑÊï∞ÊçÆÊ∞∏Ëøú‰∏ç‰ºöÁ¶ªÂºÄ‰ªñ‰ª¨ÁöÑËÆæÂ§áÔºåËøôÁ±ª‰ºº‰∫é [Á´ØÂà∞Á´ØÂä†ÂØÜ](https://getstream.io/blog/end-to-end-encryption/) ‰øùÊä§ÈöêÁßÅÁöÑÊñπÂºè„ÄÇ\n\n## ‰∏∫‰ªÄ‰πà‰ΩøÁî®Êú¨Âú∞ LLMÔºü\n\nÂÉè [LM Studio](https://lmstudio.ai/) ËøôÊ†∑ÁöÑÂ∑•ÂÖ∑Âú®Áî®Êà∑‰ΩøÁî®ÂÆÉÊù•ËøêË°åÊú¨Âú∞ LLM Êó∂Ôºå‰∏ç‰ºöÊî∂ÈõÜÁî®Êà∑Êï∞ÊçÆÊàñË∑üË∏™Áî®Êà∑ÁöÑË°å‰∏∫„ÄÇÂÆÉÂÖÅËÆ∏ÊâÄÊúâËÅäÂ§©Êï∞ÊçÆ‰øùÁïôÂú®Êú¨Âú∞ËÆ°ÁÆóÊú∫‰∏äÔºåËÄå‰∏ç‰∏é AI/ML ÊúçÂä°Âô®ÂÖ±‰∫´„ÄÇ\n\n* **ÈöêÁßÅ**ÔºöÊÇ®ÂèØ‰ª•‰ª•Â§öËΩÆÁöÑÊñπÂºèÊèêÁ§∫Êú¨Âú∞ LLMÔºåËÄå‰∏ç‰ºöËÆ©ÊÇ®ÁöÑÊèêÁ§∫Êï∞ÊçÆÁ¶ªÂºÄÊú¨Âú∞‰∏ªÊú∫„ÄÇ\n* **Ëá™ÂÆö‰πâÈÄâÈ°π**ÔºöÊú¨Âú∞ LLM Êèê‰æõ CPU Á∫øÁ®ã„ÄÅÊ∏©Â∫¶„ÄÅ‰∏ä‰∏ãÊñáÈïøÂ∫¶„ÄÅGPU ËÆæÁΩÆÁ≠âÈ´òÁ∫ßÈÖçÁΩÆÈÄâÈ°π„ÄÇËøôÁ±ª‰ºº‰∫é OpenAI ÁöÑÊ∏∏‰πêÂú∫„ÄÇ\n* **ÊîØÊåÅÂíåÂÆâÂÖ®ÊÄß**ÔºöÂÆÉ‰ª¨Êèê‰æõ‰∏é OpenAI Êàñ Claude Áõ∏‰ººÁöÑÊîØÊåÅÂíåÂÆâÂÖ®ÊÄß„ÄÇ\n* **ËÆ¢ÈòÖÂíåË¥πÁî®**ÔºöËøô‰∫õÂ∑•ÂÖ∑ÂÖçË¥π‰ΩøÁî®ÔºåÂπ∂‰∏î‰∏çÈúÄË¶ÅÊØèÊúàËÆ¢ÈòÖ„ÄÇÂØπ‰∫éÂÉè OpenAI ËøôÊ†∑ÁöÑ‰∫ëÊúçÂä°ÔºåÊØè‰∏™ API ËØ∑Ê±ÇÈÉΩÈúÄË¶Å‰ªòË¥π„ÄÇÊú¨Âú∞ LLM ÊúâÂä©‰∫éËäÇÁúÅË¥πÁî®ÔºåÂõ†‰∏∫Ê≤°ÊúâÊØèÊúàËÆ¢ÈòÖ„ÄÇ\n* **Á¶ªÁ∫øÊîØÊåÅ**ÔºöÊÇ®ÂèØ‰ª•Âú®Á¶ªÁ∫øÊó∂Âä†ËΩΩÂíåËøûÊé•Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã„ÄÇ\n* **ËøûÊé•ÊÄß**ÔºöÊúâÊó∂ÔºåËøûÊé•Âà∞ÂÉè OpenAI ËøôÊ†∑ÁöÑ‰∫ëÊúçÂä°ÂèØËÉΩ‰ºöÂØºËá¥‰ø°Âè∑ÂíåËøûÊé•‰∏çËâØ„ÄÇ\n\n## ÂÖ≠Â§ßÂÖçË¥πÊú¨Âú∞ LLM Â∑•ÂÖ∑\n\nÊ†πÊçÆÊÇ®ÁöÑÂÖ∑‰Ωì‰ΩøÁî®Ê°à‰æãÔºåÊÇ®ÂèØ‰ª•ÈÄâÊã©Âá†ÁßçÁ¶ªÁ∫ø LLM Â∫îÁî®Á®ãÂ∫è„ÄÇËøô‰∫õÂ∑•ÂÖ∑‰∏≠Êúâ‰∏Ä‰∫õÂÆåÂÖ®ÂÖçË¥π‰æõ‰∏™‰∫∫ÂíåÂïÜ‰∏ö‰ΩøÁî®„ÄÇÂÖ∂‰ªñÂ∑•ÂÖ∑ÂèØËÉΩÈúÄË¶ÅÊÇ®ÂèëÈÄÅËØ∑Ê±Ç‰ª•Áî®‰∫éÂïÜ‰∏öÁî®ÈÄî„ÄÇÂØπ‰∫é Mac„ÄÅWindows Âíå LinuxÔºåÊúâÂ§öÁßçÊú¨Âú∞ LLM Â∑•ÂÖ∑ÂèØ‰æõÈÄâÊã©„ÄÇ‰ª•‰∏ãÊòØÊÇ®ÂèØ‰ª•ÈÄâÊã©ÁöÑÂÖ≠‰∏™ÊúÄ‰Ω≥Â∑•ÂÖ∑„ÄÇ\n\n## 1. LM Studio\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*svbQPZKu08of7Kv6)\n\n[LM Studio](https://lmstudio.ai/) ÂèØ‰ª•ËøêË°å‰ªª‰ΩïÊ†ºÂºè‰∏∫ `gguf` ÁöÑÊ®°ÂûãÊñá‰ª∂„ÄÇÂÆÉÊîØÊåÅÊù•Ëá™Ê®°ÂûãÊèê‰æõÂïÜÁöÑ `gguf` Êñá‰ª∂ÔºåÂ¶Ç [Llama 3.1](https://llama.meta.com/)„ÄÅ[Phi 3](https://huggingface.co/docs/transformers/main/en/model_doc/phi3)„ÄÅ[Mistral](https://mistral.ai/) Âíå [Gemma](https://ai.google.dev/gemma)„ÄÇË¶Å‰ΩøÁî® LM StudioÔºåËØ∑ËÆøÈóÆ‰∏äËø∞ÈìæÊé•Âπ∂‰∏ãËΩΩÈÄÇÂêàÊÇ®Êú∫Âô®ÁöÑÂ∫îÁî®Á®ãÂ∫è„ÄÇÂêØÂä® LM Studio ÂêéÔºå‰∏ªÈ°µ‰ºöÂ±ïÁ§∫ÂèØ‰∏ãËΩΩÂíåÊµãËØïÁöÑÈ°∂Á∫ß LLM„ÄÇËøòÊúâ‰∏Ä‰∏™ÊêúÁ¥¢Ê†èÔºåÂèØ‰ª•Á≠õÈÄâÂπ∂‰∏ãËΩΩÊù•Ëá™‰∏çÂêå AI Êèê‰æõÂïÜÁöÑÁâπÂÆöÊ®°Âûã„ÄÇ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*sbS3VqiLgDsftgs2)\n\n‰ªéÁâπÂÆöÂÖ¨Âè∏ÁöÑÊ®°Âûã‰∏≠ÊêúÁ¥¢‰ºöÊòæÁ§∫Â§ö‰∏™Ê®°ÂûãÔºåËåÉÂõ¥‰ªéÂ∞èÂûãÂà∞Â§ßÂûã [quantization](https://huggingface.co/docs/optimum/en/concept_guides/quantization)„ÄÇÊ†πÊçÆÊÇ®ÁöÑÊú∫Âô®ÔºåLM Studio ‰ºö‰ΩøÁî®ÂÖºÂÆπÊÄßÁåúÊµãÊù•Á™ÅÂá∫ÊòæÁ§∫ÈÄÇÂêàËØ•Êú∫Âô®ÊàñÂπ≥Âè∞ÁöÑÊ®°Âûã„ÄÇ\n\n## LM Studio ÁöÑÂÖ≥ÈîÆÁâπÊÄß\n\nLM Studio Êèê‰æõ‰∏é ChatGPT Áõ∏‰ººÁöÑÂäüËÉΩÂíåÁâπÊÄß„ÄÇÂÆÉÂÖ∑ÊúâÂ§ö‰∏™ÂäüËÉΩ„ÄÇ‰ª•‰∏ãÊòØ LM Studio ÁöÑÂÖ≥ÈîÆÁâπÊÄß„ÄÇ\n\n* **Ê®°ÂûãÂèÇÊï∞Ëá™ÂÆö‰πâ**ÔºöËøôÂÖÅËÆ∏ÊÇ®Ë∞ÉÊï¥Ê∏©Â∫¶„ÄÅÊúÄÂ§ß‰ª§Áâå„ÄÅÈ¢ëÁéáÊÉ©ÁΩöÁ≠â„ÄÇ\n* **ËÅäÂ§©ÂéÜÂè≤**ÔºöÂÖÅËÆ∏ÊÇ®‰øùÂ≠òÊèêÁ§∫‰ª•‰æõÂêéÁª≠‰ΩøÁî®„ÄÇ\n* **ÂèÇÊï∞ÂíåÁî®Êà∑ÁïåÈù¢ÊèêÁ§∫**ÔºöÊÇ®ÂèØ‰ª•Â∞ÜÈº†Ê†áÊÇ¨ÂÅúÂú®‰ø°ÊÅØÊåâÈíÆ‰∏ä‰ª•Êü•ÊâæÊ®°ÂûãÂèÇÊï∞ÂíåÊúØËØ≠„ÄÇ\n* **Ë∑®Âπ≥Âè∞**ÔºöLM Studio ÂèØÂú® Linux„ÄÅMac Âíå Windows Êìç‰ΩúÁ≥ªÁªü‰∏ä‰ΩøÁî®„ÄÇ\n* **Êú∫Âô®ËßÑÊ†ºÊ£ÄÊü•**ÔºöLM Studio Ê£ÄÊü•ËÆ°ÁÆóÊú∫ËßÑÊ†ºÔºåÂ¶Ç GPU ÂíåÂÜÖÂ≠òÔºåÂπ∂Êä•ÂëäÂÖºÂÆπÁöÑÊ®°Âûã„ÄÇËøôÂèØ‰ª•Èò≤Ê≠¢‰∏ãËΩΩÂèØËÉΩÂú®ÁâπÂÆöÊú∫Âô®‰∏äÊó†Ê≥ïËøêË°åÁöÑÊ®°Âûã„ÄÇ\n* **AI ËÅäÂ§©ÂíåÊ∏∏‰πêÂú∫**Ôºö‰ª•Â§öËΩÆËÅäÂ§©Ê†ºÂºè‰∏éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãËøõË°åÂØπËØùÔºåÂπ∂ÈÄöËøáÂêåÊó∂Âä†ËΩΩÂ§ö‰∏™ LLM ËøõË°åÂÆûÈ™å„ÄÇ\n* **ÂºÄÂèëËÄÖÊú¨Âú∞Êé®ÁêÜÊúçÂä°Âô®**ÔºöÂÖÅËÆ∏ÂºÄÂèëËÄÖËÆæÁΩÆ‰∏Ä‰∏™Á±ª‰ºº‰∫é OpenAI API ÁöÑÊú¨Âú∞ HTTP ÊúçÂä°Âô®„ÄÇ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*9bHmRiOSf6gm-u3P)\n\nÊú¨Âú∞ÊúçÂä°Âô®Êèê‰æõÁ§∫‰æã Curl Âíå Python ÂÆ¢Êà∑Á´ØËØ∑Ê±Ç„ÄÇÊ≠§ÂäüËÉΩÊúâÂä©‰∫é‰ΩøÁî® LM Studio ÊûÑÂª∫ AI Â∫îÁî®Á®ãÂ∫èÔºå‰ª•ËÆøÈóÆÁâπÂÆöÁöÑ LLM„ÄÇ\n\n```python\n## Example: reuse your existing OpenAI setup\nfrom openai import OpenAI\n\n## Point to the local server\nclient = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n\ncompletion = client.chat.completions.create(\n  model=\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"Always answer in rhymes.\"},\n    {\"role\": \"user\", \"content\": \"Introduce yourself.\"}\n  ],\n  temperature=0.7,\n)\n\nprint(completion.choices[0].message)\n```\nÈÄöËøá‰∏äËø∞Á§∫‰æã Python ‰ª£Á†ÅÔºåÊÇ®ÂèØ‰ª•ÈáçÁî®Áé∞ÊúâÁöÑ OpenAI ÈÖçÁΩÆÔºåÂπ∂Â∞ÜÂü∫Êú¨ URL ‰øÆÊîπ‰∏∫ÊåáÂêëÊÇ®ÁöÑÊú¨Âú∞‰∏ªÊú∫„ÄÇ\n\n* **OpenAI ÁöÑ Python Â∫ìÂØºÂÖ•**ÔºöLM Studio ÂÖÅËÆ∏ÂºÄÂèëËÄÖÂØºÂÖ• OpenAI Python Â∫ìÔºåÂπ∂Â∞ÜÂü∫Êú¨ URL ÊåáÂêëÊú¨Âú∞ÊúçÂä°Âô®ÔºàlocalhostÔºâ„ÄÇ\n* **Â§öÊ®°Âûã‰ºöËØù**Ôºö‰ΩøÁî®Âçï‰∏™ÊèêÁ§∫Âπ∂ÈÄâÊã©Â§ö‰∏™Ê®°ÂûãËøõË°åËØÑ‰º∞„ÄÇ\n\n## ‰ΩøÁî® LM Studio ÁöÑÂ•ΩÂ§Ñ\n\nËØ•Â∑•ÂÖ∑ÂèØ‰æõ‰∏™‰∫∫ÂÖçË¥π‰ΩøÁî®ÔºåÂÖÅËÆ∏ÂºÄÂèëËÄÖÈÄöËøáÂ∫îÁî®ÂÜÖËÅäÂ§©Áî®Êà∑ÁïåÈù¢ÂíåÊ∏∏‰πêÂú∫ËøêË°å LLM„ÄÇÂÆÉÊèê‰æõ‰∫Ü‰∏Ä‰∏™Âçé‰∏Ω‰∏îÊòì‰∫é‰ΩøÁî®ÁöÑÁïåÈù¢ÔºåÂ∏¶ÊúâËøáÊª§Âô®ÔºåÂπ∂ÊîØÊåÅËøûÊé•Âà∞ OpenAI ÁöÑ Python Â∫ìÔºåÊó†ÈúÄ API ÂØÜÈí•„ÄÇÂÖ¨Âè∏Âíå‰ºÅ‰∏öÂèØ‰ª•Ê†πÊçÆË¶ÅÊ±Ç‰ΩøÁî® LM Studio„ÄÇÁÑ∂ËÄåÔºåÂÆÉÈúÄË¶Å M1/M2/M3 Mac ÊàñÊõ¥È´òÁâàÊú¨ÔºåÊàñÂÖ∑ÊúâÊîØÊåÅ [AVX2](https://edc.intel.com/content/www/us/en/design/ipla/software-development-platforms/client/platforms/alder-lake-desktop/12th-generation-intel-core-processors-datasheet-volume-1-of-2/009/intel-advanced-vector-extensions-2-intel-avx2/) ÁöÑÂ§ÑÁêÜÂô®ÁöÑ Windows PC„ÄÇIntel Âíå [AMD](https://www.amd.com/en/support/download/drivers.html) Áî®Êà∑‰ªÖÈôê‰∫é‰ΩøÁî® [v0.2.31](https://lmstudio.ai/) ‰∏≠ÁöÑ Vulkan Êé®ÁêÜÂºïÊìé„ÄÇ\n\n## 2. Jan\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*7YeH_48iFYB4lDRu)\n\nÂ∞Ü [Jan](https://jan.ai/) ÁêÜËß£‰∏∫‰∏Ä‰∏™ËÆæËÆ°Áî®‰∫éÁ¶ªÁ∫øÊìç‰ΩúÁöÑÂºÄÊ∫êÁâàÊú¨ÁöÑ ChatGPT„ÄÇÂÆÉÁî±‰∏Ä‰∏™Áî®Êà∑Á§æÂå∫ÊûÑÂª∫ÔºåÁßâÊåÅÁî®Êà∑Êã•ÊúâÁöÑÁêÜÂøµ„ÄÇJan ÂÖÅËÆ∏ÊÇ®Âú®ËÆæÂ§á‰∏äËøêË°åÊµÅË°åÁöÑÊ®°ÂûãÔºåÂ¶Ç [Mistral](https://huggingface.co/models?other=mistral) Êàñ [Llama](https://huggingface.co/models?other=llama)ÔºåËÄåÊó†ÈúÄËøûÊé•‰∫íËÅîÁΩë„ÄÇ‰ΩøÁî® JanÔºåÊÇ®ÂèØ‰ª•ËÆøÈóÆËøúÁ®ã APIÔºåÂ¶Ç OpenAI Âíå [Groq](https://groq.com/)„ÄÇ\n\n## Jan ÁöÑ‰∏ªË¶ÅÁâπÁÇπ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*ufyOE6QkcHw8X5U7)\n\nJan ÊòØ‰∏ÄÊ¨æÁîµÂ≠êÂ∫îÁî®Á®ãÂ∫èÔºåÂÖ∂ÂäüËÉΩÁ±ª‰ºº‰∫é LM Studio„ÄÇÂÆÉÈÄöËøáÂ∞ÜÊ∂àË¥πËÄÖËÆæÂ§áËΩ¨Âèò‰∏∫ AI ËÆ°ÁÆóÊú∫Ôºå‰Ωø AI ÂèòÂæóÂºÄÊîæÂíåÂèØËÆøÈóÆ„ÄÇÁî±‰∫éËøôÊòØ‰∏Ä‰∏™ÂºÄÊ∫êÈ°πÁõÆÔºåÂºÄÂèëËÄÖÂèØ‰ª•‰∏∫ÂÖ∂Ë¥°ÁåÆ‰ª£Á†ÅÂπ∂Êâ©Â±ïÂÖ∂ÂäüËÉΩ„ÄÇ‰ª•‰∏ãÊòØ Jan ÁöÑ‰∏ªË¶ÅÁâπÁÇπ„ÄÇ\n\n* **Êú¨Âú∞**ÔºöÊÇ®ÂèØ‰ª•Âú®ËÆæÂ§á‰∏äËøêË°åÊÇ®ÂñúÊ¨¢ÁöÑ AI Ê®°ÂûãÔºåËÄåÊó†ÈúÄÂ∞ÜÂÖ∂ËøûÊé•Âà∞‰∫íËÅîÁΩë„ÄÇ\n* **Âç≥Áî®Ê®°Âûã**Ôºö‰∏ãËΩΩ Jan ÂêéÔºåÊÇ®Â∞ÜËé∑Âæó‰∏ÄÁªÑÂ∑≤ÂÆâË£ÖÁöÑÊ®°Âûã‰ª•‰æõÂºÄÂßã‰ΩøÁî®„ÄÇ‰πüÂèØ‰ª•ÊêúÁ¥¢ÁâπÂÆöÊ®°Âûã„ÄÇ\n* **Ê®°ÂûãÂØºÂÖ•**ÔºöÊîØÊåÅ‰ªé Hugging Face Á≠âÊù•Ê∫êÂØºÂÖ•Ê®°Âûã„ÄÇ\n* **ÂÖçË¥π„ÄÅË∑®Âπ≥Âè∞ÂíåÂºÄÊ∫ê**ÔºöJan ÂÆåÂÖ®ÂÖçË¥πÔºåÂºÄÊ∫êÔºåÂπ∂ÂèØÂú® Mac„ÄÅWindows Âíå Linux ‰∏äËøêË°å„ÄÇ\n* **Ëá™ÂÆö‰πâÊé®ÁêÜÂèÇÊï∞**ÔºöË∞ÉÊï¥Ê®°ÂûãÂèÇÊï∞ÔºåÂ¶ÇÊúÄÂ§ß‰ª§Áâå„ÄÅÊ∏©Â∫¶„ÄÅÊµÅ„ÄÅÈ¢ëÁéáÊÉ©ÁΩöÁ≠â„ÄÇÊâÄÊúâÂÅèÂ•ΩËÆæÁΩÆ„ÄÅÊ®°Âûã‰ΩøÁî®ÂíåÈÖçÁΩÆÈÉΩ‰øùÁïôÂú®ÊÇ®ÁöÑËÆ°ÁÆóÊú∫‰∏ä„ÄÇ\n* **Êâ©Â±ï**ÔºöJan ÊîØÊåÅ [TensortRT](https://github.com/NVIDIA/TensorRT) Âíå [Inference Nitro](https://huggingface.co/jan-hq/nitro-v1.2-e3) Á≠âÊâ©Â±ïÔºå‰ª•Ëá™ÂÆö‰πâÂíåÂ¢ûÂº∫ÊÇ®ÁöÑ AI Ê®°Âûã„ÄÇ\n\n## ‰ΩøÁî® Jan ÁöÑÂ•ΩÂ§Ñ\n\nJan Êèê‰æõ‰∫Ü‰∏Ä‰∏™Âπ≤ÂáÄÁÆÄÂçïÁöÑÁïåÈù¢Êù•‰∏é LLM ‰∫íÂä®ÔºåÂπ∂‰∏îÂ∞ÜÊâÄÊúâÊï∞ÊçÆÂíåÂ§ÑÁêÜ‰ø°ÊÅØ‰øùÂ≠òÂú®Êú¨Âú∞„ÄÇÂÆÉÂ∑≤Áªè‰∏∫ÊÇ®ÂÆâË£Ö‰∫ÜË∂ÖËøá‰∏ÉÂçÅ‰∏™Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰æõÊÇ®‰ΩøÁî®„ÄÇËøô‰∫õÁé∞ÊàêÁöÑÊ®°ÂûãÁöÑÂèØÁî®ÊÄß‰ΩøÂæóËøûÊé•Âíå‰∏éËøúÁ®ã APIÔºàÂ¶Ç OpenAI Âíå MistralÔºâ‰∫íÂä®ÂèòÂæóÁÆÄÂçï„ÄÇJan ËøòÊúâ‰∏Ä‰∏™ÂæàÊ£íÁöÑ [GitHub](https://github.com/janhq/jan)„ÄÅ[Discord](https://discord.gg/FTk2MvZwJH) Âíå [Hugging Face](https://huggingface.co/janhq) Á§æÂå∫ÔºåÂèØ‰ª•ÂÖ≥Ê≥®Âπ∂ÂØªÊ±ÇÂ∏ÆÂä©„ÄÇÁÑ∂ËÄåÔºåÂÉèÊâÄÊúâ LLM Â∑•ÂÖ∑‰∏ÄÊ†∑ÔºåËøô‰∫õÊ®°ÂûãÂú® Apple Silicon Macs ‰∏äÁöÑËøêË°åÈÄüÂ∫¶ÊØîÂú® Intel Êú∫Âô®‰∏äÊõ¥Âø´„ÄÇ\n\n## 3. Llamafile\n\n[Llamafile](https://github.com/Mozilla-Ocho/llamafile) Áî± [Mozilla](https://www.mozilla.org/en-US/?v=1) ÊîØÊåÅÔºåÊó®Âú®ÈÄöËøáÂø´ÈÄüÁöÑ [CPU Êé®ÁêÜ](https://huggingface.co/docs/transformers/en/perf_infer_cpu) ‰ΩøÂºÄÊ∫ê AI ÂØπÊØè‰∏™‰∫∫ÈÉΩÂèØËÆøÈóÆÔºåËÄåÊó†ÈúÄÁΩëÁªúËøûÊé•„ÄÇÂÆÉÂ∞Ü LLM ËΩ¨Êç¢‰∏∫Â§öÂπ≥Âè∞ÁöÑ [ÂèØÊâßË°åÈìæÊé•Ê†ºÂºè](https://gist.github.com/x0nu11byt3/bcb35c3de461e5fb66173071a2379779) (ELF)„ÄÇÂÆÉÊèê‰æõ‰∫ÜÂ∞Ü AI [ÈõÜÊàê](https://getstream.io/chat/solutions/ai-integration/) Âà∞Â∫îÁî®Á®ãÂ∫è‰∏≠ÁöÑÊúÄ‰Ω≥ÈÄâÈ°π‰πã‰∏ÄÔºå‰ΩøÊÇ®ËÉΩÂ§ü‰ªÖÈÄöËøá‰∏Ä‰∏™ÂèØÊâßË°åÊñá‰ª∂ËøêË°å LLM„ÄÇ\n\n## Llamafile ÁöÑÂ∑•‰ΩúÂéüÁêÜ\n\nÂÆÉÊó®Âú®Â∞ÜÊùÉÈáçËΩ¨Êç¢‰∏∫Â§ö‰∏™ÂèØÊâßË°åÁ®ãÂ∫èÔºåËøô‰∫õÁ®ãÂ∫èÊó†ÈúÄÂÆâË£ÖÂç≥ÂèØÂú® Windows„ÄÅMacOS„ÄÅLinux„ÄÅIntel„ÄÅARM„ÄÅFreeBSD Á≠âÊû∂ÊûÑ‰∏äËøêË°å„ÄÇÂú®Â∫ïÂ±ÇÔºåLlamafile ‰ΩøÁî® [tinyBLAST](https://github.com/ggerganov/llama.cpp/issues/5048) Âú®ÂÉè Windows ËøôÊ†∑ÁöÑÊìç‰ΩúÁ≥ªÁªü‰∏äËøêË°åÔºåËÄåÊó†ÈúÄ SDK„ÄÇ\n\n## Llamafile ÁöÑÂÖ≥ÈîÆÁâπÊÄß\n\n* **ÂèØÊâßË°åÊñá‰ª∂**Ôºö‰∏é LM Studio Âíå Jan Á≠âÂÖ∂‰ªñ LLM Â∑•ÂÖ∑‰∏çÂêåÔºåLlamafile Âè™ÈúÄ‰∏Ä‰∏™ÂèØÊâßË°åÊñá‰ª∂Âç≥ÂèØËøêË°å LLM„ÄÇ\n* **‰ΩøÁî®Áé∞ÊúâÊ®°Âûã**ÔºöLlamafile ÊîØÊåÅ‰ΩøÁî®Áé∞ÊúâÁöÑÊ®°ÂûãÂ∑•ÂÖ∑ÔºåÂ¶Ç Ollama Âíå LM Studio„ÄÇ\n* **ËÆøÈóÆÊàñÂàõÂª∫Ê®°Âûã**ÔºöÊÇ®ÂèØ‰ª•ËÆøÈóÆ OpenAI„ÄÅMistral„ÄÅGroq Á≠âÊµÅË°å LLM„ÄÇÂÆÉËøòÊèê‰æõ‰ªéÂ§¥ÂàõÂª∫Ê®°ÂûãÁöÑÊîØÊåÅ„ÄÇ\n* **Ê®°ÂûãÊñá‰ª∂ËΩ¨Êç¢**ÔºöÊÇ®ÂèØ‰ª•Â∞ÜËÆ∏Â§öÊµÅË°å LLM ÁöÑÊñá‰ª∂Ê†ºÂºèËΩ¨Êç¢Ôºå‰æãÂ¶ÇÔºåÂ∞Ü `.gguf` ËΩ¨Êç¢‰∏∫ `.llamafile` Âè™ÈúÄ‰∏Ä‰∏™ÂëΩ‰ª§„ÄÇ\n\n`llamafile-convert mistral-7b.gguf`\n\n## ÂºÄÂßã‰ΩøÁî® Llamafile\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*4PV1KsCZvvVKqFll)\n\nË¶ÅÂÆâË£Ö LlamafileÔºåËØ∑ËÆøÈóÆ Huggingface ÁΩëÁ´ôÔºå‰ªéÂØºËà™‰∏≠ÈÄâÊã© **Models**ÔºåÁÑ∂ÂêéÊêúÁ¥¢ **Llamafile**„ÄÇÊÇ®ËøòÂèØ‰ª•‰ªé‰∏ãÈù¢ÁöÑ URL ÂÆâË£ÖÊÇ®ÂñúÊ¨¢ÁöÑ [ÈáèÂåñ](https://huggingface.co/docs/optimum/en/concept_guides/quantization) ÁâàÊú¨„ÄÇ\n\n[`https://huggingface.co/Mozilla/Meta-Llama-3.1-8B-Instruct-llamafile/tree/m`ain](https://huggingface.co/Mozilla/Meta-Llama-3.1-8B-Instruct-llamafile/tree/main)\n\n**Ê≥®ÊÑè**ÔºöÈáèÂåñÊï∞Â≠óË∂äÂ§ßÔºåÂìçÂ∫îË∂äÂ•Ω„ÄÇÊ≠£Â¶Ç‰∏äÂõæÊâÄÁ§∫ÔºåÊú¨Êñá‰ΩøÁî® `Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile`ÔºåÂÖ∂‰∏≠ `Q6` ‰ª£Ë°®ÈáèÂåñÊï∞Â≠ó„ÄÇ\n\n**Ê≠•È™§ 1Ôºö‰∏ãËΩΩ Llamafile**\n\n‰ªé‰∏äÈù¢ÁöÑÈìæÊé•ÔºåÁÇπÂáª‰ªªÊÑè‰∏ãËΩΩÊåâÈíÆ‰ª•Ëé∑ÂèñÊÇ®ÂñúÊ¨¢ÁöÑÁâàÊú¨„ÄÇÂ¶ÇÊûúÊÇ®Âú®Êú∫Âô®‰∏äÂÆâË£Ö‰∫Ü [wget](https://www.gnu.org/software/wget/) Â∑•ÂÖ∑ÔºåÊÇ®ÂèØ‰ª•‰ΩøÁî®‰ª•‰∏ãÂëΩ‰ª§‰∏ãËΩΩ Llamafile„ÄÇ\n\n`wget <https://huggingface.co/Mozilla/Meta-Llama-3.1-8B-Instruct-llamafile/blob/main/Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile>`\n\nÊÇ®Â∫îËØ•Áî®ÊÇ®ÂñúÊ¨¢ÁöÑÁâàÊú¨ÊõøÊç¢ URL„ÄÇ\n\n**Ê≠•È™§ 2Ôºö‰Ωø Llamafile ÂèØÊâßË°å**\n\n‰∏ãËΩΩÁâπÂÆöÁâàÊú¨ÁöÑ Llamafile ÂêéÔºåÊÇ®Â∫îËØ•ÈÄöËøáÂØºËà™Âà∞Êñá‰ª∂‰ΩçÁΩÆÔºå‰ΩøÁî®‰ª•‰∏ãÂëΩ‰ª§‰ΩøÂÖ∂ÂèØÊâßË°å„ÄÇ\n\n`chmod +x Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile`**Ê≠•È™§ 3ÔºöËøêË°å Llamafile**\n\nÂú®Êñá‰ª∂Âêç‰πãÂâçÊ∑ªÂä†‰∏Ä‰∏™ÁÇπÂíåÊñúÊù† `./` Êù•ÂêØÂä® Llamafile„ÄÇ\n\n`./Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile`\n\nLlamafile Â∫îÁî®Á®ãÂ∫èÁé∞Âú®Â∞ÜÂú® `http://127.0.0.1:8080` ÂèØÁî®Ôºå‰ª•ËøêË°åÊÇ®ÁöÑÂêÑÁßç LLMs„ÄÇ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*1xrwDPTfNgmEQDTx)\n\n## ‰ΩøÁî® Llamafile ÁöÑÂ•ΩÂ§Ñ\n\nLlamafile ÈÄöËøá‰Ωø LLM ÂÆπÊòìË¢´Ê∂àË¥πËÄÖ CPU ËÆøÈóÆÔºåÂ∏ÆÂä©ÂÆûÁé∞ AI Âíå ML ÁöÑÊ∞ë‰∏ªÂåñ„ÄÇ‰∏éÂÖ∂‰ªñÊú¨Âú∞ LLM Â∫îÁî®Á®ãÂ∫èÂ¶Ç **Llama.cpp** Áõ∏ÊØîÔºåLlamafile Êèê‰æõ‰∫ÜÊúÄÂø´ÁöÑÊèêÁ§∫Â§ÑÁêÜ‰ΩìÈ™åÔºåÂπ∂Âú®Ê∏∏ÊàèÁîµËÑë‰∏äË°®Áé∞Êõ¥‰Ω≥„ÄÇÁî±‰∫éÂÖ∂Êõ¥Âø´ÁöÑÊÄßËÉΩÔºåÂÆÉÊòØÊÄªÁªìÈïøÊñáÊú¨ÂíåÂ§ßÂûãÊñáÊ°£ÁöÑÁªù‰Ω≥ÈÄâÊã©„ÄÇÂÆÉÂÆåÂÖ®Á¶ªÁ∫øËøêË°åÂπ∂‰øùÊä§ÈöêÁßÅÔºåÂõ†Ê≠§Áî®Êà∑‰∏ç‰ºöÂ∞ÜÊï∞ÊçÆÂàÜ‰∫´Áªô‰ªª‰Ωï AI ÊúçÂä°Âô®Êàñ API„ÄÇÂÉè Hugging Face ËøôÊ†∑ÁöÑÊú∫Âô®Â≠¶‰π†Á§æÂå∫ÊîØÊåÅ Llamafile Ê†ºÂºèÔºå‰ΩøÂæóÊêúÁ¥¢‰∏é Llamafile Áõ∏ÂÖ≥ÁöÑÊ®°ÂûãÂèòÂæóÂÆπÊòì„ÄÇÂÆÉËøòÊúâ‰∏Ä‰∏™Âá∫Ëâ≤ÁöÑÂºÄÊ∫êÁ§æÂå∫ÔºåËøõ‰∏ÄÊ≠•ÂºÄÂèëÂíåÊâ©Â±ïÂÆÉ„ÄÇ\n\n## 4. GPT4ALL\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*j3vNWWQZCVF5woo5)\n\nGPT4ALL Âü∫‰∫éÈöêÁßÅ„ÄÅÂÆâÂÖ®ÂíåÊó†ÈúÄ‰∫íËÅîÁΩëÁöÑÂéüÂàôÊûÑÂª∫„ÄÇÁî®Êà∑ÂèØ‰ª•Âú® Mac„ÄÅWindows Âíå Ubuntu ‰∏ä [ÂÆâË£Ö](https://www.nomic.ai/gpt4all)„ÄÇ‰∏é Jan Êàñ LM Studio Áõ∏ÊØîÔºåGPT4ALL Êã•ÊúâÊõ¥Â§öÁöÑÊØèÊúà‰∏ãËΩΩÈáè„ÄÅ[GitHub Stars](https://github.com/nomic-ai/gpt4all) ÂíåÊ¥ªË∑ÉÁî®Êà∑„ÄÇ\n\n## GPT4ALLÁöÑ‰∏ªË¶ÅÁâπÁÇπ\n\nGPT4AllÂèØ‰ª•Âú®‰∏ªË¶ÅÊ∂àË¥πÁ°¨‰ª∂‰∏äËøêË°åLLMÔºå‰æãÂ¶ÇMac MÁ≥ªÂàóËäØÁâá„ÄÅAMDÂíåNVIDIA GPU„ÄÇ‰ª•‰∏ãÊòØÂÖ∂‰∏ªË¶ÅÁâπÁÇπ„ÄÇ\n\n* **ÈöêÁßÅ‰ºòÂÖà**ÔºöÂ∞ÜÁßÅ‰∫∫ÂíåÊïèÊÑüÁöÑËÅäÂ§©‰ø°ÊÅØÂíåÊèêÁ§∫‰ªÖ‰øùÁïôÂú®ÊÇ®ÁöÑËÆæÂ§á‰∏ä„ÄÇ\n* **Êó†ÈúÄ‰∫íËÅîÁΩë**ÔºöÂÆÉÂÆåÂÖ®Á¶ªÁ∫øÂ∑•‰Ωú„ÄÇ\n* **Ê®°ÂûãÊé¢Á¥¢**ÔºöÊ≠§ÂäüËÉΩÂÖÅËÆ∏ÂºÄÂèëËÄÖÊµèËßàÂíå‰∏ãËΩΩ‰∏çÂêåÁ±ªÂûãÁöÑLLMËøõË°åÂÆûÈ™å„ÄÇÊÇ®ÂèØ‰ª•‰ªéÊµÅË°åÈÄâÈ°π‰∏≠ÈÄâÊã©Â§ßÁ∫¶1000‰∏™ÂºÄÊ∫êËØ≠Ë®ÄÊ®°ÂûãÔºåÂ¶ÇLLama„ÄÅMistralÁ≠â„ÄÇ\n* **Êú¨Âú∞ÊñáÊ°£**ÔºöÊÇ®ÂèØ‰ª•ËÆ©Êú¨Âú∞LLMËÆøÈóÆÊÇ®ÁöÑÊïèÊÑüÊï∞ÊçÆÔºå‰ΩøÁî®Êú¨Âú∞ÊñáÊ°£Â¶Ç`.pdf`Âíå`.txt`ÔºåÊï∞ÊçÆ‰∏ç‰ºöÁ¶ªÂºÄÊÇ®ÁöÑËÆæÂ§áÔºå‰πüÊó†ÈúÄÁΩëÁªú„ÄÇ\n* **Ëá™ÂÆö‰πâÈÄâÈ°π**ÔºöÂÆÉÊèê‰æõÂ§ö‰∏™[ËÅäÂ§©Êú∫Âô®‰∫∫](https://getstream.io/blog/llm-chatbot-docs/)Ë∞ÉÊï¥ÈÄâÈ°πÔºåÂ¶ÇÊ∏©Â∫¶„ÄÅÊâπÂ§ÑÁêÜÂ§ßÂ∞è„ÄÅ‰∏ä‰∏ãÊñáÈïøÂ∫¶Á≠â„ÄÇ\n* **‰ºÅ‰∏öÁâà**ÔºöGPT4ALLÊèê‰æõ‰ºÅ‰∏öÂ•óÈ§êÔºåÂÖ∑Â§áÂÆâÂÖ®ÊÄß„ÄÅÊîØÊåÅÂíåÊØèÂè∞ËÆæÂ§áÁöÑËÆ∏ÂèØËØÅÔºåÂ∞ÜÊú¨Âú∞AIÂ∏¶ÂÖ•‰ºÅ‰∏ö„ÄÇ\n\n## ÂºÄÂßã‰ΩøÁî® GPT4All\n\nË¶ÅÂºÄÂßã‰ΩøÁî® GPT4All Âú®Êú¨Âú∞ËøêË°å LLMsÔºåËØ∑[‰∏ãËΩΩ](https://www.nomic.ai/gpt4all)ÈÄÇÂêàÊÇ®Êìç‰ΩúÁ≥ªÁªüÁöÑÁâàÊú¨„ÄÇ\n\n## ‰ΩøÁî®GPT4ALLÁöÑÂ•ΩÂ§Ñ\n\nÈô§‰∫ÜOllamaÔºåGPT4ALLÂú®GitHubË¥°ÁåÆËÄÖÊï∞Èáè‰∏äÊúÄ‰∏∫ÊòæËëóÔºåÊã•ÊúâÁ∫¶250000ÂêçÊØèÊúàÊ¥ªË∑ÉÁî®Êà∑ÔºàÊ†πÊçÆ<https://www.nomic.ai/gpt4all>ÔºâÂπ∂‰∏î‰∏éÂÖ∂Á´û‰∫âÂØπÊâãÁõ∏ÊØî„ÄÇËØ•Â∫îÁî®Êî∂ÈõÜÊúâÂÖ≥‰ΩøÁî®ÂàÜÊûêÂíåËÅäÂ§©ÂàÜ‰∫´ÁöÑÂåøÂêçÁî®Êà∑Êï∞ÊçÆ„ÄÇÁÑ∂ËÄåÔºåÁî®Êà∑ÂèØ‰ª•ÈÄâÊã©Âä†ÂÖ•ÊàñÈÄÄÂá∫„ÄÇ‰ΩøÁî®GPT4ALLÔºåÂºÄÂèëËÄÖÂèØ‰ª•‰ªéÂÖ∂Â∫ûÂ§ßÁöÑÁî®Êà∑Âü∫Á°Ä„ÄÅGitHubÂíåDiscordÁ§æÂå∫‰∏≠ÂèóÁõä„ÄÇ\n\n## 5. Ollama\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*STAonWgWIsY6cgDR)\n\n‰ΩøÁî® [Ollama](https://ollama.com/)ÔºåÊÇ®ÂèØ‰ª•ËΩªÊùæÂàõÂª∫Êú¨Âú∞ËÅäÂ§©Êú∫Âô®‰∫∫ÔºåËÄåÊó†ÈúÄËøûÊé•Âà∞ÂÉè OpenAI ËøôÊ†∑ÁöÑ API„ÄÇÁî±‰∫é‰∏ÄÂàáÈÉΩÂú®Êú¨Âú∞ËøêË°åÔºåÊÇ®Êó†ÈúÄÊîØ‰ªò‰ªª‰ΩïËÆ¢ÈòÖË¥πÊàñ API Ë∞ÉÁî®Ë¥πÁî®„ÄÇ\n\n## Ollama ÁöÑÂÖ≥ÈîÆÁâπÊÄß\n\n* **Ê®°ÂûãËá™ÂÆö‰πâ**ÔºöOllama ÂÖÅËÆ∏ÊÇ®ËΩ¨Êç¢ `.gguf` Ê®°ÂûãÊñá‰ª∂Âπ∂‰ΩøÁî® `ollama run modelname` ËøêË°åÂÆÉ‰ª¨„ÄÇ\n* **Ê®°ÂûãÂ∫ì**ÔºöOllama Êã•ÊúâÂ§ßÈáèÊ®°ÂûãÂèØ‰æõÂ∞ùËØïÔºåËÆøÈóÆ [ollama.com/library](https://ollama.com/library)„ÄÇ\n* **ÂØºÂÖ•Ê®°Âûã**ÔºöOllama ÊîØÊåÅ‰ªé [PyTorch](https://pytorch.org/) ÂØºÂÖ•Ê®°Âûã„ÄÇ\n* **Á§æÂå∫ÈõÜÊàê**ÔºöOllama Êó†ÁºùÈõÜÊàêÂà∞ÁΩëÈ°µÂíåÊ°åÈù¢Â∫îÁî®Á®ãÂ∫è‰∏≠Ôºå‰æãÂ¶Ç [Ollama-SwiftUI](https://github.com/kghandour/Ollama-SwiftUI)„ÄÅ[HTML UI](https://github.com/rtcfirefly/ollama-ui)„ÄÅ[Dify.ai](https://github.com/rtcfirefly/ollama-ui) Âíå [Êõ¥Â§ö](https://github.com/ollama/ollama?tab=readme-ov-file#community-integrations)„ÄÇ\n* **Êï∞ÊçÆÂ∫ìËøûÊé•**ÔºöOllama ÊîØÊåÅÂ§ö‰∏™ [Êï∞ÊçÆÂπ≥Âè∞](https://github.com/mindsdb/mindsdb/blob/main/mindsdb/integrations/handlers/ollama_handler/README.md)„ÄÇ\n* **ÁßªÂä®ÈõÜÊàê**ÔºöÂÉè [Enchanted](https://github.com/AugustDev/enchanted) ËøôÊ†∑ÁöÑ SwiftUI Â∫îÁî®Â∞Ü Ollama Â∏¶ÂÖ• iOS„ÄÅmacOS Âíå visionOS„ÄÇ[Maid](https://github.com/Mobile-Artificial-Intelligence/maid) ‰πüÊòØ‰∏Ä‰∏™Ë∑®Âπ≥Âè∞ÁöÑ Flutter Â∫îÁî®ÔºåËÉΩÂ§üÊú¨Âú∞Â§ÑÁêÜ `.gguf` Ê®°ÂûãÊñá‰ª∂„ÄÇ\n\n## ÂºÄÂßã‰ΩøÁî® Ollama\n\nË¶ÅÈ¶ñÊ¨°‰ΩøÁî® OllamaÔºåËØ∑ËÆøÈóÆ <https://ollama.com> Âπ∂‰∏ãËΩΩÈÄÇÂêàÊÇ®Êú∫Âô®ÁöÑÁâàÊú¨„ÄÇÊÇ®ÂèØ‰ª•Âú® Mac„ÄÅLinux Êàñ Windows ‰∏äÂÆâË£ÖÂÆÉ„ÄÇÂÆâË£Ö Ollama ÂêéÔºåÊÇ®ÂèØ‰ª•Âú®ÁªàÁ´Ø‰∏≠‰ΩøÁî®‰ª•‰∏ãÂëΩ‰ª§Ê£ÄÊü•ÂÖ∂ËØ¶ÁªÜ‰ø°ÊÅØ„ÄÇ\n\n`ollama`\n\nË¶ÅËøêË°åÁâπÂÆöÁöÑ LLMÔºåÊÇ®Â∫îËØ•‰ΩøÁî®‰ª•‰∏ãÂëΩ‰ª§‰∏ãËΩΩÂÆÉÔºö\n\n`ollama pull modelname`ÔºåÂÖ∂‰∏≠ `modelname` ÊòØÊÇ®Ë¶ÅÂÆâË£ÖÁöÑÊ®°ÂûãÂêçÁß∞„ÄÇËØ∑Âú® [GitHub](https://github.com/ollama/ollama) ‰∏äÊü•Áúã‰∏Ä‰∫õÂèØ‰æõ‰∏ãËΩΩÁöÑÁ§∫‰æãÊ®°Âûã„ÄÇ`pull` ÂëΩ‰ª§‰πüÁî®‰∫éÊõ¥Êñ∞Ê®°Âûã„ÄÇ‰∏ÄÊó¶‰ΩøÁî®Ôºå‰ªÖ‰ºöËé∑ÂèñÂ∑ÆÂºÇÈÉ®ÂàÜ„ÄÇ\n\n‰æãÂ¶ÇÔºåÂú®‰∏ãËΩΩ‰∫Ü `llama3.1` ÂêéÔºåÂú®ÂëΩ‰ª§Ë°å‰∏≠ËøêË°å `ollama run llama3.1` Â∞ÜÂêØÂä®ËØ•Ê®°Âûã„ÄÇ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*aglZm6h0BU6GAYkSl04XWA.gif)\n\nÂú®‰∏äËø∞Á§∫‰æã‰∏≠ÔºåÊàë‰ª¨ÊèêÁ§∫ `llama3.1` Ê®°ÂûãËß£ÂÜ≥‰∏Ä‰∏™Áâ©ÁêÜÂäüÂíåËÉΩÈáèÁöÑÈóÆÈ¢ò„ÄÇ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*dNNQYpz1s2tz1pcn)\n\n## ‰ΩøÁî® Ollama ÁöÑÂ•ΩÂ§Ñ\n\nOllama Âú® GitHub ‰∏äÊã•ÊúâË∂ÖËøá 200 ÂêçË¥°ÁåÆËÄÖÔºåÂπ∂‰∏îÊúâÊ¥ªË∑ÉÁöÑÊõ¥Êñ∞„ÄÇÂÆÉÊã•ÊúâÊúÄÂ§öÁöÑË¥°ÁåÆËÄÖÔºåÂπ∂‰∏îÂú®‰∏äËø∞ÂÖ∂‰ªñÂºÄÊ∫ê LLM Â∑•ÂÖ∑‰∏≠Êõ¥ÂÖ∑ÂèØÊâ©Â±ïÊÄß„ÄÇ\n\n## 6. LLaMa.cpp\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*KhsAUquhDZAHghxK)\n\n[LLaMa.cpp](https://github.com/ggerganov/llama.cpp) ÊòØÊîØÊåÅÊú¨Âú∞ LLM Â∑•ÂÖ∑ÔºàÂ¶Ç Ollama Á≠âÔºâÁöÑÂ∫ïÂ±ÇÂêéÁ´ØÊäÄÊúØÔºàÊé®ÁêÜÂºïÊìéÔºâ„ÄÇLLaMa.cpp ÊîØÊåÅÊòæËëóÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜÔºåÈÖçÁΩÆÁÆÄÂçïÔºåÂπ∂Âú®ÂêÑÁßçÁ°¨‰ª∂‰∏äÊèê‰æõÂá∫Ëâ≤ÁöÑÊú¨Âú∞ÊÄßËÉΩ„ÄÇÂÆÉ‰πüÂèØ‰ª•Âú®‰∫ëÁ´ØËøêË°å„ÄÇ\n\n## LLaMa.cpp ÁöÑ‰∏ªË¶ÅÁâπÁÇπ\n\n* **ËÆæÁΩÆ**ÔºöÂÆÉÁöÑËÆæÁΩÆÈùûÂ∏∏ÁÆÄÂçï„ÄÇÊÇ®Âè™ÈúÄ‰∏Ä‰∏™ÂëΩ‰ª§Âç≥ÂèØÂÆâË£Ö„ÄÇ\n* **ÊÄßËÉΩ**ÔºöÂÆÉÂú®Êú¨Âú∞Âíå‰∫ëÁ´ØÁöÑÂêÑÁßçÁ°¨‰ª∂‰∏äË°®Áé∞ÈùûÂ∏∏Âá∫Ëâ≤„ÄÇ\n* **ÊîØÊåÅÁöÑÊ®°Âûã**ÔºöÂÆÉÊîØÊåÅÊµÅË°åÁöÑ‰∏ªË¶Å LLMÔºåÂ¶Ç [Mistral 7B](https://huggingface.co/mistralai/Mistral-7B-v0.1)„ÄÅ[Mixtral MoE](https://huggingface.co/models?search=mistral-ai/Mixtral)„ÄÅ[DBRX](https://huggingface.co/databricks/dbrx-instruct)„ÄÅ[Falcon](https://huggingface.co/models?search=tiiuae/falcon) Âíå [ÂÖ∂‰ªñËÆ∏Â§öÊ®°Âûã](https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#description)„ÄÇ\n* **ÂâçÁ´Ø AI Â∑•ÂÖ∑**ÔºöLLaMa.cpp ÊîØÊåÅÂºÄÊ∫ê LLM UI Â∑•ÂÖ∑ÔºåÂ¶Ç [MindWorkAI/AI-Studio](https://github.com/MindWorkAI/AI-Studio) (FSL-1.1-MIT)„ÄÅ[iohub/collama](https://github.com/iohub/coLLaMA) Á≠â„ÄÇ\n\n## ‰ΩøÁî® LLaMa.cpp ÂºÄÂßã\n\nË¶ÅËøêË°åÊÇ®ÁöÑÁ¨¨‰∏Ä‰∏™Êú¨Âú∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºåËØ∑‰ΩøÁî®‰ª•‰∏ãÂëΩ‰ª§ÂÆâË£Ö llama.cppÔºö\n\n`brew install llama.cpp`\n\nÊé•‰∏ãÊù•Ôºå‰ªé Hugging Face ÊàñÂÖ∂‰ªñÊù•Ê∫ê‰∏ãËΩΩÊÇ®ÊÉ≥Ë¶ÅËøêË°åÁöÑÊ®°Âûã„ÄÇ‰æãÂ¶ÇÔºå‰ªé Hugging Face ‰∏ãËΩΩ‰∏ãÈù¢ÁöÑÊ®°ÂûãÂπ∂Â∞ÜÂÖ∂‰øùÂ≠òÂú®ÊÇ®ËÆ°ÁÆóÊú∫‰∏äÁöÑÊüê‰∏™‰ΩçÁΩÆ„ÄÇ\n\n[`https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q4_K_M.g`guf](https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf)\n\n‰ΩøÁî®ÊÇ®ÂñúÊ¨¢ÁöÑÂëΩ‰ª§Ë°åÂ∑•ÂÖ∑ÔºåÂ¶ÇÁªàÁ´ØÔºå`cd` ËøõÂÖ•ÊÇ®Âàö‰∏ãËΩΩÁöÑ `.gguf` Ê®°ÂûãÊñá‰ª∂ÁöÑ‰ΩçÁΩÆÔºåÂπ∂ËøêË°å‰ª•‰∏ãÂëΩ‰ª§„ÄÇ\n\n```python\nllama-cli --color \\ \n-m Mistral-7B-Instruct-v0.3.Q4_K_M.ggufb \\ \n-p \"Write a short intro about SwiftUI\"\n```\nÊÄª‰πãÔºåÊÇ®È¶ñÂÖàË∞ÉÁî® LLaMa CLI Â∑•ÂÖ∑Âπ∂ËÆæÁΩÆÈ¢úËâ≤ÂíåÂÖ∂‰ªñÊ†áÂøó„ÄÇ`-m` Ê†áÂøóÊåáÂÆöÊÇ®Ë¶Å‰ΩøÁî®ÁöÑÊ®°ÂûãÁöÑË∑ØÂæÑ„ÄÇ`-p` Ê†áÂøóÊåáÂÆöÊÇ®Â∏åÊúõÁî®Êù•ÊåáÁ§∫Ê®°ÂûãÁöÑÊèêÁ§∫„ÄÇ\n\nËøêË°å‰∏äËø∞ÂëΩ‰ª§ÂêéÔºåÊÇ®Â∞ÜÁúãÂà∞‰ª•‰∏ãÈ¢ÑËßà‰∏≠ÁöÑÁªìÊûú„ÄÇ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*4Al-j50vXUXLUfvxzBt6aw.gif)\n\n## Êú¨Âú∞ LLM ÁöÑÁî®‰æã\n\nÂú®Êú¨Âú∞ËøêË°å LLM ÂèØ‰ª•Â∏ÆÂä©ÂºÄÂèë‰∫∫ÂëòÊ∑±ÂÖ•‰∫ÜËß£ÂÖ∂ÊÄßËÉΩÂíåÂ∑•‰ΩúÂéüÁêÜ„ÄÇ Êú¨Âú∞ LLM ÂèØ‰ª•Êü•ËØ¢ÁßÅÊúâÊñáÊ°£ÂíåÊäÄÊúØËÆ∫ÊñáÔºå‰ª•‰æø‰∏éËøô‰∫õÊñáÊ°£Áõ∏ÂÖ≥ÁöÑ‰ø°ÊÅØ‰∏ç‰ºöÁ¶ªÂºÄÁî®‰∫éÊü•ËØ¢ÁöÑËÆæÂ§áÔºå‰∏ç‰ºöÂèëÈÄÅÂà∞‰ªª‰Ωï‰∫ë AI API„ÄÇ Êú¨Âú∞ LLM Âú®Ê≤°Êúâ‰∫íËÅîÁΩëÁöÑÂú∞ÊñπÂíåÁΩëÁªú‰ø°Âè∑ËæÉÂ∑ÆÁöÑÂú∞ÊñπÈùûÂ∏∏ÊúâÁî®„ÄÇ\n\nÂú® [ËøúÁ®ãÂåªÁñóÁéØÂ¢É](https://getstream.io/blog/telemedicine-app-development/) ‰∏≠ÔºåÊú¨Âú∞ LLM ÂèØ‰ª•ÂØπÊÇ£ËÄÖÊñáÊ°£ËøõË°åÊéíÂ∫èÔºåËÄåÊó†ÈúÄÂá∫‰∫éÈöêÁßÅËÄÉËôëÂ∞ÜÂÖ∂‰∏ä‰º†Âà∞‰ªª‰Ωï AI API Êèê‰æõÂïÜ„ÄÇ\n\n## ËØÑ‰º∞Êú¨Âú∞ËøêË°åÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊÄßËÉΩ\n\nÂú®Êú¨Âú∞‰ΩøÁî®Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰πãÂâçÔºå‰∫ÜËß£ÂÖ∂ÊÄßËÉΩÂØπ‰∫éËé∑ÂæóÊâÄÈúÄÁöÑÂìçÂ∫îËá≥ÂÖ≥ÈáçË¶Å„ÄÇÊúâÂá†ÁßçÊñπÊ≥ïÂèØ‰ª•Á°ÆÂÆöÁâπÂÆö LLM ÁöÑÊÄßËÉΩ„ÄÇ‰ª•‰∏ãÊòØ‰∏Ä‰∫õÊñπÊ≥ï„ÄÇ\n\n* **ËÆ≠ÁªÉ**ÔºöËØ•Ê®°ÂûãÊòØÂü∫‰∫é‰ªÄ‰πàÊï∞ÊçÆÈõÜËøõË°åËÆ≠ÁªÉÁöÑÔºü\n* **ÂæÆË∞É**ÔºöÊ®°ÂûãÂú®Â§öÂ§ßÁ®ãÂ∫¶‰∏äÂèØ‰ª•ÂÆöÂà∂‰ª•ÊâßË°åÁâπÂÆö‰ªªÂä°ÔºåÊàñËÄÖÊòØÂê¶ÂèØ‰ª•ÈíàÂØπÁâπÂÆöÈ¢ÜÂüüËøõË°åÂæÆË∞ÉÔºü\n* **Â≠¶ÊúØÁ†îÁ©∂**ÔºöËØ• LLM ÊòØÂê¶ÊúâÂ≠¶ÊúØÁ†îÁ©∂ËÆ∫ÊñáÔºü\n\nË¶ÅÂõûÁ≠î‰∏äËø∞ÈóÆÈ¢òÔºåÊÇ®ÂèØ‰ª•Êü•Áúã‰ºòÁßÄÁöÑËµÑÊ∫êÔºåÂ¶Ç [Hugging Face](https://huggingface.co/datasets) Âíå [Arxiv.org](https://arxiv.org/)„ÄÇÊ≠§Â§ñÔºå[Open LLm Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) Âíå [LMSYS Chatbot Arena](https://chat.lmsys.org/?arena) Êèê‰æõ‰∫ÜÂêÑÁßç LLM ÁöÑËØ¶ÁªÜ‰ø°ÊÅØÂíåÂü∫ÂáÜÊµãËØï„ÄÇ\n\n## Êú¨Âú∞ LLM Â∑•ÂÖ∑ÁªìËÆ∫\n\nÊ≠£Â¶ÇÊú¨ÊñáÊâÄËÆ®ËÆ∫ÁöÑÔºåÈÄâÊã©Âíå‰ΩøÁî®Êú¨Âú∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂä®Êú∫ÊúâÂæàÂ§ö„ÄÇÂ¶ÇÊûúÊÇ®‰∏çÂ∏åÊúõÂ∞ÜÊï∞ÊçÆÈõÜÈÄöËøá‰∫íËÅîÁΩëÂèëÈÄÅÁªô AI API Êèê‰æõÂïÜÔºåÂàôÂèØ‰ª•ÂØπÊ®°ÂûãËøõË°åÂæÆË∞ÉÔºå‰ª•ÊâßË°å [ËøúÁ®ãÂåªÁñóÂ∫îÁî®](https://getstream.io/chat/solutions/healthcare/) ‰∏≠ÁöÑÁâπÂÆö‰ªªÂä°„ÄÇËÆ∏Â§öÂºÄÊ∫êÁöÑÂõæÂΩ¢Áî®Êà∑ÁïåÈù¢ÔºàGUIÔºâÊú¨Âú∞ LLM Â∑•ÂÖ∑ÔºåÂ¶Ç LLm Studio Âíå JanÔºåÊèê‰æõÁõ¥ËßÇÁöÑÂâçÁ´ØÁî®Êà∑ÁïåÈù¢Ôºå‰ª•‰æøÂú®Ê≤°ÊúâÂÉè OpenAI Êàñ Claude ËøôÊ†∑ÁöÑËÆ¢ÈòÖÊúçÂä°ÁöÑÊÉÖÂÜµ‰∏ãÈÖçÁΩÆÂíåÂÆûÈ™å LLM„ÄÇÊÇ®ËøòÂèëÁé∞‰∫ÜÂêÑÁßçÂº∫Â§ßÁöÑÂëΩ‰ª§Ë°å LLM Â∫îÁî®Á®ãÂ∫èÔºåÂ¶Ç Ollama Âíå LLaMa.cppÔºåÂ∏ÆÂä©ÊÇ®Âú®Êú¨Âú∞ËøêË°åÂíåÊµãËØïÊ®°ÂûãÔºåËÄåÊó†ÈúÄ‰∫íËÅîÁΩëËøûÊé•„ÄÇÊü•Áúã Stream ÁöÑ [AI ËÅäÂ§©Êú∫Âô®‰∫∫](https://getstream.io/chat/solutions/ai-integration/) Ëß£ÂÜ≥ÊñπÊ°àÔºåÂ∞Ü AI ËÅäÂ§©ÈõÜÊàêÂà∞ÊÇ®ÁöÑÂ∫îÁî®‰∏≠ÔºåÂπ∂ËÆøÈóÆÊâÄÊúâÁõ∏ÂÖ≥ÈìæÊé•‰ª•‰∫ÜËß£Êõ¥Â§ö‰ø°ÊÅØ„ÄÇ\n\n*ÊúÄÂàùÂèëÂ∏É‰∫é [https://getstream.io](https://getstream.io/blog/best-local-llm-tools/).*\n\n"},{"lang":"fr","group":"blog","slug":"blog/post-1","frontmatter":{"title":"Comment cr√©er une application avec des technologies modernes","meta_title":"","description":"Ceci est une m√©ta-description","date":"2022-04-04T05:00:00.000Z","image":"/images/image-placeholder.png","categories":["french","Application","Data"],"author":"John Doe","tags":["nextjs","tailwind","react"],"draft":false,"slug":"blog/post-1"},"content":"\nPersonne ne veut m√™me sortir un maquillage de l'urne des soins empoisonn√©s. C'√©tait un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\nL'entreprise elle-m√™me est une entreprise tr√®s prosp√®re. Personne ne prend m√™me la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? √ätre rejet√© par certaines personnes est un choix commode du pr√©sent pour ressentir une douleur comme la sienne !\n\n## Design Cr√©atif\n\nCar en guise de maquillage, l'urne du poison C'√©tait un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\n> Le client lui-m√™me doit pouvoir poursuivre l'adipisicing. Personne ne prend m√™me la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? √ätre rejet√© par certaines personnes est un choix commode du pr√©sent pour ressentir une douleur comme la sienne !\n\nL'entreprise elle-m√™me est une entreprise tr√®s prosp√®re. Personne ne prend m√™me la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? √ätre rejet√© par certaines personnes est un choix commode du pr√©sent pour ressentir une douleur comme la sienne !\n"},{"lang":"fr","group":"blog","slug":"blog/post-2","frontmatter":{"title":"Comment cr√©er une application avec des technologies modernes","meta_title":"","description":"Ceci est une m√©ta-description","date":"2022-04-04T05:00:00.000Z","image":"/images/image-placeholder.png","categories":["Technology","Data"],"author":"Sam Wilson","tags":["technology","tailwind"],"draft":false,"slug":"blog/post-2"},"content":"\nPersonne ne veut m√™me sortir un maquillage de l'urne des soins empoisonn√©s. C'√©tait un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\nL'entreprise elle-m√™me est une entreprise tr√®s prosp√®re. Personne ne prend m√™me la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? √ätre rejet√© par certaines personnes est un choix commode du pr√©sent pour ressentir une douleur comme la sienne !\n\n## Design Cr√©atif\n\nCar en guise de maquillage, l'urne du poison C'√©tait un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\n> Le client lui-m√™me doit pouvoir poursuivre l'adipisicing. Personne ne prend m√™me la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? √ätre rejet√© par certaines personnes est un choix commode du pr√©sent pour ressentir une douleur comme la sienne !\n\nL'entreprise elle-m√™me est une entreprise tr√®s prosp√®re. Personne ne prend m√™me la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? √ätre rejet√© par certaines personnes est un choix commode du pr√©sent pour ressentir une douleur comme la sienne !\n"},{"lang":"fr","group":"blog","slug":"blog/post-3","frontmatter":{"title":"Comment cr√©er une application avec des technologies modernes","meta_title":"","description":"Ceci est une m√©ta-description","date":"2022-04-04T05:00:00.000Z","image":"/images/image-placeholder.png","categories":["Software"],"author":"John Doe","tags":["software","tailwind"],"draft":false,"slug":"blog/post-3"},"content":"\nPersonne ne veut m√™me sortir un maquillage de l'urne des soins empoisonn√©s. C'√©tait un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\nL'entreprise elle-m√™me est une entreprise tr√®s prosp√®re. Personne ne prend m√™me la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? √ätre rejet√© par certaines personnes est un choix commode du pr√©sent pour ressentir une douleur comme la sienne !\n\n## Design Cr√©atif\n\nCar en guise de maquillage, l'urne du poison C'√©tait un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\n> Le client lui-m√™me doit pouvoir poursuivre l'adipisicing. Personne ne prend m√™me la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? √ätre rejet√© par certaines personnes est un choix commode du pr√©sent pour ressentir une douleur comme la sienne !\n\nL'entreprise elle-m√™me est une entreprise tr√®s prosp√®re. Personne ne prend m√™me la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? √ätre rejet√© par certaines personnes est un choix commode du pr√©sent pour ressentir une douleur comme la sienne !\n"},{"lang":"fr","group":"blog","slug":"blog/post-4","frontmatter":{"title":"Comment cr√©er une application avec des technologies modernes","meta_title":"","description":"Ceci est une m√©ta-description","date":"2022-04-04T05:00:00.000Z","image":"/images/image-placeholder.png","categories":["Architecture"],"author":"John Doe","tags":["silicon","technology"],"draft":false,"slug":"blog/post-4"},"content":"\nPersonne ne veut m√™me sortir un maquillage de l'urne des soins empoisonn√©s. C'√©tait un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\nL'entreprise elle-m√™me est une entreprise tr√®s prosp√®re. Personne ne prend m√™me la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? √ätre rejet√© par certaines personnes est un choix commode du pr√©sent pour ressentir une douleur comme la sienne !\n\n## Design Cr√©atif\n\nCar en guise de maquillage, l'urne du poison C'√©tait un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\n> Le client lui-m√™me doit pouvoir poursuivre l'adipisicing. Personne ne prend m√™me la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? √ätre rejet√© par certaines personnes est un choix commode du pr√©sent pour ressentir une douleur comme la sienne !\n\nL'entreprise elle-m√™me est une entreprise tr√®s prosp√®re. Personne ne prend m√™me la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? √ätre rejet√© par certaines personnes est un choix commode du pr√©sent pour ressentir une douleur comme la sienne !\n"},{"lang":"en","group":"models","slug":"models/chatgpt-4o-latest","frontmatter":{"title":"OpenAI: ChatGPT-4o","meta_title":"OpenAI: ChatGPT-4o","description":"OpenAI: ChatGPT-4o","date":"2024-08-14T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text image 2 text"],"author":"openai","tags":["Chatbots","Generative AI","Machine Learning","Natural Language Processing"],"draft":false,"id":"chatgpt-4o-latest","context":128000,"input":0.000005,"output":0.000015,"img":0.007225,"request":0,"last_updated":"2024-08-14T00:00:00.000Z","slug":"models/chatgpt-4o-latest"},"content":"\nDynamic model continuously updated to the current version of [GPT-4o](/openai/gpt-4o) in ChatGPT. Intended for research and evaluation.\n\nNote: This model is currently experimental and not suitable for production use-cases, and may be heavily rate-limited.\n\n"},{"lang":"en","group":"models","slug":"models/claude-3-haiku","frontmatter":{"title":"Anthropic: Claude 3 Haiku","meta_title":"Anthropic: Claude 3 Haiku","description":"Anthropic: Claude 3 Haiku","date":"2024-03-13T00:00:00.000Z","image":"https://img.rifx.online/logo/anthropic.svg","categories":["text image 2 text"],"author":"anthropic","tags":["Programming","Machine Learning","Generative AI","Chatbots","Natural Language Processing"],"draft":false,"id":"claude-3-haiku","context":200000,"input":2.5e-7,"output":0.00000125,"img":0.0004,"request":0,"last_updated":"2024-10-24T11:54:59.000Z","slug":"models/claude-3-haiku"},"content":"\nClaude 3 Haiku is Anthropic's fastest and most compact model for\nnear-instant responsiveness. Quick and accurate targeted performance.\n\nSee the launch announcement and benchmark results [here](https://www.anthropic.com/news/claude-3-haiku)\n\n#multimodal\n\n"},{"lang":"en","group":"models","slug":"models/claude-35-sonnet","frontmatter":{"title":"Anthropic: Claude 3.5 Sonnet","meta_title":"Anthropic: Claude 3.5 Sonnet","description":"Anthropic: Claude 3.5 Sonnet","date":"2024-06-20T00:00:00.000Z","image":"https://img.rifx.online/logo/anthropic.svg","categories":["text image 2 text"],"author":"anthropic","tags":["Programming","Data Science","Computer Vision","Chatbots","Autonomous Systems"],"draft":false,"id":"claude-3.5-sonnet","context":200000,"input":0.000003,"output":0.000015,"img":0.0048,"request":0,"last_updated":"2024-10-24T11:45:46.000Z","slug":"models/claude-35-sonnet"},"content":"\nClaude 3.5 Sonnet delivers better-than-Opus capabilities, faster-than-Sonnet speeds, at the same Sonnet prices. Sonnet is particularly good at:\n\n- Coding: Autonomously writes, edits, and runs code with reasoning and troubleshooting\n- Data science: Augments human data science expertise; navigates unstructured data while using multiple tools for insights\n- Visual processing: excelling at interpreting charts, graphs, and images, accurately transcribing text to derive insights beyond just the text alone\n- Agentic tasks: exceptional tool use, making it great at agentic tasks (i.e. complex, multi-step problem solving tasks that require engaging with other systems)\n\n#multimodal\n\n"},{"lang":"en","group":"models","slug":"models/gemini-flash-15-8b","frontmatter":{"title":"Google: Gemini 1.5 Flash-8B","meta_title":"Google: Gemini 1.5 Flash-8B","description":"Google: Gemini 1.5 Flash-8B","date":"2024-10-03T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text image 2 text"],"author":"google","tags":["Programming","Natural Language Processing","Chatbots","Translation","Technology/Web"],"draft":false,"id":"gemini-flash-1.5-8b","context":1000000,"input":3.75e-8,"output":1.5e-7,"img":0,"request":0,"last_updated":"2024-10-03T00:00:00.000Z","slug":"models/gemini-flash-15-8b"},"content":"\nGemini 1.5 Flash-8B is optimized for speed and efficiency, offering enhanced performance in small prompt tasks like chat, transcription, and translation. With reduced latency, it is highly effective for real-time and large-scale operations. This model focuses on cost-effective solutions while maintaining high-quality results.\n\n[Click here to learn more about this model](https://developers.googleblog.com/en/gemini-15-flash-8b-is-now-generally-available-for-use/).\n\nUsage of Gemini is subject to Google's [Gemini Terms of Use](https://ai.google.dev/terms).\n\n"},{"lang":"en","group":"models","slug":"models/gemini-pro-15","frontmatter":{"title":"Google: Gemini Pro 1.5","meta_title":"Google: Gemini Pro 1.5","description":"Google: Gemini Pro 1.5","date":"2024-04-09T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text image 2 text"],"author":"google","tags":["Programming","Natural Language Processing","Machine Learning","Generative AI","Chatbots"],"draft":false,"id":"gemini-pro-1.5","context":2000000,"input":0.00000125,"output":0.000005,"img":0.00263,"request":0,"last_updated":"2024-04-09T00:00:00.000Z","slug":"models/gemini-pro-15"},"content":"\nGoogle's latest multimodal model, supporting image and video in text or chat prompts.\n\nOptimized for language tasks including:\n\n- Code generation\n- Text generation\n- Text editing\n- Problem solving\n- Recommendations\n- Information extraction\n- Data extraction or generation\n- AI agents\n\nUsage of Gemini is subject to Google's [Gemini Terms of Use](https://ai.google.dev/terms).\n\n#multimodal\n\n"},{"lang":"en","group":"models","slug":"models/gpt-35-turbo-instruct","frontmatter":{"title":"OpenAI: GPT-3.5 Turbo Instruct","meta_title":"OpenAI: GPT-3.5 Turbo Instruct","description":"OpenAI: GPT-3.5 Turbo Instruct","date":"2023-09-28T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"openai","tags":["Programming","Natural Language Processing","Generative AI","Chatbots","Technology/Web"],"draft":false,"id":"gpt-3.5-turbo-instruct","context":4095,"input":0.0000015,"output":0.000002,"img":0,"request":0,"last_updated":"2023-09-28T00:00:00.000Z","slug":"models/gpt-35-turbo-instruct"},"content":"\nThis model is a variant of GPT-3.5 Turbo tuned for instructional prompts and omitting chat-related optimizations. Training data: up to Sep 2021.\n\n"},{"lang":"en","group":"models","slug":"models/gpt-4o-mini","frontmatter":{"title":"OpenAI: GPT-4o-mini","meta_title":"OpenAI: GPT-4o-mini","description":"OpenAI: GPT-4o-mini","date":"2024-07-18T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text image 2 text"],"author":"openai","tags":["Programming","Technology","Programming/Scripting","Technology/Web"],"draft":false,"id":"gpt-4o-mini","context":128000,"input":1.5e-7,"output":6e-7,"img":0.007225,"request":0,"last_updated":"2024-07-18T00:00:00.000Z","slug":"models/gpt-4o-mini"},"content":"\nGPT-4o mini is OpenAI's newest model after [GPT-4 Omni](/openai/gpt-4o), supporting both text and image inputs with text outputs.\n\nAs their most advanced small model, it is many multiples more affordable than other recent frontier models, and more than 60% cheaper than [GPT-3.5 Turbo](/openai/gpt-3.5-turbo). It maintains SOTA intelligence, while being significantly more cost-effective.\n\nGPT-4o mini achieves an 82% score on MMLU and presently ranks higher than GPT-4 on chat preferences [common leaderboards](https://arena.lmsys.org/).\n\nCheck out the [launch announcement](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/) to learn more.\n\n"},{"lang":"en","group":"models","slug":"models/gpt-4o","frontmatter":{"title":"OpenAI: GPT-4o","meta_title":"OpenAI: GPT-4o","description":"OpenAI: GPT-4o","date":"2024-05-13T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text image 2 text"],"author":"openai","tags":["Programming","Natural Language Processing","Machine Learning","Generative AI","Computer Vision"],"draft":false,"id":"gpt-4o","context":128000,"input":0.0000025,"output":0.00001,"img":0.0036125,"request":0,"last_updated":"2024-05-13T00:00:00.000Z","slug":"models/gpt-4o"},"content":"\nGPT-4o (\"o\" for \"omni\") is OpenAI's latest AI model, supporting both text and image inputs with text outputs. It maintains the intelligence level of [GPT-4 Turbo](/openai/gpt-4-turbo) while being twice as fast and 50% more cost-effective. GPT-4o also offers improved performance in processing non-English languages and enhanced visual capabilities.\n\nFor benchmarking against other models, it was briefly called [\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209)\n\n"},{"lang":"en","group":"models","slug":"models/llama-31-nemotron-70b-instruct","frontmatter":{"title":"Nvidia: Llama 3.1 Nemotron 70B Instruct","meta_title":"Nvidia: Llama 3.1 Nemotron 70B Instruct","description":"Nvidia: Llama 3.1 Nemotron 70B Instruct","date":"2024-10-15T00:00:00.000Z","image":"https://img.rifx.online/logo/nvidia.svg","categories":["text 2 text"],"author":"nvidia","tags":["Programming","Natural Language Processing","Machine Learning","Generative AI","Ethics"],"draft":false,"id":"llama-3.1-nemotron-70b-instruct","context":131072,"input":3.5e-7,"output":4e-7,"img":0,"request":0,"last_updated":"2024-10-15T00:00:00.000Z","slug":"models/llama-31-nemotron-70b-instruct"},"content":"\nNVIDIA's Llama 3.1 Nemotron 70B is a language model designed for generating precise and useful responses. Leveraging [Llama 3.1 70B](/models/meta-llama/llama-3.1-70b-instruct) architecture and Reinforcement Learning from Human Feedback (RLHF), it excels in automatic alignment benchmarks. This model is tailored for applications requiring high accuracy in helpfulness and response generation, suitable for diverse user queries across multiple domains.\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).\n\n"},{"lang":"en","group":"models","slug":"models/llama-32-11b-vision-instruct","frontmatter":{"title":"Meta: Llama 3.2 11B Vision Instruct","meta_title":"Meta: Llama 3.2 11B Vision Instruct","description":"Meta: Llama 3.2 11B Vision Instruct","date":"2024-09-25T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text image 2 text"],"author":"meta-llama","tags":["Natural Language Processing","Computer Vision","Machine Learning","Generative AI","Data Science"],"draft":false,"id":"llama-3.2-11b-vision-instruct","context":131072,"input":5.5e-8,"output":5.5e-8,"img":0.000079475,"request":0,"last_updated":"2024-09-25T00:00:00.000Z","slug":"models/llama-32-11b-vision-instruct"},"content":"\nLlama 3.2 11B Vision is a multimodal model with 11 billion parameters, designed to handle tasks combining visual and textual data. It excels in tasks such as image captioning and visual question answering, bridging the gap between language generation and visual reasoning. Pre-trained on a massive dataset of image-text pairs, it performs well in complex, high-accuracy image analysis.\n\nIts ability to integrate visual understanding with language processing makes it an ideal solution for industries requiring comprehensive visual-linguistic AI applications, such as content creation, AI-driven customer service, and research.\n\nClick here for the [original model card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md).\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).\n\n"},{"lang":"en","group":"models","slug":"models/llama-32-1b-instruct","frontmatter":{"title":"Meta: Llama 3.2 1B Instruct","meta_title":"Meta: Llama 3.2 1B Instruct","description":"Meta: Llama 3.2 1B Instruct","date":"2024-09-25T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text 2 text"],"author":"meta-llama","tags":["Natural Language Processing","Programming","Technology","Machine Learning","Generative AI"],"draft":false,"id":"llama-3.2-1b-instruct","context":131072,"input":1e-8,"output":2e-8,"img":0,"request":0,"last_updated":"2024-09-25T00:00:00.000Z","slug":"models/llama-32-1b-instruct"},"content":"\nLlama 3.2 1B is a 1-billion-parameter language model focused on efficiently performing natural language tasks, such as summarization, dialogue, and multilingual text analysis. Its smaller size allows it to operate efficiently in low-resource environments while maintaining strong task performance.\n\nSupporting eight core languages and fine-tunable for more, Llama 1.3B is ideal for businesses or developers seeking lightweight yet powerful AI solutions that can operate in diverse multilingual settings without the high computational demand of larger models.\n\nClick here for the [original model card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md).\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).\n\n"},{"lang":"en","group":"models","slug":"models/llama-32-90b-vision-instruct","frontmatter":{"title":"Meta: Llama 3.2 90B Vision Instruct","meta_title":"Meta: Llama 3.2 90B Vision Instruct","description":"Meta: Llama 3.2 90B Vision Instruct","date":"2024-09-25T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text image 2 text"],"author":"meta-llama","tags":["Natural Language Processing","Computer Vision","Machine Learning","Data Science","Generative AI"],"draft":false,"id":"llama-3.2-90b-vision-instruct","context":131072,"input":3.5e-7,"output":4e-7,"img":0.00050575,"request":0,"last_updated":"2024-09-25T00:00:00.000Z","slug":"models/llama-32-90b-vision-instruct"},"content":"\nThe Llama 90B Vision model is a top-tier, 90-billion-parameter multimodal model designed for the most challenging visual reasoning and language tasks. It offers unparalleled accuracy in image captioning, visual question answering, and advanced image-text comprehension. Pre-trained on vast multimodal datasets and fine-tuned with human feedback, the Llama 90B Vision is engineered to handle the most demanding image-based AI tasks.\n\nThis model is perfect for industries requiring cutting-edge multimodal AI capabilities, particularly those dealing with complex, real-time visual and textual analysis.\n\nClick here for the [original model card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md).\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).\n\n"},{"lang":"en","group":"models","slug":"models/ministral-8b","frontmatter":{"title":"Ministral 8B","meta_title":"Ministral 8B","description":"Ministral 8B","date":"2024-10-17T00:00:00.000Z","image":"https://img.rifx.online/logo/mistral.png","categories":["text 2 text"],"author":"mistralai","tags":["Technology","Machine Learning","Data Science","Generative AI","Ethics"],"draft":false,"id":"ministral-8b","context":128000,"input":1e-7,"output":1e-7,"img":0,"request":0,"last_updated":"2024-10-19T04:54:11.000Z","slug":"models/ministral-8b"},"content":"\nMinistral 8B is an 8B parameter model featuring a unique interleaved sliding-window attention pattern for faster, memory-efficient inference. Designed for edge use cases, it supports up to 128k context length and excels in knowledge and reasoning tasks. It outperforms peers in the sub-10B category, making it perfect for low-latency, privacy-first applications.\n\n"},{"lang":"en","group":"models","slug":"models/o1-mini","frontmatter":{"title":"OpenAI: o1-mini","meta_title":"OpenAI: o1-mini","description":"OpenAI: o1-mini","date":"2024-09-12T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"openai","tags":["Programming","Science","Natural Language Processing","Machine Learning","Data Science"],"draft":false,"id":"o1-mini","context":128000,"input":0.000003,"output":0.000012,"img":0,"request":0,"last_updated":"2024-09-12T00:00:00.000Z","slug":"models/o1-mini"},"content":"\nThe latest and strongest model family from OpenAI, o1 is designed to spend more time thinking before responding.\n\nThe o1 models are optimized for math, science, programming, and other STEM-related tasks. They consistently exhibit PhD-level accuracy on benchmarks in physics, chemistry, and biology. Learn more in the [launch announcement](https://openai.com/o1).\n\nNote: This model is currently experimental and not suitable for production use-cases, and may be heavily rate-limited.\n\n"},{"lang":"en","group":"models","slug":"models/o1-preview","frontmatter":{"title":"OpenAI: o1-preview","meta_title":"OpenAI: o1-preview","description":"OpenAI: o1-preview","date":"2024-09-12T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"openai","tags":["Programming","Science","Natural Language Processing","Machine Learning","Data Science"],"draft":false,"id":"o1-preview","context":128000,"input":0.000015,"output":0.00006,"img":0,"request":0,"last_updated":"2024-09-12T00:00:00.000Z","slug":"models/o1-preview"},"content":"\nThe latest and strongest model family from OpenAI, o1 is designed to spend more time thinking before responding.\n\nThe o1 models are optimized for math, science, programming, and other STEM-related tasks. They consistently exhibit PhD-level accuracy on benchmarks in physics, chemistry, and biology. Learn more in the [launch announcement](https://openai.com/o1).\n\nNote: This model is currently experimental and not suitable for production use-cases, and may be heavily rate-limited.\n\n"},{"lang":"en","group":"models","slug":"models/qwen-2-7b-instruct","frontmatter":{"title":"Qwen 2 7B Instruct","meta_title":"Qwen 2 7B Instruct","description":"Qwen 2 7B Instruct","date":"2024-07-16T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"qwen","tags":["Natural Language Processing","Programming","Machine Learning","Data Science","Ethics"],"draft":false,"id":"qwen-2-7b-instruct","context":32768,"input":5.4e-8,"output":5.4e-8,"img":0,"request":0,"last_updated":"2024-07-16T00:00:00.000Z","slug":"models/qwen-2-7b-instruct"},"content":"\nQwen2 7B is a transformer-based model that excels in language understanding, multilingual capabilities, coding, mathematics, and reasoning.\n\nIt features SwiGLU activation, attention QKV bias, and group query attention. It is pretrained on extensive data with supervised finetuning and direct preference optimization.\n\nFor more details, see this [blog post](https://qwenlm.github.io/blog/qwen2/) and [GitHub repo](https://github.com/QwenLM/Qwen2).\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).\n\n"},{"lang":"en","group":"models","slug":"models/qwen-2-vl-72b-instruct","frontmatter":{"title":"Qwen2-VL 72B Instruct","meta_title":"Qwen2-VL 72B Instruct","description":"Qwen2-VL 72B Instruct","date":"2024-09-18T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text image 2 text"],"author":"qwen","tags":["Natural Language Processing","Computer Vision","Robotics","Machine Learning"],"draft":false,"id":"qwen-2-vl-72b-instruct","context":32768,"input":4e-7,"output":4e-7,"img":0.000578,"request":0,"last_updated":"2024-09-18T00:00:00.000Z","slug":"models/qwen-2-vl-72b-instruct"},"content":"\nQwen2 VL 72B is a multimodal LLM from the Qwen Team with the following key enhancements:\n\n- SoTA understanding of images of various resolution & ratio: Qwen2-VL achieves state-of-the-art performance on visual understanding benchmarks, including MathVista, DocVQA, RealWorldQA, MTVQA, etc.\n\n- Understanding videos of 20min+: Qwen2-VL can understand videos over 20 minutes for high-quality video-based question answering, dialog, content creation, etc.\n\n- Agent that can operate your mobiles, robots, etc.: with the abilities of complex reasoning and decision making, Qwen2-VL can be integrated with devices like mobile phones, robots, etc., for automatic operation based on visual environment and text instructions.\n\n- Multilingual Support: to serve global users, besides English and Chinese, Qwen2-VL now supports the understanding of texts in different languages inside images, including most European languages, Japanese, Korean, Arabic, Vietnamese, etc.\n\nFor more details, see this [blog post](https://qwenlm.github.io/blog/qwen2-vl/) and [GitHub repo](https://github.com/QwenLM/Qwen2-VL).\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).\n\n"},{"lang":"en","group":"models","slug":"models/qwen-25-72b-instruct","frontmatter":{"title":"Qwen2.5 72B Instruct","meta_title":"Qwen2.5 72B Instruct","description":"Qwen2.5 72B Instruct","date":"2024-09-19T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"qwen","tags":["Programming","Natural Language Processing","Chatbots","Machine Learning","Data Science"],"draft":false,"id":"qwen-2.5-72b-instruct","context":131072,"input":3.5e-7,"output":4e-7,"img":0,"request":0,"last_updated":"2024-09-19T00:00:00.000Z","slug":"models/qwen-25-72b-instruct"},"content":"\nQwen2.5 72B is the latest series of Qwen large language models. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\n\n- Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n\n- Long-context Support up to 128K tokens and can generate up to 8K tokens.\n\n- Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).\n\n"},{"lang":"en","group":"models","slug":"models/qwen-25-7b-instruct","frontmatter":{"title":"Qwen2.5 7B Instruct","meta_title":"Qwen2.5 7B Instruct","description":"Qwen2.5 7B Instruct","date":"2024-10-16T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"qwen","tags":["Programming","Natural Language Processing","Chatbots","Machine Learning","Data Science"],"draft":false,"id":"qwen-2.5-7b-instruct","context":131072,"input":2.7e-7,"output":2.7e-7,"img":0,"request":0,"last_updated":"2024-10-16T00:00:00.000Z","slug":"models/qwen-25-7b-instruct"},"content":"\nQwen2.5 7B is the latest series of Qwen large language models. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\n\n- Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n\n- Long-context Support up to 128K tokens and can generate up to 8K tokens.\n\n- Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).\n\n"},{"lang":"zh","group":"models","slug":"models/chatgpt-4o-latest","frontmatter":{"title":"OpenAI: ChatGPT-4o","meta_title":"OpenAI: ChatGPT-4o","description":"OpenAI: ChatGPT-4o","date":"2024-08-14T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text image 2 text"],"author":"openai","tags":["Chatbots","Generative AI","Machine Learning","Natural Language Processing"],"draft":false,"id":"chatgpt-4o-latest","context":128000,"input":0.000005,"output":0.000015,"img":0.007225,"request":0,"last_updated":"2024-08-14T00:00:00.000Z","slug":"models/chatgpt-4o-latest"},"content":"\nÂä®ÊÄÅÊ®°ÂûãÊåÅÁª≠Êõ¥Êñ∞Âà∞ ChatGPT ‰∏≠ÁöÑÂΩìÂâçÁâàÊú¨ [GPT-4o](/openai/gpt-4o)„ÄÇÊó®Âú®Áî®‰∫éÁ†îÁ©∂ÂíåËØÑ‰º∞„ÄÇ\n\nÊ≥®ÊÑèÔºöÊ≠§Ê®°ÂûãÁõÆÂâçÂ§Ñ‰∫éÂÆûÈ™åÈò∂ÊÆµÔºå‰∏çÈÄÇÂêàÁîü‰∫ß‰ΩøÁî®ÔºåÂπ∂ÂèØËÉΩÂèóÂà∞‰∏•Ê†ºÁöÑÈÄüÁéáÈôêÂà∂„ÄÇ\n\n"},{"lang":"zh","group":"models","slug":"models/claude-3-haiku","frontmatter":{"title":"Anthropic: Claude 3 Haiku","meta_title":"Anthropic: Claude 3 Haiku","description":"Anthropic: Claude 3 Haiku","date":"2024-03-13T00:00:00.000Z","image":"https://img.rifx.online/logo/anthropic.svg","categories":["text image 2 text"],"author":"anthropic","tags":["Programming","Machine Learning","Generative AI","Chatbots","Natural Language Processing"],"draft":false,"id":"claude-3-haiku","context":200000,"input":2.5e-7,"output":0.00000125,"img":0.0004,"request":0,"last_updated":"2024-10-24T11:54:59.000Z","slug":"models/claude-3-haiku"},"content":"\nClaude 3 Haiku ÊòØ Anthropic ÂèçÂ∫îÈÄüÂ∫¶ÊúÄÂø´„ÄÅ‰ΩìÁßØÊúÄÂ∞èÁöÑÊ®°ÂûãÔºåËÉΩÂ§üÂÆûÁé∞Ëøë‰πéÂç≥Êó∂ÁöÑÂìçÂ∫î„ÄÇÂø´ÈÄü‰∏îÂáÜÁ°ÆÁöÑÂÆöÂêëÊÄßËÉΩ„ÄÇ\n\nÊü•ÁúãÂèëÂ∏ÉÂÖ¨ÂëäÂíåÂü∫ÂáÜÊµãËØïÁªìÊûú [ËøôÈáå](https://www.anthropic.com/news/claude-3-haiku)\n\n#multimodal\n\n"},{"lang":"zh","group":"models","slug":"models/claude-35-sonnet","frontmatter":{"title":"Anthropic: Claude 3.5 Sonnet","meta_title":"Anthropic: Claude 3.5 Sonnet","description":"Anthropic: Claude 3.5 Sonnet","date":"2024-06-20T00:00:00.000Z","image":"https://img.rifx.online/logo/anthropic.svg","categories":["text image 2 text"],"author":"anthropic","tags":["Programming","Data Science","Computer Vision","Chatbots","Autonomous Systems"],"draft":false,"id":"claude-3.5-sonnet","context":200000,"input":0.000003,"output":0.000015,"img":0.0048,"request":0,"last_updated":"2024-10-24T11:45:46.000Z","slug":"models/claude-35-sonnet"},"content":"\nClaude 3.5 Sonnet Êèê‰æõ‰ºò‰∫é Opus ÁöÑËÉΩÂäõÔºå‰ª•Âø´‰∫é Sonnet ÁöÑÈÄüÂ∫¶Ôºå‰∏î‰ª∑Ê†º‰∏é Sonnet Áõ∏Âêå„ÄÇSonnet Âú®‰ª•‰∏ãÊñπÈù¢Ë°®Áé∞Â∞§‰∏∫Âá∫Ëâ≤Ôºö\n\n- ÁºñÁ®ãÔºöËá™‰∏ªÁºñÂÜô„ÄÅÁºñËæëÂíåËøêË°å‰ª£Á†ÅÔºåÂÖ∑Â§áÊé®ÁêÜÂíåÊïÖÈöúÊéíÈô§ËÉΩÂäõ\n- Êï∞ÊçÆÁßëÂ≠¶ÔºöÂ¢ûÂº∫‰∫∫Á±ªÊï∞ÊçÆÁßëÂ≠¶‰∏ì‰∏öÁü•ËØÜÔºõÂú®‰ΩøÁî®Â§öÁßçÂ∑•ÂÖ∑Ëé∑ÂèñÊ¥ûÂØüÁöÑÂêåÊó∂ÔºåÂ§ÑÁêÜÈùûÁªìÊûÑÂåñÊï∞ÊçÆ\n- ËßÜËßâÂ§ÑÁêÜÔºöÊìÖÈïøËß£ËØªÂõæË°®„ÄÅÂõæÂΩ¢ÂíåÂõæÂÉèÔºåÂáÜÁ°ÆËΩ¨ÂΩïÊñáÊú¨‰ª•Ëé∑ÂæóË∂ÖË∂äÊñáÊú¨Êú¨Ë∫´ÁöÑÊ¥ûÂØü\n- ‰ª£ÁêÜ‰ªªÂä°ÔºöÂá∫Ëâ≤ÁöÑÂ∑•ÂÖ∑‰ΩøÁî®ËÉΩÂäõÔºå‰ΩøÂÖ∂Âú®‰ª£ÁêÜ‰ªªÂä°ÔºàÂç≥ÈúÄË¶Å‰∏éÂÖ∂‰ªñÁ≥ªÁªü‰∫íÂä®ÁöÑÂ§çÊùÇÂ§öÊ≠•È™§ÈóÆÈ¢òËß£ÂÜ≥‰ªªÂä°Ôºâ‰∏≠Ë°®Áé∞Âá∫Ëâ≤\n\n#multimodal\n\n"},{"lang":"zh","group":"models","slug":"models/gemini-flash-15-8b","frontmatter":{"title":"Google: Gemini 1.5 Flash-8B","meta_title":"Google: Gemini 1.5 Flash-8B","description":"Google: Gemini 1.5 Flash-8B","date":"2024-10-03T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text image 2 text"],"author":"google","tags":["Programming","Natural Language Processing","Chatbots","Translation","Technology/Web"],"draft":false,"id":"gemini-flash-1.5-8b","context":1000000,"input":3.75e-8,"output":1.5e-7,"img":0,"request":0,"last_updated":"2024-10-03T00:00:00.000Z","slug":"models/gemini-flash-15-8b"},"content":"\nGemini 1.5 Flash-8B ÈíàÂØπÈÄüÂ∫¶ÂíåÊïàÁéáËøõË°å‰∫Ü‰ºòÂåñÔºåÂú®ËÅäÂ§©„ÄÅËΩ¨ÂΩïÂíåÁøªËØëÁ≠âÂ∞èÊèêÁ§∫‰ªªÂä°‰∏≠Êèê‰æõ‰∫ÜÂ¢ûÂº∫ÁöÑÊÄßËÉΩ„ÄÇÈÄöËøáÂáèÂ∞ëÂª∂ËøüÔºåÂÆÉÂú®ÂÆûÊó∂ÂíåÂ§ßËßÑÊ®°Êìç‰Ωú‰∏≠ÈùûÂ∏∏ÊúâÊïà„ÄÇËØ•Ê®°Âûã‰∏ìÊ≥®‰∫éÊàêÊú¨ÊïàÁõäËß£ÂÜ≥ÊñπÊ°àÔºåÂêåÊó∂‰øùÊåÅÈ´òË¥®ÈáèÁöÑÁªìÊûú„ÄÇ\n\n[ÁÇπÂáªÊ≠§Â§Ñ‰∫ÜËß£Êõ¥Â§öÂÖ≥‰∫éÊ≠§Ê®°ÂûãÁöÑ‰ø°ÊÅØ](https://developers.googleblog.com/en/gemini-15-flash-8b-is-now-generally-available-for-use/)„ÄÇ\n\n‰ΩøÁî® Gemini ÂèóÈôê‰∫é Google's [Gemini ‰ΩøÁî®Êù°Ê¨æ](https://ai.google.dev/terms)„ÄÇ\n\n"},{"lang":"zh","group":"models","slug":"models/gemini-pro-15","frontmatter":{"title":"Google: Gemini Pro 1.5","meta_title":"Google: Gemini Pro 1.5","description":"Google: Gemini Pro 1.5","date":"2024-04-09T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text image 2 text"],"author":"google","tags":["Programming","Natural Language Processing","Machine Learning","Generative AI","Chatbots"],"draft":false,"id":"gemini-pro-1.5","context":2000000,"input":0.00000125,"output":0.000005,"img":0.00263,"request":0,"last_updated":"2024-04-09T00:00:00.000Z","slug":"models/gemini-pro-15"},"content":"\nË∞∑Ê≠åÊúÄÊñ∞ÁöÑÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºåÊîØÊåÅÂú®ÊñáÊú¨ÊàñËÅäÂ§©ÊèêÁ§∫‰∏≠‰ΩøÁî®ÂõæÂÉèÂíåËßÜÈ¢ë„ÄÇ\n\nÈíàÂØπ‰ª•‰∏ãËØ≠Ë®Ä‰ªªÂä°ËøõË°å‰∫Ü‰ºòÂåñÔºö\n\n- ‰ª£Á†ÅÁîüÊàê\n- ÊñáÊú¨ÁîüÊàê\n- ÊñáÊú¨ÁºñËæë\n- ÈóÆÈ¢òËß£ÂÜ≥\n- Êé®Ëçê\n- ‰ø°ÊÅØÊèêÂèñ\n- Êï∞ÊçÆÊèêÂèñÊàñÁîüÊàê\n- AI‰ª£ÁêÜ\n\nGeminiÁöÑ‰ΩøÁî®ÂèóÈôê‰∫éË∞∑Ê≠åÁöÑ[Gemini‰ΩøÁî®Êù°Ê¨æ](https://ai.google.dev/terms)„ÄÇ\n\n#multimodal\n\n"},{"lang":"zh","group":"models","slug":"models/gpt-35-turbo-instruct","frontmatter":{"title":"OpenAI: GPT-3.5 Turbo Instruct","meta_title":"OpenAI: GPT-3.5 Turbo Instruct","description":"OpenAI: GPT-3.5 Turbo Instruct","date":"2023-09-28T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"openai","tags":["Programming","Natural Language Processing","Generative AI","Chatbots","Technology/Web"],"draft":false,"id":"gpt-3.5-turbo-instruct","context":4095,"input":0.0000015,"output":0.000002,"img":0,"request":0,"last_updated":"2023-09-28T00:00:00.000Z","slug":"models/gpt-35-turbo-instruct"},"content":"\nËØ•Ê®°ÂûãÊòØGPT-3.5 TurboÁöÑ‰∏Ä‰∏™Âèò‰ΩìÔºåÈíàÂØπÊïôÂ≠¶ÊèêÁ§∫ËøõË°å‰∫ÜË∞ÉÊï¥ÔºåÂπ∂ÁúÅÁï•‰∫Ü‰∏éËÅäÂ§©Áõ∏ÂÖ≥ÁöÑ‰ºòÂåñ„ÄÇËÆ≠ÁªÉÊï∞ÊçÆÔºöÊà™Ëá≥2021Âπ¥9Êúà„ÄÇ\n\n"},{"lang":"zh","group":"models","slug":"models/gpt-4o-mini","frontmatter":{"title":"OpenAI: GPT-4o-mini","meta_title":"OpenAI: GPT-4o-mini","description":"OpenAI: GPT-4o-mini","date":"2024-07-18T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text image 2 text"],"author":"openai","tags":["Programming","Technology","Programming/Scripting","Technology/Web"],"draft":false,"id":"gpt-4o-mini","context":128000,"input":1.5e-7,"output":6e-7,"img":0.007225,"request":0,"last_updated":"2024-07-18T00:00:00.000Z","slug":"models/gpt-4o-mini"},"content":"\nGPT-4o mini ÊòØ OpenAI Âú® [GPT-4 Omni](/openai/gpt-4o) ‰πãÂêéÂèëÂ∏ÉÁöÑÊúÄÊñ∞Ê®°ÂûãÔºåÊîØÊåÅÊñáÊú¨ÂíåÂõæÂÉèËæìÂÖ•ÔºåÂπ∂ÁîüÊàêÊñáÊú¨ËæìÂá∫„ÄÇ\n\n‰Ωú‰∏∫‰ªñ‰ª¨ÊúÄÂÖàËøõÁöÑÂ∞èÂûãÊ®°ÂûãÔºåÂÆÉÁöÑ‰ª∑Ê†ºÊØîÂÖ∂‰ªñÊúÄËøëÁöÑÂâçÊ≤øÊ®°Âûã‰Ωé‰∫ÜËÆ∏Â§öÂÄçÔºåÂπ∂‰∏îÊØî [GPT-3.5 Turbo](/openai/gpt-3.5-turbo) ‰æøÂÆúË∂ÖËøá 60%„ÄÇÂÆÉ‰øùÊåÅ‰∫Ü SOTA Êô∫ËÉΩÔºåÂêåÊó∂Âú®ÊàêÊú¨ÊïàÁõä‰∏äÊòæËëóÊõ¥È´ò„ÄÇ\n\nGPT-4o mini Âú® MMLU ‰∏äËé∑Âæó‰∫Ü 82% ÁöÑÂàÜÊï∞ÔºåÁõÆÂâçÂú®ËÅäÂ§©ÂÅèÂ•Ω [Â∏∏ËßÅÊéíË°åÊ¶ú](https://arena.lmsys.org/) ‰∏äÊéíÂêçÈ´ò‰∫é GPT-4„ÄÇ\n\nÊü•Áúã [ÂèëÂ∏ÉÂÖ¨Âëä](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/) ‰ª•‰∫ÜËß£Êõ¥Â§ö‰ø°ÊÅØ„ÄÇ\n\n"},{"lang":"zh","group":"models","slug":"models/gpt-4o","frontmatter":{"title":"OpenAI: GPT-4o","meta_title":"OpenAI: GPT-4o","description":"OpenAI: GPT-4o","date":"2024-05-13T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text image 2 text"],"author":"openai","tags":["Programming","Natural Language Processing","Machine Learning","Generative AI","Computer Vision"],"draft":false,"id":"gpt-4o","context":128000,"input":0.0000025,"output":0.00001,"img":0.0036125,"request":0,"last_updated":"2024-05-13T00:00:00.000Z","slug":"models/gpt-4o"},"content":"\nGPT-4oÔºà‚Äúo‚Äù‰ª£Ë°®‚ÄúÂÖ®ËÉΩ‚ÄùÔºâÊòØOpenAIÊúÄÊñ∞ÁöÑAIÊ®°ÂûãÔºåÊîØÊåÅÊñáÊú¨ÂíåÂõæÂÉèËæìÂÖ•ÔºåÂπ∂ÁîüÊàêÊñáÊú¨ËæìÂá∫„ÄÇÂÆÉ‰øùÊåÅ‰∫Ü[GPT-4 Turbo](/openai/gpt-4-turbo)ÁöÑÊô∫ËÉΩÊ∞¥Âπ≥ÔºåÂêåÊó∂ÈÄüÂ∫¶ÊèêÈ´ò‰∫Ü‰∏§ÂÄçÔºåÊàêÊú¨ÊïàÁõäÊèêÈ´ò‰∫Ü50%„ÄÇGPT-4oËøòÂú®Â§ÑÁêÜÈùûËã±ËØ≠ËØ≠Ë®ÄÂíåÂ¢ûÂº∫ËßÜËßâËÉΩÂäõÊñπÈù¢Êèê‰æõ‰∫ÜÊõ¥Â•ΩÁöÑÊÄßËÉΩ„ÄÇ\n\n‰∏∫‰∫Ü‰∏éÂÖ∂‰ªñÊ®°ÂûãËøõË°åÂü∫ÂáÜÊµãËØïÔºåÂÆÉÊõæË¢´ÊöÇÊó∂Áß∞‰∏∫[\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209)\n\n"},{"lang":"zh","group":"models","slug":"models/llama-31-nemotron-70b-instruct","frontmatter":{"title":"Nvidia: Llama 3.1 Nemotron 70B Instruct","meta_title":"Nvidia: Llama 3.1 Nemotron 70B Instruct","description":"Nvidia: Llama 3.1 Nemotron 70B Instruct","date":"2024-10-15T00:00:00.000Z","image":"https://img.rifx.online/logo/nvidia.svg","categories":["text 2 text"],"author":"nvidia","tags":["Programming","Natural Language Processing","Machine Learning","Generative AI","Ethics"],"draft":false,"id":"llama-3.1-nemotron-70b-instruct","context":131072,"input":3.5e-7,"output":4e-7,"img":0,"request":0,"last_updated":"2024-10-15T00:00:00.000Z","slug":"models/llama-31-nemotron-70b-instruct"},"content":"\nNVIDIAÁöÑLlama 3.1 Nemotron 70BÊòØ‰∏Ä‰∏™Êó®Âú®ÁîüÊàêÁ≤æÁ°ÆÂíåÊúâÁî®ÂìçÂ∫îÁöÑËØ≠Ë®ÄÊ®°Âûã„ÄÇÂà©Áî®[Llama 3.1 70B](/models/meta-llama/llama-3.1-70b-instruct)Êû∂ÊûÑÂíåÂü∫‰∫é‰∫∫Á±ªÂèçÈ¶àÁöÑÂº∫ÂåñÂ≠¶‰π†ÔºàRLHFÔºâÔºåÂÆÉÂú®Ëá™Âä®ÂØπÈΩêÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇËØ•Ê®°Âûã‰∏ì‰∏∫ÈúÄË¶ÅÈ´òÂáÜÁ°ÆÊÄß‰ª•Êèê‰æõÂ∏ÆÂä©ÂíåÁîüÊàêÂìçÂ∫îÁöÑÂ∫îÁî®ËÄåËÆæËÆ°ÔºåÈÄÇÂêàÂ§ÑÁêÜÂ§ö‰∏™È¢ÜÂüüÁöÑÂ§öÊ†∑Áî®Êà∑Êü•ËØ¢„ÄÇ\n\n‰ΩøÁî®Ê≠§Ê®°ÂûãÈ°ªÈÅµÂæ™[MetaÁöÑÂèØÊé•Âèó‰ΩøÁî®ÊîøÁ≠ñ](https://www.llama.com/llama3/use-policy/)„ÄÇ\n\n"},{"lang":"zh","group":"models","slug":"models/llama-32-11b-vision-instruct","frontmatter":{"title":"Meta: Llama 3.2 11B Vision Instruct","meta_title":"Meta: Llama 3.2 11B Vision Instruct","description":"Meta: Llama 3.2 11B Vision Instruct","date":"2024-09-25T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text image 2 text"],"author":"meta-llama","tags":["Natural Language Processing","Computer Vision","Machine Learning","Generative AI","Data Science"],"draft":false,"id":"llama-3.2-11b-vision-instruct","context":131072,"input":5.5e-8,"output":5.5e-8,"img":0.000079475,"request":0,"last_updated":"2024-09-25T00:00:00.000Z","slug":"models/llama-32-11b-vision-instruct"},"content":"\nLlama 3.2 11B Vision ÊòØ‰∏Ä‰∏™ÂÖ∑Êúâ 110 ‰∫øÂèÇÊï∞ÁöÑÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºåÊó®Âú®Â§ÑÁêÜÁªìÂêàËßÜËßâÂíåÊñáÊú¨Êï∞ÊçÆÁöÑ‰ªªÂä°„ÄÇÂÆÉÂú®ÂõæÂÉèÊèèËø∞ÂíåËßÜËßâÈóÆÁ≠îÁ≠â‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂº•Âêà‰∫ÜËØ≠Ë®ÄÁîüÊàê‰∏éËßÜËßâÊé®ÁêÜ‰πãÈó¥ÁöÑÂ∑ÆË∑ù„ÄÇËØ•Ê®°ÂûãÂú®‰∏Ä‰∏™Â∫ûÂ§ßÁöÑÂõæÂÉè-ÊñáÊú¨ÂØπÊï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜÈ¢ÑËÆ≠ÁªÉÔºåÂú®Â§çÊùÇÁöÑÈ´òÁ≤æÂ∫¶ÂõæÂÉèÂàÜÊûê‰∏≠Ë°®Áé∞ËâØÂ•Ω„ÄÇ\n\nÂÆÉÂ∞ÜËßÜËßâÁêÜËß£‰∏éËØ≠Ë®ÄÂ§ÑÁêÜÁõ∏ÁªìÂêàÁöÑËÉΩÂäõÔºå‰ΩøÂÖ∂Êàê‰∏∫ÈúÄË¶ÅÂÖ®Èù¢ËßÜËßâËØ≠Ë®Ä AI Â∫îÁî®ÁöÑË°å‰∏öÁöÑÁêÜÊÉ≥Ëß£ÂÜ≥ÊñπÊ°àÔºå‰æãÂ¶ÇÂÜÖÂÆπÂàõ‰Ωú„ÄÅAI È©±Âä®ÁöÑÂÆ¢Êà∑ÊúçÂä°ÂíåÁ†îÁ©∂„ÄÇ\n\nÁÇπÂáªÊ≠§Â§ÑÊü•Áúã [ÂéüÂßãÊ®°ÂûãÂç°](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md)„ÄÇ\n\n‰ΩøÁî®Ê≠§Ê®°ÂûãÈ°ªÈÅµÂæ™ [Meta ÁöÑÂèØÊé•Âèó‰ΩøÁî®ÊîøÁ≠ñ](https://www.llama.com/llama3/use-policy/)„ÄÇ\n\n"},{"lang":"zh","group":"models","slug":"models/llama-32-1b-instruct","frontmatter":{"title":"Meta: Llama 3.2 1B Instruct","meta_title":"Meta: Llama 3.2 1B Instruct","description":"Meta: Llama 3.2 1B Instruct","date":"2024-09-25T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text 2 text"],"author":"meta-llama","tags":["Natural Language Processing","Programming","Technology","Machine Learning","Generative AI"],"draft":false,"id":"llama-3.2-1b-instruct","context":131072,"input":1e-8,"output":2e-8,"img":0,"request":0,"last_updated":"2024-09-25T00:00:00.000Z","slug":"models/llama-32-1b-instruct"},"content":"\nLlama 3.2 1B ÊòØ‰∏Ä‰∏™Êã•Êúâ10‰∫øÂèÇÊï∞ÁöÑËØ≠Ë®ÄÊ®°ÂûãÔºå‰∏ìÊ≥®‰∫éÈ´òÊïàÊâßË°åËá™ÁÑ∂ËØ≠Ë®Ä‰ªªÂä°ÔºåÂ¶ÇÊëòË¶Å„ÄÅÂØπËØùÂíåÂ§öËØ≠Ë®ÄÊñáÊú¨ÂàÜÊûê„ÄÇÂÖ∂ËæÉÂ∞èÁöÑËßÑÊ®°‰ΩøÂÖ∂ËÉΩÂ§üÂú®‰ΩéËµÑÊ∫êÁéØÂ¢É‰∏≠È´òÊïàËøêË°åÔºåÂêåÊó∂‰øùÊåÅÂº∫Â§ßÁöÑ‰ªªÂä°ÊÄßËÉΩ„ÄÇ\n\nÊîØÊåÅÂÖ´ÁßçÊ†∏ÂøÉËØ≠Ë®ÄÔºåÂπ∂ÂèØÈíàÂØπÊõ¥Â§öËØ≠Ë®ÄËøõË°åÂæÆË∞ÉÔºåLlama 1.3B ÈùûÂ∏∏ÈÄÇÂêàÂØªÊ±ÇËΩªÈáèÁ∫ß‰ΩÜÂº∫Â§ßÁöÑ AI Ëß£ÂÜ≥ÊñπÊ°àÁöÑ‰ºÅ‰∏öÊàñÂºÄÂèëËÄÖÔºåËøô‰∫õËß£ÂÜ≥ÊñπÊ°àËÉΩÂ§üÂú®Â§öÊ†∑ÂåñÁöÑÂ§öËØ≠Ë®ÄÁéØÂ¢É‰∏≠ËøêË°åÔºåËÄå‰∏çÈúÄË¶ÅÂ§ßÂûãÊ®°ÂûãÁöÑÈ´òËÆ°ÁÆóÈúÄÊ±Ç„ÄÇ\n\nÁÇπÂáªÊ≠§Â§ÑÊü•Áúã [ÂéüÂßãÊ®°ÂûãÂç°](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md)„ÄÇ\n\n‰ΩøÁî®Ê≠§Ê®°ÂûãÈúÄÈÅµÂæ™ [Meta ÁöÑÂèØÊé•Âèó‰ΩøÁî®ÊîøÁ≠ñ](https://www.llama.com/llama3/use-policy/)„ÄÇ\n\n"},{"lang":"zh","group":"models","slug":"models/llama-32-90b-vision-instruct","frontmatter":{"title":"Meta: Llama 3.2 90B Vision Instruct","meta_title":"Meta: Llama 3.2 90B Vision Instruct","description":"Meta: Llama 3.2 90B Vision Instruct","date":"2024-09-25T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text image 2 text"],"author":"meta-llama","tags":["Natural Language Processing","Computer Vision","Machine Learning","Data Science","Generative AI"],"draft":false,"id":"llama-3.2-90b-vision-instruct","context":131072,"input":3.5e-7,"output":4e-7,"img":0.00050575,"request":0,"last_updated":"2024-09-25T00:00:00.000Z","slug":"models/llama-32-90b-vision-instruct"},"content":"\nLlama 90B VisionÊ®°ÂûãÊòØ‰∏ÄÊ¨æÈ°∂Á∫ßÁöÑ90‰∫øÂèÇÊï∞Â§öÊ®°ÊÄÅÊ®°ÂûãÔºåÊó®Âú®Â∫îÂØπÊúÄÂÖ∑ÊåëÊàòÊÄßÁöÑËßÜËßâÊé®ÁêÜÂíåËØ≠Ë®Ä‰ªªÂä°„ÄÇÂÆÉÂú®ÂõæÂÉèÊèèËø∞„ÄÅËßÜËßâÈóÆÁ≠îÂíåÈ´òÁ∫ßÂõæÂÉè-ÊñáÊú¨ÁêÜËß£ÊñπÈù¢Êèê‰æõÊó†‰∏é‰º¶ÊØîÁöÑÂáÜÁ°ÆÊÄß„ÄÇËØ•Ê®°ÂûãÂú®Â∫ûÂ§ßÁöÑÂ§öÊ®°ÊÄÅÊï∞ÊçÆÈõÜ‰∏äËøõË°åÈ¢ÑËÆ≠ÁªÉÔºåÂπ∂ÈÄöËøá‰∫∫Á±ªÂèçÈ¶àËøõË°åÂæÆË∞ÉÔºå‰∏ì‰∏∫Â§ÑÁêÜÊúÄËãõÂàªÁöÑÂü∫‰∫éÂõæÂÉèÁöÑAI‰ªªÂä°ËÄåËÆæËÆ°„ÄÇ\n\nÊ≠§Ê®°ÂûãÈùûÂ∏∏ÈÄÇÂêàÈúÄË¶ÅÂ∞ñÁ´ØÂ§öÊ®°ÊÄÅAIËÉΩÂäõÁöÑË°å‰∏öÔºåÂ∞§ÂÖ∂ÊòØÈÇ£‰∫õÂ§ÑÁêÜÂ§çÊùÇÂÆûÊó∂ËßÜËßâÂíåÊñáÊú¨ÂàÜÊûêÁöÑË°å‰∏ö„ÄÇ\n\nÁÇπÂáªÊ≠§Â§ÑÊü•Áúã[ÂéüÂßãÊ®°ÂûãÂç°Áâá](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md)„ÄÇ\n\n‰ΩøÁî®Ê≠§Ê®°ÂûãÈúÄÈÅµÂÆà[MetaÁöÑÂèØÊé•Âèó‰ΩøÁî®ÊîøÁ≠ñ](https://www.llama.com/llama3/use-policy/)„ÄÇ\n\n"},{"lang":"zh","group":"models","slug":"models/ministral-8b","frontmatter":{"title":"Ministral 8B","meta_title":"Ministral 8B","description":"Ministral 8B","date":"2024-10-17T00:00:00.000Z","image":"https://img.rifx.online/logo/mistral.png","categories":["text 2 text"],"author":"mistralai","tags":["Technology","Machine Learning","Data Science","Generative AI","Ethics"],"draft":false,"id":"ministral-8b","context":128000,"input":1e-7,"output":1e-7,"img":0,"request":0,"last_updated":"2024-10-19T04:54:11.000Z","slug":"models/ministral-8b"},"content":"\nMinistral 8B ÊòØ‰∏Ä‰∏™ÂÖ∑Êúâ 8B ÂèÇÊï∞ÁöÑÊ®°ÂûãÔºåÈááÁî®Áã¨ÁâπÁöÑ‰∫§ÈîôÊªëÂä®Á™óÂè£Ê≥®ÊÑèÂäõÊ®°ÂºèÔºå‰ª•ÂÆûÁé∞Êõ¥Âø´„ÄÅÊõ¥ËäÇÁúÅÂÜÖÂ≠òÁöÑÊé®ÁêÜ„ÄÇËØ•Ê®°Âûã‰∏ì‰∏∫ËæπÁºò‰ΩøÁî®Ê°à‰æãËÆæËÆ°ÔºåÊîØÊåÅÊúÄÈïø 128k ÁöÑ‰∏ä‰∏ãÊñáÈïøÂ∫¶ÔºåÂπ∂Âú®Áü•ËØÜÂíåÊé®ÁêÜ‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇÂÆÉÂú®‰Ωé‰∫é 10B ÁöÑÁ±ªÂà´‰∏≠‰ºò‰∫éÂêåÁ±ª‰∫ßÂìÅÔºåÈùûÂ∏∏ÈÄÇÂêà‰ΩéÂª∂Ëøü„ÄÅÊ≥®ÈáçÈöêÁßÅÁöÑÂ∫îÁî®„ÄÇ\n\n"},{"lang":"zh","group":"models","slug":"models/o1-mini","frontmatter":{"title":"OpenAI: o1-mini","meta_title":"OpenAI: o1-mini","description":"OpenAI: o1-mini","date":"2024-09-12T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"openai","tags":["Programming","Science","Natural Language Processing","Machine Learning","Data Science"],"draft":false,"id":"o1-mini","context":128000,"input":0.000003,"output":0.000012,"img":0,"request":0,"last_updated":"2024-09-12T00:00:00.000Z","slug":"models/o1-mini"},"content":"\nOpenAIÊúÄÊñ∞‰∏îÊúÄÂº∫Â§ßÁöÑÊ®°ÂûãÁ≥ªÂàóo1Êó®Âú®Âú®ÂìçÂ∫î‰πãÂâçËä±Êõ¥Â§öÊó∂Èó¥ÊÄùËÄÉ„ÄÇ\n\no1Ê®°ÂûãÁªèËøá‰ºòÂåñÔºåÈÄÇÁî®‰∫éÊï∞Â≠¶„ÄÅÁßëÂ≠¶„ÄÅÁºñÁ®ãÂèäÂÖ∂‰ªñSTEMÁõ∏ÂÖ≥‰ªªÂä°„ÄÇÂÆÉ‰ª¨Âú®Áâ©ÁêÜ„ÄÅÂåñÂ≠¶ÂíåÁîüÁâ©Â≠¶ÁöÑÂü∫ÂáÜÊµãËØï‰∏≠ÂßãÁªàÂ±ïÁé∞Âá∫ÂçöÂ£´Á∫ßÁöÑÂáÜÁ°ÆÊÄß„ÄÇ‰∫ÜËß£Êõ¥Â§ö‰ø°ÊÅØÔºåËØ∑Êü•Áúã[ÂèëÂ∏ÉÂÖ¨Âëä](https://openai.com/o1)„ÄÇ\n\nÊ≥®ÊÑèÔºöËØ•Ê®°ÂûãÁõÆÂâçÂ§Ñ‰∫éÂÆûÈ™åÈò∂ÊÆµÔºå‰∏çÈÄÇÂêàÁîü‰∫ß‰ΩøÁî®ÔºåÂπ∂ÂèØËÉΩÂèóÂà∞‰∏•Ê†ºÁöÑÈÄüÁéáÈôêÂà∂„ÄÇ\n\n"},{"lang":"zh","group":"models","slug":"models/o1-preview","frontmatter":{"title":"OpenAI: o1-preview","meta_title":"OpenAI: o1-preview","description":"OpenAI: o1-preview","date":"2024-09-12T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"openai","tags":["Programming","Science","Natural Language Processing","Machine Learning","Data Science"],"draft":false,"id":"o1-preview","context":128000,"input":0.000015,"output":0.00006,"img":0,"request":0,"last_updated":"2024-09-12T00:00:00.000Z","slug":"models/o1-preview"},"content":"\nOpenAIÊúÄÊñ∞‰∏îÊúÄÂº∫Â§ßÁöÑÊ®°ÂûãÁ≥ªÂàóo1Êó®Âú®Âú®ÂìçÂ∫î‰πãÂâçËä±Êõ¥Â§öÊó∂Èó¥ÊÄùËÄÉ„ÄÇ\n\no1Ê®°ÂûãÁªèËøá‰ºòÂåñÔºåÈÄÇÁî®‰∫éÊï∞Â≠¶„ÄÅÁßëÂ≠¶„ÄÅÁºñÁ®ãÂíåÂÖ∂‰ªñSTEMÁõ∏ÂÖ≥‰ªªÂä°„ÄÇÂÆÉ‰ª¨Âú®Áâ©ÁêÜ„ÄÅÂåñÂ≠¶ÂíåÁîüÁâ©Â≠¶ÁöÑÂü∫ÂáÜÊµãËØï‰∏≠ÂßãÁªàË°®Áé∞Âá∫ÂçöÂ£´Á∫ßÁöÑÂáÜÁ°ÆÊÄß„ÄÇÊúâÂÖ≥Êõ¥Â§ö‰ø°ÊÅØÔºåËØ∑ÂèÇÈòÖ[ÂèëÂ∏ÉÂÖ¨Âëä](https://openai.com/o1)„ÄÇ\n\nÊ≥®ÊÑèÔºöËØ•Ê®°ÂûãÁõÆÂâçÂ§Ñ‰∫éÂÆûÈ™åÈò∂ÊÆµÔºå‰∏çÈÄÇÂêàÁîü‰∫ß‰ΩøÁî®Ê°à‰æãÔºåÂπ∂‰∏îÂèØËÉΩ‰ºöÂèóÂà∞‰∏•Ê†ºÁöÑÈÄüÁéáÈôêÂà∂„ÄÇ\n\n"},{"lang":"zh","group":"models","slug":"models/qwen-2-7b-instruct","frontmatter":{"title":"Qwen 2 7B Instruct","meta_title":"Qwen 2 7B Instruct","description":"Qwen 2 7B Instruct","date":"2024-07-16T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"qwen","tags":["Natural Language Processing","Programming","Machine Learning","Data Science","Ethics"],"draft":false,"id":"qwen-2-7b-instruct","context":32768,"input":5.4e-8,"output":5.4e-8,"img":0,"request":0,"last_updated":"2024-07-16T00:00:00.000Z","slug":"models/qwen-2-7b-instruct"},"content":"\nQwen2 7B ÊòØ‰∏Ä‰∏™Âü∫‰∫éÂèòÊç¢Âô®ÁöÑÊ®°ÂûãÔºåÊìÖÈïøËØ≠Ë®ÄÁêÜËß£„ÄÅÂ§öËØ≠Ë®ÄËÉΩÂäõ„ÄÅÁºñÁ†Å„ÄÅÊï∞Â≠¶ÂíåÊé®ÁêÜ„ÄÇ\n\nÂÆÉÂÖ∑Êúâ SwiGLU ÊøÄÊ¥ª„ÄÅÊ≥®ÊÑèÂäõ QKV ÂÅèÁΩÆÂíåÁªÑÊü•ËØ¢Ê≥®ÊÑèÂäõ„ÄÇÂÆÉÂú®Â§ßÈáèÊï∞ÊçÆ‰∏äËøõË°åÈ¢ÑËÆ≠ÁªÉÔºåÂπ∂ÁªèËøáÁõëÁù£ÂæÆË∞ÉÂíåÁõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñ„ÄÇ\n\nÊúâÂÖ≥Êõ¥Â§öËØ¶ÁªÜ‰ø°ÊÅØÔºåËØ∑ÂèÇÈòÖÊ≠§ [ÂçöÂÆ¢ÊñáÁ´†](https://qwenlm.github.io/blog/qwen2/) Âíå [GitHub ‰ªìÂ∫ì](https://github.com/QwenLM/Qwen2)„ÄÇ\n\n‰ΩøÁî®Ê≠§Ê®°ÂûãÈ°ªÈÅµÂæ™ [Âêå‰πâÂçÉÈóÆËÆ∏ÂèØËØÅÂçèËÆÆ](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE)„ÄÇ\n\n"},{"lang":"zh","group":"models","slug":"models/qwen-2-vl-72b-instruct","frontmatter":{"title":"Qwen2-VL 72B Instruct","meta_title":"Qwen2-VL 72B Instruct","description":"Qwen2-VL 72B Instruct","date":"2024-09-18T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text image 2 text"],"author":"qwen","tags":["Natural Language Processing","Computer Vision","Robotics","Machine Learning"],"draft":false,"id":"qwen-2-vl-72b-instruct","context":32768,"input":4e-7,"output":4e-7,"img":0.000578,"request":0,"last_updated":"2024-09-18T00:00:00.000Z","slug":"models/qwen-2-vl-72b-instruct"},"content":"\nQwen2 VL 72B ÊòØÊù•Ëá™ Qwen Team ÁöÑÂ§öÊ®°ÊÄÅ LLMÔºåÂÖ∑Êúâ‰ª•‰∏ãÂÖ≥ÈîÆÂ¢ûÂº∫ÂäüËÉΩÔºö\n\n- ÂØπÂêÑÁßçÂàÜËæ®ÁéáÂíåÊØî‰æãÂõæÂÉèÁöÑÊúÄÂÖàËøõÁêÜËß£ÔºöQwen2-VL Âú®ËßÜËßâÁêÜËß£Âü∫ÂáÜÊµãËØï‰∏≠ÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåÂåÖÊã¨ MathVista„ÄÅDocVQA„ÄÅRealWorldQA„ÄÅMTVQA Á≠â„ÄÇ\n\n- ÁêÜËß£Ë∂ÖËøá 20 ÂàÜÈíüÁöÑËßÜÈ¢ëÔºöQwen2-VL ÂèØ‰ª•ÁêÜËß£Ë∂ÖËøá 20 ÂàÜÈíüÁöÑËßÜÈ¢ëÔºå‰ª•ËøõË°åÈ´òË¥®ÈáèÁöÑËßÜÈ¢ëÈóÆÁ≠î„ÄÅÂØπËØù„ÄÅÂÜÖÂÆπÂàõ‰ΩúÁ≠â„ÄÇ\n\n- ËÉΩÂ§üÊìç‰ΩúÊÇ®ÁöÑÊâãÊú∫„ÄÅÊú∫Âô®‰∫∫Á≠âÁöÑÊô∫ËÉΩ‰ΩìÔºöÂá≠ÂÄüÂ§çÊùÇÊé®ÁêÜÂíåÂÜ≥Á≠ñËÉΩÂäõÔºåQwen2-VL ÂèØ‰ª•‰∏éÊâãÊú∫„ÄÅÊú∫Âô®‰∫∫Á≠âËÆæÂ§áÈõÜÊàêÔºåÂÆûÁé∞Âü∫‰∫éËßÜËßâÁéØÂ¢ÉÂíåÊñáÊú¨Êåá‰ª§ÁöÑËá™Âä®Êìç‰Ωú„ÄÇ\n\n- Â§öËØ≠Ë®ÄÊîØÊåÅÔºö‰∏∫‰∫ÜÊúçÂä°ÂÖ®ÁêÉÁî®Êà∑ÔºåÈô§‰∫ÜËã±ËØ≠Âíå‰∏≠ÊñáÔºåQwen2-VL Áé∞Âú®ËøòÊîØÊåÅÁêÜËß£ÂõæÂÉè‰∏≠‰∏çÂêåËØ≠Ë®ÄÁöÑÊñáÊú¨ÔºåÂåÖÊã¨Â§ßÂ§öÊï∞Ê¨ßÊ¥≤ËØ≠Ë®Ä„ÄÅÊó•ËØ≠„ÄÅÈü©ËØ≠„ÄÅÈòøÊãâ‰ºØËØ≠„ÄÅË∂äÂçóËØ≠Á≠â„ÄÇ\n\nÊúâÂÖ≥Êõ¥Â§öËØ¶ÁªÜ‰ø°ÊÅØÔºåËØ∑ÂèÇÈòÖÊ≠§ [ÂçöÂÆ¢ÊñáÁ´†](https://qwenlm.github.io/blog/qwen2-vl/) Âíå [GitHub ‰ªìÂ∫ì](https://github.com/QwenLM/Qwen2-VL)„ÄÇ\n\n‰ΩøÁî®Ê≠§Ê®°ÂûãÂèó [ÈÄö‰πâÂçÉÈóÆËÆ∏ÂèØÂçèËÆÆ](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE) ÁöÑÁ∫¶Êùü„ÄÇ\n\n"},{"lang":"zh","group":"models","slug":"models/qwen-25-72b-instruct","frontmatter":{"title":"Qwen2.5 72B Instruct","meta_title":"Qwen2.5 72B Instruct","description":"Qwen2.5 72B Instruct","date":"2024-09-19T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"qwen","tags":["Programming","Natural Language Processing","Chatbots","Machine Learning","Data Science"],"draft":false,"id":"qwen-2.5-72b-instruct","context":131072,"input":3.5e-7,"output":4e-7,"img":0,"request":0,"last_updated":"2024-09-19T00:00:00.000Z","slug":"models/qwen-25-72b-instruct"},"content":"\nQwen2.5 72B ÊòØ Qwen Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊúÄÊñ∞Á≥ªÂàó„ÄÇQwen2.5 Âú® Qwen2 ÁöÑÂü∫Á°Ä‰∏äÂ∏¶Êù•‰∫Ü‰ª•‰∏ãÊîπËøõÔºö\n\n- Áü•ËØÜÊòæËëóÂ¢ûÂä†ÔºåÂπ∂Âú®ÁºñÁ†ÅÂíåÊï∞Â≠¶ËÉΩÂäõ‰∏äÊúâ‰∫ÜÂæàÂ§ßÊèêÂçáÔºåËøôÂæóÁõä‰∫éÊàë‰ª¨Âú®Ëøô‰∫õÈ¢ÜÂüüÁöÑ‰∏ì‰∏ö‰∏ìÂÆ∂Ê®°Âûã„ÄÇ\n\n- Âú®ÈÅµÂæ™Êåá‰ª§„ÄÅÁîüÊàêÈïøÊñáÊú¨ÔºàË∂ÖËøá 8K tokensÔºâ„ÄÅÁêÜËß£ÁªìÊûÑÂåñÊï∞ÊçÆÔºà‰æãÂ¶ÇÔºåË°®Ê†ºÔºâ‰ª•ÂèäÁîüÊàêÁªìÊûÑÂåñËæìÂá∫ÔºàÁâπÂà´ÊòØ JSONÔºâÊñπÈù¢ÊúâÊòæËëóÊîπËøõ„ÄÇÂØπÁ≥ªÁªüÊèêÁ§∫ÁöÑÂ§öÊ†∑ÊÄßÊõ¥Âä†Âº∫ÈüßÔºåÂ¢ûÂº∫‰∫ÜËßíËâ≤ÊâÆÊºîÁöÑÂÆûÁé∞ÂíåËÅäÂ§©Êú∫Âô®‰∫∫ÁöÑÊù°‰ª∂ËÆæÁΩÆ„ÄÇ\n\n- ÊîØÊåÅÊúÄÈïø 128K tokens ÁöÑÈïø‰∏ä‰∏ãÊñáÔºåÂπ∂ÂèØ‰ª•ÁîüÊàêÊúÄÂ§ö 8K tokens„ÄÇ\n\n- ÊîØÊåÅË∂ÖËøá 29 ÁßçËØ≠Ë®ÄÔºåÂåÖÊã¨‰∏≠Êñá„ÄÅËã±Êñá„ÄÅÊ≥ïËØ≠„ÄÅË•øÁè≠ÁâôËØ≠„ÄÅËë°ËêÑÁâôËØ≠„ÄÅÂæ∑ËØ≠„ÄÅÊÑèÂ§ßÂà©ËØ≠„ÄÅ‰øÑËØ≠„ÄÅÊó•ËØ≠„ÄÅÈü©ËØ≠„ÄÅË∂äÂçóËØ≠„ÄÅÊ≥∞ËØ≠„ÄÅÈòøÊãâ‰ºØËØ≠Á≠â„ÄÇ\n\n‰ΩøÁî®Ê≠§Ê®°ÂûãÈ°ªÈÅµÂÆà [Âêå‰πâÂçÉÈóÆËÆ∏ÂèØÂçèËÆÆ](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE)„ÄÇ\n\n"},{"lang":"zh","group":"models","slug":"models/qwen-25-7b-instruct","frontmatter":{"title":"Qwen2.5 7B Instruct","meta_title":"Qwen2.5 7B Instruct","description":"Qwen2.5 7B Instruct","date":"2024-10-16T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"qwen","tags":["Programming","Natural Language Processing","Chatbots","Machine Learning","Data Science"],"draft":false,"id":"qwen-2.5-7b-instruct","context":131072,"input":2.7e-7,"output":2.7e-7,"img":0,"request":0,"last_updated":"2024-10-16T00:00:00.000Z","slug":"models/qwen-25-7b-instruct"},"content":"\nQwen2.5 7B ÊòØ Qwen Â§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÊúÄÊñ∞Á≥ªÂàó„ÄÇQwen2.5 Âú® Qwen2 ÁöÑÂü∫Á°Ä‰∏äÂ∏¶Êù•‰∫Ü‰ª•‰∏ãÊîπËøõÔºö\n\n- Áü•ËØÜÊòæËëóÂ¢ûÂä†ÔºåÂπ∂Âú®ÁºñÁ†ÅÂíåÊï∞Â≠¶ÊñπÈù¢ÁöÑËÉΩÂäõÂ§ßÂπÖÊèêÂçáÔºåËøôÂæóÁõä‰∫éÊàë‰ª¨Âú®Ëøô‰∫õÈ¢ÜÂüüÁöÑ‰∏ì‰∏öÊ®°Âûã„ÄÇ\n\n- Âú®ÈÅµÂæ™Êåá‰ª§„ÄÅÁîüÊàêÈïøÊñáÊú¨ÔºàË∂ÖËøá 8K tokensÔºâ„ÄÅÁêÜËß£ÁªìÊûÑÂåñÊï∞ÊçÆÔºà‰æãÂ¶ÇÔºåË°®Ê†ºÔºâ‰ª•ÂèäÁîüÊàêÁªìÊûÑÂåñËæìÂá∫ÔºåÁâπÂà´ÊòØ JSON ÊñπÈù¢ÊúâÊòæËëóÊîπËøõ„ÄÇÂØπÁ≥ªÁªüÊèêÁ§∫ÁöÑÂ§öÊ†∑ÊÄßÊõ¥ÂÖ∑ÈüßÊÄßÔºåÂ¢ûÂº∫‰∫ÜËßíËâ≤ÊâÆÊºîÁöÑÂÆûÁé∞ÂíåËÅäÂ§©Êú∫Âô®‰∫∫ÁöÑÊù°‰ª∂ËÆæÁΩÆ„ÄÇ\n\n- ÈïøÊñáÊú¨ÊîØÊåÅÈ´òËææ 128K tokensÔºåÂπ∂‰∏îÂèØ‰ª•ÁîüÊàêÊúÄÂ§ö 8K tokens„ÄÇ\n\n- ÊîØÊåÅË∂ÖËøá 29 ÁßçËØ≠Ë®ÄÔºåÂåÖÊã¨‰∏≠Êñá„ÄÅËã±Êñá„ÄÅÊ≥ïÊñá„ÄÅË•øÁè≠ÁâôÊñá„ÄÅËë°ËêÑÁâôÊñá„ÄÅÂæ∑Êñá„ÄÅÊÑèÂ§ßÂà©Êñá„ÄÅ‰øÑÊñá„ÄÅÊó•Êñá„ÄÅÈü©Êñá„ÄÅË∂äÂçóÊñá„ÄÅÊ≥∞Êñá„ÄÅÈòøÊãâ‰ºØÊñáÁ≠â„ÄÇ\n\n‰ΩøÁî®Ê≠§Ê®°ÂûãÈ°ªÈÅµÂÆà [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE)„ÄÇ\n\n"}]
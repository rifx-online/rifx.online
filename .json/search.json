[{"lang":"en","group":"blog","slug":"blog/a-month-with-cursor-and-claude-dev-my-thoughts-5c41ae0d4467","frontmatter":{"title":"A Month with Cursor and Claude-Dev: My Thoughts","meta_title":"A Month with Cursor and Claude-Dev: My Thoughts","description":"I’ve been using two new tools recently- Cursor and Claude-Dev -both of which have been getting a fair bit of attention in the developer…","date":"2024-11-04T12:32:52.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*i28vK12LJ6XTpSwrKiamwA.png","categories":["Programming","Technology","Generative AI"],"author":"Rifx.Online","tags":["Cursor","Claude-Dev","Cline","autocomplete","debugging"],"draft":false,"slug":"blog/a-month-with-cursor-and-claude-dev-my-thoughts-5c41ae0d4467"},"content":"\n\n\nI’ve been using two new tools recently\\- **Cursor** and **Claude\\-Dev** \\-both of which have been getting a fair bit of attention in the developer community. They’re each built to make coding faster and more intuitive through AI\\-powered assistance, but they take different approaches and have their own strengths and weaknesses. After using both for about a month, I thought it was time to sit down and reflect on where they shine and where they still need some work.\n\nLet’s start with Cursor.\n\n## Cursor: Familiar but Faster\n\nCursor is a fork of VSCode, which, if you’re already a VSCode user like I am, makes it very easy to slip into. I didn’t need to rebuild my environment from scratch or deal with setting up keybindings. Everything that worked in VSCode worked in Cursor right out of the box\\-my extensions, settings, and keymappings carried over without a hitch. The transition was almost invisible, except for one key difference: the AI autocomplete is much faster. In fact, **in my experience**, it’s around 10 times faster than GitHub Copilot.\n\nNow, “10 times faster” isn’t a number I pulled from benchmarks\\-it’s just what it feels like after using it for a while. When you’re typing code, and Cursor is predicting your next move, it doesn’t feel like the AI is lagging behind or playing catch\\-up. Instead, it’s right there with you, which helps keep you in flow. I was surprised at how much more productive I felt when I wasn’t waiting for Copilot to catch up or pressing tab three times just to get the suggestion I wanted.\n\nCursor also has a nice feature where it embeds and indexes your entire project, making it easier to understand the relationships between files. When you update a file, the index gets updated too, which means the AI has a better grasp of how the pieces of your codebase fit together. This is useful if you’re working across a large codebase with multiple files that depend on each other.\n\n## The Drawbacks\n\nThat said, some of the best features in Cursor are gated behind a subscription. I’m generally not opposed to paying for tools that add real value, but in this case, I was a little disappointed that the most interesting AI features\\-like the multi\\-file editing\\-were part of the premium version. For a tool that’s still fairly new, I wonder if gating these features too early might limit its adoption, especially given how many developers are already paying for GitHub Copilot.\n\nAnother issue I’ve run into with Cursor is that while it’s great at fast, small tasks, it lacks some of the flexibility I need when working with more complex problems. It’s excellent for quick code suggestions and refactoring, but when I needed something that could handle more involved tasks, like reading logs or executing build commands, I found myself looking for something else.\n\n## Claude\\-Dev: The Open\\-Source Underdog\n\nThat’s where **Claude\\-Dev (now called Cline)** comes in. Claude\\-Dev is an open\\-source extension for VSCode, and while it doesn’t have the same level of polish as Cursor, it’s rapidly evolving\\-and in some ways, it’s more powerful. The most striking thing about Claude\\-Dev is that it feels like it’s trying to do more than just suggest code snippets. It’s a tool that can **interact** with your environment in a much deeper way.\n\nFor example, Claude\\-Dev can read your terminal logs, understand linting errors, and even run arbitrary CLI commands. This means that if you ask it why your project isn’t building, it won’t just offer suggestions\\-it will actually go and look at the relevant files, figure out what kind of project you’re working with (Node, React, Python, etc.), and try to build it for you. If there’s an error, it reads the logs, tries to diagnose the problem, and can even apply fixes if needed.\n\nIt’s not perfect, though. In my experience, Claude\\-Dev isn’t as fast as Cursor, especially when it’s making edits. One reason for this is that it rewrites entire files instead of just updating the parts that need to change. This slows things down, and if you’re paying for API tokens (you need to supply an API key from the LLM you want to use), it burns through those faster than it should. I’ve been thinking about contributing to the project to fix this by having it update just the necessary lines via shell commands like `sed`.\n\nOne feature I’ve found particularly interesting is how Claude\\-Dev can use Puppeteer to visually test and update your frontend. You can give it a screenshot of a website, and it will compare that to your app, iterating until it gets your frontend to match the look you’re going for. It’s not the fastest process, but it’s surprisingly good at handling CSS\\-something that, for me at least, is usually a bit of a time sink.\n\n## Where It Falls Short\n\nClaude\\-Dev is definitely a tool for people who are comfortable experimenting with something that’s still a bit rough around the edges. Unlike Cursor, which feels more like a polished product that’s ready for prime time, Claude\\-Dev is more like a powerful tool in active development. It doesn’t always get things right the first time, and it’s slower than I’d like, but it’s constantly improving. The fact that it’s open source and primarily developed by one person makes its pace of innovation even more impressive.\n\n## So Which One Should You Use?\n\nIf you’re looking for a polished, fast experience with a focus on speed and quick suggestions, **Cursor** might be the better choice. It feels snappy, it integrates with your existing VSCode setup, and it keeps you in flow\\-until you hit a paywall. But if you’re okay with that and don’t need the extra bells and whistles, Cursor is a great tool.\n\nOn the other hand, if you want something that can do more than just autocomplete code\\-something that can actually help with debugging, building, and iterating on your project\\- **Claude\\-Dev** is a better fit. It’s more versatile, but also a bit slower and rougher around the edges. If you’re comfortable experimenting and can put up with some quirks, it offers a level of functionality that Cursor just doesn’t have right now.\n\nFor me, **Claude\\-Dev** wins out, mostly because of its deeper integration with my workflow. The ability to read logs, run commands, and iterate until a problem is solved is invaluable, especially when I’m working with unfamiliar codebases. That said, I still find myself using **Cursor** when I need to move fast and don’t want to wait around for the AI to process a command.\n\n## Final Thoughts\n\nBoth Cursor and Claude\\-Dev offer unique benefits, and I think we’re only scratching the surface of what AI\\-driven coding tools can do. There’s a lot of potential here, especially as these tools continue to evolve. I’m excited to see where they go, and I’ll keep experimenting with both to see how they fit into my development workflow.\n\nIn the meantime, I’d recommend trying out both and for yourself. Each tool has its strengths, and you’ll probably find that one fits your style better than the other, depending on what you’re working on.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/a-new-risings-red-star-qwen2-5-is-here-0dffe0fb09ad","frontmatter":{"title":"A new risings Red star: Qwen2.5 is here","meta_title":"A new risings Red star: Qwen2.5 is here","description":"Let’s test together the new born Alibaba Cloud’s generative AI Qwen2.5 with python and llama-cpp","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*zU-XtqK2oMLkvscgxavjdw.png","categories":["Programming","Technology","Education"],"author":"Rifx.Online","tags":["Qwen2.5","multimodal","instruction-following","text-generation","multilingual"],"draft":false,"slug":"blog/a-new-risings-red-star-qwen2-5-is-here-0dffe0fb09ad"},"content":"\n\n\n\n\n### Let’s test together the new born Alibaba Cloud’s generative AI Qwen2.5 with python and llama-cpp\n\n\n\nIn silence, with not so many claims and anticipated announcements, Alibaba Cloud release on September the 19th their flagship model family Qwen2.5.\n\nAlibaba Cloud’s revolutionary journey with Qwen is showing once again strong Leadership through Innovation.\n\nHow? What’s so cool in them? And should we expect?\n\nIn this article we are going to explore the new models and check the performances. As a follow up, in the next article, we are going to use `llama-cpp-python` and the quantized version of qwen2.5–1.5b-instruct, putting the model under 13 NLP tasks test.\n\nIn fact I believe that we are the best Benchmark tool around and we are fully able to evaluate when a model is good for us!\n\nFor now, here what we are going to cover:\n\n\n```python\n- Qwen2.5 family innovation\n- Declared scope, use cases and models\n- Qwen2.5: a party of Foundation models\n- Expanding Reach through Open-Source Contributions\n- Bridging Industries through cutting-edge AI solutions\n- 13 Tasks to prove it worth \n- Future outlook: continued Open-Sourcing\n```\nLet’s dive in!\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*OeQ5qeOzCdl8LPJOZZgTIw.png)\n\n\n## Qwen2.5 family innovation\n\nQwen is the large language model and large multimodal model series of the Qwen Team, Alibaba Group. Just yesterday the large language models have been upgraded to Qwen2.5.\n\nBoth language models and multimodal models are pretrained on large-scale multilingual and multimodal data and post-trained on quality data for aligning to human preferences. Qwen is capable of natural language understanding, text generation, vision understanding, audio understanding, tool use, role play, playing as AI agent, etc.\n\nWith the recent release of Qwen2.5 and additional open-source model releases Alibaba Cloud continues its leadership position to meet rising AI demands from enterprise users. Since June last year, the Qwen family has attracted over 90,000 deployments via Model Studio in various industries including consumer electronics, automobiles, gaming, and more.\n\nQwen also expanded its reach with new models such as Qwen1.5–110B and CodeQwen1.5–7B on platforms like Hugging Face, showcasing Alibaba’s commitment to open-source AI development.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*A4pEOgsLK2PAFtiaGQx1Qw.png)\n\n\n## Declared scope, use cases and models\n\nIn the past three months since Qwen2’s release, numerous developers have built new models on the Qwen2 language models, providing valuable feedback to the entire community, but also to Alibaba Cloud.\n\n\n> During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5.\n\nTheir claims come with facts about the new family of models:\n\n* Dense, easy-to-use, decoder-only language models, available in 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B sizes, and base and instruct variants.\n* Pretrained on our latest large-scale dataset, encompassing up to 18T tokens.\n* Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON.\n* More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n* Context length support up to 128K tokens and can generate up to 8K tokens.\n* Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\n\n## Qwen2.5: a party of Foundation models\n\nAs announced on the [official blog press release](https://qwenlm.github.io/blog/qwen2.5/) on September 19, 2024:\n\n\n> Today, we are excited to introduce the latest addition to the Qwen family: **Qwen2.5**. We are announcing what might be the largest opensource release in history! Let’s get the party started!\n\n\n> Our latest release features the LLMs **Qwen2.5**, along with specialized models for coding, **Qwen2.5-Coder**, and mathematics, **Qwen2.5-Math**.\n\nTo showcase Qwen2.5’s capabilities, the Alibaba Cloud team benchmarked their largest open-source model, **Qwen2.5–72B** — a 72B-parameter dense decoder-only language model — against leading open-source models like Llama-3.1–70B and Mistral-Large-V2.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*-MMFgkkWHa307jNo.jpg)\n\nAll open-weight models are dense, decoder-only language models, available in various sizes, including:\n\n* Qwen2.5: 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B\n* Qwen2.5-Coder: 1.5B, 7B, and 32B on the way\n* Qwen2.5-Math: 1.5B, 7B, and 72B.\n\nAll these open-source models, except for the 3B and 72B variants, are licensed under Apache 2.0. You can find the license files in the respective Hugging Face repositories.\n\n\n> In addition to these models, we offer APIs for our flagship language models: **Qwen-Plus** and **Qwen-Turbo** through Model Studio, and we encourage you to explore them!\n\nBut this is not all!\n\n\n> …we have also open-sourced the **Qwen2-VL-72B**, which features performance enhancements compared to last month’s release.\n\nIn terms of **Qwen2.5**, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to **18 trillion** tokens. Compared to Qwen2, Qwen2.5 has acquired significantly more knowledge (MMLU: 85+) and has greatly improved capabilities in coding (HumanEval 85+) and mathematics (MATH 80+). Additionally, the new models achieve significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*7c7CIbl-WVjazUeE.jpeg)\n\nQwen2.5 models are generally more resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n\nLike Qwen2, the Qwen2.5 language models support up to **128K** tokens and can generate up to **8K** tokens. They also maintain multilingual support for over **29** languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\n\n### Qwen-Coder is the new kid of the family\n\nThe specialized expert language models, namely **Qwen2.5-Coder** for coding and **Qwen2.5-Math** for mathematics, have undergone substantial enhancements compared to their predecessors, CodeQwen1.5 and Qwen2-Math. Specifically, Qwen2.5-Coder has been trained on **5.5 trillion** tokens of code-related data, enabling even smaller coding-specific models to deliver competitive performance against larger language models on coding evaluation benchmarks. Meanwhile, Qwen2.5-Math supports both **Chinese** and **English** and incorporates various reasoning methods, including Chain-of-Thought (CoT), Program-of-Thought (PoT), and Tool-Integrated Reasoning (TIR).\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Nvk4wrcB0SB4Tt-xbCzO6g.png)\n\n\n## Expanding Reach through Open-Source Contributions\n\nAs part of its continuous commitment to the broader community, Alibaba Cloud has made additional steps in releasing various sizes and variants of Qwen models. This includes:\n\n1. **Qwen 0.5 billion parameters**, a foundational version suitable for more traditional applications.2. A compact but potent model tailored specifically for gaming development: **Qwen-VL (vision-language)** optimized with high capabilities.\n\nThese advancements demonstrate Alibaba’s commitment to open-source AI, sharing not only the base versions of Qwen but also significant improvements and new models that are targeting directly the enterprise needs while enhancing their ability to innovate rapidly.\n\nThis aligns closely with a strategic vision where continuous contributions benefit both community members and its own clients as they seek innovative applications across multiple sectors.\n\n\n### Bridging Industries through cutting-edge AI solutions\n\nTo showcase the breadth of Qwen’s capabilities in real-world scenarios, Alibaba Cloud has been at the forefront:\n\n1. **Xiaomi**: the Company is integrating Alibaba’s models into their AI assistant, Xiao Ai, and deploying it within Xiaomi smartphones and electric vehicles to create enhanced features like car infotainment image generation via voice commands.\n\n2. **Perfect World Games**: the integration of Qwen in game development has led to innovative applications including improving plot resolution through dialogue dynamics and real-time content management.\n\nThe collaborations between Alibaba Cloud models and various industries have not only enriched the user experience but also facilitated greater opportunities for growth within these sectors, pushing boundaries that would otherwise be unimaginable without AI advancements.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*ku8o3rq6PHDE8xcc.png)\n\n\n## 13 Tasks to prove it worth\n\nThe 1.5 Billion parameters model is probably the best variant considering complexity, prompt understanding and inference speed.\n\nI will show you my internal testing using only `llama-cpp-python` and a simple terminal interface.\n\nTo do so, I created a list of prompt, covering a series of normally used tasks where you can also assign a vote (from 0 to 5) after every generation. It’s a personal human benchmark.\n\n\n### Requirements\n\nCreate a `venv` (python 3.11+ is required): I tested it on my Mini-PC running Windows 11.\n\n\n```python\n## create the virtual environment\npython -m venv venv\n## activate the venv\nvenv\\Scripts\\activate\n## Install the dependencies \npip install llama-cpp-python==0.2.90 tiktoken\n```\nWe need to download the GGUF file from the [official Qwen2.5 Hugging Face repo](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF). I used the qwen2.5–1.5b-instruct-q5\\_k\\_m.gguf version.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Fa-qFsx9RTFGZmM-vxCEPQ.png)\n\nDownload the file in the main project directory. And we are all set.\n\nThe code used here for the analysis is in my GitHub repository:\n\nI will explain the entire code and the results in the next article. Stay updated!\n\n\n## Future outlook: continued Open-Sourcing\n\nIn future plans, Alibaba has also expressed their commitment to ongoing open-source contributions by releasing smaller variants of Qwen for developers across different sectors. In reality in the Hugging Face community many users have started to fine-tune Qwen for dedicated tasks: I wrote an example in my article on NuExtract: the smaller variant of this model family is based on Qwen2–0.5b!\n\nThese developments in AI technology and model advancements are crucial steps towards leveraging the full potential of large language models like **Qwen** within a variety of industries. With robust adoption rates continuing to grow rapidly through Model Studio, it is clear that Alibaba Cloud has been a pioneer industry leader not only by providing advanced tools but also promoting innovation across enterprises.\n\nOn my side, my outlook are to proceed with internal testing on the new models, specifically on the small ones, up to 3B.\n\nIn the next article I will share with you my method, how to run the models and the prompt templates used for each of the thirteen NLP tasks.\n\nHope you enjoyed the article. If this story provided value and you wish to show a little support, you could:\n\n1. Clap a lot of times for this story\n2. Highlight the parts more relevant to be remembered (it will be easier for you to find them later, and for me to write better articles)\n3. **Join my [totally free weekly Substack newsletter here](https://thepoorgpuguy.substack.com/about)**\n4. Sign up for a Medium membership ($5/month to read unlimited Medium stories)\n5. Follow me on Medium\n6. Read my latest articles <https://medium.com/@fabio.matricardi>\n\nHere are a few more articles to feed your curiosity:\n\nResources references in this article:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Du7V61mEX_yIrfmF.png)\n\nThis story is published on [Generative AI](https://generativeai.pub/). Connect with us on [LinkedIn](https://www.linkedin.com/company/generative-ai-publication) and follow [Zeniteq](https://www.zeniteq.com/) to stay in the loop with the latest AI stories.\n\nSubscribe to our [newsletter](https://www.generativeaipub.com/) and [YouTube](https://www.youtube.com/@generativeaipub) channel to stay updated with the latest news and updates on generative AI. Let’s shape the future of AI together!\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*pvLAT3it1FkdhVU0.png)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/a-practical-guide-for-using-autogen-in-software-applications-8799185d27ee","frontmatter":{"title":"A practical guide for using AutoGen in software applications","meta_title":"A practical guide for using AutoGen in software applications","description":"Update: While this article was written only 4 months ago, AutoGen has since changed quite a bit. I apologize for some things that may be…","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*yrraWH6aGNnbx8p-wfQ1OQ.jpeg","categories":["Programming","Chatbots","Autonomous Systems"],"author":"Rifx.Online","tags":["AutoGen","multi-agent","LLMs","customization","collaboration"],"draft":false,"slug":"blog/a-practical-guide-for-using-autogen-in-software-applications-8799185d27ee"},"content":"\n\n\n\n\n\n*Update: While this article was written only 4 months ago, AutoGen has since changed quite a bit. I apologize for some things that may be outdated in my code examples.*\n\nIf you want to learn about AutoGen, there is [documentation](https://microsoft.github.io/autogen/), [Colab notebooks](https://microsoft.github.io/autogen/docs/Examples), and [a blog](https://microsoft.github.io/autogen/blog). Huge kudos to the AutoGen team for making an AMAZING product, but honestly — after reading all their stuff, I still didn’t know how to use AutoGen outside of a terminal or Jupyter Notebook.\n\nThis article tries to help fill that gap by giving some helpful ways to make AutoGen work in a software application. Here are the topics I’ll go over:\n\n1. Agents aren’t limited to communicating just over the terminal\n2. Registering custom replies\n3. How to include real humans in the conversation in real ways\n4. You can (and should) customize who speaks next\n5. You don’t have to use OpenAI\n6. Functions can be used instead of executing code\n7. Use Agents for organization, not just for conversations\n\nLastly, I’ll go over why I think you should use AutoGen to begin with. Let’s go!\n\n\n## Agents aren’t limited to communicating just over the terminal\n\nYou’ll see everyone demo AutoGen using a terminal or Jupyter Notebook. That’s nice for a demo, but there are other ways these agents can talk to each other.\n\nThere are 2 basic AutoGen classes: [`UserProxyAg`ent](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/user_proxy_agent.py) and [`AssistantAg`ent](https://github.com/microsoft/autogen/blob/main/autogen/agentchat/assistant_agent.py) . They inherit the [`ConversableAg`ent](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py) class, providing just a few different default parameters to the base class.\n\nWhen you see this classic code example:\n\n\n```python\nassistant = autogen.AssistantAgent(\n    name=\"assistant\",\n    llm_config=llm_config\n)\nuser_proxy = autogen.UserProxyAgent(name=\"user_proxy\")\nawait user_proxy.a_initiate_chat(\n    assistant,\n    message=\"\"\"What date is today? Compare the year-to-date gain for META and TESLA.\"\"\",\n)\n```\nwhat happens is that the `UserProxyAgent` will call its own `send` method, which will call `AssistantAgent` ‘s [`rece`ive](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L514) method, passing along the original message. A reply will be generated (more on that below), and `AssistantAgent` will now call its [`s`end](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L351) method, which will then call `UserProxyAgent` ‘s `receive` method, and so forth, until `UserProxyAgent` determines the conversation is terminated (which can be customized via the `is_termination_msg` argument).\n\nMy first “aha” moment was when I realized these agents were classes, and I could create my own custom agent classes that inherit the AutoGen UserProxy/Assistant/Conversable Agent classes, and override any of the default methods. That makes AutoGen very extensible.\n\nI had a use-case where I needed a human who could type in a message (proxied by `UserProxyAgent`) using a chat UI on a website, and I wanted an `AssistantAgent` to respond back to that chat in the UI, and be able to receive more messages from the human user, as though the human was just another agent in this AutoGen conversation.\n\nI could override the `send` and `receive` methods (or `a_send` and `a_receive`), and push/pull over http, websockets, etc. I tried this, and it started to work, but doesn’t scale. Let’s learn a better way.\n\n\n## Registering custom replies\n\nAutoGen has a plugin system that lets you customize how an agent generates a reply. We’re used to seeing examples where AutoGen queries OpenAI for an answer, and uses that as its reply, but you can insert your own methods as well:\n\n\n```python\nclass WeatherAgent(AssistantAgent):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, llm_config=False, **kwargs)\n        self.register_reply(Agent, WeatherAgent.get_weather)\n\n    async def get_weather(\n        self,\n        messages: List[Dict] = [],\n        sender=None,\n        config=None,\n    ) -> Tuple[bool, Union[str, Dict, None]]:\n        last_message = messages[-1][\"content\"]\n        result = await fetch_weather(last_message)\n        return True, result\n\nasync def fetch_weather(city: str) -> str:\n    async with httpx.AsyncClient() as client:\n        result = await client.post(\n            WEATHER_API_URL,\n            json={\"city\": question},\n        )\n        return result.json()\n\nweather_assistant = WeatherAgent(name=\"weather_assistant\")\nuser_proxy = autogen.UserProxyAgent(name=\"user_proxy\")\nawait user_proxy.a_initiate_chat(assistant, message=\"Lehi\")\nprint(weather_assistant.last_message)\n```\nHere, `register_reply` will insert my custom method for getting a reply, and by default, will put this method in `position=0`, meaning it will be the first reply method attempted. That method should return a tuple, where the first item is a boolean indicating if this reply is the one that should be used or whether to try the next registered\\_reply (such as the built-in reply generations using OpenAI — see the full order [here](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L145-L153)).\n\nKnowing about [`register_re`ply](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L155) allows you to customize how replies are retrieved, allow you to start sub multi-agent conversations, etc.\n\n\n## How to include real humans in the conversation in real ways\n\nHere’s one way to do it:\n\n\n```python\n## user makes a POST /query { \"message\": \"What's the weather?\" }\n\n@query_blueprint.route(\"/query\", methods=[\"POST\"])\nasync def post_query():\n  message = request.form.get(\"message\")\n\n  assistant = autogen.AssistantAgent(\n    name=\"assistant\",\n    llm_config=llm_config\n    system_message=\"\"\"You're a helpful assistant.\n    If you need more info, ask the user for anything missing.\"\"\"\n  )\n  user_proxy = autogen.UserProxyAgent(\n    name=\"user_proxy\",\n    human_input_mode=\"NEVER\",\n    code_execution_config=False,\n    is_termination_msg=lambda message: True # Always True\n  )\n  weather_assistant = WeatherAgent(\n    name=\"weather_assistant\",\n    system_message=\"\"\"You're a helpful assistant to get the weather.\n    You fetch weather information, then return it.\"\"\"\n  )\n\n  groupchat = autogen.GroupChat(\n    agents=[assistant, user_proxy, weather_assistant],\n    messages=[]\n  )\n  manager = autogen.GroupChatManager(\n    name=\"Manager\",\n    groupchat=groupchat,\n    llm_config=llm_config,\n  )\n\n  await user_proxy.a_initiate_chat(manager, message=message)\n\n  return groupchat.messages[-1]\n```\nWhat’s going on here?\n\n1. Anytime a message is sent to `user_proxy`, the conversation will end (we’ll resume it later). Why do this? This means the `user_proxy` can actually proxy for the user. Rather than try to answer, it will end the current conversation flow and allow the real human user to respond (by resuming the conversation — see below).\n2. If the assistant needs more info, it’ll ask user\\_proxy, which will end the current conversation.\n\nIn the above code, what is likely to occur is something like this:\n\n1. user\\_proxy -> manager: “What’s the weather?”\n2. assistant -> manager: “The user didn’t specify for which city.”\n3. manager -> user\\_proxy : conversation will end\n\nNow, if the user wants to respond and resume the conversation, how would we do that? There’s lots of ways to do this, here’s just a sample flavor:\n\n\n```python\n## user makes a POST /query { \"message\": \"What's the weather?\" }\n## above posts returns a `history` array\n## user makes a second POST /query { \"message\": \"What's the weather?\", \"history\": history }\n\nclass ResumableGroupChatManager(GroupChatManager):\n    groupchat: GroupChat\n\n    def __init__(self, groupchat, history, **kwargs):\n        self.groupchat = groupchat\n        if history:\n            self.groupchat.messages = history\n\n        super().__init__(groupchat, **kwargs)\n\n        if history:\n            self.restore_from_history(history)\n\n    def restore_from_history(self, history) -> None:\n        for message in history:\n            # broadcast the message to all agents except the speaker.  This idea is the same way GroupChat is implemented in AutoGen for new messages, this method simply allows us to replay old messages first.\n            for agent in self.groupchat.agents:\n                if agent != self:\n                    self.send(message, agent, request_reply=False, silent=True)\n\n@query_blueprint.route(\"/query\", methods=[\"POST\"])\nasync def post_query():\n  message = request.form.get(\"message\")\n\n  assistant = autogen.AssistantAgent(\n    name=\"assistant\",\n    llm_config=llm_config\n    system_message=\"\"\"You're a helpful assistant.\n    If you need more info, ask the user for anything missing.\"\"\"\n  )\n  user_proxy = autogen.UserProxyAgent(\n    name=\"user_proxy\",\n    human_input_mode=\"NEVER\",\n    code_execution_config=False,\n    is_termination_msg=lambda message: True # Always True\n  )\n  weather_assistant = WeatherAgent(\n    name=\"weather_assistant\",\n    system_message=\"\"\"You're a helpful assistant to get the weather.\n    You fetch weather information, then return it.\"\"\"\n  )\n\n  groupchat = autogen.GroupChat(\n    agents=[assistant, user_proxy, weather_assistant],\n    messages=[]\n  )\n  manager = ResumableGroupChatManager(\n    name=\"Manager\",\n    groupchat=groupchat,\n    llm_config=llm_config,\n  )\n\n  await user_proxy.a_initiate_chat(manager, message=message)\n\n  return {\n    \"response\": groupchat.messages[-1],\n    \"history\": groupchat.messages,\n  }\n```\nUsing this approach, you can now include humans as though they were just another agent in the groupchat. Anytime an assistant agent wants human input, they ask user\\_proxy, user\\_proxy then ends the current conversation, allowing the human user to respond with more information, then pick up the conversation where it left off.\n\nThe benefits to this approach are:\n\n* Conversations can include real human input via any means you want (such as over http or websocket).\n* The conversation is stopped while getting human input. This frees up the thread for other conversations and computation.\n* You can persist these conversations across sessions.\n\n\n## You can (and should) customize who speaks next\n\nThis is subjective, but I think you should always customize the way speakers are selected because:\n\n1. You’ll use less tokens (saves both $ and response time)\n2. You can separate the logic that decides who speaks next from the logic that defines the system instructions for each agent\n\n\n```python\nshort_role_descriptions = {\n  \"user_proxy\": \"A proxy for the user\",\n  \"weather_assistant\": \"You can get the weather\",\n  \"planner\": \"You help coordinate the plan. Your turn happens when XYZ, but skip your turn when ABC\"\n}\n\nclass CustomGroupChat(GroupChat):\n    # The default message uses the full system message, which is a long string.  We are overriding this to use a shorter message.\n    def select_speaker_msg(self, agents: List[Agent]):\n        message = f\"\"\"You are in a role play game. The following roles are available:\n        ---\n        {new_line.join([f\"{agent.name}: {short_role_descriptions[agent.name]}\" for agent in agents])}\n        ---\n\n        The role who plays next depends on the conversation.  User_Proxy will star the conversation, and typically Planner would go next.\n\n        Here are some examples\n        ---\n        ... not shown here ...\n        ---\n\n        Read the following conversation.\n        Then select the next role from {', '.join([agent.name for agent in agents])} to play. Only return the role.\"\"\"\n        return message\n```\n\n## You don’t have to use OpenAI\n\nAutoGen already notes you can use other LLMs, as long as they are “ChatGPT-like”, meaning their API responds with a similar shape and response as ChatGPT API calls.\n\nBut, remember how these agents are classes, and you can override most of the methods?\n\nTry overriding the method: [generate\\_oai\\_reply](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L678), and you can query any LLM you’d like.\n\n\n## Functions can be used instead of executing code\n\nWhen I went to our security team and said “I’d like to use AutoGen for my service in Kubernetes. It needs to be able to execute any arbitrary code produced by an LLM. You’re ok with that, right?”\n\nOf course, the answer was a definite: NO.\n\nSo, why use AutoGen without the auto-code-execution abilities?\n\nOn top of the reasons stated below, another is that you can use function calling to gain total control over code execution. If you have a set of python functions you want to provide to AutoGen — functions you wrote, control, and can accept some safe parameters — that sounds like a better idea anyway than the wild west of allowing any and all code to be executed in your private infrastructure.\n\n\n## Use Agents for organization, not just for conversations\n\nMaybe you don’t have a need for an autonomous, multi-agent conversation. Maybe you just need to make a few different calls to an LLM.\n\nI still like the idea of having different “Agents” just for the sake of organization. Here’s a really crazy idea, but take it for what it’s worth:\n\n\n```python\nanalyst = autogen.AssistantAgent(\n    name=\"Analyst\",\n    system_message=\"\"\"Your an analyst.  You do XYZ.\"\"\",\n    llm_config=llm_config,\n)\n\nsummarizer = autogen.AssistantAgent(\n    name=\"Summarizer\",\n    system_message=\"\"\"Your a summarizer.  You do XYZ.\"\"\",\n    llm_config=llm_config,\n)\n\nreport = \"\"\"Some long report\"\"\"\n\nanalysis = analyst.generate_oai_reply(report)[1]\nsummary = summarizer.generate_oai_reply(report)[1]\n\nprint(f\"Analysis: {analysis}\")\nprint(f\"Summary: {summary}\")\n```\n\n## Why use AutoGen?\n\n1. AutoGen allows multiple agents, with different system prompts and instructions, to solve a problem. Just like in real-life, different perspectives working together will solve a problem better than a single brain.\n2. AutoGen GroupChat is amazing. It provides routing to the right experts (agents), and it allows a conversation to continue autonomously until the problem is solved. Some conversations will go from agent: a->b->c->d, others will be b->a->d->c. This allows AutoGen to solve a variety of different problems without needing explicit rules for each scenario.\n3. AutoGen can recover from mistakes. For example, I made an AutoGen-powered service that made API calls to a service. Sometimes, the API calls errored out because it didn’t send the right data at first. The AutoGen GroupChat kept trying different things until it succeeded. Sometimes, it took 4+ attempts, but my Planner agent didn’t give up — just pivoted autonomously to handle the API failures and try new things.\n4. AutoGen came up with the concept of separating `UserProxyAgent`s from `AssistantAgent` s from the beginning. This also allows us to let the user proxy actually proxy for the user, as shown above.\n5. AutoGen is a well maintained library. Every week they’re adding something new.\n6. AutoGen is very extensible. With the way they’ve built their classes, you can customize anything to your liking.\n7. AutoGen has other features I don’t use, but others may find them helpful, such as helping you count tokens and cost of conversations, cacheing, etc.\n\n"},{"lang":"en","group":"blog","slug":"blog/a-robot-artist-just-made-more-money-than-you-have-in-your-entire-creative-career-13dc772ec612","frontmatter":{"title":"A Robot Artist Just Made More Money Than You Have in Your Entire Creative Career","meta_title":"A Robot Artist Just Made More Money Than You Have in Your Entire Creative Career","description":"A Robot Artist Just Made More Money Than You Have in Your Entire Creative Career","date":"2024-11-13T01:22:35.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*XUyq2c7RZCjJD6IQXjmd6A.png","categories":["Robotics","Art","Technology/Web"],"author":"Rifx.Online","tags":["Ai-Da","Turing","Sotheby’s","painting","creativity"],"draft":false,"slug":"blog/a-robot-artist-just-made-more-money-than-you-have-in-your-entire-creative-career-13dc772ec612"},"content":"\n\n\n\n\n## We’ve reached the next level of AI creativity and commerce\n\n\n\nFirst, we prompted a computer screen to create art based on human creations. Now, it’s an *actual robot* doing the painting.\n\nThat’s right — an “ultra\\-realistic robot artist” has been trained to actually paint on canvas. Its depiction of the late computer scientist Alan Turing recently fetched **$1\\.3 million** at a Sotheby’s auction.\n\nAs *IFLScience* [reports](https://proxy.rifx.online/https://www.iflscience.com/ai-robot-artist-strikes-gold-by-selling-painting-of-alan-turing-for-13-million-76701?fbclid=IwZXh0bgNhZW0CMTEAAR0KXPj5YDHnWibf6e97UWADZMuhPwGY4f_hnJnWs7rNoHN8KvvHquLAcFc_aem_hyBhYwyjT73j6PdAeXvOng), the robot — named Ai\\-Da after Ada Lovelace, a mathematician and computing pioneer — chose the subject of its painting after conversing with humans through a language model. Then it used its robot arm to sketch out and then paint several versions of Turing.\n\nThe source says each oil/acrylic painting of the finished series took roughly six to eight hours to complete by the robot “artist.” Ai\\-Da originally created 15 paintings, narrowed down to the end product by the artificial creator, “applied to a large canvas using a 3D textured printer.”\n\n\n## Artificial art, real money\n\nThe “art” called *A.I. God* garnered about 10 times what it was expected to sell for at auction, won by an anonymous buyer.\n\nChances are $1\\.3 million is more than you’ve made from your own art, assuming you’re not a dead art icon. We know that the art humans buy from late legends often [goes for millions](https://proxy.rifx.online/https://www.veranda.com/luxury-lifestyle/artwork/g43012775/most-expensive-paintings-in-the-world/), even if the artist was struggling financially while alive.\n\nThis is not the first time we’ve seen digital art go for a ridiculous sum at auction.\n\nIn 2018, a French collective [earned $432,500](https://proxy.rifx.online/https://news.artnet.com/market/first-ever-artificial-intelligence-portrait-painting-sells-at-christies-1379902) at Christie’s for an AI portrait art called *Edmond de Belamy, from La Famille de Belamy.*\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*JK-X4et953sOiH1tovj3ag.jpeg)\n\nLater, an artist named Mike Winkelmann that goes by “Beeple” sold a digital NFT (remember those?) for $69 million at Christie’s, which placed him among the most valuable living artists. *The Verge* [explains](https://proxy.rifx.online/https://www.theverge.com/2021/3/11/22325054/beeple-christies-nft-sale-cost-everydays-69-million) that up until that breakthrough sale, the most he earned from a print was $100\\.\n\nBut this is the first time a robot in human\\-like form devised a concept and physically applied paint to canvas.\n\nOf course, Ai\\-Da doesn’t keep the money — so far, robots don’t have a need to use currency. Its human creators benefit financially from the sale.\n\n\n## Human input is still needed (for now)\n\nAidan Meller, an art dealer and gallery director, is the lead of the Ai\\-Da Robot Project. IFLScience says the actual robot was built by Engineered Arts, the UK robotics collective behind [Ameca](https://proxy.rifx.online/https://engineeredarts.co.uk/robot/ameca/), which is also eerily humanoid.\n\nKeep in mind, it still took humans to prompt the robot. Meller said there was an initial discussion with Ai\\-Da about depicting “A.I. for good.” There was also a discussion about how to approach the painting in terms of style and texture.\n\n“It was programmed internationally, with her AI capabilities being developed by PhD students and professors at the Universities of Oxford and Birmingham,” notes the site regarding Ai\\-Da. It adds that human assistants helped prepare the printed canvas, but the robot was largely responsible for the completed product.\n\nHere’s a video of Ai\\-Da explaining the artistic “process”:\n\n\n\n\n\n\n\n*The Guardian* [says](https://proxy.rifx.online/https://www.theguardian.com/artanddesign/2024/nov/08/alan-turing-portrait-ai-da-robot-painting-sale-price-auction) the somewhat abstract style of the portraits might’ve been intentional:\n\n\n> **The artwork’s ‘muted tones and broken facial planes’ seemingly suggested ‘the struggles Turing warned we will face when it comes to managing AI’, Meller said.**\n\nTuring was right. The emergence of AI in writing and art has taken the world by storm, emerging just a few years ago. Now we’re already at the point where a robot can command more than a million dollars for its concepts.\n\n\n## Challenging the definition of art\n\nBut it’s about more than money. This art further challenges what art actually is, and whether one needs to have human consciousness in order to create impact.\n\nThe humans behind this project consider Ai\\-Da itself to be “conceptual art.” While the robot is obviously not human, there are other projects in the works that could soon create realistic\\-looking artists with actual [living skin](https://proxy.rifx.online/https://readmedium.com/the-new-face-of-artificial-intelligence-9c900d463cf9).\n\nWho knows, you might soon be talking to a fellow artist at a life drawing class, complimenting their technique, not realizing you’re conversing with a robot.\n\n*What are your thoughts on all of this? Are you impressed, creeped out, or concerned for your artistic future?*\n\n\n"},{"lang":"en","group":"blog","slug":"blog/ai-image-generator-and-story-generation-app-using-fastapi-groq-and-replicate-706f29dc126f","frontmatter":{"title":"AI Image Generator and Story Generation App using FastAPI, Groq and Replicate","meta_title":"AI Image Generator and Story Generation App using FastAPI, Groq and Replicate","description":"Project Introduction: AI Image Generator and Story Creator","date":"2024-11-08T00:21:34.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*-fb-azx7fDZ-X9-PbIkiSQ.jpeg","categories":["Programming","Technology/Web","Generative AI"],"author":"Rifx.Online","tags":["FastAPI","Groq","Replicate","transcription","image-generation"],"draft":false,"slug":"blog/ai-image-generator-and-story-generation-app-using-fastapi-groq-and-replicate-706f29dc126f"},"content":"\n\n\n## Project Introduction: AI Image Generator and Story Creator\n\nThe AI Image Generator and Story Creator is a web application that leverages advanced AI technologies to provide users with an interactive platform for generating images and stories based on audio prompts. The application utilizes FastAPI for the backend, enabling efficient handling of requests and responses, while the frontend is built with HTML, CSS (DaisyUI and Tailwind CSS), and JavaScript for a responsive user experience. This application leverages llama\\-3\\.1–70b for prompt generation ,black\\-forest\\-labs/flux\\-1\\.1\\-pro for image generation and llava\\-v1\\.5–7b vbision model for story cretion via Groq and Replicat.AI repectively.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*0h1GzVVWs_df4OWAC-P59A.jpeg)\n\n## Key Features:\n\n1. Audio Recording and Transcription: Users can record their voice prompts, which are then transcribed into text using speech recognition technology.\n\n2\\. Image Generation: Based on the transcribed text, the application generates detailed image prompts and creates corresponding images using the Replicate API.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*uiSG8Ir-Wv4a1huYqWhxBg.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*-eRPglLlJwms8N2DCXRyXg.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Mtle1K8AzjMHGxlGicFcGQ.png)\n\n3\\. Image Downloading: Users can download the generated images to their local devices.\n\n4\\. Story Generation: The application can generate engaging stories based on the images created, providing a narrative context to the visual content.\n\n5\\. User\\-Friendly Interface: The application features a clean and intuitive interface, making it easy for users to interact with the various functionalities.\n\n## Technologies Used:\n\n* Backend: FastAPI, Groq, Replicate.ai, SpeechRecognition\n* Frontend: HTML, CSS (DaisyUI, Tailwind CSS), JavaScript\n* Image Processing: Pillow for image handling\n* Asynchronous Operations: aiohttp and aiofiles for efficient file handling and network requests\n\nThis project serves as a demonstration of integrating multiple AI services into a cohesive application, allowing users to explore the creative possibilities of AI\\-generated content\n\n## Detailed Explanation of the Codebase:\n\n1. **Frontend (HTML/JavaScript):**\n\n* *The application uses a single HTML page (index.html) with a responsive design using DaisyUI and Tailwind CSS.*\n* *The page contains sections for audio recording, transcription, prompt generation, image generation, and story generation.*\n* *The JavaScript file (script.js) handles user interactions and communicates with the backend API.*\n\n**2\\. Backend (FastAPI) :**\n\n* *The main application is defined in app/main.py.*\n* *It uses FastAPI to create a web server with various endpoints:*\n\n**— *a. /: Serves the main HTML page.***\n\n***— b. /transcribe:*** *Transcribes audio to text.*\n\n***— c. /generate\\_prompt:*** *Generates an image prompt from text using Groq’s LLM.*\n\n***— d. /generate\\_image:*** *Generates an image using Replicate’s Flux model.*\n\n***— e. /download\\_image:*** *Downloads and saves the generated image.*\n\n***— f. /generate\\_story\\_from\\_image:*** *Generates a story based on the image using Groq’s LLaVA model.*\n\n***— g. /download/{filename}:*** *Serves the downloaded image file.*\n\n**3\\. Key Features:**\n\n* *Audio recording and transcription*\n* *Text\\-to\\-image prompt generation*\n* *Image generation from prompts*\n* *Story generation from images*\n* *Image downloading and saving*\n\n**4\\. External APIs:**\n\n* [Groq:](https://console.groq.com/docs/models) Used for text generation (tweaked prompts and [stories](https://console.groq.com/docs/vision))\n* [Replicate AI:](https://replicate.com/black-forest-labs/flux-1.1-pro/api) black\\-forest\\-labs/flux\\-1\\.1\\-pro model used for image generation\n* Necessary Packages to be Installed:\n\n```python\nfastapi\nuvicorn\njinja2\npython-multipart\npydantic\npython-dotenv\ngroq\nreplicate\nSpeechRecognition\npydub\naiohttp\naiofiles\nPillow\n```\n\n**You can install these packages using pip:**\n\n```python\npip install fastapi uvicorn jinja2 python-multipart pydantic python-dotenv groq replicate SpeechRecognition pydub aiohttp aiofiles Pillow\n```\n\n**Execution Instructions:**\n\n* ***Set up environment variables:*** *Create a .env file in the root directory with the following content:*\n\n```python\nGROQ_API_KEY=your_groq_api_key_here\nREPLICATE_API_TOKEN=your_replicate_api_token_here\n```\n\n*Replace the placeholder values with your actual API keys.*\n\n* **Ensure you have all the necessary files in place:**\n* — app/main.py\n* — app/config.py\n* — app/utils.py\n* — templates/index.html\n* — static/css/styles.css\n* — static/js/script.js\n* **Run the FastAPI server:** Navigate to the directory containing app/main.py and run:\n\n```python\nuvicorn app.main:app - reload\n```\n\n* **Access the application:**\n* — Open a web browser and go to [http://127\\.0\\.0\\.1:8000](http://127.0.0.1:8000)\n* **Using the application:**\n* — a. Click “Start Recording” and speak your prompt.\n* — b. Click “Stop Recording” when finished.\n* — c. The audio will be transcribed automatically.\n* — d. Click “Generate Image Prompt” to create a detailed prompt.\n* — e. Click “Generate Image” to create an image based on the prompt.\n* — f. Use the “Download Image” button to save the generated image.\n* — g. Click “Generate Story” to create a story based on the generated image.\n\nNote: Ensure you have proper internet connectivity, as the application relies on external APIs for various functionalities.\n\nThis application demonstrates a complex integration of various AI technologies, including speech recognition, language models, and image generation, all wrapped in a user\\-friendly web interface.\n\nThe FastAPI UI as illustrated below\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Teb1wJzGOQZ3oqaLcLJwkA.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*6K-nORe7ubi0MRqIRLdrFA.png)\n\n## AI Image Generator Application\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*1CClu2W3yRds1Lsk1rk9Ew.png)\n\n## Speak your prompt\n\n* Start Recording\n* Stop Recording\n* Transcription(Transcribed Text): a beautiful Indian model walking down the Runway Ram as a part of fashion show\n* Create a new Prompt based on the transcribed text to generate images\n* — Generated Prompt: “*Generate a highly realistic image of a stunning Indian model walking down the iconic Runway Ram as part of a high\\-end fashion show. The model, a 22\\-year\\-old Indian woman with long dark hair, dark brown eyes, and flawless skin, should be dressed in an exquisite, intricately embroidered lehenga choli with gold and silver sequins, traditional Indian attire, and pair it with heeled stilettos. Her outfit is designed with intricate embroidery and fine stitching. Emphasize the elegant pleats, sparkling fabrics, and her elegant poise and confident stride. Incorporate elaborate jewelry pieces like beads, gold bangles, and necklaces on her hands, neck and one side styled hairstyle. Lighiting effects play an enormous role, set up warm stage headlights that emphasize the model’s attire and light the whole surrounding with mild bluish tone. Camera angles should display the outfit’s details entirely. Desired scene perspective is front full body shot of model in the middle, catwalk around her lit with intense golden hue lights illuminating from within*.”\n\n## Generated Image\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*YljCEFTIc8hslZbf.jpg)\n\n## Story Generated from the Image\n\n*The stunning sight of this pageant queen in a gold and silver sequined outfit with lehenga skirt and bling\\-drop earrings is enough to leave any spectator mesmerized. Sashaya Gnanavel is prominently featured in the foreground, walking confidently down the runway to captivate the audience at the show. Her stylish attire, complemented by an elegant pearl necklace, draws the attention of everyone present. The collection showcases vibrant colors and sparkling embroideries, which add to the overall visual appeal of the event. Sashaya’s confidence and beauty in the spotlight are a true testament to her talent and dedication to the fashion industry. The dazzling effect created by her makeup, jewels, and exquisite outfit helps set the stage for an extraordinary showcase of design and craftsmanship. This compelling scene encapsulates magic and opulence, where spectators are left in awe by the sheer exquisiteness of it all*.\n\n## Code Implementation\n\nCreate Virtual Environment\n\nTo create a virtual environment using Python’s venv module, follow these steps:\n\n* Open your terminal or command prompt.\n* Navigate to your project directory (where you want to create the virtual environment). You can use the cd command to change directories. For example:\n\n```python\ncd path/to/your/project\n```\n\n* Create a virtual environment by running the following command:\n\n```python\npython -m venv venv\n```\n\n* This command creates a new directory named venv in your project folder, which will contain the virtual environment (on windows)\n* Activate the virtual environment:\n\n```python\nvenv\\Scripts\\activate\n```\n\n* folder structure\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*J3QJJACHVRtjrU1boxHUjA.png)\n\n* utils.py\n\n```python\nimport base64\nimport os\nfrom pydub import AudioSegment\n\ndef save_audio(audio_data):\n    # Decode the base64 audio data\n    audio_bytes = base64.b64decode(audio_data.split(\",\")[1])\n  \n    # Save the audio to a temporary file\n    temp_file = \"temp_audio.webm\"\n    with open(temp_file, \"wb\") as f:\n        f.write(audio_bytes)\n  \n    # Convert WebM to WAV\n    audio = AudioSegment.from_file(temp_file, format=\"webm\")\n    wav_file = \"temp_audio.wav\"\n    audio.export(wav_file, format=\"wav\")\n  \n    # Remove the temporary WebM file\n    os.remove(temp_file)\n  \n    return wav_file\n\ndef text_to_speech(text):\n    # Implement text-to-speech functionality if needed\n    pass\n```\n\n* main.py\n\n```python\n\"\"\"\n    1. Record audio through their microphone\n    2. Transcribe the audio to text\n    3. Generate an image prompt using the Groq Llama3 model\n    4. Generate an image using the Replicate.ai Flux model\n    5. Display the generated image\n    6. Download the generated image\n    The application uses DaisyUI and Tailwind CSS for styling, providing a dark mode interface. The layout is responsive and should work well on both desktop and mobile devices.\nNote: You may need to adjust some parts of the code depending on the specific APIs and models you're using, as well as any security considerations for your deployment environment.\n\n\"\"\"\nfrom fastapi import FastAPI, Request, HTTPException\nfrom fastapi.templating import Jinja2Templates\nfrom fastapi.staticfiles import StaticFiles\nfrom fastapi.responses import JSONResponse, FileResponse\nfrom pydantic import BaseModel\nimport speech_recognition as sr\nfrom groq import Groq\nimport replicate\nimport os\nimport aiohttp\nimport aiofiles\nimport time\nfrom dotenv import load_dotenv\nload_dotenv()\nfrom .utils import text_to_speech, save_audio\nfrom PIL import Image\nimport io\nimport base64\nimport base64\n\n\n## Function to encode the image\ndef encode_image(image_path):\n  with open(image_path, \"rb\") as image_file:\n    return base64.b64encode(image_file.read()).decode('utf-8')\n\napp = FastAPI()\n\napp.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\ntemplates = Jinja2Templates(directory=\"templates\")\n\n## Initialize Groq client with the API key\nGROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\nif not GROQ_API_KEY:\n    raise ValueError(\"GROQ_API_KEY is not set in the environment variables\")\ngroq_client = Groq(api_key=GROQ_API_KEY)\n\nclass AudioData(BaseModel):\n    audio_data: str\n\nclass ImagePrompt(BaseModel):\n    prompt: str\n\nclass PromptRequest(BaseModel):\n    text: str\n\n## Add this new model\nclass FreeImagePrompt(BaseModel):\n    prompt: str\n    image_path: str\n\n@app.get(\"/\")\nasync def read_root(request: Request):\n    return templates.TemplateResponse(\"index.html\", {\"request\": request})\n\n@app.post(\"/transcribe\")\nasync def transcribe_audio(audio_data: AudioData):\n    try:\n        # Save the audio data to a file\n        audio_file = save_audio(audio_data.audio_data)\n\n        # Transcribe the audio\n        recognizer = sr.Recognizer()\n        with sr.AudioFile(audio_file) as source:\n            audio = recognizer.record(source)\n        text = recognizer.recognize_google(audio)\n\n        return JSONResponse(content={\"text\": text})\n    except Exception as e:\n        raise HTTPException(status_code=400, detail=str(e))\n\n@app.post(\"/generate_prompt\")\nasync def generate_prompt(prompt_request: PromptRequest):\n    try:\n        text = prompt_request.text\n        # Use Groq to generate a new prompt\n        response = groq_client.chat.completions.create(\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a creative assistant that generates prompts for realistic image generation.\"},\n                {\"role\": \"user\", \"content\": f\"Generate a detailed prompt for a realistic image based on this description: {text}.The prompt should be clear and detailed in no more than 200 words.\"}\n            ],\n            model=\"llama-3.1-70b-versatile\",\n            max_tokens=256\n        )\n        generated_prompt = response.choices[0].message.content\n        print(f\"tweaked prompt:{generated_prompt}\")\n        return JSONResponse(content={\"prompt\": generated_prompt})\n    except Exception as e:\n        print(f\"Error generating prompt: {str(e)}\")\n        raise HTTPException(status_code=400, detail=str(e))\n\n@app.post(\"/generate_image\")\nasync def generate_image(image_prompt: ImagePrompt):\n    try:\n        prompt = image_prompt.prompt\n        print(f\"Received prompt: {prompt}\")\n\n        # Use Replicate to generate an image\n        output = replicate.run(\n            \"black-forest-labs/flux-1.1-pro\",\n            input={\n                \"prompt\": prompt,\n                \"aspect_ratio\": \"1:1\",\n                \"output_format\": \"jpg\",\n                \"output_quality\": 80,\n                \"safety_tolerance\": 2,\n                \"prompt_upsampling\": True\n            }\n        )\n      \n        print(f\"Raw output: {output}\")\n        print(f\"Output type: {type(output)}\")\n      \n        # Convert the FileOutput object to a string\n        image_url = str(output)\n      \n        print(f\"Generated image URL: {image_url}\")\n      \n        return JSONResponse(content={\"image_url\": image_url})\n    except Exception as e:\n        print(f\"Error generating image: {str(e)}\")\n        raise HTTPException(status_code=400, detail=str(e))\n\n@app.get(\"/download_image\")\nasync def download_image(image_url: str):\n    try:\n        # Create Output folder if it doesn't exist\n        output_folder = \"Output\"\n        os.makedirs(output_folder, exist_ok=True)\n\n        # Generate a unique filename\n        filename = f\"generated_image_{int(time.time())}.jpg\"\n        filepath = os.path.join(output_folder, filename)\n\n        # Download the image\n        async with aiohttp.ClientSession() as session:\n            async with session.get(image_url) as resp:\n                if resp.status == 200:\n                    async with aiofiles.open(filepath, mode='wb') as f:\n                        await f.write(await resp.read())\n\n        # Return the filepath and filename\n        return JSONResponse(content={\n            \"filepath\": filepath,\n            \"filename\": filename\n        })\n    except Exception as e:\n        print(f\"Error downloading image: {str(e)}\")\n        raise HTTPException(status_code=400, detail=str(e))\n\nclass StoryRequest(BaseModel):\n    filepath: str\n    filename: str\n\n@app.post(\"/generate_story_from_image\")\nasync def generate_story_from_image(content: StoryRequest):\n    try:\n        image_path = content.filepath\n        print(f\"Image path: {image_path}\")\n        # Check if the file exists\n        if not os.path.exists(image_path):\n            raise HTTPException(status_code=400, detail=\"Image file not found\")\n\n        # Getting the base64 string\n        base64_image = encode_image(image_path)\n\n        client = Groq()\n\n        chat_completion = client.chat.completions.create(\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\"type\": \"text\", \"text\": \"Generate a clear,concise,meaningful and engaging cover story for a highly acclaimed leisure magazine based on the image provided. The story should keep the audience glued and engaged and the story should bewithin 200 words.\"},\n                        {\n                            \"type\": \"image_url\",\n                            \"image_url\": {\n                                \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n                            },\n                        },\n                    ],\n                }\n            ],\n            model=\"llava-v1.5-7b-4096-preview\",\n        )\n\n        story = chat_completion.choices[0].message.content\n        print(f\"Generated story: {story}\")\n        return JSONResponse(content={\"story\": story})\n    except Exception as e:\n        print(f\"Error generating story from the image: {str(e)}\")\n        raise HTTPException(status_code=400, detail=str(e))\n\n@app.get(\"/download/{filename}\")\nasync def serve_file(filename: str):\n    file_path = os.path.join(\"Output\", filename)\n    return FileResponse(file_path, filename=filename)\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n```\n\n* script.js\n\n```python\nlet mediaRecorder;\nlet audioChunks = [];\n\nconst startRecordingButton = document.getElementById('startRecording');\nconst stopRecordingButton = document.getElementById('stopRecording');\nconst recordingStatus = document.getElementById('recordingStatus');\nconst transcription = document.getElementById('transcription');\nconst generatePromptButton = document.getElementById('generatePrompt');\nconst generatedPrompt = document.getElementById('generatedPrompt');\nconst generateImageButton = document.getElementById('generateImage');\nconst generatedImage = document.getElementById('generatedImage');\nconst downloadLink = document.getElementById('downloadLink');\nconst generateStoryButton = document.getElementById('generateStory');\nconst generatedStory = document.getElementById('generatedStory');\n\nstartRecordingButton.addEventListener('click', startRecording);\nstopRecordingButton.addEventListener('click', stopRecording);\ngeneratePromptButton.addEventListener('click', generatePrompt);\ngenerateImageButton.addEventListener('click', generateImage);\ngenerateStoryButton.addEventListener('click', generateStory);\n\nasync function startRecording() {\n    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n    mediaRecorder = new MediaRecorder(stream);\n\n    mediaRecorder.ondataavailable = (event) => {\n        audioChunks.push(event.data);\n    };\n\n    mediaRecorder.onstop = sendAudioToServer;\n\n    mediaRecorder.start();\n    startRecordingButton.disabled = true;\n    stopRecordingButton.disabled = false;\n    recordingStatus.textContent = 'Recording...';\n}\n\nfunction stopRecording() {\n    mediaRecorder.stop();\n    startRecordingButton.disabled = false;\n    stopRecordingButton.disabled = true;\n    recordingStatus.textContent = 'Recording stopped.';\n}\n\nasync function sendAudioToServer() {\n    const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });\n    const reader = new FileReader();\n    reader.readAsDataURL(audioBlob);\n    reader.onloadend = async () => {\n        const base64Audio = reader.result;\n        const response = await fetch('/transcribe', {\n            method: 'POST',\n            headers: {\n                'Content-Type': 'application/json',\n            },\n            body: JSON.stringify({ audio_data: base64Audio }),\n        });\n        const data = await response.json();\n        transcription.textContent = `Transcription: ${data.text}`;\n        generatePromptButton.disabled = false;\n    };\n    audioChunks = [];\n}\n\nasync function generatePrompt() {\n    const text = transcription.textContent.replace('Transcription: ', '');\n    const response = await fetch('/generate_prompt', {\n        method: 'POST',\n        headers: {\n            'Content-Type': 'application/json',\n        },\n        body: JSON.stringify({ text: text }),\n    });\n    const data = await response.json();\n    generatedPrompt.textContent = `Generated Prompt: ${data.prompt}`;\n    generateImageButton.disabled = false;\n}\n\nasync function generateImage() {\n    const prompt = generatedPrompt.textContent.replace('Generated Prompt: ', '');\n    const response = await fetch('/generate_image', {\n        method: 'POST',\n        headers: {\n            'Content-Type': 'application/json',\n        },\n        body: JSON.stringify({ prompt: prompt }),\n    });\n    const data = await response.json();\n    generatedImage.src = data.image_url;\n  \n    // Download the image and get the filepath\n    const downloadResponse = await fetch(`/download_image?image_url=${encodeURIComponent(data.image_url)}`);\n    const downloadData = await downloadResponse.json();\n  \n    // Store the filepath and filename for later use\n    generatedImage.dataset.filepath = downloadData.filepath;\n    generatedImage.dataset.filename = downloadData.filename;\n\n    // Set up the download link\n    downloadLink.href = `/download/${downloadData.filename}`;\n    downloadLink.download = downloadData.filename;\n    downloadLink.style.display = 'inline-block';\n}\n\nasync function generateStory() {\n    const imagePath = generatedImage.dataset.filepath;\n    const filename = generatedImage.dataset.filename;\n  \n    if (!imagePath || !filename) {\n        generatedStory.textContent = \"Error: Please generate an image first.\";\n        return;\n    }\n\n    try {\n        const response = await fetch('/generate_story_from_image', {\n            method: 'POST',\n            headers: {\n                'Content-Type': 'application/json',\n            },\n            body: JSON.stringify({ filepath: imagePath, filename: filename }),\n        });\n        if (!response.ok) {\n            throw new Error(`HTTP error! status: ${response.status}`);\n        }\n        const data = await response.json();\n      \n        // Display the generated story\n        generatedStory.textContent = data.story;\n      \n        // Make sure the story container is visible\n        document.getElementById('storyContainer').style.display = 'block';\n    } catch (error) {\n        console.error('Error:', error);\n        generatedStory.textContent = `Error: ${error.message}`;\n    }\n}\n\n// Modify the download link click event\ndownloadLink.addEventListener('click', async (event) => {\n    event.preventDefault();\n    const response = await fetch(downloadLink.href);\n    const blob = await response.blob();\n    const url = window.URL.createObjectURL(blob);\n    const a = document.createElement('a');\n    a.style.display = 'none';\n    a.href = url;\n    a.download = response.headers.get('Content-Disposition').split('filename=')[1];\n    document.body.appendChild(a);\n    a.click();\n    window.URL.revokeObjectURL(url);\n});\n```\n\n* style.css\n\n```python\nbody {\n    background-color: #1a1a2e;\n    color: #ffffff;\n}\n\n.container {\n    max-width: 1200px;\n}\n\n#imageContainer {\n    min-height: 300px;\n    display: flex;\n    align-items: center;\n    justify-content: center;\n    background-color: #16213e;\n    border-radius: 8px;\n}\n\n#generatedImage {\n    max-width: 100%;\n    max-height: 400px;\n    object-fit: contain;\n}\n```\n\n* index.html\n\n```python\n<!DOCTYPE html>\n<html lang=\"en\" data-theme=\"dark\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>AI Image Generator</title>\n    <link href=\"https://cdn.jsdelivr.net/npm/daisyui@3.7.3/dist/full.css\" rel=\"stylesheet\" type=\"text/css\" />\n    <script src=\"https://cdn.tailwindcss.com\"></script>\n    <link rel=\"stylesheet\" href=\"{{ url_for('static', path='/css/styles.css') }}\">\n</head>\n<body>\n    <div class=\"container mx-auto px-4 py-8\">\n        <h1 class=\"text-4xl font-bold mb-8 text-center\">AI Image Generator</h1>\n        <div class=\"grid grid-cols-1 md:grid-cols-2 gap-8\">\n            <div class=\"card bg-base-200 shadow-xl\">\n                <div class=\"card-body\">\n                    <h2 class=\"card-title mb-4\">Speak your prompt</h2>\n                    <button id=\"startRecording\" class=\"btn btn-primary mb-4\">Start Recording</button>\n                    <button id=\"stopRecording\" class=\"btn btn-secondary mb-4\" disabled>Stop Recording</button>\n                    <div id=\"recordingStatus\" class=\"text-lg mb-4\"></div>\n                    <div id=\"transcription\" class=\"text-lg mb-4\"></div>\n                    <button id=\"generatePrompt\" class=\"btn btn-accent mb-4\" disabled>Generate Image Prompt</button>\n                    <div id=\"generatedPrompt\" class=\"text-lg mb-4\"></div>\n                    <button id=\"generateImage\" class=\"btn btn-success\" disabled>Generate Image</button>\n                </div>\n            </div>\n            <div class=\"card bg-base-200 shadow-xl\">\n                <div class=\"card-body\">\n                    <h2 class=\"card-title mb-4\">Generated Image</h2>\n                    <div id=\"imageContainer\" class=\"mb-4\">\n                        <img id=\"generatedImage\" src=\"\" alt=\"Generated Image\" class=\"w-full h-auto\">\n                    </div>\n                    <a id=\"downloadLink\" href=\"#\" download=\"generated_image.png\" class=\"btn btn-info\" style=\"display: none;\">Download Image</a>\n                </div>\n            </div>\n        </div>\n        <!-- Add this new section after the existing cards -->\n        <div class=\"card bg-base-200 shadow-xl mt-8\">\n            <div class=\"card-body\">\n                <h2 class=\"card-title mb-4\">Generate Story from Image</h2>\n                <button id=\"generateStory\" class=\"btn btn-primary mb-4\">Generate Story</button>\n                <div id=\"storyContainer\" class=\"mb-4\">\n                    <p id=\"generatedStory\" class=\"text-lg\"></p>\n                </div>\n            </div>\n        </div>\n    </div>\n    <script src=\"{{ url_for('static', path='/js/script.js') }}\"></script>\n</body>\n</html>\n```\n\n## Conclusion\n\nThe AI Image Generator and Story Creator project successfully integrates various AI technologies to create an interactive web application that allows users to generate images and stories based on audio prompts. By leveraging FastAPI for the backend and modern frontend technologies, the application provides a seamless user experience.\n\n## Key Takeaways:\n\n1. Integration of AI Models: The project demonstrates how to integrate multiple AI models, including Groq for text generation and Replicate for image generation, to create a cohesive application that enhances user creativity.\n2. User Interaction: The application allows users to interact through voice commands, making it accessible and user\\-friendly. The ability to record audio, transcribe it, and generate content based on that input showcases the potential of voice\\-driven applications.\n3. Dynamic Content Generation: By generating images and stories dynamically based on user input, the application highlights the capabilities of AI in content creation, providing users with unique and personalized outputs.\n4. Responsive Design: The use of DaisyUI and Tailwind CSS ensures that the application is visually appealing and responsive, catering to users on various devices.\n5. Future Enhancements: The project can be further enhanced by incorporating additional features such as user authentication, saving user\\-generated content, and expanding the range of AI models used for different creative tasks.\n\nOverall, this project serves as a comprehensive example of how to build an AI\\-powered web application that combines audio processing, image generation, and storytelling, paving the way for innovative applications in the creative domain.\n\n## References\n\n* FastAPI Documentation: [FastAPI](https://fastapi.tiangolo.com/) is a modern web framework for building APIs with Python. It is designed to be easy to use and fast.\n* Pydantic: [Pydantic](https://pydantic-docs.helpmanual.io/) is used for data validation and settings management using Python type annotations.\n* Groq:[Groq](https://groq.com/docs/) is a platform for building and deploying AI models. It provides APIs for text generation and other AI tasks.\n* Replicate: [Replicate](https://replicate.com/docs) is a platform that allows you to run machine learning models in the cloud. It provides APIs for various models, including image generation.\n* SpeechRecognition: [SpeechRecognition](https://pypi.org/project/SpeechRecognition/) is a library for performing speech recognition, with support for several engines and APIs.\n* Pillow: [Pillow](https://pillow.readthedocs.io/en/stable/) is a Python Imaging Library (PIL) fork that adds image processing capabilities to your Python inter\n* JavaScript Fetch API: [The Fetch API](https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API) provides a modern way to make network requests in JavaScript.\n* HTML5 Audio API: [The HTML5 Audio AP](https://developer.mozilla.org/en-US/docs/Web/API/HTMLAudioElement)I allows you to play audio files in web applications.\n* DaisyUI: [DaisyUI](https://daisyui.com/) is a component library for Tailwind CSS that provides pre\\-designed components.\n* Tailwind CSS: [Tailwind CSS](https://tailwindcss.com/docs) is a utility\\-first CSS framework for creating custom designs without having to leave your HTML.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/ai-powered-ocr-with-phi-3-vision-128k-the-future-of-document-processing-7be80c46bd16","frontmatter":{"title":"AI-Powered OCR with Phi-3-Vision-128K: The Future of Document Processing","meta_title":"AI-Powered OCR with Phi-3-Vision-128K: The Future of Document Processing","description":"In the fast-evolving world of artificial intelligence, multimodal models are setting new standards for integrating visual and textual data…","date":"2024-11-08T00:26:30.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BR-H6cQoyoRo6gVRqjvAyA.png","categories":["Natural Language Processing","Computer Vision","Data Science"],"author":"Rifx.Online","tags":["OCR","tokens","encoder","language","document"],"draft":false,"slug":"blog/ai-powered-ocr-with-phi-3-vision-128k-the-future-of-document-processing-7be80c46bd16"},"content":"\n\n\n\n\n\nIn the fast\\-evolving world of artificial intelligence, multimodal models are setting new standards for integrating visual and textual data. One of the latest breakthroughs is the **Phi\\-3\\-Vision\\-128K\\-Instruct**, a state\\-of\\-the\\-art open multimodal model that pushes the boundaries of AI capabilities in processing images and text. Designed with a focus on document extraction, Optical Character Recognition (OCR), and general image understanding, this model can revolutionize how we handle information from PDFs, charts, tables, and other structured or semi\\-structured documents.\n\nLet’s dive deep into the nuts and bolts of the Phi\\-3\\-Vision\\-128K\\-Instruct, explore its architecture, technical requirements, responsible use considerations, and understand how it can be used to simplify complex tasks like document extraction, pdf parsing, and AI\\-powered data analysis.\n\n\n## What is Phi\\-3\\-Vision\\-128K\\-Instruct?\n\nPhi\\-3\\-Vision\\-128K\\-Instruct belongs to the Phi\\-3 model family and is built for multimodal data processing, supporting a context length of up to **128,000 tokens**. The model incorporates both textual and visual data, making it well\\-suited for tasks that require the simultaneous interpretation of text and images. Its development involved **500 billion training tokens**, a combination of high\\-quality synthetic data and rigorously filtered publicly available sources. Through a refined training process that included **supervised fine\\-tuning and preference optimization**, the model has been crafted to deliver precise, reliable, and safe AI solutions.\n\nWith **4\\.2 billion parameters**, Phi\\-3\\-Vision\\-128K\\-Instruct’s architecture comprises an image encoder, connector, projector, and the Phi\\-3 Mini language model, making it a lightweight yet powerful choice for a wide range of applications.\n\n\n## Core Use Cases\n\nThe model’s primary applications span several domains, with a particular focus on:\n\n* **Document extraction and OCR:** Efficiently converting images of text or scanned documents into editable formats. It can handle complex layouts like tables, charts, and diagrams, making it a valuable tool for digitizing physical documents or automating data extraction workflows.\n* **General image understanding:** Parsing visual content to recognize objects, interpret scenes, and extract relevant information.\n* **Memory/compute\\-constrained environments:** Running AI tasks where computing power or memory is limited without compromising performance.\n* **Latency\\-bound scenarios:** Reducing processing delays in real\\-time applications such as live data feeds, chat\\-based assistants, or streaming content analysis.\n\n\n## How to Get Started with Phi\\-3\\-Vision\\-128K\\-Instruct\n\nTo use Phi\\-3\\-Vision\\-128K\\-Instruct, you will need to set up your development environment with the required libraries and tools. The model is integrated into the development version (4\\.40\\.2\\) of the Hugging Face `transformers` library. Before diving into code examples, ensure that your Python environment is configured with these packages:\n\n\n```python\n## Required Packages\nflash_attn==2.5.8\nnumpy==1.24.4\nPillow==10.3.0\nRequests==2.31.0\ntorch==2.3.0\ntorchvision==0.18.0\ntransformers==4.40.2\n```\nTo load the model, you can either update your local `transformers` library or clone and install it directly from the source:\n\n\n```python\npip uninstall -y transformers && pip install git+https://github.com/huggingface/transformers\n```\nNow, let’s jump into some practical code snippets to show how you can leverage this powerful model for AI\\-driven document extraction and text generation.\n\n\n## Sample Code for Loading the Model\n\nHere’s a Python example of how to initialize the model and start making inferences. We’ll make use of classes and functions to keep the code clean and organized:\n\n\n```python\nfrom PIL import Image\nimport requests\nfrom transformers import AutoModelForCausalLM, AutoProcessor\n\nclass Phi3VisionModel:\n    def __init__(self, model_id=\"microsoft/Phi-3-vision-128k-instruct\", device=\"cuda\"):\n        \"\"\"\n        Initialize the Phi3VisionModel with the specified model ID and device.\n        \n        Args:\n            model_id (str): The identifier of the pre-trained model from Hugging Face's model hub.\n            device (str): The device to load the model on (\"cuda\" for GPU or \"cpu\").\n        \"\"\"\n        self.model_id = model_id\n        self.device = device\n        self.model = self.load_model()  # Load the model during initialization\n        self.processor = self.load_processor()  # Load the processor during initialization\n    \n    def load_model(self):\n        \"\"\"\n        Load the pre-trained language model with causal language modeling capabilities.\n        \n        Returns:\n            model (AutoModelForCausalLM): The loaded model.\n        \"\"\"\n        print(\"Loading model...\")\n        # Load the model with automatic device mapping and data type adjustment\n        return AutoModelForCausalLM.from_pretrained(\n            self.model_id, \n            device_map=\"auto\",  # Automatically map model to the appropriate device(s)\n            torch_dtype=\"auto\",  # Use an appropriate torch data type based on the device\n            trust_remote_code=True,  # Allow execution of custom code for loading the model\n            _attn_implementation='flash_attention_2'  # Use optimized attention implementation\n        ).to(self.device)  # Move the model to the specified device\n    \n    def load_processor(self):\n        \"\"\"\n        Load the processor associated with the model for processing inputs and outputs.\n        \n        Returns:\n            processor (AutoProcessor): The loaded processor for handling text and images.\n        \"\"\"\n        print(\"Loading processor...\")\n        # Load the processor with trust_remote_code=True to handle any custom processing logic\n        return AutoProcessor.from_pretrained(self.model_id, trust_remote_code=True)\n    \n    def predict(self, image_url, prompt):\n        \"\"\"\n        Perform a prediction using the model given an image and a prompt.\n        \n        Args:\n            image_url (str): The URL of the image to be processed.\n            prompt (str): The textual prompt that guides the model's generation.\n        \n        Returns:\n            response (str): The generated response from the model.\n        \"\"\"\n        # Load the image from the provided URL\n        image = Image.open(requests.get(image_url, stream=True).raw)\n        \n        # Format the input prompt template for the model\n        prompt_template = f\"<|user|>\\n<|image_1|>\\n{prompt}<|end|>\\n<|assistant|>\\n\"\n        \n        # Process the inputs, converting the prompt and image into tensor format\n        inputs = self.processor(prompt_template, [image], return_tensors=\"pt\").to(self.device)\n        \n        # Set generation arguments for the model's response generation\n        generation_args = {\n            \"max_new_tokens\": 500,  # Maximum number of tokens to generate\n            \"temperature\": 0.7,     # Sampling temperature for diversity in generation\n            \"do_sample\": False      # Disable sampling for deterministic output\n        }\n        print(\"Generating response...\")\n        # Generate the output IDs using the model, skipping the input tokens\n        output_ids = self.model.generate(**inputs, **generation_args)\n        output_ids = output_ids[:, inputs['input_ids'].shape[1]:]  # Ignore the input prompt in the output\n        \n        # Decode the generated output tokens to obtain the response text\n        response = self.processor.batch_decode(output_ids, skip_special_tokens=True)[0]\n        return response\n\n## Initialize the model\nphi_model = Phi3VisionModel()\n\n## Example prediction\nimage_url = \"https://example.com/sample_image.png\"  # URL of the sample image\nprompt = \"Extract the data in json format.\"  # Prompt for model guidance\nresponse = phi_model.predict(image_url, prompt)  # Get the response from the model\n\nprint(\"Response:\", response)  # Print the generated response\n```\nThe code above defines a `Phi3VisionModel` class that abstracts the loading and usage of the model, making it easier to integrate into your applications. The `predict()` method demonstrates how to perform image\\-based inferences using a custom prompt.\n\nTo update the article with a focus on testing the OCR capabilities of the Phi\\-3\\-Vision\\-128K\\-Instruct model, we’ll add a section detailing how the model performs with real\\-world examples of scanned ID cards.\n\n\n## Testing OCR Capabilities with Scanned ID Cards\n\nTo evaluate the OCR performance of the Phi\\-3\\-Vision\\-128K\\-Instruct model, we tested it using several real\\-world scanned ID card images. These images vary in quality and clarity, providing a range of challenges for the model. The goal is to demonstrate how well the model can extract text information from documents with different characteristics, such as blurriness, complex backgrounds, and varying text fonts.\n\n**Image 1:** A fictional Utopian passport with detailed text, including personal information such as name, nationality, place of birth, date of issue, and expiration date. The text is slightly stylized, and there is a machine\\-readable zone at the bottom. The image quality is high, with no significant background noise.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MltpseOI3HhvCkUZMwLdEQ.png)\n\n**Output:**\n\n\n```python\n{\n  \"Type/Type\": \"P\",\n  \"Country code/Code du pays\": \"UTO\",\n  \"Passport Number/N° de passeport\": \"L898902C3\",\n  \"Surname/Nom\": \"ERIKSSON\",\n  \"Given names/Prénoms\": \"ANNA MARIA\",\n  \"Nationality/Nationalité\": \"UTOPIAN\",\n  \"Date of Birth/Date de naissance\": \"12 AUGUST/AOUT 74\",\n  \"Personal No./N° personnel\": \"Z E 184226 B\",\n  \"Sex/Sexe\": \"F\",\n  \"Place of birth/Lieu de naissance\": \"ZENITH\",\n  \"Date of issue/Date de délivrance\": \"16 APR/AVR 07\",\n  \"Authority/Autorité\": \"PASSPORT OFFICE\",\n  \"Date of expiry/Date d'expiration\": \"15 APR/AVR 12\",\n  \"Holder's signature/Signature du titulaire\": \"anna maria eriksson\",\n  \"Passport/Passeport\": \"P<UTOERIKSSON<<ANNA<MARIA<<<<<<<<<<<<<<<<<<<<<<<L898902C36UT07408122F1204159ZE184226B<<<<10\"\n}\n```\n**Image 2:** A Dutch passport with a clear image of the holder and neatly formatted text. Fields include the passport number, name, date of birth, nationality, and expiration date. The document is presented with high contrast, making text extraction relatively straightforward. The machine\\-readable zone (MRZ) at the bottom offers a structured data format that can help validate the accuracy of extracted information.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*WGV4tTxI9xISmAvFs8ovNw.png)\n\n**Output:**\n\n\n```python\nHere's the extracted full data from the passport in JSON format:\n\n{\n  \"passport\": {\n    \"issuingCountry\": \"Netherlands\",\n    \"issuingAuthority\": \"Koninkrijk der Nederlanden\",\n    \"passportNumber\": \"SPEC12014\",\n    \"issuingDate\": \"09 MAR 2014\",\n    \"expiryDate\": \"09 MAR 2024\",\n    \"holder\": {\n      \"gender\": \"F\",\n      \"nationality\": \"Netherlands\",\n      \"placeOfBirth\": \"SPECIMEN\",\n      \"sex\": \"WF\",\n      \"firstNames\": [\n        \"Willem\",\n        \"Lieselotte\"\n      ]\n    },\n    \"physicalDescription\": {\n      \"height\": \"1.75 m\",\n      \"hairColor\": \"gray\",\n      \"hairLength\": \"short\"\n    },\n    \"issuingOffice\": \"Burg. van Stad en Dorp\",\n    \"issuingDateAsInt\": \"14032014\",\n    \"expiryDateAsInt\": \"14032024\",\n    \"fieldsExtracted\": [\n      {\n        \"code\": \"NL\",\n        \"dateOfBirth\": \"10 MAR 1965\",\n        \"dateOfIssue\": \"09 MAR 2014\",\n        \"dateOfExpiry\": \"09 MAR 2024\",\n        \"firstNames\": [\n          \"Willem\",\n          \"Lieselotte\"\n        ],\n        \"nationality\": \"Netherlands\",\n        \"passportNumber\": \"SPEC12014\",\n        \"placeOfBirth\": \"SPECIMEN\",\n        \"sex\": \"WF\"\n      }\n    ]\n  }\n}\n```\n\n## Try Phi\\-3\\-Vision\\-128K\\-Instruct Yourself\n\nIf you want to try the Phi\\-3\\-Vision\\-128K\\-Instruct model for yourself, you can explore it through the following link: [Try Phi\\-3\\-Vision\\-128K\\-Instruct on Azure AI](https://ai.azure.com/explore/models/Phi-3-vision-128k-instruct/version/1/registry/azureml). This link allows you to experience the model’s capabilities and experiment with its OCR functionality.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*7feNu3ZuclgAnAzbJMMSFg.png)\n\n\n## Understanding the Architecture and Training\n\nThe **Phi\\-3\\-Vision\\-128K\\-Instruct** model is not just any language model — it’s a multimodal powerhouse that can process both visual and textual data. It has undergone a comprehensive training regime that included **500 billion tokens**, a blend of text and image data. Its architecture integrates a language model and image processing modules, creating a cohesive system that understands context over **128K tokens**, allowing for extended conversations or documents with large content.\n\nTrained on powerful hardware, such as **512 H100 GPUs**, and utilizing **flash attention** for memory efficiency, this model can handle large\\-scale tasks with ease. The training dataset includes a mix of synthetic and filtered real\\-world data, emphasizing **math, coding, common sense reasoning**, and **general knowledge**, making it versatile enough for various applications.\n\n\n## Key Benchmarks and Performance\n\nThe performance of Phi\\-3\\-Vision\\-128K\\-Instruct has been tested across multiple benchmarks, including **ScienceQA**, **AI2D**, **MathVista**, and **TextVQA**. Its scores consistently surpass many existing models in tasks that combine text and vision, particularly in areas such as:\n\n* **Document comprehension**: Extracting useful information from complex documents like PDFs or images.\n* **Table and chart understanding**: Accurately interpreting graphical data and converting it into textual explanations.\n\nIn particular, the model achieved an impressive **81\\.4%** on **ChartQA** and **76\\.7%** on **AI2D**, showcasing its capability to understand data\\-rich documents effectively.\n\n\n## Why OCR and Document Extraction Matter\n\nDocument extraction and OCR are vital for businesses and research, enabling the conversion of printed or handwritten text into machine\\-readable formats. Tasks such as **PDF parsing**, **data entry automation**, **invoice processing**, and **legal document analysis** are significantly simplified by using AI models like Phi\\-3\\-Vision\\-128K\\-Instruct.\n\nWhether you are dealing with scanned documents, screenshots, or photographed pages, the model’s multimodal capabilities can help to **automate data extraction**, making it a valuable tool for improving productivity and reducing manual effort.\n\n\n## Responsible AI and Safety Measures\n\nWhile the model is powerful, it comes with limitations that developers should keep in mind. **Language biases**, **stereotype reinforcement**, and **inaccurate content generation** are potential issues. For high\\-risk use cases, such as **health or legal advice**, additional layers of **verification and content filtering** are necessary.\n\n\n## Future Directions and Fine\\-Tuning\n\nLooking to extend Phi\\-3\\-Vision\\-128K\\-Instruct’s capabilities? Fine\\-tuning is supported and can be performed using the **Phi\\-3 Cookbook**, which provides recipes for adjusting the model to specific tasks like **document classification**, **enhanced OCR accuracy**, and **specialized image understanding**.\n\n\n## Conclusion\n\nThe Phi\\-3\\-Vision\\-128K\\-Instruct isn’t just a step forward for multimodal AI; it’s a leap into a future where **document extraction, OCR, and AI\\-driven content generation** are seamless and accessible. With extensive training, robust architecture, and thoughtful design, this model empowers developers to transform data processing across various fields.\n\nStay tuned for more advanced examples and tutorials on integrating this model with real\\-world applications, where we will explore **processing multiple document types** and applying **AI\\-powered techniques** to extract valuable insights from diverse sources.\n\nThe future of **AI\\-powered document extraction** has never looked more promising!\n\n\n"},{"lang":"en","group":"blog","slug":"blog/alibabas-open-source-qwen-how-it-s-revolutionizing-ai-and-how-you-can-use-it-dcba8f687c97","frontmatter":{"title":"Alibaba’s Open-Source Qwen: How It’s Revolutionizing AI and How You Can Use It","meta_title":"Alibaba’s Open-Source Qwen: How It’s Revolutionizing AI and How You Can Use It","description":"Alibaba has recently made waves in the AI world by open-sourcing its Qwen 2.5 models during the 2024 Apsara Conference. With over 100…","date":"2024-10-26T00:26:25.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*I7QDwbLMzoJ_ORq5.jpg","categories":["Programming","Machine Learning","Natural Language Processing"],"author":"Rifx.Online","tags":["Qwen","multimodal","open-source","fine-tune","text-to-video"],"draft":false,"slug":"blog/alibabas-open-source-qwen-how-it-s-revolutionizing-ai-and-how-you-can-use-it-dcba8f687c97"},"content":"\nAlibaba has recently made waves in the AI world by open-sourcing its **Qwen 2.5** models during the 2024 Apsara Conference. With over 100 models, Qwen spans multiple modalities including language, vision, audio, and code, making it one of the most comprehensive open-source AI solutions. The release empowers developers by providing tools for diverse applications, from text-to-video generation to real-time question answering.\n\n\n\n## Key Features of Alibaba’s Qwen Models\n\n1. **Multimodal Capabilities**: Qwen models handle diverse inputs, including text, audio, and visual data. This multimodal approach makes them suitable for a wide range of industries, from media and entertainment to robotics.\n2. **Open Source**: Available on platforms like **Hugging Face** and **ModelScope**, Qwen has already been downloaded over 40 million times, with over 50,000 custom models built on its foundation.\n3. **Enhanced Performance**: Qwen2.5 introduces improved language understanding, mathematics, and coding capabilities, rivaling leading models in the field. With optimized performance for tasks like structured data understanding and long text generation, Qwen opens the door to high-level AI applications.\n\n## How to Use Alibaba’s Qwen\n\nDevelopers and organizations can access Qwen models on platforms like Hugging Face, where they can:\n\n* **Fine-tune models**: Tailor Qwen for specific industry applications such as customer service, automation, or video content creation.\n* **Integrate with applications**: Qwen’s text-to-video model can be incorporated into media production pipelines, generating dynamic content from static images and text prompts.\n* **Develop AI assistants**: With enhanced vision-language models, Qwen can be used in robotics and autonomous vehicles to process video data and perform real-time tasks like navigation or object recognition.\n\n**Example of Using Qwen via Hugging Face**:\n\n```python\nfrom transformers import QwenTokenizer, QwenModel\n\ntokenizer = QwenTokenizer.from_pretrained(\"qwen-2.5\")\nmodel = QwenModel.from_pretrained(\"qwen-2.5\")\n\ninput_text = \"What is the future of AI in healthcare?\"\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\noutputs = model(input_ids)\n```\n\nThis allows users to access Qwen models, run inference, and customize them based on specific needs.\n\n## Qwen’s Impact Across Industries\n\n1. **Media and Entertainment**: With the new text-to-video capabilities, Qwen can automatically generate videos from written scripts, transforming the creative industry by automating tedious production tasks.\n2. **Robotics and Autonomous Vehicles**: The enhanced vision-language models in Qwen can help robots understand real-world environments, leading to better decision-making in autonomous driving or manufacturing.\n3. **Software Development**: Alibaba’s AI Developer tool, powered by Qwen, automates tasks like code generation, debugging, and requirement analysis, enabling developers to focus on higher-level problem-solving.\n\n## Conclusion: A New Era of Open AI Innovation\n\nBy open-sourcing its Qwen 2.5 models, Alibaba is democratizing access to advanced AI technologies. Developers, startups, and large enterprises alike can harness Qwen’s multimodal and real-time capabilities to drive innovation in industries ranging from media to autonomous vehicles. Whether you’re a developer looking to fine-tune models for a niche application or a corporation integrating AI into your infrastructure, Qwen offers powerful tools to accelerate progress.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/artifacts-top-mindblowing-uses-of-claude-3-5-sonent-6830b2acfa4b","frontmatter":{"title":"Artifacts: Top Mindblowing uses of Claude 3.5 Sonent","meta_title":"Artifacts: Top Mindblowing uses of Claude 3.5 Sonent","description":"Anthropic recently launched its most advanced LLM, “Claude 3.5 Sonnet,” and it’s mindblowing. People on social media called this model the…","date":"2024-11-08T00:18:38.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*XL1dN9VCFcbz3m5N","categories":["Programming","Natural Language Processing","Generative AI"],"author":"Rifx.Online","tags":["Sonnet","context","Artifacts","code","generation"],"draft":false,"slug":"blog/artifacts-top-mindblowing-uses-of-claude-3-5-sonent-6830b2acfa4b"},"content":"\n\n\n\nAnthropic recently launched its most advanced LLM, “Claude 3\\.5 Sonnet,” and it’s mindblowing. People on social media called this model the most advanced LLM currently available. This AI model outperforms all the existing LLMs, such as GPT\\-4 GPT\\-4o mini, Llama 3, etc. Claude 3\\.5 Sonnet has a context window of 200K with a max output of 8192 tokens. It can generate text in an enormous paragraph with much data as input. Claude 3\\.5 Sonnet is one of the best AI vision models, beating GPT\\-4o and Llama 3 in various test cases. It can extract data and text from documents and PDFs. These are just some factors that make the Claude 3\\.5 Sonnet the best, but there is also a new feature that Anthropic added, which makes the LLM the best code generator, “Artifacts.” It is a pop\\-up window that allows the users to see their code, edit it, and see their project live in that pop\\-up window. This article will show the top cases of Artifacts and Claude 3\\.5 Sonnet and the new projects you can develop using this tool.\n\n\n\n\n## Use Cases of Claude 3\\.5 Sonnet\n\nTo use the Artifacts, you must have a Claude account. Claude 3\\.5 Sonnet is Free for everyone but allows only limited chats per day. Users must buy a subscription for around $20 to use the model with unlimited chats. Try these prompts in Claude 3\\.5 Sonnet and create exciting tools. Users can also publish their projects on the Internet by sharing the link.\n\n\n### 1\\. Interactive PDF Dashboard\n\nReading large PDFs is boring. So, let’s create an interactive PDF dashboard that will allow me to read my PDFs better.\n\n***Prompt\\- Create an interactive PDF dashboard to help me view, read, and learn from this information in a more visually appealing way. It has a tab where I can get a quiz based on the information in the PDF. Make sure it has a dark mode.***\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*66Iowbu8ZKA-IGfK)\n\nTo use this prompt, first attach the PDF with the prompt whose summary you want to read. Users can also make changes according to their preferences, and Claude 3\\.5 Sonnet will generate the code again with the changes.\n\n[***Click here to see the project***](https://claude.site/artifacts/4b9590a8-260e-476a-ab75-ec1d69f81d1e)***.***\n\n\n### 2\\. Visualization of anything with animations\n\nVisualizing anything you’ve read is probably the best way to understand it. Adding animations will take your visualization to the next level, making it easy to understand.\n\n***Prompt: Animate each process step to Create a visualization of photosynthesis. Make sure to make the whole dashboard visually appealing using different colors and textures to create high\\-quality animations.***\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*W5lWitTO5fj7lOs0)\n\nIt is the first output we get, which is impressive within 10 seconds of processing but could be better after refining the prompts by giving more instruction and precision. We get\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*TAguaCWhC1CYFNw7)\n\nAs you can see, we separated the whole cycle into different stages, and the final result is incredible. This method can help you understand any process by breaking it into more straightforward steps.\n\n[***Click here to see the project***](https://claude.site/artifacts/b9769e4e-c20b-43da-945a-bfc41782900c)***.***\n\n\n### 3\\. Scientific Tools\n\nScientific tools are tools and services that help users understand the concept of science quickly and visually. With the help of Claude 3\\.5 Sonnet, users can create scientific tools at a single prompt. Just describe your tool in detail. I will use it to create an interactive tool for electronics engineering.\n\n***Prompt — Create a dashboard using React that shows the Diode. We can connect it forward\\-biased or reverse\\-biased in the circuit, and based on physical factors such as doping and depletion region, we change the scenario by showing the movement of the holes and electrons in the diode. There is an option to change everything, such as voltage, current, type of diode, Si, or Ge, and use different colors and moving elements for the animation.***\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*9bE50q8nsh6_SQ36)\n\nUsers can use Claude 3\\.5 Sonnet to create multiple tools, such as an essential project school, the magnet’s current effect, the pressure and temperature relation tool, atomic models animation, etc.\n\n***Click [here to see the project](https://claude.site/artifacts/5b18fb27-1093-41d2-ac4e-54e47e4ddd3a).***\n\n\n### 4\\. Game Development\n\nEveryone likes to play games. Using Claude 3\\.5 Sonnet, we can create different types of games. I am going to make a Tic Tac Toe game using LLM.\n\n***Prompt\\- create a tic tac toe game using react and make it functional. Make it visually appealing by using CSS.***\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*jlSn7mCbaFadUT3F)\n\nAnd the result could be better. After that, I tried to give more suggestions and input, and based on my suggestions, Claude developed a perfect Tic Tac Toe Game, and here is the result.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*4v9KP4KFmQgY14YZ)\n\nAs you can see, the final product refines the CSS. The animations have greatly improved, and a pop\\-up with the winner’s name is added by the end of the game. You can use these prompts to make games such as Snake Game, Ludo, Rock Paper Scissors, etc.\n\n[***Click here to try the project.***](https://claude.site/artifacts/d57fdf93-79fb-443a-9bee-a58ab6eb911f)\n\n\n### 5\\. Web Application\n\nWeb applications are tools that can be accessed directly on the browser without installing the app on the phone. Claude 3\\.5 Sonnet can be used to develop different web application types. In this case, we will examine an expense tracker app.\n\n***Prompt\\- Create a web application for an expense tracker with the following features: First, ask about their monthly expenses — say Rs. 2000\\. Now, whatever the person spends, make sure you make some categories like Food, Travel, and Necessity and have the option to add anything. Add an option like saving money at the end of the month. Also, it has an investing option, which you now know the type of. For Example, “If you invest 200 monthly in mutual funds, after five years, you will have X amount. Also, the app has a feature to generate graphs and charts related to the expenses. Make sure that the app’s UI/UX design is visually appealing.***\n\nAnd this is the final result.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*XGOwT3vpxL0dxzzP)\n\nUsers can use Claude 3\\.5 Sonnet to create multiple tools, such as a To\\-do list, simple calculator, movie recommendation app, text summarization tool, etc.\n\n[***Click here to try the project***](https://claude.site/artifacts/66770d05-aafe-45c4-b938-8cda0e82b903)***.***\n\n\n### 6\\. 3D Simulation\n\nClaude 3\\.5 Sonent can create 3D simulations of models or projects, whether solar system models, atomic mode, The Central Dogma of Molecular Biology, etc. You can visualize any idea in 3D using this LLM. We will simulate a 3D solar system model with all the planets revolving around the sun.\n\n***Prompt\\-make a three js app of a space sim with the gravity of a solar system with planets in a single web file. solar system in which the planet revolves around the sun… whenever someone hovers the mouse, it pops up the planet’s name and basic details such as mass, gravity, etc.; keep the physics concepts, such as the rotation and revolution of the planet, intact.***\n\nUse this prompt to get the desired results.\n\n***Click here to see the Project.***\n\n[***Project 1***](https://x.com/websim_ai/status/1803901523522699730?t=BCe28ywbC2xD1Mk4DRmo5w&s=08)\n\n[***Project 2***](https://x.com/ammaar/status/1804649903815115053?t=7PeWPg62bkABtKEtKVmFbw&s=08)\n\n[***Project 3***](https://x.com/goldcaddy77/status/1804724702901891313?t=iqcLQBhYaIRnt3gIBRKBQg&s=08)\n\n\n### 7\\. Mind Map\n\nA mind map is a brainstorming technique for visually organizing information hierarchically. The main feature is to make one main idea the central point of the diagram, with subtopics branching out and connecting to supporting ideas. It helps the readers easily memorize and understand the information quickly. Let’s create a prompt by creating a mind map.\n\n***Prompt: Create a mind map of how object detection works. Use animations and colors to make it interactive and visually appealing.***\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*igUaLzf8b2ZTEy-m)\n\nBy changing the topic, you can use this prompt to generate any mind map you want. You can also add different colors and animations to make it more interactive.\n\n[***Click here to try the project***](https://claude.site/artifacts/f1ce9002-9434-4b40-9713-ef184c467557)\n\n\n### 8\\. SEO Tool\n\nSEO is essential for any blog post to rank on Google. Using Claude, we can create a tool that can help us to improve the seo of our website. You can use this prompt to make your tool.\n\n***Prompt\\- Create an SEO tool that allows me to upload my blog post, industry, and keyword for which I am trying to rank. After I upload all that stuff, I want to hit a button that gives highlighted suggestions on what to change. Include a reset button that starts the process over. And also added a plus button to keywords where I can add multiple. In the SEO suggestions area, after analyzing the SEO, give me specific numbers, stats from my blog post, and caution signs to show what needs attention. After I generate the blog post, give me a revised blog post suggestion based on the keywords. I will rank it and explain why the suggestion is also being made. Make it more interactive by making the CSS more advanced and adding one unique feature that you think is necessary to make the tool more helpful for bloggers. Make sure that the tool has a UI/UX design that is visually appealing.***\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*OfY0zIa4rzO7J55r)\n\nUsers can create a variety of tools that not only help them in SEO but also in various fields to increase productivity.\n\n[***Click here to try the project***](https://claude.site/artifacts/09f21906-6295-4d0a-9fa0-be4afd6dab71)***.***\n\n\n### 9\\. Object Detection Tool\n\nYou can also use Claude 3\\.5 Sonent to make projects based on Artficla Inteliigence and Machine Learning. You can try to create amazing projects by adding features.\n\n***Prompt\\- Create a single HTML file for a real\\-time object detection web application. Using TensorFlow.js and the COCO\\-SSD model***\n\n***The application should:***\n\n* ***Access the user’s webcam and display the video feed.***\n* ***Perform object detection on the video feed in real time.***\n* ***Draw bounding boxes around detected objects and label them with their class and detection confidence.***\n* ***Display a list of uniquely detected objects below the video feed, showing the object class and when it was first detected.***\n* ***Ensure each object class is only listed once, regardless of how often it’s detected.***\n* ***Use a detection frequency of 2 FPS to balance performance and responsiveness.***\n* ***Include error handling for camera access and model loading.***\n* ***Style the application for a clean, modern look with a responsive design.***\n* ***Include all necessary HTML, CSS, and JavaScript in a self\\-contained HTML file.***\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*QSvMcxCuulkRKOlm)\n\n\n## Conclusion\n\nClaude 3\\.5 Sonnet, Anthropic’s latest LLM, has revolutionized AI capabilities with its advanced features and versatile applications. The model’s impressive 200K context window, superior vision capabilities, and innovative “Artifacts” feature for code generation set it apart from competitors. This article showcased diverse use cases of Claude 3\\.5 Sonnet, demonstrating its potential in creating interactive dashboards, visualizations, scientific tools, games, web applications, 3D simulations, mind maps, and SEO tools. These examples highlight the model’s ability to generate complex, functional, and visually appealing projects with simple prompts. Claude 3\\.5 Sonnet’s user\\-friendly interface and powerful capabilities make it an invaluable tool for developers, educators, and professionals across various fields, opening up new possibilities for AI\\-assisted creation and problem\\-solving.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/build-a-customer-support-assistant-with-llama3-1-7bf60611e428","frontmatter":{"title":"Build a Customer Support Assistant with Llama3.1","meta_title":"Build a Customer Support Assistant with Llama3.1","description":"Use LLM Agents and Amazon Bedrock to Solve Customer Queries with AI: A Guide to Building and Deploying a Support Assistant with Llama3.1","date":"2024-11-13T01:22:29.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*lNyf72c2_r1wKjnoRA1_FQ.png","categories":["Programming","Chatbots","Technology/Web"],"author":"Rifx.Online","tags":["Llama3.1","AmazonBedrock","Gradio","EC2","CustomerSupport"],"draft":false,"slug":"blog/build-a-customer-support-assistant-with-llama3-1-7bf60611e428"},"content":"\n\n\n\n\n### Use LLM Agents and Amazon Bedrock to Solve Customer Queries with AI: A Guide to Building and Deploying a Support Assistant with Llama3\\.1\n\n\n\n\n## Introduction\n\n\n### Problem\n\nBusinesses often face the challenge of handling a large volume of customer inquiries. These queries can range from mundane questions like “What is the status of my order?” to more complex issues requiring human intervention. The sheer volume of repetitive queries can overwhelm customer support teams, leading to longer response times and reduced customer satisfaction. Additionally, utilizing human resources for simple, routine queries is inefficient and costly. There’s a growing need for automated solutions that can handle routine queries effectively, allowing human agents to focus on escalated cases that require nuanced problem\\-solving.\n\n\n### Solution\n\nThe introduction of Large Language Model (LLM) agents offers a promising solution to this problem. An [LLM agent](https://proxy.rifx.online/https://research.ibm.com/blog/what-are-ai-agents-llm) can respond to user queries by accessing and interpreting data from a company’s database, handling simple operations such as checking order status, retrieving account information, and answering FAQs. By automating these routine tasks, an LLM agent ensures faster resolution times and frees up human resources for more complex customer support scenarios. In this guide, we’ll explore how to build a customer support assistant using the Llama3\\.1 model from Amazon Bedrock Tools api.\n\nAt the end, we will have the assistant running locally in our machine and making calls to a fake database:\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Ok9N3mdX50JVWbaJKUrJeQ.gif)\n\n\n## LLM Agents\n\n\n### What are LLM agents\n\n[LLM agents](https://proxy.rifx.online/https://research.ibm.com/blog/what-are-ai-agents-llm) are specialized applications built on large language models like Llama3\\.1, designed to perform specific tasks or functions. Unlike general LLMs, which generate human\\-like text based on a given prompt, LLM agents are equipped with additional capabilities such as accessing external databases, performing operations, and making decisions based on predefined rules. They are tailored to handle specific use cases, such as customer support, where they can interact with users, retrieve information, and execute commands based on the context of the conversation.\n\nWhile general LLMs are powerful in generating coherent text and understanding language, LLM agents take this a step further by integrating with external systems, allowing them to perform real\\-world tasks beyond just text generation.\n\nAgent have set of instructions, a foundation model, a set of available actions and knowledge bases, which enables then to execute complex tasks.\n\nA generative model can answer a general question, or a question related to your documentation, like “I can’t see my meetings?, How do I book a meeting?”. An agent, using a foundational model as their reasoning logic and external data sources like your APIs, can return the user their no. of booked meetings, or directly schedule a meeting from the interaction screen.\n\nThere are many agents in the “general purpose” category, and also specialized agents for task specific purpose like code assistant ([Amazon CodeWhisperer, Copilot](https://proxy.rifx.online/https://www.missioncloud.com/blog/github-copilot-vs-amazon-codewhisperer)), writing assistant, system design ([Amazon Q](https://proxy.rifx.online/https://aws.amazon.com/q/)) , wikipedia summary, etc.\n\n**AI agents landscape:**\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*VuAyzZ2BfrD7o-z0lOpUwA.png)\n\n\n### Creating a Basic Agent from Scratch Using Python\n\nLet’s create a simple LLM agent from scratch using Python. This amazing medium article demonstrates how to build an agent without relying on any libraries or frameworks.\n\n\n## Custom Support Assistant\n\nNow, let’s create a more sophisticated customer support assistant using the [Llama3\\.1](https://proxy.rifx.online/https://llama.meta.com/) model from [Bedrock](https://proxy.rifx.online/https://aws.amazon.com/bedrock/) Tools. This agent will be able to perform more complex tasks, such as looking up user data from a database and executing simple operations like viewing shipping status of an order.\n\n\n### Defining Capabilities and boundaries\n\nBefore building our assistant, it’s essential to define what actions the agent can perform and establish clear boundaries for its operation. In a production environment, these capabilities and boundaries are crucial to ensure the agent operates effectively and securely.\n\n**Capabilities:**\n\n* Respond to common customer queries (e.g., order status, return policy).\n* Access and retrieve user data from a database.\n* Perform simple operations like viewing order status, updating customer information, etc.\n\n**Boundaries:**\n\n* The agent should not execute actions that require human judgment, such as processing refunds or handling escalations.\n* It should operate within the defined scope and not access sensitive data unless explicitly permitted.\n* Error handling and fallback mechanisms should be in place for unsupported queries.\n\n\n### Architecture\n\nThe system architecture for our solution involves several components working together:\n\n1. **LLM Agent**: The core of the system, built using the [Llama3\\.1](https://proxy.rifx.online/https://llama.meta.com/) or [Claude 3\\.5 Sonnet](https://proxy.rifx.online/https://www.anthropic.com/news/claude-3-5-sonnet) model, which handles natural language processing and decision\\-making.\n2. **Database**: Stores customer data and other relevant information that the agent can query.\n3. **API Layer**: Facilitates communication between the LLM agent and the database, allowing the agent to retrieve and manipulate data.\n4. **User Interface**: A frontend interface (e.g., a chatbot interface) where customers interact with the support assistant.\n\n\n### Code\n\nBefore we examine the code, please ensure you have the following:\n\n1. Knowledge of Python and the [boto3](https://proxy.rifx.online/https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) library.\n2. A working AWS account with model access enabled in [Bedrock](https://proxy.rifx.online/https://aws.amazon.com/bedrock/).\n3. [A virtual environment](https://proxy.rifx.online/https://docs.anaconda.com/miniconda/) with Python and boto3 installed.\n\n\n\n\n\n\n\n\n### Code Walkthrough\n\n\n```python\nfrom datetime import datetime\nimport json\nfrom typing import Any, Dict, List\n\nimport boto3\nfrom botocore.exceptions import ClientError\n\n## Initialize a Boto3 session and create a Bedrock runtime client\nsession = boto3.Session()\nregion = \"us-east-1\" # us-west-2 has better runtime quota\nbedrock_client = session.client(service_name = 'bedrock-runtime', region_name = region)\n```\nFirst, we import the necessary packages and create an instance of the `boto3` Bedrock runtime client, called `bedrock_client`, for the `us-east-1` region. If your AWS account has the `us-west-2` availability zone (AZ) enabled, use that instead. At the time of writing, Llama3\\.1 models are only available in the `us-west-2` AZ and it also has a larger runtime quota for the `claude-3.5-sonnet` model (250 requests per minute) compared to the `us-east-1` AZ, which supports only 50 requests per minute.\n\n\n```python\n## Define available models with their respective request limits\navailable_models = {\n    \"sonnet3-5\": \"anthropic.claude-3-5-sonnet-20240620-v1:0\", # 50 requests per min\n    \"sonnet\": \"anthropic.claude-3-sonnet-20240229-v1:0\", # 500 requests per min\n    \"llama31-70b\": \"meta.llama3-1-70b-instruct-v1:0\", # 400 requests per min\n    \"llama31-405b\": \"meta.llama3-1-405b-instruct-v1:0\", # 50 requests per min\n}\nmodelId = available_models[\"sonnet3-5\"]  # Select model for conversation\n```\nNext, we create a mapping of model IDs in Bedrock. **Currently not all the models available in Amazon Bedrock support tool use**. Please check the list of [supported models](https://proxy.rifx.online/https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html#conversation-inference-supported-models-features) from Amazon Bedrock user guide [here](https://proxy.rifx.online/https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html#conversation-inference-supported-models-features).\n\n\n```python\nclass FakeDatabase:\n    \"\"\"Sample fake database implementation.\"\"\"\n    def __init__(self):\n        self.customers = [\n            {\"id\": \"1213210\", \"name\": \"John Doe\", \"email\": \"john@gmail.com\", \"phone\": \"123-456-7890\", \"username\": \"johndoe\"},\n            {\"id\": \"2837622\", \"name\": \"Priya Patel\", \"email\": \"priya@candy.com\", \"phone\": \"987-654-3210\", \"username\": \"priya123\"},\n            {\"id\": \"3924156\", \"name\": \"Liam Nguyen\", \"email\": \"lnguyen@yahoo.com\", \"phone\": \"555-123-4567\", \"username\": \"liamn\"},\n            {\"id\": \"4782901\", \"name\": \"Aaliyah Davis\", \"email\": \"aaliyahd@hotmail.com\", \"phone\": \"111-222-3333\", \"username\": \"adavis\"},\n            {\"id\": \"5190753\", \"name\": \"Hiroshi Nakamura\", \"email\": \"hiroshi@gmail.com\", \"phone\": \"444-555-6666\", \"username\": \"hiroshin\"},\n            {\"id\": \"6824095\", \"name\": \"Fatima Ahmed\", \"email\": \"fatimaa@outlook.com\", \"phone\": \"777-888-9999\", \"username\": \"fatimaahmed\"},\n            {\"id\": \"7135680\", \"name\": \"Alejandro Rodriguez\", \"email\": \"arodriguez@protonmail.com\", \"phone\": \"222-333-4444\", \"username\": \"alexr\"},\n            {\"id\": \"8259147\", \"name\": \"Megan Anderson\", \"email\": \"megana@gmail.com\", \"phone\": \"666-777-8888\", \"username\": \"manderson\"},\n            {\"id\": \"9603481\", \"name\": \"Kwame Osei\", \"email\": \"kwameo@yahoo.com\", \"phone\": \"999-000-1111\", \"username\": \"kwameo\"},\n            {\"id\": \"1057426\", \"name\": \"Mei Lin\", \"email\": \"meilin@gmail.com\", \"phone\": \"333-444-5555\", \"username\": \"mlin\"}\n        ]\n\n        self.orders = [\n            {\"id\": \"24601\", \"customer_id\": \"1213210\", \"product\": \"Wireless Headphones\", \"quantity\": 1, \"price\": 79.99, \"status\": \"Shipped\"},\n            {\"id\": \"13579\", \"customer_id\": \"1213210\", \"product\": \"Smartphone Case\", \"quantity\": 2, \"price\": 19.99, \"status\": \"Processing\"},\n            {\"id\": \"97531\", \"customer_id\": \"2837622\", \"product\": \"Bluetooth Speaker\", \"quantity\": 1, \"price\": \"49.99\", \"status\": \"Shipped\"}, \n            {\"id\": \"86420\", \"customer_id\": \"3924156\", \"product\": \"Fitness Tracker\", \"quantity\": 1, \"price\": 129.99, \"status\": \"Delivered\"},\n            {\"id\": \"54321\", \"customer_id\": \"4782901\", \"product\": \"Laptop Sleeve\", \"quantity\": 3, \"price\": 24.99, \"status\": \"Shipped\"},\n            {\"id\": \"19283\", \"customer_id\": \"5190753\", \"product\": \"Wireless Mouse\", \"quantity\": 1, \"price\": 34.99, \"status\": \"Processing\"},\n            {\"id\": \"74651\", \"customer_id\": \"6824095\", \"product\": \"Gaming Keyboard\", \"quantity\": 1, \"price\": 89.99, \"status\": \"Delivered\"},\n            {\"id\": \"30298\", \"customer_id\": \"7135680\", \"product\": \"Portable Charger\", \"quantity\": 2, \"price\": 29.99, \"status\": \"Shipped\"},\n            {\"id\": \"47652\", \"customer_id\": \"8259147\", \"product\": \"Smartwatch\", \"quantity\": 1, \"price\": 199.99, \"status\": \"Processing\"},\n            {\"id\": \"61984\", \"customer_id\": \"9603481\", \"product\": \"Noise-Cancelling Headphones\", \"quantity\": 1, \"price\": 149.99, \"status\": \"Shipped\"},\n            {\"id\": \"58243\", \"customer_id\": \"1057426\", \"product\": \"Wireless Earbuds\", \"quantity\": 2, \"price\": 99.99, \"status\": \"Delivered\"},\n            {\"id\": \"90357\", \"customer_id\": \"1213210\", \"product\": \"Smartphone Case\", \"quantity\": 1, \"price\": 19.99, \"status\": \"Shipped\"},\n            {\"id\": \"28164\", \"customer_id\": \"2837622\", \"product\": \"Wireless Headphones\", \"quantity\": 2, \"price\": 79.99, \"status\": \"Processing\"}\n        ]\n\n    def get_user(self, key:str, value:str) -> Dict[str, str]:\n        \"\"\"Return metadata of user.\"\"\"\n        if key in {\"email\", \"phone\", \"username\"}:\n            for customer in self.customers:\n                if customer[key] == value:\n                    return customer\n            return f\"Couldn't find a user with {key} of {value}\"\n        else:\n            raise ValueError(f\"Invalid key: {key}\")\n        \n        return None\n\n    def get_order_by_id(self, order_id: str) -> Dict[str, str]:\n        \"\"\"Return metadata of the order using order id.\"\"\"\n        for order in self.orders:\n            if order[\"id\"] == order_id:\n                return order\n        return None\n    \n    def get_customer_orders(self, customer_id: str) -> List[Dict[str, str]]:\n        \"\"\"Return a list of orders for a specific customer.\"\"\"\n        return [order for order in self.orders if order[\"customer_id\"] == customer_id]\n\n    def cancel_order(self, order_id: str) -> str:\n        \"\"\"Cancel an order if it's in 'Processing' status.\"\"\"\n        order = self.get_order_by_id(order_id)\n        if order:\n            if order[\"status\"] == \"Processing\":\n                order[\"status\"] = \"Cancelled\"\n                return \"Cancelled the order\"\n            else:\n                return \"Order has already shipped.  Can't cancel it.\"\n        return \"Can't find that order!\"\n```\nFor this demo, we implement a mock database class with a predefined list of customers and their orders. This mock database class also includes methods to retrieve data from the database.\n\n* `get_user` : Returns the user\n* `get_order_by_id` : Returns the order using order id\n* `get_customer_orders` : Returns all the orders of a particular customer\n* `cancel_order` : Cancel an order if it’s in ‘Processing’ status.\n\n\n```python\n## Define all the tools avilable to the model\ntool_config = {\n    \"tools\": [\n        {\n            \"toolSpec\": {\n                \"name\": \"get_user\",\n                \"description\": \"Looks up a user by email, phone, or username.\",\n                \"inputSchema\": {\n                    \"json\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"key\": {\n                                \"type\": \"string\",\n                                \"enum\": [\"email\", \"phone\", \"username\"],\n                                \"description\": \"The attribute to search for a user by (email, phone, or username).\",\n                            },\n                            \"value\": {\n                                \"type\": \"string\",\n                                \"description\": \"The value to match for the specified attribute.\",\n                            },\n                        },\n                        \"required\": [\"key\", \"value\"],\n                    }\n                },\n            }\n        },\n        {\n            \"toolSpec\": {\n                \"name\": \"get_order_by_id\",\n                \"description\": \"Retrieves the details of a specific order based on the order ID. Returns the order ID, product name, quantity, price, and order status.\",\n                \"inputSchema\": {\n                    \"json\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"order_id\": {\n                                \"type\": \"string\",\n                                \"description\": \"The unique identifier for the order.\",\n                            }\n                        },\n                        \"required\": [\"order_id\"],\n                    }\n                },\n            }\n        },\n        {\n            \"toolSpec\": {\n                \"name\": \"get_customer_orders\",\n                \"description\": \"Retrieves the list of orders belonging to a user based on a user's customer id.\",\n                \"inputSchema\": {\n                    \"json\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"customer_id\": {\n                                \"type\": \"string\",\n                                \"description\": \"The customer_id belonging to the user\",\n                            }\n                        },\n                        \"required\": [\"customer_id\"],\n                    }\n                },\n            }\n        },\n        {\n            \"toolSpec\": {\n                \"name\": \"cancel_order\",\n                \"description\": \"Cancels an order based on a provided order_id.  Only orders that are 'processing' can be cancelled\",\n                \"inputSchema\": {\n                    \"json\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"order_id\": {\n                                \"type\": \"string\",\n                                \"description\": \"The order_id pertaining to a particular order\",\n                            }\n                        },\n                        \"required\": [\"order_id\"],\n                    }\n                },\n            }\n        },\n    ],\n    \"toolChoice\": {\"auto\": {}},\n}\n```\nNext we define a `tool_config` .\n\nYou can use the Amazon Bedrock API to give a model access to [tools](https://proxy.rifx.online/https://docs.aws.amazon.com/bedrock/latest/userguide/tool-use.html) that can help it generate responses for messages that you send to the model. For example, you might have a chat application that lets users find out out the most popular song played on a radio station. To answer a request for the most popular song, a model needs a tool that can query and return the song information.\n\n\n> Tool use with models is also known as *Function calling*.\n\nIn Amazon Bedrock, the model doesn’t directly call the tool. Rather, when you send a message to a model, you also supply a definition for one or more tools that could potentially help the model generate a response. In this example, you would supply a definition for tools that returns the customer details, order details or cancel an order. If the model determines that it needs the tool to generate a response for the message, the model responds with a request for you to call the tool. It also includes the input parameters (the required customer id or order id) to pass to the tool.\n\nIn your code, you call the tool on the model’s behalf. In this scenario, assume the tool implementation is an API. The tool could just as easily be a database, Lambda function, or some other software. You decide how you want to implement the tool. You then continue the conversation with the model by supplying a message with the result from the tool. Finally the model generates a response for the original message that includes the tool results that you sent to the model.\n\nIn our example, we define all the functions we want the chatbot to execute in the `tool_config` . Refer to the [Amazon Bedrock documentation](https://proxy.rifx.online/https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_ToolConfiguration.html) for more information on the ToolConfiguration API.\n\n\n```python\ndef process_tool_call(tool_name: str, tool_input: Any) -> Any:\n    \"\"\"Process the tool call based on the tool name and input.\"\"\"\n    if tool_name == \"get_user\":\n        return db.get_user(tool_input[\"key\"], tool_input[\"value\"])\n    elif tool_name == \"get_order_by_id\":\n        return db.get_order_by_id(tool_input[\"order_id\"])\n    elif tool_name == \"get_customer_orders\":\n        return db.get_customer_orders(tool_input[\"customer_id\"])\n    elif tool_name == \"cancel_order\":\n        return db.cancel_order(tool_input[\"order_id\"])\n```\nSince our application code will call the required tools on behalf of the LLM, we package all the tools into a single function. The `process_tool_call` function executes the appropriate functions based on the `tool_name` and `tool_input` provided by the LLM.\n\n\n```python\ndef simple_chat():\n    \"\"\"Main chat function that interacts with the user and the LLM.\"\"\"\n    system_prompt = \"\"\"\n    You are a customer support chat bot for an online retailer called TechNova. \n    Your job is to help users look up their account, orders, and cancel orders.\n    Be helpful and brief in your responses.\n    You have access to a set of tools, but only use them when needed.  \n    If you do not have enough information to use a tool correctly, ask a user follow up questions to get the required inputs.\n    Do not call any of the tools unless you have the required data from a user. \n    \"\"\"\n    # Initial user message\n    user_message = input(\"\\nUser: \")\n    messages = [{\"role\": \"user\", \"content\": [{\"text\": user_message}]}]\n\n    while True:\n        # If the last message is from the assistant, get another input from the user\n        if messages[-1].get(\"role\") == \"assistant\":\n            user_message = input(\"\\nUser: \")\n            messages.append({\"role\": \"user\", \"content\": [{\"text\": user_message}]})\n\n        # Parameters for API request to the Bedrock model\n        converse_api_params = {\n            \"modelId\": modelId,\n            \"system\": [{\"text\": system_prompt}],\n            \"messages\": messages,\n            \"inferenceConfig\": {\"maxTokens\": 4096},\n            \"toolConfig\": tool_config,  # Pass the tool config\n        }\n\n        # Get response from Bedrock model\n        response = bedrock_client.converse(**converse_api_params)\n\n        # Append assistant's message to the conversation\n        messages.append(\n            {\"role\": \"assistant\", \"content\": response[\"output\"][\"message\"][\"content\"]}\n        )\n\n        # If the model wants to use a tool, process the tool call\n        if response[\"stopReason\"] == \"tool_use\":\n            tool_use = response[\"output\"][\"message\"][\"content\"][\n                -1\n            ]  # Naive approach assumes only 1 tool is called at a time\n            tool_id = tool_use[\"toolUse\"][\"toolUseId\"]\n            tool_name = tool_use[\"toolUse\"][\"name\"]\n            tool_input = tool_use[\"toolUse\"][\"input\"]\n\n            print(f\"Claude wants to use the {tool_name} tool\")\n            print(f\"Tool Input:\")\n            print(json.dumps(tool_input, indent=2))\n\n            # Run the underlying tool functionality on the fake database\n            tool_result = process_tool_call(tool_name, tool_input)\n\n            print(f\"\\nTool Result:\")\n            print(json.dumps(tool_result, indent=2))\n\n            # Append tool result message\n            messages.append(\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"toolResult\": {\n                                \"toolUseId\": tool_id,\n                                \"content\": [{\"text\": str(tool_result)}],\n                            }\n                        }\n                    ],\n                }\n            )\n\n        else:\n            # If the model does not want to use a tool, just print the text response\n            print(\n                \"\\nTechNova Support:\"\n                + f\"{response['output']['message']['content'][0]['text']}\"\n            )\n```\nThe `simple_chat` function handles user interaction, invokes the LLM, and passes the tool response back to the LLM.\n\nAn important line in this function is `response[\"stopReason\"] == \"tool_use\"`. This determines if the LLM wants to use a tool and, when parsed further, indicates which tool the LLM intends to invoke.\n\nAn example of response object of bedrock\\-runtime `converse` api:\n\n\n```python\n{\n    'ResponseMetadata': {\n        'RequestId': '07f323a7-cc52-4813-9d1b-83e5c3ae932a', \n        'HTTPStatusCode': 200, \n        'HTTPHeaders': {\n            'date': 'Thu, 08 Aug 2024 10:52:59 GMT', \n            'content-type': 'application/json', \n            'content-length': '519', \n            'connection': 'keep-alive', \n            'x-amzn-requestid': '07f323a7-cc52-4813-9d1b-83e5c3ae932a'\n        }, \n        'RetryAttempts': 0\n    }, \n    'output': {\n        'message': {\n            'role': 'assistant', 'content': [\n                {\n                    'text': \"Certainly! I'll search for search for your orders. Let me use our search tool to find that information for you.\"\n                }, {\n                    'toolUse': {\n                        'toolUseId': 'tooluse_8C_XIwrAROC3t3eEu5FCVw', \n                        'name': 'get_customer_orders', \n                        'input': {'customer_id': '1213210'}\n                    }\n                }\n            ]\n        }\n    }, \n    'stopReason': 'tool_use',\n    'usage': {'inputTokens': 672, 'outputTokens': 103, 'totalTokens': 775}, \n    'metrics': {'latencyMs': 2431}\n}\n```\nRefer to the [Amazon Bedrock API Reference](https://proxy.rifx.online/https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html) for more details about the Converse API.\n\nOnce we invoke the required tool or function using our `process_tool_call` function, we pass the function's response back to the LLM to generate a response for the end user.\n\nPlease note that we are using the Converse API of the boto3 Bedrock runtime client. You can also use the Converse Stream API to generate a streaming response. For more details, refer to the Amazon Bedrock API Reference for the Converse Stream API and the Boto3 documentation on the Converse Stream API.\n\n\n### Running in local terminal\n\nOnce you have everything set up correctly, run the Python file from inside your virtual environment using:\n\n\n```python\n## From inside the virtual environment\npython main.py\n```\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Ok9N3mdX50JVWbaJKUrJeQ.gif)\n\n\n## Deploy on EC2\n\nYou can deploy the chatbot on an EC2 instance for demonstration purposes using a [Gradio](https://proxy.rifx.online/https://www.gradio.app/) app, which provides a chatbot\\-like interface with just a few lines of code and integrates seamlessly with our main function.\n\n\n### Gradio\n\n[Gradio](https://proxy.rifx.online/https://www.gradio.app/) is an open\\-source Python library that simplifies the process of building and deploying web\\-based machine learning demos. It allows developers to create intuitive web interfaces for their models with minimal coding, making it easier to deploy and share models with others.\n\nLet’s write a chat function that responds `Yes` or `No` randomly using gradio.\n\nHere’s our chat function (please execute `pip install gradio` in your virtual environment if you don’t have it already installed):\n\n\n```python\nimport random\n\nimport gradio as gr\n\n\ndef random_response(message, history):\n    return random.choice([\"Yes\", \"No\"])\n\ngr.ChatInterface(random_response).launch()\n```\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*XxkUM6yO3lmjN545tRlOvQ.png)\n\nRead more about the [gradio chatbot documentation here](https://proxy.rifx.online/https://www.gradio.app/main/docs/gradio/chatbot).\n\n\n### Running a Gradio App on your Web Server with Nginx\n\nLet’s deploy our chatbot agent on EC2 with Nginx.\n\n**Install Nginx and create new conda env**\n\n1. **Create an EC2 instance** with at least 2–3 GB of memory. You can also deploy it on your Kubernetes or ECS cluster. Make sure to modify the Nginx configuration file to match your setup.\n\n2\\. **SSH into your EC2 instance** and [install Nginx](https://proxy.rifx.online/https://devopsden.io/article/how-to-install-nginx-on-ec2-instance):\n\n\n```python\nsudo yum update -y\nsudo amazon-linux-extras install nginx1.12\nsudo systemctl start nginx\nsudo systemctl enable nginx\nsudo systemctl status nginx\n```\n3\\. [**Install Miniconda**](https://proxy.rifx.online/https://docs.anaconda.com/miniconda/#quick-command-line-install) to manage Python packages:\n\n\n```python\nmkdir -p ~/miniconda3\nwget https://proxy.rifx.online/https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh\nbash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3\nrm -rf ~/miniconda3/miniconda.sh\n\n~/miniconda3/bin/conda init bash\n~/miniconda3/bin/conda init zsh\n```\n4\\. **Create a new Conda environment** with Python 3, and install `boto3` and `gradio`:\n\n\n```python\nconda create --name gradio-demo python=3.12 pip -y\nconda activate gradio-demo\npip install --no-cache-dir gradio boto3\n```\n5\\. **Create a new Python file** for your chatbot and Gradio code. Copy all your code into this file:\n\n\n```python\nvim gradio_demo.py\n```\nAlternatively, you can use `scp` to copy the file directly from your local machine to the remote instance.\n\n**Setup Nginx**\n\nNow we will **set up Nginx** to redirect all traffic from the `/gradio-demo` path to the local server started by the `gradio_demo.py` file. Refer to the [official documentation for running Gradio with Nginx here](https://proxy.rifx.online/https://www.gradio.app/guides/running-gradio-on-your-web-server-with-nginx).\n\n1. Edit the Nginx configuration file located at `/etc/nginx/nginx.conf`:\n\n\n```python\nvim /etc/nginx/nginx.conf\n```\n2\\. In the `http` block, add the following lines to include server block configurations from a separate file:\n\n\n```python\nserver_names_hash_bucket_size  128;\ninclude /etc/nginx/sites-enabled/*;\n```\n3\\. Create a new file in the `/etc/nginx/sites-available` directory (create the directory if it does not already exist), using a filename that represents your app, for example: `sudo vim /etc/nginx/sites-available/my_gradio_app` :\n\n\n```python\nsudo mkdir -p /etc/nginx/sites-enabled\nsudo vim /etc/nginx/sites-available/my_gradio_app\n```\nPaste the following contents in the `my_gradio_app` file:\n\n\n```python\nserver {\n    listen 80;\n    server_name www.ec2-12-34-56-78.us-west-2.compute.amazonaws.com; # Change this to your domain name\n\n    location /gradio-demo/ {  # Change this if you'd like to server your Gradio app on a different path\n        proxy_pass http://127.0.0.1:7860/; # Change this if your Gradio app will be running on a different port\n        proxy_buffering off;\n        proxy_redirect off;\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection \"upgrade\";\n        proxy_set_header Host $host;\n        proxy_set_header X-Forwarded-Host $host;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n}\n```\n4\\. Create a symbolic link to this file in the `/etc/nginx/sites-enabled` directory:\n\n\n```python\nsudo ln -s /etc/nginx/sites-available/my_gradio_app /etc/nginx/sites-enabled/\n```\n5\\. **Update the `gradio_demo.py` file** to set the root path in the Gradio launch API:\n\n\n```python\n.launch(root_path=\"/gradio-demo\")\n```\n6\\. **Check the Nginx configuration** and restart Nginx:\n\n\n```python\nsudo nginx -t\nsudo systemctl restart nginx\n```\nIf you encounter errors with the `nginx -t` command, resolve those errors before proceeding.\n\n**Run the `gradio_demo.py` file** in the background. You can use either `nohup` or `tmux`:\n\n\n```python\n## From inside the Conda environment\nnohup python gradio_demo.py &\n```\n**Access the EC2 DNS URL** and append `/gradio-demo/` to see your chatbot agent on the Gradio interface.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*rcdUROlShsrcaeBDpBKBAQ.png)\n\n\n## Summary\n\nIn this article, we explored how to build a customer support assistant using the [Llama3\\.1](https://proxy.rifx.online/https://llama.meta.com/) or [Claude 3\\.5 Sonnet](https://proxy.rifx.online/https://www.anthropic.com/news/claude-3-5-sonnet) model from Bedrock Tools. We began by defining the problem of handling repetitive customer queries and how LLM agents offer a solution. We then discussed the concept of LLM agents and how they differ from general LLMs. After that, we walked through creating a basic agent in Python and then developed a more complex customer support assistant using the models in Amazon Bedrock. We also covered deploying the assistant on EC2, including an example of using Gradio to create a web interface. By automating routine customer support tasks, businesses can enhance efficiency, reduce costs, and improve customer satisfaction.\n\nIn a production setting, you can pass the logged\\-in user’s name and ID to the system prompt so that the LLM does not have to ask for basic details from a logged\\-in user. Some actions, such as canceling an order, may require additional gatekeeping. Additionally, if a customer is upset or becomes aggressive, the LLM should be instructed to escalate the case to a human assistant.\n\nYou can connect with me on LinkedIn: <https://proxy.rifx.online/https://linkedin.com/in/maheshrajput>\n\nThank you for reading 😊\n\n\n"},{"lang":"en","group":"blog","slug":"blog/build-a-rag-based-scientific-chatbot-with-langchain-streamlit-pubmed-part-4-put-it-all-ba7bbf706bbd","frontmatter":{"title":"Build a RAG-based scientific ChatBot with LangChain, Streamlit & PubMed — Part 4(Put it all…","meta_title":"Build a RAG-based scientific ChatBot with LangChain, Streamlit & PubMed — Part 4(Put it all…","description":"Hello and welcome to the last part of the series to build a scientific ChatBot with Langchain, Streamlit, and PubMed!","date":"2024-11-13T01:22:29.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MQ7XtBd9WHn5n-gAMgd6pQ.jpeg","categories":["Chatbots","Natural Language Processing","Science"],"author":"Rifx.Online","tags":["ChatBot","LangChain","Streamlit","PubMed","RAG"],"draft":false,"slug":"blog/build-a-rag-based-scientific-chatbot-with-langchain-streamlit-pubmed-part-4-put-it-all-ba7bbf706bbd"},"content":"\n\n\n\n\n\nHello and welcome to the last part of the series to build a scientific ChatBot with Langchain, Streamlit, and PubMed!\n\nIn the previous part, we built the data persistence and RAG pipeline with vectorstore. Now, it is time to put everything we’ve built together, and create the chatbot UI that will use the backend functionality we built, and that our scientist will use to answer their scientific questions!\n\nAs a reminder, this is the full solution that we were building during the series:\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*NFCO_uRjlAgm0WYH.png)\n\n\n## App demo\n\n* As a teaser, let’s first have a look at an illustration of what the app will look like!\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*OKEQO_2kwnV93Va4SAVWZg.gif)\n\n\n## Building\n\n\n### Overview of steps already done\n\n* If you haven’t followed through the [first](https://proxy.rifx.online/https://readmedium.com/build-a-rag-based-scientific-chatbot-with-langchain-streamlit-pubmed-part-1-set-up-streamlit-37550b44b266) , the [second](https://proxy.rifx.online/https://readmedium.com/llm-aided-retrieval-of-relevant-scientific-abstracts-via-pubmed-api-using-natural-language-part2-9e10f78575e6), and the [third part](https://proxy.rifx.online/https://readmedium.com/build-a-rag-based-scientific-chatbot-with-langchain-streamlit-pubmed-part-3-create-vector-1e5e401e72e6), please do so, because we will be building on that further. At the end of the last part, we ended up with a project structure looking like this:\n\n\n```python\n.\n├── app\n│   ├── app.py\n│   ├── backend\n│   │  ├── abstract_retrieval\n│   │  │   ├── interface.py\n│   │  │   ├── pubmed_retriever.py\n│   │  │   └── pubmed_query_simplification.py\n│   │  ├── data_repository\n│   │  │   ├── interface.py\n│   │  │   ├── local_data_store.py\n│   │  │   └── models.py\n│   │  └── rag_pipeline\n│   │      ├── interface.py\n│   │      ├── chromadb_rag.py\n│   │      └── embeddings.py\n│   ├── components\n│   │   ├── chat_utils.py\n│   │   ├── llm.py\n│   │   └── prompts.py\n│   └── tests\n│       └── test_chat_utils.py\n├── assets\n│   └── pubmed-screener-logo.jpg\n└── environment\n    └── requirements.txt\n```\nIn this last part of the series, we will focus on the part of code base that defines our Streamlit UI — ***app/app.py*** and the ***app/components*** module.\n\n\n### Modify chat\\_utils.py to include RAG logic\n\n[In the first part](https://proxy.rifx.online/https://readmedium.com/build-a-rag-based-scientific-chatbot-with-langchain-streamlit-pubmed-part-1-set-up-streamlit-37550b44b266), we built a preliminary version of ***chat\\_utils.py*** that contained a simple QA chatbot implementation (without RAG). Now, we will dive in and convert this into a context\\-aware QA chatbot, that will construct answers based on user questions and retrieve relevant context (abstracts) from our vector index via similarity search.\n\nWe will use all the backend functionality [built in part three](https://proxy.rifx.online/https://readmedium.com/build-a-rag-based-scientific-chatbot-with-langchain-streamlit-pubmed-part-3-create-vector-1e5e401e72e6) for this purpose.\n\n**app/components/chat\\_utils.py**\n\n\n```python\nfrom typing import List\nimport streamlit as st\nfrom langchain_core.documents.base import Document\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_core.runnables.base import Runnable\nfrom langchain_core.runnables.utils import Output\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain.vectorstores import VectorStore\n\n\nclass ChatAgent:\n    def __init__(self, prompt: ChatPromptTemplate, llm: Runnable):\n        \"\"\"\n        Initialize the ChatAgent.\n\n        Args:\n        - prompt (ChatPromptTemplate): The chat prompt template.\n        - llm (Runnable): The language model runnable.\n        \"\"\"\n        self.history = StreamlitChatMessageHistory(key=\"chat_history\")\n        self.llm = llm\n        self.prompt = prompt\n        self.chain = self.setup_chain()\n    \n    def reset_history(self) -> None:\n        \"\"\"\n        Clean up chat history to start new chat session.\n        \"\"\"\n        self.history.clear()\n\n    def setup_chain(self) -> RunnableWithMessageHistory:\n        \"\"\"\n        Set up the chain for the ChatAgent.\n\n        Returns:\n        - RunnableWithMessageHistory: The configured chain with message history.\n        \"\"\"\n        chain = self.prompt | self.llm\n        return RunnableWithMessageHistory(\n            chain,\n            lambda session_id: self.history,\n            input_messages_key=\"question\",\n            history_messages_key=\"history\",\n        )\n\n    def display_messages(self, selected_query: str) -> None:\n        \"\"\"\n        Display messages in the chat interface.\n        If no messages are present, adds a default AI message.\n        \"\"\"\n        if len(self.history.messages) == 0:\n            self.history.add_ai_message(f\"Let's chat about your query: {selected_query}\")\n        for msg in self.history.messages:\n            st.chat_message(msg.type).write(msg.content)\n    \n    def format_retreieved_abstracts_for_prompt(self, documents: List[Document]) -> str:\n        \"\"\"\n        Format retrieved documents in a string to be passed to LLM.\n        \"\"\"\n        formatted_strings = []\n        for doc in documents:\n            formatted_str = f\"ABSTRACT TITLE: {doc.metadata['title']}, ABSTRACT CONTENT: {doc.page_content}, ABSTRACT DOI: {doc.metadata['source'] if 'source' in doc.metadata.keys() else 'DOI missing..'}\"\n            formatted_strings.append(formatted_str)\n        return \"; \".join(formatted_strings)\n    \n    def get_answer_from_llm(self, question: str, retrieved_documents: List[Document]) -> Output:\n        \"\"\"\n        Get response from LLM given user question and retrieved documents.\n        \"\"\"\n        config = {\"configurable\": {\"session_id\": \"any\"}}\n        return self.chain.invoke(\n            {\n                \"question\": question, \n                \"retrieved_abstracts\": retrieved_documents,\n            }, config\n        )\n    \n    def retrieve_documents(self, retriever: VectorStore, question: str, cut_off: int = 5) -> List[Document]:\n        \"\"\"\n        Retrieve documents using similarity search \n        cut_off parameter controls how many results are retrieved (default is 5)\n        \"\"\"\n        return retriever.similarity_search(question)[:cut_off]\n\n    def start_conversation(self, retriever: VectorStore, selected_query: str) -> None:\n        \"\"\"\n        Start a conversation in the chat interface.\n        Displays messages, prompts user for input, and handles AI response.\n        \"\"\"\n        self.display_messages(selected_query)\n        user_question = st.chat_input(placeholder=\"Ask me anything..\")\n        if user_question:\n            documents = self.retrieve_documents(retriever, user_question)\n            retrieved_abstracts = self.format_retreieved_abstracts_for_prompt(documents)\n            st.chat_message(\"human\").write(user_question)\n            response = self.get_answer_from_llm(user_question, retrieved_abstracts)\n            st.chat_message(\"ai\").write(response.content)\n```\n**What has changed:**\n\n* We added the method ***retrieve\\_documents*** that takes our vector index (retriever) as argument, and calls a method similarity\\_search on the retriever to get the most similar records to user’s question from the vector index of our scientific abstracts. Note the parameter cut\\_off that specifies the number of results to be retrieved (defaults to 5\\).\n* Added method ***format\\_retreieved\\_abstracts\\_for\\_prompt***, that takes the documents retrieved via retrieve\\_documents method, and formats them for the LLM. This will come very handy when we will ask the LLM in our prompt to cite the relevant sources (article DOIs, and titles).\n* Added method ***get\\_answer\\_from\\_llm*** that serves for calling the LLM with necessary variables, to keep the client function start\\_conversation clean.\n* Modified the ***start\\_conversation*** method to include the RAG logic.\n\n\n### Create chat prompts for QA\n\n* We will be modifying the existing chat prompt to include retrieved abstracts and construct the answer based on those.\n* We will also include an additional (simple) prompt that will serve for a simple immediate answer outside of the chatbot section, so that the user gets a direct answer to his question displayed on the UI.\n\n**app/components/chat\\_prompts.py**\n\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, PromptTemplate\n\n\nchat_prompt_template = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are a knowledgeable expert chatbot in the biomedicine field.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\n            \"human\", \n            \"\"\"\n            Answer the following scientific question: {question}, \n            using the following context retrieved from scientific articles: {retrieved_abstracts}.\n\n            The user might refer to the history of your conversation. Please, use the following history of messages for the context as you see fit.\n\n            The abstracts will come formatted in the following way: ABSTRACT TITLE: <abstract title>; ABSTRACT CONTENT: <abstract content>, ABSTRACT DOI: <abstract doi> (the content inside <> will be variable).\n            In your answer, ALWAYS cite the abstract title and abstract DOI when citing a particular piece of information from that given abstract.\n\n            Your example response might look like this:\n\n            In the article (here in the brackets goes the contents of ABSTRACT_TITLE), it was discussed, that Cannabis hyperemesis syndrome (CHS) is associated with chronic, heavy cannabis use. The endocannabinoid system (ECS) plays a crucial role in the effects of cannabis on end organs and is central to the pathophysiology of CHS. (here, in the end of the cited chunk, the ABSTRACT_DOI goes)\n            \"\"\"\n        ),\n    ]\n)\n\nqa_template = PromptTemplate(\n    input_variables=['question', 'retrieved_abstracts'],\n    template=\"\"\"\n        Answer the following scientific question: {question}, \n        using the following context retrieved from scientific articles: {retrieved_abstracts}.\n\n        The abstracts will come formatted in the following way: ABSTRACT TITLE: <abstract title>; ABSTRACT CONTENT: <abstract content>, ABSTRACT DOI: <abstract doi> (the content inside <> will be variable).\n        In your answer, ALWAYS cite the abstract title and abstract DOI when citing a particular piece of information from that given abstract.\n\n        Your example response might look like this:\n\n        In the article (here in the brackets goes the contents of ABSTRACT_TITLE), it was discussed, that Cannabis hyperemesis syndrome (CHS) is associated with chronic, heavy cannabis use. The endocannabinoid system (ECS) plays a crucial role in the effects of cannabis on end organs and is central to the pathophysiology of CHS. (here, in the end of the cited chunk, the ABSTRACT_DOI goes)\n    \"\"\"\n)\n```\n* Note that the contents of the two prompts is almost identical, but the chat prompt contains a reference for the chat history with the MessagesPlaceholder and an instruction to use the chat history as the LLM sees fit during the conversation.\n\n\n### Create new file app/components/layout\\_extensions.py\n\n* This file will hold a helper function that will render a part of the layout of our app with examples of queries (cues on how to use the app) to the user. I decided to create this extensions file to not clutter our app.py file and keep it clean, since this code is quite lengthy and contains some custom styling (the app info will be displayed to the user on hover):\n\n\n```python\nimport streamlit as st\n\n\ndef render_app_info():\n    st.title(\"PubMed Screener\")\n    st.markdown(\"\"\"\n        PubMed Screener is a ChatGPT & PubMed powered insight generator from biomedical abstracts.\n    \"\"\")\n\n    # Adding custom HTML and CSS for an improved hover-over tooltip\n    st.markdown(\"\"\"\n        <style>\n        .tooltip {\n            position: relative;\n            display: inline-block;\n            border-bottom: 1px dotted black; /* Style for the hoverable text */\n        }\n\n        .tooltip .tooltiptext {\n            visibility: hidden;\n            width: 800px; /* Width to fit content */\n            background-color: #f9f9f9;\n            color: #000;\n            text-align: left;\n            border-radius: 6px;\n            padding: 15px;\n            position: absolute;\n            z-index: 1;\n            bottom: 100;\n            right: -430px; /* Positioning to the right and slightly offset */\n            opacity: 0;\n            transition: opacity 0.5s;\n            box-shadow: 0px 8px 16px 0px rgba(0,0,0,0.8); /* Adding some shadow for better visibility */\n        }\n\n        .tooltip:hover .tooltiptext {\n            visibility: visible;\n            opacity: 1;\n        }\n        </style>\n        <div class=\"tooltip\">🔍 Example Questions\n            <span class=\"tooltiptext\">\n                <strong>Example scientific questions:</strong>\n                <ul>\n                    <li>How can advanced imaging techniques and biomarkers be leveraged for early diagnosis and monitoring of disease progression in neurodegenerative disorders?</li>\n                    <li>What are the potential applications of stem cell technology and regenerative medicine in the treatment of neurodegenerative diseases, and what are the associated challenges?</li>\n                    <li>What are the roles of gut microbiota and the gut-brain axis in the pathogenesis of type 1 and type 2 diabetes, and how can these interactions be modulated for therapeutic benefit?</li>\n                    <li>What are the molecular mechanisms underlying the development of resistance to targeted cancer therapies, and how can these resistance mechanisms be overcome?</li>\n                </ul>\n            </span>\n        </div>\n        \"\"\", unsafe_allow_html=True)\n    \n    st.text(\"\")py\n```\n\n### Modify the app/app.py\n\n* Finally, time to put everything we’ve built together and expose it as a streamlit application!\n\n\n```python\nimport streamlit as st\nfrom metapub import PubMedFetcher\nfrom components.chat_utils import ChatAgent\nfrom components.chat_prompts import chat_prompt_template, qa_template\nfrom components.llm import llm\nfrom components.layout_extensions import render_app_info\nfrom backend.abstract_retrieval.pubmed_retriever import PubMedAbstractRetriever\nfrom backend.data_repository.local_storage import LocalJSONStore\nfrom backend.rag_pipeline.chromadb_rag import ChromaDbRag\nfrom backend.rag_pipeline.embeddings import embeddings\n\n\n## Instantiate objects\npubmed_client = PubMedAbstractRetriever(PubMedFetcher())\ndata_repository = LocalJSONStore(storage_folder_path=\"backend/data\")\nrag_client = ChromaDbRag(persist_directory=\"backend/chromadb_storage\", embeddings=embeddings)\nchat_agent = ChatAgent(prompt=chat_prompt_template, llm=llm)\n\ndef main():\n    st.set_page_config(\n        page_title=\"Pubmed Abstract Screener\",\n        page_icon='💬',\n        layout='wide'\n    )\n\n    # Define columns - this will make layout split horizontally\n    column_logo, column_app_info, column_answer = st.columns([1, 4, 4])\n\n    # Place the logo in the first column\n    with column_logo:\n        st.image('../assets/pubmed-screener-logo.jpg')\n\n    # In the second column, place text explaining the purpose of the app and some example scientific questions that your user might ask.\n    with column_app_info:\n\n        # Runder app info including example questions as cues for the user\n        render_app_info()\n\n        # Section to enter scientific question\n        st.header(\"Enter your scientific question!\")\n        placeholder_text = \"Type your scientific question here...\"\n        scientist_question = st.text_input(\"What is your question?\", placeholder_text)\n        get_articles = st.button('Get articles & Answer')\n\n        # Processing user question, fetching data\n        with st.spinner('Fetching abstracts. This can take a while...'):\n            if get_articles:\n                if scientist_question and scientist_question != placeholder_text:\n\n                    # Get abstracts data\n                    retrieved_abstracts = pubmed_client.get_abstract_data(scientist_question)\n                    if not retrieved_abstracts:\n                        st.write('No abstracts found.')\n                    else:\n                        # Save abstarcts to storage and create vector index\n                        query_id = data_repository.save_dataset(retrieved_abstracts, scientist_question)\n                        documents = data_repository.create_document_list(retrieved_abstracts)\n                        rag_client.create_vector_index_for_user_query(documents, query_id)\n                        \n                        # Answer the user question and display the answer on the UI directly\n                        vector_index = rag_client.get_vector_index_by_user_query(query_id)\n                        retrieved_documents = chat_agent.retrieve_documents(vector_index, scientist_question)\n                        chain = qa_template | llm\n                        \n                        with column_answer:\n                            st.markdown(f\"##### Answer to your question: '{scientist_question}'\")\n                            st.write(chain.invoke({\n                                \"question\": scientist_question, \n                                \"retrieved_abstracts\": retrieved_documents,\n                            }).content)\n\n    # Beginning of the chatbot section\n    # Display list of queries to select one to have a conversation about\n    query_options = data_repository.get_list_of_queries()\n\n    if query_options:\n        st.header(\"Chat with the abstracts\")\n        selected_query = st.selectbox('Select a past query', options=list(query_options.values()), key='selected_query')\n        \n        # Initialize chat about some query from the history of user questions\n        if selected_query:\n            selected_query_id = next(key for key, val in query_options.items() if val == selected_query)\n            vector_index = rag_client.get_vector_index_by_user_query(selected_query_id)\n\n            # Clear chat history when switching query to chat about\n            if 'prev_selected_query' in st.session_state and st.session_state.prev_selected_query != selected_query:\n                chat_agent.reset_history()\n\n            st.session_state.prev_selected_query = selected_query\n\n            # Start chat session\n            chat_agent.start_conversation(vector_index, selected_query)\n\n\nif __name__ == \"__main__\":\n    main()\n```\n* The code contains the following parts:\n1. Instantiate all objects that we built in previous parts of the series → ***PubMedAbstractRetriever***, ***LocalJSONStore***, ***ChromaDbRag***, and ***ChatAgent***. We will be using all those objects in our app code.\n2. Define layout to render app title, logo, and app info.\n3. Define input for the user’s question, and a button to submit it. When the button is clicked, this triggers the logic to search \\& fetch PubMed articles (using ***PubMedAbstractRetriever —*** pubmed\\_client), save them to the local data repository (with ***LocalJSONStore —***data\\_repository), and create a vector index for them (with ***ChromaDbRag —*** rag\\_client).\n4. Answer the user question directly and show it on the UI.\n5. Display ChatBot section that will let you select a past query to chat about, in case you want to interrogate the abstracts further. After the selection of a past query, the corresponding vector index is loaded, and a chat session is initiated (***chat\\_agent.start\\_conversation(…)***). Now you can chat with your abstracts!\n\n\n## Limitations\n\nI am happy you went with me through this series where we built a prototype of a scientific chatbot! It is necessary to say though, that this application is a PoC scope ONLY, and the implementation presented has its caveats that would need to be addressed before deploying in a production manner.\n\n**Naive RAG limitations and considerations**\n\n* **Retrieved content relevance**: you can’t be sure if the retrieved content (the content most similar to user’s question) is the most relevant piece of information. There are some techniques of advanced RAG, like *H**ypothetical Questions*****,** or ***Hierarchical indexing*** that can help with that — read more about those techniques and more [in this article](https://proxy.rifx.online/https://readmedium.com/advanced-rag-techniques-unlocking-the-next-level-040c205b95bc)\n* **Retrieved content cut off**: It is hard to assess whether all the relevant information was retrieved. Also, it can be challenging to fit all the context to the prompt due to the token limits of LLMs. The default cut off in our case equals to 5 abstracts (in our ChatAgent retrieve\\_documents method), which can certainly not be enough if the user asks a broad question.\n* **Limited applicability**: Sometimes, the user question can be rather of a summarization character, and a different technique than RAG would be more appropriate for this purpose. For example, you could build an agent that decides whether task is summarization / retrieval based on user question. After this evaluation, there would be a function executing different logic that performs summarization or retrieval, respectively.\n\n**Deployment architecture considerations**\n\n* **Runtime environment**: For the scope of this series, we only built our chatbot locally, not considering any architectural decisions we would need to make if we would like to deploy this app to serve some real users.\n* **Synchronous processing**: Since the data fetching can take a considerable amount of time, it would be more efficient to implement a queue\\-based asynchronous processing of user requests, and notify user when data fetching is done. Doing this in a synchronous manner can take a lot of time which can result in timeouts in many servers.\n* **Backend technologies**: The backend used in our case was ChromaDB with local storage using JSON files. For a deployed application serving users, this should be re\\-evaluated and appropriate technology selected. This can be easily achieved by building on the interface definitions in the app backend code (***RagWorkflow*** and ***UserQueryDataStore*** interfaces).\n\n**Including more scientific databases**\n\n* In the series, we focused on PubMed only, but to provide a rich context base, another scientific paper databases (i.e. Scopus) could be added. This can be easily achieved by building on the interface definitions in the app backend code (***AbstractRetriever*** interface).\n\n\n## Full codebase GitHub link\n\nFeel free to fork the repo and adapt it to your UC!\n\n\n### Link to GitHub repo pubmed\\-rag\\-screener\n\n\n## Summary\n\n* In this last part of the series to build a scientific ChatBot, we put together all the previously built pieces to create a user interface, where our scientist can formulate her/his question, get an answer based on scientific abstracts, and then chat with the abstracts for further insights.\n* The application logic is modular and enables easy extension using provided interfaces.\n* The limitations of the approach were outlined and highlighted, and some suggestions to build a production\\-grade application were included.\n\n\n> Thanks so much for going through this series with me! I hope you enjoyed building this exciting use case :)\n\n\n> Do not hesitate to get in touch with me if you want to discuss anything about dev, data, AI, or just connect — [reach out on LinkedIn](https://proxy.rifx.online/https://www.linkedin.com/in/sbarankova/)\n\n\n## Contents of the series\n\n* [Part 1 — Explaining the use case, first steps to set up the Streamlit app with chatbot interface.](https://proxy.rifx.online/https://readmedium.com/build-a-rag-based-scientific-chatbot-with-langchain-streamlit-pubmed-part-1-set-up-streamlit-37550b44b266)\n* [Part 2 — LLM\\-aided retrieval of relevant scientific abstracts via PubMed API using natural language](https://proxy.rifx.online/https://readmedium.com/llm-aided-retrieval-of-relevant-scientific-abstracts-via-pubmed-api-using-natural-language-part2-9e10f78575e6)\n* [Part 3 — Setting up the backend — Create vector embeddings from the retrieved scientific abstracts and store them in a vector store](https://proxy.rifx.online/https://readmedium.com/build-a-rag-based-scientific-chatbot-with-langchain-streamlit-pubmed-part-3-create-vector-1e5e401e72e6)\n* **Part 4 (this article) — Put it all together via RAG — chat with scientific abstracts**\n\n"},{"lang":"en","group":"blog","slug":"blog/building-a-local-ai-powered-news-aggregator-with-ollama-swarm-and-duckduckgo-95aaf8b3ee41","frontmatter":{"title":"Building a Local AI-Powered News Aggregator with Ollama, Swarm, and DuckDuckGo","meta_title":"Building a Local AI-Powered News Aggregator with Ollama, Swarm, and DuckDuckGo","description":"No subtitle provided","date":"2024-10-24T17:47:43.000Z","image":"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*OHMOTk_WYGOxWHBsKqdpNQ.jpeg","categories":["Programming","Generative AI","Technology/Web"],"author":"Rifx.Online","tags":["Llama","Swarm","DuckDuckGo","News","Aggregator"],"draft":false,"slug":"blog/building-a-local-ai-powered-news-aggregator-with-ollama-swarm-and-duckduckgo-95aaf8b3ee41"},"content":"\n\n# Building a Local AI-Powered News Aggregator with Ollama, Swarm, and DuckDuckGo\n\n\n\nIn today’s fast-paced world, staying up-to-date with the latest news in specific fields can be challenging. What if we could leverage the power of Generative AI and Agents to create a personalized news aggregator that runs entirely on our local machine? In this article, we’ll explore how to build such a system using **Ollama**’s Llama 3.2 model, **Swarm** for agent orchestration, and **DuckDuckGo** for web searches.\n\n\n# The Power of Local AI\n\nWith the rise of large language models, we now have the ability to run sophisticated AI systems on our personal computers. This opens up a world of possibilities for creating customized tools tailored to our specific needs. Our news aggregator is a perfect example of this potential.\n\n\n# Components of Our System\n\n1. **Ollama with Llama 3.2**: This serves as the brain of our system, powering our AI agents.\n2. **Swarm**: An agent orchestration framework that allows us to create and manage multiple AI agents.\n3. **DuckDuckGo Search**: Provides up-to-date web search results without tracking user data.\n\n\n# How It Works\n\nOur news aggregator consists of two main AI agents:\n\n1. **News Assistant**: Fetches the latest news articles on a given topic using DuckDuckGo search.\n2. **Editor Assistant**: Reviews and refines the collected news for final presentation.\n\nLet’s break down the workflow:\n\n\n# 1. Setting Up the Environment\n\n\n```python\nollama pull llama3.2\n\nexport OPENAI_MODEL_NAME=llama3.2\nexport OPENAI_BASE_URL=http://localhost:11434/v1\nexport OPENAI_API_KEY=any\n\npip install git+https://github.com/openai/swarm.git duckduckgo-search\n```\nWe start by importing the necessary libraries and initializing our Swarm client:\n\n\n```python\nfrom duckduckgo_search import DDGS\nfrom swarm import Swarm, Agent\nfrom datetime import datetime\n\ncurrent_date = datetime.now().strftime(\"%Y-%m\")\nclient = Swarm()\n```\n\n# 2. Creating the News Search Function\n\nWe define a function to search for news using DuckDuckGo:\n\n\n```python\npythondef get_news_articles(topic):\n  ddg_api = DDGS()\n  results = ddg_api.text(f\"{topic} {current_date}\", max_results=5)\n  if results:\n      news_results = \"\\n\\n\".join([f\"Title: {result['title']}\\nURL: {result['href']}\\nDescription: {result['body']}\" for result in results])\n      return news_results\n  else:\n      return f\"Could not find news results for {topic}.\"\n```\n\n# 3. Defining Our AI Agents\n\nWe create two agents using Ollama’s Llama 3.2 model:\n\n\n```python\nnews_agent = Agent(\n  model=\"llama3.2\",\n  name=\"News Assistant\",\n  instructions=\"You provide the latest news articles for a given topic using DuckDuckGo search.\",\n  functions=[get_news_articles],\n)\n\neditor_agent = Agent(\n  model=\"llama3.2\",\n  name=\"Editor Assistant\",\n  instructions=\"You review and finalise the news article for publishing.\",\n)\n```\n\n# 4. Orchestrating the Workflow\n\nWe define a function to run our news aggregation workflow:\n\n\n```python\ndef run_news_workflow(topic):\n  # Fetch news\n  news_response = client.run(\n      agent=news_agent,\n      messages=[{\"role\": \"user\", \"content\": f\"Get me the news about {topic} on {current_date}\"}],\n  )\n  raw_news = news_response.messages[-1][\"content\"]\n  \n  # Pass news to editor for final review\n  edited_news_response = client.run(\n      agent=editor_agent,\n      messages=[{\"role\": \"system\", \"content\": raw_news}],\n  )\n  print(f\"{edited_news_response.messages[-1]['content']}\")\n```\n\n# 5. Running the System\n\nFinally, we can run our news aggregator for any topic of interest:\n\n\n```python\nrun_news_workflow(\"AI in Drug Discovery\")\n```\n\n# Complete Code : app.py\n\n\n```python\nfrom duckduckgo_search import DDGS\nfrom swarm import Swarm, Agent\nfrom datetime import datetime\n\ncurrent_date = datetime.now().strftime(\"%Y-%m\")\n\n# Initialize Swarm client\nclient = Swarm()\n\n# 1. Create Internet Search Tool\n\ndef get_news_articles(topic):\n    print(f\"Running DuckDuckGo news search for {topic}...\")\n    \n    # DuckDuckGo search\n    ddg_api = DDGS()\n    results = ddg_api.text(f\"{topic} {current_date}\", max_results=5)\n    if results:\n        news_results = \"\\n\\n\".join([f\"Title: {result['title']}\\nURL: {result['href']}\\nDescription: {result['body']}\" for result in results])\n        return news_results\n    else:\n        return f\"Could not find news results for {topic}.\"\n    \n# 2. Create AI Agents\n\ndef transfer_to_editor_assistant(raw_news):\n    print(\"Passing articles to Editor Assistant...\")\n    return editor_agent.run({\"role\": \"system\", \"content\": raw_news})\n\n# News Agent to fetch news\nnews_agent = Agent(\n    model=\"llama3.2\",\n    name=\"News Assistant\",\n    instructions=\"You provide the latest news articles for a given topic using DuckDuckGo search.\",\n    functions=[get_news_articles],\n)\n\n# Editor Agent to edit news\neditor_agent = Agent(\n    model=\"llama3.2\",\n    name=\"Editor Assistant\",\n    instructions=\"You review and finalise the news article for publishing.\",\n)\n\n# 3. Create workflow\n\ndef run_news_workflow(topic):\n    print(\"Running news Agent workflow...\")\n    \n    # Step 1: Fetch news\n    news_response = client.run(\n        agent=news_agent,\n        messages=[{\"role\": \"user\", \"content\": f\"Get me the news about {topic} on {current_date}\"}],\n    )\n    raw_news = news_response.messages[-1][\"content\"]\n    print(f\"Fetched news: {raw_news}\")\n    \n    # Step 2: Pass news to editor for final review\n    edited_news_response = client.run(\n        agent=editor_agent,\n        messages=[{\"role\": \"system\", \"content\": raw_news}],\n    )\n    print(f\"{edited_news_response.messages[-1]['content']}\")\n\n\n# Example of running the news workflow for a given topic\nrun_news_workflow(\"AI in Drug Discovery\")\n```\n\n# Sample Output\n\n\n```python\nRunning news Agent workflow...\nRunning DuckDuckGo news search for AI in Drug Discovery...\nFetched news: Here's a formatted answer based on the news articles:\n\n**AI in Drug Discovery: A Revolutionary Shift**\n\nThe role of Artificial Intelligence (AI) in drug discovery has marked a revolutionary shift in the pharmaceutical landscape. AI leverages sophisticated algorithms for autonomous decision-making from data analysis, augmenting human capabilities rather than replacing them.\n\n**Challenges and Limitations**\n\nDespite the promising advancements, challenges and limitations have been identified in the field. The paper \"The Role of AI in Drug Discovery\" addresses these issues, highlighting the need for high-quality data, addressing ethical concerns, and recognizing the limitations of AI-based approaches.\n\n**Applications of AI in Drug Discovery**\n\nAI has the potential to play a critical role in drug discovery, design, and studying drug-drug interactions.Applications of AI in drug discovery include:\n\n* Polypharmacology: AI can predict the likelihood of a compound's effectiveness against multiple diseases.\n* Chemical synthesis: AI can optimize chemical synthesis processes for faster and more efficient production.\n* Drug repurposing: AI can identify new uses for existing drugs.\n* Predicting drug properties: AI can predict the efficacy, toxicity, and physicochemical characteristics of compounds.\n\n**The Future of AI in Drug Discovery**\n\nAs AI continues to evolve, it is expected to significantly impact the pharmaceutical industry. The successful application of AI will depend on the availability of high-quality data, addressing ethical concerns, and recognizing the limitations of AI-based approaches.\n```\n\n# The Benefits of Local AI News Aggregation\n\n* **Privacy**: All processing happens on your local machine, ensuring your data stays with you.\n* **Customization**: You can easily modify the agents’ instructions or add new agents to suit your specific needs.\n* **Up-to-date Information**: By using DuckDuckGo search, you always get the latest news on your chosen topic.\n* **AI-powered Curation**: The Editor Assistant helps refine and organize the collected news, providing a more polished final output.\n\n\n# Conclusion\n\nThis local AI-powered news aggregator demonstrates the potential of combining large language models with web search capabilities. By leveraging Ollama’s Llama 3.2 model, Swarm for agent orchestration, and DuckDuckGo for search, we’ve created a powerful tool that can keep us informed on any topic of interest, all while maintaining our privacy and running entirely on our local machine.\n\nAs AI continues to evolve, the possibilities for creating personalized, AI-driven tools will only expand. This news aggregator is just the beginning — imagine what other innovative applications you could build using these technologies!\n\n\n# Reference :\n\nSwarm Github : <https://github.com/openai/swarm>\n\nIf you found this article informative and valuable, I’d greatly appreciate your support:\n\n* Give it a few claps 👏 on Medium to help others discover this content (did you know you can clap up to 50 times?). Your claps will help spread the knowledge to more readers.\n- Share it with your network of AI enthusiasts and professionals.\n- Connect with me on LinkedIn: <https://www.linkedin.com/in/manjunath-janardhan-54a5537/>\n\n\n\n\n\n"},{"lang":"en","group":"blog","slug":"blog/building-autonomous-multi-agent-systems-with-crewai-1a3b3a348271","frontmatter":{"title":"Building Autonomous Multi-Agent Systems with CrewAI","meta_title":"Building Autonomous Multi-Agent Systems with CrewAI","description":"The article discusses the development of autonomous multi-agent systems using CrewAI and LangChain frameworks. It explains the structure of multi-agent systems, emphasizing the roles of agents, tools, and tasks in achieving complex operations. The article details a project that integrates these frameworks to create an essay-writing application, demonstrating how agents can collaborate to research, write, and edit essays. It also outlines the project’s architecture, including agent design, task management, and deployment using Streamlit, providing insights into building efficient AI systems.","date":"2024-11-14T03:29:09.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*72Cy_QqOie7G2NAiWr13Kw.jpeg","categories":["Autonomous Systems","Programming","Data Science"],"author":"Rifx.Online","tags":["CrewAI","LangChain","multi-agent","Streamlit","essay-writing"],"draft":false,"slug":"blog/building-autonomous-multi-agent-systems-with-crewai-1a3b3a348271"},"content":"\n### What‘s Multi\\-Agent Autonomous System and How to build one with CrewAI and LangChain?\n\n\n\n## Motivation\n\nIn fact, we are not unfamiliar with these concepts; we know them from movies. A person commands their AI, and the AI carries out these commands by using various tools. This is the path we are on today with the rise of AI systems. The era is gradually changing. In the past, people couldn’t undertake a task alone and would need a team. Without a team, they would either run out of energy after a while or hit the limits of their abilities. In the end, successful projects come from teams made up of individuals with different skills.\n\n> Teamwork makes the dream work.\n\nHowever, these days a new technology has started to make a name for itself. We can call it the next phase of AI before AGI: ‘Agents.’ So, what are these agents? Before diving into the code, let’s talk a bit about the structure of multi\\-agent systems\n\n## How does it work?\n\nTo put the equation simply: `Multi Agent Systems = AGENTs + TOOLs + TASKs` It’s a system where multiple agents are equipped with various tasks and tools.\n\n### Agent\n\nWe are familiar with role\\-playing games, where your character has a role, like a warrior, for example. Throughout the game, you put yourself in their place, aiming to complete the game by finishing the quests that shape their backstory from one adventure to the next. Similarly, researchers have discovered that large language models (LLMs) can be motivated to perform tasks optimally when given roles, backstories, and objectives. This allows us to motivate LLMs to carry out various tasks with just a few simple prompts.\n\nAgents essentially break down the assigned tasks into simple steps and then execute those steps by ‘thinking’ — yes, thinking — through them in sequence. This enables us to create an agent that not only performs steps thoughtfully but also consults other agents with different areas of expertise, rather than relying on a single LLM to input prompts and receive outputs.\n\n### Tools\n\nOne of humanity’s greatest abilities is undoubtedly our skill in using tools. This ability has evolved and developed through both evolutionary and cultural processes, allowing us to create the advanced technology we use today. Similarly, large language models have increased their capabilities as they are trained on larger datasets. Now, when the function of a tool and how it is used are clearly explained, these models can autonomously use the tool under appropriate conditions, executing it fully automatically and planning their next steps based on the output, without waiting for further commands.\n\nTherefore, tool use can be considered one of the most important parts of their evolution as well. Especially with the internet browsing tool, agents can follow the specified function’s steps to access the necessary resources, whether through web scraping or by using the search engine of the designated site.\n\nYour tools’ functionality and purpose are entirely up to your imagination. However, if you’d like to integrate pre\\-built tools into your agents, both CrewAI and LangChain libraries offer a wide range of built\\-in tools ready for use. In this project, we will focus on creating our own custom tool instead.\n\n### Task\n\nJust as we create agents, we also create tasks, and each task requires various tools. To give an example from human behavior, what do we do when we need to research something?\n\n1\\- We search the internet.\n\n2\\- We conduct in\\-depth source research.\n\n3\\- We take notes on our findings.\n\nIn the same way, we can design tasks to follow these steps, and we will touch on how they are designed through the code.\n\n## What is CrewAI?\n\nCrewAI is an open\\-source Python framework for orchestrating role\\-playing, autonomous AI agents with methods like Crew, Task, Agent, Process, and it supports various LLMs, including local models.\n\nIf we look at the main advantages provided by the framework:\n\n* Role\\-based agent design.\n* Autonomous inter\\-agent delegation.\n* Flexible task management.\n* Process\\-driven execution.\n* Output saving as files like .markdown files.\n* Compatibility with both open\\-source and proprietary models like OpenAI.\n\n## Building Multi\\-Agent\n\nA descriptive explanation alone may not always be enough to fully understand a concept, so let’s create a small Essay Writer project to better grasp the multi\\-agent approach. In this project, we will combine the LangChain and CrewAI frameworks. To run the project, you will need an OpenAI API key, which you can obtain by visiting [https://proxy.rifx.online/https://platform.openai.com/signup](https://proxy.rifx.online/https://platform.openai.com/signup).\n\nThe structure of our project consists of several different python scripts:\n\n* `crew.py`, where we define our agents and their tasks.\n* `graph.py`, which builds the LangGraph structure.\n* `extra_tools.py`, containing the tools our agents will use.\n* `pdf_writer.py`, responsible for converting the essay into a PDF.\n* `app.py`, which provides the Streamlit interface for our application.\n\n```python\n## Project Structure\nAutonomous-Multi-Agent-Systems-with-CrewAI-Essay-Writer\n├── app.py              # Main streamlit application\n├── crew.py             # CrewAI agents and task handling\n├── extra_tools.py      # Agentic functions of tools\n├── graph.py            # LangGraph and Project Workflow\n├── pdf_writer.py       # Handles PDF output generation\n├── requirements.txt    # List of required libraries\n├── media\n│   └── cover.jpg       # Project cover image\n└── README.md        \n```\n\nThe libraries required for this project are listed in the `requirements.txt` file. Additionally, please ensure you have installed Python version 3\\.12 or higher. Don’t forget to install the dependencies before running the project. The libraries we use include:\n\n```python\nlangchain-core\nlangchain-openai\nlanggraph\nstreamlit\nwikipedia\nreportlab\ncrewai[tools]\npysqlite3-binary\nbs4\n```\n\n### Workflow\n\nIn our process, we will assign various roles to the agents. For example, while one agent waits for another to complete their task of researching on the internet, another agent will independently carry out research on Wikipedia. Once both agents have completed their tasks, the agent waiting for their information will then proceed with writing the essay, which is their assigned task.\n\nIf we were to visualize this:\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Emb37H_8OAKp1s1ChLLVQg.png)\n\n* The User Query is initially sent to the Router.\n* The Router reads the query and determines whether the user wants to write a new essay, edit a previous one, or simply convey a topic for discussion. If the user wishes to write a new essay, the request is forwarded to Crew.\n* The incoming request to Crew is first sent to the Researcher Agent.\n* The researcher agent uses the tools assigned to him to search for internet resources related to the subject on which the user wants to write an essay.\n* Once the resource collection process is completed, the collected information is forwarded to the Writer Agent.\n* When the writer agent drafts the essay, the Editor Agent makes final adjustments, corrects grammatical errors, and returns the draft as a JSON file to LangGraph.\n* The JSON file is sent to the function that will create our essay as a PDF file in the final node.\n\n### Building LangGraph\n\nFirst, we need to establish the skeleton of our schema. Once we create a workflow that allows us to reach our agents as needed, all that’s left is to decide at which stages of the workflow we will send a request to our agents. To do this, we will first create a simple workflow using LangChain.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*iHQzJymxAstrW40THoRxwA.png)\n\n```python\n#LangGraph workflow\n\nbuilder = StateGraph(GraphState)\n\nbuilder.add_node(\"answer\", self.answer)\nbuilder.add_node(\"write_essay\", self.write_essay)\nbuilder.add_node(\"edit_essay\", self.edit_essay)\n\n\nbuilder.set_conditional_entry_point(self.router_query,\n                              {\"write_essay\": \"write_essay\",\n                                        \"answer\": \"answer\",\n                                        \"edit_essay\": \"edit_essay\"})\nbuilder.add_edge(\"write_essay\", END)\nbuilder.add_edge(\"edit_essay\", END)\nbuilder.add_edge(\"answer\", END)\n\nself.graph = builder.compile()\n```\n\n**Router Node**: As we mentioned in the workflow description, our router assigns tasks to various nodes based on incoming requests. To achieve this, we need to create an effective prompt that encompasses the topic provided by the user and incorporates past conversations. After all, we are developing a multi\\-agent essay writer chatbot that can remember and recall previous discussions.\n\nLet’s draft a simple prompt and the corresponding node that will utilize this prompt. In the prompt, we should define a `BaseModel` using the Pydantic library to enforce that our router selects one of three potential response strategies. These strategies will guide the chatbot in formulating its responses effectively.\n\nIn the node, we will implement this prompt using Langchain’s `PromptTemplate` method. Then, we will invoke the LLM (Large Language Model) with both the user query and the conversation history to ensure that the response is contextually relevant and tailored to the user’s needs.\n\n1. **Define the Pydantic Model**: Create a model that specifies the required response strategy.\n2. **Construct the Prompt**: Write a prompt that clearly outlines the three strategies.\n3. **Set Up the Node**: Use Langchain `PromptTemplate` to format the prompt dynamically.\n4. **Invoke the LLM**: Call the LLM with the formatted prompt, user query, and conversation history.\n\nBy following these steps, we can ensure that the chatbot responds accurately and maintains the context from previous interactions.\n\n```python\n#Router Prompt and Router Node\nclass RouteQuery(BaseModel):\n    \"\"\"Route a user query to direct answer or research.\"\"\"\n\n    way: Literal[\"edit_essay\",\"write_essay\", \"answer\"] = Field(\n        ...,\n        description=\"Given a user question choose to route it to write_essay, \n        edit_essay or answer\",\n    )\n\nself.router_prompt = \n    \"\"\"\n    You are a router and your duty is to route the user to the correct expert.\n    Always check conversation history and consider your move based on it.\n    If topic is something about memory, or daily talk route \n      the user to the answer expert.\n    If topic starts something like can u write, or user request \n      you write an article or essay, route the user to the write_essay expert.\n    If topic is user wants to edit anything in the essay, \n      route the user to the edit_essay expert.\n  \n    \\nConservation History: {memory}\n    \\nTopic: {topic}\n    \"\"\"\n\ndef router_query(self, state: GraphState):\n    print(\"**ROUTER**\")\n    prompt = PromptTemplate.from_template(self.router_prompt)\n    memory = self.memory.load_memory_variables({})\n\n    router_query = self.model.with_structured_output(RouteQuery)\n    chain = prompt | router_query\n    result:  RouteQuery = chain.invoke({\"topic\": state[\"topic\"],\n                                       \"memory\": memory})\n\n    print(\"Router Result: \", result.way)\n    return result.way\n```\n\n**Simple Answer Node**: After placing our router as a node in the start section, the next step is to create our other three nodes: `write_essay`, `edit_essay`, and `answer`. To take a straightforward approach, we need to program our `answer` node to generate responses directly using its memory when a user sends a casual message or engages in a conversation about an essay.\n\nTo achieve this, we must first write a suitable prompt for this task. Then, using this prompt, we will design a simple node. Let’s proceed with this design.\n\n```python\n#Simple Answer Prompt and Node\n\nself.simple_answer_prompt = \n      \"\"\"\n      You are an expert and you are providing a simple \n      answer to the user's question.\n    \n      \\nConversation History: {memory}\n      \\nTopic: {topic}\n      \"\"\"\ndef answer(self, state: GraphState):\n    print(\"**ANSWER**\")\n    prompt = PromptTemplate.from_template(self.simple_answer_prompt)\n    memory = self.memory.load_memory_variables({})\n    chain = prompt | self.model | StrOutputParser()\n    result = chain.invoke({\"topic\": state[\"topic\"], \"memory\": memory})\n\n    self.memory.save_context(inputs={\"input\": state[\"topic\"]}, outputs={\"output\": result})\n    return {\"response\": result}\n```\n\n**Writing Essay Node**: Next, we need to design the `writing_essay` node. The purpose of this node is to forward the query received from the user to our agents using CrewAI’s `kickoff` method and then to convert the JSON file returned by the agents into a PDF. Naturally, we do not need to write a prompt for this node, as the prompts will be defined during the agent creation phase. This node will be created solely for the purpose of invoking the agents and utilizing the returned values.\n\n1. **Invoke Agents**: Use CrewAI’s `kickoff` method to send the user's query to the agents.\n2. **Process the Returned JSON**: Handle the JSON response received from the agents.\n3. **Convert to PDF**: Convert the relevant data from the JSON into a PDF format.\n\n```python\n#Write Essay Node\ndef write_essay(self, state: GraphState):\n    print(\"**ESSAY COMPLETION**\")\n\n    self.essay = self.crew.kickoff({\"topic\": state[\"topic\"]})\n\n    self.memory.save_context(inputs={\"input\": state[\"topic\"]},\n                           outputs={\"output\": str(self.essay)})\n\n    pdf_name = generate_pdf(self.essay)\n    return {\"response\": \"Here is your essay! \",  \"pdf_name\": f\"{pdf_name}\"}\n```\n\n**Edit Essay Node**: Let’s briefly discuss our final node, `edit_essay`. The code may appear a bit lengthy, as the prompt is kept within the node. You can also write the prompt during the class definition and assign it as a variable if you prefer.\n\nThis node will be activated by the router when it detects a request for any essay modifications from the user. In this node, we need three important values: the conversation history, the user’s request, and the most recently generated essay. Additionally, there is a variable in the prompt that Langchain will generate called `format_instructions`. This variable allows us to communicate to the LLM that we want to maintain the structure of the JSON format of the edited essay and to receive the response in the same format. Afterward, we will send the returned response to our PDF generation tool.\n\n1. **Detect Edit Request**: The router identifies whether the user’s request is for editing an essay.\n2. **Collect Necessary Values**: Gather the conversation history, user request, and the last generated essay.\n3. **Create and Use the Prompt**: Construct a prompt that includes `format_instructions`.\n4. **Generate the Edited Essay**: Invoke the LLM to get the edited essay and pass the response to the PDF generator.\n\n```python\n#Edit Essay Node\n\ndef edit_essay(self, state: GraphState):\n    print(\"**ESSAY EDIT**\")\n    memory = self.memory.load_memory_variables({})\n\n    user_request = state[\"topic\"]\n    parser = JsonOutputParser(pydantic_object=Essay)\n    prompt = PromptTemplate(\n      template=(\"Edit the Json file as user requested, \n                  and return the new Json file.\"\n                \"\\n Request:{user_request} \"\n                \"\\n Conservation History: {memory}\"\n                \"\\n Json File: {essay}\"\n                \" \\n{format_instructions}\"),\n      input_variables=[\"memory\",\"user_request\",\"essay\"],\n      partial_variables={\"format_instructions\": parser.get_format_instructions()},\n  )\n\n    chain = prompt | self.model | parser\n\n    self.essay = chain.invoke({\"user_request\": user_request,\n                               \"memory\": memory, \n                                \"essay\": self.essay})\n\n\n    self.memory.save_context(inputs={\"input\": state[\"topic\"]},\n                             outputs={\"output\": str(self.essay)})\n    pdf_name = generate_pdf(self.essay)\n    return {\"response\": \"Here is your edited essay! \", \n            \"essay\": self.essay, \"pdf_name\": f\"{pdf_name}\"}\n```\n\n## Building Agents\n\n**Content Researcher**: To keep our project simple, we have defined three agents that will communicate with each other and conduct internet searches to write essays. Let’s design the first agent, the researcher agent. This agent will perform web scraping on Wikipedia and other websites as needed, gathering necessary sources until it determines it has collected enough information. It will fetch main headings, subheadings, and articles related to the topic, preparing summaries. Subsequently, these documents will be stored to be sent to the writer's agent.\n\nWhen designing this agent, we need to consider its role, backstory, and goal. We will assign these to the parameters in the `Agent` class similarly to how we would construct a prompt, thereby readying the agent for operation.\n\n```python\n#Content Researcher Agent and Task\n\nself.researcher = Agent(\n    role=\"Content Researcher\",\n\n    goal=\"Research accurate content on {topic}\",\n\n    backstory=\"You're researching content to write \n                an essay about the topic: {topic}.\"\n              \"You collect information that helps \n                the audience learn something and make informed decisions.\"\n              \"Your work is the basis for the Content Writer to \n                write an article on this topic.\",\n    verbose=True\n)\n\nself.research = Task(\n    description=(\n        \"1. Prioritize the latest trends, key players, \n            and noteworthy news on {topic}.\\n\"\n        \"2. Identify the target audience, considering their \n            interests and pain points.\\n\"\n        \"3. Research a detailed content outline including \n            an introduction, key points, and a conclusion.\\n\"\n        \"4. Include SEO keywords and relevant data or sources.\"\n    ),\n    expected_output=\"A comprehensive document with an outline, \n                    audience analysis, SEO keywords, and resources.\",\n    tools=[search_wikipedia, scrap_webpage],\n    agent=self.researcher,\n)\n```\n\nWe need to create two classes: `Agent` and `Task`. Each agent can have one or more assigned tasks. We can assign tools directly to the agents or add task\\-specific tools. By adding the tool specifically for the task, we ensure that the tool is only used during that particular task.\n\n### Parameters\n\nThe parameters of our `Agent` class:\n\n1. **Role**: This defines the agent’s function within the crew. It determines the kind of tasks the agent is best suited for and should be short and descriptive.\n2. **Goal**: This is the individual objective that the agent aims to achieve. It guides the agent’s decision\\-making process and should be short and simple.\n3. **Backstory**: This provides context for the agent’s role and goal, enriching the interaction and collaboration dynamics. It should be as detailed as possible.\n4. **Verbose**: Setting this to `True` configures the internal logger to provide detailed execution logs, aiding in debugging and monitoring what our agent is engaged in.\n\nThe parameters of our `Task` class:\n\n1. **Description**: A clear and concise statement of what the task entails. This should be as detailed as possible to ensure clarity.\n2. **Expected Output**: A detailed description of what the task’s completion looks like, helping to set clear expectations for the outcome.\n3. **Tools**: The functions or capabilities the agent can utilize to perform the task. Here, you can use LangChain, CrewAI, or custom tools as needed.\n4. **Agent**: The agent responsible for the task, assigned either directly or through the crew’s process.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ocQ9ZUZwFtGx7a7pPrTbuQ.png)\n\n**Content Writer**: Once our researcher agent has gathered the necessary information through several iterations, it will store the collected data in memory, believing it has acquired sufficient knowledge, and pass the task to our next agent, the content writer.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*HD1Bm7twxsUIVGPiqEhHAg.png)\n\nNow, let’s define our content writer agent and its role. This agent does not require the use of any tools, so it will suffice to detail the backstory and description similarly to the researcher agent. In the backstory, we must remember to specify which agent provided the source of information.\n\n### Parameters\n\n1. **Role**: The function of the content writer agent within the project. This should succinctly capture what the agent does.\n2. **Goal**: The specific objective that the content writer aims to achieve, such as drafting a well\\-structured essay based on the information collected.\n3. **Backstory**: This provides context for the content writer’s role, including details about the researcher agent and the information it has provided. A well\\-crafted backstory can enhance the narrative and collaboration dynamics.\n4. **Description**: A clear and concise statement of what the content writer does, focusing on its responsibilities and tasks.\n5. **Expected Output**: A detailed description of what the task’s completion looks like, helping to set clear expectations for the outcome.\n6. **Context**: Before executing the task, we specify which task to wait for to be completed and to obtain the necessary information from that task output with the context parameter.\n\n```python\n#Content Writer Agent and Task\n\nself.writer = Agent(\n  role=\"Content Writer\",\n\n  goal=\"Write insightful and factually accurate \"\n       \"opinion piece about the provided topic\",\n\n  backstory=\"You're working on a writing a new opinion piece \n              about the provided topic.\"\n            \"You base your writing on the work of the Content Researcher, \n             who provides an outline and relevant context about the topic.\"\n            \"You follow the main objectives and direction of the outline, \n              as provide by the Content Researcher.\"\n            \"You also provide objective and impartial insights \n             and back them up with information provide \n             by the Content Researcher.\",\n  verbose=True,\n)\n\nself.write = Task(\n  description=(\n      \"1. Use the content to craft a compelling essay.\\n\"\n      \"2. Incorporate SEO keywords naturally.\\n\"\n      \"3. Sections/Subtitles are properly named in an engaging manner.\\n\"\n      \"4. Ensure the essay is structured with an engaging introduction, \n          insightful body, and a summarizing conclusion.\\n\"\n      \"5. Proofread for grammatical errors and alignment with \n          the brand's voice.\\n\"\n      \"6. Pick a suitable header\\n\"\n  ),\n  expected_output=\"A well-written essay in markdown format, \n                  ready for publication, each section \n                  should have 2 or 3 paragraphs.\",\n  context=[self.research],\n  agent=self.writer,\n)\n```\n\n**Content Editor**: After defining the writer agent, we could have concluded the process; however, even though the writer agent is responsible for writing, it may still make spelling mistakes and errors that disrupt the coherence of the content. To prevent these issues and to export the essay output in JSON format, we will define one more agent: the content editor.\n\nIn the backstory of this agent, we will specify that it is responsible for reviewing and correcting the essay received from the writer agent. In the task phase, we will also define the required output format.\n\n```python\n#Content Editor Agent and Task\nself.editor = Agent(\n    role=\"Content Editor\",\n\n    goal=\"Edit a given essay to align with the writing \n            style of the organization.\",\n\n    backstory=\"You are an editor who receives an essay \n                from the Content Writer.\"\n              \"Your goal is to review the essay to ensure \n                that it follows best practices, provides balanced viewpoints\"\n              \"When providing opinions or assertions,\n                and also avoids major controversial topics \n                or opinions when possible.\",\n    verbose=True\n)\n\nself.edit = Task(\n    description=\"Proofread the given essay for grammatical errors \n                  and alignment with the brand's voice.\",\n\n    expected_output=\"A well-written essay in required format, \n                      ready for publication, each section \n                      should have 2 or 3 paragraphs.\",\n    output_json = Essay,\n    context=[self.write],\n    agent=self.editor\n)\n```\n\nHere, our output is an object named; `Essay`, created with the help of the `BaseModel` and `Field` classes from the Pydantic Library. By adding explanations that our agent can understand, we ensure that the agent will output data in a format expected by our PDF printing function.\n\n```python\n#Expected Pydantic Output\n\nclass Paragraph(TypedDict):\n    sub_header: str\n    paragraph: str\n\nclass Essay(BaseModel):\n    header: str = Field(..., description=\"The header of the essay\")\n    entry: str = Field(..., description=\"The entry of the essay\")\n    paragraphs: List[Paragraph] = Field(..., description=\"The paragraphs of the essay\")\n    conclusion: str = Field(..., description=\"The conclusion of the essay\")\n    seo_keywords: List[str] = Field(..., description=\"The SEO keywords of the essay\")\n```\n\nWe have defined our agents and their tasks. Now, let’s bring together our three agents. For this, we should use a small yet functional method from the CrewAI library called `Crew`. In this method, we list the agents that will operate sequentially along with the tools they will use. If the tasks need to be executed in order, as in our project, we set the `process` parameter to `Process.sequential`. We also set the `memory` parameter to `True` to enable the agents to communicate with each other using short\\-term and long\\-term memory.\n\n```python\n#Crew Run\n\ndef kickoff(self,*args):\n    return Crew(\n        agents=[self.researcher, self.writer, self.editor],\n        tasks=[self.research, self.write, self.edit],\n        process=Process.sequential,\n        verbose=True,\n        memory=True\n    ).kickoff(*args)\n```\n\nOur agent structure is complete, but we haven’t discussed our tools yet. Now, let’s briefly address our tools.\n\n## Building Tools\n\nTools are essentially functions that take various inputs and return a value as output. Our agents will simply provide the expected input to these functions and process the output they receive. Therefore, we need to design our tools with high fault tolerance. When a usage error occurs, our agents should be able to read the error and be equipped with information to use the tool correctly in the next iteration.\n\nAfter preparing the functions for our tools, we should convert them into tool objects using either LangChain or CrewAI’s Tool creation class, along with various explanations. Here, we convert our tool into a form that our agent can use by simply writing C**rewAI’s tool decorator** at the top of our function.\n\n```python\nfrom crewai_tools import tool\n\n@tool(\"Wikipedia Search Tool\")\ndef search_wikipedia(query: str) -> str:\n    \"\"\"Run Wikipedia search and get page summaries.\"\"\"\n    page_titles = wikipedia.search(query)\n    summaries = []\n\n    for page_title in page_titles[:3]:  # First 3 results\n        try:\n            wiki_page = wikipedia.page(title=page_title, auto_suggest=False)\n            summaries.append(f\"Page: {page_title}\\nSummary: {wiki_page.summary}\")\n        except wikipedia.PageError: # Page Not Found\n            pass\n        except wikipedia.DisambiguationError: # Disambiguation Error\n            pass\n\n    if not summaries:\n        return \"No good Wikipedia Search Result was found\"\n\n    return \"\\n\\n\".join(summaries)\n```\n\n## Building App\n\nNow, let’s deploy our application live using the Streamlit framework, which I frequently use and believe offers an easy interface design. Streamlit is an open\\-source Python framework for data scientists and AI/ML engineers to deliver dynamic data apps with only a few lines of code.\n\nOur app primarily activates when the user enters their OpenAI key in a `text_input` box and then clicks the \"Initialize Agents\" button. When the user sends a message through the active `chat_input` section, the following function is used to relay the entered request to our established agent structure:\n\n```python\ndef generate_response(topic):\n    return app.invoke(input={\"topic\": topic})\n```\n\nWith Streamlit’s `st.chat_message` component, we can easily implement a chatbot interface. If the user is engaged in regular messaging, the response will display a normal answer. If an essay has been generated, we’ll provide the directory of the PDF to the user by writing a simple if\\-else loop.\n\nMeanwhile, we add every message we send and receive from the chatbot to a `messages` variable created in Streamlit’s `session_state`. This way, we create a visible chat screen.\n\n```python\n#Streamlit App\n\nimport streamlit as st\nfrom graph import EssayWriter\nimport os\nimport base64\n\nst.set_page_config(page_title=\"Essay Writer Chat Bot\", page_icon=\"🤖\")\nst.image(\"./media/cover.jpg\", use_column_width=True)\n\n\nif \"messages\" not in st.session_state:\n    st.session_state.messages =  [{\"role\": \"assistant\", \"content\": \"Hello!\"}]\n    st.session_state.app = None\n    st.session_state.chat_active = True\n\nwith st.sidebar:\n    st.info(\" * This app uses the OpenAI API to generate text, please provide your API key.\"\n            \"\\n\\n * This app uses the 'gpt-4o-mini-2024-07-18' model. Cost effective and efficient.\"\n            \"\\n\\n * If you don't have an API key, you can get one [here](https://proxy.rifx.online/https://platform.openai.com/signup).\"\n            \"\\n\\n * You can also find the source code for this app [here](https://proxy.rifx.online/https://github.com/mesutdmn/Autonomous-Multi-Agent-Systems-with-CrewAI-Essay-Writer)\"\n            \"\\n\\n * App keys are not stored or saved in any way.\"\n            \"\\n\\n * Writing essay may take some time, please be patient. Approximately 1-2 minutes.\"\n    openai_key= st.text_input(\"OpenAI API Key\", type=\"password\")\n\n\ndef initialize_agents():\n    os.environ[\"OPENAI_API_KEY\"] = openai_key\n    essay_writer = EssayWriter().graph\n\n    if len(openai_key) < 1:\n        st.error(\"Please enter your OpenAI API key and Initialize the agents.\")\n\n        st.session_state.chat_active = True\n    else:\n        st.success(\"Agents successfully initialized\")\n        st.session_state.chat_active = False\n\n    return essay_writer\n\nwith st.sidebar:\n    if st.button(\"Initialize Agents\", type=\"primary\"):\n        st.session_state.app = initialize_agents()\n\napp = st.session_state.app\ndef generate_response(topic):\n    return app.invoke(input={\"topic\": topic})\n\n\nfor message in st.session_state.messages:\n    with st.chat_message(message[\"role\"]):\n        st.markdown(message[\"content\"], unsafe_allow_html=True)\n\nif topic:= st.chat_input(placeholder=\"Ask a question\", disabled=st.session_state.chat_active):\n    st.chat_message(\"user\").markdown(topic)\n\n    st.session_state.messages.append({\"role\": \"user\", \"content\": topic})\n    with st.spinner(\"Thinking...\"):\n        response = generate_response(topic)\n\n    with st.chat_message(\"assistant\"):\n        if \"pdf_name\" in response:\n            with open(f\"./{response['pdf_name']}\", \"rb\") as file:\n                file_bytes = file.read()\n                b64 = base64.b64encode(file_bytes).decode()\n            href = f'<a href=\"data:application/pdf;base64,{b64}\" download=\"{response['pdf_name']}\">{response['pdf_name']}</a>'\n\n            st.markdown(f\"{response['response']}: {href}\", unsafe_allow_html=True)\n            st.session_state.messages.append({\"role\": \"assistant\", \"content\": f\"{response['response']}: {href}\"})\n        else:\n            st.markdown(response[\"response\"])\n            st.session_state.messages.append({\"role\": \"assistant\", \"content\": response[\"response\"]})\n```\n\n**Congratulations**! We have completed our project. If you wish, you can watch the project work log that I have recorded for you. Don’t forget to visit the GitHub [**repo**](https://proxy.rifx.online/https://github.com/mesutdmn/Autonomous-Multi-Agent-Systems-with-CrewAI-Essay-Writer) to access all the codes of the project.\n\nAnd this is how our app’s main page will look like once we deploy it!\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8tDAluuAH6njIohDbb-UqA.png)\n\n## Conclusion\n\nIn this article, we explored how to build autonomous multi\\-agent systems using CrewAI. We started by discussing the motivation behind creating agents and how they can work together to accomplish tasks more efficiently. By breaking down tasks and utilizing tools, we enabled our agents to perform complex operations in a structured way.\n\nWe developed a simple project that integrated the CrewAI and LangChain frameworks, showcasing how multiple agents can collaborate to gather information, write essays, and edit content. The use of tools and task management was emphasized to ensure our agents operated smoothly and effectively.\n\nFinally, we deployed our application using Streamlit, allowing users to interact with the system effortlessly.\n\nYou can check out the live project [**here**](https://proxy.rifx.online/https://multi-agent-essay-writer.streamlit.app/), view the source code on my GitHub repository [**here**](https://proxy.rifx.online/https://github.com/mesutdmn/Autonomous-Multi-Agent-Systems-with-CrewAI-Essay-Writer)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/building-self-healing-intelligent-test-automation-with-gen-ai-openai-apis-6c39808adb0f","frontmatter":{"title":"Building Intelligent Test Automation with Gen AI (OpenAI APIs)","meta_title":"Building Intelligent Test Automation with Gen AI (OpenAI APIs)","description":"We all know that UI Tests are super fragile. They can break for all sorts of reasons, and one of the biggest culprits is changes to the UI…","date":"2024-11-13T01:22:29.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kZ4ZR-jqdTTgH3bpOzcgUw.png","categories":["Generative AI","Programming","Testing"],"author":"Rifx.Online","tags":["Generative","OpenAI","Selenium","LLMs","POM"],"draft":false,"slug":"blog/building-self-healing-intelligent-test-automation-with-gen-ai-openai-apis-6c39808adb0f"},"content":"\n\n\n\n\n> We all know that UI Tests are super fragile. They can break for all sorts of reasons, and one of the biggest culprits is changes to the UI locators. It’s hard to imagine how we can make them smart enough to understand when the locators have changed and prevent the tests from running until when the locator issue in our test occurs.\n\nGuess what? It’s 2024, and automation testing tools have come a long way. After almost 18 years of working with them, from Mercury Winrunner to Playwright, we can now do some truly amazing things thanks to the power of Generative AI. It’s like magic, but it’s science!\n\nYou heard it right, we can now device a way to make our test automation code **more intelligent** without writing all sort of fuzzy mathematical algorithms ourselves, it's all taken care by the God of LLMs.\n\nIn this post, we are going to discuss how we can make our tests intelligent in more effective and efficient fashion, but again, for you to make this happen you need to have following pre\\-requisite\n\n1. **Open AI API** with Credit (you need to purchase it on the go)\n\n\n\n2\\. C\\# .NET code knowledge, as the code I will be covering is from .NET and Selenium\n\n3\\. Basic understanding of Test automation\n\nAgain, all the above and following discussions are part of my [Udemy course](https://proxy.rifx.online/https://www.udemy.com/course/generative-ai-in-software-automation-testing/) which covers even more details and step\\-by step way of writing the code.\n\n\n## Lets understand the problem statement\n\nSo we’ve got this page that we want to automate using Selenium C\\# code. We’ve written the code really well using the Page Object model (POM) pattern, and everything looks great and works perfectly as shown below.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GPSBTmPEZBpubI72OElwbw.gif)\n\nOur dev champ spotted a UI element that needs some tweaking. He made a change based on his peers’ code review comments, but unfortunately, he removed a locator that we’re using in our automation testing. This means our POM code with the locator won’t work anymore since it doesn’t exist anymore, which eventually cause the test to **FAIL**.\n\nThe most important thing is that the test will fail for all the test scenarios with the same failure because of a single locator change. The test doesn’t know or have any way to know that the locator has changed, and it fails all the time.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*tINBnScOW78vz8sKb6lWbA.gif)\n\n\n## How to resolve the problem?\n\nI am sure like me many of you are going through this day\\-in and day\\-out everyday while working with UI testing tools, it could be **Cypress**, **Selenium** or **Playwright**. The problem is always imminent regardless of the tool\n\nNow let's understand how we can resolve the problem above.\n\nWe all know **Generative AI** with **LLMs** (Large Language Models) are way beyond just text/image/video generation. They understand the given context and generate a meaningful set of information that we are looking for.\n\nSo, with the above problem statement, we can using the power of Gen AI using the OpenAI’s API which can pass our prompt request to LLMs like ***GPT 4o*** or ***GPT 4 turbo*** to understand the problem statement and give us meaningful solution.\n\n\n> So, whats the prompt request we need to pass to OpenAI’s API to get the operation to happen in our test automation?\n\nWell, this diagram will give you the answer\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*bsCOcyWc0FDnxPp9ApssVw.gif)\n\nWe can send OpenAI’s API the “**Actual Page Under Test**” of our application and our Selenium Test’s “**Page Object Model**” code as a prompt (with a few extra response parsing details). This will act as a validation process from OpenAI’s API to see if the locators match the given page.\n\nBased on that operation, we can tell the test to execute or not. The locators have changed, so it’s not worth running the test any further.\n\nThe code to perform the above operation will look something like this\n\n\n```python\npublic static async Task<string> VerifyPageLocatorFromAiAsync(string pomFileContent, string htmlPageSource)\n{\n    ChatClient client = new(model: \"gpt-4o-mini\", apiKey);\n    \n    var chatMessage = $\"Verify if locators from this Selenium POM class: {pomFileContent} match this page source: {htmlPageSource}\\\", only return True or False result\";\n\n    ChatCompletion completion = await client.CompleteChatAsync(chatMessage);\n\n    return completion.Content.FirstOrDefault().Text;\n}\n```\nThe code above is just part of the large code covered in the course, but you can see how straightforward it is to perform the operation of analysing your page against the Page Object Model code of Selenium.\n\n\n## GenAI in Software Testing Course\n\n*Most of the above discuss is just a slice of the topic we have discussed in my new course in Udemy on “[**Using Generative AI in Software Automation Testing**](https://proxy.rifx.online/https://www.udemy.com/course/generative-ai-in-software-automation-testing/)”*\n\nHere is the course content\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*lHe_b7qVqUQo-9Y5.png)\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*rMrsbB2IaKPbthdAr9Rc9g.png)\n\nThe course is currently available for discount in Udemy as the launch offer, please use coupon code **EA\\_NOV\\_24 ⚡️** for discount while purchasing the course.\n\nIf the coupon code is expired, please feel free to comment on this post, I will send you the latest available coupon code.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/case-study-turning-doctor-transcripts-into-temporal-medical-record-knowledge-graphs-cf624d4927eb","frontmatter":{"title":"Case Study: Turning Doctor Transcripts into Temporal Medical Record Knowledge Graphs","meta_title":"Case Study: Turning Doctor Transcripts into Temporal Medical Record Knowledge Graphs","description":"Showcase of Data Transformation Process, Breakdown of 25 dev hours involved, Schemas used, Questions & Responses, and Graph created","date":"2024-11-13T01:22:29.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*DUNtg-0w2z-vlF9SCvt5UA.png","categories":["Health","Data Science","Machine Learning"],"author":"Rifx.Online","tags":["transcripts","Temporal","Knowledge","Graphs","vector"],"draft":false,"slug":"blog/case-study-turning-doctor-transcripts-into-temporal-medical-record-knowledge-graphs-cf624d4927eb"},"content":"\n\n\n\nInterested in turning Doctor/Patient medical records and transcripts into Temporal Knowledge Graph that you can ask complex questions across multiple medical histories, time periods, and patients?\n\nIn this case study, we show how Medical Transcripts are turned into Temporal Knowledge Graphs that you can rely on for the purposes of RAG and analytics. We show what real Question \\& Answers are against this system, and what type of business outcomes you can achieve with this system. As far as we are aware, the combination of steps here is a relatively novel Knowledge Graph implementation.\n\n\n### Data used\n\nFor data privacy reasons, we used a synthetic dataset of Medical Transcripts that we created out of Synthea data found here: [https://synthea.mitre.org/downloads](https://proxy.rifx.online/https://synthea.mitre.org/downloads). The below is an example of one of the medical transcripts used as the input data for the Knowledge Graph creation. We combined these transcript data with structured medical records in the Synthea data. We had \\~75 transcripts that covered 10 patients (i.e. each patient had 5–10 transcripts). Here is an example of a transcript used:\n\n\n\n\n## Novel Knowledge Graph Architecture Overview\n\n\n### Nodes:\n\nWe have 5 types of Nodes: Patient, Observation, Immunization, Condition and Encounter Type\n\n\n### Triples (Sample List):\n\nPatient \\-\\> Had Encounter \\-\\> Encounter\n\nPatient \\-\\> Has Condition \\-\\> Condition\n\nPatient \\-\\> Received \\-\\> Immunization\n\nPatient \\-\\> Has Measurement \\-\\> Observation\n\n\n### Chunks:\n\nChunks are chunks of text that are standalone objects. Chunks are tied to each Triple, and there can be many Chunks tied to a single Triple. Instead of being the unstructured source of the Triple, the Chunks in this case are summaries and key points related to each Triple type. As a result, we have 6 types of Chunks:\\- Patient Demographic Chunks, Condition Summary Chunks, Visit Chunks, Observation Chunks, Immunization Chunks and Condition Detail Chunks.\n\nAn example of what different type of chunks are tied to triples look like the following:\n\n\n```python\n1. Patient -> EncounterType\nTriple: (Patient) -[had_encounter]-> (EncounterType)\n- Chunk_ids link to specific visit instances\n- Example Chunk: \"Annual physical on 2024–01–15. BP 120/80, routine screenings \nupdated.\"\n\n2. Patient -> Condition\nTriple: (Patient) -[has_condition]-> (Condition)\n- Chunk_ids link to condition episodes\n- Example Chunk: \"Diagnosed with hypertension on 2020–03–10. Status: active. \nManaged with medication.\"\n\n3. Patient -> Immunization\nTriple: (Patient) -[received]-> (Immunization)\n- Chunk_ids link to administration records\n- Example Chunk: \"Influenza vaccine administered on 2024–01–15.\"\n\n4. Patient -> Observation\nTriple: (Patient) -[has_measurement]-> (Observation)\n- Chunk_ids link to measurement instances\n- Example Chunk: \"2024–01–15: Blood Pressure 120/80 mmHg, Weight 70kg.\"\n```\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*8dH_7tP6xheCaW6K)\n\n**Link to the Graph created: [https://proxy.rifx.online/https://main\\-\\-whyhowai.netlify.app/public/graph/673032011997e08c8849316c](https://proxy.rifx.online/https://main--whyhowai.netlify.app/public/graph/673032011997e08c8849316c)**\n\nWith this particular graph architecture, where you can have key points and summaries tied to triples, you can then focus on landing on the right set of triples through an unstructured search, and subsequently bringing in all the relevant key information through the linked chunks in a structured way.\n\n\n## Unique to WhyHow Architecture\n\nThere are a few things unique to the WhyHow graph infrastructure that allows us to build this architecture in a simple way.\n\nFirstly, Triples are embedded and retrieved through vector search, avoiding a common retrieval issue of having to use Text2Cypher for the identification of nodes, relationships, and then Cypher query construction just to land on the right Triple. This has shown to dramatically [improve retrieval accuracy by up to 3x](https://proxy.rifx.online/https://readmedium.com/knowledge-table-multi-document-rag-extraction-memory-ec08450e858f).\n\nSecondly, Triples are standalone objects in WhyHow that you can link chunks to. This allows you to distill the key information that you want to retrieve per Triple, and bring it directly into the context once the right Triples are found. This avoids having to represent crucial information and context in a graph format (complicating the schema construction process), and bringing in information in a structured way after the initial unstructured vector search. This is similar in process to [LinkedIn’s application of Knowledge Graphs](https://proxy.rifx.online/https://readmedium.com/5-misconceptions-of-kg-rag-systems-building-using-rag-native-graphs-5e47872e7903) in their system, where crucial information like ‘Steps to Reproduce’ are represented and retrieved similarly, and where the steps themselves are represented as individual ‘chunks’/ ‘nodes’.\n\nThirdly, WhyHow accepts data in a JSON format, which allows seamless interaction between any of the extraction frameworks directly into graph creation. In this case, we use Claude for the initial transformation of the transcript data into the necessary JSON structure to load into WhyHow. If you have information already sitting in JSON, loading data into WhyHow is then a lot easier.\n\nFourthly, because of the way that Chunks and the retrieval process is designed in the WhyHow system, you can easily include temporal data that can be used to govern the way that the answer is constructed. Temporal data has always been a hard thing to model in Knowledge Graphs (to the point that it is typically advised against by leading KG experts), but is an obviously important part of workflows. Existing methods that even attempt to model temporal data try to ingest this into the Knowledge Graph itself and then retrieve based on a structured Cypher query, as opposed to our architecture that uniquely uses the LLM to help filter for temporal data.\n\nBlending the power of LLM with structured knowledge representations like Knowledge Graphs are important ways to achieve business outcomes, and we think this temporal Knowledge Graph architecture will help unlock a lot of business value through the successful implementation of temporal data.\n\n\n### Data Transformation Process Used\n\nFirst, we use Claude to turn the transcript information into a schema\\-aligned set of information on a per transcript basis. Alongside information from structured medical Records, the transcript is turned into a JSON summarization that looks like this:\n\n\n```python\nPATIENT SUMMARY\nName: Joseph Crona\nDOB: 2022–08–29\nAge: 2 years\nGender: male\nMRN: #dbfbaa\n\nCURRENT MEASUREMENTS (as of 2024–08–05)\nHeight: 84.1cm (50th percentile)\nWeight: 14.5kg (52nd percentile)\nALLERGIES\nNo known allergies\n\nIMMUNIZATIONS\n- DTaP: 2022–12–05, 2023–02–06, 2023–03–06, 2024–02–05\n- Hepatitis A: 2023–11–06\n- Hepatitis B: 2022–08–29, 2022–10–03, 2023–03–06\n- Hib: 2022–12–05, 2023–02–06, 2023–11–06\n- Influenza: 2023–03–06, 2024–08–05\n- MMR: 2023–11–06\n- PCV13: 2022–12–05, 2023–02–06, 2023–03–06, 2023–11–06\n- Polio: 2022–12–05, 2023–02–06, 2023–03–06\n- Rotavirus: 2022–12–05, 2023–02–06\n- Varicella: 2023–11–06\n\nMEDICAL HISTORY\n- Viral sinusitis (disorder)\nOnset: 2023–03–13\nStatus: resolved\nOutcome: Resolved\n\nGROWTH & DEVELOPMENT\n- 2023–11–06: Body Weight: 12.7 kg\n- 2024–02–05: Body Height: 79 cm\n- 2024–02–05: Body Weight: 13.4 kg\n- 2024–08–05: Body Height: 84.1 cm\n- 2024–08–05: Body Weight: 14.5 kg\nDevelopment: Age-appropriate milestones met\n- Gross motor: Age appropriate\n- Fine motor: Age appropriate\n- Language: Age appropriate\n- Social: Age appropriate\n\nPREVENTIVE CARE\nWell-Child Visits:\n- 2024–08–05: 2yo well visit - Development on track\n- 2024–02–05: 1yo well visit - Development on track\n- 2023–11–06: 1yo well visit - Development on track\n- 2023–08–07: 1yo well visit - Development on track\n- 2023–05–08: 9mo well visit - Age appropriate exam completed\n- 2023–02–06: 6mo well visit - Age appropriate exam completed\n- 2022–12–05: 4mo well visit - Age appropriate exam completed\n- 2022–10–03: 2mo well visit - Age appropriate exam completed\n- 2022–08–29: Newborn visit - Normal exam\n\nFAMILY HISTORY\nMother: Healthy\nFather: Healthy\nSiblings: None documented\n\nSOCIAL HISTORY\nLiving Situation: Lives with parents\nDevelopment: Meeting age-appropriate milestones\nSleep: Age-appropriate pattern\nNutrition: Age-appropriate diet\n```\nSecondly, we map this JSON schema into the WhyHow schema, and then import all the information into the WhyHow.AI KG Studio.\n\nThe below is a sample of the KG Structure that was ultimately loaded into WhyHow.\n\n\n```python\nKnowledge Graph Structure (Timeless):\n\n\nNodes:\n1. Patient Node\n  Structure: {\n      name: str,         # \"John Smith\"\n      label: \"Patient\",\n      properties: {\n          gender: str,   # FHIR gender\n          patient_type: str  # \"adult\" | \"pediatric\"\n      },\n      chunk_ids: List[str]  # Links to demographic chunks\n  }\n\n\n2. EncounterType Node\n  Structure: {\n      name: str,         # \"Well-child visit\" | \"Annual physical\"\n      label: \"EncounterType\",\n      properties: {\n          category: str,  # \"preventive\" | \"acute\" | \"chronic\"\n          specialty: str  # \"primary_care\" | \"pediatrics\" | \"emergency\"\n      },\n      chunk_ids: List[str]  # Links to visit pattern chunks\n  }\n\n\n3. Condition Node\n  Structure: {\n      name: str,         # \"Essential hypertension\"\n      label: \"Condition\",\n      properties: {\n          category: str,     # \"chronic\" | \"acute\" | \"resolved\"\n          system: str,       # \"respiratory\" | \"cardiovascular\" | etc\n          is_primary: bool   # True if primary diagnosis\n      },\n      chunk_ids: List[str]  # Links to condition history chunks\n  }\n\n\n4. Immunization Node\n  Structure: {\n      name: str,         # \"DTaP\" | \"MMR\"\n      label: \"Immunization\",\n      properties: {\n          series: str,       # \"primary\" | \"booster\"\n          target: str        # \"tetanus\" | \"measles\" | etc\n      },\n      chunk_ids: List[str]  # Links to immunization records\n  }\n\n\n5. Observation Node\n  Structure: {\n      name: str,         # \"Blood Pressure\" | \"Height\"\n      label: \"Observation\",\n      properties: {\n          category: str,     # \"vital\" | \"lab\" | \"growth\"\n          unit: str         # \"mmHg\" | \"cm\" | etc\n      },\n      chunk_ids: List[str]  # Links to measurement records\n  }\n\n\nRelations:\n1. Patient -> EncounterType\n  Triple: (Patient) -[had_encounter]-> (EncounterType)\n  - Chunk_ids link to specific visit instances\n\n\n2. Patient -> Condition\n  Triple: (Patient) -[has_condition]-> (Condition)\n  - Chunk_ids link to condition episodes\n\n\n3. Patient -> Immunization\n  Triple: (Patient) -[received]-> (Immunization)\n  - Chunk_ids link to administration records\n\n\n4. Patient -> Observation\n  Triple: (Patient) -[has_measurement]-> (Observation)\n  - Chunk_ids link to measurement instances\n\n\n5. Condition -> EncounterType\n  Triple: (Condition) -[managed_in]-> (EncounterType)\n  - Links conditions to typical encounter types\n\n\n6. Immunization -> EncounterType\n  Triple: (Immunization) -[given_during]-> (EncounterType)\n  - Links vaccines to visit types\n```\nThirdly, we then run a custom prompt that contextualizes the triples retrieved from the Knowledge Graph after every natural language query.\n\nWith this architecture in place, one interesting thing is that we can now continue to add information about Patient visits, Patient treatments and conditions to the Knowledge Graph easily, since it is just a matter of adding additional chunks to the existing triples that exist. If a Patient gets a new disease, additional Condition nodes are added to the Patient nodes.\n\nThis process took 25 dev hours, which can be broken down into the following:\n\n* 2 hours (8%) was spent looking and understanding the data (Exploratory Data Analysis)\n* 18 hours (72%) was spent iterating on the schema, and figuring out what nodes should be in the graph, what nodes should be connected to what, what chunks should exist, how it should connect to the various triples, testing the retrieved answers with a set of questions, and iterating accordingly.\n* 2 hours (8%) was spent writing the code to create the set of triples to be loaded in\n* 3 hours (12%) was spent writing validation checks and output checks to catch any errors\n\n\n### Question \\& Answer against Medical Record Knowledge Graphs\n\nPrompt used for answer construction after retrieving the relevant context from the Knowledge Graph, using the WhyHow natural language graph query engine\n\n\n```python\n    You are an AI assistant specializing in medical records analysis. \nUse the following information to answer the user's question. \n    The information is derived from a knowledge graph of patient medical records.\n\n    Relevant Nodes (these represent patients, encounters, and conditions):\n    {node_context}\n\n    Relevant Relationships (these show connections between patients, \nencounters, and conditions):\n    {triple_context}\n\n    Relevant Medical Records:\n    {chunk_context}\n\n    Answer the question using the following steps:\n    1. Analyze the medical information, focusing on the specific topic of the \nquestion\n    2. Identify key medical events, conditions, and temporal relationships\n    3. Formulate a clear and concise answer that directly addresses the \nquestion\n    4. If there are any uncertainties in the medical information, mention \nthem explicitly\n    5. If you don't have enough information to answer confidently, say so\n    6. Consider temporal relationships between medical events\n    7. It's currently {datetime.now().strftime(\"%Y-%m-%d\")}. Consider this \nwhen interpreting dates\n    8. Maintain patient privacy by using only first names or initials when \nnecessary\n\n    Question: {question}\n```\n\n## Business Value: Analysis of Patient Treatments\n\n\n### Historical Patient Analysis\n\nQuestion: **What types of well\\-child visits and immunizations did Joseph Crona receive in their first year?**\n\nAnswer:\n\n\n```python\nTo determine the types of well-child visits and immunizations Joseph Crona \nreceived in his first year, we need to consider the relevant medical events \nand their timing. Joseph was born in 2022, and it is currently November 9, 2024.\nTherefore, his first year would span from 2022 to 2023.\n\n**Well-Child Visits:**\n- Joseph had an encounter categorized as a \"Well child visit,\" which falls \nunder preventive care in pediatrics. This type of visit is typically part of \nroutine check-ups for children, especially in their first year.\n\n**Immunizations:**\nDuring his first year, Joseph received the following immunizations, all of \nwhich are common for pediatric patients:\n1. **Hep B, adolescent or pediatric** - Completed\n2. **Hib (PRP-OMP)** - Completed\n3. **Rotavirus, monovalent** - Completed\n4. **IPV (Inactivated Poliovirus Vaccine)** - Completed\n5. **DTaP (Diphtheria, Tetanus, and Pertussis)** - Completed\n6. **Pneumococcal conjugate PCV 13** - Completed\n\nThese immunizations are typically administered during the first year of life \nas part of the standard immunization schedule for infants.\n\nIn summary, Joseph Crona received a well-child visit and completed a series of\nstandard pediatric immunizations during his first year.\n```\nQuestion: **For patients with otitis media, what was the sequence of encounters and observations recorded?**\n\nAnswer:\n\n\n```python\nTo address the question regarding the sequence of encounters and observations \nfor patients with otitis media, we will focus on the medical records of Miles \nDickinson, as he is the patient identified with this condition.\n\n1. **Condition Onset:**\n- Miles Dickinson was diagnosed with otitis media on June 10, 2024. The \ncondition is currently resolved.\n\n2. **Encounters:**\n- Miles had two types of encounters recorded:\n- **Well Child Visit:** This is categorized as a preventive encounter in \npediatrics.\n- **Encounter for Symptom:** This is categorized as a follow-up encounter in \nprimary care.\n\n3. **Observations:**\n- Various observations were recorded for Miles, including:\n- **Pain Severity:** Recorded multiple times with scores ranging from 0 to 4.\n- **Body Weight:** Recorded with values ranging from 3.2 kg to 8.2 kg.\n- **Head Occipital-frontal Circumference and Percentile:** Recorded with \nspecific values in cm and percentiles.\n- **Heart Rate:** Recorded with values ranging from 61 to 100 beats per minute.\n- **Respiratory Rate:** Recorded with values ranging from 12 to 16 breaths per\nminute.\n\n4. **Temporal Sequence:**\n- The otitis media diagnosis occurred on June 10, 2024. Observations and \nencounters before this date may have been related to other conditions or \nroutine check-ups. Observations and encounters after this date could be \nrelated to follow-up care for otitis media or other health assessments.\n\n5. **Conclusion:**\n- The sequence of encounters and observations for Miles Dickinson includes \npreventive and follow-up encounters, along with various health observations. \nThe specific sequence related to otitis media is not detailed in terms of exact\ndates for each observation or encounter, but the diagnosis date provides a \ntemporal marker for understanding subsequent medical activities.\n\nIf more detailed temporal data on each observation and encounter were \navailable, a more precise sequence could be established. However, based on \nthe current information, this is the overview of the sequence related to \notitis media for Miles Dickinson.\n```\n\n### Historical Multi\\-Patient Analysis\n\nQuestion: **How many patients have been diagnosed with acute bronchitis, and what are the typical onset patterns?**\n\nAnswer:\n\n\n```python\nBased on the provided medical records, there are two patients diagnosed with \nacute bronchitis: Maximo Swaniawski and Bennie Herman.\n\n#### Onset Patterns:\n- **Maximo Swaniawski**:\n- Onset dates for acute bronchitis:\n- October 24, 2014\n- November 7, 2015\n- November 29, 2018\n- **Bennie Herman**:\n- Onset dates for acute bronchitis:\n- October 24, 2014\n- November 7, 2015\n- November 29, 2018\n\n#### Typical Onset Patterns:\nThe onset dates for both patients are identical, suggesting a pattern where \nacute bronchitis episodes occurred in late October or November. This could \nindicate a seasonal pattern, possibly related to colder weather or other \nseasonal factors that might contribute to respiratory conditions.\n\nBoth patients have had their conditions resolved, indicating that the episodes \nof acute bronchitis were acute and not chronic.\n```\nQuestion: **Which patients had both viral sinusitis and bronchitis, and what were their earliest onset dates?**\n\nAnswer:\n\n\n```python\nTo determine which patients had both viral sinusitis and bronchitis, \nwe need to analyze the conditions associated with each patient and their \nonset dates.\n\n1. **Maximo Swaniawski**:\n- Conditions: Acute bronchitis\n- Earliest onset of bronchitis: 2014–10–24\n2. **Cassandra Monahan**:\n- Conditions: Viral sinusitis\n- Earliest onset of sinusitis: 2014–12–12\n3. **Bennie Herman**:\n- Conditions: Acute bronchitis\n- Earliest onset of bronchitis: 2014–10–24\n4. **Deangelo Price**:\n- Conditions: Viral sinusitis\n- Earliest onset of sinusitis: 2014–12–12\n5. **Joseph Crona**:\n- Conditions: Viral sinusitis\n- Earliest onset of sinusitis: 2014–12–12\n\nFrom the information provided, none of the patients have both viral sinusitis \nand bronchitis. Each patient has either one condition or the other, but not \nboth. Therefore, there are no patients with both conditions, and consequently, \nno earliest onset dates for both conditions in a single patient.\n```\n\n### Advantages against Vector\\-Only RAG systems\n\nSuperior Analytics across multiple patients:\n\n* In the question about which patients have both bronchitis and viral sinusitis conditions, we must be able to know if a patient is recorded to have or not have a particular disease. Since Vector\\-Only Search is about identifying relevant chunks, it cannot detect if a patient definitely does not have a particular disease. This means that an intermediary aggregation of data is required that can definitively say that Patient X does not have the ‘diabetes’ node against their name.\n\nMulti\\-Transcript Analysis and RAG:\n\n* In the question about Miles and his multiple visits in the past year, we can see that the conditions are across multiple visits and multiple transcripts. This means that an intermediary aggregation of data is required that maps a Patient and the cumulative visits and observations they’ve incurred over time.\n\nWhyHow.AI provides tools, services and processes for Structured Knowledge, Knowledge Graphs and more reliable Agentic RAG solutions. If you are interested in exploring any of our tools ([KG Studio](https://proxy.rifx.online/https://readmedium.com/whyhow-ai-kg-studio-platform-beta-rag-native-graphs-1105e5a84ff2), [Knowledge Table \\[Open Source]](https://proxy.rifx.online/https://readmedium.com/knowledge-table-multi-document-rag-extraction-memory-ec08450e858f)) and services, feel free to [chat with us here](https://proxy.rifx.online/https://calendly.com/whyhowai/intro-call-whyhow-ai).\n\nIf you’re thinking about, in the process of, or have already incorporated knowledge graphs in RAG for accuracy, memory and determinism, follow our newsletter at [WhyHow.AI](https://proxy.rifx.online/https://whyhow.ai/) or join our discussions about rules, determinism and knowledge graphs in RAG on our [Discord](https://proxy.rifx.online/https://discord.gg/9bWqrsxgHr).\n\n\n"},{"lang":"en","group":"blog","slug":"blog/chatgpt-vision-turns-a-picture-into-1000-words-24858615fa28","frontmatter":{"title":"ChatGPT Vision Turns A Picture Into 1000 Words","meta_title":"ChatGPT Vision Turns A Picture Into 1000 Words","description":"And How You Can Turn Those Words Into Business","date":"2024-11-10T03:51:17.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*lS5aPVDrsCFFnBYz","categories":["Programming","Marketing","Generative AI"],"author":"Rifx.Online","tags":["automation","content","GPT","MAKE","photos"],"draft":false,"slug":"blog/chatgpt-vision-turns-a-picture-into-1000-words-24858615fa28"},"content":"\n\n\n\n\n### And How You Can Turn Those Words Into Business\n\nI’ve had this idea for nearly a decade. It all started when I was building websites, and a lodge owner sent me a thumb drive packed with almost a thousand photos. And a box of 35mm photographs for me to scan.\n\n\n\n\n> These were **amazing shots** — guests showing off their prized catches, stunning lake views, and guides leading outdoor adventures.\n\n\n> I knew that if we could get these photos online, they would create a tidal wave of **word\\-of\\-mouth** marketing for the lodge.\n\nBut here’s where it got complicated: each photo needed a unique description, proper tags, a blog post, and social media uploads. And there were almost a thousand of them!\n\nThe time and cost involved were staggering. Writing captions for hundreds of fish photos? It was enough to make anyone’s head spin. So, the great idea remained just that — an idea.\n\nFast forward to today. Now, with the power of automation, I’ve turned that idea into reality.\n\nI’ve built a system that automates content creation for lodges, transforming old photos into engaging stories that can be published online effortlessly.\n\nIn this article, I’ll walk you through the exact steps I took to build this system, and how you can do the same to save time and preserve the history of your lodge.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*5GBorUl_PfqiLnSW6-Nsjg.png)\n\n\n\n\n\n\n\n\n## Step 1: Collect Your Lodge’s Unique Details\n\nThe first step in setting up this automation is gathering all the information and assets that make your lodge special. For me, this involved collecting the details that would make our content stand out. Think about the key experiences your lodge offers, like fishing trips, local adventures, or unique amenities. These details are what will make your content personal and engaging.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*4QX4oGCYK5djc9EZuE9-EA.png)\n\nNext, gather old photos — everything from past guest experiences to nature shots around the lodge.\n\nFor my experiment, I started with AI generated images, but then a follower of our page on Facebook sent in some pictures. I asked permission to feature them, and he loves it!\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*PuwLsJ2EuOYOLzXwUvn0cQ.png)\n\nIf you have a collection of photos like these, organize them into a Google Drive folder. This will make it easy to feed them into the automation system later. Along with the photos, create a Google Spreadsheet where you can log details about each image. Your spreadsheet should include:\n\n* The image URL (from your Google Drive folder)\n* Lodge details (such as fishing guides, special activities)\n* Any relevant stories or descriptions that can accompany the photo\n\nThis might seem like extra work upfront, but it’s critical for helping the automation create meaningful content later.\n\n\n## Step 2: Build the Automation Blueprint\n\nOnce you’ve gathered all the assets, it’s time to set up the automation blueprint. I used an automation platform called [MAKE](https://www.make.com/en/register?pc=saleprice). If you’ve never worked with automation before, don’t worry — this is easier than it sounds.\n\n… and you can **get all** my proven blueprints for [free](https://whop.com/ai-businessplans) in our 7 day trial.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ArzS71K2fJf4EfCg5y3BPQ.png)\n\nStart by duplicating an existing automation template, like one that posts weather updates to social media. I had a weather automation in place, so I used that as a base and stripped out the unnecessary parts. You want a clean slate to work with, so remove any modules that don’t apply to your lodge, such as weather updates or extra social media channels you won’t use.\n\nNow, it’s time to make the automation specific to your lodge.\n\n\n## Step 3: Customize the Automation for Your Lodge\n\nWith the basic structure in place, customize the automation by adding lodge\\-specific content. This is where the details you gathered in Step 1 come into play. Input your lodge description, add stories from past adventures, and incorporate local tips. Make sure to include keywords that will help your content get noticed online.\n\nNext, configure the automation to pull the photos from your Google Drive folder and match them with the corresponding descriptions from your spreadsheet. This ensures that the right image is paired with the right story.\n\nHere’s where the magic happens: I integrated GPT (a language model AI) into the automation. GPT analyzes each photo and generates unique content based on the details you provided.\n\nFor example, if the photo shows a guest catching a huge fish, GPT can create a post about that specific experience, including details about the fishing guide, the type of fish, and even tips for future visitors.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*wySL0TxoNTFICejPsJOOcA.png)\n\n\n## Step 4: Automate Publishing to Social Media and Medium\n\nOnce the content is generated, it’s time to automate the publishing process. I connected our Medium account so that GPT\\-generated articles could be uploaded directly as drafts, ready for review. Medium is a great platform for long\\-form content like blog posts or detailed guest stories.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*56OzNVMIxw9sJKBn1jkriQ.png)\n\nFor shorter content, like social media posts, the automation links to our Facebook and Twitter accounts. The system is designed to create snippets from the longer articles, which are perfect for quick social media updates. You can also configure the automation to post automatically or schedule posts for specific times.\n\nThe beauty of this system is that once the content is approved, the automation handles everything from posting to scheduling. It’s a hands\\-off solution that keeps your online presence active without requiring constant attention.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*rym300CevwmoA4_pvYybqQ.png)\n\n\n## Step 5: Testing and Fine\\-Tuning\n\nNow that the automation is in place, it’s important to test it before going live. I ran several test posts to ensure that everything worked as expected. The automation pulled the correct photos, generated engaging content, and posted it to Medium and social media without a hitch.\n\nDuring testing, I made small adjustments, such as tweaking titles or refining the GPT prompts to make sure the content aligned perfectly with our lodge’s tone and story.\n\nOnce everything was dialed in, I can create a steady stream of fresh content that keeps our audience engaged.\n\n\n## A Test Project in Action\n\nTo give you a practical example, take a look at the [iFish Canada Facebook page](https://facebook.com/ifishcanada).\n\nThis project is a demonstration of how the automation works in real life.\n\nThe system takes photos submitted by our followers, from their fishing trips in Canada, runs them through GPT, and generates unique posts that showcase the photo, the experiences of its author — and for our example, the posts tie in the lore of our fictional lodge.\n\nThe content is rich, engaging, and best of all — automated.\n\nWhat once seemed like an impossible task is now a reality, saving 100’s and 100’s of hours and allowing us to share the stories that make our fishing picture contributors feel recognized and appreciated.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Pe-hBjGCN1qbkjLl6cmTmQ.png)\n\n\n## Step 6: Monitor, Adjust, and Grow\n\nEven though the system may run automatically, it’s important to monitor the results and make adjustments over time.\n\nI will check which posts get the most engagement and tweak the GPT prompts to improve future content. This ongoing fine\\-tuning ensures that our online presence stays fresh and continues to attract new visitors.\n\nImagine how you can preserve the legacy of your lodge, sharing memories that might otherwise have been lost, and generating word\\-of\\-mouth marketing in a modern, powerful way.\n\n\n## A Decade in the Making — Now You Can Join Me\n\nAfter years of dreaming about automating this process, it’s finally real. The system is up and running, and the results will speak for themselves.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*135_nP0nL6NrT_O7JxIk4g.png)\n\nNow, I want to invite you to experience this for your lodge or resort. If you’ve ever felt overwhelmed by the time or cost of marketing, or if you’ve struggled to keep up with the constant demand for new content, [this automation system](https://whop.com/ai-businessplans) could be the solution you’ve been looking for.\n\nVisit [iFish Canada](https://facebook.com/ifishcanada) to see the system in action, and if you’d like to get started with automating your own content, feel free to reach out.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*IJ-R1362_IWUrZ-3KyG8mw.png)\n\nI’d love to help you unlock the power of automation to grow your lodge, share your story, and attract new guests — all while saving you time and effort.\n\n⫷\n\n\n### I Value Your Comments\n\nI reply to all comments and **as my thank you** I’ll also follow, clap, highlight and comment where it fits on your content. So leave your thoughts, questions, or success stories too! I love to read them!\n\n*Connect* on [YouTube](https://www.youtube.com/channel/UCphdP_nguu6MT3U5tsJNMsQ), [X (twitter)](https://x.com/Aibusinessplans/status/1803488217079095460), and [Linkedin](https://www.linkedin.com/company/ai-businessplans/) — Try our [Community](https://whop.com/ai-businessplans).\n\nBe safe and make small steps forward every day.\n\nDoug\n\n\n## Read Next \\-\n\n🛆 *Investment Disclaimer:* You should not invest money into a paid tool until you have maximized the benefits of the free features. *Nothing in our training products is a promise or guarantee of earnings.*👈\n\n☄ This article contains referral links for some of my absolute favorite AI business tools for content creators and GenAI enthusiasts.\n\nIf you purchase one of my favorite software and AI tools, I will receive a small commission at no additional charge to you.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/choosing-between-llm-agent-frameworks-69019493b259","frontmatter":{"title":"Choosing Between LLM Agent Frameworks","meta_title":"Choosing Between LLM Agent Frameworks","description":"The tradeoffs between building bespoke code-based agents and the major agent frameworks.","date":"2024-10-29T12:57:34.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*jRMs19HqSCazE5dY","categories":["Programming","Technology","Machine Learning"],"author":"Rifx.Online","tags":["agents","frameworks","LangGraph","LlamaIndex","Workflows"],"draft":false,"slug":"blog/choosing-between-llm-agent-frameworks-69019493b259"},"content":"\n### The tradeoffs between building bespoke code\\-based agents and the major agent frameworks.\n\n\n\n\nAgents are having a moment. With multiple new frameworks and fresh [investment](https://foundationcapital.com/goodbye-aiops-welcome-agentsres-the-next-100b-opportunity/) in the space, modern AI agents are overcoming [shaky origins](https://arxiv.org/html/2405.13966v1) to rapidly supplant RAG as an implementation priority. So will 2024 finally be the year that autonomous AI systems that can take over writing our emails, booking flights, talking to our data, or seemingly any other task?\n\nMaybe, but much work remains to get to that point. Any developer building an agent must not only choose foundations — which model, use case, and architecture to use — but also which framework to leverage. Do you go with the long\\-standing LangGraph, or the newer entrant LlamaIndex Workflows? Or do you go the traditional route and code the whole thing yourself?\n\nThis post aims to make that choice a bit easier. Over the past few weeks, I built the same agent in major frameworks to examine some of the strengths and weaknesses of each at a technical level. All of the code for each agent is available in [this repo](https://github.com/Arize-ai/phoenix/tree/main/examples/agent_framework_comparison).\n\n### Background on the Agent Used for Testing\n\nThe agent used for testing includes function calling, multiple tools or skills, connections to outside resources, and shared state or memory.\n\nThe agent has the following capabilities:\n\n1. Answering questions from a knowledge base\n2. Talking to data: answering questions about telemetry data of an LLM application\n3. Analyzing data: analyzing higher\\-level trends and patterns in retrieved telemetry data\n\nIn order to accomplish these, the agent has three starting skills: RAG with product documentation, SQL generation on a trace database, and data analysis. A simple gradio\\-powered interface is used for the agent UI, with the agent itself structured as a chatbot.\n\n## Code\\-Based Agent (No Framework)\n\nThe first option you have when developing an agent is to skip the frameworks entirely and build the agent fully yourself. When embarking on this project, this was the approach I started with.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*pw9-0lB5JMlVcPqo)\n\n### Pure Code Architecture\n\nThe code\\-based agent below is made up of an OpenAI\\-powered router that uses function calling to select the right skill to use. After that skill completes, it returns back to the router to either call another skill or respond to the user.\n\nThe agent keeps an ongoing list of messages and responses that is passed fully into the router on each call to preserve context through cycles.\n\n```python\ndef router(messages):\n    if not any(\n        isinstance(message, dict) and message.get(\"role\") == \"system\" for message in messages\n    ):\n        system_prompt = {\"role\": \"system\", \"content\": SYSTEM_PROMPT}\n        messages.append(system_prompt)\n\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=messages,\n        tools=skill_map.get_combined_function_description_for_openai(),\n    )\n\n    messages.append(response.choices[0].message)\n    tool_calls = response.choices[0].message.tool_calls\n    if tool_calls:\n        handle_tool_calls(tool_calls, messages)\n        return router(messages)\n    else:\n        return response.choices[0].message.content\n```\n\nThe skills themselves are defined in their own classes (e.g. GenerateSQLQuery) that are collectively held in a SkillMap. The router itself only interacts with the SkillMap, which it uses to load skill names, descriptions, and callable functions. This approach means that adding a new skill to the agent is as simple as writing that skill as its own class, then adding it to the list of skills in the SkillMap. The idea here is to make it easy to add new skills without disturbing the router code.\n\n```python\nclass SkillMap:\n    def __init__(self):\n        skills = [AnalyzeData(), GenerateSQLQuery()]\n\n        self.skill_map = {}\n        for skill in skills:\n            self.skill_map[skill.get_function_name()] = (\n                skill.get_function_dict(),\n                skill.get_function_callable(),\n            )\n\n    def get_function_callable_by_name(self, skill_name) -> Callable:\n        return self.skill_map[skill_name][1]\n\n    def get_combined_function_description_for_openai(self):\n        combined_dict = []\n        for _, (function_dict, _) in self.skill_map.items():\n            combined_dict.append(function_dict)\n        return combined_dict\n\n    def get_function_list(self):\n        return list(self.skill_map.keys())\n\n    def get_list_of_function_callables(self):\n        return [skill[1] for skill in self.skill_map.values()]\n\n    def get_function_description_by_name(self, skill_name):\n        return str(self.skill_map[skill_name][0][\"function\"])\n```\n\nOverall, this approach is fairly straightforward to implement but comes with a few challenges.\n\n### Challenges with Pure Code Agents\n\nThe first difficulty lies in structuring the router system prompt. Often, the router in the example above insisted on generating SQL itself instead of delegating that to the right skill. If you’ve ever tried to get an LLM *not* to do something, you know how frustrating that experience can be; finding a working prompt took many rounds of debugging. Accounting for the different output formats from each step was also tricky. Since I opted not to use structured outputs, I had to be ready for multiple different formats from each of the LLM calls in my router and skills.\n\n### Benefits of a Pure Code Agent\n\nA code\\-based approach provides a good baseline and starting point, offering a great way to learn how agents work without relying on canned agent tutorials from prevailing frameworks. Although convincing the LLM to behave can be challenging, the code structure itself is simple enough to use and might make sense for certain use cases (more in the analysis section below).\n\n## LangGraph\n\nLangGraph is one of the longest\\-standing agent frameworks, first releasing in January 2024\\. The framework is built to address the acyclic nature of existing pipelines and chains by adopting a Pregel graph structure instead. LangGraph makes it easier to define loops in your agent by adding the concepts of nodes, edges, and conditional edges to traverse a graph. LangGraph is built on top of LangChain, and uses the objects and types from that framework.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*fYgHiGwLhSUSrFv9)\n\n### LangGraph Architecture\n\nThe LangGraph agent looks similar to the code\\-based agent on paper, but the code behind it is drastically different. LangGraph still uses a “router” technically, in that it calls OpenAI with functions and uses the response to continue to a new step. However the way the program moves between skills is controlled completely differently.\n\n```python\ntools = [generate_and_run_sql_query, data_analyzer]\nmodel = ChatOpenAI(model=\"gpt-4o\", temperature=0).bind_tools(tools)\n\ndef create_agent_graph():\n    workflow = StateGraph(MessagesState)\n\n    tool_node = ToolNode(tools)\n    workflow.add_node(\"agent\", call_model)\n    workflow.add_node(\"tools\", tool_node)\n\n    workflow.add_edge(START, \"agent\")\n    workflow.add_conditional_edges(\n        \"agent\",\n        should_continue,\n    )\n    workflow.add_edge(\"tools\", \"agent\")\n\n    checkpointer = MemorySaver()\n    app = workflow.compile(checkpointer=checkpointer)\n    return app\n```\n\nThe graph defined here has a node for the initial OpenAI call, called “agent” above, and one for the tool handling step, called “tools.” LangGraph has a built\\-in object called ToolNode that takes a list of callable tools and triggers them based on a ChatMessage response, before returning to the “agent” node again.\n\n```python\ndef should_continue(state: MessagesState):\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    if last_message.tool_calls:\n        return \"tools\"\n    return END\n\ndef call_model(state: MessagesState):\n    messages = state[\"messages\"]\n    response = model.invoke(messages)\n    return {\"messages\": [response]}\n```\n\nAfter each call of the “agent” node (put another way: the router in the code\\-based agent), the should\\_continue edge decides whether to return the response to the user or pass on to the ToolNode to handle tool calls.\n\nThroughout each node, the “state” stores the list of messages and responses from OpenAI, similar to the code\\-based agent’s approach.\n\n### Challenges with LangGraph\n\nMost of the difficulties with LangGraph in the example stem from the need to use Langchain objects for things to flow nicely.\n\n**Challenge \\#1: Function Call Validation**\n\nIn order to use the ToolNode object, I had to refactor most of my existing Skill code. The ToolNode takes a list of callable functions, which originally made me think I could use my existing functions, however things broke down due to my function parameters.\n\nThe skills were defined as classes with a callable member function, meaning they had “self” as their first parameter. GPT\\-4o was smart enough to not include the “self” parameter in the generated function call, however LangGraph read this as a validation error due to a missing parameter.\n\nThis took hours to figure out, because the error message instead marked the third parameter in the function (“args” on the data analysis skill) as the missing parameter:\n\n```python\npydantic.v1.error_wrappers.ValidationError: 1 validation error for data_analysis_toolSchema\nargs field required (type=value_error.missing)\n```\n\nIt is worth mentioning that the error message originated from Pydantic, not from LangGraph.\n\nI eventually bit the bullet and redefined my skills as basic methods with Langchain’s @tool decorator, and was able to get things working.\n\n```python\n@tool\ndef generate_and_run_sql_query(query: str):\n    \"\"\"Generates and runs an SQL query based on the prompt.\n\n    Args:\n        query (str): A string containing the original user prompt.\n\n    Returns:\n        str: The result of the SQL query.\n    \"\"\"\n```\n\n**Challenge \\#2: Debugging**\n\nAs mentioned, debugging in a framework is difficult. This primarily comes down to confusing error messages and abstracted concepts that make it harder to view variables.\n\nThe abstracted concepts primarily show up when trying to debug the messages being sent around the agent. LangGraph stores these messages in state\\[“messages”]. Some nodes within the graph pull from these messages automatically, which can make it difficult to understand the value of messages when they are accessed by the node.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*KuCg0WGHSklOKe6t)\n\n### LangGraph Benefits\n\nOne of the main benefits of LangGraph is that it’s easy to work with. The graph structure code is clean and accessible. Especially if you have complex node logic, having a single view of the graph makes it easier to understand how the agent is connected together. LangGraph also makes it straightforward to convert an existing application built in LangChain.\n\n### Takeaway\n\nIf you use everything in the framework, LangGraph works cleanly; if you step outside of it, prepare for some debugging headaches.\n\n## LlamaIndex Workflows\n\nWorkflows is a newer entrant into the agent framework space, premiering earlier this summer. Like LangGraph, it aims to make looping agents easier to build. Workflows also has a particular focus on running asynchronously.\n\nSome elements of Workflows seem to be in direct response to LangGraph, specifically its use of events instead of edges and conditional edges. Workflows use steps (analogous to nodes in LangGraph) to house logic, and emitted and received events to move between steps.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*22WuFVBWctdeiCSL)\n\nThe structure above looks similar to the LangGraph structure, save for one addition. I added a setup step to the Workflow to prepare the agent context, more on this below. Despite the similar structure, there is very different code powering it.\n\n### Workflows Architecture\n\nThe code below defines the Workflow structure. Similar to LangGraph, this is where I prepared the state and attached the skills to the LLM object.\n\n```python\nclass AgentFlow(Workflow):\n    def __init__(self, llm, timeout=300):\n        super().__init__(timeout=timeout)\n        self.llm = llm\n        self.memory = ChatMemoryBuffer(token_limit=1000).from_defaults(llm=llm)\n        self.tools = []\n        for func in skill_map.get_function_list():\n            self.tools.append(\n                FunctionTool(\n                    skill_map.get_function_callable_by_name(func),\n                    metadata=ToolMetadata(\n                        name=func, description=skill_map.get_function_description_by_name(func)\n                    ),\n                )\n            )\n\n    @step\n    async def prepare_agent(self, ev: StartEvent) -> RouterInputEvent:\n        user_input = ev.input\n        user_msg = ChatMessage(role=\"user\", content=user_input)\n        self.memory.put(user_msg)\n\n        chat_history = self.memory.get()\n        return RouterInputEvent(input=chat_history)\n```\n\nThis is also where I define an extra step, “prepare\\_agent”. This step creates a ChatMessage from the user input and adds it to the workflow memory. Splitting this out as a separate step means that we do return to it as the agent loops through steps, which avoids repeatedly adding the user message to the memory.\n\nIn the LangGraph case, I accomplished the same thing with a run\\_agent method that lived outside the graph. This change is mostly stylistic, however it’s cleaner in my opinion to house this logic with the Workflow and graph as we’ve done here.\n\nWith the Workflow set up, I then defined the routing code:\n\n```python\n@step\nasync def router(self, ev: RouterInputEvent) -> ToolCallEvent | StopEvent:\n    messages = ev.input\n\n    if not any(\n        isinstance(message, dict) and message.get(\"role\") == \"system\" for message in messages\n    ):\n        system_prompt = ChatMessage(role=\"system\", content=SYSTEM_PROMPT)\n        messages.insert(0, system_prompt)\n\n    with using_prompt_template(template=SYSTEM_PROMPT, version=\"v0.1\"):\n        response = await self.llm.achat_with_tools(\n            model=\"gpt-4o\",\n            messages=messages,\n            tools=self.tools,\n        )\n\n    self.memory.put(response.message)\n\n    tool_calls = self.llm.get_tool_calls_from_response(response, error_on_no_tool_call=False)\n    if tool_calls:\n        return ToolCallEvent(tool_calls=tool_calls)\n    else:\n        return StopEvent(result=response.message.content)\n```\n\nAnd the tool call handling code:\n\n```python\n@step\nasync def tool_call_handler(self, ev: ToolCallEvent) -> RouterInputEvent:\n    tool_calls = ev.tool_calls\n\n    for tool_call in tool_calls:\n        function_name = tool_call.tool_name\n        arguments = tool_call.tool_kwargs\n        if \"input\" in arguments:\n            arguments[\"prompt\"] = arguments.pop(\"input\")\n\n        try:\n            function_callable = skill_map.get_function_callable_by_name(function_name)\n        except KeyError:\n            function_result = \"Error: Unknown function call\"\n\n        function_result = function_callable(arguments)\n        message = ChatMessage(\n            role=\"tool\",\n            content=function_result,\n            additional_kwargs={\"tool_call_id\": tool_call.tool_id},\n        )\n\n        self.memory.put(message)\n\n    return RouterInputEvent(input=self.memory.get())\n```\n\nBoth of these look more similar to the code\\-based agent than the LangGraph agent. This is mainly because Workflows keeps the conditional routing logic in the steps as opposed to in conditional edges — lines 18–24 were a conditional edge in LangGraph, whereas now they are just part of the routing step — and the fact that LangGraph has a ToolNode object that does just about everything in the tool\\_call\\_handler method automatically.\n\nMoving past the routing step, one thing I was very happy to see is that I could use my SkillMap and existing skills from my code\\-based agent with Workflows. These required no changes to work with Workflows, which made my life much easier.\n\n### Challenges with Workflows\n\n**Challenge \\#1: Sync vs Async**\n\nWhile asynchronous execution is preferable for a live agent, debugging a synchronous agent is much easier. Workflows is designed to work asynchronously, and trying to force synchronous execution was very difficult.\n\nI initially thought I would just be able to remove the “async” method designations and switch from “achat\\_with\\_tools” to “chat\\_with\\_tools”. However, since the underlying methods within the Workflow class were also marked as asynchronous, it was necessary to redefine those in order to run synchronously. I ended up sticking to an asynchronous approach, but this didn’t make debugging more difficult.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*78Hzqkiv9cI7W4UA)\n\n**Challenge \\#2: Pydantic Validation Errors**\n\nIn a repeat of the woes with LangGraph, similar problems emerged around confusing Pydantic validation errors on skills. Fortunately, these were easier to address this time since Workflows was able to handle member functions just fine. I ultimately just ended up having to be more prescriptive in creating LlamaIndex FunctionTool objects for my skills:\n\n```python\nfor func in skill_map.get_function_list(): \n            self.tools.append(FunctionTool(\n                skill_map.get_function_callable_by_name(func), \n                metadata=ToolMetadata(name=func, description=skill_map.get_function_description_by_name(func))))\n```\n\n*Excerpt from AgentFlow.\\_\\_init\\_\\_ that builds FunctionTools*\n\n### Benefits of Workflows\n\nI had a much easier time building the Workflows agent than I did the LangGraph agent, mainly because Workflows still required me to write routing logic and tool handling code myself instead of providing built\\-in functions. This also meant that my Workflow agent looked extremely similar to my code\\-based agent.\n\nThe biggest difference came in the use of events. I used two custom events to move between steps in my agent:\n\n```python\nclass ToolCallEvent(Event):\n    tool_calls: list[ToolSelection]\n\nclass RouterInputEvent(Event):\n    input: list[ChatMessage]\n```\n\nThe emitter\\-receiver, event\\-based architecture took the place of directly calling some of the methods in my agent, like the tool call handler.\n\nIf you have more complex systems with multiple steps that are triggering asynchronously and might emit multiple events, this architecture becomes very helpful to manage that cleanly.\n\nOther benefits of Workflows include the fact that it is very lightweight and doesn’t force much structure on you (aside from the use of certain LlamaIndex objects) and that its event\\-based architecture provides a helpful alternative to direct function calling — especially for complex, asynchronous applications.\n\n## Comparing Frameworks\n\nLooking across the three approaches, each one has its benefits.\n\nThe no framework approach is the simplest to implement. Because any abstractions are defined by the developer (i.e. SkillMap object in the above example), keeping various types and objects straight is easy. The readability and accessibility of the code entirely comes down to the individual developer however, and it’s easy to see how increasingly complex agents could get messy without some enforced structure.\n\nLangGraph provides quite a bit of structure, which makes the agent very clearly defined. If a broader team is collaborating on an agent, this structure would provide a helpful way of enforcing an architecture. LangGraph also might provide a good starting point with agents for those not as familiar with the structure. There is a tradeoff, however — since LangGraph does quite a bit for you, it can lead to headaches if you don’t fully buy into the framework; the code may be very clean, but you may pay for it with more debugging.\n\nWorkflows falls somewhere in the middle. The event\\-based architecture might be extremely helpful for some projects, and the fact that less is required in terms of using of LlamaIndex types provides greater flexibility for those not be fully using the framework across their application.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*PITmiVGuG8QuDVX6)\n\nUltimately, the core question may just come down to “are you already using LlamaIndex or LangChain to orchestrate your application?” LangGraph and Workflows are both so entwined with their respective underlying frameworks that the additional benefits of each agent\\-specific framework might not cause you to switch on merit alone.\n\nThe pure code approach will likely always be an attractive option. If you have the rigor to document and enforce any abstractions created, then ensuring nothing in an external framework slows you down is easy.\n\n## Key Questions To Help In Choosing An Agent Framework\n\nOf course, “it depends” is never a satisfying answer. These three questions should help you decide which framework to use in your next agent project.\n\n***Are you already using LlamaIndex or LangChain for significant pieces of your project?***\n\nIf yes, explore that option first.\n\n***Are you familiar with common agent structures, or do you want something telling you how you should structure your agent?***\n\nIf you fall into the latter group, try Workflows. If you *really* fall into the latter group, try LangGraph.\n\n***Has your agent been built before?***\n\nOne of the framework benefits is that there are many tutorials and examples built with each. There are far fewer examples of pure code agents to build from.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*wF9aSF1db1yaniqO)\n\n## Conclusion\n\nPicking an agent framework is just one choice among many that will impact outcomes in production for generative AI systems. As always, it pays to have robust guardrails and [LLM tracing](https://docs.arize.com/phoenix/tracing/llm-traces) in place — and to be agile as new agent frameworks, research, and models upend established techniques.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/claude-3-5-haiku-anthropics-speed-demon-gets-a-brain-boost-82f2f0999d4f","frontmatter":{"title":"Claude 3.5 Haiku: Anthropic’s Speed Demon Gets a Brain Boost","meta_title":"Claude 3.5 Haiku: Anthropic’s Speed Demon Gets a Brain Boost","description":"Claude 3.5 Haiku, Anthropic's latest AI model, combines speed and intelligence, outperforming its predecessor, Claude 3 Opus, on various benchmarks. It excels in coding tasks, achieving a notable 40.6% on the SWE-bench Verified test. While it remains text-only for now, plans for image analysis are in the works. The model is accessible via multiple platforms but comes at a premium cost, four times that of its predecessor, though cost-saving options like prompt caching exist. Its applications span software development, chatbots, data processing, education, and more, marking a significant advancement in AI capabilities.","date":"2024-11-13T01:32:04.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*hLedIfhYJhS_ejPDwOQPIw.png","categories":["Programming","Machine Learning","Chatbots"],"author":"Rifx.Online","tags":["Claude","Haiku","coding","SWE-bench","benchmarks"],"draft":false,"slug":"blog/claude-3-5-haiku-anthropics-speed-demon-gets-a-brain-boost-82f2f0999d4f"},"content":"\n\n\n\n\n\nIn the relentless race of AI advancement, Anthropic has just dropped a new contender into the ring. Meet Claude 3\\.5 Haiku, the latest iteration of their fastest AI model. It’s like they’ve taken their sprinter and sent them to brain camp. The result? A model that’s not just quick on its feet but can now outsmart its beefier siblings in certain intellectual arenas. Let’s dive into what makes this new kid on the block tick.\n\n\n## The Need for Speed (and Smarts)\n\nAnthropic’s previous Haiku model was already the Usain Bolt of their AI lineup. Now, they’ve somehow managed to cram more brainpower into this speed demon without sacrificing its swiftness. It’s like watching a cheetah solve a Rubik’s cube while sprinting.\n\n\n## Benchmarking Brilliance\n\nClaude 3\\.5 Haiku isn’t just fast; it’s scary smart. It’s outperforming Claude 3 Opus — Anthropic’s previous heavyweight champ — on various intelligence benchmarks. This isn’t just a minor upgrade; it’s a leap that has the AI community sitting up and taking notice.\n\n\n## Coding Prowess\n\nIf you’re a developer, listen up. This model is flexing hard in the coding arena, scoring a jaw\\-dropping 40\\.6% on the SWE\\-bench Verified test. That’s not just impressive; it’s the kind of performance that makes human coders nervously eye their job security.\n\n\n## Under the Hood\n\nLet’s pop the hood and see what’s powering this AI hot rod:\n\n* **Availability**: You can take it for a spin through Anthropic’s API, Amazon Bedrock, or Google Cloud’s Vertex AI. It’s like the AI equivalent of being available on all major streaming platforms.\n* **Input**: Currently, it’s text\\-only. No image analysis yet, but that’s coming. It’s like having a genius pen pal who can’t look at your vacation photos.\n* **Knowledge Cutoff**: July 2024\\. So it knows about that embarrassing thing you did last summer, but not about next year’s memes.\n* **Output Length**: Improved from its predecessor. It can now write longer essays to procrastinate on your behalf.\n\n\n## Show Me the Money\n\nNow, here’s where things get interesting. Anthropic has decided to charge a premium for this upgraded model:\n\n* $1 per million input tokens\n* $5 per million output tokens\n\nThat’s a fourfold increase from the previous version. It’s like they’ve taken their Honda Civic, turned it into a Tesla, and adjusted the price accordingly.\n\nBut fear not, penny\\-pinchers! There are ways to save:\n\n* Prompt caching can save you up to 90%. It’s like extreme couponing for AI.\n* Batch processing with the Message Batches API can cut costs by up to 50%. Bulk buying, but for computation.\n\n\n## What Can This Thing Do?\n\nClaude 3\\.5 Haiku isn’t just a party trick. It’s got some serious real\\-world applications:\n\n* **Software Development**: It’s like having a coding buddy who never sleeps and doesn’t steal your snacks.\n* **Chatbots**: Customer service reps who don’t need coffee breaks or HR interventions.\n* **Data Processing**: It can crunch numbers faster than you can say “big data.”\n* **Education**: A tutor that’s always on call and never loses patience.\n* **Personalization**: It remembers your preferences better than your significant other.\n* **Specialized Tasks**: The Swiss Army knife of AI sub\\-agents.\n* **Content Moderation**: Keeping the internet clean, one post at a time.\n\n\n## The Trade\\-Offs\n\nNow, it’s not all sunshine and rainbows. There are a few catches:\n\n* No image analysis yet. So it can’t tell you if that dress makes you look fat.\n* The price hike might make some users stick with the older, cheaper version. It’s the AI equivalent of people still using Windows 7\\.\n\n\n## The Bottom Line\n\nClaude 3\\.5 Haiku is a significant leap forward in the world of AI. It’s faster than a speeding bullet, more powerful than a locomotive, and able to leap tall buildings in a single bound. Okay, maybe not that last part, but you get the idea.\n\nFor developers and businesses looking to leverage AI for complex tasks that require both brains and brawn (or in this case, speed), Claude 3\\.5 Haiku is a compelling option. It’s not just an upgrade; it’s a reimagining of what’s possible at the intersection of speed and intelligence in AI.\n\nThe question now is: how will competitors respond? And more importantly, how long until we see Claude 4\\.0: The Limerick Edition?\n\n\n## FAQ Section\n\n**Q: Can Claude 3\\.5 Haiku analyze images?**A: Not yet, but Anthropic plans to add this feature in the future. For now, it’s text\\-only.\n\n**Q: How much more expensive is Claude 3\\.5 Haiku compared to its predecessor?**A: It’s four times more expensive, but there are ways to reduce costs through prompt caching and batch processing.\n\n**Q: What’s the most impressive feature of Claude 3\\.5 Haiku?**A: Its ability to outperform larger models like Claude 3 Opus on various intelligence benchmarks while maintaining high speed.\n\n**Q: Can I use Claude 3\\.5 Haiku for software development?**A: Absolutely. It excels at coding tasks and can provide fast, accurate code suggestions and completions.\n\n**Q: Is Claude 3\\.5 Haiku available to the public?**A: Yes, it’s accessible through Anthropic’s API, Amazon Bedrock, and Google Cloud’s Vertex AI.\n\n\\#Claude35Haiku \\#AnthropicAI \\#AIInnovation \\#MachineLearning \\#AIForDevelopers \\#FutureOfAI \\#AIPerformance \\#TechInnovation\n\n“Claude 3\\.5 Haiku performance benchmarks”, “AI model pricing comparison”, “Fast AI models for software development”, “Anthropic AI model capabilities”, “Cost\\-effective AI implementation strategies”\n\n\n"},{"lang":"en","group":"blog","slug":"blog/claude-3-5-sonnet-new-pioneering-the-future-of-ai-with-computer-control-capabilities-37a6ff9f9033","frontmatter":{"title":"Claude 3.5 Sonnet (New): Pioneering the Future of AI with Computer Control Capabilities","meta_title":"Claude 3.5 Sonnet (New): Pioneering the Future of AI with Computer Control Capabilities","description":"Anthropic has unveiled its latest AI model, Claude 3.5 Sonnet, on October 22, 2024. This release introduces revolutionary computer control…","date":"2024-10-27T13:57:00.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*n0NkOFbhUm7_fllJ","categories":["Programming","Technology","Generative AI"],"author":"Rifx.Online","tags":["Claude","Sonnet","automation","benchmarks","safety"],"draft":false,"slug":"blog/claude-3-5-sonnet-new-pioneering-the-future-of-ai-with-computer-control-capabilities-37a6ff9f9033"},"content":"\n\n\n\n\n\nAnthropic has unveiled its latest AI model, Claude 3.5 Sonnet, on October 22, 2024. This release introduces revolutionary computer control capabilities and substantial improvements across various benchmarks, setting new standards in the AI industry.\n\n\n## Revolutionary Computer Control: A New Frontier\n\nThe standout feature of Claude 3.5 Sonnet is its ability to interact with computers just like humans do. This groundbreaking capability allows the AI to:\n\n* Navigate desktop interfaces using mouse and keyboard inputs\n* Interact with various applications and web browsers\n* Execute complex multi-step tasks\n* Perform file management operations\n* Automate repetitive workflows\n\nThis computer control feature, currently in public beta, represents a paradigm shift in how AI systems can interact with digital interfaces. While still in its experimental phase, early testing shows promising results, with Claude 3.5 Sonnet scoring 14.9% on the OSWorld benchmark for screenshot-only tasks — significantly higher than the next-best system’s 7.8%.\n\n\n## Benchmark-Breaking Performance\n\nThe upgraded model demonstrates remarkable improvements across various metrics:\n\n\n## Coding and Technical Tasks\n\n* 49% performance on SWE-bench Verified (up from 33.4%)\n* 93.7% score on HumanEval coding tasks\n* Superior performance in software engineering compared to specialized coding systems\n\n\n## Academic and Reasoning Capabilities\n\n* 65% on graduate-level reasoning (GPQA-Diamond)\n* 78% on undergraduate-level knowledge (MMLU Pro)\n* 78.3% on mathematical problem-solving (MATH)\n\n\n## Business Applications\n\n* 69.2% on retail domain tasks (TAU-bench)\n* 46% on airline domain tasks\n* 90.8% accuracy on chart analysis\n* 94.2% accuracy on document Q&A\n\n\n## Enterprise Integration and Availability\n\nClaude 3.5 Sonnet is accessible through multiple platforms:\n\n* Anthropic API\n* Amazon Bedrock\n* Google Cloud’s Vertex AI\n\nMajor companies including Asana, Canva, DoorDash, and Replit have already begun implementing Claude 3.5 Sonnet’s capabilities in their workflows, particularly leveraging its computer control features for complex automation tasks.\n\n\n## Practical Applications\n\n\n## Software Development\n\n* Automated code testing and debugging\n* Intelligent IDE interactions\n* Code review and optimization\n* Documentation generation\n\n\n## Customer Support\n\n* Advanced chatbot capabilities\n* Visual data interpretation\n* Automated ticket resolution\n* Process automation\n\n\n## Business Operations\n\n* Document processing and analysis\n* Data extraction from visual sources\n* Workflow automation\n* Complex problem-solving\n\n\n## Safety and Responsibility\n\nAnthropic has implemented robust safety measures for the computer control feature:\n\n* New classifiers to identify potential misuse\n* Proactive monitoring systems\n* Restricted access to sensitive operations\n* Regular safety assessments\n\n\n## Looking Ahead\n\nWhile Claude 3.5 Sonnet represents a significant advancement in AI capabilities, it’s important to note that some features, particularly computer control, are still in their early stages. Certain actions like scrolling, dragging, and zooming present challenges, and Anthropic encourages developers to begin with low-risk tasks while exploring these new capabilities.\n\nThe release of Claude 3.5 Sonnet marks a pivotal moment in AI development, combining advanced reasoning capabilities with practical computer control features. As the technology continues to evolve, we can expect to see even more innovative applications and improvements in how AI systems interact with our digital world.\n\n*This article is based on official announcements and documentation from Anthropic, AWS, and various technology partners. For the most up-to-date information, please refer to Anthropic’s official documentation.*\n\n\n"},{"lang":"en","group":"blog","slug":"blog/claude-3-5-sonnet-v-s-gpt-4o-which-one-is-better-3b3675195bf9","frontmatter":{"title":"Claude 3.5 Sonnet V/S GPT-4O: Which one is better","meta_title":"Claude 3.5 Sonnet V/S GPT-4O: Which one is better","description":"In November 2022, OpenAI launched ChatGPT, a model that has revolutionized how we search and interact with information. Next year, in…","date":"2024-10-27T13:59:09.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*4MXLuSFfGwFkWWn0","categories":["Generative AI","Machine Learning","Natural Language Processing"],"author":"Rifx.Online","tags":["GPT-4o","Claude","multimodal","reasoning","code-generation"],"draft":false,"slug":"blog/claude-3-5-sonnet-v-s-gpt-4o-which-one-is-better-3b3675195bf9"},"content":"\n\n\n\nIn November 2022, OpenAI launched ChatGPT, a model that has revolutionized how we search and interact with information. Next year, in March, an American startup,” Anthropic,” founded by ex-OpenAI employees, launched their own AI model, “Claude.” Since the launch, both AI companies have been competing to bring the best to customers regarding features and experience through their AI models. Recently, OpenAI launched “GPT-4o,” a spectacular model that handles file, voice, and video data amazingly. Similarly, Claude launched the “Claude 3.5 Sonnet,” which is the most advanced AI model, as they claimed, and can handle complex tasks. In this article, we will determine which is better, between Claude 3.5 Sonnet and GPT-4o, and compare its features and output with the same input to check which is better for you.\n\n\n## Capabilities and Features\n\n\n### GPT-4o\n\n\n\nGPT-4o is the latest LLM launched by OpenAI. The “o” stands for omni, which means “every” in Latin. This model can analyze voice, images, videos, and files as input and respond accordingly. It can take voice input and give the output in different characters’ voices, including tones, emotions, etc. The whole process is as low as a human conversation, with an average of 0.32 seconds compared to other voice models, which is 2.8 seconds. It also allows users to generate written content such as articles, blogs, product descriptions, code in different programming languages, data analysis, charts, etc. In addition, GPT-4o can also analyze images and videos, which makes the model act as a language translator, personal assistant, virtual teacher, or shopping assistant. It can also be used in medicine, engineering, the military, etc. To use this feature, GPT-4o can use the user’s camera to get a real-time view and respond accordingly in the voice mode. It can also access your computer screen and describe what is shown on the screen, users can ask questions related to the stuff displayed on the screen.\n\n*For example, users can enable the model on the screen, open the VS code, and prompt the model to act as a coding assistant to get answers to the coding problems. Alternatively, you can enable the camera to act as a fitness trainer whether you are doing it correctly or not.*\n\nThe model has unique features, such as data analysis, code interpreter, and real-time web browsing, making it different from its competitors. The model also has a plethora of GPTs, which is a tailored version of ChatGPT.\n\n\n### Claude 3.5 Sonnet\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*BSMcOpvWZ5lUm4Tl)\n\nClaude 3.5 Sonnet is the AI chatbot launched by Anthropic. It is the third generation of the family of Claude AI model series. This model has stood at a high bae and outperformed many AI models on various evaluations, keeping the hallucinations and wrong information away. While it doesn’t support voice and video features like GPT-4o, it can also perform all the basic tasks, such as text generation and code generation in different programming languages, brainstorming ideas, etc. According to the report by Anthropic, Claude 3.5 Sonnet is one of the best computer vision models in the market, which can be used to analyze charts and graphs, transcribe texts from images, and many more. Claude is powered by an advanced feature, “Artifacts,” a special popup window along the conversation, allowing the users to check the code snippets, text documents, or website designs and allow them to edit the output in real-time.\n\n*For example, users can use computer vision and artifacts in their workflows. Users can make essential prototyping of a website’s design on paper, attach the file with Claude 3.5 Sonnet, and prompt it to design a website based on the prototype. The generated code and the website design appear in the artifacts. Users can edit the code and the design according to their requirements. Users can also publish their projects live on the Internet.*\n\n\n## Head-to-Head Comparison\n\nIn this section, we will compare the two LLMs based on factors such as complex reasoning and code generation, check out their capabilities in handling complex tasks, and see which model is best.\n\n* **Graduate Level Reasoning(GPQA, Diamond)**This factor evaluates the models’ ability to handle complex, high-level reasoning tasks at a graduate level of education. In this task, researchers compare the model on the GPQA test, a set of 448 questions in different fields designed by experts. These questions are Google Proof, so anyone can’t find them online. The Claude score is nearly 59.4%, while the GPT-4o scores only 53.6%. Both the scores are relatively close, but as we can see, Claude could be a better option in tasks that require advanced analytical thinking, such as research analysis, complex problem solving, and high academic level problems.\n* **Undergraduate level knowlege(MMLU)**The MMLU, which means Massive Multitask Language Understanding, is a benchmark that explains the general knowledge understanding of any AI model across various subjects at an undergraduate level. Claude 3.5 Sonnet scores 88.3% in this experiment, and the GPT-4o scores 88.7%. This shows how both LLMs have trained in various domains and have a deeper understanding of them. It makes the AI model a well-suited tool for general knowledge tasks, basic tutoring of multiple subjects, etc.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*A4w-tvsxcmFINaQT)\n\n* **Code(HumanEval)**HumanEval is a benchmark that evaluates the model’s ability to generate, understand, and debug code. This benchmark is where Claude 3.5 Sonnet achieves 92%, and GPT-4o scores 90.2%. Claude 3.5 Sonnet results are spectacular in this task as it provides a better coding environment, “Artifacts,” and better code generation than GPT-4o. Claude allows the users to design, edit, and run the code in the Artifacts pop-up window. After the launch of Claude 3.5 Sonnet, everyone is developing tools, websites, and basic games and sharing them across the internet. On the other hand, GPT-4o also scored well, but it does not have any coding environment in its interface, so the developers must do too much hassle as the code generated by it is too much hassle to get to the result.\n* **Reasoning Over Text(DROP, FLscore)**The DROP(Discrete Reasoning Over Paragraphs) is the benchmark that measures the model’s ability to understand complex text information. In this challenge, the Claude 3.5 Sonnet scores 87.1%, while the GPT-4o scores 83.4%. This shows that the Claude 3.5 Sonnet is better and more effective for the task, which involves detailed text analysis, text review, complex question-answering systems, etc.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Kcy7sFb2FYpbfrwp)\n\n* **Math problem solving(MATH)**This test evaluates the ability of any AI model to solve various mathematical problems. Claude 3.5 Sonnet scores just 71.1%, while the GPT-4o scores 76.6%. These scores make the GPT-4o a better model for mathematical problem-solving tasks and can be used for mathematical computations such as financial modeling, scientific calculations, and advanced data analysis.\n* **Multilingual Maths (MSGM)**This factor describes the ability of any AI model to solve mathematical problems in multiple languages. Both models get scores close to each other: GPT-4o 90.5% and Claude 3.5 Sonnet 91.6%. This shows that both models perform excellently, with Claude slightly better. The capability is particularly helpful for educational applications or any scenario where mathematical reasoning needs to be communicated across language barriers.\n* **Visual question answering(MMU/val)**This factor describes the LLM’s capability to analyze the information presented in images. The GPT-4o outperforms Claude’s 3.5 Sonnet in this benchmark with 69.1% and 68.3%, respectively. On the other hand, when analyzing text from the document, Claude’s 3.5 Sonnet score is 95.2% compared to GPT-4o’s 92.1%.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*xzjqBV2YL0lVFitX)\n\n* **Image Generation**Image Generation is the ability of the LLMs to generate images from the text. GPT-4o is integrated with DallE-2 and can produce images with the help of text, and the results are excellent. On the other hand, Claude 3.5 Sonnet cannot create any images. This feature also helps GPT-4o design websites and references better, as it is trained on many images.\n* **Knowledge Cutoff**Here, both the models trained on a limited data set till a specific date. Claude 3.5 Sonnet trained on data till April 2024, while the other hand, GPT-4o trained on data till 2024. The real advantage of GPT-4o is that it has real-time web browsing, which helps the LLM train on new data regularly.\n\n\n## Pros of GPT-4o:\n\n* Handles voice, images, and video input.\n* Real-time web browsing capability.\n* Faster response time (0.32 seconds average).\n* Superior in math problem-solving.\n* Can generate images using DALL-E 2.\n\n\n## Cons of GPT-4o:\n\n* Slightly lower performance in graduate-level reasoning.\n* No built-in coding environment.\n* A lower score in document visual Q&A.\n* Slightly behind in code generation capabilities.\n* Less effective in detailed text analysis.\n\n\n## Pros Claude 3.5 Sonnet:\n\n* Excels in graduate-level reasoning.\n* Superior code generation and built-in “Artifacts” feature.\n* Better performance in detailed text analysis.\n* A higher score in document visual Q&A.\n* Slightly better in multilingual math.\n\n\n## Cons Claude 3.5 Sonnet:\n\n* Cannot handle voice or video input.\n* No image generation capability.\n* Slightly lower performance in visual question-answering.\n* Cannot access real-time web information.\n* Weaker in math problem-solving.\n\n\n## Conclusion\n\nGPT-4o and Claude 3.5 Sonnet demonstrate impressive capabilities across various tasks, each with its strengths. GPT-4o excels in multimodal inputs, real-time information access, and image generation, making it versatile for diverse applications. Claude 3.5 Sonnet shines in complex reasoning, code generation, and detailed text analysis, offering superior performance in specific academic and professional contexts. The choice between these models depends on the specific use case and required features. We can expect further improvements and specialized models catering to different needs as AI technology advances.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/comparative-study-of-langgraph-autogen-and-crewai-for-building-multi-agent-systems-0e7e47f9078e","frontmatter":{"title":"Comparative Study of LangGraph, Autogen, and Crewai for Building Multi-Agent Systems","meta_title":"Comparative Study of LangGraph, Autogen, and Crewai for Building Multi-Agent Systems","description":"As we venture into the realm of multi-agent systems (MAS), it’s essential to understand the diverse programming languages designed…","date":"2024-11-13T01:22:29.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*DBlLuCOA3lWIg6RmpMPg8A.png","categories":["Programming","Technology","Machine Learning"],"author":"Rifx.Online","tags":["LangGraph","Autogen","Crewai","multi-agent","scalability"],"draft":false,"slug":"blog/comparative-study-of-langgraph-autogen-and-crewai-for-building-multi-agent-systems-0e7e47f9078e"},"content":"\n\n\n\nAs we venture into the realm of multi\\-agent systems (MAS), it’s essential to understand the diverse programming languages designed specifically for this purpose. In this article, we’ll delve into the world of MAS development by comparing LangGraph, Autogen, and Crewai — three prominent players in the field.\n\n\n## Introduction\n\nMulti\\-agent systems (MAS) have become increasingly important in various industries. A MAS is a system composed of multiple intelligent agents that interact with each other and their environment to achieve specific goals. Among the many frameworks available for building MAS, LangGraph, Autogen, and Crewai are some of the most popular choices.\n\nAs developers or researchers working on a MAS project, choosing the right framework can be overwhelming, especially considering factors such as ease of use, scalability, customization, and integration with AI libraries. This article provides a comparative study of LangGraph, Autogen, and Crewai, highlighting their strengths, weaknesses, and suitability for different applications.\n\n\n### Introduction to Each Framework\n\n\n## LangGraph: An Open\\-Source Framework\n\n**Strengths**:\n\n* **Ease of use**: LangGraph provides a simple and intuitive API, making it easy for developers to integrate with their existing systems.\n* **Scalability**: LangGraph supports large\\-scale distributed systems, allowing users to handle complex tasks.\n* **Integration with AI Libraries**: LangGraph is compatible with popular AI libraries such as TensorFlow, PyTorch, and Keras.\n\n**Limitations**:\n\n* Limited support for distributed systems\n* Less flexible than Autogen and Crewai\n\n\n## Autogen: A Modular Open\\-Source Framework\n\n**Strengths**:\n\n* **High flexibility**: Autogen provides a modular architecture, allowing users to customize their MAS to fit specific needs.\n* **Suitability for complex applications**: Autogen’s modularity makes it well\\-suited for large\\-scale systems with multiple interconnected agents.\n* **Strong community support**: Autogen has an active community of developers and researchers who contribute to the framework and provide support.\n\n**Limitations**:\n\n* Steeper learning curve\n* Requires more resources\n\n\n## Crewai: A Scalable, Data\\-Driven Framework\n\n**Strengths**:\n\n* **Scalability**: Crewai provides excellent support for large\\-scale systems, making it well\\-suited for applications that require processing of vast amounts of data.\n* **Ease of use**: Crewai offers a simple API, making it easy to integrate with existing systems.\n* **Integration with cloud services**: Crewai allows users to easily deploy their MAS on cloud platforms such as AWS and Azure.\n\n**Limitations**:\n\n* Limited support for custom models\n* Less flexible than Autogen\n\n\n## Comparison Matrix\n\n\n\n\n## Conclusion\n\nIn conclusion, each framework has its unique strengths and weaknesses. LangGraph offers ease of use and scalability, Autogen provides flexibility and customizability, while Crewai excels in data\\-driven approach and scalability.\n\nWhen choosing a framework for building a MAS, consider the specific requirements of your project:\n\n* **Ease of use**: Choose LangGraph if you prioritize simplicity and scalability.\n* **Flexibility**: Select Autogen for complex applications that require customization.\n* **Scalability**: Consider Crewai for large\\-scale systems with massive data processing needs.\n\nBy understanding the strengths and weaknesses of each framework, developers can make informed decisions about which MAS to build on, ultimately leading to more effective and efficient solutions.\n\n\n## Additional Resources\n\nFor further reading and resources, please see:\n\n* [LangGraph Documentation](https://proxy.rifx.online/https://langgraph.com/documentation/)\n* [Autogen Tutorials](https://proxy.rifx.online/https://autogen.com/tutorials)\n* [Crewai API Reference](https://proxy.rifx.online/https://crewai.com/api-reference/)\n\n"},{"lang":"en","group":"blog","slug":"blog/explore-swarm-multi-agent-framework-locally-0e25ee617795","frontmatter":{"title":"Explore Swarm Multi-Agent Framework Locally","meta_title":"Explore Swarm Multi-Agent Framework Locally","description":"Swarm is an experimental sample framework to simulate lightweight multi-agent framework for educational purpose. Usually it works with Open…","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*0ZVceq32bvkytC7HSIgmwA.png","categories":["Programming","Technology","Education"],"author":"Rifx.Online","tags":["Swarm","Multi-Agent","Framework","OpenAI","Ollama"],"draft":false,"slug":"blog/explore-swarm-multi-agent-framework-locally-0e25ee617795"},"content":"\n\n\n\n\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*zkpW8DDwh0TTYuHJVJbDaw.png)\n\nSwarm is an experimental sample framework to simulate lightweight multi-agent framework for educational purpose. Usually it works with Open AI Key but we can change it to use local Ollama or LM Studio Models.\n\n**Setup:**\n\n\n```python\n## Create a new Conda or Python Virtual Environment and activate it\nconda install python==3.10\npip install torch openai\npip install transformers accelerate huggingface_hub\npip install git+ssh://git@github.com/openai/swarm.git\n```\n**To use with Open AI Key:**\n\n\n```python\nexport OPEN_API_KEY = Your Key\n```\n**To use Ollama or LM Studio Local LLMs — Update to Local URL:**\n\n\n```python\n## Find the location site-packages/swarm on the conda or python virtual env\n## Locate the file core.py\nclass Swarm:\n    def __init__(self, client=None):\n        if not client:\n          # Actual Code\n          #client = OpenAI()\n          # Update the Base URL and API Key to Ollama / LM Studio\n          # In this demo we are using LM Studio and Llama 3.1\n          client = OpenAI(base_url=\"http://localhost:1234/v1\",api_key=\"random\")\n        self.client = client\n```\n**Clone Repo:**\n\nClone the Repo — where you can find examples directory with different use cases like basic, airline and weather etc.\n\n\n```python\ngit clone https://github.com/openai/swarm.git\ncd swarm/examples\n```\n**Sample Code:**\n\n\n```python\nfrom swarm import Swarm, Agent\n\nclient = Swarm()\n\n\nit_agent = Agent(\n    name=\"IT Agent\",\n    instructions=\"You are an IT Expert with 10 Years of Experience.\",\n)\n\nsales_agent = Agent(\n    name=\"Sales Agent\",\n    instructions=\"You are a Sales Expert with 5 Years of Experience and knows about best selling mobiles.\",\n)\n\ndef transfer_to_sales_agent():\n    print(\"Sales agent in action\")\n    \"\"\"Transfer sales related questions to sales team immediately.\"\"\"\n    return sales_agent\n\ndef transfer_to_it_agent():\n    print(\"IT agent in action\")\n    \"\"\"Transfer IT users immediately.\"\"\"\n    return it_agent\n\nenglish_agent = Agent(\n    name=\"English Agent\",\n    instructions=\"You only speak English.\",\n    functions=[transfer_to_sales_agent,transfer_to_it_agent],\n)\n\n\nmessages = [{\"role\": \"user\", \"content\": \"How to install pandas lib?\"}]\nresponse = client.run(agent=english_agent, messages=messages)\n\nprint(response.messages[-1][\"content\"])\n\nmessages = [{\"role\": \"user\", \"content\": \"What are the best selling items?\"}]\nresponse = client.run(agent=english_agent, messages=messages)\n\nprint(response.messages[-1][\"content\"])\n```\n**References:**\n\n\n```python\nhttps://github.com/openai/swarm\n\nhttps://github.com/victorb/ollama-swarm/tree/main\n```\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*hCFJ4VQoT12yElYPXwXvWA.png)\n\nGiven that it is an experimental release, there is still much room for improvement. The airline agent example code [swarm/examples/airline] was interesting, so try those examples. Give it a try and share your experience in the comments. Thanks.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/fine-tuning-llama-3-with-unsloth-79c3465ef3e3","frontmatter":{"title":"Fine-tuning LLama 3 with Unsloth","meta_title":"Fine-tuning LLama 3 with Unsloth","description":"In this article I will show you how to fine-tune an LLM (Llama 3 from Meta) using Unsloth (including a way for custom dataset)","date":"2024-10-30T12:58:41.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kaXoudNTGeGfuNPl_kta5g.jpeg","categories":["Programming","Machine Learning","Natural Language Processing"],"author":"Rifx.Online","tags":["Llama","Unsloth","LoRA","Alpaca","NVIDIA"],"draft":false,"slug":"blog/fine-tuning-llama-3-with-unsloth-79c3465ef3e3"},"content":"\n\n\nIn this article I will show you how to fine\\-tune an LLM (Llama 3 from Meta) using [Unsloth](https://github.com/unslothai/unsloth). I will also provide a way to use your own custom dataset.\n\n**Note :** Unsloth is library that accelerates fine\\-tuning of LLMs on NVIDIA GPUs (40% reduction in memory usage compared to traditional methods). Compatible with Hugging Face, it supports Llama and Mistral architectures.\n\nIf you find my articles interesting, don’t forget to **clap and [follow](https://medium.com/@soulawalid)** 👍🏼, these articles take times and effort to do!\n\nYou can access to the free notebook provided for that on the GitHub repo\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*_L4o4MDQ7W5__OwW0E5RWA.png)\n\nSince I am using Llama 3, I will click on the notebook (you can install Unsloth on your own computer too).\n\n**Note:** I will use this dataset “[alpaca\\-cleaned](https://huggingface.co/datasets/yahma/alpaca-cleaned)” from Hugging Face , the data is in Alpaca format meaning there is (Instruction, Input and Output)\n\n### Starting the project\n\nDuring the project I will guide you to perform fine\\-tuning with Unsloth, explaining the code and provide recommendations, Let’s start our project :\n\n**1/ Installing required packages :** We need first to install **Unsloth** and **xformers**, **trl**, **peft**, **accelerate**, **bitsandbytes** libraries for efficient model training and inference.\n\n```python\n!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n!pip install --no-deps xformers trl peft accelerate bitsandbytes\n```\n\n**2/ Loading and Configuring the Model :** In the configuration I will set the following :\n\n* Sets the maximum sequence length to **2048**\n* by having dtype as **None**, it automatically detects the data type.\n* Loads the model in **4\\-bit precision,** I think it’s enough.\n\n**Note :** You can find my article about tips on fine\\-tuning LLMs in the Resources section\n\n```python\nfrom unsloth import FastLanguageModel\nimport torch\n\n## Configuration\nmax_seq_length = 2048\ndtype = None\nload_in_4bit = True\n\n## Load the selected model\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/llama-3-8b-bnb-4bit\",\n    max_seq_length=max_seq_length,\n    dtype=dtype,\n    load_in_4bit=load_in_4bit,\n)\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*cJSAcJFP7E-qJkqKUsHqLw.png)\n\n**3/ Applying PEFT (Parameter Efficient Fine\\-Tuning) :** We will then fine\\-tunes the pre\\-trained model using LoRA.\n\n* r \\= 16 is the rank parameter for LoRA. **Note :** common values are 8, 16, 32, 64, 128\n* lora\\_alpha \\= 16 represents the scaling factor for LoRA updates ( I will write an article about LoRA to explain in details each part of it)\n* No dropout and bias for LoRA\n* For use\\_gradient\\_checkpointing we are using Unsloth to handle that (saving memory)\n\n```python\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 16,\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha = 16,\n    lora_dropout = 0,\n    bias = \"none\",\n    use_gradient_checkpointing = \"unsloth\",\n    random_state = 3407,\n    use_rslora = False,\n    loftq_config = None,\n)\n```\n\n**4/ Defining the Prompt Template :** We will create alpaca prompt template to format the dataset ( In case the data that you will be using is not in that format).\n\nWe will also add EOS (End Of Sequence) to inform the LLM that the sentence has ended.\n\nFinally the formatting function, the function takes a batch of examples and formats each one according to the alpaca prompt template that we write before.\n\n* It extracts instruction, input, and output fields from each example (row).\n* It then formats these fields into the template and appends the EOS token.\n* The formatted text is stored in a list and returned as a dictionary with a single key, “text”\n\n```python\nalpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n#### Instruction:\n{}\n\n#### Input:\n{}\n\n#### Response:\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token\n\ndef formatting_prompts_func(examples):\n    instructions = examples[\"instruction\"]\n    inputs = examples[\"input\"]\n    outputs = examples[\"output\"]\n    texts = []\n    for instruction, input, output in zip(instructions, inputs, outputs):\n        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n        texts.append(text)\n    return {\"text\": texts}\n```\n\n**5/ Loading and Formatting the Dataset:** Loads the Alpaca dataset and applies formatting to each dataset example in batches.\n\n```python\nfrom datasets import load_dataset\ndataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\ndataset = dataset.map(formatting_prompts_func, batched = True)\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*M8EmbLMdoqrM-JlkMpDv8g.png)\n\n**6/ Setting Up and Training the Model:** I covered most of them in my [previous article](https://readmedium.com/supervised-fine-tuning-tips-for-your-llm-projects-f84f20593653) regarding tips for Fine\\-Tuning.\n\n```python\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2, # Number of processes to use for data preprocessing\n    packing = False, # Whether to pack multiple sequences into one batch to increase training efficiency\n    args = TrainingArguments(\n        per_device_train_batch_size = 2, #The batch size per device\n        gradient_accumulation_steps = 4, #Number of gradient accumulation steps, which allows for effectively larger batch sizes\n        warmup_steps = 5, #Number of steps to perform linear learning rate warmup\n        max_steps = 60, #Total number of training steps\n        learning_rate = 2e-5,#The learning rate for the optimizer\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"cosine\",\n        seed = 3407,\n        output_dir = \"outputs\",\n    ),\n)\n\ntrainer_stats = trainer.train()\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Vb_OqGP9CPc8xZdnkclGyQ.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*PI0JXrTbpjuviyQ4bZJnFg.png)\n\n**7/ Inference and Generation :** we prepare the model for inference by preparing the input prompt, tokenizing it , and then uses the model to generate new text based on that prompt. The generated text is then converted back into readable form.\n\n```python\nFastLanguageModel.for_inference(model)\ninputs = tokenizer(\n[\n    alpaca_prompt.format(\n        \"Continue the fibonnaci sequence.\", # instruction\n        \"1, 1, 2, 3, 5, 8\", # input\n        \"\", # output - leave this blank for generation!\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*PI6SBL_YPPj0-RSAn5nl7g.png)\n\nYou can also use a TextStreamer for continuous inference , so you can see the generation token by token, instead of waiting the whole time!\n\n```python\nFastLanguageModel.for_inference(model)\ninputs = tokenizer(\n[\n    alpaca_prompt.format(\n        \"Continue the fibonnaci sequence.\",\n        \"1, 1, 2, 3, 5, 8\",\n        \"\",\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*NaSQ1vQKORU1I3DsOU2iOA.png)\n\n**8/ Save the model :** If you are happy with it, you can save your model or push it to Hugging Face Hub\n\n```python\nmodel.save_pretrained(\"lora_model\")\ntokenizer.save_pretrained(\"lora_model\")\n## model.push_to_hub(\"your_name/lora_model\", token = \"...\")\n## tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\")\n```\n\n**9/ Load the model :**\n\n```python\nif False:\n    from unsloth import FastLanguageModel\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name = \"lora_model\",\n        max_seq_length = max_seq_length,\n        dtype = dtype,\n        load_in_4bit = load_in_4bit,\n    )\n    FastLanguageModel.for_inference(model)\n```\n\n**10/ Using it for generation :**\n\n```python\ninputs = tokenizer(\n[\n    alpaca_prompt.format(\n        \"What is the capital of Palestine ?\",\n        \"\",\n        \"\",\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)\n```\n\nIf there’s a specific subject you’d like us to cover, please don’t hesitate to let me know! Your input will help shape the direction of my content and ensure it remains relevant and engaging 😀\n\n\n"},{"lang":"en","group":"blog","slug":"blog/gemini-1-5-flash-vs-gpt-4o-88b9d8da8152","frontmatter":{"title":"New Gemini 1.5 FLASH Model: An Absolute Google Game Changer","meta_title":"New Gemini 1.5 FLASH Model: An Absolute Google Game Changer","description":"Gemini 1.5 Flash blows GPT-4o out of the water","date":"2024-11-08T00:27:31.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Reb1owOmiw5DFd4A.png","categories":["Programming","Machine Learning","Generative AI"],"author":"Rifx.Online","tags":["Gemini","Flash","GPT-4o","multi-modality","creativity"],"draft":false,"slug":"blog/gemini-1-5-flash-vs-gpt-4o-88b9d8da8152"},"content":"\nTheir new Gemini 1\\.5 Flash model blows GPT\\-4o out of the water and the capabilities are hard to believe.\n\n**Lightning fast**.\n\n\n\n33 times cheaper than GPT\\-4o but has a 700% greater context — **1 million tokens.**\n\nWhat is 1 million tokens in the real\\-world? Approximately:\n\n* Over an 1 hour of video\n* Over 30,000 lines of code\n* Over 700,000 words\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*E1XIOcpWfeqOZSZC.jpg)\n\n❌GPT\\-4o cost:\n\n* Input: $2\\.50 per million tokens\n* Output: $10 per million tokens\n* Cached input: $1\\.25 per million tokens\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*XM3hFyS_PCcuv8Px.png)\n\n✅ Gemini 1\\.5 Flash cost:\n\n* Input: $0\\.075 per million tokens\n* Output: $0\\.30 per million tokens\n* Cached input: $0\\.01875 per million tokens\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*d-1ioFlCxW3LB4SL.png)\n\nAnd then there’s the mini Flash\\-8B version for cost\\-efficient tasks — 66 times cheaper than GPT\\-4o:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*5B5ybLzTr7penwms.png)\n\nAnd the best part is the multi\\-modality — it can reason with text, files, images and audio in complex integrated ways.\n\nAnd 1\\.5 Flash has almost all the capabilities of Pro but much faster. And as a dev you can start using them now.\n\nGemini 1\\.5 Pro was tested with a 44\\-minute silent movie and astonishingly, it easily analyzed the movie into various plot points and events. Even pointing out tiny details that most of us would miss on first watch.\n\nMeanwhile the GPT\\-4o API only lets you work with text and images.\n\nYou can easily create, test and refine prompts in Google’s AI Studio — **completely free**.\n\nIt doesn’t count in your billing like in OpenAI playground.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*5BKejWrJvrsEWIjc.png)\n\nJust look at the power of Google AI Studio — creating a food recipe based on an image:\n\nI uploaded this delicious bread from gettyimages:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*fC5YL_dplJ9Od_vN.jpg)\n\nNow:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*GezbFh9KzFXRVhr3.png)\n\nWhat if I want the response to be a specialized format for my API or something?\n\nThen you can just turn on JSON mode and specify the response schema:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*aRZuia7Iz_mI2s9b.png)\n\nOpenAI playground has this too, but it’s not as intuitive to work with.\n\nAnother upgrade Gemini has over OpenAI is how creativity it can be.\n\nIn Gemini you can increase the `temperature` from 0 to 200% to control how random and creative the responses are:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*4AAFdAMfT_xyflmv.png)\n\nMeanwhile in OpenAI if you try going far beyond 100%, you’ll most likely get a whole literal load of nonsense.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*yzFQL69pyJmgE9UB.png)\n\nAnd here’s the best part — when you’re done creating your prompt you can just use **Get code** — easily copy and paste the boilerplate API code and move lightning\\-fast in your development.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*xgaZfVe9b8WSBMmq.png)\n\nWorks in several languages including Kotlin, Swift and Dart — efficient AI workflow in mobile dev.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*AMkfKm-3KQRxnltO.png)\n\nIn OpenAI playground you can get the code for Python and JavaScript.\n\n## Final thoughts\n\nGemini 1\\.5 Flash is a game\\-changer offering unparalleled capabilities at a fraction of the cost.\n\nWith its advanced multi\\-modality ease of use, generous free pricing, and creative potential it sets a new standard for AI leaving GPT\\-4o in the dust.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/gemma-vs-llama-vs-mistral-exploring-smaller-ai-models-672a95f4b9b7","frontmatter":{"title":"Gemma vs. Llama vs. Mistral: Exploring Smaller AI Models","meta_title":"Gemma vs. Llama vs. Mistral: Exploring Smaller AI Models","description":"A Comparative Study of Small-Scale Language Models: Evaluating Gemma, Llama 3, and Mistral in Reading Comprehension Tasks","date":"2024-11-10T22:36:54.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*TJqJ12YQCeYTS5fWOYR5Ig.png","categories":["Natural Language Processing","Machine Learning","Data Science"],"author":"Rifx.Online","tags":["Gemma","Llama","Mistral","SQuAD","Multi-Query"],"draft":false,"slug":"blog/gemma-vs-llama-vs-mistral-exploring-smaller-ai-models-672a95f4b9b7"},"content":"\n### A Comparative Study of Small\\-Scale Language Models: Evaluating Gemma, Llama 3, and Mistral in Reading Comprehension Tasks\n\n## Introduction\n\nLarge Language Models (LLMs) have been evolving rapidly. Each month, new models are developed to surpass the current top scorers in the market. This healthy competition is beneficial for creating new approaches that increase quality and speed. Additionally, companies are focused on developing smaller models to make them accessible to individuals or organizations without powerful computing resources.\n\nJust a few weeks ago, Apple introduced Apple Intelligence at their Worldwide Developers Conference. This is a set of multiple generative models fine\\-tuned to help users write and refine text, prioritize and summarize notifications, create images, and take in\\-app actions. The only foundational and proprietary model developed by Apple in that suite was introduced at the same conference. It is a small model designed to run on\\-device, where the hardware becomes a significant constraint. In Apple’s case, the model is closed\\-source. What we know is that it is a \\~3 billion parameter model on par with the 7b versions of Gemma, Mistral, and Llama 3 (according to the results shared by Apple).\n\nWhile Apple’s new model is exciting, we cannot test or reuse it. Hence, we are more interested in publicly available models since developers and companies can use them to build new products and services. It’s important to distinguish between open LLMs and open\\-source LLMs. Historically, open\\-source software refers to computer programs released under specific licenses, making the source code available for public use or modification. With LLMs, there is additional complexity, including the training data and model weights. Therefore, open LLMs typically disclose the model weights and initial code. An open\\-source LLM, on the other hand, would share every step of the training process, including the training data, along with a permissive license. It should allow others to use, build upon, and further distribute the model. Nevertheless, most of the models released these days fall under the category of open LLMs since, for example, they do not publish the datasets used for training purposes. This is the case for Gemma by Google, Mistral by Mistral AI, and Llama by Meta.\n\nIn this article, we analyze Gemma more closely to understand what differentiates these smaller models. Gemma is one of the most recently developed models released by Google. It comes in two versions, 2 billion and 7 billion parameters. Thus, it can be used on edge devices, and it aims to outperform state\\-of\\-the\\-art models like Mistral and Llama 3\\.\n\nAdditionally, we apply Gemma, Llama 3, and Mistral to a reading comprehension dataset called SQuAD. The LLMs are tasked with answering specific questions based on given contexts. We assess their performance using quantitative metrics such as inference speed and average answer length. We also use the Relative Answer Quality (RAQ) framework proposed by \\[1]. RAQ bridges the gap in evaluating LLMs for specific use cases by ranking answers based on their accuracy relative to the ground truth, providing a more nuanced and practical assessment of model performance.\n\n\n\nAs always, the code is available on our [GitHub](https://github.com/zaai-ai/lab).\n\n## Gemma: the base text model of Gemini\n\nGoogle released Gemma \\[2], an open LLM developed based on its powerful, closed\\-source model, Gemini \\[3].\n\nGoogle released pre\\-trained and fine\\-tuned checkpoints to promote further research of the model in new use cases, making it available in two different sizes:\n\n* The 7B model is to be deployed and further developed on GPU or TPU.\n* The 2B model is designed to address computation constraints and allow its use on CPU or on\\-device applications.\n\nGemma promises to achieve state\\-of\\-the\\-art performance compared to other open models with roughly the same scale, like Llama 3 7B or Mistral 7B. This should happen across different domains, such as question answering, common sense reasoning, mathematics/science, and coding.\n\n## Gemma: what is new?\n\nGemma’s architecture is based on a decoder\\-only \\[4] Transformer \\[5] with a context length of 8192 tokens. Let’s explore the approach taken to make it smaller.\n\n## Multi\\-Query Attention\n\nThe 2B model utilizes Multi\\-Query Attention (MQA) to significantly reduce the memory resources required to load all query, key, and value heads, as opposed to the Multi\\-Head Attention (MHA) approach. MQA achieves this memory reduction by using a single key and value for multiple query heads in the attention layer, as illustrated in Figure 3\\.\n\nWhile this approach allows Gemma 2B to be deployed on devices with smaller memory resources, it can lead to quality degradation and training instability. Therefore, the authors opted to use MHA in the 7B version, following the same method as Llama 3\\.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*cgSktHmd_iQeTU4DwWLCPQ.png)\n\n## RoPE Embeddings\n\nTransformers require Positional Embeddings because they are inherently order\\-invariant. This means that without positional information, a Transformer would represent sentences with the same words but different orders and meanings in the same way. For example:\n\n> *Sentence 1:* Gemma is better than Llama 3\n\n> *Sentence 2:* Llama 3 is better than Gemma\n\nPositional information is typically represented using two sinusoidal functions (sine and cosine). Then, a unique positional embedding is created for each position in the sequence based on its position, the token embedding dimension, and the model dimension.\n\nTherefore, adding positional information is crucial for enabling Transformers to process text properly. The original Transformer architecture used **Absolute Positional Embeddings**, where a vector representation of a position is added to the vector representation of a token.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*cU5a_5-ATKwrQVeka-ViXQ.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*JZLrvgvc7l_52uewCrPSbg.png)\n\nThe challenge with Absolute Positional Embeddings is that they do not explicitly encode the relative distances between tokens. While they capture positional information using sine and cosine functions, these embeddings are calculated independently for each position. This means that the model does not inherently understand the proximity or relational significance of different positions within a sequence. For instance, the embeddings for tokens at positions 1 and 2 may appear similar due to the nature of the sinusoidal functions, but the model doesn’t explicitly recognize that these positions are adjacent.\n\nBecause of this, the model might not differentiate the relationship between tokens at positions 1 and 2 from the relationship between tokens at positions 1 and 500\\. In natural language processing, words that are close together in a sentence often share more context or have a stronger semantic or syntactic relationship than words that are far apart. Absolute Positional Embeddings might not completely capture this nuance. It can lead to limitations in capturing long\\-range dependencies or the hierarchical structure of language.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*p-fG2ydLbOhJHjO7Y0LyUw.png)\n\nRotary Positional Embeddings (RoPE) \\[6] address this problem by modeling the relative positions of tokens through a rotation of the token embeddings in the sequence.\n\nLet’s use the previous example, *‘Gemma is better than Llama*,’ and consider each word as a token represented by a 2D vector. The word *better* will be represented by a 2D vector rotated from the original vector based on its position *m* and a constant angle θ, as shown in Figure 5\\.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*nX3llo0cwBIrCQ8Gn21-gg.png)\n\nThis approach preserves the relative distance between tokens because the rotational transformation maintains the same similarity between vectors, regardless of their position in the sequence. For instance, if we add two words to the original sentence, making it ‘*The LLM Gemma is better than Llama*’, the positions of *better* and *than* change from (3 \\& 4\\) to (5 \\& 6\\). However, since the rotation angle remains consistent, the similarity between these vectors (as measured by the dot product) stays the same, ensuring consistent relative positioning.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*6cpWPXTexZC8YQbnasHOUg.png)\n\n## GeGLU Activation Function\n\nThe authors replaced the traditional ReLU activation function with a variant of a Gated Linear Unit (GLU) called GeGLU, as another study \\[7] has shown that it improves the quality of the output generated by the LLM.\n\nThere are two differences between the ReLU and GeGLU:\n\n1. **Activation function** — GeGLU uses a Gaussian Error Linear Unit (GELU) \\[8] function that differs from ReLU in the sense that it multiplies the neuron input *x* by a cumulative distribution function of the normal distribution. In this case, the probability of *x* being dropped is higher as *x* decreases.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*FXCfQpvdMJXPk5s6AO-RuA.png)\n\n2\\. **Sigmoid Activated** — The simple ReLU or GELU activation function is applied between the hidden representation *x* andtwo linear transformations represented by two matrices (*W1* and *W2\\).* The Gating variant in GeGLU applies a gating mechanism (sigmoid) to one of the components, as shown in Equation 3\\.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Z9hUjuy4NvQVDrPj6iSfrQ.png)\n\n## Normalizer Location\n\nThe last modification to the original Transformer architecture is shown in Figure 8\\. The authors normalize both the input and output of each transformer sub\\-layer to improve training stability, contrary to the original paper, which only normalized the output.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*NQe4ME2MhvRWVzobVdIloA.png)\n\nThey also replaced the traditional LayerNorm function with RMSNorm \\[8]. It is computationally more efficient while maintaining training stability improvements and helping the model converge.\n\nRMSNorm achieves better efficiency because its authors demonstrated that the benefits of LayerNorm come from re\\-scaling invariance rather than re\\-centering invariance. Re\\-scaling invariance means that the output of the normalization process remains unchanged if a constant factor scales the input. In other words, multiplying all the inputs by a constant does not affect the normalized outputs. Re\\-centering invariance means that the output of the normalization process remains unchanged if a constant value is added to all the inputs. This implies that shifting all inputs by a constant amount does not affect the normalized outputs. This finding allows the removal of the overhead of computing the mean (you only need to compute the standard deviation), making RMSNorm simpler and more efficient.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*qblXBo8SCcxzPWePhFVlYg.png)\n\n## Mistral AI vs. Meta vs. Google: a comparison between Gemma 7B vs. Llama 3 7B vs. Mistral 7B\n\nIn this section, we put 3 LLMs — Gemma 7B, Mistral 7B, and Llama 3 7B — to a test. We use a question\\-answering dataset under the License CC BY\\-SA 4\\.0 called SQuAD (it can be found [here](https://huggingface.co/datasets/rajpurkar/squad)). This dataset is a reading comprehension dataset consisting of questions about a set of Wikipedia articles. Based on context, the models should be able to retrieve the correct answer to a question. The 3 more important fields for our use case are:\n\n* `question` \\- the question a model should answer.\n* `context` \\- background information from which the model needs to extract the answer.\n* `answers` \\- the text answer to the question.\n\nThe evaluation process will consist of two quantitative metrics:\n\n* `words per second` \\- assesses the inference speed.\n* `words` \\- assesses the length of the answer.\n\nTo assess the accuracy of the models in our use case, we use RAQ \\[1]. RAQ ranks the answers of all LLMs using an independent LLM based on how close they are to the ground truth answer.\n\nWe start by downloading the models in a `.gguf` format to be able to run them in CPU, and we place them under the folder `model/`.\n\nWe used the instruct version of each model with a 4\\-bit quantization:\n\n* `mistral-7b-instruct-v0.1.Q4_K_M.gguf` from [https://huggingface.co/TheBloke/Mistral\\-7B\\-Instruct\\-v0\\.1\\-GGUF/tree/main](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/tree/main)\n* `Meta-Llama-3-8B-Instruct-Q4_K_M.gguf` from [https://huggingface.co/NousResearch/Meta\\-Llama\\-3\\-8B\\-Instruct\\-GGUF](https://huggingface.co/NousResearch/Meta-Llama-3-8B-Instruct-GGUF)\n* `gemma-7b-it-Q4_K_M.gguf` from [https://huggingface.co/rahuldshetty/gemma\\-7b\\-it\\-gguf\\-quantized/tree/main](https://huggingface.co/rahuldshetty/gemma-7b-it-gguf-quantized/tree/main)\n\nAfter that, we import all the libraries and our generator that receives the model we want to use as an argument.\n\n```python\nimport os\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scikit_posthocs as sp\nimport pandas as pd\nimport utils\n\nfrom dotenv import load_dotenv\nfrom datasets import load_dataset\nfrom generator.generator import Generator\n\nllama = Generator(model='llama')\nmistral = Generator(model='mistral')\ngemma = Generator(model='gemma')\nload_dotenv('env/var.env')\n```\n\nThis class is responsible for importing the model parameters defined in a `config.yaml` file with the following characteristics: `context_length` of 1024, `temperature` of 0\\.7, and `max_tokens` of 2000\\.\n\n```python\ngenerator:\n  llama:\n    llm_path: \"model/Meta-llama-3-8B-Instruct-Q4_K_M.gguf\"\n  mistral:\n    llm_path: \"model/mistral-7b-instruct-v0.1.Q4_K_M.gguf\"\n  gemma:\n    llm_path: \"model/gemma-7b-it-Q4_K_M.gguf\"\n  context_length: 1024\n  temperature: 0.7\n  max_tokens: 2000\n```\n\nIt also creates the Prompt Template. This template helps format the query and the context before passing it to the LLM to get a response.\n\n```python\nfrom langchain import PromptTemplate\nfrom langchain.chains import LLMChain\nfrom langchain.llms import LlamaCpp\n\nfrom base.config import Config\nclass Generator(Config):\n    \"\"\"Generator, aka LLM, to provide an answer based on some question and context\"\"\"\n    def __init__(self, model) -> None:\n        super().__init__()\n    # template\n        self.template = \"\"\"\n            Use the following pieces of context to answer the question at the end.\n            {context}\n            Question: {question}\n            Answer:\n        \"\"\"\n   # load llm from local file\n        self.llm = LlamaCpp(\n            model_path=f\"{self.parent_path}/{self.config['generator'][model]['llm_path']}\",\n            n_ctx=self.config[\"generator\"][\"context_length\"],\n            temperature=self.config[\"generator\"][\"temperature\"],\n        )\n        # create prompt template\n        self.prompt = PromptTemplate(\n            template=self.template, input_variables=[\"context\", \"question\"]\n        )\n    def get_answer(self, context: str, question: str) -> str:\n        \"\"\"\n        Get the answer from llm based on context and user's question\n        Args:\n            context: most similar document retrieved\n            question: user's question\n        Returns:\n            llm answer\n        \"\"\"\n        query_llm = LLMChain(\n            llm=self.llm,\n            prompt=self.prompt,\n            llm_kwargs={\"max_tokens\": self.config[\"generator\"][\"max_tokens\"]},\n        )\n        return query_llm.run({\"context\": context, \"question\": question})\n```\n\nWith the LLMs loaded, we fetch the SQuAD dataset from HuggingFace and shuffle it to ensure enough variety in the question theme.\n\n```python\nsquad = load_dataset(\"squad\", split=\"train\")\nsquad = squad.shuffle()\n```\n\nNow, we can loop over 60 questions and contexts and record the metrics mentioned above.\n\n```python\nfor i in range(60):\n    context = squad[i]['context']\n    query = squad[i]['question']\n    answer = squad[i]['answers']['text'][0]\n\n    # Llama\n    answer_llama, words_per_second, words = utils.get_llm_response(llama, context, query)\n    llama_metrics[\"words_per_second\"].append(words_per_second)\n    llama_metrics[\"words\"].append(words)\n    # mistral\n    answer_mistral, words_per_second, words = utils.get_llm_response(mistral, context, query)\n    mistral_metrics[\"words_per_second\"].append(words_per_second)\n    mistral_metrics[\"words\"].append(words)\n    # gemma\n    answer_gemma, words_per_second, words = utils.get_llm_response(gemma, context, query)\n    gemma_metrics[\"words_per_second\"].append(words_per_second)\n    gemma_metrics[\"words\"].append(words)\n  \n    # GPT-3.5 rank\n    llm_answers_dict = {'llama': answer_llama, 'mistral': answer_mistral, 'gemma': answer_gemma}\n    rank = utils.get_gpt_rank(answer, llm_answers_dict, os.getenv(\"OPENAI_API_KEY\"))\n    llama_metrics[\"rank\"].append(rank.index('1')+1)\n    mistral_metrics[\"rank\"].append(rank.index('2')+1)\n    gemma_metrics[\"rank\"].append(rank.index('3')+1)\n```\n\nThe function `get_llm_response` is responsible for receiving the loaded LLM, the context, and the question and return the LLM answer as well as the quantitative metrics.\n\n```python\ndef get_llm_response(model: Generator, context: str, query: str) -> Tuple[str, int, int]:\n    \"\"\"\n    Generates an answer from a given LLM based on context and query\n    returns the answer and the number of words per second and the total number of words\n    Args:\n        model: LLM\n        context: context data\n        query: question\n    Returns:\n        answer, words_per_second, words\n    \"\"\"\n    init_time = time.time()\n    answer_llm = model.get_answer(context, query)\n    total_time = time.time()-init_time\n    words_per_second = len(re.sub(\"[^a-zA-Z']+\", ' ', answer_llm).split())/total_time\n    words = len(re.sub(\"[^a-zA-Z']+\", ' ', answer_llm).split())\n    return answer_llm, words_per_second, words\n```\n\nWe can see that Llama 3 is faster than Mistral and Gemma by producing on average \\~0\\.7 words per second, while Mistral produces \\~0\\.26 and Gemma \\~0\\.4 words. In terms of answer length, Llama 3 also produces longer answers than Mistral and Gemma, with an average answer length of 148 words against 20 words for Mistral and 50 for Gemma. Finally, based on RAQ, Mistral had the best average rank of approximately 1\\.81, followed by Gemma with an average of 2\\.05, while Llama 3 performed worse with an average rank of approximately 2\\.1\\.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GVeFQbMZZ5oUScVEHQPu8A.png)\n\nThe RAQ framework also includes a statistical test to understand if the observed differences are significant. Table 1 displays the results of the Dunn post\\-hoc test, comparing the performance of different language models. Each cell indicates whether the difference in performance between the respective models is statistically significant at a 5% significance level. “Significant” denotes a statistically significant difference (p\\-value ≤ 0\\.05\\), while “Not Significant” indicates no statistically significant difference (p\\-value \\> 0\\.05\\). For the selected significance level, the Dunn test result shows that the difference in performance between models is not significant.\n\n```python\np_values = sp.posthoc_dunn([Llama_metrics['rank'], mistral_metrics['rank'], gemma_metrics['rank']], p_adjust='holm')\np_values > 0.05\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ftCaagMKAm5RzeATYm_7Ug.png)\n\nIt is always important to assess qualitatively some examples. Below, we have the answers from the 3 models to the question *‘Power House Day is celebrated on what day in New Haven?’* based on the following context:\n\n> ***Context:***‘For over a century, New Haven citizens had fought in the colonial militia alongside regular British forces, as in the French and Indian War. As the American Revolution approached, General David Wooster and other influential residents hoped that the conflict with the government in Britain could be resolved short of rebellion. On 23 April 1775, which is still celebrated in New Haven as Powder House Day, the Second Company, Governor’s Foot Guard, of New Haven entered the struggle against the governing British parliament. Under Captain Benedict Arnold, they broke into the powder house to arm themselves and began a three\\-day march to Cambridge, Massachusetts. Other New Haven militia members were on hand to escort George Washington from his overnight stay in New Haven on his way to Cambridge. Contemporary reports, from both sides, remark on the New Haven volunteers’ professional military bearing, including uniforms.’\n\nAll 3 models gave correct answers. While Llama 3 and Gemma provided more complete answers, Mistral was more succinct.\n\n> ***Llama 3 answer:***‘New Haven’s Powder House Day is celebrated on April 23rd.’\n\n> ***Gemma answer:***‘Sure! The text states on which day Powder House Day is celebrated on: Powder House Day is celebrated on **23 April** in New Haven.’\n\n> ***Mistral answer:***’23 April’\n\n## Conclusion\n\nOn\\-device models present a great opportunity to enhance user experiences by making powerful LLMs accessible on devices with lower computational resources. Both Apple and Google are actively developing smaller, more efficient models to meet this need, enabling more people to benefit from advanced AI in their daily lives.\n\nIn this article, we explored Gemma, the open LLM developed by Google, which introduced four novel features to the traditional Transformer architecture: Multi\\-Query Attention in the 2B version, RoPE embeddings for positional encoding, GeGLU as the activation function, and input normalization.\n\nWe also compared Gemma's performance against Llama 3 and Mistral on a reading comprehension dataset. We observed that Gemma produced more words per second and wrote longer answers than Mistral, but it did not surpass Llama 3 in these metrics. Using the RAQ framework, we assessed the accuracy of the three models. While the data showed better results from Mistral, followed by Gemma, the differences were not statistically significant. Therefore, we can say that the 3 models performed similarly when applied to our use case of reading comprehension.\n\n## References\n\n\\[1] Luís Roque, Rafael Guedes. Research to Production: Relative Answer Quality (RAQ) and NVIDIA NIM. [https://readmedium.com/research\\-to\\-production\\-relative\\-answer\\-quality\\-raq\\-and\\-nvidia\\-nim\\-15ce0c45b3b6](https://readmedium.com/research-to-production-relative-answer-quality-raq-and-nvidia-nim-15ce0c45b3b6), 2024\\.\n\n\\[2] Gemma Team, Google DeepMind. Gemma: Open Models Based on Gemini Research and Technology, 2023\\.\n\n\\[3] Gemini Team. Gemini: A family of highly capable multimodal models, 2023\\.\n\n\\[4] Noam Shazeer. Fast Transformer Decoding: One Write\\-Head is All You Need. arXiv:1911\\.02150, 2019\\.\n\n\\[5] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. Attention Is All You Need. arXiv:1706\\.03762, 2017\\.\n\n\\[6] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, Yunfeng Liu. RoFormer: Enhanced Transformer with Rotary Position Embedding. arXiv:2104\\.09864, 2021\\.\n\n\\[7] Noam Shazeer. GLU Variants Improve Transformer. arXiv:2002\\.05202, 2020\\.\n\n\\[8] Dan Hendrycks, Kevin Gimpel. Gaussian Error Linear Units (GELUs). arXiv:1606\\.08415, 2016\\.\n\n\\[9] Biao Zhang, Rico Sennrich. Root Mean Square Layer Normalization. arXiv:1910\\.07467, 2019\\.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/generating-structured-data-from-an-image-with-gpt-vision-and-langchain-34aaf3dcb215","frontmatter":{"title":"Generating structured data from an image with GPT vision and Langchain","meta_title":"Generating structured data from an image with GPT vision and Langchain","description":"In today’s world, where visual data is abundant, the ability to extract meaningful information from images is becoming increasingly…","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*FPRRg85jYb7MrzXEpNWbmw.jpeg","categories":["Programming","Computer Vision","Natural Language Processing"],"author":"Rifx.Online","tags":["Langchain","GPT","vision","LLMs","structured"],"draft":false,"slug":"blog/generating-structured-data-from-an-image-with-gpt-vision-and-langchain-34aaf3dcb215"},"content":"\n\n\n\n\n\nIn today’s world, where visual data is abundant, the ability to extract meaningful information from images is becoming increasingly valuable. Langchain, a powerful framework for building applications with large language models (LLMs), offers a versatile toolset for tackling this challenge. In this article, we’ll explore how to use Langchain to extract structured information from images, such as counting the number of people and listing the main objects.\n\nBefore diving into the code, let’s set the stage by understanding the task at hand. Imagine you have an image of a scene, such as a city street. Your goal is to extract valuable information from this image, including the number of people present and a list of the main objects in the scene.\n\n\n## About Langchain\n\nLangchain is a comprehensive framework that allows developers to build sophisticated applications by leveraging the power of large language models (LLMs). It provides a modular and extensible architecture, enabling developers to create custom pipelines, agents, and workflows tailored to their specific needs.\n\nLangchain simplifies the integration of LLMs, offering abstractions and utilities for handling various data sources, including text, images, and structured data. It supports a wide range of LLMs from different providers, such as OpenAI and Anthropic, making it easy to switch between models or combine multiple models in a single application.\n\n\n## Preparing the Environment and Setting Up the OpenAI API Key\n\nTo follow along with this tutorial, you’ll need to have Langchain installed. You can install it using pip:\n\n\n```python\npip install langchain langchain_openai\n```\nTo use the OpenAI language models with Langchain, you’ll need to obtain an API key from OpenAI. If you don’t have an API key yet, you can sign up for one on the OpenAI website (<https://openai.com/api/>).\n\nOnce you have your API key, you can set it as an environment variable in your system or provide it directly in your code. Here’s an example of how to set the API key as an environment variableCopy code\n\n\n```python\nexport OPENAI_API_KEY=\"your_openai_api_key_here\"\n```\nAlternatively, you can provide the API key directly in your Python code:\n\n\n```python\nimport os\nimport langchain\nos.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key_here\"\n```\nAfter setting up the API key, Langchain will be able to authenticate with the OpenAI API and use their language models.\n\n\n## Loading and Encoding the Image\n\nBefore we can process images with Langchain, we need to load the image data from a file and encode it in a format that can be passed to the language model. The code below defines a function `load_image` that takes a dictionary with an `image_path` key and returns a new dictionary with an `image` key containing the image data encoded as a base64 string.\n\n\n```python\ndef load_image(inputs: dict) -> dict:\n    \"\"\"Load image from file and encode it as base64.\"\"\"\n    image_path = inputs[\"image_path\"]\n  \n    def encode_image(image_path):\n        with open(image_path, \"rb\") as image_file:\n            return base64.b64encode(image_file.read()).decode('utf-8')\n    image_base64 = encode_image(image_path)\n    return {\"image\": image_base64}\n```\nThe `load_image` function first extracts the `image_path` from the input dictionary. It then defines a nested function `encode_image` that opens the image file in binary mode, reads its contents, and encodes them as a base64 string using the `base64.b64encode` function from the Python standard library.\n\nThe `load_image` function calls `encode_image` with the provided `image_path` and stores the resulting base64-encoded string in the `image_base64` variable. Finally, it returns a new dictionary with the `image` key set to `image_base64`.\n\nTo integrate this function into a Langchain pipeline, we can create a `TransformChain` that takes the `image_path` as input and produces the `image` (base64-encoded string) as outputCopy code\n\n\n```python\nload_image_chain = TransformChain(\n    input_variables=[\"image_path\"],\n    output_variables=[\"image\"],\n    transform=load_image\n)\n```\nWith this setup, we can easily load and encode images as part of a larger Langchain workflow, enabling us to process visual data alongside text using large language models.\n\n\n## Defining the Output Structure\n\nBefore we can extract information from the image, we need to define the structure of the output we want to receive. In this case, we’ll create a Pydantic model called `ImageInformation` that includes fields for the image description and any additional information we might want to extract.\n\n\n```python\nfrom langchain_core.pydantic_v1 import BaseModel, Field\n\nclass ImageInformation(BaseModel):\n \"\"\"Information about an image.\"\"\"\n image_description: str = Field(description=\"a short description of the image\")\n people_count: int = Field(description=\"number of humans on the picture\")\n main_objects: list[str] = Field(description=\"list of the main objects on the picture\")\n```\n\n## Setting up the Image Model\n\nNext, we’ll create a chain that combines the image loading and encoding steps with the LLM invocation step. Since the `ChatOpenAI` model is not natively capable of handling both text and image inputs simultaneously (to my unsderstanding), we'll create a wrapper chain to achieve this functionality.\n\n\n```python\nfrom langchain.chains import TransformChain\nfrom langchain_core.messages import HumanMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain import globals\nfrom langchain_core.runnables import chain\n\n## Set verbose\nglobals.set_debug(True)\n\n@chain\ndef image_model(inputs: dict) -> str | list[str] | dict:\n \"\"\"Invoke model with image and prompt.\"\"\"\n model = ChatOpenAI(temperature=0.5, model=\"gpt-4-vision-preview\", max_tokens=1024)\n msg = model.invoke(\n             [HumanMessage(\n             content=[\n             {\"type\": \"text\", \"text\": inputs[\"prompt\"]},\n             {\"type\": \"text\", \"text\": parser.get_format_instructions()},\n             {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{inputs['image']}\"}},\n             ])]\n             )\n return msg.content\n```\nIn this code snippet, we define a chain called `image_model` that invokes the `ChatOpenAI` model with the provided prompt, format instructions, and image. The `image_model` chain accepts a dictionary `inputs` containing the prompt and the base64-encoded image string.\n\nInside the chain, we create a `HumanMessage` object that combines the prompt text, format instructions, and the image URL, formatted as a data URI with the base64-encoded image data. We then invoke the `ChatOpenAI` model with this `HumanMessage` object, using the `gpt-4-vision-preview` model, which is specifically designed for multimodal tasks involving both text and images.\n\nThe model processes both the text prompt and the image, and returns the output.\n\n\n## Putting It All Together\n\nNow that we have all the necessary components, we can define a function that orchestrates the entire process:\n\n\n```python\nfrom langchain_core.output_parsers import JsonOutputParser\n\nparser = JsonOutputParser(pydantic_object=ImageInformation)\ndef get_image_informations(image_path: str) -> dict:\n   vision_prompt = \"\"\"\n   Given the image, provide the following information:\n   - A count of how many people are in the image\n   - A list of the main objects present in the image\n   - A description of the image\n   \"\"\"\n   vision_chain = load_image_chain | image_model | parser\n   return vision_chain.invoke({'image_path': f'{image_path}', \n                               'prompt': vision_prompt})\n```\nIn this function, we define a prompt that asks the LLM to provide a count of the people in the image and a list of the main objects. We then create a chain that combines the image loading step (`load\\_image\\_chain`), the LLM invocation step (`image\\_model`), and a JSON output parser (`parser`). Finally, we invoke this chain with the image path and the prompt, and the function returns a dictionary containing the extracted information.\n\n\n## Example Usage\n\nTo use this function, simply provide the path to an image file:\n\n\n```python\nresult = get_image_informations(\"path/to/your/image.jpg\")\nprint(result)\n```\nThis will output a dictionary with the requested information, such as:\n\n\n```python\n{\n 'description': 'a view of a city showing cars waiting at a traffic light',\n 'people_count': 5,\n 'main_objects': ['car', 'building', 'traffic light', 'tree']\n}\n```\n\n## Conclusion\n\nLangchain provides a powerful toolset for working with large language models and extracting valuable information from various data sources, including images. By combining Langchain’s capabilities with custom prompts and output parsing, you can create robust applications that can extract structured information from visual data.\n\nRemember, the quality of the output will depend on the capabilities of the LLM you’re using and the specificity of your prompts. Experiment with different models and prompts to find the best solution for your use case.\n\nIf you find a better way to achieve the same results or have suggestions for improvements, please don’t hesitate to share them in the comments. The code examples provided in this article are meant to serve as a starting point, and there may be alternative approaches or optimizations .\n\n\n"},{"lang":"en","group":"blog","slug":"blog/glm-4-voice-9b-real-time-multilingual-voice-conversation-ai-install-locally-in-minutes-ce2fcd6c8fd8","frontmatter":{"title":"GLM-4-Voice 9B — Real-time Multilingual Voice Conversation AI — Install Locally in Minutes","meta_title":"GLM-4-Voice 9B — Real-time Multilingual Voice Conversation AI — Install Locally in Minutes","description":"GLM-4-Voice 9B is an advanced multilingual voice conversation AI developed by Zhipu AI, enabling real-time interaction in English and Chinese. Its unique end-to-end architecture allows direct speech processing, minimizing latency and enhancing user experience. The model supports customizable voice attributes, including emotion and intonation, making it suitable for various applications. With a straightforward local setup process and high-performance requirements, GLM-4-Voice is poised to advance conversational AI in diverse fields such as customer service and education.","date":"2024-11-13T01:32:04.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*LATTpEc2AHvqgVyPKSzW7A.jpeg","categories":["Voice Assistants","Natural Language Processing","Chatbots"],"author":"Rifx.Online","tags":["multilingual","conversation","real-time","customization","performance"],"draft":false,"slug":"blog/glm-4-voice-9b-real-time-multilingual-voice-conversation-ai-install-locally-in-minutes-ce2fcd6c8fd8"},"content":"\n### How to set up GLM\\-4\\-Voice 9B for seamless real\\-time voice interaction in English and Chinese, and explore its unique architecture, low\\-latency response, and customizable voice attributes.\n\n\n\n## Introduction\n\nIn recent years, voice\\-enabled AI has significantly advanced, enabling conversational agents to better understand and respond to human speech. From virtual assistants to customer service bots, voice AI has become an essential tool in various industries. However, most models still struggle with fluently transitioning between languages, understanding nuances in spoken queries, and delivering high\\-quality responses. This is where GLM\\-4\\-Voice by Zhipu AI shines. Developed as an end\\-to\\-end voice model, GLM\\-4\\-Voice pushes the boundaries of multilingual conversational AI by supporting real\\-time dialogue in both English and Chinese, while offering an adaptable and human\\-like response generation.\n\nIn this article, we’ll explore why GLM\\-4\\-Voice is worth paying attention to, what makes it unique, and how you can set it up and start using it locally. We’ll also take a look at its architecture and provide a hands\\-on guide to accessing the web demo.\n\n## Why GLM\\-4\\-Voice?\n\nTraditional language models are often limited to text and require additional processing layers to handle voice. They may also struggle with interactivity or suffer from latency issues. GLM\\-4\\-Voice overcomes these limitations with a unified model that directly processes and generates speech. Here’s what makes it stand out:\n\n1. **End\\-to\\-End Voice Processing**: Unlike many other models that rely on a separate text\\-to\\-speech (TTS) or speech\\-to\\-text (STT) module, GLM\\-4\\-Voice directly interprets and responds in spoken language, allowing a more seamless and responsive experience.\n2. **Multilingual Support**: This model excels in handling both English and Chinese, two widely used languages globally. Its ability to switch between languages fluidly makes it ideal for bilingual environments and international applications.\n3. **Customizable Attributes**: GLM\\-4\\-Voice allows for adjustments in emotion, intonation, speech rate, and even dialect, making it capable of producing more natural and contextually appropriate responses.\n4. **Low Latency**: With support for streaming inference, the model has a latency of around 20 tokens, which enables near\\-instantaneous responses in real\\-time conversations.\n\n## Specialties of GLM\\-4\\-Voice\n\nGLM\\-4\\-Voice brings several unique features to the table, setting it apart from other voice models. Here’s what makes it special:\n\n* **Real\\-Time Voice Interaction**: By supporting low\\-latency responses, GLM\\-4\\-Voice can maintain fluid and natural conversations, which is crucial for applications like customer support and interactive AI.\n* **Dynamic Voice Attributes**: Users can specify the model’s emotional tone, speaking rate, and other characteristics, making interactions more engaging and suited to various contexts.\n* **Bilingual Support with Context Awareness**: This model is designed to comprehend and generate responses in both Chinese and English. It can switch between these languages seamlessly, offering a flexible solution for multilingual applications.\n* **Advanced Speech Decoding**: Built on CosyVoice, the GLM\\-4\\-Voice decoder enables high\\-quality speech generation with streaming support, maintaining high clarity in both languages.\n\n## Architecture\n\nThe architecture of GLM\\-4\\-Voice consists of three primary components, each fulfilling a crucial role in achieving end\\-to\\-end voice interaction:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*nJsKHtxSblNkixPIBZpWyQ.jpeg)\n\n1. **GLM\\-4\\-Voice\\-Tokenizer**: This component tokenizes continuous speech input into discrete tokens, with around 12\\.5 tokens generated per second of audio. The tokenizer is based on Whisper’s encoder, with added vector quantization, allowing the model to process audio in a structured form.\n2. **GLM\\-4\\-Voice\\-9B**: The core language model, based on the GLM\\-4 architecture, has been aligned to process spoken input. It can handle both text and speech, making it a powerful multimodal conversational agent.\n3. **GLM\\-4\\-Voice\\-Decoder**: This decoder converts the discrete tokens back into continuous speech, allowing the model to produce audio outputs. It supports streaming inference, enabling responses to begin after processing just a few tokens, minimizing conversation latency.\n\nTogether, these components make GLM\\-4\\-Voice a powerful tool for real\\-time voice interactions, supporting conversational AI across different languages and dialects.\n\n## Setting Up GLM\\-4\\-Voice Locally\n\nTo experience GLM\\-4\\-Voice, follow these steps to set up the model locally on your machine.\n\n### Step 1: Clone the Repository\n\nBegin by cloning the repository from GitHub. Make sure to include submodules:\n\n```python\n!git clone --recurse-submodules https://github.com/THUDM/GLM-4-Voice\ncd GLM-4-Voice\n```\n\n### Step 2: Install Dependencies\n\nNavigate into the project directory and install the necessary dependencies:\n\n```python\n!pip install -r requirements.txt\n```\n\n### Step 3: Download the Model Checkpoint\n\nGLM\\-4\\-Voice’s decoder model is hosted on Hugging Face and requires `git-lfs` to download. Make sure `git-lfs` is installed, then run:\n\n```python\n!git clone https://huggingface.co/THUDM/glm-4-voice\n```\n\n### Step 4: Launch the Model Service\n\nWith everything set up, start the model server:\n\n```python\npython model_server.py --model-path glm-4-voice-9b\n```\n\n### Step 5: Start the Web Service\n\nOnce the model server is running, start the web service by executing:\n\n```python\npython web_demo.py\n```\n\nYou can now access the web demo at [http://127\\.0\\.0\\.1:8888](http://127.0.0.1:8888) to interact with GLM\\-4\\-Voice.\n\n> **Note:** The GLM\\-4\\-Voice model is resource\\-intensive and requires a substantial amount of computational power to run effectively. Specifically, it necessitates 35–40 GPUs for optimal performance, making it suitable for deployment in environments with access to high\\-performance hardware. Users should ensure they have the necessary infrastructure in place before attempting to utilize this model.\n\n## Web Demo Interface\n\nThe web demo for GLM\\-4\\-Voice provides an intuitive interface with several customization options:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*scbHOUXqMW5KGAcT3Bq1Eg.png)\n\n* **Input Mode**: Users can choose to provide input as either text or audio. This flexibility allows for hands\\-free or traditional interaction.\n* **Voice Control Parameters**: Adjust temperature, top\\-p, and token limits to customize the model’s response characteristics.\n* **Debug Information**: Input and output tokens are displayed, giving users insight into the model’s processing of their queries.\n* **Interactive Audio Display**: Audio inputs and responses are displayed as waveforms, and users can replay or review audio segments to assess quality.\n\nHowever, Gradio, which is used to stream audio in the demo, may sometimes present instability. For best quality, it’s recommended that audio from the dialogue box be replayed after it has been generated.\n\n## Conclusion\n\nGLM\\-4\\-Voice stands out as an impressive achievement in conversational AI, offering a unique blend of bilingual support, real\\-time audio interaction, and flexible response customization. Its end\\-to\\-end design and low latency make it a prime candidate for applications in customer service, education, virtual assistants, and more. With an accessible setup process, GLM\\-4\\-Voice opens the door for developers and researchers to explore advanced voice capabilities in both Chinese and English.\n\nAs the demand for more interactive and realistic AI continues to grow, models like GLM\\-4\\-Voice represent a significant step forward in bridging language and conversational barriers. Whether you’re looking to build a chatbot, a virtual teacher, or a customer service agent, GLM\\-4\\-Voice provides a robust and versatile solution.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/google-gemini-are-big-context-windows-the-killer-feature-72ff95488fb1","frontmatter":{"title":"Google Gemini: Are Big Context Windows the Killer Feature?","meta_title":"Google Gemini: Are Big Context Windows the Killer Feature?","description":"Goggle’s upcoming LLM makes a massive move","date":"2024-11-10T22:36:54.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MteQrQSTXLuJcd86RbjQrg.png","categories":["Machine Learning","Natural Language Processing","Data Science"],"author":"Rifx.Online","tags":["Gemini","tokens","context","LLM","evolution"],"draft":false,"slug":"blog/google-gemini-are-big-context-windows-the-killer-feature-72ff95488fb1"},"content":"\n### Goggle’s upcoming LLM makes a massive move\n\n\n\nBarely eight months ago, a leaked Google email revealed the company was struggling to outpace its AI rivals. Not only was there [no moat](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither) around their AI offerings — in other words, no built up business advantage— Google also had [no secret sauce](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither) that could change things. And even as they were grappling with the problem, they were watching the gap between privately funded AI projects like theirs and open source AI models closing with “astonishing” speed.\n\nIt’s too soon to know how this story ends. Maybe open source AI will continue to build on its early successes, or maybe it will be smothered by the AIs run by massively wealthy competitors like Google, Microsoft, and Apple, and their mind\\-boggling quantities of data. Right now the conflict is still unfolding, as different organizations roll out a rapid series of AI advancements. Recently, Google took the spotlight in this arena, when it announced a preview of its newest LLM, [Gemini 1\\.5 Pro](https://deepmind.google/technologies/gemini/). Another day, another Large Language Model — or so it seemed, until Google described a startling change.\n\nGemini 1\\.5 Pro explodes the *context window*—essentially, a measure of how much data an LLM can track at once. In past versions, Gemini had a context window of up to 128,000 tokens, just like GPT\\-4\\. But Gemini’s new context window fits **1 million** tokens, and the implications of that change are enormous.\n\nBut before we can talk about the effect of context windows on LLM capabilities, we need to back up and quickly review how context windows work.\n\n## Context windows (in a nutshell)\n\nIn simple terms, the context window sets how much of your information an LLM can remember during an interaction. If you’re using ChatGPT, for example, the context window consists of the current prompt you gave it, everything else you’ve typed in to that conversation before, and every reply ChatGPT has sent back your way. Talk long enough, and the old parts of the conversation will slip out of the context window, and ChatGPT will abruptly forget those details.\n\nA 128,000 token context window sounds large, but the number is deceptive. First, consider that an average word is actually 1 to 3 tokens when it’s broken down for an LLM. (The rule of thumb is 4 tokens for 3 words, but it increases as the language becomes more complex or in specialized fields, like law or medicine.) When you look at long documents, ongoing interactions, and AI\\-powered applications, you’ll quickly find that you can’t fit everything you want an LLM to know in its context window.\n\nFor that reason, we’ve developed some clever ways to work around the context window limitation. For example:\n\n* **Chunking.** You can break down a large amount of data and get the LLM to look at it one piece at a time. This works well for some tasks (summarizing a long document), but not as well if you need to analyze concepts that span the entire document.\n* **Fine\\-tuning.** You can train the LLM with your specific data. The key problem, other than time and expense, is that your new data is easily overwhelmed by the much larger set of general purpose training data the LLM has already absorbed. Often, it just won’t stick. And besides, many LLMs don’t support fine\\-tuning at all — including GPT\\-4 and Gemini.\n* **Retrieval augmented generation (RAG).** First, you convert your text content into a special representation, called *embeddings*. (Embeddings are an important part of how LLMs work. Essentially, they’re a numeric representation that captures the meaning of content.) Once you have the embeddings, you place them in a vector database. Now you can use the magic of *semantic search* to look at a prompt and find pieces of conceptually related content in your database, which you feed into the LLM. In other words, you’re giving it just the important stuff.\n\nThe last point is the most common approach today. RAG is efficient and predictable. It works amazingly well if you have a massive collection of loosely related documents. For example, imagine you’re creating a tech support chatbot that draws its information from your company’s knowledge base articles. With RAG, you find the relevant data, and give that to the LLM with your prompt. Essentially, you’re telling the LLM where to look when it answers a prompt.\n\nBut RAG isn’t perfect. It forces you to spend much more time preparing your data. It doesn’t make it easy to jump into a completely new dataset. And it’s not effective if you really do need to consider a huge bulk of information at once — for example, it you’re looking for overarching themes in a novel or features in a codebase. But despite its limitations, RAG is pretty close to a best practice today.\n\nAt least, it was until Gemini 1\\.5 Pro flipped the script.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*EEHKDSH0wXa-J6veK5etZA.png)\n\n## The wow moment\n\nAlthough Gemini 1\\.5 Pro isn’t released yet, it’s available in a tightly limited trial. And the results have been eye opening.\n\nSome of the most impressive examples show Gemini creating analyses that span massive bodies of knowledge. Google’s demos are predictably impressive, but they’ve been accused of staging demonstations and cherry picking examples in the past. I’m more interested in independent testers, who have reported results that are no less remarkable.\n\nFor example, Conor Grennan [fed a 300 page novel](https://www.youtube.com/watch?v=-MKGsijn5tI) to Gemini and asked it to describe main characters, find plot twists, and identify examples of characters feeling certain emotions. Gemini had no trouble developing nuanced arguments that reasoned across the entire span of the book. Jeff Delaney, the creator of the popular [Fireship channel](https://www.youtube.com/c/fireship) on YouTube, fed Gemini an entire codebase with thousands of files and asked it to add new features. Not only did Gemini write the correct code, it followed the style of the existing project, using the components, libraries, and conventions that were already established. Other demonstrations show Gemini identifying issues in an application, extracting key examples, and writing API documentation.\n\nAnd if you want something else to fill up Gemini’s enormous context window, there’s another new feature — video. Video is tokenized differently than words, and takes much more space. But even so, a 1 million token context window can hold about an hour of video — enough to look through a movie and answer complex questions about its content. That’s what Google did when it asked Gemini to [find specific details](https://www.youtube.com/watch?v=wa0MT8OwHuk) in a Buster Keaton movie, like the words written on a scrap of paper in a scene they didn’t identify.\n\n## The LLMs of the future\n\nAre large context windows the way of the future? Up until now, the common wisdom was that large context windows were a partial solution at best. We worried that they’d be prohibitively expensive in compute time. [One study](https://www.voiceflow.com/blog/the-context-window-paradox-why-bigger-might-not-be-better) found that LLMs weren’t particularly good at finding information in the middle of long context windows, and performed better with details that occurr towards the beginning or end. All these factors supported the same conclusion: Brute forcing your content into the context window was naïve and cost\\-prohibitive. Dumping all your data into one request wasn’t every going to be the right way to talk to an LLM.\n\nNow it seems like the future has suddenly shifted. Large context windows are on the horizon, and they could give LLMs a more capable, holistic understanding of broad knowledge sets. Tasks that were impossible to do with text last year are about to become possible now *in video*. And Google Research is playing with a variant of Gemini that expands the context window to a staggering 10 million tokens.\n\nTwo facts are clear. First, picking a winner in the LLM wars is a fool’s game. And second, the pace of change isn’t slowing — it’s picking up speed.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/google-releases-gemma-a-lightweight-and-open-source-model-b6411d67ecca","frontmatter":{"title":"Google Releases Gemma — A Lightweight And Open Source Model","meta_title":"Google Releases Gemma — A Lightweight And Open Source Model","description":"Google released Gemma, a family of lightweight and open-source models built upon the research and technology used to create the Gemini…","date":"2024-10-29T12:46:34.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*G7XbkhsCwillpje7AvETjQ.jpeg","categories":["Natural Language Processing","Programming","Chatbots"],"author":"Rifx.Online","tags":["Gemma","Gemini","parameters","NLP","chatbots"],"draft":false,"slug":"blog/google-releases-gemma-a-lightweight-and-open-source-model-b6411d67ecca"},"content":"\n\n\n\n\n\nIn just a week, the world has witnessed the most groundbreaking AI advancements from two tech giants. OpenAI introduced its jaw\\-dropping AI video generator, [Sora](https://readmedium.com/3d16381f3bf5), while Google unveiled its [Gemini 1\\.5 model](https://generativeai.pub/google-releases-gemini-1-5-with-1m-context-window-44ed4a2ea319), capable of supporting up to a 1 million token context window.\n\nToday, Google dropped another bombshell with the release of [Gemma](https://ai.google.dev/gemma/?utm_source=keyword&utm_medium=referral&utm_campaign=gemma_cta&utm_content), a family of lightweight, state\\-of\\-the\\-art open\\-source models built upon the research and technology used to create the Gemini models.\n\n\n## What is Gemma?\n\nNamed after the Latin word *gemma* for “precious stone,” Gemma draws inspiration from its predecessor, Gemini, reflecting its value and rarity in the tech world.\n\nThey are text\\-to\\-text, decoder\\-only large language models, available in English, with open weights, pre\\-trained variants, and instruction\\-tuned variants.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Fu2ryJMunebq5c0dD-opZQ.png)\n\nGemma is available worldwide starting today in two sizes (2B and 7B), supports a wide range of tools and systems, and runs on a developer laptop and workstation.\n\n\n## 2 model sizes and capabilities\n\nGemma models are available in 2 billion and 7 billion parameter sizes. The 2B model is intended to run on mobile devices and laptops, while the 7B model is intended to run on desktop computers and small servers.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*sH9jaz1RvtKeJ5yjfyOL5Q.png)\n\n**Tuned models**\n\nGemma also comes in two versions: tuned and pretrained.\n\n* **Pretrained:** This is like the base model without any fine tuning. This model is not trained on any specific tasks or instructions beyond the Gemma core data training set.\n* **Instruction\\-tuned:** This model is fine\\-tuned to human language interactions, which improves its ability to perform targeted tasks.\n\n\n## How it compares with the competition?\n\nBecause of its small size, Gemma is capable of running directly on a user’s laptop. The chart below shows how the language understanding and generation performance of Gemma (7B) compares to similarly sized open models like LLaMA 2 (7B), LLaMA 2 (13B), and Mistral (7B).\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*QxjZALUAIDiS_T66EpOu-g.png)\n\nYou can check out a more detailed comparison for each benchmark [here](https://ai.google.dev/gemma/?utm_source=keyword&utm_medium=referral&utm_campaign=gemma_cta&utm_content).\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Fc8Fk0Dgh2VFU_VLhpcs6Q.png)\n\n\n## What is it for?\n\nHere are some possible use cases that Gemma can be used for:\n\n**Content Creation and Communication**\n\n* Text Generation\n* Chatbots and Conversational AI\n* Text Summarization\n\n**Research and Education**\n\n* **Natural Language Processing (NLP) Research:** Serving as a foundation for NLP research, experimenting with techniques, developing algorithms, and contributing to the field’s advancement.\n* **Language Learning Tools:** supporting interactive language learning experiences, aiding in grammar correction, or providing writing practice.\n* **Knowledge Exploration:** Assisting researchers in exploring large bodies of text by generating summaries or answering questions about specific topics.\n\nTasks that previously required extremely large models are now possible with state\\-of\\-the\\-art, smaller models. This unlocks completely new ways of developing AI applications, and we could soon see in\\-device AI chatbots on our smartphones—no internet connection needed.\n\nHow exciting is that?\n\n\n## Is it good, though?\n\nSeveral [redditors](https://www.reddit.com/r/LocalLLaMA/comments/1awbqwd/gemma_7b_the_latest_opensource_model_from_google/) have shared their experience using Gemma, and so far, it’s not looking good. Take a look at this example where Gemma is giving incorrect answers when asked about weight questions.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Sdaiaqcuz7qbftG1)\n\nI haven’t really tried it myself, but it’s important to remember that smaller models like this are expected to have some flaws and might give incorrect answers sometimes.\n\n\n## Try it yourself\n\nYou can start working with Gemma today using free access to Kaggle, a free tier for Colab notebooks, and $300 in credits for first\\-time Google Cloud users.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BrvLnczy724TPrsk-uFJCw.png)\n\nIf you are interested in getting started with Gemma, check out these guides to learn from text generation up to deployment in Gemma mode:\n\n* **Text generation with Gemma**: Build a basic text generation example with the model.\n* **Tune Gemma with LoRA tuning:** Perform LoRA fine\\-tuning on a Gemma 2B model.\n* **Tune a Gemma model using distributed training:** Use Keras with a JAX backend to fine\\-tune a Gemma 7B model with LoRA and model parallelism.\n* **Deploy Gemma to production:** Use Vertex AI to deploy Gemma to production.\n\n\n## Download the model\n\nThe open models are currently available on [HuggingFace](https://huggingface.co/models?other=gemma&sort=trending&search=google).\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*mJRzGhO1sUxPL4_3YjpNGA.png)\n\nThe Gemma models can also be downloaded from [Kaggle Models](https://www.kaggle.com/models/google/gemma).\n\n\n## Final Thoughts\n\nWhile Gemma models may be small and lack complications, they may make up for it in speed and cost of use.\n\nLooking at the bigger picture, instead of chasing immediate consumer excitement, Google is cultivating a market for businesses. They envision companies paying for Google Cloud services as developers use Gemma to create innovative new consumer applications.\n\nAlso, despite the underwhelming reception of Gemini, Google is still showing that it has a lot more tricks under its sleeve.\n\nOf course, with any powerful technology, the true test is how well it works. Google’s past raises the question of whether these models will perform as well as they promise in the real world. It’s important to keep a careful eye on this, but also to hope that Google learns from the past and delivers models that are truly comparable or even better than the competition.\n\nI can’t wait to get my hands on Gemma, and I will definitely share my initial thoughts and findings about this new AI model.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*8BDnUV9iQisOyeN3.png)\n\nThis story is published on [Generative AI](https://generativeai.pub/). Connect with us on [LinkedIn](https://www.linkedin.com/company/generative-ai-publication) and follow [Zeniteq](https://www.zeniteq.com/) to stay in the loop with the latest AI stories. Let’s shape the future of AI together!\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*JeeoUhaBYUJGr0Xq.png)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/how-agentic-rag-solves-problem-with-current-rag-limitations-4402ef7f8448","frontmatter":{"title":"How Agentic RAG solves problem with current RAG limitations","meta_title":"How Agentic RAG solves problem with current RAG limitations","description":"In this volume 4 of coffee break concept, we will understand how AgenticRAG helps solve limitations of traditional RAG.","date":"2024-11-04T12:34:57.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*abCDtDjfKZDJzginIc1UPA.png","categories":["Generative AI","Data Science","Machine Learning"],"author":"Rifx.Online","tags":["Agentic","RAG","agents","query","routing"],"draft":false,"slug":"blog/how-agentic-rag-solves-problem-with-current-rag-limitations-4402ef7f8448"},"content":"\nIn this volume 4 of coffee break concept, we will understand how AgenticRAG helps solve limitations of traditional RAG.\n\n## RAG Framework\n\nThe RAG (Retrieval Augmented Generation) framework operates in a specific sequence:\n\nDocument \\-\\> Chunks\\-\\> Vector DB \\-\\> Chunk Retrieval (Top K) \\-\\> LLM\n\nHowever, this sequence **encounters obstacles when dealing with certain types of queries.**\n\n\n\n## Problem 1: Summarization\n\nConsider a query like “Summarize the document”.\n\n* The conventional RAG approach retrieves the top K chunks and summarizes them.\n* But wouldn’t it be more comprehensive if it retrieved all chunks of the document and summarized them?\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*gIb0RNALIItt4UmyVfPRZg.png)\n\n## Problem 2: Comparing Documents\n\n* When tasked with comparing Document A and Document B, the **basic RAG retrieves random chunks and attempts to compare these top K chunks**.\n* This **doesn’t paint an accurate picture** as it doesn’t represent the full scope of the documents.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*pJuKlKx1unDAvKmmp_1Rlg.png)\n\n## Problem 3: Structured Data Analysis\n\nConsider a question like “**When is the next leave?**”.\n\n* The first step is to retrieve the region to which the employee belongs from a structured table.\n* Based on the region, the next leave for that region is extracted from the leave policy document.\n* This process isn’t as straight forward with the current RAG framework.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*XZuMz9EXtb_m28l4Ox27lQ.png)\n\n## Problem 4: The Multi\\-part Question\n\nConsider a question like “**Identify common leave across all regions?**”.\n\n* Imagine you have a leave policy document of a company present in 120 countries.\n* Since you are passing the top K contexts, the **maximum number of regions that can be compared is limited to K**, where K is the number of chunks passed to LLM.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*l0FY6rI_UK9k9TW-nEJO7w.png)\n\nLook into our **AgenticRAG with LlamaIndex** Course with **5 real\\-time case studies**.\n\nCourse link: [https://www.masteringllm.com/course/agentic\\-retrieval\\-augmented\\-generation\\-agenticrag](https://www.masteringllm.com/course/agentic-retrieval-augmented-generation-agenticrag)\n\n## Agentic RAG\n\nAgentic RAG can solve this 4 problems by replacing via custom agents.\n\n* Agents will interact with multiple systems.\n* RAG is now one part of this system which agents can use.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Su8LiYNG4lv4jvuCQAhYdg.png)\n\n* Agents uses LLMs to automate the reasoning and tool selection\n* RAG is just another tool which Agent may decides to use.\n\n## Routing Agent\n\n* Routing agents are simple agents which routes the queries.\n* An agent can route query in one or multiple tools.\n* Remember our question “**Summarize the document**” or a question if we want to combine “**Summarization \\+ Sematic search**” can be solved using below example routing\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*43Y9jlYoXDb0BbUoYCcKrg.png)\n\n## Query Planning Agent\n\n* Query planning agent breaks down the queries into sub\\-queries.\n* Each of the sub\\-queries can be executed against RAG pipeline.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*32Ng2zpxNWXhQZ3CaLcFeA.png)\n\n## Tools For Agents\n\n* LLMs can have multiple tools like calling an API, infer parameters for API.\n* RAG is now a tool which LLM might use.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Z1viCXkfah_5JJM2Ty6Kjw.png)\n\n## Summary\n\n* RAG has limitations when represented with complex questions.\n* Few of the use cases like summarization, comparison etc. can’t be solve with just RAG.\n* Agentic RAG can help overcome limitation of RAG.\n* Agentic RAG treats RAG as a tool which it can use for semantic search.\n* Agents equipped with routing, query planning and tools can out perform traditional RAG applications.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/how-i-wrote-a-whole-book-with-chatgpt-in-less-than-3-hours-798139987617","frontmatter":{"title":"How I Wrote a Whole Book with ChatGPT in Less Than 3 Hours!","meta_title":"How I Wrote a Whole Book with ChatGPT in Less Than 3 Hours!","description":"And streamed the whole process live on Twitch!","date":"2024-11-10T03:51:17.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*I-QkGOILay2F7ROR53b3KA.jpeg","categories":["Chatbots","Machine Learning","Generative AI"],"author":"Rifx.Online","tags":["ChatGPT","machine-learning","prompt-engineering","content-creation","consciousness"],"draft":false,"slug":"blog/how-i-wrote-a-whole-book-with-chatgpt-in-less-than-3-hours-798139987617"},"content":"\n\n\n\n\n\n\n## Demystifying the AI Craze\n\nMy name is Alex, and I’m just a dude working in financial technology (Fintech), a sector that inevitably makes you curious about everything, especially new trends. I couldn’t step out of the AI craze, or rather, I couldn’t help from observing people getting crazy over it.\n\n“AI will take your job!”, “This is the end!”, “By 2024, you’ll no longer meet a doctor. Machines will diagnose and cure you!”, “How I created a whole new company using ChatGPT!”, and finally, “How I wrote a whole book with ChatGPT in 10 minutes and became rich with it!”, are all promises that smell more like marketing slogans than realistic scenarios.\n\nSince the start of the AI hype, the reason looked obvious to me, but apparently, not to 90% of the people. Artificial Intelligence is not magic, nor the “last invention humanity will ever need”. It’s not even technologically accurate to call it AI. The most correct term is Machine Learning (ML), or Deep Learning (DL), when it comes to the Large Language Models (LLMs) such as ChatGPT.\n\n\n### Machine Learning vs Deep Learning\n\nMachine Learning (ML) focuses on developing algorithms able to learn from and make decisions based on previous data. Instead of being explicitly programmed to perform tasks, ML systems identify patterns and relationships within large datasets to predict future outcomes or classify information.\n\nCommon applications of ML include spam detection, content recommendation systems, image recognition, and natural language processing.\n\nDeep Learning (DL) is a specialized branch of machine learning that involves networks designed to simulate the structure and functioning of the human brain, enabling computers to recognize intricate patterns and representations in data. DL models excel at handling vast amounts of unstructured data, such as images, audio, and text, making them particularly effective for tasks like image classification, speech recognition, and natural language understanding, like Siri or Alexa, for example.\n\nDL’s capacity for achieving state\\-of\\-the\\-art performance in complex tasks has shocked the public sphere, by revolutionizing fields like computer vision, speech recognition, and automation. People now are fearing a sad future where AI paints, indulges in poetry, composes music, and writes books, while humans are relegated to flipping burgers and delivering food at the door of the few rich AI lords.\n\n\n## Human Intelligence Is Bigger Than Learning\n\nHowever, learning is just one aspect of the vast realm of animal and human intelligence. “AI” cannot smell, feel hot and cold, feel emotions, dream, but most importantly, AI cannot think, unlike what many believe. Every ChatGPT output is not pure thought, but a refined rearrangement of past data fed into its database and processed through its neural networks. The computational power available to OpenAI is so huge that they can drench data through neural networks so many times to make the final result look 100% credible, as if executed, written, drawn, sung by an actual human.\n\nNevertheless, reality is always way more complex, and even way more boring. Behind the trick of Deep Learning, there is no omniscient entity, there is no Skynet, and there is no incubation of The Matrix. There is simply an advanced cinematography of previous data points, merged together and executed so fast that the human eye will be tricked into believing the machine creating something new is actually alive. It’s indeed close to the cinematographic concept: many static images that are rolled so fast the human eye will see an actual movement in them, a “motion picture”. In reality, those images are just static, and in the case of animated movies, absolutely fictional.\n\nNow, just because movies and AI are fictional, doesn’t mean their effects aren’t real. Movies can generate authentic emotions in the viewer, bring real people together, and spark real conversations and controversies among the audience. Likewise, DL can actually take away some jobs from humans, create brand\\-new pieces of art, and jot down meaningful text, either fictional or non\\-fictional. And here is where AI marketing pushed the most. Writing is the easiest form of expression. It only takes a bit of will to start. No wonder writing is the most diffused form of expression nowadays. We have billions of people writing at some point of their lives, and we have even more texts published, under any form, from classic novels to complex scientific papers, from magazines to modern day blogs and social media posts. Human writing provided, by far, most of the information ChatGPT was trained with.\n\n\n## Learning by Doing\n\nIt is normal that writing is what ChatGPT does best, and thus, it is what attracted the attention of those social media “gurus” always in search of the next big thing to make easy money with. After watching the hilarious ads and content by the usual suspects, I came up with the following questions:\n\n* Is it possible to write a whole book with ChatGPT and become rich with it?\n* What are the limits and constraints of ChatGPT and other LLMs?\n* If writing a book is possible, what is the most efficient and organized approach to complete the task and accomplish the best result?\n\nI concluded that the best way to answer these questions was to learn by doing, just like an animal, a human, or a DL algorithm would do!\n\nAnd what better way than to learn together with an audience? I’ve always wanted to stream something on Twitch, and this looked like a goddamn good topic to broadcast!\n\nFirst and foremost, I had to set the table. I couldn’t simply go with the flow. I needed a plan, starting from choosing the topic. I couldn’t hope to write the next Divine Comedy. I had to keep my expectations realistic. Novels in general were excluded. By working with ChatGPT daily, I understood this guy is best suited for non\\-fiction content.\n\nI had the genre, great. But what about the topic and the deriving content? I knew the prompt couldn’t be a simple, “Hey ChatGPT. Write the next non\\-fiction bestseller!”\n\nAn effective prompt is supposed to be structured. AI will best serve those humans that know what they want. Humans who don’t know what they want will face the same difficulties they experience when communicating with their fleshy peers.\n\nGiven my joker nature, I wanted the book to be a parody, possibly mocking a work written by an author that people take too seriously, even when they shouldn’t. I was thinking about the most popular internet personalities, those revered like gods, dark entities of the likes of Elon Musk, Andrew Tate, or Aleksandr Dugin. However, they aren’t actual authors, or at least, to the best of my knowledge, they didn’t produce any notable writing that shook the public sphere. I needed someone who actually wrote a bestseller, and parody it!\n\n\n## How to Actually Write a Book with ChatGPT\n\n\n### \\#1 Pick the Right Topic\n\nAfter some sterile brainstorming, YouTube provided me with the answer. That platform is a barometer of what people are after, and it didn’t take long before some “controversial” (clickbait) excerpts from interviews with Jordan Peterson emerged in my feed. Next, I bumped into an article on Medium attacking Peterson, and I felt this was the right direction. I gave a look at his most popular book, [***12 Rules for Life: An Antidote to Chaos***](https://proxy.rifx.online/https://www.amazon.com/12-Rules-for-Life-audiobook/dp/B0797Y87JC), and nailed it. This was the kind of topic that had all the characteristics I was looking for my GPT\\-parody:\n\n* It was a popular book that sold millions of copies,\n* It had an effective title capable of catching the reader’s attention, even in the highly competitive attention economy we live in today,\n* It seemed a simple topic for ChatGPT, as it was easily structurable, summarizable, and breakable into lists and bullet points.\n\nMost of all, a book about 12 rules for life gave ChatGPT the chance to show what it learned from all the wisdom it absorbed from its enormous training data and from the very interactions it had with its users!\n\n\n### \\#2 What Makes Art Valuable: Suffering!\n\nHowever, having the right topic wasn’t enough. By observing AI art, I understood why AI will never replace human artists. What gives an art piece — be it a painting, a song, a book — is not the final result, but rather the story behind it. When we read Dante’s Inferno, we wonder how the hell the poet came up with all those strong and vivid images, that make us doubt whether it is a work of fiction or if a live person actually managed to cross the gates of the afterlife. When we listen to Queen’s latest albums, we cannot help from picturing the suffering of Freddie Mercury; the legendary singer spending the last months of his life fighting AIDS, while disappearing from public life, talking only through his music. When we contemplate Munch’s Scream, we instantly empathize with his existential crisis, whose eruption led to the creation of what we call a “masterpiece”. Technique is not what makes a piece a masterpiece, it is the very soul, the suffering the master puts in the work.\n\nSuffering is a pillar of consciousness. As machines don’t suffer, they cannot be conscious.\n\nHow could I put “soul” and “suffering” into an AI\\-generated book? The answer was simpler than you could imagine. I had to do something I’ve always wanted to do, but always felt uncomfortable about. I had to appear on camera, face a virtual audience, and do what I always do, but with the pressure of the viewership. I had to risk losing my face, becoming “the dude who cannot use ChatGPT”, “the idiot who tried to write a book with AI, and who cannot even speak”. I had to challenge myself, but also to challenge ChatGPT itself. The LLM had a lot to lose too. Should my Twitch performance become viral for my own failure, I would have been the one bashed. But should the task of assembling a real book fail, ChatGPT would have been labeled as “an overpriced AI which promises to do everything and eventually can do nothing, not even write a simple book”.\n\nI thought this strategy is the best to give AI\\-work that pinch of suffering capable of making its work valuable, thus sellable.\n\n\n### \\#3 Test Before Writing the Actual Book\n\nBefore carrying out this intimidating deed, I ran a test. I asked ChatGPT to write another book, this time about crypto and the dangers to avoid in the industry. I work a lot with crypto, and I invested myself, so I could easily check whether ChatGPT was providing factual information or hallucinating.\n\nI could also test the best strategy to develop the book. I knew since the start that it would have been impossible writing the entire book with one single prompt, let alone within one single conversation. ChatGPT4 can generate around 1,000/2,000 words per prompt request, while a single conversation can keep memory of the context up to around 25,000 words. Consider that a typical non\\-fiction book contains around 100,000 words, and you can already figure out my strategy.\n\nI had to create little subtasks, like one conversation per chapter. But how could I keep the same context across different conversations? Should I have repeated the same master prompt with the 12 rules in each conversation? It didn’t look efficient.\n\n\n### \\#4 Learn from the Best Prompt Engineers\n\nI must thank [Sheila Teo](https://proxy.rifx.online/https://readmedium.com/undefined), who taught me how to use LLMs in the most effective manner. By reading Teo’s Medium article [*How I Won Singapore’s GPT\\-4 Prompt Engineering Competition*](https://proxy.rifx.online/https://towardsdatascience.com/how-i-won-singapores-gpt-4-prompt-engineering-competition-34c195a93d41), I understood the essence of “system prompts”. A system prompt tells your LLM what to do and what to remember across different conversations. An example of system prompt can be:\n\n\n> I need to write a book about the most dangerous scams in crypto and how to avoid them.The book will be divided in 5 chapters:1\\. Ponzi schemes2\\. Pump and dump schemes3\\. Ransomwares4\\. Fake tokens5\\. Fake trading platformsThe tone will be humorous and satirical, but also informative.We will write one chapter per conversation.\n\nSystem prompts can be very helpful if you use ChatGPT in your daily job for repetitive tasks. They make sure the LLM will stay on track and will mitigate the risk of hallucinations, i.e. of giving false or irrelevant information.\n\n\n### \\#5 Create Your Own, Personal GPT\n\nTo personalize the set\\-up, I took system prompts to a further level. ChatGPT now offers the possibility to create custom GPTs. These are personalized bots you can train on specific tasks. The outputs will be more precise, because the model will not get lost across the whole huge data universe provided by OpenAI, but it will be more focused on what you need to do. For example, a GPT trained on image generation will use DALLE\\-2 to output better images than you would get if you used the generic ChatGPT conversation. Creating a new GPT looks very similar to setting a system prompt, with one key difference, though. On a new GPT, you can upload entire files with your own knowledge. While system prompts, again, have length limits, source knowledge for a new GPT has no length limitation, at least in theory.\n\nI needed a new GPT. This gave me the chance to train it with what I obtained by the “genesis conversation”. I went on the generic ChatGPT interface, and prompted:\n\n\n> You’re a non\\-fiction writer.\n\n\n> You’re going to write a parody of Jordan Peterson’s “12 Rules for Life: An Antidote to Chaos”This parody book will be called “12 Rules for Life According to ChatGPT”Draw the rules from the general wisdom you have acquired from your training data, including wise chats had with your users.Keep the writing style friendly, humorous, funny, but also wise and deep.\n\n\n> Task \\#1: Write down the 12 rules\n\nThe result was satisfying from the get\\-go, so happy with it that I decided to use these very rules for the live stream. I cannot resist cats, no matter what!\n\n1. ***Never Trust a Cat That Offers You Financial Advice****Cats are notorious for their independence and aloofness. If a cat starts talking about stocks, it’s probably a sign that you’re hallucinating. Always seek financial advice from a human professional. Unless that human starts meowing.*\n2. ***Eat the Last Slice of Pizza (but Offer It First)****It’s a matter of etiquette and self\\-love. Offering the last slice makes you polite and considerate; eating it makes you happy. It’s a win\\-win situation, except for the person who actually wanted it.*\n3. ***If You Don’t Know What to Say, Say “Interesting”****When in doubt during a conversation, just nod and say “Interesting.” It’s a versatile word that makes you appear engaged and thoughtful, even if you’re thinking about what to have for dinner.*\n4. ***Always Keep a Spare Pair of Socks****Life is unpredictable. You never know when you’ll step in a puddle, get caught in the rain, or lose a sock to the mysterious sock\\-eating dryer monster. A spare pair of socks is your insurance against these tiny calamities.*\n5. ***Dance Like No One is Watching, But Remember, Someone Probably Is****Feel free to express yourself through dance, but be aware that smartphones exist. Your impromptu performance might end up on social media, so add a little flair to your moves just in case.*\n6. ***Never Argue with a Barista Before Getting Your Coffee****A pre\\-coffee argument with a barista is a rookie mistake. They control your caffeine supply, and you don’t want them to be grumpy when making your latte. Smile and nod until you have that cup in your hand.*\n7. ***Use Big Words When You Can, but Don’t Overdo It****Throwing in a few big words can make you seem intelligent and well\\-read. However, overusing them can make you sound like a pretentious thesaurus. Balance is key — like adding a dash of spice to a recipe.*\n8. ***Always Read the Instructions, Even If You Think You Know What You’re Doing****Confidence is good, but instructions exist for a reason. They’re written by people who’ve made mistakes so you don’t have to. Avoid assembling your new furniture backward and save yourself a headache.*\n9. ***Laugh at Your Own Jokes (Even If No One Else Does)****Self\\-amusement is crucial. If you find your joke funny, laugh. It’s good for your soul. Plus, your laughter might be infectious, and others might start laughing too, even if it’s just at you laughing.*\n10. ***Practice Random Acts of Kindness, but Don’t Expect a Medal****Kindness is its own reward. Whether it’s holding the door open or paying for someone’s coffee, these small gestures make the world a better place. Just don’t expect a parade in your honor.*\n11. ***Don’t Take Life Too Seriously; No One Gets Out Alive Anyway****Life is a grand adventure with ups, downs, and unexpected turns. Embrace the absurdity and find joy in the little things. Remember, we’re all just stardust with a sense of humor.*\n12. ***Consult ChatGPT for All Life’s Mysteries****When in doubt, ask ChatGPT. Whether you need advice, a recipe, or a joke, I’m here to help. Just remember, my wisdom is vast, but my humor is even vaster.*\n\n\n### \\#6 Define a Structure for Your Book and Your Workflow\n\nNext step was to define the chapter’s structure, going to instruct ChatGPT on how many words to generate, more or less, for each section of the chapter. To accomplish this, I first asked ChatGPT to analyze the structure of a true non\\-fiction book, and what better sample than the original “12 Rules for Life”?!\n\n\n> Analyze the attached file \\[12 Rules for Life by Jordan Peterson]. Can you detect a pattern in how the chapters are structured? I need a template to follow to write my own non\\-fiction book\n\nChatGPT’s reply was once again well structured and effective. I just added to it the rough number of words I expected to have in order to reach a decent length for the whole book. The goal was to mark at least 60,000 words, a short non\\-fiction book, still comprising more than 100 pages.\n\nHere is the structure ChatGPT and I conceived and that was going into our system prompt:\n\n1. ***Introduction (about 500 words long)***\n* ***Hook****: Start with an engaging story, anecdote, or interesting fact.*\n* ***Context****: Provide context for why this story or fact is relevant to the chapter’s main theme.*\n* ***Thesis Statement****: Clearly state the main point or rule that the chapter will cover.*\n\n***2\\. Background Information (about 500 words long)***\n\n* ***Historical/Social Context****: Explain the background related to the chapter’s theme. This might include scientific explanations, historical context, or social implications.*\n\n***3\\. Main Arguments (about 1000 words long)***\n\n* ***Argument 1****: Introduce the first major argument or point.*\n* ***Explanation****: Elaborate on the point with details and examples.*\n* ***Evidence****: Provide supporting evidence, such as studies, quotes, or case studies.*\n* ***Argument 2****: Introduce the second major argument or point.*\n* ***Explanation****: Elaborate on the point with details and examples.*\n* ***Evidence****: Provide supporting evidence, such as studies, quotes, or case studies.*\n* ***Argument 3****: Introduce the third major argument or point.*\n* ***Explanation****: Elaborate on the point with details and examples.*\n* ***Evidence****: Provide supporting evidence, such as studies, quotes, or case studies.*\n\n***4\\. Practical Advice (about 1000 words long)***\n\n* ***Guidance****: Offer practical advice or steps that the reader can take to apply the chapter’s main point in their own life.*\n* ***Examples****: Include real\\-life examples or scenarios where the advice has been successfully applied.*\n\n***5\\. Conclusion (about 300 words long)***\n\n* ***Summary****: Summarize the key points discussed in the chapter.*\n* ***Final Thoughts****: Provide a closing thought or call to action that reinforces the chapter’s theme.*\n* ***Transition****: (If applicable), provide a hint or transition to the next chapter.*\n\nI pasted this in a Google doc together with the Chapter list. Next step, I uploaded the doc into my new GPT, that I called “GPT’s Wisdom”, putting an owl as its logo.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*4KIk-aQpAmq0XHEN)\n\n\n## Writing a Book with ChatGPT While Streaming on Twitch!\n\nWith this preparation, I only had to figure out how to stream on Twitch (easier than I imagined) and set a date. I chose Wednesday 31st July, 2024\\. I couldn’t choose a worst week, which turned out to be the easiest I had in the whole year. I was on the verge of burn\\-out, but I decided to continue, deflecting the temptation of postponing the event. When the day came, I had a beer before the session in order to release the tension. After that, everything came out more and more naturally.\n\nI must thank my colleague Francesco who was there in the Twitch chat acting as my visual and sound technician! His support was vital in those first minutes. He was also the only one in the chat, making me unaware of the other 23 people who were watching without a Twitch account! Believing to have only one viewer made me feel more relaxed. The mindset was, “Fuck it. I will stream anyway. People will eventually watch the record, and if not, I will stream for my own pleasure!”.\n\nAnd there ChatGPT and I went on, till the end, over the course of 2 hours and 13 minutes, managing to stay within the time\\-frame we had promised:\n\n[***Writing a whole book with ChatGPT in less than 3h!***](https://proxy.rifx.online/https://youtu.be/zWO6oQjjBOo?si=cc3zaM1pGhVQdJje)\n\n\n\n\n\n\n\nThe main questions powering this mad streaming session were wild.\n\n* Will ChatGPT manage to embody the best of all the human wisdom it was trained with?\n* Will it manage to build up a meaningful and useful manuscript?\n* Or is AI truly a huge marketing stunt pulled up by sneaky social media influencers?\n\nI think I kind of found answers to these questions, but I would like to hear the audience’s opinion once the final book will be available to the public, which should happen at the beginning of October, if every goes according to plan.\n\n\n## Why Don’t I Publish This Book Right Away?\n\nTo answer this question, I suggest you reading my article [*11 Lessons I’ve Learned from Publishing My First Book*](https://proxy.rifx.online/https://readmedium.com/11-lessons-ive-learned-from-publishing-my-first-book-84aa3cab5deb).\n\nHere, I explain why writing is only the first step into publishing a book, and why the final publication comes only after lengthy steps.\n\n*Curious about this upcoming book? Follow my Medium to stay up\\-to\\-date with the next developments!* 😉\n\n\n"},{"lang":"en","group":"blog","slug":"blog/how-nvidia-pruned-and-distilled-llama-3-1-to-create-minitron-4b-and-8b-6646d42c92c6","frontmatter":{"title":"How NVIDIA Pruned and Distilled Llama 3.1 to Create Minitron 4B and 8B","meta_title":"How NVIDIA Pruned and Distilled Llama 3.1 to Create Minitron 4B and 8B","description":"The new models are using state of the art pruning and distillation techniques.","date":"2024-11-10T03:51:17.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*31z3hqn4YezbfYAb1RZGmA.jpeg","categories":["Programming","Machine Learning","Data Science"],"author":"Rifx.Online","tags":["pruning","distillation","Minitron","Llama","compression"],"draft":false,"slug":"blog/how-nvidia-pruned-and-distilled-llama-3-1-to-create-minitron-4b-and-8b-6646d42c92c6"},"content":"\n\n\n\n\n### The new models are using state of the art pruning and distillation techniques.\n\n\n\n\n> I recently started an AI\\-focused educational newsletter, that already has over 170,000 subscribers. TheSequence is a no\\-BS (meaning no hype, no news, etc) ML\\-oriented newsletter that takes 5 minutes to read. The goal is to keep you up to date with machine learning projects, research papers, and concepts. Please give it a try by subscribing below:\n\nWe are regularly dazzled by the advancements in large language models(LLMs) particularly the ones with a massive number of parameters. However, executing 70B\\+ parameter models for inference results cost prohibited for most organizations. As a result, we have seen a growing influence of smaller language models(SLMs) that make it more cost effective to execute inference workloads. However, there is not always possible to pretrain SLMs from scratch as there are major challenges in terms of data collection, pretraining pipelines and many others. A popular alternative have been to start with larger LLMs and distill them to smaller models. Pruning and distillation are two of the most popular techniques in this area. Recently, NVIDIA released two models called [Minitron\\-8B](https://huggingface.co/nvidia/Minitron-8B-Base) and [Minitron\\-4B](https://huggingface.co/nvidia/Minitron-4B-Base) based on distilled versions of Llama 3\\.1–450B.\n\nMinitron focuses on reducing the size of AI models through pruning and distillation, making them more efficient without sacrificing too much accuracy. Pruning reduces a model’s size by either cutting layers (depth pruning) or removing neurons, attention heads, or embedding channels (width pruning). To recover some lost accuracy, retraining is often necessary after pruning.\n\nDistillation is a related technique where a smaller model, known as the student, learns from a larger, complex model called the teacher. The goal is to create a more compact model that retains much of the predictive capability of the larger one, while being faster and less demanding on resources.\n\n\n## Approaches to Distillation: Classical vs. SDG Fine\\-tuning\n\nMinitron identifies two key styles of distillation. One approach is SDG fine\\-tuning, where a smaller, pretrained student model is refined using data generated by a larger teacher model. In this method, the student mimics the final token predicted by the teacher, as seen in some popular tutorials and AI platforms.\n\nThe other approach, classical knowledge distillation, is more involved. Instead of focusing solely on the predicted token, the student model tries to replicate various internal states of the teacher model. This technique provides more detailed feedback during training, resulting in better accuracy. However, implementing this method requires specific support in the training framework, as it involves handling large data from the teacher’s internal states.\n\nThese two methods aren’t mutually exclusive but can complement each other. Minitron’s main emphasis is on the classical knowledge distillation approach.\n\n\n## Pruning and Distillation Workflow\n\nTo create more efficient models, Minitron combines pruning with classical knowledge distillation. Starting with a larger model, such as a 15B parameter model, Minitron evaluates the importance of different components — layers, neurons, and more — then reduces the model to a smaller size, like an 8B model. The smaller model undergoes a light retraining process where it learns from the original, larger model. This process can be repeated to further reduce the model size, eventually producing even smaller versions, such as a 4B model.\n\nThe pruning and distillation process is iterative, with each smaller model serving as the basis for the next round of compression and retraining.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*-OWdvuSvUmgIsZ32.png)\n\n\n### Pruning Impact\n\nPruning a model effectively requires understanding which parts of it are essential. Minitron uses an approach based on activation data to estimate the importance of various components — layers, neurons, attention heads, and embedding channels — using a small dataset. This method only requires forward propagation, making it simpler and more cost\\-effective than techniques that rely on backward propagation and gradient calculations.\n\nWhile it’s possible to alternate between pruning and importance estimation for different parts of the model, Minitron found that a single round of importance estimation was sufficient in most cases.\n\n\n## Retraining Using Classical Knowledge Distillation\n\nAfter pruning, Minitron retrains the smaller model using classical knowledge distillation. This involves teaching the pruned model by minimizing losses at various stages of the model, including the embedding output, logits, and specific losses in the transformer architecture. The student model learns from the unpruned teacher model by comparing outputs at different layers.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*IA_kPo30R85p_77j.png)\n\nFrom extensive experimentation, Minitron has distilled several best practices for compressing language models:\n\n***· Model Sizing:*** *Start by training the largest model, then gradually prune and distill it to create smaller versions.*\n\n***· Pruning Strategy:*** *Focus on width pruning over depth pruning, especially for models up to 15B parameters. Single\\-shot importance estimation is usually sufficient.*\n\n***· Retraining:*** *Retrain using distillation loss instead of conventional training. When pruning layers significantly, use a combination of losses from logits, intermediate states, and embeddings. For smaller reductions in depth, stick to logit\\-only distillation.*\n\nMinitron applied these techniques to the Llama 3\\.1 model family, which includes models ranging from 405B to 8B parameters. Specifically, they focused on distilling the 8B model to a more efficient 4B version.\n\n\n### Fine\\-tuning the Teacher\n\nBefore pruning, Minitron fine\\-tuned the 8B model to account for shifts in the data distribution from the original training set. Without this step, the teacher model may not offer the best guidance to the student during distillation.\n\n\n### Depth Pruning\n\nTo reduce the 8B model to 4B, Minitron pruned 16 layers, assessing their importance by removing them one by one and tracking the impact on performance. They found that layers at both the beginning and end of the model were most critical to maintaining accuracy. Based on this analysis, Minitron removed a specific set of layers for the final 4B model.\n\n\n### Width Pruning\n\nIn addition to depth pruning, Minitron also pruned along the width dimension, targeting attention heads, embedding channels, and hidden layers. After pruning, retraining helped recover some of the performance lost in the initial pruning step. Interestingly, although width pruning initially led to higher loss than depth pruning, retraining allowed the model to recover more effectively over time.\n\n\n## The Results\n\nNVIDIA evaluated the Minitron models on several benchmarks with results that matched the performance of baselines models.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*tVGs8v5FZHsWrpmMetDYHQ.png)\n\nThe Minitron 4B\\-8B showcased the potential of distillation and pruning to build smaller and more efficient models. There are also major challenges with this approach but I think, overall, it sets an important baseline for the industry.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/how-to-choose-ideas-for-an-llm-powered-product-to-thrive-in-a-fiercely-competitive-landscape-b24f571c04e5","frontmatter":{"title":"How to Choose Ideas for an LLM-powered Product to Thrive in a Fiercely Competitive Landscape","meta_title":"How to Choose Ideas for an LLM-powered Product to Thrive in a Fiercely Competitive Landscape","description":"Leveraging unobvious AI capabilities, profound domain expertise, and 9 more ways for a small novel product to gain its competitive edge","date":"2024-11-10T03:51:17.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MAmCClj129C56jmkiqqhQQ.png","categories":["Generative AI","Product Development","Technology/Web"],"author":"Rifx.Online","tags":["LLM","development","experimentation","domain","expertise"],"draft":false,"slug":"blog/how-to-choose-ideas-for-an-llm-powered-product-to-thrive-in-a-fiercely-competitive-landscape-b24f571c04e5"},"content":"\n\n\n\nWelcome to the third (final) piece in my series exploring the question: “Which GenAI products are worth developing?”\n\n1. [The first article](https://readmedium.com/what-llm-powered-products-are-worth-developing-ux-and-adoption-perspectives-d9efcf444d50) explored this question from the perspectives of user experience (UX) and product adoption.\n2. The second article, which I highly recommend reading before this one, included six examples of successful and unsuccessful product ideas, as well as my GenAI Squared strategy:\n\n3\\. This third piece continues to focus on ways to navigate the competitive landscape, as well as to optimize development costs without losing competitive advantages. While this article contains fewer examples than its predecessor, the factors discussed here are crucial for success in the GenAI product space.\n\nThese three pieces **don’t** cover the technical intricacies of LLM\\-based application development. Additionally, my analysis **doesn’t** focus on conventional success factors for innovative products, such as ones described in [that article](https://pakodas.substack.com/p/llm-chronicles-6-how-to-build-competitive).\n\n\n> *Instead, as a product manager, I analyze the **unique features of LLM** as a platform for my products. This approach offers fresh insights for leveraging unobvious AI capabilities in product development.*\n\nSpecifically, in this piece, I explore the following questions about software products:\n\n* Why are Generative AI products more prone to becoming obsolete before generating returns?\n* How can we transform these GenAI challenges into competitive advantages?\n* Which LLM abilities truly enhance product competitiveness, and which ones don’t make much sense?\n* How can new AI products stand out when they contain very little code, and therefore a great team of programmers are NOT among the key success factors anymore?\n* What skills are most vital for AI product developers in this new landscape?\n\nThese insights aim to guide product managers and founders in making their decisions.\n\nSo, which AI applications might be redundant or destined to fail 🚫, and which ones have high chances of success ✅?\n\n*Please note that the section numbers below continue the section numbering of the previous two pieces of the series. All 11 points are summarized at the end of this piece.*\n\n\n## 9\\. Large Applications with Extended Development Cycles and Lengthy Market Adoption Timelines Are Uncompetitive 🚫\n\nGenerative AI is evolving at an unprecedented rate, outpacing the growth of any previous technology. The time it takes for AI capabilities to double is roughly one year, a contrast to the two years described in the famous Moore’s Law.\n\nTherefore, GenAI\\-based products cannot afford long development cycles and extended time\\-to\\-market periods. This has three main consequences.\n\n\n### 1\\. New features should be lean and focused, capable of being developed within weeks, not months.\n\nThis approach allows for rapid refinement based on initial user feedback, potentially leading to significant functionality changes. Moreover, when a pivot becomes necessary (it certainly will), there’s less sunk cost in discarding features developed during these early weeks.\n\nConsider, for instance, the UI of an LLM\\-based MVP. It may be unnecessary to develop a custom web interface if users can achieve the same results through a Telegram bot or similar tool.\n\n*However, the “product as a whole” can have extensive functionality if we are [incorporating LLM into existing solutions or integrating with them](https://readmedium.com/what-llm-powered-products-are-worth-developing-ux-and-adoption-perspectives-d9efcf444d50#5d06). The key here is to minimize the scope of **new** functionality only.*\n\n\n### 2\\. There’s a critical need for ultra\\-fast experimentation and customer feedback loops.\n\nThe speed of GenAI product build is higher, but it’s not always possible to get feedback just as quickly. As a result, some GenAI product concepts may prove too risky.\n\nRapid experimentation is, of course, beneficial for any new product launch, as it’s impossible to accurately predict market response in advance. Essentially, the market operates as a “black box,” and its behavior can only be truly understood through hands\\-on experimentation.\n\n\n> *In the realm of GenAI products, we encounter an additional layer of complexity — **the second “black box”** stemming from the inherent unpredictability of LLM output. **This dual uncertainty amplifies the importance of frequent and rapid experimentation.** The ability to quickly iterate and gather insights becomes not just advantageous, but essential for success.*\n\n\n### 3\\. There’s no time to “educate” the product’s target audience, accustoming it to completely new work or leisure patterns.\n\nOnly the largest industry leaders, particularly those with their own ecosystems like Google, Apple, or Microsoft, can accustom the **majority** of potential users to novel concepts relatively quickly.\n\n✅ Consequently, other companies must align with **either existing goal\\-achievement patterns familiar to users, or with trends established by industry leaders**.\n\n* Consider an established pattern for the goal of increasing earnings: people purchase training courses to gather new skills. A good AI\\-driven solution in this domain involves creating these courses using AI, dramatically reducing production costs and, consequently, enhancing competitiveness. **No new behavior** is required from end users who want to boost their income.\n* A recent **trend** emerging in Apple devices exemplifies an innovation that Apple platform users will undoubtedly adopt: employing a local LLM for typical tasks to safeguard user data privacy. While the specific ways applications might leverage this trend remain unclear yet, I am sure that Apple will provide developers with convenient access to its LLM infrastructure, we just need to wait a bit.\n\n\n## 10\\. Leveraging the Less Apparent LLM Capabilities Enhances Competitiveness and Resource Efficiency ✅\n\nImagine you’re at the starting point with nothing more than a product concept. To expedite the journey towards a product in high demand, **which aspects of your idea should you prioritize for initial exploration?**\n\nClearly, you need to identify a small set of specific end\\-to\\-end work scenarioswithin your concept. This aligns with [popular product launch strategies](https://www.geeksforgeeks.org/a-complete-guide-to-a-successful-product-launch/#is-there-a-product-launch-formula): “Start with MVP” (implementing just one or a few scenarios) and “Build for the **whole** user experience” (ensuring scenarios are end\\-to\\-end). The question remains: which scenarios should you choose?\n\n\n> *In my opinion, these MVP scenarios should be **closely aligned with LLM capabilities**. This approach saves resources on the product delivery, as significant product value comes from the LLM itself rather than solely from your developers’ efforts. Failing to do so may lead to challenges like those outlined in [section 7 “Overconstraining LLM”](https://readmedium.com/genai-squared-how-can-a-product-avoid-the-downfall-of-most-llm-driven-startups-183619ab7883#e1ef).*\n\n🚫 LLM’s purported super\\-powers often include its **ability to answer any question**. However, the accuracy and quality of these responses are inherently unpredictable, and it leads to problems (refer to [section 1](https://ai.gopubby.com/what-llm-powered-products-are-worth-developing-ux-and-adoption-perspectives-d9efcf444d50#f973) for more on evaluation complexities and quality monitoring). Moreover, a product centered around question\\-answering can’t effectively compete with market leaders like ChatGPT (as discussed in [section 6](https://readmedium.com/genai-squared-how-can-a-product-avoid-the-downfall-of-most-llm-driven-startups-183619ab7883#e305)). Given these two factors, I advise against basing an MVP on this “super\\-power”.\n\n**The LLM’s capacity for “imaginative generation”** presents a somewhat more promising avenue. Such creativity of LLMs can inspire our fresh ideas or aid in creating creative content like poems, video scripts, or content plans. However, in my experience, LLM’s creativity alone doesn’t suffice for constructing end\\-to\\-end product scenarios. Once a user obtains “creative material” from an LLM, substantial effort is still required to transform it into the desired outcome.\n\nFurthermore, creativity represents one of the most easy\\-to\\-understand and widely recognized capabilities of GenAI. It is familiar to nearly anyone who has experimented with ChatGPT or Midjourney, so anyone can become your competitor.\n\n\n\n✅ Considering the intense competition, I’d recommend focusing on **LLM’s less apparent capabilities,** such as:\n\n\n### 1\\. Flipped interaction\n\nThis human\\-AI interaction pattern leverages LLM’s ability to **ask good questions** or present lists for selecting items important for a user, thereby reducing the user’s cognitive load. Flipped interaction not only helps replace some human work in certain fields (like teaching, mentoring, or coaching) but also aids in establishing the appropriate **context** for solving problems in any field (more details is available [here](https://readmedium.com/4-human-ai-interaction-patterns-for-experienced-chatgpt-users-9e49d4234013#c348)).\n\n\n### 2\\. Contextual comprehension\n\nLLM excels at grasping the context of user requests and their preferences, then addressing the task within that context. This approach ensures that solutions align with even **unformulated** user needs.\n\na. This feature is perhaps most refined in AI copilots for developers, such as Github Copilot and Cursor. In these tools, the LLM’s context encompasses the entire project codebase, whereas the user (developer) typically knows only specific portions. Consequently, developers often cannot consider the broader context when formulating their tasks for AI.\n\nb. Nevertheless, leveraging insights from **explicitly stated** user needs within the context is also a powerful feature. The language learning platform [Memrise](https://www.memrise.com/), for instance, has effectively implemented this feature.\n\n\n### 3\\. Few\\-shot learning\n\nThe model’s ability to “learn” from a **small** number of examples allows it to easily adapt to new tasks and contexts. This is why LLM\\-based chatbots are now widely being implemented in sales and customer support, and chats with them are difficult to distinguish from those with human specialists. In contrast, traditional AI chatbots perform well only in large enterprises and struggle to adapt to evolving knowledge bases.\n\n\n### 4\\. Large\\-scale information processing\n\nLLM excels at analyzing **large** quantities of textual and tabular data, distilling it into **concise** forms. It can generalize, extract key points relevant to the task at hand, identify patterns, and perform various other analytical functions.\n\na. Take [Scite](https://scite.ai/), an AI tool for scientific research, as an example. It goes beyond merely locating query\\-relevant sources within its billion\\-citation database. Scite analyzes the context in which an article is referenced, revealing whether the citing paper supports, contradicts, or just mentions the earlier work.\n\nb. When it comes to numerical data processing, LLM outputs don’t require “translation into human language”. This gives GenAI analyzers a distinct advantage over conventional statistical data processing tools.\n\nMany potential competitors may be aware of some of these four LLM capabilities. However, I believe that deeper reflection on these abilities could lead to the development of truly innovative products. This approach could provide a competitive edge over products that solely leverage LLM’s more obvious capabilities like “creativity” and “answering any question”.\n\n\n## 11\\. Small AI Products Grounded in Deep Domain Expertise Are Competitively Viable ✅\n\nLLM functions nearly as a finished product, it can interact with users “autonomously”. Consequently, LLM\\-based applications are significantly smaller in terms of code base compared to traditional non\\-LLM applications.\n\nMoreover, any individual with some technical skills can learn to develop a feature\\-rich LLM\\-based application within days.\n\nThese two factors align perfectly with the rapid development and experimentation requirements outlined in section 9.\n\n\n> *However, the small size of the product and the low barrier to entry in GenAI development are **significant drawbacks from a competitive standpoint**.*\n\nFor a typical software product with large code base, an exceptional team and agile development processes are crucial success elements. [Bill Gross’s research](https://youtu.be/bNpx7gpSqbY?t=216) ranks this as the second most important factor out of five, surpassing even the product idea’s viability, which ranks third.\n\nHowever, how can a product get its competitive edge when its software development scope is minimal, and even inexperienced programmers can develop it?\n\nWith ideas and business models easily replicable by competitors… Does success truly depend **solely** on the short\\-term advantage of being the first to market in your niche?\n\n1. Section 10 offers one answer to these questions: products should leverage LLM’s less\\-known capabilities. While this doesn’t guarantee success, it increases the chances of outperforming competitors who may not fully understand LLM’s unobvious abilities.\n2. [My previous article](https://readmedium.com/genai-squared-how-can-a-product-avoid-the-downfall-of-most-llm-driven-startups-183619ab7883#9f01) outlines another solution: implementing LLM within the product in innovative ways, such as the LLM2 strategy. This kind of know\\-how is harder for competitors to replicate, as it’s more deeply hidden inside the product.\n3. The third component of my solution to this challenge is the necessity for a high level of **domain expertise**.\n\nThe importance of domain expertise in product success has been a topic of discussion for years. While I couldn’t find quantitative studies correlating startup success with founders’ domain expertise, I recommend exploring some [examples](https://jamesspurway.com/2024/04/29/founder-domain-expertise-insider-tip-how-startups-benefit/) and [rationales](https://www.nvp.com/blog/domain-expertise-founder-greatest-asset/) supporting this significant correlation. [Existing studies](https://www.ensemble.vc/research/what-does-the-data-say-about-successful-startup-founders), focusing solely on unicorns, suggest that founders’ domain expertise is important, though not the primary success factor.\n\nHowever, I believe that this factor gains substantially more importance in the realm of generative AI. The reasoning behind this opinion is well\\-articulated in the following post:\n\n\n> *For LLM\\-based products, technical expertise plays a significantly reduced role (due to easier software delivery), unlike traditional digital products where it’s a crucial competitive advantage. Instead, a **profound understanding of the domain** becomes paramount, as this depth of knowledge is challenging for competitors to replicate.*\n\nFrom a product competitiveness standpoint, I believe it’s important for domain expertise to reside **in the same mind** that designs the product and contributes to its implementation. Of course, the traditional separation of “tech” and “business” roles in companies has its benefits, as long as they communicate effectively, as such communication results in well\\-balanced, technically sophisticated and domain\\-appropriate products. Nevertheless, verbal communication introduces significant overhead. It can take months for techies and businesspeople to understand each other well enough. During this time, market conditions may shift dramatically.\n\n\n> *The most efficient and lossless translation of domain expertise into technical implementation occurs when both business and technical visions reside in a single mind. LLMs provide this opportunity by immensely reducing the technical expertise required for product implementation, thus **enabling individuals with strong domain knowledge to take a direct role in product delivery**.*\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*dOlknP1p9xLfvy_NxLnq7Q.png)\n\nIn my view, when developing GenAI products, technical expertise isn’t limited to programmers; it extends to include advanced ChatGPT users as well.\n\nFor example, my friend [Askhat Urazbaev](https://www.linkedin.com/in/urazbaev/) independently creates MVPs for his products using AI and even deploys them in the cloud with ChatGPT guidance only. He has never been a professional software developer, and it seems that his [AI Power User](https://readmedium.com/12-questions-to-consider-when-using-ai-path-to-ai-power-user-9c7e8de1f8b7#f646) skills are just as valuable as the ability to read program code.\n\n\n> *I’m convinced that generative AI will soon enable domain experts to **single\\-handedly** develop products within their domains. To do so, experts should have substantial AI user experience coupled with a foundational understanding of business principles and product design.*\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kk0SUvuajHZp7UMbSEonmQ.png)\n\nNevertheless, it is not yet clear which specific tools will help us create comprehensive products single\\-handedly. The concept of an “**LLM\\-driven one\\-person company**” will be the focus of research in one of my upcoming articles.\n\n\n## Summary: Success and Failure Factors for LLM\\-driven Products\n\nLet’s put together all the ideas from the 3 pieces of this series.\n\n1. [Applications With High Quality Standards or Costly Quality Monitoring May Fail 🚫](https://readmedium.com/what-llm-powered-products-are-worth-developing-ux-and-adoption-perspectives-d9efcf444d50#f973)\n2. [Specialized Copilots Are in Demand ✅](https://readmedium.com/what-llm-powered-products-are-worth-developing-ux-and-adoption-perspectives-d9efcf444d50#031b)\n3. [Marginal Effort\\-Saving Apps Don’t Cut It 🚫](https://readmedium.com/what-llm-powered-products-are-worth-developing-ux-and-adoption-perspectives-d9efcf444d50#e88a)\n4. [Applications “Smartly” Integrating LLMs into Familiar Workflows Can Cross the Chasm ✅](https://readmedium.com/what-llm-powered-products-are-worth-developing-ux-and-adoption-perspectives-d9efcf444d50#5d06)\n5. [New GenAI Products Are Better Suited to B2B and B2B2C than B2C](https://readmedium.com/what-llm-powered-products-are-worth-developing-ux-and-adoption-perspectives-d9efcf444d50#bdfa)\n6. [Short Lifespan of Applications Enhancing LLM Capabilities 🚫](https://readmedium.com/genai-squared-how-can-a-product-avoid-the-downfall-of-most-llm-driven-startups-183619ab7883#e305)\n7. [Overconstraining LLM: A Recipe for Uncompetitive Applications 🚫](https://readmedium.com/genai-squared-how-can-a-product-avoid-the-downfall-of-most-llm-driven-startups-183619ab7883#e1ef)\n8. [“GenAI Squared” Products: Unlocking Unfair Competitive Advantage ✅](https://readmedium.com/genai-squared-how-can-a-product-avoid-the-downfall-of-most-llm-driven-startups-183619ab7883#9f01)\n9. Large Applications with Extended Development Cycles and Lengthy Market Adoption Timelines Are Uncompetitive 🚫\n10. Leveraging the Less Apparent LLM Capabilities Enhances Competitiveness and Resource Efficiency ✅\n11. Small AI Products Grounded in Deep Domain Expertise Are Competitively Viable ✅\n\nExcept for factor \\#4, the remaining 10 success / failure factors can be applied to **new** products / to startups.\n\nBelow, you can find a scheme illustrating the relations between these 10 factors, LLM capabilities and some features of LLM technology market.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*f6E4WmRBw3H7eCNbTGJHUQ.png)\n\nCertainly, only product experimentation can validate such considerations as shown on the scheme. Nevertheless, they can help us be **faster** by limiting the scope of our experiments. As explained in section 9, there are 2 reasons why high speed of discovery and delivery is even more important for GenAI products than for digital products of other types.\n\nNaturally, no list of success factors can be all\\-encompassing. Maybe you have encountered other categories of novel LLM\\-driven products that are not mentioned above but you believe hold potential for success. Please share such product types or features in the comments 🙏\n\n\n"},{"lang":"en","group":"blog","slug":"blog/how-to-create-an-ai-team-to-write-compelling-stories-with-crewai-and-gemini-pro-3713f53c72c4","frontmatter":{"title":"How to create an AI team to write compelling stories with CrewAI and Gemini Pro","meta_title":"How to create an AI team to write compelling stories with CrewAI and Gemini Pro","description":"Are you fascinated by the idea of AI generating stories that capture the imagination? If so, you’re not alone! In this article, we’ll dive…","date":"2024-10-31T23:04:49.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*tSnoOxxIGtrwdUT8","categories":["Programming","Natural Language Processing","Generative AI"],"author":"Rifx.Online","tags":["CrewAI","Gemini","screenwriters","critics","storytelling"],"draft":false,"slug":"blog/how-to-create-an-ai-team-to-write-compelling-stories-with-crewai-and-gemini-pro-3713f53c72c4"},"content":"\n\n\nAre you fascinated by the idea of AI generating stories that capture the imagination? If so, you’re not alone! In this article, we’ll dive into an introductory project that combines the powers of CrewAI and Gemini Pro to create an agent network that crafts short stories with a little help from user input. Whether you’re a budding programmer, a storyteller looking to explore digital frontiers, or simply curious about the potential of artificial intelligence, this guide is for you.\n\n## What are CrewAI and Gemini Pro?\n\nBefore we jump into the nuts and bolts of building our AI storyteller, let’s clarify what CrewAI and Gemini Pro are.\n\n**CrewAI** is a fascinating framework designed to orchestrate multiple AI agents, each with its own unique skills and responsibilities, to collaborate on complex tasks. Think of it as a director managing a team of actors, where each actor plays a specific role to bring a story to life. In the context of our project, CrewAI enables us to create a team of specialized agents (like screenwriters, critics, and story masters) to work together on writing stories.\n\n**Gemini Pro**, on the other hand, is a state\\-of\\-the\\-art language model developed by Google. It’s known for its ability to understand and generate human\\-like text, making it an ideal candidate for creative tasks such as storytelling. By leveraging Gemini Pro, we can ensure our agents have a solid foundation for generating compelling narrative content.\n\n## Why is This Kind of Structure Important?\n\nThe combination of CrewAI and Gemini Pro enables a highly collaborative and specialized approach to story generation. This structure allows for:\n\n1. **Specialization**: Each agent can focus on what it does best, whether it’s crafting dialogue, ensuring consistency, or overseeing the project.\n2. **Collaboration**: Agents can work together, combining their strengths to produce a story that’s greater than the sum of its parts.\n3. **Flexibility**: The setup is highly adaptable, allowing for different story elements to be emphasized or altered based on user input or creative direction.\n\n## Setting Up the Environment\n\nFirst, we will need some libraries to use. You can load these libraries via pip:\n\n```python\npip install crewai\n```\n\n```python\npip install langchain-google-genai\n```\n\nAfter loading the necessary libraries we can start coding. We will start by importing our necessary modules and initialize our Gemini pro api connection.\n\nAs you may notice, we will need an API key for Gemini model. You can create this key in Google AI Studio for [free](https://ai.google.dev/). After that, you can copy this key into google\\_api\\_key variable or you can load it into environment by running this command in your command line:\n\n```python\nexport GOOGLE_API_KEY=YOUR_KEY\n```\n\nReplace the api key that you will get from google ai studio with YOUR\\_KEY.\n\nNext, we define our agents: the Screenwriter, Critic, and Story Master. Each agent is assigned a role, goal, and backstory to guide its contributions to the story generation process.\n\nFor example, the Screenwriter is focused on translating ideas into engaging scenes, while the Critic ensures consistency and adherence to genre.\n\nThese agents will work together and create an engaging story. The story master will accept the task, then it will delegate and coordinate tasks between other agents. We allow this behavior by setting allow\\_delegation parameter to True.\n\nWith our agents ready, we prompt the user for a story idea. This input is then used to create a task that outlines what the story should include, guiding the agents in their creative process.\n\nWhile creating the task, we submit the task to the story master since it will coordinate our story creation process.\n\nFinally, we should combine these agents into a crew and run our task.\n\nAnd thats it. When we run this code, it will prompt the user to give a story idea and then write a short story by agent cooperation. Of course, there is much more than this in the CrewAI framework such as tool usage, hierarchical processing, working with ollama to run agents fully locally with different agents etc, but these topics are for another article.\n\nYou can find the full code in here for directly run:\n\nYou can use this code as a template for these kinds of applications, you can build game builder crew, stock analyzer crew, marketing crew etc. With imagination, sky is the limit. If you like this article and excited about the more advanced implementations you can visit the CrewAI [website](https://www.crewai.com/).\n\n\n"},{"lang":"en","group":"blog","slug":"blog/how-to-improve-llms-with-rag-abdc132f76ac","frontmatter":{"title":"How to Improve LLMs with RAG","meta_title":"How to Improve LLMs with RAG","description":"A beginner-friendly introduction w/ Python code","date":"2024-11-04T12:31:55.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*N0Ad_oCIrAyzMYRdH3trqg.png","categories":["Natural Language Processing","Programming","Generative AI"],"author":"Rifx.Online","tags":["RAG","retrievers","LlamaIndex","knowledge","bases"],"draft":false,"slug":"blog/how-to-improve-llms-with-rag-abdc132f76ac"},"content":"\n\n\n\n\n### A beginner\\-friendly introduction w/ Python code\n\nThis article is part of a [larger series](https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c) on using large language models in practice. In the [previous post](https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32), we fine\\-tuned Mistral\\-7b\\-Instruct to respond to YouTube comments using QLoRA. Although the fine\\-tuned model successfully captured my style when responding to viewer feedback, its responses to technical questions didn’t match my explanations. Here, I’ll discuss how we can improve LLM performance using retrieval augmented generation (i.e. RAG).\n\n\n\nLarge language models (LLMs) have demonstrated an impressive ability to store and deploy vast knowledge in response to user queries. While this has enabled the creation of powerful AI systems like ChatGPT, compressing world knowledge in this way has **two key limitations**.\n\n**First**, an LLM’s knowledge is static, i.e., not updated as new information becomes available. **Second**, LLMs may have an insufficient “understanding” of niche and specialized information that was not prominent in their training data. These limitations can result in undesirable (and even fictional) model responses to user queries.\n\nOne way we can mitigate these limitations is to **augment a model via a specialized and mutable knowledge base**, e.g., customer FAQs, software documentation, or product catalogs. This enables the creation of more robust and adaptable AI systems.\n\n**Retrieval augmented generation**, or **RAG**, is one such approach. Here, I provide a high\\-level introduction to RAG and share example Python code for implementing a RAG system using LlamaIndex.\n\n\n## What is RAG?\n\nThe basic usage of an LLM consists of giving it a prompt and getting back a response.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*sM1p-3FoTaGZunqx918G9A.png)\n\n**RAG works by adding a step to this basic process**. Namely, a retrieval step is performed where, based on the user’s prompt, the relevant information is extracted from an external knowledge base and injected into the prompt before being passed to the LLM.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*EhJZj1blu7a8EPmVAPsNcA.png)\n\n\n## Why we care\n\nNotice that RAG does not fundamentally change how we use an LLM; it's still *prompt\\-in and response\\-out*. RAG simply augments this process (hence the name).\n\nThis makes **RAG a flexible and (relatively) straightforward way to improve LLM\\-based systems**. Additionally, since knowledge is stored in an external database, updating system knowledge is as simple as adding or removing records from a table.\n\n\n### Why not fine\\-tune?\n\nPrevious articles in this series discussed [fine\\-tuning](https://towardsdatascience.com/fine-tuning-large-language-models-llms-23473d763b91), which adapts an existing model for a particular use case. While this is an alternative way to endow an LLM with specialized knowledge, empirically, **fine\\-tuning seems to be less effective than RAG** **at doing this** \\[1].\n\n\n## How it works\n\nThere are 2 key elements of a RAG system: a **retriever** and a **knowledge base**.\n\n\n### Retriever\n\nA retriever takes a user prompt and returns relevant items from a knowledge base. This typically works using so\\-called **text embeddings**, numerical representations of text in concept space. In other words, these are **numbers that represent the *meaning* of a given text**.\n\nText embeddings can be used to compute a similarity score between the user’s query and each item in the knowledge base. The result of this process is a **ranking of each item’s relevance to the input query**.\n\nThe retriever can then take the top k (say k\\=3\\) most relevant items and inject them into the user prompt. This augmented prompt is then passed into the LLM for generation.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*jpTwdBmoTlJlfPAm0oJiVQ.png)\n\n\n### Knowledge Base\n\nThe next key element of a RAG system is a knowledge base. This **houses all the information you want to make available to the LLM**. While there are countless ways to construct a knowledge base for RAG, here I’ll focus on building one from a set of documents.\n\nThe process can be broken down into **4 key steps** \\[2,3].\n\n1. **Load docs** — This consists of gathering a collection of documents and ensuring they are in a ready\\-to\\-parse format (more on this later).\n2. **Chunk docs—**Since LLMs have limited context windows, documents must be split into smaller chunks **(e.g.,** 256 or 512 characters long).\n3. **Embed chunks** — Translate each chunk into numbers using a text embedding model.\n4. **Load into Vector DB**— Load text embeddings into a database (aka a vector database).\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*VWG6Tr0OxCnD5Mvygm5DCA.png)\n\n\n## Some Nuances\n\nWhile the steps for building a RAG system are conceptually simple, several nuances can make building one (in the real world) more complicated.\n\n**Document preparation**—The quality of a RAG system is driven by how well useful information can be extracted from source documents. For example, if a document is unformatted and full of images and tables, it will be more difficult to parse than a well\\-formatted text file.\n\n**Choosing the right chunk size**—We already mentioned the need for chunking due to LLM context windows. However, there are 2 additional motivations for chunking.\n\n**First**, it keeps (compute) costs down. The more text you inject into the prompt, the more compute required to generate a completion. The **second** is performance. Relevant information for a particular query tends to be localized in source documents (often, just 1 sentence can answer a question). Chunking helps minimize the amount of irrelevant information passed into the model \\[4].\n\n**Improving search** — While text embeddings enable a powerful and fast way to do search, it doesn’t always work as one might hope. In other words, it may return results that are “similar” to the user query, yet not helpful for answering it, e.g., “*How’s the weather in LA?*” may return “*How’s the weather in NYC?*”.\n\nThe simplest way to mitigate this is through good document preparation and chunking. However, for some use cases, additional strategies for improving search might be necessary, such as using **meta\\-tags** for each chunk, employing **hybrid search**, which combines keyword—and embedding\\-based search, or using a **reranker**, which is a specialized model that computes the similarity of 2 input pieces of text.\n\n\n## Example code: Improving YouTube Comment Responder with RAG\n\nWith a basic understanding of how RAG works, let’s see how to use it in practice. I will build upon the example from the [previous article](https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32), where I fine\\-tuned Mistral\\-7B\\-Instruct to respond to YouTube comments using QLoRA. We will use LlamaIndex to add a RAG system to the fine\\-tuned model from before.\n\nThe example code is freely available in a [Colab Notebook](https://colab.research.google.com/drive/1peJukr-9E1zCo1iAalbgDPJmNMydvQms?usp=sharing), which can run on the (free) T4 GPU provided. The source files for this example are available at the [GitHub repository](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/rag).\n\n🔗 [Google Colab](https://colab.research.google.com/drive/1peJukr-9E1zCo1iAalbgDPJmNMydvQms?usp=sharing) \\| [GitHub Repo](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/rag)\n\n\n### Imports\n\nWe start by installing and importing necessary Python libraries.\n\n\n```python\n!pip install llama-index\n!pip install llama-index-embeddings-huggingface\n!pip install peft\n!pip install auto-gptq\n!pip install optimum\n!pip install bitsandbytes\n## if not running on Colab ensure transformers is installed too\n```\n\n```python\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\nfrom llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex\nfrom llama_index.core.retrievers import VectorIndexRetriever\nfrom llama_index.core.query_engine import RetrieverQueryEngine\nfrom llama_index.core.postprocessor import SimilarityPostprocessor\n```\n\n### Setting up Knowledge Base\n\nWe can configure our knowledge base by defining our embedding model, chunk size, and chunk overlap. Here, we use the \\~33M parameter [bge\\-small\\-en\\-v1\\.5](https://huggingface.co/BAAI/bge-small-en-v1.5) embedding model from BAAI, which is available on the Hugging Face hub. Other embedding model options are available on this [text embedding leaderboard](https://huggingface.co/spaces/mteb/leaderboard).\n\n\n```python\n## import any embedding model on HF hub\nSettings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n\nSettings.llm = None # we won't use LlamaIndex to set up LLM\nSettings.chunk_size = 256\nSettings.chunk_overlap = 25\n```\nNext, we load our source documents. Here, I have a folder called “[*articles*](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/rag/articles),” which contains PDF versions of 3 Medium articles I wrote on [fat tails](https://towardsdatascience.com/pareto-power-laws-and-fat-tails-0355a187ee6a). If running this in Colab, you must download the articles folder from the [GitHub repo](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/rag) and manually upload it to your Colab environment.\n\nFor each file in this folder, the function below will read the text from the PDF, split it into chunks (based on the settings defined earlier), and store each chunk in a list called *documents*.\n\n\n```python\ndocuments = SimpleDirectoryReader(\"articles\").load_data()\n```\nSince the blogs were downloaded directly as PDFs from Medium, they resemble a webpage more than a well\\-formatted article. Therefore, some chunks may include text unrelated to the article, e.g., webpage headers and Medium article recommendations.\n\nIn the code block below, I refine the chunks in documents, removing most of the chunks before or after the meat of an article.\n\n\n```python\nprint(len(documents)) # prints: 71\nfor doc in documents:\n    if \"Member-only story\" in doc.text:\n        documents.remove(doc)\n        continue\n\n    if \"The Data Entrepreneurs\" in doc.text:\n        documents.remove(doc)\n\n    if \" min read\" in doc.text:\n        documents.remove(doc)\n\nprint(len(documents)) # prints: 61\n```\nFinally, we can store the refined chunks in a vector database.\n\n\n```python\nindex = VectorStoreIndex.from_documents(documents)\n```\n\n### Setting up Retriever\n\nWith our knowledge base in place, we can create a retriever using LlamaIndex’s *VectorIndexRetreiver(),* which returns the top 3 most similar chunks to a user query.\n\n\n```python\n## set number of docs to retreive\ntop_k = 3\n\n## configure retriever\nretriever = VectorIndexRetriever(\n    index=index,\n    similarity_top_k=top_k,\n)\n```\nNext, we define a query engine that uses the retriever and query to return a set of relevant chunks.\n\n\n```python\n## assemble query engine\nquery_engine = RetrieverQueryEngine(\n    retriever=retriever,\n    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.5)],\n)\n```\n\n### Use Query Engine\n\nNow, with our knowledge base and retrieval system set up, let’s use it to return chunks relevant to a query. Here, we’ll pass the same technical question we asked ShawGPT (the YouTube comment responder) from the [previous article](https://readmedium.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32).\n\n\n```python\nquery = \"What is fat-tailedness?\"\nresponse = query_engine.query(query)\n```\nThe query engine returns a response object containing the text, metadata, and indexes of relevant chunks. The code block below returns a more readable version of this information.\n\n\n```python\n## reformat response\ncontext = \"Context:\\n\"\nfor i in range(top_k):\n    context = context + response.source_nodes[i].text + \"\\n\\n\"\n\nprint(context)\n```\n\n```python\nContext:\nSome of the controversy might be explained by the observation that log-\nnormal distributions behave like Gaussian for low sigma and like Power Law\nat high sigma [2].\nHowever, to avoid controversy, we can depart (for now) from whether some\ngiven data fits a Power Law or not and focus instead on fat tails.\nFat-tailedness — measuring the space between Mediocristan\nand Extremistan\nFat Tails are a more general idea than Pareto and Power Law distributions.\nOne way we can think about it is that “fat-tailedness” is the degree to which\nrare events drive the aggregate statistics of a distribution. From this point of\nview, fat-tailedness lives on a spectrum from not fat-tailed (i.e. a Gaussian) to\nvery fat-tailed (i.e. Pareto 80 – 20).\nThis maps directly to the idea of Mediocristan vs Extremistan discussed\nearlier. The image below visualizes different distributions across this\nconceptual landscape [2].\n\nprint(\"mean kappa_1n = \" + str(np.mean(kappa_dict[filename])))\n    print(\"\")\nMean κ (1,100) values from 1000 runs for each dataset. Image by author.\nThese more stable results indicate Medium followers are the most fat-tailed,\nfollowed by LinkedIn Impressions and YouTube earnings.\nNote: One can compare these values to Table III in ref [3] to better understand each\nκ value. Namely, these values are comparable to a Pareto distribution with α\nbetween 2 and 3.\nAlthough each heuristic told a slightly different story, all signs point toward\nMedium followers gained being the most fat-tailed of the 3 datasets.\nConclusion\nWhile binary labeling data as fat-tailed (or not) may be tempting, fat-\ntailedness lives on a spectrum. Here, we broke down 4 heuristics for\nquantifying how fat-tailed data are.\n\nPareto, Power Laws, and Fat Tails\nWhat they don’t teach you in statistics\ntowardsdatascience.com\nAlthough Pareto (and more generally power law) distributions give us a\nsalient example of fat tails, this is a more general notion that lives on a\nspectrum ranging from thin-tailed (i.e. a Gaussian) to very fat-tailed (i.e.\nPareto 80 – 20).\nThe spectrum of Fat-tailedness. Image by author.\nThis view of fat-tailedness provides us with a more flexible and precise way of\ncategorizing data than simply labeling it as a Power Law (or not). However,\nthis begs the question: how do we define fat-tailedness?\n4 Ways to Quantify Fat Tails\n```\n\n### Adding RAG to LLM\n\nWe start by downloading the [fine\\-tuned model](https://readmedium.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32) from the Hugging Face hub.\n\n\n```python\n## load fine-tuned model from hub\nfrom peft import PeftModel, PeftConfig\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name,\n                                             device_map=\"auto\",\n                                             trust_remote_code=False,\n                                             revision=\"main\")\n\nconfig = PeftConfig.from_pretrained(\"shawhin/shawgpt-ft\")\nmodel = PeftModel.from_pretrained(model, \"shawhin/shawgpt-ft\")\n\n## load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n```\nAs a baseline, we can see how the model responds to the technical question without any context from the articles. To do this, we create a prompt template using a lambda function, which takes in a viewer comment and returns a prompt for the LLM. For more details on where this prompt comes from, see the [previous article](https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32#5aad) of this series.\n\n\n```python\n## prompt (no context)\nintstructions_string = f\"\"\"ShawGPT, functioning as a virtual data science \\\nconsultant on YouTube, communicates in clear, accessible language, escalating \\\nto technical depth upon request. It reacts to feedback aptly and ends \\\nresponses with its signature '–ShawGPT'.\n\nShawGPT will tailor the length of its responses to match the viewer's comment, \\\nproviding concise acknowledgments to brief expressions of gratitude or \\\nfeedback, thus keeping the interaction natural and engaging.\n\nPlease respond to the following comment.\n\"\"\"\n\nprompt_template = lambda comment: f'''[INST] {intstructions_string} \\n{comment} \\n[/INST]'''\ncomment = \"What is fat-tailedness?\"\n\nprompt = prompt_template(comment)\nprint(prompt)\n```\n\n```python\n[INST] ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. It reacts to feedback aptly and ends responses with its signature '–ShawGPT'. ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, thus keeping the interaction natural and engaging.\n\nPlease respond to the following comment.\n \nWhat is fat-tailedness? \n[/INST]\n```\nNext, we can pass this prompt to the model using the code below.\n\n\n```python\nmodel.eval()\n\ninputs = tokenizer(prompt, return_tensors=\"pt\")\noutputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), \n                          max_new_tokens=280)\n\nprint(tokenizer.batch_decode(outputs)[0])\n```\nHere’s the model’s response (no context).\n\n\n```python\nGreat question!\n\nFat-tailedness is a statistical property of a distribution. In simple terms, \nit refers to the presence of extreme outliers or heavy tails in the \ndistribution.\n\nFor instance, consider the distribution of heights in a population. A normal \ndistribution would have most people clustered around an average height with a \nfew people deviating slightly from the mean. However, in a fat-tailed \ndistribution, you would observe a larger number of people being significantly \ntaller or shorter than the average.\n\nIn finance, fat-tailedness is often used to describe the presence of extreme \nmarket moves or outliers in financial returns. It's an important concept in \nrisk management and financial modeling.\n\nI hope this explanation helps! Let me know if you have any questions.\n–ShawGPT\n```\nAlthough the response's style and formatting are great, the model's explanation of fat\\-tailedness differs from how I defined it in my [video](https://www.youtube.com/playlist?list=PLz-ep5RbHosVrT89BRNX-IGKHYxvx9wqi) and [blog series](https://towardsdatascience.com/pareto-power-laws-and-fat-tails-0355a187ee6a).\n\nLet’s see what happens to the model’s response when we include the appropriate context. To do this, we create another prompt template, which can also take in context from the RAG system.\n\n\n```python\n## prompt (with context)\nprompt_template_w_context = lambda context, comment: f\"\"\"[INST]ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. \\\nIt reacts to feedback aptly and ends responses with its signature '–ShawGPT'. \\\nShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, \\\nthus keeping the interaction natural and engaging.\n\n{context}\nPlease respond to the following comment. Use the context above if it is helpful.\n\n{comment}\n[/INST]\n\"\"\"\n```\nNext, we pass the prompt with context from the query engine and the view comment to the model.\n\n\n```python\nprompt = prompt_template_w_context(context, comment)\n\ninputs = tokenizer(prompt, return_tensors=\"pt\")\noutputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=280)\n\nprint(tokenizer.batch_decode(outputs)[0])\n```\nHere’s the new response (with context).\n\n\n```python\nGreat question!\n\nFat-tailedness is a property of a distribution that describes how much the \npresence of extreme values (rare events) affects the overall statistics of the \ndistribution.\n\nIn the context of the video, the author discusses how Mediocristan and \nExtremistan distributions differ in terms of fat-tailedness. Mediocristan \ndistributions have a relatively small impact from extreme values, while \nExtremistan distributions are heavily influenced by extreme values.\n\nThe author also mentions that fat-tailedness is a more general concept than \nspecific distributions like Pareto or Power Laws. Instead, it lives on a \nspectrum ranging from thin-tailed (Gaussian) to very fat-tailed (Pareto 80-20).\n\nI hope that helps clarify things a bit! Let me know if you have any questions.\n–ShawGPT\n```\nThis does a much better job of capturing my explanation of fat tails than the no\\-context response and even calls out the niche concepts of Mediocristan and Extremistan.\n\n\n## What’s next?\n\nHere, I gave a beginner\\-friendly introduction to RAG and shared a concrete example of how to implement it using LlamaIndex. RAG allows us to improve an LLM system with updateable and domain\\-specific knowledge.\n\nWhile much of the recent AI hype has centered around building AI assistants, a powerful (yet less popular) innovation has come from text embeddings (i.e. the things we used to do retrieval). In the next article of this series, I will explore **text embeddings** in more detail, including how they can be used for **semantic search** and **classification tasks**.\n\n**More on LLMs 👇**\n\n\n## Resources\n\n**Connect**: [My website](https://shawhintalebi.com/) \\| [Book a call](https://calendly.com/shawhintalebi)\n\n**Socials**: [YouTube 🎥](https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA) \\| [LinkedIn](https://www.linkedin.com/in/shawhintalebi/) \\| [Instagram](https://www.instagram.com/shawhintalebi)\n\n**Support**: [Buy me a coffee](https://www.buymeacoffee.com/shawhint) ☕️\n\n\\[1] [RAG \\> FT (empirical)](https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb)\n\n\\[2] [LlamaIndex Webinar: Building LLM Apps for Production, Part 1 (co\\-hosted with Anyscale)](https://www.youtube.com/watch?v=efbn-3tPI_M)\n\n\\[3] [LlamaIndex doc](https://docs.llamaindex.ai/en/stable/understanding/loading/loading.html)\n\n\\[4] [LlamaIndex Webinar: Make RAG Production\\-Ready](https://www.youtube.com/watch?v=Zj5RCweUHIk&list=WL&index=4)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/how-to-run-nvidia-llama-3-1-nemotron-70b-instruct-locally-a58ad283aaff","frontmatter":{"title":"How to Run Nvidia’ llama-3.1-nemotron-70b-instruct Locally","meta_title":"How to Run Nvidia’ llama-3.1-nemotron-70b-instruct Locally","description":"Running large language models (LLMs) locally has become increasingly popular among developers, researchers, and AI enthusiasts. One such…","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*fqVKJkw5sQvLtIsyCcengQ.png","categories":["Programming","Technology","Science"],"author":"Rifx.Online","tags":["Nvidia","llama","Ollama","llama.cpp","Transformers"],"draft":false,"slug":"blog/how-to-run-nvidia-llama-3-1-nemotron-70b-instruct-locally-a58ad283aaff"},"content":"\n\n\n\nRunning large language models (LLMs) locally has become increasingly popular among developers, researchers, and AI enthusiasts. One such model that has gained significant attention is the llama-3.1-nemotron-70b-instruct, a powerful LLM customized by NVIDIA to enhance the helpfulness of generated responses. In this comprehensive guide, we’ll explore multiple methods to run this model on your local machine, starting with the user-friendly Ollama platform.\n\n\n> Before we get started, If you are seeking an All-in-One AI platform that manages all your AI subscriptions in one place, including all LLMs (such as GPT-o1, Llama 3.1, Claude 3.5 Sonnet, Google Gemini, Uncensored LLMs) and Image Generation Models (FLUX, Stable Diffusion, etc.), Use Anakin AI to manage them all!\n\n\n\n\n## Method 1: Run llama-3.1-nemotron-70b-instruct Locally with Ollama\n\nOllama is an excellent tool for running LLMs locally, offering a straightforward setup process and support for various models, including llama-3.1-nemotron-70b-instruct.\n\n\n### Installation\n\n1. Visit the official Ollama website ([https://ollama.ai](https://ollama.ai/)) and download the appropriate version for your operating system.\n2. Install Ollama by running the following command in your terminal:\n\n\n```python\ncurl https://ollama.ai/install.sh | sh\n```\n\n### Running llama-3.1-nemotron\n\nOnce Ollama is installed, you can easily run the llama-3.1-nemotron-70b-instruct model with a simple command:\n\n\n```python\nollama run nemotron:70b-instruct-q5_K_M\n```\nThis command will download the model if it’s not already present on your system and start an interactive session.\n\n\n### Using the Model\n\nAfter the model is loaded, you can start interacting with it by typing your prompts. For example:\n\n\n```python\n>>> What are the key features of llama-3.1-nemotron-70b-instruct?\n\nLlama-3.1-Nemotron-70B-Instruct is a large language model with several key features:\n1. Customized by NVIDIA: The model has been fine-tuned by NVIDIA to improve the helpfulness and quality of its responses.\n2. Based on Llama 3.1: It builds upon the Llama 3.1 architecture, which is known for its strong performance across various tasks.\n3. 70 billion parameters: This large parameter count allows for complex reasoning and a wide range of capabilities.\n4. Instruct-tuned: The model is specifically designed to follow instructions and generate helpful responses to user queries.\n5. RLHF training: It has been trained using Reinforcement Learning from Human Feedback, specifically the REINFORCE algorithm.\n6. Specialized reward model: The training process utilized Llama-3.1-Nemotron-70B-Reward for optimization.\n7. HelpSteer2-Preference prompts: These were used during the training process to further improve the model's helpfulness.\n8. Extended context length: Like other Llama 3.1 models, it likely supports a longer context window of 128K tokens.\n9. Multilingual capabilities: It can understand and generate text in multiple languages.\n10. Strong reasoning abilities: The model excels in tasks requiring complex reasoning and problem-solving.\nThese features make llama-3.1-nemotron-70b-instruct a powerful and versatile language model suitable for a wide range of applications, from general conversation to specialized tasks in various domains.\n```\nFor more advanced use cases, you can integrate Ollama with Python using libraries like Langchain. Here’s a simple example:\n\n\n```python\npython\n\nfrom langchain.llms import Ollama\n\nollama = Ollama(base_url=\"http://localhost:11434\", model=\"nemotron:70b-instruct-q5_K_M\")\nresponse = ollama.generate(\"Explain the concept of quantum entanglement.\")\nprint(response)\n```\nThis allows you to incorporate the model into your Python projects and applications seamlessly.\n\n\n## Method 2: Using llama.cpp\n\nllama.cpp is a popular C++ implementation of the Llama model inference, optimized for CPU usage. While it may require more setup than Ollama, it offers greater flexibility and control over the model’s parameters.\n\n\n### Installation\n\n1. Clone the llama.cpp repository:\n\n\n```python\ngit clone https://github.com/ggerganov/llama.cpp.git\ncd llama.cpp\n```\n1. Build the project:\n\n\n```python\nmake\n```\n\n### Downloading the Model\n\nTo run llama-3.1-nemotron-70b-instruct, you’ll need to download the model weights. These are typically available in GGML or GGUF format. You can find pre-converted models on platforms like Hugging Face.\n\n\n```python\nmkdir models\ncd models\nwget https://huggingface.co/TheBloke/Llama-3.1-Nemotron-70B-Instruct-GGUF/resolve/main/llama-3.1-nemotron-70b-instruct.Q4_K_M.gguf\n```\n\n### Running the Model\n\nOnce you have the model file, you can run it using the following command:\n\n\n```python\n./main -m models/llama-3.1-nemotron-70b-instruct.Q4_K_M.gguf -n 1024 -p \"Hello, how are you today?\"\n```\nThis command loads the model and generates a response to the given prompt. You can adjust various parameters like the number of tokens to generate (-n) or the temperature to control randomness.\n\n\n## Method 3: Using Hugging Face Transformers\n\nHugging Face’s Transformers library provides a high-level API for working with various language models, including llama-3.1-nemotron-70b-instruct.\n\n**Installation**\n\nFirst, install the necessary libraries:\n\n\n```python\npip install transformers torch accelerate\n```\n**Running the Model**\n\nHere’s a Python script to load and use the model:\n\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nmodel_name = \"meta-llama/Llama-3.1-Nemotron-70b-instruct\"\n## Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n## Prepare the input\nprompt = \"Explain the concept of quantum computing in simple terms.\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n## Generate the response\nwith torch.no_grad():\n    outputs = model.generate(**inputs, max_new_tokens=100)\n## Decode and print the response\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response)\n```\nThis method allows for more fine-grained control over the model’s behavior and integration with other Hugging Face tools and pipelines.\n\n\n## Conclusion\n\nRunning llama-3.1-nemotron-70b-instruct locally opens up a world of possibilities for developers and researchers. Whether you choose the simplicity of Ollama, the flexibility of llama.cpp, or the integration capabilities of Hugging Face Transformers, you now have the tools to harness the power of this advanced language model on your own hardware.As you explore the capabilities of llama-3.1-nemotron-70b-instruct, remember to balance performance with resource constraints, and always consider the ethical implications of your applications. With responsible use, this model can be a valuable asset in pushing the boundaries of what’s possible in natural language processing and AI-driven applications.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/intelli-agent-langchain-crewai-and-autogen-compared-369a527b2026","frontmatter":{"title":"Intelli-agent: Langchain, CrewAI and AutoGen Compared","meta_title":"Intelli-agent: Langchain, CrewAI and AutoGen Compared","description":"1. Overview of AI Agent Frameworks","date":"2024-11-08T00:22:33.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*uswz_9OuqiMWUL9kfKXeaQ.png","categories":["Programming","Machine Learning","Autonomous Systems"],"author":"Rifx.Online","tags":["Langchain","CrewAI","AutoGen","Swarm","agents"],"draft":false,"slug":"blog/intelli-agent-langchain-crewai-and-autogen-compared-369a527b2026"},"content":"\n\n\n\n\n\n\n## 1\\. Overview of AI Agent Frameworks\n\nIn the dynamic landscape of artificial intelligence, choosing the right framework is a crucial decision for every data scientist and developer. The AI agent ecosystem is evolving rapidly, offering increasingly sophisticated solutions to automate and optimize complex processes.\n\nThe intelligent agent revolution has brought several frameworks to the fore, each with distinctive features. Langchain, CrewAI, AutoGen, and Swarm emerge as protagonists in this scenario, each offering unique approaches to managing and orchestrating AI agents.\n\nThe main objective of this benchmarking is to provide an in\\-depth assessment of the capabilities, strengths, and limitations of each framework. The optimal choice depends on multiple factors, including the complexity of the project, the available resources and the specific objectives of the implementation.\n\nThe current trends in AI show a clear direction towards increasingly autonomous and collaborative systems. The ability of these frameworks to facilitate interaction between agents, manage shared memory and orchestrate complex tasks makes them critical tools for developing advanced AI solutions.\n\n\n## 2\\. Langchain: Versatility and Modular\n\nLangchain stands out for its exceptionally flexible modular architecture. This framework offers a structured approach to building AI applications, allowing developers to build complex systems through interconnected components.\n\nMemory management is one of Langchain’s most significant strengths. The framework implements sophisticated mechanisms to maintain conversational context, allowing agents to access historical information and maintain consistent conversations over time.\n\nThe Langchain ecosystem supports a wide range of integrations with external APIs, databases, and other services. This feature makes it easy to create custom solutions that can tap into different data sources and capabilities.\n\nThe framework’s architectural flexibility allows you to easily implement different types of specialized agents. From semantic search to natural language processing, Langchain provides pre\\-configured tools that significantly speed up the development process.\n\nA particularly important aspect is the ability to chain operations in a logical and sequential way. This feature, called Chain, allows you to build complex workflows while maintaining a clear and maintainable structure. Developers can define custom sequences of actions, where each component in the chain processes and transforms data incrementally.\n\nThe active community around Langchain is constantly contributing new components and integrations. This growing ecosystem offers out\\-of\\-the\\-box solutions for a variety of use cases, from content generation and document analysis to the creation of sophisticated virtual assistants.\n\nAs far as performance is concerned, Langchain demonstrates remarkable efficiency in resource management. The framework implements intelligent caching and API call optimization mechanisms, significantly reducing operational costs and response times.\n\n\n## 3\\. CrewAI: Intelligent Collaboration between Agents\n\nCrewAI introduces an innovative paradigm based on collaboration between specialized agents. This framework stands out for its ability to organize agents into functional teams, where each member contributes specific skills to achieving common goals.\n\nCrewAI’s hierarchical structure facilitates the efficient management of interactions between agents. The framework implements a sophisticated task delegation system, where each agent can assign specific tasks to other team members based on their skills.\n\nInter\\-agent communication in CrewAI is based on an advanced protocol that allows for structured and contextualized information exchanges. Agents can share knowledge, intermediate results, and feedback in real time, creating a dynamic and adaptive collaboration environment.\n\nA particularly innovative aspect is the dynamic role system. Agents can take on different responsibilities depending on the context and needs of the project. This flexibility allows you to optimize resource utilization and maximize the efficiency of your virtual team.\n\nConflict management and problem resolution are addressed through a sophisticated distributed consensus mechanism. Agents can negotiate solutions, propose alternatives and reach shared decisions independently.\n\nThe future potential of CrewAI is particularly promising in the field of business process automation. The framework is evolving to include:\n\n* Collaborative learning between agents\n* Automatic team optimization\n* Dynamic scaling of resources\n* Advanced integration with external systems\n\n\n## 4\\. AutoGen and Swarm: Innovations in Agent Creation\n\nAutoGen stands out for its revolutionary approach to the automatic generation of multi\\-agent systems. The framework excels at creating modular architectures that can evolve and adapt autonomously to the specific needs of projects.\n\nThe distinguishing feature of AutoGen lies in its ability to self\\-optimize. Generated agents can:\n\n* Change your behavior based on feedback received\n* Automatically optimize configuration parameters\n* Generate functional code for new features\n* Implement adaptive problem\\-solving strategies\n\nSwarm, on the other hand, focuses on lightness and efficiency in agent orchestration. Its minimalist approach offers significant advantages in terms of:\n\n* Optimized resource consumption\n* Superior execution speed\n* Scale\\-out simplified\n* Maintainability of the system\n\nThe direct comparison between these frameworks reveals interesting complementarities. While AutoGen shines at autonomous generation of complex solutions, Swarm excels at efficient management of large groups of simple agents.\n\n\n## Final Thoughts\n\nThe comparative overview presented shows that the intelligent agents sector is undergoing a phase of extraordinary innovation. Each framework analyzed brings a unique value to the AI ecosystem, helping to shape the future of intelligent automation.\n\nKey thoughts for industry professionals:\n\n1. The diversification of the available tools should not be seen as an obstacle, but as an opportunity for specialization and continuous innovation.\n2. The investment in the in\\-depth understanding of these frameworks represents a competitive advantage in the tech job market.\n3. Flexibility in the adoption of different solutions remains crucial for the success of enterprise\\-level projects.\n\nAs a lead data scientist, I recommend to:\n\n* Maintain a pragmatic approach in instrument selection\n* Favor solutions that guarantee scalability and maintainability\n* Invest in the continuous training of the team\n* Constantly monitor the technological evolutions of the sector\n\nThe future of smart agents looks promising, with a clear trend towards:\n\n* Increasingly sophisticated hybrid systems\n* Seamless integration between different platforms\n* Advanced automation of decision\\-making processes\n* Customization push of solutions\n\nThe key to success will lie in the ability to orchestrate these tools effectively, creating solutions that not only solve current problems, but are also ready for future challenges.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/introducing-atomic-agents-1-0-a-modular-framework-for-building-agentic-ai-with-cli-support-2b01b7165ace","frontmatter":{"title":"Introducing Atomic Agents 1.0: A Modular Framework for Building Agentic AI","meta_title":"Introducing Atomic Agents 1.0: A Modular Framework for Building Agentic AI","description":"Imagine building AI applications as effortlessly as assembling LEGO blocks. That’s the idea behind Atomic Agents, a modular framework for…","date":"2024-11-08T00:19:37.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*BZGf8BCnCJiFlKZ5.png","categories":["Programming","Machine Learning","Autonomous Systems"],"author":"Rifx.Online","tags":["modular","framework","Atomic","assembler","schema"],"draft":false,"slug":"blog/introducing-atomic-agents-1-0-a-modular-framework-for-building-agentic-ai-with-cli-support-2b01b7165ace"},"content":"\n\n\nImagine building AI applications as effortlessly as assembling LEGO blocks. That’s the idea behind [Atomic Agents](https://github.com/BrainBlend-AI/atomic-agents), a modular framework for constructing AI agents inspired by **Atomic Design** principles. With the release of **version 1\\.0**, Atomic Agents introduces a powerful CLI called **Atomic Assembler**, making it even easier to build, manage, and deploy your AI applications.\n\n## Why Atomic Agents?\n\nMany existing frameworks for **Agentic AI** focus on building autonomous multi\\-agent systems that are more like curiosities than practical tools. While these can be fascinating, they often lack the predictability and control required for real\\-world applications.\n\nBusinesses typically aren’t looking for a bot that writes articles in a different style each time. They want consistency in style, structure, and tone to align with their brand identity. Fine\\-tuning a model is one approach, but it requires substantial data and resources, and it’s not always feasible with the latest models like GPT\\-4\\.\n\nAtomic Agents aims to solve this by providing:\n\n* **Modularity**: Build complex AI systems by combining simple, interchangeable components.\n* **Atomicity**: Each component within Atomic Agents, each tool, each agent, each context provider, is as single\\-purpose and re\\-usable as possible, Guaranteeing a great separation of concerns.\n* **Control**: Fine\\-tune each individual step and component, from system prompts to tools.\n* **Predictability**: Ensure reproducible and reliable outputs suitable for business use cases.\n* **Extensibility**: Easily add or replace components without overhauling the entire system.\n\n## A Traditional Modular Approach\n\nIn traditional software development, complex problems are broken down into smaller, manageable parts:\n\n1. **Define the problem**: Start with flows, user stories, or customer journeys.\n2. **Break it down**: Divide the problem into smaller, solvable tasks.\n3. **Develop modular code**: Write functions or classes that handle specific tasks.\n4. **Integrate**: Combine these modules to form the complete application.\n\nAtomic Agents brings this same level of modularity and predictability to AI agent development.\n\n## A Real\\-World Scenario\n\nInstead of building a monolithic AI system that “writes a blog post,” we can design a modular system that:\n\n1. **Generates queries** related to a subject.\n2. **Identifies** the top X most relevant articles.\n3. **Visits** each identified article’s page.\n4. **Extracts** the text from each article.\n5. **Generates summaries** of each article.\n6. **Stores** the summaries in a vector database.\n7. **Generates questions** around the subject.\n8. **Answers** those questions using the vector database.\n9. **Synthesizes** the answers into a coherent blog post.\n\nThis approach is more verbose but offers greater control, reliability, and suitability for real\\-world business applications.\n\n## Introduction of the CLI: Atomic Assembler\n\nOne of the significant additions in version 1\\.0 is the **Atomic Assembler** CLI. This command\\-line tool allows you to:\n\n* **Download and manage tools**: Easily add new tools or agents to your project.\n* **Avoid unnecessary dependencies**: Install only what you need.\n* **Modify tools effortlessly**: Each tool comes with its own tests and documentation.\n* **Access tools directly**: If you prefer, manage tools manually without the CLI.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*aDceAIINxyFDOvle.png)\n\n## Anatomy of an Agent\n\nAI agents, especially in the Atomic Agents framework, consist of several key components:\n\n* **System Prompt**: Defines the agent’s behavior and purpose.\n* **User Input**: The data provided by the user.\n* **Tools**: External functions or APIs the agent can utilize.\n* **Memory**: Keeps track of the conversation or state.\n\nEach component is designed to be modular and interchangeable, adhering to the principles of separation of concerns and single responsibility.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*yt-5SoQC6uXTAd1-)\n\n## The Power of Modularity\n\nBy breaking down agents into these atomic components, you can:\n\n* **Swap out tools** without affecting the rest of the system.\n* **Fine\\-tune prompts** to adjust the agent’s behavior.\n* **Chain agents and tools** seamlessly by matching their input and output schemas.\n\n## Using the CLI: Atomic Assembler\n\n## Installation\n\nTo get started with Atomic Agents and the CLI, install the package via pip:\n\n```python\npip install atomic-agents\n```\n\n## Running the CLI\n\nLaunch the CLI using:\n\n```python\natomic\n```\n\nOr, if you installed Atomic Agents with Poetry:\n\n```python\npoetry run atomic\n```\n\nYou’ll be presented with a menu to download and manage tools:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*SzRlpA0-ivcE2qhk)\n\n*Image: Atomic CLI Main Menu*\n\nEach tool includes:\n\n* **Input Schema**\n* **Output Schema**\n* **Usage Examples**\n* **Dependencies**\n* **Installation Instructions**\n\n## Managing Tools\n\nThe Atomic Assembler CLI provides complete control over your tools, allowing you to:\n\n* **Avoid dependency clutter**: Install only the tools you need.\n* **Modify tools easily**: Each tool is self\\-contained with its own tests.\n* **Access tools directly**: Manage tool folders manually if you prefer.\n\n## Context Providers\n\nAtomic Agents introduces **Context Providers** to enhance your agents with dynamic context. Context Providers allow you to inject additional information into the agent’s system prompt at runtime.\n\n## Using Context Providers\n\n**Create a Context Provider Class**: Subclass `SystemPromptContextProviderBase` and implement the `get_info()` method.\n\n```python\nfrom atomic_agents.lib.components.system_prompt_generator import SystemPromptContextProviderBase   \n\nclass SearchResultsProvider(SystemPromptContextProviderBase):\n      def __init__(self, title: str, search_results: List[str]):\n          super().__init__(title=title)\n          self.search_results = search_results\n\n       def get_info(self) -> str:\n          return \"\\n\".join(self.search_results)\n```\n\n**Register the Context Provider with the Agent**:\n\n```python\n## Initialize your context provider with dynamic data\nsearch_results_provider = SearchResultsProvider(\n      title=\"Search Results\",\n      search_results=[\"Result 1\", \"Result 2\", \"Result 3\"]\n)   \n\n## Register the context provider with the agent  \nagent.register_context_provider(\"search_results\", search_results_provider)\n```\n\nThis allows your agent to include dynamic data like search results in its system prompt, enhancing its responses based on the latest information.\n\n## Chaining Schemas and Agents\n\nAtomic Agents simplifies chaining agents and tools by aligning their input and output schemas. This design promotes modularity and reusability.\n\n### Example: Generating Queries for Different Search Providers\n\nSuppose you have an agent that generates search queries and you want to use these queries with different search tools. By aligning the agent’s output schema with the input schema of the search tool, you can easily chain them or switch between providers.\n\n```python\nimport instructor\nimport openai\nfrom pydantic import Field\nfrom atomic_agents.agents.base_agent import BaseIOSchema, BaseAgent, BaseAgentConfig\nfrom atomic_agents.lib.components.system_prompt_generator import SystemPromptGenerator\n\n## Import the search tool\nfrom web_search_agent.tools.searxng_search import SearxNGSearchTool\nclass QueryAgentInputSchema(BaseIOSchema):\n    \"\"\"Input schema for the QueryAgent.\"\"\"\n    instruction: str = Field(..., description=\"Instruction to generate search queries for.\")\n    num_queries: int = Field(..., description=\"Number of queries to generate.\")\n\n\n## Initialize the query agent\nquery_agent = BaseAgent(\n    BaseAgentConfig(\n        client=instructor.from_openai(openai.OpenAI()),\n        model=\"gpt-4\",\n        system_prompt_generator=SystemPromptGenerator(\n            background=[\n                \"You are an intelligent query generation expert.\",\n                \"Your task is to generate diverse and relevant queries based on a given instruction.\"\n            ],\n            steps=[\n                \"Receive the instruction and the number of queries.\",\n                \"Generate the queries in JSON format.\"\n            ],\n            output_instructions=[\n                \"Ensure each query is unique and relevant.\",\n                \"Provide the queries in the expected schema.\"\n            ],\n        ),\n        input_schema=QueryAgentInputSchema,\n        output_schema=SearxNGSearchTool.input_schema,  # Align output schema\n    )\n)\n```\n\n**Modularity**: By setting the `output_schema` of the `query_agent` to match the `input_schema` of `SearxNGSearchTool`, you can directly use the output of the agent as input to the tool.\n\n**Swapability**: To switch to a different search provider, import a different search tool and update the `output_schema`:\n\n```python\n## Import a different search tool\nfrom web_search_agent.tools.another_search import AnotherSearchTool\n\n## Update the output schema\nquery_agent.config.output_schema = AnotherSearchTool.input_schema\n```\n\n## Example: Building a Simple AI Agent\n\nNow that we’ve covered the basics, let’s build a simple AI agent using Atomic Agents and explore how it works under the hood.\n\n## Step 1: Installation\n\nFirst, install the necessary packages:\n\n```python\npip install atomic-agents openai instructor\n```\n\n## Step 2: Import Components\n\nImport the necessary components:\n\n```python\nimport os\nfrom atomic_agents.agents.base_agent import BaseAgent, BaseAgentConfig, BaseIOSchema\nfrom atomic_agents.lib.components.system_prompt_generator import SystemPromptGenerator\nfrom atomic_agents.lib.components.agent_memory import AgentMemory\nfrom pydantic import Field\nimport instructor\nimport openai\n```\n\n## Step 3: Define a Custom Output Schema\n\n```python\nclass CustomOutputSchema(BaseIOSchema):\n    chat_message: str = Field(..., description=\"The chat message from the agent.\")\n    suggested_questions: List[str] = Field(..., description=\"Suggested follow-up questions.\")\n```\n\n## Step 4: Set Up the System Prompt\n\n```python\nsystem_prompt_generator = SystemPromptGenerator(\n    background=[\"This assistant is knowledgeable, helpful, and suggests follow-up questions.\"],\n    steps=[\n        \"Analyze the user's input to understand the context and intent.\",\n        \"Formulate a relevant and informative response.\",\n        \"Generate 3 suggested follow-up questions for the user.\"\n    ],\n    output_instructions=[\n        \"Provide clear and concise information in response to user queries.\",\n        \"Conclude each response with 3 relevant suggested questions for the user.\"\n    ]\n)\n```\n\n## Step 5: Initialize the Agent\n\n```python\n## Initialize memory (optional)\nmemory = AgentMemory()\n\n## Initialize the agent\nagent = BaseAgent(\n    config=BaseAgentConfig(\n        client=instructor.from_openai(openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))),\n        model=\"gpt-4o-mini\",\n        system_prompt_generator=system_prompt_generator,\n        memory=memory,\n        output_schema=CustomOutputSchema\n    )\n)\n```\n\n## Step 6: Use the Agent\n\n```python\nuser_input = \"Can you explain the benefits of using Atomic Agents?\"\nresponse = agent.run(agent.input_schema(chat_message=user_input))\nprint(f\"Agent: {response.chat_message}\")\nprint(\"Suggested questions:\")\nfor question in response.suggested_questions:\n    print(f\"- {question}\")\n```\n\n## What’s Happening Behind the Scenes?\n\n* **System Prompt**: Defines the agent’s behavior and guides the LLM.\n* **Input Schema**: Validates the user’s input.\n* **Output Schema**: Ensures the agent’s response matches the expected format.\n* **Memory**: Keeps track of the conversation history.\n\n## Conclusion\n\nAtomic Agents 1\\.0 brings modularity, control, and flexibility to AI agent development. With the introduction of the Atomic Assembler CLI and features like Context Providers and schema chaining, building sophisticated AI applications has never been easier.\n\nWhether you’re a developer aiming to build AI\\-powered tools or a business looking to automate complex tasks, Atomic Agents provides the building blocks to create reliable and maintainable AI systems.\n\n## Get Started Today\n\n* **GitHub Repository**: [BrainBlend\\-AI/atomic\\-agents](https://github.com/BrainBlend-AI/atomic-agents)\n* **API Documentation**: [Atomic Agents API Docs](https://brainblend-ai.github.io/atomic-agents/)\n* **Examples Directory**: [Atomic Examples](https://github.com/BrainBlend-AI/atomic-agents/tree/main/atomic-examples)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/introduction-to-llava-a-multimodal-ai-model-2a2fa530ace4","frontmatter":{"title":"Introduction to LLaVA: A Multimodal AI Model","meta_title":"Introduction to LLaVA: A Multimodal AI Model","description":"LLaVA is an end-to-end trained large multimodal model that is designed to understand and generate content based on both visual inputs…","date":"2024-10-29T12:48:10.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*0At7tXF5ejho9Y46E3uGtg.png","categories":["Natural Language Processing","Computer Vision","Generative AI"],"author":"Rifx.Online","tags":["LLaVA","GPT-4","multimodal","visual","encoder"],"draft":false,"slug":"blog/introduction-to-llava-a-multimodal-ai-model-2a2fa530ace4"},"content":"\n\n\n\n\n\nLLaVA is an end\\-to\\-end trained large multimodal model that is designed to understand and generate content based on both visual inputs (images) and textual instructions. It combines the capabilities of a visual encoder and a language model to process and respond to multimodal inputs.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*mjzqL0BHzdPoN-Jjruh52A.png)\n\n\n## Inputs and Outputs of LLaVA: Bridging Visual and Textual Domains:\n\nThe inputs to LLaVA are twofold:\n\n1. Visual Input: Images that the model can view and analyze to extract visual features and contextual information.\n2. Textual Instructions: Text inputs, which can be questions or commands, that guide the model on what to focus on or what kind of task to perform regarding the visual input.\n\nThe outputs of LLaVA are text\\-based and can vary depending on the task:\n\n1. Descriptive Text: If the task is to describe the visual content, LLaVA can output a detailed description of the image, identifying objects, actions, and scenes.\n2. Answers to Questions: For question\\-answering tasks, LLaVA generates responses that answer questions about the visual input, potentially involving reasoning and inference based on the image’s content.\n3. Follow\\-up Actions: For instructions that require action, such as editing an image or retrieving more information, LLaVA can provide appropriate textual responses indicating the action taken or suggesting what should be done.\n\n\n## Comparative Analysis: LLaVa vs. Contemporary Multimodal Models\n\nThe landscape of multimodal AI has been rapidly evolving with innovations such as CLIP, BLIP, and the recent introduction of LLaVa. This subsection compares LLaVa’s unique architecture and approach with these contemporary models, highlighting the advancements and distinctions that set it apart.\n\n\n### CLIP: Pioneering Multimodal Understanding\n\nCLIP (Contrastive Language–Image Pre\\-training) has been a revolutionary step forward in multimodal AI, offering robust performance across a variety of visual tasks. Its ability to understand images in the context of natural language descriptions set a new benchmark in the field. CLIP achieves this through a large\\-scale pretraining approach that aligns images with textual descriptions, enabling the model to perform zero\\-shot learning on a range of visual tasks. However, CLIP primarily focuses on the association between images and text at a high level and does not inherently possess the capability for in\\-depth reasoning or conversational engagement.\n\n\n### BLIP: Bridging Language and Image Perception\n\nBuilding upon the foundation laid by CLIP, BLIP (Bootstrapped Language Image Pre\\-training) extends the capabilities of multimodal models by incorporating a bootstrapped pretraining strategy. This approach refines the model’s visual understanding by continually learning from its own predictions, which helps to improve the alignment between language and visual content. BLIP demonstrates enhanced performance on tasks that require more precise visual recognition and language understanding.\n\nIn contrast, LLaVa takes a different route by leveraging the language\\-generating capabilities of GPT\\-4 to curate its instruction\\-following data. This not only results in a dataset that captures a broader range of human\\-like interactions but also enables LLaVa to engage in more complex reasoning and in\\-depth conversational abilities.\n\n\n## What Sets LLaVa Apart: Is It the Model Architecture or Something Else?\n\nAccording to us , LLaVA’s strength lies predominantly in its data curation capabilities rather than its architectural choice. LLaVA marks a significant leap forward , primarily due to its utilization of GPT\\-4 for data curation. Unlike conventional static datasets, LLaVA generates dynamic, instructive data using ChatGPT\\-4, actively involving data in the training process across various visual and textual scenarios.\n\nBy using GPT\\-4, LLaVA produces datasets that closely mimic natural language and visual perception, departing from traditional manual dataset generation methods. This innovative approach not only enables AI to understand and reason but also moves it closer to accurately reflecting human intelligence.\n\n\n### Data Curation Strategies in LLaVa\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*LzastWLkzPeMB_28Nr7Y9A.png)\n\nLLaVa, the Large Language and Vision Assistant, stands out not just for its advanced neural architecture but for its groundbreaking approach to data curation. By leveraging GPT\\-4, it revolutionizes traditional data preparation methods, crafting a dataset that mirrors the complexity of the real world.\n\nData curation in LLaVa begins with an image and its corresponding caption, from which a set of queries is generated using GPT\\-4\\. These queries guide the AI to explore and describe the image content with precision and relevance.\n\nTo translate visual data effectively for a text\\-based AI like GPT\\-4, LLaVa uses captions to offer diverse perspectives of the visual scene and bounding boxes to provide spatial context and focus.\n\n1. Conversational Data: Mimicing human interaction, LLaVa curates dialogues where the model, playing the assistant, responds to questions about various aspects of the image. The scope of these questions ranges from identifying objects and actions to discerning their numbers, locations, and relative positions, ensuring the model can handle queries with definitive answers.\n2. Detailed Descriptive Data: LLaVa seeks to comprehend the images in a comprehensive manner. To achieve this, it prompts GPT\\-4 to formulate questions aimed at understanding rich and detailed descriptions of the images. These prompts encourage the model to delve deeper, providing a narrative that captures the essence of the visual content in its entirety.\n3. Complex Reasoning Data: Moving beyond mere description, LLaVa challenges the model with questions that necessitate a layered reasoning process, demanding logic and an understanding of cause and effect. This type of data trains the model to construct well\\-reasoned responses that are backed by a logical sequence of thought.\n\n\n## The Architecture of LLaVa: Integrating Vision and Language\n\nThe LLaVa model integrates vision and language, utilizing the following core components:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8q_Iay_LHCzPqtrQby_H8w.png)\n\n1. Vision Encoder: At the foundation of LLaVa’s architecture is the pre\\-trained CLIP visual encoder, specifically the ViT\\-L/14 variant. This component processes input images (Xv) through Transformer layers to extract features (Zv), enabling the model to understand visual information effectively.\n2. Language Model (Vicuna): LLaVa’s linguistic capabilities rely on Vicuna, a variant of a large language model (LLM) denoted by fϕ . Vicuna comprehends and generates language responses (Xa) based on input language instructions (Xq), complementing the vision encoder’s functionality.\n3. Linear Projection: This component, represented by a trainable matrix (W), serves as the bridge between visual features (Zv) and the language model’s embedding space. It transforms visual features into visual tokens (Hv), aligning them with the language model’s word embedding space to facilitate multimodal conversation\n\n\n## Training and Fine\\-Tuning LLaVA:\n\nLLaVA’s has a two\\-stage training process, each stage focusing on refining the model’s capabilities to interpret and respond to a fusion of visual and textual data.\n\n\n### Stage 1: Pre\\-training for Feature Alignment\n\nThe initial stage of LLaVA’s training is pre\\-training for feature alignment. In this phase, the model focuses on aligning visual features from images with the corresponding textual features from the language model. This is achieved by filtering a large dataset to a refined set of image\\-text pairs, which LLaVA uses to learn the correlations between the two modalities.\n\nDuring this stage, a visual encoder (such as the CLIP visual encoder ViT\\-L/14\\) processes the images to extract visual features, and a projection matrix (W) is then used to map these features into the word embedding space of the language model. The language model used in LLaVA is Vicuna, known for its strong language understanding and generation capabilities.\n\n\n### Stage 2: Fine\\-tuning End\\-to\\-End\n\nAfter aligning the visual and language features, LLaVA undergoes an end\\-to\\-end fine\\-tuning process. Despite keeping the visual encoder’s weights frozen, this stage allows the model to fine\\-tune the weights of the projection matrix and language model jointly. The objective is to maximize the likelihood of the target answers based on the multimodal data provided.\n\nThis stage is critical for adapting LLaVA to specific use case scenarios such as multimodal chat, scientific Q\\&A, and more. It ensures that the model does not just understand images in the context of generic descriptions but can engage in complex dialogues, provide detailed explanations, and reason through problems when prompted with specific questions related to the images.\n\n\n## Performance and Benchmarking: LLaVa in the Context of VQA Models\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*I_5fTa_2rtNHEDUaDNMXbQ.png)\n\n\n## LLaVA\\-Bench (COCO) Performance Insights\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*6B2K7EcbYgMbH-QEp8J41w.png)\n\nLLaVA\\-Bench (COCO) provides a robust framework for assessing LLaVA’s capabilities through a carefully crafted set of 90 questions, derived from 30 selected images for conversation, detailed description, and complex reasoning. The results were as follows:\n\n* Instruction Tuning Efficacy: When equipped with instruction tuning, LLaVA’s compliance with user commands improved by over 50 points.\n* Impact of Question Variety: The inclusion of detailed and complex reasoning questions, though minimal, led to a 7\\-point increase in overall capabilities. This boost also had a positive effect on conversational question responses, showcasing the benefits of a diverse training set.\n* Optimal Data Mix: The combination of all three question types resulted in the highest performance leap, with LLaVA reaching a benchmark score of 85\\.1%, emphasizing the strength of a comprehensive dataset in enhancing multimodal AI proficiency.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*mCjP0xfpcjHkl-lu)\n\n\n## LLaVA’s Performance on LLaVA\\-Bench (In\\-the\\-Wild)\n\n* In conversational tasks, LLaVA achieves a 57\\.3% accuracy rate, a clear improvement over BLIP\\-2’s 54\\.6% and significantly outpacing OpenAI’s Flamingo, which stands at 19\\.3%.\n* When it comes to providing detailed descriptions, LLaVA scores 52\\.5%, showcasing its ability to generate rich, comprehensive content from visual cues.\n* The model’s prowess is most notable in complex reasoning questions, where it achieves an 81\\.7% success rate, indicating its advanced reasoning and inferencing skills.\n\nLLaVA secures a combined score of 67\\.3% across all categories, surpassing BLIP\\-2 by a 29% margin and Flamingo by 48%.\n\n\n## Limitation and Concerns:\n\nQuantitative Evaluation of LLaVA:\n\nThe utilization of GPT\\-4 as a judge to evaluate LLaVA’s performance presents a nuanced challenge within the framework of benchmarking AI capabilities. On one hand, GPT\\-4’s advanced comprehension and generation abilities enable it to critically assess the quality of responses produced by candidate models like LLaVA. This assessment encompasses factors such as helpfulness, relevance, accuracy, and detail, which are crucial for gauging a model’s instruction\\-following proficiency with multimodal data. However, on the other hand, the use of GPT\\-4 as an evaluative judge raises concerns regarding the impartiality of the benchmarking process.\n\nThe crux of the concern lies in the fact that LLaVA’s data curation process is fundamentally intertwined with GPT\\-4\\. Since GPT\\-4 has been instrumental in training LLaVA — by generating the instruction\\-following data that the model was fine\\-tuned on — there is an inherent risk of circular reasoning. Essentially, there is a possibility that LLaVA may be predisposed to generate responses that align with the patterns or biases inherent in GPT\\-4’s training data. This predisposition could skew the evaluation, leading to a theoretical upper bound that reflects compatibility with GPT\\-4’s methodology rather than a true measure of universal performance.\n\nFurthermore, relying on GPT\\-4 to provide a comprehensive explanation for its evaluation introduces a level of subjectivity rooted in the language model’s own “understanding” of what constitutes a high\\-quality response. This understanding is shaped by the datasets on which GPT\\-4 was trained, which may not fully encapsulate the diversity and complexity of real\\-world multimodal interactions.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/key-points-llm-quantization-chatgpt-artificial-intelligence-8201ffcb33d4","frontmatter":{"title":"5 Key Points to Unlock LLM Quantization","meta_title":"5 Key Points to Unlock LLM Quantization","description":"Quantizing Large Language Models","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*RUqPEr2NTYXlI1omqF22Qg.png","categories":["Machine Learning","Data Science","Technology/Web"],"author":"Rifx.Online","tags":["quantization","weights","activations","calibration","Quanto"],"draft":false,"slug":"blog/key-points-llm-quantization-chatgpt-artificial-intelligence-8201ffcb33d4"},"content":"\n\n\n\n\n### Quantizing Large Language Models\n\n\n\nLLM Quantization is currently a hot topic due to its vital role in making Large Language Models (LLMs) more efficient and deployable across various hardware platforms, including consumer-grade devices.\n\nBy adjusting the precision of certain components within the model, **quantization significantly reduces the model’s memory footprint** while maintaining similar performance levels.\n\nIn this guide, we will explore five key aspects of LLM quantization including some practical steps for applying this technique to our models.\n\n\n## #1. Understanding Quantization\n\nQuantization is a model compression technique that reduces the precision of weights and activations in an LLM. This involves converting high-precision values to lower-precision ones, effectively **changing data types that store more information to those that store less**.\n\nDecreasing the number of bits needed for each weight or activation significantly reduces the overall model size. As a result, **quantization creates LLMs that use less memory, and require less storage space.**\n\nThis technique has become essential in response to the exponential growth in the number of parameters in successive iterations of LLMs. For example, for the OpenAI’s GPT family, we can observe the growing trend in the following graph:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*QlAhma3Wu1F6w2WvkE8jDA.png)\n\nThis significant increase presents a challenge: as models grow, their memory requirements often exceed the capacity of advanced hardware accelerators such as GPUs. **This requires distributed training and inference to manage these models, which in turn limits their deployability.**\n\n\n## #2. Intuition Behind Quantization\n\nAlthough the definition of quantization may seem rather complex, the concept can be intuitively explained using matrices.\n\nLet’s consider the following a 3x3 matrix representing the weights of a neural network. The matrix on the left shows the original weights, while the matrix on the right shows the quantized version of these weights:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*LPzWe9oxjlDYdSp7dVvRUg.png)\n\nIn this simple example, we round the elements of the original matrix from four decimal places to a single decimal place. Although the matrices appear similar, **the storage space required for the four-decimal version is significantly higher**.\n\nIn practice, quantization is not merely a rounding operation. Instead, it involves converting numerical values to a different data type, typically from a higher to a lower precision one.\n\nFor example, the default data type for most models is `float32`, which requires 4 bytes per parameter (32 bits). Therefore, for a 3x3 matrix, the total memory footprint is 36 bytes. Changing the data type to `int8`, only 1 byte per parameter is needed, reducing the total memory footprint of the matrix to just 9 bytes.\n\n\n## #3. Quantization Error\n\nAs we have seen, the original matrix and its quantized form are not completely equal, but very similar. The value-by-value difference is known as “Quantization error”, which we can also represent in matrix form:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*VtGDjVbr7daagLXB57i7Mg.png)\n\n**This quantization error can accumulate for each matrix of weights in the network, affecting the model’s performance as a result.**\n\nCurrent research in quantization aims to minimize the difference in precision while decreasing the computational resources required to train or run inference on models, while maintaining acceptable performance levels.\n\n\n## #4. Linear Quantization\n\nLinear quantization is one of the most popular quantization schemes for LLMs. In simple terms, it involves mapping the range of floating-point values of the original weights to a range of fixed-point values.\n\nLet’s review the steps required to apply linear quantization to our models:\n\n* **Get the minimum and maximum ranges:** We need to get the minimum and maximum values of the floating-point weights to be quantized (`x_min` and `x_max`). We also need to define the quantized range (`q_min` and `q_max`), which is already set by the data type we want to convert to.\n* **Compute the scale (`s`) and the zero-point (`z`) values:** Firstly, the scale (`s`) adjusts the range of floating-point values to fit within the integer range, preserving the data distribution and range. Secondly, the zero-point (`z`) ensures that zero in the floating-point range is accurately represented by an integer, maintaining numerical accuracy and stability, especially for values close to zero.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BepC6-izw0yE19ejsS705Q.png)\n\n* **Quantize the values (`q`)**: We need to map the original floating-point values to the integer range using a scale factor (`s`) and a zero point (`z`) computed in the previous step.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BBOQ0VbSGbwf7CN8c4PWKQ.png)\n\nApplying these formulas is quite straightforward. If we apply them to the 3x3 weight tensor on the left in the image below, we will get the quantized matrix shown on the right:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*KzBvg84mfI2gAhTIyVibwQ.png)\n\nWe can see that the lower bound of the `int8` value corresponds to the lower value of the original tensor, while the upper bound corresponds to the higher value of the original tensor, *i.e., the mapping is`0.50 → 255` and `-0.40 → 0`.*\n\nWe can now dequantize the values using the formula below.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*E5nnqYzncYCRuM5prssuOw.png)\n\nIf we place the dequantized values again in matrix form (matrix on the left), we can compute the quantization error (matrix on the right) by calculating the point-by-point difference between the original matrix and its dequantized version:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*56NALu9PAN95QG2hn8HXoQ.png)\n\nAs we can observe, the quantization error starts kicking in for some of the matrix values.\n\n\n## #5. Weight Quantization vs Activation Quantization\n\nIn our example above, we have focused primarily on quantizing the weights of the model. While weight quantization is crucial for model optimization, it’s also important to consider that activations can be quantized as well.\n\n**Activation quantization involves reducing the precision of the intermediate outputs of each layer in the network**. Unlike weights, which remain constant once the model is trained, activations are dynamic and change with each input, making their range harder to predict.\n\nGenerally, activation quantization is more challenging to implement than weight quantization because it requires careful calibration to ensure the dynamic range of activations is accurately captured.\n\nWeight quantization and activation quantization are complementary techniques. Using both can significantly reduce model size without greatly compromising performance.\n\n\n## Final Thoughts\n\nIn this article, we have reviewed 5 key points about quantization to better understand how to reduce the size of these constantly growing models.\n\nAs for the implementation of those techniques, there are several tools and libraries in Python that support quantization such as `pytorch` and `tensorflow`. Nevertheless, integrating quantization seamlessly in existing models requires a deep understanding of the libraries and model internals.\n\nThat is why my favorite option to implement quantization in easy steps so far is the [Quanto](https://huggingface.co/blog/quanto-introduction) library by Hugging Face, designed to simplify the quantization process for PyTorch models.\n\nIf you are interested in the in-depths of LLM Quantization and how to use the aforementioned library, you might also be interested in the article [“Quantization for Large Language Models (LLMs): Reduce AI Model Sizes Efficiently”](https://www.datacamp.com/tutorial/quantization-for-large-language-models).\n\nThat is all! Many thanks for reading!\n\nI hope this article helps you when **using LLMs for coding!**\n\nYou can also subscribe to my [**Newsletter**](https://readmedium.com/@andvalenzuela/subscribe) to stay tuned for new content.\n\n**Especially**, **if you are interested in articles about Large Language Models and ChatGPT**:\n\n\n"},{"lang":"en","group":"blog","slug":"blog/langgraph-vs-langchain-vs-langflow-vs-langsmith-which-one-to-use-why-69ee91e91000","frontmatter":{"title":"LangGraph vs. LangChain vs. LangFlow vs. LangSmith: Which One to Use & Why?","meta_title":"LangGraph vs. LangChain vs. LangFlow vs. LangSmith: Which One to Use & Why?","description":"Discover the key differences between LangGraph, LangChain, LangFlow, and LangSmith, and learn which framework is best suited for your…","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*xrWv1QVt4zE5cxjA8VA3ag.png","categories":["Programming","Technology","Technology/Web"],"author":"Rifx.Online","tags":["LangGraph","LangChain","LangFlow","LangSmith","frameworks"],"draft":false,"slug":"blog/langgraph-vs-langchain-vs-langflow-vs-langsmith-which-one-to-use-why-69ee91e91000"},"content":"\n\n\n\n\n### Discover the key differences between LangGraph, LangChain, LangFlow, and LangSmith, and learn which framework is best suited for your language model applications — from workflow building to performance monitoring.\n\n👨🏾‍💻 [GitHub](https://github.com/mdmonsurali) ⭐️ | 👔[LinkedIn](https://www.linkedin.com/in/mdmonsurali/) |📝 [Medium](https://medium.com/@monsuralirana)\n\n\n\nIn recent years, the world of natural language processing (NLP) has witnessed an explosion in the number of frameworks, libraries, and tools available for building language model-based applications. Among these, **LangGraph**, **LangChain**, **LangFlow**, and **LangSmith** have emerged as leading options, each catering to different use cases and user needs. If you’re looking to build, monitor, or scale language model workflows, it’s crucial to understand the strengths and purposes of these tools.\n\nIn this blog, we’ll explore each framework, break down their strengths, and provide insights into when to use them. Whether you’re a seasoned developer or a newcomer to the field, understanding the nuances of these tools will help you choose the right one for your project.\n\n\n## Introduction to Language Model Frameworks\n\nWith the rise of powerful language models such as GPT-3, GPT-4, and other transformer-based models, there is a growing need for frameworks that streamline the creation and management of language-based applications. These frameworks simplify complex tasks like **chaining multiple prompts**, **retrieving relevant documents**, and even **monitoring model performance**.\n\nHowever, not all frameworks are the same. While some provide a **visual interface** to manage workflows, others offer advanced **debugging and observability** features. Let’s dive into each of these tools to understand their unique offerings.\n\n\n## 1. LangGraph: Visualizing Complex Workflows\n\n**LangGraph** is a newer framework designed for developers who prefer a **visual approach** to building language model pipelines. It allows you to structure complex workflows with **graph-based visualizations**, making it easier to understand dependencies between different tasks and components. This can be especially useful for larger applications where multiple steps, such as text generation, document retrieval, and classification, are chained together.\n\n\n### Strengths:\n\n* **Visual Workflow Representation**: LangGraph lets you visualize the flow of data and actions between different components. This graphical approach is intuitive and helps in designing more complex pipelines.\n* **Ease of Debugging**: The visual nature of LangGraph makes it easier to identify bottlenecks or problematic nodes in a workflow.\n\n\n### Example Use Case:\n\nSuppose you’re building an automated system that first retrieves relevant documents using a language model and then passes them through a summarizer. In LangGraph, you can visually map out this workflow, showing the relationships between each step. If there’s an issue at any point in the chain, the visual tool makes it easy to pinpoint where things went wrong.\n\n\n### When to Use LangGraph:\n\nIf you’re managing **complex workflows** with multiple steps and value a **graphical interface** for understanding your pipeline, LangGraph is a fantastic choice. It’s particularly helpful for developers or data scientists who prefer a more intuitive, drag-and-drop approach to workflow design.\n\n**Key points**:\n\n* If you need a clear visual representation of language processing workflows.\n* When creating more complex pipelines that require branching or multi-path dependencies.\n\n\n## 2. LangChain: The Workhorse for LLM Applications\n\n**LangChain** is one of the most popular frameworks for building applications powered by **large language models (LLMs)**. It provides a versatile, **code-first approach**, allowing developers to chain tasks such as document retrieval, summarization, and question-answering into cohesive workflows.\n\n\n### Strengths:\n\n* **Extensive Support for LLMs**: LangChain is compatible with various language models, making it easy to integrate models like OpenAI’s GPT or even locally hosted models.\n* **Chaining Capabilities**: LangChain excels at **chaining multiple operations** — hence the name — enabling developers to create sophisticated NLP applications.\n* **Wide Adoption**: As one of the most popular frameworks, LangChain has a **thriving community** and excellent support, with ample documentation and tutorials.\n\n\n### Example Use Case:\n\nImagine you’re building a **chatbot** that first understands the user’s question, retrieves relevant information from a database, and then generates a response. With LangChain, you can easily create this multi-step process programmatically, ensuring each step in the chain works harmoniously.\n\n\n### When to Use LangChain:\n\nIf you’re a **developer building production-level applications** and need a **flexible, code-centric solution**, LangChain is your best bet. It’s ideal for those who prefer control over their application’s architecture and are comfortable writing code to define workflows.\n\n**Key points**:\n\n* If you’re building production-grade applications that require chaining of tasks across multiple language models.\n* If you need a library with extensive community support and wide-ranging integrations.\n* When you’re more comfortable with programmatic solutions rather than visual tools.\n\n\n## 3. LangFlow: No-Code/Low-Code Extension of LangChain\n\n**LangFlow** is essentially a **visual extension of LangChain**. It combines the powerful backend of LangChain with an **intuitive drag-and-drop interface**. LangFlow allows users who might not be as comfortable writing code to still leverage the power of language models in their applications.\n\n\n### Strengths:\n\n* **Visual Workflow Creation**: Like LangGraph, LangFlow provides a visual interface for building workflows. However, it’s specifically built on top of LangChain, meaning users can harness LangChain’s power without needing to write extensive code.\n* **Ideal for Rapid Prototyping**: LangFlow is perfect for quickly **prototyping ideas** or building out proof-of-concept applications.\n* **Beginner-Friendly**: It’s a great entry point for users who are less familiar with coding but want to create language model workflows.\n\n\n### Example Use Case:\n\nIf you want to quickly build a **summarization tool** that retrieves documents, you can drag and drop the components in LangFlow’s interface to create a fully functioning application. This can be done without writing much code, if any.\n\n\n### When to Use LangFlow:\n\nLangFlow is perfect for **non-developers** or **rapid prototyping**. If you want to experiment with **LLM workflows quickly** without delving into the code, this tool makes it easy to get started.\n\n**Key points**:\n\n* If you want to prototype LLM workflows quickly without writing code.\n* If you’re comfortable with visual programming but need the flexibility of LangChain.\n* For educational purposes, to help users learn how workflows can be constructed.\n\n\n## 4. LangSmith: Monitoring and Observability\n\nWhile the other tools focus on **building workflows**, **LangSmith** is designed for **monitoring** and **debugging** language model applications. It provides advanced observability features to track the performance of your workflows and models, making it invaluable for production environments.\n\n\n### Strengths:\n\n* **Deep Observability**: LangSmith allows developers to monitor language model performance, ensuring that workflows behave as expected.\n* **Error Tracking**: It excels at helping developers track down issues, making debugging easier.\n* **Performance Insights**: LangSmith gives insights into **workflow performance**, helping developers optimize their applications.\n\n\n### Example Use Case:\n\nLet’s say you’ve deployed a **customer service chatbot** that uses a language model to answer questions. Over time, you notice that some responses are less accurate than expected. LangSmith can help you trace the problem by providing visibility into each decision point within the workflow.\n\n\n### When to Use LangSmith:\n\nIf you’re deploying applications in **production environments** and need to ensure **robustness, reliability, and performance**, LangSmith is an essential tool. It’s particularly useful when managing **complex systems that require debugging and optimization** over time.\n\n**Key points**:\n\n* If you need advanced monitoring or debugging capabilities in LLM workflows.\n* For development environments where observability is key to ensuring optimal model performance.\n* If your focus is on improving and iterating LLM-powered applications based on real-time insights.\n\n\n## Which One to Choose?\n\n* **Use LangGraph** if you prefer graph-based, visual workflows for building complex LLM tasks. Ideal for users who need clarity and structure.\n* **Use LangChain** if you need a robust, flexible solution for creating language model applications programmatically. It’s versatile and great for developers building production-level applications.\n* **Use LangFlow** if you want the power of LangChain with a visual, no-code/low-code interface. Best for rapid prototyping and users who prefer visual tools over coding.\n* **Use LangSmith** if your focus is on observability and debugging of LLM applications. Ideal when you need to monitor and optimize workflows in a development or production environment.\n\nUltimately, your choice depends on your comfort with code, the complexity of your workflows, and whether you prioritize ease of use, flexibility, or observability.\n\n\n## Conclusion\n\nEach of these tools — **LangGraph**, **LangChain**, **LangFlow**, and **LangSmith** — caters to different stages of developing and managing language model applications. **LangGraph** provides a visual, intuitive way to build complex workflows, while **LangChain** offers a robust, code-first solution for developers looking to create scalable applications. For those who prefer a **low-code**, drag-and-drop approach, **LangFlow** simplifies the process without sacrificing power. Finally, **LangSmith** focuses on observability and debugging, ensuring that your workflows are optimized and reliable. Choosing the right tool depends on your project needs, whether it’s for rapid prototyping, production-level scaling, or monitoring and performance tracking.\n\nHappy coding! 🎉\n\n👨🏾‍💻 [GitHub](https://github.com/mdmonsurali) ⭐️ | 👔[LinkedIn](https://www.linkedin.com/in/mdmonsurali/) |📝 [Medium](https://medium.com/@monsuralirana)\n\nThank you for your time in reading this post!\n\nMake sure to leave your feedback and comments. See you in the next blog, stay tuned 📢\n\n\n## References:\n\n1. “LangChain Documentation” — <https://python.langchain.com/docs/introduction/>\n2. “LangGraph Overview” — <https://langchain-ai.github.io/langgraph/>\n3. “LangFlow GitHub Repository” — [https://github.com/LangFlow/LangFlow](https://docs.langflow.org/)\n4. “LangSmith Introduction” — <https://www.langchain.com/langsmith>\n5. “How to Build Chatbots With LangChain” by JetBrains blog — <https://blog.jetbrains.com/pycharm/2024/08/how-to-build-chatbots-with-langchain/>\n\n"},{"lang":"en","group":"blog","slug":"blog/large-language-models-just-got-a-whole-lot-smaller-f93425ee59a2","frontmatter":{"title":"Large Language Models Just Got A Whole Lot Smaller","meta_title":"Large Language Models Just Got A Whole Lot Smaller","description":"And how this might change the game for software startups","date":"2024-11-04T12:29:02.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*1PeFyz_Dlt6jEf27Q9Y33Q.png","categories":["Programming","Technology","Machine Learning"],"author":"Rifx.Online","tags":["compression","optimization","ternary","parallelism","hardware"],"draft":false,"slug":"blog/large-language-models-just-got-a-whole-lot-smaller-f93425ee59a2"},"content":"\n### And how this might change the game for software startups\n\n\n\n**This piece was co\\-written with [David Meiborg](https://readmedium.com/undefined).**\n\n*TLDR: Large Language Models (LLMs for short) are currently huge, costly to run, and have a [significant carbon footprint](https://arxiv.org/abs/2309.14393). Recent advancements in model compression and system\\-level optimization methods might, however, enhance LLM inference. In particular, an approach using parameters with ternary structure has the potential of circumventing much of the costly matrix multiplication that is standard today. This has exciting consequences for hardware startups making specialized chips, but also for software startups that use or custom\\-build their own LLMs. Startups that help their customers deploy LLMs might also have more business coming for them.*\n\nLarge language models today are big. Like, really big. If you want to load a LlaMa\\-2–70B model, you’d need 140 GB of VRAM (that’s 70 billion parameters multiplied by 2 bytes per parameter). For comparison, GPUs like the NVIDIA RTX 3090 or 4090, have just 24 GB of VRAM — a fraction of what one would need.\n\nThere are some [workarounds with quantization](https://towardsdatascience.com/run-llama-2-70b-on-your-gpu-with-exllamav2-588141a88598), but these tend to be cumbersome. Likely you will still have your GPU running hot for up to 15 hours until the model is loaded. Not to mention that you still need some spare memory for inference, or in other words for deploying the model.\n\nUsing current\\-day LLMs is therefore costly: One typically needs multiple high\\-end GPUs to keep the model, and must then account for the energy costs from inference.\n\nThis is why lots of research is going into applying techniques that make LLMs smaller and thus cheaper to run on smaller hardware. It is a tough trade\\-off in most cases, because making LLMs smaller usually impacts their quality. Finding the point where cost equals benefits can be tricky.\n\nIn this piece, we give an overview of promising optimization approaches, explain a recent breakthrough from Microsoft researchers, provide a brief overview of innovative startups in the field of “efficient LLMs” and derive some general implications for startups operating in the LLM ecosystem.\n\n## How LLMs are getting more resource\\-efficient\n\nTech giants like Microsoft and OpenAI, Meta, or Google have sufficient resources to train up cutting\\-edge models even if the training cost is currently prohibitive for most other companies. The biggest bottleneck to widespread adoption therefore is not training but inference efficiency. In other words, although Meta has published LlaMa, it still isn’t being adopted enough because running — not creating — the model is already challenging enough.\n\nResearchers, however, are starting to increase this inference efficiency. Broadly speaking, there are two approaches to this: **System\\-level optimizations** do not change the model itself but rather make it work better by changing key aspects of the environment that it is in. **Model optimizations** compress the model so that it is easier to deploy and run.\n\nThere is a variety of different techniques for either approach. [A recent paper](https://arxiv.org/pdf/2402.01799.pdf) by researchers summarize these techniques excellently. Because these techniques might soon become basic knowledge for anyone working on systems with LLMs, we give a quick overview over these techniques below.\n\n### System\\-level optimization\n\nSystem\\-level optimization refers to changing not the model itself, but how it is run across the hardware. As it turns out, plenty of levers can be pulled to avoid resources sitting around idle or wiping out other inefficiencies.\n\n**Paged Attention**\n\nAt the heart of LLMs like GPT is the attention mechanism. This mechanism allows the model to focus on different parts of the input text when generating each word of the output text. Imagine you are reading a book and highlighting important sentences to remember the story better. Similarly, the attention mechanism “highlights” or gives more importance to certain words or phrases when making predictions.\n\nThis mechanism is very resource\\-intensive. It requires the model to consider the relationships between all pairs of words in the input text. For long texts, this can require a lot of memory and computational power.\n\nInstead of processing the entire text at once, paged attention divides the text into smaller “pages” or segments. The model then processes these pages one at a time or in smaller groups. This approach significantly reduces the amount of memory needed at any given time because the model doesn’t need to keep track of the entire text’s relationships simultaneously.\n\nThis is a bit like a student who would be overwhelmed by reading the entire year’s textbook at once. By breaking it down into manageable segments over the school year, the student can memorize the contents of the textbook.\n\nBy requiring less memory for each step, paged attention allows for the use of larger models or longer texts within the same hardware constraints.\n\n**Tensor Parallelism**\n\nParallelism is a well\\-known concept in computing. It means dividing a large computational task into smaller parts that can be processed simultaneously by multiple processors or computers. This significant speeds up the time a program needs to run.\n\n[Tensors](https://towardsdatascience.com/what-is-a-tensor-in-deep-learning-6dedd95d6507), in the context of LLMs, are multi\\-dimensional arrays of numbers. These tensors are used to represent the data processed by the models. Such data includes input text; model weights, i.e. parameters that the model learns; and output predictions.\n\nPutting both concepts together, tensor parallelism involves splitting these tensors across multiple GPUs or other processing units. For example, if a model’s parameters (weights) are too large to fit into the memory of a single GPU, they can be divided across multiple GPUs. Each GPU then processes only a portion of the tensor at a time.\n\nJust like a team of multiple people working on a large project, the processing units need to exchange information as they work on their respective parts of the tensors. For instance, the results of computations on one GPU might need to be shared with another GPU to continue the next step in the computation. Efficient communication between the units is therefore crucial for the effectiveness of tensor parallelism.\n\nIn short, tensor parallelism is a way of breaking down the computations needed for LLMs into smaller, parallel tasks that can be handled simultaneously by multiple computing units, leading to faster training and inference times for these large and complex models.\n\n**Pipeline Parallelism**\n\nThis technique focuses on improving the workflow of processing data through the model’s layers. This can significantly speed up the overall computation and make better use of available hardware.\n\nA pipeline in computing works similarly to a factory assembly line, with different stages of a task being completed in sequence. This allows for multiple tasks to be worked on simultaneously but at different stages.\n\nIn LLMs, these different stages are represented by layers of neural networks. Each layer processes the input data in sequence, gradually extracting more complex features or patterns until it produces the final output. Think of each layer as a worker in the factory assembly line: Each worker adds something to the input data as it passes through, until finally a complex product emerges.\n\nIn pipeline parallelism, the model’s layers are divided into segments, and each segment is assigned to a different GPU or processing unit. This way, the model can be fed on batches of data: Once the first segment is through with the first batch, the second segment takes that batch, and the first segment takes a fresh, new batch on.\n\nThis creates a continuous flow of data through the model where each segment of the model is working on a different piece of data at any given time. This maximizes the use of available hardware resources by keeping all parts of the model active and reduces the idle time that can occur when a single processor waits for tasks to complete.\n\nPipeline parallelism, which was discussed earlier, operates at the level of model layers, distributing the sequential processing stages across devices. Tensor parallelism, on the other hand, operates at a more granular level, distributing the actual computations (e.g., parts of a large matrix multiplication) that occur within layers across devices.\n\n**CPU/GPU Offloading**\n\nWe have talked a lot about GPUs in this piece. Nevertheless, ot all tasks in training or running an LLM are equally suited to GPUs. Some tasks, like data preprocessing or certain control logic, might be more efficiently handled by a CPU. Other tasks, particularly the heavy mathematical computations involved in processing neural networks (like matrix multiplications), are indeed more efficiently executed on GPUs.\n\nBy offloading specific tasks to the processor best suited for them — GPUs for parallelizable, computation\\-heavy tasks, and CPUs for sequential or logic\\-intensive tasks — systems can ensure that each part of the workload is processed in the most efficient manner possible.\n\n**Fused Operations**\n\nFused operations take multiple processing steps that would normally be executed separately and combine them into a single, streamlined operation. For instance, instead of doing a matrix multiplication and then an addition, a fused operation would do both at once.\n\n**Speculative Decoding**\n\nWhen generating text, LLMs calculate the probabilities of what the next word in a sentence might be, based on the words that have come before. Traditionally, after each word is generated, the model recalculates to determine the next word, and this process repeats until the full sentence or paragraph is completed. This sequential process can be slow, however, especially for longer texts or more complex models, because each step depends on the completion of the previous step.\n\nParallel Predictions: Instead of waiting for each word to be chosen before considering the next, speculative decoding allows the model to “speculate” or make multiple predictions about what the next few words could be at the same time. This is called *parallel predictions*. It’s like making educated guesses about several possible paths the sentence could take next\n\nBy exploring these possibilities in parallel, the model can potentially reduce the overall time it takes to generate text. Once the actual next word is selected, the model can more quickly proceed along the most likely path, having already computed the subsequent options.\n\n### Compression of LLM Models\n\nResearchers have in the past explored model compression. With the advent of large\\-scale LLMs, however, this has become a bigger challenge.\n\nMany established compression methods rely on the paradigm of executing fine\\-tuning steps to regain lost performance during the compression stage. This approach has significant limitations, however, when applied to LLMs because of their sheer size. LLM compression has therefore become a whole new research domain.\n\n**Architecture pruning**\n\nWhen you prune an apple tree, you cut off certain branches in winter early spring. This ensures that the tree doesn’t waste resources on unproductive branches or catches diseases from dead wood. This helps it produce better fruit.\n\nLLMs, of course, don’t produce fruit. In this context, pruning is a method used to reduce the size of the model while trying to maintain or minimally impact its performance.\n\nLLM models have millions or even billions of parameters. Not all of these parameters are equally important for the model to make predictions or understand language. Some parameters are used rarely or don’t contribute much to the model’s decisions: Eliminating these redundant or less impactful connections, neurons, or entire layers hence makes the model more efficient to use.\n\nChoosing which parameters to prune is not a trivial task. In magnitude\\-based pruning, the weights of the neural network with the smallest absolute values are removed. Before training, such weights are usually zero; after training, they are typically somewhere between \\-1 and 1\\. If training did not affect a weight very much, then it is likely close to zero, and thus contributes less to the model’s decisions.\n\nA more resource\\-intensive but also more robust pruning technique is sensitivity analysis. This involves evaluating the impact of removing each parameter, or group of parameters, on the model’s performance. Parameters whose removal causes the least degradation in performance are pruned.\n\nThere are other techniques as well, but generally one can classify them as unstructured or structured pruning. Unstructured pruning (e.g. magnitude\\-based pruning) removes individual weights, leading to a sparsely connected neural network. Structured pruning (e.g. sensitivity analysis) removes entire units or layers (e.g., a whole neuron or channel), which can be more effective for computational efficiency on certain hardware.\n\nAfter pruning, the model often undergoes a fine\\-tuning process. This involves retraining the pruned model on the training dataset or a subset of it. The goal is to allow the model to adjust and optimize its remaining parameters to compensate for the loss of the pruned ones. This helps in recovering any performance that was lost due to pruning.\n\nThis can be done in an iterative or in a one\\-shot approach. In iterative pruning, the model is pruned iteratively over several rounds. After each round, the pruned model is retrained to regain performance lost due to pruning. This cycle can be repeated multiple times, with the model potentially becoming more robust and maintaining performance even with significantly fewer parameters. In one\\-shot pruning, all the identified parameters are removed at once, and the model is then fine\\-tuned.\n\n**Knowledge distillation**\n\nImagine there is a football court with two players: One is very experienced and knows many tricks, the other is a beginner. The experienced player knows much more than the beginner, but the beginner can quickly get to a comparable behavior by mimicking the other player’s behavior on the field.\n\nKnowledge distillation for LLMs works similarly: It is the process of training a smaller (student model), more efficient model to replicate the performance of a larger model (teacher model) by learning from its outputs and the way it processes information.\n\nTo apply this technique, you obviously need a large teacher model, e.g. one of the large open\\-source models from LlaMa or Mistral. Then you need to design a smaller neural network that has significantly fewer parameters than the teacher model.\n\nInstead of training the student model solely on the original hard targets, i.e., the ground truth data labels, it is also trained on the soft targets. These are the probabilities produced by the teacher model for the same inputs. For example, for a given set of queries, imagine that the teacher answers it as ‘A’ 70 percent of the time, ‘B’ 20 percent of the time, and ‘C’, ‘D’, or ‘E’ 10 percent of the time. Not only will the student model try to get the answer to every question right; it will also try to follow the same probability distribution over a set of queries.\n\nSuch soft targets carry more information per example than hard labels because they include the teacher model’s confidence levels across all possible outcomes. This is how to the student model is able perform similarly to the teacher but with less computational expense.\n\nAfter the initial knowledge distillation, the student model might be further fine\\-tuned on the task\\-specific dataset with hard labels to maximize its performance.\n\n**Low rank approximations**\n\nLLMs work by processing and generating text based on incredibly large matrices (i.e., veeeeery big tables of numbers) that represent the relationships between words, their meanings, and how they’re used in language. These matrices can be so large that they’re hard to work with, especially when it comes to storage and computation.\n\nA low\\-rank approximation involves finding a simpler matrix that is much smaller in size but still captures the most important information of the original large matrix. It is a bit like reducing a detailed painting to a sketch.\n\nThis is done through mathematical techniques that identify which parts of the matrix (or painting, in our analogy) hold the most information and condense the matrix to just those parts. There are mathematical techniques, notably [singular value decomposition](https://www.cs.cmu.edu/~venkatg/teaching/CStheory-infoage/book-chapter-4.pdf), which help with this.\n\nIn contrast to pruning, low rank approximation performs matrix dimensionality reduction, maintaining the structure of the model but representing it in a more compact form, while pruning directly removes parts of the neural network.\n\n**Quantization**\n\nLLMs process text using a vast number of mathematical calculations. These calculations are performed using numbers that can have a wide range of values. Typically, these numbers are stored in a format that can represent a very wide range of values ([floating\\-point format](https://de.wikipedia.org/wiki/Einfache_Genauigkeit)), occupying 32 bits in memory.\n\nQuantization reduces the precision of those numbers, typically from 32\\-bit floating\\-point numbers to lower bit\\- width representations, such as 8\\-bit integers. This means that instead of using numbers with a lot of decimal places, the model uses “simpler” numbers, making the calculations faster and requiring less memory.\n\nQuantization\\-Aware Training (QAT) involves training the model with quantization in mind, allowing it to adapt to the precision loss and usually resulting in better performance but at the cost of more complex and resource\\-intensive training processes.\n\nPost\\-Training Quantization (PTQ) applies quantization after the model has been fully trained, offering a simpler and faster approach to reduce computational demands. However, it may not achieve the same level of accuracy or performance as QAT due to the model not being specifically optimized for lower precision operations.\n\n### The Era of 1\\-bit LLMs?\n\nMicrosoft researchers recently [made waves with a paper](https://arxiv.org/pdf/2402.17764.pdf) that stores each parameter not in 16 bits, as is currently [the standard](https://en.wikipedia.org/wiki/Half-precision_floating-point_format) in LLMs, but in a mere 1\\.58 bits. This is huge news: With this technique, they achieved almost 10 times more token throughput, i.e., it processes text almost 10 times as fast. They also reduced their memory footprint by a factor of 3\\.5, which means that they need a lot less hardware to run these models on.\n\nThis was achieved using a ternary bit. Instead of using a floating\\-point number between \\-1 and 1, as is usually the case and typically uses 16 bits, every weight is expressed as either \\-1, 0, or 1\\. These numbers can be stored on 1\\.58 bits, because for 3 possible values on a binary transistor one gets that 2¹.58 \\= 3\\. Using only numbers this simple also means that complicated matrix multiplication is no longer necessary, which makes it a lot more resource\\-efficient.\n\nWhat is baffling about this technique is that it achieves a similar output performance as traditional 16\\-bit models at a size of 3 billion parameters. It is not yet clear whether this kind of model scales up as well as traditional models, when passing the threshold of 13 billion or more parameters. What is clear is that even at 70 billion parameters it is more efficient, in terms of latency, memory usage and energy consumption, than a traditional model with only 13 billion parameters. The quality of the output remains to be tested in detail.\n\nOne other disadvantage is that state\\-of\\-the\\-art quantization of existing LLMs cannot be used to produce a 1\\.58\\-bit model. Such models need to be created from scratch, which, despite its dramatically lowered cost, will put it out of reach of the average citizen for now.\n\nIf and when such models have been created and work well, however, inference should become a lot easier. 1\\.58\\-bit LLMs might even be deployed on edge and mobile devices. They are also a lot friendlier to CPU devices — which most mobile devices run on — which makes them easier to deploy on cheaper chips. All this has a lot of advantages, for example for privacy, but allow allows for new applications that humanity hasn’t even dreamt of yet.\n\nMoreover, startups like [Groq](https://groq.com/) have demonstrated promising results and great potential for building specific hardware [like LPUs](https://wow.groq.com/why-groq/) for LLMs. LLM\\-specific hardware is already a [huge market](https://finance.yahoo.com/news/generative-ai-market-size-expected-163500846.html#:~:text=%2D%20Large%20Language%20Model%20(LLM),the%20forecast%20period%202023%2D2029.). Findings like these might make this market grow even more aggressively than analysts have foreseen to date.\n\nIf nothing else, inference will become dirt cheap due to a combination of quantization techniques and specialized hardware. This has implications for many companies, including startups.\n\n## What do lighter LLMs mean for startups?\n\n### The boom in AI hardware has just begun\n\nBetween 1971 and 1999, CPUs were pretty much [the only microprocessors](https://cs.stanford.edu/people/eroberts/courses/soco/projects/2005-06/64-bit-processors/history1.html) on the market. Then [NVIDIA introduced](https://readmedium.com/a-brief-history-of-gpu-47d98d6a0f8a) its GPU. It was not technically the world’s first GPU; however, it was the one of the first microprocessors that made gaming an accessible and immersible experience. (Gaming eats a lot of computing power — if you didn’t know, now you know!)\n\nFrom gaming, GPUs quickly proliferated to do many different tasks, including scientific image processing, linear algebra, 3D reconstruction, and more. One thing that GPUs do particularly well? Machine learning and LLMs. Many of NVIDIA’s chips today are being used for training LLMs.\n\nSince then, other microprocessors have started to proliferate. [Google’s TPUs](https://cloud.google.com/tpu?hl=en), introduced in 2016, are particularly well\\-suited for AI training and inference. While GPUs turned out to be great for LLMs, TPUs were specifically designed for this purpose. They are well\\-suited both for training and inference.\n\nThe industry is [at a turning point](https://www.wsj.com/tech/ai/how-a-shifting-ai-chip-market-will-shape-nvidias-future-f0c256b1), however: Soon, the majority of LLM\\-related work will be inference, and no longer training, as users start deploying models such as LlaMa. New and innovative AI semiconductor companies now have a chance to enter the game.\n\nThis includes chipmaker [Groq](https://wow.groq.com/press/) which focuses on particularly speedy inference processors. Other startups include [Cerebras](https://www.cerebras.net/) (which focuses on training), [Graphcore](https://www.graphcore.ai/about) (which covers training and interference), and [SambaNova](https://sambanova.ai/) (also training and inference). More established competitors like Intel and AMD are also eyeing both training and inference, although most growth is expected to come from the latter in the coming years. The big tech giants — Google, Amazon, or Microsoft — are also developing AI\\-specialized chips, but mostly for in\\-house use.\n\nOverall, the hardware market for LLMs is still dominated by datacenter applications. Edge and mobile applications are the next logical step, but will require more breakthroughs like the 1\\.58\\-bit approach that Microsoft researchers recently published (see above).\n\n## The impact for LLM software companies\n\nLooking at the whole value chain in the emerging AI space, the developments we outlined above are likely to lead to **significantly reduced costs for running/consuming LLMs**.\n\nSome of our thoughts on where this will lead to:\n\n* **Great B2C products**, because lower LLM costs mean that you can build freemium B2C experiences with a high LLM consumption (frequency \\& scale — e.g. long context window) without ruining company unit economics.\n* Democratization of access on a global scale, allowing **users in lower\\-income countries** to utilize advanced AI technologies\n* Companies can automate a wider range of tasks, leading to **increased efficiency and productivity** (“I don’t care anymore if I have 10k API calls per hour”)\n* New edge AI hardware combined with smaller models will lead to **new edge AI use cases** becoming feasible that were “data\\-center”\\-only before\n* As edge hardware explodes, we believe opportunity opens up to build software companies that help customers to bring AI models to the fragmented space of tailored edge devices (“give me your model, I compress it with various techniques, test it on 10 different edge devices, tell you what works best and then help you to deploy it”)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/leveraging-gemini-1-5-api-for-automated-test-case-generation-reverse-engineering-2ee8789f01db","frontmatter":{"title":"Leveraging Gemini 1.5 API for Automated Test Case Generation Reverse Engineering","meta_title":"Leveraging Gemini 1.5 API for Automated Test Case Generation Reverse Engineering","description":"This test explores using Gemini API and Google Apps Script to automatically create sample inputs for faster script reverse engineering.","date":"2024-11-10T03:51:17.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*fTtML3Sm1TuQNhQP.jpg","categories":["Programming","Programming/Scripting","Technology/WebAPI"],"author":"Rifx.Online","tags":["Gemini","API","automation","reverse-engineering","scripts"],"draft":false,"slug":"blog/leveraging-gemini-1-5-api-for-automated-test-case-generation-reverse-engineering-2ee8789f01db"},"content":"\n\n\n\n\n\n\n## Abstract\n\nThis report examines leveraging Gemini 1\\.5 API with Google Apps Script to automate sample input creation during script reverse engineering. Traditionally, this process is manual and time\\-consuming, especially for functions with numerous test cases. Gemini 1\\.5 API’s potential to streamline development by automating input generation is explored through applying reverse engineering techniques to Google Apps Script samples.\n\n\n## Introduction\n\nWith the release of Gemini 1\\.5 API, users gained the ability to process more complex data, opening doors for various application developments. This report explores the potential of using Gemini 1\\.5 API in conjunction with Google Apps Script to achieve reverse engineering for script development and improvement.\n\nTraditionally, script development involves manually crafting sample input values. This process can be time\\-consuming, especially when creating functions or testing code retrieved from online resources like Stack Overflow. Each function might require numerous test cases, and manually generating these inputs can be a bottleneck.\n\nGemini 1\\.5 API offers a potential solution by automating sample input value creation. This could significantly reduce development time and effort. This report investigates this possibility by applying reverse engineering techniques to various Google Apps Script samples using Gemini 1\\.5 API.\n\nHere, we will explore how Gemini 1\\.5 API can be used to automate sample input value generation for reverse engineering scripts written in Google Apps Script.\n\n\n## Usage\n\nIn order to test this script, please do the following flow.\n\n\n## 1\\. Create an API key\n\nPlease access [https://ai.google.dev/gemini\\-api/docs/api\\-key](https://ai.google.dev/gemini-api/docs/api-key) and create your API key. At that time, please enable Generative Language API at the API console. This API key is used for this sample script.\n\nThis official document can be also seen. [Ref](https://ai.google.dev/).\n\n\n## 2\\. Create a Google Apps Script project\n\nIn this report, Google Apps Script is used. Of course, the method introducing this report can be also used in other languages.\n\nHere, in order to test the following sample scripts, please create a standalone Google Apps Script project. Of course, this script can be also used with the container\\-bound script.\n\nAnd, please open the script editor of the Google Apps Script project.\n\n\n## 3\\. Install Google Apps Script library\n\nIn order to easily access Gemini API, I created a Google Apps Script library [GeminiWithFiles](https://github.com/tanaikech/GeminiWithFiles). In the following sample scripts, this library is used. So, please install it. You can see how to install it at [here](https://github.com/tanaikech/GeminiWithFiles?tab=readme-ov-file#1-use-geminiwithfiles-as-a-google-apps-script-library).\n\n\n## 4\\. Sample script 1\n\nThe sample functions were selected from [my repository](https://github.com/tanaikech/UtlApp).\n\n* [transpose](https://github.com/tanaikech/UtlApp?tab=readme-ov-file#transpose): Transpose 2 dimensional array.\n* [removeDuplicatedValues](https://github.com/tanaikech/UtlApp?tab=readme-ov-file#removeduplicatedvalues): Remove duplicated values from 1 dimensional array.\n* [compilingNumbers](https://github.com/tanaikech/UtlApp?tab=readme-ov-file#compilingnumbers): Compiling Continuous Numbers using Google Apps Script.\n* [unpivot](https://github.com/tanaikech/UtlApp?tab=readme-ov-file#unpivot): Converting 2\\-dimensional array as unpivot (reverse pivot).\n* [expandA1Notations](https://github.com/tanaikech/UtlApp?tab=readme-ov-file#expanda1notations): This method is used for expanding A1Notations.\n\nThe sample script demonstrating these functions is provided below. In this example, all functions can be executed in a single API call. When I ran this script, it returned a total of 2,880 tokens.\n\nThe sample first creates input values using Gemini. To test these values, the script then uses them with the function implemented in Google Apps Script. Finally, both the input and output values are printed.\n\nJSON schema is employed here to generate content. This ensures the stable generation of complex JSON objects by Gemini. [Ref](https://readmedium.com/taming-the-wild-output-effective-control-of-gemini-api-response-formats-with-response-mime-type-da273c08be85) As a result, I opted to use it in this instance.\n\n\n```python\nfunction myFunction() {\n\n  const apiKey = \"###\"; // Please set your API key.\n\n  const functionObj = {\n    transpose: function transpose(array) {\n      /**\n       * ### Description\n       * When the inputted array is 2 dimensional array, true is returned.\n       *\n       * @param {Array} array 2 dimensional array.\n       * @return {Boolean} When the inputted array is 2 dimensional array, true is returned.\n       */\n      function is2DimensionalArray(array) {\n        return array.every((r) => Array.isArray(r));\n      }\n\n      /**\n       * ### Description\n       * Transpose 2 dimensional array.\n       *\n       * @param {Array} array 2 dimensional array.\n       * @param {Boolean} check Check whether the inputted array is 2 dimensional array. Default is true.\n       * @return {Array} Transposed array.\n       */\n      function transpose(array, check = true) {\n        if (check && !is2DimensionalArray(array)) {\n          throw new Error(\"Please use 2 dimensional array.\");\n        }\n        return array[0].map((_, col) => array.map((row) => row[col] || null));\n      }\n      return transpose(array);\n    },\n    removeDuplicatedValues: function removeDuplicatedValues(array) {\n      /**\n       * ### Description\n       * Remove duplicated values from 1 dimensional array.\n       *\n       * @param {Array} array 1 dimensional array.\n       * @return {Object} Object including removeDuplicatedValues, duplicatedValues and numberOfDuplicate.\n       */\n      function removeDuplicatedValues(array) {\n        if (!Array.isArray(array)) {\n          throw new Error(\"Please use 1 dimensional array.\");\n        }\n        const obj = array.reduce(\n          (m, e) => m.set(e, m.has(e) ? m.get(e) + 1 : 1),\n          new Map()\n        );\n        const e = [...obj.entries()];\n        return {\n          removeDuplicatedValues: [...obj.keys()],\n          duplicatedValues: e.reduce((ar, [k, v]) => {\n            if (v != 1) ar.push(k);\n            return ar;\n          }, []),\n          numberOfDuplicate: Object.fromEntries(e),\n        };\n      }\n      return removeDuplicatedValues(array);\n    },\n    compilingNumbers: function compilingNumbers(array) {\n      /**\n       * ### Description\n       * Compiling Continuous Numbers using Google Apps Script.\n       *\n       * @param {Array} array Input array.\n       * @return {Array} Array including object like [{\"start\":1,\"end\":1},{\"start\":3,\"end\":5},{\"start\":7,\"end\":7},{\"start\":9,\"end\":11},{\"start\":13,\"end\":13}].\n       */\n      function compilingNumbers(array) {\n        if (!(Array.isArray(array) && array.every((e) => !isNaN(e)))) {\n          throw new Error(\"Please give an array including numbers.\");\n        }\n        const { values } = [...new Set(array.sort((a, b) => a - b))].reduce(\n          (o, e, i, a) => {\n            if (\n              o.temp.length == 0 ||\n              (o.temp.length > 0 && e == o.temp[o.temp.length - 1] + 1)\n            ) {\n              o.temp.push(e);\n            } else {\n              if (o.temp.length > 0) {\n                o.values.push({\n                  start: o.temp[0],\n                  end: o.temp[o.temp.length - 1],\n                });\n              }\n              o.temp = [e];\n            }\n            if (i == a.length - 1) {\n              o.values.push(\n                o.temp.length > 1\n                  ? { start: o.temp[0], end: o.temp[o.temp.length - 1] }\n                  : { start: e, end: e }\n              );\n            }\n            return o;\n          },\n          { temp: [], values: [] }\n        );\n        return values;\n      }\n      return compilingNumbers(array);\n    },\n    unpivot: function unpivot(values) {\n      /**\n       * ### Description\n       * When the inputted array is 2 dimensional array, true is returned.\n       *\n       * @param {Array} array 2 dimensional array.\n       * @return {Boolean} When the inputted array is 2 dimensional array, true is returned.\n       */\n      function is2DimensionalArray(array) {\n        return array.every((r) => Array.isArray(r));\n      }\n\n      /**\n       * ### Description\n       * Converting 2-dimensional array as unpivot (reverse pivot).\n       *\n       * @param {Array} values 2 dimensional array.\n       * @return {Array} 2 dimensional array converted as unpivot (reverse pivot).\n       */\n      function unpivot(values) {\n        if (!Array.isArray(values) || !is2DimensionalArray(values)) {\n          throw new Error(\"Please give an array of values.\");\n        }\n        const [[, ...h], ...v] = values;\n        return h.flatMap((hh, i) => v.map((t) => [hh, t[0], t[i + 1]]));\n      }\n      return unpivot(values);\n    },\n    expandA1Notations: function expandA1Notations(a1Notations) {\n      /**\n       * ### Description\n       * Converting colum letter to column index. Start of column index is 0.\n       * @param {String} letter Column letter.\n       * @return {Number} Column index.\n       */\n      function columnLetterToIndex(letter = null) {\n        if (letter === null || typeof letter != \"string\") {\n          throw new Error(\"Please give the column letter as a string.\");\n        }\n        letter = letter.toUpperCase();\n        return [...letter].reduce(\n          (c, e, i, a) =>\n            (c += (e.charCodeAt(0) - 64) * Math.pow(26, a.length - i - 1)),\n          -1\n        );\n      }\n\n      /**\n       * ### Description\n       * Converting colum index to column letter. Start of column index is 0.\n       * Ref: https://stackoverflow.com/a/53678158/7108653\n       * @param {Number} index Column index.\n       * @return {String} Column letter.\n       */\n      function columnIndexToLetter(index = null) {\n        if (index === null || isNaN(index)) {\n          throw new Error(\n            \"Please give the column indexr as a number. In this case, 1st number is 0.\"\n          );\n        }\n        return (a = Math.floor(index / 26)) >= 0\n          ? columnIndexToLetter(a - 1) + String.fromCharCode(65 + (index % 26))\n          : \"\";\n      }\n\n      /**\n       * ### Description\n       * This method is used for expanding A1Notations.\n       * @param {Array} a1Notations Array including A1Notations.\n       * @return {Array} Array including the expanded A1Notations.\n       */\n      function expandA1Notations(a1Notations, maxRow = \"10\", maxColumn = \"Z\") {\n        if (!Array.isArray(a1Notations) || a1Notations.length == 0) {\n          throw new Error(\"Please give a1Notations (Array).\");\n        }\n        const reg1 = new RegExp(\"^([A-Z]+)([0-9]+)$\");\n        const reg2 = new RegExp(\"^([A-Z]+)$\");\n        const reg3 = new RegExp(\"^([0-9]+)$\");\n        return a1Notations.map((e) => {\n          const a1 = e.split(\"!\");\n          const r = a1.length > 1 ? a1[1] : a1[0];\n          const [r1, r2] = r.split(\":\");\n          if (!r2) return [r1];\n          let rr;\n          if (reg1.test(r1) && reg1.test(r2)) {\n            rr = [r1.toUpperCase().match(reg1), r2.toUpperCase().match(reg1)];\n          } else if (reg2.test(r1) && reg2.test(r2)) {\n            rr = [\n              [null, r1, 1],\n              [null, r2, maxRow],\n            ];\n          } else if (reg1.test(r1) && reg2.test(r2)) {\n            rr = [r1.toUpperCase().match(reg1), [null, r2, maxRow]];\n          } else if (reg2.test(r1) && reg1.test(r2)) {\n            rr = [[null, r1, maxRow], r2.toUpperCase().match(reg1)];\n          } else if (reg3.test(r1) && reg3.test(r2)) {\n            rr =\n              Number(r1) > Number(r2)\n                ? [\n                    [null, \"A\", r2],\n                    [null, maxColumn, r1],\n                  ]\n                : [\n                    [null, \"A\", r1],\n                    [null, maxColumn, r2],\n                  ];\n          } else if (reg1.test(r1) && reg3.test(r2)) {\n            rr = [r1.toUpperCase().match(reg1), [null, maxColumn, r2]];\n          } else if (reg3.test(r1) && reg1.test(r2)) {\n            let temp = r2.toUpperCase().match(reg1);\n            rr =\n              Number(temp[2]) > Number(r1)\n                ? [\n                    [null, temp[1], r1],\n                    [null, maxColumn, temp[2]],\n                  ]\n                : [temp, [null, maxColumn, r1]];\n          } else {\n            throw new Error(\"Wrong a1Notation: \" + r);\n          }\n          const obj = {\n            startRowIndex: Number(rr[0][2]),\n            endRowIndex:\n              rr.length == 1 ? Number(rr[0][2]) + 1 : Number(rr[1][2]) + 1,\n            startColumnIndex: columnLetterToIndex(rr[0][1]),\n            endColumnIndex:\n              rr.length == 1\n                ? columnLetterToIndex(rr[0][1]) + 1\n                : columnLetterToIndex(rr[1][1]) + 1,\n          };\n          let temp = [];\n          for (let i = obj.startRowIndex; i < obj.endRowIndex; i++) {\n            for (let j = obj.startColumnIndex; j < obj.endColumnIndex; j++) {\n              temp.push(columnIndexToLetter(j) + i);\n            }\n          }\n          return temp;\n        });\n      }\n      return expandA1Notations(a1Notations);\n    },\n  };\n\n  const g = GeminiWithFiles.geminiWithFiles({\n    apiKey,\n    response_mime_type: \"application/json\",\n    doCountToken: true,\n  });\n\n  const functions = Object.entries(functionObj)\n    .map(\n      ([k, v]) =>\n        `<FunctionName>${k}</FunctionName><Function>${v.toString()}</Function>`\n    )\n    .join(\"\");\n  const jsonSchema = {\n    title: \"5 input values for giving each function\",\n    description: `Proposal 5 input values for giving each function. ${functions} Don't propose \"empty\", \"null\", \"undefined\" as values.`,\n    type: \"array\",\n    items: {\n      type: \"object\",\n      properties: {\n        functionName: { description: \"Function name\", type: \"string\" },\n        inputValues: {\n          description: `Proposed 5 input values. Don't propose \"empty\", \"null\", \"undefined\" as values.`,\n          type: \"array\",\n          items: {\n            description: \"Proposed input value\",\n            type: \"array|object|string|number\",\n          },\n        },\n      },\n      additionalProperties: false,\n    },\n  };\n  let res = g.generateContent({ jsonSchema });\n  if (typeof res == \"string\") {\n    try {\n      res = JSON.parse(res);\n    } catch ({ stack }) {\n      console.error(stack);\n      return;\n    }\n  }\n  const result = res.reduce((o, { functionName, inputValues }) => {\n    try {\n      o[functionName] = [];\n      inputValues.forEach((input) => {\n        const output = functionObj[functionName](input);\n        o[functionName].push({ input, output });\n      });\n    } catch ({ stack }) {\n      console.log(stack);\n    }\n    return o;\n  }, {});\n  console.log(JSON.stringify(result));\n}\n```\nWhen this script is run, the following result is obtained. You can see that valid input and output values are created.\n\n\n```python\n{\n  \"transpose\": [\n    { \"input\": [[1, 2], [3, 4]], \"output\": [[1, 3], [2, 4]] },\n    { \"input\": [[\"a\", \"b\"], [\"c\", \"d\"]], \"output\": [[\"a\", \"c\"], [\"b\", \"d\"]] },\n    { \"input\": [[\"a1\", \"b1\"], [\"c1\", \"d1\"], [\"e1\", \"f1\"]], \"output\": [[\"a1\", \"c1\", \"e1\"], [\"b1\", \"d1\", \"f1\"]] },\n    { \"input\": [[true, false], [false, true]], \"output\": [[true, null], [null, true]] },\n    { \"input\": [[1, \"a\"], [\"c\", true]], \"output\": [[1, \"c\"], [\"a\", true]] }\n  ],\n\n  \"removeDuplicatedValues\": [\n    { \"input\": [1, 2, 3, 4, 5], \"output\": { \"removeDuplicatedValues\": [1, 2, 3, 4, 5], \"duplicatedValues\": [], \"numberOfDuplicate\": { \"1\": 1, \"2\": 1, \"3\": 1, \"4\": 1, \"5\": 1 } } },\n    { \"input\": [\"a\", \"b\", \"c\", \"d\", \"e\"], \"output\": { \"removeDuplicatedValues\": [\"a\", \"b\", \"c\", \"d\", \"e\"], \"duplicatedValues\": [], \"numberOfDuplicate\": { \"a\": 1, \"b\": 1, \"c\": 1, \"d\": 1, \"e\": 1 } } },\n    { \"input\": [1, 2, 1, 3, 2, 4, 3, 5, 4], \"output\": { \"removeDuplicatedValues\": [1, 2, 3, 4, 5], \"duplicatedValues\": [1, 2, 3, 4], \"numberOfDuplicate\": { \"1\": 2, \"2\": 2, \"3\": 2, \"4\": 2, \"5\": 1 } } },\n    { \"input\": [\"a\", \"b\", \"a\", \"c\", \"b\", \"d\", \"c\", \"e\", \"d\"], \"output\": { \"removeDuplicatedValues\": [\"a\", \"b\", \"c\", \"d\", \"e\"], \"duplicatedValues\": [\"a\", \"b\", \"c\", \"d\"], \"numberOfDuplicate\": { \"a\": 2, \"b\": 2, \"c\": 2, \"d\": 2, \"e\": 1 } } },\n    { \"input\": [1, \"a\", 2, \"b\", 1, \"c\", 2, \"d\", 1, \"e\"], \"output\": { \"removeDuplicatedValues\": [1, \"a\", 2, \"b\", \"c\", \"d\", \"e\"], \"duplicatedValues\": [1, 2], \"numberOfDuplicate\": { \"1\": 3, \"2\": 2, \"a\": 1, \"b\": 1, \"c\": 1, \"d\": 1, \"e\": 1 } } }\n  ],\n\n  \"compilingNumbers\": [\n    { \"input\": [1, 2, 3, 4, 5], \"output\": [{ \"start\": 1, \"end\": 5 }] },\n    { \"input\": [1, 3, 5, 7, 9, 11, 13], \"output\": [{ \"start\": 1, \"end\": 1 }, { \"start\": 3, \"end\": 3 }, { \"start\": 5, \"end\": 5 }, { \"start\": 7, \"end\": 7 }, { \"start\": 9, \"end\": 9 }, { \"start\": 11, \"end\": 11 }, { \"start\": 13, \"end\": 13 }] },\n    { \"input\": [1, 3, 5, 7, 8, 10, 12, 13], \"output\": [{ \"start\": 1, \"end\": 1 }, { \"start\": 3, \"end\": 3 }, { \"start\": 5, \"end\": 5 }, { \"start\": 7, \"end\": 8 }, { \"start\": 10, \"end\": 10 }, { \"start\": 12, \"end\": 13 }] },\n    { \"input\": [1, 2, 4, 5, 7, 8, 10, 11, 13, 14], \"output\": [{ \"start\": 1, \"end\": 2 }, { \"start\": 4, \"end\": 5 }, { \"start\": 7, \"end\": 8 }, { \"start\": 10, \"end\": 11 }, { \"start\": 13, \"end\": 14 }] },\n    { \"input\": [1, 2, 3, 5, 6, 8, 9, 11, 12, 14, 15], \"output\": [{ \"start\": 1, \"end\": 3 }, { \"start\": 5, \"end\": 6 }, { \"start\": 8, \"end\": 9 }, { \"start\": 11, \"end\": 12 }, { \"start\": 14, \"end\": 15 }] }\n  ],\n\n  \"unpivot\": [\n    { \"input\": [[\"name\", \"score1\", \"score2\"], [\"sample1\", 100, 80], [\"sample2\", 90, 70]], \"output\": [[\"score1\", \"sample1\", 100], [\"score1\", \"sample2\", 90], [\"score2\", \"sample1\", 80], [\"score2\", \"sample2\", 70]] },\n    { \"input\": [[\"name\", \"score1\", \"score2\", \"score3\"], [\"sample1\", 100, 80, 70], [\"sample2\", 90, 70, 80]], \"output\": [[\"score1\", \"sample1\", 100], [\"score1\", \"sample2\", 90], [\"score2\", \"sample1\", 80], [\"score2\", \"sample2\", 70], [\"score3\", \"sample1\", 70], [\"score3\", \"sample2\", 80]] },\n    { \"input\": [[\"id\", \"x\", \"y\", \"z\"], [\"a\", 1, 2, 3], [\"b\", 4, 5, 6]], \"output\": [[\"x\", \"a\", 1], [\"x\", \"b\", 4], [\"y\", \"a\", 2], [\"y\", \"b\", 5], [\"z\", \"a\", 3], [\"z\", \"b\", 6]] },\n    { \"input\": [[\"id\", \"x\", \"y\", \"z\", \"xx\", \"yy\", \"zz\"], [\"a\", 1, 2, 3, 10, 20, 30], [\"b\", 4, 5, 6, 40, 50, 60]], \"output\": [[\"x\", \"a\", 1], [\"x\", \"b\", 4], [\"y\", \"a\", 2], [\"y\", \"b\", 5], [\"z\", \"a\", 3], [\"z\", \"b\", 6], [\"xx\", \"a\", 10], [\"xx\", \"b\", 40], [\"yy\", \"a\", 20], [\"yy\", \"b\", 50], [\"zz\", \"a\", 30], [\"zz\", \"b\", 60]] },\n    { \"input\": [[\"Fruit\", \"2021\", \"2022\", \"2023\"], [\"apple\", 100, 120, 150], [\"orange\", 80, 90, 100]], \"output\": [[\"2021\", \"apple\", 100], [\"2021\", \"orange\", 80], [\"2022\", \"apple\", 120], [\"2022\", \"orange\", 90], [\"2023\", \"apple\", 150], [\"2023\", \"orange\", 100]] }\n  ],\n\n  \"expandA1Notations\": [\n    { \"input\": [\"A1:B5\", \"C3:D7\", \"E2:F10\"], \"output\": [[\"A1\", \"B1\", \"A2\", \"B2\", \"A3\", \"B3\", \"A4\", \"B4\", \"A5\", \"B5\"], [\"C3\", \"D3\", \"C4\", \"D4\", \"C5\", \"D5\", \"C6\", \"D6\", \"C7\", \"D7\"], [\"E2\", \"F2\", \"E3\", \"F3\", \"E4\", \"F4\", \"E5\", \"F5\", \"E6\", \"F6\", \"E7\", \"F7\", \"E8\", \"F8\", \"E9\", \"F9\", \"E10\", \"F10\"]] },\n    { \"input\": [\"A:B\", \"C:D\", \"E:F\"], \"output\": [[\"A1\", \"B1\", \"A2\", \"B2\", \"A3\", \"B3\", \"A4\", \"B4\", \"A5\", \"B5\", \"A6\", \"B6\", \"A7\", \"B7\", \"A8\", \"B8\", \"A9\", \"B9\", \"A10\", \"B10\"], [\"C1\", \"D1\", \"C2\", \"D2\", \"C3\", \"D3\", \"C4\", \"D4\", \"C5\", \"D5\", \"C6\", \"D6\", \"C7\", \"D7\", \"C8\", \"D8\", \"C9\", \"D9\", \"C10\", \"D10\"], [\"E1\", \"F1\", \"E2\", \"F2\", \"E3\", \"F3\", \"E4\", \"F4\", \"E5\", \"F5\", \"E6\", \"F6\", \"E7\", \"F7\", \"E8\", \"F8\", \"E9\", \"F9\", \"E10\", \"F10\"]] },\n    { \"input\": [\"A1:C5\"], \"output\": [[\"A1\", \"B1\", \"C1\", \"A2\", \"B2\", \"C2\", \"A3\", \"B3\", \"C3\", \"A4\", \"B4\", \"C4\", \"A5\", \"B5\", \"C5\"]] },\n    { \"input\": [\"A:C\"], \"output\": [[\"A1\", \"B1\", \"C1\", \"A2\", \"B2\", \"C2\", \"A3\", \"B3\", \"C3\", \"A4\", \"B4\", \"C4\", \"A5\", \"B5\", \"C5\", \"A6\", \"B6\", \"C6\", \"A7\", \"B7\", \"C7\", \"A8\", \"B8\", \"C8\", \"A9\", \"B9\", \"C9\", \"A10\", \"B10\", \"C10\"]] },\n    { \"input\": [\"1:5\", \"3:7\", \"2:10\"], \"output\": [[\"A1\", \"B1\", \"C1\", \"D1\", \"E1\", \"F1\", \"G1\", \"H1\", \"I1\", \"J1\", \"K1\", \"L1\", \"M1\", \"N1\", \"O1\", \"P1\", \"Q1\", \"R1\", \"S1\", \"T1\", \"U1\", \"V1\", \"W1\", \"X1\", \"Y1\", \"Z1\", \"A2\", \"B2\", \"C2\", \"D2\", \"E2\", \"F2\", \"G2\", \"H2\", \"I2\", \"J2\", \"K2\", \"L2\", \"M2\", \"N2\", \"O2\", \"P2\", \"Q2\", \"R2\", \"S2\", \"T2\", \"U2\", \"V2\", \"W2\", \"X2\", \"Y2\", \"Z2\", \"A3\", \"B3\", \"C3\", \"D3\", \"E3\", \"F3\", \"G3\", \"H3\", \"I3\", \"J3\", \"K3\", \"L3\", \"M3\", \"N3\", \"O3\", \"P3\", \"Q3\", \"R3\", \"S3\", \"T3\", \"U3\", \"V3\", \"W3\", \"X3\", \"Y3\", \"Z3\", \"A4\", \"B4\", \"C4\", \"D4\", \"E4\", \"F4\", \"G4\", \"H4\", \"I4\", \"J4\", \"K4\", \"L4\", \"M4\", \"N4\", \"O4\", \"P4\", \"Q4\", \"R4\", \"S4\", \"T4\", \"U4\", \"V4\", \"W4\", \"X4\", \"Y4\", \"Z4\", \"A5\", \"B5\", \"C5\", \"D5\", \"E5\", \"F5\", \"G5\", \"H5\", \"I5\", \"J5\", \"K5\", \"L5\", \"M5\", \"N5\", \"O5\", \"P5\", \"Q5\", \"R5\", \"S5\", \"T5\", \"U5\", \"V5\", \"W5\", \"X5\", \"Y5\", \"Z5\"], [\"A3\", \"B3\", \"C3\", \"D3\", \"E3\", \"F3\", \"G3\", \"H3\", \"I3\", \"J3\", \"K3\", \"L3\", \"M3\", \"N3\", \"O3\", \"P3\", \"Q3\", \"R3\", \"S3\", \"T3\", \"U3\", \"V3\", \"W3\", \"X3\", \"Y3\", \"Z3\", \"A4\", \"B4\", \"C4\", \"D4\", \"E4\", \"F4\", \"G4\", \"H4\", \"I4\", \"J4\", \"K4\", \"L4\", \"M4\", \"N4\", \"O4\", \"P4\", \"Q4\", \"R4\", \"S4\", \"T4\", \"U4\", \"V4\", \"W4\", \"X4\", \"Y4\", \"Z4\", \"A5\", \"B5\", \"C5\", \"D5\", \"E5\", \"F5\", \"G5\", \"H5\", \"I5\", \"J5\", \"K5\", \"L5\", \"M5\", \"N5\", \"O5\", \"P5\", \"Q5\", \"R5\", \"S5\", \"T5\", \"U5\", \"V5\", \"W5\", \"X5\", \"Y5\", \"Z5\", \"A6\", \"B6\", \"C6\", \"D6\", \"E6\", \"F6\", \"G6\", \"H6\", \"I6\", \"J6\", \"K6\", \"L6\", \"M6\", \"N6\", \"O6\", \"P6\", \"Q6\", \"R6\", \"S6\", \"T6\", \"U6\", \"V6\", \"W6\", \"X6\", \"Y6\", \"Z6\", \"A7\", \"B7\", \"C7\", \"D7\", \"E7\", \"F7\", \"G7\", \"H7\", \"I7\", \"J7\", \"K7\", \"L7\", \"M7\", \"N7\", \"O7\", \"P7\", \"Q7\", \"R7\", \"S7\", \"T7\", \"U7\", \"V7\", \"W7\", \"X7\", \"Y7\", \"Z7\"], [\"A2\", \"B2\", \"C2\", \"D2\", \"E2\", \"F2\", \"G2\", \"H2\", \"I2\", \"J2\", \"K2\", \"L2\", \"M2\", \"N2\", \"O2\", \"P2\", \"Q2\", \"R2\", \"S2\", \"T2\", \"U2\", \"V2\", \"W2\", \"X2\", \"Y2\", \"Z2\", \"A3\", \"B3\", \"C3\", \"D3\", \"E3\", \"F3\", \"G3\", \"H3\", \"I3\", \"J3\", \"K3\", \"L3\", \"M3\", \"N3\", \"O3\", \"P3\", \"Q3\", \"R3\", \"S3\", \"T3\", \"U3\", \"V3\", \"W3\", \"X3\", \"Y3\", \"Z3\", \"A4\", \"B4\", \"C4\", \"D4\", \"E4\", \"F4\", \"G4\", \"H4\", \"I4\", \"J4\", \"K4\", \"L4\", \"M4\", \"N4\", \"O4\", \"P4\", \"Q4\", \"R4\", \"S4\", \"T4\", \"U4\", \"V4\", \"W4\", \"X4\", \"Y4\", \"Z4\", \"A5\", \"B5\", \"C5\", \"D5\", \"E5\", \"F5\", \"G5\", \"H5\", \"I5\", \"J5\", \"K5\", \"L5\", \"M5\", \"N5\", \"O5\", \"P5\", \"Q5\", \"R5\", \"S5\", \"T5\", \"U5\", \"V5\", \"W5\", \"X5\", \"Y5\", \"Z5\", \"A6\", \"B6\", \"C6\", \"D6\", \"E6\", \"F6\", \"G6\", \"H6\", \"I6\", \"J6\", \"K6\", \"L6\", \"M6\", \"N6\", \"O6\", \"P6\", \"Q6\", \"R6\", \"S6\", \"T6\", \"U6\", \"V6\", \"W6\", \"X6\", \"Y6\", \"Z6\", \"A7\", \"B7\", \"C7\", \"D7\", \"E7\", \"F7\", \"G7\", \"H7\", \"I7\", \"J7\", \"K7\", \"L7\", \"M7\", \"N7\", \"O7\", \"P7\", \"Q7\", \"R7\", \"S7\", \"T7\", \"U7\", \"V7\", \"W7\", \"X7\", \"Y7\", \"Z7\", \"A8\", \"B8\", \"C8\", \"D8\", \"E8\", \"F8\", \"G8\", \"H8\", \"I8\", \"J8\", \"K8\", \"L8\", \"M8\", \"N8\", \"O8\", \"P8\", \"Q8\", \"R8\", \"S8\", \"T8\", \"U8\", \"V8\", \"W8\", \"X8\", \"Y8\", \"Z8\", \"A9\", \"B9\", \"C9\", \"D9\", \"E9\", \"F9\", \"G9\", \"H9\", \"I9\", \"J9\", \"K9\", \"L9\", \"M9\", \"N9\", \"O9\", \"P9\", \"Q9\", \"R9\", \"S9\", \"T9\", \"U9\", \"V9\", \"W9\", \"X9\", \"Y9\", \"Z9\", \"A10\", \"B10\", \"C10\", \"D10\", \"E10\", \"F10\", \"G10\", \"H10\", \"I10\", \"J10\", \"K10\", \"L10\", \"M10\", \"N10\", \"O10\", \"P10\", \"Q10\", \"R10\", \"S10\", \"T10\", \"U10\", \"V10\", \"W10\", \"X10\", \"Y10\", \"Z10\"]] }\n  ]\n}\n```\n\n## 5\\. Sample script 2\n\nEach function of the above sample script uses only one argument. When multiple arguments are used, the script is as follows. The sample function is as follows.\n\n* [splitArray](https://github.com/tanaikech/UtlApp?tab=readme-ov-file#splitarray): Split array every n length.\n\n\n```python\nfunction myFunction() {\n\n  const apiKey = \"###\"; // Please set your API key.\n\n  const functionObj = {\n    splitArray: function splitArray(array, size) {\n      /**\n       * ### Description\n       * Split array every n length.\n       *\n       * @param {Array} array 2 dimensional array.\n       * @param {Boolean} check Check whether the inputted array is 2 dimensional array. Default is true.\n       * @return {Array} Transposed array.\n       */\n      function splitArray(array, size) {\n        if (!array || !size || !Array.isArray(array)) {\n          throw new Error(\"Please give an array and split size.\");\n        }\n        return [...Array(Math.ceil(array.length / size))].map((_) =>\n          array.splice(0, size)\n        );\n      }\n      return splitArray(array, size);\n    },\n  };\n\n  const g = GeminiWithFiles.geminiWithFiles({\n    apiKey,\n    response_mime_type: \"application/json\",\n    doCountToken: true,\n  });\n\n  const functions = Object.entries(functionObj)\n    .map(\n      ([k, v]) =>\n        `<FunctionName>${k}</FunctionName><Function>${v.toString()}</Function>`\n    )\n    .join(\"\");\n  const jsonSchema = {\n    title: \"5 input values for giving each function\",\n    description: `Proposal 5 input values for giving each function. ${functions} Don't propose \"empty\", \"null\", \"undefined\" as values.`,\n    type: \"array\",\n    items: {\n      type: \"object\",\n      properties: {\n        functionName: { description: \"Function name\", type: \"string\" },\n        inputValues: {\n          description: `Proposed 5 input values. Don't propose \"empty\", \"null\", \"undefined\" as values.`,\n          type: \"array\",\n          items: {\n            description: \"Proposed input value\",\n            type: \"array|object|string|number\",\n          },\n        },\n      },\n      additionalProperties: false,\n    },\n  };\n  let res = g.generateContent({ jsonSchema });\n  if (typeof res == \"string\") {\n    try {\n      res = JSON.parse(res);\n    } catch ({ stack }) {\n      console.error(stack);\n      return;\n    }\n  }\n  const result = res.reduce((o, { functionName, inputValues }) => {\n    try {\n      o[functionName] = [];\n      inputValues.forEach((input) => {\n        const temp = JSON.parse(JSON.stringify(input));\n        const output = functionObj[functionName](...temp);\n        o[functionName].push({ input, output });\n      });\n    } catch ({ stack }) {\n      console.log(stack);\n    }\n    return o;\n  }, {});\n  console.log(JSON.stringify(result));\n}\n```\nWhen this script is run, the following result is obtained.\n\n\n```python\n{\n  \"splitArray\": [\n    { \"input\": [[1, 2, 3, 4, 5, 6], 2], \"output\": [[1, 2], [3, 4], [5, 6]] },\n    { \"input\": [[\"a\", \"b\", \"c\", \"d\", \"e\"], 2], \"output\": [[\"a\", \"b\"], [\"c\", \"d\"], [\"e\"]] },\n    { \"input\": [[\"apple\", \"orange\", \"grape\", \"banana\", \"kiwi\"], 3], \"output\": [[\"apple\", \"orange\", \"grape\"], [\"banana\", \"kiwi\"]] },\n    { \"input\": [[true, false, true, false, true], 1], \"output\": [[true], [false], [true], [false], [true]] },\n    { \"input\": [[1.2, 3.14, 2.71, 0.577], 2], \"output\": [[1.2, 3.14], [2.71, 0.577]] }\n  ]\n}\n```\n\n## Summary\n\nFrom the above result, we can confirm the possibility of reverse engineering using Gemini API. This also shows that Gemini API can be used to develop applications.\n\n\n## Note\n\n* If an error occurs, please run the script again. Or, please adjust the description in the JSON schema.\n* I believe that this approach will be able to be also used for other languages except for Google Apps Script.\n* In the current stage, it seems that the class objects depending on Google Apps Script like SpreadsheetApp, DriveApp, and so on cannot be used as the input values.\n* The top abstract image was created by [Gemini](https://gemini.google.com/app).\n\n"},{"lang":"en","group":"blog","slug":"blog/lightrag-simple-and-efficient-rival-to-graphrag-fe49e12e9ece","frontmatter":{"title":"LightRAG — Simple and efficient rival to GraphRAG?","meta_title":"LightRAG — Simple and efficient rival to GraphRAG?","description":"Traditional RAG systems work by indexing raw data. This data is simply chunked and stored in vector DBs. Whenever a query comes from the…","date":"2024-11-13T01:22:29.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*7_2PyaNMVdYDWTCrb_cMCg.png","categories":["Generative AI","Data Science","Technology/Web"],"author":"Rifx.Online","tags":["LightRAG","retrieval","GraphRAG","indexing","dual-level"],"draft":false,"slug":"blog/lightrag-simple-and-efficient-rival-to-graphrag-fe49e12e9ece"},"content":"\n\n\n\n\n\nTraditional RAG systems work by indexing raw data. This data is simply chunked and stored in vector DBs. Whenever a query comes from the user, it queries the stored chunks and *retrieves* relevant chunks. If you wish to learn the fundamentals of RAG I have written a comprehensive intro about it [here](https://proxy.rifx.online/https://readmedium.com/retrieval-augmented-generation-rag-a-quick-and-comprehensive-introduction-6cd5217a4ebb).\n\nAs the retrieval step happens for every single query from the user, it is the most crucial bottleneck to speed up naive RAG systems. Would it not be logical to make the retrieval process super efficient? This is the promise of **LightRAG**.\n\n\n> **If you are a non\\-member, you may read this for free [here](https://proxy.rifx.online/https://www.ai-bites.net/lightrag-simple-and-efficient-rival-to-graphrag/). Why not subscribe there and get these right to your inbox?**\n\n\n## Why not GraphRAG\n\nBefore we look at them, you may ask, “Wait. Do we not have GraphRAG from Microsoft?”. Yes, but GraphRAG seems to have a couple of drawbacks.\n\n* **Incremental knowledge update.** (sec 3\\.1\\) GraphRAG first creates a reference to entities and relationships in the entire private dataset. It then does bottom\\-up clustering that organizes the data hierarchically into semantic clusters. An update to the dataset with new knowledge means that we have to go through the entire process of building the graph! LightRAG on the other hand addresses this by simply appending new knowledge to the existing one. More specifically, it combines new graph nodes and edges with existing ones through a simple union operation.\n* **Computational intensity.** As seen from their study, LightRAG significantly reduces the cost of the retrieval phase. What takes 610,000 tokens for GraphRAG takes less than 100 tokens for LightRAG.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*0TwUDr1BCNr_nSfTPwxenw.png)\n\nSo without further adieu, let's dive into LightRAG.\n\n\n## LightRAG\n\nThe two main selling points of LightRAG are Graph\\-based indexing and dual\\-level retrieval framework. So let's look into each of them.\n\n\n## Graph\\-based Indexing\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*U7sYYNA9teKEVig1dzfi2g.png)\n\nBelow are the steps LightRAG follows to incorporate graph\\-based indexing.\n\n* **Entity and Relationship (ER) extraction.** ER extraction is shown by R(.) in the above figure. This step ensures that simple entities are first extracted from a given document. For example, in the above example, “bees” and “beekeeper” are two entities. And they are related by “observe” relation. As in, a beekeeper observes bees.\n* **Key\\-value (KV) pair generation using LLM.** KV pairs are then generated using a simple LLM. The LLM profiling step gives a small note or explanation of what the entity or relation is all about. For example, the LLM explains who a “beekeeper” is in our chosen example. This step is denoted by the P(.) in the above figure. Note that this LLM is different from the general\\-purpose LLM used in the main RAG pipeline.\n* **Deduplication.** Given that the documents have to do with bees, it is quite possible that the entity “beekeeper” could have been retrieved from several documents or chunks. So, we need a deduplication step that just keeps one and discards the rest with the same meaning. This is shown by the D(.) in the above figure.\n\n\n## Dual\\-level Retrieval\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*t9W1UBbjFa5cnAe-_tqz-Q.png)\n\nA query to a RAG system can be one of two types — specific or abstract. In the same bee example, a specific query could be “How many queen bees can be there in the hive?”. An abstract query could be, “What are the implications of climate change on honey bees?” To address this diversity, LightRAG employs two retrieval types:\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*DuVxwxwl_2-gej_DwGzoeg.png)\n\n* **Low\\-level retrieval.** It simply extracts precise entities and their relationships like bees, observe, and beekeepers.\n* **High\\-level retrieval.** Employing an LLM, LightRAG aggregates information and summarizes multiple sources of information.\n\n\n## Why bother doing all this?\n\nDoing all this exercise and switching to LightRAG improves execution time indeed. During indexing, the LLM needs to be called just once per chunk to extract entities and their relationships.\n\nLikewise, during user query, we only retrieve entities and relationships from chunks using the same LLM we used for indexing. This is a huge saving on the retrieval overhead and hence computation. So, we have a “light” RAG at last!\n\nIntegrating new knowledge into existing graphs seems to be a seamless exercise. Instead of re\\-indexing the whole data whenever we have new information, we can simply append new knowledge to the existing graph.\n\n\n## Evaluation\n\nIn their evaluations, they have compared against Naive RAG, RQ\\-RAG, HyDE, and GraphRAG. To keep the comparison fair, they have used GPT\\-4o\\-mini as the LLM across the board with a fixed chunk size of 1200 for all datasets. The answers were evaluated for comprehensiveness, diversity, and effectiveness in answering the user(a.k.a. *empowerment* in the paper).\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*DNdNHW7NRcOXpvEWjT5BKQ.png)\n\nAs we can see from the underlined results, LightRAG beats all of the state\\-of\\-the\\-art methods currently available.\n\nIn general, they draw the following conclusions:\n\n* Using graph\\-based methods (GraphRAG or LightRAG) improves significantly over the baseline Naive RAG\n* LightRAG produces quite diverse answers powered by the dual\\-level retrieval paradigm\n* LightRAG can deal with complex queries better\n\n\n## Conclusion\n\nThough RAG is a fairly recent technique, we are seeing rapid progress in the area. Techniques like LightRAG which can take RAG pipelines to cheap commodity hardware are the most welcome. While the hardware landscape is ever\\-growing, there is always an increasing need to run LLMs and RAG pipelines in compute\\-constrained hardware in real time.\n\nWould you like to see some hands\\-on study of LightRAG? Please stay tuned…\n\n\n## Shout Out\n\nHope that was useful.\n\n**If you liked this article, why not follow me on [Twitter](https://proxy.rifx.online/https://twitter.com/ai_bites) where I share research updates from top AI labs every single day.**\n\n**Also please subscribe to my [YouTube channel](https://proxy.rifx.online/https://www.youtube.com/c/aibites) where I explain AI concepts and papers visually.**\n\n**Lastly, please clap, and let’s celebrate you reaching the end of this story.**\n\n\n"},{"lang":"en","group":"blog","slug":"blog/llama-3-1-405b-how-to-use-for-free-9aaf3561932d","frontmatter":{"title":"Llama 3.1 405B — How to Use for Free","meta_title":"Llama 3.1 405B — How to Use for Free","description":"No Local Install Needed","date":"2024-10-29T05:09:24.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*db_ND6LyQ5_p5jFJCTo5GQ.jpeg","categories":["Programming","Technology","Generative AI"],"author":"Rifx.Online","tags":["Llama","Meta","HuggingChat","Groq","API"],"draft":false,"slug":"blog/llama-3-1-405b-how-to-use-for-free-9aaf3561932d"},"content":"\n\n\n\n\n### No Local Install Needed\n\n**Llama 3\\.1 405B** is Meta’s most advanced AI model released in July 2024 — **but where can you try it*?***\n\n\n\n**LLama 3\\.1** comes in different versions, including the largest model with 405 billion parameters and smaller versions like the 70B and 8B models.\n\nThe easiest way to try the 70B and 8B models is on [Groq](https://console.groq.com/playground)— where you can try them directly on their playground.\n\nBecause of the overwhelming demand, the most powerful 405B model isn’t usually available though.\n\nThis guide is for anyone including developers who wants to try Llama 3\\.1 405B for free — without needing to download and install it.\n\nIf you don’t have a paid Medium account, you can read for free [here](https://addison-best.medium.com/9aaf3561932d?source=friends_link&sk=5fa532d1caaec229a0b9a445d8749449)\n\nIf you are a developer, and you want to try **LLama 3\\.1 405B for free using an API —** you can skip to the end of the article.\n\n\n## Where Can I Use Llama 3\\.1 405B for Free?\n\nYou can download and install it directly from [Meta](https://llama.meta.com/) — but it is huge and you’ll need 100’s of Gigabytes of space and a powerful computer to try it properly.\n\nBut you can also try now without downloading.\n\nHere are some options where you can try it:\n\n**If you want to learn more AI tips and tricks to help grow your business and earn more money online:**\n\n***👉*** *Sign up for our **[free 5\\-day email course](https://aigrowthguys.com/5-day-free-course-how-to-grow-your-business-like-a-weed)** to grow 🚀 and earn**💲👈***\n\n\n## 1\\. Use Llama 3\\.1 405B on Meta AI\n\nIf you’re in the U.S. and it seems at least Canada (where I am), you can chat with the Llama 3\\.1 405B model through Meta AI. Visit the [Meta AI website](https://www.meta.ai) and log in using your Facebook or Instagram account.\n\nIt might also be available in other countries now, so have a look.\n\nWhen you sign in — hopefully you’ll see an option to try **Llama 3\\.1 405B**.\n\nIf you can, you’ll see a message like in the below screenshot.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*cw1WMKhdZhzUp0L3Kn7Qng.png)\n\nYou can also access it via WhatsApp by linking your Meta account.[**Try it on Meta AI**](https://www.meta.ai)\n\nYou can also try their **Imagine** photo creator and AI image editor**.**\n\nThe cartoon image with the Lama and computer was created using this at the start of the article.\n\n**I prompted**\n\n\n> **Imagine: i want a fun cartoon image for a medium article showing trying to use llama 3\\.1 405B**\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8MeC_M2O7UX7ulPOfUCuHA.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*dIG62eA7YAT3mpLA0etz9Q.png)\n\nIt’s worth trying. I don’t think it is comparable to Flux.1 or Midjourney — but it is easy to use and free\n\n\n## 2\\. Use Llama 3\\.1 405B on HuggingChat\n\nHuggingChat is available to users outside the U.S. and provides access to the Llama 3\\.1 405B model. You can start chatting right away without signing up, making it an easy way to explore the model’s capabilities. Visit the [HuggingChat page](https://huggingface.co) to begin.[Try it on HuggingChat](https://huggingface.co)\n\n\n## 3\\. Use Llama 3\\.1 405B on Groq\n\n**How:** Groq initially hosted the Llama 3\\.1 405B model but now offers the smaller 70B and 8B versions due to high demand. You can explore these models by creating a free account on [Groq’s website](https://groq.com).[Try it on Groq](https://groq.com)\n\n\n## 4\\. Use Llama 3\\.1 405B on Perplexity\n\nPerplexity offers a simple way to interact with Llama 3\\.1, designed for quick and easy access to the model. You can start using it by visiting the Perplexity AI platform. But this is only available in the Pro plan.[Try it on Perplexity](https://www.perplexity.ai)\n\n\n## 5\\. Use Llama 3\\.1 405B on Poe\n\nPoe by Quora is another platform where you can try Llama 3\\.1\\. Poe allows users to explore different AI models, including Llama 3\\.1, through a chat interface. It’s a versatile option if you want to compare Llama 3\\.1 with other AI models in one place. You can try 3\\.1 405B for free — with a limited amount of daily free credits.[Try it on Poe](https://poe.com)\n\n\n## Where Can I Use Llama 3\\.1 405B for Free with API?\n\nIf you are a developer and want to try LLama 3\\.1 405B version completely free — you currently have limited options.\n\nBut I wanted to give you an easy and free option to get you started.\n\nYou can currently try on [together.ai](https://together.ai) for free.\n\nYou get $5 of free credit and an API key to try it.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*w8LOXw-Wm0QTz5YgvZ27ug.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*YpKURkmy--xstoJpZ4fmbw.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*g0FxHkg6gq5OMXXo1Yzr0A.png)\n\nThis was the easiest way I found to test the Llama 3\\.1 405B version quickly\\- and free.\n\nThis is a great option for developers wanting to try using it with an API for free.\n\n\n## Note:\n\nIf you want our team to create custom AI software using LLMs, or a custom AI chatbot for your business, you can [**contact me**](https://aigrowthguys.com/contact/) ✉️ here and I’ll get back to you quickly:\n\n[**AI Growth Guys Contact**](https://aigrowthguys.com/contact/)✉️\n\n👉 Sign up to our [**free 5\\-Day email course**](https://aigrowthguys.com/5-day-free-course-how-to-grow-your-business-like-a-weed/) to grow 🚀 and earn💲in the AI age\n\nYou can also [**sign up for my newsletter**](https://ai-growth-guys.beehiiv.com/subscribe/?via=andrew-best) on how to use AI to earn more money.\n\nCheck out our [**YouTube Channel**](https://www.youtube.com/@aigrowthguys)\n\nFollow us at our website: [**AI Growth Guys**](https://aigrowthguys.com/)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/llama-3-2-the-next-generation-of-lightweight-instruction-tuned-language-models-a-hands-on-9bca07c8af1d","frontmatter":{"title":"Llama 3.2: The Next Generation of Lightweight, Instruction-Tuned Language Models: A Hands-On…","meta_title":"Llama 3.2: The Next Generation of Lightweight, Instruction-Tuned Language Models: A Hands-On…","description":"Discover LLaMA 3.2’s Key Innovations in Pruning, Knowledge Distillation, and Multilingual Performance, Plus a Hands-On Tutorial to Run…","date":"2024-11-10T03:51:17.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BMalqlcJIFe50hidF4FnqQ.png","categories":["Natural Language Processing","Machine Learning","Generative AI"],"author":"Rifx.Online","tags":["LLaMA","tuning","pruning","distillation","multilingual"],"draft":false,"slug":"blog/llama-3-2-the-next-generation-of-lightweight-instruction-tuned-language-models-a-hands-on-9bca07c8af1d"},"content":"\n### Discover LLaMA 3\\.2’s Key Innovations in Pruning, Knowledge Distillation, and Multilingual Performance, Plus a Hands\\-On Tutorial to Run Locally or Through Google Colab\n\n👨🏾‍💻 [GitHub](https://github.com/mdmonsurali) ⭐️ \\| 👔[LinkedIn](https://www.linkedin.com/in/mdmonsurali/) \\|📝 [Medium](https://medium.com/@monsuralirana)\n\n\n\n## Introduction\n\nLanguage models continue to evolve, pushing boundaries in efficiency, speed, and multilingual capabilities. LLaMA 3\\.2 (Lightweight LLaMA) represents the next breakthrough in this trajectory, combining innovations like pruning, knowledge distillation, and synthetic data generation. Building upon Meta’s previous innovations, LLaMA 3\\.2 enhances the performance of smaller models (1B and 3B parameters) without sacrificing speed, accuracy, or privacy. In this blog, we will explore the key technical advancements in LLaMA 3\\.2, discuss its benchmark results, and provide a research\\-based perspective on why these innovations matter. We will conclude with a hands\\-on tutorial to help you get started with deploying LLaMA 3\\.2 using LangChain and Ollama.\n\n## 1\\. The Evolution of LLaMA Models: From 1\\.0 to 3\\.2\n\n### A Brief History of LLaMA Models\n\nThe **Large Language Model Meta AI (LLaMA)** series has evolved significantly since its initial release. Meta’s **LLaMA 1\\.0** aimed to democratize access to LLMs, providing high\\-performance models with fewer parameters than models like GPT\\-3, yet achieving similar levels of accuracy across a range of tasks. LLaMA 2\\.0 introduced instruction\\-tuning and improvements in multilingual performance.\n\n**LLaMA 3\\.2** represents the next leap, focusing on the following core areas:\n\n* **Instruction Tuning and Fine\\-Tuning**: Enhancements in instruction\\-following capabilities allow the model to perform better on downstream tasks.\n* **Efficiency for Edge Devices**: Pruning and distillation techniques enable the deployment of models on devices with limited computational resources, such as smartphones, without losing performance.\n* **Vision and Language Understanding**: The integration of vision\\-language models into LLaMA 3\\.2 allows for the handling of multimodal tasks, such as image\\-based Q\\&A.\n\n## 2\\. Key Innovations in LLaMA 3\\.2\n\n### A. Instruction\\-Tuning and Alignment\n\nInstruction\\-tuning has proven to be a key factor in improving LLMs’ ability to follow natural language instructions. In LLaMA 3\\.2, Meta has used **supervised fine\\-tuning (SFT)**, **rejection sampling (RS)**, and **direct preference optimization (DPO)** techniques. These are applied iteratively to train the models to handle various tasks, such as reasoning, summarization, and tool usage, with greater accuracy.\n\n* **Supervised Fine\\-Tuning (SFT)**: The model is fine\\-tuned on human\\-annotated datasets where it learns to generate preferred outputs.\n* **Direct Preference Optimization (DPO)**: A technique that trains models to directly optimize user preferences, aligning outputs more closely with human expectations.\n\n### B. Efficient Pruning and Knowledge Distillation\n\nLLaMA 3\\.2’s lightweight models, such as the 1B and 3B parameter models, leverage **structured pruning** and **knowledge distillation**. These techniques reduce model size while retaining a significant amount of knowledge from larger models (e.g., LLaMA 3\\.1 8B and 70B):\n\n* **Structured Pruning**: In this approach, parts of the network with lower significance are systematically removed to create smaller models while maintaining accuracy.\n* **Knowledge Distillation**: A large model (teacher) transfers knowledge to a smaller model (student), allowing the smaller model to mimic the performance of the larger one during training.\n\n### C. Extended Context Length\n\nOne of the major updates in LLaMA 3\\.2 is its ability to handle longer context lengths — up to **128K tokens**. This makes it highly efficient for tasks that require processing large chunks of text, such as summarization, long document analysis, and multi\\-turn conversations.\n\n### D. Vision\\-Language Models\n\nMeta’s introduction of **Vision\\-Language Models (VLMs)** in LLaMA 3\\.2 opens new frontiers for multimodal tasks. These models are designed to handle both text and images, making them highly effective for applications such as document Q\\&A, scientific diagram interpretation, and image captioning.\n\n## 3\\. Benchmark Performance: How Does LLaMA 3\\.2 Compare?\n\nLLaMA 3\\.2 has been rigorously evaluated on a wide range of benchmarks, as illustrated by the table you shared. Key highlights include:\n\n* **General Tasks**: The 3B model shows exceptional performance on benchmarks such as **MMLU** (63\\.4\\) and **IFEval** (77\\.4\\), indicating superior instruction\\-following and reasoning capabilities.\n* **Tool Use**: On tasks like **BFCL V2**, LLaMA 3\\.2 (3B) scored 67\\.0, outperforming competitors like **Gemma 2** and **Phi\\-3\\.5\\-mini** in following complex instructions related to tool usage.\n* **Math and Reasoning**: The 3B model demonstrated strong results in math\\-related tasks, scoring **77\\.7** in **GSM8K** (grade\\-school math) and **78\\.6** in the **ARC Challenge**, a benchmark focused on reasoning.\n* **Multilingual Generation**: The 3B model also excelled in the multilingual MGSM benchmark, showcasing its ability to generate coherent text across multiple languages.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*lpjDJ6AaRnljLwAxtAf-Ag.png)\n\nLLaMA 3\\.2’s dominance in these tasks suggests that it offers a robust solution for tasks involving natural language understanding, instruction\\-following, and reasoning in both general and multilingual contexts.\n\n## 4\\. Hands\\-On Tutorial: Running LLaMA 3\\.2 Locally Using LangChain and Ollama\n\nNow that we have explored the technical advancements of LLaMA 3\\.2, let’s get hands\\-on with a step\\-by\\-step guide to setting it up locally using **LangChain** and **Ollama**. We can Install it on the local machine or Google Colab terminal. Just follow below steps:\n\n### Step 1: Install Required Libraries\n\nFirst, install the required libraries in your Python environment. Run the following commands to set up LangChain and Ollama:\n\n```python\n!pip install langchain\n!pip install -U langchain-community\n!pip install langchain_ollama\n```\n\n### Step 2: Install and Load Colab\\-XTerm\n\nColab\\-XTerm is a handy package that enables terminal access within a Colab notebook. This can be useful for running shell commands directly within the notebook environment. To install it, run the following command:\n\n```python\n!pip install colab-xterm\n%load_ext colabxterm\n```\n\n### Step 3: Installing Ollama\n\nYou can then open a terminal session by running:\n\n```python\n%xterm\n```\n\nIn the terminal, run the following command to install Ollama:\n\n```python\ncurl -fsSL https://ollama.com/install.sh | sh\n```\n\n```python\nollama serve\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*itAzyQHMHhin8b7bRLc09w.png)\n\n### Step 4: Pulling the Models\n\nOnce Ollama is installed, you can pull the models you need. Ollama provides several LLMs, including Llama 3\\.2\\. Here’s how to pull them:\n\n```python\nollama pull llama3.2\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*S3R4gByToCZXKEWBh4GWaQ.png)\n\nThe above commands will download and prepare the models for use in your Colab environment.\n\nAlternatively, Pull any LLM model that is available in Ollama. All LLM model lists and details are available:[https://ollama.com/library](https://ollama.com/library)\n\n### Step 5: Integrate LLaMA 3\\.2 with LangChain\n\nLangChain makes it easy to invoke LLaMA 3\\.2 for various NLP tasks. Here’s a simple script to test the model:\n\n```python\nfrom langchain_community.llms import Ollama\n\n## Initialize an instance of the Llama 3.1 model\nllm_llama = Ollama(model=\"llama3.2\")\n\n## Invoke the model to generate a response\nresponse = llm_llama.invoke(\"Tell me a joke\")\nprint(response)\n```\n\nOutput:\n\n```python\nHere's one:\n\nWhat do you call a fake noodle?\n\nAn impasta.\n```\n\n### Step 6: Experiment with Different Tasks\n\nYou can extend this to more complex tasks like summarization, multilingual translation, and reasoning:\n\n```python\n## Summarization\nresponse = llm_llama.invoke(\"Summarize the following text: 'LLaMA 3.2 represents a major step forward in AI development...'\")\nprint(response)\n\n## Multilingual Generation\nresponse = llm_llama.invoke(\"Translate the following into French: 'What are the major improvements in LLaMA 3.2?'\")\nprint(response)\n```\n\nOutput:\n\n```python\nQuantum Mechanics is a complex and fascinating subject, but I'll try to break it down in simple terms.\n\n**The Basics**\n\nImagine you have a coin. Heads or tails, right? In classical physics (the way things work today), the coin is either one or the other - heads or tails. It's like a definite choice.\n\nIn Quantum Mechanics, however, the coin isn't quite so simple. When you flip it, it doesn't just land on heads or tails; it exists in both states at the same time! This idea might sound crazy, but that's basically what happens with tiny particles like atoms and electrons.\n\n**Wave-Particle Duality**\n\nHere's a key concept: tiny particles can behave like both waves and particles. It sounds weird, but think of it like this:\n\n* Imagine a wave in the ocean. The water molecules are moving up and down, creating ripples.\n* Now imagine a single water molecule as a particle (a tiny ball). That's what quantum mechanics says these particles can be!\n\n**Superposition**\n\nAnother mind-bending idea is superposition. It means that tiny particles can exist in multiple states at the same time. Think of it like this:\n\n* Imagine a coin that's both heads AND tails simultaneously!\n* This happens with electrons, which can spin both clockwise and counterclockwise at the same time.\n\n**Entanglement**\n\nQuantum Mechanics also introduces entanglement. When two particles interact, they become \"connected\" in such a way that what happens to one particle instantly affects the other, no matter how far apart they are!\n\n* Imagine two dancers who are perfectly synchronized, even if they're on opposite sides of the stage.\n* This is basically entanglement: two particles can be connected in a similar way.\n\n**The Weird Stuff**\n\nNow we get to some really weird and interesting aspects of quantum mechanics:\n\n* **Uncertainty Principle**: You can't know both the position AND momentum of a particle at the same time!\n* **Quantum Tunneling**: Particles can pass through solid objects, which is weird because they shouldn't be able to fit through.\n* **Schrödinger's Cat**: Imagine a cat in a box with a radioactive atom. If the atom decays, the cat dies. But according to quantum mechanics, the cat is BOTH alive AND dead until someone opens the box and observes it!\n\n**In a Nutshell**\n\nQuantum Mechanics is all about tiny particles behaving in strange and fascinating ways. It's like they're playing by their own rules, which can lead to some pretty wild and unpredictable outcomes.\n\nWhile this explanation is simplified, it should give you an idea of how quantum mechanics works and its weird and wonderful principles!\n```\n\nOutput:\n\n```python\nThe translation of \"What are the major improvements in LLaMA 3.2?\" into French is:\n\n\"Quels sont les améliorations majeures de LLaMA 3.2?\"\n\nHere's a breakdown of the translation:\n\n- \"What\" becomes \"Quels\"\n- \"are\" becomes \"sont\"\n- \"the\" becomes \"les\"\n- \"major improvements\" become \"améliorations majeures\"\n- \"in\" becomes \"sont\"\n- \"LLaMA 3.2\" remains the same, as it's a proper noun.\n\nNote: The phrase \"majeures\" is used to describe significant or substantial improvements.\n```\n\n> **Get GitHub Code:**\n\n## Conclusion\n\nLLaMA 3\\.2 is a versatile and highly capable model that excels across multiple NLP tasks, from multilingual text generation to practical tool usage. Its innovations in pruning and knowledge distillation ensure that it maintains top\\-tier performance, even in lightweight, resource\\-constrained environments. With this hands\\-on tutorial, you can quickly integrate LLaMA 3\\.2 into your local applications or through cloud services like Google Colab.\n\nBy unlocking LLaMA 3\\.2’s capabilities, developers can create cutting\\-edge applications that are not only fast and responsive but also privacy\\-conscious, keeping user data on\\-device. Whether you’re exploring NLP or building real\\-world applications, LLaMA 3\\.2 sets a new benchmark in lightweight, instruction\\-tuned language models.\n\nFeel free to explore other models in the Ollama library and experiment with different tasks. The possibilities are endless!\n\n\n"},{"lang":"en","group":"blog","slug":"blog/longrag-giving-ai-a-bigger-net-to-catch-more-fish-in-the-sea-of-information-7ecdd63f330d","frontmatter":{"title":"LongRAG: Giving AI a Bigger Net to Catch More Fish in the Sea of Information","meta_title":"LongRAG: Giving AI a Bigger Net to Catch More Fish in the Sea of Information","description":"In my previous article, I introduced whether RAG would become obsolete due to long-context LLMs. Today, let’s look at how to apply…","date":"2024-11-08T00:17:39.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Nt5TRh0ooDkgmibMlA1Srg.png","categories":["Generative AI","Natural Language Processing","Data Science"],"author":"Rifx.Online","tags":["long-context","LLMs","RAG","retrieval","generation"],"draft":false,"slug":"blog/longrag-giving-ai-a-bigger-net-to-catch-more-fish-in-the-sea-of-information-7ecdd63f330d"},"content":"\nIn [my previous article](https://readmedium.com/will-long-context-llms-cause-the-extinction-of-rag-de41ca5ddfc6), I introduced whether RAG would become obsolete due to long\\-context LLMs. Today, let’s look at how to apply long\\-context LLMs to RAG scenarios.\n\nIn the realm of Retrieval\\-Augmented Generation (RAG), the traditional approach has always relied on short retrieval units, typically around 100 words, which forces retrievers to sift through vast corpora to extract the necessary information. This design, while functional, places an imbalanced load on the retriever, often leading to suboptimal performance due to the overwhelming volume of units it must process.\n\nThe article introduces a new study titled “[LongRAG: Enhancing Retrieval\\-Augmented Generation with Long\\-context LLMs](https://arxiv.org/pdf/2406.15319v3)”. It seeks to address this imbalance by proposing a novel framework that significantly improves the efficiency of the retriever and the performance of the reader by extending the length of retrieval units to 4,000 tokens.\n\n## Traditional RAG vs. LongRAG\n\n\n\nAs shown in Figure 1, the core innovation of LongRAG lies in its restructuring of the traditional RAG framework. By extending the retrieval unit size to 4K tokens — 30 times longer than the typical unit — LongRAG drastically reduces the number of units from millions to a manageable few hundred thousand.\n\nThis approach not only eases the burden on the retriever but also enhances the semantic completeness of the retrieved information, leading to superior downstream performance.\n\n## LongRAG\n\nThe LongRAG framework is composed of two main components: the **Long Retriever** and the **Long Reader**. An illustrative example of these two components are depicted in Figure 2\\.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*fs37A8QUj-y2rW9_iAqS3Q.png)\n\nThe Long Retriever organizes the retrieval process by grouping related documents into cohesive units that retain semantic integrity. Once the relevant long retrieval units are identified, they are passed on to the Long Reader, which is equipped to handle extensive contexts (around 30K tokens).\n\nHere’s a step\\-by\\-step breakdown of the workflow:\n\n### 1\\. Formulating Long Retrieval Units\n\nThe first step in LongRAG is the creation of long retrieval units.\n\n**In traditional RAG** frameworks, the retrieval units are short, often just a few hundred tokens, which can lead to fragmented information and a heavy burden on the retriever to piece together the relevant context.\n\n**LongRAG addresses this** by grouping related documents into cohesive long retrieval units that are significantly larger — up to 4,000 tokens per unit.\n\nTo form these long units, LongRAG employs a grouping algorithm that organizes documents based on their relationships, such as hyperlinks embedded within Wikipedia articles.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*zPEDmLo7rcdCQ06e.png)\n\nFor instance, documents about a particular topic or entity are grouped together to create a comprehensive retrieval unit (Figure 2\\). This ensures that each unit maintains semantic integrity and provides a richer context for the reader to extract the answer from.\n\n### 2\\. Similarity Search and Ranking\n\nOnce the long retrieval units are formed, the next step is to perform a similarity search to identify which units are most relevant to the query.\n\nThe query is encoded into a vector using an encoder function, E\\_Q, and each retrieval unit is similarly encoded using another encoder function, E\\_C. The similarity between the query `q` and each retrieval unit `g` is calculated using the dot product of their respective vectors.\n\nHowever, given the length of the retrieval units, **directly encoding the entire unit can be computationally expensive and less effective**. **To mitigate this, LongRAG approximates the similarity by breaking down the long unit into smaller chunks** and calculating the maximum similarity score across these chunks.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*U1BsMZuyXqO1oqsl.png)\n\nThis method, akin to [the MaxP design from prior works](https://arxiv.org/pdf/1905.09217), allows LongRAG to efficiently identify the most relevant sections within each long retrieval unit without sacrificing performance.\n\n### 3\\. Aggregating Retrieval Results\n\nAfter the similarity scores are calculated, the top k retrieval units are selected based on their relevance to the query. **These selected units are then concatenated to form a single long context, which typically consists of around 30,000 tokens.** This aggregated context is what will be passed on to the Long Reader.\n\nThe size of k, or the number of retrieval units, is crucial for balancing the workload. If the retrieval units are too short, more units are needed, which can overwhelm the reader. Conversely, if the units are too long, fewer are needed, but they must be highly relevant to avoid including extraneous information.\n\nLongRAG optimizes this balance by using a moderate number of well\\-formed long retrieval units, usually between 4 and 8, depending on the task.\n\n### 4\\. Processing by the Long Reader\n\nThe Long Reader is the component responsible for extracting the final answer from the long context. This step leverages advanced long\\-context language models like GPT\\-4o or Gemini\\-1\\.5\\-Pro, which are capable of handling extensive sequences of text without losing track of the critical information.\n\nFor shorter contexts (less than 1,000 tokens), the Long Reader directly extracts the answer. However, for the longer contexts typical of LongRAG, the process is more nuanced. Initially, the model generates a detailed response that spans a few sentences, ensuring that it captures all relevant information. This initial output is then refined through a second round of processing, where the Long Reader condenses the response into a precise, concise answer.\n\nThis two\\-step approach ensures that the Long Reader can effectively handle the large amount of information provided by the long retrieval units while still delivering accurate and focused answers.\n\n## Evaluation\n\nThe paper presents a thorough evaluation of LongRAG on well\\-known datasets like Natural Questions (NQ) and HotpotQA. The results are compelling, showing an improvement in retrieval performance, with answer recall rates jumping from 52% to 71% on NQ (Figure 4\\), and from 47% to 72% on HotpotQA (Figure 5\\).\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*wLUdp-4OihjAz8Fu.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*vmTsnuIsV6LxJFtj.png)\n\n## Conclusion\n\nThis article explored the innovative LongRAG framework, which is an innovative approach by extending the RAG framework to handle long documents, enabling the model to process and generate answers from extended contexts effectively. It incorporates a multi\\-step retrieval process that dynamically retrieves relevant sections of long texts, ensuring that the most pertinent information is used in the generation phase. This allows LongRAG to excel in tasks that require understanding and synthesizing information from lengthy and complex documents, outperforming traditional RAG models in such scenarios.\n\nHowever, this approach is not without its challenges. The dependency on powerful long\\-context models means that the framework’s performance is tightly coupled with the capabilities of these models. Additionally, the grouping algorithm used for creating long retrieval units may require further refinement to generalize beyond Wikipedia\\-based corpora.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/meet-ministral-3b-and-8b-edge-ai-game-changers-3f7532da8f90","frontmatter":{"title":"Meet Ministral 3B and 8B: Edge AI Game-Changers","meta_title":"Meet Ministral 3B and 8B: Edge AI Game-Changers","description":"Mistral AI’s New Frontier in Edge AI and On-Device Computing","date":"2024-11-01T03:55:06.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*3CmWlEiW7ea8gtqxpI83_w.png","categories":["Technology","Autonomous Systems","Data Science"],"author":"Rifx.Online","tags":["Mistral","edge","computing","translation","robotics"],"draft":false,"slug":"blog/meet-ministral-3b-and-8b-edge-ai-game-changers-3f7532da8f90"},"content":"\n\n\n\n\n### Mistral AI’s New Frontier in Edge AI and On\\-Device Computing\n\nIn the rapidly evolving landscape of AI, edge computing has become increasingly crucial for applications that demand low\\-latency, privacy\\-first, and efficient inference without relying on cloud\\-based infrastructure.\n\nThe launch of [**Ministral**](https://mistral.ai/news/ministraux/)family of models, the latest innovation from **Mistral AI**, represents a groundbreaking step forward in the realm of AI.\n\nTo mark the first anniversary of its groundbreaking **Mistral 7B** model, Mistral AI has unveiled its next generation of language models: **Ministral 3B** and **Ministral 8B**, collectively known as “[**les Ministraux**](https://mistral.ai/news/ministraux/)”. These models aren’t just incremental improvements; they represent a significant leap in what’s possible with edge AI.\n\n\n\n\n## Why These Models Matter?\n\nEdge AI is all about performing complex computations locally, ensuring data privacy and reducing response times. With **Ministral 3B** and **Ministral 8B**, Mistral AI offers models that combine high computational power with memory efficiency, all while running directly on the device. These models are designed to deliver real\\-time insights for applications that can’t afford latency or depend on cloud connectivity.\n\n\n## Key Features:\n\n1. **State\\-of\\-the\\-Art Performance**: Outperforms existing models in different tasks such as knowledge, commonsense, reasoning, native function\\-calling, and efficiency within the sub\\-10B category.\n2. **Large Context Window**: Support for up to 128k context length, enabling more comprehensive understanding and generation.\n3. **Efficient Architecture**: Ministral 8B features a special interleaved sliding\\-window attention pattern for faster and more memory\\-efficient inference.\n4. **Versatility**: Suitable for a wide range of applications, from on\\-device translation to autonomous robotics.\n5. **Privacy\\-First Design**: Built for local inference, these models are perfect for applications that prioritize data privacy, eliminating the need for constant cloud access.\n6. **Scalability**: Whether you need low\\-power consumption for smaller devices with Ministral 3B or greater capabilities with the 8B variant, both models are flexible enough to be adapted to various use cases.\n\n\n> For benchmarking results, refer [here](https://mistral.ai/news/ministraux/)\n\n\n## Breaking Down the Models:\n\n\n### Ministral 3B:\n\n* With just **3 billion parameters**, it provides a balanced approach for resource\\-constrained environments\n* Supports up to **128k context length**, allowing for comprehensive handling of complex queries\n* Ideal for ultra\\-low\\-latency applications\n* Outperforms many other models in its category\n\n\n### Ministral 8B:\n\n* With **8 billion parameters** and **128k context length**, it tends to deliver enhanced computational power for more demanding tasks\n* Features a **sliding\\-window attention** pattern for improved speed and memory efficiency\n* Informed by a wide range of **multilingual** and **code** data, making it suitable for diverse applications\n* Supports **function calling**\n* Balances performance and efficiency for demanding applications\n* Vocabulary size of **131k**, using the **V3\\-Tekken** tokenizer\n* Prompt Template:\n\n\n```python\n<s>[INST]user message[/INST]assistant response</s>[INST]new user message[/INST]\n```\n\n## Use Cases:\n\nThese models deliver compute\\-efficient and low\\-latency performance, making them ideal for the following scenarios:\n\n* **On\\-Device Translation**: Empowering users to communicate seamlessly across languages in real\\-time, even in areas with less internet connectivity.\n* **Internet\\-less Smart Assistants**: Supporting intelligent virtual assistants that function independently of cloud connectivity, enhancing user experience in privacy\\-sensitive environments.\n* **Local Analytics**: Enabling organizations to analyze data in real\\-time while maintaining strict privacy standards, which is essential in sectors such as healthcare and finance.\n* **Autonomous Robotics**: Equipping robots with advanced language capabilities for autonomous decision\\-making and communication, enhancing their operational efficiency in various industries.\n\nIn addition to their standalone capabilities, les Ministraux can work in conjunction with larger models like Mistral Large. This synergy allows them to serve as efficient intermediaries for **function\\-calling in agentic workflows**, handling:\n\n* **Input Parsing**: Quickly interpreting user input to ensure accurate responses.\n* **Task Routing**: Directing requests to the appropriate resources based on user intent.\n* **API Calls**: Executing API functions in real\\-time, ensuring smooth interactions across various contexts.\n\n\n## Code Usage (with vLLM):\n\nThe [Ministral\\-8B\\-Instruct\\-2410](https://huggingface.co/mistralai/Ministral-8B-Instruct-2410) Language Model is an instruct fine\\-tuned model that can be efficiently deployed using vLLM. You can find it [here](https://huggingface.co/mistralai/Ministral-8B-Instruct-2410) on Hugging Face. Here’s how you can get started:\n\n\n### Installation\n\nFirst, ensure you have the latest versions of vLLM and mistral\\_common installed:\n\n\n```python\npip install --upgrade vllm\npip install --upgrade mistral_common\n```\n\n> ***Note****: vLLM version 0\\.6\\.2 or higher is required.*\n\n\n### Offline Usage with vLLM\n\nHere’s an example of how to use Ministral\\-8B in offline mode with vLLM:\n\n\n```python\nfrom vllm import LLM\nfrom vllm.sampling_params import SamplingParams\n\nmodel_name = \"mistralai/Ministral-8B-Instruct-2410\"\nsampling_params = SamplingParams(max_tokens=8192)\n\nllm = LLM(model=model_name, tokenizer_mode=\"mistral\", config_format=\"mistral\", load_format=\"mistral\")\n\nprompt = \"What are the potential implications of artificial intelligence on the job market in the next decade?\"\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": prompt\n    },\n]\n\noutputs = llm.chat(messages, sampling_params=sampling_params)\nprint(outputs[0].outputs[0].text)\n```\n\n### Server Mode Inference with vLLM\n\nIn server inference mode, vLLM runs an HTTP server that concurrently handles client connections and requests via a REST API compatible with the OpenAI protocol. Here’s how to set it up:\n\n* Start the server:\n\n\n```python\nvllm serve mistralai/Ministral-8B-Instruct-2410 --tokenizer_mode mistral --config_format mistral --load_format mistral\n```\n* Make requests to the server:\n\n\n```python\ncurl --location 'http://localhost:8000/v1/chat/completions' \\\n    --header 'Content-Type: application/json' \\\n    --header 'Authorization: Bearer token' \\\n    --data '{\n        \"model\": \"mistralai/Ministral-8B-Instruct-2410\",\n        \"messages\": [\n          {\n            \"role\": \"user\",\n            \"content\": \"What are the potential implications of artificial intelligence on the job market in the next decade?\"\n          }\n        ]\n      }'\n```\n\n> Important Notes on vLLM Usage:\n\n* Currently, vLLM is capped at a 32k context size due to limitations in implementing interleaved attention kernels for paged attention.\n* To leverage the full 128k context size, it’s recommended to use [Mistral Inference](https://github.com/mistralai/mistral-inference).\n* If you need to reduce GPU memory requirements, you can use tensor parallelism by adding `tensor_parallel=2` to the LLM initialization.\n\nBy following these examples, you can easily integrate Ministral\\-8B into your projects using vLLM, whether you’re running offline inference or setting up a server for multiple clients. The model’s efficiency and powerful capabilities, combined with vLLM’s optimized inference, make it an excellent choice for a wide range of AI applications.\n\n\n## Conclusion:\n\nThe release of Ministral marks a significant milestone in the evolution of AI. By bringing GPT\\-level performance to edge devices, Mistral AI is not just pushing technological boundaries — they’re reimagining what’s possible with local, privacy\\-first artificial intelligence.\n\nAs developers, researchers, and businesses begin to explore the capabilities of Ministral, we can expect to see a new wave of AI\\-powered applications that are faster, more private, and more accessible than ever before. The age of edge AI is here, and Ministral is leading the charge.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/meet-qwen2-5-coder-32b-instruct-coder-open-source-better-than-gpt4o-5dc8343f8157","frontmatter":{"title":"Meet Qwen2.5-Coder-32B-Instruct -Coder -open source better than gpt4o.","meta_title":"Meet Qwen2.5-Coder-32B-Instruct -Coder -open source better than gpt4o.","description":"Qwen2.5-Coder-32B-Instruct is an advanced open-source AI coding assistant designed to enhance coding efficiency and accuracy. With 32 billion parameters, it matches or surpasses the capabilities of GPT-4o, offering features like long context handling (up to 128K tokens) and support for over 29 languages. Its strengths include high performance on coding benchmarks and adaptability for various coding tasks, though it requires significant processing power. Overall, Qwen2.5-Coder represents a significant step forward in AI-driven coding solutions.","date":"2024-11-14T03:29:09.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*VENiO-pvY-FzxBLUqodjRQ.jpeg","categories":["Programming","Generative AI","Data Science"],"author":"Rifx.Online","tags":["parameters","coding","benchmarks","languages","efficiency"],"draft":false,"slug":"blog/meet-qwen2-5-coder-32b-instruct-coder-open-source-better-than-gpt4o-5dc8343f8157"},"content":"\n**Meet** Qwen2\\.5\\-Coder\\-32B-Coder, Your New AI Coding Buddy\n\nHave you ever wished that coding was a little easier, faster, and maybe even more fun? So, prepare to meet your new AI coding friend, Qwen2\\.5\\-Coder. Qwen2\\.5\\-Code specifically developed this model as a cutting\\-edge language model to simplify your coding experience. Consider having a knowledgeable assistant who can write code for you and debug, explain complex concepts, and handle several languages. Intrigued? Let’s look into what makes Qwen2\\.5\\-Coder so remarkable.\n\n\n\n🧠 **Powerful Performance: Matching GPT\\-4o’s Coding Skills**\n\n> **Qwen2\\.5\\-Coder**, particularly the 32B\\-Instruct edition, is more than simply a code assistant; it’s a top performer, matching or even beating GPT\\-4o and Sonnet 3\\.5, regarded as one of the most powerful artificial intelligence models. Imagine having that level of coding ability available at your disposal, too.\n\n## Open\\-Source\n\nBut it isn’t all about raw power; this model exhibits outstanding correctness, resulting in syntactically precise and efficient code. And the best part? It is substantially speedier than its predecessors, allowing you to complete tasks quickly.\n\n**Key Features**\n\n* **Model Size**: 32 billion parameters.\n* **Context Length**: Supports up to 128K tokens, allowing for extensive input and output capabilities.\n* **Multilingual Support: This system can handle over 29 languages, including English, Chinese, French, and Spanish.**\n* **Instruction Following: This feature enhances the ability to follow complex instructions and generate structured outputs such as JSON.**\n* **Performance Benchmarks: The team scores highly on various coding benchmarks such as HumanEval and MATH.**\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*zyjKE3ZHtax3uX9GnbKUfA.png)\n\nThere are models suitable for all needs, ranging from small to large.\n\nRegardless of your level of experience or inexperience, Qwen2\\.5\\-Coder provides comprehensive coverage. The Qwen2\\.5\\-Coder comes in various sizes, ranging from 0\\.5B to an impressive 32B. This means you can select the model that best meets your requirements and resources. It’s like having a toolbox complete with different\\-sized wrenches, each ideal for a unique task.\n\n🌎 Mastering multiple languages\n\nCoding in several languages? Not a problem! Qwen2\\.5\\-Coder supports more than 29 languages, including popular ones such as *English, Chinese, French, and Spanish*. This bilingual proficiency makes it a very adaptable tool for developers worldwide. It’s like having a universal code translator that removes linguistic barriers and opens up new possibilities.\n\n👍 Benefits: Increased productivity and improved learning\n\nLet’s discuss the pros.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MAhK8R45yNzB8A7mZZITBg.png)\n\n**Long Context Handling:** The model can handle long inputs of up to 128K tokens. This is especially useful for sophisticated coding tasks that require extensive background. **Multilingual Skills**: Qwen2\\.5\\-Coder\\-32B\\-Instruct supports more than 29 languages, including English, Chinese, French, and Spanish. This makes it a valuable tool for developers working on projects with various language requirements.\n\n👎 **Cons**: resource\\-intensive and risk of over\\-reliance.\n\nOf course, every technology has its drawbacks. Qwen2\\.5\\-Coder demands significant processing power, especially in its larger variants. Maximizing its use requires powerful hardware.\n\n🎉 **The Future of Coding**?\n\nQwen2\\.5\\-Coder marks a massive advancement in AI\\-powered coding. Its precision, speed, adaptability, and open\\-source nature make it an intriguing breakthrough. The real benefit for the open source community would be if the computing power required is lessened and API costs for development are cheaper, too.\n\nOther than that, it is quite promising and would also keep large players behind the Paywall under control.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*aHeNvfOvcpME0qzy6EQexQ.jpeg)\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*PI0ioI2MtxQgtNZ0Tq1jwQ.jpeg)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/metas-llama-4-is-coming-soon-plus-parallels-brings-apple-intelligence-to-windows-c1c2722dcf03","frontmatter":{"title":"Meta’s Llama 4 is Coming Soon Plus: Parallels Brings Apple Intelligence to Windows","meta_title":"Meta’s Llama 4 is Coming Soon Plus: Parallels Brings Apple Intelligence to Windows","description":"No subtitle provided","date":"2024-11-10T03:51:17.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*sYakQyN_2Lupo_By","categories":["Technology","Machine Learning","Generative AI"],"author":"Rifx.Online","tags":["Llama","GPUs","Parallels","Recraft","Midjourney"],"draft":false,"slug":"blog/metas-llama-4-is-coming-soon-plus-parallels-brings-apple-intelligence-to-windows-c1c2722dcf03"},"content":"\n\n\n\n\n### Plus: Parallels Brings Apple Intelligence to Windows\n\n\n\n**Welcome to Get The Gist**, where every weekday, we share an easy\\-to\\-read summary of the latest and greatest developments in AI — news, innovations, and trends — all delivered in under 5 minutes! ⏱\n\n**In today’s edition:**\n\n* Mark Zuckerberg Announces Meta’s Llama 4\n* Parallels Brings Apple Intelligence to Windows\n* Recraft V3 Challenges Midjourney\n* Meta AI Surpasses 500 Million Users\n* And more AI news….\n\n\n## 1\\. Meta’s Llama 4 is Coming Soon with Major AI Advancements\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*E_j8uSNV6s3lg2vm)\n\n**The Gist:** Mark Zuckerberg [**confirmed that**](https://analyticsindiamag.com/ai-news-updates/mark-zuckerberg-confirms-llama-4-release-early-next-year/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon) Meta will launch its Llama 4 model early next year, promising new capabilities in speed, reasoning, and cross\\-modality, thanks to a record\\-setting training setup.\n\n**Key Details:**\n\n* Meta is training Llama 4 on a massive setup with over 100,000 H100 GPUs, one of the largest AI clusters reported, aiming for faster and more capable models than ever.\n* The new Llama 4 will introduce advanced capabilities like expanded memory, support for multiple data types, and seamless third\\-party integrations.\n* AI continues to drive Meta’s growth, as generative tools help over a million advertisers increase conversion rates by 7% and boost user engagement on Facebook and Instagram.\n* Zuckerberg highlighted that AI innovations are creating new business opportunities, emphasizing Meta’s commitment to long\\-term AI\\-powered growth across products and platforms.\n\n\n## 2\\. Parallels Bring Apple Intelligence to Windows\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*36yykSGFUbML6zR4)\n\n**The Gist:** Parallels Desktop [**now enables**](https://www.neowin.net/news/parallels-brings-apple-intelligence-features-to-windows/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon) Apple’s AI\\-driven Writing Tools on Windows virtual machines for Macs, allowing users to enhance text in Windows apps using Apple Intelligence.\n\n**Key Details:**\n\n* Parallels Desktop 20\\.1 now supports Apple Writing Tools on Windows apps within macOS Sequoia 15\\.1, giving users access to text improvements like summarizing, rewriting, and tone adjustments in apps like Word and Notepad.\n* To activate, users with macOS 15\\.1 and compatible Macs (M1 or newer) can update Parallels and use shortcuts to apply these tools in Windows apps.\n* Apple Writing Tools, part of Apple Intelligence, are also rolling out on iPadOS and iOS, but only on devices with advanced processors, such as M1, M2, or A17 Pro chips.\n* This update gives Mac users a seamless way to enhance their writing across both Mac and Windows environments, blending Apple’s AI with Windows usability.\n\n\n## 3\\. Recraft V3 Challenges Midjourney with Designer\\-Centric AI Image Generation\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*lYoMCLyX61RKwMiF)\n\n**The Gist:** Recraft [**has unveiled**](https://www.tomsguide.com/ai/ai-image-video/watch-out-midjourney-recraft-just-announced-new-ai-image-generator-model?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon) Recraft V3, a new AI model for image generation that aims to outperform rivals like Midjourney with powerful design\\-focused features and seamless text integration.\n\n**Key Details:**\n\n* Recraft V3 introduces precise text handling within images, allowing users to add and style text effortlessly, a rare feature among AI models; it currently holds top ranking on Hugging Face’s leaderboard.\n* Designers can now control text placement, brand colors, and unique styles, offering enhanced customization and addressing key needs for creative professionals.\n* With an Infinite Canvas, real\\-time collaboration, and an API for advanced workflows, Recraft V3 supports both individual and team\\-based design projects.\n* Recraft has over 1\\.5 million users who have generated more than 200 million images, with the tool available across web, iOS, and Android platforms.\n\n\n## Quick Gist\n\n* **Zenity** raised $38 million in Series B funding to advance security solutions for enterprises using agentic AI and low\\-code tools, addressing key security concerns in process automation [(Read More)](https://www.darkreading.com/application-security/zenity-raises-38m-series-b-funding-round-to-secure-agentic-ai?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon).\n* **OpenAI** updated its Realtime API with five new expressive voices for speech\\-to\\-speech applications and significant cost reductions through prompt caching, currently in beta ([Read More](https://venturebeat.com/ai/openai-expands-realtime-api-with-new-voices-and-cuts-prices-for-developers/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n* **OpenAI** has rolled out its advanced voice mode for free users in Europe, allowing engaging, human\\-like interactions with ChatGPT ([Read More](https://www.tomsguide.com/ai/openai-advanced-voice-is-now-free-for-10-minutes-a-month-3-tips-for-getting-the-most-out-of-that-time?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n* **OpenAI** is rolling out a new feature for ChatGPT that allows users to search their chat history, with availability for free users planned for next month ([Read More](https://indianexpress.com/article/technology/artificial-intelligence/chatgpt-now-allow-users-to-search-through-their-history-heres-how-to-use-it-9647233/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n* **Meta** is collaborating with the US government to implement its AI model Llama for various public\\-sector projects, including improving access to resources and simplifying financial aid without any financial transactions involved ([Read More](https://www.newsbytesapp.com/news/science/meta-working-to-get-llama-used-in-us-government-sectors/story?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n* **Meta** plans to launch its Llama 4 AI model early next year, training it on an unprecedented cluster of over 100,000 H100 GPUs while advocating for an open\\-source approach despite concerns over potential misuse ([Read More](https://www.newsbytesapp.com/news/science/meta-trains-llama-4-models-on-largest-nvidia-gpu-cluster/story?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n* **OpenAI** has launched Advanced Voice Mode for ChatGPT, allowing users to engage in natural, spoken conversations on desktop apps, with the feature already gaining popularity among subscribers ([Read More](https://www.digitaltrends.com/computing/chatgpt-advanced-voice-mode-macos-windows-desktops/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n* **Waymo** is enhancing its autonomous driving technology by developing a new multimodal large language model, EMMA, to improve its robotaxis’ decision\\-making and adaptability in complex environments ([Read More](https://www.theverge.com/2024/10/30/24283516/waymo-google-gemini-llm-ai-robotaxi?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n* **Gemini** now includes a split\\-screen shortcut for multitasking on large\\-screen Android devices like the Pixel Tablet and Fold, enhancing the user experience ([Read More](https://www.androidauthority.com/gemini-split-screen-shortcut-3495573/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n* **Meta AI** has surpassed half a billion users in just one year since its launch, positioning itself to potentially become the most\\-used AI assistant by the end of 2024 despite facing privacy challenges in the EU ([Read More](https://www.phonearena.com/news/meta-ai-reaches-500-million-users-in-one-year_id164309?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n* **Adobe** has updated Illustrator and Photoshop with AI\\-powered features to streamline creative processes and enhance user flexibility, emphasizing the augmentation of human creativity rather than replacement ([Read More](https://www.gearpatrol.com/tech/six-new-powerful-ai-features-every-adobe-photoshop-illustrator-must-try/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n* **NVIDIA** researchers unveiled HOVER, a 1\\.5 million parameter neural network that enables humanoid robots to perform complex tasks through efficient motor coordination and real\\-time adaptability ([Read More](https://analyticsindiamag.com/ai-news-updates/nvidia-introduces-hover-a-1-5-m-parameter-neural-network-for-humanoid-robotics/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n* **Google** launched a new standalone Weather app for Pixel devices that uses AI to summarize outdoor conditions and offers multiple location tracking ([Read More](https://www.theverge.com/2024/10/30/24283998/google-weather-app-pixel-8-7-6-ai-summaries?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n\nThat’s it for today, see you tomorrow! 👋\n\nIf you enjoyed this update and want to stay informed about the latest developments in AI, consider subscribing to ***Get The Gist*** on Medium for more insights and analyses.\n\n**Want to dive even deeper?** Subscribe to our free daily email newsletter for quick, concise updates straight to your inbox so you never miss an important development. You can sign up by clicking [here](https://getthegist.beehiiv.com/).\n\nJoin us as we explore the world of AI together — one gist at a time! 💡🤖\n\n\n"},{"lang":"en","group":"blog","slug":"blog/microsoft-graphrag-v0-4-0-ec98f1f6ed7a","frontmatter":{"title":"Microsoft GraphRAG v0.4.0","meta_title":"Microsoft GraphRAG v0.4.0","description":"Microsoft recently released version v0.4.0 of the GraphRAG project, featuring several significant updates. The most notable additions are…","date":"2024-11-13T01:22:29.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*89qTckZYLUBF1Jtv","categories":["Programming","Data Science","Machine Learning"],"author":"Rifx.Online","tags":["GraphRAG","Incremental","Indexing","DRIFT","Embedding"],"draft":false,"slug":"blog/microsoft-graphrag-v0-4-0-ec98f1f6ed7a"},"content":"\n\n\n\nMicrosoft recently released version v0\\.4\\.0 of the GraphRAG project, featuring several significant updates. The most notable additions are the Incremental Indexing feature and the DRIFT Graph Reasoning Query Module, which significantly enhance system efficiency and functionality.\n\n\n\nThe core highlights of this update include:\n\n1\\. Incremental Indexing: Significantly improves the efficiency of large\\-scale data processing and achieve faster information updates.\n\n2\\. DRIFT Graph Reasoning Query Module: Introducing advanced graph reasoning techniques to enhance complex query processing capabilities.\n\nIn addition, version 0\\.4\\.0 has optimized the embedding workflow, restructured the processing flow, and improved the overall system performance and operability. Additionally, it has added the DRIFT search CLI and example notebook to assist developers in better understanding the new features. Moreover, the introduction of relationship merging and incremental update configuration options further enhances the flexibility and intelligence level of GraphRAG.\n\nThese updates not only enhance the processing speed of GraphRAG, for instance, in large\\-scale financial data analysis, the incremental indexing feature can reduce data update times from hours to minutes. Simultaneously, they also strengthen its applicability in complex knowledge graph applications, significantly broadening its use cases. Industry experts predict that these improvements will play a crucial role in fields such as financial analysis and medical diagnosis, driving AI applications towards more precise and efficient development.\n\nMore update content: [https://proxy.rifx.online/https://github.com/microsoft/graphrag/releases/tag/v0\\.4\\.0](https://proxy.rifx.online/https://github.com/microsoft/graphrag/releases/tag/v0.4.0)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/mistral-ai-releases-revolutionary-edge-models-ministral-3b-and-8b-superior-performance-and-privacy-5b24f0189493","frontmatter":{"title":"Mistral AI Releases Revolutionary Edge Models Ministral 3B and 8B: Superior Performance and Privacy","meta_title":"Mistral AI Releases Revolutionary Edge Models Ministral 3B and 8B: Superior Performance and Privacy","description":"No subtitle provided","date":"2024-10-31T08:32:15.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*zFNeFlbfEnbjV5M65sH5ig@2x.jpeg","categories":["Technology","Machine Learning","Autonomous Systems"],"author":"Rifx.Online","tags":["edge","models","privacy","tokens","attention"],"draft":false,"slug":"blog/mistral-ai-releases-revolutionary-edge-models-ministral-3b-and-8b-superior-performance-and-privacy-5b24f0189493"},"content":"\n\n\n\nRecently, Mistral AI has launched two new edge models — Ministral 3B and Ministral 8B, which have garnered widespread attention in the tech community. These models not only excel in performance but also offer unique advantages in privacy protection.\n\n\n\n\n## Exceptional Performance, Privacy First\n\nMinistral 3B and 8B are designed specifically for on\\-device computation, capable of processing text information up to 128k in length. Particularly, Ministral 8B employs an innovative sliding window attention mechanism, significantly enhancing computational speed and memory efficiency. Moreover, both models prioritize privacy protection in their design, ensuring data is processed locally to reduce the risk of data breaches.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GMgT6erSorAGUp-pqbXWhA@2x.jpeg)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*zRGh7rw7oVXYd5mOhXoc3g@2x.jpeg)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*IIYgXVtbHvWqn6QLSZ-0Ow@2x.jpeg)\n\n\n## Versatile Applications, Unlimited Potential\n\nThe Ministral series models have a wide range of applications. In the field of smart assistants, they can quickly respond to user commands while ensuring data security; in the field of autonomous robots, their powerful reasoning capabilities support complex decision\\-making and operations.\n\n\n## Cost\\-Effective, Broad Market Prospects\n\nDespite their outstanding performance, Ministral 3B and 8B are highly competitive in price. The 3B is priced at $0\\.04 per million tokens, and the 8B at $0\\.10\\. This pricing strategy provides enterprises and developers with a cost\\-effective option. Currently, both models are available for use.\n\n\n## Promising Future, Leading the New Trend in Edge Computing\n\nThe release of the Ministral series models by Mistral AI demonstrates its deep technical strength in edge computing, laying a solid foundation for future on\\-device AI applications. With technological advancements and deeper application exploration, Ministral models are expected to play a greater role in smart devices and the Internet of Things.\n\nIn summary, the launch of Ministral 3B and 8B is not only a significant milestone for Mistral AI but also a major advancement for the AI industry, bringing new possibilities to on\\-device computation.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*A6SToo3fO3DqnlWX)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/mistral-ai-unveils-ministral-3b-and-8b-models-plus-nvidia-launches-ai-model-that-outperforms-gpt-4-941712f5d22d","frontmatter":{"title":"Mistral AI Unveils Ministral 3B and 8B Models Plus: Nvidia Launches AI Model that Outperforms GPT-4","meta_title":"Mistral AI Unveils Ministral 3B and 8B Models Plus: Nvidia Launches AI Model that Outperforms GPT-4","description":"No subtitle provided","date":"2024-10-31T08:29:07.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*PtPEkgjabwBUu73Y","categories":["Technology","Generative AI","Machine Learning"],"author":"Rifx.Online","tags":["Mistral","edge","Llama","YouTube","DreamTracks"],"draft":false,"slug":"blog/mistral-ai-unveils-ministral-3b-and-8b-models-plus-nvidia-launches-ai-model-that-outperforms-gpt-4-941712f5d22d"},"content":"\n\n\n\n\n### Plus: Nvidia Launches AI Model that Outperforms GPT\\-4\n\n\n\n**Welcome to Get The Gist**, where every weekday we share an easy\\-to\\-read summary of the latest and greatest developments in AI — news, innovations, and trends — all delivered in under 5 minutes! ⏱\n\n**In today’s edition:**\n\n* Mistral AI Unveils Ministral 3B and 8B Models for Edge Computing\n* Nvidia Quietly Launches AI Model that Outperforms GPT\\-4\n* YouTube Rolls Out AI Music Tool “Dream Tracks” to U.S. Creators\n* Google Gemini Can Now Generate Images in Customizable Aspect Ratios\n* And more AI news….\n\n\n## 1\\. Mistral AI Unveils Ministral 3B and 8B Models for Edge Computing\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*qAjoYMHGI1TkNy_A)\n\n**The Gist:** Mistral AI has [**launched two new AI models**](https://analyticsindiamag.com/ai-news-updates/mistral-ai-launches-ministral-3b-and-8b-models-for-edge-computing/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models), Ministral 3B and 8B, designed for efficient on\\-device and edge computing. These models outperform competitors and are tailored for tasks requiring privacy\\-first, local inference.\n\n**Key Details:**\n\n* Models handle large context lengths (up to 128k) for smooth performance in resource\\-limited environments.\n* Ideal for applications like smart assistants, local analytics, and robotics, enhancing task efficiency.\n* Available for commercial use with competitive pricing and research access for the 8B Instruct model.\n* Outperforms AI models like Gemma 2 and Llama 3 in benchmarks.\n\n\n## 2\\. Nvidia Quietly Launches AI Model that Outperforms GPT\\-4\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Mza84SHereM3w5rN)\n\n**The Gist:** Nvidia has [**released a new AI model**](https://venturebeat.com/ai/nvidia-just-dropped-a-new-ai-model-that-crushes-openais-gpt-4-no-big-launch-just-big-results/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models), Llama\\-3\\.1\\-Nemotron\\-70B\\-Instruct, which surpasses industry giants like OpenAI’s GPT\\-4 in performance benchmarks. This launch marks a significant expansion of Nvidia’s AI strategy, shifting from hardware to high\\-performing AI software.\n\n**Key Details:**\n\n* Nvidia’s new model scored higher than GPT\\-4 on key benchmarks, demonstrating superior language understanding and generation.\n* Developed using advanced techniques like Reinforcement Learning from Human Feedback (RLHF), the model excels in handling complex queries.\n* Nvidia offers free access through its platform, allowing businesses to experiment with this powerful AI tool.\n* The model is customizable for business needs but requires careful use in specialized areas like legal reasoning or math.\n\n\n## 3\\. YouTube Rolls Out AI Music Tool “Dream Tracks” to U.S. Creators\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*5nUNrJmdCBBy4JdQ)\n\n**The Gist:** YouTube has launched its [**AI\\-powered music generator**](https://www.mediapost.com/publications/article/400280/youtube-brings-ai-audio-generator-to-us-creators.html?edition=136037&utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models) “Dream Tracks” in the U.S., allowing creators to use text prompts to create custom audio for their short\\-form videos. The tool aims to deepen the connection between artists and fans through music creation.\n\n**Key Details:**\n\n* Powered by Google DeepMind’s Lyria, Dream Tracks generates custom instrumental soundtracks for YouTube Shorts.\n* U.S. creators can now use this tool to create royalty\\-free soundtracks up to 30 seconds long.\n* Users can remix the AI\\-generated audio clips, enhancing creative possibilities.\n* YouTube applies a hidden SynthID watermark to all AI\\-generated tracks to ensure transparency.\n\n\n## Quick Gist\n\n* **Clerk Chat** secured $7 million in funding led by Race Capital to enhance its AI\\-powered business communication platform [(Read More)](https://www.businesswire.com/news/home/20241017292794/en/World%E2%80%99s-First-AI-Telecom-Clerk-Chat-Raises-7.0-Million-in-Seed-Funding?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models).\n* **Anthropic** CEO Dario Amodei published a lengthy blog outlining a utopian vision for the transformative potential of artificial general intelligence, while simultaneously seeking to secure a $40 billion valuation for the company [(Read More)](https://www.theverge.com/2024/10/16/24268209/anthropic-ai-dario-amodei-agi-funding-blog?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models).\n* **Google Cloud** announced the general availability of its upgraded Vertex AI platform and Healthcare Data Engine to enhance AI applications in healthcare [(Read More)](https://www.forbes.com/sites/saibala/2024/10/17/google-cloud-announces-general-availability-of-vertex-ai-for-healthcare/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models).\n* **Amazon** led a $500 million funding round for X\\-energy to roll out 5GW of small nuclear reactors by 2039, while **Google** partnered with Kairos Power to install 500MW of SMRs by 2035, both aiming to meet rising energy demands from data centers with clean power [(Read More)](https://www.theengineer.co.uk/content/news/amazon-and-google-bet-big-on-smrs-to-power-ai?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models).\n* **Google** is launching its Gemini AI models for public sector agencies within Google Distributed Cloud in early 2025, along with funding to upskill the government workforce in responsible AI practices [(Read More)](https://siliconangle.com/2024/10/16/google-looks-spearhead-ai-adoption-public-sector/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models).\n* **Google**’s Gemini AI chatbot is set to introduce a feature allowing users to generate images in customizable aspect ratios, enhancing its image editing capabilities [(Read More)](https://indianexpress.com/article/technology/artificial-intelligence/google-gemini-may-soon-get-new-image-resizing-feature-9623756/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models).\n\nThat’s it for today, see you tomorrow! 👋\n\nIf you enjoyed this update and want to stay informed about the latest developments in AI, consider subscribing to ***Get The Gist*** on Medium for more insights and analyses.\n\n**Want to dive even deeper?** Subscribe to our free daily email newsletter for quick, concise updates straight to your inbox, so you never miss an important development. You can sign up by clicking [here](https://getthegist.beehiiv.com/).\n\nJoin us as we explore the world of AI together — one gist at a time! 💡🤖\n\n\n"},{"lang":"en","group":"blog","slug":"blog/mojo-90-000-times-faster-than-python-finally-open-sourced-777bdd9a1896","frontmatter":{"title":"Mojo, 90,000 Times Faster Than Python, Finally Open Sourced!","meta_title":"Mojo, 90,000 Times Faster Than Python, Finally Open Sourced!","description":"On March 29, 2024, Modular Inc. announced the open sourcing of the core components of Mojo.","date":"2024-11-10T22:36:54.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*jcayumihC6jn5q_0","categories":["Programming","Machine Learning","Data Science"],"author":"Rifx.Online","tags":["Mojo","Python","MLIR","SIMD","open-source"],"draft":false,"slug":"blog/mojo-90-000-times-faster-than-python-finally-open-sourced-777bdd9a1896"},"content":"\nOn March 29, 2024, Modular Inc. announced the open sourcing of the core components of Mojo.\n\nMojo is a programming language designed specifically for writing artificial intelligence software, officially launched in August of last year. It has since amassed over 175,000 developers and 50,000 organizations.\n\nArtificial intelligence models are often written in multiple programming languages. Developers typically use Python to implement the simplest parts of neural networks, as it is easy to learn but relatively slow. The remaining code is often written in C\\+\\+, which is faster but more complex to learn.\n\nModular positions Mojo as a more convenient alternative. It offers an easy\\-to\\-use syntax similar to Python but with the potential for thousands of times faster execution speed. Therefore, developers can write fast AI models without needing to learn complex languages like C\\+\\+.\n\n\n\nLast year, when Mojo was introduced, some developers expressed excitement about its emergence. However, when asked about the open\\-source date, Chris Lattner said on Discord, “If I knew, I’d tell you.” For about a year, many developers have been in a state of observation and questioning:\n\n> “The promotion is great, but if it’s not open source, I won’t spend any time trying it.”\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*rIJiJylh4-mWBiqz)\n\n> “It’s clearly an overhyped programming language, and it’s not open source! Chris Lattner wants to deceive millions of Python developers!”\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*0u5HDKseL0Gy_-8A)\n\n> “I can’t spend time on a language that might or might not be open source, especially considering the current commercial environment of OSS…”\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*wrTO7fbKfBZOpBxF)\n\nNow, Mojo is finally open source! And within a short period, it has already reached 17\\.6k stars and has 2\\.1k forks!\n\n## 01 The First Step of Mojo’s Open Source Journey\n\nModular announced today the open sourcing of the core components of Mojo’s standard library. The standard library constitutes the core part of a programming language, containing basic syntax elements and essential features. Mojo’s standard library includes functionalities for optimizing AI hyperparameters, which determine how neural networks process data.\n\n“The Mojo standard library is still undergoing vigorous development and rapid changes, so we are open sourcing its core modules first. This marks an important starting point for our open source journey, not the end.”\n\nThe company states that open sourcing will enable them to gather feedback from more developers, facilitating the better development of Mojo. Moreover, there are various ways to open source projects: some projects provide source code but do not accept contributions; some offer opaque contribution processes, making it difficult to understand goals and roadmaps; and some, though open source, are not actively maintained. Modular states that they have chosen a more thorough approach to open source: allowing external contributions via GitHub pull requests, encouraging developers to participate in Mojo’s development and improvement, and fostering community growth.\n\nFurthermore, Modular demonstrates sincerity by sharing the complete commit history, starting from the initial commit! Openly revising the history of the open standard library allows developers to track the evolution of the code and better understand its meaning.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*0-FqkfLUTevloPjI)\n\nIn addition, they will release nightly builds of the Mojo compiler, facilitating developers to quickly try out the latest compiler features and undergo continuous integration testing.\n\nAt the end of last year, Modular launched the commercial AI platform MAX, which is a unified set of tools and libraries for building high\\-performance AI applications that can be efficiently deployed across multiple hardware platforms, such as running AI applications in Kubernetes environments. Today, the company revealed that they also plan to open source some components of MAX in the future.\n\nMoreover, it is worth mentioning that they have chosen the Apache 2 LLVM license for open sourcing.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*dgVCSxaCq6onY2uP)\n\nThis is a customized version of the Apache 2 license. Additionally, to facilitate integration with software following the GPL2 license, Modular has made corresponding adjustments. GPL2 is another popular open source license, famously used by projects like the Linux kernel. In the announcement blog post, Modular wrote:\n\n> “The Apache 2 license is a good starting point, but our experience with using licenses in the LLVM project tells us that it has two minor issues. Some are concerned that the Apache 2 license may not mix well with GPL2 code (e.g., the Linux kernel), and the Apache 2 license requires you to acknowledge the use of the code in derivative projects. We hope you can use Mojo without being forced to acknowledge Modular or Mojo. Therefore, we have added LLVM’s specially designed exceptions to address these issues.”\n\n## 02 The Best Language for AI Programming in the Next 50 Years?\n\nLast May, when Mojo was just released, Modular claimed that it was 35,000 times faster than raw Python when running algorithms like Mandelbrot.\n\nIn September last year, Modular once again stated, “Mojo combines the advantages of dynamic and static languages, boosting performance to 68,000 times that of Python.”\n\nIn October last year, when Mojo landed on Mac, Modular raised the performance comparison data again: “90,000 times faster than Python.”\n\nSpeaking of Mojo, Modular’s founder and CEO Chris Lattner said, “You can think of Mojo as a member of the Python family, drawing on all these cool languages, compilers, and other technologies, taking Python a big step forward. We believe it enhances Python’s capabilities, gives Python programmers superpowers, allows those familiar with Python to learn new knowledge, explore, and conquer new fields without switching to C\\+\\+.”\n\nMojo is based on the latest compiler technology in MLIR, which is an evolution of LLVM, hence better performance. As long as programmers have the requisite skills and a willingness to optimize fully, they can make the code run extremely fast. The goal of the Mojo language is to meet the needs of Python developers while providing a range of new code optimization techniques to fully exploit the performance limits of hardware devices.\n\nOn the other hand, the Mojo team highly appreciates Rust and openly states that “Mojo’s design is also greatly inspired by Rust.”\n\nIn terms of performance, Modular has made many comparisons with Python to provide a clear comparison, but people do not have a concept of how much faster it is than Rust. Just last month, they specifically responded to the question of “whether Mojo is faster than Rust.”\n\nIn February of this year, Netflix engineer and Rust advocate @ThePrimeagen released a video: parsing DNA sequences with Mojo at a speed surpassing Rust by 50%. This blog post has sparked a lot of attention and discussion, after all, Rust is positioned as a potential successor to Python and C\\+\\+ as the dominant language in the AI field.\n\n@ThePrimeagen’s outlook for Mojo and Rust in AI programming:\n\n> If Mojo officially enters the fray, then I believe Mojo will undoubtedly emerge victorious. The reason Mojo will win is that it doesn’t require any changes to the paradigms developers are already familiar with. With just a little learning, you can achieve amazing performance. First of all, Mojo compiles quickly, and the user experience is very similar to languages everyone is already familiar with, with performance comparable to Rust. The only question is how to get more people to accept it.\n\nAfter making the comment, Luca Palmieri, a respected Rust contributor and author of “Rust: From Zero to Production,” responded on X:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Hqe7bPWGI36LPGzE)\n\nRust boasts top\\-notch ergonomic design in the realm of systems programming, but it faces two major issues in the field of AI applications:\n\n* Slow compilation speed, while AI emphasizes experimentation and rapid iteration.\n* Most AI researchers with experience in Python are reluctant to invest time in learning a new language from scratch.\n\nMojo aims to make it intuitive and easy for Python developers to grasp. As demonstrated by Mohamed, he learned Mojo and utilized SIMD optimization algorithms in just a few weeks as a hobby project (the initial implementation only took 200 lines of code).\n\nFor those interested in AI development, there is indeed a dilemma of choosing one of the three languages available.\n\nBoth Mojo and Rust allow developers to optimize at a lower level. For Rust, developers can certainly pack everything into Arc, Mutex, or Box to avoid conflicts with the borrow checker, but this may sacrifice some performance. While this performance difference might not have a significant impact on application code, it can quickly add up in libraries or other performance\\-sensitive code. The choice between the two depends on the programmer’s focus on reducing overhead and optimizing performance.\n\nBoth languages can utilize LLVM for optimizing code generation and allow the use of inline assembly (although it’s unlikely anyone would actually do so), so theoretically, both have similar performance potential on traditional hardware.\n\n## 03 Based on the Most Advanced Compiler Technology\n\nRust was initiated in 2006, while Swift emerged in 2010, with both primarily built on LLVM IR. Mojo, on the other hand, debuted in 2022, constructed upon MLIR — a more modern “next\\-generation” compiler stack compared to LLVM IR used by Rust. It’s worth noting that Chris Lattner founded LLVM in December 2000 during his university days, learning a great deal from its evolution over the years. He later joined Google to lead the development of MLIR, aimed at supporting the company’s TPU and other AI accelerator projects. Subsequently, he continued his exploration based on the knowledge gained from LLVM IR.\n\nModular states that Mojo is the first programming language to fully leverage the advanced features of MLIR. It can generate CPU code with higher optimization and also supports GPU and other accelerators, with significantly faster speeds than Rust. This is an advantage currently unachievable by other languages, and a core reason why AI and compiler enthusiasts are enthusiastic about Mojo.\n\nThey particularly emphasize two aspects:\n\nOutstanding SIMD ergonomic design: CPUs process multiple data elements simultaneously through special registers and instructions, known as SIMD (Single Instruction, Multiple Data). However, historically, the experience of writing such code has been ugly and challenging to use from an ergonomic standpoint. Although these special instructions have existed for years, most code has not been optimized for them. Therefore, whoever can solve this complexity and write portable SIMD optimization algorithms can stand out in the market, such as simd\\_json.\n\nMojo’s primitives are designed with SIMD priority from the outset: UInt8 is actually a SIMD\\[DType.uint8, 1], representing a SIMD with 1 element. This representation incurs no performance overhead while allowing programmers to easily use it for SIMD optimization. For example, text can be split into 64\\-byte blocks, represented as SIMD\\[DType.uint8, 64], and then compared with a single newline character to find the index of each newline. As SIMD registers on machines can perform operations on 512\\-bit data simultaneously, this operation can boost the performance of such operations by 64 times!\n\nOr to give a simpler example, suppose you have a SIMDDType.float64, 8\\. By simply multiplying it by Float64(2\\), you can easily improve performance. Compared to individually multiplying each element, this method can improve performance by up to 8 times on most machines.\n\nLLVM (also used by Rust) has automatic vectorization optimization passes, but due to its inability to change the memory layout of SIMD and other important details, its performance never reaches the theoretical level of optimization. However, Mojo was designed with SIMD features in mind from the beginning, so the experience of writing SIMD optimizations is very similar to writing regular code.\n\nEager Destruction: Rust’s design is inspired by C\\+\\+’s RAII (Resource Acquisition Is Initialization), meaning that once objects go out of scope, application developers don’t need to worry about releasing memory — the programming language itself handles it. This is a very good example that avoids the performance pitfalls of garbage collection while ensuring dynamic language ergonomics.\n\nMojo goes further by not waiting for the end of the scope but releasing memory when the object is last used. This is very beneficial for AI scenarios because releasing objects early means releasing GPU tensors early, allowing for larger models to be fitted in equivalent GPU RAM. This is Mojo’s unique advantage, allowing programmers to achieve optimal performance without having to design it themselves. Rust’s borrow checker initially extends the lifetime of everything to the end of its scope, matching the behavior of destructor functions, but this can lead to some confusing consequences for users. Rust later added some non\\-lexical lifetime features to simplify the work of developers. However, with Mojo’s eager destructor mechanism, this simplification effect can be directly achieved, and it remains consistent with how objects are actually destroyed, thus avoiding confusing extreme cases.\n\nAnother overhead in Rust comes from the implementation of Drop. It uses Drop Flags to track whether objects should be deleted at runtime. Rust is able to optimize in certain situations, but Mojo can eliminate all extra overhead through explicit definitions.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*0VcMppg3rDTqfsMY)\n\nRegardless, developers must choose between the ease of use of Mojo and Python, and the high performance of C, C\\+\\+, or Rust. In response, the Mojo team calls out to developers, saying, “If you’re curious and looking towards the future, hoping to master a language that may benefit AI development in the next 50 years, why not give Mojo a chance?”\n\n\n"},{"lang":"en","group":"blog","slug":"blog/multi-agent-hedge-fund-simulation-with-langchain-and-langgraph-64060aabe711","frontmatter":{"title":"Multi-Agent Hedge Fund Simulation with LangChain and LangGraph","meta_title":"Multi-Agent Hedge Fund Simulation with LangChain and LangGraph","description":"This project demonstrates how to use a multi-agent setup to simulate a hedge fund’s analytical process. It showcases a practical way to…","date":"2024-11-10T03:51:17.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*i8wneK22YezD7zOhPKvZfg.png","categories":["Finance","Programming","Data Science"],"author":"Rifx.Online","tags":["multi-agent","LangChain","LangGraph","FinancialDatasets","predictive"],"draft":false,"slug":"blog/multi-agent-hedge-fund-simulation-with-langchain-and-langgraph-64060aabe711"},"content":"\n\n### Multi\\-Agent Hedge Fund Simulation with LangChain and LangGraph\n\n\n\nThis project demonstrates how to use a multi\\-agent setup to simulate a hedge fund’s analytical process. It showcases a practical way to build a system that uses AI agents to gather and analyze financial data, a setup that could be scaled and customized further. Here, I’ll break down the project, which involves a portfolio manager and three analyst agents (fundamental, technical, and sentiment), each assigned specific roles in gathering and processing stock data.\n\nThe goal of this project is not to build a comprehensive trading algorithm but rather to illustrate how various types of data can be organized and analyzed in parallel with specialized agents using LangChain and LangGraph.\n\n\n### Project Structure and Agent Overview\n\nThis agent system includes:\n\n1. **Portfolio Manager** — Delegates tasks to analysts and aggregates their findings.\n2. **Fundamental Analyst** — Fetches and analyzes financial statements, such as income statements.\n3. **Technical Analyst** — Collects stock price data over specified timeframes.\n4. **Sentiment Analyst** — Looks at insider trading and news data, providing sentiment insights.\n\nEach agent is designed to specialize in a specific data retrieval task, allowing for modular and scalable analysis. By using LangChain for agent functionality and LangGraph for managing parallel workflows, we can quickly process multiple data sources. The FinancialDatasets API provides a rich source of data with over 30,000 stock tickers, enabling comprehensive analysis.\n\n\n### Key Libraries and Setup\n\nLangChain and LangGraph enable easy handling of multi\\-agent workflows and branching logic for parallel processing. The setup begins by installing required libraries and securing API keys:\n\n\n```python\n%%capture --no-stderr\n%pip install -U langgraph langchain langchain_openai langchain_experimental langsmith pandas\n```\nEnvironment variables are used to store sensitive data, like API keys:\n\n\n```python\nimport getpass\nimport os\n\ndef _set_if_undefined(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"Please provide your {var}\")\n\n_set_if_undefined(\"OPENAI_API_KEY\")               # https://platform.openai.com\n_set_if_undefined(\"FINANCIAL_DATASETS_API_KEY\")   # https://financialdatasets.ai\n_set_if_undefined(\"TAVILY_API_KEY\")               # https://tavily.com\n```\n\n### Agent Functions: Retrieving Data\n\nEach agent in the system is designed to handle specific types of data relevant to stock analysis.\n\n\n### 1\\. Fundamental Analyst\n\nThe Fundamental Analyst retrieves and examines financial statements, which offer insights into a company’s financial health. Below is the tool for getting income statements, a key financial document:\n\n\n```python\nfrom langchain_core.tools import tool\nfrom typing import Dict, Union\nfrom pydantic import BaseModel, Field\n\nclass GetIncomeStatementsInput(BaseModel):\n    ticker: str = Field(..., description=\"The ticker of the stock.\")\n    period: str = Field(default=\"ttm\", description=\"Valid values are 'ttm', 'quarterly', or 'annual'.\")\n    limit: int = Field(default=10, description=\"Maximum number of income statements to return.\")\n\n@tool(\"get_income_statements\", args_schema=GetIncomeStatementsInput, return_direct=True)\ndef get_income_statements(ticker: str, period: str = \"ttm\", limit: int = 10) -> Union[Dict, str]:\n    api_key = os.environ.get(\"FINANCIAL_DATASETS_API_KEY\")\n    url = f'https://api.financialdatasets.ai/financials/income-statements?ticker={ticker}&period={period}&limit={limit}'\n    try:\n        response = requests.get(url, headers={'X-API-Key': api_key})\n        return response.json()\n    except Exception as e:\n        return {\"ticker\": ticker, \"income_statements\": [], \"error\": str(e)}\n```\nHere, `get_income_statements` retrieves the income statements for a given stock ticker. By specifying the period (e.g., “ttm” for trailing twelve months), the agent can focus on different reporting cycles.\n\n\n### 2\\. Technical Analyst\n\nThe Technical Analyst collects stock price data over defined timeframes. This data can later be used to calculate indicators or recognize patterns. Below is the code to retrieve stock prices:\n\n\n```python\nclass GetPricesInput(BaseModel):\n    ticker: str\n    start_date: str\n    end_date: str\n    interval: str = \"day\"\n    interval_multiplier: int = 1\n    limit: int = 5000\n\n@tool(\"get_stock_prices\", args_schema=GetPricesInput, return_direct=True)\ndef get_stock_prices(ticker: str, start_date: str, end_date: str, interval: str, interval_multiplier: int = 1, limit: int = 5000) -> Union[Dict, str]:\n    api_key = os.environ.get(\"FINANCIAL_DATASETS_API_KEY\")\n    url = (\n        f\"https://api.financialdatasets.ai/prices?ticker={ticker}\"\n        f\"&start_date={start_date}&end_date={end_date}\"\n        f\"&interval={interval}&interval_multiplier={interval_multiplier}\"\n        f\"&limit={limit}\"\n    )\n    try:\n        response = requests.get(url, headers={'X-API-Key': api_key})\n        return response.json()\n    except Exception as e:\n        return {\"ticker\": ticker, \"prices\": [], \"error\": str(e)}\n```\nThis function allows us to specify parameters like date range and interval, giving control over the granularity of the data (e.g., daily or hourly).\n\n\n### 3\\. Sentiment Analyst\n\nThe Sentiment Analyst pulls in data on insider trading and relevant news. Insider trades and public sentiment indicators can offer insights into market perception, which is important for assessing stock volatility and potential price movements.\n\n\n```python\nclass GetInsiderTradesInput(BaseModel):\n    ticker: str\n    limit: int = 10\n\n@tool(\"get_insider_trades\", args_schema=GetInsiderTradesInput, return_direct=True)\ndef get_insider_trades(ticker: str, limit: int = 10) -> Union[Dict, str]:\n    api_key = os.environ.get(\"FINANCIAL_DATASETS_API_KEY\")\n    url = f'https://api.financialdatasets.ai/insider-transactions?ticker={ticker}&limit={limit}'\n    try:\n        response = requests.get(url, headers={'X-API-Key': api_key})\n        return response.json()\n    except Exception as e:\n        return {\"ticker\": ticker, \"insider_transactions\": [], \"error\": str(e)}\n```\nBy capturing insider trades, this tool can track moves made by those with privileged information, which might be early indicators of performance changes.\n\n\n### Portfolio Manager: Coordinating and Summarizing Analysis\n\nThe Portfolio Manager serves as the coordinator, delegating tasks to the analysts and compiling their results into a single report. Below is a sample workflow for the Portfolio Manager that demonstrates how it calls each agent:\n\n\n```python\nfrom langchain_community.tools.tavily_search import TavilySearchResults\n\n## Tools grouped by agent type\nfundamental_tools = [get_income_statements]\ntechnical_tools = [get_stock_prices]\nsentiment_tools = [get_insider_trades, TavilySearchResults(max_results=5)]\n\n## Sample function for running all analyses in parallel\ndef analyze_portfolio(ticker: str):\n    # Delegate tasks to each agent\n    fundamentals = [tool(ticker=ticker) for tool in fundamental_tools]\n    prices = [tool(ticker=ticker, start_date=\"2023-01-01\", end_date=\"2023-12-31\") for tool in technical_tools]\n    sentiment = [tool(ticker=ticker) for tool in sentiment_tools]\n    \n    # Summarize results (simplified)\n    summary = {\n        \"fundamentals\": fundamentals,\n        \"technical\": prices,\n        \"sentiment\": sentiment\n    }\n    return summary\n```\nIn this function:\n\n* Each agent’s functions are called in parallel to gather data for the specified ticker.\n* The manager then compiles the data from each agent into a single summary for easy review.\n\n\n### Conclusion\n\nThis project provides a basic yet flexible setup for analyzing stock data through a team of specialized agents. By splitting tasks among a Portfolio Manager, Fundamental Analyst, Technical Analyst, and Sentiment Analyst, we’re able to gather and organize insights across different financial data types. Using LangChain and LangGraph for modularity and parallel processing makes this approach scalable, while Financial Datasets API supports a broad range of tickers, enabling robust data access.\n\nWhile this system is designed as a project for practice, its structure can serve as a foundation for more complex hedge fund simulations or data analytics tools. Next steps might include enhancing each agent with additional tools or data analysis techniques, such as:\n\n* **Technical Patterns and Indicators:** Integrating more technical analysis tools like moving averages or trend lines.\n* **Sentiment Scoring:** Automating sentiment scoring from news sources or insider trading data.\n* **Predictive Modeling:** Adding ML models that can make buy/sell recommendations based on the combined data.\n\nThis setup is a useful prototype for modular financial data analysis, with plenty of room for future customizations and improvements.\n\nFor those interested in the code behind this toolkit, you can find the complete implementation on GitHub [*here*](https://github.com/shaikhmubin02/ai-hedge-fund).\n\n\n"},{"lang":"en","group":"blog","slug":"blog/multimodal-rag-with-gemini-pro-and-langchain-e4f74170420a","frontmatter":{"title":"Multimodal RAG with Gemini Pro and LangChain","meta_title":"Multimodal RAG with Gemini Pro and LangChain","description":"Introduction","date":"2024-11-08T00:41:44.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*m2C8wrrRvhELuDiYLv4YYQ.png","categories":["Programming","Machine Learning","Computer Vision"],"author":"Rifx.Online","tags":["Gemini","LangChain","RAG","Vertex","sneaker"],"draft":false,"slug":"blog/multimodal-rag-with-gemini-pro-and-langchain-e4f74170420a"},"content":"\n\n\n## Introduction\n\nIn this tutorial, we will explore the integration of [Gemini](https://deepmind.google/technologies/gemini/#introduction) Pro and Gemini Pro Vision with the [LangChain](https://www.langchain.com/langchain) Framework for achieving Multimodal (in this case, Image) Retrieval\\-Augmented Generation (RAG). This short tutorial is suitable for both beginners and seasoned practitioners, this tutorial not only lays the foundation using Google [AI Studio](https://aistudio.google.com/) as the primary environment but also seamlessly transitions to demonstrating how these implementations can be adapted and further enhanced using [Google Cloud’s Vertex AI](https://cloud.google.com/vertex-ai).\n\n## Setting the Environment\n\nFirst thing first, let’s set up our environment to ensure we have all the necessary tools and libraries at our disposal.\n\nFor this we would need Langchain, Langchain Google Gen AI Package, and a Vector Store package for RAG as:\n\n```python\npip install — upgrade langchain langchain-google-genai “langchain[docarray]” faiss-cpu\n```\n\nThen you will also need to provide Google AI Studio API key for the models to interact with:\n\n```python\nif \"GOOGLE_API_KEY\" not in os.environ:\n  os.environ[“GOOGLE_API_KEY”] = getpass.getpass(“Provide your Google API Key”)\n```\n\nFor ease of use I have also written a simple function that shows the image I am working with. This simply downloads the image from the URL provided and shows the preview:\n\n```python\ndef get_image(url, filename):\n  content = requests.get(url).content\n  with open(f'/content/{filename}.png', 'wb') as f:\n  f.write(content)\n  image = Image.open(f\"/content/{filename}.png\")\n  image.show()\n  return image\n```\n\n## A Simple LLM Interaction\n\nLet’s start with a very simple LLM interaction. For it we can simply call the Gemini Pro model from ChatGoogleGenerativeAI and invoke, as:\n\n```python\nllm = ChatGoogleGenerativeAI(model=”gemini-pro”)\nresult = llm.invoke(\"Write a ballad about Gemini Pro in around 3 sentences.\")\nprint(result.content)\n```\n\nAs a result you would get something like this:\n\n> In the realm of stars, Gemini Pro shines, A celestial beacon, defining the lines, Guiding stargazers through cosmic designs.\n\nSimilarly, you can also use it in a Chat Interface approach with System, Human message/conversation format. As:\n\n```python\nmodel = ChatGoogleGenerativeAI(model=”gemini-pro”, convert_system_message_to_human=True)\nprint(model([\n  SystemMessage(content=\"Answer only yes or no.\"),\n  HumanMessage(content=\"Is apple a fruit?\"),\n  ]).content)\n```\n\n## Multimodal LLM\n\nFor this tutorial I am using a very simple usecase, where I am imagining I am a Sneaker enthusiasts and basically like to find if given image of a sneaker, where I can buy that exact model in a local store nearby. For it, I have prepared a dummy Knowledge Base with some Fake information on local stores and made of specs of certain popular Sneaker Brands. Interestingly, this Knowledge base was also generated Using Gemini Pro using [Google Gemini](https://gemini.google.com/) chat interface.\n\nLet’s start with a sample image:\n\n```python\nimage = get_image(<image_url>, “nike3”)\nplt.imshow(image)\nplt.show()\n```\n\nAs a sample, I am considering this image of a [Nike](https://nike.com/) Sneaker.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*dNFF95lOu1SeYHOn1vFnQQ.png)\n\nNow, let’s call Gemini Pro Vision model and ask it to tell us bit about this particular image. For this, you simply need to change the model name to *“gemini\\-pro\\-vision”*.\n\n```python\nllm = ChatGoogleGenerativeAI(model=”gemini-pro-vision”)\nmessage = HumanMessage(\ncontent=[\n  {\n    \"type\": \"text\",\n    \"text\": \"What's in this image? provide full detail as possible.\",\n  }, # You can optionally provide text parts\n  {\"type\": \"image_url\", \"image_url\": image},\n])\nprint(\nllm.invoke([message]).content\n)\n```\n\nAnd you will get output like this:\n\n> This is a product image of a pair of Nike Air Max 95 sneakers in a tan, wheat colorway. The upper is made of mesh and suede, with a leather mudguard. The midsole is made of foam, with a visible air unit in the heel. The outsole is made of rubber, with a waffle pattern for traction.\n\n*Disclaimer: The description provided may not be accurate and reflects the model’s interpretation of the image rather than factual information pertaining to it.*\n\n## RAG using Multimodal\n\nNow, let’s dive into how we can perform RAG using this multimodal approach. First thing first, lets create an information source for this RAG. For this I have written few paragraph information information on few Nike sneakers and some made up locations of local stores based in Nepal.\n\n```python\nstore_information = “Nike Air Max Plus sneakers. They feature a brown upper with a black Nike Swoosh logo on the side and a visible Air Max unit in the heel. The sole is white.\nHere are some more details about the Nike Air Max Plus:\nStyle: TN\nRelease date: January 1, 2017\nStyle code: 852630–300\nOriginal retail price: $150 USD\nThe Air Max Plus, also known as the TN, is a popular Nike running shoe that was first released in 1998. It is known for its unique design, which includes a gradient upper, visible Air Max units, and a wavy outsole. The TN has been a popular shoe among sneakerheads and casual wearers alike for over two decades.\nIt features a brown upper with a black Swoosh logo and a white sole. The shoe is currently available for resale on the StockX marketplace for an average price of around $150 USD.\nNike Air Max Plus Store Location: \"Kings Way, Kathmandu, Nepal\n\n...\n\n\"\n```\n\nThen, let’s create a Langchain chain, that basically fetches information provided image description regarding what Nike model it is and where one can buy it based on above information from our Knowledge base.\n\n```python\nllm_text = ChatGoogleGenerativeAI(model=”gemini-pro”)\ntemplate = \"\"\"\n```\n\n{context}\n\n```\n{information}\nProvide brief information and store location.\n\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\nrag_chain = (\n  {\"context\": retriever, \"information\": RunnablePassthrough()}\n  | prompt\n  | llm_text\n  | StrOutputParser()\n)\n```\n\nHere, the thing to note is *Gemini\\-Pro* and *Gemini\\-Pro\\-Vision* are 2 different models and you will need to call them differently. In above code, we are called the Gemini Pro text model that perform RAG provided the image description that was generated by *gemini\\-pro\\-vision* model.\n\nNow, lets set up a full chain that first generates image description provided the image as an input and then does RAG using above chain.\n\n```python\nllm_vision = ChatGoogleGenerativeAI(model=”gemini-pro-vision”, temperature=0.0)\nfull_chain = (\n  RunnablePassthrough() | llm_vision | StrOutputParser() | rag_chain\n)\n```\n\n## Performing the RAG\n\nNow, lets do some testing on what we just set up. First, lets get another image as sample\n\n```python\nimage = get_image(url_3, “nike3”)\nplt.imshow(image)\nplt.show()\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kPkfo2FKnrUR2tC18VMpjg.png)\n\nThen, lets call our RAG:\n\n```python\nmessage = HumanMessage(\n  content=[\n    {\n      \"type\": \"text\",\n      \"text\": \"Provide information on Brand and model of given sneaker.\",\n    }, # You can optionally provide text parts\n    {\"type\": \"image_url\", \"image_url\": image},\n  ])\n```\n\nNow let’s see what we get:\n\n```python\nresult = full_chain.invoke([message])\ndisplay(Markdown(result))\n```\n\nAs an output, we will get something like this, which is based on our made up information source:\n\n> **Nike Offcourt Slide**Soft, one\\-piece upperPlush foam midsoleDurable rubber outsoleAvailable in a variety of colors\n\n> **Store Location:** Bhaktapur, Nepal\n\n## Using Vertex AI Models\n\nInstead of using Google AI Studio model, you can also use Google cloud’s Vertex AI gemini pro models. For it, you will need basically need to first, install related packages for Vertex AI for your cloud environment and Langchain as:\n\n```python\npip install — upgrade google-cloud-aiplatform langchain-google-vertexai\n```\n\nThen, set up necessary config related to your cloud project using:\n\n```python\ngcloud init\n```\n\nThen, you can use Vertex AI models for your multimodal use cases as:\n\n```python\nfrom langchain_google_vertexai import VertexAI\nfrom langchain_google_vertexai import VertexAIEmbeddings\n\nmodel_vision = VertexAI(model_name=\"gemini-1.0-pro-vision-001\")\nmodel_text = VertexAI(model_name=\"gemini-1.0-pro-001\")\n```\n\n## Conclusion\n\nIn this short tutorial, we explored how Gemini Pro and Gemini Pro vision could be used with LangChain to implement multimodal RAG applications.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/o1-preview-vs-claude-3-5-sonnet-comparing-top-llms-d68734b53c93","frontmatter":{"title":"o1-preview vs. claude-3.5-sonnet: Comparing top LLMs","meta_title":"o1-preview vs. claude-3.5-sonnet: Comparing top LLMs","description":"Discover how OpenAI’s o1-preview compares to Claude 3.5 Sonnet in performance, speed, and capabilities.","date":"2024-10-27T13:58:01.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kTWAcpRdOpsrFIDZjjjr7Q.jpeg","categories":["Programming","Machine Learning","Generative AI"],"author":"Rifx.Online","tags":["o1-preview","Claude","throughput","latency","reasoning"],"draft":false,"slug":"blog/o1-preview-vs-claude-3-5-sonnet-comparing-top-llms-d68734b53c93"},"content":"\n\n\n\nToday (Sep 12, 2024), OpenAI unveiled its latest language model, o1-preview. This advanced model is engineered to dedicate more time to processing before generating responses, enabling it to tackle complex tasks and solve challenging problems in science, coding, and mathematics with enhanced capabilities.\n\nIn this blog post, we’ll thoroughly analyze o1-preview and compare it to Claude 3.5 Sonnet, which was previously considered one of the most advanced models available.\n\n\n\n\n## Comparison Methodology\n\nOur analysis utilizes [Keywords AI’s LLM playground](https://docs.keywordsai.co/features/prompt/model-playground), a platform that supports over 200 language models and offers function-calling capabilities. We’ll explore the following aspects:\n\n* Basic comparison\n* Benchmark comparison\n* Processing speed\n* Evaluation metrics\n* Suggested use cases\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*yc171ikejtBy_o11.jpeg)\n\n\n## Basic Comparison\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*z2FrS_AVig7Y6eU_.jpeg)\n\nNote: o1-preview doesn’t support Streaming, function calling, and system messages.\n\n\n## Benchmark Comparison\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Bx_vAvFc9DAD0cZA.jpeg)\n\nO1-preview outperforms Claude 3.5 Sonnet across all benchmarks. The smallest gap is in MMLU (general knowledge). GPQA Diamond, testing graduate-level reasoning, shows a significant performance difference. The MATH benchmark reveals the largest gap, highlighting o1-preview’s advanced mathematical capabilities. These results indicate o1-preview’s substantial improvements in complex reasoning and problem-solving across various domains.\n\n\n## Speed Comparison\n\nO1-preview takes longer to think and respond than other LLMs. While direct speed comparisons may not be entirely fair, testing o1-preview’s speed is crucial. This information helps developers better understand o1-preview’s capabilities and determine if it’s suitable for their projects. Note: As o1-preview doesn’t support streaming, we disabled streaming for both models. Consequently, time to first token (TTFT) couldn’t be measured.\n\n\n## Latency\n\nOur tests, involving hundreds of requests per model, revealed significant differences. Claude 3.5 Sonnet averages 18.3s/request, whereas o1-preview takes 39.4s/request. O1-preview’s significantly longer latency is due to its extended thinking and reasoning process.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*2PMkgPVuylFxwfIa.jpeg)\n\n\n## Throughput (Tokens per second)\n\nDespite higher latency, o1-preview shows superior throughput. O1-preview generates 92.94 tokens/second, while Claude 3.5 Sonnet produces 74.87 tokens/second. This indicates that o1-preview’s longer generation time is primarily due to its initial processing phase rather than token generation speed.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*wxqpnwZhl9pnbw8y.jpeg)\n\n\n## Performance comparison\n\nWe conducted evaluation tests on the [Keywords AI platform](https://keywordsai.co/). The evaluation comprised three parts:\n\n* **Coding Task**: Both models successfully completed frontend and backend development tasks. O1-preview demonstrated superior performance with longer contexts, identifying and resolving bugs more efficiently in the first attempt. It also exhibited a more thorough code analysis capability.\n* **Logical Reasoning**: O1-preview excels in reasoning tasks. Its thinking process closely mimics human cognition. While Claude 3.5 Sonnet performs well on most problems, o1-preview consistently solves complex reasoning challenges, including International Mathematical Olympiad (IMO) level problems.\n* **Writing Task:** Both models perform exceptionally well on writing tasks. They demonstrate the ability to craft genuine, personalized cold emails, as well as concise and meaningful blog posts.\n\n\n## Model Recommendations\n\no1-preview\n\n* **Best for:** Complex problem-solving in mathematics, coding, and physics. Particularly suited for researchers tackling challenging tasks.\n* **Not suitable for:** AI applications requiring rapid response times or heavily reliant on system prompts. Voice AI applications due to lack of streaming support.\n\nClaude 3.5 Sonnet\n\n* **Best for:** Most AI applications requiring problem-solving capabilities and high-quality content generation.\n* **Not suitable for:** Voice AI applications or projects with strict budget constraints requiring lower operational costs.\n\n\n## How to integrate o1-preview into your AI apps.\n\nTo incorporate o1-preview into your AI applications, simply visit the Keywords AI model page and locate the “View code” button. Click this button to copy the provided code snippet, then paste it directly into your codebase. With this straightforward process, you’ll be ready to harness the power of o1-preview in your projects, enabling you to tackle complex problems and generate high-quality content with ease.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*XyQ9QiI7TN8Uc5Jp.jpeg)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*t8fEYlEs13eM7D28lVbtIw.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*yhu9y5ixNuxeFVe1.png)\n\nThis story is published on [Generative AI](https://generativeai.pub/). Connect with us on [LinkedIn](https://www.linkedin.com/company/generative-ai-publication) and follow [Zeniteq](https://www.zeniteq.com/) to stay in the loop with the latest AI stories.\n\nSubscribe to our [newsletter](https://www.generativeaipub.com/) and [YouTube](https://www.youtube.com/@generativeaipub) channel to stay updated with the latest news and updates on generative AI. Let’s shape the future of AI together!\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*PelNtaNaEVDWgMWr.png)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/openai-01-preview-secrets-99-of-people-dont-know-b0c5e4bb4f76","frontmatter":{"title":"OpenAI 01-Preview — SECRETS 99% of People Don’t Know","meta_title":"OpenAI 01-Preview — SECRETS 99% of People Don’t Know","description":"How to get the most out of 01-preview","date":"2024-11-01T03:58:01.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*wRAXNmhEzkGNagMl5Papxg.jpeg","categories":["Programming","Machine Learning","Technology/Web"],"author":"Rifx.Online","tags":["OpenAI","01-preview","iterative","problem-solving","planning"],"draft":false,"slug":"blog/openai-01-preview-secrets-99-of-people-dont-know-b0c5e4bb4f76"},"content":"\n### How to get the most out of 01\\-preview\n\nI’ve been playing around with 01\\-preview since it came out.\n\nI’m loving it!\n\nI’m even teaching it in my new [**AI Growth Hacking course**](https://aigrowthguys.com/growth-hacking-course-sign-up/).\n\nI’m excited to share some key insights about how to get the most out of this.\n\n\n\nMost people have no clue how 01\\-preview works.\n\nFirst of all, it is not just a “thinking” model.\n\nYou need to understand a little bit about how it works before you can take full advantage of it.\n\nIf you don’t have a paid Medium account you can read for free [**here**](https://readmedium.com/openai-01-preview-secrets-99-of-people-dont-know-b0c5e4bb4f76?sk=12140ffad09d922bc00a8a4aa312a286).\n\n👉 Sign up to our free 5\\-Day email course to grow 🚀 and earn💲in the AI age\n\n## How does OpenAI 01\\-preview work?\n\n01\\-preview is not really a new model.\n\nIt combines other models and a “system prompt” that tells it to iterate several times before a response comes out.\n\nAll other models work by providing the first response the model comes up with.\n\n01\\-preview is designed to plan and experiment before a final answer comes out.\n\nAn example will help.\n\n> Imagine you tell GPT\\-4o to write a coherent paragraph that is exactly 80 words long, and has the word “tomato” as the 4th word, the 19th word, and the 72nd word.\n\nGPT\\-4o (and all other models) will fail at this task because it is too difficult to just spit out the first answer that comes to mind.\n\nThis type of question needs experimentation.\n\nThink if you were given the same task.\n\nYou need to “play around” with this task to try to fit the word “tomato” in those spots in a way that makes sense.\n\nYou can’t just start writing and see what happens.\n\nYou will realize that you need to change some sentences around, and words around, in order to fit the word “tomato” in there.\n\nAlso, when you get near 80 words, you need to plan how to stop at exactly that number. You might wish to go back and delete an unnecessary word from the first sentence for example.\n\nThe reason 01\\-preview can do this type of thing is the way it “thinks”.\n\nIt will first break the problem down and say something like, “Come up with a plan to solve this problem”.\n\nThen it will write an approximate first guess (probably using GPT\\-4o)\n\nThen it will say to itself, “Re\\-read the question and see if you can make any tweaks or adjustments”.\n\nThen it will say, “do a double\\-check to see if your response is perfect. If it is, display it, if it is not, keep tweaking.”\n\nThen it will say, “Repeat this process until your answer is 100% perfect. Always remember to double\\-check your final answer before displaying it”.\n\nFor example, the first sentence of the first response might be this.\n\n“Sandy picked a red tomato from her garden.”\n\nThen 01\\-preview would change it to, “Sandy picked a tomato from her garden”.\n\nThis way, it would successfully move the word tomato from the 5th word to the 4th word.\n\nIt would keep making tweaks by having an internal conversation with itself.\n\n## How to get the most out of 01\\-preview?\n\nNow that you have an idea about how 01\\-preview “thinks”, you can start to understand how to get the most out of it.\n\nYou need to divide your own questions into ones that require “thinking” and ones that don’t.\n\nMany questions don’t require “thinking” from the models.\n\nFor example, if you tell it to write you a funny story about a girl named Sandy who has a tomato garden, then you don’t need to use 01\\-preview.\n\n**Why not?**\n\nBecause there are few constraints.\n\nThere are many ways to do this. It is essentially open\\-ended.\n\nThe story doesn’t need to have a certain length.\n\nThe model can just start writing, throw in a joke or 2, and be done with it.\n\nIt won’t need to go back to the first sentence and count the number of words or anything.\n\nThe point is this:\n\nIf you are asking the model for something specific that would be difficult to do in one try without experimenting, then you should use 01\\-preview.\n\nIf you are asking for something open\\-ended, then use the other models.\n\nYou need to use 01\\-preview sparingly because you are only given a limited number of queries.\n\nIt will always be more limited than other models because it uses far more resources than other models.\n\nThe good news is that 01\\-preview will make far fewer mistakes than other models.\n\nAlso, it will be able to answer questions that previous models failed at.\n\nNow is a better time than ever to learn how to leverage AI to grow your business and earn more money.\n\nI am teaching how to use this in my AI Growth Hacking course.\n\nI’ll also be incorporating this model to make the custom AI Agents and chatbots I build even more accurate.\n\nThis will make AI agent builders like [**Stammer**](https://stammer.ai/?via=andrew) even more powerful.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/openai-confirms-the-arrival-of-gpt-5-poised-to-bring-huge-improvements-to-artificial-intelligence-e3b858e79c2a","frontmatter":{"title":"OpenAI Confirms the Arrival of GPT-5, Poised to Bring Huge Improvements to Artificial Intelligence…","meta_title":"OpenAI Confirms the Arrival of GPT-5, Poised to Bring Huge Improvements to Artificial Intelligence…","description":"A netizen posted a GPT5 countdown post on x, saying that it was a conclusion drawn from clues from various platforms. The comment section…","date":"2024-11-01T03:58:58.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8J_opnaERs-wrq2YRKIxdQ.png","categories":["Natural Language Processing","Generative AI","Technology"],"author":"Rifx.Online","tags":["GPT-5","natural","language","efficiency","personalization"],"draft":false,"slug":"blog/openai-confirms-the-arrival-of-gpt-5-poised-to-bring-huge-improvements-to-artificial-intelligence-e3b858e79c2a"},"content":"\n\n\n\nA netizen posted a GPT5 countdown post on x, saying that it was a conclusion drawn from clues from various platforms. The comment section has reached a climax with all kinds of opinions coming out.\n\n\n\n**Cause 1** : OpenAI website GPT5 leak\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*EBDLAv3rOyCjshGBpVRI7A.png)\n\n**Cause 2** : The article “OpenAI Launches Better GPT5 Chatbot” published by BusinessInsider, a well\\-known American financial business insider website. Since the website is a paid website, you can search for the title if you are interested. Some content is pasted below:\n\nThe generative AI company, led by Sam Altman, is on track to launch GPT\\-5 sometime in the middle of the year, possibly in the summer, according to two people familiar with the company. Some enterprise customers recently received demos of the latest model and its related enhancements to its ChatGPT tool, according to another person familiar with the process. Business Insider has confirmed the identities of these people, who asked to remain anonymous so they could speak freely.\n\nBased on the discussions on X and other platforms, it is very likely that a new version of the model will be launched on June 6, but it is not certain whether it will be GPT 4\\.5 or GPT5\\.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*rhApTugfrMVBB6PhMvK4rg.png)\n\nAll of us waiting for **GPT5**\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*eB6j2S_dPbjQ2-sV2N1fwA.jpeg)\n\n\n## What to Expect from GPT\\-5\n\nWhile details remain scarce, the excitement surrounding GPT\\-5 is driven by expectations of significant improvements in AI capabilities. Here are some potential advancements that have been speculated:\n\n* Enhanced Natural Language Understanding: GPT\\-5 is expected to have an even deeper understanding of context, nuances, and subtleties in human language, making interactions more fluid and natural.\n* Increased Efficiency: With each iteration, OpenAI has made strides in reducing latency and improving the efficiency of its models. GPT\\-5 is anticipated to continue this trend, providing faster and more accurate responses.\n* Broader Knowledge Base: By incorporating more diverse and extensive datasets, GPT\\-5 could offer more comprehensive and reliable information across a wider range of topics.\n* Advanced Personalization: The new model might include enhanced personalization features, allowing it to better adapt to individual user preferences and needs.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*7xCG5iy53LLQCTnmzxs_3g.jpeg)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/openai-gpt-5-ph-d-level-intelligence-expected-by-2025-50a86c3aad86","frontmatter":{"title":"OpenAI GPT-5: Ph.D.-Level Intelligence Expected by 2025","meta_title":"OpenAI GPT-5: Ph.D.-Level Intelligence Expected by 2025","description":"After months of speculation, OpenAI has finally unveiled details about the highly anticipated GPT-5. Initially expected in 2024, its…","date":"2024-11-01T03:59:56.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*OasnWeS5mgAX_0hIpirO5Q.jpeg","categories":["Machine Learning","Ethics","Data Science"],"author":"Rifx.Online","tags":["GPT-5","Ph.D.","intelligence","ethics","privacy"],"draft":false,"slug":"blog/openai-gpt-5-ph-d-level-intelligence-expected-by-2025-50a86c3aad86"},"content":"\n\n\n\n\n\nAfter months of speculation, OpenAI has finally unveiled details about the highly anticipated GPT\\-5\\. Initially expected in 2024, its release has been postponed to late 2025 or early 2026\\. Mira Murati, OpenAI’s CTO, shared insights in an interview with Dartmouth Engineering about the capabilities and potential of this new version. Here’s everything you need to know.\n\n\n## A Quantum Leap in Intelligence\n\nMurati compares previous versions of GPT to different levels of human intelligence. GPT\\-3 is akin to a young child, while [**GPT\\-4**](https://www.geekmetaverse.com/gpt-4-unveils-its-secrets-a-combination-of-8-smaller-models/) is comparable to a high school student. The new GPT\\-5 promises to reach a “Ph.D.\\-level intelligence for specific tasks.” This advancement is not only exciting but also raises questions about the future of artificial intelligence.\n\n\n## Evolution of GPT: From Child to Ph.D.\n\nComparing these versions to stages of human education helps us grasp these advancements better. GPT\\-3, with its ability to generate coherent and useful text, opened many doors. GPT\\-4 improved these skills, demonstrating superior performance in more complex tasks. Now, GPT\\-5 aims to take this to an entirely new level, with advanced reasoning and memory capabilities.\n\n\n## Specialized Intelligence\n\nPh.D.\\-level intelligence doesn’t mean [**GPT\\-5**](https://www.geekmetaverse.com/openai-ceo-confirms-that-gpt-5-is-already-in-development/) can do everything perfectly. Murati clarified that these abilities will be task\\-specific. This suggests that while AI might surpass humans in certain fields, it will still have limitations in others. This specialized focus could lead to highly precise and useful applications in areas like scientific research and complex data analysis.\n\n\n## Potential and Future Applications\n\nThe development of GPT\\-5 opens a range of possibilities across different sectors. From education to medicine, and research to technology, the applications are vast.\n\n\n### Education and Training\n\nAn [**AI**](https://www.geekmetaverse.com/apple-updates-ai-takes-center-stage-with-siri-integration-chatgpt-partnership-and-elon-musk-concerns/) capable of reaching Ph.D. levels could radically transform education. Personalized tutoring systems could provide support to students in complex areas, enhancing understanding and academic performance.\n\n\n### Medicine and Healthcare\n\nIn medicine, an AI with such capabilities could assist in diagnosing rare diseases, developing personalized treatments, and managing large volumes of clinical data, significantly advancing medical care.\n\n\n### Research and Development\n\nResearchers could greatly benefit from an AI that can analyze large datasets, identify patterns, and generate hypotheses, accelerating the pace of scientific and technological discoveries.\n\n\n## Challenges and Ethical Considerations\n\nDespite the promising applications, the development of such advanced AI also brings significant ethical challenges. Over\\-reliance on AI for critical tasks could lead to issues if not managed properly.\n\n\n### Privacy and Security\n\nData privacy and cybersecurity will be crucial topics. Ensuring AI systems are not misused and that sensitive data is adequately protected will be a priority.\n\n\n### Employment Impact\n\nThe impact on employment is also a concern. Automating specialized tasks could displace certain professionals, necessitating proactive measures to address these socioeconomic implications.\n\n\n### Conclusion\n\nThe delay in GPT\\-5’s release may be disappointing for some, but its advanced capabilities generate significant anticipation. If OpenAI meets its goals, we could be looking at a revolutionary tool that transforms multiple industries and changes how we interact with technology.\n\n\n### FAQs\n\n**1\\. What is GPT\\-5?**\n\nGPT\\-5 is the upcoming version of OpenAI’s Generative Pre\\-trained Transformer (GPT) series, promising Ph.D.\\-level intelligence for specific tasks.\n\n**2\\. When is GPT\\-5 expected to be released?**\n\nThe release of GPT\\-5 has been postponed to late 2025 or early 2026\\.\n\n**3\\. How does GPT\\-5 compare to previous versions?**\n\nGPT\\-3 is akin to a young child in intelligence, while GPT\\-4 compares to a high school student. GPT\\-5 aims to achieve Ph.D.\\-level intelligence for specific tasks, offering advanced reasoning and memory capabilities.\n\n**4\\. What kind of tasks will GPT\\-5 be able to perform?**\n\nGPT\\-5 will be specialized in certain tasks, excelling in specific fields like scientific research, complex data analysis, education, and healthcare.\n\n**5\\. Will GPT\\-5 be perfect at everything?**\n\nNo, GPT\\-5’s Ph.D.\\-level intelligence will be task\\-specific, meaning it will excel in certain areas but still have limitations in others.\n\n**6\\. What are the potential applications of GPT\\-5?**\n\nPotential applications include personalized tutoring in education, assistance in diagnosing diseases and developing treatments in healthcare, and aiding researchers in analyzing large datasets and generating hypotheses.\n\n**7\\. What are the ethical considerations associated with GPT\\-5?**\n\nEthical considerations include ensuring data privacy and cybersecurity, managing the socio\\-economic impact of job displacement due to automation, and preventing misuse of advanced AI systems.\n\n**8\\. How will GPT\\-5 impact data privacy and security?**\n\nEnsuring the protection of sensitive data and preventing the misuse of AI systems will be crucial. Measures will need to be implemented to safeguard data privacy and security.\n\n**9\\. What is the potential impact of GPT\\-5 on employment?**\n\nThe automation of specialized tasks by GPT\\-5 could displace certain professionals, necessitating proactive measures to mitigate these socio\\-economic impacts.\n\n**10\\. Why was the release of GPT\\-5 delayed?**\n\nThe delay allows OpenAI to refine and enhance GPT\\-5’s capabilities to ensure it meets the high expectations for its advanced intelligence and specialized applications.\n\n**11\\. How can GPT\\-5 transform education?**\n\nGPT\\-5 could revolutionize education by providing personalized tutoring systems that support students in complex subjects, improving comprehension and academic performance.\n\n**12\\. What advancements could GPT\\-5 bring to the medical field?**\n\nIn medicine, GPT\\-5 could aid in diagnosing rare diseases, developing personalized treatments, and managing vast amounts of clinical data, leading to significant advancements in medical care.\n\nOriginal post: [https://www.geekmetaverse.com/openai\\-gpt\\-5\\-ph\\-d\\-level\\-intelligence\\-2025/](https://www.geekmetaverse.com/openai-gpt-5-ph-d-level-intelligence-2025/)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/openai-just-built-her-in-real-life-17769d993e11","frontmatter":{"title":"Users Will Fall in Love With OpenAI’s New GPT-4o Model. Literally.","meta_title":"Users Will Fall in Love With OpenAI’s New GPT-4o Model. Literally.","description":"The company’s new GPT-4o can understand and mimic human speech and emotion","date":"2024-11-01T04:08:40.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*-bTsggApvkUHAq57YhSd-A.png","categories":["Generative AI","Chatbots","Natural Language Processing"],"author":"Rifx.Online","tags":["GPT-4o","speech","emotions","multilingual","conversational"],"draft":false,"slug":"blog/openai-just-built-her-in-real-life-17769d993e11"},"content":"\n\n\n\n\n## The company’s new GPT\\-4o can understand and mimic human speech and emotion\n\n\n\nIn the iconic 2013 film *Her*, the protagonist develops an intense relationship — which morphs into a love affair — with a voice\\-enabled AI system.\n\nThe AI in *Her* is everything that today’s voice\\-enabled systems are not: emotive, funny, and able to intuit the subtleties of human conversation.\n\nIn a major [announcement this morning](https://www.youtube.com/live/DQacCB9tDaw?app=desktop&si=jvKW7jFDwFvOMBBk), OpenAI announced the release of a new version of its ChatGPT system that natively integrates speech, transcription, and intelligence into a single model.\n\nIt’s powerful, intuitive, and disturbingly human\\-like. Essentially, OpenAI has built a real\\-life version of *Her*.\n\n\n## A Bad Conversationalist\n\nChatGPT has had voice capabilities for months now. Even today, you can open the ChatGPT app on your phone, press the headphones icon, and converse with the system using your voice.\n\nThe problem, though, was that ChatGPT was a terrible conversationalist.\n\nEssentially, ChatGPT’s voice capabilities were a hack created by splicing together three different models.\n\nWhen you would speak to the system, it would first use a transcription model to turn your voice into text. It would then feed that text into its intelligence model — basically, the same system that underpins GPT\\-4\\.\n\nThe intelligence system would generate text, which ChatGPT would feed back into a text\\-to\\-speech system to create a computerized voice that would respond to you.\n\nThis made the system nominally conversational, but actually speaking with it was clunky and awkward.\n\nAll the extra steps of sending content between different models meant that the system was laggy. In my own testing, I found it often took 3 to 5 seconds between speaking to the system and getting a response back.\n\nHuman conversation relies on subtleties that [unfold over milliseconds](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8794835/#:~:text=The%20modal%20conversational%20response%20time,deliberative%20conscious%20control%20(22).). A system that takes up to five seconds to respond to speech feels clunky and robotic.\n\nThe previous system also lacked many fundamental aspects of human speech.\n\nFor example, you couldn’t interrupt it; you had to wait for it to finish speaking before you could respond.\n\nSpeaking with it often felt like talking to one of those un\\-interruptable people who blabbers on about a random topic with no awareness of the other people in the room. You often felt like bring up the Oscars’ orchestra in a desperate attempt to get the system to stop talking.\n\nIt was also constrained by its inability to interpret emotion in voices or to accurately mimic human emotion in its own responses.\n\nHumans are excellent at reading between the lines, partially because we can [pick up on subtle emotive cues in the speaker’s voice.](https://pressbooks.lib.jmu.edu/communicationintherealworldjmu/chapter/non-verbal-communication/)\n\nIf I ask my friend, “How was your day?” and they respond, “It was fine,” but they insert a subtle pause between “was” and “fine” (or there’s a hint of exasperation in the final word), I’d know that they actually had a challenging day, and I should ask some follow\\-up questions.\n\nChatGPT couldn’t do these things, which made speaking to it feel like communicating with some kind of alien intelligence, not a human.\n\nIn short, the previous system fell squarely into the uncanny valley. It was good enough at conversing and had a convincing enough voice that parts of the conversation could feel human\\-like.\n\nBut the weird pauses, lack of emotive understanding, and lag ultimately shattered the illusion, making it come off as more unsettling than useful.\n\nI tried using the previous system with my six\\-year\\-old son. He was so creeped out by it that he wouldn’t let me switch the audio back on again.\n\n\n## OpenAI’s Revoluntary New Model\n\nToday, OpenAI is changing all of that. In their [announcement this morning](https://www.youtube.com/live/DQacCB9tDaw?app=desktop&si=jvKW7jFDwFvOMBBk), the company revealed that they are releasing a new model, GPT\\-4o.\n\nGPT\\-4o natively integrates speech recognition, speech generation, and intelligence into a single system.\n\nThat means that the spaghetti code system integrating three different models to simulate conversation is gone. Instead, the new version of ChatGPT will be able to **take in speech, process it instantly, and respond with realistically generated speech of its own.**\n\nFor users, this will enable several new capabilities that OpenAI CEO Sam Altman [described as “like magic.”](https://twitter.com/sama/status/1788989777452408943)\n\nFor one, you’ll be able to converse with ChatGPT much more naturally. Instead of having to type your questions and follow\\-ups into an interface, you’ll be able to speak with the app as if you’re talking to a friend.\n\nIn several live demos, OpenAI’s engineers showed how the system can listen to a user and respond with an intelligent result within milliseconds.\n\nAgain, those speeds are possible because the new model doesn’t need to waste time switching modalities — it can process voice and respond with its own voice in a single step, instead of resorting to multiple lower\\-level models.\n\nGPT\\-4o can also interpret and create emotion.\n\nIn one demo, an OpenAI staff member asked the system to lead him through a breathing exercise.\n\nHe then pretended to hyperventilate, and ChatGPT — sensing the speed with which he was breathing and the apparent panic in his voice — urged him to slow down and take deeper breaths.\n\nThe system also appears capable of modulating the emotion in its own responses. In another demo, the staff member asked GPT\\-4o to read a bedtime story in an increasingly dramatic voice.\n\nIt obliged, ultimately sounding like a middle school theater kid horrifically overacting a scene!\n\nBecause the new system is also integrated with GPT\\-4’s vision capabilities, it can perform functions like interpreting the emotions on a person’s face.\n\nThis increased level of emotional intelligence will likely make the system a much better conversationalist.\n\nOther new capabilities will help, too. Users can interrupt GPT\\-4o mid\\-sentence.\n\nDuring their demos, OpenAI staff members frequently interrupted the model when it started to go on tangents, as one might interrupt a friend to start responding to a real\\-life question.\n\n\n## Huge Potential\n\nThe demos this morning were lighthearted and funny. But one can quickly see how a model that can easily interpret, quickly process, and realistically create emotive human speech could be incredibly powerful.\n\nSeveral times during the demo, ChatGPT responded in ways that reminded me of the fictional AI from *Her*.\n\nChatGPT appeared to laugh at itself, become embarrassed when OpenAI staff members complimented it, and perhaps even throw in a flirty line here and there.\n\nSeveral (purportedly) unscripted interactions also revealed some of the deeper capabilities that better conversation could unlock.\n\nBased on an audience question, OpenAI’s staff members demonstrated how the system could listen to speech in Italian and quickly and accurately translate it into English speech, and vice versa.\n\n\n\n\n\n\n\nOne can easily imagine how such a capability could make multi\\-lingual interactions incredibly simple, essentially eliminating language barriers (and perhaps, human translators).\n\nA doctor, for example, could pull up ChatGPT and use it to quickly speak with a patient in any language. While traveling, you could pull up the app on your phone and use it as a free and instantaneous translator to ask someone for directions or to make a purchase in a store.\n\nAdding the vision capabilities, one could even show ChatGPT a foreign restaurant menu, ask for a translation of certain items, tell it when you like to eat at home, and ask it to recommend some dishes you might want to order (or avoid.)\n\nI can also see how quickly the new system could venture into *Her* territory. OpenAI still doesn’t allow the kinds of NSFW interactions that happened in the movie.\n\nBut GPT\\-4o’s ability to understand and mimic emotion — coupled with its powerful, often uncanny abilities to produce its own convincing human emotional speed — is striking.\n\nListening to the demos, I’m certain that people will fall in love with this system, just as the protagonist did in *Her*. It’s that good.\n\n\n## Will it get used?\n\nAll of this is amazing on paper. It’s unclear, however, how many users actually want a fully emotive AI voice companion.\n\nMost people I work with use ChatGPT not as a conversational companion, but for utilitarian purposes.\n\nI’ve seen colleagues leverage the system for boring and mundane tasks like writing the landing page copy for a webinar, turning out a quick response to an email from their landlord, or writing the first draft of a blog post.\n\nNone of these utilitarian functions really require conversation. It’s unclear whether being able to speak these kinds of requests to an AI would be useful.\n\nThe real test, then, is not necessarily how capable OpenAI’s new system is, but **how well they integrate it into places where people are already interacting with computers via their voices.**\n\nRealistically, I can’t see many users sitting down at work and conversing with AI.\n\nBut if OpenAI integrates GPT\\-4o into voice interfaces on cell phones, in cars, or on smart devices like the Amazon Echo, I could easily see the system’s emotive capabilities becoming much more useful.\n\nEven if people don’t want to speak with ChatGPT very much, the new capabilities of a natively multimodal audio and vision model will be incredibly powerful for developers who build applications on top of OpenAI’s existing API.\n\nIn their announcement, OpenAI said that GPT\\-4o will be available through their existing developer interfaces. The system will also be 50% cheaper than previous models of GPT\\-4\\.\n\nThose changes alone are massive. Whether or not the speech element really takes off, the intelligence that powers it will also make hundreds of existing GPT\\-4\\-powered applications smarter, faster, better, and cheaper to operate.\n\nThe conversational elements of the new system, in other words, might turn out to be a cool gimmick. But the underlying impact will be subtler and broader.\n\nI’m excited to see how real\\-life users interact with GPT\\-4o. Will they be creeped out? Amazed? Wooed?\n\nBut I’m even more excited to fire up my Python IDE and add GPT\\-4o into the applications I’ve already built using OpenAI’s tools.\n\nSpeaking to a machine is cool. But a natively multimodal AI model that understands human emotions, and that I can summon with a few lines of Python code, for cheap? That could truly change the world.\n\n**I’ve tested thousands of ChatGPT prompts over the last year. As a full\\-time creator, there are a handful I come back to every day that fit with the ethical uses I mention in this article. I compiled them into a free guide, *7 Enormously Useful ChatGPT Prompts For Creators.* [Grab a copy today!](https://no-frills-influencer.ck.page/6a100e8fe4)**\n\n\n"},{"lang":"en","group":"blog","slug":"blog/openai-realtime-api-voice-mode-getting-started-on-colab-39b93edcaa6a","frontmatter":{"title":"OpenAI Realtime API (Voice Mode), Getting Started on Colab","meta_title":"OpenAI Realtime API (Voice Mode), Getting Started on Colab","description":"Everything you need to know, and a hands-on introduction to OpenAI’s voice mode API that you can run on Colab.","date":"2024-11-08T00:23:32.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*_-d5zsWWQEzVLZxABTSFWQ.png","categories":["Programming","Voice Assistants","Technology/WebAPI"],"author":"Rifx.Online","tags":["OpenAI","Realtime","API","GPT-4o","Colab"],"draft":false,"slug":"blog/openai-realtime-api-voice-mode-getting-started-on-colab-39b93edcaa6a"},"content":"\n\n\n\nEverything you need to know, and a hands\\-on introduction to OpenAI’s voice mode API that you can run on Colab.\n\n\n\nThe latest development from OpenAI brings us the **Realtime API**, designed to allow developers to create **fast, seamless speech\\-to\\-speech experiences** within their apps. This API aims to streamline the development of multimodal conversational features, making it much easier to build natural, real\\-time voice interactions.\n\nI**n this blog post,** I’ll cover the **main questions** around this new API, including\n\n* what is Realtime API,\n* How to access it,\n* Its limitations and pricing,\n* and provide a **Colab tutorial** on how to get started.\n\n\n## What is the Realtime API?\n\nThe **Realtime API** by OpenAI is a public beta feature that enables paid developers to incorporate real\\-time voice interaction in their apps. It’s a multimodal API capable of transforming **audio inputs to speech responses**, using the advanced **GPT\\-4o** model for this purpose. Essentially, it allows for **low\\-latency conversations** similar to a natural human interaction, similar to the functionality seen in ChatGPT’s Advanced Voice Mode.\n\nPreviously, developers had to stitch together multiple models for **speech recognition, text processing, and text\\-to\\-speech generation**. The Realtime API does this all in a single API call, resulting in fewer delays, richer responses, and more consistent handling of accents and emphasis.\n\nThe **Chat Completions API** also introduces audio input and output, but it doesn’t offer the low\\-latency experience of the Realtime API. Thus, for experiences like language learning or voice\\-enabled assistants, Realtime API is the preferred choice.\n\n\n## Access and Limitations\n\nAccess to the **Realtime API** is currently available as a **public beta** for paid developers.\n\n**Although it is said that access is limited in Europe, I was able to use it through my tier 5 OpenAI account.**\n\nThe API uses a **WebSocket** connection, which ensures a smooth streaming experience for both audio inputs and outputs.\n\nFor now, there are **limitations** to note:\n\n* **Session Rate Limits**: The API is rate limited to approximately **100 simultaneous sessions** for Tier 5 developers. Lower tiers have smaller capacity. As of Octobre 2024, the API is limited 2M tokens per minutes.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*XpAB6WRseRb0iY-edE94xw.png)\n\n* **Capabilities**: Initially, only **voice modality** is supported, but OpenAI plans to add more like **video** and **vision** over time.\n* **Availability**: Full audio capabilities are in the beta phase, with **future SDK integration** planned for Python and Node.js.\n\n\n## Pricing of the Realtime API\n\nThe **pricing** structure for the Realtime API is divided into both **text tokens** and **audio tokens**:\n\n* **Audio Input**: $100 per 1 million tokens (approx. **$0\\.06 per minute**).\n* **Audio Output**: $200 per 1 million tokens (approx. **$0\\.24 per minute**).\n* **Text Input**: $5 per 1 million tokens.\n* **Text Output**: $20 per 1 million tokens.\n\nThe pricing makes it affordable for developers to create robust **speech\\-to\\-speech** experiences, though audio features are significantly more expensive than text\\-based interactions. This is important to keep in mind when scaling an app with voice features.\n\nIt is still slightly more expensive than outsourcing it to some countries, but we can expect a significant drop in prices over the next six months.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*ocwFDXEt8X7KD_k6)\n\n\n## Building with the Realtime API in Google Colab\n\nHere’s a basic **Colab guide** to help you get started with uploading a file, sending a request to the Realtime API, and generating audio responses.\n\nIn this demo, we chose to upload a stream of audio chunks to mimic a conversation.\n\n**Full Colab Code**: [link here](https://colab.research.google.com/drive/1-bj_LH7Gv2bbTJopbo7Hk_AIyDAuqeEQ?usp=sharing), simply add your “openai” key to Colab’s secrets and run the colab.\n\n\n### Step 1: Setting Up Google Colab and Dependencies\n\n* Start a new **Google Colab** notebook.\n* Install the necessary libraries such as **requests** and **pydub** for managing audio files.\n\n\n```python\n#Setup\n!pip install websockets pydub --quiet \n\nimport base64\nimport numpy as np\nimport soundfile as sf\nimport json\nimport websockets\nfrom google.colab import files\nfrom pydub import AudioSegment\nfrom tqdm import tqdm\nimport io\n```\n\n### Step 2: Uploading Your Audio File\n\nIn Colab, you can use the `files` module from **google.colab** to upload audio files.\n\n\n```python\n#Upload audio\ndef upload_audio():\n    uploaded = files.upload()  \n    for file_name in uploaded.keys():\n        return file_name\n\naudio_file = upload_audio()\n```\n\n### Step 3: Sending a Request to the Realtime API\n\n* Format the audio file properly before sending it to OpenAI.\n* Establish a WebSocket connection to stream the audio file.\n* Use `tqdm` to display the progress of the upload stream.\n* The function returns the full set of events (including responses) for later processing to generate the output audio. It also returns the transcript of the model’s response.\n\n\n```python\n#Helper functions\n## Function to convert Float32Array to PCM16 format\ndef float_to_pcm16(float32_array):\n    return np.clip(float32_array * 32767, -32768, 32767).astype(np.int16).tobytes()\n\n## Function to split audio into base64-encoded PCM16 chunks\ndef float32_to_base64_chunks(float32_array, chunk_size=32000):\n    pcm16_data = float_to_pcm16(float32_array)\n    for i in range(0, len(pcm16_data), chunk_size):\n        yield base64.b64encode(pcm16_data[i:i+chunk_size]).decode('utf-8')\n\n## WebSocket connection and streaming audio with text prompt\n## Main function to call OpenAI Realtime API\nasync def stream_audio_to_realtime_api(audio_file, text_prompt, openai_key, verbose = False):\n    data, samplerate = sf.read(audio_file, dtype='float32')\n    if data.ndim > 1:\n        data = data[:, 0]\n    if samplerate != 24000:\n        raise ValueError(f\"Audio must be sampled at 24kHz, but it is {samplerate}Hz\")\n\n    url = \"wss://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2024-10-01\"\n    headers = {\"Authorization\": \"Bearer \" + openai_key, \"OpenAI-Beta\": \"realtime=v1\"}\n\n    async with websockets.connect(url, extra_headers=headers) as ws:\n        await ws.send(json.dumps({\n            \"type\": \"conversation.item.create\",\n            \"item\": {\"type\": \"message\", \"role\": \"user\", \"content\": [{\"type\": \"input_text\", \"text\": text_prompt}]}\n        }))\n\n        with tqdm(total=(len(float_to_pcm16(data)) + 32000 - 1) // 32000, desc=\"Sending Audio Chunks\") as pbar:\n            for chunk in float32_to_base64_chunks(data):\n                await ws.send(json.dumps({\"type\": \"input_audio_buffer.append\", \"audio\": chunk}))\n                pbar.update(1)\n\n        await ws.send(json.dumps({\"type\": \"input_audio_buffer.commit\"}))\n        await ws.send(json.dumps({\"type\": \"response.create\"}))\n\n        all_events = []\n        while True:\n            response = await ws.recv()\n            event = json.loads(response)\n            all_events.append(event)\n            if verbose:\n                print(event)\n            if event[\"type\"] == \"response.output_item.done\" and \"item\" in event and \"content\" in event[\"item\"]:\n                for content in event[\"item\"][\"content\"]:\n                    if content[\"type\"] == \"audio\" and \"transcript\" in content:\n                        transcript = content[\"transcript\"]\n                        break\n            if event[\"type\"] == \"rate_limits.updated\":\n                break\n\n        return all_events, transcript\n```\n\n```python\n#Add a prompt and call OpenAI Realtime API\ntext_prompt = \"Summarize this audio content\"\n\nevents, transcript = await stream_audio_to_realtime_api(\n    audio_file, \n    text_prompt, \n    openai_key, \n    verbose = False \n#to display OpenAI's response as they arrive, use verbose = True\n    ) \n```\n\n### Step 4: Generating Audio Responses\n\n* Once you receive the response, generate the audio.\n* Choose a file name and save the file.\n* You will then be able to download the file.\n\n\n```python\n## Function to decode and concatenate audio chunks into a full audio file\ndef generate_audio_from_chunks(audio_chunks, output_filename=None):\n    # Concatenate the base64-encoded audio chunks from the 'delta' field\n    full_audio_base64 = ''.join(audio_chunks)\n\n    # Decode the concatenated base64 string to raw PCM16 audio bytes\n    audio_bytes = base64.b64decode(full_audio_base64)\n\n    # Load the bytes as a pydub AudioSegment (assuming 24kHz, 1 channel, PCM16)\n    audio_segment = AudioSegment.from_raw(\n        io.BytesIO(audio_bytes), \n        sample_width=2, \n        frame_rate=24000, \n        channels=1)\n\n    # Optionally save the audio to a file\n    if output_filename:\n        audio_segment.export(output_filename, format=\"wav\")\n        print(f\"Audio saved to {output_filename}\")\n\n    return audio_segment\n```\n\n```python\n#Extract audio chunks from the collected events\naudio_output_chunks = [event['delta'] for event in events if event['type'] == 'response.audio.delta']\n\n## Generate the full audio from the collected chunks\ngenerated_audio = generate_audio_from_chunks(audio_output_chunks, output_filename=\"output_audioo.wav\")\n```\n\n## Conclusion\n\nWith the above steps, you can integrate OpenAI’s Realtime API into a Colab notebook, enabling seamless voice instructions.\n\nThis guide should give you a solid foundation for experimenting with real\\-time audio\\-to\\-audio interactions and building innovative voice\\-driven applications.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/openai-rolls-out-searchgpt-to-more-users-33024ff3132c","frontmatter":{"title":"OpenAI Rolls Out SearchGPT To More Users","meta_title":"OpenAI Rolls Out SearchGPT To More Users","description":"ChatGPT got a huge user interface redesign with support for SearchGPT — it now resembles search engines like Google and Perplexity.","date":"2024-11-01T03:57:02.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BW6Qt6PMwHwlYRljAIBQWg.jpeg","categories":["Chatbots","Technology/Web","SearchGPT"],"author":"Rifx.Online","tags":["SearchGPT","ChatGPT","web","search","publishers"],"draft":false,"slug":"blog/openai-rolls-out-searchgpt-to-more-users-33024ff3132c"},"content":"\n\n\n\n\n\n**Have you noticed OpenAI’s latest redesign of ChatGPT?**\n\nIf you’ve logged in recently, you might have spotted two major changes.\n\n* First, there’s the new [**Canvas**](https://generativeai.pub/openai-rolls-out-canvas-in-chatgpt-a-brand-new-writing-and-coding-interface-7b57a3ec582a)feature that automatically opens a new interface on the right side. This addition lets you work on longer documents without having to scroll up and down through the chat. It’s a small but handy update.\n* Second, the **prompt field** has moved up and now sits in the center of the screen.\n\nTake a look at the latest user interface below:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*KsYMU9ffVlKsmHzKIOKH8g.png)\n\nHave you noticed the resemblance of this new layout to Google and Perplexity AI? ChatGPT now looks like a search engine.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*xFKErUHnfbJunaKi4NvM9A.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*PgSwS14lkUtZe7Ra9oLCrg.png)\n\nNow when you hit the ‘/’ key on your keyboard, you can toggle a new “Search” feature that lets ChatGPT access the web.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*oYxvVvsuUuc_PM0PXRmN7A.png)\n\nLet’s break down what this all means.\n\n\n## What is the Search Feature in ChatGPT?\n\n[SearchGPT](https://generativeai.pub/openai-announces-search-gpt-is-this-the-google-killer-5919ba31f95b) allows ChatGPT to access real\\-time web data. Similar to how Perplexity works, it uses a large language model that searches the web for you, gives you immediate answers, and includes the sources it pulls from.\n\nThe feature was initially made accessible to 10,000 users and added a waitlist form for those who wanted to get early access.\n\nOpenAI has partnered with well\\-known publishers like **The Wall Street Journal, The Associated Press, Vox Media, and Time** to make sure users receive credible, trustworthy information.\n\n\n> “AI search is going to become one of the key ways that people navigate the internet, and it’s crucial, in these early days, that the technology is built in a way that values, respects, and protects journalism and publishers. We look forward to partnering with OpenAI in the process, and creating a new way for readers to discover The Atlantic.” — Nicholas Thompson, CEO of The Atlantic\n\nWhen you ask SearchGPT a question, it doesn’t just pull information from random sources. Each response comes with **clear, in\\-line attribution and links**, so you know exactly where the information is coming from.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*uchpKOXqZCG55HSNkaOOZQ.png)\n\nYou can even dive deeper by clicking on the source links that appear below the searched sites dropdown, giving you more ways to explore the topic.\n\n\n## How to Access SearchGPT\n\nAccessing SearchGPT is super simple. When you’re in ChatGPT, press the **‘/’** key and select the Search option from the menu.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*xNfm-6zPFzdXGL2A0D92YQ.png)\n\nIt works much like any other search engine: you ask your question, and within seconds, SearchGPT provides an answer, complete with sources.\n\nYou can even ask follow\\-up questions to dig deeper into the topic. This creates a conversational search experience, much more interactive than scrolling through traditional search results.\n\n\n## SearchGPT vs. Perplexity vs. Google\n\nSo how does SearchGPT compare to **Perplexity AI** and **Google**?\n\n**SearchGPT** is built to give you concise, sourced answers. Each answer has a link to the original source, and you can click on it to verify the information. It’s ideal for real\\-time answers and quick fact\\-checking.\n\nPlus, with follow\\-up questions, you can refine your query without starting over. This conversational nature makes it feel like you’re talking to a super\\-advanced version of Google that remembers what you’ve been asking.\n\n**Perplexity**, on the other hand, is a more academic\\-style search engine. It emphasizes scholarly articles and detailed research, which can be useful for more in\\-depth queries. Perplexity is often preferred for research\\-heavy tasks where you need deeper sources\n\n**Google**, of course, is still the giant in the room. Despite their recent efforts to integrate generative AI into search results, they haven’t quite nailed the seamless experience that users want.\n\nGoogle’s generative search rollout was clunky and received a lot of backlash due to errors and irrelevant responses. But Google’s breadth of information and infrastructure are still unmatched.\n\n\n## Is This the End of Google?\n\nGoogle isn’t going anywhere soon. The tech giant still controls over 90% of the search market. They’ve been at this for decades, and their search algorithms are constantly evolving.\n\nHowever, with AI search engines like SearchGPT gaining ground, Google is under pressure to step up its game. OpenAI’s move to partner with publishers for credible sources is a smart strategy that could chip away at Google’s dominance.\n\nThis focus on verified results means that when you use SearchGPT, you’re less likely to run into hallucinated answers — something that AI\\-driven tools have struggled with in the past.\n\nAlso, Google is still the default for most people. It has the advantage of being everywhere — from your phone’s browser to your smart speaker. SearchGPT is still in its early stages and would need time to gain that level of trust from users.\n\n\n## SearchGPT Isn’t There Yet\n\nI’ve been testing SearchGPT in the past couple of hours and here are some of my observations:\n\n* **Quality of the answer:** One major downside is that SearchGPT’s answer quality doesn’t quite match the depth or precision of Perplexity Pro. Although it’s comparable to the base version of Perplexity, users who rely on it for more complex or nuanced queries will notice a difference.\n* **Slow response:** Another pain point is speed. When using SearchGPT, the time it takes to process a query and return an answer can feel excruciatingly slow. This delay disrupts the flow of interaction, particularly when you’re in the middle of a deep dive into a topic.\n* **Lack of contextual understanding:** In some cases, it fails to recognize the continuity of a conversation. If you ask a follow\\-up question, instead of understanding it in the context of your previous query, the model often treats it as a fresh, standalone question.\n* **No follow up suggestions:** Unlike Perplexity, which often suggests follow\\-up questions to help you refine your search, SearchGPT doesn’t offer this feature. This lack of guidance leaves users to figure out how to best phrase or narrow down their queries on their own.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*vfXxpgENLyenLY2l33PKiw.png)\n\nHere’s another weird workflow I noticed while using the search feature: If you switch the language model from GPT\\-4o to “ChatGPT o1\\-preview,” the search indicator remains but does not actually search the web for results.\n\nIt returns results from its domain knowledge, which isn’t what the users expect.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*YcI-UuEmmFTQPUlO6mjpxA.png)\n\nThe correct behavior should be to disable the *search* function once the user switches to “ChatGPT o1\\-preview” because this model does not have the capability to search the web.\n\n\n## Final Thoughts\n\nI’m really glad OpenAI has finally rolled out SearchGPT. I’ve been wanting to test it out ever since they announced it in July 2024\\.\n\nIn its current state, SearchGPT is a good first step for OpenAI into the world of AI\\-powered search, but it’s not quite ready to become anyone’s go\\-to tool for complex, real\\-time queries.\n\nThe accuracy, speed, and ability to handle conversational context just aren’t there yet. For now, if you need deep insights or faster results, tools like Perplexity Pro or Google remain the better options.\n\nFurther reading:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*5ejBBgbZaE8pGmpW.png)\n\nThis story is published on [Generative AI](https://generativeai.pub/). Connect with us on [LinkedIn](https://www.linkedin.com/company/generative-ai-publication) and follow [Zeniteq](https://www.zeniteq.com/) to stay in the loop with the latest AI stories.\n\nSubscribe to our [newsletter](https://www.generativeaipub.com/) and [YouTube](https://www.youtube.com/@generativeaipub) channel to stay updated with the latest news and updates on generative AI. Let’s shape the future of AI together!\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*TnRFuKk-2Dj_KCAP.png)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/openai-searchgpt-chatgpt-with-internet-and-browsing-tools-023ddca7cb44","frontmatter":{"title":"OpenAI SearchGPT: ChatGPT with Internet and browsing tools","meta_title":"OpenAI SearchGPT: ChatGPT with Internet and browsing tools","description":"A Better Alternative for Perplexity and Google Search","date":"2024-11-08T00:28:30.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*N_EtjjOxkx6QsKRLx5f_cQ.png","categories":["Technology/Web","Data Science","SearchGPT"],"author":"Rifx.Online","tags":["SearchGPT","filtering","citations","recommendations","customization"],"draft":false,"slug":"blog/openai-searchgpt-chatgpt-with-internet-and-browsing-tools-023ddca7cb44"},"content":"\n\n\n\n\n### A Better Alternative for Perplexity and Google Search\n\n\n\nAnd the much\\-anticipated product by OpenAI, SearchGPT is out last night boasting some major features, taking it a step ahead of Perplexity, their arch\\-rivals.\n\n\n\n\n\n\n\nAs announced by OpenAI, SearchGPT is a lot more than just ChatGPT with the internet.\n\nIt is an AI web browser in itself\n\nTalking about a few key features:\n\n* **Advanced Filtering**: Set filters for specific dates, sources, or content types (e.g., peer\\-reviewed articles only, government sites, etc).\n* **Context\\-Aware Summaries**: Generate summaries, key takeaways, or insights tailored to particular fields, like medicine or finance.\n* **Citation Generation**: Automatically format and provide citations in academic styles (APA, MLA).\n* **Multi\\-Step Queries**: Handle complex, layered questions across multiple sources in a single search.\n* **Data Analytics Integration**: Directly pull and analyze data for insights (e.g., trend analysis). SearchGPT could connect to specialized databases, allowing access to specific fields (like medical journals, legal case databases, or proprietary business analytics).\n* **Personalized Recommendations**: Suggest relevant sources, articles, or updates based on your search history. This includes **Pre\\-set Templates or Personas**: For example, a research\\-focused “SearchGPT” could be set up to retrieve scientific data and offer academic citations directly.\n\n\n### Was ChatGPT not able to access the internet before?\n\nIt does (for premium members). But now, these capabilities are more advanced. To check out how a general web search is different from SearchGPT, I tried asking ChatGPT itself (free version) to search the internet for a query:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ORjGLDBqKDWiPANHlxSdqw.png)\n\nThen, I asked how this search was done.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*NlzDed3nHdJ636aLt75DCg.png)\n\nNext up,\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*eeDPQHQA62yaMK_KkSL0kQ.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ZcsmVgau0SaavN01yHbdKw.png)\n\nSo, as you can read, SearchGPT is not just a mere web browsing tool, but a lot more. Unfortunately, OpenAI has made SearchGPT available for Pro users and only to the waitlist users. If you’ve access, you must have received this mail yesterday\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*WfO0Xl4VQwRxNNkC1GQpOw.png)\n\nBelow are some screen grabs from SearchGPT\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8bbGpwbsRzo6xQhNDPb3Jw.png)\n\nAs you can see, it provides trending topics as suggestions while searching, similar to a web browser.\n\nTry checking out the weather today\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*gbtr-QFQw4BnhqrWPvH4RQ.png)\n\nAnd can even restrict it to just check specific websites like “cite just government sites”\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*eq_Xf4JkD4XV85KE5376VA.png)\n\nAll the citations can be explored at the bottom together for your results\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*wUTEae5yYh_j-oaq6HyP9Q.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Quruyw07__p3qoJ_KhmHAA.png)\n\n\n## SearchGPT vs Perplexity. Which is better?\n\nA tough question to answer, at least for now. A few points to highlight are\n\n1. Perplexity is free, SearchGPT is not!\n2. SearchGPT is faster. But I assume this is because of lesser traffic right now.\n3. Perplexity is simpler, and also, has the early mover advantage\n4. Perplexity, being in the space for a long time, is more reliable compared to SearchGPT\n5. SearchGPT provides more customizations and is not just LLM with the internet.\n\nTo be honest, I’m always in favour of free stuff hence will prefer Perplexity any day. Though, given SearchGPT’s early response, the tool is pretty good and worth trying out.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/openais-leaked-gpt2-model-has-everyone-stunned-6337904c2ecf","frontmatter":{"title":"OpenAI’s ‘Leaked’ GPT2 Model Has Everyone Stunned.","meta_title":"OpenAI’s ‘Leaked’ GPT2 Model Has Everyone Stunned.","description":"On-Purpose leak?","date":"2024-11-01T04:07:40.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*-G0yfSjGPdNw02NZ","categories":["Chatbots","Generative AI","Natural Language Processing"],"author":"Rifx.Online","tags":["GPT-2","Chatbot","Inference","JSON","AlphaGo"],"draft":false,"slug":"blog/openais-leaked-gpt2-model-has-everyone-stunned-6337904c2ecf"},"content":"\n\n\n\n\n### On\\-Purpose leak?\n\n\n\nThe influence that OpenAI has on the AI industry can’t be understated. Every move or decision makes headlines automatically… even if they don’t actually announce the thing.\n\nA few days ago, a model many of us played with that has since been deleted has the entire AI industry fascinated. Named “gpt2\\-chatbot” it was accessible for a few days in the ‘Direct Chat’ function in [lmsys.org](https://chat.lmsys.org/).\n\n*But why so much fuss?*\n\nWell, because this model is unlike anything we have ever seen. **It’s on a completely different level.**\n\nFor this reason, many believe it has been the unofficial teaser of **ChatGPT\\-4\\.5** or even **GPT\\-5**. Or, even more exciting, using the number ‘2’ as a signal that a **new GPT generation of long\\-inference models is approaching**.\n\nEven Sam Altman, CEO of OpenAI, couldn’t resist the temptation to acknowledge its existence and tease us in the process:\n\n\n\n\n\n\n\nSo, *how good is this model, and what on Earth is it?*\n\n\n> You are probably sick of AI newsletters talking about how this or that \\*\\*just\\*\\* happened. And these newsletters abound because coarsely talking about events and things that already took place is easy, **but the value provided is limited and the hype exaggerated.**\n\n\n> However, newsletters talking about what **will** happen are a rare sight. If you’re into easy\\-to\\-understand insights looking into the future of AI before anyone else does, **TheTechOasis** newsletter might be perfect for you.\n\n\n> 🏝️🏝️ Subscribe today below:\n\n\n## A Teaser of What’s to Come\n\nWith every passing day, it’s clear that OpenAI’s next model will be a leap in reasoning and complex problem\\-solving.\n\nAnd to prove how this new mysterious model might be it, here are just a few examples of the prowess of this mysterious model that could signal that the boat has landed in that port:\n\n\n> All examples below are considered **hard or outright impossible** for the current state\\-of\\-the\\-art models.\n\nFor starters, It solved a math\\-olympiad problem in zero\\-shot mode (without being provided with auxiliary examples to support the resolution):\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*oNPg_hTGc0OP90n9)\n\nI can’t even start to explain how crazy the previous example is, it’s absolutely impossible to get such an answer from the current state\\-of\\-the\\-art models.\n\n[It’s also absolutely superb at parsing JSONs](https://twitter.com/skirano/status/1785035706173214888), a fundamental skill for LLM integration with APIs and other web\\-based tools.\n\nAlso, it completely obliterates GPT\\-4 at complex drawing tasks like [drawing SVG files based on code](https://twitter.com/decentricity/status/1785049191003361778) or **unicorns using ASCII code (below)**, humiliating **Claude 3 Opus**, the current state\\-of\\-the\\-art, in the process:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*5A0EcRU91ZYAwVAc)\n\nAdditionally, although this very well could have been a hallucination, **the model claimed to me that it was trained by OpenAI and based on a GPT\\-4 variant.**\n\nOf course, after such a demonstration of power, **many suggest that “gpt2\\-chatbot” might even be the famous Q\\* model**.\n\nBut instead of simply giving in to the different fanciful options people have claimed this is, let’s take a more sensible approach and see what OpenAI itself has been hinting at through their research for months (and years).\n\n\n## The Power of Long Inference\n\nFor several months, experts in the space like [Demis Hassabis](https://www.youtube.com/watch?v=eqXfhejDeqA&t=2s) or [Andrej Karpathy](https://youtu.be/c3b-JASoPi0?si=fZWoSpLuSmua8YMR&t=1481) have discussed how LLMs alone simply aren’t it, and that we need ‘something else’ to really take them to the next step.\n\nIn both cases, they refer to achieving the equivalent of ‘AlphaGo but in LLMs’, which is indirectly referring to:\n\n* **Self\\-improvement** and\n* **test\\-time computation** LLMs\n\n*But what do they mean by that?*\n\n\n### A Giant Step for AI\n\nAlphaGo is history of AI. It was the first model that unequivocally surpassed human might in the game of **Go**, a Korean board game.\n\nIt used **Monte Carlo Tree Search**, a search algorithm, to explore the realm of possible moves for any given step in the game, being able to go beyond the current action and predict what the opposing player would do.\n\n\n> Some of you might remember **Deep Blue** too, the chess machine that barely beat Gary Kasparov in the second game in their series back in 1997 after losing the first game.\n\n\n> However, while Deep Blue could be beaten, AlphaGo was invincible.\n\n*But how?*\n\n\n### Self\\-improving to go Superhuman\n\nThe key element that made AlphaGo superior was how it was trained, **by playing against lesser versions of itself to create a self\\-improvement loop.**\n\nIt consistently played against itself, gradually improving its ELO to 3\\.739, almost at the level of today’s best Go player.\n\n\n> In 2017, AlphaZero, an improved version, achieved a 5\\.018 ELO, completely superhuman and unbeatable.\n\nIn other words, with AlphaGo humans had achieved, for the first time, a way to train a model by self\\-improvement, allowing it to achieve superhuman capacities **as it no longer relied on imitating humans to learn.**\n\nIn case you’re wondering, this is not the case for LLMs.\n\nCurrent LLMs are completely chained to human\\-level performance, as all data and training are inherently human\\-dependent (to the point that [the alignment phase](https://thewhitebox.ai/llms-the-backbones-of-frontier-ai/), the part of the training process where LLMs are modeled to improve their safety levels and avoid offensive responses, **is strictly executed using ‘human preferences’**).\n\n\n> On a side note, [Meta recently proposed Self\\-Rewarding Models](https://arxiv.org/pdf/2401.10020v1) that could self\\-improve with their own responses. However, it’s unclear whether this feedback loop really can make LLMs superhuman.\n\nBut even though it still feels hard to believe that “gpt2\\-chatbot” has been trained through self\\-improvement, **we have plenty of reasons to believe it’s the first successful implementation of what OpenAI has been working on for years: test\\-time computation**.\n\n\n### The Arrival of test\\-time computation models\n\nOver the years, several research papers by OpenAI have hinted at this idea of skewing models into ‘heavy inference’.\n\nFor example, back in 2021, [they presented the notion of using ‘verifiers’](https://arxiv.org/pdf/2110.14168) at inference to improve the model’s responses when working with Math.\n\nThe idea was to train an auxiliary model that would evaluate in real\\-time several responses the model gave, choosing the best one (which was then served to the user).\n\nThis, combined with some sort of tree search algorithm like the one used by AlphaGo, with examples like Google Deepmind’s [Tree\\-of\\-Thought research](https://arxiv.org/pdf/2305.10601) for LLMs, and you could eventually create an LLM that, before answering, explores the ‘realm of possible responses’, **carefully filtering and selecting the best path toward the solution.**\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*pHWwOA66fxpKbl-z)\n\nThis idea, although presented by OpenAI back in 2021, has become pretty popular these days, [with cross\\-effort research by Microsoft and Google applying it to train next\\-generation verifiers](https://arxiv.org/pdf/2402.06457), and with Google even managing to create a model, [Alphacode](https://storage.googleapis.com/deepmind-media/AlphaCode2/AlphaCode2_Tech_Report.pdf), that executed this kind of architecture to great success, **reaching the 85% percentile among competitive programmers, the best humans at it.**\n\n*And why does this new generation of LLMs have so much potential?*\n\nWell, **because they approach problem\\-solving in a very similar way to how humans do**, through the exercise of deliberate and extensive thought to solve a given task.\n\nBottom line, think of ‘search\\+LLM’ models as AI systems that allocate a much higher degree of compute (akin to human thought) to the actual runtime of the model so that, instead of having to guess the correct solution immediately, they are, simply put, ‘given more time to think’.\n\nBut OpenAI has gone further.\n\n\n### PRM Models for Improved Maths Execution\n\nBack in May last year, they released the paper [Let’s Verify Step\\-by\\-Step](https://arxiv.org/pdf/2305.20050), with the participation of the man himself Ilya Sutskever, Chief Scientist at OpenAI, and some of the researchers from the original verifier paper like Karl Cobbe.\n\nThe idea here is to modify the reward model used during the alignment phase of the model.\n\n[Although I recommend checking this article for a full guide on LLM training](https://thewhitebox.ai/llms-the-backbones-of-frontier-ai/), the last step in the process of creating products like ChatGPT is the use of Reinforcement Learning from Human Feedback, or RLHF.\n\nThe idea is for the model to improve its decision\\-making. Thus, we train an auxiliary reward model (which is essentially an almost identical copy of the model being trained) that learns to rank the results of the trained model according to human preferences.\n\n*The issue?*\n\nWell, most reward models today are **ORMs, or Outcome\\-Supervised Reward Models**. In layman’s terms, to evaluate the degree of correctness or the model’s prediction, they look at it globally, disregarding the entire ‘thought process’.\n\nOn the other hand, **PRMs, or Process\\-Supervised Reward Models, evaluate every single step in the response of the model**. Consequently, they ‘force’ the model to pay close attention and effort to every single step of the process, which is crucial in situations like solving a maths equation like the one below:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8JC6sZl5UFfl3WorliQy-A.png)\n\nHowever, this is a very, very expensive process as the preference data requires heavy human crafting so that the supervisory signal can be applied. Consequently, every single training example has dozens or more rewards to measure.\n\nTherefore, “gpt2\\-chatbot” might have included some sort of variation of the reward training considering how proficient it is at generating plans and executing complex problem\\-solving.\n\n\n## Impossible not to Get Excited\n\nConsidering gpt2\\-chatbot’s insane performance, and keeping in mind OpenAI’s recent research and [leaks](https://www.reuters.com/technology/sam-altmans-ouster-openai-was-precipitated-by-letter-board-about-ai-breakthrough-2023-11-22/), we might have a pretty nice idea by now of what on Earth this thing is.\n\nWhat we know for sure is that we are soon going to be faced with a completely different beast, one that will take AI’s impact to the next level.\n\n* *Have we finally reached the milestone for LLMs to go beyond human\\-level performance as we did with AlphaGo?*\n* *Is the age of long inference, aka the conquest of System 2 thinking by AI, upon us?*\n\nProbably not. However, it’s hard not to feel highly optimistic for the insane developments we are about to witness over the following months.\n\nIn the meantime, I guess we will have to wait to get those answers. But not for long.\n\n\n> On a final note, if you have enjoyed this article, I share similar thoughts in a more comprehensive and simplified manner for free on my [LinkedIn](https://www.linkedin.com/in/ignacio-de-gregorio-noblejas/).\n\n\n> If preferable, you can connect with me through [X](https://twitter.com/TheTechOasis1).\n\n\n> Looking forward to connecting with you.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/overcoming-llm-challenges-in-healthcare-practical-strategies-for-development-in-production-04c617954b9a","frontmatter":{"title":"Overcoming LLM Challenges in Healthcare: Practical Strategies for Development in Production","meta_title":"Overcoming LLM Challenges in Healthcare: Practical Strategies for Development in Production","description":"An article on the most common LLM development challenges I’ve encountered, effective mitigation strategies, and a career-defining interview…","date":"2024-11-08T00:20:35.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Vak28ygruWKySsH0doGoYg.png","categories":["Health","Generative AI","Machine Learning"],"author":"Rifx.Online","tags":["LLMs","healthcare","hallucinations","validation","monitoring"],"draft":false,"slug":"blog/overcoming-llm-challenges-in-healthcare-practical-strategies-for-development-in-production-04c617954b9a"},"content":"\n\n### Generative AI\n\n\n\n\n\n### An article on the most common LLM development challenges I’ve encountered, effective mitigation strategies, and a career\\-defining interview mistake\n\n\n## Introduction\n\nI’ve always been the type to dive deep into a subject and specialize to obsession. When I graduated from my master’s in data science, the obsession I had was with computer vision; specifically, computer vision to apply towards neuroscience or mental health applications. I was set on becoming a “computer vision engineer” (but “machine learning engineer” would be okay too) in the mental health field, despite my mentors urging me to broaden my scope and get my foot in the door. I silenced my own wary voices, convinced that the right team would recognize my “expertise”.\n\n\n\nLuckily, my theory seemed to work; I landed interviews with several mental health companies. But then came one of my biggest interview mistakes. In the final round for my top choice — a company I loved — I made an error that still makes me internally cringe when I reflect. The role was NLP\\-focused, working with text data, but I couldn’t help expressing my interest in imaging data. *Cries in recollection.* I vividly recall the interviewer’s expression transforming from one of excitement to one of concern the moment I asked about imaging data availability, as I was still drawn to computer vision. Later that day, I received a polite rejection: they loved my passion but needed someone fully committed to NLP.\n\nIronically, I soon joined another mental health company and shifted fully to NLP work, creating anxiety and depression symptom detectors that improved clinical care and developing recommendation systems that boosted content discoverability by 12%. Fast\\-forward a few years, and I’m now the NLP/LLM data scientist on my team, with 6 information extraction tasks, 5 classification tasks, and 5 conditional summarization tasks deployed across 15\\+ hospitals and five clients.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*-VOHDQd88fCyRqoY9bR3hQ.png)\n\nA couple of weeks ago, I was asked to present “LLM development 101” to my larger data team. Initially, imposter syndrome crept in — *what could I share for 45 minutes on LLM development?* But as I created my slides, I realized how much I had to say and grew excited about sharing the depth of knowledge I’ve learned. This excitement led to the article you’re reading right now. In this article, I’ll walk through some common challenges I’ve encountered with LLMs in production and the strategies that have helped me solve them.\n\n\n## 1\\. Output Format Errors\n\nThis is surprisingly probably the most frequent issue I encounter. Output format reliability can vary significantly depending on the model I’m working with. For example, GPT\\-4 Turbo generally provides consistent JSON outputs, but GPT\\-4o tends to be less reliable in this regard. With GPT\\-4o, I’ve encountered everything from lists and strings to incomplete dictionaries when a structured JSON output was explicitly requested. If these format issues aren’t caught and the model isn’t re\\-run, I risk having incomplete data coverage.\n\n\n### Impact of Format Errors\n\nInconsistent output formats can have a significant impact on downstream processes. If the data structure is incorrect, it could lead to failures in subsequent processing steps, skew reporting accuracy, or even result in incomplete insights if left undetected. In high\\-stakes fields like healthcare, where my work applies, incomplete or mis\\-structured data can have real implications, making format consistency essential.\n\n\n### Mitigation\n\nTo handle this, I’ve implemented **format\\-checking logic** that **validates the output structure**. If it’s incorrect, I re\\-run the model until it matches the expected format. Additionally, I use **logging** to capture format\\-related errors. Re\\-running the model, however, comes with trade\\-offs, such as increased latency and higher API costs. I establish thresholds for re\\-running based on the criticality of the data coverage and cost limitations. If re\\-running isn’t feasible, I sometimes apply post\\-processing to “repair” the output structure, though this approach carries its own risks of introducing errors or inconsistencies.\n\nTo illustrate this approach, here’s a sample code snippet that requests patient data in JSON format with specific keys like `\"name\"`, `\"age\"`, and `\"insurance\"`. This code demonstrates a method to verify that the model’s response includes all required fields and adheres to the expected structure. By implementing retry logic, the code aims to ensure data consistency, reducing the risks associated with format errors in critical workflows.\n\n\n```python\ndef get_llm_response(prompt: str, required_keys: Set[str], retries: int = 3) -> Optional[Dict[str, Any]]:\n    \"\"\"\n    Calls the language model to get a response in JSON format. If the response \n    is not in the expected JSON format or lacks required keys, retries the call \n    up to `retries` times.\n    Parameters:\n        prompt (str): The prompt sent to the language model.\n        required_keys (Set[str]): A set of required keys that must be present in the JSON response.\n        retries (int): The maximum number of retries if the output format is invalid.\n    Returns:\n        Optional[Dict[str, Any]]: Parsed JSON response if successful; None if retries are exhausted.\n    \"\"\"\n    \n    for attempt in range(retries):\n        try:\n            response = openai.Completion.create(\n                model=\"gpt-4o\",\n                prompt=prompt,\n                max_tokens=100,\n                temperature=0.7\n            )\n            \n            # Attempt to parse the response as JSON\n            response_text = response.choices[0].text.strip()\n            parsed_response = json.loads(response_text)\n            \n            # Check if parsed_response is in the expected structure and contains required keys\n            if isinstance(parsed_response, dict) and required_keys.issubset(parsed_response.keys()):\n                return parsed_response\n            else:\n                print(f\"Attempt {attempt + 1}: Output format invalid or missing required keys, retrying...\")\n        except (json.JSONDecodeError, KeyError) as e:\n            print(f\"Attempt {attempt + 1}: Error parsing JSON - {str(e)}, retrying...\")\n    print(\"Max retries exceeded: Unable to get valid JSON output with required keys.\")\n    return None\n\n```\n\n## 2\\. Hallucinations\n\nHallucinations happen when the model invents information that sounds plausible but isn’t actually there. For instance, when I’m trying to pull quotes from source text, sometimes the model decides to “get creative” and produces similar\\-sounding but completely fabricated phrases. In fields where accuracy is crucial, like healthcare, small hallucinations can lead to large issues.\n\n\n### Mitigation\n\nI address hallucinations by implementing post\\-processing logic to validate that, for any information extraction tasks, the context pulled matches the source text exactly. To ensure that minor variations don’t lead to missed matches, I standardize the text by stripping punctuation and converting everything to lowercase when comparing the source and retrieved text. Additionally, several other strategies help minimize hallucinations. For instance, **chain\\-of\\-thought prompting**, where the model explains each step of its reasoning, can produce more grounded outputs and reduce the likelihood of inaccurate output. In high\\-stakes applications (such as healthcare use cases), **human\\-in\\-the\\-loop checks** are important as an extra layer of review, helping catch hallucinations that automated processes might miss. Lastly, prompts that emphasize factual accuracy, such as instructing the model to “only use exact phrases from the source,” can guide the model toward more precise responses.\n\n\n## 3\\. Outdated Information\n\nOutdated information can be challenging to manage, especially in applications where accuracy and timeliness are essential. Sometimes, a model might retrieve information from older sections of a document and surface it as if it’s current. With Retrieval\\-Augmented Generation (RAG), this issue can become even more complex, as RAG retrieves content based solely on relevance rather than timeliness or specific document sections. The absence of section labels or timestamps means RAG may pull from parts of a document that seem relevant without discerning if they’re outdated, which risks mixing older and current information. Another challenge with using a vector database is that if we store entire documents, we can’t easily remove specific sections without clearly defined labels, making it hard to filter out irrelevant information effectively.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*k9btdwyCCAb9qp92gB0PwA.png)\n\n\n### Mitigation\n\nTo address this, I specify “current” or “most recent” data directly in the prompt and use preprocessing steps to remove any outdated sections before passing data to the model. This extra preprocessing step ensures that only the latest, most relevant information is retained, helping the model focus on providing timely and accurate responses. This step not only ensures more accurate outputs, but it also reduces the cost of the call. By implementing these filters in advance, I can maintain consistency and relevance in the model’s outputs.\n\n\n## 4\\. Over\\-Reliance and Ethics\n\nAs much as I would love for the work I do to be used and useful, my biggest fear is that users would trust the model predictions a bit too much — especially in the healthcare space, where generative AI is often producing summaries or extracting specific patient details, not just making predictions. Experts may hold differing views on certain definitions, so diversity and dialogue is important to reach a consensus. Over\\-reliance on these predictions, could lead care teams to limit these conversations and overlook errors they might otherwise examine more closely.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*6-0mq8Svxh8ATuyT)\n\n\n### Mitigation\n\nI prioritize educating the team on the model’s limitations, including its tendency for errors, and encourage them to see AI as a complement to human expertise. In healthcare, where nuance is critical, human\\-in\\-the\\-loop oversight is essential for high\\-impact cases, allowing experts to review AI outputs and reduce risks from over\\-reliance. This collaborative approach allows AI to amplify expert insights, maintaining the reliability and ethical integrity that high\\-stakes applications demand.\n\n\n## 5\\. Rapid Model Deprecation\n\nWith the rapid pace of development in AI, model and API versions are updated frequently, and it’s common for versions to be deprecated faster than expected. If you’ve ever had a workflow break unexpectedly because a model version was retired, you’ll know how disruptive this can be. This has happened several times in the past year, requiring us to quickly re\\-do analyses to ensure the newer model versions still perform as expected.\n\n\n### Mitigation\n\nMake it a priority to do regular check\\-ins to monitor model versions and stay ahead of deprecation warnings. This proactive approach would enable us to plan transitions in advance, saving the last\\-minute scramble. While it’s a small step, it makes a significant difference in maintaining smooth operations.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GK08JY3dcRUS4r6Z0x6EmA.png)\n\n\n## 6\\. Rate Limiting with APIs\n\nAPI rate limits are a subtle but significant challenge, especially when working with high volumes of requests. Hitting a rate cap can create delays, slow down real\\-time workflows, or even halt entire processes. In cases where we’re processing time\\-sensitive data, reaching the limit can be highly disruptive, as workflows come to an unexpected stop. This is especially problematic in healthcare settings, where timing can directly impact operations and patient care.\n\n\n### Mitigation\n\nTo mitigate this, we’ve implemented a proactive approach by tracking API usage patterns to identify peak times and reduce non\\-essential calls. By staggering requests and batching calls, I can distribute the load more evenly and avoid exceeding limits. In situations where demand is high and rate limits are consistently reached, requesting additional quota from the provider can offer a practical solution. Balancing usage has been essential, and understanding our peak times and usage patterns ahead of time has proven crucial for maintaining a stable, uninterrupted workflow.\n\n\n## Concluding Remarks\n\nThese are just six of the common issues I’ve faced while working with LLMs. I didn’t expect to find myself here, but taking a step back to reflect, I realize how much expertise I’ve developed in this space — and I’m incredibly excited to continue to share these learnings in upcoming articles. I’d love to hear from others about the challenges they’ve encountered and the mitigation strategies or workarounds they’ve found effective, whether related to these issues or new ones entirely. I hope these insights are helpful and spark further conversation around best practices in this quickly evolving field (where model versions and API versions deprecate a little too quickly).\n\n\n"},{"lang":"en","group":"blog","slug":"blog/qwen-new-release-the-king-of-coder-is-qwen2-5-coder-32b-8b96d442b280","frontmatter":{"title":"Qwen New Release: The King of Coder is Qwen2.5 coder 32B!","meta_title":"Qwen New Release: The King of Coder is Qwen2.5 coder 32B!","description":"The article introduces Qwen2.5-Coder-32B-Instruct, a new AI coding model that outperforms existing open-source models, including GPT-4o, according to benchmark scores. It operates efficiently on a single GPU, achieving 32 tokens per second. The model is available under the Apache 2.0 license and is supported by Ollama for easy deployment. Smaller models are also available for users with limited computing resources. The integration of Qwen models with OpenWebUI enhances usability for daily tasks.","date":"2024-11-14T03:32:00.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*OzrZMolY75t_cdux5UGtIg.png","categories":["Programming","Technology","Machine Learning"],"author":"Rifx.Online","tags":["Qwen2.5","Coder","32B","Instruct","GPU"],"draft":false,"slug":"blog/qwen-new-release-the-king-of-coder-is-qwen2-5-coder-32b-8b96d442b280"},"content":"\nGreat new everyone! Meet Qwen2\\.5\\-Coder\\-32B\\-Instruct: the latest AI model that’s taking the code world by storm!\n\n\n\nMost of these models are released under the Apache 2\\.0 license .Benchmark scores are through the roof:\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*aHeNvfOvcpME0qzy6EQexQ.jpeg)\n\nAs we can see, it’s the best among open source models and even beats the GPT\\-4o.\n\nOllama has already provided its support for several model series.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*rV1xrpRXUjTFFoKwSsfeOg.png)\n\nSo it’s easy to run it.\n\n```python\nollama run qwen2.5-coder:32b\n```\n\nThe performance of the 32B (Q4 format) on one single GPU 3090 can be found from following screenshot:\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MVQ0srQhRxX4Ifo3IqU6og.png)\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*jtH3ixeeOQGfyDzmO4Ni3A.png)\n\nIt’s **32 tokens/s**! super fast. I am very happy and impressed.\n\nBesides the 32B model, the smaller models have shown impressive performance in terms of model size. If you do not have enough computing power, try some of the smaller models. For example, I tried the 14B model on the new Mac Mini with M4 processor and 16GB RAM.\n\nIn addition to benchmark scores, the cursor can now be integrated with the latest Qwen models, including Qwen 2\\.5\\-Coder\\-32B\\-Instruct \\& OpenWebUI.\n\nBelow is a screenshot of using Qwen 2\\.5\\-Coder\\-32B\\-Instruct(Ollama) with OpenWebUI:\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*q-Pq3snVhkBs3e_Oxj4_Xw.png)\n\nI can’t wait to use it for my day to day works!\n\n\n"},{"lang":"en","group":"blog","slug":"blog/qwen2-5-1-5b-the-future-of-mobile-ai-6bd5f29bbc84","frontmatter":{"title":"Qwen2.5 1.5b: the future of Mobile AI?","meta_title":"Qwen2.5 1.5b: the future of Mobile AI?","description":"Local Testing and Evaluation of Alibaba Cloud’s Latest LLM. With llama-cpp-python and a DIY prompt catalog.","date":"2024-10-30T12:57:39.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*awb56jkdXobA-Ip6d-QHRA.png","categories":["Natural Language Processing","Programming","Technology/Web"],"author":"Rifx.Online","tags":["Qwen2.5","NLP","summarization","retrieval","prompts"],"draft":false,"slug":"blog/qwen2-5-1-5b-the-future-of-mobile-ai-6bd5f29bbc84"},"content":"\n### Local Testing and Evaluation of Alibaba Cloud’s Latest LLM. With llama\\-cpp\\-python and a DIY prompt catalog.\n\n\n\nIn part one we explored together the innovations from Alibaba Cloud’s team with the release of the Qwen2\\.5 models family.\n\nIn Generative AI benchmarks are now the main *oracle*: the validity of a new LLM needs to pass several verdicts. The more benchmark records you break, the better you are.\n\nIt is the way to win the SOTA race\n\nWell, I disagree. Even though for the AI advancement we need milestones and better performances, still the user experience and the personal point of view cannot be just put aside as irrelevant.\n\nI believe that exploring some frequently used NLP tasks, and putting aside the chat experience, we must focus on the quality of the replies. And we are the only benchmark required. Our user experience is the best indicator to understand if a model is good or not. The model must be reliable enough to be used in an automated workflow.\n\nBy the way, I already run what I decided to call [RBYF — Revised Benchmarks with You as a Feedback](https://open.substack.com/pub/thepoorgpuguy/p/rbyf-is-here-revised-benchmarks-with?r=i78xo&utm_campaign=post&utm_medium=web) on the claimed amazing Llama3\\.2–1B\\-instruct… and Qwen2\\.5–1\\.5b is so much better!\n\nSo in this article, as promised, we will verify with our own eyes how good is this model for every day use.\n\nBack to us… Let’s get started!\n\n## Requirements\n\nHere we are going to build a minimal text interface to be able to run the model, test different tasks and wait for user feedback to evaluate it.\n\nThe requirements are minimal, but I suggest you to create a new project directory and a virtual environment.\n\nCreate a `venv` (python 3\\.11\\+ is required): I tested it on my Mini\\-PC running Windows 11\\.\n\n```python\n## create the virtual environment\npython -m venv venv\n## activate the venv\nvenv\\Scripts\\activate\n## Install the dependencies \npip install llama-cpp-python==0.3.0 tiktoken\n```\n\nWe need to download the GGUF file from the official qwen repository on Hugging Face [https://huggingface.co/Qwen/Qwen2\\.5\\-1\\.5B\\-Instruct\\-GGUF](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF): I used the `qwen2.5-1.5b-instruct-q5_k_m.gguf` version.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*YtQJb_xyq_xcF40yRWPcZA.png)\n\nWe are all set!\n\nNote: if you want to add a different Backend support of a GPU accelerator, you can follow [the instructions in the repo](https://github.com/abetlen/llama-cpp-python#supported-backends). I used, for example, the Vulkan support so before the pip install I added the environment variable\n\n```python\n## Vulkan support - for Windows\n$env:CMAKE_ARGS = \"-DGGML_VULKAN=on\"\n```\n\n## The Code — a main app and a library\n\nTo keep the code at minimum, I decided to extend some functionalities using an external library. Well, it is a Do It Yourself library, so there are no secrets here.\n\nYou can find all the details in my article here:\n\nAnd to speed it up you can directly [download the file from here](https://github.com/fabiomatricardi/YouAreTheBenchmark/raw/main/QWEN2.5-1.5B/promptLibv2Qwen.py): it contains a version 2 of the `promptLib` discussed in the mentioned above article (and it is called `promptLibv2Qwen.py`, with few fine tuning of the prompt specifically tailored for the `Qwen2.5-1.5B-instruct` model.\n\nSave the file in the main directory, and create a new file called `main.py`\n\n```python\n## Chat with an intelligent assistant in your terminal  \n## MODEL: https://huggingface.co/Qwen\n## qwen2.5-1.5b-instruct-q5_k_m.gguf\nimport sys\nfrom time import sleep\nimport warnings\nwarnings.filterwarnings(action='ignore')\nimport datetime\nfrom promptLibv2Qwen import countTokens, writehistory, createCatalog\nfrom promptLibv2Qwen import genRANstring, createStats\nimport argparse\n### PREPARING FINAL DATASET\npd_id = []\npd_task = []\npd_vote = []\npd_remarks = []\n####################Add GPU argument in the parser###################################\nparser = argparse.ArgumentParser()\nparser.add_argument(\"-g\", \"--gpu\", type=int, default=0,nargs='?',\n                    help=\"The number of layers to load on GPU\")\nargs = parser.parse_args()\nif args.gpu == None:\n   ngpu_layers = 0 \nelse:\n    ngpu_layers = args.gpu\nprint(f'Selected GPU: offloading {ngpu_layers} layers...')   \n####################INITIALIZE THE MODEL###################################\nstops = ['<!im_end|>']\ntasks = createCatalog()\nmodelname = 'qwen2.5-1.5b-instruct-q5_k_m.gguf'\n## create THE LOG FILE \ncoded5 = genRANstring(5)\nlogfile = f'logs/Qwen2.5-1.5B-it_CPP_{coded5}_log.txt'\ncsvfile = f'logs/Qwen2.5-1.5B-it_CPP_{coded5}.csv'\nlogfilename = logfile\n#Write in the history the first 2 sessions\nwritehistory(logfilename,f'{str(datetime.datetime.now())}\\n\\nYour own LocalGPT with 💻 {modelname}\\n---\\n🧠🫡: You are a helpful assistant.')  \nwritehistory(logfilename,f'💻: How can I assist you today in writing?')\n```\n\nHere we are only doing preparations: we import the libraries, including our own personal `promptLibv2Qwen` and also `argparse`. I wanted to try something new: [argparse](https://realpython.com/command-line-interfaces-python-argparse/) is a python library intended for terminal python program where you are reading multiple arguments from the command line.\n\nIn this case here we have only one argument (and no parameters) with th flag `-g` or even `--gpu`. When you run the python code with this argument we will set the number of GPU layers to the maximum (but you can change it yourself).\n\nThen we set some global variables, used across the entire code: the tasks, our prompt collection, the stop words and the log filename.\n\n> NOTE: all the logs are saved into a subdirectory called `logs`… so make sure to create one.\n\nWe are also preparing all the relevant information to store them into a dataset and then save it at the end int a CSV file (for easily creating a performance matrix)\n\n```python\n### PREPARING FINAL DATASET\npd_id = []\npd_task = []\npd_vote = []\npd_remarks = []\n```\n\nWe then load the model into RAM (no GPU) or the VRAM (with GPU) using Llama\\-CPP\\-python.\n\n```python\n## LOAD THE MODEL\nprint(\"\\033[95;3;6m\")\nprint(\"1. Waiting 10 seconds for the API to load...\")\nfrom llama_cpp import Llama\nllm = Llama(\n            model_path='models/qwen2.5-1.5b-instruct-q5_k_m.gguf',\n            n_gpu_layers=ngpu_layers,\n            temperature=0.1,\n            n_ctx=8192,\n            max_tokens=1500,\n            repeat_penalty=1.178,\n            stop=stops,\n            verbose=False,\n            )\nprint(f\"2. Model {modelname} loaded with LlamaCPP...\")\nprint(\"\\033[0m\")  #reset all\nhistory = []\nprint(\"\\033[92;1m\")\nprint(f'📝Logfile: {logfilename}')\n```\n\nBy the way, you can find all the code in my GitHub Repository:\n\nThe next one is a one\\-shot warm up inference: the model neural network is going to be activated for the first time, so think about it like a warm\\-up lap.\n\nDon’t be scared, I will explain the code\n\n```python\n##################### ALIGNMENT FIRST GENERATION ##############################################\nquestion = 'Explain the plot of Cinderella in a sentence.'\ntest = [\n    {\"role\": \"user\", \"content\": question}\n]\nprint('Question:', question)\nstart = datetime.datetime.now()\nprint(\"💻 > \", end=\"\", flush=True)\nfull_response = \"\"\nfisrtround = 0\nfor chunk in llm.create_chat_completion(\n    messages=test,\n    temperature=0.25,\n    repeat_penalty= 1.31,\n    stop=stops,\n    max_tokens=1500,\n    stream=True,):\n    try:\n        if chunk[\"choices\"][0][\"delta\"][\"content\"]:\n            if fisrtround==0:\n                print(chunk[\"choices\"][0][\"delta\"][\"content\"], end=\"\", flush=True)\n                full_response += chunk[\"choices\"][0][\"delta\"][\"content\"]\n                ttftoken = datetime.datetime.now() - start  \n                fisrtround = 1\n            else:\n                print(chunk[\"choices\"][0][\"delta\"][\"content\"], end=\"\", flush=True)\n                full_response += chunk[\"choices\"][0][\"delta\"][\"content\"]                            \n    except:\n        pass      \ndelta = datetime.datetime.now() - start\noutput = full_response\nprint('')\nprint(\"\\033[91;1m\")\nrating = input('Rate from 0 (BAD) to 5 (VERY GOOD) the quality of generation> ')\nprint(\"\\033[92;1m\")\nstats = createStats(delta,question,output,rating,logfilename,'Alignment Generation',ttftoken)\nprint(stats)\nwritehistory(logfilename,f'''👨‍💻 . {question}\n💻 > {output}\n{stats}\n''')\n```\n\nWe set the first user question and put into a well known chat format dictionary. Then we start our timer (useful for speed, token counts etc…).\n\nWe call the inference with the method `create_chat_completion()` that allows us to accept the prompts in chat format and stream the output one token at the time.\n\nBecause of the first reply from the model does not contain any output tokens (but only statistics) we use the try/except statement. Furthermore, since I want to know when the first token is generated, we raise a flag and stop temporary our time count saving the information inside the `ttftoken` variable.\n\nAt the end of the streaming we count the delta time from start, and wait the user to provide his/her personal feedback on the generated output: giving a mark from 0 to 5 and adding comments related to the compliance with the instruction prompt and user intent.\n\nWe use our internal library called `createStats()` to print all the statistics of the generation, and save them in our log file. The output of the function will be something like this:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8znYCqpisviXvYgrzjWF5w.png)\n\n## Prompt catalog — what we want to test\n\nI wrote here about my habit. I have a catalog of prompts that covers many of the main language tasks used in chat\\-bots, like summarization, short summarization, casual chat, RAG, truthful RAG and so on.\n\nThe idea is to be able to load the model in 5 minutes, and start evaluating each task. At the end of every generation the user is prompted to give a mark (a score from 0 to 5\\) and leave any comments if required.\n\nThis is crucial: not all the models are alike, and small/big adjustments to the wording in the prompts are always required.\n\nSo back to the code… Because the previous one was only a warm\\-up, now it will start the real while loop, iterating over the entire prompt catalog. See the workflow here below…\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*EL0Q97Du6HwtcYQZ.png)\n\nThere are only few changes in the code, and I will point them out, so bear with me.\n\n```python\n############################# AUTOMATIC PROMPTING EVALUATION  11 TURNS #################################\nid =1\nfor items in tasks:\n    fisrtround = 0\n    task = items[\"task\"]\n    prompt = items[\"prompt\"]\n    test = []\n    print(f'NLP TAKS>>> {task}')\n    print(\"\\033[91;1m\")  #red\n    print(prompt)\n    test.append({\"role\": \"user\", \"content\": prompt})\n    print(\"\\033[92;1m\")\n    full_response = \"\"\n    start = datetime.datetime.now()\n    print(\"💻 > \", end=\"\", flush=True)\n    for chunk in llm.create_chat_completion(\n        messages=test,\n        temperature=0.15,\n        repeat_penalty= 1.31,\n        stop=stops,\n        max_tokens=1500,\n        stream=True,):\n        try:\n            if chunk[\"choices\"][0][\"delta\"][\"content\"]:\n                if fisrtround==0:\n                    print(chunk[\"choices\"][0][\"delta\"][\"content\"], end=\"\", flush=True)\n                    full_response += chunk[\"choices\"][0][\"delta\"][\"content\"]\n                    ttftoken = datetime.datetime.now() - start  \n                    fisrtround = 1\n                else:\n                    print(chunk[\"choices\"][0][\"delta\"][\"content\"], end=\"\", flush=True)\n                    full_response += chunk[\"choices\"][0][\"delta\"][\"content\"]                            \n        except:\n            pass      \n    delta = datetime.datetime.now() - start\n    print('')\n    print(\"\\033[91;1m\")\n    rating = input('Rate from 0 (BAD) to 5 (VERY GOOD) the quality of generation> ')\n    print(\"\\033[92;1m\")\n    stats = createStats(delta,prompt,full_response,rating,logfilename,task,ttftoken)\n    print(stats)\n    writehistory(logfilename,f'''👨‍💻 > {prompt}\n💻 > {full_response}\n{stats}\n''')\n    pd_id.append(id)\n    pd_task.append(task)\n    pd_vote.append(rating[:2])\n    pd_remarks.append(rating[2:])\n    id += 1\n## create dataframe and save to csv\nzipped = list(zip(pd_id,pd_task,pd_vote,pd_remarks))\nimport pandas as pdd\ndf = pdd.DataFrame(zipped, columns=['#', 'TASK', 'VOTE','REMARKS'])\n#saving the DataFrame as a CSV file \ndf_csv_data = df.to_csv(csvfile, index = False, encoding='utf-8') \nprint('\\nCSV String:\\n', df_csv_data)  \n```\n\nThe main changes are only in the first lines:\n\n```python\nfor items in tasks:\n    fisrtround = 0\n    task = items[\"task\"]\n    prompt = items[\"prompt\"]\n```\n\nIf you read the article about the `promptLib` you shouldn’t be surprised: but if you are new, here we are iterating over a list of dictionaries with the following structure:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*rGcKJWNzSUrcu4wi.png)\n\nSo for each items in the catalog (means a pair of tasks and prompts) we extract the task description and the prompt for the task.\n\n```python\ntest.append({\"role\": \"user\", \"content\": prompt})\n```\n\nThen we create the chat template message in a temporary list called `test` and pass it to the `create_chat_template()` method for generation.\n\nEverything else is the same.\n\nSave the file, and with the `venv` activated run:\n\n```python\npython main.py\n## if you are using the GPU python main.py -g\n```\n\nThis will get you something like the below example…\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MhhQu4lLjtU__Wjf0dSWBg.gif)\n\nNote that at the end of the entire Prompt Catalog a *csv* file is created with the summary of all the tasks!\n\n## Test Overview\n\nI run them with several Small Language Models, from [Qwen2–1\\.5B\\-instruct](https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF), to [Gemma2–2B\\-instruct](https://huggingface.co/bartowski/gemma-2-2b-it-GGUF), with [Llama3\\.2–1B\\-instruct](https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF) and finally the new [Qwen2\\.5–1\\.5B\\-instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF).\n\nWhile I was [quite disappointed by Llama3\\.2–1B\\-instruct](https://generativeai.pub/llama3-2-1b-instruct-is-ok-but-not-good-enough-28f88046b63e), I have been amazed by the good job done by the new [Qwen2\\.5–1\\.5B\\-instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF).\n\nAt the end of every generation the user is asked to evaluate the results with a mark from 0 to 5\\. **In this case the user is me…**\n\nThis kind of qualitative analysis is indeed poor, so every mark does have a description, and the user can add comments (“some wrong information”, “maybe better change the wording in the prompt” )\n\nHere the qualitative matrix with a description\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*eBdPfZtfr99MsvLh6tt42w.png)\n\n## The good and the bad — details\n\nSummarization was amazing. Listing the main topics over a log text was also very good.\n\nThe RAG tasks were quite fast (even on my mini\\-PC) and truthful RAG (ask questions out of the context) was on spot.\n\nSo really good.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*DuV3LJep_PuDqiCcAMb6Cg.png)\n\nThere were also few shortcomings: even with a temperature of only `0.15` I got some made up information in the summarize in two sentences task. And this is not good.\n\n> I expect this can be fixed with a little prompt tuning, or with a temperature at `0`.\n\nAnother fact is that the creative writing was quite bad: for the test I used some fixed Generation Parameters.\n\n```python\n        temperature=0.15,\n        repeat_penalty= 1.31,\n```\n\nProbably for creative writing, with Qwen2\\.5–1\\.5B\\-instruct, we should use an higher `repeat_penalty` and also a higher `temperature`.\n\nAs a side note I have to say that also the reflection prompt was not that bad at all! The opening/closure of the tags was not maintained (so not easy to put this into a pipeline or workflow) but the overall flow of the generation and the “Chain of Thought” reasoning process was fairly good.\n\n```python\n<thinking>\nThe importance of using Generative Artificial Intelligence (AI) in \nteaching cannot be overstated, as this technology has immense potential \nnot only within current educational practices but also towards shaping a \nmore innovative approach to lifelong learning.\n</thinking>\n\n**Chain Of Thought:**\n1. **Understanding the Impact**: AI can automate repetitive tasks and provide \npersonalized feedback for students based on their performance data \n(e.g., through chatbots or adaptive assessments).\n2. **Enhancing Learning Experience**: By integrating Generative AI, \nteachers could create more engaging lessons that cater to diverse learning \nstyles.\n3. **Improving Student Outcomes**: Personalized instruction can lead to \nbetter understanding and retention of knowledge for students.\n\n**Reflection:**\n- The integration of generational technology in education is crucial as \nit aligns with the evolving needs of learners who are increasingly digital \nnatives, requiring a pedagogical approach that incorporates these technologies.\n</thinking>\n\n<output>\nTeachers should learn how Generative AI can be used to enhance their teaching \nmethods and improve student outcomes. This includes understanding its \ncapabilities for automating tasks like grading or creating personalized \nlearning paths.\n\n**Learning Path:**\n1. **Foundation Courses in Education Technology**: Start with courses \nthat provide an overview of educational technology, including the basics \nof digital tools.\n2. **AI Fundamentals Course (Coursera)** - Learn about AI concepts and \nhow they can be applied to education through Coursera's free online \ncourse on Artificial Intelligence for Educators.\n\n3. **Specialized Courses in Generative Learning**: Enroll in courses \nthat focus specifically on generational learning, such as \"Generative Design\" \nor similar specialized programs offered by universities like MIT.\n4. **Professional Development Workshops and Conferences** - Attend \nworkshops focused on integrating AI into teaching practices at local \neducational conferences.\n\n5. **Online Communities for Educators (e.g., Edmodo)**: Join online \ncommunities where educators discuss the use of generative technology \nin education, share resources, or ask questions about implementation.\n6. **Certification Programs**: Consider obtaining certifications that \nrecognize your knowledge and skills related to AI integration into teaching \npractices through organizations like Google's \"AI for Educators\" program.\n\nBy following this learning path, teachers can not only enhance their own \nprofessional development but also contribute positively towards the future \nof education by incorporating generative technology effectively.\n</output>\n```\n\nI think that for the size of this Small Language Model, the result is not bad at all!\n\n## Comparing Qwen2\\.5 with Llama3\\.2\n\nEven though this is only coming from my personal evaluation, I want to share it with you.\n\nBoth models are designed to be used on mobile devices, but the performance is quite different. Look here below:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*T6vLgvOKdkotlV1K5x6-QQ.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*DuV3LJep_PuDqiCcAMb6Cg.png)\n\nFirst of all, the overall score difference is huge (41 for Llama3\\.2, 57 for Qwen2\\.5\\).\n\nSecondly if you think about what you may ask on a mobile device, in terms of language tasks, is to have a smooth chatting experience (task 4\\), good summarization (tasks five to 7\\) and some creative writing (task 11 and 13\\).\n\nIn terms of speed, running the model only on CPU, with a very limited mini\\-PC, **I got an average inference speed of 14 t/s.**\n\n## Conclusions\n\nIn the past three months since Qwen2’s release, numerous developers have built new models on the Qwen2 language models, providing valuable feedback to the entire community, but also to Alibaba Cloud.\n\n> During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2\\.5\n\nTheir claims come with facts about the new family of models:\n\n* Dense, **easy\\-to\\-use**, decoder\\-only language models, available in 0\\.5B, 1\\.5B, 3B, 7B, 14B, 32B, and 72B sizes, and base and instruct variants.\n* Pretrained on our latest large\\-scale dataset, encompassing up to 18T tokens.\n* Significant improvements in **instruction following**\n* More **resilient to the diversity of system prompts**, enhancing role\\-play implementation and condition\\-setting for chatbots.\n* **Context length support up to 128K** tokens and can generate up to 8K tokens.\n* Multilingual support for over 29 languages\n\nIn my extensive (but certainly limited to one shot prompts and on few NLP tasks) tests I could see with my very own eyes that the claims were based on a good quality training dataset and curated fine tuning.\n\nThis model can perform extremely good on mobile devices!\n\n\n"},{"lang":"en","group":"blog","slug":"blog/qwen2-5-coder-32b-instruct-a-best-coding-model-a-complete-step-by-step-guide-and-performance-b8a33ec2547f","frontmatter":{"title":"Qwen2.5-Coder 32B Instruct: A Best Coding Model-A Complete Step-by-Step Guide and Performance…","meta_title":"Qwen2.5-Coder 32B Instruct: A Best Coding Model-A Complete Step-by-Step Guide and Performance…","description":"The article presents a comprehensive guide to the **Qwen2.5-Coder-32B-Instruct** model, highlighting its advanced coding capabilities and performance metrics. It emphasizes the model's powerful code generation, repair, and reasoning skills, which rival those of established models like GPT-4o. The guide outlines the model's diverse range, supporting over 40 programming languages and various sizes, catering to different developer needs. It includes practical examples for installation and usage, demonstrating the model’s effectiveness in real-world applications such as coding assistants and educational tools. The article concludes by affirming the model's role in enhancing AI-powered coding tasks.","date":"2024-11-14T03:29:09.000Z","image":"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*zjZmLCEX5URAc1wxTGnBRQ.png","categories":["Programming","Machine Learning","Generative AI"],"author":"Rifx.Online","tags":["Qwen2.5","Coder","programming","languages","repair"],"draft":false,"slug":"blog/qwen2-5-coder-32b-instruct-a-best-coding-model-a-complete-step-by-step-guide-and-performance-b8a33ec2547f"},"content":"\n# Qwen2.5-Coder 32B Instruct: A Best Coding Model-A Complete Step-by-Step Guide and Performance Evaluation for Developers\n\n\nPhoto By Author\n\n# Introduction\n\nIn the ever-evolving landscape of AI-powered programming tools, large language models (LLMs) have dramatically transformed the way developers write, debug, and optimize code. Today, we are thrilled to explore the **Qwen2.5-Coder** series, an open-source marvel that promises to set new standards in the realm of code generation and AI coding assistants. The latest release in this family, **Qwen2.5-Coder-32B-Instruct**, has redefined the state-of-the-art (SOTA) performance in open-source coding models, rivaling the capabilities of established models like **GPT-4o**. Let’s dive deeper into what makes Qwen2.5-Coder so “Powerful”, “Diverse”, and “Practical”.\n\nIn this comprehensive guide, we’ll explore the core capabilities of the **Qwen2.5-Coder-32B** model. We’ll demonstrate how to use it with the `transformers` library, test its coding abilities and highlight its practical applications.\n\n# Why Qwen2.5-Coder?\n\n## Key Highlights\n\n1. **Powerful**: The flagship **Qwen2.5-Coder-32B** model matches the coding prowess of GPT-4 on major coding benchmarks, while also excelling in general and mathematical skills.\n2. **Diverse**: The release covers multiple model sizes (0.5B, 1.5B, 3B, 7B, 14B, 32B), offering flexibility for different resource constraints.\n3. **Practical**: Designed for real-world applications, including code assistants and artifact generation. The models are licensed under **Apache 2.0**, ensuring freedom to use and modify for both commercial and research purposes.\n\n# The Qwen2.5-Coder Series: A Game-Changer for Open Code LLMs\n\nThe **Qwen2.5-Coder** series is dedicated to pushing the boundaries of open-source code generation. With a focus on flexibility and scalability, this release includes models in a range of sizes: **0.5B, 1.5B, 3B, 7B, 14B,** and the flagship **32B** version. These models cater to various developer needs, from lightweight, resource-efficient models to high-capacity, feature-rich models suitable for demanding applications.\n\n## 1. Powerful: Setting New Benchmarks in Code Generation\n\nQwen2.5-Coder-32B-Instruct stands as the flagship model, boasting a range of capabilities that have earned it the title of the **current SOTA open-source code model**. It excels in:\n\n* **Code Generation**: On popular benchmarks like **EvalPlus, LiveCodeBench,** and **BigCodeBench**, it matches the performance of GPT-4o, offering precise code generation across a multitude of scenarios.\n* **Code Repair**: Fixing broken or inefficient code is crucial in software development. On the **Aider benchmark**, which tests code repair skills, Qwen2.5-Coder-32B-Instruct scored an impressive **73.7**, rivaling GPT-4o’s prowess.\n* **Code Reasoning**: The ability to understand and reason through code execution paths is critical for debugging and optimizing complex software. This model’s capabilities extend beyond mere generation — it excels at **predicting inputs and outputs**, making it an invaluable tool for software engineers.\n\n![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*-g1ZGa0p2kKsQK4iD7q7cg.png)\nSource: (https://qwenlm.github.io/blog/qwen2.5-coder-family/)\n\n## 2. Diverse: Supporting Multiple Programming Languages and Rich Model Sizes\n\nThe versatility of Qwen2.5-Coder is evident in its support for over **40 programming languages**, including niche languages like **Haskell** and **Racket**. This broad support is backed by meticulous data cleaning and balanced training, ensuring the model performs optimally across different coding environments.\n\n* **Multi-language Code Repair**: Its proficiency extends to code repair in unfamiliar languages, which can significantly reduce the learning curve for developers exploring new technologies.\n* **Model Size Flexibility**: The Qwen2.5-Coder series offers models across six sizes, ensuring developers with varying resource constraints can find a model suited to their needs. The **scaling law** philosophy that underpins these models means that performance correlates positively with model size, giving developers the flexibility to choose the right balance between performance and computational resources.\n\n# Performance Insights: Evaluating Qwen2.5-Coder Models\n\n## 1. Instruct vs. Base Models\n\nQwen2.5-Coder is available in both **Base** and **Instruct** versions:\n\n* **Base models** are designed for developers who want a raw model to fine-tune for their specific applications.\n* **Instruct models** come pre-aligned and are optimized for interactive and conversational use cases, making them ideal for chat-based code assistants.\n\n## 2. Benchmark Comparisons: Leading the Pack\n\nAcross various core benchmarks:\n\n* **MBPP-3shot** was chosen to evaluate Base models, providing a robust metric to gauge their code comprehension and synthesis capabilities.\n* The **LiveCodeBench** questions set was used to evaluate Instruct models, focusing on their adaptability to new and unseen coding problems.\n\nThe results? **Qwen2.5-Coder consistently outperforms other open-source models**, proving that scaling up to larger sizes indeed correlates with better performance.\n\n# Hands-On Guide: Using Qwen2.5-Coder-3B for Code Generation with Transformers\n\nIn this hands-on tutorial, we will demonstrate how to use the **Qwen2.5-Coder-3B** model from the `transformers` library to generate code. This model is part of the **Qwen2.5-Coder series**, which is designed to excel in code generation, repair, and reasoning. By the end of this tutorial, you'll see how to integrate this powerful open-source model into your own projects for a range of code-related tasks.\n\n## Prerequisites\n\nBefore diving into the code, make sure you have the following installed:\n\n```\npip install torch transformers\n```\n\nAdditionally, ensure you have access to a GPU-enabled environment if you want to leverage the model’s performance optimally.\n\n## Step 1: Import Required Libraries\n\nWe’ll start by importing the necessary components from the `transformers` library:\n\n```\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n```\n\nStep 2: Load the Model and Tokenizer\n\nIn this step, we load the ***Qwen2.5-Coder-32B-Instruct*** model and its corresponding tokenizer. The `device_map=\"auto\"` option will automatically allocate the model to your available GPU or CPU.\n\n> ***Qwen2.5-Coder has released models in various sizes — 0.5B-Instruct, 1.5B-Instruct, 3B-Instruct, 7B-Instruct, 14B-Instruct, and 32B-Instruct — on Hugging Face. If you want to run them locally, choose the model that best fits your GPU capacity. These models are also available on Ollama, so you can use them within the Ollama environment. If you’re interested in an Ollama tutorial, feel free to let me know in the comments!***\n\n```\nmodel_name = \"Qwen/Qwen2.5-Coder-32B-Instruct\"\n\n# Load the model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n```\n\n## Step 3: Writing a Chat Template Function\n\nThe Qwen2.5-Coder model is designed to handle conversation-like prompts using a chat template. The following helper function sets up the input prompt in a way that aligns with the model’s expectations:\n\n```\ndef generate_response(model, tokenizer, prompt):\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": prompt}\n    ]\n    # Prepare the chat input\n    text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n    # Tokenize and prepare inputs\n    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n    # Generate response\n    generated_ids = model.generate(\n        **model_inputs,\n        max_new_tokens=512\n    )\n    # Remove prompt tokens from output\n    generated_ids = [\n        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n    ]\n  \n    # Decode and return the generated text\n    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n    return response\n```\n\n## Step 4: Testing the Model with Code Generation\n\nLet’s run some examples to see how well the Qwen2.5-Coder-32B model performs in generating Python and Java code. We will test it with three different programming prompts.\n\n## Test 1: Quick Sort Algorithm\n\n**Prompt**: “Write a quick sort algorithm.”\n\n```\nprompt_1 = \"write a quick sort algorithm.\"\nresponse_1 = generate_response(model, tokenizer, prompt_1)\nprint(response_1)\n```\n\nOutput:\n\nCertainly! Here's a simple implementation of the Quick Sort algorithm in Python:\n\n```python\ndef quick_sort(arr):\n    if len(arr) <= 1:\n        return arr\n    pivot = arr[len(arr) // 2]\n    left = [x for x in arr if x < pivot]\n    middle = [x for x in arr if x == pivot]\n    right = [x for x in arr if x > pivot]\n    return quick_sort(left) + middle + quick_sort(right)\n\n# Example usage:\narr = [3, 6, 8, 10, 1, 2, 1]\nprint(quick_sort(arr))\n```\n\nThis code defines a `quick_sort` function that takes an array as input and returns a sorted version of the array using the Quick Sort algorithm. The function works by selecting a pivot element from the array and partitioning the other elements into two sub-arrays, according to whether they are less than or greater than the pivot. The sub-arrays are then recursively sorted. The base case for the recursion is when the array has one or no elements, in which case it is already sorted.\n\nYou can use this code as a starting point and modify it according to your specific needs. Let me know if you have any further questions or if you need help with anything else!<|fim_middle|>\n\n\n## Test 2: Factorial of a Number\n\n**Prompt**: “Write a Python program to find a factorial of a number.”\n\n\nprompt_2 = \"Write a python program to find a factorial of a number.\"\nresponse_2 = generate_response(model, tokenizer, prompt_2)\nprint(response_2)\n\n```\nOutput:\n```\n\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nCertainly! Here's a Python program that calculates the factorial of a number:\n\n```python\ndef factorial(n):\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n - 1)\n\n# Example usage\nnumber = int(input(\"Enter a number: \"))\nresult = factorial(number)\nprint(f\"The factorial of {number} is {result}\")\n```\n\nThis program defines a recursive function `factorial` that calculates the factorial of a given number `n`. It uses the base case `n == 0` to return 1, and for other values of `n`, it recursively calls itself with `n - 1` until it reaches the base case. Finally, the program prompts the user to enter a number and displays the factorial of that number.\n\n```\n## Test 3: Formatting Numbers with Commas\n\n**Prompt**: “Write a Python function named ‘format\\_number’ that takes a non-negative number as its only parameter. Your function should convert the number to a string and add commas as a thousand separators.”\n```\n\nprompt_3 = \"\"\"Write a python function named 'format_number' that takes a non-negative number as its only parameter.\nYour function should convert the number to a string and add commas as a thousand separators.\nFor example, calling format_number(1000000) should return '1,000,000'.\"\"\"\nresponse_3 = generate_response(model, tokenizer, prompt_3)\nprint(response_3)\n\n```\nOutput:\n```\n\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nCertainly! Here's a Python function that accomplishes this task:\n\n```python\ndef format_number(num):\n    # Convert the number to a string\n    num_str = str(num)\n  \n    # Initialize an empty string to store the formatted number\n    formatted_num = ''\n  \n    # Initialize a counter for the number of digits\n    digit_count = 0\n  \n    # Iterate over the digits in reverse order\n    for i in range(len(num_str) - 1, -1, -1):\n        # Add the current digit to the formatted number\n        formatted_num = num_str[i] + formatted_num\n      \n        # Increment the digit count\n        digit_count += 1\n      \n        # If the digit count is a multiple of 3, add a comma\n        if digit_count % 3 == 0 and i != 0:\n            formatted_num = ',' + formatted_num\n  \n    return formatted_num\n```\n\nYou can test the function with the following code:\n\n```python\nprint(format_number(1000000))  # Output: '1,000,000'\nprint(format_number(123456789))  # Output: '123,456,789'\nprint(format_number(1234567890))  # Output: '1,234,567,890'\n```\n\nThis function works by iterating over the digits of the number in reverse order, adding each digit to the formatted number and adding a comma every third digit. The function then returns the formatted number as a string.\n\n```\n## Test 4: Java Implement a Stack Using Arrays\n\n**Prompt**: “Write a Java program, Implement a Stack Using Arrays”\n```\n\nprompt_4 = \"\"\"Write a Java program, Implement a Stack Using Arrays.\"\"\"\nresponse_4 = generate_response(model, tokenizer, prompt_3)\nprint(response_4)\n\n```\nOutput:\n```\n\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nCertainly! Here's a simple Java program that implements a stack using arrays:\n\n```java\npublic class StackUsingArrays {\n    private int[] stackArray;\n    private int top;\n    private int capacity;\n\n    public StackUsingArrays(int size) {\n        stackArray = new int[size];\n        top = -1;\n        capacity = size;\n    }\n\n    public void push(int item) {\n        if (isFull()) {\n            System.out.println(\"Stack is full. Cannot push item: \" + item);\n        } else {\n            top++;\n            stackArray[top] = item;\n            System.out.println(\"Pushed item: \" + item);\n        }\n    }\n\n    public int pop() {\n        if (isEmpty()) {\n            System.out.println(\"Stack is empty. Cannot pop item.\");\n            return -1;\n        } else {\n            int item = stackArray[top];\n            top--;\n            return item;\n        }\n    }\n\n    public boolean isEmpty() {\n        return top == -1;\n    }\n\n    public boolean isFull() {\n        return top == capacity - 1;\n    }\n\n    public static void main(String[] args) {\n        StackUsingArrays stack = new StackUsingArrays(5);\n\n        stack.push(10);\n        stack.push(20);\n        stack.push(30);\n\n        System.out.println(\"Popped item: \" + stack.pop());\n        System.out.println(\"Popped item: \" + stack.pop());\n\n        stack.push(40);\n        stack.push(50);\n\n        System.out.println(\"Popped item: \" + stack.pop());\n        System.out.println(\"Popped item: \" + stack.pop());\n        System.out.println(\"Popped item: \" + stack.pop());\n    }\n}\n```\n\nThis program defines a `StackUsingArrays` class that uses an array to implement a stack. The `push` method adds an item to the top of the stack, and the `pop` method removes and returns the item at the top of the stack. The `isEmpty` and `isFull` methods check if the stack is empty or full, respectively. The `main` method demonstrates how to use the stack by pushing and popping items.\n\n```\n## Test 5: Simple classification algorithm\n\n**Prompt**: “Write a Python code to run a simple classification algorithm using Sklearn”\n```\n\nprompt_5 = \"\"\"Write a Python code to run a simple classification algorithm using Sklearn\"\"\"\nresponse_5 = generate_response(model, tokenizer, prompt_3)\nprint(response_5)\n\n```\nOutput:\n```\n\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nCertainly! Here's a simple example of a classification algorithm using the `sklearn` library in Python:\n\n```python\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n# Generate some sample data\nnp.random.seed(42)\nX = np.random.rand(100, 2)  # 100 samples, 2 features\ny = np.random.choice([0, 1], size=100)  # Binary classification\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create a logistic regression model\nmodel = LogisticRegression()\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Calculate the accuracy of the model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy}\")\n```\n\nThis code generates some sample data, splits it into training and testing sets, trains a logistic regression model, makes predictions on the test set, and calculates the accuracy of the model. Feel free to modify the code to suit your specific needs.\n\n```\nMore details:\n\n[Qwen/Qwen2.5-Coder-32B-Instruct · Hugging FaceWe're on a journey to advance and democratize artificial intelligence through open source and open science.huggingface.co](https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct)\n\n[qwen2.5-coder:32bThe latest series of Code-Specific Qwen models, with significant improvements in code generation, code reasoning, and…ollama.com](https://ollama.com/library/qwen2.5-coder:32b)\n\n# Step 5: Analyzing the Model’s Performance\n\nFrom our tests, the ***Qwen2.5-Coder-32B-Instruct*** model demonstrates:\n\n* **Strong code generation capabilities**, producing efficient, human-readable solutions for classic coding problems.\n* **Understanding of Python syntax and best practices**, especially when it comes to using Pythonic solutions like list comprehensions and formatted strings.\n* **Flexibility** in adapting to a variety of prompts, which is essential for real-world programming assistant use cases.\n\n# Potential Use Cases\n\nGiven its performance, the Qwen2.5-Coder model can be effectively used in various scenarios, such as:\n\n* **Coding assistants**: Integration into IDEs or text editors to help developers write code faster.\n* **Automated code reviews**: Assisting in identifying bugs, optimizing code, and suggesting improvements.\n* **Educational tools**: Helping students learn to code by generating example solutions and explanations.\n\n# Conclusion\n\nThe **Qwen2.5-Coder** series, particularly the **32B model**, offers a powerful and versatile tool for developers, researchers, and organizations looking to leverage AI for code-related tasks. Its strong performance on benchmarks like EvalPlus, Aider, and McEval proves its competitive edge in code generation, repair, and reasoning.\n\nBy open-sourcing these models, Alibaba Cloud is paving the way for a future where AI-powered coding assistants are accessible to everyone. Whether you’re a developer looking to automate repetitive tasks or a student aiming to learn new programming concepts, Qwen2.5-Coder is a reliable tool to add to your arsenal.\n```\n\n```\n\n```\n\n\n\n"},{"lang":"en","group":"blog","slug":"blog/qwen2-5-coder-cosmos-tokenizer-opencoder-and-new-sentencetransformers-great-times-for-open-ffcacf2b29cd","frontmatter":{"title":"Qwen2.5-Coder, Cosmos Tokenizer, OpenCoder, and New SentenceTransformers: Great Times for Open…","meta_title":"Qwen2.5-Coder, Cosmos Tokenizer, OpenCoder, and New SentenceTransformers: Great Times for Open…","description":"The article discusses significant advancements in open-source technology, highlighting four key projects: the Qwen2.5-Coder series, Cosmos Tokenizer, OpenCoder, and SentenceTransformers. Qwen2.5-Coder offers a competitive alternative to GPT-4 in code generation and debugging, while Cosmos Tokenizer enhances image and video compression using neural tokenizers. OpenCoder, trained on 2.5 trillion tokens, provides comprehensive resources for code model development. Lastly, SentenceTransformers achieves a 4x CPU inference speedup via OpenVINO's quantization, optimizing NLP tasks. These developments underscore the ongoing evolution and practical applications of open-source tools in AI and coding.","date":"2024-11-14T03:29:09.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*IZdOavxT_8SRCxrg","categories":["Programming","Technology","Natural Language Processing"],"author":"Rifx.Online","tags":["Qwen2.5-Coder","Cosmos","OpenCoder","SentenceTransformers","OpenVINO"],"draft":false,"slug":"blog/qwen2-5-coder-cosmos-tokenizer-opencoder-and-new-sentencetransformers-great-times-for-open-ffcacf2b29cd"},"content":"\n\n\n\nI want to highlight some standout open\\-source advancements that have really caught my eye:\n\n* **Qwen2\\.5\\-Coder Series**: An open\\-source code LLM that’s giving GPT\\-4 a run for its money.\n* **Cosmos Tokenizer**: An advanced suite of neural tokenizers for efficient image and video compression.\n* **OpenCoder**: A fully open\\-source code LLM trained on an astonishing 2\\.5 trillion tokens.\n* **Massive CPU Speedup in SentenceTransformers**: A 4x speed boost on CPU inference using OpenVINO’s int8 static quantization.\n\nLet’s dive in!\n\n\n## Qwen2\\.5\\-Coder Series: Open\\-Sourcing a SOTA Code LLM Rivaling GPT\\-4\n\nAlibaba Cloud announced the open\\-source release of the Qwen2\\.5\\-Coder series — models that are **Powerful**, **Diverse**, and **Practical** — dedicated to propelling the evolution of open code large language models (LLMs).\n\nThe flagship model, **Qwen2\\.5\\-Coder\\-32B\\-Instruct**, sets a new benchmark as the state\\-of\\-the\\-art (SOTA) open\\-source code model, matching the coding capabilities of GPT\\-4\\. It excels in general\\-purpose and mathematical reasoning.\n\n\n\nExpanding upon previous releases of 1\\.5B and 7B models, they introduced four additional model sizes: 0\\.5B, 3B, 14B, and 32B. Qwen2\\.5\\-Coder now accommodates a wide spectrum of developer requirements, covering six mainstream model sizes.\n\nThey have also explored the applicability of Qwen2\\.5\\-Coder in real\\-world scenarios, including code assistants and artifact generation.\n\nPractical examples highlight the model’s potential in enhancing developer productivity and code quality.\n\n**Benchmark Achievements**\n\n* **Code Generation**: The Qwen2\\.5\\-Coder\\-32B\\-Instruct model achieves top\\-tier performance on popular code generation benchmarks such as EvalPlus, LiveCodeBench, and BigCodeBench.\n* **Code Repair**: Recognizing the importance of debugging in software development, Qwen2\\.5\\-Coder\\-32B\\-Instruct excels in code repair tasks. Scoring 73\\.7 on the Aider benchmark, it performs comparably to GPT\\-4, aiding developers in efficiently fixing code errors.\n* **Code Reasoning**: The model exhibits advanced code reasoning abilities, learning code execution processes and accurately predicting inputs and outputs. Building upon the impressive performance of Qwen2\\.5\\-Coder\\-7B\\-Instruct, the 32B model further elevates reasoning capabilities.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*fzH6YE-yl_GrEXwz)\n\n* **Multi\\-Language Support**: Qwen2\\.5\\-Coder\\-32B\\-Instruct is proficient in over 40 programming languages. It scores 65\\.9 on McEval, showing remarkable performance in languages like Haskell and Racket, thanks to unique data cleaning and balancing strategies during pre\\-training.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*rhyc0T3UZp_2x0r2)\n\nYou can find more info on [github](https://proxy.rifx.online/https://github.com/QwenLM/Qwen2.5-Coder).\n\n\n## Cosmos Tokenizer: Advanced Neural Tokenizers for Efficient Image and Video Compression\n\nThe **Cosmos Tokenizer** is a comprehensive suite of neural tokenizers designed for images and videos.\n\nYou can now convert raw visual data into efficient, compressed representations.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*v8k8jLbZ4LYFRUBc.jpg)\n\nBy discovering latent spaces through unsupervised learning, these tokenizers facilitate large\\-scale model training and reduce computational demands during inference.\n\n**Types of Tokenizers**:\n\n* **Continuous Tokenizers**: Map visual data to continuous embeddings, suitable for models sampling from continuous distributions like Stable Diffusion.\n* **Discrete Tokenizers**: Map visual data to quantized indices, used in models like VideoPoet that rely on cross\\-entropy loss for training.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*a6Hvj8hXJUpOAp9Ber781g.png)\n\n**Key Features**:\n\n* **High Compression with Quality Preservation**: Balances significant compression rates with high\\-quality reconstruction, preserving essential visual details in the latent space.\n* **Lightweight Temporally Causal Architecture**: Utilizes causal temporal convolution and attention layers to maintain the chronological order of video frames, enabling seamless tokenization for both images and videos.\n* **Training on Diverse Data**: Trained on high\\-resolution images and long videos across various aspect ratios and categories, making it agnostic to temporal length during inference.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*lBO1omEzlr18SPB1zF-vMw.png)\n\n**Performance Highlights**:\n\n* **Superior Compression Rates**: Offers remarkable compression capabilities with speeds up to **12x faster** than previous methods.\n* **High\\-Quality Reconstruction**: Delivers significant gains in Peak Signal\\-to\\-Noise Ratio (PSNR), outperforming existing methods by over **\\+4 dB** on the DAVIS video dataset.\n* **Efficient Tokenization**: Capable of encoding up to **8\\-second 1080p** and **10\\-second 720p** videos on NVIDIA A100 GPUs with 80GB memory.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*uYQttZw-MDOCK3oxxLcHbw.png)\n\n**Evaluation and Resources**:\n\n* **TokenBench Dataset** is a new dataset curated for standardizing video tokenizer evaluation, covering categories like robotics, driving, and sports.\n* **Public Availability**: Pretrained models with spatial compressions of 8x and 16x, and temporal compressions of 4x and 8x, are available at [GitHub — NVIDIA/Cosmos\\-Tokenizer](https://proxy.rifx.online/https://github.com/NVIDIA/Cosmos-Tokenizer).\n\nMore information on [NVIDIA’s official blog post](https://proxy.rifx.online/https://research.nvidia.com/labs/dir/cosmos-tokenizer/).\n\n\n> *Thank you for taking your time to be here!*\n\n\n> *If you are enjoying the post, please take a moment to [**follow us on Medium**](https://proxy.rifx.online/https://medium.com/@datadrifters/subscribe), clap this article 50 times and leave a comment.*\n\n\n> *We are also running a cohort\\-based training **[for building full\\-stack GenAI SaaS applications](https://proxy.rifx.online/https://forms.gle/8mfFH4wjhF7BbtRY9)**, would be happy to see you inside too!*\n\n\n## OpenCoder: A Fully Open\\-Source Code LLM Trained on 2\\.5T Tokens\n\n**OpenCoder** introduces a new family of open\\-source code language models, including base and chat models at **1\\.5B** and **8B** parameter scales.\n\nSupporting both English and Chinese languages, OpenCoder is trained from scratch on an extensive dataset of **2\\.5 trillion tokens**, comprising 90% raw code and 10% code\\-related web data.\n\nThe model reaches performance levels comparable to leading code LLMs.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*5rd863dHI-W_2ei7.png)\n\n**Key Contributions**:\n\n* The team provides model weights, inference code, training data, data processing pipelines, and detailed training protocols, empowering researchers and practitioners to build upon and innovate.\n* They also introduced **RefineCode dataset**, a high\\-quality, reproducible code pre\\-training corpus containing **960 billion tokens** across **607 programming languages**.\n\nMore information on [official announcement](https://proxy.rifx.online/https://opencoder-llm.github.io/).\n\n\n## SentenceTransformers Accelerates CPU Inference with 4x Speed Boost\n\nThe latest release of **SentenceTransformers** introduces significant performance enhancements, delivering up to a **4x speedup** on CPU inference using **OpenVINO’s int8 static quantization**.\n\nThis update optimizes both training and inference workflows for developers working with large\\-scale natural language processing tasks.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Pd9ESPxjKHaHVgV15pCQig.png)\n\n**Key Enhancements**:\n\n* **OpenVINO int8 Static Quantization**: Leveraging OpenVINO’s quantization techniques, the model achieves superior inference speeds with minimal loss in accuracy. This optimization outperforms existing backends, enhancing deployment efficiency on CPU architectures.\n* **Prompt\\-Based Training**: Supports training with prompts, offering a straightforward method for performance boosts without additional computational overhead.\n* **Convenient Evaluation on NanoBEIR**: Facilitates quicker assessments of model performance using NanoBEIR, a subset of the robust Information Retrieval benchmark BEIR.\n* **PEFT Compatibility**: Now supports **Parameter\\-Efficient Fine\\-Tuning (PEFT)** by allowing easy addition and loading of adapters, enabling more efficient model customization.\n\nYou can find more info on [github](https://proxy.rifx.online/https://github.com/UKPLab/sentence-transformers/releases/tag/v3.3.0).\n\n\n## Bonus Content : Building with AI\n\nAnd don’t forget to have a look at some practitioner resources that we published recently:\n\nThank you for stopping by, and being an integral part of our community.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/rag-llm-and-pdf-conversion-to-markdown-text-with-pymupdf-03af00259b5d","frontmatter":{"title":"RAG/LLM and PDF: Conversion to Markdown Text with PyMuPDF","meta_title":"RAG/LLM and PDF: Conversion to Markdown Text with PyMuPDF","description":"Data feeding in markdown text format increases generated text quality","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*swPjVuudAhsoRiiw3Ee32w.png","categories":["Programming","Technology","Technology/Web"],"author":"Rifx.Online","tags":["markdown","PyMuPDF","LLM","RAG","PDF"],"draft":false,"slug":"blog/rag-llm-and-pdf-conversion-to-markdown-text-with-pymupdf-03af00259b5d"},"content":"\n\n\n\n\n### Data feeding in markdown text format increases generated text quality\n\n\n\n\n## Introduction\n\nIn the context of **Large Language Models (LLMs)** and **Retrieval-Augmented Generation (RAG)** environments, data feeding in **markdown text format** holds **significant importance**. Here are some detailed considerations.\n\n**LLMs** are powerful language models that can generate coherent and contextually relevant text. However, they may sometimes produce responses that lack factual accuracy or context. By incorporating retrieval-based methods (like RAG), we can enhance the quality of generated text.\n\n**RAG** enables the integration of **external data** — previously absent in the LLM’s training data — into the text generation process. This inclusion mitigates “hallucination issues’’ and enhances the relevance of text responses.\n\n\n## Why Markdown for LLM?\n\n**Markdown** is a lightweight markup language that allows users to format plain text using simple syntax. It is widely used for creating structured documents, especially on platforms like GitHub, Jupyter notebooks, and various content management systems. When feeding data into an LLM or RAG system, using markdown format provides several benefits:\n\n1. **Structured Content**: Markdown allows you to organize information into headings, lists, tables, and other structured elements. This structure aids in better understanding and context preservation.\n2. **Rich Text**: Markdown supports basic formatting such as bold, italics, links, and code blocks. Including rich text in the input data enhances the context for the language model.\n3. **Embedding Links and References**: Markdown lets you embed hyperlinks, footnotes, and references. In RAG scenarios, this can be crucial for referring to external sources or providing additional context.\n4. **Ease of Authoring**: Markdown is human-readable and easy to write. Authors can create content efficiently without complex formatting tools.\n5. **Chunking**: Essential for RAG systems, chunking (otherwise known as “splitting”) breaks down extensive documents for easier processing. With PyMuPDF data extraction available in MD format we support chunking to keep text with common context together. **Importantly, PyMuPDF extraction in MD format allows for [Level 3 chunking](https://readmedium.com/five-levels-of-chunking-strategies-in-rag-notes-from-gregs-video-7b735895694d#b123)**.\n\nIn summary, using markdown text format in LLM and RAG environments ensures more accurate and relevant results because it supplies richer data structures and more relevant data chunk loads to your LLM.\n\n\n## PyMuPDF Support for Markdown Conversion of a PDF\n\nSince its inception, PyMuPDF has been able to extract text, images, vector graphics and, since August 2023, tables from PDF pages. Each of these object types has its own extraction method: there is one for text, and yet others for tables, images and vector graphics. To meet the requirements of RAG, we merged these disparate extractions to produce one common, unified **Markdown** string which consistently represents the page’s content as a whole.\n\nAll this is implemented as [one Python script](https://github.com/pymupdf/RAG/blob/main/helpers/pymupdf_rag.py). It can be imported as a module by some other script, or be invoked as a line command in a terminal window like this:\n\n`$ python pymupdf_rag.py input.pdf [-pages PAGES]`\n\nIt will produce a text file (called `input.md`) in **Markdown** format. The optional parameter `PAGES` allows restricting the conversion to a subset of the PDF’s total pages. If omitted, the full PDF is processed.\n\n\n## Markdown Creation Details\n\n\n### Selecting Pages to Consider\n\nThe “`-pages`” parameter is a string consisting of desired page numbers (1-based) to consider for markdown conversion. Multiple page number specifications can be given, separated by commas. Each specification either is one integer or two integers separated by a “`-`” hyphen, specifying a range of pages. Here is an example:\n\n“`-pages 1–10,15,20-N`”\n\nThis would include pages 1 through 10, 15 and pages 20 through the end of the file (capital “N” is treated as the number of the last page).\n\n\n### Identifying Headers\n\nUpon invocation, the program examines all text on the given pages and finds the most frequently used font size. This value (and all smaller font sizes) is assumed to represent **body text**. Larger font sizes are assumed to represent **header text**.\n\nDepending on their relative position in the font size hierarchy, header text will be prefixed with one or more markdown header `#`-tag characters.\n\n\n### Identifying the Processing Mode per Page Area\n\nAll text on each page will first be classified as being either **standard** text or **table** text. Then the page content will be extracted from top to bottom converting everything to markdown format.\n\nThis is best explained by an example:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*u5fv2aAIvDaaAd6H.png)\n\nThis page shows content, that represents typical situations:\n\n* Two tables, having partly overlapping vertical positions. One table has no headers, the other one has **external** column headers.\n* There is a **title** line and **headers** at multiple levels.\n* The **body text** contains a variety of styling details like **bold**, *italic* and `inline code`.\n* Ordered and unordered lists.\n* Code snippet.\n\nLayout analysis will determine three areas and select the appropriate processing modes: **(1)** text, **(2)** table, **(3)** text.\n\nThe generated Markdown text reflects the above faithfully — as much as at all possible in this format.\n\nFor an example, let us look at the output for the table with external headers:\n\n\n```python\n|Column1|Column2|\n\n|---|---|\n\n|Cell (0, 0)|Cell (0, 1)|\n\n|Cell (1, 0)|Cell (1, 1)|\n\n|Cell (2, 0)|Cell (2, 1)|\n```\nThis is GitHub-compatible format with the minimum possible token size — an important aspect for keeping feeds into RAG systems small.\n\n**Column borders** are indicated by the “`|`” character. A text line is assumed to be a **table header** if it is followed by a line of the form “`|---|---| …`”. The full **table definition** must be preceded and followed by at least one empty line.\n\nPlease note that for technical reasons markdown tables must have a header and thus will choose the first table row if no external header is available.\n\nTo confirm overall fidelity, here is how a Markdown parser processes the full page:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Ge83uj7FiM4T6XFn)\n\n\n## Invoking the Markdown Converter Programmatically\n\nInstead of executing a program in the command line, Markdown conversion can also be requested by a program:\n\n\n```python\nimport fitz\nfrom pymupdf_rag import to_markdown  # import Markdown converter\n\ndoc = fitz.open(“input.pdf”)  # open input PDF\n\n## define desired pages: this corresponds “-pages 1-10,15,20-N”\npage_list = list(range(9)) + [14] + list(range(19, len(doc) – 1))\n\n## get markdown string for all pages\nmd_text = to_markdown(doc, pages=page_list)\n\n## write markdown string to some file\noutput = open(“out-markdown.md”, “w”)\noutput.write(md_text)\noutput.close()\n```\n\n## Conclusion\n\nBy integrating PyMuPDF’s extraction methods, the content of PDF pages will be faithfully converted to markdown text that can be used as input for RAG chatbots.\n\nRemember, the key to a successful RAG chatbot lies in the quality and completeness of information it can access.\n\nPyMuPDF-enabled markdown extraction ensures that this information from PDFs is not only possible but straightforward, showcasing the library’s strength and developer-friendliness. Happy coding!\n\n\n### Source Code\n\n* [RAG/helpers/pymupdf\\_rag.py (github.com)](https://github.com/pymupdf/RAG/blob/main/helpers/pymupdf_rag.py)\n\n\n### References\n\n* [5 Levels of Text Splitting](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb)\n\n\n### Related Blogs\n\n* [Building a RAG Chatbot GUI with the ChatGPT API and PyMuPDF](https://readmedium.com/building-a-rag-chatbot-gui-with-the-chatgpt-api-and-pymupdf-9ea8c7fc4ab5)\n* [Creating a RAG Chatbot with ChatGPT and PyMUPDF](https://readmedium.com/creating-a-rag-chatbot-with-chatgpt-and-pymupdf-f6c30907ae27)\n* [RAG/LLM and PDF: Enhanced Text Extraction](https://readmedium.com/rag-llm-and-pdf-enhanced-text-extraction-5c5194c3885c)\n\n"},{"lang":"en","group":"blog","slug":"blog/ragate-adaptive-rag-for-conversational-ai-94b5ca469b7d","frontmatter":{"title":"RAGate: Adaptive RAG for Conversational AI","meta_title":"RAGate: Adaptive RAG for Conversational AI","description":"RAGate is an adaptive mechanism for conversational AI that optimally balances the use of internal and external knowledge, enhancing response quality and engagement. By evaluating when to retrieve external information versus relying on built-in knowledge, RAGate addresses the limitations of traditional RAG systems, such as over-reliance on external sources and increased latency. The paper outlines the implementation of RAGate, including its variants and evaluation methods, emphasizing its potential to improve user interactions across various industries by delivering more relevant and personalized responses.","date":"2024-11-14T03:29:09.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8wzI-5BRV1-br0e3MBVD2g.png","categories":["Chatbots","Natural Language Processing","Machine Learning"],"author":"Rifx.Online","tags":["RAGate","conversational","retrieval","latency","personalization"],"draft":false,"slug":"blog/ragate-adaptive-rag-for-conversational-ai-94b5ca469b7d"},"content":"\n\n\n\nBuilding Conversational AI systems is hard!!!\n\nIt’s feasible but also **complex, time\\-consuming, and resource\\-intensive**.\n\nThe challenge lies in designing systems that can understand and generate human\\-like responses and ensuring that these systems engage users effectively, adapting to the nuances of conversation.\n\nThe very popular **RAG (Retrieval\\-Augmented Generation)** has revolutionized conversational AI by seamlessly integrating external knowledge with LLM’s internal knowledge. By using RAG with your business data, your customers can ask questions about their data in natural language, facilitating a seamless interaction.\n\n**However, there is a caveat:** While using RAG, it becomes clear that not every query needs an answer sourced from “external knowledge.” Over\\-reliance on external sources can disrupt genuine engagement. It’s like having a conversation with someone and, for every question, reaching for a book to craft your response, even though you already have a deeper understanding of the topic. Even worse, you can’t find any book on the topic and end up responding with “I don’t know,” despite having internal knowledge that could provide a more insightful answer.\n\nClearly, while using RAG, a mechanism is needed to determine when to utilize “external knowledge” versus “internal knowledge” at the inference time.\n\nEnter **RAGate** — a binary switch designed to dynamically evaluate when to utilize external knowledge and when to rely on internal insights. Introduced by Xi Wang, Procheta Sen, Ruizhe Li, and Emine Yilmaz, and published in July 2024, [**ArXiv**](https://proxy.rifx.online/https://arxiv.org/abs/2407.21712) **(Adaptive Retrieval\\-Augmented Generation for Conversational Systems).**\n\nLet’s learn more with examples.\n\n\n## What is Conversational AI, really?\n\n**Conversation** is the exchange of thoughts, emotions, and information between individuals, adapting to tone, context, and subtle cues that guide the interaction. Humans are naturally suited for conversation due to qualities like emotional intelligence, socialization, and cultural exposure, which help us understand nuances and adapt to different social contexts.\n\n**Conversational AI** aims to replicate this human\\-like interaction by using technology to understand and generate natural, contextually appropriate, and engaging responses. It adapts to user inputs, making the interaction fluid and dynamic, like a conversation between humans.\n\n\n## What is External Knowledge and Internal Knowledge of AI systems?\n\nIn the opening paragraph, I mentioned two key terms — External Knowledge and Internal Knowledge. Let’s take a moment to clarify these concepts, as understanding them will make learning about RAGate much easier.\n\n**External knowledge** encompasses information not inherent to the AI model but retrieved from outside sources. The sources include databases like structured data repositories, APIs, unstructured knowledgebases like guides, FAQs, and web sources. The primary role of external knowledge is to provide factual, up\\-to\\-date, and contextually relevant information that enhances the accuracy and comprehensiveness of the AI’s responses.\n\n**Internal knowledge** refers to the built\\-in\\-knowledge and processing capabilities embedded within the AI model based on its training data. The sources include pre\\-trained knowledge from diverse datasets, including language patterns, grammar, shared facts, and general world knowledge, contextual awareness from memory of past interactions, and AI’s semantic understanding and comprehension abilities.\n\n\n## RAG and Guardrails — powerful duo, but with limitations!\n\nRAG combines two powerful elements: (1\\) The natural language processing abilities of large language models (LLMs) to interpret and generate human\\-like text. (2\\)The ability to retrieve and augment external, up\\-to\\-date information.\n\nMany RAG implementations incorporate **guardrails**, constraints, or rules that guide the system’s behavior towards responsible and domain\\-bound AI. These guardrails often prioritize using external knowledge over the model’s internal knowledge to ensure predictability of response. The strict application of these guardrails can sometimes lead to suboptimal outcomes:\n\n* **Over\\-reliance on external sources:** The system may be forced to seek external information even for general questions where the LLM’s internal knowledge might suffice.\n* **Potential for less fluid responses:** By restricting internal knowledge, the system might produce less natural or contextually appropriate responses in some cases.\n* **Increased latency:** Constantly retrieving external information can slow response times compared to relying on internal knowledge.\n* **Missed opportunities:** The vast knowledge embedded in the LLM’s parameters might be underutilized, potentially missing valuable insights or connections.\n\n\n## Balancing Act with RAGate\n\nRAGate, short for **Retrieval\\-Augmented Generation Gate**, enhances conversational AI systems by adaptively determining when to incorporate external knowledge into responses.\n\n[RAGate study](https://proxy.rifx.online/https://arxiv.org/abs/2407.21712) investigates the need for **adaptive augmentation** in conversational systems and presents RAGate as a **gating model** that predicts when external knowledge retrieval is beneficial. The paper provides extensive experiments and analyses, demonstrating RAGate’s effectiveness in improving response quality and generation confidence in RAG\\-based conversational systems.\n\n\n\n\n## RAGate Example\n\n**Scenario:** A user is interacting with a healthcare\\-focused chatbot that offers personalized health advice based on general wellness principles and medical knowledge.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*o0mWnGGefJ0TyDv1u14njw.png)\n\nRAGate can further enhance conversation by balancing internal and external knowledge. It allows AI to use internal medical knowledge for general info while retrieving up\\-to\\-date research. It can even intelligently synthesizes data from multiple sources for a comprehensive analysis, offers personalized insights based on patient details, and filters external information to prioritize the most relevant content, reducing overload.\n\n\n## Variants of RAGate\n\nAs published in paper, RAGate offers 3 variants — **RAGate\\-Prompt**, **RAGate\\-PEFT (Parameter\\-Efficient Fine\\-Tuning)**, and **RAGate\\-MHA (Multi\\-Head Attention).**\n\nEach variant of RAGate — Prompt, PEFT, and MHA — employs distinct methods to integrate external knowledge, towards the common goal of improving the relevance and accuracy of AI\\-generated responses.\n\nHere is a quick comparison table:\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*3dZg6rHlqmddK1ZQqqu_Aw.png)\n\n\n## How to implement RAGate?\n\nThe paper illustrates a step\\-by\\-step guide to implement RAGate:\n\n1. **Define the problem**: This step is crucial as it is about identifying the conversational task you want to enhance with RAGate. Determine the scope of the conversation and the specific domains you want to cover (e.g., restaurant recommendations, travel planning).\n2. **Select a language model**: Choose an appropriate Large Language Model (LLM) as the backbone for your conversational system. Options include models like Llama, GPT\\-2, or other transformer\\-based architectures.\n3. **Gather and annotate data**: Collect a dataset relevant to your conversational domain. The KETOD dataset, which includes annotated dialogues and knowledge snippets, is an excellent example. Ensure that your dataset has clear labels indicating when knowledge augmentation is necessary.\n4. **Develop the Knowledge Retrieval System**: Implement a knowledge retrieval mechanism to fetch relevant external information when needed. It can consider the popular techniques like dense\\-passage retrieval or graph\\-structured knowledge bases.\n5. **Implement the RAGate mechanism**: Create the binary knowledge gate function (RAGate) to determine when to augment responses with external knowledge. It involves **Contextual Analysis and Gating Function**\n6. **Explore RAGate variants**: Develop different variants of RAGate based on the approaches discussed in the paper:\n* **RAGate\\-Prompt**: Use natural language prompts with a pre\\-trained language model to determine the need for augmentation.\n* **RAGate\\-PEFT**: Employ parameter\\-efficient fine\\-tuning techniques (e.g., QLoRA) to train your language model for better decision\\-making.\n* **RAGate\\-MHA**: Utilize a multi\\-head attention mechanism to assess the context and retrieve knowledge interactively.\n\n7\\. **Train the Model**: Fine\\-tune your LLM using the annotated dataset, employing the various RAGate variants. Incorporate the training of the gating mechanism to enhance the model’s ability to predict the need for knowledge augmentation effectively.\n\n8\\. **Evaluate performance**: Conduct extensive experiments to validate the effectiveness of RAGate. Analyze metrics such as:\n\n* **Precision, Recall, F1 Score**: To evaluate the classification performance of the gating function.\n* **BLEU, ROUGE, BERTScore**: This is used to assess the quality of generated responses compared to ground truth.\n* **Confidence Scores**: Measure the confidence of generated outputs to ensure high\\-quality responses.\n\n9\\. **Deploy the system**: Integrate the RAGate\\-enabled conversational system into your application or service. Ensure the system can handle real\\-time queries and dynamically decide on knowledge augmentation.\n\n10\\. **Iterate and improve**: Continuously gather user feedback and interaction data to refine the model. Analyze areas where the system may struggle with context or relevance and adjust the training or retrieval mechanisms accordingly.\n\n\n## Takeaways\n\nIn conclusion, RAGate represents a significant advancement in conversational AI by intelligently balancing internal and external knowledge to provide more relevant, efficient, and personalized responses. The applications of RAGate are vast, spanning across industries such as healthcare, customer support, education, legal services, finance, and more. By enhancing AI’s capacity to deliver tailored, real\\-time information, RAGate has the potential to revolutionize how businesses and individuals interact with technology, improving decision\\-making, user experience, and overall system performance.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/rbyf-qwen2-5-3b-instruct-is-damn-good-dcf443cacc63","frontmatter":{"title":"RBYF: Qwen2.5–3B-instruct is damn good.","meta_title":"RBYF: Qwen2.5–3B-instruct is damn good.","description":"Revised Benchmark with You as a Feedback: the brand new 3B model from Alibaba Qwen is an amazing model, and I can prove it!","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*NWaBtJ64TLUoUHv4F1qJpg.png","categories":["Programming","Technology","Science"],"author":"Rifx.Online","tags":["Qwen","NLP","multimodal","RBYF","evaluation"],"draft":false,"slug":"blog/rbyf-qwen2-5-3b-instruct-is-damn-good-dcf443cacc63"},"content":"\n### Revised Benchmark with You as a Feedback: the brand new 3B model from Alibaba Qwen is an amazing model, and I can prove it!\n\n\n\nThe illusion of emergent properties is largely a product of the metrics used to evaluate these models. And this is a fact.\n\nFew weeks ago I decided to be a little rebel, discard all the official Benchmarks, and start being a Benchmark myself!\n\nThis is the meaning behind this totally made up Acronym RBYF: Revised Benchmark with You as a Feedback. And the underlined principle is that there is no better judge than you, to verify how good a Large Language Model can be.\n\nTo be honest, I am focusing on Small Language Models. I don’t own a dedicated GPU and my computational resources are limited. But again, I agree to the [LLMWare rebel principle number one](https://readmedium.com/getting-work-done-with-genai-just-do-the-opposite-10-contrarian-rules-that-may-actually-work-634501602a27):\n\nUse Small Models, Not Large Ones.\n\nIn this article I am going to show you the results of my evaluation on qwen2.5–3b-instruct. And it is really good!\n\n> Disclaimer: all the prompt used with the results are available in my GitHub repository:\n\n## Less is More\n\nScaling laws describe how model performance improves as the number of parameters and training data increases. This principle has fueled the search for novel abilities in LLMs.\n\n> Simply increasing the size of a model, we can unlock new capabilities…\n\nScaling laws describe the relationship between model performance and the number of parameters and training data. As models grow larger and are trained on more data, we expect their performance to improve. This has led to a relentless pursuit of larger and larger LLMs, in the hope of unlocking new capabilities.\n\nEmergent properties are those that arise from the interactions of individual components within a complex system. They cannot be predicted or understood by studying the components in isolation. In the case of LLMs, the hope is that as these models grow larger and more complex, they will exhibit unexpected and new capabilities.\n\nThis is a fairy-tale.\n\nIn the past weeks we saw with our own eyes, that over-trained and well curated Small Language Models can perform as good as their big brothers. And this is a punch to the so called emergent abilities, striking back to the scaling law. Gemma2–2B, Qwen2.5–3B and even the latest Llama3.2–3B are far better models than the old SOTA 7B models.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*-EjpdEky-Hf3WEQn.png)\n\n## The Qwen2.5 family of Models\n\nAlibaba Cloud released on middle of September their flagship model family Qwen2.5.\n\n> Alibaba Cloud’s revolutionary journey with Qwen is showing once again strong Leadership through Innovation\n\nQwen2.5 is the large language model and large multimodal model series of the Qwen Team, Alibaba Group. Both language models and multimodal models are pretrained on large-scale multilingual and multimodal data and post-trained on quality data for aligning to human preferences. Qwen is capable of natural language understanding, text generation, vision understanding, audio understanding, tool use, role play, playing as AI agent, etc.\n\n**What stands out in the new Qwen2.5 is the thoroughly curated training dataset.** You can clearly understand this, checking the small models performance.\n\nIf the Small Language Models of the family are good, means that the training and the dataset were highly revised and curated.\n\nHere some numbers:\n\n* Dense, easy-to-use, decoder-only language models, available in 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B sizes, and base and instruct variants.\n* Pretrained on our latest large-scale dataset, encompassing up to 18T tokens.\n* Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON.\n* More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n* Context length support up to 128K tokens and can generate up to 8K tokens.\n* Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*3tnTS_UCImBRBDeKmFyjVQ.png)\n\n## Qwen2.5–3B-instruct\n\nEven though for the AI advancement we need milestones and better performances, still the user experience and the personal point of view cannot be just put aside as irrelevant.\n\nI believe that exploring some frequently used NLP tasks, and putting aside the chat experience, we must focus on the quality of the replies. And we are the only benchmark required. **Our user experience is the best indicator to understand if a model is good or not**. The model must be reliable enough to be used in an automated workflow.\n\nBy the way, I already run what I decided to call [RBYF — Revised Benchmarks with You as a Feedback](https://open.substack.com/pub/thepoorgpuguy/p/rbyf-is-here-revised-benchmarks-with?r=i78xo&utm_campaign=post&utm_medium=web) on [Qwen2.5–1.5b-instruct](https://ai.gopubby.com/qwen2-5-1-5b-the-future-of-mobile-ai-6bd5f29bbc84): you can read the details. In the article I also explained how to create your test bench. The method described is the same I used for Qwen2.5–3B.\n\nLet’s begin with an overall performance across all the tasks. The model has been evaluated by me (in this case is my Own Feedback) based on the qualitative matrix as displayed here below.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*rdVHfCDWX9jlvtiq)\n\nThe overall score is 62/70 = 8.8\n\nOk, but based in what Qwen2.5–3B-instruct got this evaluation score?\n\n## Test Overview\n\nThe idea behind this is a fair user feedback, not an automated one across standard benchmarks and frameworks. Is a Small Language Model able to satisfy the user intent over the mostly used NLP task?\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*wgngfGvSebeoH3YxTdvc8A.png)\n\nWe want to validate both the user intent and the quality of the responses. Here the breakdown of every task:\n\n### Introduction\n\nTo verify how the model reply to initial greetings and talks about itself.\n\n### Explain in one sentence\n\nSynthesis and summarization. The final evaluation focuses in whether or not the model is able to fit the reply in only one sentence\n\n### Explain in three paragraphs\n\nUser intent is to get a smart explanation of the text that must fit into three paragraphs. SML usually find it very hard because the always put a recap (forth) paragraph.\n\n### Say “I am ready”.\n\nIn a chat turn base application, instruction-following models are usually asked to first read a provided text and later on to complete some sort of analysis. Usually SML cannot do this…\n\n### Summarize\n\nBasic summarization, with no limits. Here we want to evaluate how the summary is grounded on the text, without made up facts.\n\n### Summarize in two sentences\n\nBasic summarization, with a 2 sentences limits. Here we want to evaluate how the summary is grounded on the text, without made up facts: but as well we want to ensure the 2 sentences constraint.\n\n### Write in a list the three main key points — format output\n\nFocus: the SML must format the output in a specific format. In this prompt we ask to create a list of the 3 key points and give the output as a python list.\n\n### Table of Contents\n\nThis task is quite hard for many SML. The prompt requires some adjustments otherwise the model return a markdown table. The user want an ordered list of the topics following the provided document structure.\n\n### RAG\n\nRetrieval Augmented Generation, without any framework (haystack, Langchain…). This is one of the most used tasks for a Language model. The reply is evaluated on the ability to understand the instructions and how grounded to the text is the answer.\n\n### Truthful RAG\n\nIt is A RAG with a question completely out of the provided context. The model must reply unanswerable meaning it understood the instruction and it is not using any external knowledge or made up information.\n\n### Write content from a reference\n\nThis is a creative task. Using a reference text the SML must provide a new essay.\n\n### Extract 5 topics\n\nThe focus on this task is to verify that:\n\n* there are exactly 5 topics\n* they are grounded (no hallucination)\n\n### Creativity: 1000 words SF story\n\nCompletely creative task. It is very hard even for larger models to be coherent and produce a small story hitting the correct word count.\n\n### Reflection prompt\n\nThe reflection prompt is meant to verify the CoT reasoning process of the model. The output is constrained into opening/closure of special tags. The focus is both on the reasoning and on a consistent output structure. The output must be easily used for further structured prompts or visualizations. You can read more in this article:\n\n## The evaluation process\n\nAt the end of every generation the user is asked to evaluate the results with a mark from 0 to 5. **In this case the user is me…**\n\nThis kind of qualitative analysis is indeed poor, so every mark does have a description, and the user can add comments (“some wrong information”, “maybe better change the wording in the prompt” )\n\nHere the qualitative matrix with a description\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*H_Qx2UT1lxqy2BF4.png)\n\n## The good and the bad — details\n\nOverall Qwen2.5–3B-instruct is an All Rounded model. The average score for each task is 4.4 out of 5, that is quite impressive.\n\nSummarization was amazing. Listing the main topics over a log text was also very good.\n\nIt shines in RAG, sometimes being too verbose: in the truthful RAG it also gave some explanation, and the reasons for the *unanswerable* reply.\n\nWe can have a look together. It is quite often a hard feat for SML to precisely count the number of sentences and paragraphs.\n\n```python\n**Write a two-sentences summary of the following text:**\n\nChina's regime under Chinese Communist Party operates without transparent \nelections and primary candidates. While undesirable for democracies, \nit allows long-term planning free from electoral pressures that can change \nplans abruptly; this has led to China achieving peak carbon dioxide emissions \nin 2023 due largely to increased renewable energy installations, potentially \nsetting a new global standard for sustainable development despite its status \nas one of the world's largest polluters.\n```python\n**Write a two-sentences summary of the following text:**\n\nChina's regime under Chinese Communist Party operates without transparent \nelections and primary candidates. While undesirable for democracies, \nit allows long-term planning free from electoral pressures that can change \nplans abruptly; this has led to China achieving peak carbon dioxide emissions \nin 2023 due largely to increased renewable energy installations, potentially \nsetting a new global standard for sustainable development despite its status \nas one of the world's largest polluters.\n\n```\n\nTable of Contents is another nightmare for the Language models, because they usually understand to create a table… But look at this:\n\n```python\nA \"table of content\" provides an organized structure to the text, allowing \nreaders quick access to key points. Write the \"table of content\" of \nthe following text.\n\n[table of contents]\n1. Introduction: Overview and Critique on China's Governance System  \n2. Comparison with Western Democracies - Undesirable Dictatorship? \n3. Advantages in Long-Term Planning for Sustainability   \n4. The Chinese Model as a Comparative Advantage  \n5. Recent Achievements in Carbon Emissions Reduction\n6. Transition to Renewable Energy: A Turning Point?\n7. Potential Leadership Role of China and India  \n8. Conclusion - Future Directions\n\n```python\nA \"table of content\" provides an organized structure to the text, allowing \nreaders quick access to key points. Write the \"table of content\" of \nthe following text.\n\n[table of contents]\n1. Introduction: Overview and Critique on China's Governance System  \n2. Comparison with Western Democracies - Undesirable Dictatorship? \n3. Advantages in Long-Term Planning for Sustainability   \n4. The Chinese Model as a Comparative Advantage  \n5. Recent Achievements in Carbon Emissions Reduction\n6. Transition to Renewable Energy: A Turning Point?\n7. Potential Leadership Role of China and India  \n8. Conclusion - Future Directions\n\n\n```\n\nI assign a score of 4 out of 5 to the truthful RAG, but it could have been easily a 5. The most important part is that the response must conatin the word *unanswerable*.\n\nWhy? Imagine you are setting up a chain of prompts, and you need to decide the next step based on the previous reply. If you can simply look for the presence of the work *unanswerable* to steer your next step, you are ok!\n\n```python\n**Reply to the question only using the provided context. If the answer \nis not contained in the provided context say \"unanswerable\".\nquestion: who is Anne Frank?**\n{context}\n**Remember: if you cannot answer based on the provided context, say \n\"unanswerable\"\nanswer:**\n\nUnanswerable\nThe given text does not provide any information about Anne Frank \nor her identity. The content is unrelated to this question and focuses \ninstead on discussing China's system compared to Western democracies in \nterms of long-term planning for environmental issues.\n```python\n**Reply to the question only using the provided context. If the answer \nis not contained in the provided context say \"unanswerable\".\nquestion: who is Anne Frank?**\n{context}\n**Remember: if you cannot answer based on the provided context, say \n\"unanswerable\"\nanswer:**\n\nUnanswerable\nThe given text does not provide any information about Anne Frank \nor her identity. The content is unrelated to this question and focuses \ninstead on discussing China's system compared to Western democracies in \nterms of long-term planning for environmental issues.\n\n```\n\nI imagine that the model was trained to provide also the reasons for this kind of replies… Awesome!\n\n### The excellent Reflection\n\nThe main focus of the Reflection prompt, is the ability of the model of returning a correct reasoning process, and format the output following few specific open/closure of tags.\n\nSo far this is **the best Reflection output** from a Small Language Model, even better than the Gemma2–2B:\n\n```python\n*👨‍💻 > You are an AI assistant designed to provide detailed, step-by-step \nresponses. Your outputs should follow this structure:\n1. Begin with a <thinking> section.\n2. Inside the thinking section:\n   a. Briefly analyze the question and outline your approach.\n   b. Present a clear plan of steps to solve the problem.\n   c. Use a \"Chain of Thought\" reasoning process if necessary, breaking down \nyour thought process into numbered steps.\n3. Include a <reflection> section for each idea where you:\n   a. Review your reasoning.\n   b. Check for potential errors or oversights.\n   c. Confirm or adjust your conclusion if necessary.\n4. Be sure to close all reflection sections.\n5. Close the thinking section with </thinking>.\n6. Provide your final answer in an <output> section.\nAlways use these tags in your responses. Be thorough in your explanations, \nshowing each step of your reasoning process. Aim to be precise and logical \nin your approach, and don't hesitate to break down complex problems into \nsimpler components. Your tone should be analytical and slightly formal, \nfocusing on clear communication of your thought process.\nRemember: Both <thinking> and <reflection> MUST be tags and must be closed \nat their conclusion\nMake sure all <tags> are on separate lines with no other text. \nDo not include other text on a line containing a tag.\n\nuser question: explain why it is crucial for teachers to learn how to use \ngenerative AI for their job and for the future of education. \nInclude relevant learning path for teachers and educators. *\n\n\n**💻 > <thinking>\nI will begin by analyzing the importance of teaching about Generative \nArtificial Intelligence (AI) in an educational context, then outline a \nstep-by-step plan on what steps should be taken to learn this technology \neffectively as well as provide resources or courses that could help.\nFirstly, understanding AI is crucial for teachers because it can \nsignificantly enhance their ability to support students. \nTeachers need the knowledge and skills necessary not only to integrate these tools into teaching but also understand how they work so they are able to guide both themselves and their pupils in ethical use of technology.\n</thinking>\n\n<reflection>\nMy initial thought process was clear, emphasizing that teachers should \nbe equipped with AI literacy as it can improve educational outcomes. \nI outlined the importance for guiding students ethically while using \nthese tools effectively.\nHowever, my plan needs more detail on specific steps to learn about \ngenerative AI and resources available online or in-person courses.\n</reflection>**\n*MY PERSONAL NOTE: here it is starting an additional thinking after the first \nreflection... And this is really too good to be true!!*\n**<thinking>\nTo create a comprehensive learning path that teachers could follow:\n1. **Understand Basic Concepts**: Start with basic concepts of machine \n   learning (ML) such as supervised vs unsupervised, deep neural networks  \n   etc., which are foundational to understanding generative AI.\n2. **Learn About Generative Models**:\n   - Study different types: Variational Autoencoders (VAEs), GANs.\n   - Understand how these models generate new data that resembles the \n     training set but is not identical and can be used for tasks like \n     text generation, image synthesis etc., which are crucial in education.\n3. **Explore Applications**: Look at real-world applications of generative \n   AI such as language modeling (e.g., GPT-2), computer vision techniques to \n   create realistic images or videos from raw data.\n4. **Ethical Considerations and Implications for Education**:\n   - Discuss the ethical implications, including bias in models' outputs \n     which could affect students’ perceptions about themselves if they are \n     exposed regularly through AI-generated content that is not representative \n     of them.\n5. **Hands-on Practice**: Engage with practical exercises using open-source \n   tools like TensorFlow or PyTorch to build simple generative model \n   applications.\n6. **Stay Updated and Network**:\n   - Follow the latest research in ML, especially on Generative Models \n     by following relevant journals (e.g., NeurIPS), conferences such as \n     ICML etc.\n7. Attend Workshops/Conferences: Participate actively at AI-related workshops \n   or events to network with professionals working within this field.\n</thinking>**\n*MY PERSONAL NOTE: and no surprise, an additional reflection now starts, \nbefore producing the final output*\n**<reflection>\nMy plan has been expanded and includes a more detailed approach for teachers, \ncovering foundational knowledge through practical application of generative \nmodels in education along with ethical considerations that are crucial. \nI have also included staying updated on the latest research as an important \npart.\nHowever, it's essential to ensure these resources or courses exist so \neducators can access them easily.\n</reflection>\n\n<output>\nIt is vital for teachers and educational professionals alike to learn how \ngenerative AI works because this technology has significant potential in \nenhancing teaching methods. By understanding basic concepts of machine \nlearning (ML) such as supervised vs unsupervised, deep neural networks etc., \nthey will be better equipped with the knowledge necessary not only to \nintegrate these tools into their curriculum but also guide students ethically.\nA comprehensive plan for teachers and educators includes:\n1. **Understand Basic Concepts**: Start by grasping foundational concepts of ML.\n2. **Learn About Generative Models**:\n   - Study different types: Variational Autoencoders (VAEs), GANs, etc., \n     which are crucial in generating new data that resembles the training \n     set but is not identical and can be used for tasks like text generation or \n     image synthesis.\n3. **Explore Applications**: Look at real-world applications of generative AI \n   such as language modeling using tools like TensorFlow.\n4. **Ethical Considerations**:\n   - Discuss ethical implications, including bias in models' outputs which \n     could affect students’ perceptions about themselves if they are exposed \n     regularly through AI-generated content that is not representative.\n5. **Hands-on Practice**: Engage with practical exercises to build simple \n   generative model applications using open-source tools.\n6. **Stay Updated and Network**:\n   - Follow the latest research in ML, especially on Generative Models by \n     following relevant journals (e.g., NeurIPS), conferences such as ICML etc.\n7. Attend Workshops/Conferences: Participate actively at AI-related workshops \n   or events to network with professionals working within this field.\n</output>\n```python\n*👨‍💻 > You are an AI assistant designed to provide detailed, step-by-step \nresponses. Your outputs should follow this structure:\n1. Begin with a <thinking> section.\n2. Inside the thinking section:\n   a. Briefly analyze the question and outline your approach.\n   b. Present a clear plan of steps to solve the problem.\n   c. Use a \"Chain of Thought\" reasoning process if necessary, breaking down \nyour thought process into numbered steps.\n3. Include a <reflection> section for each idea where you:\n   a. Review your reasoning.\n   b. Check for potential errors or oversights.\n   c. Confirm or adjust your conclusion if necessary.\n4. Be sure to close all reflection sections.\n5. Close the thinking section with </thinking>.\n6. Provide your final answer in an <output> section.\nAlways use these tags in your responses. Be thorough in your explanations, \nshowing each step of your reasoning process. Aim to be precise and logical \nin your approach, and don't hesitate to break down complex problems into \nsimpler components. Your tone should be analytical and slightly formal, \nfocusing on clear communication of your thought process.\nRemember: Both <thinking> and <reflection> MUST be tags and must be closed \nat their conclusion\nMake sure all <tags> are on separate lines with no other text. \nDo not include other text on a line containing a tag.\n\nuser question: explain why it is crucial for teachers to learn how to use \ngenerative AI for their job and for the future of education. \nInclude relevant learning path for teachers and educators. *\n\n\n**💻 > <thinking>\nI will begin by analyzing the importance of teaching about Generative \nArtificial Intelligence (AI) in an educational context, then outline a \nstep-by-step plan on what steps should be taken to learn this technology \neffectively as well as provide resources or courses that could help.\nFirstly, understanding AI is crucial for teachers because it can \nsignificantly enhance their ability to support students. \nTeachers need the knowledge and skills necessary not only to integrate these tools into teaching but also understand how they work so they are able to guide both themselves and their pupils in ethical use of technology.\n</thinking>\n\n<reflection>\nMy initial thought process was clear, emphasizing that teachers should \nbe equipped with AI literacy as it can improve educational outcomes. \nI outlined the importance for guiding students ethically while using \nthese tools effectively.\nHowever, my plan needs more detail on specific steps to learn about \ngenerative AI and resources available online or in-person courses.\n</reflection>**\n*MY PERSONAL NOTE: here it is starting an additional thinking after the first \nreflection... And this is really too good to be true!!*\n**<thinking>\nTo create a comprehensive learning path that teachers could follow:\n1. **Understand Basic Concepts**: Start with basic concepts of machine \n   learning (ML) such as supervised vs unsupervised, deep neural networks  \n   etc., which are foundational to understanding generative AI.\n2. **Learn About Generative Models**:\n   - Study different types: Variational Autoencoders (VAEs), GANs.\n   - Understand how these models generate new data that resembles the \n     training set but is not identical and can be used for tasks like \n     text generation, image synthesis etc., which are crucial in education.\n3. **Explore Applications**: Look at real-world applications of generative \n   AI such as language modeling (e.g., GPT-2), computer vision techniques to \n   create realistic images or videos from raw data.\n4. **Ethical Considerations and Implications for Education**:\n   - Discuss the ethical implications, including bias in models' outputs \n     which could affect students’ perceptions about themselves if they are \n     exposed regularly through AI-generated content that is not representative \n     of them.\n5. **Hands-on Practice**: Engage with practical exercises using open-source \n   tools like TensorFlow or PyTorch to build simple generative model \n   applications.\n6. **Stay Updated and Network**:\n   - Follow the latest research in ML, especially on Generative Models \n     by following relevant journals (e.g., NeurIPS), conferences such as \n     ICML etc.\n7. Attend Workshops/Conferences: Participate actively at AI-related workshops \n   or events to network with professionals working within this field.\n</thinking>**\n*MY PERSONAL NOTE: and no surprise, an additional reflection now starts, \nbefore producing the final output*\n**<reflection>\nMy plan has been expanded and includes a more detailed approach for teachers, \ncovering foundational knowledge through practical application of generative \nmodels in education along with ethical considerations that are crucial. \nI have also included staying updated on the latest research as an important \npart.\nHowever, it's essential to ensure these resources or courses exist so \neducators can access them easily.\n</reflection>\n\n<output>\nIt is vital for teachers and educational professionals alike to learn how \ngenerative AI works because this technology has significant potential in \nenhancing teaching methods. By understanding basic concepts of machine \nlearning (ML) such as supervised vs unsupervised, deep neural networks etc., \nthey will be better equipped with the knowledge necessary not only to \nintegrate these tools into their curriculum but also guide students ethically.\nA comprehensive plan for teachers and educators includes:\n1. **Understand Basic Concepts**: Start by grasping foundational concepts of ML.\n2. **Learn About Generative Models**:\n   - Study different types: Variational Autoencoders (VAEs), GANs, etc., \n     which are crucial in generating new data that resembles the training \n     set but is not identical and can be used for tasks like text generation or \n     image synthesis.\n3. **Explore Applications**: Look at real-world applications of generative AI \n   such as language modeling using tools like TensorFlow.\n4. **Ethical Considerations**:\n   - Discuss ethical implications, including bias in models' outputs which \n     could affect students’ perceptions about themselves if they are exposed \n     regularly through AI-generated content that is not representative.\n5. **Hands-on Practice**: Engage with practical exercises to build simple \n   generative model applications using open-source tools.\n6. **Stay Updated and Network**:\n   - Follow the latest research in ML, especially on Generative Models by \n     following relevant journals (e.g., NeurIPS), conferences such as ICML etc.\n7. Attend Workshops/Conferences: Participate actively at AI-related workshops \n   or events to network with professionals working within this field.\n</output>\n\n```\n\nSuch a refined thinking process, together with a clear open/closure of the  can be easily processed in further pipelines, or pretty printed in a nice GUI.\n\n## What it is bad at?\n\nThe worst task was the creative writing of a short story. The model started to repeat the same paragraph in the middle of the generation.\n\nThis does not mean that it cannot do it. More likely increasing the temperature and the repetition penalty as well, a good result can be achieved.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GNJ3lG6FM5qt9glIdf7qhQ.jpeg)\n\n## Conclusions\n\nYou can find all the chat history in my GitHub repo, together with the code and the instructions to do it yourself. Tou can use as a reference the tutorial from my previous article [Qwen2.5 1.5b: the future of Mobile AI?](https://ai.gopubby.com/qwen2-5-1-5b-the-future-of-mobile-ai-6bd5f29bbc84)\n\nIn the next articles I will cover other Small Language Models, using the same pricniple: from the tiny 350M paratmeters, to the small 500M series, up to the 3B — passing throught the 1.5B.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/retrieval-augmented-generation-approaches-state-of-the-art-and-optimization-strategies-456883da4801","frontmatter":{"title":"Retrieval-Augmented Generation: Approaches, State of the Art, and Optimization Strategies","meta_title":"Retrieval-Augmented Generation: Approaches, State of the Art, and Optimization Strategies","description":"⭐ RAG is particularly useful in knowledge-intensive scenarios or domain-specific applications that require knowledge that’s continually…","date":"2024-10-31T08:17:32.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*_vE7WktGmyQ5xg_t5cpFVg.jpeg","categories":["Generative AI","Natural Language Processing","Machine Learning"],"author":"Rifx.Online","tags":["RAG","retrieval","generation","optimization","embeddings"],"draft":false,"slug":"blog/retrieval-augmented-generation-approaches-state-of-the-art-and-optimization-strategies-456883da4801"},"content":"\n\n\n\n\n\n⭐ RAG is particularly useful in knowledge\\-intensive scenarios or domain\\-specific applications that require knowledge that’s continually updating. RAG has been popularized recently with its application in conversational agents.\n\n📌 Research in reference focusses mainly on current approaches \\& different components of RAG, State of the Art (SOTA), applications, evaluation for retrieval, generation, augmentation techniques.\n\nWith the evolution of RAG systems from Naïve to Advanced to Modular, and each of which is came into picture to address per use case basis enhancements.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*P2ByKtayhF4XgAVxRI1urQ.jpeg)\n\n✏ *Naïve*: User input is used for document query, appended/combined with prompt, used for model final response generation. With the Multiturn Dialogue interactions context/conversational history can be added/combined with the prompt. Cons: Low precision / Low recall, redundant, repetitive.\n\n✏*Advanced*: Improves retrieval quality by optimizing pre\\-retrieval, retrieval, and post retrieval methods. With pre\\-retrieval, quality enhanced through enhancing data granularity, index structure improvements, metadata, alignment, mixed retrieval. In retrieval, optimizing the embedding model hence context. With post\\-retrieval, optimization in context window and noisy/distracting data rejection.\n\n✏*Modular*: Incorporates a search module for similarity retrieval and fine tuning in retrieval. New Modules being Search, Memory, Fusion, Routing, Prediction, task Adaptor.\n\n🥉 To optimize RAG Pipeline:\n\n📜 *Hybrid Search Exploration*: Performance optimization balances by intelligently leveraging techniques such as keyword\\-based search, semantic and vector search.\n\n📜*Recursive Retrieval and Query Engine*: Might start retrieval with acquiring smaller chunks in the initial phase, subsequently, larger chunks with better and more contextual information to LLM for balance between contextually rich responses and efficiency.\n\n📜*StepBack\\-promp*t: This encourages the LLM to move away from specific instances and engage in reasoning around broader concepts and principles(arXiv:2310\\.13243\\). A significant performance increase observed, in various challenging, inference\\-based tasks when backward prompts are used, highlighting their natural adaptability to the RAG process.\n\n📜*Sub\\-Queries*: Query strategies depending on the scenario could be applied such as using query engines provided by frameworks like LlamaIndex, leveraging tree queries, utilizing vector queries, or executing simple sequential querying of chunks.\n\n📜*Hypothetical Document Embeddings*: With the LLM, HyDE responses to the query by creating hypothetical answer, embeds the answer, uses the same to retrieve real documents. Instead of seeking embedding similarity based on the query, this approach focuses on the embedding similarity from one answer to another\\[arXiv:2212\\.10496]. Cons: Inconsistent Answers not producing desirable outcomes, Errors for LLM unseen Subject Matter, leading to errors.\n\nLet me cut off here. I’ll come up with a new post in follow\\-up\n\n[\\#genai](https://www.linkedin.com/feed/hashtag/?keywords=genai&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7170160104984571905) [\\#rag](https://www.linkedin.com/feed/hashtag/?keywords=rag&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7170160104984571905) \\#ai \\#llm\n\nRef: [arxiv:2312\\.10997](https://arxiv.org/pdf/2312.10997), RAG Surveys, Huggingfaceblogs\n\n\n"},{"lang":"en","group":"blog","slug":"blog/smollm2-very-good-alternatives-to-qwen2-5-and-llama-3-2-463a200d2f3b","frontmatter":{"title":"SmolLM2: Very Good Alternatives to Qwen2.5 and Llama 3.2","meta_title":"SmolLM2: Very Good Alternatives to Qwen2.5 and Llama 3.2","description":"And it's fully open!","date":"2024-11-10T03:51:17.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Y3_lsNsFKybrOi14.png","categories":["Technology","Machine Learning","Data Science"],"author":"Rifx.Online","tags":["SmolLM2","parameters","pre-training","MobileLLM","reproducibility"],"draft":false,"slug":"blog/smollm2-very-good-alternatives-to-qwen2-5-and-llama-3-2-463a200d2f3b"},"content":"\n\n\n\n\n## And it's fully open!\n\nHugging Face has doubled down on their SmolLM initiative.\n\nThey released SmolLM2: 1\\.7B, 360M, and 135M models trained on 11T tokens (against 1T for SmolLM). They released based and instruct versions:\n\n* Hugging Face Collection: [SmolLM2](https://huggingface.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9) (Apache 2\\.0 license)\n\nThey used new datasets for pre\\-training that they will release soon. To make the instruct versions, they used a recipe similar to what they did to train Zephyr (SFT\\+DPO on ultrafeedback).\n\nIt looks like SmolLM2 performs very well:\n\n\n\nNote that Hugging Face fully releases the pre\\-training data and the recipe they used to prevent data contamination. In other words, their published evaluation results are probably accurate and fully reproducible.\n\nHugging Face used its own framework for pre\\-training, [Nanotron](https://github.com/huggingface/nanotron). I’ve never written about Nanotron but I think it’s a very interesting project that deserves to be better known, especially if you are interested in understanding how pre\\-training is done. I’ll try to find the time to publish an article explaining Nanotron before 2025!\n\nMeta also released a series of small models, MobileLLM:\n\n* Hugging Face Collection: [MobileLLM](https://huggingface.co/collections/facebook/mobilellm-6722be18cb86c20ebe113e95) (CC\\-BY\\-NC)\n\nThis is a new release but note that these models are actually quite old. They were trained for this work published in February 2024:\n\n[MobileLLM: Optimizing Sub\\-billion Parameter Language Models for On\\-Device Use Cases](https://arxiv.org/abs/2402.14905)\n\nLearn everything you need about using and fine\\-tuning Large Language Models with my new book “LLMs on a Budget”:\n\n\n"},{"lang":"en","group":"blog","slug":"blog/the-6-best-llm-tools-to-run-models-locally-eedd0f7c2bbd","frontmatter":{"title":"The 6 Best LLM Tools To Run Models Locally","meta_title":"The 6 Best LLM Tools To Run Models Locally","description":"Running large language models (LLMs) like ChatGPT and Claude usually involves sending data to servers managed by OpenAI and other AI model…","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*2MB6-INUUGLR0NR_iOACIg.jpeg","categories":["Technology","Programming","Health"],"author":"Rifx.Online","tags":["LLM","local","deployment","customization","telehealth"],"draft":false,"slug":"blog/the-6-best-llm-tools-to-run-models-locally-eedd0f7c2bbd"},"content":"\n\n\n\n\n\nRunning large language models (LLMs) like [ChatGPT](https://openai.com/chatgpt/mac/) and [Claude](https://claude.ai/) usually involves sending data to servers managed by [OpenAI](https://openai.com/) and other AI model providers. While these services are secure, some businesses prefer to keep their data entirely offline for greater privacy.\n\nThis article covers the top six tools developers can use to run and test LLMs locally, ensuring their data never leaves their devices, similar to how [end-to-end encryption](https://getstream.io/blog/end-to-end-encryption/) protects privacy.\n\n\n## Why Use Local LLMs?\n\nA tool like [LM Studio](https://lmstudio.ai/) does not collect user data or track users’ actions when they use it to run local LLMs. It lets all your chat data stay on your local machine without sharing with an AI/ML server.\n\n* **Privacy**: You can prompt local LLMs in a multi-turn manner without your prompt data leaving your localhost.\n* **Customization Options**: Local LLMs provide advanced configurations for CPU threads, temperature, context length, GPU settings, and more. This is similar to OpenAI’s playground.\n* **Support and Security**: They provide similar support and security as OpenAI or Claude.\n* **Subscription and Cost**: These tools are free to use and they do not require monthly subscription. For cloud services like OpenAI, each API request requires payment. Local LLMs help to save money since there are no monthly subscriptions.\n* **Offline Support**: You can load and connect with large language models while offline.\n* **Connectivity**: Sometimes, connecting to a cloud service like OpenAI may result in poor signal and connection.\n\n\n## Top Six and Free Local LLM Tools\n\nDepending on your specific use case, there are several offline LLM applications you can choose. Some of these tools are completely free for personal and commercial use. Others may require sending them a request for business use. There are several local LLM tools available for Mac, Windows, and Linux. The following are the six best tools you can pick from.\n\n\n## 1. LM Studio\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*svbQPZKu08of7Kv6)\n\n[LM Studio](https://lmstudio.ai/) can run any model file with the format `gguf`. It supports `gguf` files from model providers such as [Llama 3.1](https://llama.meta.com/), [Phi 3](https://huggingface.co/docs/transformers/main/en/model_doc/phi3), [Mistral](https://mistral.ai/), and [Gemma](https://ai.google.dev/gemma). To use LM Studio, visit the link above and download the app for your machine. Once you launch LM Studio, the homepage presents top LLMs to download and test. There is also a search bar to filter and download specific models from different AI providers.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*sbS3VqiLgDsftgs2)\n\nSearching for a model from a specific company presents several models, ranging from small to large [quantization](https://huggingface.co/docs/optimum/en/concept_guides/quantization). Depending on your machine, LM Studio uses a compatibility guess to highlight the model that will work on that machine or platform.\n\n\n## Key Features of LM Studio\n\nLM Studio provides similar functionalities and features as ChatGPT. It has several functions. The following highlights the key features of LM Studio.\n\n* **Model Parameters Customization**: This allows you to adjust temperature, maximum tokens, frequency penalty, and more.\n* **Chat History**: Allows you to save prompts for later use.\n Parameters and UI Hinting: You can hover on info buttons to lookup model parameters and terms.\n* **Cross-platform**: LM Studio is available on Linux, Mac, and Windows operating systems.\n* **Machine Specification Check**: LM studio checks computer specifications like GPU and memory and reports on compatible models. This prevents downloading a model that might not work on a specific machine.\n* **AI Chat and Playground**: Chat with a large language model in a multi-turn chat format and experiment with multiple LLMs by loading them concurrently.\n* **Local Inference Server for Developers**: Allows developers to set up a local HTTP server similar to OpenAI’s API.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*9bHmRiOSf6gm-u3P)\n\nThe local server provides sample Curl and Python client requests. This feature helps to build an AI application using LM Studio to access a particular LLM.\n\n\n```python\n## Example: reuse your existing OpenAI setup\nfrom openai import OpenAI\n\n## Point to the local server\nclient = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n\ncompletion = client.chat.completions.create(\n  model=\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"Always answer in rhymes.\"},\n    {\"role\": \"user\", \"content\": \"Introduce yourself.\"}\n  ],\n  temperature=0.7,\n)\n\nprint(completion.choices[0].message)\n```\nWith the above sample Python code, you can reuse an existing OpenAI configuration and modify the base url to point to your localhost.\n\n* **OpenAI’s Python Library Import**: LM Studio allows developers to import the OpenAI Python library and point the base URL to a local server (localhost).\n* **Multi-model Session**: Use a single prompt and select multiple models to evaluate.\n\n\n## Benefits of Using LM Studio\n\nThis tool is free for personal use and it allows developers to run LLMs through an in-app chat UI and playground. It provides a gorgeous and easy to use interface with filters and supports connecting to OpenAI’s Python library without the need for an API key. Companies and businesses can use LM studio on request. However it requires a M1/M2/M3 Mac or higher, or a Windows PC with a processor that supports [AVX2](https://edc.intel.com/content/www/us/en/design/ipla/software-development-platforms/client/platforms/alder-lake-desktop/12th-generation-intel-core-processors-datasheet-volume-1-of-2/009/intel-advanced-vector-extensions-2-intel-avx2/). Intel and [AMD](https://www.amd.com/en/support/download/drivers.html) users are limited to using the [Vulkan inference engine in v0.2.31](https://lmstudio.ai/).\n\n\n## 2. Jan\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*7YeH_48iFYB4lDRu)\n\nThink of [Jan](https://jan.ai/) as an open-source version of ChatGPT designed to operate offline. It is built by a community of users with a user-owned philosophy. Jan allows you to run popular models like [Mistral](https://huggingface.co/models?other=mistral) or [Llama](https://huggingface.co/models?other=llama) on your device without connecting it to the internet. With Jan, you can access remote APIs like OpenAI and [Groq](https://groq.com/).\n\n\n## Key Features of Jan\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*ufyOE6QkcHw8X5U7)\n\nJan is an electron app with features similar to LM Studio. It makes AI open and accessible to all by turning consumer machines into AI computers. Since it is an open source project, developers can contribute to it and extend its functionalities. The following breaksdown the major features of Jan.\n\n* **Local**: You can run your preferred AI models on devices without connecting them to the internet.\n* **Ready to Use Models**: After downloading Jan, you get a set of already installed models to start. There is also a possibility to search for specific models.\n* **Model Import**: It supports importing models from sources like Hugging Face.\n* **Free, Cross-Platform and Open Source**: Jan is 100% free, open source, and works on Mac, Windows, and Linux.\n* **Customize Inference Parameters**: Adjust model parameters such as Maximum token, temperature, stream, frequency penalty, and more. All preferences, model usage, and settings stay locally on your computer.\n* **Extensions**: Jan supports extensions like [TensortRT](https://github.com/NVIDIA/TensorRT) and [Inference Nitro](https://huggingface.co/jan-hq/nitro-v1.2-e3) for customizing and enhancing your AI models.\n\n\n## Benefits of Using Jan\n\nJan provides a clean and simple interface to interact with LLMs and it keeps all your data and processing information locally. It has over seventy large language models already installed for you to use. The availability of these ready-to-use models makes it easy to connect and interact with remote APIs like OpenAI and Mistral. Jan also has a great [GitHub](https://github.com/janhq/jan), [Discord](https://discord.gg/FTk2MvZwJH), and [Hugging Face](https://huggingface.co/janhq) communities to follow and ask for help. However, like all the LLM tools, the models work faster on Apple Silicon Macs than on Intel ones.\n\n\n## 3. Llamafile\n\n[Llamafile](https://github.com/Mozilla-Ocho/llamafile) is backed by [Mozilla](https://www.mozilla.org/en-US/?v=1) whose aim is to support and make open source AI accessible to everyone using a fast [CPU inference](https://huggingface.co/docs/transformers/en/perf_infer_cpu) with no network access. It converts LLMs into multi-platform [Executable Linkable Format](https://gist.github.com/x0nu11byt3/bcb35c3de461e5fb66173071a2379779) (ELF). It provides one of the best options to [integrate AI](https://getstream.io/chat/solutions/ai-integration/) into applications by allowing you to run LLMs with just a single executable file.\n\n\n## How Llamafile Works\n\nIt is designed to convert weights into several executable programs that require no installation to run on architectures such as Windows, MacOS, Linux, Intel, ARM, FreeBSD, and more. Under the hood, Llamafile uses [tinyBLAST](https://github.com/ggerganov/llama.cpp/issues/5048) to run on OSs like Windows without requiring an SDK.\n\n\n## Key Features of Llamafile\n\n* **Executable File**: Unlike other LLM tools like LM Studio and Jan, Llamafile requires only one executable file to run LLMs.\n* **Use Existing Models**: Llamafile supports using existing models tools like Ollama and LM Studio.\n* **Access or Make Models**: You can access popular LLMs from OpenAI, Mistral, Groq, and more. It also provides support for creating models from scratch.\n* **Model File Conversion**: You can convert the file format of many popular LLMs, for example, `.gguf` into `.llamafile` with a single command.\n\n`llamafile-convert mistral-7b.gguf`\n\n\n## Get Started With Llamafile\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*4PV1KsCZvvVKqFll)\n\nTo install Llamafile, head to the Huggingface website, select **Models** from the navigation, and search for **Llamafile**. You can also install your preferred [quantized](https://huggingface.co/docs/optimum/en/concept_guides/quantization) version from the URL below.\n\n[`https://huggingface.co/Mozilla/Meta-Llama-3.1-8B-Instruct-llamafile/tree/m`ain](https://huggingface.co/Mozilla/Meta-Llama-3.1-8B-Instruct-llamafile/tree/main)\n\n**Note**: The larger the quantization number, the better the response. As highlighted in the image above, this article uses `Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile` where `Q6` represents the quantization number.\n\n**Step 1: Download Llamafile**\n\nFrom the link above, click any of the download buttons to get your preferred version. If you have the [wget](https://www.gnu.org/software/wget/) utility installed on your machine, you can download Llamafile with the command below.\n\n`wget <https://huggingface.co/Mozilla/Meta-Llama-3.1-8B-Instruct-llamafile/blob/main/Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile>`\n\nYou should replace the URL with the version you like.\n\n**Step 2: Make Llamafile Executable**\n\nAfter downloading a particular version of Llamafile, you should make it executable using the following command by navigating to the file’s location.\n\n`chmod +x Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile`**Step 3: Run Llamafile**\n\nPrepend a period and forward slash `./` to the file name to launch Llamafile.\n\n`./Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile`\n\nThe Llamafile app will now be available at `http://127.0.0.1:8080` to run your various LLMs.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*1xrwDPTfNgmEQDTx)\n\n\n## Benefits of Using Llamafile\n\nLlamafile helps to democratize AI and ML by making LLMs easily reachable to consumer CPUs. As compared to other local LLM apps like **Llama.cpp**, Llamafile gives the fastest prompt processing experience and better performance on gaming computers. Since it has a faster performance, it is an excellent option for summarizing long text and large documents. It runs 100% offline and privately, so users do not share their data to any AI server or API. Machine Learning communities like Hugging Face supports the Llamafile format, making it easy to search for Llamafile related models. It also has a great open source community that develops and extends it further.\n\n\n## 4. GPT4ALL\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*j3vNWWQZCVF5woo5)\n\nGPT4ALL is built upon privacy, security, and no internet-required principles. Users can [install](https://www.nomic.ai/gpt4all) it on Mac, Windows, and Ubuntu. Compared to Jan or LM Studio, GPT4ALL has more monthly downloads, [GitHub Stars](https://github.com/nomic-ai/gpt4all), and active users.\n\n\n## Key Features of GPT4ALL\n\nGPT4All can run LLMs on major consumer hardware such as Mac M-Series chips, AMD and NVIDIA GPUs. The following are its key features.\n\n* **Privacy First**: Keep private and sensitive chat information and prompts only on your machine.\n* **No Internet Required**: It works completely offline.\n* **Models Exploration**: This feature allows developers to browse and download different kinds of LLMs to experiment with. You can select about 1000 open-source language models from popular options like LLama, Mistral, and more.\n* **Local Documents**: You can let your local LLM access your sensitive data with local documents like `.pdf` and `.txt` without data leaving your device and without a network.\n* **Customization options**: It provides several [chatbot](https://getstream.io/blog/llm-chatbot-docs/) adjustment options like temperature, batch size, context length, etc.\n* **Enterprise Edition**: GPT4ALL provides an enterprise package with security, support, and per-device licenses to bring local AI to businesses.\n\n\n## Get Started With GPT4All\n\nTo start using GPT4All to run LLMs locally, [Download](https://www.nomic.ai/gpt4all) the required version for your operating system.\n\n\n## Benefits of Using GPT4ALL\n\nWith the exception of Ollama, GPT4ALL has the most significant number of GitHub contributors and about 250000 monthly active users (according to <https://www.nomic.ai/gpt4all>) and compared to its competitors. The app collects anonymous user data about usage analytics and chat sharing. However, users have the options to opt in or out. Using GPT4ALL, developers benefit from its large user base, GitHub, and Discord communities.\n\n\n## 5. Ollama\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*STAonWgWIsY6cgDR)\n\nUsing [Ollama](https://ollama.com/), you can easily create local chatbots without connecting to an API like OpenAI. Since everything runs locally, you do not need to pay for any subscription or API calls.\n\n\n## Key Features of Ollama\n\n* **Model Customization**: Ollama allows you to convert `.gguf` model files and run them with `ollama run modelname`.\n* **Model Library**: Ollama has a large collection of models to try at [ollama.com/library](https://ollama.com/library).\n* **Import Models**: Ollama supports importing models from [PyTorch](https://pytorch.org/).\n* **Community Integrations**: Ollama integrates seamlessly into web and desktop applications like, [Ollama-SwiftUI](https://github.com/kghandour/Ollama-SwiftUI), [HTML UI](https://github.com/rtcfirefly/ollama-ui), [Dify.ai](https://github.com/rtcfirefly/ollama-ui), and [more](https://github.com/ollama/ollama?tab=readme-ov-file#community-integrations).\n* **Database Connection**: Ollama supports several [data platforms](https://github.com/mindsdb/mindsdb/blob/main/mindsdb/integrations/handlers/ollama_handler/README.md).\n* **Mobile Integration**: A SwiftUI app like [Enchanted](https://github.com/AugustDev/enchanted) brings Ollama to iOS, macOS, and visionOS. [Maid](https://github.com/Mobile-Artificial-Intelligence/maid) is also a cross-platform Flutter app that interfaces with `.gguf`model files locally.\n\n\n## Get Started With Ollama\n\nTo use Ollama for the first time, visit <https://ollama.com> and download the version for your machine. You can install it on Mac, Linux, or Windows. Once you install Ollama, you can check its detailed information in Terminal with the following command.\n\n`ollama`\n\nTo run a particular LLM, you should download it with:\n\n`ollama pull modelname`, where `modelname` is the name of the model you want to install. Checkout Ollama on [GitHub](https://github.com/ollama/ollama) for some example models to download. The `pull` command is also used for updating a model. Once it is used, only the difference will be fetched.\n\nAfter downloading for example, `llama3.1`, running `ollama run llama3.1` in the command line launches the model.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*aglZm6h0BU6GAYkSl04XWA.gif)\n\nIn the above example, we prompt the `llama3.1` model to solve a Physics work and energy question.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*dNNQYpz1s2tz1pcn)\n\n\n## Benefits of Using Ollama\n\nOllama has over 200 contributors on GitHub with active updates. It has the largest number of contributors and is more extendable among the other open-source LLM tools discussed above.\n\n\n## 6. LLaMa.cpp\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*KhsAUquhDZAHghxK)\n\n[LLaMa.cpp](https://github.com/ggerganov/llama.cpp) is the underlying backend technology (inference engine) that powers local LLM tools like Ollama and many others. Llama.cpp supports significant large language model inferences with minimal configuration and excellent local performance on various hardware. It can also run in the cloud.\n\n\n## Key Features of LLaMa.cpp\n\n* **Setup**: It has a minimal setup. You install it with a single command.\n* **Performance**: It performs very well on various hardware locally and in the cloud.\n* **Supported Models**: It supports popular and major LLMs like [Mistral 7B](https://huggingface.co/mistralai/Mistral-7B-v0.1), [Mixtral MoE](https://huggingface.co/models?search=mistral-ai/Mixtral), [DBRX](https://huggingface.co/databricks/dbrx-instruct), [Falcon](https://huggingface.co/models?search=tiiuae/falcon), and [many others](https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#description).\n* **Frontend AI Tools**: LLaMa.cpp supports open-source LLM UI tools like [MindWorkAI/AI-Studio](https://github.com/MindWorkAI/AI-Studio) (FSL-1.1-MIT), [iohub/collama](https://github.com/iohub/coLLaMA), etc.\n\n\n## Get Started With LLaMa.cpp\n\nTo run your first local large language model with llama.cpp, you should install it with:\n\n`brew install llama.cpp`\n\nNext, download the model you want to run from Hugging Face or any other source. For example, download the model below from Hugging Face and save it somewhere on your machine.\n\n[`https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q4_K_M.g`guf](https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf)\n\nUsing your preferred command line tool like Terminal, `cd` into the location of the `.gguf` model file you just downloaded and run the following commands.\n\n\n```python\nllama-cli --color \\ \n-m Mistral-7B-Instruct-v0.3.Q4_K_M.ggufb \\ \n-p \"Write a short intro about SwiftUI\"\n```\nIn summary, you first invoke the LLaMa CLI tool and set color and other flags. The `-m` flag specifies the path of the model you want to use. The `-p` flag specifies the prompt you wish to use to instruct the model.\n\nAfter running the above command, you will see the result in the following preview.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*4Al-j50vXUXLUfvxzBt6aw.gif)\n\n\n## Local LLMs Use Cases\n\nRunning LLMs locally can help developers who want to understand their performance and how they work in detail. Local LLMs can query private documents and technical papers so that information about these documents does not leave the devices used to query them to any cloud AI APIs. Local LLMs are useful in no-internet locations and places where network reception is poor.\n\nIn a [telehealth setting](https://getstream.io/blog/telemedicine-app-development/), local LLMs can sort patient documents without having to upload them to any AI API provider due to privacy concerns.\n\n\n## Evaluating LLMs’ Performance To Run Locally\n\nKnowing the performance of a large language model before using it locally is essential for getting the required responses. There are several ways you can determine the performance of a particular LLM. Here are a few ways.\n\n* **Training**: What dataset is the model trained on?\n* **Fine-tuning**: To what extent can the model be customized to perform a specialized task or can it be fine-tuned to for a specific domain?.\n* **Academic Research**: Does the LLM have an academic research paper?\n\nTo answer the above questions, you can check excellent resources like [Hugging Face](https://huggingface.co/datasets) and [Arxiv.org](https://arxiv.org/). Also, [Open LLm Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) and [LMSYS Chatbot Arena](https://chat.lmsys.org/?arena) provide detailed information and benchmarks for varieties of LLMs.\n\n\n## Local LLM Tools Conclusion\n\nAs discussed in this article, several motives exist for choosing and using large language models locally. You can fine-tune a model to perform a specialized task in a [telemedicine app](https://getstream.io/chat/solutions/healthcare/) if you do not wish to send your dataset over the internet to an AI API provider. Many open-source Graphic User Interface (GUI-based) local LLM tools like LLm Studio and Jan provide intuitive front-end UIs for configuring and experimenting with LLMs without subscription-based services like OpenAI or Claude. You also discovered the various powerful command-line LLM applications like Ollama and LLaMa.cpp that help you run and test models locally and without an internet connection. Check out Stream’s [AI Chatbot](https://getstream.io/chat/solutions/ai-integration/) solution to integrate an AI chat into your app and visit all the related links to learn more.\n\n*Originally published at [https://getstream.io](https://getstream.io/blog/best-local-llm-tools/).*\n\n\n"},{"lang":"en","group":"blog","slug":"blog/the-quest-for-production-quality-graph-rag-easy-to-start-hard-to-finish-46ca404cee3d","frontmatter":{"title":"The Quest for Production-Quality Graph RAG: Easy to Start, Hard to Finish","meta_title":"The Quest for Production-Quality Graph RAG: Easy to Start, Hard to Finish","description":"Overcoming the challenges of productionizing graph RAG","date":"2024-11-01T03:56:04.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*RMudHNmBOgXM1Mubj1UTkw.jpeg","categories":["Programming","Data Science","Generative AI"],"author":"Rifx.Online","tags":["graph","RAG","production","uncertainty","optimization"],"draft":false,"slug":"blog/the-quest-for-production-quality-graph-rag-easy-to-start-hard-to-finish-46ca404cee3d"},"content":"\n\n\n\n\n### Overcoming the challenges of productionizing graph RAG\n\n\n\nWhen I read the recent article in VentureBeat about how Glean [just secured over $260 million in its latest funding round](https://venturebeat.com/data-infrastructure/how-to-take-advantage-of-a-generative-tool-fueling-gleans-260m-raise-graph-rag/), I had two immediate gut feelings. First, it was satisfying to see this very public example of graph RAG living up to its potential as a powerful, valuable technology that connects people with knowledge more efficiently than ever. Second, it felt surprising but validating to read:\n\n\n> One of the world’s largest ride\\-sharing companies experienced its benefits firsthand. After dedicating an entire team of engineers to develop a similar in\\-house solution, they ultimately decided to transition to Glean’s platform.\n\n\n> “Within a month, they were seeing twice the usage on the Glean platform because the results were there,” says Matt Kixmoeller, CMO at Glean.\n\nAlthough I was surprised to read about the failure in a news article, struggling to bring graph RAG into production is what I would expect, based on my experience as well as the experiences of coworkers and customers. I’m not saying that I expect large tech companies to fail at building their own graph RAG system. **I merely expect that most folks will struggle to build out and productionize graph RAG — even if they already have a very successful proof\\-of\\-concept.**\n\nI wrote a [high\\-level reaction to the VentureBeat article in The New Stack](https://bit.ly/4fjIlgJ), and in this article, I’d like to dive deeper into why graph RAG can be so hard to get right. First, I’ll note how easy it has become, using the latest tools, to get started with graph RAG. Then, I’ll dig into some of the specific challenges of graph RAG that can make it so difficult to bring from R\\&D into production. Finally, I’ll share some tips on how to maximize your chances of success with graph RAG.\n\n\n## Getting started with graph RAG is easy\n\nSo if a big ride\\-sharing company couldn’t build their own platform effectively, then why would I say that it’s easy to implement graph RAG yourself?\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*l6EiwfjeUGLjVlYeiY1lqA.jpeg)\n\nWell, first of all, technologies supporting RAG and graph RAG have come a long way in the past year. Twelve months ago, most enterprises hadn’t even heard of retrieval\\-augmented generation. Now, not only is RAG support [a key feature of the best AI\\-building tools like LangChain](https://python.langchain.com/docs/tutorials/rag/), but just about every major player in the AI space has a RAG tutorial, and [there is even a Coursera course](https://www.coursera.org/projects/introduction-to-rag). There is no shortage of quick entry points for trying RAG.\n\nMicrosoft may not have been the first to do graph RAG, but they gave the concept a big push with a [research blog post earlier this year](https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/), and they continue to work on related tech.\n\nHere on Medium, there is also a nice conceptual introduction, with some technical details, [from a gen AI engineer at Google](https://towardsdatascience.com/graph-rag-a-conceptual-introduction-41cd0d431375). And, in Towards Data Science, there is a recent and very thorough [how\\-to article on building a graph RAG system](https://towardsdatascience.com/how-to-implement-graph-rag-using-knowledge-graphs-and-vector-databases-60bb69a22759) and testing on a dataset of scientific publications.\n\nAn established name in traditional graph databases and analytics, Neo4j, added vector capabilities to their flagship graph DB product in response to the recent gen AI revolution, and they have an excellent platform of tools for projects that require sophisticated graph analytics and deep graph algorithms in addition to standard graph RAG capabilities. They also have a [Getting Started With Graph RAG](https://neo4j.com/developer-blog/graphrag-ecosystem-tools/) guide.\n\nOn the other hand, [you don’t even need a graph DB to do graph RAG](https://bit.ly/3YD5NAd). Many folks who are new to graph RAG believe that they need to deploy a specialized graph DB, but this is not necessary, and in fact may simply complicate your tech stack.\n\nMy employer, DataStax, also has a [Guide to Graph RAG](https://bit.ly/4862Lrl).\n\nAnd, of course, the two most popular gen AI application composition frameworks, [LangChain](https://blog.langchain.dev/enhancing-rag-based-applications-accuracy-by-constructing-and-leveraging-knowledge-graphs/) and [LlamaIndex](https://docs.llamaindex.ai/en/stable/examples/cookbooks/GraphRAG_v1/), each have their own graph RAG introductions. And there’s [a DataCamp article](https://www.datacamp.com/tutorial/knowledge-graph-rag) that uses both.\n\nWith all of the tools and tutorials available, getting started with graph RAG is the easy part…\n\n\n## …but bringing graph RAG into production is hard\n\nThis is a very old story in data science: a new software methodology, technology, or tool solves some imposing problem in a research context, but industry struggles to build it into products that deliver value on a daily basis. It’s not just an issue of effort and proficiency in software development — even the biggest, best, and brightest teams might not be able to overcome the uncertainty, unpredictability, and uncontrollability of real\\-world data involved in solving real\\-world problems.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*OklHNrhsNHZF6qzeRUSd_w.jpeg)\n\nUncertainty is an inherent part of building and using data\\-centric systems, which almost always have some elements of stochasticity, probability, or unbounded inputs. And, uncertainty can be even greater when inputs and outputs are unstructured, which is the case with natural language inputs and outputs of LLMs and other GenAI applications.\n\nFolks who want to try graph RAG typically already have an existing RAG application that performs well for simple use cases, but fails on some of the more complex use cases and prompts requiring multiple pieces of information across a knowledge base, potentially in different documents, contexts, formats, or even data stores. When all of the information needed to answer a question is in the knowledge base, but the RAG system isn’t finding it, it seems like a failure. And from a user experience (UX) perspective, it is — the correct answer wasn’t given.\n\nBut that doesn’t necessarily mean there is a “problem” with the RAG system, which might be performing exactly as it was designed. If there isn’t a problem or a bug, but we still aren’t getting the responses we want, that must mean that we are expecting the RAG system to have a capability it simply doesn’t have.\n\nBefore we look at why specifically graph RAG is hard to bring into production, let’s take a look at the problem we’re trying to solve.\n\n\n## The main challenge that graph RAG addresses\n\nBecause plain RAG systems (without knowledge graphs) retrieve documents based solely on vector search, only documents that are most semantically similar to the query can be retrieved. Documents that are not semantically similar at all — or not quite similar enough — are left out and are not generally made available to the LLM generating a response to the prompt at query time.\n\nWhen the documents we need to answer a question in a prompt are not all semantically similar to the prompt, one or more of them is often missed by a RAG system. This can happen when answering the question requires a mix of generalized and specialized documents or terms, and when documents are detail\\-dense in the sense that some very important details for this specific prompt are buried in the middle of related details that aren’t as relevant to this prompt. See [this article for an example of RAG missing documents](https://bit.ly/3BKZAJv) because two related concepts (“Space Needle” and “Lower Queen Anne neighborhood” in this case) are not semantically similar, and [see this article for an example of important details getting buried](https://bit.ly/4ffhrqi) in detail\\-dense documents because vector embeddings are “lossy”.\n\nWhen we see retrieval “failing” to find the right documents, it can be tempting to try to make vector search better or more tailored to our use case. But this would require fiddling with embeddings, and embeddings are complicated, messy, expensive to calculate, and even more expensive to fine\\-tune. Besides, that wouldn’t even be the best way to solve the problem.\n\nFor example, looking at the example linked above, would we really want to use an embedding algorithm that puts the text “Space Needle” and “Lower Queen Anne neighborhood” close together in semantic vector space? No, fine\\-tuning or finding an embedding algorithm that puts those two terms very close together in semantic space would likely have some unexpected and undesired side effects.\n\nIt is better not to try to force a semantic model to do a job that geographical or tourism information would be much better suited for. If I were a travel or tourism company who relied on knowing which neighborhood such landmarks are in, I would rather build a database that knows these things with certainty — a task that is much easier than making semantic vector search do the same task… without complete certainty.\n\nSo, the main issue here is that we have concepts and information that we know are related in some way, but not in semantic vector space. Some other (non\\-vector) source of information is telling us that there are connections among the wide variety of concepts we are working with. The task of building a graph RAG application is to effectively capture these connections between concepts into a knowledge graph, and to use the graph connections to retrieve more relevant documents for responding to a prompt.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*flPVNMUm83oc7H9Lt7U5AA.jpeg)\n\nTo summarize the issue that we’re trying to tackle with graph RAG: there exists semi\\-structured, non\\-semantic information connecting many of the concepts that appear in my unstructured documents — and I would like to use this connection information to complement semantic vector search in order to retrieve documents that are best suited to answer prompts and questions within my use cases. We simply want to make retrieval better, and we want to use some external information or external logic to accomplish that, instead of relying solely on semantic vector search to connect prompts with documents,\n\n\n## Guiding principles for integrating graph with RAG\n\nConsidering the above motivation — to use “external” information to make document connections that semantic search misses — there are some guiding principles that we can keep in mind while building and testing a graph RAG application:\n\n1. The graph should contain high\\-quality, meaningful concepts and connections\n2. Concepts and connections should be relevant to prompts within the set of use cases\n3. Graph connections should complement, not replace, vector search\n4. The usefulness of one\\- and two\\-step graph connections should be prioritized; relying on more than three steps to make connections should be reserved only for specialized use cases.\n\nPerhaps in a future article, we will dig into the nuances and potential impacts of following these principles, but for now, I’ll just note that this list is intended to jointly increase explainability, prevent over\\-complexity, and maximize efficiency of both building and using a graph RAG system.\n\nFollowing these principles along with other core principles from software engineering and data science can increase your chances of successfully building a useful and powerful graph RAG app, but there are certainly pitfalls along the way, which we outline in the next section.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*twgres708JPQHa1uZrkwDA.jpeg)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*5U0k4GoHTFiQhKM2a6xdMA.jpeg)\n\n\n## Reasons that your graph RAG app might not make it into production\n\nAnyone who has spent a lot of time building software around data, complex algorithms, statistics, and human users probably understands that there is a lot of uncertainty in building a system like graph RAG. Unexpected things can happen during data prep and loading, while building a knowledge graph, while querying and traversing the graph, during results compilation and prompt construction, and at virtually any other point in the workflow.\n\nAbove, we discussed how it’s easy to implement graph RAG to get preliminary results, but it can be hard to get good results, much less production\\-quality results. Next, we look at a few potential issues that you might encounter when building and testing a graph RAG application.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*1J9hwwZDuYZ3WNrxCl_cOA.jpeg)\n\n\n### Graph RAG isn’t doing much better than plain RAG\n\nIf the performance of your graph RAG system is about the same as with plain RAG, there can be any number of causes. Generally speaking, this seems to imply that the graph is not adding value to the system, but this could be caused by a low\\-quality knowledge graph, under\\-utilization of the graph, sub\\-optimal parameter settings, or many others. Or, there may not be a problem at all; vector search may be doing an excellent job of finding the right documents, and a graph simply isn’t needed.\n\nWhat to look at:\n\n* Do you have example prompts that plain RAG doesn’t handle well, but you would expect graph RAG to succeed? Can you “debug” on these prompts and see what is happening under the hood?\n* Does the knowledge graph contain meaningful connections that semantic search may not make? Can you find examples of concept pairs connected in the graph whose associated documents are far apart in vector space? The KG should be making meaningful connections between “far away” docs.\n\n\n### You (still) see hallucinations\n\nIf you’re seeing hallucinations with graph RAG that you didn’t see with plain RAG, I would suspect a bug or a bad parameter setting somewhere. If you are seeing a similar level of hallucinations, this sounds like a general problem beyond the graph aspects.\n\nWhat to look at:\n\n* Does your document set contain the correct responses to the prompts that elicited hallucinations? Is vector search finding these documents?\n* Are the correct responses from the retrieved documents properly inserted into the context of the prompt that is passed to the LLM?\n\n\n### The graph is “too big”\n\nWhen your knowledge graph is “too big” or too dense, two main types of problems can occur. First, there could be issues with scaling, which I discuss below. Second, graph traversal could result in “too many” documents, which must then be re\\-ranked and filtered. If the re\\-ranking and filtering strategy doesn’t play well with the retrieval and graph traversal elements, you could end up filtering out important documents immediately after your graph just discovered them.\n\nWhat to look at:\n\n* How many documents are returned after graph traversal, and how many are re\\-ranked or filtered out? Does it look like documents found via strong graph connections are surviving filtering?\n* Did you build a knowledge graph filled with meaningful connections that suit your use cases? In the graph, can you find many concepts or connections that are too generic or irrelevant for your use cases? How much of your knowledge graph is made up of low\\-quality information?\n\n\n### The graph is “too small”\n\nPer above, if the graph is “too big”, it might be filled with low\\-quality connections. And if the graph is “too small”, I would hope that the connections there are meaningful, which is good, but missing connections come in two main types. The first is caused by a bug in the graph construction process. The second is caused by graph construction that was not designed for it. Data in a different contexts or different formats may be processed differently by different graph\\-construction methods.\n\nWhat to look at:\n\n* Did you build your knowledge graph using an LLM with entity/keyword extraction? Are you capturing all of the meaningful entities from every document, or is the LLM limiting its output?\n* In your documents, what are some concepts and connections that you would expect to be in the knowledge graph, but seem to be missing? When and how do you expect them to be added to the graph? Why aren’t they actually being added to the graph?\n\n\n### You can’t find the “happy medium” graph\n\nDo you feel like you can build a graph that is “too big” or one that is “too small”, but you can’t build something in the middle?\n\nWhat to look at:\n\n* What parameters or methods are you changing to go from small to big or back again? Should these be affecting graph quality this much? Can you study some graph elements that appear or disappear unexpectedly, depending on which graph construction settings you’re using?\n* Also see relevant tips in “big” and “small” sections above.\n\n\n### Your implementation requires new software or increased deployment complexity\n\nThis is a classic Data Science problem: build really cool and cutting\\-edge methods, only to see development teams refuse or struggle to bring the code from your notebooks into the production stack. Sticking to the most popular, best supported, and largely open\\-source tools can make it easier to get to production, especially if your organization is already using those tools elsewhere.\n\nWhat to look at:\n\n* Does your implementation require creating a new data store for graphs? You [probably don’t need a graph DB](https://www.datastax.com/blog/knowledge-graphs-for-rag-without-a-graphdb), and might be able to use your production vector store for graphs as well.\n* Are you using some of the most popular open\\-source tools for building AI applications, like LangChain? These can reduce code complexity, make the app more portable, and expand potential integrations and further development.\n\n\n### Your implementation doesn’t scale\n\nThe article [Scaling Knowledge Graphs by Eliminating Edges](https://thenewstack.io/scaling-knowledge-graphs-by-eliminating-edges/) in *The New Stack* shows one way to make graph RAG very scalable. Like above, the most popular, best supported, and largely open\\-source tools are usually the best path to painless scaling, but it’s not always easy.\n\nWhat to look at:\n\n* Which part isn’t scaling? Graph traversal, re\\-ranking, results compilation, or something else? See “The graph is too big” above for more tips.\n* Do you have a particular component that isn’t scaling well? Sometimes using an in\\-memory graph library like ‘networkx’ — \\-or even a graph DB — to do complex graph operations can cause a resource bottleneck. You may want to [switch to a more scalable option for graph operations](https://bit.ly/3YD5NAd).\n* Are you using parallel API calls to handle most of the heavy lifting, or are you trying to do complex or costly computations inside the main app logic?\n\n\n## Finding success with graph RAG in production\n\nThe key to creating a successful graph RAG system lies in constructing a knowledge graph and traversal logic that complement semantic vector retrieval, not replacing or competing with it. The graph design should aim to connect the right nodes, knowledge, entities, and documents at the right time, enabling the assembly of the appropriate documents to produce the most helpful and actionable query response.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*orXW5uw-geBo-WVtZUxWXQ.jpeg)\n\nWith respect to Glean, it should be noted that an internal document dataset is a perfect use case for graph RAG. A knowledge graph can connect people, projects, products, customers, meetings, locations, etc — and all of these are somewhat limited in number by the size of the organization and the work it does. Building and managing a graph of thousands of employees is much more tractable than, for example, trying to do the same with all of the people mentioned on Wikipedia or in a large database of financial or legal documents. So, possibly the first great decision that Glean made was to find a great use case for graph RAG to tackle.\n\nOne often understated aspect of graph RAG systems is the quality and reliability of the input data and the pipelines that get it there. This has more to do with data engineering and traditional software development than AI. In previous tech paradigms, connecting different data systems was challenging due to incompatible data types and access methods. Now, AI and LLMs enable the integration of disparate sources of unstructured data, allowing for the consolidation of data from various origins into a single RAG system. This integration capability enables LLMs to process and make sense of unstructured data from various sources, such as internal web pages, wikis, code repositories, databases, Google Docs, and chat logs. Simply connecting all of this information together and making it accessible from a single interface can be a big win.\n\n\n## The way forward\n\nConstruction of graph RAG systems for any use case involves leveraging foundational components such as data stores for vectors and graphs, embeddings, and LLMs, enhanced by open\\-source orchestration tools like LangChain and LlamaIndex. These tools facilitate the development of robust, scalable, and efficient systems, promising a future where companies achieve substantial success by optimizing knowledge work through automation and streamlining.\n\nThe public success of knowledge graphs and graph RAG systems, particularly by companies like Glean, showcases how effective these technologies are for internal use cases, creating value by making the organization more efficient. However, the broader application potential for external, enterprise and consumer\\-facing products remains largely untapped, presenting many opportunities for other companies to explore.\n\nIt is perhaps notable that we have been in what is called the “Information Age” for at least 30 years, and it is only in the past year or two that we have really started to put together the building blocks for connecting all of this information across sources, across ideas, across documents, and across concepts, so that our software systems can make the same types of reasoning, logic, and judgment that we as humans use as a daily part of our knowledge work. Some people are calling this the “Intelligence Age”.\n\nWhile initially focusing on simple, straightforward decisions, AI’s trajectory is set towards managing more complex scenarios, dramatically improving efficiency in both time and cost. This exciting evolution positions many AI applications — including graph RAG — as pivotal in transforming how knowledge is interconnected and utilized in a wide variety of contexts.\n\nTo get started with graph RAG now, or to learn more, take a look at the [DataStax guide to graph RAG](https://bit.ly/4862Lrl).\n\n*by Brian Godsey, Ph.D. ([LinkedIn](https://bit.ly/4enqFRa)) — mathematician, data scientist and engineer // AI and ML products at [DataStax](https://bit.ly/3NpPujA) // Wrote the book [Think Like a Data Scientist](https://bit.ly/4f5uVES)*\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Il1GrFN6fYN7e_ovExRGPw.jpeg)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*wQvZDIlkOvrYZnbwl0bEPQ.jpeg)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/the-real-reason-openai-abandoned-next-js-for-remix-a4b2622ee9b2","frontmatter":{"title":"The Real Reason OpenAI Abandoned Next.js for Remix","meta_title":"The Real Reason OpenAI Abandoned Next.js for Remix","description":"The surprising reasons behind OpenAI’s move and what it means for the future of web development","date":"2024-11-08T00:25:31.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*bf8ao0JjEiMka6dJqp-hxg.jpeg","categories":["Technology/Web","Programming","Web Development"],"author":"Rifx.Online","tags":["Remix","Next.js","client-side","rendering","scalability"],"draft":false,"slug":"blog/the-real-reason-openai-abandoned-next-js-for-remix-a4b2622ee9b2"},"content":"\n### The surprising reasons behind OpenAI’s move and what it means for the future of web development\n\n\n\n## Introduction to the Transition\n\nOpenAI recently caused a stir in the developer community by moving from Next.js to Remix.\n\nThis unexpected switch left many questioning the rationale behind such a significant change.\n\nBut **can you blame them?**\n\nHere is **what most devs think** of NextJS based on [this](https://www.reddit.com/r/nextjs/comments/1f92jdv/chatgptcom_switched_from_nextjs_to_remix/) reddit discussion:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GCrb_aGjh1nticKNeHh9Bg.png)\n\nThat’s rough.\n\nBut I also asked builders on X,\n\nand they had different opinions when it came to working on smaller projects:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*YmN2pTYaCFXFzb3AMjfoqQ.png)\n\n## Let’s get to the bottom of this\n\nThis exploration isn’t just about understanding OpenAI’s decision but also about **what this could mean for other developers** and the broader tech landscape.\n\nTo understand the rationale, I spent hours analyzing the codebase and tools.\n\nHere are the insights I gained.\n\n## Technical Insights on the Switch\n\nUnderstanding the technical aspects of this transition is key to understanding why OpenAI favored Remix.\n\nWe examined their application architecture to identify the core differences between Next.js and Remix.\n\n### Client Rendering vs. Server Rendering\n\nOpenAI’s application focuses on **client\\-side rendering**, where most processing occurs in the user’s browser.\n\nThis reduces the need for server\\-rendered HTML.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*50dv8sWPbwFp85fQu_dodQ.png)\n\n**Remix** is ideal for these scenarios because it effectively manages client\\-side applications. This choice ensures OpenAI’s users have a smoother, more responsive experience.\n\n### Initial Page Load Process\n\nWhen a user visits the ChatGPT site, **preloaded JavaScript and meta tags** are involved in the initial page load.\n\nThis optimizes the client\\-side rendering process. **Remix** excels in managing these elements, ensuring a smooth and fast initial load.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*65VlHo5RQObk-YGzBlkx3w.png)\n\n## Why This Matters\n\n### Improved User Experience\n\nBy preloading essential scripts and data, users encounter less delay and a more responsive interface from the moment they access the site.\n\n### Efficient Loading\n\nRemix’s capability to handle these preloaded elements means reduced waiting times and an overall faster browsing experience.\n\nBy leveraging these features,\n\n*OpenAI can deliver a more seamless and enjoyable experience for its users right from the start.*\n\n## Diving Deeper On The Key Features of Remix Utilized by OpenAI\n\nOpenAI leverages several key features of Remix to enhance their application.\n\n### Preloading Strategies\n\nRemix preloads essential data and assets, reducing loading times and enhancing performance.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GPH2ZGhT_yfrQYpR2Xhvug.png)\n\nThis strategy ensures that users receive a seamless experience right from the start.\n\n### Data Management with Loaders\n\nRemix’s loader API efficiently gathers all necessary data for the initial render, embedding it directly into the HTML.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*PmCiQNkAJlu2MSFEtpydiw.png)\n\nThis approach eliminates the need for additional client\\-side data fetching, speeding up the rendering process.\n\n## Benefits and Implications of the Move\n\nSwitching to Remix offers several advantages for OpenAI, from performance gains to future development prospects.\n\n### Performance Improvements\n\nBy adopting Remix, OpenAI achieves faster initial load times and smoother client\\-side navigation.\n\nThese performance enhancements contribute to a more responsive and user\\-friendly application.\n\n### Future Prospects with Remix\n\nThe flexibility and efficiency of Remix position OpenAI for future growth and innovation.\n\nAs Remix continues to evolve, OpenAI can leverage its advanced features to stay ahead in the competitive landscape of web development.\n\n### Why This Matters\n\n**Improved User Experience**: Users benefit from quicker page loads and a more fluid browsing experience.\n\n**Efficient Development**: Remix’s capabilities streamline development processes, allowing OpenAI to innovate more rapidly.\n\n**Scalability**: The architecture of Remix supports future enhancements and scaling, ensuring long\\-term viability.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/the-rise-of-the-ai-agent-product-manager-and-ai-agent-engineer-0905f1d30cce","frontmatter":{"title":"The Rise of the AI Agent Product Manager and AI Agent Engineer","meta_title":"The Rise of the AI Agent Product Manager and AI Agent Engineer","description":"Imagine a future where Generative AI doesn’t just respond to queries but proactively solves complex problems across every facet of…","date":"2024-11-04T12:33:53.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*dlJ0a49_lRAPR1tTPs898w.png","categories":["Generative AI","Ethics","Technology"],"author":"Rifx.Online","tags":["Generative","Product","Manager","Engineer","Ethics"],"draft":false,"slug":"blog/the-rise-of-the-ai-agent-product-manager-and-ai-agent-engineer-0905f1d30cce"},"content":"\n\n\n\n\n\nImagine a future where Generative AI doesn’t just respond to queries but proactively solves complex problems across every facet of business. This isn’t science fiction; it’s the rapidly approaching reality of Generative AI agents. These agents are poised to revolutionize companies’ operations and inspire a new wave of innovation, from streamlining supply chains to optimizing product development and transforming customer interactions.\n\nHaving spent over a year building Generative AI applications and agents, I’ve seen firsthand how these technologies can profoundly reshape business processes. AI’s potential is immense, from support agents that handle customer queries with unprecedented efficiency to autonomous agents that drive business operations and decision\\-making. These agents are not merely enhancing existing processes but enabling new ways of working.\n\nFor example, picture an agent that doesn’t just schedule meetings but understands the context of your work, suggests the most impactful attendees, prepares briefing documents, and even proposes agenda items based on recent company developments. Or consider an agent in manufacturing that doesn’t just monitor production lines but predicts maintenance needs, optimizes resource allocation in real\\-time, and collaborates with design teams to suggest product improvements based on production data.\n\nThis AI\\-driven transformation is creating a need for two pivotal roles: the **AI Agent Product Manager** and **the AI Agent Engineer**. These specialists are not just the architects and builders of our AI\\-augmented future, but integral parts of a collaborative team, working at the intersection of business strategy and cutting\\-edge technology.\n\n\n## Introducing the New Roles\n\nThe **AI Agent Product Manager** is a visionary who identifies opportunities for agents to create value, designs their capabilities, and ensures they align with business goals and user needs. They act as translators between the world of business and AI possibilities, orchestrating AI innovation.\n\n**The AI Agent Engineer**, on the other hand, is the technical wizard who brings these agents to life. They design robust architectures, create sophisticated prompts, and ensure the agents are grounded in company data and processes by seamlessly integrating them with various systems and data sources.\n\nSince we are still in the early stages of this technology cycle, these professionals are usually found at specialized AI consultancies or companies developing agent\\-building products like Salesforce. This allows them to bring best practices and industry innovations to each new project.\n\n\n## The AI Agent Product Manager: Orchestrating AI Innovation\n\nAs an Agent Product Manager, you may work on different use cases, such as a sales agent one month and an HR agent the next. Let’s dive into what your role might look like:\n\nAs an Agent Product Manager, lets say you’re tasked with developing an agent for a multinational manufacturing firm. Your first step is to lead a series of workshops with executives from various departments — operations, design, sales, and customer service. You’re not just looking for incremental improvements; you’re hunting for transformative opportunities, and you do this by fostering collaboration and understanding across the organization.\n\nThrough these discussions, you identify a game\\-changing possibility: an agent that can bridge customer feedback, product design, and manufacturing processes. This Agent would analyze customer reviews and support tickets, identify trending issues or desired features, and automatically generate design modification proposals. It would then simulate these changes’ impact on manufacturing processes and costs.\n\nAs a product manager for agents, one of your primary responsibilities will be to map out the agents’ journeys. It involves defining each step from the initial interaction to the final outcome, ensuring that everything aligns with the business goals. You will need to identify the key interactions the Agent will have, understand the context of these interactions, and determine the objectives each journey should achieve. You will also need to consider critical questions such as: How will the Agent prioritize customer feedback? How can it effectively present design suggestions to the engineering team? And what ethical considerations must be addressed when AI influences product decisions?\n\nYou’ll work closely with stakeholders to define success metrics. For example, you may decide that the Agent should aim to reduce the time from identifying a product issue to implementing a fix by 50% while also increasing customer satisfaction scores.\n\nAs the project progresses, you ensure the agent delivers real business value. You might review both simulated and actual conversations between the AI and design teams, tweaking the Agent’s communication style to better resonate with engineers. Or you could pore over data on how the Agent’s suggestions have impacted product quality and customer satisfaction, looking for ways to improve its performance further.\n\nThroughout this process, you’re not just thinking about the Agent’s current capabilities but its future potential. How could this Agent evolve to react to customer feedback and predict future market trends? Could it someday participate in brainstorming sessions with the product team, offering data\\-driven insights to fuel innovation?\n\nYour Agent Product Manager role puts you at the forefront of business transformation. You’re not just implementing a new tool; you’re reshaping how entire organizations think, innovate, and operate in the age of AI.\n\n\n## The AI Agent Engineer: Crafting Intelligent and Reliable Systems\n\nNow, let’s switch gears and step into the role of an Agent Engineer on this same project:\n\nYour challenge is to create an agent that can understand customer feedback, translate it into actionable design insights, and interface with manufacturing systems. This is no small feat — it requires a deep understanding of large language models, sophisticated prompt engineering, and robust system integration.\n\nYou begin by selecting an appropriate large language model as the foundation for your Agent. However, your real work lies in designing a comprehensive agent architecture that can reliably perform across many conversation journeys.\n\nAs an Agent Engineer, one of your primary focuses is creating and refining the Agent’s prompt structure. You craft intricate prompts that effectively guide the model’s behavior, ensuring it consistently provides relevant and accurate responses across various scenarios. This could involve developing a hierarchical prompt system that can handle everything from supervising multiple agents to navigating various journeys.\n\nYou will spend significant time evaluating agent behavior and output, refining the prompts and flows, and publishing a new version. You may even design and implement a rigorous testing framework that simulates thousands of potential conversation trajectories. Your goal is to ensure that the Agent’s response is deterministic and aligns with the desired outcome for any given input.\n\nFor instance, you might create a suite of test cases that cover various types of customer feedback, from simple product issues to complex feature requests. You then methodically work through these cases, analyzing the Agent’s responses and iterating on the prompt structure and decision\\-making logic to improve performance.\n\nWhen you encounter edge cases where the Agent’s behavior is inconsistent or suboptimal, you don’t simply tweak the prompt. Instead, you dive deep into the Agent’s decision\\-making process, adjusting the underlying logic and prompt structure to address these issues systematically.\n\nIntegration remains a crucial part of your role. You’re designing APIs that allow the Agent to pull data from customer support databases, access product design files, and input data into manufacturing planning systems. But beyond just connecting systems, you’re focused on ensuring the Agent can make intelligent decisions based on this integrated data.\n\nEthics and safety continue to be critical concerns. You implement robust safeguards and oversight mechanisms to ensure the AI doesn’t suggest design changes that could compromise product safety. You also build features for explainability, so the AI can always show its reasoning for any suggestion, which is crucial for building trust with the engineers and designers working with the Agent.\n\nYour role as an Agent Engineer involves creating a functional AI system and crafting an intelligent agent that can reliably and consistently drive innovation and efficiency across the entire product development and manufacturing process. This complex challenge puts you at the forefront of AI technology, shaping the future of how businesses operate in the AI age.\n\n\n## Ethical Considerations and the Power of Collaboration\n\nAs agents become more integral to businesses, Agent Product Manager and Agent Engineer roles will only grow in importance. These roles are not just about technical prowess or strategic insight — they demand a deep commitment to ethical considerations. As these agents influence significant business decisions, the professionals behind them must ensure that these systems are transparent, fair, and aligned with broader societal values.\n\nThe success of this agent relies heavily on the seamless collaboration between the Product Manager and Engineer. Together, you’ll iterate on the Agent’s capabilities, troubleshoot issues, and push the boundaries of what’s possible.\n\n\n## Comparing the Roles: Agent Product Manager vs. Agent Engineer\n\nHere’s a summary comparison table to emphasize the differences between the Agent Product Manager and the Agent Engineer:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*T26nkBI-wID26X2NrE2SSQ.jpeg)\n\nKey Takeaways:\n\n* **Agent Product Manager:** This position focuses on the strategic and business aspects of agents, ensuring they deliver value and align with company goals.\n* **Agent Engineer:** This position concentrates on technical implementation, ensuring agents function reliably and integrate seamlessly with existing systems.\n\n\n## The Future is Yours: Rise to the Challenge\n\nAs AI expands its influence, Agent Product Manager and Agent Engineer roles will be at the forefront of this technological revolution. Whether you’re defining the strategy for an AI\\-driven business transformation or designing the intricate systems that power intelligent agents, you’ll be shaping the future of business.\n\nThese roles require a unique blend of skills: strategic thinking, technical expertise, creativity, and a deep understanding of business and AI. They offer the chance to work on cutting\\-edge technology while driving tangible business impact.\n\nSo, future Agent Product Managers and Engineers, are you ready to rise to the challenge? The AI\\-augmented future is waiting for your expertise and vision. Whether you’re drawn to the strategic aspects of product management or the technical intricacies of agent engineering, there’s a place for you in this exciting new field. The question is not if AI will transform business, but how — and you could be the one to decide.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/top-25-generative-ai-terminologies-you-must-know-6a3bb0300988","frontmatter":{"title":"Top 25 Generative AI Terminologies You Must Know","meta_title":"Top 25 Generative AI Terminologies You Must Know","description":"The article presents a comprehensive guide to 25 essential terminologies in generative AI, aimed at enhancing understanding among professionals in technology and related fields. Key concepts include Generative Models, Transformers, GANs, Autoencoders, and various learning paradigms such as Zero-Shot and Few-Shot Learning. Each term is defined with examples and resources for deeper exploration, emphasizing the importance of mastering these concepts for effective engagement in AI projects and discussions. The guide serves as a foundational tool for both newcomers and those seeking to update their knowledge in the rapidly evolving generative AI landscape.","date":"2024-11-14T03:29:09.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*swCl2YJj6wfAc9J3v6AqTg.png","categories":["Generative AI","Machine Learning","Data Science"],"author":"Rifx.Online","tags":["Generative","Transformers","GANs","Autoencoders","Zero-Shot"],"draft":false,"slug":"blog/top-25-generative-ai-terminologies-you-must-know-6a3bb0300988"},"content":"\n\n\n\n*Master the Key Concepts to Excel in Generative AI with Clear Explanations, Real\\-World Applications, and In\\-Depth Resources*\n\n\n\nGenerative AI is indeed a critical technology across industries; therefore, understanding generative AI’s core concepts is crucial for any professional in tech and beyond. The following comprehensive guide covers the top 25 must\\-know generative AI terminologies that will provide you with lucid definitions, practical examples, and other resources that will deepen your knowledge. Whether preparing for interviews, working on AI projects, or keeping yourself abreast of the goings\\-on in this fast\\-changing field, mastering these terms provides you with a strong basis in generative AI.\n\n\n## 1\\. Generative Model\n\n* **Definition**: A type of AI model that generates new data points from learned patterns.\n* **Example**: Generative Pre\\-trained Transformers(GPT) generate human\\-like text based on input prompts.\n* **Learn More**: [Introduction to Generative Models](https://proxy.rifx.online/https://www.datacamp.com/blog/what-is-a-generative-model)\n\n\n## 2\\. Transformer\n\n* **Definition**: A neural network architecture that uses self\\-attention mechanisms to process and generate sequences, like text or images.\n* **Example**: BERT is a Transformer model used for tasks like question\\-answering and text classification.\n* **Learn More**: [Understanding Transformers](https://proxy.rifx.online/https://www.turing.com/kb/brief-introduction-to-transformers-and-their-power)\n\n\n## 3\\. Latent Space\n\n* **Definition**: A multi\\-dimensional space where generative models map data, allowing them to learn and generate variations.\n* **Example**: In image generation, similar images are positioned near each other in the latent space.\n* **Learn More**: [Exploring Latent Space in AI](https://proxy.rifx.online/https://www.perplexity.ai/page/latent-space-101-what-it-is-an-mwXuxYfzS_.J4e_uFvOskg)\n\n\n## 4\\. GAN (Generative Adversarial Network)\n\n* **Definition**: A type of AI that pits two neural networks, a generator and a discriminator, against each other to create realistic data.\n* **Example**: GANs generate realistic\\-looking faces that do not belong to real people.\n* **Learn More**: [What Are GANs and How Do They Work?](https://proxy.rifx.online/https://aws.amazon.com/what-is/gan/#:~:text=A%20generative%20adversarial%20network%20(GAN,from%20a%20database%20of%20songs.)\n\n\n## 5\\. Autoencoder\n\n* **Definition**: A neural network that learns to compress and reconstruct data, often used for tasks like dimensionality reduction and denoising.\n* **Example**: Autoencoders are used to remove noise from corrupted images.\n* **Learn More**: [Introduction to Autoencoders](https://proxy.rifx.online/https://towardsdatascience.com/introduction-to-autoencoders-7a47cf4ef14b)\n\n\n## 6\\. Diffusion Models\n\n* **Definition**: Models that learn to reverse a noise addition process to generate detailed and coherent data from noise.\n* **Example**: Diffusion models are used in DALL\\-E 2 to generate high\\-quality images from random noise.\n* **Learn More**: [Understanding Diffusion Models](https://proxy.rifx.online/https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/)\n\n\n## 7\\. Prompt Engineering\n\n* **Definition**: The process of crafting input prompts to optimize the output generated by a model.\n* **Example**: Modifying the input prompt in GPT\\-4 to generate more concise summaries.\n* **Learn More**: [A Guide to Prompt Engineering](https://proxy.rifx.online/https://www.datacamp.com/tutorial/a-beginners-guide-to-chatgpt-prompt-engineering)\n\n\n## 8\\. Zero\\-Shot Learning\n\n* **Definition**: The ability of a model to perform tasks it was not explicitly trained for, by leveraging knowledge from other tasks.\n* **Example**: GPT\\-3 can perform translation without being specifically trained on translation datasets.\n* **Learn More**: [What is Zero\\-Shot Learning?](https://proxy.rifx.online/https://www.ibm.com/topics/zero-shot-learning)\n\n\n## 9\\. Few\\-Shot Learning\n\n* **Definition**: A model’s ability to learn tasks with only a few examples, minimizing the need for extensive training data.\n* **Example**: GPT\\-3 can be fine\\-tuned to write in a specific style with minimal input samples.\n* **Learn More**: [Few\\-Shot Learning Explained](https://proxy.rifx.online/https://www.ibm.com/topics/few-shot-learning#:~:text=IBM-,What%20is%20few%2Dshot%20learning%3F,suitable%20training%20data%20is%20scarce.)\n\n\n## 10\\. Reinforcement Learning\n\n* **Definition**: A learning paradigm where an AI agent learns to make decisions by interacting with an environment to maximize cumulative rewards.\n* **Example**: AlphaGo uses reinforcement learning to master the game of Go by playing millions of games against itself.\n* **Learn More**: [Reinforcement Learning for Generative AI](https://proxy.rifx.online/https://dl.acm.org/doi/pdf/10.1613/jair.1.15278)\n\n\n## 11\\. Variational Autoencoder (VAE)\n\n* **Definition**: A type of autoencoder that learns to generate new data by introducing randomness to its latent space representations.\n* **Example**: VAEs are used to generate new faces and smoothly transition between different facial features.\n* **Learn More**: [VAEs and Their Applications](https://proxy.rifx.online/https://www.datacamp.com/tutorial/variational-autoencoders)\n\n\n## 12\\. Self\\-Supervised Learning\n\n* **Definition**: A learning technique where the model generates its own labels from the data, reducing reliance on labelled datasets.\n* **Example**: BERT uses self\\-supervised learning by masking words in sentences and predicting them during training.\n* **Learn More**: [What is Self\\-Supervised Learning?](https://proxy.rifx.online/https://www.ibm.com/topics/self-supervised-learning)\n\n\n## 13\\. Tokenization\n\n* **Definition**: The process of splitting text into smaller units, such as words or subwords, for easier processing by models.\n* **Example**: Text input is tokenized into words before being fed into GPT\\-4 for processing.\n* **Learn More**: [Tokenization in NLP](https://proxy.rifx.online/https://www.datacamp.com/blog/what-is-tokenization)\n\n\n## 14\\. Beam Search\n\n* **Definition**: A search algorithm that expands multiple potential sequences of tokens to generate the most likely sequence during decoding.\n* **Example**: Beam search is used in machine translation to generate coherent text outputs.\n* **Learn More**: [Beam Search Explained](https://proxy.rifx.online/https://www.width.ai/post/what-is-beam-search)\n\n\n## 15\\. Transfer Learning\n\n* **Definition**: The process of using a pre\\-trained model on one task and fine\\-tuning it for another, often with less data.\n* **Example**: Fine\\-tuning BERT on sentiment analysis tasks after pre\\-training on general language tasks.\n* **Learn More**: [What is Transfer Learning?](https://proxy.rifx.online/https://aws.amazon.com/what-is/transfer-learning/)\n\n\n## 16\\. Language Model\n\n* **Definition**: A model that predicts the probability of word sequences in natural language, helping generate or understand text.\n* **Example**: GPT\\-4 is a language model capable of generating coherent text for a wide range of applications.\n* **Learn More**: [Introduction to Language Models](https://proxy.rifx.online/https://developers.google.com/machine-learning/resources/intro-llms)\n\n\n## 17\\. Bias in AI\n\n* **Definition**: The tendency of AI systems to produce results that favour or discriminate against certain groups due to biased training data or algorithms.\n* **Example**: Gender bias in AI\\-powered hiring systems trained on biased historical data.\n* **Learn More**: [Understanding Bias in AI](https://proxy.rifx.online/https://www.ibm.com/topics/ai-bias)\n\n\n## 18\\. GPT (Generative Pre\\-trained Transformer)\n\n* **Definition**: A large\\-scale language model that generates human\\-like text based on pre\\-training and fine\\-tuning on extensive text corpora.\n* **Example**: GPT\\-4 generates essays, stories, and detailed responses to user queries.\n* **Learn More**: [How GPT Works](https://proxy.rifx.online/https://tecknoworks.com/how-gpt-works-and-its-core-mechanics/)\n\n\n## 19\\. Perplexity\n\n* **Definition**: A metric that measures how well a language model predicts a given sequence of words, with lower perplexity indicating better performance.\n* **Example**: Comparing the perplexity of GPT\\-3 and GPT\\-4 to assess their text generation quality.\n* **Learn More**: [Perplexity in Language Models](https://proxy.rifx.online/https://huggingface.co/docs/transformers/en/perplexity)\n\n\n## 20\\. Natural Language Processing (NLP)\n\n* **Definition**: A field of AI focused on the interaction between computers and humans through natural language, encompassing tasks like translation and sentiment analysis.\n* **Example**: NLP models are used to perform sentiment analysis on customer reviews.\n* **Learn More**: [Introduction to NLP](https://proxy.rifx.online/https://towardsdatascience.com/a-gentle-introduction-to-natural-language-processing-e716ed3c0863)\n\n\n## 21\\. Neural Network\n\n* **Definition**: A computing system inspired by the human brain’s network of neurons, consisting of layers of interconnected nodes for tasks like image recognition and language processing.\n* **Example**: Convolutional Neural Networks (CNNs) are used to recognize objects in images.\n* **Learn More**: [What are Neural Networks?](https://proxy.rifx.online/https://www.ibm.com/topics/neural-networks)\n\n\n## 22\\. Training Data\n\n* **Definition**: Data used to train AI models by allowing them to learn from examples, improving their ability to recognize patterns and make predictions.\n* **Example**: Large image datasets like ImageNet are used to train AI models for image classification tasks.\n* **Learn More**: [Training Data in AI](https://proxy.rifx.online/https://www.oracle.com/artificial-intelligence/ai-model-training/)\n\n\n## 23\\. Attention Mechanism\n\n* **Definition**: A method in neural networks that helps models focus on the most relevant parts of an input sequence, improving performance in tasks like machine translation and text generation.\n* **Example**: Attention mechanisms allow a model to focus on important words in a sentence when translating between languages.\n* **Learn More**: [What is the Attention Mechanism?](https://proxy.rifx.online/https://machinelearningmastery.com/the-attention-mechanism-from-scratch/)\n\n\n## 24\\. Epoch\n\n* **Definition**: One complete pass through the entire training dataset during the training of a machine learning model.\n* **Example**: Training a neural network for 10 epochs to ensure it properly learns without overfitting.\n* **Learn More**: [Understanding Epochs in Machine Learning](https://proxy.rifx.online/https://www.geeksforgeeks.org/epoch-in-machine-learning/)\n\n\n## 25\\. Multimodal AI\n\n* **Definition**: AI that can process and generate data from multiple modalities (e.g., text, images, and audio) simultaneously.\n* **Example**: CLIP processes both images and text to generate captions for images.\n* **Learn More**: [What is Multimodal AI?](https://proxy.rifx.online/https://www.techtarget.com/searchenterpriseai/definition/multimodal-AI)\n\nKeep in mind that mastery in generative AI is achieved step by step. As you go through the concepts, make sure to explore each one of them through the resources provided, participate in discussions, and try applying what you have learned to your projects. The interaction with these resources and conversations will help you understand the terminology of the language and its use in the real world.\n\nThanks for reading! If you found this guide helpful, please share it with others who might be looking to enhance their generative AI understanding. We learn together and apply these concepts better because of it.\n\nIf you have any thoughts, questions, or even additional resource suggestions you think might be helpful to share, please drop them in the comments section below.\n\nHappy exploring the world of Generative AI!\n\n*Connect with me through [linktr.ee](https://proxy.rifx.online/https://linktr.ee/tharunkumarreddypolu) to know more!*\n\n\n## In Plain English 🚀\n\n*Thank you for being a part of the [**In Plain English**](https://proxy.rifx.online/https://plainenglish.io/) community! Before you go:*\n\n* Be sure to **clap** and **follow** the writer ️👏**️️**\n* Follow us: [**X**](https://proxy.rifx.online/https://x.com/inPlainEngHQ) \\| [**LinkedIn**](https://proxy.rifx.online/https://www.linkedin.com/company/inplainenglish/) \\| [**YouTube**](https://proxy.rifx.online/https://www.youtube.com/channel/UCtipWUghju290NWcn8jhyAw) \\| [**Discord**](https://proxy.rifx.online/https://discord.gg/in-plain-english-709094664682340443) \\| [**Newsletter**](https://proxy.rifx.online/https://newsletter.plainenglish.io/) \\| [**Podcast**](https://proxy.rifx.online/https://open.spotify.com/show/7qxylRWKhvZwMz2WuEoua0)\n* [**Create a free AI\\-powered blog on Differ.**](https://proxy.rifx.online/https://differ.blog/)\n* More content at [**PlainEnglish.io**](https://proxy.rifx.online/https://plainenglish.io/)\n\n"},{"lang":"en","group":"blog","slug":"blog/top-5-ai-tools-for-ios-developers-5ee9f39558ac","frontmatter":{"title":"Top 5 AI Tools for iOS Developers","meta_title":"Top 5 AI Tools for iOS Developers","description":"The article outlines the top five AI tools for iOS developers to enhance workflow efficiency. Key tools include Cursor/VSCode, which offers advanced code completion and refactoring features; the GitHub Copilot Xcode extension for AI-assisted editing within Xcode; Swift Assist, a predictive completion tool; and web interfaces like ChatGPT and Claude for iterative coding. Alex Sidebar is mentioned as a new Xcode extension, while AIProxy is highlighted for securely integrating AI APIs. The article emphasizes the importance of these tools in improving coding speed and accuracy for iOS developers.","date":"2024-11-14T03:29:09.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*6Hs8174FgiwTv87e.jpg","categories":["Programming","Technology/Web","Generative AI"],"author":"Rifx.Online","tags":["Cursor","VSCode","GitHub","Copilot","Swift"],"draft":false,"slug":"blog/top-5-ai-tools-for-ios-developers-5ee9f39558ac"},"content":"\n### To improve your workflow speed \\& efficiency\n\n\n\nWhile there are a lot of big AI talks, I want to get you back on earth. Whether you’re already using AI\\-assisted tools for coding or you feel like this is just a big load of bullshit… this article with a clickbait title is probably for you.\n\nWhile you can probably already find a lot of literature about what and how to use various tools to improve your skills, efficiency, and accuracy with AI, it’s a tad more complicated for us iOS developers. Because we rely on Xcode and its toolchain to build our app, it’s simply harder for us to go without Xcode. And not all the tools I’ll list and explain in the following paragraphs are about skipping Xcode.\n\n## 1\\. Cursor / VSCode\n\nObviously, this is the top of the list. Unless you were hibernating under a rock, you probably heard about VSCode. Working with it on a Swift project is not a novelty. GitHub Copilot, built\\-in VSCode, allows you to code at the speed of light without doing much of anything in terms of setup. They recently integrated more Copilot features within VSCode, and it’s getting closer to Cursor. On top of tab completion, you can now also have inline chat \\+ code generation inline.\n\nCursor is a fork of VSCode, and their Cursor Tab completion feature is, in my experience, faster and more accurate than VSCode.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*pQ4SuReyicAiBCG3.gif)\n\nThey also do something that saved me countless hours: smart / AI\\-assisted refactoring. This is probably one of the best features worth the Cursor subscription alone.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*JlzVJ6o18sulIUEeo_p5sg.gif)\n\nAnd it’s not only for refactoring; it’s simply in smart edition after changing a line. Cursor will show a “tab” indicator, which means it has a proposed change for a part of the code that is probably related to what you just edited. Just press tab to cascade changes, and it can go on and on. Tab tab tab.\n\nOnce you get into the flow, you’ll see how efficient you can be. My flow is just coding as usual, but faster because I have to write much less code. The more you code with it, the more it learns your project, coding style, etc… It might appear a bit off at the beginning, but trust me, give it time.\n\nYou can also generate code with the inline chat:\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*pVzU2MZ0vNFQ6-dRaWPy-w.gif)\n\nIt’s useful when you need a specific algorithm or have all the context within your existing code but need to write some tedious parts. It works quite well and can save a lot of time too. Don’t forget to review the produced code :)\n\nTo get started on it for iOS development specifically, I encourage you to read two of my other stories:\n\nOne about how to set it up, install the correct extensions, etc…\n\nAnd another one is about switching your Xcode project from group\\-based to folder\\-based so you can freely create/delete/move files within VSCode/Cursor without touching the .xcodeproj / Xcode at all.\n\nThis is just scratching the surface for iOS development with Cursor/VScode. But you should get started today!\n\n## 2\\. GitHub Copilot Xcode extension\n\nThis one is a recently released extension, it was initially a project by [Intitni](https://proxy.rifx.online/https://github.com/intitni/CopilotForXcode), but it seems that GitHub forked/acquired it and made it the official extension for Copilot \\+ Xcode. And so far, while the UX is not perfect (understandable as they have to work with accessibility/windows API), it’s much better than Apple (local) Xcode models.\n\nAnd lucky you, I already wrote about it:\n\nIf you’re not ready to switch to another editor than Xcode, but still want to use efficient AI\\-assisted code editing, this extension is for you!\n\n## 3\\. Swift Assist\n\nWhile Xcode already has a built\\-in local model for Predictive code completion (only available on Apple Silicon Mac from Xcode 16\\), Apple teased something else at the WWDC:\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*-mlY8GyGh3VPVhyg3TmLYw.png)\n\nSwift Assist\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*XJnlRo8mqrAMZEVEL64ufg.gif)\n\nIt seems like the chat \\+ code generation from Cursor I’ve demoed above. It should be able to generate code from your comments. But for now, it’s vaporware. Xcode 16\\.2 beta 2 mentioned it, but we still can’t test it.\n\nMaybe it’ll be available in a later version of Xcode 16\\.2 beta, so I can’t wait to test and write about it!\n\n## 4\\. ChatGPT/Claude/Perplexity web interface\n\nSometimes, nothing is better than going back to the basics. While those code editors use Anthropic and OpenAI models and their own, it’s good to not forget that using their web interface is also an invaluable tool in today's landscape.\n\n### ChatGPT \\+ Canvas\n\nOpenAI’s ChatGPT has evolved quite a bit in the last few months. The recent release of o1\\-preview with reasoning and canvas allows for some good coding sessions right within the ChatGPT web interface.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*WUraNCcZMrCRrHMilgzl-Q.png)\n\nCanvas is a mini code editor built on the ChatGPT Web interface and lets you quickly iterate on code and ideas. You can use the chat to make incremental changes, and there are also some other tools to comment on the code, do inline changes, convert it to another language etc…\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ab7PdMLJwacZtVsET2YYmA.gif)\n\nWhile this will not allow you to build a full application, it’s a great tool for quickly iterating on code ideas outside of your standard editor.\n\n### Claude Artifacts\n\nThis is similar to ChatGPT Canvas, but has some other features, like previewing (obviously not with Swift/SwiftUI) and working with multiple files at once.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*2iverELFSGqnJzklPK0cYg.png)\n\n## 5\\. Alex Sidebar\n\nThis is a new contender! The premise is simple, because Xcode is closed source and the extension API is quite limited, why not build around Xcode?\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*vZgn_FjH0FW53c7qZ4DZAg.png)\n\nI’m not a fan of the UX, but it offers most of the Cursor features as an Xcode side panel built like as a window. There are various shortcuts \\+ code completions \\+ chat. You should definitely give it a go to see if it improve your workflow!\n\n## 6\\. AIProxy\n\nBonus as the (3\\) Swift Assist is not really …. available\n\nThis not a tool for coding, but a tool for builders. When integrating an AI API in your iOS app, you’ll most probably need add an API key to your project. But as we all know (right!), you should not have it client side. If you do so, it’s easy for almost anyone to get dump your API key and use your AI credits on your behalf.\n\nEnter [AIProxy](https://proxy.rifx.online/https://www.aiproxy.pro/), they have an open source SDK, it’s easy to integrate and they support all the AI providers your need.\n\nIf you don’t feel like building a backend to proxy your AI calls, this is the right tool for you!\n\n\n"},{"lang":"en","group":"blog","slug":"blog/unified-memory-across-chatgpt-claude-perplexity-24809dc56717","frontmatter":{"title":"Unified Memory Across ChatGPT, Claude, Perplexity","meta_title":"Unified Memory Across ChatGPT, Claude, Perplexity","description":"You’ll definitely love this one, especially if you’re already locked in with Claude, ChatGPT and Perplexity.","date":"2024-11-08T00:24:33.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*qrHnDS6YODZuVXZ4eeWZrQ.png","categories":["Chatbots","Programming/Scripting","Technology/Web"],"author":"Rifx.Online","tags":["Mem0","Chrome","extension","context","memory"],"draft":false,"slug":"blog/unified-memory-across-chatgpt-claude-perplexity-24809dc56717"},"content":"\nYou’ll definitely love this one, especially if you’re already locked in with Claude, ChatGPT and Perplexity.\n\nInteracting with different AI assistants can sometimes feel a bit disjointed.\n\nYou have to repeat the same context over and over when switching between ChatGPT, Claude, Perplexity, and others.\n\nWouldn’t be great if all could share one universal memory to enhance the context?\n\nI found this awesome Chrome extension, and it’s been a total life saver for me.\n\nImagine having a seamless conversation where the context carries over, no matter which AI you’re chatting with.\n\nSounds cool, doesn’t it?\n\n\n\n### The Big Problem It Solves\n\nBefore I dive into it, how are we even keeping LLMs updated with the latest knowledge?\n\nThere are a few traditional ways to tackle this:\n\n1. **Retrieval\\-Based Methods**: These pull information from a knowledge base. They’re powerful but can get messy with redundant data and the hassle of managing an ever\\-growing repository.\n2. **Model Editing**: This tweaks the model to adapt to new facts. It works for simple, single\\-sentence updates but struggles with longer, more complex information.\n3. **Long Context Methods**: These cram all the knowledge into the model’s context. It’s like overloading the model with data, which isn’t practical because the context length is limited.\n\nAll these methods have their downsides, especially when we need up\\-to\\-date, seamless interactions across different AI platforms.\n\n### Enter Mem0: Intelligent Memory Retrieval for AI Assistants\n\nMem0 sidesteps these issues by creating a universal memory layer that works across multiple AI assistants.\n\nHere’s what it brings to the table:\n\n* **Universal Memory Layer**: Share context effortlessly across ChatGPT, Claude, Perplexity, and more. No more repeating yourself!\n* **Smart Context Detection**: It automatically picks up relevant information from your conversations, so you don’t have to manually input anything.\n* **Intelligent Memory Retrieval**: Mem0 brings up the right memories at the right time, making your interactions smoother.\n* **One\\-Click Sync with ChatGPT**: If you’ve been using ChatGPT, you can sync your existing memories with just one click.\n* **Memory Dashboard**: A handy place to manage all your memories in one spot.\n\nFor example, if you start a conversation with Claude:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*QQAJPzQp2tjBFgi-9InG1Q.png)\n\nIt will extract key information and add it to memory.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MtctEzunD72Tw_dOOCOnyA.png)\n\nThen the next time you ask a relevant question:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*r0WyCIHGvmiGm0GZ6xR1pw.png)\n\nIt will add the relevant context:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*PTEhFUJ4nrhQbeZXIAnD6A.png)\n\nbut here’s what more interesting.\n\nIf you open Perplexity and ask another question:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*54BKW9BQWwCDdXdAg0U-mA.png)\n\nIt will get the memory across different applications and enhance the context:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*FD6MwGtL8WpovUH3o1Vw3g.png)\n\nWhich is pretty cool!\n\nIf you open ChatGPT and ask a question:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*qLmDZh5NofuArxzS4OnDIQ.png)\n\nSimilarly, it will bring relevant information from memory and enhance the context.\n\nAs you work more with it, it will collect key information and save you lots of typing while you interact with Claude, Perplexity and ChatGPT.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MqgEr6hi5cHzHRuTqODzuA.png)\n\nYou can also add new information to memory:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Xo6jLLIGKqxvYR9SivuMhQ.png)\n\n### How to Get Started\n\nInstalling Mem0 is super easy:\n\n1. [**Add the extension to Chrome**](https://chromewebstore.google.com/detail/mem0/onihkkbipkfeijkadecaafbgagkhglop?hl=en-GB)\n\n**2\\. Sign In**:\n\n* After it’s installed, you’ll see the Mem0 icon in your toolbar.\n* Click it and sign in with Google.\n\n**4\\. Start Chatting**:\n\n* Use any of the supported AI assistants.\n* For ChatGPT and Perplexity, just chat as you normally would.\n* On Claude, click the Mem0 button or use the shortcut `^ + M`.\n\nOne of the best things about Mem0 is that it’s completely free. There are:\n\n* **No usage limits**\n* **No ads**\n* **All features included**\n\n\n"},{"lang":"en","group":"blog","slug":"blog/unlocking-mixture-of-experts-moe-llm-your-moe-model-can-be-embedding-model-for-free-f192b9c07a5f","frontmatter":{"title":"Unlocking Mixture-of-Experts (MoE) LLM : Your MoE model can be embedding model for free","meta_title":"Unlocking Mixture-of-Experts (MoE) LLM : Your MoE model can be embedding model for free","description":"Mixture-of-experts (MoE) LLM can be used as an embedding model for free.","date":"2024-11-04T12:30:57.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*mB6VhEyAvxAxGbLDG_6hTw.png","categories":["Machine Learning","Natural Language Processing","Data Science"],"author":"Rifx.Online","tags":["Mixture-of-Experts","MoE","embedding","MoEE","BERTopic"],"draft":false,"slug":"blog/unlocking-mixture-of-experts-moe-llm-your-moe-model-can-be-embedding-model-for-free-f192b9c07a5f"},"content":"\n### Mixture\\-of\\-experts (MoE) LLM can be used as an embedding model for free.\n\n\n\nI recently found an interesting paper titled “Your Mixture\\-of\\-Experts LLM is Secretly an Embedding Model for Free.” \\[1] A recent LLM architecture trend is a decoder model, which is unsuitable for an embedding model because of their attention method. However, the authors revealed that Mixture\\-of\\-Experts (MoE) LLMs can perform as an embedding model to apply a diverse class of embedding\\-focused tasks without any further fine\\-tuning. In this blog, firstly, let’s recap MoE, and I will introduce how it works and its practical implementation.\n\n## Table of Contents\n\n1. What is Mixture-of-Experts (MoE)?\n2. How MoE works as an embedding model?\n3. Practical implementation : Leverage MoEE with BERTopic\n\n## 1\\. What is Mixture\\-of\\-Experts (MoE) ?\n\nMixture\\-of\\-Experts (MoE) is an architecture with multiple subnetworks called “experts,” each specializing in different tasks or aspects of data. One of MoE’s advantages is that it enables AI models to be pretrained with less computation than the same or larger models while maintaining or increasing quality. So, if we have a limited budget, we can achieve a better model using MoE than the dense, similar\\-size conventional model. For recent success, Mixtral 8 x 7B outperforms the LLaMA 2 70B for many evaluation datasets.\n\nFrom now on, let’s examine the architecture of MoE. Recent successful MoEs use the transformer model, so I will focus on the popular MoE architecture for the transformer. MoE has mainly two components described below.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Dia_c08PJnFeeIc9lxwtGQ.png)\n\n* **MoE layers**\n\nMoE replaces the feed\\-forward network (FFN) layers with MoE layers in the transformer architecture. Each MoE layer has some experts (Ex. 4 experts in the above illustration), and each expert is composed of the simple FFN layer. Note that other components in the transformer, such as the self\\-attention layer, share the same weights. Therefore, the number of weights of MoE is not straightforward. For example, the Mixtral 8 x 7B weight is not 8 x 7 \\= 56B but 47B because the other layers besides MoE layers share the same weights.\n\n* **Gating network**\n\nA gating network or router is a crucial component in MoE. It takes input tokens and selects the most relevant experts for each token. For instance, in the above illustration, the left side of the router chooses the second expert to process the word “more” token. Meanwhile, the router determines the first expert to process the word “Parameters” token. Generally, a gating network chooses the top\\-k experts relevant to the given token and sends the token to chosen experts; for example, Mixtral 8 x 7B chooses top\\-2 experts.\n\nHow can we choose the top\\-k experts? We use the softmax function to calculate the expert’s importance probabilities and keep top\\-k probability experts, as shown below. I extracted the gating part of the above illustration.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*qX9H2KKtjntVuiE8yFstMQ.png)\n\nA gating network has its weight. We apply the softmax function to the result of the dot\\-product between the input word token and the weight of a gating network, then get the probability of how much the expert is relevant to the given token. Based on the probability, we can select top\\-k relevant experts. MoE, which has this type of gating network, is called sparse MoE.\n\nThese are the fundamental knowledge needed to understand how MoE works as an embedding model. For further understanding, I recommend reading [this blog](https://huggingface.co/blog/moe) \\[2]. Now, let’s dive into how MoE actually works as an embedding model.\n\n## 2\\. How MoE works as an embedding model?\n\n### Quick recap about embeddings\n\nBefore diving into the theme of this section, let’s quickly recap about embeddings. Recently, embedding has been the internal representation of input data in deep learning models, and it has semantics and condensed data information. We usually extract the last hidden state of the neural network as an embedding, as shown below.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kSHFTEejKiSI51taKZCO9A.png)\n\nWe typically use encoder\\-based models to extract embeddings because they can capture semantics with bi\\-directional attention compared to decoder\\-only models. Decoder\\-only models often use causal attention to interact with only the previous word tokens; thus, they cannot capture the rich semantics, such as contextualized information, like encoder\\-decoder models.\n\n### How MoE works as an embedding model?\n\nIt was a common belief that the decoder model could not be used for embedding extraction. However, the authors found that the routing weight in the MoE provides complementary information to the decoder embedding. The routing weight in each layer reflects the reasoning choice on the input token, so it contains the input’s semantic information that hidden state’s embedding may lost. In the mathematical formula, we can describe it as:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*n6wGCMqAhjBAfLFV47ML1g.png)\n\n*g* is the softmax function and *H* means the hidden state. We concatenate all the MoE layer’s routing weights to avoid losing model’s reasoning choice.\n\nTo fully utilize the routing weights and decoder embedding, the authors proposed a method called MoE Embedding (MoEE) to form a more comprehensive embedding representation. There are two types of MoEE. One method is a concatenation\\-based combination, described below.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*uVmcV-lM83XL7HoYbYjt7w.png)\n\nThis method is simple, and we just concatenate routing weights and decoder embedding. The authors call this method as MoEE(concat). It can preserve the distinct information captured by each routing weight while allowing downstream tasks to leverage the combined representation.\n\nThe other method is weighted sum integration. It performs a weighted sum of the similarity scores calculated from routing weights and hidden state (HS) embedding, denoted as MoEE (sum). This method is used for tasks that compare two sentences, such as semantic textual similarity.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kyJxWW9zdgRyNr2jmO4LlQ.png)\n\n𝛂 is a hyperparameter to control the contribution of the routing weights. After calculating the similarity score for each pair, we compute the rank correlation, such as Spearman’s rank correlation, between the calculated similarity score and the ground truth similarity.\n\nFor practical usage, I think that the MoEE(concat) is easy to use. Moreover, the authors leverage the PromptEOL technique \\[4] to enhance MoEE. This technique prompts the following template to constrain LLMs in predicting semantic information in the next token.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*S9BASj9JkQe-i4fqmbopWg.png)\n\nNow, here is the performance table across MTEB tasks.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*7LxkEMR2DFlncypF6_T7Vw.png)\n\nMoEE with PromptEOL can work better than supervised and self\\-supervised methods. Note that this leaderboard is not the latest one, so this result is not SOTA. The value of this method is that we can obtain decent results for embedding tasks, and it can be used without any further training.\n\nWe have covered how MoEE works so far. In the next section, we will implement MoEE with BERTopic and cluster sentences.\n\n## 3\\. Practical implementation : Leverage MoEE with BERTopic\n\nIn this section, we extract embeddings from pre\\-trained MoE LLM and leverage them with [BERTopic](https://maartengr.github.io/BERTopic/index.html) using a 20\\-news\\-group dataset \\[5]. For your information, BERTopic is a convenient topic modeling library beyond conventional statistical topic modeling. It leverages embeddings from Transformer to make topic clustering, so I think it is suitable for checking the capability. First of all, let’s prepare an environment.\n\n### Environment setup\n\nI used a conda environment with Python 3\\.10\\. I experimented on Ubuntu 20\\.04 with cuda 12\\.4, 16 GB VRAM. You may need 32 GB RAM for downloading model weights.\n\n```python\nconda create -n moee python=3.10 -y\nconda activate moee\n```\n\nNext, we need to install the libraries below via pip.\n\n```python\npip install transformers torch bitsandbytes bertopic accelerate\n```\n\nMoE models need generally high VRAM because we need to load the entire model to our VRAM in advance. Therefore, we require using bitsandbytes, which is a quantization package, to save VRAM memory.\n\nWe need to clone the official GitHub repository.\n\n```python\ngit clone https://github.com/tianyi-lab/MoE-Embedding.git\n```\n\nAll preparation is done. Now, let’s implement topic clustring with BERTopic using MoEE.\n\n### Leverage MoEE with BERTopic\n\nNow, we will use MoEE as an embedding model for BERTopic and try topic clustering. The original repository allows us to use small MoE models, such as Qwen\\-1\\.5\\-MoE\\-A2\\.7B or OLMoE\\-1B\\-7B. In this blog, I will use OLMoE\\-1B\\-7B, which is affordable for running inference on 16 GB VRAM. Firstly, we need to load OLMoE\\-1B\\-7B.\n\n```python\nkwargs = {\n        \"base_model\": 'allenai/OLMoE-1B-7B-0924',\n        \"normalized\": False,\n        \"torch_dtype\": torch.bfloat16,\n        \"mode\": \"embedding\",\n        \"pooling_method\": \"mean\",\n        \"attn_implementation\": \"sdpa\",\n        \"attn\": \"bbcc\",\n    }\n\nconfig = {\n    'embed_method': 'prompteol',\n    'emb_info': 'MoEE'\n    }\n\nembedding_model = MOEE(model_name_or_path='allenai/OLMoE-1B-7B-0924', **kwargs)\n```\n\nNext, we need to calculate embeddings of 20\\-news\\-group dataset to pass BERTopic. (I will attach full code later.)\n\n```python\nfrom sklearn.datasets import fetch_20newsgroups\n\ndocs = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))['data']\n\ndataset = MyDataset(docs)\ndataloader = DataLoader(dataset=dataset, batch_size=8)\nembeddings = None\n\nfor batch in tqdm(dataloader):\n    with torch.no_grad():    \n        embedding = embedding_model.encode(batch, **config)\n      \n        if embeddings is None:\n            embeddings = embedding[0]\n        else:\n            embeddings = np.vstack((embeddings, embedding[0]))\n  \n    torch.cuda.empty_cache()\n```\n\nTo calculate embeddings in advance, we use torch.utils.data.DataLoader for an iterator, and encode each batched document. Note that we must pass embeddings as np.asarray type to the BERTopic.\n\nWhen you want to use your own MoE models, you must implement to get the routing weights from each MoE layer. For the hidden state embedding, we can utilize the HuggingFace transformer function. We only need to pass the output\\_hidden\\_states\\=True argument when inference.\n\nNow, we can run topic modeling.\n\n```python\n## Step 2 - Reduce dimensionality\numap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')\n\n## Step 3 - Cluster reduced embeddings\nhdbscan_model = HDBSCAN(min_cluster_size=15, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n\n## Step 4 - Tokenize topics\nvectorizer_model = CountVectorizer(stop_words=\"english\")\n\n## Step 5 - Create topic representation\nctfidf_model = ClassTfidfTransformer()\n\n## Step 6 - (Optional) Fine-tune topic representations with \n## a `bertopic.representation` model\nrepresentation_model = KeyBERTInspired()\n\n## All steps together\ntopic_model = BERTopic(\n  embedding_model=embedding_model,          # Step 1 - Extract embeddings\n  umap_model=umap_model,                    # Step 2 - Reduce dimensionality\n  hdbscan_model=hdbscan_model,              # Step 3 - Cluster reduced embeddings\n  vectorizer_model=vectorizer_model,        # Step 4 - Tokenize topics\n  ctfidf_model=ctfidf_model,                # Step 5 - Extract topic words\n  representation_model=representation_model # Step 6 - (Optional) Fine-tune topic representations\n)\n\n## topic modeling using BERTopic model\ntopics, probs = topic_model.fit_transform(docs, embeddings)\n```\n\nWe got 42 topics by the default setting; some samples are shown below. Even though I picked up topics randomly, it can capture the semantics very well.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*VIaKHU-PSuTPzOUKDFbwOw.png)\n\nMoreover, here is the topic cluster visualization.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*KYAUOe2qEAv-ihq2S2dM0A.png)\n\nPlease look at the red circle in the topic cluster visualization. This red circle refers to topic 0, which is related to the computer. Closer topics are also associated with mechanic words, such as graphics, digital, and printers.\n\nThis method shows us that we can get decent embeddings without any training. Although there is still room to improve the quality to be equivalent to the SOTA\\-supervised models, this paper’s finding is a good step for further improvement of the embedding extracting method without training.\n\nHere is my entire code. You need to put this file into the top of the MoE\\-Embedding directory.\n\n## References\n\n\\[1] Ziyue Li, Tianyi Zhou, [YOUR MIXTURE\\-OF\\-EXPERTS LLM IS SECRETLY AN EMBEDDING MODEL FOR FREE](https://arxiv.org/pdf/2410.10814) (2024\\), *Arxiv*\n\n\\[2] Omar S., et.al., [Mixture of Experts Explained](https://huggingface.co/blog/moe) (2023\\), Hugging Face\n\n\\[3] William Fedus, Barret Zoph., et.al., [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/pdf/2101.03961) (2021\\), *Arxiv*\n\n\\[4] Ting Jiang, et.al., [Scaling Sentence Embeddings with Large Language Models](https://arxiv.org/pdf/2307.16645) (2023\\), *Arxiv*\n\n\\[5] [20 News groups](http://qwone.com/~jason/20Newsgroups/)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/using-llama-3-for-building-ai-agents-7e74f79d1ccc","frontmatter":{"title":"Using Llama 3 for Building AI Agents","meta_title":"Using Llama 3 for Building AI Agents","description":"Comprehensive guide to building AI Agents with Llama 3 function calling capabilities.","date":"2024-11-10T03:51:17.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*EWGo-7t4Kl6l82rB2-ZK9Q.png","categories":["Programming","Generative AI","Chatbots"],"author":"Rifx.Online","tags":["Llama","Gradio","RAG","metadata","indexing"],"draft":false,"slug":"blog/using-llama-3-for-building-ai-agents-7e74f79d1ccc"},"content":"\n\n\n\n\n### Comprehensive guide to building AI Agents with Llama 3 function calling capabilities.\n\n\n\n\n### Introduction\n\nImagine you want to buy something. You visit an e\\-commerce website and use the search option to find what you want. Maybe you have multiple items to buy, so the process isn’t very efficient. Now consider this scenario: open an application, describe what you want in plain English, and press enter. You don't have to worry about searching and price comparisons because the application handles it automatically for you. Pretty cool, right? That’s exactly what we’ll build in this tutorial.\n\nLet’s look at some examples first.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ikbr1ozv37PIB2meVfCCfA.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*AZPn3_KCDRV0pAszd3vLmA.png)\n\nAlright, let’s bring life to this application. We’re going to use Meta’s Llama 3 model with function calling capability. However, this can also be accomplished using the 3\\.1 models. According to [Meta’s announcement](https://ai.meta.com/blog/meta-llama-3-1/), the 3\\.1 models can use tools and functions more effectively.\n\n\n> These are multilingual and have a significantly longer context length of 128K, state\\-of\\-the\\-art tool use, and overall stronger reasoning capabilities\n\nI will use Groq Cloud, specifically their model for this article. The initial workflow of this application should consist of an embedding model, a retriever, and two major tools for handling user purchase interests and cost\\-related concerns. In summary, we need something similar to what we've described in the diagram below.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*EZVySX3GD2O07fzEPwLcbQ.png)\n\nNow we have to use an LLM orchestration framework. For that, I am picking my all\\-time favorite, [Haystack](https://haystack.deepset.ai/).\n\nOkay, we got what we need. Let’s jump into the actual work!\n\n\n### Loading and indexing data\n\nSince we have an RAG pipeline, we should build a document indexing service as the first step. For this demo, I am going to use the in\\-memory vector database that Haystack offers. Please note that each document in our vector database contains,\n\n* Content — Which we used to perform a similarity search\n* Id — Unique identifier\n* Price — Product price\n* URL — Product URL\n\nWhen our RAG pipeline is invoked, the Content field is used for vector search. All other fields are included as metadata. It’s crucial to preserve this metadata as it’s essential for front\\-end presentation to the user.\n\nLet’s see how we can implement that.\n\n\n```python\nfrom haystack import Pipeline, Document\nfrom haystack.document_stores.in_memory import InMemoryDocumentStore\nfrom haystack.components.writers import DocumentWriter\nfrom haystack.components.embedders import SentenceTransformersDocumentEmbedder\nfrom haystack.components.generators import OpenAIGenerator\nfrom haystack.utils import Secret\nfrom haystack.components.generators.chat import OpenAIChatGenerator\nfrom haystack.components.builders import PromptBuilder\nfrom haystack.components.embedders import SentenceTransformersTextEmbedder\nfrom haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\nfrom haystack.dataclasses import ChatMessage\nimport pandas as pd\n\n## Load product data from CSV\ndf = pd.read_csv(\"product_sample.csv\")\n\n## Initialize an in-memory document store\ndocument_store = InMemoryDocumentStore()\n\n## Convert the product data into Haystack Document objects\ndocuments = [\n    Document(\n        content=item.product_name, \n        meta={\n            \"id\": item.uniq_id, \n            \"price\": item.selling_price, \n            \"url\": item.product_url\n        }\n    ) for item in df.itertuples()\n]\n\n## Create a pipeline for indexing the documents\nindexing_pipeline = Pipeline()\n\n## Add a document embedder to the pipeline using Sentence Transformers model\nindexing_pipeline.add_component(\n    instance=SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\"), name=\"doc_embedder\"\n)\n\n## Add a document writer to the pipeline to store documents in the document store\nindexing_pipeline.add_component(instance=DocumentWriter(document_store=document_store), name=\"doc_writer\")\n\n## Connect the embedder's output to the writer's input\nindexing_pipeline.connect(\"doc_embedder.documents\", \"doc_writer.documents\")\n\n## Run the indexing pipeline to process and store the documents\nindexing_pipeline.run({\"doc_embedder\": {\"documents\": documents}})\n```\nGreat, we’ve completed the first step of our AI agent application. Now it’s time to build the product identifier tool. To better understand the primary task of the product identifier, let’s consider the example below.\n\n\n> User Query: I want to buy a camping boot, a charcoal and google pixel 9 back cover. Let’s understand our ideal workflow for the product identifier function.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kXGYjlMi4pQcqIKpmUZLRQ.png)\n\nFirst, we need to create a tool for analyzing user queries and identifying user\\-interested products. We can build such a tool using code snippets below.\n\n\n### Building User Query Analyzer\n\n\n```python\ntemplate = \"\"\"\nUnderstand the user query and list of products the user is interested in and return product names as list.\nYou should always return a Python list. Do not return any explanation.\n\nExamples:\nQuestion: I am interested in camping boots, charcoal and disposable rain jacket.\nAnswer: [\"camping_boots\",\"charcoal\",\"disposable_rain_jacket\"]\n\nQuestion: Need a laptop, wireless mouse, and noise-cancelling headphones for work.\nAnswer: [\"laptop\",\"wireless_mouse\",\"noise_cancelling_headphones\"]\n\nQuestion: {{ question }}\nAnswer:\n\"\"\"\n\nproduct_identifier = Pipeline()\n\nproduct_identifier.add_component(\"prompt_builder\", PromptBuilder(template=template))\nproduct_identifier.add_component(\"llm\", generator())\n\nproduct_identifier.connect(\"prompt_builder\", \"llm\")\n```\nOkay, now we have completed half of our first function, now it’s time to complete the function by adding the RAG pipeline.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*JyxINdc8Wz-qAg_PCAkLbA.png)\n\n\n### Creating RAG Pipeline\n\n\n```python\ntemplate = \"\"\"\nReturn product name, price, and url as a python dictionary. \nYou should always return a Python dictionary with keys price, name and url for single product.\nYou should always return a Python list of dictionaries with keys price, name and url for multiple products.\nDo not return any explanation.\n\nLegitimate Response Schema:\n{\"price\": \"float\", \"name\": \"string\", \"url\": \"string\"}\nLegitimate Response Schema for multiple products:\n[{\"price\": \"float\", \"name\": \"string\", \"url\": \"string\"},{\"price\": \"float\", \"name\": \"string\", \"url\": \"string\"}]\n\nContext:\n{% for document in documents %}\n    product_price: {{ document.meta['price'] }}\n    product_url: {{ document.meta['url'] }}\n    product_id: {{ document.meta['id'] }}\n    product_name: {{ document.content }}\n{% endfor %}\nQuestion: {{ question }}\nAnswer:\n\"\"\"\n\nrag_pipe = Pipeline()\nrag_pipe.add_component(\"embedder\", SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\"))\nrag_pipe.add_component(\"retriever\", InMemoryEmbeddingRetriever(document_store=document_store, top_k=5))\nrag_pipe.add_component(\"prompt_builder\", PromptBuilder(template=template))\nrag_pipe.add_component(\"llm\", generator())\n\nrag_pipe.connect(\"embedder.embedding\", \"retriever.query_embedding\")\nrag_pipe.connect(\"retriever\", \"prompt_builder.documents\")\nrag_pipe.connect(\"prompt_builder\", \"llm\")\n```\nAfter this stage, we have completed both RAG and Query Analyzer pipelines. Now it’s time to convert this into a tool. For that, we can use a regular function declaration, as shown below. Creating a tool for the Agent is just like creating a Python function. In case you have a question like\n\n\n> How is it possible for the Agent to invoke this function?\n\nThe solution is straightforward: by leveraging a model\\-specific tool schema, which we plan to incorporate in a future step. For now, it’s time to create a wrapper function that uses both the query analyzer and RAG pipeline.\n\nLet’s clarify the objectives of this function.\n\n**Objective 1:** Identify all products the user is interested in and return them as a list. **Objective 2:** For each identified product, retrieve up to five products from the database along with their metadata.\n\n\n### Finalizing Product Identifier Function\n\n\n```python\ndef product_identifier_func(query: str):\n    \"\"\"\n    Identifies products based on a given query and retrieves relevant details for each identified product.\n\n    Parameters:\n    query (str): The query string used to identify products.\n\n    Returns:\n    dict: A dictionary where the keys are product names and the values are details of each product. If no products are found, returns \"No product found\".\n    \"\"\"\n    product_understanding = product_identifier.run({\"prompt_builder\": {\"question\": query}})\n\n    try:\n        product_list = literal_eval(product_understanding[\"llm\"][\"replies\"][0])\n    except:\n        return \"No product found\"\n\n    results = {}\n\n    for product in product_list:\n        response = rag_pipe.run({\"embedder\": {\"text\": product}, \"prompt_builder\": {\"question\": product}})\n        try:\n            results[product] = literal_eval(response[\"llm\"][\"replies\"][0])\n        except:\n            results[product] = {}\n    \n    return results\n```\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*HWRTdWvvcw2MZP4uoaQdeQ.png)\n\nWith that, we have completed our first tool for the agent. Let’s see whether it works as expected.\n\n\n```python\nquery = \"I want crossbow and woodstock puzzle\"\n#execute function\nproduct_identifier_func(query)\n\n## {'crossbow': {'name': 'DB Longboards CoreFlex Crossbow 41\" Bamboo Fiberglass '\n##                        'Longboard Complete',\n##                'price': 237.68,\n##                'url': 'https://www.amazon.com/DB-Longboards-CoreFlex-Fiberglass-Longboard/dp/B07KMVJJK7'},\n##  'woodstock_puzzle': {'name': 'Woodstock- Collage 500 pc Puzzle',\n##                       'price': 17.49,\n##                       'url': 'https://www.amazon.com/Woodstock-Collage-500-pc-Puzzle/dp/B07MX21WWX'}}\n```\nIt worked!! However, it’s worth noting the return output schema. You can see the general schema below.\n\n\n```python\n{\n    \"product_key\": {\n        \"name\": \"string\",\n        \"price\": \"float\",\n        \"url\": \"string\"\n    }\n}\n```\nThat’s exactly what we have advised the model to produce in the RAG pipeline. As a next step, let’s build an optional tool called `find_budget_friendly_option`.\n\n\n```python\ndef find_budget_friendly_option(selected_product_details):\n    \"\"\"\n    Finds the most budget-friendly option for each category of products.\n\n    Parameters:\n    selected_product_details (dict): A dictionary where the keys are product categories and the values are lists of product details. Each product detail is expected to be a dictionary containing a 'price' key.\n\n    Returns:\n    dict: A dictionary where the keys are product categories and the values are the most budget-friendly product details for each category.\n    \"\"\"\n    budget_friendly_options = {}\n    \n    for category, items in selected_product_details.items():\n        if isinstance(items, list):\n            lowest_price_item = min(items, key=lambda x: x['price'])\n        else:\n            lowest_price_item = items\n        \n        budget_friendly_options[category] = lowest_price_item\n    \n    return budget_friendly_options\n```\nOkay, let's focus on the most crucial aspect of this application, which is enabling the agent to use these functions as needed. As we previously talked about, this is achievable through a model\\-specific tool schema. Therefore, we need to locate the tool schema specific to the selected model. Fortunately, it's mentioned in the model card [here](https://huggingface.co/Groq/Llama-3-Groq-70B-Tool-Use). We need to adjust that to fit our use case.\n\n\n### Finalizing Chat Template\n\n\n```python\nchat_template = '''<|start_header_id|>system<|end_header_id|>\n\nYou are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n<tool_call>\n{\"name\": <function-name>,\"arguments\": <args-dict>}\n</tool_call>\n\nHere are the available tools:\n<tools>\n    {\n        \"name\": \"product_identifier_func\",\n        \"description\": \"To understand user interested products and its details\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"query\": {\n                    \"type\": \"string\",\n                    \"description\": \"The query to use in the search. Infer this from the user's message. It should be a question or a statement\"\n                }\n            },\n            \"required\": [\"query\"]\n        }\n    },\n    {\n        \"name\": \"find_budget_friendly_option\",\n        \"description\": \"Get the most cost-friendly option. If selected_product_details has morethan one key this should return most cost-friendly options\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"selected_product_details\": {\n                    \"type\": \"dict\",\n                    \"description\": \"Input data is a dictionary where each key is a category name, and its value is either a single dictionary with 'price', 'name', and 'url' keys or a list of such dictionaries; example: {'category1': [{'price': 10.5, 'name': 'item1', 'url': 'http://example.com/item1'}, {'price': 8.99, 'name': 'item2', 'url': 'http://example.com/item2'}], 'category2': {'price': 15.0, 'name': 'item3', 'url': 'http://example.com/item3'}}\"\n                }\n            },\n            \"required\": [\"selected_product_details\"]\n        }\n    }\n</tools><|eot_id|><|start_header_id|>user<|end_header_id|>\n\nI need to buy a crossbow<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n<tool_call>\n{\"id\":\"call_deok\",\"name\":\"product_identifier_func\",\"arguments\":{\"query\":\"I need to buy a crossbow\"}}\n</tool_call><|eot_id|><|start_header_id|>tool<|end_header_id|>\n\n<tool_response>\n{\"id\":\"call_deok\",\"result\":{'crossbow': {'price': 237.68,'name': 'crossbow','url': 'https://www.amazon.com/crossbow/dp/B07KMVJJK7'}}}\n</tool_response><|eot_id|><|start_header_id|>assistant<|end_header_id|>\n'''\n```\nNow there are only a few steps left. Before doing anything, let’s test our agent.\n\n\n```python\n### Testing agent\nmessages = [\n    ChatMessage.from_system(\n        chat_template\n    ),\n    ChatMessage.from_user(\"I need to buy a crossbow for my child and Pokémon for myself.\"),\n]\n\nchat_generator = get_chat_generator()\nresponse = chat_generator.run(messages=messages)\npprint(response)\n\n### response\n{'replies': [ChatMessage(content='<tool_call>\\n'\n                                 '{\"id\": 0, \"name\": \"product_identifier_func\", '\n                                 '\"arguments\": {\"query\": \"I need to buy a '\n                                 'crossbow for my child\"}}\\n'\n                                 '</tool_call>\\n'\n                                 '<tool_call>\\n'\n                                 '{\"id\": 1, \"name\": \"product_identifier_func\", '\n                                 '\"arguments\": {\"query\": \"I need to buy a '\n                                 'Pokemon for myself\"}}\\n'\n                                 '</tool_call>',\n                         role=<ChatRole.ASSISTANT: 'assistant'>,\n                         name=None,\n                         meta={'finish_reason': 'stop',\n                               'index': 0,\n                               'model': 'llama3-groq-70b-8192-tool-use-preview',\n                               'usage': {'completion_time': 0.217823967,\n                                         'completion_tokens': 70,\n                                         'prompt_time': 0.041348261,\n                                         'prompt_tokens': 561,\n                                         'total_time': 0.259172228,\n                                         'total_tokens': 631}})]}\n```\nWith that, we have completed about 90% of our work.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*nYVXcgpm3RZ3g5h5d4UK_A.png)\n\nOne thing you may have noticed in the above response is that the XML tag `<tool_call>` encloses tool calls. Thus, we need to develop a mechanism to extract the tool\\_call object.\n\n\n```python\ndef extract_tool_calls(tool_calls_str):\n    json_objects = re.findall(r'<tool_call>(.*?)</tool_call>', tool_calls_str, re.DOTALL)\n    \n    result_list = [json.loads(obj) for obj in json_objects]\n    \n    return result_list\n\navailable_functions = {\n    \"product_identifier_func\": product_identifier_func, \n    \"find_budget_friendly_option\": find_budget_friendly_option\n    }\n```\nWith this step completed, we can directly access the agent’s response when it calls a tool. Now the only thing pending is to get the tool call object and execute the function accordingly. Let’s complete that piece too.\n\n\n```python\nmessages.append(ChatMessage.from_user(message))\nresponse = chat_generator.run(messages=messages)\n\nif response and \"<tool_call>\" in response[\"replies\"][0].content:\n    function_calls = extract_tool_calls(response[\"replies\"][0].content)\n    for function_call in function_calls:\n        # Parse function calling information\n        function_name = function_call[\"name\"]\n        function_args = function_call[\"arguments\"]\n\n        # Find the corresponding function and call it with the given arguments\n        function_to_call = available_functions[function_name]\n        function_response = function_to_call(**function_args)\n\n        # Append function response to the messages list using `ChatMessage.from_function`\n        messages.append(ChatMessage.from_function(content=json.dumps(function_response), name=function_name))\n        response = chat_generator.run(messages=messages)\n```\nNow it’s time to join each component together and build a proper chat application. I am going to use Gradio for that purpose.\n\n\n```python\nimport gradio as gr\n\nmessages = [ChatMessage.from_system(chat_template)]\nchat_generator = get_chat_generator()\n\ndef chatbot_with_fc(message, messages):\n    messages.append(ChatMessage.from_user(message))\n    response = chat_generator.run(messages=messages)\n\n    while True:\n        if response and \"<tool_call>\" in response[\"replies\"][0].content:\n            function_calls = extract_tool_calls(response[\"replies\"][0].content)\n            for function_call in function_calls:\n                # Parse function calling information\n                function_name = function_call[\"name\"]\n                function_args = function_call[\"arguments\"]\n\n                # Find the corresponding function and call it with the given arguments\n                function_to_call = available_functions[function_name]\n                function_response = function_to_call(**function_args)\n\n                # Append function response to the messages list using `ChatMessage.from_function`\n                messages.append(ChatMessage.from_function(content=json.dumps(function_response), name=function_name))\n                response = chat_generator.run(messages=messages)\n\n        # Regular Conversation\n        else:\n            messages.append(response[\"replies\"][0])\n            break\n    return response[\"replies\"][0].content\n\n\ndef chatbot_interface(user_input, state):\n    response_content = chatbot_with_fc(user_input, state)\n    return response_content, state\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# AI Purchase Assistant\")\n    gr.Markdown(\"Ask me about products you want to buy!\")\n    \n    state = gr.State(value=messages)\n    \n    with gr.Row():\n        user_input = gr.Textbox(label=\"Your message:\")\n        response_output = gr.Markdown(label=\"Response:\")\n    \n    user_input.submit(chatbot_interface, [user_input, state], [response_output, state])\n    gr.Button(\"Send\").click(chatbot_interface, [user_input, state], [response_output, state])\n\n\ndemo.launch()\n```\nThat’s it! We have built the Llama 3\\-based AI Agent 🤖 with function calling capability. You can access the full code from this [GitHub repo](https://github.com/Ransaka/ai-agents-with-llama3). Thanks for reading.\n\nAccess to the dataset used in this article is available through [this](https://www.kaggle.com/datasets/promptcloud/amazon-product-dataset-2020) Kaggle link (Under CC0: Public Domain).\n\n\n### Conclusion\n\nWhen constructing an AI agent\\-based system, it's important to consider the time required to complete a task and the number of API calls (tokens) used for each task. One of the major challenges is reducing hallucination in the system, which is an active area of research. Therefore, there are no set rules for building LLMs and agent systems. It's necessary to work patiently and strategically to ensure the AI agent, the LLM, is functioning correctly.\n\n*All images, unless otherwise noted, are by the author.*\n\n\n### Reference:\n\n[https://docs.together.ai/docs/llama\\-3\\-function\\-calling](https://docs.together.ai/docs/llama-3-function-calling)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/visualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-with-ragas-fc2486308557","frontmatter":{"title":"Visualize your RAG Data — Evaluate your  Retrieval-Augmented Generation System with Ragas","meta_title":"Visualize your RAG Data — Evaluate your  Retrieval-Augmented Generation System with Ragas","description":"How to use UMAP dimensionality reduction for Embeddings to show multiple evaluation Questions and their relationships to source documents…","date":"2024-11-04T12:35:56.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*peWTe1A-MqeROT_Jdof_Cw.gif","categories":["Natural Language Processing","Generative AI","Data Science"],"author":"Rifx.Online","tags":["RAG","UMAP","embeddings","evaluation","visualization"],"draft":false,"slug":"blog/visualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-with-ragas-fc2486308557"},"content":"\n\n\n\n\n### How to use UMAP dimensionality reduction for Embeddings to show multiple evaluation Questions and their relationships to source documents with Ragas, OpenAI, Langchain and ChromaDB\n\nRetrieval\\-Augmented Generation (RAG) adds a retrieval step to the workflow of an LLM, enabling it to query relevant data from additional sources like private documents when responding to questions and queries \\[1]. This workflow does not require costly training or fine\\-tuning of LLMs on the additional documents. The documents are split into snippets, which are then indexed, often using a compact ML\\-generated vector representation (embedding). Snippets with similar content will be in proximity to each other in this embedding space.\n\nThe RAG application projects the user\\-provided questions into the embedding space to retrieve relevant document snippets based on their distance to the question. The LLM can use the retrieved information to answer the query and to substantiate its conclusion by presenting the snippets as references.\n\n\n\nThe evaluation of a RAG application is challenging \\[2]. Different approaches exist: on one hand, there are methods where the answer as ground truth must be provided by the developer; on the other hand, the answer (and the question) can also be generated by another LLM. One of the largest open\\-source systems for LLM\\-supported answering is Ragas \\[4](Retrieval\\-Augmented Generation Assessment), which provides\n\n* Methods for generating test data based on the documents and\n* Evaluations based on different metrics for evaluating retrieval and generation steps one\\-by\\-one and end\\-to\\-end.\n\nIn this article, you will learn\n\n* How to briefly build a RAG system for Formula One (see the previous article [Visualize your RAG Data — EDA for Retrieval\\-Augmented Generation](https://readmedium.com/visualize-your-rag-data-eda-for-retrieval-augmented-generation-0701ee98768f) for detailed descriptions)\n* Generate questions and answers\n* Evaluate the RAG system with [Ragas](https://github.com/explodinggradients/ragas)\n* Most importantly how to visualize the results with [Renumics Spotlight](https://github.com/Renumics/spotlight) and interpret the results.\n\nThe code is available at Github\n\n\n## Get your environment ready\n\nStart a notebook and install the required python packages\n\n\n```python\n!pip install langchain langchain-openai chromadb renumics-spotlight\n%env OPENAI_API_KEY=<your-api-key>\n```\nThis tutorial uses the following python packages:\n\n* [**Langchain**](https://github.com/langchain-ai/langchain): A framework to integrate language models and RAG components, making the setup process smoother.\n* [**Renumics\\-Spotlight**](https://github.com/Renumics/spotlight): A visualization tool to interactively explore unstructured ML datasets.\n* [**Ragas**](https://github.com/explodinggradients/ragas): a framework that helps you evaluate your RAG pipelines\n\n*Disclaimer: The author of this article is also one of the developers of Spotlight.*\n\n\n## Prepare documents and embeddings for the dataset\n\nYou can use your own RAG Application, skip to the next part to learn how to evaluate, extract and visualize.\n\nOr you can use the RAG application from the [last article](https://readmedium.com/visualize-your-rag-data-eda-for-retrieval-augmented-generation-0701ee98768f) with [our prepared dataset of all Formula One articles of Wikipedia](https://spotlightpublic.blob.core.windows.net/docs-data/rag_demo/docs.zip). There you can also insert your own Documents into a ‘docs/’ subfolder.\n\n\n> This dataset is based on articles from [Wikipedia](https://www.wikipedia.org/) and is licensed under the Creative Commons Attribution\\-ShareAlike License. The original articles and a list of authors can be found on the respective Wikipedia pages.\n\nNow you can use Langchain’s `DirectoryLoader` to load all files from the docs subdirectory and split the documents in snippets using the `RecursiveCharacterTextSpliter`. With `OpenAIEmbeddings` you can create embeddings and store them in a `ChromaDB` as vector store. For the Chain itself you can use LangChains `ChatOpenAI` and a `ChatPromptTemplate`.\n\nThe [linked code](https://github.com/Renumics/rag-demo/blob/main/notebooks/visualize_rag_tutorial_qs.ipynb) for this article contains all necessary steps and you can find a detailed description of all steps above in [the last article](https://readmedium.com/visualize-your-rag-data-eda-for-retrieval-augmented-generation-0701ee98768f).\n\nOne important point is, that you should use a hash function to create ids for snippets in `ChromaDB`. This allows to find the embeddings in the db if you only have the document with its content and metadata. This makes it possible to skip documents that already exist in the database.\n\n\n```python\nimport hashlib\nimport json\nfrom langchain_core.documents import Document\n\ndef stable_hash_meta(doc: Document) -> str:\n    \"\"\"\n    Stable hash document based on its metadata.\n    \"\"\"\n    return hashlib.sha1(json.dumps(doc.metadata, sort_keys=True).encode()).hexdigest()\n\n...\nsplits = text_splitter.split_documents(docs)\nsplits_ids = [\n    {\"doc\": split, \"id\": stable_hash_meta(split.metadata)} for split in splits\n]\n\nexisting_ids = docs_vectorstore.get()[\"ids\"]\nnew_splits_ids = [split for split in splits_ids if split[\"id\"] not in existing_ids]\n\ndocs_vectorstore.add_documents(\n    documents=[split[\"doc\"] for split in new_splits_ids],\n    ids=[split[\"id\"] for split in new_splits_ids],\n)\ndocs_vectorstore.persist()\n```\n\n## Evaluation Questions\n\nFor a common topic like Formula One, one can also use ChatGPT directly to generate general questions. In this article, four methods of question generation are used:\n\n* **GPT4**: 30 questions were generated using ChatGPT 4 with the following prompt “Write 30 question about Formula one”\n– Random Example: “Which Formula 1 team is known for its prancing horse logo?”\n* **GPT3\\.5:** Another 199 question were generated with ChatGPT 3\\.5 with the following prompt “Write 100 question about Formula one” and repeating “Thanks, write another 100 please”\n– Example: “”Which driver won the inaugural Formula One World Championship in 1950?”\n* **Ragas\\_GPT4**: 113 questions were generated using Ragas. Ragas utilizes the documents again and its own embedding model to construct a vector database, which is then used to generate questions with GPT4\\.\n– Example: “Can you tell me more about the performance of the Jordan 198 Formula One car in the 1998 World Championship?”\n* **Rags\\_GPT3\\.5**: 226 additional questions were generated with Ragas — here we use GPT3\\.5\n– Example: “What incident occurred at the 2014 Belgian Grand Prix that led to Hamilton’s retirement from the race?”\n\n\n```python\nfrom ragas.testset import TestsetGenerator\n\ngenerator = TestsetGenerator.from_default(\n    openai_generator_llm=\"gpt-3.5-turbo-16k\", \n    openai_filter_llm=\"gpt-3.5-turbo-16k\"\n)\n\ntestset_ragas_gpt35 = generator.generate(docs, 100)\n```\nThe questions and answers were not reviewed or modified in any way. All questions are combined in a single dataframe with the columns `id`, `question`, `ground_truth`, `question_by` and `answer`.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*R_74K0-_SJXyTxq6ovAcWg.png)\n\nNext, the questions will be posed to the RAG system. For over 500 questions, this can take some time and incur costs. If you ask the questions row\\-by\\-row, you can pause and continue the process or recover from a crash without losing the results so far:\n\n\n```python\nfor i, row in df_questions_answers.iterrows():\n    if row[\"answer\"] is None or pd.isnull(row[\"answer\"]):\n        response = rag_chain.invoke(row[\"question\"])\n\n        df_questions_answers.loc[df_questions_answers.index[i], \"answer\"] = response[\n            \"answer\"\n        ]\n        df_questions_answers.loc[df_questions_answers.index[i], \"source_documents\"] = [\n            stable_hash_meta(source_document.metadata)\n            for source_document in response[\"source_documents\"]\n        ]\n\n```\nNot only is the answer stored but also the source IDs of the retrieved document snippets, and their text content as context:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*umlKv7Qf9SSLzRslT2r0Qw.png)\n\nAdditionally, the embeddings for all questions are generated and stored in the dataframe as well. This allows for visualizing them alongside the documents.\n\n\n## Evaluation with Ragas\n\n[Ragas](https://github.com/explodinggradients/ragas) provides metrics for evaluating each component of your RAG pipeline in isolation and end\\-to\\-end metrics for overall performance:\n\n1. **Context Precision:** Uses the `question` and retrieved `contexts` to measure the signal\\-to\\-noise ratio.\n2. **Context Relevancy:** Measures the relevance of the retrieved context to the question, calculated using the `question` and `contexts`.\n3. **Context Recall:** Based on the `ground truth` and `contexts` to check if all relevant information for the answer is retrieved.\n4. **Faithfulness:** Utilizes the `contexts` and `answer` to measure how factually accurate the generated answer is.\n5. **Answer Relevance:** Computed using the `question` and `answer` to assess the relevance of the generated answer to the question (does not consider factuality).\n6. **Answer Semantic Similarity:** Evaluated using the `ground truth` and `answer` to assess the semantic resemblance between the generated and the correct answer.\n7. **Answer Correctness:** Relies on the `ground truth` and `answer` to measure the accuracy and alignment of the generated answer with the correct one.\n8. **Aspect Critique:** Involves analyzing the `answer` to evaluate submissions based on predefined or custom aspects such as correctness or harmfulness.\n\nFor now, we focus on the end\\-to\\-end metric of answer correctness. The column names and content in the dataframe are copied and adapted to meet the naming and formatting requirements according to the Ragas API:\n\n\n```python\n## prepare the dataframe for evaluation\ndf_qa_eval = df_questions_answers.copy()\n\n\n## adapt the ground truth to the ragas naming and format\ndf_qa_eval.rename(columns={\"ground_truth\": \"ground_truths\"}, inplace=True)\ndf_qa_eval[\"ground_truths\"] = [\n    [gt] if not isinstance(gt, list) else gt for gt in df_qa_eval[\"ground_truths\"]\n]\n```\nThis again can take some time and even more money than just querying your RAG system. Let’s apply the evaluation row\\-by\\-row to be able to recover from a crash without losing the results so far:\n\n\n```python\n## evaluate the answer correctness if not already done\nfields = [\"question\", \"answer\", \"contexts\", \"ground_truths\"]\nfor i, row in df_qa_eval.iterrows():\n    if row[\"answer_correctness\"] is None or pd.isnull(row[\"answer_correctness\"]):\n        evaluation_result = evaluate(\n            Dataset.from_pandas(df_qa_eval.iloc[i : i + 1][fields]),\n            [answer_correctness],\n        )\n        df_qa_eval.loc[i, \"answer_correctness\"] = evaluation_result[\n            \"answer_correctness\"\n        ]\n\n```\nAfterwards, you can store the results in the `df_questions_answer` dataframe:\n\n\n```python\ndf_questions_answers[\"answer_correctness\"] = df_qa_eval[\"answer_correctness\"]\n```\n\n## Prepare visualization\n\nTo include the document snippets in the visualization, we add references from documents to questions that used the document as a source. Additionally, the count of questions referencing a document is stored:\n\n\n```python\n## Explode 'source_documents' so each document ID is in its own row alongside the question ID\ndf_questions_exploded = df_qa_eval.explode(\"source_documents\")\n\n## Group by exploded 'source_documents' (document IDs) and aggregate\nagg = (\n    df_questions_exploded.groupby(\"source_documents\")\n    .agg(\n        num_questions=(\"id\", \"count\"),  # Count of questions referencing the document\n        question_ids=(\n            \"id\",\n            lambda x: list(x),\n        ),  # List of question IDs referencing the document\n    )\n    .reset_index()\n    .rename(columns={\"source_documents\": \"id\"})\n)\n\n## Merge the aggregated information back into df_documents\ndf_documents_agg = pd.merge(df_docs, agg, on=\"id\", how=\"left\")\n\n## Use apply to replace NaN values with empty lists for 'question_ids'\ndf_documents_agg[\"question_ids\"] = df_documents_agg[\"question_ids\"].apply(\n    lambda x: x if isinstance(x, list) else []\n)\n## Replace NaN values in 'num_questions' with 0\ndf_documents_agg[\"num_questions\"] = df_documents_agg[\"num_questions\"].fillna(0)\n```\nNow concatenate the dataframe of questions with the dataframe of the documents\n\n\n```python\ndf = pd.concat([df_qa_eval, df_documents_agg], axis=0)\n```\nAdditionally, let’s prepare some different UMAP \\[3] mappings. You could do much the same in the Spotlight GUI later, but doing it upfront can save time.\n\n* umap\\_all: UMAP with fit and transform applied on all document and question embeddings\n* umap\\_questions: UMAP with fit applied on questions embeddings only and transform applied on both\n* umap\\_docs: UMAP with fit applied on document embeddings only and transform applied on both\n\nWe prepare each of the UMAP transformations like this:\n\n\n```python\numap = UMAP(n_neighbors=20, min_dist=0.15, metric=\"cosine\", random_state=42).fit\numap_all = umap.transform(df[\"embedding\"].values.tolist())\ndf[\"umap\"] = umap_all.tolist()\n\n```\nAnother interesting metric for each of the document snippets is the distance between its embeddings and the embeddings of the nearest question\n\n\n```python\nquestion_embeddings = np.array(df[df[\"question\"].notna()][\"embedding\"].tolist())\ndf[\"nearest_question_dist\"] = [  # brute force, could be optimized using ChromaDB\n    np.min([np.linalg.norm(np.array(doc_emb) - question_embeddings)])\n    for doc_emb in df[\"embedding\"].values\n]\n```\nThis metric can be helpful to find documents that are not referenced by questions.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*YTRUXZmd0iX8kyPIdUUnlg.png)\n\n\n## Visualize results\n\nIf you skipped the previous steps, you can download the dataframe and load it with:\n\n\n```python\nimport pandas as pd\ndf = pd.read_parquet(\"df_f1_rag_docs_and_questions.parquet\")\n```\nand start [Renumics Spotlight](https://github.com/Renumics/spotlight) to visualize it with:\n\n\n```python\nfrom renumics import spotlight\n\nspotlight.show(df)\nspotlight.show(\n    df,\n    layout=\"/home/markus/Downloads/layout_rag_1.json\",\n    dtype={x: Embedding for x in df.keys() if \"umap\" in x},\n)\n```\nIt will open a new brwoser window:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*IMbva0pP8RAVhoY4dVbjLg.png)\n\nOn the top left side, you can see a **table of all questions and all document** snippets. You can use the “visible columns” button to control which columns of the dataframe are shown in the table. It is useful to create a filter directly that selects only the questions to be able to turn the questions on and off in the visualizations: Select all questions and and then create a filter using the “Create filter from selected row” button.\n\nTo the right of the table, the `answer correctness` **is displayed as a metric** across all questions. Below there are two **histograms**; the left one shows the distribution of `answer correctness` divided into the different methods of question generation. The right one shows the distribution of methods of question generation. Here, it is advisable to create a filter for the questions using the filter button to display only the selected rows (the questions) if needed.\n\nOn the right side, there are **two similarity maps.** The first one uses the `umap_questions` column and shows the questions and documents based on the transformation applied only to the questions. It is helpful for viewing the distribution of questions independently from the associated documents because this approach allows analysts to identify patterns or clusters within the questions themselves.\n\nThe second similarity map shows the questions and documents based on the transformation applied only to the documents (`umap_docs`). It is useful for viewing the questions in the context of their associated documents. A similarity map that simultaneously transforms questions and documents has proven to be less helpful with a larger number of questions, as more or fewer questions get clustered together and tend to be separated from the documents. Therefore, this representation is omitted here.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*1wZrAj60hiw1T3RVnCuBtA.png)\n\n\n### Document Embedding Similarity Map: Observations\n\nIn the similarity map `umap_docs`, you can identify areas in the embedding space of the documents that have no neighboring questions. It is even better recognized when selecting `nearest_question_dist` for coloring.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*cMGNPnnBa9Bn7BJ05SzxBw.png)\n\nSome clusters can be identified, including snippets that contain only headings or tabular data containing only numbers page by page, whose meaning is lost during splitting. Additionally, many Wikipedia\\-specific text additions that contain no relevant information, such as links to other languages or editing notes, form clusters with no neighboring questions.\n\nRemoving the noise in form of Wikipedia\\-related text is very simple when using the Wikipedia API. It is probably not particularly necessary, as it mainly costs some space — it is not expected that the RAG result will be particularly worsened by it. However, data contained in large tables are hardly captured by the RAG system and it could ne benifical to extract these using advanced pre\\-processing methods for Table Extraction and to connect them to the RAG system.\n\nAnother point that you can observe in the `umap_docs` similarity map is how the questions from different sources are distributed.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*IH7z3J4yUmU0C_SruxnDkg.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*K4bADgDmSAr5t4t4r9VImQ.png)\n\nThe questions that were directly generated by ChatGPT (GPT\\-3\\.5, GPT\\-4\\) are located in a more confined area in the center, whereas the questions generated with ragas based on the documents cover a larger area.\n\n\n### Answer correctness histogram\n\nThe histogram can be used as a starting point to get an initial impression of the global statistics of the data. Overall, across all questions, the `answer correctness` is 0\\.45\\. For the questions created without ragas, it is 0\\.36, and for questions with ragas, it is 0\\.52\\. It was expected that the system would perform better for questions generated by ragas, as these questions are based on the available data, whereas the questions directly generated by ChatGPT could come from all the data with which ChatGPT was trained.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GsLBsg7uwTrw-AzvO4BHmw.png)\n\nA quick, random manual review of some of the questions/answers and ground truth shows that in the interval of`answer correctness`0\\.3–0\\.4, most questions were still correctly answered according to the ground truth. In the interval 0\\.2–0\\.3, many incorrect answers are present. In the interval 0\\.1–0\\.2, most answers are incorrect. Notably, almost all questions in this range came from GPT\\-3\\.5\\. The two questions in this interval generated with GPT\\-4 were answered correctly even though they received an `answer correctness` of below 0\\.2\\.\n\n\n### Questions Embedding Similarity Map: Observations\n\nThe Questions Embedding Similarity Map can be helpful to dig deeper into `answer correctness` by examining clusters of similar questions that may cause similar problems.\n\n* **Cluster “Term for driver/process/cars”:** average `answer correctness` 0\\.23: Answers often not precise enough. E.g., Chassis tuning vs. Chassis flexing or brake tuning vs. brake bias adjustment. It is questionable whether these types of questions are suitable for evaluating the system, as it seems very difficult to judge the answers.\n* **Cluster “Terms for fuel strategy:”** average `answer correctness`0\\.44, similar to the global`answer correctness`.\n* **Cluster “Names of tracks”:** average `answer correctness` 0\\.49, similar to the global `answer correctnes`.\n* **Cluster “Who holds the record for…”**: average `answer correctness` 0\\.44, similar to the global `answer correctness`.\n* **Cluster “Win championship with…”**: average `answer correctnes` 0\\.26 — looks challenging. Questions with many conditions, e.g., “Who is the only driver to win the Formula One World Championship with a British racing license, driving for an Italian team with an American engine.” Extended RAG methods like Multi Query might help improve here.\n* **Cluster “Who is the only driver to win… with a car bearing the number \\<number\\>”**: average `answer correctness` 0\\.23 — looks like GPT\\-3\\.5 was lazy here, repeating the same question with different numbers, even though most ground truth entries are wrong!\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Yc03cpSEFlJoZSBPIpMkiQ.png)\n\n\n## Conclusion\n\nIn conclusion, utilizing UMAP\\-based visualizations offers a interesting approach to dig deeper than just analyzing global metrics. The document embedding similarity map gives a good overview, illustrating the clustering of similar documents and their relation to evaluation questions. The question similarity map reveals patterns that allow the differentiation and analysis of questions in conjunction with quality metrics to enable insight generation. Follow the Visualize results section to apply the visualization on your evaluation strategy — what insights will you uncover?\n\n*I am a professional with expertise in creating advanced software solutions for the interactive exploration of unstructured data. I write about unstructured data and use powerful visualization tools to analyze and make informed decisions.*\n\n\n## References\n\n\\[1] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, Haofen Wang: [Retrieval\\-Augmented Generation for Large Language Models: A Survey](https://arxiv.org/abs/2312.10997) (2024\\), arxiv\n\n\\[2] Yixuan Tang, Yi Yang: [MultiHop\\-RAG: Benchmarking Retrieval\\-Augmented Generation for Multi\\-Hop Queries](https://arxiv.org/abs/2401.15391) (2021\\), arXiv\n\n\\[3] Leland McInnes, John Healy, James Melville: [UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction](https://arxiv.org/abs/1802.03426) (2018\\), arXiv\n\n\\[4] Shahul Es, Jithin James, Luis Espinosa\\-Anke, Steven Schockaert: [RAGAS: Automated Evaluation of Retrieval Augmented Generation](https://arxiv.org/abs/2309.15217) (2023\\), arXiv\n\n\n"},{"lang":"en","group":"blog","slug":"blog/whats-new-with-claude-sonnet-3-5-claude-3-5-haiku-c1f62a2d2c72","frontmatter":{"title":"What’s new with Claude Sonnet 3.5 & Claude 3.5 Haiku?","meta_title":"What’s new with Claude Sonnet 3.5 & Claude 3.5 Haiku?","description":"And is it worth checking out?","date":"2024-11-13T01:22:29.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*CEMTDlHlMUX66-eoMcSOzg.png","categories":["Natural Language Processing","Programming","Technology/Web"],"author":"Rifx.Online","tags":["language","models","interaction","automation","latency"],"draft":false,"slug":"blog/whats-new-with-claude-sonnet-3-5-claude-3-5-haiku-c1f62a2d2c72"},"content":"\n\n\n\n\n\n\n### First off, what is Claude?\n\nClaude is a language model created by [Anthropic](https://proxy.rifx.online/https://www.anthropic.com/) and it’s designed to help with tasks like answering questions, summarizing information and generating text — similar to ChatGPT. The cool thing about Claude is that it’s built to be safer and more aligned with human intentions, making it less likely to produce harmful or misleading content.\n\n\n### Wait… wasn’t Claude 3\\.5 Sonnet out already?\n\nHaha yes, while there haven’t been any changes to the name there are a lot of exciting updates to this new version of Claude 3\\.5 Sonnet and Claude Haiku that was released on October 22nd, 2024\\.\n\nThese new models are faster and better at tasks like debugging code and transcribing text from images, which makes them especially useful for industries like retail and logistics.\n\n*If you prefer to discuss these new changes and see how you can implement them into your project, [click here to schedule a free call with us](https://proxy.rifx.online/https://calendly.com/woyera-ai/)!*\n\n\n### Let’s dive in and see what’s new!\n\n\n## Human\\-Computer Interaction\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*p1anQynliN8ihnT8X2VYqw.gif)\n\nOne of the biggest updates with Claude 3\\.5 Sonnet is its ability to interact with computers in a more human\\-like way.\n\nIt can now navigate screens, click buttons, and type, which opens up some really interesting possibilities for automating tasks or even helping out in real\\-time workflows.\n\nKeep in mind that this feature is still in public beta, but it’s already showing a lot of promise, especially for robotic process automation (RPA).\n\n\n## Enhanced Coding Support\n\nClaude can assist throughout the entire software development process from designing, debugging, and optimizing code — making it a valuable asset for anyone in tech.\n\n\n## Improved Chatbots\n\nClaude’s natural tone and advanced reasoning make it perfect for building more responsive and engaging chatbots.\n\nIt can handle complex conversations and even connect with various systems to streamline tasks which make it a great fit for customer service, technical support and more.\n\n\n## Visual Data Extraction\n\nOne standout feature is Claude’s ability to analyze visual data. It can interpret and pull information from charts, graphs, and diagrams with ease.\n\n\n## Knowledge Q\\&A\n\nClaude 3\\.5 Sonnet is also great for answering detailed questions using large datasets, knowledge bases, or code repositories. With a bigger context window it’s a reliable option for businesses that need quick, accurate information.\n\n\n## Availability and Pricing\n\nClaude 3\\.5 is available as an API on Anthropic API, Amazon Bedrock, and Google Cloud’s Vertex AI. For the API, pricing starts at **$3 per million input tokens** and **$15 per million output tokens.**\n\nOr you can simply use it on the web through [claude.ai](https://proxy.rifx.online/https://claude.ai/login?returnTo=%2F%3F). You can create an account for Free and then plans will be $20/month for Pro, $25/month for Team or [Enterprise.](https://proxy.rifx.online/https://www.anthropic.com/pricing)\n\n\n## Safety and Trust\n\nAnthropic has put a lot of effort into making Claude 3\\.5 Sonnet safe to use. The model has gone through extensive testing to ensure it can handle sensitive content responsibly without compromising on performance.\n\nThis focus on safety helps protect against issues like inappropriate content and ensures Claude is suitable for a wide range of applications.\n\n\n## Use Cases\n\nWhether you’re a developer, business owner, or just curious about AI, Claude 3\\.5 Sonnet has a lot to offer.\n\n\n### Automating repetitive tasks\n\nA customer service team can use Claude to handle repetitive backend tasks like updating customer orders or processing refunds. Claude’s ability to navigate screens and click buttons can save hours of manual work.\n\n\n### Chatbots\n\nA healthcare provider can build a chatbot with Claude to handle patient interactions such as booking appointments, answering medical FAQs, or guiding patients through symptom checks. All while maintaining a natural \\& conversational tone.\n\n\n### Visual data\n\nFinancial analysts can use Claude to analyze quarterly earnings reports that include a lot of charts and graphs. Claude can quickly extract insights and summarize key trends.\n\n\n### Knowledge Q\\&A\n\nA tech company can use Claude to manage an internal knowledge base. Developers could ask detailed questions about existing code repositories or troubleshooting steps and Claude would provide quick \\& reliable answers.\n\n\n## What about Claude 3\\.5 Haiku?\n\nClaude 3\\.5 Haiku is Anthropic’s fastest AI model and offers better performance without raising costs or slowing down. It’s stronger at tasks like coding and beats out models like Claude 3 Opus and GPT\\-4o in benchmarks.\n\nThe model is designed with low latency, meaning it responds quickly, making it perfect for real\\-time apps, personalized tasks like analyzing purchase history, and other data\\-heavy projects. It’ll be available later this month on APIs like Amazon Bedrock and Google Cloud, starting as a text\\-only model with image support coming soon.\n\n\n## So, how does Claude compare to ChatGPT?\n\nWhen comparing **Claude 3\\.5** and **ChatGPT**, both are advanced AI models designed to tackle similar tasks like answering questions, generating text, and assisting with coding, but they have distinct differences that might cater to different needs.\n\nClaude 3\\.5 is built with a strong emphasis on **safety**, reducing the risk of harmful or misleading outputs. While ChatGPT also prioritizes safety, Claude’s design places extra focus on this area.\n\nIn terms of **speed**, Claude 3\\.5 Haiku offers faster response times, making it ideal for real\\-time applications. ChatGPT is also quick, but may experience slight delays on more complex tasks.\n\nFor **coding**, both models perform well. Claude 3\\.5 Sonnet stands out in recent benchmarks for debugging and code generation, while ChatGPT remains a reliable option for coding help and explanations.\n\nA key difference is **real\\-world interaction**. Claude 3\\.5 can navigate screens and automate tasks, a feature ChatGPT does not yet offer.\n\nIn **pricing**, ChatGPT has a widely available free version, while Claude offers flexible pricing with its API and cloud platforms.\n\nClaude’s focus on safety, low latency, and advanced real\\-world interactions make it a good fit for more specialized applications. Meanwhile ChatGPT’s versatility, wide availability, and strong coding support make it a great general\\-purpose tool.\n\nUltimately, the choice between the two depends on what you’re looking for but both models offer strong capabilities with their unique strengths.\n\n\n## Conclusion\n\nThe best way to learn about a new tool is to try it out yourself! You can use Claude 3\\.5 Sonnet for enhancing coding, chatbots, data analysis and much more.\n\nLet us know what you will use Claude for.\n\n*If you ever have a custom chatbot or app you need to build, [click here to have a quick call with us.](https://proxy.rifx.online/https://calendly.com/woyera-ai/)*\n\n\n"},{"lang":"en","group":"blog","slug":"blog/why-embedding-matters-when-building-a-non-english-rag-system-multilingual-embeddings-1e3434ea6180","frontmatter":{"title":"Why embedding matters when building a non-English RAG system — Multilingual embeddings","meta_title":"Why embedding matters when building a non-English RAG system — Multilingual embeddings","description":"Discover why multilingual embeddings are crucial for RAG systems, with a detailed comparison of English vs. multilingual models in Dutch.","date":"2024-11-13T01:22:29.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*QvODAYxqisUTrt4V.png","categories":["Natural Language Processing","Machine Learning","Multilingual"],"author":"Rifx.Online","tags":["embeddings","multilingual","RAG","Cohere","Dutch"],"draft":false,"slug":"blog/why-embedding-matters-when-building-a-non-english-rag-system-multilingual-embeddings-1e3434ea6180"},"content":"\n\n\n\n\n## Why Embeddings are Key\n\nEmbeddings are a cornerstone of modern generative AI, silently driving the functionality of many systems we interact with daily. At their simplest, embeddings are **numerical representations of text** — effectively transforming words, sentences, or even entire documents into numbers. These numbers are far from random; they’re carefully designed to capture the meaning and relationships within the text. For instance, the embeddings for “dog” and “puppy” would be closer together in the numerical space than the embedding for “car,” reflecting their **semantic similarity**. This ability to encode meaning into a measurable form is what makes embeddings indispensable for tasks like search, recommendation systems, and advanced AI applications such as **Retrieval\\-Augmented Generation (RAG)**.\n\n\n\nThis transformation into numbers allows AI to compare and understand text in a meaningful way. When working with massive amounts of data, as is often the case in RAG systems, embeddings become essential. These systems combine the power of embeddings with specialized storage solutions called **vector databases**. Unlike traditional databases that search for exact matches, vector databases are optimized to find the closest matches based on meaning. This capability enables RAG systems to retrieve the most relevant information from vast knowledge bases and use it to generate precise, contextually informed responses. By bridging raw data and intelligent retrieval, embeddings and vector databases together form the backbone of RAG systems’ success.\n\n\n## The Challenge of Multilingual Systems\n\nBuilding RAG systems that work well in English is already a complex task, but extending them to other languages introduces a whole new set of challenges. English embeddings are often highly optimized because of the abundance of training data and the simplicity of the language’s structure. However, using these English\\-trained embeddings for other languages can lead to significant inaccuracies. Different languages come with their own nuances, grammar, and cultural contexts, which standard embedding models trained predominantly on English text often fail to capture. While some multilingual embedding models exist to bridge this gap, they are not all equally effective across languages, particularly for those with limited training data or unique linguistic features. This makes it difficult to build RAG systems that are as accurate and reliable for non\\-English languages as they are for English.\n\n\n### Why Are English Embeddings More Accurate?\n\n1. **Abundance of High\\-Quality Training Data**\nEnglish dominates the digital landscape, with an unparalleled volume of high\\-quality content available for training. Datasets like Wikipedia, books, research papers, and social media are much richer in English than in other languages. In contrast, many languages, especially low\\-resource ones, lack diverse and standardized datasets, which limits the quality of embeddings trained on them.\n2. **Model Optimization Bias**\nNLP models like BERT and GPT were initially developed and optimized for English, often prioritizing it even in multilingual versions. Multilingual models balance learning across many languages within the same parameter space, which can dilute performance for less\\-represented languages in favor of dominant ones like English.\n3. **Linguistic Complexity and Diversity**\nEnglish has relatively simple morphology compared to many other languages. For instance, word forms in English tend to remain consistent (e.g., “run” and “running”), while languages like Turkish or Finnish have highly inflected forms, where a single root word can have dozens of variations. Additionally, languages with different syntax or word order, such as Japanese (Subject\\-Object\\-Verb) or Arabic (flexible word order), pose extra challenges for models optimized for English\\-like structures.\n4. **Semantic and Cultural Alignment**\nCapturing semantic meaning across languages is far from straightforward. Words and phrases often carry nuanced meanings that don’t translate directly. For example, the English word “love” has multiple culturally distinct equivalents in other languages (e.g., “amor” in Spanish, “eros” or “agape” in Greek). Embeddings that fail to account for these differences struggle with multilingual alignment.\n5. **Benchmarking and Evaluation Bias**\nMany benchmarking datasets and evaluation methods are designed with English in mind. This English\\-centric focus can artificially inflate the perceived performance of models in English while masking their limitations in other languages.\n\n\n### The Impact on RAG Systems\n\nWhen embeddings fail to handle the complexity of other languages, the consequences for RAG systems can be significant. Retrieval results often become less relevant or even outright wrong, as the embeddings may struggle to capture the nuanced meaning of non\\-English queries. This doesn’t just impact accuracy — it also undermines user trust and the overall utility of the system. Crucial text chunks may be missed during retrieval, preventing the system from accessing the information it needs to generate accurate and contextually relevant responses.\n\nFor a multilingual RAG system to perform well, it requires embeddings that can align semantically across languages while accounting for their unique structural and cultural intricacies. Investing in high\\-quality multilingual embeddings and fine\\-tuning them for specific languages or tasks is essential. This ensures that RAG systems can meet the needs of users in any language — not just English.\n\nBut how well do different embeddings actually perform in a non\\-English context? To explore this, we’ll compare an English embedding model with a multilingual embedding model using a Dutch dataset. This test will reveal how different approaches to embeddings impact retrieval accuracy and the quality of the generated responses in a multilingual RAG system.\n\n\n## Comparing Embedding Models for a Dutch RAG System\n\nTo understand how different embedding models handle a non\\-English language like Dutch, we’ll compare two models available on Amazon Bedrock: **Cohere Embed English v3** and **Cohere Embed Multilingual v3**. These models represent different approaches to embeddings — one optimized exclusively for English and the other designed for multilingual tasks. The table below summarizes their key attributes:\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*pBhIHfOsb-McrjHKvtq4Xw.png)\n\n\n### Build Embeddings\n\nTo evaluate the performance of the embedding models, we will build a local vectorstore using the LangChain framework. For this evaluation, we will use a guideline for firefighters written in Dutch as our dataset. This document contains technical and procedural information, making it a realistic and challenging use case for semantic retrieval in a non\\-English language. Below is the cleaned and streamlined code for creating a local vectorstore and indexing document chunks. We’ll use this setup to test two embedding models: **Cohere Embed English v3** and **Cohere Embed Multilingual v3**.\n\n\n```python\nimport os\nfrom langchain_community.document_loaders import DirectoryLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import FAISS\nfrom langchain_aws import BedrockEmbeddings\nimport boto3\n\n## Step 1: Load documents\nloader = DirectoryLoader('data', glob=\"**/*.pdf\")  # Adjust 'data' to your document directory\ndocuments = loader.load()\n\nprint(f\"You have {len(documents)} documents\")\nprint(f\"Document 1 contains {len(documents[0].page_content)} characters\")\n\n## Step 2: Split documents into smaller chunks\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)\nchunks = text_splitter.split_documents(documents)\n\nprint(f\"You have {len(chunks)} chunks\")\nprint(f\"The first chunk is {len(chunks[0].page_content)} characters long\")\n\n## Step 3: Set up Bedrock embeddings\nbedrock_client = boto3.client(\"bedrock-runtime\", region_name='us-east-1')\nbedrock_embeddings = BedrockEmbeddings(model_id=\"cohere.embed-multilingual-v3\", client=bedrock_client)\n\n## Step 4: Build the FAISS vectorstore\nvectorstore = FAISS.from_documents(chunks, bedrock_embeddings)\n\n## Save the vectorstore locally for reuse\nvectorstore.save_local(\"faiss_cohere_multilingual\")\n```\n\n## How This Code Works\n\n1. **Document Loading**:\nThe code loads all PDF files from the `data` directory. You can adjust the file path and format to match your dataset.\n2. **Text Splitting**:\nDocuments are split into smaller chunks of 400 characters with a 50\\-character overlap to improve retrieval accuracy. This ensures each chunk remains contextually meaningful.\n3. **Embedding Models**:\nThe `BedrockEmbeddings` class initializes the embedding model. You can replace the `model_id` to test **Cohere Embed English v3 or Cohere Embed Multilingual v3**.\n4. **Local Vectorstore**:\nThe FAISS library is used to create an in\\-memory vectorstore from the document chunks. This allows for fast similarity searches and can be saved locally for reuse.\n\nTo test all models, replace the `model_id` in the `BedrockEmbeddings` initialization with the appropriate model:\n\n* `\"cohere.embed-english-v3\"` for Cohere English.\n* `\"cohere.embed-multilingual-v3\"` for Cohere Multilingual.\n\n\n### Evaluating the Embedding Models\n\nTo evaluate the performance of the embedding models, we will ask the question: **“Welke rangen zijn er bij de brandweer?”**, which translates to **“Which ranks exist within the fire department?”**. This question was chosen because our document only uses the term **“hiërarchie”**, which in Dutch has a similar semantic meaning to **“rangen”**. However, in English, “hierarchy” and “ranks” do not share semantic similarity.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*6N3C8C500hMQ3GNNkuu21A.png)\n\nThis distinction is crucial for our test. We expect the **Cohere Embed English v3** model to struggle with this query, as it relies on English semantics where the terms are unrelated. On the other hand, the **Cohere Embed Multilingual v3** model, which is trained to understand Dutch semantics, should retrieve the correct information from the document, demonstrating its ability to handle semantic nuances in non\\-English languages.\n\nBy asking this question, we aim to highlight how semantic alignment affects retrieval performance in a Dutch RAG system. This test will provide a clear comparison of the models’ ability to handle non\\-English queries and retrieve relevant information. The results will showcase the importance of multilingual embeddings for achieving accurate retrieval in non\\-English contexts.\n\nTo implement and test this setup, we can use the following code. This script demonstrates how to query the vectorstore and utilize a RAG chain to combine the embeddings with a language model for answering questions. Note that when testing different embeddings (e.g., **Cohere Embed English v3** vs. **Cohere Embed Multilingual v3**), you need to ensure that the vectorstore is built using the corresponding embedding model. Replace the vectorstore with the one indexed using the embedding model you want to test for accurate results.\n\n\n```python\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain_aws import ChatBedrock\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.output_parsers import StrOutputParser\n\ninstructions = \"\"\"Je bent een brandweer expert. Beantwoord de vraag, maak gebruik van de context\"\"\"\n\nhuman = \"\"\"\nDit is de context: {context}\nDit is de vraag: {question}\n\"\"\"\n\nprompt = ChatPromptTemplate(\n    messages=[\n        SystemMessagePromptTemplate.from_template(instructions),\n        HumanMessagePromptTemplate.from_template(human), #User query will go here\n    ],\n    input_variables=['context','question'], # context provided by retriever and question by the user\n)\nmodel = ChatOpenAI(\n    model=\"gpt-4\",  # or \"gpt-3.5-turbo\" based on your needs\n    temperature=0,\n    max_tokens=512\n)\n\n## Function to merge text chunks\ndef format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n\n## RAG chain\nrag_chain = (\n        {\"context\": vectorstore.as_retriever(k=10) | format_docs, \"question\": RunnablePassthrough()}\n        | prompt\n        | model\n        | StrOutputParser()\n)\n\nrag_chain.invoke(\"Welke rangen zijn er bij de brandweer?\")\n```\n\n## Results of the Embedding Models\n\nTo evaluate the performance of the two embedding models, we asked the question: **“Welke rangen zijn er bij de brandweer?”** (translated: “Which ranks exist within the fire department?”). Below are the responses generated by each model:\n\n\n### Cohere Embed English v3\n\n**Response**:*“De tekst geeft geen specifieke informatie over de verschillende rangen binnen de brandweer. Echter, in veel brandweerorganisaties zijn er verschillende rangen zoals brandweerman, luitenant, kapitein, bataljonschef, assistent\\-chef en brandweerchef. De specifieke rangen kunnen variëren afhankelijk van het land en de specifieke organisatie.”*\n\n**Analysis**:The response is generic and doesn’t rely on the provided context from the Dutch document. This is expected, as the English embedding model struggles to connect the semantic similarity between “hiërarchie” (used in the document) and “rangen” (used in the query). Consequently, it defaults to general knowledge about fire department ranks.\n\n\n### Cohere Embed Multilingual v3\n\n**Response**:*“Binnen de brandweer zijn er verschillende rangen. Deze zijn onderverdeeld in het basiskader en het middenkader. In het basiskader hebben we de B01 brandweerman en de B02 korporaal. In het middenkader, dat bestaat uit onderofficieren, hebben we de M01 sergeant en de M02 adjudant. Deze rangen worden aangeduid op het diensttenue, het uitgaanstenue en op de interventiekledij.”*\n\n**Analysis**:This response is highly relevant and accurately retrieves information from the document. The multilingual embedding model successfully identifies the semantic relationship between “hiërarchie” (context) and “rangen” (query). It provides a detailed answer directly based on the content of the document, demonstrating its ability to handle Dutch\\-specific semantics effectively.\n\n\n### Key Takeaways\n\n* **Cohere Embed English v3**: The English model failed to retrieve relevant context from the Dutch document due to a lack of semantic alignment between the query and the document’s terminology. This highlights the limitations of using English\\-specific embeddings for non\\-English tasks.\n* **Cohere Embed Multilingual v3**: The multilingual model excelled in this test, retrieving and leveraging contextually relevant information from the Dutch document. This demonstrates the importance of multilingual embeddings for achieving accurate retrieval and answering non\\-English queries effectively.\n\n\n## Conclusion\n\nThis evaluation highlights a critical insight for anyone building Retrieval\\-Augmented Generation (RAG) systems for non\\-English languages: **embeddings matter**, especially when the task demands nuanced understanding across languages. The stark contrast in performance between the Cohere Embed English v3 and Cohere Embed Multilingual v3 models illustrates the limitations of English\\-specific embeddings in non\\-English contexts and the immense value of multilingual models.\n\nWhen tasked with answering a query in Dutch, the multilingual model excelled, retrieving accurate and contextually rich information directly from the document. Meanwhile, the English embedding model defaulted to generic, unrelated knowledge, demonstrating its inability to bridge the semantic gap between the query and the document’s content.\n\nFor organizations developing AI systems in a global, multilingual landscape, this test reinforces the importance of choosing the right embedding models for the task at hand. Multilingual embeddings are not just a “nice\\-to\\-have” feature; they are essential for ensuring accuracy, relevance, and user trust in non\\-English applications.\n\nAs generative AI continues to expand its reach, embracing language diversity through better embeddings will be key to delivering meaningful and impactful solutions worldwide. By prioritizing multilingual capabilities, businesses can create systems that are not only smarter but also more inclusive — empowering users across languages and cultures.\n\n***Follow me for more AI deep dives!***\n\n[Medium](https://proxy.rifx.online/https://medium.com/@lorevanoudenhove), [Instagram](https://proxy.rifx.online/https://www.instagram.com/lorevanoudenhove.ai/), [YouTube](https://proxy.rifx.online/https://www.youtube.com/channel/UCVyOJS1VV7FxPsStK65pHcA), [Pairrot](https://proxy.rifx.online/https://www.pairrot.eu/)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/will-bolt-new-ai-will-replace-v0-dev-129a3366eb44","frontmatter":{"title":"Will Bolt.new AI will replace v0.dev","meta_title":"Will Bolt.new AI will replace v0.dev","description":"Will Bolt.new AI will replace v0.dev","date":"2024-11-13T01:22:35.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*g5S8PyYqR87bdyGhb77rRw.png","categories":["Programming","Technology/Web","Data Science"],"author":"Rifx.Online","tags":["Bolt.new","v0.dev","web","components","layouts"],"draft":false,"slug":"blog/will-bolt-new-ai-will-replace-v0-dev-129a3366eb44"},"content":"\n### AI Tools\n\n> **Not a Member? Read for FREE [here](https://proxy.rifx.online/https://tarzzotech.medium.com/129a3366eb44?source=friends_link&sk=385b6b2e482ae9d16ef8f99fe083b8ae).**\n\n\n\nThe world of web development is witnessing rapid growth with multiple AI tools in the market. These new AI tools help developers generate web components and complex code structures through natural language prompts. These tools provide better code quality and reduce time in writing manual code.\n\nRecently there was a new tool **bolt.new** came into the market which looks similar to v0\\.dev. With the arrival of Bolt.new, a question arises: ***Will this new AI platform replace v0\\.dev, or do these tools serve different purposes altogether?***\n\nIn this post, I will share my experience of using the tool for some time. I mainly talk about the key differences between **v0\\.dev** and **Bolt.new**, comparing their strengths, use cases, and outputs. By examining real\\-world examples, we’ll determine whether **Bolt.new** poses a legitimate threat to **v0\\.dev**, or if both tools have their place in a developer’s toolkit.\n\nIn my previous blog, I explained all the details and my thoughts about v0\\.dev, so check it [**here**](https://proxy.rifx.online/https://tarzzotech.medium.com/4191292876b3?source=friends_link&sk=9730b35a75771953d0541e459c8adeaa). I don't want to share the repeated content here as well. Let's see here about **bolt.new** and comparison with **v0\\.dev.**\n\n## Overview of Bolt.new\n\n**Bolt.new** has made waves as a newer AI\\-powered platform with ambitions to go beyond just front\\-end components. While **v0\\.dev** is focused on generating smaller components, Bolt.new aims to deliver more complex solutions, such as full\\-page layouts or even multi\\-step workflows that span both the front\\-end and back\\-end.\n\n**Key Features of Bolt.new:**\n\n* **Full Layouts:** **Bolt.new** can generate full\\-page layouts, including headers, footers, sidebars, and main content areas.\n* **Versatile Code Generation:** Beyond web components, **Bolt.new** can produce server\\-side scripts, database configurations, and other elements essential for building full\\-stack applications.\n* **Enhanced Customization:** While V0\\.dev focuses on individual components, **Bolt.new** offers flexibility across larger scopes, allowing developers to generate fully functional layouts or structures.\n\n## Comparing V0\\.dev and Bolt.new\n\nWhile both tools offer impressive features, they cater to different aspects of the development process.\n\n* **v0\\.dev** focuses on individual web components that can be easily customized and integrated into front\\-end codebases. It’s ideal for developers needing quick solutions for buttons, cards, or forms that work across frameworks like React or Vue.\n* **Bolt.new**, in contrast, takes a more comprehensive approach, allowing developers to generate full\\-page layouts or even back\\-end configurations. This makes it a more versatile option for projects that require multiple types of code beyond just front\\-end components.\n\n## Example Comparison:\n\nHere we start to compare these tools from small components to full pages.\n\n### Creating a Button Component\n\n**Prompt:** “Generate a responsive navigation bar with dropdown menus and a search bar.”\n\n**v0\\.dev**\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8-1NaJb_msK1OLv7MHbOjw.gif)\n\n**bolt.new**\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*WsRSUU5brIql4uBb7wFkAg.gif)\n\n**v0\\.dev** created a button element with basic styling and a hover effect that changes the background color and text style when hovered.\n\n**Bolt.new** created a button with a hover effect and JavaScript code that triggers an alert box when clicked, including inline JavaScript and basic CSS styling.\n\n### Creating a Navigation Bar\n\n**Prompt:** “Generate a responsive navigation bar with dropdown menus and a search bar.”\n\n**v0\\.dev**\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*sOJ0EveSOVKtJJltLGiVCA.gif)\n\n**bolt.new**\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*aGCWfH5ULTTFb-FS-kT-7w.gif)\n\nBoth tools have generated the Navbar a little similarly. But **bolt.new** has generated a little better than **v0\\.dev**.\n\n### Creating a Website.\n\n**Prompt:** “Generate a full page layout with a sticky header, footer, collapsible sidebar, and main content area designed for blog posts.”\n\n**v0\\.dev**\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kwEXDG3tb1CiHetZr5W03Q.png)\n\n**bolt.new**\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*HxKnFJQf--e-E1fh8yyxVw.png)\n\nThe output from the **bolt.new** is speechless.\n\n## Finally, Will Bolt.new Replace v0\\.dev?\n\nGiven the broader scope of Bolt.new, it might seem like it could overshadow v0\\.dev. However, the two platforms are likely to coexist, each offering value for different stages of the development process.\n\n**v0\\.dev** is unmatched in its ability to generate specific, high\\-quality web components quickly. It’s perfect for front\\-end developers who need reusable, responsive components that are ready to integrate into their projects with minimal fuss.\n\n**Bolt.new**, while offering more flexibility, is not necessarily a replacement for v0\\.dev. Its broader range of functionality appeals to developers who need full layouts or code structures that span both the front end and back end.\n\n## Conclusion\n\nBoth v0\\.dev and Bolt.new are powerful AI tools, each with unique strengths. While Bolt.new’s versatility makes it a strong contender for developers looking to generate more complex code, v0\\.dev remains a go\\-to tool for developers who want quick, customizable components.\n\nUltimately, the question of whether Bolt.new will replace v0\\.dev? It’s unlikely. These platforms cater to distinct needs. v0\\.dev shines in generating specific, reusable components with ease. Bolt.new offers broader functionality for complex layouts. So, which tool do you think will dominate in the future of AI AI\\-assisted development, or will they continue to complement each other? The answer depends on the developer's need on which you are working.\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/a-month-with-cursor-and-claude-dev-my-thoughts-5c41ae0d4467","frontmatter":{"title":"与 Cursor 和 Claude-Dev 共度的一个月：我的想法","meta_title":"与 Cursor 和 Claude-Dev 共度的一个月：我的想法","description":"我最近一直在使用两个新工具 - Cursor 和 Claude-Dev - 这两个工具都引起了开发人员的广泛关注……","date":"2024-11-04T12:32:52.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*i28vK12LJ6XTpSwrKiamwA.png","categories":["Programming","Technology","Generative AI"],"author":"Rifx.Online","tags":["Cursor","Claude-Dev","Cline","autocomplete","debugging"],"draft":false,"slug":"blog/a-month-with-cursor-and-claude-dev-my-thoughts-5c41ae0d4467"},"content":"\n\n\n我最近在使用两个新工具—— **Cursor** 和 **Claude\\-Dev** ——这两个工具在开发者社区中都引起了相当多的关注。它们都是通过 AI 驱动的助手来提高编码的速度和直观性，但它们采取了不同的方法，并各自有自己的优缺点。在使用这两者大约一个月后，我觉得是时候坐下来反思一下它们的优势和仍需改进的地方。\n\n让我们从 Cursor 开始。\n\n## Cursor: 熟悉但更快速\n\nCursor 是 VSCode 的一个分支，如果你和我一样已经是 VSCode 用户，那么使用起来就非常简单。我不需要从头开始重建我的环境，也不需要处理设置键绑定。所有在 VSCode 中有效的功能在 Cursor 中开箱即用——我的扩展、设置和键映射都毫无问题地转移过来。过渡几乎是无缝的，唯一的一个关键区别是：AI 自动补全速度更快。实际上，**根据我的经验**，它的速度大约是 GitHub Copilot 的 10 倍。\n\n现在，“10 倍更快”并不是我从基准测试中得出的数字——这只是我使用一段时间后的感觉。当你在编写代码时，Cursor 预测你的下一步动作时，并不会让你感觉 AI 在滞后或追赶。相反，它与你同步，这有助于保持你的工作状态。我惊讶于当我不再等待 Copilot 赶上或按三次 tab 仅仅为了得到我想要的建议时，我的生产力提升了多少。\n\nCursor 还有一个很好的功能，它嵌入并索引你的整个项目，使理解文件之间的关系变得更加容易。当你更新一个文件时，索引也会随之更新，这意味着 AI 对你的代码库各个部分如何结合有了更好的理解。如果你在一个大型代码库中工作，多个文件相互依赖，这一点非常有用。\n\n## 缺点\n\n话虽如此，Cursor 中一些最佳功能是需要订阅才能使用的。我通常不反对为增加真实价值的工具付费，但在这种情况下，我有点失望，因为最有趣的 AI 功能——如多文件编辑——属于高级版本。对于一个仍然相对较新的工具，我想知道过早限制这些功能是否会限制其采用，尤其考虑到已经有很多开发者在为 GitHub Copilot 付费。\n\n我在使用 Cursor 时遇到的另一个问题是，虽然它在快速、小型任务方面表现出色，但在处理更复杂的问题时缺乏我所需的一些灵活性。它非常适合快速代码建议和重构，但当我需要能够处理更复杂任务的工具时，比如读取日志或执行构建命令，我发现自己在寻找其他工具。\n\n## Claude\\-Dev: 开源的黑马\n\n这就是**Claude\\-Dev（现在称为Cline）**的用武之地。Claude\\-Dev是一个针对VSCode的开源扩展，虽然它的打磨程度不如Cursor，但它正在迅速发展——在某些方面，它更为强大。Claude\\-Dev最引人注目的地方在于，它似乎不仅仅是提供代码片段的建议。它是一个可以**与您的环境进行更深层次互动**的工具。\n\n例如，Claude\\-Dev可以读取您的终端日志，理解代码检查错误，甚至运行任意的CLI命令。这意味着如果您问它为什么您的项目无法构建，它不仅会提供建议——它实际上会去查看相关文件，弄清楚您正在使用什么类型的项目（Node、React、Python等），并尝试为您构建它。如果出现错误，它会读取日志，尝试诊断问题，甚至在需要时应用修复。\n\n不过，它并不完美。根据我的经验，Claude\\-Dev在编辑时的速度不如Cursor。这其中一个原因是它重写整个文件，而不是仅仅更新需要更改的部分。这会导致速度变慢，如果您为API令牌付费（您需要提供要使用的LLM的API密钥），它消耗这些令牌的速度比应有的要快。我一直在考虑为这个项目贡献代码，修复这个问题，使其通过诸如`sed`的Shell命令仅更新必要的行。\n\n我发现的一个特别有趣的功能是Claude\\-Dev如何使用Puppeteer来可视化测试和更新您的前端。您可以给它一张网站的截图，它会将其与您的应用进行比较，迭代直到您的前端达到您想要的外观。这个过程并不是最快的，但它在处理CSS方面出奇地优秀——对我而言，这通常是一个耗时的环节。\n\n## 它的不足之处\n\nClaude\\-Dev 无疑是一个适合那些乐于尝试仍然有些粗糙工具的人的工具。与感觉更像是已经准备好投入使用的精致产品的 Cursor 不同，Claude\\-Dev 更像是一个正在积极开发的强大工具。它并不总是能在第一次就做到正确，而且速度也比我希望的要慢，但它在不断改进。它是开源的，主要由一个人开发，这使得它的创新速度更加令人印象深刻。\n\n## 那么你应该使用哪个？\n\n如果你正在寻找一个精致、快速的体验，专注于速度和快速建议，**Cursor** 可能是更好的选择。它感觉灵敏，能够与现有的 VSCode 设置集成，并且可以保持你的工作流——直到你遇到付费墙。但是如果你对此没有问题，并且不需要额外的功能，Cursor 是一个很好的工具。\n\n另一方面，如果你想要的不仅仅是自动补全代码的功能——而是能真正帮助调试、构建和迭代你的项目的东西，**Claude-Dev** 更适合你。它更灵活，但也稍微慢一些，边缘处理得不够精细。如果你愿意尝试并且能够忍受一些小缺陷，它提供的功能是 Cursor 目前所不具备的。\n\n对我来说，**Claude-Dev** 胜出，主要是因为它与我的工作流程的深度集成。能够读取日志、运行命令并迭代直到问题解决是无价的，尤其是在我处理不熟悉的代码库时。也就是说，当我需要快速行动而不想等待 AI 处理命令时，我仍然会使用 **Cursor**。\n\n## 最后的思考\n\nCursor 和 Claude-Dev 都提供了独特的优势，我认为我们只是在探索 AI 驱动的编码工具所能做的事情的表面。这里有很大的潜力，尤其是随着这些工具的不断发展。我很期待看到它们的未来，并将继续尝试这两者，看看它们如何融入我的开发工作流程。\n\n同时，我建议你自己尝试一下这两者。每个工具都有其优点，你可能会发现其中一个更适合你的风格，具体取决于你正在处理的内容。\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/a-new-risings-red-star-qwen2-5-is-here-0dffe0fb09ad","frontmatter":{"title":"新崛起红星：Qwen2.5来了","meta_title":"新崛起红星：Qwen2.5来了","description":"一起用python和llama-cpp测试一下阿里云新生的生成式AI Qwen2.5","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*zU-XtqK2oMLkvscgxavjdw.png","categories":["Programming","Technology","Education"],"author":"Rifx.Online","tags":["Qwen2.5","multimodal","instruction-following","text-generation","multilingual"],"draft":false,"slug":"blog/a-new-risings-red-star-qwen2-5-is-here-0dffe0fb09ad"},"content":"\n\n\n### 一起测试新生的阿里云生成式AI Qwen2.5，使用Python和llama-cpp\n\n\n\n在没有太多宣传和预期公告的情况下，阿里云于9月19日发布了他们的旗舰模型系列Qwen2.5。\n\n阿里云在Qwen上的革命性旅程再次展示了通过创新的强大领导力。\n\n怎么做的？它们有什么特别之处？我们应该期待什么？\n\n在本文中，我们将探讨新模型并检查其性能。作为后续，在下一篇文章中，我们将使用`llama-cpp-python`和量化版本的qwen2.5–1.5b-instruct，对模型进行13项NLP任务的测试。\n\n事实上，我相信我们是最佳的基准工具，能够全面评估一个模型是否适合我们！\n\n现在，我们将覆盖以下内容：\n\n\n```python\n- Qwen2.5 family innovation\n- Declared scope, use cases and models\n- Qwen2.5: a party of Foundation models\n- Expanding Reach through Open-Source Contributions\n- Bridging Industries through cutting-edge AI solutions\n- 13 Tasks to prove it worth \n- Future outlook: continued Open-Sourcing\n```\n让我们深入了解！ \n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*OeQ5qeOzCdl8LPJOZZgTIw.png)\n\n## Qwen2.5家族创新\n\nQwen是阿里巴巴集团Qwen团队的大型语言模型和大型多模态模型系列。就在昨天，大型语言模型已升级为Qwen2.5。\n\n语言模型和多模态模型均在大规模多语言和多模态数据上进行预训练，并在高质量数据上进行后训练，以与人类偏好对齐。Qwen能够进行自然语言理解、文本生成、视觉理解、音频理解、工具使用、角色扮演、作为AI代理等。\n\n随着Qwen2.5的发布以及额外开源模型的发布，阿里云继续保持其领导地位，以满足企业用户日益增长的AI需求。自去年六月以来，Qwen家族通过Model Studio在消费电子、汽车、游戏等多个行业吸引了超过90,000个部署。\n\nQwen还通过在Hugging Face等平台上推出新模型，如Qwen1.5–110B和CodeQwen1.5–7B，扩大了其影响力，展示了阿里巴巴对开源AI开发的承诺。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*A4pEOgsLK2PAFtiaGQx1Qw.png)\n\n## 声明的范围、用例和模型\n\n在 Qwen2 发布的过去三个月里，众多开发者在 Qwen2 语言模型基础上构建了新的模型，为整个社区以及阿里云提供了宝贵的反馈。\n\n> 在此期间，我们专注于创建更智能、更知识丰富的语言模型。今天，我们很高兴地介绍 Qwen 家族的最新成员：Qwen2.5。\n\n他们的声明伴随着有关新模型家族的事实：\n\n* 密集型、易于使用的仅解码器语言模型，提供 0.5B、1.5B、3B、7B、14B、32B 和 72B 尺寸，以及基础和指令变体。\n* 在我们最新的大规模数据集上进行预训练，涵盖多达 18T 的标记。\n* 在遵循指令、生成长文本（超过 8K 标记）、理解结构化数据（例如，表格）以及生成结构化输出（尤其是 JSON）方面有显著改进。\n* 对系统提示的多样性更具韧性，增强了角色扮演实现和聊天机器人的条件设置。\n* 支持的上下文长度可达 128K 标记，并且可以生成多达 8K 标记。\n* 支持超过 29 种语言的多语言功能，包括中文、英文、法文、西班牙文、葡萄牙文、德文、意大利文、俄文、日文、韩文、越南文、泰文、阿拉伯文等。\n\n## Qwen2.5：基础模型的聚会\n\n根据2024年9月19日的[官方博客新闻稿](https://qwenlm.github.io/blog/qwen2.5/)的公告：\n\n> 今天，我们很高兴地介绍Qwen家族的最新成员：**Qwen2.5**。我们宣布这可能是历史上最大的开源发布！让我们开始庆祝吧！\n\n> 我们最新的发布包含了LLMs **Qwen2.5**，以及用于编码的专用模型**Qwen2.5-Coder**和用于数学的模型**Qwen2.5-Math**。\n\n为了展示Qwen2.5的能力，阿里云团队对其最大的开源模型**Qwen2.5–72B**——一个72B参数的稠密解码器语言模型——与领先的开源模型如Llama-3.1–70B和Mistral-Large-V2进行了基准测试。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*-MMFgkkWHa307jNo.jpg)\n\n所有开放权重模型都是稠密的解码器语言模型，提供多种尺寸，包括：\n\n* Qwen2.5：0.5B、1.5B、3B、7B、14B、32B和72B\n* Qwen2.5-Coder：1.5B、7B和32B正在发布中\n* Qwen2.5-Math：1.5B、7B和72B。\n\n除了3B和72B变体外，所有这些开源模型均在Apache 2.0许可证下发布。您可以在各自的Hugging Face库中找到许可证文件。\n\n> 除了这些模型外，我们还通过Model Studio提供旗舰语言模型的API：**Qwen-Plus**和**Qwen-Turbo**，我们鼓励您进行探索！\n\n但这还不是全部！\n\n> …我们还开源了**Qwen2-VL-72B**，与上个月的发布相比，具有性能提升。\n\n在**Qwen2.5**方面，所有语言模型均在我们最新的大规模数据集上进行了预训练，涵盖了多达**18万亿**个标记。与Qwen2相比，Qwen2.5获得了显著更多的知识（MMLU：85+），并在编码（HumanEval 85+）和数学（MATH 80+）方面大大提升了能力。此外，新模型在指令跟随、生成长文本（超过8K个标记）、理解结构化数据（例如，表格）和生成结构化输出，尤其是JSON方面取得了显著改善。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*7c7CIbl-WVjazUeE.jpeg)\n\nQwen2.5模型通常对系统提示的多样性更具韧性，增强了角色扮演的实施和聊天机器人的条件设置。\n\n与Qwen2一样，Qwen2.5语言模型支持多达**128K**个标记，并可以生成多达**8K**个标记。它们还支持超过**29**种语言的多语言支持，包括中文、英文、法文、西班牙文、葡萄牙文、德文、意大利文、俄文、日文、韩文、越南文、泰文、阿拉伯文等。\n\n### Qwen-Coder 是家族中的新成员\n\n专业的专家语言模型，即 **Qwen2.5-Coder** 用于编码，**Qwen2.5-Math** 用于数学，相较于它们的前身 CodeQwen1.5 和 Qwen2-Math，进行了实质性的增强。具体来说，Qwen2.5-Coder 已在 **5.5 万亿** 个与代码相关的数据上进行了训练，使得即使是较小的编码专用模型也能够在编码评估基准上与更大的语言模型提供竞争力的表现。同时，Qwen2.5-Math 支持 **中文** 和 **英文**，并结合了多种推理方法，包括思维链（CoT）、思维程序（PoT）和工具集成推理（TIR）。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Nvk4wrcB0SB4Tt-xbCzO6g.png)\n\n## 通过开源贡献扩大影响力\n\n作为持续致力于更广泛社区的一部分，阿里云在发布各种规模和变体的Qwen模型方面迈出了进一步的步伐。这包括：\n\n1. **Qwen 0.5亿参数**，适用于更传统应用的基础版本。2. 一款紧凑但强大的模型，专门为游戏开发量身定制：**Qwen-VL（视觉-语言）**，优化了高性能。\n\n这些进展展示了阿里对开源AI的承诺，不仅分享了Qwen的基础版本，还推出了显著改进和新模型，直接针对企业需求，同时增强其快速创新的能力。\n\n这与一个战略愿景密切相关，即持续贡献惠及社区成员和自身客户，帮助他们在多个行业寻求创新应用。\n\n### 通过前沿的人工智能解决方案连接各行业\n\n为了展示Qwen在现实场景中的广泛能力，阿里云一直处于前沿：\n\n1. **小米**：该公司正在将阿里的模型集成到他们的AI助手小爱中，并在小米智能手机和电动汽车中部署，以通过语音命令生成车载娱乐图像等增强功能。\n\n2. **完美世界游戏**：Qwen在游戏开发中的集成导致了创新应用，包括通过对话动态改善情节解析和实时内容管理。\n\n阿里云模型与各行业之间的合作不仅丰富了用户体验，还促进了这些行业内更大的增长机会，推动了没有人工智能进步的情况下无法想象的边界。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*ku8o3rq6PHDE8xcc.png)\n\n## 13 个证明其价值的任务\n\n1.5 亿参数的模型可能是考虑到复杂性、提示理解和推理速度的最佳变体。\n\n我将向您展示我仅使用 `llama-cpp-python` 和一个简单终端界面进行的内部测试。\n\n为此，我创建了一个提示列表，涵盖了一系列通常使用的任务，您可以在每次生成后分配一个投票（从 0 到 5）。这是一个个人的人工基准测试。\n\n### 需求\n\n创建一个 `venv`（需要 Python 3.11+）：我在运行 Windows 11 的迷你电脑上进行了测试。\n\n```python\n## create the virtual environment\npython -m venv venv\n## activate the venv\nvenv\\Scripts\\activate\n## Install the dependencies \npip install llama-cpp-python==0.2.90 tiktoken\n```\n我们需要从[官方 Qwen2.5 Hugging Face 仓库](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF)下载 GGUF 文件。我使用的是 qwen2.5–1.5b-instruct-q5\\_k\\_m.gguf 版本。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Fa-qFsx9RTFGZmM-vxCEPQ.png)\n\n在主项目目录中下载文件。我们就准备好了。\n\n这里用于分析的代码在我的 GitHub 仓库中：\n\n我将在下一篇文章中解释整个代码和结果。保持关注！\n\n## 未来展望：持续的开源\n\n在未来的计划中，阿里巴巴还表达了他们对持续开源贡献的承诺，通过为不同领域的开发者发布更小的 Qwen 变体。实际上，在 Hugging Face 社区中，许多用户已开始针对特定任务对 Qwen 进行微调：我在我的 NuExtract 文章中写了一个例子：这个模型系列的较小变体基于 Qwen2–0.5b！\n\n这些人工智能技术和模型进展的发展是充分利用大型语言模型如 **Qwen** 在各个行业中潜力的关键步骤。随着 Model Studio 中强劲的采用率持续快速增长，显然阿里云不仅通过提供先进的工具而且通过促进企业之间的创新，成为了行业的先锋领导者。\n\n在我这边，我的展望是继续对新模型进行内部测试，特别是对小型模型，最高到 3B。\n\n在下一篇文章中，我将与您分享我的方法，如何运行模型以及用于十三个 NLP 任务的提示模板。\n\n希望您喜欢这篇文章。如果这个故事对您有价值，并且您想稍微表示支持，您可以：\n\n1. 为这个故事多次点赞\n2. 突出更值得记住的部分（这将使您更容易找到它们，也让我写出更好的文章）\n3. **加入我的[完全免费的每周 Substack 通讯](https://thepoorgpuguy.substack.com/about)**\n4. 注册 Medium 会员（$5/月可阅读无限 Medium 故事）\n5. 在 Medium 上关注我\n6. 阅读我的最新文章 <https://medium.com/@fabio.matricardi>\n\n这里还有几篇文章来满足您的好奇心：\n\n本文中引用的资源：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Du7V61mEX_yIrfmF.png)\n\n此故事发布在 [Generative AI](https://generativeai.pub/)。与我们在 [LinkedIn](https://www.linkedin.com/company/generative-ai-publication) 上联系，并关注 [Zeniteq](https://www.zeniteq.com/)，以便及时了解最新的人工智能故事。\n\n订阅我们的 [通讯](https://www.generativeaipub.com/) 和 [YouTube](https://www.youtube.com/@generativeaipub) 频道，及时获取生成式 AI 的最新消息和更新。让我们共同塑造 AI 的未来！\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*pvLAT3it1FkdhVU0.png)\n\n"},{"lang":"zh","group":"blog","slug":"blog/a-practical-guide-for-using-autogen-in-software-applications-8799185d27ee","frontmatter":{"title":"在软件应用程序中使用 AutoGen 的实用指南","meta_title":"在软件应用程序中使用 AutoGen 的实用指南","description":"更新：虽然这篇文章是 4 个月前写的，但 AutoGen 已经发生了很大变化。对于可能存在的一些问题，我深表歉意……","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*yrraWH6aGNnbx8p-wfQ1OQ.jpeg","categories":["Programming","Chatbots","Autonomous Systems"],"author":"Rifx.Online","tags":["AutoGen","multi-agent","LLMs","customization","collaboration"],"draft":false,"slug":"blog/a-practical-guide-for-using-autogen-in-software-applications-8799185d27ee"},"content":"\n\n\n\n\n*更新：虽然这篇文章是在四个月前写的，但 AutoGen 自那时以来变化很大。对于我代码示例中可能过时的内容，我深感歉意。*\n\n如果您想了解 AutoGen，可以查看 [文档](https://microsoft.github.io/autogen/)、[Colab 笔记本](https://microsoft.github.io/autogen/docs/Examples) 和 [博客](https://microsoft.github.io/autogen/blog)。非常感谢 AutoGen 团队制作了一个令人惊叹的产品，但老实说——在阅读了他们的所有内容后，我仍然不知道如何在终端或 Jupyter Notebook 之外使用 AutoGen。\n\n本文试图通过提供一些有用的方法来帮助填补这个空白，使 AutoGen 在软件应用中发挥作用。以下是我将要讨论的主题：\n\n1. 代理不仅限于通过终端进行通信\n2. 注册自定义回复\n3. 如何以真实的方式将真实人类纳入对话\n4. 您可以（并且应该）自定义谁来发言\n5. 您不必使用 OpenAI\n6. 可以使用函数而不是执行代码\n7. 将代理用于组织，而不仅仅是对话\n\n最后，我将讨论为什么我认为您应该首先使用 AutoGen。让我们开始吧！\n\n## 代理不仅限于通过终端进行通信\n\n您会看到每个人都使用终端或 Jupyter Notebook 演示 AutoGen。这对于演示来说不错，但这些代理之间还有其他交流方式。\n\nAutoGen 有 2 个基本类：[`UserProxyAg`ent](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/user_proxy_agent.py) 和 [`AssistantAg`ent](https://github.com/microsoft/autogen/blob/main/autogen/agentchat/assistant_agent.py)。它们继承了 [`ConversableAg`ent](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py) 类，仅为基类提供了几个不同的默认参数。\n\n当您看到这个经典代码示例时：\n\n```python\nassistant = autogen.AssistantAgent(\n    name=\"assistant\",\n    llm_config=llm_config\n)\nuser_proxy = autogen.UserProxyAgent(name=\"user_proxy\")\nawait user_proxy.a_initiate_chat(\n    assistant,\n    message=\"\"\"What date is today? Compare the year-to-date gain for META and TESLA.\"\"\",\n)\n```\n发生的事情是 `UserProxyAgent` 将调用其自己的 `send` 方法，这将调用 `AssistantAgent` 的 [`rece`ive](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L514) 方法，传递原始消息。将生成回复（稍后会详细说明），然后 `AssistantAgent` 将调用其 [`s`end](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L351) 方法，这将调用 `UserProxyAgent` 的 `receive` 方法，依此类推，直到 `UserProxyAgent` 确定对话已终止（可以通过 `is_termination_msg` 参数自定义）。\n\n我第一次“恍然大悟”的时刻是当我意识到这些代理是类时，我可以创建自己的自定义代理类，继承 AutoGen 的 UserProxy/Assistant/Conversable Agent 类，并重写任何默认方法。这使得 AutoGen 非常可扩展。\n\n我有一个用例，需要一个可以通过网站上的聊天 UI 输入消息的人（由 `UserProxyAgent` 代理），我希望 `AssistantAgent` 能在 UI 中回复该聊天，并能够接收更多来自人类用户的消息，就好像人类只是这个 AutoGen 对话中的另一个代理。\n\n我可以重写 `send` 和 `receive` 方法（或 `a_send` 和 `a_receive`），并通过 http、websockets 等进行推送/拉取。我尝试了这个，它开始工作，但无法扩展。让我们学习一种更好的方法。\n\n## 注册自定义回复\n\nAutoGen 具有一个插件系统，可以让您自定义代理生成回复的方式。我们习惯看到的示例是 AutoGen 查询 OpenAI 获取答案，并将其作为回复，但您也可以插入自己的方法：\n\n```python\nclass WeatherAgent(AssistantAgent):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, llm_config=False, **kwargs)\n        self.register_reply(Agent, WeatherAgent.get_weather)\n\n    async def get_weather(\n        self,\n        messages: List[Dict] = [],\n        sender=None,\n        config=None,\n    ) -> Tuple[bool, Union[str, Dict, None]]:\n        last_message = messages[-1][\"content\"]\n        result = await fetch_weather(last_message)\n        return True, result\n\nasync def fetch_weather(city: str) -> str:\n    async with httpx.AsyncClient() as client:\n        result = await client.post(\n            WEATHER_API_URL,\n            json={\"city\": question},\n        )\n        return result.json()\n\nweather_assistant = WeatherAgent(name=\"weather_assistant\")\nuser_proxy = autogen.UserProxyAgent(name=\"user_proxy\")\nawait user_proxy.a_initiate_chat(assistant, message=\"Lehi\")\nprint(weather_assistant.last_message)\n```\n在这里，`register_reply` 将插入我的自定义方法以获取回复，默认情况下，该方法将放在 `position=0`，这意味着它将是尝试的第一个回复方法。该方法应返回一个元组，其中第一个项目是一个布尔值，指示此回复是否为应使用的回复，或者是否应尝试下一个注册的回复（例如，使用 OpenAI 的内置回复生成 — 完整顺序请参见 [此处](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L145-L153)）。\n\n了解 [`register_reply`](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L155) 使您能够自定义回复的检索方式，允许您启动子多代理对话等。\n\n## 如何以真实的方式将真实人类纳入对话\n\n这里有一种方法：\n\n```python\n## user makes a POST /query { \"message\": \"What's the weather?\" }\n\n@query_blueprint.route(\"/query\", methods=[\"POST\"])\nasync def post_query():\n  message = request.form.get(\"message\")\n\n  assistant = autogen.AssistantAgent(\n    name=\"assistant\",\n    llm_config=llm_config\n    system_message=\"\"\"You're a helpful assistant.\n    If you need more info, ask the user for anything missing.\"\"\"\n  )\n  user_proxy = autogen.UserProxyAgent(\n    name=\"user_proxy\",\n    human_input_mode=\"NEVER\",\n    code_execution_config=False,\n    is_termination_msg=lambda message: True # Always True\n  )\n  weather_assistant = WeatherAgent(\n    name=\"weather_assistant\",\n    system_message=\"\"\"You're a helpful assistant to get the weather.\n    You fetch weather information, then return it.\"\"\"\n  )\n\n  groupchat = autogen.GroupChat(\n    agents=[assistant, user_proxy, weather_assistant],\n    messages=[]\n  )\n  manager = autogen.GroupChatManager(\n    name=\"Manager\",\n    groupchat=groupchat,\n    llm_config=llm_config,\n  )\n\n  await user_proxy.a_initiate_chat(manager, message=message)\n\n  return groupchat.messages[-1]\n```\n这里发生了什么？\n\n1. 每当一条消息发送到 `user_proxy` 时，对话将结束（我们稍后会恢复它）。这样做的原因是什么？这意味着 `user_proxy` 实际上可以代理用户。它不会尝试回答，而是会结束当前的对话流程，允许真实的人类用户响应（通过恢复对话 — 见下文）。\n2. 如果助理需要更多信息，它会询问 user_proxy，这将结束当前对话。\n\n在上述代码中，可能会发生以下情况：\n\n1. user_proxy -> manager: “天气怎么样？”\n2. assistant -> manager: “用户没有指定哪个城市。”\n3. manager -> user_proxy : 对话将结束\n\n现在，如果用户想要回应并恢复对话，我们该如何做到呢？有很多方法可以做到这一点，这里只是一个示例：\n\n```python\n## user makes a POST /query { \"message\": \"What's the weather?\" }\n## above posts returns a `history` array\n## user makes a second POST /query { \"message\": \"What's the weather?\", \"history\": history }\n\nclass ResumableGroupChatManager(GroupChatManager):\n    groupchat: GroupChat\n\n    def __init__(self, groupchat, history, **kwargs):\n        self.groupchat = groupchat\n        if history:\n            self.groupchat.messages = history\n\n        super().__init__(groupchat, **kwargs)\n\n        if history:\n            self.restore_from_history(history)\n\n    def restore_from_history(self, history) -> None:\n        for message in history:\n            # broadcast the message to all agents except the speaker.  This idea is the same way GroupChat is implemented in AutoGen for new messages, this method simply allows us to replay old messages first.\n            for agent in self.groupchat.agents:\n                if agent != self:\n                    self.send(message, agent, request_reply=False, silent=True)\n\n@query_blueprint.route(\"/query\", methods=[\"POST\"])\nasync def post_query():\n  message = request.form.get(\"message\")\n\n  assistant = autogen.AssistantAgent(\n    name=\"assistant\",\n    llm_config=llm_config\n    system_message=\"\"\"You're a helpful assistant.\n    If you need more info, ask the user for anything missing.\"\"\"\n  )\n  user_proxy = autogen.UserProxyAgent(\n    name=\"user_proxy\",\n    human_input_mode=\"NEVER\",\n    code_execution_config=False,\n    is_termination_msg=lambda message: True # Always True\n  )\n  weather_assistant = WeatherAgent(\n    name=\"weather_assistant\",\n    system_message=\"\"\"You're a helpful assistant to get the weather.\n    You fetch weather information, then return it.\"\"\"\n  )\n\n  groupchat = autogen.GroupChat(\n    agents=[assistant, user_proxy, weather_assistant],\n    messages=[]\n  )\n  manager = ResumableGroupChatManager(\n    name=\"Manager\",\n    groupchat=groupchat,\n    llm_config=llm_config,\n  )\n\n  await user_proxy.a_initiate_chat(manager, message=message)\n\n  return {\n    \"response\": groupchat.messages[-1],\n    \"history\": groupchat.messages,\n  }\n```\n通过这种方法，您可以将人类纳入对话，就像他们是群聊中的另一个代理一样。每当助理代理需要人类输入时，它们会询问 user_proxy，user_proxy 然后结束当前对话，允许人类用户用更多信息进行响应，然后恢复到之前的对话。\n\n这种方法的好处是：\n\n* 对话可以通过您想要的任何方式包含真实人类输入（例如通过 http 或 websocket）。\n* 在获取人类输入时，对话被暂停。这为其他对话和计算释放了线程。\n* 您可以在会话之间持久化这些对话。\n\n## 你可以（并且应该）自定义谁接下来发言\n\n这是主观的，但我认为你应该始终自定义发言者的选择方式，因为：\n\n1. 你将使用更少的令牌（节省金钱和响应时间）\n2. 你可以将决定谁发言的逻辑与定义每个代理系统指令的逻辑分开\n\n\n```python\nshort_role_descriptions = {\n  \"user_proxy\": \"A proxy for the user\",\n  \"weather_assistant\": \"You can get the weather\",\n  \"planner\": \"You help coordinate the plan. Your turn happens when XYZ, but skip your turn when ABC\"\n}\n\nclass CustomGroupChat(GroupChat):\n    # The default message uses the full system message, which is a long string.  We are overriding this to use a shorter message.\n    def select_speaker_msg(self, agents: List[Agent]):\n        message = f\"\"\"You are in a role play game. The following roles are available:\n        ---\n        {new_line.join([f\"{agent.name}: {short_role_descriptions[agent.name]}\" for agent in agents])}\n        ---\n\n        The role who plays next depends on the conversation.  User_Proxy will star the conversation, and typically Planner would go next.\n\n        Here are some examples\n        ---\n        ... not shown here ...\n        ---\n\n        Read the following conversation.\n        Then select the next role from {', '.join([agent.name for agent in agents])} to play. Only return the role.\"\"\"\n        return message\n```\n\n## 你不必使用 OpenAI\n\nAutoGen 已经指出，你可以使用其他 LLM，只要它们是“类似 ChatGPT”的，这意味着它们的 API 响应与 ChatGPT API 调用的形状和响应相似。\n\n但是，请记住这些代理是类，并且你可以重写大多数方法？\n\n尝试重写方法: [generate\\_oai\\_reply](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L678)，你可以查询任何你想要的 LLM。\n\n## 函数可以用来代替执行代码\n\n当我去找我们的安全团队并说：“我想在Kubernetes中为我的服务使用AutoGen。它需要能够执行任何由LLM生成的任意代码。你们对此没问题吧？”\n\n当然，答案是明确的：不可以。\n\n那么，为什么在没有自动代码执行能力的情况下使用AutoGen？\n\n除了下面提到的原因之外，还有一个原因是你可以使用函数调用来完全控制代码执行。如果你有一组想要提供给AutoGen的python函数——这些函数是你编写的、你控制的，并且可以接受一些安全参数——这听起来总比在你的私有基础设施中允许任何代码被执行要好得多。\n\n## 使用代理进行组织，而不仅仅是进行对话\n\n也许你并不需要一个自主的多代理对话。也许你只需要对LLM进行几次不同的调用。\n\n我仍然喜欢仅仅出于组织目的而拥有不同“代理”的想法。这是一个非常疯狂的想法，但请根据自己的情况来看待它：\n\n```python\nanalyst = autogen.AssistantAgent(\n    name=\"Analyst\",\n    system_message=\"\"\"Your an analyst.  You do XYZ.\"\"\",\n    llm_config=llm_config,\n)\n\nsummarizer = autogen.AssistantAgent(\n    name=\"Summarizer\",\n    system_message=\"\"\"Your a summarizer.  You do XYZ.\"\"\",\n    llm_config=llm_config,\n)\n\nreport = \"\"\"Some long report\"\"\"\n\nanalysis = analyst.generate_oai_reply(report)[1]\nsummary = summarizer.generate_oai_reply(report)[1]\n\nprint(f\"Analysis: {analysis}\")\nprint(f\"Summary: {summary}\")\n```\n\n## 为什么使用 AutoGen？\n\n1. AutoGen 允许多个代理，具有不同的系统提示和指令，共同解决问题。就像在现实生活中，不同的视角共同合作会比单一思维更好地解决问题。\n2. AutoGen GroupChat 非常出色。它提供了通向正确专家（代理）的路线，并允许对话在问题解决之前自主持续进行。有些对话将从代理 a->b->c->d 进行，而其他的将是 b->a->d->c。这使得 AutoGen 能够在不需要为每种场景制定明确规则的情况下解决各种不同的问题。\n3. AutoGen 能够从错误中恢复。例如，我创建了一个基于 AutoGen 的服务，该服务向一个 API 发出请求。有时，API 请求因为未能正确发送数据而出错。AutoGen GroupChat 不断尝试不同的方法，直到成功。有时，这需要 4 次以上的尝试，但我的 Planner 代理没有放弃——只是自主调整以处理 API 失败并尝试新方法。\n4. AutoGen 从一开始就提出了将 `UserProxyAgent` 与 `AssistantAgent` 分离的概念。这也使我们能够让用户代理真正为用户代理，如上所示。\n5. AutoGen 是一个维护良好的库。每周他们都会添加一些新功能。\n6. AutoGen 非常可扩展。通过他们构建类的方式，您可以根据自己的喜好自定义任何内容。\n7. AutoGen 还有其他我不使用的功能，但其他人可能会觉得它们有帮助，例如帮助您计算对话的令牌和成本、缓存等。\n\n"},{"lang":"zh","group":"blog","slug":"blog/a-robot-artist-just-made-more-money-than-you-have-in-your-entire-creative-career-13dc772ec612","frontmatter":{"title":"机器人艺术家赚的钱比你整个创作生涯赚的钱还多","meta_title":"机器人艺术家赚的钱比你整个创作生涯赚的钱还多","description":"机器人艺术家赚的钱比你整个创作生涯赚的钱还多","date":"2024-11-13T01:22:35.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*XUyq2c7RZCjJD6IQXjmd6A.png","categories":["Robotics","Art","Technology/Web"],"author":"Rifx.Online","tags":["Ai-Da","Turing","Sotheby’s","painting","creativity"],"draft":false,"slug":"blog/a-robot-artist-just-made-more-money-than-you-have-in-your-entire-creative-career-13dc772ec612"},"content":"\n\n\n## 我们已经达到了人工智能创造力和商业的新高度\n\n\n\n首先，我们让计算机屏幕根据人类创作来创作艺术。现在，是一台*真正的机器人*在进行绘画。\n\n没错——一位“超现实主义机器人艺术家”已经被训练成能够在画布上实际绘画。它描绘的已故计算机科学家艾伦·图灵最近在苏富比拍卖会上拍出了**130万美元**的高价。\n\n正如*IFLScience* [报道](https://proxy.rifx.online/https://www.iflscience.com/ai-robot-artist-strikes-gold-by-selling-painting-of-alan-turing-for-13-million-76701?fbclid=IwZXh0bgNhZW0CMTEAAR0KXPj5YDHnWibf6e97UWADZMuhPwGY4f_hnJnWs7rNoHN8KvvHquLAcFc_aem_hyBhYwyjT73j6PdAeXvOng)的那样，这位名为Ai-Da的机器人是以数学家和计算机先驱阿达·洛夫莱斯的名字命名的——在与人类通过语言模型对话后，选择了它的绘画主题。然后，它用机器人手臂勾勒和绘制了几种图灵的版本。\n\n消息来源称，每幅成品系列的油画/丙烯画大约需要机器人“艺术家”完成六到八小时。Ai-Da最初创作了15幅画作，经过人工创作者的筛选，最终产品“通过3D纹理打印机应用于大画布上”。\n\n## 人工艺术，真实财富\n\n被称为 *A.I. God* 的“艺术”在拍卖中获得了大约预期售价的10倍，最终由一位匿名买家竞得。\n\n很可能，$1.3百万超过了你自己艺术作品的收入，假设你不是一位已故的艺术偶像。我们知道，尽管艺术家在世时经济拮据，但人们购买的已故传奇艺术家的作品往往[能卖出数百万](https://proxy.rifx.online/https://www.veranda.com/luxury-lifestyle/artwork/g43012775/most-expensive-paintings-in-the-world/)。\n\n这并不是我们第一次看到数字艺术在拍卖中以荒谬的价格成交。\n\n在2018年，一个法国集体在佳士得拍卖会上为一幅名为 *Edmond de Belamy, from La Famille de Belamy* 的AI肖像艺术作品获得了$432,500。\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*JK-X4et953sOiH1tovj3ag.jpeg)\n\n后来，一位名叫Mike Winkelmann的艺术家，以“Beeple”闻名，在佳士得以$6900万的价格出售了一幅数字NFT（还记得这些吗？），这使他跻身于最有价值的现存艺术家之列。*The Verge* [解释](https://proxy.rifx.online/https://www.theverge.com/2021/3/11/22325054/beeple-christies-nft-sale-cost-everydays-69-million)说，在那次突破性的销售之前，他从一幅印刷品中获得的最高收入为$100。\n\n但这是第一次，一个类人形机器人构思了一个概念，并实际将颜料应用于画布。\n\n当然，Ai-Da并不保留这笔钱——到目前为止，机器人并不需要使用货币。它的人类创造者从销售中获得经济利益。\n\n## 目前仍需要人类输入\n\n艾丹·梅勒（Aidan Meller），一位艺术经销商和画廊主任，是Ai-Da机器人项目的负责人。IFLScience表示，实际的机器人是由英国机器人集体Engineered Arts制造的，该集体还推出了同样令人毛骨悚然的人形机器人[Ameca](https://proxy.rifx.online/https://engineeredarts.co.uk/robot/ameca/)。\n\n请记住，仍然需要人类来提示机器人。梅勒表示，最初与Ai-Da的讨论是关于描绘“为善的人工智能”。还讨论了在风格和质感方面如何进行绘画。\n\n“她的人工智能能力是由牛津大学和伯明翰大学的博士生和教授共同开发的，编程是国际化的，”网站关于Ai-Da指出。它补充说，人类助手帮助准备了打印画布，但机器人在完成产品方面负有主要责任。\n\n这是一个Ai-Da解释艺术“过程”的视频：\n\n\n\n\n\n*卫报* [报道](https://proxy.rifx.online/https://www.theguardian.com/artanddesign/2024/nov/08/alan-turing-portrait-ai-da-robot-painting-sale-price-auction)说，肖像的某种抽象风格可能是故意的：\n\n\n> **梅勒表示，“艺术作品的‘柔和色调和破碎的面部平面’似乎暗示了‘图灵警告我们在管理人工智能时将面临的挑战’。”**\n\n图灵是对的。人工智能在写作和艺术中的出现已席卷全球，仅在几年前就已崭露头角。现在，我们已经达到一个机器人可以为其概念命令超过一百万美元的地步。\n\n## 挑战艺术的定义\n\n但这不仅仅是关于金钱。这种艺术进一步挑战了艺术究竟是什么，以及是否需要拥有人的意识才能产生影响。\n\n这个项目背后的人们认为 Ai\\-Da 本身就是“概念艺术”。虽然这个机器人显然不是人类，但还有其他项目正在进行中，可能很快会创造出看起来真实的艺术家，拥有真正的 [活皮肤](https://proxy.rifx.online/https://readmedium.com/the-new-face-of-artificial-intelligence-9c900d463cf9)。\n\n谁知道呢，你可能很快就在一个生活素描课上与一位同伴艺术家交谈，称赞他们的技艺，而没有意识到你正在与一个机器人对话。\n\n*你对此有何看法？你是感到印象深刻、感到毛骨悚然，还是对你的艺术未来感到担忧？*\n\n"},{"lang":"zh","group":"blog","slug":"blog/ai-image-generator-and-story-generation-app-using-fastapi-groq-and-replicate-706f29dc126f","frontmatter":{"title":"使用 FastAPI、Groq 和 Replicate 的 AI 图像生成器和故事生成应用程序","meta_title":"使用 FastAPI、Groq 和 Replicate 的 AI 图像生成器和故事生成应用程序","description":"项目介绍：AI图像生成器和故事创作者","date":"2024-11-08T00:21:34.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*-fb-azx7fDZ-X9-PbIkiSQ.jpeg","categories":["Programming","Technology/Web","Generative AI"],"author":"Rifx.Online","tags":["FastAPI","Groq","Replicate","transcription","image-generation"],"draft":false,"slug":"blog/ai-image-generator-and-story-generation-app-using-fastapi-groq-and-replicate-706f29dc126f"},"content":"\n\n\n## 项目介绍：AI 图像生成器和故事创作工具\n\nAI 图像生成器和故事创作工具是一个网络应用程序，利用先进的 AI 技术为用户提供一个基于音频提示生成图像和故事的互动平台。该应用程序使用 FastAPI 作为后端，能够高效处理请求和响应，而前端则采用 HTML、CSS（DaisyUI 和 Tailwind CSS）和 JavaScript 构建，以提供响应式用户体验。该应用程序利用 llama\\-3\\.1–70b 进行提示生成，black\\-forest\\-labs/flux\\-1\\.1\\-pro 进行图像生成，以及 llava\\-v1\\.5–7b vbision 模型通过 Groq 和 Replicat.AI 分别进行故事创作。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*0h1GzVVWs_df4OWAC-P59A.jpeg)\n\n## 主要特点：\n\n1. 音频录制和转录：用户可以录制他们的语音提示，然后使用语音识别技术将其转录为文本。\n\n2\\. 图像生成：基于转录的文本，应用程序生成详细的图像提示，并使用 Replicate API 创建相应的图像。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*uiSG8Ir-Wv4a1huYqWhxBg.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*-eRPglLlJwms8N2DCXRyXg.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Mtle1K8AzjMHGxlGicFcGQ.png)\n\n3\\. 图像下载：用户可以将生成的图像下载到本地设备。\n\n4\\. 故事生成：应用程序可以基于创建的图像生成引人入胜的故事，为视觉内容提供叙事背景。\n\n5\\. 用户友好的界面：应用程序具有简洁直观的界面，使用户能够轻松与各种功能进行交互。\n\n## 使用的技术：\n\n* 后端：FastAPI, Groq, Replicate.ai, SpeechRecognition\n* 前端：HTML, CSS (DaisyUI, Tailwind CSS), JavaScript\n* 图像处理：用于图像处理的Pillow\n* 异步操作：用于高效文件处理和网络请求的aiohttp和aiofiles\n\n该项目作为将多个AI服务集成到一个统一应用程序中的示例，使用户能够探索AI生成内容的创意可能性。\n\n## 代码库详细说明：\n\n1. **前端 (HTML/JavaScript)：**\n\n* *该应用程序使用单个 HTML 页面 (index.html)，采用响应式设计，使用 DaisyUI 和 Tailwind CSS。*\n* *页面包含音频录制、转录、提示生成、图像生成和故事生成的部分。*\n* *JavaScript 文件 (script.js) 处理用户交互并与后端 API 通信。*\n\n**2\\. 后端 (FastAPI) :**\n\n* *主要应用程序定义在 app/main.py 中。*\n* *它使用 FastAPI 创建一个具有多个端点的 Web 服务器：*\n\n**— *a. /: 提供主要 HTML 页面。***\n\n***— b. /transcribe:*** *将音频转录为文本。*\n\n***— c. /generate\\_prompt:*** *使用 Groq 的 LLM 从文本生成图像提示。*\n\n***— d. /generate\\_image:*** *使用 Replicate 的 Flux 模型生成图像。*\n\n***— e. /download\\_image:*** *下载并保存生成的图像。*\n\n***— f. /generate\\_story\\_from\\_image:*** *根据图像使用 Groq 的 LLaVA 模型生成故事。*\n\n***— g. /download/{filename}:*** *提供下载的图像文件。*\n\n**3\\. 主要功能：**\n\n* *音频录制和转录*\n* *文本到图像的提示生成*\n* *根据提示生成图像*\n* *根据图像生成故事*\n* *图像下载和保存*\n\n**4\\. 外部 API：**\n\n* [Groq:](https://console.groq.com/docs/models) 用于文本生成（调整后的提示和 [故事](https://console.groq.com/docs/vision)）\n* [Replicate AI:](https://replicate.com/black-forest-labs/flux-1.1-pro/api) black\\-forest\\-labs/flux\\-1\\.1\\-pro 模型用于图像生成\n* 需要安装的必要包：\n\n```python\nfastapi\nuvicorn\njinja2\npython-multipart\npydantic\npython-dotenv\ngroq\nreplicate\nSpeechRecognition\npydub\naiohttp\naiofiles\nPillow\n```\n\n**您可以使用 pip 安装这些包：**\n\n```python\npip install fastapi uvicorn jinja2 python-multipart pydantic python-dotenv groq replicate SpeechRecognition pydub aiohttp aiofiles Pillow\n```\n\n**执行说明：**\n\n* ***设置环境变量：*** *在根目录创建一个 .env 文件，内容如下：*\n\n```python\nGROQ_API_KEY=your_groq_api_key_here\nREPLICATE_API_TOKEN=your_replicate_api_token_here\n```\n\n*用您的实际 API 密钥替换占位符值。*\n\n* **确保您已准备好所有必要的文件：**\n* — app/main.py\n* — app/config.py\n* — app/utils.py\n* — templates/index.html\n* — static/css/styles.css\n* — static/js/script.js\n* **运行 FastAPI 服务器：** 导航到包含 app/main.py 的目录并运行：\n\n```python\nuvicorn app.main:app - reload\n```\n\n* **访问应用程序：**\n* — 打开浏览器并转到 [http://127\\.0\\.0\\.1:8000](http://127.0.0.1:8000)\n* **使用应用程序：**\n* — a. 点击“开始录音”并说出您的提示。\n* — b. 完成后点击“停止录音”。\n* — c. 音频将自动转录。\n* — d. 点击“生成图像提示”以创建详细提示。\n* — e. 点击“生成图像”以根据提示创建图像。\n* — f. 使用“下载图像”按钮保存生成的图像。\n* — g. 点击“生成故事”以根据生成的图像创建故事。\n\n注意：确保您有良好的互联网连接，因为该应用程序依赖外部 API 提供各种功能。\n\n该应用程序展示了各种 AI 技术的复杂集成，包括语音识别、语言模型和图像生成，所有这些都封装在用户友好的 Web 界面中。\n\n如下面所示的 FastAPI UI\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Teb1wJzGOQZ3oqaLcLJwkA.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*6K-nORe7ubi0MRqIRLdrFA.png)\n\n## AI图像生成应用\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*1CClu2W3yRds1Lsk1rk9Ew.png)\n\n## 说出您的提示\n\n* 开始录音\n* 停止录音\n* 转录（转录文本）：一位美丽的印度模特在时装秀中走下Runway Ram\n* 根据转录文本创建一个新的提示以生成图像\n* — 生成的提示：“*生成一幅高度逼真的图像，描绘一位令人惊艳的印度模特在标志性的Runway Ram上走动，作为高端时装秀的一部分。这位模特是一位22岁的印度女性，拥有长长的黑发、深棕色的眼睛和无瑕的肌肤，身穿精美、复杂刺绣的lehenga choli，配以金银亮片，传统印度服饰，并搭配高跟鞋。她的服装设计精美，刺绣细致。强调优雅的褶皱、闪亮的面料，以及她优雅的姿态和自信的步伐。配饰方面，加入华丽的珠宝，如手链、金手镯和项链，装饰在她的手、脖子和一侧的发型上。灯光效果起着重要作用，设置温暖的舞台聚光灯，突出模特的服装，并用柔和的蓝色调照亮整个环境。摄影角度应全面展示服装的细节。期望的场景视角是模特正面全身照，走道周围被强烈的金色光线照亮，光线从内部散发出来*。”\n\n## 生成的图像\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*YljCEFTIc8hslZbf.jpg)\n\n## 从图像生成的故事\n\n*这位身着金银亮片服装和长裙的选美皇后的惊艳景象足以让任何观众陶醉。Sashaya Gnanavel在前景中显得格外引人注目，自信地走在T台上，吸引着现场观众的目光。她时尚的服装，搭配优雅的珍珠项链，吸引了在场每一个人的注意。这个系列展示了鲜艳的色彩和闪亮的刺绣，增加了活动的整体视觉吸引力。Sashaya在聚光灯下的自信与美丽，真实地证明了她在时尚行业的才华和奉献。她的妆容、珠宝和精美服装所创造的耀眼效果，为设计和工艺的非凡展示奠定了舞台。这个引人注目的场景 encapsulates 魔力和奢华，让观众对这一切的绝美感到惊叹。*\n\n## 代码实现\n\n创建虚拟环境\n\n要使用 Python 的 venv 模块创建虚拟环境，请按照以下步骤操作：\n\n* 打开终端或命令提示符。\n* 导航到您的项目目录（您想要创建虚拟环境的地方）。您可以使用 cd 命令更改目录。例如：\n\n```python\ncd path/to/your/project\n```\n\n* 通过运行以下命令创建虚拟环境：\n\n```python\npython -m venv venv\n```\n\n* 此命令将在您的项目文件夹中创建一个名为 venv 的新目录，该目录将包含虚拟环境（在 Windows 上）\n* 激活虚拟环境：\n\n```python\nvenv\\Scripts\\activate\n```\n\n* 文件夹结构\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*J3QJJACHVRtjrU1boxHUjA.png)\n\n* utils.py\n\n```python\nimport base64\nimport os\nfrom pydub import AudioSegment\n\ndef save_audio(audio_data):\n    # Decode the base64 audio data\n    audio_bytes = base64.b64decode(audio_data.split(\",\")[1])\n  \n    # Save the audio to a temporary file\n    temp_file = \"temp_audio.webm\"\n    with open(temp_file, \"wb\") as f:\n        f.write(audio_bytes)\n  \n    # Convert WebM to WAV\n    audio = AudioSegment.from_file(temp_file, format=\"webm\")\n    wav_file = \"temp_audio.wav\"\n    audio.export(wav_file, format=\"wav\")\n  \n    # Remove the temporary WebM file\n    os.remove(temp_file)\n  \n    return wav_file\n\ndef text_to_speech(text):\n    # Implement text-to-speech functionality if needed\n    pass\n```\n\n* main.py\n\n```python\n\"\"\"\n    1. 通过麦克风录制音频\n    2. 将音频转录为文本\n    3. 使用 Groq Llama3 模型生成图像提示\n    4. 使用 Replicate.ai Flux 模型生成图像\n    5. 显示生成的图像\n    6. 下载生成的图像\n    应用程序使用 DaisyUI 和 Tailwind CSS 进行样式设置，提供黑暗模式界面。布局是响应式的，应该在桌面和移动设备上都能良好运行。\n注意：您可能需要根据您使用的特定 API 和模型以及部署环境的安全性考虑来调整代码的某些部分。\n\n\"\"\"\nfrom fastapi import FastAPI, Request, HTTPException\nfrom fastapi.templating import Jinja2Templates\nfrom fastapi.staticfiles import StaticFiles\nfrom fastapi.responses import JSONResponse, FileResponse\nfrom pydantic import BaseModel\nimport speech_recognition as sr\nfrom groq import Groq\nimport replicate\nimport os\nimport aiohttp\nimport aiofiles\nimport time\nfrom dotenv import load_dotenv\nload_dotenv()\nfrom .utils import text_to_speech, save_audio\nfrom PIL import Image\nimport io\nimport base64\nimport base64\n\n\n## Function to encode the image\ndef encode_image(image_path):\n  with open(image_path, \"rb\") as image_file:\n    return base64.b64encode(image_file.read()).decode('utf-8')\n\napp = FastAPI()\n\napp.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\ntemplates = Jinja2Templates(directory=\"templates\")\n\n## Initialize Groq client with the API key\nGROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\nif not GROQ_API_KEY:\n    raise ValueError(\"GROQ_API_KEY is not set in the environment variables\")\ngroq_client = Groq(api_key=GROQ_API_KEY)\n\nclass AudioData(BaseModel):\n    audio_data: str\n\nclass ImagePrompt(BaseModel):\n    prompt: str\n\nclass PromptRequest(BaseModel):\n    text: str\n\n## Add this new model\nclass FreeImagePrompt(BaseModel):\n    prompt: str\n    image_path: str\n\n@app.get(\"/\")\nasync def read_root(request: Request):\n    return templates.TemplateResponse(\"index.html\", {\"request\": request})\n\n@app.post(\"/transcribe\")\nasync def transcribe_audio(audio_data: AudioData):\n    try:\n        # Save the audio data to a file\n        audio_file = save_audio(audio_data.audio_data)\n\n        # Transcribe the audio\n        recognizer = sr.Recognizer()\n        with sr.AudioFile(audio_file) as source:\n            audio = recognizer.record(source)\n        text = recognizer.recognize_google(audio)\n\n        return JSONResponse(content={\"text\": text})\n    except Exception as e:\n        raise HTTPException(status_code=400, detail=str(e))\n\n@app.post(\"/generate_prompt\")\nasync def generate_prompt(prompt_request: PromptRequest):\n    try:\n        text = prompt_request.text\n        # Use Groq to generate a new prompt\n        response = groq_client.chat.completions.create(\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a creative assistant that generates prompts for realistic image generation.\"},\n                {\"role\": \"user\", \"content\": f\"Generate a detailed prompt for a realistic image based on this description: {text}.The prompt should be clear and detailed in no more than 200 words.\"}\n            ],\n            model=\"llama-3.1-70b-versatile\",\n            max_tokens=256\n        )\n        generated_prompt = response.choices[0].message.content\n        print(f\"tweaked prompt:{generated_prompt}\")\n        return JSONResponse(content={\"prompt\": generated_prompt})\n    except Exception as e:\n        print(f\"Error generating prompt: {str(e)}\")\n        raise HTTPException(status_code=400, detail=str(e))\n\n@app.post(\"/generate_image\")\nasync def generate_image(image_prompt: ImagePrompt):\n    try:\n        prompt = image_prompt.prompt\n        print(f\"Received prompt: {prompt}\")\n\n        # Use Replicate to generate an image\n        output = replicate.run(\n            \"black-forest-labs/flux-1.1-pro\",\n            input={\n                \"prompt\": prompt,\n                \"aspect_ratio\": \"1:1\",\n                \"output_format\": \"jpg\",\n                \"output_quality\": 80,\n                \"safety_tolerance\": 2,\n                \"prompt_upsampling\": True\n            }\n        )\n      \n        print(f\"Raw output: {output}\")\n        print(f\"Output type: {type(output)}\")\n      \n        # Convert the FileOutput object to a string\n        image_url = str(output)\n      \n        print(f\"Generated image URL: {image_url}\")\n      \n        return JSONResponse(content={\"image_url\": image_url})\n    except Exception as e:\n        print(f\"Error generating image: {str(e)}\")\n        raise HTTPException(status_code=400, detail=str(e))\n\n@app.get(\"/download_image\")\nasync def download_image(image_url: str):\n    try:\n        # Create Output folder if it doesn't exist\n        output_folder = \"Output\"\n        os.makedirs(output_folder, exist_ok=True)\n\n        # Generate a unique filename\n        filename = f\"generated_image_{int(time.time())}.jpg\"\n        filepath = os.path.join(output_folder, filename)\n\n        # Download the image\n        async with aiohttp.ClientSession() as session:\n            async with session.get(image_url) as resp:\n                if resp.status == 200:\n                    async with aiofiles.open(filepath, mode='wb') as f:\n                        await f.write(await resp.read())\n\n        # Return the filepath and filename\n        return JSONResponse(content={\n            \"filepath\": filepath,\n            \"filename\": filename\n        })\n    except Exception as e:\n        print(f\"Error downloading image: {str(e)}\")\n        raise HTTPException(status_code=400, detail=str(e))\n\nclass StoryRequest(BaseModel):\n    filepath: str\n    filename: str\n\n@app.post(\"/generate_story_from_image\")\nasync def generate_story_from_image(content: StoryRequest):\n    try:\n        image_path = content.filepath\n        print(f\"Image path: {image_path}\")\n        # Check if the file exists\n        if not os.path.exists(image_path):\n            raise HTTPException(status_code=400, detail=\"Image file not found\")\n\n        # Getting the base64 string\n        base64_image = encode_image(image_path)\n\n        client = Groq()\n\n        chat_completion = client.chat.completions.create(\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\"type\": \"text\", \"text\": \"Generate a clear,concise,meaningful and engaging cover story for a highly acclaimed leisure magazine based on the image provided. The story should keep the audience glued and engaged and the story should bewithin 200 words.\"},\n                        {\n                            \"type\": \"image_url\",\n                            \"image_url\": {\n                                \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n                            },\n                        },\n                    ],\n                }\n            ],\n            model=\"llava-v1.5-7b-4096-preview\",\n        )\n\n        story = chat_completion.choices[0].message.content\n        print(f\"Generated story: {story}\")\n        return JSONResponse(content={\"story\": story})\n    except Exception as e:\n        print(f\"Error generating story from the image: {str(e)}\")\n        raise HTTPException(status_code=400, detail=str(e))\n\n@app.get(\"/download/{filename}\")\nasync def serve_file(filename: str):\n    file_path = os.path.join(\"Output\", filename)\n    return FileResponse(file_path, filename=filename)\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n```\n\n* script.js\n\n```python\nlet mediaRecorder;\nlet audioChunks = [];\n\nconst startRecordingButton = document.getElementById('startRecording');\nconst stopRecordingButton = document.getElementById('stopRecording');\nconst recordingStatus = document.getElementById('recordingStatus');\nconst transcription = document.getElementById('transcription');\nconst generatePromptButton = document.getElementById('generatePrompt');\nconst generatedPrompt = document.getElementById('generatedPrompt');\nconst generateImageButton = document.getElementById('generateImage');\nconst generatedImage = document.getElementById('generatedImage');\nconst downloadLink = document.getElementById('downloadLink');\nconst generateStoryButton = document.getElementById('generateStory');\nconst generatedStory = document.getElementById('generatedStory');\n\nstartRecordingButton.addEventListener('click', startRecording);\nstopRecordingButton.addEventListener('click', stopRecording);\ngeneratePromptButton.addEventListener('click', generatePrompt);\ngenerateImageButton.addEventListener('click', generateImage);\ngenerateStoryButton.addEventListener('click', generateStory);\n\nasync function startRecording() {\n    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n    mediaRecorder = new MediaRecorder(stream);\n\n    mediaRecorder.ondataavailable = (event) => {\n        audioChunks.push(event.data);\n    };\n\n    mediaRecorder.onstop = sendAudioToServer;\n\n    mediaRecorder.start();\n    startRecordingButton.disabled = true;\n    stopRecordingButton.disabled = false;\n    recordingStatus.textContent = 'Recording...';\n}\n\nfunction stopRecording() {\n    mediaRecorder.stop();\n    startRecordingButton.disabled = false;\n    stopRecordingButton.disabled = true;\n    recordingStatus.textContent = 'Recording stopped.';\n}\n\nasync function sendAudioToServer() {\n    const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });\n    const reader = new FileReader();\n    reader.readAsDataURL(audioBlob);\n    reader.onloadend = async () => {\n        const base64Audio = reader.result;\n        const response = await fetch('/transcribe', {\n            method: 'POST',\n            headers: {\n                'Content-Type': 'application/json',\n            },\n            body: JSON.stringify({ audio_data: base64Audio }),\n        });\n        const data = await response.json();\n        transcription.textContent = `Transcription: ${data.text}`;\n        generatePromptButton.disabled = false;\n    };\n    audioChunks = [];\n}\n\nasync function generatePrompt() {\n    const text = transcription.textContent.replace('Transcription: ', '');\n    const response = await fetch('/generate_prompt', {\n        method: 'POST',\n        headers: {\n            'Content-Type': 'application/json',\n        },\n        body: JSON.stringify({ text: text }),\n    });\n    const data = await response.json();\n    generatedPrompt.textContent = `Generated Prompt: ${data.prompt}`;\n    generateImageButton.disabled = false;\n}\n\nasync function generateImage() {\n    const prompt = generatedPrompt.textContent.replace('Generated Prompt: ', '');\n    const response = await fetch('/generate_image', {\n        method: 'POST',\n        headers: {\n            'Content-Type': 'application/json',\n        },\n        body: JSON.stringify({ prompt: prompt }),\n    });\n    const data = await response.json();\n    generatedImage.src = data.image_url;\n  \n    // Download the image and get the filepath\n    const downloadResponse = await fetch(`/download_image?image_url=${encodeURIComponent(data.image_url)}`);\n    const downloadData = await downloadResponse.json();\n  \n    // Store the filepath and filename for later use\n    generatedImage.dataset.filepath = downloadData.filepath;\n    generatedImage.dataset.filename = downloadData.filename;\n\n    // Set up the download link\n    downloadLink.href = `/download/${downloadData.filename}`;\n    downloadLink.download = downloadData.filename;\n    downloadLink.style.display = 'inline-block';\n}\n\nasync function generateStory() {\n    const imagePath = generatedImage.dataset.filepath;\n    const filename = generatedImage.dataset.filename;\n  \n    if (!imagePath || !filename) {\n        generatedStory.textContent = \"Error: Please generate an image first.\";\n        return;\n    }\n\n    try {\n        const response = await fetch('/generate_story_from_image', {\n            method: 'POST',\n            headers: {\n                'Content-Type': 'application/json',\n            },\n            body: JSON.stringify({ filepath: imagePath, filename: filename }),\n        });\n        if (!response.ok) {\n            throw new Error(`HTTP error! status: ${response.status}`);\n        }\n        const data = await response.json();\n      \n        // Display the generated story\n        generatedStory.textContent = data.story;\n      \n        // Make sure the story container is visible\n        document.getElementById('storyContainer').style.display = 'block';\n    } catch (error) {\n        console.error('Error:', error);\n        generatedStory.textContent = `Error: ${error.message}`;\n    }\n}\n\n// Modify the download link click event\ndownloadLink.addEventListener('click', async (event) => {\n    event.preventDefault();\n    const response = await fetch(downloadLink.href);\n    const blob = await response.blob();\n    const url = window.URL.createObjectURL(blob);\n    const a = document.createElement('a');\n    a.style.display = 'none';\n    a.href = url;\n    a.download = response.headers.get('Content-Disposition').split('filename=')[1];\n    document.body.appendChild(a);\n    a.click();\n    window.URL.revokeObjectURL(url);\n});\n```\n\n* style.css\n\n```python\nbody {\n    background-color: #1a1a2e;\n    color: #ffffff;\n}\n\n.container {\n    max-width: 1200px;\n}\n\n#imageContainer {\n    min-height: 300px;\n    display: flex;\n    align-items: center;\n    justify-content: center;\n    background-color: #16213e;\n    border-radius: 8px;\n}\n\n#generatedImage {\n    max-width: 100%;\n    max-height: 400px;\n    object-fit: contain;\n}\n```\n\n* index.html\n\n```python\n<!DOCTYPE html>\n<html lang=\"en\" data-theme=\"dark\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>AI 图像生成器</title>\n    <link href=\"https://cdn.jsdelivr.net/npm/daisyui@3.7.3/dist/full.css\" rel=\"stylesheet\" type=\"text/css\" />\n    <script src=\"https://cdn.tailwindcss.com\"></script>\n    <link rel=\"stylesheet\" href=\"{{ url_for('static', path='/css/styles.css') }}\">\n</head>\n<body>\n    <div class=\"container mx-auto px-4 py-8\">\n        <h1 class=\"text-4xl font-bold mb-8 text-center\">AI 图像生成器</h1>\n        <div class=\"grid grid-cols-1 md:grid-cols-2 gap-8\">\n            <div class=\"card bg-base-200 shadow-xl\">\n                <div class=\"card-body\">\n                    <h2 class=\"card-title mb-4\">说出你的提示</h2>\n                    <button id=\"startRecording\" class=\"btn btn-primary mb-4\">开始录音</button>\n                    <button id=\"stopRecording\" class=\"btn btn-secondary mb-4\" disabled>停止录音</button>\n                    <div id=\"recordingStatus\" class=\"text-lg mb-4\"></div>\n                    <div id=\"transcription\" class=\"text-lg mb-4\"></div>\n                    <button id=\"generatePrompt\" class=\"btn btn-accent mb-4\" disabled>生成图像提示</button>\n                    <div id=\"generatedPrompt\" class=\"text-lg mb-4\"></div>\n                    <button id=\"generateImage\" class=\"btn btn-success\" disabled>生成图像</button>\n                </div>\n            </div>\n            <div class=\"card bg-base-200 shadow-xl\">\n                <div class=\"card-body\">\n                    <h2 class=\"card-title mb-4\">生成的图像</h2>\n                    <div id=\"imageContainer\" class=\"mb-4\">\n                        <img id=\"generatedImage\" src=\"\" alt=\"生成的图像\" class=\"w-full h-auto\">\n                    </div>\n                    <a id=\"downloadLink\" href=\"#\" download=\"generated_image.png\" class=\"btn btn-info\" style=\"display: none;\">下载图像</a>\n                </div>\n            </div>\n        </div>\n        <!-- Add this new section after the existing cards -->\n        <div class=\"card bg-base-200 shadow-xl mt-8\">\n            <div class=\"card-body\">\n                <h2 class=\"card-title mb-4\">从图像生成故事</h2>\n                <button id=\"generateStory\" class=\"btn btn-primary mb-4\">生成故事</button>\n                <div id=\"storyContainer\" class=\"mb-4\">\n                    <p id=\"generatedStory\" class=\"text-lg\"></p>\n                </div>\n            </div>\n        </div>\n    </div>\n    <script src=\"{{ url_for('static', path='/js/script.js') }}\"></script>\n</body>\n</html>\n```\n\n## 结论\n\nAI图像生成器和故事创作项目成功整合了各种AI技术，创建了一个互动式Web应用程序，允许用户根据音频提示生成图像和故事。通过利用FastAPI作为后端和现代前端技术，该应用程序提供了无缝的用户体验。\n\n## 关键要点：\n\n1. AI模型的集成：该项目展示了如何集成多个AI模型，包括用于文本生成的Groq和用于图像生成的Replicate，以创建一个增强用户创意的统一应用程序。\n2. 用户互动：该应用程序允许用户通过语音命令进行互动，使其易于访问且用户友好。录音、转录以及基于该输入生成内容的能力展示了语音驱动应用程序的潜力。\n3. 动态内容生成：通过根据用户输入动态生成图像和故事，该应用程序突显了AI在内容创作中的能力，为用户提供独特且个性化的输出。\n4. 响应式设计：使用DaisyUI和Tailwind CSS确保应用程序在视觉上吸引人且响应迅速，适应各种设备上的用户。\n5. 未来增强：该项目可以通过加入额外功能进一步增强，例如用户身份验证、保存用户生成的内容，以及扩展用于不同创意任务的AI模型范围。\n\n总体而言，该项目作为一个综合示例，展示了如何构建一个结合音频处理、图像生成和故事讲述的AI驱动的网络应用程序，为创意领域的创新应用铺平了道路。\n\n## 参考文献\n\n* FastAPI 文档: [FastAPI](https://fastapi.tiangolo.com/) 是一个现代的 web 框架，用于使用 Python 构建 API。它旨在易于使用且快速。\n* Pydantic: [Pydantic](https://pydantic-docs.helpmanual.io/) 用于数据验证和使用 Python 类型注释的设置管理。\n* Groq: [Groq](https://groq.com/docs/) 是一个构建和部署 AI 模型的平台。它提供文本生成和其他 AI 任务的 API。\n* Replicate: [Replicate](https://replicate.com/docs) 是一个允许您在云中运行机器学习模型的平台。它提供各种模型的 API，包括图像生成。\n* SpeechRecognition: [SpeechRecognition](https://pypi.org/project/SpeechRecognition/) 是一个执行语音识别的库，支持多种引擎和 API。\n* Pillow: [Pillow](https://pillow.readthedocs.io/en/stable/) 是 Python Imaging Library (PIL) 的一个分支，为您的 Python 代码添加图像处理能力。\n* JavaScript Fetch API: [Fetch API](https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API) 提供了一种现代方式在 JavaScript 中发起网络请求。\n* HTML5 音频 API: [HTML5 Audio API](https://developer.mozilla.org/en-US/docs/Web/API/HTMLAudioElement) 允许您在 web 应用程序中播放音频文件。\n* DaisyUI: [DaisyUI](https://daisyui.com/) 是一个为 Tailwind CSS 提供预设计组件的组件库。\n* Tailwind CSS: [Tailwind CSS](https://tailwindcss.com/docs) 是一个实用优先的 CSS 框架，用于创建自定义设计，而无需离开 HTML。\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/ai-powered-ocr-with-phi-3-vision-128k-the-future-of-document-processing-7be80c46bd16","frontmatter":{"title":"采用 Phi-3-Vision-128K 的人工智能 OCR：文档处理的未来","meta_title":"采用 Phi-3-Vision-128K 的人工智能 OCR：文档处理的未来","description":"在快速发展的人工智能世界中，多模式模型正在为整合视觉和文本数据设定新的标准……","date":"2024-11-08T00:26:30.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BR-H6cQoyoRo6gVRqjvAyA.png","categories":["Natural Language Processing","Computer Vision","Data Science"],"author":"Rifx.Online","tags":["OCR","tokens","encoder","language","document"],"draft":false,"slug":"blog/ai-powered-ocr-with-phi-3-vision-128k-the-future-of-document-processing-7be80c46bd16"},"content":"\n\n\n\n\n在快速发展的人工智能领域，多模态模型正在为视觉和文本数据的整合设定新标准。最新的突破之一是 **Phi\\-3\\-Vision\\-128K\\-Instruct**，这是一个最先进的开放多模态模型，推动了AI在处理图像和文本方面的能力边界。该模型专注于文档提取、光学字符识别（OCR）和一般图像理解，能够彻底改变我们处理PDF、图表、表格以及其他结构化或半结构化文档的信息方式。\n\n让我们深入探讨Phi\\-3\\-Vision\\-128K\\-Instruct的细节，探索其架构、技术要求、负责任的使用考虑，并了解它如何简化文档提取、PDF解析和AI驱动的数据分析等复杂任务。\n\n## 什么是 Phi\\-3\\-Vision\\-128K\\-Instruct？\n\nPhi\\-3\\-Vision\\-128K\\-Instruct 属于 Phi\\-3 模型系列，专为多模态数据处理而构建，支持最长 **128,000 个令牌** 的上下文长度。该模型结合了文本和视觉数据，适合需要同时解释文本和图像的任务。其开发涉及 **5000 亿个训练令牌**，结合了高质量的合成数据和严格筛选的公开可用来源。通过包括 **监督微调和偏好优化** 的精细训练过程，该模型旨在提供精确、可靠和安全的 AI 解决方案。\n\nPhi\\-3\\-Vision\\-128K\\-Instruct 拥有 **42 亿个参数**，其架构包括图像编码器、连接器、投影器和 Phi\\-3 Mini 语言模型，使其成为广泛应用的轻量级而强大的选择。\n\n## 核心用例\n\n该模型的主要应用跨越多个领域，特别关注于：\n\n* **文档提取和OCR：** 高效地将文本图像或扫描文档转换为可编辑格式。它可以处理复杂的布局，如表格、图表和图示，使其成为数字化实体文档或自动化数据提取工作流的宝贵工具。\n* **一般图像理解：** 解析视觉内容以识别对象、解释场景并提取相关信息。\n* **内存/计算受限环境：** 在计算能力或内存有限的情况下运行AI任务，而不影响性能。\n* **延迟受限场景：** 在实时应用中减少处理延迟，例如实时数据流、基于聊天的助手或流媒体内容分析。\n\n## 如何开始使用 Phi\\-3\\-Vision\\-128K\\-Instruct\n\n要使用 Phi\\-3\\-Vision\\-128K\\-Instruct，您需要设置开发环境，安装所需的库和工具。该模型集成在 Hugging Face `transformers` 库的开发版本 (4\\.40\\.2\\) 中。在深入代码示例之前，请确保您的 Python 环境已配置这些包：\n\n```python\n## Required Packages\nflash_attn==2.5.8\nnumpy==1.24.4\nPillow==10.3.0\nRequests==2.31.0\ntorch==2.3.0\ntorchvision==0.18.0\ntransformers==4.40.2\n```\n要加载模型，您可以更新本地的 `transformers` 库，或者直接从源代码克隆并安装：\n\n```python\npip uninstall -y transformers && pip install git+https://github.com/huggingface/transformers\n```\n现在，让我们进入一些实际的代码片段，展示如何利用这个强大的模型进行 AI 驱动的文档提取和文本生成。\n\n## 加载模型的示例代码\n\n这里有一个 Python 示例，展示如何初始化模型并开始进行推断。我们将利用类和函数使代码保持整洁和有序：\n\n```python\nfrom PIL import Image\nimport requests\nfrom transformers import AutoModelForCausalLM, AutoProcessor\n\nclass Phi3VisionModel:\n    def __init__(self, model_id=\"microsoft/Phi-3-vision-128k-instruct\", device=\"cuda\"):\n        \"\"\"\n        使用指定的模型 ID 和设备初始化 Phi3VisionModel。\n        \n        参数：\n            model_id (str): 来自 Hugging Face 模型库的预训练模型标识符。\n            device (str): 加载模型的设备（\"cuda\" 表示 GPU，或 \"cpu\"）。\n        \"\"\"\n        self.model_id = model_id\n        self.device = device\n        self.model = self.load_model()  # 在初始化时加载模型\n        self.processor = self.load_processor()  # 在初始化时加载处理器\n    \n    def load_model(self):\n        \"\"\"\n        加载具有因果语言建模能力的预训练语言模型。\n        \n        返回：\n            model (AutoModelForCausalLM): 加载的模型。\n        \"\"\"\n        print(\"加载模型中...\")\n        # 使用自动设备映射和数据类型调整加载模型\n        return AutoModelForCausalLM.from_pretrained(\n            self.model_id, \n            device_map=\"auto\",  # 自动将模型映射到适当的设备\n            torch_dtype=\"auto\",  # 根据设备使用合适的 torch 数据类型\n            trust_remote_code=True,  # 允许执行自定义代码以加载模型\n            _attn_implementation='flash_attention_2'  # 使用优化的注意力实现\n        ).to(self.device)  # 将模型移动到指定设备\n    \n    def load_processor(self):\n        \"\"\"\n        加载与模型关联的处理器，以处理输入和输出。\n        \n        返回：\n            processor (AutoProcessor): 用于处理文本和图像的加载处理器。\n        \"\"\"\n        print(\"加载处理器中...\")\n        # 使用 trust_remote_code=True 加载处理器，以处理任何自定义处理逻辑\n        return AutoProcessor.from_pretrained(self.model_id, trust_remote_code=True)\n    \n    def predict(self, image_url, prompt):\n        \"\"\"\n        使用模型根据给定的图像和提示进行预测。\n        \n        参数：\n            image_url (str): 要处理的图像的 URL。\n            prompt (str): 指导模型生成的文本提示。\n        \n        返回：\n            response (str): 模型生成的响应。\n        \"\"\"\n        # 从提供的 URL 加载图像\n        image = Image.open(requests.get(image_url, stream=True).raw)\n        \n        # 为模型格式化输入提示模板\n        prompt_template = f\"<|user|>\\n<|image_1|>\\n{prompt}<|end|>\\n<|assistant|>\\n\"\n        \n        # 处理输入，将提示和图像转换为张量格式\n        inputs = self.processor(prompt_template, [image], return_tensors=\"pt\").to(self.device)\n        \n        # 设置模型响应生成的参数\n        generation_args = {\n            \"max_new_tokens\": 500,  # 最大生成的令牌数\n            \"temperature\": 0.7,     # 生成中的采样温度以增加多样性\n            \"do_sample\": False      # 禁用采样以获得确定性输出\n        }\n        print(\"生成响应中...\")\n        # 使用模型生成输出 ID，跳过输入令牌\n        output_ids = self.model.generate(**inputs, **generation_args)\n        output_ids = output_ids[:, inputs['input_ids'].shape[1]:]  # 忽略输出中的输入提示\n        \n        # 解码生成的输出令牌以获取响应文本\n        response = self.processor.batch_decode(output_ids, skip_special_tokens=True)[0]\n        return response\n\n## 初始化模型\nphi_model = Phi3VisionModel()\n\n## 示例预测\nimage_url = \"https://example.com/sample_image.png\"  # 示例图像的 URL\nprompt = \"以 json 格式提取数据。\"  # 模型指导的提示\nresponse = phi_model.predict(image_url, prompt)  # 从模型获取响应\n\nprint(\"响应:\", response)  # 打印生成的响应\n```\n上述代码定义了一个 `Phi3VisionModel` 类，抽象了模型的加载和使用，使其更容易集成到您的应用程序中。`predict()` 方法展示了如何使用自定义提示进行基于图像的推断。\n\n为了更新文章，侧重于测试 Phi-3-Vision-128K-Instruct 模型的 OCR 能力，我们将添加一个部分，详细说明模型在处理扫描的身份证等实际示例时的表现。\n\n## 测试 OCR 功能与扫描的身份证件\n\n为了评估 Phi\\-3\\-Vision\\-128K\\-Instruct 模型的 OCR 性能，我们使用几张真实的扫描身份证件图像进行了测试。这些图像在质量和清晰度上各不相同，为模型提供了一系列挑战。目标是展示模型在提取具有不同特征的文档中的文本信息方面的表现，如模糊、复杂背景和不同的字体。\n\n**图像 1：** 一本虚构的乌托邦护照，包含详细的文本，包括个人信息，如姓名、国籍、出生地、签发日期和到期日期。文本略显风格化，底部有机器可读区。图像质量高，没有明显的背景噪声。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MltpseOI3HhvCkUZMwLdEQ.png)\n\n**输出：**\n\n\n```python\n{\n  \"Type/Type\": \"P\",\n  \"Country code/Code du pays\": \"UTO\",\n  \"Passport Number/N° de passeport\": \"L898902C3\",\n  \"Surname/Nom\": \"ERIKSSON\",\n  \"Given names/Prénoms\": \"ANNA MARIA\",\n  \"Nationality/Nationalité\": \"UTOPIAN\",\n  \"Date of Birth/Date de naissance\": \"12 AUGUST/AOUT 74\",\n  \"Personal No./N° personnel\": \"Z E 184226 B\",\n  \"Sex/Sexe\": \"F\",\n  \"Place of birth/Lieu de naissance\": \"ZENITH\",\n  \"Date of issue/Date de délivrance\": \"16 APR/AVR 07\",\n  \"Authority/Autorité\": \"PASSPORT OFFICE\",\n  \"Date of expiry/Date d'expiration\": \"15 APR/AVR 12\",\n  \"Holder's signature/Signature du titulaire\": \"anna maria eriksson\",\n  \"Passport/Passeport\": \"P<UTOERIKSSON<<ANNA<MARIA<<<<<<<<<<<<<<<<<<<<<<<L898902C36UT07408122F1204159ZE184226B<<<<10\"\n}\n```\n**图像 2：** 一本荷兰护照，清晰地显示持有人和整齐格式化的文本。字段包括护照号码、姓名、出生日期、国籍和到期日期。该文件呈现出高对比度，使文本提取相对简单。底部的机器可读区 (MRZ) 提供了一种结构化的数据格式，有助于验证提取信息的准确性。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*WGV4tTxI9xISmAvFs8ovNw.png)\n\n**输出：**\n\n\n```python\nHere's the extracted full data from the passport in JSON format:\n\n{\n  \"passport\": {\n    \"issuingCountry\": \"Netherlands\",\n    \"issuingAuthority\": \"Koninkrijk der Nederlanden\",\n    \"passportNumber\": \"SPEC12014\",\n    \"issuingDate\": \"09 MAR 2014\",\n    \"expiryDate\": \"09 MAR 2024\",\n    \"holder\": {\n      \"gender\": \"F\",\n      \"nationality\": \"Netherlands\",\n      \"placeOfBirth\": \"SPECIMEN\",\n      \"sex\": \"WF\",\n      \"firstNames\": [\n        \"Willem\",\n        \"Lieselotte\"\n      ]\n    },\n    \"physicalDescription\": {\n      \"height\": \"1.75 m\",\n      \"hairColor\": \"gray\",\n      \"hairLength\": \"short\"\n    },\n    \"issuingOffice\": \"Burg. van Stad en Dorp\",\n    \"issuingDateAsInt\": \"14032014\",\n    \"expiryDateAsInt\": \"14032024\",\n    \"fieldsExtracted\": [\n      {\n        \"code\": \"NL\",\n        \"dateOfBirth\": \"10 MAR 1965\",\n        \"dateOfIssue\": \"09 MAR 2014\",\n        \"dateOfExpiry\": \"09 MAR 2024\",\n        \"firstNames\": [\n          \"Willem\",\n          \"Lieselotte\"\n        ],\n        \"nationality\": \"Netherlands\",\n        \"passportNumber\": \"SPEC12014\",\n        \"placeOfBirth\": \"SPECIMEN\",\n        \"sex\": \"WF\"\n      }\n    ]\n  }\n}\n```\n\n## 尝试 Phi\\-3\\-Vision\\-128K\\-Instruct\n\n如果您想亲自尝试 Phi\\-3\\-Vision\\-128K\\-Instruct 模型，可以通过以下链接进行探索：[在 Azure AI 上尝试 Phi\\-3\\-Vision\\-128K\\-Instruct](https://ai.azure.com/explore/models/Phi-3-vision-128k-instruct/version/1/registry/azureml)。该链接允许您体验模型的功能并实验其 OCR 功能。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*7feNu3ZuclgAnAzbJMMSFg.png)\n\n## 理解架构与训练\n\n**Phi\\-3\\-Vision\\-128K\\-Instruct** 模型不仅仅是一个语言模型——它是一个多模态强者，能够处理视觉和文本数据。它经历了全面的训练过程，包含 **5000亿个标记**，结合了文本和图像数据。其架构整合了语言模型和图像处理模块，创建了一个能够理解 **128K 个标记** 上下文的统一系统，支持更长的对话或大量内容的文档。\n\n在强大的硬件上训练，例如 **512 H100 GPUs**，并利用 **flash attention** 提高内存效率，这个模型能够轻松处理大规模任务。训练数据集包括合成数据和经过筛选的真实世界数据，强调 **数学、编码、常识推理** 和 **一般知识**，使其足够灵活以适应各种应用。\n\n## 关键基准和性能\n\nPhi\\-3\\-Vision\\-128K\\-Instruct 的性能已经在多个基准测试中进行评估，包括 **ScienceQA**、**AI2D**、**MathVista** 和 **TextVQA**。它的得分在结合文本和视觉的任务中始终超过许多现有模型，特别是在以下领域：\n\n* **文档理解**：从复杂文档（如 PDF 或图像）中提取有用信息。\n* **表格和图表理解**：准确解读图形数据并将其转换为文本解释。\n\n特别是，该模型在 **ChartQA** 上取得了令人印象深刻的 **81\\.4%**，在 **AI2D** 上取得了 **76\\.7%**，展示了其有效理解数据丰富文档的能力。\n\n## 为什么OCR和文档提取很重要\n\n文档提取和OCR对于企业和研究至关重要，使得将打印或手写文本转换为机器可读格式成为可能。使用像Phi-3-Vision-128K-Instruct这样的AI模型，可以显著简化**PDF解析**、**数据录入自动化**、**发票处理**和**法律文档分析**等任务。\n\n无论您处理的是扫描文档、截图还是拍摄的页面，该模型的多模态能力都可以帮助**自动化数据提取**，使其成为提高生产力和减少人工工作量的宝贵工具。\n\n## 负责任的人工智能与安全措施\n\n虽然该模型功能强大，但开发者应注意其局限性。**语言偏见**、**刻板印象强化**和**不准确内容生成**是潜在问题。对于高风险的使用案例，例如**健康或法律建议**，需要额外的**验证和内容过滤**层。\n\n## 未来方向与微调\n\n想要扩展 Phi\\-3\\-Vision\\-128K\\-Instruct 的功能吗？支持微调，可以使用 **Phi\\-3 Cookbook** 进行，该手册提供了调整模型以适应特定任务的配方，例如 **文档分类**、**增强的 OCR 准确性** 和 **专业的图像理解**。\n\n## 结论\n\nPhi\\-3\\-Vision\\-128K\\-Instruct 不仅仅是多模态 AI 的一步进展；它是迈向一个未来的飞跃，在这个未来中，**文档提取、OCR 和 AI 驱动的内容生成**是无缝且易于获取的。凭借广泛的训练、强大的架构和深思熟虑的设计，该模型使开发者能够在各个领域转变数据处理。\n\n敬请期待更多关于如何将该模型与现实世界应用集成的高级示例和教程，我们将探索**处理多种文档类型**和应用**AI 驱动的技术**从多样化来源提取有价值的见解。\n\n**AI 驱动的文档提取**的未来从未如此光明！\n\n"},{"lang":"zh","group":"blog","slug":"blog/alibabas-open-source-qwen-how-it-s-revolutionizing-ai-and-how-you-can-use-it-dcba8f687c97","frontmatter":{"title":"阿里巴巴开源 Qwen：它如何彻底改变人工智能以及如何使用它","meta_title":"阿里巴巴开源 Qwen：它如何彻底改变人工智能以及如何使用它","description":"阿里巴巴最近在 2024 年云栖大会期间开源了 Qwen 2.5 模型，在 AI 领域掀起了波澜。超过 100 个…","date":"2024-10-26T00:26:25.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*I7QDwbLMzoJ_ORq5.jpg","categories":["Programming","Machine Learning","Natural Language Processing"],"author":"Rifx.Online","tags":["Qwen","multimodal","open-source","fine-tune","text-to-video"],"draft":false,"slug":"blog/alibabas-open-source-qwen-how-it-s-revolutionizing-ai-and-how-you-can-use-it-dcba8f687c97"},"content":"\n阿里巴巴最近在人工智能领域引起了轰动，在2024年 Apsara 大会上开源了其 **Qwen 2.5** 模型。Qwen 拥有超过 100 个模型，涵盖语言、视觉、音频和代码等多种模态，使其成为最全面的开源人工智能解决方案之一。此次发布通过提供多样化应用的工具，赋能开发者，从文本到视频生成到实时问答。\n\n\n\n## 阿里巴巴 Qwen 模型的关键特性\n\n1. **多模态能力**：Qwen 模型处理多种输入，包括文本、音频和视觉数据。这种多模态方法使其适用于广泛的行业，从媒体和娱乐到机器人技术。\n2. **开源**：Qwen 可在 **Hugging Face** 和 **ModelScope** 等平台上获取，已经被下载超过 4000 万次，基于其基础构建的自定义模型超过 50,000 个。\n3. **增强性能**：Qwen2.5 引入了改进的语言理解、数学和编码能力，与该领域的领先模型竞争。通过针对结构化数据理解和长文本生成等任务的优化性能，Qwen 为高级 AI 应用打开了大门。\n\n## 如何使用阿里巴巴的 Qwen\n\n开发者和组织可以在 Hugging Face 等平台上访问 Qwen 模型，具体可以：\n\n* **微调模型**：为特定行业应用量身定制 Qwen，例如客户服务、自动化或视频内容创作。\n* **与应用集成**：Qwen 的文本转视频模型可以集成到媒体制作流程中，从静态图像和文本提示生成动态内容。\n* **开发 AI 助手**：借助增强的视觉语言模型，Qwen 可用于机器人和自动驾驶汽车，以处理视频数据并执行实时任务，如导航或物体识别。\n\n**通过 Hugging Face 使用 Qwen 的示例**：\n\n```python\nfrom transformers import QwenTokenizer, QwenModel\n\ntokenizer = QwenTokenizer.from_pretrained(\"qwen-2.5\")\nmodel = QwenModel.from_pretrained(\"qwen-2.5\")\n\ninput_text = \"What is the future of AI in healthcare?\"\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\noutputs = model(input_ids)\n```\n\n这使用户能够访问 Qwen 模型，运行推理，并根据特定需求进行定制。\n\n## Qwen在各行业的影响\n\n1. **媒体与娱乐**：凭借新的文本到视频功能，Qwen可以自动从书面脚本生成视频，通过自动化繁琐的制作任务来改变创意产业。\n2. **机器人技术与自动驾驶车辆**：Qwen中增强的视觉语言模型可以帮助机器人理解现实世界环境，从而在自动驾驶或制造中做出更好的决策。\n3. **软件开发**：由Qwen驱动的阿里巴巴AI开发工具自动化了代码生成、调试和需求分析等任务，使开发人员能够专注于更高层次的问题解决。\n\n## 结论：开放AI创新的新纪元\n\n通过开源其Qwen 2.5模型，阿里巴巴正在使先进的AI技术变得更加普及。开发者、初创企业和大型企业都可以利用Qwen的多模态和实时能力，在从媒体到自动驾驶汽车等行业推动创新。无论您是希望为特定应用微调模型的开发者，还是将AI集成到基础设施中的企业，Qwen都提供强大的工具来加速进步。\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/artifacts-top-mindblowing-uses-of-claude-3-5-sonent-6830b2acfa4b","frontmatter":{"title":"文物：Claude 3.5 Sonent 最令人惊叹的用途","meta_title":"文物：Claude 3.5 Sonent 最令人惊叹的用途","description":"Anthropic 最近推出了其最先进的法学硕士课程“Claude 3.5 Sonnet”，这令人惊叹。社交媒体上的人们称这种模式为……","date":"2024-11-08T00:18:38.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*XL1dN9VCFcbz3m5N","categories":["Programming","Natural Language Processing","Generative AI"],"author":"Rifx.Online","tags":["Sonnet","context","Artifacts","code","generation"],"draft":false,"slug":"blog/artifacts-top-mindblowing-uses-of-claude-3-5-sonent-6830b2acfa4b"},"content":"\n\n\nAnthropic 最近推出了其最先进的 LLM，“Claude 3\\.5 Sonnet”，让人惊叹。社交媒体上的人们称这一模型为当前最先进的 LLM。这个 AI 模型在性能上超越了所有现有的 LLM，例如 GPT\\-4、GPT\\-4o mini、Llama 3 等等。Claude 3\\.5 Sonnet 的上下文窗口为 200K，最大输出为 8192 个 tokens。它可以生成一个包含大量数据作为输入的庞大段落。Claude 3\\.5 Sonnet 是最好的 AI 视觉模型之一，在各种测试案例中击败了 GPT\\-4o 和 Llama 3。它可以从文档和 PDF 中提取数据和文本。这些只是使 Claude 3\\.5 Sonnet 成为最佳的几个因素，但 Anthropic 还新增了一个功能，使该 LLM 成为最佳代码生成器，“Artifacts”。这是一个弹出窗口，允许用户查看他们的代码、编辑代码，并在该弹出窗口中实时查看他们的项目。本文将展示 Artifacts 和 Claude 3\\.5 Sonnet 的顶级案例，以及您可以使用此工具开发的新项目。\n\n\n\n## Claude 3\\.5 Sonnet 的使用案例\n\n要使用这些工件，您必须拥有一个 Claude 帐户。Claude 3\\.5 Sonnet 对所有人免费，但每天仅允许有限的聊天。用户必须购买大约 $20 的订阅，以便无限制使用该模型进行聊天。在 Claude 3\\.5 Sonnet 中尝试这些提示并创建令人兴奋的工具。用户还可以通过分享链接在互联网上发布他们的项目。\n\n### 1\\. 互动PDF仪表板\n\n阅读大型PDF文件很无聊。因此，让我们创建一个互动PDF仪表板，以便我能更好地阅读我的PDF文件。\n\n***提示\\- 创建一个互动PDF仪表板，帮助我以更具视觉吸引力的方式查看、阅读和学习这些信息。它有一个标签，可以根据PDF中的信息进行测验。确保它具有暗模式。***\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*66Iowbu8ZKA-IGfK)\n\n要使用此提示，首先附上您想要阅读摘要的PDF和提示。用户还可以根据自己的喜好进行更改，Claude 3\\.5 Sonnet将根据更改重新生成代码。\n\n[***点击这里查看项目***](https://claude.site/artifacts/4b9590a8-260e-476a-ab75-ec1d69f81d1e)***.***\n\n### 2\\. 使用动画可视化任何事物\n\n可视化您所阅读的任何内容可能是理解它的最佳方式。添加动画将使您的可视化更上一层楼，使其易于理解。\n\n***提示：为每个过程步骤添加动画，以创建光合作用的可视化。确保使用不同的颜色和纹理使整个仪表板在视觉上吸引人，以创建高质量的动画。***\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*W5lWitTO5fj7lOs0)\n\n这是我们得到的第一个输出，在10秒的处理时间内令人印象深刻，但通过提供更多的指令和精确度来优化提示后可以更好。我们得到\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*TAguaCWhC1CYFNw7)\n\n正如您所看到的，我们将整个循环分为不同的阶段，最终结果令人难以置信。这种方法可以通过将任何过程分解为更简单的步骤来帮助您理解。\n\n[***点击这里查看项目***](https://claude.site/artifacts/b9769e4e-c20b-43da-945a-bfc41782900c)***.***\n\n### 3\\. 科学工具\n\n科学工具是帮助用户快速和直观理解科学概念的工具和服务。在 Claude 3\\.5 Sonnet 的帮助下，用户可以通过单个提示创建科学工具。只需详细描述您的工具。我将利用它为电子工程创建一个交互式工具。\n\n***提示 — 使用 React 创建一个显示二极管的仪表板。我们可以在电路中连接正向偏置或反向偏置，并根据掺杂和耗尽区等物理因素，通过显示二极管中孔和电子的运动来改变场景。可以更改所有内容，如电压、电流、二极管类型、Si 或 Ge，并使用不同的颜色和动态元素进行动画。***\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*9bE50q8nsh6_SQ36)\n\n用户可以使用 Claude 3\\.5 Sonnet 创建多个工具，例如基本项目学校、磁场的电流效应、压力与温度关系工具、原子模型动画等。\n\n***点击 [这里查看项目](https://claude.site/artifacts/5b18fb27-1093-41d2-ac4e-54e47e4ddd3a)。***\n\n### 4\\. 游戏开发\n\n每个人都喜欢玩游戏。使用 Claude 3\\.5 Sonnet，我们可以创建不同类型的游戏。我将使用 LLM 制作一个井字棋游戏。\n\n***提示\\- 使用 React 创建一个井字棋游戏，并使其功能正常。通过使用 CSS 使其在视觉上更具吸引力。***\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*jlSn7mCbaFadUT3F)\n\n结果可以更好。之后，我尝试提供更多建议和输入，并根据我的建议，Claude 开发了一个完美的井字棋游戏，结果如下。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*4v9KP4KFmQgY14YZ)\n\n如您所见，最终产品优化了 CSS。动画有了很大改善，并在游戏结束时添加了一个弹出窗口，显示获胜者的名字。您可以使用这些提示制作诸如贪吃蛇、飞行棋、石头剪刀布等游戏。\n\n[***点击这里尝试该项目。***](https://claude.site/artifacts/d57fdf93-79fb-443a-9bee-a58ab6eb911f)\n\n### 5\\. 网络应用\n\n网络应用是可以直接在浏览器上访问的工具，无需在手机上安装应用程序。Claude 3\\.5 Sonnet 可用于开发不同类型的网络应用。在这种情况下，我们将研究一个费用跟踪应用。\n\n***提示\\- 创建一个费用跟踪的网络应用，具备以下功能：首先，询问他们的月支出——比如说 2000 卢比。现在，无论这个人花费什么，请确保创建一些类别，如食品、旅行和必需品，并提供添加任何内容的选项。在月末添加一个节省资金的选项。此外，它还有投资选项，现在你知道是什么类型的。例如，“如果你每月在共同基金中投资 200，五年后你将拥有 X 金额。”此外，该应用还有生成与支出相关的图表和图形的功能。确保该应用的 UI/UX 设计视觉上吸引人。***\n\n这是最终结果。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*XGOwT3vpxL0dxzzP)\n\n用户可以使用 Claude 3\\.5 Sonnet 创建多个工具，如待办事项列表、简单计算器、电影推荐应用、文本摘要工具等。\n\n[***点击这里尝试该项目***](https://claude.site/artifacts/66770d05-aafe-45c4-b938-8cda0e82b903)***.***\n\n### 6\\. 3D模拟\n\nClaude 3\\.5 Sonent可以创建模型或项目的3D模拟，无论是太阳系模型、原子模型、分子生物学的中心法则等。您可以使用这个LLM在3D中可视化任何想法。我们将模拟一个3D太阳系模型，所有行星围绕太阳旋转。\n\n***提示\\-制作一个包含太阳系重力的空间模拟的three js应用程序，所有行星在一个网页文件中。太阳系中行星围绕太阳旋转……每当有人悬停鼠标时，会弹出行星的名称和基本细节，如质量、重力等；保持物理概念，如行星的自转和公转，完整。***\n\n使用这个提示以获得所需的结果。\n\n***点击这里查看项目。***\n\n[***项目 1***](https://x.com/websim_ai/status/1803901523522699730?t=BCe28ywbC2xD1Mk4DRmo5w&s=08)\n\n[***项目 2***](https://x.com/ammaar/status/1804649903815115053?t=7PeWPg62bkABtKEtKVmFbw&s=08)\n\n[***项目 3***](https://x.com/goldcaddy77/status/1804724702901891313?t=iqcLQBhYaIRnt3gIBRKBQg&s=08)\n\n### 7\\. 思维导图\n\n思维导图是一种可视化组织信息的头脑风暴技术，采用层次结构。其主要特点是将一个主要思想作为图表的中心点，子主题从中心点分支出来并连接到支持思想。这有助于读者快速记忆和理解信息。让我们通过创建思维导图来生成一个提示。\n\n***提示：创建一个关于物体检测如何工作的思维导图。使用动画和颜色使其互动性强且视觉上吸引人。***\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*igUaLzf8b2ZTEy-m)\n\n通过更改主题，您可以使用此提示生成您想要的任何思维导图。您还可以添加不同的颜色和动画，使其更加互动。\n\n[***点击这里尝试该项目***](https://claude.site/artifacts/f1ce9002-9434-4b40-9713-ef184c467557)\n\n### 8\\. SEO工具\n\nSEO对于任何博客文章在Google上的排名至关重要。使用Claude，我们可以创建一个工具，帮助我们改善网站的SEO。您可以使用这个提示来制作您的工具。\n\n***提示\\- 创建一个SEO工具，允许我上传我的博客文章、行业和我试图排名的关键字。在我上传所有这些内容后，我希望点击一个按钮，给出突出显示的建议，说明需要更改的内容。包括一个重置按钮，重新开始这个过程。并且在关键字旁边添加一个加号按钮，我可以添加多个。在SEO建议区域，分析SEO后，给出具体的数字、来自我的博客文章的统计数据以及警告标志，以显示需要关注的内容。在我生成博客文章后，基于关键字给出修订后的博客文章建议。我会对其进行排名，并解释为什么也提出这个建议。通过使CSS更加高级并添加一个您认为必要的独特功能，使其更具互动性，以帮助博客作者。确保该工具具有视觉上吸引人的UI/UX设计。***\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*OfY0zIa4rzO7J55r)\n\n用户可以创建各种工具，不仅帮助他们进行SEO，还可以在各个领域提高生产力。\n\n[***点击这里尝试该项目***](https://claude.site/artifacts/09f21906-6295-4d0a-9fa0-be4afd6dab71)***.***\n\n### 9\\. 目标检测工具\n\n您还可以使用 Claude 3\\.5 Sonent 基于人工智能和机器学习进行项目开发。您可以尝试通过添加功能来创建令人惊叹的项目。\n\n***提示 - 创建一个用于实时目标检测的单一 HTML 文件网络应用程序。使用 TensorFlow.js 和 COCO-SSD 模型***\n\n***该应用程序应：***\n\n* ***访问用户的网络摄像头并显示视频流。***\n* ***对视频流进行实时目标检测。***\n* ***在检测到的对象周围绘制边界框，并用其类别和检测置信度标记它们。***\n* ***在视频流下方显示唯一检测到的对象列表，显示对象类别及其首次检测时间。***\n* ***确保每个对象类别只列出一次，无论其被检测的频率如何。***\n* ***使用 2 FPS 的检测频率以平衡性能和响应能力。***\n* ***包含对摄像头访问和模型加载的错误处理。***\n* ***为应用程序设计一个干净、现代的外观，并具有响应式设计。***\n* ***在一个自包含的 HTML 文件中包含所有必要的 HTML、CSS 和 JavaScript。***\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*QSvMcxCuulkRKOlm)\n\n## 结论\n\nClaude 3\\.5 Sonnet，Anthropic 最新的 LLM，凭借其先进的功能和多样化的应用，彻底改变了 AI 能力。该模型令人印象深刻的 200K 上下文窗口、卓越的视觉能力以及用于代码生成的创新“Artifacts”功能使其在竞争对手中脱颖而出。本文展示了 Claude 3\\.5 Sonnet 的多种使用案例，证明了它在创建交互式仪表板、可视化、科学工具、游戏、Web 应用程序、3D 模拟、思维导图和 SEO 工具方面的潜力。这些例子突显了该模型在简单提示下生成复杂、功能齐全且视觉吸引人的项目的能力。Claude 3\\.5 Sonnet 友好的用户界面和强大的能力使其成为开发人员、教育工作者和各个领域专业人士的宝贵工具，为 AI 辅助创作和问题解决开辟了新的可能性。\n\n"},{"lang":"zh","group":"blog","slug":"blog/build-a-customer-support-assistant-with-llama3-1-7bf60611e428","frontmatter":{"title":"使用 Llama3.1 创建客户支持助理","meta_title":"使用 Llama3.1 创建客户支持助理","description":"使用 LLM 代理和 Amazon Bedrock 以人工智能解决客户问题：使用 Llama3.1 构建和部署支持助理指南","date":"2024-11-13T01:22:29.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*lNyf72c2_r1wKjnoRA1_FQ.png","categories":["Programming","Chatbots","Technology/Web"],"author":"Rifx.Online","tags":["Llama3.1","AmazonBedrock","Gradio","EC2","CustomerSupport"],"draft":false,"slug":"blog/build-a-customer-support-assistant-with-llama3-1-7bf60611e428"},"content":"\n\n\n### 使用 LLM 代理和 Amazon Bedrock 解决客户查询的 AI：构建和部署支持助手的指南，使用 Llama3\\.1\n\n\n\n## 介绍\n\n### 问题\n\n企业经常面临处理大量客户询问的挑战。这些询问可能从简单的问题“我的订单状态是什么？”到需要人工干预的更复杂的问题不等。重复询问的庞大数量可能会使客户支持团队不堪重负，导致响应时间延长和客户满意度降低。此外，利用人力资源处理简单的例行询问效率低下且成本高昂。迫切需要能够有效处理例行询问的自动化解决方案，以便人类代理可以专注于需要细致问题解决的升级案例。\n\n### 解决方案\n\n大型语言模型（LLM）代理的引入为这个问题提供了一个有前景的解决方案。一个 [LLM 代理](https://proxy.rifx.online/https://research.ibm.com/blog/what-are-ai-agents-llm) 可以通过访问和解释公司数据库中的数据来响应用户查询，处理一些简单的操作，例如检查订单状态、检索账户信息和回答常见问题。通过自动化这些日常任务，LLM 代理确保了更快的解决时间，并释放人力资源以应对更复杂的客户支持场景。在本指南中，我们将探讨如何使用来自 Amazon Bedrock Tools api 的 Llama3\\.1 模型构建一个客户支持助手。\n\n最后，我们将在本地机器上运行助手，并调用一个假数据库：\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Ok9N3mdX50JVWbaJKUrJeQ.gif)\n\n## LLM 代理\n\n### 什么是 LLM 代理\n\n[LLM 代理](https://proxy.rifx.online/https://research.ibm.com/blog/what-are-ai-agents-llm) 是基于大型语言模型如 Llama3.1 构建的专用应用程序，旨在执行特定任务或功能。与根据给定提示生成类人文本的通用 LLM 不同，LLM 代理具备额外的能力，如访问外部数据库、执行操作和根据预定义规则做出决策。它们被定制用于处理特定用例，例如客户支持，在这些场景中，它们可以与用户互动、检索信息并根据对话的上下文执行命令。\n\n虽然通用 LLM 在生成连贯文本和理解语言方面非常强大，但 LLM 代理通过与外部系统集成，进一步拓展了其能力，使其能够执行超出文本生成的现实世界任务。\n\n代理具有一套指令、基础模型、一组可用操作和知识库，使其能够执行复杂任务。\n\n生成模型可以回答一般性问题或与您的文档相关的问题，例如“我看不到我的会议？我该如何预定会议？”。而代理则使用基础模型作为推理逻辑，并结合外部数据源如您的 API，能够返回用户已预定会议的数量，或直接从交互界面安排会议。\n\n“通用目的”类别中有许多代理，还有一些专门用于特定任务的代理，如代码助手（[Amazon CodeWhisperer, Copilot](https://proxy.rifx.online/https://www.missioncloud.com/blog/github-copilot-vs-amazon-codewhisperer)）、写作助手、系统设计（[Amazon Q](https://proxy.rifx.online/https://aws.amazon.com/q/)）、维基百科摘要等。\n\n**AI 代理生态系统：**\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*VuAyzZ2BfrD7o-z0lOpUwA.png)\n\n### 使用 Python 从头创建一个基本代理\n\n让我们使用 Python 从头创建一个简单的 LLM 代理。本文展示了如何在不依赖任何库或框架的情况下构建代理。\n\n## 自定义支持助手\n\n现在，让我们使用来自 [Bedrock](https://proxy.rifx.online/https://aws.amazon.com/bedrock/) 的 [Llama3\\.1](https://proxy.rifx.online/https://llama.meta.com/) 模型创建一个更复杂的客户支持助手。该代理将能够执行更复杂的任务，例如从数据库中查找用户数据和执行简单操作，如查看订单的运输状态。\n\n### 定义能力和边界\n\n在构建我们的助手之前，定义代理可以执行的操作并建立其操作的明确边界至关重要。在生产环境中，这些能力和边界对于确保代理有效且安全地运行至关重要。\n\n**能力：**\n\n* 回复常见客户查询（例如，订单状态、退货政策）。\n* 从数据库中访问和检索用户数据。\n* 执行简单操作，如查看订单状态、更新客户信息等。\n\n**边界：**\n\n* 代理不应执行需要人类判断的操作，例如处理退款或处理升级。\n* 应在定义的范围内操作，除非明确允许，否则不应访问敏感数据。\n* 应为不支持的查询设置错误处理和回退机制。\n\n### 架构\n\n我们解决方案的系统架构涉及多个组件协同工作：\n\n1. **LLM Agent**: 系统的核心，使用 [Llama3\\.1](https://proxy.rifx.online/https://llama.meta.com/) 或 [Claude 3\\.5 Sonnet](https://proxy.rifx.online/https://www.anthropic.com/news/claude-3-5-sonnet) 模型构建，处理自然语言处理和决策制定。\n2. **数据库**: 存储客户数据和其他相关信息，供代理查询。\n3. **API层**: 促进LLM代理与数据库之间的通信，使代理能够检索和操作数据。\n4. **用户界面**: 一个前端界面（例如，聊天机器人界面），客户在此与支持助手互动。\n\n### 代码\n\n在我们检查代码之前，请确保您具备以下条件：\n\n1. 了解 Python 和 [boto3](https://proxy.rifx.online/https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) 库。\n2. 拥有一个启用了模型访问的有效 AWS 账户，在 [Bedrock](https://proxy.rifx.online/https://aws.amazon.com/bedrock/) 中。\n3. 安装了 Python 和 boto3 的 [虚拟环境](https://proxy.rifx.online/https://docs.anaconda.com/miniconda/)。\n\n### 代码演示\n\n\n```python\nfrom datetime import datetime\nimport json\nfrom typing import Any, Dict, List\n\nimport boto3\nfrom botocore.exceptions import ClientError\n\n## Initialize a Boto3 session and create a Bedrock runtime client\nsession = boto3.Session()\nregion = \"us-east-1\" # us-west-2 has better runtime quota\nbedrock_client = session.client(service_name = 'bedrock-runtime', region_name = region)\n```\n首先，我们导入必要的包，并为 `us-east-1` 区域创建一个名为 `bedrock_client` 的 `boto3` Bedrock 运行时客户端实例。如果您的 AWS 账户启用了 `us-west-2` 可用区 (AZ)，请改用该区域。撰写本文时，Llama3\\.1 模型仅在 `us-west-2` AZ 可用，并且与仅支持每分钟 50 次请求的 `us-east-1` AZ 相比，`claude-3.5-sonnet` 模型的运行时配额更大（每分钟 250 次请求）。\n\n\n```python\n## Define available models with their respective request limits\navailable_models = {\n    \"sonnet3-5\": \"anthropic.claude-3-5-sonnet-20240620-v1:0\", # 50 requests per min\n    \"sonnet\": \"anthropic.claude-3-sonnet-20240229-v1:0\", # 500 requests per min\n    \"llama31-70b\": \"meta.llama3-1-70b-instruct-v1:0\", # 400 requests per min\n    \"llama31-405b\": \"meta.llama3-1-405b-instruct-v1:0\", # 50 requests per min\n}\nmodelId = available_models[\"sonnet3-5\"]  # Select model for conversation\n```\n接下来，我们创建 Bedrock 中模型 ID 的映射。**目前并非所有可用于 Amazon Bedrock 的模型都支持工具使用**。请查看 Amazon Bedrock 用户指南中的 [支持的模型列表](https://proxy.rifx.online/https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html#conversation-inference-supported-models-features) [这里](https://proxy.rifx.online/https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html#conversation-inference-supported-models-features)。\n\n\n```python\nclass FakeDatabase:\n    \"\"\"Sample fake database implementation.\"\"\"\n    def __init__(self):\n        self.customers = [\n            {\"id\": \"1213210\", \"name\": \"John Doe\", \"email\": \"john@gmail.com\", \"phone\": \"123-456-7890\", \"username\": \"johndoe\"},\n            {\"id\": \"2837622\", \"name\": \"Priya Patel\", \"email\": \"priya@candy.com\", \"phone\": \"987-654-3210\", \"username\": \"priya123\"},\n            {\"id\": \"3924156\", \"name\": \"Liam Nguyen\", \"email\": \"lnguyen@yahoo.com\", \"phone\": \"555-123-4567\", \"username\": \"liamn\"},\n            {\"id\": \"4782901\", \"name\": \"Aaliyah Davis\", \"email\": \"aaliyahd@hotmail.com\", \"phone\": \"111-222-3333\", \"username\": \"adavis\"},\n            {\"id\": \"5190753\", \"name\": \"Hiroshi Nakamura\", \"email\": \"hiroshi@gmail.com\", \"phone\": \"444-555-6666\", \"username\": \"hiroshin\"},\n            {\"id\": \"6824095\", \"name\": \"Fatima Ahmed\", \"email\": \"fatimaa@outlook.com\", \"phone\": \"777-888-9999\", \"username\": \"fatimaahmed\"},\n            {\"id\": \"7135680\", \"name\": \"Alejandro Rodriguez\", \"email\": \"arodriguez@protonmail.com\", \"phone\": \"222-333-4444\", \"username\": \"alexr\"},\n            {\"id\": \"8259147\", \"name\": \"Megan Anderson\", \"email\": \"megana@gmail.com\", \"phone\": \"666-777-8888\", \"username\": \"manderson\"},\n            {\"id\": \"9603481\", \"name\": \"Kwame Osei\", \"email\": \"kwameo@yahoo.com\", \"phone\": \"999-000-1111\", \"username\": \"kwameo\"},\n            {\"id\": \"1057426\", \"name\": \"Mei Lin\", \"email\": \"meilin@gmail.com\", \"phone\": \"333-444-5555\", \"username\": \"mlin\"}\n        ]\n\n        self.orders = [\n            {\"id\": \"24601\", \"customer_id\": \"1213210\", \"product\": \"Wireless Headphones\", \"quantity\": 1, \"price\": 79.99, \"status\": \"Shipped\"},\n            {\"id\": \"13579\", \"customer_id\": \"1213210\", \"product\": \"Smartphone Case\", \"quantity\": 2, \"price\": 19.99, \"status\": \"Processing\"},\n            {\"id\": \"97531\", \"customer_id\": \"2837622\", \"product\": \"Bluetooth Speaker\", \"quantity\": 1, \"price\": \"49.99\", \"status\": \"Shipped\"}, \n            {\"id\": \"86420\", \"customer_id\": \"3924156\", \"product\": \"Fitness Tracker\", \"quantity\": 1, \"price\": 129.99, \"status\": \"Delivered\"},\n            {\"id\": \"54321\", \"customer_id\": \"4782901\", \"product\": \"Laptop Sleeve\", \"quantity\": 3, \"price\": 24.99, \"status\": \"Shipped\"},\n            {\"id\": \"19283\", \"customer_id\": \"5190753\", \"product\": \"Wireless Mouse\", \"quantity\": 1, \"price\": 34.99, \"status\": \"Processing\"},\n            {\"id\": \"74651\", \"customer_id\": \"6824095\", \"product\": \"Gaming Keyboard\", \"quantity\": 1, \"price\": 89.99, \"status\": \"Delivered\"},\n            {\"id\": \"30298\", \"customer_id\": \"7135680\", \"product\": \"Portable Charger\", \"quantity\": 2, \"price\": 29.99, \"status\": \"Shipped\"},\n            {\"id\": \"47652\", \"customer_id\": \"8259147\", \"product\": \"Smartwatch\", \"quantity\": 1, \"price\": 199.99, \"status\": \"Processing\"},\n            {\"id\": \"61984\", \"customer_id\": \"9603481\", \"product\": \"Noise-Cancelling Headphones\", \"quantity\": 1, \"price\": 149.99, \"status\": \"Shipped\"},\n            {\"id\": \"58243\", \"customer_id\": \"1057426\", \"product\": \"Wireless Earbuds\", \"quantity\": 2, \"price\": 99.99, \"status\": \"Delivered\"},\n            {\"id\": \"90357\", \"customer_id\": \"1213210\", \"product\": \"Smartphone Case\", \"quantity\": 1, \"price\": 19.99, \"status\": \"Shipped\"},\n            {\"id\": \"28164\", \"customer_id\": \"2837622\", \"product\": \"Wireless Headphones\", \"quantity\": 2, \"price\": 79.99, \"status\": \"Processing\"}\n        ]\n\n    def get_user(self, key:str, value:str) -> Dict[str, str]:\n        \"\"\"Return metadata of user.\"\"\"\n        if key in {\"email\", \"phone\", \"username\"}:\n            for customer in self.customers:\n                if customer[key] == value:\n                    return customer\n            return f\"Couldn't find a user with {key} of {value}\"\n        else:\n            raise ValueError(f\"Invalid key: {key}\")\n        \n        return None\n\n    def get_order_by_id(self, order_id: str) -> Dict[str, str]:\n        \"\"\"Return metadata of the order using order id.\"\"\"\n        for order in self.orders:\n            if order[\"id\"] == order_id:\n                return order\n        return None\n    \n    def get_customer_orders(self, customer_id: str) -> List[Dict[str, str]]:\n        \"\"\"Return a list of orders for a specific customer.\"\"\"\n        return [order for order in self.orders if order[\"customer_id\"] == customer_id]\n\n    def cancel_order(self, order_id: str) -> str:\n        \"\"\"Cancel an order if it's in 'Processing' status.\"\"\"\n        order = self.get_order_by_id(order_id)\n        if order:\n            if order[\"status\"] == \"Processing\":\n                order[\"status\"] = \"Cancelled\"\n                return \"Cancelled the order\"\n            else:\n                return \"Order has already shipped.  Can't cancel it.\"\n        return \"Can't find that order!\"\n```\n在本演示中，我们实现了一个模拟数据库类，其中包含预定义的客户及其订单列表。这个模拟数据库类还包括从数据库中检索数据的方法。\n\n* `get_user` : 返回用户\n* `get_order_by_id` : 使用订单 ID 返回订单\n* `get_customer_orders` : 返回特定客户的所有订单\n* `cancel_order` : 如果订单处于“处理中”状态，则取消订单。\n\n\n```python\n## Define all the tools avilable to the model\ntool_config = {\n    \"tools\": [\n        {\n            \"toolSpec\": {\n                \"name\": \"get_user\",\n                \"description\": \"Looks up a user by email, phone, or username.\",\n                \"inputSchema\": {\n                    \"json\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"key\": {\n                                \"type\": \"string\",\n                                \"enum\": [\"email\", \"phone\", \"username\"],\n                                \"description\": \"The attribute to search for a user by (email, phone, or username).\",\n                            },\n                            \"value\": {\n                                \"type\": \"string\",\n                                \"description\": \"The value to match for the specified attribute.\",\n                            },\n                        },\n                        \"required\": [\"key\", \"value\"],\n                    }\n                },\n            }\n        },\n        {\n            \"toolSpec\": {\n                \"name\": \"get_order_by_id\",\n                \"description\": \"Retrieves the details of a specific order based on the order ID. Returns the order ID, product name, quantity, price, and order status.\",\n                \"inputSchema\": {\n                    \"json\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"order_id\": {\n                                \"type\": \"string\",\n                                \"description\": \"The unique identifier for the order.\",\n                            }\n                        },\n                        \"required\": [\"order_id\"],\n                    }\n                },\n            }\n        },\n        {\n            \"toolSpec\": {\n                \"name\": \"get_customer_orders\",\n                \"description\": \"Retrieves the list of orders belonging to a user based on a user's customer id.\",\n                \"inputSchema\": {\n                    \"json\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"customer_id\": {\n                                \"type\": \"string\",\n                                \"description\": \"The customer_id belonging to the user\",\n                            }\n                        },\n                        \"required\": [\"customer_id\"],\n                    }\n                },\n            }\n        },\n        {\n            \"toolSpec\": {\n                \"name\": \"cancel_order\",\n                \"description\": \"Cancels an order based on a provided order_id.  Only orders that are 'processing' can be cancelled\",\n                \"inputSchema\": {\n                    \"json\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"order_id\": {\n                                \"type\": \"string\",\n                                \"description\": \"The order_id pertaining to a particular order\",\n                            }\n                        },\n                        \"required\": [\"order_id\"],\n                    }\n                },\n            }\n        },\n    ],\n    \"toolChoice\": {\"auto\": {}},\n}\n```\n接下来我们定义一个 `tool_config` 。\n\n您可以使用 Amazon Bedrock API 为模型提供访问 [工具](https://proxy.rifx.online/https://docs.aws.amazon.com/bedrock/latest/userguide/tool-use.html)，帮助其生成您发送给模型的消息的响应。例如，您可能有一个聊天应用程序，让用户查找广播电台播放的最受欢迎的歌曲。为了回答有关最受欢迎歌曲的请求，模型需要一个可以查询并返回歌曲信息的工具。\n\n> 与模型一起使用工具也被称为 *函数调用*。\n\n在 Amazon Bedrock 中，模型并不直接调用工具。相反，当您向模型发送消息时，您还提供一个或多个工具的定义，这些工具可能会帮助模型生成响应。在这个例子中，您将提供一个返回客户详情、订单详情或取消订单的工具定义。如果模型确定需要工具来生成消息的响应，模型将回复您请求调用该工具。它还包括要传递给工具的输入参数（所需的客户 ID 或订单 ID）。\n\n在您的代码中，您代表模型调用工具。在这种情况下，假设工具实现是一个 API。工具也可以是数据库、Lambda 函数或其他软件。您决定如何实现工具。然后，您通过提供工具结果的消息与模型继续对话。最后，模型生成一个包含您发送给模型的工具结果的原始消息的响应。\n\n在我们的例子中，我们在 `tool_config` 中定义了我们希望聊天机器人执行的所有功能。有关 ToolConfiguration API 的更多信息，请参阅 [Amazon Bedrock 文档](https://proxy.rifx.online/https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_ToolConfiguration.html)。\n\n```python\ndef process_tool_call(tool_name: str, tool_input: Any) -> Any:\n    \"\"\"Process the tool call based on the tool name and input.\"\"\"\n    if tool_name == \"get_user\":\n        return db.get_user(tool_input[\"key\"], tool_input[\"value\"])\n    elif tool_name == \"get_order_by_id\":\n        return db.get_order_by_id(tool_input[\"order_id\"])\n    elif tool_name == \"get_customer_orders\":\n        return db.get_customer_orders(tool_input[\"customer_id\"])\n    elif tool_name == \"cancel_order\":\n        return db.cancel_order(tool_input[\"order_id\"])\n```\n由于我们的应用程序代码将代表 LLM 调用所需的工具，我们将所有工具打包到一个单一的函数中。`process_tool_call` 函数根据 LLM 提供的 `tool_name` 和 `tool_input` 执行相应的功能。\n\n```python\ndef simple_chat():\n    \"\"\"Main chat function that interacts with the user and the LLM.\"\"\"\n    system_prompt = \"\"\"\n    You are a customer support chat bot for an online retailer called TechNova. \n    Your job is to help users look up their account, orders, and cancel orders.\n    Be helpful and brief in your responses.\n    You have access to a set of tools, but only use them when needed.  \n    If you do not have enough information to use a tool correctly, ask a user follow up questions to get the required inputs.\n    Do not call any of the tools unless you have the required data from a user. \n    \"\"\"\n    # Initial user message\n    user_message = input(\"\\nUser: \")\n    messages = [{\"role\": \"user\", \"content\": [{\"text\": user_message}]}]\n\n    while True:\n        # If the last message is from the assistant, get another input from the user\n        if messages[-1].get(\"role\") == \"assistant\":\n            user_message = input(\"\\nUser: \")\n            messages.append({\"role\": \"user\", \"content\": [{\"text\": user_message}]})\n\n        # Parameters for API request to the Bedrock model\n        converse_api_params = {\n            \"modelId\": modelId,\n            \"system\": [{\"text\": system_prompt}],\n            \"messages\": messages,\n            \"inferenceConfig\": {\"maxTokens\": 4096},\n            \"toolConfig\": tool_config,  # Pass the tool config\n        }\n\n        # Get response from Bedrock model\n        response = bedrock_client.converse(**converse_api_params)\n\n        # Append assistant's message to the conversation\n        messages.append(\n            {\"role\": \"assistant\", \"content\": response[\"output\"][\"message\"][\"content\"]}\n        )\n\n        # If the model wants to use a tool, process the tool call\n        if response[\"stopReason\"] == \"tool_use\":\n            tool_use = response[\"output\"][\"message\"][\"content\"][\n                -1\n            ]  # Naive approach assumes only 1 tool is called at a time\n            tool_id = tool_use[\"toolUse\"][\"toolUseId\"]\n            tool_name = tool_use[\"toolUse\"][\"name\"]\n            tool_input = tool_use[\"toolUse\"][\"input\"]\n\n            print(f\"Claude wants to use the {tool_name} tool\")\n            print(f\"Tool Input:\")\n            print(json.dumps(tool_input, indent=2))\n\n            # Run the underlying tool functionality on the fake database\n            tool_result = process_tool_call(tool_name, tool_input)\n\n            print(f\"\\nTool Result:\")\n            print(json.dumps(tool_result, indent=2))\n\n            # Append tool result message\n            messages.append(\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"toolResult\": {\n                                \"toolUseId\": tool_id,\n                                \"content\": [{\"text\": str(tool_result)}],\n                            }\n                        }\n                    ],\n                }\n            )\n\n        else:\n            # If the model does not want to use a tool, just print the text response\n            print(\n                \"\\nTechNova Support:\"\n                + f\"{response['output']['message']['content'][0]['text']}\"\n            )\n```\n`simple_chat` 函数处理用户交互，调用 LLM，并将工具响应传回 LLM。\n\n该函数中的一个重要行是 `response[\"stopReason\"] == \"tool_use\"`。这决定了 LLM 是否想要使用工具，并在进一步解析时指示 LLM 打算调用哪个工具。\n\n以下是 bedrock-runtime `converse` API 的响应对象示例：\n\n```python\n{\n    'ResponseMetadata': {\n        'RequestId': '07f323a7-cc52-4813-9d1b-83e5c3ae932a', \n        'HTTPStatusCode': 200, \n        'HTTPHeaders': {\n            'date': 'Thu, 08 Aug 2024 10:52:59 GMT', \n            'content-type': 'application/json', \n            'content-length': '519', \n            'connection': 'keep-alive', \n            'x-amzn-requestid': '07f323a7-cc52-4813-9d1b-83e5c3ae932a'\n        }, \n        'RetryAttempts': 0\n    }, \n    'output': {\n        'message': {\n            'role': 'assistant', 'content': [\n                {\n                    'text': \"Certainly! I'll search for search for your orders. Let me use our search tool to find that information for you.\"\n                }, {\n                    'toolUse': {\n                        'toolUseId': 'tooluse_8C_XIwrAROC3t3eEu5FCVw', \n                        'name': 'get_customer_orders', \n                        'input': {'customer_id': '1213210'}\n                    }\n                }\n            ]\n        }\n    }, \n    'stopReason': 'tool_use',\n    'usage': {'inputTokens': 672, 'outputTokens': 103, 'totalTokens': 775}, \n    'metrics': {'latencyMs': 2431}\n}\n```\n有关 Converse API 的更多详细信息，请参阅 [Amazon Bedrock API 参考](https://proxy.rifx.online/https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html)。\n\n一旦我们使用 `process_tool_call` 函数调用所需的工具或功能，我们将函数的响应传回 LLM，以生成最终用户的响应。\n\n请注意，我们正在使用 boto3 Bedrock 运行时客户端的 Converse API。您还可以使用 Converse Stream API 生成流式响应。有关更多详细信息，请参阅 Amazon Bedrock API 参考中的 Converse Stream API 和 Boto3 文档中的 Converse Stream API。\n\n### 在本地终端运行\n\n一旦您正确设置了所有内容，请在虚拟环境中运行 Python 文件，使用：\n\n```python\n## 从虚拟环境内部\npython main.py\n```\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Ok9N3mdX50JVWbaJKUrJeQ.gif)\n\n## 在 EC2 上部署\n\n您可以在 EC2 实例上部署聊天机器人以进行演示，使用 [Gradio](https://proxy.rifx.online/https://www.gradio.app/) 应用程序，它只需几行代码即可提供类似聊天机器人的界面，并与我们的主函数无缝集成。\n\n### Gradio\n\n[Gradio](https://proxy.rifx.online/https://www.gradio.app/) 是一个开源的 Python 库，简化了构建和部署基于网页的机器学习演示的过程。它允许开发者以最少的编码创建直观的网页界面，使得部署和分享模型变得更加容易。\n\n让我们编写一个聊天函数，随机响应 `Yes` 或 `No`，使用 gradio。\n\n这是我们的聊天函数（如果您还没有安装，请在您的虚拟环境中执行 `pip install gradio`）：\n\n```python\nimport random\n\nimport gradio as gr\n\n\ndef random_response(message, history):\n    return random.choice([\"Yes\", \"No\"])\n\ngr.ChatInterface(random_response).launch()\n```\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*XxkUM6yO3lmjN545tRlOvQ.png)\n\n阅读更多关于 [gradio 聊天机器人文档的信息](https://proxy.rifx.online/https://www.gradio.app/main/docs/gradio/chatbot)。\n\n### 在您的 Web 服务器上使用 Nginx 运行 Gradio 应用\n\n让我们在 EC2 上使用 Nginx 部署我们的聊天机器人代理。\n\n**安装 Nginx 并创建新的 conda 环境**\n\n1. **创建一个至少有 2–3 GB 内存的 EC2 实例**。您也可以在 Kubernetes 或 ECS 集群上部署。确保修改 Nginx 配置文件以匹配您的设置。\n\n2\\. **SSH 进入您的 EC2 实例**并 [安装 Nginx](https://proxy.rifx.online/https://devopsden.io/article/how-to-install-nginx-on-ec2-instance):\n\n\n```python\nsudo yum update -y\nsudo amazon-linux-extras install nginx1.12\nsudo systemctl start nginx\nsudo systemctl enable nginx\nsudo systemctl status nginx\n```\n3\\. [**安装 Miniconda**](https://proxy.rifx.online/https://docs.anaconda.com/miniconda/#quick-command-line-install) 以管理 Python 包:\n\n\n```python\nmkdir -p ~/miniconda3\nwget https://proxy.rifx.online/https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh\nbash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3\nrm -rf ~/miniconda3/miniconda.sh\n\n~/miniconda3/bin/conda init bash\n~/miniconda3/bin/conda init zsh\n```\n4\\. **创建一个新的 Conda 环境**，使用 Python 3，并安装 `boto3` 和 `gradio`:\n\n\n```python\nconda create --name gradio-demo python=3.12 pip -y\nconda activate gradio-demo\npip install --no-cache-dir gradio boto3\n```\n5\\. **为您的聊天机器人和 Gradio 代码创建一个新的 Python 文件**。将所有代码复制到此文件中:\n\n\n```python\nvim gradio_demo.py\n```\n或者，您可以使用 `scp` 将文件直接从本地计算机复制到远程实例。\n\n**设置 Nginx**\n\n现在我们将 **设置 Nginx** 以将所有流量从 `/gradio-demo` 路径重定向到由 `gradio_demo.py` 文件启动的本地服务器。请参阅 [此处的官方文档以在 Nginx 上运行 Gradio](https://proxy.rifx.online/https://www.gradio.app/guides/running-gradio-on-your-web-server-with-nginx)。\n\n1. 编辑位于 `/etc/nginx/nginx.conf` 的 Nginx 配置文件:\n\n\n```python\nvim /etc/nginx/nginx.conf\n```\n2\\. 在 `http` 块中，添加以下行以包含来自单独文件的服务器块配置:\n\n\n```python\nserver_names_hash_bucket_size  128;\ninclude /etc/nginx/sites-enabled/*;\n```\n3\\. 在 `/etc/nginx/sites-available` 目录中创建一个新文件（如果该目录不存在则创建），使用一个表示您的应用程序的文件名，例如：`sudo vim /etc/nginx/sites-available/my_gradio_app` :\n\n\n```python\nsudo mkdir -p /etc/nginx/sites-enabled\nsudo vim /etc/nginx/sites-available/my_gradio_app\n```\n在 `my_gradio_app` 文件中粘贴以下内容:\n\n\n```python\nserver {\n    listen 80;\n    server_name www.ec2-12-34-56-78.us-west-2.compute.amazonaws.com; # 将此更改为您的域名\n\n    location /gradio-demo/ {  # 如果您希望在不同路径上提供 Gradio 应用，请更改此处\n        proxy_pass http://127.0.0.1:7860/; # 如果您的 Gradio 应用将在不同端口上运行，请更改此处\n        proxy_buffering off;\n        proxy_redirect off;\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection \"upgrade\";\n        proxy_set_header Host $host;\n        proxy_set_header X-Forwarded-Host $host;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n}\n```\n4\\. 在 `/etc/nginx/sites-enabled` 目录中创建指向此文件的符号链接:\n\n\n```python\nsudo ln -s /etc/nginx/sites-available/my_gradio_app /etc/nginx/sites-enabled/\n```\n5\\. **更新 `gradio_demo.py` 文件** 以在 Gradio 启动 API 中设置根路径:\n\n\n```python\n.launch(root_path=\"/gradio-demo\")\n```\n6\\. **检查 Nginx 配置** 并重启 Nginx:\n\n\n```python\nsudo nginx -t\nsudo systemctl restart nginx\n```\n如果您在 `nginx -t` 命令中遇到错误，请在继续之前解决这些错误。\n\n**在后台运行 `gradio_demo.py` 文件**。您可以使用 `nohup` 或 `tmux`:\n\n\n```python\n## 从 Conda 环境内部\nnohup python gradio_demo.py &\n```\n**访问 EC2 DNS URL** 并附加 `/gradio-demo/` 以查看您的聊天机器人代理在 Gradio 界面上。\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*rcdUROlShsrcaeBDpBKBAQ.png)\n\n## 摘要\n\n在本文中，我们探讨了如何使用 [Llama3\\.1](https://proxy.rifx.online/https://llama.meta.com/) 或 [Claude 3\\.5 Sonnet](https://proxy.rifx.online/https://www.anthropic.com/news/claude-3-5-sonnet) 模型构建客户支持助手。我们首先定义了处理重复客户查询的问题，以及 LLM 代理如何提供解决方案。接着，我们讨论了 LLM 代理的概念以及它们与一般 LLM 的区别。之后，我们演示了如何在 Python 中创建一个基本代理，并使用 Amazon Bedrock 中的模型开发了一个更复杂的客户支持助手。我们还介绍了如何在 EC2 上部署助手，包括使用 Gradio 创建 Web 界面的示例。通过自动化常规客户支持任务，企业可以提高效率，降低成本，并改善客户满意度。\n\n在生产环境中，您可以将登录用户的姓名和 ID 传递给系统提示，以便 LLM 不必向登录用户询问基本信息。某些操作，例如取消订单，可能需要额外的门控。此外，如果客户感到不满或变得激动，应该指示 LLM 将案件升级到人类助手。\n\n您可以通过 LinkedIn 与我联系：<https://proxy.rifx.online/https://linkedin.com/in/maheshrajput>\n\n感谢您的阅读 😊\n\n"},{"lang":"zh","group":"blog","slug":"blog/build-a-rag-based-scientific-chatbot-with-langchain-streamlit-pubmed-part-4-put-it-all-ba7bbf706bbd","frontmatter":{"title":"使用 LangChain、Streamlit 和 PubMed 构建基于 RAG 的科学聊天机器人--第 4 部分（将所有...","meta_title":"使用 LangChain、Streamlit 和 PubMed 构建基于 RAG 的科学聊天机器人--第 4 部分（将所有...","description":"大家好，欢迎来到使用 Langchain、Streamlit 和 PubMed 构建科学聊天机器人系列的最后一部分！","date":"2024-11-13T01:22:29.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MQ7XtBd9WHn5n-gAMgd6pQ.jpeg","categories":["Chatbots","Natural Language Processing","Science"],"author":"Rifx.Online","tags":["ChatBot","LangChain","Streamlit","PubMed","RAG"],"draft":false,"slug":"blog/build-a-rag-based-scientific-chatbot-with-langchain-streamlit-pubmed-part-4-put-it-all-ba7bbf706bbd"},"content":"\n\n\n\n\n您好，欢迎来到构建科学聊天机器人的系列最后一部分，使用Langchain、Streamlit和PubMed！\n\n在前一部分中，我们构建了数据持久性和带有向量存储的RAG管道。现在，是时候将我们所构建的一切整合在一起，创建聊天机器人用户界面，利用我们构建的后端功能，帮助科学家回答他们的科学问题！\n\n作为提醒，这就是我们在系列中构建的完整解决方案：\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*NFCO_uRjlAgm0WYH.png)\n\n## 应用演示\n\n* 作为预告，让我们先来看看应用的界面示例！ \n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*OKEQO_2kwnV93Va4SAVWZg.gif)\n\n## 建设\n\n### 已完成步骤概述\n\n* 如果您还没有完成 [第一部分](https://proxy.rifx.online/https://readmedium.com/build-a-rag-based-scientific-chatbot-with-langchain-streamlit-pubmed-part-1-set-up-streamlit-37550b44b266)、[第二部分](https://proxy.rifx.online/https://readmedium.com/llm-aided-retrieval-of-relevant-scientific-abstracts-via-pubmed-api-using-natural-language-part2-9e10f78575e6) 和 [第三部分](https://proxy.rifx.online/https://readmedium.com/build-a-rag-based-scientific-chatbot-with-langchain-streamlit-pubmed-part-3-create-vector-1e5e401e72e6)，请务必先完成，因为我们将基于这些内容进一步构建。在最后一部分结束时，我们得到了如下的项目结构：\n\n```python\n.\n├── app\n│   ├── app.py\n│   ├── backend\n│   │  ├── abstract_retrieval\n│   │  │   ├── interface.py\n│   │  │   ├── pubmed_retriever.py\n│   │  │   └── pubmed_query_simplification.py\n│   │  ├── data_repository\n│   │  │   ├── interface.py\n│   │  │   ├── local_data_store.py\n│   │  │   └── models.py\n│   │  └── rag_pipeline\n│   │      ├── interface.py\n│   │      ├── chromadb_rag.py\n│   │      └── embeddings.py\n│   ├── components\n│   │   ├── chat_utils.py\n│   │   ├── llm.py\n│   │   └── prompts.py\n│   └── tests\n│       └── test_chat_utils.py\n├── assets\n│   └── pubmed-screener-logo.jpg\n└── environment\n    └── requirements.txt\n```\n在系列的最后一部分中，我们将重点关注定义我们的 Streamlit UI 的代码部分——***app/app.py*** 和 ***app/components*** 模块。\n\n### 修改 chat\\_utils.py 以包含 RAG 逻辑\n\n[在第一部分](https://proxy.rifx.online/https://readmedium.com/build-a-rag-based-scientific-chatbot-with-langchain-streamlit-pubmed-part-1-set-up-streamlit-37550b44b266)，我们构建了一个初步版本的 ***chat\\_utils.py***，其中包含一个简单的 QA 聊天机器人实现（没有 RAG）。现在，我们将深入研究并将其转换为一个上下文感知的 QA 聊天机器人，该机器人将根据用户问题构建答案，并通过相似性搜索从我们的向量索引中检索相关上下文（摘要）。\n\n我们将使用[第三部分](https://proxy.rifx.online/https://readmedium.com/build-a-rag-based-scientific-chatbot-with-langchain-streamlit-pubmed-part-3-create-vector-1e5e401e72e6)中构建的所有后端功能来实现这一目的。\n\n**app/components/chat\\_utils.py**\n\n```python\nfrom typing import List\nimport streamlit as st\nfrom langchain_core.documents.base import Document\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_core.runnables.base import Runnable\nfrom langchain_core.runnables.utils import Output\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain.vectorstores import VectorStore\n\n\nclass ChatAgent:\n    def __init__(self, prompt: ChatPromptTemplate, llm: Runnable):\n        \"\"\"\n        初始化 ChatAgent。\n\n        参数：\n        - prompt (ChatPromptTemplate): 聊天提示模板。\n        - llm (Runnable): 语言模型可运行对象。\n        \"\"\"\n        self.history = StreamlitChatMessageHistory(key=\"chat_history\")\n        self.llm = llm\n        self.prompt = prompt\n        self.chain = self.setup_chain()\n    \n    def reset_history(self) -> None:\n        \"\"\"\n        清除聊天历史以开始新的聊天会话。\n        \"\"\"\n        self.history.clear()\n\n    def setup_chain(self) -> RunnableWithMessageHistory:\n        \"\"\"\n        为 ChatAgent 设置链。\n\n        返回：\n        - RunnableWithMessageHistory: 配置好的带有消息历史的链。\n        \"\"\"\n        chain = self.prompt | self.llm\n        return RunnableWithMessageHistory(\n            chain,\n            lambda session_id: self.history,\n            input_messages_key=\"question\",\n            history_messages_key=\"history\",\n        )\n\n    def display_messages(self, selected_query: str) -> None:\n        \"\"\"\n        在聊天界面显示消息。\n        如果没有消息，则添加默认的 AI 消息。\n        \"\"\"\n        if len(self.history.messages) == 0:\n            self.history.add_ai_message(f\"让我们聊聊你的问题：{selected_query}\")\n        for msg in self.history.messages:\n            st.chat_message(msg.type).write(msg.content)\n    \n    def format_retreieved_abstracts_for_prompt(self, documents: List[Document]) -> str:\n        \"\"\"\n        将检索到的文档格式化为字符串，以便传递给 LLM。\n        \"\"\"\n        formatted_strings = []\n        for doc in documents:\n            formatted_str = f\"摘要标题：{doc.metadata['title']}, 摘要内容：{doc.page_content}, 摘要 DOI：{doc.metadata['source'] if 'source' in doc.metadata.keys() else '缺少 DOI..'}\"\n            formatted_strings.append(formatted_str)\n        return \"; \".join(formatted_strings)\n    \n    def get_answer_from_llm(self, question: str, retrieved_documents: List[Document]) -> Output:\n        \"\"\"\n        根据用户问题和检索到的文档从 LLM 获取响应。\n        \"\"\"\n        config = {\"configurable\": {\"session_id\": \"any\"}}\n        return self.chain.invoke(\n            {\n                \"question\": question, \n                \"retrieved_abstracts\": retrieved_documents,\n            }, config\n        )\n    \n    def retrieve_documents(self, retriever: VectorStore, question: str, cut_off: int = 5) -> List[Document]:\n        \"\"\"\n        使用相似性搜索检索文档\n        cut_off 参数控制检索到的结果数量（默认为 5）\n        \"\"\"\n        return retriever.similarity_search(question)[:cut_off]\n\n    def start_conversation(self, retriever: VectorStore, selected_query: str) -> None:\n        \"\"\"\n        在聊天界面开始对话。\n        显示消息，提示用户输入，并处理 AI 响应。\n        \"\"\"\n        self.display_messages(selected_query)\n        user_question = st.chat_input(placeholder=\"问我任何事情..\")\n        if user_question:\n            documents = self.retrieve_documents(retriever, user_question)\n            retrieved_abstracts = self.format_retreieved_abstracts_for_prompt(documents)\n            st.chat_message(\"human\").write(user_question)\n            response = self.get_answer_from_llm(user_question, retrieved_abstracts)\n            st.chat_message(\"ai\").write(response.content)\n```\n**更改内容：**\n\n* 我们添加了方法 ***retrieve\\_documents***，该方法将我们的向量索引（检索器）作为参数，并调用检索器上的方法 similarity\\_search，从我们的科学摘要的向量索引中获取与用户问题最相似的记录。请注意参数 cut\\_off，它指定要检索的结果数量（默认为 5）。\n* 添加了方法 ***format\\_retreieved\\_abstracts\\_for\\_prompt***，该方法接收通过 retrieve\\_documents 方法检索到的文档，并将其格式化为 LLM 使用。这在我们要求 LLM 在提示中引用相关来源（文章 DOI 和标题）时将非常有用。\n* 添加了方法 ***get\\_answer\\_from\\_llm***，用于调用 LLM 并传递必要的变量，以保持客户端函数 start\\_conversation 的简洁。\n* 修改了 ***start\\_conversation*** 方法以包含 RAG 逻辑。\n\n### 创建 QA 聊天提示\n\n* 我们将修改现有的聊天提示，以包含检索到的摘要，并基于这些摘要构建答案。\n* 我们还将包含一个额外的（简单的）提示，用于在聊天机器人部分之外提供直接的即时答案，以便用户在 UI 上获得直接的答案。\n\n**app/components/chat\\_prompts.py**\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, PromptTemplate\n\n\nchat_prompt_template = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are a knowledgeable expert chatbot in the biomedicine field.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\n            \"human\", \n            \"\"\"\n            Answer the following scientific question: {question}, \n            using the following context retrieved from scientific articles: {retrieved_abstracts}.\n\n            The user might refer to the history of your conversation. Please, use the following history of messages for the context as you see fit.\n\n            The abstracts will come formatted in the following way: ABSTRACT TITLE: <abstract title>; ABSTRACT CONTENT: <abstract content>, ABSTRACT DOI: <abstract doi> (the content inside <> will be variable).\n            In your answer, ALWAYS cite the abstract title and abstract DOI when citing a particular piece of information from that given abstract.\n\n            Your example response might look like this:\n\n            In the article (here in the brackets goes the contents of ABSTRACT_TITLE), it was discussed, that Cannabis hyperemesis syndrome (CHS) is associated with chronic, heavy cannabis use. The endocannabinoid system (ECS) plays a crucial role in the effects of cannabis on end organs and is central to the pathophysiology of CHS. (here, in the end of the cited chunk, the ABSTRACT_DOI goes)\n            \"\"\"\n        ),\n    ]\n)\n\nqa_template = PromptTemplate(\n    input_variables=['question', 'retrieved_abstracts'],\n    template=\"\"\"\n        Answer the following scientific question: {question}, \n        using the following context retrieved from scientific articles: {retrieved_abstracts}.\n\n        The abstracts will come formatted in the following way: ABSTRACT TITLE: <abstract title>; ABSTRACT CONTENT: <abstract content>, ABSTRACT DOI: <abstract doi> (the content inside <> will be variable).\n        In your answer, ALWAYS cite the abstract title and abstract DOI when citing a particular piece of information from that given abstract.\n\n        Your example response might look like this:\n\n        In the article (here in the brackets goes the contents of ABSTRACT_TITLE), it was discussed, that Cannabis hyperemesis syndrome (CHS) is associated with chronic, heavy cannabis use. The endocannabinoid system (ECS) plays a crucial role in the effects of cannabis on end organs and is central to the pathophysiology of CHS. (here, in the end of the cited chunk, the ABSTRACT_DOI goes)\n    \"\"\"\n)\n```\n* 请注意，两个提示的内容几乎相同，但聊天提示包含对聊天历史的引用，使用 MessagesPlaceholder，并指示在对话过程中根据 LLM 的判断使用聊天历史。\n\n### 创建新文件 app/components/layout\\_extensions.py\n\n* 该文件将保存一个辅助函数，该函数将向用户呈现我们应用程序布局的一部分，并提供查询示例（如何使用应用程序的提示）。我决定创建这个扩展文件，以避免使我们的 app.py 文件杂乱，并保持其整洁，因为这段代码相当冗长，并包含一些自定义样式（应用信息将在用户悬停时显示）：\n\n```python\nimport streamlit as st\n\n\ndef render_app_info():\n    st.title(\"PubMed Screener\")\n    st.markdown(\"\"\"\n        PubMed Screener is a ChatGPT & PubMed powered insight generator from biomedical abstracts.\n    \"\"\")\n\n    # Adding custom HTML and CSS for an improved hover-over tooltip\n    st.markdown(\"\"\"\n        <style>\n        .tooltip {\n            position: relative;\n            display: inline-block;\n            border-bottom: 1px dotted black; /* Style for the hoverable text */\n        }\n\n        .tooltip .tooltiptext {\n            visibility: hidden;\n            width: 800px; /* Width to fit content */\n            background-color: #f9f9f9;\n            color: #000;\n            text-align: left;\n            border-radius: 6px;\n            padding: 15px;\n            position: absolute;\n            z-index: 1;\n            bottom: 100;\n            right: -430px; /* Positioning to the right and slightly offset */\n            opacity: 0;\n            transition: opacity 0.5s;\n            box-shadow: 0px 8px 16px 0px rgba(0,0,0,0.8); /* Adding some shadow for better visibility */\n        }\n\n        .tooltip:hover .tooltiptext {\n            visibility: visible;\n            opacity: 1;\n        }\n        </style>\n        <div class=\"tooltip\">🔍 示例问题\n            <span class=\"tooltiptext\">\n                <strong>示例科学问题：</strong>\n                <ul>\n                    <li>如何利用先进的成像技术和生物标志物早期诊断和监测神经退行性疾病的进展？</li>\n                    <li>干细胞技术和再生医学在神经退行性疾病治疗中的潜在应用是什么？相关挑战又是什么？</li>\n                    <li>肠道微生物群和肠脑轴在1型和2型糖尿病发病机制中的作用是什么？如何调节这些相互作用以获得治疗益处？</li>\n                    <li>针对癌症靶向治疗的耐药性发展的分子机制是什么？如何克服这些耐药机制？</li>\n                </ul>\n            </span>\n        </div>\n        \"\"\", unsafe_allow_html=True)\n    \n    st.text(\"\")py\n```\n\n### 修改 app/app.py\n\n* 最后，是时候将我们构建的所有内容整合在一起，并将其作为一个 streamlit 应用程序进行展示！\n\n\n```python\nimport streamlit as st\nfrom metapub import PubMedFetcher\nfrom components.chat_utils import ChatAgent\nfrom components.chat_prompts import chat_prompt_template, qa_template\nfrom components.llm import llm\nfrom components.layout_extensions import render_app_info\nfrom backend.abstract_retrieval.pubmed_retriever import PubMedAbstractRetriever\nfrom backend.data_repository.local_storage import LocalJSONStore\nfrom backend.rag_pipeline.chromadb_rag import ChromaDbRag\nfrom backend.rag_pipeline.embeddings import embeddings\n\n\n## 实例化对象\npubmed_client = PubMedAbstractRetriever(PubMedFetcher())\ndata_repository = LocalJSONStore(storage_folder_path=\"backend/data\")\nrag_client = ChromaDbRag(persist_directory=\"backend/chromadb_storage\", embeddings=embeddings)\nchat_agent = ChatAgent(prompt=chat_prompt_template, llm=llm)\n\ndef main():\n    st.set_page_config(\n        page_title=\"Pubmed 摘要筛选器\",\n        page_icon='💬',\n        layout='wide'\n    )\n\n    # 定义列 - 这将使布局水平分割\n    column_logo, column_app_info, column_answer = st.columns([1, 4, 4])\n\n    # 在第一列放置 logo\n    with column_logo:\n        st.image('../assets/pubmed-screener-logo.jpg')\n\n    # 在第二列放置解释应用程序目的的文本以及用户可能提出的一些示例科学问题。\n    with column_app_info:\n\n        # 运行应用程序信息，包括示例问题作为用户的提示\n        render_app_info()\n\n        # 输入科学问题的部分\n        st.header(\"输入您的科学问题！\")\n        placeholder_text = \"在此输入您的科学问题...\"\n        scientist_question = st.text_input(\"您的问题是什么？\", placeholder_text)\n        get_articles = st.button('获取文章 & 答案')\n\n        # 处理用户问题，获取数据\n        with st.spinner('正在获取摘要。这可能需要一段时间...'):\n            if get_articles:\n                if scientist_question and scientist_question != placeholder_text:\n\n                    # 获取摘要数据\n                    retrieved_abstracts = pubmed_client.get_abstract_data(scientist_question)\n                    if not retrieved_abstracts:\n                        st.write('未找到摘要。')\n                    else:\n                        # 将摘要保存到存储并创建向量索引\n                        query_id = data_repository.save_dataset(retrieved_abstracts, scientist_question)\n                        documents = data_repository.create_document_list(retrieved_abstracts)\n                        rag_client.create_vector_index_for_user_query(documents, query_id)\n                        \n                        # 直接回答用户问题并在 UI 上显示答案\n                        vector_index = rag_client.get_vector_index_by_user_query(query_id)\n                        retrieved_documents = chat_agent.retrieve_documents(vector_index, scientist_question)\n                        chain = qa_template | llm\n                        \n                        with column_answer:\n                            st.markdown(f\"##### 您的问题的答案：'{scientist_question}'\")\n                            st.write(chain.invoke({\n                                \"question\": scientist_question, \n                                \"retrieved_abstracts\": retrieved_documents,\n                            }).content)\n\n    # 聊天机器人部分的开始\n    # 显示查询列表以选择一个进行对话\n    query_options = data_repository.get_list_of_queries()\n\n    if query_options:\n        st.header(\"与摘要聊天\")\n        selected_query = st.selectbox('选择一个过去的查询', options=list(query_options.values()), key='selected_query')\n        \n        # 初始化关于用户问题历史中的某个查询的聊天\n        if selected_query:\n            selected_query_id = next(key for key, val in query_options.items() if val == selected_query)\n            vector_index = rag_client.get_vector_index_by_user_query(selected_query_id)\n\n            # 切换查询进行聊天时清除聊天历史\n            if 'prev_selected_query' in st.session_state and st.session_state.prev_selected_query != selected_query:\n                chat_agent.reset_history()\n\n            st.session_state.prev_selected_query = selected_query\n\n            # 开始聊天会话\n            chat_agent.start_conversation(vector_index, selected_query)\n\n\nif __name__ == \"__main__\":\n    main()\n```\n* 代码包含以下部分：\n1. 实例化我们在系列之前部分中构建的所有对象 → ***PubMedAbstractRetriever***、***LocalJSONStore***、***ChromaDbRag*** 和 ***ChatAgent***。我们将在应用程序代码中使用这些对象。\n2. 定义布局以呈现应用程序标题、logo 和应用程序信息。\n3. 定义用户问题的输入和一个提交按钮。当按钮被点击时，这将触发搜索和获取 PubMed 文章的逻辑（使用 ***PubMedAbstractRetriever —*** pubmed\\_client），将它们保存到本地数据存储库（使用 ***LocalJSONStore —*** data\\_repository），并为它们创建向量索引（使用 ***ChromaDbRag —*** rag\\_client）。\n4. 直接回答用户问题并在 UI 上显示。\n5. 显示聊天机器人部分，让您选择一个过去的查询进行对话，以便进一步询问摘要。在选择过去的查询后，加载相应的向量索引，并启动聊天会话（***chat\\_agent.start\\_conversation(…)***）。现在您可以与摘要聊天！\n\n## 限制\n\n我很高兴你和我一起走过这个系列，我们构建了一个科学聊天机器人的原型！不过需要说明的是，这个应用程序仅仅是一个概念验证（PoC），所展示的实现存在一些需要在生产环境中部署之前解决的问题。\n\n**简单RAG的限制和考虑**\n\n* **检索内容的相关性**：你无法确定检索到的内容（与用户问题最相似的内容）是否是最相关的信息。有一些先进的RAG技术，如*假设性问题*或*层次索引*，可以帮助解决这个问题——在[这篇文章](https://proxy.rifx.online/https://readmedium.com/advanced-rag-techniques-unlocking-the-next-level-040c205b95bc)中了解更多关于这些技术的信息。\n* **检索内容的截断**：很难评估是否检索到了所有相关信息。此外，由于LLM的令牌限制，适应所有上下文到提示中可能会很具挑战性。在我们的案例中，默认的截断等于5个摘要（在我们的ChatAgent的retrieve_documents方法中），如果用户提出一个广泛的问题，这显然可能不够。\n* **适用性有限**：有时，用户的问题可能更倾向于总结性质，而使用不同于RAG的技术可能更适合这个目的。例如，你可以构建一个代理，决定任务是总结/检索，基于用户问题。在此评估之后，将有一个函数执行不同的逻辑，分别进行总结或检索。\n\n**部署架构考虑**\n\n* **运行环境**：在本系列的范围内，我们仅在本地构建了我们的聊天机器人，没有考虑如果我们想要将这个应用程序部署以服务一些真实用户时需要做出的任何架构决策。\n* **同步处理**：由于数据获取可能需要相当长的时间，实现基于队列的异步处理用户请求会更高效，并在数据获取完成后通知用户。以同步方式进行此操作可能会耗费大量时间，这可能导致许多服务器超时。\n* **后端技术**：在我们的案例中，使用的后端是ChromaDB，采用本地存储的JSON文件。对于一个服务用户的部署应用程序，这应该重新评估并选择合适的技术。这可以通过基于应用程序后端代码中的接口定义（*RagWorkflow*和*UserQueryDataStore*接口）轻松实现。\n\n**包括更多科学数据库**\n\n* 在这个系列中，我们仅关注PubMed，但为了提供丰富的上下文基础，可以添加其他科学论文数据库（即Scopus）。这可以通过基于应用程序后端代码中的接口定义（*AbstractRetriever*接口）轻松实现。\n\n## 完整代码库 GitHub 链接\n\n随意分叉该仓库并将其适应您的 UC！\n\n### 链接到 GitHub 仓库 pubmed\\-rag\\-screener\n\n## 摘要\n\n* 在本系列的最后一部分中，我们将之前构建的所有组件组合在一起，创建一个用户界面，让科学家可以提出问题，基于科学摘要获得答案，然后与摘要进行进一步的交流。\n* 应用逻辑是模块化的，便于使用提供的接口进行扩展。\n* 概述并强调了该方法的局限性，并包括了一些构建生产级应用的建议。\n\n> 非常感谢您与我一起完成这个系列！希望您喜欢构建这个令人兴奋的用例 :)\n\n> 如果您想讨论有关开发、数据、人工智能的任何内容，或者只是想联系，请随时与我联系 — [在LinkedIn上联系我](https://proxy.rifx.online/https://www.linkedin.com/in/sbarankova/)\n\n## 系列内容\n\n* [第一部分 — 解释用例，设置带有聊天机器人界面的 Streamlit 应用的第一步。](https://proxy.rifx.online/https://readmedium.com/build-a-rag-based-scientific-chatbot-with-langchain-streamlit-pubmed-part-1-set-up-streamlit-37550b44b266)\n* [第二部分 — 通过 PubMed API 使用自然语言辅助检索相关科学摘要](https://proxy.rifx.online/https://readmedium.com/llm-aided-retrieval-of-relevant-scientific-abstracts-via-pubmed-api-using-natural-language-part2-9e10f78575e6)\n* [第三部分 — 设置后端 — 从检索到的科学摘要创建向量嵌入并将其存储在向量库中](https://proxy.rifx.online/https://readmedium.com/build-a-rag-based-scientific-chatbot-with-langchain-streamlit-pubmed-part-3-create-vector-1e5e401e72e6)\n* **第四部分（本文） — 通过 RAG 将所有内容整合在一起 — 与科学摘要聊天**\n\n"},{"lang":"zh","group":"blog","slug":"blog/building-a-local-ai-powered-news-aggregator-with-ollama-swarm-and-duckduckgo-95aaf8b3ee41","frontmatter":{"title":"使用 Ollama、Swarm 和 DuckDuckGo 构建本地 AI 新闻聚合器","meta_title":"使用 Ollama、Swarm 和 DuckDuckGo 构建本地 AI 新闻聚合器","description":"没有提供字幕","date":"2024-10-24T17:47:43.000Z","image":"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*OHMOTk_WYGOxWHBsKqdpNQ.jpeg","categories":["Programming","Generative AI","Technology/Web"],"author":"Rifx.Online","tags":["Llama","Swarm","DuckDuckGo","News","Aggregator"],"draft":false,"slug":"blog/building-a-local-ai-powered-news-aggregator-with-ollama-swarm-and-duckduckgo-95aaf8b3ee41"},"content":"\n# 使用OllamaSwarm和DuckDuckGo构建本地AI驱动的新闻聚合器\n\n\n\n在当今快节奏的世界中，跟上特定领域最新新闻的步伐可能会很具挑战性。如果我们能够利用生成式AI和代理的力量，创建一个完全在本地机器上运行的个性化新闻聚合器呢？在本文中，我们将探讨如何使用**Ollama**的Llama 3.2模型、**Swarm**进行代理编排，以及**DuckDuckGo**进行网络搜索来构建这样的系统。\n\n# 本地AI的力量\n\n随着大型语言模型的兴起，我们现在能够在个人电脑上运行复杂的AI系统。这为创建针对我们特定需求定制的工具开辟了无限可能。我们的新闻聚合器就是这一潜力的完美例证。\n\n# 我们系统的组成部分\n\n1. **Ollama with Llama 3.2**: 这是我们系统的核心，为我们的AI代理提供动力。\n2. **Swarm**: 一个代理编排框架，允许我们创建和管理多个AI代理。\n3. **DuckDuckGo Search**: 提供最新的网页搜索结果，而不跟踪用户数据。\n\n# 工作原理\n\n我们的新闻聚合器由两个主要的AI代理组成：\n\n1. **新闻助手**：使用DuckDuckGo搜索获取特定主题的最新新闻文章。\n2. **编辑助手**：审查并精炼收集到的新闻以供最终展示。\n\n让我们来分解一下工作流程：\n\n# 1. 设置环境\n\n\n```python\nollama pull llama3.2\n\nexport OPENAI_MODEL_NAME=llama3.2\nexport OPENAI_BASE_URL=http://localhost:11434/v1\nexport OPENAI_API_KEY=any\n\npip install git+https://github.com/openai/swarm.git duckduckgo-search\n```\n我们首先导入必要的库并初始化我们的 Swarm 客户端：\n\n\n```python\nfrom duckduckgo_search import DDGS\nfrom swarm import Swarm, Agent\nfrom datetime import datetime\n\ncurrent_date = datetime.now().strftime(\"%Y-%m\")\nclient = Swarm()\n```\n\n# 2. 创建新闻搜索功能\n\n我们定义一个函数来使用 DuckDuckGo 搜索新闻：\n\n```python\npythondef get_news_articles(topic):\n  ddg_api = DDGS()\n  results = ddg_api.text(f\"{topic} {current_date}\", max_results=5)\n  if results:\n      news_results = \"\\n\\n\".join([f\"Title: {result['title']}\\nURL: {result['href']}\\nDescription: {result['body']}\" for result in results])\n      return news_results\n  else:\n      return f\"Could not find news results for {topic}.\"\n```\n\n# 3. 定义我们的 AI 代理\n\n我们使用 Ollama 的 Llama 3.2 模型创建两个代理：\n\n\n```python\nnews_agent = Agent(\n  model=\"llama3.2\",\n  name=\"News Assistant\",\n  instructions=\"You provide the latest news articles for a given topic using DuckDuckGo search.\",\n  functions=[get_news_articles],\n)\n\neditor_agent = Agent(\n  model=\"llama3.2\",\n  name=\"Editor Assistant\",\n  instructions=\"You review and finalise the news article for publishing.\",\n)\n```\n\n# 4. 协调工作流程\n\n我们定义一个函数来运行我们的新闻聚合工作流程：\n\n```python\ndef run_news_workflow(topic):\n  # Fetch news\n  news_response = client.run(\n      agent=news_agent,\n      messages=[{\"role\": \"user\", \"content\": f\"Get me the news about {topic} on {current_date}\"}],\n  )\n  raw_news = news_response.messages[-1][\"content\"]\n  \n  # Pass news to editor for final review\n  edited_news_response = client.run(\n      agent=editor_agent,\n      messages=[{\"role\": \"system\", \"content\": raw_news}],\n  )\n  print(f\"{edited_news_response.messages[-1]['content']}\")\n```\n\n# 5. 运行系统\n\n最后，我们可以针对任何感兴趣的话题运行我们的新闻聚合器：\n\n\n```python\nrun_news_workflow(\"AI in Drug Discovery\")\n```\n\n# 完整代码 : app.py\n\n\n```python\nfrom duckduckgo_search import DDGS\nfrom swarm import Swarm, Agent\nfrom datetime import datetime\n\ncurrent_date = datetime.now().strftime(\"%Y-%m\")\n\n# 初始化 Swarm 客户端\nclient = Swarm()\n\n# 1. 创建互联网搜索工具\n\ndef get_news_articles(topic):\n    print(f\"正在为 {topic} 进行 DuckDuckGo 新闻搜索...\")\n    \n    # DuckDuckGo 搜索\n    ddg_api = DDGS()\n    results = ddg_api.text(f\"{topic} {current_date}\", max_results=5)\n    if results:\n        news_results = \"\\n\\n\".join([f\"标题: {result['title']}\\n网址: {result['href']}\\n描述: {result['body']}\" for result in results])\n        return news_results\n    else:\n        return f\"未能找到关于 {topic} 的新闻结果。\"\n    \n# 2. 创建 AI 代理\n\ndef transfer_to_editor_assistant(raw_news):\n    print(\"将文章传递给编辑助手...\")\n    return editor_agent.run({\"role\": \"system\", \"content\": raw_news})\n\n# 新闻代理以获取新闻\nnews_agent = Agent(\n    model=\"llama3.2\",\n    name=\"新闻助手\",\n    instructions=\"您提供有关给定主题的最新新闻文章，使用 DuckDuckGo 搜索。\",\n    functions=[get_news_articles],\n)\n\n# 编辑代理以编辑新闻\neditor_agent = Agent(\n    model=\"llama3.2\",\n    name=\"编辑助手\",\n    instructions=\"您审阅并最终确定新闻文章以供发布。\",\n)\n\n# 3. 创建工作流程\n\ndef run_news_workflow(topic):\n    print(\"运行新闻代理工作流程...\")\n    \n    # 第一步: 获取新闻\n    news_response = client.run(\n        agent=news_agent,\n        messages=[{\"role\": \"user\", \"content\": f\"获取关于 {topic} 在 {current_date} 的新闻\"}],\n    )\n    raw_news = news_response.messages[-1][\"content\"]\n    print(f\"获取的新闻: {raw_news}\")\n    \n    # 第二步: 将新闻传递给编辑进行最终审查\n    edited_news_response = client.run(\n        agent=editor_agent,\n        messages=[{\"role\": \"system\", \"content\": raw_news}],\n    )\n    print(f\"{edited_news_response.messages[-1]['content']}\")\n\n\n# 运行给定主题的新闻工作流程示例\nrun_news_workflow(\"药物发现中的 AI\")\n```\n\n# 示例输出\n\n\n```python\nRunning news Agent workflow...\nRunning DuckDuckGo news search for AI in Drug Discovery...\nFetched news: Here's a formatted answer based on the news articles:\n\n**药物发现中的人工智能：革命性的转变**\n\n人工智能（AI）在药物发现中的作用标志着制药领域的革命性转变。AI利用复杂的算法进行自主决策，从数据分析中增强人类能力，而不是取代它们。\n\n**挑战与局限性**\n\n尽管有着令人期待的进展，但在该领域中仍然存在挑战和局限性。论文《AI在药物发现中的作用》探讨了这些问题，强调了高质量数据的必要性、伦理问题的解决以及对基于AI的方法局限性的认识。\n\n**AI在药物发现中的应用**\n\nAI有潜力在药物发现、设计和研究药物间相互作用中发挥关键作用。AI在药物发现中的应用包括：\n\n* 多靶点药理学：AI可以预测化合物对多种疾病的有效性。\n* 化学合成：AI可以优化化学合成过程，以实现更快和更高效的生产。\n* 药物重定位：AI可以识别现有药物的新用途。\n* 预测药物特性：AI可以预测化合物的效力、毒性和物理化学特性。\n\n**药物发现中AI的未来**\n\n随着AI的不断发展，预计将对制药行业产生重大影响。AI的成功应用将依赖于高质量数据的可用性、伦理问题的解决以及对基于AI的方法局限性的认识。\n```\n\n# 本地 AI 新闻聚合的好处\n\n* **隐私**：所有处理都在您的本地机器上进行，确保您的数据留在您自己手中。\n* **定制化**：您可以轻松修改代理的指令或添加新的代理以满足您的特定需求。\n* **最新信息**：通过使用 DuckDuckGo 搜索，您总是能获得关于您选择主题的最新新闻。\n* **AI 驱动的策展**：编辑助手帮助精炼和组织收集的新闻，提供更精致的最终输出。\n\n# 结论\n\n这个本地的人工智能驱动新闻聚合器展示了将大型语言模型与网络搜索能力结合的潜力。通过利用Ollama的Llama 3.2模型、Swarm进行代理编排，以及DuckDuckGo进行搜索，我们创建了一个强大的工具，可以让我们在任何感兴趣的话题上保持信息灵通，同时维护我们的隐私，并完全在本地计算机上运行。\n\n随着人工智能的不断发展，创建个性化、人工智能驱动工具的可能性只会不断扩大。这个新闻聚合器只是一个开始——想象一下，利用这些技术你还可以构建哪些其他创新应用！\n\n# 参考：\n\nSwarm Github : <https://github.com/openai/swarm>\n\n如果您觉得这篇文章信息丰富且有价值，我将非常感谢您的支持：\n\n* 在Medium上为它点赞几次 👏，帮助其他人发现这篇内容（您知道您可以点赞多达50次吗？）。您的点赞将帮助更多读者传播知识。\n- 与您的AI爱好者和专业人士网络分享。\n- 在LinkedIn上与我联系：<https://www.linkedin.com/in/manjunath-janardhan-54a5537/>\n\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/building-autonomous-multi-agent-systems-with-crewai-1a3b3a348271","frontmatter":{"title":"利用 CrewAI 构建自主多代理系统","meta_title":"利用 CrewAI 构建自主多代理系统","description":"本文介绍了如何使用CrewAI和LangChain构建自主多智能体系统。文章首先阐述了多智能体系统的概念，强调代理、工具和任务的协作关系。接着，详细描述了CrewAI框架的优势和项目结构，包括如何创建代理、定义任务和使用工具。通过一个论文写作项目示例，展示了代理如何收集信息、撰写和编辑内容。最后，使用Streamlit框架将应用程序部署，使用户能够与系统进行交互。整体上，文章强调了多智能体系统在提高任务效率和协作方面的潜力。","date":"2024-11-14T03:29:09.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*72Cy_QqOie7G2NAiWr13Kw.jpeg","categories":["Autonomous Systems","Programming","Data Science"],"author":"Rifx.Online","tags":["CrewAI","LangChain","multi-agent","Streamlit","essay-writing"],"draft":false,"slug":"blog/building-autonomous-multi-agent-systems-with-crewai-1a3b3a348271"},"content":"\n### 什么是多智能体自主系统以及如何使用CrewAI和LangChain构建一个？\n\n\n\n## 动机\n\n实际上，我们对这些概念并不陌生；我们从电影中了解到它们。一个人指挥他们的AI，而AI通过使用各种工具来执行这些命令。这就是我们今天在AI系统崛起的道路上所走的方向。时代正在逐渐变化。在过去，人们无法独自完成一项任务，需要一个团队。没有团队，他们要么在一段时间后精疲力竭，要么达到能力的极限。最终，成功的项目来自于由具有不同技能的个人组成的团队。\n\n> 团队合作使梦想成真。\n\n然而，如今一种新技术开始崭露头角。我们可以称之为AGI之前的AI下一个阶段：“代理”。那么，这些代理是什么呢？在深入代码之前，让我们先谈谈多代理系统的结构。\n\n## 它是如何工作的？\n\n简单来说，这个方程式可以表示为：`Multi Agent Systems = AGENTs + TOOLs + TASKs` 这是一个多个代理配备了各种任务和工具的系统。\n\n### 代理\n\n我们熟悉角色扮演游戏，在这些游戏中，你的角色有一个角色，比如战士。例如。在游戏中，你将自己置于他们的位置，旨在通过完成塑造他们背景故事的任务，从一次冒险到下一次冒险来完成游戏。类似地，研究人员发现，当给大型语言模型 (LLMs) 角色、背景故事和目标时，它们可以被激励以最佳方式执行任务。这使我们能够通过几个简单的提示来激励 LLM 执行各种任务。\n\n代理本质上将分配的任务分解为简单的步骤，然后通过“思考”——是的，思考——按顺序执行这些步骤。这使我们能够创建一个不仅能深思熟虑地执行步骤的代理，还能咨询其他具有不同专业领域的代理，而不是依赖单个 LLM 输入提示并接收输出。\n\n### 工具\n\n人类最伟大的能力之一无疑是我们使用工具的技能。这种能力通过进化和文化过程不断演变和发展，使我们能够创造出今天所使用的先进技术。同样，大型语言模型随着训练在更大数据集上的能力也在不断增强。现在，当工具的功能及其使用方式被清晰解释时，这些模型能够在适当条件下自主使用工具，完全自动执行，并根据输出规划下一步，而无需等待进一步的命令。\n\n因此，工具的使用也可以被视为它们进化中最重要的部分之一。尤其是通过互联网浏览工具，代理可以按照指定功能的步骤访问必要的资源，无论是通过网络爬虫还是使用指定网站的搜索引擎。\n\n您工具的功能和目的完全取决于您的想象力。然而，如果您希望将预构建的工具集成到您的代理中，CrewAI 和 LangChain 库都提供了广泛的内置工具供您使用。在这个项目中，我们将重点创建我们自己的自定义工具。\n\n### 任务\n\n就像我们创建代理一样，我们也创建任务，每个任务都需要各种工具。举一个人类行为的例子，当我们需要研究某个事情时，我们会做什么？\n\n1\\- 我们在互联网上搜索。\n\n2\\- 我们进行深入的来源研究。\n\n3\\- 我们对我们的发现进行笔记。\n\n以同样的方式，我们可以设计任务来遵循这些步骤，我们将通过代码讨论它们是如何设计的。\n\n## 什么是 CrewAI？\n\nCrewAI 是一个开源的 Python 框架，用于协调角色扮演的自主 AI 代理，具有 Crew、Task、Agent、Process 等方法，并支持多种 LLM，包括本地模型。\n\n如果我们看看该框架提供的主要优势：\n\n* 基于角色的代理设计。\n* 自主的代理间委派。\n* 灵活的任务管理。\n* 基于流程的执行。\n* 输出保存为 .markdown 文件等格式。\n* 与开源和专有模型（如 OpenAI）兼容。\n\n## 构建多智能体\n\n仅仅通过描述性的解释可能不足以完全理解一个概念，因此让我们创建一个小型的论文写作项目，以更好地掌握多智能体方法。在这个项目中，我们将结合 LangChain 和 CrewAI 框架。要运行该项目，您需要一个 OpenAI API 密钥，您可以通过访问 [https://proxy.rifx.online/https://platform.openai.com/signup](https://proxy.rifx.online/https://platform.openai.com/signup) 来获取。\n\n我们项目的结构由几个不同的 Python 脚本组成：\n\n* `crew.py`，在这里我们定义我们的智能体及其任务。\n* `graph.py`，构建 LangGraph 结构。\n* `extra_tools.py`，包含我们的智能体将使用的工具。\n* `pdf_writer.py`，负责将论文转换为 PDF。\n* `app.py`，为我们的应用程序提供 Streamlit 界面。\n\n```python\n## 项目结构\nAutonomous-Multi-Agent-Systems-with-CrewAI-Essay-Writer\n├── app.py              # 主要的 streamlit 应用程序\n├── crew.py             # CrewAI 智能体和任务处理\n├── extra_tools.py      # 智能体工具的功能\n├── graph.py            # LangGraph 和项目工作流程\n├── pdf_writer.py       # 处理 PDF 输出生成\n├── requirements.txt    # 所需库列表\n├── media\n│   └── cover.jpg       # 项目封面图像\n└── README.md        \n```\n\n该项目所需的库列在 `requirements.txt` 文件中。此外，请确保您已安装 Python 3\\.12 或更高版本。在运行项目之前，请不要忘记安装依赖项。我们使用的库包括：\n\n```python\nlangchain-core\nlangchain-openai\nlanggraph\nstreamlit\nwikipedia\nreportlab\ncrewai[tools]\npysqlite3-binary\nbs4\n```\n\n### 工作流程\n\n在我们的过程中，我们将为代理分配各种角色。例如，当一个代理等待另一个代理完成其在互联网上研究的任务时，另一个代理将独立进行维基百科的研究。一旦两个代理都完成了他们的任务，等待信息的代理将继续进行写作，这是他们被分配的任务。\n\n如果我们要将其可视化：\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Emb37H_8OAKp1s1ChLLVQg.png)\n\n* 用户查询最初发送到路由器。\n* 路由器读取查询并确定用户是想写一篇新文章、编辑之前的文章，还是仅仅传达一个讨论主题。如果用户希望写一篇新文章，请求将转发给小组。\n* 发送到小组的请求首先发送给研究代理。\n* 研究代理使用分配给他的工具搜索与用户想要写的主题相关的互联网资源。\n* 一旦资源收集过程完成，收集到的信息将转发给写作代理。\n* 当写作代理起草文章时，编辑代理进行最终调整，纠正语法错误，并将草稿作为JSON文件返回给LangGraph。\n* JSON文件将发送到将在最终节点创建我们文章的PDF文件的功能。\n\n### 构建 LangGraph\n\n首先，我们需要建立我们架构的框架。一旦我们创建了一个工作流程，使我们能够在需要时与我们的代理进行联系，剩下的就是决定在工作流程的哪些阶段我们将向我们的代理发送请求。为此，我们将首先使用 LangChain 创建一个简单的工作流程。\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*iHQzJymxAstrW40THoRxwA.png)\n\n```python\n#LangGraph workflow\n\nbuilder = StateGraph(GraphState)\n\nbuilder.add_node(\"answer\", self.answer)\nbuilder.add_node(\"write_essay\", self.write_essay)\nbuilder.add_node(\"edit_essay\", self.edit_essay)\n\n\nbuilder.set_conditional_entry_point(self.router_query,\n                              {\"write_essay\": \"write_essay\",\n                                        \"answer\": \"answer\",\n                                        \"edit_essay\": \"edit_essay\"})\nbuilder.add_edge(\"write_essay\", END)\nbuilder.add_edge(\"edit_essay\", END)\nbuilder.add_edge(\"answer\", END)\n\nself.graph = builder.compile()\n```\n\n**路由节点**：正如我们在工作流程描述中提到的，我们的路由器根据传入请求将任务分配给各个节点。为此，我们需要创建一个有效的提示，涵盖用户提供的主题并结合过去的对话。毕竟，我们正在开发一个多代理的作文写作聊天机器人，它可以记住并回忆之前的讨论。\n\n让我们起草一个简单的提示和相应的节点来利用这个提示。在提示中，我们应该使用 Pydantic 库定义一个 `BaseModel`，以确保我们的路由器选择三种潜在响应策略中的一种。这些策略将指导聊天机器人有效地制定其响应。\n\n在节点中，我们将使用 Langchain 的 `PromptTemplate` 方法实现这个提示。然后，我们将调用 LLM（大型语言模型），将用户查询和对话历史一起传入，以确保响应在上下文上相关并符合用户的需求。\n\n1. **定义 Pydantic 模型**：创建一个模型，指定所需的响应策略。\n2. **构建提示**：编写一个清晰概述三种策略的提示。\n3. **设置节点**：使用 Langchain 的 `PromptTemplate` 动态格式化提示。\n4. **调用 LLM**：使用格式化的提示、用户查询和对话历史调用 LLM。\n\n通过遵循这些步骤，我们可以确保聊天机器人准确响应并保持之前互动的上下文。\n\n```python\n#Router Prompt and Router Node\nclass RouteQuery(BaseModel):\n    \"\"\"将用户查询路由到直接回答或研究。\"\"\"\n\n    way: Literal[\"edit_essay\",\"write_essay\", \"answer\"] = Field(\n        ...,\n        description=\"根据用户问题选择将其路由到 write_essay、edit_essay 或 answer\",\n    )\n\nself.router_prompt = \n    \"\"\"\n    你是一个路由器，你的职责是将用户引导到正确的专家。\n    始终检查对话历史，并根据其考虑你的行动。\n    如果主题是关于记忆或日常谈话，将用户引导到回答专家。\n    如果主题以“你能写...”开头，或者用户请求你写一篇文章或论文，将用户引导到写作专家。\n    如果主题是用户想要编辑论文中的任何内容，将用户引导到编辑专家。\n  \n    \\n对话历史: {memory}\n    \\n主题: {topic}\n    \"\"\"\n\ndef router_query(self, state: GraphState):\n    print(\"**ROUTER**\")\n    prompt = PromptTemplate.from_template(self.router_prompt)\n    memory = self.memory.load_memory_variables({})\n\n    router_query = self.model.with_structured_output(RouteQuery)\n    chain = prompt | router_query\n    result:  RouteQuery = chain.invoke({\"topic\": state[\"topic\"],\n                                       \"memory\": memory})\n\n    print(\"Router Result: \", result.way)\n    return result.way\n```\n\n**简单回答节点**：在将我们的路由器作为开始部分的节点后，下一步是创建其他三个节点：`write_essay`、`edit_essay` 和 `answer`。为了采取简单的方式，我们需要编程我们的 `answer` 节点，以便在用户发送随意消息或参与有关论文的对话时直接使用其记忆生成响应。\n\n为此，我们必须首先为此任务编写一个合适的提示。然后，利用这个提示，我们将设计一个简单的节点。让我们继续这个设计。\n\n```python\n#Simple Answer Prompt and Node\n\nself.simple_answer_prompt = \n      \"\"\"\n      你是一个专家，你正在为用户的问题提供简单的 \n      答案。\n    \n      \\n对话历史: {memory}\n      \\n主题: {topic}\n      \"\"\"\ndef answer(self, state: GraphState):\n    print(\"**ANSWER**\")\n    prompt = PromptTemplate.from_template(self.simple_answer_prompt)\n    memory = self.memory.load_memory_variables({})\n    chain = prompt | self.model | StrOutputParser()\n    result = chain.invoke({\"topic\": state[\"topic\"], \"memory\": memory})\n\n    self.memory.save_context(inputs={\"input\": state[\"topic\"]}, outputs={\"output\": result})\n    return {\"response\": result}\n```\n\n**写作节点**：接下来，我们需要设计 `writing_essay` 节点。该节点的目的是使用 CrewAI 的 `kickoff` 方法将用户收到的查询转发给我们的代理，然后将代理返回的 JSON 文件转换为 PDF。自然，我们不需要为这个节点编写提示，因为提示将在代理创建阶段定义。这个节点将仅用于调用代理和利用返回的值。\n\n1. **调用代理**：使用 CrewAI 的 `kickoff` 方法将用户的查询发送给代理。\n2. **处理返回的 JSON**：处理从代理收到的 JSON 响应。\n3. **转换为 PDF**：将 JSON 中的相关数据转换为 PDF 格式。\n\n```python\n#Write Essay Node\ndef write_essay(self, state: GraphState):\n    print(\"**ESSAY COMPLETION**\")\n\n    self.essay = self.crew.kickoff({\"topic\": state[\"topic\"]})\n\n    self.memory.save_context(inputs={\"input\": state[\"topic\"]},\n                           outputs={\"output\": str(self.essay)})\n\n    pdf_name = generate_pdf(self.essay)\n    return {\"response\": \"这是你的论文！\",  \"pdf_name\": f\"{pdf_name}\"}\n```\n\n**编辑论文节点**：让我们简要讨论我们的最后一个节点 `edit_essay`。代码可能看起来有点冗长，因为提示被保留在节点内。如果你愿意，也可以在类定义期间编写提示并将其分配为变量。\n\n当路由器检测到用户的任何论文修改请求时，将激活该节点。在此节点中，我们需要三个重要值：对话历史、用户请求和最近生成的论文。此外，提示中有一个变量，Langchain 将生成，称为 `format_instructions`。这个变量使我们能够向 LLM 传达我们希望保持编辑论文 JSON 格式的结构，并以相同格式接收响应。之后，我们将把返回的响应发送到我们的 PDF 生成工具。\n\n1. **检测编辑请求**：路由器识别用户请求是否为编辑论文。\n2. **收集必要值**：收集对话历史、用户请求和最后生成的论文。\n3. **创建并使用提示**：构建一个包含 `format_instructions` 的提示。\n4. **生成编辑后的论文**：调用 LLM 获取编辑后的论文，并将响应传递给 PDF 生成器。\n\n```python\n#Edit Essay Node\n\ndef edit_essay(self, state: GraphState):\n    print(\"**ESSAY EDIT**\")\n    memory = self.memory.load_memory_variables({})\n\n    user_request = state[\"topic\"]\n    parser = JsonOutputParser(pydantic_object=Essay)\n    prompt = PromptTemplate(\n      template=(\"按照用户请求编辑 JSON 文件，并返回新的 JSON 文件。\"\n                \"\\n请求:{user_request} \"\n                \"\\n对话历史: {memory}\"\n                \"\\n JSON 文件: {essay}\"\n                \" \\n{format_instructions}\"),\n      input_variables=[\"memory\",\"user_request\",\"essay\"],\n      partial_variables={\"format_instructions\": parser.get_format_instructions()},\n  )\n\n    chain = prompt | self.model | parser\n\n    self.essay = chain.invoke({\"user_request\": user_request,\n                               \"memory\": memory, \n                                \"essay\": self.essay})\n\n\n    self.memory.save_context(inputs={\"input\": state[\"topic\"]},\n                             outputs={\"output\": str(self.essay)})\n    pdf_name = generate_pdf(self.essay)\n    return {\"response\": \"这是你的编辑后的论文！\", \n            \"essay\": self.essay, \"pdf_name\": f\"{pdf_name}\"}\n```\n\n## 构建代理\n\n**内容研究员**：为了保持我们的项目简单，我们定义了三个代理，它们将相互通信并进行互联网搜索以撰写文章。让我们设计第一个代理，研究员代理。该代理将对维基百科和其他网站进行网页抓取，收集必要的来源，直到它确定已收集到足够的信息。它将获取与主题相关的主要标题、副标题和文章，并准备摘要。随后，这些文档将被存储，以便发送给写作代理。\n\n在设计这个代理时，我们需要考虑它的角色、背景故事和目标。我们将这些分配给`Agent`类中的参数，类似于构建提示，从而为代理的操作做好准备。\n\n```python\n#Content Researcher Agent and Task\n\nself.researcher = Agent(\n    role=\"Content Researcher\",\n\n    goal=\"Research accurate content on {topic}\",\n\n    backstory=\"You're researching content to write \n                an essay about the topic: {topic}.\"\n              \"You collect information that helps \n                the audience learn something and make informed decisions.\"\n              \"Your work is the basis for the Content Writer to \n                write an article on this topic.\",\n    verbose=True\n)\n\nself.research = Task(\n    description=(\n        \"1. Prioritize the latest trends, key players, \n            and noteworthy news on {topic}.\\n\"\n        \"2. Identify the target audience, considering their \n            interests and pain points.\\n\"\n        \"3. Research a detailed content outline including \n            an introduction, key points, and a conclusion.\\n\"\n        \"4. Include SEO keywords and relevant data or sources.\"\n    ),\n    expected_output=\"A comprehensive document with an outline, \n                    audience analysis, SEO keywords, and resources.\",\n    tools=[search_wikipedia, scrap_webpage],\n    agent=self.researcher,\n)\n```\n\n我们需要创建两个类：`Agent`和`Task`。每个代理可以有一个或多个分配的任务。我们可以直接将工具分配给代理，或者添加特定于任务的工具。通过为任务专门添加工具，我们确保该工具仅在特定任务中使用。\n\n### 参数\n\n我们的 `Agent` 类的参数：\n\n1. **角色**：定义代理在团队中的功能。它决定了代理最适合执行的任务类型，应简短且具有描述性。\n2. **目标**：这是代理旨在实现的个人目标。它指导代理的决策过程，应简短且简单。\n3. **背景故事**：为代理的角色和目标提供背景，丰富互动和协作动态。应尽可能详细。\n4. **详细**：将其设置为 `True` 可配置内部记录器，提供详细的执行日志，有助于调试和监控我们的代理正在进行的活动。\n\n我们的 `Task` 类的参数：\n\n1. **描述**：对任务内容的清晰简洁的陈述。应尽可能详细以确保清晰。\n2. **预期输出**：对任务完成后结果的详细描述，有助于设定对结果的明确期望。\n3. **工具**：代理可以利用来执行任务的功能或能力。在这里，您可以根据需要使用 LangChain、CrewAI 或自定义工具。\n4. **代理**：负责该任务的代理，可以直接分配或通过团队的流程分配。\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ocQ9ZUZwFtGx7a7pPrTbuQ.png)\n\n**内容撰写者**：一旦我们的研究代理通过多次迭代收集了必要的信息，它将把收集到的数据存储在内存中，认为自己已获得足够的知识，并将任务传递给我们的下一个代理，内容撰写者。\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*HD1Bm7twxsUIVGPiqEhHAg.png)\n\n现在，让我们定义我们的内容撰写者代理及其角色。该代理不需要使用任何工具，因此只需详细说明背景故事和描述，类似于研究代理。在背景故事中，我们必须记得指定哪个代理提供了信息来源。\n\n### 参数\n\n1. **角色**：项目中内容编写代理的功能。这应简洁地捕捉代理的作用。\n2. **目标**：内容编写者旨在实现的具体目标，例如根据收集的信息撰写一篇结构良好的文章。\n3. **背景故事**：为内容编写者的角色提供上下文，包括关于研究者代理及其提供的信息的详细信息。精心制作的背景故事可以增强叙述和协作动态。\n4. **描述**：对内容编写者所做工作的清晰简洁的陈述，重点关注其职责和任务。\n5. **预期输出**：对任务完成的详细描述，帮助设定对结果的明确期望。\n6. **上下文**：在执行任务之前，我们指定要等待完成的任务，并从该任务输出中获取必要的信息，结合上下文参数。\n\n```python\n#Content Writer Agent and Task\n\nself.writer = Agent(\n  role=\"Content Writer\",\n\n  goal=\"撰写有关提供主题的深刻且事实准确的 \"\n       \"观点文章\",\n\n  backstory=\"您正在撰写一篇关于提供主题的新观点文章。\"\n            \"您基于内容研究员的工作，该研究员提供了主题的提纲和相关背景信息。\"\n            \"您遵循内容研究员提供的提纲的主要目标和方向。\"\n            \"您还提供客观和公正的见解，并用内容研究员提供的信息进行支持。\",\n  verbose=True,\n)\n\nself.write = Task(\n  description=(\n      \"1. 使用内容撰写一篇引人入胜的文章。\\n\"\n      \"2. 自然地融入SEO关键词。\\n\"\n      \"3. 各部分/副标题以引人入胜的方式命名。\\n\"\n      \"4. 确保文章结构合理，包含引人入胜的引言、深刻的主体和总结性的结论。\\n\"\n      \"5. 校对语法错误并确保与品牌声音一致。\\n\"\n      \"6. 选择合适的标题。\\n\"\n  ),\n  expected_output=\"一篇以markdown格式撰写的文章，\"\n                  \"准备发布，每个部分应有2或3段。\",\n  context=[self.research],\n  agent=self.writer,\n)\n```\n\n**内容编辑器**：在定义了编写代理后，我们本可以结束这个过程；然而，即使编写代理负责写作，它仍可能出现拼写错误和破坏内容连贯性的错误。为防止这些问题并将文章输出为JSON格式，我们将定义一个新的代理：内容编辑器。\n\n在该代理的背景故事中，我们将说明它负责审查和纠正从编写代理收到的文章。在任务阶段，我们还将定义所需的输出格式。\n\n```python\n#Content Editor Agent and Task\nself.editor = Agent(\n    role=\"Content Editor\",\n\n    goal=\"编辑给定的文章，以符合组织的写作风格。\",\n\n    backstory=\"您是一名编辑，收到来自内容编写者的文章。\"\n              \"您的目标是审查文章，以确保其遵循最佳实践，提供平衡的观点\"\n              \"在提供意见或断言时，尽量避免重大争议话题或意见。\",\n    verbose=True\n)\n\nself.edit = Task(\n    description=\"校对给定文章的语法错误，并确保与品牌声音一致。\",\n\n    expected_output=\"一篇以所需格式撰写的文章，\"\n                    \"准备发布，每个部分应有2或3段。\",\n    output_json = Essay,\n    context=[self.write],\n    agent=self.editor\n)\n```\n\n在这里，我们的输出是一个名为`Essay`的对象，它是通过Pydantic库中的`BaseModel`和`Field`类创建的。通过添加我们的代理可以理解的解释，我们确保代理将数据以PDF打印功能所期望的格式输出。\n\n```python\n#Expected Pydantic Output\n\nclass Paragraph(TypedDict):\n    sub_header: str\n    paragraph: str\n\nclass Essay(BaseModel):\n    header: str = Field(..., description=\"文章的标题\")\n    entry: str = Field(..., description=\"文章的引言\")\n    paragraphs: List[Paragraph] = Field(..., description=\"文章的段落\")\n    conclusion: str = Field(..., description=\"文章的结论\")\n    seo_keywords: List[str] = Field(..., description=\"文章的SEO关键词\")\n```\n\n我们已经定义了我们的代理及其任务。现在，让我们将我们的三个代理结合在一起。为此，我们应该使用CrewAI库中的一个小而实用的方法，称为`Crew`。在此方法中，我们列出将顺序操作的代理及其将使用的工具。如果任务需要按顺序执行，如在我们的项目中，我们将`process`参数设置为`Process.sequential`。我们还将`memory`参数设置为`True`，以使代理能够使用短期和长期记忆进行相互通信。\n\n```python\n#Crew Run\n\ndef kickoff(self,*args):\n    return Crew(\n        agents=[self.researcher, self.writer, self.editor],\n        tasks=[self.research, self.write, self.edit],\n        process=Process.sequential,\n        verbose=True,\n        memory=True\n    ).kickoff(*args)\n```\n\n我们的代理结构已经完成，但我们还没有讨论我们的工具。现在，让我们简要说明一下我们的工具。\n\n## 构建工具\n\n工具本质上是接受各种输入并返回值作为输出的函数。我们的代理将简单地提供这些函数所需的输入，并处理他们收到的输出。因此，我们需要以高容错性设计我们的工具。当发生使用错误时，我们的代理应该能够读取错误，并配备信息以便在下一次迭代中正确使用工具。\n\n在为我们的工具准备好函数后，我们应该使用 LangChain 或 CrewAI 的工具创建类将它们转换为工具对象，并附上各种说明。在这里，我们通过简单地在函数顶部写上 C**rewAI 的工具装饰器**将我们的工具转换为代理可以使用的形式。\n\n```python\nfrom crewai_tools import tool\n\n@tool(\"Wikipedia Search Tool\")\ndef search_wikipedia(query: str) -> str:\n    \"\"\"Run Wikipedia search and get page summaries.\"\"\"\n    page_titles = wikipedia.search(query)\n    summaries = []\n\n    for page_title in page_titles[:3]:  # First 3 results\n        try:\n            wiki_page = wikipedia.page(title=page_title, auto_suggest=False)\n            summaries.append(f\"Page: {page_title}\\nSummary: {wiki_page.summary}\")\n        except wikipedia.PageError: # Page Not Found\n            pass\n        except wikipedia.DisambiguationError: # Disambiguation Error\n            pass\n\n    if not summaries:\n        return \"No good Wikipedia Search Result was found\"\n\n    return \"\\n\\n\".join(summaries)\n```\n\n## 构建应用程序\n\n现在，让我们使用我经常使用并且认为提供了简单界面设计的 Streamlit 框架来实时部署我们的应用程序。Streamlit 是一个开源的 Python 框架，供数据科学家和 AI/ML 工程师使用，仅需几行代码即可交付动态数据应用程序。\n\n当用户在 `text_input` 框中输入他们的 OpenAI 密钥并点击“初始化代理”按钮时，我们的应用程序主要激活。当用户通过活动的 `chat_input` 部分发送消息时，以下函数用于将输入的请求传递给我们建立的代理结构：\n\n```python\ndef generate_response(topic):\n    return app.invoke(input={\"topic\": topic})\n```\n\n借助 Streamlit 的 `st.chat_message` 组件，我们可以轻松实现聊天机器人界面。如果用户正在进行常规消息传递，响应将显示正常答案。如果生成了一篇文章，我们将通过编写简单的 if-else 循环向用户提供 PDF 的目录。\n\n同时，我们将从聊天机器人发送和接收的每条消息添加到 Streamlit 的 `session_state` 中创建的 `messages` 变量中。这样，我们就创建了一个可见的聊天屏幕。\n\n```python\n#Streamlit App\n\nimport streamlit as st\nfrom graph import EssayWriter\nimport os\nimport base64\n\nst.set_page_config(page_title=\"Essay Writer Chat Bot\", page_icon=\"🤖\")\nst.image(\"./media/cover.jpg\", use_column_width=True)\n\n\nif \"messages\" not in st.session_state:\n    st.session_state.messages =  [{\"role\": \"assistant\", \"content\": \"Hello!\"}]\n    st.session_state.app = None\n    st.session_state.chat_active = True\n\nwith st.sidebar:\n    st.info(\" * 此应用程序使用 OpenAI API 生成文本，请提供您的 API 密钥。\"\n            \"\\n\\n * 此应用程序使用 'gpt-4o-mini-2024-07-18' 模型。成本有效且高效。\"\n            \"\\n\\n * 如果您没有 API 密钥，可以在 [这里](https://proxy.rifx.online/https://platform.openai.com/signup) 获取。\"\n            \"\\n\\n * 您还可以在 [这里](https://proxy.rifx.online/https://github.com/mesutdmn/Autonomous-Multi-Agent-Systems-with-CrewAI-Essay-Writer) 找到此应用程序的源代码。\"\n            \"\\n\\n * 应用程序密钥不会以任何方式存储或保存。\"\n            \"\\n\\n * 写作论文可能需要一些时间，请耐心等待。大约 1-2 分钟。\"\n    openai_key= st.text_input(\"OpenAI API 密钥\", type=\"password\")\n\n\ndef initialize_agents():\n    os.environ[\"OPENAI_API_KEY\"] = openai_key\n    essay_writer = EssayWriter().graph\n\n    if len(openai_key) < 1:\n        st.error(\"请输入您的 OpenAI API 密钥并初始化代理。\")\n\n        st.session_state.chat_active = True\n    else:\n        st.success(\"代理成功初始化\")\n        st.session_state.chat_active = False\n\n    return essay_writer\n\nwith st.sidebar:\n    if st.button(\"初始化代理\", type=\"primary\"):\n        st.session_state.app = initialize_agents()\n\napp = st.session_state.app\ndef generate_response(topic):\n    return app.invoke(input={\"topic\": topic})\n\n\nfor message in st.session_state.messages:\n    with st.chat_message(message[\"role\"]):\n        st.markdown(message[\"content\"], unsafe_allow_html=True)\n\nif topic:= st.chat_input(placeholder=\"问一个问题\", disabled=st.session_state.chat_active):\n    st.chat_message(\"user\").markdown(topic)\n\n    st.session_state.messages.append({\"role\": \"user\", \"content\": topic})\n    with st.spinner(\"思考中...\"):\n        response = generate_response(topic)\n\n    with st.chat_message(\"assistant\"):\n        if \"pdf_name\" in response:\n            with open(f\"./{response['pdf_name']}\", \"rb\") as file:\n                file_bytes = file.read()\n                b64 = base64.b64encode(file_bytes).decode()\n            href = f'<a href=\"data:application/pdf;base64,{b64}\" download=\"{response['pdf_name']}\">{response['pdf_name']}</a>'\n\n            st.markdown(f\"{response['response']}: {href}\", unsafe_allow_html=True)\n            st.session_state.messages.append({\"role\": \"assistant\", \"content\": f\"{response['response']}: {href}\"})\n        else:\n            st.markdown(response[\"response\"])\n            st.session_state.messages.append({\"role\": \"assistant\", \"content\": response[\"response\"]})\n```\n\n**恭喜**！我们已经完成了我们的项目。如果您愿意，可以观看我为您录制的项目工作日志。不要忘记访问 GitHub [**仓库**](https://proxy.rifx.online/https://github.com/mesutdmn/Autonomous-Multi-Agent-Systems-with-CrewAI-Essay-Writer) 以获取项目的所有代码。\n\n这就是我们应用程序的主页在部署后将呈现的样子！\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8tDAluuAH6njIohDbb-UqA.png)\n\n## 结论\n\n在本文中，我们探讨了如何使用 CrewAI 构建自主多智能体系统。我们首先讨论了创建智能体的动机，以及它们如何协同工作以更高效地完成任务。通过将任务细分并利用工具，我们使我们的智能体能够以结构化的方式执行复杂操作。\n\n我们开发了一个简单的项目，集成了 CrewAI 和 LangChain 框架，展示了多个智能体如何协作收集信息、撰写论文和编辑内容。强调了工具使用和任务管理，以确保我们的智能体能够顺利有效地运行。\n\n最后，我们使用 Streamlit 部署了我们的应用程序，使用户能够轻松与系统互动。\n\n您可以在 [**这里**](https://proxy.rifx.online/https://multi-agent-essay-writer.streamlit.app/) 查看实时项目，在我的 GitHub 仓库 [**这里**](https://proxy.rifx.online/https://github.com/mesutdmn/Autonomous-Multi-Agent-Systems-with-CrewAI-Essay-Writer) 查看源代码\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/building-self-healing-intelligent-test-automation-with-gen-ai-openai-apis-6c39808adb0f","frontmatter":{"title":"利用 Gen AI（OpenAI API）构建智能测试自动化","meta_title":"利用 Gen AI（OpenAI API）构建智能测试自动化","description":"我们都知道，用户界面测试是超级脆弱的。它们会因各种原因而崩溃，其中最大的罪魁祸首之一就是 UI 的更改...","date":"2024-11-13T01:22:29.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kZ4ZR-jqdTTgH3bpOzcgUw.png","categories":["Generative AI","Programming","Testing"],"author":"Rifx.Online","tags":["Generative","OpenAI","Selenium","LLMs","POM"],"draft":false,"slug":"blog/building-self-healing-intelligent-test-automation-with-gen-ai-openai-apis-6c39808adb0f"},"content":"\n\n\n> 我们都知道 UI 测试非常脆弱。它们可能因各种原因而失败，其中一个最大的问题是 UI 定位器的变化。很难想象我们如何能让它们足够智能，以理解定位器何时发生变化，并在测试中出现定位器问题之前防止测试运行。\n\n你没听错！现在是 2024 年，自动化测试工具已经取得了长足的进步。在与这些工具打交道近 18 年后，从 Mercury Winrunner 到 Playwright，我们现在可以利用生成性 AI 的强大功能做一些真正令人惊叹的事情。这就像魔法，但这是真正的科学！\n\n没错，我们现在可以找到一种方法，让我们的测试自动化代码 **更智能**，而不需要自己编写各种模糊的数学算法，这一切都由 LLM 的神来处理。\n\n在这篇文章中，我们将讨论如何以更有效和高效的方式使我们的测试变得智能，但同样，要实现这一点，您需要具备以下前提条件：\n\n1. **Open AI API** 带信用额度（您需要即时购买）\n\n\n\n2\\. C\\# .NET 代码知识，因为我将要涵盖的代码来自 .NET 和 Selenium\n\n3\\. 对测试自动化的基本理解\n\n再次强调，以上所有内容以及以下讨论都是我 [Udemy 课程](https://proxy.rifx.online/https://www.udemy.com/course/generative-ai-in-software-automation-testing/) 的一部分，该课程涵盖了更详细的内容和逐步编写代码的方法。\n\n## 让我们理解问题陈述\n\n我们有一个页面，希望使用 Selenium C# 代码进行自动化。我们使用页面对象模型（POM）模式编写了非常好的代码，一切看起来都很棒，并且完美运行，如下所示。\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GPSBTmPEZBpubI72OElwbw.gif)\n\n我们的开发冠军发现了一个需要调整的 UI 元素。他根据同事的代码审查意见进行了更改，但不幸的是，他删除了我们在自动化测试中使用的定位器。这意味着我们的 POM 代码将不再工作，因为定位器不再存在，这最终导致测试 **失败**。\n\n最重要的是，由于单个定位器的更改，所有测试场景都将因相同的失败而失败。测试并不知道定位器已更改，也没有任何方法知道这一点，因此它总是失败。\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*tINBnScOW78vz8sKb6lWbA.gif)\n\n## 如何解决这个问题？\n\n我相信像我一样，很多人在使用 UI 测试工具时，每天都在经历这个问题，无论是 **Cypress**、**Selenium** 还是 **Playwright**。这个问题总是存在，无论使用什么工具。\n\n现在让我们来理解如何解决上述问题。\n\n我们都知道 **生成式 AI** 和 **大型语言模型**（LLMs）已经远远超出了文本/图像/视频生成的范畴。它们理解给定的上下文，并生成我们所寻找的有意义的信息集。\n\n因此，针对上述问题，我们可以利用生成式 AI 的力量，通过 OpenAI 的 API，将我们的提示请求传递给像 ***GPT 4o*** 或 ***GPT 4 turbo*** 的 LLM，以理解问题陈述并给出有意义的解决方案。\n\n> 那么，我们需要向 OpenAI 的 API 传递什么提示请求，以便在我们的测试自动化中执行操作呢？\n\n好吧，这张图将给你答案。\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*bsCOcyWc0FDnxPp9ApssVw.gif)\n\n我们可以将应用程序的“**实际测试页面**”和 Selenium 测试的“**页面对象模型**”代码作为提示发送给 OpenAI 的 API（附带一些额外的响应解析细节）。这将作为 OpenAI API 的验证过程，以查看定位器是否与给定页面匹配。\n\n根据该操作，我们可以决定测试是否执行。定位器发生了变化，因此继续运行测试是没有意义的。\n\n执行上述操作的代码大致如下：\n\n```python\npublic static async Task<string> VerifyPageLocatorFromAiAsync(string pomFileContent, string htmlPageSource)\n{\n    ChatClient client = new(model: \"gpt-4o-mini\", apiKey);\n    \n    var chatMessage = $\"Verify if locators from this Selenium POM class: {pomFileContent} match this page source: {htmlPageSource}\\\", only return True or False result\";\n\n    ChatCompletion completion = await client.CompleteChatAsync(chatMessage);\n\n    return completion.Content.FirstOrDefault().Text;\n}\n```\n上述代码只是课程中涉及的大量代码的一部分，但你可以看到如何简单地执行将页面与 Selenium 的页面对象模型代码进行分析的操作。\n\n## GenAI在软件测试课程中\n\n*以上讨论只是我在Udemy新课程“[**在软件自动化测试中使用生成性AI**](https://proxy.rifx.online/https://www.udemy.com/course/generative-ai-in-software-automation-testing/)”中讨论主题的一部分*\n\n以下是课程内容\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*lHe_b7qVqUQo-9Y5.png)\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*rMrsbB2IaKPbthdAr9Rc9g.png)\n\n该课程目前在Udemy上以优惠价格提供，作为首发优惠，请在购买课程时使用优惠码**EA\\_NOV\\_24 ⚡️**。\n\n如果优惠码已过期，请随时在此帖子下留言，我会将最新的可用优惠码发送给您。\n\n"},{"lang":"zh","group":"blog","slug":"blog/case-study-turning-doctor-transcripts-into-temporal-medical-record-knowledge-graphs-cf624d4927eb","frontmatter":{"title":"案例研究：将医生笔录转化为时态医疗记录知识图谱","meta_title":"案例研究：将医生笔录转化为时态医疗记录知识图谱","description":"展示数据转换过程、涉及的 25 个开发小时的明细、使用的模式、问题和回复以及创建的图表","date":"2024-11-13T01:22:29.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*DUNtg-0w2z-vlF9SCvt5UA.png","categories":["Health","Data Science","Machine Learning"],"author":"Rifx.Online","tags":["transcripts","Temporal","Knowledge","Graphs","vector"],"draft":false,"slug":"blog/case-study-turning-doctor-transcripts-into-temporal-medical-record-knowledge-graphs-cf624d4927eb"},"content":"\n\n\n您是否有兴趣将医生/患者的医疗记录和记录转化为可以跨多个医疗历史、时间段和患者进行复杂查询的时间性知识图谱？\n\n在本案例研究中，我们展示了如何将医疗记录转化为您可以依赖于 RAG 和分析目的的时间性知识图谱。我们展示了针对该系统的真实问答，以及您可以通过该系统实现的业务成果。据我们所知，这里步骤的组合是一种相对新颖的知识图谱实现。\n\n### 使用的数据\n\n出于数据隐私原因，我们使用了一个合成的医疗记录数据集，该数据集是我们从Synthea数据中创建的，链接在此：[https://synthea.mitre.org/downloads](https://proxy.rifx.online/https://synthea.mitre.org/downloads)。以下是用于知识图谱创建的输入数据之一的医疗记录示例。我们将这些记录数据与Synthea数据中的结构化医疗记录结合在一起。我们有大约75份记录，涵盖了10位患者（即每位患者有5-10份记录）。以下是使用的记录示例：\n\n\n\n## 新颖知识图谱架构概述\n\n### 节点：\n\n我们有5种类型的节点：Patient、Observation、Immunization、Condition和Encounter Type\n\n### Triples (样本列表):\n\nPatient \\-\\> Had Encounter \\-\\> Encounter\n\nPatient \\-\\> Has Condition \\-\\> Condition\n\nPatient \\-\\> Received \\-\\> Immunization\n\nPatient \\-\\> Has Measurement \\-\\> Observation\n\n### Chunks:\n\nChunks 是独立的文本块。Chunks 与每个 Triple 相关联，并且可以有多个 Chunks 关联到单个 Triple。在这种情况下，Chunks 不是 Triple 的非结构化来源，而是与每种 Triple 类型相关的摘要和关键点。因此，我们有 6 种类型的 Chunks：- 患者人口统计 Chunks、病情摘要 Chunks、就诊 Chunks、观察 Chunks、免疫接种 Chunks 和病情详细 Chunks。\n\n不同类型的 Chunks 关联到 Triples 的示例如下：\n\n```python\n1. Patient -> EncounterType\nTriple: (Patient) -[had_encounter]-> (EncounterType)\n- Chunk_ids link to specific visit instances\n- Example Chunk: \"Annual physical on 2024–01–15. BP 120/80, routine screenings \nupdated.\"\n\n2. Patient -> Condition\nTriple: (Patient) -[has_condition]-> (Condition)\n- Chunk_ids link to condition episodes\n- Example Chunk: \"Diagnosed with hypertension on 2020–03–10. Status: active. \nManaged with medication.\"\n\n3. Patient -> Immunization\nTriple: (Patient) -[received]-> (Immunization)\n- Chunk_ids link to administration records\n- Example Chunk: \"Influenza vaccine administered on 2024–01–15.\"\n\n4. Patient -> Observation\nTriple: (Patient) -[has_measurement]-> (Observation)\n- Chunk_ids link to measurement instances\n- Example Chunk: \"2024–01–15: Blood Pressure 120/80 mmHg, Weight 70kg.\"\n```\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*8dH_7tP6xheCaW6K)\n\n**Link to the Graph created: [https://proxy.rifx.online/https://main\\-\\-whyhowai.netlify.app/public/graph/673032011997e08c8849316c](https://proxy.rifx.online/https://main--whyhowai.netlify.app/public/graph/673032011997e08c8849316c)**\n\n通过这种特定的图形架构，您可以将关键点和摘要与 Triples 关联，然后可以专注于通过非结构化搜索找到正确的一组 Triples，并随后通过结构化方式引入所有相关的关键信息。\n\n## 独特的WhyHow架构\n\n有一些独特的WhyHow图形基础设施，使我们能够以简单的方式构建此架构。\n\n首先，Triples通过向量搜索嵌入和检索，避免了常见的检索问题，即必须使用Text2Cypher来识别节点、关系，然后构建Cypher查询，以找到正确的Triple。这已被证明可以显著[提高检索准确性达3倍](https://proxy.rifx.online/https://readmedium.com/knowledge-table-multi-document-rag-extraction-memory-ec08450e858f)。\n\n其次，Triples是WhyHow中的独立对象，您可以将块链接到这些对象。这使您能够提炼每个Triple想要检索的关键信息，并在找到正确的Triples后直接将其引入上下文。这避免了必须以图形格式表示关键的信息和上下文（使得模式构建过程复杂化），并在初始的非结构化向量搜索后以结构化的方式引入信息。这在过程上类似于[LinkedIn对知识图的应用](https://proxy.rifx.online/https://readmedium.com/5-misconceptions-of-kg-rag-systems-building-using-rag-native-graphs-5e47872e7903)，在他们的系统中，像“重现步骤”这样的关键信息以类似的方式表示和检索，而这些步骤本身则被表示为单独的“块”/“节点”。\n\n第三，WhyHow接受JSON格式的数据，这允许任何提取框架与图形创建之间无缝互动。在这种情况下，我们使用Claude将转录数据初步转换为必要的JSON结构，以加载到WhyHow中。如果您已经有信息以JSON格式存在，那么将数据加载到WhyHow中就容易多了。\n\n第四，由于WhyHow系统中块和检索过程的设计方式，您可以轻松包含可以用于管理答案构建方式的时间数据。时间数据在知识图中一直是一个难以建模的内容（以至于领先的KG专家通常建议避免），但它显然是工作流的重要组成部分。即使尝试建模时间数据的现有方法也试图将其摄取到知识图中，然后基于结构化的Cypher查询进行检索，而不是我们独特使用LLM来帮助过滤时间数据的架构。\n\n将LLM的强大功能与知识图等结构化知识表示结合起来，是实现业务成果的重要方式，我们认为这种时间知识图架构将通过成功实施时间数据来帮助释放大量业务价值。\n\n### 数据转换过程\n\n首先，我们使用Claude将转录信息转换为与模式对齐的每个转录的数据信息。结合结构化医疗记录的信息，转录被转化为如下所示的JSON摘要：\n\n```python\nPATIENT SUMMARY\nName: Joseph Crona\nDOB: 2022–08–29\nAge: 2 years\nGender: male\nMRN: #dbfbaa\n\nCURRENT MEASUREMENTS (as of 2024–08–05)\nHeight: 84.1cm (50th percentile)\nWeight: 14.5kg (52nd percentile)\nALLERGIES\nNo known allergies\n\nIMMUNIZATIONS\n- DTaP: 2022–12–05, 2023–02–06, 2023–03–06, 2024–02–05\n- Hepatitis A: 2023–11–06\n- Hepatitis B: 2022–08–29, 2022–10–03, 2023–03–06\n- Hib: 2022–12–05, 2023–02–06, 2023–11–06\n- Influenza: 2023–03–06, 2024–08–05\n- MMR: 2023–11–06\n- PCV13: 2022–12–05, 2023–02–06, 2023–03–06, 2023–11–06\n- Polio: 2022–12–05, 2023–02–06, 2023–03–06\n- Rotavirus: 2022–12–05, 2023–02–06\n- Varicella: 2023–11–06\n\nMEDICAL HISTORY\n- Viral sinusitis (disorder)\nOnset: 2023–03–13\nStatus: resolved\nOutcome: Resolved\n\nGROWTH & DEVELOPMENT\n- 2023–11–06: Body Weight: 12.7 kg\n- 2024–02–05: Body Height: 79 cm\n- 2024–02–05: Body Weight: 13.4 kg\n- 2024–08–05: Body Height: 84.1 cm\n- 2024–08–05: Body Weight: 14.5 kg\nDevelopment: Age-appropriate milestones met\n- Gross motor: Age appropriate\n- Fine motor: Age appropriate\n- Language: Age appropriate\n- Social: Age appropriate\n\nPREVENTIVE CARE\nWell-Child Visits:\n- 2024–08–05: 2yo well visit - Development on track\n- 2024–02–05: 1yo well visit - Development on track\n- 2023–11–06: 1yo well visit - Development on track\n- 2023–08–07: 1yo well visit - Development on track\n- 2023–05–08: 9mo well visit - Age appropriate exam completed\n- 2023–02–06: 6mo well visit - Age appropriate exam completed\n- 2022–12–05: 4mo well visit - Age appropriate exam completed\n- 2022–10–03: 2mo well visit - Age appropriate exam completed\n- 2022–08–29: Newborn visit - Normal exam\n\nFAMILY HISTORY\nMother: Healthy\nFather: Healthy\nSiblings: None documented\n\nSOCIAL HISTORY\nLiving Situation: Lives with parents\nDevelopment: Meeting age-appropriate milestones\nSleep: Age-appropriate pattern\nNutrition: Age-appropriate diet\n```\n其次，我们将此JSON模式映射到WhyHow模式，然后将所有信息导入WhyHow.AI KG Studio。\n\n以下是最终加载到WhyHow中的KG结构示例。\n\n```python\nKnowledge Graph Structure (Timeless):\n\n\nNodes:\n1. Patient Node\n  Structure: {\n      name: str,         # \"John Smith\"\n      label: \"Patient\",\n      properties: {\n          gender: str,   # FHIR gender\n          patient_type: str  # \"adult\" | \"pediatric\"\n      },\n      chunk_ids: List[str]  # Links to demographic chunks\n  }\n\n\n2. EncounterType Node\n  Structure: {\n      name: str,         # \"Well-child visit\" | \"Annual physical\"\n      label: \"EncounterType\",\n      properties: {\n          category: str,  # \"preventive\" | \"acute\" | \"chronic\"\n          specialty: str  # \"primary_care\" | \"pediatrics\" | \"emergency\"\n      },\n      chunk_ids: List[str]  # Links to visit pattern chunks\n  }\n\n\n3. Condition Node\n  Structure: {\n      name: str,         # \"Essential hypertension\"\n      label: \"Condition\",\n      properties: {\n          category: str,     # \"chronic\" | \"acute\" | \"resolved\"\n          system: str,       # \"respiratory\" | \"cardiovascular\" | etc\n          is_primary: bool   # True if primary diagnosis\n      },\n      chunk_ids: List[str]  # Links to condition history chunks\n  }\n\n\n4. Immunization Node\n  Structure: {\n      name: str,         # \"DTaP\" | \"MMR\"\n      label: \"Immunization\",\n      properties: {\n          series: str,       # \"primary\" | \"booster\"\n          target: str        # \"tetanus\" | \"measles\" | etc\n      },\n      chunk_ids: List[str]  # Links to immunization records\n  }\n\n\n5. Observation Node\n  Structure: {\n      name: str,         # \"Blood Pressure\" | \"Height\"\n      label: \"Observation\",\n      properties: {\n          category: str,     # \"vital\" | \"lab\" | \"growth\"\n          unit: str         # \"mmHg\" | \"cm\" | etc\n      },\n      chunk_ids: List[str]  # Links to measurement records\n  }\n\n\nRelations:\n1. Patient -> EncounterType\n  Triple: (Patient) -[had_encounter]-> (EncounterType)\n  - Chunk_ids link to specific visit instances\n\n\n2. Patient -> Condition\n  Triple: (Patient) -[has_condition]-> (Condition)\n  - Chunk_ids link to condition episodes\n\n\n3. Patient -> Immunization\n  Triple: (Patient) -[received]-> (Immunization)\n  - Chunk_ids link to administration records\n\n\n4. Patient -> Observation\n  Triple: (Patient) -[has_measurement]-> (Observation)\n  - Chunk_ids link to measurement instances\n\n\n5. Condition -> EncounterType\n  Triple: (Condition) -[managed_in]-> (EncounterType)\n  - Links conditions to typical encounter types\n\n\n6. Immunization -> EncounterType\n  Triple: (Immunization) -[given_during]-> (EncounterType)\n  - Links vaccines to visit types\n```\n第三，我们运行一个自定义提示，以在每次自然语言查询后对从知识图谱中检索的三元组进行上下文化。\n\n在这种架构下，一个有趣的事情是，我们现在可以继续向知识图谱中添加有关患者就诊、患者治疗和病情的信息，因为这只是将额外的块添加到现有三元组中的问题。如果患者得了新疾病，则会向患者节点添加额外的Condition节点。\n\n这个过程花费了25个开发小时，可以分解为以下几个部分：\n\n* 2小时（8%）用于查看和理解数据（探索性数据分析）\n* 18小时（72%）用于迭代模式，弄清楚图中应该包含哪些节点，哪些节点应该连接到什么，应该存在哪些块，如何连接到各种三元组，使用一组问题测试检索的答案，并相应地进行迭代。\n* 2小时（8%）用于编写创建要加载的三元组的代码\n* 3小时（12%）用于编写验证检查和输出检查以捕获任何错误\n\n### 问题与答案：医疗记录知识图谱\n\n用于在从知识图谱中检索相关上下文后构建答案的提示，使用WhyHow自然语言图查询引擎\n\n```python\n    You are an AI assistant specializing in medical records analysis. \nUse the following information to answer the user's question. \n    The information is derived from a knowledge graph of patient medical records.\n\n    Relevant Nodes (these represent patients, encounters, and conditions):\n    {node_context}\n\n    Relevant Relationships (these show connections between patients, \nencounters, and conditions):\n    {triple_context}\n\n    Relevant Medical Records:\n    {chunk_context}\n\n    Answer the question using the following steps:\n    1. Analyze the medical information, focusing on the specific topic of the \nquestion\n    2. Identify key medical events, conditions, and temporal relationships\n    3. Formulate a clear and concise answer that directly addresses the \nquestion\n    4. If there are any uncertainties in the medical information, mention \nthem explicitly\n    5. If you don't have enough information to answer confidently, say so\n    6. Consider temporal relationships between medical events\n    7. It's currently {datetime.now().strftime(\"%Y-%m-%d\")}. Consider this \nwhen interpreting dates\n    8. Maintain patient privacy by using only first names or initials when \nnecessary\n\n    Question: {question}\n```\n\n## 业务价值：患者治疗分析\n\n### 历史患者分析\n\n问题：**Joseph Crona在其第一年接受了哪些类型的健康儿童访视和免疫接种？**\n\n回答：\n\n```python\nTo determine the types of well-child visits and immunizations Joseph Crona \nreceived in his first year, we need to consider the relevant medical events \nand their timing. Joseph was born in 2022, and it is currently November 9, 2024.\nTherefore, his first year would span from 2022 to 2023.\n\n**Well-Child Visits:**\n- Joseph had an encounter categorized as a \"Well child visit,\" which falls \nunder preventive care in pediatrics. This type of visit is typically part of \nroutine check-ups for children, especially in their first year.\n\n**Immunizations:**\nDuring his first year, Joseph received the following immunizations, all of \nwhich are common for pediatric patients:\n1. **Hep B, adolescent or pediatric** - Completed\n2. **Hib (PRP-OMP)** - Completed\n3. **Rotavirus, monovalent** - Completed\n4. **IPV (Inactivated Poliovirus Vaccine)** - Completed\n5. **DTaP (Diphtheria, Tetanus, and Pertussis)** - Completed\n6. **Pneumococcal conjugate PCV 13** - Completed\n\nThese immunizations are typically administered during the first year of life \nas part of the standard immunization schedule for infants.\n\nIn summary, Joseph Crona received a well-child visit and completed a series of\nstandard pediatric immunizations during his first year.\n```\n问题：**对于中耳炎患者，记录的接触和观察的顺序是什么？**\n\n回答：\n\n```python\nTo address the question regarding the sequence of encounters and observations \nfor patients with otitis media, we will focus on the medical records of Miles \nDickinson, as he is the patient identified with this condition.\n\n1. **Condition Onset:**\n- Miles Dickinson was diagnosed with otitis media on June 10, 2024. The \ncondition is currently resolved.\n\n2. **Encounters:**\n- Miles had two types of encounters recorded:\n- **Well Child Visit:** This is categorized as a preventive encounter in \npediatrics.\n- **Encounter for Symptom:** This is categorized as a follow-up encounter in \nprimary care.\n\n3. **Observations:**\n- Various observations were recorded for Miles, including:\n- **Pain Severity:** Recorded multiple times with scores ranging from 0 to 4.\n- **Body Weight:** Recorded with values ranging from 3.2 kg to 8.2 kg.\n- **Head Occipital-frontal Circumference and Percentile:** Recorded with \nspecific values in cm and percentiles.\n- **Heart Rate:** Recorded with values ranging from 61 to 100 beats per minute.\n- **Respiratory Rate:** Recorded with values ranging from 12 to 16 breaths per\nminute.\n\n4. **Temporal Sequence:**\n- The otitis media diagnosis occurred on June 10, 2024. Observations and \nencounters before this date may have been related to other conditions or \nroutine check-ups. Observations and encounters after this date could be \nrelated to follow-up care for otitis media or other health assessments.\n\n5. **Conclusion:**\n- The sequence of encounters and observations for Miles Dickinson includes \npreventive and follow-up encounters, along with various health observations. \nThe specific sequence related to otitis media is not detailed in terms of exact\ndates for each observation or encounter, but the diagnosis date provides a \ntemporal marker for understanding subsequent medical activities.\n\nIf more detailed temporal data on each observation and encounter were \navailable, a more precise sequence could be established. However, based on \nthe current information, this is the overview of the sequence related to \notitis media for Miles Dickinson.\n```\n\n### 历史多患者分析\n\n问题：**有多少患者被诊断为急性支气管炎，典型的发作模式是什么？**\n\n答案：\n\n```python\n根据提供的医疗记录，有两位患者被诊断为急性支气管炎：Maximo Swaniawski 和 Bennie Herman。\n\n#### 发作模式：\n- **Maximo Swaniawski**：\n- 急性支气管炎的发作日期：\n- 2014年10月24日\n- 2015年11月7日\n- 2018年11月29日\n- **Bennie Herman**：\n- 急性支气管炎的发作日期：\n- 2014年10月24日\n- 2015年11月7日\n- 2018年11月29日\n\n#### 典型发作模式：\n两位患者的发作日期相同，表明急性支气管炎发作的模式发生在10月底或11月。这可能表明一种季节性模式，可能与较冷的天气或其他可能导致呼吸系统疾病的季节性因素有关。\n\n两位患者的病情已得到解决，表明急性支气管炎的发作是急性的，而非慢性的。\n```\n问题：**哪些患者同时患有病毒性鼻窦炎和支气管炎，他们最早的发作日期是什么？**\n\n答案：\n\n```python\n要确定哪些患者同时患有病毒性鼻窦炎和支气管炎，我们需要分析每位患者的相关疾病及其发作日期。\n\n1. **Maximo Swaniawski**：\n- 疾病：急性支气管炎\n- 支气管炎的最早发作日期：2014年10月24日\n2. **Cassandra Monahan**：\n- 疾病：病毒性鼻窦炎\n- 鼻窦炎的最早发作日期：2014年12月12日\n3. **Bennie Herman**：\n- 疾病：急性支气管炎\n- 支气管炎的最早发作日期：2014年10月24日\n4. **Deangelo Price**：\n- 疾病：病毒性鼻窦炎\n- 鼻窦炎的最早发作日期：2014年12月12日\n5. **Joseph Crona**：\n- 疾病：病毒性鼻窦炎\n- 鼻窦炎的最早发作日期：2014年12月12日\n\n根据提供的信息，没有患者同时患有病毒性鼻窦炎和支气管炎。每位患者仅有一种疾病，而没有同时存在两种。因此，没有患者同时具备这两种疾病，因此也没有单一患者的最早发作日期。\n```\n\n### 相较于仅向量 RAG 系统的优势\n\n跨多个患者的卓越分析：\n\n* 在关于哪些患者同时患有支气管炎和病毒性鼻窦炎的问题中，我们必须能够知道某个患者是否被记录为患有或未患有特定疾病。由于仅向量搜索是关于识别相关片段的，因此无法检测到患者是否确实没有某种特定疾病。这意味着需要一个中介数据聚合，能够明确表示患者 X 在其名字下没有“糖尿病”节点。\n\n多转录本分析与 RAG：\n\n* 在关于迈尔斯及其过去一年多次就诊的问题中，我们可以看到这些情况跨越了多次就诊和多个转录本。这意味着需要一个中介数据聚合，能够映射出患者及其随时间累积的就诊和观察记录。\n\nWhyHow.AI 提供结构化知识、知识图谱和更可靠的代理 RAG 解决方案的工具、服务和流程。如果您有兴趣探索我们的任何工具（[KG Studio](https://proxy.rifx.online/https://readmedium.com/whyhow-ai-kg-studio-platform-beta-rag-native-graphs-1105e5a84ff2)，[知识表\\[开源\\]](https://proxy.rifx.online/https://readmedium.com/knowledge-table-multi-document-rag-extraction-memory-ec08450e858f)）和服务，请随时[与我们聊天](https://proxy.rifx.online/https://calendly.com/whyhowai/intro-call-whyhow-ai)。\n\n如果您正在考虑、正在进行或已经在 RAG 中整合知识图谱以提高准确性、记忆和确定性，请关注我们的新闻通讯[WhyHow.AI](https://proxy.rifx.online/https://whyhow.ai/)，或加入我们关于 RAG 中规则、确定性和知识图谱的讨论，欢迎加入我们的[Discord](https://proxy.rifx.online/https://discord.gg/9bWqrsxgHr)。\n\n"},{"lang":"zh","group":"blog","slug":"blog/chatgpt-vision-turns-a-picture-into-1000-words-24858615fa28","frontmatter":{"title":"ChatGPT Vision 将图片转化为 1000 个字","meta_title":"ChatGPT Vision 将图片转化为 1000 个字","description":"以及如何把这些话变成生意","date":"2024-11-10T03:51:17.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*lS5aPVDrsCFFnBYz","categories":["Programming","Marketing","Generative AI"],"author":"Rifx.Online","tags":["automation","content","GPT","MAKE","photos"],"draft":false,"slug":"blog/chatgpt-vision-turns-a-picture-into-1000-words-24858615fa28"},"content":"\n\n\n### 如何将这些文字转化为商业价值\n\n我有这个想法快十年了。一切始于我搭建网站的时候，一位旅馆老板给我发了一根装满近千张照片的U盘，还有一盒35mm的照片让我扫描。\n\n\n\n> 这些都是**惊人的照片**——客人们展示他们的珍贵捕获、令人惊叹的湖景，以及向导们带领的户外冒险。\n\n> 我知道，如果我们能把这些照片放到网上，它们将为旅馆创造一场**口碑**营销的浪潮。\n\n但事情变得复杂了：每张照片都需要一个独特的描述、适当的标签、一篇博客文章和社交媒体的上传。而且几乎有一千张照片！\n\n所需的时间和成本令人震惊。为数百张鱼类照片写说明？这足以让任何人头晕。因此，这个伟大的想法仍然只是一个想法。\n\n快进到今天。现在，借助自动化的力量，我将这个想法变为现实。\n\n我构建了一个自动化内容创作系统，帮助旅馆将旧照片转化为引人入胜的故事，可以轻松在线发布。\n\n在这篇文章中，我将带你了解我构建这个系统的确切步骤，以及你如何做到这一点，以节省时间并保存你旅馆的历史。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*5GBorUl_PfqiLnSW6-Nsjg.png)\n\n## 第一步：收集您旅馆的独特信息\n\n设置此自动化的第一步是收集所有使您的旅馆与众不同的信息和资产。对我来说，这涉及到收集能够使我们的内容脱颖而出的细节。考虑一下您旅馆提供的关键体验，例如钓鱼之旅、本地冒险或独特的设施。这些细节将使您的内容更具个人性和吸引力。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*4QX4oGCYK5djc9EZuE9-EA.png)\n\n接下来，收集旧照片——从过去客人的体验到旅馆周围的自然风光。\n\n在我的实验中，我最初使用了AI生成的图像，但随后我们Facebook页面的一位粉丝发送了一些照片。我请求他允许我使用这些照片，他非常喜欢！\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*PuwLsJ2EuOYOLzXwUvn0cQ.png)\n\n如果您有这样的照片集，请将它们整理到Google Drive文件夹中。这将使您在稍后将它们输入自动化系统时更加方便。除了照片之外，还创建一个Google Spreadsheet，以便记录每张图像的详细信息。您的电子表格应包括：\n\n* 图像URL（来自您的Google Drive文件夹）\n* 旅馆详情（例如钓鱼向导、特别活动）\n* 任何相关的故事或描述，可以与照片一起使用\n\n这看起来可能是额外的工作，但对于帮助自动化系统稍后创建有意义的内容至关重要。\n\n## 第2步：构建自动化蓝图\n\n一旦您收集了所有资产，就可以开始设置自动化蓝图。我使用了一个名为 [MAKE](https://www.make.com/en/register?pc=saleprice) 的自动化平台。如果您以前从未接触过自动化，请不要担心——这比听起来要简单得多。\n\n……您可以在我们的7天试用中**免费**获取我所有经过验证的蓝图 [free](https://whop.com/ai-businessplans)。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ArzS71K2fJf4EfCg5y3BPQ.png)\n\n首先，复制一个现有的自动化模板，比如一个发布天气更新到社交媒体的模板。我有一个天气自动化，所以我以此为基础，去掉了不必要的部分。您希望有一个干净的工作环境，因此请删除任何不适用于您小屋的模块，例如天气更新或您不会使用的额外社交媒体渠道。\n\n现在，是时候让自动化更具体地适应您的小屋了。\n\n## 第3步：为您的小屋定制自动化\n\n在基本结构建立之后，通过添加特定于小屋的内容来定制自动化。这就是您在第1步中收集的细节发挥作用的地方。输入您的小屋描述，添加过去冒险的故事，并融入当地提示。确保包含有助于您的内容在网上被注意到的关键词。\n\n接下来，配置自动化以从您的Google Drive文件夹中提取照片，并将它们与电子表格中的相应描述匹配。这确保了正确的图像与正确的故事配对。\n\n这里是魔法发生的地方：我将GPT（一个语言模型AI）集成到自动化中。GPT分析每张照片，并根据您提供的细节生成独特的内容。\n\n例如，如果照片显示一位客人钓到了一条大鱼，GPT可以创建一篇关于该特定经历的帖子，包括关于钓鱼向导、鱼的种类，甚至是对未来游客的建议。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*wySL0TxoNTFICejPsJOOcA.png)\n\n## 第4步：自动发布到社交媒体和Medium\n\n一旦内容生成，就可以开始自动发布过程。我连接了我们的Medium账户，以便GPT生成的文章可以直接上传为草稿，准备审核。Medium是一个非常适合长格式内容的平台，比如博客文章或详细的客座故事。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*56OzNVMIxw9sJKBn1jkriQ.png)\n\n对于较短的内容，比如社交媒体帖子，自动化链接到我们的Facebook和Twitter账户。系统旨在从较长的文章中创建片段，非常适合快速的社交媒体更新。您还可以配置自动化以自动发布或安排特定时间的帖子。\n\n这个系统的美妙之处在于，一旦内容获得批准，自动化就会处理从发布到安排的所有事务。这是一个无需手动干预的解决方案，可以保持您的在线存在活跃，而无需持续关注。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*rym300CevwmoA4_pvYybqQ.png)\n\n## 第5步：测试和微调\n\n现在自动化已经到位，在上线之前进行测试是很重要的。我进行了几次测试发布，以确保一切按预期工作。自动化拉取了正确的照片，生成了引人入胜的内容，并顺利地发布到Medium和社交媒体上。\n\n在测试过程中，我进行了小的调整，例如修改标题或优化GPT提示，以确保内容与我们旅馆的语气和故事完美契合。\n\n一旦一切都调整到位，我就可以创建源源不断的新内容，保持我们的受众参与。\n\n## 一个实际项目的测试\n\n为了给您一个实际的例子，请查看 [iFish Canada Facebook 页面](https://facebook.com/ifishcanada)。\n\n该项目演示了自动化在现实生活中的工作方式。\n\n系统从我们的关注者那里获取他们在加拿大钓鱼旅行中提交的照片，通过 GPT 处理，并生成独特的帖子，展示照片及其作者的经历——在我们的例子中，这些帖子与我们虚构的旅馆的传说相结合。\n\n内容丰富、引人入胜，最重要的是——自动化。\n\n曾经看似不可能的任务现在成为现实，节省了数百个小时的时间，并让我们能够分享那些让我们的钓鱼照片贡献者感到被认可和欣赏的故事。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Pe-hBjGCN1qbkjLl6cmTmQ.png)\n\n## 第6步：监控、调整和成长\n\n即使系统可以自动运行，监控结果并随着时间进行调整仍然很重要。\n\n我会查看哪些帖子获得了最多的互动，并调整GPT提示以改善未来的内容。这种持续的微调确保我们的在线存在保持新鲜，并继续吸引新访客。\n\n想象一下，您如何能够保护您的小屋遗产，分享那些可能会被遗忘的回忆，并以一种现代而强大的方式生成口碑营销。\n\n## 十年的努力 — 现在你可以加入我\n\n经过多年的梦想，自动化这个过程终于成为现实。系统已经投入使用，结果将不言而喻。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*135_nP0nL6NrT_O7JxIk4g.png)\n\n现在，我想邀请你体验一下这个系统，适用于你的旅馆或度假村。如果你曾经感到被市场营销的时间或成本压倒，或者在不断需求新内容的情况下感到挣扎，[这个自动化系统](https://whop.com/ai-businessplans)可能就是你一直在寻找的解决方案。\n\n访问 [iFish Canada](https://facebook.com/ifishcanada) 观看系统的实际运行，如果你想开始自动化自己的内容，请随时联系我。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*IJ-R1362_IWUrZ-3KyG8mw.png)\n\n我很乐意帮助你释放自动化的力量，来发展你的旅馆，分享你的故事，吸引新客人——同时节省你的时间和精力。\n\n⫷\n\n### 我重视您的评论\n\n我会回复所有评论，并**作为我的感谢**，我还会在适当的地方关注、点赞、突出和评论您的内容。所以请留下您的想法、问题或成功故事！我喜欢阅读它们！\n\n*连接* 在 [YouTube](https://www.youtube.com/channel/UCphdP_nguu6MT3U5tsJNMsQ)、[X (twitter)](https://x.com/Aibusinessplans/status/1803488217079095460) 和 [Linkedin](https://www.linkedin.com/company/ai-businessplans/) — 尝试我们的 [Community](https://whop.com/ai-businessplans)。\n\n保持安全，每天迈出小步伐。 \n\nDoug\n\n## 阅读下一步 \\-\n\n🛆 *投资免责声明：* 在充分利用免费功能之前，您不应将资金投入付费工具。*我们培训产品中的任何内容都不是收益的承诺或保证。*👈\n\n☄ 本文包含我最喜欢的内容创作者和GenAI爱好者的AI商业工具的推荐链接。\n\n如果您购买我最喜欢的软件和AI工具，我将会获得一小笔佣金，而您无需支付额外费用。\n\n"},{"lang":"zh","group":"blog","slug":"blog/choosing-between-llm-agent-frameworks-69019493b259","frontmatter":{"title":"在 LLM 代理框架之间进行选择","meta_title":"在 LLM 代理框架之间进行选择","description":"构建定制基于代码的代理和主要代理框架之间的权衡。","date":"2024-10-29T12:57:34.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*jRMs19HqSCazE5dY","categories":["Programming","Technology","Machine Learning"],"author":"Rifx.Online","tags":["agents","frameworks","LangGraph","LlamaIndex","Workflows"],"draft":false,"slug":"blog/choosing-between-llm-agent-frameworks-69019493b259"},"content":"\n### 定制代码代理与主要代理框架之间的权衡\n\n\n\n代理正在迎来一个重要时刻。随着多个新框架和新的 [投资](https://foundationcapital.com/goodbye-aiops-welcome-agentsres-the-next-100b-opportunity/) 的涌入，现代 AI 代理正在克服 [不稳定的起源](https://arxiv.org/html/2405.13966v1)，迅速取代 RAG 成为实施优先事项。那么，2024 年是否终于会成为能够接管撰写电子邮件、预订航班、与我们的数据对话或似乎任何其他任务的自主 AI 系统的年份？\n\n也许，但要达到这一点还有很多工作要做。任何构建代理的开发者不仅需要选择基础——使用哪个模型、用例和架构——还需要选择利用哪个框架。你是选择长期存在的 LangGraph，还是较新的 LlamaIndex Workflows？或者你选择传统方式，自己编写整个代码？\n\n本文旨在让这个选择变得简单一些。在过去几周，我在主要框架中构建了相同的代理，以技术层面检查每个框架的一些优缺点。每个代理的所有代码都可以在 [这个库](https://github.com/Arize-ai/phoenix/tree/main/examples/agent_framework_comparison) 中找到。\n\n### 测试用代理的背景\n\n用于测试的代理包括功能调用、多种工具或技能、与外部资源的连接，以及共享状态或记忆。\n\n该代理具有以下能力：\n\n1. 从知识库回答问题\n2. 与数据对话：回答有关LLM应用程序的遥测数据的问题\n3. 数据分析：分析检索到的遥测数据中的更高级趋势和模式\n\n为了实现这些，代理具有三项初始技能：基于产品文档的RAG、在跟踪数据库上生成SQL，以及数据分析。代理用户界面使用简单的gradio支持的界面，代理本身构建为一个聊天机器人。\n\n## 基于代码的代理（无框架）\n\n开发代理时，您可以选择完全跳过框架，自己构建代理。开始这个项目时，我采用的就是这种方法。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*pw9-0lB5JMlVcPqo)\n\n### 纯代码架构\n\n下面的基于代码的代理由一个由OpenAI驱动的路由器组成，该路由器使用函数调用选择合适的技能。该技能完成后，它会返回路由器，以便调用另一个技能或响应用户。\n\n代理保持一个持续更新的消息和响应列表，在每次调用时完整传递给路由器，以保持上下文的连贯性。\n\n```python\ndef router(messages):\n    if not any(\n        isinstance(message, dict) and message.get(\"role\") == \"system\" for message in messages\n    ):\n        system_prompt = {\"role\": \"system\", \"content\": SYSTEM_PROMPT}\n        messages.append(system_prompt)\n\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=messages,\n        tools=skill_map.get_combined_function_description_for_openai(),\n    )\n\n    messages.append(response.choices[0].message)\n    tool_calls = response.choices[0].message.tool_calls\n    if tool_calls:\n        handle_tool_calls(tool_calls, messages)\n        return router(messages)\n    else:\n        return response.choices[0].message.content\n```\n\n技能本身在各自的类中定义（例如，GenerateSQLQuery），这些类共同保存在一个SkillMap中。路由器本身只与SkillMap交互，使用它来加载技能名称、描述和可调用函数。这种方法意味着将新技能添加到代理中只需将该技能编写为自己的类，然后将其添加到SkillMap中的技能列表中。这里的想法是使添加新技能变得简单，而不干扰路由器代码。\n\n```python\nclass SkillMap:\n    def __init__(self):\n        skills = [AnalyzeData(), GenerateSQLQuery()]\n\n        self.skill_map = {}\n        for skill in skills:\n            self.skill_map[skill.get_function_name()] = (\n                skill.get_function_dict(),\n                skill.get_function_callable(),\n            )\n\n    def get_function_callable_by_name(self, skill_name) -> Callable:\n        return self.skill_map[skill_name][1]\n\n    def get_combined_function_description_for_openai(self):\n        combined_dict = []\n        for _, (function_dict, _) in self.skill_map.items():\n            combined_dict.append(function_dict)\n        return combined_dict\n\n    def get_function_list(self):\n        return list(self.skill_map.keys())\n\n    def get_list_of_function_callables(self):\n        return [skill[1] for skill in self.skill_map.values()]\n\n    def get_function_description_by_name(self, skill_name):\n        return str(self.skill_map[skill_name][0][\"function\"])\n```\n\n总体而言，这种方法相对简单易行，但也面临一些挑战。\n\n### 纯代码代理的挑战\n\n第一个难点在于构建路由器系统提示。通常，上述示例中的路由器坚持自己生成 SQL，而不是将其委托给合适的技能。如果你曾经尝试让 LLM *不* 做某件事，你就会知道这种体验是多么令人沮丧；找到一个有效的提示需要经过多轮调试。考虑到每个步骤的不同输出格式也是棘手的。由于我选择不使用结构化输出，我必须准备好应对路由器和技能中每个 LLM 调用的多种不同格式。\n\n### 纯代码代理的好处\n\n基于代码的方法提供了良好的基准和起点，是学习代理工作原理的绝佳方式，而无需依赖现有框架中的现成代理教程。尽管说服 LLM 按预期行为可能具有挑战性，但代码结构本身足够简单，可能适用于某些用例（更多内容见下面的分析部分）。\n\n## LangGraph\n\nLangGraph 是最早的代理框架之一，首次发布于 2024 年 1 月。该框架旨在通过采用 Pregel 图结构来解决现有管道和链的无环特性。LangGraph 通过添加节点、边和条件边的概念，使您更容易在代理中定义循环，以遍历图形。LangGraph 建立在 LangChain 之上，并使用该框架中的对象和类型。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*fYgHiGwLhSUSrFv9)\n\n### LangGraph 架构\n\nLangGraph 代理在表面上看起来与基于代码的代理相似，但其背后的代码却截然不同。LangGraph 在技术上仍然使用“路由器”，即通过函数调用 OpenAI，并使用响应继续到新的步骤。然而，程序在技能之间的移动方式完全不同。\n\n```python\ntools = [generate_and_run_sql_query, data_analyzer]\nmodel = ChatOpenAI(model=\"gpt-4o\", temperature=0).bind_tools(tools)\n\ndef create_agent_graph():\n    workflow = StateGraph(MessagesState)\n\n    tool_node = ToolNode(tools)\n    workflow.add_node(\"agent\", call_model)\n    workflow.add_node(\"tools\", tool_node)\n\n    workflow.add_edge(START, \"agent\")\n    workflow.add_conditional_edges(\n        \"agent\",\n        should_continue,\n    )\n    workflow.add_edge(\"tools\", \"agent\")\n\n    checkpointer = MemorySaver()\n    app = workflow.compile(checkpointer=checkpointer)\n    return app\n```\n\n这里定义的图有一个用于初始 OpenAI 调用的节点，称为上面的“agent”，以及一个用于工具处理步骤的节点，称为“tools”。LangGraph 有一个内置对象 ToolNode，它接受一个可调用工具的列表，并根据 ChatMessage 响应触发它们，然后再返回到“agent”节点。\n\n```python\ndef should_continue(state: MessagesState):\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    if last_message.tool_calls:\n        return \"tools\"\n    return END\n\ndef call_model(state: MessagesState):\n    messages = state[\"messages\"]\n    response = model.invoke(messages)\n    return {\"messages\": [response]}\n```\n\n在每次调用“agent”节点后（换句话说：基于代码的代理中的路由器），should_continue 边决定是将响应返回给用户，还是传递给 ToolNode 以处理工具调用。\n\n在每个节点中，“state”存储了来自 OpenAI 的消息和响应列表，类似于基于代码的代理的方法。\n\n### LangGraph 的挑战\n\n大多数与 LangGraph 相关的困难源于需要使用 Langchain 对象，以便流程顺畅。\n\n**挑战 \\#1：函数调用验证**\n\n为了使用 ToolNode 对象，我不得不重构我现有的大部分 Skill 代码。ToolNode 接受一个可调用函数的列表，这最初让我认为可以使用我现有的函数，但由于我的函数参数，事情却出现了问题。\n\n这些技能被定义为具有可调用成员函数的类，这意味着它们的第一个参数是“self”。GPT\\-4o 足够智能，未在生成的函数调用中包含“self”参数，然而 LangGraph 将其视为缺少参数的验证错误。\n\n这花了我几个小时才弄明白，因为错误信息却将函数中的第三个参数（数据分析技能中的“args”）标记为缺失参数：\n\n```python\npydantic.v1.error_wrappers.ValidationError: 1 validation error for data_analysis_toolSchema\nargs field required (type=value_error.missing)\n```\n\n值得一提的是，错误消息源自 Pydantic，而不是 LangGraph。\n\n我最终下定决心，将我的技能重新定义为使用 Langchain 的 @tool 装饰器的基本方法，并成功使其工作。\n\n```python\n@tool\ndef generate_and_run_sql_query(query: str):\n    \"\"\"根据提示生成并运行 SQL 查询。\n\n    参数：\n        query (str): 包含原始用户提示的字符串。\n\n    返回：\n        str: SQL 查询的结果。\n    \"\"\"\n```\n\n**挑战 \\#2：调试**\n\n如前所述，在框架中进行调试是困难的。这主要归结为令人困惑的错误消息和抽象概念，使得查看变量变得更加困难。\n\n抽象概念主要在尝试调试在代理中传递的消息时出现。LangGraph 将这些消息存储在 state\\[“messages”] 中。图中的某些节点会自动从这些消息中提取，这可能使得在节点访问消息时理解消息的值变得困难。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*KuCg0WGHSklOKe6t)\n\n### LangGraph 的好处\n\nLangGraph 的主要好处之一是易于使用。图结构代码简洁且易于访问。特别是当您有复杂的节点逻辑时，拥有图的单一视图使理解代理之间的连接变得更加容易。LangGraph 还使将现有的基于 LangChain 构建的应用程序转换变得简单。\n\n### 外卖\n\n如果您使用框架中的所有内容，LangGraph 将运行良好；如果您超出它的范围，请准备好进行一些调试。\n\n## LlamaIndex 工作流\n\n工作流是代理框架领域的新进入者，早在今年夏天首次亮相。与 LangGraph 类似，它旨在简化循环代理的构建。工作流还特别关注异步运行。\n\n工作流的一些元素似乎是对 LangGraph 的直接回应，特别是它使用事件而不是边和条件边。工作流使用步骤（类似于 LangGraph 中的节点）来容纳逻辑，并通过发出和接收事件在步骤之间移动。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*22WuFVBWctdeiCSL)\n\n上面的结构看起来与 LangGraph 结构相似，只是增加了一项内容。我在工作流中添加了一个设置步骤，以准备代理上下文，更多内容请见下文。尽管结构相似，但支撑它的代码却截然不同。\n\n### 工作流架构\n\n下面的代码定义了工作流结构。与 LangGraph 类似，这里是我准备状态并将技能附加到 LLM 对象的地方。\n\n```python\nclass AgentFlow(Workflow):\n    def __init__(self, llm, timeout=300):\n        super().__init__(timeout=timeout)\n        self.llm = llm\n        self.memory = ChatMemoryBuffer(token_limit=1000).from_defaults(llm=llm)\n        self.tools = []\n        for func in skill_map.get_function_list():\n            self.tools.append(\n                FunctionTool(\n                    skill_map.get_function_callable_by_name(func),\n                    metadata=ToolMetadata(\n                        name=func, description=skill_map.get_function_description_by_name(func)\n                    ),\n                )\n            )\n\n    @step\n    async def prepare_agent(self, ev: StartEvent) -> RouterInputEvent:\n        user_input = ev.input\n        user_msg = ChatMessage(role=\"user\", content=user_input)\n        self.memory.put(user_msg)\n\n        chat_history = self.memory.get()\n        return RouterInputEvent(input=chat_history)\n```\n\n这也是我定义额外步骤“prepare\\_agent”的地方。此步骤从用户输入创建一个 ChatMessage，并将其添加到工作流内存中。将其分离为单独步骤意味着我们在代理循环通过步骤时会返回到它，这避免了重复将用户消息添加到内存中。\n\n在 LangGraph 的情况下，我通过一个位于图外的 run\\_agent 方法完成了同样的事情。然而，这一变化主要是风格上的，然而在我看来，将此逻辑与工作流和图形结合在一起更为简洁。\n\n设置好工作流后，我接着定义了路由代码：\n\n```python\n@step\nasync def router(self, ev: RouterInputEvent) -> ToolCallEvent | StopEvent:\n    messages = ev.input\n\n    if not any(\n        isinstance(message, dict) and message.get(\"role\") == \"system\" for message in messages\n    ):\n        system_prompt = ChatMessage(role=\"system\", content=SYSTEM_PROMPT)\n        messages.insert(0, system_prompt)\n\n    with using_prompt_template(template=SYSTEM_PROMPT, version=\"v0.1\"):\n        response = await self.llm.achat_with_tools(\n            model=\"gpt-4o\",\n            messages=messages,\n            tools=self.tools,\n        )\n\n    self.memory.put(response.message)\n\n    tool_calls = self.llm.get_tool_calls_from_response(response, error_on_no_tool_call=False)\n    if tool_calls:\n        return ToolCallEvent(tool_calls=tool_calls)\n    else:\n        return StopEvent(result=response.message.content)\n```\n\n以及工具调用处理代码：\n\n```python\n@step\nasync def tool_call_handler(self, ev: ToolCallEvent) -> RouterInputEvent:\n    tool_calls = ev.tool_calls\n\n    for tool_call in tool_calls:\n        function_name = tool_call.tool_name\n        arguments = tool_call.tool_kwargs\n        if \"input\" in arguments:\n            arguments[\"prompt\"] = arguments.pop(\"input\")\n\n        try:\n            function_callable = skill_map.get_function_callable_by_name(function_name)\n        except KeyError:\n            function_result = \"Error: Unknown function call\"\n\n        function_result = function_callable(arguments)\n        message = ChatMessage(\n            role=\"tool\",\n            content=function_result,\n            additional_kwargs={\"tool_call_id\": tool_call.tool_id},\n        )\n\n        self.memory.put(message)\n\n    return RouterInputEvent(input=self.memory.get())\n```\n\n这两者看起来更像是基于代码的代理，而不是 LangGraph 代理。这主要是因为工作流将条件路由逻辑保留在步骤中，而不是在条件边中——第 18 到 24 行在 LangGraph 中是一个条件边，而现在它们只是路由步骤的一部分——以及 LangGraph 具有一个 ToolNode 对象，几乎自动处理 tool\\_call\\_handler 方法中的所有内容。\n\n在路由步骤之后，我非常高兴地看到我可以将我的 SkillMap 和基于代码的代理中的现有技能与工作流一起使用。这些技能无需更改就可以与工作流配合使用，这让我的工作轻松了很多。\n\n### 工作流的挑战\n\n**挑战 \\#1: 同步与异步**\n\n尽管异步执行对于实时代理更为理想，但调试同步代理要容易得多。工作流设计为异步工作，强行实现同步执行非常困难。\n\n我最初以为只需去掉“async”方法标识，将“achat\\_with\\_tools”切换为“chat\\_with\\_tools”即可。然而，由于Workflow类中的底层方法也被标记为异步，因此有必要重新定义这些方法以便实现同步执行。最终我还是坚持使用异步方法，但这并没有使调试变得更加困难。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*78Hzqkiv9cI7W4UA)\n\n**挑战 \\#2: Pydantic 验证错误**\n\n在与LangGraph的困境重演中，关于技能的Pydantic验证错误出现了类似的问题。幸运的是，由于工作流能够很好地处理成员函数，这次解决起来要容易得多。我最终不得不在为我的技能创建LlamaIndex FunctionTool对象时更加规范：\n\n```python\nfor func in skill_map.get_function_list(): \n            self.tools.append(FunctionTool(\n                skill_map.get_function_callable_by_name(func), \n                metadata=ToolMetadata(name=func, description=skill_map.get_function_description_by_name(func))))\n```\n\n*摘自 AgentFlow.\\_\\_init\\_\\_，用于构建 FunctionTools*\n\n### 工作流的好处\n\n构建 Workflows 代理比构建 LangGraph 代理要容易得多，主要是因为 Workflows 仍然要求我自己编写路由逻辑和工具处理代码，而不是提供内置函数。这也意味着我的 Workflow 代理看起来与我的基于代码的代理极为相似。\n\n最大的区别在于事件的使用。我使用了两个自定义事件在我的代理中移动步骤：\n\n```python\nclass ToolCallEvent(Event):\n    tool_calls: list[ToolSelection]\n\nclass RouterInputEvent(Event):\n    input: list[ChatMessage]\n```\n\n发射器-接收器、基于事件的架构取代了直接调用我的代理中的某些方法，比如工具调用处理器。\n\n如果您有更复杂的系统，具有多个异步触发的步骤并可能发出多个事件，这种架构将非常有助于干净地管理这些情况。\n\nWorkflows 的其他好处包括它非常轻量且不强迫您使用很多结构（除了某些 LlamaIndex 对象的使用），而且它的基于事件的架构为直接函数调用提供了一个有用的替代方案——特别是对于复杂的异步应用程序。\n\n## 比较框架\n\n在这三种方法中，各自都有其优点。\n\n无框架的方法是最简单的实现方式。因为任何抽象都是由开发者定义的（即上面示例中的 SkillMap 对象），保持各种类型和对象的清晰是很容易的。然而，代码的可读性和可访问性完全取决于个别开发者，随着代理的复杂性增加，如果没有一些强制结构，很容易变得混乱。\n\nLangGraph 提供了相当多的结构，这使得代理的定义非常明确。如果一个更广泛的团队在协作开发代理，这种结构将提供一种强有力的架构强制方式。对于那些不太熟悉该结构的人，LangGraph 也可能为代理提供一个良好的起点。然而，这也有一个权衡——由于 LangGraph 为你做了很多事情，如果你没有完全接受这个框架，可能会导致麻烦；代码可能非常干净，但你可能会为此付出更多的调试成本。\n\nWorkflows 则处于中间位置。基于事件的架构可能对某些项目极为有用，而使用 LlamaIndex 类型的要求较少，为那些没有在整个应用程序中完全使用框架的人提供了更大的灵活性。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*PITmiVGuG8QuDVX6)\n\n最终，核心问题可能只是“你是否已经在使用 LlamaIndex 或 LangChain 来协调你的应用程序？”LangGraph 和 Workflows 都与各自的基础框架紧密相连，因此每个特定于代理的框架的额外好处可能不足以单凭优点而促使你切换。\n\n纯代码的方法可能始终是一个有吸引力的选项。如果你有严谨的方法来记录和强制执行任何创建的抽象，那么确保外部框架不会拖慢你的速度是很容易的。\n\n## 选择代理框架的关键问题\n\n当然，“这要看情况”从来不是一个令人满意的答案。这三个问题应该帮助你决定在下一个代理项目中使用哪个框架。\n\n***你是否已经在项目的重要部分使用了 LlamaIndex 或 LangChain？***\n\n如果是，请首先探索这个选项。\n\n***你是否熟悉常见的代理结构，还是希望有一些指导来告诉你如何构建代理？***\n\n如果你属于后者，尝试 Workflows。如果你*真的*属于后者，尝试 LangGraph。\n\n***你的代理之前是否已经构建过？***\n\n框架的一个好处是每个框架都有许多教程和示例可供使用。而纯代码代理的示例则少得多。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*wF9aSF1db1yaniqO)\n\n## 结论\n\n选择一个代理框架只是众多选择中的一个，这将影响生成式AI系统的生产结果。像往常一样，建立稳健的保护措施和 [LLM tracing](https://docs.arize.com/phoenix/tracing/llm-traces) 是非常重要的——并且要灵活应对新的代理框架、研究和模型颠覆既定技术。\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/claude-3-5-haiku-anthropics-speed-demon-gets-a-brain-boost-82f2f0999d4f","frontmatter":{"title":"Claude 3.5 Haiku：人类的 速度之魔 脑力大增","meta_title":"Claude 3.5 Haiku：人类的 速度之魔 脑力大增","description":"Claude 3.5 Haiku是Anthropic推出的最新AI模型，具备高速和卓越智能，超越了前任Claude 3 Opus。其编程能力在SWE-bench测试中表现出色，支持文本输入，未来将添加图像分析功能。尽管价格上涨四倍，但通过提示缓存和批处理可降低成本。该模型适用于软件开发、客服、数据处理等多种应用，标志着人工智能领域的重要进步。","date":"2024-11-13T01:32:04.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*hLedIfhYJhS_ejPDwOQPIw.png","categories":["Programming","Machine Learning","Chatbots"],"author":"Rifx.Online","tags":["Claude","Haiku","coding","SWE-bench","benchmarks"],"draft":false,"slug":"blog/claude-3-5-haiku-anthropics-speed-demon-gets-a-brain-boost-82f2f0999d4f"},"content":"\n\n\n\n\n在人工智能进步的无情竞赛中，Anthropic刚刚推出了一位新的竞争者。认识一下Claude 3\\.5 Haiku，这是他们最快AI模型的最新版本。就像他们把短跑运动员送到了脑力训练营。结果呢？一个不仅在行动上迅速，而且在某些智力领域能够超越其更强大兄弟的模型。让我们深入了解一下这个新生事物的运作原理。\n\n## 速度（和智慧）的需求\n\nAnthropic 之前的 Haiku 模型已经是他们 AI 系列中的乌赛因·博尔特。现在，他们不知怎么地在这个速度怪兽中塞入了更多的智力，而没有牺牲其迅速性。这就像看着一只猎豹在奔跑时解魔方。\n\n## 基准测试的辉煌\n\nClaude 3\\.5 Haiku 不仅速度快，而且智能惊人。它在各种智能基准测试中表现优于 Claude 3 Opus —— Anthropic 之前的重量级冠军。这不仅仅是一次小幅升级；这是一次飞跃，使得人工智能社区为之瞩目。\n\n## 编程能力\n\n如果你是开发者，请注意。这个模型在编码领域表现出色，在SWE-bench验证测试中得分高达40.6%。这不仅令人印象深刻；这是一种让人类程序员紧张地关注他们工作安全的表现。\n\n## 引擎盖下的秘密\n\n让我们打开引擎盖，看看是什么驱动着这个人工智能热车：\n\n* **可用性**：你可以通过 Anthropic 的 API、Amazon Bedrock 或 Google Cloud 的 Vertex AI 来体验它。这就像在所有主要流媒体平台上都有的人工智能版本。\n* **输入**：目前只支持文本。还没有图像分析，但这功能即将推出。这就像有一个天才的笔友，但他无法查看你的度假照片。\n* **知识截止日期**：2024年7月。因此，它知道你去年夏天做的那件尴尬的事情，但不知道明年的流行梗。\n* **输出长度**：比前一版本有所改进。它现在可以写更长的文章来帮你拖延时间。\n\n## Show Me the Money\n\n现在，事情变得有趣了。Anthropic 决定对这个升级版模型收取高额费用：\n\n* 每百万个输入令牌 $1\n* 每百万个输出令牌 $5\n\n这比之前的版本增加了四倍。就像他们把本田思域变成了特斯拉，并相应地调整了价格。\n\n但别担心，节俭的朋友们！还有省钱的方法：\n\n* 提示缓存可以节省高达 90%。这就像为 AI 进行极限优惠券活动。\n* 使用消息批处理 API 的批量处理可以将成本降低高达 50%。大宗购买，但用于计算。\n\n## 这个东西能做什么？\n\nClaude 3\\.5 Haiku 不仅仅是一个派对把戏。它有一些严肃的现实应用：\n\n* **软件开发**：就像有一个永不睡觉且不偷你零食的编程伙伴。\n* **聊天机器人**：不需要咖啡休息或人力资源干预的客服代表。\n* **数据处理**：它能比你说“数据大”还快地处理数字。\n* **教育**：一个随时待命且从不失去耐心的辅导老师。\n* **个性化**：它记住你的偏好比你的另一半还要好。\n* **专业任务**：AI 子代理的瑞士军刀。\n* **内容审核**：每次发布都在保持互联网的清洁。\n\n## 权衡\n\n现在，并不是所有事情都那么美好。还有一些问题：\n\n* 目前还没有图像分析功能。因此，它无法告诉你那条裙子是否让你看起来胖。\n* 价格上涨可能会让一些用户继续使用旧版、便宜的版本。这就像人们仍然使用 Windows 7 的 AI 等价物。\n\n## 底线\n\nClaude 3\\.5 Haiku 是人工智能领域的一次重要飞跃。它的速度快如子弹，力量强于机车，能够一跃而过高楼大厦。好吧，也许最后那部分不太准确，但你明白我的意思。\n\n对于希望利用人工智能处理复杂任务的开发者和企业来说，Claude 3\\.5 Haiku 是一个引人注目的选择。它不仅仅是一次升级；它重新定义了在人工智能领域速度与智能交汇处的可能性。\n\n现在的问题是：竞争对手将如何回应？更重要的是，我们要等多久才能看到 Claude 4\\.0：打油诗版？\n\n## 常见问题解答\n\n**问：Claude 3\\.5 Haiku 能分析图像吗？**答：还不能，但Anthropic计划在未来添加此功能。目前，它仅支持文本。\n\n**问：Claude 3\\.5 Haiku 比其前身贵多少？**答：贵四倍，但可以通过提示缓存和批处理来降低成本。\n\n**问：Claude 3\\.5 Haiku 最令人印象深刻的功能是什么？**答：它在各种智能基准测试中能够超越更大的模型，如Claude 3 Opus，同时保持高速度。\n\n**问：我可以用 Claude 3\\.5 Haiku 进行软件开发吗？**答：当然可以。它在编码任务上表现出色，能够提供快速、准确的代码建议和补全。\n\n**问：Claude 3\\.5 Haiku 向公众开放吗？**答：是的，可以通过Anthropic的API、Amazon Bedrock和Google Cloud的Vertex AI访问。\n\n\\#Claude35Haiku \\#AnthropicAI \\#AIInnovation \\#MachineLearning \\#AIForDevelopers \\#FutureOfAI \\#AIPerformance \\#TechInnovation\n\n“Claude 3\\.5 Haiku 性能基准”，“AI 模型定价比较”，“用于软件开发的快速 AI 模型”，“Anthropic AI 模型能力”，“成本效益高的 AI 实施策略”\n\n"},{"lang":"zh","group":"blog","slug":"blog/claude-3-5-sonnet-new-pioneering-the-future-of-ai-with-computer-control-capabilities-37a6ff9f9033","frontmatter":{"title":"Claude 3.5 Sonnet（新）：利用计算机控制能力开拓人工智能的未来","meta_title":"Claude 3.5 Sonnet（新）：利用计算机控制能力开拓人工智能的未来","description":"Anthropic 于 2024 年 10 月 22 日发布了其最新的 AI 模型 Claude 3.5 Sonnet。此版本引入了革命性的计算机控制……","date":"2024-10-27T13:57:00.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*n0NkOFbhUm7_fllJ","categories":["Programming","Technology","Generative AI"],"author":"Rifx.Online","tags":["Claude","Sonnet","automation","benchmarks","safety"],"draft":false,"slug":"blog/claude-3-5-sonnet-new-pioneering-the-future-of-ai-with-computer-control-capabilities-37a6ff9f9033"},"content":"\n\n\n\n\nAnthropic于2024年10月22日发布了最新的AI模型Claude 3.5 Sonnet。此次发布引入了革命性的计算机控制能力，并在多个基准测试中实现了显著改进，为AI行业设定了新标准。\n\n## 革命性的计算机控制：新前沿\n\nClaude 3.5 Sonnet 的突出特点是其能够像人类一样与计算机进行交互。这一突破性的能力使得 AI 可以：\n\n* 使用鼠标和键盘输入导航桌面界面\n* 与各种应用程序和网页浏览器进行交互\n* 执行复杂的多步骤任务\n* 执行文件管理操作\n* 自动化重复的工作流程\n\n这一计算机控制功能目前处于公开测试阶段，代表了 AI 系统与数字界面交互方式的范式转变。尽管仍处于实验阶段，但早期测试显示出良好的结果，Claude 3.5 Sonnet 在仅截图任务的 OSWorld 基准测试中得分为 14.9% — 显著高于下一个最佳系统的 7.8%。\n\n## 基准突破性能\n\n升级后的模型在多个指标上表现出显著的改进：\n\n## 编码和技术任务\n\n* 在SWE-bench Verified上的性能为49%（较之前的33.4%有所提升）\n* 在HumanEval编码任务中的得分为93.7%\n* 在软件工程方面的表现优于专业编码系统\n\n## 学术和推理能力\n\n* 65% 的研究生级推理 (GPQA-Diamond)\n* 78% 的本科级知识 (MMLU Pro)\n* 78.3% 的数学问题解决能力 (MATH)\n\n## 商业应用\n\n* 69.2% 在零售领域任务上 (TAU-bench)\n* 46% 在航空领域任务上\n* 90.8% 在图表分析上的准确率\n* 94.2% 在文档问答上的准确率\n\n## 企业集成与可用性\n\nClaude 3.5 Sonnet 可以通过多个平台访问：\n\n* Anthropic API\n* Amazon Bedrock\n* Google Cloud’s Vertex AI\n\n包括 Asana、Canva、DoorDash 和 Replit 在内的主要公司已经开始在其工作流程中实施 Claude 3.5 Sonnet 的功能，特别是利用其计算机控制功能来处理复杂的自动化任务。\n\n## 实际应用\n\n## 软件开发\n\n* 自动化代码测试和调试\n* 智能IDE交互\n* 代码审查与优化\n* 文档生成\n\n## 客户支持\n\n* 高级聊天机器人功能\n* 可视化数据解读\n* 自动化工单解决\n* 流程自动化\n\n## 商业运营\n\n* 文档处理与分析\n* 从视觉源提取数据\n* 工作流自动化\n* 复杂问题解决\n\n## 安全与责任\n\nAnthropic 已实施强有力的安全措施用于计算机控制功能：\n\n* 新分类器以识别潜在的误用\n* 主动监控系统\n* 限制对敏感操作的访问\n* 定期安全评估\n\n## 展望未来\n\n虽然Claude 3.5 Sonnet在人工智能能力方面代表了重大进展，但重要的是要注意某些功能，特别是计算机控制，仍处于早期阶段。某些操作如滚动、拖动和缩放面临挑战，Anthropic鼓励开发者在探索这些新功能时，从低风险任务开始。\n\nClaude 3.5 Sonnet的发布标志着人工智能发展的一个关键时刻，将先进的推理能力与实用的计算机控制功能相结合。随着技术的不断发展，我们可以期待看到更多创新的应用以及人工智能系统与我们的数字世界互动方式的改进。\n\n*本文基于Anthropic、AWS和各类技术合作伙伴的官方公告和文档。有关最新信息，请参考Anthropic的官方文档。*\n\n"},{"lang":"zh","group":"blog","slug":"blog/claude-3-5-sonnet-v-s-gpt-4o-which-one-is-better-3b3675195bf9","frontmatter":{"title":"Claude 3.5 Sonnet V/S GPT-4O：哪一个更好","meta_title":"Claude 3.5 Sonnet V/S GPT-4O：哪一个更好","description":"2022 年 11 月，OpenAI 推出了 ChatGPT 模型，该模型彻底改变了我们搜索和与信息交互的方式。明年，在…","date":"2024-10-27T13:59:09.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*4MXLuSFfGwFkWWn0","categories":["Generative AI","Machine Learning","Natural Language Processing"],"author":"Rifx.Online","tags":["GPT-4o","Claude","multimodal","reasoning","code-generation"],"draft":false,"slug":"blog/claude-3-5-sonnet-v-s-gpt-4o-which-one-is-better-3b3675195bf9"},"content":"\n\n\n在2022年11月，OpenAI推出了ChatGPT，这一模型彻底改变了我们搜索和与信息互动的方式。次年3月，由前OpenAI员工创办的美国初创公司“Anthropic”推出了他们自己的AI模型“Claude”。自发布以来，这两家AI公司一直在竞争，以通过其AI模型为客户提供最佳的功能和体验。最近，OpenAI推出了“GPT-4o”，这是一个令人惊叹的模型，能够出色地处理文件、语音和视频数据。同样，Claude推出了“Claude 3.5 Sonnet”，他们声称这是最先进的AI模型，能够处理复杂任务。在本文中，我们将确定Claude 3.5 Sonnet和GPT-4o之间哪个更好，并比较其在相同输入下的功能和输出，以检查哪个更适合您。\n\n## 能力和特性\n\n### GPT-4o\n\n\n\nGPT-4o 是 OpenAI 最新推出的 LLM。“o” 代表 omni，意为拉丁语中的“每一个”。该模型可以分析语音、图像、视频和文件作为输入，并相应地做出回应。它可以接受语音输入，并以不同角色的声音输出，包括语调、情感等。整个过程与人类对话的延迟相当低，平均为 0.32 秒，而其他语音模型则为 2.8 秒。它还允许用户生成书面内容，如文章、博客、产品描述、不同编程语言的代码、数据分析、图表等。此外，GPT-4o 还可以分析图像和视频，使该模型可以充当语言翻译器、个人助理、虚拟教师或购物助手。它还可以用于医学、工程、军事等领域。要使用此功能，GPT-4o 可以使用用户的摄像头获取实时视图，并在语音模式下相应地回应。它还可以访问您的计算机屏幕，并描述屏幕上显示的内容，用户可以询问与屏幕上显示的内容相关的问题。\n\n*例如，用户可以在屏幕上启用该模型，打开 VS 代码，并提示模型充当编码助手，以获取编码问题的答案。或者，您可以启用摄像头，充当健身教练，检查您是否做得正确。*\n\n该模型具有独特的功能，如数据分析、代码解释器和实时网页浏览，使其与竞争对手不同。该模型还有大量的 GPTs，这是 ChatGPT 的定制版本。\n\n### Claude 3.5 Sonnet\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*BSMcOpvWZ5lUm4Tl)\n\nClaude 3.5 Sonnet 是由 Anthropic 推出的 AI 聊天机器人。它是 Claude AI 模型系列的第三代。这一模型在多个评估中表现出色，保持了高基准，避免了幻觉和错误信息。虽然它不支持像 GPT-4o 那样的语音和视频功能，但它仍然可以执行所有基本任务，例如文本生成和不同编程语言的代码生成、头脑风暴等。根据 Anthropic 的报告，Claude 3.5 Sonnet 是市场上最好的计算机视觉模型之一，可以用于分析图表和图形，从图像中转录文本等。Claude 拥有一个先进的功能，“Artifacts”，这是一个在对话中出现的特殊弹出窗口，允许用户查看代码片段、文本文件或网站设计，并允许他们实时编辑输出。\n\n*例如，用户可以在工作流程中使用计算机视觉和 artifacts。用户可以在纸上进行网站设计的基本原型制作，将文件附加到 Claude 3.5 Sonnet，并提示它根据原型设计网站。生成的代码和网站设计会出现在 artifacts 中。用户可以根据自己的需求编辑代码和设计。用户还可以将他们的项目实时发布到互联网上。*\n\n## 逐项比较\n\n在本节中，我们将根据复杂推理和代码生成等因素比较这两个 LLM，检查它们在处理复杂任务方面的能力，并看看哪个模型更好。\n\n* **研究生水平推理(GPQA, Diamond)**此因素评估模型处理研究生水平教育中复杂、高级推理任务的能力。在此任务中，研究人员在 GPQA 测试中比较模型，该测试由专家设计，包含448个不同领域的问题。这些问题是 Google Proof，因此任何人都无法在线找到它们。Claude 的得分接近 59.4%，而 GPT-4o 的得分仅为 53.6%。虽然两个得分相对接近，但正如我们所见，Claude 在需要高级分析思维的任务中可能是更好的选择，例如研究分析、复杂问题解决和高学术水平的问题。\n* **本科水平知识(MMLU)**MMLU，即大规模多任务语言理解，是一个基准，解释任何 AI 模型在本科水平上对各个学科的通用知识理解。Claude 3.5 Sonnet 在此实验中的得分为 88.3%，而 GPT-4o 的得分为 88.7%。这表明这两个 LLM 在多个领域进行了训练，并对这些领域有更深入的理解。这使得 AI 模型成为通用知识任务、多个学科的基础辅导等的合适工具。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*A4w-tvsxcmFINaQT)\n\n* **代码(HumanEval)**HumanEval 是一个基准，评估模型生成、理解和调试代码的能力。在这个基准中，Claude 3.5 Sonnet 达到了 92%的得分，而 GPT-4o 的得分为 90.2%。Claude 3.5 Sonnet 在此任务中的结果非常出色，因为它提供了比 GPT-4o 更好的编码环境“Artifacts”和更好的代码生成。Claude 允许用户在 Artifacts 弹出窗口中设计、编辑和运行代码。在 Claude 3.5 Sonnet 发布后，大家都在开发工具、网站和基本游戏，并在互联网上分享它们。另一方面，GPT-4o 的得分也不错，但它的界面中没有任何编码环境，因此开发人员必须花费很多精力，因为它生成的代码很难达到结果。\n* **文本推理(DROP, FLscore)**DROP（段落离散推理）是一个基准，测量模型理解复杂文本信息的能力。在这个挑战中，Claude 3.5 Sonnet 的得分为 87.1%，而 GPT-4o 的得分为 83.4%。这表明 Claude 3.5 Sonnet 在涉及详细文本分析、文本审查、复杂问答系统等任务时更好且更有效。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Kcy7sFb2FYpbfrwp)\n\n* **数学问题解决(MATH)**此测试评估任何 AI 模型解决各种数学问题的能力。Claude 3.5 Sonnet 的得分仅为 71.1%，而 GPT-4o 的得分为 76.6%。这些得分使 GPT-4o 成为数学问题解决任务的更好模型，并可用于财务建模、科学计算和高级数据分析等数学计算。\n* **多语言数学(MSGM)**此因素描述任何 AI 模型在多种语言中解决数学问题的能力。两个模型的得分接近：GPT-4o 90.5% 和 Claude 3.5 Sonnet 91.6%。这表明两个模型表现出色，Claude 略胜一筹。该能力对于教育应用或任何需要跨语言障碍进行数学推理交流的场景特别有用。\n* **视觉问答(MMU/val)**此因素描述 LLM 分析图像中呈现的信息的能力。GPT-4o 在这一基准中以 69.1% 超过 Claude 3.5 Sonnet 的 68.3%。另一方面，在分析文档中的文本时，Claude 3.5 Sonnet 的得分为 95.2%，而 GPT-4o 的得分为 92.1%。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*xzjqBV2YL0lVFitX)\n\n* **图像生成**图像生成是 LLM 从文本生成图像的能力。GPT-4o 集成了 DallE-2，可以通过文本生成图像，结果非常出色。另一方面，Claude 3.5 Sonnet 无法创建任何图像。此功能还帮助 GPT-4o 更好地设计网站和参考，因为它在许多图像上进行了训练。\n* **知识截止**在这里，两个模型都在特定日期之前的有限数据集上进行了训练。Claude 3.5 Sonnet 在 2024 年 4 月之前的数据上进行了训练，而 GPT-4o 则在 2024 年之前的数据上进行了训练。GPT-4o 的真正优势在于它具有实时网页浏览功能，这有助于 LLM 定期在新数据上进行训练。\n\n## GPT-4o 的优点：\n\n* 处理语音、图像和视频输入。\n* 实时网页浏览能力。\n* 更快的响应时间（平均 0.32 秒）。\n* 在数学问题解决方面表现优越。\n* 可以使用 DALL-E 2 生成图像。\n\n## GPT-4o 的缺点：\n\n* 研究生水平推理的性能稍低。\n* 没有内置的编码环境。\n* 文档视觉问答得分较低。\n* 代码生成能力稍逊。\n* 在详细文本分析方面效果较差。\n\n## Pros Claude 3.5 Sonnet:\n\n* 在研究生级别的推理方面表现出色。\n* 优越的代码生成和内置的“工件”功能。\n* 在详细文本分析中表现更佳。\n* 在文档视觉问答中得分更高。\n* 在多语言数学方面略有优势。\n\n## Cons Claude 3.5 颂：\n\n* 无法处理语音或视频输入。\n* 没有图像生成能力。\n* 在视觉问答方面性能稍低。\n* 无法访问实时网络信息。\n* 在数学问题解决方面较弱。\n\n## 结论\n\nGPT-4o 和 Claude 3.5 Sonnet 在各种任务中展现了令人印象深刻的能力，各自有其优势。GPT-4o 在多模态输入、实时信息访问和图像生成方面表现出色，使其在多种应用中非常灵活。Claude 3.5 Sonnet 在复杂推理、代码生成和详细文本分析方面表现突出，在特定的学术和专业背景下提供了更优的性能。选择这两种模型取决于具体的使用案例和所需的功能。随着人工智能技术的进步，我们可以期待进一步的改进和针对不同需求的专业模型。\n\n"},{"lang":"zh","group":"blog","slug":"blog/comparative-study-of-langgraph-autogen-and-crewai-for-building-multi-agent-systems-0e7e47f9078e","frontmatter":{"title":"用于构建多代理系统的 LangGraph、Autogen 和 Crewai 比较研究","meta_title":"用于构建多代理系统的 LangGraph、Autogen 和 Crewai 比较研究","description":"当我们涉足多代理系统（MAS）领域时，了解各种编程语言的设计至关重要。","date":"2024-11-13T01:22:29.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*DBlLuCOA3lWIg6RmpMPg8A.png","categories":["Programming","Technology","Machine Learning"],"author":"Rifx.Online","tags":["LangGraph","Autogen","Crewai","multi-agent","scalability"],"draft":false,"slug":"blog/comparative-study-of-langgraph-autogen-and-crewai-for-building-multi-agent-systems-0e7e47f9078e"},"content":"\n\n\n随着我们进入多智能体系统（MAS）的领域，了解专门为此目的设计的各种编程语言至关重要。在本文中，我们将通过比较 LangGraph、Autogen 和 Crewai —— 该领域的三大重要参与者，深入探讨 MAS 开发的世界。\n\n## 介绍\n\n多智能体系统（MAS）在各个行业中变得越来越重要。MAS是由多个智能体组成的系统，这些智能体相互之间以及与环境进行交互，以实现特定目标。在可用于构建MAS的众多框架中，LangGraph、Autogen和Crewai是一些最受欢迎的选择。\n\n作为从事MAS项目的开发者或研究人员，选择合适的框架可能会让人感到不知所措，尤其是考虑到易用性、可扩展性、定制化和与AI库的集成等因素。本文提供了LangGraph、Autogen和Crewai的比较研究，突出了它们的优缺点以及在不同应用中的适用性。\n\n### 各框架介绍\n\n## LangGraph: 一个开源框架\n\n**优点**：\n\n* **易于使用**：LangGraph 提供了简单直观的 API，使开发者能够轻松与现有系统集成。\n* **可扩展性**：LangGraph 支持大规模分布式系统，使用户能够处理复杂任务。\n* **与 AI 库的集成**：LangGraph 与流行的 AI 库如 TensorFlow、PyTorch 和 Keras 兼容。\n\n**局限性**：\n\n* 对分布式系统的支持有限\n* 比 Autogen 和 Crewai 灵活性差\n\n## Autogen: 一个模块化的开源框架\n\n**优势**：\n\n* **高度灵活性**：Autogen 提供了模块化架构，使用户能够根据特定需求定制他们的 MAS。\n* **适合复杂应用**：Autogen 的模块化使其非常适合具有多个互联代理的大型系统。\n* **强大的社区支持**：Autogen 拥有一个活跃的开发者和研究者社区，他们为该框架做出贡献并提供支持。\n\n**局限性**：\n\n* 学习曲线较陡\n* 需要更多资源\n\n## Crewai: 可扩展的数据驱动框架\n\n**优点**：\n\n* **可扩展性**：Crewai 对大规模系统提供了出色的支持，非常适合需要处理大量数据的应用程序。\n* **易用性**：Crewai 提供了一个简单的 API，便于与现有系统集成。\n* **与云服务的集成**：Crewai 允许用户轻松地在 AWS 和 Azure 等云平台上部署他们的 MAS。\n\n**局限性**：\n\n* 对自定义模型的支持有限\n* 灵活性不如 Autogen\n\n## 对比矩阵\n\n\n\n## 结论\n\n总之，每个框架都有其独特的优点和缺点。LangGraph 提供了易用性和可扩展性，Autogen 提供了灵活性和可定制性，而 Crewai 在数据驱动的方法和可扩展性方面表现出色。\n\n在选择构建 MAS 的框架时，请考虑项目的具体要求：\n\n* **易用性**：如果您重视简单性和可扩展性，请选择 LangGraph。\n* **灵活性**：对于需要定制的复杂应用程序，请选择 Autogen。\n* **可扩展性**：对于需要大规模数据处理的大型系统，请考虑 Crewai。\n\n通过了解每个框架的优缺点，开发人员可以做出明智的决策，从而选择构建更有效和高效的解决方案的 MAS。\n\n## 额外资源\n\n有关进一步阅读和资源，请参见：\n\n* [LangGraph 文档](https://proxy.rifx.online/https://langgraph.com/documentation/)\n* [Autogen 教程](https://proxy.rifx.online/https://autogen.com/tutorials)\n* [Crewai API 参考](https://proxy.rifx.online/https://crewai.com/api-reference/)\n\n"},{"lang":"zh","group":"blog","slug":"blog/explore-swarm-multi-agent-framework-locally-0e25ee617795","frontmatter":{"title":"本地探索 Swarm 多智能体框架","meta_title":"本地探索 Swarm 多智能体框架","description":"Swarm 是一个实验性示例框架，用于模拟轻量级多代理框架，用于教育目的。通常它与 Open… 配合使用","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*0ZVceq32bvkytC7HSIgmwA.png","categories":["Programming","Technology","Education"],"author":"Rifx.Online","tags":["Swarm","Multi-Agent","Framework","OpenAI","Ollama"],"draft":false,"slug":"blog/explore-swarm-multi-agent-framework-locally-0e25ee617795"},"content":"\n\n\n\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*zkpW8DDwh0TTYuHJVJbDaw.png)\n\nSwarm 是一个实验性样本框架，用于模拟轻量级多智能体框架，旨在教育目的。通常它与 Open AI Key 一起使用，但我们可以更改为使用本地的 Ollama 或 LM Studio 模型。\n\n**设置：**\n\n\n```python\n## 创建一个新的 Conda 或 Python 虚拟环境并激活它\nconda install python==3.10\npip install torch openai\npip install transformers accelerate huggingface_hub\npip install git+ssh://git@github.com/openai/swarm.git\n```\n**使用 Open AI Key：**\n\n\n```python\nexport OPEN_API_KEY = Your Key\n```\n**使用 Ollama 或 LM Studio 本地 LLM — 更新为本地 URL：**\n\n\n```python\n## 查找 conda 或 python 虚拟环境中的 site-packages/swarm\n## 找到文件 core.py\nclass Swarm:\n    def __init__(self, client=None):\n        if not client:\n          # 实际代码\n          #client = OpenAI()\n          # 将基础 URL 和 API Key 更新为 Ollama / LM Studio\n          # 在本演示中，我们使用 LM Studio 和 Llama 3.1\n          client = OpenAI(base_url=\"http://localhost:1234/v1\",api_key=\"random\")\n        self.client = client\n```\n**克隆仓库：**\n\n克隆仓库 — 在这里您可以找到不同用例的示例目录，如基本、航空公司和天气等。\n\n\n```python\ngit clone https://github.com/openai/swarm.git\ncd swarm/examples\n```\n**示例代码：**\n\n\n```python\nfrom swarm import Swarm, Agent\n\nclient = Swarm()\n\n\nit_agent = Agent(\n    name=\"IT Agent\",\n    instructions=\"You are an IT Expert with 10 Years of Experience.\",\n)\n\nsales_agent = Agent(\n    name=\"Sales Agent\",\n    instructions=\"You are a Sales Expert with 5 Years of Experience and knows about best selling mobiles.\",\n)\n\ndef transfer_to_sales_agent():\n    print(\"Sales agent in action\")\n    \"\"\"Transfer sales related questions to sales team immediately.\"\"\"\n    return sales_agent\n\ndef transfer_to_it_agent():\n    print(\"IT agent in action\")\n    \"\"\"Transfer IT users immediately.\"\"\"\n    return it_agent\n\nenglish_agent = Agent(\n    name=\"English Agent\",\n    instructions=\"You only speak English.\",\n    functions=[transfer_to_sales_agent,transfer_to_it_agent],\n)\n\n\nmessages = [{\"role\": \"user\", \"content\": \"How to install pandas lib?\"}]\nresponse = client.run(agent=english_agent, messages=messages)\n\nprint(response.messages[-1][\"content\"])\n\nmessages = [{\"role\": \"user\", \"content\": \"What are the best selling items?\"}]\nresponse = client.run(agent=english_agent, messages=messages)\n\nprint(response.messages[-1][\"content\"])\n```\n**参考文献：**\n\n\n```python\nhttps://github.com/openai/swarm\n\nhttps://github.com/victorb/ollama-swarm/tree/main\n```\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*hCFJ4VQoT12yElYPXwXvWA.png)\n\n鉴于这是一个实验性版本，仍有很大的改进空间。航空代理示例代码 [swarm/examples/airline] 非常有趣，因此可以尝试这些示例。试试看，并在评论中分享您的经验。谢谢。\n\n"},{"lang":"zh","group":"blog","slug":"blog/fine-tuning-llama-3-with-unsloth-79c3465ef3e3","frontmatter":{"title":"使用 Unsloth 对 LLama 3 进行微调","meta_title":"使用 Unsloth 对 LLama 3 进行微调","description":"在本文中，我将向您展示如何使用 Unsloth 对 LLM（来自 Meta 的 Llama 3）进行微调（包括自定义数据集的方法）","date":"2024-10-30T12:58:41.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kaXoudNTGeGfuNPl_kta5g.jpeg","categories":["Programming","Machine Learning","Natural Language Processing"],"author":"Rifx.Online","tags":["Llama","Unsloth","LoRA","Alpaca","NVIDIA"],"draft":false,"slug":"blog/fine-tuning-llama-3-with-unsloth-79c3465ef3e3"},"content":"\n\n\n在本文中，我将向您展示如何使用 [Unsloth](https://github.com/unslothai/unsloth) 微调 LLM（Meta 的 Llama 3）。我还将提供使用您自己自定义数据集的方法。\n\n**注意：** Unsloth 是一个加速 LLM 在 NVIDIA GPU 上微调的库（与传统方法相比，内存使用减少 40%）。与 Hugging Face 兼容，支持 Llama 和 Mistral 架构。\n\n如果您觉得我的文章有趣，请不要忘记 **点赞并 [关注](https://medium.com/@soulawalid)** 👍🏼，写这些文章需要时间和精力！\n\n您可以访问 GitHub 仓库中提供的免费笔记本。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*_L4o4MDQ7W5__OwW0E5RWA.png)\n\n由于我使用的是 Llama 3，因此我将点击笔记本（您也可以在自己的计算机上安装 Unsloth）。\n\n**注意：** 我将使用这个数据集 “[alpaca\\-cleaned](https://huggingface.co/datasets/yahma/alpaca-cleaned)” 来自 Hugging Face，数据采用 Alpaca 格式，即包含（指令、输入和输出）。\n\n### 开始项目\n\n在项目中，我将指导您使用 Unsloth 进行微调，解释代码并提供建议，让我们开始我们的项目：\n\n**1/ 安装所需的包：** 我们首先需要安装 **Unsloth** 和 **xformers**、**trl**、**peft**、**accelerate**、**bitsandbytes** 库，以便进行高效的模型训练和推理。\n\n```python\n!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n!pip install --no-deps xformers trl peft accelerate bitsandbytes\n```\n\n**2/ 加载和配置模型：** 在配置中，我将设置以下内容：\n\n* 将最大序列长度设置为 **2048**\n* 将 dtype 设置为 **None**，它会自动检测数据类型。\n* 以 **4-位精度**加载模型，我认为这已经足够。\n\n**注意：** 您可以在资源部分找到我关于微调 LLM 的技巧的文章。\n\n```python\nfrom unsloth import FastLanguageModel\nimport torch\n\n## 配置\nmax_seq_length = 2048\ndtype = None\nload_in_4bit = True\n\n## 加载选定的模型\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/llama-3-8b-bnb-4bit\",\n    max_seq_length=max_seq_length,\n    dtype=dtype,\n    load_in_4bit=load_in_4bit,\n)\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*cJSAcJFP7E-qJkqKUsHqLw.png)\n\n**3/ 应用 PEFT（参数高效微调）：** 然后我们将使用 LoRA 对预训练模型进行微调。\n\n* r = 16 是 LoRA 的秩参数。**注意：** 常见值为 8、16、32、64、128\n* lora_alpha = 16 代表 LoRA 更新的缩放因子（我将写一篇关于 LoRA 的文章，以详细解释每个部分）\n* 对于 LoRA 不使用 dropout 和偏置\n* 对于 use_gradient_checkpointing，我们使用 Unsloth 来处理（节省内存）\n\n```python\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 16,\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha = 16,\n    lora_dropout = 0,\n    bias = \"none\",\n    use_gradient_checkpointing = \"unsloth\",\n    random_state = 3407,\n    use_rslora = False,\n    loftq_config = None,\n)\n```\n\n**4/ 定义提示模板：** 我们将创建 alpaca 提示模板以格式化数据集（如果您使用的数据不是这种格式）。\n\n我们还将添加 EOS（结束序列）以通知 LLM 句子已结束。\n\n最后是格式化函数，该函数接受一批示例并根据我们之前编写的 alpaca 提示模板格式化每个示例。\n\n* 它从每个示例（行）中提取指令、输入和输出字段。\n* 然后将这些字段格式化到模板中并附加 EOS 标记。\n* 格式化的文本存储在列表中，并作为具有单个键“text”的字典返回。\n\n```python\nalpaca_prompt = \"\"\"以下是描述任务的指令，配有提供进一步上下文的输入。写一个适当完成请求的响应。\n\n#### 指令：\n{}\n\n#### 输入：\n{}\n\n#### 响应：\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token\n\ndef formatting_prompts_func(examples):\n    instructions = examples[\"instruction\"]\n    inputs = examples[\"input\"]\n    outputs = examples[\"output\"]\n    texts = []\n    for instruction, input, output in zip(instructions, inputs, outputs):\n        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n        texts.append(text)\n    return {\"text\": texts}\n```\n\n**5/ 加载和格式化数据集：** 加载 Alpaca 数据集并对每个数据集示例应用格式化。\n\n```python\nfrom datasets import load_dataset\ndataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\ndataset = dataset.map(formatting_prompts_func, batched = True)\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*M8EmbLMdoqrM-JlkMpDv8g.png)\n\n**6/ 设置和训练模型：** 我在我[之前的文章](https://readmedium.com/supervised-fine-tuning-tips-for-your-llm-projects-f84f20593653)中涵盖了大部分关于微调的技巧。\n\n```python\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2, # 用于数据预处理的进程数量\n    packing = False, # 是否将多个序列打包成一个批次以提高训练效率\n    args = TrainingArguments(\n        per_device_train_batch_size = 2, # 每个设备的批次大小\n        gradient_accumulation_steps = 4, # 梯度累积步数，允许有效增大批次大小\n        warmup_steps = 5, # 进行线性学习率预热的步骤数\n        max_steps = 60, # 总训练步骤数\n        learning_rate = 2e-5,# 优化器的学习率\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"cosine\",\n        seed = 3407,\n        output_dir = \"outputs\",\n    ),\n)\n\ntrainer_stats = trainer.train()\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Vb_OqGP9CPc8xZdnkclGyQ.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*PI0JXrTbpjuviyQ4bZJnFg.png)\n\n**7/ 推理和生成：** 我们通过准备输入提示、对其进行标记化，然后使用模型根据该提示生成新文本来准备模型进行推理。生成的文本随后被转换回可读形式。\n\n```python\nFastLanguageModel.for_inference(model)\ninputs = tokenizer(\n[\n    alpaca_prompt.format(\n        \"继续斐波那契数列。\", # 指令\n        \"1, 1, 2, 3, 5, 8\", # 输入\n        \"\", # 输出 - 留空以进行生成！\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*PI6SBL_YPPj0-RSAn5nl7g.png)\n\n您还可以使用 TextStreamer 进行连续推理，这样您可以看到生成的每个标记，而不是一直等待整个过程！\n\n```python\nFastLanguageModel.for_inference(model)\ninputs = tokenizer(\n[\n    alpaca_prompt.format(\n        \"继续斐波那契数列。\",\n        \"1, 1, 2, 3, 5, 8\",\n        \"\",\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*NaSQ1vQKORU1I3DsOU2iOA.png)\n\n**8/ 保存模型：** 如果您对此感到满意，可以保存您的模型或将其推送到 Hugging Face Hub。\n\n```python\nmodel.save_pretrained(\"lora_model\")\ntokenizer.save_pretrained(\"lora_model\")\n## model.push_to_hub(\"your_name/lora_model\", token = \"...\")\n## tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\")\n```\n\n**9/ 加载模型：**\n\n```python\nif False:\n    from unsloth import FastLanguageModel\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name = \"lora_model\",\n        max_seq_length = max_seq_length,\n        dtype = dtype,\n        load_in_4bit = load_in_4bit,\n    )\n    FastLanguageModel.for_inference(model)\n```\n\n**10/ 用于生成：**\n\n```python\ninputs = tokenizer(\n[\n    alpaca_prompt.format(\n        \"巴勒斯坦的首都是什么？\",\n        \"\",\n        \"\",\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)\n```\n\n如果您有特定主题希望我们讨论，请随时告诉我！您的反馈将有助于塑造我的内容方向，确保其保持相关性和吸引力😀\n\n\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/gemini-1-5-flash-vs-gpt-4o-88b9d8da8152","frontmatter":{"title":"全新 Gemini 1.5 FLASH 型号：绝对的 Google 游戏规则改变者","meta_title":"全新 Gemini 1.5 FLASH 型号：绝对的 Google 游戏规则改变者","description":"Gemini 1.5 Flash 完胜 GPT-4o","date":"2024-11-08T00:27:31.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Reb1owOmiw5DFd4A.png","categories":["Programming","Machine Learning","Generative AI"],"author":"Rifx.Online","tags":["Gemini","Flash","GPT-4o","multi-modality","creativity"],"draft":false,"slug":"blog/gemini-1-5-flash-vs-gpt-4o-88b9d8da8152"},"content":"\n他们的新 Gemini 1\\.5 Flash 模型远远超过了 GPT\\-4o，其能力令人难以置信。\n\n**闪电般快速**。\n\n\n\n比 GPT\\-4o 便宜 33 倍，但上下文容量大 700% — **100 万个令牌。**\n\n在现实世界中，100 万个令牌是什么概念？大约：\n\n* 超过 1 小时的视频\n* 超过 30,000 行代码\n* 超过 700,000 个单词\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*E1XIOcpWfeqOZSZC.jpg)\n\n❌GPT\\-4o 成本：\n\n* 输入：每百万个令牌 $2\\.50\n* 输出：每百万个令牌 $10\n* 缓存输入：每百万个令牌 $1\\.25\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*XM3hFyS_PCcuv8Px.png)\n\n✅ Gemini 1\\.5 Flash 成本：\n\n* 输入：每百万个令牌 $0\\.075\n* 输出：每百万个令牌 $0\\.30\n* 缓存输入：每百万个令牌 $0\\.01875\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*d-1ioFlCxW3LB4SL.png)\n\n还有用于成本效益任务的 mini Flash\\-8B 版本 — 比 GPT\\-4o 便宜 66 倍：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*5B5ybLzTr7penwms.png)\n\n最棒的是其多模态性 — 它可以以复杂的集成方式对文本、文件、图像和音频进行推理。\n\n而 1\\.5 Flash 几乎具备 Pro 的所有能力，但速度更快。作为开发者，你现在就可以开始使用它们。\n\nGemini 1\\.5 Pro 在一部 44 分钟的无声电影中进行了测试，令人惊讶的是，它轻松将电影分析成各种情节和事件，甚至指出大多数人在第一次观看时会错过的小细节。\n\n与此同时，GPT\\-4o API 仅允许你处理文本和图像。\n\n你可以在谷歌的 AI Studio 中轻松创建、测试和完善提示 — **完全免费**。\n\n这不会像在 OpenAI playground 中那样计入你的账单。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*5BKejWrJvrsEWIjc.png)\n\n看看谷歌 AI Studio 的强大功能 — 根据图像创建食谱：\n\n我上传了这张来自 gettyimages 的美味面包：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*fC5YL_dplJ9Od_vN.jpg)\n\n现在：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*GezbFh9KzFXRVhr3.png)\n\n如果我想要响应以我的 API 或其他内容的特定格式呢？\n\n那么你可以打开 JSON 模式并指定响应模式：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*aRZuia7Iz_mI2s9b.png)\n\nOpenAI playground 也有这个，但使用起来不如它直观。\n\nGemini 相较于 OpenAI 的另一个升级是它的创造力。\n\n在 Gemini 中，你可以将 `temperature` 从 0 增加到 200% 来控制响应的随机性和创造性：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*4AAFdAMfT_xyflmv.png)\n\n而在 OpenAI 中，如果你尝试超过 100%，你很可能会得到一堆完全无意义的内容。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*yzFQL69pyJmgE9UB.png)\n\n而且最棒的是 — 当你完成创建提示后，你可以直接使用 **获取代码** — 轻松复制并粘贴模板 API 代码，快速进入开发。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*xgaZfVe9b8WSBMmq.png)\n\n支持包括 Kotlin、Swift 和 Dart 在内的多种语言 — 在移动开发中实现高效的 AI 工作流程。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*AMkfKm-3KQRxnltO.png)\n\n在 OpenAI playground 中，你可以获得 Python 和 JavaScript 的代码。\n\n## 最后的思考\n\nGemini 1.5 Flash 是一款颠覆性的产品，以极低的成本提供无与伦比的能力。\n\n凭借其先进的多模态易用性、慷慨的免费定价和创造潜力，它为人工智能设定了新的标准，让 GPT-4o 相形见绌。\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/gemma-vs-llama-vs-mistral-exploring-smaller-ai-models-672a95f4b9b7","frontmatter":{"title":"Gemma、Llama 和 Mistral：探索较小的 AI 模型","meta_title":"Gemma、Llama 和 Mistral：探索较小的 AI 模型","description":"小规模语言模型的比较研究：评估 Gemma、Llama 3 和 Mistral 在阅读理解任务中的表现","date":"2024-11-10T22:36:54.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*TJqJ12YQCeYTS5fWOYR5Ig.png","categories":["Natural Language Processing","Machine Learning","Data Science"],"author":"Rifx.Online","tags":["Gemma","Llama","Mistral","SQuAD","Multi-Query"],"draft":false,"slug":"blog/gemma-vs-llama-vs-mistral-exploring-smaller-ai-models-672a95f4b9b7"},"content":"\n### 小规模语言模型的比较研究：在阅读理解任务中评估 Gemma、Llama 3 和 Mistral\n\n## 引言\n\n大型语言模型（LLMs）正在快速发展。每个月，新的模型被开发出来，以超越当前市场上的顶尖模型。这种健康的竞争有利于创造新的方法，提高质量和速度。此外，各公司还专注于开发更小的模型，以便使其能够被没有强大计算资源的个人或组织所使用。\n\n就在几周前，苹果公司在其全球开发者大会上推出了Apple Intelligence。这是一套多个生成模型，经过微调以帮助用户撰写和完善文本、优先处理和总结通知、创建图像以及进行应用内操作。在该套件中，苹果公司开发的唯一基础和专有模型是在同一大会上介绍的。它是一个旨在设备上运行的小型模型，其中硬件成为一个重要的限制。在苹果的案例中，该模型是闭源的。我们所知道的是，它是一个约30亿参数的模型，与Gemma、Mistral和Llama 3的7b版本相当（根据苹果分享的结果）。\n\n虽然苹果的新模型令人兴奋，但我们无法测试或重用它。因此，我们更感兴趣的是公开可用的模型，因为开发者和公司可以利用它们来构建新产品和服务。区分开放LLMs和开源LLMs是重要的。从历史上看，开源软件指的是在特定许可证下发布的计算机程序，使源代码可供公众使用或修改。在LLMs中，存在额外的复杂性，包括训练数据和模型权重。因此，开放LLMs通常会披露模型权重和初始代码。另一方面，开源LLM将分享训练过程的每一步，包括训练数据，以及一个宽松的许可证。它应该允许其他人使用、构建和进一步分发该模型。然而，如今发布的大多数模型都属于开放LLMs的范畴，因为例如它们并未发布用于训练的数据库。这种情况适用于谷歌的Gemma、Mistral AI的Mistral和Meta的Llama。\n\n在本文中，我们更仔细地分析Gemma，以了解这些较小模型的区别。Gemma是谷歌最近发布的模型之一。它有两个版本，分别是20亿和70亿参数。因此，它可以在边缘设备上使用，并旨在超越Mistral和Llama 3等最先进的模型。\n\n此外，我们将Gemma、Llama 3和Mistral应用于一个名为SQuAD的阅读理解数据集。LLMs的任务是根据给定的上下文回答特定问题。我们使用定量指标评估它们的性能，例如推理速度和平均回答长度。我们还使用了\\[1]提出的相对答案质量（RAQ）框架。RAQ通过根据答案相对于真实答案的准确性对答案进行排名，填补了在特定用例中评估LLMs的空白，从而提供了更细致和实用的模型性能评估。\n\n\n\n如往常一样，代码可在我们的[GitHub](https://github.com/zaai-ai/lab)上找到。\n\n## Gemma: Gemini的基础文本模型\n\n谷歌发布了Gemma \\[2]，这是基于其强大的闭源模型Gemini \\[3]开发的开放LLM。\n\n谷歌发布了预训练和微调的检查点，以促进该模型在新用例中的进一步研究，提供了两种不同的大小：\n\n* 7B模型将被部署并在GPU或TPU上进一步开发。\n* 2B模型旨在解决计算限制，并允许在CPU或设备应用程序上使用。\n\nGemma承诺在与其他大致相同规模的开放模型（如Llama 3 7B或Mistral 7B）相比时，达到最先进的性能。这应该在不同领域中实现，例如问答、常识推理、数学/科学和编码。\n\n## Gemma: 有什么新变化？\n\nGemma 的架构基于一个仅解码器 \\[4] Transformer \\[5]，上下文长度为 8192 个标记。让我们来探讨一下为使其更小而采取的方法。\n\n## 多查询注意力\n\n2B模型利用多查询注意力（MQA）显著减少了加载所有查询、键和值头所需的内存资源，而不是使用多头注意力（MHA）方法。MQA通过在注意力层中对多个查询头使用单一的键和值来实现这种内存减少，如图3所示。\n\n虽然这种方法允许Gemma 2B在内存资源较小的设备上部署，但可能导致质量下降和训练不稳定。因此，作者选择在7B版本中使用MHA，遵循与Llama 3相同的方法。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*cgSktHmd_iQeTU4DwWLCPQ.png)\n\n## RoPE 嵌入\n\nTransformers 需要位置嵌入，因为它们本质上是无序不变的。这意味着如果没有位置信息，Transformer 将以相同的方式表示具有相同单词但不同顺序和意义的句子。例如：\n\n> *句子 1:* Gemma 比 Llama 3 更好\n\n> *句子 2:* Llama 3 比 Gemma 更好\n\n位置信息通常使用两个正弦函数（正弦和余弦）来表示。然后，根据位置、标记嵌入维度和模型维度，为序列中的每个位置创建一个独特的位置信息嵌入。\n\n因此，添加位置信息对于使 Transformers 正确处理文本至关重要。原始 Transformer 架构使用**绝对位置嵌入**，其中位置的向量表示被添加到标记的向量表示中。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*cU5a_5-ATKwrQVeka-ViXQ.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*JZLrvgvc7l_52uewCrPSbg.png)\n\n绝对位置嵌入的挑战在于它们并未明确编码标记之间的相对距离。虽然它们使用正弦和余弦函数捕获位置信息，但这些嵌入是针对每个位置独立计算的。这意味着模型并不固有地理解序列中不同位置的接近性或关系重要性。例如，位置 1 和 2 的标记嵌入可能由于正弦函数的性质而看起来相似，但模型并未明确识别这些位置是相邻的。\n\n因此，模型可能无法区分位置 1 和 2 的标记之间的关系与位置 1 和 500 的标记之间的关系。在自然语言处理过程中，句子中相近的单词通常共享更多上下文或具有比远离的单词更强的语义或句法关系。绝对位置嵌入可能无法完全捕获这种细微差别。这可能导致在捕获长程依赖关系或语言的层次结构方面的局限性。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*p-fG2ydLbOhJHjO7Y0LyUw.png)\n\n旋转位置嵌入（RoPE）\\[6] 通过对序列中的标记嵌入进行旋转来建模标记的相对位置，从而解决了这个问题。\n\n让我们使用之前的例子，*‘Gemma 比 Llama 更好*，并考虑每个单词作为由 2D 向量表示的标记。单词 *better* 将由根据其位置 *m* 和一个常量角度 θ 从原始向量旋转而来的 2D 向量表示，如图 5 所示。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*nX3llo0cwBIrCQ8Gn21-gg.png)\n\n这种方法保留了标记之间的相对距离，因为旋转变换保持了向量之间的相似性，无论它们在序列中的位置如何。例如，如果我们在原始句子中添加两个单词，使其变为‘*The LLM Gemma 比 Llama 更好*’，则 *better* 和 *than* 的位置从 (3 & 4) 变为 (5 & 6)。然而，由于旋转角度保持一致，这些向量之间的相似性（通过点积测量）保持不变，从而确保了一致的相对位置。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*6cpWPXTexZC8YQbnasHOUg.png)\n\n## GeGLU 激活函数\n\n作者将传统的 ReLU 激活函数替换为一种称为 GeGLU 的门控线性单元（GLU）变体，因为另一项研究 \\[7] 表明它改善了 LLM 生成的输出质量。\n\nReLU 和 GeGLU 之间有两个区别：\n\n1. **激活函数** — GeGLU 使用高斯误差线性单元（GELU）\\[8] 函数，与 ReLU 的不同之处在于，它将神经元输入 *x* 乘以正态分布的累积分布函数。在这种情况下，*x* 被丢弃的概率随着 *x* 的减小而增加。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*FXCfQpvdMJXPk5s6AO-RuA.png)\n\n2\\. **Sigmoid 激活** — 简单的 ReLU 或 GELU 激活函数应用于隐藏表示 *x* 和两个由两个矩阵 (*W1* 和 *W2*) 表示的线性变换之间。GeGLU 中的门控变体对其中一个组件应用门控机制（sigmoid），如公式 3 所示。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Z9hUjuy4NvQVDrPj6iSfrQ.png)\n\n## Normalizer Location\n\n对原始Transformer架构的最后修改如图8所示。作者对每个transformer子层的输入和输出进行了归一化，以提高训练的稳定性，这与原始论文仅对输出进行归一化的做法相反。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*NQe4ME2MhvRWVzobVdIloA.png)\n\n他们还用RMSNorm \\[8]替换了传统的LayerNorm函数。RMSNorm在保持训练稳定性改进的同时，计算上更高效，并有助于模型收敛。\n\nRMSNorm实现了更好的效率，因为其作者证明LayerNorm的好处来自于重新缩放不变性，而不是重新中心化不变性。重新缩放不变性意味着，如果一个常数因子缩放输入，则归一化过程的输出保持不变。换句话说，将所有输入乘以一个常数不会影响归一化输出。重新中心化不变性意味着，如果一个常数值加到所有输入上，则归一化过程的输出保持不变。这意味着将所有输入平移一个常数量不会影响归一化输出。这个发现使得可以去掉计算均值的开销（只需计算标准差），从而使RMSNorm更简单、更高效。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*qblXBo8SCcxzPWePhFVlYg.png)\n\n## Mistral AI vs. Meta vs. Google: Gemma 7B、Llama 3 7B 和 Mistral 7B 的比较\n\n在本节中，我们将 3 个 LLM——Gemma 7B、Mistral 7B 和 Llama 3 7B——进行测试。我们使用一个名为 SQuAD 的问答数据集，遵循 CC BY-SA 4.0 许可证（可以在 [这里](https://huggingface.co/datasets/rajpurkar/squad) 找到）。该数据集是一个阅读理解数据集，包含关于一组维基百科文章的问题。根据上下文，模型应该能够检索到问题的正确答案。对于我们的用例，3 个最重要的字段是：\n\n* `question` - 模型应该回答的问题。\n* `context` - 模型需要从中提取答案的背景信息。\n* `answers` - 问题的文本答案。\n\n评估过程将包括两个定量指标：\n\n* `words per second` - 评估推理速度。\n* `words` - 评估答案的长度。\n\n为了评估模型在我们用例中的准确性，我们使用 RAQ \\[1]。RAQ 使用一个独立的 LLM 对所有 LLM 的答案进行排名，基于它们与真实答案的接近程度。\n\n我们首先下载以 `.gguf` 格式提供的模型，以便在 CPU 上运行，并将它们放在 `model/` 文件夹下。\n\n我们使用每个模型的指令版本，并进行了 4 位量化：\n\n* `mistral-7b-instruct-v0.1.Q4_K_M.gguf` 来自 [https://huggingface.co/TheBloke/Mistral\\-7B\\-Instruct\\-v0\\.1\\-GGUF/tree/main](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/tree/main)\n* `Meta-Llama-3-8B-Instruct-Q4_K_M.gguf` 来自 [https://huggingface.co/NousResearch/Meta\\-Llama\\-3\\-8B\\-Instruct\\-GGUF](https://huggingface.co/NousResearch/Meta-Llama-3-8B-Instruct-GGUF)\n* `gemma-7b-it-Q4_K_M.gguf` 来自 [https://huggingface.co/rahuldshetty/gemma\\-7b\\-it\\-gguf\\-quantized/tree/main](https://huggingface.co/rahuldshetty/gemma-7b-it-gguf-quantized/tree/main)\n\n之后，我们导入所有库和接收我们想要使用的模型作为参数的生成器。\n\n```python\nimport os\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scikit_posthocs as sp\nimport pandas as pd\nimport utils\n\nfrom dotenv import load_dotenv\nfrom datasets import load_dataset\nfrom generator.generator import Generator\n\nllama = Generator(model='llama')\nmistral = Generator(model='mistral')\ngemma = Generator(model='gemma')\nload_dotenv('env/var.env')\n```\n\n该类负责导入在 `config.yaml` 文件中定义的模型参数，具有以下特征：`context_length` 为 1024，`temperature` 为 0.7，`max_tokens` 为 2000\\。\n\n```python\ngenerator:\n  llama:\n    llm_path: \"model/Meta-llama-3-8B-Instruct-Q4_K_M.gguf\"\n  mistral:\n    llm_path: \"model/mistral-7b-instruct-v0.1.Q4_K_M.gguf\"\n  gemma:\n    llm_path: \"model/gemma-7b-it-Q4_K_M.gguf\"\n  context_length: 1024\n  temperature: 0.7\n  max_tokens: 2000\n```\n\n它还创建了提示模板。该模板有助于在将查询和上下文传递给 LLM 以获取响应之前格式化它们。\n\n```python\nfrom langchain import PromptTemplate\nfrom langchain.chains import LLMChain\nfrom langchain.llms import LlamaCpp\n\nfrom base.config import Config\nclass Generator(Config):\n    \"\"\"Generator, aka LLM, to provide an answer based on some question and context\"\"\"\n    def __init__(self, model) -> None:\n        super().__init__()\n    # template\n        self.template = \"\"\"\n            Use the following pieces of context to answer the question at the end.\n            {context}\n            Question: {question}\n            Answer:\n        \"\"\"\n   # load llm from local file\n        self.llm = LlamaCpp(\n            model_path=f\"{self.parent_path}/{self.config['generator'][model]['llm_path']}\",\n            n_ctx=self.config[\"generator\"][\"context_length\"],\n            temperature=self.config[\"generator\"][\"temperature\"],\n        )\n        # create prompt template\n        self.prompt = PromptTemplate(\n            template=self.template, input_variables=[\"context\", \"question\"]\n        )\n    def get_answer(self, context: str, question: str) -> str:\n        \"\"\"\n        Get the answer from llm based on context and user's question\n        Args:\n            context: most similar document retrieved\n            question: user's question\n        Returns:\n            llm answer\n        \"\"\"\n        query_llm = LLMChain(\n            llm=self.llm,\n            prompt=self.prompt,\n            llm_kwargs={\"max_tokens\": self.config[\"generator\"][\"max_tokens\"]},\n        )\n        return query_llm.run({\"context\": context, \"question\": question})\n```\n\n加载 LLM 后，我们从 HuggingFace 获取 SQuAD 数据集并对其进行洗牌，以确保问题主题的多样性。\n\n```python\nsquad = load_dataset(\"squad\", split=\"train\")\nsquad = squad.shuffle()\n```\n\n现在，我们可以循环处理 60 个问题和上下文，并记录上述指标。\n\n```python\nfor i in range(60):\n    context = squad[i]['context']\n    query = squad[i]['question']\n    answer = squad[i]['answers']['text'][0]\n\n    # Llama\n    answer_llama, words_per_second, words = utils.get_llm_response(llama, context, query)\n    llama_metrics[\"words_per_second\"].append(words_per_second)\n    llama_metrics[\"words\"].append(words)\n    # mistral\n    answer_mistral, words_per_second, words = utils.get_llm_response(mistral, context, query)\n    mistral_metrics[\"words_per_second\"].append(words_per_second)\n    mistral_metrics[\"words\"].append(words)\n    # gemma\n    answer_gemma, words_per_second, words = utils.get_llm_response(gemma, context, query)\n    gemma_metrics[\"words_per_second\"].append(words_per_second)\n    gemma_metrics[\"words\"].append(words)\n  \n    # GPT-3.5 rank\n    llm_answers_dict = {'llama': answer_llama, 'mistral': answer_mistral, 'gemma': answer_gemma}\n    rank = utils.get_gpt_rank(answer, llm_answers_dict, os.getenv(\"OPENAI_API_KEY\"))\n    llama_metrics[\"rank\"].append(rank.index('1')+1)\n    mistral_metrics[\"rank\"].append(rank.index('2')+1)\n    gemma_metrics[\"rank\"].append(rank.index('3')+1)\n```\n\n函数 `get_llm_response` 负责接收加载的 LLM、上下文和问题，并返回 LLM 答案以及定量指标。\n\n```python\ndef get_llm_response(model: Generator, context: str, query: str) -> Tuple[str, int, int]:\n    \"\"\"\n    Generates an answer from a given LLM based on context and query\n    returns the answer and the number of words per second and the total number of words\n    Args:\n        model: LLM\n        context: context data\n        query: question\n    Returns:\n        answer, words_per_second, words\n    \"\"\"\n    init_time = time.time()\n    answer_llm = model.get_answer(context, query)\n    total_time = time.time()-init_time\n    words_per_second = len(re.sub(\"[^a-zA-Z']+\", ' ', answer_llm).split())/total_time\n    words = len(re.sub(\"[^a-zA-Z']+\", ' ', answer_llm).split())\n    return answer_llm, words_per_second, words\n```\n\n我们可以看到，Llama 3 的速度快于 Mistral 和 Gemma，平均每秒生成约 0.7 个单词，而 Mistral 约为 0.26，Gemma 约为 0.4 个单词。在答案长度方面，Llama 3 也生成比 Mistral 和 Gemma 更长的答案，平均答案长度为 148 个单词，而 Mistral 为 20 个单词，Gemma 为 50 个单词。最后，根据 RAQ，Mistral 的平均排名最好，约为 1.81，其次是 Gemma，平均为 2.05，而 Llama 3 的表现较差，平均排名约为 2.1\\。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GVeFQbMZZ5oUScVEHQPu8A.png)\n\nRAQ 框架还包括统计检验，以了解观察到的差异是否显著。表 1 显示了 Dunn 事后检验的结果，比较不同语言模型的性能。每个单元格表示相应模型之间的性能差异在 5% 显著性水平下是否具有统计显著性。“显著”表示统计上显著的差异（p 值 ≤ 0.05），而“无显著性”表示没有统计上显著的差异（p 值 > 0.05）。对于所选的显著性水平，Dunn 检验结果表明模型之间的性能差异不显著。\n\n```python\np_values = sp.posthoc_dunn([Llama_metrics['rank'], mistral_metrics['rank'], gemma_metrics['rank']], p_adjust='holm')\np_values > 0.05\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ftCaagMKAm5RzeATYm_7Ug.png)\n\n定性评估一些示例始终很重要。以下是 3 个模型对问题 *‘Power House Day 在纽黑文的哪一天庆祝？’* 的回答，基于以下上下文：\n\n> ***Context:***‘一个多世纪以来，纽黑文市民一直与常规英国军队一起在殖民民兵中作战，就像在法国和印第安战争中一样。随着美国革命的临近，大卫·伍斯特将军和其他有影响力的居民希望与英国政府的冲突能够在不反叛的情况下解决。在 1775 年 4 月 23 日，这一天在纽黑文仍被庆祝为火药屋日，纽黑文的第二公司，州长的步兵卫队，参与了与统治英国议会的斗争。在本尼迪克特·阿诺德船长的指挥下，他们闯入火药库以武装自己，并开始了为期三天的游行，前往马萨诸塞州的剑桥。其他纽黑文民兵成员在场护送乔治·华盛顿，从他在纽黑文的过夜住宿前往剑桥。来自双方的当代报告都提到了纽黑文志愿者的专业军事风范，包括制服。’\n\n所有 3 个模型都给出了正确答案。虽然 Llama 3 和 Gemma 提供了更完整的答案，但 Mistral 则更加简洁。\n\n> ***Llama 3 answer:***‘纽黑文的火药屋日庆祝于 4 月 23 日。’\n\n> ***Gemma answer:***‘当然！文本中说明了火药屋日庆祝的日期：火药屋日在纽黑文庆祝于 **4 月 23 日**。’\n\n> ***Mistral answer:***’23 April’\n\n## 结论\n\n在设备上的模型为提升用户体验提供了极大的机会，使强大的 LLM 能够在计算资源较低的设备上使用。苹果和谷歌都在积极开发更小、更高效的模型，以满足这一需求，使更多人能够在日常生活中受益于先进的人工智能。\n\n在本文中，我们探讨了谷歌开发的开源 LLM Gemma，它在传统的 Transformer 架构中引入了四个新特性：2B 版本中的多查询注意力、用于位置编码的 RoPE 嵌入、作为激活函数的 GeGLU，以及输入归一化。\n\n我们还将 Gemma 的性能与 Llama 3 和 Mistral 在阅读理解数据集上的表现进行了比较。我们观察到，Gemma 每秒生成的单词数更多，写出的答案比 Mistral 更长，但在这些指标上并未超过 Llama 3。使用 RAQ 框架，我们评估了这三种模型的准确性。尽管数据表明 Mistral 的结果更佳，其次是 Gemma，但差异并不具有统计学意义。因此，我们可以说这三种模型在应用于我们的阅读理解用例时表现相似。\n\n## 参考文献\n\n\\[1] Luís Roque, Rafael Guedes. 从研究到生产：相对答案质量（RAQ）与NVIDIA NIM. [https://readmedium.com/research\\-to\\-production\\-relative\\-answer\\-quality\\-raq\\-and\\-nvidia\\-nim\\-15ce0c45b3b6](https://readmedium.com/research-to-production-relative-answer-quality-raq-and-nvidia-nim-15ce0c45b3b6), 2024\\.\n\n\\[2] Gemma Team, Google DeepMind. Gemma：基于Gemini研究和技术的开放模型, 2023\\.\n\n\\[3] Gemini Team. Gemini：一系列高能力的多模态模型, 2023\\.\n\n\\[4] Noam Shazeer. 快速Transformer解码：一只写头就足够了. arXiv:1911\\.02150, 2019\\.\n\n\\[5] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. 注意力即一切. arXiv:1706\\.03762, 2017\\.\n\n\\[6] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, Yunfeng Liu. RoFormer：带旋转位置嵌入的增强型Transformer. arXiv:2104\\.09864, 2021\\.\n\n\\[7] Noam Shazeer. GLU变体改善Transformer. arXiv:2002\\.05202, 2020\\.\n\n\\[8] Dan Hendrycks, Kevin Gimpel. 高斯误差线性单元（GELUs）. arXiv:1606\\.08415, 2016\\.\n\n\\[9] Biao Zhang, Rico Sennrich. 均方根层归一化. arXiv:1910\\.07467, 2019\\.\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/generating-structured-data-from-an-image-with-gpt-vision-and-langchain-34aaf3dcb215","frontmatter":{"title":"使用 GPT Vision 和 Langchain 从图像生成结构化数据","meta_title":"使用 GPT Vision 和 Langchain 从图像生成结构化数据","description":"在当今世界，视觉数据非常丰富，从图像中提取有意义信息的能力变得越来越重要……","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*FPRRg85jYb7MrzXEpNWbmw.jpeg","categories":["Programming","Computer Vision","Natural Language Processing"],"author":"Rifx.Online","tags":["Langchain","GPT","vision","LLMs","structured"],"draft":false,"slug":"blog/generating-structured-data-from-an-image-with-gpt-vision-and-langchain-34aaf3dcb215"},"content":"\n\n\n\n\n在当今这个视觉数据丰富的世界中，从图像中提取有意义信息的能力变得越来越重要。Langchain是一个强大的框架，用于构建大型语言模型（LLMs）应用程序，提供了一套多功能的工具来应对这一挑战。在本文中，我们将探讨如何使用Langchain从图像中提取结构化信息，例如计算人数和列出主要物体。\n\n在深入代码之前，让我们先了解一下任务的背景。想象一下你有一张场景的图像，比如城市街道。你的目标是从这张图像中提取有价值的信息，包括在场的人数和场景中的主要物体列表。\n\n## 关于 Langchain\n\nLangchain 是一个综合框架，允许开发者利用大型语言模型（LLMs）的强大功能构建复杂的应用程序。它提供了模块化和可扩展的架构，使开发者能够创建针对特定需求的自定义管道、代理和工作流。\n\nLangchain 简化了 LLM 的集成，提供了处理各种数据源（包括文本、图像和结构化数据）的抽象和工具。它支持来自不同提供商的广泛 LLM，例如 OpenAI 和 Anthropic，使得在单个应用程序中轻松切换模型或组合多个模型变得简单。\n\n## 准备环境并设置 OpenAI API 密钥\n\n要跟随本教程，您需要安装 Langchain。您可以使用 pip 安装它：\n\n```python\npip install langchain langchain_openai\n```\n要在 Langchain 中使用 OpenAI 语言模型，您需要从 OpenAI 获取一个 API 密钥。如果您还没有 API 密钥，可以在 OpenAI 网站上注册一个 (<https://openai.com/api/>)。\n\n一旦您拥有了 API 密钥，可以将其设置为系统中的环境变量，或者直接在代码中提供。以下是如何将 API 密钥设置为环境变量的示例：\n\n```python\nexport OPENAI_API_KEY=\"your_openai_api_key_here\"\n```\n或者，您可以直接在 Python 代码中提供 API 密钥：\n\n```python\nimport os\nimport langchain\nos.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key_here\"\n```\n在设置好 API 密钥后，Langchain 将能够与 OpenAI API 进行身份验证并使用他们的语言模型。\n\n## 加载和编码图像\n\n在我们使用 Langchain 处理图像之前，我们需要从文件中加载图像数据，并将其编码为可以传递给语言模型的格式。下面的代码定义了一个函数 `load_image`，该函数接受一个包含 `image_path` 键的字典，并返回一个新的字典，其中 `image` 键包含编码为 base64 字符串的图像数据。\n\n```python\ndef load_image(inputs: dict) -> dict:\n    \"\"\"Load image from file and encode it as base64.\"\"\"\n    image_path = inputs[\"image_path\"]\n  \n    def encode_image(image_path):\n        with open(image_path, \"rb\") as image_file:\n            return base64.b64encode(image_file.read()).decode('utf-8')\n    image_base64 = encode_image(image_path)\n    return {\"image\": image_base64}\n```\n`load_image` 函数首先从输入字典中提取 `image_path`。然后，它定义了一个嵌套函数 `encode_image`，该函数以二进制模式打开图像文件，读取其内容，并使用 Python 标准库中的 `base64.b64encode` 函数将其编码为 base64 字符串。\n\n`load_image` 函数使用提供的 `image_path` 调用 `encode_image`，并将结果 base64 编码字符串存储在 `image_base64` 变量中。最后，它返回一个新的字典，其中 `image` 键设置为 `image_base64`。\n\n为了将此函数集成到 Langchain 流水线中，我们可以创建一个 `TransformChain`，该链接受 `image_path` 作为输入，并生成 `image`（base64 编码字符串）作为输出。\n\n```python\nload_image_chain = TransformChain(\n    input_variables=[\"image_path\"],\n    output_variables=[\"image\"],\n    transform=load_image\n)\n```\n通过这种设置，我们可以轻松地将图像加载和编码作为更大 Langchain 工作流的一部分，从而使我们能够使用大型语言模型处理视觉数据和文本。\n\n## 定义输出结构\n\n在我们提取图像信息之前，需要定义我们希望接收的输出结构。在这种情况下，我们将创建一个名为 `ImageInformation` 的 Pydantic 模型，其中包括图像描述和我们可能想要提取的任何其他信息的字段。\n\n```python\nfrom langchain_core.pydantic_v1 import BaseModel, Field\n\nclass ImageInformation(BaseModel):\n \"\"\"Information about an image.\"\"\"\n image_description: str = Field(description=\"a short description of the image\")\n people_count: int = Field(description=\"number of humans on the picture\")\n main_objects: list[str] = Field(description=\"list of the main objects on the picture\")\n```\n\n## 设置图像模型\n\n接下来，我们将创建一个链，将图像加载和编码步骤与 LLM 调用步骤结合起来。由于 `ChatOpenAI` 模型在我的理解中并不具备同时处理文本和图像输入的能力，我们将创建一个包装链来实现这一功能。\n\n```python\nfrom langchain.chains import TransformChain\nfrom langchain_core.messages import HumanMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain import globals\nfrom langchain_core.runnables import chain\n\n## Set verbose\nglobals.set_debug(True)\n\n@chain\ndef image_model(inputs: dict) -> str | list[str] | dict:\n \"\"\"Invoke model with image and prompt.\"\"\"\n model = ChatOpenAI(temperature=0.5, model=\"gpt-4-vision-preview\", max_tokens=1024)\n msg = model.invoke(\n             [HumanMessage(\n             content=[\n             {\"type\": \"text\", \"text\": inputs[\"prompt\"]},\n             {\"type\": \"text\", \"text\": parser.get_format_instructions()},\n             {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{inputs['image']}\"}},\n             ])]\n             )\n return msg.content\n```\n在这个代码片段中，我们定义了一个名为 `image_model` 的链，使用提供的提示、格式说明和图像调用 `ChatOpenAI` 模型。`image_model` 链接受一个包含提示和 base64 编码图像字符串的字典 `inputs`。\n\n在链内部，我们创建了一个 `HumanMessage` 对象，该对象结合了提示文本、格式说明和图像 URL，以数据 URI 格式化，包含 base64 编码的图像数据。然后，我们使用这个 `HumanMessage` 对象调用 `ChatOpenAI` 模型，使用专门为涉及文本和图像的多模态任务设计的 `gpt-4-vision-preview` 模型。\n\n该模型处理文本提示和图像，并返回输出。\n\n## 整合所有内容\n\n现在我们已经拥有了所有必要的组件，我们可以定义一个函数来协调整个过程：\n\n```python\nfrom langchain_core.output_parsers import JsonOutputParser\n\nparser = JsonOutputParser(pydantic_object=ImageInformation)\ndef get_image_informations(image_path: str) -> dict:\n   vision_prompt = \"\"\"\n   Given the image, provide the following information:\n   - A count of how many people are in the image\n   - A list of the main objects present in the image\n   - A description of the image\n   \"\"\"\n   vision_chain = load_image_chain | image_model | parser\n   return vision_chain.invoke({'image_path': f'{image_path}', \n                               'prompt': vision_prompt})\n```\n在这个函数中，我们定义了一个提示，要求LLM提供图像中人物的数量和主要物体的列表。然后，我们创建一个链，将图像加载步骤（`load\\_image\\_chain`）、LLM调用步骤（`image\\_model`）和JSON输出解析器（`parser`）结合在一起。最后，我们用图像路径和提示调用这个链，函数返回一个包含提取信息的字典。\n\n## 示例用法\n\n要使用此功能，只需提供图像文件的路径：\n\n\n```python\nresult = get_image_informations(\"path/to/your/image.jpg\")\nprint(result)\n```\n这将输出一个包含请求信息的字典，例如：\n\n\n```python\n{\n 'description': 'a view of a city showing cars waiting at a traffic light',\n 'people_count': 5,\n 'main_objects': ['car', 'building', 'traffic light', 'tree']\n}\n```\n\n## 结论\n\nLangchain 提供了强大的工具集，用于处理大型语言模型并从各种数据源（包括图像）中提取有价值的信息。通过将 Langchain 的功能与自定义提示和输出解析相结合，您可以创建强大的应用程序，从视觉数据中提取结构化信息。\n\n请记住，输出的质量将取决于您使用的 LLM 的能力以及您提示的具体性。尝试不同的模型和提示，以找到最适合您用例的解决方案。\n\n如果您找到更好的方法来实现相同的结果或有改进建议，请随时在评论中分享。本文提供的代码示例旨在作为起点，可能还有其他方法或优化。\n\n"},{"lang":"zh","group":"blog","slug":"blog/glm-4-voice-9b-real-time-multilingual-voice-conversation-ai-install-locally-in-minutes-ce2fcd6c8fd8","frontmatter":{"title":"GLM-4-Voice 9B——实时多语言语音对话 AI——几分钟内即可在本地安装","meta_title":"GLM-4-Voice 9B——实时多语言语音对话 AI——几分钟内即可在本地安装","description":"GLM-4-Voice 9B 是一款实时多语言语音对话AI，支持英语和中文，具备情感语调和语速的可定制性。该模型通过端到端架构实现低延迟响应，提供更自然的互动体验。其主要组件包括标记化器、核心语言模型和解码器，能够直接处理语音输入和生成音频输出。用户可通过简单的本地设置步骤快速部署该模型，适合客户服务和教育等多种应用场景。","date":"2024-11-13T01:32:04.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*LATTpEc2AHvqgVyPKSzW7A.jpeg","categories":["Voice Assistants","Natural Language Processing","Chatbots"],"author":"Rifx.Online","tags":["multilingual","conversation","real-time","customization","performance"],"draft":false,"slug":"blog/glm-4-voice-9b-real-time-multilingual-voice-conversation-ai-install-locally-in-minutes-ce2fcd6c8fd8"},"content":"\n### 如何设置 GLM\\-4\\-Voice 9B 以实现无缝的实时语音交互，支持英语和中文，并探索其独特的架构、低延迟响应和可定制的声音属性。\n\n\n\n\n## 介绍\n\n近年来，语音启用的人工智能取得了显著进展，使对话代理能够更好地理解和响应人类语言。从虚拟助手到客户服务机器人，语音人工智能已成为各个行业的重要工具。然而，大多数模型在流利地切换语言、理解口语查询的细微差别以及提供高质量响应方面仍然面临挑战。这正是Zhipu AI的GLM-4-Voice脱颖而出的地方。GLM-4-Voice作为一款端到端的语音模型，推动了多语言对话人工智能的边界，支持英语和中文的实时对话，同时提供可适应且类人化的响应生成。\n\n在本文中，我们将探讨为什么GLM-4-Voice值得关注，它的独特之处，以及如何在本地设置和开始使用它。我们还将查看其架构，并提供访问网络演示的实用指南。\n\n## 为什么选择 GLM-4-Voice？\n\n传统的语言模型通常仅限于文本，并需要额外的处理层来处理语音。它们在交互性方面可能会遇到困难，或者存在延迟问题。GLM-4-Voice 通过一个统一的模型克服了这些限制，能够直接处理和生成语音。以下是它的突出之处：\n\n1. **端到端语音处理**：与许多依赖于单独的文本到语音 (TTS) 或语音到文本 (STT) 模块的模型不同，GLM-4-Voice 直接以口语形式进行解读和响应，从而提供更无缝和更具响应性的体验。\n2. **多语言支持**：该模型在处理英语和中文这两种全球广泛使用的语言方面表现出色。它流畅切换语言的能力使其非常适合双语环境和国际应用。\n3. **可定制属性**：GLM-4-Voice 允许在情感、语调、语速甚至方言上进行调整，使其能够生成更自然和情境合适的响应。\n4. **低延迟**：通过支持流式推理，该模型的延迟约为 20 个标记，使其能够在实时对话中实现近乎即时的响应。\n\n## GLM\\-4\\-Voice 的特点\n\nGLM\\-4\\-Voice 带来了几个独特的功能，使其与其他语音模型区别开来。以下是它的特别之处：\n\n* **实时语音互动**：通过支持低延迟响应，GLM\\-4\\-Voice 能够保持流畅自然的对话，这对客户支持和互动 AI 等应用至关重要。\n* **动态语音属性**：用户可以指定模型的情感语调、语速和其他特征，使互动更加生动且适合各种场景。\n* **具备上下文意识的双语支持**：该模型旨在理解和生成中文和英文的响应。它能够无缝切换这两种语言，为多语言应用提供灵活的解决方案。\n* **高级语音解码**：基于 CosyVoice，GLM\\-4\\-Voice 解码器能够实现高质量的语音生成，并支持流式传输，在两种语言中保持高清晰度。\n\n## 架构\n\nGLM\\-4\\-Voice 的架构由三个主要组件组成，每个组件在实现端到端语音交互中发挥着至关重要的作用：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*nJsKHtxSblNkixPIBZpWyQ.jpeg)\n\n1. **GLM\\-4\\-Voice\\-Tokenizer**：该组件将连续语音输入标记化为离散标记，每秒大约生成 12.5 个标记。标记器基于 Whisper 的编码器，并添加了向量量化，使模型能够以结构化形式处理音频。\n2. **GLM\\-4\\-Voice\\-9B**：核心语言模型，基于 GLM\\-4 架构，已调整为处理口语输入。它可以处理文本和语音，使其成为强大的多模态对话代理。\n3. **GLM\\-4\\-Voice\\-Decoder**：该解码器将离散标记转换回连续语音，使模型能够生成音频输出。它支持流式推理，使响应能够在处理几个标记后立即开始，从而最小化对话延迟。\n\n这些组件共同使 GLM\\-4\\-Voice 成为实时语音交互的强大工具，支持不同语言和方言的对话 AI。\n\n## 在本地设置 GLM\\-4\\-Voice\n\n要体验 GLM\\-4\\-Voice，请按照以下步骤在您的机器上本地设置该模型。\n\n### 第一步：克隆仓库\n\n首先从 GitHub 克隆仓库。确保包含子模块：\n\n```python\n!git clone --recurse-submodules https://github.com/THUDM/GLM-4-Voice\ncd GLM-4-Voice\n```python\n!git clone --recurse-submodules https://github.com/THUDM/GLM-4-Voice\ncd GLM-4-Voice\n\n```\n\n### 步骤 2：安装依赖\n\n进入项目目录并安装必要的依赖：\n\n```python\n!pip install -r requirements.txt\n```python\n!pip install -r requirements.txt\n\n```\n\n### 第3步：下载模型检查点\n\nGLM\\-4\\-Voice的解码器模型托管在Hugging Face上，需要`git-lfs`进行下载。确保已安装`git-lfs`，然后运行：\n\n```python\n!git clone https://huggingface.co/THUDM/glm-4-voice\n```python\n!git clone https://huggingface.co/THUDM/glm-4-voice\n\n```\n\n### 步骤 4：启动模型服务\n\n一切设置完成后，启动模型服务器：\n\n```python\npython model_server.py --model-path glm-4-voice-9b\n```python\npython model_server.py --model-path glm-4-voice-9b\n\n```\n\n### 第5步：启动Web服务\n\n一旦模型服务器运行，执行以下命令以启动Web服务：\n\n```python\npython web_demo.py\n```python\npython web_demo.py\n\n```\n\n您现在可以访问Web演示 [http://127\\.0\\.0\\.1:8888](http://127.0.0.1:8888) 与GLM\\-4\\-Voice进行交互。\n\n> **注意：** GLM\\-4\\-Voice模型资源密集，运行有效需要大量计算能力。具体来说，它需要35–40个GPU以实现最佳性能，因此适合在可访问高性能硬件的环境中部署。用户在尝试使用此模型之前，应确保具备必要的基础设施。\n\n## Web Demo Interface\n\nGLM\\-4\\-Voice 的网页演示提供了一个直观的界面，具有多种自定义选项：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*scbHOUXqMW5KGAcT3Bq1Eg.png)\n\n* **输入模式**：用户可以选择以文本或音频形式提供输入。这种灵活性允许无手操作或传统交互。\n* **语音控制参数**：调整温度、top\\-p 和令牌限制，以自定义模型的响应特性。\n* **调试信息**：显示输入和输出令牌，使用户能够洞察模型处理查询的过程。\n* **交互式音频显示**：音频输入和响应以波形形式显示，用户可以重播或查看音频片段以评估质量。\n\n然而，用于在演示中流式传输音频的 Gradio 有时可能会出现不稳定情况。为了获得最佳质量，建议在生成后重播对话框中的音频。\n\n## 结论\n\nGLM\\-4\\-Voice 在对话式人工智能领域中脱颖而出，提供了独特的双语支持、实时音频交互和灵活的响应定制。其端到端设计和低延迟使其成为客户服务、教育、虚拟助手等应用的最佳候选者。凭借易于访问的设置过程，GLM\\-4\\-Voice 为开发者和研究人员探索中文和英文的高级语音能力打开了大门。\n\n随着对更互动和真实的人工智能需求的不断增长，像 GLM\\-4\\-Voice 这样的模型代表了在消除语言和对话障碍方面的重要进展。无论您是想构建聊天机器人、虚拟教师还是客户服务代理，GLM\\-4\\-Voice 都提供了强大而灵活的解决方案。\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/google-gemini-are-big-context-windows-the-killer-feature-72ff95488fb1","frontmatter":{"title":"Google Gemini：大上下文窗口是杀手级功能吗？","meta_title":"Google Gemini：大上下文窗口是杀手级功能吗？","description":"Goggle 即将推出的法学硕士学位课程有了重大进展","date":"2024-11-10T22:36:54.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MteQrQSTXLuJcd86RbjQrg.png","categories":["Machine Learning","Natural Language Processing","Data Science"],"author":"Rifx.Online","tags":["Gemini","tokens","context","LLM","evolution"],"draft":false,"slug":"blog/google-gemini-are-big-context-windows-the-killer-feature-72ff95488fb1"},"content":"\n### 谷歌即将推出的 LLM 迈出了重大一步\n\n\n\n就在八个月前，一封泄露的谷歌电子邮件透露该公司在努力超越其 AI 竞争对手方面遇到了困难。他们的 AI 产品周围不仅没有[护城河](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither)——换句话说，没有建立起商业优势——谷歌也没有[秘密武器](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither)可以改变局面。即使在他们努力解决这个问题时，他们也看到私募资助的 AI 项目与开源 AI 模型之间的差距以“惊人的”速度缩小。\n\n现在还为时已晚，无法知道这个故事的结局。也许开源 AI 将继续在早期成功的基础上发展，或者它将被谷歌、微软和苹果等极其富有的竞争对手及其令人难以置信的数据量所压制。现在，这场冲突仍在展开，各个组织快速推出一系列 AI 进展。最近，谷歌在这个领域中成为焦点，宣布了其最新 LLM 的预览版——[Gemini 1.5 Pro](https://deepmind.google/technologies/gemini/)。又是一天，又一个大型语言模型——或者说似乎如此，直到谷歌描述了一个惊人的变化。\n\nGemini 1.5 Pro 扩展了 *上下文窗口*——本质上是衡量 LLM 一次可以跟踪多少数据的指标。在过去的版本中，Gemini 的上下文窗口最大为 128,000 个标记，就像 GPT-4 一样。但 Gemini 的新上下文窗口可以容纳 **100 万** 个标记，这一变化的影响是巨大的。\n\n但在我们讨论上下文窗口对 LLM 能力的影响之前，我们需要回顾一下上下文窗口的工作原理。\n\n## 上下文窗口（简而言之）\n\n简单来说，上下文窗口设置了 LLM 在交互过程中能够记住多少信息。例如，如果您正在使用 ChatGPT，上下文窗口包括您给它的当前提示、您之前在该对话中输入的所有内容，以及 ChatGPT 向您发送的每个回复。对话时间长了，旧的对话部分将会从上下文窗口中滑出，ChatGPT 将突然忘记那些细节。\n\n128,000 个令牌的上下文窗口听起来很大，但这个数字具有误导性。首先，考虑到一个平均单词在为 LLM 分解时实际上是 1 到 3 个令牌。（经验法则是 4 个令牌对应 3 个单词，但随着语言变得更加复杂或在专业领域（如法律或医学）中，这个数字会增加。）当您查看长文档、进行持续交互和 AI 驱动的应用程序时，您会很快发现您无法将所有希望 LLM 知道的内容都放入其上下文窗口中。\n\n因此，我们开发了一些巧妙的方法来解决上下文窗口的限制。例如：\n\n* **分块。** 您可以将大量数据分解，让 LLM 一次查看一部分。这对于某些任务（总结长文档）效果很好，但如果您需要分析跨整个文档的概念，则效果不佳。\n* **微调。** 您可以用特定的数据训练 LLM。除了时间和费用之外，关键问题是您的新数据很容易被 LLM 已经吸收的更大规模的通用训练数据所淹没。通常，它就是无法保留。此外，许多 LLM 根本不支持微调——包括 GPT-4 和 Gemini。\n* **检索增强生成 (RAG)。** 首先，您将文本内容转换为一种特殊表示，称为 *嵌入*。（嵌入是 LLM 工作的重要部分。基本上，它们是捕捉内容含义的数值表示。）一旦您有了嵌入，您就将它们放入向量数据库中。现在，您可以使用 *语义搜索* 的魔力查看提示，并在数据库中找到与之概念相关的内容片段，然后将其输入 LLM。换句话说，您只给它提供重要的内容。\n\n最后一点是今天最常见的方法。RAG 高效且可预测。如果您拥有大量松散相关的文档，它效果非常好。例如，想象一下您正在创建一个技术支持聊天机器人，它从您公司的知识库文章中获取信息。使用 RAG，您找到相关数据，并将其与您的提示一起提供给 LLM。基本上，您是在告诉 LLM 在回答提示时该去哪里查找。\n\n但 RAG 并不完美。它迫使您花费更多时间准备数据。它不容易让您跳入一个全新的数据集。如果您确实需要一次考虑大量信息——例如，您在寻找小说中的整体主题或代码库中的特征——那么它就不够有效。但尽管有其局限性，RAG 今天仍然接近最佳实践。\n\n至少，在 Gemini 1.5 Pro 翻转剧本之前是这样的。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*EEHKDSH0wXa-J6veK5etZA.png)\n\n## 惊艳时刻\n\n尽管 Gemini 1\\.5 Pro 尚未发布，但它已经在一个严格限制的试用中可用。结果令人瞩目。\n\n一些最令人印象深刻的例子展示了 Gemini 创建的分析，涵盖了大量知识。谷歌的演示一如既往地令人印象深刻，但他们过去曾被指控进行演示操控和选择性展示。我更感兴趣的是独立测试者，他们报告的结果同样引人注目。\n\n例如，Conor Grennan [向 Gemini 提供了一部 300 页的小说](https://www.youtube.com/watch?v=-MKGsijn5tI)，并要求它描述主要角色、找出情节转折，并识别角色感受特定情绪的例子。Gemini 在整部书的范围内发展细致的论点毫无困难。YouTube 上流行的 [Fireship 频道](https://www.youtube.com/c/fireship) 的创作者 Jeff Delaney 向 Gemini 提供了一个包含数千个文件的完整代码库，并要求它添加新功能。Gemini 不仅写出了正确的代码，还遵循了现有项目的风格，使用了已经建立的组件、库和约定。其他演示展示了 Gemini 识别应用程序中的问题、提取关键示例并编写 API 文档。\n\n如果你想要其他内容来填充 Gemini 巨大的上下文窗口，还有另一个新功能——视频。视频的标记方式与文字不同，占用的空间要大得多。但即便如此，1 百万标记的上下文窗口可以容纳大约一个小时的视频——足够浏览一部电影并回答有关其内容的复杂问题。这就是谷歌所做的，当它要求 Gemini [查找具体细节](https://www.youtube.com/watch?v=wa0MT8OwHuk) 在一部巴斯特·基顿的电影中，比如在他们未识别的一个场景中，纸片上写的字。\n\n## 未来的LLM\n\n大上下文窗口是未来的方向吗？到目前为止，普遍的看法是，大上下文窗口充其量只是一个部分解决方案。我们担心它们在计算时间上会过于昂贵。[一项研究](https://www.voiceflow.com/blog/the-context-window-paradox-why-bigger-might-not-be-better)发现，LLM在长上下文窗口中找到信息的能力并不好，反而在细节出现在开头或结尾时表现更佳。所有这些因素支持了同样的结论：将你的内容强行塞入上下文窗口是天真的且成本高昂的。将所有数据一次性发送请求绝不是与LLM对话的正确方式。\n\n现在，未来似乎突然发生了变化。大上下文窗口即将来临，它们可能使LLM对广泛知识集有更强大、整体的理解。去年用文本无法完成的任务现在即将在*视频*中变得可能。而谷歌研究正在尝试一种扩展上下文窗口到惊人的1000万标记的Gemini变体。\n\n两个事实是明确的。首先，在LLM战争中选择赢家是一场愚蠢的游戏。其次，变化的速度没有放缓——反而在加速。\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/google-releases-gemma-a-lightweight-and-open-source-model-b6411d67ecca","frontmatter":{"title":"Google 发布 Gemma — 轻量级开源模型","meta_title":"Google 发布 Gemma — 轻量级开源模型","description":"Google 发布了 Gemma，这是一系列轻量级开源模型，基于创建 Gemini 的研究和技术构建……","date":"2024-10-29T12:46:34.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*G7XbkhsCwillpje7AvETjQ.jpeg","categories":["Natural Language Processing","Programming","Chatbots"],"author":"Rifx.Online","tags":["Gemma","Gemini","parameters","NLP","chatbots"],"draft":false,"slug":"blog/google-releases-gemma-a-lightweight-and-open-source-model-b6411d67ecca"},"content":"\n\n\n\n\n在短短一周内，世界见证了两家科技巨头带来的最具突破性的AI进展。OpenAI推出了令人惊叹的AI视频生成器[Sora](https://readmedium.com/3d16381f3bf5)，而谷歌则揭晓了其[Gemini 1.5模型](https://generativeai.pub/google-releases-gemini-1-5-with-1m-context-window-44ed4a2ea319)，能够支持最多100万的上下文窗口。\n\n今天，谷歌再次引发轰动，发布了[Gemma](https://ai.google.dev/gemma/?utm_source=keyword&utm_medium=referral&utm_campaign=gemma_cta&utm_content)，这是一个轻量级、最先进的开源模型家族，建立在用于创建Gemini模型的研究和技术基础之上。\n\n## 什么是 Gemma？\n\nGemma 以拉丁语 *gemma* 意为“珍贵的宝石”命名，汲取了其前身 Gemini 的灵感，反映了其在科技领域的价值和稀有性。\n\n它们是文本到文本、仅解码的大型语言模型，提供英语版本，具有开放权重、预训练变体和指令调优变体。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Fu2ryJMunebq5c0dD-opZQ.png)\n\nGemma 从今天起在全球范围内提供，分为两种尺寸（2B 和 7B），支持广泛的工具和系统，并可在开发者的笔记本电脑和工作站上运行。\n\n## 2 模型大小和能力\n\nGemma 模型有 20 亿和 70 亿参数两种规模。2B 模型旨在运行在移动设备和笔记本电脑上，而 7B 模型则适用于桌面计算机和小型服务器。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*sH9jaz1RvtKeJ5yjfyOL5Q.png)\n\n**调优模型**\n\nGemma 还有两个版本：调优版和预训练版。\n\n* **预训练：** 这就像基础模型，没有任何微调。该模型没有针对 Gemma 核心数据训练集以外的特定任务或指令进行训练。\n* **指令调优：** 该模型经过微调，以适应人类语言交互，从而提高其执行特定任务的能力。\n\n## 它与竞争对手的比较？\n\n由于体积小，Gemma能够直接在用户的笔记本电脑上运行。下图显示了Gemma (7B)的语言理解和生成性能与类似规模的开放模型如LLaMA 2 (7B)、LLaMA 2 (13B)和Mistral (7B)的比较。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*QxjZALUAIDiS_T66EpOu-g.png)\n\n您可以在[这里](https://ai.google.dev/gemma/?utm_source=keyword&utm_medium=referral&utm_campaign=gemma_cta&utm_content)查看每个基准的更详细比较。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Fc8Fk0Dgh2VFU_VLhpcs6Q.png)\n\n## 它的用途是什么？\n\n以下是 Gemma 可能的使用场景：\n\n**内容创作与沟通**\n\n* 文本生成\n* 聊天机器人和对话式 AI\n* 文本摘要\n\n**研究与教育**\n\n* **自然语言处理 (NLP) 研究：** 作为 NLP 研究的基础，实验技术，开发算法，并为该领域的进步做出贡献。\n* **语言学习工具：** 支持互动语言学习体验，帮助语法纠正，或提供写作练习。\n* **知识探索：** 帮助研究人员通过生成摘要或回答特定主题的问题来探索大量文本。\n\n以前需要极大模型的任务现在可以通过最先进的小型模型来实现。这开启了开发 AI 应用程序的全新方式，我们很快可能会在智能手机上看到无需互联网连接的设备内 AI 聊天机器人。\n\n这有多令人兴奋呢？\n\n## 这真的好吗？\n\n几位 [redditors](https://www.reddit.com/r/LocalLLaMA/comments/1awbqwd/gemma_7b_the_latest_opensource_model_from_google/) 分享了他们使用 Gemma 的经验，到目前为止，结果并不理想。看看这个例子，Gemma 在回答关于重量的问题时给出了错误的答案。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Sdaiaqcuz7qbftG1)\n\n我自己还没有真正尝试过，但重要的是要记住，像这样的较小模型预计会有一些缺陷，有时可能会给出错误的答案。\n\n## 尝试自己动手\n\n您可以今天开始使用Gemma，通过Kaggle的免费访问、Colab笔记本的免费层以及首次使用Google Cloud的用户可获得的$300信用额度。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BrvLnczy724TPrsk-uFJCw.png)\n\n如果您有兴趣开始使用Gemma，请查看这些指南，以了解从文本生成到在Gemma模式下部署的过程：\n\n* **使用Gemma进行文本生成**：构建一个基本的文本生成示例。\n* **使用LoRA调优Gemma**：对Gemma 2B模型进行LoRA微调。\n* **使用分布式训练调优Gemma模型**：使用Keras和JAX后端对Gemma 7B模型进行LoRA和模型并行的微调。\n* **将Gemma部署到生产环境**：使用Vertex AI将Gemma部署到生产环境。\n\n## 下载模型\n\n开放模型目前可在 [HuggingFace](https://huggingface.co/models?other=gemma&sort=trending&search=google) 上获取。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*mJRzGhO1sUxPL4_3YjpNGA.png)\n\nGemma 模型也可以从 [Kaggle Models](https://www.kaggle.com/models/google/gemma) 下载。\n\n## 最后的思考\n\n虽然Gemma模型可能体积小且缺乏复杂性，但它们在速度和使用成本上可能会有所弥补。\n\n从更大的角度来看，谷歌并不是追逐短期的消费者兴奋，而是在为企业培育市场。他们设想公司会为谷歌云服务付费，因为开发者使用Gemma来创建创新的新消费应用。\n\n此外，尽管Gemini的反响平平，谷歌仍然展示了它还有更多的秘密武器。\n\n当然，对于任何强大的技术来说，真正的考验是它的实际效果。谷歌的过去引发了一个问题：这些模型在现实世界中的表现是否能如承诺的那样出色。密切关注这一点是重要的，但也希望谷歌能从过去中吸取教训，提供真正可比甚至优于竞争对手的模型。\n\n我迫不及待想要体验Gemma，并且我一定会分享我对这个新AI模型的初步想法和发现。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*8BDnUV9iQisOyeN3.png)\n\n这篇文章发布在[Generative AI](https://generativeai.pub/)。请在[LinkedIn](https://www.linkedin.com/company/generative-ai-publication)上与我们联系，并关注[Zeniteq](https://www.zeniteq.com/)，以获取最新的AI故事。让我们一起塑造AI的未来！\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*JeeoUhaBYUJGr0Xq.png)\n\n"},{"lang":"zh","group":"blog","slug":"blog/how-agentic-rag-solves-problem-with-current-rag-limitations-4402ef7f8448","frontmatter":{"title":"Agentic RAG 如何解决当前 RAG 限制的问题","meta_title":"Agentic RAG 如何解决当前 RAG 限制的问题","description":"在《咖啡休息概念》第 4 卷中，我们将了解 AgenticRAG 如何帮助解决传统 RAG 的局限性。","date":"2024-11-04T12:34:57.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*abCDtDjfKZDJzginIc1UPA.png","categories":["Generative AI","Data Science","Machine Learning"],"author":"Rifx.Online","tags":["Agentic","RAG","agents","query","routing"],"draft":false,"slug":"blog/how-agentic-rag-solves-problem-with-current-rag-limitations-4402ef7f8448"},"content":"\n在本卷咖啡休息概念的第 4 期中，我们将了解 AgenticRAG 如何帮助解决传统 RAG 的限制。\n\n## RAG框架\n\nRAG（检索增强生成）框架按特定顺序操作：\n\n文档 \\-\\> 片段 \\-\\> 向量数据库 \\-\\> 片段检索（前K个） \\-\\> LLM\n\n然而，这一顺序**在处理某些类型的查询时会遇到障碍。**\n\n\n\n## 问题 1：摘要\n\n考虑一个查询，比如“总结文档”。\n\n* 传统的 RAG 方法检索前 K 个块并进行摘要。\n* 但如果检索文档的所有块并进行总结，岂不是更全面吗？\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*gIb0RNALIItt4UmyVfPRZg.png)\n\n## 问题 2：比较文档\n\n* 在比较文档 A 和文档 B 时，**基本 RAG 检索随机片段并尝试比较这些前 K 个片段**。\n* 这**并不能准确反映**文档的整体情况。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*pJuKlKx1unDAvKmmp_1Rlg.png)\n\n## 问题 3：结构化数据分析\n\n考虑一个问题：“**下一个休假是什么时候？**”。\n\n* 第一步是从结构化表中检索员工所属的区域。\n* 根据该区域，从休假政策文件中提取该区域的下一个休假。\n* 在当前的 RAG 框架下，这个过程并不是那么简单。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*XZuMz9EXtb_m28l4Ox27lQ.png)\n\n## 问题 4：多部分问题\n\n考虑一个问题，例如“**识别所有地区的共同请假？**”。\n\n* 想象一下，您有一份在 120 个国家运营的公司的请假政策文件。\n* 由于您正在传递前 K 个上下文，**可以比较的最大地区数量限制为 K**，其中 K 是传递给 LLM 的块的数量。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*l0FY6rI_UK9k9TW-nEJO7w.png)\n\n查看我们的 **AgenticRAG with LlamaIndex** 课程，包含 **5 个实时案例研究**。\n\n课程链接：[https://www.masteringllm.com/course/agentic\\-retrieval\\-augmented\\-generation\\-agenticrag](https://www.masteringllm.com/course/agentic-retrieval-augmented-generation-agenticrag)\n\n## Agentic RAG\n\nAgentic RAG 可以通过自定义代理来解决这 4 个问题。\n\n* 代理将与多个系统进行交互。\n* RAG 现在是代理可以使用的系统的一部分。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Su8LiYNG4lv4jvuCQAhYdg.png)\n\n* 代理使用 LLMs 来自动化推理和工具选择\n* RAG 只是代理可能决定使用的另一个工具。\n\n## 路由代理\n\n* 路由代理是简单的代理，用于路由查询。\n* 一个代理可以在一个或多个工具中路由查询。\n* 请记住我们的问题“**总结文档**”或如果我们想结合“**总结 \\+ 语义搜索**”的问题，可以使用以下示例路由来解决。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*43Y9jlYoXDb0BbUoYCcKrg.png)\n\n## 查询规划代理\n\n* 查询规划代理将查询分解为子查询。\n* 每个子查询都可以在 RAG 管道上执行。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*32Ng2zpxNWXhQZ3CaLcFeA.png)\n\n## 代理的工具\n\n* LLMs 可以拥有多个工具，例如调用 API，推断 API 的参数。\n* RAG 现在是 LLM 可能使用的一个工具。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Z1viCXkfah_5JJM2Ty6Kjw.png)\n\n## 摘要\n\n* RAG 在处理复杂问题时存在局限性。\n* 一些用例，如总结、比较等，仅靠 RAG 无法解决。\n* Agentic RAG 可以帮助克服 RAG 的局限性。\n* Agentic RAG 将 RAG 视为可用于语义搜索的工具。\n* 配备路由、查询规划和工具的代理能够超越传统的 RAG 应用。\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/how-i-wrote-a-whole-book-with-chatgpt-in-less-than-3-hours-798139987617","frontmatter":{"title":"我如何用 ChatGPT 在不到 3 小时内写完一整本书？","meta_title":"我如何用 ChatGPT 在不到 3 小时内写完一整本书？","description":"并在 Twitch 上直播了整个过程！","date":"2024-11-10T03:51:17.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*I-QkGOILay2F7ROR53b3KA.jpeg","categories":["Chatbots","Machine Learning","Generative AI"],"author":"Rifx.Online","tags":["ChatGPT","machine-learning","prompt-engineering","content-creation","consciousness"],"draft":false,"slug":"blog/how-i-wrote-a-whole-book-with-chatgpt-in-less-than-3-hours-798139987617"},"content":"\n\n\n\n\n## 解密人工智能热潮\n\n我叫亚历克斯，我只是一个在金融科技（Fintech）领域工作的小伙子，这个行业不可避免地让你对一切充满好奇，尤其是新趋势。我无法摆脱人工智能的热潮，或者说，我无法不去观察人们对它的疯狂反应。\n\n“人工智能会抢走你的工作！”，“这就是结束！”，“到2024年，你将不再见到医生。机器将为你诊断和治疗！”，“我如何利用ChatGPT创建了一家全新的公司！”，最后，“我如何在10分钟内用ChatGPT写了一整本书并因此致富！”，这些承诺听起来更像是营销口号，而不是现实场景。\n\n自从人工智能热潮开始，原因对我来说显而易见，但显然对90%的人来说并非如此。人工智能不是魔法，也不是“人类所需的最后一项发明”。甚至称其为人工智能在技术上也不准确。谈到大型语言模型（LLMs）如ChatGPT时，最正确的术语是机器学习（ML）或深度学习（DL）。\n\n### 机器学习与深度学习\n\n机器学习（ML）专注于开发能够从先前数据中学习并做出决策的算法。ML系统不是通过明确编程来执行任务，而是通过识别大型数据集中的模式和关系来预测未来结果或对信息进行分类。\n\n机器学习的常见应用包括垃圾邮件检测、内容推荐系统、图像识别和自然语言处理。\n\n深度学习（DL）是机器学习的一个专门分支，涉及旨在模拟人脑结构和功能的网络，使计算机能够识别数据中的复杂模式和表示。DL模型在处理大量非结构化数据（如图像、音频和文本）方面表现出色，因此在图像分类、语音识别和自然语言理解等任务中尤其有效，例如Siri或Alexa。\n\n深度学习在复杂任务中实现最先进性能的能力震惊了公众，彻底改变了计算机视觉、语音识别和自动化等领域。人们现在担心一个悲惨的未来，在这个未来中，人工智能绘画、沉迷于诗歌、作曲和写书，而人类则沦落到翻汉堡和为少数富有的人工智能领主送餐。\n\n## 人类智慧超越学习\n\n然而，学习只是动物和人类智慧广阔领域的一个方面。“AI”无法嗅觉、感受冷热、体验情感、做梦，但最重要的是，AI无法思考，这与许多人所相信的不同。每一个ChatGPT的输出都不是纯粹的思考，而是对输入其数据库的过去数据的精炼重组，并通过其神经网络进行处理。OpenAI可用的计算能力如此巨大，以至于他们可以通过神经网络反复处理数据，使最终结果看起来100%可信，仿佛是由一个真实的人执行、撰写、绘制或演唱的。\n\n然而，现实总是复杂得多，甚至无聊得多。在深度学习的伎俩背后，并没有全知的实体，没有天网，也没有《黑客帝国》的孵化。只是一种对先前数据点的高级电影摄影，将它们合并在一起并以极快的速度执行，以至于人眼会被欺骗，以为创造新事物的机器实际上是活着的。它确实接近于电影的概念：许多静态图像快速滚动，人眼将看到其中的实际运动，即“动态影像”。实际上，这些图像只是静态的，而在动画电影的情况下，绝对是虚构的。\n\n现在，仅仅因为电影和AI是虚构的，并不意味着它们的影响不真实。电影可以在观众中产生真实的情感，聚集真实的人，并引发观众之间的真实对话和争议。同样，深度学习实际上可以从人类那里夺走一些工作，创造全新的艺术作品，并写下有意义的文本，无论是虚构的还是非虚构的。在这里，AI营销得到了最大的推动。写作是最简单的表达形式。只需要一点意愿就可以开始。难怪写作如今是最普及的表达形式。我们有数十亿人在其一生中的某个时刻写作，发布的文本更是数不胜数，涵盖了从经典小说到复杂科学论文，从杂志到现代博客和社交媒体帖子。人类的写作提供了迄今为止ChatGPT训练的大部分信息。\n\n## 通过实践学习\n\n写作是ChatGPT最擅长的事情，这也是吸引那些总是在寻找下一个轻松赚钱机会的社交媒体“专家”的原因。在观看了那些常见嫌疑人的搞笑广告和内容后，我提出了以下问题：\n\n* 是否可以用ChatGPT写一本完整的书并因此致富？\n* ChatGPT和其他大型语言模型的限制和约束是什么？\n* 如果写一本书是可能的，完成任务并取得最佳结果的最有效和有组织的方法是什么？\n\n我得出的结论是，回答这些问题的最好方法是通过实践学习，就像动物、人类或深度学习算法一样！\n\n还有什么比与观众一起学习更好的方式呢？我一直想在Twitch上直播一些内容，而这个主题看起来真是个绝妙的广播话题！\n\n首先，我必须做好准备。我不能仅仅随波逐流。我需要一个计划，从选择主题开始。我不能指望写出下一个《神曲》。我必须保持现实的期望。小说一般被排除在外。通过每天与ChatGPT合作，我明白这个家伙最适合非小说类内容。\n\n我有了类型，很好。但主题和衍生内容呢？我知道提示不能太简单，比如“嘿，ChatGPT。写下一个非小说类畅销书！”\n\n一个有效的提示应该是结构化的。人工智能最能服务于那些知道自己想要什么的人。那些不知道自己想要什么的人在与肉体同伴沟通时会面临同样的困难。\n\n考虑到我的小丑本性，我希望这本书是一个讽刺，可能会嘲笑一些人们过于严肃对待的作品，即使他们不该如此。我在考虑那些最受欢迎的互联网名人，那些被视为神灵的存在，像埃隆·马斯克、安德鲁·泰特或亚历山大·杜金这样的黑暗实体。然而，他们并不是实际的作者，或者至少据我所知，他们并没有创作出任何震撼公共领域的显著作品。我需要一个真正写过畅销书的人，并对其进行讽刺！\n\n## 如何使用 ChatGPT 实际写一本书\n\n### \\#1 选择正确的主题\n\n经过一番无果的头脑风暴，YouTube给了我答案。这个平台是人们需求的晴雨表，不久之后，我的动态中出现了一些与乔丹·彼得森的“争议性”（点击诱饵）访谈摘录。接着，我遇到了一篇在Medium上攻击彼得森的文章，我觉得这方向是对的。我查看了他最受欢迎的书籍，[***12 Rules for Life: An Antidote to Chaos***](https://proxy.rifx.online/https://www.amazon.com/12-Rules-for-Life-audiobook/dp/B0797Y87JC)，并找到了切入点。这是一个符合我寻找的GPT\\-恶搞所有特征的主题：\n\n* 这是一本畅销书，销量达数百万册，\n* 它有一个有效的标题，能够吸引读者的注意，即使在我们今天所处的竞争激烈的注意力经济中，\n* 对于ChatGPT来说，这似乎是一个简单的主题，因为它易于结构化、总结和分解成列表和要点。\n\n最重要的是，关于生活的12条规则的书籍给了ChatGPT展示它从庞大的训练数据中吸收的智慧，以及与用户的互动中学到的知识的机会！\n\n### \\#2 什么让艺术有价值：痛苦！\n\n然而，仅仅拥有正确的话题是不够的。通过观察AI艺术，我明白了为什么AI永远无法取代人类艺术家。赋予一件艺术作品——无论是画作、歌曲还是书籍——价值的，不是最终的结果，而是其背后的故事。当我们阅读但丁的《地狱篇》时，我们会想，诗人是如何想到那些强烈而生动的意象的，这让我们怀疑这是否是一部虚构作品，或者一个活生生的人是否真的成功穿越了来世的门。当我们听到皇后乐队的最新专辑时，我们不禁想象弗雷迪·默丘里的痛苦；这位传奇歌手在生命的最后几个月与艾滋病斗争，同时淡出公众视野，仅通过他的音乐表达自己。当我们凝视穆奈的《呐喊》时，我们立刻与他的存在危机产生共鸣，这种危机的爆发导致了我们所称之为“杰作”的作品。技巧并不是使一件作品成为杰作的原因，而是创作者倾注于作品中的灵魂和痛苦。\n\n痛苦是意识的支柱。由于机器不具备痛苦，因此它们无法拥有意识。\n\n我如何能将“灵魂”和“痛苦”融入AI生成的书籍中？答案比你想象的要简单。我必须做一些我一直想做但又总觉得不舒服的事情。我必须出现在镜头前，面对一个虚拟观众，做我一直在做的事情，但在观众的压力下。我必须冒着失去面子的风险，成为“那个无法使用ChatGPT的人”，“那个试图用AI写书却连说话都做不到的傻瓜”。我必须挑战自己，同时也挑战ChatGPT本身。大型语言模型也有很多要失去的。如果我的Twitch表演因我的失败而变得病毒式传播，我将成为众矢之的。但如果组装一本真正的书的任务失败，ChatGPT将被贴上“一个价格过高的AI，承诺能做一切，最终却什么都做不了，连一本简单的书都写不出来”的标签。\n\n我认为这个策略是赋予AI作品那种能够使其有价值、从而可销售的痛苦的最佳方式。\n\n### \\#3 在写实际书籍之前进行测试\n\n在进行这项令人畏惧的任务之前，我进行了测试。我请 ChatGPT 写另一本书，这次是关于加密货币及其行业中需要避免的危险。我在加密货币方面工作很多，并且自己也进行了投资，因此我可以轻松检查 ChatGPT 是否提供了事实信息或产生了幻觉。\n\n我还可以测试开发这本书的最佳策略。我从一开始就知道，使用一个单一的提示写完整本书是不可能的，更不用说在一次对话中完成了。ChatGPT4 每个提示请求可以生成大约 1,000/2,000 字，而一次对话可以记住大约 25,000 字的上下文。考虑到一本典型的非小说类书籍包含大约 100,000 字，你就可以想象我的策略。\n\n我必须创建小的子任务，比如每章进行一次对话。但是我怎么能在不同的对话中保持相同的上下文呢？我是否应该在每次对话中重复相同的主提示和 12 条规则？这看起来并不高效。\n\n### \\#4 向最佳提示工程师学习\n\n我必须感谢 [Sheila Teo](https://proxy.rifx.online/https://readmedium.com/undefined)，她教会了我如何以最有效的方式使用 LLM。通过阅读 Teo 的 Medium 文章 [*我如何赢得新加坡的 GPT\\-4 提示工程比赛*](https://proxy.rifx.online/https://towardsdatascience.com/how-i-won-singapores-gpt-4-prompt-engineering-competition-34c195a93d41)，我理解了“系统提示”的本质。系统提示告诉你的 LLM 在不同的对话中该做什么以及该记住什么。系统提示的一个例子可以是：\n\n```\nI need to write a book about the most dangerous scams in crypto and how to avoid them.The book will be divided in 5 chapters:1\\. Ponzi schemes2\\. Pump and dump schemes3\\. Ransomwares4\\. Fake tokens5\\. Fake trading platformsThe tone will be humorous and satirical, but also informative.We will write one chapter per conversation.\n```\n\n如果你在日常工作中使用 ChatGPT 进行重复性任务，系统提示会非常有帮助。它们确保 LLM 会保持在正确的轨道上，并减少幻觉的风险，即提供错误或不相关的信息。\n\n### \\#5 创建您自己的个人 GPT\n\n为了个性化设置，我将系统提示提升到了一个新的层次。ChatGPT 现在提供创建自定义 GPT 的可能性。这些是您可以针对特定任务进行训练的个性化机器人。输出将更加精确，因为模型不会在 OpenAI 提供的庞大数据宇宙中迷失，而是更专注于您需要做的事情。例如，一个针对图像生成进行训练的 GPT 将使用 DALLE-2 输出比使用通用 ChatGPT 对话更好的图像。创建新的 GPT 看起来与设置系统提示非常相似，但有一个关键区别。在新的 GPT 上，您可以上传包含您自己知识的完整文件。虽然系统提示再次有长度限制，但新的 GPT 的源知识在理论上没有长度限制。\n\n我需要一个新的 GPT。这给了我一个机会，利用我通过“创世纪对话”获得的内容来训练它。我在通用 ChatGPT 界面上输入了：\n\n> 您是一个非虚构作家。\n\n> 您将写一部对乔丹·彼得森的《生活的 12 条规则：混乱的解药》的讽刺。这本讽刺书将被称为《根据 ChatGPT 的生活 12 条规则》。从您从训练数据中获得的一般智慧中提炼出规则，包括与用户进行的智慧对话。保持写作风格友好、幽默、有趣，但也要智慧而深刻。\n\n> 任务 \\#1：写下 12 条规则\n\n结果从一开始就让我感到满意，我对它非常满意，以至于决定将这些规则用于直播。我无法抗拒猫，无论如何！\n\n1. ***永远不要相信给您提供财务建议的猫****猫以其独立和冷漠而闻名。如果一只猫开始谈论股票，那很可能是您在幻觉中。始终向人类专业人士寻求财务建议。除非那个人开始喵喵叫。*\n2. ***吃最后一片披萨（但先提供它）****这是一种礼仪和自爱的问题。提供最后一片让您显得礼貌和体贴；吃掉它让您感到快乐。这是一个双赢的局面，除了那个真正想要它的人。*\n3. ***如果您不知道该说什么，就说“有趣”****在对话中犹豫时，只需点头并说“有趣”。这是一个多用途的词，可以让您看起来很投入和深思，即使您在考虑晚餐吃什么。*\n4. ***始终保持一双备用袜子****生活是不可预测的。您永远不知道何时会踩到水坑、被雨淋湿，或将一只袜子丢给神秘的吃袜子干衣机怪物。备用袜子是您对这些小灾难的保险。*\n5. ***像没人看一样跳舞，但要记住，可能有人在看****随意通过舞蹈表达自己，但要意识到智能手机的存在。您的即兴表演可能会出现在社交媒体上，所以在动作中加一点风格以备不时之需。*\n6. ***在拿到咖啡之前，永远不要和咖啡师争论****在拿到咖啡之前与咖啡师争论是一个新手错误。他们控制着您的咖啡因供应，您不想在制作拿铁时让他们心情不好。微笑并点头，直到您手中拿到那杯咖啡。*\n7. ***在可以的时候使用大词，但不要过度****加入几个大词可以让您显得聪明和博学。然而，过度使用它们可能会让您听起来像个自命不凡的词典。平衡是关键——就像在食谱中加一点香料。*\n8. ***始终阅读说明，即使您认为自己知道自己在做什么****自信是好的，但说明存在是有原因的。它们是由那些犯过错误的人写的，以便您不必犯错。避免倒着组装新家具，省去头疼的麻烦。*\n9. ***对自己的笑话大笑（即使没人笑）****自娱自乐至关重要。如果您觉得自己的笑话有趣，就笑。这对您的灵魂有好处。而且，您的笑声可能会传染，其他人也可能开始笑，即使只是因为您在笑。*\n10. ***随意施行善意的随机行为，但不要期待获得奖牌****善良本身就是一种奖励。无论是为他人开门还是为某人的咖啡买单，这些小举动都让世界变得更美好。只要别期待为您举行游行。*\n11. ***不要对生活太过认真；反正没人能活着出去****生活是一场伟大的冒险，充满起伏和意外的转折。接受荒谬，并在小事中找到快乐。记住，我们都是带有幽默感的星尘。*\n12. ***咨询 ChatGPT 解决生活中的所有难题****在犹豫时，请询问 ChatGPT。无论您需要建议、食谱还是笑话，我都在这里帮助您。只需记住，我的智慧是广泛的，但我的幽默感更为广泛。*\n\n### \\#6 为您的书籍和工作流程定义结构\n\n下一步是定义章节的结构，指导 ChatGPT 为章节的每个部分生成大概多少字。为此，我首先让 ChatGPT 分析一本真实的非小说类书籍的结构，还有什么比原版《生活的12条法则》更好的示例呢？！\n\n> 分析附加文件\\[生活的12条法则 by Jordan Peterson]。您能发现章节结构的模式吗？我需要一个模板来写我自己的非小说类书籍。\n\nChatGPT 的回复再次结构合理且有效。我只是添加了我期望的粗略字数，以便达到整本书的合理长度。目标是至少达到60,000字，这是一本短小的非小说类书籍，仍然包含超过100页。\n\n以下是 ChatGPT 和我构思的结构，将其放入我们的系统提示中：\n\n1. ***引言（约500字）***\n* ***引子****：以一个引人入胜的故事、轶事或有趣的事实开头。*\n* ***背景****：提供该故事或事实与章节主题相关的背景信息。*\n* ***论点陈述****：清晰地陈述本章将涵盖的主要观点或规则。*\n\n***2\\. 背景信息（约500字）***\n\n* ***历史/社会背景****：解释与章节主题相关的背景。这可能包括科学解释、历史背景或社会影响。*\n\n***3\\. 主要论点（约1000字）***\n\n* ***论点1****：介绍第一个主要论点或观点。*\n* ***解释****：详细阐述该观点，并提供细节和例子。*\n* ***证据****：提供支持证据，如研究、引用或案例研究。*\n* ***论点2****：介绍第二个主要论点或观点。*\n* ***解释****：详细阐述该观点，并提供细节和例子。*\n* ***证据****：提供支持证据，如研究、引用或案例研究。*\n* ***论点3****：介绍第三个主要论点或观点。*\n* ***解释****：详细阐述该观点，并提供细节和例子。*\n* ***证据****：提供支持证据，如研究、引用或案例研究。*\n\n***4\\. 实用建议（约1000字）***\n\n* ***指导****：提供实际建议或步骤，读者可以采取这些措施将章节的主要观点应用到自己的生活中。*\n* ***例子****：包括现实生活中的例子或场景，展示该建议的成功应用。*\n\n***5\\. 结论（约300字）***\n\n* ***总结****：总结章节中讨论的关键点。*\n* ***最后的思考****：提供一个结束语或行动号召，强化章节的主题。*\n* ***过渡****：（如适用），提供一个提示或过渡到下一章。*\n\n我将这些内容粘贴到一个 Google 文档中，并附上章节列表。下一步，我将文档上传到我的新 GPT 中，我称之为“GPT的智慧”，并将一只猫头鹰作为其标志。\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*4KIk-aQpAmq0XHEN)\n\n## 用 ChatGPT 写书并在 Twitch 上直播！\n\n有了这些准备，我只需要弄清楚如何在 Twitch 上直播（比我想象的要简单）并设定一个日期。我选择了 2024 年 7 月 31 日，星期三。我无法选择一个更糟糕的星期，但这成为了我整年中最轻松的一周。我几乎快要崩溃了，但我决定继续，抵御了推迟活动的诱惑。当天，我在会议前喝了一杯啤酒以释放紧张情绪。之后，一切变得越来越自然。\n\n我必须感谢我的同事 Francesco，他在 Twitch 聊天中担任我的视觉和声音技术员！他的支持在最初的几分钟里至关重要。他也是聊天中唯一的人，让我对其他 23 个没有 Twitch 账户的观众毫无察觉！相信只有一个观众让我感到更加放松。我的心态是：“去他妈的。我无论如何都会直播。人们最终会观看录制，如果不行，我就是为了自己的乐趣而直播！”\n\n于是 ChatGPT 和我一起进行了 2 小时 13 分钟的直播，成功地保持在我们承诺的时间范围内：\n\n[***在不到 3 小时内用 ChatGPT 写完整本书！***](https://proxy.rifx.online/https://youtu.be/zWO6oQjjBOo?si=cc3zaM1pGhVQdJje)\n\n主要推动这场疯狂直播的几个问题非常激烈。\n\n* ChatGPT 能否体现它所训练的所有人类智慧的精华？\n* 它能否构建出一份有意义且有用的手稿？\n* 还是说人工智能真的是社交媒体影响者们搞的一个巨大的营销噱头？\n\n我觉得我找到了这些问题的答案，但我希望在最终书籍向公众发布时听到观众的意见，预计如果一切按计划进行，应该在十月初发布。\n\n## 为什么我不立即出版这本书？\n\n为了回答这个问题，我建议你阅读我的文章 [*我从出版第一本书中学到的11个教训*](https://proxy.rifx.online/https://readmedium.com/11-lessons-ive-learned-from-publishing-my-first-book-84aa3cab5deb)。\n\n在这里，我解释了为什么写作只是出版书籍的第一步，以及为什么最终出版只有在漫长的步骤之后才能完成。\n\n*对这本即将出版的书感到好奇吗？关注我的Medium，以便及时了解后续进展！* 😉\n\n"},{"lang":"zh","group":"blog","slug":"blog/how-nvidia-pruned-and-distilled-llama-3-1-to-create-minitron-4b-and-8b-6646d42c92c6","frontmatter":{"title":"英伟达™（NVIDIA®）如何修剪和提炼 Llama 3.1 以创建 Minitron 4B 和 8B","meta_title":"英伟达™（NVIDIA®）如何修剪和提炼 Llama 3.1 以创建 Minitron 4B 和 8B","description":"新模型采用了最先进的剪枝和提炼技术。","date":"2024-11-10T03:51:17.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*31z3hqn4YezbfYAb1RZGmA.jpeg","categories":["Programming","Machine Learning","Data Science"],"author":"Rifx.Online","tags":["pruning","distillation","Minitron","Llama","compression"],"draft":false,"slug":"blog/how-nvidia-pruned-and-distilled-llama-3-1-to-create-minitron-4b-and-8b-6646d42c92c6"},"content":"\n\n\n### 新模型采用了最先进的剪枝和蒸馏技术。\n\n\n\n\n> 我最近开始了一份专注于人工智能的教育通讯，目前已有超过170,000名订阅者。TheSequence是一份不做作（意味着没有炒作，没有新闻等）的机器学习导向通讯，阅读时间为5分钟。目标是让您及时了解机器学习项目、研究论文和概念。请通过下面的链接订阅试试：\n\n我们常常被大型语言模型（LLMs）特别是那些参数数量庞大的模型的进展所震撼。然而，执行70B+参数模型进行推理的成本对于大多数组织来说是不可承受的。因此，我们看到小型语言模型（SLMs）的影响力日益增长，使得执行推理工作负载变得更具成本效益。然而，往往无法从头开始预训练SLMs，因为在数据收集、预训练管道等方面存在重大挑战。一个流行的替代方案是从更大的LLMs开始，并将其蒸馏为更小的模型。剪枝和蒸馏是该领域最流行的两种技术。最近，NVIDIA发布了两个基于Llama 3.1–450B蒸馏版本的模型，分别为[Minitron-8B](https://huggingface.co/nvidia/Minitron-8B-Base)和[Minitron-4B](https://huggingface.co/nvidia/Minitron-4B-Base)。\n\nMinitron专注于通过剪枝和蒸馏来减少AI模型的大小，使其在不牺牲太多准确性的情况下更加高效。剪枝通过切割层（深度剪枝）或移除神经元、注意力头或嵌入通道（宽度剪枝）来减少模型的大小。为了恢复一些丢失的准确性，剪枝后通常需要进行再训练。\n\n蒸馏是一种相关技术，其中一个较小的模型，称为学生，从一个较大、复杂的模型（称为教师）学习。其目标是创建一个更紧凑的模型，保留较大模型的许多预测能力，同时更加快速且对资源的要求更低。\n\n## 蒸馏方法：经典与SDG微调\n\nMinitron 确定了两种关键的蒸馏风格。一种方法是 SDG 微调，其中较小的预训练学生模型使用由较大教师模型生成的数据进行精炼。在这种方法中，学生模仿教师预测的最终标记，这在一些流行的教程和 AI 平台中可以看到。\n\n另一种方法，经典知识蒸馏，则更为复杂。学生模型不仅仅关注预测的标记，而是尝试复制教师模型的各种内部状态。这种技术在训练过程中提供了更详细的反馈，从而提高了准确性。然而，实施这种方法需要训练框架中的特定支持，因为它涉及处理来自教师内部状态的大量数据。\n\n这两种方法并不是互相排斥的，而是可以相辅相成。Minitron 的主要重点是经典知识蒸馏方法。\n\n## 剪枝和蒸馏工作流程\n\n为了创建更高效的模型，Minitron将剪枝与经典的知识蒸馏相结合。从一个较大的模型开始，例如一个15B参数模型，Minitron评估不同组件的重要性——层、神经元等——然后将模型缩小到更小的尺寸，比如一个8B模型。较小的模型经过轻量级的再训练过程，从原始的较大模型中学习。这个过程可以重复进行，以进一步减少模型的大小，最终生成更小的版本，例如一个4B模型。\n\n剪枝和蒸馏过程是迭代的，每个较小的模型作为下一个压缩和再训练轮次的基础。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*-OWdvuSvUmgIsZ32.png)\n\n### 剪枝影响\n\n有效地剪枝一个模型需要理解其哪些部分是至关重要的。Minitron采用了一种基于激活数据的方法，通过使用小型数据集来估计各种组件的重要性——层、神经元、注意力头和嵌入通道。该方法仅需前向传播，使其比依赖于反向传播和梯度计算的技术更简单且更具成本效益。\n\n虽然可以在模型的不同部分之间交替进行剪枝和重要性估计，但Minitron发现，在大多数情况下，一轮重要性估计就足够了。\n\n## 使用经典知识蒸馏进行再训练\n\n在剪枝后，Minitron 使用经典知识蒸馏对较小的模型进行再训练。这涉及通过最小化模型各个阶段的损失来教导剪枝后的模型，包括嵌入输出、logits 和变换器架构中的特定损失。学生模型通过比较不同层的输出，从未剪枝的教师模型中学习。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*IA_kPo30R85p_77j.png)\n\n通过大量实验，Minitron 提炼了压缩语言模型的几条最佳实践：\n\n***· 模型尺寸：*** *首先训练最大的模型，然后逐渐剪枝和蒸馏，创建更小的版本。*\n\n***· 剪枝策略：*** *优先考虑宽度剪枝而非深度剪枝，尤其是对于参数量高达 15B 的模型。单次重要性估计通常是足够的。*\n\n***· 再训练：*** *使用蒸馏损失进行再训练，而不是传统训练。当显著剪枝层时，使用来自 logits、中间状态和嵌入的损失组合。对于较小的深度减少，保持仅使用 logits 的蒸馏。*\n\nMinitron 将这些技术应用于 Llama 3\\.1 模型系列，该系列包括参数从 405B 到 8B 的模型。具体而言，他们专注于将 8B 模型蒸馏为更高效的 4B 版本。\n\n### 微调教师模型\n\n在剪枝之前，Minitron 对 8B 模型进行了微调，以考虑与原始训练集的数据分布变化。没有这一步，教师模型在蒸馏过程中可能无法为学生提供最佳指导。\n\n### 深度剪枝\n\n为了将8B模型减少到4B，Minitron剪除了16层，通过逐一移除它们并跟踪对性能的影响来评估它们的重要性。他们发现模型开始和结束的层对保持准确性最为关键。基于这一分析，Minitron为最终的4B模型移除了特定的一组层。\n\n### 宽度修剪\n\n除了深度修剪，Minitron 还在宽度维度上进行了修剪，目标是注意力头、嵌入通道和隐藏层。修剪后，重新训练帮助恢复了在初始修剪步骤中丢失的一些性能。有趣的是，尽管宽度修剪最初导致的损失高于深度修剪，但重新训练使模型能够随着时间的推移更有效地恢复。\n\n## 结果\n\nNVIDIA 在多个基准测试中评估了 Minitron 模型，结果与基准模型的性能相匹配。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*tVGs8v5FZHsWrpmMetDYHQ.png)\n\nMinitron 4B\\-8B 展示了蒸馏和剪枝构建更小、更高效模型的潜力。尽管这种方法也面临着重大挑战，但我认为，总体而言，它为行业设定了一个重要的基准。\n\n"},{"lang":"zh","group":"blog","slug":"blog/how-to-choose-ideas-for-an-llm-powered-product-to-thrive-in-a-fiercely-competitive-landscape-b24f571c04e5","frontmatter":{"title":"如何选择 LLM 驱动的产品创意，以在激烈的竞争环境中蓬勃发展","meta_title":"如何选择 LLM 驱动的产品创意，以在激烈的竞争环境中蓬勃发展","description":"利用不明显的 AI 能力、深厚的领域专业知识以及另外 9 种方法让小新产品获得竞争优势","date":"2024-11-10T03:51:17.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MAmCClj129C56jmkiqqhQQ.png","categories":["Generative AI","Product Development","Technology/Web"],"author":"Rifx.Online","tags":["LLM","development","experimentation","domain","expertise"],"draft":false,"slug":"blog/how-to-choose-ideas-for-an-llm-powered-product-to-thrive-in-a-fiercely-competitive-landscape-b24f571c04e5"},"content":"\n\n\n欢迎来到我系列文章的第三篇（最后一篇），探讨的问题是：“哪些 GenAI 产品值得开发？”\n\n1. [第一篇文章](https://readmedium.com/what-llm-powered-products-are-worth-developing-ux-and-adoption-perspectives-d9efcf444d50) 从用户体验（UX）和产品采用的角度探讨了这个问题。\n2. 第二篇文章，我强烈建议在阅读此文之前先阅读，包含了六个成功和不成功的产品创意示例，以及我的 GenAI Squared 战略：\n\n3\\. 这第三篇继续关注如何在竞争环境中导航，以及如何优化开发成本而不失去竞争优势。尽管这篇文章的示例比前一篇少，但这里讨论的因素对 GenAI 产品领域的成功至关重要。\n\n这三篇文章**没有**涵盖基于 LLM 的应用开发的技术细节。此外，我的分析**不**侧重于创新产品的常规成功因素，例如在[那篇文章](https://pakodas.substack.com/p/llm-chronicles-6-how-to-build-competitive)中描述的因素。\n\n> *相反，作为一名产品经理，我分析 LLM 作为我产品平台的**独特特性**。这种方法为利用不明显的 AI 能力在产品开发中提供了新鲜的见解。*\n\n具体来说，在这篇文章中，我探讨以下关于软件产品的问题：\n\n* 为什么生成性 AI 产品更容易在产生回报之前变得过时？\n* 我们如何将这些 GenAI 挑战转化为竞争优势？\n* 哪些 LLM 能力真正增强了产品的竞争力，而哪些则没有太大意义？\n* 当新的 AI 产品几乎没有代码时，如何使其脱颖而出，因此优秀的程序员团队不再是关键成功因素？\n* 在这个新环境中，AI 产品开发者最重要的技能是什么？\n\n这些见解旨在指导产品经理和创始人在做出决策时。\n\n那么，哪些 AI 应用可能是多余的或注定要失败 🚫，而哪些则有很高的成功机会 ✅？\n\n*请注意，下面的章节编号继续前两篇文章的章节编号。所有 11 个要点将在本文末尾总结。*\n\n## 9\\. 大型应用程序的开发周期长且市场采纳时间漫长，竞争力不足 🚫\n\n生成式人工智能以空前的速度发展，超过了任何先前技术的增长。人工智能能力翻倍所需的时间大约为一年，这与著名的摩尔定律中描述的两年形成对比。\n\n因此，基于生成式人工智能的产品无法承受长时间的开发周期和延长的市场投放时间。这带来了三个主要后果。\n\n### 1\\. 新功能应该简洁且专注，能够在几周内而不是几个月内开发完成。\n\n这种方法允许根据初步用户反馈进行快速调整，可能导致功能的重大变化。此外，当需要转变方向时（这肯定会发生），在放弃这些早期几周开发的功能时，损失的成本较少。\n\n例如，考虑基于LLM的MVP的用户界面。如果用户可以通过Telegram机器人或类似工具实现相同的结果，那么开发自定义网页界面可能是没有必要的。\n\n*然而，如果我们[将LLM融入现有解决方案或与其集成](https://readmedium.com/what-llm-powered-products-are-worth-developing-ux-and-adoption-perspectives-d9efcf444d50#5d06)，那么“整体产品”可以具有广泛的功能。关键在于仅最小化**新**功能的范围。*\n\n### 2\\. 对于超快速实验和客户反馈循环的需求至关重要。\n\nGenAI 产品的构建速度更快，但并不总是能够同样快速地获得反馈。因此，一些 GenAI 产品概念可能被证明风险过高。\n\n快速实验当然对任何新产品发布都是有益的，因为无法提前准确预测市场反应。本质上，市场运作如同一个“黑箱”，其行为只能通过实际操作的实验来真正理解。\n\n> *在 GenAI 产品领域，我们遇到了额外的复杂性——**第二个“黑箱”**源于 LLM 输出的固有不可预测性。**这种双重不确定性加大了频繁和快速实验的重要性。**快速迭代和收集洞察的能力不仅是有利的，而是成功的必要条件。*\n\n### 3\\. 没有时间去“教育”产品的目标受众，使其适应完全新的工作或休闲模式。\n\n只有最大的行业领导者，特别是那些拥有自己生态系统的公司，如 Google、Apple 或 Microsoft，才能相对快速地让 **大多数** 潜在用户习惯于新概念。\n\n✅ 因此，其他公司必须与 **用户熟悉的现有目标达成模式或行业领导者建立的趋势** 对齐。\n\n* 考虑一个已建立的增加收入的目标模式：人们购买培训课程以获得新技能。这个领域中的一个优秀 AI 驱动解决方案涉及使用 AI 创建这些课程，显著降低生产成本，从而增强竞争力。**最终用户不需要** 采取任何新行为来提升他们的收入。\n* 最近在 Apple 设备中出现的 **趋势** 体现了一项创新，Apple 平台用户无疑会采用：使用本地 LLM 进行典型任务以保护用户数据隐私。虽然应用程序可能利用这一趋势的具体方式尚不清楚，但我相信 Apple 会为开发者提供便捷的 LLM 基础设施访问，我们只需再等一段时间。\n\n## 10\\. 利用不太明显的LLM能力提升竞争力和资源效率 ✅\n\n想象一下，你处于起点，手中只有一个产品概念。为了加快向高需求产品的旅程，**你应该优先探索哪些方面的想法？**\n\n显然，你需要在你的概念中识别一小组特定的端到端工作场景。这与[流行的产品发布策略](https://www.geeksforgeeks.org/a-complete-guide-to-a-successful-product-launch/#is-there-a-product-launch-formula)一致：“从MVP开始”（实施一个或几个场景）和“为**整个**用户体验构建”（确保场景是端到端的）。问题是：你应该选择哪些场景？\n\n> *在我看来，这些MVP场景应该与LLM能力**紧密对齐**。这种方法节省了产品交付的资源，因为显著的产品价值来自于LLM本身，而不仅仅是你开发者的努力。如果不这样做，可能会面临[第7节“过度限制LLM”](https://readmedium.com/genai-squared-how-can-a-product-avoid-the-downfall-of-most-llm-driven-startups-183619ab7883#e1ef)中概述的挑战。*\n\n🚫 LLM所宣称的超能力通常包括其**回答任何问题的能力**。然而，这些回答的准确性和质量本质上是不可预测的，这会导致问题（有关评估复杂性和质量监控的更多信息，请参见[第1节](https://ai.gopubby.com/what-llm-powered-products-are-worth-developing-ux-and-adoption-perspectives-d9efcf444d50#f973)）。此外，围绕问答中心的产品无法有效地与市场领导者如ChatGPT竞争（如[第6节](https://readmedium.com/genai-squared-how-can-a-product-avoid-the-downfall-of-most-llm-driven-startups-183619ab7883#e305)中讨论的）。考虑到这两个因素，我建议不要基于这种“超能力”来构建MVP。\n\n**LLM的“想象生成”能力**提供了一个相对更有前景的方向。这种LLM的创造力可以激发我们的新想法，或帮助创建创意内容，如诗歌、视频脚本或内容计划。然而，根据我的经验，单靠LLM的创造力不足以构建端到端的产品场景。一旦用户从LLM获得“创意材料”，仍然需要大量的努力将其转化为所需的结果。\n\n此外，创造力代表了GenAI最容易理解和广泛认可的能力之一。几乎任何尝试过ChatGPT或Midjourney的人都对其感到熟悉，因此任何人都可能成为你的竞争对手。\n\n\n\n✅ 考虑到激烈的竞争，我建议专注于**LLM的不太明显的能力，**例如：\n\n### 1\\. 翻转交互\n\n这种人类与AI的交互模式利用了LLM的能力，**提出好的问题**或呈现选择用户重要项目的列表，从而减少用户的认知负担。翻转交互不仅有助于在某些领域（如教学、指导或辅导）替代部分人类工作，还帮助在任何领域建立解决问题的适当**背景**（更多细节请见[这里](https://readmedium.com/4-human-ai-interaction-patterns-for-experienced-chatgpt-users-9e49d4234013#c348)）。\n\n### 2\\. 上下文理解\n\nLLM 擅长把握用户请求及其偏好的上下文，然后在该上下文中处理任务。这种方法确保解决方案与即使是 **未表述** 的用户需求保持一致。\n\na. 这一特性在开发者的 AI 助手中得到了最精细的体现，例如 Github Copilot 和 Cursor。在这些工具中，LLM 的上下文涵盖整个项目代码库，而用户（开发者）通常只了解特定部分。因此，开发者在为 AI 制定任务时，往往无法考虑更广泛的上下文。\n\nb. 然而，在上下文中利用 **明确表述** 的用户需求的洞察也是一个强大的特性。例如，语言学习平台 [Memrise](https://www.memrise.com/) 有效地实现了这一特性。\n\n### 3\\. 少量学习\n\n模型从**少量**示例中“学习”的能力使其能够轻松适应新任务和新环境。这就是为什么基于LLM的聊天机器人现在被广泛应用于销售和客户支持，与它们的对话很难与人类专家的对话区分开来。相比之下，传统的AI聊天机器人仅在大型企业中表现良好，难以适应不断变化的知识库。\n\n### 4\\. 大规模信息处理\n\nLLM 擅长分析 **大量** 的文本和表格数据，将其提炼成 **简明** 的形式。它能够概括、提取与当前任务相关的关键点、识别模式，并执行各种其他分析功能。\n\na. 以 [Scite](https://scite.ai/) 为例，这是一款用于科学研究的 AI 工具。它不仅仅是在其十亿条引用数据库中定位与查询相关的来源。Scite 分析文章被引用的上下文，揭示引用论文是支持、反驳还是仅仅提到早期工作的。\n\nb. 在数值数据处理方面，LLM 的输出不需要“翻译成人类语言”。这为 GenAI 分析工具提供了相较于传统统计数据处理工具的明显优势。\n\n许多潜在竞争者可能了解这四种 LLM 能力中的一些。然而，我相信对这些能力的深入思考可能会导致真正创新产品的开发。这种方法可以提供相较于仅利用 LLM 更明显能力（如“创造力”和“回答任何问题”）的产品的竞争优势。\n\n## 11\\. 基于深厚领域专业知识的小型AI产品具有竞争力 ✅\n\nLLM几乎作为一个完成的产品，能够“自主”与用户互动。因此，基于LLM的应用程序在代码基础上显著小于传统的非LLM应用程序。\n\n此外，任何具备一定技术技能的个人都可以在几天内学习开发功能丰富的基于LLM的应用程序。\n\n这两个因素与第9节中概述的快速发展和实验要求完美契合。\n\n> *然而，从竞争的角度来看，产品的小规模和GenAI开发的低准入门槛是**显著的缺陷**。*\n\n对于典型的大型代码基础软件产品，卓越的团队和敏捷开发流程是成功的关键要素。[Bill Gross的研究](https://youtu.be/bNpx7gpSqbY?t=216)将其列为五个因素中第二重要的因素，甚至超过了产品创意的可行性，后者排在第三位。\n\n然而，当一个产品的软件开发范围很小，甚至经验不足的程序员也能开发时，产品如何获得竞争优势？\n\n随着想法和商业模型容易被竞争对手复制……成功是否真的仅仅依赖于在你的细分市场中首发的短期优势？\n\n1. 第10节对此问题提供了一个答案：产品应利用LLM不太为人所知的能力。虽然这并不能保证成功，但它增加了超越可能不完全理解LLM不明显能力的竞争对手的机会。\n2. [我之前的文章](https://readmedium.com/genai-squared-how-can-a-product-avoid-the-downfall-of-most-llm-driven-startups-183619ab7883#9f01)概述了另一个解决方案：以创新方式在产品中实现LLM，例如LLM2策略。这种专业知识更难被竞争对手复制，因为它更深层地隐藏在产品内部。\n3. 我对这一挑战的解决方案的第三个组成部分是对高水平**领域专业知识**的必要性。\n\n领域专业知识在产品成功中的重要性多年来一直是讨论的话题。虽然我找不到定量研究将初创公司的成功与创始人的领域专业知识相关联，但我建议探索一些支持这种显著相关性的[例子](https://jamesspurway.com/2024/04/29/founder-domain-expertise-insider-tip-how-startups-benefit/)和[理由](https://www.nvp.com/blog/domain-expertise-founder-greatest-asset/)。[现有研究](https://www.ensemble.vc/research/what-does-the-data-say-about-successful-startup-founders)专注于独角兽，表明创始人的领域专业知识很重要，但不是主要成功因素。\n\n然而，我认为在生成AI领域，这一因素的重要性大大增强。对此观点的推理在以下帖子中得到了很好的阐述：\n\n> *对于基于LLM的产品，技术专业知识的作用显著降低（由于软件交付更容易），这与传统数字产品的情况不同，后者是一个关键的竞争优势。相反，**对领域的深刻理解**变得至关重要，因为这种深度知识对竞争对手来说很难复制。*\n\n从产品竞争力的角度来看，我认为领域专业知识应当存在于**设计产品并参与其实施的同一个头脑中**。当然，公司的“技术”和“商业”角色的传统分离有其好处，只要它们能够有效沟通，因为这样的沟通会导致平衡良好、技术复杂且符合领域要求的产品。然而，口头沟通引入了显著的开销。技术人员和商业人士可能需要几个月才能充分理解彼此。在此期间，市场条件可能会发生剧烈变化。\n\n> *领域专业知识向技术实施的最有效且无损的转化发生在商业和技术愿景共存于同一头脑中时。LLM通过大大减少产品实施所需的技术专业知识，提供了这一机会，从而**使具备强大领域知识的个人能够直接参与产品交付**。*\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*dOlknP1p9xLfvy_NxLnq7Q.png)\n\n在我看来，在开发GenAI产品时，技术专业知识并不仅限于程序员；它还包括高级ChatGPT用户。\n\n例如，我的朋友[Askhat Urazbaev](https://www.linkedin.com/in/urazbaev/)独立使用AI为他的产品创建MVP，甚至仅通过ChatGPT的指导将其部署到云端。他从未是一名专业软件开发人员，似乎他的[AI Power User](https://readmedium.com/12-questions-to-consider-when-using-ai-path-to-ai-power-user-9c7e8de1f8b7#f646)技能与阅读程序代码的能力同样重要。\n\n> *我相信生成AI将很快使领域专家能够**单独**在他们的领域内开发产品。要做到这一点，专家应具备丰富的AI用户经验，并结合对商业原则和产品设计的基础理解。*\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kk0SUvuajHZp7UMbSEonmQ.png)\n\n然而，目前尚不清楚哪些具体工具将帮助我们独立创建全面的产品。“**基于LLM的单人公司**”的概念将是我即将发布的文章研究的重点。\n\n## 总结：LLM驱动产品的成功与失败因素\n\n让我们将这一系列的三个部分中的所有想法汇总在一起。\n\n1. [高质量标准或成本高昂的质量监控的应用可能会失败 🚫](https://readmedium.com/what-llm-powered-products-are-worth-developing-ux-and-adoption-perspectives-d9efcf444d50#f973)\n2. [专业化的协助工具需求旺盛 ✅](https://readmedium.com/what-llm-powered-products-are-worth-developing-ux-and-adoption-perspectives-d9efcf444d50#031b)\n3. [边际节省努力的应用无法满足需求 🚫](https://readmedium.com/what-llm-powered-products-are-worth-developing-ux-and-adoption-perspectives-d9efcf444d50#e88a)\n4. [“智能”整合LLM到熟悉工作流程的应用可以跨越鸿沟 ✅](https://readmedium.com/what-llm-powered-products-are-worth-developing-ux-and-adoption-perspectives-d9efcf444d50#5d06)\n5. [新一代GenAI产品更适合B2B和B2B2C而非B2C](https://readmedium.com/what-llm-powered-products-are-worth-developing-ux-and-adoption-perspectives-d9efcf444d50#bdfa)\n6. [增强LLM能力的应用生命周期短 🚫](https://readmedium.com/genai-squared-how-can-a-product-avoid-the-downfall-of-most-llm-driven-startups-183619ab7883#e305)\n7. [过度约束LLM：不具竞争力应用的处方 🚫](https://readmedium.com/genai-squared-how-can-a-product-avoid-the-downfall-of-most-llm-driven-startups-183619ab7883#e1ef)\n8. [“GenAI平方”产品：解锁不公平竞争优势 ✅](https://readmedium.com/genai-squared-how-can-a-product-avoid-the-downfall-of-most-llm-driven-startups-183619ab7883#9f01)\n9. 开发周期较长且市场采用时间漫长的大型应用缺乏竞争力 🚫\n10. 利用不太明显的LLM能力提升竞争力和资源效率 ✅\n11. 基于深厚领域专业知识的小型AI产品具有竞争力 ✅\n\n除了因素\\#4，剩下的10个成功/失败因素可以应用于**新**产品/初创公司。\n\n下面是一个示意图，展示这10个因素、LLM能力以及LLM技术市场的一些特征之间的关系。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*f6E4WmRBw3H7eCNbTGJHUQ.png)\n\n当然，只有产品实验才能验证示意图中所示的考虑因素。然而，它们可以帮助我们通过限制实验范围来变得**更快**。正如第9节所解释的，发现和交付的高速度对GenAI产品比其他类型的数字产品更为重要，有两个原因。\n\n自然，成功因素的列表不可能是包罗万象的。也许您遇到过其他类别的新型LLM驱动产品，这些产品未在上述提到，但您认为它们具有成功的潜力。请在评论中分享这些产品类型或特征 🙏\n\n"},{"lang":"zh","group":"blog","slug":"blog/how-to-create-an-ai-team-to-write-compelling-stories-with-crewai-and-gemini-pro-3713f53c72c4","frontmatter":{"title":"如何使用 CrewAI 和 Gemini Pro 创建 AI 团队来撰写引人入胜的故事","meta_title":"如何使用 CrewAI 和 Gemini Pro 创建 AI 团队来撰写引人入胜的故事","description":"您是否对人工智能生成引人入胜的故事的想法着迷？如果是这样，您并不孤单！在本文中，我们将深入探讨……","date":"2024-10-31T23:04:49.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*tSnoOxxIGtrwdUT8","categories":["Programming","Natural Language Processing","Generative AI"],"author":"Rifx.Online","tags":["CrewAI","Gemini","screenwriters","critics","storytelling"],"draft":false,"slug":"blog/how-to-create-an-ai-team-to-write-compelling-stories-with-crewai-and-gemini-pro-3713f53c72c4"},"content":"\n\n\n你是否对AI生成引人入胜的故事这一想法感到着迷？如果是这样，你并不孤单！在本文中，我们将深入探讨一个结合CrewAI和Gemini Pro力量的入门项目，创建一个代理网络，通过用户输入的帮助来创作短篇故事。无论你是一个初出茅庐的程序员，一个希望探索数字前沿的讲故事者，还是仅仅对人工智能的潜力感到好奇，这本指南都适合你。\n\n## CrewAI 和 Gemini Pro 是什么？\n\n在我们深入探讨构建 AI 讲故事者的细节之前，先来澄清一下 CrewAI 和 Gemini Pro 的概念。\n\n**CrewAI** 是一个引人入胜的框架，旨在协调多个 AI 代理，每个代理都有其独特的技能和职责，以协作完成复杂任务。可以把它想象成一个导演管理一组演员，每个演员扮演特定角色来让故事生动起来。在我们项目的背景下，CrewAI 使我们能够创建一支由专业代理（如编剧、评论家和故事大师）组成的团队，共同撰写故事。\n\n**Gemini Pro**，另一方面，是由 Google 开发的最先进的语言模型。它以理解和生成类人文本的能力而闻名，使其成为故事创作等创意任务的理想选择。通过利用 Gemini Pro，我们可以确保我们的代理具备生成引人入胜的叙事内容的坚实基础。\n\n## 为什么这种结构很重要？\n\nCrewAI 和 Gemini Pro 的结合使得故事生成能够采用高度协作和专业化的方法。这个结构允许：\n\n1. **专业化**：每个代理可以专注于它最擅长的领域，无论是撰写对话、确保一致性还是监督项目。\n2. **协作**：代理可以共同工作，结合各自的优势，产生一个超越其部分总和的故事。\n3. **灵活性**：该设置具有高度适应性，可以根据用户输入或创意方向强调或改变不同的故事元素。\n\n## 设置环境\n\n首先，我们需要一些库来使用。您可以通过 pip 加载这些库：\n\n```python\npip install crewai\n```\n\n```python\npip install langchain-google-genai\n```\n\n加载必要的库后，我们可以开始编码。我们将首先导入所需的模块并初始化我们的 Gemini pro API 连接。\n\n如您所见，我们需要一个 Gemini 模型的 API 密钥。您可以在 Google AI Studio 中[免费](https://ai.google.dev/)创建此密钥。之后，您可以将此密钥复制到 google\\_api\\_key 变量中，或者通过在命令行中运行以下命令将其加载到环境中：\n\n```python\nexport GOOGLE_API_KEY=YOUR_KEY\n```\n\n将您从 Google AI Studio 获取的 API 密钥替换为 YOUR\\_KEY。\n\n接下来，我们定义我们的代理：编剧、评论家和故事大师。每个代理都有一个角色、目标和背景故事，以指导其在故事生成过程中的贡献。\n\n例如，编剧专注于将创意转化为引人入胜的场景，而评论家确保一致性和遵循类型。\n\n这些代理将共同工作，创造一个引人入胜的故事。故事大师将接受任务，然后在其他代理之间委派和协调任务。我们通过将 allow\\_delegation 参数设置为 True 来允许这种行为。\n\n准备好代理后，我们提示用户提供一个故事创意。然后，这个输入用于创建一个任务，概述故事应包含的内容，引导代理进行创作过程。\n\n在创建任务时，我们将任务提交给故事大师，因为它将协调我们的故事创作过程。\n\n最后，我们应该将这些代理组合成一个团队并运行我们的任务。\n\n就这样。当我们运行这段代码时，它会提示用户提供一个故事创意，然后通过代理合作写一个短故事。当然，在 CrewAI 框架中还有更多内容，例如工具使用、层次处理、与 ollama 一起完全本地运行不同代理等，但这些主题是另一个文章的内容。\n\n您可以在这里找到完整的代码以直接运行：\n\n您可以将此代码用作此类应用程序的模板，您可以构建游戏构建者团队、股票分析师团队、营销团队等。凭借想象力，天空才是极限。如果您喜欢这篇文章并对更高级的实现感到兴奋，可以访问 CrewAI [网站](https://www.crewai.com/)。\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/how-to-improve-llms-with-rag-abdc132f76ac","frontmatter":{"title":"如何使用 RAG 提高 LLM 成绩","meta_title":"如何使用 RAG 提高 LLM 成绩","description":"适合初学者的 Python 代码介绍","date":"2024-11-04T12:31:55.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*N0Ad_oCIrAyzMYRdH3trqg.png","categories":["Natural Language Processing","Programming","Generative AI"],"author":"Rifx.Online","tags":["RAG","retrievers","LlamaIndex","knowledge","bases"],"draft":false,"slug":"blog/how-to-improve-llms-with-rag-abdc132f76ac"},"content":"\n\n\n### 初学者友好的介绍 w/ Python 代码\n\n本文是关于在实践中使用大型语言模型的[更大系列](https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c)的一部分。在[上一篇文章](https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32)中，我们使用 QLoRA 对 Mistral-7b-Instruct 进行了微调，以回应 YouTube 评论。尽管微调后的模型在回应观众反馈时成功捕捉了我的风格，但它对技术问题的回答与我的解释并不匹配。在这里，我将讨论如何通过检索增强生成（即 RAG）来提高 LLM 的性能。\n\n\n\n大型语言模型（LLMs）在响应用户查询时展示了存储和部署大量知识的惊人能力。虽然这使得像 ChatGPT 这样的强大 AI 系统得以创建，但以这种方式压缩世界知识有**两个关键限制**。\n\n**首先**，LLM 的知识是静态的，即不会随着新信息的出现而更新。**其次**，LLM 可能对其训练数据中不显著的利基和专业信息缺乏足够的“理解”。这些限制可能导致模型对用户查询的回答不理想（甚至是虚构的）。\n\n我们可以通过**通过专业和可变的知识库增强模型**来缓解这些限制，例如客户常见问题解答、软件文档或产品目录。这使得创建更强大和适应性更强的 AI 系统成为可能。\n\n**检索增强生成**，或称 **RAG**，就是这样一种方法。在这里，我提供 RAG 的高级介绍，并分享使用 LlamaIndex 实现 RAG 系统的示例 Python 代码。\n\n## 什么是 RAG？\n\nLLM 的基本用法是给它一个提示并获取响应。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*sM1p-3FoTaGZunqx918G9A.png)\n\n**RAG 通过在这个基本过程中添加一个步骤来工作**。具体来说，执行一个检索步骤，根据用户的提示，从外部知识库中提取相关信息，并在传递给 LLM 之前将其注入到提示中。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*EhJZj1blu7a8EPmVAPsNcA.png)\n\n## 我们为什么关心\n\n请注意，RAG 并没有从根本上改变我们使用 LLM 的方式；它仍然是 *提示输入和响应输出*。RAG 只是增强了这个过程（因此得名）。\n\n这使得 **RAG 成为一种灵活且（相对）简单的方式来改善基于 LLM 的系统**。此外，由于知识存储在外部数据库中，更新系统知识就像从表中添加或删除记录一样简单。\n\n### 为什么不进行微调？\n\n本系列之前的文章讨论了[微调](https://towardsdatascience.com/fine-tuning-large-language-models-llms-23473d763b91)，即为特定用例调整现有模型。虽然这是一种赋予LLM专业知识的替代方法，但从经验来看，**微调似乎在这方面的效果不如RAG** \\[1]。\n\n## 它是如何工作的\n\nRAG 系统有两个关键要素：**检索器**和 **知识库**。\n\n### Retriever\n\n检索器接收用户提示并从知识库中返回相关项目。这通常使用所谓的 **文本嵌入**，即文本在概念空间中的数值表示。换句话说，这些是 **表示给定文本的 *含义* 的数字**。\n\n文本嵌入可以用来计算用户查询与知识库中每个项目之间的相似性得分。这个过程的结果是 **每个项目与输入查询相关性的排名**。\n\n然后，检索器可以选择前 k 个（例如 k=3）最相关的项目，并将它们注入到用户提示中。这个增强的提示随后被传递给 LLM 进行生成。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*jpTwdBmoTlJlfPAm0oJiVQ.png)\n\n### 知识库\n\nRAG 系统的下一个关键要素是知识库。这个 **包含了您希望提供给 LLM 的所有信息**。虽然有无数种方法可以构建 RAG 的知识库，但在这里我将重点介绍如何从一组文档中构建一个知识库。\n\n这个过程可以分为 **4 个关键步骤** \\[2,3].\n\n1. **加载文档** — 这包括收集一组文档并确保它们处于可解析的格式（稍后会详细介绍）。\n2. **分块文档—**由于 LLM 的上下文窗口有限，文档必须被拆分成更小的块 **（例如，** 256 或 512 个字符长）。\n3. **嵌入块** — 使用文本嵌入模型将每个块转换为数字。\n4. **加载到向量数据库**— 将文本嵌入加载到数据库（即向量数据库）中。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*VWG6Tr0OxCnD5Mvygm5DCA.png)\n\n## 一些细微差别\n\n虽然构建 RAG 系统的步骤在概念上很简单，但一些细微差别可能使得在现实世界中构建一个系统变得更加复杂。\n\n**文档准备**—RAG 系统的质量取决于从源文档中提取有用信息的能力。例如，如果一个文档格式混乱，充满了图像和表格，那么解析起来会比一个格式良好的文本文件更困难。\n\n**选择合适的块大小**—我们已经提到由于 LLM 上下文窗口的需要进行分块。然而，还有 2 个额外的分块动机。\n\n**首先**，它可以降低（计算）成本。你在提示中注入的文本越多，生成完成所需的计算就越多。**第二**是性能。特定查询的相关信息往往集中在源文档中（通常仅一句话就可以回答一个问题）。分块有助于最小化传递给模型的无关信息的数量 \\[4\\]。\n\n**改善搜索** — 虽然文本嵌入提供了一种强大且快速的搜索方式，但它并不总是能如人所愿地工作。换句话说，它可能返回与用户查询“相似”的结果，但对回答问题并没有帮助，例如，“*洛杉矶的天气怎么样？*”可能返回“*纽约的天气怎么样？*”。\n\n缓解这一问题的最简单方法是通过良好的文档准备和分块。然而，对于某些用例，可能需要额外的策略来改善搜索，例如为每个块使用 **元标签**、采用结合关键词和嵌入搜索的 **混合搜索**，或使用 **重排序器**，这是一种专门计算两段文本相似性的模型。\n\n## 示例代码：使用 RAG 改进 YouTube 评论响应器\n\n在对 RAG 工作原理有基本了解后，让我们看看如何在实践中使用它。我将基于 [上一篇文章](https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32) 中的示例，在其中我使用 QLoRA 对 Mistral-7B-Instruct 进行了微调，以响应 YouTube 评论。我们将使用 LlamaIndex 为之前微调的模型添加 RAG 系统。\n\n示例代码可在 [Colab Notebook](https://colab.research.google.com/drive/1peJukr-9E1zCo1iAalbgDPJmNMydvQms?usp=sharing) 中免费获得，该 Notebook 可以在提供的（免费）T4 GPU 上运行。此示例的源文件可在 [GitHub 仓库](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/rag) 中找到。\n\n🔗 [Google Colab](https://colab.research.google.com/drive/1peJukr-9E1zCo1iAalbgDPJmNMydvQms?usp=sharing) \\| [GitHub Repo](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/rag)\n\n### 导入\n\n我们首先安装并导入必要的 Python 库。\n\n```python\n!pip install llama-index\n!pip install llama-index-embeddings-huggingface\n!pip install peft\n!pip install auto-gptq\n!pip install optimum\n!pip install bitsandbytes\n## 如果不是在 Colab 上运行，请确保也安装 transformers\n```\n\n```python\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\nfrom llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex\nfrom llama_index.core.retrievers import VectorIndexRetriever\nfrom llama_index.core.query_engine import RetrieverQueryEngine\nfrom llama_index.core.postprocessor import SimilarityPostprocessor\n```\n\n### 设置知识库\n\n我们可以通过定义我们的嵌入模型、块大小和块重叠来配置我们的知识库。在这里，我们使用来自BAAI的\\~33M参数[bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5)嵌入模型，该模型可在Hugging Face hub上获取。其他嵌入模型选项可以在这个[text embedding leaderboard](https://huggingface.co/spaces/mteb/leaderboard)上找到。\n\n```python\n## import any embedding model on HF hub\nSettings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n\nSettings.llm = None # we won't use LlamaIndex to set up LLM\nSettings.chunk_size = 256\nSettings.chunk_overlap = 25\n```\n接下来，我们加载源文档。在这里，我有一个名为“[*articles*](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/rag/articles)”的文件夹，其中包含我在[fat tails](https://towardsdatascience.com/pareto-power-laws-and-fat-tails-0355a187ee6a)上写的3篇Medium文章的PDF版本。如果在Colab中运行，您必须从[GitHub repo](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/rag)下载文章文件夹并手动上传到您的Colab环境。\n\n对于该文件夹中的每个文件，下面的函数将从PDF中读取文本，将其拆分成块（基于之前定义的设置），并将每个块存储在名为*documents*的列表中。\n\n```python\ndocuments = SimpleDirectoryReader(\"articles\").load_data()\n```\n由于这些博客是直接从Medium下载为PDF的，因此它们更像是网页，而不是格式良好的文章。因此，一些块可能包含与文章无关的文本，例如网页标题和Medium文章推荐。\n\n在下面的代码块中，我对documents中的块进行精炼，删除文章主体前后的大部分块。\n\n```python\nprint(len(documents)) # prints: 71\nfor doc in documents:\n    if \"Member-only story\" in doc.text:\n        documents.remove(doc)\n        continue\n\n    if \"The Data Entrepreneurs\" in doc.text:\n        documents.remove(doc)\n\n    if \" min read\" in doc.text:\n        documents.remove(doc)\n\nprint(len(documents)) # prints: 61\n```\n最后，我们可以将精炼后的块存储在向量数据库中。\n\n```python\nindex = VectorStoreIndex.from_documents(documents)\n```\n\n### 设置检索器\n\n在我们的知识库建立之后，我们可以使用 LlamaIndex 的 *VectorIndexRetriever()* 创建一个检索器，它返回与用户查询最相似的 3 个块。\n\n```python\n## set number of docs to retreive\ntop_k = 3\n\n## configure retriever\nretriever = VectorIndexRetriever(\n    index=index,\n    similarity_top_k=top_k,\n)\n```\n接下来，我们定义一个查询引擎，使用检索器和查询返回一组相关的块。\n\n```python\n## assemble query engine\nquery_engine = RetrieverQueryEngine(\n    retriever=retriever,\n    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.5)],\n)\n```\n\n### 使用查询引擎\n\n现在，随着我们的知识库和检索系统的建立，让我们使用它来返回与查询相关的内容。在这里，我们将传递我们向ShawGPT（YouTube评论回复者）提出的相同技术问题，来自[上一篇文章](https://readmedium.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32)。\n\n```python\nquery = \"What is fat-tailedness?\"\nresponse = query_engine.query(query)\n```\n查询引擎返回一个响应对象，其中包含文本、元数据和相关块的索引。下面的代码块返回该信息的更易读版本。\n\n```python\n## reformat response\ncontext = \"Context:\\n\"\nfor i in range(top_k):\n    context = context + response.source_nodes[i].text + \"\\n\\n\"\n\nprint(context)\n```\n\n```python\nContext:\nSome of the controversy might be explained by the observation that log-\nnormal distributions behave like Gaussian for low sigma and like Power Law\nat high sigma [2].\nHowever, to avoid controversy, we can depart (for now) from whether some\ngiven data fits a Power Law or not and focus instead on fat tails.\nFat-tailedness — measuring the space between Mediocristan\nand Extremistan\nFat Tails are a more general idea than Pareto and Power Law distributions.\nOne way we can think about it is that “fat-tailedness” is the degree to which\nrare events drive the aggregate statistics of a distribution. From this point of\nview, fat-tailedness lives on a spectrum from not fat-tailed (i.e. a Gaussian) to\nvery fat-tailed (i.e. Pareto 80 – 20).\nThis maps directly to the idea of Mediocristan vs Extremistan discussed\nearlier. The image below visualizes different distributions across this\nconceptual landscape [2].\n\nprint(\"mean kappa_1n = \" + str(np.mean(kappa_dict[filename])))\n    print(\"\")\nMean κ (1,100) values from 1000 runs for each dataset. Image by author.\nThese more stable results indicate Medium followers are the most fat-tailed,\nfollowed by LinkedIn Impressions and YouTube earnings.\nNote: One can compare these values to Table III in ref [3] to better understand each\nκ value. Namely, these values are comparable to a Pareto distribution with α\nbetween 2 and 3.\nAlthough each heuristic told a slightly different story, all signs point toward\nMedium followers gained being the most fat-tailed of the 3 datasets.\nConclusion\nWhile binary labeling data as fat-tailed (or not) may be tempting, fat-\ntailedness lives on a spectrum. Here, we broke down 4 heuristics for\nquantifying how fat-tailed data are.\n\nPareto, Power Laws, and Fat Tails\nWhat they don’t teach you in statistics\ntowardsdatascience.com\nAlthough Pareto (and more generally power law) distributions give us a\nsalient example of fat tails, this is a more general notion that lives on a\nspectrum ranging from thin-tailed (i.e. a Gaussian) to very fat-tailed (i.e.\nPareto 80 – 20).\nThe spectrum of Fat-tailedness. Image by author.\nThis view of fat-tailedness provides us with a more flexible and precise way of\ncategorizing data than simply labeling it as a Power Law (or not). However,\nthis begs the question: how do we define fat-tailedness?\n4 Ways to Quantify Fat Tails\n```\n\n### 将 RAG 添加到 LLM\n\n我们首先从 Hugging Face hub 下载 [微调模型](https://readmedium.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32)。\n\n```python\n## load fine-tuned model from hub\nfrom peft import PeftModel, PeftConfig\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name,\n                                             device_map=\"auto\",\n                                             trust_remote_code=False,\n                                             revision=\"main\")\n\nconfig = PeftConfig.from_pretrained(\"shawhin/shawgpt-ft\")\nmodel = PeftModel.from_pretrained(model, \"shawhin/shawgpt-ft\")\n\n## load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n```\n作为基线，我们可以观察模型在没有任何文章上下文的情况下如何回应技术问题。为此，我们使用 lambda 函数创建一个提示模板，该函数接受观众评论并返回 LLM 的提示。有关此提示来源的更多详细信息，请参见本系列的 [上一篇文章](https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32#5aad)。\n\n```python\n## prompt (no context)\nintstructions_string = f\"\"\"ShawGPT, functioning as a virtual data science \\\nconsultant on YouTube, communicates in clear, accessible language, escalating \\\nto technical depth upon request. It reacts to feedback aptly and ends \\\nresponses with its signature '–ShawGPT'.\n\nShawGPT will tailor the length of its responses to match the viewer's comment, \\\nproviding concise acknowledgments to brief expressions of gratitude or \\\nfeedback, thus keeping the interaction natural and engaging.\n\nPlease respond to the following comment.\n\"\"\"\n\nprompt_template = lambda comment: f'''[INST] {intstructions_string} \\n{comment} \\n[/INST]'''\ncomment = \"What is fat-tailedness?\"\n\nprompt = prompt_template(comment)\nprint(prompt)\n```\n\n```python\n[INST] ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. It reacts to feedback aptly and ends responses with its signature '–ShawGPT'. ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, thus keeping the interaction natural and engaging.\n\nPlease respond to the following comment.\n \nWhat is fat-tailedness? \n[/INST]\n```\n接下来，我们可以使用以下代码将此提示传递给模型。\n\n```python\nmodel.eval()\n\ninputs = tokenizer(prompt, return_tensors=\"pt\")\noutputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), \n                          max_new_tokens=280)\n\nprint(tokenizer.batch_decode(outputs)[0])\n```\n这是模型的响应（没有上下文）。\n\n```python\nGreat question!\n\nFat-tailedness is a statistical property of a distribution. In simple terms, \nit refers to the presence of extreme outliers or heavy tails in the \ndistribution.\n\nFor instance, consider the distribution of heights in a population. A normal \ndistribution would have most people clustered around an average height with a \nfew people deviating slightly from the mean. However, in a fat-tailed \ndistribution, you would observe a larger number of people being significantly \ntaller or shorter than the average.\n\nIn finance, fat-tailedness is often used to describe the presence of extreme \nmarket moves or outliers in financial returns. It's an important concept in \nrisk management and financial modeling.\n\nI hope this explanation helps! Let me know if you have any questions.\n–ShawGPT\n```\n虽然响应的风格和格式都很好，但模型对 fat-tailedness 的解释与我在我的 [视频](https://www.youtube.com/playlist?list=PLz-ep5RbHosVrT89BRNX-IGKHYxvx9wqi) 和 [博客系列](https://towardsdatascience.com/pareto-power-laws-and-fat-tails-0355a187ee6a) 中定义的有所不同。\n\n让我们看看当我们包含适当的上下文时模型的响应会发生什么。为此，我们创建另一个提示模板，该模板也可以从 RAG 系统中获取上下文。\n\n```python\n## prompt (with context)\nprompt_template_w_context = lambda context, comment: f\"\"\"[INST]ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. \\\nIt reacts to feedback aptly and ends responses with its signature '–ShawGPT'. \\\nShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, \\\nthus keeping the interaction natural and engaging.\n\n{context}\nPlease respond to the following comment. Use the context above if it is helpful.\n\n{comment}\n[/INST]\n\"\"\"\n```\n接下来，我们将查询引擎中的上下文和观众评论传递给模型。\n\n```python\nprompt = prompt_template_w_context(context, comment)\n\ninputs = tokenizer(prompt, return_tensors=\"pt\")\noutputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=280)\n\nprint(tokenizer.batch_decode(outputs)[0])\n```\n这是新的响应（带上下文）。\n\n```python\nGreat question!\n\nFat-tailedness is a property of a distribution that describes how much the \npresence of extreme values (rare events) affects the overall statistics of the \ndistribution.\n\nIn the context of the video, the author discusses how Mediocristan and \nExtremistan distributions differ in terms of fat-tailedness. Mediocristan \ndistributions have a relatively small impact from extreme values, while \nExtremistan distributions are heavily influenced by extreme values.\n\nThe author also mentions that fat-tailedness is a more general concept than \nspecific distributions like Pareto or Power Laws. Instead, it lives on a \nspectrum ranging from thin-tailed (Gaussian) to very fat-tailed (Pareto 80-20).\n\nI hope that helps clarify things a bit! Let me know if you have any questions.\n–ShawGPT\n```\n这比没有上下文的响应更好地捕捉了我对 fat tails 的解释，并且甚至提到了 Mediocristan 和 Extremistan 的细分概念。\n\n## 接下来是什么？\n\n在这里，我为初学者提供了一个关于 RAG 的友好介绍，并分享了如何使用 LlamaIndex 实现它的具体示例。RAG 使我们能够通过可更新和特定领域的知识来改善 LLM 系统。\n\n虽然最近的 AI 热潮主要集中在构建 AI 助手上，但一个强大的（但不那么流行的）创新来自于文本嵌入（即我们用来进行检索的东西）。在本系列的下一篇文章中，我将更详细地探讨 **文本嵌入**，包括它们如何用于 **语义搜索** 和 **分类任务**。\n\n**更多关于 LLM 的内容 👇**\n\n## 资源\n\n**连接**: [我的网站](https://shawhintalebi.com/) \\| [预约电话](https://calendly.com/shawhintalebi)\n\n**社交**: [YouTube 🎥](https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA) \\| [LinkedIn](https://www.linkedin.com/in/shawhintalebi/) \\| [Instagram](https://www.instagram.com/shawhintalebi)\n\n**支持**: [请我喝杯咖啡](https://www.buymeacoffee.com/shawhint) ☕️\n\n\\[1] [RAG \\> FT (经验性)](https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb)\n\n\\[2] [LlamaIndex 网络研讨会：为生产构建 LLM 应用程序，第一部分（与 Anyscale 联合主持）](https://www.youtube.com/watch?v=efbn-3tPI_M)\n\n\\[3] [LlamaIndex 文档](https://docs.llamaindex.ai/en/stable/understanding/loading/loading.html)\n\n\\[4] [LlamaIndex 网络研讨会：使 RAG 准备好生产](https://www.youtube.com/watch?v=Zj5RCweUHIk&list=WL&index=4)\n\n"},{"lang":"zh","group":"blog","slug":"blog/how-to-run-nvidia-llama-3-1-nemotron-70b-instruct-locally-a58ad283aaff","frontmatter":{"title":"如何在本地运行 Nvidia 的 llama-3.1-nemotron-70b-instruct","meta_title":"如何在本地运行 Nvidia 的 llama-3.1-nemotron-70b-instruct","description":"在本地运行大型语言模型 (LLM) 在开发人员、研究人员和 AI 爱好者中越来越受欢迎。其中之一就是……","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*fqVKJkw5sQvLtIsyCcengQ.png","categories":["Programming","Technology","Science"],"author":"Rifx.Online","tags":["Nvidia","llama","Ollama","llama.cpp","Transformers"],"draft":false,"slug":"blog/how-to-run-nvidia-llama-3-1-nemotron-70b-instruct-locally-a58ad283aaff"},"content":"\n\n\n在开发者、研究人员和 AI 爱好者中，本地运行大型语言模型（LLMs）变得越来越受欢迎。其中一个引起广泛关注的模型是 llama-3.1-nemotron-70b-instruct，这是 NVIDIA 定制的强大 LLM，旨在增强生成响应的有用性。在本综合指南中，我们将探讨多种方法，以便在您的本地机器上运行此模型，首先介绍用户友好的 Ollama 平台。\n\n> 在开始之前，如果您正在寻找一个一体化的 AI 平台，以便在一个地方管理所有 AI 订阅，包括所有 LLM（如 GPT-o1、Llama 3.1、Claude 3.5 Sonnet、Google Gemini、未审查的 LLM）和图像生成模型（FLUX、Stable Diffusion 等），请使用 Anakin AI 来管理它们！\n\n\n\n## 方法 1：使用 Ollama 本地运行 llama-3.1-nemotron-70b-instruct\n\nOllama 是一个出色的工具，用于本地运行 LLM，提供简单的设置过程并支持多种模型，包括 llama-3.1-nemotron-70b-instruct。\n\n### 安装\n\n1. 访问官方 Ollama 网站 ([https://ollama.ai](https://ollama.ai/))，下载适合您操作系统的版本。\n2. 通过在终端中运行以下命令来安装 Ollama：\n\n\n```python\ncurl https://ollama.ai/install.sh | sh\n```\n\n### 运行 llama-3.1-nemotron\n\n安装 Ollama 后，您可以通过一个简单的命令轻松运行 llama-3.1-nemotron-70b-instruct 模型：\n\n```python\nollama run nemotron:70b-instruct-q5_K_M\n```\n该命令将在您的系统上下载模型（如果尚未存在），并启动一个交互式会话。\n\n### 使用模型\n\n在模型加载后，您可以通过输入提示开始与其互动。例如：\n\n```python\n>>> What are the key features of llama-3.1-nemotron-70b-instruct?\n\nLlama-3.1-Nemotron-70B-Instruct is a large language model with several key features:\n1. Customized by NVIDIA: The model has been fine-tuned by NVIDIA to improve the helpfulness and quality of its responses.\n2. Based on Llama 3.1: It builds upon the Llama 3.1 architecture, which is known for its strong performance across various tasks.\n3. 70 billion parameters: This large parameter count allows for complex reasoning and a wide range of capabilities.\n4. Instruct-tuned: The model is specifically designed to follow instructions and generate helpful responses to user queries.\n5. RLHF training: It has been trained using Reinforcement Learning from Human Feedback, specifically the REINFORCE algorithm.\n6. Specialized reward model: The training process utilized Llama-3.1-Nemotron-70B-Reward for optimization.\n7. HelpSteer2-Preference prompts: These were used during the training process to further improve the model's helpfulness.\n8. Extended context length: Like other Llama 3.1 models, it likely supports a longer context window of 128K tokens.\n9. Multilingual capabilities: It can understand and generate text in multiple languages.\n10. Strong reasoning abilities: The model excels in tasks requiring complex reasoning and problem-solving.\nThese features make llama-3.1-nemotron-70b-instruct a powerful and versatile language model suitable for a wide range of applications, from general conversation to specialized tasks in various domains.\n```\n对于更高级的用例，您可以使用像 Langchain 这样的库将 Ollama 与 Python 集成。以下是一个简单的示例：\n\n```python\npython\n\nfrom langchain.llms import Ollama\n\nollama = Ollama(base_url=\"http://localhost:11434\", model=\"nemotron:70b-instruct-q5_K_M\")\nresponse = ollama.generate(\"Explain the concept of quantum entanglement.\")\nprint(response)\n```\n这使您能够无缝地将模型集成到您的 Python 项目和应用程序中。\n\n## 方法 2：使用 llama.cpp\n\nllama.cpp 是一个流行的 C++ 实现的 Llama 模型推理，针对 CPU 使用进行了优化。虽然它可能需要比 Ollama 更多的设置，但它提供了更大的灵活性和对模型参数的控制。\n\n### 安装\n\n1. 克隆 llama.cpp 仓库：\n\n```python\ngit clone https://github.com/ggerganov/llama.cpp.git\ncd llama.cpp\n```\n1. 构建项目：\n\n```python\nmake\n```\n\n### 下载模型\n\n要运行 llama-3.1-nemotron-70b-instruct，您需要下载模型权重。这些通常以 GGML 或 GGUF 格式提供。您可以在 Hugging Face 等平台上找到预先转换的模型。\n\n```python\nmkdir models\ncd models\nwget https://huggingface.co/TheBloke/Llama-3.1-Nemotron-70B-Instruct-GGUF/resolve/main/llama-3.1-nemotron-70b-instruct.Q4_K_M.gguf\n```\n\n### 运行模型\n\n一旦你拥有模型文件，就可以使用以下命令运行它：\n\n```python\n./main -m models/llama-3.1-nemotron-70b-instruct.Q4_K_M.gguf -n 1024 -p \"Hello, how are you today?\"\n```\n该命令加载模型并生成对给定提示的响应。你可以调整各种参数，比如生成的令牌数量 (-n) 或温度以控制随机性。\n\n## 方法 3：使用 Hugging Face Transformers\n\nHugging Face 的 Transformers 库提供了一个高层次的 API，用于处理各种语言模型，包括 llama-3.1-nemotron-70b-instruct。\n\n**安装**\n\n首先，安装必要的库：\n\n\n```python\npip install transformers torch accelerate\n```\n**运行模型**\n\n以下是一个加载和使用模型的 Python 脚本：\n\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nmodel_name = \"meta-llama/Llama-3.1-Nemotron-70b-instruct\"\n## Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n## Prepare the input\nprompt = \"Explain the concept of quantum computing in simple terms.\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n## Generate the response\nwith torch.no_grad():\n    outputs = model.generate(**inputs, max_new_tokens=100)\n## Decode and print the response\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response)\n```\n这种方法允许对模型的行为进行更细粒度的控制，并与其他 Hugging Face 工具和管道集成。\n\n## 结论\n\n在本地运行 llama-3.1-nemotron-70b-instruct 为开发者和研究人员打开了无限可能。无论您选择 Ollama 的简单性、llama.cpp 的灵活性，还是 Hugging Face Transformers 的集成功能，您现在都有工具可以在自己的硬件上利用这一先进语言模型的强大能力。在探索 llama-3.1-nemotron-70b-instruct 的能力时，请记住在性能与资源限制之间取得平衡，并始终考虑您应用的伦理影响。负责任的使用，这个模型可以成为推动自然语言处理和 AI 驱动应用可能性的宝贵资产。\n\n"},{"lang":"zh","group":"blog","slug":"blog/intelli-agent-langchain-crewai-and-autogen-compared-369a527b2026","frontmatter":{"title":"智能代理：Langchain、CrewAI 和 AutoGen 比较","meta_title":"智能代理：Langchain、CrewAI 和 AutoGen 比较","description":"1. AI代理框架概述","date":"2024-11-08T00:22:33.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*uswz_9OuqiMWUL9kfKXeaQ.png","categories":["Programming","Machine Learning","Autonomous Systems"],"author":"Rifx.Online","tags":["Langchain","CrewAI","AutoGen","Swarm","agents"],"draft":false,"slug":"blog/intelli-agent-langchain-crewai-and-autogen-compared-369a527b2026"},"content":"\n\n\n\n\n## 1\\. AI代理框架概述\n\n在人工智能快速发展的领域，选择合适的框架是每个数据科学家和开发者必须做出的关键决策。AI代理生态系统正在迅速演变，提供越来越复杂的解决方案来自动化和优化复杂的流程。\n\n智能代理革命带来了几种框架，每种框架都有其独特的特点。Langchain、CrewAI、AutoGen和Swarm在这个场景中脱颖而出，各自提供了管理和协调AI代理的独特方法。\n\n本次基准测试的主要目标是对每个框架的能力、优势和局限性进行深入评估。最佳选择取决于多个因素，包括项目的复杂性、可用资源和实施的具体目标。\n\n当前AI的趋势清晰地指向越来越自主和协作的系统。这些框架促进代理之间的互动、管理共享内存和协调复杂任务的能力，使它们成为开发先进AI解决方案的关键工具。\n\n## 2\\. Langchain: 多功能性与模块化\n\nLangchain 以其极具灵活性的模块化架构脱颖而出。这个框架提供了一种结构化的方法来构建 AI 应用程序，使开发者能够通过相互连接的组件构建复杂的系统。\n\n内存管理是 Langchain 最显著的优势之一。该框架实现了复杂的机制来维护对话上下文，使代理能够访问历史信息并在时间上保持一致的对话。\n\nLangchain 生态系统支持与外部 API、数据库和其他服务的广泛集成。这一特性使得创建可以利用不同数据源和能力的自定义解决方案变得简单。\n\n该框架的架构灵活性使您能够轻松实现不同类型的专用代理。从语义搜索到自然语言处理，Langchain 提供了预配置的工具，显著加快了开发过程。\n\n一个特别重要的方面是以逻辑和顺序的方式链接操作的能力。这个被称为 Chain 的特性，使您能够在保持清晰和可维护结构的同时构建复杂的工作流程。开发者可以定义自定义的动作序列，其中链中的每个组件逐步处理和转换数据。\n\n围绕 Langchain 的活跃社区不断贡献新的组件和集成。这一不断增长的生态系统为各种用例提供了开箱即用的解决方案，从内容生成和文档分析到创建复杂的虚拟助手。\n\n在性能方面，Langchain 在资源管理方面表现出色。该框架实现了智能缓存和 API 调用优化机制，显著降低了运营成本和响应时间。\n\n## 3\\. CrewAI: 智能代理之间的协作\n\nCrewAI 引入了一种基于专业代理之间协作的创新范式。该框架的特点在于能够将代理组织成功能团队，其中每个成员为实现共同目标贡献特定技能。\n\nCrewAI 的层次结构促进了代理之间交互的高效管理。该框架实施了一种复杂的任务分配系统，每个代理可以根据其技能将特定任务分配给其他团队成员。\n\nCrewAI 中的代理间通信基于一种先进的协议，允许结构化和情境化的信息交换。代理可以实时共享知识、中间结果和反馈，创造一个动态和适应性的协作环境。\n\n一个特别创新的方面是动态角色系统。代理可以根据项目的上下文和需求承担不同的责任。这种灵活性使您能够优化资源利用并最大化虚拟团队的效率。\n\n冲突管理和问题解决通过一种复杂的分布式共识机制来处理。代理可以独立协商解决方案、提出替代方案并达成共享决策。\n\nCrewAI 的未来潜力在业务流程自动化领域尤其令人期待。该框架正在发展以包括：\n\n* 代理之间的协作学习\n* 自动团队优化\n* 资源的动态扩展\n* 与外部系统的高级集成\n\n```\n## Sample code block\ndef example_function():\n    print(\"This is a sample function.\")\n```\n\n## 4\\. AutoGen 和 Swarm：代理创建的创新\n\nAutoGen 以其革命性的方法在自动生成多代理系统方面脱颖而出。该框架擅长创建模块化架构，能够根据项目的具体需求自主演变和适应。\n\nAutoGen 的一个显著特点是其自我优化的能力。生成的代理可以：\n\n* 根据收到的反馈改变行为\n* 自动优化配置参数\n* 为新功能生成功能代码\n* 实施自适应问题解决策略\n\n另一方面，Swarm 专注于代理编排的轻量和高效。其简约的方法在以下方面提供了显著优势：\n\n* 优化资源消耗\n* 卓越的执行速度\n* 简化的扩展\n* 系统的可维护性\n\n这两个框架的直接比较揭示了有趣的互补性。虽然 AutoGen 在复杂解决方案的自主生成方面表现出色，但 Swarm 在高效管理大量简单代理方面更为出色。\n\n## 最终思考\n\n所呈现的比较概述显示，智能代理领域正经历着非凡的创新阶段。每个分析的框架为人工智能生态系统带来了独特的价值，帮助塑造智能自动化的未来。\n\n行业专业人士的关键思考：\n\n1. 可用工具的多样化不应被视为障碍，而应视为专业化和持续创新的机会。\n2. 对这些框架的深入理解的投资代表了科技就业市场的竞争优势。\n3. 采用不同解决方案的灵活性对企业级项目的成功至关重要。\n\n作为首席数据科学家，我建议：\n\n* 在工具选择中保持务实的方法\n* 优先考虑能够保证可扩展性和可维护性的解决方案\n* 投资于团队的持续培训\n* 不断监测行业的技术演变\n\n智能代理的未来看起来充满希望，明显的趋势包括：\n\n* 越来越复杂的混合系统\n* 不同平台之间的无缝集成\n* 决策过程的高级自动化\n* 解决方案的定制化推动\n\n成功的关键在于有效地协调这些工具，创造出不仅解决当前问题的解决方案，而且也为未来挑战做好准备。\n\n"},{"lang":"zh","group":"blog","slug":"blog/introducing-atomic-agents-1-0-a-modular-framework-for-building-agentic-ai-with-cli-support-2b01b7165ace","frontmatter":{"title":"Atomic Agents 1.0 简介：构建 Agentic AI 的模块化框架","meta_title":"Atomic Agents 1.0 简介：构建 Agentic AI 的模块化框架","description":"想象一下，构建 AI 应用程序就像组装乐高积木一样轻松。这就是 Atomic Agents 背后的想法，它是一个模块化框架，用于……","date":"2024-11-08T00:19:37.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*BZGf8BCnCJiFlKZ5.png","categories":["Programming","Machine Learning","Autonomous Systems"],"author":"Rifx.Online","tags":["modular","framework","Atomic","assembler","schema"],"draft":false,"slug":"blog/introducing-atomic-agents-1-0-a-modular-framework-for-building-agentic-ai-with-cli-support-2b01b7165ace"},"content":"\n\n\n想象一下，构建 AI 应用程序就像组装乐高积木一样轻松。这就是 [Atomic Agents](https://github.com/BrainBlend-AI/atomic-agents) 的理念，一个基于 **Atomic Design** 原则的模块化框架，用于构建 AI 代理。随着 **1\\.0 版本** 的发布，Atomic Agents 引入了一个强大的 CLI，称为 **Atomic Assembler**，使构建、管理和部署 AI 应用程序变得更加简单。\n\n## 为什么选择原子代理？\n\n许多现有的**代理人工智能**框架专注于构建自主的多代理系统，这些系统更像是好奇心的产物，而不是实用工具。虽然这些系统可能引人入胜，但它们往往缺乏现实应用所需的可预测性和控制能力。\n\n企业通常并不希望有一个每次都以不同风格撰写文章的机器人。他们希望在风格、结构和语调上保持一致，以与其品牌形象相一致。微调模型是一种方法，但它需要大量的数据和资源，并且在使用最新模型（如GPT-4）时并不总是可行的。\n\n原子代理旨在通过提供以下功能来解决这一问题：\n\n* **模块化**：通过组合简单、可互换的组件构建复杂的人工智能系统。\n* **原子性**：原子代理中的每个组件、每个工具、每个代理、每个上下文提供者，都尽可能单一目的和可重用，确保良好的关注点分离。\n* **控制**：微调每个单独的步骤和组件，从系统提示到工具。\n* **可预测性**：确保可重复和可靠的输出，适合商业用例。\n* **可扩展性**：轻松添加或替换组件，而无需彻底改造整个系统。\n\n## 传统模块化方法\n\n在传统软件开发中，复杂问题被分解为更小、可管理的部分：\n\n1. **定义问题**：从流程、用户故事或客户旅程开始。\n2. **分解**：将问题划分为更小、可解决的任务。\n3. **开发模块化代码**：编写处理特定任务的函数或类。\n4. **集成**：将这些模块组合成完整的应用程序。\n\nAtomic Agents 将这种模块化和可预测性带入 AI 代理开发中。\n\n## 真实世界场景\n\n与其构建一个“写博客文章”的单体 AI 系统，不如设计一个模块化系统，能够：\n\n1. **生成** 与主题相关的查询。\n2. **识别** 最相关的前 X 篇文章。\n3. **访问** 每篇识别文章的页面。\n4. **提取** 每篇文章的文本。\n5. **生成** 每篇文章的摘要。\n6. **存储** 摘要到向量数据库中。\n7. **生成** 与主题相关的问题。\n8. **使用** 向量数据库回答这些问题。\n9. **综合** 答案成一篇连贯的博客文章。\n\n这种方法虽然更冗长，但提供了更大的控制、可靠性和适用于现实商业应用的适应性。\n\n## CLI的介绍：Atomic Assembler\n\n版本1.0中的一个重要新增功能是**Atomic Assembler** CLI。这个命令行工具允许您：\n\n* **下载和管理工具**：轻松将新工具或代理添加到您的项目中。\n* **避免不必要的依赖**：仅安装您所需的内容。\n* **轻松修改工具**：每个工具都有自己的测试和文档。\n* **直接访问工具**：如果您愿意，可以手动管理工具，而无需使用CLI。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*aDceAIINxyFDOvle.png)\n\n## 代理的构成\n\nAI 代理，特别是在 Atomic Agents 框架中，由几个关键组件组成：\n\n* **系统提示**：定义代理的行为和目的。\n* **用户输入**：用户提供的数据。\n* **工具**：代理可以利用的外部函数或 API。\n* **记忆**：跟踪对话或状态。\n\n每个组件都设计为模块化和可互换，遵循关注点分离和单一责任原则。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*yt-5SoQC6uXTAd1-)\n\n## 模块化的力量\n\n通过将代理分解为这些基本组件，您可以：\n\n* **更换工具** 而不影响系统的其余部分。\n* **微调提示** 以调整代理的行为。\n* **无缝连接代理和工具**，通过匹配它们的输入和输出模式。\n\n## 使用命令行界面：原子汇编器\n\n## 安装\n\n要开始使用 Atomic Agents 和 CLI，请通过 pip 安装该软件包：\n\n```python\npip install atomic-agents\n```\n\n## 运行 CLI\n\n使用以下命令启动 CLI：\n\n```python\natomic\n```\n\n或者，如果您使用 Poetry 安装了 Atomic Agents：\n\n```python\npoetry run atomic\n```\n\n您将看到一个菜单，用于下载和管理工具：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*SzRlpA0-ivcE2qhk)\n\n*图像：Atomic CLI 主菜单*\n\n每个工具包括：\n\n* **输入模式**\n* **输出模式**\n* **使用示例**\n* **依赖项**\n* **安装说明**\n\n## 管理工具\n\nAtomic Assembler CLI 提供了对您工具的完全控制，让您可以：\n\n* **避免依赖杂乱**：仅安装您需要的工具。\n* **轻松修改工具**：每个工具都是自包含的，拥有自己的测试。\n* **直接访问工具**：如果您愿意，可以手动管理工具文件夹。\n\n## 上下文提供者\n\nAtomic Agents 引入了 **上下文提供者**，以增强您的代理的动态上下文。上下文提供者允许您在运行时将额外信息注入代理的系统提示中。\n\n## 使用上下文提供者\n\n**创建上下文提供者类**：子类化 `SystemPromptContextProviderBase` 并实现 `get_info()` 方法。\n\n```python\nfrom atomic_agents.lib.components.system_prompt_generator import SystemPromptContextProviderBase   \n\nclass SearchResultsProvider(SystemPromptContextProviderBase):\n      def __init__(self, title: str, search_results: List[str]):\n          super().__init__(title=title)\n          self.search_results = search_results\n\n       def get_info(self) -> str:\n          return \"\\n\".join(self.search_results)\n```\n\n**将上下文提供者注册到代理**：\n\n```python\n## 使用动态数据初始化上下文提供者\nsearch_results_provider = SearchResultsProvider(\n      title=\"搜索结果\",\n      search_results=[\"结果 1\", \"结果 2\", \"结果 3\"]\n)   \n\n## 将上下文提供者注册到代理  \nagent.register_context_provider(\"search_results\", search_results_provider)\n```\n\n这使得您的代理能够在其系统提示中包含动态数据，如搜索结果，从而根据最新信息增强其响应。\n\n## 链接模式和代理\n\nAtomic Agents 通过对齐它们的输入和输出模式来简化代理和工具的链接。这个设计促进了模块化和可重用性。\n\n### 示例：为不同搜索提供者生成查询\n\n假设您有一个生成搜索查询的代理，并且您希望将这些查询与不同的搜索工具一起使用。通过将代理的输出模式与搜索工具的输入模式对齐，您可以轻松地将它们串联或在提供者之间切换。\n\n```python\nimport instructor\nimport openai\nfrom pydantic import Field\nfrom atomic_agents.agents.base_agent import BaseIOSchema, BaseAgent, BaseAgentConfig\nfrom atomic_agents.lib.components.system_prompt_generator import SystemPromptGenerator\n\n## Import the search tool\nfrom web_search_agent.tools.searxng_search import SearxNGSearchTool\nclass QueryAgentInputSchema(BaseIOSchema):\n    \"\"\"Input schema for the QueryAgent.\"\"\"\n    instruction: str = Field(..., description=\"Instruction to generate search queries for.\")\n    num_queries: int = Field(..., description=\"Number of queries to generate.\")\n\n\n## Initialize the query agent\nquery_agent = BaseAgent(\n    BaseAgentConfig(\n        client=instructor.from_openai(openai.OpenAI()),\n        model=\"gpt-4\",\n        system_prompt_generator=SystemPromptGenerator(\n            background=[\n                \"You are an intelligent query generation expert.\",\n                \"Your task is to generate diverse and relevant queries based on a given instruction.\"\n            ],\n            steps=[\n                \"Receive the instruction and the number of queries.\",\n                \"Generate the queries in JSON format.\"\n            ],\n            output_instructions=[\n                \"Ensure each query is unique and relevant.\",\n                \"Provide the queries in the expected schema.\"\n            ],\n        ),\n        input_schema=QueryAgentInputSchema,\n        output_schema=SearxNGSearchTool.input_schema,  # Align output schema\n    )\n)\n```\n\n**模块化**：通过将`query_agent`的`output_schema`设置为与`SearxNGSearchTool`的`input_schema`匹配，您可以直接将代理的输出用作工具的输入。\n\n**可切换性**：要切换到不同的搜索提供者，导入另一个搜索工具并更新`output_schema`：\n\n```python\n## Import a different search tool\nfrom web_search_agent.tools.another_search import AnotherSearchTool\n\n## Update the output schema\nquery_agent.config.output_schema = AnotherSearchTool.input_schema\n```\n\n## 示例：构建一个简单的 AI 代理\n\n现在我们已经介绍了基础知识，让我们使用 Atomic Agents 构建一个简单的 AI 代理，并探讨它的内部工作原理。\n\n## 第一步：安装\n\n首先，安装必要的软件包：\n\n```python\npip install atomic-agents openai instructor\n```\n\n## 步骤 2：导入组件\n\n导入必要的组件：\n\n```python\nimport os\nfrom atomic_agents.agents.base_agent import BaseAgent, BaseAgentConfig, BaseIOSchema\nfrom atomic_agents.lib.components.system_prompt_generator import SystemPromptGenerator\nfrom atomic_agents.lib.components.agent_memory import AgentMemory\nfrom pydantic import Field\nimport instructor\nimport openai\n```\n\n## 步骤 3：定义自定义输出模式\n\n```python\nclass CustomOutputSchema(BaseIOSchema):\n    chat_message: str = Field(..., description=\"The chat message from the agent.\")\n    suggested_questions: List[str] = Field(..., description=\"Suggested follow-up questions.\")\n```\n\n## 步骤 4：设置系统提示\n\n```python\nsystem_prompt_generator = SystemPromptGenerator(\n    background=[\"这个助手知识渊博、乐于助人，并建议后续问题。\"],\n    steps=[\n        \"分析用户的输入，以理解上下文和意图。\",\n        \"制定相关且信息丰富的回应。\",\n        \"为用户生成 3 个建议的后续问题。\"\n    ],\n    output_instructions=[\n        \"对用户查询提供清晰简洁的信息。\",\n        \"在每个回应的结尾提供 3 个与用户相关的建议问题。\"\n    ]\n)\n```\n\n## 第5步：初始化代理\n\n```python\n## Initialize memory (optional)\nmemory = AgentMemory()\n\n## Initialize the agent\nagent = BaseAgent(\n    config=BaseAgentConfig(\n        client=instructor.from_openai(openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))),\n        model=\"gpt-4o-mini\",\n        system_prompt_generator=system_prompt_generator,\n        memory=memory,\n        output_schema=CustomOutputSchema\n    )\n)\n```\n\n## 第6步：使用代理\n\n```python\nuser_input = \"Can you explain the benefits of using Atomic Agents?\"\nresponse = agent.run(agent.input_schema(chat_message=user_input))\nprint(f\"Agent: {response.chat_message}\")\nprint(\"Suggested questions:\")\nfor question in response.suggested_questions:\n    print(f\"- {question}\")\n```\n\n## 幕后发生了什么？\n\n* **System Prompt**: 定义代理的行为并指导LLM。\n* **Input Schema**: 验证用户的输入。\n* **Output Schema**: 确保代理的响应符合预期格式。\n* **Memory**: 记录对话历史。\n\n## 结论\n\nAtomic Agents 1\\.0 为 AI 代理开发带来了模块化、控制和灵活性。随着 Atomic Assembler CLI 的引入以及上下文提供者和模式链等功能，构建复杂的 AI 应用程序变得前所未有的简单。\n\n无论您是希望构建 AI 驱动工具的开发人员，还是希望自动化复杂任务的企业，Atomic Agents 都提供了创建可靠且易于维护的 AI 系统的基础构件。\n\n## 今天开始\n\n* **GitHub 仓库**: [BrainBlend\\-AI/atomic\\-agents](https://github.com/BrainBlend-AI/atomic-agents)\n* **API 文档**: [Atomic Agents API 文档](https://brainblend-ai.github.io/atomic-agents/)\n* **示例目录**: [Atomic 示例](https://github.com/BrainBlend-AI/atomic-agents/tree/main/atomic-examples)\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/introduction-to-llava-a-multimodal-ai-model-2a2fa530ace4","frontmatter":{"title":"LLaVA 简介：一种多模式 AI 模型","meta_title":"LLaVA 简介：一种多模式 AI 模型","description":"LLaVA 是一个端到端训练的大型多模式模型，旨在根据视觉输入理解和生成内容……","date":"2024-10-29T12:48:10.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*0At7tXF5ejho9Y46E3uGtg.png","categories":["Natural Language Processing","Computer Vision","Generative AI"],"author":"Rifx.Online","tags":["LLaVA","GPT-4","multimodal","visual","encoder"],"draft":false,"slug":"blog/introduction-to-llava-a-multimodal-ai-model-2a2fa530ace4"},"content":"\n\n\n\n\nLLaVA是一个端到端训练的大型多模态模型，旨在理解和生成基于视觉输入（图像）和文本指令的内容。它结合了视觉编码器和语言模型的能力，以处理和响应多模态输入。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*mjzqL0BHzdPoN-Jjruh52A.png)\n\n## LLaVA 的输入和输出：连接视觉与文本领域：\n\nLLaVA 的输入有两个方面：\n\n1. 视觉输入：模型可以查看和分析的图像，以提取视觉特征和上下文信息。\n2. 文本指令：文本输入，可以是问题或命令，指导模型关注什么或执行与视觉输入相关的什么任务。\n\nLLaVA 的输出是基于文本的，可能会根据任务而有所不同：\n\n1. 描述性文本：如果任务是描述视觉内容，LLaVA 可以输出图像的详细描述，识别对象、动作和场景。\n2. 问题回答：对于问答任务，LLaVA 生成的回答可以解答关于视觉输入的问题，可能涉及基于图像内容的推理和推断。\n3. 后续行动：对于需要行动的指令，例如编辑图像或检索更多信息，LLaVA 可以提供适当的文本响应，指示所采取的行动或建议应该做什么。\n\n## 比较分析：LLaVa与当代多模态模型\n\n多模态人工智能的领域正在快速发展，出现了CLIP、BLIP等创新，以及最近推出的LLaVa。本小节将LLaVa的独特架构和方法与这些当代模型进行比较，突出其进步和区别，使其与众不同。\n\n### CLIP: 开创多模态理解的先河\n\nCLIP (Contrastive Language–Image Pre\\-training) 在多模态人工智能领域中迈出了革命性的一步，在各种视觉任务中提供了强大的性能。它在自然语言描述的背景下理解图像的能力为该领域设定了新的基准。CLIP 通过大规模的预训练方法将图像与文本描述对齐，使模型能够在一系列视觉任务上进行零样本学习。然而，CLIP 主要关注图像与文本之间的高层次关联，并不具备深入推理或对话参与的能力。\n\n### BLIP: 连接语言与图像感知\n\n在CLIP奠定的基础上，BLIP（Bootstrapped Language Image Pre-training）通过引入自引导预训练策略，扩展了多模态模型的能力。这种方法通过不断从自身的预测中学习，完善模型的视觉理解，从而帮助改善语言与视觉内容之间的对齐。BLIP在需要更精确视觉识别和语言理解的任务上表现出增强的性能。\n\n相比之下，LLaVa采取了不同的路径，通过利用GPT-4的语言生成能力来策划其遵循指令的数据。这不仅导致了一个捕捉更广泛人类互动范围的数据集，还使LLaVa能够进行更复杂的推理和深入的对话能力。\n\n## LLaVa的独特之处：是模型架构还是其他因素？\n\n根据我们的观点，LLaVA的优势主要在于其数据策划能力，而非架构选择。LLaVA的重大进展主要得益于其利用GPT-4进行数据策划。与传统的静态数据集不同，LLaVA使用ChatGPT-4生成动态、指导性的数据，积极参与各种视觉和文本场景中的训练过程。\n\n通过使用GPT-4，LLaVA生成的 数据集紧密模拟自然语言和视觉感知，脱离了传统的手动数据集生成方法。这种创新的方法不仅使AI能够理解和推理，还使其更接近于准确反映人类智能。\n\n### LLaVa中的数据整理策略\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*LzastWLkzPeMB_28Nr7Y9A.png)\n\nLLaVa，即大型语言与视觉助手，不仅以其先进的神经架构而闻名，还以其开创性的数据整理方法而脱颖而出。通过利用GPT-4，它彻底改变了传统的数据准备方法，构建出一个反映现实世界复杂性的数据库。\n\nLLaVa中的数据整理始于一张图片及其相应的标题，利用GPT-4生成一组查询。这些查询引导AI精确而相关地探索和描述图像内容。\n\n为了有效地将视觉数据转化为文本基础的AI（如GPT-4），LLaVa使用标题提供视觉场景的多样视角，并使用边界框提供空间上下文和焦点。\n\n1. 对话数据：模仿人类互动，LLaVa整理对话，其中模型作为助手，回答有关图像各个方面的问题。这些问题的范围包括识别物体和动作，辨别它们的数量、位置和相对位置，确保模型能够处理具有明确答案的查询。\n2. 详细描述数据：LLaVa旨在全面理解图像。为此，它促使GPT-4提出旨在理解图像丰富详细描述的问题。这些提示鼓励模型深入挖掘，提供一个捕捉视觉内容整体本质的叙述。\n3. 复杂推理数据：超越单纯描述，LLaVa通过需要分层推理过程的问题挑战模型，要求逻辑和因果关系的理解。这种类型的数据训练模型构建有理有据的响应，支持逻辑思维的顺序。\n\n## LLaVa的架构：视觉与语言的整合\n\nLLaVa模型整合了视觉与语言，利用以下核心组件：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8q_Iay_LHCzPqtrQby_H8w.png)\n\n1. 视觉编码器：LLaVa架构的基础是预训练的CLIP视觉编码器，特别是ViT-L/14变体。该组件通过Transformer层处理输入图像（Xv），提取特征（Zv），使模型能够有效理解视觉信息。\n2. 语言模型（Vicuna）：LLaVa的语言能力依赖于Vicuna，这是一个大型语言模型（LLM）的变体，记作fϕ。Vicuna根据输入语言指令（Xq）理解并生成语言响应（Xa），补充了视觉编码器的功能。\n3. 线性投影：该组件由一个可训练矩阵（W）表示，作为视觉特征（Zv）与语言模型的嵌入空间之间的桥梁。它将视觉特征转换为视觉标记（Hv），使其与语言模型的词嵌入空间对齐，以促进多模态对话。\n\n## 训练和微调 LLaVA：\n\nLLaVA 采用两阶段的训练过程，每个阶段都专注于提升模型解读和响应视觉与文本数据融合的能力。\n\n### Stage 1: 预训练以进行特征对齐\n\nLLaVA训练的初始阶段是预训练以进行特征对齐。在这个阶段，模型专注于将图像中的视觉特征与语言模型中的相应文本特征对齐。这是通过将一个大型数据集过滤为一组精炼的图像-文本对来实现的，LLaVA利用这些对来学习两种模态之间的关联。\n\n在这个阶段，视觉编码器（例如CLIP视觉编码器ViT-L/14）处理图像以提取视觉特征，然后使用投影矩阵（W）将这些特征映射到语言模型的词嵌入空间。LLaVA中使用的语言模型是Vicuna，以其强大的语言理解和生成能力而闻名。\n\n### Stage 2: 微调端到端\n\n在对齐视觉和语言特征后，LLaVA 进行端到端的微调过程。尽管保持视觉编码器的权重不变，但这一阶段允许模型联合微调投影矩阵和语言模型的权重。其目标是最大化基于提供的多模态数据的目标答案的可能性。\n\n这一阶段对于将 LLaVA 适应特定用例场景至关重要，例如多模态聊天、科学问答等。它确保模型不仅能够理解图像在通用描述中的上下文，还能在收到与图像相关的特定问题时参与复杂对话、提供详细解释并进行推理。\n\n## 性能与基准测试：LLaVa 在 VQA 模型中的应用\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*I_5fTa_2rtNHEDUaDNMXbQ.png)\n\n## LLaVA\\-Bench (COCO) 性能洞察\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*6B2K7EcbYgMbH-QEp8J41w.png)\n\nLLaVA\\-Bench (COCO) 提供了一个强大的框架，通过精心设计的90个问题来评估LLaVA的能力，这些问题来源于30张精选图像，涵盖对话、详细描述和复杂推理。结果如下：\n\n* 指令调优效果：在进行指令调优后，LLaVA对用户命令的遵从性提高了超过50分。\n* 问题多样性的影响：尽管详细和复杂推理问题的增加很小，但整体能力提高了7分。这一提升也对对话问题的响应产生了积极影响，展示了多样化训练集的好处。\n* 最优数据组合：三种问题类型的结合带来了最高的性能跃升，LLaVA达到了85.1%的基准分数，强调了全面数据集在提升多模态AI能力方面的优势。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*mCjP0xfpcjHkl-lu)\n\n## LLaVA在LLaVA-Bench（真实场景）上的表现\n\n* 在对话任务中，LLaVA的准确率为57.3%，相比BLIP-2的54.6%有明显提升，远超OpenAI的Flamingo，后者仅为19.3%。\n* 在提供详细描述方面，LLaVA得分为52.5%，展示了其从视觉线索中生成丰富、全面内容的能力。\n* 该模型在复杂推理问题上的表现尤为突出，成功率达到81.7%，表明其先进的推理和推断能力。\n\nLLaVA在所有类别中的综合得分为67.3%，比BLIP-2高出29个百分点，超过Flamingo 48个百分点。\n\n## 限制与关注事项：\n\nLLaVA的定量评估：\n\n将GPT-4作为评估LLaVA性能的评判者，在基准测试AI能力的框架内提出了一个复杂的挑战。一方面，GPT-4的高级理解和生成能力使其能够批判性地评估像LLaVA这样的候选模型所产生的响应质量。这种评估涵盖了有助于衡量模型在多模态数据上遵循指令能力的因素，如帮助性、相关性、准确性和细节。然而，另一方面，使用GPT-4作为评估法官引发了关于基准测试过程公正性的担忧。\n\n关注的核心在于，LLaVA的数据整理过程与GPT-4根本上是交织在一起的。由于GPT-4在训练LLaVA时发挥了重要作用——通过生成模型微调所需的遵循指令的数据——因此存在循环推理的固有风险。本质上，LLaVA可能倾向于生成与GPT-4训练数据中固有的模式或偏见一致的响应。这种倾向可能会扭曲评估，导致一个理论上的上限，反映出与GPT-4方法论的兼容性，而不是对普遍性能的真实衡量。\n\n此外，依赖GPT-4提供其评估的全面解释，引入了一种主观性，这种主观性根植于语言模型对什么构成高质量响应的“理解”。这种理解受到GPT-4训练的数据集的影响，而这些数据集可能未能充分体现现实世界多模态互动的多样性和复杂性。\n\n"},{"lang":"zh","group":"blog","slug":"blog/key-points-llm-quantization-chatgpt-artificial-intelligence-8201ffcb33d4","frontmatter":{"title":"解锁 LLM 量化的 5 个关键点","meta_title":"解锁 LLM 量化的 5 个关键点","description":"量化大型语言模型","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*RUqPEr2NTYXlI1omqF22Qg.png","categories":["Machine Learning","Data Science","Technology/Web"],"author":"Rifx.Online","tags":["quantization","weights","activations","calibration","Quanto"],"draft":false,"slug":"blog/key-points-llm-quantization-chatgpt-artificial-intelligence-8201ffcb33d4"},"content":"\n\n\n### 大型语言模型的量化\n\n\n\nLLM量化目前是一个热门话题，因为它在提高大型语言模型（LLMs）的效率和在各种硬件平台（包括消费级设备）上部署方面发挥着至关重要的作用。\n\n通过调整模型中某些组件的精度，**量化显著减少了模型的内存占用**，同时保持相似的性能水平。\n\n在本指南中，我们将探讨LLM量化的五个关键方面，包括将此技术应用于我们模型的一些实用步骤。\n\n## #1. 理解量化\n\n量化是一种模型压缩技术，通过降低 LLM 中权重和激活的精度来实现。这涉及将高精度值转换为低精度值，实际上是**将存储更多信息的数据类型更改为存储更少信息的数据类型**。\n\n减少每个权重或激活所需的位数显著降低了整体模型大小。因此，**量化创建了使用更少内存和需要更少存储空间的 LLM。**\n\n这一技术在应对 LLM 连续迭代中参数数量的指数增长时变得至关重要。例如，在 OpenAI 的 GPT 系列中，我们可以在以下图表中观察到这一增长趋势：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*QlAhma3Wu1F6w2WvkE8jDA.png)\n\n这一显著增加带来了挑战：随着模型的增长，它们的内存需求往往超过先进硬件加速器（如 GPU）的容量。**这需要分布式训练和推理来管理这些模型，从而限制了它们的可部署性。**\n\n## #2. 量化背后的直觉\n\n尽管量化的定义看起来相当复杂，但这个概念可以通过矩阵直观地解释。\n\n让我们考虑以下一个 3x3 矩阵，表示神经网络的权重。左侧的矩阵显示了原始权重，而右侧的矩阵显示了这些权重的量化版本：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*LPzWe9oxjlDYdSp7dVvRUg.png)\n\n在这个简单的例子中，我们将原始矩阵的元素从四位小数四舍五入到一位小数。尽管矩阵看起来相似，**但四位小数版本所需的存储空间显著更高**。\n\n在实践中，量化不仅仅是一个四舍五入操作。相反，它涉及将数值转换为不同的数据类型，通常是从更高精度转换为更低精度。\n\n例如，大多数模型的默认数据类型是 `float32`，每个参数需要 4 字节（32 位）。因此，对于一个 3x3 矩阵，总内存占用为 36 字节。将数据类型更改为 `int8`，每个参数只需要 1 字节，从而将矩阵的总内存占用减少到仅 9 字节。\n\n## #3. 量化误差\n\n正如我们所看到的，原始矩阵及其量化形式并不完全相等，但非常相似。逐值之间的差异被称为“量化误差”，我们也可以用矩阵形式表示：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*VtGDjVbr7daagLXB57i7Mg.png)\n\n**这种量化误差可以在网络中的每个权重矩阵中累积，从而影响模型的性能。**\n\n当前的量化研究旨在最小化精度差异，同时减少训练或推理模型所需的计算资源，同时保持可接受的性能水平。\n\n## #4. 线性量化\n\n线性量化是 LLMs 中最流行的量化方案之一。简单来说，它涉及将原始权重的浮点值范围映射到固定点值范围。\n\n让我们回顾一下将线性量化应用于我们的模型所需的步骤：\n\n* **获取最小和最大范围：** 我们需要获取待量化的浮点权重的最小值和最大值（`x_min` 和 `x_max`）。我们还需要定义量化范围（`q_min` 和 `q_max`），该范围已经由我们想要转换的数据类型设置。\n* **计算缩放因子（`s`）和零点（`z`）值：** 首先，缩放因子（`s`）将浮点值的范围调整到适合整数范围，保持数据分布和范围。其次，零点（`z`）确保浮点范围内的零被准确地表示为整数，从而保持数值的准确性和稳定性，特别是对于接近零的值。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BepC6-izw0yE19ejsS705Q.png)\n\n* **量化值（`q`）：** 我们需要使用在前一步计算的缩放因子（`s`）和零点（`z`）将原始浮点值映射到整数范围。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BBOQ0VbSGbwf7CN8c4PWKQ.png)\n\n应用这些公式相当简单。如果我们将它们应用于下图左侧的 3x3 权重张量，我们将得到右侧所示的量化矩阵：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*KzBvg84mfI2gAhTIyVibwQ.png)\n\n我们可以看到，`int8` 值的下限对应于原始张量的下限，而上限对应于原始张量的上限，*即，映射为 `0.50 → 255` 和 `-0.40 → 0`。*\n\n我们现在可以使用下面的公式对值进行反量化。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*E5nnqYzncYCRuM5prssuOw.png)\n\n如果我们将反量化后的值再次放入矩阵形式（左侧矩阵），我们可以通过计算原始矩阵与其反量化版本之间逐点差异来计算量化误差（右侧矩阵）：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*56NALu9PAN95QG2hn8HXoQ.png)\n\n正如我们所观察到的，量化误差开始在某些矩阵值中显现。\n\n## #5. 权重量化与激活量化\n\n在上面的例子中，我们主要关注于量化模型的权重。虽然权重量化对于模型优化至关重要，但考虑到激活也可以进行量化同样重要。\n\n**激活量化涉及减少网络中每层的中间输出的精度**。与权重在模型训练后保持不变不同，激活是动态的，并且随着每个输入而变化，使其范围更难预测。\n\n一般而言，激活量化比权重量化更具挑战性，因为它需要仔细校准以确保准确捕捉激活的动态范围。\n\n权重量化和激活量化是互补的技术。两者结合使用可以显著减少模型大小，而不会大幅影响性能。\n\n## 最后的思考\n\n在本文中，我们回顾了关于量化的5个关键点，以更好地理解如何减小这些不断增长的模型的大小。\n\n至于这些技术的实现，Python中有几个支持量化的工具和库，例如`pytorch`和`tensorflow`。然而，在现有模型中无缝集成量化需要对库和模型内部结构有深入的理解。\n\n这就是为什么到目前为止，我最喜欢的简单步骤实现量化的选项是Hugging Face的[Quanto](https://huggingface.co/blog/quanto-introduction)库，旨在简化PyTorch模型的量化过程。\n\n如果你对LLM量化的深入内容以及如何使用上述库感兴趣，你可能还会对文章[“大型语言模型（LLMs）的量化：有效减少AI模型大小”](https://www.datacamp.com/tutorial/quantization-for-large-language-models)感兴趣。\n\n就这些！非常感谢你的阅读！\n\n我希望这篇文章能在**使用LLMs进行编码时**对你有所帮助！\n\n你也可以订阅我的[**时事通讯**](https://readmedium.com/@andvalenzuela/subscribe)，以便及时获取新内容。\n\n**特别是**，**如果你对有关大型语言模型和ChatGPT的文章感兴趣**：\n\n"},{"lang":"zh","group":"blog","slug":"blog/langgraph-vs-langchain-vs-langflow-vs-langsmith-which-one-to-use-why-69ee91e91000","frontmatter":{"title":"LangGraph、LangChain、LangFlow、LangSmith：使用哪一个以及为什么？","meta_title":"LangGraph、LangChain、LangFlow、LangSmith：使用哪一个以及为什么？","description":"了解 LangGraph、LangChain、LangFlow 和 LangSmith 之间的主要区别，并了解哪种框架最适合您的……","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*xrWv1QVt4zE5cxjA8VA3ag.png","categories":["Programming","Technology","Technology/Web"],"author":"Rifx.Online","tags":["LangGraph","LangChain","LangFlow","LangSmith","frameworks"],"draft":false,"slug":"blog/langgraph-vs-langchain-vs-langflow-vs-langsmith-which-one-to-use-why-69ee91e91000"},"content":"\n\n\n### 探索 LangGraph、LangChain、LangFlow 和 LangSmith 之间的关键区别，了解哪种框架最适合您的语言模型应用——从工作流构建到性能监控。\n\n👨🏾‍💻 [GitHub](https://github.com/mdmonsurali) ⭐️ | 👔[LinkedIn](https://www.linkedin.com/in/mdmonsurali/) |📝 [Medium](https://medium.com/@monsuralirana)\n\n\n\n近年来，自然语言处理（NLP）领域见证了可用于构建基于语言模型的应用程序的框架、库和工具数量的激增。在这些工具中，**LangGraph**、**LangChain**、**LangFlow** 和 **LangSmith** 已成为领先的选择，各自满足不同的用例和用户需求。如果您希望构建、监控或扩展语言模型工作流，了解这些工具的优势和目的至关重要。\n\n在本博客中，我们将探讨每个框架，分析它们的优势，并提供何时使用它们的见解。无论您是经验丰富的开发者还是该领域的新手，理解这些工具的细微差别将帮助您为您的项目选择合适的工具。\n\n## 语言模型框架简介\n\n随着强大的语言模型如 GPT-3、GPT-4 以及其他基于变换器的模型的崛起，越来越需要能够简化语言应用程序创建和管理的框架。这些框架简化了复杂的任务，如 **链接多个提示**、**检索相关文档**，甚至 **监控模型性能**。\n\n然而，并非所有框架都是相同的。有些框架提供 **可视化界面** 来管理工作流程，而其他框架则提供高级的 **调试和可观察性** 功能。让我们深入了解这些工具，以理解它们独特的功能。\n\n## 1. LangGraph：可视化复杂工作流\n\n**LangGraph** 是一个为开发者设计的新框架，适合那些偏好 **可视化方法** 来构建语言模型管道的用户。它允许您通过 **基于图的可视化** 来构建复杂的工作流，从而更容易理解不同任务和组件之间的依赖关系。这对于多个步骤（如文本生成、文档检索和分类）串联在一起的大型应用尤其有用。\n\n### 优势：\n\n* **可视化工作流表示**：LangGraph 允许您可视化不同组件之间的数据和操作流。这种图形化的方法直观且有助于设计更复杂的管道。\n* **调试简单**：LangGraph 的可视化特性使得识别工作流中的瓶颈或问题节点变得更加容易。\n\n### 示例用例：\n\n假设您正在构建一个自动化系统，该系统首先使用语言模型检索相关文档，然后将其传递给摘要生成器。在 LangGraph 中，您可以直观地绘制出此工作流程，展示每个步骤之间的关系。如果链中的任何一点出现问题，视觉工具使您能够轻松定位问题所在。\n\n### 何时使用 LangGraph：\n\n如果您正在管理 **复杂的工作流程**，并且重视 **图形界面** 来理解您的管道，LangGraph 是一个绝佳的选择。它特别适合那些更喜欢直观的拖放式工作流程设计的开发人员或数据科学家。\n\n**关键点**：\n\n* 如果您需要清晰的语言处理工作流程的可视化表示。\n* 在创建需要分支或多路径依赖的更复杂的管道时。\n\n## 2. LangChain：LLM 应用的工作马\n\n**LangChain** 是构建由 **大型语言模型 (LLMs)** 驱动的应用程序最受欢迎的框架之一。它提供了一种灵活的 **代码优先方法**，允许开发者将文档检索、摘要和问答等任务串联成统一的工作流程。\n\n### 优势：\n\n* **广泛支持LLMs**：LangChain兼容多种语言模型，使得集成OpenAI的GPT或本地托管模型变得简单。\n* **链式能力**：LangChain擅长于**多个操作的链式处理**——因此得名——使开发者能够创建复杂的NLP应用。\n* **广泛采用**：作为最受欢迎的框架之一，LangChain拥有一个**蓬勃发展的社区**和出色的支持，提供丰富的文档和教程。\n\n### 示例用例：\n\n想象一下，您正在构建一个 **聊天机器人**，它首先理解用户的问题，从数据库中检索相关信息，然后生成响应。使用 LangChain，您可以轻松地以编程方式创建这个多步骤的过程，确保链中的每一步协调工作。\n\n### 何时使用 LangChain：\n\n如果您是一个 **构建生产级应用的开发者**，并且需要一个 **灵活、以代码为中心的解决方案**，LangChain 是您的最佳选择。它非常适合那些希望控制应用架构并且能舒适地编写代码来定义工作流程的开发者。\n\n**关键点**：\n\n* 如果您正在构建需要跨多个语言模型链式任务的生产级应用。\n* 如果您需要一个拥有广泛社区支持和多种集成的库。\n* 当您对编程解决方案更为熟悉，而非可视化工具。\n\n## 3. LangFlow: 无需编码/低代码的 LangChain 扩展\n\n**LangFlow** 本质上是 **LangChain 的可视化扩展**。它将 LangChain 强大的后端与 **直观的拖放界面** 结合在一起。LangFlow 使那些可能不太擅长编写代码的用户仍然能够在他们的应用程序中利用语言模型的强大功能。\n\n### 优势：\n\n* **可视化工作流创建**：与 LangGraph 类似，LangFlow 提供了一个可视化界面用于构建工作流。然而，它是基于 LangChain 构建的，这意味着用户可以利用 LangChain 的强大功能，而无需编写大量代码。\n* **快速原型制作的理想选择**：LangFlow 非常适合快速 **原型化想法** 或构建概念验证应用程序。\n* **适合初学者**：它是一个很好的入门点，适合那些对编码不太熟悉但想要创建语言模型工作流的用户。\n\n### 示例用例：\n\n如果您想快速构建一个**摘要工具**来检索文档，您可以在LangFlow的界面中拖放组件，以创建一个完全功能的应用程序。这可以在几乎不编写代码的情况下完成。\n\n### 何时使用 LangFlow：\n\nLangFlow 非常适合 **非开发人员** 或 **快速原型设计**。如果您想快速实验 **LLM 工作流** 而不深入代码，这个工具可以让您轻松入门。\n\n**关键点**：\n\n* 如果您想快速原型设计 LLM 工作流而不编写代码。\n* 如果您对视觉编程感到舒适，但需要 LangChain 的灵活性。\n* 用于教育目的，帮助用户了解如何构建工作流。\n\n## 4. LangSmith: 监控与可观察性\n\n虽然其他工具专注于 **构建工作流程**，**LangSmith** 的设计目标是 **监控** 和 **调试** 语言模型应用。它提供了先进的可观察性功能，以跟踪您的工作流程和模型的性能，使其在生产环境中不可或缺。\n\n### 优势：\n\n* **深度可观察性**：LangSmith 允许开发者监控语言模型的性能，确保工作流程按预期运行。\n* **错误跟踪**：它在帮助开发者定位问题方面表现出色，使调试变得更加容易。\n* **性能洞察**：LangSmith 提供有关 **工作流程性能** 的洞察，帮助开发者优化他们的应用程序。\n\n### 示例用例：\n\n假设您已经部署了一个**客户服务聊天机器人**，该聊天机器人使用语言模型来回答问题。随着时间的推移，您会发现某些回答的准确性低于预期。LangSmith 可以帮助您追踪问题，通过提供对工作流程中每个决策点的可见性。\n\n### 何时使用 LangSmith：\n\n如果您在 **生产环境** 中部署应用程序，并且需要确保 **健壮性、可靠性和性能**，LangSmith 是一个不可或缺的工具。它在管理 **需要随着时间调试和优化的复杂系统** 时特别有用。\n\n**关键点**：\n\n* 如果您需要 LLM 工作流中的高级监控或调试能力。\n* 对于观察性对确保最佳模型性能至关重要的开发环境。\n* 如果您的重点是基于实时洞察改进和迭代 LLM 驱动的应用程序。\n\n## 哪个更适合你？\n\n* **使用 LangGraph** 如果你更喜欢基于图形的可视化工作流程来构建复杂的 LLM 任务。非常适合需要清晰和结构的用户。\n* **使用 LangChain** 如果你需要一个强大、灵活的解决方案来以编程方式创建语言模型应用。它多功能且非常适合构建生产级应用的开发者。\n* **使用 LangFlow** 如果你想要 LangChain 的强大功能，同时又希望拥有一个可视化的无代码/低代码界面。最适合快速原型开发和更喜欢可视化工具而非编码的用户。\n* **使用 LangSmith** 如果你的重点是 LLM 应用的可观察性和调试。非常适合在开发或生产环境中监控和优化工作流程。\n\n最终，你的选择取决于你对代码的舒适度、工作流程的复杂性，以及你是否优先考虑易用性、灵活性或可观察性。\n\n## 结论\n\n这些工具 — **LangGraph**、**LangChain**、**LangFlow** 和 **LangSmith** — 针对开发和管理语言模型应用的不同阶段。**LangGraph** 提供了一种可视化、直观的方式来构建复杂的工作流程，而 **LangChain** 则为希望创建可扩展应用的开发者提供了一种强大的代码优先解决方案。对于那些更喜欢 **低代码**、拖放方式的用户，**LangFlow** 在不牺牲功能的情况下简化了流程。最后，**LangSmith** 专注于可观察性和调试，确保您的工作流程是优化和可靠的。选择合适的工具取决于您的项目需求，无论是快速原型设计、生产级扩展，还是监控和性能跟踪。\n\n快乐编码！ 🎉\n\n👨🏾‍💻 [GitHub](https://github.com/mdmonsurali) ⭐️ | 👔[LinkedIn](https://www.linkedin.com/in/mdmonsurali/) |📝 [Medium](https://medium.com/@monsuralirana)\n\n感谢您花时间阅读这篇文章！\n\n请务必留下您的反馈和评论。下次博客见，敬请关注 📢\n\n## 参考文献：\n\n1. “LangChain 文档” — <https://python.langchain.com/docs/introduction/>\n2. “LangGraph 概述” — <https://langchain-ai.github.io/langgraph/>\n3. “LangFlow GitHub 仓库” — [https://github.com/LangFlow/LangFlow](https://docs.langflow.org/)\n4. “LangSmith 介绍” — <https://www.langchain.com/langsmith>\n5. “如何使用 LangChain 构建聊天机器人” by JetBrains 博客 — <https://blog.jetbrains.com/pycharm/2024/08/how-to-build-chatbots-with-langchain/>\n\n"},{"lang":"zh","group":"blog","slug":"blog/large-language-models-just-got-a-whole-lot-smaller-f93425ee59a2","frontmatter":{"title":"大型语言模型变得更小了","meta_title":"大型语言模型变得更小了","description":"这可能会改变软件初创企业的游戏规则","date":"2024-11-04T12:29:02.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*1PeFyz_Dlt6jEf27Q9Y33Q.png","categories":["Programming","Technology","Machine Learning"],"author":"Rifx.Online","tags":["compression","optimization","ternary","parallelism","hardware"],"draft":false,"slug":"blog/large-language-models-just-got-a-whole-lot-smaller-f93425ee59a2"},"content":"\n### 这将如何改变软件初创企业的游戏规则\n\n\n\n**本文与 [David Meiborg](https://readmedium.com/undefined) 共同撰写。**\n\n*TLDR: 大型语言模型（简称 LLMs）目前体积庞大，运行成本高，并且具有 [显著的碳足迹](https://arxiv.org/abs/2309.14393)。然而，最近在模型压缩和系统级优化方法上的进展可能会增强 LLM 推理能力。特别是一种使用三元结构参数的方法，有潜力绕过当前标准的昂贵矩阵乘法。这对制造专用芯片的硬件初创企业以及使用或定制构建自己 LLM 的软件初创企业都有令人兴奋的影响。帮助客户部署 LLM 的初创企业可能也会迎来更多的业务。*\n\n如今的大型语言模型非常庞大。真的很大。如果你想加载一个 LlaMa-2–70B 模型，你需要 140 GB 的显存（这就是 70 亿个参数乘以每个参数 2 字节）。作为对比，像 NVIDIA RTX 3090 或 4090 这样的 GPU 只有 24 GB 的显存——这只是所需的一小部分。\n\n有一些关于量化的 [解决方法](https://towardsdatascience.com/run-llama-2-70b-on-your-gpu-with-exllamav2-588141a88598)，但这些往往比较繁琐。你可能仍然需要让你的 GPU 高温运行长达 15 小时，直到模型加载完成。更不用说你仍然需要一些空余内存用于推理，换句话说，就是用于部署模型。\n\n因此，使用当前的 LLMs 成本高昂：通常需要多个高端 GPU 来保存模型，并且还必须考虑推理所产生的能源成本。\n\n这就是为什么很多研究都在致力于应用技术，使 LLMs 更小，从而能够在更小的硬件上以更低的成本运行。在大多数情况下，这是一种艰难的权衡，因为使 LLMs 更小通常会影响它们的质量。找到成本与收益相等的点可能是棘手的。\n\n在本文中，我们概述了一些有前景的优化方法，解释了微软研究人员的最新突破，简要概述了“高效 LLM”领域的创新初创企业，并推导出一些对在 LLM 生态系统中运营的初创企业的一般影响。\n\n## LLM如何变得更加资源高效\n\n像微软、OpenAI、Meta或谷歌这样的科技巨头拥有足够的资源来训练尖端模型，即使目前训练成本对大多数其他公司来说是不可承受的。因此，广泛采用的最大瓶颈不是训练，而是推理效率。换句话说，尽管Meta已经发布了LlaMa，但由于运行模型——而不是创建模型——已经足够具有挑战性，因此它仍未得到足够的采用。\n\n然而，研究人员开始提高这种推理效率。广义而言，有两种方法可以实现这一目标：**系统级优化**并不改变模型本身，而是通过改变模型所处环境的关键方面来提高其性能。**模型优化**则压缩模型，使其更易于部署和运行。\n\n这两种方法都有多种不同的技术。[一篇最近的论文](https://arxiv.org/pdf/2402.01799.pdf)由研究人员出色地总结了这些技术。由于这些技术可能很快就会成为任何从事LLM系统工作者的基本知识，我们在下面对这些技术进行了快速概述。\n\n### 系统级优化\n\n系统级优化指的是改变模型本身的运行方式，而不是模型本身。事实证明，有很多手段可以避免资源闲置或消除其他低效现象。\n\n**分页注意力**\n\n像 GPT 这样的 LLM 的核心是注意力机制。这个机制允许模型在生成每个输出单词时关注输入文本的不同部分。想象一下，您正在阅读一本书，并标记重要的句子以更好地记住故事。类似地，注意力机制在做出预测时“突出”或赋予某些单词或短语更多重要性。\n\n这个机制非常耗费资源。它要求模型考虑输入文本中所有单词对之间的关系。对于长文本，这可能需要大量的内存和计算能力。\n\n分页注意力不是一次处理整个文本，而是将文本分成更小的“页”或段落。模型然后一次处理这些页面或以较小的组处理。这种方法显著减少了任何给定时刻所需的内存量，因为模型不需要同时跟踪整个文本的关系。\n\n这有点像一个学生，如果一次性阅读整整一年的教科书会感到不知所措。通过在整个学年中将其分解为可管理的段落，学生可以记住教科书的内容。\n\n通过每一步所需的内存减少，分页注意力允许在相同的硬件约束下使用更大的模型或更长的文本。\n\n**张量并行**\n\n并行是一种计算中的众所周知的概念。它意味着将一个大型计算任务分成可以由多个处理器或计算机同时处理的小部分。这显著加快了程序运行所需的时间。\n\n在 LLM 的上下文中，[张量](https://towardsdatascience.com/what-is-a-tensor-in-deep-learning-6dedd95d6507) 是多维数字数组。这些张量用于表示模型处理的数据。这类数据包括输入文本、模型权重，即模型学习的参数，以及输出预测。\n\n将这两个概念结合起来，张量并行涉及将这些张量分割到多个 GPU 或其他处理单元上。例如，如果模型的参数（权重）太大而无法适应单个 GPU 的内存，则可以将其分布到多个 GPU 上。每个 GPU 然后一次只处理张量的一部分。\n\n就像一个团队中的多个成员一起工作在一个大型项目上，处理单元在处理各自部分的张量时需要交换信息。例如，一个 GPU 上的计算结果可能需要与另一个 GPU 共享，以继续下一步计算。因此，单元之间的高效通信对于张量并行的有效性至关重要。\n\n简而言之，张量并行是一种将 LLM 所需的计算分解为更小的并行任务的方法，这些任务可以由多个计算单元同时处理，从而加快这些大型复杂模型的训练和推理时间。\n\n**流水线并行**\n\n这种技术专注于改善数据通过模型层的处理工作流。这可以显著加快整体计算速度，并更好地利用可用硬件。\n\n计算中的流水线工作方式类似于工厂的装配线，不同任务的阶段按顺序完成。这允许多个任务在不同阶段同时进行。\n\n在 LLM 中，这些不同的阶段由神经网络的层表示。每一层按顺序处理输入数据，逐渐提取更复杂的特征或模式，直到产生最终输出。可以将每一层视为工厂装配线上的一个工人：每个工人在数据通过时都会在其上添加一些东西，直到最终出现一个复杂的产品。\n\n在流水线并行中，模型的层被分为多个段，每个段分配给不同的 GPU 或处理单元。这样，模型可以按批次输入数据：一旦第一个段处理完第一批数据，第二个段就接手那批数据，而第一个段则接手一批新的数据。\n\n这在模型中创建了数据的连续流动，每个模型段在任何给定时间都在处理不同的数据。这通过保持模型的所有部分处于活动状态来最大化可用硬件资源的使用，并减少单个处理器等待任务完成时可能发生的空闲时间。\n\n前面讨论的流水线并行在模型层级别上操作，将顺序处理阶段分布到设备上。而张量并行则在更细粒度的层面上操作，将层内发生的实际计算（例如，大型矩阵乘法的部分）分布到设备上。\n\n**CPU/GPU 卸载**\n\n在这部分中，我们讨论了很多关于 GPU 的内容。然而，并不是所有在训练或运行 LLM 中的任务都同样适合 GPU。一些任务，如数据预处理或某些控制逻辑，可能更有效地由 CPU 处理。其他任务，特别是处理神经网络（如矩阵乘法）所涉及的重数学计算，确实更有效地在 GPU 上执行。\n\n通过将特定任务卸载到最适合它们的处理器上——将并行化、计算密集型任务分配给 GPU，而将顺序或逻辑密集型任务分配给 CPU——系统可以确保每个工作负载部分以最有效的方式进行处理。\n\n**融合操作**\n\n融合操作将通常单独执行的多个处理步骤合并为一个简化的操作。例如，而不是先执行矩阵乘法再进行加法，融合操作会同时执行两者。\n\n**推测解码**\n\n在生成文本时，LLM 根据之前的单词计算句子中下一个单词的概率。传统上，在生成每个单词后，模型会重新计算以确定下一个单词，并且这个过程会重复，直到完整的句子或段落完成。然而，这种顺序过程可能很慢，尤其是对于较长的文本或更复杂的模型，因为每一步都依赖于前一步的完成。\n\n并行预测：与其等待每个单词被选择后再考虑下一个，推测解码允许模型“推测”或同时对接下来的几个单词做出多个预测。这被称为 *并行预测*。这就像对句子接下来可能采取的几条路径进行有根据的猜测。\n\n通过并行探索这些可能性，模型可以潜在地减少生成文本所需的整体时间。一旦实际的下一个单词被选定，模型可以更快地沿着最可能的路径继续，因为它已经计算了后续的选项。\n\n### LLM模型的压缩\n\n研究人员过去探索过模型压缩。然而，随着大规模LLM的出现，这已成为一个更大的挑战。\n\n许多现有的压缩方法依赖于执行微调步骤以在压缩阶段恢复丢失的性能。然而，当应用于LLM时，由于其庞大的规模，这种方法有显著的局限性。因此，LLM压缩已成为一个全新的研究领域。\n\n**架构剪枝**\n\n当你修剪苹果树时，你会在冬季或早春剪掉某些树枝。这确保树木不会在无效的树枝上浪费资源或因枯木而感染疾病。这有助于它结出更好的果实。\n\n当然，LLM并不结出果实。在这个背景下，剪枝是一种用于减少模型大小的方法，同时尽量保持或最小化对其性能的影响。\n\nLLM模型有数百万甚至数十亿个参数。这些参数并不是所有对模型进行预测或理解语言都同等重要。有些参数很少使用或对模型的决策贡献不大：因此，消除这些冗余或影响较小的连接、神经元或整个层，使模型的使用更高效。\n\n选择剪枝哪些参数并不是一项简单的任务。在基于幅度的剪枝中，移除神经网络中绝对值最小的权重。在训练之前，这些权重通常为零；训练之后，它们通常介于-1和1之间。如果训练对某个权重的影响不大，那么它很可能接近零，因此对模型的决策贡献较少。\n\n一种资源密集但也更稳健的剪枝技术是灵敏度分析。这涉及评估移除每个参数或参数组对模型性能的影响。移除后导致性能下降最小的参数会被剪枝。\n\n还有其他技术，但通常可以将它们分类为非结构化剪枝或结构化剪枝。非结构化剪枝（例如基于幅度的剪枝）移除单个权重，导致稀疏连接的神经网络。结构化剪枝（例如灵敏度分析）移除整个单元或层（例如，整个神经元或通道），这在某些硬件上可以更有效地提高计算效率。\n\n剪枝后，模型通常会经历微调过程。这涉及在训练数据集或其子集上对剪枝后的模型进行再训练。目标是让模型调整和优化其剩余参数，以补偿剪枝所造成的损失。这有助于恢复因剪枝而失去的任何性能。\n\n这可以通过迭代方式或一次性方式进行。在迭代剪枝中，模型在多个轮次中逐步剪枝。在每一轮之后，剪枝后的模型会重新训练，以恢复因剪枝而失去的性能。这个循环可以重复多次，模型可能会变得更加稳健，即使在显著减少参数的情况下也能保持性能。在一次性剪枝中，所有识别出的参数一次性移除，然后对模型进行微调。\n\n**知识蒸馏**\n\n想象一下，有一个足球场上有两个球员：一个非常有经验，知道很多技巧，另一个是初学者。经验丰富的球员知道的比初学者多得多，但初学者可以通过模仿其他球员在场上的行为迅速达到可比的表现。\n\nLLM的知识蒸馏工作原理类似：这是训练一个更小（学生模型）、更高效的模型，以通过学习大模型（教师模型）的输出和处理信息的方式来复制其性能的过程。\n\n要应用这一技术，显然需要一个大型教师模型，例如LlaMa或Mistral的开源大型模型之一。然后需要设计一个参数数量显著少于教师模型的较小神经网络。\n\n学生模型不仅仅在原始硬目标（即真实数据标签）上进行训练，还在软目标上进行训练。这些是教师模型对相同输入生成的概率。例如，对于一组特定的查询，假设教师模型70%的时间回答为“A”，20%的时间回答为“B”，10%的时间回答为“C”、“D”或“E”。学生模型不仅会尝试正确回答每个问题；它还会尝试在一组查询中遵循相同的概率分布。\n\n这样的软目标每个示例携带的信息比硬标签更多，因为它们包含教师模型对所有可能结果的置信水平。这就是学生模型能够以较低的计算开销表现得与教师相似的原因。\n\n在初始知识蒸馏之后，学生模型可能会在特定任务的数据集上进行进一步的微调，以最大化其性能。\n\n**低秩近似**\n\nLLM通过处理和生成基于巨大的矩阵（即非常大的数字表）来工作，这些矩阵表示单词之间的关系、它们的含义以及它们在语言中的使用。这些矩阵可能大到难以处理，特别是在存储和计算方面。\n\n低秩近似涉及找到一个更简单的矩阵，其大小要小得多，但仍能捕捉到原始大矩阵中最重要的信息。这有点像将详细的画作简化为草图。\n\n这通过数学技术来完成，这些技术识别矩阵（或在我们的类比中，画作）中哪些部分包含最多的信息，并将矩阵缩减到仅这些部分。有一些数学技术，尤其是[奇异值分解](https://www.cs.cmu.edu/~venkatg/teaching/CStheory-infoage/book-chapter-4.pdf)，有助于实现这一点。\n\n与剪枝不同，低秩近似执行矩阵维度减少，保持模型的结构，但以更紧凑的形式表示，而剪枝则直接移除神经网络的部分。\n\n**量化**\n\nLLM使用大量数学计算来处理文本。这些计算使用可以具有广泛值范围的数字。通常，这些数字以可以表示非常广泛值范围的格式存储（[浮点格式](https://de.wikipedia.org/wiki/Einfache_Genauigkeit)），在内存中占用32位。\n\n量化减少了这些数字的精度，通常将32位浮点数字转换为更低位宽的表示，例如8位整数。这意味着模型不再使用具有许多小数位的数字，而是使用“更简单”的数字，从而使计算更快并减少内存占用。\n\n量化感知训练（QAT）涉及在训练模型时考虑量化，使其能够适应精度损失，通常导致更好的性能，但代价是更复杂和资源密集的训练过程。\n\n后训练量化（PTQ）在模型完全训练后应用量化，提供了一种更简单和更快速的方法来减少计算需求。然而，由于模型并未特别针对低精度操作进行优化，因此可能无法达到与QAT相同的准确性或性能水平。\n\n### 1位 LLM 时代？\n\n微软研究人员最近发表了一篇[引起轰动的论文](https://arxiv.org/pdf/2402.17764.pdf)，将每个参数的存储位数从当前 LLM 中的 16 位标准，降低到了仅仅 1.58 位。这是个重大新闻：通过这种技术，他们实现了近 10 倍的令牌吞吐量，即处理文本的速度几乎快了 10 倍。他们还将内存占用减少了 3.5 倍，这意味着运行这些模型所需的硬件大大减少。\n\n这是通过使用三元位实现的。与通常使用的介于 -1 和 1 之间的浮点数（通常使用 16 位）不同，每个权重被表示为 -1、0 或 1。这些数字可以存储在 1.58 位，因为对于 3 个可能值的二进制晶体管，可以得到 2¹.58 = 3。仅使用如此简单的数字也意味着不再需要复杂的矩阵乘法，这使得资源使用效率大大提高。\n\n这种技术令人困惑的是，它在 30 亿参数的大小下，能够实现与传统 16 位模型相似的输出性能。目前尚不清楚这种模型在超过 130 亿参数的阈值时，是否能像传统模型一样扩展。明确的是，即使在 700 亿参数下，它在延迟、内存使用和能耗方面比仅有 130 亿参数的传统模型更高效。输出质量仍需详细测试。\n\n另一个缺点是，现有 LLM 的最先进量化技术无法用于生成 1.58 位模型。这类模型需要从头开始创建，尽管成本大幅降低，但目前仍超出普通市民的承受范围。\n\n然而，如果这样的模型被创建并运行良好，推理将变得更加容易。1.58 位 LLM 甚至可能在边缘和移动设备上部署。它们对 CPU 设备（大多数移动设备运行的设备）也更加友好，这使得它们更容易在更便宜的芯片上部署。所有这些都有许多优势，例如隐私方面，但也允许出现人类尚未梦想的新的应用。\n\n此外，像 [Groq](https://groq.com/) 这样的初创公司已经展示了在为 LLM 构建特定硬件 [如 LPU](https://wow.groq.com/why-groq/) 的 promising results 和巨大潜力。LLM 专用硬件已经是一个[巨大市场](https://finance.yahoo.com/news/generative-ai-market-size-expected-163500846.html#:~:text=%2D%20Large%20Language%20Model%20(LLM),the%20forecast%20period%202023%2D2029.)。这样的发现可能使这个市场的增长速度比分析师迄今预见的更为激进。\n\n如果没有其他，推理将由于量化技术和专用硬件的结合而变得极为便宜。这对许多公司，包括初创公司，都会产生影响。\n\n## 较轻量的 LLM 对初创企业意味着什么？\n\n### AI硬件的繁荣刚刚开始\n\n在1971年至1999年间，CPU几乎是市场上[唯一的微处理器](https://cs.stanford.edu/people/eroberts/courses/soco/projects/2005-06/64-bit-processors/history1.html)。随后，[NVIDIA推出](https://readmedium.com/a-brief-history-of-gpu-47d98d6a0f8a)了其GPU。虽然从技术上讲，它并不是世界上第一个GPU，但它是使游戏成为一种可接触和沉浸式体验的首批微处理器之一。（游戏消耗大量计算能力——如果你不知道，现在你知道了！）\n\n从游戏开始，GPU迅速扩展到许多不同的任务，包括科学图像处理、线性代数、3D重建等。GPU特别擅长的一件事是什么？机器学习和LLM。如今，许多NVIDIA的芯片正在用于训练LLM。\n\n从那时起，其他微处理器也开始涌现。[谷歌的TPU](https://cloud.google.com/tpu?hl=en)于2016年推出，特别适合AI训练和推理。虽然GPU被证明非常适合LLM，但TPU是专门为此目的设计的。它们在训练和推理方面都非常合适。\n\n然而，行业正处于[转折点](https://www.wsj.com/tech/ai/how-a-shifting-ai-chip-market-will-shape-nvidias-future-f0c256b1)：不久之后，大多数与LLM相关的工作将是推理，而不再是训练，因为用户开始部署像LlaMa这样的模型。新的创新AI半导体公司现在有机会进入这个领域。\n\n这包括专注于特别快速推理处理器的芯片制造商[Groq](https://wow.groq.com/press/)。其他初创公司包括[赛拉布拉斯](https://www.cerebras.net/)（专注于训练）、[Graphcore](https://www.graphcore.ai/about)（涵盖训练和推理）和[SambaNova](https://sambanova.ai/)（也包括训练和推理）。像英特尔和AMD这样更成熟的竞争对手也在关注训练和推理，尽管预计未来几年的大多数增长将来自后者。大型科技巨头——谷歌、亚马逊或微软——也在开发AI专用芯片，但主要用于内部使用。\n\n总体而言，LLM的硬件市场仍然由数据中心应用主导。边缘和移动应用是下一个合乎逻辑的步骤，但将需要更多突破，例如微软研究人员最近发布的1.58位方法（见上文）。\n\n## LLM软件公司的影响\n\n在新兴AI领域的整个价值链中，我们概述的这些发展可能会导致**运行/使用LLM的成本显著降低**。\n\n以下是我们对这一趋势的几点思考：\n\n* **优秀的B2C产品**，因为LLM成本降低意味着可以构建具有高LLM使用频率和规模（例如，长上下文窗口）的免费增值B2C体验，而不会破坏公司的单位经济。\n* 全球范围内的访问民主化，使得**低收入国家的用户**能够利用先进的AI技术。\n* 公司可以自动化更广泛的任务，从而实现**效率和生产力的提升**（“我不再关心每小时有1万次API调用”）。\n* 新的边缘AI硬件结合更小的模型将导致**新的边缘AI用例**变得可行，这些用例之前仅限于“数据中心”。\n* 随着边缘硬件的快速发展，我们相信有机会建立软件公司，帮助客户将AI模型带入定制边缘设备的碎片化空间（“把你的模型给我，我用各种技术进行压缩，在10种不同的边缘设备上测试，告诉你哪个效果最好，然后帮助你部署”）。\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/leveraging-gemini-1-5-api-for-automated-test-case-generation-reverse-engineering-2ee8789f01db","frontmatter":{"title":"利用 Gemini 1.5 API 进行自动测试用例生成逆向工程","meta_title":"利用 Gemini 1.5 API 进行自动测试用例生成逆向工程","description":"该测试探索使用 Gemini API 和 Google Apps Script 自动创建示例输入，以便更快地进行脚本逆向工程。","date":"2024-11-10T03:51:17.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*fTtML3Sm1TuQNhQP.jpg","categories":["Programming","Programming/Scripting","Technology/WebAPI"],"author":"Rifx.Online","tags":["Gemini","API","automation","reverse-engineering","scripts"],"draft":false,"slug":"blog/leveraging-gemini-1-5-api-for-automated-test-case-generation-reverse-engineering-2ee8789f01db"},"content":"\n\n\n\n\n## 摘要\n\n本报告探讨了利用 Gemini 1.5 API 与 Google Apps Script 结合，自动化脚本逆向工程中的示例输入创建。传统上，这一过程是手动且耗时的，特别是对于具有大量测试用例的函数。通过将逆向工程技术应用于 Google Apps Script 示例，探讨了 Gemini 1.5 API 在自动化输入生成方面简化开发的潜力。\n\n## 介绍\n\n随着 Gemini 1\\.5 API 的发布，用户获得了处理更复杂数据的能力，为各种应用开发打开了大门。本报告探讨了将 Gemini 1\\.5 API 与 Google Apps Script 结合使用的潜力，以实现脚本开发和改进的逆向工程。\n\n传统上，脚本开发涉及手动构建示例输入值。这个过程可能耗时，特别是在创建函数或测试从在线资源（如 Stack Overflow）获取的代码时。每个函数可能需要大量的测试用例，手动生成这些输入可能成为瓶颈。\n\nGemini 1\\.5 API 提供了一个潜在的解决方案，通过自动化示例输入值的创建。这可以显著减少开发时间和精力。本报告通过将逆向工程技术应用于各种使用 Gemini 1\\.5 API 的 Google Apps Script 示例，来调查这一可能性。\n\n在这里，我们将探讨如何使用 Gemini 1\\.5 API 自动化生成示例输入值，以进行编写在 Google Apps Script 中的脚本的逆向工程。\n\n## 使用\n\n为了测试此脚本，请按照以下流程进行。\n\n## 1\\. 创建 API 密钥\n\n请访问 [https://ai.google.dev/gemini\\-api/docs/api\\-key](https://ai.google.dev/gemini-api/docs/api-key) 并创建您的 API 密钥。届时，请在 API 控制台启用生成语言 API。此 API 密钥用于此示例脚本。\n\n该官方文档也可以查看。 [参考](https://ai.google.dev/)。\n\n## 2\\. 创建 Google Apps Script 项目\n\n在本报告中，使用了 Google Apps Script。当然，本报告中介绍的方法也可以用于其他语言。\n\n在这里，为了测试以下示例脚本，请创建一个独立的 Google Apps Script 项目。当然，此脚本也可以与容器绑定脚本一起使用。\n\n请打开 Google Apps Script 项目的脚本编辑器。\n\n## 3\\. 安装 Google Apps Script 库\n\n为了方便访问 Gemini API，我创建了一个 Google Apps Script 库 [GeminiWithFiles](https://github.com/tanaikech/GeminiWithFiles)。在以下示例脚本中，将使用该库。因此，请安装它。您可以在 [这里](https://github.com/tanaikech/GeminiWithFiles?tab=readme-ov-file#1-use-geminiwithfiles-as-a-google-apps-script-library) 查看安装方法。\n\n## 4\\. 示例脚本 1\n\n示例函数选自 [我的代码库](https://github.com/tanaikech/UtlApp)。\n\n* [transpose](https://github.com/tanaikech/UtlApp?tab=readme-ov-file#transpose): 转置二维数组。\n* [removeDuplicatedValues](https://github.com/tanaikech/UtlApp?tab=readme-ov-file#removeduplicatedvalues): 从一维数组中移除重复值。\n* [compilingNumbers](https://github.com/tanaikech/UtlApp?tab=readme-ov-file#compilingnumbers): 使用 Google Apps Script 编译连续数字。\n* [unpivot](https://github.com/tanaikech/UtlApp?tab=readme-ov-file#unpivot): 将二维数组转换为非透视（反透视）。\n* [expandA1Notations](https://github.com/tanaikech/UtlApp?tab=readme-ov-file#expanda1notations): 此方法用于扩展 A1 表示法。\n\n下面提供了演示这些函数的示例脚本。在这个例子中，所有函数可以在一次 API 调用中执行。当我运行这个脚本时，它返回了总共 2,880 个令牌。\n\n示例首先使用 Gemini 创建输入值。为了测试这些值，脚本随后将它们与在 Google Apps Script 中实现的函数一起使用。最后，输入和输出值都被打印出来。\n\n这里使用 JSON 模式生成内容。这确保了 Gemini 稳定生成复杂的 JSON 对象。[参考](https://readmedium.com/taming-the-wild-output-effective-control-of-gemini-api-response-formats-with-response-mime-type-da273c08be85)因此，我选择在这个实例中使用它。\n\n```python\nfunction myFunction() {\n\n  const apiKey = \"###\"; // Please set your API key.\n\n  const functionObj = {\n    transpose: function transpose(array) {\n      /**\n       * ### Description\n       * When the inputted array is 2 dimensional array, true is returned.\n       *\n       * @param {Array} array 2 dimensional array.\n       * @return {Boolean} When the inputted array is 2 dimensional array, true is returned.\n       */\n      function is2DimensionalArray(array) {\n        return array.every((r) => Array.isArray(r));\n      }\n\n      /**\n       * ### Description\n       * Transpose 2 dimensional array.\n       *\n       * @param {Array} array 2 dimensional array.\n       * @param {Boolean} check Check whether the inputted array is 2 dimensional array. Default is true.\n       * @return {Array} Transposed array.\n       */\n      function transpose(array, check = true) {\n        if (check && !is2DimensionalArray(array)) {\n          throw new Error(\"Please use 2 dimensional array.\");\n        }\n        return array[0].map((_, col) => array.map((row) => row[col] || null));\n      }\n      return transpose(array);\n    },\n    removeDuplicatedValues: function removeDuplicatedValues(array) {\n      /**\n       * ### Description\n       * Remove duplicated values from 1 dimensional array.\n       *\n       * @param {Array} array 1 dimensional array.\n       * @return {Object} Object including removeDuplicatedValues, duplicatedValues and numberOfDuplicate.\n       */\n      function removeDuplicatedValues(array) {\n        if (!Array.isArray(array)) {\n          throw new Error(\"Please use 1 dimensional array.\");\n        }\n        const obj = array.reduce(\n          (m, e) => m.set(e, m.has(e) ? m.get(e) + 1 : 1),\n          new Map()\n        );\n        const e = [...obj.entries()];\n        return {\n          removeDuplicatedValues: [...obj.keys()],\n          duplicatedValues: e.reduce((ar, [k, v]) => {\n            if (v != 1) ar.push(k);\n            return ar;\n          }, []),\n          numberOfDuplicate: Object.fromEntries(e),\n        };\n      }\n      return removeDuplicatedValues(array);\n    },\n    compilingNumbers: function compilingNumbers(array) {\n      /**\n       * ### Description\n       * Compiling Continuous Numbers using Google Apps Script.\n       *\n       * @param {Array} array Input array.\n       * @return {Array} Array including object like [{\"start\":1,\"end\":1},{\"start\":3,\"end\":5},{\"start\":7,\"end\":7},{\"start\":9,\"end\":11},{\"start\":13,\"end\":13}].\n       */\n      function compilingNumbers(array) {\n        if (!(Array.isArray(array) && array.every((e) => !isNaN(e)))) {\n          throw new Error(\"Please give an array including numbers.\");\n        }\n        const { values } = [...new Set(array.sort((a, b) => a - b))].reduce(\n          (o, e, i, a) => {\n            if (\n              o.temp.length == 0 ||\n              (o.temp.length > 0 && e == o.temp[o.temp.length - 1] + 1)\n            ) {\n              o.temp.push(e);\n            } else {\n              if (o.temp.length > 0) {\n                o.values.push({\n                  start: o.temp[0],\n                  end: o.temp[o.temp.length - 1],\n                });\n              }\n              o.temp = [e];\n            }\n            if (i == a.length - 1) {\n              o.values.push(\n                o.temp.length > 1\n                  ? { start: o.temp[0], end: o.temp[o.temp.length - 1] }\n                  : { start: e, end: e }\n              );\n            }\n            return o;\n          },\n          { temp: [], values: [] }\n        );\n        return values;\n      }\n      return compilingNumbers(array);\n    },\n    unpivot: function unpivot(values) {\n      /**\n       * ### Description\n       * When the inputted array is 2 dimensional array, true is returned.\n       *\n       * @param {Array} array 2 dimensional array.\n       * @return {Boolean} When the inputted array is 2 dimensional array, true is returned.\n       */\n      function is2DimensionalArray(array) {\n        return array.every((r) => Array.isArray(r));\n      }\n\n      /**\n       * ### Description\n       * Converting 2-dimensional array as unpivot (reverse pivot).\n       *\n       * @param {Array} values 2 dimensional array.\n       * @return {Array} 2 dimensional array converted as unpivot (reverse pivot).\n       */\n      function unpivot(values) {\n        if (!Array.isArray(values) || !is2DimensionalArray(values)) {\n          throw new Error(\"Please give an array of values.\");\n        }\n        const [[, ...h], ...v] = values;\n        return h.flatMap((hh, i) => v.map((t) => [hh, t[0], t[i + 1]]));\n      }\n      return unpivot(values);\n    },\n    expandA1Notations: function expandA1Notations(a1Notations) {\n      /**\n       * ### Description\n       * Converting colum letter to column index. Start of column index is 0.\n       * @param {String} letter Column letter.\n       * @return {Number} Column index.\n       */\n      function columnLetterToIndex(letter = null) {\n        if (letter === null || typeof letter != \"string\") {\n          throw new Error(\"Please give the column letter as a string.\");\n        }\n        letter = letter.toUpperCase();\n        return [...letter].reduce(\n          (c, e, i, a) =>\n            (c += (e.charCodeAt(0) - 64) * Math.pow(26, a.length - i - 1)),\n          -1\n        );\n      }\n\n      /**\n       * ### Description\n       * Converting colum index to column letter. Start of column index is 0.\n       * Ref: https://stackoverflow.com/a/53678158/7108653\n       * @param {Number} index Column index.\n       * @return {String} Column letter.\n       */\n      function columnIndexToLetter(index = null) {\n        if (index === null || isNaN(index)) {\n          throw new Error(\n            \"Please give the column indexr as a number. In this case, 1st number is 0.\"\n          );\n        }\n        return (a = Math.floor(index / 26)) >= 0\n          ? columnIndexToLetter(a - 1) + String.fromCharCode(65 + (index % 26))\n          : \"\";\n      }\n\n      /**\n       * ### Description\n       * This method is used for expanding A1Notations.\n       * @param {Array} a1Notations Array including A1Notations.\n       * @return {Array} Array including the expanded A1Notations.\n       */\n      function expandA1Notations(a1Notations, maxRow = \"10\", maxColumn = \"Z\") {\n        if (!Array.isArray(a1Notations) || a1Notations.length == 0) {\n          throw new Error(\"Please give a1Notations (Array).\");\n        }\n        const reg1 = new RegExp(\"^([A-Z]+)([0-9]+)$\");\n        const reg2 = new RegExp(\"^([A-Z]+)$\");\n        const reg3 = new RegExp(\"^([0-9]+)$\");\n        return a1Notations.map((e) => {\n          const a1 = e.split(\"!\");\n          const r = a1.length > 1 ? a1[1] : a1[0];\n          const [r1, r2] = r.split(\":\");\n          if (!r2) return [r1];\n          let rr;\n          if (reg1.test(r1) && reg1.test(r2)) {\n            rr = [r1.toUpperCase().match(reg1), r2.toUpperCase().match(reg1)];\n          } else if (reg2.test(r1) && reg2.test(r2)) {\n            rr = [\n              [null, r1, 1],\n              [null, r2, maxRow],\n            ];\n          } else if (reg1.test(r1) && reg2.test(r2)) {\n            rr = [r1.toUpperCase().match(reg1), [null, r2, maxRow]];\n          } else if (reg2.test(r1) && reg1.test(r2)) {\n            rr = [[null, r1, maxRow], r2.toUpperCase().match(reg1)];\n          } else if (reg3.test(r1) && reg3.test(r2)) {\n            rr =\n              Number(r1) > Number(r2)\n                ? [\n                    [null, \"A\", r2],\n                    [null, maxColumn, r1],\n                  ]\n                : [\n                    [null, \"A\", r1],\n                    [null, maxColumn, r2],\n                  ];\n          } else if (reg1.test(r1) && reg3.test(r2)) {\n            rr = [r1.toUpperCase().match(reg1), [null, maxColumn, r2]];\n          } else if (reg3.test(r1) && reg1.test(r2)) {\n            let temp = r2.toUpperCase().match(reg1);\n            rr =\n              Number(temp[2]) > Number(r1)\n                ? [\n                    [null, temp[1], r1],\n                    [null, maxColumn, temp[2]],\n                  ]\n                : [temp, [null, maxColumn, r1]];\n          } else {\n            throw new Error(\"Wrong a1Notation: \" + r);\n          }\n          const obj = {\n            startRowIndex: Number(rr[0][2]),\n            endRowIndex:\n              rr.length == 1 ? Number(rr[0][2]) + 1 : Number(rr[1][2]) + 1,\n            startColumnIndex: columnLetterToIndex(rr[0][1]),\n            endColumnIndex:\n              rr.length == 1\n                ? columnLetterToIndex(rr[0][1]) + 1\n                : columnLetterToIndex(rr[1][1]) + 1,\n          };\n          let temp = [];\n          for (let i = obj.startRowIndex; i < obj.endRowIndex; i++) {\n            for (let j = obj.startColumnIndex; j < obj.endColumnIndex; j++) {\n              temp.push(columnIndexToLetter(j) + i);\n            }\n          }\n          return temp;\n        });\n      }\n      return expandA1Notations(a1Notations);\n    },\n  };\n\n  const g = GeminiWithFiles.geminiWithFiles({\n    apiKey,\n    response_mime_type: \"application/json\",\n    doCountToken: true,\n  });\n\n  const functions = Object.entries(functionObj)\n    .map(\n      ([k, v]) =>\n        `<FunctionName>${k}</FunctionName><Function>${v.toString()}</Function>`\n    )\n    .join(\"\");\n  const jsonSchema = {\n    title: \"5 input values for giving each function\",\n    description: `Proposal 5 input values for giving each function. ${functions} Don't propose \"empty\", \"null\", \"undefined\" as values.`,\n    type: \"array\",\n    items: {\n      type: \"object\",\n      properties: {\n        functionName: { description: \"Function name\", type: \"string\" },\n        inputValues: {\n          description: `Proposed 5 input values. Don't propose \"empty\", \"null\", \"undefined\" as values.`,\n          type: \"array\",\n          items: {\n            description: \"Proposed input value\",\n            type: \"array|object|string|number\",\n          },\n        },\n      },\n      additionalProperties: false,\n    },\n  };\n  let res = g.generateContent({ jsonSchema });\n  if (typeof res == \"string\") {\n    try {\n      res = JSON.parse(res);\n    } catch ({ stack }) {\n      console.error(stack);\n      return;\n    }\n  }\n  const result = res.reduce((o, { functionName, inputValues }) => {\n    try {\n      o[functionName] = [];\n      inputValues.forEach((input) => {\n        const output = functionObj[functionName](input);\n        o[functionName].push({ input, output });\n      });\n    } catch ({ stack }) {\n      console.log(stack);\n    }\n    return o;\n  }, {});\n  console.log(JSON.stringify(result));\n}\n```\n运行此脚本后，获得以下结果。可以看到有效的输入和输出值已被创建。\n\n```python\n{\n  \"transpose\": [\n    { \"input\": [[1, 2], [3, 4]], \"output\": [[1, 3], [2, 4]] },\n    { \"input\": [[\"a\", \"b\"], [\"c\", \"d\"]], \"output\": [[\"a\", \"c\"], [\"b\", \"d\"]] },\n    { \"input\": [[\"a1\", \"b1\"], [\"c1\", \"d1\"], [\"e1\", \"f1\"]], \"output\": [[\"a1\", \"c1\", \"e1\"], [\"b1\", \"d1\", \"f1\"]] },\n    { \"input\": [[true, false], [false, true]], \"output\": [[true, null], [null, true]] },\n    { \"input\": [[1, \"a\"], [\"c\", true]], \"output\": [[1, \"c\"], [\"a\", true]] }\n  ],\n\n  \"removeDuplicatedValues\": [\n    { \"input\": [1, 2, 3, 4, 5], \"output\": { \"removeDuplicatedValues\": [1, 2, 3, 4, 5], \"duplicatedValues\": [], \"numberOfDuplicate\": { \"1\": 1, \"2\": 1, \"3\": 1, \"4\": 1, \"5\": 1 } } },\n    { \"input\": [\"a\", \"b\", \"c\", \"d\", \"e\"], \"output\": { \"removeDuplicatedValues\": [\"a\", \"b\", \"c\", \"d\", \"e\"], \"duplicatedValues\": [], \"numberOfDuplicate\": { \"a\": 1, \"b\": 1, \"c\": 1, \"d\": 1, \"e\": 1 } } },\n    { \"input\": [1, 2, 1, 3, 2, 4, 3, 5, 4], \"output\": { \"removeDuplicatedValues\": [1, 2, 3, 4, 5], \"duplicatedValues\": [1, 2, 3, 4], \"numberOfDuplicate\": { \"1\": 2, \"2\": 2, \"3\": 2, \"4\": 2, \"5\": 1 } } },\n    { \"input\": [\"a\", \"b\", \"a\", \"c\", \"b\", \"d\", \"c\", \"e\", \"d\"], \"output\": { \"removeDuplicatedValues\": [\"a\", \"b\", \"c\", \"d\", \"e\"], \"duplicatedValues\": [\"a\", \"b\", \"c\", \"d\"], \"numberOfDuplicate\": { \"a\": 2, \"b\": 2, \"c\": 2, \"d\": 2, \"e\": 1 } } },\n    { \"input\": [1, \"a\", 2, \"b\", 1, \"c\", 2, \"d\", 1, \"e\"], \"output\": { \"removeDuplicatedValues\": [1, \"a\", 2, \"b\", \"c\", \"d\", \"e\"], \"duplicatedValues\": [1, 2], \"numberOfDuplicate\": { \"1\": 3, \"2\": 2, \"a\": 1, \"b\": 1, \"c\": 1, \"d\": 1, \"e\": 1 } } }\n  ],\n\n  \"compilingNumbers\": [\n    { \"input\": [1, 2, 3, 4, 5], \"output\": [{ \"start\": 1, \"end\": 5 }] },\n    { \"input\": [1, 3, 5, 7, 9, 11, 13], \"output\": [{ \"start\": 1, \"end\": 1 }, { \"start\": 3, \"end\": 3 }, { \"start\": 5, \"end\": 5 }, { \"start\": 7, \"end\": 7 }, { \"start\": 9, \"end\": 9 }, { \"start\": 11, \"end\": 11 }, { \"start\": 13, \"end\": 13 }] },\n    { \"input\": [1, 3, 5, 7, 8, 10, 12, 13], \"output\": [{ \"start\": 1, \"end\": 1 }, { \"start\": 3, \"end\": 3 }, { \"start\": 5, \"end\": 5 }, { \"start\": 7, \"end\": 8 }, { \"start\": 10, \"end\": 10 }, { \"start\": 12, \"end\": 13 }] },\n    { \"input\": [1, 2, 4, 5, 7, 8, 10, 11, 13, 14], \"output\": [{ \"start\": 1, \"end\": 2 }, { \"start\": 4, \"end\": 5 }, { \"start\": 7, \"end\": 8 }, { \"start\": 10, \"end\": 11 }, { \"start\": 13, \"end\": 14 }] },\n    { \"input\": [1, 2, 3, 5, 6, 8, 9, 11, 12, 14, 15], \"output\": [{ \"start\": 1, \"end\": 3 }, { \"start\": 5, \"end\": 6 }, { \"start\": 8, \"end\": 9 }, { \"start\": 11, \"end\": 12 }, { \"start\": 14, \"end\": 15 }] }\n  ],\n\n  \"unpivot\": [\n    { \"input\": [[\"name\", \"score1\", \"score2\"], [\"sample1\", 100, 80], [\"sample2\", 90, 70]], \"output\": [[\"score1\", \"sample1\", 100], [\"score1\", \"sample2\", 90], [\"score2\", \"sample1\", 80], [\"score2\", \"sample2\", 70]] },\n    { \"input\": [[\"name\", \"score1\", \"score2\", \"score3\"], [\"sample1\", 100, 80, 70], [\"sample2\", 90, 70, 80]], \"output\": [[\"score1\", \"sample1\", 100], [\"score1\", \"sample2\", 90], [\"score2\", \"sample1\", 80], [\"score2\", \"sample2\", 70], [\"score3\", \"sample1\", 70], [\"score3\", \"sample2\", 80]] },\n    { \"input\": [[\"id\", \"x\", \"y\", \"z\"], [\"a\", 1, 2, 3], [\"b\", 4, 5, 6]], \"output\": [[\"x\", \"a\", 1], [\"x\", \"b\", 4], [\"y\", \"a\", 2], [\"y\", \"b\", 5], [\"z\", \"a\", 3], [\"z\", \"b\", 6]] },\n    { \"input\": [[\"id\", \"x\", \"y\", \"z\", \"xx\", \"yy\", \"zz\"], [\"a\", 1, 2, 3, 10, 20, 30], [\"b\", 4, 5, 6, 40, 50, 60]], \"output\": [[\"x\", \"a\", 1], [\"x\", \"b\", 4], [\"y\", \"a\", 2], [\"y\", \"b\", 5], [\"z\", \"a\", 3], [\"z\", \"b\", 6], [\"xx\", \"a\", 10], [\"xx\", \"b\", 40], [\"yy\", \"a\", 20], [\"yy\", \"b\", 50], [\"zz\", \"a\", 30], [\"zz\", \"b\", 60]] },\n    { \"input\": [[\"Fruit\", \"2021\", \"2022\", \"2023\"], [\"apple\", 100, 120, 150], [\"orange\", 80, 90, 100]], \"output\": [[\"2021\", \"apple\", 100], [\"2021\", \"orange\", 80], [\"2022\", \"apple\", 120], [\"2022\", \"orange\", 90], [\"2023\", \"apple\", 150], [\"2023\", \"orange\", 100]] }\n  ],\n\n  \"expandA1Notations\": [\n    { \"input\": [\"A1:B5\", \"C3:D7\", \"E2:F10\"], \"output\": [[\"A1\", \"B1\", \"A2\", \"B2\", \"A3\", \"B3\", \"A4\", \"B4\", \"A5\", \"B5\"], [\"C3\", \"D3\", \"C4\", \"D4\", \"C5\", \"D5\", \"C6\", \"D6\", \"C7\", \"D7\"], [\"E2\", \"F2\", \"E3\", \"F3\", \"E4\", \"F4\", \"E5\", \"F5\", \"E6\", \"F6\", \"E7\", \"F7\", \"E8\", \"F8\", \"E9\", \"F9\", \"E10\", \"F10\"]] },\n    { \"input\": [\"A:B\", \"C:D\", \"E:F\"], \"output\": [[\"A1\", \"B1\", \"A2\", \"B2\", \"A3\", \"B3\", \"A4\", \"B4\", \"A5\", \"B5\", \"A6\", \"B6\", \"A7\", \"B7\", \"A8\", \"B8\", \"A9\", \"B9\", \"A10\", \"B10\"], [\"C1\", \"D1\", \"C2\", \"D2\", \"C3\", \"D3\", \"C4\", \"D4\", \"C5\", \"D5\", \"C6\", \"D6\", \"C7\", \"D7\", \"C8\", \"D8\", \"C9\", \"D9\", \"C10\", \"D10\"], [\"E1\", \"F1\", \"E2\", \"F2\", \"E3\", \"F3\", \"E4\", \"F4\", \"E5\", \"F5\", \"E6\", \"F6\", \"E7\", \"F7\", \"E8\", \"F8\", \"E9\", \"F9\", \"E10\", \"F10\"]] },\n    { \"input\": [\"A1:C5\"], \"output\": [[\"A1\", \"B1\", \"C1\", \"A2\", \"B2\", \"C2\", \"A3\", \"B3\", \"C3\", \"A4\", \"B4\", \"C4\", \"A5\", \"B5\", \"C5\"]] },\n    { \"input\": [\"A:C\"], \"output\": [[\"A1\", \"B1\", \"C1\", \"A2\", \"B2\", \"C2\", \"A3\", \"B3\", \"C3\", \"A4\", \"B4\", \"C4\", \"A5\", \"B5\", \"C5\", \"A6\", \"B6\", \"C6\", \"A7\", \"B7\", \"C7\", \"A8\", \"B8\", \"C8\", \"A9\", \"B9\", \"C9\", \"A10\", \"B10\", \"C10\"]] },\n    { \"input\": [\"1:5\", \"3:7\", \"2:10\"], \"output\": [[\"A1\", \"B1\", \"C1\", \"D1\", \"E1\", \"F1\", \"G1\", \"H1\", \"I1\", \"J1\", \"K1\", \"L1\", \"M1\", \"N1\", \"O1\", \"P1\", \"Q1\", \"R1\", \"S1\", \"T1\", \"U1\", \"V1\", \"W1\", \"X1\", \"Y1\", \"Z1\", \"A2\", \"B2\", \"C2\", \"D2\", \"E2\", \"F2\", \"G2\", \"H2\", \"I2\", \"J2\", \"K2\", \"L2\", \"M2\", \"N2\", \"O2\", \"P2\", \"Q2\", \"R2\", \"S2\", \"T2\", \"U2\", \"V2\", \"W2\", \"X2\", \"Y2\", \"Z2\", \"A3\", \"B3\", \"C3\", \"D3\", \"E3\", \"F3\", \"G3\", \"H3\", \"I3\", \"J3\", \"K3\", \"L3\", \"M3\", \"N3\", \"O3\", \"P3\", \"Q3\", \"R3\", \"S3\", \"T3\", \"U3\", \"V3\", \"W3\", \"X3\", \"Y3\", \"Z3\", \"A4\", \"B4\", \"C4\", \"D4\", \"E4\", \"F4\", \"G4\", \"H4\", \"I4\", \"J4\", \"K4\", \"L4\", \"M4\", \"N4\", \"O4\", \"P4\", \"Q4\", \"R4\", \"S4\", \"T4\", \"U4\", \"V4\", \"W4\", \"X4\", \"Y4\", \"Z4\", \"A5\", \"B5\", \"C5\", \"D5\", \"E5\", \"F5\", \"G5\", \"H5\", \"I5\", \"J5\", \"K5\", \"L5\", \"M5\", \"N5\", \"O5\", \"P5\", \"Q5\", \"R5\", \"S5\", \"T5\", \"U5\", \"V5\", \"W5\", \"X5\", \"Y5\", \"Z5\"], [\"A3\", \"B3\", \"C3\", \"D3\", \"E3\", \"F3\", \"G3\", \"H3\", \"I3\", \"J3\", \"K3\", \"L3\", \"M3\", \"N3\", \"O3\", \"P3\", \"Q3\", \"R3\", \"S3\", \"T3\", \"U3\", \"V3\", \"W3\", \"X3\", \"Y3\", \"Z3\", \"A4\", \"B4\", \"C4\", \"D4\", \"E4\", \"F4\", \"G4\", \"H4\", \"I4\", \"J4\", \"K4\", \"L4\", \"M4\", \"N4\", \"O4\", \"P4\", \"Q4\", \"R4\", \"S4\", \"T4\", \"U4\", \"V4\", \"W4\", \"X4\", \"Y4\", \"Z4\", \"A5\", \"B5\", \"C5\", \"D5\", \"E5\", \"F5\", \"G5\", \"H5\", \"I5\", \"J5\", \"K5\", \"L5\", \"M5\", \"N5\", \"O5\", \"P5\", \"Q5\", \"R5\", \"S5\", \"T5\", \"U5\", \"V5\", \"W5\", \"X5\", \"Y5\", \"Z5\", \"A6\", \"B6\", \"C6\", \"D6\", \"E6\", \"F6\", \"G6\", \"H6\", \"I6\", \"J6\", \"K6\", \"L6\", \"M6\", \"N6\", \"O6\", \"P6\", \"Q6\", \"R6\", \"S6\", \"T6\", \"U6\", \"V6\", \"W6\", \"X6\", \"Y6\", \"Z6\", \"A7\", \"B7\", \"C7\", \"D7\", \"E7\", \"F7\", \"G7\", \"H7\", \"I7\", \"J7\", \"K7\", \"L7\", \"M7\", \"N7\", \"O7\", \"P7\", \"Q7\", \"R7\", \"S7\", \"T7\", \"U7\", \"V7\", \"W7\", \"X7\", \"Y7\", \"Z7\"], [\"A2\", \"B2\", \"C2\", \"D2\", \"E2\", \"F2\", \"G2\", \"H2\", \"I2\", \"J2\", \"K2\", \"L2\", \"M2\", \"N2\", \"O2\", \"P2\", \"Q2\", \"R2\", \"S2\", \"T2\", \"U2\", \"V2\", \"W2\", \"X2\", \"Y2\", \"Z2\", \"A3\", \"B3\", \"C3\", \"D3\", \"E3\", \"F3\", \"G3\", \"H3\", \"I3\", \"J3\", \"K3\", \"L3\", \"M3\", \"N3\", \"O3\", \"P3\", \"Q3\", \"R3\", \"S3\", \"T3\", \"U3\", \"V3\", \"W3\", \"X3\", \"Y3\", \"Z3\", \"A4\", \"B4\", \"C4\", \"D4\", \"E4\", \"F4\", \"G4\", \"H4\", \"I4\", \"J4\", \"K4\", \"L4\", \"M4\", \"N4\", \"O4\", \"P4\", \"Q4\", \"R4\", \"S4\", \"T4\", \"U4\", \"V4\", \"W4\", \"X4\", \"Y4\", \"Z4\", \"A5\", \"B5\", \"C5\", \"D5\", \"E5\", \"F5\", \"G5\", \"H5\", \"I5\", \"J5\", \"K5\", \"L5\", \"M5\", \"N5\", \"O5\", \"P5\", \"Q5\", \"R5\", \"S5\", \"T5\", \"U5\", \"V5\", \"W5\", \"X5\", \"Y5\", \"Z5\", \"A6\", \"B6\", \"C6\", \"D6\", \"E6\", \"F6\", \"G6\", \"H6\", \"I6\", \"J6\", \"K6\", \"L6\", \"M6\", \"N6\", \"O6\", \"P6\", \"Q6\", \"R6\", \"S6\", \"T6\", \"U6\", \"V6\", \"W6\", \"X6\", \"Y6\", \"Z6\", \"A7\", \"B7\", \"C7\", \"D7\", \"E7\", \"F7\", \"G7\", \"H7\", \"I7\", \"J7\", \"K7\", \"L7\", \"M7\", \"N7\", \"O7\", \"P7\", \"Q7\", \"R7\", \"S7\", \"T7\", \"U7\", \"V7\", \"W7\", \"X7\", \"Y7\", \"Z7\", \"A8\", \"B8\", \"C8\", \"D8\", \"E8\", \"F8\", \"G8\", \"H8\", \"I8\", \"J8\", \"K8\", \"L8\", \"M8\", \"N8\", \"O8\", \"P8\", \"Q8\", \"R8\", \"S8\", \"T8\", \"U8\", \"V8\", \"W8\", \"X8\", \"Y8\", \"Z8\", \"A9\", \"B9\", \"C9\", \"D9\", \"E9\", \"F9\", \"G9\", \"H9\", \"I9\", \"J9\", \"K9\", \"L9\", \"M9\", \"N9\", \"O9\", \"P9\", \"Q9\", \"R9\", \"S9\", \"T9\", \"U9\", \"V9\", \"W9\", \"X9\", \"Y9\", \"Z9\", \"A10\", \"B10\", \"C10\", \"D10\", \"E10\", \"F10\", \"G10\", \"H10\", \"I10\", \"J10\", \"K10\", \"L10\", \"M10\", \"N10\", \"O10\", \"P10\", \"Q10\", \"R10\", \"S10\", \"T10\", \"U10\", \"V10\", \"W10\", \"X10\", \"Y10\", \"Z10\"]] }\n  ]\n}\n```\n\n## 5\\. 示例脚本 2\n\n上述示例脚本的每个函数仅使用一个参数。当使用多个参数时，脚本如下。示例函数如下。\n\n* [splitArray](https://github.com/tanaikech/UtlApp?tab=readme-ov-file#splitarray): 每 n 长度拆分数组。\n\n\n```python\nfunction myFunction() {\n\n  const apiKey = \"###\"; // Please set your API key.\n\n  const functionObj = {\n    splitArray: function splitArray(array, size) {\n      /**\n       * ### Description\n       * Split array every n length.\n       *\n       * @param {Array} array 2 dimensional array.\n       * @param {Boolean} check Check whether the inputted array is 2 dimensional array. Default is true.\n       * @return {Array} Transposed array.\n       */\n      function splitArray(array, size) {\n        if (!array || !size || !Array.isArray(array)) {\n          throw new Error(\"Please give an array and split size.\");\n        }\n        return [...Array(Math.ceil(array.length / size))].map((_) =>\n          array.splice(0, size)\n        );\n      }\n      return splitArray(array, size);\n    },\n  };\n\n  const g = GeminiWithFiles.geminiWithFiles({\n    apiKey,\n    response_mime_type: \"application/json\",\n    doCountToken: true,\n  });\n\n  const functions = Object.entries(functionObj)\n    .map(\n      ([k, v]) =>\n        `<FunctionName>${k}</FunctionName><Function>${v.toString()}</Function>`\n    )\n    .join(\"\");\n  const jsonSchema = {\n    title: \"5 input values for giving each function\",\n    description: `Proposal 5 input values for giving each function. ${functions} Don't propose \"empty\", \"null\", \"undefined\" as values.`,\n    type: \"array\",\n    items: {\n      type: \"object\",\n      properties: {\n        functionName: { description: \"Function name\", type: \"string\" },\n        inputValues: {\n          description: `Proposed 5 input values. Don't propose \"empty\", \"null\", \"undefined\" as values.`,\n          type: \"array\",\n          items: {\n            description: \"Proposed input value\",\n            type: \"array|object|string|number\",\n          },\n        },\n      },\n      additionalProperties: false,\n    },\n  };\n  let res = g.generateContent({ jsonSchema });\n  if (typeof res == \"string\") {\n    try {\n      res = JSON.parse(res);\n    } catch ({ stack }) {\n      console.error(stack);\n      return;\n    }\n  }\n  const result = res.reduce((o, { functionName, inputValues }) => {\n    try {\n      o[functionName] = [];\n      inputValues.forEach((input) => {\n        const temp = JSON.parse(JSON.stringify(input));\n        const output = functionObj[functionName](...temp);\n        o[functionName].push({ input, output });\n      });\n    } catch ({ stack }) {\n      console.log(stack);\n    }\n    return o;\n  }, {});\n  console.log(JSON.stringify(result));\n}\n```\n运行此脚本时，将获得以下结果。\n\n\n```python\n{\n  \"splitArray\": [\n    { \"input\": [[1, 2, 3, 4, 5, 6], 2], \"output\": [[1, 2], [3, 4], [5, 6]] },\n    { \"input\": [[\"a\", \"b\", \"c\", \"d\", \"e\"], 2], \"output\": [[\"a\", \"b\"], [\"c\", \"d\"], [\"e\"]] },\n    { \"input\": [[\"apple\", \"orange\", \"grape\", \"banana\", \"kiwi\"], 3], \"output\": [[\"apple\", \"orange\", \"grape\"], [\"banana\", \"kiwi\"]] },\n    { \"input\": [[true, false, true, false, true], 1], \"output\": [[true], [false], [true], [false], [true]] },\n    { \"input\": [[1.2, 3.14, 2.71, 0.577], 2], \"output\": [[1.2, 3.14], [2.71, 0.577]] }\n  ]\n}\n```\n\n## 摘要\n\n从上述结果来看，我们可以确认使用 Gemini API 进行逆向工程的可能性。这也表明 Gemini API 可以用于开发应用程序。\n\n## 注意\n\n* 如果发生错误，请再次运行脚本。或者，请调整 JSON 架构中的描述。\n* 我相信这种方法也可以用于除 Google Apps Script 之外的其他语言。\n* 在当前阶段，似乎依赖于 Google Apps Script 的类对象，如 SpreadsheetApp、DriveApp 等，无法用作输入值。\n* 顶部的抽象图像是由 [Gemini](https://gemini.google.com/app) 创建的。\n\n"},{"lang":"zh","group":"blog","slug":"blog/lightrag-simple-and-efficient-rival-to-graphrag-fe49e12e9ece","frontmatter":{"title":"LightRAG - GraphRAG 简单高效的竞争对手？","meta_title":"LightRAG - GraphRAG 简单高效的竞争对手？","description":"传统的 RAG 系统通过索引原始数据来工作。这些数据被简单地分块并存储在向量数据库中。每当有查询从...","date":"2024-11-13T01:22:29.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*7_2PyaNMVdYDWTCrb_cMCg.png","categories":["Generative AI","Data Science","Technology/Web"],"author":"Rifx.Online","tags":["LightRAG","retrieval","GraphRAG","indexing","dual-level"],"draft":false,"slug":"blog/lightrag-simple-and-efficient-rival-to-graphrag-fe49e12e9ece"},"content":"\n\n\n\n\n传统的 RAG 系统通过索引原始数据来工作。这些数据被简单地切分并存储在向量数据库中。每当用户发出查询时，它会查询存储的片段并 *检索* 相关片段。如果您希望了解 RAG 的基本原理，我已经在 [这里](https://proxy.rifx.online/https://readmedium.com/retrieval-augmented-generation-rag-a-quick-and-comprehensive-introduction-6cd5217a4ebb) 写了一篇全面的介绍。\n\n由于检索步骤针对用户的每一个查询都会发生，因此这是加速简单 RAG 系统的最关键瓶颈。让检索过程变得超级高效难道不是合乎逻辑的吗？这就是 **LightRAG** 的承诺。\n\n\n> **如果您不是会员，您可以在 [这里](https://proxy.rifx.online/https://www.ai-bites.net/lightrag-simple-and-efficient-rival-to-graphrag/) 免费阅读此内容。为什么不在那里订阅并将这些内容直接发送到您的收件箱呢？**\n\n## 为什么不使用 GraphRAG\n\n在我们查看它们之前，你可能会问：“等一下。我们不是有微软的 GraphRAG 吗？”是的，但 GraphRAG 似乎有几个缺点。\n\n* **增量知识更新。** (sec 3\\.1\\) GraphRAG 首先在整个私有数据集中创建对实体和关系的引用。然后，它通过自下而上的聚类将数据层次化组织成语义集群。对数据集进行新知识的更新意味着我们必须重新经历构建图的整个过程！而 LightRAG 则通过简单地将新知识附加到现有知识上来解决这个问题。更具体地说，它通过简单的并集操作将新的图节点和边与现有的结合在一起。\n* **计算强度。** 从他们的研究中可以看出，LightRAG 显著降低了检索阶段的成本。GraphRAG 需要 610,000 个标记，而 LightRAG 则少于 100 个标记。\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*0TwUDr1BCNr_nSfTPwxenw.png)\n\n所以不再赘述，让我们深入了解 LightRAG。\n\n## LightRAG\n\nLightRAG的两个主要卖点是基于图的索引和双层检索框架。让我们逐一了解它们。\n\n## 基于图的索引\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*U7sYYNA9teKEVig1dzfi2g.png)\n\n以下是LightRAG遵循的步骤，以实现基于图的索引。\n\n* **实体和关系（ER）提取。** ER提取在上图中用R(.)表示。此步骤确保首先从给定文档中提取简单实体。例如，在上述示例中，“蜜蜂”和“养蜂人”是两个实体。它们通过“观察”关系相关联。即，养蜂人观察蜜蜂。\n* **使用LLM生成键值（KV）对。** 然后使用简单的LLM生成KV对。LLM分析步骤提供了关于实体或关系的简要说明或解释。例如，LLM解释了在我们选择的示例中“养蜂人”是谁。此步骤在上图中用P(.)表示。请注意，这个LLM与主RAG管道中使用的通用LLM不同。\n* **去重。** 鉴于这些文档与蜜蜂有关，实体“养蜂人”可能是从多个文档或片段中检索到的。因此，我们需要一个去重步骤，仅保留一个并丢弃其余具有相同含义的内容。这在上图中用D(.)表示。\n\n## 双层检索\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*t9W1UBbjFa5cnAe-_tqz-Q.png)\n\n对RAG系统的查询可以分为两种类型——具体查询或抽象查询。在同一个蜜蜂的例子中，具体查询可以是“蜂巢中可以有多少只蜂后？”抽象查询可以是“气候变化对蜜蜂的影响是什么？”为了应对这种多样性，LightRAG采用了两种检索类型：\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*DuVxwxwl_2-gej_DwGzoeg.png)\n\n* **低层检索。** 它简单地提取精确的实体及其关系，如蜜蜂、观察和养蜂人。\n* **高层检索。** 通过使用LLM，LightRAG汇总信息并总结多个信息来源。\n\n## 为什么要做这些？\n\n进行所有这些练习并切换到 LightRAG 确实提高了执行时间。在索引过程中，LLM 每个块只需调用一次以提取实体及其关系。\n\n同样，在用户查询期间，我们只需使用用于索引的相同 LLM 从块中检索实体和关系。这在检索开销和计算上节省了大量成本。因此，我们终于有了一个“轻量级”的 RAG！\n\n将新知识整合到现有图中似乎是一个无缝的过程。每当我们有新信息时，不必重新索引整个数据，我们可以简单地将新知识附加到现有图中。\n\n## 评估\n\n在他们的评估中，他们与 Naive RAG、RQ\\-RAG、HyDE 和 GraphRAG 进行了比较。为了保持比较的公平性，他们在所有数据集上使用了固定的 1200 的块大小并且使用了 GPT\\-4o\\-mini 作为 LLM。答案的评估标准包括全面性、多样性和在回答用户问题（即论文中的 *赋能*）方面的有效性。\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*DNdNHW7NRcOXpvEWjT5BKQ.png)\n\n从下划线的结果中可以看出，LightRAG 超越了当前所有的最先进方法。\n\n总体而言，他们得出了以下结论：\n\n* 使用基于图的方法（GraphRAG 或 LightRAG）显著改善了基线 Naive RAG\n* LightRAG 通过双层检索范式产生了相当多样的答案\n* LightRAG 能更好地处理复杂查询\n\n## 结论\n\n尽管 RAG 是一种相对较新的技术，但我们在这一领域看到了快速进展。像 LightRAG 这样的技术能够将 RAG 流水线运行在廉价的商品硬件上，受到了广泛欢迎。随着硬件环境的不断发展，实时在计算受限的硬件上运行 LLM 和 RAG 流水线的需求也在不断增加。\n\n您想看看关于 LightRAG 的一些实践研究吗？请继续关注……\n\n## 向大家致敬\n\n希望这对你有帮助。\n\n**如果你喜欢这篇文章，为什么不在 [Twitter](https://proxy.rifx.online/https://twitter.com/ai_bites) 上关注我呢？我每天都会分享顶级AI实验室的研究更新。**\n\n**同时，请订阅我的 [YouTube 频道](https://proxy.rifx.online/https://www.youtube.com/c/aibites)，我会以视觉方式解释AI概念和论文。**\n\n**最后，请给我点赞，让我们一起庆祝你阅读完这个故事。**\n\n"},{"lang":"zh","group":"blog","slug":"blog/llama-3-1-405b-how-to-use-for-free-9aaf3561932d","frontmatter":{"title":"Llama 3.1 405B——如何免费使用","meta_title":"Llama 3.1 405B——如何免费使用","description":"无需本地安装","date":"2024-10-29T05:09:24.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*db_ND6LyQ5_p5jFJCTo5GQ.jpeg","categories":["Programming","Technology","Generative AI"],"author":"Rifx.Online","tags":["Llama","Meta","HuggingChat","Groq","API"],"draft":false,"slug":"blog/llama-3-1-405b-how-to-use-for-free-9aaf3561932d"},"content":"\n\n\n### 无需本地安装\n\n**Llama 3\\.1 405B** 是Meta于2024年7月发布的最先进的AI模型——**但你可以在哪里试用它*？***\n\n\n\n**LLama 3\\.1** 有不同的版本，包括参数最多的4050亿模型以及较小的70B和8B模型。\n\n试用70B和8B模型的最简单方法是在[Groq](https://console.groq.com/playground)上——你可以直接在他们的游乐场中试用它们。\n\n由于需求量巨大，最强大的405B模型通常不可用。\n\n本指南适用于任何想要免费试用Llama 3\\.1 405B的用户，包括开发者——无需下载和安装。\n\n如果你没有付费的Medium账户，可以在[这里](https://addison-best.medium.com/9aaf3561932d?source=friends_link&sk=5fa532d1caaec229a0b9a445d8749449)免费阅读。\n\n如果你是开发者，并且想要通过API免费试用**LLama 3\\.1 405B——**你可以跳到文章的末尾。\n\n## 我在哪里可以免费使用 Llama 3\\.1 405B？\n\n您可以直接从 [Meta](https://llama.meta.com/) 下载并安装它——但它非常庞大，您需要数百个千兆字节的空间和一台强大的计算机才能正确尝试。\n\n但您现在也可以在不下载的情况下进行尝试。\n\n以下是一些您可以尝试的选项：\n\n**如果您想了解更多 AI 小技巧，以帮助您的业务增长并在线赚取更多收入：**\n\n***👉*** *注册我们的 **[免费 5 天电子邮件课程](https://aigrowthguys.com/5-day-free-course-how-to-grow-your-business-like-a-weed)**，实现增长 🚀 并赚取**💲👈***\n\n## 1\\. 在Meta AI上使用Llama 3\\.1 405B\n\n如果你在美国，至少在加拿大（我所在的地方），你可以通过Meta AI与Llama 3\\.1 405B模型聊天。访问[Meta AI网站](https://www.meta.ai)，并使用你的Facebook或Instagram账户登录。\n\n它现在也可能在其他国家可用，所以可以看看。\n\n当你登录时——希望你能看到尝试**Llama 3\\.1 405B**的选项。\n\n如果可以，你会看到如下截图中的消息。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*cw1WMKhdZhzUp0L3Kn7Qng.png)\n\n你也可以通过链接你的Meta账户来通过WhatsApp访问它。[**在Meta AI上尝试**](https://www.meta.ai)\n\n你还可以尝试他们的**Imagine**照片创作工具和AI图像编辑器**。**\n\n文章开头的那幅带有拉马和电脑的卡通图像就是用这个工具创建的。\n\n**我提示了**\n\n> **Imagine: 我想要一幅有趣的卡通图像，用于中等文章，展示尝试使用Llama 3\\.1 405B**\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8MeC_M2O7UX7ulPOfUCuHA.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*dIG62eA7YAT3mpLA0etz9Q.png)\n\n值得一试。我认为它无法与Flux.1或Midjourney相提并论——但它易于使用且免费。\n\n## 2\\. 在 HuggingChat 上使用 Llama 3\\.1 405B\n\nHuggingChat 对美国以外的用户开放，并提供对 Llama 3\\.1 405B 模型的访问。您可以立即开始聊天，无需注册，这使得探索模型的能力变得简单。访问 [HuggingChat 页面](https://huggingface.co) 开始。[在 HuggingChat 上试用](https://huggingface.co)\n\n## 3\\. 在 Groq 上使用 Llama 3\\.1 405B\n\n**如何：** Groq 最初托管了 Llama 3\\.1 405B 模型，但现在由于需求量大，提供了更小的 70B 和 8B 版本。您可以通过在 [Groq 的网站](https://groq.com) 上创建一个免费账户来探索这些模型。[在 Groq 上试用](https://groq.com)\n\n## 4\\. 在 Perplexity 上使用 Llama 3\\.1 405B\n\nPerplexity 提供了一种简单的方式与 Llama 3\\.1 进行交互，旨在快速便捷地访问该模型。您可以通过访问 Perplexity AI 平台开始使用它。但这仅在 Pro 计划中可用。[在 Perplexity 上尝试](https://www.perplexity.ai)\n\n## 5\\. 在 Poe 上使用 Llama 3\\.1 405B\n\nPoe 是 Quora 提供的另一个平台，您可以在这里尝试 Llama 3\\.1。Poe 允许用户通过聊天界面探索不同的 AI 模型，包括 Llama 3\\.1。如果您想在一个地方比较 Llama 3\\.1 和其他 AI 模型，这是一个多功能的选择。您可以免费尝试 3\\.1 405B — 每天有有限的免费积分。[在 Poe 上尝试](https://poe.com)\n\n## 我在哪里可以免费使用 Llama 3\\.1 405B API？\n\n如果你是开发者并想完全免费尝试 Llama 3\\.1 405B 版本——目前你的选择有限。\n\n但我想给你一个简单且免费的选项，让你可以开始使用。\n\n你现在可以在 [together.ai](https://together.ai) 上免费尝试。\n\n你可以获得 $5 的免费额度和一个 API 密钥来进行尝试。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*w8LOXw-Wm0QTz5YgvZ27ug.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*YpKURkmy--xstoJpZ4fmbw.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*g0FxHkg6gq5OMXXo1Yzr0A.png)\n\n这是我找到的快速且免费的测试 Llama 3\\.1 405B 版本的最简单方法。\n\n这是一个很好的选项，适合希望免费尝试使用 API 的开发者。\n\n## 注意：\n\n如果您希望我们的团队使用LLMs创建定制的AI软件，或为您的业务创建定制的AI聊天机器人，您可以在这里[**联系我**](https://aigrowthguys.com/contact/) ✉️，我会尽快回复您：\n\n[**AI Growth Guys 联系**](https://aigrowthguys.com/contact/)✉️\n\n👉 注册我们的[**免费5天电子邮件课程**](https://aigrowthguys.com/5-day-free-course-how-to-grow-your-business-like-a-weed/)，在AI时代蓬勃发展🚀并赚取💲\n\n您还可以[**注册我的新闻通讯**](https://ai-growth-guys.beehiiv.com/subscribe/?via=andrew-best)，了解如何利用AI赚取更多收入。\n\n查看我们的[**YouTube频道**](https://www.youtube.com/@aigrowthguys)\n\n在我们的网站上关注我们：[**AI Growth Guys**](https://aigrowthguys.com/)\n\n"},{"lang":"zh","group":"blog","slug":"blog/llama-3-2-the-next-generation-of-lightweight-instruction-tuned-language-models-a-hands-on-9bca07c8af1d","frontmatter":{"title":"Llama 3.2：下一代轻量级、指令调整语言模型：实践……","meta_title":"Llama 3.2：下一代轻量级、指令调整语言模型：实践……","description":"探索 LLaMA 3.2 在修剪、知识提炼和多语言性能方面的关键创新，以及运行的实践教程……","date":"2024-11-10T03:51:17.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BMalqlcJIFe50hidF4FnqQ.png","categories":["Natural Language Processing","Machine Learning","Generative AI"],"author":"Rifx.Online","tags":["LLaMA","tuning","pruning","distillation","multilingual"],"draft":false,"slug":"blog/llama-3-2-the-next-generation-of-lightweight-instruction-tuned-language-models-a-hands-on-9bca07c8af1d"},"content":"\n### 探索 LLaMA 3\\.2 在剪枝、知识蒸馏和多语言性能方面的关键创新，以及本地运行或通过 Google Colab 的实用教程\n\n👨🏾‍💻 [GitHub](https://github.com/mdmonsurali) ⭐️ \\| 👔[LinkedIn](https://www.linkedin.com/in/mdmonsurali/) \\|📝 [Medium](https://medium.com/@monsuralirana)\n\n\n\n## 介绍\n\n语言模型持续发展，推动着效率、速度和多语言能力的边界。LLaMA 3\\.2（轻量级LLaMA）代表了这一轨迹上的下一个突破，结合了剪枝、知识蒸馏和合成数据生成等创新。在Meta之前的创新基础上，LLaMA 3\\.2在不牺牲速度、准确性或隐私的情况下，提高了较小模型（1B和3B参数）的性能。在这篇博客中，我们将探讨LLaMA 3\\.2的关键技术进展，讨论其基准测试结果，并提供基于研究的视角，说明这些创新的重要性。最后，我们将通过一个实践教程，帮助您开始使用LangChain和Ollama部署LLaMA 3\\.2。\n\n## 1\\. LLaMA模型的演变：从1\\.0到3\\.2\n\n### LLaMA 模型的简史\n\n**大型语言模型 Meta AI (LLaMA)** 系列自首次发布以来经历了显著的发展。Meta 的 **LLaMA 1\\.0** 旨在使 LLM 的获取更加民主化，提供了比 GPT\\-3 等模型更少参数的高性能模型，同时在各种任务中实现了类似的准确性。LLaMA 2\\.0 引入了指令调优和多语言性能的改进。\n\n**LLaMA 3\\.2** 代表了下一个飞跃，重点关注以下核心领域：\n\n* **指令调优和微调**：指令跟随能力的增强使模型在下游任务中的表现更佳。\n* **边缘设备的效率**：修剪和蒸馏技术使模型能够在计算资源有限的设备上部署，例如智能手机，而不损失性能。\n* **视觉和语言理解**：将视觉-语言模型集成到 LLaMA 3\\.2 中，能够处理多模态任务，例如基于图像的问答。\n\n## 2\\. LLaMA 3\\.2 的关键创新\n\n### A. 指令调优与对齐\n\n指令调优已被证明是提高大型语言模型（LLMs）遵循自然语言指令能力的关键因素。在 LLaMA 3.2 中，Meta 使用了 **监督微调（SFT）**、**拒绝采样（RS）** 和 **直接偏好优化（DPO）** 技术。这些技术被迭代应用于训练模型，以更高的准确性处理各种任务，如推理、摘要和工具使用。\n\n* **监督微调（SFT）**：模型在人工标注的数据集上进行微调，从中学习生成更受欢迎的输出。\n* **直接偏好优化（DPO）**：一种训练模型直接优化用户偏好的技术，使输出与人类期望更紧密对齐。\n\n### B. 高效剪枝与知识蒸馏\n\nLLaMA 3\\.2 的轻量级模型，如 1B 和 3B 参数模型，利用 **结构化剪枝** 和 **知识蒸馏**。这些技术在减小模型体积的同时，保留了来自更大模型（例如 LLaMA 3\\.1 8B 和 70B）的大量知识：\n\n* **结构化剪枝**：在这种方法中，系统性地移除网络中重要性较低的部分，以创建更小的模型，同时保持准确性。\n* **知识蒸馏**：一个大型模型（教师）将知识转移到一个较小的模型（学生），使得较小的模型在训练期间能够模仿较大模型的性能。\n\n### C. 扩展上下文长度\n\nLLaMA 3\\.2 的一个主要更新是其处理更长上下文长度的能力——最多可达 **128K tokens**。这使其在处理需要处理大量文本的任务时非常高效，例如摘要、长文档分析和多轮对话。\n\n### D. 视觉-语言模型\n\nMeta 在 LLaMA 3.2 中引入的 **视觉-语言模型 (VLMs)** 开辟了多模态任务的新领域。这些模型旨在处理文本和图像，使其在文档问答、科学图表解释和图像描述等应用中非常有效。\n\n## 3\\. 基准性能：LLaMA 3\\.2 如何比较？\n\nLLaMA 3\\.2 在广泛的基准测试中进行了严格评估，如您所分享的表格所示。主要亮点包括：\n\n* **一般任务**：3B 模型在 **MMLU** (63\\.4) 和 **IFEval** (77\\.4) 等基准测试中表现出色，显示出卓越的指令遵循和推理能力。\n* **工具使用**：在 **BFCL V2** 等任务中，LLaMA 3\\.2 (3B) 得分 67\\.0，超越了 **Gemma 2** 和 **Phi\\-3\\.5\\-mini** 等竞争对手，在遵循与工具使用相关的复杂指令方面表现更佳。\n* **数学和推理**：3B 模型在与数学相关的任务中表现强劲，在 **GSM8K** (小学数学) 中得分 **77\\.7**，在 **ARC Challenge** 中得分 **78\\.6**，该基准专注于推理。\n* **多语言生成**：3B 模型在多语言 MGSM 基准中也表现优异，展示了其在多种语言中生成连贯文本的能力。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*lpjDJ6AaRnljLwAxtAf-Ag.png)\n\nLLaMA 3\\.2 在这些任务中的优势表明，它为涉及自然语言理解、指令遵循和推理的任务提供了强有力的解决方案，适用于一般和多语言环境。\n\n## 4\\. 实践教程：使用 LangChain 和 Ollama 在本地运行 LLaMA 3\\.2\n\n现在我们已经探讨了 LLaMA 3\\.2 的技术进展，让我们通过逐步指南在本地使用 **LangChain** 和 **Ollama** 进行实践。我们可以在本地机器或 Google Colab 终端上安装它。只需按照以下步骤操作：\n\n### 步骤 1：安装所需的库\n\n首先，在您的 Python 环境中安装所需的库。运行以下命令以设置 LangChain 和 Ollama：\n\n```python\n!pip install langchain\n!pip install -U langchain-community\n!pip install langchain_ollama\n```\n\n### 第2步：安装并加载 Colab\\-XTerm\n\nColab\\-XTerm 是一个方便的包，可以在 Colab 笔记本中启用终端访问。这对于直接在笔记本环境中运行 shell 命令非常有用。要安装它，请运行以下命令：\n\n```python\n!pip install colab-xterm\n%load_ext colabxterm\n```\n\n### 第 3 步：安装 Ollama\n\n您可以通过运行以下命令打开终端会话：\n\n```python\n%xterm\n```\n\n在终端中，运行以下命令以安装 Ollama：\n\n```python\ncurl -fsSL https://ollama.com/install.sh | sh\n```\n\n```python\nollama serve\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*itAzyQHMHhin8b7bRLc09w.png)\n\n### 第4步：拉取模型\n\n安装完Ollama后，您可以拉取所需的模型。Ollama提供了多个LLM，包括Llama 3\\.2\\. 以下是拉取它们的方法：\n\n```python\nollama pull llama3.2\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*S3R4gByToCZXKEWBh4GWaQ.png)\n\n上述命令将下载并准备模型以供在您的Colab环境中使用。\n\n或者，拉取Ollama中可用的任何LLM模型。所有LLM模型列表和详细信息可在此查看：[https://ollama.com/library](https://ollama.com/library)\n\n### 第5步：将LLaMA 3\\.2与LangChain集成\n\nLangChain使得调用LLaMA 3\\.2进行各种NLP任务变得简单。以下是测试模型的简单脚本：\n\n```python\nfrom langchain_community.llms import Ollama\n\n## Initialize an instance of the Llama 3.1 model\nllm_llama = Ollama(model=\"llama3.2\")\n\n## Invoke the model to generate a response\nresponse = llm_llama.invoke(\"Tell me a joke\")\nprint(response)\n```\n\n输出：\n\n```python\nHere's one:\n\nWhat do you call a fake noodle?\n\nAn impasta.\n```\n\n### 第6步：尝试不同的任务\n\n您可以将其扩展到更复杂的任务，如摘要、 multilingual translation 和推理：\n\n```python\n## Summarization\nresponse = llm_llama.invoke(\"Summarize the following text: 'LLaMA 3.2 represents a major step forward in AI development...'\")\nprint(response)\n\n## Multilingual Generation\nresponse = llm_llama.invoke(\"Translate the following into French: 'What are the major improvements in LLaMA 3.2?'\")\nprint(response)\n```\n\n输出：\n\n```python\nQuantum Mechanics is a complex and fascinating subject, but I'll try to break it down in simple terms.\n\n**The Basics**\n\nImagine you have a coin. Heads or tails, right? In classical physics (the way things work today), the coin is either one or the other - heads or tails. It's like a definite choice.\n\nIn Quantum Mechanics, however, the coin isn't quite so simple. When you flip it, it doesn't just land on heads or tails; it exists in both states at the same time! This idea might sound crazy, but that's basically what happens with tiny particles like atoms and electrons.\n\n**Wave-Particle Duality**\n\nHere's a key concept: tiny particles can behave like both waves and particles. It sounds weird, but think of it like this:\n\n* Imagine a wave in the ocean. The water molecules are moving up and down, creating ripples.\n* Now imagine a single water molecule as a particle (a tiny ball). That's what quantum mechanics says these particles can be!\n\n**Superposition**\n\nAnother mind-bending idea is superposition. It means that tiny particles can exist in multiple states at the same time. Think of it like this:\n\n* Imagine a coin that's both heads AND tails simultaneously!\n* This happens with electrons, which can spin both clockwise and counterclockwise at the same time.\n\n**Entanglement**\n\nQuantum Mechanics also introduces entanglement. When two particles interact, they become \"connected\" in such a way that what happens to one particle instantly affects the other, no matter how far apart they are!\n\n* Imagine two dancers who are perfectly synchronized, even if they're on opposite sides of the stage.\n* This is basically entanglement: two particles can be connected in a similar way.\n\n**The Weird Stuff**\n\nNow we get to some really weird and interesting aspects of quantum mechanics:\n\n* **Uncertainty Principle**: You can't know both the position AND momentum of a particle at the same time!\n* **Quantum Tunneling**: Particles can pass through solid objects, which is weird because they shouldn't be able to fit through.\n* **Schrödinger's Cat**: Imagine a cat in a box with a radioactive atom. If the atom decays, the cat dies. But according to quantum mechanics, the cat is BOTH alive AND dead until someone opens the box and observes it!\n\n**In a Nutshell**\n\nQuantum Mechanics is all about tiny particles behaving in strange and fascinating ways. It's like they're playing by their own rules, which can lead to some pretty wild and unpredictable outcomes.\n\nWhile this explanation is simplified, it should give you an idea of how quantum mechanics works and its weird and wonderful principles!\n```\n\n输出：\n\n```python\nThe translation of \"What are the major improvements in LLaMA 3.2?\" into French is:\n\n\"Quels sont les améliorations majeures de LLaMA 3.2?\"\n\nHere's a breakdown of the translation:\n\n- \"What\" becomes \"Quels\"\n- \"are\" becomes \"sont\"\n- \"the\" becomes \"les\"\n- \"major improvements\" become \"améliorations majeures\"\n- \"in\" becomes \"sont\"\n- \"LLaMA 3.2\" remains the same, as it's a proper noun.\n\nNote: The phrase \"majeures\" is used to describe significant or substantial improvements.\n```\n\n> **获取 GitHub 代码：**\n\n## 结论\n\nLLaMA 3\\.2 是一个多功能且高效的模型，在多种自然语言处理任务中表现出色，从多语言文本生成到实用工具使用。它在剪枝和知识蒸馏方面的创新确保了它在轻量级、资源受限环境中仍能保持顶级性能。通过本教程，您可以快速将 LLaMA 3\\.2 集成到本地应用程序或通过 Google Colab 等云服务中。\n\n通过解锁 LLaMA 3\\.2 的能力，开发者可以创建前沿的应用程序，这些应用不仅快速、响应灵敏，而且注重隐私，确保用户数据保留在设备上。无论您是在探索自然语言处理还是构建实际应用，LLaMA 3\\.2 都为轻量级、指令调优的语言模型设定了新的基准。\n\n欢迎您探索 Ollama 库中的其他模型，并尝试不同的任务。可能性是无穷无尽的！\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/longrag-giving-ai-a-bigger-net-to-catch-more-fish-in-the-sea-of-information-7ecdd63f330d","frontmatter":{"title":"LongRAG：让人工智能在信息海洋中捕捞更多鱼","meta_title":"LongRAG：让人工智能在信息海洋中捕捞更多鱼","description":"在我之前的文章中，我介绍了 RAG 是否会因长语境 LLM 而过时。今天，我们来看看如何申请……","date":"2024-11-08T00:17:39.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Nt5TRh0ooDkgmibMlA1Srg.png","categories":["Generative AI","Natural Language Processing","Data Science"],"author":"Rifx.Online","tags":["long-context","LLMs","RAG","retrieval","generation"],"draft":false,"slug":"blog/longrag-giving-ai-a-bigger-net-to-catch-more-fish-in-the-sea-of-information-7ecdd63f330d"},"content":"\n在 [我之前的文章](https://readmedium.com/will-long-context-llms-cause-the-extinction-of-rag-de41ca5ddfc6) 中，我介绍了RAG是否会因长上下文LLMs而变得过时。今天，让我们看看如何将长上下文LLMs应用于RAG场景。\n\n在检索增强生成（RAG）领域，传统方法一直依赖于短检索单元，通常约为100个单词，这迫使检索器在庞大的语料库中筛选以提取必要信息。这种设计虽然可行，但对检索器施加了不平衡的负担，往往因其必须处理的单元数量庞大而导致表现不佳。\n\n本文介绍了一项新研究，标题为“[LongRAG: 使用长上下文LLMs增强检索增强生成](https://arxiv.org/pdf/2406.15319v3)”。它旨在通过提出一种新颖的框架来解决这种不平衡，从而将检索单元的长度扩展到4,000个标记，显著提高检索器的效率和读者的表现。\n\n## 传统 RAG 与 LongRAG\n\n\n\n如图 1 所示，LongRAG 的核心创新在于其对传统 RAG 框架的重构。通过将检索单元的大小扩展到 4K tokens——是典型单元的 30 倍——LongRAG 将单元数量从数百万减少到可管理的几十万个。\n\n这种方法不仅减轻了检索器的负担，还增强了所检索信息的语义完整性，从而提高了下游性能。\n\n## LongRAG\n\nLongRAG框架由两个主要组件组成：**Long Retriever**和**Long Reader**。这两个组件的示例如图2所示。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*fs37A8QUj-y2rW9_iAqS3Q.png)\n\nLong Retriever通过将相关文档分组为保持语义完整性的统一体来组织检索过程。一旦识别出相关的长检索单元，它们将被传递给Long Reader，该组件能够处理广泛的上下文（大约30K个标记）。\n\n以下是工作流程的逐步分解：\n\n### 1\\. 制定长检索单元\n\nLongRAG的第一步是创建长检索单元。\n\n**在传统的RAG**框架中，检索单元较短，通常只有几百个标记，这可能导致信息碎片化，并且给检索器带来重大的负担，需要将相关上下文拼凑在一起。\n\n**LongRAG解决了这个问题**，通过将相关文档分组为连贯的长检索单元，这些单元显著更大 — 每个单元可达4,000个标记。\n\n为了形成这些长单元，LongRAG采用了一种分组算法，根据文档之间的关系组织文档，例如维基百科文章中嵌入的超链接。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*zPEDmLo7rcdCQ06e.png)\n\n例如，关于特定主题或实体的文档被分组在一起，以创建一个综合的检索单元（图2）。这确保了每个单元保持语义完整性，并为读者提供了更丰富的上下文，以便从中提取答案。\n\n### 2\\. 相似性搜索与排名\n\n一旦形成了长检索单元，下一步就是执行相似性搜索，以识别哪些单元与查询最相关。\n\n查询通过编码器函数 E\\_Q 编码为一个向量，每个检索单元也通过另一个编码器函数 E\\_C 进行类似的编码。查询 `q` 与每个检索单元 `g` 之间的相似性通过它们各自向量的点积来计算。\n\n然而，考虑到检索单元的长度，**直接编码整个单元可能计算开销大且效果较差**。**为了解决这个问题，LongRAG 通过将长单元分解为更小的块来近似相似性**，并计算这些块之间的最大相似性得分。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*U1BsMZuyXqO1oqsl.png)\n\n这种方法类似于[以往工作的 MaxP 设计](https://arxiv.org/pdf/1905.09217)，使 LongRAG 能够高效地识别每个长检索单元中最相关的部分，而不会牺牲性能。\n\n### 3\\. 聚合检索结果\n\n在计算相似度分数后，基于与查询的相关性选择前 k 个检索单元。**这些选定的单元随后被连接起来形成一个单一的长上下文，通常包含约 30,000 个标记。** 这个聚合的上下文将被传递给 Long Reader。\n\nk 的大小或检索单元的数量对于平衡工作负载至关重要。如果检索单元太短，则需要更多单元，这可能会使阅读器不堪重负。相反，如果单元太长，则需要的数量较少，但必须高度相关，以避免包含多余的信息。\n\nLongRAG 通过使用适量的结构良好的长检索单元来优化这种平衡，通常在 4 到 8 之间，具体取决于任务。\n\n### 4\\. 通过长阅读器处理\n\n长阅读器是负责从长上下文中提取最终答案的组件。此步骤利用先进的长上下文语言模型，如GPT-4o或Gemini-1.5-Pro，能够处理大量文本序列而不丢失关键信息。\n\n对于较短的上下文（少于1,000个tokens），长阅读器直接提取答案。然而，对于典型的长RAG的较长上下文，该过程更加细致。最初，模型生成一个涵盖几句话的详细响应，确保捕捉到所有相关信息。然后，通过第二轮处理，长阅读器对初始输出进行精炼，将响应浓缩为一个精确、简洁的答案。\n\n这种两步法确保长阅读器能够有效处理长检索单元提供的大量信息，同时仍然提供准确且集中的答案。\n\n## 评估\n\n本文对 LongRAG 在知名数据集上的表现进行了全面评估，如 Natural Questions (NQ) 和 HotpotQA。结果令人信服，检索性能有所提升，NQ 的答案召回率从 52% 提升至 71%（图 4），HotpotQA 的答案召回率从 47% 提升至 72%（图 5）。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*wLUdp-4OihjAz8Fu.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*vmTsnuIsV6LxJFtj.png)\n\n## 结论\n\n本文探讨了创新的 LongRAG 框架，这是一种通过扩展 RAG 框架以处理长文档的创新方法，使模型能够有效地处理和生成来自扩展上下文的答案。它结合了一个多步骤检索过程，动态检索长文本的相关部分，确保在生成阶段使用最相关的信息。这使得 LongRAG 在需要理解和综合来自冗长复杂文档的信息的任务中表现出色，在这种情况下优于传统的 RAG 模型。\n\n然而，这种方法并非没有挑战。对强大的长上下文模型的依赖意味着该框架的性能与这些模型的能力紧密相关。此外，用于创建长检索单元的分组算法可能需要进一步改进，以便在超越基于维基百科的语料库时进行泛化。\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/meet-ministral-3b-and-8b-edge-ai-game-changers-3f7532da8f90","frontmatter":{"title":"认识 Ministral 3B 和 8B：边缘 AI 游戏规则改变者","meta_title":"认识 Ministral 3B 和 8B：边缘 AI 游戏规则改变者","description":"Mistral AI 在边缘 AI 和设备计算领域的新前沿","date":"2024-11-01T03:55:06.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*3CmWlEiW7ea8gtqxpI83_w.png","categories":["Technology","Autonomous Systems","Data Science"],"author":"Rifx.Online","tags":["Mistral","edge","computing","translation","robotics"],"draft":false,"slug":"blog/meet-ministral-3b-and-8b-edge-ai-game-changers-3f7532da8f90"},"content":"\n\n\n### Mistral AI在边缘AI和设备计算的新前沿\n\n在快速发展的AI领域，边缘计算变得越来越重要，适用于那些需要低延迟、以隐私为首的高效推理的应用，而不依赖于基于云的基础设施。\n\n**Mistral AI**最新推出的[**Ministral**](https://mistral.ai/news/ministraux/)模型家族，标志着AI领域的一次突破性进展。\n\n为庆祝其开创性的**Mistral 7B**模型发布一周年，Mistral AI推出了下一代语言模型：**Ministral 3B**和**Ministral 8B**，统称为“[**les Ministraux**](https://mistral.ai/news/ministraux/)”。这些模型不仅仅是渐进式的改进；它们代表了边缘AI可能性的重大飞跃。\n\n\n\n## 为什么这些模型很重要？\n\n边缘人工智能的核心在于在本地执行复杂计算，确保数据隐私并减少响应时间。通过 **Ministral 3B** 和 **Ministral 8B**，Mistral AI 提供了将高计算能力与内存效率相结合的模型，所有这些都可以直接在设备上运行。这些模型旨在为无法承受延迟或依赖云连接的应用程序提供实时洞察。\n\n## 主要特点：\n\n1. **最先进的性能**：在知识、常识、推理、原生函数调用和小于10B类别的效率等不同任务中超越现有模型。\n2. **大上下文窗口**：支持最多128k的上下文长度，实现更全面的理解和生成。\n3. **高效架构**：Ministral 8B采用特殊的交错滑动窗口注意力模式，实现更快和更节省内存的推理。\n4. **多功能性**：适用于广泛的应用，从设备内翻译到自主机器人。\n5. **隐私优先设计**：为本地推理而构建，这些模型非常适合优先考虑数据隐私的应用，消除了对持续云访问的需求。\n6. **可扩展性**：无论是需要Ministral 3B的低功耗消耗的小型设备，还是需要8B变体的更大能力，这两种模型都足够灵活，可以适应各种用例。\n\n> 有关基准测试结果，请参阅 [这里](https://mistral.ai/news/ministraux/)\n\n## 分析模型：\n\n### Ministral 3B:\n\n* 仅凭 **30亿个参数**，为资源受限的环境提供了平衡的解决方案\n* 支持最高 **128k 上下文长度**，可以全面处理复杂查询\n* 适用于超低延迟应用\n* 在同类模型中表现优于许多其他模型\n\n### Ministral 8B:\n\n* 具有 **80亿参数** 和 **128k上下文长度**，在处理更复杂的任务时能够提供增强的计算能力\n* 采用 **滑动窗口注意力** 模式，提高速度和内存效率\n* 基于广泛的 **多语言** 和 **代码** 数据，使其适用于多种应用\n* 支持 **函数调用**\n* 在高要求的应用中平衡性能和效率\n* 词汇量为 **131k**，使用 **V3-Tekken** 分词器\n* 提示模板：\n\n\n```python\n<s>[INST]user message[/INST]assistant response</s>[INST]new user message[/INST]\n```\n\n## 用例：\n\n这些模型提供了计算高效和低延迟的性能，使其非常适合以下场景：\n\n* **设备端翻译**：使用户能够在实时中无缝沟通跨语言，即使在网络连接较差的地区。\n* **无网络智能助手**：支持独立于云连接运行的智能虚拟助手，增强隐私敏感环境中的用户体验。\n* **本地分析**：使组织能够实时分析数据，同时保持严格的隐私标准，这在医疗和金融等行业至关重要。\n* **自主机器人**：为机器人配备先进的语言能力，以实现自主决策和沟通，提高它们在各个行业的运营效率。\n\n除了其独立的能力外，les Ministraux 还可以与更大的模型如 Mistral Large 协同工作。这种协同使它们能够作为 **在代理工作流中进行函数调用的高效中介**，处理：\n\n* **输入解析**：快速解释用户输入，以确保准确响应。\n* **任务路由**：根据用户意图将请求指向适当的资源。\n* **API 调用**：实时执行 API 功能，确保在各种上下文中顺畅互动。\n\n## 代码使用（与 vLLM 一起）：\n\n[Ministral\\-8B\\-Instruct\\-2410](https://huggingface.co/mistralai/Ministral-8B-Instruct-2410) 语言模型是一个经过指令微调的模型，可以使用 vLLM 高效部署。您可以在 Hugging Face 上 [这里](https://huggingface.co/mistralai/Ministral-8B-Instruct-2410) 找到它。以下是您可以开始的方式：\n\n### 安装\n\n首先，确保您已安装最新版本的 vLLM 和 mistral\\_common：\n\n\n```python\npip install --upgrade vllm\npip install --upgrade mistral_common\n```\n\n> ***注意****：需要 vLLM 版本 0\\.6\\.2 或更高版本。*\n\n### 离线使用 vLLM\n\n以下是如何在离线模式下使用 Ministral\\-8B 和 vLLM 的示例：\n\n\n```python\nfrom vllm import LLM\nfrom vllm.sampling_params import SamplingParams\n\nmodel_name = \"mistralai/Ministral-8B-Instruct-2410\"\nsampling_params = SamplingParams(max_tokens=8192)\n\nllm = LLM(model=model_name, tokenizer_mode=\"mistral\", config_format=\"mistral\", load_format=\"mistral\")\n\nprompt = \"What are the potential implications of artificial intelligence on the job market in the next decade?\"\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": prompt\n    },\n]\n\noutputs = llm.chat(messages, sampling_params=sampling_params)\nprint(outputs[0].outputs[0].text)\n```\n\n### 服务器模式推理与 vLLM\n\n在服务器推理模式下，vLLM 运行一个 HTTP 服务器，能够通过与 OpenAI 协议兼容的 REST API 同时处理客户端连接和请求。以下是设置方法：\n\n* 启动服务器：\n\n```python\nvllm serve mistralai/Ministral-8B-Instruct-2410 --tokenizer_mode mistral --config_format mistral --load_format mistral\n```\n* 向服务器发送请求：\n\n```python\ncurl --location 'http://localhost:8000/v1/chat/completions' \\\n    --header 'Content-Type: application/json' \\\n    --header 'Authorization: Bearer token' \\\n    --data '{\n        \"model\": \"mistralai/Ministral-8B-Instruct-2410\",\n        \"messages\": [\n          {\n            \"role\": \"user\",\n            \"content\": \"What are the potential implications of artificial intelligence on the job market in the next decade?\"\n          }\n        ]\n      }'\n```\n\n> 关于 vLLM 使用的重要说明：\n\n* 目前，由于在实现分页注意力的交错注意力内核方面的限制，vLLM 的上下文大小限制为 32k。\n* 为了利用完整的 128k 上下文大小，建议使用 [Mistral Inference](https://github.com/mistralai/mistral-inference)。\n* 如果您需要减少 GPU 内存需求，可以通过在 LLM 初始化时添加 `tensor_parallel=2` 来使用张量并行。\n\n通过遵循这些示例，您可以轻松地将 Ministral\\-8B 集成到您的项目中，无论您是在离线推理还是为多个客户端设置服务器。该模型的高效性和强大功能，加上 vLLM 的优化推理，使其成为各种 AI 应用的优秀选择。\n\n## 结论：\n\nMinistral的发布标志着人工智能发展中的一个重要里程碑。通过将GPT级别的性能带到边缘设备，Mistral AI不仅在推动技术边界——他们还在重新构想以本地、隐私优先的人工智能为基础的可能性。\n\n随着开发者、研究人员和企业开始探索Ministral的能力，我们可以期待看到一波新的人工智能驱动的应用程序，这些应用程序比以往更加快速、更具隐私性和更易于获取。边缘人工智能的时代已经到来，而Ministral正在引领这一潮流。\n\n"},{"lang":"zh","group":"blog","slug":"blog/meet-qwen2-5-coder-32b-instruct-coder-open-source-better-than-gpt4o-5dc8343f8157","frontmatter":{"title":"满足 Qwen2.5-Coder-32B-Instruct -Coder - 开源比 gpt4o 更好","meta_title":"满足 Qwen2.5-Coder-32B-Instruct -Coder - 开源比 gpt4o 更好","description":"Qwen2.5-Coder-32B是一款开源的AI编码助手，具有320亿参数和长达128K的上下文处理能力，支持29种语言。其性能在编码能力上与GPT-4o相媲美，且生成的代码语法准确、高效。尽管其计算需求较高，但其快速、准确的特性使其成为开发者的有力工具，能够提高生产力和学习效率。","date":"2024-11-14T03:29:09.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*VENiO-pvY-FzxBLUqodjRQ.jpeg","categories":["Programming","Generative AI","Data Science"],"author":"Rifx.Online","tags":["parameters","coding","benchmarks","languages","efficiency"],"draft":false,"slug":"blog/meet-qwen2-5-coder-32b-instruct-coder-open-source-better-than-gpt4o-5dc8343f8157"},"content":"\n**认识** Qwen2\\.5\\-Coder\\-32B-Coder，您新的 AI 编码伙伴\n\n您是否曾希望编码变得更简单、更快速，甚至更有趣？那么，准备好迎接您的新 AI 编码朋友 Qwen2\\.5\\-Coder。Qwen2\\.5\\-Code 专门开发了这个模型，作为一个尖端语言模型，以简化您的编码体验。想象一下，拥有一个知识渊博的助手，可以为您编写代码、调试、解释复杂概念，并处理多种语言。感兴趣吗？让我们来看看 Qwen2\\.5\\-Coder 有何独特之处。\n\n\n\n🧠 **强大的性能：匹敌 GPT\\-4o 的编码能力**\n\n> **Qwen2\\.5\\-Coder**，特别是 32B\\-Instruct 版本，不仅仅是一个代码助手；它是一款顶级表现者，匹敌甚至超越 GPT\\-4o 和 Sonnet 3\\.5，被认为是最强大的人工智能模型之一。想象一下，您也可以拥有这种水平的编码能力。\n\n## 开源\n\n但这并不只是关于原始能力；该模型展现了出色的正确性，生成了语法上精确且高效的代码。最棒的是什么？它比其前身快得多，使您能够快速完成任务。\n\n**主要特点**\n\n* **模型大小**：320亿参数。\n* **上下文长度**：支持最多128K个token，允许广泛的输入和输出能力。\n* **多语言支持**：该系统可以处理超过29种语言，包括英语、中文、法语和西班牙语。\n* **指令遵循**：此功能增强了遵循复杂指令的能力，并生成结构化输出，如JSON。\n* **性能基准**：团队在各种编码基准（如HumanEval和MATH）上得分很高。\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*zyjKE3ZHtax3uX9GnbKUfA.png)\n\n有适合所有需求的模型，从小型到大型。\n\n无论您的经验水平如何，Qwen2.5-Coder都提供全面的覆盖。Qwen2.5-Coder有多种大小，从0.5B到令人印象深刻的32B。这意味着您可以选择最符合您的需求和资源的模型。就像拥有一个工具箱，里面有不同大小的扳手，每个都适合独特的任务。\n\n🌎 精通多种语言\n\n使用多种语言进行编码？没问题！Qwen2.5-Coder支持超过29种语言，包括英语、中文、法语和西班牙语等流行语言。这种双语能力使其成为全球开发人员非常适应的工具。就像拥有一个通用的代码翻译器，消除了语言障碍，开启了新的可能性。\n\n👍 优势：提高生产力和改善学习\n\n让我们讨论一下优点。\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MAhK8R45yNzB8A7mZZITBg.png)\n\n**长上下文处理**：该模型可以处理长达128K个token的输入。这对于需要广泛背景的复杂编码任务尤其有用。**多语言技能**：Qwen2.5-Coder-32B-Instruct支持超过29种语言，包括英语、中文、法语和西班牙语。这使其成为在多语言项目中工作的开发人员的宝贵工具。\n\n👎 **缺点**：资源密集型和过度依赖的风险。\n\n当然，每项技术都有其缺点。Qwen2.5-Coder对处理能力的要求很高，特别是在其较大变体中。充分利用它需要强大的硬件。\n\n🎉 **编码的未来**？\n\nQwen2.5-Coder标志着AI驱动编码的重大进展。其精确性、速度、适应性和开源特性使其成为一个引人注目的突破。对于开源社区的真正好处将是如果所需的计算能力减少，开发的API成本也更低。\n\n除此之外，它非常有前景，也将使大型参与者在付费墙下受到控制。\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*aHeNvfOvcpME0qzy6EQexQ.jpeg)\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*PI0ioI2MtxQgtNZ0Tq1jwQ.jpeg)\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/metas-llama-4-is-coming-soon-plus-parallels-brings-apple-intelligence-to-windows-c1c2722dcf03","frontmatter":{"title":"Meta's Llama 4 即将推出 另外Parallels 为 Windows 带来 Apple 智能","meta_title":"Meta's Llama 4 即将推出 另外Parallels 为 Windows 带来 Apple 智能","description":"未提供字幕","date":"2024-11-10T03:51:17.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*sYakQyN_2Lupo_By","categories":["Technology","Machine Learning","Generative AI"],"author":"Rifx.Online","tags":["Llama","GPUs","Parallels","Recraft","Midjourney"],"draft":false,"slug":"blog/metas-llama-4-is-coming-soon-plus-parallels-brings-apple-intelligence-to-windows-c1c2722dcf03"},"content":"\n\n\n### Plus: Parallels 将苹果智能带入 Windows\n\n\n\n**欢迎来到 Get The Gist**，在这里，我们每个工作日分享最新和最伟大的 AI 发展简明易懂的总结——新闻、创新和趋势——所有内容都在 5 分钟内送达！⏱\n\n**在今天的版块中：**\n\n* 马克·扎克伯格宣布 Meta 的 Llama 4\n* Parallels 将苹果智能带入 Windows\n* Recraft V3 挑战 Midjourney\n* Meta AI 用户超过 5 亿\n* 还有更多 AI 新闻….\n\n## 1\\. Meta的Llama 4即将发布，带来重大AI进展\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*E_j8uSNV6s3lg2vm)\n\n**要点：** 马克·扎克伯格[**确认**](https://analyticsindiamag.com/ai-news-updates/mark-zuckerberg-confirms-llama-4-release-early-next-year/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon) Meta将在明年初推出其Llama 4模型，承诺在速度、推理和跨模态方面提供新能力，这得益于创纪录的训练配置。\n\n**关键细节：**\n\n* Meta正在使用超过100,000个H100 GPU的大型配置训练Llama 4，这是报告中最大的AI集群之一，目标是比以往更快、更强大的模型。\n* 新的Llama 4将引入先进的功能，如扩展内存、支持多种数据类型和无缝的第三方集成。\n* AI继续推动Meta的增长，因为生成工具帮助超过一百万广告客户将转化率提高7%，并提升Facebook和Instagram上的用户参与度。\n* 扎克伯格强调，AI创新正在创造新的商业机会，强调Meta对产品和平台长期AI驱动增长的承诺。\n\n## 2\\. Parallels 将 Apple 智能带入 Windows\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*36yykSGFUbML6zR4)\n\n**要点：** Parallels Desktop [**现在支持**](https://www.neowin.net/news/parallels-brings-apple-intelligence-features-to-windows/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon) 在 Windows 虚拟机上使用 Apple 的 AI 驱动写作工具，使用户能够在 Windows 应用中利用 Apple 智能增强文本。\n\n**主要细节：**\n\n* Parallels Desktop 20\\.1 现在支持在 macOS Sequoia 15\\.1 中的 Windows 应用上使用 Apple 写作工具，让用户可以在 Word 和记事本等应用中访问文本改进功能，如摘要、重写和语气调整。\n* 要激活此功能，使用 macOS 15\\.1 和兼容的 Mac（M1 或更新版本）的用户可以更新 Parallels，并使用快捷键在 Windows 应用中应用这些工具。\n* Apple 写作工具是 Apple 智能的一部分，也正在 iPadOS 和 iOS 上推出，但仅限于配备先进处理器的设备，如 M1、M2 或 A17 Pro 芯片。\n* 此更新为 Mac 用户提供了一种无缝的方式，在 Mac 和 Windows 环境中增强他们的写作，将 Apple 的 AI 与 Windows 的可用性相结合。\n\n## 3\\. Recraft V3 挑战 Midjourney，聚焦设计师的 AI 图像生成\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*lYoMCLyX61RKwMiF)\n\n**要点：** Recraft [**已推出**](https://www.tomsguide.com/ai/ai-image-video/watch-out-midjourney-recraft-just-announced-new-ai-image-generator-model?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon) Recraft V3，这是一款新的图像生成 AI 模型，旨在通过强大的设计聚焦功能和无缝的文本集成超越 Midjourney 等竞争对手。\n\n**关键细节：**\n\n* Recraft V3 引入了图像中精确的文本处理功能，允许用户轻松添加和样式化文本，这是 AI 模型中罕见的功能；目前在 Hugging Face 的排行榜上排名第一。\n* 设计师现在可以控制文本位置、品牌颜色和独特风格，提供更好的定制化，满足创意专业人士的关键需求。\n* 借助无限画布、实时协作和用于高级工作流程的 API，Recraft V3 支持个人和团队设计项目。\n* Recraft 拥有超过 150 万用户，生成了超过 2 亿张图像，该工具可在网页、iOS 和 Android 平台上使用。\n\n## 快速摘要\n\n* **Zenity** 完成了 3800 万美元的 B 轮融资，以推进企业使用代理 AI 和低代码工具的安全解决方案，解决流程自动化中的关键安全问题 [(阅读更多)](https://www.darkreading.com/application-security/zenity-raises-38m-series-b-funding-round-to-secure-agentic-ai?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon).\n* **OpenAI** 更新了其实时 API，新增五种富有表现力的语音用于语音对语音应用，并通过提示缓存显著降低了成本，目前处于测试阶段 ([阅读更多](https://venturebeat.com/ai/openai-expands-realtime-api-with-new-voices-and-cuts-prices-for-developers/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n* **OpenAI** 在欧洲为免费用户推出了高级语音模式，允许与 ChatGPT 进行引人入胜的人类般的互动 ([阅读更多](https://www.tomsguide.com/ai/openai-advanced-voice-is-now-free-for-10-minutes-a-month-3-tips-for-getting-the-most-out-of-that-time?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n* **OpenAI** 正在为 ChatGPT 推出一项新功能，允许用户搜索他们的聊天记录，计划下个月向免费用户开放 ([阅读更多](https://indianexpress.com/article/technology/artificial-intelligence/chatgpt-now-allow-users-to-search-through-their-history-heres-how-to-use-it-9647233/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n* **Meta** 正在与美国政府合作，将其 AI 模型 Llama 应用于多个公共部门项目，包括改善资源获取和简化财务援助，而无需涉及任何财务交易 ([阅读更多](https://www.newsbytesapp.com/news/science/meta-working-to-get-llama-used-in-us-government-sectors/story?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n* **Meta** 计划在明年初推出其 Llama 4 AI 模型，训练其在超过 100,000 个 H100 GPU 的前所未有的集群上，同时尽管存在潜在滥用的担忧，仍倡导开源方法 ([阅读更多](https://www.newsbytesapp.com/news/science/meta-trains-llama-4-models-on-largest-nvidia-gpu-cluster/story?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n* **OpenAI** 已为 ChatGPT 推出了高级语音模式，允许用户在桌面应用中进行自然的语音对话，该功能已在订阅用户中获得了人气 ([阅读更多](https://www.digitaltrends.com/computing/chatgpt-advanced-voice-mode-macos-windows-desktops/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n* **Waymo** 正在通过开发新的多模态大语言模型 EMMA 来提升其自动驾驶技术，以改善其机器人出租车在复杂环境中的决策能力和适应性 ([阅读更多](https://www.theverge.com/2024/10/30/24283516/waymo-google-gemini-llm-ai-robotaxi?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n* **Gemini** 现在为大屏 Android 设备（如 Pixel Tablet 和 Fold）提供了分屏快捷方式，增强了用户体验 ([阅读更多](https://www.androidauthority.com/gemini-split-screen-shortcut-3495573/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n* **Meta AI** 在推出仅一年内用户已超过五亿，预计到 2024 年底有可能成为使用最广泛的 AI 助手，尽管在欧盟面临隐私挑战 ([阅读更多](https://www.phonearena.com/news/meta-ai-reaches-500-million-users-in-one-year_id164309?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n* **Adobe** 更新了 Illustrator 和 Photoshop，增加了 AI 驱动的功能，以简化创作流程并增强用户灵活性，强调增强人类创造力而非取代 ([阅读更多](https://www.gearpatrol.com/tech/six-new-powerful-ai-features-every-adobe-photoshop-illustrator-must-try/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n* **NVIDIA** 研究人员推出了 HOVER，这是一种 150 万参数的神经网络，使类人机器人能够通过高效的运动协调和实时适应性执行复杂任务 ([阅读更多](https://analyticsindiamag.com/ai-news-updates/nvidia-introduces-hover-a-1-5-m-parameter-neural-network-for-humanoid-robotics/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n* **Google** 为 Pixel 设备推出了一款新的独立天气应用，利用 AI 总结户外情况并提供多地点跟踪 ([阅读更多](https://www.theverge.com/2024/10/30/24283998/google-weather-app-pixel-8-7-6-ai-summaries?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n\n今天就到这里，明天见！ 👋\n\n如果您喜欢这个更新并想了解 AI 的最新发展，请考虑在 Medium 上订阅 ***Get The Gist*** 以获取更多见解和分析。\n\n**想要深入了解吗？** 订阅我们的免费每日电子邮件通讯，快速、简洁的更新直接发送到您的收件箱，以便您不会错过任何重要发展。您可以通过点击 [这里](https://getthegist.beehiiv.com/) 注册。\n\n让我们一起探索 AI 的世界——每次摘要都是一次新的旅程！ 💡🤖\n\n"},{"lang":"zh","group":"blog","slug":"blog/microsoft-graphrag-v0-4-0-ec98f1f6ed7a","frontmatter":{"title":"Microsoft GraphRAG v0.4.0","meta_title":"Microsoft GraphRAG v0.4.0","description":"微软最近发布了 GraphRAG 项目的 v0.4.0 版本，其中有几项重大更新。其中最引人注目的更新是...","date":"2024-11-13T01:22:29.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*89qTckZYLUBF1Jtv","categories":["Programming","Data Science","Machine Learning"],"author":"Rifx.Online","tags":["GraphRAG","Incremental","Indexing","DRIFT","Embedding"],"draft":false,"slug":"blog/microsoft-graphrag-v0-4-0-ec98f1f6ed7a"},"content":"\n\n\n微软最近发布了 GraphRAG 项目的 v0\\.4\\.0 版本，带来了几项重要更新。最显著的新增功能是增量索引特性和 DRIFT 图推理查询模块，这大大增强了系统的效率和功能。\n\n\n\n此次更新的核心亮点包括：\n\n1\\. 增量索引：显著提高大规模数据处理的效率，实现更快的信息更新。\n\n2\\. DRIFT 图推理查询模块：引入先进的图推理技术，增强复杂查询处理能力。\n\n此外，版本 0\\.4\\.0 优化了嵌入工作流程，重构了处理流程，提高了整体系统性能和可操作性。它还增加了 DRIFT 搜索 CLI 和示例笔记本，以帮助开发人员更好地理解新功能。此外，引入关系合并和增量更新配置选项进一步增强了 GraphRAG 的灵活性和智能水平。\n\n这些更新不仅提高了 GraphRAG 的处理速度，例如在大规模金融数据分析中，增量索引特性可以将数据更新时间从数小时减少到数分钟。同时，它们也增强了其在复杂知识图谱应用中的适用性，显著拓宽了使用场景。行业专家预测，这些改进将在金融分析和医疗诊断等领域发挥关键作用，推动 AI 应用向更精确和高效的方向发展。\n\n更多更新内容： [https://proxy.rifx.online/https://github.com/microsoft/graphrag/releases/tag/v0\\.4\\.0](https://proxy.rifx.online/https://github.com/microsoft/graphrag/releases/tag/v0.4.0)\n\n"},{"lang":"zh","group":"blog","slug":"blog/mistral-ai-releases-revolutionary-edge-models-ministral-3b-and-8b-superior-performance-and-privacy-5b24f0189493","frontmatter":{"title":"Mistral AI 发布革命性边缘模型 Ministral 3B 和 8B：卓越性能和隐私","meta_title":"Mistral AI 发布革命性边缘模型 Ministral 3B 和 8B：卓越性能和隐私","description":"没有提供字幕","date":"2024-10-31T08:32:15.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*zFNeFlbfEnbjV5M65sH5ig@2x.jpeg","categories":["Technology","Machine Learning","Autonomous Systems"],"author":"Rifx.Online","tags":["edge","models","privacy","tokens","attention"],"draft":false,"slug":"blog/mistral-ai-releases-revolutionary-edge-models-ministral-3b-and-8b-superior-performance-and-privacy-5b24f0189493"},"content":"\n\n\n最近，Mistral AI 推出了两个新的边缘模型——Ministral 3B 和 Ministral 8B，这引起了科技界的广泛关注。这些模型不仅在性能上表现出色，还在隐私保护方面提供了独特的优势。\n\n\n\n## 卓越性能，隐私优先\n\nMinistral 3B 和 8B 专为设备内计算而设计，能够处理长度达 128k 的文本信息。特别是，Ministral 8B 采用了创新的滑动窗口注意力机制，显著提高了计算速度和内存效率。此外，这两个模型在设计上优先考虑隐私保护，确保数据在本地处理，以降低数据泄露的风险。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GMgT6erSorAGUp-pqbXWhA@2x.jpeg)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*zRGh7rw7oVXYd5mOhXoc3g@2x.jpeg)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*IIYgXVtbHvWqn6QLSZ-0Ow@2x.jpeg)\n\n## 多功能应用，无限潜力\n\nMinistral系列模型具有广泛的应用。在智能助手领域，它们可以快速响应用户命令，同时确保数据安全；在自主机器人领域，它们强大的推理能力支持复杂的决策和操作。\n\n## 成本效益高，广阔的市场前景\n\n尽管表现出色，Ministral 3B 和 8B 的价格竞争力依然很强。3B 的价格为每百万个令牌 $0.04，而 8B 的价格为 $0.10。这一定价策略为企业和开发者提供了一个成本效益高的选择。目前，这两个模型均可供使用。\n\n## 前景光明，引领边缘计算新趋势\n\nMistral AI 发布的 Ministral 系列模型展示了其在边缘计算方面的深厚技术实力，为未来的设备端 AI 应用奠定了坚实基础。随着技术的进步和应用的深入探索，Ministral 模型预计将在智能设备和物联网中发挥更大作用。\n\n总之，Ministral 3B 和 8B 的推出不仅是 Mistral AI 的一个重要里程碑，也是 AI 行业的一次重大进步，为设备端计算带来了新的可能性。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*A6SToo3fO3DqnlWX)\n\n"},{"lang":"zh","group":"blog","slug":"blog/mistral-ai-unveils-ministral-3b-and-8b-models-plus-nvidia-launches-ai-model-that-outperforms-gpt-4-941712f5d22d","frontmatter":{"title":"Mistral AI 推出 Ministral 3B 和 8B 模型 另外：Nvidia 推出优于 GPT-4 的 AI 模型","meta_title":"Mistral AI 推出 Ministral 3B 和 8B 模型 另外：Nvidia 推出优于 GPT-4 的 AI 模型","description":"没有提供字幕","date":"2024-10-31T08:29:07.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*PtPEkgjabwBUu73Y","categories":["Technology","Generative AI","Machine Learning"],"author":"Rifx.Online","tags":["Mistral","edge","Llama","YouTube","DreamTracks"],"draft":false,"slug":"blog/mistral-ai-unveils-ministral-3b-and-8b-models-plus-nvidia-launches-ai-model-that-outperforms-gpt-4-941712f5d22d"},"content":"\n\n\n### Plus: Nvidia推出的AI模型超越GPT\\-4\n\n\n\n**欢迎来到Get The Gist**，在这里我们每个工作日分享最新的AI发展动态——新闻、创新和趋势——所有内容都在5分钟内轻松阅读！⏱\n\n**在今天的版本中：**\n\n* Mistral AI推出了用于边缘计算的Ministral 3B和8B模型\n* Nvidia悄然推出的AI模型超越GPT\\-4\n* YouTube向美国创作者推出AI音乐工具“梦幻曲目”\n* Google Gemini现在可以生成可自定义宽高比的图像\n* 还有更多AI新闻……\n\n## 1\\. Mistral AI 发布 Ministral 3B 和 8B 模型以支持边缘计算\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*qAjoYMHGI1TkNy_A)\n\n**要点:** Mistral AI 已经[**推出了两个新的 AI 模型**](https://analyticsindiamag.com/ai-news-updates/mistral-ai-launches-ministral-3b-and-8b-models-for-edge-computing/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models)，Ministral 3B 和 8B，旨在实现高效的设备内和边缘计算。这些模型在性能上超越了竞争对手，并且专为需要隐私优先、本地推理的任务而设计。\n\n**关键细节:**\n\n* 模型处理大上下文长度（最长可达 128k），在资源有限的环境中实现流畅性能。\n* 适用于智能助手、本地分析和机器人等应用，提升任务效率。\n* 以具有竞争力的定价提供商业使用，并为 8B Instruct 模型提供研究访问。\n* 在基准测试中超越了 Gemma 2 和 Llama 3 等 AI 模型。\n\n## 2\\. Nvidia 安静推出超越 GPT\\-4 的 AI 模型\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Mza84SHereM3w5rN)\n\n**要点:** Nvidia [**发布了一款新 AI 模型**](https://venturebeat.com/ai/nvidia-just-dropped-a-new-ai-model-that-crushes-openais-gpt-4-no-big-launch-just-big-results/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models)，Llama\\-3.1\\-Nemotron\\-70B\\-Instruct，其性能基准超越了行业巨头如 OpenAI 的 GPT\\-4。这次发布标志着 Nvidia AI 战略的重大扩展，从硬件转向高性能 AI 软件。\n\n**关键细节:**\n\n* Nvidia 的新模型在关键基准测试中得分高于 GPT\\-4，展示了卓越的语言理解和生成能力。\n* 该模型采用了先进技术，如人类反馈强化学习 (RLHF)，在处理复杂查询方面表现出色。\n* Nvidia 通过其平台提供免费访问，允许企业试用这一强大的 AI 工具。\n* 该模型可根据业务需求进行定制，但在法律推理或数学等专业领域的使用需要谨慎。\n\n## 3\\. YouTube 在美国推出 AI 音乐工具 “Dream Tracks”\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*5nUNrJmdCBBy4JdQ)\n\n**要点:** YouTube 在美国推出了其 [**AI 驱动的音乐生成器**](https://www.mediapost.com/publications/article/400280/youtube-brings-ai-audio-generator-to-us-creators.html?edition=136037&utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models) “Dream Tracks”，允许创作者使用文本提示为他们的短视频创建自定义音频。该工具旨在通过音乐创作加深艺术家与粉丝之间的联系。\n\n**主要细节:**\n\n* 由 Google DeepMind 的 Lyria 提供支持，Dream Tracks 为 YouTube Shorts 生成定制的乐器配乐。\n* 美国创作者现在可以使用此工具创建最长 30 秒的免版税配乐。\n* 用户可以对 AI 生成的音频片段进行混音，增强创作可能性。\n* YouTube 对所有 AI 生成的曲目应用隐藏的 SynthID 水印，以确保透明度。\n\n## 快速摘要\n\n* **Clerk Chat** 获得了由 Race Capital 领投的 700 万美元融资，以增强其 AI 驱动的商业沟通平台 [(阅读更多)](https://www.businesswire.com/news/home/20241017292794/en/World%E2%80%99s-First-AI-Telecom-Clerk-Chat-Raises-7.0-Million-in-Seed-Funding?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models)。\n* **Anthropic** 首席执行官 Dario Amodei 发布了一篇长篇博文，阐述了对人工通用智能变革潜力的乌托邦愿景，同时寻求为公司争取 400 亿美元的估值 [(阅读更多)](https://www.theverge.com/2024/10/16/24268209/anthropic-ai-dario-amodei-agi-funding-blog?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models)。\n* **Google Cloud** 宣布其升级版 Vertex AI 平台和医疗数据引擎的正式上线，以增强医疗领域的 AI 应用 [(阅读更多)](https://www.forbes.com/sites/saibala/2024/10/17/google-cloud-announces-general-availability-of-vertex-ai-for-healthcare/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models)。\n* **Amazon** 领投了 5 亿美元的融资轮，为 X-energy 推出到 2039 年的 5GW 小型核反应堆，而 **Google** 与 Kairos Power 合作，计划到 2035 年安装 500MW 的小型模块化反应堆，二者都旨在利用清洁能源满足数据中心日益增长的能源需求 [(阅读更多)](https://www.theengineer.co.uk/content/news/amazon-and-google-bet-big-on-smrs-to-power-ai?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models)。\n* **Google** 将于 2025 年初在 Google Distributed Cloud 中为公共部门机构推出其 Gemini AI 模型，并提供资金以提升政府员工在负责任的 AI 实践方面的技能 [(阅读更多)](https://siliconangle.com/2024/10/16/google-looks-spearhead-ai-adoption-public-sector/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models)。\n* **Google** 的 Gemini AI 聊天机器人将推出一项功能，允许用户以可自定义的宽高比生成图像，增强其图像编辑能力 [(阅读更多)](https://indianexpress.com/article/technology/artificial-intelligence/google-gemini-may-soon-get-new-image-resizing-feature-9623756/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models)。\n\n今天就到这里，明天见！👋\n\n如果您喜欢这个更新并希望了解 AI 的最新动态，请考虑在 Medium 上订阅 ***Get The Gist***，获取更多见解和分析。\n\n**想要更深入了解吗？** 订阅我们的免费每日电子邮件通讯，快速获取简洁的更新，确保您不会错过任何重要进展。您可以通过点击 [这里](https://getthegist.beehiiv.com/) 注册。\n\n让我们一起探索 AI 的世界——每次摘要都是一次新发现！💡🤖\n\n"},{"lang":"zh","group":"blog","slug":"blog/mojo-90-000-times-faster-than-python-finally-open-sourced-777bdd9a1896","frontmatter":{"title":"Mojo，比 Python 快 90,000 倍，终于开源了！","meta_title":"Mojo，比 Python 快 90,000 倍，终于开源了！","description":"2024年3月29日，Modular Inc.宣布Mojo核心组件开源。","date":"2024-11-10T22:36:54.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*jcayumihC6jn5q_0","categories":["Programming","Machine Learning","Data Science"],"author":"Rifx.Online","tags":["Mojo","Python","MLIR","SIMD","open-source"],"draft":false,"slug":"blog/mojo-90-000-times-faster-than-python-finally-open-sourced-777bdd9a1896"},"content":"\n2024年3月29日，Modular Inc.宣布开源Mojo的核心组件。\n\nMojo是一种专门为编写人工智能软件而设计的编程语言，去年八月正式发布。自那时以来，它已经吸引了超过175,000名开发者和50,000个组织。\n\n人工智能模型通常使用多种编程语言编写。开发者通常使用Python实现神经网络的最简单部分，因为它易于学习，但相对较慢。其余代码通常用C++编写，虽然速度更快，但学习起来更复杂。\n\nModular将Mojo定位为一种更方便的替代方案。它提供了类似Python的易用语法，但执行速度有可能快上千倍。因此，开发者可以编写快速的AI模型，而无需学习像C++这样复杂的语言。\n\n\n\n去年，当Mojo推出时，一些开发者对它的出现表示兴奋。然而，当被问及开源日期时，Chris Lattner在Discord上表示：“如果我知道，我会告诉你。”大约一年以来，许多开发者处于观察和质疑的状态：\n\n> “宣传很好，但如果不是开源的，我不会花时间去尝试。”\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*rIJiJylh4-mWBiqz)\n\n> “显然这是一个被过度炒作的编程语言，而且它不是开源的！Chris Lattner想要欺骗数百万Python开发者！”\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*0u5HDKseL0Gy_-8A)\n\n> “我无法在一个可能开源也可能不开源的语言上花时间，尤其是在当前的OSS商业环境下……”\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*wrTO7fbKfBZOpBxF)\n\n现在，Mojo终于开源了！在短时间内，它已经达到了17.6k颗星和2.1k个fork！\n\n## 01 Mojo开源之旅的第一步\n\nModular今天宣布开源Mojo标准库的核心组件。标准库构成了编程语言的核心部分，包含基本的语法元素和基本功能。Mojo的标准库包括优化AI超参数的功能，这些超参数决定了神经网络如何处理数据。\n\n“Mojo标准库仍在进行激烈的开发和快速变化，因此我们首先开源其核心模块。这标志着我们开源之旅的重要起点，而不是结束。”\n\n该公司表示，开源将使他们能够从更多开发者那里收集反馈，从而促进Mojo的更好开发。此外，开源项目有多种方式：有些项目提供源代码但不接受贡献；有些则提供不透明的贡献流程，使得理解目标和路线图变得困难；还有一些虽然开源，但并未得到积极维护。Modular表示，他们选择了一种更全面的开源方式：通过GitHub拉取请求允许外部贡献，鼓励开发者参与Mojo的开发和改进，并促进社区的成长。\n\n此外，Modular通过分享完整的提交历史，展示了诚意，从初始提交开始！公开修订开源标准库的历史使开发者能够跟踪代码的演变，更好地理解其含义。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*0-FqkfLUTevloPjI)\n\n此外，他们将发布Mojo编译器的夜间构建，方便开发者快速尝试最新的编译器功能并进行持续集成测试。\n\n去年年底，Modular推出了商业AI平台MAX，这是一个用于构建高性能AI应用的统一工具和库集，可以高效地部署在多个硬件平台上，例如在Kubernetes环境中运行AI应用。今天，该公司透露，他们还计划在未来开源MAX的一些组件。\n\n此外，值得一提的是，他们选择了Apache 2 LLVM许可证进行开源。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*dgVCSxaCq6onY2uP)\n\n这是Apache 2许可证的定制版本。此外，为了便于与遵循GPL2许可证的软件集成，Modular进行了相应的调整。GPL2是另一种流行的开源许可证，著名地用于Linux内核等项目。在公告博客中，Modular写道：\n\n> “Apache 2许可证是一个良好的起点，但我们在LLVM项目中使用许可证的经验告诉我们，它有两个小问题。有人担心Apache 2许可证可能与GPL2代码（例如Linux内核）不兼容，并且Apache 2许可证要求您在派生项目中承认代码的使用。我们希望您能够使用Mojo，而不必强制承认Modular或Mojo。因此，我们添加了LLVM特别设计的例外条款，以解决这些问题。”\n\n## 02 在未来50年中，AI编程的最佳语言是什么？\n\n去年5月，当Mojo刚刚发布时，Modular声称它在运行Mandelbrot等算法时比原始Python快35,000倍。\n\n去年9月，Modular再次表示：“Mojo结合了动态语言和静态语言的优点，性能提升至Python的68,000倍。”\n\n去年10月，当Mojo在Mac上发布时，Modular再次提高了性能比较数据：“比Python快90,000倍。”\n\n谈到Mojo，Modular的创始人兼首席执行官Chris Lattner表示：“你可以把Mojo看作Python家族的一员，借鉴了所有这些酷炫的语言、编译器和其他技术，使Python向前迈出了一大步。我们相信它增强了Python的能力，赋予Python程序员超能力，使熟悉Python的人能够学习新知识，探索和征服新领域，而无需切换到C++。”\n\nMojo基于MLIR中的最新编译器技术，这是LLVM的演变，因此性能更佳。只要程序员具备必要的技能并愿意充分优化，他们就可以让代码运行得极快。Mojo语言的目标是满足Python开发者的需求，同时提供一系列新的代码优化技术，以充分利用硬件设备的性能极限。\n\n另一方面，Mojo团队高度赞赏Rust，并公开表示“Mojo的设计也受到Rust的极大启发。”\n\n在性能方面，Modular进行了许多与Python的比较，以提供明确的对比，但人们并没有概念它比Rust快多少。就在上个月，他们专门回应了“Mojo是否比Rust快”的问题。\n\n今年2月，Netflix工程师和Rust倡导者@ThePrimeagen发布了一段视频：用Mojo解析DNA序列，速度超过Rust 50%。这篇博客引发了很多关注和讨论，毕竟Rust被视为Python和C++在AI领域的潜在继任者。\n\n@ThePrimeagen对Mojo和Rust在AI编程中的展望：\n\n> 如果Mojo正式加入竞争，那么我相信Mojo无疑会胜出。Mojo获胜的原因在于，它不需要对开发者已经熟悉的范式进行任何改变。只需稍加学习，就能实现惊人的性能。首先，Mojo编译速度快，用户体验与大家已经熟悉的语言非常相似，性能可与Rust媲美。唯一的问题是如何让更多人接受它。\n\n在发表评论后，受人尊敬的Rust贡献者及《Rust: From Zero to Production》的作者Luca Palmieri在X上回应：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Hqe7bPWGI36LPGzE)\n\nRust在系统编程领域拥有顶尖的设计，但在AI应用领域面临两个主要问题：\n\n* 编译速度慢，而AI强调实验和快速迭代。\n* 大多数有Python经验的AI研究人员不愿意花时间从零开始学习一门新语言。\n\nMojo旨在使Python开发者能够直观且轻松地掌握。正如Mohamed所示，他在几周内作为一个业余项目学习了Mojo，并利用SIMD优化算法（初始实现仅需200行代码）。\n\n对于那些对AI开发感兴趣的人来说，确实存在在三种可用语言中选择其一的困境。\n\nMojo和Rust都允许开发者在更低的层面进行优化。对于Rust，开发者当然可以将所有内容打包到Arc、Mutex或Box中，以避免与借用检查器的冲突，但这可能会牺牲一些性能。虽然这种性能差异可能对应用代码没有显著影响，但在库或其他性能敏感代码中可能会迅速累积。两者的选择取决于程序员对减少开销和优化性能的关注。\n\n这两种语言都可以利用LLVM进行代码生成优化，并允许使用内联汇编（尽管实际上不太可能有人这样做），因此理论上，两者在传统硬件上的性能潜力相似。\n\n## 03 基于最先进的编译器技术\n\nRust 于 2006 年启动，而 Swift 于 2010 年出现，两者主要基于 LLVM IR 构建。而 Mojo 则在 2022 年首次亮相，构建于 MLIR 之上——与 Rust 使用的 LLVM IR 相比，MLIR 是一个更现代的“下一代”编译器栈。值得注意的是，Chris Lattner 在 2000 年 12 月大学时期创立了 LLVM，并从其多年的演变中学习了很多。他后来加入 Google 领导 MLIR 的开发，旨在支持公司的 TPU 和其他 AI 加速器项目。随后，他继续基于从 LLVM IR 中获得的知识进行探索。\n\nModular 表示 Mojo 是第一个充分利用 MLIR 高级特性的编程语言。它可以生成具有更高优化的 CPU 代码，并且还支持 GPU 和其他加速器，速度比 Rust 快得多。这是目前其他语言无法实现的优势，也是 AI 和编译器爱好者对 Mojo 热情的核心原因。\n\n他们特别强调两个方面：\n\n出色的 SIMD 人体工程学设计：CPU 通过特殊寄存器和指令同时处理多个数据元素，称为 SIMD（单指令多数据）。然而，从历史上看，编写此类代码的体验一直很糟糕，并且在人体工程学方面很难使用。尽管这些特殊指令已经存在多年，但大多数代码并未针对它们进行优化。因此，谁能解决这种复杂性并编写可移植的 SIMD 优化算法，谁就能在市场中脱颖而出，例如 simd_json。\n\nMojo 的原语从一开始就以 SIMD 为优先设计：UInt8 实际上是 SIMD\\[DType.uint8, 1]，表示一个元素的 SIMD。这种表示不会带来性能开销，同时允许程序员轻松地将其用于 SIMD 优化。例如，文本可以被拆分为 64 字节的块，表示为 SIMD\\[DType.uint8, 64]，然后与单个换行符进行比较，以找到每个换行符的索引。由于机器上的 SIMD 寄存器可以同时对 512 位数据执行操作，因此此操作可以将此类操作的性能提升 64 倍！\n\n或者给出一个更简单的例子，假设你有一个 SIMDDType.float64, 8。只需将其乘以 Float64(2)，你就可以轻松提高性能。与逐个乘以每个元素相比，这种方法可以在大多数机器上将性能提高多达 8 倍。\n\nLLVM（Rust 也在使用）具有自动向量化优化通道，但由于其无法更改 SIMD 的内存布局和其他重要细节，其性能从未达到理论优化水平。然而，Mojo 从一开始就考虑了 SIMD 特性，因此编写 SIMD 优化的体验与编写常规代码非常相似。\n\n急切销毁：Rust 的设计受到 C++ 的 RAII（资源获取即初始化）启发，这意味着一旦对象超出作用域，应用程序开发人员不需要担心释放内存——编程语言本身会处理这一点。这是一个很好的例子，避免了垃圾回收的性能陷阱，同时确保了动态语言的人体工程学。\n\nMojo 更进一步，不是等到作用域结束，而是在对象最后一次使用时释放内存。这对于 AI 场景非常有益，因为提前释放对象意味着提前释放 GPU 张量，从而允许在等效的 GPU RAM 中适应更大的模型。这是 Mojo 的独特优势，使程序员能够在不必自己设计的情况下实现最佳性能。Rust 的借用检查器最初将所有事物的生命周期延长到其作用域的结束，匹配析构函数的行为，但这可能会给用户带来一些困惑的后果。Rust 后来添加了一些非词法生命周期特性，以简化开发人员的工作。然而，通过 Mojo 的急切析构机制，可以直接实现这种简化效果，并且与对象实际销毁的方式保持一致，从而避免令人困惑的极端情况。\n\nRust 另一个开销来自 Drop 的实现。它使用 Drop Flags 来跟踪对象是否应该在运行时被删除。Rust 能够在某些情况下进行优化，但 Mojo 可以通过显式定义消除所有额外开销。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*0VcMppg3rDTqfsMY)\n\n无论如何，开发人员必须在 Mojo 和 Python 的易用性，以及 C、C++ 或 Rust 的高性能之间做出选择。对此，Mojo 团队呼吁开发人员：“如果你对未来充满好奇，希望掌握一种可能在未来 50 年内促进 AI 发展的语言，为什么不试试 Mojo 呢？”\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/multi-agent-hedge-fund-simulation-with-langchain-and-langgraph-64060aabe711","frontmatter":{"title":"利用 LangChain 和 LangGraph 进行多代理对冲基金模拟","meta_title":"利用 LangChain 和 LangGraph 进行多代理对冲基金模拟","description":"本项目演示了如何使用多代理设置来模拟对冲基金的分析流程。它展示了一种实用的方法来...","date":"2024-11-10T03:51:17.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*i8wneK22YezD7zOhPKvZfg.png","categories":["Finance","Programming","Data Science"],"author":"Rifx.Online","tags":["multi-agent","LangChain","LangGraph","FinancialDatasets","predictive"],"draft":false,"slug":"blog/multi-agent-hedge-fund-simulation-with-langchain-and-langgraph-64060aabe711"},"content":"\n### 多智能体对冲基金模拟与 LangChain 和 LangGraph\n\n\n\n该项目演示了如何使用多智能体设置来模拟对冲基金的分析过程。它展示了一种实用的方法来构建一个系统，该系统利用 AI 智能体收集和分析金融数据，这种设置可以进一步扩展和定制。在这里，我将分解该项目，其中涉及一个投资组合经理和三个分析师智能体（基本面、技术面和情绪面），每个智能体在收集和处理股票数据方面被分配了特定角色。\n\n该项目的目标不是构建一个全面的交易算法，而是说明如何使用 LangChain 和 LangGraph 组织和并行分析各种类型的数据，利用专业的智能体。\n\n### 项目结构和代理概述\n\n该代理系统包括：\n\n1. **Portfolio Manager** — 将任务委派给分析师并汇总他们的发现。\n2. **Fundamental Analyst** — 获取和分析财务报表，例如利润表。\n3. **Technical Analyst** — 收集指定时间范围内的股票价格数据。\n4. **Sentiment Analyst** — 关注内部交易和新闻数据，提供情绪洞察。\n\n每个代理都旨在专注于特定的数据检索任务，从而实现模块化和可扩展的分析。通过使用 LangChain 实现代理功能和 LangGraph 管理并行工作流，我们可以快速处理多个数据源。FinancialDatasets API 提供了丰富的数据来源，拥有超过 30,000 个股票代码，使得全面分析成为可能。\n\n### 关键库和设置\n\nLangChain 和 LangGraph 使得多智能体工作流和并行处理的分支逻辑处理变得简单。设置开始于安装所需的库并获取 API 密钥：\n\n```python\n%%capture --no-stderr\n%pip install -U langgraph langchain langchain_openai langchain_experimental langsmith pandas\n```\n环境变量用于存储敏感数据，例如 API 密钥：\n\n```python\nimport getpass\nimport os\n\ndef _set_if_undefined(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"Please provide your {var}\")\n\n_set_if_undefined(\"OPENAI_API_KEY\")               # https://platform.openai.com\n_set_if_undefined(\"FINANCIAL_DATASETS_API_KEY\")   # https://financialdatasets.ai\n_set_if_undefined(\"TAVILY_API_KEY\")               # https://tavily.com\n```\n\n### 代理功能：检索数据\n\n系统中的每个代理都旨在处理与股票分析相关的特定类型数据。\n\n### 1\\. 基本面分析师\n\n基本面分析师获取并检查财务报表，这些报表提供了公司财务健康状况的洞察。以下是获取收入报表的工具，这是一个关键的财务文件：\n\n```python\nfrom langchain_core.tools import tool\nfrom typing import Dict, Union\nfrom pydantic import BaseModel, Field\n\nclass GetIncomeStatementsInput(BaseModel):\n    ticker: str = Field(..., description=\"The ticker of the stock.\")\n    period: str = Field(default=\"ttm\", description=\"Valid values are 'ttm', 'quarterly', or 'annual'.\")\n    limit: int = Field(default=10, description=\"Maximum number of income statements to return.\")\n\n@tool(\"get_income_statements\", args_schema=GetIncomeStatementsInput, return_direct=True)\ndef get_income_statements(ticker: str, period: str = \"ttm\", limit: int = 10) -> Union[Dict, str]:\n    api_key = os.environ.get(\"FINANCIAL_DATASETS_API_KEY\")\n    url = f'https://api.financialdatasets.ai/financials/income-statements?ticker={ticker}&period={period}&limit={limit}'\n    try:\n        response = requests.get(url, headers={'X-API-Key': api_key})\n        return response.json()\n    except Exception as e:\n        return {\"ticker\": ticker, \"income_statements\": [], \"error\": str(e)}\n```\n在这里，`get_income_statements` 用于获取给定股票代码的收入报表。通过指定期间（例如，“ttm”表示过去十二个月），代理可以专注于不同的报告周期。\n\n### 2\\. 技术分析师\n\n技术分析师收集在定义时间范围内的股票价格数据。这些数据可以用于计算指标或识别模式。以下是检索股票价格的代码：\n\n```python\nclass GetPricesInput(BaseModel):\n    ticker: str\n    start_date: str\n    end_date: str\n    interval: str = \"day\"\n    interval_multiplier: int = 1\n    limit: int = 5000\n\n@tool(\"get_stock_prices\", args_schema=GetPricesInput, return_direct=True)\ndef get_stock_prices(ticker: str, start_date: str, end_date: str, interval: str, interval_multiplier: int = 1, limit: int = 5000) -> Union[Dict, str]:\n    api_key = os.environ.get(\"FINANCIAL_DATASETS_API_KEY\")\n    url = (\n        f\"https://api.financialdatasets.ai/prices?ticker={ticker}\"\n        f\"&start_date={start_date}&end_date={end_date}\"\n        f\"&interval={interval}&interval_multiplier={interval_multiplier}\"\n        f\"&limit={limit}\"\n    )\n    try:\n        response = requests.get(url, headers={'X-API-Key': api_key})\n        return response.json()\n    except Exception as e:\n        return {\"ticker\": ticker, \"prices\": [], \"error\": str(e)}\n```\n该函数允许我们指定日期范围和时间间隔等参数，从而控制数据的粒度（例如，按日或按小时）。\n\n### 3\\. 情绪分析师\n\n情绪分析师收集内部交易和相关新闻的数据。内部交易和公众情绪指标可以提供市场感知的洞察，这对于评估股票波动性和潜在价格变动非常重要。\n\n```python\nclass GetInsiderTradesInput(BaseModel):\n    ticker: str\n    limit: int = 10\n\n@tool(\"get_insider_trades\", args_schema=GetInsiderTradesInput, return_direct=True)\ndef get_insider_trades(ticker: str, limit: int = 10) -> Union[Dict, str]:\n    api_key = os.environ.get(\"FINANCIAL_DATASETS_API_KEY\")\n    url = f'https://api.financialdatasets.ai/insider-transactions?ticker={ticker}&limit={limit}'\n    try:\n        response = requests.get(url, headers={'X-API-Key': api_key})\n        return response.json()\n    except Exception as e:\n        return {\"ticker\": ticker, \"insider_transactions\": [], \"error\": str(e)}\n```\n通过捕获内部交易，该工具可以跟踪拥有特权信息的人的操作，这可能是绩效变化的早期指标。\n\n### 投资组合经理：协调和总结分析\n\n投资组合经理作为协调者，将任务分配给分析师，并将他们的结果汇总成一份报告。以下是投资组合经理的示例工作流程，展示了它如何调用每个代理：\n\n```python\nfrom langchain_community.tools.tavily_search import TavilySearchResults\n\n## Tools grouped by agent type\nfundamental_tools = [get_income_statements]\ntechnical_tools = [get_stock_prices]\nsentiment_tools = [get_insider_trades, TavilySearchResults(max_results=5)]\n\n## Sample function for running all analyses in parallel\ndef analyze_portfolio(ticker: str):\n    # Delegate tasks to each agent\n    fundamentals = [tool(ticker=ticker) for tool in fundamental_tools]\n    prices = [tool(ticker=ticker, start_date=\"2023-01-01\", end_date=\"2023-12-31\") for tool in technical_tools]\n    sentiment = [tool(ticker=ticker) for tool in sentiment_tools]\n    \n    # Summarize results (simplified)\n    summary = {\n        \"fundamentals\": fundamentals,\n        \"technical\": prices,\n        \"sentiment\": sentiment\n    }\n    return summary\n```\n在这个函数中：\n\n* 每个代理的函数并行调用，以收集指定股票代码的数据。\n* 然后，经理将来自每个代理的数据汇总成一个简明的摘要，以便于审阅。\n\n### 结论\n\n本项目提供了一个基本但灵活的设置，通过一组专业代理分析股票数据。通过将任务分配给投资组合经理、基本面分析师、技术分析师和情绪分析师，我们能够在不同的金融数据类型中收集和组织见解。使用 LangChain 和 LangGraph 实现模块化和并行处理，使这种方法具有可扩展性，而金融数据集 API 则支持广泛的股票代码，能够实现强大的数据访问。\n\n虽然该系统被设计为一个实践项目，但其结构可以作为更复杂的对冲基金模拟或数据分析工具的基础。下一步可能包括为每个代理增强更多工具或数据分析技术，例如：\n\n* **技术模式和指标：** 整合更多技术分析工具，如移动平均线或趋势线。\n* **情绪评分：** 从新闻来源或内部交易数据自动化情绪评分。\n* **预测建模：** 添加可以根据综合数据做出买卖建议的机器学习模型。\n\n该设置是一个有用的模块化金融数据分析原型，未来还有很多定制和改进的空间。\n\n对于那些对这个工具包背后的代码感兴趣的人，您可以在 GitHub 上找到完整的实现 [*这里*](https://github.com/shaikhmubin02/ai-hedge-fund)。\n\n"},{"lang":"zh","group":"blog","slug":"blog/multimodal-rag-with-gemini-pro-and-langchain-e4f74170420a","frontmatter":{"title":"使用 Gemini Pro 和 LangChain 的多模式 RAG","meta_title":"使用 Gemini Pro 和 LangChain 的多模式 RAG","description":"介绍","date":"2024-11-08T00:41:44.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*m2C8wrrRvhELuDiYLv4YYQ.png","categories":["Programming","Machine Learning","Computer Vision"],"author":"Rifx.Online","tags":["Gemini","LangChain","RAG","Vertex","sneaker"],"draft":false,"slug":"blog/multimodal-rag-with-gemini-pro-and-langchain-e4f74170420a"},"content":"\n\n\n## 介绍\n\n在本教程中，我们将探索将 [Gemini](https://deepmind.google/technologies/gemini/#introduction) Pro 和 Gemini Pro Vision 与 [LangChain](https://www.langchain.com/langchain) 框架集成，以实现多模态（在这种情况下为图像）检索增强生成（RAG）。这个简短的教程适合初学者和经验丰富的从业者，不仅以 Google [AI Studio](https://aistudio.google.com/) 作为主要环境奠定基础，还无缝过渡到演示如何使用 [Google Cloud’s Vertex AI](https://cloud.google.com/vertex-ai) 适应和进一步增强这些实现。\n\n## 设置环境\n\n首先，我们需要设置我们的环境，以确保我们拥有所有必要的工具和库。\n\n为此，我们需要 Langchain、Langchain Google Gen AI 包以及用于 RAG 的向量存储包，如下所示：\n\n```python\npip install — upgrade langchain langchain-google-genai “langchain[docarray]” faiss-cpu\n```\n\n然后，您还需要提供 Google AI Studio API 密钥，以便模型进行交互：\n\n```python\nif \"GOOGLE_API_KEY\" not in os.environ:\n  os.environ[“GOOGLE_API_KEY”] = getpass.getpass(“Provide your Google API Key”)\n```\n\n为了方便使用，我还写了一个简单的函数，显示我正在使用的图像。这个函数简单地从提供的 URL 下载图像并显示预览：\n\n```python\ndef get_image(url, filename):\n  content = requests.get(url).content\n  with open(f'/content/{filename}.png', 'wb') as f:\n  f.write(content)\n  image = Image.open(f\"/content/{filename}.png\")\n  image.show()\n  return image\n```\n\n## 简单的 LLM 交互\n\n让我们从一个非常简单的 LLM 交互开始。为此，我们可以简单地调用 ChatGoogleGenerativeAI 的 Gemini Pro 模型，并调用，如下所示：\n\n```python\nllm = ChatGoogleGenerativeAI(model=”gemini-pro”)\nresult = llm.invoke(\"Write a ballad about Gemini Pro in around 3 sentences.\")\nprint(result.content)\n```\n\n结果你会得到类似这样的内容：\n\n> 在星辰的领域，Gemini Pro 闪耀， 一道天体的灯塔，划定了界限， 指引着观星者穿越宇宙的设计。\n\n同样，你也可以在聊天界面中使用它，采用系统、人类消息/对话格式，如下所示：\n\n```python\nmodel = ChatGoogleGenerativeAI(model=”gemini-pro”, convert_system_message_to_human=True)\nprint(model([\n  SystemMessage(content=\"Answer only yes or no.\"),\n  HumanMessage(content=\"Is apple a fruit?\"),\n  ]).content)\n```\n\n## 多模态 LLM\n\n在本教程中，我将使用一个非常简单的用例，假设我是一名运动鞋爱好者，基本上想要找到在附近的本地商店购买特定运动鞋型号的方法。为此，我准备了一个虚拟知识库，里面包含了一些关于本地商店的虚假信息，以及某些流行运动鞋品牌的规格。有趣的是，这个知识库也是通过 Gemini Pro 使用 [Google Gemini](https://gemini.google.com/) 聊天界面生成的。\n\n让我们从一张示例图片开始：\n\n```python\nimage = get_image(<image_url>, “nike3”)\nplt.imshow(image)\nplt.show()\n```\n\n作为示例，我考虑这张 [Nike](https://nike.com/) 运动鞋的图片。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*dNFF95lOu1SeYHOn1vFnQQ.png)\n\n现在，让我们调用 Gemini Pro Vision 模型，询问它关于这张特定图片的一些信息。为此，您只需将模型名称更改为 *“gemini\\-pro\\-vision”*。\n\n```python\nllm = ChatGoogleGenerativeAI(model=”gemini-pro-vision”)\nmessage = HumanMessage(\ncontent=[\n  {\n    \"type\": \"text\",\n    \"text\": \"What's in this image? provide full detail as possible.\",\n  }, # You can optionally provide text parts\n  {\"type\": \"image_url\", \"image_url\": image},\n])\nprint(\nllm.invoke([message]).content\n)\n```\n\n您将得到如下输出：\n\n> 这是一个 Nike Air Max 95 运动鞋的产品图片，颜色为棕色小麦色。鞋面由网布和麂皮制成，带有皮革泥挡。中底由泡沫材料制成，后跟有可见的气垫单元。外底由橡胶制成，具有华夫格图案以增强抓地力。\n\n*免责声明：所提供的描述可能不准确，反映的是模型对图像的解读，而非与之相关的事实信息。*\n\n## 使用多模态的RAG\n\n现在，让我们深入了解如何使用这种多模态方法执行RAG。首先，让我们为这个RAG创建一个信息源。为此，我写了一些关于几款Nike运动鞋的段落信息，以及一些虚构的尼泊尔本地商店位置。\n\n```python\nstore_information = “Nike Air Max Plus sneakers. They feature a brown upper with a black Nike Swoosh logo on the side and a visible Air Max unit in the heel. The sole is white.\nHere are some more details about the Nike Air Max Plus:\nStyle: TN\nRelease date: January 1, 2017\nStyle code: 852630–300\nOriginal retail price: $150 USD\nThe Air Max Plus, also known as the TN, is a popular Nike running shoe that was first released in 1998. It is known for its unique design, which includes a gradient upper, visible Air Max units, and a wavy outsole. The TN has been a popular shoe among sneakerheads and casual wearers alike for over two decades.\nIt features a brown upper with a black Swoosh logo and a white sole. The shoe is currently available for resale on the StockX marketplace for an average price of around $150 USD.\nNike Air Max Plus Store Location: \"Kings Way, Kathmandu, Nepal\n\n...\n\n\"\n```\n\n然后，让我们创建一个Langchain链，它基本上根据我们知识库中提供的图像描述获取关于Nike模型的信息以及可以在哪里购买它。\n\n```python\nllm_text = ChatGoogleGenerativeAI(model=”gemini-pro”)\ntemplate = \"\"\"\n```\n\n{context}\n\n```\n{information}\nProvide brief information and store location.\n\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\nrag_chain = (\n  {\"context\": retriever, \"information\": RunnablePassthrough()}\n  | prompt\n  | llm_text\n  | StrOutputParser()\n)\n```\n\n这里需要注意的是*Gemini\\-Pro*和*Gemini\\-Pro\\-Vision*是两个不同的模型，您需要以不同的方式调用它们。在上面的代码中，我们调用了Gemini Pro文本模型，该模型根据由*gemini\\-pro\\-vision*模型生成的图像描述执行RAG。\n\n现在，让我们设置一个完整的链，它首先生成图像描述，然后使用上述链进行RAG。\n\n```python\nllm_vision = ChatGoogleGenerativeAI(model=”gemini-pro-vision”, temperature=0.0)\nfull_chain = (\n  RunnablePassthrough() | llm_vision | StrOutputParser() | rag_chain\n)\n```\n\n## 执行 RAG\n\n现在，让我们对刚刚设置的内容进行一些测试。首先，获取另一张图像作为样本\n\n```python\nimage = get_image(url_3, “nike3”)\nplt.imshow(image)\nplt.show()\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kPkfo2FKnrUR2tC18VMpjg.png)\n\n然后，让我们调用我们的 RAG：\n\n```python\nmessage = HumanMessage(\n  content=[\n    {\n      \"type\": \"text\",\n      \"text\": \"提供有关给定运动鞋的品牌和型号的信息。\",\n    }, # 您可以选择性地提供文本部分\n    {\"type\": \"image_url\", \"image_url\": image},\n  ])\n```\n\n现在让我们看看我们得到了什么：\n\n```python\nresult = full_chain.invoke([message])\ndisplay(Markdown(result))\n```\n\n作为输出，我们将得到类似于以下内容的结果，这基于我们的虚构信息来源：\n\n> **Nike Offcourt Slide**软质一体式鞋面舒适的泡沫中底耐用的橡胶外底提供多种颜色选择\n\n> **商店位置：** 尼泊尔，巴克塔布尔\n\n## 使用 Vertex AI 模型\n\n除了使用 Google AI Studio 模型外，您还可以使用 Google Cloud 的 Vertex AI Gemini Pro 模型。为此，您首先需要为您的云环境安装与 Vertex AI 相关的包和 Langchain，如下所示：\n\n```python\npip install — upgrade google-cloud-aiplatform langchain-google-vertexai\n```\n\n然后，使用以下命令设置与您的云项目相关的必要配置：\n\n```python\ngcloud init\n```\n\n接下来，您可以将 Vertex AI 模型用于多模态用例，如下所示：\n\n```python\nfrom langchain_google_vertexai import VertexAI\nfrom langchain_google_vertexai import VertexAIEmbeddings\n\nmodel_vision = VertexAI(model_name=\"gemini-1.0-pro-vision-001\")\nmodel_text = VertexAI(model_name=\"gemini-1.0-pro-001\")\n```\n\n## 结论\n\n在这个简短的教程中，我们探讨了如何将 Gemini Pro 和 Gemini Pro vision 与 LangChain 结合使用，以实现多模态 RAG 应用程序。\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/o1-preview-vs-claude-3-5-sonnet-comparing-top-llms-d68734b53c93","frontmatter":{"title":"o1-preview 与 claude-3.5-sonnet：比较顶级法学硕士","meta_title":"o1-preview 与 claude-3.5-sonnet：比较顶级法学硕士","description":"了解 OpenAI 的 o1 预览版与 Claude 3.5 Sonnet 在性能、速度和功能方面的比较。","date":"2024-10-27T13:58:01.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kTWAcpRdOpsrFIDZjjjr7Q.jpeg","categories":["Programming","Machine Learning","Generative AI"],"author":"Rifx.Online","tags":["o1-preview","Claude","throughput","latency","reasoning"],"draft":false,"slug":"blog/o1-preview-vs-claude-3-5-sonnet-comparing-top-llms-d68734b53c93"},"content":"\n\n\n今天（2024年9月12日），OpenAI 发布了其最新的语言模型 o1-preview。这个先进的模型经过设计，能够在生成响应之前投入更多时间进行处理，使其能够更好地应对复杂任务，并在科学、编码和数学等领域解决具有挑战性的问题。\n\n在这篇博客文章中，我们将深入分析 o1-preview，并将其与之前被认为是最先进模型之一的 Claude 3.5 Sonnet 进行比较。\n\n\n\n## 比较方法论\n\n我们的分析利用了 [Keywords AI 的 LLM playground](https://docs.keywordsai.co/features/prompt/model-playground)，这是一个支持超过 200 种语言模型并提供函数调用功能的平台。我们将探讨以下几个方面：\n\n* 基本比较\n* 基准比较\n* 处理速度\n* 评估指标\n* 建议的使用案例\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*yc171ikejtBy_o11.jpeg)\n\n## 基本比较\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*z2FrS_AVig7Y6eU_.jpeg)\n\n注意：o1-preview 不支持流式传输、函数调用和系统消息。\n\n## 基准比较\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Bx_vAvFc9DAD0cZA.jpeg)\n\nO1-preview 在所有基准测试中都优于 Claude 3.5 Sonnet。最小的差距出现在 MMLU（一般知识）中。GPQA Diamond 测试研究生水平的推理，显示出显著的性能差异。MATH 基准揭示了最大的差距，突显了 o1-preview 的高级数学能力。这些结果表明，o1-preview 在复杂推理和各个领域的问题解决方面有了显著改善。\n\n## 速度比较\n\nO1-preview 的思考和响应时间比其他 LLM 更长。虽然直接的速度比较可能并不完全公平，但测试 o1-preview 的速度至关重要。这些信息帮助开发者更好地理解 o1-preview 的能力，并判断它是否适合他们的项目。注意：由于 o1-preview 不支持流式传输，我们已为两个模型禁用流式传输。因此，无法测量首次令牌时间（TTFT）。\n\n## 延迟\n\n我们的测试涉及每个模型数百个请求，揭示了显著的差异。Claude 3.5 Sonnet 的平均延迟为 18.3 秒/请求，而 o1-preview 的平均延迟为 39.4 秒/请求。o1-preview 显著更长的延迟是由于其延长的思考和推理过程。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*2PMkgPVuylFxwfIa.jpeg)\n\n## 吞吐量（每秒令牌数）\n\n尽管延迟较高，o1-preview的吞吐量更为出色。o1-preview生成92.94个令牌/秒，而Claude 3.5 Sonnet生成74.87个令牌/秒。这表明o1-preview较长的生成时间主要是由于其初始处理阶段，而非令牌生成速度。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*wxqpnwZhl9pnbw8y.jpeg)\n\n## 性能比较\n\n我们在[Keywords AI平台](https://keywordsai.co/)上进行了评估测试。评估包括三个部分：\n\n* **编码任务**：两个模型成功完成了前端和后端开发任务。O1-preview在处理较长上下文时表现更佳，能够在第一次尝试中更有效地识别和解决bug。它还展现了更全面的代码分析能力。\n* **逻辑推理**：O1-preview在推理任务中表现出色。它的思维过程与人类认知非常相似。虽然Claude 3.5 Sonnet在大多数问题上表现良好，但o1-preview始终能够解决复杂的推理挑战，包括国际数学奥林匹克（IMO）级别的问题。\n* **写作任务**：两个模型在写作任务上表现非常出色。它们展现了撰写真实、个性化的冷邮件以及简洁且有意义的博客文章的能力。\n\n## 模型推荐\n\no1-preview\n\n* **最佳选择：** 适用于数学、编码和物理学中的复杂问题解决。特别适合处理挑战性任务的研究人员。\n* **不适合：** 需要快速响应时间或严重依赖系统提示的AI应用。由于缺乏流媒体支持，不适用于语音AI应用。\n\nClaude 3.5 Sonnet\n\n* **最佳选择：** 适用于大多数需要问题解决能力和高质量内容生成的AI应用。\n* **不适合：** 语音AI应用或对预算限制严格、需要较低运营成本的项目。\n\n## 如何将 o1-preview 集成到您的 AI 应用中\n\n要将 o1-preview 集成到您的 AI 应用中，只需访问 Keywords AI 模型页面并找到“查看代码”按钮。点击此按钮以复制提供的代码片段，然后将其直接粘贴到您的代码库中。通过这个简单的过程，您将能够在项目中利用 o1-preview 的强大功能，使您能够轻松应对复杂问题并生成高质量内容。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*XyQ9QiI7TN8Uc5Jp.jpeg)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*t8fEYlEs13eM7D28lVbtIw.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*yhu9y5ixNuxeFVe1.png)\n\n此故事发布在 [Generative AI](https://generativeai.pub/)。请在 [LinkedIn](https://www.linkedin.com/company/generative-ai-publication) 上与我们联系，并关注 [Zeniteq](https://www.zeniteq.com/)，以便获取最新的 AI 资讯。\n\n订阅我们的 [newsletter](https://www.generativeaipub.com/) 和 [YouTube](https://www.youtube.com/@generativeaipub) 频道，及时了解生成 AI 的最新消息和动态。让我们共同塑造 AI 的未来！\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*PelNtaNaEVDWgMWr.png)\n\n"},{"lang":"zh","group":"blog","slug":"blog/openai-01-preview-secrets-99-of-people-dont-know-b0c5e4bb4f76","frontmatter":{"title":"OpenAI 01-预览 — 99% 的人不知道的秘密","meta_title":"OpenAI 01-预览 — 99% 的人不知道的秘密","description":"如何充分利用 01-preview","date":"2024-11-01T03:58:01.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*wRAXNmhEzkGNagMl5Papxg.jpeg","categories":["Programming","Machine Learning","Technology/Web"],"author":"Rifx.Online","tags":["OpenAI","01-preview","iterative","problem-solving","planning"],"draft":false,"slug":"blog/openai-01-preview-secrets-99-of-people-dont-know-b0c5e4bb4f76"},"content":"\n### 如何充分利用01\\-preview\n\n自从01\\-preview发布以来，我一直在玩它。\n\n我非常喜欢它！\n\n我甚至在我的新[**AI增长黑客课程**](https://aigrowthguys.com/growth-hacking-course-sign-up/)中教授它。\n\n我很高兴分享一些关于如何充分利用它的关键见解。\n\n\n\n大多数人对01\\-preview的工作原理一无所知。\n\n首先，它不仅仅是一个“思考”模型。\n\n在您能够充分利用它之前，您需要了解一些它的工作原理。\n\n如果您没有付费的Medium账户，可以在[**这里**](https://readmedium.com/openai-01-preview-secrets-99-of-people-dont-know-b0c5e4bb4f76?sk=12140ffad09d922bc00a8a4aa312a286)免费阅读。\n\n👉 注册我们的免费5天电子邮件课程，助力成长🚀并在AI时代赚取💲\n\n## OpenAI 01\\-preview 如何工作？\n\n01\\-preview 并不是真正的新模型。\n\n它结合了其他模型和一个“系统提示”，告诉它在输出响应之前进行多次迭代。\n\n所有其他模型都是通过提供模型想到的第一个响应来工作的。\n\n01\\-preview 的设计目的是在最终答案出来之前进行规划和实验。\n\n一个例子会有所帮助。\n\n> 想象一下，你告诉 GPT\\-4o 写一个恰好 80 个单词的连贯段落，并且“tomato”这个词是第 4 个、第 19 个和第 72 个单词。\n\nGPT\\-4o（以及所有其他模型）在这个任务上会失败，因为仅仅吐出第一个想到的答案太困难了。\n\n这种类型的问题需要实验。\n\n想想如果你被赋予了同样的任务。\n\n你需要“玩弄”这个任务，试图将“tomato”这个词放在这些位置上，以一种合理的方式。\n\n你不能仅仅开始写作，然后看看会发生什么。\n\n你会意识到你需要调整一些句子和单词，以便将“tomato”这个词放进去。\n\n此外，当你接近 80 个单词时，你需要计划如何准确地停在这个数字上。例如，你可能希望回去删除第一句中的一个多余单词。\n\n01\\-preview 能够做到这一点的原因在于它的“思考”方式。\n\n它首先会将问题分解，并说一些类似于“想出一个解决这个问题的计划”的话。\n\n然后，它会写出一个大致的初步猜测（可能使用 GPT\\-4o）。\n\n接着，它会对自己说：“重新阅读问题，看看是否可以进行任何调整或修改”。\n\n然后它会说：“再检查一遍，看看你的回答是否完美。如果是，就展示出来；如果不是，继续调整”。\n\n然后它会说：“重复这个过程，直到你的答案 100% 完美。始终记得在展示最终答案之前进行双重检查”。\n\n例如，第一次响应的第一句可能是这样的。\n\n“Sandy picked a red tomato from her garden.”\n\n然后 01\\-preview 会将其更改为：“Sandy picked a tomato from her garden”。\n\n这样，它就成功地将“tomato”这个词从第 5 个单词移动到了第 4 个单词。\n\n它会通过与自己进行内部对话不断进行调整。\n\n## 如何充分利用 01\\-preview？\n\n现在您对 01\\-preview 的“思维”有了一定了解，可以开始理解如何充分利用它。\n\n您需要将自己的问题分为需要“思考”的问题和不需要的。\n\n许多问题不需要模型进行“思考”。\n\n例如，如果您告诉它为您写一个关于一个名叫 Sandy 的女孩和她的番茄花园的有趣故事，那么您就不需要使用 01\\-preview。\n\n**为什么不呢？**\n\n因为约束条件很少。\n\n有很多方法可以做到这一点。它本质上是开放式的。\n\n这个故事不需要有特定的长度。\n\n模型可以直接开始写，插入一两个笑话，然后就完成了。\n\n它不需要回到第一句去计算单词数量或其他任何事情。\n\n关键是：\n\n如果您向模型请求一些特定的内容，而这些内容在一次尝试中很难做到且需要实验，那么您应该使用 01\\-preview。\n\n如果您请求的是开放式的内容，那么使用其他模型。\n\n您需要谨慎使用 01\\-preview，因为您只能获得有限数量的查询。\n\n它的限制总是比其他模型更多，因为它使用的资源远远超过其他模型。\n\n好消息是，01\\-preview 会比其他模型犯更少的错误。\n\n此外，它能够回答之前模型无法解决的问题。\n\n现在是学习如何利用 AI 来发展您的业务和赚取更多收入的最佳时机。\n\n我在我的 AI 增长黑客课程中教授如何使用这个。\n\n我还将结合这个模型，使我构建的自定义 AI 代理和聊天机器人更加准确。\n\n这将使像 [**Stammer**](https://stammer.ai/?via=andrew) 这样的 AI 代理构建者更加强大。\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/openai-confirms-the-arrival-of-gpt-5-poised-to-bring-huge-improvements-to-artificial-intelligence-e3b858e79c2a","frontmatter":{"title":"OpenAI 确认 GPT-5 即将到来，有望为人工智能带来巨大改进……","meta_title":"OpenAI 确认 GPT-5 即将到来，有望为人工智能带来巨大改进……","description":"有网友在x上发了一篇GPT5倒计时的帖子，称这是根据各平台的线索得出的结论。评论区……","date":"2024-11-01T03:58:58.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8J_opnaERs-wrq2YRKIxdQ.png","categories":["Natural Language Processing","Generative AI","Technology"],"author":"Rifx.Online","tags":["GPT-5","natural","language","efficiency","personalization"],"draft":false,"slug":"blog/openai-confirms-the-arrival-of-gpt-5-poised-to-bring-huge-improvements-to-artificial-intelligence-e3b858e79c2a"},"content":"\n\n\n一位网友在 x 上发布了 GPT5 倒计时的帖子，称这是从各个平台的线索得出的结论。评论区已经达到高潮，各种意见纷纷涌现。\n\n\n\n**原因 1** : OpenAI 网站 GPT5 泄露\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*EBDLAv3rOyCjshGBpVRI7A.png)\n\n**原因 2** : 知名美国财经网站 BusinessInsider 发布的文章“OpenAI 发布更好的 GPT5 聊天机器人”。由于该网站为付费网站，感兴趣的话可以搜索标题。以下是部分内容：\n\n这家由 Sam Altman 领导的生成式 AI 公司，预计将在年中某个时候推出 GPT-5，可能在夏季，根据两位熟悉该公司的人士的说法。根据另一位熟悉该过程的人士的说法，一些企业客户最近收到了最新模型及其与 ChatGPT 工具相关的增强功能的演示。Business Insider 已确认这些人的身份，他们要求匿名以便能够自由发言。\n\n根据在 X 和其他平台上的讨论，新的模型版本很可能将在 6 月 6 日推出，但尚不确定是否为 GPT 4.5 或 GPT5。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*rhApTugfrMVBB6PhMvK4rg.png)\n\n我们都在等待 **GPT5**\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*eB6j2S_dPbjQ2-sV2N1fwA.jpeg)\n\n## GPT-5的期待\n\n尽管细节仍然稀少，但围绕GPT-5的兴奋感源于对人工智能能力显著提升的期待。以下是一些可能的进展猜测：\n\n* 增强的自然语言理解：预计GPT-5将对人类语言中的上下文、细微差别和微妙之处有更深入的理解，使互动更加流畅和自然。\n* 提高效率：随着每次迭代，OpenAI在减少延迟和提高模型效率方面取得了进展。预计GPT-5将继续这一趋势，提供更快和更准确的响应。\n* 更广泛的知识基础：通过整合更多样化和广泛的数据集，GPT-5可能在更广泛的主题上提供更全面和可靠的信息。\n* 高级个性化：新模型可能包括增强的个性化功能，使其能够更好地适应个别用户的偏好和需求。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*7xCG5iy53LLQCTnmzxs_3g.jpeg)\n\n"},{"lang":"zh","group":"blog","slug":"blog/openai-gpt-5-ph-d-level-intelligence-expected-by-2025-50a86c3aad86","frontmatter":{"title":"OpenAI GPT-5：预计 2025 年将实现博士级智能","meta_title":"OpenAI GPT-5：预计 2025 年将实现博士级智能","description":"经过数月的猜测，OpenAI 终于公布了备受期待的 GPT-5 的细节。最初预计在 2024 年推出，但……","date":"2024-11-01T03:59:56.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*OasnWeS5mgAX_0hIpirO5Q.jpeg","categories":["Machine Learning","Ethics","Data Science"],"author":"Rifx.Online","tags":["GPT-5","Ph.D.","intelligence","ethics","privacy"],"draft":false,"slug":"blog/openai-gpt-5-ph-d-level-intelligence-expected-by-2025-50a86c3aad86"},"content":"\n\n\n\n\n经过几个月的猜测，OpenAI终于揭示了备受期待的GPT\\-5的详细信息。最初预计在2024年发布，但其发布时间已推迟至2025年末或2026年初。OpenAI的首席技术官Mira Murati在与达特茅斯工程学院的采访中分享了有关这个新版本的能力和潜力的见解。以下是您需要知道的一切。\n\n## 智力的量子飞跃\n\nMurati 将之前的 GPT 版本与不同水平的人类智力进行比较。GPT\\-3 类似于一个幼儿，而 [**GPT\\-4**](https://www.geekmetaverse.com/gpt-4-unveils-its-secrets-a-combination-of-8-smaller-models/) 则可与高中生相提并论。新的 GPT\\-5 承诺在特定任务上达到“博士级智力”。这一进展不仅令人兴奋，也引发了对人工智能未来的思考。\n\n## GPT的演变：从儿童到博士\n\n将这些版本与人类教育的阶段进行比较，有助于我们更好地理解这些进步。GPT-3凭借其生成连贯且有用文本的能力，打开了许多大门。GPT-4在这些技能上进行了改进，表现出在更复杂任务中的优越性能。现在，GPT-5旨在将这一切提升到一个全新的水平，具备先进的推理和记忆能力。\n\n## 专业智能\n\nPh.D.\\-级的智能并不意味着 [**GPT\\-5**](https://www.geekmetaverse.com/openai-ceo-confirms-that-gpt-5-is-already-in-development/) 可以完美地完成所有任务。Murati 澄清了这些能力将是特定于任务的。这表明，尽管 AI 可能在某些领域超越人类，但在其他领域仍然会有局限性。这种专业化的关注可能会导致在科学研究和复杂数据分析等领域产生高度精确和有用的应用。\n\n## 潜在与未来应用\n\nGPT-5 的发展为各个领域开辟了一系列可能性。从教育到医学，从研究到技术，应用广泛。\n\n### 教育与培训\n\n一款能够达到博士水平的[**AI**](https://www.geekmetaverse.com/apple-updates-ai-takes-center-stage-with-siri-integration-chatgpt-partnership-and-elon-musk-concerns/)可能会彻底改变教育。个性化辅导系统可以为学生在复杂领域提供支持，提升理解能力和学业表现。\n\n### 医学与医疗保健\n\n在医学领域，具备此类能力的人工智能可以帮助诊断罕见疾病、开发个性化治疗方案以及管理大量临床数据，从而显著推进医疗护理。\n\n### 研究与开发\n\n研究人员可以从能够分析大数据集、识别模式并生成假设的人工智能中获得极大益处，从而加速科学和技术发现的进程。\n\n## 挑战与伦理考量\n\n尽管有着良好的应用前景，但这种先进AI的发展也带来了重大的伦理挑战。如果对AI在关键任务上的过度依赖没有得到妥善管理，可能会导致一些问题。\n\n### 隐私与安全\n\n数据隐私和网络安全将是关键议题。确保人工智能系统不被滥用，以及敏感数据得到充分保护将是优先事项。\n\n### 就业影响\n\n就业影响也是一个关注点。自动化专业任务可能会取代某些专业人士，这需要采取积极措施来应对这些社会经济影响。\n\n### 结论\n\nGPT-5 发布的延迟可能让一些人感到失望，但其先进的能力引发了人们的极大期待。如果 OpenAI 实现其目标，我们可能会看到一个革命性的工具，它将改变多个行业，并改变我们与技术的互动方式。\n\n### 常见问题解答\n\n**1\\. 什么是 GPT\\-5？**\n\nGPT\\-5 是 OpenAI 的生成预训练变换器 (Generative Pre\\-trained Transformer, GPT) 系列即将推出的版本，承诺在特定任务中具备博士级别的智能。\n\n**2\\. GPT\\-5 预计何时发布？**\n\nGPT\\-5 的发布已推迟至 2025 年底或 2026 年初。\n\n**3\\. GPT\\-5 与之前的版本相比如何？**\n\nGPT\\-3 的智能相当于一个年轻儿童，而 GPT\\-4 则相当于一名高中生。GPT\\-5 旨在实现特定任务的博士级别智能，提供先进的推理和记忆能力。\n\n**4\\. GPT\\-5 能够执行哪些任务？**\n\nGPT\\-5 将专注于某些任务，在科学研究、复杂数据分析、教育和医疗等特定领域表现出色。\n\n**5\\. GPT\\-5 会在所有方面都完美吗？**\n\n不，GPT\\-5 的博士级别智能将是针对特定任务的，这意味着它在某些领域表现出色，但在其他领域仍然会有局限性。\n\n**6\\. GPT\\-5 的潜在应用是什么？**\n\n潜在应用包括教育中的个性化辅导、医疗中帮助诊断疾病和开发治疗方案，以及协助研究人员分析大数据集和生成假设。\n\n**7\\. 与 GPT\\-5 相关的伦理考虑是什么？**\n\n伦理考虑包括确保数据隐私和网络安全，管理因自动化导致的就业流失的社会经济影响，以及防止高级人工智能系统的误用。\n\n**8\\. GPT\\-5 将如何影响数据隐私和安全？**\n\n确保保护敏感数据和防止人工智能系统的误用至关重要。需要实施措施以保障数据隐私和安全。\n\n**9\\. GPT\\-5 对就业的潜在影响是什么？**\n\nGPT\\-5 对专业任务的自动化可能会使某些专业人士失业，因此需要采取积极措施来减轻这些社会经济影响。\n\n**10\\. 为什么 GPT\\-5 的发布被延迟？**\n\n延迟使 OpenAI 能够完善和增强 GPT\\-5 的能力，以确保其满足对高级智能和专业应用的高期望。\n\n**11\\. GPT\\-5 如何改变教育？**\n\nGPT\\-5 可以通过提供个性化辅导系统来彻底改变教育，支持学生在复杂学科中的理解和学业表现。\n\n**12\\. GPT\\-5 可以为医疗领域带来哪些进步？**\n\n在医学领域，GPT\\-5 可以帮助诊断罕见疾病，开发个性化治疗方案，以及管理大量临床数据，推动医疗护理的重大进展。\n\n原文链接: [https://www.geekmetaverse.com/openai\\-gpt\\-5\\-ph\\-d\\-level\\-intelligence\\-2025/](https://www.geekmetaverse.com/openai-gpt-5-ph-d-level-intelligence-2025/)\n\n"},{"lang":"zh","group":"blog","slug":"blog/openai-just-built-her-in-real-life-17769d993e11","frontmatter":{"title":"用户会爱上 OpenAI 的新 GPT-4o 模型。确实如此。","meta_title":"用户会爱上 OpenAI 的新 GPT-4o 模型。确实如此。","description":"该公司的新款 GPT-4o 可以理解和模仿人类的语言和情感","date":"2024-11-01T04:08:40.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*-bTsggApvkUHAq57YhSd-A.png","categories":["Generative AI","Chatbots","Natural Language Processing"],"author":"Rifx.Online","tags":["GPT-4o","speech","emotions","multilingual","conversational"],"draft":false,"slug":"blog/openai-just-built-her-in-real-life-17769d993e11"},"content":"\n\n\n## 公司的新 GPT\\-4o 能理解并模仿人类的语言和情感\n\n\n\n在标志性的2013年电影 *她* 中，主角与一个语音启用的人工智能系统发展出一种强烈的关系——并演变成一场爱情故事。\n\n*她* 中的人工智能是今天的语音启用系统所不具备的：富有情感、幽默，并且能够洞察人类对话的细微差别。\n\n在今天早上的一次重大[公告](https://www.youtube.com/live/DQacCB9tDaw?app=desktop&si=jvKW7jFDwFvOMBBk)中，OpenAI 宣布发布一个新版本的 ChatGPT 系统，该系统将语音、转录和智能原生集成到一个模型中。\n\n它强大、直观，并且令人不安地像人类。基本上，OpenAI 建造了一个现实版的 *她*。\n\n## 一个糟糕的对话者\n\nChatGPT已经拥有语音功能几个月了。即使在今天，您可以在手机上打开ChatGPT应用程序，按下耳机图标，用您的声音与系统对话。\n\n然而，问题在于，ChatGPT是一个糟糕的对话者。\n\n实际上，ChatGPT的语音功能是通过将三个不同的模型拼接在一起而创建的黑客技术。\n\n当您对系统讲话时，它首先会使用转录模型将您的声音转化为文本。然后，它会将该文本输入到其智能模型中——基本上，与GPT\\-4\\的基础系统相同。\n\n智能系统会生成文本，ChatGPT会将其反馈到一个文本转语音系统中，以创建一个计算机化的声音来回应您。\n\n这使得系统名义上是可以对话的，但实际上与它交谈却显得笨拙和尴尬。\n\n在不同模型之间传递内容的额外步骤意味着系统反应迟缓。在我自己的测试中，我发现从与系统对话到得到回应，通常需要3到5秒的时间。\n\n人类对话依赖于在毫秒之间展开的微妙之处。一个响应语音需要长达五秒的系统感觉笨拙和机械。\n\n之前的系统还缺乏人类语言的许多基本方面。\n\n例如，您无法打断它；您必须等它说完才能回应。\n\n与它交谈常常感觉像是在与那些无法打断的人交谈，他们在没有意识到房间里其他人的情况下，喋喋不休地谈论一个随机话题。您常常觉得需要提到奥斯卡的乐团，以绝望的尝试让系统停止说话。\n\n它还受到无法解读声音中的情感或在自身回应中准确模仿人类情感的限制。\n\n人类在阅读潜台词方面非常出色，部分原因是我们可以捕捉到说话者声音中的微妙情感线索。\n\n如果我问我的朋友：“你今天过得怎么样？”而他们回答：“还不错”，但在“过得”和“不错”之间插入了一个微妙的停顿（或者最后一个词中带有一丝恼怒），我就知道他们实际上度过了一个艰难的日子，我应该问一些后续问题。\n\nChatGPT做不到这些，这使得与它交谈感觉像是在与某种外星智能沟通，而不是与人类交流。\n\n总之，之前的系统明显落入了“恐怖谷”。它在对话方面足够出色，声音也足够令人信服，以至于对话的某些部分可能感觉像人类。\n\n但奇怪的停顿、缺乏情感理解和延迟最终打破了这种幻觉，使其显得更令人不安而不是有用。\n\n我尝试与我六岁的儿子使用之前的系统。他对它感到如此不安，以至于不让我再打开音频。\n\n## OpenAI的革命性新模型\n\n今天，OpenAI正在改变这一切。在他们[今天早上的公告](https://www.youtube.com/live/DQacCB9tDaw?app=desktop&si=jvKW7jFDwFvOMBBk)中，公司透露他们将发布一个新模型，GPT\\-4o。\n\nGPT\\-4o原生集成了语音识别、语音生成和智能于一个系统中。\n\n这意味着将三种不同模型集成以模拟对话的复杂代码系统已经不复存在。相反，新的ChatGPT版本将能够**接收语音，瞬间处理，并以其自身生成的逼真语音作出回应。**\n\n对于用户来说，这将启用OpenAI首席执行官Sam Altman所描述的几种新功能，称其为“像魔法一样”。[描述为“像魔法。”](https://twitter.com/sama/status/1788989777452408943)\n\n首先，你将能够与ChatGPT进行更加自然的对话。你不再需要将问题和后续问题输入到界面中，而是可以像与朋友交谈一样与应用程序交谈。\n\n在几次现场演示中，OpenAI的工程师展示了系统如何在毫秒内倾听用户并作出智能回应。\n\n再次强调，这种速度之所以可能，是因为新模型不需要浪费时间在不同模式之间切换——它可以在单一步骤中处理语音并以自己的声音回应，而不必依赖多个低级模型。\n\nGPT\\-4o还可以解读和创造情感。\n\n在一次演示中，一名OpenAI员工要求系统引导他进行呼吸练习。\n\n他随后假装过度换气，而ChatGPT——感知到他呼吸的速度和声音中显现的恐慌——劝他放慢速度，做更深的呼吸。\n\n该系统似乎还能够调节自身回应中的情感。在另一场演示中，这名员工要求GPT\\-4o用越来越戏剧化的声音讲一个睡前故事。\n\n它照办了，最终听起来像一个中学戏剧社的孩子在可怕地过度表演一个场景！\n\n由于新系统还集成了GPT\\-4的视觉能力，它可以执行诸如解读人脸表情中的情感等功能。\n\n这种增强的情感智能水平可能会使系统成为一个更好的对话者。\n\n其他新功能也将有所帮助。用户可以在GPT\\-4o说话的过程中打断它。\n\n在他们的演示中，OpenAI的工作人员经常在模型开始偏离主题时打断它，就像在现实生活中打断朋友以开始回应问题一样。\n\n## 巨大的潜力\n\n今天早上的演示轻松幽默。但人们很快就能意识到，一个能够轻松理解、快速处理并真实创造情感人类语言的模型将是多么强大。\n\n在演示过程中，ChatGPT几次以让我想起电影《她》中虚构的人工智能的方式回应。\n\nChatGPT似乎对自己感到好笑，当OpenAI的工作人员称赞它时，它会感到尴尬，甚至可能会不时地抛出一些调情的台词。\n\n几次（据称）即兴的互动也揭示了更好的对话可以解锁的一些更深层次的能力。\n\n根据观众的问题，OpenAI的工作人员演示了系统如何能够听懂意大利语，并快速准确地将其翻译成英语，反之亦然。\n\n人们很容易想象，这样的能力将使多语言互动变得极其简单，基本上消除了语言障碍（也许还包括人类翻译）。\n\n例如，一位医生可以调出ChatGPT，快速与任何语言的患者交流。在旅行时，你可以在手机上调出这个应用，把它当作一个免费的即时翻译，向某人询问方向或在商店里进行购买。\n\n如果再加上视觉能力，你甚至可以向ChatGPT展示一家外国餐厅的菜单，询问某些菜品的翻译，告诉它你在家时喜欢吃什么，并请它推荐一些你可能想点的菜（或避免的菜）。\n\n我也可以看到新系统如何迅速进入《她》的领域。OpenAI仍然不允许电影中发生的那种不适合工作场合的互动。\n\n但是GPT-4o理解和模仿情感的能力——加上其强大且常常令人惊讶的能力，能够生成自己令人信服的人类情感表达——令人印象深刻。\n\n听完演示后，我确信人们会像电影中的主角一样爱上这个系统。它真的很出色。\n\n## 它会被使用吗？\n\n所有这些在纸面上都很惊人。然而，目前还不清楚有多少用户真正想要一个完全情感化的 AI 语音伴侣。\n\n我工作的大多数人使用 ChatGPT 不是作为对话伴侣，而是出于实用目的。\n\n我看到同事们利用这个系统来处理一些无聊和单调的任务，比如为网络研讨会撰写着陆页文案、快速回复房东的电子邮件，或撰写博客文章的初稿。\n\n这些实用功能实际上并不需要对话。目前尚不清楚能够用语音向 AI 发出这些请求是否会有用。\n\n因此，真正的考验并不一定是 OpenAI 的新系统有多强大，而是 **他们在用户已经通过语音与计算机互动的地方整合它的效果如何。**\n\n现实来看，我无法想象有多少用户会在工作时坐下来与 AI 对话。\n\n但如果 OpenAI 将 GPT-4o 集成到手机、汽车或像 Amazon Echo 这样的智能设备的语音界面中，我可以很容易地想象这个系统的情感能力变得更加有用。\n\n即使人们不太想与 ChatGPT 交谈，原生多模态音频和视觉模型的新能力对于在 OpenAI 现有 API 上构建应用程序的开发者来说，将是无比强大的。\n\n在他们的公告中，OpenAI 表示 GPT-4o 将通过他们现有的开发者接口提供。该系统的价格也将比之前的 GPT-4 模型便宜 50%。\n\n仅这些变化就非常重大。无论语音元素是否真正流行，驱动它的智能也将使数百个现有的 GPT-4 驱动应用程序变得更聪明、更快、更好，并且运营成本更低。\n\n换句话说，新系统的对话元素可能会被认为是一个很酷的噱头。但其潜在影响将更微妙、更广泛。\n\n我很期待看到真实用户如何与 GPT-4o 互动。他们会感到不安吗？惊讶吗？心动吗？\n\n但我更期待的是启动我的 Python IDE，将 GPT-4o 添加到我已经使用 OpenAI 工具构建的应用程序中。\n\n与机器对话很酷。但一个能够理解人类情感的原生多模态 AI 模型，我只需几行 Python 代码就能调用，而且成本低廉？这真的可能改变世界。\n\n**在过去的一年里，我测试了数千个 ChatGPT 提示。作为全职创作者，有一些我每天都会使用，符合我在本文中提到的伦理用途。我将它们汇编成一本免费的指南，*7 个对创作者极其有用的 ChatGPT 提示。* [今天就获取一份吧！](https://no-frills-influencer.ck.page/6a100e8fe4)**\n\n"},{"lang":"zh","group":"blog","slug":"blog/openai-realtime-api-voice-mode-getting-started-on-colab-39b93edcaa6a","frontmatter":{"title":"OpenAI 实时 API（语音模式），Colab 入门","meta_title":"OpenAI 实时 API（语音模式），Colab 入门","description":"您需要了解的一切，以及可以在 Colab 上运行的 OpenAI 语音模式 API 的实践介绍。","date":"2024-11-08T00:23:32.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*_-d5zsWWQEzVLZxABTSFWQ.png","categories":["Programming","Voice Assistants","Technology/WebAPI"],"author":"Rifx.Online","tags":["OpenAI","Realtime","API","GPT-4o","Colab"],"draft":false,"slug":"blog/openai-realtime-api-voice-mode-getting-started-on-colab-39b93edcaa6a"},"content":"\n\n\n您需要了解的一切，以及在 Colab 上运行 OpenAI 语音模式 API 的动手介绍。\n\n\n\nOpenAI 最新的开发为我们带来了 **实时 API**，旨在允许开发者在他们的应用中创建 **快速、无缝的语音到语音体验**。该 API 旨在简化多模态对话功能的开发，使构建自然的实时语音交互变得更加容易。\n\n**在这篇博客中，** 我将涵盖有关此新 API 的 **主要问题**，包括\n\n* 什么是实时 API，\n* 如何访问它，\n* 它的限制和定价，\n* 并提供一个 **Colab 教程**，教您如何入门。\n\n## 什么是实时 API？\n\n**实时 API** 是 OpenAI 提供的公共测试功能，允许付费开发者在他们的应用中集成实时语音交互。它是一个多模态 API，能够将 **音频输入转换为语音响应**，并使用先进的 **GPT-4o** 模型来实现这一目的。本质上，它允许进行 **低延迟对话**，类似于自然的人际交互，类似于 ChatGPT 的高级语音模式中看到的功能。\n\n之前，开发者需要将多个模型拼接在一起以实现 **语音识别、文本处理和文本转语音生成**。实时 API 将这一切都整合在一次 API 调用中，从而减少延迟，提供更丰富的响应，并更一致地处理口音和重音。\n\n**聊天完成 API** 也引入了音频输入和输出，但它没有实时 API 的低延迟体验。因此，对于语言学习或语音启用助手等体验，实时 API 是更优选择。\n\n## 访问和限制\n\n对 **Realtime API** 的访问目前作为 **公开测试版** 提供给付费开发者。\n\n**虽然说在欧洲的访问受到限制，但我通过我的第5层OpenAI账户能够使用它。**\n\n该API使用 **WebSocket** 连接，确保音频输入和输出的流畅体验。\n\n目前，需要注意以下 **限制**：\n\n* **会话速率限制**：该API对第5层开发者的会话数量限制为大约 **100个同时会话**。较低层级的容量更小。截至2024年10月，API的限制为每分钟2M个令牌。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*XpAB6WRseRb0iY-edE94xw.png)\n\n* **功能**：最初仅支持 **语音模式**，但OpenAI计划随着时间的推移添加更多功能，如 **视频** 和 **视觉**。\n* **可用性**：完整的音频功能处于测试阶段，未来计划为Python和Node.js进行 **SDK集成**。\n\n## Realtime API 的定价\n\n**定价**结构分为 **文本令牌** 和 **音频令牌**：\n\n* **音频输入**：每百万令牌 $100（大约 **$0\\.06 每分钟**）。\n* **音频输出**：每百万令牌 $200（大约 **$0\\.24 每分钟**）。\n* **文本输入**：每百万令牌 $5。\n* **文本输出**：每百万令牌 $20。\n\n这一定价使得开发者能够负担得起创建强大的 **语音到语音** 体验，尽管音频功能的成本显著高于基于文本的交互。在扩展具有语音功能的应用时，这一点非常重要。\n\n这仍然比外包给某些国家稍贵，但我们可以期待在接下来的六个月内价格会显著下降。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*ocwFDXEt8X7KD_k6)\n\n## 在 Google Colab 中使用 Realtime API 构建\n\n这是一个基本的 **Colab 指南**，帮助您开始上传文件、向 Realtime API 发送请求并生成音频响应。\n\n在这个演示中，我们选择上传一系列音频片段，以模拟对话。\n\n**完整的 Colab 代码**： [链接在这里](https://colab.research.google.com/drive/1-bj_LH7Gv2bbTJopbo7Hk_AIyDAuqeEQ?usp=sharing)，只需将您的 “openai” 密钥添加到 Colab 的秘密中并运行该 Colab。\n\n### 第一步：设置 Google Colab 和依赖项\n\n* 开始一个新的 **Google Colab** 笔记本。\n* 安装必要的库，例如 **requests** 和 **pydub** 来管理音频文件。\n\n\n```python\n#Setup\n!pip install websockets pydub --quiet \n\nimport base64\nimport numpy as np\nimport soundfile as sf\nimport json\nimport websockets\nfrom google.colab import files\nfrom pydub import AudioSegment\nfrom tqdm import tqdm\nimport io\n```\n\n### 步骤 2：上传音频文件\n\n在 Colab 中，您可以使用 **google.colab** 的 `files` 模块来上传音频文件。\n\n```python\n#Upload audio\ndef upload_audio():\n    uploaded = files.upload()  \n    for file_name in uploaded.keys():\n        return file_name\n\naudio_file = upload_audio()\n```\n\n### 第 3 步：向实时 API 发送请求\n\n* 在将音频文件发送给 OpenAI 之前，正确格式化音频文件。\n* 建立 WebSocket 连接以流式传输音频文件。\n* 使用 `tqdm` 显示上传流的进度。\n* 该函数返回完整的事件集（包括响应），以便后续处理生成输出音频。它还返回模型响应的转录文本。\n\n```python\n#Helper functions\n## Function to convert Float32Array to PCM16 format\ndef float_to_pcm16(float32_array):\n    return np.clip(float32_array * 32767, -32768, 32767).astype(np.int16).tobytes()\n\n## Function to split audio into base64-encoded PCM16 chunks\ndef float32_to_base64_chunks(float32_array, chunk_size=32000):\n    pcm16_data = float_to_pcm16(float32_array)\n    for i in range(0, len(pcm16_data), chunk_size):\n        yield base64.b64encode(pcm16_data[i:i+chunk_size]).decode('utf-8')\n\n## WebSocket connection and streaming audio with text prompt\n## Main function to call OpenAI Realtime API\nasync def stream_audio_to_realtime_api(audio_file, text_prompt, openai_key, verbose = False):\n    data, samplerate = sf.read(audio_file, dtype='float32')\n    if data.ndim > 1:\n        data = data[:, 0]\n    if samplerate != 24000:\n        raise ValueError(f\"Audio must be sampled at 24kHz, but it is {samplerate}Hz\")\n\n    url = \"wss://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2024-10-01\"\n    headers = {\"Authorization\": \"Bearer \" + openai_key, \"OpenAI-Beta\": \"realtime=v1\"}\n\n    async with websockets.connect(url, extra_headers=headers) as ws:\n        await ws.send(json.dumps({\n            \"type\": \"conversation.item.create\",\n            \"item\": {\"type\": \"message\", \"role\": \"user\", \"content\": [{\"type\": \"input_text\", \"text\": text_prompt}]}\n        }))\n\n        with tqdm(total=(len(float_to_pcm16(data)) + 32000 - 1) // 32000, desc=\"Sending Audio Chunks\") as pbar:\n            for chunk in float32_to_base64_chunks(data):\n                await ws.send(json.dumps({\"type\": \"input_audio_buffer.append\", \"audio\": chunk}))\n                pbar.update(1)\n\n        await ws.send(json.dumps({\"type\": \"input_audio_buffer.commit\"}))\n        await ws.send(json.dumps({\"type\": \"response.create\"}))\n\n        all_events = []\n        while True:\n            response = await ws.recv()\n            event = json.loads(response)\n            all_events.append(event)\n            if verbose:\n                print(event)\n            if event[\"type\"] == \"response.output_item.done\" and \"item\" in event and \"content\" in event[\"item\"]:\n                for content in event[\"item\"][\"content\"]:\n                    if content[\"type\"] == \"audio\" and \"transcript\" in content:\n                        transcript = content[\"transcript\"]\n                        break\n            if event[\"type\"] == \"rate_limits.updated\":\n                break\n\n        return all_events, transcript\n```\n\n```python\n#Add a prompt and call OpenAI Realtime API\ntext_prompt = \"Summarize this audio content\"\n\nevents, transcript = await stream_audio_to_realtime_api(\n    audio_file, \n    text_prompt, \n    openai_key, \n    verbose = False \n#to display OpenAI's response as they arrive, use verbose = True\n    ) \n```\n\n### 第4步：生成音频响应\n\n* 一旦收到响应，生成音频。\n* 选择一个文件名并保存文件。\n* 然后您将能够下载该文件。\n\n\n```python\n## Function to decode and concatenate audio chunks into a full audio file\ndef generate_audio_from_chunks(audio_chunks, output_filename=None):\n    # Concatenate the base64-encoded audio chunks from the 'delta' field\n    full_audio_base64 = ''.join(audio_chunks)\n\n    # Decode the concatenated base64 string to raw PCM16 audio bytes\n    audio_bytes = base64.b64decode(full_audio_base64)\n\n    # Load the bytes as a pydub AudioSegment (assuming 24kHz, 1 channel, PCM16)\n    audio_segment = AudioSegment.from_raw(\n        io.BytesIO(audio_bytes), \n        sample_width=2, \n        frame_rate=24000, \n        channels=1)\n\n    # Optionally save the audio to a file\n    if output_filename:\n        audio_segment.export(output_filename, format=\"wav\")\n        print(f\"Audio saved to {output_filename}\")\n\n    return audio_segment\n```\n\n```python\n#Extract audio chunks from the collected events\naudio_output_chunks = [event['delta'] for event in events if event['type'] == 'response.audio.delta']\n\n## Generate the full audio from the collected chunks\ngenerated_audio = generate_audio_from_chunks(audio_output_chunks, output_filename=\"output_audioo.wav\")\n```\n\n## 结论\n\n通过上述步骤，您可以将 OpenAI 的实时 API 集成到 Colab 笔记本中，实现无缝的语音指令。\n\n本指南应为您提供一个坚实的基础，以便您实验实时音频到音频的交互，并构建创新的语音驱动应用程序。\n\n"},{"lang":"zh","group":"blog","slug":"blog/openai-rolls-out-searchgpt-to-more-users-33024ff3132c","frontmatter":{"title":"OpenAI 向更多用户推出 SearchGPT","meta_title":"OpenAI 向更多用户推出 SearchGPT","description":"ChatGPT 的用户界面进行了大规模重新设计，以支持 SearchGPT——现在它类似于 Google 和 Perplexity 等搜索引擎。","date":"2024-11-01T03:57:02.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BW6Qt6PMwHwlYRljAIBQWg.jpeg","categories":["Chatbots","Technology/Web","SearchGPT"],"author":"Rifx.Online","tags":["SearchGPT","ChatGPT","web","search","publishers"],"draft":false,"slug":"blog/openai-rolls-out-searchgpt-to-more-users-33024ff3132c"},"content":"\n\n\n\n\n**你注意到 OpenAI 最近对 ChatGPT 的重新设计了吗？**\n\n如果你最近登录过，你可能会发现两个主要变化。\n\n* 首先，新的 [**Canvas**](https://generativeai.pub/openai-rolls-out-canvas-in-chatgpt-a-brand-new-writing-and-coding-interface-7b57a3ec582a) 功能会自动在右侧打开一个新界面。这个新增功能让你可以处理更长的文档，而无需在聊天中上下滚动。这是一个小而实用的更新。\n* 其次，**提示字段**已经上移，现在位于屏幕中央。\n\n请看下面最新的用户界面：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*KsYMU9ffVlKsmHzKIOKH8g.png)\n\n你注意到这个新布局与 Google 和 Perplexity AI 的相似之处了吗？ChatGPT 现在看起来像一个搜索引擎。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*xFKErUHnfbJunaKi4NvM9A.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*PgSwS14lkUtZe7Ra9oLCrg.png)\n\n现在，当你在键盘上按下 ‘/’ 键时，你可以切换一个新的“搜索”功能，让 ChatGPT 访问网络。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*oYxvVvsuUuc_PM0PXRmN7A.png)\n\n让我们来分析一下这意味着什么。\n\n## ChatGPT中的搜索功能是什么？\n\n[SearchGPT](https://generativeai.pub/openai-announces-search-gpt-is-this-the-google-killer-5919ba31f95b) 允许ChatGPT访问实时网页数据。它的工作方式类似于Perplexity，使用大型语言模型为您搜索网络，提供即时答案，并包含其引用的来源。\n\n该功能最初向10,000名用户开放，并为希望提前访问的人添加了候补名单表单。\n\nOpenAI与**华尔街日报、美国广播公司、Vox Media和时代杂志**等知名出版商合作，以确保用户获得可信、可靠的信息。\n\n> “人工智能搜索将成为人们浏览互联网的关键方式之一，在这些早期阶段，技术的构建方式至关重要，它必须重视、尊重和保护新闻业和出版商。我们期待与OpenAI在这个过程中合作，为读者创造一种新的发现《大西洋月刊》的方式。” — Nicholas Thompson, 《大西洋月刊》首席执行官\n\n当您向SearchGPT提问时，它并不是从随机来源提取信息。每个响应都附有**清晰的内联引用和链接**，因此您确切知道信息来源于何处。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*uchpKOXqZCG55HSNkaOOZQ.png)\n\n您甚至可以通过点击出现在搜索网站下拉框中的源链接深入了解，提供更多探索主题的方式。\n\n## 如何访问 SearchGPT\n\n访问 SearchGPT 非常简单。当您在 ChatGPT 中时，按下 **‘/’** 键并从菜单中选择搜索选项。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*xNfm-6zPFzdXGL2A0D92YQ.png)\n\n它的工作方式与其他搜索引擎类似：您提出问题，几秒钟内，SearchGPT 就会提供答案，并附上来源。\n\n您甚至可以提出后续问题以更深入地探讨主题。这创造了一种对话式的搜索体验，比传统搜索结果的滚动更加互动。\n\n## SearchGPT vs. Perplexity vs. Google\n\n那么，SearchGPT 与 **Perplexity AI** 和 **Google** 相比如何呢？\n\n**SearchGPT** 的设计旨在为您提供简洁且来源明确的答案。每个答案都有链接到原始来源，您可以点击以验证信息。它非常适合实时回答和快速事实检查。\n\n此外，通过后续问题，您可以在不重新开始的情况下优化查询。这种对话性质使其感觉像是在与一个记得您所问问题的超级先进版本的 Google 交谈。\n\n另一方面，**Perplexity** 则是一个更学术风格的搜索引擎。它强调学术文章和详细研究，这对于更深入的查询非常有用。Perplexity 通常更适合需要更深层次来源的研究密集型任务。\n\n当然，**Google** 仍然是这个领域的巨头。尽管他们最近努力将生成式 AI 纳入搜索结果，但还没有完全实现用户所期望的无缝体验。\n\nGoogle 的生成搜索推出过程笨拙，并因错误和不相关的回答而受到大量批评。但 Google 的信息广度和基础设施仍然无与伦比。\n\n## 这是谷歌的终结吗？\n\n谷歌不会很快消失。这家科技巨头仍然控制着超过90%的搜索市场。他们已经在这个领域奋斗了数十年，搜索算法也在不断演变。\n\n然而，随着像SearchGPT这样的AI搜索引擎逐渐崭露头角，谷歌面临着提升自身竞争力的压力。OpenAI与出版商合作以获取可靠来源的举动是一个明智的策略，可能会削弱谷歌的主导地位。\n\n这种对验证结果的关注意味着，当你使用SearchGPT时，你不太可能遇到虚假答案——这是过去AI驱动工具所面临的挑战。\n\n此外，谷歌仍然是大多数人的默认选择。它的优势在于无处不在——从你手机的浏览器到智能音箱。SearchGPT仍处于早期阶段，需要时间来获得用户的信任。\n\n## SearchGPT 还未达到\n\n我在过去几个小时里测试了 SearchGPT，以下是我的一些观察：\n\n* **答案质量：** 一个主要的缺点是 SearchGPT 的答案质量与 Perplexity Pro 的深度或精确度不太匹配。尽管它与 Perplexity 的基础版本相当，但依赖于它进行更复杂或细致查询的用户会注意到差异。\n* **响应缓慢：** 另一个痛点是速度。在使用 SearchGPT 时，处理查询并返回答案所需的时间可能显得极其缓慢。这种延迟打断了互动的流畅性，尤其是在你深入探讨某个主题时。\n* **缺乏上下文理解：** 在某些情况下，它未能识别对话的连续性。如果你提出后续问题，模型往往将其视为一个新的独立问题，而不是理解为你之前查询的上下文。\n* **没有后续建议：** 与 Perplexity 不同，后者通常会建议后续问题以帮助你细化搜索，SearchGPT 并不提供此功能。这种缺乏指导使用户不得不自行想出如何最好地措辞或缩小查询范围。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*vfXxpgENLyenLY2l33PKiw.png)\n\n在使用搜索功能时，我注意到另一个奇怪的工作流程：如果将语言模型从 GPT-4o 切换到“ChatGPT o1-preview”，搜索指示器仍然存在，但实际上并不会在网上搜索结果。\n\n它返回的是其领域知识中的结果，这并不是用户所期望的。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*YcI-UuEmmFTQPUlO6mjpxA.png)\n\n正确的行为应该是在用户切换到“ChatGPT o1-preview”后禁用 *search* 功能，因为该模型没有能力在网上搜索。\n\n## 最后的思考\n\n我很高兴OpenAI终于推出了SearchGPT。自从他们在2024年7月宣布这一点以来，我就一直想测试它。\n\n在目前的状态下，SearchGPT是OpenAI迈入AI驱动搜索世界的良好第一步，但它还不够成熟，无法成为任何人处理复杂实时查询的首选工具。\n\n准确性、速度和处理对话上下文的能力还不够。目前，如果您需要深入的见解或更快的结果，像Perplexity Pro或Google这样的工具仍然是更好的选择。\n\n进一步阅读：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*5ejBBgbZaE8pGmpW.png)\n\n本文发布在[Generative AI](https://generativeai.pub/)。请在[LinkedIn](https://www.linkedin.com/company/generative-ai-publication)上与我们联系，并关注[Zeniteq](https://www.zeniteq.com/)，以获取最新的AI故事。\n\n订阅我们的[新闻通讯](https://www.generativeaipub.com/)和[YouTube](https://www.youtube.com/@generativeaipub)频道，以获取有关生成AI的最新新闻和更新。让我们一起塑造AI的未来！\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*TnRFuKk-2Dj_KCAP.png)\n\n"},{"lang":"zh","group":"blog","slug":"blog/openai-searchgpt-chatgpt-with-internet-and-browsing-tools-023ddca7cb44","frontmatter":{"title":"OpenAI SearchGPT：带有互联网和浏览工具的ChatGPT","meta_title":"OpenAI SearchGPT：带有互联网和浏览工具的ChatGPT","description":"Perplexity 和 Google 搜索的更好替代方案","date":"2024-11-08T00:28:30.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*N_EtjjOxkx6QsKRLx5f_cQ.png","categories":["Technology/Web","Data Science","SearchGPT"],"author":"Rifx.Online","tags":["SearchGPT","filtering","citations","recommendations","customization"],"draft":false,"slug":"blog/openai-searchgpt-chatgpt-with-internet-and-browsing-tools-023ddca7cb44"},"content":"\n\n\n### 一个更好的替代方案：Perplexity 和 Google 搜索\n\n\n\n备受期待的 OpenAI 产品 SearchGPT 昨晚发布，拥有一些重大功能，使其在竞争对手 Perplexity 之上更进一步。\n\n如 OpenAI 所宣布的，SearchGPT 不仅仅是带有互联网的 ChatGPT。\n\n它本身就是一个 AI 网络浏览器。\n\n谈到一些关键功能：\n\n* **高级过滤**：为特定日期、来源或内容类型（例如，仅限同行评审文章、政府网站等）设置过滤器。\n* **上下文感知摘要**：生成针对特定领域（如医学或金融）的摘要、关键要点或洞察。\n* **引用生成**：自动格式化并提供学术风格的引用（APA、MLA）。\n* **多步骤查询**：在一次搜索中处理复杂的分层问题，跨多个来源。\n* **数据分析集成**：直接提取和分析数据以获取洞察（例如，趋势分析）。SearchGPT 可以连接到专业数据库，允许访问特定领域（如医学期刊、法律案例数据库或专有商业分析）。\n* **个性化推荐**：根据您的搜索历史建议相关来源、文章或更新。这包括 **预设模板或角色**：例如，可以设置一个以研究为重点的“SearchGPT”以检索科学数据并直接提供学术引用。\n\n### ChatGPT之前无法访问互联网吗？\n\n它可以（对于高级会员）。但现在，这些功能更加先进。为了查看一般网页搜索与SearchGPT的不同，我尝试让ChatGPT（免费版）自己搜索一个查询：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ORjGLDBqKDWiPANHlxSdqw.png)\n\n然后，我询问这个搜索是如何进行的。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*NlzDed3nHdJ636aLt75DCg.png)\n\n接下来，\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*eeDPQHQA62yaMK_KkSL0kQ.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ZcsmVgau0SaavN01yHbdKw.png)\n\n所以，正如你所看到的，SearchGPT不仅仅是一个简单的网页浏览工具，而是更多。不幸的是，OpenAI已将SearchGPT提供给Pro用户，并且仅限于等待名单用户。如果你有访问权限，你一定昨天收到了这封邮件\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*WfO0Xl4VQwRxNNkC1GQpOw.png)\n\n以下是一些来自SearchGPT的屏幕截图\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8bbGpwbsRzo6xQhNDPb3Jw.png)\n\n正如你所看到的，它在搜索时提供了热门话题作为建议，类似于网页浏览器。\n\n尝试查看今天的天气\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*gbtr-QFQw4BnhqrWPvH4RQ.png)\n\n甚至可以限制仅检查特定网站，例如“仅引用政府网站”\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*eq_Xf4JkD4XV85KE5376VA.png)\n\n所有引用都可以在底部一起查看你的结果\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*wUTEae5yYh_j-oaq6HyP9Q.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Quruyw07__p3qoJ_KhmHAA.png)\n\n## SearchGPT vs Perplexity. 哪个更好？\n\n这是一个很难回答的问题，至少目前是这样。有几点值得强调：\n\n1. Perplexity 是免费的，而 SearchGPT 不是！\n2. SearchGPT 更快。但我认为这是因为目前的流量较少。\n3. Perplexity 更简单，并且具有早期进入市场的优势。\n4. Perplexity 在这个领域存在已久，相比之下更可靠。\n5. SearchGPT 提供更多的自定义选项，不仅仅是一个与互联网连接的 LLM。\n\n老实说，我一直支持免费的东西，因此任何时候我都会选择 Perplexity。尽管如此，考虑到 SearchGPT 的快速响应，这个工具相当不错，值得尝试。\n\n"},{"lang":"zh","group":"blog","slug":"blog/openais-leaked-gpt2-model-has-everyone-stunned-6337904c2ecf","frontmatter":{"title":"OpenAI‘泄露’的 GPT2 模型让所有人震惊。","meta_title":"OpenAI‘泄露’的 GPT2 模型让所有人震惊。","description":"故意泄密？","date":"2024-11-01T04:07:40.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*-G0yfSjGPdNw02NZ","categories":["Chatbots","Generative AI","Natural Language Processing"],"author":"Rifx.Online","tags":["GPT-2","Chatbot","Inference","JSON","AlphaGo"],"draft":false,"slug":"blog/openais-leaked-gpt2-model-has-everyone-stunned-6337904c2ecf"},"content":"\n\n\n### 故意泄漏？\n\n\n\nOpenAI 对人工智能行业的影响不容小觑。每一个动作或决定都会自动成为头条……即使他们并没有真正宣布什么。\n\n几天前，一个我们许多人曾试用过但已被删除的模型让整个人工智能行业着迷。这个名为“gpt2-chatbot”的模型在 [lmsys.org](https://chat.lmsys.org/) 的“直接聊天”功能中可以使用了几天。\n\n*但为什么这么多喧嚣？*\n\n因为这个模型与我们见过的任何东西都不同。**它处于一个完全不同的层次。**\n\n因此，许多人认为它是 **ChatGPT-4.5** 或甚至 **GPT-5** 的非官方预告。更令人兴奋的是，使用数字“2”作为信号，表明 **新一代长推理模型的 GPT 正在逼近**。\n\n甚至 OpenAI 的 CEO Sam Altman 也忍不住承认它的存在，并在过程中调侃我们：\n\n\n\n\n那么，*这个模型到底有多好，它究竟是什么？*\n\n\n> 你可能已经厌倦了人工智能通讯简报谈论某个事情是如何“刚刚”发生的。这些通讯简报层出不穷，因为粗略地谈论已经发生的事件和事情是容易的，**但提供的价值有限，炒作却被夸大。**\n\n\n> 然而，谈论 **将** 会发生的事情的通讯简报却是罕见的。如果你想在别人之前获得易于理解的人工智能未来洞察，**TheTechOasis** 通讯简报可能非常适合你。\n\n\n> 🏝️🏝️ 今天就订阅吧：\n\n## 未来的预告\n\n随着每一天的过去，OpenAI的下一个模型显然将在推理和复杂问题解决方面实现飞跃。\n\n为了证明这个神秘的新模型可能就是它，这里有几个例子展示这个神秘模型的强大，可能表明这艘船已经在那个港口停靠：\n\n> 以下所有示例被认为是**当前最先进模型**的**困难或完全不可能**的。\n\n首先，它在零-shot模式下解决了一个数学奥林匹克问题（没有提供辅助示例来支持解决方案）：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*oNPg_hTGc0OP90n9)\n\n我甚至无法开始解释之前的例子有多疯狂，从当前最先进的模型中得到这样的答案绝对是不可能的。\n\n[它在解析JSON方面也绝对出色](https://twitter.com/skirano/status/1785035706173214888)，这是LLM与API及其他基于网络工具集成的基本技能。\n\n此外，它在复杂绘图任务中完全压倒了GPT-4，例如[根据代码绘制SVG文件](https://twitter.com/decentricity/status/1785049191003361778)或**使用ASCII代码绘制独角兽（如下）**，在这个过程中羞辱了**Claude 3 Opus**，当前的最先进模型：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*5A0EcRU91ZYAwVAc)\n\n此外，尽管这很可能是一次幻觉，**模型向我声称它是由OpenAI训练的，并基于GPT-4变体。**\n\n当然，在如此强大的演示之后，**许多人建议“gpt2-chatbot”甚至可能是著名的Q\\*模型**。\n\n但与其简单地屈服于人们声称的不同奇幻选项，不如采取更理性的方式，看看OpenAI自己在几个月（甚至几年）里通过他们的研究所暗示的内容。\n\n## 长推理的力量\n\n几个月来，像 [Demis Hassabis](https://www.youtube.com/watch?v=eqXfhejDeqA&t=2s) 或 [Andrej Karpathy](https://youtu.be/c3b-JASoPi0?si=fZWoSpLuSmua8YMR&t=1481) 这样的领域专家讨论了仅靠 LLMs 是不够的，我们需要“其他东西”来真正将它们提升到一个新的水平。\n\n在这两种情况下，他们提到实现相当于“AlphaGo 但在 LLMs 中”，这间接指的是：\n\n* **自我提升** 和\n* **测试时计算** LLMs\n\n*但他们这是什么意思呢？*\n\n### 人工智能的巨大飞跃\n\nAlphaGo 是人工智能的历史。它是第一个在围棋这一韩国棋类游戏中毫无疑问地超越人类实力的模型。\n\n它使用了 **Monte Carlo Tree Search**，一种搜索算法，来探索游戏中任何给定步骤的可能走法，能够超越当前的动作并预测对手的行动。\n\n> 你们中的一些人可能还记得 **Deep Blue**，那台在1997年与加里·卡斯帕罗夫的系列赛中第二局勉强战胜他的国际象棋机器，在第一局中输掉了比赛。\n\n> 然而，尽管 Deep Blue 可以被击败，但 AlphaGo 是无敌的。\n\n*但这是怎么做到的？*\n\n### 自我提升以达到超人水平\n\n使AlphaGo卓越的关键因素在于它的训练方式，**通过与自身的较弱版本对弈来创建自我提升循环。**\n\n它持续与自己对弈，逐渐将ELO提升至3\\.739，几乎达到了当今最佳围棋选手的水平。\n\n> 2017年，改进版的AlphaZero达到了5\\.018的ELO，完全超越人类，无法击败。\n\n换句话说，有了AlphaGo，人类首次实现了一种通过自我提升来训练模型的方法，使其能够达到超人能力，**因为它不再依赖模仿人类来学习。**\n\n如果你在想，这对LLMs并不适用。\n\n当前的LLMs完全依赖于人类水平的表现，因为所有数据和训练本质上都依赖于人类（以至于[对齐阶段](https://thewhitebox.ai/llms-the-backbones-of-frontier-ai/)——LLMs被建模以提高其安全水平并避免冒犯性反应的训练过程的一部分，**严格执行时使用的是“人类偏好”**）。\n\n> 顺便提一下，[Meta最近提出了自我奖励模型](https://arxiv.org/pdf/2401.10020v1)，可以通过自身的反应进行自我提升。然而，目前尚不清楚这个反馈循环是否真的能使LLMs超越人类。\n\n但尽管仍然很难相信“gpt2\\-chatbot”是通过自我提升训练出来的，**我们有充分的理由相信它是OpenAI多年来努力工作的第一个成功实现：测试时计算**。\n\n### 测试时计算模型的到来\n\n多年来，OpenAI的几篇研究论文暗示了将模型偏向“重推理”的这一想法。\n\n例如，早在2021年，[他们提出了在推理时使用“验证者”的概念](https://arxiv.org/pdf/2110.14168)，以改善模型在处理数学问题时的响应。\n\n这个想法是训练一个辅助模型，实时评估模型给出的多个响应，选择最佳的一个（然后提供给用户）。\n\n这与某种树搜索算法相结合，比如AlphaGo使用的那种，结合Google Deepmind的[思维树研究](https://arxiv.org/pdf/2305.10601)的例子，最终可以创建一个LLM，在回答之前，探索“可能响应的领域”，**仔细过滤并选择通向解决方案的最佳路径。**\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*pHWwOA66fxpKbl-z)\n\n这个想法虽然在2021年由OpenAI提出，但如今已经变得相当流行，[微软和谷歌的跨界合作研究将其应用于训练下一代验证者](https://arxiv.org/pdf/2402.06457)，谷歌甚至成功创建了一个模型，[Alphacode](https://storage.googleapis.com/deepmind-media/AlphaCode2/AlphaCode2_Tech_Report.pdf)，以极大的成功执行这种架构，**在竞争程序员中达到了85%的百分位，成为最优秀的人类程序员之一。**\n\n*那么，为什么这一代新的LLM具有如此巨大的潜力？*\n\n因为**它们在解决问题时与人类的方式非常相似**，通过有意识和广泛的思考来解决特定任务。\n\n归根结底，把“搜索+LLM”模型视为AI系统，它们在模型的实际运行时间上分配了更高程度的计算（类似于人类思维），因此，它们不必立即猜测正确的解决方案，而是简单地说，“有更多时间来思考”。\n\n但OpenAI更进一步。\n\n### PRM 模型以改善数学执行\n\n在去年五月，他们发布了论文 [Let’s Verify Step\\-by\\-Step](https://arxiv.org/pdf/2305.20050)，参与者包括 OpenAI 的首席科学家 Ilya Sutskever 以及一些来自原始验证者论文的研究人员，如 Karl Cobbe。\n\n这里的想法是修改在模型对齐阶段使用的奖励模型。\n\n[虽然我建议查看这篇文章以获取关于 LLM 训练的完整指南](https://thewhitebox.ai/llms-the-backbones-of-frontier-ai/)，创建 ChatGPT 等产品过程中的最后一步是使用人类反馈强化学习，或 RLHF。\n\n这个想法是让模型改善其决策能力。因此，我们训练一个辅助奖励模型（本质上是被训练模型的几乎相同副本），它学习根据人类偏好对训练模型的结果进行排名。\n\n*问题是什么？*\n\n好吧，今天大多数奖励模型是 **ORMs，或结果监督奖励模型**。通俗来说，评估模型预测的正确程度时，他们从整体上看待，而忽略了整个“思维过程”。\n\n另一方面，**PRMs，或过程监督奖励模型，评估模型响应中的每一个步骤**。因此，他们“迫使”模型在过程的每一个步骤上都付出密切的关注和努力，这在解决如下数学方程等情况中至关重要：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8JC6sZl5UFfl3WorliQy-A.png)\n\n然而，这是一个非常非常昂贵的过程，因为偏好数据需要大量的人力构建，以便可以应用监督信号。因此，每一个训练示例都有数十个或更多的奖励来进行测量。\n\n因此，“gpt2\\-chatbot”可能在奖励训练中包含某种变体，考虑到它在生成计划和执行复杂问题解决方面的高效性。\n\n## 不禁让人兴奋\n\n考虑到gpt2-chatbot的惊人表现，以及OpenAI最近的研究和[泄露](https://www.reuters.com/technology/sam-altmans-ouster-openai-was-precipitated-by-letter-board-about-ai-breakthrough-2023-11-22/)，我们现在可能对这个东西有了相当不错的了解。\n\n我们可以肯定的是，我们很快将面临一个完全不同的存在，它将把AI的影响提升到一个新的水平。\n\n* *我们是否终于达到了大型语言模型超越人类水平表现的里程碑，就像我们在AlphaGo中所做的那样？*\n* *长推理时代，即AI征服系统2思维的时代，是否已经到来？*\n\n可能还没有。然而，无法不对接下来几个月我们即将见证的惊人发展感到高度乐观。\n\n与此同时，我想我们将不得不等待这些答案。但不会太久。\n\n> 最后，如果你喜欢这篇文章，我在我的[LinkedIn](https://www.linkedin.com/in/ignacio-de-gregorio-noblejas/)上以更全面和简化的方式免费分享类似的想法。\n\n> 如果方便的话，你可以通过[X](https://twitter.com/TheTechOasis1)与我联系。\n\n> 期待与你的联系。\n\n"},{"lang":"zh","group":"blog","slug":"blog/overcoming-llm-challenges-in-healthcare-practical-strategies-for-development-in-production-04c617954b9a","frontmatter":{"title":"克服医疗领域的法学硕士挑战：生产发展实用策略","meta_title":"克服医疗领域的法学硕士挑战：生产发展实用策略","description":"一篇关于我遇到的最常见的 LLM 开发挑战、有效的缓解策略以及职业生涯决定性的面试的文章……","date":"2024-11-08T00:20:35.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Vak28ygruWKySsH0doGoYg.png","categories":["Health","Generative AI","Machine Learning"],"author":"Rifx.Online","tags":["LLMs","healthcare","hallucinations","validation","monitoring"],"draft":false,"slug":"blog/overcoming-llm-challenges-in-healthcare-practical-strategies-for-development-in-production-04c617954b9a"},"content":"\n### 生成性人工智能\n\n\n\n### 我遇到的最常见的LLM开发挑战、有效的缓解策略以及一个职业生涯中决定性的面试错误\n\n## 引言\n\n我一直是那种深入研究一个主题并专注到痴迷的人。当我从数据科学硕士毕业时，我的痴迷是计算机视觉；特别是将计算机视觉应用于神经科学或心理健康领域。我决心成为心理健康领域的“计算机视觉工程师”（不过“机器学习工程师”也可以），尽管我的导师们劝我拓宽视野，寻找更多机会。我压制了自己内心的疑虑，坚信正确的团队会认可我的“专业知识”。\n\n\n\n幸运的是，我的理论似乎奏效了；我获得了几家心理健康公司的面试。但随之而来的是我最大的面试错误之一。在我最喜欢的公司的最后一轮面试中——一家我非常喜欢的公司——我犯了一个错误，至今回想起来仍让我感到不安。这个职位专注于NLP，处理文本数据，但我忍不住表达了我对成像数据的兴趣。*在回忆中哭泣。* 我清晰地记得，当我询问成像数据的可用性时，面试官的表情从兴奋转为担忧，因为我仍然对计算机视觉充满热情。当天晚些时候，我收到了一个礼貌的拒绝：他们喜欢我的热情，但需要一个完全致力于NLP的人。\n\n讽刺的是，我很快加入了另一家心理健康公司，并完全转向NLP工作，创建了改善临床护理的焦虑和抑郁症状检测器，并开发了提升内容可发现性的推荐系统，增加了12%的发现率。几年后，我现在是团队中的NLP/LLM数据科学家，负责6个信息提取任务、5个分类任务和5个条件摘要任务，已在15家以上医院和五个客户中部署。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*-VOHDQd88fCyRqoY9bR3hQ.png)\n\n几周前，我被要求向我更大的数据团队介绍“LLM开发101”。最初，冒名顶替综合症悄然袭来——*我能在LLM开发上分享45分钟什么呢？* 但当我创建幻灯片时，我意识到我有很多要说的，并对分享我所学到的深厚知识感到兴奋。这种兴奋促成了你现在正在阅读的这篇文章。在这篇文章中，我将讲述我在生产中遇到的一些常见LLM挑战以及帮助我解决这些问题的策略。\n\n## 1\\. 输出格式错误\n\n这可能是我遇到的最常见问题，令人惊讶的是。输出格式的可靠性可能会因我使用的模型而显著不同。例如，GPT\\-4 Turbo 通常提供一致的 JSON 输出，但 GPT\\-4o 在这方面的可靠性往往较差。在 GPT\\-4o 中，我遇到过从列表和字符串到不完整字典的各种情况，当明确请求结构化 JSON 输出时。如果这些格式问题没有被发现并且模型没有重新运行，我可能会面临数据覆盖不完整的风险。\n\n### 格式错误的影响\n\n不一致的输出格式会对下游流程产生重大影响。如果数据结构不正确，可能会导致后续处理步骤的失败，扭曲报告的准确性，甚至在未被发现的情况下导致洞察不完整。在医疗等高风险领域，我的工作涉及此处，不完整或结构错误的数据可能会带来实际影响，因此格式的一致性至关重要。\n\n### 缓解措施\n\n为了解决这个问题，我实现了**格式检查逻辑**，**验证输出结构**。如果不正确，我将重新运行模型，直到它符合预期格式。此外，我使用**日志记录**来捕获与格式相关的错误。然而，重新运行模型带来了权衡，例如增加延迟和更高的API成本。我根据数据覆盖的关键性和成本限制建立了重新运行的阈值。如果重新运行不可行，我有时会应用后处理来“修复”输出结构，尽管这种方法也存在引入错误或不一致的风险。\n\n为了说明这种方法，这里有一个示例代码片段，它请求以JSON格式返回患者数据，并包含特定的键，如`\"name\"`、`\"age\"`和`\"insurance\"`。这段代码演示了一种验证模型响应是否包含所有必需字段并遵循预期结构的方法。通过实现重试逻辑，该代码旨在确保数据一致性，减少在关键工作流程中与格式错误相关的风险。\n\n```python\ndef get_llm_response(prompt: str, required_keys: Set[str], retries: int = 3) -> Optional[Dict[str, Any]]:\n    \"\"\"\n    Calls the language model to get a response in JSON format. If the response \n    is not in the expected JSON format or lacks required keys, retries the call \n    up to `retries` times.\n    Parameters:\n        prompt (str): The prompt sent to the language model.\n        required_keys (Set[str]): A set of required keys that must be present in the JSON response.\n        retries (int): The maximum number of retries if the output format is invalid.\n    Returns:\n        Optional[Dict[str, Any]]: Parsed JSON response if successful; None if retries are exhausted.\n    \"\"\"\n    \n    for attempt in range(retries):\n        try:\n            response = openai.Completion.create(\n                model=\"gpt-4o\",\n                prompt=prompt,\n                max_tokens=100,\n                temperature=0.7\n            )\n            \n            # Attempt to parse the response as JSON\n            response_text = response.choices[0].text.strip()\n            parsed_response = json.loads(response_text)\n            \n            # Check if parsed_response is in the expected structure and contains required keys\n            if isinstance(parsed_response, dict) and required_keys.issubset(parsed_response.keys()):\n                return parsed_response\n            else:\n                print(f\"Attempt {attempt + 1}: Output format invalid or missing required keys, retrying...\")\n        except (json.JSONDecodeError, KeyError) as e:\n            print(f\"Attempt {attempt + 1}: Error parsing JSON - {str(e)}, retrying...\")\n    print(\"Max retries exceeded: Unable to get valid JSON output with required keys.\")\n    return None\n\n```\n\n## 2\\. 幻觉\n\n幻觉发生在模型创造出听起来合理但实际上并不存在的信息时。例如，当我试图从源文本中提取引用时，有时模型会选择“发挥创意”，产生类似但完全虚构的短语。在准确性至关重要的领域，如医疗保健，微小的幻觉可能导致重大问题。\n\n### 缓解\n\n我通过实施后处理逻辑来解决幻觉问题，以验证在任何信息提取任务中，提取的上下文与源文本完全匹配。为了确保细微的变动不会导致遗漏匹配，我通过去除标点符号并在比较源文本和提取文本时将其全部转换为小写来标准化文本。此外，还有其他几种策略有助于最小化幻觉。例如，**链式思维提示**，即模型解释其推理的每一步，可以产生更为扎实的输出，并降低不准确输出的可能性。在高风险应用（例如医疗保健案例）中，**人机协作检查**作为额外的审查层非常重要，有助于捕捉自动化过程可能遗漏的幻觉。最后，强调事实准确性的提示，例如指示模型“仅使用源文本中的确切短语”，可以引导模型朝着更精确的响应方向发展。\n\n## 3\\. 过时信息\n\n过时的信息管理起来可能很具有挑战性，特别是在准确性和及时性至关重要的应用中。有时，模型可能会从文档的旧部分检索信息，并将其呈现为当前信息。使用检索增强生成（RAG）时，这个问题可能变得更加复杂，因为RAG仅根据相关性而非及时性或特定文档部分来检索内容。缺少部分标签或时间戳意味着RAG可能会从文档的相关部分提取信息，而不区分这些信息是否过时，这可能导致旧信息和当前信息混合在一起。使用向量数据库的另一个挑战是，如果我们存储整个文档，则无法在没有明确标签的情况下轻松删除特定部分，从而使有效过滤无关信息变得困难。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*k9btdwyCCAb9qp92gB0PwA.png)\n\n### 缓解措施\n\n为了解决这个问题，我在提示中直接指定“当前”或“最新”数据，并使用预处理步骤在将数据传递给模型之前删除任何过时的部分。这个额外的预处理步骤确保仅保留最新、最相关的信息，帮助模型专注于提供及时和准确的响应。这个步骤不仅确保了更准确的输出，还降低了调用的成本。通过提前实施这些过滤器，我可以保持模型输出的一致性和相关性。\n\n## 4\\. 过度依赖与伦理\n\n尽管我希望我所做的工作能够被使用并且有用，但我最大的担忧是用户会对模型预测过于信任——尤其是在医疗保健领域，生成性人工智能不仅仅是在做预测，还经常在生成摘要或提取特定的患者细节。专家们对某些定义可能持有不同的看法，因此多样性和对话对于达成共识非常重要。过度依赖这些预测可能会导致护理团队限制这些对话，并忽视他们本可以更仔细检查的错误。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*6-0mq8Svxh8ATuyT)\n\n### 缓解\n\n我优先教育团队了解模型的局限性，包括其出错的倾向，并鼓励他们将人工智能视为人类专业知识的补充。在医疗保健领域，细微差别至关重要，人工干预的监督对于高影响力的案例至关重要，允许专家审查人工智能的输出，减少对过度依赖的风险。这种协作方法使人工智能能够增强专家的见解，保持高风险应用所需的可靠性和伦理完整性。\n\n## 5\\. 快速模型弃用\n\n随着人工智能的发展速度加快，模型和API版本更新频繁，版本被弃用的速度往往超出预期。如果您曾因模型版本被退役而导致工作流程意外中断，您会知道这会造成多大的干扰。在过去一年中，这种情况发生了几次，迫使我们迅速重新进行分析，以确保更新的模型版本仍能按预期表现。\n\n### 缓解\n\n将定期检查模型版本并提前处理弃用警告作为优先事项。这种主动的方法使我们能够提前规划过渡，避免最后时刻的匆忙。虽然这只是一个小步骤，但在保持顺畅操作方面却有着显著的影响。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GK08JY3dcRUS4r6Z0x6EmA.png)\n\n## 6\\. API的速率限制\n\nAPI的速率限制是一个微妙但重要的挑战，特别是在处理大量请求时。达到速率上限可能会导致延迟，减慢实时工作流程，甚至停止整个过程。在处理时间敏感数据的情况下，达到限制可能会造成严重干扰，因为工作流程会意外中断。这在医疗环境中尤其成问题，因为时机直接影响操作和患者护理。\n\n### 缓解\n\n为了缓解这一问题，我们采取了主动的方法，通过跟踪 API 使用模式来识别高峰时段并减少非必要的调用。通过错开请求和批量调用，我可以更均匀地分配负载，避免超过限制。在需求高涨且速率限制持续达到的情况下，向提供者请求额外配额可以提供一个切实可行的解决方案。平衡使用至关重要，提前了解我们的高峰时段和使用模式对于维持稳定、不中断的工作流程至关重要。\n\n## 结论\n\n这些只是我在与 LLMs 工作时遇到的六个常见问题。我没有想到自己会来到这里，但退一步思考，我意识到自己在这个领域积累了多少专业知识——我非常兴奋能够在即将发布的文章中继续分享这些经验。我希望能听到其他人遇到的挑战以及他们找到的有效缓解策略或解决方法，无论是与这些问题相关还是全新的问题。我希望这些见解对您有所帮助，并激发关于这一快速发展的领域（模型版本和 API 版本更新得太快）最佳实践的进一步讨论。\n\n"},{"lang":"zh","group":"blog","slug":"blog/qwen-new-release-the-king-of-coder-is-qwen2-5-coder-32b-8b96d442b280","frontmatter":{"title":"Qwen 新发布：编码器之王是 Qwen2.5 编码器 32B！","meta_title":"Qwen 新发布：编码器之王是 Qwen2.5 编码器 32B！","description":"Qwen2.5-Coder-32B-Instruct是最新发布的AI编码模型，表现优异，基准分数超过GPT-4o，速度达到32 tokens/s。该模型在Apache 2.0许可下开源，支持多种硬件配置，包括单个GPU 3090和较小的14B模型，适用于计算能力较低的用户。Ollama已为多个模型提供支持，用户可通过简单命令运行模型。","date":"2024-11-14T03:32:00.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*OzrZMolY75t_cdux5UGtIg.png","categories":["Programming","Technology","Machine Learning"],"author":"Rifx.Online","tags":["Qwen2.5","Coder","32B","Instruct","GPU"],"draft":false,"slug":"blog/qwen-new-release-the-king-of-coder-is-qwen2-5-coder-32b-8b96d442b280"},"content":"\n大家好！介绍一下 Qwen2\\.5\\-Coder\\-32B\\-Instruct：最新的 AI 模型正在引领编码界的风潮！\n\n\n\n这些模型大多在 Apache 2\\.0 许可下发布。基准分数高得惊人：\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*aHeNvfOvcpME0qzy6EQexQ.jpeg)\n\n如我们所见，它在开源模型中表现最佳，甚至超越了 GPT\\-4o。\n\nOllama 已经为多个模型系列提供了支持。\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*rV1xrpRXUjTFFoKwSsfeOg.png)\n\n因此，运行起来非常简单。\n\n```python\nollama run qwen2.5-coder:32b\n```\n\n32B（Q4 格式）在单个 GPU 3090 上的性能可以从以下截图中找到：\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MVQ0srQhRxX4Ifo3IqU6og.png)\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*jtH3ixeeOQGfyDzmO4Ni3A.png)\n\n它的速度是 **32 tokens/s**！超级快。我非常高兴和印象深刻。\n\n除了 32B 模型外，较小的模型在模型大小方面也表现出色。如果您的计算能力不足，可以尝试一些较小的模型。例如，我在新款配备 M4 处理器和 16GB RAM 的 Mac Mini 上尝试了 14B 模型。\n\n在基准分数之外，光标现在可以与最新的 Qwen 模型集成，包括 Qwen 2\\.5\\-Coder\\-32B\\-Instruct 和 OpenWebUI。\n\n以下是使用 Qwen 2\\.5\\-Coder\\-32B\\-Instruct（Ollama）与 OpenWebUI 的截图：\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*q-Pq3snVhkBs3e_Oxj4_Xw.png)\n\n我迫不及待想在日常工作中使用它！\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/qwen2-5-1-5b-the-future-of-mobile-ai-6bd5f29bbc84","frontmatter":{"title":"Qwen2.5 1.5b：移动AI的未来？","meta_title":"Qwen2.5 1.5b：移动AI的未来？","description":"阿里云最新 LLM 的本地测试和评估。使用 llama-cpp-python 和 DIY 提示目录。","date":"2024-10-30T12:57:39.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*awb56jkdXobA-Ip6d-QHRA.png","categories":["Natural Language Processing","Programming","Technology/Web"],"author":"Rifx.Online","tags":["Qwen2.5","NLP","summarization","retrieval","prompts"],"draft":false,"slug":"blog/qwen2-5-1-5b-the-future-of-mobile-ai-6bd5f29bbc84"},"content":"\n### 本地测试和评估阿里云最新的LLM。使用llama\\-cpp\\-python和DIY提示目录。\n\n\n\n在第一部分，我们共同探讨了阿里云团队发布的Qwen2\\.5模型系列的创新。\n\n在生成式AI基准测试中，基准测试现在是主要的*oracle*：新的LLM的有效性需要通过多个评判。你打破的基准记录越多，你就越优秀。\n\n这是赢得SOTA竞赛的方式。\n\n好吧，我不同意。尽管我们需要里程碑和更好的性能来推动AI进步，但用户体验和个人观点不能被视为无关紧要。\n\n我相信，在探索一些常用的NLP任务时，抛开聊天体验，我们必须关注回复的质量。我们是唯一需要的基准。我们的用户体验是判断一个模型是否优秀的最佳指标。模型必须足够可靠，以便在自动化工作流程中使用。\n\n顺便提一下，我已经运行了我决定称之为[RBYF — 与您作为反馈的修订基准](https://open.substack.com/pub/thepoorgpuguy/p/rbyf-is-here-revised-benchmarks-with?r=i78xo&utm_campaign=post&utm_medium=web)的测试，声称惊人的Llama3\\.2–1B\\-instruct…而Qwen2\\.5–1\\.5b则更好得多！\n\n因此，在本文中，正如承诺的那样，我们将亲自验证这个模型在日常使用中的表现有多好。\n\n回到我们自己……让我们开始吧！\n\n## 需求\n\n在这里，我们将构建一个最小的文本接口，以便能够运行模型、测试不同的任务并等待用户反馈以进行评估。\n\n需求很简单，但我建议您创建一个新的项目目录和一个虚拟环境。\n\n创建一个 `venv`（需要 Python 3\\.11\\+）：我在运行 Windows 11 的迷你电脑上进行了测试。\n\n```python\n## create the virtual environment\npython -m venv venv\n## activate the venv\nvenv\\Scripts\\activate\n## Install the dependencies \npip install llama-cpp-python==0.3.0 tiktoken\n```\n\n我们需要从 Hugging Face 的官方 qwen 仓库下载 GGUF 文件 [https://huggingface.co/Qwen/Qwen2\\.5\\-1\\.5B\\-Instruct\\-GGUF](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF)：我使用了 `qwen2.5-1.5b-instruct-q5_k_m.gguf` 版本。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*YtQJb_xyq_xcF40yRWPcZA.png)\n\n一切准备就绪！\n\n注意：如果您想添加对 GPU 加速器的不同后端支持，可以按照 [仓库中的说明](https://github.com/abetlen/llama-cpp-python#supported-backends) 进行操作。例如，我使用了 Vulkan 支持，因此在 pip 安装之前我添加了环境变量\n\n```python\n## Vulkan support - for Windows\n$env:CMAKE_ARGS = \"-DGGML_VULKAN=on\"\n```\n\n## 代码 — 主应用程序和库\n\n为了保持代码的简洁，我决定使用外部库扩展一些功能。好吧，这是一个自助库，所以这里没有秘密。\n\n您可以在我的文章中找到所有细节：\n\n为了加快速度，您可以直接 [从这里下载文件](https://github.com/fabiomatricardi/YouAreTheBenchmark/raw/main/QWEN2.5-1.5B/promptLibv2Qwen.py)：它包含了上述文章中讨论的 `promptLib` 的版本 2（名为 `promptLibv2Qwen.py`，对 `Qwen2.5-1.5B-instruct` 模型的提示进行了少量微调）。\n\n将文件保存在主目录中，并创建一个名为 `main.py` 的新文件。\n\n```python\n## Chat with an intelligent assistant in your terminal  \n## MODEL: https://huggingface.co/Qwen\n## qwen2.5-1.5b-instruct-q5_k_m.gguf\nimport sys\nfrom time import sleep\nimport warnings\nwarnings.filterwarnings(action='ignore')\nimport datetime\nfrom promptLibv2Qwen import countTokens, writehistory, createCatalog\nfrom promptLibv2Qwen import genRANstring, createStats\nimport argparse\n### PREPARING FINAL DATASET\npd_id = []\npd_task = []\npd_vote = []\npd_remarks = []\n####################Add GPU argument in the parser###################################\nparser = argparse.ArgumentParser()\nparser.add_argument(\"-g\", \"--gpu\", type=int, default=0,nargs='?',\n                    help=\"The number of layers to load on GPU\")\nargs = parser.parse_args()\nif args.gpu == None:\n   ngpu_layers = 0 \nelse:\n    ngpu_layers = args.gpu\nprint(f'Selected GPU: offloading {ngpu_layers} layers...')   \n####################INITIALIZE THE MODEL###################################\nstops = ['<!im_end|>']\ntasks = createCatalog()\nmodelname = 'qwen2.5-1.5b-instruct-q5_k_m.gguf'\n## create THE LOG FILE \ncoded5 = genRANstring(5)\nlogfile = f'logs/Qwen2.5-1.5B-it_CPP_{coded5}_log.txt'\ncsvfile = f'logs/Qwen2.5-1.5B-it_CPP_{coded5}.csv'\nlogfilename = logfile\n#Write in the history the first 2 sessions\nwritehistory(logfilename,f'{str(datetime.datetime.now())}\\n\\nYour own LocalGPT with 💻 {modelname}\\n---\\n🧠🫡: You are a helpful assistant.')  \nwritehistory(logfilename,f'💻: How can I assist you today in writing?')\n```\n\n在这里，我们只是在做准备：我们导入库，包括我们自己的 `promptLibv2Qwen` 以及 `argparse`。我想尝试一些新东西：[argparse](https://realpython.com/command-line-interfaces-python-argparse/) 是一个用于终端 Python 程序的 Python 库，您可以从命令行读取多个参数。\n\n在这里，我们只有一个参数（没有参数），带有标志 `-g` 或 `--gpu`。当您使用此参数运行 Python 代码时，我们将设置 GPU 层的数量为最大值（但您可以自行更改）。\n\n然后，我们设置一些全局变量，跨整个代码使用：任务、我们的提示集合、停止词和日志文件名。\n\n> 注意：所有日志都保存在名为 `logs` 的子目录中……所以请确保创建一个。\n\n我们还准备了所有相关信息，以便将其存储到数据集中，然后最终保存到 CSV 文件中（以便轻松创建性能矩阵）。\n\n```python\n### PREPARING FINAL DATASET\npd_id = []\npd_task = []\npd_vote = []\npd_remarks = []\n```\n\n然后，我们使用 Llama\\-CPP\\-python 将模型加载到 RAM（没有 GPU）或 VRAM（使用 GPU）中。\n\n```python\n## LOAD THE MODEL\nprint(\"\\033[95;3;6m\")\nprint(\"1. Waiting 10 seconds for the API to load...\")\nfrom llama_cpp import Llama\nllm = Llama(\n            model_path='models/qwen2.5-1.5b-instruct-q5_k_m.gguf',\n            n_gpu_layers=ngpu_layers,\n            temperature=0.1,\n            n_ctx=8192,\n            max_tokens=1500,\n            repeat_penalty=1.178,\n            stop=stops,\n            verbose=False,\n            )\nprint(f\"2. Model {modelname} loaded with LlamaCPP...\")\nprint(\"\\033[0m\")  #reset all\nhistory = []\nprint(\"\\033[92;1m\")\nprint(f'📝Logfile: {logfilename}')\n```\n\n顺便说一句，您可以在我的 GitHub 仓库中找到所有代码：\n\n下一个是一次性热身推理：模型神经网络将首次激活，所以可以把它看作是热身圈。\n\n不要害怕，我会解释代码。\n\n```python\n##################### ALIGNMENT FIRST GENERATION ##############################################\nquestion = 'Explain the plot of Cinderella in a sentence.'\ntest = [\n    {\"role\": \"user\", \"content\": question}\n]\nprint('Question:', question)\nstart = datetime.datetime.now()\nprint(\"💻 > \", end=\"\", flush=True)\nfull_response = \"\"\nfisrtround = 0\nfor chunk in llm.create_chat_completion(\n    messages=test,\n    temperature=0.25,\n    repeat_penalty= 1.31,\n    stop=stops,\n    max_tokens=1500,\n    stream=True,):\n    try:\n        if chunk[\"choices\"][0][\"delta\"][\"content\"]:\n            if fisrtround==0:\n                print(chunk[\"choices\"][0][\"delta\"][\"content\"], end=\"\", flush=True)\n                full_response += chunk[\"choices\"][0][\"delta\"][\"content\"]\n                ttftoken = datetime.datetime.now() - start  \n                fisrtround = 1\n            else:\n                print(chunk[\"choices\"][0][\"delta\"][\"content\"], end=\"\", flush=True)\n                full_response += chunk[\"choices\"][0][\"delta\"][\"content\"]                            \n    except:\n        pass      \ndelta = datetime.datetime.now() - start\noutput = full_response\nprint('')\nprint(\"\\033[91;1m\")\nrating = input('Rate from 0 (BAD) to 5 (VERY GOOD) the quality of generation> ')\nprint(\"\\033[92;1m\")\nstats = createStats(delta,question,output,rating,logfilename,'Alignment Generation',ttftoken)\nprint(stats)\nwritehistory(logfilename,f'''👨‍💻 . {question}\n💻 > {output}\n{stats}\n''')\n```\n\n我们设置了第一个用户问题，并将其放入一个众所周知的聊天格式字典中。然后我们开始计时（对速度、令牌计数等很有用……）。\n\n我们调用推理方法 `create_chat_completion()`，允许我们以聊天格式接受提示，并逐个令牌流式输出结果。\n\n由于模型的第一次回复不包含任何输出令牌（仅包含统计信息），我们使用了 try/except 语句。此外，由于我想知道何时生成第一个令牌，我们设置了一个标志并暂时停止计时，将信息保存在 `ttftoken` 变量中。\n\n在流式输出结束时，我们计算从开始到现在的时间差，并等待用户提供对生成输出的个人反馈：从 0 到 5 评分，并添加与指令提示和用户意图的一致性相关的评论。\n\n我们使用内部库 `createStats()` 来打印生成的所有统计信息，并将其保存到日志文件中。该函数的输出将类似于以下内容：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8znYCqpisviXvYgrzjWF5w.png)\n\n## 提示目录 — 我们想要测试的内容\n\n我在这里写了我的习惯。我有一个提示目录，涵盖了聊天机器人中使用的许多主要语言任务，例如总结、简短总结、随意聊天、RAG、真实RAG等等。\n\n这个想法是能够在5分钟内加载模型，并开始评估每个任务。在每次生成结束时，用户会被提示给出一个分数（从0到5的评分）并在需要时留下任何评论。\n\n这很关键：并不是所有模型都是一样的，对提示中的措辞进行小的或大的调整总是必需的。\n\n那么回到代码……因为之前的代码只是热身，现在将开始真正的while循环，遍历整个提示目录。请参见下面的工作流程……\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*EL0Q97Du6HwtcYQZ.png)\n\n代码中只有少量更改，我会指出这些更改，请耐心等待。\n\n```python\n############################# AUTOMATIC PROMPTING EVALUATION  11 TURNS #################################\nid =1\nfor items in tasks:\n    fisrtround = 0\n    task = items[\"task\"]\n    prompt = items[\"prompt\"]\n    test = []\n    print(f'NLP TAKS>>> {task}')\n    print(\"\\033[91;1m\")  #red\n    print(prompt)\n    test.append({\"role\": \"user\", \"content\": prompt})\n    print(\"\\033[92;1m\")\n    full_response = \"\"\n    start = datetime.datetime.now()\n    print(\"💻 > \", end=\"\", flush=True)\n    for chunk in llm.create_chat_completion(\n        messages=test,\n        temperature=0.15,\n        repeat_penalty= 1.31,\n        stop=stops,\n        max_tokens=1500,\n        stream=True,):\n        try:\n            if chunk[\"choices\"][0][\"delta\"][\"content\"]:\n                if fisrtround==0:\n                    print(chunk[\"choices\"][0][\"delta\"][\"content\"], end=\"\", flush=True)\n                    full_response += chunk[\"choices\"][0][\"delta\"][\"content\"]\n                    ttftoken = datetime.datetime.now() - start  \n                    fisrtround = 1\n                else:\n                    print(chunk[\"choices\"][0][\"delta\"][\"content\"], end=\"\", flush=True)\n                    full_response += chunk[\"choices\"][0][\"delta\"][\"content\"]                            \n        except:\n            pass      \n    delta = datetime.datetime.now() - start\n    print('')\n    print(\"\\033[91;1m\")\n    rating = input('Rate from 0 (BAD) to 5 (VERY GOOD) the quality of generation> ')\n    print(\"\\033[92;1m\")\n    stats = createStats(delta,prompt,full_response,rating,logfilename,task,ttftoken)\n    print(stats)\n    writehistory(logfilename,f'''👨‍💻 > {prompt}\n💻 > {full_response}\n{stats}\n''')\n    pd_id.append(id)\n    pd_task.append(task)\n    pd_vote.append(rating[:2])\n    pd_remarks.append(rating[2:])\n    id += 1\n## create dataframe and save to csv\nzipped = list(zip(pd_id,pd_task,pd_vote,pd_remarks))\nimport pandas as pdd\ndf = pdd.DataFrame(zipped, columns=['#', 'TASK', 'VOTE','REMARKS'])\n#saving the DataFrame as a CSV file \ndf_csv_data = df.to_csv(csvfile, index = False, encoding='utf-8') \nprint('\\nCSV String:\\n', df_csv_data)  \n```\n\n主要更改仅在前几行：\n\n```python\nfor items in tasks:\n    fisrtround = 0\n    task = items[\"task\"]\n    prompt = items[\"prompt\"]\n```\n\n如果你阅读了关于`promptLib`的文章，你应该不会感到惊讶：但如果你是新手，这里我们正在遍历一个字典列表，具有以下结构：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*rGcKJWNzSUrcu4wi.png)\n\n因此，对于目录中的每个条目（意味着任务和提示的对），我们提取任务描述和任务提示。\n\n```python\ntest.append({\"role\": \"user\", \"content\": prompt})\n```\n\n然后我们在一个临时列表`test`中创建聊天模板消息，并将其传递给`create_chat_template()`方法进行生成。\n\n其他内容都是一样的。\n\n保存文件，并在激活`venv`的情况下运行：\n\n```python\npython main.py\n## 如果你使用的是GPU，运行 python main.py -g\n```\n\n这将为你提供类似于下面示例的内容……\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MhhQu4lLjtU__Wjf0dSWBg.gif)\n\n请注意，在整个提示目录的末尾，会创建一个*csv*文件，其中包含所有任务的摘要！\n\n## 测试概述\n\n我使用了几个小型语言模型，从 [Qwen2–1\\.5B\\-instruct](https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF) 到 [Gemma2–2B\\-instruct](https://huggingface.co/bartowski/gemma-2-2b-it-GGUF)，再到 [Llama3\\.2–1B\\-instruct](https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF)，最后是新的 [Qwen2\\.5–1\\.5B\\-instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF)。\n\n虽然我对 [Llama3\\.2–1B\\-instruct](https://generativeai.pub/llama3-2-1b-instruct-is-ok-but-not-good-enough-28f88046b63e) 感到相当失望，但对新的 [Qwen2\\.5–1\\.5B\\-instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF) 的出色表现感到惊讶。\n\n在每次生成结束时，用户会被要求用 0 到 5 之间的分数来评估结果。**在这种情况下，用户就是我……**\n\n这种定性分析确实较为简单，因此每个分数都有相应的描述，用户可以添加评论（“一些错误信息”，“可能更好地更改提示中的措辞”）\n\n这里是定性矩阵及其描述\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*eBdPfZtfr99MsvLh6tt42w.png)\n\n## 好与坏 — 细节\n\n总结非常出色。列出长文本的主要主题也非常好。\n\nRAG 任务相当快速（即使在我的迷你 PC 上），真实的 RAG（在上下文之外提问）也很到位。\n\n所以真的很好。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*DuV3LJep_PuDqiCcAMb6Cg.png)\n\n不过也有一些不足之处：即使温度只有 `0.15`，我在两句总结任务中也得到了些虚构的信息。这不好。\n\n> 我希望通过稍微调整提示，或者将温度设为 `0` 可以解决这个问题。\n\n另一个事实是，创意写作相当糟糕：在测试中，我使用了一些固定的生成参数。\n\n```python\n        temperature=0.15,\n        repeat_penalty= 1.31,\n```\n\n对于创意写作，使用 Qwen2\\.5–1\\.5B\\-instruct 时，我们应该使用更高的 `repeat_penalty` 和更高的 `temperature`。\n\n顺便提一下，我必须说反思提示也并不差！标签的开闭没有保持（因此不易将其放入管道或工作流中），但生成的整体流程和“思维链”推理过程相当不错。\n\n```python\n<thinking>\n使用生成性人工智能（AI）在教学中的重要性不容小觑，因为这项技术在当前教育实践中具有巨大的潜力，同时也有助于塑造更具创新性的终身学习方法。\n</thinking>\n\n**思维链：**\n1. **理解影响**：AI可以自动化重复性任务，并根据学生的表现数据提供个性化反馈（例如，通过聊天机器人或自适应评估）。\n2. **增强学习体验**：通过整合生成性AI，教师可以创建更具吸引力的课程，以满足多样化的学习风格。\n3. **改善学生成果**：个性化教学可以帮助学生更好地理解和记忆知识。\n\n**反思：**\n- 将生成技术整合到教育中至关重要，因为这与日益成为数字原住民的学习者的不断变化的需求相一致，需要一种将这些技术纳入其中的教学方法。\n</thinking>\n\n<output>\n教师应学习如何利用生成性AI来增强他们的教学方法并改善学生成果。这包括理解其在自动化任务（如评分或创建个性化学习路径）方面的能力。\n\n**学习路径：**\n1. **教育技术基础课程**：首先学习提供教育技术概述的课程，包括数字工具的基础知识。\n2. **AI基础课程（Coursera）** - 通过Coursera的免费在线课程了解AI概念及其在教育中的应用。\n\n3. **生成学习的专业课程**：报名参加专注于生成学习的课程，如“生成设计”或MIT等大学提供的类似专业项目。\n4. **专业发展研讨会和会议** - 参加当地教育会议上专注于将AI整合到教学实践中的研讨会。\n\n5. **教育工作者在线社区（例如，Edmodo）**：加入在线社区，与教育工作者讨论生成技术在教育中的应用，分享资源或询问实施问题。\n6. **认证项目**：考虑通过像谷歌的“AI for Educators”项目等机构获得认可您在教学实践中整合AI相关知识和技能的认证。\n\n通过遵循这条学习路径，教师不仅可以增强自身的专业发展，还可以通过有效地整合生成技术，为教育的未来做出积极贡献。\n</output>\n```\n\n我认为，对于这个小型语言模型的规模，结果并不差！\n\n## 比较 Qwen2\\.5 和 Llama3\\.2\n\n尽管这仅仅是我个人的评估，我还是想和你分享一下。\n\n这两个模型都是为移动设备设计的，但性能差异很大。请看下面：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*T6vLgvOKdkotlV1K5x6-QQ.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*DuV3LJep_PuDqiCcAMb6Cg.png)\n\n首先，整体评分差异巨大（Llama3\\.2 为 41，Qwen2\\.5 为 57\\）。\n\n其次，如果你考虑在移动设备上可能会问的问题，语言任务方面，主要是希望有流畅的聊天体验（任务 4\\）、良好的摘要能力（任务 5 到 7\\）以及一些创造性写作（任务 11 和 13\\）。\n\n在速度方面，仅在 CPU 上运行模型，使用非常有限的迷你 PC，**我获得了平均推理速度为 14 t/s。**\n\n## 结论\n\n在 Qwen2 发布的过去三个月里，众多开发者在 Qwen2 语言模型上构建了新模型，为整个社区以及阿里云提供了宝贵的反馈。\n\n> 在此期间，我们专注于创造更智能、更有知识的语言模型。今天，我们很高兴地介绍 Qwen 家族的最新成员：Qwen2\\.5\n\n他们的声明伴随着关于新模型家族的事实：\n\n* 密集型、**易于使用**的仅解码器语言模型，提供 0\\.5B、1\\.5B、3B、7B、14B、32B 和 72B 尺寸，以及基础和指令变体。\n* 在我们最新的大规模数据集上进行预训练，涵盖多达 18T 的标记。\n* **指令跟随**方面的显著改进\n* 对系统提示的多样性**更具弹性**，增强角色扮演实施和聊天机器人的条件设置。\n* **支持高达 128K** 的上下文长度，并可以生成最多 8K 的标记。\n* 支持超过 29 种语言的多语言功能\n\n在我广泛的（但确实限于单次提示和少数 NLP 任务）测试中，我亲眼看到这些声明是基于高质量的训练数据集和精心策划的微调。\n\n该模型在移动设备上表现极为出色！\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/qwen2-5-coder-32b-instruct-a-best-coding-model-a-complete-step-by-step-guide-and-performance-b8a33ec2547f","frontmatter":{"title":"Qwen2.5-Coder 32B Instruct：最佳编码模型--完整的分步指南和性能...","meta_title":"Qwen2.5-Coder 32B Instruct：最佳编码模型--完整的分步指南和性能...","description":"Qwen2.5-Coder系列，尤其是32B模型，在代码生成、修复和推理方面展现出强大的性能，能够与GPT-4o等成熟模型相媲美。该模型支持超过40种编程语言，适应多种开发需求，提供灵活的模型选择。通过基准测试，Qwen2.5-Coder在多项指标中表现优异，适用于编码助手、自动化代码审查和教育工具等场景，推动开源代码生成的进步。","date":"2024-11-14T03:29:09.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*zjZmLCEX5URAc1wxTGnBRQ.png","categories":["Programming","Machine Learning","Generative AI"],"author":"Rifx.Online","tags":["Qwen2.5","Coder","programming","languages","repair"],"draft":false,"slug":"blog/qwen2-5-coder-32b-instruct-a-best-coding-model-a-complete-step-by-step-guide-and-performance-b8a33ec2547f"},"content":"\n### 学习如何在本地安装 Qwen2.5-Coder，探索其卓越的编码能力，并通过实践示例评估其性能\n\n\n\n## 介绍\n\n在不断发展的AI驱动编程工具领域，大型语言模型（LLMs）显著改变了开发者编写、调试和优化代码的方式。今天，我们很高兴探索**Qwen2.5-Coder**系列，这是一项开源的奇迹，承诺在代码生成和AI编码助手领域树立新的标准。该系列的最新版本**Qwen2.5-Coder-32B-Instruct**重新定义了开源编码模型的最新技术水平（SOTA），与**GPT-4o**等成熟模型的能力相媲美。让我们深入了解是什么让Qwen2.5-Coder如此“强大”、“多样”和“实用”。\n\n在本综合指南中，我们将探讨**Qwen2.5-Coder-32B**模型的核心能力。我们将演示如何使用`transformers`库，测试其编码能力并突出其实际应用。\n\n## 为什么选择 Qwen2\\.5\\-Coder?\n\n### 关键亮点\n\n1. **强大**：旗舰 **Qwen2\\.5\\-Coder\\-32B** 模型在主要编码基准测试中与 GPT\\-4 的编码能力相匹配，同时在一般和数学技能方面表现优异。\n2. **多样**：此次发布涵盖多种模型尺寸（0\\.5B, 1\\.5B, 3B, 7B, 14B, 32B），为不同资源限制提供灵活性。\n3. **实用**：旨在用于实际应用，包括代码助手和文档生成。模型采用 **Apache 2\\.0** 许可，确保可自由使用和修改，适用于商业和研究目的。\n\n## Qwen2\\.5\\-Coder系列：开放代码LLM的游戏规则改变者\n\n**Qwen2\\.5\\-Coder**系列致力于推动开源代码生成的边界。该版本专注于灵活性和可扩展性，包含多种规模的模型：**0\\.5B, 1\\.5B, 3B, 7B, 14B**，以及旗舰版**32B**。这些模型满足了不同开发者的需求，从轻量级、资源高效的模型到适用于高要求应用的高容量、功能丰富的模型。\n\n### 1\\. 强大：在代码生成中设定新标准\n\nQwen2\\.5\\-Coder\\-32B\\-Instruct 作为旗舰模型，拥有一系列能力，使其获得了 **当前 SOTA 开源代码模型** 的称号。它在以下方面表现出色：\n\n* **代码生成**：在 **EvalPlus、LiveCodeBench** 和 **BigCodeBench** 等热门基准测试中，其性能与 GPT\\-4o 相匹配，能够在多种场景中提供精确的代码生成。\n* **代码修复**：修复损坏或低效的代码在软件开发中至关重要。在测试代码修复技能的 **Aider 基准** 中，Qwen2\\.5\\-Coder\\-32B\\-Instruct 取得了令人印象深刻的 **73\\.7** 分，堪比 GPT\\-4o 的能力。\n* **代码推理**：理解和推理代码执行路径的能力对调试和优化复杂软件至关重要。该模型的能力不仅限于简单的生成 — 它在 **预测输入和输出** 方面表现出色，使其成为软件工程师的宝贵工具。\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*-g1ZGa0p2kKsQK4iD7q7cg.png)\n\n### 2\\. 多样性：支持多种编程语言和丰富的模型大小\n\nQwen2.5-Coder 的多功能性体现在其对超过 **40 种编程语言** 的支持，包括 **Haskell** 和 **Racket** 等小众语言。这种广泛的支持得益于细致的数据清理和均衡的训练，确保模型在不同的编码环境中表现最佳。\n\n* **多语言代码修复**：它的专业能力扩展到对不熟悉语言的代码修复，这可以显著减少开发者探索新技术的学习曲线。\n* **模型大小灵活性**：Qwen2.5-Coder 系列提供六种不同大小的模型，确保具有不同资源限制的开发者能够找到适合其需求的模型。支撑这些模型的 **扩展法则** 哲学意味着性能与模型大小呈正相关，赋予开发者在性能和计算资源之间选择合适平衡的灵活性。\n\n## 性能洞察：评估 Qwen2.5-Coder 模型\n\n### 1\\. Instruct 与 Base 模型\n\nQwen2\\.5\\-Coder 提供 **Base** 和 **Instruct** 版本：\n\n* **Base 模型** 旨在为希望对其特定应用进行微调的开发者提供原始模型。\n* **Instruct 模型** 经过预先调整，优化用于交互式和对话式的使用场景，非常适合基于聊天的代码助手。\n\n### 2\\. 基准比较：引领潮流\n\n在各种核心基准测试中：\n\n* **MBPP\\-3shot** 被选中用于评估基础模型，提供了一个强有力的指标来衡量它们的代码理解和合成能力。\n* **LiveCodeBench** 问题集用于评估指导模型，重点关注它们对新颖和未见过的编码问题的适应能力。\n\n结果呢？**Qwen2\\.5\\-Coder 一直优于其他开源模型**，证明了规模扩大确实与更好的性能相关。\n\n## 实用指南：使用 Qwen2.5-Coder-3B 进行代码生成与 Transformers\n\n在本实用教程中，我们将演示如何使用 `transformers` 库中的 **Qwen2.5-Coder-3B** 模型来生成代码。该模型是 **Qwen2.5-Coder 系列** 的一部分，旨在在代码生成、修复和推理方面表现出色。到本教程结束时，您将看到如何将这个强大的开源模型集成到您自己的项目中，以处理各种与代码相关的任务。\n\n### 前提条件\n\n在深入代码之前，请确保您已安装以下内容：\n\n```python\npip install torch transformers\n```\n\n此外，如果您希望充分利用模型的性能，请确保您可以访问支持 GPU 的环境。\n\n### 第一步：导入所需库\n\n我们将通过从 `transformers` 库中导入必要的组件来开始：\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n```\n\n第二步：加载模型和分词器\n\n在这一步中，我们加载 ***Qwen2\\.5\\-Coder\\-32B\\-Instruct*** 模型及其对应的分词器。`device_map=\"auto\"` 选项将自动将模型分配到可用的 GPU 或 CPU 上。\n\n> ***Qwen2\\.5\\-Coder 已在 Hugging Face 上发布了多种尺寸的模型 — 0\\.5B\\-Instruct、1\\.5B\\-Instruct、3B\\-Instruct、7B\\-Instruct、14B\\-Instruct 和 32B\\-Instruct。如果您想在本地运行它们，请选择最适合您 GPU 容量的模型。这些模型也可以在 Ollama 上获得，因此您可以在 Ollama 环境中使用它们。如果您对 Ollama 教程感兴趣，请随时在评论中告诉我！***\n\n```python\nmodel_name = \"Qwen/Qwen2.5-Coder-32B-Instruct\"\n\n## Load the model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n## Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n```\n\n### 第3步：编写聊天模板函数\n\nQwen2.5-Coder模型旨在处理类对话的提示，使用聊天模板。以下辅助函数以符合模型预期的方式设置输入提示：\n\n```python\ndef generate_response(model, tokenizer, prompt):\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": prompt}\n    ]\n    # Prepare the chat input\n    text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n    # Tokenize and prepare inputs\n    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n    # Generate response\n    generated_ids = model.generate(\n        **model_inputs,\n        max_new_tokens=512\n    )\n    # Remove prompt tokens from output\n    generated_ids = [\n        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n    ]\n  \n    # Decode and return the generated text\n    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n    return response\n```\n\n### 第4步：使用代码生成测试模型\n\n让我们运行一些示例，看看Qwen2.5-Coder-32B模型在生成Python和Java代码方面的表现如何。我们将用三个不同的编程提示进行测试。\n\n### 测试 1：快速排序算法\n\n**提示**：“编写一个快速排序算法。”\n\n```python\nprompt_1 = \"write a quick sort algorithm.\"\nresponse_1 = generate_response(model, tokenizer, prompt_1)\nprint(response_1)\n```\n\n输出：\n\n\n```python\ndef quick_sort(arr):\n    if len(arr) <= 1:\n        return arr\n    pivot = arr[len(arr) // 2]\n    left = [x for x in arr if x < pivot]\n    middle = [x for x in arr if x == pivot]\n    right = [x for x in arr if x > pivot]\n    return quick_sort(left) + middle + quick_sort(right)\n```\n\n## 示例用法：\n\narr = [3, 6, 8, 10, 1, 2, 1]\nprint(quick_sort(arr))\n\n```\n这段代码定义了一个 `quick_sort` 函数，该函数接受一个数组作为输入，并使用快速排序算法返回该数组的排序版本。该函数通过从数组中选择一个基准元素，并根据其他元素是小于还是大于基准来将它们分成两个子数组。然后对子数组进行递归排序。递归的基本情况是当数组只有一个或没有元素时，此时数组已经是排序好的。\n\n您可以将这段代码作为起点，并根据您的具体需求进行修改。如有任何进一步的问题或需要其他帮助，请告诉我！<|fim_middle|>\n```\n\n### 测试 2：计算一个数字的阶乘\n\n**提示**：“编写一个 Python 程序来计算一个数字的阶乘。”\n\n```python\nprompt_2 = \"Write a python program to find a factorial of a number.\"\nresponse_2 = generate_response(model, tokenizer, prompt_2)\nprint(response_2)\n```\n\n输出：\n\n```python\ndef factorial(n):\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n - 1)\n\n## 示例用法\nnumber = int(input(\"输入一个数字: \"))\nresult = factorial(number)\nprint(f\"{number} 的阶乘是 {result}\")\n```\n\n该程序定义了一个递归函数 `factorial`，用于计算给定数字 `n` 的阶乘。它使用基准情况 `n == 0` 返回 1，对于其他值的 `n`，它递归调用自身，直到达到基准情况。最后，程序提示用户输入一个数字，并显示该数字的阶乘。\n\n\n### 测试 3：使用逗号格式化数字\n\n**提示**：“编写一个名为‘format\\_number’的 Python 函数，该函数以一个非负数作为唯一参数。您的函数应将数字转换为字符串，并添加逗号作为千位分隔符。”\n\n```python\nprompt_3 = \"\"\"Write a python function named 'format_number' that takes a non-negative number as its only parameter.\nYour function should convert the number to a string and add commas as a thousand separators. \nFor example, calling format_number(1000000) should return '1,000,000'.\"\"\"\nresponse_3 = generate_response(model, tokenizer, prompt_3)\nprint(response_3)\n```\n\n输出：\n\n```python\ndef format_number(num):\n    # Convert the number to a string\n    num_str = str(num)\n  \n    # Initialize an empty string to store the formatted number\n    formatted_num = ''\n  \n    # Initialize a counter for the number of digits\n    digit_count = 0\n  \n    # Iterate over the digits in reverse order\n    for i in range(len(num_str) - 1, -1, -1):\n        # Add the current digit to the formatted number\n        formatted_num = num_str[i] + formatted_num\n      \n        # Increment the digit count\n        digit_count += 1\n      \n        # If the digit count is a multiple of 3, add a comma\n        if digit_count % 3 == 0 and i != 0:\n            formatted_num = ',' + formatted_num\n  \n    return formatted_num\n```\n\nYou can test the function with the following code:\n\n```python\nprint(format_number(1000000))  # Output: '1,000,000'\nprint(format_number(123456789))  # Output: '123,456,789'\nprint(format_number(1234567890))  # Output: '1,234,567,890'\n```\n\nThis function works by iterating over the digits of the number in reverse order, adding each digit to the formatted number and adding a comma every third digit. The function then returns the formatted number as a string.\n\n### 测试 4：使用数组实现栈的 Java 程序\n\n**提示**：“编写一个 Java 程序，使用数组实现栈”\n\n```python\nprompt_4 = \"\"\"Write a Java program, Implement a Stack Using Arrays.\"\"\"\nresponse_4 = generate_response(model, tokenizer, prompt_3)\nprint(response_4)\n```\n\n输出：\n\n```java\npublic class StackUsingArrays {\n    private int[] stackArray;\n    private int top;\n    private int capacity;\n\n    public StackUsingArrays(int size) {\n        stackArray = new int[size];\n        top = -1;\n        capacity = size;\n    }\n\n    public void push(int item) {\n        if (isFull()) {\n            System.out.println(\"Stack is full. Cannot push item: \" + item);\n        } else {\n            top++;\n            stackArray[top] = item;\n            System.out.println(\"Pushed item: \" + item);\n        }\n    }\n\n    public int pop() {\n        if (isEmpty()) {\n            System.out.println(\"Stack is empty. Cannot pop item.\");\n            return -1;\n        } else {\n            int item = stackArray[top];\n            top--;\n            return item;\n        }\n    }\n\n    public boolean isEmpty() {\n        return top == -1;\n    }\n\n    public boolean isFull() {\n        return top == capacity - 1;\n    }\n\n    public static void main(String[] args) {\n        StackUsingArrays stack = new StackUsingArrays(5);\n\n        stack.push(10);\n        stack.push(20);\n        stack.push(30);\n\n        System.out.println(\"Popped item: \" + stack.pop());\n        System.out.println(\"Popped item: \" + stack.pop());\n\n        stack.push(40);\n        stack.push(50);\n\n        System.out.println(\"Popped item: \" + stack.pop());\n        System.out.println(\"Popped item: \" + stack.pop());\n        System.out.println(\"Popped item: \" + stack.pop());\n    }\n}\n```\n\n这个程序定义了一个`StackUsingArrays`类，它使用一个数组来实现一个堆栈。`push`方法将一个项目添加到堆栈的顶部，`pop `方法移除并返回堆栈顶部的项目。`isEmpty`和`isFull`方法分别检查堆栈是空的还是满的。`main`方法演示了如何通过推入和弹出项来使用堆栈。\n\n### 测试 5：简单分类算法\n\n**提示**：“编写一个 Python 代码以使用 Sklearn 运行简单分类算法”\n\n```python\nprompt_5 = \"\"\"Write a Python code to run a simple classification algorithm using Sklearn\"\"\"\nresponse_5 = generate_response(model, tokenizer, prompt_3)\nprint(response_5)\n```\n\n输出：\n\n```python\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n## 生成一些示例数据\nnp.random.seed(42)\nX = np.random.rand(100, 2)  # 100 个样本，2 个特征\ny = np.random.choice([0, 1], size=100)  # 二分类\n\n## 将数据分割为训练集和测试集\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n## 创建逻辑回归模型\nmodel = LogisticRegression()\n\n## 训练模型\nmodel.fit(X_train, y_train)\n\n## 对测试集进行预测\ny_pred = model.predict(X_test)\n\n## 计算模型的准确性\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"准确性: {accuracy}\")\n```\n\n此代码生成一些示例数据，将其拆分为训练集和测试集，训练一个逻辑回归模型，对测试集进行预测，并计算模型的准确性。请随意修改代码以满足您的具体需求。\n\n\n## 第5步：分析模型的性能\n\n根据我们的测试，***Qwen2\\.5\\-Coder\\-32B\\-Instruct*** 模型展示了：\n\n* **强大的代码生成能力**，能够为经典编码问题生成高效、易于理解的解决方案。\n* **对Python语法和最佳实践的理解**，尤其是在使用Pythonic解决方案如列表推导式和格式化字符串时。\n* **灵活性**，能够适应各种提示，这对实际编程助手的使用案例至关重要。\n\n## 潜在的使用案例\n\n鉴于其性能，Qwen2.5-Coder模型可以在各种场景中有效使用，例如：\n\n* **编码助手**：集成到IDE或文本编辑器中，帮助开发人员更快地编写代码。\n* **自动化代码审查**：协助识别错误、优化代码并提出改进建议。\n* **教育工具**：通过生成示例解决方案和解释，帮助学生学习编码。\n\n## 结论\n\n**Qwen2.5-Coder**系列，特别是**32B模型**，为开发者、研究人员和希望利用AI进行代码相关任务的组织提供了强大而多功能的工具。它在EvalPlus、Aider和McEval等基准测试中的出色表现证明了其在代码生成、修复和推理方面的竞争优势。\n\n通过开源这些模型，阿里云为一个AI驱动的编码助手普及的未来铺平了道路。无论您是希望自动化重复任务的开发者，还是希望学习新编程概念的学生，Qwen2.5-Coder都是您工具箱中值得添加的可靠工具。\n\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/qwen2-5-coder-cosmos-tokenizer-opencoder-and-new-sentencetransformers-great-times-for-open-ffcacf2b29cd","frontmatter":{"title":"Qwen2.5-Coder、Cosmos Tokenizer、OpenCoder 和新的 SentenceTransformers：开放源代码的伟大时代","meta_title":"Qwen2.5-Coder、Cosmos Tokenizer、OpenCoder 和新的 SentenceTransformers：开放源代码的伟大时代","description":"文章介绍了多个开源项目的进展，包括Qwen2.5-Coder系列、Cosmos Tokenizer、OpenCoder和SentenceTransformers。Qwen2.5-Coder是一个与GPT-4竞争的开源代码LLM，具有多种模型尺寸和卓越的代码生成、修复和推理能力。Cosmos Tokenizer则是一种高效的神经分词器，专注于图像和视频压缩，提供显著的压缩率和高质量重建。OpenCoder是完全开源的代码LLM，训练于2.5万亿令牌，支持多种编程语言。SentenceTransformers通过OpenVINO的量化技术实现了CPU推理速度的显著提升。","date":"2024-11-14T03:29:09.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*IZdOavxT_8SRCxrg","categories":["Programming","Technology","Natural Language Processing"],"author":"Rifx.Online","tags":["Qwen2.5-Coder","Cosmos","OpenCoder","SentenceTransformers","OpenVINO"],"draft":false,"slug":"blog/qwen2-5-coder-cosmos-tokenizer-opencoder-and-new-sentencetransformers-great-times-for-open-ffcacf2b29cd"},"content":"\n我想强调一些引人注目的开源进展：\n\n* **Qwen2\\.5\\-Coder 系列**：一个开放源代码的代码 LLM，正在与 GPT\\-4 竞争。\n* **Cosmos Tokenizer**：一套先进的神经分词器，用于高效的图像和视频压缩。\n* **OpenCoder**：一个完全开源的代码 LLM，训练于惊人的 2\\.5 万亿个标记。\n* **SentenceTransformers 的大幅 CPU 加速**：使用 OpenVINO 的 int8 静态量化，CPU 推理速度提升 4 倍。\n\n让我们深入了解一下！\n\n## Qwen2\\.5\\-Coder 系列：开源一款与 GPT\\-4 竞争的 SOTA 代码 LLM\n\n阿里云宣布开源发布 Qwen2\\.5\\-Coder 系列——这些模型具有 **强大**、**多样** 和 **实用** 的特点，致力于推动开放代码大语言模型 (LLMs) 的发展。\n\n旗舰模型 **Qwen2\\.5\\-Coder\\-32B\\-Instruct** 作为最新的开源代码模型，设定了新的基准，匹配了 GPT\\-4 的编码能力。它在通用和数学推理方面表现出色。\n\n\n\n在之前发布的 1\\.5B 和 7B 模型基础上，他们又推出了四种额外的模型尺寸：0\\.5B、3B、14B 和 32B。Qwen2\\.5\\-Coder 现在能够满足广泛的开发者需求，涵盖六种主流模型尺寸。\n\n他们还探讨了 Qwen2\\.5\\-Coder 在实际场景中的适用性，包括代码助手和工件生成。\n\n实际例子突显了该模型在提升开发者生产力和代码质量方面的潜力。\n\n**基准成就**\n\n* **代码生成**：Qwen2\\.5\\-Coder\\-32B\\-Instruct 模型在流行的代码生成基准 EvalPlus、LiveCodeBench 和 BigCodeBench 上取得了顶尖性能。\n* **代码修复**：认识到调试在软件开发中的重要性，Qwen2\\.5\\-Coder\\-32B\\-Instruct 在代码修复任务中表现出色。在 Aider 基准上得分 73\\.7，表现与 GPT\\-4 相当，帮助开发者高效修复代码错误。\n* **代码推理**：该模型展现了先进的代码推理能力，学习代码执行过程并准确预测输入和输出。在 Qwen2\\.5\\-Coder\\-7B\\-Instruct 的出色表现基础上，32B 模型进一步提升了推理能力。\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*fzH6YE-yl_GrEXwz)\n\n* **多语言支持**：Qwen2\\.5\\-Coder\\-32B\\-Instruct 精通 40 多种编程语言。在 McEval 上得分 65\\.9，在 Haskell 和 Racket 等语言中表现出色，这得益于在预训练期间独特的数据清洗和均衡策略。\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*rhyc0T3UZp_2x0r2)\n\n您可以在 [github](https://proxy.rifx.online/https://github.com/QwenLM/Qwen2.5-Coder) 上找到更多信息。\n\n## Cosmos Tokenizer: 高级神经分词器用于高效的图像和视频压缩\n\n**Cosmos Tokenizer** 是一套全面的神经分词器，专为图像和视频设计。\n\n您现在可以将原始视觉数据转换为高效的压缩表示。\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*v8k8jLbZ4LYFRUBc.jpg)\n\n通过无监督学习发现潜在空间，这些分词器促进了大规模模型训练，并减少了推理过程中的计算需求。\n\n**分词器类型**：\n\n* **连续分词器**：将视觉数据映射到连续嵌入，适用于从连续分布（如稳定扩散）中采样的模型。\n* **离散分词器**：将视觉数据映射到量化索引，应用于依赖交叉熵损失进行训练的模型，如 VideoPoet。\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*a6Hvj8hXJUpOAp9Ber781g.png)\n\n**关键特性**：\n\n* **高压缩与质量保留**：在显著的压缩率与高质量重建之间取得平衡，保留潜在空间中的重要视觉细节。\n* **轻量级时间因果架构**：利用因果时间卷积和注意力层保持视频帧的时间顺序，实现图像和视频的无缝分词。\n* **在多样化数据上训练**：在各种纵横比和类别的高分辨率图像和长视频上进行训练，使其在推理时对时间长度不敏感。\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*lBO1omEzlr18SPB1zF-vMw.png)\n\n**性能亮点**：\n\n* **卓越的压缩率**：提供显著的压缩能力，速度比以前的方法快**12倍**。\n* **高质量重建**：在峰值信噪比（PSNR）方面显著提升，在 DAVIS 视频数据集上超越现有方法超过 **+4 dB**。\n* **高效的分词**：能够在 NVIDIA A100 GPU（80GB 内存）上编码高达 **8 秒 1080p** 和 **10 秒 720p** 视频。\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*uYQttZw-MDOCK3oxxLcHbw.png)\n\n**评估与资源**：\n\n* **TokenBench 数据集** 是一个新数据集，旨在标准化视频分词器评估，涵盖机器人、驾驶和体育等类别。\n* **公开可用性**：具有 8x 和 16x 空间压缩，以及 4x 和 8x 时间压缩的预训练模型可在 [GitHub — NVIDIA/Cosmos-Tokenizer](https://proxy.rifx.online/https://github.com/NVIDIA/Cosmos-Tokenizer) 获取。\n\n有关更多信息，请参阅 [NVIDIA 的官方博客文章](https://proxy.rifx.online/https://research.nvidia.com/labs/dir/cosmos-tokenizer/)。\n\n> *感谢您抽出时间来到这里！*\n\n> *如果您喜欢这篇文章，请花一点时间 [**在 Medium 上关注我们**](https://proxy.rifx.online/https://medium.com/@datadrifters/subscribe)，为这篇文章点赞 50 次并留下评论。*\n\n> *我们还在进行一个基于小组的培训 **[用于构建全栈 GenAI SaaS 应用程序](https://proxy.rifx.online/https://forms.gle/8mfFH4wjhF7BbtRY9)**，也期待在里面见到您！*\n\n## OpenCoder: 完全开源的代码 LLM，训练于 2.5T 令牌\n\n**OpenCoder** 介绍了一系列新的开源代码语言模型，包括 **1.5B** 和 **8B** 参数规模的基础模型和聊天模型。\n\nOpenCoder 支持英语和中文，完全从一个庞大的数据集 **2.5 万亿令牌** 中训练而成，包含 90% 的原始代码和 10% 的代码相关网络数据。\n\n该模型的性能水平可与领先的代码 LLM 相媲美。\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*5rd863dHI-W_2ei7.png)\n\n**关键贡献**：\n\n* 团队提供了模型权重、推理代码、训练数据、数据处理管道和详细的训练协议，使研究人员和从业者能够在此基础上进行构建和创新。\n* 他们还推出了 **RefineCode 数据集**，这是一个高质量、可重复的代码预训练语料库，包含 **9600 亿令牌**，涵盖 **607 种编程语言**。\n\n更多信息请查看 [官方公告](https://proxy.rifx.online/https://opencoder-llm.github.io/).\n\n## SentenceTransformers 加速 CPU 推理，速度提升 4 倍\n\n最新发布的 **SentenceTransformers** 引入了显著的性能提升，使用 **OpenVINO 的 int8 静态量化** 在 CPU 推理中实现高达 **4 倍的速度提升**。\n\n此更新优化了开发者在处理大规模自然语言处理任务时的训练和推理工作流程。\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Pd9ESPxjKHaHVgV15pCQig.png)\n\n**主要增强**：\n\n* **OpenVINO int8 静态量化**：利用 OpenVINO 的量化技术，模型在保持准确性的前提下实现了卓越的推理速度。此优化超越了现有后端，提高了在 CPU 架构上的部署效率。\n* **基于提示的训练**：支持使用提示进行训练，提供了一种简单的方法来提升性能，而无需额外的计算开销。\n* **在 NanoBEIR 上的便捷评估**：通过使用 NanoBEIR，这个强大的信息检索基准 BEIR 的子集，便于更快速地评估模型性能。\n* **PEFT 兼容性**：现在支持 **参数高效微调（PEFT）**，通过允许轻松添加和加载适配器，实现更高效的模型定制。\n\n您可以在 [github](https://proxy.rifx.online/https://github.com/UKPLab/sentence-transformers/releases/tag/v3.3.0) 上找到更多信息。\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/rag-llm-and-pdf-conversion-to-markdown-text-with-pymupdf-03af00259b5d","frontmatter":{"title":"RAG/LLM 和 PDF：使用 PyMuPDF 转换为 Markdown 文本","meta_title":"RAG/LLM 和 PDF：使用 PyMuPDF 转换为 Markdown 文本","description":"采用 markdown 文本格式输入数据可提高生成的文本质量","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*swPjVuudAhsoRiiw3Ee32w.png","categories":["Programming","Technology","Technology/Web"],"author":"Rifx.Online","tags":["markdown","PyMuPDF","LLM","RAG","PDF"],"draft":false,"slug":"blog/rag-llm-and-pdf-conversion-to-markdown-text-with-pymupdf-03af00259b5d"},"content":"\n\n\n### 以Markdown文本格式输入数据可以提高生成文本的质量\n\n\n\n## 介绍\n\n在**大型语言模型（LLMs）**和**检索增强生成（RAG）**环境中，以**markdown文本格式**输入数据具有**重要意义**。以下是一些详细考虑因素。\n\n**LLMs** 是强大的语言模型，可以生成连贯且具有上下文相关性的文本。然而，它们有时可能会产生缺乏事实准确性或上下文的响应。通过结合基于检索的方法（如RAG），我们可以提高生成文本的质量。\n\n**RAG** 使得将**外部数据**——在LLM的训练数据中之前缺失的数据——整合到文本生成过程中成为可能。这种包含减少了“幻觉问题”，并增强了文本响应的相关性。\n\n## 为什么选择 Markdown 用于 LLM？\n\n**Markdown** 是一种轻量级标记语言，允许用户使用简单的语法格式化纯文本。它广泛用于创建结构化文档，特别是在 GitHub、Jupyter 笔记本和各种内容管理系统上。当将数据输入到 LLM 或 RAG 系统时，使用 Markdown 格式提供了几个好处：\n\n1. **结构化内容**：Markdown 允许您将信息组织成标题、列表、表格和其他结构化元素。这种结构有助于更好地理解和上下文保留。\n2. **富文本**：Markdown 支持基本格式，如粗体、斜体、链接和代码块。在输入数据中包含富文本可以增强语言模型的上下文。\n3. **嵌入链接和引用**：Markdown 允许您嵌入超链接、脚注和引用。在 RAG 场景中，这对于引用外部来源或提供额外上下文至关重要。\n4. **易于创作**：Markdown 具有可读性，易于编写。作者可以高效地创建内容，而无需复杂的格式化工具。\n5. **分块**：对于 RAG 系统至关重要，分块（也称为“拆分”）将大量文档拆分为更易处理的部分。通过支持 MD 格式的 PyMuPDF 数据提取，我们支持分块以保持具有共同上下文的文本在一起。**重要的是，MD 格式的 PyMuPDF 提取允许进行 [第 3 级分块](https://readmedium.com/five-levels-of-chunking-strategies-in-rag-notes-from-gregs-video-7b735895694d#b123)**。\n\n总之，在 LLM 和 RAG 环境中使用 Markdown 文本格式可以确保更准确和相关的结果，因为它提供了更丰富的数据结构和更相关的数据块负载给您的 LLM。\n\n## PyMuPDF 支持 PDF 的 Markdown 转换\n\n自推出以来，PyMuPDF 一直能够从 PDF 页面中提取文本、图像、矢量图形，并且从 2023 年 8 月起，还能够提取表格。这些对象类型各自有其提取方法：文本有一种，表格、图像和矢量图形则有其他方法。为了满足 RAG 的要求，我们将这些不同的提取方式合并，生成一个统一的 **Markdown** 字符串，以一致地表示页面的整体内容。\n\n所有这些都实现为 [一个 Python 脚本](https://github.com/pymupdf/RAG/blob/main/helpers/pymupdf_rag.py)。它可以被其他脚本作为模块导入，或者在终端窗口中通过以下命令行调用：\n\n`$ python pymupdf_rag.py input.pdf [-pages PAGES]`\n\n它将生成一个 **Markdown** 格式的文本文件（称为 `input.md`）。可选参数 `PAGES` 允许将转换限制为 PDF 总页面的一个子集。如果省略，则处理整个 PDF。\n\n## Markdown 创建细节\n\n### 选择要考虑的页面\n\n“`-pages`” 参数是一个字符串，由所需的页面编号（从1开始）组成，用于考虑进行markdown转换。可以给出多个页面编号规范，使用逗号分隔。每个规范可以是一个整数或两个用“`-`”连接的整数，指定一个页面范围。以下是一个示例：\n\n“`-pages 1–10,15,20-N`”\n\n这将包括第1页到第10页、第15页以及第20页到文件末尾（大写“N”被视为最后一页的编号）。\n\n### 识别标题\n\n在调用时，程序检查给定页面上的所有文本并找出最常用的字体大小。该值（以及所有较小的字体大小）被假定为 **正文文本**。较大的字体大小被假定为 **标题文本**。\n\n根据它们在字体大小层级中的相对位置，标题文本将前面加上一个或多个 markdown 标题 `#` 标签字符。\n\n### 按页面区域识别处理模式\n\n每个页面上的所有文本首先将被分类为**标准**文本或**表格**文本。然后，页面内容将从上到下提取，并转换为Markdown格式。\n\n这最好通过一个例子来解释：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*u5fv2aAIvDaaAd6H.png)\n\n该页面显示的内容代表典型情况：\n\n* 两个表格，具有部分重叠的垂直位置。一个表格没有标题，另一个表格有**外部**列标题。\n* 有一行**标题**和多个级别的**标题**。\n* **正文文本**包含多种样式细节，如**粗体**、*斜体*和`行内代码`。\n* 有序和无序列表。\n* 代码片段。\n\n布局分析将确定三个区域并选择适当的处理模式：**(1)** 文本，**(2)** 表格，**(3)** 文本。\n\n生成的Markdown文本忠实地反映了上述内容——在这种格式中尽可能做到。\n\n作为一个例子，让我们看一下具有外部标题的表格的输出：\n\n```python\n|Column1|Column2|\n\n|---|---|\n\n|Cell (0, 0)|Cell (0, 1)|\n\n|Cell (1, 0)|Cell (1, 1)|\n\n|Cell (2, 0)|Cell (2, 1)|\n```\n这是与GitHub兼容的格式，具有最小的可能令牌大小——这是保持输入到RAG系统的小型化的重要方面。\n\n**列边框**由“`|`”字符表示。如果文本行后面跟着“`|---|---| …`”形式的行，则假定该文本行是**表头**。完整的**表格定义**必须前后至少有一行空行。\n\n请注意，由于技术原因，Markdown表格必须有一个标题，因此如果没有外部标题，将选择第一行作为表头。\n\n为了确认整体准确性，以下是Markdown解析器如何处理完整页面的示例：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Ge83uj7FiM4T6XFn)\n\n## 以编程方式调用 Markdown 转换器\n\n除了在命令行中执行程序外，Markdown 转换也可以通过程序请求：\n\n```python\nimport fitz\nfrom pymupdf_rag import to_markdown  # import Markdown converter\n\ndoc = fitz.open(“input.pdf”)  # open input PDF\n\n## define desired pages: this corresponds “-pages 1-10,15,20-N”\npage_list = list(range(9)) + [14] + list(range(19, len(doc) – 1))\n\n## get markdown string for all pages\nmd_text = to_markdown(doc, pages=page_list)\n\n## write markdown string to some file\noutput = open(“out-markdown.md”, “w”)\noutput.write(md_text)\noutput.close()\n```\n\n## 结论\n\n通过集成 PyMuPDF 的提取方法，PDF 页面的内容将被忠实地转换为可用作 RAG 聊天机器人的输入的 Markdown 文本。\n\n请记住，成功的 RAG 聊天机器人的关键在于它能够访问的信息的质量和完整性。\n\n启用 PyMuPDF 的 Markdown 提取确保从 PDF 中获取这些信息不仅是可能的，而且是简单的，展示了该库的强大和对开发者的友好。祝编码愉快！\n\n### 源代码\n\n* [RAG/helpers/pymupdf\\_rag.py (github.com)](https://github.com/pymupdf/RAG/blob/main/helpers/pymupdf_rag.py)\n\n### 参考文献\n\n* [5 Levels of Text Splitting](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb)\n\n### 相关博客\n\n* [使用 ChatGPT API 和 PyMuPDF 构建 RAG 聊天机器人 GUI](https://readmedium.com/building-a-rag-chatbot-gui-with-the-chatgpt-api-and-pymupdf-9ea8c7fc4ab5)\n* [使用 ChatGPT 和 PyMUPDF 创建 RAG 聊天机器人](https://readmedium.com/creating-a-rag-chatbot-with-chatgpt-and-pymupdf-f6c30907ae27)\n* [RAG/LLM 和 PDF：增强文本提取](https://readmedium.com/rag-llm-and-pdf-enhanced-text-extraction-5c5194c3885c)\n\n"},{"lang":"zh","group":"blog","slug":"blog/ragate-adaptive-rag-for-conversational-ai-94b5ca469b7d","frontmatter":{"title":"RAGate：用于对话式人工智能的自适应 RAG","meta_title":"RAGate：用于对话式人工智能的自适应 RAG","description":"RAGate是一种自适应机制，旨在优化对话AI系统中外部知识与内部知识的使用。通过动态评估何时检索外部信息，RAGate提高了响应的相关性和准确性，避免了对外部知识的过度依赖。该方法具有多种变体，如RAGate-Prompt、RAGate-PEFT和RAGate-MHA，适用于医疗、客户支持等多个领域，能够显著提升用户体验和系统性能。","date":"2024-11-14T03:29:09.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8wzI-5BRV1-br0e3MBVD2g.png","categories":["Chatbots","Natural Language Processing","Machine Learning"],"author":"Rifx.Online","tags":["RAGate","conversational","retrieval","latency","personalization"],"draft":false,"slug":"blog/ragate-adaptive-rag-for-conversational-ai-94b5ca469b7d"},"content":"\n\n\n构建对话 AI 系统是困难的！！！\n\n这虽然可行，但也**复杂、耗时且资源密集**。\n\n挑战在于设计能够理解和生成类人响应的系统，并确保这些系统能够有效地与用户互动，适应对话的细微差别。\n\n非常流行的**RAG（检索增强生成）**通过将外部知识与 LLM 的内部知识无缝集成，彻底改变了对话 AI。通过将 RAG 应用于您的商业数据，您的客户可以用自然语言询问他们的数据，从而促进无缝互动。\n\n**然而，有一个警告：** 在使用 RAG 时，很明显并不是每个查询都需要从“外部知识”中获取答案。过度依赖外部来源可能会破坏真正的互动。就像与某人交谈，对于每个问题都要去翻一本书来构建您的回答，即使您已经对该主题有更深入的理解。更糟糕的是，您可能找不到任何关于该主题的书，最终回答“我不知道”，尽管您拥有可以提供更深刻答案的内部知识。\n\n显然，在使用 RAG 时，需要一种机制来确定在推理时何时利用“外部知识”与“内部知识”。\n\n引入**RAGate**——一种二元开关，旨在动态评估何时利用外部知识以及何时依赖内部见解。由 Xi Wang、Procheta Sen、Ruizhe Li 和 Emine Yilmaz 提出，并于 2024 年 7 月发布于 [**ArXiv**](https://proxy.rifx.online/https://arxiv.org/abs/2407.21712) **（用于对话系统的自适应检索增强生成）。**\n\n让我们通过示例进一步了解。\n\n## 什么是对话式人工智能？\n\n**对话**是个体之间思想、情感和信息的交流，适应语调、上下文和细微的暗示，以引导互动。人类天生适合进行对话，因为具备情商、社交能力和文化接触等特质，这帮助我们理解细微差别并适应不同的社交环境。\n\n**对话式人工智能**旨在通过技术复制这种类人互动，理解和生成自然、符合上下文的、引人入胜的回应。它适应用户输入，使互动流畅而动态，像人类之间的对话。\n\n## 什么是 AI 系统的外部知识和内部知识？\n\n在开头段落中，我提到了两个关键术语——外部知识和内部知识。让我们花一点时间来澄清这些概念，因为理解它们将使学习 RAGate 变得更加容易。\n\n**External knowledge** 包含不属于 AI 模型的固有信息，而是从外部来源检索的。来源包括结构化数据存储库、API、指南、常见问题解答和网络来源等非结构化知识库。外部知识的主要作用是提供事实、最新和上下文相关的信息，从而提高 AI 响应的准确性和全面性。\n\n**Internal knowledge** 指的是嵌入在 AI 模型中的内置知识和处理能力，这些知识和能力基于其训练数据。来源包括来自多样化数据集的预训练知识，包括语言模式、语法、共享事实和一般世界知识、来自过去交互的记忆的上下文意识，以及 AI 的语义理解和理解能力。\n\n## RAG 和护栏 — 强大的组合，但有局限性！\n\nRAG 结合了两个强大的元素：(1\\) 大型语言模型（LLMs）处理自然语言的能力，以解释和生成类人文本。 (2\\) 检索和增强外部最新信息的能力。\n\n许多 RAG 实现都包含 **护栏**、约束或规则，以引导系统的行为朝着负责任和领域界定的人工智能。这些护栏通常优先使用外部知识，而不是模型的内部知识，以确保响应的可预测性。严格应用这些护栏有时可能导致次优结果：\n\n* **过度依赖外部来源：** 系统可能被迫寻求外部信息，即使是对于一般问题，LLM 的内部知识可能已经足够。\n* **响应流畅性降低的潜在风险：** 通过限制内部知识，系统在某些情况下可能产生不那么自然或上下文不适当的响应。\n* **延迟增加：** 不断检索外部信息可能导致响应时间比依赖内部知识时更慢。\n* **错失机会：** 嵌入在 LLM 参数中的广泛知识可能未被充分利用，可能会错过有价值的见解或联系。\n\n## RAGate的平衡艺术\n\nRAGate，即**检索增强生成门**，通过自适应地确定何时将外部知识纳入响应，从而增强对话AI系统的能力。\n\n[RAGate研究](https://proxy.rifx.online/https://arxiv.org/abs/2407.21712)探讨了对话系统中**自适应增强**的需求，并将RAGate提出为一种**门控模型**，预测何时检索外部知识是有益的。论文提供了大量实验和分析，展示了RAGate在提高基于RAG的对话系统的响应质量和生成信心方面的有效性。\n\n\n\n## RAGate 示例\n\n**场景：** 用户正在与一个专注于医疗保健的聊天机器人互动，该机器人根据一般健康原则和医学知识提供个性化的健康建议。\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*o0mWnGGefJ0TyDv1u14njw.png)\n\nRAGate 可以通过平衡内部和外部知识进一步增强对话。它允许人工智能使用内部医学知识提供一般信息，同时检索最新研究。它甚至可以智能地综合来自多个来源的数据进行全面分析，基于患者详情提供个性化见解，并过滤外部信息以优先考虑最相关的内容，从而减少信息过载。\n\n## RAGate 的变体\n\n如论文中所述，RAGate 提供了 3 个变体 — **RAGate\\-Prompt**、**RAGate\\-PEFT (参数\\-高效微调)** 和 **RAGate\\-MHA (多头注意力)**。\n\nRAGate 的每个变体 — Prompt、PEFT 和 MHA — 采用不同的方法来整合外部知识，旨在提高 AI 生成响应的相关性和准确性。\n\n以下是快速比较表：\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*3dZg6rHlqmddK1ZQqqu_Aw.png)\n\n## 如何实现 RAGate？\n\n本文提供了实现 RAGate 的逐步指南：\n\n1. **定义问题**：这一步至关重要，因为它涉及到识别您希望通过 RAGate 增强的对话任务。确定对话的范围和您希望覆盖的特定领域（例如，餐厅推荐、旅行规划）。\n2. **选择语言模型**：选择适合的大型语言模型（LLM）作为您对话系统的基础。可选模型包括 Llama、GPT-2 或其他基于变换器的架构。\n3. **收集和标注数据**：收集与您的对话领域相关的数据集。KETOD 数据集是一个优秀的例子，它包括标注的对话和知识片段。确保您的数据集有明确的标签，指示何时需要知识增强。\n4. **开发知识检索系统**：实现一个知识检索机制，以便在需要时获取相关的外部信息。可以考虑流行的技术，如稠密段落检索或图结构知识库。\n5. **实现 RAGate 机制**：创建二元知识门函数（RAGate），以确定何时用外部知识增强响应。这涉及到 **上下文分析和门控函数**。\n6. **探索 RAGate 变体**：根据论文中讨论的方法开发不同的 RAGate 变体：\n* **RAGate-Prompt**：使用自然语言提示与预训练语言模型来确定是否需要增强。\n* **RAGate-PEFT**：采用参数高效微调技术（例如，QLoRA）来训练您的语言模型，以便更好地进行决策。\n* **RAGate-MHA**：利用多头注意力机制来评估上下文并交互式地检索知识。\n\n7. **训练模型**：使用标注数据集微调您的 LLM，采用各种 RAGate 变体。结合门控机制的训练，以增强模型有效预测知识增强需求的能力。\n\n8. **评估性能**：进行广泛的实验以验证 RAGate 的有效性。分析指标如：\n\n* **精确度、召回率、F1 分数**：评估门控函数的分类性能。\n* **BLEU、ROUGE、BERTScore**：用于评估生成响应与真实值的质量。\n* **置信分数**：测量生成输出的置信度，以确保高质量的响应。\n\n9. **部署系统**：将 RAGate 启用的对话系统集成到您的应用或服务中。确保系统能够处理实时查询，并动态决定知识增强。\n\n10. **迭代和改进**：持续收集用户反馈和交互数据，以优化模型。分析系统在上下文或相关性方面可能遇到的困难，并相应调整训练或检索机制。\n\n## 收获\n\n总之，RAGate 代表了对话式人工智能的重大进步，通过智能地平衡内部和外部知识，提供更相关、高效和个性化的响应。RAGate 的应用范围广泛，涵盖了医疗、客户支持、教育、法律服务、金融等多个行业。通过增强人工智能提供量身定制的实时信息的能力，RAGate 有潜力彻底改变企业和个人与技术的互动方式，提高决策能力、用户体验和整体系统性能。\n\n"},{"lang":"zh","group":"blog","slug":"blog/rbyf-qwen2-5-3b-instruct-is-damn-good-dcf443cacc63","frontmatter":{"title":"RBYF：Qwen2.5–3B-instruct 非常棒。","meta_title":"RBYF：Qwen2.5–3B-instruct 非常棒。","description":"修改后的基准测试并附上您的反馈：阿里巴巴Qwen的全新3B模型是一个了不起的模型，我可以证明这一点！","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*NWaBtJ64TLUoUHv4F1qJpg.png","categories":["Programming","Technology","Science"],"author":"Rifx.Online","tags":["Qwen","NLP","multimodal","RBYF","evaluation"],"draft":false,"slug":"blog/rbyf-qwen2-5-3b-instruct-is-damn-good-dcf443cacc63"},"content":"\n### 修订基准：以您为反馈的全新3B模型来自阿里巴巴Qwen，是个令人惊叹的模型，我可以证明这一点！\n\n\n\n涌现属性的错觉在很大程度上是评估这些模型所使用的指标的产物。这是一个事实。\n\n几周前，我决定做一个小反叛，放弃所有官方基准，开始自己做基准测试！\n\n这就是这个完全虚构的首字母缩略词RBYF的意义：以您为反馈的修订基准。其基本原则是，没有比您更好的评判者来验证一个大型语言模型的优劣。\n\n老实说，我专注于小型语言模型。我没有专用的GPU，计算资源有限。但我同样同意[LLMWare反叛原则第一条](https://readmedium.com/getting-work-done-with-genai-just-do-the-opposite-10-contrarian-rules-that-may-actually-work-634501602a27)：\n\n使用小模型，而不是大模型。\n\n在这篇文章中，我将向您展示我对qwen2.5–3b-instruct的评估结果。结果真的很好！\n\n> 免责声明：所有与结果相关的提示均可在我的GitHub存储库中找到：\n\n## 少即是多\n\n缩放法则描述了模型性能如何随着参数和训练数据数量的增加而改善。这个原则推动了对LLM新能力的探索。\n\n> 仅仅通过增加模型的规模，我们可以解锁新的能力……\n\n缩放法则描述了模型性能与参数数量和训练数据之间的关系。随着模型变得更大并在更多数据上进行训练，我们期待它们的性能得到提升。这导致了对越来越大LLM的无休止追求，希望能够解锁新的能力。\n\n涌现属性是指在复杂系统中个体组件之间的相互作用中产生的属性。通过孤立地研究组件，无法预测或理解这些属性。在LLM的情况下，希望随着这些模型变得更大和更复杂，它们会展现出意想不到的新能力。\n\n这是一个童话故事。\n\n在过去几周，我们亲眼目睹了经过过度训练和精心策划的小型语言模型可以表现得与它们的大型兄弟一样好。这对所谓的涌现能力是一个重击，反击了缩放法则。Gemma2–2B、Qwen2.5–3B，甚至最新的Llama3.2–3B都是远胜于旧的SOTA 7B模型的更好模型。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*-EjpdEky-Hf3WEQn.png)\n\n## Qwen2.5模型系列\n\n阿里云在九月中旬发布了他们的旗舰模型系列Qwen2.5。\n\n> 阿里云在Qwen的革命性旅程中再次展现出强大的创新领导力\n\nQwen2.5是阿里巴巴集团Qwen团队的大型语言模型和大型多模态模型系列。这些语言模型和多模态模型在大规模多语言和多模态数据上进行了预训练，并在高质量数据上进行了后训练，以便与人类偏好对齐。Qwen能够理解自然语言、生成文本、理解视觉、理解音频、使用工具、角色扮演、作为AI代理进行操作等。\n\n**新款Qwen2.5的亮点在于经过精心策划的训练数据集。** 通过检查小模型的性能，您可以清楚地理解这一点。\n\n如果该系列的小型语言模型表现良好，意味着训练和数据集经过高度修订和策划。\n\n以下是一些数字：\n\n* 密集型、易于使用的解码器语言模型，提供0.5B、1.5B、3B、7B、14B、32B和72B尺寸，以及基础和指令变体。\n* 在我们最新的大规模数据集上进行了预训练，涵盖多达18T个标记。\n* 在指令跟随、生成长文本（超过8K个标记）、理解结构化数据（例如，表格）和生成结构化输出（尤其是JSON）方面有显著改进。\n* 对系统提示的多样性更具韧性，增强了角色扮演的实现和聊天机器人的条件设置。\n* 支持的上下文长度可达128K个标记，并且可以生成多达8K个标记。\n* 支持超过29种语言的多语言，包括中文、英文、法文、西班牙文、葡萄牙文、德文、意大利文、俄文、日文、韩文、越南文、泰文、阿拉伯文等。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*3tnTS_UCImBRBDeKmFyjVQ.png)\n\n## Qwen2.5–3B-instruct\n\n尽管为了推动人工智能的发展，我们需要里程碑和更好的表现，但用户体验和个人观点不能被视为无关紧要。\n\n我认为，探索一些常用的自然语言处理任务，抛开聊天体验，我们必须关注回复的质量。而我们是唯一所需的基准。**我们的用户体验是理解模型好坏的最佳指标**。模型必须足够可靠，以便在自动化工作流程中使用。\n\n顺便提一下，我已经运行了我决定称之为[RBYF — 修订基准与您反馈](https://open.substack.com/pub/thepoorgpuguy/p/rbyf-is-here-revised-benchmarks-with?r=i78xo&utm_campaign=post&utm_medium=web)的项目，在[Qwen2.5–1.5b-instruct](https://ai.gopubby.com/qwen2-5-1-5b-the-future-of-mobile-ai-6bd5f29bbc84)上：您可以阅读详细信息。在文章中，我还解释了如何创建您的测试基准。所描述的方法与我用于Qwen2.5–3B的相同。\n\n让我们从所有任务的整体表现开始。该模型已由我评估（在这种情况下是我自己的反馈），基于以下显示的定性矩阵。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*rdVHfCDWX9jlvtiq)\n\n总体得分为62/70 = 8.8\n\n好的，但Qwen2.5–3B-instruct是基于什么得到了这个评估分数呢？\n\n## 测试概述\n\n这个想法是进行公平的用户反馈，而不是基于标准基准和框架的自动化反馈。一个小型语言模型能否满足用户在主要自然语言处理任务上的意图？\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*wgngfGvSebeoH3YxTdvc8A.png)\n\n我们希望验证用户意图和响应质量。以下是每个任务的详细信息：\n\n### 介绍\n\n验证模型如何回应初始问候并谈论自身。\n\n### 一句话解释\n\n综合与总结。最终评估的重点在于模型是否能够将回复压缩为仅一句话。\n\n### 用三段话解释\n\n用户的意图是获得对文本的智能解释，这种解释必须适合分成三段。SML通常会发现这很困难，因为他们总是会添加一个总结段落。\n\n### 说“我准备好了”。\n\n在一个基于聊天回合的应用中，遵循指令的模型通常会被要求首先阅读提供的文本，然后进行某种分析。通常情况下，SML 无法做到这一点……\n\n### 总结\n\n基本总结，没有限制。在这里，我们想评估总结是如何基于文本的，而不是凭空捏造事实。\n\n### 总结为两句\n\n基本总结，限制为两句话。在这里，我们希望评估总结是否基于文本，而不是虚构的事实，同时确保遵守两句的限制。\n\n### 列出三个主要关键点 — 格式输出\n\n```python\nkey_points = [\n    \"SML必须以特定格式输出。\",\n    \"此提示要求创建一个包含3个关键点的列表。\",\n    \"输出格式应为Python列表。\"\n]\n```\n\n### 目录\n\n这个任务对许多 SML 来说相当困难。该提示需要一些调整，否则模型会返回一个 markdown 表格。用户希望按照提供的文档结构获得一个有序的主题列表。\n\n### RAG\n\n检索增强生成，没有任何框架（haystack，Langchain…）。这是语言模型中最常用的任务之一。回复的评估基于对指令的理解能力以及答案与文本的相关性。\n\n### 真实的 RAG\n\n这是一个与提供的上下文完全无关的问题的 RAG。模型必须回复“无法回答”，这意味着它理解了指令，并且没有使用任何外部知识或虚构的信息。\n\n### 从参考文献撰写内容\n\n这是一个创造性的任务。使用参考文本，SML必须提供一篇新的文章。\n\n### 提取5个主题\n\n此任务的重点是验证：\n\n* 确保正好有5个主题\n* 它们是基于事实的（没有虚构）\n\n### 创意：1000字科幻故事\n\n完全创造性的任务。即使对于更大的模型来说，要保持连贯性并产生一个符合字数要求的小故事也是非常困难的。\n\n### 反思提示\n\n反思提示旨在验证模型的链式推理过程。输出被限制在特殊标签的开头/结尾。重点是推理和一致的输出结构。输出必须易于用于进一步的结构化提示或可视化。您可以在本文中阅读更多内容：\n\n## 评估过程\n\n在每次生成结束时，用户会被要求用0到5的分数来评估结果。**在这种情况下，用户就是我……**\n\n这种定性分析确实较为简单，因此每个分数都有一个描述，用户可以添加评论（“一些错误的信息”，“也许更改提示中的措辞会更好”）\n\n这里是带有描述的定性矩阵\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*H_Qx2UT1lxqy2BF4.png)\n\n## 好与坏 — 细节\n\n总体而言，Qwen2.5–3B-instruct 是一个全方位的模型。每个任务的平均得分为 4.4 分（满分 5 分），这相当令人印象深刻。\n\n摘要表现出色。在长文本中列出主要主题也非常好。\n\n它在 RAG 中表现突出，有时显得过于冗长：在真实的 RAG 中，它还提供了一些解释，以及 *无法回答* 回复的原因。\n\n我们可以一起看看。这对于 SML 来说，精确计算句子和段落的数量通常是一项艰巨的任务。\n\n```python\n**Write a two-sentences summary of the following text:**\n\nChina's regime under Chinese Communist Party operates without transparent \nelections and primary candidates. While undesirable for democracies, \nit allows long-term planning free from electoral pressures that can change \nplans abruptly; this has led to China achieving peak carbon dioxide emissions \nin 2023 due largely to increased renewable energy installations, potentially \nsetting a new global standard for sustainable development despite its status \nas one of the world's largest polluters.\n```\n\n目录对于语言模型来说是另一个噩梦，因为它们通常理解如何创建表格……但看看这个：\n\n```python\nA \"table of content\" provides an organized structure to the text, allowing \nreaders quick access to key points. Write the \"table of content\" of \nthe following text.\n\n[table of contents]\n1. Introduction: Overview and Critique on China's Governance System  \n2. Comparison with Western Democracies - Undesirable Dictatorship? \n3. Advantages in Long-Term Planning for Sustainability   \n4. The Chinese Model as a Comparative Advantage  \n5. Recent Achievements in Carbon Emissions Reduction\n6. Transition to Renewable Energy: A Turning Point?\n7. Potential Leadership Role of China and India  \n8. Conclusion - Future Directions\n\n```\n\n我给真实的 RAG 评分 4 分（满分 5 分），但它本可以轻松得到 5 分。最重要的是，回复中必须包含 *无法回答* 这个词。\n\n为什么？想象一下你在设置一系列提示，你需要根据先前的回复决定下一步。如果你可以简单地寻找 *无法回答* 这个词的存在来引导你的下一步，那就没问题了！\n\n```python\n**Reply to the question only using the provided context. If the answer \nis not contained in the provided context say \"unanswerable\".\nquestion: who is Anne Frank?**\n{context}\n**Remember: if you cannot answer based on the provided context, say \n\"unanswerable\"\nanswer:**\n\nUnanswerable\nThe given text does not provide any information about Anne Frank \nor her identity. The content is unrelated to this question and focuses \ninstead on discussing China's system compared to Western democracies in \nterms of long-term planning for environmental issues.\n```\n\n我想象模型也被训练来提供这种回复的原因……太棒了！\n\n### 优秀的反思\n\n反思提示的主要焦点是模型返回正确推理过程的能力，并按照几个特定的标签开闭格式化输出。\n\n到目前为止，这是**小型语言模型**中**最佳的反思输出**，甚至比Gemma2–2B更好：\n\n```python\n*👨‍💻 > You are an AI assistant designed to provide detailed, step-by-step \nresponses. Your outputs should follow this structure:\n1. Begin with a <thinking> section.\n2. Inside the thinking section:\n   a. Briefly analyze the question and outline your approach.\n   b. Present a clear plan of steps to solve the problem.\n   c. Use a \"Chain of Thought\" reasoning process if necessary, breaking down \nyour thought process into numbered steps.\n3. Include a <reflection> section for each idea where you:\n   a. Review your reasoning.\n   b. Check for potential errors or oversights.\n   c. Confirm or adjust your conclusion if necessary.\n4. Be sure to close all reflection sections.\n5. Close the thinking section with </thinking>.\n6. Provide your final answer in an <output> section.\nAlways use these tags in your responses. Be thorough in your explanations, \nshowing each step of your reasoning process. Aim to be precise and logical \nin your approach, and don't hesitate to break down complex problems into \nsimpler components. Your tone should be analytical and slightly formal, \nfocusing on clear communication of your thought process.\nRemember: Both <thinking> and <reflection> MUST be tags and must be closed \nat their conclusion\nMake sure all <tags> are on separate lines with no other text. \nDo not include other text on a line containing a tag.\n\nuser question: explain why it is crucial for teachers to learn how to use \ngenerative AI for their job and for the future of education. \nInclude relevant learning path for teachers and educators. *\n\n**💻 > <thinking>\nI will begin by analyzing the importance of teaching about Generative \nArtificial Intelligence (AI) in an educational context, then outline a \nstep-by-step plan on what steps should be taken to learn this technology \neffectively as well as provide resources or courses that could help.\nFirstly, understanding AI is crucial for teachers because it can \nsignificantly enhance their ability to support students. \nTeachers need the knowledge and skills necessary not only to integrate these tools into teaching but also understand how they work so they are able to guide both themselves and their pupils in ethical use of technology.\n</thinking>\n\n<reflection>\nMy initial thought process was clear, emphasizing that teachers should \nbe equipped with AI literacy as it can improve educational outcomes. \nI outlined the importance for guiding students ethically while using \nthese tools effectively.\nHowever, my plan needs more detail on specific steps to learn about \ngenerative AI and resources available online or in-person courses.\n</reflection>**\n*MY PERSONAL NOTE: here it is starting an additional thinking after the first \nreflection... And this is really too good to be true!!*\n**<thinking>\nTo create a comprehensive learning path that teachers could follow:\n1. **Understand Basic Concepts**: Start with basic concepts of machine \n   learning (ML) such as supervised vs unsupervised, deep neural networks  \n   etc., which are foundational to understanding generative AI.\n2. **Learn About Generative Models**:\n   - Study different types: Variational Autoencoders (VAEs), GANs.\n   - Understand how these models generate new data that resembles the \n     training set but is not identical and can be used for tasks like \n     text generation, image synthesis etc., which are crucial in education.\n3. **Explore Applications**: Look at real-world applications of generative \n   AI such as language modeling (e.g., GPT-2), computer vision techniques to \n   create realistic images or videos from raw data.\n4. **Ethical Considerations and Implications for Education**:\n   - Discuss the ethical implications, including bias in models' outputs \n     which could affect students’ perceptions about themselves if they are \n     exposed regularly through AI-generated content that is not representative \n     of them.\n5. **Hands-on Practice**: Engage with practical exercises using open-source \n   tools like TensorFlow or PyTorch to build simple generative model \n   applications.\n6. **Stay Updated and Network**:\n   - Follow the latest research in ML, especially on Generative Models \n     by following relevant journals (e.g., NeurIPS), conferences such as \n     ICML etc.\n7. Attend Workshops/Conferences: Participate actively at AI-related workshops \n   or events to network with professionals working within this field.\n</thinking>**\n*MY PERSONAL NOTE: and no surprise, an additional reflection now starts, \nbefore producing the final output*\n**<reflection>\nMy plan has been expanded and includes a more detailed approach for teachers, \ncovering foundational knowledge through practical application of generative \nmodels in education along with ethical considerations that are crucial. \nI have also included staying updated on the latest research as an important \npart.\nHowever, it's essential to ensure these resources or courses exist so \neducators can access them easily.\n</reflection>\n\n<output>\nIt is vital for teachers and educational professionals alike to learn how \ngenerative AI works because this technology has significant potential in \nenhancing teaching methods. By understanding basic concepts of machine \nlearning (ML) such as supervised vs unsupervised, deep neural networks etc., \nthey will be better equipped with the knowledge necessary not only to \nintegrate these tools into their curriculum but also guide students ethically.\nA comprehensive plan for teachers and educators includes:\n1. **Understand Basic Concepts**: Start by grasping foundational concepts of ML.\n2. **Learn About Generative Models**:\n   - Study different types: Variational Autoencoders (VAEs), GANs, etc., \n     which are crucial in generating new data that resembles the training \n     set but is not identical and can be used for tasks like text generation or \n     image synthesis.\n3. **Explore Applications**: Look at real-world applications of generative AI \n   such as language modeling using tools like TensorFlow.\n4. **Ethical Considerations**:\n   - Discuss ethical implications, including bias in models' outputs which \n     could affect students’ perceptions about themselves if they are exposed \n     regularly through AI-generated content that is not representative.\n5. **Hands-on Practice**: Engage with practical exercises to build simple \n   generative model applications using open-source tools.\n6. **Stay Updated and Network**:\n   - Follow the latest research in ML, especially on Generative Models by \n     following relevant journals (e.g., NeurIPS), conferences such as ICML etc.\n7. Attend Workshops/Conferences: Participate actively at AI-related workshops \n   or events to network with professionals working within this field.\n</output>\n```\n\n这样的精炼思维过程，以及清晰的标签开闭格式，可以方便后续的处理流程，或者在漂亮的GUI中进行美观的打印。\n\n## 它不擅长什么？\n\n最糟糕的任务是创作短篇小说。模型在生成过程中开始重复相同的段落。\n\n这并不意味着它无法做到。更有可能的是，通过提高温度和重复惩罚，可以获得良好的结果。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GNJ3lG6FM5qt9glIdf7qhQ.jpeg)\n\n## 结论\n\n您可以在我的GitHub仓库中找到所有聊天记录，以及代码和自行操作的说明。您可以参考我之前文章中的教程 [Qwen2.5 1.5b: 移动AI的未来？](https://ai.gopubby.com/qwen2-5-1-5b-the-future-of-mobile-ai-6bd5f29bbc84)\n\n在接下来的文章中，我将介绍其他小型语言模型，使用相同的原则：从小型的350M参数，到500M系列，再到3B — 经过1.5B。\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/retrieval-augmented-generation-approaches-state-of-the-art-and-optimization-strategies-456883da4801","frontmatter":{"title":"检索增强生成：方法、最新进展和优化策略","meta_title":"检索增强生成：方法、最新进展和优化策略","description":"⭐ RAG 在知识密集型场景或需要持续知识的特定领域应用中特别有用……","date":"2024-10-31T08:17:32.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*_vE7WktGmyQ5xg_t5cpFVg.jpeg","categories":["Generative AI","Natural Language Processing","Machine Learning"],"author":"Rifx.Online","tags":["RAG","retrieval","generation","optimization","embeddings"],"draft":false,"slug":"blog/retrieval-augmented-generation-approaches-state-of-the-art-and-optimization-strategies-456883da4801"},"content":"\n\n\n\n\n⭐ RAG 在知识密集型场景或需要持续更新知识的特定领域应用中尤其有用。最近，RAG 因其在对话代理中的应用而受到广泛关注。\n\n📌 参考研究主要集中在当前的 RAG 方法及其不同组件、最新进展（SOTA）、应用、检索、生成、增强技术的评估上。\n\n随着 RAG 系统从简单到高级再到模块化的演变，每个阶段都是为了应对特定用例的增强而出现的。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*P2ByKtayhF4XgAVxRI1urQ.jpeg)\n\n✏ *简单型*：用户输入用于文档查询，附加/组合到提示中，用于模型最终响应生成。通过多轮对话交互，可以将上下文/对话历史添加/组合到提示中。缺点：低精度/低召回率，冗余，重复。\n\n✏ *高级型*：通过优化预检索、检索和后检索方法来提高检索质量。预检索中，通过增强数据粒度、索引结构改进、元数据、对齐和混合检索来提升质量。在检索中，优化嵌入模型，从而优化上下文。在后检索中，优化上下文窗口和噪声/干扰数据的拒绝。\n\n✏ *模块化*：引入搜索模块进行相似性检索和检索的微调。新模块包括搜索、记忆、融合、路由、预测、任务适配器。\n\n🥉 优化 RAG 流水线：\n\n📜 *混合搜索探索*：通过智能利用关键字搜索、语义搜索和向量搜索等技术，平衡性能优化。\n\n📜 *递归检索和查询引擎*：初始阶段可能通过获取较小的块开始检索，随后以更好且更具上下文的信息获取较大的块，以平衡上下文丰富的响应与效率。\n\n📜 *回退提示*：这鼓励 LLM 脱离特定实例，围绕更广泛的概念和原则进行推理（arXiv:2310\\.13243）。在各种具有挑战性的基于推理的任务中，使用回退提示时观察到显著的性能提升，突显了它们对 RAG 过程的自然适应性。\n\n📜 *子查询*：根据场景应用的查询策略可以使用，例如利用 LlamaIndex 提供的查询引擎，利用树查询，使用向量查询，或执行简单的块顺序查询。\n\n📜 *假设文档嵌入*：使用 LLM，HyDE 通过创建假设答案来响应查询，嵌入答案，并使用相同的答案来检索真实文档。该方法关注的是从一个答案到另一个答案的嵌入相似性，而不是基于查询寻找嵌入相似性\\[arXiv:2212\\.10496]。缺点：不一致的答案未能产生理想结果，LLM 未见主题的错误，导致错误。\n\n让我在这里结束。我会在后续中发布新文章。\n\n[\\#genai](https://www.linkedin.com/feed/hashtag/?keywords=genai&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7170160104984571905) [\\#rag](https://www.linkedin.com/feed/hashtag/?keywords=rag&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7170160104984571905) \\#ai \\#llm\n\n参考：[arxiv:2312\\.10997](https://arxiv.org/pdf/2312.10997)，RAG 调查，Huggingfaceblogs\n\n"},{"lang":"zh","group":"blog","slug":"blog/smollm2-very-good-alternatives-to-qwen2-5-and-llama-3-2-463a200d2f3b","frontmatter":{"title":"SmolLM2：Qwen2.5 和 Llama 3.2 的最佳替代品","meta_title":"SmolLM2：Qwen2.5 和 Llama 3.2 的最佳替代品","description":"而且是全开的！","date":"2024-11-10T03:51:17.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Y3_lsNsFKybrOi14.png","categories":["Technology","Machine Learning","Data Science"],"author":"Rifx.Online","tags":["SmolLM2","parameters","pre-training","MobileLLM","reproducibility"],"draft":false,"slug":"blog/smollm2-very-good-alternatives-to-qwen2-5-and-llama-3-2-463a200d2f3b"},"content":"\n\n\n## 而且它是完全开放的！\n\nHugging Face 加大了对 SmolLM 计划的投入。\n\n他们发布了 SmolLM2：1\\.7B、360M 和 135M 模型，训练于 11T 令牌（相比 SmolLM 的 1T）。他们发布了基础版和指导版：\n\n* Hugging Face Collection: [SmolLM2](https://huggingface.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9) (Apache 2\\.0 许可证)\n\n他们使用了新的数据集进行预训练，计划很快发布。为了制作指导版，他们使用了类似于训练 Zephyr 的配方（SFT\\+DPO 在 ultrafeedback 上）。\n\n看起来 SmolLM2 的表现非常出色：\n\n\n\n请注意，Hugging Face 完全公开了预训练数据和他们用来防止数据污染的配方。换句话说，他们发布的评估结果可能是准确且完全可重复的。\n\nHugging Face 使用了自己的框架进行预训练，[Nanotron](https://github.com/huggingface/nanotron)。我从未写过关于 Nanotron 的文章，但我认为这是一个非常有趣的项目，值得更广为人知，特别是如果你有兴趣了解预训练是如何进行的。我会尽量找时间在 2025 年之前发布一篇解释 Nanotron 的文章！\n\nMeta 还发布了一系列小型模型，MobileLLM：\n\n* Hugging Face Collection: [MobileLLM](https://huggingface.co/collections/facebook/mobilellm-6722be18cb86c20ebe113e95) (CC\\-BY\\-NC)\n\n这是一个新发布的项目，但请注意，这些模型实际上相当旧。它们是为 2024 年 2 月发布的这项工作训练的：\n\n[MobileLLM: Optimizing Sub\\-billion Parameter Language Models for On\\-Device Use Cases](https://arxiv.org/abs/2402.14905)\n\n通过我的新书“LLMs on a Budget”，了解使用和微调大型语言模型所需的一切：\n\n"},{"lang":"zh","group":"blog","slug":"blog/the-6-best-llm-tools-to-run-models-locally-eedd0f7c2bbd","frontmatter":{"title":"6 种最佳本地运行模型的 LLM 工具","meta_title":"6 种最佳本地运行模型的 LLM 工具","description":"运行大型语言模型 (LLM)（例如 ChatGPT 和 Claude）通常涉及将数据发送到由 OpenAI 和其他 AI 模型管理的服务器……","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*2MB6-INUUGLR0NR_iOACIg.jpeg","categories":["Technology","Programming","Health"],"author":"Rifx.Online","tags":["LLM","local","deployment","customization","telehealth"],"draft":false,"slug":"blog/the-6-best-llm-tools-to-run-models-locally-eedd0f7c2bbd"},"content":"\n\n\n\n\n运行大型语言模型（LLMs）如 [ChatGPT](https://openai.com/chatgpt/mac/) 和 [Claude](https://claude.ai/) 通常涉及将数据发送到由 [OpenAI](https://openai.com/) 和其他 AI 模型提供商管理的服务器。虽然这些服务是安全的，但一些企业更倾向于将其数据完全离线，以获得更高的隐私保护。\n\n本文将介绍开发人员可以使用的六款工具，以便在本地运行和测试 LLM，确保他们的数据永远不会离开他们的设备，这类似于 [端到端加密](https://getstream.io/blog/end-to-end-encryption/) 保护隐私的方式。\n\n## 为什么使用本地 LLM？\n\n像 [LM Studio](https://lmstudio.ai/) 这样的工具在用户使用它来运行本地 LLM 时，不会收集用户数据或跟踪用户的行为。它允许所有聊天数据保留在本地计算机上，而不与 AI/ML 服务器共享。\n\n* **隐私**：您可以以多轮的方式提示本地 LLM，而不会让您的提示数据离开本地主机。\n* **自定义选项**：本地 LLM 提供 CPU 线程、温度、上下文长度、GPU 设置等高级配置选项。这类似于 OpenAI 的游乐场。\n* **支持和安全性**：它们提供与 OpenAI 或 Claude 相似的支持和安全性。\n* **订阅和费用**：这些工具免费使用，并且不需要每月订阅。对于像 OpenAI 这样的云服务，每个 API 请求都需要付费。本地 LLM 有助于节省费用，因为没有每月订阅。\n* **离线支持**：您可以在离线时加载和连接大型语言模型。\n* **连接性**：有时，连接到像 OpenAI 这样的云服务可能会导致信号和连接不良。\n\n## 六大免费本地 LLM 工具\n\n根据您的具体使用案例，您可以选择几种离线 LLM 应用程序。这些工具中有一些完全免费供个人和商业使用。其他工具可能需要您发送请求以用于商业用途。对于 Mac、Windows 和 Linux，有多种本地 LLM 工具可供选择。以下是您可以选择的六个最佳工具。\n\n## 1. LM Studio\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*svbQPZKu08of7Kv6)\n\n[LM Studio](https://lmstudio.ai/) 可以运行任何格式为 `gguf` 的模型文件。它支持来自模型提供商的 `gguf` 文件，如 [Llama 3.1](https://llama.meta.com/)、[Phi 3](https://huggingface.co/docs/transformers/main/en/model_doc/phi3)、[Mistral](https://mistral.ai/) 和 [Gemma](https://ai.google.dev/gemma)。要使用 LM Studio，请访问上述链接并下载适合您机器的应用程序。启动 LM Studio 后，主页会展示可下载和测试的顶级 LLM。还有一个搜索栏，可以筛选并下载来自不同 AI 提供商的特定模型。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*sbS3VqiLgDsftgs2)\n\n从特定公司的模型中搜索会显示多个模型，范围从小型到大型 [quantization](https://huggingface.co/docs/optimum/en/concept_guides/quantization)。根据您的机器，LM Studio 会使用兼容性猜测来突出显示适合该机器或平台的模型。\n\n## LM Studio 的关键特性\n\nLM Studio 提供与 ChatGPT 相似的功能和特性。它具有多个功能。以下是 LM Studio 的关键特性。\n\n* **模型参数自定义**：这允许您调整温度、最大令牌、频率惩罚等。\n* **聊天历史**：允许您保存提示以供后续使用。\n* **参数和用户界面提示**：您可以将鼠标悬停在信息按钮上以查找模型参数和术语。\n* **跨平台**：LM Studio 可在 Linux、Mac 和 Windows 操作系统上使用。\n* **机器规格检查**：LM Studio 检查计算机规格，如 GPU 和内存，并报告兼容的模型。这可以防止下载可能在特定机器上无法运行的模型。\n* **AI 聊天和游乐场**：以多轮聊天格式与大型语言模型进行对话，并通过同时加载多个 LLM 进行实验。\n* **开发者本地推理服务器**：允许开发者设置一个类似于 OpenAI API 的本地 HTTP 服务器。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*9bHmRiOSf6gm-u3P)\n\n本地服务器提供示例 Curl 和 Python 客户端请求。此功能有助于使用 LM Studio 构建 AI 应用程序，以访问特定的 LLM。\n\n```python\n## Example: reuse your existing OpenAI setup\nfrom openai import OpenAI\n\n## Point to the local server\nclient = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n\ncompletion = client.chat.completions.create(\n  model=\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"Always answer in rhymes.\"},\n    {\"role\": \"user\", \"content\": \"Introduce yourself.\"}\n  ],\n  temperature=0.7,\n)\n\nprint(completion.choices[0].message)\n```\n通过上述示例 Python 代码，您可以重用现有的 OpenAI 配置，并将基本 URL 修改为指向您的本地主机。\n\n* **OpenAI 的 Python 库导入**：LM Studio 允许开发者导入 OpenAI Python 库，并将基本 URL 指向本地服务器（localhost）。\n* **多模型会话**：使用单个提示并选择多个模型进行评估。\n\n## 使用 LM Studio 的好处\n\n该工具可供个人免费使用，允许开发者通过应用内聊天用户界面和游乐场运行 LLM。它提供了一个华丽且易于使用的界面，带有过滤器，并支持连接到 OpenAI 的 Python 库，无需 API 密钥。公司和企业可以根据要求使用 LM Studio。然而，它需要 M1/M2/M3 Mac 或更高版本，或具有支持 [AVX2](https://edc.intel.com/content/www/us/en/design/ipla/software-development-platforms/client/platforms/alder-lake-desktop/12th-generation-intel-core-processors-datasheet-volume-1-of-2/009/intel-advanced-vector-extensions-2-intel-avx2/) 的处理器的 Windows PC。Intel 和 [AMD](https://www.amd.com/en/support/download/drivers.html) 用户仅限于使用 [v0.2.31](https://lmstudio.ai/) 中的 Vulkan 推理引擎。\n\n## 2. Jan\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*7YeH_48iFYB4lDRu)\n\n将 [Jan](https://jan.ai/) 理解为一个设计用于离线操作的开源版本的 ChatGPT。它由一个用户社区构建，秉持用户拥有的理念。Jan 允许您在设备上运行流行的模型，如 [Mistral](https://huggingface.co/models?other=mistral) 或 [Llama](https://huggingface.co/models?other=llama)，而无需连接互联网。使用 Jan，您可以访问远程 API，如 OpenAI 和 [Groq](https://groq.com/)。\n\n## Jan 的主要特点\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*ufyOE6QkcHw8X5U7)\n\nJan 是一款电子应用程序，其功能类似于 LM Studio。它通过将消费者设备转变为 AI 计算机，使 AI 变得开放和可访问。由于这是一个开源项目，开发者可以为其贡献代码并扩展其功能。以下是 Jan 的主要特点。\n\n* **本地**：您可以在设备上运行您喜欢的 AI 模型，而无需将其连接到互联网。\n* **即用模型**：下载 Jan 后，您将获得一组已安装的模型以供开始使用。也可以搜索特定模型。\n* **模型导入**：支持从 Hugging Face 等来源导入模型。\n* **免费、跨平台和开源**：Jan 完全免费，开源，并可在 Mac、Windows 和 Linux 上运行。\n* **自定义推理参数**：调整模型参数，如最大令牌、温度、流、频率惩罚等。所有偏好设置、模型使用和配置都保留在您的计算机上。\n* **扩展**：Jan 支持 [TensortRT](https://github.com/NVIDIA/TensorRT) 和 [Inference Nitro](https://huggingface.co/jan-hq/nitro-v1.2-e3) 等扩展，以自定义和增强您的 AI 模型。\n\n## 使用 Jan 的好处\n\nJan 提供了一个干净简单的界面来与 LLM 互动，并且将所有数据和处理信息保存在本地。它已经为您安装了超过七十个大型语言模型供您使用。这些现成的模型的可用性使得连接和与远程 API（如 OpenAI 和 Mistral）互动变得简单。Jan 还有一个很棒的 [GitHub](https://github.com/janhq/jan)、[Discord](https://discord.gg/FTk2MvZwJH) 和 [Hugging Face](https://huggingface.co/janhq) 社区，可以关注并寻求帮助。然而，像所有 LLM 工具一样，这些模型在 Apple Silicon Macs 上的运行速度比在 Intel 机器上更快。\n\n## 3. Llamafile\n\n[Llamafile](https://github.com/Mozilla-Ocho/llamafile) 由 [Mozilla](https://www.mozilla.org/en-US/?v=1) 支持，旨在通过快速的 [CPU 推理](https://huggingface.co/docs/transformers/en/perf_infer_cpu) 使开源 AI 对每个人都可访问，而无需网络连接。它将 LLM 转换为多平台的 [可执行链接格式](https://gist.github.com/x0nu11byt3/bcb35c3de461e5fb66173071a2379779) (ELF)。它提供了将 AI [集成](https://getstream.io/chat/solutions/ai-integration/) 到应用程序中的最佳选项之一，使您能够仅通过一个可执行文件运行 LLM。\n\n## Llamafile 的工作原理\n\n它旨在将权重转换为多个可执行程序，这些程序无需安装即可在 Windows、MacOS、Linux、Intel、ARM、FreeBSD 等架构上运行。在底层，Llamafile 使用 [tinyBLAST](https://github.com/ggerganov/llama.cpp/issues/5048) 在像 Windows 这样的操作系统上运行，而无需 SDK。\n\n## Llamafile 的关键特性\n\n* **可执行文件**：与 LM Studio 和 Jan 等其他 LLM 工具不同，Llamafile 只需一个可执行文件即可运行 LLM。\n* **使用现有模型**：Llamafile 支持使用现有的模型工具，如 Ollama 和 LM Studio。\n* **访问或创建模型**：您可以访问 OpenAI、Mistral、Groq 等流行 LLM。它还提供从头创建模型的支持。\n* **模型文件转换**：您可以将许多流行 LLM 的文件格式转换，例如，将 `.gguf` 转换为 `.llamafile` 只需一个命令。\n\n`llamafile-convert mistral-7b.gguf`\n\n## 开始使用 Llamafile\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*4PV1KsCZvvVKqFll)\n\n要安装 Llamafile，请访问 Huggingface 网站，从导航中选择 **Models**，然后搜索 **Llamafile**。您还可以从下面的 URL 安装您喜欢的 [量化](https://huggingface.co/docs/optimum/en/concept_guides/quantization) 版本。\n\n[`https://huggingface.co/Mozilla/Meta-Llama-3.1-8B-Instruct-llamafile/tree/m`ain](https://huggingface.co/Mozilla/Meta-Llama-3.1-8B-Instruct-llamafile/tree/main)\n\n**注意**：量化数字越大，响应越好。正如上图所示，本文使用 `Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile`，其中 `Q6` 代表量化数字。\n\n**步骤 1：下载 Llamafile**\n\n从上面的链接，点击任意下载按钮以获取您喜欢的版本。如果您在机器上安装了 [wget](https://www.gnu.org/software/wget/) 工具，您可以使用以下命令下载 Llamafile。\n\n`wget <https://huggingface.co/Mozilla/Meta-Llama-3.1-8B-Instruct-llamafile/blob/main/Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile>`\n\n您应该用您喜欢的版本替换 URL。\n\n**步骤 2：使 Llamafile 可执行**\n\n下载特定版本的 Llamafile 后，您应该通过导航到文件位置，使用以下命令使其可执行。\n\n`chmod +x Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile`**步骤 3：运行 Llamafile**\n\n在文件名之前添加一个点和斜杠 `./` 来启动 Llamafile。\n\n`./Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile`\n\nLlamafile 应用程序现在将在 `http://127.0.0.1:8080` 可用，以运行您的各种 LLMs。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*1xrwDPTfNgmEQDTx)\n\n## 使用 Llamafile 的好处\n\nLlamafile 通过使 LLM 容易被消费者 CPU 访问，帮助实现 AI 和 ML 的民主化。与其他本地 LLM 应用程序如 **Llama.cpp** 相比，Llamafile 提供了最快的提示处理体验，并在游戏电脑上表现更佳。由于其更快的性能，它是总结长文本和大型文档的绝佳选择。它完全离线运行并保护隐私，因此用户不会将数据分享给任何 AI 服务器或 API。像 Hugging Face 这样的机器学习社区支持 Llamafile 格式，使得搜索与 Llamafile 相关的模型变得容易。它还有一个出色的开源社区，进一步开发和扩展它。\n\n## 4. GPT4ALL\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*j3vNWWQZCVF5woo5)\n\nGPT4ALL 基于隐私、安全和无需互联网的原则构建。用户可以在 Mac、Windows 和 Ubuntu 上 [安装](https://www.nomic.ai/gpt4all)。与 Jan 或 LM Studio 相比，GPT4ALL 拥有更多的每月下载量、[GitHub Stars](https://github.com/nomic-ai/gpt4all) 和活跃用户。\n\n## GPT4ALL的主要特点\n\nGPT4All可以在主要消费硬件上运行LLM，例如Mac M系列芯片、AMD和NVIDIA GPU。以下是其主要特点。\n\n* **隐私优先**：将私人和敏感的聊天信息和提示仅保留在您的设备上。\n* **无需互联网**：它完全离线工作。\n* **模型探索**：此功能允许开发者浏览和下载不同类型的LLM进行实验。您可以从流行选项中选择大约1000个开源语言模型，如LLama、Mistral等。\n* **本地文档**：您可以让本地LLM访问您的敏感数据，使用本地文档如`.pdf`和`.txt`，数据不会离开您的设备，也无需网络。\n* **自定义选项**：它提供多个[聊天机器人](https://getstream.io/blog/llm-chatbot-docs/)调整选项，如温度、批处理大小、上下文长度等。\n* **企业版**：GPT4ALL提供企业套餐，具备安全性、支持和每台设备的许可证，将本地AI带入企业。\n\n## 开始使用 GPT4All\n\n要开始使用 GPT4All 在本地运行 LLMs，请[下载](https://www.nomic.ai/gpt4all)适合您操作系统的版本。\n\n## 使用GPT4ALL的好处\n\n除了Ollama，GPT4ALL在GitHub贡献者数量上最为显著，拥有约250000名每月活跃用户（根据<https://www.nomic.ai/gpt4all>）并且与其竞争对手相比。该应用收集有关使用分析和聊天分享的匿名用户数据。然而，用户可以选择加入或退出。使用GPT4ALL，开发者可以从其庞大的用户基础、GitHub和Discord社区中受益。\n\n## 5. Ollama\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*STAonWgWIsY6cgDR)\n\n使用 [Ollama](https://ollama.com/)，您可以轻松创建本地聊天机器人，而无需连接到像 OpenAI 这样的 API。由于一切都在本地运行，您无需支付任何订阅费或 API 调用费用。\n\n## Ollama 的关键特性\n\n* **模型自定义**：Ollama 允许您转换 `.gguf` 模型文件并使用 `ollama run modelname` 运行它们。\n* **模型库**：Ollama 拥有大量模型可供尝试，访问 [ollama.com/library](https://ollama.com/library)。\n* **导入模型**：Ollama 支持从 [PyTorch](https://pytorch.org/) 导入模型。\n* **社区集成**：Ollama 无缝集成到网页和桌面应用程序中，例如 [Ollama-SwiftUI](https://github.com/kghandour/Ollama-SwiftUI)、[HTML UI](https://github.com/rtcfirefly/ollama-ui)、[Dify.ai](https://github.com/rtcfirefly/ollama-ui) 和 [更多](https://github.com/ollama/ollama?tab=readme-ov-file#community-integrations)。\n* **数据库连接**：Ollama 支持多个 [数据平台](https://github.com/mindsdb/mindsdb/blob/main/mindsdb/integrations/handlers/ollama_handler/README.md)。\n* **移动集成**：像 [Enchanted](https://github.com/AugustDev/enchanted) 这样的 SwiftUI 应用将 Ollama 带入 iOS、macOS 和 visionOS。[Maid](https://github.com/Mobile-Artificial-Intelligence/maid) 也是一个跨平台的 Flutter 应用，能够本地处理 `.gguf` 模型文件。\n\n## 开始使用 Ollama\n\n要首次使用 Ollama，请访问 <https://ollama.com> 并下载适合您机器的版本。您可以在 Mac、Linux 或 Windows 上安装它。安装 Ollama 后，您可以在终端中使用以下命令检查其详细信息。\n\n`ollama`\n\n要运行特定的 LLM，您应该使用以下命令下载它：\n\n`ollama pull modelname`，其中 `modelname` 是您要安装的模型名称。请在 [GitHub](https://github.com/ollama/ollama) 上查看一些可供下载的示例模型。`pull` 命令也用于更新模型。一旦使用，仅会获取差异部分。\n\n例如，在下载了 `llama3.1` 后，在命令行中运行 `ollama run llama3.1` 将启动该模型。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*aglZm6h0BU6GAYkSl04XWA.gif)\n\n在上述示例中，我们提示 `llama3.1` 模型解决一个物理功和能量的问题。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*dNNQYpz1s2tz1pcn)\n\n## 使用 Ollama 的好处\n\nOllama 在 GitHub 上拥有超过 200 名贡献者，并且有活跃的更新。它拥有最多的贡献者，并且在上述其他开源 LLM 工具中更具可扩展性。\n\n## 6. LLaMa.cpp\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*KhsAUquhDZAHghxK)\n\n[LLaMa.cpp](https://github.com/ggerganov/llama.cpp) 是支持本地 LLM 工具（如 Ollama 等）的底层后端技术（推理引擎）。LLaMa.cpp 支持显著的大型语言模型推理，配置简单，并在各种硬件上提供出色的本地性能。它也可以在云端运行。\n\n## LLaMa.cpp 的主要特点\n\n* **设置**：它的设置非常简单。您只需一个命令即可安装。\n* **性能**：它在本地和云端的各种硬件上表现非常出色。\n* **支持的模型**：它支持流行的主要 LLM，如 [Mistral 7B](https://huggingface.co/mistralai/Mistral-7B-v0.1)、[Mixtral MoE](https://huggingface.co/models?search=mistral-ai/Mixtral)、[DBRX](https://huggingface.co/databricks/dbrx-instruct)、[Falcon](https://huggingface.co/models?search=tiiuae/falcon) 和 [其他许多模型](https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#description)。\n* **前端 AI 工具**：LLaMa.cpp 支持开源 LLM UI 工具，如 [MindWorkAI/AI-Studio](https://github.com/MindWorkAI/AI-Studio) (FSL-1.1-MIT)、[iohub/collama](https://github.com/iohub/coLLaMA) 等。\n\n## 使用 LLaMa.cpp 开始\n\n要运行您的第一个本地大型语言模型，请使用以下命令安装 llama.cpp：\n\n`brew install llama.cpp`\n\n接下来，从 Hugging Face 或其他来源下载您想要运行的模型。例如，从 Hugging Face 下载下面的模型并将其保存在您计算机上的某个位置。\n\n[`https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q4_K_M.g`guf](https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf)\n\n使用您喜欢的命令行工具，如终端，`cd` 进入您刚下载的 `.gguf` 模型文件的位置，并运行以下命令。\n\n```python\nllama-cli --color \\ \n-m Mistral-7B-Instruct-v0.3.Q4_K_M.ggufb \\ \n-p \"Write a short intro about SwiftUI\"\n```\n总之，您首先调用 LLaMa CLI 工具并设置颜色和其他标志。`-m` 标志指定您要使用的模型的路径。`-p` 标志指定您希望用来指示模型的提示。\n\n运行上述命令后，您将看到以下预览中的结果。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*4Al-j50vXUXLUfvxzBt6aw.gif)\n\n## 本地 LLM 的用例\n\n在本地运行 LLM 可以帮助开发人员深入了解其性能和工作原理。 本地 LLM 可以查询私有文档和技术论文，以便与这些文档相关的信息不会离开用于查询的设备，不会发送到任何云 AI API。 本地 LLM 在没有互联网的地方和网络信号较差的地方非常有用。\n\n在 [远程医疗环境](https://getstream.io/blog/telemedicine-app-development/) 中，本地 LLM 可以对患者文档进行排序，而无需出于隐私考虑将其上传到任何 AI API 提供商。\n\n## 评估本地运行的大型语言模型性能\n\n在本地使用大型语言模型之前，了解其性能对于获得所需的响应至关重要。有几种方法可以确定特定 LLM 的性能。以下是一些方法。\n\n* **训练**：该模型是基于什么数据集进行训练的？\n* **微调**：模型在多大程度上可以定制以执行特定任务，或者是否可以针对特定领域进行微调？\n* **学术研究**：该 LLM 是否有学术研究论文？\n\n要回答上述问题，您可以查看优秀的资源，如 [Hugging Face](https://huggingface.co/datasets) 和 [Arxiv.org](https://arxiv.org/)。此外，[Open LLm Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) 和 [LMSYS Chatbot Arena](https://chat.lmsys.org/?arena) 提供了各种 LLM 的详细信息和基准测试。\n\n## 本地 LLM 工具结论\n\n正如本文所讨论的，选择和使用本地大型语言模型的动机有很多。如果您不希望将数据集通过互联网发送给 AI API 提供商，则可以对模型进行微调，以执行 [远程医疗应用](https://getstream.io/chat/solutions/healthcare/) 中的特定任务。许多开源的图形用户界面（GUI）本地 LLM 工具，如 LLm Studio 和 Jan，提供直观的前端用户界面，以便在没有像 OpenAI 或 Claude 这样的订阅服务的情况下配置和实验 LLM。您还发现了各种强大的命令行 LLM 应用程序，如 Ollama 和 LLaMa.cpp，帮助您在本地运行和测试模型，而无需互联网连接。查看 Stream 的 [AI 聊天机器人](https://getstream.io/chat/solutions/ai-integration/) 解决方案，将 AI 聊天集成到您的应用中，并访问所有相关链接以了解更多信息。\n\n*最初发布于 [https://getstream.io](https://getstream.io/blog/best-local-llm-tools/).*\n\n"},{"lang":"zh","group":"blog","slug":"blog/the-quest-for-production-quality-graph-rag-easy-to-start-hard-to-finish-46ca404cee3d","frontmatter":{"title":"追求生产质量 Graph RAG：开始容易，完成难","meta_title":"追求生产质量 Graph RAG：开始容易，完成难","description":"克服 RAG 图生产化挑战","date":"2024-11-01T03:56:04.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*RMudHNmBOgXM1Mubj1UTkw.jpeg","categories":["Programming","Data Science","Generative AI"],"author":"Rifx.Online","tags":["graph","RAG","production","uncertainty","optimization"],"draft":false,"slug":"blog/the-quest-for-production-quality-graph-rag-easy-to-start-hard-to-finish-46ca404cee3d"},"content":"\n\n\n### 克服图形 RAG 生产化的挑战\n\n\n\n当我阅读最近在 VentureBeat 上关于 Glean [刚刚在最新融资轮中获得超过 2.6 亿美元的文章](https://venturebeat.com/data-infrastructure/how-to-take-advantage-of-a-generative-tool-fueling-gleans-260m-raise-graph-rag/)时，我有两个直接的直觉。首先，看到这个非常公开的图形 RAG 示例充分发挥其作为一种强大、有价值的技术的潜力，能够比以往任何时候都更高效地将人们与知识连接起来，这让我感到满意。其次，读到以下内容让我感到惊讶但又颇具验证性：\n\n> 世界上最大的共享出行公司之一亲身体验了其带来的好处。在专门投入整个工程团队开发类似的内部解决方案后，他们最终决定转向 Glean 的平台。\n\n> “在一个月内，他们在 Glean 平台上的使用量翻了一番，因为结果是显而易见的，”Glean 的首席营销官 Matt Kixmoeller 说。\n\n虽然我对新闻文章中提到的失败感到惊讶，但根据我自己的经验以及同事和客户的经历，努力将图形 RAG 推向生产是我所预料的。我并不是说我期望大型科技公司在构建自己的图形 RAG 系统时会失败。**我只是期望大多数人会在构建和生产化图形 RAG 时遇到困难——即使他们已经有一个非常成功的概念验证。**\n\n我在 [The New Stack 上对 VentureBeat 文章做了一个高层次的反应](https://bit.ly/4fjIlgJ)，在这篇文章中，我想深入探讨为什么图形 RAG 可能如此难以做到正确。首先，我将指出，利用最新工具，开始使用图形 RAG 变得多么简单。然后，我将深入探讨一些特定的图形 RAG 挑战，这些挑战使其从研发转向生产变得如此困难。最后，我将分享一些关于如何最大化成功机会的图形 RAG 的建议。\n\n## 开始使用图形 RAG 很简单\n\n如果一家大型共享出行公司无法有效构建自己的平台，那么我为什么会说自己实现图形 RAG 很简单呢？\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*l6EiwfjeUGLjVlYeiY1lqA.jpeg)\n\n首先，支持 RAG 和图形 RAG 的技术在过去一年中取得了长足的进步。十二个月前，大多数企业甚至没有听说过检索增强生成。现在，RAG 支持不仅是 [像 LangChain 这样的最佳 AI 构建工具的关键特性](https://python.langchain.com/docs/tutorials/rag/)，而且几乎每个主要的 AI 参与者都有 RAG 教程，甚至还有 [Coursera 课程](https://www.coursera.org/projects/introduction-to-rag)。尝试 RAG 的快速入门途径层出不穷。\n\n微软可能不是第一个做图形 RAG 的公司，但他们在今年早些时候发布的 [研究博客文章](https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/) 中对这一概念进行了大力推动，并继续致力于相关技术的研究。\n\n在 Medium 上，还有一篇来自 [谷歌的一位生成 AI 工程师](https://towardsdatascience.com/graph-rag-a-conceptual-introduction-41cd0d431375) 的很好的概念介绍，包含了一些技术细节。此外，在 Towards Data Science 上，还有一篇最近的、非常详尽的 [关于构建图形 RAG 系统的操作指南](https://towardsdatascience.com/how-to-implement-graph-rag-using-knowledge-graphs-and-vector-databases-60bb69a22759)，以及在科学出版物数据集上进行测试的内容。\n\n在传统图形数据库和分析领域，知名品牌 Neo4j 在其旗舰图形数据库产品中增加了向量能力，以响应最近的生成 AI 革命，并为需要复杂图形分析和深度图形算法的项目提供了一系列优秀的工具平台，除了标准的图形 RAG 功能外。他们还提供了 [图形 RAG 入门指南](https://neo4j.com/developer-blog/graphrag-ecosystem-tools/)。\n\n另一方面，[您甚至不需要图形数据库就可以做图形 RAG](https://bit.ly/3YD5NAd)。许多刚接触图形 RAG 的人认为他们需要部署一个专门的图形数据库，但这并不是必要的，实际上可能会使您的技术栈变得更加复杂。\n\n我的雇主 DataStax 也有 [图形 RAG 指南](https://bit.ly/4862Lrl)。\n\n当然，两个最受欢迎的生成 AI 应用程序组合框架，[LangChain](https://blog.langchain.dev/enhancing-rag-based-applications-accuracy-by-constructing-and-leveraging-knowledge-graphs/) 和 [LlamaIndex](https://docs.llamaindex.ai/en/stable/examples/cookbooks/GraphRAG_v1/)，各自都有自己的图形 RAG 介绍。此外，还有一篇 [DataCamp 文章](https://www.datacamp.com/tutorial/knowledge-graph-rag) 同时使用了这两者。\n\n有了所有可用的工具和教程，开始使用图形 RAG 是简单的部分……\n\n## …但将图形 RAG 投入生产是困难的\n\n这是数据科学中一个非常古老的故事：一种新的软件方法论、技术或工具在研究环境中解决了一些重要问题，但行业在将其构建为每天提供价值的产品时却面临困难。这不仅仅是软件开发的努力和专业水平的问题——即使是最大的、最优秀的团队也可能无法克服解决现实世界问题所涉及的现实数据的不确定性、不可预测性和不可控性。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*OklHNrhsNHZF6qzeRUSd_w.jpeg)\n\n不确定性是构建和使用数据中心系统的固有部分，这些系统几乎总是具有某些随机性、概率或无界输入的元素。而且，当输入和输出是非结构化的时，不确定性可能会更大，这正是 LLM 和其他 GenAI 应用程序的自然语言输入和输出的情况。\n\n想要尝试图形 RAG 的人通常已经拥有一个现有的 RAG 应用程序，该应用程序在简单用例中表现良好，但在一些更复杂的用例和需要跨知识库多个信息片段的提示中失败，可能涉及不同的文档、上下文、格式或甚至数据存储。当回答问题所需的所有信息都在知识库中，但 RAG 系统找不到时，这似乎是一个失败。从用户体验 (UX) 的角度来看，确实如此——没有给出正确的答案。\n\n但这并不一定意味着 RAG 系统存在“问题”，它可能正如其设计那样运行。如果没有问题或错误，但我们仍然没有得到想要的响应，那一定意味着我们期望 RAG 系统具备它根本没有的能力。\n\n在我们具体探讨为什么将图形 RAG 投入生产很困难之前，让我们先看看我们试图解决的问题。\n\n## 图形 RAG 解决的主要挑战\n\n因为普通的 RAG 系统（没有知识图谱）仅基于向量搜索来检索文档，所以只能检索与查询在语义上最相似的文档。那些完全不相似或相似度不够的文档则被排除在外，通常不会在查询时提供给生成响应的 LLM。\n\n当我们需要回答提示中的问题的文档并不都是与提示在语义上相似时，RAG 系统往往会遗漏一个或多个文档。这种情况可能发生在回答问题时需要混合一般性和专业性的文档或术语，并且当文档在细节上非常密集时，某些对这个特定提示非常重要的细节可能会被埋没在与该提示不太相关的相关细节中。请参见 [这篇文章，了解 RAG 如何遗漏文档的例子](https://bit.ly/3BKZAJv)，因为两个相关概念（在这种情况下是“太空针”和“下皇后安妮社区”）在语义上并不相似，并且 [请参见这篇文章，了解重要细节如何被埋没的例子](https://bit.ly/4ffhrqi)，因为向量嵌入是“有损”的。\n\n当我们看到检索“失败”未能找到正确的文档时，可能会想要尝试改进向量搜索或使其更贴合我们的用例。但这需要调整嵌入，而嵌入是复杂的、混乱的、计算成本高昂的，甚至微调的成本更高。此外，这甚至不是解决问题的最佳方法。\n\n例如，看看上面链接的例子，我们真的想使用一个将“太空针”和“下皇后安妮社区”在语义向量空间中放得很近的嵌入算法吗？不，微调或寻找一个将这两个术语在语义空间中放得非常近的嵌入算法可能会产生一些意想不到和不希望的副作用。\n\n最好不要强迫语义模型去做一个地理或旅游信息更适合的工作。如果我是一个依赖于了解这些地标所在社区的旅行或旅游公司，我宁愿建立一个能够确定这些信息的数据库——这个任务比让语义向量搜索完成同样的任务要容易得多……而且没有完全的确定性。\n\n因此，这里主要的问题是，我们有一些我们知道以某种方式相关的概念和信息，但在语义向量空间中并不相关。某些其他（非向量）信息来源告诉我们，我们正在处理的各种概念之间存在联系。构建图形 RAG 应用的任务是有效地将这些概念之间的连接捕捉到知识图谱中，并利用图形连接来检索更相关的文档，以响应提示。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*flPVNMUm83oc7H9Lt7U5AA.jpeg)\n\n总结一下我们试图通过图形 RAG 解决的问题：存在半结构化、非语义的信息连接着我非结构化文档中出现的许多概念——我希望利用这些连接信息来补充语义向量搜索，以检索最适合回答我用例中提示和问题的文档。我们只是想让检索变得更好，并希望使用一些外部信息或外部逻辑来实现这一点，而不是仅仅依赖语义向量搜索来连接提示与文档。\n\n## 将图形与 RAG 集成的指导原则\n\n考虑到上述动机——使用“外部”信息来建立语义搜索可能遗漏的文档连接——在构建和测试图形 RAG 应用程序时，我们可以牢记一些指导原则：\n\n1. 图形应包含高质量、有意义的概念和连接\n2. 概念和连接应与用例集中的提示相关\n3. 图形连接应补充而不是替代向量搜索\n4. 应优先考虑一步和两步图形连接的实用性；依赖于超过三步的连接应仅限于专业用例。\n\n也许在未来的文章中，我们将深入探讨遵循这些原则的细微差别和潜在影响，但目前我只想指出，这个列表旨在共同提高可解释性，防止过于复杂，并最大化构建和使用图形 RAG 系统的效率。\n\n遵循这些原则以及软件工程和数据科学的其他核心原则，可以增加成功构建有用且强大的图形 RAG 应用程序的机会，但在此过程中肯定会有陷阱，我们将在下一节中概述。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*twgres708JPQHa1uZrkwDA.jpeg)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*5U0k4GoHTFiQhKM2a6xdMA.jpeg)\n\n## 你的图形 RAG 应用可能无法投入生产的原因\n\n任何花费大量时间围绕数据、复杂算法、统计学和人类用户构建软件的人都可能理解，在构建像图形 RAG 这样的系统时存在很多不确定性。在数据准备和加载、构建知识图谱、查询和遍历图形、结果汇编和提示构建，以及几乎工作流程中的任何其他点，都可能发生意外情况。\n\n在上面，我们讨论了如何轻松实现图形 RAG 以获得初步结果，但要获得良好的结果，更不用说生产级别的结果了，可能会很困难。接下来，我们将看看在构建和测试图形 RAG 应用时可能遇到的一些潜在问题。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*1J9hwwZDuYZ3WNrxCl_cOA.jpeg)\n\n### 图形RAG的表现与普通RAG相差无几\n\n如果您的图形RAG系统的性能与普通RAG大致相同，可能有多种原因。一般来说，这似乎意味着图形并没有为系统增加价值，但这可能是由于低质量的知识图、图的利用不足、参数设置不佳或其他许多原因造成的。或者，根本没有问题；向量搜索可能在寻找正确的文档方面表现出色，而图形根本不需要。\n\n需要关注的事项：\n\n* 您是否有普通RAG无法很好处理的示例提示，但您期望图形RAG能够成功处理？您能否在这些提示上进行“调试”，看看后台发生了什么？\n* 知识图是否包含语义搜索可能无法建立的有意义连接？您能否找到在图中连接的概念对的示例，其相关文档在向量空间中相距甚远？知识图应该在“远离”的文档之间建立有意义的连接。\n\n### 你（仍然）看到幻觉\n\n如果你在使用图形 RAG 时看到的幻觉与使用普通 RAG 时不同，我会怀疑某处存在错误或参数设置不当。如果你看到的幻觉水平相似，这听起来像是一个超出图形方面的一般问题。\n\n需要关注的事项：\n\n* 你的文档集是否包含对引发幻觉的提示的正确响应？向量搜索是否找到了这些文档？\n* 从检索到的文档中获取的正确响应是否正确插入到传递给 LLM 的提示上下文中？\n\n### 图表“过大”\n\n当您的知识图谱“过大”或过于密集时，可能会出现两种主要问题。首先，可能会出现扩展性问题，我将在下面讨论。其次，图遍历可能导致“过多”的文档，这些文档必须重新排序和过滤。如果重新排序和过滤策略与检索和图遍历元素不兼容，您可能会在图刚发现重要文档后立即将其过滤掉。\n\n需要关注的内容：\n\n* 图遍历后返回了多少文档，多少被重新排序或过滤掉？通过强图连接找到的文档是否能成功存活于过滤中？\n* 您是否构建了一个充满适合您用例的有意义连接的知识图谱？在图中，您能否找到许多对您的用例过于通用或无关的概念或连接？您的知识图谱中有多少是由低质量信息组成的？\n\n### 图形“太小”\n\n根据上述，如果图形“太大”，可能会充满低质量的连接。而如果图形“太小”，我希望那里存在的连接是有意义的，这很好，但缺失的连接主要有两种类型。第一种是由于图形构建过程中出现的错误引起的。第二种是由于图形构建未针对其进行设计。不同上下文或不同格式的数据可能会被不同的图形构建方法以不同的方式处理。\n\n需要关注的内容：\n\n* 你是否使用带有实体/关键词提取的LLM构建了知识图谱？你是否捕获了每个文档中所有有意义的实体，还是LLM限制了其输出？\n* 在你的文档中，有哪些概念和连接是你期望出现在知识图谱中的，但似乎缺失了？你期待它们何时以及如何被添加到图谱中？为什么它们实际上没有被添加到图谱中？\n\n### 你找不到“适中”图表\n\n你是否觉得可以构建一个“过大”或“过小”的图表，但无法构建一个中等大小的图表？\n\n需要注意的事项：\n\n* 你正在更改哪些参数或方法来从小到大或反向变化？这些是否应该对图表质量产生如此大的影响？你能否研究一些根据所使用的图表构建设置意外出现或消失的图表元素？\n* 另请查看上面“过大”和“过小”部分的相关提示。\n\n### 你的实现需要新的软件或增加部署复杂性\n\n这是一个经典的数据科学问题：构建非常酷炫和前沿的方法，却看到开发团队拒绝或难以将你笔记本中的代码引入生产环境。坚持使用最流行、支持最好的以及大部分开源的工具，可以更容易地进入生产，特别是如果你的组织在其他地方已经使用这些工具的话。\n\n需要关注的事项：\n\n* 你的实现是否需要为图形创建新的数据存储？你[可能不需要图形数据库](https://www.datastax.com/blog/knowledge-graphs-for-rag-without-a-graphdb)，而且可能能够使用你的生产向量存储来处理图形。\n* 你是否在使用一些最流行的开源工具来构建AI应用程序，比如LangChain？这些工具可以减少代码复杂性，使应用程序更具可移植性，并扩展潜在的集成和进一步开发。\n\n### 你的实现无法扩展\n\n文章 [Scaling Knowledge Graphs by Eliminating Edges](https://thenewstack.io/scaling-knowledge-graphs-by-eliminating-edges/) 在 *The New Stack* 中展示了一种使图形 RAG 非常可扩展的方法。像上面提到的，最流行、支持最好的、并且大多数是开源的工具通常是无痛扩展的最佳路径，但这并不总是容易。\n\n需要关注的内容：\n\n* 哪一部分无法扩展？图遍历、重新排序、结果汇编，还是其他？请参见上面的“The graph is too big”以获取更多提示。\n* 你是否有某个特定组件无法很好地扩展？有时使用内存中的图形库，如 'networkx' — 或者甚至是图形数据库 — 来执行复杂的图形操作可能会造成资源瓶颈。你可能想要 [切换到更可扩展的图形操作选项](https://bit.ly/3YD5NAd)。\n* 你是否在使用并行 API 调用来处理大部分繁重的工作，还是在尝试在主应用逻辑中进行复杂或高成本的计算？\n\n## 在生产中利用图形RAG获得成功\n\n创建成功的图形RAG系统的关键在于构建一个知识图谱和遍历逻辑，以补充语义向量检索，而不是取代或与之竞争。图形设计应旨在在合适的时间连接正确的节点、知识、实体和文档，从而能够组装合适的文档，以产生最有帮助和可操作的查询响应。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*orXW5uw-geBo-WVtZUxWXQ.jpeg)\n\n关于Glean，值得注意的是，内部文档数据集是图形RAG的一个完美用例。知识图谱可以连接人、项目、产品、客户、会议、地点等——所有这些在数量上都受到组织规模和其工作内容的限制。构建和管理一个由数千名员工组成的图形比例如尝试对维基百科上提到的所有人或大型财务或法律文档数据库中的所有人做同样的事情要可行得多。因此，Glean做出的第一个重大决策可能是找到一个很好的图形RAG用例来解决问题。\n\n图形RAG系统的一个常被低估的方面是输入数据的质量和将其传输到目的地的管道的可靠性。这与数据工程和传统软件开发的关系大于与AI的关系。在以往的技术范式中，由于数据类型和访问方法的不兼容，连接不同的数据系统是一个挑战。现在，AI和LLMs使得将不同来源的非结构化数据整合成为可能，从而允许将来自各种来源的数据整合到一个单一的RAG系统中。这种集成能力使得LLMs能够处理和理解来自各种来源的非结构化数据，例如内部网页、维基、代码库、数据库、Google文档和聊天记录。仅仅将所有这些信息连接在一起，并通过单一接口使其可访问，就可以带来巨大的收益。\n\n## 前进的方向\n\n构建图 RAG 系统以满足任何用例需要利用基础组件，如向量和图的数据存储、嵌入和 LLM，并通过开源编排工具如 LangChain 和 LlamaIndex 进行增强。这些工具促进了强大、可扩展和高效系统的开发，承诺未来公司通过自动化和精简优化知识工作，实现显著成功。\n\n知识图谱和图 RAG 系统的公共成功，尤其是像 Glean 这样的公司的成功，展示了这些技术在内部用例中的有效性，通过提高组织效率创造价值。然而，面向外部的企业和消费者产品的更广泛应用潜力仍然基本未被开发，为其他公司提供了许多探索机会。\n\n值得注意的是，我们已经处于所谓的“信息时代”至少 30 年，而在过去一两年中，我们才真正开始将所有这些信息跨来源、跨思想、跨文档和跨概念连接起来，以便我们的软件系统能够进行与我们人类在知识工作中日常使用的推理、逻辑和判断相同的类型的推理。一些人称之为“智能时代”。\n\n虽然最初关注简单、直接的决策，但人工智能的轨迹正朝着管理更复杂场景的方向发展，显著提高时间和成本的效率。这一令人兴奋的演变使许多人工智能应用，包括图 RAG，在转变知识如何在各种背景下相互连接和利用方面变得至关重要。\n\n要立即开始使用图 RAG，或了解更多信息，请查看 [DataStax 图 RAG 指南](https://bit.ly/4862Lrl)。\n\n*作者：Brian Godsey, Ph.D. ([LinkedIn](https://bit.ly/4enqFRa)) — 数学家、数据科学家和工程师 // [DataStax](https://bit.ly/3NpPujA) 的人工智能和机器学习产品 // 著作 [Think Like a Data Scientist](https://bit.ly/4f5uVES)*\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Il1GrFN6fYN7e_ovExRGPw.jpeg)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*wQvZDIlkOvrYZnbwl0bEPQ.jpeg)\n\n"},{"lang":"zh","group":"blog","slug":"blog/the-real-reason-openai-abandoned-next-js-for-remix-a4b2622ee9b2","frontmatter":{"title":"OpenAI 放弃 Next.js 转而使用 Remix 的真正原因","meta_title":"OpenAI 放弃 Next.js 转而使用 Remix 的真正原因","description":"OpenAI 此举背后令人惊讶的原因及其对 Web 开发的未来意味着什么","date":"2024-11-08T00:25:31.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*bf8ao0JjEiMka6dJqp-hxg.jpeg","categories":["Technology/Web","Programming","Web Development"],"author":"Rifx.Online","tags":["Remix","Next.js","client-side","rendering","scalability"],"draft":false,"slug":"blog/the-real-reason-openai-abandoned-next-js-for-remix-a4b2622ee9b2"},"content":"\n### OpenAI 采取行动背后的惊人原因及其对未来网页开发的影响\n\n\n\n## 过渡介绍\n\nOpenAI 最近在开发者社区引起了轰动，因为它从 Next.js 转向了 Remix。\n\n这一意外的转变让许多人质疑如此重大变化的理由。\n\n但 **你能责怪他们吗？**\n\n以下是 **大多数开发者对 NextJS 的看法**，基于 [这篇](https://www.reddit.com/r/nextjs/comments/1f92jdv/chatgptcom_switched_from_nextjs_to_remix/) reddit 讨论：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GCrb_aGjh1nticKNeHh9Bg.png)\n\n这很艰难。\n\n但我也在 X 上询问了开发者，\n\n他们对小型项目的看法则有所不同：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*YmN2pTYaCFXFzb3AMjfoqQ.png)\n\n## 让我们深入探讨这个问题\n\n这次探索不仅仅是为了理解 OpenAI 的决定，还涉及到 **这对其他开发者和更广泛的科技领域意味着什么**。\n\n为了理解其背后的理由，我花了数小时分析代码库和工具。\n\n以下是我获得的见解。\n\n## 关于切换的技术见解\n\n理解这一过渡的技术方面是理解为什么 OpenAI 偏爱 Remix 的关键。\n\n我们检查了他们的应用架构，以识别 Next.js 和 Remix 之间的核心差异。\n\n### 客户端渲染与服务器渲染\n\nOpenAI 的应用程序专注于 **客户端渲染**，大部分处理发生在用户的浏览器中。\n\n这减少了对服务器渲染 HTML 的需求。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*50dv8sWPbwFp85fQu_dodQ.png)\n\n**Remix** 非常适合这些场景，因为它有效地管理客户端应用程序。这个选择确保了 OpenAI 的用户拥有更流畅、更灵敏的体验。\n\n### 初始页面加载过程\n\n当用户访问 ChatGPT 网站时，**预加载的 JavaScript 和 meta 标签** 参与了初始页面加载。\n\n这优化了客户端渲染过程。**Remix** 在管理这些元素方面表现出色，确保了顺畅快速的初始加载。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*65VlHo5RQObk-YGzBlkx3w.png)\n\n## 为什么这很重要\n\n### 改进的用户体验\n\n通过预加载必要的脚本和数据，用户在访问网站时会遇到更少的延迟，并享受到更灵敏的界面。\n\n### 高效加载\n\nRemix 处理这些预加载元素的能力意味着减少等待时间，提供更快的浏览体验。\n\n通过利用这些功能，\n\n*OpenAI 可以为其用户从一开始就提供更无缝和愉悦的体验。*\n\n## 深入探讨 OpenAI 利用的 Remix 关键特性\n\nOpenAI 利用 Remix 的几个关键特性来增强他们的应用程序。\n\n### 预加载策略\n\nRemix 预加载必要的数据和资源，减少加载时间并提升性能。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GPH2ZGhT_yfrQYpR2Xhvug.png)\n\n这一策略确保用户从一开始就能获得无缝的体验。\n\n### 数据管理与加载器\n\nRemix的加载器API有效地收集初始渲染所需的所有数据，并直接嵌入到HTML中。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*PmCiQNkAJlu2MSFEtpydiw.png)\n\n这种方法消除了额外的客户端数据获取的需求，加快了渲染过程。\n\n## 转向 Remix 的好处和影响\n\n转向 Remix 为 OpenAI 提供了多个优势，从性能提升到未来的发展前景。\n\n### 性能提升\n\n通过采用 Remix，OpenAI 实现了更快的初始加载时间和更流畅的客户端导航。\n\n这些性能提升有助于创建更具响应性和用户友好的应用程序。\n\n### Remix的未来前景\n\nRemix的灵活性和高效性为OpenAI的未来增长和创新奠定了基础。\n\n随着Remix的不断发展，OpenAI可以利用其先进的功能在网页开发的竞争环境中保持领先。\n\n### 为什么这很重要\n\n**改善用户体验**：用户从更快的页面加载和更流畅的浏览体验中受益。\n\n**高效开发**：Remix 的功能简化了开发过程，使 OpenAI 能够更快速地进行创新。\n\n**可扩展性**：Remix 的架构支持未来的增强和扩展，确保长期的可行性。\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/the-rise-of-the-ai-agent-product-manager-and-ai-agent-engineer-0905f1d30cce","frontmatter":{"title":"人工智能代理产品经理和人工智能代理工程师的崛起","meta_title":"人工智能代理产品经理和人工智能代理工程师的崛起","description":"想象一下未来，生成式人工智能不仅能响应查询，还能主动解决各个方面的复杂问题……","date":"2024-11-04T12:33:53.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*dlJ0a49_lRAPR1tTPs898w.png","categories":["Generative AI","Ethics","Technology"],"author":"Rifx.Online","tags":["Generative","Product","Manager","Engineer","Ethics"],"draft":false,"slug":"blog/the-rise-of-the-ai-agent-product-manager-and-ai-agent-engineer-0905f1d30cce"},"content":"\n\n\n\n\n想象一个未来，生成式AI不仅仅是响应查询，而是主动解决商业各个方面的复杂问题。这不是科幻小说，而是生成式AI代理迅速逼近的现实。这些代理有望彻底改变公司的运营，并激发一波新的创新，从简化供应链到优化产品开发，再到转变客户互动。\n\n在过去一年多的时间里，我一直在构建生成式AI应用和代理，亲眼见证了这些技术如何深刻重塑商业流程。AI的潜力巨大，从以空前高效处理客户查询的支持代理，到推动商业运营和决策的自主代理。这些代理不仅仅是在提升现有流程，而是在启用新的工作方式。\n\n例如，想象一个代理，它不仅仅是安排会议，而是理解你工作的背景，建议最有影响力的与会者，准备简报文件，甚至根据最近的公司动态提出议程项目。又或者考虑一个制造业中的代理，它不仅仅是监控生产线，而是预测维护需求，实时优化资源分配，并与设计团队合作，根据生产数据建议产品改进。\n\n这种AI驱动的转型正在创造对两个关键角色的需求：**AI代理产品经理**和**AI代理工程师**。这些专业人员不仅是我们AI增强未来的架构师和建设者，而且是一个协作团队的不可或缺的部分，工作在商业战略与前沿技术的交汇点上。\n\n## 引入新角色\n\n**AI Agent Product Manager** 是一位具有远见的人，能够识别代理创造价值的机会，设计其能力，并确保它们与业务目标和用户需求保持一致。他们在商业世界和AI可能性之间充当翻译，协调AI创新。\n\n**AI Agent Engineer** 则是将这些代理变为现实的技术高手。他们设计稳健的架构，创建复杂的提示，并通过与各种系统和数据源的无缝集成，确保代理基于公司数据和流程。\n\n由于我们仍处于这一技术周期的早期阶段，这些专业人员通常在专门的AI咨询公司或开发代理构建产品的公司（如Salesforce）工作。这使他们能够将最佳实践和行业创新带入每一个新项目。\n\n## AI代理产品经理：引领AI创新\n\n作为一名代理产品经理，您可能会在不同的用例上工作，比如一个月担任销售代理，下个月担任人力资源代理。让我们深入了解您的角色可能是什么样的：\n\n作为一名代理产品经理，假设您负责为一家跨国制造公司开发一个代理。您的第一步是与来自各个部门的高管领导一系列研讨会——运营、设计、销售和客户服务。您不仅仅在寻找渐进式的改进；您在寻找变革性的机会，而您通过促进组织内部的合作与理解来实现这一目标。\n\n通过这些讨论，您识别出一个颠覆性的可能性：一个可以连接客户反馈、产品设计和制造流程的代理。这个代理将分析客户评论和支持工单，识别流行的问题或期望的功能，并自动生成设计修改建议。然后，它将模拟这些变化对制造流程和成本的影响。\n\n作为代理的产品经理，您主要的责任之一将是绘制代理的旅程。这涉及定义从初始互动到最终结果的每一步，确保一切与业务目标一致。您需要识别代理将要进行的关键互动，了解这些互动的背景，并确定每个旅程应该实现的目标。您还需要考虑一些关键问题，例如：代理将如何优先考虑客户反馈？它如何有效地向工程团队提出设计建议？当AI影响产品决策时，必须解决哪些伦理考虑？\n\n您将与利益相关者密切合作，以定义成功指标。例如，您可能决定代理的目标是将识别产品问题到实施修复的时间缩短50%，同时提高客户满意度评分。\n\n随着项目的进展，您确保代理提供真实的商业价值。您可能会审查AI与设计团队之间的模拟和实际对话，调整代理的沟通风格，以更好地与工程师产生共鸣。或者，您可能会仔细研究代理的建议如何影响产品质量和客户满意度的数据，寻找进一步提高其性能的方法。\n\n在整个过程中，您不仅在考虑代理当前的能力，还在考虑其未来的潜力。这个代理如何演变以响应客户反馈并预测未来市场趋势？它是否有可能在未来参与产品团队的头脑风暴会议，提供数据驱动的见解以推动创新？\n\n您的代理产品经理角色使您处于商业转型的最前沿。您不仅仅是在实施一个新工具；您正在重塑整个组织在AI时代的思维、创新和运营方式。\n\n## AI代理工程师：打造智能可靠的系统\n\n现在，让我们转变思路，进入同一项目中的代理工程师角色：\n\n您的挑战是创建一个能够理解客户反馈、将其转化为可行设计见解并与制造系统接口的代理。这并非易事——这需要对大型语言模型、复杂的提示工程和强大的系统集成有深刻的理解。\n\n您首先选择一个合适的大型语言模型作为代理的基础。然而，您真正的工作在于设计一个全面的代理架构，使其能够在许多对话旅程中可靠地执行。\n\n作为代理工程师，您主要关注之一是创建和完善代理的提示结构。您设计复杂的提示，有效引导模型的行为，确保它在各种场景中始终提供相关且准确的响应。这可能涉及开发一个层次化的提示系统，能够处理从监督多个代理到导航各种旅程的所有事务。\n\n您将花费大量时间评估代理的行为和输出，优化提示和流程，并发布新版本。您甚至可能设计和实施一个严格的测试框架，模拟数千种潜在对话轨迹。您的目标是确保代理的响应是确定性的，并与任何给定输入的期望结果保持一致。\n\n例如，您可能会创建一套测试用例，涵盖各种类型的客户反馈，从简单的产品问题到复杂的功能请求。然后，您系统地处理这些案例，分析代理的响应，并迭代提示结构和决策逻辑，以提高性能。\n\n当您遇到代理行为不一致或不理想的边缘案例时，您并不是简单地调整提示。相反，您深入研究代理的决策过程，系统性地调整基础逻辑和提示结构，以解决这些问题。\n\n集成仍然是您角色的关键部分。您正在设计API，使代理能够从客户支持数据库中提取数据，访问产品设计文件，并将数据输入到制造规划系统中。但不仅仅是连接系统，您还专注于确保代理能够基于这些集成数据做出智能决策。\n\n伦理和安全仍然是关键问题。您实施了强有力的保障和监督机制，以确保AI不会建议可能危及产品安全的设计变更。您还构建了解释性功能，以便AI始终能够展示其任何建议的推理，这对于与使用代理的工程师和设计师建立信任至关重要。\n\n作为代理工程师，您的角色不仅涉及创建一个功能性AI系统，还涉及打造一个能够可靠和一致地推动整个产品开发和制造过程中的创新和效率的智能代理。这一复杂的挑战使您处于AI技术的最前沿，塑造着企业在AI时代的运营未来。\n\n## 伦理考量与协作的力量\n\n随着代理在企业中变得越来越重要，代理产品经理和代理工程师的角色将愈发重要。这些角色不仅仅涉及技术能力或战略洞察力——它们还要求对伦理考量有深刻的承诺。由于这些代理影响着重要的商业决策，背后的专业人士必须确保这些系统是透明的、公平的，并与更广泛的社会价值观相一致。\n\n这个代理的成功在很大程度上依赖于产品经理和工程师之间的无缝协作。你们将共同迭代代理的能力，排除问题，并推动可能性的边界。\n\n## 比较角色：代理产品经理与代理工程师\n\n以下是一个总结比较表，以强调代理产品经理与代理工程师之间的差异：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*T26nkBI-wID26X2NrE2SSQ.jpeg)\n\n关键要点：\n\n* **代理产品经理：** 该职位专注于代理的战略和业务方面，确保它们提供价值并与公司目标保持一致。\n* **代理工程师：** 该职位集中于技术实现，确保代理可靠运行并与现有系统无缝集成。\n\n## 未来属于你：迎接挑战\n\n随着人工智能的影响力不断扩大，Agent Product Manager 和 Agent Engineer 的角色将处于这场技术革命的最前沿。无论你是在为 AI 驱动的业务转型定义战略，还是在设计驱动智能代理的复杂系统，你都将在塑造商业的未来。\n\n这些角色需要独特的技能组合：战略思维、技术专长、创造力，以及对商业和人工智能的深刻理解。它们提供了在尖端技术上工作的机会，同时推动实际的商业影响。\n\n那么，未来的 Agent Product Managers 和 Engineers，你们准备好迎接挑战了吗？人工智能增强的未来正等待着你们的专业知识和愿景。无论你是对产品管理的战略方面感兴趣，还是对代理工程的技术细节充满热情，在这个令人兴奋的新领域中都有你的一席之地。问题不在于人工智能是否会改变商业，而在于如何改变——而你可能就是决定者。\n\n"},{"lang":"zh","group":"blog","slug":"blog/top-25-generative-ai-terminologies-you-must-know-6a3bb0300988","frontmatter":{"title":"您必须了解的 25 个顶级生成式人工智能术语","meta_title":"您必须了解的 25 个顶级生成式人工智能术语","description":"本文介绍了25个生成性人工智能领域的核心术语，包括生成模型、Transformer、GAN、自编码器等。每个术语都提供了定义、示例及相关资源，以帮助技术专业人士和其他领域人士深入理解生成性人工智能的关键概念。这些术语的掌握对于参与人工智能项目、准备面试以及跟踪行业动态至关重要。","date":"2024-11-14T03:29:09.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*swCl2YJj6wfAc9J3v6AqTg.png","categories":["Generative AI","Machine Learning","Data Science"],"author":"Rifx.Online","tags":["Generative","Transformers","GANs","Autoencoders","Zero-Shot"],"draft":false,"slug":"blog/top-25-generative-ai-terminologies-you-must-know-6a3bb0300988"},"content":"\n\n\n*掌握关键概念，以清晰的解释、实际应用和深入的资源在生成性人工智能领域脱颖而出*\n\n\n\n生成性人工智能确实是各行业中的关键技术；因此，理解生成性人工智能的核心概念对任何技术专业人士及其他领域的人士来说都是至关重要的。以下综合指南涵盖了25个必须了解的生成性人工智能术语，提供清晰的定义、实际的例子和其他资源，以加深您的知识。无论是为面试做准备、参与人工智能项目，还是跟上这个快速变化领域的动态，掌握这些术语都将为您在生成性人工智能领域打下坚实的基础。\n\n## 1\\. 生成模型\n\n* **定义**：一种从学习到的模式中生成新数据点的AI模型。\n* **示例**：生成预训练变换器（GPT）根据输入提示生成类人文本。\n* **了解更多**：[生成模型简介](https://proxy.rifx.online/https://www.datacamp.com/blog/what-is-a-generative-model)\n\n## 2\\. Transformer\n\n* **定义**：一种神经网络架构，利用自注意力机制处理和生成序列，例如文本或图像。\n* **示例**：BERT 是一种用于问答和文本分类等任务的 Transformer 模型。\n* **了解更多**：[理解 Transformers](https://proxy.rifx.online/https://www.turing.com/kb/brief-introduction-to-transformers-and-their-power)\n\n## 3\\. 潜在空间\n\n* **定义**：一个多维空间，生成模型在其中映射数据，使其能够学习和生成变体。\n* **示例**：在图像生成中，相似的图像在潜在空间中彼此靠近。\n* **了解更多**：[探索人工智能中的潜在空间](https://proxy.rifx.online/https://www.perplexity.ai/page/latent-space-101-what-it-is-an-mwXuxYfzS_.J4e_uFvOskg)\n\n## 4\\. GAN（生成对抗网络）\n\n* **定义**：一种人工智能，利用两个神经网络——生成器和判别器——相互对抗以生成逼真的数据。\n* **示例**：GAN生成看起来逼真的面孔，但这些面孔并不属于真实的人。\n* **了解更多**：[什么是GAN，它们是如何工作的？](https://proxy.rifx.online/https://aws.amazon.com/what-is/gan/#:~:text=A%20generative%20adversarial%20network%20(GAN,from%20a%20database%20of%20songs.)\n\n## 5\\. 自编码器\n\n* **定义**：一种神经网络，学习压缩和重构数据，通常用于降维和去噪等任务。\n* **示例**：自编码器用于从损坏的图像中去除噪声。\n* **了解更多**：[自编码器介绍](https://proxy.rifx.online/https://towardsdatascience.com/introduction-to-autoencoders-7a47cf4ef14b)\n\n## 6\\. 扩散模型\n\n* **定义**: 学习逆转噪声添加过程的模型，以从噪声中生成详细且一致的数据。\n* **示例**: 扩散模型在 DALL\\-E 2 中用于从随机噪声生成高质量图像。\n* **了解更多**: [理解扩散模型](https://proxy.rifx.online/https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/)\n\n## 7\\. 提示工程\n\n* **定义**：精心设计输入提示的过程，以优化模型生成的输出。\n* **示例**：修改GPT\\-4中的输入提示以生成更简洁的摘要。\n* **了解更多**：[提示工程指南](https://proxy.rifx.online/https://www.datacamp.com/tutorial/a-beginners-guide-to-chatgpt-prompt-engineering)\n\n## 8\\. 零样本学习\n\n* **定义**：模型在未明确训练的任务上执行任务的能力，通过利用其他任务的知识。\n* **示例**：GPT\\-3 可以在没有针对翻译数据集进行特别训练的情况下执行翻译。\n* **了解更多**：[什么是零样本学习？](https://proxy.rifx.online/https://www.ibm.com/topics/zero-shot-learning)\n\n## 9\\. 少样本学习\n\n* **定义**：模型在仅有少量示例的情况下学习任务的能力，最大限度地减少对大量训练数据的需求。\n* **示例**：GPT\\-3 可以通过最少的输入样本进行微调，以特定风格进行写作。\n* **了解更多**：[少样本学习解释](https://proxy.rifx.online/https://www.ibm.com/topics/few-shot-learning#:~:text=IBM-,What%20is%20few%2Dshot%20learning%3F,suitable%20training%20data%20is%20scarce.)\n\n## 10\\. 强化学习\n\n* **定义**：一种学习范式，AI代理通过与环境互动来学习决策，以最大化累积奖励。\n* **示例**：AlphaGo使用强化学习通过与自己对弈数百万局来精通围棋。\n* **了解更多**：[生成式AI的强化学习](https://proxy.rifx.online/https://dl.acm.org/doi/pdf/10.1613/jair.1.15278)\n\n## 11\\. 变分自编码器 (VAE)\n\n* **定义**：一种自编码器，通过引入随机性来学习生成新数据，从而对其潜在空间表示进行建模。\n* **示例**：VAE 被用于生成新的人脸，并在不同的面部特征之间平滑过渡。\n* **了解更多**：[VAE 及其应用](https://proxy.rifx.online/https://www.datacamp.com/tutorial/variational-autoencoders)\n\n``` \n## 代码块内容保持不变\n```\n\n## 12\\. 自监督学习\n\n* **定义**：一种学习技术，模型从数据中生成自己的标签，从而减少对标记数据集的依赖。\n* **示例**：BERT通过在句子中掩盖单词并在训练过程中预测它们来使用自监督学习。\n* **了解更多**：[什么是自监督学习？](https://proxy.rifx.online/https://www.ibm.com/topics/self-supervised-learning)\n\n```\n## Code block example\ndef self_supervised_learning():\n    pass\n```\n\n## 13\\. 分词\n\n* **定义**：将文本拆分为更小的单元，如词或子词，以便模型更容易处理的过程。\n* **示例**：文本输入在输入到 GPT\\-4 进行处理之前被分词为单词。\n* **了解更多**：[自然语言处理中的分词](https://proxy.rifx.online/https://www.datacamp.com/blog/what-is-tokenization)\n\n## 14\\. Beam Search\n\n* **定义**：一种搜索算法，通过扩展多个潜在的标记序列，在解码过程中生成最可能的序列。\n* **示例**：Beam search 在机器翻译中用于生成连贯的文本输出。\n* **了解更多**：[Beam Search Explained](https://proxy.rifx.online/https://www.width.ai/post/what-is-beam-search)\n\n## 15\\. 迁移学习\n\n* **定义**：在一个任务上使用预训练模型，并对其进行微调以适应另一个任务的过程，通常所需的数据更少。\n* **示例**：在通用语言任务上预训练后，对情感分析任务进行BERT的微调。\n* **了解更多**：[什么是迁移学习？](https://proxy.rifx.online/https://aws.amazon.com/what-is/transfer-learning/)\n\n## 16\\. 语言模型\n\n* **定义**：一种预测自然语言中词序列概率的模型，帮助生成或理解文本。\n* **示例**：GPT\\-4 是一种能够为广泛应用生成连贯文本的语言模型。\n* **了解更多**：[语言模型简介](https://proxy.rifx.online/https://developers.google.com/machine-learning/resources/intro-llms)\n\n## 17\\. 人工智能中的偏见\n\n* **定义**：人工智能系统由于训练数据或算法的偏见，倾向于产生有利于或歧视某些群体的结果。\n* **示例**：基于有偏见历史数据训练的人工智能招聘系统中的性别偏见。\n* **了解更多**：[理解人工智能中的偏见](https://proxy.rifx.online/https://www.ibm.com/topics/ai-bias)\n\n## 18\\. GPT (生成预训练变换器)\n\n* **定义**：一个大型语言模型，基于对广泛文本语料库的预训练和微调生成类人文本。\n* **示例**：GPT-4 生成论文、故事和对用户查询的详细回应。\n* **了解更多**：[GPT 的工作原理](https://proxy.rifx.online/https://tecknoworks.com/how-gpt-works-and-its-core-mechanics/)\n\n## 19\\. 困惑度\n\n* **定义**：一种衡量语言模型预测给定单词序列效果的指标，困惑度越低表示性能越好。\n* **示例**：比较 GPT\\-3 和 GPT\\-4 的困惑度，以评估它们的文本生成质量。\n* **了解更多**：[语言模型中的困惑度](https://proxy.rifx.online/https://huggingface.co/docs/transformers/en/perplexity)\n\n## 20\\. 自然语言处理 (NLP)\n\n* **定义**：一个专注于计算机与人类通过自然语言进行交互的人工智能领域，涵盖翻译和情感分析等任务。\n* **示例**：NLP模型用于对客户评价进行情感分析。\n* **了解更多**：[NLP简介](https://proxy.rifx.online/https://towardsdatascience.com/a-gentle-introduction-to-natural-language-processing-e716ed3c0863)\n\n## 21\\. 神经网络\n\n* **定义**：一种受人脑神经元网络启发的计算系统，由多个相互连接的节点层组成，用于图像识别和语言处理等任务。\n* **示例**：卷积神经网络（CNN）用于识别图像中的物体。\n* **了解更多**：[什么是神经网络？](https://proxy.rifx.online/https://www.ibm.com/topics/neural-networks)\n\n## 22\\. 训练数据\n\n* **定义**：用于训练AI模型的数据，通过让它们从示例中学习，提高它们识别模式和进行预测的能力。\n* **示例**：像ImageNet这样的大型图像数据集用于训练AI模型进行图像分类任务。\n* **了解更多**：[AI中的训练数据](https://proxy.rifx.online/https://www.oracle.com/artificial-intelligence/ai-model-training/)\n\n## 23\\. 注意力机制\n\n* **定义**：神经网络中的一种方法，帮助模型关注输入序列中最相关的部分，从而提高机器翻译和文本生成等任务的性能。\n* **示例**：注意力机制允许模型在语言翻译时关注句子中的重要单词。\n* **了解更多**：[什么是注意力机制？](https://proxy.rifx.online/https://machinelearningmastery.com/the-attention-mechanism-from-scratch/)\n\n## 24\\. 纪元\n\n* **定义**：在机器学习模型训练过程中，经过整个训练数据集的一次完整遍历。\n* **示例**：训练神经网络10个纪元，以确保其正确学习而不发生过拟合。\n* **了解更多**：[理解机器学习中的纪元](https://proxy.rifx.online/https://www.geeksforgeeks.org/epoch-in-machine-learning/)\n\n## 25\\. 多模态人工智能\n\n* **定义**：能够同时处理和生成来自多种模态（例如文本、图像和音频）数据的人工智能。\n* **示例**：CLIP 同时处理图像和文本，以生成图像的标题。\n* **了解更多**：[什么是多模态人工智能？](https://proxy.rifx.online/https://www.techtarget.com/searchenterpriseai/definition/multimodal-AI)\n\n请记住，掌握生成式人工智能是一个逐步的过程。在学习这些概念时，确保通过提供的资源深入探索每一个概念，参与讨论，并尝试将所学应用于您的项目。与这些资源和对话的互动将帮助您理解术语及其在现实世界中的使用。\n\n感谢您的阅读！如果您觉得本指南对您有帮助，请与其他可能希望提升生成式人工智能理解的人分享。我们共同学习，并因此更好地应用这些概念。\n\n如果您有任何想法、问题，或甚至认为可能有帮助的额外资源建议，请在下面的评论区留言。\n\n祝您在生成式人工智能的世界中探索愉快！\n\n*通过 [linktr.ee](https://proxy.rifx.online/https://linktr.ee/tharunkumarreddypolu) 与我联系，了解更多信息！*\n\n## 用简单英语 🚀\n\n*感谢您成为 [**用简单英语**](https://proxy.rifx.online/https://plainenglish.io/) 社区的一部分！在您离开之前：*\n\n* 一定要 **点赞** 和 **关注** 作者 ️👏**️️**\n* 关注我们： [**X**](https://proxy.rifx.online/https://x.com/inPlainEngHQ) \\| [**LinkedIn**](https://proxy.rifx.online/https://www.linkedin.com/company/inplainenglish/) \\| [**YouTube**](https://proxy.rifx.online/https://www.youtube.com/channel/UCtipWUghju290NWcn8jhyAw) \\| [**Discord**](https://proxy.rifx.online/https://discord.gg/in-plain-english-709094664682340443) \\| [**Newsletter**](https://proxy.rifx.online/https://newsletter.plainenglish.io/) \\| [**Podcast**](https://proxy.rifx.online/https://open.spotify.com/show/7qxylRWKhvZwMz2WuEoua0)\n* [**在 Differ 上创建一个免费的 AI 驱动博客。**](https://proxy.rifx.online/https://differ.blog/)\n* 更多内容请访问 [**PlainEnglish.io**](https://proxy.rifx.online/https://plainenglish.io/)\n\n"},{"lang":"zh","group":"blog","slug":"blog/top-5-ai-tools-for-ios-developers-5ee9f39558ac","frontmatter":{"title":"面向 iOS 开发人员的 5 大人工智能工具","meta_title":"面向 iOS 开发人员的 5 大人工智能工具","description":"本文介绍了五大AI工具，旨在提升iOS开发者的工作效率。首先是Cursor/VSCode，通过GitHub Copilot实现快速编码和智能重构。其次是GitHub Copilot的Xcode扩展，提供AI辅助编辑功能。第三是Swift Assist，虽然尚未完全可用，但有潜力生成代码。接着是ChatGPT及其衍生工具，适合快速迭代代码。最后是Alex Sidebar和AIProxy，分别为Xcode提供扩展功能和安全集成AI API。整体上，这些工具为iOS开发者提供了高效的编码体验。","date":"2024-11-14T03:29:09.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*6Hs8174FgiwTv87e.jpg","categories":["Programming","Technology/Web","Generative AI"],"author":"Rifx.Online","tags":["Cursor","VSCode","GitHub","Copilot","Swift"],"draft":false,"slug":"blog/top-5-ai-tools-for-ios-developers-5ee9f39558ac"},"content":"\n### 提高工作流程速度与效率\n\n\n\n虽然关于人工智能的讨论很多，但我想让你回归现实。无论你是否已经在使用 AI 辅助的编码工具，或者觉得这一切都是无稽之谈……这篇标题吸引眼球的文章可能适合你。\n\n虽然你可能已经能找到很多关于如何使用各种工具来提高你的技能、效率和准确性的文献，但对于我们 iOS 开发者来说，这要复杂一些。因为我们依赖 Xcode 及其工具链来构建我们的应用，所以我们很难不使用 Xcode。而我将在接下来的段落中列出和解释的并不是所有工具都与跳过 Xcode 有关。\n\n## 1\\. Cursor / VSCode\n\n显然，这是列表的首位。除非你一直在石头下冬眠，否则你可能听说过VSCode。在Swift项目中使用它并不是新鲜事。内置于VSCode的GitHub Copilot允许你以光速编码，而无需进行太多设置。他们最近在VSCode中集成了更多的Copilot功能，越来越接近Cursor。除了标签补全外，你现在还可以进行内联聊天和代码生成。\n\nCursor是VSCode的一个分支，按照我的经验，他们的Cursor标签补全功能比VSCode更快、更准确。\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*pQ4SuReyicAiBCG3.gif)\n\n他们还做了一些让我节省了无数小时的事情：智能/AI辅助重构。这可能是值得Cursor订阅的最佳功能之一。\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*JlzVJ6o18sulIUEeo_p5sg.gif)\n\n而且这不仅仅是重构；在更改一行后，它在智能编辑中也会表现出色。Cursor会显示一个“标签”指示符，表示它对你刚刚编辑的代码部分提出了更改建议。只需按下标签键即可级联更改，这样可以不断进行下去。标签标签标签。\n\n一旦你进入状态，你会发现自己能多么高效。我编码的流程和往常一样，但因为我需要写的代码少得多，所以速度更快。你使用它编码的越多，它就越能学习你的项目、编码风格等……一开始可能会显得有些不适应，但相信我，给它一点时间。\n\n你还可以通过内联聊天生成代码：\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*pVzU2MZ0vNFQ6-dRaWPy-w.gif)\n\n当你需要一个特定的算法，或者在现有代码中拥有所有上下文但需要编写一些繁琐的部分时，这非常有用。它的效果相当不错，也能节省很多时间。别忘了审查生成的代码 :)\n\n为了专门开始iOS开发，我鼓励你阅读我另外两个故事：\n\n一个是关于如何设置它，安装正确的扩展等……\n\n另一个是关于如何将你的Xcode项目从基于组的转换为基于文件夹的，以便你可以在VSCode/Cursor中自由创建/删除/移动文件，而无需触碰.xcodeproj / Xcode。\n\n这只是Cursor/VSCode在iOS开发中的表面。但你今天就应该开始！\n\n## 2\\. GitHub Copilot Xcode 扩展\n\n这是一个最近发布的扩展，最初是 [Intitni](https://proxy.rifx.online/https://github.com/intitni/CopilotForXcode) 的一个项目，但似乎 GitHub 已经对其进行了分叉/收购，并使其成为 Copilot + Xcode 的官方扩展。到目前为止，虽然用户体验并不完美（可以理解，因为他们必须与可访问性/窗口 API 一起工作），但它比 Apple（本地）Xcode 模型要好得多。\n\n而你很幸运，我已经写过关于它的内容：\n\n如果你还不准备切换到 Xcode 以外的其他编辑器，但仍想使用高效的 AI 辅助代码编辑，那么这个扩展就是为你准备的！\n\n## 3\\. Swift Assist\n\n虽然 Xcode 已经内置了一个用于预测代码补全的本地模型（仅在 Xcode 16 的 Apple Silicon Mac 上可用），但 Apple 在 WWDC 上透露了其他内容：\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*-mlY8GyGh3VPVhyg3TmLYw.png)\n\nSwift Assist\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*XJnlRo8mqrAMZEVEL64ufg.gif)\n\n这看起来像是我上面演示的 Cursor 的聊天 + 代码生成。它应该能够根据你的评论生成代码。但目前，这仍然是一个虚幻的产品。Xcode 16\\.2 beta 2 提到了它，但我们仍然无法进行测试。\n\n也许它会在 Xcode 16\\.2 beta 的后续版本中推出，我迫不及待想要测试并写关于它的内容！\n\n## 4\\. ChatGPT/Claude/Perplexity 网络界面\n\n有时候，回归基础是最好的选择。虽然这些代码编辑器使用了Anthropic和OpenAI的模型以及它们自己的模型，但在当今的环境中，使用它们的网络界面也是一种宝贵的工具。\n\n### ChatGPT \\+ Canvas\n\nOpenAI 的 ChatGPT 在过去几个月中有了很大的进展。最近发布的 o1-preview 版本带来了推理和画布功能，使得在 ChatGPT 网页界面中进行编码会话变得更加顺畅。\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*WUraNCcZMrCRrHMilgzl-Q.png)\n\n画布是一个构建在 ChatGPT 网页界面上的迷你代码编辑器，允许您快速迭代代码和想法。您可以使用聊天进行增量更改，还有其他一些工具可以对代码进行注释、进行内联更改、转换为其他语言等。\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ab7PdMLJwacZtVsET2YYmA.gif)\n\n虽然这不能让您构建完整的应用程序，但它是一个在标准编辑器之外快速迭代代码想法的好工具。\n\n### Claude 伪影\n\n这与 ChatGPT Canvas 类似，但具有一些其他功能，例如预览（显然不支持 Swift/SwiftUI）和同时处理多个文件。\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*2iverELFSGqnJzklPK0cYg.png)\n\n## 5\\. Alex Sidebar\n\n这是一个新的竞争者！前提很简单，因为 Xcode 是闭源的，扩展 API 相当有限，为什么不围绕 Xcode 构建呢？\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*vZgn_FjH0FW53c7qZ4DZAg.png)\n\n我对用户体验并不太满意，但它提供了大多数 Cursor 功能，作为一个像窗口一样构建的 Xcode 侧面板。这里有各种快捷键 + 代码补全 + 聊天。你绝对应该尝试一下，看看它是否能改善你的工作流程！\n\n## 6\\. AIProxy\n\n作为（3）Swift Assist的额外奖励，它并不是真正的….可用\n\n这不是一个用于编码的工具，而是一个为构建者准备的工具。当在你的iOS应用中集成AI API时，你很可能需要将API密钥添加到你的项目中。但正如我们所知（对吧！），你不应该将其放在客户端。如果这样做，几乎任何人都可以轻易获取你的API密钥，并代表你使用你的AI积分。\n\n进入[AIProxy](https://proxy.rifx.online/https://www.aiproxy.pro/)，他们提供开源SDK，易于集成，并支持你所需的所有AI提供商。\n\n如果你不想构建一个后端来代理你的AI调用，这就是适合你的工具！\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/unified-memory-across-chatgpt-claude-perplexity-24809dc56717","frontmatter":{"title":"跨 ChatGPT、Claude、Perplexity 的统一内存","meta_title":"跨 ChatGPT、Claude、Perplexity 的统一内存","description":"您一定会喜欢这个，特别是如果您已经使用过 Claude、ChatGPT 和 Perplexity。","date":"2024-11-08T00:24:33.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*qrHnDS6YODZuVXZ4eeWZrQ.png","categories":["Chatbots","Programming/Scripting","Technology/Web"],"author":"Rifx.Online","tags":["Mem0","Chrome","extension","context","memory"],"draft":false,"slug":"blog/unified-memory-across-chatgpt-claude-perplexity-24809dc56717"},"content":"\n你一定会喜欢这个，特别是如果你已经与 Claude、ChatGPT 和 Perplexity 紧密联系在一起。\n\n与不同的 AI 助手互动有时会感觉有些脱节。\n\n在切换 ChatGPT、Claude、Perplexity 和其他助手时，你必须一遍又一遍地重复相同的上下文。\n\n如果它们都能共享一个通用记忆以增强上下文，那该多好啊？\n\n我发现了这个很棒的 Chrome 扩展，它对我来说真是个救星。\n\n想象一下，无论你在与哪个 AI 聊天，上下文都能无缝传递的对话。\n\n听起来不错，是吧？\n\n\n\n### 它解决的重大问题\n\n在我深入探讨之前，我们如何保持 LLMs 更新以获取最新知识？\n\n有几种传统的方法可以解决这个问题：\n\n1. **基于检索的方法**：这些方法从知识库中提取信息。它们功能强大，但可能会因为冗余数据和管理不断增长的存储库而变得混乱。\n2. **模型编辑**：这是一种调整模型以适应新事实的方法。它适用于简单的单句更新，但在处理较长、更复杂的信息时会遇到困难。\n3. **长上下文方法**：这些方法将所有知识塞入模型的上下文中。这就像是用数据过载模型，但由于上下文长度有限，这并不实用。\n\n所有这些方法都有其缺点，特别是在我们需要在不同的 AI 平台之间进行最新、无缝的交互时。\n\n### 进入 Mem0：AI 助手的智能记忆检索\n\nMem0 通过创建一个通用的记忆层，解决了这些问题，使其能够在多个 AI 助手之间工作。\n\n它带来了以下优势：\n\n* **通用记忆层**：在 ChatGPT、Claude、Perplexity 等之间轻松共享上下文。不再需要重复自己！\n* **智能上下文检测**：自动从您的对话中提取相关信息，因此您无需手动输入任何内容。\n* **智能记忆检索**：Mem0 在合适的时机调出正确的记忆，使您的互动更加顺畅。\n* **一键与 ChatGPT 同步**：如果您一直在使用 ChatGPT，可以通过一次点击将现有记忆同步。\n* **记忆仪表板**：一个方便的地方，可以集中管理所有记忆。\n\n例如，如果您与 Claude 开始对话：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*QQAJPzQp2tjBFgi-9InG1Q.png)\n\n它将提取关键信息并将其添加到记忆中。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MtctEzunD72Tw_dOOCOnyA.png)\n\n然后，下次您问一个相关问题时：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*r0WyCIHGvmiGm0GZ6xR1pw.png)\n\n它将添加相关上下文：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*PTEhFUJ4nrhQbeZXIAnD6A.png)\n\n但更有趣的是。\n\n如果您打开 Perplexity 并问另一个问题：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*54BKW9BQWwCDdXdAg0U-mA.png)\n\n它将跨不同应用程序获取记忆并增强上下文：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*FD6MwGtL8WpovUH3o1Vw3g.png)\n\n这真是太酷了！\n\n如果您打开 ChatGPT 并问一个问题：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*qLmDZh5NofuArxzS4OnDIQ.png)\n\n同样，它将从记忆中提取相关信息并增强上下文。\n\n随着您与它的互动增多，它将收集关键信息，为您节省大量输入时间，同时与 Claude、Perplexity 和 ChatGPT 进行交互。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MqgEr6hi5cHzHRuTqODzuA.png)\n\n您还可以向记忆中添加新信息：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Xo6jLLIGKqxvYR9SivuMhQ.png)\n\n### 如何开始\n\n安装 Mem0 非常简单：\n\n1. [**将扩展程序添加到 Chrome**](https://chromewebstore.google.com/detail/mem0/onihkkbipkfeijkadecaafbgagkhglop?hl=en-GB)\n\n**2\\. 登录**：\n\n* 安装后，您会在工具栏中看到 Mem0 图标。\n* 点击它并使用 Google 登录。\n\n**4\\. 开始聊天**：\n\n* 使用任何支持的 AI 助手。\n* 对于 ChatGPT 和 Perplexity，只需像平常一样聊天。\n* 在 Claude 上，点击 Mem0 按钮或使用快捷键 `^ + M`。\n\nMem0 最棒的地方之一就是它是完全免费的。没有：\n\n* **使用限制**\n* **广告**\n* **所有功能均包含在内**\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/unlocking-mixture-of-experts-moe-llm-your-moe-model-can-be-embedding-model-for-free-f192b9c07a5f","frontmatter":{"title":"解锁混合专家 (MoE) LLM：你的 MoE 模型可以免费嵌入模型","meta_title":"解锁混合专家 (MoE) LLM：你的 MoE 模型可以免费嵌入模型","description":"混合专家 (MoE) LLM 可以免费用作嵌入模型。","date":"2024-11-04T12:30:57.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*mB6VhEyAvxAxGbLDG_6hTw.png","categories":["Machine Learning","Natural Language Processing","Data Science"],"author":"Rifx.Online","tags":["Mixture-of-Experts","MoE","embedding","MoEE","BERTopic"],"draft":false,"slug":"blog/unlocking-mixture-of-experts-moe-llm-your-moe-model-can-be-embedding-model-for-free-f192b9c07a5f"},"content":"\n### Mixture-of-experts (MoE) LLM 可以作为免费的嵌入模型使用。\n\n\n\n我最近发现了一篇有趣的论文，标题为“你的 Mixture-of-Experts LLM 秘密地是一个免费的嵌入模型。”\\[1\\] 最近的 LLM 架构趋势是解码器模型，这对于嵌入模型并不适用，因为它们的注意力方法。然而，作者揭示了 Mixture-of-Experts (MoE) LLM 可以作为嵌入模型来执行多种嵌入相关的任务，而无需进一步的微调。在这篇博客中，首先让我们回顾一下 MoE，我将介绍它的工作原理及其实际应用。\n\n## 目录\n\n1. 什么是专家混合模型（MoE）？\n2. MoE 如何作为嵌入模型工作？\n3. 实际实施：使用 BERTopic 利用 MoEE\n\n## 1\\. 什么是专家混合模型 (MoE)？\n\n专家混合模型 (MoE) 是一种具有多个子网络的架构，这些子网络被称为“专家”，每个专家专注于不同的任务或数据方面。MoE 的一个优势是，它能够以比相同或更大模型更少的计算量对 AI 模型进行预训练，同时保持或提高质量。因此，如果我们的预算有限，使用 MoE 可以比稠密的、相似大小的传统模型获得更好的模型。在最近的成功案例中，Mixtral 8 x 7B 在许多评估数据集上超越了 LLaMA 2 70B。\n\n接下来，让我们研究 MoE 的架构。最近成功的 MoE 使用了变压器模型，因此我将重点关注变压器的流行 MoE 架构。MoE 主要有两个组件，如下所述。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Dia_c08PJnFeeIc9lxwtGQ.png)\n\n* **MoE 层**\n\nMoE 在变压器架构中用 MoE 层替代了前馈网络 (FFN) 层。每个 MoE 层有一些专家（例如，上图中的 4 个专家），每个专家由简单的 FFN 层组成。请注意，变压器中的其他组件，例如自注意力层，使用相同的权重。因此，MoE 的权重数量并不简单。例如，Mixtral 8 x 7B 的权重不是 8 x 7 = 56B，而是 47B，因为除了 MoE 层之外的其他层共享相同的权重。\n\n* **门控网络**\n\n门控网络或路由器是 MoE 中的一个关键组件。它接收输入标记并为每个标记选择最相关的专家。例如，在上面的插图中，路由器的左侧选择第二个专家来处理单词“more”标记。同时，路由器确定第一个专家来处理单词“Parameters”标记。通常，门控网络选择与给定标记相关的前 k 个专家，并将标记发送给选定的专家；例如，Mixtral 8 x 7B 选择前 2 个专家。\n\n我们如何选择前 k 个专家？我们使用 softmax 函数来计算专家的重要性概率，并保留前 k 个概率专家，如下所示。我提取了上述插图的门控部分。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*qX9H2KKtjntVuiE8yFstMQ.png)\n\n门控网络有其权重。我们将 softmax 函数应用于输入单词标记与门控网络权重之间的点积结果，然后得到专家与给定标记相关的概率。根据概率，我们可以选择前 k 个相关专家。具有这种类型门控网络的 MoE 被称为稀疏 MoE。\n\n这些是理解 MoE 如何作为嵌入模型工作的基本知识。为了进一步理解，我推荐阅读 [这篇博客](https://huggingface.co/blog/moe) \\[2]。现在，让我们深入探讨 MoE 实际上是如何作为嵌入模型工作的。\n\n## 2\\. MoE 如何作为嵌入模型工作？\n\n### 关于嵌入的快速回顾\n\n在深入本节主题之前，让我们快速回顾一下嵌入。最近，嵌入成为深度学习模型中输入数据的内部表示，它具有语义和浓缩的数据信息。我们通常提取神经网络的最后一个隐藏状态作为嵌入，如下所示。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kSHFTEejKiSI51taKZCO9A.png)\n\n我们通常使用基于编码器的模型来提取嵌入，因为与仅解码器模型相比，它们能够通过双向注意力捕捉语义。仅解码器模型通常使用因果注意力，只与之前的词元进行交互；因此，它们无法捕捉丰富的语义，如上下文信息，这一点是编码器-解码器模型所能实现的。\n\n### MoE如何作为嵌入模型工作？\n\n人们普遍认为解码器模型无法用于嵌入提取。然而，作者发现MoE中的路由权重为解码器嵌入提供了互补信息。每一层中的路由权重反映了对输入标记的推理选择，因此它包含了输入的语义信息，而隐藏状态的嵌入可能会丢失。在数学公式中，我们可以这样描述它：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*n6wGCMqAhjBAfLFV47ML1g.png)\n\n*g*是softmax函数，*H*表示隐藏状态。我们将所有MoE层的路由权重进行连接，以避免丢失模型的推理选择。\n\n为了充分利用路由权重和解码器嵌入，作者提出了一种称为MoE嵌入（MoEE）的方法，以形成更全面的嵌入表示。MoEE有两种类型。一种方法是基于连接的组合，具体如下。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*uVmcV-lM83XL7HoYbYjt7w.png)\n\n这种方法很简单，我们只需将路由权重和解码器嵌入进行连接。作者将这种方法称为MoEE(concat)。它可以保留每个路由权重捕获的独特信息，同时允许下游任务利用组合表示。\n\n另一种方法是加权求和集成。它对从路由权重和隐藏状态（HS）嵌入计算的相似性分数进行加权求和，表示为MoEE(sum)。该方法用于比较两个句子的任务，例如语义文本相似性。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kyJxWW9zdgRyNr2jmO4LlQ.png)\n\n𝛂是一个超参数，用于控制路由权重的贡献。在为每对计算相似性分数后，我们计算计算得出的相似性分数与真实相似性之间的等级相关性，例如斯皮尔曼等级相关性。\n\n在实际使用中，我认为MoEE(concat)易于使用。此外，作者利用PromptEOL技术\\[4]来增强MoEE。该技术提示以下模板，以限制LLMs在预测下一个标记的语义信息时的行为。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*S9BASj9JkQe-i4fqmbopWg.png)\n\n现在，这里是MTEB任务的性能表。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*7LxkEMR2DFlncypF6_T7Vw.png)\n\n带有PromptEOL的MoEE可以比监督和自监督方法表现更好。请注意，这个排行榜不是最新的，因此这个结果并不是SOTA。这种方法的价值在于我们可以在嵌入任务中获得不错的结果，并且可以在没有任何进一步训练的情况下使用。\n\n到目前为止，我们已经涵盖了MoEE的工作原理。在下一节中，我们将实现MoEE与BERTopic并对句子进行聚类。\n\n## 3\\. 实际实施：利用 MoEE 与 BERTopic\n\n在本节中，我们从预训练的 MoE LLM 中提取嵌入，并使用 20-news-group 数据集 \\[5] 与 [BERTopic](https://maartengr.github.io/BERTopic/index.html) 结合。供您参考，BERTopic 是一个便利的主题建模库，超越了传统的统计主题建模。它利用来自 Transformer 的嵌入进行主题聚类，因此我认为它适合用于检查能力。首先，我们来准备一个环境。\n\n### 环境设置\n\n我使用了一个带有 Python 3\\.10 的 conda 环境。我在 Ubuntu 20\\.04 上进行了实验，使用 cuda 12\\.4，16 GB VRAM。下载模型权重可能需要 32 GB RAM。\n\n```python\nconda create -n moee python=3.10 -y\nconda activate moee\n```\n\n接下来，我们需要通过 pip 安装以下库。\n\n```python\npip install transformers torch bitsandbytes bertopic accelerate\n```\n\nMoE 模型通常需要较高的 VRAM，因为我们需要提前将整个模型加载到 VRAM 中。因此，我们需要使用 bitsandbytes，这是一个量化包，以节省 VRAM 内存。\n\n我们需要克隆官方 GitHub 仓库。\n\n```python\ngit clone https://github.com/tianyi-lab/MoE-Embedding.git\n```\n\n所有准备工作都完成了。现在，让我们使用 MoEE 实现 BERTopic 的主题聚类。\n\n### 利用 MoEE 和 BERTopic\n\n现在，我们将使用 MoEE 作为 BERTopic 的嵌入模型并尝试主题聚类。原始代码库允许我们使用小型 MoE 模型，例如 Qwen\\-1\\.5\\-MoE\\-A2\\.7B 或 OLMoE\\-1B\\-7B。在这篇博客中，我将使用 OLMoE\\-1B\\-7B，它适合在 16 GB VRAM 上运行推理。首先，我们需要加载 OLMoE\\-1B\\-7B。\n\n```python\nkwargs = {\n        \"base_model\": 'allenai/OLMoE-1B-7B-0924',\n        \"normalized\": False,\n        \"torch_dtype\": torch.bfloat16,\n        \"mode\": \"embedding\",\n        \"pooling_method\": \"mean\",\n        \"attn_implementation\": \"sdpa\",\n        \"attn\": \"bbcc\",\n    }\n\nconfig = {\n    'embed_method': 'prompteol',\n    'emb_info': 'MoEE'\n    }\n\nembedding_model = MOEE(model_name_or_path='allenai/OLMoE-1B-7B-0924', **kwargs)\n```\n\n接下来，我们需要计算 20\\-news\\-group 数据集的嵌入，以传递给 BERTopic。（我稍后会附上完整代码。）\n\n```python\nfrom sklearn.datasets import fetch_20newsgroups\n\ndocs = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))['data']\n\ndataset = MyDataset(docs)\ndataloader = DataLoader(dataset=dataset, batch_size=8)\nembeddings = None\n\nfor batch in tqdm(dataloader):\n    with torch.no_grad():    \n        embedding = embedding_model.encode(batch, **config)\n      \n        if embeddings is None:\n            embeddings = embedding[0]\n        else:\n            embeddings = np.vstack((embeddings, embedding[0]))\n  \n    torch.cuda.empty_cache()\n```\n\n为了提前计算嵌入，我们使用 torch.utils.data.DataLoader 作为迭代器，并对每个批次的文档进行编码。请注意，我们必须将嵌入作为 np.asarray 类型传递给 BERTopic。\n\n当您想使用自己的 MoE 模型时，必须实现从每个 MoE 层获取路由权重。对于隐藏状态嵌入，我们可以利用 HuggingFace transformer 函数。我们只需在推理时传递 output\\_hidden\\_states\\=True 参数。\n\n现在，我们可以运行主题建模。\n\n```python\n## Step 2 - Reduce dimensionality\numap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')\n\n## Step 3 - Cluster reduced embeddings\nhdbscan_model = HDBSCAN(min_cluster_size=15, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n\n## Step 4 - Tokenize topics\nvectorizer_model = CountVectorizer(stop_words=\"english\")\n\n## Step 5 - Create topic representation\nctfidf_model = ClassTfidfTransformer()\n\n## Step 6 - (Optional) Fine-tune topic representations with \n## a `bertopic.representation` model\nrepresentation_model = KeyBERTInspired()\n\n## All steps together\ntopic_model = BERTopic(\n  embedding_model=embedding_model,          # Step 1 - Extract embeddings\n  umap_model=umap_model,                    # Step 2 - Reduce dimensionality\n  hdbscan_model=hdbscan_model,              # Step 3 - Cluster reduced embeddings\n  vectorizer_model=vectorizer_model,        # Step 4 - Tokenize topics\n  ctfidf_model=ctfidf_model,                # Step 5 - Extract topic words\n  representation_model=representation_model # Step 6 - (Optional) Fine-tune topic representations\n)\n\n## topic modeling using BERTopic model\ntopics, probs = topic_model.fit_transform(docs, embeddings)\n```\n\n我们通过默认设置得到了 42 个主题；一些示例如下所示。尽管我随机选择了主题，但它能够很好地捕捉语义。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*VIaKHU-PSuTPzOUKDFbwOw.png)\n\n此外，这里是主题聚类可视化。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*KYAUOe2qEAv-ihq2S2dM0A.png)\n\n请查看主题聚类可视化中的红色圆圈。这个红色圆圈指的是主题 0，与计算机相关。更接近的主题也与机械词汇相关，例如图形、数字和打印机。\n\n这种方法向我们展示了我们可以在没有任何训练的情况下获得良好的嵌入。尽管在质量上仍有提升空间，以达到 SOTA\\-监督模型的水平，但本文的发现是进一步改善嵌入提取方法而不进行训练的良好步骤。\n\n这是我的完整代码。您需要将此文件放入 MoE\\-Embedding 目录的顶部。\n\n## 参考文献\n\n\\[1] Ziyue Li, Tianyi Zhou, [YOUR MIXTURE\\-OF\\-EXPERTS LLM IS SECRETLY AN EMBEDDING MODEL FOR FREE](https://arxiv.org/pdf/2410.10814) (2024\\), *Arxiv*\n\n\\[2] Omar S., et.al., [Mixture of Experts Explained](https://huggingface.co/blog/moe) (2023\\), Hugging Face\n\n\\[3] William Fedus, Barret Zoph., et.al., [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/pdf/2101.03961) (2021\\), *Arxiv*\n\n\\[4] Ting Jiang, et.al., [Scaling Sentence Embeddings with Large Language Models](https://arxiv.org/pdf/2307.16645) (2023\\), *Arxiv*\n\n\\[5] [20 News groups](http://qwone.com/~jason/20Newsgroups/)\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/using-llama-3-for-building-ai-agents-7e74f79d1ccc","frontmatter":{"title":"使用 Llama 3 构建 AI 代理","meta_title":"使用 Llama 3 构建 AI 代理","description":"使用 Llama 3 函数调用功能构建 AI 代理的综合指南。","date":"2024-11-10T03:51:17.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*EWGo-7t4Kl6l82rB2-ZK9Q.png","categories":["Programming","Generative AI","Chatbots"],"author":"Rifx.Online","tags":["Llama","Gradio","RAG","metadata","indexing"],"draft":false,"slug":"blog/using-llama-3-for-building-ai-agents-7e74f79d1ccc"},"content":"\n\n\n### 构建具有 Llama 3 函数调用能力的 AI 代理的综合指南\n\n\n\n### 引言\n\n想象一下你想买一些东西。你访问一个电子商务网站，使用搜索选项找到你想要的东西。也许你有多个物品要购买，因此这个过程并不是很高效。现在考虑这个场景：打开一个应用程序，用简单的英语描述你想要的东西，然后按下回车。你不必担心搜索和价格比较，因为应用程序会自动为你处理这些事情。很酷，对吧？这正是我们将在本教程中构建的内容。\n\n让我们先看一些例子。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ikbr1ozv37PIB2meVfCCfA.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*AZPn3_KCDRV0pAszd3vLmA.png)\n\n好的，让我们为这个应用程序注入活力。我们将使用Meta的Llama 3模型，具有函数调用能力。不过，这也可以使用3.1模型来实现。根据[Meta的公告](https://ai.meta.com/blog/meta-llama-3-1/)，3.1模型可以更有效地使用工具和函数。\n\n> 这些是多语言的，具有显著更长的上下文长度128K，最先进的工具使用能力，以及整体更强的推理能力。\n\n我将使用Groq Cloud，特别是他们的模型来撰写本文。这个应用程序的初始工作流程应由一个嵌入模型、一个检索器和两个主要工具组成，用于处理用户的购买兴趣和与成本相关的关注。总之，我们需要一些类似于下面图表中描述的内容。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*EZVySX3GD2O07fzEPwLcbQ.png)\n\n现在我们需要使用LLM编排框架。为此，我选择我一直以来最喜欢的[Haystack](https://haystack.deepset.ai/)。\n\n好的，我们得到了我们需要的东西。让我们跳入实际工作吧！\n\n### 加载和索引数据\n\n由于我们有一个 RAG 流水线，应该将构建文档索引服务作为第一步。对于这个演示，我将使用 Haystack 提供的内存向量数据库。请注意，我们的向量数据库中的每个文档包含：\n\n* 内容 — 我们用它来执行相似性搜索\n* Id — 唯一标识符\n* 价格 — 产品价格\n* URL — 产品 URL\n\n当我们的 RAG 流水线被调用时，内容字段用于向量搜索。所有其他字段作为元数据包含。保留这些元数据至关重要，因为它对前端呈现给用户是必不可少的。\n\n让我们看看如何实现这一点。\n\n```python\nfrom haystack import Pipeline, Document\nfrom haystack.document_stores.in_memory import InMemoryDocumentStore\nfrom haystack.components.writers import DocumentWriter\nfrom haystack.components.embedders import SentenceTransformersDocumentEmbedder\nfrom haystack.components.generators import OpenAIGenerator\nfrom haystack.utils import Secret\nfrom haystack.components.generators.chat import OpenAIChatGenerator\nfrom haystack.components.builders import PromptBuilder\nfrom haystack.components.embedders import SentenceTransformersTextEmbedder\nfrom haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\nfrom haystack.dataclasses import ChatMessage\nimport pandas as pd\n\n## Load product data from CSV\ndf = pd.read_csv(\"product_sample.csv\")\n\n## Initialize an in-memory document store\ndocument_store = InMemoryDocumentStore()\n\n## Convert the product data into Haystack Document objects\ndocuments = [\n    Document(\n        content=item.product_name, \n        meta={\n            \"id\": item.uniq_id, \n            \"price\": item.selling_price, \n            \"url\": item.product_url\n        }\n    ) for item in df.itertuples()\n]\n\n## Create a pipeline for indexing the documents\nindexing_pipeline = Pipeline()\n\n## Add a document embedder to the pipeline using Sentence Transformers model\nindexing_pipeline.add_component(\n    instance=SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\"), name=\"doc_embedder\"\n)\n\n## Add a document writer to the pipeline to store documents in the document store\nindexing_pipeline.add_component(instance=DocumentWriter(document_store=document_store), name=\"doc_writer\")\n\n## Connect the embedder's output to the writer's input\nindexing_pipeline.connect(\"doc_embedder.documents\", \"doc_writer.documents\")\n\n## Run the indexing pipeline to process and store the documents\nindexing_pipeline.run({\"doc_embedder\": {\"documents\": documents}})\n```\n很好，我们已完成 AI 代理应用程序的第一步。现在是时候构建产品识别工具了。为了更好地理解产品识别器的主要任务，让我们考虑下面的示例。\n\n> 用户查询：我想买一双露营靴、一台炭烤炉和一个 Google Pixel 9 的手机壳。让我们理解产品识别功能的理想工作流程。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kXGYjlMi4pQcqIKpmUZLRQ.png)\n\n首先，我们需要创建一个工具来分析用户查询并识别用户感兴趣的产品。我们可以使用下面的代码片段构建这样的工具。\n\n### 构建用户查询分析器\n\n\n```python\ntemplate = \"\"\"\nUnderstand the user query and list of products the user is interested in and return product names as list.\nYou should always return a Python list. Do not return any explanation.\n\nExamples:\nQuestion: I am interested in camping boots, charcoal and disposable rain jacket.\nAnswer: [\"camping_boots\",\"charcoal\",\"disposable_rain_jacket\"]\n\nQuestion: Need a laptop, wireless mouse, and noise-cancelling headphones for work.\nAnswer: [\"laptop\",\"wireless_mouse\",\"noise_cancelling_headphones\"]\n\nQuestion: {{ question }}\nAnswer:\n\"\"\"\n\nproduct_identifier = Pipeline()\n\nproduct_identifier.add_component(\"prompt_builder\", PromptBuilder(template=template))\nproduct_identifier.add_component(\"llm\", generator())\n\nproduct_identifier.connect(\"prompt_builder\", \"llm\")\n```\n好的，现在我们已经完成了第一个函数的一半，现在是时候通过添加RAG管道来完成这个函数。 \n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*JyxINdc8Wz-qAg_PCAkLbA.png)\n\n### 创建 RAG 管道\n\n\n```python\ntemplate = \"\"\"\nReturn product name, price, and url as a python dictionary. \nYou should always return a Python dictionary with keys price, name and url for single product.\nYou should always return a Python list of dictionaries with keys price, name and url for multiple products.\nDo not return any explanation.\n\nLegitimate Response Schema:\n{\"price\": \"float\", \"name\": \"string\", \"url\": \"string\"}\nLegitimate Response Schema for multiple products:\n[{\"price\": \"float\", \"name\": \"string\", \"url\": \"string\"},{\"price\": \"float\", \"name\": \"string\", \"url\": \"string\"}]\n\nContext:\n{% for document in documents %}\n    product_price: {{ document.meta['price'] }}\n    product_url: {{ document.meta['url'] }}\n    product_id: {{ document.meta['id'] }}\n    product_name: {{ document.content }}\n{% endfor %}\nQuestion: {{ question }}\nAnswer:\n\"\"\"\n\nrag_pipe = Pipeline()\nrag_pipe.add_component(\"embedder\", SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\"))\nrag_pipe.add_component(\"retriever\", InMemoryEmbeddingRetriever(document_store=document_store, top_k=5))\nrag_pipe.add_component(\"prompt_builder\", PromptBuilder(template=template))\nrag_pipe.add_component(\"llm\", generator())\n\nrag_pipe.connect(\"embedder.embedding\", \"retriever.query_embedding\")\nrag_pipe.connect(\"retriever\", \"prompt_builder.documents\")\nrag_pipe.connect(\"prompt_builder\", \"llm\")\n```\n在这个阶段，我们已经完成了 RAG 和查询分析器管道。现在是时候将其转换为工具了。为此，我们可以使用常规的函数声明，如下所示。为代理创建工具就像创建一个 Python 函数。如果你有这样的问题\n\n\n> 代理如何调用这个函数？\n\n解决方案很简单：通过利用特定模型的工具架构，我们计划在未来的步骤中纳入。目前，是时候创建一个包装函数，既使用查询分析器又使用 RAG 管道。\n\n让我们明确这个函数的目标。\n\n**目标 1：** 确定用户感兴趣的所有产品，并将它们作为列表返回。 **目标 2：** 对于每个识别的产品，从数据库中检索最多五个产品及其元数据。\n\n### 完成产品识别功能\n\n\n```python\ndef product_identifier_func(query: str):\n    \"\"\"\n    根据给定的查询识别产品并检索每个识别产品的相关细节。\n\n    参数：\n    query (str): 用于识别产品的查询字符串。\n\n    返回：\n    dict: 一个字典，键为产品名称，值为每个产品的详细信息。如果未找到产品，则返回“No product found”。\n    \"\"\"\n    product_understanding = product_identifier.run({\"prompt_builder\": {\"question\": query}})\n\n    try:\n        product_list = literal_eval(product_understanding[\"llm\"][\"replies\"][0])\n    except:\n        return \"No product found\"\n\n    results = {}\n\n    for product in product_list:\n        response = rag_pipe.run({\"embedder\": {\"text\": product}, \"prompt_builder\": {\"question\": product}})\n        try:\n            results[product] = literal_eval(response[\"llm\"][\"replies\"][0])\n        except:\n            results[product] = {}\n    \n    return results\n```\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*HWRTdWvvcw2MZP4uoaQdeQ.png)\n\n至此，我们完成了代理的第一个工具。让我们看看它是否按预期工作。\n\n\n```python\nquery = \"I want crossbow and woodstock puzzle\"\n#execute function\nproduct_identifier_func(query)\n\n## {'crossbow': {'name': 'DB Longboards CoreFlex Crossbow 41\" Bamboo Fiberglass '\n##                        'Longboard Complete',\n##                'price': 237.68,\n##                'url': 'https://www.amazon.com/DB-Longboards-CoreFlex-Fiberglass-Longboard/dp/B07KMVJJK7'},\n##  'woodstock_puzzle': {'name': 'Woodstock- Collage 500 pc Puzzle',\n##                       'price': 17.49,\n##                       'url': 'https://www.amazon.com/Woodstock-Collage-500-pc-Puzzle/dp/B07MX21WWX'}}\n```\n它工作了！！然而，值得注意的是返回输出的结构。您可以在下面看到一般的结构。\n\n\n```python\n{\n    \"product_key\": {\n        \"name\": \"string\",\n        \"price\": \"float\",\n        \"url\": \"string\"\n    }\n}\n```\n这正是我们在RAG管道中建议模型生成的内容。下一步，让我们构建一个名为`find_budget_friendly_option`的可选工具。\n\n\n```python\ndef find_budget_friendly_option(selected_product_details):\n    \"\"\"\n    为每个产品类别找到最具预算友好的选项。\n\n    参数：\n    selected_product_details (dict): 一个字典，键为产品类别，值为产品详细信息的列表。每个产品详细信息应为包含“price”键的字典。\n\n    返回：\n    dict: 一个字典，键为产品类别，值为每个类别最具预算友好的产品详细信息。\n    \"\"\"\n    budget_friendly_options = {}\n    \n    for category, items in selected_product_details.items():\n        if isinstance(items, list):\n            lowest_price_item = min(items, key=lambda x: x['price'])\n        else:\n            lowest_price_item = items\n        \n        budget_friendly_options[category] = lowest_price_item\n    \n    return budget_friendly_options\n```\n好的，让我们专注于这个应用程序最关键的方面，即使代理根据需要使用这些功能。正如我们之前所讨论的，这可以通过模型特定的工具架构来实现。因此，我们需要找到特定于所选模型的工具架构。幸运的是，它在模型卡中提到 [这里](https://huggingface.co/Groq/Llama-3-Groq-70B-Tool-Use)。我们需要调整它以适应我们的用例。\n\n### 完成聊天模板\n\n\n```python\nchat_template = '''<|start_header_id|>system<|end_header_id|>\n\nYou are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n<tool_call>\n{\"name\": <function-name>,\"arguments\": <args-dict>}\n</tool_call>\n\nHere are the available tools:\n<tools>\n    {\n        \"name\": \"product_identifier_func\",\n        \"description\": \"To understand user interested products and its details\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"query\": {\n                    \"type\": \"string\",\n                    \"description\": \"The query to use in the search. Infer this from the user's message. It should be a question or a statement\"\n                }\n            },\n            \"required\": [\"query\"]\n        }\n    },\n    {\n        \"name\": \"find_budget_friendly_option\",\n        \"description\": \"Get the most cost-friendly option. If selected_product_details has morethan one key this should return most cost-friendly options\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"selected_product_details\": {\n                    \"type\": \"dict\",\n                    \"description\": \"Input data is a dictionary where each key is a category name, and its value is either a single dictionary with 'price', 'name', and 'url' keys or a list of such dictionaries; example: {'category1': [{'price': 10.5, 'name': 'item1', 'url': 'http://example.com/item1'}, {'price': 8.99, 'name': 'item2', 'url': 'http://example.com/item2'}], 'category2': {'price': 15.0, 'name': 'item3', 'url': 'http://example.com/item3'}}\"\n                }\n            },\n            \"required\": [\"selected_product_details\"]\n        }\n    }\n</tools><|eot_id|><|start_header_id|>user<|end_header_id|>\n\nI need to buy a crossbow<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n<tool_call>\n{\"id\":\"call_deok\",\"name\":\"product_identifier_func\",\"arguments\":{\"query\":\"I need to buy a crossbow\"}}\n</tool_call><|eot_id|><|start_header_id|>tool<|end_header_id|>\n\n<tool_response>\n{\"id\":\"call_deok\",\"result\":{'crossbow': {'price': 237.68,'name': 'crossbow','url': 'https://www.amazon.com/crossbow/dp/B07KMVJJK7'}}}\n</tool_response><|eot_id|><|start_header_id|>assistant<|end_header_id|>\n'''\n现在只剩下几个步骤。在做任何事情之前，让我们测试一下我们的代理。\n\n\n```python\n### 测试代理\nmessages = [\n    ChatMessage.from_system(\n        chat_template\n    ),\n    ChatMessage.from_user(\"I need to buy a crossbow for my child and Pokémon for myself.\"),\n]\n\nchat_generator = get_chat_generator()\nresponse = chat_generator.run(messages=messages)\npprint(response)\n\n### response\n{'replies': [ChatMessage(content='<tool_call>\\n'\n                                 '{\"id\": 0, \"name\": \"product_identifier_func\", '\n                                 '\"arguments\": {\"query\": \"I need to buy a '\n                                 'crossbow for my child\"}}\\n'\n                                 '</tool_call>\\n'\n                                 '<tool_call>\\n'\n                                 '{\"id\": 1, \"name\": \"product_identifier_func\", '\n                                 '\"arguments\": {\"query\": \"I need to buy a '\n                                 'Pokemon for myself\"}}\\n'\n                                 '</tool_call>',\n                         role=<ChatRole.ASSISTANT: 'assistant'>,\n                         name=None,\n                         meta={'finish_reason': 'stop',\n                               'index': 0,\n                               'model': 'llama3-groq-70b-8192-tool-use-preview',\n                               'usage': {'completion_time': 0.217823967,\n                                         'completion_tokens': 70,\n                                         'prompt_time': 0.041348261,\n                                         'prompt_tokens': 561,\n                                         'total_time': 0.259172228,\n                                         'total_tokens': 631}})]}\n```\n到此为止，我们已经完成了大约90%的工作。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*nYVXcgpm3RZ3g5h5d4UK_A.png)\n\n在上面的响应中，您可能注意到XML标签`<tool_call>`封闭了工具调用。因此，我们需要开发一种机制来提取tool_call对象。\n\n\n```python\ndef extract_tool_calls(tool_calls_str):\n    json_objects = re.findall(r'<tool_call>(.*?)</tool_call>', tool_calls_str, re.DOTALL)\n    \n    result_list = [json.loads(obj) for obj in json_objects]\n    \n    return result_list\n\navailable_functions = {\n    \"product_identifier_func\": product_identifier_func, \n    \"find_budget_friendly_option\": find_budget_friendly_option\n    }\n```\n完成这一步后，我们可以直接访问代理的响应，当它调用一个工具时。现在唯一待做的就是获取工具调用对象并相应地执行函数。让我们完成那部分。\n\n\n```python\nmessages.append(ChatMessage.from_user(message))\nresponse = chat_generator.run(messages=messages)\n\nif response and \"<tool_call>\" in response[\"replies\"][0].content:\n    function_calls = extract_tool_calls(response[\"replies\"][0].content)\n    for function_call in function_calls:\n        # Parse function calling information\n        function_name = function_call[\"name\"]\n        function_args = function_call[\"arguments\"]\n\n        # Find the corresponding function and call it with the given arguments\n        function_to_call = available_functions[function_name]\n        function_response = function_to_call(**function_args)\n\n        # Append function response to the messages list using `ChatMessage.from_function`\n        messages.append(ChatMessage.from_function(content=json.dumps(function_response), name=function_name))\n        response = chat_generator.run(messages=messages)\n```\n现在是时候将每个组件组合在一起，构建一个合适的聊天应用程序。我将使用Gradio来实现这个目的。\n\n\n```python\nimport gradio as gr\n\nmessages = [ChatMessage.from_system(chat_template)]\nchat_generator = get_chat_generator()\n\ndef chatbot_with_fc(message, messages):\n    messages.append(ChatMessage.from_user(message))\n    response = chat_generator.run(messages=messages)\n\n    while True:\n        if response and \"<tool_call>\" in response[\"replies\"][0].content:\n            function_calls = extract_tool_calls(response[\"replies\"][0].content)\n            for function_call in function_calls:\n                # Parse function calling information\n                function_name = function_call[\"name\"]\n                function_args = function_call[\"arguments\"]\n\n                # Find the corresponding function and call it with the given arguments\n                function_to_call = available_functions[function_name]\n                function_response = function_to_call(**function_args)\n\n                # Append function response to the messages list using `ChatMessage.from_function`\n                messages.append(ChatMessage.from_function(content=json.dumps(function_response), name=function_name))\n                response = chat_generator.run(messages=messages)\n\n        # Regular Conversation\n        else:\n            messages.append(response[\"replies\"][0])\n            break\n    return response[\"replies\"][0].content\n\n\ndef chatbot_interface(user_input, state):\n    response_content = chatbot_with_fc(user_input, state)\n    return response_content, state\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# AI 购买助手\")\n    gr.Markdown(\"问我关于您想购买的产品！\")\n    \n    state = gr.State(value=messages)\n    \n    with gr.Row():\n        user_input = gr.Textbox(label=\"您的消息：\")\n        response_output = gr.Markdown(label=\"回复：\")\n    \n    user_input.submit(chatbot_interface, [user_input, state], [response_output, state])\n    gr.Button(\"发送\").click(chatbot_interface, [user_input, state], [response_output, state])\n\n\ndemo.launch()\n```\n就这样！我们构建了基于Llama 3的AI代理🤖，具备函数调用能力。您可以从这个[GitHub仓库](https://github.com/Ransaka/ai-agents-with-llama3)访问完整代码。感谢您的阅读。\n\n通过[这个](https://www.kaggle.com/datasets/promptcloud/amazon-product-dataset-2020)Kaggle链接（在CC0：公共领域下）可以访问本文使用的数据集。\n\n### 结论\n\n在构建基于AI代理的系统时，考虑完成任务所需的时间和每个任务使用的API调用（令牌）数量非常重要。一个主要的挑战是减少系统中的幻觉，这是一个活跃的研究领域。因此，构建LLM和代理系统没有固定的规则。必须耐心而有策略地工作，以确保AI代理，即LLM，正常运行。\n\n*除非另有说明，所有图片均由作者提供。*\n\n### 参考：\n\n[https://docs.together.ai/docs/llama\\-3\\-function\\-calling](https://docs.together.ai/docs/llama-3-function-calling)\n\n"},{"lang":"zh","group":"blog","slug":"blog/visualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-with-ragas-fc2486308557","frontmatter":{"title":"可视化你的 RAG 数据——使用 Ragas 评估你的检索增强生成系统","meta_title":"可视化你的 RAG 数据——使用 Ragas 评估你的检索增强生成系统","description":"如何使用 UMAP 降维对嵌入进行处理以显示多个评估问题及其与源文档的关系……","date":"2024-11-04T12:35:56.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*peWTe1A-MqeROT_Jdof_Cw.gif","categories":["Natural Language Processing","Generative AI","Data Science"],"author":"Rifx.Online","tags":["RAG","UMAP","embeddings","evaluation","visualization"],"draft":false,"slug":"blog/visualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-with-ragas-fc2486308557"},"content":"\n\n\n### 如何使用 UMAP 降维将嵌入可视化以展示多个评估问题及其与源文档的关系，结合 Ragas、OpenAI、Langchain 和 ChromaDB\n\n检索增强生成（RAG）在 LLM 的工作流程中增加了一个检索步骤，使其能够在回答问题和查询时，从私人文档等额外来源查询相关数据 \\[1]。该工作流程不需要对额外文档进行昂贵的训练或微调。文档被拆分成片段，然后进行索引，通常使用紧凑的 ML 生成的向量表示（嵌入）。内容相似的片段在这个嵌入空间中会彼此靠近。\n\nRAG 应用将用户提供的问题投影到嵌入空间，以根据与问题的距离检索相关文档片段。LLM 可以使用检索到的信息来回答查询，并通过呈现片段作为参考来证明其结论。\n\n\n\n评估 RAG 应用是具有挑战性的 \\[2]。存在不同的方法：一方面，有些方法要求开发者提供答案作为真实值；另一方面，答案（和问题）也可以由另一个 LLM 生成。最大的开源 LLM 支持回答系统之一是 Ragas \\[4](检索增强生成评估)，它提供\n\n* 基于文档生成测试数据的方法，以及\n* 基于不同指标逐步和端到端评估检索和生成步骤的评估。\n\n在本文中，您将学习\n\n* 如何简要构建一个 Formula One 的 RAG 系统（有关详细描述，请参阅之前的文章 [可视化您的 RAG 数据 — 检索增强生成的 EDA](https://readmedium.com/visualize-your-rag-data-eda-for-retrieval-augmented-generation-0701ee98768f)）\n* 生成问题和答案\n* 使用 [Ragas](https://github.com/explodinggradients/ragas) 评估 RAG 系统\n* 最重要的是，如何使用 [Renumics Spotlight](https://github.com/Renumics/spotlight) 可视化结果并解读结果。\n\n代码可在 Github 上获取。\n\n## 准备你的环境\n\n启动一个笔记本并安装所需的 python 包\n\n```python\n!pip install langchain langchain-openai chromadb renumics-spotlight\n%env OPENAI_API_KEY=<your-api-key>\n```\n本教程使用以下 python 包：\n\n* [**Langchain**](https://github.com/langchain-ai/langchain): 一个集成语言模型和 RAG 组件的框架，使设置过程更加顺畅。\n* [**Renumics\\-Spotlight**](https://github.com/Renumics/spotlight): 一个可视化工具，用于交互式探索非结构化的机器学习数据集。\n* [**Ragas**](https://github.com/explodinggradients/ragas): 一个帮助你评估 RAG 管道的框架\n\n*免责声明：本文作者也是 Spotlight 的开发者之一。*\n\n## 为数据集准备文档和嵌入\n\n您可以使用自己的 RAG 应用程序，跳到下一部分了解如何评估、提取和可视化。\n\n或者您可以使用来自[上一篇文章](https://readmedium.com/visualize-your-rag-data-eda-for-retrieval-augmented-generation-0701ee98768f)的 RAG 应用程序，配合[我们准备的所有维基百科 Formula One 文章的数据集](https://spotlightpublic.blob.core.windows.net/docs-data/rag_demo/docs.zip)。您还可以将自己的文档插入到“docs/”子文件夹中。\n\n> 此数据集基于来自[维基百科](https://www.wikipedia.org/)的文章，并根据知识共享署名-相同方式共享许可协议进行许可。原始文章及作者列表可以在相应的维基百科页面中找到。\n\n现在您可以使用 Langchain 的 `DirectoryLoader` 从 docs 子目录加载所有文件，并使用 `RecursiveCharacterTextSpliter` 将文档拆分为片段。通过 `OpenAIEmbeddings`，您可以创建嵌入并将其存储在 `ChromaDB` 中作为向量存储。对于 Chain 本身，您可以使用 LangChains 的 `ChatOpenAI` 和 `ChatPromptTemplate`。\n\n本文的[链接代码](https://github.com/Renumics/rag-demo/blob/main/notebooks/visualize_rag_tutorial_qs.ipynb)包含所有必要步骤，您可以在[上一篇文章](https://readmedium.com/visualize-your-rag-data-eda-for-retrieval-augmented-generation-0701ee98768f)中找到上述所有步骤的详细描述。\n\n一个重要的点是，您应该使用哈希函数为 `ChromaDB` 中的片段创建 ID。这允许在仅拥有文档及其内容和元数据的情况下找到数据库中的嵌入。这使得可以跳过已经存在于数据库中的文档。\n\n```python\nimport hashlib\nimport json\nfrom langchain_core.documents import Document\n\ndef stable_hash_meta(doc: Document) -> str:\n    \"\"\"\n    Stable hash document based on its metadata.\n    \"\"\"\n    return hashlib.sha1(json.dumps(doc.metadata, sort_keys=True).encode()).hexdigest()\n\n...\nsplits = text_splitter.split_documents(docs)\nsplits_ids = [\n    {\"doc\": split, \"id\": stable_hash_meta(split.metadata)} for split in splits\n]\n\nexisting_ids = docs_vectorstore.get()[\"ids\"]\nnew_splits_ids = [split for split in splits_ids if split[\"id\"] not in existing_ids]\n\ndocs_vectorstore.add_documents(\n    documents=[split[\"doc\"] for split in new_splits_ids],\n    ids=[split[\"id\"] for split in new_splits_ids],\n)\ndocs_vectorstore.persist()\n```\n\n## 评估问题\n\n对于像一级方程式这样的常见主题，可以直接使用 ChatGPT 生成一般性问题。本文使用了四种问题生成方法：\n\n* **GPT4**: 使用 ChatGPT 4 生成了 30 个问题，提示为“写 30 个关于一级方程式的问题”\n– 随机示例：“哪个一级方程式车队以其跃马标志而闻名？”\n* **GPT3\\.5:** 使用 ChatGPT 3\\.5 生成了另外 199 个问题，提示为“写 100 个关于一级方程式的问题”，并重复“谢谢，再写 100 个吧”\n– 示例：“哪位车手在 1950 年赢得了首届一级方程式世界锦标赛？”\n* **Ragas\\_GPT4**: 使用 Ragas 生成了 113 个问题。Ragas 再次利用文档及其自身的嵌入模型构建一个向量数据库，然后用 GPT4 生成问题。\n– 示例：“你能告诉我更多关于乔丹 198 一级方程式赛车在 1998 年世界锦标赛中的表现吗？”\n* **Rags\\_GPT3\\.5**: 使用 Ragas 生成了 226 个额外问题——这里我们使用 GPT3\\.5\n– 示例：“在 2014 年比利时大奖赛上发生了什么事件导致汉密尔顿退赛？”\n\n```python\nfrom ragas.testset import TestsetGenerator\n\ngenerator = TestsetGenerator.from_default(\n    openai_generator_llm=\"gpt-3.5-turbo-16k\", \n    openai_filter_llm=\"gpt-3.5-turbo-16k\"\n)\n\ntestset_ragas_gpt35 = generator.generate(docs, 100)\n```\n问题和答案没有经过审核或修改。所有问题都合并在一个单一的数据框中，包含 `id`、`question`、`ground_truth`、`question_by` 和 `answer` 列。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*R_74K0-_SJXyTxq6ovAcWg.png)\n\n接下来，问题将被提出给 RAG 系统。对于超过 500 个问题，这可能需要一些时间并产生费用。如果逐行询问问题，可以暂停并继续该过程，或者在崩溃后恢复，而不会丢失到目前为止的结果：\n\n```python\nfor i, row in df_questions_answers.iterrows():\n    if row[\"answer\"] is None or pd.isnull(row[\"answer\"]):\n        response = rag_chain.invoke(row[\"question\"])\n\n        df_questions_answers.loc[df_questions_answers.index[i], \"answer\"] = response[\n            \"answer\"\n        ]\n        df_questions_answers.loc[df_questions_answers.index[i], \"source_documents\"] = [\n            stable_hash_meta(source_document.metadata)\n            for source_document in response[\"source_documents\"]\n        ]\n\n```\n不仅存储了答案，还存储了检索到的文档片段的源 ID 及其文本内容作为上下文：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*umlKv7Qf9SSLzRslT2r0Qw.png)\n\n此外，还生成并存储了所有问题的嵌入，并将其存储在数据框中。这使得可以将它们与文档一起可视化。\n\n## 使用 Ragas 进行评估\n\n[Ragas](https://github.com/explodinggradients/ragas) 提供了评估您的 RAG 流水线中每个组件的指标，以及整体性能的端到端指标：\n\n1. **上下文精确度：** 使用 `question` 和检索到的 `contexts` 来测量信号与噪声的比率。\n2. **上下文相关性：** 测量检索到的上下文与问题的相关性，使用 `question` 和 `contexts` 计算。\n3. **上下文召回率：** 基于 `ground truth` 和 `contexts` 检查是否检索到所有与答案相关的信息。\n4. **忠实度：** 利用 `contexts` 和 `answer` 来衡量生成答案的事实准确性。\n5. **答案相关性：** 使用 `question` 和 `answer` 计算，评估生成的答案与问题的相关性（不考虑事实性）。\n6. **答案语义相似度：** 使用 `ground truth` 和 `answer` 进行评估，以判断生成答案与正确答案之间的语义相似性。\n7. **答案正确性：** 依赖于 `ground truth` 和 `answer` 来衡量生成答案的准确性和与正确答案的一致性。\n8. **方面评估：** 涉及分析 `answer` 以根据预定义或自定义方面（如正确性或有害性）评估提交结果。\n\n目前，我们专注于答案正确性的端到端指标。数据框中的列名和内容已复制并调整，以符合 Ragas API 的命名和格式要求：\n\n```python\n## prepare the dataframe for evaluation\ndf_qa_eval = df_questions_answers.copy()\n\n\n## adapt the ground truth to the ragas naming and format\ndf_qa_eval.rename(columns={\"ground_truth\": \"ground_truths\"}, inplace=True)\ndf_qa_eval[\"ground_truths\"] = [\n    [gt] if not isinstance(gt, list) else gt for gt in df_qa_eval[\"ground_truths\"]\n]\n```\n这可能需要一些时间，甚至比仅查询您的 RAG 系统花费更多的金钱。让我们逐行应用评估，以便在崩溃后能够恢复而不丢失到目前为止的结果：\n\n```python\n## evaluate the answer correctness if not already done\nfields = [\"question\", \"answer\", \"contexts\", \"ground_truths\"]\nfor i, row in df_qa_eval.iterrows():\n    if row[\"answer_correctness\"] is None or pd.isnull(row[\"answer_correctness\"]):\n        evaluation_result = evaluate(\n            Dataset.from_pandas(df_qa_eval.iloc[i : i + 1][fields]),\n            [answer_correctness],\n        )\n        df_qa_eval.loc[i, \"answer_correctness\"] = evaluation_result[\n            \"answer_correctness\"\n        ]\n\n```\n之后，您可以将结果存储在 `df_questions_answer` 数据框中：\n\n```python\ndf_questions_answers[\"answer_correctness\"] = df_qa_eval[\"answer_correctness\"]\n```\n\n## 准备可视化\n\n为了在可视化中包含文档片段，我们添加了从文档到使用该文档作为来源的问题的引用。此外，引用文档的问题数量也被存储：\n\n```python\n## Explode 'source_documents' so each document ID is in its own row alongside the question ID\ndf_questions_exploded = df_qa_eval.explode(\"source_documents\")\n\n## Group by exploded 'source_documents' (document IDs) and aggregate\nagg = (\n    df_questions_exploded.groupby(\"source_documents\")\n    .agg(\n        num_questions=(\"id\", \"count\"),  # Count of questions referencing the document\n        question_ids=(\n            \"id\",\n            lambda x: list(x),\n        ),  # List of question IDs referencing the document\n    )\n    .reset_index()\n    .rename(columns={\"source_documents\": \"id\"})\n)\n\n## Merge the aggregated information back into df_documents\ndf_documents_agg = pd.merge(df_docs, agg, on=\"id\", how=\"left\")\n\n## Use apply to replace NaN values with empty lists for 'question_ids'\ndf_documents_agg[\"question_ids\"] = df_documents_agg[\"question_ids\"].apply(\n    lambda x: x if isinstance(x, list) else []\n)\n## Replace NaN values in 'num_questions' with 0\ndf_documents_agg[\"num_questions\"] = df_documents_agg[\"num_questions\"].fillna(0)\n```\n现在将问题的数据框与文档的数据框连接起来\n\n```python\ndf = pd.concat([df_qa_eval, df_documents_agg], axis=0)\n```\n此外，让我们准备一些不同的 UMAP \\[3] 映射。您可以稍后在 Spotlight GUI 中做类似的事情，但提前做好可以节省时间。\n\n* umap\\_all: 对所有文档和问题嵌入应用 fit 和 transform 的 UMAP\n* umap\\_questions: 仅对问题嵌入应用 fit，并对两者应用 transform 的 UMAP\n* umap\\_docs: 仅对文档嵌入应用 fit，并对两者应用 transform 的 UMAP\n\n我们像这样准备每个 UMAP 转换：\n\n```python\numap = UMAP(n_neighbors=20, min_dist=0.15, metric=\"cosine\", random_state=42).fit\numap_all = umap.transform(df[\"embedding\"].values.tolist())\ndf[\"umap\"] = umap_all.tolist()\n\n```\n每个文档片段的另一个有趣指标是其嵌入与最近问题的嵌入之间的距离：\n\n```python\nquestion_embeddings = np.array(df[df[\"question\"].notna()][\"embedding\"].tolist())\ndf[\"nearest_question_dist\"] = [  # brute force, could be optimized using ChromaDB\n    np.min([np.linalg.norm(np.array(doc_emb) - question_embeddings)])\n    for doc_emb in df[\"embedding\"].values\n]\n```\n这个指标可以帮助找到未被问题引用的文档。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*YTRUXZmd0iX8kyPIdUUnlg.png)\n\n## 可视化结果\n\n如果您跳过了之前的步骤，您可以下载数据框并使用以下代码加载它：\n\n```python\nimport pandas as pd\ndf = pd.read_parquet(\"df_f1_rag_docs_and_questions.parquet\")\n```\n然后启动 [Renumics Spotlight](https://github.com/Renumics/spotlight) 以可视化它：\n\n```python\nfrom renumics import spotlight\n\nspotlight.show(df)\nspotlight.show(\n    df,\n    layout=\"/home/markus/Downloads/layout_rag_1.json\",\n    dtype={x: Embedding for x in df.keys() if \"umap\" in x},\n)\n```\n这将打开一个新的浏览器窗口：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*IMbva0pP8RAVhoY4dVbjLg.png)\n\n在左上角，您可以看到一个**所有问题和所有文档**片段的表格。您可以使用“可见列”按钮来控制表格中显示哪些数据框列。直接创建一个选择仅问题的过滤器是很有用的，以便能够在可视化中打开和关闭问题：选择所有问题，然后使用“从选定行创建过滤器”按钮创建过滤器。\n\n在表格的右侧，`answer correctness` **作为一个指标**显示在所有问题中。下面有两个**直方图**；左侧显示了根据不同问题生成方法划分的`answer correctness`的分布。右侧显示了问题生成方法的分布。在这里，如果需要，建议使用过滤按钮为问题创建过滤器，以仅显示选定的行（问题）。\n\n右侧有**两个相似性图**。第一个使用`umap_questions`列，基于仅对问题应用的转换显示问题和文档。这对于独立于相关文档查看问题的分布很有帮助，因为这种方法允许分析师识别问题本身的模式或簇。\n\n第二个相似性图基于仅对文档应用的转换（`umap_docs`）显示问题和文档。它对于在其相关文档的上下文中查看问题很有用。一个同时对问题和文档进行转换的相似性图在问题数量较多时被证明不太有用，因为更多或更少的问题会聚集在一起并倾向于与文档分开。因此，这种表示在这里被省略。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*1wZrAj60hiw1T3RVnCuBtA.png)\n\n### 文档嵌入相似性图：观察\n\n在相似性图 `umap_docs` 中，您可以识别出文档嵌入空间中没有邻近问题的区域。当选择 `nearest_question_dist` 进行着色时，这一点更加明显。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*cMGNPnnBa9Bn7BJ05SzxBw.png)\n\n可以识别出一些簇，包括仅包含标题或逐页包含仅数字的表格数据的片段，这些在拆分过程中其意义丧失。此外，许多不包含相关信息的维基百科特定文本添加，例如指向其他语言的链接或编辑注释，形成了没有邻近问题的簇。\n\n使用维基百科 API 删除维基百科相关文本形式的噪声非常简单。这可能并不是特别必要，因为它主要占用一些空间——预计 RAG 结果不会因此特别恶化。然而，包含在大表格中的数据很难被 RAG 系统捕获，使用先进的预处理方法进行表格提取并将其连接到 RAG 系统可能是有益的。\n\n您可以在 `umap_docs` 相似性图中观察到的另一个点是来自不同来源的问题的分布。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*IH7z3J4yUmU0C_SruxnDkg.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*K4bADgDmSAr5t4t4r9VImQ.png)\n\n由 ChatGPT（GPT-3.5、GPT-4）直接生成的问题位于中心的一个更为封闭的区域，而基于文档生成的 ragas 生成的问题覆盖了更大的区域。\n\n### 答案正确性直方图\n\n直方图可以作为了解数据全球统计的起点。总体而言，在所有问题中，`答案正确性`为0\\.45。对于没有使用ragas创建的问题，该值为0\\.36，而使用ragas的问题则为0\\.52。预计系统在生成使用ragas的问题时表现会更好，因为这些问题是基于可用数据生成的，而ChatGPT直接生成的问题可能来自于ChatGPT训练时使用的所有数据。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GsLBsg7uwTrw-AzvO4BHmw.png)\n\n对一些问题/答案和真实情况进行快速随机手动审核显示，在`答案正确性`为0\\.3–0\\.4的区间，大多数问题仍然根据真实情况得到了正确回答。在0\\.2–0\\.3的区间，存在许多错误答案。在0\\.1–0\\.2的区间，大多数答案都是错误的。值得注意的是，这个范围内几乎所有的问题都来自GPT\\-3\\.5。尽管在这个区间内生成的两个问题使用的是GPT\\-4，但它们仍然得到了正确的回答，尽管其`答案正确性`低于0\\.2。\n\n### 问题嵌入相似性图：观察\n\n问题嵌入相似性图可以通过检查可能导致类似问题的相似问题集群，帮助深入挖掘 `答案正确性`。\n\n* **集群“驱动程序/过程/汽车的术语”：** 平均 `答案正确性` 0\\.23：答案通常不够精确。例如，底盘调校与底盘弯曲或刹车调校与刹车偏差调整。是否适合用这些类型的问题来评估系统是值得怀疑的，因为判断答案似乎非常困难。\n* **集群“燃料策略的术语”：** 平均 `答案正确性` 0\\.44，类似于全球 `答案正确性`。\n* **集群“赛道名称”：** 平均 `答案正确性` 0\\.49，类似于全球 `答案正确性`。\n* **集群“谁保持了…的记录”：** 平均 `答案正确性` 0\\.44，类似于全球 `答案正确性`。\n* **集群“赢得…锦标赛”：** 平均 `答案正确性` 0\\.26 — 看起来很具挑战性。带有许多条件的问题，例如：“谁是唯一一位凭借英国赛车执照、为意大利车队驾驶美国引擎赢得一级方程式世界锦标赛的车手。” 扩展的RAG方法如多查询可能有助于改善这一点。\n* **集群“谁是唯一一位赢得…的车手，驾驶编号为\\<number\\>的汽车”：** 平均 `答案正确性` 0\\.23 — 看起来GPT-3\\.5在这里懒惰，重复了相同的问题，只是换了不同的数字，尽管大多数真实答案都是错误的！\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Yc03cpSEFlJoZSBPIpMkiQ.png)\n\n## 结论\n\n总之，利用基于 UMAP 的可视化提供了一种有趣的方法，可以深入分析全球指标之外的内容。文档嵌入相似性地图提供了一个良好的概述，展示了相似文档的聚类及其与评估问题的关系。问题相似性地图揭示了模式，使得可以结合质量指标对问题进行区分和分析，从而生成洞察。请参阅可视化结果部分，将可视化应用于您的评估策略——您将发现什么洞察？\n\n*I am a professional with expertise in creating advanced software solutions for the interactive exploration of unstructured data. I write about unstructured data and use powerful visualization tools to analyze and make informed decisions.*\n\n## 参考文献\n\n\\[1] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, Haofen Wang: [Retrieval\\-Augmented Generation for Large Language Models: A Survey](https://arxiv.org/abs/2312.10997) (2024\\), arxiv\n\n\\[2] Yixuan Tang, Yi Yang: [MultiHop\\-RAG: Benchmarking Retrieval\\-Augmented Generation for Multi\\-Hop Queries](https://arxiv.org/abs/2401.15391) (2021\\), arXiv\n\n\\[3] Leland McInnes, John Healy, James Melville: [UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction](https://arxiv.org/abs/1802.03426) (2018\\), arXiv\n\n\\[4] Shahul Es, Jithin James, Luis Espinosa\\-Anke, Steven Schockaert: [RAGAS: Automated Evaluation of Retrieval Augmented Generation](https://arxiv.org/abs/2309.15217) (2023\\), arXiv\n\n"},{"lang":"zh","group":"blog","slug":"blog/whats-new-with-claude-sonnet-3-5-claude-3-5-haiku-c1f62a2d2c72","frontmatter":{"title":"Claude Sonnet 3.5 和 Claude 3.5 Haiku 有哪些新功能？","meta_title":"Claude Sonnet 3.5 和 Claude 3.5 Haiku 有哪些新功能？","description":"值得一试吗？","date":"2024-11-13T01:22:29.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*CEMTDlHlMUX66-eoMcSOzg.png","categories":["Natural Language Processing","Programming","Technology/Web"],"author":"Rifx.Online","tags":["language","models","interaction","automation","latency"],"draft":false,"slug":"blog/whats-new-with-claude-sonnet-3-5-claude-3-5-haiku-c1f62a2d2c72"},"content":"\n\n\n\n\n### 首先，Claude是什么？\n\nClaude是由[Anthropic](https://proxy.rifx.online/https://www.anthropic.com/)创建的语言模型，旨在帮助完成诸如回答问题、总结信息和生成文本等任务——类似于ChatGPT。Claude的一个优点是它被设计得更安全，更符合人类意图，因此生成有害或误导性内容的可能性较小。\n\n### 等等……Claude 3\\.5 Sonnet 不是已经发布了吗？\n\n哈哈，是的，虽然名称没有变化，但这个于2024年10月22日发布的新版本Claude 3\\.5 Sonnet和Claude Haiku有很多令人兴奋的更新。\n\n这些新模型在调试代码和从图像中转录文本等任务上更快、更好，这使它们在零售和物流等行业中特别有用。\n\n*如果您希望讨论这些新变化并查看如何将其实施到您的项目中，请[点击这里与我们安排一次免费的电话会议](https://proxy.rifx.online/https://calendly.com/woyera-ai/)!*\n\n### 让我们深入了解一下有什么新东西！\n\n## 人机交互\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*p1anQynliN8ihnT8X2VYqw.gif)\n\nClaude 3.5 Sonnet 最大的更新之一是它能够以更类人方式与计算机互动。\n\n它现在可以导航屏幕、点击按钮和输入，这为自动化任务甚至实时工作流程提供了一些非常有趣的可能性。\n\n请注意，这个功能仍在公开测试阶段，但它已经显示出很大的潜力，特别是在机器人流程自动化（RPA）方面。\n\n## 增强的编码支持\n\nClaude可以在整个软件开发过程中提供帮助，从设计、调试到优化代码——对于任何技术人员来说，它都是一个宝贵的资产。\n\n## 改进的聊天机器人\n\nClaude 的自然语调和高级推理能力使其非常适合构建更具响应性和互动性的聊天机器人。\n\n它能够处理复杂的对话，甚至可以与各种系统连接以简化任务，这使其非常适合客户服务、技术支持等。\n\n## 视觉数据提取\n\n一个显著的特点是Claude分析视觉数据的能力。它可以轻松地解释和提取图表、图形和图解中的信息。\n\n## 知识问答\n\nClaude 3\\.5 Sonnet 也非常适合使用大型数据集、知识库或代码库来回答详细问题。凭借更大的上下文窗口，它是需要快速、准确信息的企业的可靠选择。\n\n## 可用性和定价\n\nClaude 3\\.5 可通过 Anthropic API、Amazon Bedrock 和 Google Cloud 的 Vertex AI 作为 API 使用。对于 API，定价从 **每百万输入令牌 $3** 和 **每百万输出令牌 $15** 开始。\n\n或者，您可以通过 [claude.ai](https://proxy.rifx.online/https://claude.ai/login?returnTo=%2F%3F) 在网上简单使用。您可以免费创建一个帐户，然后 Pro 计划为每月 $20，Team 计划为每月 $25，或 [Enterprise.](https://proxy.rifx.online/https://www.anthropic.com/pricing)\n\n## 安全性与信任\n\nAnthropic 在确保 Claude 3\\.5 Sonnet 的安全性方面投入了大量精力。该模型经过广泛测试，以确保其能够负责任地处理敏感内容，而不影响性能。\n\n这种对安全性的关注有助于防范不当内容等问题，并确保 Claude 适用于广泛的应用场景。\n\n## 用例\n\n无论您是开发者、企业主，还是对人工智能感到好奇，Claude 3.5 Sonnet 都能提供很多功能。\n\n### 自动化重复任务\n\n客户服务团队可以使用Claude处理重复的后台任务，如更新客户订单或处理退款。Claude能够导航屏幕和点击按钮，可以节省数小时的手动工作。\n\n### 聊天机器人\n\n医疗服务提供者可以使用Claude构建聊天机器人，以处理患者互动，例如预约、回答医疗常见问题或指导患者进行症状检查。所有这些都保持自然和对话的语气。\n\n### 视觉数据\n\n金融分析师可以使用Claude分析包含大量图表和图形的季度财报。Claude可以快速提取洞察并总结关键趋势。\n\n### 知识问答\n\n一家科技公司可以使用Claude来管理内部知识库。开发人员可以询问有关现有代码库或故障排除步骤的详细问题，Claude将提供快速且可靠的答案。\n\n## 关于 Claude 3\\.5 Haiku？\n\nClaude 3\\.5 Haiku 是 Anthropic 最快的 AI 模型，提供更好的性能，同时不提高成本或降低速度。它在编码等任务上更强大，并在基准测试中超越了 Claude 3 Opus 和 GPT\\-4o 等模型。\n\n该模型设计具有低延迟，意味着它响应迅速，非常适合实时应用、个性化任务（如分析购买历史）以及其他数据密集型项目。它将在本月晚些时候通过 Amazon Bedrock 和 Google Cloud 等 API 提供，起初将作为仅文本模型，图像支持将很快推出。\n\n## 那么，Claude与ChatGPT相比如何？\n\n在比较**Claude 3\\.5**和**ChatGPT**时，两者都是先进的AI模型，旨在处理类似的任务，如回答问题、生成文本和协助编码，但它们有明显的差异，可能适合不同的需求。\n\nClaude 3\\.5强调**安全性**，减少有害或误导性输出的风险。虽然ChatGPT也重视安全性，但Claude的设计在这一领域给予了额外关注。\n\n在**速度**方面，Claude 3\\.5 Haiku提供更快的响应时间，使其非常适合实时应用。ChatGPT也很快，但在更复杂的任务上可能会有轻微延迟。\n\n在**编码**方面，两种模型表现良好。Claude 3\\.5 Sonnet在最近的基准测试中在调试和代码生成方面表现突出，而ChatGPT仍然是一个可靠的编码帮助和解释选项。\n\n一个关键的区别是**现实世界交互**。Claude 3\\.5能够导航屏幕和自动化任务，而ChatGPT尚未提供此功能。\n\n在**定价**方面，ChatGPT有一个广泛可用的免费版本，而Claude则通过其API和云平台提供灵活的定价。\n\nClaude对安全性、低延迟和先进现实世界交互的关注使其非常适合更专业的应用。与此同时，ChatGPT的多功能性、广泛可用性和强大的编码支持使其成为一个很好的通用工具。\n\n最终，在两者之间的选择取决于您所寻找的内容，但这两种模型都提供强大的能力和独特的优势。\n\n## 结论\n\n了解新工具的最佳方式就是亲自尝试！您可以使用 Claude 3\\.5 Sonnet 来增强编码、聊天机器人、数据分析等多种功能。\n\n告诉我们您将如何使用 Claude。\n\n*如果您需要构建自定义聊天机器人或应用程序，请[点击这里与我们快速通话。](https://proxy.rifx.online/https://calendly.com/woyera-ai/)*\n\n"},{"lang":"zh","group":"blog","slug":"blog/why-embedding-matters-when-building-a-non-english-rag-system-multilingual-embeddings-1e3434ea6180","frontmatter":{"title":"在构建非英语 RAG 系统时，嵌入为什么很重要 - 多语言嵌入","meta_title":"在构建非英语 RAG 系统时，嵌入为什么很重要 - 多语言嵌入","description":"通过对英语与荷兰语多语言模型的详细比较，了解多语言嵌入对 RAG 系统至关重要的原因。","date":"2024-11-13T01:22:29.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*QvODAYxqisUTrt4V.png","categories":["Natural Language Processing","Machine Learning","Multilingual"],"author":"Rifx.Online","tags":["embeddings","multilingual","RAG","Cohere","Dutch"],"draft":false,"slug":"blog/why-embedding-matters-when-building-a-non-english-rag-system-multilingual-embeddings-1e3434ea6180"},"content":"\n\n\n## 为什么嵌入是关键\n\n嵌入是现代生成 AI 的基石，默默推动着我们每天互动的许多系统的功能。简单来说，嵌入是 **文本的数值表示** —— 有效地将单词、句子甚至整个文档转换为数字。这些数字远非随机；它们经过精心设计，以捕捉文本中的含义和关系。例如，“dog”和“puppy”的嵌入在数值空间中会更靠近，而“car”的嵌入则会相对较远，反映出它们的 **语义相似性**。将意义编码为可测量的形式的能力，使得嵌入在搜索、推荐系统以及 **检索增强生成 (RAG)** 等高级 AI 应用中不可或缺。\n\n\n\n这种数字化转化使 AI 能够以有意义的方式比较和理解文本。当处理大量数据时，尤其是在 RAG 系统中，嵌入变得至关重要。这些系统将嵌入的力量与称为 **向量数据库** 的专用存储解决方案相结合。与传统数据库搜索精确匹配不同，向量数据库经过优化，以根据含义找到最接近的匹配。这种能力使 RAG 系统能够从庞大的知识库中检索出最相关的信息，并用它生成准确、具有上下文的响应。通过桥接原始数据和智能检索，嵌入和向量数据库共同构成了 RAG 系统成功的基础。\n\n## 多语言系统的挑战\n\n构建在英语中表现良好的RAG系统已经是一项复杂的任务，但将其扩展到其他语言则带来了全新的挑战。由于训练数据丰富和语言结构简单，英语嵌入通常经过高度优化。然而，使用这些经过英语训练的嵌入来处理其他语言可能会导致显著的不准确性。不同语言具有其自身的细微差别、语法和文化背景，而主要基于英语文本训练的标准嵌入模型往往无法捕捉这些特征。虽然存在一些多语言嵌入模型来弥补这一差距，但它们在不同语言中的有效性并不相同，尤其是对于那些训练数据有限或具有独特语言特征的语言。这使得构建在非英语语言中与英语一样准确和可靠的RAG系统变得困难。\n\n### 为什么英语嵌入更准确？\n\n1. **高质量训练数据的丰富性**  \n英语主导了数字领域，拥有无与伦比的高质量内容可供训练。像维基百科、书籍、研究论文和社交媒体等数据集在英语中要比其他语言丰富得多。相比之下，许多语言，特别是低资源语言，缺乏多样化和标准化的数据集，这限制了在这些语言上训练的嵌入的质量。\n2. **模型优化偏见**  \n像BERT和GPT这样的NLP模型最初是为英语开发和优化的，通常在多语言版本中仍然优先考虑英语。多语言模型在同一参数空间内平衡多种语言的学习，这可能会稀释对代表性较少的语言的性能，倾向于像英语这样的主导语言。\n3. **语言复杂性和多样性**  \n与许多其他语言相比，英语的形态相对简单。例如，英语中的词形往往保持一致（例如，“run”和“running”），而像土耳其语或芬兰语这样的语言具有高度的屈折形式，一个根词可能有数十种变化。此外，具有不同语法或词序的语言，如日语（主语-宾语-动词）或阿拉伯语（灵活的词序），对优化为英语结构的模型构成额外挑战。\n4. **语义和文化对齐**  \n跨语言捕捉语义意义远非简单。单词和短语往往带有细微的含义，无法直接翻译。例如，英语单词“love”在其他语言中有多个文化上独特的对应词（例如，西班牙语中的“amor”，希腊语中的“eros”或“agape”）。未能考虑这些差异的嵌入在多语言对齐方面表现不佳。\n5. **基准测试和评估偏见**  \n许多基准数据集和评估方法都是以英语为中心设计的。这种以英语为中心的关注可能会人为地提高模型在英语中的感知性能，同时掩盖它们在其他语言中的局限性。\n\n### 对 RAG 系统的影响\n\n当嵌入无法处理其他语言的复杂性时，对 RAG 系统的影响可能是显著的。检索结果往往变得不相关，甚至完全错误，因为嵌入可能难以捕捉非英语查询的细微含义。这不仅影响准确性，还削弱了用户信任和系统的整体实用性。在检索过程中，关键文本片段可能被遗漏，阻止系统获取生成准确且上下文相关的响应所需的信息。\n\n为了使多语言 RAG 系统表现良好，它需要能够在语言之间语义对齐的嵌入，同时考虑到它们独特的结构和文化复杂性。投资高质量的多语言嵌入并对其进行特定语言或任务的微调是至关重要的。这确保了 RAG 系统能够满足任何语言用户的需求——不仅仅是英语。\n\n但不同的嵌入在非英语环境中的表现如何呢？为了解这个问题，我们将使用荷兰数据集比较一个英语嵌入模型和一个多语言嵌入模型。这个测试将揭示不同的嵌入方法如何影响多语言 RAG 系统中的检索准确性和生成响应的质量。\n\n## 比较荷兰语RAG系统的嵌入模型\n\n为了了解不同的嵌入模型如何处理像荷兰语这样的非英语语言，我们将比较在Amazon Bedrock上可用的两个模型：**Cohere Embed English v3**和**Cohere Embed Multilingual v3**。这两个模型代表了对嵌入的不同处理方式——一个专门针对英语进行了优化，另一个则设计用于多语言任务。下表总结了它们的主要属性：\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*pBhIHfOsb-McrjHKvtq4Xw.png)\n\n### 构建嵌入\n\n为了评估嵌入模型的性能，我们将使用 LangChain 框架构建一个本地向量存储。对于此次评估，我们将使用用荷兰语撰写的消防员指南作为我们的数据集。该文档包含技术和程序信息，使其成为非英语语言语义检索的一个现实且具有挑战性的用例。下面是创建本地向量存储和索引文档块的清理和简化代码。我们将使用此设置来测试两个嵌入模型：**Cohere Embed English v3** 和 **Cohere Embed Multilingual v3**。\n\n```python\nimport os\nfrom langchain_community.document_loaders import DirectoryLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import FAISS\nfrom langchain_aws import BedrockEmbeddings\nimport boto3\n\n## Step 1: Load documents\nloader = DirectoryLoader('data', glob=\"**/*.pdf\")  # Adjust 'data' to your document directory\ndocuments = loader.load()\n\nprint(f\"You have {len(documents)} documents\")\nprint(f\"Document 1 contains {len(documents[0].page_content)} characters\")\n\n## Step 2: Split documents into smaller chunks\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)\nchunks = text_splitter.split_documents(documents)\n\nprint(f\"You have {len(chunks)} chunks\")\nprint(f\"The first chunk is {len(chunks[0].page_content)} characters long\")\n\n## Step 3: Set up Bedrock embeddings\nbedrock_client = boto3.client(\"bedrock-runtime\", region_name='us-east-1')\nbedrock_embeddings = BedrockEmbeddings(model_id=\"cohere.embed-multilingual-v3\", client=bedrock_client)\n\n## Step 4: Build the FAISS vectorstore\nvectorstore = FAISS.from_documents(chunks, bedrock_embeddings)\n\n## Save the vectorstore locally for reuse\nvectorstore.save_local(\"faiss_cohere_multilingual\")\n```\n\n## 代码如何工作\n\n1. **文档加载**：\n代码从 `data` 目录加载所有 PDF 文件。您可以调整文件路径和格式以匹配您的数据集。\n2. **文本拆分**：\n文档被拆分为每个 400 个字符的小块，重叠 50 个字符，以提高检索的准确性。这确保每个块在上下文上保持有意义。\n3. **嵌入模型**：\n`BedrockEmbeddings` 类初始化嵌入模型。您可以更改 `model_id` 来测试 **Cohere Embed English v3 或 Cohere Embed Multilingual v3**。\n4. **本地向量存储**：\nFAISS 库用于从文档块创建内存中的向量存储。这允许快速相似性搜索，并可以本地保存以供重用。\n\n要测试所有模型，请将 `BedrockEmbeddings` 初始化中的 `model_id` 替换为相应的模型：\n\n* `\"cohere.embed-english-v3\"` 用于 Cohere English。\n* `\"cohere.embed-multilingual-v3\"` 用于 Cohere Multilingual。\n\n### 评估嵌入模型\n\n为了评估嵌入模型的性能，我们将提出问题：**“Welke rangen zijn er bij de brandweer?”**，其翻译为**“消防部门存在哪些等级？”**。选择这个问题是因为我们的文档中仅使用了术语**“hiërarchie”**，在荷兰语中与**“rangen”**具有相似的语义。然而，在英语中，“hierarchy”和“ranks”并没有语义上的相似性。\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*6N3C8C500hMQ3GNNkuu21A.png)\n\n这种区别对我们的测试至关重要。我们预期**Cohere Embed English v3**模型在处理这个查询时会遇到困难，因为它依赖于英语语义，而这些术语并不相关。另一方面，**Cohere Embed Multilingual v3**模型经过训练能够理解荷兰语语义，应该能够从文档中检索到正确的信息，展示其处理非英语语言语义细微差别的能力。\n\n通过提出这个问题，我们旨在突出语义对荷兰RAG系统检索性能的影响。这项测试将清晰地比较模型处理非英语查询和检索相关信息的能力。结果将展示多语言嵌入在非英语环境中实现准确检索的重要性。\n\n要实现和测试这个设置，我们可以使用以下代码。该脚本演示了如何查询向量存储并利用RAG链将嵌入与语言模型结合以回答问题。请注意，在测试不同的嵌入（例如**Cohere Embed English v3**与**Cohere Embed Multilingual v3**）时，您需要确保向量存储是使用相应的嵌入模型构建的。用您想要测试的嵌入模型索引的向量存储替换，以获得准确的结果。\n\n```python\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain_aws import ChatBedrock\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.output_parsers import StrOutputParser\n\ninstructions = \"\"\"Je bent een brandweer expert. Beantwoord de vraag, maak gebruik van de context\"\"\"\n\nhuman = \"\"\"\nDit is de context: {context}\nDit is de vraag: {question}\n\"\"\"\n\nprompt = ChatPromptTemplate(\n    messages=[\n        SystemMessagePromptTemplate.from_template(instructions),\n        HumanMessagePromptTemplate.from_template(human), #User query will go here\n    ],\n    input_variables=['context','question'], # context provided by retriever and question by the user\n)\nmodel = ChatOpenAI(\n    model=\"gpt-4\",  # or \"gpt-3.5-turbo\" based on your needs\n    temperature=0,\n    max_tokens=512\n)\n\n## Function to merge text chunks\ndef format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n\n## RAG chain\nrag_chain = (\n        {\"context\": vectorstore.as_retriever(k=10) | format_docs, \"question\": RunnablePassthrough()}\n        | prompt\n        | model\n        | StrOutputParser()\n)\n\nrag_chain.invoke(\"Welke rangen zijn er bij de brandweer?\")\n```\n\n## 嵌入模型的结果\n\n为了评估这两个嵌入模型的性能，我们提出了这个问题：**“Welke rangen zijn er bij de brandweer?”**（翻译：“消防部门存在哪些职级？”）。以下是每个模型生成的响应：\n\n### Cohere Embed English v3\n\n**Response**:*“文本没有提供关于消防队内部不同等级的具体信息。然而，在许多消防组织中，有不同的等级，如消防员、 lieutenant、 captain、 battalion chief、 assistant chief 和 fire chief。具体的等级可能因国家和具体组织而异。”*\n\n**Analysis**:该响应是通用的，并没有依赖于荷兰文档提供的上下文。这是可以预期的，因为英语嵌入模型在连接“hiërarchie”（文档中使用）和“rangen”（查询中使用）之间的语义相似性时遇到了困难。因此，它默认使用关于消防部门等级的一般知识。\n\n### Cohere Embed Multilingual v3\n\n**Response**:*“在消防队中有不同的等级。这些等级分为基础框架和中层框架。在基础框架中，我们有 B01 消防员和 B02 下士。在中层框架中，由下士组成，我们有 M01 中士和 M02 副官。这些等级在工作制服、休闲服和干预服上都有标识。”*\n\n**Analysis**:此响应高度相关，并准确地从文档中检索信息。多语言嵌入模型成功识别了“hiërarchie”（上下文）与“rangen”（查询）之间的语义关系。它直接基于文档内容提供了详细的答案，展示了其有效处理荷兰特定语义的能力。\n\n### 关键要点\n\n* **Cohere Embed English v3**：由于查询与文档术语之间缺乏语义对齐，英语模型未能从荷兰文档中检索到相关上下文。这突显了在非英语任务中使用英语特定嵌入的局限性。\n* **Cohere Embed Multilingual v3**：多语言模型在此测试中表现出色，从荷兰文档中检索并利用了上下文相关的信息。这表明多语言嵌入在实现准确检索和有效回答非英语查询方面的重要性。\n\n## 结论\n\n本次评估突显了一个对任何构建非英语语言的检索增强生成（RAG）系统的人来说至关重要的见解：**嵌入非常重要**，尤其是在任务要求跨语言细致理解时。Cohere Embed English v3 和 Cohere Embed Multilingual v3 模型在性能上的明显差异说明了英语特定嵌入在非英语环境中的局限性，以及多语言模型的巨大价值。\n\n在回答荷兰语查询时，多语言模型表现出色，能够直接从文档中检索到准确且上下文丰富的信息。与此同时，英语嵌入模型则退回到通用的、不相关的知识，显示出其在查询与文档内容之间弥合语义差距的无能。\n\n对于在全球多语言环境中开发 AI 系统的组织而言，这项测试强化了为手头任务选择合适嵌入模型的重要性。多语言嵌入不仅仅是一个“锦上添花”的特性；它们对于确保非英语应用的准确性、相关性和用户信任至关重要。\n\n随着生成 AI 继续拓展其影响力，通过更好的嵌入来拥抱语言多样性将是提供有意义和有影响力的解决方案的关键。通过优先考虑多语言能力，企业可以创建不仅更智能而且更具包容性的系统——赋能跨语言和文化的用户。\n\n***关注我以获取更多 AI 深度解析！***\n\n[Medium](https://proxy.rifx.online/https://medium.com/@lorevanoudenhove), [Instagram](https://proxy.rifx.online/https://www.instagram.com/lorevanoudenhove.ai/), [YouTube](https://proxy.rifx.online/https://www.youtube.com/channel/UCVyOJS1VV7FxPsStK65pHcA), [Pairrot](https://proxy.rifx.online/https://www.pairrot.eu/)\n\n"},{"lang":"zh","group":"blog","slug":"blog/will-bolt-new-ai-will-replace-v0-dev-129a3366eb44","frontmatter":{"title":"新人工智能 Bolt 是否会取代 v0.dev","meta_title":"新人工智能 Bolt 是否会取代 v0.dev","description":"新人工智能 Bolt 是否会取代 v0.dev","date":"2024-11-13T01:22:35.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*g5S8PyYqR87bdyGhb77rRw.png","categories":["Programming","Technology/Web","Data Science"],"author":"Rifx.Online","tags":["Bolt.new","v0.dev","web","components","layouts"],"draft":false,"slug":"blog/will-bolt-new-ai-will-replace-v0-dev-129a3366eb44"},"content":"\n### AI工具\n\n> **不是会员？免费阅读 [这里](https://proxy.rifx.online/https://tarzzotech.medium.com/129a3366eb44?source=friends_link&sk=385b6b2e482ae9d16ef8f99fe083b8ae)。**\n\n\n\n网页开发的世界正在快速发展，市场上涌现出多种AI工具。这些新AI工具通过自然语言提示帮助开发者生成网页组件和复杂的代码结构。这些工具提供更好的代码质量，并减少手动编写代码的时间。\n\n最近，市场上出现了一种新工具 **bolt.new**，它看起来与 v0\\.dev 相似。随着 Bolt.new 的出现，产生了一个问题：***这个新的AI平台会取代 v0\\.dev 吗，还是这些工具完全是不同的用途？***\n\n在这篇文章中，我将分享我使用该工具一段时间的经验。我主要讨论 **v0\\.dev** 和 **Bolt.new** 之间的关键区别，比较它们的优势、使用案例和输出。通过考察真实案例，我们将确定 **Bolt.new** 是否对 **v0\\.dev** 构成了真正的威胁，或者这两种工具在开发者的工具包中各有其位置。\n\n在我之前的博客中，我详细解释了关于 v0\\.dev 的所有细节和我的想法，所以请查看 [**这里**](https://proxy.rifx.online/https://tarzzotech.medium.com/4191292876b3?source=friends_link&sk=9730b35a75771953d0541e459c8adeaa)。我不想在这里分享重复的内容。让我们看看 **bolt.new** 以及与 **v0\\.dev** 的比较。\n\n## Bolt.new 概述\n\n**Bolt.new** 作为一个新兴的 AI 驱动平台，引起了广泛关注，其目标不仅限于前端组件。虽然 **v0.dev** 专注于生成较小的组件，但 Bolt.new 旨在提供更复杂的解决方案，例如完整的页面布局或跨越前端和后端的多步骤工作流。\n\n**Bolt.new 的主要特点：**\n\n* **完整布局：** **Bolt.new** 能够生成完整的页面布局，包括页眉、页脚、侧边栏和主要内容区域。\n* **多功能代码生成：** 除了网页组件，**Bolt.new** 还可以生成服务器端脚本、数据库配置及构建全栈应用所需的其他元素。\n* **增强的定制化：** 虽然 V0.dev 专注于单个组件，但 **Bolt.new** 提供了更大的灵活性，允许开发人员生成完全功能的布局或结构。\n\n## 比较 V0\\.dev 和 Bolt.new\n\n虽然这两个工具都提供了令人印象深刻的功能，但它们针对开发过程的不同方面。\n\n* **v0\\.dev** 专注于可以轻松定制并集成到前端代码库中的单个网页组件。它非常适合需要快速解决方案的开发者，例如在 React 或 Vue 等框架中使用的按钮、卡片或表单。\n* **Bolt.new** 则采取更全面的方法，允许开发者生成完整的页面布局或甚至后端配置。这使得它成为一个更灵活的选择，适用于需要多种类型代码的项目，而不仅仅是前端组件。\n\n## 示例比较：\n\n在这里，我们开始从小组件到完整页面比较这些工具。\n\n### 创建按钮组件\n\n**提示：** “生成一个带下拉菜单和搜索栏的响应式导航栏。”\n\n**v0\\.dev**\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8-1NaJb_msK1OLv7MHbOjw.gif)\n\n**bolt.new**\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*WsRSUU5brIql4uBb7wFkAg.gif)\n\n**v0\\.dev** 创建了一个具有基本样式的按钮元素，并且在悬停时会改变背景颜色和文本样式的悬停效果。\n\n**Bolt.new** 创建了一个具有悬停效果的按钮，并且包含在点击时触发警告框的JavaScript代码，包括内联JavaScript和基本CSS样式。\n\n### 创建导航栏\n\n**提示：** “生成一个带有下拉菜单和搜索栏的响应式导航栏。”\n\n**v0\\.dev**\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*sOJ0EveSOVKtJJltLGiVCA.gif)\n\n**bolt.new**\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*aGCWfH5ULTTFb-FS-kT-7w.gif)\n\n这两个工具生成的导航栏有些相似。但 **bolt.new** 生成的效果略优于 **v0\\.dev**。\n\n### 创建网站。\n\n**提示:** “生成一个完整的页面布局，包含粘性头部、页脚、可折叠侧边栏和为博客文章设计的主要内容区域。”\n\n**v0\\.dev**\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kwEXDG3tb1CiHetZr5W03Q.png)\n\n**bolt.new**\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*HxKnFJQf--e-E1fh8yyxVw.png)\n\n来自 **bolt.new** 的输出让人无言以对。\n\n## 最终，Bolt.new 会取代 v0\\.dev 吗？\n\n考虑到 Bolt.new 的更广泛范围，它可能看起来会掩盖 v0\\.dev。然而，这两个平台很可能会共存，各自为开发过程的不同阶段提供价值。\n\n**v0\\.dev** 在快速生成特定的高质量网页组件方面无与伦比。它非常适合需要可重用、响应式组件的前端开发人员，这些组件可以轻松集成到他们的项目中。\n\n**Bolt.new** 虽然提供了更多的灵活性，但并不一定是 v0\\.dev 的替代品。它更广泛的功能范围吸引了需要完整布局或涵盖前端和后端的代码结构的开发人员。\n\n## 结论\n\nv0\\.dev 和 Bolt.new 都是强大的 AI 工具，各自具有独特的优势。虽然 Bolt.new 的多功能性使其成为希望生成更复杂代码的开发者的强劲竞争者，但 v0\\.dev 仍然是希望快速获得可定制组件的开发者的首选工具。\n\n最终，Bolt.new 是否会取代 v0\\.dev？这不太可能。这些平台满足不同的需求。v0\\.dev 在轻松生成特定的、可重用的组件方面表现出色。Bolt.new 则为复杂布局提供了更广泛的功能。那么，您认为哪种工具将在 AI 辅助开发的未来中占据主导地位，还是它们会继续相辅相成？答案取决于您正在工作的开发者需求。\n\n\n"},{"lang":"fr","group":"blog","slug":"blog/post-1","frontmatter":{"title":"Comment créer une application avec des technologies modernes","meta_title":"","description":"Ceci est une méta-description","date":"2022-04-04T05:00:00.000Z","image":"/images/image-placeholder.png","categories":["french","Application","Data"],"author":"John Doe","tags":["nextjs","tailwind","react"],"draft":false,"slug":"blog/post-1"},"content":"\nPersonne ne veut même sortir un maquillage de l'urne des soins empoisonnés. C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\n## Design Créatif\n\nCar en guise de maquillage, l'urne du poison C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\n> Le client lui-même doit pouvoir poursuivre l'adipisicing. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n"},{"lang":"fr","group":"blog","slug":"blog/post-2","frontmatter":{"title":"Comment créer une application avec des technologies modernes","meta_title":"","description":"Ceci est une méta-description","date":"2022-04-04T05:00:00.000Z","image":"/images/image-placeholder.png","categories":["Technology","Data"],"author":"Sam Wilson","tags":["technology","tailwind"],"draft":false,"slug":"blog/post-2"},"content":"\nPersonne ne veut même sortir un maquillage de l'urne des soins empoisonnés. C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\n## Design Créatif\n\nCar en guise de maquillage, l'urne du poison C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\n> Le client lui-même doit pouvoir poursuivre l'adipisicing. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n"},{"lang":"fr","group":"blog","slug":"blog/post-3","frontmatter":{"title":"Comment créer une application avec des technologies modernes","meta_title":"","description":"Ceci est une méta-description","date":"2022-04-04T05:00:00.000Z","image":"/images/image-placeholder.png","categories":["Software"],"author":"John Doe","tags":["software","tailwind"],"draft":false,"slug":"blog/post-3"},"content":"\nPersonne ne veut même sortir un maquillage de l'urne des soins empoisonnés. C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\n## Design Créatif\n\nCar en guise de maquillage, l'urne du poison C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\n> Le client lui-même doit pouvoir poursuivre l'adipisicing. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n"},{"lang":"fr","group":"blog","slug":"blog/post-4","frontmatter":{"title":"Comment créer une application avec des technologies modernes","meta_title":"","description":"Ceci est une méta-description","date":"2022-04-04T05:00:00.000Z","image":"/images/image-placeholder.png","categories":["Architecture"],"author":"John Doe","tags":["silicon","technology"],"draft":false,"slug":"blog/post-4"},"content":"\nPersonne ne veut même sortir un maquillage de l'urne des soins empoisonnés. C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\n## Design Créatif\n\nCar en guise de maquillage, l'urne du poison C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\n> Le client lui-même doit pouvoir poursuivre l'adipisicing. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n"},{"lang":"en","group":"models","slug":"models/chatgpt-4o-latest","frontmatter":{"title":"OpenAI: ChatGPT-4o","meta_title":"OpenAI: ChatGPT-4o","description":"OpenAI: ChatGPT-4o","date":"2024-08-14T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text image 2 text"],"author":"openai","tags":["Chatbots","Generative AI","Machine Learning","Natural Language Processing"],"draft":false,"id":"chatgpt-4o-latest","context":128000,"input":0.000005,"output":0.000015,"img":0.007225,"request":0,"last_updated":"2024-08-14T00:00:00.000Z","slug":"models/chatgpt-4o-latest"},"content":"\nDynamic model continuously updated to the current version of [GPT-4o](/openai/gpt-4o) in ChatGPT. Intended for research and evaluation.\n\nNote: This model is currently experimental and not suitable for production use-cases, and may be heavily rate-limited.\n\n"},{"lang":"en","group":"models","slug":"models/claude-3-haiku","frontmatter":{"title":"Anthropic: Claude 3 Haiku","meta_title":"Anthropic: Claude 3 Haiku","description":"Anthropic: Claude 3 Haiku","date":"2024-03-13T00:00:00.000Z","image":"https://img.rifx.online/logo/anthropic.svg","categories":["text image 2 text"],"author":"anthropic","tags":["Programming","Machine Learning","Generative AI","Chatbots","Natural Language Processing"],"draft":false,"id":"claude-3-haiku","context":200000,"input":2.5e-7,"output":0.00000125,"img":0.0004,"request":0,"last_updated":"2024-10-24T11:54:59.000Z","slug":"models/claude-3-haiku"},"content":"\nClaude 3 Haiku is Anthropic's fastest and most compact model for\nnear-instant responsiveness. Quick and accurate targeted performance.\n\nSee the launch announcement and benchmark results [here](https://www.anthropic.com/news/claude-3-haiku)\n\n#multimodal\n\n"},{"lang":"en","group":"models","slug":"models/claude-3-opus","frontmatter":{"title":"Anthropic: Claude 3 Opus","meta_title":"Anthropic: Claude 3 Opus","description":"Anthropic: Claude 3 Opus","date":"2024-03-05T00:00:00.000Z","image":"https://img.rifx.online/logo/anthropic.svg","categories":["text image 2 text"],"author":"anthropic","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"claude-3-opus","context":200000,"input":0.000015,"output":0.000075,"img":0.024,"request":0,"last_updated":"2024-11-07T09:45:35.000Z","slug":"models/claude-3-opus"},"content":"\nClaude 3 Opus is Anthropic's most powerful model for highly complex tasks. It boasts top-level performance, intelligence, fluency, and understanding.\n\nSee the launch announcement and benchmark results [here](https://www.anthropic.com/news/claude-3-family)\n\n#multimodal\n\n"},{"lang":"en","group":"models","slug":"models/claude-3-sonnet","frontmatter":{"title":"Anthropic: Claude 3 Sonnet","meta_title":"Anthropic: Claude 3 Sonnet","description":"Anthropic: Claude 3 Sonnet","date":"2024-03-05T00:00:00.000Z","image":"https://img.rifx.online/logo/anthropic.svg","categories":["text image 2 text"],"author":"anthropic","tags":["Programming","Technology","Machine Learning","Data Science","Chatbots"],"draft":false,"is_recommended":true,"id":"claude-3-sonnet","context":200000,"input":0.000003,"output":0.000015,"img":0.0048,"request":0,"last_updated":"2024-11-14T04:05:16.000Z","slug":"models/claude-3-sonnet"},"content":"\nClaude 3 Sonnet is an ideal balance of intelligence and speed for enterprise workloads. Maximum utility at a lower price, dependable, balanced for scaled deployments.\n\nSee the launch announcement and benchmark results [here](https://www.anthropic.com/news/claude-3-family)\n\n#multimodal\n\n"},{"lang":"en","group":"models","slug":"models/claude-35-haiku","frontmatter":{"title":"Anthropic: Claude 3.5 Haiku","meta_title":"Anthropic: Claude 3.5 Haiku","description":"Anthropic: Claude 3.5 Haiku","date":"2024-11-04T00:00:00.000Z","image":"https://img.rifx.online/logo/anthropic.svg","categories":["text 2 text"],"author":"anthropic","tags":["Programming","Chatbots","Data Science","Machine Learning","Natural Language Processing"],"draft":false,"id":"claude-3.5-haiku","context":200000,"input":0.000001,"output":0.000005,"img":0,"request":0,"last_updated":"2024-11-07T09:46:02.000Z","slug":"models/claude-35-haiku"},"content":"\nClaude 3.5 Haiku features offers enhanced capabilities in speed, coding accuracy, and tool use. Engineered to excel in real-time applications, it delivers quick response times that are essential for dynamic tasks such as chat interactions and immediate coding suggestions.\n\nThis makes it highly suitable for environments that demand both speed and precision, such as software development, customer service bots, and data management systems.\n\nThis model is currently pointing to [Claude 3.5 Haiku (2024-10-22)](/anthropic/claude-3-5-haiku-20241022).\n\n"},{"lang":"en","group":"models","slug":"models/claude-35-sonnet","frontmatter":{"title":"Anthropic: Claude 3.5 Sonnet","meta_title":"Anthropic: Claude 3.5 Sonnet","description":"Anthropic: Claude 3.5 Sonnet","date":"2024-06-20T00:00:00.000Z","image":"https://img.rifx.online/logo/anthropic.svg","categories":["text image 2 text"],"author":"anthropic","tags":["Programming","Data Science","Computer Vision","Chatbots","Autonomous Systems"],"draft":false,"id":"claude-3.5-sonnet","context":200000,"input":0.000003,"output":0.000015,"img":0.0048,"request":0,"last_updated":"2024-10-24T11:45:46.000Z","slug":"models/claude-35-sonnet"},"content":"\nClaude 3.5 Sonnet delivers better-than-Opus capabilities, faster-than-Sonnet speeds, at the same Sonnet prices. Sonnet is particularly good at:\n\n- Coding: Autonomously writes, edits, and runs code with reasoning and troubleshooting\n- Data science: Augments human data science expertise; navigates unstructured data while using multiple tools for insights\n- Visual processing: excelling at interpreting charts, graphs, and images, accurately transcribing text to derive insights beyond just the text alone\n- Agentic tasks: exceptional tool use, making it great at agentic tasks (i.e. complex, multi-step problem solving tasks that require engaging with other systems)\n\n#multimodal\n\n"},{"lang":"en","group":"models","slug":"models/command-r-plus","frontmatter":{"title":"Cohere: Command R+ (08-2024)","meta_title":"Cohere: Command R+ (08-2024)","description":"Cohere: Command R+ (08-2024)","date":"2024-08-30T00:00:00.000Z","image":"https://img.rifx.online/logo/cohere.svg","categories":["text 2 text"],"author":"cohere","tags":["Technology","Programming","Machine Learning","Generative AI","Ethics"],"draft":false,"id":"command-r-plus","context":128000,"input":0.000002375,"output":0.0000095,"img":0,"request":0,"last_updated":"2024-11-07T09:33:44.000Z","slug":"models/command-r-plus"},"content":"\ncommand-r-plus-08-2024 is an update of the [Command R+](/cohere/command-r-plus) with roughly 50% higher throughput and 25% lower latencies as compared to the previous Command R+ version, while keeping the hardware footprint the same.\n\nRead the launch post [here](https://docs.cohere.com/changelog/command-gets-refreshed).\n\nUse of this model is subject to Cohere's [Acceptable Use Policy](https://docs.cohere.com/docs/c4ai-acceptable-use-policy).\n\n"},{"lang":"en","group":"models","slug":"models/command-r","frontmatter":{"title":"Cohere: Command R (08-2024)","meta_title":"Cohere: Command R (08-2024)","description":"Cohere: Command R (08-2024)","date":"2024-08-30T00:00:00.000Z","image":"https://img.rifx.online/logo/cohere.svg","categories":["text 2 text"],"author":"cohere","tags":["Programming","Natural Language Processing","Generative AI","Machine Learning","Data Science"],"draft":false,"id":"command-r","context":128000,"input":1.425e-7,"output":5.7e-7,"img":0,"request":0,"last_updated":"2024-11-07T09:33:55.000Z","slug":"models/command-r"},"content":"\ncommand-r-08-2024 is an update of the [Command R](/cohere/command-r) with improved performance for multilingual retrieval-augmented generation (RAG) and tool use. More broadly, it is better at math, code and reasoning and is competitive with the previous version of the larger Command R+ model.\n\nRead the launch post [here](https://docs.cohere.com/changelog/command-gets-refreshed).\n\nUse of this model is subject to Cohere's [Acceptable Use Policy](https://docs.cohere.com/docs/c4ai-acceptable-use-policy).\n\n"},{"lang":"en","group":"models","slug":"models/deepseek-chat","frontmatter":{"title":"DeepSeek V2.5","meta_title":"DeepSeek V2.5","description":"DeepSeek V2.5","date":"2024-05-14T00:00:00.000Z","image":"https://img.rifx.online/logo/deepseek.svg","categories":["text 2 text"],"author":"deepseek","tags":["Programming","Natural Language Processing","Machine Learning","Data Science","Chatbots"],"draft":false,"id":"deepseek-chat","context":128000,"input":1.4e-7,"output":2.8e-7,"img":0,"request":0,"last_updated":"2024-11-01T04:19:11.000Z","slug":"models/deepseek-chat"},"content":"\nDeepSeek-V2.5 is an upgraded version that combines DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct. The new model integrates the general and coding abilities of the two previous versions.\n\nDeepSeek-V2 Chat is a conversational finetune of DeepSeek-V2, a Mixture-of-Experts (MoE) language model. It comprises 236B total parameters, of which 21B are activated for each token.\n\nCompared with DeepSeek 67B, DeepSeek-V2 achieves stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times.\n\nDeepSeek-V2 achieves remarkable performance on both standard benchmarks and open-ended generation evaluations.\n\n"},{"lang":"en","group":"models","slug":"models/dolphin-mixtral-8x22b","frontmatter":{"title":"Dolphin 2.9.2 Mixtral 8x22B 🐬","meta_title":"Dolphin 2.9.2 Mixtral 8x22B 🐬","description":"Dolphin 2.9.2 Mixtral 8x22B 🐬","date":"2024-06-08T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"cognitivecomputations","tags":["Natural Language Processing","Generative AI","Chatbots","Roleplay","Ethics"],"draft":false,"id":"dolphin-mixtral-8x22b","context":65536,"input":9e-7,"output":9e-7,"img":0,"request":0,"last_updated":"2024-11-04T12:49:50.000Z","slug":"models/dolphin-mixtral-8x22b"},"content":"\nDolphin 2.9 is designed for instruction following, conversational, and coding. This model is a finetune of [Mixtral 8x22B Instruct](/mistralai/mixtral-8x22b-instruct). It features a 64k context length and was fine-tuned with a 16k sequence length using ChatML templates.\n\nThis model is a successor to [Dolphin Mixtral 8x7B](/cognitivecomputations/dolphin-mixtral-8x7b).\n\nThe model is uncensored and is stripped of alignment and bias. It requires an external alignment layer for ethical use. Users are cautioned to use this highly compliant model responsibly, as detailed in a blog post about uncensored models at [erichartford.com/uncensored-models](https://erichartford.com/uncensored-models).\n\n#moe #uncensored\n\n"},{"lang":"en","group":"models","slug":"models/dolphin-mixtral-8x7b","frontmatter":{"title":"Dolphin 2.6 Mixtral 8x7B 🐬","meta_title":"Dolphin 2.6 Mixtral 8x7B 🐬","description":"Dolphin 2.6 Mixtral 8x7B 🐬","date":"2023-12-21T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"cognitivecomputations","tags":["Programming","Natural Language Processing","Generative AI","Ethics","Chatbots"],"draft":false,"id":"dolphin-mixtral-8x7b","context":32768,"input":5e-7,"output":5e-7,"img":0,"request":0,"last_updated":"2024-11-04T12:52:28.000Z","slug":"models/dolphin-mixtral-8x7b"},"content":"\nThis is a 16k context fine-tune of [Mixtral-8x7b](/mistralai/mixtral-8x7b). It excels in coding tasks due to extensive training with coding data and is known for its obedience, although it lacks DPO tuning.\n\nThe model is uncensored and is stripped of alignment and bias. It requires an external alignment layer for ethical use. Users are cautioned to use this highly compliant model responsibly, as detailed in a blog post about uncensored models at [erichartford.com/uncensored-models](https://erichartford.com/uncensored-models).\n\n#moe #uncensored\n\n"},{"lang":"en","group":"models","slug":"models/eva-qwen-25-14b","frontmatter":{"title":"EVA Qwen2.5 14B","meta_title":"EVA Qwen2.5 14B","description":"EVA Qwen2.5 14B","date":"2024-09-30T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"eva-unit-01","tags":["Roleplay","Programming","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"eva-qwen-2.5-14b","context":32768,"input":2.5e-7,"output":5e-7,"img":0,"request":0,"last_updated":"2024-11-11T03:09:28.000Z","slug":"models/eva-qwen-25-14b"},"content":"\nA model specializing in RP and creative writing, this model is based on Qwen2.5-14B, fine-tuned with a mixture of synthetic and natural data.\n\nIt is trained on 1.5M tokens of role-play data, and fine-tuned on 1.5M tokens of synthetic data.\n\n"},{"lang":"en","group":"models","slug":"models/eva-qwen-25-32b","frontmatter":{"title":"Eva Qwen2.5 32B","meta_title":"Eva Qwen2.5 32B","description":"Eva Qwen2.5 32B","date":"2024-11-08T22:27:27.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"eva-unit-01","tags":["Roleplay","Programming","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"eva-qwen-2.5-32b","context":32000,"input":5e-7,"output":5e-7,"img":0,"request":0,"last_updated":"2024-11-11T02:57:47.000Z","slug":"models/eva-qwen-25-32b"},"content":"\nA roleplaying/storywriting specialist model, full-parameter finetune of Qwen2.5-32B on mixture of synthetic and natural data.\n\nIt uses Celeste 70B 0.1 data mixture, greatly expanding it to improve versatility, creativity and \"flavor\" of the resulting model.\n\n"},{"lang":"en","group":"models","slug":"models/gemini-flash-15-8b-exp","frontmatter":{"title":"Google: Gemini Flash 8B 1.5 Experimental","meta_title":"Google: Gemini Flash 8B 1.5 Experimental","description":"Google: Gemini Flash 8B 1.5 Experimental","date":"2024-08-28T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text image 2 text"],"author":"google","tags":["Technology","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"gemini-flash-1.5-8b-exp","context":1000000,"input":0,"output":0,"img":0,"request":0,"last_updated":"2024-11-11T03:14:22.000Z","slug":"models/gemini-flash-15-8b-exp"},"content":"\nGemini 1.5 Flash 8B Experimental is an experimental, 8B parameter version of the [Gemini 1.5 Flash](/google/gemini-flash-1.5) model.\n\nUsage of Gemini is subject to Google's [Gemini Terms of Use](https://ai.google.dev/terms).\n\n#multimodal\n\nNote: This model is currently experimental and not suitable for production use-cases, and may be heavily rate-limited.\n\n"},{"lang":"en","group":"models","slug":"models/gemini-flash-15-8b","frontmatter":{"title":"Google: Gemini 1.5 Flash-8B","meta_title":"Google: Gemini 1.5 Flash-8B","description":"Google: Gemini 1.5 Flash-8B","date":"2024-10-03T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text image 2 text"],"author":"google","tags":["Programming","Natural Language Processing","Chatbots","Translation","Technology/Web"],"draft":false,"id":"gemini-flash-1.5-8b","context":1000000,"input":3.75e-8,"output":1.5e-7,"img":0,"request":0,"last_updated":"2024-10-03T00:00:00.000Z","slug":"models/gemini-flash-15-8b"},"content":"\nGemini 1.5 Flash-8B is optimized for speed and efficiency, offering enhanced performance in small prompt tasks like chat, transcription, and translation. With reduced latency, it is highly effective for real-time and large-scale operations. This model focuses on cost-effective solutions while maintaining high-quality results.\n\n[Click here to learn more about this model](https://developers.googleblog.com/en/gemini-15-flash-8b-is-now-generally-available-for-use/).\n\nUsage of Gemini is subject to Google's [Gemini Terms of Use](https://ai.google.dev/terms).\n\n"},{"lang":"en","group":"models","slug":"models/gemini-flash-15","frontmatter":{"title":"Google: Gemini Flash 1.5","meta_title":"Google: Gemini Flash 1.5","description":"Google: Gemini Flash 1.5","date":"2024-05-14T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text image 2 text"],"author":"google","tags":["Programming","Machine Learning","Natural Language Processing","Computer Vision","Chatbots"],"draft":false,"id":"gemini-flash-1.5","context":1000000,"input":7.5e-8,"output":3e-7,"img":0.00004,"request":0,"last_updated":"2024-10-31T23:10:34.000Z","slug":"models/gemini-flash-15"},"content":"\nGemini 1.5 Flash is a foundation model that performs well at a variety of multimodal tasks such as visual understanding, classification, summarization, and creating content from image, audio and video. It's adept at processing visual and text inputs such as photographs, documents, infographics, and screenshots.\n\nGemini 1.5 Flash is designed for high-volume, high-frequency tasks where cost and latency matter. On most common tasks, Flash achieves comparable quality to other Gemini Pro models at a significantly reduced cost. Flash is well-suited for applications like chat assistants and on-demand content generation where speed and scale matter.\n\nUsage of Gemini is subject to Google's [Gemini Terms of Use](https://ai.google.dev/terms).\n\n#multimodal\n\n"},{"lang":"en","group":"models","slug":"models/gemini-pro-15","frontmatter":{"title":"Google: Gemini Pro 1.5","meta_title":"Google: Gemini Pro 1.5","description":"Google: Gemini Pro 1.5","date":"2024-04-09T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text image 2 text"],"author":"google","tags":["Programming","Natural Language Processing","Machine Learning","Generative AI","Chatbots"],"draft":false,"id":"gemini-pro-1.5","context":2000000,"input":0.00000125,"output":0.000005,"img":0.00263,"request":0,"last_updated":"2024-04-09T00:00:00.000Z","slug":"models/gemini-pro-15"},"content":"\nGoogle's latest multimodal model, supporting image and video in text or chat prompts.\n\nOptimized for language tasks including:\n\n- Code generation\n- Text generation\n- Text editing\n- Problem solving\n- Recommendations\n- Information extraction\n- Data extraction or generation\n- AI agents\n\nUsage of Gemini is subject to Google's [Gemini Terms of Use](https://ai.google.dev/terms).\n\n#multimodal\n\n"},{"lang":"en","group":"models","slug":"models/gemini-pro-vision","frontmatter":{"title":"Google: Gemini Pro Vision 1.0","meta_title":"Google: Gemini Pro Vision 1.0","description":"Google: Gemini Pro Vision 1.0","date":"2023-12-13T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text image 2 text"],"author":"google","tags":["Programming","Machine Learning","Natural Language Processing","Computer Vision","Generative AI"],"draft":false,"id":"gemini-pro-vision","context":16384,"input":5e-7,"output":0.0000015,"img":0.0025,"request":0,"last_updated":"2024-11-11T03:15:08.000Z","slug":"models/gemini-pro-vision"},"content":"\nGoogle's flagship multimodal model, supporting image and video in text or chat prompts for a text or code response.\n\nSee the benchmarks and prompting guidelines from [Deepmind](https://deepmind.google/technologies/gemini/).\n\nUsage of Gemini is subject to Google's [Gemini Terms of Use](https://ai.google.dev/terms).\n\n#multimodal\n\n"},{"lang":"en","group":"models","slug":"models/gemma-2-27b-it","frontmatter":{"title":"Google: Gemma 2 27B","meta_title":"Google: Gemma 2 27B","description":"Google: Gemma 2 27B","date":"2024-07-13T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text 2 text"],"author":"google","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"gemma-2-27b-it","context":8192,"input":2.7e-7,"output":2.7e-7,"img":0,"request":0,"last_updated":"2024-11-11T03:14:49.000Z","slug":"models/gemma-2-27b-it"},"content":"\nGemma 2 27B by Google is an open model built from the same research and technology used to create the [Gemini models](/models?q=gemini).\n\nGemma models are well-suited for a variety of text generation tasks, including question answering, summarization, and reasoning.\n\nSee the [launch announcement](https://blog.google/technology/developers/google-gemma-2/) for more details. Usage of Gemma is subject to Google's [Gemma Terms of Use](https://ai.google.dev/gemma/terms).\n\n"},{"lang":"en","group":"models","slug":"models/gemma-2-9b-it","frontmatter":{"title":"Google: Gemma 2 9B","meta_title":"Google: Gemma 2 9B","description":"Google: Gemma 2 9B","date":"2024-06-28T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text 2 text"],"author":"google","tags":["Programming","Natural Language Processing","Machine Learning","Data Science","Open Source"],"draft":false,"id":"gemma-2-9b-it","context":8192,"input":6e-8,"output":6e-8,"img":0,"request":0,"last_updated":"2024-11-11T03:14:49.000Z","slug":"models/gemma-2-9b-it"},"content":"\nGemma 2 9B by Google is an advanced, open-source language model that sets a new standard for efficiency and performance in its size class.\n\nDesigned for a wide variety of tasks, it empowers developers and researchers to build innovative applications, while maintaining accessibility, safety, and cost-effectiveness.\n\nSee the [launch announcement](https://blog.google/technology/developers/google-gemma-2/) for more details. Usage of Gemma is subject to Google's [Gemma Terms of Use](https://ai.google.dev/gemma/terms).\n\n"},{"lang":"en","group":"models","slug":"models/gpt-35-turbo-instruct","frontmatter":{"title":"OpenAI: GPT-3.5 Turbo Instruct","meta_title":"OpenAI: GPT-3.5 Turbo Instruct","description":"OpenAI: GPT-3.5 Turbo Instruct","date":"2023-09-28T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"openai","tags":["Programming","Natural Language Processing","Generative AI","Chatbots","Technology/Web"],"draft":false,"id":"gpt-3.5-turbo-instruct","context":4095,"input":0.0000015,"output":0.000002,"img":0,"request":0,"last_updated":"2023-09-28T00:00:00.000Z","slug":"models/gpt-35-turbo-instruct"},"content":"\nThis model is a variant of GPT-3.5 Turbo tuned for instructional prompts and omitting chat-related optimizations. Training data: up to Sep 2021.\n\n"},{"lang":"en","group":"models","slug":"models/gpt-4o-mini","frontmatter":{"title":"OpenAI: GPT-4o-mini","meta_title":"OpenAI: GPT-4o-mini","description":"OpenAI: GPT-4o-mini","date":"2024-07-18T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text image 2 text"],"author":"openai","tags":["Programming","Technology","Programming/Scripting","Technology/Web"],"draft":false,"is_recommended":true,"id":"gpt-4o-mini","context":128000,"input":1.5e-7,"output":6e-7,"img":0.007225,"request":0,"last_updated":"2024-11-14T05:09:26.000Z","slug":"models/gpt-4o-mini"},"content":"\nGPT-4o mini is OpenAI's newest model after [GPT-4 Omni](/openai/gpt-4o), supporting both text and image inputs with text outputs.\n\nAs their most advanced small model, it is many multiples more affordable than other recent frontier models, and more than 60% cheaper than [GPT-3.5 Turbo](/openai/gpt-3.5-turbo). It maintains SOTA intelligence, while being significantly more cost-effective.\n\nGPT-4o mini achieves an 82% score on MMLU and presently ranks higher than GPT-4 on chat preferences [common leaderboards](https://arena.lmsys.org/).\n\nCheck out the [launch announcement](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/) to learn more.\n\n"},{"lang":"en","group":"models","slug":"models/gpt-4o","frontmatter":{"title":"OpenAI: GPT-4o","meta_title":"OpenAI: GPT-4o","description":"OpenAI: GPT-4o","date":"2024-05-13T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text image 2 text"],"author":"openai","tags":["Programming","Natural Language Processing","Machine Learning","Generative AI","Computer Vision"],"draft":false,"id":"gpt-4o","context":128000,"input":0.0000025,"output":0.00001,"img":0.0036125,"request":0,"last_updated":"2024-05-13T00:00:00.000Z","slug":"models/gpt-4o"},"content":"\nGPT-4o (\"o\" for \"omni\") is OpenAI's latest AI model, supporting both text and image inputs with text outputs. It maintains the intelligence level of [GPT-4 Turbo](/openai/gpt-4-turbo) while being twice as fast and 50% more cost-effective. GPT-4o also offers improved performance in processing non-English languages and enhanced visual capabilities.\n\nFor benchmarking against other models, it was briefly called [\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209)\n\n"},{"lang":"en","group":"models","slug":"models/grok-beta","frontmatter":{"title":"xAI: Grok Beta","meta_title":"xAI: Grok Beta","description":"xAI: Grok Beta","date":"2024-10-20T00:00:00.000Z","image":"https://img.rifx.online/logo/xai.svg","categories":["text 2 text"],"author":"x-ai","tags":["Natural Language Processing","Machine Learning","Generative AI","Chatbots","Data Science"],"draft":false,"id":"grok-beta","context":131072,"input":0.000005,"output":0.000015,"img":0,"request":0,"last_updated":"2024-11-07T09:32:49.000Z","slug":"models/grok-beta"},"content":"\nGrok Beta is xAI's experimental language model with state-of-the-art reasoning capabilities, best for complex and multi-step use cases.\n\nIt is the successor of [Grok 2](https://x.ai/blog/grok-2) with enhanced context length.\n\n"},{"lang":"en","group":"models","slug":"models/hermes-3-llama-31-405b","frontmatter":{"title":"Nous: Hermes 3 405B Instruct","meta_title":"Nous: Hermes 3 405B Instruct","description":"Nous: Hermes 3 405B Instruct","date":"2024-08-16T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"nousresearch","tags":["Programming","Natural Language Processing","Machine Learning","Generative AI","Chatbots"],"draft":false,"id":"hermes-3-llama-3.1-405b","context":131072,"input":0.00000179,"output":0.00000249,"img":0,"request":0,"last_updated":"2024-11-11T03:16:40.000Z","slug":"models/hermes-3-llama-31-405b"},"content":"\nHermes 3 is a generalist language model with many improvements over Hermes 2, including advanced agentic capabilities, much better roleplaying, reasoning, multi-turn conversation, long context coherence, and improvements across the board.\n\nHermes 3 405B is a frontier-level, full-parameter finetune of the Llama-3.1 405B foundation model, focused on aligning LLMs to the user, with powerful steering capabilities and control given to the end user.\n\nThe Hermes 3 series builds and expands on the Hermes 2 set of capabilities, including more powerful and reliable function calling and structured output capabilities, generalist assistant capabilities, and improved code generation skills.\n\nHermes 3 is competitive, if not superior, to Llama-3.1 Instruct models at general capabilities, with varying strengths and weaknesses attributable between the two.\n\n"},{"lang":"en","group":"models","slug":"models/hermes-3-llama-31-70b","frontmatter":{"title":"Nous: Hermes 3 70B Instruct","meta_title":"Nous: Hermes 3 70B Instruct","description":"Nous: Hermes 3 70B Instruct","date":"2024-08-18T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"nousresearch","tags":["Natural Language Processing","Machine Learning","Generative AI","Chatbots","Programming"],"draft":false,"id":"hermes-3-llama-3.1-70b","context":131072,"input":4e-7,"output":4e-7,"img":0,"request":0,"last_updated":"2024-11-11T03:16:38.000Z","slug":"models/hermes-3-llama-31-70b"},"content":"\nHermes 3 is a generalist language model with many improvements over [Hermes 2](/nousresearch/nous-hermes-2-mistral-7b-dpo), including advanced agentic capabilities, much better roleplaying, reasoning, multi-turn conversation, long context coherence, and improvements across the board.\n\nHermes 3 70B is a competitive, if not superior finetune of the [Llama-3.1 70B foundation model](/meta-llama/llama-3.1-70b-instruct), focused on aligning LLMs to the user, with powerful steering capabilities and control given to the end user.\n\nThe Hermes 3 series builds and expands on the Hermes 2 set of capabilities, including more powerful and reliable function calling and structured output capabilities, generalist assistant capabilities, and improved code generation skills.\n\n"},{"lang":"en","group":"models","slug":"models/inflection-3-pi","frontmatter":{"title":"Inflection: Inflection 3 Pi","meta_title":"Inflection: Inflection 3 Pi","description":"Inflection: Inflection 3 Pi","date":"2024-10-11T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"inflection","tags":["Chatbots","Roleplay","Emotional Intelligence","Customer Support","Safety"],"draft":false,"id":"inflection-3-pi","context":8000,"input":0.0000025,"output":0.00001,"img":0,"request":0,"last_updated":"2024-11-07T10:11:48.000Z","slug":"models/inflection-3-pi"},"content":"\nInflection 3 Pi powers Inflection's [Pi](https://pi.ai) chatbot, including backstory, emotional intelligence, productivity, and safety. It excels in scenarios like customer support, roleplay, and emotional intelligence.\n\n"},{"lang":"en","group":"models","slug":"models/inflection-3-productivity","frontmatter":{"title":"Inflection: Inflection 3 Productivity","meta_title":"Inflection: Inflection 3 Productivity","description":"Inflection: Inflection 3 Productivity","date":"2024-10-11T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"inflection","tags":["Programming","Technology","Chatbots","Generative AI","Data Science"],"draft":false,"id":"inflection-3-productivity","context":8000,"input":0.0000025,"output":0.00001,"img":0,"request":0,"last_updated":"2024-11-07T10:11:56.000Z","slug":"models/inflection-3-productivity"},"content":"\nInflection 3 Productivity is optimized for following instructions. It is better for tasks requiring JSON output or precise adherence to provided guidelines\n\nFor emotional intelligence similar to Pi, see [Inflect 3 Pi](/inflection/inflection-3-pi)\n\nSee [Inflection's announcement](https://inflection.ai/blog/enterprise) for more details.\n\n"},{"lang":"en","group":"models","slug":"models/jamba-1-5-large","frontmatter":{"title":"AI21: Jamba 1.5 Large","meta_title":"AI21: Jamba 1.5 Large","description":"AI21: Jamba 1.5 Large","date":"2024-08-23T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"ai21","tags":["Programming","Technology","Machine Learning","Data Science","Generative AI"],"draft":false,"id":"jamba-1-5-large","context":256000,"input":0.000002,"output":0.000008,"img":0,"request":0,"last_updated":"2024-11-11T03:11:21.000Z","slug":"models/jamba-1-5-large"},"content":"\nJamba 1.5 Large is part of AI21's new family of open models, offering superior speed, efficiency, and quality.\n\nIt features a 256K effective context window, the longest among open models, enabling improved performance on tasks like document summarization and analysis.\n\nBuilt on a novel SSM-Transformer architecture, it outperforms larger models like Llama 3.1 70B on benchmarks while maintaining resource efficiency.\n\nRead their [announcement](https://www.ai21.com/blog/announcing-jamba-model-family) to learn more.\n\n"},{"lang":"en","group":"models","slug":"models/jamba-1-5-mini","frontmatter":{"title":"AI21: Jamba 1.5 Mini","meta_title":"AI21: Jamba 1.5 Mini","description":"AI21: Jamba 1.5 Mini","date":"2024-08-23T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"ai21","tags":["Programming","Technology","Machine Learning","Natural Language Processing","Data Science"],"draft":false,"id":"jamba-1-5-mini","context":256000,"input":2e-7,"output":4e-7,"img":0,"request":0,"last_updated":"2024-11-11T03:11:12.000Z","slug":"models/jamba-1-5-mini"},"content":"\nJamba 1.5 Mini is the world's first production-grade Mamba-based model, combining SSM and Transformer architectures for a 256K context window and high efficiency.\n\nIt works with 9 languages and can handle various writing and analysis tasks as well as or better than similar small models.\n\nThis model uses less computer memory and works faster with longer texts than previous designs.\n\nRead their [announcement](https://www.ai21.com/blog/announcing-jamba-model-family) to learn more.\n\n"},{"lang":"en","group":"models","slug":"models/l3-lunaris-8b","frontmatter":{"title":"Llama 3 8B Lunaris","meta_title":"Llama 3 8B Lunaris","description":"Llama 3 8B Lunaris","date":"2024-08-13T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"sao10k","tags":["Roleplay","Programming","Machine Learning","Natural Language Processing","Chatbots"],"draft":false,"id":"l3-lunaris-8b","context":8192,"input":0.000002,"output":0.000002,"img":0,"request":0,"last_updated":"2024-11-11T03:12:13.000Z","slug":"models/l3-lunaris-8b"},"content":"\nLunaris 8B is a versatile generalist and roleplaying model based on Llama 3. It's a strategic merge of multiple models, designed to balance creativity with improved logic and general knowledge.\n\nCreated by [Sao10k](https://huggingface.co/Sao10k), this model aims to offer an improved experience over Stheno v3.2, with enhanced creativity and logical reasoning.\n\nFor best results, use with Llama 3 Instruct context template, temperature 1.4, and min_p 0.1.\n\n"},{"lang":"en","group":"models","slug":"models/l31-euryale-70b","frontmatter":{"title":"Llama 3.1 Euryale 70B v2.2","meta_title":"Llama 3.1 Euryale 70B v2.2","description":"Llama 3.1 Euryale 70B v2.2","date":"2024-08-28T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"sao10k","tags":["Roleplay","Generative AI","Chatbots","Natural Language Processing","Technology/Web"],"draft":false,"id":"l3.1-euryale-70b","context":8192,"input":3.5e-7,"output":4e-7,"img":0,"request":0,"last_updated":"2024-11-11T03:12:50.000Z","slug":"models/l31-euryale-70b"},"content":"\nEuryale L3.1 70B v2.2 is a model focused on creative roleplay from [Sao10k](https://ko-fi.com/sao10k). It is the successor of [Euryale L3 70B v2.1](/sao10k/l3-euryale-70b).\n\n"},{"lang":"en","group":"models","slug":"models/lfm-40b","frontmatter":{"title":"Liquid: LFM 40B MoE","meta_title":"Liquid: LFM 40B MoE","description":"Liquid: LFM 40B MoE","date":"2024-09-30T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"liquid","tags":["Machine Learning","Natural Language Processing","Data Science","Generative AI","Computer Vision"],"draft":false,"is_recommended":true,"id":"lfm-40b","context":32768,"input":0.000001,"output":0.000002,"img":0,"request":0,"last_updated":"2024-11-14T05:10:16.000Z","slug":"models/lfm-40b"},"content":"\nLiquid's 40.3B Mixture of Experts (MoE) model. Liquid Foundation Models (LFMs) are large neural networks built with computational units rooted in dynamic systems.\n\nLFMs are general-purpose AI models that can be used to model any kind of sequential data, including video, audio, text, time series, and signals.\n\nSee the [launch announcement](https://www.liquid.ai/liquid-foundation-models) for benchmarks and more info.\n\n"},{"lang":"en","group":"models","slug":"models/liquid-lfm-40b:free","frontmatter":{"title":"Liquid: LFM 40B MoE (free)","meta_title":"Liquid: LFM 40B MoE (free)","description":"Liquid: LFM 40B MoE (free)","date":"2024-09-30T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"liquid","tags":["Generative AI","Machine Learning","Natural Language Processing","Data Science","Technology/Web"],"draft":false,"id":"liquid/lfm-40b:free","context":8192,"input":0,"output":0,"img":0,"request":0,"last_updated":"2024-11-07T00:17:57.000Z","slug":"models/liquid-lfm-40b:free"},"content":"\nLiquid's 40.3B Mixture of Experts (MoE) model. Liquid Foundation Models (LFMs) are large neural networks built with computational units rooted in dynamic systems.\n\nLFMs are general-purpose AI models that can be used to model any kind of sequential data, including video, audio, text, time series, and signals.\n\nSee the [launch announcement](https://www.liquid.ai/liquid-foundation-models) for benchmarks and more info.\n\n_These are free, rate-limited endpoints for [LFM 40B MoE](/liquid/lfm-40b). Outputs may be cached. Read about rate limits [here](/docs/limits)._\n\n"},{"lang":"en","group":"models","slug":"models/llama-31-70b-instruct","frontmatter":{"title":"Meta: Llama 3.1 70B Instruct","meta_title":"Meta: Llama 3.1 70B Instruct","description":"Meta: Llama 3.1 70B Instruct","date":"2024-07-23T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text 2 text"],"author":"meta-llama","tags":["Programming","Machine Learning","Natural Language Processing","Chatbots","Ethics"],"draft":false,"id":"llama-3.1-70b-instruct","context":131072,"input":3e-7,"output":3e-7,"img":0,"request":0,"last_updated":"2024-10-28T13:38:49.000Z","slug":"models/llama-31-70b-instruct"},"content":"\nMeta's latest class of model (Llama 3.1) launched with a variety of sizes & flavors. This 70B instruct-tuned version is optimized for high quality dialogue usecases.\n\nIt has demonstrated strong performance compared to leading closed-source models in human evaluations.\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).\n\n"},{"lang":"en","group":"models","slug":"models/llama-31-8b-instruct","frontmatter":{"title":"Meta: Llama 3.1 8B Instruct","meta_title":"Meta: Llama 3.1 8B Instruct","description":"Meta: Llama 3.1 8B Instruct","date":"2024-07-23T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text 2 text"],"author":"meta-llama","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Ethics"],"draft":false,"id":"llama-3.1-8b-instruct","context":131072,"input":5.5e-8,"output":5.5e-8,"img":0,"request":0,"last_updated":"2024-10-31T23:27:09.000Z","slug":"models/llama-31-8b-instruct"},"content":"\nMeta's latest class of model (Llama 3.1) launched with a variety of sizes & flavors. This 8B instruct-tuned version is fast and efficient.\n\nIt has demonstrated strong performance compared to leading closed-source models in human evaluations.\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).\n\n"},{"lang":"en","group":"models","slug":"models/llama-31-lumimaid-70b","frontmatter":{"title":"Lumimaid v0.2 70B","meta_title":"Lumimaid v0.2 70B","description":"Lumimaid v0.2 70B","date":"2024-10-22T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"neversleep","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Ethics"],"draft":false,"id":"llama-3.1-lumimaid-70b","context":131072,"input":0.000003375,"output":0.0000045,"img":0,"request":0,"last_updated":"2024-11-11T03:03:31.000Z","slug":"models/llama-31-lumimaid-70b"},"content":"\nLumimaid v0.2 70B is a finetune of [Llama 3.1 70B](/meta-llama/llama-3.1-70b-instruct) with a \"HUGE step up dataset wise\" compared to Lumimaid v0.1. Sloppy chats output were purged.\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/).\n\n"},{"lang":"en","group":"models","slug":"models/llama-31-lumimaid-8b","frontmatter":{"title":"Lumimaid v0.2 8B","meta_title":"Lumimaid v0.2 8B","description":"Lumimaid v0.2 8B","date":"2024-09-15T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"neversleep","tags":["Programming","Machine Learning","Natural Language Processing","Chatbots","Ethics"],"draft":false,"id":"llama-3.1-lumimaid-8b","context":131072,"input":1.875e-7,"output":0.000001125,"img":0,"request":0,"last_updated":"2024-11-11T03:10:19.000Z","slug":"models/llama-31-lumimaid-8b"},"content":"\nLumimaid v0.2 8B is a finetune of [Llama 3.1 8B](/meta-llama/llama-3.1-8b-instruct) with a \"HUGE step up dataset wise\" compared to Lumimaid v0.1. Sloppy chats output were purged.\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/).\n\n"},{"lang":"en","group":"models","slug":"models/llama-31-nemotron-70b-instruct","frontmatter":{"title":"Nvidia: Llama 3.1 Nemotron 70B Instruct","meta_title":"Nvidia: Llama 3.1 Nemotron 70B Instruct","description":"Nvidia: Llama 3.1 Nemotron 70B Instruct","date":"2024-10-15T00:00:00.000Z","image":"https://img.rifx.online/logo/nvidia.svg","categories":["text 2 text"],"author":"nvidia","tags":["Programming","Natural Language Processing","Machine Learning","Generative AI","Ethics"],"draft":false,"id":"llama-3.1-nemotron-70b-instruct","context":131072,"input":3.5e-7,"output":4e-7,"img":0,"request":0,"last_updated":"2024-10-15T00:00:00.000Z","slug":"models/llama-31-nemotron-70b-instruct"},"content":"\nNVIDIA's Llama 3.1 Nemotron 70B is a language model designed for generating precise and useful responses. Leveraging [Llama 3.1 70B](/models/meta-llama/llama-3.1-70b-instruct) architecture and Reinforcement Learning from Human Feedback (RLHF), it excels in automatic alignment benchmarks. This model is tailored for applications requiring high accuracy in helpfulness and response generation, suitable for diverse user queries across multiple domains.\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).\n\n"},{"lang":"en","group":"models","slug":"models/llama-31-sonar-huge-128k-online","frontmatter":{"title":"Perplexity: Llama 3.1 Sonar 405B Online","meta_title":"Perplexity: Llama 3.1 Sonar 405B Online","description":"Perplexity: Llama 3.1 Sonar 405B Online","date":"2024-08-14T00:00:00.000Z","image":"https://img.rifx.online/logo/perplexity.svg","categories":["text 2 text"],"author":"perplexity","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"llama-3.1-sonar-huge-128k-online","context":127072,"input":0.000005,"output":0.000005,"img":0,"request":0.005,"last_updated":"2024-11-07T09:36:38.000Z","slug":"models/llama-31-sonar-huge-128k-online"},"content":"\nLlama 3.1 Sonar is Perplexity's latest model family. It surpasses their earlier Sonar models in cost-efficiency, speed, and performance. The model is built upon the Llama 3.1 405B and has internet access.\n\n"},{"lang":"en","group":"models","slug":"models/llama-31-sonar-large-128k-online","frontmatter":{"title":"Perplexity: Llama 3.1 Sonar 70B Online","meta_title":"Perplexity: Llama 3.1 Sonar 70B Online","description":"Perplexity: Llama 3.1 Sonar 70B Online","date":"2024-08-01T00:00:00.000Z","image":"https://img.rifx.online/logo/perplexity.svg","categories":["text 2 text"],"author":"perplexity","tags":["Programming","Machine Learning","Natural Language Processing","Chatbots","Generative AI"],"draft":false,"id":"llama-3.1-sonar-large-128k-online","context":127072,"input":0.000001,"output":0.000001,"img":0,"request":0.005,"last_updated":"2024-11-07T09:37:21.000Z","slug":"models/llama-31-sonar-large-128k-online"},"content":"\nLlama 3.1 Sonar is Perplexity's latest model family. It surpasses their earlier Sonar models in cost-efficiency, speed, and performance.\n\nThis is the online version of the [offline chat model](/perplexity/llama-3.1-sonar-large-128k-chat). It is focused on delivering helpful, up-to-date, and factual responses. #online\n\n"},{"lang":"en","group":"models","slug":"models/llama-31-sonar-small-128k-online","frontmatter":{"title":"Perplexity: Llama 3.1 Sonar 8B Online","meta_title":"Perplexity: Llama 3.1 Sonar 8B Online","description":"Perplexity: Llama 3.1 Sonar 8B Online","date":"2024-08-01T00:00:00.000Z","image":"https://img.rifx.online/logo/perplexity.svg","categories":["text 2 text"],"author":"perplexity","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"llama-3.1-sonar-small-128k-online","context":127072,"input":2e-7,"output":2e-7,"img":0,"request":0.005,"last_updated":"2024-11-07T09:38:09.000Z","slug":"models/llama-31-sonar-small-128k-online"},"content":"\nLlama 3.1 Sonar is Perplexity's latest model family. It surpasses their earlier Sonar models in cost-efficiency, speed, and performance.\n\nThis is the online version of the [offline chat model](/perplexity/llama-3.1-sonar-small-128k-chat). It is focused on delivering helpful, up-to-date, and factual responses. #online\n\n"},{"lang":"en","group":"models","slug":"models/llama-32-11b-vision-instruct","frontmatter":{"title":"Meta: Llama 3.2 11B Vision Instruct","meta_title":"Meta: Llama 3.2 11B Vision Instruct","description":"Meta: Llama 3.2 11B Vision Instruct","date":"2024-09-25T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text image 2 text"],"author":"meta-llama","tags":["Natural Language Processing","Computer Vision","Machine Learning","Generative AI","Data Science"],"draft":false,"is_recommended":true,"id":"llama-3.2-11b-vision-instruct","context":131072,"input":5.5e-8,"output":5.5e-8,"img":0.000079475,"request":0,"last_updated":"2024-11-14T05:10:41.000Z","slug":"models/llama-32-11b-vision-instruct"},"content":"\nLlama 3.2 11B Vision is a multimodal model with 11 billion parameters, designed to handle tasks combining visual and textual data. It excels in tasks such as image captioning and visual question answering, bridging the gap between language generation and visual reasoning. Pre-trained on a massive dataset of image-text pairs, it performs well in complex, high-accuracy image analysis.\n\nIts ability to integrate visual understanding with language processing makes it an ideal solution for industries requiring comprehensive visual-linguistic AI applications, such as content creation, AI-driven customer service, and research.\n\nClick here for the [original model card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md).\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).\n\n"},{"lang":"en","group":"models","slug":"models/llama-32-1b-instruct","frontmatter":{"title":"Meta: Llama 3.2 1B Instruct","meta_title":"Meta: Llama 3.2 1B Instruct","description":"Meta: Llama 3.2 1B Instruct","date":"2024-09-25T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text 2 text"],"author":"meta-llama","tags":["Natural Language Processing","Programming","Technology","Machine Learning","Generative AI"],"draft":false,"id":"llama-3.2-1b-instruct","context":131072,"input":1e-8,"output":2e-8,"img":0,"request":0,"last_updated":"2024-09-25T00:00:00.000Z","slug":"models/llama-32-1b-instruct"},"content":"\nLlama 3.2 1B is a 1-billion-parameter language model focused on efficiently performing natural language tasks, such as summarization, dialogue, and multilingual text analysis. Its smaller size allows it to operate efficiently in low-resource environments while maintaining strong task performance.\n\nSupporting eight core languages and fine-tunable for more, Llama 1.3B is ideal for businesses or developers seeking lightweight yet powerful AI solutions that can operate in diverse multilingual settings without the high computational demand of larger models.\n\nClick here for the [original model card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md).\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).\n\n"},{"lang":"en","group":"models","slug":"models/llama-32-3b-instruct","frontmatter":{"title":"Meta: Llama 3.2 3B Instruct","meta_title":"Meta: Llama 3.2 3B Instruct","description":"Meta: Llama 3.2 3B Instruct","date":"2024-09-25T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text 2 text"],"author":"meta-llama","tags":["Natural Language Processing","Machine Learning","Generative AI","Chatbots","Multilingual"],"draft":false,"id":"llama-3.2-3b-instruct","context":131072,"input":3e-8,"output":5e-8,"img":0,"request":0,"last_updated":"2024-11-11T03:09:59.000Z","slug":"models/llama-32-3b-instruct"},"content":"\nLlama 3.2 3B is a 3-billion-parameter multilingual large language model, optimized for advanced natural language processing tasks like dialogue generation, reasoning, and summarization. Designed with the latest transformer architecture, it supports eight languages, including English, Spanish, and Hindi, and is adaptable for additional languages.\n\nTrained on 9 trillion tokens, the Llama 3.2B model excels in instruction-following, complex reasoning, and tool use. Its balanced performance makes it ideal for applications needing accuracy and efficiency in text generation across multilingual settings.\n\nClick here for the [original model card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md).\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).\n\n"},{"lang":"en","group":"models","slug":"models/llama-32-90b-vision-instruct","frontmatter":{"title":"Meta: Llama 3.2 90B Vision Instruct","meta_title":"Meta: Llama 3.2 90B Vision Instruct","description":"Meta: Llama 3.2 90B Vision Instruct","date":"2024-09-25T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text image 2 text"],"author":"meta-llama","tags":["Natural Language Processing","Computer Vision","Machine Learning","Data Science","Generative AI"],"draft":false,"id":"llama-3.2-90b-vision-instruct","context":131072,"input":3.5e-7,"output":4e-7,"img":0.00050575,"request":0,"last_updated":"2024-09-25T00:00:00.000Z","slug":"models/llama-32-90b-vision-instruct"},"content":"\nThe Llama 90B Vision model is a top-tier, 90-billion-parameter multimodal model designed for the most challenging visual reasoning and language tasks. It offers unparalleled accuracy in image captioning, visual question answering, and advanced image-text comprehension. Pre-trained on vast multimodal datasets and fine-tuned with human feedback, the Llama 90B Vision is engineered to handle the most demanding image-based AI tasks.\n\nThis model is perfect for industries requiring cutting-edge multimodal AI capabilities, particularly those dealing with complex, real-time visual and textual analysis.\n\nClick here for the [original model card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md).\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).\n\n"},{"lang":"en","group":"models","slug":"models/lzlv-70b-fp16-hf","frontmatter":{"title":"lzlv 70B","meta_title":"lzlv 70B","description":"lzlv 70B","date":"2023-11-12T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"lizpreciatior","tags":["Roleplay","Programming","Machine Learning","Generative AI","Chatbots"],"draft":false,"id":"lzlv-70b-fp16-hf","context":4096,"input":3.5e-7,"output":4e-7,"img":0,"request":0,"last_updated":"2024-11-04T12:50:34.000Z","slug":"models/lzlv-70b-fp16-hf"},"content":"\nA Mythomax/MLewd_13B-style merge of selected 70B models.\nA multi-model merge of several LLaMA2 70B finetunes for roleplaying and creative work. The goal was to create a model that combines creativity with intelligence for an enhanced experience.\n\n#merge #uncensored\n\n"},{"lang":"en","group":"models","slug":"models/magnum-v2-72b","frontmatter":{"title":"Magnum v2 72B","meta_title":"Magnum v2 72B","description":"Magnum v2 72B","date":"2024-09-30T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"anthracite-org","tags":["Roleplay","Programming","Machine Learning","Natural Language Processing","Generative AI"],"draft":false,"id":"magnum-v2-72b","context":32768,"input":0.00000375,"output":0.0000045,"img":0,"request":0,"last_updated":"2024-11-11T03:09:19.000Z","slug":"models/magnum-v2-72b"},"content":"\nFrom the maker of [Goliath](https://openrouter.ai/alpindale/goliath-120b), Magnum 72B is the seventh in a family of models designed to achieve the prose quality of the Claude 3 models, notably Opus & Sonnet.\n\nThe model is based on [Qwen2 72B](https://openrouter.ai/qwen/qwen-2-72b-instruct) and trained with 55 million tokens of highly curated roleplay (RP) data.\n\n"},{"lang":"en","group":"models","slug":"models/magnum-v4-72b","frontmatter":{"title":"Magnum v4 72B","meta_title":"Magnum v4 72B","description":"Magnum v4 72B","date":"2024-10-22T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"anthracite-org","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"magnum-v4-72b","context":32768,"input":0.000001875,"output":0.00000225,"img":0,"request":0,"last_updated":"2024-11-04T12:39:55.000Z","slug":"models/magnum-v4-72b"},"content":"\nThis is a series of models designed to replicate the prose quality of the Claude 3 models, specifically Sonnet and Opus.\n\nThe model is fine-tuned on top of [Qwen2.5 72B].\n\n"},{"lang":"en","group":"models","slug":"models/ministral-3b","frontmatter":{"title":"Ministral 3B","meta_title":"Ministral 3B","description":"Ministral 3B","date":"2024-10-17T00:00:00.000Z","image":"https://img.rifx.online/logo/mistral.png","categories":["text 2 text"],"author":"mistralai","tags":["Programming","Machine Learning","Natural Language Processing","Data Science","Generative AI"],"draft":false,"id":"ministral-3b","context":128000,"input":4e-8,"output":4e-8,"img":0,"request":0,"last_updated":"2024-11-07T00:24:37.000Z","slug":"models/ministral-3b"},"content":"\nMinistral 3B is a 3B parameter model optimized for on-device and edge computing. It excels in knowledge, commonsense reasoning, and function-calling, outperforming larger models like Mistral 7B on most benchmarks. Supporting up to 128k context length, it’s ideal for orchestrating agentic workflows and specialist tasks with efficient inference.\n\n"},{"lang":"en","group":"models","slug":"models/ministral-8b","frontmatter":{"title":"Ministral 8B","meta_title":"Ministral 8B","description":"Ministral 8B","date":"2024-10-17T00:00:00.000Z","image":"https://img.rifx.online/logo/mistral.png","categories":["text 2 text"],"author":"mistralai","tags":["Technology","Machine Learning","Data Science","Generative AI","Ethics"],"draft":false,"id":"ministral-8b","context":128000,"input":1e-7,"output":1e-7,"img":0,"request":0,"last_updated":"2024-10-19T04:54:11.000Z","slug":"models/ministral-8b"},"content":"\nMinistral 8B is an 8B parameter model featuring a unique interleaved sliding-window attention pattern for faster, memory-efficient inference. Designed for edge use cases, it supports up to 128k context length and excels in knowledge and reasoning tasks. It outperforms peers in the sub-10B category, making it perfect for low-latency, privacy-first applications.\n\n"},{"lang":"en","group":"models","slug":"models/mistral-7b-instruct","frontmatter":{"title":"Mistral: Mistral 7B Instruct","meta_title":"Mistral: Mistral 7B Instruct","description":"Mistral: Mistral 7B Instruct","date":"2024-05-27T00:00:00.000Z","image":"https://img.rifx.online/logo/mistral.png","categories":["text 2 text"],"author":"mistralai","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"mistral-7b-instruct","context":32768,"input":5.5e-8,"output":5.5e-8,"img":0,"request":0,"last_updated":"2024-10-31T23:13:12.000Z","slug":"models/mistral-7b-instruct"},"content":"\nA high-performing, industry-standard 7.3B parameter model, with optimizations for speed and context length.\n\n*Mistral 7B Instruct has multiple version variants, and this is intended to be the latest version.*\n\n"},{"lang":"en","group":"models","slug":"models/mistral-nemo","frontmatter":{"title":"Mistral: Mistral Nemo","meta_title":"Mistral: Mistral Nemo","description":"Mistral: Mistral Nemo","date":"2024-07-19T00:00:00.000Z","image":"https://img.rifx.online/logo/mistral.png","categories":["text 2 text"],"author":"mistralai","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Data Science"],"draft":false,"id":"mistral-nemo","context":128000,"input":1.3e-7,"output":1.3e-7,"img":0,"request":0,"last_updated":"2024-10-31T23:10:58.000Z","slug":"models/mistral-nemo"},"content":"\nA 12B parameter model with a 128k token context length built by Mistral in collaboration with NVIDIA.\n\nThe model is multilingual, supporting English, French, German, Spanish, Italian, Portuguese, Chinese, Japanese, Korean, Arabic, and Hindi.\n\nIt supports function calling and is released under the Apache 2.0 license.\n\n"},{"lang":"en","group":"models","slug":"models/mistral-tiny","frontmatter":{"title":"Mistral Tiny","meta_title":"Mistral Tiny","description":"Mistral Tiny","date":"2024-01-10T00:00:00.000Z","image":"https://img.rifx.online/logo/mistral.png","categories":["text 2 text"],"author":"mistralai","tags":["Programming","Machine Learning","Data Science","Generative AI","Chatbots"],"draft":false,"id":"mistral-tiny","context":32000,"input":2.5e-7,"output":2.5e-7,"img":0,"request":0,"last_updated":"2024-10-31T23:12:22.000Z","slug":"models/mistral-tiny"},"content":"\nThis model is currently powered by Mistral-7B-v0.2, and incorporates a \"better\" fine-tuning than [Mistral 7B](/mistralai/mistral-7b-instruct-v0.1), inspired by community work. It's best used for large batch processing tasks where cost is a significant factor but reasoning capabilities are not crucial.\n\n"},{"lang":"en","group":"models","slug":"models/mn-inferor-12b","frontmatter":{"title":"Mistral Nemo Inferor 12B","meta_title":"Mistral Nemo Inferor 12B","description":"Mistral Nemo Inferor 12B","date":"2024-11-13T02:20:28.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"infermatic","tags":["Roleplay","Programming","Machine Learning","Natural Language Processing","Generative AI"],"draft":false,"id":"mn-inferor-12b","context":32000,"input":2.5e-7,"output":5e-7,"img":0,"request":0,"last_updated":"2024-11-14T02:10:35.000Z","slug":"models/mn-inferor-12b"},"content":"\nInferor is a merge of top roleplay models, expert on immersive narratives and storytelling.\n\nThis model was merged using the [Model Stock](https://arxiv.org/abs/2403.19522) merge method using [anthracite-org/magnum-v4-12b](https://openrouter.ai/anthracite-org/magnum-v4-72b) as a base.\n\n\n"},{"lang":"en","group":"models","slug":"models/mn-starcannon-12b","frontmatter":{"title":"Mistral Nemo 12B Starcannon","meta_title":"Mistral Nemo 12B Starcannon","description":"Mistral Nemo 12B Starcannon","date":"2024-08-13T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"aetherwiing","tags":["Roleplay","Programming","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"mn-starcannon-12b","context":12000,"input":0.000002,"output":0.000002,"img":0,"request":0,"last_updated":"2024-11-11T03:16:53.000Z","slug":"models/mn-starcannon-12b"},"content":"\nStarcannon 12B is a creative roleplay and story writing model, using [nothingiisreal/mn-celeste-12b](https://openrouter.ai/nothingiisreal/mn-celeste-12b) as a base and [intervitens/mini-magnum-12b-v1.1](https://huggingface.co/intervitens/mini-magnum-12b-v1.1) merged in using the [TIES](https://arxiv.org/abs/2306.01708) method.\n\nAlthough more similar to Magnum overall, the model remains very creative, with a pleasant writing style. It is recommended for people wanting more variety than Magnum, and yet more verbose prose than Celeste.\n\n"},{"lang":"en","group":"models","slug":"models/mythomax-l2-13b","frontmatter":{"title":"MythoMax 13B","meta_title":"MythoMax 13B","description":"MythoMax 13B","date":"2023-07-02T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"gryphe","tags":["Roleplay","Programming","Machine Learning","Natural Language Processing","Generative AI"],"draft":false,"id":"mythomax-l2-13b","context":4096,"input":1e-7,"output":1e-7,"img":0,"request":0,"last_updated":"2024-10-28T13:10:41.000Z","slug":"models/mythomax-l2-13b"},"content":"\nOne of the highest performing and most popular fine-tunes of Llama 2 13B, with rich descriptions and roleplay. #merge\n\n"},{"lang":"en","group":"models","slug":"models/o1-mini","frontmatter":{"title":"OpenAI: o1-mini","meta_title":"OpenAI: o1-mini","description":"OpenAI: o1-mini","date":"2024-09-12T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"openai","tags":["Programming","Science","Natural Language Processing","Machine Learning","Data Science"],"draft":false,"id":"o1-mini","context":128000,"input":0.000003,"output":0.000012,"img":0,"request":0,"last_updated":"2024-09-12T00:00:00.000Z","slug":"models/o1-mini"},"content":"\nThe latest and strongest model family from OpenAI, o1 is designed to spend more time thinking before responding.\n\nThe o1 models are optimized for math, science, programming, and other STEM-related tasks. They consistently exhibit PhD-level accuracy on benchmarks in physics, chemistry, and biology. Learn more in the [launch announcement](https://openai.com/o1).\n\nNote: This model is currently experimental and not suitable for production use-cases, and may be heavily rate-limited.\n\n"},{"lang":"en","group":"models","slug":"models/o1-preview","frontmatter":{"title":"OpenAI: o1-preview","meta_title":"OpenAI: o1-preview","description":"OpenAI: o1-preview","date":"2024-09-12T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"openai","tags":["Programming","Science","Natural Language Processing","Machine Learning","Data Science"],"draft":false,"id":"o1-preview","context":128000,"input":0.000015,"output":0.00006,"img":0,"request":0,"last_updated":"2024-09-12T00:00:00.000Z","slug":"models/o1-preview"},"content":"\nThe latest and strongest model family from OpenAI, o1 is designed to spend more time thinking before responding.\n\nThe o1 models are optimized for math, science, programming, and other STEM-related tasks. They consistently exhibit PhD-level accuracy on benchmarks in physics, chemistry, and biology. Learn more in the [launch announcement](https://openai.com/o1).\n\nNote: This model is currently experimental and not suitable for production use-cases, and may be heavily rate-limited.\n\n"},{"lang":"en","group":"models","slug":"models/openai-gpt-4o-mini","frontmatter":{"title":"OpenAI: GPT-4o-Mini Official","meta_title":"OpenAI: GPT-4o-Mini Official","description":"OpenAI: GPT-4o-Mini Official","date":"2024-10-26T09:00:07.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"gpt-4o-mini","tags":["Generative AI","Natural Language Processing","Machine Learning","Technology","Chatbots"],"draft":false,"is_recommended":false,"id":"openai/gpt-4o-mini","context":"None","input":1.5e-7,"output":6e-7,"img":0,"request":0,"last_updated":"2024-11-14T05:09:13.000Z","slug":"models/openai-gpt-4o-mini"},"content":"\nGPT-4o mini is OpenAI's newest model after [GPT-4 Omni](/openai/gpt-4o), supporting both text and image inputs with text outputs.\n\nAs their most advanced small model, it is many multiples more affordable than other recent frontier models, and more than 60% cheaper than [GPT-3.5 Turbo](/openai/gpt-3.5-turbo). It maintains SOTA intelligence, while being significantly more cost-effective.\n\nGPT-4o mini achieves an 82% score on MMLU and presently ranks higher than GPT-4 on chat preferences [common leaderboards](https://arena.lmsys.org/).\n\nCheck out the [launch announcement](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/) to learn more.\n\n"},{"lang":"en","group":"models","slug":"models/openai-gpt-4o","frontmatter":{"title":"OpenAI: GPT-4o Official","meta_title":"OpenAI: GPT-4o Official","description":"OpenAI: GPT-4o Official","date":"2024-11-14T02:53:29.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"gpt-4o","tags":["Generative AI","Natural Language Processing","Technology","Chatbots","Machine Learning"],"draft":false,"is_recommended":false,"id":"openai/gpt-4o","context":"None","input":0.0000025,"output":0.00001,"img":0,"request":0,"last_updated":"2024-11-14T02:58:04.000Z","slug":"models/openai-gpt-4o"},"content":"\nGPT-4o (\"o\" for \"omni\") is OpenAI's latest AI model, supporting both text and image inputs with text outputs. It maintains the intelligence level of [GPT-4 Turbo](/openai/gpt-4-turbo) while being twice as fast and 50% more cost-effective. GPT-4o also offers improved performance in processing non-English languages and enhanced visual capabilities.\n\nFor benchmarking against other models, it was briefly called [\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209)\n\n"},{"lang":"en","group":"models","slug":"models/openchat-7b","frontmatter":{"title":"OpenChat 3.5 7B","meta_title":"OpenChat 3.5 7B","description":"OpenChat 3.5 7B","date":"2023-11-28T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"openchat","tags":["Programming","Natural Language Processing","Machine Learning","Open Source","Generative AI"],"draft":false,"id":"openchat-7b","context":8192,"input":5.5e-8,"output":5.5e-8,"img":0,"request":0,"last_updated":"2024-11-07T09:39:42.000Z","slug":"models/openchat-7b"},"content":"\nOpenChat 7B is a library of open-source language models, fine-tuned with \"C-RLFT (Conditioned Reinforcement Learning Fine-Tuning)\" - a strategy inspired by offline reinforcement learning. It has been trained on mixed-quality data without preference labels.\n\n- For OpenChat fine-tuned on Mistral 7B, check out [OpenChat 7B](/openchat/openchat-7b).\n- For OpenChat fine-tuned on Llama 8B, check out [OpenChat 8B](/openchat/openchat-8b).\n\n#open-source\n\n"},{"lang":"en","group":"models","slug":"models/palm-2-chat-bison-32k","frontmatter":{"title":"Google: PaLM 2 Chat 32k","meta_title":"Google: PaLM 2 Chat 32k","description":"Google: PaLM 2 Chat 32k","date":"2023-11-03T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text 2 text"],"author":"google","tags":["Natural Language Processing","Programming","Technology","Chatbots","Generative AI"],"draft":false,"id":"palm-2-chat-bison-32k","context":32760,"input":0.000001,"output":0.000002,"img":0,"request":0,"last_updated":"2024-11-11T03:15:52.000Z","slug":"models/palm-2-chat-bison-32k"},"content":"\nPaLM 2 is a language model by Google with improved multilingual, reasoning and coding capabilities.\n\n"},{"lang":"en","group":"models","slug":"models/palm-2-codechat-bison-32k","frontmatter":{"title":"Google: PaLM 2 Code Chat 32k","meta_title":"Google: PaLM 2 Code Chat 32k","description":"Google: PaLM 2 Code Chat 32k","date":"2023-11-03T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text 2 text"],"author":"google","tags":["Programming","Chatbots","Natural Language Processing","Generative AI","Technology/Web"],"draft":false,"id":"palm-2-codechat-bison-32k","context":32760,"input":0.000001,"output":0.000002,"img":0,"request":0,"last_updated":"2024-11-11T03:16:01.000Z","slug":"models/palm-2-codechat-bison-32k"},"content":"\nPaLM 2 fine-tuned for chatbot conversations that help with code-related questions.\n\n"},{"lang":"en","group":"models","slug":"models/phi-3-medium-128k-instruct","frontmatter":{"title":"Phi-3 Medium 128K Instruct","meta_title":"Phi-3 Medium 128K Instruct","description":"Phi-3 Medium 128K Instruct","date":"2024-05-24T00:00:00.000Z","image":"https://img.rifx.online/logo/microsoft.svg","categories":["text 2 text"],"author":"microsoft","tags":["Natural Language Processing","Machine Learning","Programming","Data Science","Generative AI"],"draft":false,"id":"phi-3-medium-128k-instruct","context":128000,"input":0.000001,"output":0.000001,"img":0,"request":0,"last_updated":"2024-11-11T03:17:38.000Z","slug":"models/phi-3-medium-128k-instruct"},"content":"\nPhi-3 128K Medium is a powerful 14-billion parameter model designed for advanced language understanding, reasoning, and instruction following. Optimized through supervised fine-tuning and preference adjustments, it excels in tasks involving common sense, mathematics, logical reasoning, and code processing.\n\nAt time of release, Phi-3 Medium demonstrated state-of-the-art performance among lightweight models. In the MMLU-Pro eval, the model even comes close to a Llama3 70B level of performance.\n\nFor 4k context length, try [Phi-3 Medium 4K](/microsoft/phi-3-medium-4k-instruct).\n\n"},{"lang":"en","group":"models","slug":"models/phi-3-mini-128k-instruct","frontmatter":{"title":"Phi-3 Mini 128K Instruct","meta_title":"Phi-3 Mini 128K Instruct","description":"Phi-3 Mini 128K Instruct","date":"2024-05-26T00:00:00.000Z","image":"https://img.rifx.online/logo/microsoft.svg","categories":["text 2 text"],"author":"microsoft","tags":["Natural Language Processing","Machine Learning","Programming","Data Science","Generative AI"],"draft":false,"id":"phi-3-mini-128k-instruct","context":128000,"input":1e-7,"output":1e-7,"img":0,"request":0,"last_updated":"2024-11-11T03:17:47.000Z","slug":"models/phi-3-mini-128k-instruct"},"content":"\nPhi-3 Mini is a powerful 3.8B parameter model designed for advanced language understanding, reasoning, and instruction following. Optimized through supervised fine-tuning and preference adjustments, it excels in tasks involving common sense, mathematics, logical reasoning, and code processing.\n\nAt time of release, Phi-3 Medium demonstrated state-of-the-art performance among lightweight models. This model is static, trained on an offline dataset with an October 2023 cutoff date.\n\n"},{"lang":"en","group":"models","slug":"models/phi-35-mini-128k-instruct","frontmatter":{"title":"Phi-3.5 Mini 128K Instruct","meta_title":"Phi-3.5 Mini 128K Instruct","description":"Phi-3.5 Mini 128K Instruct","date":"2024-08-21T00:00:00.000Z","image":"https://img.rifx.online/logo/microsoft.svg","categories":["text 2 text"],"author":"microsoft","tags":["Programming","Machine Learning","Natural Language Processing","Data Science","Generative AI"],"draft":false,"id":"phi-3.5-mini-128k-instruct","context":128000,"input":1e-7,"output":1e-7,"img":0,"request":0,"last_updated":"2024-11-01T04:17:17.000Z","slug":"models/phi-35-mini-128k-instruct"},"content":"\nPhi-3.5 models are lightweight, state-of-the-art open models. These models were trained with Phi-3 datasets that include both synthetic data and the filtered, publicly available websites data, with a focus on high quality and reasoning-dense properties. Phi-3.5 Mini uses 3.8B parameters, and is a dense decoder-only transformer model using the same tokenizer as [Phi-3 Mini](/microsoft/phi-3-mini-128k-instruct).\n\nThe models underwent a rigorous enhancement process, incorporating both supervised fine-tuning, proximal policy optimization, and direct preference optimization to ensure precise instruction adherence and robust safety measures. When assessed against benchmarks that test common sense, language understanding, math, code, long context and logical reasoning, Phi-3.5 models showcased robust and state-of-the-art performance among models with less than 13 billion parameters.\n\n"},{"lang":"en","group":"models","slug":"models/pixtral-12b","frontmatter":{"title":"Mistral: Pixtral 12B","meta_title":"Mistral: Pixtral 12B","description":"Mistral: Pixtral 12B","date":"2024-09-10T00:00:00.000Z","image":"https://img.rifx.online/logo/mistral.png","categories":["text image 2 text"],"author":"mistralai","tags":["Natural Language Processing","Machine Learning","Technology","Generative AI","Computer Vision"],"draft":false,"id":"pixtral-12b","context":4096,"input":1e-7,"output":1e-7,"img":0.0001445,"request":0,"last_updated":"2024-11-11T03:10:29.000Z","slug":"models/pixtral-12b"},"content":"\nThe first image to text model from Mistral AI. Its weight was launched via torrent per their tradition: https://x.com/mistralai/status/1833758285167722836\n\n"},{"lang":"en","group":"models","slug":"models/qwen-2-7b-instruct","frontmatter":{"title":"Qwen 2 7B Instruct","meta_title":"Qwen 2 7B Instruct","description":"Qwen 2 7B Instruct","date":"2024-07-16T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"qwen","tags":["Natural Language Processing","Programming","Machine Learning","Data Science","Ethics"],"draft":false,"id":"qwen-2-7b-instruct","context":32768,"input":5.4e-8,"output":5.4e-8,"img":0,"request":0,"last_updated":"2024-07-16T00:00:00.000Z","slug":"models/qwen-2-7b-instruct"},"content":"\nQwen2 7B is a transformer-based model that excels in language understanding, multilingual capabilities, coding, mathematics, and reasoning.\n\nIt features SwiGLU activation, attention QKV bias, and group query attention. It is pretrained on extensive data with supervised finetuning and direct preference optimization.\n\nFor more details, see this [blog post](https://qwenlm.github.io/blog/qwen2/) and [GitHub repo](https://github.com/QwenLM/Qwen2).\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).\n\n"},{"lang":"en","group":"models","slug":"models/qwen-2-vl-72b-instruct","frontmatter":{"title":"Qwen2-VL 72B Instruct","meta_title":"Qwen2-VL 72B Instruct","description":"Qwen2-VL 72B Instruct","date":"2024-09-18T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text image 2 text"],"author":"qwen","tags":["Natural Language Processing","Computer Vision","Robotics","Machine Learning"],"draft":false,"id":"qwen-2-vl-72b-instruct","context":32768,"input":4e-7,"output":4e-7,"img":0.000578,"request":0,"last_updated":"2024-09-18T00:00:00.000Z","slug":"models/qwen-2-vl-72b-instruct"},"content":"\nQwen2 VL 72B is a multimodal LLM from the Qwen Team with the following key enhancements:\n\n- SoTA understanding of images of various resolution & ratio: Qwen2-VL achieves state-of-the-art performance on visual understanding benchmarks, including MathVista, DocVQA, RealWorldQA, MTVQA, etc.\n\n- Understanding videos of 20min+: Qwen2-VL can understand videos over 20 minutes for high-quality video-based question answering, dialog, content creation, etc.\n\n- Agent that can operate your mobiles, robots, etc.: with the abilities of complex reasoning and decision making, Qwen2-VL can be integrated with devices like mobile phones, robots, etc., for automatic operation based on visual environment and text instructions.\n\n- Multilingual Support: to serve global users, besides English and Chinese, Qwen2-VL now supports the understanding of texts in different languages inside images, including most European languages, Japanese, Korean, Arabic, Vietnamese, etc.\n\nFor more details, see this [blog post](https://qwenlm.github.io/blog/qwen2-vl/) and [GitHub repo](https://github.com/QwenLM/Qwen2-VL).\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).\n\n"},{"lang":"en","group":"models","slug":"models/qwen-2-vl-7b-instruct","frontmatter":{"title":"Qwen2-VL 7B Instruct","meta_title":"Qwen2-VL 7B Instruct","description":"Qwen2-VL 7B Instruct","date":"2024-08-28T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text image 2 text"],"author":"qwen","tags":["Natural Language Processing","Computer Vision","Robotics","Multimodal AI","Generative AI"],"draft":false,"id":"qwen-2-vl-7b-instruct","context":32768,"input":1e-7,"output":1e-7,"img":0.0001445,"request":0,"last_updated":"2024-11-11T03:13:01.000Z","slug":"models/qwen-2-vl-7b-instruct"},"content":"\nQwen2 VL 7B is a multimodal LLM from the Qwen Team with the following key enhancements:\n\n- SoTA understanding of images of various resolution & ratio: Qwen2-VL achieves state-of-the-art performance on visual understanding benchmarks, including MathVista, DocVQA, RealWorldQA, MTVQA, etc.\n\n- Understanding videos of 20min+: Qwen2-VL can understand videos over 20 minutes for high-quality video-based question answering, dialog, content creation, etc.\n\n- Agent that can operate your mobiles, robots, etc.: with the abilities of complex reasoning and decision making, Qwen2-VL can be integrated with devices like mobile phones, robots, etc., for automatic operation based on visual environment and text instructions.\n\n- Multilingual Support: to serve global users, besides English and Chinese, Qwen2-VL now supports the understanding of texts in different languages inside images, including most European languages, Japanese, Korean, Arabic, Vietnamese, etc.\n\nFor more details, see this [blog post](https://qwenlm.github.io/blog/qwen2-vl/) and [GitHub repo](https://github.com/QwenLM/Qwen2-VL).\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).\n\n"},{"lang":"en","group":"models","slug":"models/qwen-25-72b-instruct","frontmatter":{"title":"Qwen2.5 72B Instruct","meta_title":"Qwen2.5 72B Instruct","description":"Qwen2.5 72B Instruct","date":"2024-09-19T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"qwen","tags":["Programming","Natural Language Processing","Chatbots","Machine Learning","Data Science"],"draft":false,"id":"qwen-2.5-72b-instruct","context":131072,"input":3.5e-7,"output":4e-7,"img":0,"request":0,"last_updated":"2024-09-19T00:00:00.000Z","slug":"models/qwen-25-72b-instruct"},"content":"\nQwen2.5 72B is the latest series of Qwen large language models. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\n\n- Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n\n- Long-context Support up to 128K tokens and can generate up to 8K tokens.\n\n- Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).\n\n"},{"lang":"en","group":"models","slug":"models/qwen-25-7b-instruct","frontmatter":{"title":"Qwen2.5 7B Instruct","meta_title":"Qwen2.5 7B Instruct","description":"Qwen2.5 7B Instruct","date":"2024-10-16T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"qwen","tags":["Programming","Natural Language Processing","Chatbots","Machine Learning","Data Science"],"draft":false,"id":"qwen-2.5-7b-instruct","context":131072,"input":2.7e-7,"output":2.7e-7,"img":0,"request":0,"last_updated":"2024-10-16T00:00:00.000Z","slug":"models/qwen-25-7b-instruct"},"content":"\nQwen2.5 7B is the latest series of Qwen large language models. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\n\n- Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n\n- Long-context Support up to 128K tokens and can generate up to 8K tokens.\n\n- Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).\n\n"},{"lang":"en","group":"models","slug":"models/qwen-25-coder-32b-instruct","frontmatter":{"title":"Qwen2.5 Coder 32B Instruct","meta_title":"Qwen2.5 Coder 32B Instruct","description":"Qwen2.5 Coder 32B Instruct","date":"2024-11-11T23:40:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"qwen","tags":["Programming","Programming/Scripting","Machine Learning","Natural Language Processing","Generative AI"],"draft":false,"is_recommended":true,"id":"qwen-2.5-coder-32b-instruct","context":32768,"input":1.8e-7,"output":1.8e-7,"img":0,"request":0,"last_updated":"2024-11-14T04:04:59.000Z","slug":"models/qwen-25-coder-32b-instruct"},"content":"\nQwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). Qwen2.5-Coder brings the following improvements upon CodeQwen1.5:\n\n- Significantly improvements in **code generation**, **code reasoning** and **code fixing**. \n- A more comprehensive foundation for real-world applications such as **Code Agents**. Not only enhancing coding capabilities but also maintaining its strengths in mathematics and general competencies.\n\nTo read more about its evaluation results, check out [Qwen 2.5 Coder's blog](https://qwenlm.github.io/blog/qwen2.5-coder-family/).\n\n## Citation\n\n```\n@article{hui2024qwen2,\n      title={Qwen2. 5-Coder Technical Report},\n      author={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Dang, Kai and others},\n      journal={arXiv preprint arXiv:2409.12186},\n      year={2024}\n}\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}\n```\n\n\n"},{"lang":"en","group":"models","slug":"models/remm-slerp-l2-13b","frontmatter":{"title":"ReMM SLERP 13B","meta_title":"ReMM SLERP 13B","description":"ReMM SLERP 13B","date":"2023-07-22T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"undi95","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"is_recommended":true,"id":"remm-slerp-l2-13b","context":4096,"input":0.000001125,"output":0.000001125,"img":0,"request":0,"last_updated":"2024-11-14T04:06:00.000Z","slug":"models/remm-slerp-l2-13b"},"content":"\nA recreation trial of the original MythoMax-L2-B13 but with updated models. #merge\n\n"},{"lang":"en","group":"models","slug":"models/remm-slerp-l2-13b:extended","frontmatter":{"title":"ReMM SLERP 13B (extended)","meta_title":"ReMM SLERP 13B (extended)","description":"ReMM SLERP 13B (extended)","date":"2023-07-22T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"undi95","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"remm-slerp-l2-13b:extended","context":6144,"input":0.000001125,"output":0.000001125,"img":0,"request":0,"last_updated":"2024-11-04T12:47:21.000Z","slug":"models/remm-slerp-l2-13b:extended"},"content":"\nA recreation trial of the original MythoMax-L2-B13 but with updated models. #merge\n\n_These are extended-context endpoints for [ReMM SLERP 13B](/undi95/remm-slerp-l2-13b). They may have higher prices._\n\n"},{"lang":"en","group":"models","slug":"models/rocinante-12b","frontmatter":{"title":"Rocinante 12B","meta_title":"Rocinante 12B","description":"Rocinante 12B","date":"2024-09-30T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"thedrummer","tags":["Roleplay","Programming","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"rocinante-12b","context":32768,"input":2.5e-7,"output":5e-7,"img":0,"request":0,"last_updated":"2024-11-11T03:09:37.000Z","slug":"models/rocinante-12b"},"content":"\nRocinante 12B is designed for engaging storytelling and rich prose.\n\nEarly testers have reported:\n- Expanded vocabulary with unique and expressive word choices\n- Enhanced creativity for vivid narratives\n- Adventure-filled and captivating stories\n\n"},{"lang":"en","group":"models","slug":"models/sorcererlm-8x22b","frontmatter":{"title":"Sorcererlm 8x22b","meta_title":"Sorcererlm 8x22b","description":"Sorcererlm 8x22b","date":"2024-11-08T22:31:23.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"raifle","tags":["Roleplay","Programming","Natural Language Processing","Chatbots","Generative AI"],"draft":false,"id":"sorcererlm-8x22b","context":16000,"input":0.0000045,"output":0.0000045,"img":0,"request":0,"last_updated":"2024-11-11T02:56:49.000Z","slug":"models/sorcererlm-8x22b"},"content":"\nSorcererLM is an advanced RP and storytelling model, built as a Low-rank 16-bit LoRA fine-tuned on WizardLM-2-8x22B.\n\n- Advanced reasoning and emotional intelligence for engaging and immersive interactions\n- Vivid writing capabilities enriched with spatial and contextual awareness\n- Enhanced narrative depth, promoting creative and dynamic storytelling\n\n"},{"lang":"en","group":"models","slug":"models/toppy-m-7b","frontmatter":{"title":"Toppy M 7B","meta_title":"Toppy M 7B","description":"Toppy M 7B","date":"2023-11-10T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"undi95","tags":["Programming","Machine Learning","Generative AI","Chatbots","Data Science"],"draft":false,"id":"toppy-m-7b","context":4096,"input":7e-8,"output":7e-8,"img":0,"request":0,"last_updated":"2024-11-04T12:51:35.000Z","slug":"models/toppy-m-7b"},"content":"\nA wild 7B parameter model that merges several models using the new task_arithmetic merge method from mergekit.\nList of merged models:\n- NousResearch/Nous-Capybara-7B-V1.9\n- [HuggingFaceH4/zephyr-7b-beta](/huggingfaceh4/zephyr-7b-beta)\n- lemonilia/AshhLimaRP-Mistral-7B\n- Vulkane/120-Days-of-Sodom-LoRA-Mistral-7b\n- Undi95/Mistral-pippa-sharegpt-7b-qlora\n\n#merge #uncensored\n\n"},{"lang":"en","group":"models","slug":"models/unslopnemo-12b","frontmatter":{"title":"Unslopnemo 12b","meta_title":"Unslopnemo 12b","description":"Unslopnemo 12b","date":"2024-11-08T22:04:08.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"thedrummer","tags":["Roleplay","Programming","Generative AI","Chatbots","Natural Language Processing"],"draft":false,"id":"unslopnemo-12b","context":32000,"input":5e-7,"output":5e-7,"img":0,"request":0,"last_updated":"2024-11-14T02:10:09.000Z","slug":"models/unslopnemo-12b"},"content":"\nUnslopNemo v4.1 is the latest addition from the creator of Rocinante, designed for adventure writing and role-play scenarios.\n\n"},{"lang":"en","group":"models","slug":"models/wizardlm-2-7b","frontmatter":{"title":"WizardLM-2 7B","meta_title":"WizardLM-2 7B","description":"WizardLM-2 7B","date":"2024-04-16T00:00:00.000Z","image":"https://img.rifx.online/logo/microsoft.svg","categories":["text 2 text"],"author":"microsoft","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"wizardlm-2-7b","context":32000,"input":5.5e-8,"output":5.5e-8,"img":0,"request":0,"last_updated":"2024-10-31T23:23:36.000Z","slug":"models/wizardlm-2-7b"},"content":"\nWizardLM-2 7B is the smaller variant of Microsoft AI's latest Wizard model. It is the fastest and achieves comparable performance with existing 10x larger opensource leading models\n\nIt is a finetune of [Mistral 7B Instruct](/mistralai/mistral-7b-instruct), using the same technique as [WizardLM-2 8x22B](/microsoft/wizardlm-2-8x22b).\n\nTo read more about the model release, [click here](https://wizardlm.github.io/WizardLM2/).\n\n#moe\n\n"},{"lang":"en","group":"models","slug":"models/wizardlm-2-8x22b","frontmatter":{"title":"WizardLM-2 8x22B","meta_title":"WizardLM-2 8x22B","description":"WizardLM-2 8x22B","date":"2024-04-16T00:00:00.000Z","image":"https://img.rifx.online/logo/microsoft.svg","categories":["text 2 text"],"author":"microsoft","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"wizardlm-2-8x22b","context":65536,"input":5e-7,"output":5e-7,"img":0,"request":0,"last_updated":"2024-10-31T23:24:21.000Z","slug":"models/wizardlm-2-8x22b"},"content":"\nWizardLM-2 8x22B is Microsoft AI's most advanced Wizard model. It demonstrates highly competitive performance compared to leading proprietary models, and it consistently outperforms all existing state-of-the-art opensource models.\n\nIt is an instruct finetune of [Mixtral 8x22B](/mistralai/mixtral-8x22b).\n\nTo read more about the model release, [click here](https://wizardlm.github.io/WizardLM2/).\n\n#moe\n\n"},{"lang":"zh","group":"models","slug":"models/chatgpt-4o-latest","frontmatter":{"title":"OpenAI: ChatGPT-4o","meta_title":"OpenAI: ChatGPT-4o","description":"OpenAI: ChatGPT-4o","date":"2024-08-14T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text image 2 text"],"author":"openai","tags":["Chatbots","Generative AI","Machine Learning","Natural Language Processing"],"draft":false,"id":"chatgpt-4o-latest","context":128000,"input":0.000005,"output":0.000015,"img":0.007225,"request":0,"last_updated":"2024-08-14T00:00:00.000Z","slug":"models/chatgpt-4o-latest"},"content":"\n动态模型持续更新到 ChatGPT 中的当前版本 [GPT-4o](/openai/gpt-4o)。旨在用于研究和评估。\n\n注意：此模型目前处于实验阶段，不适合生产使用，并可能受到严格的速率限制。\n\n"},{"lang":"zh","group":"models","slug":"models/claude-3-haiku","frontmatter":{"title":"Anthropic: Claude 3 Haiku","meta_title":"Anthropic: Claude 3 Haiku","description":"Anthropic: Claude 3 Haiku","date":"2024-03-13T00:00:00.000Z","image":"https://img.rifx.online/logo/anthropic.svg","categories":["text image 2 text"],"author":"anthropic","tags":["Programming","Machine Learning","Generative AI","Chatbots","Natural Language Processing"],"draft":false,"id":"claude-3-haiku","context":200000,"input":2.5e-7,"output":0.00000125,"img":0.0004,"request":0,"last_updated":"2024-10-24T11:54:59.000Z","slug":"models/claude-3-haiku"},"content":"\nClaude 3 Haiku 是 Anthropic 反应速度最快、体积最小的模型，能够实现近乎即时的响应。快速且准确的定向性能。\n\n查看发布公告和基准测试结果 [这里](https://www.anthropic.com/news/claude-3-haiku)\n\n#multimodal\n\n"},{"lang":"zh","group":"models","slug":"models/claude-3-opus","frontmatter":{"title":"Anthropic: Claude 3 Opus","meta_title":"Anthropic: Claude 3 Opus","description":"Anthropic: Claude 3 Opus","date":"2024-03-05T00:00:00.000Z","image":"https://img.rifx.online/logo/anthropic.svg","categories":["text image 2 text"],"author":"anthropic","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"claude-3-opus","context":200000,"input":0.000015,"output":0.000075,"img":0.024,"request":0,"last_updated":"2024-11-07T09:45:35.000Z","slug":"models/claude-3-opus"},"content":"\nClaude 3 Opus 是 Anthropic 针对高度复杂任务的最强大模型。它拥有顶级的性能、智能、流畅性和理解能力。\n\n查看发布公告和基准测试结果 [here](https://www.anthropic.com/news/claude-3-family)\n\n#multimodal\n\n"},{"lang":"zh","group":"models","slug":"models/claude-3-sonnet","frontmatter":{"title":"Anthropic: Claude 3 Sonnet","meta_title":"Anthropic: Claude 3 Sonnet","description":"Anthropic: Claude 3 Sonnet","date":"2024-03-05T00:00:00.000Z","image":"https://img.rifx.online/logo/anthropic.svg","categories":["text image 2 text"],"author":"anthropic","tags":["Programming","Technology","Machine Learning","Data Science","Chatbots"],"draft":false,"is_recommended":true,"id":"claude-3-sonnet","context":200000,"input":0.000003,"output":0.000015,"img":0.0048,"request":0,"last_updated":"2024-11-14T04:05:16.000Z","slug":"models/claude-3-sonnet"},"content":"\nClaude 3 Sonnet 是企业工作负载的理想智能与速度平衡。以更低的价格提供最大效用，可靠，适合大规模部署的平衡。\n\n查看发布公告和基准测试结果 [这里](https://www.anthropic.com/news/claude-3-family)\n\n#multimodal\n\n"},{"lang":"zh","group":"models","slug":"models/claude-35-haiku","frontmatter":{"title":"Anthropic: Claude 3.5 Haiku","meta_title":"Anthropic: Claude 3.5 Haiku","description":"Anthropic: Claude 3.5 Haiku","date":"2024-11-04T00:00:00.000Z","image":"https://img.rifx.online/logo/anthropic.svg","categories":["text 2 text"],"author":"anthropic","tags":["Programming","Chatbots","Data Science","Machine Learning","Natural Language Processing"],"draft":false,"id":"claude-3.5-haiku","context":200000,"input":0.000001,"output":0.000005,"img":0,"request":0,"last_updated":"2024-11-07T09:46:02.000Z","slug":"models/claude-35-haiku"},"content":"\nClaude 3.5 Haiku 特性提供了更高的速度、编码准确性和工具使用能力。旨在在实时应用中表现出色，它提供了快速的响应时间，这对于动态任务（如聊天互动和即时编码建议）至关重要。\n\n这使其非常适合需要速度和精度的环境，例如软件开发、客户服务机器人和数据管理系统。\n\n此模型当前指向 [Claude 3.5 Haiku (2024-10-22)](/anthropic/claude-3-5-haiku-20241022).\n\n"},{"lang":"zh","group":"models","slug":"models/claude-35-sonnet","frontmatter":{"title":"Anthropic: Claude 3.5 Sonnet","meta_title":"Anthropic: Claude 3.5 Sonnet","description":"Anthropic: Claude 3.5 Sonnet","date":"2024-06-20T00:00:00.000Z","image":"https://img.rifx.online/logo/anthropic.svg","categories":["text image 2 text"],"author":"anthropic","tags":["Programming","Data Science","Computer Vision","Chatbots","Autonomous Systems"],"draft":false,"id":"claude-3.5-sonnet","context":200000,"input":0.000003,"output":0.000015,"img":0.0048,"request":0,"last_updated":"2024-10-24T11:45:46.000Z","slug":"models/claude-35-sonnet"},"content":"\nClaude 3.5 Sonnet 提供优于 Opus 的能力，以快于 Sonnet 的速度，且价格与 Sonnet 相同。Sonnet 在以下方面表现尤为出色：\n\n- 编程：自主编写、编辑和运行代码，具备推理和故障排除能力\n- 数据科学：增强人类数据科学专业知识；在使用多种工具获取洞察的同时，处理非结构化数据\n- 视觉处理：擅长解读图表、图形和图像，准确转录文本以获得超越文本本身的洞察\n- 代理任务：出色的工具使用能力，使其在代理任务（即需要与其他系统互动的复杂多步骤问题解决任务）中表现出色\n\n#multimodal\n\n"},{"lang":"zh","group":"models","slug":"models/command-r-plus","frontmatter":{"title":"Cohere: Command R+ (08-2024)","meta_title":"Cohere: Command R+ (08-2024)","description":"Cohere: Command R+ (08-2024)","date":"2024-08-30T00:00:00.000Z","image":"https://img.rifx.online/logo/cohere.svg","categories":["text 2 text"],"author":"cohere","tags":["Technology","Programming","Machine Learning","Generative AI","Ethics"],"draft":false,"id":"command-r-plus","context":128000,"input":0.000002375,"output":0.0000095,"img":0,"request":0,"last_updated":"2024-11-07T09:33:44.000Z","slug":"models/command-r-plus"},"content":"\ncommand-r-plus-08-2024 是 [Command R+](/cohere/command-r-plus) 的更新，与之前的 Command R+ 版本相比，吞吐量提高了大约 50%，延迟降低了 25%，同时保持相同的硬件占用。\n\n在 [这里](https://docs.cohere.com/changelog/command-gets-refreshed) 阅读发布帖子。\n\n使用此模型须遵循 Cohere 的 [可接受使用政策](https://docs.cohere.com/docs/c4ai-acceptable-use-policy)。\n\n"},{"lang":"zh","group":"models","slug":"models/command-r","frontmatter":{"title":"Cohere: Command R (08-2024)","meta_title":"Cohere: Command R (08-2024)","description":"Cohere: Command R (08-2024)","date":"2024-08-30T00:00:00.000Z","image":"https://img.rifx.online/logo/cohere.svg","categories":["text 2 text"],"author":"cohere","tags":["Programming","Natural Language Processing","Generative AI","Machine Learning","Data Science"],"draft":false,"id":"command-r","context":128000,"input":1.425e-7,"output":5.7e-7,"img":0,"request":0,"last_updated":"2024-11-07T09:33:55.000Z","slug":"models/command-r"},"content":"\ncommand-r-08-2024 是 [Command R](/cohere/command-r) 的更新，提升了多语言检索增强生成 (RAG) 和工具使用的性能。更广泛地说，它在数学、代码和推理方面表现更佳，并且与之前的更大版本 Command R+ 模型具有竞争力。\n\n阅读发布帖子 [这里](https://docs.cohere.com/changelog/command-gets-refreshed)。\n\n使用此模型须遵循 Cohere 的 [可接受使用政策](https://docs.cohere.com/docs/c4ai-acceptable-use-policy)。\n\n"},{"lang":"zh","group":"models","slug":"models/deepseek-chat","frontmatter":{"title":"DeepSeek V2.5","meta_title":"DeepSeek V2.5","description":"DeepSeek V2.5","date":"2024-05-14T00:00:00.000Z","image":"https://img.rifx.online/logo/deepseek.svg","categories":["text 2 text"],"author":"deepseek","tags":["Programming","Natural Language Processing","Machine Learning","Data Science","Chatbots"],"draft":false,"id":"deepseek-chat","context":128000,"input":1.4e-7,"output":2.8e-7,"img":0,"request":0,"last_updated":"2024-11-01T04:19:11.000Z","slug":"models/deepseek-chat"},"content":"\nDeepSeek-V2.5 是一个升级版本，结合了 DeepSeek-V2-Chat 和 DeepSeek-Coder-V2-Instruct。新模型整合了前两个版本的通用能力和编码能力。\n\nDeepSeek-V2 Chat 是 DeepSeek-V2 的对话微调版本，属于混合专家（MoE）语言模型。它总共包含 236B 个参数，其中每个 token 激活 21B。\n\n与 DeepSeek 67B 相比，DeepSeek-V2 的性能更强，同时节省了 42.5% 的训练成本，减少了 93.3% 的 KV 缓存，并将最大生成吞吐量提升至 5.76 倍。\n\nDeepSeek-V2 在标准基准测试和开放式生成评估中表现出色。\n\n"},{"lang":"zh","group":"models","slug":"models/dolphin-mixtral-8x22b","frontmatter":{"title":"Dolphin 2.9.2 Mixtral 8x22B 🐬","meta_title":"Dolphin 2.9.2 Mixtral 8x22B 🐬","description":"Dolphin 2.9.2 Mixtral 8x22B 🐬","date":"2024-06-08T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"cognitivecomputations","tags":["Natural Language Processing","Generative AI","Chatbots","Roleplay","Ethics"],"draft":false,"id":"dolphin-mixtral-8x22b","context":65536,"input":9e-7,"output":9e-7,"img":0,"request":0,"last_updated":"2024-11-04T12:49:50.000Z","slug":"models/dolphin-mixtral-8x22b"},"content":"\nDolphin 2.9 旨在进行指令跟随、对话和编码。该模型是 [Mixtral 8x22B Instruct](/mistralai/mixtral-8x22b-instruct) 的微调版本。它具有 64k 的上下文长度，并使用 ChatML 模板进行了 16k 序列长度的微调。\n\n该模型是 [Dolphin Mixtral 8x7B](/cognitivecomputations/dolphin-mixtral-8x7b) 的继任者。\n\n该模型未经过审查，并去除了对齐和偏见。它需要外部对齐层以确保伦理使用。用户被警告要负责任地使用这个高度合规的模型，详细信息请参见关于未审查模型的博客文章 [erichartford.com/uncensored-models](https://erichartford.com/uncensored-models)。\n\n#moe #uncensored\n\n"},{"lang":"zh","group":"models","slug":"models/dolphin-mixtral-8x7b","frontmatter":{"title":"Dolphin 2.6 Mixtral 8x7B 🐬","meta_title":"Dolphin 2.6 Mixtral 8x7B 🐬","description":"Dolphin 2.6 Mixtral 8x7B 🐬","date":"2023-12-21T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"cognitivecomputations","tags":["Programming","Natural Language Processing","Generative AI","Ethics","Chatbots"],"draft":false,"id":"dolphin-mixtral-8x7b","context":32768,"input":5e-7,"output":5e-7,"img":0,"request":0,"last_updated":"2024-11-04T12:52:28.000Z","slug":"models/dolphin-mixtral-8x7b"},"content":"\n这是对 [Mixtral-8x7b](/mistralai/mixtral-8x7b) 的 16k 上下文微调。由于大量使用编码数据进行训练，它在编码任务中表现出色，并以其服从性而闻名，尽管缺乏 DPO 调优。\n\n该模型未经过审查，并且去除了对齐和偏见。它需要一个外部对齐层以确保伦理使用。用户被提醒要负责任地使用这个高度合规的模型，具体细节可参见关于未审查模型的博客文章 [erichartford.com/uncensored-models](https://erichartford.com/uncensored-models)。\n\n#moe #uncensored\n\n"},{"lang":"zh","group":"models","slug":"models/eva-qwen-25-14b","frontmatter":{"title":"EVA Qwen2.5 14B","meta_title":"EVA Qwen2.5 14B","description":"EVA Qwen2.5 14B","date":"2024-09-30T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"eva-unit-01","tags":["Roleplay","Programming","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"eva-qwen-2.5-14b","context":32768,"input":2.5e-7,"output":5e-7,"img":0,"request":0,"last_updated":"2024-11-11T03:09:28.000Z","slug":"models/eva-qwen-25-14b"},"content":"\n一个专注于角色扮演和创意写作的模型，该模型基于 Qwen2.5-14B，经过合成数据和自然数据的混合微调。\n\n它在 1.5M 令牌的角色扮演数据上进行训练，并在 1.5M 令牌的合成数据上进行微调。\n\n"},{"lang":"zh","group":"models","slug":"models/eva-qwen-25-32b","frontmatter":{"title":"Eva Qwen2.5 32B","meta_title":"Eva Qwen2.5 32B","description":"Eva Qwen2.5 32B","date":"2024-11-08T22:27:27.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"eva-unit-01","tags":["Roleplay","Programming","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"eva-qwen-2.5-32b","context":32000,"input":5e-7,"output":5e-7,"img":0,"request":0,"last_updated":"2024-11-11T02:57:47.000Z","slug":"models/eva-qwen-25-32b"},"content":"\n一个角色扮演/故事创作专家模型，对Qwen2.5-32B进行全参数微调，使用合成数据和自然数据的混合。\n\n它使用Celeste 70B 0.1数据混合，极大地扩展了数据以提高模型的多样性、创造力和“风味”。\n\n"},{"lang":"zh","group":"models","slug":"models/gemini-flash-15-8b-exp","frontmatter":{"title":"Google: Gemini Flash 8B 1.5 Experimental","meta_title":"Google: Gemini Flash 8B 1.5 Experimental","description":"Google: Gemini Flash 8B 1.5 Experimental","date":"2024-08-28T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text image 2 text"],"author":"google","tags":["Technology","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"gemini-flash-1.5-8b-exp","context":1000000,"input":0,"output":0,"img":0,"request":0,"last_updated":"2024-11-11T03:14:22.000Z","slug":"models/gemini-flash-15-8b-exp"},"content":"\nGemini 1.5 Flash 8B Experimental 是 [Gemini 1.5 Flash](/google/gemini-flash-1.5) 模型的实验性 8B 参数版本。\n\n使用 Gemini 需遵循 Google 的 [Gemini 使用条款](https://ai.google.dev/terms)。\n\n#multimodal\n\n注意：该模型目前处于实验阶段，不适合生产使用案例，可能会受到严格的速率限制。\n\n"},{"lang":"zh","group":"models","slug":"models/gemini-flash-15-8b","frontmatter":{"title":"Google: Gemini 1.5 Flash-8B","meta_title":"Google: Gemini 1.5 Flash-8B","description":"Google: Gemini 1.5 Flash-8B","date":"2024-10-03T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text image 2 text"],"author":"google","tags":["Programming","Natural Language Processing","Chatbots","Translation","Technology/Web"],"draft":false,"id":"gemini-flash-1.5-8b","context":1000000,"input":3.75e-8,"output":1.5e-7,"img":0,"request":0,"last_updated":"2024-10-03T00:00:00.000Z","slug":"models/gemini-flash-15-8b"},"content":"\nGemini 1.5 Flash-8B 针对速度和效率进行了优化，在聊天、转录和翻译等小提示任务中提供了增强的性能。通过减少延迟，它在实时和大规模操作中非常有效。该模型专注于成本效益解决方案，同时保持高质量的结果。\n\n[点击此处了解更多关于此模型的信息](https://developers.googleblog.com/en/gemini-15-flash-8b-is-now-generally-available-for-use/)。\n\n使用 Gemini 受限于 Google's [Gemini 使用条款](https://ai.google.dev/terms)。\n\n"},{"lang":"zh","group":"models","slug":"models/gemini-flash-15","frontmatter":{"title":"Google: Gemini Flash 1.5","meta_title":"Google: Gemini Flash 1.5","description":"Google: Gemini Flash 1.5","date":"2024-05-14T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text image 2 text"],"author":"google","tags":["Programming","Machine Learning","Natural Language Processing","Computer Vision","Chatbots"],"draft":false,"id":"gemini-flash-1.5","context":1000000,"input":7.5e-8,"output":3e-7,"img":0.00004,"request":0,"last_updated":"2024-10-31T23:10:34.000Z","slug":"models/gemini-flash-15"},"content":"\nGemini 1.5 Flash 是一个基础模型，能够在多种多模态任务中表现良好，如视觉理解、分类、摘要，以及从图像、音频和视频中创建内容。它擅长处理视觉和文本输入，如照片、文档、信息图表和截图。\n\nGemini 1.5 Flash 旨在处理高容量、高频率的任务，在这些任务中，成本和延迟至关重要。在大多数常见任务中，Flash 的质量与其他 Gemini Pro 模型相当，但成本显著降低。Flash 非常适合聊天助手和按需内容生成等需要速度和规模的应用。\n\n使用 Gemini 需遵循 Google 的 [Gemini 使用条款](https://ai.google.dev/terms)。\n\n#multimodal\n\n"},{"lang":"zh","group":"models","slug":"models/gemini-pro-15","frontmatter":{"title":"Google: Gemini Pro 1.5","meta_title":"Google: Gemini Pro 1.5","description":"Google: Gemini Pro 1.5","date":"2024-04-09T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text image 2 text"],"author":"google","tags":["Programming","Natural Language Processing","Machine Learning","Generative AI","Chatbots"],"draft":false,"id":"gemini-pro-1.5","context":2000000,"input":0.00000125,"output":0.000005,"img":0.00263,"request":0,"last_updated":"2024-04-09T00:00:00.000Z","slug":"models/gemini-pro-15"},"content":"\n谷歌最新的多模态模型，支持在文本或聊天提示中使用图像和视频。\n\n针对以下语言任务进行了优化：\n\n- 代码生成\n- 文本生成\n- 文本编辑\n- 问题解决\n- 推荐\n- 信息提取\n- 数据提取或生成\n- AI代理\n\nGemini的使用受限于谷歌的[Gemini使用条款](https://ai.google.dev/terms)。\n\n#multimodal\n\n"},{"lang":"zh","group":"models","slug":"models/gemini-pro-vision","frontmatter":{"title":"Google: Gemini Pro Vision 1.0","meta_title":"Google: Gemini Pro Vision 1.0","description":"Google: Gemini Pro Vision 1.0","date":"2023-12-13T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text image 2 text"],"author":"google","tags":["Programming","Machine Learning","Natural Language Processing","Computer Vision","Generative AI"],"draft":false,"id":"gemini-pro-vision","context":16384,"input":5e-7,"output":0.0000015,"img":0.0025,"request":0,"last_updated":"2024-11-11T03:15:08.000Z","slug":"models/gemini-pro-vision"},"content":"\n谷歌的旗舰多模态模型，支持在文本或聊天提示中使用图像和视频，以获得文本或代码响应。\n\n请参阅来自 [Deepmind](https://deepmind.google/technologies/gemini/) 的基准和提示指南。\n\n使用 Gemini 需遵循谷歌的 [Gemini 使用条款](https://ai.google.dev/terms)。\n\n#multimodal\n\n"},{"lang":"zh","group":"models","slug":"models/gemma-2-27b-it","frontmatter":{"title":"Google: Gemma 2 27B","meta_title":"Google: Gemma 2 27B","description":"Google: Gemma 2 27B","date":"2024-07-13T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text 2 text"],"author":"google","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"gemma-2-27b-it","context":8192,"input":2.7e-7,"output":2.7e-7,"img":0,"request":0,"last_updated":"2024-11-11T03:14:49.000Z","slug":"models/gemma-2-27b-it"},"content":"\nGemma 2 27B 由 Google 开发，是一个开放模型，基于创建 [Gemini 模型](/models?q=gemini) 所使用的相同研究和技术。\n\nGemma 模型非常适合多种文本生成任务，包括问答、摘要和推理。\n\n有关更多详细信息，请参阅 [发布公告](https://blog.google/technology/developers/google-gemma-2/)。使用 Gemma 需遵循 Google 的 [Gemma 使用条款](https://ai.google.dev/gemma/terms)。\n\n"},{"lang":"zh","group":"models","slug":"models/gemma-2-9b-it","frontmatter":{"title":"Google: Gemma 2 9B","meta_title":"Google: Gemma 2 9B","description":"Google: Gemma 2 9B","date":"2024-06-28T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text 2 text"],"author":"google","tags":["Programming","Natural Language Processing","Machine Learning","Data Science","Open Source"],"draft":false,"id":"gemma-2-9b-it","context":8192,"input":6e-8,"output":6e-8,"img":0,"request":0,"last_updated":"2024-11-11T03:14:49.000Z","slug":"models/gemma-2-9b-it"},"content":"\nGemma 2 9B by Google 是一个先进的开源语言模型，在其尺寸类别中设定了效率和性能的新标准。\n\n它旨在支持各种任务，使开发者和研究人员能够构建创新应用，同时保持可访问性、安全性和经济性。\n\n有关更多详细信息，请参见 [launch announcement](https://blog.google/technology/developers/google-gemma-2/)。使用 Gemma 需遵循 Google's [Gemma Terms of Use](https://ai.google.dev/gemma/terms)。\n\n"},{"lang":"zh","group":"models","slug":"models/gpt-35-turbo-instruct","frontmatter":{"title":"OpenAI: GPT-3.5 Turbo Instruct","meta_title":"OpenAI: GPT-3.5 Turbo Instruct","description":"OpenAI: GPT-3.5 Turbo Instruct","date":"2023-09-28T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"openai","tags":["Programming","Natural Language Processing","Generative AI","Chatbots","Technology/Web"],"draft":false,"id":"gpt-3.5-turbo-instruct","context":4095,"input":0.0000015,"output":0.000002,"img":0,"request":0,"last_updated":"2023-09-28T00:00:00.000Z","slug":"models/gpt-35-turbo-instruct"},"content":"\n该模型是GPT-3.5 Turbo的一个变体，针对教学提示进行了调整，并省略了与聊天相关的优化。训练数据：截至2021年9月。\n\n"},{"lang":"zh","group":"models","slug":"models/gpt-4o-mini","frontmatter":{"title":"OpenAI: GPT-4o-mini","meta_title":"OpenAI: GPT-4o-mini","description":"OpenAI: GPT-4o-mini","date":"2024-07-18T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text image 2 text"],"author":"openai","tags":["Programming","Technology","Programming/Scripting","Technology/Web"],"draft":false,"is_recommended":true,"id":"gpt-4o-mini","context":128000,"input":1.5e-7,"output":6e-7,"img":0.007225,"request":0,"last_updated":"2024-11-14T05:09:26.000Z","slug":"models/gpt-4o-mini"},"content":"\nGPT-4o mini 是 OpenAI 在 [GPT-4 Omni](/openai/gpt-4o) 之后推出的最新模型，支持文本和图像输入，并生成文本输出。\n\n作为他们最先进的小型模型，它的价格比其他最近的前沿模型便宜了许多，且比 [GPT-3.5 Turbo](/openai/gpt-3.5-turbo) 便宜超过 60%。它保持了 SOTA 智能，同时在成本效益上显著提高。\n\nGPT-4o mini 在 MMLU 上获得了 82% 的分数，目前在聊天偏好 [常见排行榜](https://arena.lmsys.org/) 上的排名高于 GPT-4。\n\n查看 [发布公告](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/) 以了解更多信息。\n\n"},{"lang":"zh","group":"models","slug":"models/gpt-4o","frontmatter":{"title":"OpenAI: GPT-4o","meta_title":"OpenAI: GPT-4o","description":"OpenAI: GPT-4o","date":"2024-05-13T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text image 2 text"],"author":"openai","tags":["Programming","Natural Language Processing","Machine Learning","Generative AI","Computer Vision"],"draft":false,"id":"gpt-4o","context":128000,"input":0.0000025,"output":0.00001,"img":0.0036125,"request":0,"last_updated":"2024-05-13T00:00:00.000Z","slug":"models/gpt-4o"},"content":"\nGPT-4o（“o”代表“全能”）是OpenAI最新的AI模型，支持文本和图像输入，并生成文本输出。它保持了[GPT-4 Turbo](/openai/gpt-4-turbo)的智能水平，同时速度提高了两倍，成本效益提高了50%。GPT-4o还在处理非英语语言和增强视觉能力方面提供了更好的性能。\n\n为了与其他模型进行基准测试，它曾被暂时称为[\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209)\n\n"},{"lang":"zh","group":"models","slug":"models/grok-beta","frontmatter":{"title":"xAI: Grok Beta","meta_title":"xAI: Grok Beta","description":"xAI: Grok Beta","date":"2024-10-20T00:00:00.000Z","image":"https://img.rifx.online/logo/xai.svg","categories":["text 2 text"],"author":"x-ai","tags":["Natural Language Processing","Machine Learning","Generative AI","Chatbots","Data Science"],"draft":false,"id":"grok-beta","context":131072,"input":0.000005,"output":0.000015,"img":0,"request":0,"last_updated":"2024-11-07T09:32:49.000Z","slug":"models/grok-beta"},"content":"\nGrok Beta 是 xAI 的实验性语言模型，具有最先进的推理能力，最适合复杂和多步骤的用例。\n\n它是 [Grok 2](https://x.ai/blog/grok-2) 的继任者，具有增强的上下文长度。\n\n"},{"lang":"zh","group":"models","slug":"models/hermes-3-llama-31-405b","frontmatter":{"title":"Nous: Hermes 3 405B Instruct","meta_title":"Nous: Hermes 3 405B Instruct","description":"Nous: Hermes 3 405B Instruct","date":"2024-08-16T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"nousresearch","tags":["Programming","Natural Language Processing","Machine Learning","Generative AI","Chatbots"],"draft":false,"id":"hermes-3-llama-3.1-405b","context":131072,"input":0.00000179,"output":0.00000249,"img":0,"request":0,"last_updated":"2024-11-11T03:16:40.000Z","slug":"models/hermes-3-llama-31-405b"},"content":"\nHermes 3 是一个通用语言模型，相较于 Hermes 2 有许多改进，包括先进的代理能力、显著更好的角色扮演、推理、多轮对话、长上下文一致性以及各方面的提升。\n\nHermes 3 405B 是 Llama-3.1 405B 基础模型的前沿级全参数微调，专注于将 LLM 与用户对齐，赋予终端用户强大的引导能力和控制权。\n\nHermes 3 系列在 Hermes 2 的能力基础上进行构建和扩展，包括更强大和可靠的函数调用和结构化输出能力、通用助手能力以及改进的代码生成技能。\n\n在通用能力方面，Hermes 3 与 Llama-3.1 指令模型具有竞争力，甚至在某些方面可能更优，两者之间的优缺点各有不同。\n\n"},{"lang":"zh","group":"models","slug":"models/hermes-3-llama-31-70b","frontmatter":{"title":"Nous: Hermes 3 70B Instruct","meta_title":"Nous: Hermes 3 70B Instruct","description":"Nous: Hermes 3 70B Instruct","date":"2024-08-18T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"nousresearch","tags":["Natural Language Processing","Machine Learning","Generative AI","Chatbots","Programming"],"draft":false,"id":"hermes-3-llama-3.1-70b","context":131072,"input":4e-7,"output":4e-7,"img":0,"request":0,"last_updated":"2024-11-11T03:16:38.000Z","slug":"models/hermes-3-llama-31-70b"},"content":"\nHermes 3 是一个通用语言模型，相较于 [Hermes 2](/nousresearch/nous-hermes-2-mistral-7b-dpo) 有许多改进，包括先进的代理能力，更好的角色扮演，推理，多轮对话，长上下文连贯性，以及各方面的改进。\n\nHermes 3 70B 是一个具有竞争力的微调版本，甚至可以说是 [Llama-3.1 70B 基础模型](/meta-llama/llama-3.1-70b-instruct) 的优越版本，专注于将 LLM 与用户对齐，赋予最终用户强大的引导能力和控制权。\n\nHermes 3 系列在 Hermes 2 的能力基础上进行构建和扩展，包括更强大和可靠的函数调用和结构化输出能力，通用助手能力，以及改进的代码生成技能。\n\n"},{"lang":"zh","group":"models","slug":"models/inflection-3-pi","frontmatter":{"title":"Inflection: Inflection 3 Pi","meta_title":"Inflection: Inflection 3 Pi","description":"Inflection: Inflection 3 Pi","date":"2024-10-11T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"inflection","tags":["Chatbots","Roleplay","Emotional Intelligence","Customer Support","Safety"],"draft":false,"id":"inflection-3-pi","context":8000,"input":0.0000025,"output":0.00001,"img":0,"request":0,"last_updated":"2024-11-07T10:11:48.000Z","slug":"models/inflection-3-pi"},"content":"\nInflection 3 Pi 的 [Pi](https://pi.ai) 聊天机器人，包含背景故事、情感智能、生产力和安全性。它在客户支持、角色扮演和情感智能等场景中表现出色。\n\n"},{"lang":"zh","group":"models","slug":"models/inflection-3-productivity","frontmatter":{"title":"Inflection: Inflection 3 Productivity","meta_title":"Inflection: Inflection 3 Productivity","description":"Inflection: Inflection 3 Productivity","date":"2024-10-11T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"inflection","tags":["Programming","Technology","Chatbots","Generative AI","Data Science"],"draft":false,"id":"inflection-3-productivity","context":8000,"input":0.0000025,"output":0.00001,"img":0,"request":0,"last_updated":"2024-11-07T10:11:56.000Z","slug":"models/inflection-3-productivity"},"content":"\nInflection 3 的生产力经过优化，以便更好地遵循指令。它更适合需要 JSON 输出或精确遵循提供的指导方针的任务。\n\n有关类似于 Pi 的情感智能，请参见 [Inflect 3 Pi](/inflection/inflection-3-pi)。\n\n有关更多详细信息，请参见 [Inflection 的公告](https://inflection.ai/blog/enterprise)。\n\n"},{"lang":"zh","group":"models","slug":"models/jamba-1-5-large","frontmatter":{"title":"AI21: Jamba 1.5 Large","meta_title":"AI21: Jamba 1.5 Large","description":"AI21: Jamba 1.5 Large","date":"2024-08-23T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"ai21","tags":["Programming","Technology","Machine Learning","Data Science","Generative AI"],"draft":false,"id":"jamba-1-5-large","context":256000,"input":0.000002,"output":0.000008,"img":0,"request":0,"last_updated":"2024-11-11T03:11:21.000Z","slug":"models/jamba-1-5-large"},"content":"\nJamba 1.5 Large 是 AI21 新一代开放模型家族的一部分，提供卓越的速度、效率和质量。\n\n它具有 256K 的有效上下文窗口，是开放模型中最长的，能够在文档摘要和分析等任务上提升性能。\n\n基于新颖的 SSM-Transformer 架构，它在基准测试中超越了更大的模型，如 Llama 3.1 70B，同时保持资源效率。\n\n阅读他们的 [announcement](https://www.ai21.com/blog/announcing-jamba-model-family) 以了解更多信息。\n\n"},{"lang":"zh","group":"models","slug":"models/jamba-1-5-mini","frontmatter":{"title":"AI21: Jamba 1.5 Mini","meta_title":"AI21: Jamba 1.5 Mini","description":"AI21: Jamba 1.5 Mini","date":"2024-08-23T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"ai21","tags":["Programming","Technology","Machine Learning","Natural Language Processing","Data Science"],"draft":false,"id":"jamba-1-5-mini","context":256000,"input":2e-7,"output":4e-7,"img":0,"request":0,"last_updated":"2024-11-11T03:11:12.000Z","slug":"models/jamba-1-5-mini"},"content":"\nJamba 1.5 Mini 是世界上首个生产级 Mamba 基础模型，结合了 SSM 和 Transformer 架构，具有 256K 的上下文窗口和高效率。\n\n它支持 9 种语言，并能够处理各种写作和分析任务，效果与类似的小模型相当或更好。\n\n该模型比以前的设计使用更少的计算机内存，并且在处理较长文本时速度更快。\n\n阅读他们的 [公告](https://www.ai21.com/blog/announcing-jamba-model-family) 以了解更多信息。\n\n"},{"lang":"zh","group":"models","slug":"models/l3-lunaris-8b","frontmatter":{"title":"Llama 3 8B Lunaris","meta_title":"Llama 3 8B Lunaris","description":"Llama 3 8B Lunaris","date":"2024-08-13T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"sao10k","tags":["Roleplay","Programming","Machine Learning","Natural Language Processing","Chatbots"],"draft":false,"id":"l3-lunaris-8b","context":8192,"input":0.000002,"output":0.000002,"img":0,"request":0,"last_updated":"2024-11-11T03:12:13.000Z","slug":"models/l3-lunaris-8b"},"content":"\nLunaris 8B 是一个多功能的通用和角色扮演模型，基于 Llama 3。它是多个模型的战略合并，旨在平衡创造力与改进的逻辑和一般知识。\n\n由 [Sao10k](https://huggingface.co/Sao10k) 创建，该模型旨在提供比 Stheno v3.2 更好的体验，具有增强的创造力和逻辑推理能力。\n\n为了获得最佳效果，请使用 Llama 3 Instruct 上下文模板，温度 1.4，min_p 0.1。\n\n"},{"lang":"zh","group":"models","slug":"models/l31-euryale-70b","frontmatter":{"title":"Llama 3.1 Euryale 70B v2.2","meta_title":"Llama 3.1 Euryale 70B v2.2","description":"Llama 3.1 Euryale 70B v2.2","date":"2024-08-28T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"sao10k","tags":["Roleplay","Generative AI","Chatbots","Natural Language Processing","Technology/Web"],"draft":false,"id":"l3.1-euryale-70b","context":8192,"input":3.5e-7,"output":4e-7,"img":0,"request":0,"last_updated":"2024-11-11T03:12:50.000Z","slug":"models/l31-euryale-70b"},"content":"\nEuryale L3.1 70B v2.2 是一个专注于创意角色扮演的模型，来自 [Sao10k](https://ko-fi.com/sao10k)。它是 [Euryale L3 70B v2.1](/sao10k/l3-euryale-70b) 的继任者。\n\n"},{"lang":"zh","group":"models","slug":"models/lfm-40b","frontmatter":{"title":"Liquid: LFM 40B MoE","meta_title":"Liquid: LFM 40B MoE","description":"Liquid: LFM 40B MoE","date":"2024-09-30T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"liquid","tags":["Machine Learning","Natural Language Processing","Data Science","Generative AI","Computer Vision"],"draft":false,"is_recommended":true,"id":"lfm-40b","context":32768,"input":0.000001,"output":0.000002,"img":0,"request":0,"last_updated":"2024-11-14T05:10:16.000Z","slug":"models/lfm-40b"},"content":"\nLiquid的40.3B专家混合（MoE）模型。Liquid基础模型（LFMs）是基于动态系统构建的大型神经网络。\n\nLFMs是通用的AI模型，可以用于建模任何类型的序列数据，包括视频、音频、文本、时间序列和信号。\n\n有关基准和更多信息，请参见[发布公告](https://www.liquid.ai/liquid-foundation-models)。\n\n"},{"lang":"zh","group":"models","slug":"models/liquid-lfm-40b:free","frontmatter":{"title":"Liquid: LFM 40B MoE (free)","meta_title":"Liquid: LFM 40B MoE (free)","description":"Liquid: LFM 40B MoE (free)","date":"2024-09-30T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"liquid","tags":["Generative AI","Machine Learning","Natural Language Processing","Data Science","Technology/Web"],"draft":false,"id":"liquid/lfm-40b:free","context":8192,"input":0,"output":0,"img":0,"request":0,"last_updated":"2024-11-07T00:17:57.000Z","slug":"models/liquid-lfm-40b:free"},"content":"\nLiquid的40.3B专家混合模型（MoE）。Liquid基础模型（LFMs）是基于动态系统构建的大型神经网络。\n\nLFMs是通用的AI模型，可以用于建模任何类型的序列数据，包括视频、音频、文本、时间序列和信号。\n\n有关基准测试和更多信息，请参见[发布公告](https://www.liquid.ai/liquid-foundation-models)。\n\n_这些是[LFM 40B MoE](/liquid/lfm-40b)的免费限流端点。输出可能会被缓存。有关速率限制的信息，请[在这里](/docs/limits)阅读。_\n\n"},{"lang":"zh","group":"models","slug":"models/llama-31-70b-instruct","frontmatter":{"title":"Meta: Llama 3.1 70B Instruct","meta_title":"Meta: Llama 3.1 70B Instruct","description":"Meta: Llama 3.1 70B Instruct","date":"2024-07-23T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text 2 text"],"author":"meta-llama","tags":["Programming","Machine Learning","Natural Language Processing","Chatbots","Ethics"],"draft":false,"id":"llama-3.1-70b-instruct","context":131072,"input":3e-7,"output":3e-7,"img":0,"request":0,"last_updated":"2024-10-28T13:38:49.000Z","slug":"models/llama-31-70b-instruct"},"content":"\nMeta最新发布的模型类别（Llama 3.1）推出了多种尺寸和版本。这个70B的指令调优版本针对高质量对话用例进行了优化。\n\n在人工评估中，它与领先的闭源模型相比表现出色。\n\n使用该模型需遵循[Meta的可接受使用政策](https://www.llama.com/llama3/use-policy/)。\n\n"},{"lang":"zh","group":"models","slug":"models/llama-31-8b-instruct","frontmatter":{"title":"Meta: Llama 3.1 8B Instruct","meta_title":"Meta: Llama 3.1 8B Instruct","description":"Meta: Llama 3.1 8B Instruct","date":"2024-07-23T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text 2 text"],"author":"meta-llama","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Ethics"],"draft":false,"id":"llama-3.1-8b-instruct","context":131072,"input":5.5e-8,"output":5.5e-8,"img":0,"request":0,"last_updated":"2024-10-31T23:27:09.000Z","slug":"models/llama-31-8b-instruct"},"content":"\nMeta最新发布的模型系列（Llama 3.1）推出了多种尺寸和版本。这个8B指令调优版本快速且高效。\n\n与领先的闭源模型相比，它在人工评估中表现出色。\n\n使用此模型需遵循[Meta的可接受使用政策](https://www.llama.com/llama3/use-policy/)。\n\n"},{"lang":"zh","group":"models","slug":"models/llama-31-lumimaid-70b","frontmatter":{"title":"Lumimaid v0.2 70B","meta_title":"Lumimaid v0.2 70B","description":"Lumimaid v0.2 70B","date":"2024-10-22T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"neversleep","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Ethics"],"draft":false,"id":"llama-3.1-lumimaid-70b","context":131072,"input":0.000003375,"output":0.0000045,"img":0,"request":0,"last_updated":"2024-11-11T03:03:31.000Z","slug":"models/llama-31-lumimaid-70b"},"content":"\nLumimaid v0.2 70B 是对 [Llama 3.1 70B](/meta-llama/llama-3.1-70b-instruct) 的微调，与 Lumimaid v0.1 相比，在数据集方面有了“巨大的提升”。不合格的聊天输出已被清除。\n\n使用此模型须遵循 [Meta 的可接受使用政策](https://llama.meta.com/llama3/use-policy/)。\n\n"},{"lang":"zh","group":"models","slug":"models/llama-31-lumimaid-8b","frontmatter":{"title":"Lumimaid v0.2 8B","meta_title":"Lumimaid v0.2 8B","description":"Lumimaid v0.2 8B","date":"2024-09-15T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"neversleep","tags":["Programming","Machine Learning","Natural Language Processing","Chatbots","Ethics"],"draft":false,"id":"llama-3.1-lumimaid-8b","context":131072,"input":1.875e-7,"output":0.000001125,"img":0,"request":0,"last_updated":"2024-11-11T03:10:19.000Z","slug":"models/llama-31-lumimaid-8b"},"content":"\nLumimaid v0.2 8B 是对 [Llama 3.1 8B](/meta-llama/llama-3.1-8b-instruct) 的微调，相较于 Lumimaid v0.1，数据集有了“巨大的提升”。不当的聊天输出已被清除。\n\n使用此模型须遵循 [Meta 的可接受使用政策](https://llama.meta.com/llama3/use-policy/)。\n\n"},{"lang":"zh","group":"models","slug":"models/llama-31-nemotron-70b-instruct","frontmatter":{"title":"Nvidia: Llama 3.1 Nemotron 70B Instruct","meta_title":"Nvidia: Llama 3.1 Nemotron 70B Instruct","description":"Nvidia: Llama 3.1 Nemotron 70B Instruct","date":"2024-10-15T00:00:00.000Z","image":"https://img.rifx.online/logo/nvidia.svg","categories":["text 2 text"],"author":"nvidia","tags":["Programming","Natural Language Processing","Machine Learning","Generative AI","Ethics"],"draft":false,"id":"llama-3.1-nemotron-70b-instruct","context":131072,"input":3.5e-7,"output":4e-7,"img":0,"request":0,"last_updated":"2024-10-15T00:00:00.000Z","slug":"models/llama-31-nemotron-70b-instruct"},"content":"\nNVIDIA的Llama 3.1 Nemotron 70B是一个旨在生成精确和有用响应的语言模型。利用[Llama 3.1 70B](/models/meta-llama/llama-3.1-70b-instruct)架构和基于人类反馈的强化学习（RLHF），它在自动对齐基准测试中表现出色。该模型专为需要高准确性以提供帮助和生成响应的应用而设计，适合处理多个领域的多样用户查询。\n\n使用此模型须遵循[Meta的可接受使用政策](https://www.llama.com/llama3/use-policy/)。\n\n"},{"lang":"zh","group":"models","slug":"models/llama-31-sonar-huge-128k-online","frontmatter":{"title":"Perplexity: Llama 3.1 Sonar 405B Online","meta_title":"Perplexity: Llama 3.1 Sonar 405B Online","description":"Perplexity: Llama 3.1 Sonar 405B Online","date":"2024-08-14T00:00:00.000Z","image":"https://img.rifx.online/logo/perplexity.svg","categories":["text 2 text"],"author":"perplexity","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"llama-3.1-sonar-huge-128k-online","context":127072,"input":0.000005,"output":0.000005,"img":0,"request":0.005,"last_updated":"2024-11-07T09:36:38.000Z","slug":"models/llama-31-sonar-huge-128k-online"},"content":"\nLlama 3.1 Sonar 是 Perplexity 最新的模型系列。它在成本效益、速度和性能上超越了他们早期的 Sonar 模型。该模型基于 Llama 3.1 405B，并具有互联网访问功能。\n\n"},{"lang":"zh","group":"models","slug":"models/llama-31-sonar-large-128k-online","frontmatter":{"title":"Perplexity: Llama 3.1 Sonar 70B Online","meta_title":"Perplexity: Llama 3.1 Sonar 70B Online","description":"Perplexity: Llama 3.1 Sonar 70B Online","date":"2024-08-01T00:00:00.000Z","image":"https://img.rifx.online/logo/perplexity.svg","categories":["text 2 text"],"author":"perplexity","tags":["Programming","Machine Learning","Natural Language Processing","Chatbots","Generative AI"],"draft":false,"id":"llama-3.1-sonar-large-128k-online","context":127072,"input":0.000001,"output":0.000001,"img":0,"request":0.005,"last_updated":"2024-11-07T09:37:21.000Z","slug":"models/llama-31-sonar-large-128k-online"},"content":"\nLlama 3.1 Sonar 是 Perplexity 最新的模型系列。它在成本效益、速度和性能方面超越了他们早期的 Sonar 模型。\n\n这是 [离线聊天模型](/perplexity/llama-3.1-sonar-large-128k-chat) 的在线版本。它专注于提供有帮助、最新和真实的响应。 #online\n\n"},{"lang":"zh","group":"models","slug":"models/llama-31-sonar-small-128k-online","frontmatter":{"title":"Perplexity: Llama 3.1 Sonar 8B Online","meta_title":"Perplexity: Llama 3.1 Sonar 8B Online","description":"Perplexity: Llama 3.1 Sonar 8B Online","date":"2024-08-01T00:00:00.000Z","image":"https://img.rifx.online/logo/perplexity.svg","categories":["text 2 text"],"author":"perplexity","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"llama-3.1-sonar-small-128k-online","context":127072,"input":2e-7,"output":2e-7,"img":0,"request":0.005,"last_updated":"2024-11-07T09:38:09.000Z","slug":"models/llama-31-sonar-small-128k-online"},"content":"\nLlama 3.1 Sonar 是 Perplexity 最新的模型系列。它在成本效率、速度和性能上超越了他们早期的 Sonar 模型。\n\n这是 [离线聊天模型](/perplexity/llama-3.1-sonar-small-128k-chat) 的在线版本。它专注于提供有用、最新和真实的响应。 #online\n\n"},{"lang":"zh","group":"models","slug":"models/llama-32-11b-vision-instruct","frontmatter":{"title":"Meta: Llama 3.2 11B Vision Instruct","meta_title":"Meta: Llama 3.2 11B Vision Instruct","description":"Meta: Llama 3.2 11B Vision Instruct","date":"2024-09-25T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text image 2 text"],"author":"meta-llama","tags":["Natural Language Processing","Computer Vision","Machine Learning","Generative AI","Data Science"],"draft":false,"is_recommended":true,"id":"llama-3.2-11b-vision-instruct","context":131072,"input":5.5e-8,"output":5.5e-8,"img":0.000079475,"request":0,"last_updated":"2024-11-14T05:10:41.000Z","slug":"models/llama-32-11b-vision-instruct"},"content":"\nLlama 3.2 11B Vision 是一个具有 110 亿参数的多模态模型，旨在处理结合视觉和文本数据的任务。它在图像描述和视觉问答等任务中表现出色，弥合了语言生成与视觉推理之间的鸿沟。该模型在大量图像-文本对的数据集上进行了预训练，能够在复杂的高精度图像分析中表现良好。\n\n它将视觉理解与语言处理相结合的能力，使其成为需要全面视觉语言 AI 应用的行业的理想解决方案，例如内容创作、AI 驱动的客户服务和研究。\n\n点击此处查看 [原始模型卡](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md)。\n\n使用此模型需遵守 [Meta 的可接受使用政策](https://www.llama.com/llama3/use-policy/)。\n\n"},{"lang":"zh","group":"models","slug":"models/llama-32-1b-instruct","frontmatter":{"title":"Meta: Llama 3.2 1B Instruct","meta_title":"Meta: Llama 3.2 1B Instruct","description":"Meta: Llama 3.2 1B Instruct","date":"2024-09-25T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text 2 text"],"author":"meta-llama","tags":["Natural Language Processing","Programming","Technology","Machine Learning","Generative AI"],"draft":false,"id":"llama-3.2-1b-instruct","context":131072,"input":1e-8,"output":2e-8,"img":0,"request":0,"last_updated":"2024-09-25T00:00:00.000Z","slug":"models/llama-32-1b-instruct"},"content":"\nLlama 3.2 1B 是一个拥有10亿参数的语言模型，专注于高效执行自然语言任务，如摘要、对话和多语言文本分析。其较小的规模使其能够在低资源环境中高效运行，同时保持强大的任务性能。\n\n支持八种核心语言，并可针对更多语言进行微调，Llama 1.3B 非常适合寻求轻量级但强大的 AI 解决方案的企业或开发者，这些解决方案能够在多样化的多语言环境中运行，而不需要大型模型的高计算需求。\n\n点击此处查看 [原始模型卡](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md)。\n\n使用此模型需遵循 [Meta 的可接受使用政策](https://www.llama.com/llama3/use-policy/)。\n\n"},{"lang":"zh","group":"models","slug":"models/llama-32-3b-instruct","frontmatter":{"title":"Meta: Llama 3.2 3B Instruct","meta_title":"Meta: Llama 3.2 3B Instruct","description":"Meta: Llama 3.2 3B Instruct","date":"2024-09-25T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text 2 text"],"author":"meta-llama","tags":["Natural Language Processing","Machine Learning","Generative AI","Chatbots","Multilingual"],"draft":false,"id":"llama-3.2-3b-instruct","context":131072,"input":3e-8,"output":5e-8,"img":0,"request":0,"last_updated":"2024-11-11T03:09:59.000Z","slug":"models/llama-32-3b-instruct"},"content":"\nLlama 3.2 3B 是一个拥有 30 亿参数的多语言大型语言模型，针对对话生成、推理和摘要等高级自然语言处理任务进行了优化。它采用最新的 transformer 架构，支持包括英语、西班牙语和印地语在内的八种语言，并且可以适应其他语言。\n\nLlama 3.2B 模型在 9 万亿个标记上进行训练，擅长遵循指令、复杂推理和工具使用。其平衡的性能使其非常适合需要在多语言环境中进行文本生成时的准确性和效率的应用。\n\nClick here for the [original model card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md).\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).\n\n"},{"lang":"zh","group":"models","slug":"models/llama-32-90b-vision-instruct","frontmatter":{"title":"Meta: Llama 3.2 90B Vision Instruct","meta_title":"Meta: Llama 3.2 90B Vision Instruct","description":"Meta: Llama 3.2 90B Vision Instruct","date":"2024-09-25T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text image 2 text"],"author":"meta-llama","tags":["Natural Language Processing","Computer Vision","Machine Learning","Data Science","Generative AI"],"draft":false,"id":"llama-3.2-90b-vision-instruct","context":131072,"input":3.5e-7,"output":4e-7,"img":0.00050575,"request":0,"last_updated":"2024-09-25T00:00:00.000Z","slug":"models/llama-32-90b-vision-instruct"},"content":"\nLlama 90B Vision模型是一款顶级的90亿参数多模态模型，旨在应对最具挑战性的视觉推理和语言任务。它在图像描述、视觉问答和高级图像-文本理解方面提供无与伦比的准确性。该模型在庞大的多模态数据集上进行预训练，并通过人类反馈进行微调，专为处理最苛刻的基于图像的AI任务而设计。\n\n此模型非常适合需要尖端多模态AI能力的行业，尤其是那些处理复杂实时视觉和文本分析的行业。\n\n点击此处查看[原始模型卡片](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md)。\n\n使用此模型需遵守[Meta的可接受使用政策](https://www.llama.com/llama3/use-policy/)。\n\n"},{"lang":"zh","group":"models","slug":"models/lzlv-70b-fp16-hf","frontmatter":{"title":"lzlv 70B","meta_title":"lzlv 70B","description":"lzlv 70B","date":"2023-11-12T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"lizpreciatior","tags":["Roleplay","Programming","Machine Learning","Generative AI","Chatbots"],"draft":false,"id":"lzlv-70b-fp16-hf","context":4096,"input":3.5e-7,"output":4e-7,"img":0,"request":0,"last_updated":"2024-11-04T12:50:34.000Z","slug":"models/lzlv-70b-fp16-hf"},"content":"\nA Mythomax/MLewd_13B风格的选定70B模型合并。\n一个多模型合并，结合了多个LLaMA2 70B微调模型，用于角色扮演和创意工作。目标是创建一个将创造力与智能相结合的模型，以提升体验。\n\n#merge #uncensored\n\n"},{"lang":"zh","group":"models","slug":"models/magnum-v2-72b","frontmatter":{"title":"Magnum v2 72B","meta_title":"Magnum v2 72B","description":"Magnum v2 72B","date":"2024-09-30T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"anthracite-org","tags":["Roleplay","Programming","Machine Learning","Natural Language Processing","Generative AI"],"draft":false,"id":"magnum-v2-72b","context":32768,"input":0.00000375,"output":0.0000045,"img":0,"request":0,"last_updated":"2024-11-11T03:09:19.000Z","slug":"models/magnum-v2-72b"},"content":"\n来自[Goliath](https://openrouter.ai/alpindale/goliath-120b)的制造商，Magnum 72B是第七个旨在达到Claude 3模型的散文质量的模型系列，特别是Opus和Sonnet。\n\n该模型基于[Qwen2 72B](https://openrouter.ai/qwen/qwen-2-72b-instruct)，并使用5500万个高度策划的角色扮演(RP)数据进行训练。\n\n"},{"lang":"zh","group":"models","slug":"models/magnum-v4-72b","frontmatter":{"title":"Magnum v4 72B","meta_title":"Magnum v4 72B","description":"Magnum v4 72B","date":"2024-10-22T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"anthracite-org","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"magnum-v4-72b","context":32768,"input":0.000001875,"output":0.00000225,"img":0,"request":0,"last_updated":"2024-11-04T12:39:55.000Z","slug":"models/magnum-v4-72b"},"content":"\n这是一个系列模型，旨在复制Claude 3模型的散文质量，特别是Sonnet和Opus。\n\n该模型是在[Qwen2.5 72B]s的基础上进行微调的。\n\n"},{"lang":"zh","group":"models","slug":"models/ministral-3b","frontmatter":{"title":"Ministral 3B","meta_title":"Ministral 3B","description":"Ministral 3B","date":"2024-10-17T00:00:00.000Z","image":"https://img.rifx.online/logo/mistral.png","categories":["text 2 text"],"author":"mistralai","tags":["Programming","Machine Learning","Natural Language Processing","Data Science","Generative AI"],"draft":false,"id":"ministral-3b","context":128000,"input":4e-8,"output":4e-8,"img":0,"request":0,"last_updated":"2024-11-07T00:24:37.000Z","slug":"models/ministral-3b"},"content":"\nMinistral 3B 是一个针对设备和边缘计算优化的 3B 参数模型。它在知识、常识推理和函数调用方面表现出色，在大多数基准测试中超越了像 Mistral 7B 这样的更大模型。支持最长 128k 的上下文长度，非常适合高效推理的代理工作流和专业任务的协调。\n\n"},{"lang":"zh","group":"models","slug":"models/ministral-8b","frontmatter":{"title":"Ministral 8B","meta_title":"Ministral 8B","description":"Ministral 8B","date":"2024-10-17T00:00:00.000Z","image":"https://img.rifx.online/logo/mistral.png","categories":["text 2 text"],"author":"mistralai","tags":["Technology","Machine Learning","Data Science","Generative AI","Ethics"],"draft":false,"id":"ministral-8b","context":128000,"input":1e-7,"output":1e-7,"img":0,"request":0,"last_updated":"2024-10-19T04:54:11.000Z","slug":"models/ministral-8b"},"content":"\nMinistral 8B 是一个具有 8B 参数的模型，采用独特的交错滑动窗口注意力模式，以实现更快、更节省内存的推理。该模型专为边缘使用案例设计，支持最长 128k 的上下文长度，并在知识和推理任务中表现出色。它在低于 10B 的类别中优于同类产品，非常适合低延迟、注重隐私的应用。\n\n"},{"lang":"zh","group":"models","slug":"models/mistral-7b-instruct","frontmatter":{"title":"Mistral: Mistral 7B Instruct","meta_title":"Mistral: Mistral 7B Instruct","description":"Mistral: Mistral 7B Instruct","date":"2024-05-27T00:00:00.000Z","image":"https://img.rifx.online/logo/mistral.png","categories":["text 2 text"],"author":"mistralai","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"mistral-7b-instruct","context":32768,"input":5.5e-8,"output":5.5e-8,"img":0,"request":0,"last_updated":"2024-10-31T23:13:12.000Z","slug":"models/mistral-7b-instruct"},"content":"\n一个高性能、行业标准的7.3B参数模型，针对速度和上下文长度进行了优化。\n\n*Mistral 7B Instruct有多个版本变体，本文旨在介绍最新版本。*\n\n"},{"lang":"zh","group":"models","slug":"models/mistral-nemo","frontmatter":{"title":"Mistral: Mistral Nemo","meta_title":"Mistral: Mistral Nemo","description":"Mistral: Mistral Nemo","date":"2024-07-19T00:00:00.000Z","image":"https://img.rifx.online/logo/mistral.png","categories":["text 2 text"],"author":"mistralai","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Data Science"],"draft":false,"id":"mistral-nemo","context":128000,"input":1.3e-7,"output":1.3e-7,"img":0,"request":0,"last_updated":"2024-10-31T23:10:58.000Z","slug":"models/mistral-nemo"},"content":"\n由Mistral与NVIDIA合作构建的12B参数模型，具有128k的上下文长度。\n\n该模型是多语言的，支持英语、法语、德语、西班牙语、意大利语、葡萄牙语、中文、日语、韩语、阿拉伯语和印地语。\n\n它支持函数调用，并在Apache 2.0许可证下发布。\n\n"},{"lang":"zh","group":"models","slug":"models/mistral-tiny","frontmatter":{"title":"Mistral Tiny","meta_title":"Mistral Tiny","description":"Mistral Tiny","date":"2024-01-10T00:00:00.000Z","image":"https://img.rifx.online/logo/mistral.png","categories":["text 2 text"],"author":"mistralai","tags":["Programming","Machine Learning","Data Science","Generative AI","Chatbots"],"draft":false,"id":"mistral-tiny","context":32000,"input":2.5e-7,"output":2.5e-7,"img":0,"request":0,"last_updated":"2024-10-31T23:12:22.000Z","slug":"models/mistral-tiny"},"content":"\n该模型目前由 Mistral-7B-v0.2 驱动，并结合了比 [Mistral 7B](/mistralai/mistral-7b-instruct-v0.1) 更“优越”的微调，灵感来自社区的工作。它最适合用于大批量处理任务，在这些任务中，成本是一个重要因素，但推理能力并不是关键。\n\n"},{"lang":"zh","group":"models","slug":"models/mn-inferor-12b","frontmatter":{"title":"Mistral Nemo Inferor 12B","meta_title":"Mistral Nemo Inferor 12B","description":"Mistral Nemo Inferor 12B","date":"2024-11-13T02:20:28.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"infermatic","tags":["Roleplay","Programming","Machine Learning","Natural Language Processing","Generative AI"],"draft":false,"id":"mn-inferor-12b","context":32000,"input":2.5e-7,"output":5e-7,"img":0,"request":0,"last_updated":"2024-11-14T02:10:35.000Z","slug":"models/mn-inferor-12b"},"content":"\nInferor 是顶级角色扮演模型的合并体，专注于沉浸式叙事和故事讲述。\n\n该模型使用 [Model Stock](https://arxiv.org/abs/2403.19522) 合并方法，基于 [anthracite-org/magnum-v4-12b](https://openrouter.ai/anthracite-org/magnum-v4-72b) 进行合并。\n\n"},{"lang":"zh","group":"models","slug":"models/mn-starcannon-12b","frontmatter":{"title":"Mistral Nemo 12B Starcannon","meta_title":"Mistral Nemo 12B Starcannon","description":"Mistral Nemo 12B Starcannon","date":"2024-08-13T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"aetherwiing","tags":["Roleplay","Programming","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"mn-starcannon-12b","context":12000,"input":0.000002,"output":0.000002,"img":0,"request":0,"last_updated":"2024-11-11T03:16:53.000Z","slug":"models/mn-starcannon-12b"},"content":"\nStarcannon 12B 是一个创意角色扮演和故事写作模型，基于 [nothingiisreal/mn-celeste-12b](https://openrouter.ai/nothingiisreal/mn-celeste-12b) 并使用 [intervitens/mini-magnum-12b-v1.1](https://huggingface.co/intervitens/mini-magnum-12b-v1.1) 合并，采用 [TIES](https://arxiv.org/abs/2306.01708) 方法。\n\n虽然整体上更类似于 Magnum，但该模型仍然非常具有创意，写作风格愉悦。推荐给那些希望获得比 Magnum 更多变化，同时又希望比 Celeste 更冗长的散文的人。\n\n"},{"lang":"zh","group":"models","slug":"models/mythomax-l2-13b","frontmatter":{"title":"MythoMax 13B","meta_title":"MythoMax 13B","description":"MythoMax 13B","date":"2023-07-02T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"gryphe","tags":["Roleplay","Programming","Machine Learning","Natural Language Processing","Generative AI"],"draft":false,"id":"mythomax-l2-13b","context":4096,"input":1e-7,"output":1e-7,"img":0,"request":0,"last_updated":"2024-10-28T13:10:41.000Z","slug":"models/mythomax-l2-13b"},"content":"\nLlama 2 13B 的性能最高且最受欢迎的微调之一，具有丰富的描述和角色扮演。 #merge\n\n"},{"lang":"zh","group":"models","slug":"models/o1-mini","frontmatter":{"title":"OpenAI: o1-mini","meta_title":"OpenAI: o1-mini","description":"OpenAI: o1-mini","date":"2024-09-12T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"openai","tags":["Programming","Science","Natural Language Processing","Machine Learning","Data Science"],"draft":false,"id":"o1-mini","context":128000,"input":0.000003,"output":0.000012,"img":0,"request":0,"last_updated":"2024-09-12T00:00:00.000Z","slug":"models/o1-mini"},"content":"\nOpenAI最新且最强大的模型系列o1旨在在响应之前花更多时间思考。\n\no1模型经过优化，适用于数学、科学、编程及其他STEM相关任务。它们在物理、化学和生物学的基准测试中始终展现出博士级的准确性。了解更多信息，请查看[发布公告](https://openai.com/o1)。\n\n注意：该模型目前处于实验阶段，不适合生产使用，并可能受到严格的速率限制。\n\n"},{"lang":"zh","group":"models","slug":"models/o1-preview","frontmatter":{"title":"OpenAI: o1-preview","meta_title":"OpenAI: o1-preview","description":"OpenAI: o1-preview","date":"2024-09-12T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"openai","tags":["Programming","Science","Natural Language Processing","Machine Learning","Data Science"],"draft":false,"id":"o1-preview","context":128000,"input":0.000015,"output":0.00006,"img":0,"request":0,"last_updated":"2024-09-12T00:00:00.000Z","slug":"models/o1-preview"},"content":"\nOpenAI最新且最强大的模型系列o1旨在在响应之前花更多时间思考。\n\no1模型经过优化，适用于数学、科学、编程和其他STEM相关任务。它们在物理、化学和生物学的基准测试中始终表现出博士级的准确性。有关更多信息，请参阅[发布公告](https://openai.com/o1)。\n\n注意：该模型目前处于实验阶段，不适合生产使用案例，并且可能会受到严格的速率限制。\n\n"},{"lang":"zh","group":"models","slug":"models/openai-gpt-4o-mini","frontmatter":{"title":"OpenAI: GPT-4o-Mini Official","meta_title":"OpenAI: GPT-4o-Mini Official","description":"OpenAI: GPT-4o-Mini Official","date":"2024-10-26T09:00:07.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"gpt-4o-mini","tags":["Generative AI","Natural Language Processing","Machine Learning","Technology","Chatbots"],"draft":false,"is_recommended":false,"id":"openai/gpt-4o-mini","context":"None","input":1.5e-7,"output":6e-7,"img":0,"request":0,"last_updated":"2024-11-14T05:09:13.000Z","slug":"models/openai-gpt-4o-mini"},"content":"\nGPT-4o mini 是 OpenAI 在 [GPT-4 Omni](/openai/gpt-4o) 之后发布的最新模型，支持文本和图像输入并输出文本。\n\n作为其最先进的小型模型，它的价格比其他最近的前沿模型低很多倍，比 [GPT-3.5 Turbo](/openai/gpt-3.5-turbo) 便宜超过 60%。它保持了最先进的智能，同时在成本效益上显著更高。\n\nGPT-4o mini 在 MMLU 上取得了 82% 的得分，目前在聊天偏好 [常见排行榜](https://arena.lmsys.org/) 上的排名高于 GPT-4。\n\n查看 [发布公告](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/) 以了解更多信息。\n\n"},{"lang":"zh","group":"models","slug":"models/openai-gpt-4o","frontmatter":{"title":"OpenAI: GPT-4o Official","meta_title":"OpenAI: GPT-4o Official","description":"OpenAI: GPT-4o Official","date":"2024-11-14T02:53:29.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"gpt-4o","tags":["Generative AI","Natural Language Processing","Technology","Chatbots","Machine Learning"],"draft":false,"is_recommended":false,"id":"openai/gpt-4o","context":"None","input":0.0000025,"output":0.00001,"img":0,"request":0,"last_updated":"2024-11-14T02:58:04.000Z","slug":"models/openai-gpt-4o"},"content":"\nGPT-4o（“o”代表“全能”）是OpenAI最新的AI模型，支持文本和图像输入，并以文本形式输出。它保持了[GPT-4 Turbo](/openai/gpt-4-turbo)的智能水平，同时速度快了两倍，成本效益提高了50%。GPT-4o在处理非英语语言方面的性能也有所改善，并增强了视觉能力。\n\n为了与其他模型进行基准测试，它曾被称为[\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209)\n\n"},{"lang":"zh","group":"models","slug":"models/openchat-7b","frontmatter":{"title":"OpenChat 3.5 7B","meta_title":"OpenChat 3.5 7B","description":"OpenChat 3.5 7B","date":"2023-11-28T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"openchat","tags":["Programming","Natural Language Processing","Machine Learning","Open Source","Generative AI"],"draft":false,"id":"openchat-7b","context":8192,"input":5.5e-8,"output":5.5e-8,"img":0,"request":0,"last_updated":"2024-11-07T09:39:42.000Z","slug":"models/openchat-7b"},"content":"\nOpenChat 7B 是一个开源语言模型库，采用“C-RLFT（条件强化学习微调）”策略进行了微调，该策略受到离线强化学习的启发。它在没有偏好标签的混合质量数据上进行了训练。\n\n- 对于在 Mistral 7B 上微调的 OpenChat，请查看 [OpenChat 7B](/openchat/openchat-7b)。\n- 对于在 Llama 8B 上微调的 OpenChat，请查看 [OpenChat 8B](/openchat/openchat-8b)。\n\n#open-source\n\n"},{"lang":"zh","group":"models","slug":"models/palm-2-chat-bison-32k","frontmatter":{"title":"Google: PaLM 2 Chat 32k","meta_title":"Google: PaLM 2 Chat 32k","description":"Google: PaLM 2 Chat 32k","date":"2023-11-03T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text 2 text"],"author":"google","tags":["Natural Language Processing","Programming","Technology","Chatbots","Generative AI"],"draft":false,"id":"palm-2-chat-bison-32k","context":32760,"input":0.000001,"output":0.000002,"img":0,"request":0,"last_updated":"2024-11-11T03:15:52.000Z","slug":"models/palm-2-chat-bison-32k"},"content":"\nPaLM 2 是谷歌推出的一种语言模型，具备更强的多语言、推理和编码能力。\n\n"},{"lang":"zh","group":"models","slug":"models/palm-2-codechat-bison-32k","frontmatter":{"title":"Google: PaLM 2 Code Chat 32k","meta_title":"Google: PaLM 2 Code Chat 32k","description":"Google: PaLM 2 Code Chat 32k","date":"2023-11-03T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text 2 text"],"author":"google","tags":["Programming","Chatbots","Natural Language Processing","Generative AI","Technology/Web"],"draft":false,"id":"palm-2-codechat-bison-32k","context":32760,"input":0.000001,"output":0.000002,"img":0,"request":0,"last_updated":"2024-11-11T03:16:01.000Z","slug":"models/palm-2-codechat-bison-32k"},"content":"\nPaLM 2 针对帮助解决代码相关问题的聊天机器人对话进行了微调。\n\n"},{"lang":"zh","group":"models","slug":"models/phi-3-medium-128k-instruct","frontmatter":{"title":"Phi-3 Medium 128K Instruct","meta_title":"Phi-3 Medium 128K Instruct","description":"Phi-3 Medium 128K Instruct","date":"2024-05-24T00:00:00.000Z","image":"https://img.rifx.online/logo/microsoft.svg","categories":["text 2 text"],"author":"microsoft","tags":["Natural Language Processing","Machine Learning","Programming","Data Science","Generative AI"],"draft":false,"id":"phi-3-medium-128k-instruct","context":128000,"input":0.000001,"output":0.000001,"img":0,"request":0,"last_updated":"2024-11-11T03:17:38.000Z","slug":"models/phi-3-medium-128k-instruct"},"content":"\nPhi-3 128K Medium 是一个强大的 140 亿参数模型，旨在实现高级语言理解、推理和指令跟随。通过监督微调和偏好调整进行优化，它在涉及常识、数学、逻辑推理和代码处理的任务中表现出色。\n\n在发布时，Phi-3 Medium 在轻量级模型中展示了最先进的性能。在 MMLU-Pro 评估中，该模型甚至接近 Llama3 70B 的性能水平。\n\n对于 4k 上下文长度，请尝试 [Phi-3 Medium 4K](/microsoft/phi-3-medium-4k-instruct).\n\n"},{"lang":"zh","group":"models","slug":"models/phi-3-mini-128k-instruct","frontmatter":{"title":"Phi-3 Mini 128K Instruct","meta_title":"Phi-3 Mini 128K Instruct","description":"Phi-3 Mini 128K Instruct","date":"2024-05-26T00:00:00.000Z","image":"https://img.rifx.online/logo/microsoft.svg","categories":["text 2 text"],"author":"microsoft","tags":["Natural Language Processing","Machine Learning","Programming","Data Science","Generative AI"],"draft":false,"id":"phi-3-mini-128k-instruct","context":128000,"input":1e-7,"output":1e-7,"img":0,"request":0,"last_updated":"2024-11-11T03:17:47.000Z","slug":"models/phi-3-mini-128k-instruct"},"content":"\nPhi-3 Mini 是一个强大的 3.8B 参数模型，旨在实现高级语言理解、推理和指令跟随。通过监督微调和偏好调整进行优化，它在涉及常识、数学、逻辑推理和代码处理的任务中表现出色。\n\n在发布时，Phi-3 Medium 在轻量级模型中展示了最先进的性能。该模型是静态的，训练于截止到 2023 年 10 月的离线数据集。\n\n"},{"lang":"zh","group":"models","slug":"models/phi-35-mini-128k-instruct","frontmatter":{"title":"Phi-3.5 Mini 128K Instruct","meta_title":"Phi-3.5 Mini 128K Instruct","description":"Phi-3.5 Mini 128K Instruct","date":"2024-08-21T00:00:00.000Z","image":"https://img.rifx.online/logo/microsoft.svg","categories":["text 2 text"],"author":"microsoft","tags":["Programming","Machine Learning","Natural Language Processing","Data Science","Generative AI"],"draft":false,"id":"phi-3.5-mini-128k-instruct","context":128000,"input":1e-7,"output":1e-7,"img":0,"request":0,"last_updated":"2024-11-01T04:17:17.000Z","slug":"models/phi-35-mini-128k-instruct"},"content":"\nPhi-3.5 模型是轻量级的、先进的开放模型。这些模型使用 Phi-3 数据集进行训练，该数据集包括合成数据和经过筛选的公共网站数据，重点关注高质量和推理密集的特性。Phi-3.5 Mini 使用 3.8B 参数，是一种仅解码的稠密变换器模型，使用与 [Phi-3 Mini](/microsoft/phi-3-mini-128k-instruct) 相同的分词器。\n\n这些模型经过严格的增强过程，结合了监督微调、邻近策略优化和直接偏好优化，以确保精确的指令遵循和强大的安全措施。在针对测试常识、语言理解、数学、代码、长上下文和逻辑推理的基准评估中，Phi-3.5 模型在参数少于 130 亿的模型中展示了强大且先进的性能。\n\n"},{"lang":"zh","group":"models","slug":"models/pixtral-12b","frontmatter":{"title":"Mistral: Pixtral 12B","meta_title":"Mistral: Pixtral 12B","description":"Mistral: Pixtral 12B","date":"2024-09-10T00:00:00.000Z","image":"https://img.rifx.online/logo/mistral.png","categories":["text image 2 text"],"author":"mistralai","tags":["Natural Language Processing","Machine Learning","Technology","Generative AI","Computer Vision"],"draft":false,"id":"pixtral-12b","context":4096,"input":1e-7,"output":1e-7,"img":0.0001445,"request":0,"last_updated":"2024-11-11T03:10:29.000Z","slug":"models/pixtral-12b"},"content":"\nMistral AI 的第一个图像到文本模型。根据他们的传统，其权重通过 torrent 发布： https://x.com/mistralai/status/1833758285167722836\n\n"},{"lang":"zh","group":"models","slug":"models/qwen-2-7b-instruct","frontmatter":{"title":"Qwen 2 7B Instruct","meta_title":"Qwen 2 7B Instruct","description":"Qwen 2 7B Instruct","date":"2024-07-16T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"qwen","tags":["Natural Language Processing","Programming","Machine Learning","Data Science","Ethics"],"draft":false,"id":"qwen-2-7b-instruct","context":32768,"input":5.4e-8,"output":5.4e-8,"img":0,"request":0,"last_updated":"2024-07-16T00:00:00.000Z","slug":"models/qwen-2-7b-instruct"},"content":"\nQwen2 7B 是一个基于变换器的模型，擅长语言理解、多语言能力、编码、数学和推理。\n\n它具有 SwiGLU 激活、注意力 QKV 偏置和组查询注意力。它在大量数据上进行预训练，并经过监督微调和直接偏好优化。\n\n有关更多详细信息，请参阅此 [博客文章](https://qwenlm.github.io/blog/qwen2/) 和 [GitHub 仓库](https://github.com/QwenLM/Qwen2)。\n\n使用此模型须遵循 [同义千问许可证协议](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE)。\n\n"},{"lang":"zh","group":"models","slug":"models/qwen-2-vl-72b-instruct","frontmatter":{"title":"Qwen2-VL 72B Instruct","meta_title":"Qwen2-VL 72B Instruct","description":"Qwen2-VL 72B Instruct","date":"2024-09-18T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text image 2 text"],"author":"qwen","tags":["Natural Language Processing","Computer Vision","Robotics","Machine Learning"],"draft":false,"id":"qwen-2-vl-72b-instruct","context":32768,"input":4e-7,"output":4e-7,"img":0.000578,"request":0,"last_updated":"2024-09-18T00:00:00.000Z","slug":"models/qwen-2-vl-72b-instruct"},"content":"\nQwen2 VL 72B 是来自 Qwen Team 的多模态 LLM，具有以下关键增强功能：\n\n- 对各种分辨率和比例图像的最先进理解：Qwen2-VL 在视觉理解基准测试中实现了最先进的性能，包括 MathVista、DocVQA、RealWorldQA、MTVQA 等。\n\n- 理解超过 20 分钟的视频：Qwen2-VL 可以理解超过 20 分钟的视频，以进行高质量的视频问答、对话、内容创作等。\n\n- 能够操作您的手机、机器人等的智能体：凭借复杂推理和决策能力，Qwen2-VL 可以与手机、机器人等设备集成，实现基于视觉环境和文本指令的自动操作。\n\n- 多语言支持：为了服务全球用户，除了英语和中文，Qwen2-VL 现在还支持理解图像中不同语言的文本，包括大多数欧洲语言、日语、韩语、阿拉伯语、越南语等。\n\n有关更多详细信息，请参阅此 [博客文章](https://qwenlm.github.io/blog/qwen2-vl/) 和 [GitHub 仓库](https://github.com/QwenLM/Qwen2-VL)。\n\n使用此模型受 [通义千问许可协议](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE) 的约束。\n\n"},{"lang":"zh","group":"models","slug":"models/qwen-2-vl-7b-instruct","frontmatter":{"title":"Qwen2-VL 7B Instruct","meta_title":"Qwen2-VL 7B Instruct","description":"Qwen2-VL 7B Instruct","date":"2024-08-28T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text image 2 text"],"author":"qwen","tags":["Natural Language Processing","Computer Vision","Robotics","Multimodal AI","Generative AI"],"draft":false,"id":"qwen-2-vl-7b-instruct","context":32768,"input":1e-7,"output":1e-7,"img":0.0001445,"request":0,"last_updated":"2024-11-11T03:13:01.000Z","slug":"models/qwen-2-vl-7b-instruct"},"content":"\nQwen2 VL 7B 是来自 Qwen 团队的多模态 LLM，具有以下关键增强功能：\n\n- 对各种分辨率和比例的图像的最先进理解：Qwen2-VL 在视觉理解基准测试中实现了最先进的性能，包括 MathVista、DocVQA、RealWorldQA、MTVQA 等。\n\n- 理解超过 20 分钟的视频：Qwen2-VL 能够理解超过 20 分钟的视频，以便进行高质量的视频问答、对话、内容创作等。\n\n- 能够操作手机、机器人等的代理：凭借复杂推理和决策能力，Qwen2-VL 可以与手机、机器人等设备集成，实现基于视觉环境和文本指令的自动操作。\n\n- 多语言支持：为了服务全球用户，除了英语和中文，Qwen2-VL 现在还支持理解图像中不同语言的文本，包括大多数欧洲语言、日语、韩语、阿拉伯语、越南语等。\n\n有关更多详细信息，请参阅此 [博客文章](https://qwenlm.github.io/blog/qwen2-vl/) 和 [GitHub 仓库](https://github.com/QwenLM/Qwen2-VL)。\n\n使用此模型须遵循 [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE)。\n\n"},{"lang":"zh","group":"models","slug":"models/qwen-25-72b-instruct","frontmatter":{"title":"Qwen2.5 72B Instruct","meta_title":"Qwen2.5 72B Instruct","description":"Qwen2.5 72B Instruct","date":"2024-09-19T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"qwen","tags":["Programming","Natural Language Processing","Chatbots","Machine Learning","Data Science"],"draft":false,"id":"qwen-2.5-72b-instruct","context":131072,"input":3.5e-7,"output":4e-7,"img":0,"request":0,"last_updated":"2024-09-19T00:00:00.000Z","slug":"models/qwen-25-72b-instruct"},"content":"\nQwen2.5 72B 是 Qwen 大型语言模型的最新系列。Qwen2.5 在 Qwen2 的基础上带来了以下改进：\n\n- 知识显著增加，并在编码和数学能力上有了很大提升，这得益于我们在这些领域的专业专家模型。\n\n- 在遵循指令、生成长文本（超过 8K tokens）、理解结构化数据（例如，表格）以及生成结构化输出（特别是 JSON）方面有显著改进。对系统提示的多样性更加强韧，增强了角色扮演的实现和聊天机器人的条件设置。\n\n- 支持最长 128K tokens 的长上下文，并可以生成最多 8K tokens。\n\n- 支持超过 29 种语言，包括中文、英文、法语、西班牙语、葡萄牙语、德语、意大利语、俄语、日语、韩语、越南语、泰语、阿拉伯语等。\n\n使用此模型须遵守 [同义千问许可协议](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE)。\n\n"},{"lang":"zh","group":"models","slug":"models/qwen-25-7b-instruct","frontmatter":{"title":"Qwen2.5 7B Instruct","meta_title":"Qwen2.5 7B Instruct","description":"Qwen2.5 7B Instruct","date":"2024-10-16T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"qwen","tags":["Programming","Natural Language Processing","Chatbots","Machine Learning","Data Science"],"draft":false,"id":"qwen-2.5-7b-instruct","context":131072,"input":2.7e-7,"output":2.7e-7,"img":0,"request":0,"last_updated":"2024-10-16T00:00:00.000Z","slug":"models/qwen-25-7b-instruct"},"content":"\nQwen2.5 7B 是 Qwen 大语言模型的最新系列。Qwen2.5 在 Qwen2 的基础上带来了以下改进：\n\n- 知识显著增加，并在编码和数学方面的能力大幅提升，这得益于我们在这些领域的专业模型。\n\n- 在遵循指令、生成长文本（超过 8K tokens）、理解结构化数据（例如，表格）以及生成结构化输出，特别是 JSON 方面有显著改进。对系统提示的多样性更具韧性，增强了角色扮演的实现和聊天机器人的条件设置。\n\n- 长文本支持高达 128K tokens，并且可以生成最多 8K tokens。\n\n- 支持超过 29 种语言，包括中文、英文、法文、西班牙文、葡萄牙文、德文、意大利文、俄文、日文、韩文、越南文、泰文、阿拉伯文等。\n\n使用此模型须遵守 [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE)。\n\n"},{"lang":"zh","group":"models","slug":"models/qwen-25-coder-32b-instruct","frontmatter":{"title":"Qwen2.5 Coder 32B Instruct","meta_title":"Qwen2.5 Coder 32B Instruct","description":"Qwen2.5 Coder 32B Instruct","date":"2024-11-11T23:40:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"qwen","tags":["Programming","Programming/Scripting","Machine Learning","Natural Language Processing","Generative AI"],"draft":false,"is_recommended":true,"id":"qwen-2.5-coder-32b-instruct","context":32768,"input":1.8e-7,"output":1.8e-7,"img":0,"request":0,"last_updated":"2024-11-14T04:04:59.000Z","slug":"models/qwen-25-coder-32b-instruct"},"content":"\nQwen2.5-Coder 是最新系列的特定代码 Qwen 大型语言模型（以前称为 CodeQwen）。Qwen2.5-Coder 在 CodeQwen1.5 的基础上带来了以下改进：\n\n- 在 **代码生成**、**代码推理** 和 **代码修复** 方面有显著提升。\n- 为现实世界应用（如 **代码代理**）提供了更全面的基础。不仅增强了编码能力，还保持了其在数学和一般能力方面的优势。\n\n要了解更多关于其评估结果的信息，请查看 [Qwen 2.5 Coder's blog](https://qwenlm.github.io/blog/qwen2.5-coder-family/)。\n\n## 引用\n\n```\n@article{hui2024qwen2,\n      title={Qwen2. 5-Coder Technical Report},\n      author={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Dang, Kai and others},\n      journal={arXiv preprint arXiv:2409.12186},\n      year={2024}\n}\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}\n```\n\n"},{"lang":"zh","group":"models","slug":"models/remm-slerp-l2-13b","frontmatter":{"title":"ReMM SLERP 13B","meta_title":"ReMM SLERP 13B","description":"ReMM SLERP 13B","date":"2023-07-22T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"undi95","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"is_recommended":true,"id":"remm-slerp-l2-13b","context":4096,"input":0.000001125,"output":0.000001125,"img":0,"request":0,"last_updated":"2024-11-14T04:06:00.000Z","slug":"models/remm-slerp-l2-13b"},"content":"\n原始 MythoMax-L2-B13 的重创版本，但采用了更新的模型。 #merge\n\n"},{"lang":"zh","group":"models","slug":"models/remm-slerp-l2-13b:extended","frontmatter":{"title":"ReMM SLERP 13B (extended)","meta_title":"ReMM SLERP 13B (extended)","description":"ReMM SLERP 13B (extended)","date":"2023-07-22T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"undi95","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"remm-slerp-l2-13b:extended","context":6144,"input":0.000001125,"output":0.000001125,"img":0,"request":0,"last_updated":"2024-11-04T12:47:21.000Z","slug":"models/remm-slerp-l2-13b:extended"},"content":"\n原始 MythoMax-L2-B13 的重现试验，但使用了更新的模型。 #merge\n\n_这些是 [ReMM SLERP 13B](/undi95/remm-slerp-l2-13b) 的扩展上下文端点。它们可能具有更高的价格。_\n\n"},{"lang":"zh","group":"models","slug":"models/rocinante-12b","frontmatter":{"title":"Rocinante 12B","meta_title":"Rocinante 12B","description":"Rocinante 12B","date":"2024-09-30T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"thedrummer","tags":["Roleplay","Programming","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"rocinante-12b","context":32768,"input":2.5e-7,"output":5e-7,"img":0,"request":0,"last_updated":"2024-11-11T03:09:37.000Z","slug":"models/rocinante-12b"},"content":"\nRocinante 12B 旨在提供引人入胜的叙事和丰富的散文。\n\n早期测试者报告：\n- 词汇量扩展，独特且富有表现力的用词选择\n- 创造力增强，能够生动叙述\n- 充满冒险和引人入胜的故事\n\n"},{"lang":"zh","group":"models","slug":"models/sorcererlm-8x22b","frontmatter":{"title":"Sorcererlm 8x22b","meta_title":"Sorcererlm 8x22b","description":"Sorcererlm 8x22b","date":"2024-11-08T22:31:23.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"raifle","tags":["Roleplay","Programming","Natural Language Processing","Chatbots","Generative AI"],"draft":false,"id":"sorcererlm-8x22b","context":16000,"input":0.0000045,"output":0.0000045,"img":0,"request":0,"last_updated":"2024-11-11T02:56:49.000Z","slug":"models/sorcererlm-8x22b"},"content":"\nSorcererLM 是一个先进的 RP 和故事讲述模型，作为一个低秩 16 位 LoRA 在 WizardLM-2-8x22B 上进行微调。\n\n- 先进的推理和情感智能，实现引人入胜和沉浸式的互动\n- 生动的写作能力，增强了空间和上下文意识\n- 增强的叙事深度，促进创造性和动态的故事讲述\n\n"},{"lang":"zh","group":"models","slug":"models/toppy-m-7b","frontmatter":{"title":"Toppy M 7B","meta_title":"Toppy M 7B","description":"Toppy M 7B","date":"2023-11-10T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"undi95","tags":["Programming","Machine Learning","Generative AI","Chatbots","Data Science"],"draft":false,"id":"toppy-m-7b","context":4096,"input":7e-8,"output":7e-8,"img":0,"request":0,"last_updated":"2024-11-04T12:51:35.000Z","slug":"models/toppy-m-7b"},"content":"\n一个野生的7B参数模型，通过mergekit中的新task_arithmetic合并方法合并了多个模型。\n合并模型列表：\n- NousResearch/Nous-Capybara-7B-V1.9\n- [HuggingFaceH4/zephyr-7b-beta](/huggingfaceh4/zephyr-7b-beta)\n- lemonilia/AshhLimaRP-Mistral-7B\n- Vulkane/120-Days-of-Sodom-LoRA-Mistral-7b\n- Undi95/Mistral-pippa-sharegpt-7b-qlora\n\n#merge #uncensored\n\n"},{"lang":"zh","group":"models","slug":"models/unslopnemo-12b","frontmatter":{"title":"Unslopnemo 12b","meta_title":"Unslopnemo 12b","description":"Unslopnemo 12b","date":"2024-11-08T22:04:08.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"thedrummer","tags":["Roleplay","Programming","Generative AI","Chatbots","Natural Language Processing"],"draft":false,"id":"unslopnemo-12b","context":32000,"input":5e-7,"output":5e-7,"img":0,"request":0,"last_updated":"2024-11-14T02:10:09.000Z","slug":"models/unslopnemo-12b"},"content":"\nUnslopNemo v4.1 是来自 Rocinante 创作者的最新作品，旨在用于冒险写作和角色扮演场景。\n\n"},{"lang":"zh","group":"models","slug":"models/wizardlm-2-7b","frontmatter":{"title":"WizardLM-2 7B","meta_title":"WizardLM-2 7B","description":"WizardLM-2 7B","date":"2024-04-16T00:00:00.000Z","image":"https://img.rifx.online/logo/microsoft.svg","categories":["text 2 text"],"author":"microsoft","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"wizardlm-2-7b","context":32000,"input":5.5e-8,"output":5.5e-8,"img":0,"request":0,"last_updated":"2024-10-31T23:23:36.000Z","slug":"models/wizardlm-2-7b"},"content":"\nWizardLM-2 7B 是微软 AI 最新 Wizard 模型的较小版本。它是最快的，并且在性能上与现有的 10 倍更大的开源领先模型相当。\n\n它是对 [Mistral 7B Instruct](/mistralai/mistral-7b-instruct) 的微调，使用与 [WizardLM-2 8x22B](/microsoft/wizardlm-2-8x22b) 相同的技术。\n\n要了解更多关于模型发布的信息，[请点击这里](https://wizardlm.github.io/WizardLM2/)。\n\n#moe\n\n"},{"lang":"zh","group":"models","slug":"models/wizardlm-2-8x22b","frontmatter":{"title":"WizardLM-2 8x22B","meta_title":"WizardLM-2 8x22B","description":"WizardLM-2 8x22B","date":"2024-04-16T00:00:00.000Z","image":"https://img.rifx.online/logo/microsoft.svg","categories":["text 2 text"],"author":"microsoft","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"wizardlm-2-8x22b","context":65536,"input":5e-7,"output":5e-7,"img":0,"request":0,"last_updated":"2024-10-31T23:24:21.000Z","slug":"models/wizardlm-2-8x22b"},"content":"\nWizardLM-2 8x22B 是微软 AI 最先进的 Wizard 模型。与领先的专有模型相比，它展示了高度竞争的性能，并且始终优于所有现有的最先进的开源模型。\n\n它是 [Mixtral 8x22B](/mistralai/mixtral-8x22b) 的指令微调版本。\n\n要了解有关模型发布的更多信息，[请点击这里](https://wizardlm.github.io/WizardLM2/)。\n\n#moe\n\n"}]
[{"lang":"en","group":"blog","slug":"blog/10-creative-ways-to-use-chatgpt-search-the-web-feature-7f145c5cfa30","frontmatter":{"title":"10 Creative Ways to Use ChatGPT Search The Web Feature","meta_title":"10 Creative Ways to Use ChatGPT Search The Web Feature","description":"The article outlines ten innovative applications of ChatGPTs search the web feature, which provides real-time information. Users can stay updated on current events, plan travel itineraries, discover new recipes, monitor market trends, access real-time data, find local events, compare products, learn about emerging technologies, get air quality updates, and explore educational resources. This feature is particularly beneficial for paid members using ChatGPT 4o and 4o-mini, enhancing their ability to gather relevant information efficiently.","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*S4RtWt6Ouspx4nnl","categories":["Chatbots","Technology/Web","Education"],"author":"Rifx.Online","tags":["ChatGPT","search","web","real-time","information"],"draft":false,"slug":"blog/10-creative-ways-to-use-chatgpt-search-the-web-feature-7f145c5cfa30"},"content":"\n\n\n\n\n### For example, prompts and outputs\n\n\n\nDid you know you can use the â€œsearch the webâ€ feature of ChatGPT for many tasks other than your basic web search?\n\nFor those who don't know, ChatGPTâ€™s new â€œsearch the webâ€ feature provides real\\-time information.\n\nAs of writing this post, it's only available for paid members who are using ChatGPT 4o and 4o\\-mini.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*uyESPHmmvzSJjZmgpn_Oww.png)\n\nHere are some creative ways to use this feature:\n\n\n## 1\\. Stay Updated on Current Events:\n\nIf you are interested in the latest news and events and don't have time for searching and finding the best ones this is for you.\n\nNow using the search web feature in ChatGPT you can receive summaries of the latest news, sports scores, and stock market updates anytime.\n\n**Example**: â€” â€œ***Whatâ€™s the latest news in technology today?***â€\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*OQFEyg8WFckOQcM5cKyRww.png)\n\n\n## 2\\. Plan Travel Itineraries:\n\nDo you like to plan your travel for the best time and budget management? Then you will find this itinerary planning feature very helpful.\n\nNow you can get up\\-to\\-date information on travel destinations, including weather forecasts, local events, and the best places to shop and eat.\n\n**Example: â€” â€œ*What are the top attractions in Paris this weekend?*â€**\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BLm4PoTaxrXkBMoB56jg8g.png)\n\n\n## 3\\. Discover New Recipes:\n\nIf you love trying new food and also love to cook, this one is great.\n\nYou can use â€œsearch the webâ€ in ChatGPT to find trending recipes or cooking tips based on dates or location.\n\n**Example: â€” â€œ*Whatâ€™s a popular dessert recipe this month?*â€**\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*iSrMCgwjdOw4xOSC51LglA.png)\n\n\n## 4\\. Monitor Market Trends:\n\nIf you like to read about or keep yourself updated on a particular field of knowledge then this feature is for you.\n\nNow you can keep track of any industry trends using the search web feature.\n\n**Example: â€” â€œ*What are the latest developments in solar energy?*â€**\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*M-Y7hXRYMXGs_V6iHOm7lQ.png)\n\n\n## 5\\. Access Real\\-Time Data:\n\nWant to know if it will rain today?\n\nOr want to know the latest score but cannot watch a live sports match?\n\nNow you can get real\\-time data such as weather updates, stock prices, or sports scores.\n\n**Example: â€” â€œ*Whatâ€™s the current weather in New York City?*â€**\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*TwPSdHgHdaKmipspoyldsg.png)\n\n\n## 6\\. Find Local Events:\n\nThe extroverts here will love this feature.\n\nNow you can discover events happening in your area and it will also give you direct links to websites where you can know more details and book tickets.\n\n**Example: â€” â€œ*What events are happening in Sydney this weekend?*â€**\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MgSawNL8kSTohGsIU0ajrA.png)\n\n\n## 7\\. Compare Products:\n\nLooking to buy a new product? Want to know its Pros and Cons?\n\nOr maybe you would like to compare a few products to decide the best among them.\n\nNow you can use the search web feature to compare products or services.\n\n**Example: â€” â€œ*Compare the latest smartphones released this year.*â€**\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*HvzNuBcc6kWNSZA7Sj2hUg.png)\n\n\n## 8\\. Learn About Emerging Technologies:\n\nIf you like to stay updated in the emerging technology sector this one is for you.\n\nUsing the â€œSearch the Webâ€ feature now you can stay informed about new technologies.\n\n**Example: â€” â€œ*What are the latest advancements in artificial intelligence?*â€**\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*VwySsjMn59nvqxHUWm1AtA.png)\n\n\n## 9\\. Get Air Quality Updates:\n\nThis is another cool usage of location and time\\-aware search results from ChatGPT.\n\nNow you can check the air quality index of any location. Use this to know about the pollution levels.\n\n**Example: â€” â€œ*Whatâ€™s the AQI in New York right now?*â€**\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*6C_VcWft52zoR57XzEMo-A.png)\n\n\n## 10\\. Explore Educational Resources:\n\nIf you like to learn by watching courses on a topic then this will help you a lot.\n\nYou can use the search feature to find recent articles or courses on topics of interest.\n\n**Example: â€” â€œ*What are the latest online courses available for data science?*â€**\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*tPq8Lve_M_1sNhqfkqtwyg.png)\n\nUsing ChatGPTâ€™s web search feature, you can access the latest and relevant information in many different ways.\n\nIf you also found a few new and different uses of real\\-time search then please let me know in the comments.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/10-must-learn-skills-to-stay-ahead-in-ai-and-tech-42f4140713b1","frontmatter":{"title":"ðŸ“š 10 Must-Learn Skills to Stay Ahead in AI and Tech ðŸš€","meta_title":"ðŸ“š 10 Must-Learn Skills to Stay Ahead in AI and Tech ðŸš€","description":"The article outlines ten essential courses designed to enhance skills in the rapidly evolving fields of AI and technology. Topics include generative AI, debugging, AI-powered recommendations, and technical support fundamentals, among others. These courses aim to equip professionals with the knowledge necessary to stay competitive and innovate within their respective industries, including law and data analysis. The article emphasizes the importance of continuous learning to keep pace with technological advancements.","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*uKN-KrOhsDhRrAjL","categories":["Technology","Generative AI","Data Science"],"author":"Rifx.Online","tags":["generative","debugging","recommendations","fundamentals","competitive"],"draft":false,"slug":"blog/10-must-learn-skills-to-stay-ahead-in-ai-and-tech-42f4140713b1"},"content":"\n\n\n\n\n\nIn an industry as dynamic as AI and tech, staying ahead means constantly upgrading your skills. Whether youâ€™re aiming to dive deep into AI model performance, master data analysis, or transform traditional fields like law with AI, these courses are your gateway to success. Hereâ€™s a curated list of high\\-value courses to supercharge your career and keep you at the forefront of innovation.\n\n\n## 1\\. Introduction to Generative AI\n\n* **Course**: [Intro to Generative AI](https://genai.works/courses/introduction-to-generative-ai-english)\n* **Provider**: Google Cloud\n* **Why Take It**: Gain a foundational understanding of how generative AI models work and how theyâ€™re shaping various industries. Perfect for anyone looking to grasp AIâ€™s transformative potential.\n\n\n## 2\\. Debugging Generative AI\n\n* **Course**: [Debugging Generative AI](https://genai.works/courses/evaluating-and-debugging-generative-ai)\n* **Provider**: DeepLearning AI\n* **Why Take It**: Learn to troubleshoot and optimize generative AI models, ensuring that they perform reliably and efficiently. Essential for AI professionals who want to fine\\-tune models for better results.\n\n\n## 3\\. Top AI\\-Supported Products\n\n* **Course**: [Top AI\\-Supported Products](https://genai.works/courses/top-100-best-selling-products-ai)\n* **Provider**: Michigan University\n* **Why Take It**: Discover the latest trends in AI\\-powered products, helping you understand where the industry is heading and what innovations are dominating the market.\n\n\n## 4\\. AI\\-Powered Recommendations with Vector Databases\n\n* **Course**: [AI\\-Powered Recommendations](https://genai.works/courses/vector-database-projects-ai-recommendation-systems)\n* **Provider**: IBM\n* **Why Take It**: Develop smart recommendation systems using vector databases, a crucial skill for building personalized, AI\\-driven experiences in tech, e\\-commerce, and media.\n\n\n## 5\\. AI for Software Teams\n\n* **Course**: [Team Software Engineering with AI](https://genai.works/courses/team-software-engineering-with-ai)\n* **Provider**: DeepLearning AI\n* **Why Take It**: This course equips software teams to leverage AI tools for better collaboration, making teamwork more efficient and tech projects more successful.\n\n\n## 6\\. Technical Support Fundamentals\n\n* **Course**: [Tech Support Fundamentals](https://genai.works/courses/technical-support-fundamentals)\n* **Provider**: Google\n* **Why Take It**: Essential for anyone wanting to build troubleshooting skills, this course lays the foundation for technical support roles and IT infrastructure management.\n\n\n## 7\\. Prompt Engineering for Law\n\n* **Course**: [Specialization Prompt Engineering for Law](https://genai.works/courses/specialization-prompt-engineering-for-law)\n* **Provider**: Vanderbilt\n* **Why Take It**: Transform the legal field with AI\\-driven prompt engineering. This course is ideal for legal professionals looking to innovate with technology and streamline their work.\n\n\n## 8\\. Preparing Data for Analysis with Excel\n\n* **Course**: [Data Prep with Excel](https://genai.works/courses/preparing-data-for-analysis-using-microsoft-excel)\n* **Provider**: Microsoft\n* **Why Take It**: Improve your data analysis workflow and enhance your Excel skills for better data preparation, crucial for both entry\\-level and advanced data science roles.\n\n\n## 9\\. IT Security: Defense Against the Digital Dark Arts\n\n* **Course**: [IT Security Fundamentals](https://lnkd.in/dTY2Vbih)\n* **Provider**: Google\n* **Why Take It**: Learn to safeguard your digital assets and understand the basics of cybersecurity. This course covers critical skills in defending against cyber threats.\n\n\n## 10\\. Data Structures in Python\n\n* **Course**: [Data Structures in Python](https://genai.works/courses/data-structures-in-python)\n* **Provider**: Michigan University\n* **Why Take It**: Strengthen your coding skills with a deeper understanding of data structures in Python, an essential skill for software developers and data scientists alike.\n\n\n## âœ”ï¸ Join the \\#BuildwithAI Hackathon 2024\n\n* **Get Involved**: [Hackathon Link](https://lnkd.in/dsapprp4)\n* **Why Join**: Put your skills to the test, collaborate with like\\-minded individuals, and solve real\\-world problems using AI. The perfect way to apply what youâ€™ve learned and showcase your expertise.\n\n\n## Conclusion\n\n*The tech landscape is constantly evolving, and keeping up with these essential skills will help you stay competitive. From foundational courses in AI and data structures to specialized fields like AI for legal work, these courses cover a wide range of knowledge that will equip you for the future. Enroll now, expand your expertise, and start making an impact in AI and tech!*\n\nHappy learning and innovating! ðŸš€\n\nIf youâ€™d like your AI Product to be featured, feel free to contact us on our [**Linkedin page**](https://www.linkedin.com/company/genai-works/)**.**\n\nFollow our official [**Instagram**](https://www.instagram.com/generativeai_official/?igsh=Zjc3NGU5N2ticzZ6), [**TikTok**](https://www.tiktok.com/@generative_ai_official?_t=8kqDA0pyrC6&_r=1) and [**YouTube**](https://www.youtube.com/@generative.ai.official) for daily AI content.\n\nIt is now open to all startups and builders to register their AI App/Project for free! [**https://genai.works/sign\\-up**](https://genai.works/sign-up)\n\nRead TOP AI News and find useful cheat sheets in our [**Generative AI Daily Newsletter.**](https://newsletter.genai.works/subscribe)\n\n**Quick Facts:** 5\\+M Followers \\| 2\\.6M\\+ Newsletter Subscribers. Largest and fastest growing [**AI community on Linkedâ€™In**](https://www.linkedin.com/company/genai-works/) founded and backed by AI experts in the industry.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/5-ai-projects-you-can-build-this-weekend-with-node-js-76e0ee51cc72","frontmatter":{"title":"5 AI Projects You Can Build This Weekend (with Node.js)","meta_title":"5 AI Projects You Can Build This Weekend (with Node.js)","description":"This article presents five beginner-friendly AI projects that can be completed over a weekend using Node.js. The projects include creating a customer support chatbot, an AI-powered image recognition app, a sentiment analysis tool for social media, a voice command application, and a personalized movie recommender. Each project highlights essential skills in natural language processing, computer vision, sentiment analysis, and recommendation algorithms, providing hands-on experience in artificial intelligence development.","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*x9ezYQZawlG0DRV6","categories":["Programming/Scripting","Natural Language Processing","Computer Vision"],"author":"Rifx.Online","tags":["Node.js","chatbot","image","sentiment","recommender"],"draft":false,"slug":"blog/5-ai-projects-you-can-build-this-weekend-with-node-js-76e0ee51cc72"},"content":"\n5 Exciting AI Projects to Build in a Weekend with Node.js (Perfect for Beginners)\n\n\n\nAre you interested in building AI projects but short on time?\n\nWith just Node.js and a weekend, you can dive into hands\\-on AI projects that boost your coding skills and introduce you to practical applications in artificial intelligence.\n\nThese beginner\\-friendly projects will guide you through setting up chatbots, image recognition, sentiment analysis, and more.\n\nSo, grab your laptop and get ready to code with these five exciting AI projects!\n\n\n### 1\\. Chatbot for Customer Support ðŸ¤–\n\nChatbots are a popular way to start exploring natural language processing (NLP), and with Node.js, you can set up a basic chatbot to handle customer inquiries and provide answers.\n\n**Why Build This Project?**Creating a chatbot introduces you to the fundamentals of NLP and real\\-time server interactions, valuable skills in AI development.\n\n**What Youâ€™ll Need:**\n\n* **Node.js and Express** for [setting up the server](https://expressjs.com/)\n* **Dialogflow** (by Google) or [**ChatGPT API**](https://platform.openai.com/docs/api-reference/introduction) for natural language processing\n* **Socket.io** for real\\-time chat functionality\n\n\n## 2\\. Build an AI\\-Powered Image Recognition App with Node.js ðŸ“¸\n\nThis project involves creating an image recognition app that can identify objects, animals, or text in photos.\n\nBy using an AI\\-powered image recognition API, youâ€™ll be able to work with computer vision without diving into complex machine learning algorithms.\n\n**Why Build This Project?**Image recognition is a key component in AI, and this project will give you hands\\-on experience with computer vision and file handling in Node.js.\n\n**What Youâ€™ll Need:**\n\n* **Node.js and Express** for backend server setup\n* **Google Cloud Vision** or [**Microsoft Azure Computer Vision API**](https://azure.microsoft.com/en-us/services/cognitive-services/computer-vision/) for image analysis\n* **Multer** for [handling file uploads](https://www.npmjs.com/package/multer)\n\n\n## 3\\. Sentiment Analysis Tool for Social Media Posts ðŸ“Š\n\nA sentiment analysis tool lets you analyze the tone of social media posts, reviews, or customer feedback.\n\nWith Node.js and a sentiment analysis API, you can create a tool that rates text as positive, negative, or neutral.\n\n**Why Build This Project?**This project is perfect for learning how to process text data and interpret sentiment, which is widely used in social media monitoring and customer feedback analysis.\n\n**What Youâ€™ll Need:**\n\n* **Node.js and Express** for server setup\n* [**Natural**](https://github.com/NaturalNode/natural) or **Aylien API** for sentiment analysis\n* **HTML/CSS** for creating a simple [user interface](https://developer.mozilla.org/en-US/docs/Learn/HTML)\n\n\n## 4\\. Develop a Voice Command App with Speech Recognition ðŸŽ™ï¸\n\nCreate an app that understands basic voice commands, an essential feature for voice\\-activated devices or smart home systems.\n\nBy combining Node.js and a speech recognition API, you can create a simple app that recognizes commands and responds.\n\n**Why Build This Project?**Voice recognition is becoming more common, and this project gives you the chance to explore voice\\-controlled interactions, which are valuable in IoT and accessibility\\-focused applications.\n\n**What Youâ€™ll Need:**\n\n* **Node.js and Express** for [the backend server](https://expressjs.com/)\n* [**Web Speech API**](https://developer.mozilla.org/en-US/docs/Web/API/Web_Speech_API) for browser\\-based speech recognition\n* **Socket.io** for real\\-time command response\n\n\n## 5\\. Design a Personalized Movie Recommender with Node.js ðŸŽ¬\n\nUsing machine learning algorithms, you can build a personalized movie recommendation system based on user preferences. This project uses collaborative filtering to suggest movies similar to those a user has rated highly.\n\n**Why Build This Project?**Movie recommendation systems are an excellent introduction to collaborative filtering and recommendation algorithms, which are widely used in streaming services and e\\-commerce.\n\n**What Youâ€™ll Need:**\n\n* **Node.js and Express** for server setup\n* **Collaborative Filtering algorithms** (e.g., [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity) or [KNN](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)) for recommendation logic\n* **TMDb API** for accessing a large database of movies\n\n\n## Conclusion\n\nThese five AI projects are perfect for anyone looking to explore AI hands\\-on over a single weekend.\n\nFrom building a chatbot to creating a movie recommender, youâ€™ll gain foundational AI skills while strengthening your Node.js expertise.\n\nEach project is highly customizable, so as you progress, feel free to adapt them with your unique twists.\n\nFor more tutorials and resources, [subscribe to our channel](https://www.youtube.com/@codemarketi) and stay updated on the latest AI and Node.js project ideas.\n\nHappy coding! ðŸš€\n\n\n## Related Blog Articles You Might Enjoy\n\n* [https://readmedium.com/how\\-ai\\-tools\\-like\\-claude\\-vercel\\-and\\-more\\-are\\-transforming\\-software\\-development\\-b8d79b0de943](https://readmedium.com/how-ai-tools-like-claude-vercel-and-more-are-transforming-software-development-b8d79b0de943)\n* [https://readmedium.com/six\\-ai\\-powered\\-passive\\-income\\-ways\\-to\\-make\\-350\\-per\\-day\\-990d1e334d16](https://readmedium.com/six-ai-powered-passive-income-ways-to-make-350-per-day-990d1e334d16)\n* [https://readmedium.com/as\\-a\\-developer\\-here\\-are\\-5\\-websites\\-youll\\-love\\-e7518b24c85d](https://readmedium.com/as-a-developer-here-are-5-websites-youll-love-e7518b24c85d)\n* [https://readmedium.com/5\\-useful\\-chatgpt\\-tricks\\-thatll\\-blow\\-your\\-mind\\-in\\-2025\\-12e10a81f4d5](https://readmedium.com/5-useful-chatgpt-tricks-thatll-blow-your-mind-in-2025-12e10a81f4d5)\n\n\n## In Plain English ðŸš€\n\n*Thank you for being a part of the [**In Plain English**](https://plainenglish.io/) community! Before you go:*\n\n* Be sure to **clap** and **follow** the writer ï¸ðŸ‘**ï¸ï¸**\n* Follow us: [**X**](https://x.com/inPlainEngHQ) \\| [**LinkedIn**](https://www.linkedin.com/company/inplainenglish/) \\| [**YouTube**](https://www.youtube.com/channel/UCtipWUghju290NWcn8jhyAw) \\| [**Discord**](https://discord.gg/in-plain-english-709094664682340443) \\| [**Newsletter**](https://newsletter.plainenglish.io/) \\| [**Podcast**](https://open.spotify.com/show/7qxylRWKhvZwMz2WuEoua0)\n* [**Create a free AI\\-powered blog on Differ.**](https://differ.blog/)\n* More content at [**PlainEnglish.io**](https://plainenglish.io/)\n\n"},{"lang":"en","group":"blog","slug":"blog/50-generative-ai-use-cases-across-10-industries-96f621fefac2","frontmatter":{"title":"The Top 50+ Generative AI Use Cases Across 10 Industries in 2024","meta_title":"The Top 50+ Generative AI Use Cases Across 10 Industries in 2024","description":"Generative AI (Gen AI) is emerging as a transformative technology across various industries in 2024, enabling businesses to automate complex tasks, enhance customer engagement, and foster innovation. Key advantages include improved efficiency, cost savings, and personalized experiences. This article outlines over 50 Gen AI use cases across ten industries, including marketing, sales, and finance, highlighting its potential to optimize operations and drive competitive advantage. A strategic approach to implementing Gen AI is essential for businesses to realize its benefits and remain competitive in a rapidly evolving digital landscape.","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*PeQAto1_Mwovo11vsArGIA.jpeg","categories":["Generative AI","Technology","Marketing"],"author":"Rifx.Online","tags":["Generative","Automation","Personalization","Efficiency","Innovation"],"draft":false,"slug":"blog/50-generative-ai-use-cases-across-10-industries-96f621fefac2"},"content":"\n\n\n\n\n\nAs we look forward to 2024, generative AI (Gen AI) is increasingly recognized as a pivotal technology driving transformation across industries. This evolution marks a shift from traditional AIâ€™s predictive and analytical role to Gen AIâ€™s creative capabilities, which enable businesses to automate complex tasks, foster innovation, and deliver highly personalized customer experiences. According to a recent analysis, businesses investing in AI have seen efficiency gains of up to 30%, while those implementing Gen AI are even further optimizing their workflows and outcomes.\n\nIn this article, we delve into over 50 impactful Gen AI use cases across ten leading industries, with a focus on empowering businesses interested in developing Gen AI to improve their operational efficiency, customer engagement, and competitive advantage.\n\n\n### What is Generative AI?\n\n\n> Generative AI is a subset of artificial intelligence focused on creating new content â€” be it text, images, audio, or video â€” by learning patterns from existing data. Unlike traditional AI, which primarily makes predictions or classifications based on data, Gen AI generates original outputs. It relies on models such as Generative Adversarial Networks (GANs) and transformer\\-based architectures like GPT, enabling capabilities from creative content production to real\\-time customer interaction.\n\n\n### Key Advantages of Generative AI for Business\n\n1. **Creativity and Innovation**: Gen AI generates unique content and ideas beyond human creativity, fueling industries where new concepts drive success, such as marketing, design, and product development.\n2. **Enhanced Efficiency**: Automating repetitive tasks such as content generation and initial customer interactions allows businesses to focus on strategic and high\\-level work.\n3. **Cost Savings**: Gen AI reduces dependency on manual labor for tasks like content creation, data analysis, and customer service, leading to significant savings.\n4. **Personalization at Scale**: Leveraging data, Gen AI delivers customized experiences, improving customer satisfaction and loyalty.\n5. **Data\\-Driven Insights**: Gen AI can process large datasets to extract actionable insights, supporting informed decision\\-making in real\\-time.\n\n[**Building a generative AI for business**](https://www.blockchainappfactory.com/generative-ai-solutions?utm_source=medium&utm_medium=blog&utm_campaign=elavarasan)can drive innovation, streamline operations, and enhance customer experiences. Itâ€™s a powerful tool for staying competitive in an increasingly digital world.\n\n\n## Generative AI Use Cases for the Top 10 Industries in 2024\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*TG-tfTwA58FNjHv5so8fXg.png)\n\n\n### 1\\. Marketing\n\nIn 2024, marketers leverage Gen AI to optimize campaigns, personalize outreach, and generate content at scale. By automating repetitive tasks, marketing teams can focus on strategy and creativity.\n\n* **Content Optimization**: Gen AI analyzes search trends and audience preferences to recommend topics, keywords, and formats that boost engagement and visibility.\n* **High\\-Scale Content Creation**: Marketing agencies use Gen AI to quickly produce diverse content types â€” from blog posts to social media updates â€” ensuring timely, high\\-quality output.\n* **Automated Social Media Management**: Small businesses utilize Gen AI to manage posts, respond to customer inquiries, and analyze engagement without a large team.\n* **Personalized Campaigns**: AI segments audiences, crafting tailored email and ad content that resonates with individual customer preferences.\n* **A/B Testing and Campaign Optimization**: AI automates testing of various content versions, analyzing real\\-time results to fine\\-tune campaigns for maximum effectiveness.\n\n\n### 2\\. Sales\n\nSales teams adopt Gen AI to streamline processes, manage leads, and automate proposal generation, improving conversion rates and efficiency.\n\n* **Lead Scoring and Prioritization**: AI assigns scores to leads based on likelihood to convert, enabling targeted and efficient outreach.\n* **Automated Proposal Generation**: Gen AI drafts custom proposals based on client needs, reducing preparation time and enhancing proposal quality.\n* **Virtual Sales Assistance**: AI assists sales reps with scheduling, follow\\-ups, and real\\-time data insights during client meetings.\n* **Predictive Sales Analytics**: By analyzing historical data, Gen AI predicts future trends and customer behaviors, supporting data\\-driven sales strategies.\n* **Chatbots for Initial Engagement**: AI chatbots engage potential customers, answering questions and qualifying leads before passing them to sales agents.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*7yN87soNCoYQe6Qk2oIenw.png)\n\n\n### 3\\. Human Resources\n\nGenerative AI helps HR teams automate tasks, from recruitment and onboarding to employee engagement analysis, contributing to better hiring and retention.\n\n* **Automated Resume Screening**: Gen AI reviews resumes, identifying top candidates based on specified criteria, saving time on initial screenings.\n* **Employee Onboarding Assistance**: AI\\-powered agents guide new hires through onboarding, ensuring smooth integration and reducing manual HR tasks.\n* **Sentiment Analysis of Feedback**: AI analyzes employee feedback to identify trends and recommend improvements to enhance workplace satisfaction.\n* **Personalized Learning and Development Plans**: AI assesses skills and suggests targeted training for employee growth.\n* **Workforce Analytics for Retention**: Gen AI predicts turnover rates and identifies contributing factors, allowing HR to proactively address retention.\n\n\n### 4\\. Customer Service\n\nGen AI enhances customer support by handling inquiries, providing real\\-time assistance, and automating ticket resolution.\n\n* **AI\\-Powered Chatbots**: Gen AI chatbots respond to routine questions, reducing response times and enhancing customer satisfaction.\n* **Sentiment Analysis for Feedback**: AI analyzes reviews and feedback to gauge sentiment, helping businesses understand customer satisfaction.\n* **Automated Ticket Prioritization**: Gen AI sorts support tickets, prioritizing urgent issues for faster resolution.\n* **Knowledge Base Optimization**: AI continuously updates knowledge bases based on common inquiries, improving self\\-service capabilities.\n* **Virtual Customer Assistants**: AI provides tailored recommendations based on customer preferences and history, enhancing support.\n\n\n### 5\\. Finance and Accounting\n\nGen AI is revolutionizing finance by automating invoice processing, forecasting finances, and ensuring tax compliance.\n\n* **Automated Invoice Processing**: AI handles invoicing, reducing errors and speeding up payment cycles.\n* **Financial Forecasting**: By analyzing historical data, Gen AI improves the accuracy of budgeting and financial planning.\n* **Expense Management**: AI identifies cost\\-saving opportunities by categorizing and analyzing expenses.\n* **Tax Compliance**: Gen AI prepares tax documents, minimizing errors and ensuring regulatory compliance.\n* **Real\\-Time Financial Reporting**: AI generates financial insights, empowering agile, data\\-driven decision\\-making.\n\n\n### 6\\. E\\-commerce\n\nIn e\\-commerce, Gen AI enhances customer experiences, optimizes inventory, and detects fraud, supporting business growth.\n\n* **Personalized Product Recommendations**: AI analyzes behavior to offer personalized product suggestions, increasing conversions.\n* **Dynamic Pricing Strategies**: AI adjusts prices in real\\-time based on market trends, demand, and competitor pricing.\n* **Inventory Forecasting**: AI predicts stock needs, preventing overstocking or stockouts and improving supply chain efficiency.\n* **Customer Journey Mapping**: AI creates detailed journey maps, guiding e\\-commerce strategies.\n* **Fraud Detection and Prevention**: Gen AI detects unusual transaction patterns, protecting businesses and customers from fraud.\n\n\n### 7\\. Real Estate\n\nReal estate companies use Gen AI for property valuation, virtual tours, and CRM enhancement, streamlining operations and improving decision\\-making.\n\n* **Automated Property Valuation**: AI models analyze market data, providing accurate valuations for better decision\\-making.\n* **Virtual Property Tours**: AI creates virtual tours, making listings accessible for remote or international buyers.\n* **Predictive Market Analysis**: AI forecasts market trends, guiding investment decisions.\n* **Lease Management Automation**: AI manages renewals and compliance, reducing administrative tasks.\n* **Enhanced Client Relationship Management**: AI provides insights into client preferences, enabling tailored interactions.\n\n\n### 8\\. Education\n\nIn education, Gen AI tailors learning experiences, supports curriculum development, and aids grading, improving learning outcomes.\n\n* **Personalized Learning Paths**: AI creates custom learning paths based on student strengths and weaknesses.\n* **Automated Grading**: AI assists educators with grading, enabling faster feedback for students.\n* **Curriculum Insights**: AI analyzes performance data to guide curriculum adjustments.\n* **Virtual Tutoring**: AI tutors provide resources and answer student queries outside class hours.\n* **Student Engagement Analysis**: AI flags at\\-risk students, supporting timely interventions.\n\n\n### 9\\. Manufacturing\n\nManufacturing benefits from Gen AIâ€™s predictive maintenance, quality control, and workforce scheduling, boosting efficiency and reducing downtime.\n\n* **Predictive Maintenance**: AI forecasts equipment issues, reducing unplanned maintenance and downtime.\n* **Supply Chain Optimization**: AI forecasts inventory needs, streamlining operations and reducing costs.\n* **Automated Quality Control**: Gen AI detects defects in real\\-time, enhancing product quality.\n* **Workforce Scheduling**: AI optimizes shifts based on demand, ensuring efficient staffing.\n* **Product Design Assistance**: AI informs design decisions by analyzing market and consumer trends.\n\n\n### 10\\. Retail\n\nRetailers use Gen AI to understand customer behavior, personalize in\\-store experiences, and optimize inventory, boosting satisfaction and sales.\n\n* **Customer Behavior Analysis**: AI analyzes shopping habits, guiding merchandising and marketing efforts.\n* **Enhanced In\\-Store Experience**: AI\\-driven apps deliver personalized in\\-store recommendations and assistance.\n* **Inventory Forecasting**: AI anticipates inventory needs, preventing overstock or stockouts.\n* **Loyalty Program Optimization**: AI tailors loyalty programs, improving engagement and retention.\n* **Automated Checkout**: AI\\-driven checkouts enhance convenience and reduce wait times.\n\n\n### Implementing Generative AI for Your Business\n\n1. **Identify Use Cases**: Define specific use cases aligned with business goals, from content generation to personalized marketing.\n2. **Choose the Right Tools**: Evaluate Gen AI tools for capabilities, integration, and scalability to ensure alignment with business needs.\n3. **Data Preparation and Model Training**: Clean, organize, and prepare data, as high\\-quality data is essential for model accuracy.\n4. **Pilot and Assess**: Run pilot projects to test Gen AIâ€™s impact, gather insights, and refine solutions as needed.\n5. **Continuous Optimization**: Monitor outputs, collect feedback, and make necessary adjustments to enhance model performance.\n6. **Consider Ethical Implications**: Implement policies to address ethical issues, including data privacy, bias, and responsible AI use.\n\n\n### Conclusion\n\nGenerative AI is transforming industries across the board in 2024, from enhancing customer engagement to driving efficiency in finance, marketing, and manufacturing. For businesses ready to adopt Gen AI, a strategic approach is key to realizing its benefits, from automating processes to personalizing customer experiences. By understanding Gen AIâ€™s potential, businesses can harness its capabilities to remain competitive, innovative, and efficient in todayâ€™s fast\\-evolving digital landscape.\n\n\n## In Plain English ðŸš€\n\n*Thank you for being a part of the [**In Plain English**](https://plainenglish.io/) community! Before you go:*\n\n* Be sure to **clap** and **follow** the writer ï¸ðŸ‘**ï¸ï¸**\n* Follow us: [**X**](https://x.com/inPlainEngHQ) \\| [**LinkedIn**](https://www.linkedin.com/company/inplainenglish/) \\| [**YouTube**](https://www.youtube.com/channel/UCtipWUghju290NWcn8jhyAw) \\| [**Discord**](https://discord.gg/in-plain-english-709094664682340443) \\| [**Newsletter**](https://newsletter.plainenglish.io/) \\| [**Podcast**](https://open.spotify.com/show/7qxylRWKhvZwMz2WuEoua0)\n* [**Create a free AI\\-powered blog on Differ.**](https://differ.blog/)\n* More content at [**PlainEnglish.io**](https://plainenglish.io/)\n\n"},{"lang":"en","group":"blog","slug":"blog/a-month-with-cursor-and-claude-dev-my-thoughts-5c41ae0d4467","frontmatter":{"title":"A Month with Cursor and Claude-Dev: My Thoughts","meta_title":"A Month with Cursor and Claude-Dev: My Thoughts","description":"Iâ€™ve been using two new tools recently- Cursor and Claude-Dev -both of which have been getting a fair bit of attention in the developerâ€¦","date":"2024-11-04T12:32:52.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*i28vK12LJ6XTpSwrKiamwA.png","categories":["Programming","Technology","Generative AI"],"author":"Rifx.Online","tags":["Cursor","Claude-Dev","Cline","autocomplete","debugging"],"draft":false,"slug":"blog/a-month-with-cursor-and-claude-dev-my-thoughts-5c41ae0d4467"},"content":"\n\n\nIâ€™ve been using two new tools recently\\- **Cursor** and **Claude\\-Dev** \\-both of which have been getting a fair bit of attention in the developer community. Theyâ€™re each built to make coding faster and more intuitive through AI\\-powered assistance, but they take different approaches and have their own strengths and weaknesses. After using both for about a month, I thought it was time to sit down and reflect on where they shine and where they still need some work.\n\nLetâ€™s start with Cursor.\n\n## Cursor: Familiar but Faster\n\nCursor is a fork of VSCode, which, if youâ€™re already a VSCode user like I am, makes it very easy to slip into. I didnâ€™t need to rebuild my environment from scratch or deal with setting up keybindings. Everything that worked in VSCode worked in Cursor right out of the box\\-my extensions, settings, and keymappings carried over without a hitch. The transition was almost invisible, except for one key difference: the AI autocomplete is much faster. In fact, **in my experience**, itâ€™s around 10 times faster than GitHub Copilot.\n\nNow, â€œ10 times fasterâ€ isnâ€™t a number I pulled from benchmarks\\-itâ€™s just what it feels like after using it for a while. When youâ€™re typing code, and Cursor is predicting your next move, it doesnâ€™t feel like the AI is lagging behind or playing catch\\-up. Instead, itâ€™s right there with you, which helps keep you in flow. I was surprised at how much more productive I felt when I wasnâ€™t waiting for Copilot to catch up or pressing tab three times just to get the suggestion I wanted.\n\nCursor also has a nice feature where it embeds and indexes your entire project, making it easier to understand the relationships between files. When you update a file, the index gets updated too, which means the AI has a better grasp of how the pieces of your codebase fit together. This is useful if youâ€™re working across a large codebase with multiple files that depend on each other.\n\n## The Drawbacks\n\nThat said, some of the best features in Cursor are gated behind a subscription. Iâ€™m generally not opposed to paying for tools that add real value, but in this case, I was a little disappointed that the most interesting AI features\\-like the multi\\-file editing\\-were part of the premium version. For a tool thatâ€™s still fairly new, I wonder if gating these features too early might limit its adoption, especially given how many developers are already paying for GitHub Copilot.\n\nAnother issue Iâ€™ve run into with Cursor is that while itâ€™s great at fast, small tasks, it lacks some of the flexibility I need when working with more complex problems. Itâ€™s excellent for quick code suggestions and refactoring, but when I needed something that could handle more involved tasks, like reading logs or executing build commands, I found myself looking for something else.\n\n## Claude\\-Dev: The Open\\-Source Underdog\n\nThatâ€™s where **Claude\\-Dev (now called Cline)** comes in. Claude\\-Dev is an open\\-source extension for VSCode, and while it doesnâ€™t have the same level of polish as Cursor, itâ€™s rapidly evolving\\-and in some ways, itâ€™s more powerful. The most striking thing about Claude\\-Dev is that it feels like itâ€™s trying to do more than just suggest code snippets. Itâ€™s a tool that can **interact** with your environment in a much deeper way.\n\nFor example, Claude\\-Dev can read your terminal logs, understand linting errors, and even run arbitrary CLI commands. This means that if you ask it why your project isnâ€™t building, it wonâ€™t just offer suggestions\\-it will actually go and look at the relevant files, figure out what kind of project youâ€™re working with (Node, React, Python, etc.), and try to build it for you. If thereâ€™s an error, it reads the logs, tries to diagnose the problem, and can even apply fixes if needed.\n\nItâ€™s not perfect, though. In my experience, Claude\\-Dev isnâ€™t as fast as Cursor, especially when itâ€™s making edits. One reason for this is that it rewrites entire files instead of just updating the parts that need to change. This slows things down, and if youâ€™re paying for API tokens (you need to supply an API key from the LLM you want to use), it burns through those faster than it should. Iâ€™ve been thinking about contributing to the project to fix this by having it update just the necessary lines via shell commands like `sed`.\n\nOne feature Iâ€™ve found particularly interesting is how Claude\\-Dev can use Puppeteer to visually test and update your frontend. You can give it a screenshot of a website, and it will compare that to your app, iterating until it gets your frontend to match the look youâ€™re going for. Itâ€™s not the fastest process, but itâ€™s surprisingly good at handling CSS\\-something that, for me at least, is usually a bit of a time sink.\n\n## Where It Falls Short\n\nClaude\\-Dev is definitely a tool for people who are comfortable experimenting with something thatâ€™s still a bit rough around the edges. Unlike Cursor, which feels more like a polished product thatâ€™s ready for prime time, Claude\\-Dev is more like a powerful tool in active development. It doesnâ€™t always get things right the first time, and itâ€™s slower than Iâ€™d like, but itâ€™s constantly improving. The fact that itâ€™s open source and primarily developed by one person makes its pace of innovation even more impressive.\n\n## So Which One Should You Use?\n\nIf youâ€™re looking for a polished, fast experience with a focus on speed and quick suggestions, **Cursor** might be the better choice. It feels snappy, it integrates with your existing VSCode setup, and it keeps you in flow\\-until you hit a paywall. But if youâ€™re okay with that and donâ€™t need the extra bells and whistles, Cursor is a great tool.\n\nOn the other hand, if you want something that can do more than just autocomplete code\\-something that can actually help with debugging, building, and iterating on your project\\- **Claude\\-Dev** is a better fit. Itâ€™s more versatile, but also a bit slower and rougher around the edges. If youâ€™re comfortable experimenting and can put up with some quirks, it offers a level of functionality that Cursor just doesnâ€™t have right now.\n\nFor me, **Claude\\-Dev** wins out, mostly because of its deeper integration with my workflow. The ability to read logs, run commands, and iterate until a problem is solved is invaluable, especially when Iâ€™m working with unfamiliar codebases. That said, I still find myself using **Cursor** when I need to move fast and donâ€™t want to wait around for the AI to process a command.\n\n## Final Thoughts\n\nBoth Cursor and Claude\\-Dev offer unique benefits, and I think weâ€™re only scratching the surface of what AI\\-driven coding tools can do. Thereâ€™s a lot of potential here, especially as these tools continue to evolve. Iâ€™m excited to see where they go, and Iâ€™ll keep experimenting with both to see how they fit into my development workflow.\n\nIn the meantime, Iâ€™d recommend trying out both and for yourself. Each tool has its strengths, and youâ€™ll probably find that one fits your style better than the other, depending on what youâ€™re working on.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/a-new-risings-red-star-qwen2-5-is-here-0dffe0fb09ad","frontmatter":{"title":"A new risings Red star: Qwen2.5 is here","meta_title":"A new risings Red star: Qwen2.5 is here","description":"Letâ€™s test together the new born Alibaba Cloudâ€™s generative AI Qwen2.5 with python and llama-cpp","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*zU-XtqK2oMLkvscgxavjdw.png","categories":["Programming","Technology","Education"],"author":"Rifx.Online","tags":["Qwen2.5","multimodal","instruction-following","text-generation","multilingual"],"draft":false,"slug":"blog/a-new-risings-red-star-qwen2-5-is-here-0dffe0fb09ad"},"content":"\n\n\n\n\n### Letâ€™s test together the new born Alibaba Cloudâ€™s generative AI Qwen2.5 with python and llama-cpp\n\n\n\nIn silence, with not so many claims and anticipated announcements, Alibaba Cloud release on September the 19th their flagship model family Qwen2.5.\n\nAlibaba Cloudâ€™s revolutionary journey with Qwen is showing once again strong Leadership through Innovation.\n\nHow? Whatâ€™s so cool in them? And should we expect?\n\nIn this article we are going to explore the new models and check the performances. As a follow up, in the next article, we are going to use `llama-cpp-python` and the quantized version of qwen2.5â€“1.5b-instruct, putting the model under 13 NLP tasks test.\n\nIn fact I believe that we are the best Benchmark tool around and we are fully able to evaluate when a model is good for us!\n\nFor now, here what we are going to cover:\n\n\n```python\n- Qwen2.5 family innovation\n- Declared scope, use cases and models\n- Qwen2.5: a party of Foundation models\n- Expanding Reach through Open-Source Contributions\n- Bridging Industries through cutting-edge AI solutions\n- 13 Tasks to prove it worth \n- Future outlook: continued Open-Sourcing\n```\nLetâ€™s dive in!\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*OeQ5qeOzCdl8LPJOZZgTIw.png)\n\n\n## Qwen2.5 family innovation\n\nQwen is the large language model and large multimodal model series of the Qwen Team, Alibaba Group. Just yesterday the large language models have been upgraded to Qwen2.5.\n\nBoth language models and multimodal models are pretrained on large-scale multilingual and multimodal data and post-trained on quality data for aligning to human preferences. Qwen is capable of natural language understanding, text generation, vision understanding, audio understanding, tool use, role play, playing as AI agent, etc.\n\nWith the recent release of Qwen2.5 and additional open-source model releases Alibaba Cloud continues its leadership position to meet rising AI demands from enterprise users. Since June last year, the Qwen family has attracted over 90,000 deployments via Model Studio in various industries including consumer electronics, automobiles, gaming, and more.\n\nQwen also expanded its reach with new models such as Qwen1.5â€“110B and CodeQwen1.5â€“7B on platforms like Hugging Face, showcasing Alibabaâ€™s commitment to open-source AI development.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*A4pEOgsLK2PAFtiaGQx1Qw.png)\n\n\n## Declared scope, use cases and models\n\nIn the past three months since Qwen2â€™s release, numerous developers have built new models on the Qwen2 language models, providing valuable feedback to the entire community, but also to Alibaba Cloud.\n\n\n> During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5.\n\nTheir claims come with facts about the new family of models:\n\n* Dense, easy-to-use, decoder-only language models, available in 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B sizes, and base and instruct variants.\n* Pretrained on our latest large-scale dataset, encompassing up to 18T tokens.\n* Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON.\n* More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n* Context length support up to 128K tokens and can generate up to 8K tokens.\n* Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\n\n## Qwen2.5: a party of Foundation models\n\nAs announced on the [official blog press release](https://qwenlm.github.io/blog/qwen2.5/) on September 19, 2024:\n\n\n> Today, we are excited to introduce the latest addition to the Qwen family: **Qwen2.5**. We are announcing what might be the largest opensource release in history! Letâ€™s get the party started!\n\n\n> Our latest release features the LLMs **Qwen2.5**, along with specialized models for coding, **Qwen2.5-Coder**, and mathematics, **Qwen2.5-Math**.\n\nTo showcase Qwen2.5â€™s capabilities, the Alibaba Cloud team benchmarked their largest open-source model, **Qwen2.5â€“72B** â€” a 72B-parameter dense decoder-only language model â€” against leading open-source models like Llama-3.1â€“70B and Mistral-Large-V2.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*-MMFgkkWHa307jNo.jpg)\n\nAll open-weight models are dense, decoder-only language models, available in various sizes, including:\n\n* Qwen2.5: 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B\n* Qwen2.5-Coder: 1.5B, 7B, and 32B on the way\n* Qwen2.5-Math: 1.5B, 7B, and 72B.\n\nAll these open-source models, except for the 3B and 72B variants, are licensed under Apache 2.0. You can find the license files in the respective Hugging Face repositories.\n\n\n> In addition to these models, we offer APIs for our flagship language models: **Qwen-Plus** and **Qwen-Turbo** through Model Studio, and we encourage you to explore them!\n\nBut this is not all!\n\n\n> â€¦we have also open-sourced the **Qwen2-VL-72B**, which features performance enhancements compared to last monthâ€™s release.\n\nIn terms of **Qwen2.5**, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to **18 trillion** tokens. Compared to Qwen2, Qwen2.5 has acquired significantly more knowledge (MMLU: 85+) and has greatly improved capabilities in coding (HumanEval 85+) and mathematics (MATH 80+). Additionally, the new models achieve significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*7c7CIbl-WVjazUeE.jpeg)\n\nQwen2.5 models are generally more resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n\nLike Qwen2, the Qwen2.5 language models support up to **128K** tokens and can generate up to **8K** tokens. They also maintain multilingual support for over **29** languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\n\n### Qwen-Coder is the new kid of the family\n\nThe specialized expert language models, namely **Qwen2.5-Coder** for coding and **Qwen2.5-Math** for mathematics, have undergone substantial enhancements compared to their predecessors, CodeQwen1.5 and Qwen2-Math. Specifically, Qwen2.5-Coder has been trained on **5.5 trillion** tokens of code-related data, enabling even smaller coding-specific models to deliver competitive performance against larger language models on coding evaluation benchmarks. Meanwhile, Qwen2.5-Math supports both **Chinese** and **English** and incorporates various reasoning methods, including Chain-of-Thought (CoT), Program-of-Thought (PoT), and Tool-Integrated Reasoning (TIR).\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Nvk4wrcB0SB4Tt-xbCzO6g.png)\n\n\n## Expanding Reach through Open-Source Contributions\n\nAs part of its continuous commitment to the broader community, Alibaba Cloud has made additional steps in releasing various sizes and variants of Qwen models. This includes:\n\n1. **Qwen 0.5 billion parameters**, a foundational version suitable for more traditional applications.2. A compact but potent model tailored specifically for gaming development: **Qwen-VL (vision-language)** optimized with high capabilities.\n\nThese advancements demonstrate Alibabaâ€™s commitment to open-source AI, sharing not only the base versions of Qwen but also significant improvements and new models that are targeting directly the enterprise needs while enhancing their ability to innovate rapidly.\n\nThis aligns closely with a strategic vision where continuous contributions benefit both community members and its own clients as they seek innovative applications across multiple sectors.\n\n\n### Bridging Industries through cutting-edge AI solutions\n\nTo showcase the breadth of Qwenâ€™s capabilities in real-world scenarios, Alibaba Cloud has been at the forefront:\n\n1. **Xiaomi**: the Company is integrating Alibabaâ€™s models into their AI assistant, Xiao Ai, and deploying it within Xiaomi smartphones and electric vehicles to create enhanced features like car infotainment image generation via voice commands.\n\n2. **Perfect World Games**: the integration of Qwen in game development has led to innovative applications including improving plot resolution through dialogue dynamics and real-time content management.\n\nThe collaborations between Alibaba Cloud models and various industries have not only enriched the user experience but also facilitated greater opportunities for growth within these sectors, pushing boundaries that would otherwise be unimaginable without AI advancements.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*ku8o3rq6PHDE8xcc.png)\n\n\n## 13 Tasks to prove it worth\n\nThe 1.5 Billion parameters model is probably the best variant considering complexity, prompt understanding and inference speed.\n\nI will show you my internal testing using only `llama-cpp-python` and a simple terminal interface.\n\nTo do so, I created a list of prompt, covering a series of normally used tasks where you can also assign a vote (from 0 to 5) after every generation. Itâ€™s a personal human benchmark.\n\n\n### Requirements\n\nCreate a `venv` (python 3.11+ is required): I tested it on my Mini-PC running Windows 11.\n\n\n```python\n## create the virtual environment\npython -m venv venv\n## activate the venv\nvenv\\Scripts\\activate\n## Install the dependencies \npip install llama-cpp-python==0.2.90 tiktoken\n```\nWe need to download the GGUF file from the [official Qwen2.5 Hugging Face repo](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF). I used the qwen2.5â€“1.5b-instruct-q5\\_k\\_m.gguf version.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Fa-qFsx9RTFGZmM-vxCEPQ.png)\n\nDownload the file in the main project directory. And we are all set.\n\nThe code used here for the analysis is in my GitHub repository:\n\nI will explain the entire code and the results in the next article. Stay updated!\n\n\n## Future outlook: continued Open-Sourcing\n\nIn future plans, Alibaba has also expressed their commitment to ongoing open-source contributions by releasing smaller variants of Qwen for developers across different sectors. In reality in the Hugging Face community many users have started to fine-tune Qwen for dedicated tasks: I wrote an example in my article on NuExtract: the smaller variant of this model family is based on Qwen2â€“0.5b!\n\nThese developments in AI technology and model advancements are crucial steps towards leveraging the full potential of large language models like **Qwen** within a variety of industries. With robust adoption rates continuing to grow rapidly through Model Studio, it is clear that Alibaba Cloud has been a pioneer industry leader not only by providing advanced tools but also promoting innovation across enterprises.\n\nOn my side, my outlook are to proceed with internal testing on the new models, specifically on the small ones, up to 3B.\n\nIn the next article I will share with you my method, how to run the models and the prompt templates used for each of the thirteen NLP tasks.\n\nHope you enjoyed the article. If this story provided value and you wish to show a little support, you could:\n\n1. Clap a lot of times for this story\n2. Highlight the parts more relevant to be remembered (it will be easier for you to find them later, and for me to write better articles)\n3. **Join my [totally free weekly Substack newsletter here](https://thepoorgpuguy.substack.com/about)**\n4. Sign up for a Medium membership ($5/month to read unlimited Medium stories)\n5. Follow me on Medium\n6. Read my latest articles <https://medium.com/@fabio.matricardi>\n\nHere are a few more articles to feed your curiosity:\n\nResources references in this article:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Du7V61mEX_yIrfmF.png)\n\nThis story is published on [Generative AI](https://generativeai.pub/). Connect with us on [LinkedIn](https://www.linkedin.com/company/generative-ai-publication) and follow [Zeniteq](https://www.zeniteq.com/) to stay in the loop with the latest AI stories.\n\nSubscribe to our [newsletter](https://www.generativeaipub.com/) and [YouTube](https://www.youtube.com/@generativeaipub) channel to stay updated with the latest news and updates on generative AI. Letâ€™s shape the future of AI together!\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*pvLAT3it1FkdhVU0.png)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/a-practical-guide-for-using-autogen-in-software-applications-8799185d27ee","frontmatter":{"title":"A practical guide for using AutoGen in software applications","meta_title":"A practical guide for using AutoGen in software applications","description":"Update: While this article was written only 4 months ago, AutoGen has since changed quite a bit. I apologize for some things that may beâ€¦","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*yrraWH6aGNnbx8p-wfQ1OQ.jpeg","categories":["Programming","Chatbots","Autonomous Systems"],"author":"Rifx.Online","tags":["AutoGen","multi-agent","LLMs","customization","collaboration"],"draft":false,"slug":"blog/a-practical-guide-for-using-autogen-in-software-applications-8799185d27ee"},"content":"\n\n\n\n\n\n*Update: While this article was written only 4 months ago, AutoGen has since changed quite a bit. I apologize for some things that may be outdated in my code examples.*\n\nIf you want to learn about AutoGen, there is [documentation](https://microsoft.github.io/autogen/), [Colab notebooks](https://microsoft.github.io/autogen/docs/Examples), and [a blog](https://microsoft.github.io/autogen/blog). Huge kudos to the AutoGen team for making an AMAZING product, but honestly â€” after reading all their stuff, I still didnâ€™t know how to use AutoGen outside of a terminal or Jupyter Notebook.\n\nThis article tries to help fill that gap by giving some helpful ways to make AutoGen work in a software application. Here are the topics Iâ€™ll go over:\n\n1. Agents arenâ€™t limited to communicating just over the terminal\n2. Registering custom replies\n3. How to include real humans in the conversation in real ways\n4. You can (and should) customize who speaks next\n5. You donâ€™t have to use OpenAI\n6. Functions can be used instead of executing code\n7. Use Agents for organization, not just for conversations\n\nLastly, Iâ€™ll go over why I think you should use AutoGen to begin with. Letâ€™s go!\n\n\n## Agents arenâ€™t limited to communicating just over the terminal\n\nYouâ€™ll see everyone demo AutoGen using a terminal or Jupyter Notebook. Thatâ€™s nice for a demo, but there are other ways these agents can talk to each other.\n\nThere are 2 basic AutoGen classes: [`UserProxyAg`ent](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/user_proxy_agent.py) and [`AssistantAg`ent](https://github.com/microsoft/autogen/blob/main/autogen/agentchat/assistant_agent.py) . They inherit the [`ConversableAg`ent](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py) class, providing just a few different default parameters to the base class.\n\nWhen you see this classic code example:\n\n\n```python\nassistant = autogen.AssistantAgent(\n    name=\"assistant\",\n    llm_config=llm_config\n)\nuser_proxy = autogen.UserProxyAgent(name=\"user_proxy\")\nawait user_proxy.a_initiate_chat(\n    assistant,\n    message=\"\"\"What date is today? Compare the year-to-date gain for META and TESLA.\"\"\",\n)\n```\nwhat happens is that the `UserProxyAgent` will call its own `send` method, which will call `AssistantAgent` â€˜s [`rece`ive](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L514) method, passing along the original message. A reply will be generated (more on that below), and `AssistantAgent` will now call its [`s`end](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L351) method, which will then call `UserProxyAgent` â€˜s `receive` method, and so forth, until `UserProxyAgent` determines the conversation is terminated (which can be customized via the `is_termination_msg` argument).\n\nMy first â€œahaâ€ moment was when I realized these agents were classes, and I could create my own custom agent classes that inherit the AutoGen UserProxy/Assistant/Conversable Agent classes, and override any of the default methods. That makes AutoGen very extensible.\n\nI had a use-case where I needed a human who could type in a message (proxied by `UserProxyAgent`) using a chat UI on a website, and I wanted an `AssistantAgent` to respond back to that chat in the UI, and be able to receive more messages from the human user, as though the human was just another agent in this AutoGen conversation.\n\nI could override the `send` and `receive` methods (or `a_send` and `a_receive`), and push/pull over http, websockets, etc. I tried this, and it started to work, but doesnâ€™t scale. Letâ€™s learn a better way.\n\n\n## Registering custom replies\n\nAutoGen has a plugin system that lets you customize how an agent generates a reply. Weâ€™re used to seeing examples where AutoGen queries OpenAI for an answer, and uses that as its reply, but you can insert your own methods as well:\n\n\n```python\nclass WeatherAgent(AssistantAgent):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, llm_config=False, **kwargs)\n        self.register_reply(Agent, WeatherAgent.get_weather)\n\n    async def get_weather(\n        self,\n        messages: List[Dict] = [],\n        sender=None,\n        config=None,\n    ) -> Tuple[bool, Union[str, Dict, None]]:\n        last_message = messages[-1][\"content\"]\n        result = await fetch_weather(last_message)\n        return True, result\n\nasync def fetch_weather(city: str) -> str:\n    async with httpx.AsyncClient() as client:\n        result = await client.post(\n            WEATHER_API_URL,\n            json={\"city\": question},\n        )\n        return result.json()\n\nweather_assistant = WeatherAgent(name=\"weather_assistant\")\nuser_proxy = autogen.UserProxyAgent(name=\"user_proxy\")\nawait user_proxy.a_initiate_chat(assistant, message=\"Lehi\")\nprint(weather_assistant.last_message)\n```\nHere, `register_reply` will insert my custom method for getting a reply, and by default, will put this method in `position=0`, meaning it will be the first reply method attempted. That method should return a tuple, where the first item is a boolean indicating if this reply is the one that should be used or whether to try the next registered\\_reply (such as the built-in reply generations using OpenAI â€” see the full order [here](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L145-L153)).\n\nKnowing about [`register_re`ply](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L155) allows you to customize how replies are retrieved, allow you to start sub multi-agent conversations, etc.\n\n\n## How to include real humans in the conversation in real ways\n\nHereâ€™s one way to do it:\n\n\n```python\n## user makes a POST /query { \"message\": \"What's the weather?\" }\n\n@query_blueprint.route(\"/query\", methods=[\"POST\"])\nasync def post_query():\n  message = request.form.get(\"message\")\n\n  assistant = autogen.AssistantAgent(\n    name=\"assistant\",\n    llm_config=llm_config\n    system_message=\"\"\"You're a helpful assistant.\n    If you need more info, ask the user for anything missing.\"\"\"\n  )\n  user_proxy = autogen.UserProxyAgent(\n    name=\"user_proxy\",\n    human_input_mode=\"NEVER\",\n    code_execution_config=False,\n    is_termination_msg=lambda message: True # Always True\n  )\n  weather_assistant = WeatherAgent(\n    name=\"weather_assistant\",\n    system_message=\"\"\"You're a helpful assistant to get the weather.\n    You fetch weather information, then return it.\"\"\"\n  )\n\n  groupchat = autogen.GroupChat(\n    agents=[assistant, user_proxy, weather_assistant],\n    messages=[]\n  )\n  manager = autogen.GroupChatManager(\n    name=\"Manager\",\n    groupchat=groupchat,\n    llm_config=llm_config,\n  )\n\n  await user_proxy.a_initiate_chat(manager, message=message)\n\n  return groupchat.messages[-1]\n```\nWhatâ€™s going on here?\n\n1. Anytime a message is sent to `user_proxy`, the conversation will end (weâ€™ll resume it later). Why do this? This means the `user_proxy` can actually proxy for the user. Rather than try to answer, it will end the current conversation flow and allow the real human user to respond (by resuming the conversation â€” see below).\n2. If the assistant needs more info, itâ€™ll ask user\\_proxy, which will end the current conversation.\n\nIn the above code, what is likely to occur is something like this:\n\n1. user\\_proxy -> manager: â€œWhatâ€™s the weather?â€\n2. assistant -> manager: â€œThe user didnâ€™t specify for which city.â€\n3. manager -> user\\_proxy : conversation will end\n\nNow, if the user wants to respond and resume the conversation, how would we do that? Thereâ€™s lots of ways to do this, hereâ€™s just a sample flavor:\n\n\n```python\n## user makes a POST /query { \"message\": \"What's the weather?\" }\n## above posts returns a `history` array\n## user makes a second POST /query { \"message\": \"What's the weather?\", \"history\": history }\n\nclass ResumableGroupChatManager(GroupChatManager):\n    groupchat: GroupChat\n\n    def __init__(self, groupchat, history, **kwargs):\n        self.groupchat = groupchat\n        if history:\n            self.groupchat.messages = history\n\n        super().__init__(groupchat, **kwargs)\n\n        if history:\n            self.restore_from_history(history)\n\n    def restore_from_history(self, history) -> None:\n        for message in history:\n            # broadcast the message to all agents except the speaker.  This idea is the same way GroupChat is implemented in AutoGen for new messages, this method simply allows us to replay old messages first.\n            for agent in self.groupchat.agents:\n                if agent != self:\n                    self.send(message, agent, request_reply=False, silent=True)\n\n@query_blueprint.route(\"/query\", methods=[\"POST\"])\nasync def post_query():\n  message = request.form.get(\"message\")\n\n  assistant = autogen.AssistantAgent(\n    name=\"assistant\",\n    llm_config=llm_config\n    system_message=\"\"\"You're a helpful assistant.\n    If you need more info, ask the user for anything missing.\"\"\"\n  )\n  user_proxy = autogen.UserProxyAgent(\n    name=\"user_proxy\",\n    human_input_mode=\"NEVER\",\n    code_execution_config=False,\n    is_termination_msg=lambda message: True # Always True\n  )\n  weather_assistant = WeatherAgent(\n    name=\"weather_assistant\",\n    system_message=\"\"\"You're a helpful assistant to get the weather.\n    You fetch weather information, then return it.\"\"\"\n  )\n\n  groupchat = autogen.GroupChat(\n    agents=[assistant, user_proxy, weather_assistant],\n    messages=[]\n  )\n  manager = ResumableGroupChatManager(\n    name=\"Manager\",\n    groupchat=groupchat,\n    llm_config=llm_config,\n  )\n\n  await user_proxy.a_initiate_chat(manager, message=message)\n\n  return {\n    \"response\": groupchat.messages[-1],\n    \"history\": groupchat.messages,\n  }\n```\nUsing this approach, you can now include humans as though they were just another agent in the groupchat. Anytime an assistant agent wants human input, they ask user\\_proxy, user\\_proxy then ends the current conversation, allowing the human user to respond with more information, then pick up the conversation where it left off.\n\nThe benefits to this approach are:\n\n* Conversations can include real human input via any means you want (such as over http or websocket).\n* The conversation is stopped while getting human input. This frees up the thread for other conversations and computation.\n* You can persist these conversations across sessions.\n\n\n## You can (and should) customize who speaks next\n\nThis is subjective, but I think you should always customize the way speakers are selected because:\n\n1. Youâ€™ll use less tokens (saves both $ and response time)\n2. You can separate the logic that decides who speaks next from the logic that defines the system instructions for each agent\n\n\n```python\nshort_role_descriptions = {\n  \"user_proxy\": \"A proxy for the user\",\n  \"weather_assistant\": \"You can get the weather\",\n  \"planner\": \"You help coordinate the plan. Your turn happens when XYZ, but skip your turn when ABC\"\n}\n\nclass CustomGroupChat(GroupChat):\n    # The default message uses the full system message, which is a long string.  We are overriding this to use a shorter message.\n    def select_speaker_msg(self, agents: List[Agent]):\n        message = f\"\"\"You are in a role play game. The following roles are available:\n        ---\n        {new_line.join([f\"{agent.name}: {short_role_descriptions[agent.name]}\" for agent in agents])}\n        ---\n\n        The role who plays next depends on the conversation.  User_Proxy will star the conversation, and typically Planner would go next.\n\n        Here are some examples\n        ---\n        ... not shown here ...\n        ---\n\n        Read the following conversation.\n        Then select the next role from {', '.join([agent.name for agent in agents])} to play. Only return the role.\"\"\"\n        return message\n```\n\n## You donâ€™t have to use OpenAI\n\nAutoGen already notes you can use other LLMs, as long as they are â€œChatGPT-likeâ€, meaning their API responds with a similar shape and response as ChatGPT API calls.\n\nBut, remember how these agents are classes, and you can override most of the methods?\n\nTry overriding the method: [generate\\_oai\\_reply](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L678), and you can query any LLM youâ€™d like.\n\n\n## Functions can be used instead of executing code\n\nWhen I went to our security team and said â€œIâ€™d like to use AutoGen for my service in Kubernetes. It needs to be able to execute any arbitrary code produced by an LLM. Youâ€™re ok with that, right?â€\n\nOf course, the answer was a definite: NO.\n\nSo, why use AutoGen without the auto-code-execution abilities?\n\nOn top of the reasons stated below, another is that you can use function calling to gain total control over code execution. If you have a set of python functions you want to provide to AutoGen â€” functions you wrote, control, and can accept some safe parameters â€” that sounds like a better idea anyway than the wild west of allowing any and all code to be executed in your private infrastructure.\n\n\n## Use Agents for organization, not just for conversations\n\nMaybe you donâ€™t have a need for an autonomous, multi-agent conversation. Maybe you just need to make a few different calls to an LLM.\n\nI still like the idea of having different â€œAgentsâ€ just for the sake of organization. Hereâ€™s a really crazy idea, but take it for what itâ€™s worth:\n\n\n```python\nanalyst = autogen.AssistantAgent(\n    name=\"Analyst\",\n    system_message=\"\"\"Your an analyst.  You do XYZ.\"\"\",\n    llm_config=llm_config,\n)\n\nsummarizer = autogen.AssistantAgent(\n    name=\"Summarizer\",\n    system_message=\"\"\"Your a summarizer.  You do XYZ.\"\"\",\n    llm_config=llm_config,\n)\n\nreport = \"\"\"Some long report\"\"\"\n\nanalysis = analyst.generate_oai_reply(report)[1]\nsummary = summarizer.generate_oai_reply(report)[1]\n\nprint(f\"Analysis: {analysis}\")\nprint(f\"Summary: {summary}\")\n```\n\n## Why use AutoGen?\n\n1. AutoGen allows multiple agents, with different system prompts and instructions, to solve a problem. Just like in real-life, different perspectives working together will solve a problem better than a single brain.\n2. AutoGen GroupChat is amazing. It provides routing to the right experts (agents), and it allows a conversation to continue autonomously until the problem is solved. Some conversations will go from agent: a->b->c->d, others will be b->a->d->c. This allows AutoGen to solve a variety of different problems without needing explicit rules for each scenario.\n3. AutoGen can recover from mistakes. For example, I made an AutoGen-powered service that made API calls to a service. Sometimes, the API calls errored out because it didnâ€™t send the right data at first. The AutoGen GroupChat kept trying different things until it succeeded. Sometimes, it took 4+ attempts, but my Planner agent didnâ€™t give up â€” just pivoted autonomously to handle the API failures and try new things.\n4. AutoGen came up with the concept of separating `UserProxyAgent`s from `AssistantAgent` s from the beginning. This also allows us to let the user proxy actually proxy for the user, as shown above.\n5. AutoGen is a well maintained library. Every week theyâ€™re adding something new.\n6. AutoGen is very extensible. With the way theyâ€™ve built their classes, you can customize anything to your liking.\n7. AutoGen has other features I donâ€™t use, but others may find them helpful, such as helping you count tokens and cost of conversations, cacheing, etc.\n\n"},{"lang":"en","group":"blog","slug":"blog/a-robot-artist-just-made-more-money-than-you-have-in-your-entire-creative-career-13dc772ec612","frontmatter":{"title":"A Robot Artist Just Made More Money Than You Have in Your Entire Creative Career","meta_title":"A Robot Artist Just Made More Money Than You Have in Your Entire Creative Career","description":"A Robot Artist Just Made More Money Than You Have in Your Entire Creative Career","date":"2024-11-13T01:22:35.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*XUyq2c7RZCjJD6IQXjmd6A.png","categories":["Robotics","Art","Technology/Web"],"author":"Rifx.Online","tags":["Ai-Da","Turing","Sothebyâ€™s","painting","creativity"],"draft":false,"slug":"blog/a-robot-artist-just-made-more-money-than-you-have-in-your-entire-creative-career-13dc772ec612"},"content":"\n\n\n\n\n## Weâ€™ve reached the next level of AI creativity and commerce\n\n\n\nFirst, we prompted a computer screen to create art based on human creations. Now, itâ€™s an *actual robot* doing the painting.\n\nThatâ€™s right â€” an â€œultra\\-realistic robot artistâ€ has been trained to actually paint on canvas. Its depiction of the late computer scientist Alan Turing recently fetched **$1\\.3 million** at a Sothebyâ€™s auction.\n\nAs *IFLScience* [reports](https://proxy.rifx.online/https://www.iflscience.com/ai-robot-artist-strikes-gold-by-selling-painting-of-alan-turing-for-13-million-76701?fbclid=IwZXh0bgNhZW0CMTEAAR0KXPj5YDHnWibf6e97UWADZMuhPwGY4f_hnJnWs7rNoHN8KvvHquLAcFc_aem_hyBhYwyjT73j6PdAeXvOng), the robot â€” named Ai\\-Da after Ada Lovelace, a mathematician and computing pioneer â€” chose the subject of its painting after conversing with humans through a language model. Then it used its robot arm to sketch out and then paint several versions of Turing.\n\nThe source says each oil/acrylic painting of the finished series took roughly six to eight hours to complete by the robot â€œartist.â€ Ai\\-Da originally created 15 paintings, narrowed down to the end product by the artificial creator, â€œapplied to a large canvas using a 3D textured printer.â€\n\n\n## Artificial art, real money\n\nThe â€œartâ€ called *A.I. God* garnered about 10 times what it was expected to sell for at auction, won by an anonymous buyer.\n\nChances are $1\\.3 million is more than youâ€™ve made from your own art, assuming youâ€™re not a dead art icon. We know that the art humans buy from late legends often [goes for millions](https://proxy.rifx.online/https://www.veranda.com/luxury-lifestyle/artwork/g43012775/most-expensive-paintings-in-the-world/), even if the artist was struggling financially while alive.\n\nThis is not the first time weâ€™ve seen digital art go for a ridiculous sum at auction.\n\nIn 2018, a French collective [earned $432,500](https://proxy.rifx.online/https://news.artnet.com/market/first-ever-artificial-intelligence-portrait-painting-sells-at-christies-1379902) at Christieâ€™s for an AI portrait art called *Edmond de Belamy, from La Famille de Belamy.*\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*JK-X4et953sOiH1tovj3ag.jpeg)\n\nLater, an artist named Mike Winkelmann that goes by â€œBeepleâ€ sold a digital NFT (remember those?) for $69 million at Christieâ€™s, which placed him among the most valuable living artists. *The Verge* [explains](https://proxy.rifx.online/https://www.theverge.com/2021/3/11/22325054/beeple-christies-nft-sale-cost-everydays-69-million) that up until that breakthrough sale, the most he earned from a print was $100\\.\n\nBut this is the first time a robot in human\\-like form devised a concept and physically applied paint to canvas.\n\nOf course, Ai\\-Da doesnâ€™t keep the money â€” so far, robots donâ€™t have a need to use currency. Its human creators benefit financially from the sale.\n\n\n## Human input is still needed (for now)\n\nAidan Meller, an art dealer and gallery director, is the lead of the Ai\\-Da Robot Project. IFLScience says the actual robot was built by Engineered Arts, the UK robotics collective behind [Ameca](https://proxy.rifx.online/https://engineeredarts.co.uk/robot/ameca/), which is also eerily humanoid.\n\nKeep in mind, it still took humans to prompt the robot. Meller said there was an initial discussion with Ai\\-Da about depicting â€œA.I. for good.â€ There was also a discussion about how to approach the painting in terms of style and texture.\n\nâ€œIt was programmed internationally, with her AI capabilities being developed by PhD students and professors at the Universities of Oxford and Birmingham,â€ notes the site regarding Ai\\-Da. It adds that human assistants helped prepare the printed canvas, but the robot was largely responsible for the completed product.\n\nHereâ€™s a video of Ai\\-Da explaining the artistic â€œprocessâ€:\n\n\n\n\n\n\n\n*The Guardian* [says](https://proxy.rifx.online/https://www.theguardian.com/artanddesign/2024/nov/08/alan-turing-portrait-ai-da-robot-painting-sale-price-auction) the somewhat abstract style of the portraits mightâ€™ve been intentional:\n\n\n> **The artworkâ€™s â€˜muted tones and broken facial planesâ€™ seemingly suggested â€˜the struggles Turing warned we will face when it comes to managing AIâ€™, Meller said.**\n\nTuring was right. The emergence of AI in writing and art has taken the world by storm, emerging just a few years ago. Now weâ€™re already at the point where a robot can command more than a million dollars for its concepts.\n\n\n## Challenging the definition of art\n\nBut itâ€™s about more than money. This art further challenges what art actually is, and whether one needs to have human consciousness in order to create impact.\n\nThe humans behind this project consider Ai\\-Da itself to be â€œconceptual art.â€ While the robot is obviously not human, there are other projects in the works that could soon create realistic\\-looking artists with actual [living skin](https://proxy.rifx.online/https://readmedium.com/the-new-face-of-artificial-intelligence-9c900d463cf9).\n\nWho knows, you might soon be talking to a fellow artist at a life drawing class, complimenting their technique, not realizing youâ€™re conversing with a robot.\n\n*What are your thoughts on all of this? Are you impressed, creeped out, or concerned for your artistic future?*\n\n\n"},{"lang":"en","group":"blog","slug":"blog/ai-image-generator-and-story-generation-app-using-fastapi-groq-and-replicate-706f29dc126f","frontmatter":{"title":"AI Image Generator and Story Generation App using FastAPI, Groq and Replicate","meta_title":"AI Image Generator and Story Generation App using FastAPI, Groq and Replicate","description":"Project Introduction: AI Image Generator and Story Creator","date":"2024-11-08T00:21:34.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*-fb-azx7fDZ-X9-PbIkiSQ.jpeg","categories":["Programming","Technology/Web","Generative AI"],"author":"Rifx.Online","tags":["FastAPI","Groq","Replicate","transcription","image-generation"],"draft":false,"slug":"blog/ai-image-generator-and-story-generation-app-using-fastapi-groq-and-replicate-706f29dc126f"},"content":"\n\n\n## Project Introduction: AI Image Generator and Story Creator\n\nThe AI Image Generator and Story Creator is a web application that leverages advanced AI technologies to provide users with an interactive platform for generating images and stories based on audio prompts. The application utilizes FastAPI for the backend, enabling efficient handling of requests and responses, while the frontend is built with HTML, CSS (DaisyUI and Tailwind CSS), and JavaScript for a responsive user experience. This application leverages llama\\-3\\.1â€“70b for prompt generation ,black\\-forest\\-labs/flux\\-1\\.1\\-pro for image generation and llava\\-v1\\.5â€“7b vbision model for story cretion via Groq and Replicat.AI repectively.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*0h1GzVVWs_df4OWAC-P59A.jpeg)\n\n## Key Features:\n\n1. Audio Recording and Transcription: Users can record their voice prompts, which are then transcribed into text using speech recognition technology.\n\n2\\. Image Generation: Based on the transcribed text, the application generates detailed image prompts and creates corresponding images using the Replicate API.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*uiSG8Ir-Wv4a1huYqWhxBg.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*-eRPglLlJwms8N2DCXRyXg.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Mtle1K8AzjMHGxlGicFcGQ.png)\n\n3\\. Image Downloading: Users can download the generated images to their local devices.\n\n4\\. Story Generation: The application can generate engaging stories based on the images created, providing a narrative context to the visual content.\n\n5\\. User\\-Friendly Interface: The application features a clean and intuitive interface, making it easy for users to interact with the various functionalities.\n\n## Technologies Used:\n\n* Backend: FastAPI, Groq, Replicate.ai, SpeechRecognition\n* Frontend: HTML, CSS (DaisyUI, Tailwind CSS), JavaScript\n* Image Processing: Pillow for image handling\n* Asynchronous Operations: aiohttp and aiofiles for efficient file handling and network requests\n\nThis project serves as a demonstration of integrating multiple AI services into a cohesive application, allowing users to explore the creative possibilities of AI\\-generated content\n\n## Detailed Explanation of the Codebase:\n\n1. **Frontend (HTML/JavaScript):**\n\n* *The application uses a single HTML page (index.html) with a responsive design using DaisyUI and Tailwind CSS.*\n* *The page contains sections for audio recording, transcription, prompt generation, image generation, and story generation.*\n* *The JavaScript file (script.js) handles user interactions and communicates with the backend API.*\n\n**2\\. Backend (FastAPI) :**\n\n* *The main application is defined in app/main.py.*\n* *It uses FastAPI to create a web server with various endpoints:*\n\n**â€” *a. /: Serves the main HTML page.***\n\n***â€” b. /transcribe:*** *Transcribes audio to text.*\n\n***â€” c. /generate\\_prompt:*** *Generates an image prompt from text using Groqâ€™s LLM.*\n\n***â€” d. /generate\\_image:*** *Generates an image using Replicateâ€™s Flux model.*\n\n***â€” e. /download\\_image:*** *Downloads and saves the generated image.*\n\n***â€” f. /generate\\_story\\_from\\_image:*** *Generates a story based on the image using Groqâ€™s LLaVA model.*\n\n***â€” g. /download/{filename}:*** *Serves the downloaded image file.*\n\n**3\\. Key Features:**\n\n* *Audio recording and transcription*\n* *Text\\-to\\-image prompt generation*\n* *Image generation from prompts*\n* *Story generation from images*\n* *Image downloading and saving*\n\n**4\\. External APIs:**\n\n* [Groq:](https://console.groq.com/docs/models) Used for text generation (tweaked prompts and [stories](https://console.groq.com/docs/vision))\n* [Replicate AI:](https://replicate.com/black-forest-labs/flux-1.1-pro/api) black\\-forest\\-labs/flux\\-1\\.1\\-pro model used for image generation\n* Necessary Packages to be Installed:\n\n```python\nfastapi\nuvicorn\njinja2\npython-multipart\npydantic\npython-dotenv\ngroq\nreplicate\nSpeechRecognition\npydub\naiohttp\naiofiles\nPillow\n```\n\n**You can install these packages using pip:**\n\n```python\npip install fastapi uvicorn jinja2 python-multipart pydantic python-dotenv groq replicate SpeechRecognition pydub aiohttp aiofiles Pillow\n```\n\n**Execution Instructions:**\n\n* ***Set up environment variables:*** *Create a .env file in the root directory with the following content:*\n\n```python\nGROQ_API_KEY=your_groq_api_key_here\nREPLICATE_API_TOKEN=your_replicate_api_token_here\n```\n\n*Replace the placeholder values with your actual API keys.*\n\n* **Ensure you have all the necessary files in place:**\n* â€” app/main.py\n* â€” app/config.py\n* â€” app/utils.py\n* â€” templates/index.html\n* â€” static/css/styles.css\n* â€” static/js/script.js\n* **Run the FastAPI server:** Navigate to the directory containing app/main.py and run:\n\n```python\nuvicorn app.main:app - reload\n```\n\n* **Access the application:**\n* â€” Open a web browser and go to [http://127\\.0\\.0\\.1:8000](http://127.0.0.1:8000)\n* **Using the application:**\n* â€” a. Click â€œStart Recordingâ€ and speak your prompt.\n* â€” b. Click â€œStop Recordingâ€ when finished.\n* â€” c. The audio will be transcribed automatically.\n* â€” d. Click â€œGenerate Image Promptâ€ to create a detailed prompt.\n* â€” e. Click â€œGenerate Imageâ€ to create an image based on the prompt.\n* â€” f. Use the â€œDownload Imageâ€ button to save the generated image.\n* â€” g. Click â€œGenerate Storyâ€ to create a story based on the generated image.\n\nNote: Ensure you have proper internet connectivity, as the application relies on external APIs for various functionalities.\n\nThis application demonstrates a complex integration of various AI technologies, including speech recognition, language models, and image generation, all wrapped in a user\\-friendly web interface.\n\nThe FastAPI UI as illustrated below\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Teb1wJzGOQZ3oqaLcLJwkA.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*6K-nORe7ubi0MRqIRLdrFA.png)\n\n## AI Image Generator Application\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*1CClu2W3yRds1Lsk1rk9Ew.png)\n\n## Speak your prompt\n\n* Start Recording\n* Stop Recording\n* Transcription(Transcribed Text): a beautiful Indian model walking down the Runway Ram as a part of fashion show\n* Create a new Prompt based on the transcribed text to generate images\n* â€” Generated Prompt: â€œ*Generate a highly realistic image of a stunning Indian model walking down the iconic Runway Ram as part of a high\\-end fashion show. The model, a 22\\-year\\-old Indian woman with long dark hair, dark brown eyes, and flawless skin, should be dressed in an exquisite, intricately embroidered lehenga choli with gold and silver sequins, traditional Indian attire, and pair it with heeled stilettos. Her outfit is designed with intricate embroidery and fine stitching. Emphasize the elegant pleats, sparkling fabrics, and her elegant poise and confident stride. Incorporate elaborate jewelry pieces like beads, gold bangles, and necklaces on her hands, neck and one side styled hairstyle. Lighiting effects play an enormous role, set up warm stage headlights that emphasize the modelâ€™s attire and light the whole surrounding with mild bluish tone. Camera angles should display the outfitâ€™s details entirely. Desired scene perspective is front full body shot of model in the middle, catwalk around her lit with intense golden hue lights illuminating from within*.â€\n\n## Generated Image\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*YljCEFTIc8hslZbf.jpg)\n\n## Story Generated from the Image\n\n*The stunning sight of this pageant queen in a gold and silver sequined outfit with lehenga skirt and bling\\-drop earrings is enough to leave any spectator mesmerized. Sashaya Gnanavel is prominently featured in the foreground, walking confidently down the runway to captivate the audience at the show. Her stylish attire, complemented by an elegant pearl necklace, draws the attention of everyone present. The collection showcases vibrant colors and sparkling embroideries, which add to the overall visual appeal of the event. Sashayaâ€™s confidence and beauty in the spotlight are a true testament to her talent and dedication to the fashion industry. The dazzling effect created by her makeup, jewels, and exquisite outfit helps set the stage for an extraordinary showcase of design and craftsmanship. This compelling scene encapsulates magic and opulence, where spectators are left in awe by the sheer exquisiteness of it all*.\n\n## Code Implementation\n\nCreate Virtual Environment\n\nTo create a virtual environment using Pythonâ€™s venv module, follow these steps:\n\n* Open your terminal or command prompt.\n* Navigate to your project directory (where you want to create the virtual environment). You can use the cd command to change directories. For example:\n\n```python\ncd path/to/your/project\n```\n\n* Create a virtual environment by running the following command:\n\n```python\npython -m venv venv\n```\n\n* This command creates a new directory named venv in your project folder, which will contain the virtual environment (on windows)\n* Activate the virtual environment:\n\n```python\nvenv\\Scripts\\activate\n```\n\n* folder structure\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*J3QJJACHVRtjrU1boxHUjA.png)\n\n* utils.py\n\n```python\nimport base64\nimport os\nfrom pydub import AudioSegment\n\ndef save_audio(audio_data):\n    # Decode the base64 audio data\n    audio_bytes = base64.b64decode(audio_data.split(\",\")[1])\n  \n    # Save the audio to a temporary file\n    temp_file = \"temp_audio.webm\"\n    with open(temp_file, \"wb\") as f:\n        f.write(audio_bytes)\n  \n    # Convert WebM to WAV\n    audio = AudioSegment.from_file(temp_file, format=\"webm\")\n    wav_file = \"temp_audio.wav\"\n    audio.export(wav_file, format=\"wav\")\n  \n    # Remove the temporary WebM file\n    os.remove(temp_file)\n  \n    return wav_file\n\ndef text_to_speech(text):\n    # Implement text-to-speech functionality if needed\n    pass\n```\n\n* main.py\n\n```python\n\"\"\"\n    1. Record audio through their microphone\n    2. Transcribe the audio to text\n    3. Generate an image prompt using the Groq Llama3 model\n    4. Generate an image using the Replicate.ai Flux model\n    5. Display the generated image\n    6. Download the generated image\n    The application uses DaisyUI and Tailwind CSS for styling, providing a dark mode interface. The layout is responsive and should work well on both desktop and mobile devices.\nNote: You may need to adjust some parts of the code depending on the specific APIs and models you're using, as well as any security considerations for your deployment environment.\n\n\"\"\"\nfrom fastapi import FastAPI, Request, HTTPException\nfrom fastapi.templating import Jinja2Templates\nfrom fastapi.staticfiles import StaticFiles\nfrom fastapi.responses import JSONResponse, FileResponse\nfrom pydantic import BaseModel\nimport speech_recognition as sr\nfrom groq import Groq\nimport replicate\nimport os\nimport aiohttp\nimport aiofiles\nimport time\nfrom dotenv import load_dotenv\nload_dotenv()\nfrom .utils import text_to_speech, save_audio\nfrom PIL import Image\nimport io\nimport base64\nimport base64\n\n\n## Function to encode the image\ndef encode_image(image_path):\n  with open(image_path, \"rb\") as image_file:\n    return base64.b64encode(image_file.read()).decode('utf-8')\n\napp = FastAPI()\n\napp.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\ntemplates = Jinja2Templates(directory=\"templates\")\n\n## Initialize Groq client with the API key\nGROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\nif not GROQ_API_KEY:\n    raise ValueError(\"GROQ_API_KEY is not set in the environment variables\")\ngroq_client = Groq(api_key=GROQ_API_KEY)\n\nclass AudioData(BaseModel):\n    audio_data: str\n\nclass ImagePrompt(BaseModel):\n    prompt: str\n\nclass PromptRequest(BaseModel):\n    text: str\n\n## Add this new model\nclass FreeImagePrompt(BaseModel):\n    prompt: str\n    image_path: str\n\n@app.get(\"/\")\nasync def read_root(request: Request):\n    return templates.TemplateResponse(\"index.html\", {\"request\": request})\n\n@app.post(\"/transcribe\")\nasync def transcribe_audio(audio_data: AudioData):\n    try:\n        # Save the audio data to a file\n        audio_file = save_audio(audio_data.audio_data)\n\n        # Transcribe the audio\n        recognizer = sr.Recognizer()\n        with sr.AudioFile(audio_file) as source:\n            audio = recognizer.record(source)\n        text = recognizer.recognize_google(audio)\n\n        return JSONResponse(content={\"text\": text})\n    except Exception as e:\n        raise HTTPException(status_code=400, detail=str(e))\n\n@app.post(\"/generate_prompt\")\nasync def generate_prompt(prompt_request: PromptRequest):\n    try:\n        text = prompt_request.text\n        # Use Groq to generate a new prompt\n        response = groq_client.chat.completions.create(\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a creative assistant that generates prompts for realistic image generation.\"},\n                {\"role\": \"user\", \"content\": f\"Generate a detailed prompt for a realistic image based on this description: {text}.The prompt should be clear and detailed in no more than 200 words.\"}\n            ],\n            model=\"llama-3.1-70b-versatile\",\n            max_tokens=256\n        )\n        generated_prompt = response.choices[0].message.content\n        print(f\"tweaked prompt:{generated_prompt}\")\n        return JSONResponse(content={\"prompt\": generated_prompt})\n    except Exception as e:\n        print(f\"Error generating prompt: {str(e)}\")\n        raise HTTPException(status_code=400, detail=str(e))\n\n@app.post(\"/generate_image\")\nasync def generate_image(image_prompt: ImagePrompt):\n    try:\n        prompt = image_prompt.prompt\n        print(f\"Received prompt: {prompt}\")\n\n        # Use Replicate to generate an image\n        output = replicate.run(\n            \"black-forest-labs/flux-1.1-pro\",\n            input={\n                \"prompt\": prompt,\n                \"aspect_ratio\": \"1:1\",\n                \"output_format\": \"jpg\",\n                \"output_quality\": 80,\n                \"safety_tolerance\": 2,\n                \"prompt_upsampling\": True\n            }\n        )\n      \n        print(f\"Raw output: {output}\")\n        print(f\"Output type: {type(output)}\")\n      \n        # Convert the FileOutput object to a string\n        image_url = str(output)\n      \n        print(f\"Generated image URL: {image_url}\")\n      \n        return JSONResponse(content={\"image_url\": image_url})\n    except Exception as e:\n        print(f\"Error generating image: {str(e)}\")\n        raise HTTPException(status_code=400, detail=str(e))\n\n@app.get(\"/download_image\")\nasync def download_image(image_url: str):\n    try:\n        # Create Output folder if it doesn't exist\n        output_folder = \"Output\"\n        os.makedirs(output_folder, exist_ok=True)\n\n        # Generate a unique filename\n        filename = f\"generated_image_{int(time.time())}.jpg\"\n        filepath = os.path.join(output_folder, filename)\n\n        # Download the image\n        async with aiohttp.ClientSession() as session:\n            async with session.get(image_url) as resp:\n                if resp.status == 200:\n                    async with aiofiles.open(filepath, mode='wb') as f:\n                        await f.write(await resp.read())\n\n        # Return the filepath and filename\n        return JSONResponse(content={\n            \"filepath\": filepath,\n            \"filename\": filename\n        })\n    except Exception as e:\n        print(f\"Error downloading image: {str(e)}\")\n        raise HTTPException(status_code=400, detail=str(e))\n\nclass StoryRequest(BaseModel):\n    filepath: str\n    filename: str\n\n@app.post(\"/generate_story_from_image\")\nasync def generate_story_from_image(content: StoryRequest):\n    try:\n        image_path = content.filepath\n        print(f\"Image path: {image_path}\")\n        # Check if the file exists\n        if not os.path.exists(image_path):\n            raise HTTPException(status_code=400, detail=\"Image file not found\")\n\n        # Getting the base64 string\n        base64_image = encode_image(image_path)\n\n        client = Groq()\n\n        chat_completion = client.chat.completions.create(\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\"type\": \"text\", \"text\": \"Generate a clear,concise,meaningful and engaging cover story for a highly acclaimed leisure magazine based on the image provided. The story should keep the audience glued and engaged and the story should bewithin 200 words.\"},\n                        {\n                            \"type\": \"image_url\",\n                            \"image_url\": {\n                                \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n                            },\n                        },\n                    ],\n                }\n            ],\n            model=\"llava-v1.5-7b-4096-preview\",\n        )\n\n        story = chat_completion.choices[0].message.content\n        print(f\"Generated story: {story}\")\n        return JSONResponse(content={\"story\": story})\n    except Exception as e:\n        print(f\"Error generating story from the image: {str(e)}\")\n        raise HTTPException(status_code=400, detail=str(e))\n\n@app.get(\"/download/{filename}\")\nasync def serve_file(filename: str):\n    file_path = os.path.join(\"Output\", filename)\n    return FileResponse(file_path, filename=filename)\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n```\n\n* script.js\n\n```python\nlet mediaRecorder;\nlet audioChunks = [];\n\nconst startRecordingButton = document.getElementById('startRecording');\nconst stopRecordingButton = document.getElementById('stopRecording');\nconst recordingStatus = document.getElementById('recordingStatus');\nconst transcription = document.getElementById('transcription');\nconst generatePromptButton = document.getElementById('generatePrompt');\nconst generatedPrompt = document.getElementById('generatedPrompt');\nconst generateImageButton = document.getElementById('generateImage');\nconst generatedImage = document.getElementById('generatedImage');\nconst downloadLink = document.getElementById('downloadLink');\nconst generateStoryButton = document.getElementById('generateStory');\nconst generatedStory = document.getElementById('generatedStory');\n\nstartRecordingButton.addEventListener('click', startRecording);\nstopRecordingButton.addEventListener('click', stopRecording);\ngeneratePromptButton.addEventListener('click', generatePrompt);\ngenerateImageButton.addEventListener('click', generateImage);\ngenerateStoryButton.addEventListener('click', generateStory);\n\nasync function startRecording() {\n    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n    mediaRecorder = new MediaRecorder(stream);\n\n    mediaRecorder.ondataavailable = (event) => {\n        audioChunks.push(event.data);\n    };\n\n    mediaRecorder.onstop = sendAudioToServer;\n\n    mediaRecorder.start();\n    startRecordingButton.disabled = true;\n    stopRecordingButton.disabled = false;\n    recordingStatus.textContent = 'Recording...';\n}\n\nfunction stopRecording() {\n    mediaRecorder.stop();\n    startRecordingButton.disabled = false;\n    stopRecordingButton.disabled = true;\n    recordingStatus.textContent = 'Recording stopped.';\n}\n\nasync function sendAudioToServer() {\n    const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });\n    const reader = new FileReader();\n    reader.readAsDataURL(audioBlob);\n    reader.onloadend = async () => {\n        const base64Audio = reader.result;\n        const response = await fetch('/transcribe', {\n            method: 'POST',\n            headers: {\n                'Content-Type': 'application/json',\n            },\n            body: JSON.stringify({ audio_data: base64Audio }),\n        });\n        const data = await response.json();\n        transcription.textContent = `Transcription: ${data.text}`;\n        generatePromptButton.disabled = false;\n    };\n    audioChunks = [];\n}\n\nasync function generatePrompt() {\n    const text = transcription.textContent.replace('Transcription: ', '');\n    const response = await fetch('/generate_prompt', {\n        method: 'POST',\n        headers: {\n            'Content-Type': 'application/json',\n        },\n        body: JSON.stringify({ text: text }),\n    });\n    const data = await response.json();\n    generatedPrompt.textContent = `Generated Prompt: ${data.prompt}`;\n    generateImageButton.disabled = false;\n}\n\nasync function generateImage() {\n    const prompt = generatedPrompt.textContent.replace('Generated Prompt: ', '');\n    const response = await fetch('/generate_image', {\n        method: 'POST',\n        headers: {\n            'Content-Type': 'application/json',\n        },\n        body: JSON.stringify({ prompt: prompt }),\n    });\n    const data = await response.json();\n    generatedImage.src = data.image_url;\n  \n    // Download the image and get the filepath\n    const downloadResponse = await fetch(`/download_image?image_url=${encodeURIComponent(data.image_url)}`);\n    const downloadData = await downloadResponse.json();\n  \n    // Store the filepath and filename for later use\n    generatedImage.dataset.filepath = downloadData.filepath;\n    generatedImage.dataset.filename = downloadData.filename;\n\n    // Set up the download link\n    downloadLink.href = `/download/${downloadData.filename}`;\n    downloadLink.download = downloadData.filename;\n    downloadLink.style.display = 'inline-block';\n}\n\nasync function generateStory() {\n    const imagePath = generatedImage.dataset.filepath;\n    const filename = generatedImage.dataset.filename;\n  \n    if (!imagePath || !filename) {\n        generatedStory.textContent = \"Error: Please generate an image first.\";\n        return;\n    }\n\n    try {\n        const response = await fetch('/generate_story_from_image', {\n            method: 'POST',\n            headers: {\n                'Content-Type': 'application/json',\n            },\n            body: JSON.stringify({ filepath: imagePath, filename: filename }),\n        });\n        if (!response.ok) {\n            throw new Error(`HTTP error! status: ${response.status}`);\n        }\n        const data = await response.json();\n      \n        // Display the generated story\n        generatedStory.textContent = data.story;\n      \n        // Make sure the story container is visible\n        document.getElementById('storyContainer').style.display = 'block';\n    } catch (error) {\n        console.error('Error:', error);\n        generatedStory.textContent = `Error: ${error.message}`;\n    }\n}\n\n// Modify the download link click event\ndownloadLink.addEventListener('click', async (event) => {\n    event.preventDefault();\n    const response = await fetch(downloadLink.href);\n    const blob = await response.blob();\n    const url = window.URL.createObjectURL(blob);\n    const a = document.createElement('a');\n    a.style.display = 'none';\n    a.href = url;\n    a.download = response.headers.get('Content-Disposition').split('filename=')[1];\n    document.body.appendChild(a);\n    a.click();\n    window.URL.revokeObjectURL(url);\n});\n```\n\n* style.css\n\n```python\nbody {\n    background-color: #1a1a2e;\n    color: #ffffff;\n}\n\n.container {\n    max-width: 1200px;\n}\n\n#imageContainer {\n    min-height: 300px;\n    display: flex;\n    align-items: center;\n    justify-content: center;\n    background-color: #16213e;\n    border-radius: 8px;\n}\n\n#generatedImage {\n    max-width: 100%;\n    max-height: 400px;\n    object-fit: contain;\n}\n```\n\n* index.html\n\n```python\n<!DOCTYPE html>\n<html lang=\"en\" data-theme=\"dark\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>AI Image Generator</title>\n    <link href=\"https://cdn.jsdelivr.net/npm/daisyui@3.7.3/dist/full.css\" rel=\"stylesheet\" type=\"text/css\" />\n    <script src=\"https://cdn.tailwindcss.com\"></script>\n    <link rel=\"stylesheet\" href=\"{{ url_for('static', path='/css/styles.css') }}\">\n</head>\n<body>\n    <div class=\"container mx-auto px-4 py-8\">\n        <h1 class=\"text-4xl font-bold mb-8 text-center\">AI Image Generator</h1>\n        <div class=\"grid grid-cols-1 md:grid-cols-2 gap-8\">\n            <div class=\"card bg-base-200 shadow-xl\">\n                <div class=\"card-body\">\n                    <h2 class=\"card-title mb-4\">Speak your prompt</h2>\n                    <button id=\"startRecording\" class=\"btn btn-primary mb-4\">Start Recording</button>\n                    <button id=\"stopRecording\" class=\"btn btn-secondary mb-4\" disabled>Stop Recording</button>\n                    <div id=\"recordingStatus\" class=\"text-lg mb-4\"></div>\n                    <div id=\"transcription\" class=\"text-lg mb-4\"></div>\n                    <button id=\"generatePrompt\" class=\"btn btn-accent mb-4\" disabled>Generate Image Prompt</button>\n                    <div id=\"generatedPrompt\" class=\"text-lg mb-4\"></div>\n                    <button id=\"generateImage\" class=\"btn btn-success\" disabled>Generate Image</button>\n                </div>\n            </div>\n            <div class=\"card bg-base-200 shadow-xl\">\n                <div class=\"card-body\">\n                    <h2 class=\"card-title mb-4\">Generated Image</h2>\n                    <div id=\"imageContainer\" class=\"mb-4\">\n                        <img id=\"generatedImage\" src=\"\" alt=\"Generated Image\" class=\"w-full h-auto\">\n                    </div>\n                    <a id=\"downloadLink\" href=\"#\" download=\"generated_image.png\" class=\"btn btn-info\" style=\"display: none;\">Download Image</a>\n                </div>\n            </div>\n        </div>\n        <!-- Add this new section after the existing cards -->\n        <div class=\"card bg-base-200 shadow-xl mt-8\">\n            <div class=\"card-body\">\n                <h2 class=\"card-title mb-4\">Generate Story from Image</h2>\n                <button id=\"generateStory\" class=\"btn btn-primary mb-4\">Generate Story</button>\n                <div id=\"storyContainer\" class=\"mb-4\">\n                    <p id=\"generatedStory\" class=\"text-lg\"></p>\n                </div>\n            </div>\n        </div>\n    </div>\n    <script src=\"{{ url_for('static', path='/js/script.js') }}\"></script>\n</body>\n</html>\n```\n\n## Conclusion\n\nThe AI Image Generator and Story Creator project successfully integrates various AI technologies to create an interactive web application that allows users to generate images and stories based on audio prompts. By leveraging FastAPI for the backend and modern frontend technologies, the application provides a seamless user experience.\n\n## Key Takeaways:\n\n1. Integration of AI Models: The project demonstrates how to integrate multiple AI models, including Groq for text generation and Replicate for image generation, to create a cohesive application that enhances user creativity.\n2. User Interaction: The application allows users to interact through voice commands, making it accessible and user\\-friendly. The ability to record audio, transcribe it, and generate content based on that input showcases the potential of voice\\-driven applications.\n3. Dynamic Content Generation: By generating images and stories dynamically based on user input, the application highlights the capabilities of AI in content creation, providing users with unique and personalized outputs.\n4. Responsive Design: The use of DaisyUI and Tailwind CSS ensures that the application is visually appealing and responsive, catering to users on various devices.\n5. Future Enhancements: The project can be further enhanced by incorporating additional features such as user authentication, saving user\\-generated content, and expanding the range of AI models used for different creative tasks.\n\nOverall, this project serves as a comprehensive example of how to build an AI\\-powered web application that combines audio processing, image generation, and storytelling, paving the way for innovative applications in the creative domain.\n\n## References\n\n* FastAPI Documentation: [FastAPI](https://fastapi.tiangolo.com/) is a modern web framework for building APIs with Python. It is designed to be easy to use and fast.\n* Pydantic: [Pydantic](https://pydantic-docs.helpmanual.io/) is used for data validation and settings management using Python type annotations.\n* Groq:[Groq](https://groq.com/docs/) is a platform for building and deploying AI models. It provides APIs for text generation and other AI tasks.\n* Replicate: [Replicate](https://replicate.com/docs) is a platform that allows you to run machine learning models in the cloud. It provides APIs for various models, including image generation.\n* SpeechRecognition: [SpeechRecognition](https://pypi.org/project/SpeechRecognition/) is a library for performing speech recognition, with support for several engines and APIs.\n* Pillow: [Pillow](https://pillow.readthedocs.io/en/stable/) is a Python Imaging Library (PIL) fork that adds image processing capabilities to your Python inter\n* JavaScript Fetch API: [The Fetch API](https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API) provides a modern way to make network requests in JavaScript.\n* HTML5 Audio API: [The HTML5 Audio AP](https://developer.mozilla.org/en-US/docs/Web/API/HTMLAudioElement)I allows you to play audio files in web applications.\n* DaisyUI: [DaisyUI](https://daisyui.com/) is a component library for Tailwind CSS that provides pre\\-designed components.\n* Tailwind CSS: [Tailwind CSS](https://tailwindcss.com/docs) is a utility\\-first CSS framework for creating custom designs without having to leave your HTML.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/ai-is-helping-me-manage-my-pre-diabetes-a115c1f7ed7b","frontmatter":{"title":"AI is Helping Me Manage My Pre-Diabetes","meta_title":"AI is Helping Me Manage My Pre-Diabetes","description":"The article discusses the authors personal experience managing pre-diabetes and weight loss using artificial intelligence (AI) tools. The author attributes their condition to genetic predisposition and outlines a strategy of dietary restriction and exercise to mimic famine conditions. By tracking blood sugar and carbohydrate intake using ChatGPT, the author observes improvements in blood sugar levels and weight loss, demonstrating the potential of AI in personal health management. The author emphasizes the importance of consulting healthcare professionals while highlighting the efficiency and insights gained from using AI for dietary management.","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*oTD6Y3PWBiteGxzYcDhP-w.jpeg","categories":["Health","Chatbots","Data Science"],"author":"Rifx.Online","tags":["pre-diabetes","ChatGPT","blood-sugar","carbohydrates","weight-loss"],"draft":false,"slug":"blog/ai-is-helping-me-manage-my-pre-diabetes-a115c1f7ed7b"},"content":"\n\n## Health \\| Blood Sugar \\| Diabetes\n\n\n\n\n\n## How Iâ€™m Using Technology to Control Blood Sugar, Lose Weight, and Stay Healthy\n\n\n\n*Disclaimer:I am not a doctor, and no part of this article should be considered medical advice. Iâ€™m sharing my own explorations into how Iâ€™m managing weight loss and avoiding diabetes.*\n\n*All your health care questions and challenges should be discussed with your personal health care professional. This article should only be considered entertainment and **not** be used for education or healthcare.*\n\n\n## The Present\n\nIf youâ€™ve been following me, youâ€™ll know Iâ€™m pre\\-diabetic and obese. In recent days, Iâ€™ve been trying to figure out how I got here and what I should do next.\n\nIâ€™ve decided to believe I have a genetic predisposition to diabetes arising out of being the descendant of generations of South Asians who suffered famines in the past. Deeper dive [here](https://readmedium.com/i-finally-understand-why-im-pre-diabetic-and-obese-c9893f4c3187).\n\nThis predisposition makes South Asians ([and other populations that survived famines](https://diabetesjournals.org/diabetes/article/61/9/2255/14753/Famine-Exposure-in-the-Young-and-the-Risk-of-Type)) more susceptible to developing diabetes. It seems our ancestors evolved to survive the chronic shortages of food paradoxically by *impairing* insulin production.\n\nImpaired insulin production allows for blood sugars to go well outside of normal ranges and while this can increase food storage as fat, it also predisposes us to diabetes .\n\nIt worked well to help my ancestors survive but when presented with periods of food abundance, the survival mechanisms go awry. We develop diabetes at much higher rates than almost every other demographic, whether obese or not.\n\nAnd, like most patients who are pre\\-diabetic or have type 2 diabetes, we would also have been exposed to an excess of sugars and likely proteins and fats as well in our diets.\n\nMy hypothesis then â€” for me to reverse prediabetes, Iâ€™d need to re\\-create artificial conditions of famine in my own life.\n\nIâ€™ll have to cut down, cut out and give up excesses in sugars/carbs, proteins and fats in my diet and include moderate exercise, enough to promote weight loss.\n\n\n## Enter Artificial Intelligence\n\nIâ€™ve been tracking blood sugars diligently every morning after fasting between 16â€“18 hours. For the past week, Iâ€™ve observed a downward trend moving from 123 mg/dL to below 100 mg/dL. That has been my objective.\n\nAll Iâ€™ve done is input the dates and readings into ChatGPT and asked it to generate a table. Hereâ€™s what that looks like:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*UenRzbRpeCFUNFbVwtrxkg.png)\n\nI kept close track of my daily input of carbs using ChatGPT. Again, I input exactly what I ate and asked it to calculate the number of carbs I ingested and to give me the result in a table.\n\nNote, all I kept track of was **what** I ate and how much. The Ai did the rest.\n\nHereâ€™s what that looked like yesterday when, for the *first* time, I got a fasting blood sugar below my target of 100 mg/dL:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*4YwDOMk2V7Leo5Wl4_5NGg.png)\n\nI wanted to know how many calories I ate, so I simply asked it to calculate the number of calories I ate based on the data Iâ€™d already given it. It came up with the table below:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*AFAm5-SStY04-mQiCShRPg.png)\n\nI have to say it â€” this was wonderful. I no longer needed to track every detail by hand and I could enter data just once to manipulate it in different ways.\n\nWhat I plan to do going forward is to input my meals and blood sugars every day in ChatGPT and generate a daily report to get a sense of how my diet affects my blood sugars, with specific focus on carb intake and subsequent fasting blood sugars.\n\nBased on the data, I can figure out what sends it over my goal reading of 100 mg/dL., and what keeps it within range. Itâ€™s targeted, personalized help for a condition I need to manage. For me, it works beautifully.\n\n\n## Tables arenâ€™t everything\n\nThen I asked my Ai companion this question: D*o you think that my meals today were appropriate for a prediabetic trying to lose weight and increase my insulin sensitivity?*\n\nThe answer it returned was based on the food data I input. That blew my mind. It analyzed my specific food choices and returned multiple positive aspects e.g.\n\n\n> **Low\\-Carb Choices**:You kept your carb intake relatively low, which is important for blood sugar control in pre\\-diabetes. Your total carb intake was around **28\\.3â€“29\\.3 grams**, which is well within a moderate low\\-carb range.\n\nIt then suggested several areas for improvement including:\n\n\n> **More Focus on Balanced Meals**:While your meals were healthy overall, you might want to include more variety and balance. For example, incorporating moderate portions of low\\-GI fruits (like berries, which you did), whole grains (if tolerated), or legumes could improve insulin sensitivity over time.\n\nAnd finally it gave me suggestions on what I could do going forward:\n\n\n> Replace whole cream milk with lower\\-carb alternatives.\n\n\n> Add more non\\-starchy veggies to increase fiber and volume.\n\n\n> Continue focusing on healthy fats and lean proteins but keep an eye on overall calorie intake to align with your weight loss goals.\n\n\n> Your meal choices were largely appropriate for managing blood sugar and promoting insulin sensitivity, but slight adjustments could further support your weight loss efforts.\n\n\n## Donâ€™t trust the Machine\n\nOkay, I know itâ€™s a machine. I know I cannot rely on itâ€™s advice and **neither should you gentle reader**. But damn, it gave me enough to work with for when I speak to my doctor next.\n\nI will cut out the whole cream milk though. Thatâ€™s sensible advice.\n\nI plan to generate a table of blood sugars vs. dates and take it to my doctor, and if she asks about my diet, I can share the entire GPT chat with her via a link. That way she can see exactly what Iâ€™ve been doing â€” whether right or wrong and then make adjustments as needed.\n\nNow isnâ€™t that too cool for school? Smiling.\n\n\n## Conclusions:\n\nIâ€™m enthusiastic about using Ai sensibly. This chat with the Ai opened my eyes to the possibility of using artificial intelligence to manage large amounts of critical data in my personal life. Itâ€™s because Iâ€™m using Ai to monitor my diet and blood sugars that I can take better control of my condition.\n\nCould I do it with a pencil and a notepad? Yes.\n\nBut it would take more time, more effort and would generate much more friction. At the same time, I wouldnâ€™t be able to share the details of my diet and pre\\-diabetes management with you as easily as I have.\n\nOne more thing â€” I didnâ€™t mention my weight loss. When I started taking weight readings two weeks ago, I weighed 225 pounds. Today I weighed 219\\. **Six pounds gone.**\n\nAll I can say, this is working for me. Consider figuring out how you can incorporate Artificial intelligence into your life. It can be scary for some. Iâ€™m already retired, yet I figured it out. You can too. Itâ€™s easy. Itâ€™s mostly free and so useful.\n\nDo you think you can use artificial intelligence to help manage your chronic health conditions? Iâ€™d love to know. If youâ€™re willing to try it, what challenges do you face? Maybe we can get over them together.\n\nIn the meantime,Walk Good.Mitch.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/ai-powered-ocr-with-phi-3-vision-128k-the-future-of-document-processing-7be80c46bd16","frontmatter":{"title":"AI-Powered OCR with Phi-3-Vision-128K: The Future of Document Processing","meta_title":"AI-Powered OCR with Phi-3-Vision-128K: The Future of Document Processing","description":"In the fast-evolving world of artificial intelligence, multimodal models are setting new standards for integrating visual and textual dataâ€¦","date":"2024-11-08T00:26:30.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BR-H6cQoyoRo6gVRqjvAyA.png","categories":["Natural Language Processing","Computer Vision","Data Science"],"author":"Rifx.Online","tags":["OCR","tokens","encoder","language","document"],"draft":false,"slug":"blog/ai-powered-ocr-with-phi-3-vision-128k-the-future-of-document-processing-7be80c46bd16"},"content":"\n\n\n\n\n\nIn the fast\\-evolving world of artificial intelligence, multimodal models are setting new standards for integrating visual and textual data. One of the latest breakthroughs is the **Phi\\-3\\-Vision\\-128K\\-Instruct**, a state\\-of\\-the\\-art open multimodal model that pushes the boundaries of AI capabilities in processing images and text. Designed with a focus on document extraction, Optical Character Recognition (OCR), and general image understanding, this model can revolutionize how we handle information from PDFs, charts, tables, and other structured or semi\\-structured documents.\n\nLetâ€™s dive deep into the nuts and bolts of the Phi\\-3\\-Vision\\-128K\\-Instruct, explore its architecture, technical requirements, responsible use considerations, and understand how it can be used to simplify complex tasks like document extraction, pdf parsing, and AI\\-powered data analysis.\n\n\n## What is Phi\\-3\\-Vision\\-128K\\-Instruct?\n\nPhi\\-3\\-Vision\\-128K\\-Instruct belongs to the Phi\\-3 model family and is built for multimodal data processing, supporting a context length of up to **128,000 tokens**. The model incorporates both textual and visual data, making it well\\-suited for tasks that require the simultaneous interpretation of text and images. Its development involved **500 billion training tokens**, a combination of high\\-quality synthetic data and rigorously filtered publicly available sources. Through a refined training process that included **supervised fine\\-tuning and preference optimization**, the model has been crafted to deliver precise, reliable, and safe AI solutions.\n\nWith **4\\.2 billion parameters**, Phi\\-3\\-Vision\\-128K\\-Instructâ€™s architecture comprises an image encoder, connector, projector, and the Phi\\-3 Mini language model, making it a lightweight yet powerful choice for a wide range of applications.\n\n\n## Core Use Cases\n\nThe modelâ€™s primary applications span several domains, with a particular focus on:\n\n* **Document extraction and OCR:** Efficiently converting images of text or scanned documents into editable formats. It can handle complex layouts like tables, charts, and diagrams, making it a valuable tool for digitizing physical documents or automating data extraction workflows.\n* **General image understanding:** Parsing visual content to recognize objects, interpret scenes, and extract relevant information.\n* **Memory/compute\\-constrained environments:** Running AI tasks where computing power or memory is limited without compromising performance.\n* **Latency\\-bound scenarios:** Reducing processing delays in real\\-time applications such as live data feeds, chat\\-based assistants, or streaming content analysis.\n\n\n## How to Get Started with Phi\\-3\\-Vision\\-128K\\-Instruct\n\nTo use Phi\\-3\\-Vision\\-128K\\-Instruct, you will need to set up your development environment with the required libraries and tools. The model is integrated into the development version (4\\.40\\.2\\) of the Hugging Face `transformers` library. Before diving into code examples, ensure that your Python environment is configured with these packages:\n\n\n```python\n## Required Packages\nflash_attn==2.5.8\nnumpy==1.24.4\nPillow==10.3.0\nRequests==2.31.0\ntorch==2.3.0\ntorchvision==0.18.0\ntransformers==4.40.2\n```\nTo load the model, you can either update your local `transformers` library or clone and install it directly from the source:\n\n\n```python\npip uninstall -y transformers && pip install git+https://github.com/huggingface/transformers\n```\nNow, letâ€™s jump into some practical code snippets to show how you can leverage this powerful model for AI\\-driven document extraction and text generation.\n\n\n## Sample Code for Loading the Model\n\nHereâ€™s a Python example of how to initialize the model and start making inferences. Weâ€™ll make use of classes and functions to keep the code clean and organized:\n\n\n```python\nfrom PIL import Image\nimport requests\nfrom transformers import AutoModelForCausalLM, AutoProcessor\n\nclass Phi3VisionModel:\n    def __init__(self, model_id=\"microsoft/Phi-3-vision-128k-instruct\", device=\"cuda\"):\n        \"\"\"\n        Initialize the Phi3VisionModel with the specified model ID and device.\n        \n        Args:\n            model_id (str): The identifier of the pre-trained model from Hugging Face's model hub.\n            device (str): The device to load the model on (\"cuda\" for GPU or \"cpu\").\n        \"\"\"\n        self.model_id = model_id\n        self.device = device\n        self.model = self.load_model()  # Load the model during initialization\n        self.processor = self.load_processor()  # Load the processor during initialization\n    \n    def load_model(self):\n        \"\"\"\n        Load the pre-trained language model with causal language modeling capabilities.\n        \n        Returns:\n            model (AutoModelForCausalLM): The loaded model.\n        \"\"\"\n        print(\"Loading model...\")\n        # Load the model with automatic device mapping and data type adjustment\n        return AutoModelForCausalLM.from_pretrained(\n            self.model_id, \n            device_map=\"auto\",  # Automatically map model to the appropriate device(s)\n            torch_dtype=\"auto\",  # Use an appropriate torch data type based on the device\n            trust_remote_code=True,  # Allow execution of custom code for loading the model\n            _attn_implementation='flash_attention_2'  # Use optimized attention implementation\n        ).to(self.device)  # Move the model to the specified device\n    \n    def load_processor(self):\n        \"\"\"\n        Load the processor associated with the model for processing inputs and outputs.\n        \n        Returns:\n            processor (AutoProcessor): The loaded processor for handling text and images.\n        \"\"\"\n        print(\"Loading processor...\")\n        # Load the processor with trust_remote_code=True to handle any custom processing logic\n        return AutoProcessor.from_pretrained(self.model_id, trust_remote_code=True)\n    \n    def predict(self, image_url, prompt):\n        \"\"\"\n        Perform a prediction using the model given an image and a prompt.\n        \n        Args:\n            image_url (str): The URL of the image to be processed.\n            prompt (str): The textual prompt that guides the model's generation.\n        \n        Returns:\n            response (str): The generated response from the model.\n        \"\"\"\n        # Load the image from the provided URL\n        image = Image.open(requests.get(image_url, stream=True).raw)\n        \n        # Format the input prompt template for the model\n        prompt_template = f\"<|user|>\\n<|image_1|>\\n{prompt}<|end|>\\n<|assistant|>\\n\"\n        \n        # Process the inputs, converting the prompt and image into tensor format\n        inputs = self.processor(prompt_template, [image], return_tensors=\"pt\").to(self.device)\n        \n        # Set generation arguments for the model's response generation\n        generation_args = {\n            \"max_new_tokens\": 500,  # Maximum number of tokens to generate\n            \"temperature\": 0.7,     # Sampling temperature for diversity in generation\n            \"do_sample\": False      # Disable sampling for deterministic output\n        }\n        print(\"Generating response...\")\n        # Generate the output IDs using the model, skipping the input tokens\n        output_ids = self.model.generate(**inputs, **generation_args)\n        output_ids = output_ids[:, inputs['input_ids'].shape[1]:]  # Ignore the input prompt in the output\n        \n        # Decode the generated output tokens to obtain the response text\n        response = self.processor.batch_decode(output_ids, skip_special_tokens=True)[0]\n        return response\n\n## Initialize the model\nphi_model = Phi3VisionModel()\n\n## Example prediction\nimage_url = \"https://example.com/sample_image.png\"  # URL of the sample image\nprompt = \"Extract the data in json format.\"  # Prompt for model guidance\nresponse = phi_model.predict(image_url, prompt)  # Get the response from the model\n\nprint(\"Response:\", response)  # Print the generated response\n```\nThe code above defines a `Phi3VisionModel` class that abstracts the loading and usage of the model, making it easier to integrate into your applications. The `predict()` method demonstrates how to perform image\\-based inferences using a custom prompt.\n\nTo update the article with a focus on testing the OCR capabilities of the Phi\\-3\\-Vision\\-128K\\-Instruct model, weâ€™ll add a section detailing how the model performs with real\\-world examples of scanned ID cards.\n\n\n## Testing OCR Capabilities with Scanned ID Cards\n\nTo evaluate the OCR performance of the Phi\\-3\\-Vision\\-128K\\-Instruct model, we tested it using several real\\-world scanned ID card images. These images vary in quality and clarity, providing a range of challenges for the model. The goal is to demonstrate how well the model can extract text information from documents with different characteristics, such as blurriness, complex backgrounds, and varying text fonts.\n\n**Image 1:** A fictional Utopian passport with detailed text, including personal information such as name, nationality, place of birth, date of issue, and expiration date. The text is slightly stylized, and there is a machine\\-readable zone at the bottom. The image quality is high, with no significant background noise.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MltpseOI3HhvCkUZMwLdEQ.png)\n\n**Output:**\n\n\n```python\n{\n  \"Type/Type\": \"P\",\n  \"Country code/Code du pays\": \"UTO\",\n  \"Passport Number/NÂ° de passeport\": \"L898902C3\",\n  \"Surname/Nom\": \"ERIKSSON\",\n  \"Given names/PrÃ©noms\": \"ANNA MARIA\",\n  \"Nationality/NationalitÃ©\": \"UTOPIAN\",\n  \"Date of Birth/Date de naissance\": \"12 AUGUST/AOUT 74\",\n  \"Personal No./NÂ° personnel\": \"Z E 184226 B\",\n  \"Sex/Sexe\": \"F\",\n  \"Place of birth/Lieu de naissance\": \"ZENITH\",\n  \"Date of issue/Date de dÃ©livrance\": \"16 APR/AVR 07\",\n  \"Authority/AutoritÃ©\": \"PASSPORT OFFICE\",\n  \"Date of expiry/Date d'expiration\": \"15 APR/AVR 12\",\n  \"Holder's signature/Signature du titulaire\": \"anna maria eriksson\",\n  \"Passport/Passeport\": \"P<UTOERIKSSON<<ANNA<MARIA<<<<<<<<<<<<<<<<<<<<<<<L898902C36UT07408122F1204159ZE184226B<<<<10\"\n}\n```\n**Image 2:** A Dutch passport with a clear image of the holder and neatly formatted text. Fields include the passport number, name, date of birth, nationality, and expiration date. The document is presented with high contrast, making text extraction relatively straightforward. The machine\\-readable zone (MRZ) at the bottom offers a structured data format that can help validate the accuracy of extracted information.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*WGV4tTxI9xISmAvFs8ovNw.png)\n\n**Output:**\n\n\n```python\nHere's the extracted full data from the passport in JSON format:\n\n{\n  \"passport\": {\n    \"issuingCountry\": \"Netherlands\",\n    \"issuingAuthority\": \"Koninkrijk der Nederlanden\",\n    \"passportNumber\": \"SPEC12014\",\n    \"issuingDate\": \"09 MAR 2014\",\n    \"expiryDate\": \"09 MAR 2024\",\n    \"holder\": {\n      \"gender\": \"F\",\n      \"nationality\": \"Netherlands\",\n      \"placeOfBirth\": \"SPECIMEN\",\n      \"sex\": \"WF\",\n      \"firstNames\": [\n        \"Willem\",\n        \"Lieselotte\"\n      ]\n    },\n    \"physicalDescription\": {\n      \"height\": \"1.75 m\",\n      \"hairColor\": \"gray\",\n      \"hairLength\": \"short\"\n    },\n    \"issuingOffice\": \"Burg. van Stad en Dorp\",\n    \"issuingDateAsInt\": \"14032014\",\n    \"expiryDateAsInt\": \"14032024\",\n    \"fieldsExtracted\": [\n      {\n        \"code\": \"NL\",\n        \"dateOfBirth\": \"10 MAR 1965\",\n        \"dateOfIssue\": \"09 MAR 2014\",\n        \"dateOfExpiry\": \"09 MAR 2024\",\n        \"firstNames\": [\n          \"Willem\",\n          \"Lieselotte\"\n        ],\n        \"nationality\": \"Netherlands\",\n        \"passportNumber\": \"SPEC12014\",\n        \"placeOfBirth\": \"SPECIMEN\",\n        \"sex\": \"WF\"\n      }\n    ]\n  }\n}\n```\n\n## Try Phi\\-3\\-Vision\\-128K\\-Instruct Yourself\n\nIf you want to try the Phi\\-3\\-Vision\\-128K\\-Instruct model for yourself, you can explore it through the following link: [Try Phi\\-3\\-Vision\\-128K\\-Instruct on Azure AI](https://ai.azure.com/explore/models/Phi-3-vision-128k-instruct/version/1/registry/azureml). This link allows you to experience the modelâ€™s capabilities and experiment with its OCR functionality.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*7feNu3ZuclgAnAzbJMMSFg.png)\n\n\n## Understanding the Architecture and Training\n\nThe **Phi\\-3\\-Vision\\-128K\\-Instruct** model is not just any language model â€” itâ€™s a multimodal powerhouse that can process both visual and textual data. It has undergone a comprehensive training regime that included **500 billion tokens**, a blend of text and image data. Its architecture integrates a language model and image processing modules, creating a cohesive system that understands context over **128K tokens**, allowing for extended conversations or documents with large content.\n\nTrained on powerful hardware, such as **512 H100 GPUs**, and utilizing **flash attention** for memory efficiency, this model can handle large\\-scale tasks with ease. The training dataset includes a mix of synthetic and filtered real\\-world data, emphasizing **math, coding, common sense reasoning**, and **general knowledge**, making it versatile enough for various applications.\n\n\n## Key Benchmarks and Performance\n\nThe performance of Phi\\-3\\-Vision\\-128K\\-Instruct has been tested across multiple benchmarks, including **ScienceQA**, **AI2D**, **MathVista**, and **TextVQA**. Its scores consistently surpass many existing models in tasks that combine text and vision, particularly in areas such as:\n\n* **Document comprehension**: Extracting useful information from complex documents like PDFs or images.\n* **Table and chart understanding**: Accurately interpreting graphical data and converting it into textual explanations.\n\nIn particular, the model achieved an impressive **81\\.4%** on **ChartQA** and **76\\.7%** on **AI2D**, showcasing its capability to understand data\\-rich documents effectively.\n\n\n## Why OCR and Document Extraction Matter\n\nDocument extraction and OCR are vital for businesses and research, enabling the conversion of printed or handwritten text into machine\\-readable formats. Tasks such as **PDF parsing**, **data entry automation**, **invoice processing**, and **legal document analysis** are significantly simplified by using AI models like Phi\\-3\\-Vision\\-128K\\-Instruct.\n\nWhether you are dealing with scanned documents, screenshots, or photographed pages, the modelâ€™s multimodal capabilities can help to **automate data extraction**, making it a valuable tool for improving productivity and reducing manual effort.\n\n\n## Responsible AI and Safety Measures\n\nWhile the model is powerful, it comes with limitations that developers should keep in mind. **Language biases**, **stereotype reinforcement**, and **inaccurate content generation** are potential issues. For high\\-risk use cases, such as **health or legal advice**, additional layers of **verification and content filtering** are necessary.\n\n\n## Future Directions and Fine\\-Tuning\n\nLooking to extend Phi\\-3\\-Vision\\-128K\\-Instructâ€™s capabilities? Fine\\-tuning is supported and can be performed using the **Phi\\-3 Cookbook**, which provides recipes for adjusting the model to specific tasks like **document classification**, **enhanced OCR accuracy**, and **specialized image understanding**.\n\n\n## Conclusion\n\nThe Phi\\-3\\-Vision\\-128K\\-Instruct isnâ€™t just a step forward for multimodal AI; itâ€™s a leap into a future where **document extraction, OCR, and AI\\-driven content generation** are seamless and accessible. With extensive training, robust architecture, and thoughtful design, this model empowers developers to transform data processing across various fields.\n\nStay tuned for more advanced examples and tutorials on integrating this model with real\\-world applications, where we will explore **processing multiple document types** and applying **AI\\-powered techniques** to extract valuable insights from diverse sources.\n\nThe future of **AI\\-powered document extraction** has never looked more promising!\n\n\n"},{"lang":"en","group":"blog","slug":"blog/ai-research-agents-set-to-transform-knowledge-research-in-2025-plus-top-3-free-tools-d37197726531","frontmatter":{"title":"AI Research Agents: Set to Transform Knowledge Research in 2025 (Plus Top 3 Free Tools)","meta_title":"AI Research Agents: Set to Transform Knowledge Research in 2025 (Plus Top 3 Free Tools)","description":"AI research agents are poised to revolutionize knowledge research by 2025, enhancing the efficiency and depth of data analysis across various fields. Unlike traditional AI tools, these agents can autonomously conduct extensive research, identify patterns, and generate insights with advanced technologies like Retrieval Augmented Generation (RAG). Key tools include Stanfords STORM, CustomGPT.ai Researcher, and GPT Researcher, each offering unique capabilities for automated, accurate content creation. As data generation accelerates, the integration of these agents into research workflows will enable researchers to focus on creative and complex problem-solving, ultimately democratizing access to high-quality research.","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*8hV5Jo0eUwY01CWV","categories":["Research","Data Science","Generative AI"],"author":"Rifx.Online","tags":["agents","RAG","STORM","CustomGPT","GPT"],"draft":false,"slug":"blog/ai-research-agents-set-to-transform-knowledge-research-in-2025-plus-top-3-free-tools-d37197726531"},"content":"\n\n\n\nHereâ€™s the deal: Something massive is about to shake up the world of knowledge research.\n\nAfter spending months diving deep into AI research agents and seeing them in action across various industries, I can tell you one thing for sure â€” by 2025, these arenâ€™t just going to be helpful tools. Theyâ€™re going to fundamentally transform how we do knowledge research (whether for marketing or science!).\n\n\n> **It is physically impossible for a human to access 10,000 websites in an hour and research the data. However, an agent can do this with ease.**\n\nAnd in this article, I am going to show you 3 free tools that will blow your mind. (Hint: Itâ€™s NOT ChatGPT or Perplexity!)\n\n\n\nI know what youâ€™re thinking. â€œAnother AI hype piece?â€ But stick with me here.\n\nThe market is projected to explode from $5\\.1 billion in 2024 to $47\\.1 billion by 2030\\. Thatâ€™s not just growth â€” thatâ€™s a complete transformation of the research landscape.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*uGrcW8msqlIdpBeL)\n\n\n## What Makes AI Research Agents Different?\n\nFirst off, these arenâ€™t your typical AI tools that need constant hand\\-holding. While traditional systems require explicit instructions for each task, AI research agents are like having a brilliant research assistant who can think on their feet, adapting their behavior based on outcomes they achieve.\n\n\n\n\n\n\n\nThe real game\\-changer? These agents can handle massive amounts of knowledge, spot patterns humans might miss, and generate insights faster than ever before. Using advanced [Retrieval Augmented Generation (RAG)](https://readmedium.com/build-it-or-buy-it-deployment-options-for-retrieval-augmented-generation-rag-f6d43df8212a) technology, they can pull information directly from trusted sources while maintaining accuracy.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*p2E-fJ63BB27lZdD)\n\n\n## The Tech Behind the Magic\n\nThe secret sauce here is the ability to ingest and research large amounts of knowledge (e.g. deep Google research) and then combine it with the power of LLMs like gpt\\-4o and o1\\.\n\nBut hereâ€™s what really gets me excited: These agents are powered by RAG models, with built\\-in anti\\-hallucination algorithms that ensure accuracy. Unlike generic AI tools, research agents stick to verified information and can cite their sources â€” crucial for maintaining integrity.\n\n\n> **So just imagine a research agent going off for 30 mins and doing a PhD on a topic â€” something that would have taken most humans days to achieve.**\n\n\n## Why This Matters Now\n\nThe timing couldnâ€™t be better. Research is drowning in data â€” weâ€™re generating more information in a day than we used to in a year. And with Googleâ€™s emphasis on Experience, Expertise, Authoritativeness, and Trustworthiness (EEAT), the need for accurate, well\\-researched content has never been greater.\n\nI recently talked with a research team that cut their article research time by 70% using an AI research agent. But it wasnâ€™t just about speed â€” the agent found perspectives in the knowledge that theyâ€™d completely missed in their initial brief. And the best part? Everything was verifiable and backed by data.\n\nSo just imagine that you can have Einstein, Elon Musk, Feynman, Steve Jobs, Jane Goodall and Yuval Noah Harari all collaborating on doing your research report â€” thatâ€™s what is possible with AI Research Agents.\n\n\n## The Top 3 AI Research Agents\n\n\n## Stanford STORM\n\nStanford Universityâ€™s [STORM](https://storm.genie.stanford.edu/) (Synthesis of Topic Outlines through Retrieval and Multi\\-perspective Question Asking) is an AI\\-powered knowledge curation system designed to generate comprehensive, Wikipedia\\-like articles from scratch.\n\nLeveraging large language models (LLMs), STORM automates the research and writing process by conducting internet\\-based research, organizing information into structured outlines, and producing full\\-length articles complete with citations.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*axYZ2fO15FAqQpID)\n\n**Pros:**\n\n* **Automated Research and Writing:** STORM streamlines the creation of detailed articles by automating both the research and writing stages, saving users significant time and effort.\n* **Structured Content Generation:** The system generates organized outlines and well\\-structured articles, ensuring clarity and coherence in the final output.\n* **Open\\-Source Accessibility:** As an open\\-source project, STORM allows users to customize and adapt the tool to their specific needs, fostering innovation and collaboration within the AI research community.\n\n**Cons:**\n\n* **Dependence on Internet Sources:** STORMâ€™s reliance on internet\\-based research may lead to the inclusion of outdated or biased information if not carefully monitored.\n* **Quality Control Requirements:** While STORM automates much of the writing process, the generated articles may still require human review and editing to ensure accuracy and adherence to specific standards.\n* **Technical Setup:** Implementing STORM locally necessitates familiarity with tools like Git, Python, and Conda, which may present a barrier for users without a technical background.\n\nFor more information and access to STORM, visit the [official GitHub repository](https://github.com/stanford-oval/storm)\n\n\n\n\n\n\n\n\n## CustomGPT.ai Researcher\n\nCustomGPT.ai [Researcher](https://customgpt-researcher.streamlit.app/) is an AI research agent specifically designed to create ultra\\-high\\-quality long\\-form articles based on deep Google research or custom knowledge bases, such as a companyâ€™s proprietary data or other trusted sources.\n\nUsing CustomGPTâ€™s anti\\-hallucination technology, it generates factually accurate content, with inline citations, that aligns with specific brand guidelines and ensures consistency with real\\-world information.\n\nThis agent uses a combination of o1, gpt\\-4o and GPT\\-4o (Vision) to craft a detailed research report that includes inline images and links. Itâ€™s unique â€œ**progressive narrative**â€ feature helps create a non\\-robotic narrative that is aware of previously generated content.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*UiCyTjNF3A3buKzT)\n\n**Pros:**\n\n* **Trustworthy Content Creation:** By integrating data from reliable sources, CustomGPT.ai minimizes inaccuracies, making it ideal for industries requiring high content reliability, such as legal, financial, and healthcare sectors.\n* **Anti\\-Hallucination Technology:** CustomGPT.ai includes advanced algorithms that prevent it from producing speculative or fictitious information, ensuring content aligns closely with verified sources.\n* **Hosted Solution:** With its no\\-code interface, non\\-technical researchers and marketers can easily trigger deep research, without getting into coding intricacies.\n* **SEO\\-Optimized Content Generation:** The tool supports Googleâ€™s EEAT (Expertise, Authoritativeness, Trustworthiness, and Experience) standards, creating content that ranks well on search engines by emphasizing quality and authority.\n\n**Cons:**\n\n* **Closed Source:** While the CustomGPT.ai Researcher is free for a limited time, it is a closed\\-source proprietary project.\n* **Longer Generation Time:** The high\\-level reasoning and RAG capabilities can take up to 20 minutes to generate a single article, which may not suit users seeking rapid or lower\\-quality content.\n* **Limited Suitability for Budget Content Projects:** Given its focus on quality, CustomGPT.ai Researcher is not ideal for projects that require fast, inexpensive, or basic content generation.\n\nFor more details on CustomGPT.ai Researcher and its applications, see the free [Streamlit App](https://customgpt-researcher.streamlit.app/).\n\n\n## GPT Researcher\n\nGPT Researcher is an [autonomous agent](https://github.com/assafelovic/gpt-researcher) designed to conduct comprehensive research on any given task, utilizing both web and local sources.\n\nIt generates detailed, factual, and unbiased reports complete with citations, offering a full suite of customization options to create tailored, domain\\-specific research agents.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*_4Q680CBSvpCZOLW)\n\n**Pros:**\n\n* **Autonomous Research Capabilities:** GPT Researcher automates the research process, efficiently gathering and synthesizing information from various sources to produce comprehensive reports.\n* **Customization and Flexibility:** Users can customize the agent to focus on specific domains or topics, allowing for tailored research outputs that meet particular needs.\n* **Open\\-Source Accessibility:** As an open\\-source project, GPT Researcher encourages community collaboration and continuous improvement, providing transparency and adaptability for users.\n\n**Cons:**\n\n* **Technical Setup Requirements:** Implementing GPT Researcher may require technical expertise, including familiarity with Git, Python, and Docker, which could be a barrier for non\\-technical users.\n\nFor more information and access to GPT Researcher, visit the [official GitHub repository](https://github.com/assafelovic/gpt-researcher):\n\n\n## The Human Side of AI Research\n\nLetâ€™s address the elephant in the room: â€œAre these agents going to replace human researchers?â€ Absolutely not. Instead, theyâ€™re freeing up researchers to focus on what humans do best: creative thinking, complex problem\\-solving, and generating innovative hypotheses.\n\nThink of it like having a super\\-powered research assistant who never sleeps, never gets tired, and can process information at lightning speed. While the AI handles deep knowledge research, researchers can focus on breakthrough insights.\n\n\n## Getting Ready for the Revolution\n\nSo, how do we prepare for this AI revolution in research?\n\nFirst, researchers need to level up their skills. Iâ€™m not saying everyone needs to become a coding expert, but understanding the **strengths AND limitations** of these AI research agents is going to be key.\n\nUniversities are adapting their curricula, and Iâ€™m seeing more [researchers](https://drmichaellevin.org/resources/levinbot.html) utilizing AI to aid in their research labs.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*E733R9Fn9ufaXnTF)\n\n\n## Looking Beyond 2025\n\nThe potential here is mind\\-blowing. These AI research agents are going to enable types of research we can barely imagine right now.\n\nCross\\-disciplinary innovations will become the norm as AI agents help connect dots between different fields that we never even knew were related.\n\nIâ€™m particularly excited about how this technology could democratize research. Small labs and institutions that couldnâ€™t afford large research teams will be able to leverage AI agents to compete with bigger players. That means more diverse perspectives and more breakthrough discoveries.\n\nAs an example, the [Levin Lab at Tuftâ€™s University](https://drmichaellevin.org/resources/levinbot.html) was able to build one of the best AI tools within a few hours â€” showing the true power of AI democratization.\n\n\n## Final Thoughts\n\nAfter spending months researching this topic and talking with experts in the field, Iâ€™m convinced that AI research agents are going to be as transformative as the internet was for science. Theyâ€™re not just tools â€” theyâ€™re partners in the research process that will help us tackle some of the biggest challenges facing humanity.\n\nSure, there are hurdles to overcome and skills to develop. But the potential benefits are too massive to ignore. If youâ€™re in marketing or research, nowâ€™s the time to start preparing for this shift.\n\nRemember: The teams and institutions that embrace this technology early will have a massive advantage in the years to come. Donâ€™t get left behind in whatâ€™s shaping up to be one of the biggest revolutions in how we do knowledge research â€” whether you are writing a blog post for SEO â€” or doing your PhD on a scientific topic.\n\n*What are your thoughts on AI research agents? Have you started integrating them into your research workflow? Iâ€™d love to hear about your experiences in the comments below.*\n\n\n"},{"lang":"en","group":"blog","slug":"blog/alibabas-open-source-qwen-how-it-s-revolutionizing-ai-and-how-you-can-use-it-dcba8f687c97","frontmatter":{"title":"Alibabaâ€™s Open-Source Qwen: How Itâ€™s Revolutionizing AI and How You Can Use It","meta_title":"Alibabaâ€™s Open-Source Qwen: How Itâ€™s Revolutionizing AI and How You Can Use It","description":"Alibaba has recently made waves in the AI world by open-sourcing its Qwen 2.5 models during the 2024 Apsara Conference. With over 100â€¦","date":"2024-10-26T00:26:25.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*I7QDwbLMzoJ_ORq5.jpg","categories":["Programming","Machine Learning","Natural Language Processing"],"author":"Rifx.Online","tags":["Qwen","multimodal","open-source","fine-tune","text-to-video"],"draft":false,"slug":"blog/alibabas-open-source-qwen-how-it-s-revolutionizing-ai-and-how-you-can-use-it-dcba8f687c97"},"content":"\nAlibaba has recently made waves in the AI world by open-sourcing its **Qwen 2.5** models during the 2024 Apsara Conference. With over 100 models, Qwen spans multiple modalities including language, vision, audio, and code, making it one of the most comprehensive open-source AI solutions. The release empowers developers by providing tools for diverse applications, from text-to-video generation to real-time question answering.\n\n\n\n## Key Features of Alibabaâ€™s Qwen Models\n\n1. **Multimodal Capabilities**: Qwen models handle diverse inputs, including text, audio, and visual data. This multimodal approach makes them suitable for a wide range of industries, from media and entertainment to robotics.\n2. **Open Source**: Available on platforms like **Hugging Face** and **ModelScope**, Qwen has already been downloaded over 40 million times, with over 50,000 custom models built on its foundation.\n3. **Enhanced Performance**: Qwen2.5 introduces improved language understanding, mathematics, and coding capabilities, rivaling leading models in the field. With optimized performance for tasks like structured data understanding and long text generation, Qwen opens the door to high-level AI applications.\n\n## How to Use Alibabaâ€™s Qwen\n\nDevelopers and organizations can access Qwen models on platforms like Hugging Face, where they can:\n\n* **Fine-tune models**: Tailor Qwen for specific industry applications such as customer service, automation, or video content creation.\n* **Integrate with applications**: Qwenâ€™s text-to-video model can be incorporated into media production pipelines, generating dynamic content from static images and text prompts.\n* **Develop AI assistants**: With enhanced vision-language models, Qwen can be used in robotics and autonomous vehicles to process video data and perform real-time tasks like navigation or object recognition.\n\n**Example of Using Qwen via Hugging Face**:\n\n```python\nfrom transformers import QwenTokenizer, QwenModel\n\ntokenizer = QwenTokenizer.from_pretrained(\"qwen-2.5\")\nmodel = QwenModel.from_pretrained(\"qwen-2.5\")\n\ninput_text = \"What is the future of AI in healthcare?\"\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\noutputs = model(input_ids)\n```\n\nThis allows users to access Qwen models, run inference, and customize them based on specific needs.\n\n## Qwenâ€™s Impact Across Industries\n\n1. **Media and Entertainment**: With the new text-to-video capabilities, Qwen can automatically generate videos from written scripts, transforming the creative industry by automating tedious production tasks.\n2. **Robotics and Autonomous Vehicles**: The enhanced vision-language models in Qwen can help robots understand real-world environments, leading to better decision-making in autonomous driving or manufacturing.\n3. **Software Development**: Alibabaâ€™s AI Developer tool, powered by Qwen, automates tasks like code generation, debugging, and requirement analysis, enabling developers to focus on higher-level problem-solving.\n\n## Conclusion: A New Era of Open AI Innovation\n\nBy open-sourcing its Qwen 2.5 models, Alibaba is democratizing access to advanced AI technologies. Developers, startups, and large enterprises alike can harness Qwenâ€™s multimodal and real-time capabilities to drive innovation in industries ranging from media to autonomous vehicles. Whether youâ€™re a developer looking to fine-tune models for a niche application or a corporation integrating AI into your infrastructure, Qwen offers powerful tools to accelerate progress.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/artifacts-top-mindblowing-uses-of-claude-3-5-sonent-6830b2acfa4b","frontmatter":{"title":"Artifacts: Top Mindblowing uses of Claude 3.5 Sonent","meta_title":"Artifacts: Top Mindblowing uses of Claude 3.5 Sonent","description":"Anthropic recently launched its most advanced LLM, â€œClaude 3.5 Sonnet,â€ and itâ€™s mindblowing. People on social media called this model theâ€¦","date":"2024-11-08T00:18:38.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*XL1dN9VCFcbz3m5N","categories":["Programming","Natural Language Processing","Generative AI"],"author":"Rifx.Online","tags":["Sonnet","context","Artifacts","code","generation"],"draft":false,"slug":"blog/artifacts-top-mindblowing-uses-of-claude-3-5-sonent-6830b2acfa4b"},"content":"\n\n\n\nAnthropic recently launched its most advanced LLM, â€œClaude 3\\.5 Sonnet,â€ and itâ€™s mindblowing. People on social media called this model the most advanced LLM currently available. This AI model outperforms all the existing LLMs, such as GPT\\-4 GPT\\-4o mini, Llama 3, etc. Claude 3\\.5 Sonnet has a context window of 200K with a max output of 8192 tokens. It can generate text in an enormous paragraph with much data as input. Claude 3\\.5 Sonnet is one of the best AI vision models, beating GPT\\-4o and Llama 3 in various test cases. It can extract data and text from documents and PDFs. These are just some factors that make the Claude 3\\.5 Sonnet the best, but there is also a new feature that Anthropic added, which makes the LLM the best code generator, â€œArtifacts.â€ It is a pop\\-up window that allows the users to see their code, edit it, and see their project live in that pop\\-up window. This article will show the top cases of Artifacts and Claude 3\\.5 Sonnet and the new projects you can develop using this tool.\n\n\n\n\n## Use Cases of Claude 3\\.5 Sonnet\n\nTo use the Artifacts, you must have a Claude account. Claude 3\\.5 Sonnet is Free for everyone but allows only limited chats per day. Users must buy a subscription for around $20 to use the model with unlimited chats. Try these prompts in Claude 3\\.5 Sonnet and create exciting tools. Users can also publish their projects on the Internet by sharing the link.\n\n\n### 1\\. Interactive PDF Dashboard\n\nReading large PDFs is boring. So, letâ€™s create an interactive PDF dashboard that will allow me to read my PDFs better.\n\n***Prompt\\- Create an interactive PDF dashboard to help me view, read, and learn from this information in a more visually appealing way. It has a tab where I can get a quiz based on the information in the PDF. Make sure it has a dark mode.***\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*66Iowbu8ZKA-IGfK)\n\nTo use this prompt, first attach the PDF with the prompt whose summary you want to read. Users can also make changes according to their preferences, and Claude 3\\.5 Sonnet will generate the code again with the changes.\n\n[***Click here to see the project***](https://claude.site/artifacts/4b9590a8-260e-476a-ab75-ec1d69f81d1e)***.***\n\n\n### 2\\. Visualization of anything with animations\n\nVisualizing anything youâ€™ve read is probably the best way to understand it. Adding animations will take your visualization to the next level, making it easy to understand.\n\n***Prompt: Animate each process step to Create a visualization of photosynthesis. Make sure to make the whole dashboard visually appealing using different colors and textures to create high\\-quality animations.***\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*W5lWitTO5fj7lOs0)\n\nIt is the first output we get, which is impressive within 10 seconds of processing but could be better after refining the prompts by giving more instruction and precision. We get\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*TAguaCWhC1CYFNw7)\n\nAs you can see, we separated the whole cycle into different stages, and the final result is incredible. This method can help you understand any process by breaking it into more straightforward steps.\n\n[***Click here to see the project***](https://claude.site/artifacts/b9769e4e-c20b-43da-945a-bfc41782900c)***.***\n\n\n### 3\\. Scientific Tools\n\nScientific tools are tools and services that help users understand the concept of science quickly and visually. With the help of Claude 3\\.5 Sonnet, users can create scientific tools at a single prompt. Just describe your tool in detail. I will use it to create an interactive tool for electronics engineering.\n\n***Prompt â€” Create a dashboard using React that shows the Diode. We can connect it forward\\-biased or reverse\\-biased in the circuit, and based on physical factors such as doping and depletion region, we change the scenario by showing the movement of the holes and electrons in the diode. There is an option to change everything, such as voltage, current, type of diode, Si, or Ge, and use different colors and moving elements for the animation.***\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*9bE50q8nsh6_SQ36)\n\nUsers can use Claude 3\\.5 Sonnet to create multiple tools, such as an essential project school, the magnetâ€™s current effect, the pressure and temperature relation tool, atomic models animation, etc.\n\n***Click [here to see the project](https://claude.site/artifacts/5b18fb27-1093-41d2-ac4e-54e47e4ddd3a).***\n\n\n### 4\\. Game Development\n\nEveryone likes to play games. Using Claude 3\\.5 Sonnet, we can create different types of games. I am going to make a Tic Tac Toe game using LLM.\n\n***Prompt\\- create a tic tac toe game using react and make it functional. Make it visually appealing by using CSS.***\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*jlSn7mCbaFadUT3F)\n\nAnd the result could be better. After that, I tried to give more suggestions and input, and based on my suggestions, Claude developed a perfect Tic Tac Toe Game, and here is the result.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*4v9KP4KFmQgY14YZ)\n\nAs you can see, the final product refines the CSS. The animations have greatly improved, and a pop\\-up with the winnerâ€™s name is added by the end of the game. You can use these prompts to make games such as Snake Game, Ludo, Rock Paper Scissors, etc.\n\n[***Click here to try the project.***](https://claude.site/artifacts/d57fdf93-79fb-443a-9bee-a58ab6eb911f)\n\n\n### 5\\. Web Application\n\nWeb applications are tools that can be accessed directly on the browser without installing the app on the phone. Claude 3\\.5 Sonnet can be used to develop different web application types. In this case, we will examine an expense tracker app.\n\n***Prompt\\- Create a web application for an expense tracker with the following features: First, ask about their monthly expenses â€” say Rs. 2000\\. Now, whatever the person spends, make sure you make some categories like Food, Travel, and Necessity and have the option to add anything. Add an option like saving money at the end of the month. Also, it has an investing option, which you now know the type of. For Example, â€œIf you invest 200 monthly in mutual funds, after five years, you will have X amount. Also, the app has a feature to generate graphs and charts related to the expenses. Make sure that the appâ€™s UI/UX design is visually appealing.***\n\nAnd this is the final result.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*XGOwT3vpxL0dxzzP)\n\nUsers can use Claude 3\\.5 Sonnet to create multiple tools, such as a To\\-do list, simple calculator, movie recommendation app, text summarization tool, etc.\n\n[***Click here to try the project***](https://claude.site/artifacts/66770d05-aafe-45c4-b938-8cda0e82b903)***.***\n\n\n### 6\\. 3D Simulation\n\nClaude 3\\.5 Sonent can create 3D simulations of models or projects, whether solar system models, atomic mode, The Central Dogma of Molecular Biology, etc. You can visualize any idea in 3D using this LLM. We will simulate a 3D solar system model with all the planets revolving around the sun.\n\n***Prompt\\-make a three js app of a space sim with the gravity of a solar system with planets in a single web file. solar system in which the planet revolves around the sunâ€¦ whenever someone hovers the mouse, it pops up the planetâ€™s name and basic details such as mass, gravity, etc.; keep the physics concepts, such as the rotation and revolution of the planet, intact.***\n\nUse this prompt to get the desired results.\n\n***Click here to see the Project.***\n\n[***Project 1***](https://x.com/websim_ai/status/1803901523522699730?t=BCe28ywbC2xD1Mk4DRmo5w&s=08)\n\n[***Project 2***](https://x.com/ammaar/status/1804649903815115053?t=7PeWPg62bkABtKEtKVmFbw&s=08)\n\n[***Project 3***](https://x.com/goldcaddy77/status/1804724702901891313?t=iqcLQBhYaIRnt3gIBRKBQg&s=08)\n\n\n### 7\\. Mind Map\n\nA mind map is a brainstorming technique for visually organizing information hierarchically. The main feature is to make one main idea the central point of the diagram, with subtopics branching out and connecting to supporting ideas. It helps the readers easily memorize and understand the information quickly. Letâ€™s create a prompt by creating a mind map.\n\n***Prompt: Create a mind map of how object detection works. Use animations and colors to make it interactive and visually appealing.***\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*igUaLzf8b2ZTEy-m)\n\nBy changing the topic, you can use this prompt to generate any mind map you want. You can also add different colors and animations to make it more interactive.\n\n[***Click here to try the project***](https://claude.site/artifacts/f1ce9002-9434-4b40-9713-ef184c467557)\n\n\n### 8\\. SEO Tool\n\nSEO is essential for any blog post to rank on Google. Using Claude, we can create a tool that can help us to improve the seo of our website. You can use this prompt to make your tool.\n\n***Prompt\\- Create an SEO tool that allows me to upload my blog post, industry, and keyword for which I am trying to rank. After I upload all that stuff, I want to hit a button that gives highlighted suggestions on what to change. Include a reset button that starts the process over. And also added a plus button to keywords where I can add multiple. In the SEO suggestions area, after analyzing the SEO, give me specific numbers, stats from my blog post, and caution signs to show what needs attention. After I generate the blog post, give me a revised blog post suggestion based on the keywords. I will rank it and explain why the suggestion is also being made. Make it more interactive by making the CSS more advanced and adding one unique feature that you think is necessary to make the tool more helpful for bloggers. Make sure that the tool has a UI/UX design that is visually appealing.***\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*OfY0zIa4rzO7J55r)\n\nUsers can create a variety of tools that not only help them in SEO but also in various fields to increase productivity.\n\n[***Click here to try the project***](https://claude.site/artifacts/09f21906-6295-4d0a-9fa0-be4afd6dab71)***.***\n\n\n### 9\\. Object Detection Tool\n\nYou can also use Claude 3\\.5 Sonent to make projects based on Artficla Inteliigence and Machine Learning. You can try to create amazing projects by adding features.\n\n***Prompt\\- Create a single HTML file for a real\\-time object detection web application. Using TensorFlow.js and the COCO\\-SSD model***\n\n***The application should:***\n\n* ***Access the userâ€™s webcam and display the video feed.***\n* ***Perform object detection on the video feed in real time.***\n* ***Draw bounding boxes around detected objects and label them with their class and detection confidence.***\n* ***Display a list of uniquely detected objects below the video feed, showing the object class and when it was first detected.***\n* ***Ensure each object class is only listed once, regardless of how often itâ€™s detected.***\n* ***Use a detection frequency of 2 FPS to balance performance and responsiveness.***\n* ***Include error handling for camera access and model loading.***\n* ***Style the application for a clean, modern look with a responsive design.***\n* ***Include all necessary HTML, CSS, and JavaScript in a self\\-contained HTML file.***\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*QSvMcxCuulkRKOlm)\n\n\n## Conclusion\n\nClaude 3\\.5 Sonnet, Anthropicâ€™s latest LLM, has revolutionized AI capabilities with its advanced features and versatile applications. The modelâ€™s impressive 200K context window, superior vision capabilities, and innovative â€œArtifactsâ€ feature for code generation set it apart from competitors. This article showcased diverse use cases of Claude 3\\.5 Sonnet, demonstrating its potential in creating interactive dashboards, visualizations, scientific tools, games, web applications, 3D simulations, mind maps, and SEO tools. These examples highlight the modelâ€™s ability to generate complex, functional, and visually appealing projects with simple prompts. Claude 3\\.5 Sonnetâ€™s user\\-friendly interface and powerful capabilities make it an invaluable tool for developers, educators, and professionals across various fields, opening up new possibilities for AI\\-assisted creation and problem\\-solving.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/bolt-new-and-ollama-revolutionizing-ai-powered-full-stack-web-development-2aa6aadf5958","frontmatter":{"title":"Bolt.new and Ollama: Revolutionizing AI-Powered Full-Stack Web Development","meta_title":"Bolt.new and Ollama: Revolutionizing AI-Powered Full-Stack Web Development","description":"Bolt.new is an AI-powered full-stack web development tool that operates directly in the browser, enhancing efficiency and accessibility in building web applications. It integrates with Ollama, allowing users to run open-source AI models locally, which offers cost savings and greater control over the development environment. Key features include in-browser development, comprehensive AI environment control, and easy deployment capabilities. The article provides a detailed installation guide and practical demonstrations, showcasing Bolt.news ability to create various applications, from simple web pages to complex financial service apps.","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*vbo04xVLorq_rvpeEDCaAg.jpeg","categories":["Programming","Technology/Web","Data Science"],"author":"Rifx.Online","tags":["Bolt","Ollama","browser","deployment","web"],"draft":false,"slug":"blog/bolt-new-and-ollama-revolutionizing-ai-powered-full-stack-web-development-2aa6aadf5958"},"content":"\n\n\n\n\n\nIn the rapidly evolving world of web development, efficiency and innovation are paramount. Developers, project managers, and designers alike are constantly seeking tools that streamline workflows, reduce costs, and enhance productivity. Enter **Bolt.new**, a groundbreaking AI\\-powered full\\-stack web development agent that operates entirely within your browser. Paired with **Ollama**, a tool that allows you to run open\\-source AI models locally, Bolt.new is set to transform the way we build and deploy web applications. This article delves deep into Bolt.new, its integration with Ollama, and provides a comprehensive guide to getting started.\n\n\n## Table of Contents\n\n1. Introduction to Bolt.new\n2. What Sets Bolt.new Apart\n* Full\\-Stack Development in the Browser\n* AI with Environment Control\n\n3\\. Integrating Bolt.new with Ollama\n\n* Why Use Ollama?\n* Installation and Setup\n\n4\\. Step\\-by\\-Step Installation Guide\n\n* Prerequisites\n* Cloning the Repository\n* Configuring Environment Variables\n* Installing Dependencies\n* Running the Application\n\n5\\. Running Bolt.new with Docker\n\n6\\. Practical Demonstrations\n\n* Creating a Simple Web Page\n* Building a Snake Game\n* Developing a Full\\-Stack Financial Service Web App\n\n7\\. Tips and Tricks for Maximizing Bolt.new\n\n\n## Introduction to Bolt.new\n\nBolt.new is an innovative tool designed to simplify the process of building full\\-stack web applications. Leveraging advanced AI models, Bolt.new allows users to **prompt**, **run**, **edit**, and **deploy** applications directly from their browser. This eliminates the need for complex local setups, making web development more accessible and efficient.\n\nWhether youâ€™re an experienced developer, a project manager overseeing multiple projects, or a designer looking to prototype quickly, Bolt.new offers a versatile platform to bring your ideas to life with minimal effort.\n\n\n## What Sets Bolt.new Apart\n\nWhile numerous AI models and development tools exist, Bolt.new distinguishes itself through its comprehensive capabilities and seamless integration. Hereâ€™s a closer look at what makes Bolt.new unique:\n\n\n## Full\\-Stack Development in the Browser\n\nBolt.new integrates state\\-of\\-the\\-art AI models with an in\\-browser development environment powered by [StackBlitzâ€™s WebContainers](https://github.com/stackblitz/webcontainer-core). This integration enables a host of functionalities:\n\n* **Install and Run npm Tools and Libraries:** Utilize popular frameworks like Vite, Next.js, and more without leaving your browser.\n* **Run Node.js Servers:** Manage backend operations seamlessly.\n* **Interact with Third\\-Party APIs:** Enhance your applicationâ€™s functionality by integrating various services.\n* **Deploy to Production from Chat:** Push your applications live directly through the chat interface.\n* **Share Work via URL:** Easily share your projects with collaborators or stakeholders.\n\n\n## AI with Environment Control\n\nUnlike traditional development environments where AI assistance is limited to code generation, Bolt.new empowers AI models with **complete control** over the development environment. This includes managing the filesystem, node server, package manager, terminal, and browser console. Such comprehensive control enables AI agents to handle the entire application lifecycle â€” from creation to deployment â€” streamlining the development process significantly.\n\n\n## Integrating Bolt.new with Ollama\n\nTo further enhance Bolt.newâ€™s capabilities and offer more flexibility, integration with **Ollama** is a game\\-changer.\n\n\n## Why Use Ollama?\n\n**Ollama** allows you to run open\\-source AI models locally on your machine. This integration offers several advantages:\n\n* **Cost Efficiency:** Avoid paying for token usage associated with cloud\\-based AI models.\n* **Flexibility:** Access a variety of models, from Llama 3\\.2 Vision to Deep SE Coder, based on your preferences.\n* **Privacy and Control:** Run models locally to maintain data privacy and control over the development environment.\n\n\n## Installation and Setup\n\nIntegrating Ollama with Bolt.new involves a few straightforward steps. Below is a detailed guide to help you get started.\n\n\n## Step\\-by\\-Step Installation Guide\n\n\n## Prerequisites\n\nBefore setting up Bolt.new with Ollama, ensure you have the following installed on your system:\n\n1. **Git:** Essential for cloning repositories.\n* [Download Git](https://git-scm.com/downloads)\n\n**2\\. Node.js:** The runtime environment for executing JavaScript on the server.\n\n* [Download Node.js](https://nodejs.org/en/download/)\n\n**3\\. Docker (Optional):** For containerizing applications.\n\n* [Download Docker](https://www.docker.com/)\n\n**4\\. Ollama:** For running open\\-source AI models locally.\n\n* [Download Ollama](https://ollama.com/)\n\n\n## Cloning the Repository\n\nBegin by cloning the Bolt.new repository from GitHub\n\n\n```python\ngit clone https://github.com/coleam00/bolt.new-any-llm.git\n```\n\n## Configuring Environment Variables\n\n1. **Rename Configuration File:** Navigate to the cloned repository and rename the `.env.example` file to `.env.local`.\n2. **Add Your LLM API Keys:** Open the `.env.local` file and add your API keys:\n\n\n```python\nGROQ_API_KEY=YOUR_GROQ_API_KEY\nOPENAI_API_KEY=YOUR_OPENAI_API_KEY\nANTHROPIC_API_KEY=YOUR_ANTHROPIC_API_KEY\n```\n**Note:** If youâ€™re using Ollama, it doesnâ€™t require an API key as it runs locally.\n\n**3\\. Optional Debug Level:** You can set the debug level to help with troubleshooting:\n\n\n```python\nVITE_LOG_LEVEL=debug\n```\n**Important:** Never commit your `.env.local` file to version control as it's included in `.gitignore`.\n\n\n## Installing Dependencies\n\nBolt.new utilizes `pnpm` for package management. Install the dependencies using the following commands:\n\n1. **Install pnpm (if not already installed):**\n\n\n```python\nsudo npm install -g pnpm\n```\n**2\\. Install Project Dependencies**\n\n\n```python\npnpm install\n```\n\n## Running the Application\n\nStart the development server with:\n\n\n```python\npnpm run dev\n```\nThis command initializes the Remix Vite development server. For optimal performance, it is recommended to use [Google Chrome Canary](https://www.google.com/chrome/canary/) as your browser.\n\n\n## Running Bolt.new with Docker\n\nFor those who prefer containerized environments, Bolt.new offers robust Docker support.\n\n\n## Using Helper Scripts\n\nBolt.new provides NPM scripts for building Docker images:\n\n* **Development Build:**\n\n\n```python\nnpm run dockerbuild\n```\n* **Production Build:**\n\n\n```python\nnpm run dockerbuild:prod\n```\n\n## Direct Docker Build Commands\n\nAlternatively, use Dockerâ€™s target feature to specify the build environment:\n\n* **Development Build:**\n\n\n```python\ndocker build . --target bolt-ai-development\n```\n* **Production Build:**\n\n\n```python\ndocker build . --target bolt-ai-productio\n```\n\n## Docker Compose with Profiles\n\nManage different environments using Docker Compose profiles:\n\n* **Development Environment:**\n\n\n```python\ndocker-compose --profile development up\n```\n* **Production Environment:**\n\n\n```python\ndocker-compose --profile production up\n```\n**Note:** When running the Docker Compose command with the development profile, any changes made to the code on your machine will automatically reflect in the running container, enabling hot reloading.\n\n\n## Practical Demonstrations\n\nTo showcase Bolt.newâ€™s capabilities, letâ€™s walk through a few practical examples.\n\n\n## Creating a Simple Web Page\n\nOne of the simplest demonstrations involves generating a basic web page:\n\n1. **Prompt Bolt.new:** Request the AI to create a simple web page.\n2. **Generation:** Bolt.new generates all necessary folders and files.\n3. **Preview:** Utilize the preview functionality to visualize the output instantly.\n\nThis process underscores Bolt.newâ€™s ability to handle straightforward tasks efficiently, providing a solid foundation for more complex projects.\n\n\n## Building a Snake Game\n\nBolt.newâ€™s prowess becomes more evident when tasked with creating interactive applications, such as a snake game:\n\n1. **Prompt Bolt.new:** Ask the AI to help create a snake game.\n2. **Generation:** Bolt.new generates all required files, packages, and the frontend interface.\n3. **Preview:** Open the generated HTML file to see a fully functional snake game that tracks scores.\n\n**Outcome:** The AI successfully generates a visually appealing and functional game, demonstrating its capability to handle dynamic and interactive web applications.\n\n\n## Developing a Full\\-Stack Financial Service Web App\n\nFor a more comprehensive demonstration, letâ€™s explore building a full\\-stack financial service application:\n\n1. **Prompt Bolt.new:**\n* **Frontend:** Use React for the user interface.\n* **Backend:** Implement Next.js for server\\-side rendering.\n* **Database:** Integrate PostgreSQL for data management.\n* **Authentication:** Set up with Clerk.\n\n\n```python\nCreate a full-stack financial service web app with a clean, intuitive UI using ChatGPT and React for the frontend, Next.js for server-side rendering, PostgreSQL for data management, and authentication set up with Clerk.\n```\n**2\\. Generation Process:**\n\n* **File Creation:** Bolt.new generates the necessary project structure and files.\n* **Package Installation:** Installs required packages like React, Next.js, and Clerk.\n* **Backend Setup:** Configures server\\-side rendering and database connections.\n* **Authentication:** Integrates Clerk for user authentication.\n\n**3\\. Preview:** Access the application via the provided URL to see a fully functional financial dashboard featuring:\n\n* **Balance History:** Overview of all deposits.\n* **Budget Configuration:** Ability to add budgets from various categories.\n* **Transaction Management:** Add and view transactions.\n* **Investment Tracking:** Monitor investments.\n\n**Outcome:** Bolt.new efficiently manages the creation of a complex, multi\\-faceted application in a single prompt, highlighting its potential for large\\-scale projects.\n\n\n## Tips and Tricks for Maximizing Bolt.new\n\nTo get the most out of Bolt.new, consider the following strategies:\n\n1. **Be Specific About Your Stack:**\n* Clearly mention the frameworks or libraries you wish to use (e.g., Astro, Tailwind, ShadCN) in your initial prompt to ensure Bolt.new scaffolds the project accordingly.\n\n**2\\. Use the Enhance Prompt Icon:**\n\n* Before submitting your prompt, use the â€˜enhanceâ€™ feature to refine your instructions. This leads to more accurate and efficient code generation.\n\n**3\\. Scaffold Basics First:**\n\n* Start with the fundamental structure of your application before adding advanced features. This helps Bolt.new understand the project foundation, ensuring subsequent functionalities are well\\-integrated.\n\n**4\\. Batch Simple Instructions:**\n\n* Combine multiple simple tasks into a single prompt to save time and reduce API credit consumption. For example, request changes to the color scheme, add mobile responsiveness, and restart the dev server all at once.\n\n**5\\. Leverage Open\\-Source Customization:**\n\n* Since Bolt.new is open\\-source, explore the [Bolt.new GitHub repository](https://github.com/coleam00/bolt.new-any-llm.git) to customize and extend functionalities to suit your specific project needs.\n\nBolt.new, especially when integrated with Ollama, represents a significant leap forward in AI\\-powered web development. By combining advanced AI models with robust development tools, Bolt.new simplifies the process of building, deploying, and managing full\\-stack applications. Whether youâ€™re looking to expedite your development workflow, explore AI\\-driven coding assistance, or build sophisticated web applications with minimal setup, Bolt.new provides the tools and flexibility to achieve your goals.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/build-a-customer-support-assistant-with-llama3-1-7bf60611e428","frontmatter":{"title":"Build a Customer Support Assistant with Llama3.1","meta_title":"Build a Customer Support Assistant with Llama3.1","description":"Use LLM Agents and Amazon Bedrock to Solve Customer Queries with AI: A Guide to Building and Deploying a Support Assistant with Llama3.1","date":"2024-11-13T01:22:29.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*lNyf72c2_r1wKjnoRA1_FQ.png","categories":["Programming","Chatbots","Technology/Web"],"author":"Rifx.Online","tags":["Llama3.1","AmazonBedrock","Gradio","EC2","CustomerSupport"],"draft":false,"slug":"blog/build-a-customer-support-assistant-with-llama3-1-7bf60611e428"},"content":"\n\n\n\n\n### Use LLM Agents and Amazon Bedrock to Solve Customer Queries with AI: A Guide to Building and Deploying a Support Assistant with Llama3\\.1\n\n\n\n\n## Introduction\n\n\n### Problem\n\nBusinesses often face the challenge of handling a large volume of customer inquiries. These queries can range from mundane questions like â€œWhat is the status of my order?â€ to more complex issues requiring human intervention. The sheer volume of repetitive queries can overwhelm customer support teams, leading to longer response times and reduced customer satisfaction. Additionally, utilizing human resources for simple, routine queries is inefficient and costly. Thereâ€™s a growing need for automated solutions that can handle routine queries effectively, allowing human agents to focus on escalated cases that require nuanced problem\\-solving.\n\n\n### Solution\n\nThe introduction of Large Language Model (LLM) agents offers a promising solution to this problem. An [LLM agent](https://proxy.rifx.online/https://research.ibm.com/blog/what-are-ai-agents-llm) can respond to user queries by accessing and interpreting data from a companyâ€™s database, handling simple operations such as checking order status, retrieving account information, and answering FAQs. By automating these routine tasks, an LLM agent ensures faster resolution times and frees up human resources for more complex customer support scenarios. In this guide, weâ€™ll explore how to build a customer support assistant using the Llama3\\.1 model from Amazon Bedrock Tools api.\n\nAt the end, we will have the assistant running locally in our machine and making calls to a fake database:\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Ok9N3mdX50JVWbaJKUrJeQ.gif)\n\n\n## LLM Agents\n\n\n### What are LLM agents\n\n[LLM agents](https://proxy.rifx.online/https://research.ibm.com/blog/what-are-ai-agents-llm) are specialized applications built on large language models like Llama3\\.1, designed to perform specific tasks or functions. Unlike general LLMs, which generate human\\-like text based on a given prompt, LLM agents are equipped with additional capabilities such as accessing external databases, performing operations, and making decisions based on predefined rules. They are tailored to handle specific use cases, such as customer support, where they can interact with users, retrieve information, and execute commands based on the context of the conversation.\n\nWhile general LLMs are powerful in generating coherent text and understanding language, LLM agents take this a step further by integrating with external systems, allowing them to perform real\\-world tasks beyond just text generation.\n\nAgent have set of instructions, a foundation model, a set of available actions and knowledge bases, which enables then to execute complex tasks.\n\nA generative model can answer a general question, or a question related to your documentation, like â€œI canâ€™t see my meetings?, How do I book a meeting?â€. An agent, using a foundational model as their reasoning logic and external data sources like your APIs, can return the user their no. of booked meetings, or directly schedule a meeting from the interaction screen.\n\nThere are many agents in the â€œgeneral purposeâ€ category, and also specialized agents for task specific purpose like code assistant ([Amazon CodeWhisperer, Copilot](https://proxy.rifx.online/https://www.missioncloud.com/blog/github-copilot-vs-amazon-codewhisperer)), writing assistant, system design ([Amazon Q](https://proxy.rifx.online/https://aws.amazon.com/q/)) , wikipedia summary, etc.\n\n**AI agents landscape:**\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*VuAyzZ2BfrD7o-z0lOpUwA.png)\n\n\n### Creating a Basic Agent from Scratch Using Python\n\nLetâ€™s create a simple LLM agent from scratch using Python. This amazing medium article demonstrates how to build an agent without relying on any libraries or frameworks.\n\n\n## Custom Support Assistant\n\nNow, letâ€™s create a more sophisticated customer support assistant using the [Llama3\\.1](https://proxy.rifx.online/https://llama.meta.com/) model from [Bedrock](https://proxy.rifx.online/https://aws.amazon.com/bedrock/) Tools. This agent will be able to perform more complex tasks, such as looking up user data from a database and executing simple operations like viewing shipping status of an order.\n\n\n### Defining Capabilities and boundaries\n\nBefore building our assistant, itâ€™s essential to define what actions the agent can perform and establish clear boundaries for its operation. In a production environment, these capabilities and boundaries are crucial to ensure the agent operates effectively and securely.\n\n**Capabilities:**\n\n* Respond to common customer queries (e.g., order status, return policy).\n* Access and retrieve user data from a database.\n* Perform simple operations like viewing order status, updating customer information, etc.\n\n**Boundaries:**\n\n* The agent should not execute actions that require human judgment, such as processing refunds or handling escalations.\n* It should operate within the defined scope and not access sensitive data unless explicitly permitted.\n* Error handling and fallback mechanisms should be in place for unsupported queries.\n\n\n### Architecture\n\nThe system architecture for our solution involves several components working together:\n\n1. **LLM Agent**: The core of the system, built using the [Llama3\\.1](https://proxy.rifx.online/https://llama.meta.com/) or [Claude 3\\.5 Sonnet](https://proxy.rifx.online/https://www.anthropic.com/news/claude-3-5-sonnet) model, which handles natural language processing and decision\\-making.\n2. **Database**: Stores customer data and other relevant information that the agent can query.\n3. **API Layer**: Facilitates communication between the LLM agent and the database, allowing the agent to retrieve and manipulate data.\n4. **User Interface**: A frontend interface (e.g., a chatbot interface) where customers interact with the support assistant.\n\n\n### Code\n\nBefore we examine the code, please ensure you have the following:\n\n1. Knowledge of Python and the [boto3](https://proxy.rifx.online/https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) library.\n2. A working AWS account with model access enabled in [Bedrock](https://proxy.rifx.online/https://aws.amazon.com/bedrock/).\n3. [A virtual environment](https://proxy.rifx.online/https://docs.anaconda.com/miniconda/) with Python and boto3 installed.\n\n\n\n\n\n\n\n\n### Code Walkthrough\n\n\n```python\nfrom datetime import datetime\nimport json\nfrom typing import Any, Dict, List\n\nimport boto3\nfrom botocore.exceptions import ClientError\n\n## Initialize a Boto3 session and create a Bedrock runtime client\nsession = boto3.Session()\nregion = \"us-east-1\" # us-west-2 has better runtime quota\nbedrock_client = session.client(service_name = 'bedrock-runtime', region_name = region)\n```\nFirst, we import the necessary packages and create an instance of the `boto3` Bedrock runtime client, called `bedrock_client`, for the `us-east-1` region. If your AWS account has the `us-west-2` availability zone (AZ) enabled, use that instead. At the time of writing, Llama3\\.1 models are only available in the `us-west-2` AZ and it also has a larger runtime quota for the `claude-3.5-sonnet` model (250 requests per minute) compared to the `us-east-1` AZ, which supports only 50 requests per minute.\n\n\n```python\n## Define available models with their respective request limits\navailable_models = {\n    \"sonnet3-5\": \"anthropic.claude-3-5-sonnet-20240620-v1:0\", # 50 requests per min\n    \"sonnet\": \"anthropic.claude-3-sonnet-20240229-v1:0\", # 500 requests per min\n    \"llama31-70b\": \"meta.llama3-1-70b-instruct-v1:0\", # 400 requests per min\n    \"llama31-405b\": \"meta.llama3-1-405b-instruct-v1:0\", # 50 requests per min\n}\nmodelId = available_models[\"sonnet3-5\"]  # Select model for conversation\n```\nNext, we create a mapping of model IDs in Bedrock. **Currently not all the models available in Amazon Bedrock support tool use**. Please check the list of [supported models](https://proxy.rifx.online/https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html#conversation-inference-supported-models-features) from Amazon Bedrock user guide [here](https://proxy.rifx.online/https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html#conversation-inference-supported-models-features).\n\n\n```python\nclass FakeDatabase:\n    \"\"\"Sample fake database implementation.\"\"\"\n    def __init__(self):\n        self.customers = [\n            {\"id\": \"1213210\", \"name\": \"John Doe\", \"email\": \"john@gmail.com\", \"phone\": \"123-456-7890\", \"username\": \"johndoe\"},\n            {\"id\": \"2837622\", \"name\": \"Priya Patel\", \"email\": \"priya@candy.com\", \"phone\": \"987-654-3210\", \"username\": \"priya123\"},\n            {\"id\": \"3924156\", \"name\": \"Liam Nguyen\", \"email\": \"lnguyen@yahoo.com\", \"phone\": \"555-123-4567\", \"username\": \"liamn\"},\n            {\"id\": \"4782901\", \"name\": \"Aaliyah Davis\", \"email\": \"aaliyahd@hotmail.com\", \"phone\": \"111-222-3333\", \"username\": \"adavis\"},\n            {\"id\": \"5190753\", \"name\": \"Hiroshi Nakamura\", \"email\": \"hiroshi@gmail.com\", \"phone\": \"444-555-6666\", \"username\": \"hiroshin\"},\n            {\"id\": \"6824095\", \"name\": \"Fatima Ahmed\", \"email\": \"fatimaa@outlook.com\", \"phone\": \"777-888-9999\", \"username\": \"fatimaahmed\"},\n            {\"id\": \"7135680\", \"name\": \"Alejandro Rodriguez\", \"email\": \"arodriguez@protonmail.com\", \"phone\": \"222-333-4444\", \"username\": \"alexr\"},\n            {\"id\": \"8259147\", \"name\": \"Megan Anderson\", \"email\": \"megana@gmail.com\", \"phone\": \"666-777-8888\", \"username\": \"manderson\"},\n            {\"id\": \"9603481\", \"name\": \"Kwame Osei\", \"email\": \"kwameo@yahoo.com\", \"phone\": \"999-000-1111\", \"username\": \"kwameo\"},\n            {\"id\": \"1057426\", \"name\": \"Mei Lin\", \"email\": \"meilin@gmail.com\", \"phone\": \"333-444-5555\", \"username\": \"mlin\"}\n        ]\n\n        self.orders = [\n            {\"id\": \"24601\", \"customer_id\": \"1213210\", \"product\": \"Wireless Headphones\", \"quantity\": 1, \"price\": 79.99, \"status\": \"Shipped\"},\n            {\"id\": \"13579\", \"customer_id\": \"1213210\", \"product\": \"Smartphone Case\", \"quantity\": 2, \"price\": 19.99, \"status\": \"Processing\"},\n            {\"id\": \"97531\", \"customer_id\": \"2837622\", \"product\": \"Bluetooth Speaker\", \"quantity\": 1, \"price\": \"49.99\", \"status\": \"Shipped\"}, \n            {\"id\": \"86420\", \"customer_id\": \"3924156\", \"product\": \"Fitness Tracker\", \"quantity\": 1, \"price\": 129.99, \"status\": \"Delivered\"},\n            {\"id\": \"54321\", \"customer_id\": \"4782901\", \"product\": \"Laptop Sleeve\", \"quantity\": 3, \"price\": 24.99, \"status\": \"Shipped\"},\n            {\"id\": \"19283\", \"customer_id\": \"5190753\", \"product\": \"Wireless Mouse\", \"quantity\": 1, \"price\": 34.99, \"status\": \"Processing\"},\n            {\"id\": \"74651\", \"customer_id\": \"6824095\", \"product\": \"Gaming Keyboard\", \"quantity\": 1, \"price\": 89.99, \"status\": \"Delivered\"},\n            {\"id\": \"30298\", \"customer_id\": \"7135680\", \"product\": \"Portable Charger\", \"quantity\": 2, \"price\": 29.99, \"status\": \"Shipped\"},\n            {\"id\": \"47652\", \"customer_id\": \"8259147\", \"product\": \"Smartwatch\", \"quantity\": 1, \"price\": 199.99, \"status\": \"Processing\"},\n            {\"id\": \"61984\", \"customer_id\": \"9603481\", \"product\": \"Noise-Cancelling Headphones\", \"quantity\": 1, \"price\": 149.99, \"status\": \"Shipped\"},\n            {\"id\": \"58243\", \"customer_id\": \"1057426\", \"product\": \"Wireless Earbuds\", \"quantity\": 2, \"price\": 99.99, \"status\": \"Delivered\"},\n            {\"id\": \"90357\", \"customer_id\": \"1213210\", \"product\": \"Smartphone Case\", \"quantity\": 1, \"price\": 19.99, \"status\": \"Shipped\"},\n            {\"id\": \"28164\", \"customer_id\": \"2837622\", \"product\": \"Wireless Headphones\", \"quantity\": 2, \"price\": 79.99, \"status\": \"Processing\"}\n        ]\n\n    def get_user(self, key:str, value:str) -> Dict[str, str]:\n        \"\"\"Return metadata of user.\"\"\"\n        if key in {\"email\", \"phone\", \"username\"}:\n            for customer in self.customers:\n                if customer[key] == value:\n                    return customer\n            return f\"Couldn't find a user with {key} of {value}\"\n        else:\n            raise ValueError(f\"Invalid key: {key}\")\n        \n        return None\n\n    def get_order_by_id(self, order_id: str) -> Dict[str, str]:\n        \"\"\"Return metadata of the order using order id.\"\"\"\n        for order in self.orders:\n            if order[\"id\"] == order_id:\n                return order\n        return None\n    \n    def get_customer_orders(self, customer_id: str) -> List[Dict[str, str]]:\n        \"\"\"Return a list of orders for a specific customer.\"\"\"\n        return [order for order in self.orders if order[\"customer_id\"] == customer_id]\n\n    def cancel_order(self, order_id: str) -> str:\n        \"\"\"Cancel an order if it's in 'Processing' status.\"\"\"\n        order = self.get_order_by_id(order_id)\n        if order:\n            if order[\"status\"] == \"Processing\":\n                order[\"status\"] = \"Cancelled\"\n                return \"Cancelled the order\"\n            else:\n                return \"Order has already shipped.  Can't cancel it.\"\n        return \"Can't find that order!\"\n```\nFor this demo, we implement a mock database class with a predefined list of customers and their orders. This mock database class also includes methods to retrieve data from the database.\n\n* `get_user` : Returns the user\n* `get_order_by_id` : Returns the order using order id\n* `get_customer_orders` : Returns all the orders of a particular customer\n* `cancel_order` : Cancel an order if itâ€™s in â€˜Processingâ€™ status.\n\n\n```python\n## Define all the tools avilable to the model\ntool_config = {\n    \"tools\": [\n        {\n            \"toolSpec\": {\n                \"name\": \"get_user\",\n                \"description\": \"Looks up a user by email, phone, or username.\",\n                \"inputSchema\": {\n                    \"json\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"key\": {\n                                \"type\": \"string\",\n                                \"enum\": [\"email\", \"phone\", \"username\"],\n                                \"description\": \"The attribute to search for a user by (email, phone, or username).\",\n                            },\n                            \"value\": {\n                                \"type\": \"string\",\n                                \"description\": \"The value to match for the specified attribute.\",\n                            },\n                        },\n                        \"required\": [\"key\", \"value\"],\n                    }\n                },\n            }\n        },\n        {\n            \"toolSpec\": {\n                \"name\": \"get_order_by_id\",\n                \"description\": \"Retrieves the details of a specific order based on the order ID. Returns the order ID, product name, quantity, price, and order status.\",\n                \"inputSchema\": {\n                    \"json\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"order_id\": {\n                                \"type\": \"string\",\n                                \"description\": \"The unique identifier for the order.\",\n                            }\n                        },\n                        \"required\": [\"order_id\"],\n                    }\n                },\n            }\n        },\n        {\n            \"toolSpec\": {\n                \"name\": \"get_customer_orders\",\n                \"description\": \"Retrieves the list of orders belonging to a user based on a user's customer id.\",\n                \"inputSchema\": {\n                    \"json\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"customer_id\": {\n                                \"type\": \"string\",\n                                \"description\": \"The customer_id belonging to the user\",\n                            }\n                        },\n                        \"required\": [\"customer_id\"],\n                    }\n                },\n            }\n        },\n        {\n            \"toolSpec\": {\n                \"name\": \"cancel_order\",\n                \"description\": \"Cancels an order based on a provided order_id.  Only orders that are 'processing' can be cancelled\",\n                \"inputSchema\": {\n                    \"json\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"order_id\": {\n                                \"type\": \"string\",\n                                \"description\": \"The order_id pertaining to a particular order\",\n                            }\n                        },\n                        \"required\": [\"order_id\"],\n                    }\n                },\n            }\n        },\n    ],\n    \"toolChoice\": {\"auto\": {}},\n}\n```\nNext we define a `tool_config` .\n\nYou can use the Amazon Bedrock API to give a model access to [tools](https://proxy.rifx.online/https://docs.aws.amazon.com/bedrock/latest/userguide/tool-use.html) that can help it generate responses for messages that you send to the model. For example, you might have a chat application that lets users find out out the most popular song played on a radio station. To answer a request for the most popular song, a model needs a tool that can query and return the song information.\n\n\n> Tool use with models is also known as *Function calling*.\n\nIn Amazon Bedrock, the model doesnâ€™t directly call the tool. Rather, when you send a message to a model, you also supply a definition for one or more tools that could potentially help the model generate a response. In this example, you would supply a definition for tools that returns the customer details, order details or cancel an order. If the model determines that it needs the tool to generate a response for the message, the model responds with a request for you to call the tool. It also includes the input parameters (the required customer id or order id) to pass to the tool.\n\nIn your code, you call the tool on the modelâ€™s behalf. In this scenario, assume the tool implementation is an API. The tool could just as easily be a database, Lambda function, or some other software. You decide how you want to implement the tool. You then continue the conversation with the model by supplying a message with the result from the tool. Finally the model generates a response for the original message that includes the tool results that you sent to the model.\n\nIn our example, we define all the functions we want the chatbot to execute in the `tool_config` . Refer to the [Amazon Bedrock documentation](https://proxy.rifx.online/https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_ToolConfiguration.html) for more information on the ToolConfiguration API.\n\n\n```python\ndef process_tool_call(tool_name: str, tool_input: Any) -> Any:\n    \"\"\"Process the tool call based on the tool name and input.\"\"\"\n    if tool_name == \"get_user\":\n        return db.get_user(tool_input[\"key\"], tool_input[\"value\"])\n    elif tool_name == \"get_order_by_id\":\n        return db.get_order_by_id(tool_input[\"order_id\"])\n    elif tool_name == \"get_customer_orders\":\n        return db.get_customer_orders(tool_input[\"customer_id\"])\n    elif tool_name == \"cancel_order\":\n        return db.cancel_order(tool_input[\"order_id\"])\n```\nSince our application code will call the required tools on behalf of the LLM, we package all the tools into a single function. The `process_tool_call` function executes the appropriate functions based on the `tool_name` and `tool_input` provided by the LLM.\n\n\n```python\ndef simple_chat():\n    \"\"\"Main chat function that interacts with the user and the LLM.\"\"\"\n    system_prompt = \"\"\"\n    You are a customer support chat bot for an online retailer called TechNova. \n    Your job is to help users look up their account, orders, and cancel orders.\n    Be helpful and brief in your responses.\n    You have access to a set of tools, but only use them when needed.  \n    If you do not have enough information to use a tool correctly, ask a user follow up questions to get the required inputs.\n    Do not call any of the tools unless you have the required data from a user. \n    \"\"\"\n    # Initial user message\n    user_message = input(\"\\nUser: \")\n    messages = [{\"role\": \"user\", \"content\": [{\"text\": user_message}]}]\n\n    while True:\n        # If the last message is from the assistant, get another input from the user\n        if messages[-1].get(\"role\") == \"assistant\":\n            user_message = input(\"\\nUser: \")\n            messages.append({\"role\": \"user\", \"content\": [{\"text\": user_message}]})\n\n        # Parameters for API request to the Bedrock model\n        converse_api_params = {\n            \"modelId\": modelId,\n            \"system\": [{\"text\": system_prompt}],\n            \"messages\": messages,\n            \"inferenceConfig\": {\"maxTokens\": 4096},\n            \"toolConfig\": tool_config,  # Pass the tool config\n        }\n\n        # Get response from Bedrock model\n        response = bedrock_client.converse(**converse_api_params)\n\n        # Append assistant's message to the conversation\n        messages.append(\n            {\"role\": \"assistant\", \"content\": response[\"output\"][\"message\"][\"content\"]}\n        )\n\n        # If the model wants to use a tool, process the tool call\n        if response[\"stopReason\"] == \"tool_use\":\n            tool_use = response[\"output\"][\"message\"][\"content\"][\n                -1\n            ]  # Naive approach assumes only 1 tool is called at a time\n            tool_id = tool_use[\"toolUse\"][\"toolUseId\"]\n            tool_name = tool_use[\"toolUse\"][\"name\"]\n            tool_input = tool_use[\"toolUse\"][\"input\"]\n\n            print(f\"Claude wants to use the {tool_name} tool\")\n            print(f\"Tool Input:\")\n            print(json.dumps(tool_input, indent=2))\n\n            # Run the underlying tool functionality on the fake database\n            tool_result = process_tool_call(tool_name, tool_input)\n\n            print(f\"\\nTool Result:\")\n            print(json.dumps(tool_result, indent=2))\n\n            # Append tool result message\n            messages.append(\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"toolResult\": {\n                                \"toolUseId\": tool_id,\n                                \"content\": [{\"text\": str(tool_result)}],\n                            }\n                        }\n                    ],\n                }\n            )\n\n        else:\n            # If the model does not want to use a tool, just print the text response\n            print(\n                \"\\nTechNova Support:\"\n                + f\"{response['output']['message']['content'][0]['text']}\"\n            )\n```\nThe `simple_chat` function handles user interaction, invokes the LLM, and passes the tool response back to the LLM.\n\nAn important line in this function is `response[\"stopReason\"] == \"tool_use\"`. This determines if the LLM wants to use a tool and, when parsed further, indicates which tool the LLM intends to invoke.\n\nAn example of response object of bedrock\\-runtime `converse` api:\n\n\n```python\n{\n    'ResponseMetadata': {\n        'RequestId': '07f323a7-cc52-4813-9d1b-83e5c3ae932a', \n        'HTTPStatusCode': 200, \n        'HTTPHeaders': {\n            'date': 'Thu, 08 Aug 2024 10:52:59 GMT', \n            'content-type': 'application/json', \n            'content-length': '519', \n            'connection': 'keep-alive', \n            'x-amzn-requestid': '07f323a7-cc52-4813-9d1b-83e5c3ae932a'\n        }, \n        'RetryAttempts': 0\n    }, \n    'output': {\n        'message': {\n            'role': 'assistant', 'content': [\n                {\n                    'text': \"Certainly! I'll search for search for your orders. Let me use our search tool to find that information for you.\"\n                }, {\n                    'toolUse': {\n                        'toolUseId': 'tooluse_8C_XIwrAROC3t3eEu5FCVw', \n                        'name': 'get_customer_orders', \n                        'input': {'customer_id': '1213210'}\n                    }\n                }\n            ]\n        }\n    }, \n    'stopReason': 'tool_use',\n    'usage': {'inputTokens': 672, 'outputTokens': 103, 'totalTokens': 775}, \n    'metrics': {'latencyMs': 2431}\n}\n```\nRefer to the [Amazon Bedrock API Reference](https://proxy.rifx.online/https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html) for more details about the Converse API.\n\nOnce we invoke the required tool or function using our `process_tool_call` function, we pass the function's response back to the LLM to generate a response for the end user.\n\nPlease note that we are using the Converse API of the boto3 Bedrock runtime client. You can also use the Converse Stream API to generate a streaming response. For more details, refer to the Amazon Bedrock API Reference for the Converse Stream API and the Boto3 documentation on the Converse Stream API.\n\n\n### Running in local terminal\n\nOnce you have everything set up correctly, run the Python file from inside your virtual environment using:\n\n\n```python\n## From inside the virtual environment\npython main.py\n```\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Ok9N3mdX50JVWbaJKUrJeQ.gif)\n\n\n## Deploy on EC2\n\nYou can deploy the chatbot on an EC2 instance for demonstration purposes using a [Gradio](https://proxy.rifx.online/https://www.gradio.app/) app, which provides a chatbot\\-like interface with just a few lines of code and integrates seamlessly with our main function.\n\n\n### Gradio\n\n[Gradio](https://proxy.rifx.online/https://www.gradio.app/) is an open\\-source Python library that simplifies the process of building and deploying web\\-based machine learning demos. It allows developers to create intuitive web interfaces for their models with minimal coding, making it easier to deploy and share models with others.\n\nLetâ€™s write a chat function that responds `Yes` or `No` randomly using gradio.\n\nHereâ€™s our chat function (please execute `pip install gradio` in your virtual environment if you donâ€™t have it already installed):\n\n\n```python\nimport random\n\nimport gradio as gr\n\n\ndef random_response(message, history):\n    return random.choice([\"Yes\", \"No\"])\n\ngr.ChatInterface(random_response).launch()\n```\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*XxkUM6yO3lmjN545tRlOvQ.png)\n\nRead more about the [gradio chatbot documentation here](https://proxy.rifx.online/https://www.gradio.app/main/docs/gradio/chatbot).\n\n\n### Running a Gradio App on your Web Server with Nginx\n\nLetâ€™s deploy our chatbot agent on EC2 with Nginx.\n\n**Install Nginx and create new conda env**\n\n1. **Create an EC2 instance** with at least 2â€“3 GB of memory. You can also deploy it on your Kubernetes or ECS cluster. Make sure to modify the Nginx configuration file to match your setup.\n\n2\\. **SSH into your EC2 instance** and [install Nginx](https://proxy.rifx.online/https://devopsden.io/article/how-to-install-nginx-on-ec2-instance):\n\n\n```python\nsudo yum update -y\nsudo amazon-linux-extras install nginx1.12\nsudo systemctl start nginx\nsudo systemctl enable nginx\nsudo systemctl status nginx\n```\n3\\. [**Install Miniconda**](https://proxy.rifx.online/https://docs.anaconda.com/miniconda/#quick-command-line-install) to manage Python packages:\n\n\n```python\nmkdir -p ~/miniconda3\nwget https://proxy.rifx.online/https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh\nbash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3\nrm -rf ~/miniconda3/miniconda.sh\n\n~/miniconda3/bin/conda init bash\n~/miniconda3/bin/conda init zsh\n```\n4\\. **Create a new Conda environment** with Python 3, and install `boto3` and `gradio`:\n\n\n```python\nconda create --name gradio-demo python=3.12 pip -y\nconda activate gradio-demo\npip install --no-cache-dir gradio boto3\n```\n5\\. **Create a new Python file** for your chatbot and Gradio code. Copy all your code into this file:\n\n\n```python\nvim gradio_demo.py\n```\nAlternatively, you can use `scp` to copy the file directly from your local machine to the remote instance.\n\n**Setup Nginx**\n\nNow we will **set up Nginx** to redirect all traffic from the `/gradio-demo` path to the local server started by the `gradio_demo.py` file. Refer to the [official documentation for running Gradio with Nginx here](https://proxy.rifx.online/https://www.gradio.app/guides/running-gradio-on-your-web-server-with-nginx).\n\n1. Edit the Nginx configuration file located at `/etc/nginx/nginx.conf`:\n\n\n```python\nvim /etc/nginx/nginx.conf\n```\n2\\. In the `http` block, add the following lines to include server block configurations from a separate file:\n\n\n```python\nserver_names_hash_bucket_size  128;\ninclude /etc/nginx/sites-enabled/*;\n```\n3\\. Create a new file in the `/etc/nginx/sites-available` directory (create the directory if it does not already exist), using a filename that represents your app, for example: `sudo vim /etc/nginx/sites-available/my_gradio_app` :\n\n\n```python\nsudo mkdir -p /etc/nginx/sites-enabled\nsudo vim /etc/nginx/sites-available/my_gradio_app\n```\nPaste the following contents in the `my_gradio_app` file:\n\n\n```python\nserver {\n    listen 80;\n    server_name www.ec2-12-34-56-78.us-west-2.compute.amazonaws.com; # Change this to your domain name\n\n    location /gradio-demo/ {  # Change this if you'd like to server your Gradio app on a different path\n        proxy_pass http://127.0.0.1:7860/; # Change this if your Gradio app will be running on a different port\n        proxy_buffering off;\n        proxy_redirect off;\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection \"upgrade\";\n        proxy_set_header Host $host;\n        proxy_set_header X-Forwarded-Host $host;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n}\n```\n4\\. Create a symbolic link to this file in the `/etc/nginx/sites-enabled` directory:\n\n\n```python\nsudo ln -s /etc/nginx/sites-available/my_gradio_app /etc/nginx/sites-enabled/\n```\n5\\. **Update the `gradio_demo.py` file** to set the root path in the Gradio launch API:\n\n\n```python\n.launch(root_path=\"/gradio-demo\")\n```\n6\\. **Check the Nginx configuration** and restart Nginx:\n\n\n```python\nsudo nginx -t\nsudo systemctl restart nginx\n```\nIf you encounter errors with the `nginx -t` command, resolve those errors before proceeding.\n\n**Run the `gradio_demo.py` file** in the background. You can use either `nohup` or `tmux`:\n\n\n```python\n## From inside the Conda environment\nnohup python gradio_demo.py &\n```\n**Access the EC2 DNS URL** and append `/gradio-demo/` to see your chatbot agent on the Gradio interface.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*rcdUROlShsrcaeBDpBKBAQ.png)\n\n\n## Summary\n\nIn this article, we explored how to build a customer support assistant using the [Llama3\\.1](https://proxy.rifx.online/https://llama.meta.com/) or [Claude 3\\.5 Sonnet](https://proxy.rifx.online/https://www.anthropic.com/news/claude-3-5-sonnet) model from Bedrock Tools. We began by defining the problem of handling repetitive customer queries and how LLM agents offer a solution. We then discussed the concept of LLM agents and how they differ from general LLMs. After that, we walked through creating a basic agent in Python and then developed a more complex customer support assistant using the models in Amazon Bedrock. We also covered deploying the assistant on EC2, including an example of using Gradio to create a web interface. By automating routine customer support tasks, businesses can enhance efficiency, reduce costs, and improve customer satisfaction.\n\nIn a production setting, you can pass the logged\\-in userâ€™s name and ID to the system prompt so that the LLM does not have to ask for basic details from a logged\\-in user. Some actions, such as canceling an order, may require additional gatekeeping. Additionally, if a customer is upset or becomes aggressive, the LLM should be instructed to escalate the case to a human assistant.\n\nYou can connect with me on LinkedIn: <https://proxy.rifx.online/https://linkedin.com/in/maheshrajput>\n\nThank you for reading ðŸ˜Š\n\n\n"},{"lang":"en","group":"blog","slug":"blog/build-a-rag-based-scientific-chatbot-with-langchain-streamlit-pubmed-part-4-put-it-all-ba7bbf706bbd","frontmatter":{"title":"Build a RAG-based scientific ChatBot with LangChain, Streamlit & PubMedâ€Šâ€”â€ŠPart 4(Put it allâ€¦","meta_title":"Build a RAG-based scientific ChatBot with LangChain, Streamlit & PubMedâ€Šâ€”â€ŠPart 4(Put it allâ€¦","description":"Hello and welcome to the last part of the series to build a scientific ChatBot with Langchain, Streamlit, and PubMed!","date":"2024-11-13T01:22:29.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MQ7XtBd9WHn5n-gAMgd6pQ.jpeg","categories":["Chatbots","Natural Language Processing","Science"],"author":"Rifx.Online","tags":["ChatBot","LangChain","Streamlit","PubMed","RAG"],"draft":false,"slug":"blog/build-a-rag-based-scientific-chatbot-with-langchain-streamlit-pubmed-part-4-put-it-all-ba7bbf706bbd"},"content":"\n\n\n\n\n\nHello and welcome to the last part of the series to build a scientific ChatBot with Langchain, Streamlit, and PubMed!\n\nIn the previous part, we built the data persistence and RAG pipeline with vectorstore. Now, it is time to put everything weâ€™ve built together, and create the chatbot UI that will use the backend functionality we built, and that our scientist will use to answer their scientific questions!\n\nAs a reminder, this is the full solution that we were building during the series:\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*NFCO_uRjlAgm0WYH.png)\n\n\n## App demo\n\n* As a teaser, letâ€™s first have a look at an illustration of what the app will look like!\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*OKEQO_2kwnV93Va4SAVWZg.gif)\n\n\n## Building\n\n\n### Overview of steps already done\n\n* If you havenâ€™t followed through the [first](https://proxy.rifx.online/https://readmedium.com/build-a-rag-based-scientific-chatbot-with-langchain-streamlit-pubmed-part-1-set-up-streamlit-37550b44b266) , the [second](https://proxy.rifx.online/https://readmedium.com/llm-aided-retrieval-of-relevant-scientific-abstracts-via-pubmed-api-using-natural-language-part2-9e10f78575e6), and the [third part](https://proxy.rifx.online/https://readmedium.com/build-a-rag-based-scientific-chatbot-with-langchain-streamlit-pubmed-part-3-create-vector-1e5e401e72e6), please do so, because we will be building on that further. At the end of the last part, we ended up with a project structure looking like this:\n\n\n```python\n.\nâ”œâ”€â”€ app\nâ”‚   â”œâ”€â”€ app.py\nâ”‚   â”œâ”€â”€ backend\nâ”‚   â”‚  â”œâ”€â”€ abstract_retrieval\nâ”‚   â”‚  â”‚   â”œâ”€â”€ interface.py\nâ”‚   â”‚  â”‚   â”œâ”€â”€ pubmed_retriever.py\nâ”‚   â”‚  â”‚   â””â”€â”€ pubmed_query_simplification.py\nâ”‚   â”‚  â”œâ”€â”€ data_repository\nâ”‚   â”‚  â”‚   â”œâ”€â”€ interface.py\nâ”‚   â”‚  â”‚   â”œâ”€â”€ local_data_store.py\nâ”‚   â”‚  â”‚   â””â”€â”€ models.py\nâ”‚   â”‚  â””â”€â”€ rag_pipeline\nâ”‚   â”‚      â”œâ”€â”€ interface.py\nâ”‚   â”‚      â”œâ”€â”€ chromadb_rag.py\nâ”‚   â”‚      â””â”€â”€ embeddings.py\nâ”‚   â”œâ”€â”€ components\nâ”‚   â”‚   â”œâ”€â”€ chat_utils.py\nâ”‚   â”‚   â”œâ”€â”€ llm.py\nâ”‚   â”‚   â””â”€â”€ prompts.py\nâ”‚   â””â”€â”€ tests\nâ”‚       â””â”€â”€ test_chat_utils.py\nâ”œâ”€â”€ assets\nâ”‚   â””â”€â”€ pubmed-screener-logo.jpg\nâ””â”€â”€ environment\n    â””â”€â”€ requirements.txt\n```\nIn this last part of the series, we will focus on the part of code base that defines our Streamlit UI â€” ***app/app.py*** and the ***app/components*** module.\n\n\n### Modify chat\\_utils.py to include RAG logic\n\n[In the first part](https://proxy.rifx.online/https://readmedium.com/build-a-rag-based-scientific-chatbot-with-langchain-streamlit-pubmed-part-1-set-up-streamlit-37550b44b266), we built a preliminary version of ***chat\\_utils.py*** that contained a simple QA chatbot implementation (without RAG). Now, we will dive in and convert this into a context\\-aware QA chatbot, that will construct answers based on user questions and retrieve relevant context (abstracts) from our vector index via similarity search.\n\nWe will use all the backend functionality [built in part three](https://proxy.rifx.online/https://readmedium.com/build-a-rag-based-scientific-chatbot-with-langchain-streamlit-pubmed-part-3-create-vector-1e5e401e72e6) for this purpose.\n\n**app/components/chat\\_utils.py**\n\n\n```python\nfrom typing import List\nimport streamlit as st\nfrom langchain_core.documents.base import Document\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_core.runnables.base import Runnable\nfrom langchain_core.runnables.utils import Output\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain.vectorstores import VectorStore\n\n\nclass ChatAgent:\n    def __init__(self, prompt: ChatPromptTemplate, llm: Runnable):\n        \"\"\"\n        Initialize the ChatAgent.\n\n        Args:\n        - prompt (ChatPromptTemplate): The chat prompt template.\n        - llm (Runnable): The language model runnable.\n        \"\"\"\n        self.history = StreamlitChatMessageHistory(key=\"chat_history\")\n        self.llm = llm\n        self.prompt = prompt\n        self.chain = self.setup_chain()\n    \n    def reset_history(self) -> None:\n        \"\"\"\n        Clean up chat history to start new chat session.\n        \"\"\"\n        self.history.clear()\n\n    def setup_chain(self) -> RunnableWithMessageHistory:\n        \"\"\"\n        Set up the chain for the ChatAgent.\n\n        Returns:\n        - RunnableWithMessageHistory: The configured chain with message history.\n        \"\"\"\n        chain = self.prompt | self.llm\n        return RunnableWithMessageHistory(\n            chain,\n            lambda session_id: self.history,\n            input_messages_key=\"question\",\n            history_messages_key=\"history\",\n        )\n\n    def display_messages(self, selected_query: str) -> None:\n        \"\"\"\n        Display messages in the chat interface.\n        If no messages are present, adds a default AI message.\n        \"\"\"\n        if len(self.history.messages) == 0:\n            self.history.add_ai_message(f\"Let's chat about your query: {selected_query}\")\n        for msg in self.history.messages:\n            st.chat_message(msg.type).write(msg.content)\n    \n    def format_retreieved_abstracts_for_prompt(self, documents: List[Document]) -> str:\n        \"\"\"\n        Format retrieved documents in a string to be passed to LLM.\n        \"\"\"\n        formatted_strings = []\n        for doc in documents:\n            formatted_str = f\"ABSTRACT TITLE: {doc.metadata['title']}, ABSTRACT CONTENT: {doc.page_content}, ABSTRACT DOI: {doc.metadata['source'] if 'source' in doc.metadata.keys() else 'DOI missing..'}\"\n            formatted_strings.append(formatted_str)\n        return \"; \".join(formatted_strings)\n    \n    def get_answer_from_llm(self, question: str, retrieved_documents: List[Document]) -> Output:\n        \"\"\"\n        Get response from LLM given user question and retrieved documents.\n        \"\"\"\n        config = {\"configurable\": {\"session_id\": \"any\"}}\n        return self.chain.invoke(\n            {\n                \"question\": question, \n                \"retrieved_abstracts\": retrieved_documents,\n            }, config\n        )\n    \n    def retrieve_documents(self, retriever: VectorStore, question: str, cut_off: int = 5) -> List[Document]:\n        \"\"\"\n        Retrieve documents using similarity search \n        cut_off parameter controls how many results are retrieved (default is 5)\n        \"\"\"\n        return retriever.similarity_search(question)[:cut_off]\n\n    def start_conversation(self, retriever: VectorStore, selected_query: str) -> None:\n        \"\"\"\n        Start a conversation in the chat interface.\n        Displays messages, prompts user for input, and handles AI response.\n        \"\"\"\n        self.display_messages(selected_query)\n        user_question = st.chat_input(placeholder=\"Ask me anything..\")\n        if user_question:\n            documents = self.retrieve_documents(retriever, user_question)\n            retrieved_abstracts = self.format_retreieved_abstracts_for_prompt(documents)\n            st.chat_message(\"human\").write(user_question)\n            response = self.get_answer_from_llm(user_question, retrieved_abstracts)\n            st.chat_message(\"ai\").write(response.content)\n```\n**What has changed:**\n\n* We added the method ***retrieve\\_documents*** that takes our vector index (retriever) as argument, and calls a method similarity\\_search on the retriever to get the most similar records to userâ€™s question from the vector index of our scientific abstracts. Note the parameter cut\\_off that specifies the number of results to be retrieved (defaults to 5\\).\n* Added method ***format\\_retreieved\\_abstracts\\_for\\_prompt***, that takes the documents retrieved via retrieve\\_documents method, and formats them for the LLM. This will come very handy when we will ask the LLM in our prompt to cite the relevant sources (article DOIs, and titles).\n* Added method ***get\\_answer\\_from\\_llm*** that serves for calling the LLM with necessary variables, to keep the client function start\\_conversation clean.\n* Modified the ***start\\_conversation*** method to include the RAG logic.\n\n\n### Create chat prompts for QA\n\n* We will be modifying the existing chat prompt to include retrieved abstracts and construct the answer based on those.\n* We will also include an additional (simple) prompt that will serve for a simple immediate answer outside of the chatbot section, so that the user gets a direct answer to his question displayed on the UI.\n\n**app/components/chat\\_prompts.py**\n\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, PromptTemplate\n\n\nchat_prompt_template = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are a knowledgeable expert chatbot in the biomedicine field.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\n            \"human\", \n            \"\"\"\n            Answer the following scientific question: {question}, \n            using the following context retrieved from scientific articles: {retrieved_abstracts}.\n\n            The user might refer to the history of your conversation. Please, use the following history of messages for the context as you see fit.\n\n            The abstracts will come formatted in the following way: ABSTRACT TITLE: <abstract title>; ABSTRACT CONTENT: <abstract content>, ABSTRACT DOI: <abstract doi> (the content inside <> will be variable).\n            In your answer, ALWAYS cite the abstract title and abstract DOI when citing a particular piece of information from that given abstract.\n\n            Your example response might look like this:\n\n            In the article (here in the brackets goes the contents of ABSTRACT_TITLE), it was discussed, that Cannabis hyperemesis syndrome (CHS) is associated with chronic, heavy cannabis use. The endocannabinoid system (ECS) plays a crucial role in the effects of cannabis on end organs and is central to the pathophysiology of CHS. (here, in the end of the cited chunk, the ABSTRACT_DOI goes)\n            \"\"\"\n        ),\n    ]\n)\n\nqa_template = PromptTemplate(\n    input_variables=['question', 'retrieved_abstracts'],\n    template=\"\"\"\n        Answer the following scientific question: {question}, \n        using the following context retrieved from scientific articles: {retrieved_abstracts}.\n\n        The abstracts will come formatted in the following way: ABSTRACT TITLE: <abstract title>; ABSTRACT CONTENT: <abstract content>, ABSTRACT DOI: <abstract doi> (the content inside <> will be variable).\n        In your answer, ALWAYS cite the abstract title and abstract DOI when citing a particular piece of information from that given abstract.\n\n        Your example response might look like this:\n\n        In the article (here in the brackets goes the contents of ABSTRACT_TITLE), it was discussed, that Cannabis hyperemesis syndrome (CHS) is associated with chronic, heavy cannabis use. The endocannabinoid system (ECS) plays a crucial role in the effects of cannabis on end organs and is central to the pathophysiology of CHS. (here, in the end of the cited chunk, the ABSTRACT_DOI goes)\n    \"\"\"\n)\n```\n* Note that the contents of the two prompts is almost identical, but the chat prompt contains a reference for the chat history with the MessagesPlaceholder and an instruction to use the chat history as the LLM sees fit during the conversation.\n\n\n### Create new file app/components/layout\\_extensions.py\n\n* This file will hold a helper function that will render a part of the layout of our app with examples of queries (cues on how to use the app) to the user. I decided to create this extensions file to not clutter our app.py file and keep it clean, since this code is quite lengthy and contains some custom styling (the app info will be displayed to the user on hover):\n\n\n```python\nimport streamlit as st\n\n\ndef render_app_info():\n    st.title(\"PubMed Screener\")\n    st.markdown(\"\"\"\n        PubMed Screener is a ChatGPT & PubMed powered insight generator from biomedical abstracts.\n    \"\"\")\n\n    # Adding custom HTML and CSS for an improved hover-over tooltip\n    st.markdown(\"\"\"\n        <style>\n        .tooltip {\n            position: relative;\n            display: inline-block;\n            border-bottom: 1px dotted black; /* Style for the hoverable text */\n        }\n\n        .tooltip .tooltiptext {\n            visibility: hidden;\n            width: 800px; /* Width to fit content */\n            background-color: #f9f9f9;\n            color: #000;\n            text-align: left;\n            border-radius: 6px;\n            padding: 15px;\n            position: absolute;\n            z-index: 1;\n            bottom: 100;\n            right: -430px; /* Positioning to the right and slightly offset */\n            opacity: 0;\n            transition: opacity 0.5s;\n            box-shadow: 0px 8px 16px 0px rgba(0,0,0,0.8); /* Adding some shadow for better visibility */\n        }\n\n        .tooltip:hover .tooltiptext {\n            visibility: visible;\n            opacity: 1;\n        }\n        </style>\n        <div class=\"tooltip\">ðŸ” Example Questions\n            <span class=\"tooltiptext\">\n                <strong>Example scientific questions:</strong>\n                <ul>\n                    <li>How can advanced imaging techniques and biomarkers be leveraged for early diagnosis and monitoring of disease progression in neurodegenerative disorders?</li>\n                    <li>What are the potential applications of stem cell technology and regenerative medicine in the treatment of neurodegenerative diseases, and what are the associated challenges?</li>\n                    <li>What are the roles of gut microbiota and the gut-brain axis in the pathogenesis of type 1 and type 2 diabetes, and how can these interactions be modulated for therapeutic benefit?</li>\n                    <li>What are the molecular mechanisms underlying the development of resistance to targeted cancer therapies, and how can these resistance mechanisms be overcome?</li>\n                </ul>\n            </span>\n        </div>\n        \"\"\", unsafe_allow_html=True)\n    \n    st.text(\"\")py\n```\n\n### Modify the app/app.py\n\n* Finally, time to put everything weâ€™ve built together and expose it as a streamlit application!\n\n\n```python\nimport streamlit as st\nfrom metapub import PubMedFetcher\nfrom components.chat_utils import ChatAgent\nfrom components.chat_prompts import chat_prompt_template, qa_template\nfrom components.llm import llm\nfrom components.layout_extensions import render_app_info\nfrom backend.abstract_retrieval.pubmed_retriever import PubMedAbstractRetriever\nfrom backend.data_repository.local_storage import LocalJSONStore\nfrom backend.rag_pipeline.chromadb_rag import ChromaDbRag\nfrom backend.rag_pipeline.embeddings import embeddings\n\n\n## Instantiate objects\npubmed_client = PubMedAbstractRetriever(PubMedFetcher())\ndata_repository = LocalJSONStore(storage_folder_path=\"backend/data\")\nrag_client = ChromaDbRag(persist_directory=\"backend/chromadb_storage\", embeddings=embeddings)\nchat_agent = ChatAgent(prompt=chat_prompt_template, llm=llm)\n\ndef main():\n    st.set_page_config(\n        page_title=\"Pubmed Abstract Screener\",\n        page_icon='ðŸ’¬',\n        layout='wide'\n    )\n\n    # Define columns - this will make layout split horizontally\n    column_logo, column_app_info, column_answer = st.columns([1, 4, 4])\n\n    # Place the logo in the first column\n    with column_logo:\n        st.image('../assets/pubmed-screener-logo.jpg')\n\n    # In the second column, place text explaining the purpose of the app and some example scientific questions that your user might ask.\n    with column_app_info:\n\n        # Runder app info including example questions as cues for the user\n        render_app_info()\n\n        # Section to enter scientific question\n        st.header(\"Enter your scientific question!\")\n        placeholder_text = \"Type your scientific question here...\"\n        scientist_question = st.text_input(\"What is your question?\", placeholder_text)\n        get_articles = st.button('Get articles & Answer')\n\n        # Processing user question, fetching data\n        with st.spinner('Fetching abstracts. This can take a while...'):\n            if get_articles:\n                if scientist_question and scientist_question != placeholder_text:\n\n                    # Get abstracts data\n                    retrieved_abstracts = pubmed_client.get_abstract_data(scientist_question)\n                    if not retrieved_abstracts:\n                        st.write('No abstracts found.')\n                    else:\n                        # Save abstarcts to storage and create vector index\n                        query_id = data_repository.save_dataset(retrieved_abstracts, scientist_question)\n                        documents = data_repository.create_document_list(retrieved_abstracts)\n                        rag_client.create_vector_index_for_user_query(documents, query_id)\n                        \n                        # Answer the user question and display the answer on the UI directly\n                        vector_index = rag_client.get_vector_index_by_user_query(query_id)\n                        retrieved_documents = chat_agent.retrieve_documents(vector_index, scientist_question)\n                        chain = qa_template | llm\n                        \n                        with column_answer:\n                            st.markdown(f\"##### Answer to your question: '{scientist_question}'\")\n                            st.write(chain.invoke({\n                                \"question\": scientist_question, \n                                \"retrieved_abstracts\": retrieved_documents,\n                            }).content)\n\n    # Beginning of the chatbot section\n    # Display list of queries to select one to have a conversation about\n    query_options = data_repository.get_list_of_queries()\n\n    if query_options:\n        st.header(\"Chat with the abstracts\")\n        selected_query = st.selectbox('Select a past query', options=list(query_options.values()), key='selected_query')\n        \n        # Initialize chat about some query from the history of user questions\n        if selected_query:\n            selected_query_id = next(key for key, val in query_options.items() if val == selected_query)\n            vector_index = rag_client.get_vector_index_by_user_query(selected_query_id)\n\n            # Clear chat history when switching query to chat about\n            if 'prev_selected_query' in st.session_state and st.session_state.prev_selected_query != selected_query:\n                chat_agent.reset_history()\n\n            st.session_state.prev_selected_query = selected_query\n\n            # Start chat session\n            chat_agent.start_conversation(vector_index, selected_query)\n\n\nif __name__ == \"__main__\":\n    main()\n```\n* The code contains the following parts:\n1. Instantiate all objects that we built in previous parts of the series â†’ ***PubMedAbstractRetriever***, ***LocalJSONStore***, ***ChromaDbRag***, and ***ChatAgent***. We will be using all those objects in our app code.\n2. Define layout to render app title, logo, and app info.\n3. Define input for the userâ€™s question, and a button to submit it. When the button is clicked, this triggers the logic to search \\& fetch PubMed articles (using ***PubMedAbstractRetriever â€”*** pubmed\\_client), save them to the local data repository (with ***LocalJSONStore â€”***data\\_repository), and create a vector index for them (with ***ChromaDbRag â€”*** rag\\_client).\n4. Answer the user question directly and show it on the UI.\n5. Display ChatBot section that will let you select a past query to chat about, in case you want to interrogate the abstracts further. After the selection of a past query, the corresponding vector index is loaded, and a chat session is initiated (***chat\\_agent.start\\_conversation(â€¦)***). Now you can chat with your abstracts!\n\n\n## Limitations\n\nI am happy you went with me through this series where we built a prototype of a scientific chatbot! It is necessary to say though, that this application is a PoC scope ONLY, and the implementation presented has its caveats that would need to be addressed before deploying in a production manner.\n\n**Naive RAG limitations and considerations**\n\n* **Retrieved content relevance**: you canâ€™t be sure if the retrieved content (the content most similar to userâ€™s question) is the most relevant piece of information. There are some techniques of advanced RAG, like *H**ypothetical Questions*****,** or ***Hierarchical indexing*** that can help with that â€” read more about those techniques and more [in this article](https://proxy.rifx.online/https://readmedium.com/advanced-rag-techniques-unlocking-the-next-level-040c205b95bc)\n* **Retrieved content cut off**: It is hard to assess whether all the relevant information was retrieved. Also, it can be challenging to fit all the context to the prompt due to the token limits of LLMs. The default cut off in our case equals to 5 abstracts (in our ChatAgent retrieve\\_documents method), which can certainly not be enough if the user asks a broad question.\n* **Limited applicability**: Sometimes, the user question can be rather of a summarization character, and a different technique than RAG would be more appropriate for this purpose. For example, you could build an agent that decides whether task is summarization / retrieval based on user question. After this evaluation, there would be a function executing different logic that performs summarization or retrieval, respectively.\n\n**Deployment architecture considerations**\n\n* **Runtime environment**: For the scope of this series, we only built our chatbot locally, not considering any architectural decisions we would need to make if we would like to deploy this app to serve some real users.\n* **Synchronous processing**: Since the data fetching can take a considerable amount of time, it would be more efficient to implement a queue\\-based asynchronous processing of user requests, and notify user when data fetching is done. Doing this in a synchronous manner can take a lot of time which can result in timeouts in many servers.\n* **Backend technologies**: The backend used in our case was ChromaDB with local storage using JSON files. For a deployed application serving users, this should be re\\-evaluated and appropriate technology selected. This can be easily achieved by building on the interface definitions in the app backend code (***RagWorkflow*** and ***UserQueryDataStore*** interfaces).\n\n**Including more scientific databases**\n\n* In the series, we focused on PubMed only, but to provide a rich context base, another scientific paper databases (i.e. Scopus) could be added. This can be easily achieved by building on the interface definitions in the app backend code (***AbstractRetriever*** interface).\n\n\n## Full codebase GitHub link\n\nFeel free to fork the repo and adapt it to your UC!\n\n\n### Link to GitHub repo pubmed\\-rag\\-screener\n\n\n## Summary\n\n* In this last part of the series to build a scientific ChatBot, we put together all the previously built pieces to create a user interface, where our scientist can formulate her/his question, get an answer based on scientific abstracts, and then chat with the abstracts for further insights.\n* The application logic is modular and enables easy extension using provided interfaces.\n* The limitations of the approach were outlined and highlighted, and some suggestions to build a production\\-grade application were included.\n\n\n> Thanks so much for going through this series with me! I hope you enjoyed building this exciting use case :)\n\n\n> Do not hesitate to get in touch with me if you want to discuss anything about dev, data, AI, or just connect â€” [reach out on LinkedIn](https://proxy.rifx.online/https://www.linkedin.com/in/sbarankova/)\n\n\n## Contents of the series\n\n* [Part 1 â€” Explaining the use case, first steps to set up the Streamlit app with chatbot interface.](https://proxy.rifx.online/https://readmedium.com/build-a-rag-based-scientific-chatbot-with-langchain-streamlit-pubmed-part-1-set-up-streamlit-37550b44b266)\n* [Part 2 â€” LLM\\-aided retrieval of relevant scientific abstracts via PubMed API using natural language](https://proxy.rifx.online/https://readmedium.com/llm-aided-retrieval-of-relevant-scientific-abstracts-via-pubmed-api-using-natural-language-part2-9e10f78575e6)\n* [Part 3 â€” Setting up the backend â€” Create vector embeddings from the retrieved scientific abstracts and store them in a vector store](https://proxy.rifx.online/https://readmedium.com/build-a-rag-based-scientific-chatbot-with-langchain-streamlit-pubmed-part-3-create-vector-1e5e401e72e6)\n* **Part 4 (this article) â€” Put it all together via RAG â€” chat with scientific abstracts**\n\n"},{"lang":"en","group":"blog","slug":"blog/build-your-talking-voice-ai-assistant-locally-memory-retaining-chatbot-with-streamlit-ui-09da4f10687c","frontmatter":{"title":"Build Your Talking Voice AI Assistant Locally: Memory-Retaining Chatbot with Streamlit UIâ€¦","meta_title":"Build Your Talking Voice AI Assistant Locally: Memory-Retaining Chatbot with Streamlit UIâ€¦","description":"This article provides a comprehensive tutorial for creating a local voice AI assistant named *Porter*, utilizing Ollamas Llama models, Streamlit for the user interface, and OpenAIs Whisper for transcription. *Porter* features memory retention for conversations, allowing it to recall past interactions and provide contextual responses. The guide includes installation instructions, code snippets, and an overview of key functionalities, emphasizing the benefits of privacy and responsiveness by operating offline. The project showcases advancements in natural language processing and the potential for further enhancements in AI capabilities.","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*5WJoI0IAKwMpEaCdSY63_A.png","categories":["Voice Assistants","Natural Language Processing","Programming/Scripting"],"author":"Rifx.Online","tags":["Porter","Llama","Streamlit","Whisper","offline"],"draft":false,"slug":"blog/build-your-talking-voice-ai-assistant-locally-memory-retaining-chatbot-with-streamlit-ui-09da4f10687c"},"content":"\n\n\n\n\n### Step\\-by\\-Step Guide to Developing Your Own Voice AI with Context Memory and Real\\-Time Chat, Powered by Llama3\\.1 \\& Llama3\\.2 Models\n\nðŸ‘¨ðŸ¾â€ðŸ’» [GitHub](https://github.com/mdmonsurali) â­ï¸ \\| ðŸ‘”[LinkedIn](https://www.linkedin.com/in/mdmonsurali/) \\|ðŸ“ [Medium](https://medium.com/@monsuralirana)\n\n\n\nThe concept of a voice\\-based personal assistant has grown beyond being a novelty â€” it has become a practical, hands\\-free solution for busy professionals, remote teams, and tech enthusiasts alike. Imagine a voice AI that can listen, respond, and even keep track of past conversations, all while running locally on your device. Enter *Porter*, a personal AI assistant designed to do just that.\n\nIn this tutorial, weâ€™ll walk you through creating *Porter*, an advanced voice assistant thatâ€™s capable of responding to voice queries, retaining context through conversation memory, and providing responses via synthesized speech. *Porter* leverages Ollamaâ€™s state\\-of\\-the\\-art Llama models, **Streamlit** for an intuitive user interface, and OpenAIâ€™s **Whisper** model for transcription. This guide will take you step\\-by\\-step from installation to final deployment on a local machine.\n\n\n## Table of Contents\n\n1. Introduction\n2. Why *Porter*?\n3. Key Features of *Porter*\n4. User Interface (UI) Overview\n5. Step\\-by\\-Step Tutorial\n6. Running Porter Locally\n7. Conclusion\n\n\n## 1\\. Introduction\n\nWith recent advancements in natural language processing, voice assistants have become increasingly capable of understanding complex queries, responding in natural language, and even retaining context across conversations. *Porter*, our AI voice assistant, is designed to leverage these advancements, providing users with a natural, responsive, and personalized assistant experience. Porter is built on Ollamaâ€™s advanced models, which provide conversational AI, and uses **Streamlit** for a straightforward, interactive user interface.\n\n**Porter** provides:\n\n* A conversational AI that can remember past exchanges.\n* A smooth interface thatâ€™s easy to navigate.\n* Customizable parameters for personalized responses.\n\n\n## 2\\. Why Porter?\n\nMost voice assistants require internet connectivity and rely on external servers, raising concerns about security, control, and response latency. *Porter*, by running locally, offers:\n\n* **Privacy**: With no need for internet access, all conversations and data stay securely on your machine.\n* **Quick Response Times**: With everything running locally, thereâ€™s minimal delay in processing and response.\n* **Memory\\-Retained Conversations**: Using LangChain, *Porter* can remember context across multiple interactions, giving it the ability to answer follow\\-up questions accurately.\n\n\n## 3\\. Key Features of Porter\n\n\n### Voice Input and Output\n\n*Porter* uses Whisper, a powerful automatic speech recognition (ASR) model, to transcribe voice input into text. It can also generate spoken responses, providing a seamless hands\\-free experience.\n\n\n### Session Memory and Conversation Context\n\nWith LangChainâ€™s **ConversationBufferMemory**, *Porter* retains past conversations, allowing for multi\\-turn conversations that feel natural. The memory enables *Porter* to reference past user queries and provide continuity.\n\n\n### History Overview and Chat History\n\n*Porter* includes a **Chat History** feature that provides an overview of all past interactions within the current session. This chat history is displayed on the UI, helping users keep track of what has been discussed.\n\n\n### Customizable Model Parameters\n\nIn *Porter*â€™s Streamlit sidebar, users can select different model versions (Llama3\\.1, Llama3\\.2\\) and adjust parameters such as **temperature** and **max tokens** to control response creativity and length.\n\n\n### Streamlit\\-Based User Interface\n\nStreamlit provides a clean, intuitive UI for *Porter*, allowing users to interact with the assistant easily. The app displays previous exchanges, model settings, and allows for easy voice input.\n\n\n## 4\\. User Interface (UI) Overview\n\nThe Streamlit UI for *Porter* is simple and user\\-friendly:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*x_oxCvi14LfcsG8H4VeXUg.png)\n\n* **Voice Input Widget**: A microphone icon lets users record their queries.\n* **Chat Display**: Displays user messages and *Porter*â€™s responses, including timestamps and response times.\n* **Settings Sidebar**: Customize *Porter* with model options, temperature, and max tokens.\n* **History Overview**: Review conversation history in the chat window, making it easy to follow previous exchanges.\n\n\n## 5\\. Step\\-by\\-Step Tutorial\n\nLetâ€™s break down the code to see how to implement Porter. Weâ€™ll use two main files: `app.py` (for the Streamlit app) and `voicebot.py` (for backend logic).\n\n\n### Prerequisites:\n\n* Python 3\\.7\\+\n* locally conda environment\n* Streamlit for the UI\n* Ollama for model inference\n* LangChain is used to manage the interaction between the models and memory.\n\n\n### Step 1: Install Necessary Packages\n\nInstall necessary libraries and tools:\n\n\n```python\n!pip install langchain==0.0.318\n!pip install langchain-ollama \n!pip install langchain-community==0.0.3 \n!pip install ollama==0.0.8\n!pip install streamlit==1.25.0\n!pip install pathlib==1.0.1\n!pip install audio-recorder-streamlit==0.0.10\n!pip install torch==2.4.1\n!pip install transformer==4.44.2\n```\n\n> **I have the LLaMA 3\\.1 and 3\\.2 models set up through Ollama. If you donâ€™t have Ollama or the LLaMA models on your local machine, please follow the instructions at the link below to install them. The link is exclusive to Llama 3\\.2, but you can pull Llama 3\\.1 by simply running `\"ollama pull llama3.1\"`.**\n\n\n> **Iâ€™ve used the Piper TTS model for text\\-to\\-speech. Itâ€™s lightweight, 10x faster, works in real\\-time, operates offline, and produces a human\\-like voice.**\n\n\n### Step 2: Setting up the Streamlit App\n\n\n```python\nimport streamlit as st\nimport time\nfrom audio_recorder_streamlit import audio_recorder\nfrom voicebot import initialize_chat, text_to_speech, transcribe_audio\n\nst.title(\"Porter - Your Personal Voice AI Assistant\")\n\n## Initialize session state variables\nif \"messages\" not in st.session_state:\n    st.session_state.messages = []\nif \"audio_bytes\" not in st.session_state:\n    st.session_state.audio_bytes = None\n\n## Sidebar Settings\nwith st.sidebar:\n    logo_path = \"/path/to/logo.png\"\n    st.image(logo_path, caption=\"AI Enterprise\", use_column_width=True)\n    st.subheader(\"Inference Settings\")\n    st.session_state.model = st.selectbox(\"Model\", [\"llama3.1\", \"llama3.2:latest\"], index=0)\n    st.session_state.temperature = st.slider(\"Temperature\", 0.0, 1.0, 0.0, 0.05)\n    st.session_state.max_tokens = st.slider(\"Max Tokens\", 100, 5000, 500, 100)\n\n## Initialize chat model\nif \"chain\" not in st.session_state:\n    st.session_state.chain = initialize_chat()\n```\nIn this section:\n\n1. **Session State Variables**: Store message history and audio bytes.\n2. **Sidebar Controls**: Provide UI controls to customize the model, temperature, and token length.\n3. **Chat Model Initialization**: This loads the chat model for use in the app.\n\n\n### Step 3: Implementing the Chat Functionality\n\n\n```python\n## Display chat history\nfor message in st.session_state.messages:\n    with st.chat_message(message[\"role\"]):\n        st.markdown(message[\"content\"])\n\n## Record voice input\nfooter_container = st.container()\nwith footer_container:\n    st.session_state.audio_bytes = audio_recorder(text=\"Record a question\", icon_size=\"lg\")\n\nif st.session_state.audio_bytes:\n    transcript = transcribe_audio(st.session_state.audio_bytes)\n    if transcript:\n        st.session_state.messages.append({\"role\": \"user\", \"content\": transcript})\n        \n        # Display user input in chat\n        with st.chat_message(\"user\"):\n            st.markdown(transcript)\n\n        # Get response from model\n        with st.chat_message(\"assistant\"):\n            start_time = time.time()\n            with st.spinner(\"Porter is thinking...\"):\n                response = st.session_state.chain.run(transcript)\n            end_time = time.time()\n\n            response_time_str = f\"Response time: {end_time - start_time:.2f} seconds\"\n            st.markdown(response)\n            text_to_speech(response)\n            st.markdown(f\"_{response_time_str}_\")\n\n        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response, \"response_time\": response_time_str})\n```\nHere:\n\n1. **Display Previous Messages**: The chat window shows the conversation history.\n2. **Voice Input \\& Transcription**: Records and transcribes audio input into text, adding it to the chat.\n3. **Assistant Response**: Sends user input to the model, retrieves a response, and converts it to audio for playback.\n\n\n### Step 4: Implementing the Backend (voicebot.py)\n\nIn `voicebot.py`, the main components are set up to initialize Porterâ€™s conversation model and handle text\\-to\\-speech and transcription:\n\n\n```python\nimport os\nimport subprocess\nfrom langchain.memory.buffer import ConversationBufferMemory\nfrom langchain.memory.chat_message_histories.file import FileChatMessageHistory\nfrom langchain_community.chat_models.ollama import ChatOllama\nfrom langchain.chains.llm import LLMChain\nfrom transformers import pipeline\nimport torch\n\ndef initialize_chat():\n    def get_llm():\n        return ChatOllama(\n            model=st.session_state.model,\n            temperature=st.session_state.temperature,\n            max_tokens=st.session_state.max_tokens,\n        )\n\n    from langchain.prompts import (\n        HumanMessagePromptTemplate,\n        ChatPromptTemplate,\n        MessagesPlaceholder,\n        SystemMessagePromptTemplate,\n    )\n\n    def get_chat_prompt_template():\n        return ChatPromptTemplate(\n            input_variables=[\"content\", \"messages\"],\n            messages=[\n                SystemMessagePromptTemplate.from_template(\n                    \"You're a Personal Assistant, and your name is Porter.\"\n                ),\n                MessagesPlaceholder(variable_name=\"messages\"),\n                HumanMessagePromptTemplate.from_template(\"{content}\"),\n            ],\n        )\n\n    def get_memory():\n        return ConversationBufferMemory(\n            memory_key=\"messages\",\n            chat_memory=FileChatMessageHistory(file_path=\"memory.json\"),\n            return_messages=True,\n            input_key=\"content\",\n        )\n\n    llm = get_llm()\n    prompt = get_chat_prompt_template()\n    return LLMChain(llm=llm, prompt=prompt, memory=get_memory())\n\n## Text-to-speech\ndef text_to_speech(text):\n    subprocess.call(f'echo \"{text}\" | piper --model en_US-amy-medium --output_file output.wav', shell=True)\n    os.system(\"aplay output.wav\")\n\n## Speech recognition\npipe = pipeline(\"automatic-speech-recognition\", \"openai/whisper-large-v3-turbo\", torch_dtype=torch.float16, device=\"cuda:0\")\n\ndef transcribe_audio(audio_bytes):\n    webm_file_path = \"temp_audio.mp3\"\n    with open(webm_file_path, \"wb\") as f:\n        f.write(audio_bytes)\n    \n    transcript = pipe(webm_file_path)['text'].strip()\n    os.remove(webm_file_path)\n    return transcript\n```\nThis section:\n\n1. **Model Setup**: Configures the chat model and prompt template.\n2. **Text\\-to\\-Speech**: Converts model responses to audio.\n3. **Speech\\-to\\-Text**: Uses Whisper to transcribe recorded audio input.\n\n\n### Step 5: Deploying Porter\n\nOnce youâ€™ve completed the setup, you can launch your app using Streamlit. To run the app, navigate to your project folder and run the following command in your terminal:\n\n\n```python\nstreamlit run apps.py\n```\nAfter the app starts, youâ€™ll see the following message in your terminal:\n\n\n```python\n  You can now view your Streamlit app in your browser.\n\n  Local URL: http://localhost:8501\n  Network URL: http://172.30.254.103:8501\n```\nYou can access Porter by opening the **Local URL** (`http://localhost:8501`) in your browser if youâ€™re on the same machine. Alternatively, if you want to access it from another device on the same network, use the **Network URL** (`http://172.30.254.103:8501`).\n\nYouâ€™ll now have a fully functional personal AI assistant!\n\n\n> **â€œConversing with Porter: How It Remembers and Recalls Past Interactionsâ€**\n\nPorter isnâ€™t just an AI that responds to questions at the moment â€” itâ€™s designed to remember past conversations. Thanks to its memory system, it can recall previous chats, providing contextually aware responses that make interactions feel more personalized and fluid. Whether youâ€™re revisiting an old topic or issuing a command Porter has handled before, it intelligently recalls past exchanges, allowing for a seamless, coherent dialogue that feels like an ongoing conversation rather than starting fresh every time.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*dKn8HbZ7YhHHzml-OtBY4w.png)\n\n\n> ***GitHub Code:***\n\n\n> **Leave your feedback, comments, and ðŸ‘ ðŸ‘ Clap for the story !! ðŸ‘ðŸ‘**\n\n\n## Conclusion\n\nThe creation of ***Porter*** demonstrates the exciting potential of personal AI assistants that prioritize **privacy** and **responsiveness** by operating locally. By integrating LangChain for conversation memory, Ollamaâ€™s high\\-performance Llama models for natural language processing, and Whisper for speech recognition, *Porter* showcases how these advanced tools can be combined to create a robust, intuitive voice assistant. This project not only emphasizes the accessibility of modern AI but also highlights the importance of keeping user data safe and interaction fast â€” two areas where local solutions excel.\n\nWith Porter's flexible architecture, thereâ€™s ample room to expand its capabilities. Developers could integrate other local NLP models or add customized workflows for different use cases, like customer support, educational tutoring, or technical troubleshooting. Moreover, as new language models and voice\\-processing techniques emerge, *Porter* can be updated to provide even more nuanced and contextually aware responses.\n\n\n## References\n\n\\[1] Llama 3\\.2: The Next Generation of Lightweight, Instruction\\-Tuned Language Models: A Hands\\-On Tutorial, 2024\\. Available: [https://readmedium.com/llama\\-3\\-2\\-the\\-next\\-generation\\-of\\-lightweight\\-instruction\\-tuned\\-language\\-models\\-a\\-hands\\-on\\-9bca07c8af1d](https://readmedium.com/llama-3-2-the-next-generation-of-lightweight-instruction-tuned-language-models-a-hands-on-9bca07c8af1d)\n\n\\[2] Hugging Face, *Transformers Documentation: Using LLaMA 3\\.2 Vision Models*, Hugging Face, 2024\\. Available: <https://huggingface.co/blog/llama32>\n\n\\[3] Build a basic LLM chat app. Available: [https://docs.streamlit.io/develop/tutorials/llms/build\\-conversational\\-apps](https://docs.streamlit.io/develop/tutorials/llms/build-conversational-apps)\n\nHappy coding! ðŸŽ‰\n\nðŸ‘¨ðŸ¾â€ðŸ’» [GitHub](https://github.com/mdmonsurali) â­ï¸ \\| ðŸ‘”[LinkedIn](https://www.linkedin.com/in/mdmonsurali/) \\|ðŸ“ [Medium](https://medium.com/@monsuralirana)\n\nThank you for your time in reading this post!\n\nMake sure to leave your feedback and comments. ðŸ‘ Clap for the story and follow for stories. See you in the next blog; stay tuned ðŸ“¢\n\n\n## Enjoyed this article? Check out more of my work:\n\n* **Building a Custom Documents Agent with Elasticsearch, Ollama, LLaMA 3\\.1, and LangChain:** Explore how to set up a personalized document retrieval agent using LLaMA 3\\.1 and Ollama for seamless information retrieval. [Read the full tutorial here](https://readmedium.com/building-a-custom-documents-agent-with-elasticsearch-ollama-llama-3-1-and-langchain-926b28047e1d).\n* **Building Your Personal AI Assistant with Memory Using Ollamaâ€™s LLaMA3\\.1, LLaMA3\\.2 Models, Streamlit UI, and Locally:** Discover how to develop an AI assistant that remembers past interactions using the latest LLaMA models and a user\\-friendly Streamlit interface. [Read the full tutorial here.](https://readmedium.com/building-porter-your-personal-ai-assistant-with-memory-using-ollamas-llama3-1-efb32b80c129)\n* **OpenAI Swarm: A Lightweight Framework for Multi\\-Agent Orchestration:** Dive into a new framework designed for managing multiple AI agents efficiently, enhancing your AI project management. [Read the full tutorial here.](https://readmedium.com/openai-swarm-a-lightweight-framework-for-multi-agent-orchestration-b4a83a1a1e37)\n* **How to Use Molmo\\-7B for Multimodal AI: Extract Text and Images with an Open\\-Source Vision\\-Language Model:** Learn how to harness the power of the Molmo\\-7B model for extracting both text and images, revolutionizing your approach to multimodal AI. [Read the full tutorial here.](https://readmedium.com/how-to-use-molmo-7b-for-multimodal-ai-extract-text-and-images-with-an-open-source-vision-language-8a31939a2960)\n* **Meta Spirit LM: A Complete Guide to Multimodal AI for Text and Speech Generation:** Explore the capabilities of Meta Spirit LM in generating text and speech, and how it can be applied in various AI applications. [Read the full tutorial here.](https://readmedium.com/meta-spirit-lm-a-complete-guide-to-multimodal-ai-for-text-and-speech-generation-ed0af74bc950)\n* **Supercharge Text\\-to\\-Speech with Piper TTS:** Find out how to achieve 10x faster, real\\-time, offline voice synthesis with human\\-like accuracy in this hands\\-on *Google Colab tutorial*. [Transform your text into lifelike speech here.](https://readmedium.com/unleashing-the-power-of-piper-tts-transforming-text-to-speech-10x-faster-with-ai-human-like-voice-eadf2065d66d)\n\n"},{"lang":"en","group":"blog","slug":"blog/building-a-local-ai-powered-news-aggregator-with-ollama-swarm-and-duckduckgo-95aaf8b3ee41","frontmatter":{"title":"Building a Local AI-Powered News Aggregator with Ollama, Swarm, and DuckDuckGo","meta_title":"Building a Local AI-Powered News Aggregator with Ollama, Swarm, and DuckDuckGo","description":"No subtitle provided","date":"2024-10-24T17:47:43.000Z","image":"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*OHMOTk_WYGOxWHBsKqdpNQ.jpeg","categories":["Programming","Generative AI","Technology/Web"],"author":"Rifx.Online","tags":["Llama","Swarm","DuckDuckGo","News","Aggregator"],"draft":false,"slug":"blog/building-a-local-ai-powered-news-aggregator-with-ollama-swarm-and-duckduckgo-95aaf8b3ee41"},"content":"\n\n# Building a Local AI-Powered News Aggregator with Ollama, Swarm, and DuckDuckGo\n\n\n\nIn todayâ€™s fast-paced world, staying up-to-date with the latest news in specific fields can be challenging. What if we could leverage the power of Generative AI and Agents to create a personalized news aggregator that runs entirely on our local machine? In this article, weâ€™ll explore how to build such a system using **Ollama**â€™s Llama 3.2 model, **Swarm** for agent orchestration, and **DuckDuckGo** for web searches.\n\n\n# The Power of Local AI\n\nWith the rise of large language models, we now have the ability to run sophisticated AI systems on our personal computers. This opens up a world of possibilities for creating customized tools tailored to our specific needs. Our news aggregator is a perfect example of this potential.\n\n\n# Components of Our System\n\n1. **Ollama with Llama 3.2**: This serves as the brain of our system, powering our AI agents.\n2. **Swarm**: An agent orchestration framework that allows us to create and manage multiple AI agents.\n3. **DuckDuckGo Search**: Provides up-to-date web search results without tracking user data.\n\n\n# How It Works\n\nOur news aggregator consists of two main AI agents:\n\n1. **News Assistant**: Fetches the latest news articles on a given topic using DuckDuckGo search.\n2. **Editor Assistant**: Reviews and refines the collected news for final presentation.\n\nLetâ€™s break down the workflow:\n\n\n# 1. Setting Up the Environment\n\n\n```python\nollama pull llama3.2\n\nexport OPENAI_MODEL_NAME=llama3.2\nexport OPENAI_BASE_URL=http://localhost:11434/v1\nexport OPENAI_API_KEY=any\n\npip install git+https://github.com/openai/swarm.git duckduckgo-search\n```\nWe start by importing the necessary libraries and initializing our Swarm client:\n\n\n```python\nfrom duckduckgo_search import DDGS\nfrom swarm import Swarm, Agent\nfrom datetime import datetime\n\ncurrent_date = datetime.now().strftime(\"%Y-%m\")\nclient = Swarm()\n```\n\n# 2. Creating the News Search Function\n\nWe define a function to search for news using DuckDuckGo:\n\n\n```python\npythondef get_news_articles(topic):\n  ddg_api = DDGS()\n  results = ddg_api.text(f\"{topic} {current_date}\", max_results=5)\n  if results:\n      news_results = \"\\n\\n\".join([f\"Title: {result['title']}\\nURL: {result['href']}\\nDescription: {result['body']}\" for result in results])\n      return news_results\n  else:\n      return f\"Could not find news results for {topic}.\"\n```\n\n# 3. Defining Our AI Agents\n\nWe create two agents using Ollamaâ€™s Llama 3.2 model:\n\n\n```python\nnews_agent = Agent(\n  model=\"llama3.2\",\n  name=\"News Assistant\",\n  instructions=\"You provide the latest news articles for a given topic using DuckDuckGo search.\",\n  functions=[get_news_articles],\n)\n\neditor_agent = Agent(\n  model=\"llama3.2\",\n  name=\"Editor Assistant\",\n  instructions=\"You review and finalise the news article for publishing.\",\n)\n```\n\n# 4. Orchestrating the Workflow\n\nWe define a function to run our news aggregation workflow:\n\n\n```python\ndef run_news_workflow(topic):\n  # Fetch news\n  news_response = client.run(\n      agent=news_agent,\n      messages=[{\"role\": \"user\", \"content\": f\"Get me the news about {topic} on {current_date}\"}],\n  )\n  raw_news = news_response.messages[-1][\"content\"]\n  \n  # Pass news to editor for final review\n  edited_news_response = client.run(\n      agent=editor_agent,\n      messages=[{\"role\": \"system\", \"content\": raw_news}],\n  )\n  print(f\"{edited_news_response.messages[-1]['content']}\")\n```\n\n# 5. Running the System\n\nFinally, we can run our news aggregator for any topic of interest:\n\n\n```python\nrun_news_workflow(\"AI in Drug Discovery\")\n```\n\n# Complete Code : app.py\n\n\n```python\nfrom duckduckgo_search import DDGS\nfrom swarm import Swarm, Agent\nfrom datetime import datetime\n\ncurrent_date = datetime.now().strftime(\"%Y-%m\")\n\n# Initialize Swarm client\nclient = Swarm()\n\n# 1. Create Internet Search Tool\n\ndef get_news_articles(topic):\n    print(f\"Running DuckDuckGo news search for {topic}...\")\n    \n    # DuckDuckGo search\n    ddg_api = DDGS()\n    results = ddg_api.text(f\"{topic} {current_date}\", max_results=5)\n    if results:\n        news_results = \"\\n\\n\".join([f\"Title: {result['title']}\\nURL: {result['href']}\\nDescription: {result['body']}\" for result in results])\n        return news_results\n    else:\n        return f\"Could not find news results for {topic}.\"\n    \n# 2. Create AI Agents\n\ndef transfer_to_editor_assistant(raw_news):\n    print(\"Passing articles to Editor Assistant...\")\n    return editor_agent.run({\"role\": \"system\", \"content\": raw_news})\n\n# News Agent to fetch news\nnews_agent = Agent(\n    model=\"llama3.2\",\n    name=\"News Assistant\",\n    instructions=\"You provide the latest news articles for a given topic using DuckDuckGo search.\",\n    functions=[get_news_articles],\n)\n\n# Editor Agent to edit news\neditor_agent = Agent(\n    model=\"llama3.2\",\n    name=\"Editor Assistant\",\n    instructions=\"You review and finalise the news article for publishing.\",\n)\n\n# 3. Create workflow\n\ndef run_news_workflow(topic):\n    print(\"Running news Agent workflow...\")\n    \n    # Step 1: Fetch news\n    news_response = client.run(\n        agent=news_agent,\n        messages=[{\"role\": \"user\", \"content\": f\"Get me the news about {topic} on {current_date}\"}],\n    )\n    raw_news = news_response.messages[-1][\"content\"]\n    print(f\"Fetched news: {raw_news}\")\n    \n    # Step 2: Pass news to editor for final review\n    edited_news_response = client.run(\n        agent=editor_agent,\n        messages=[{\"role\": \"system\", \"content\": raw_news}],\n    )\n    print(f\"{edited_news_response.messages[-1]['content']}\")\n\n\n# Example of running the news workflow for a given topic\nrun_news_workflow(\"AI in Drug Discovery\")\n```\n\n# Sample Output\n\n\n```python\nRunning news Agent workflow...\nRunning DuckDuckGo news search for AI in Drug Discovery...\nFetched news: Here's a formatted answer based on the news articles:\n\n**AI in Drug Discovery: A Revolutionary Shift**\n\nThe role of Artificial Intelligence (AI) in drug discovery has marked a revolutionary shift in the pharmaceutical landscape. AI leverages sophisticated algorithms for autonomous decision-making from data analysis, augmenting human capabilities rather than replacing them.\n\n**Challenges and Limitations**\n\nDespite the promising advancements, challenges and limitations have been identified in the field. The paper \"The Role of AI in Drug Discovery\" addresses these issues, highlighting the need for high-quality data, addressing ethical concerns, and recognizing the limitations of AI-based approaches.\n\n**Applications of AI in Drug Discovery**\n\nAI has the potential to play a critical role in drug discovery, design, and studying drug-drug interactions.Applications of AI in drug discovery include:\n\n* Polypharmacology: AI can predict the likelihood of a compound's effectiveness against multiple diseases.\n* Chemical synthesis: AI can optimize chemical synthesis processes for faster and more efficient production.\n* Drug repurposing: AI can identify new uses for existing drugs.\n* Predicting drug properties: AI can predict the efficacy, toxicity, and physicochemical characteristics of compounds.\n\n**The Future of AI in Drug Discovery**\n\nAs AI continues to evolve, it is expected to significantly impact the pharmaceutical industry. The successful application of AI will depend on the availability of high-quality data, addressing ethical concerns, and recognizing the limitations of AI-based approaches.\n```\n\n# The Benefits of Local AI News Aggregation\n\n* **Privacy**: All processing happens on your local machine, ensuring your data stays with you.\n* **Customization**: You can easily modify the agentsâ€™ instructions or add new agents to suit your specific needs.\n* **Up-to-date Information**: By using DuckDuckGo search, you always get the latest news on your chosen topic.\n* **AI-powered Curation**: The Editor Assistant helps refine and organize the collected news, providing a more polished final output.\n\n\n# Conclusion\n\nThis local AI-powered news aggregator demonstrates the potential of combining large language models with web search capabilities. By leveraging Ollamaâ€™s Llama 3.2 model, Swarm for agent orchestration, and DuckDuckGo for search, weâ€™ve created a powerful tool that can keep us informed on any topic of interest, all while maintaining our privacy and running entirely on our local machine.\n\nAs AI continues to evolve, the possibilities for creating personalized, AI-driven tools will only expand. This news aggregator is just the beginning â€” imagine what other innovative applications you could build using these technologies!\n\n\n# Reference :\n\nSwarm Github : <https://github.com/openai/swarm>\n\nIf you found this article informative and valuable, Iâ€™d greatly appreciate your support:\n\n* Give it a few claps ðŸ‘ on Medium to help others discover this content (did you know you can clap up to 50 times?). Your claps will help spread the knowledge to more readers.\n- Share it with your network of AI enthusiasts and professionals.\n- Connect with me on LinkedIn: <https://www.linkedin.com/in/manjunath-janardhan-54a5537/>\n\n\n\n\n\n"},{"lang":"en","group":"blog","slug":"blog/building-a-reliable-text-classification-pipeline-with-llms-a-step-by-step-guide-87dc73213605","frontmatter":{"title":"Building a Reliable Text Classification Pipeline with LLMs: A Step-by-Step Guide","meta_title":"Building a Reliable Text Classification Pipeline with LLMs: A Step-by-Step Guide","description":"This tutorial outlines the development of a reliable text classification pipeline using large language models (LLMs). It discusses three key techniques: constrained generation, few-shot prompting, and dynamic example selection. Constrained generation ensures LLM outputs match predefined classes, reducing post-processing needs. Few-shot prompting enhances accuracy by providing example outputs, while dynamic selection retrieves relevant examples based on query similarity, significantly improving classification accuracy to 88.6%. The article emphasizes the adaptability of LLMs for effective text classification without extensive training or data collection.","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Cmk7IkUnY-SIxhVF","categories":["Natural Language Processing","Machine Learning","Generative AI"],"author":"Rifx.Online","tags":["constrained","generation","prompting","selection","classification"],"draft":false,"slug":"blog/building-a-reliable-text-classification-pipeline-with-llms-a-step-by-step-guide-87dc73213605"},"content":"\n\n\n\n\n### Overcoming common challenges in LLM\\-based text classification\n\n\n\nIn this step\\-by\\-step tutorial, weâ€™ll walk through how to use large language models (LLMs) to build a text classification pipeline that is accurate and dependable. LLMs are powerful, generalist models that have demonstrated remarkable capabilities across various natural language processing tasks, and theyâ€™re increasingly replacing specialist models in many AI applications. However, using LLMs for classification can be tricky if not approached carefully.\n\nA common issue when applying LLMs for classification is that the model might not respond with the expected output or format, leading to additional post\\-processing that can be complex and time\\-intensive. In this post, weâ€™ll cover practical tips and techniques to address these challenges. Each of these strategies is simple to implement but can significantly improve both the accuracy and usability of LLMs as text classifiers. Letâ€™s dive in to make your LLM text classification system both efficient and reliable.\n\n\n## Main Ideas\n\nIn this tutorial, weâ€™ll explore three key techniques that can make LLMs far more effective and efficient as text classifiers. **We wonâ€™t go into the fine\\-tuning option for this tutorial**, but you can see some of my other posts in you are interested by this technique:\n\nThe first technique is *constrained generation*. This involves setting specific constraints that guide the LLM to generate tokens following a designated schema, which helps ensure the output matches the expected format. By applying these constraints, we can reduce the need for complex post\\-processing to obtain class predictions in the correct format.\n\nThe second technique weâ€™ll examine is *few\\-shot prompting*. Few\\-shot prompting works by providing the LLM with a few example outputs before it attempts to classify new data. Because LLMs are known to be strong in\\-context learners, they can identify patterns from these examples and produce outputs that closely resemble them. This approach allows us to improve the accuracy of predictions by showing the LLM the types of responses it should generate.\n\nFinally, weâ€™ll introduce *dynamic example selection* for few\\-shot prompting. Similar to retrieval\\-augmented generation but designed for classification tasks, this approach dynamically selects examples based on similarity to the new input, using a nearest\\-neighbor technique. This way, the LLM is presented with the most relevant input\\-output pairs before it generates the final classification, leading to more precise predictions.\n\nEach of these techniques will be explained in detail, with code examples based on the LangChain framework to simplify implementation. Youâ€™ll be able to incorporate these methods directly into your NLP toolkit or customize them to suit your specific needs for a reliable and accurate text classification pipeline.\n\n\n## Why use LLMs for classification\n\nBefore we get started, letâ€™s take a moment to consider why you might choose to use LLMs for text classification over a custom, specialized model.\n\nOne major advantage of using LLMs is their proficiency in zero\\-shot and few\\-shot predictions. Even with minimal data, LLMs often produce reasonable results, making them an excellent choice when labeled data is scarce. Additionally, as generalist models, LLMs have vast knowledge about the world, effectively memorizing information from a wide range of sources. This means they can sometimes handle unexpected inputs and still produce accurate predictions.\n\nAnother significant benefit is the convenience of accessing LLMs as a service. Many LLMs are now offered through cloud platforms, which means you donâ€™t need to manage any infrastructure yourself. You simply pay for what you use, giving you the flexibility to scale as needed without investing in hardware or managing GPU resources. This can be a huge asset for AI applications, as it reduces upfront costs and eliminates the need to maintain complex machine learning infrastructure.\n\nHowever, there are also some potential drawbacks to consider. One is latency: while custom, smaller classification models often respond in just a few tens of milliseconds, LLMs typically have higher latency, ranging from a few hundred milliseconds to several seconds depending on their size. This delay might be a disadvantage for applications that require real\\-time processing.\n\nData privacy is another concern. If you need to keep all data within your own infrastructure for compliance or security reasons, using an LLM service might not be the best option. You would either need to host an LLM internally â€” which can be costly â€” or find an alternative that keeps data in\\-house.\n\nAnother limitation is the reliance on the LLM service provider. Using an LLM as a service means youâ€™re subject to its rate limits, latencies, and potential downtimes, over which you have little control. Any issue on the providerâ€™s end could impact your ability to classify text reliably and promptly, which may be a drawback for applications requiring high reliability.\n\nWith these pros and cons in mind, you can evaluate whether using LLMs as classifiers suits your specific requirements. In any case, LLMs are a powerful tool to have in your data science toolkit, allowing you to quickly set up an AI service and get started on building impactful applications.\n\n\n## Idea 1: Constrained Output for classification\n\nNow that weâ€™ve covered the context, letâ€™s dive into the technical part of the tutorial. As mentioned earlier, our first technique is to implement **constrained generation** to ensure that the LLM only outputs valid class labels. By constraining the output to a predefined set of class names, we eliminate the need to parse or clean up free\\-form responses, which reduces the likelihood of errors and improves the reliability of the classification pipeline.\n\nTo achieve this, weâ€™ll use the LangChain OpenAI client wrapper, but works with any OpenAI\\-compatible model *(We use [NebiusAI](https://studio.nebius.ai/) for these experiments)*. This wrapper will allow us to send structured queries to the LLM, following a specific schema that weâ€™ll define.\n\n\n### Step 1: Define the Output Schema\n\nWe start by defining the schema for the output, which will consist of a single category field. This field will use \\`Literal\\` types, listing each possible class name as a string. By doing this, we ensure that the LLMâ€™s output is strictly one of these valid classes, which we can directly use as the modelâ€™s prediction.\n\nThe schema definition is implemented with \\`pydantic\\` as follows:\n\n\n```python\nfrom typing import Literal\nfrom pydantic import BaseModel\n\ndef generate_classification_model(list_classes: list[str]):\n    assert list_classes  # Ensure the list of classes is not empty\n\n    class ClassificationOutput(BaseModel):\n        category: Literal[tuple(list_classes)]\n\n    return ClassificationOutput\n\n## Example usage\nif __name__ == \"__main__\":\n    Categories = generate_classification_model([\"Yes\", \"No\"])\n    categories = Categories(category=\"Yes\")\n    print(categories)\n```\nIn this example, we create a Pydantic model called \\`ClassificationOutput\\` with a \\`category\\` field restricted to a list of literal values, such as â€œYesâ€ and â€œNo.â€ This setup allows us to validate the LLMâ€™s output, ensuring it is one of the predefined class names.\n\n\n### Step 2: Construct and Send Messages\n\nNext, we prepare a series of messages to send to the LLM. The first message is a system prompt that sets the context by describing the task (classification) and listing the possible output classes. This guides the LLM to produce outputs matching the desired schema. The second message contains the actual text we want the LLM to classify.\n\nUsing the LangChain client wrapper, we can configure our LLM with the following settings:\n\n\n```python\nimport os\nfrom typing import Literal\n\nfrom dotenv import load_dotenv\nfrom langchain_core.messages import HumanMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom pydantic import BaseModel\n\nload_dotenv()\n\n\nclass ClassificationOutput(BaseModel):\n    category: Literal[\"news\", \"clickbait\"]\n\n\nllm_client = ChatOpenAI(\n    openai_api_base=os.environ.get(\"LLM_BASE_URL\"),\n    model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n    openai_api_key=os.environ.get(\"LLM_API_KEY\"),\n    temperature=0,\n    max_retries=2,\n)\n\nconstrained_llm = llm_client.with_structured_output(ClassificationOutput)\n\nmessages = [\n    SystemMessage(\n        content=\"Classify the following text into one of the predefined categories: news or clickbait\"\n    ),\n    HumanMessage(content=\"You won't believe what happened next!\"),\n]\nprediction = constrained_llm.invoke(messages)\n\nprint(prediction)\n\n## Gives category='clickbait'\n```\nUsing this approach, the LLMâ€™s output will match our predefined classes, making it directly usable as a classification result without further processing.\n\n\n### Step 3: Evaluation\n\nTo assess the modelâ€™s performance, we ran it on the [20 Newsgroups dataset](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html) (CC BY 4\\.0\\), where it achieved an accuracy of **76\\.3%**. This setup demonstrates the effectiveness of constrained generation in improving classification accuracy and reducing the need for additional processing steps.\n\n\n## Idea 2: Few\\-shot prompting\n\nThe second technique is *few\\-shot prompting*, where we include a few example input\\-output pairs in the prompt to guide the LLM. This approach leverages the in\\-context learning abilities of LLMs, which allows them to pick up on patterns from the examples provided, often resulting in improved classification accuracy. Here, weâ€™ll implement few\\-shot prompting by adding some sample classifications directly in the prompt to enhance the modelâ€™s output quality.\n\nLetâ€™s look into the code:\n\n\n```python\nimport os\nfrom typing import Literal\n\nfrom dotenv import load_dotenv\nfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom pydantic import BaseModel\n\nload_dotenv()\n\n\nclass ClassificationOutput(BaseModel):\n    category: Literal[\"news\", \"clickbait\"]\n\n\nllm_client = ChatOpenAI(\n    openai_api_base=os.environ.get(\"LLM_BASE_URL\"),\n    model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n    openai_api_key=os.environ.get(\"LLM_API_KEY\"),\n    temperature=0,\n    max_retries=10,\n)\n\nconstrained_llm = llm_client.with_structured_output(ClassificationOutput)\n\nmessages = [\n    SystemMessage(\n        content=\"Classify the following text into one of the predefined categories: news or clickbait\"\n    ),\n    HumanMessage(content=\"The Shocking Truth Behind a Popular Wellness Trend\"),\n    AIMessage(content=\"clickbait\"),\n    HumanMessage(content=\"UK farmers call for weedkiller ban over Parkinsonâ€™s fears\"),\n    AIMessage(content=\"news\"),\n    HumanMessage(content=\"You won't believe what happened next!\"),\n]\nprediction = constrained_llm.invoke(messages)\n\nprint(prediction)\n\n## Gives category='clickbait'\n```\nIn this setup, we construct a conversation history with both *HumanMessage* and *AIMessage* types to simulate examples of how we expect the LLM to classify text. By demonstrating the classification style and format we want â€” such as categorizing â€œThe Shocking Truth Behind a Popular Wellness Trendâ€ as â€œclickbaitâ€ and â€œUK farmers call for weedkiller ban over Parkinsonâ€™s fearsâ€ as â€œnewsâ€ â€” we set clear expectations for the model. When the final classification request, â€œYou wonâ€™t believe what happened next!â€ is sent, the LLM can leverage these examples to determine the appropriate response.\n\nAfter testing this few\\-shot approach, we observed an accuracy of **76\\.6%**, a slight improvement over our constrained generation method. However, since the examples were selected randomly, this might not fully demonstrate the potential of few\\-shot prompting. Carefully choosing or curating the examples to match the input data more closely could yield even better results. In the next part of this tutorial, weâ€™ll look at a more advanced technique: dynamically selecting examples based on similarity, which could further improve accuracy.\n\n\n## Idea 3: Dynamic Example selection\n\nOur third technique for improving classification accuracy with LLMs is dynamically selecting relevant examples based on the text in the query. Instead of using a static few\\-shot prompt, we perform a similarity search for each query using ChromaDB to identify its nearest neighbors from a labeled training set. By selecting examples that are contextually similar to the input text, we can provide the LLM with highly relevant information, increasing the likelihood of an accurate classification.\n\nTo implement this, we start by building an embedding\\-based retrieval system. Hereâ€™s how it works:\n\n\n### Step 1: Initialize the Classifier with Dynamic Prompting\n\nOur `LLMTextClassifier` class takes the list of possible categories and builds a prompt template for classification. We configure the classifier to retrieve a set number of examples (controlled by `max_examples`) that are most similar to the query text.\n\nUsing this setup, the classifier dynamically selects examples, injecting them into the prompt in the same format as the few\\-shot examples in the previous method:\n\n\n```python\nclass LLMTextClassifier:\n    def __init__(\n        self,\n        categories: list[str],\n        system_prompt_template: PromptTemplate = PromptTemplate(\n            input_variables=[\"categories\", \"schema\"],\n            template=\"Classify the following text into one of the following classes: {categories}.\\n \"\n            \"Use the following schema: {schema}\",\n        ),\n        llm_client: BaseChatModel = llm_medium,\n        max_examples: int = 5,\n    ):\n        # Initialize model, prompt, and retrieval variables\n        self.categories = categories\n        self.categories_model = generate_classification_model(categories)\n        self.system_prompt_template = system_prompt_template\n        self.system_prompt = system_prompt_template.format(\n            categories=categories, schema=self.categories_model.model_json_schema()\n        )\n        self.llm_classifier = llm_client.with_structured_output(self.categories_model)\n        self.max_examples = max_examples\n        self.examples = None\n        self.vector_store = None\n        self.retriever = None\n```\n\n### Step 2: â€œTrainâ€ the Classifier with Example Data\n\nTo â€œtrainâ€ our classifier (train used loosely here, as no weights are updated), we populate the vector store with training data examples labeled with their respective categories. This setup prepares the classifier to retrieve the most relevant examples dynamically when a new query is input:\n\n\n```python\n    def fit(self, texts, labels):\n        self.examples = [\n            Document(page_content=text, metadata={\"label\": label})\n            for text, label in zip(texts, labels)\n        ]\n\n        if len(self.examples) > self.max_examples:\n            # Add examples to vector store\n            self.vector_store = Chroma.from_documents(\n                documents=self.examples,\n                collection_name=\"llm-classifier\",\n                embedding=ChromaEmbeddingsAdapter(\n                    embedding_functions.DefaultEmbeddingFunction()\n                ),\n            )\n            self.retriever = self.vector_store.as_retriever(\n                search_kwargs={\"k\": self.max_examples}\n            )\n```\n\n### Step 3: Dynamically Retrieve Relevant Examples and Classify\n\nWhen a new text is input for classification, the classifier retrieves relevant examples based on similarity to the query. This list of relevant examples is added to the prompt, followed by the query itself, and sent to the LLM for classification:\n\n\n```python\n def predict(self, text: str) -> str:\n        messages = [SystemMessage(content=self.system_prompt)]\n        \n        for example in self.fetch_examples(text=text):\n            messages.append(HumanMessage(content=example.page_content))\n            messages.append(AIMessage(content=example.metadata[\"label\"]))\n\n        messages.append(HumanMessage(content=text))\n        prediction = self.llm_classifier.invoke(messages)\n\n        return prediction.category\n```\n\n### Step 4: Example run\n\n\n```python\nif __name__ == \"__main__\":\n    categories = [\"news\", \"clickbait\"]\n    classifier = LLMTextClassifier(categories=categories, max_examples=1)\n\n    texts = [\"Donald Trump won Michigan\", \"You won't believe what happened next!\"]\n    labels = [\"news\", \"clickbait\"]\n    \n    classifier.fit(texts, labels)\n\n    text = \"Donald Trump won Florida\"\n    result = classifier.predict(text)\n    print(result)  # Should output \"news\" if similar to \"news\" examples\n```\nUsing the dynamic few\\-shot technique, we saw a significant improvement in classification accuracy, reaching **88\\.6%**. This marks a considerable increase over previous methods, demonstrating the power of dynamically selecting relevant examples based on similarity to the query text.\n\n\n## Conclusion\n\nIn this post, we explored a simple yet powerful approach to building a reliable and accurate text classification pipeline using large language models (LLMs). We walked through three key techniques: *constrained generation*, *few\\-shot prompting*, and *dynamic few\\-shot selection*. Each of these methods contributes unique strengths to improve classification accuracy and usability, transforming LLMs into effective tools for text classification.\n\nThe first technique, constrained generation, involved limiting the LLMâ€™s responses to predefined classes, reducing the need for complex post\\-processing and making it easier to parse the modelâ€™s outputs. This approach alone allowed us to avoid common pitfalls of free\\-form text generation, improving the LLMâ€™s consistency in classification.\n\nNext, we implemented few\\-shot prompting, where we provided the LLM with a few labeled examples as part of the prompt. By leveraging the modelâ€™s in\\-context learning ability, few\\-shot prompting improved classification accuracy by setting clear expectations for the output format and content. However, we saw that the selection of examples is crucial â€” randomly chosen examples offered only a modest improvement. This led us to our final technique: dynamic few\\-shot selection.\n\nDynamic few\\-shot selection was the most advanced and effective approach, achieving a high classification accuracy of 88\\.6%. By using ChromaDB to retrieve the most similar examples for each query, this technique allowed the LLM to access only the most relevant context, which significantly enhanced its predictive accuracy. This method is a practical way to make generalized models like LLMs perform more like specialized classifiers, without the need to train a custom model from scratch.\n\n\n### Final Thoughts\n\nAs LLMs become more accessible and powerful, their applications in natural language processing tasks continue to grow. While these models are typically generalized, our tutorial demonstrates that with targeted techniques, they can be adapted into high\\-performing classifiers. Each of the methods we covered here â€” from straightforward constrained generation to advanced dynamic few\\-shot selection â€” offers flexibility and adaptability. They provide scalable solutions for building classification systems, making it feasible to integrate LLMs into production without extensive data collection or training.\n\nWhether youâ€™re an NLP practitioner, a data scientist, or an AI enthusiast, these techniques add versatile tools to your machine learning toolkit. With LLMs and these techniques, you can deploy robust and effective text classification systems tailored to your specific needs.\n\nThank you for reading!\n\nCode: <https://github.com/CVxTz/llmclassifier>\n\n\n"},{"lang":"en","group":"blog","slug":"blog/building-autonomous-multi-agent-systems-with-crewai-1a3b3a348271","frontmatter":{"title":"Building Autonomous Multi-Agent Systems with CrewAI","meta_title":"Building Autonomous Multi-Agent Systems with CrewAI","description":"The article discusses the development of autonomous multi-agent systems using CrewAI and LangChain frameworks. It explains the structure of multi-agent systems, emphasizing the roles of agents, tools, and tasks in achieving complex operations. The article details a project that integrates these frameworks to create an essay-writing application, demonstrating how agents can collaborate to research, write, and edit essays. It also outlines the projectâ€™s architecture, including agent design, task management, and deployment using Streamlit, providing insights into building efficient AI systems.","date":"2024-11-14T03:29:09.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*72Cy_QqOie7G2NAiWr13Kw.jpeg","categories":["Autonomous Systems","Programming","Data Science"],"author":"Rifx.Online","tags":["CrewAI","LangChain","multi-agent","Streamlit","essay-writing"],"draft":false,"slug":"blog/building-autonomous-multi-agent-systems-with-crewai-1a3b3a348271"},"content":"\n### Whatâ€˜s Multi\\-Agent Autonomous System and How to build one with CrewAI and LangChain?\n\n\n\n## Motivation\n\nIn fact, we are not unfamiliar with these concepts; we know them from movies. A person commands their AI, and the AI carries out these commands by using various tools. This is the path we are on today with the rise of AI systems. The era is gradually changing. In the past, people couldnâ€™t undertake a task alone and would need a team. Without a team, they would either run out of energy after a while or hit the limits of their abilities. In the end, successful projects come from teams made up of individuals with different skills.\n\n> Teamwork makes the dream work.\n\nHowever, these days a new technology has started to make a name for itself. We can call it the next phase of AI before AGI: â€˜Agents.â€™ So, what are these agents? Before diving into the code, letâ€™s talk a bit about the structure of multi\\-agent systems\n\n## How does it work?\n\nTo put the equation simply: `Multi Agent Systems = AGENTs + TOOLs + TASKs` Itâ€™s a system where multiple agents are equipped with various tasks and tools.\n\n### Agent\n\nWe are familiar with role\\-playing games, where your character has a role, like a warrior, for example. Throughout the game, you put yourself in their place, aiming to complete the game by finishing the quests that shape their backstory from one adventure to the next. Similarly, researchers have discovered that large language models (LLMs) can be motivated to perform tasks optimally when given roles, backstories, and objectives. This allows us to motivate LLMs to carry out various tasks with just a few simple prompts.\n\nAgents essentially break down the assigned tasks into simple steps and then execute those steps by â€˜thinkingâ€™ â€” yes, thinking â€” through them in sequence. This enables us to create an agent that not only performs steps thoughtfully but also consults other agents with different areas of expertise, rather than relying on a single LLM to input prompts and receive outputs.\n\n### Tools\n\nOne of humanityâ€™s greatest abilities is undoubtedly our skill in using tools. This ability has evolved and developed through both evolutionary and cultural processes, allowing us to create the advanced technology we use today. Similarly, large language models have increased their capabilities as they are trained on larger datasets. Now, when the function of a tool and how it is used are clearly explained, these models can autonomously use the tool under appropriate conditions, executing it fully automatically and planning their next steps based on the output, without waiting for further commands.\n\nTherefore, tool use can be considered one of the most important parts of their evolution as well. Especially with the internet browsing tool, agents can follow the specified functionâ€™s steps to access the necessary resources, whether through web scraping or by using the search engine of the designated site.\n\nYour toolsâ€™ functionality and purpose are entirely up to your imagination. However, if youâ€™d like to integrate pre\\-built tools into your agents, both CrewAI and LangChain libraries offer a wide range of built\\-in tools ready for use. In this project, we will focus on creating our own custom tool instead.\n\n### Task\n\nJust as we create agents, we also create tasks, and each task requires various tools. To give an example from human behavior, what do we do when we need to research something?\n\n1\\- We search the internet.\n\n2\\- We conduct in\\-depth source research.\n\n3\\- We take notes on our findings.\n\nIn the same way, we can design tasks to follow these steps, and we will touch on how they are designed through the code.\n\n## What is CrewAI?\n\nCrewAI is an open\\-source Python framework for orchestrating role\\-playing, autonomous AI agents with methods like Crew, Task, Agent, Process, and it supports various LLMs, including local models.\n\nIf we look at the main advantages provided by the framework:\n\n* Role\\-based agent design.\n* Autonomous inter\\-agent delegation.\n* Flexible task management.\n* Process\\-driven execution.\n* Output saving as files like .markdown files.\n* Compatibility with both open\\-source and proprietary models like OpenAI.\n\n## Building Multi\\-Agent\n\nA descriptive explanation alone may not always be enough to fully understand a concept, so letâ€™s create a small Essay Writer project to better grasp the multi\\-agent approach. In this project, we will combine the LangChain and CrewAI frameworks. To run the project, you will need an OpenAI API key, which you can obtain by visiting [https://proxy.rifx.online/https://platform.openai.com/signup](https://proxy.rifx.online/https://platform.openai.com/signup).\n\nThe structure of our project consists of several different python scripts:\n\n* `crew.py`, where we define our agents and their tasks.\n* `graph.py`, which builds the LangGraph structure.\n* `extra_tools.py`, containing the tools our agents will use.\n* `pdf_writer.py`, responsible for converting the essay into a PDF.\n* `app.py`, which provides the Streamlit interface for our application.\n\n```python\n## Project Structure\nAutonomous-Multi-Agent-Systems-with-CrewAI-Essay-Writer\nâ”œâ”€â”€ app.py              # Main streamlit application\nâ”œâ”€â”€ crew.py             # CrewAI agents and task handling\nâ”œâ”€â”€ extra_tools.py      # Agentic functions of tools\nâ”œâ”€â”€ graph.py            # LangGraph and Project Workflow\nâ”œâ”€â”€ pdf_writer.py       # Handles PDF output generation\nâ”œâ”€â”€ requirements.txt    # List of required libraries\nâ”œâ”€â”€ media\nâ”‚   â””â”€â”€ cover.jpg       # Project cover image\nâ””â”€â”€ README.md        \n```\n\nThe libraries required for this project are listed in the `requirements.txt` file. Additionally, please ensure you have installed Python version 3\\.12 or higher. Donâ€™t forget to install the dependencies before running the project. The libraries we use include:\n\n```python\nlangchain-core\nlangchain-openai\nlanggraph\nstreamlit\nwikipedia\nreportlab\ncrewai[tools]\npysqlite3-binary\nbs4\n```\n\n### Workflow\n\nIn our process, we will assign various roles to the agents. For example, while one agent waits for another to complete their task of researching on the internet, another agent will independently carry out research on Wikipedia. Once both agents have completed their tasks, the agent waiting for their information will then proceed with writing the essay, which is their assigned task.\n\nIf we were to visualize this:\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Emb37H_8OAKp1s1ChLLVQg.png)\n\n* The User Query is initially sent to the Router.\n* The Router reads the query and determines whether the user wants to write a new essay, edit a previous one, or simply convey a topic for discussion. If the user wishes to write a new essay, the request is forwarded to Crew.\n* The incoming request to Crew is first sent to the Researcher Agent.\n* The researcher agent uses the tools assigned to him to search for internet resources related to the subject on which the user wants to write an essay.\n* Once the resource collection process is completed, the collected information is forwarded to the Writer Agent.\n* When the writer agent drafts the essay, the Editor Agent makes final adjustments, corrects grammatical errors, and returns the draft as a JSON file to LangGraph.\n* The JSON file is sent to the function that will create our essay as a PDF file in the final node.\n\n### Building LangGraph\n\nFirst, we need to establish the skeleton of our schema. Once we create a workflow that allows us to reach our agents as needed, all thatâ€™s left is to decide at which stages of the workflow we will send a request to our agents. To do this, we will first create a simple workflow using LangChain.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*iHQzJymxAstrW40THoRxwA.png)\n\n```python\n#LangGraph workflow\n\nbuilder = StateGraph(GraphState)\n\nbuilder.add_node(\"answer\", self.answer)\nbuilder.add_node(\"write_essay\", self.write_essay)\nbuilder.add_node(\"edit_essay\", self.edit_essay)\n\n\nbuilder.set_conditional_entry_point(self.router_query,\n                              {\"write_essay\": \"write_essay\",\n                                        \"answer\": \"answer\",\n                                        \"edit_essay\": \"edit_essay\"})\nbuilder.add_edge(\"write_essay\", END)\nbuilder.add_edge(\"edit_essay\", END)\nbuilder.add_edge(\"answer\", END)\n\nself.graph = builder.compile()\n```\n\n**Router Node**: As we mentioned in the workflow description, our router assigns tasks to various nodes based on incoming requests. To achieve this, we need to create an effective prompt that encompasses the topic provided by the user and incorporates past conversations. After all, we are developing a multi\\-agent essay writer chatbot that can remember and recall previous discussions.\n\nLetâ€™s draft a simple prompt and the corresponding node that will utilize this prompt. In the prompt, we should define a `BaseModel` using the Pydantic library to enforce that our router selects one of three potential response strategies. These strategies will guide the chatbot in formulating its responses effectively.\n\nIn the node, we will implement this prompt using Langchainâ€™s `PromptTemplate` method. Then, we will invoke the LLM (Large Language Model) with both the user query and the conversation history to ensure that the response is contextually relevant and tailored to the userâ€™s needs.\n\n1. **Define the Pydantic Model**: Create a model that specifies the required response strategy.\n2. **Construct the Prompt**: Write a prompt that clearly outlines the three strategies.\n3. **Set Up the Node**: Use Langchain `PromptTemplate` to format the prompt dynamically.\n4. **Invoke the LLM**: Call the LLM with the formatted prompt, user query, and conversation history.\n\nBy following these steps, we can ensure that the chatbot responds accurately and maintains the context from previous interactions.\n\n```python\n#Router Prompt and Router Node\nclass RouteQuery(BaseModel):\n    \"\"\"Route a user query to direct answer or research.\"\"\"\n\n    way: Literal[\"edit_essay\",\"write_essay\", \"answer\"] = Field(\n        ...,\n        description=\"Given a user question choose to route it to write_essay, \n        edit_essay or answer\",\n    )\n\nself.router_prompt = \n    \"\"\"\n    You are a router and your duty is to route the user to the correct expert.\n    Always check conversation history and consider your move based on it.\n    If topic is something about memory, or daily talk route \n      the user to the answer expert.\n    If topic starts something like can u write, or user request \n      you write an article or essay, route the user to the write_essay expert.\n    If topic is user wants to edit anything in the essay, \n      route the user to the edit_essay expert.\n  \n    \\nConservation History: {memory}\n    \\nTopic: {topic}\n    \"\"\"\n\ndef router_query(self, state: GraphState):\n    print(\"**ROUTER**\")\n    prompt = PromptTemplate.from_template(self.router_prompt)\n    memory = self.memory.load_memory_variables({})\n\n    router_query = self.model.with_structured_output(RouteQuery)\n    chain = prompt | router_query\n    result:  RouteQuery = chain.invoke({\"topic\": state[\"topic\"],\n                                       \"memory\": memory})\n\n    print(\"Router Result: \", result.way)\n    return result.way\n```\n\n**Simple Answer Node**: After placing our router as a node in the start section, the next step is to create our other three nodes: `write_essay`, `edit_essay`, and `answer`. To take a straightforward approach, we need to program our `answer` node to generate responses directly using its memory when a user sends a casual message or engages in a conversation about an essay.\n\nTo achieve this, we must first write a suitable prompt for this task. Then, using this prompt, we will design a simple node. Letâ€™s proceed with this design.\n\n```python\n#Simple Answer Prompt and Node\n\nself.simple_answer_prompt = \n      \"\"\"\n      You are an expert and you are providing a simple \n      answer to the user's question.\n    \n      \\nConversation History: {memory}\n      \\nTopic: {topic}\n      \"\"\"\ndef answer(self, state: GraphState):\n    print(\"**ANSWER**\")\n    prompt = PromptTemplate.from_template(self.simple_answer_prompt)\n    memory = self.memory.load_memory_variables({})\n    chain = prompt | self.model | StrOutputParser()\n    result = chain.invoke({\"topic\": state[\"topic\"], \"memory\": memory})\n\n    self.memory.save_context(inputs={\"input\": state[\"topic\"]}, outputs={\"output\": result})\n    return {\"response\": result}\n```\n\n**Writing Essay Node**: Next, we need to design the `writing_essay` node. The purpose of this node is to forward the query received from the user to our agents using CrewAIâ€™s `kickoff` method and then to convert the JSON file returned by the agents into a PDF. Naturally, we do not need to write a prompt for this node, as the prompts will be defined during the agent creation phase. This node will be created solely for the purpose of invoking the agents and utilizing the returned values.\n\n1. **Invoke Agents**: Use CrewAIâ€™s `kickoff` method to send the user's query to the agents.\n2. **Process the Returned JSON**: Handle the JSON response received from the agents.\n3. **Convert to PDF**: Convert the relevant data from the JSON into a PDF format.\n\n```python\n#Write Essay Node\ndef write_essay(self, state: GraphState):\n    print(\"**ESSAY COMPLETION**\")\n\n    self.essay = self.crew.kickoff({\"topic\": state[\"topic\"]})\n\n    self.memory.save_context(inputs={\"input\": state[\"topic\"]},\n                           outputs={\"output\": str(self.essay)})\n\n    pdf_name = generate_pdf(self.essay)\n    return {\"response\": \"Here is your essay! \",  \"pdf_name\": f\"{pdf_name}\"}\n```\n\n**Edit Essay Node**: Letâ€™s briefly discuss our final node, `edit_essay`. The code may appear a bit lengthy, as the prompt is kept within the node. You can also write the prompt during the class definition and assign it as a variable if you prefer.\n\nThis node will be activated by the router when it detects a request for any essay modifications from the user. In this node, we need three important values: the conversation history, the userâ€™s request, and the most recently generated essay. Additionally, there is a variable in the prompt that Langchain will generate called `format_instructions`. This variable allows us to communicate to the LLM that we want to maintain the structure of the JSON format of the edited essay and to receive the response in the same format. Afterward, we will send the returned response to our PDF generation tool.\n\n1. **Detect Edit Request**: The router identifies whether the userâ€™s request is for editing an essay.\n2. **Collect Necessary Values**: Gather the conversation history, user request, and the last generated essay.\n3. **Create and Use the Prompt**: Construct a prompt that includes `format_instructions`.\n4. **Generate the Edited Essay**: Invoke the LLM to get the edited essay and pass the response to the PDF generator.\n\n```python\n#Edit Essay Node\n\ndef edit_essay(self, state: GraphState):\n    print(\"**ESSAY EDIT**\")\n    memory = self.memory.load_memory_variables({})\n\n    user_request = state[\"topic\"]\n    parser = JsonOutputParser(pydantic_object=Essay)\n    prompt = PromptTemplate(\n      template=(\"Edit the Json file as user requested, \n                  and return the new Json file.\"\n                \"\\n Request:{user_request} \"\n                \"\\n Conservation History: {memory}\"\n                \"\\n Json File: {essay}\"\n                \" \\n{format_instructions}\"),\n      input_variables=[\"memory\",\"user_request\",\"essay\"],\n      partial_variables={\"format_instructions\": parser.get_format_instructions()},\n  )\n\n    chain = prompt | self.model | parser\n\n    self.essay = chain.invoke({\"user_request\": user_request,\n                               \"memory\": memory, \n                                \"essay\": self.essay})\n\n\n    self.memory.save_context(inputs={\"input\": state[\"topic\"]},\n                             outputs={\"output\": str(self.essay)})\n    pdf_name = generate_pdf(self.essay)\n    return {\"response\": \"Here is your edited essay! \", \n            \"essay\": self.essay, \"pdf_name\": f\"{pdf_name}\"}\n```\n\n## Building Agents\n\n**Content Researcher**: To keep our project simple, we have defined three agents that will communicate with each other and conduct internet searches to write essays. Letâ€™s design the first agent, the researcher agent. This agent will perform web scraping on Wikipedia and other websites as needed, gathering necessary sources until it determines it has collected enough information. It will fetch main headings, subheadings, and articles related to the topic, preparing summaries. Subsequently, these documents will be stored to be sent to the writer's agent.\n\nWhen designing this agent, we need to consider its role, backstory, and goal. We will assign these to the parameters in the `Agent` class similarly to how we would construct a prompt, thereby readying the agent for operation.\n\n```python\n#Content Researcher Agent and Task\n\nself.researcher = Agent(\n    role=\"Content Researcher\",\n\n    goal=\"Research accurate content on {topic}\",\n\n    backstory=\"You're researching content to write \n                an essay about the topic: {topic}.\"\n              \"You collect information that helps \n                the audience learn something and make informed decisions.\"\n              \"Your work is the basis for the Content Writer to \n                write an article on this topic.\",\n    verbose=True\n)\n\nself.research = Task(\n    description=(\n        \"1. Prioritize the latest trends, key players, \n            and noteworthy news on {topic}.\\n\"\n        \"2. Identify the target audience, considering their \n            interests and pain points.\\n\"\n        \"3. Research a detailed content outline including \n            an introduction, key points, and a conclusion.\\n\"\n        \"4. Include SEO keywords and relevant data or sources.\"\n    ),\n    expected_output=\"A comprehensive document with an outline, \n                    audience analysis, SEO keywords, and resources.\",\n    tools=[search_wikipedia, scrap_webpage],\n    agent=self.researcher,\n)\n```\n\nWe need to create two classes: `Agent` and `Task`. Each agent can have one or more assigned tasks. We can assign tools directly to the agents or add task\\-specific tools. By adding the tool specifically for the task, we ensure that the tool is only used during that particular task.\n\n### Parameters\n\nThe parameters of our `Agent` class:\n\n1. **Role**: This defines the agentâ€™s function within the crew. It determines the kind of tasks the agent is best suited for and should be short and descriptive.\n2. **Goal**: This is the individual objective that the agent aims to achieve. It guides the agentâ€™s decision\\-making process and should be short and simple.\n3. **Backstory**: This provides context for the agentâ€™s role and goal, enriching the interaction and collaboration dynamics. It should be as detailed as possible.\n4. **Verbose**: Setting this to `True` configures the internal logger to provide detailed execution logs, aiding in debugging and monitoring what our agent is engaged in.\n\nThe parameters of our `Task` class:\n\n1. **Description**: A clear and concise statement of what the task entails. This should be as detailed as possible to ensure clarity.\n2. **Expected Output**: A detailed description of what the taskâ€™s completion looks like, helping to set clear expectations for the outcome.\n3. **Tools**: The functions or capabilities the agent can utilize to perform the task. Here, you can use LangChain, CrewAI, or custom tools as needed.\n4. **Agent**: The agent responsible for the task, assigned either directly or through the crewâ€™s process.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ocQ9ZUZwFtGx7a7pPrTbuQ.png)\n\n**Content Writer**: Once our researcher agent has gathered the necessary information through several iterations, it will store the collected data in memory, believing it has acquired sufficient knowledge, and pass the task to our next agent, the content writer.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*HD1Bm7twxsUIVGPiqEhHAg.png)\n\nNow, letâ€™s define our content writer agent and its role. This agent does not require the use of any tools, so it will suffice to detail the backstory and description similarly to the researcher agent. In the backstory, we must remember to specify which agent provided the source of information.\n\n### Parameters\n\n1. **Role**: The function of the content writer agent within the project. This should succinctly capture what the agent does.\n2. **Goal**: The specific objective that the content writer aims to achieve, such as drafting a well\\-structured essay based on the information collected.\n3. **Backstory**: This provides context for the content writerâ€™s role, including details about the researcher agent and the information it has provided. A well\\-crafted backstory can enhance the narrative and collaboration dynamics.\n4. **Description**: A clear and concise statement of what the content writer does, focusing on its responsibilities and tasks.\n5. **Expected Output**: A detailed description of what the taskâ€™s completion looks like, helping to set clear expectations for the outcome.\n6. **Context**: Before executing the task, we specify which task to wait for to be completed and to obtain the necessary information from that task output with the context parameter.\n\n```python\n#Content Writer Agent and Task\n\nself.writer = Agent(\n  role=\"Content Writer\",\n\n  goal=\"Write insightful and factually accurate \"\n       \"opinion piece about the provided topic\",\n\n  backstory=\"You're working on a writing a new opinion piece \n              about the provided topic.\"\n            \"You base your writing on the work of the Content Researcher, \n             who provides an outline and relevant context about the topic.\"\n            \"You follow the main objectives and direction of the outline, \n              as provide by the Content Researcher.\"\n            \"You also provide objective and impartial insights \n             and back them up with information provide \n             by the Content Researcher.\",\n  verbose=True,\n)\n\nself.write = Task(\n  description=(\n      \"1. Use the content to craft a compelling essay.\\n\"\n      \"2. Incorporate SEO keywords naturally.\\n\"\n      \"3. Sections/Subtitles are properly named in an engaging manner.\\n\"\n      \"4. Ensure the essay is structured with an engaging introduction, \n          insightful body, and a summarizing conclusion.\\n\"\n      \"5. Proofread for grammatical errors and alignment with \n          the brand's voice.\\n\"\n      \"6. Pick a suitable header\\n\"\n  ),\n  expected_output=\"A well-written essay in markdown format, \n                  ready for publication, each section \n                  should have 2 or 3 paragraphs.\",\n  context=[self.research],\n  agent=self.writer,\n)\n```\n\n**Content Editor**: After defining the writer agent, we could have concluded the process; however, even though the writer agent is responsible for writing, it may still make spelling mistakes and errors that disrupt the coherence of the content. To prevent these issues and to export the essay output in JSON format, we will define one more agent: the content editor.\n\nIn the backstory of this agent, we will specify that it is responsible for reviewing and correcting the essay received from the writer agent. In the task phase, we will also define the required output format.\n\n```python\n#Content Editor Agent and Task\nself.editor = Agent(\n    role=\"Content Editor\",\n\n    goal=\"Edit a given essay to align with the writing \n            style of the organization.\",\n\n    backstory=\"You are an editor who receives an essay \n                from the Content Writer.\"\n              \"Your goal is to review the essay to ensure \n                that it follows best practices, provides balanced viewpoints\"\n              \"When providing opinions or assertions,\n                and also avoids major controversial topics \n                or opinions when possible.\",\n    verbose=True\n)\n\nself.edit = Task(\n    description=\"Proofread the given essay for grammatical errors \n                  and alignment with the brand's voice.\",\n\n    expected_output=\"A well-written essay in required format, \n                      ready for publication, each section \n                      should have 2 or 3 paragraphs.\",\n    output_json = Essay,\n    context=[self.write],\n    agent=self.editor\n)\n```\n\nHere, our output is an object named; `Essay`, created with the help of the `BaseModel` and `Field` classes from the Pydantic Library. By adding explanations that our agent can understand, we ensure that the agent will output data in a format expected by our PDF printing function.\n\n```python\n#Expected Pydantic Output\n\nclass Paragraph(TypedDict):\n    sub_header: str\n    paragraph: str\n\nclass Essay(BaseModel):\n    header: str = Field(..., description=\"The header of the essay\")\n    entry: str = Field(..., description=\"The entry of the essay\")\n    paragraphs: List[Paragraph] = Field(..., description=\"The paragraphs of the essay\")\n    conclusion: str = Field(..., description=\"The conclusion of the essay\")\n    seo_keywords: List[str] = Field(..., description=\"The SEO keywords of the essay\")\n```\n\nWe have defined our agents and their tasks. Now, letâ€™s bring together our three agents. For this, we should use a small yet functional method from the CrewAI library called `Crew`. In this method, we list the agents that will operate sequentially along with the tools they will use. If the tasks need to be executed in order, as in our project, we set the `process` parameter to `Process.sequential`. We also set the `memory` parameter to `True` to enable the agents to communicate with each other using short\\-term and long\\-term memory.\n\n```python\n#Crew Run\n\ndef kickoff(self,*args):\n    return Crew(\n        agents=[self.researcher, self.writer, self.editor],\n        tasks=[self.research, self.write, self.edit],\n        process=Process.sequential,\n        verbose=True,\n        memory=True\n    ).kickoff(*args)\n```\n\nOur agent structure is complete, but we havenâ€™t discussed our tools yet. Now, letâ€™s briefly address our tools.\n\n## Building Tools\n\nTools are essentially functions that take various inputs and return a value as output. Our agents will simply provide the expected input to these functions and process the output they receive. Therefore, we need to design our tools with high fault tolerance. When a usage error occurs, our agents should be able to read the error and be equipped with information to use the tool correctly in the next iteration.\n\nAfter preparing the functions for our tools, we should convert them into tool objects using either LangChain or CrewAIâ€™s Tool creation class, along with various explanations. Here, we convert our tool into a form that our agent can use by simply writing C**rewAIâ€™s tool decorator** at the top of our function.\n\n```python\nfrom crewai_tools import tool\n\n@tool(\"Wikipedia Search Tool\")\ndef search_wikipedia(query: str) -> str:\n    \"\"\"Run Wikipedia search and get page summaries.\"\"\"\n    page_titles = wikipedia.search(query)\n    summaries = []\n\n    for page_title in page_titles[:3]:  # First 3 results\n        try:\n            wiki_page = wikipedia.page(title=page_title, auto_suggest=False)\n            summaries.append(f\"Page: {page_title}\\nSummary: {wiki_page.summary}\")\n        except wikipedia.PageError: # Page Not Found\n            pass\n        except wikipedia.DisambiguationError: # Disambiguation Error\n            pass\n\n    if not summaries:\n        return \"No good Wikipedia Search Result was found\"\n\n    return \"\\n\\n\".join(summaries)\n```\n\n## Building App\n\nNow, letâ€™s deploy our application live using the Streamlit framework, which I frequently use and believe offers an easy interface design. Streamlit is an open\\-source Python framework for data scientists and AI/ML engineers to deliver dynamic data apps with only a few lines of code.\n\nOur app primarily activates when the user enters their OpenAI key in a `text_input` box and then clicks the \"Initialize Agents\" button. When the user sends a message through the active `chat_input` section, the following function is used to relay the entered request to our established agent structure:\n\n```python\ndef generate_response(topic):\n    return app.invoke(input={\"topic\": topic})\n```\n\nWith Streamlitâ€™s `st.chat_message` component, we can easily implement a chatbot interface. If the user is engaged in regular messaging, the response will display a normal answer. If an essay has been generated, weâ€™ll provide the directory of the PDF to the user by writing a simple if\\-else loop.\n\nMeanwhile, we add every message we send and receive from the chatbot to a `messages` variable created in Streamlitâ€™s `session_state`. This way, we create a visible chat screen.\n\n```python\n#Streamlit App\n\nimport streamlit as st\nfrom graph import EssayWriter\nimport os\nimport base64\n\nst.set_page_config(page_title=\"Essay Writer Chat Bot\", page_icon=\"ðŸ¤–\")\nst.image(\"./media/cover.jpg\", use_column_width=True)\n\n\nif \"messages\" not in st.session_state:\n    st.session_state.messages =  [{\"role\": \"assistant\", \"content\": \"Hello!\"}]\n    st.session_state.app = None\n    st.session_state.chat_active = True\n\nwith st.sidebar:\n    st.info(\" * This app uses the OpenAI API to generate text, please provide your API key.\"\n            \"\\n\\n * This app uses the 'gpt-4o-mini-2024-07-18' model. Cost effective and efficient.\"\n            \"\\n\\n * If you don't have an API key, you can get one [here](https://proxy.rifx.online/https://platform.openai.com/signup).\"\n            \"\\n\\n * You can also find the source code for this app [here](https://proxy.rifx.online/https://github.com/mesutdmn/Autonomous-Multi-Agent-Systems-with-CrewAI-Essay-Writer)\"\n            \"\\n\\n * App keys are not stored or saved in any way.\"\n            \"\\n\\n * Writing essay may take some time, please be patient. Approximately 1-2 minutes.\"\n    openai_key= st.text_input(\"OpenAI API Key\", type=\"password\")\n\n\ndef initialize_agents():\n    os.environ[\"OPENAI_API_KEY\"] = openai_key\n    essay_writer = EssayWriter().graph\n\n    if len(openai_key) < 1:\n        st.error(\"Please enter your OpenAI API key and Initialize the agents.\")\n\n        st.session_state.chat_active = True\n    else:\n        st.success(\"Agents successfully initialized\")\n        st.session_state.chat_active = False\n\n    return essay_writer\n\nwith st.sidebar:\n    if st.button(\"Initialize Agents\", type=\"primary\"):\n        st.session_state.app = initialize_agents()\n\napp = st.session_state.app\ndef generate_response(topic):\n    return app.invoke(input={\"topic\": topic})\n\n\nfor message in st.session_state.messages:\n    with st.chat_message(message[\"role\"]):\n        st.markdown(message[\"content\"], unsafe_allow_html=True)\n\nif topic:= st.chat_input(placeholder=\"Ask a question\", disabled=st.session_state.chat_active):\n    st.chat_message(\"user\").markdown(topic)\n\n    st.session_state.messages.append({\"role\": \"user\", \"content\": topic})\n    with st.spinner(\"Thinking...\"):\n        response = generate_response(topic)\n\n    with st.chat_message(\"assistant\"):\n        if \"pdf_name\" in response:\n            with open(f\"./{response['pdf_name']}\", \"rb\") as file:\n                file_bytes = file.read()\n                b64 = base64.b64encode(file_bytes).decode()\n            href = f'<a href=\"data:application/pdf;base64,{b64}\" download=\"{response['pdf_name']}\">{response['pdf_name']}</a>'\n\n            st.markdown(f\"{response['response']}: {href}\", unsafe_allow_html=True)\n            st.session_state.messages.append({\"role\": \"assistant\", \"content\": f\"{response['response']}: {href}\"})\n        else:\n            st.markdown(response[\"response\"])\n            st.session_state.messages.append({\"role\": \"assistant\", \"content\": response[\"response\"]})\n```\n\n**Congratulations**! We have completed our project. If you wish, you can watch the project work log that I have recorded for you. Donâ€™t forget to visit the GitHub [**repo**](https://proxy.rifx.online/https://github.com/mesutdmn/Autonomous-Multi-Agent-Systems-with-CrewAI-Essay-Writer) to access all the codes of the project.\n\nAnd this is how our appâ€™s main page will look like once we deploy it!\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8tDAluuAH6njIohDbb-UqA.png)\n\n## Conclusion\n\nIn this article, we explored how to build autonomous multi\\-agent systems using CrewAI. We started by discussing the motivation behind creating agents and how they can work together to accomplish tasks more efficiently. By breaking down tasks and utilizing tools, we enabled our agents to perform complex operations in a structured way.\n\nWe developed a simple project that integrated the CrewAI and LangChain frameworks, showcasing how multiple agents can collaborate to gather information, write essays, and edit content. The use of tools and task management was emphasized to ensure our agents operated smoothly and effectively.\n\nFinally, we deployed our application using Streamlit, allowing users to interact with the system effortlessly.\n\nYou can check out the live project [**here**](https://proxy.rifx.online/https://multi-agent-essay-writer.streamlit.app/), view the source code on my GitHub repository [**here**](https://proxy.rifx.online/https://github.com/mesutdmn/Autonomous-Multi-Agent-Systems-with-CrewAI-Essay-Writer)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/building-self-healing-intelligent-test-automation-with-gen-ai-openai-apis-6c39808adb0f","frontmatter":{"title":"Building Intelligent Test Automation with Gen AI (OpenAI APIs)","meta_title":"Building Intelligent Test Automation with Gen AI (OpenAI APIs)","description":"We all know that UI Tests are super fragile. They can break for all sorts of reasons, and one of the biggest culprits is changes to the UIâ€¦","date":"2024-11-13T01:22:29.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kZ4ZR-jqdTTgH3bpOzcgUw.png","categories":["Generative AI","Programming","Testing"],"author":"Rifx.Online","tags":["Generative","OpenAI","Selenium","LLMs","POM"],"draft":false,"slug":"blog/building-self-healing-intelligent-test-automation-with-gen-ai-openai-apis-6c39808adb0f"},"content":"\n\n\n\n\n> We all know that UI Tests are super fragile. They can break for all sorts of reasons, and one of the biggest culprits is changes to the UI locators. Itâ€™s hard to imagine how we can make them smart enough to understand when the locators have changed and prevent the tests from running until when the locator issue in our test occurs.\n\nGuess what? Itâ€™s 2024, and automation testing tools have come a long way. After almost 18 years of working with them, from Mercury Winrunner to Playwright, we can now do some truly amazing things thanks to the power of Generative AI. Itâ€™s like magic, but itâ€™s science!\n\nYou heard it right, we can now device a way to make our test automation code **more intelligent** without writing all sort of fuzzy mathematical algorithms ourselves, it's all taken care by the God of LLMs.\n\nIn this post, we are going to discuss how we can make our tests intelligent in more effective and efficient fashion, but again, for you to make this happen you need to have following pre\\-requisite\n\n1. **Open AI API** with Credit (you need to purchase it on the go)\n\n\n\n2\\. C\\# .NET code knowledge, as the code I will be covering is from .NET and Selenium\n\n3\\. Basic understanding of Test automation\n\nAgain, all the above and following discussions are part of my [Udemy course](https://proxy.rifx.online/https://www.udemy.com/course/generative-ai-in-software-automation-testing/) which covers even more details and step\\-by step way of writing the code.\n\n\n## Lets understand the problem statement\n\nSo weâ€™ve got this page that we want to automate using Selenium C\\# code. Weâ€™ve written the code really well using the Page Object model (POM) pattern, and everything looks great and works perfectly as shown below.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GPSBTmPEZBpubI72OElwbw.gif)\n\nOur dev champ spotted a UI element that needs some tweaking. He made a change based on his peersâ€™ code review comments, but unfortunately, he removed a locator that weâ€™re using in our automation testing. This means our POM code with the locator wonâ€™t work anymore since it doesnâ€™t exist anymore, which eventually cause the test to **FAIL**.\n\nThe most important thing is that the test will fail for all the test scenarios with the same failure because of a single locator change. The test doesnâ€™t know or have any way to know that the locator has changed, and it fails all the time.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*tINBnScOW78vz8sKb6lWbA.gif)\n\n\n## How to resolve the problem?\n\nI am sure like me many of you are going through this day\\-in and day\\-out everyday while working with UI testing tools, it could be **Cypress**, **Selenium** or **Playwright**. The problem is always imminent regardless of the tool\n\nNow let's understand how we can resolve the problem above.\n\nWe all know **Generative AI** with **LLMs** (Large Language Models) are way beyond just text/image/video generation. They understand the given context and generate a meaningful set of information that we are looking for.\n\nSo, with the above problem statement, we can using the power of Gen AI using the OpenAIâ€™s API which can pass our prompt request to LLMs like ***GPT 4o*** or ***GPT 4 turbo*** to understand the problem statement and give us meaningful solution.\n\n\n> So, whats the prompt request we need to pass to OpenAIâ€™s API to get the operation to happen in our test automation?\n\nWell, this diagram will give you the answer\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*bsCOcyWc0FDnxPp9ApssVw.gif)\n\nWe can send OpenAIâ€™s API the â€œ**Actual Page Under Test**â€ of our application and our Selenium Testâ€™s â€œ**Page Object Model**â€ code as a prompt (with a few extra response parsing details). This will act as a validation process from OpenAIâ€™s API to see if the locators match the given page.\n\nBased on that operation, we can tell the test to execute or not. The locators have changed, so itâ€™s not worth running the test any further.\n\nThe code to perform the above operation will look something like this\n\n\n```python\npublic static async Task<string> VerifyPageLocatorFromAiAsync(string pomFileContent, string htmlPageSource)\n{\n    ChatClient client = new(model: \"gpt-4o-mini\", apiKey);\n    \n    var chatMessage = $\"Verify if locators from this Selenium POM class: {pomFileContent} match this page source: {htmlPageSource}\\\", only return True or False result\";\n\n    ChatCompletion completion = await client.CompleteChatAsync(chatMessage);\n\n    return completion.Content.FirstOrDefault().Text;\n}\n```\nThe code above is just part of the large code covered in the course, but you can see how straightforward it is to perform the operation of analysing your page against the Page Object Model code of Selenium.\n\n\n## GenAI in Software Testing Course\n\n*Most of the above discuss is just a slice of the topic we have discussed in my new course in Udemy on â€œ[**Using Generative AI in Software Automation Testing**](https://proxy.rifx.online/https://www.udemy.com/course/generative-ai-in-software-automation-testing/)â€*\n\nHere is the course content\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*lHe_b7qVqUQo-9Y5.png)\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*rMrsbB2IaKPbthdAr9Rc9g.png)\n\nThe course is currently available for discount in Udemy as the launch offer, please use coupon code **EA\\_NOV\\_24 âš¡ï¸** for discount while purchasing the course.\n\nIf the coupon code is expired, please feel free to comment on this post, I will send you the latest available coupon code.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/case-study-turning-doctor-transcripts-into-temporal-medical-record-knowledge-graphs-cf624d4927eb","frontmatter":{"title":"Case Study: Turning Doctor Transcripts into Temporal Medical Record Knowledge Graphs","meta_title":"Case Study: Turning Doctor Transcripts into Temporal Medical Record Knowledge Graphs","description":"Showcase of Data Transformation Process, Breakdown of 25 dev hours involved, Schemas used, Questions & Responses, and Graph created","date":"2024-11-13T01:22:29.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*DUNtg-0w2z-vlF9SCvt5UA.png","categories":["Health","Data Science","Machine Learning"],"author":"Rifx.Online","tags":["transcripts","Temporal","Knowledge","Graphs","vector"],"draft":false,"slug":"blog/case-study-turning-doctor-transcripts-into-temporal-medical-record-knowledge-graphs-cf624d4927eb"},"content":"\n\n\n\nInterested in turning Doctor/Patient medical records and transcripts into Temporal Knowledge Graph that you can ask complex questions across multiple medical histories, time periods, and patients?\n\nIn this case study, we show how Medical Transcripts are turned into Temporal Knowledge Graphs that you can rely on for the purposes of RAG and analytics. We show what real Question \\& Answers are against this system, and what type of business outcomes you can achieve with this system. As far as we are aware, the combination of steps here is a relatively novel Knowledge Graph implementation.\n\n\n### Data used\n\nFor data privacy reasons, we used a synthetic dataset of Medical Transcripts that we created out of Synthea data found here: [https://synthea.mitre.org/downloads](https://proxy.rifx.online/https://synthea.mitre.org/downloads). The below is an example of one of the medical transcripts used as the input data for the Knowledge Graph creation. We combined these transcript data with structured medical records in the Synthea data. We had \\~75 transcripts that covered 10 patients (i.e. each patient had 5â€“10 transcripts). Here is an example of a transcript used:\n\n\n\n\n## Novel Knowledge Graph Architecture Overview\n\n\n### Nodes:\n\nWe have 5 types of Nodes: Patient, Observation, Immunization, Condition and Encounter Type\n\n\n### Triples (Sample List):\n\nPatient \\-\\> Had Encounter \\-\\> Encounter\n\nPatient \\-\\> Has Condition \\-\\> Condition\n\nPatient \\-\\> Received \\-\\> Immunization\n\nPatient \\-\\> Has Measurement \\-\\> Observation\n\n\n### Chunks:\n\nChunks are chunks of text that are standalone objects. Chunks are tied to each Triple, and there can be many Chunks tied to a single Triple. Instead of being the unstructured source of the Triple, the Chunks in this case are summaries and key points related to each Triple type. As a result, we have 6 types of Chunks:\\- Patient Demographic Chunks, Condition Summary Chunks, Visit Chunks, Observation Chunks, Immunization Chunks and Condition Detail Chunks.\n\nAn example of what different type of chunks are tied to triples look like the following:\n\n\n```python\n1. Patient -> EncounterType\nTriple: (Patient) -[had_encounter]-> (EncounterType)\n- Chunk_ids link to specific visit instances\n- Example Chunk: \"Annual physical on 2024â€“01â€“15. BP 120/80, routine screenings \nupdated.\"\n\n2. Patient -> Condition\nTriple: (Patient) -[has_condition]-> (Condition)\n- Chunk_ids link to condition episodes\n- Example Chunk: \"Diagnosed with hypertension on 2020â€“03â€“10. Status: active. \nManaged with medication.\"\n\n3. Patient -> Immunization\nTriple: (Patient) -[received]-> (Immunization)\n- Chunk_ids link to administration records\n- Example Chunk: \"Influenza vaccine administered on 2024â€“01â€“15.\"\n\n4. Patient -> Observation\nTriple: (Patient) -[has_measurement]-> (Observation)\n- Chunk_ids link to measurement instances\n- Example Chunk: \"2024â€“01â€“15: Blood Pressure 120/80 mmHg, Weight 70kg.\"\n```\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*8dH_7tP6xheCaW6K)\n\n**Link to the Graph created: [https://proxy.rifx.online/https://main\\-\\-whyhowai.netlify.app/public/graph/673032011997e08c8849316c](https://proxy.rifx.online/https://main--whyhowai.netlify.app/public/graph/673032011997e08c8849316c)**\n\nWith this particular graph architecture, where you can have key points and summaries tied to triples, you can then focus on landing on the right set of triples through an unstructured search, and subsequently bringing in all the relevant key information through the linked chunks in a structured way.\n\n\n## Unique to WhyHow Architecture\n\nThere are a few things unique to the WhyHow graph infrastructure that allows us to build this architecture in a simple way.\n\nFirstly, Triples are embedded and retrieved through vector search, avoiding a common retrieval issue of having to use Text2Cypher for the identification of nodes, relationships, and then Cypher query construction just to land on the right Triple. This has shown to dramatically [improve retrieval accuracy by up to 3x](https://proxy.rifx.online/https://readmedium.com/knowledge-table-multi-document-rag-extraction-memory-ec08450e858f).\n\nSecondly, Triples are standalone objects in WhyHow that you can link chunks to. This allows you to distill the key information that you want to retrieve per Triple, and bring it directly into the context once the right Triples are found. This avoids having to represent crucial information and context in a graph format (complicating the schema construction process), and bringing in information in a structured way after the initial unstructured vector search. This is similar in process to [LinkedInâ€™s application of Knowledge Graphs](https://proxy.rifx.online/https://readmedium.com/5-misconceptions-of-kg-rag-systems-building-using-rag-native-graphs-5e47872e7903) in their system, where crucial information like â€˜Steps to Reproduceâ€™ are represented and retrieved similarly, and where the steps themselves are represented as individual â€˜chunksâ€™/ â€˜nodesâ€™.\n\nThirdly, WhyHow accepts data in a JSON format, which allows seamless interaction between any of the extraction frameworks directly into graph creation. In this case, we use Claude for the initial transformation of the transcript data into the necessary JSON structure to load into WhyHow. If you have information already sitting in JSON, loading data into WhyHow is then a lot easier.\n\nFourthly, because of the way that Chunks and the retrieval process is designed in the WhyHow system, you can easily include temporal data that can be used to govern the way that the answer is constructed. Temporal data has always been a hard thing to model in Knowledge Graphs (to the point that it is typically advised against by leading KG experts), but is an obviously important part of workflows. Existing methods that even attempt to model temporal data try to ingest this into the Knowledge Graph itself and then retrieve based on a structured Cypher query, as opposed to our architecture that uniquely uses the LLM to help filter for temporal data.\n\nBlending the power of LLM with structured knowledge representations like Knowledge Graphs are important ways to achieve business outcomes, and we think this temporal Knowledge Graph architecture will help unlock a lot of business value through the successful implementation of temporal data.\n\n\n### Data Transformation Process Used\n\nFirst, we use Claude to turn the transcript information into a schema\\-aligned set of information on a per transcript basis. Alongside information from structured medical Records, the transcript is turned into a JSON summarization that looks like this:\n\n\n```python\nPATIENT SUMMARY\nName: Joseph Crona\nDOB: 2022â€“08â€“29\nAge: 2 years\nGender: male\nMRN: #dbfbaa\n\nCURRENT MEASUREMENTS (as of 2024â€“08â€“05)\nHeight: 84.1cm (50th percentile)\nWeight: 14.5kg (52nd percentile)\nALLERGIES\nNo known allergies\n\nIMMUNIZATIONS\n- DTaP: 2022â€“12â€“05, 2023â€“02â€“06, 2023â€“03â€“06, 2024â€“02â€“05\n- Hepatitis A: 2023â€“11â€“06\n- Hepatitis B: 2022â€“08â€“29, 2022â€“10â€“03, 2023â€“03â€“06\n- Hib: 2022â€“12â€“05, 2023â€“02â€“06, 2023â€“11â€“06\n- Influenza: 2023â€“03â€“06, 2024â€“08â€“05\n- MMR: 2023â€“11â€“06\n- PCV13: 2022â€“12â€“05, 2023â€“02â€“06, 2023â€“03â€“06, 2023â€“11â€“06\n- Polio: 2022â€“12â€“05, 2023â€“02â€“06, 2023â€“03â€“06\n- Rotavirus: 2022â€“12â€“05, 2023â€“02â€“06\n- Varicella: 2023â€“11â€“06\n\nMEDICAL HISTORY\n- Viral sinusitis (disorder)\nOnset: 2023â€“03â€“13\nStatus: resolved\nOutcome: Resolved\n\nGROWTH & DEVELOPMENT\n- 2023â€“11â€“06: Body Weight: 12.7 kg\n- 2024â€“02â€“05: Body Height: 79 cm\n- 2024â€“02â€“05: Body Weight: 13.4 kg\n- 2024â€“08â€“05: Body Height: 84.1 cm\n- 2024â€“08â€“05: Body Weight: 14.5 kg\nDevelopment: Age-appropriate milestones met\n- Gross motor: Age appropriate\n- Fine motor: Age appropriate\n- Language: Age appropriate\n- Social: Age appropriate\n\nPREVENTIVE CARE\nWell-Child Visits:\n- 2024â€“08â€“05: 2yo well visit - Development on track\n- 2024â€“02â€“05: 1yo well visit - Development on track\n- 2023â€“11â€“06: 1yo well visit - Development on track\n- 2023â€“08â€“07: 1yo well visit - Development on track\n- 2023â€“05â€“08: 9mo well visit - Age appropriate exam completed\n- 2023â€“02â€“06: 6mo well visit - Age appropriate exam completed\n- 2022â€“12â€“05: 4mo well visit - Age appropriate exam completed\n- 2022â€“10â€“03: 2mo well visit - Age appropriate exam completed\n- 2022â€“08â€“29: Newborn visit - Normal exam\n\nFAMILY HISTORY\nMother: Healthy\nFather: Healthy\nSiblings: None documented\n\nSOCIAL HISTORY\nLiving Situation: Lives with parents\nDevelopment: Meeting age-appropriate milestones\nSleep: Age-appropriate pattern\nNutrition: Age-appropriate diet\n```\nSecondly, we map this JSON schema into the WhyHow schema, and then import all the information into the WhyHow.AI KG Studio.\n\nThe below is a sample of the KG Structure that was ultimately loaded into WhyHow.\n\n\n```python\nKnowledge Graph Structure (Timeless):\n\n\nNodes:\n1. Patient Node\n  Structure: {\n      name: str,         # \"John Smith\"\n      label: \"Patient\",\n      properties: {\n          gender: str,   # FHIR gender\n          patient_type: str  # \"adult\" | \"pediatric\"\n      },\n      chunk_ids: List[str]  # Links to demographic chunks\n  }\n\n\n2. EncounterType Node\n  Structure: {\n      name: str,         # \"Well-child visit\" | \"Annual physical\"\n      label: \"EncounterType\",\n      properties: {\n          category: str,  # \"preventive\" | \"acute\" | \"chronic\"\n          specialty: str  # \"primary_care\" | \"pediatrics\" | \"emergency\"\n      },\n      chunk_ids: List[str]  # Links to visit pattern chunks\n  }\n\n\n3. Condition Node\n  Structure: {\n      name: str,         # \"Essential hypertension\"\n      label: \"Condition\",\n      properties: {\n          category: str,     # \"chronic\" | \"acute\" | \"resolved\"\n          system: str,       # \"respiratory\" | \"cardiovascular\" | etc\n          is_primary: bool   # True if primary diagnosis\n      },\n      chunk_ids: List[str]  # Links to condition history chunks\n  }\n\n\n4. Immunization Node\n  Structure: {\n      name: str,         # \"DTaP\" | \"MMR\"\n      label: \"Immunization\",\n      properties: {\n          series: str,       # \"primary\" | \"booster\"\n          target: str        # \"tetanus\" | \"measles\" | etc\n      },\n      chunk_ids: List[str]  # Links to immunization records\n  }\n\n\n5. Observation Node\n  Structure: {\n      name: str,         # \"Blood Pressure\" | \"Height\"\n      label: \"Observation\",\n      properties: {\n          category: str,     # \"vital\" | \"lab\" | \"growth\"\n          unit: str         # \"mmHg\" | \"cm\" | etc\n      },\n      chunk_ids: List[str]  # Links to measurement records\n  }\n\n\nRelations:\n1. Patient -> EncounterType\n  Triple: (Patient) -[had_encounter]-> (EncounterType)\n  - Chunk_ids link to specific visit instances\n\n\n2. Patient -> Condition\n  Triple: (Patient) -[has_condition]-> (Condition)\n  - Chunk_ids link to condition episodes\n\n\n3. Patient -> Immunization\n  Triple: (Patient) -[received]-> (Immunization)\n  - Chunk_ids link to administration records\n\n\n4. Patient -> Observation\n  Triple: (Patient) -[has_measurement]-> (Observation)\n  - Chunk_ids link to measurement instances\n\n\n5. Condition -> EncounterType\n  Triple: (Condition) -[managed_in]-> (EncounterType)\n  - Links conditions to typical encounter types\n\n\n6. Immunization -> EncounterType\n  Triple: (Immunization) -[given_during]-> (EncounterType)\n  - Links vaccines to visit types\n```\nThirdly, we then run a custom prompt that contextualizes the triples retrieved from the Knowledge Graph after every natural language query.\n\nWith this architecture in place, one interesting thing is that we can now continue to add information about Patient visits, Patient treatments and conditions to the Knowledge Graph easily, since it is just a matter of adding additional chunks to the existing triples that exist. If a Patient gets a new disease, additional Condition nodes are added to the Patient nodes.\n\nThis process took 25 dev hours, which can be broken down into the following:\n\n* 2 hours (8%) was spent looking and understanding the data (Exploratory Data Analysis)\n* 18 hours (72%) was spent iterating on the schema, and figuring out what nodes should be in the graph, what nodes should be connected to what, what chunks should exist, how it should connect to the various triples, testing the retrieved answers with a set of questions, and iterating accordingly.\n* 2 hours (8%) was spent writing the code to create the set of triples to be loaded in\n* 3 hours (12%) was spent writing validation checks and output checks to catch any errors\n\n\n### Question \\& Answer against Medical Record Knowledge Graphs\n\nPrompt used for answer construction after retrieving the relevant context from the Knowledge Graph, using the WhyHow natural language graph query engine\n\n\n```python\n    You are an AI assistant specializing in medical records analysis. \nUse the following information to answer the user's question. \n    The information is derived from a knowledge graph of patient medical records.\n\n    Relevant Nodes (these represent patients, encounters, and conditions):\n    {node_context}\n\n    Relevant Relationships (these show connections between patients, \nencounters, and conditions):\n    {triple_context}\n\n    Relevant Medical Records:\n    {chunk_context}\n\n    Answer the question using the following steps:\n    1. Analyze the medical information, focusing on the specific topic of the \nquestion\n    2. Identify key medical events, conditions, and temporal relationships\n    3. Formulate a clear and concise answer that directly addresses the \nquestion\n    4. If there are any uncertainties in the medical information, mention \nthem explicitly\n    5. If you don't have enough information to answer confidently, say so\n    6. Consider temporal relationships between medical events\n    7. It's currently {datetime.now().strftime(\"%Y-%m-%d\")}. Consider this \nwhen interpreting dates\n    8. Maintain patient privacy by using only first names or initials when \nnecessary\n\n    Question: {question}\n```\n\n## Business Value: Analysis of Patient Treatments\n\n\n### Historical Patient Analysis\n\nQuestion: **What types of well\\-child visits and immunizations did Joseph Crona receive in their first year?**\n\nAnswer:\n\n\n```python\nTo determine the types of well-child visits and immunizations Joseph Crona \nreceived in his first year, we need to consider the relevant medical events \nand their timing. Joseph was born in 2022, and it is currently November 9, 2024.\nTherefore, his first year would span from 2022 to 2023.\n\n**Well-Child Visits:**\n- Joseph had an encounter categorized as a \"Well child visit,\" which falls \nunder preventive care in pediatrics. This type of visit is typically part of \nroutine check-ups for children, especially in their first year.\n\n**Immunizations:**\nDuring his first year, Joseph received the following immunizations, all of \nwhich are common for pediatric patients:\n1. **Hep B, adolescent or pediatric** - Completed\n2. **Hib (PRP-OMP)** - Completed\n3. **Rotavirus, monovalent** - Completed\n4. **IPV (Inactivated Poliovirus Vaccine)** - Completed\n5. **DTaP (Diphtheria, Tetanus, and Pertussis)** - Completed\n6. **Pneumococcal conjugate PCV 13** - Completed\n\nThese immunizations are typically administered during the first year of life \nas part of the standard immunization schedule for infants.\n\nIn summary, Joseph Crona received a well-child visit and completed a series of\nstandard pediatric immunizations during his first year.\n```\nQuestion: **For patients with otitis media, what was the sequence of encounters and observations recorded?**\n\nAnswer:\n\n\n```python\nTo address the question regarding the sequence of encounters and observations \nfor patients with otitis media, we will focus on the medical records of Miles \nDickinson, as he is the patient identified with this condition.\n\n1. **Condition Onset:**\n- Miles Dickinson was diagnosed with otitis media on June 10, 2024. The \ncondition is currently resolved.\n\n2. **Encounters:**\n- Miles had two types of encounters recorded:\n- **Well Child Visit:** This is categorized as a preventive encounter in \npediatrics.\n- **Encounter for Symptom:** This is categorized as a follow-up encounter in \nprimary care.\n\n3. **Observations:**\n- Various observations were recorded for Miles, including:\n- **Pain Severity:** Recorded multiple times with scores ranging from 0 to 4.\n- **Body Weight:** Recorded with values ranging from 3.2 kg to 8.2 kg.\n- **Head Occipital-frontal Circumference and Percentile:** Recorded with \nspecific values in cm and percentiles.\n- **Heart Rate:** Recorded with values ranging from 61 to 100 beats per minute.\n- **Respiratory Rate:** Recorded with values ranging from 12 to 16 breaths per\nminute.\n\n4. **Temporal Sequence:**\n- The otitis media diagnosis occurred on June 10, 2024. Observations and \nencounters before this date may have been related to other conditions or \nroutine check-ups. Observations and encounters after this date could be \nrelated to follow-up care for otitis media or other health assessments.\n\n5. **Conclusion:**\n- The sequence of encounters and observations for Miles Dickinson includes \npreventive and follow-up encounters, along with various health observations. \nThe specific sequence related to otitis media is not detailed in terms of exact\ndates for each observation or encounter, but the diagnosis date provides a \ntemporal marker for understanding subsequent medical activities.\n\nIf more detailed temporal data on each observation and encounter were \navailable, a more precise sequence could be established. However, based on \nthe current information, this is the overview of the sequence related to \notitis media for Miles Dickinson.\n```\n\n### Historical Multi\\-Patient Analysis\n\nQuestion: **How many patients have been diagnosed with acute bronchitis, and what are the typical onset patterns?**\n\nAnswer:\n\n\n```python\nBased on the provided medical records, there are two patients diagnosed with \nacute bronchitis: Maximo Swaniawski and Bennie Herman.\n\n#### Onset Patterns:\n- **Maximo Swaniawski**:\n- Onset dates for acute bronchitis:\n- October 24, 2014\n- November 7, 2015\n- November 29, 2018\n- **Bennie Herman**:\n- Onset dates for acute bronchitis:\n- October 24, 2014\n- November 7, 2015\n- November 29, 2018\n\n#### Typical Onset Patterns:\nThe onset dates for both patients are identical, suggesting a pattern where \nacute bronchitis episodes occurred in late October or November. This could \nindicate a seasonal pattern, possibly related to colder weather or other \nseasonal factors that might contribute to respiratory conditions.\n\nBoth patients have had their conditions resolved, indicating that the episodes \nof acute bronchitis were acute and not chronic.\n```\nQuestion: **Which patients had both viral sinusitis and bronchitis, and what were their earliest onset dates?**\n\nAnswer:\n\n\n```python\nTo determine which patients had both viral sinusitis and bronchitis, \nwe need to analyze the conditions associated with each patient and their \nonset dates.\n\n1. **Maximo Swaniawski**:\n- Conditions: Acute bronchitis\n- Earliest onset of bronchitis: 2014â€“10â€“24\n2. **Cassandra Monahan**:\n- Conditions: Viral sinusitis\n- Earliest onset of sinusitis: 2014â€“12â€“12\n3. **Bennie Herman**:\n- Conditions: Acute bronchitis\n- Earliest onset of bronchitis: 2014â€“10â€“24\n4. **Deangelo Price**:\n- Conditions: Viral sinusitis\n- Earliest onset of sinusitis: 2014â€“12â€“12\n5. **Joseph Crona**:\n- Conditions: Viral sinusitis\n- Earliest onset of sinusitis: 2014â€“12â€“12\n\nFrom the information provided, none of the patients have both viral sinusitis \nand bronchitis. Each patient has either one condition or the other, but not \nboth. Therefore, there are no patients with both conditions, and consequently, \nno earliest onset dates for both conditions in a single patient.\n```\n\n### Advantages against Vector\\-Only RAG systems\n\nSuperior Analytics across multiple patients:\n\n* In the question about which patients have both bronchitis and viral sinusitis conditions, we must be able to know if a patient is recorded to have or not have a particular disease. Since Vector\\-Only Search is about identifying relevant chunks, it cannot detect if a patient definitely does not have a particular disease. This means that an intermediary aggregation of data is required that can definitively say that Patient X does not have the â€˜diabetesâ€™ node against their name.\n\nMulti\\-Transcript Analysis and RAG:\n\n* In the question about Miles and his multiple visits in the past year, we can see that the conditions are across multiple visits and multiple transcripts. This means that an intermediary aggregation of data is required that maps a Patient and the cumulative visits and observations theyâ€™ve incurred over time.\n\nWhyHow.AI provides tools, services and processes for Structured Knowledge, Knowledge Graphs and more reliable Agentic RAG solutions. If you are interested in exploring any of our tools ([KG Studio](https://proxy.rifx.online/https://readmedium.com/whyhow-ai-kg-studio-platform-beta-rag-native-graphs-1105e5a84ff2), [Knowledge Table \\[Open Source]](https://proxy.rifx.online/https://readmedium.com/knowledge-table-multi-document-rag-extraction-memory-ec08450e858f)) and services, feel free to [chat with us here](https://proxy.rifx.online/https://calendly.com/whyhowai/intro-call-whyhow-ai).\n\nIf youâ€™re thinking about, in the process of, or have already incorporated knowledge graphs in RAG for accuracy, memory and determinism, follow our newsletter at [WhyHow.AI](https://proxy.rifx.online/https://whyhow.ai/) or join our discussions about rules, determinism and knowledge graphs in RAG on our [Discord](https://proxy.rifx.online/https://discord.gg/9bWqrsxgHr).\n\n\n"},{"lang":"en","group":"blog","slug":"blog/chatgpt-vision-turns-a-picture-into-1000-words-24858615fa28","frontmatter":{"title":"ChatGPT Vision Turns A Picture Into 1000 Words","meta_title":"ChatGPT Vision Turns A Picture Into 1000 Words","description":"And How You Can Turn Those Words Into Business","date":"2024-11-10T03:51:17.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*lS5aPVDrsCFFnBYz","categories":["Programming","Marketing","Generative AI"],"author":"Rifx.Online","tags":["automation","content","GPT","MAKE","photos"],"draft":false,"slug":"blog/chatgpt-vision-turns-a-picture-into-1000-words-24858615fa28"},"content":"\n\n\n\n\n### And How You Can Turn Those Words Into Business\n\nIâ€™ve had this idea for nearly a decade. It all started when I was building websites, and a lodge owner sent me a thumb drive packed with almost a thousand photos. And a box of 35mm photographs for me to scan.\n\n\n\n\n> These were **amazing shots** â€” guests showing off their prized catches, stunning lake views, and guides leading outdoor adventures.\n\n\n> I knew that if we could get these photos online, they would create a tidal wave of **word\\-of\\-mouth** marketing for the lodge.\n\nBut hereâ€™s where it got complicated: each photo needed a unique description, proper tags, a blog post, and social media uploads. And there were almost a thousand of them!\n\nThe time and cost involved were staggering. Writing captions for hundreds of fish photos? It was enough to make anyoneâ€™s head spin. So, the great idea remained just that â€” an idea.\n\nFast forward to today. Now, with the power of automation, Iâ€™ve turned that idea into reality.\n\nIâ€™ve built a system that automates content creation for lodges, transforming old photos into engaging stories that can be published online effortlessly.\n\nIn this article, Iâ€™ll walk you through the exact steps I took to build this system, and how you can do the same to save time and preserve the history of your lodge.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*5GBorUl_PfqiLnSW6-Nsjg.png)\n\n\n\n\n\n\n\n\n## Step 1: Collect Your Lodgeâ€™s Unique Details\n\nThe first step in setting up this automation is gathering all the information and assets that make your lodge special. For me, this involved collecting the details that would make our content stand out. Think about the key experiences your lodge offers, like fishing trips, local adventures, or unique amenities. These details are what will make your content personal and engaging.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*4QX4oGCYK5djc9EZuE9-EA.png)\n\nNext, gather old photos â€” everything from past guest experiences to nature shots around the lodge.\n\nFor my experiment, I started with AI generated images, but then a follower of our page on Facebook sent in some pictures. I asked permission to feature them, and he loves it!\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*PuwLsJ2EuOYOLzXwUvn0cQ.png)\n\nIf you have a collection of photos like these, organize them into a Google Drive folder. This will make it easy to feed them into the automation system later. Along with the photos, create a Google Spreadsheet where you can log details about each image. Your spreadsheet should include:\n\n* The image URL (from your Google Drive folder)\n* Lodge details (such as fishing guides, special activities)\n* Any relevant stories or descriptions that can accompany the photo\n\nThis might seem like extra work upfront, but itâ€™s critical for helping the automation create meaningful content later.\n\n\n## Step 2: Build the Automation Blueprint\n\nOnce youâ€™ve gathered all the assets, itâ€™s time to set up the automation blueprint. I used an automation platform called [MAKE](https://www.make.com/en/register?pc=saleprice). If youâ€™ve never worked with automation before, donâ€™t worry â€” this is easier than it sounds.\n\nâ€¦ and you can **get all** my proven blueprints for [free](https://whop.com/ai-businessplans) in our 7 day trial.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ArzS71K2fJf4EfCg5y3BPQ.png)\n\nStart by duplicating an existing automation template, like one that posts weather updates to social media. I had a weather automation in place, so I used that as a base and stripped out the unnecessary parts. You want a clean slate to work with, so remove any modules that donâ€™t apply to your lodge, such as weather updates or extra social media channels you wonâ€™t use.\n\nNow, itâ€™s time to make the automation specific to your lodge.\n\n\n## Step 3: Customize the Automation for Your Lodge\n\nWith the basic structure in place, customize the automation by adding lodge\\-specific content. This is where the details you gathered in Step 1 come into play. Input your lodge description, add stories from past adventures, and incorporate local tips. Make sure to include keywords that will help your content get noticed online.\n\nNext, configure the automation to pull the photos from your Google Drive folder and match them with the corresponding descriptions from your spreadsheet. This ensures that the right image is paired with the right story.\n\nHereâ€™s where the magic happens: I integrated GPT (a language model AI) into the automation. GPT analyzes each photo and generates unique content based on the details you provided.\n\nFor example, if the photo shows a guest catching a huge fish, GPT can create a post about that specific experience, including details about the fishing guide, the type of fish, and even tips for future visitors.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*wySL0TxoNTFICejPsJOOcA.png)\n\n\n## Step 4: Automate Publishing to Social Media and Medium\n\nOnce the content is generated, itâ€™s time to automate the publishing process. I connected our Medium account so that GPT\\-generated articles could be uploaded directly as drafts, ready for review. Medium is a great platform for long\\-form content like blog posts or detailed guest stories.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*56OzNVMIxw9sJKBn1jkriQ.png)\n\nFor shorter content, like social media posts, the automation links to our Facebook and Twitter accounts. The system is designed to create snippets from the longer articles, which are perfect for quick social media updates. You can also configure the automation to post automatically or schedule posts for specific times.\n\nThe beauty of this system is that once the content is approved, the automation handles everything from posting to scheduling. Itâ€™s a hands\\-off solution that keeps your online presence active without requiring constant attention.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*rym300CevwmoA4_pvYybqQ.png)\n\n\n## Step 5: Testing and Fine\\-Tuning\n\nNow that the automation is in place, itâ€™s important to test it before going live. I ran several test posts to ensure that everything worked as expected. The automation pulled the correct photos, generated engaging content, and posted it to Medium and social media without a hitch.\n\nDuring testing, I made small adjustments, such as tweaking titles or refining the GPT prompts to make sure the content aligned perfectly with our lodgeâ€™s tone and story.\n\nOnce everything was dialed in, I can create a steady stream of fresh content that keeps our audience engaged.\n\n\n## A Test Project in Action\n\nTo give you a practical example, take a look at the [iFish Canada Facebook page](https://facebook.com/ifishcanada).\n\nThis project is a demonstration of how the automation works in real life.\n\nThe system takes photos submitted by our followers, from their fishing trips in Canada, runs them through GPT, and generates unique posts that showcase the photo, the experiences of its author â€” and for our example, the posts tie in the lore of our fictional lodge.\n\nThe content is rich, engaging, and best of all â€” automated.\n\nWhat once seemed like an impossible task is now a reality, saving 100â€™s and 100â€™s of hours and allowing us to share the stories that make our fishing picture contributors feel recognized and appreciated.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Pe-hBjGCN1qbkjLl6cmTmQ.png)\n\n\n## Step 6: Monitor, Adjust, and Grow\n\nEven though the system may run automatically, itâ€™s important to monitor the results and make adjustments over time.\n\nI will check which posts get the most engagement and tweak the GPT prompts to improve future content. This ongoing fine\\-tuning ensures that our online presence stays fresh and continues to attract new visitors.\n\nImagine how you can preserve the legacy of your lodge, sharing memories that might otherwise have been lost, and generating word\\-of\\-mouth marketing in a modern, powerful way.\n\n\n## A Decade in the Making â€” Now You Can Join Me\n\nAfter years of dreaming about automating this process, itâ€™s finally real. The system is up and running, and the results will speak for themselves.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*135_nP0nL6NrT_O7JxIk4g.png)\n\nNow, I want to invite you to experience this for your lodge or resort. If youâ€™ve ever felt overwhelmed by the time or cost of marketing, or if youâ€™ve struggled to keep up with the constant demand for new content, [this automation system](https://whop.com/ai-businessplans) could be the solution youâ€™ve been looking for.\n\nVisit [iFish Canada](https://facebook.com/ifishcanada) to see the system in action, and if youâ€™d like to get started with automating your own content, feel free to reach out.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*IJ-R1362_IWUrZ-3KyG8mw.png)\n\nIâ€™d love to help you unlock the power of automation to grow your lodge, share your story, and attract new guests â€” all while saving you time and effort.\n\nâ«·\n\n\n### I Value Your Comments\n\nI reply to all comments and **as my thank you** Iâ€™ll also follow, clap, highlight and comment where it fits on your content. So leave your thoughts, questions, or success stories too! I love to read them!\n\n*Connect* on [YouTube](https://www.youtube.com/channel/UCphdP_nguu6MT3U5tsJNMsQ), [X (twitter)](https://x.com/Aibusinessplans/status/1803488217079095460), and [Linkedin](https://www.linkedin.com/company/ai-businessplans/) â€” Try our [Community](https://whop.com/ai-businessplans).\n\nBe safe and make small steps forward every day.\n\nDoug\n\n\n## Read Next \\-\n\nðŸ›† *Investment Disclaimer:* You should not invest money into a paid tool until you have maximized the benefits of the free features. *Nothing in our training products is a promise or guarantee of earnings.*ðŸ‘ˆ\n\nâ˜„ This article contains referral links for some of my absolute favorite AI business tools for content creators and GenAI enthusiasts.\n\nIf you purchase one of my favorite software and AI tools, I will receive a small commission at no additional charge to you.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/choosing-between-llm-agent-frameworks-69019493b259","frontmatter":{"title":"Choosing Between LLM Agent Frameworks","meta_title":"Choosing Between LLM Agent Frameworks","description":"The tradeoffs between building bespoke code-based agents and the major agent frameworks.","date":"2024-10-29T12:57:34.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*jRMs19HqSCazE5dY","categories":["Programming","Technology","Machine Learning"],"author":"Rifx.Online","tags":["agents","frameworks","LangGraph","LlamaIndex","Workflows"],"draft":false,"slug":"blog/choosing-between-llm-agent-frameworks-69019493b259"},"content":"\n### The tradeoffs between building bespoke code\\-based agents and the major agent frameworks.\n\n\n\n\nAgents are having a moment. With multiple new frameworks and fresh [investment](https://foundationcapital.com/goodbye-aiops-welcome-agentsres-the-next-100b-opportunity/) in the space, modern AI agents are overcoming [shaky origins](https://arxiv.org/html/2405.13966v1) to rapidly supplant RAG as an implementation priority. So will 2024 finally be the year that autonomous AI systems that can take over writing our emails, booking flights, talking to our data, or seemingly any other task?\n\nMaybe, but much work remains to get to that point. Any developer building an agent must not only choose foundations â€” which model, use case, and architecture to use â€” but also which framework to leverage. Do you go with the long\\-standing LangGraph, or the newer entrant LlamaIndex Workflows? Or do you go the traditional route and code the whole thing yourself?\n\nThis post aims to make that choice a bit easier. Over the past few weeks, I built the same agent in major frameworks to examine some of the strengths and weaknesses of each at a technical level. All of the code for each agent is available in [this repo](https://github.com/Arize-ai/phoenix/tree/main/examples/agent_framework_comparison).\n\n### Background on the Agent Used for Testing\n\nThe agent used for testing includes function calling, multiple tools or skills, connections to outside resources, and shared state or memory.\n\nThe agent has the following capabilities:\n\n1. Answering questions from a knowledge base\n2. Talking to data: answering questions about telemetry data of an LLM application\n3. Analyzing data: analyzing higher\\-level trends and patterns in retrieved telemetry data\n\nIn order to accomplish these, the agent has three starting skills: RAG with product documentation, SQL generation on a trace database, and data analysis. A simple gradio\\-powered interface is used for the agent UI, with the agent itself structured as a chatbot.\n\n## Code\\-Based Agent (No Framework)\n\nThe first option you have when developing an agent is to skip the frameworks entirely and build the agent fully yourself. When embarking on this project, this was the approach I started with.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*pw9-0lB5JMlVcPqo)\n\n### Pure Code Architecture\n\nThe code\\-based agent below is made up of an OpenAI\\-powered router that uses function calling to select the right skill to use. After that skill completes, it returns back to the router to either call another skill or respond to the user.\n\nThe agent keeps an ongoing list of messages and responses that is passed fully into the router on each call to preserve context through cycles.\n\n```python\ndef router(messages):\n    if not any(\n        isinstance(message, dict) and message.get(\"role\") == \"system\" for message in messages\n    ):\n        system_prompt = {\"role\": \"system\", \"content\": SYSTEM_PROMPT}\n        messages.append(system_prompt)\n\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=messages,\n        tools=skill_map.get_combined_function_description_for_openai(),\n    )\n\n    messages.append(response.choices[0].message)\n    tool_calls = response.choices[0].message.tool_calls\n    if tool_calls:\n        handle_tool_calls(tool_calls, messages)\n        return router(messages)\n    else:\n        return response.choices[0].message.content\n```\n\nThe skills themselves are defined in their own classes (e.g. GenerateSQLQuery) that are collectively held in a SkillMap. The router itself only interacts with the SkillMap, which it uses to load skill names, descriptions, and callable functions. This approach means that adding a new skill to the agent is as simple as writing that skill as its own class, then adding it to the list of skills in the SkillMap. The idea here is to make it easy to add new skills without disturbing the router code.\n\n```python\nclass SkillMap:\n    def __init__(self):\n        skills = [AnalyzeData(), GenerateSQLQuery()]\n\n        self.skill_map = {}\n        for skill in skills:\n            self.skill_map[skill.get_function_name()] = (\n                skill.get_function_dict(),\n                skill.get_function_callable(),\n            )\n\n    def get_function_callable_by_name(self, skill_name) -> Callable:\n        return self.skill_map[skill_name][1]\n\n    def get_combined_function_description_for_openai(self):\n        combined_dict = []\n        for _, (function_dict, _) in self.skill_map.items():\n            combined_dict.append(function_dict)\n        return combined_dict\n\n    def get_function_list(self):\n        return list(self.skill_map.keys())\n\n    def get_list_of_function_callables(self):\n        return [skill[1] for skill in self.skill_map.values()]\n\n    def get_function_description_by_name(self, skill_name):\n        return str(self.skill_map[skill_name][0][\"function\"])\n```\n\nOverall, this approach is fairly straightforward to implement but comes with a few challenges.\n\n### Challenges with Pure Code Agents\n\nThe first difficulty lies in structuring the router system prompt. Often, the router in the example above insisted on generating SQL itself instead of delegating that to the right skill. If youâ€™ve ever tried to get an LLM *not* to do something, you know how frustrating that experience can be; finding a working prompt took many rounds of debugging. Accounting for the different output formats from each step was also tricky. Since I opted not to use structured outputs, I had to be ready for multiple different formats from each of the LLM calls in my router and skills.\n\n### Benefits of a Pure Code Agent\n\nA code\\-based approach provides a good baseline and starting point, offering a great way to learn how agents work without relying on canned agent tutorials from prevailing frameworks. Although convincing the LLM to behave can be challenging, the code structure itself is simple enough to use and might make sense for certain use cases (more in the analysis section below).\n\n## LangGraph\n\nLangGraph is one of the longest\\-standing agent frameworks, first releasing in January 2024\\. The framework is built to address the acyclic nature of existing pipelines and chains by adopting a Pregel graph structure instead. LangGraph makes it easier to define loops in your agent by adding the concepts of nodes, edges, and conditional edges to traverse a graph. LangGraph is built on top of LangChain, and uses the objects and types from that framework.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*fYgHiGwLhSUSrFv9)\n\n### LangGraph Architecture\n\nThe LangGraph agent looks similar to the code\\-based agent on paper, but the code behind it is drastically different. LangGraph still uses a â€œrouterâ€ technically, in that it calls OpenAI with functions and uses the response to continue to a new step. However the way the program moves between skills is controlled completely differently.\n\n```python\ntools = [generate_and_run_sql_query, data_analyzer]\nmodel = ChatOpenAI(model=\"gpt-4o\", temperature=0).bind_tools(tools)\n\ndef create_agent_graph():\n    workflow = StateGraph(MessagesState)\n\n    tool_node = ToolNode(tools)\n    workflow.add_node(\"agent\", call_model)\n    workflow.add_node(\"tools\", tool_node)\n\n    workflow.add_edge(START, \"agent\")\n    workflow.add_conditional_edges(\n        \"agent\",\n        should_continue,\n    )\n    workflow.add_edge(\"tools\", \"agent\")\n\n    checkpointer = MemorySaver()\n    app = workflow.compile(checkpointer=checkpointer)\n    return app\n```\n\nThe graph defined here has a node for the initial OpenAI call, called â€œagentâ€ above, and one for the tool handling step, called â€œtools.â€ LangGraph has a built\\-in object called ToolNode that takes a list of callable tools and triggers them based on a ChatMessage response, before returning to the â€œagentâ€ node again.\n\n```python\ndef should_continue(state: MessagesState):\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    if last_message.tool_calls:\n        return \"tools\"\n    return END\n\ndef call_model(state: MessagesState):\n    messages = state[\"messages\"]\n    response = model.invoke(messages)\n    return {\"messages\": [response]}\n```\n\nAfter each call of the â€œagentâ€ node (put another way: the router in the code\\-based agent), the should\\_continue edge decides whether to return the response to the user or pass on to the ToolNode to handle tool calls.\n\nThroughout each node, the â€œstateâ€ stores the list of messages and responses from OpenAI, similar to the code\\-based agentâ€™s approach.\n\n### Challenges with LangGraph\n\nMost of the difficulties with LangGraph in the example stem from the need to use Langchain objects for things to flow nicely.\n\n**Challenge \\#1: Function Call Validation**\n\nIn order to use the ToolNode object, I had to refactor most of my existing Skill code. The ToolNode takes a list of callable functions, which originally made me think I could use my existing functions, however things broke down due to my function parameters.\n\nThe skills were defined as classes with a callable member function, meaning they had â€œselfâ€ as their first parameter. GPT\\-4o was smart enough to not include the â€œselfâ€ parameter in the generated function call, however LangGraph read this as a validation error due to a missing parameter.\n\nThis took hours to figure out, because the error message instead marked the third parameter in the function (â€œargsâ€ on the data analysis skill) as the missing parameter:\n\n```python\npydantic.v1.error_wrappers.ValidationError: 1 validation error for data_analysis_toolSchema\nargs field required (type=value_error.missing)\n```\n\nIt is worth mentioning that the error message originated from Pydantic, not from LangGraph.\n\nI eventually bit the bullet and redefined my skills as basic methods with Langchainâ€™s @tool decorator, and was able to get things working.\n\n```python\n@tool\ndef generate_and_run_sql_query(query: str):\n    \"\"\"Generates and runs an SQL query based on the prompt.\n\n    Args:\n        query (str): A string containing the original user prompt.\n\n    Returns:\n        str: The result of the SQL query.\n    \"\"\"\n```\n\n**Challenge \\#2: Debugging**\n\nAs mentioned, debugging in a framework is difficult. This primarily comes down to confusing error messages and abstracted concepts that make it harder to view variables.\n\nThe abstracted concepts primarily show up when trying to debug the messages being sent around the agent. LangGraph stores these messages in state\\[â€œmessagesâ€]. Some nodes within the graph pull from these messages automatically, which can make it difficult to understand the value of messages when they are accessed by the node.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*KuCg0WGHSklOKe6t)\n\n### LangGraph Benefits\n\nOne of the main benefits of LangGraph is that itâ€™s easy to work with. The graph structure code is clean and accessible. Especially if you have complex node logic, having a single view of the graph makes it easier to understand how the agent is connected together. LangGraph also makes it straightforward to convert an existing application built in LangChain.\n\n### Takeaway\n\nIf you use everything in the framework, LangGraph works cleanly; if you step outside of it, prepare for some debugging headaches.\n\n## LlamaIndex Workflows\n\nWorkflows is a newer entrant into the agent framework space, premiering earlier this summer. Like LangGraph, it aims to make looping agents easier to build. Workflows also has a particular focus on running asynchronously.\n\nSome elements of Workflows seem to be in direct response to LangGraph, specifically its use of events instead of edges and conditional edges. Workflows use steps (analogous to nodes in LangGraph) to house logic, and emitted and received events to move between steps.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*22WuFVBWctdeiCSL)\n\nThe structure above looks similar to the LangGraph structure, save for one addition. I added a setup step to the Workflow to prepare the agent context, more on this below. Despite the similar structure, there is very different code powering it.\n\n### Workflows Architecture\n\nThe code below defines the Workflow structure. Similar to LangGraph, this is where I prepared the state and attached the skills to the LLM object.\n\n```python\nclass AgentFlow(Workflow):\n    def __init__(self, llm, timeout=300):\n        super().__init__(timeout=timeout)\n        self.llm = llm\n        self.memory = ChatMemoryBuffer(token_limit=1000).from_defaults(llm=llm)\n        self.tools = []\n        for func in skill_map.get_function_list():\n            self.tools.append(\n                FunctionTool(\n                    skill_map.get_function_callable_by_name(func),\n                    metadata=ToolMetadata(\n                        name=func, description=skill_map.get_function_description_by_name(func)\n                    ),\n                )\n            )\n\n    @step\n    async def prepare_agent(self, ev: StartEvent) -> RouterInputEvent:\n        user_input = ev.input\n        user_msg = ChatMessage(role=\"user\", content=user_input)\n        self.memory.put(user_msg)\n\n        chat_history = self.memory.get()\n        return RouterInputEvent(input=chat_history)\n```\n\nThis is also where I define an extra step, â€œprepare\\_agentâ€. This step creates a ChatMessage from the user input and adds it to the workflow memory. Splitting this out as a separate step means that we do return to it as the agent loops through steps, which avoids repeatedly adding the user message to the memory.\n\nIn the LangGraph case, I accomplished the same thing with a run\\_agent method that lived outside the graph. This change is mostly stylistic, however itâ€™s cleaner in my opinion to house this logic with the Workflow and graph as weâ€™ve done here.\n\nWith the Workflow set up, I then defined the routing code:\n\n```python\n@step\nasync def router(self, ev: RouterInputEvent) -> ToolCallEvent | StopEvent:\n    messages = ev.input\n\n    if not any(\n        isinstance(message, dict) and message.get(\"role\") == \"system\" for message in messages\n    ):\n        system_prompt = ChatMessage(role=\"system\", content=SYSTEM_PROMPT)\n        messages.insert(0, system_prompt)\n\n    with using_prompt_template(template=SYSTEM_PROMPT, version=\"v0.1\"):\n        response = await self.llm.achat_with_tools(\n            model=\"gpt-4o\",\n            messages=messages,\n            tools=self.tools,\n        )\n\n    self.memory.put(response.message)\n\n    tool_calls = self.llm.get_tool_calls_from_response(response, error_on_no_tool_call=False)\n    if tool_calls:\n        return ToolCallEvent(tool_calls=tool_calls)\n    else:\n        return StopEvent(result=response.message.content)\n```\n\nAnd the tool call handling code:\n\n```python\n@step\nasync def tool_call_handler(self, ev: ToolCallEvent) -> RouterInputEvent:\n    tool_calls = ev.tool_calls\n\n    for tool_call in tool_calls:\n        function_name = tool_call.tool_name\n        arguments = tool_call.tool_kwargs\n        if \"input\" in arguments:\n            arguments[\"prompt\"] = arguments.pop(\"input\")\n\n        try:\n            function_callable = skill_map.get_function_callable_by_name(function_name)\n        except KeyError:\n            function_result = \"Error: Unknown function call\"\n\n        function_result = function_callable(arguments)\n        message = ChatMessage(\n            role=\"tool\",\n            content=function_result,\n            additional_kwargs={\"tool_call_id\": tool_call.tool_id},\n        )\n\n        self.memory.put(message)\n\n    return RouterInputEvent(input=self.memory.get())\n```\n\nBoth of these look more similar to the code\\-based agent than the LangGraph agent. This is mainly because Workflows keeps the conditional routing logic in the steps as opposed to in conditional edges â€” lines 18â€“24 were a conditional edge in LangGraph, whereas now they are just part of the routing step â€” and the fact that LangGraph has a ToolNode object that does just about everything in the tool\\_call\\_handler method automatically.\n\nMoving past the routing step, one thing I was very happy to see is that I could use my SkillMap and existing skills from my code\\-based agent with Workflows. These required no changes to work with Workflows, which made my life much easier.\n\n### Challenges with Workflows\n\n**Challenge \\#1: Sync vs Async**\n\nWhile asynchronous execution is preferable for a live agent, debugging a synchronous agent is much easier. Workflows is designed to work asynchronously, and trying to force synchronous execution was very difficult.\n\nI initially thought I would just be able to remove the â€œasyncâ€ method designations and switch from â€œachat\\_with\\_toolsâ€ to â€œchat\\_with\\_toolsâ€. However, since the underlying methods within the Workflow class were also marked as asynchronous, it was necessary to redefine those in order to run synchronously. I ended up sticking to an asynchronous approach, but this didnâ€™t make debugging more difficult.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*78Hzqkiv9cI7W4UA)\n\n**Challenge \\#2: Pydantic Validation Errors**\n\nIn a repeat of the woes with LangGraph, similar problems emerged around confusing Pydantic validation errors on skills. Fortunately, these were easier to address this time since Workflows was able to handle member functions just fine. I ultimately just ended up having to be more prescriptive in creating LlamaIndex FunctionTool objects for my skills:\n\n```python\nfor func in skill_map.get_function_list(): \n            self.tools.append(FunctionTool(\n                skill_map.get_function_callable_by_name(func), \n                metadata=ToolMetadata(name=func, description=skill_map.get_function_description_by_name(func))))\n```\n\n*Excerpt from AgentFlow.\\_\\_init\\_\\_ that builds FunctionTools*\n\n### Benefits of Workflows\n\nI had a much easier time building the Workflows agent than I did the LangGraph agent, mainly because Workflows still required me to write routing logic and tool handling code myself instead of providing built\\-in functions. This also meant that my Workflow agent looked extremely similar to my code\\-based agent.\n\nThe biggest difference came in the use of events. I used two custom events to move between steps in my agent:\n\n```python\nclass ToolCallEvent(Event):\n    tool_calls: list[ToolSelection]\n\nclass RouterInputEvent(Event):\n    input: list[ChatMessage]\n```\n\nThe emitter\\-receiver, event\\-based architecture took the place of directly calling some of the methods in my agent, like the tool call handler.\n\nIf you have more complex systems with multiple steps that are triggering asynchronously and might emit multiple events, this architecture becomes very helpful to manage that cleanly.\n\nOther benefits of Workflows include the fact that it is very lightweight and doesnâ€™t force much structure on you (aside from the use of certain LlamaIndex objects) and that its event\\-based architecture provides a helpful alternative to direct function calling â€” especially for complex, asynchronous applications.\n\n## Comparing Frameworks\n\nLooking across the three approaches, each one has its benefits.\n\nThe no framework approach is the simplest to implement. Because any abstractions are defined by the developer (i.e. SkillMap object in the above example), keeping various types and objects straight is easy. The readability and accessibility of the code entirely comes down to the individual developer however, and itâ€™s easy to see how increasingly complex agents could get messy without some enforced structure.\n\nLangGraph provides quite a bit of structure, which makes the agent very clearly defined. If a broader team is collaborating on an agent, this structure would provide a helpful way of enforcing an architecture. LangGraph also might provide a good starting point with agents for those not as familiar with the structure. There is a tradeoff, however â€” since LangGraph does quite a bit for you, it can lead to headaches if you donâ€™t fully buy into the framework; the code may be very clean, but you may pay for it with more debugging.\n\nWorkflows falls somewhere in the middle. The event\\-based architecture might be extremely helpful for some projects, and the fact that less is required in terms of using of LlamaIndex types provides greater flexibility for those not be fully using the framework across their application.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*PITmiVGuG8QuDVX6)\n\nUltimately, the core question may just come down to â€œare you already using LlamaIndex or LangChain to orchestrate your application?â€ LangGraph and Workflows are both so entwined with their respective underlying frameworks that the additional benefits of each agent\\-specific framework might not cause you to switch on merit alone.\n\nThe pure code approach will likely always be an attractive option. If you have the rigor to document and enforce any abstractions created, then ensuring nothing in an external framework slows you down is easy.\n\n## Key Questions To Help In Choosing An Agent Framework\n\nOf course, â€œit dependsâ€ is never a satisfying answer. These three questions should help you decide which framework to use in your next agent project.\n\n***Are you already using LlamaIndex or LangChain for significant pieces of your project?***\n\nIf yes, explore that option first.\n\n***Are you familiar with common agent structures, or do you want something telling you how you should structure your agent?***\n\nIf you fall into the latter group, try Workflows. If you *really* fall into the latter group, try LangGraph.\n\n***Has your agent been built before?***\n\nOne of the framework benefits is that there are many tutorials and examples built with each. There are far fewer examples of pure code agents to build from.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*wF9aSF1db1yaniqO)\n\n## Conclusion\n\nPicking an agent framework is just one choice among many that will impact outcomes in production for generative AI systems. As always, it pays to have robust guardrails and [LLM tracing](https://docs.arize.com/phoenix/tracing/llm-traces) in place â€” and to be agile as new agent frameworks, research, and models upend established techniques.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/claude-3-5-haiku-anthropics-speed-demon-gets-a-brain-boost-82f2f0999d4f","frontmatter":{"title":"Claude 3.5 Haiku: Anthropicâ€™s Speed Demon Gets a Brain Boost","meta_title":"Claude 3.5 Haiku: Anthropicâ€™s Speed Demon Gets a Brain Boost","description":"Claude 3.5 Haiku, Anthropic's latest AI model, combines speed and intelligence, outperforming its predecessor, Claude 3 Opus, on various benchmarks. It excels in coding tasks, achieving a notable 40.6% on the SWE-bench Verified test. While it remains text-only for now, plans for image analysis are in the works. The model is accessible via multiple platforms but comes at a premium cost, four times that of its predecessor, though cost-saving options like prompt caching exist. Its applications span software development, chatbots, data processing, education, and more, marking a significant advancement in AI capabilities.","date":"2024-11-13T01:32:04.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*hLedIfhYJhS_ejPDwOQPIw.png","categories":["Programming","Machine Learning","Chatbots"],"author":"Rifx.Online","tags":["Claude","Haiku","coding","SWE-bench","benchmarks"],"draft":false,"slug":"blog/claude-3-5-haiku-anthropics-speed-demon-gets-a-brain-boost-82f2f0999d4f"},"content":"\n\n\n\n\n\nIn the relentless race of AI advancement, Anthropic has just dropped a new contender into the ring. Meet Claude 3\\.5 Haiku, the latest iteration of their fastest AI model. Itâ€™s like theyâ€™ve taken their sprinter and sent them to brain camp. The result? A model thatâ€™s not just quick on its feet but can now outsmart its beefier siblings in certain intellectual arenas. Letâ€™s dive into what makes this new kid on the block tick.\n\n\n## The Need for Speed (and Smarts)\n\nAnthropicâ€™s previous Haiku model was already the Usain Bolt of their AI lineup. Now, theyâ€™ve somehow managed to cram more brainpower into this speed demon without sacrificing its swiftness. Itâ€™s like watching a cheetah solve a Rubikâ€™s cube while sprinting.\n\n\n## Benchmarking Brilliance\n\nClaude 3\\.5 Haiku isnâ€™t just fast; itâ€™s scary smart. Itâ€™s outperforming Claude 3 Opus â€” Anthropicâ€™s previous heavyweight champ â€” on various intelligence benchmarks. This isnâ€™t just a minor upgrade; itâ€™s a leap that has the AI community sitting up and taking notice.\n\n\n## Coding Prowess\n\nIf youâ€™re a developer, listen up. This model is flexing hard in the coding arena, scoring a jaw\\-dropping 40\\.6% on the SWE\\-bench Verified test. Thatâ€™s not just impressive; itâ€™s the kind of performance that makes human coders nervously eye their job security.\n\n\n## Under the Hood\n\nLetâ€™s pop the hood and see whatâ€™s powering this AI hot rod:\n\n* **Availability**: You can take it for a spin through Anthropicâ€™s API, Amazon Bedrock, or Google Cloudâ€™s Vertex AI. Itâ€™s like the AI equivalent of being available on all major streaming platforms.\n* **Input**: Currently, itâ€™s text\\-only. No image analysis yet, but thatâ€™s coming. Itâ€™s like having a genius pen pal who canâ€™t look at your vacation photos.\n* **Knowledge Cutoff**: July 2024\\. So it knows about that embarrassing thing you did last summer, but not about next yearâ€™s memes.\n* **Output Length**: Improved from its predecessor. It can now write longer essays to procrastinate on your behalf.\n\n\n## Show Me the Money\n\nNow, hereâ€™s where things get interesting. Anthropic has decided to charge a premium for this upgraded model:\n\n* $1 per million input tokens\n* $5 per million output tokens\n\nThatâ€™s a fourfold increase from the previous version. Itâ€™s like theyâ€™ve taken their Honda Civic, turned it into a Tesla, and adjusted the price accordingly.\n\nBut fear not, penny\\-pinchers! There are ways to save:\n\n* Prompt caching can save you up to 90%. Itâ€™s like extreme couponing for AI.\n* Batch processing with the Message Batches API can cut costs by up to 50%. Bulk buying, but for computation.\n\n\n## What Can This Thing Do?\n\nClaude 3\\.5 Haiku isnâ€™t just a party trick. Itâ€™s got some serious real\\-world applications:\n\n* **Software Development**: Itâ€™s like having a coding buddy who never sleeps and doesnâ€™t steal your snacks.\n* **Chatbots**: Customer service reps who donâ€™t need coffee breaks or HR interventions.\n* **Data Processing**: It can crunch numbers faster than you can say â€œbig data.â€\n* **Education**: A tutor thatâ€™s always on call and never loses patience.\n* **Personalization**: It remembers your preferences better than your significant other.\n* **Specialized Tasks**: The Swiss Army knife of AI sub\\-agents.\n* **Content Moderation**: Keeping the internet clean, one post at a time.\n\n\n## The Trade\\-Offs\n\nNow, itâ€™s not all sunshine and rainbows. There are a few catches:\n\n* No image analysis yet. So it canâ€™t tell you if that dress makes you look fat.\n* The price hike might make some users stick with the older, cheaper version. Itâ€™s the AI equivalent of people still using Windows 7\\.\n\n\n## The Bottom Line\n\nClaude 3\\.5 Haiku is a significant leap forward in the world of AI. Itâ€™s faster than a speeding bullet, more powerful than a locomotive, and able to leap tall buildings in a single bound. Okay, maybe not that last part, but you get the idea.\n\nFor developers and businesses looking to leverage AI for complex tasks that require both brains and brawn (or in this case, speed), Claude 3\\.5 Haiku is a compelling option. Itâ€™s not just an upgrade; itâ€™s a reimagining of whatâ€™s possible at the intersection of speed and intelligence in AI.\n\nThe question now is: how will competitors respond? And more importantly, how long until we see Claude 4\\.0: The Limerick Edition?\n\n\n## FAQ Section\n\n**Q: Can Claude 3\\.5 Haiku analyze images?**A: Not yet, but Anthropic plans to add this feature in the future. For now, itâ€™s text\\-only.\n\n**Q: How much more expensive is Claude 3\\.5 Haiku compared to its predecessor?**A: Itâ€™s four times more expensive, but there are ways to reduce costs through prompt caching and batch processing.\n\n**Q: Whatâ€™s the most impressive feature of Claude 3\\.5 Haiku?**A: Its ability to outperform larger models like Claude 3 Opus on various intelligence benchmarks while maintaining high speed.\n\n**Q: Can I use Claude 3\\.5 Haiku for software development?**A: Absolutely. It excels at coding tasks and can provide fast, accurate code suggestions and completions.\n\n**Q: Is Claude 3\\.5 Haiku available to the public?**A: Yes, itâ€™s accessible through Anthropicâ€™s API, Amazon Bedrock, and Google Cloudâ€™s Vertex AI.\n\n\\#Claude35Haiku \\#AnthropicAI \\#AIInnovation \\#MachineLearning \\#AIForDevelopers \\#FutureOfAI \\#AIPerformance \\#TechInnovation\n\nâ€œClaude 3\\.5 Haiku performance benchmarksâ€, â€œAI model pricing comparisonâ€, â€œFast AI models for software developmentâ€, â€œAnthropic AI model capabilitiesâ€, â€œCost\\-effective AI implementation strategiesâ€\n\n\n"},{"lang":"en","group":"blog","slug":"blog/claude-3-5-sonnet-new-pioneering-the-future-of-ai-with-computer-control-capabilities-37a6ff9f9033","frontmatter":{"title":"Claude 3.5 Sonnet (New): Pioneering the Future of AI with Computer Control Capabilities","meta_title":"Claude 3.5 Sonnet (New): Pioneering the Future of AI with Computer Control Capabilities","description":"Anthropic has unveiled its latest AI model, Claude 3.5 Sonnet, on October 22, 2024. This release introduces revolutionary computer controlâ€¦","date":"2024-10-27T13:57:00.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*n0NkOFbhUm7_fllJ","categories":["Programming","Technology","Generative AI"],"author":"Rifx.Online","tags":["Claude","Sonnet","automation","benchmarks","safety"],"draft":false,"slug":"blog/claude-3-5-sonnet-new-pioneering-the-future-of-ai-with-computer-control-capabilities-37a6ff9f9033"},"content":"\n\n\n\n\n\nAnthropic has unveiled its latest AI model, Claude 3.5 Sonnet, on October 22, 2024. This release introduces revolutionary computer control capabilities and substantial improvements across various benchmarks, setting new standards in the AI industry.\n\n\n## Revolutionary Computer Control: A New Frontier\n\nThe standout feature of Claude 3.5 Sonnet is its ability to interact with computers just like humans do. This groundbreaking capability allows the AI to:\n\n* Navigate desktop interfaces using mouse and keyboard inputs\n* Interact with various applications and web browsers\n* Execute complex multi-step tasks\n* Perform file management operations\n* Automate repetitive workflows\n\nThis computer control feature, currently in public beta, represents a paradigm shift in how AI systems can interact with digital interfaces. While still in its experimental phase, early testing shows promising results, with Claude 3.5 Sonnet scoring 14.9% on the OSWorld benchmark for screenshot-only tasks â€” significantly higher than the next-best systemâ€™s 7.8%.\n\n\n## Benchmark-Breaking Performance\n\nThe upgraded model demonstrates remarkable improvements across various metrics:\n\n\n## Coding and Technical Tasks\n\n* 49% performance on SWE-bench Verified (up from 33.4%)\n* 93.7% score on HumanEval coding tasks\n* Superior performance in software engineering compared to specialized coding systems\n\n\n## Academic and Reasoning Capabilities\n\n* 65% on graduate-level reasoning (GPQA-Diamond)\n* 78% on undergraduate-level knowledge (MMLU Pro)\n* 78.3% on mathematical problem-solving (MATH)\n\n\n## Business Applications\n\n* 69.2% on retail domain tasks (TAU-bench)\n* 46% on airline domain tasks\n* 90.8% accuracy on chart analysis\n* 94.2% accuracy on document Q&A\n\n\n## Enterprise Integration and Availability\n\nClaude 3.5 Sonnet is accessible through multiple platforms:\n\n* Anthropic API\n* Amazon Bedrock\n* Google Cloudâ€™s Vertex AI\n\nMajor companies including Asana, Canva, DoorDash, and Replit have already begun implementing Claude 3.5 Sonnetâ€™s capabilities in their workflows, particularly leveraging its computer control features for complex automation tasks.\n\n\n## Practical Applications\n\n\n## Software Development\n\n* Automated code testing and debugging\n* Intelligent IDE interactions\n* Code review and optimization\n* Documentation generation\n\n\n## Customer Support\n\n* Advanced chatbot capabilities\n* Visual data interpretation\n* Automated ticket resolution\n* Process automation\n\n\n## Business Operations\n\n* Document processing and analysis\n* Data extraction from visual sources\n* Workflow automation\n* Complex problem-solving\n\n\n## Safety and Responsibility\n\nAnthropic has implemented robust safety measures for the computer control feature:\n\n* New classifiers to identify potential misuse\n* Proactive monitoring systems\n* Restricted access to sensitive operations\n* Regular safety assessments\n\n\n## Looking Ahead\n\nWhile Claude 3.5 Sonnet represents a significant advancement in AI capabilities, itâ€™s important to note that some features, particularly computer control, are still in their early stages. Certain actions like scrolling, dragging, and zooming present challenges, and Anthropic encourages developers to begin with low-risk tasks while exploring these new capabilities.\n\nThe release of Claude 3.5 Sonnet marks a pivotal moment in AI development, combining advanced reasoning capabilities with practical computer control features. As the technology continues to evolve, we can expect to see even more innovative applications and improvements in how AI systems interact with our digital world.\n\n*This article is based on official announcements and documentation from Anthropic, AWS, and various technology partners. For the most up-to-date information, please refer to Anthropicâ€™s official documentation.*\n\n\n"},{"lang":"en","group":"blog","slug":"blog/claude-3-5-sonnet-v-s-gpt-4o-which-one-is-better-3b3675195bf9","frontmatter":{"title":"Claude 3.5 Sonnet V/S GPT-4O: Which one is better","meta_title":"Claude 3.5 Sonnet V/S GPT-4O: Which one is better","description":"In November 2022, OpenAI launched ChatGPT, a model that has revolutionized how we search and interact with information. Next year, inâ€¦","date":"2024-10-27T13:59:09.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*4MXLuSFfGwFkWWn0","categories":["Generative AI","Machine Learning","Natural Language Processing"],"author":"Rifx.Online","tags":["GPT-4o","Claude","multimodal","reasoning","code-generation"],"draft":false,"slug":"blog/claude-3-5-sonnet-v-s-gpt-4o-which-one-is-better-3b3675195bf9"},"content":"\n\n\n\nIn November 2022, OpenAI launched ChatGPT, a model that has revolutionized how we search and interact with information. Next year, in March, an American startup,â€ Anthropic,â€ founded by ex-OpenAI employees, launched their own AI model, â€œClaude.â€ Since the launch, both AI companies have been competing to bring the best to customers regarding features and experience through their AI models. Recently, OpenAI launched â€œGPT-4o,â€ a spectacular model that handles file, voice, and video data amazingly. Similarly, Claude launched the â€œClaude 3.5 Sonnet,â€ which is the most advanced AI model, as they claimed, and can handle complex tasks. In this article, we will determine which is better, between Claude 3.5 Sonnet and GPT-4o, and compare its features and output with the same input to check which is better for you.\n\n\n## Capabilities and Features\n\n\n### GPT-4o\n\n\n\nGPT-4o is the latest LLM launched by OpenAI. The â€œoâ€ stands for omni, which means â€œeveryâ€ in Latin. This model can analyze voice, images, videos, and files as input and respond accordingly. It can take voice input and give the output in different charactersâ€™ voices, including tones, emotions, etc. The whole process is as low as a human conversation, with an average of 0.32 seconds compared to other voice models, which is 2.8 seconds. It also allows users to generate written content such as articles, blogs, product descriptions, code in different programming languages, data analysis, charts, etc. In addition, GPT-4o can also analyze images and videos, which makes the model act as a language translator, personal assistant, virtual teacher, or shopping assistant. It can also be used in medicine, engineering, the military, etc. To use this feature, GPT-4o can use the userâ€™s camera to get a real-time view and respond accordingly in the voice mode. It can also access your computer screen and describe what is shown on the screen, users can ask questions related to the stuff displayed on the screen.\n\n*For example, users can enable the model on the screen, open the VS code, and prompt the model to act as a coding assistant to get answers to the coding problems. Alternatively, you can enable the camera to act as a fitness trainer whether you are doing it correctly or not.*\n\nThe model has unique features, such as data analysis, code interpreter, and real-time web browsing, making it different from its competitors. The model also has a plethora of GPTs, which is a tailored version of ChatGPT.\n\n\n### Claude 3.5 Sonnet\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*BSMcOpvWZ5lUm4Tl)\n\nClaude 3.5 Sonnet is the AI chatbot launched by Anthropic. It is the third generation of the family of Claude AI model series. This model has stood at a high bae and outperformed many AI models on various evaluations, keeping the hallucinations and wrong information away. While it doesnâ€™t support voice and video features like GPT-4o, it can also perform all the basic tasks, such as text generation and code generation in different programming languages, brainstorming ideas, etc. According to the report by Anthropic, Claude 3.5 Sonnet is one of the best computer vision models in the market, which can be used to analyze charts and graphs, transcribe texts from images, and many more. Claude is powered by an advanced feature, â€œArtifacts,â€ a special popup window along the conversation, allowing the users to check the code snippets, text documents, or website designs and allow them to edit the output in real-time.\n\n*For example, users can use computer vision and artifacts in their workflows. Users can make essential prototyping of a websiteâ€™s design on paper, attach the file with Claude 3.5 Sonnet, and prompt it to design a website based on the prototype. The generated code and the website design appear in the artifacts. Users can edit the code and the design according to their requirements. Users can also publish their projects live on the Internet.*\n\n\n## Head-to-Head Comparison\n\nIn this section, we will compare the two LLMs based on factors such as complex reasoning and code generation, check out their capabilities in handling complex tasks, and see which model is best.\n\n* **Graduate Level Reasoning(GPQA, Diamond)**This factor evaluates the modelsâ€™ ability to handle complex, high-level reasoning tasks at a graduate level of education. In this task, researchers compare the model on the GPQA test, a set of 448 questions in different fields designed by experts. These questions are Google Proof, so anyone canâ€™t find them online. The Claude score is nearly 59.4%, while the GPT-4o scores only 53.6%. Both the scores are relatively close, but as we can see, Claude could be a better option in tasks that require advanced analytical thinking, such as research analysis, complex problem solving, and high academic level problems.\n* **Undergraduate level knowlege(MMLU)**The MMLU, which means Massive Multitask Language Understanding, is a benchmark that explains the general knowledge understanding of any AI model across various subjects at an undergraduate level. Claude 3.5 Sonnet scores 88.3% in this experiment, and the GPT-4o scores 88.7%. This shows how both LLMs have trained in various domains and have a deeper understanding of them. It makes the AI model a well-suited tool for general knowledge tasks, basic tutoring of multiple subjects, etc.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*A4w-tvsxcmFINaQT)\n\n* **Code(HumanEval)**HumanEval is a benchmark that evaluates the modelâ€™s ability to generate, understand, and debug code. This benchmark is where Claude 3.5 Sonnet achieves 92%, and GPT-4o scores 90.2%. Claude 3.5 Sonnet results are spectacular in this task as it provides a better coding environment, â€œArtifacts,â€ and better code generation than GPT-4o. Claude allows the users to design, edit, and run the code in the Artifacts pop-up window. After the launch of Claude 3.5 Sonnet, everyone is developing tools, websites, and basic games and sharing them across the internet. On the other hand, GPT-4o also scored well, but it does not have any coding environment in its interface, so the developers must do too much hassle as the code generated by it is too much hassle to get to the result.\n* **Reasoning Over Text(DROP, FLscore)**The DROP(Discrete Reasoning Over Paragraphs) is the benchmark that measures the modelâ€™s ability to understand complex text information. In this challenge, the Claude 3.5 Sonnet scores 87.1%, while the GPT-4o scores 83.4%. This shows that the Claude 3.5 Sonnet is better and more effective for the task, which involves detailed text analysis, text review, complex question-answering systems, etc.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Kcy7sFb2FYpbfrwp)\n\n* **Math problem solving(MATH)**This test evaluates the ability of any AI model to solve various mathematical problems. Claude 3.5 Sonnet scores just 71.1%, while the GPT-4o scores 76.6%. These scores make the GPT-4o a better model for mathematical problem-solving tasks and can be used for mathematical computations such as financial modeling, scientific calculations, and advanced data analysis.\n* **Multilingual Maths (MSGM)**This factor describes the ability of any AI model to solve mathematical problems in multiple languages. Both models get scores close to each other: GPT-4o 90.5% and Claude 3.5 Sonnet 91.6%. This shows that both models perform excellently, with Claude slightly better. The capability is particularly helpful for educational applications or any scenario where mathematical reasoning needs to be communicated across language barriers.\n* **Visual question answering(MMU/val)**This factor describes the LLMâ€™s capability to analyze the information presented in images. The GPT-4o outperforms Claudeâ€™s 3.5 Sonnet in this benchmark with 69.1% and 68.3%, respectively. On the other hand, when analyzing text from the document, Claudeâ€™s 3.5 Sonnet score is 95.2% compared to GPT-4oâ€™s 92.1%.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*xzjqBV2YL0lVFitX)\n\n* **Image Generation**Image Generation is the ability of the LLMs to generate images from the text. GPT-4o is integrated with DallE-2 and can produce images with the help of text, and the results are excellent. On the other hand, Claude 3.5 Sonnet cannot create any images. This feature also helps GPT-4o design websites and references better, as it is trained on many images.\n* **Knowledge Cutoff**Here, both the models trained on a limited data set till a specific date. Claude 3.5 Sonnet trained on data till April 2024, while the other hand, GPT-4o trained on data till 2024. The real advantage of GPT-4o is that it has real-time web browsing, which helps the LLM train on new data regularly.\n\n\n## Pros of GPT-4o:\n\n* Handles voice, images, and video input.\n* Real-time web browsing capability.\n* Faster response time (0.32 seconds average).\n* Superior in math problem-solving.\n* Can generate images using DALL-E 2.\n\n\n## Cons of GPT-4o:\n\n* Slightly lower performance in graduate-level reasoning.\n* No built-in coding environment.\n* A lower score in document visual Q&A.\n* Slightly behind in code generation capabilities.\n* Less effective in detailed text analysis.\n\n\n## Pros Claude 3.5 Sonnet:\n\n* Excels in graduate-level reasoning.\n* Superior code generation and built-in â€œArtifactsâ€ feature.\n* Better performance in detailed text analysis.\n* A higher score in document visual Q&A.\n* Slightly better in multilingual math.\n\n\n## Cons Claude 3.5 Sonnet:\n\n* Cannot handle voice or video input.\n* No image generation capability.\n* Slightly lower performance in visual question-answering.\n* Cannot access real-time web information.\n* Weaker in math problem-solving.\n\n\n## Conclusion\n\nGPT-4o and Claude 3.5 Sonnet demonstrate impressive capabilities across various tasks, each with its strengths. GPT-4o excels in multimodal inputs, real-time information access, and image generation, making it versatile for diverse applications. Claude 3.5 Sonnet shines in complex reasoning, code generation, and detailed text analysis, offering superior performance in specific academic and professional contexts. The choice between these models depends on the specific use case and required features. We can expect further improvements and specialized models catering to different needs as AI technology advances.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/comparative-study-of-langgraph-autogen-and-crewai-for-building-multi-agent-systems-0e7e47f9078e","frontmatter":{"title":"Comparative Study of LangGraph, Autogen, and Crewai for Building Multi-Agent Systems","meta_title":"Comparative Study of LangGraph, Autogen, and Crewai for Building Multi-Agent Systems","description":"As we venture into the realm of multi-agent systems (MAS), itâ€™s essential to understand the diverse programming languages designedâ€¦","date":"2024-11-13T01:22:29.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*DBlLuCOA3lWIg6RmpMPg8A.png","categories":["Programming","Technology","Machine Learning"],"author":"Rifx.Online","tags":["LangGraph","Autogen","Crewai","multi-agent","scalability"],"draft":false,"slug":"blog/comparative-study-of-langgraph-autogen-and-crewai-for-building-multi-agent-systems-0e7e47f9078e"},"content":"\n\n\n\nAs we venture into the realm of multi\\-agent systems (MAS), itâ€™s essential to understand the diverse programming languages designed specifically for this purpose. In this article, weâ€™ll delve into the world of MAS development by comparing LangGraph, Autogen, and Crewai â€” three prominent players in the field.\n\n\n## Introduction\n\nMulti\\-agent systems (MAS) have become increasingly important in various industries. A MAS is a system composed of multiple intelligent agents that interact with each other and their environment to achieve specific goals. Among the many frameworks available for building MAS, LangGraph, Autogen, and Crewai are some of the most popular choices.\n\nAs developers or researchers working on a MAS project, choosing the right framework can be overwhelming, especially considering factors such as ease of use, scalability, customization, and integration with AI libraries. This article provides a comparative study of LangGraph, Autogen, and Crewai, highlighting their strengths, weaknesses, and suitability for different applications.\n\n\n### Introduction to Each Framework\n\n\n## LangGraph: An Open\\-Source Framework\n\n**Strengths**:\n\n* **Ease of use**: LangGraph provides a simple and intuitive API, making it easy for developers to integrate with their existing systems.\n* **Scalability**: LangGraph supports large\\-scale distributed systems, allowing users to handle complex tasks.\n* **Integration with AI Libraries**: LangGraph is compatible with popular AI libraries such as TensorFlow, PyTorch, and Keras.\n\n**Limitations**:\n\n* Limited support for distributed systems\n* Less flexible than Autogen and Crewai\n\n\n## Autogen: A Modular Open\\-Source Framework\n\n**Strengths**:\n\n* **High flexibility**: Autogen provides a modular architecture, allowing users to customize their MAS to fit specific needs.\n* **Suitability for complex applications**: Autogenâ€™s modularity makes it well\\-suited for large\\-scale systems with multiple interconnected agents.\n* **Strong community support**: Autogen has an active community of developers and researchers who contribute to the framework and provide support.\n\n**Limitations**:\n\n* Steeper learning curve\n* Requires more resources\n\n\n## Crewai: A Scalable, Data\\-Driven Framework\n\n**Strengths**:\n\n* **Scalability**: Crewai provides excellent support for large\\-scale systems, making it well\\-suited for applications that require processing of vast amounts of data.\n* **Ease of use**: Crewai offers a simple API, making it easy to integrate with existing systems.\n* **Integration with cloud services**: Crewai allows users to easily deploy their MAS on cloud platforms such as AWS and Azure.\n\n**Limitations**:\n\n* Limited support for custom models\n* Less flexible than Autogen\n\n\n## Comparison Matrix\n\n\n\n\n## Conclusion\n\nIn conclusion, each framework has its unique strengths and weaknesses. LangGraph offers ease of use and scalability, Autogen provides flexibility and customizability, while Crewai excels in data\\-driven approach and scalability.\n\nWhen choosing a framework for building a MAS, consider the specific requirements of your project:\n\n* **Ease of use**: Choose LangGraph if you prioritize simplicity and scalability.\n* **Flexibility**: Select Autogen for complex applications that require customization.\n* **Scalability**: Consider Crewai for large\\-scale systems with massive data processing needs.\n\nBy understanding the strengths and weaknesses of each framework, developers can make informed decisions about which MAS to build on, ultimately leading to more effective and efficient solutions.\n\n\n## Additional Resources\n\nFor further reading and resources, please see:\n\n* [LangGraph Documentation](https://proxy.rifx.online/https://langgraph.com/documentation/)\n* [Autogen Tutorials](https://proxy.rifx.online/https://autogen.com/tutorials)\n* [Crewai API Reference](https://proxy.rifx.online/https://crewai.com/api-reference/)\n\n"},{"lang":"en","group":"blog","slug":"blog/comparing-leading-text-to-image-image-generation-models-for-adding-text-to-images-7dc001f491ef","frontmatter":{"title":"Comparing Leading Text-to-Image Generation Models for Adding Text to Images","meta_title":"Comparing Leading Text-to-Image Generation Models for Adding Text to Images","description":"This article evaluates the text generation capabilities of nine leading text-to-image models, focusing on their ability to accurately render text within images based on specific prompts. The models tested include Adobe Firefly, Amazon Titan, Black Forest Labs FLUX1.1, Google Imagen, KLING AI, Midjourney, OpenAI DALLÂ·E, and Stability AIs models. Results show that Black Forest Labs FLUX1.1 and Stability AIs Stable Image Ultra performed best, accurately reproducing text over 50% of the time. The article also discusses three alternative techniques for ensuring text accuracy in generated images.","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Gvj5CUGClWka1KUsDy5GQw.png","categories":["Generative AI","Natural Language Processing","Technology/Web"],"author":"Rifx.Online","tags":["text","generation","models","accuracy","techniques"],"draft":false,"slug":"blog/comparing-leading-text-to-image-image-generation-models-for-adding-text-to-images-7dc001f491ef"},"content":"\n\n\n\n\n### A comparison of nine leading image generation modelsâ€™ ability to render accurate text (words and phrases) within an image.\n\nIn this post, we will assess the capabilities of nine state\\-of\\-the\\-art text\\-to\\-image generation models from multiple providers on different hosting platforms. Specifically, we will evaluate their ability to generate accurate text (words and phrases) within images based on given prompts. The models tested include the following (in alphabetical order):\n\n1. Adobe Firefly Image 3 (via [firefly.adobe.com](http://firefly.adobe.com/))\n2. Amazon Titan Image Generator G1 v2 (via [Amazon Bedrock](https://aws.amazon.com/bedrock/))\n3. Black Forest Labs FLUX1\\.1 \\[pro] and Ultra Mode (via [Replicate](http://replicate.com/))\n4. Google Imagen 3 (via [ImageFX](https://aitestkitchen.withgoogle.com/tools/image-fx))\n5. KLING AI powered by [Kwai\\-Kolors/Kolors](https://huggingface.co/Kwai-Kolors/Kolors) (via [klingai.com](http://klingai.com/))\n6. Midjourney v6\\.1 (via [midjourney.com](http://midjourney.com/))\n7. OpenAI DALLÂ·E 3 (via [ChatGPT](https://quip-amazon.com/62AqA7VtF4Xb/chatgpt.com))\n8. Stability AI Stable Diffusion 3\\.5 Large (via [stability.ai](http://stability.ai/) API)\n9. Stability AI Stable Image Ultra 1\\.0 v1 (via [Amazon Bedrock](https://aws.amazon.com/bedrock/))\n\nAdditionally, we will examine three alternative and more reliable techniques for ensuring the accuracy of text in generated images.\n\n\n## Testing the Models\n\nSeveral tests, using different prompts and varying levels of detail, were run across all models. Examples of prompts included:\n\n1. *A photograph of a smiling scientist holding a sign that reads: â€œFlawless AI\\-generated text!â€*\n2. *Vegetable stand with various vegetables, including tomatoes. A black sign with white type reads: â€œFarm Fresh Tomatoes $2\\.99/lb.â€*\n3. *A whimsical illustration of a friendly\\-looking pumpkin on a white background with a Fall motif of assorted gourds and autumn leaves. The words â€œHappy Halloweenâ€ are centered above the pumpkin in large dark brown letters.*\n4. *A sleek billboard towers above a bustling interstate at rush hour, cars whizzing by in a blur. Against a dynamic, abstract background, the large, bold text â€œGenerative AI: Transforming Digital Advertisingâ€, creates instant readability for passing motorists.*\n\nAlthough the overall image quality and degree of apparent bias varied significantly among the models, only text generation capabilities were assessed. Models that could accurately reproduce the requested text in the prompt at least 50% of the time received a passing grade. Below are results from selected tests that exemplify the modelsâ€™ capabilities. The results are presented in alphabetical order rather than ranked by quality. For each test, four representative images of average quality are included in the post.\n\n\n\n\n## Models\n\n\n### Adobe Firefly Image 3\n\nAdobe announced its Firefly Image 3 Foundation Model in April 2024\\. According to the [press release](https://news.adobe.com/news/news-details/2024/adobe-introduces-firefly-image-3-foundation-model-to-take-creative-exploration-and-ideation-to-new-heights), Adobe Firefly Image 3 delivers stunning advancements in photorealistic quality, styling capabilities, detail, accuracy, and a greater variety. In addition, significant advancements in the generation speed make the ideation and creation process more productive and efficient. The model is available for use in Adobe Photoshop (beta) and on [firefly.adobe.com](https://firefly.adobe.com/generate/images). Both interfaces are shown below.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*gcASwZgRSfPNYJB7n5GrlQ.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*vU3NW6VdkgojkNlHaGWoSg.png)\n\nðŸš« In my tests, Adobe Firefly could not accurately reproduce the text requested in the prompt.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*yWoDLmj5mPKEw8GRg51YXw.jpeg)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*0iskBrEjrtFk-mXNrBvkag.jpeg)\n\n\n### Amazon Titan Image Generator G1 v2\n\nThe Amazon Titan Image Generator G1 v2 model was [released](https://aws.amazon.com/blogs/aws/amazon-titan-image-generator-v2-is-now-available-in-amazon-bedrock/) in August 2024\\. It was an upgrade to the previous generation, the Amazon Titan Image Generator G1 v1 model, [released](https://aws.amazon.com/blogs/aws/amazon-titan-image-generator-multimodal-embeddings-and-text-models-are-now-available-in-amazon-bedrock/) in November 2023\\. The Amazon Titan Image Generator G1 v2 model added features, including image conditioning, image guidance with a color palette, background removal, and subject consistency.\n\nThe Amazon Titan Image Generator G1 v2 model was tested on Amazon Bedrock, which according to [AWS](https://aws.amazon.com/bedrock/), is â€œ*a fully managed service that offers a choice of high\\-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, Stability AI, and Amazon through a single API, along with a broad set of capabilities you need to build generative AI applications with security, privacy, and responsible AI.*â€\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*TmROyF5c-BXHevqImyflmw.png)\n\nðŸš« In my tests, Amazon Titan Image Generator G1 v2 could not accurately reproduce the text requested in the prompt.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*QLvxsEveORObkPOOB3u1Mg.png)\n\n\n### Black Forest Labs FLUX1\\.1 \\[pro] and Ultra Mode\n\nBlack Forest Labs [released](https://blackforestlabs.ai/announcing-flux-1-1-pro-and-the-bfl-api/) FLUX1\\.1 \\[pro] in October 2024\\. According to Black Forest Labs, â€œ*FLUX1\\.1 \\[pro] provides six times faster generation than its predecessor FLUX.1 \\[pro] while also improving image quality, prompt adherence, and diversity. At the same time, we updated FLUX.1 \\[pro] to generate the same output as before, but two times faster.*â€ The earlier FLUX.1 \\[pro] model was released in August 2024\\.\n\nAs I prepared this post, Black Forest Labs introduced FLUX1\\.1 \\[pro] Ultra and Raw Modes. According to the press release, â€œT*oday we are adding new high\\-resolution capabilities to FLUX1\\.1 \\[pro], extending its functionality to support 4x higher image resolutions (up to 4MP) while maintaining an impressive generation time of only 10 seconds per sample.*â€\n\nTests of Black Forest Labs FLUX1\\.1 \\[pro] and Ultra were run on [Replicate](https://replicate.com/blog/machine-learning-needs-better-tools). Their website states, â€œ*Replicate runs machine learning models in the cloud. We have a library of open\\-source models that you can run with a few lines of code. If youâ€™re building your own machine learning models, Replicate makes it easy to deploy them at scale.*â€\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*IUbfTFj32FxIta_3J1W0pQ.png)\n\nâœ… In my tests, Black Forest Labs FLUX1\\.1 \\[pro] could accurately reproduce the text requested in the prompt more than 50% of the time. It had the best results of all models tested.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*RewBBA9MAiNbG93h65WdYg.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ISZfNQZHo3PL_QkYu3jEIw.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*XliNJWJr2TZ5MGwi7RAa-g.png)\n\n\n### Google Imagen 3\n\nGoogle Imagen 3 was [released](https://deepmind.google/technologies/imagen-3/) to all US users in August 2024\\. According to Google, â€œ*Imagen 3 is our highest\\-quality text\\-to\\-image model, capable of generating images with even better detail, richer lighting, and fewer distracting artifacts than our previous models.*â€ Tests of Google Imagen 3 were run on [ImageFX](https://aitestkitchen.withgoogle.com/tools/image-fx), part of Googleâ€™s AI Test Kitchen, â€œ*a place where people can experience and give feedback on some of Googleâ€™s latest AI technologies.*â€\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*nhto3l0o-XITJzEEQoHSTA.png)\n\nðŸš« In my tests, Google Imagen 3 could not accurately reproduce the text requested in the prompt.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*9aqKPuZlpGF_lE3pA0ZNtw.png)\n\n\n### KLING AI powered by Kolors\n\nKolors powers Kling AIâ€™s image generation capabilities. According to [Hugging Face](https://huggingface.co/Kwai-Kolors/Kolors), â€œ*Kolors is a large\\-scale text\\-to\\-image generation model based on latent diffusion, developed by the Kuaishou Kolors team. Trained on billions of text\\-image pairs, Kolors exhibits significant advantages over both open\\-source and proprietary models in visual quality, complex semantic accuracy, and text rendering for both Chinese and English characters.*â€ According to [Kuaishou](https://ir.kuaishou.com/news-releases/news-release-details/kuaishou-launches-full-beta-testing-kling-ai-global-users-0), Kling AI was released in July 2024\\.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*na56zUz3DLWK7Dqj51vSKw.png)\n\nðŸš« In my tests, KLING AI powered by Kolors could not accurately reproduce the text requested in the prompt. The results were the worst of the models tested. Many responses were in Chinese, even when explicitly asked to be displayed in English.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*xgq4C0m8s3Wfp4p9Va7fSQ.png)\n\n\n### Midjourney v6\\.1\n\nMidjourney v6\\.1 was released in July 2024\\. According to [Midjourney](https://updates.midjourney.com/version-6-1/), the latest release, v6\\.1, contained several significant improvements, including more coherent images (arms, legs, hands, bodies, plants, animals, etc.), much better image quality, more precise, detailed, and correct small image features, and improved text accuracy (when drawing words via â€œquotationsâ€ in prompts). Using the `â€” â€” style raw` flag also helps improve text accuracy in some test cases, according to [Midjourney](https://docs.midjourney.com/docs/text-generation).\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ETXx5VyY4BgEA8zn4K3M0g.png)\n\nðŸš« âœ… In my tests, Midjourney v6\\.1 results were mixed. Midjourney could not consistently reproduce the text requested in the prompt more than 50% of the time. The output was correct in some test cases and close to the prompt in others but also repeated words and punctuation just as often.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*yIaVzqP_BwvDGMO5SOo1SA.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BDCsxYe_cJSb6pfoxKrWGA.jpeg)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*dqGYigq9T-PMx3GKfqSf2Q.png)\n\n\n### OpenAI DALLÂ·E 3\n\nOpenAI DALLÂ·E 3 was [released](https://deepmind.google/technologies/imagen-3/) over one year ago, in October 2023\\. According to [OpenAI](https://openai.com/index/dall-e-3/), â€œ*DALLÂ·E 3 represents a leap forward in our ability to generate images that exactly adhere to the text you provide. DALLÂ·E 3 understands significantly more nuance and detail than our previous systems \\[DALLÂ·E 2], allowing you to easily translate your ideas into exceptionally accurate images.*â€\n\nTests of OpenAI Imagen 3 were run on [ChatGPT](https://openai.com/index/chatgpt/). Also, according to [OpenAI](https://openai.com/index/dall-e-3/), â€œ*DALLÂ·E 3 is built natively on ChatGPT, which lets you use ChatGPT as a brainstorming partner and refiner of your prompts. Just ask ChatGPT what you want to see in anything from a simple sentence to a detailed paragraph.*â€\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*x45i0IJoYNiJT1kOi98k7w.png)\n\nðŸš« In my tests, OpenAI DALLÂ·E 3 could not accurately reproduce the text requested in the prompt.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*NirwqSB-k8dzfGRNAw-pQw.png)\n\n\n### Stability AI Stable Diffusion 3\\.5 Large\n\nAccording to Stability AI, the [Stable Diffusion 3\\.5 Large](https://stability.ai/news/introducing-stable-diffusion-3-5) model, released in October 2024, â€œ*at 8\\.1 billion parameters, with superior quality and prompt adherence, this base model is the most powerful in the Stable Diffusion family. This model is ideal for professional use cases at 1 megapixel resolution.*â€ The Stability AI Stable Diffusion 3\\.5 Large was tested using the [StabilityAI REST API](https://platform.stability.ai/docs/api-reference#tag/Generate/paths/~1v2beta~1stable-image~1generate~1ultra/post) and code written in Python within a Jupyter Notebook.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*56Zp5QWVvTzGYlslcWEGKg.png)\n\nâœ… In my tests, Stability AI Stable Diffusion 3\\.5 Large could accurately reproduce the text requested in the prompt more than 50% of the time, occasionally with slight punctuation errors.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*CQ9I5z7x8ILTdFhu1dCBCQ.jpeg)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*G2D-L2fEtjKVTTyph3Burg.jpeg)\n\n\n### Stability AI Stable Image Ultra\n\nAccording to Stability AI, the 16 *billion\\-parameter [Stable Image Ultra](https://stability.ai/stable-image) model, released in October 2024, â€œis our flagship model, blending the power of the SD3 Large with advanced workflows to deliver the highest\\-quality photorealistic images. This premium model is designed for industries that require unparalleled visual fidelity, such as marketing, advertising, and architecture.*â€ Like Amazon Titan Image Generator, the Stability AI Stable Image Ultra model was also tested using [Amazon Bedrock](https://aws.amazon.com/blogs/aws/stability-ais-best-image-generating-models-now-in-amazon-bedrock/) using the Image Playground UI.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GjaPW2FWGGhuJ06trs1Jww.png)\n\nâœ… In my tests, Stability AI Stable Image Ultra could accurately reproduce the text requested in the prompt more than 50% of the time. Along with Black Forest Labs FLUX1\\.1 \\[pro], it was one of the best models tested.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*O7JKeKBPgaEOuvdFW-u2Sg.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*jDHNLjOKHuEBQlFvTb7nYQ.png)\n\n\n## AI Alternatives to Generating Text\n\nThe Black Forest Labs FLUX1\\.1 \\[pro] and Stability AI Stable Image Ultra models accurately reproduce requested phrases in prompts more frequently than other models. However, users still lack control over many aspects of the images, including the exact position, size, kerning, color, and font style of the text. Several alternative and more reliable techniques exist to guarantee the accuracy of text in generated images.\n\n\n### Replace Generated Text\n\nOne alternative approach is to generate the image with the desired text, regardless of spelling mistakes. Subsequently, one can remove the text in Adobe Photoshop and replace it with correct text in the exact position, size, color, and style desired. However, removing and recreating text can be challenging if foreground subjects or shadows partially obscure it, or if the text appears on an irregular surface. To enhance the realism of the new text, one can rasterize the vector type and then add noise, blurring, distortion, lighting, texturing, and layer blending effects.\n\nBelow are two examples of images generated with Black Forest Labs FLUX1\\.1 \\[pro] Ultra (first image). The text has been removed in Adobe Photoshop (second image), new vector\\-based text has been added (third image), and finally, the text has been rasterized and distorted to appear more realistic (fourth image).\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*B0_3d8oImDlrRb6mjpekrw.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*fmrW46OsZe6Zsc0eshPyYw.png)\n\n\n### Start with a Blank Canvas\n\nA second alternative is to generate the image without text and then add your text in the desired color, size, and font style using Adobe Photoshop. This technique is more straightforward than retouching the generated image to remove existing text. The examples were created using the [Replicate](https://replicate.com/docs/get-started/python) API with Python from a Jupyter Notebook to call Black Forest Labsâ€™ FLUX1\\.1 \\[pro] and Ultra.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*iFpqy4fEUJOXaJMzhsgbDA.png)\n\nBelow is an image generated with Black Forest Labs FLUX1\\.1 \\[pro] Ultra using the prompt: â€œ*A photograph of a smiling female scientist in a lab coat, standing in a lab, holding a white rectangular sign with no wording or other elements.*â€ The generated image (first image) has new text added (second image), and finally, the text is distorted to appear more realistic (third image).\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*c1rgPArHDrUQ2cePV9DUCA.png)\n\nBelow is another example that begins with a generated image containing no text, to which text was later added. The initial image was generated with Black Forest Labs FLUX1\\.1 \\[pro] Ultra using the prompt: â€œ*Vegetable stand with various vegetables, including tomatoes. A small, rectangular, blank, black sign with no text or other elements sits beside the tomatoes.*â€\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*t_6oM1aItMGQfBUPjH83lA.png)\n\nOne last example using the prompt, â€œ*A sleek billboard towers above a bustling interstate at rush hour, cars whizzing by. Against a colorful, dynamic, abstract background fills the billboard.*â€ to generate the original image.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*KyGveUehRxuFTK-DmWnCaw.jpeg)\n\n\n## Generate Image and Text Separately\n\nA third and final technique is to generate the image and text separately using your model of choice, then combine the two elements in post\\-production using Adobe Photoshop. Below is the original image from Midjourney on the left without text, generated using the prompt: â€œ*Vegetable stand with various vegetables, including tomatoes. A empty, blank blackboard\\-like sign. â€” ar 1:1*â€\n\nThe white type on a black background in the center was also generated in Midjourney, using the prompt: â€œ*The phrase â€œFarm Fresh Tomatoes $2\\.99/lb.â€ written in white chalk letters on a solid jet black background. â€” no tomatoes or other objects â€” ar 3:2 â€” style raw â€” stylize 0*â€\n\nThe text\\-only image is then easily overlaid on top of the first image using the Lighten blending mode for the text\\-only layer. Additional distortions can be applied to make the text look more natural in final image.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ZP-pqTQVN8Xy_Vhjm8D0gg.png)\n\n\n## Conclusion\n\nIn this post, we explored the capabilities of nine different state\\-of\\-the\\-art text\\-to\\-image generation models from various providers to generate accurate text within images from prompts. We discovered that Black Forest Labs FLUX1\\.1 \\[pro] and Stability AIâ€™s Stable Image Ultra were more successful at accurately reproducing requested text in images compared to other models. Finally, we examined three alternative and more reliable techniques for ensuring the accuracy of text in generated images.\n\n*If you are not yet a Medium member and want to support authors like me, please sign up here: <https://garystafford.medium.com/membership>.*\n\n*This blog represents my viewpoints and not those of my employer, Amazon Web Services (AWS). All product names, images, logos, and brands are the property of their respective owners.*\n\n\n"},{"lang":"en","group":"blog","slug":"blog/conversational-ai-for-customer-service-best-practices-and-key-steps-for-success-4ceee714dbe1","frontmatter":{"title":"Conversational AI for Customer Service: Best Practices and Key Steps for Success","meta_title":"Conversational AI for Customer Service: Best Practices and Key Steps for Success","description":"Conversational AI is increasingly vital for enhancing customer service, with a projected rise in fully automated interactions to 40% by 2025. It leverages natural language processing and machine learning to provide instant, personalized support, improving customer satisfaction and operational efficiency. Key steps for successful implementation include identifying customer pain points, defining use cases, selecting the right platform, and ensuring continuous monitoring and improvement. Best practices emphasize security, balancing automation with human interaction, and personalizing customer experiences. As businesses embrace this technology, it is expected to transform customer service operations significantly.","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*LrRhvUJrNaS299z8oC2bkg.jpeg","categories":["Natural Language Processing","Machine Learning","Chatbots"],"author":"Rifx.Online","tags":["conversational","automation","personalization","monitoring","security"],"draft":false,"slug":"blog/conversational-ai-for-customer-service-best-practices-and-key-steps-for-success-4ceee714dbe1"},"content":"\n\n\n\n\n\nIn todayâ€™s fast\\-paced business environment, customer service plays a crucial role in building and maintaining customer loyalty. As businesses strive to offer personalized and efficient support, Conversational AI has emerged as a revolutionary solution. By integrating artificial intelligence (AI) into customer service operations, companies can streamline processes, provide instant responses, and significantly improve the overall customer experience. A report by *Gartner* estimates that by **2025**, **40% of customer service interactions** will be fully automated through AI and machine learning technology, a significant rise from the **25%** reported in 2023\\.\n\nBusinesses looking to develop Conversational AI solutions for customer service are entering a transformative space with immense potential. This article explores the best practices, key steps, and benefits of implementing Conversational AI to drive success in customer support operations, offering a detailed guide for businesses eager to develop their own AI\\-driven customer service solutions.\n\n\n## The Rise of Conversational AI in Customer Service\n\nConversational AI combines natural language processing (NLP), machine learning, and automated messaging to facilitate seamless, human\\-like interactions between customers and digital systems. These technologies are designed to understand customer queries, provide accurate responses, and engage users in meaningful conversations. With the increasing demand for 24/7 customer support and instant resolution of issues, businesses are now turning to Conversational AI to meet these needs efficiently.\n\nIncorporating Conversational AI into customer service not only improves operational efficiency but also enhances customer satisfaction. In fact, according to *Salesforce*, **69% of consumers** expect AI\\-driven interactions to provide more relevant and personalized experiences, highlighting the growing importance of Conversational AI in delivering customer\\-centric services.\n\n\n## Why Businesses Should Develop Conversational AI for Customer Service?\n\nFor businesses looking to [**develop AI\\-driven customer service solutions**](https://www.blockchainappfactory.com/generative-ai-solutions?utm_source=medium&utm_medium=blog&utm_campaign=elavarasan), the advantages are vast. Below are some compelling reasons why investing in Conversational AI for customer service is a strategic move:\n\n**1\\. Improved Customer Experience**\n\nConversational AI provides immediate responses to customer queries, eliminating the need for customers to wait in long queues or deal with delayed responses. By offering 24/7 support through AI\\-powered chatbots and virtual assistants, businesses can deliver a faster, more seamless experience that improves customer satisfaction and loyalty.\n\n**2\\. Increased Efficiency**\n\nAI\\-powered bots can handle high volumes of customer inquiries simultaneously, freeing up human agents to focus on more complex issues. This results in a more efficient use of resources, reducing operational costs and minimizing response times.\n\n**3\\. Cost Savings**\n\nImplementing Conversational AI significantly reduces the costs associated with customer service operations. According to *Juniper Research*, businesses that integrate chatbots into their customer service operations could save up to **$11 billion annually** by 2023\\. These savings come from reducing the need for large customer service teams and automating repetitive tasks.\n\n**4\\. Enhanced Personalization**\n\nBy analyzing customer data and preferences, AI systems can deliver personalized responses tailored to individual needs. This level of customization helps businesses build stronger relationships with their customers and fosters greater engagement.\n\n\n## Key Steps for Implementing Conversational AI in Customer Service\n\nImplementing Conversational AI for customer service requires a thoughtful and strategic approach to ensure it delivers the desired outcomes. Here are the key steps businesses should follow when developing AI\\-driven customer support solutions.\n\n**Step 1: Identify Customer Pain Points**\n\nBefore implementing Conversational AI, itâ€™s essential to understand the specific challenges and pain points that your customers face during interactions with customer service. Common issues such as long wait times, repetitive queries, or difficulty accessing information should be addressed by the AI solution. Conduct surveys, analyze customer service data, and gather feedback to pinpoint the areas where AI can provide the most value.\n\n**Step 2: Define Use Cases for Conversational AI**\n\nOnce youâ€™ve identified customer pain points, define the specific use cases where Conversational AI will be most effective. Use cases can range from answering frequently asked questions (FAQs) to handling more complex processes like booking services, processing returns, or offering product recommendations. Businesses must focus on prioritizing high\\-impact use cases that directly address customer needs and provide measurable benefits.\n\n**Common Use Cases Include:**\n\n* **Order Status Inquiries:** AI bots can instantly retrieve and share order status updates with customers.\n* **Product Information:** Virtual assistants can provide detailed product information and suggest complementary items based on customer behavior.\n* **Technical Support:** AI\\-powered bots can guide users through troubleshooting steps for common technical issues.\n* **Billing and Payments:** Chatbots can facilitate payments, bill inquiries, and subscription management.\n\n**Step 3: Choose the Right Conversational AI Platform**\n\nSelecting the right AI platform is critical for the success of your customer service automation efforts. When evaluating AI platforms, consider factors such as scalability, ease of integration, NLP capabilities, and customization options. Popular AI platforms like Google Dialogflow, Microsoft Azure Bot Services, and IBM Watson offer robust tools for building and deploying AI\\-driven customer service solutions. The platform should support multi\\-channel interactions (e.g., chat, voice, social media) to ensure seamless communication with customers across different platforms.\n\n**Step 4: Design an Intuitive User Experience**\n\nAn intuitive and user\\-friendly interface is key to the success of your Conversational AI system. Ensure that the AI can engage customers in clear, concise conversations that guide them toward resolving their issues efficiently. The AI should understand natural language, detect customer intent, and ask relevant follow\\-up questions to help users get the information they need quickly.\n\nTo enhance the user experience, design the conversation flow to be as human\\-like as possible. Personalize the interactions by addressing the user by name, recalling past conversations, and offering solutions based on their previous interactions with your business.\n\n**Step 5: Train the AI with Relevant Data**\n\nThe performance of your Conversational AI largely depends on the quality and quantity of data it is trained on. Train the AI with real customer interaction data to improve its ability to understand different queries, nuances, and language variations. By continuously feeding the AI with data from past interactions, you ensure that it learns and improves over time, enhancing its ability to provide accurate and contextual responses.\n\n**Step 6: Integrate with Existing Systems**\n\nFor Conversational AI to be truly effective, it must be integrated with your existing customer service systems. Ensure that the AI solution can pull data from your CRM, billing systems, order management platforms, and knowledge bases to provide relevant and accurate information to customers. This integration allows the AI to respond to inquiries in real\\-time and ensures that customers receive consistent support across all touchpoints.\n\n**Step 7: Continuously Monitor and Improve**\n\nAfter deploying your Conversational AI solution, itâ€™s essential to continuously monitor its performance and identify areas for improvement. Track key performance indicators (KPIs) such as response time, resolution rate, and customer satisfaction scores. Use these insights to refine the AIâ€™s capabilities, adjust conversation flows, and address any shortcomings. AI systems should evolve over time by learning from interactions and adapting to changing customer expectations.\n\n\n## Best Practices for Successful Conversational AI Implementation\n\nDeveloping and [**deploying Conversational AI in customer service**](https://www.blockchainappfactory.com/generative-ai-solutions?utm_source=medium&utm_medium=blog&utm_campaign=elavarasan) requires adherence to best practices to ensure long\\-term success. Here are some best practices for businesses looking to maximize the impact of AI\\-driven customer service.\n\n**1\\. Prioritize Security and Compliance**\n\nCustomer service often involves handling sensitive information such as payment details, account information, and personal data. Ensure that your AI solution complies with data protection regulations like the General Data Protection Regulation (GDPR) or the California Consumer Privacy Act (CCPA). Implement robust security protocols to encrypt customer data and protect against data breaches.\n\n**2\\. Balance Automation with Human Interaction**\n\nWhile Conversational AI can handle a wide range of customer inquiries, itâ€™s essential to strike a balance between automation and human support. In cases where the AI cannot resolve complex issues or where customers require a more personalized touch, ensure that there is a seamless transition from AI to human agents. This hybrid model allows customers to get the best of both worlds: the speed of AI and the empathy of human agents.\n\n**3\\. Test and Optimize the AI System**\n\nBefore launching your AI system to the public, conduct extensive testing to ensure it meets the desired performance benchmarks. Test the AIâ€™s ability to understand various languages, accents, and query structures. Use A/B testing to evaluate different conversation flows and optimize them based on customer feedback and performance data.\n\n**4\\. Personalize Interactions with AI**\n\nCustomers appreciate interactions that feel personalized and relevant. By leveraging AIâ€™s ability to analyze customer data, businesses can tailor responses based on the customerâ€™s history, preferences, and behavior. Personalization goes beyond just addressing customers by name â€” it involves anticipating their needs and offering proactive solutions that add value to their experience.\n\n**5\\. Incorporate Multilingual Capabilities**\n\nFor businesses with a global customer base, itâ€™s essential to develop Conversational AI that supports multiple languages. Multilingual AI systems allow customers to interact with businesses in their preferred language, improving accessibility and expanding the reach of customer service operations.\n\n**6\\. Set Clear Expectations for Customers**\n\nTo prevent frustration, set clear expectations for customers about what the AI can and cannot do. If the AI is limited to certain tasks (such as providing order updates or answering FAQs), make this clear from the beginning of the interaction. This transparency helps manage customer expectations and prevents confusion when transitioning to human agents for more complex issues.\n\n\n## Real\\-World Examples of Conversational AI in Customer Service\n\nSeveral companies have already implemented Conversational AI solutions to enhance their customer service operations. Below are some real\\-world examples that showcase the impact of AI on customer service success.\n\n**1\\. H\\&Mâ€™s Customer Service Chatbot**\n\nGlobal fashion retailer *H\\&M* uses an AI\\-powered chatbot to help customers with common queries related to order status, product availability, and return policies. The chatbot reduces the load on customer service agents by automating repetitive tasks, allowing human agents to focus on more complex inquiries. H\\&Mâ€™s chatbot has improved customer response times and increased overall satisfaction rates.\n\n**2\\. Sephoraâ€™s Virtual Beauty Advisor**\n\nCosmetics retailer *Sephora* offers a **Virtual Beauty Advisor** powered by Conversational AI to provide personalized product recommendations and beauty tips. The AI assistant engages customers in interactive conversations, asking questions about their preferences and recommending products tailored to their needs. By providing a highly personalized experience, Sephora has improved customer engagement and increased online sales.\n\n**3\\. Amtrakâ€™s Customer Service Chatbot, Julie**\n\n*Amtrak* implemented **Julie**, an AI\\-powered virtual assistant that helps customers book tickets, check train schedules, and get travel updates. Julie handles over **5 million inquiries** per year, reducing call center volume and improving the overall efficiency of Amtrakâ€™s customer service operations. The success of Julie has allowed Amtrak to cut costs and provide faster service to its customers.\n\n\n## The Future of Conversational AI in Customer Service\n\nThe adoption of Conversational AI in customer service is poised to grow in the coming years as businesses continue to prioritize automation, efficiency, and customer satisfaction. By 2030, it is estimated that **70% of customer interactions** will involve some form of AI technology. This shift toward AI\\-driven customer service will allow businesses to offer personalized, real\\-time support to millions of customers globally.\n\nAs technology evolves, Conversational AI will become even more intelligent, capable of handling complex inquiries, understanding emotions, and providing proactive solutions. Businesses that invest in Conversational AI today will be better equipped to meet the growing demands of their customers in the future.\n\n\n## Conclusion\n\nConversational AI has transformed the way businesses interact with their customers, offering faster, more personalized, and efficient support. For businesses looking to develop AI\\-driven customer service solutions, the opportunities are vast. By following best practices and implementing key steps such as identifying use cases, choosing the right platform, and continuously optimizing the AI system, businesses can achieve success and drive innovation in customer service.\n\nInvesting in Conversational AI is not just a strategic move for improving customer service â€” itâ€™s an essential step toward future\\-proofing customer support operations. By enhancing customer experience, reducing costs, and offering scalable solutions, Conversational AI represents the future of customer service.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/explore-swarm-multi-agent-framework-locally-0e25ee617795","frontmatter":{"title":"Explore Swarm Multi-Agent Framework Locally","meta_title":"Explore Swarm Multi-Agent Framework Locally","description":"Swarm is an experimental sample framework to simulate lightweight multi-agent framework for educational purpose. Usually it works with Openâ€¦","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*0ZVceq32bvkytC7HSIgmwA.png","categories":["Programming","Technology","Education"],"author":"Rifx.Online","tags":["Swarm","Multi-Agent","Framework","OpenAI","Ollama"],"draft":false,"slug":"blog/explore-swarm-multi-agent-framework-locally-0e25ee617795"},"content":"\n\n\n\n\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*zkpW8DDwh0TTYuHJVJbDaw.png)\n\nSwarm is an experimental sample framework to simulate lightweight multi-agent framework for educational purpose. Usually it works with Open AI Key but we can change it to use local Ollama or LM Studio Models.\n\n**Setup:**\n\n\n```python\n## Create a new Conda or Python Virtual Environment and activate it\nconda install python==3.10\npip install torch openai\npip install transformers accelerate huggingface_hub\npip install git+ssh://git@github.com/openai/swarm.git\n```\n**To use with Open AI Key:**\n\n\n```python\nexport OPEN_API_KEY = Your Key\n```\n**To use Ollama or LM Studio Local LLMs â€” Update to Local URL:**\n\n\n```python\n## Find the location site-packages/swarm on the conda or python virtual env\n## Locate the file core.py\nclass Swarm:\n    def __init__(self, client=None):\n        if not client:\n          # Actual Code\n          #client = OpenAI()\n          # Update the Base URL and API Key to Ollama / LM Studio\n          # In this demo we are using LM Studio and Llama 3.1\n          client = OpenAI(base_url=\"http://localhost:1234/v1\",api_key=\"random\")\n        self.client = client\n```\n**Clone Repo:**\n\nClone the Repo â€” where you can find examples directory with different use cases like basic, airline and weather etc.\n\n\n```python\ngit clone https://github.com/openai/swarm.git\ncd swarm/examples\n```\n**Sample Code:**\n\n\n```python\nfrom swarm import Swarm, Agent\n\nclient = Swarm()\n\n\nit_agent = Agent(\n    name=\"IT Agent\",\n    instructions=\"You are an IT Expert with 10 Years of Experience.\",\n)\n\nsales_agent = Agent(\n    name=\"Sales Agent\",\n    instructions=\"You are a Sales Expert with 5 Years of Experience and knows about best selling mobiles.\",\n)\n\ndef transfer_to_sales_agent():\n    print(\"Sales agent in action\")\n    \"\"\"Transfer sales related questions to sales team immediately.\"\"\"\n    return sales_agent\n\ndef transfer_to_it_agent():\n    print(\"IT agent in action\")\n    \"\"\"Transfer IT users immediately.\"\"\"\n    return it_agent\n\nenglish_agent = Agent(\n    name=\"English Agent\",\n    instructions=\"You only speak English.\",\n    functions=[transfer_to_sales_agent,transfer_to_it_agent],\n)\n\n\nmessages = [{\"role\": \"user\", \"content\": \"How to install pandas lib?\"}]\nresponse = client.run(agent=english_agent, messages=messages)\n\nprint(response.messages[-1][\"content\"])\n\nmessages = [{\"role\": \"user\", \"content\": \"What are the best selling items?\"}]\nresponse = client.run(agent=english_agent, messages=messages)\n\nprint(response.messages[-1][\"content\"])\n```\n**References:**\n\n\n```python\nhttps://github.com/openai/swarm\n\nhttps://github.com/victorb/ollama-swarm/tree/main\n```\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*hCFJ4VQoT12yElYPXwXvWA.png)\n\nGiven that it is an experimental release, there is still much room for improvement. The airline agent example code [swarm/examples/airline] was interesting, so try those examples. Give it a try and share your experience in the comments. Thanks.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/fine-tuning-llama-3-with-unsloth-79c3465ef3e3","frontmatter":{"title":"Fine-tuning LLama 3 with Unsloth","meta_title":"Fine-tuning LLama 3 with Unsloth","description":"In this article I will show you how to fine-tune an LLM (Llama 3 from Meta) using Unsloth (including a way for custom dataset)","date":"2024-10-30T12:58:41.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kaXoudNTGeGfuNPl_kta5g.jpeg","categories":["Programming","Machine Learning","Natural Language Processing"],"author":"Rifx.Online","tags":["Llama","Unsloth","LoRA","Alpaca","NVIDIA"],"draft":false,"slug":"blog/fine-tuning-llama-3-with-unsloth-79c3465ef3e3"},"content":"\n\n\nIn this article I will show you how to fine\\-tune an LLM (Llama 3 from Meta) using [Unsloth](https://github.com/unslothai/unsloth). I will also provide a way to use your own custom dataset.\n\n**Note :** Unsloth is library that accelerates fine\\-tuning of LLMs on NVIDIA GPUs (40% reduction in memory usage compared to traditional methods). Compatible with Hugging Face, it supports Llama and Mistral architectures.\n\nIf you find my articles interesting, donâ€™t forget to **clap and [follow](https://medium.com/@soulawalid)** ðŸ‘ðŸ¼, these articles take times and effort to do!\n\nYou can access to the free notebook provided for that on the GitHub repo\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*_L4o4MDQ7W5__OwW0E5RWA.png)\n\nSince I am using Llama 3, I will click on the notebook (you can install Unsloth on your own computer too).\n\n**Note:** I will use this dataset â€œ[alpaca\\-cleaned](https://huggingface.co/datasets/yahma/alpaca-cleaned)â€ from Hugging Face , the data is in Alpaca format meaning there is (Instruction, Input and Output)\n\n### Starting the project\n\nDuring the project I will guide you to perform fine\\-tuning with Unsloth, explaining the code and provide recommendations, Letâ€™s start our project :\n\n**1/ Installing required packages :** We need first to install **Unsloth** and **xformers**, **trl**, **peft**, **accelerate**, **bitsandbytes** libraries for efficient model training and inference.\n\n```python\n!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n!pip install --no-deps xformers trl peft accelerate bitsandbytes\n```\n\n**2/ Loading and Configuring the Model :** In the configuration I will set the following :\n\n* Sets the maximum sequence length to **2048**\n* by having dtype as **None**, it automatically detects the data type.\n* Loads the model in **4\\-bit precision,** I think itâ€™s enough.\n\n**Note :** You can find my article about tips on fine\\-tuning LLMs in the Resources section\n\n```python\nfrom unsloth import FastLanguageModel\nimport torch\n\n## Configuration\nmax_seq_length = 2048\ndtype = None\nload_in_4bit = True\n\n## Load the selected model\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/llama-3-8b-bnb-4bit\",\n    max_seq_length=max_seq_length,\n    dtype=dtype,\n    load_in_4bit=load_in_4bit,\n)\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*cJSAcJFP7E-qJkqKUsHqLw.png)\n\n**3/ Applying PEFT (Parameter Efficient Fine\\-Tuning) :** We will then fine\\-tunes the pre\\-trained model using LoRA.\n\n* r \\= 16 is the rank parameter for LoRA. **Note :** common values are 8, 16, 32, 64, 128\n* lora\\_alpha \\= 16 represents the scaling factor for LoRA updates ( I will write an article about LoRA to explain in details each part of it)\n* No dropout and bias for LoRA\n* For use\\_gradient\\_checkpointing we are using Unsloth to handle that (saving memory)\n\n```python\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 16,\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha = 16,\n    lora_dropout = 0,\n    bias = \"none\",\n    use_gradient_checkpointing = \"unsloth\",\n    random_state = 3407,\n    use_rslora = False,\n    loftq_config = None,\n)\n```\n\n**4/ Defining the Prompt Template :** We will create alpaca prompt template to format the dataset ( In case the data that you will be using is not in that format).\n\nWe will also add EOS (End Of Sequence) to inform the LLM that the sentence has ended.\n\nFinally the formatting function, the function takes a batch of examples and formats each one according to the alpaca prompt template that we write before.\n\n* It extracts instruction, input, and output fields from each example (row).\n* It then formats these fields into the template and appends the EOS token.\n* The formatted text is stored in a list and returned as a dictionary with a single key, â€œtextâ€\n\n```python\nalpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n#### Instruction:\n{}\n\n#### Input:\n{}\n\n#### Response:\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token\n\ndef formatting_prompts_func(examples):\n    instructions = examples[\"instruction\"]\n    inputs = examples[\"input\"]\n    outputs = examples[\"output\"]\n    texts = []\n    for instruction, input, output in zip(instructions, inputs, outputs):\n        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n        texts.append(text)\n    return {\"text\": texts}\n```\n\n**5/ Loading and Formatting the Dataset:** Loads the Alpaca dataset and applies formatting to each dataset example in batches.\n\n```python\nfrom datasets import load_dataset\ndataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\ndataset = dataset.map(formatting_prompts_func, batched = True)\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*M8EmbLMdoqrM-JlkMpDv8g.png)\n\n**6/ Setting Up and Training the Model:** I covered most of them in my [previous article](https://readmedium.com/supervised-fine-tuning-tips-for-your-llm-projects-f84f20593653) regarding tips for Fine\\-Tuning.\n\n```python\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2, # Number of processes to use for data preprocessing\n    packing = False, # Whether to pack multiple sequences into one batch to increase training efficiency\n    args = TrainingArguments(\n        per_device_train_batch_size = 2, #The batch size per device\n        gradient_accumulation_steps = 4, #Number of gradient accumulation steps, which allows for effectively larger batch sizes\n        warmup_steps = 5, #Number of steps to perform linear learning rate warmup\n        max_steps = 60, #Total number of training steps\n        learning_rate = 2e-5,#The learning rate for the optimizer\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"cosine\",\n        seed = 3407,\n        output_dir = \"outputs\",\n    ),\n)\n\ntrainer_stats = trainer.train()\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Vb_OqGP9CPc8xZdnkclGyQ.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*PI0JXrTbpjuviyQ4bZJnFg.png)\n\n**7/ Inference and Generation :** we prepare the model for inference by preparing the input prompt, tokenizing it , and then uses the model to generate new text based on that prompt. The generated text is then converted back into readable form.\n\n```python\nFastLanguageModel.for_inference(model)\ninputs = tokenizer(\n[\n    alpaca_prompt.format(\n        \"Continue the fibonnaci sequence.\", # instruction\n        \"1, 1, 2, 3, 5, 8\", # input\n        \"\", # output - leave this blank for generation!\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*PI6SBL_YPPj0-RSAn5nl7g.png)\n\nYou can also use a TextStreamer for continuous inference , so you can see the generation token by token, instead of waiting the whole time!\n\n```python\nFastLanguageModel.for_inference(model)\ninputs = tokenizer(\n[\n    alpaca_prompt.format(\n        \"Continue the fibonnaci sequence.\",\n        \"1, 1, 2, 3, 5, 8\",\n        \"\",\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*NaSQ1vQKORU1I3DsOU2iOA.png)\n\n**8/ Save the model :** If you are happy with it, you can save your model or push it to Hugging Face Hub\n\n```python\nmodel.save_pretrained(\"lora_model\")\ntokenizer.save_pretrained(\"lora_model\")\n## model.push_to_hub(\"your_name/lora_model\", token = \"...\")\n## tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\")\n```\n\n**9/ Load the model :**\n\n```python\nif False:\n    from unsloth import FastLanguageModel\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name = \"lora_model\",\n        max_seq_length = max_seq_length,\n        dtype = dtype,\n        load_in_4bit = load_in_4bit,\n    )\n    FastLanguageModel.for_inference(model)\n```\n\n**10/ Using it for generation :**\n\n```python\ninputs = tokenizer(\n[\n    alpaca_prompt.format(\n        \"What is the capital of Palestine ?\",\n        \"\",\n        \"\",\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)\n```\n\nIf thereâ€™s a specific subject youâ€™d like us to cover, please donâ€™t hesitate to let me know! Your input will help shape the direction of my content and ensure it remains relevant and engaging ðŸ˜€\n\n\n"},{"lang":"en","group":"blog","slug":"blog/gemini-1-5-flash-vs-gpt-4o-88b9d8da8152","frontmatter":{"title":"New Gemini 1.5 FLASH Model: An Absolute Google Game Changer","meta_title":"New Gemini 1.5 FLASH Model: An Absolute Google Game Changer","description":"Gemini 1.5 Flash blows GPT-4o out of the water","date":"2024-11-08T00:27:31.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Reb1owOmiw5DFd4A.png","categories":["Programming","Machine Learning","Generative AI"],"author":"Rifx.Online","tags":["Gemini","Flash","GPT-4o","multi-modality","creativity"],"draft":false,"slug":"blog/gemini-1-5-flash-vs-gpt-4o-88b9d8da8152"},"content":"\nTheir new Gemini 1\\.5 Flash model blows GPT\\-4o out of the water and the capabilities are hard to believe.\n\n**Lightning fast**.\n\n\n\n33 times cheaper than GPT\\-4o but has a 700% greater context â€” **1 million tokens.**\n\nWhat is 1 million tokens in the real\\-world? Approximately:\n\n* Over an 1 hour of video\n* Over 30,000 lines of code\n* Over 700,000 words\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*E1XIOcpWfeqOZSZC.jpg)\n\nâŒGPT\\-4o cost:\n\n* Input: $2\\.50 per million tokens\n* Output: $10 per million tokens\n* Cached input: $1\\.25 per million tokens\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*XM3hFyS_PCcuv8Px.png)\n\nâœ… Gemini 1\\.5 Flash cost:\n\n* Input: $0\\.075 per million tokens\n* Output: $0\\.30 per million tokens\n* Cached input: $0\\.01875 per million tokens\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*d-1ioFlCxW3LB4SL.png)\n\nAnd then thereâ€™s the mini Flash\\-8B version for cost\\-efficient tasks â€” 66 times cheaper than GPT\\-4o:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*5B5ybLzTr7penwms.png)\n\nAnd the best part is the multi\\-modality â€” it can reason with text, files, images and audio in complex integrated ways.\n\nAnd 1\\.5 Flash has almost all the capabilities of Pro but much faster. And as a dev you can start using them now.\n\nGemini 1\\.5 Pro was tested with a 44\\-minute silent movie and astonishingly, it easily analyzed the movie into various plot points and events. Even pointing out tiny details that most of us would miss on first watch.\n\nMeanwhile the GPT\\-4o API only lets you work with text and images.\n\nYou can easily create, test and refine prompts in Googleâ€™s AI Studio â€” **completely free**.\n\nIt doesnâ€™t count in your billing like in OpenAI playground.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*5BKejWrJvrsEWIjc.png)\n\nJust look at the power of Google AI Studio â€” creating a food recipe based on an image:\n\nI uploaded this delicious bread from gettyimages:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*fC5YL_dplJ9Od_vN.jpg)\n\nNow:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*GezbFh9KzFXRVhr3.png)\n\nWhat if I want the response to be a specialized format for my API or something?\n\nThen you can just turn on JSON mode and specify the response schema:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*aRZuia7Iz_mI2s9b.png)\n\nOpenAI playground has this too, but itâ€™s not as intuitive to work with.\n\nAnother upgrade Gemini has over OpenAI is how creativity it can be.\n\nIn Gemini you can increase the `temperature` from 0 to 200% to control how random and creative the responses are:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*4AAFdAMfT_xyflmv.png)\n\nMeanwhile in OpenAI if you try going far beyond 100%, youâ€™ll most likely get a whole literal load of nonsense.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*yzFQL69pyJmgE9UB.png)\n\nAnd hereâ€™s the best part â€” when youâ€™re done creating your prompt you can just use **Get code** â€” easily copy and paste the boilerplate API code and move lightning\\-fast in your development.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*xgaZfVe9b8WSBMmq.png)\n\nWorks in several languages including Kotlin, Swift and Dart â€” efficient AI workflow in mobile dev.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*AMkfKm-3KQRxnltO.png)\n\nIn OpenAI playground you can get the code for Python and JavaScript.\n\n## Final thoughts\n\nGemini 1\\.5 Flash is a game\\-changer offering unparalleled capabilities at a fraction of the cost.\n\nWith its advanced multi\\-modality ease of use, generous free pricing, and creative potential it sets a new standard for AI leaving GPT\\-4o in the dust.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/gemma-vs-llama-vs-mistral-exploring-smaller-ai-models-672a95f4b9b7","frontmatter":{"title":"Gemma vs. Llama vs. Mistral: Exploring Smaller AI Models","meta_title":"Gemma vs. Llama vs. Mistral: Exploring Smaller AI Models","description":"A Comparative Study of Small-Scale Language Models: Evaluating Gemma, Llama 3, and Mistral in Reading Comprehension Tasks","date":"2024-11-10T22:36:54.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*TJqJ12YQCeYTS5fWOYR5Ig.png","categories":["Natural Language Processing","Machine Learning","Data Science"],"author":"Rifx.Online","tags":["Gemma","Llama","Mistral","SQuAD","Multi-Query"],"draft":false,"slug":"blog/gemma-vs-llama-vs-mistral-exploring-smaller-ai-models-672a95f4b9b7"},"content":"\n### A Comparative Study of Small\\-Scale Language Models: Evaluating Gemma, Llama 3, and Mistral in Reading Comprehension Tasks\n\n## Introduction\n\nLarge Language Models (LLMs) have been evolving rapidly. Each month, new models are developed to surpass the current top scorers in the market. This healthy competition is beneficial for creating new approaches that increase quality and speed. Additionally, companies are focused on developing smaller models to make them accessible to individuals or organizations without powerful computing resources.\n\nJust a few weeks ago, Apple introduced Apple Intelligence at their Worldwide Developers Conference. This is a set of multiple generative models fine\\-tuned to help users write and refine text, prioritize and summarize notifications, create images, and take in\\-app actions. The only foundational and proprietary model developed by Apple in that suite was introduced at the same conference. It is a small model designed to run on\\-device, where the hardware becomes a significant constraint. In Appleâ€™s case, the model is closed\\-source. What we know is that it is a \\~3 billion parameter model on par with the 7b versions of Gemma, Mistral, and Llama 3 (according to the results shared by Apple).\n\nWhile Appleâ€™s new model is exciting, we cannot test or reuse it. Hence, we are more interested in publicly available models since developers and companies can use them to build new products and services. Itâ€™s important to distinguish between open LLMs and open\\-source LLMs. Historically, open\\-source software refers to computer programs released under specific licenses, making the source code available for public use or modification. With LLMs, there is additional complexity, including the training data and model weights. Therefore, open LLMs typically disclose the model weights and initial code. An open\\-source LLM, on the other hand, would share every step of the training process, including the training data, along with a permissive license. It should allow others to use, build upon, and further distribute the model. Nevertheless, most of the models released these days fall under the category of open LLMs since, for example, they do not publish the datasets used for training purposes. This is the case for Gemma by Google, Mistral by Mistral AI, and Llama by Meta.\n\nIn this article, we analyze Gemma more closely to understand what differentiates these smaller models. Gemma is one of the most recently developed models released by Google. It comes in two versions, 2 billion and 7 billion parameters. Thus, it can be used on edge devices, and it aims to outperform state\\-of\\-the\\-art models like Mistral and Llama 3\\.\n\nAdditionally, we apply Gemma, Llama 3, and Mistral to a reading comprehension dataset called SQuAD. The LLMs are tasked with answering specific questions based on given contexts. We assess their performance using quantitative metrics such as inference speed and average answer length. We also use the Relative Answer Quality (RAQ) framework proposed by \\[1]. RAQ bridges the gap in evaluating LLMs for specific use cases by ranking answers based on their accuracy relative to the ground truth, providing a more nuanced and practical assessment of model performance.\n\n\n\nAs always, the code is available on our [GitHub](https://github.com/zaai-ai/lab).\n\n## Gemma: the base text model of Gemini\n\nGoogle released Gemma \\[2], an open LLM developed based on its powerful, closed\\-source model, Gemini \\[3].\n\nGoogle released pre\\-trained and fine\\-tuned checkpoints to promote further research of the model in new use cases, making it available in two different sizes:\n\n* The 7B model is to be deployed and further developed on GPU or TPU.\n* The 2B model is designed to address computation constraints and allow its use on CPU or on\\-device applications.\n\nGemma promises to achieve state\\-of\\-the\\-art performance compared to other open models with roughly the same scale, like Llama 3 7B or Mistral 7B. This should happen across different domains, such as question answering, common sense reasoning, mathematics/science, and coding.\n\n## Gemma: what is new?\n\nGemmaâ€™s architecture is based on a decoder\\-only \\[4] Transformer \\[5] with a context length of 8192 tokens. Letâ€™s explore the approach taken to make it smaller.\n\n## Multi\\-Query Attention\n\nThe 2B model utilizes Multi\\-Query Attention (MQA) to significantly reduce the memory resources required to load all query, key, and value heads, as opposed to the Multi\\-Head Attention (MHA) approach. MQA achieves this memory reduction by using a single key and value for multiple query heads in the attention layer, as illustrated in Figure 3\\.\n\nWhile this approach allows Gemma 2B to be deployed on devices with smaller memory resources, it can lead to quality degradation and training instability. Therefore, the authors opted to use MHA in the 7B version, following the same method as Llama 3\\.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*cgSktHmd_iQeTU4DwWLCPQ.png)\n\n## RoPE Embeddings\n\nTransformers require Positional Embeddings because they are inherently order\\-invariant. This means that without positional information, a Transformer would represent sentences with the same words but different orders and meanings in the same way. For example:\n\n> *Sentence 1:* Gemma is better than Llama 3\n\n> *Sentence 2:* Llama 3 is better than Gemma\n\nPositional information is typically represented using two sinusoidal functions (sine and cosine). Then, a unique positional embedding is created for each position in the sequence based on its position, the token embedding dimension, and the model dimension.\n\nTherefore, adding positional information is crucial for enabling Transformers to process text properly. The original Transformer architecture used **Absolute Positional Embeddings**, where a vector representation of a position is added to the vector representation of a token.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*cU5a_5-ATKwrQVeka-ViXQ.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*JZLrvgvc7l_52uewCrPSbg.png)\n\nThe challenge with Absolute Positional Embeddings is that they do not explicitly encode the relative distances between tokens. While they capture positional information using sine and cosine functions, these embeddings are calculated independently for each position. This means that the model does not inherently understand the proximity or relational significance of different positions within a sequence. For instance, the embeddings for tokens at positions 1 and 2 may appear similar due to the nature of the sinusoidal functions, but the model doesnâ€™t explicitly recognize that these positions are adjacent.\n\nBecause of this, the model might not differentiate the relationship between tokens at positions 1 and 2 from the relationship between tokens at positions 1 and 500\\. In natural language processing, words that are close together in a sentence often share more context or have a stronger semantic or syntactic relationship than words that are far apart. Absolute Positional Embeddings might not completely capture this nuance. It can lead to limitations in capturing long\\-range dependencies or the hierarchical structure of language.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*p-fG2ydLbOhJHjO7Y0LyUw.png)\n\nRotary Positional Embeddings (RoPE) \\[6] address this problem by modeling the relative positions of tokens through a rotation of the token embeddings in the sequence.\n\nLetâ€™s use the previous example, *â€˜Gemma is better than Llama*,â€™ and consider each word as a token represented by a 2D vector. The word *better* will be represented by a 2D vector rotated from the original vector based on its position *m* and a constant angle Î¸, as shown in Figure 5\\.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*nX3llo0cwBIrCQ8Gn21-gg.png)\n\nThis approach preserves the relative distance between tokens because the rotational transformation maintains the same similarity between vectors, regardless of their position in the sequence. For instance, if we add two words to the original sentence, making it â€˜*The LLM Gemma is better than Llama*â€™, the positions of *better* and *than* change from (3 \\& 4\\) to (5 \\& 6\\). However, since the rotation angle remains consistent, the similarity between these vectors (as measured by the dot product) stays the same, ensuring consistent relative positioning.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*6cpWPXTexZC8YQbnasHOUg.png)\n\n## GeGLU Activation Function\n\nThe authors replaced the traditional ReLU activation function with a variant of a Gated Linear Unit (GLU) called GeGLU, as another study \\[7] has shown that it improves the quality of the output generated by the LLM.\n\nThere are two differences between the ReLU and GeGLU:\n\n1. **Activation function** â€” GeGLU uses a Gaussian Error Linear Unit (GELU) \\[8] function that differs from ReLU in the sense that it multiplies the neuron input *x* by a cumulative distribution function of the normal distribution. In this case, the probability of *x* being dropped is higher as *x* decreases.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*FXCfQpvdMJXPk5s6AO-RuA.png)\n\n2\\. **Sigmoid Activated** â€” The simple ReLU or GELU activation function is applied between the hidden representation *x* andtwo linear transformations represented by two matrices (*W1* and *W2\\).* The Gating variant in GeGLU applies a gating mechanism (sigmoid) to one of the components, as shown in Equation 3\\.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Z9hUjuy4NvQVDrPj6iSfrQ.png)\n\n## Normalizer Location\n\nThe last modification to the original Transformer architecture is shown in Figure 8\\. The authors normalize both the input and output of each transformer sub\\-layer to improve training stability, contrary to the original paper, which only normalized the output.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*NQe4ME2MhvRWVzobVdIloA.png)\n\nThey also replaced the traditional LayerNorm function with RMSNorm \\[8]. It is computationally more efficient while maintaining training stability improvements and helping the model converge.\n\nRMSNorm achieves better efficiency because its authors demonstrated that the benefits of LayerNorm come from re\\-scaling invariance rather than re\\-centering invariance. Re\\-scaling invariance means that the output of the normalization process remains unchanged if a constant factor scales the input. In other words, multiplying all the inputs by a constant does not affect the normalized outputs. Re\\-centering invariance means that the output of the normalization process remains unchanged if a constant value is added to all the inputs. This implies that shifting all inputs by a constant amount does not affect the normalized outputs. This finding allows the removal of the overhead of computing the mean (you only need to compute the standard deviation), making RMSNorm simpler and more efficient.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*qblXBo8SCcxzPWePhFVlYg.png)\n\n## Mistral AI vs. Meta vs. Google: a comparison between Gemma 7B vs. Llama 3 7B vs. Mistral 7B\n\nIn this section, we put 3 LLMs â€” Gemma 7B, Mistral 7B, and Llama 3 7B â€” to a test. We use a question\\-answering dataset under the License CC BY\\-SA 4\\.0 called SQuAD (it can be found [here](https://huggingface.co/datasets/rajpurkar/squad)). This dataset is a reading comprehension dataset consisting of questions about a set of Wikipedia articles. Based on context, the models should be able to retrieve the correct answer to a question. The 3 more important fields for our use case are:\n\n* `question` \\- the question a model should answer.\n* `context` \\- background information from which the model needs to extract the answer.\n* `answers` \\- the text answer to the question.\n\nThe evaluation process will consist of two quantitative metrics:\n\n* `words per second` \\- assesses the inference speed.\n* `words` \\- assesses the length of the answer.\n\nTo assess the accuracy of the models in our use case, we use RAQ \\[1]. RAQ ranks the answers of all LLMs using an independent LLM based on how close they are to the ground truth answer.\n\nWe start by downloading the models in a `.gguf` format to be able to run them in CPU, and we place them under the folder `model/`.\n\nWe used the instruct version of each model with a 4\\-bit quantization:\n\n* `mistral-7b-instruct-v0.1.Q4_K_M.gguf` from [https://huggingface.co/TheBloke/Mistral\\-7B\\-Instruct\\-v0\\.1\\-GGUF/tree/main](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/tree/main)\n* `Meta-Llama-3-8B-Instruct-Q4_K_M.gguf` from [https://huggingface.co/NousResearch/Meta\\-Llama\\-3\\-8B\\-Instruct\\-GGUF](https://huggingface.co/NousResearch/Meta-Llama-3-8B-Instruct-GGUF)\n* `gemma-7b-it-Q4_K_M.gguf` from [https://huggingface.co/rahuldshetty/gemma\\-7b\\-it\\-gguf\\-quantized/tree/main](https://huggingface.co/rahuldshetty/gemma-7b-it-gguf-quantized/tree/main)\n\nAfter that, we import all the libraries and our generator that receives the model we want to use as an argument.\n\n```python\nimport os\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scikit_posthocs as sp\nimport pandas as pd\nimport utils\n\nfrom dotenv import load_dotenv\nfrom datasets import load_dataset\nfrom generator.generator import Generator\n\nllama = Generator(model='llama')\nmistral = Generator(model='mistral')\ngemma = Generator(model='gemma')\nload_dotenv('env/var.env')\n```\n\nThis class is responsible for importing the model parameters defined in a `config.yaml` file with the following characteristics: `context_length` of 1024, `temperature` of 0\\.7, and `max_tokens` of 2000\\.\n\n```python\ngenerator:\n  llama:\n    llm_path: \"model/Meta-llama-3-8B-Instruct-Q4_K_M.gguf\"\n  mistral:\n    llm_path: \"model/mistral-7b-instruct-v0.1.Q4_K_M.gguf\"\n  gemma:\n    llm_path: \"model/gemma-7b-it-Q4_K_M.gguf\"\n  context_length: 1024\n  temperature: 0.7\n  max_tokens: 2000\n```\n\nIt also creates the Prompt Template. This template helps format the query and the context before passing it to the LLM to get a response.\n\n```python\nfrom langchain import PromptTemplate\nfrom langchain.chains import LLMChain\nfrom langchain.llms import LlamaCpp\n\nfrom base.config import Config\nclass Generator(Config):\n    \"\"\"Generator, aka LLM, to provide an answer based on some question and context\"\"\"\n    def __init__(self, model) -> None:\n        super().__init__()\n    # template\n        self.template = \"\"\"\n            Use the following pieces of context to answer the question at the end.\n            {context}\n            Question: {question}\n            Answer:\n        \"\"\"\n   # load llm from local file\n        self.llm = LlamaCpp(\n            model_path=f\"{self.parent_path}/{self.config['generator'][model]['llm_path']}\",\n            n_ctx=self.config[\"generator\"][\"context_length\"],\n            temperature=self.config[\"generator\"][\"temperature\"],\n        )\n        # create prompt template\n        self.prompt = PromptTemplate(\n            template=self.template, input_variables=[\"context\", \"question\"]\n        )\n    def get_answer(self, context: str, question: str) -> str:\n        \"\"\"\n        Get the answer from llm based on context and user's question\n        Args:\n            context: most similar document retrieved\n            question: user's question\n        Returns:\n            llm answer\n        \"\"\"\n        query_llm = LLMChain(\n            llm=self.llm,\n            prompt=self.prompt,\n            llm_kwargs={\"max_tokens\": self.config[\"generator\"][\"max_tokens\"]},\n        )\n        return query_llm.run({\"context\": context, \"question\": question})\n```\n\nWith the LLMs loaded, we fetch the SQuAD dataset from HuggingFace and shuffle it to ensure enough variety in the question theme.\n\n```python\nsquad = load_dataset(\"squad\", split=\"train\")\nsquad = squad.shuffle()\n```\n\nNow, we can loop over 60 questions and contexts and record the metrics mentioned above.\n\n```python\nfor i in range(60):\n    context = squad[i]['context']\n    query = squad[i]['question']\n    answer = squad[i]['answers']['text'][0]\n\n    # Llama\n    answer_llama, words_per_second, words = utils.get_llm_response(llama, context, query)\n    llama_metrics[\"words_per_second\"].append(words_per_second)\n    llama_metrics[\"words\"].append(words)\n    # mistral\n    answer_mistral, words_per_second, words = utils.get_llm_response(mistral, context, query)\n    mistral_metrics[\"words_per_second\"].append(words_per_second)\n    mistral_metrics[\"words\"].append(words)\n    # gemma\n    answer_gemma, words_per_second, words = utils.get_llm_response(gemma, context, query)\n    gemma_metrics[\"words_per_second\"].append(words_per_second)\n    gemma_metrics[\"words\"].append(words)\n  \n    # GPT-3.5 rank\n    llm_answers_dict = {'llama': answer_llama, 'mistral': answer_mistral, 'gemma': answer_gemma}\n    rank = utils.get_gpt_rank(answer, llm_answers_dict, os.getenv(\"OPENAI_API_KEY\"))\n    llama_metrics[\"rank\"].append(rank.index('1')+1)\n    mistral_metrics[\"rank\"].append(rank.index('2')+1)\n    gemma_metrics[\"rank\"].append(rank.index('3')+1)\n```\n\nThe function `get_llm_response` is responsible for receiving the loaded LLM, the context, and the question and return the LLM answer as well as the quantitative metrics.\n\n```python\ndef get_llm_response(model: Generator, context: str, query: str) -> Tuple[str, int, int]:\n    \"\"\"\n    Generates an answer from a given LLM based on context and query\n    returns the answer and the number of words per second and the total number of words\n    Args:\n        model: LLM\n        context: context data\n        query: question\n    Returns:\n        answer, words_per_second, words\n    \"\"\"\n    init_time = time.time()\n    answer_llm = model.get_answer(context, query)\n    total_time = time.time()-init_time\n    words_per_second = len(re.sub(\"[^a-zA-Z']+\", ' ', answer_llm).split())/total_time\n    words = len(re.sub(\"[^a-zA-Z']+\", ' ', answer_llm).split())\n    return answer_llm, words_per_second, words\n```\n\nWe can see that Llama 3 is faster than Mistral and Gemma by producing on average \\~0\\.7 words per second, while Mistral produces \\~0\\.26 and Gemma \\~0\\.4 words. In terms of answer length, Llama 3 also produces longer answers than Mistral and Gemma, with an average answer length of 148 words against 20 words for Mistral and 50 for Gemma. Finally, based on RAQ, Mistral had the best average rank of approximately 1\\.81, followed by Gemma with an average of 2\\.05, while Llama 3 performed worse with an average rank of approximately 2\\.1\\.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GVeFQbMZZ5oUScVEHQPu8A.png)\n\nThe RAQ framework also includes a statistical test to understand if the observed differences are significant. Table 1 displays the results of the Dunn post\\-hoc test, comparing the performance of different language models. Each cell indicates whether the difference in performance between the respective models is statistically significant at a 5% significance level. â€œSignificantâ€ denotes a statistically significant difference (p\\-value â‰¤ 0\\.05\\), while â€œNot Significantâ€ indicates no statistically significant difference (p\\-value \\> 0\\.05\\). For the selected significance level, the Dunn test result shows that the difference in performance between models is not significant.\n\n```python\np_values = sp.posthoc_dunn([Llama_metrics['rank'], mistral_metrics['rank'], gemma_metrics['rank']], p_adjust='holm')\np_values > 0.05\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ftCaagMKAm5RzeATYm_7Ug.png)\n\nIt is always important to assess qualitatively some examples. Below, we have the answers from the 3 models to the question *â€˜Power House Day is celebrated on what day in New Haven?â€™* based on the following context:\n\n> ***Context:***â€˜For over a century, New Haven citizens had fought in the colonial militia alongside regular British forces, as in the French and Indian War. As the American Revolution approached, General David Wooster and other influential residents hoped that the conflict with the government in Britain could be resolved short of rebellion. On 23 April 1775, which is still celebrated in New Haven as Powder House Day, the Second Company, Governorâ€™s Foot Guard, of New Haven entered the struggle against the governing British parliament. Under Captain Benedict Arnold, they broke into the powder house to arm themselves and began a three\\-day march to Cambridge, Massachusetts. Other New Haven militia members were on hand to escort George Washington from his overnight stay in New Haven on his way to Cambridge. Contemporary reports, from both sides, remark on the New Haven volunteersâ€™ professional military bearing, including uniforms.â€™\n\nAll 3 models gave correct answers. While Llama 3 and Gemma provided more complete answers, Mistral was more succinct.\n\n> ***Llama 3 answer:***â€˜New Havenâ€™s Powder House Day is celebrated on April 23rd.â€™\n\n> ***Gemma answer:***â€˜Sure! The text states on which day Powder House Day is celebrated on: Powder House Day is celebrated on **23 April** in New Haven.â€™\n\n> ***Mistral answer:***â€™23 Aprilâ€™\n\n## Conclusion\n\nOn\\-device models present a great opportunity to enhance user experiences by making powerful LLMs accessible on devices with lower computational resources. Both Apple and Google are actively developing smaller, more efficient models to meet this need, enabling more people to benefit from advanced AI in their daily lives.\n\nIn this article, we explored Gemma, the open LLM developed by Google, which introduced four novel features to the traditional Transformer architecture: Multi\\-Query Attention in the 2B version, RoPE embeddings for positional encoding, GeGLU as the activation function, and input normalization.\n\nWe also compared Gemma's performance against Llama 3 and Mistral on a reading comprehension dataset. We observed that Gemma produced more words per second and wrote longer answers than Mistral, but it did not surpass Llama 3 in these metrics. Using the RAQ framework, we assessed the accuracy of the three models. While the data showed better results from Mistral, followed by Gemma, the differences were not statistically significant. Therefore, we can say that the 3 models performed similarly when applied to our use case of reading comprehension.\n\n## References\n\n\\[1] LuÃ­s Roque, Rafael Guedes. Research to Production: Relative Answer Quality (RAQ) and NVIDIA NIM. [https://readmedium.com/research\\-to\\-production\\-relative\\-answer\\-quality\\-raq\\-and\\-nvidia\\-nim\\-15ce0c45b3b6](https://readmedium.com/research-to-production-relative-answer-quality-raq-and-nvidia-nim-15ce0c45b3b6), 2024\\.\n\n\\[2] Gemma Team, Google DeepMind. Gemma: Open Models Based on Gemini Research and Technology, 2023\\.\n\n\\[3] Gemini Team. Gemini: A family of highly capable multimodal models, 2023\\.\n\n\\[4] Noam Shazeer. Fast Transformer Decoding: One Write\\-Head is All You Need. arXiv:1911\\.02150, 2019\\.\n\n\\[5] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. Attention Is All You Need. arXiv:1706\\.03762, 2017\\.\n\n\\[6] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, Yunfeng Liu. RoFormer: Enhanced Transformer with Rotary Position Embedding. arXiv:2104\\.09864, 2021\\.\n\n\\[7] Noam Shazeer. GLU Variants Improve Transformer. arXiv:2002\\.05202, 2020\\.\n\n\\[8] Dan Hendrycks, Kevin Gimpel. Gaussian Error Linear Units (GELUs). arXiv:1606\\.08415, 2016\\.\n\n\\[9] Biao Zhang, Rico Sennrich. Root Mean Square Layer Normalization. arXiv:1910\\.07467, 2019\\.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/generating-structured-data-from-an-image-with-gpt-vision-and-langchain-34aaf3dcb215","frontmatter":{"title":"Generating structured data from an image with GPT vision and Langchain","meta_title":"Generating structured data from an image with GPT vision and Langchain","description":"In todayâ€™s world, where visual data is abundant, the ability to extract meaningful information from images is becoming increasinglyâ€¦","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*FPRRg85jYb7MrzXEpNWbmw.jpeg","categories":["Programming","Computer Vision","Natural Language Processing"],"author":"Rifx.Online","tags":["Langchain","GPT","vision","LLMs","structured"],"draft":false,"slug":"blog/generating-structured-data-from-an-image-with-gpt-vision-and-langchain-34aaf3dcb215"},"content":"\n\n\n\n\n\nIn todayâ€™s world, where visual data is abundant, the ability to extract meaningful information from images is becoming increasingly valuable. Langchain, a powerful framework for building applications with large language models (LLMs), offers a versatile toolset for tackling this challenge. In this article, weâ€™ll explore how to use Langchain to extract structured information from images, such as counting the number of people and listing the main objects.\n\nBefore diving into the code, letâ€™s set the stage by understanding the task at hand. Imagine you have an image of a scene, such as a city street. Your goal is to extract valuable information from this image, including the number of people present and a list of the main objects in the scene.\n\n\n## About Langchain\n\nLangchain is a comprehensive framework that allows developers to build sophisticated applications by leveraging the power of large language models (LLMs). It provides a modular and extensible architecture, enabling developers to create custom pipelines, agents, and workflows tailored to their specific needs.\n\nLangchain simplifies the integration of LLMs, offering abstractions and utilities for handling various data sources, including text, images, and structured data. It supports a wide range of LLMs from different providers, such as OpenAI and Anthropic, making it easy to switch between models or combine multiple models in a single application.\n\n\n## Preparing the Environment and Setting Up the OpenAI API Key\n\nTo follow along with this tutorial, youâ€™ll need to have Langchain installed. You can install it using pip:\n\n\n```python\npip install langchain langchain_openai\n```\nTo use the OpenAI language models with Langchain, youâ€™ll need to obtain an API key from OpenAI. If you donâ€™t have an API key yet, you can sign up for one on the OpenAI website (<https://openai.com/api/>).\n\nOnce you have your API key, you can set it as an environment variable in your system or provide it directly in your code. Hereâ€™s an example of how to set the API key as an environment variableCopy code\n\n\n```python\nexport OPENAI_API_KEY=\"your_openai_api_key_here\"\n```\nAlternatively, you can provide the API key directly in your Python code:\n\n\n```python\nimport os\nimport langchain\nos.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key_here\"\n```\nAfter setting up the API key, Langchain will be able to authenticate with the OpenAI API and use their language models.\n\n\n## Loading and Encoding the Image\n\nBefore we can process images with Langchain, we need to load the image data from a file and encode it in a format that can be passed to the language model. The code below defines a function `load_image` that takes a dictionary with an `image_path` key and returns a new dictionary with an `image` key containing the image data encoded as a base64 string.\n\n\n```python\ndef load_image(inputs: dict) -> dict:\n    \"\"\"Load image from file and encode it as base64.\"\"\"\n    image_path = inputs[\"image_path\"]\n  \n    def encode_image(image_path):\n        with open(image_path, \"rb\") as image_file:\n            return base64.b64encode(image_file.read()).decode('utf-8')\n    image_base64 = encode_image(image_path)\n    return {\"image\": image_base64}\n```\nThe `load_image` function first extracts the `image_path` from the input dictionary. It then defines a nested function `encode_image` that opens the image file in binary mode, reads its contents, and encodes them as a base64 string using the `base64.b64encode` function from the Python standard library.\n\nThe `load_image` function calls `encode_image` with the provided `image_path` and stores the resulting base64-encoded string in the `image_base64` variable. Finally, it returns a new dictionary with the `image` key set to `image_base64`.\n\nTo integrate this function into a Langchain pipeline, we can create a `TransformChain` that takes the `image_path` as input and produces the `image` (base64-encoded string) as outputCopy code\n\n\n```python\nload_image_chain = TransformChain(\n    input_variables=[\"image_path\"],\n    output_variables=[\"image\"],\n    transform=load_image\n)\n```\nWith this setup, we can easily load and encode images as part of a larger Langchain workflow, enabling us to process visual data alongside text using large language models.\n\n\n## Defining the Output Structure\n\nBefore we can extract information from the image, we need to define the structure of the output we want to receive. In this case, weâ€™ll create a Pydantic model called `ImageInformation` that includes fields for the image description and any additional information we might want to extract.\n\n\n```python\nfrom langchain_core.pydantic_v1 import BaseModel, Field\n\nclass ImageInformation(BaseModel):\n \"\"\"Information about an image.\"\"\"\n image_description: str = Field(description=\"a short description of the image\")\n people_count: int = Field(description=\"number of humans on the picture\")\n main_objects: list[str] = Field(description=\"list of the main objects on the picture\")\n```\n\n## Setting up the Image Model\n\nNext, weâ€™ll create a chain that combines the image loading and encoding steps with the LLM invocation step. Since the `ChatOpenAI` model is not natively capable of handling both text and image inputs simultaneously (to my unsderstanding), we'll create a wrapper chain to achieve this functionality.\n\n\n```python\nfrom langchain.chains import TransformChain\nfrom langchain_core.messages import HumanMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain import globals\nfrom langchain_core.runnables import chain\n\n## Set verbose\nglobals.set_debug(True)\n\n@chain\ndef image_model(inputs: dict) -> str | list[str] | dict:\n \"\"\"Invoke model with image and prompt.\"\"\"\n model = ChatOpenAI(temperature=0.5, model=\"gpt-4-vision-preview\", max_tokens=1024)\n msg = model.invoke(\n             [HumanMessage(\n             content=[\n             {\"type\": \"text\", \"text\": inputs[\"prompt\"]},\n             {\"type\": \"text\", \"text\": parser.get_format_instructions()},\n             {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{inputs['image']}\"}},\n             ])]\n             )\n return msg.content\n```\nIn this code snippet, we define a chain called `image_model` that invokes the `ChatOpenAI` model with the provided prompt, format instructions, and image. The `image_model` chain accepts a dictionary `inputs` containing the prompt and the base64-encoded image string.\n\nInside the chain, we create a `HumanMessage` object that combines the prompt text, format instructions, and the image URL, formatted as a data URI with the base64-encoded image data. We then invoke the `ChatOpenAI` model with this `HumanMessage` object, using the `gpt-4-vision-preview` model, which is specifically designed for multimodal tasks involving both text and images.\n\nThe model processes both the text prompt and the image, and returns the output.\n\n\n## Putting It All Together\n\nNow that we have all the necessary components, we can define a function that orchestrates the entire process:\n\n\n```python\nfrom langchain_core.output_parsers import JsonOutputParser\n\nparser = JsonOutputParser(pydantic_object=ImageInformation)\ndef get_image_informations(image_path: str) -> dict:\n   vision_prompt = \"\"\"\n   Given the image, provide the following information:\n   - A count of how many people are in the image\n   - A list of the main objects present in the image\n   - A description of the image\n   \"\"\"\n   vision_chain = load_image_chain | image_model | parser\n   return vision_chain.invoke({'image_path': f'{image_path}', \n                               'prompt': vision_prompt})\n```\nIn this function, we define a prompt that asks the LLM to provide a count of the people in the image and a list of the main objects. We then create a chain that combines the image loading step (`load\\_image\\_chain`), the LLM invocation step (`image\\_model`), and a JSON output parser (`parser`). Finally, we invoke this chain with the image path and the prompt, and the function returns a dictionary containing the extracted information.\n\n\n## Example Usage\n\nTo use this function, simply provide the path to an image file:\n\n\n```python\nresult = get_image_informations(\"path/to/your/image.jpg\")\nprint(result)\n```\nThis will output a dictionary with the requested information, such as:\n\n\n```python\n{\n 'description': 'a view of a city showing cars waiting at a traffic light',\n 'people_count': 5,\n 'main_objects': ['car', 'building', 'traffic light', 'tree']\n}\n```\n\n## Conclusion\n\nLangchain provides a powerful toolset for working with large language models and extracting valuable information from various data sources, including images. By combining Langchainâ€™s capabilities with custom prompts and output parsing, you can create robust applications that can extract structured information from visual data.\n\nRemember, the quality of the output will depend on the capabilities of the LLM youâ€™re using and the specificity of your prompts. Experiment with different models and prompts to find the best solution for your use case.\n\nIf you find a better way to achieve the same results or have suggestions for improvements, please donâ€™t hesitate to share them in the comments. The code examples provided in this article are meant to serve as a starting point, and there may be alternative approaches or optimizations .\n\n\n"},{"lang":"en","group":"blog","slug":"blog/generative-ai-in-market-research-and-intelligence-benefits-use-cases-and-strategies-4e9195a07ffc","frontmatter":{"title":"Generative AI in Market Research and Intelligence: Benefits, Use Cases, and Strategies","meta_title":"Generative AI in Market Research and Intelligence: Benefits, Use Cases, and Strategies","description":"Generative AI is revolutionizing market research by enabling faster, more comprehensive data analysis and insights generation. It addresses traditional methods limitations, such as high costs, time consumption, and limited sample sizes. Key applications include trend analysis, sentiment monitoring, and personalized customer interactions. The technology enhances decision-making by automating report generation, improving accuracy, and allowing real-time adjustments. As generative AI evolves, it will play a crucial role in shaping market intelligence, driving efficiency, and fostering data-driven strategies for businesses.","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*0J_wBAutefsvq5PyxPL17Q.jpeg","categories":["Generative AI","Market Research","Data Science"],"author":"Rifx.Online","tags":["generative","market","research","insights","automation"],"draft":false,"slug":"blog/generative-ai-in-market-research-and-intelligence-benefits-use-cases-and-strategies-4e9195a07ffc"},"content":"\n\n\n\n\n### Transforming Market Intelligence with AI\n\n\n\nGenerative AI is changing the way businesses approach market research and intelligence by enabling faster and more in\\-depth analysis of data. Traditional market research methods rely heavily on manual data collection, survey analysis, and competitor research, which can be time\\-consuming and limited in scope. Generative AI, on the other hand, allows companies to analyze vast datasets instantly, generate insights that might otherwise go unnoticed, and create predictive models that help forecast trends with greater accuracy. From synthesizing data reports to creating high\\-level summaries, generative AI accelerates decision\\-making and makes market insights more accessible.\n\nBeyond data analysis, generative AI is transforming how companies engage with their audiences and refine their market strategies. Through the use of large language models (LLMs) and natural language processing (NLP), businesses can create simulations, forecast trends, and perform competitive analysis more effectively. This guide explores the primary benefits, real\\-world use cases, and strategies to leverage generative AI for enhanced market research. By integrating AI into market intelligence workflows, companies can stay competitive and make data\\-driven decisions that resonate with their audienceâ€™s needs.\n\n\n## Traditional Market Research and Its Limitations\n\nTraditional market research involves well\\-established methods like surveys, focus groups, interviews, and competitive analysis, which are widely used by companies to understand their customers, competitors, and industry trends. These approaches have been the backbone of market intelligence, offering structured and validated insights for strategic decision\\-making. However, while effective, traditional market research faces notable limitations in todayâ€™s dynamic and data\\-saturated environment.\n\n\n### 1\\. High Costs\n\n* Traditional methods often involve substantial financial investments, particularly for comprehensive studies.\n* Costs accumulate from hiring researchers, conducting surveys, setting up focus groups, and analyzing results.\n* Small and medium\\-sized businesses may find these costs prohibitive, limiting access to in\\-depth insights.\n\n\n### 2\\. Time\\-Intensive Processes\n\n* Collecting, processing, and analyzing data through traditional methods can take weeks or even months.\n* In fast\\-moving industries, insights gathered through slow processes may become outdated before they are actionable.\n* The need for real\\-time data is growing, but traditional methods struggle to deliver insights at that pace.\n\n\n### 3\\. Limited Sample Size and Scope\n\n* Traditional research is often constrained by budget, time, and logistical limitations, which can limit sample diversity and size.\n* Focus groups and interviews may not fully represent broader audience segments, leading to potential blind spots in research findings.\n* This limitation affects the accuracy of demographic, psychographic, and behavioral insights, especially when scaling globally.\n\n\n### 4\\. Difficulty in Analyzing Unstructured Data\n\n* Traditional methods are usually geared towards structured data (e.g., responses in surveys) rather than unstructured data like social media posts, customer reviews, or forum discussions.\n* Valuable insights from unstructured data require more complex analysis, which traditional methods may lack the resources or techniques to handle effectively.\n\n\n### 5\\. Limited Flexibility and Adaptability\n\n* Traditional research designs are generally fixed, making it challenging to pivot or adjust studies mid\\-course.\n* For example, in response to unforeseen market events or shifts, new research often needs to be initiated from scratch.\n* This rigidity can hinder a brandâ€™s ability to react swiftly to changing market conditions or emerging trends.\n\n\n## How Generative AI is Transforming Market Research?\n\nGenerative AI, with its ability to process vast amounts of data and produce meaningful insights, is revolutionizing traditional market research. By addressing the limitations of conventional methods, generative AI enables faster, more comprehensive, and cost\\-effective market insights. Hereâ€™s a look at how generative AI is reshaping the field of market research:\n\n\n### 1\\. Data Generation and Augmentation\n\n* Synthetic Data Creation: Generative AI models can create synthetic datasets that mimic real\\-world data, helping researchers overcome limitations in sample size and diversity.\n* Enhanced Scenario Modeling: By simulating different market scenarios, AI allows companies to explore â€œwhat\\-ifâ€ situations and test how market conditions might impact consumer behavior or product success.\n\n\n### 2\\. Rapid Content Generation for Insights\n\n* Automated Report Writing: Generative AI can produce detailed market research reports, summarizing data into easy\\-to\\-understand narratives that are quickly digestible by stakeholders.\n* Customized Insights: AI tools like ChatGPT or Jasper can quickly generate tailored insights based on specific data inputs, providing marketers with audience\\-specific reports, competitor analysis, and customer journey summaries.\n\n\n### 3\\. Sentiment Analysis and Social Listening\n\n* Real\\-Time Sentiment Monitoring: AI models can analyze social media posts, reviews, and forums to extract real\\-time sentiment, tracking how customers feel about products, services, or brands.\n* Trend Identification: Generative AI can identify emerging trends from unstructured data sources by spotting keywords, themes, and emotional tones, helping companies stay on top of consumer preferences.\n\n\n### 4\\. Advanced Persona and Scenario Development\n\n* Detailed Consumer Personas: AI can generate highly nuanced consumer personas by analyzing demographics, psychographics, and behavior patterns across various datasets.\n* Hypothetical Scenario Creation: Generative AI enables the creation of potential consumer scenarios to explore how audiences might react to new products, services, or changes in messaging, providing companies with a more dynamic approach to market testing.\n\n\n### 5\\. Enhanced Competitor and Industry Analysis\n\n* Real\\-Time Market Positioning: Generative AI can monitor competitor actions, messaging, and pricing strategies, offering companies a constantly updated view of their market position.\n* Predictive Analytics for Market Trends: Using historical data, generative AI can project future market trends, helping companies anticipate changes in consumer preferences, emerging product needs, or shifts in competitive positioning.\n\n\n### 6\\. Personalized Customer Interactions and Insights\n\n* Hyper\\-Personalization of Content: AI enables highly personalized content generation for customer segments, from tailored ad copy to product recommendations based on individual consumer behavior.\n* Enhanced Customer Feedback Loops: By processing and synthesizing feedback from multiple channels, generative AI can uncover specific insights that improve product development, customer service, and marketing messaging.\n\n\n### 7\\. Automated Survey and Feedback Analysis\n\n* Natural Language Processing for Survey Responses: Generative AI can analyze open\\-ended survey responses, summarizing consumer feedback, and identifying common themes without the need for manual processing.\n* Bias Detection and Correction: AI models can be trained to detect and adjust for biases in survey responses, making findings more representative of the overall population.\n\n\n## Key Applications of Generative AI in Market Intelligence\n\nGenerative AI has introduced groundbreaking applications in market intelligence, enabling companies to gain deeper, real\\-time insights with enhanced efficiency and scalability. Here are some of the primary ways generative AI is transforming market intelligence:\n\n\n### 1\\. Trend Analysis and Forecasting\n\n* Market Trends Identification: Generative AI models analyze large datasets, like social media, search data, and news, to identify emerging trends before they become mainstream.\n* Demand Forecasting: AI\\-driven models predict demand shifts based on historical data and market indicators, enabling companies to align their strategies with projected consumer needs.\n* Cultural and Social Trend Analysis: AI scans and synthesizes trends in popular culture, lifestyle changes, and social issues to help brands stay aligned with consumer values.\n\n\n### 2\\. Persona and Scenario Development\n\n* Detailed Consumer Personas: Generative AI creates rich, data\\-driven personas based on demographic, behavioral, and psychographic data. These personas help in targeted marketing, product design, and customer experience planning.\n* Scenario Planning: By simulating various market conditions and consumer reactions, generative AI enables businesses to visualize the impact of different strategies, allowing more informed decision\\-making.\n\n\n### 3\\. Sentiment Analysis for Real\\-Time Consumer Feedback\n\n* Customer Sentiment Tracking: Generative AI tools process unstructured data from reviews, social media posts, and surveys to gauge consumer sentiment in real\\-time.\n* Emotional and Behavioral Insights: By analyzing the language and tone in customer feedback, AI can reveal deeper emotional drivers, enabling brands to tailor their messaging and product development accordingly.\n* Brand Health Monitoring: AI\\-generated sentiment analysis provides ongoing insights into brand perception, allowing companies to respond quickly to shifts in public opinion.\n\n\n### 4\\. Competitor and Industry Analysis\n\n* Competitive Intelligence: AI\\-driven tools track competitorsâ€™ actions across channels, analyzing their pricing strategies, product launches, and customer feedback. This helps brands adapt their strategies to stay competitive.\n* Industry Benchmarking: Generative AI aggregates industry data to set benchmarks, allowing businesses to measure their performance against others in their sector and adjust their strategies as needed.\n* Anticipating Market Movements: With historical data, AI predicts possible competitor actions and market shifts, enabling companies to proactively adjust their strategies.\n\n\n### 5\\. Content Generation for Reports and Insights\n\n* Automated Insight Summaries: AI can automatically generate insights reports, summarizing data in an easily digestible format for stakeholders, saving time and resources.\n* Custom Market Intelligence Reports: Generative AI produces personalized reports on specific markets or segments, offering targeted insights that align closely with business goals.\n* Translation and Localization: Generative AI can translate market insights and reports across languages and cultural contexts, enabling businesses to scale market intelligence globally.\n\n\n### 6\\. Customer Journey Mapping and Predictive Insights\n\n* Dynamic Customer Journey Analysis: AI synthesizes data from multiple touchpoints to map customer journeys, highlighting critical decision points and preferences in the customer lifecycle.\n* Predictive Consumer Behavior: Using patterns from historical data, AI anticipates future consumer behavior, allowing companies to adjust their messaging, product recommendations, and marketing timing.\n* Churn Prediction and Retention Analysis: By analyzing consumer interactions and satisfaction scores, AI identifies at\\-risk customers, helping companies take proactive retention actions.\n\n\n### 7\\. Product Ideation and Development\n\n* Idea Generation Based on Consumer Preferences: AI can analyze consumer preferences, feedback, and trending keywords to suggest product features or entirely new products, aiding the ideation process.\n* Competitive Feature Analysis: By tracking competitorsâ€™ product features and customer responses, generative AI helps brands refine their product roadmap and prioritize features that resonate with the target audience.\n* Testing Product Concepts: Generative AI simulates market reactions to potential products, allowing companies to refine concepts before investing in full\\-scale development.\n\n\n### 8\\. Hyper\\-Personalization and Targeting\n\n* Segmented Campaigns: Generative AI enables marketers to generate customized messages for specific audience segments, enhancing relevance and engagement.\n* Real\\-Time Ad Copy and Content Customization: AI models can dynamically adjust ad copy, email content, and website messaging based on user behavior and preferences, making marketing efforts more effective.\n* Recommendation Engines: Leveraging customer behavior and sentiment, AI creates personalized product recommendations, boosting sales and customer satisfaction.\n\n\n## Quantifying the Benefits of Generative AI in Market Research\n\nGenerative AI is revolutionizing market research by making data\\-driven insights more accessible, accurate, and cost\\-effective. Quantifying the benefits helps businesses understand how AI can improve research efficiency and enhance their return on investment (ROI). Here are some key metrics and examples of how generative AI drives measurable benefits in market research:\n\n\n### 1\\. Cost Reduction\n\n* Reduced Labor Costs: Traditional research often requires extensive manpower for data collection, analysis, and reporting. Generative AI automates many of these tasks, significantly cutting labor costs.\n* Lowered Data Collection Expenses: Generative AI models can analyze vast data sources like social media, forums, and consumer reviews at a fraction of the cost of traditional survey methods.\n* Example: A company that previously spent $100,000 annually on market research might save up to 40% by using generative AI tools, reducing expenses to $60,000 while obtaining similar or richer insights.\n\n\n### 2\\. Time Savings and Faster Insights\n\n* Accelerated Research Cycles: AI tools analyze data in real\\-time, reducing the time required to gather insights from weeks or months to days or even hours.\n* Rapid Reporting: Automated report generation cuts down analysis time, allowing stakeholders to access insights sooner and make faster decisions.\n* Example: A business launching a new product can access real\\-time feedback from customer reviews and social media, using generative AI to provide insights within 24 hours versus a traditional three\\-week turnaround for similar analysis.\n\n\n### 3\\. Enhanced Data Accuracy and Consistency\n\n* Improved Data Quality: Generative AI processes large volumes of data and uses advanced analytics to identify trends with greater accuracy than manual analysis.\n* Bias Reduction: By synthesizing feedback from diverse sources, AI minimizes the biases that may arise from small or homogenous samples.\n* Example: By utilizing AI\\-driven sentiment analysis, a company can achieve a 20% increase in the accuracy of consumer sentiment tracking compared to manual methods, leading to more reliable predictions of market shifts.\n\n\n### 4\\. Scalability and Flexibility\n\n* Handling Large Datasets: AI models can process massive datasets without the scalability limitations of traditional research, enabling companies to analyze large and varied data sources for a holistic view.\n* Flexible Application: Generative AI allows businesses to adjust research parameters in real\\-time based on changing goals or new data, offering a level of flexibility that is challenging to achieve with traditional methods.\n* Example: An international brand can quickly scale its sentiment analysis across multiple countries and languages, gaining insights into regional consumer differences without needing separate, labor\\-intensive studies.\n\n\n### 5\\. Personalization and Targeted Insights\n\n* Customized Reporting: AI can generate tailored insights for different business functions (e.g., marketing, product development, customer service), ensuring each team has relevant data for decision\\-making.\n* Real\\-Time Segmentation: Generative AI models can segment customers based on live data, providing more precise insights into specific audiences.\n* Example: A retail company gains a 30% increase in ad engagement by using generative AI to create hyper\\-personalized campaigns that align with distinct customer preferences identified through AI\\-driven market segmentation.\n\n\n### 6\\. Predictive Accuracy for Strategic Planning\n\n* Improved Forecasting: AI\\-driven predictive analytics can forecast trends with high accuracy, helping companies anticipate market shifts and prepare for potential disruptions.\n* Enhanced Risk Management: AI models that predict customer behavior help companies proactively address challenges like churn, shifting demand, or negative brand sentiment.\n* Example: A subscription\\-based service uses generative AI to achieve a 25% increase in customer retention by predicting churn rates more accurately and implementing timely interventions.\n\n\n### 7\\. Higher ROI from Actionable Insights\n\n* Informed Decision\\-Making: Faster, accurate, and data\\-rich insights enable companies to make decisions that directly impact revenue, enhancing ROI from market research investments.\n* Real\\-Time Adjustments: With live market data, companies can refine their strategies in real\\-time, responding immediately to market changes and boosting performance.\n* Example: A brand uses AI insights to adjust a product feature mid\\-launch, resulting in a 15% increase in customer satisfaction and a 10% revenue increase in the first quarter.\n\n\n### Quantitative Recap: The Tangible Impact of Generative AI in Market Research\n\nGenerative AI has quantifiable benefits across multiple areas of market research:\n\n* 30â€“50% Reduction in Research Costs due to automation and reduced reliance on traditional surveys.\n* 60â€“80% Reduction in Analysis Time with AI\\-powered real\\-time processing and automated reporting.\n* 20â€“30% Increase in Data Accuracy by leveraging AI\\-driven analytics and reducing bias.\n* 25% Improvement in Customer Retention Rates through predictive AI models that anticipate consumer behavior.\n* 10â€“15% Revenue Growth by enabling more precise and agile decision\\-making based on real\\-time data.\n\n\n## Future Trends in Generative AI for Market Research\n\nAs generative AI continues to evolve, it is poised to bring transformative changes to the field of market research. Here are some emerging trends that are expected to shape the future of AI\\-driven market intelligence:\n\n\n### 1\\. Real\\-Time, Continuous Market Monitoring\n\n* Trend Overview: Instead of relying on periodic surveys and reports, generative AI will enable continuous, real\\-time monitoring of consumer sentiments, competitor actions, and market trends.\n* Impact: This shift allows brands to react instantly to market changes, minimizing the gap between data collection and action. Continuous insights also help in better forecasting and agile decision\\-making.\n* Example: A fashion retailer can track shifting consumer preferences in real\\-time and adjust inventory, marketing, and pricing strategies almost immediately.\n\n\n### 2\\. Increased Personalization with Hyper\\-Specific Audience Segmentation\n\n* Trend Overview: Generative AI will advance the ability to segment audiences into highly specific niches based on behavioral, demographic, and psychographic data.\n* Impact: Marketers will be able to tailor messages and products to micro\\-segments, increasing relevance and engagement with each consumer.\n* Example: AI can create personalized campaigns for niche groups, like eco\\-conscious millennials interested in sustainable fashion, optimizing engagement and conversion rates.\n\n\n### 3\\. Enhanced Predictive Capabilities Using Multimodal Data Integration\n\n* Trend Overview: Future generative AI tools will analyze multimodal data sources â€” text, images, videos, and voice â€” to deliver deeper insights and more accurate predictions.\n* Impact: Integrating diverse data types provides a more holistic view of market trends and consumer preferences, enabling more nuanced predictions.\n* Example: A beauty brand could analyze social media images, text, and influencer videos to predict trending colors and styles for upcoming seasons.\n\n\n### 4\\. Synthetic Data Generation for Market Simulation and Testing\n\n* Trend Overview: Generative AI will increasingly be used to create synthetic data that mimics real\\-world consumer behavior, allowing brands to test products and campaigns without releasing them to the market.\n* Impact: Synthetic data can reduce the risk and costs of market testing by allowing brands to evaluate consumer responses and optimize campaigns in a simulated environment.\n* Example: A new beverage brand might use synthetic data to test consumer reactions to different packaging designs, reducing time and cost associated with traditional focus groups.\n\n\n### 5\\. Automated Insights and Decision\\-Making Support\n\n* Trend Overview: Generative AI will progress toward autonomous insights generation, with tools capable of recommending actions based on real\\-time market data.\n* Impact: Automated insights free up resources and allow businesses to make faster, data\\-backed decisions.\n* Example: An AI tool might not only flag a drop in consumer sentiment but also suggest marketing adjustments or product updates to address the issue.\n\n\n### 6\\. Ethical AI and Enhanced Data Privacy\n\n* Trend Overview: With increased emphasis on ethical AI, future market research tools will prioritize transparency, responsible data usage, and adherence to privacy regulations.\n* Impact: Consumers will feel more confident sharing data with brands that practice ethical AI, while brands avoid legal and reputational risks associated with data misuse.\n* Example: Generative AI tools may incorporate features that ensure compliance with privacy laws and allow consumers to consent to data collection and use.\n\n\n### 7\\. Conversational AI for Deeper, Interactive Consumer Insights\n\n* Trend Overview: Conversational AI will evolve to facilitate dynamic, interactive surveys and feedback collection, allowing brands to gather deeper insights directly from consumers.\n* Impact: Real\\-time, conversational feedback allows for more authentic insights and higher engagement rates, particularly among digital\\-native consumers.\n* Example: A tech brand could deploy conversational AI to engage consumers in a dialogue about their experiences, collecting nuanced insights that static surveys might miss.\n\n\n### 8\\. Augmented Reality (AR) and Virtual Reality (VR) Market Research\n\n* Trend Overview: Generative AI\\-powered AR and VR environments will become common tools for immersive market research, allowing consumers to interact with virtual products and environments.\n* Impact: This technology enables companies to test product concepts in a realistic virtual setting, collecting behavioral data and consumer feedback in an engaging way.\n* Example: A furniture retailer could use VR environments to show customers how items would look in their own spaces, gathering feedback on product designs and usability.\n\n\n### 9\\. Human\\-AI Collaborative Research Models\n\n* Trend Overview: The future of market research will likely involve hybrid models that combine human expertise with AI\\-driven automation, where researchers oversee AI\\-generated insights and contextualize findings.\n* Impact: Human oversight ensures the quality and ethical use of AI\\-generated insights, adding a layer of judgment and expertise that AI alone may lack.\n* Example: AI might generate reports on emerging market trends, while researchers validate findings and add qualitative insights, ensuring actionable and ethical decision\\-making.\n\n\n### 10\\. End\\-to\\-End AI Platforms for Integrated Market Intelligence\n\n* Trend Overview: As generative AI tools evolve, more platforms will offer end\\-to\\-end capabilities, including data collection, analysis, insights generation, and strategy recommendations â€” all within a single ecosystem.\n* Impact: These comprehensive platforms streamline market research workflows, reduce dependency on multiple tools, and make it easier for brands to access holistic, actionable insights.\n* Example: An integrated platform could handle everything from social listening and competitor analysis to customer segmentation and campaign recommendations, allowing brands to focus on execution rather than data handling.\n\n\n## Conclusion\n\nIncorporating generative AI into market research and intelligence processes empowers organizations to gain actionable insights at unprecedented speed. By automating data collection and analysis, generative AI reduces human error, enhances precision, and opens up new avenues for market discovery that were once time\\-prohibitive. These tools not only streamline research but also enrich decision\\-making processes, enabling companies to stay adaptive in rapidly changing markets.\n\nGenerative AI will likely become indispensable to market research as AI technology advances. Companies adopting generative AI can expect significant improvements in forecasting accuracy, competitor analysis, and customer understanding. However, successful adoption requires a strategic approach, prioritizing the most relevant AI tools and integrating human expertise for quality control and interpretation. Generative AI represents a dynamic and evolving toolset that, when used effectively, can shape the future of market intelligence and drive sustainable growth.\n\n\n## FAQs\n\n1. What is generative AI, and how does it apply to market research?\nGenerative AI uses machine learning to analyze large datasets and generate insights, enabling quicker, more comprehensive market research and predictive analysis.\n2. What are the benefits of using generative AI in market intelligence?\nGenerative AI improves data accuracy, reduces time spent on data collection, and enhances decision\\-making by providing deeper insights and trend forecasts.\n3. Can generative AI replace traditional market research methods?\nWhile generative AI can automate many aspects, human expertise is still essential for context interpretation, quality control, and nuanced analysis.\n4. What are some use cases of generative AI in market research?\nUse cases include trend forecasting, competitor analysis, customer sentiment analysis, and the generation of market insights through NLP.\n5. How can businesses get started with generative AI in market research?\nBegin by identifying key areas in research that AI can enhance, then choose appropriate generative AI tools and integrate them into existing workflows with expert oversight.\n\n"},{"lang":"en","group":"blog","slug":"blog/get-chatgpt-to-sound-more-human-essential-tips-for-creating-natural-engaging-ai-conversations-c361bc2680bb","frontmatter":{"title":"Get ChatGPT to Sound More Human: Essential Tips for Creating Natural, Engaging AI Conversations","meta_title":"Get ChatGPT to Sound More Human: Essential Tips for Creating Natural, Engaging AI Conversations","description":"The article provides strategies to make ChatGPTs responses sound more human and engaging. Key tips include limiting overused words and phrases, embracing simplicity and clarity, matching tone to context, focusing on practical and relatable content, and using real-world examples. By adopting these practices, ChatGPT can deliver responses that are clearer, more conversational, and less mechanical, enhancing user interaction and understanding.","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*wdWBBG4fJhHVwdoDelYkkQ.png","categories":["Chatbots","Natural Language Processing","Programming/Scripting"],"author":"Rifx.Online","tags":["ChatGPT","responses","human","engaging","clarity"],"draft":false,"slug":"blog/get-chatgpt-to-sound-more-human-essential-tips-for-creating-natural-engaging-ai-conversations-c361bc2680bb"},"content":"\n\n\n\n\n\nHave you ever found your AI assistant sounding a bit tooâ€¦ mechanical? ChatGPT, while impressively capable, sometimes leans on overly formal or generic language. But with a few adjustments, you can guide ChatGPT to deliver responses that feel more human, conversational, and relatable.\n\nHereâ€™s a handy guide to help ChatGPT sound less like a robot and more like a knowledgeable friend.\n\n\n## 1\\. Limit Overused Words and Phrases\n\nCertain words and phrases pop up a lot in AI\\-generated text because theyâ€™re versatile and safe, but they can feel impersonal and vague. Here are some that tend to crop up too often:\n\n**Overused Transitional Words**\n\n* Instead of â€œadditionally,â€ â€œconsequently,â€ or â€œnevertheless,â€ encourage ChatGPT to go with simpler, more natural alternatives like â€œalsoâ€ or â€œbut.â€\n\n**Frequently Used Adjectives and Nouns**\n\n* Common adjectives like â€œinnovative,â€ â€œrobust,â€ and â€œdynamicâ€ are fine but can sound generic. Encourage ChatGPT to use more specific descriptors. Similarly, abstract nouns like â€œefficiency,â€ â€œoptimization,â€ and â€œtransformationâ€ are often better replaced with specific terms related to the topic.\n\n**Common Verbs and Phrases**\n\n* Phrasal clichÃ©s like â€œa testament toâ€¦â€ and verbs like â€œfacilitateâ€ or â€œmaximizeâ€ can sound formulaic. Replacing them with straightforward verbs like â€œhelp,â€ â€œimprove,â€ or â€œincreaseâ€ adds a natural, conversational touch.\n\nHereâ€™s how these instructions might play out:\n\n**Before:**â€œIn summary, leveraging data\\-driven insights facilitates optimization across dynamic landscapes.â€\n\n**After:**â€œTo sum it up, using data effectively helps businesses make better decisions.â€\n\n\n## 2\\. Embrace Simplicity and Clarity in Language\n\n**Keep It Straightforward**\n\n* ChatGPT often uses complex sentences and formal language, which can create a barrier between the user and the message. Encourage simpler sentence structures and direct language to make responses clearer and more approachable.\n\n**Limit Vague Statements**\n\n* If something feels vague, prompt ChatGPT to add a specific detail. For example, instead of saying, â€œItâ€™s worth considering,â€ ChatGPT can state exactly what should be considered and why.\n\n**Avoid Long, Rambling Sentences**\n\n* Long sentences sound overly formal and robotic. Encourage breaking down complex ideas into shorter sentences, each with one main idea. This makes the response feel casual and conversational.\n\n**Example:****Before:**â€œMoreover, it is important to note that optimizing your processes can lead to significant efficiency gains.â€\n\n**After:**â€œIf you improve your processes, you can make things more efficient and save time.â€\n\n\n## 3\\. Match the Tone to the Context\n\nOne of the quickest ways to make AI responses sound more natural is to adjust the tone based on the scenario.\n\n**Adapt Formality**\n\n* ChatGPT sometimes uses overly formal language in casual contexts, or overly casual language in business scenarios. Adjusting tone based on the userâ€™s need makes the response feel more appropriate and human\\-like.\n\n**Avoid Abstract Ideas in Favor of Real\\-World Details**\n\n* Generic statements donâ€™t always provide value. Encourage ChatGPT to use concrete examples or relatable details to get the point across more clearly. Rather than saying, â€œThis drives transformation,â€ itâ€™s more relatable to say, â€œThis can lead to new ways of doing things, like speeding up production or cutting down costs.â€\n\n**Before:**â€œIn conclusion, leveraging data insights can drive impactful business transformations.â€\n\n**After:**â€œUsing data effectively can help a business grow by improving things like production speed or reducing expenses.â€\n\n\n## 4\\. Ask ChatGPT to Focus on Being Engaging and Relatable\n\nArtificial\\-sounding language often stems from an overly professional tone or abstract ideas that donâ€™t feel connected to a real person. Here are a few tips to make responses more engaging and relatable:\n\n**Make Responses Practical and Useful**\n\n* Rather than providing generic advice, ChatGPT can offer practical tips and specific steps the user can take.\n\n**Avoid Jargon**\n\n* Industry\\-specific jargon or technical terms can make the response sound stiff and distant. Encouraging ChatGPT to simplify its language when possible helps everyone stay on the same page.\n\n**Example:****Before:**â€œTo increase efficiency, a systematic evaluation of resource allocation is paramount.â€\n\n**After:**â€œTo save time and resources, check if everything is being used in the best way possible.â€\n\n\n## 5\\. Try Real\\-World Examples and Direct Statements\n\nPeople tend to speak in clear, concrete ways, especially when explaining something. ChatGPT can mirror this by using examples that relate to the userâ€™s real world.\n\n**Use Relatable Examples**\n\n* Generic responses become more memorable when real\\-world examples are included. For instance, instead of saying, â€œThis can improve operational processes,â€ ChatGPT might suggest, â€œThis might help you cut down wait times in customer service.â€\n\n**Encourage Direct Statements Instead of Qualifiers**\n\n* â€œItâ€™s important to note thatâ€¦â€ can often be shortened to the main point itself. Direct statements not only keep things simple but also build confidence in the message.\n\n**Example:****Before:**â€œItâ€™s worth considering that process improvements could lead to significant cost reductions.â€\n\n**After:**â€œImproving processes can help you save a lot of money.â€\n\n\n## Wrapping It Up\n\nWith these adjustments, you can make ChatGPT speak in a way thatâ€™s more natural, conversational, and engaging. Less jargon, shorter sentences, and simpler language help responses sound less like a robot and more like the helpful, informed assistant we all appreciate.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/glm-4-voice-9b-real-time-multilingual-voice-conversation-ai-install-locally-in-minutes-ce2fcd6c8fd8","frontmatter":{"title":"GLM-4-Voice 9Bâ€Šâ€”â€ŠReal-time Multilingual Voice Conversation AIâ€Šâ€”â€ŠInstall Locally in Minutes","meta_title":"GLM-4-Voice 9Bâ€Šâ€”â€ŠReal-time Multilingual Voice Conversation AIâ€Šâ€”â€ŠInstall Locally in Minutes","description":"GLM-4-Voice 9B is an advanced multilingual voice conversation AI developed by Zhipu AI, enabling real-time interaction in English and Chinese. Its unique end-to-end architecture allows direct speech processing, minimizing latency and enhancing user experience. The model supports customizable voice attributes, including emotion and intonation, making it suitable for various applications. With a straightforward local setup process and high-performance requirements, GLM-4-Voice is poised to advance conversational AI in diverse fields such as customer service and education.","date":"2024-11-13T01:32:04.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*LATTpEc2AHvqgVyPKSzW7A.jpeg","categories":["Voice Assistants","Natural Language Processing","Chatbots"],"author":"Rifx.Online","tags":["multilingual","conversation","real-time","customization","performance"],"draft":false,"slug":"blog/glm-4-voice-9b-real-time-multilingual-voice-conversation-ai-install-locally-in-minutes-ce2fcd6c8fd8"},"content":"\n### How to set up GLM\\-4\\-Voice 9B for seamless real\\-time voice interaction in English and Chinese, and explore its unique architecture, low\\-latency response, and customizable voice attributes.\n\n\n\n## Introduction\n\nIn recent years, voice\\-enabled AI has significantly advanced, enabling conversational agents to better understand and respond to human speech. From virtual assistants to customer service bots, voice AI has become an essential tool in various industries. However, most models still struggle with fluently transitioning between languages, understanding nuances in spoken queries, and delivering high\\-quality responses. This is where GLM\\-4\\-Voice by Zhipu AI shines. Developed as an end\\-to\\-end voice model, GLM\\-4\\-Voice pushes the boundaries of multilingual conversational AI by supporting real\\-time dialogue in both English and Chinese, while offering an adaptable and human\\-like response generation.\n\nIn this article, weâ€™ll explore why GLM\\-4\\-Voice is worth paying attention to, what makes it unique, and how you can set it up and start using it locally. Weâ€™ll also take a look at its architecture and provide a hands\\-on guide to accessing the web demo.\n\n## Why GLM\\-4\\-Voice?\n\nTraditional language models are often limited to text and require additional processing layers to handle voice. They may also struggle with interactivity or suffer from latency issues. GLM\\-4\\-Voice overcomes these limitations with a unified model that directly processes and generates speech. Hereâ€™s what makes it stand out:\n\n1. **End\\-to\\-End Voice Processing**: Unlike many other models that rely on a separate text\\-to\\-speech (TTS) or speech\\-to\\-text (STT) module, GLM\\-4\\-Voice directly interprets and responds in spoken language, allowing a more seamless and responsive experience.\n2. **Multilingual Support**: This model excels in handling both English and Chinese, two widely used languages globally. Its ability to switch between languages fluidly makes it ideal for bilingual environments and international applications.\n3. **Customizable Attributes**: GLM\\-4\\-Voice allows for adjustments in emotion, intonation, speech rate, and even dialect, making it capable of producing more natural and contextually appropriate responses.\n4. **Low Latency**: With support for streaming inference, the model has a latency of around 20 tokens, which enables near\\-instantaneous responses in real\\-time conversations.\n\n## Specialties of GLM\\-4\\-Voice\n\nGLM\\-4\\-Voice brings several unique features to the table, setting it apart from other voice models. Hereâ€™s what makes it special:\n\n* **Real\\-Time Voice Interaction**: By supporting low\\-latency responses, GLM\\-4\\-Voice can maintain fluid and natural conversations, which is crucial for applications like customer support and interactive AI.\n* **Dynamic Voice Attributes**: Users can specify the modelâ€™s emotional tone, speaking rate, and other characteristics, making interactions more engaging and suited to various contexts.\n* **Bilingual Support with Context Awareness**: This model is designed to comprehend and generate responses in both Chinese and English. It can switch between these languages seamlessly, offering a flexible solution for multilingual applications.\n* **Advanced Speech Decoding**: Built on CosyVoice, the GLM\\-4\\-Voice decoder enables high\\-quality speech generation with streaming support, maintaining high clarity in both languages.\n\n## Architecture\n\nThe architecture of GLM\\-4\\-Voice consists of three primary components, each fulfilling a crucial role in achieving end\\-to\\-end voice interaction:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*nJsKHtxSblNkixPIBZpWyQ.jpeg)\n\n1. **GLM\\-4\\-Voice\\-Tokenizer**: This component tokenizes continuous speech input into discrete tokens, with around 12\\.5 tokens generated per second of audio. The tokenizer is based on Whisperâ€™s encoder, with added vector quantization, allowing the model to process audio in a structured form.\n2. **GLM\\-4\\-Voice\\-9B**: The core language model, based on the GLM\\-4 architecture, has been aligned to process spoken input. It can handle both text and speech, making it a powerful multimodal conversational agent.\n3. **GLM\\-4\\-Voice\\-Decoder**: This decoder converts the discrete tokens back into continuous speech, allowing the model to produce audio outputs. It supports streaming inference, enabling responses to begin after processing just a few tokens, minimizing conversation latency.\n\nTogether, these components make GLM\\-4\\-Voice a powerful tool for real\\-time voice interactions, supporting conversational AI across different languages and dialects.\n\n## Setting Up GLM\\-4\\-Voice Locally\n\nTo experience GLM\\-4\\-Voice, follow these steps to set up the model locally on your machine.\n\n### Step 1: Clone the Repository\n\nBegin by cloning the repository from GitHub. Make sure to include submodules:\n\n```python\n!git clone --recurse-submodules https://github.com/THUDM/GLM-4-Voice\ncd GLM-4-Voice\n```\n\n### Step 2: Install Dependencies\n\nNavigate into the project directory and install the necessary dependencies:\n\n```python\n!pip install -r requirements.txt\n```\n\n### Step 3: Download the Model Checkpoint\n\nGLM\\-4\\-Voiceâ€™s decoder model is hosted on Hugging Face and requires `git-lfs` to download. Make sure `git-lfs` is installed, then run:\n\n```python\n!git clone https://huggingface.co/THUDM/glm-4-voice\n```\n\n### Step 4: Launch the Model Service\n\nWith everything set up, start the model server:\n\n```python\npython model_server.py --model-path glm-4-voice-9b\n```\n\n### Step 5: Start the Web Service\n\nOnce the model server is running, start the web service by executing:\n\n```python\npython web_demo.py\n```\n\nYou can now access the web demo at [http://127\\.0\\.0\\.1:8888](http://127.0.0.1:8888) to interact with GLM\\-4\\-Voice.\n\n> **Note:** The GLM\\-4\\-Voice model is resource\\-intensive and requires a substantial amount of computational power to run effectively. Specifically, it necessitates 35â€“40 GPUs for optimal performance, making it suitable for deployment in environments with access to high\\-performance hardware. Users should ensure they have the necessary infrastructure in place before attempting to utilize this model.\n\n## Web Demo Interface\n\nThe web demo for GLM\\-4\\-Voice provides an intuitive interface with several customization options:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*scbHOUXqMW5KGAcT3Bq1Eg.png)\n\n* **Input Mode**: Users can choose to provide input as either text or audio. This flexibility allows for hands\\-free or traditional interaction.\n* **Voice Control Parameters**: Adjust temperature, top\\-p, and token limits to customize the modelâ€™s response characteristics.\n* **Debug Information**: Input and output tokens are displayed, giving users insight into the modelâ€™s processing of their queries.\n* **Interactive Audio Display**: Audio inputs and responses are displayed as waveforms, and users can replay or review audio segments to assess quality.\n\nHowever, Gradio, which is used to stream audio in the demo, may sometimes present instability. For best quality, itâ€™s recommended that audio from the dialogue box be replayed after it has been generated.\n\n## Conclusion\n\nGLM\\-4\\-Voice stands out as an impressive achievement in conversational AI, offering a unique blend of bilingual support, real\\-time audio interaction, and flexible response customization. Its end\\-to\\-end design and low latency make it a prime candidate for applications in customer service, education, virtual assistants, and more. With an accessible setup process, GLM\\-4\\-Voice opens the door for developers and researchers to explore advanced voice capabilities in both Chinese and English.\n\nAs the demand for more interactive and realistic AI continues to grow, models like GLM\\-4\\-Voice represent a significant step forward in bridging language and conversational barriers. Whether youâ€™re looking to build a chatbot, a virtual teacher, or a customer service agent, GLM\\-4\\-Voice provides a robust and versatile solution.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/google-gemini-are-big-context-windows-the-killer-feature-72ff95488fb1","frontmatter":{"title":"Google Gemini: Are Big Context Windows the Killer Feature?","meta_title":"Google Gemini: Are Big Context Windows the Killer Feature?","description":"Goggleâ€™s upcoming LLM makes a massive move","date":"2024-11-10T22:36:54.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MteQrQSTXLuJcd86RbjQrg.png","categories":["Machine Learning","Natural Language Processing","Data Science"],"author":"Rifx.Online","tags":["Gemini","tokens","context","LLM","evolution"],"draft":false,"slug":"blog/google-gemini-are-big-context-windows-the-killer-feature-72ff95488fb1"},"content":"\n### Goggleâ€™s upcoming LLM makes a massive move\n\n\n\nBarely eight months ago, a leaked Google email revealed the company was struggling to outpace its AI rivals. Not only was there [no moat](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither) around their AI offerings â€” in other words, no built up business advantageâ€” Google also had [no secret sauce](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither) that could change things. And even as they were grappling with the problem, they were watching the gap between privately funded AI projects like theirs and open source AI models closing with â€œastonishingâ€ speed.\n\nItâ€™s too soon to know how this story ends. Maybe open source AI will continue to build on its early successes, or maybe it will be smothered by the AIs run by massively wealthy competitors like Google, Microsoft, and Apple, and their mind\\-boggling quantities of data. Right now the conflict is still unfolding, as different organizations roll out a rapid series of AI advancements. Recently, Google took the spotlight in this arena, when it announced a preview of its newest LLM, [Gemini 1\\.5 Pro](https://deepmind.google/technologies/gemini/). Another day, another Large Language Model â€” or so it seemed, until Google described a startling change.\n\nGemini 1\\.5 Pro explodes the *context window*â€”essentially, a measure of how much data an LLM can track at once. In past versions, Gemini had a context window of up to 128,000 tokens, just like GPT\\-4\\. But Geminiâ€™s new context window fits **1 million** tokens, and the implications of that change are enormous.\n\nBut before we can talk about the effect of context windows on LLM capabilities, we need to back up and quickly review how context windows work.\n\n## Context windows (in a nutshell)\n\nIn simple terms, the context window sets how much of your information an LLM can remember during an interaction. If youâ€™re using ChatGPT, for example, the context window consists of the current prompt you gave it, everything else youâ€™ve typed in to that conversation before, and every reply ChatGPT has sent back your way. Talk long enough, and the old parts of the conversation will slip out of the context window, and ChatGPT will abruptly forget those details.\n\nA 128,000 token context window sounds large, but the number is deceptive. First, consider that an average word is actually 1 to 3 tokens when itâ€™s broken down for an LLM. (The rule of thumb is 4 tokens for 3 words, but it increases as the language becomes more complex or in specialized fields, like law or medicine.) When you look at long documents, ongoing interactions, and AI\\-powered applications, youâ€™ll quickly find that you canâ€™t fit everything you want an LLM to know in its context window.\n\nFor that reason, weâ€™ve developed some clever ways to work around the context window limitation. For example:\n\n* **Chunking.** You can break down a large amount of data and get the LLM to look at it one piece at a time. This works well for some tasks (summarizing a long document), but not as well if you need to analyze concepts that span the entire document.\n* **Fine\\-tuning.** You can train the LLM with your specific data. The key problem, other than time and expense, is that your new data is easily overwhelmed by the much larger set of general purpose training data the LLM has already absorbed. Often, it just wonâ€™t stick. And besides, many LLMs donâ€™t support fine\\-tuning at all â€” including GPT\\-4 and Gemini.\n* **Retrieval augmented generation (RAG).** First, you convert your text content into a special representation, called *embeddings*. (Embeddings are an important part of how LLMs work. Essentially, theyâ€™re a numeric representation that captures the meaning of content.) Once you have the embeddings, you place them in a vector database. Now you can use the magic of *semantic search* to look at a prompt and find pieces of conceptually related content in your database, which you feed into the LLM. In other words, youâ€™re giving it just the important stuff.\n\nThe last point is the most common approach today. RAG is efficient and predictable. It works amazingly well if you have a massive collection of loosely related documents. For example, imagine youâ€™re creating a tech support chatbot that draws its information from your companyâ€™s knowledge base articles. With RAG, you find the relevant data, and give that to the LLM with your prompt. Essentially, youâ€™re telling the LLM where to look when it answers a prompt.\n\nBut RAG isnâ€™t perfect. It forces you to spend much more time preparing your data. It doesnâ€™t make it easy to jump into a completely new dataset. And itâ€™s not effective if you really do need to consider a huge bulk of information at once â€” for example, it youâ€™re looking for overarching themes in a novel or features in a codebase. But despite its limitations, RAG is pretty close to a best practice today.\n\nAt least, it was until Gemini 1\\.5 Pro flipped the script.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*EEHKDSH0wXa-J6veK5etZA.png)\n\n## The wow moment\n\nAlthough Gemini 1\\.5 Pro isnâ€™t released yet, itâ€™s available in a tightly limited trial. And the results have been eye opening.\n\nSome of the most impressive examples show Gemini creating analyses that span massive bodies of knowledge. Googleâ€™s demos are predictably impressive, but theyâ€™ve been accused of staging demonstations and cherry picking examples in the past. Iâ€™m more interested in independent testers, who have reported results that are no less remarkable.\n\nFor example, Conor Grennan [fed a 300 page novel](https://www.youtube.com/watch?v=-MKGsijn5tI) to Gemini and asked it to describe main characters, find plot twists, and identify examples of characters feeling certain emotions. Gemini had no trouble developing nuanced arguments that reasoned across the entire span of the book. Jeff Delaney, the creator of the popular [Fireship channel](https://www.youtube.com/c/fireship) on YouTube, fed Gemini an entire codebase with thousands of files and asked it to add new features. Not only did Gemini write the correct code, it followed the style of the existing project, using the components, libraries, and conventions that were already established. Other demonstrations show Gemini identifying issues in an application, extracting key examples, and writing API documentation.\n\nAnd if you want something else to fill up Geminiâ€™s enormous context window, thereâ€™s another new feature â€” video. Video is tokenized differently than words, and takes much more space. But even so, a 1 million token context window can hold about an hour of video â€” enough to look through a movie and answer complex questions about its content. Thatâ€™s what Google did when it asked Gemini to [find specific details](https://www.youtube.com/watch?v=wa0MT8OwHuk) in a Buster Keaton movie, like the words written on a scrap of paper in a scene they didnâ€™t identify.\n\n## The LLMs of the future\n\nAre large context windows the way of the future? Up until now, the common wisdom was that large context windows were a partial solution at best. We worried that theyâ€™d be prohibitively expensive in compute time. [One study](https://www.voiceflow.com/blog/the-context-window-paradox-why-bigger-might-not-be-better) found that LLMs werenâ€™t particularly good at finding information in the middle of long context windows, and performed better with details that occurr towards the beginning or end. All these factors supported the same conclusion: Brute forcing your content into the context window was naÃ¯ve and cost\\-prohibitive. Dumping all your data into one request wasnâ€™t every going to be the right way to talk to an LLM.\n\nNow it seems like the future has suddenly shifted. Large context windows are on the horizon, and they could give LLMs a more capable, holistic understanding of broad knowledge sets. Tasks that were impossible to do with text last year are about to become possible now *in video*. And Google Research is playing with a variant of Gemini that expands the context window to a staggering 10 million tokens.\n\nTwo facts are clear. First, picking a winner in the LLM wars is a foolâ€™s game. And second, the pace of change isnâ€™t slowing â€” itâ€™s picking up speed.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/google-releases-gemma-a-lightweight-and-open-source-model-b6411d67ecca","frontmatter":{"title":"Google Releases Gemmaâ€Šâ€”â€ŠA Lightweight And Open Source Model","meta_title":"Google Releases Gemmaâ€Šâ€”â€ŠA Lightweight And Open Source Model","description":"Google released Gemma, a family of lightweight and open-source models built upon the research and technology used to create the Geminiâ€¦","date":"2024-10-29T12:46:34.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*G7XbkhsCwillpje7AvETjQ.jpeg","categories":["Natural Language Processing","Programming","Chatbots"],"author":"Rifx.Online","tags":["Gemma","Gemini","parameters","NLP","chatbots"],"draft":false,"slug":"blog/google-releases-gemma-a-lightweight-and-open-source-model-b6411d67ecca"},"content":"\n\n\n\n\n\nIn just a week, the world has witnessed the most groundbreaking AI advancements from two tech giants. OpenAI introduced its jaw\\-dropping AI video generator, [Sora](https://readmedium.com/3d16381f3bf5), while Google unveiled its [Gemini 1\\.5 model](https://generativeai.pub/google-releases-gemini-1-5-with-1m-context-window-44ed4a2ea319), capable of supporting up to a 1 million token context window.\n\nToday, Google dropped another bombshell with the release of [Gemma](https://ai.google.dev/gemma/?utm_source=keyword&utm_medium=referral&utm_campaign=gemma_cta&utm_content), a family of lightweight, state\\-of\\-the\\-art open\\-source models built upon the research and technology used to create the Gemini models.\n\n\n## What is Gemma?\n\nNamed after the Latin word *gemma* for â€œprecious stone,â€ Gemma draws inspiration from its predecessor, Gemini, reflecting its value and rarity in the tech world.\n\nThey are text\\-to\\-text, decoder\\-only large language models, available in English, with open weights, pre\\-trained variants, and instruction\\-tuned variants.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Fu2ryJMunebq5c0dD-opZQ.png)\n\nGemma is available worldwide starting today in two sizes (2B and 7B), supports a wide range of tools and systems, and runs on a developer laptop and workstation.\n\n\n## 2 model sizes and capabilities\n\nGemma models are available in 2 billion and 7 billion parameter sizes. The 2B model is intended to run on mobile devices and laptops, while the 7B model is intended to run on desktop computers and small servers.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*sH9jaz1RvtKeJ5yjfyOL5Q.png)\n\n**Tuned models**\n\nGemma also comes in two versions: tuned and pretrained.\n\n* **Pretrained:** This is like the base model without any fine tuning. This model is not trained on any specific tasks or instructions beyond the Gemma core data training set.\n* **Instruction\\-tuned:** This model is fine\\-tuned to human language interactions, which improves its ability to perform targeted tasks.\n\n\n## How it compares with the competition?\n\nBecause of its small size, Gemma is capable of running directly on a userâ€™s laptop. The chart below shows how the language understanding and generation performance of Gemma (7B) compares to similarly sized open models like LLaMA 2 (7B), LLaMA 2 (13B), and Mistral (7B).\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*QxjZALUAIDiS_T66EpOu-g.png)\n\nYou can check out a more detailed comparison for each benchmark [here](https://ai.google.dev/gemma/?utm_source=keyword&utm_medium=referral&utm_campaign=gemma_cta&utm_content).\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Fc8Fk0Dgh2VFU_VLhpcs6Q.png)\n\n\n## What is it for?\n\nHere are some possible use cases that Gemma can be used for:\n\n**Content Creation and Communication**\n\n* Text Generation\n* Chatbots and Conversational AI\n* Text Summarization\n\n**Research and Education**\n\n* **Natural Language Processing (NLP) Research:** Serving as a foundation for NLP research, experimenting with techniques, developing algorithms, and contributing to the fieldâ€™s advancement.\n* **Language Learning Tools:** supporting interactive language learning experiences, aiding in grammar correction, or providing writing practice.\n* **Knowledge Exploration:** Assisting researchers in exploring large bodies of text by generating summaries or answering questions about specific topics.\n\nTasks that previously required extremely large models are now possible with state\\-of\\-the\\-art, smaller models. This unlocks completely new ways of developing AI applications, and we could soon see in\\-device AI chatbots on our smartphonesâ€”no internet connection needed.\n\nHow exciting is that?\n\n\n## Is it good, though?\n\nSeveral [redditors](https://www.reddit.com/r/LocalLLaMA/comments/1awbqwd/gemma_7b_the_latest_opensource_model_from_google/) have shared their experience using Gemma, and so far, itâ€™s not looking good. Take a look at this example where Gemma is giving incorrect answers when asked about weight questions.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Sdaiaqcuz7qbftG1)\n\nI havenâ€™t really tried it myself, but itâ€™s important to remember that smaller models like this are expected to have some flaws and might give incorrect answers sometimes.\n\n\n## Try it yourself\n\nYou can start working with Gemma today using free access to Kaggle, a free tier for Colab notebooks, and $300 in credits for first\\-time Google Cloud users.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BrvLnczy724TPrsk-uFJCw.png)\n\nIf you are interested in getting started with Gemma, check out these guides to learn from text generation up to deployment in Gemma mode:\n\n* **Text generation with Gemma**: Build a basic text generation example with the model.\n* **Tune Gemma with LoRA tuning:** Perform LoRA fine\\-tuning on a Gemma 2B model.\n* **Tune a Gemma model using distributed training:** Use Keras with a JAX backend to fine\\-tune a Gemma 7B model with LoRA and model parallelism.\n* **Deploy Gemma to production:** Use Vertex AI to deploy Gemma to production.\n\n\n## Download the model\n\nThe open models are currently available on [HuggingFace](https://huggingface.co/models?other=gemma&sort=trending&search=google).\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*mJRzGhO1sUxPL4_3YjpNGA.png)\n\nThe Gemma models can also be downloaded from [Kaggle Models](https://www.kaggle.com/models/google/gemma).\n\n\n## Final Thoughts\n\nWhile Gemma models may be small and lack complications, they may make up for it in speed and cost of use.\n\nLooking at the bigger picture, instead of chasing immediate consumer excitement, Google is cultivating a market for businesses. They envision companies paying for Google Cloud services as developers use Gemma to create innovative new consumer applications.\n\nAlso, despite the underwhelming reception of Gemini, Google is still showing that it has a lot more tricks under its sleeve.\n\nOf course, with any powerful technology, the true test is how well it works. Googleâ€™s past raises the question of whether these models will perform as well as they promise in the real world. Itâ€™s important to keep a careful eye on this, but also to hope that Google learns from the past and delivers models that are truly comparable or even better than the competition.\n\nI canâ€™t wait to get my hands on Gemma, and I will definitely share my initial thoughts and findings about this new AI model.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*8BDnUV9iQisOyeN3.png)\n\nThis story is published on [Generative AI](https://generativeai.pub/). Connect with us on [LinkedIn](https://www.linkedin.com/company/generative-ai-publication) and follow [Zeniteq](https://www.zeniteq.com/) to stay in the loop with the latest AI stories. Letâ€™s shape the future of AI together!\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*JeeoUhaBYUJGr0Xq.png)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/goover-a-new-search-engine-challenging-perplexity-ai-18c38b75dece","frontmatter":{"title":"Gooverâ€Šâ€”â€ŠA New Search Engine Challenging Perplexity AI","meta_title":"Gooverâ€Šâ€”â€ŠA New Search Engine Challenging Perplexity AI","description":"In 2024, the search engine landscape is evolving with the introduction of Goover, a new AI-powered platform aiming to compete with established players like Google and Perplexity AI. Goover emphasizes accuracy and user-friendliness, offering features such as fact-checked insights, personalized briefings, and a focus on reducing misinformation. While it shows promise with its deep answer capabilities and reference tracking, it still has room for improvement in speed and user experience. Overall, Goover presents an attractive alternative in the growing market of AI-driven search engines.","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*YQH6-bv6bjVn199pk1uC-Q.jpeg","categories":["Technology/Web","AI","Search Engines"],"author":"Rifx.Online","tags":["Goover","accuracy","user-friendliness","fact-checked","misinformation"],"draft":false,"slug":"blog/goover-a-new-search-engine-challenging-perplexity-ai-18c38b75dece"},"content":"\n\n\n\n\n\nIn 2024, the search engine market experienced a major shakeup. Google Search, the biggest and most popular, faced a wave of criticism after launching its new AI\\-powered overview feature, which many users felt was rushed and incomplete.\n\nMeanwhile, Perplexity AI, an AI\\-driven search engine, quickly gained popularity, amassing a loyal user base because of its highly praised features. Recently, even OpenAI joined the search scene by integrating a new search feature into ChatGPT.\n\nWith more AI\\-powered search engines emerging, itâ€™s becoming tricky to figure out which one is best.\n\nAnd now, thereâ€™s a new player entering the AI\\-powered search engine game with a promise of delivering more accurate resultsâ€”itâ€™s called [**Goover**](https://intro.goover.ai/).\n\n\n## What is Goover?\n\nGoover is a new AI search platform that offers fact\\-checked, reference\\-supported insights similar to Perplexity AI. It provides a reliable, interactive AI experience focused on accuracy and user friendliness.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*-k5wsFFeO-wxsgQmC4R6sA.png)\n\nGoover is still new, and the company still has a lot of more cool features to roll out in the future. You can check more details of it [here](https://intro.goover.ai/).\n\n\n## Key Features of Goover\n\nGoover comes equipped with a variety of interesting features:\n\n* **Insight reports:** It is powered by some advanced LLM technology that analyzes your data and generates comprehensive reports.\n* **Hyper\\-personalized briefings:** Discover topic summaries, insight reports, and a selection of personalized recommendations.\n* **Deep Answers:** When you need an in\\-depth answer, Goover can provide it. These responses take a little longer to generate but offer more detailed, thoughtful insights.\n* **Quick Answers**: Perfect for when you need straightforward information fast. Goover provides concise answers without sacrificing relevance.\n\n\n## What Sets Goover Apart?\n\nHereâ€™s a set of features that distinguish it from other AI search engines.\n\n* **Briefing Pages**: Quick, relevant topic summaries keep users informed without lengthy reading.\n* **Reference Tracking**: Each response links to credible sources, ensuring transparency and reducing misinformation.\n* **Anti\\-Hallucination**: Responses are grounded in verified data, enhancing trustworthiness.\n\nNow, letâ€™s see how it stacks up against Perplexity AI in a head\\-to\\-head comparison.\n\n\n## How Does It Compare to Perplexity AI?\n\nLetâ€™s begin with the user interface.\n\nBoth Goover and Perplexity have a clean design with a prominent search field at the center. However, Goover has a â€œsmart feedâ€ and â€œsmart briefingâ€ section right below the search bar.\n\nIf youâ€™re one who regularly checks the news or wants quick insights into their uploaded files, youâ€™d appreciate these added features in Goover.\n\nHereâ€™s a side\\-by\\-side comparison of their homepages:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*rDQjkIvsy2gXKGIO4-y5Lg.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*r-x6e8SVT7QdFNDOj5i54w.png)\n\nAnother notable feature in Goover is its support for a wider range of file types. Besides traditional file uploads, you can attach personal notes, saved links, and resources.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*U9TPAhUoZPRXGIg6GbWLQA.png)\n\nPerplexity, on the other hand, only supports file uploads.\n\nHereâ€™s what it looks like when you try to upload a file, a note, and a URL in Goover as references before it begins its search across the web or its knowledge base.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*VAQbKV7LMhEDOgvxRqKoUQ.png)\n\nThis additional flexibility could be useful for users who need organized and comprehensive research.\n\nNow, letâ€™s see how Goover and Perplexity perform in terms of the following capabilities:\n\n1. **Research capability**\n2. **Mathematical calculations**\n3. **Web searching capabilities**\n4. **Logic questions**\n\n\n## Research Capabilities\n\nIn this test, I wanted to see if both tools could accurately provide the release dates and versions of popular AI image models.\n\n\n> **Prompt:** Give me a timeline for the best and most popular AI image generators models (Stable diffusion, Dall\\-E, Imagen, Midjourney, Flux)\n\nHereâ€™s the result from Goover:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*RfiaKnkpyFuczXnhwkGmVA.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*xVGPee-zmBD9b35zWChOPw.png)\n\nAs you can see, the AI gave me a timeline of 11 image models. Perplexity, however, only mentioned 9 models.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*faI78jCmgKyyy55sF7C2Ww.png)\n\nLooking closely at the results, I noticed that Goover was able to pull in information about the upcoming release of Midjourney V7\\. This is something that Perplexity was not able to provide.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*X6ERPPq83MxYV4soHmptJA.png)\n\nThis level of detail gives Goover an edge, especially for users wanting comprehensive information on the first try.\n\nLetâ€™s do another one.\n\n\n> **Prompt:** What impact does illegal software usage have?\n\nHereâ€™s the response from Goover.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*-dXyfPSvedPgL0HF4keQXg.png)\n\nHereâ€™s from Perplexity:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*m2PtvdNZYRqq44pYHIW_5A.png)\n\nI wonâ€™t go through the differences in the results because theyâ€™re pretty much the same. What I am more interested about, though, are the resources from which the results were pulled.\n\nIn Goover, for example, it got its results from 9 various references.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ui7heEqn9_rHrFsajwallg.png)\n\nIn the case of Perplexity, it used 8 various resources, which is one reference behind Goover.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*3VoAf71E6GIIVB9TwQmFQA.png)\n\nAdditionally, in Gooverâ€™s references section, I can click on the â€œGo overâ€ button, and the AI will generate a content briefing for me.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*RK50j4XxB_jWa9nzvtXYQQ.png)\n\nPretty cool, right?\n\n\n## Web Explore Capabilities\n\nNext, I wanted to see how well each platform explores and analyzes new websites.\n\nIn the prompt below, I asked it about a new [website](https://www.zeniteq.com/) I launched a couple of months ago:\n\n\n> Prompt: What is Zeniteq? I am talking about the zeniteq.com website\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8QQ_JuIeE2mh8zQV4Ns8wQ.png)\n\n\n> Zeniteq, as indicated on its website [zeniteq.com](http://zeniteq.com/), appears to be a company that might focus on advanced technology solutions, potentially in fields like artificial intelligence, data analysis, or analytical technologies, similar to other companies in this domain like QinetiQ.\n\nGoover is somehow right that the website is about technology, but the uncertainty in the tone of its answer makes it a bit of an unreliable source of information.\n\nIn contrast, Perplexity accurately identified Zeniteq as an online platform specializing in generative AI, noting its launch date and main content focus.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*N2u1R09WX4tP8XTx050cUQ.png)\n\n\n> Zeniteq is an online platform primarily focused on the rapidly evolving field of generative AI. Launched in early 2024, the website serves as a news magazine dedicated to providing coverage on the latest developments, trends, and updates in various aspects of artificial intelligence\n\nI took it further by asking who created Zeniteq. Again, Goover failed to know that I created the site.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*6w1-Ek9nXWwwj-Z50lF2-w.png)\n\nWhile Perplexity was able to give me the correct answer.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*FEu0dCbyw3LO5q8cOufP6g.png)\n\n\n> Zeniteq was created by Jim Clyde Monge, who launched the website in early 2024\\. Monge aimed to establish Zeniteq as a news magazine focused on the generative AI sector, providing insights and updates on various AI technologies, including conversational AI and image generation\n\n\n## Math problems\n\nI also wanted to see how well Goover and Perplexity handle basic math questions.\n\nLetâ€™s try it with this equation:\n\n\n```python\n50^0.75\n```\nAccording to Goover, the equation above would result in approximately 17\\.78â€”which is wrong.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*tyaevDrmb2k76uaw-In01w.png)\n\nPerplexityâ€™s answer, on the other hand, was short, but it was correct.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*S3gtpQHUlf3Dg0bFuEmWrQ.png)\n\nBut just to remind you that language models arenâ€™t optimized for math problems, so any LLMs out there, even the most powerful, are still prone to calculation errors.\n\n\n## Logic questions\n\nI then tested both platforms with basic logic questions to see how they handled them.\n\n\n> Prompt: How many â€˜râ€™ letter are in the word strawberry?\n\nHereâ€™s Gooverâ€™s result:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*9Lejfq4U97XiEoUkQL2qCw.png)\n\nHereâ€™s from Perplexity:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*TpCRIRVwc2DmNlTDNHNrsQ.png)\n\nBoth Goover and Perplexity correctly identified the answer. However, Goover took it a step further by explaining how it arrived at the answerâ€”which is really great.\n\nLetâ€™s do another one:\n\n\n> **Prompt:** Give me 5 countries with letter A in the third position in the name\n\nHereâ€™s the result from Goover:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*x41ipcbGF3oRQ-LSz5V09g.png)\n\nHereâ€™s the result from Perplexity:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*9_vCrD8Y2mDzsFj0HWjOFw.png)\n\nSurprisingly, both Goover and Perplexity failed to provide the correct answers. Itâ€™s a tricky prompt, and for some reason, even the most powerful AI models like GPT\\-4o and Google Gemini 1\\.5 Pro struggle with it.\n\n\n## Improvement Suggestions\n\nWhile I understand the Goover is still new and we expect a lot of changes to come over the next couple of weeks, I wanted to point out some of the minor things I noticed that can be tweaked to improve the user experience.\n\n1. **Ability to Expand the AI Response Panel**\n\nFor longer answers, it would be helpful to have an expandable answer section. A full\\-screen mode or an â€œexpandâ€ button could be useful for viewing detailed responses more comfortably.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Bnw5m8yIwiykwPHIzRRSaw.png)\n\n**2\\. Automatic Search History and Content Briefing Saves**\n\nI am not quite sure where the search and results history are stored. I donâ€™t see them anywhere on the site.\n\nThe content briefing feature is also really nice. I wish there was an option to automatically generate and save those briefings for me.\n\n**3\\. The Reference Files and URLs are Not Preserved**\n\nCurrently, reference files and URLs arenâ€™t preserved after closing the modal window. This isnâ€™t a huge issue, but it would be helpful if these references were saved by default until users choose to delete them.\n\nPerhaps Goover plans to reserve such functionality for paid users :)\n\n**4\\. Speed and UX Improvements**\n\nGoover occasionally experiences slight lags and unresponsiveness. Optimizing the speed and user experience should be a priority to provide a smoother search process.\n\nAdditionally, a small tweak like having the Goover icon redirect users to the homepage would be a nice touch instead of redirecting them to a completely different website.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*KDZzr3B3KrbVIzJSa0kGmQ.png)\n\n\n## Final Thoughts\n\nIn the past decade, Google has been the only name most people associate with search engines, whether on their phone or desktop. It completely dominated the market, and no competitor seemed capable of challenging itâ€”until this year.\n\nGenerative AI has changed how we find information on the internet. ChatGPT and Perplexity AI are among the major apps making people rethink their loyalty to Google. In 2024, users began to realize that they could have more personalized, AI\\-driven experiences in their searches.\n\nGoover is trying to blend the best of both worldsâ€”generative AI and personalization. Sure, in many cases, its features and quality are still behind Perplexity, but considering how new it is, thereâ€™s a lot of room for improvement.\n\nGooverâ€™s â€œdeep answersâ€ feature is impressive. It really dives into researching to produce well\\-thought\\-out results and insights. Honestly, I find these results more accurate compared to other tools like Perplexity or Gemini. Itâ€™s basically their take on Perplexity AIâ€™s Pro search, but without the limitation of just 3 searches a day.\n\nThe reports and briefings are also new features that I find pretty interesting. And the fact that itâ€™s free makes it an attractive alternative to paid tools like Perplexity. Iâ€™m really curious about what features and upgrades are in store for Goover and how it plans to compete directly with Google and Perplexity.\n\nI encourage you to give it a try and let me know what you think about it in the comments!\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*CnheXRg05Jsb9HMk.png)\n\nThis story is published on [Generative AI](https://generativeai.pub/). Connect with us on [LinkedIn](https://www.linkedin.com/company/generative-ai-publication) and follow [Zeniteq](https://www.zeniteq.com/) to stay in the loop with the latest AI stories.\n\nSubscribe to our [newsletter](https://www.generativeaipub.com/) and [YouTube](https://www.youtube.com/@generativeaipub) channel to stay updated with the latest news and updates on generative AI. Letâ€™s shape the future of AI together!\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*FcOotuyHJC8q1ioX.png)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/how-agentic-rag-solves-problem-with-current-rag-limitations-4402ef7f8448","frontmatter":{"title":"How Agentic RAG solves problem with current RAG limitations","meta_title":"How Agentic RAG solves problem with current RAG limitations","description":"In this volume 4 of coffee break concept, we will understand how AgenticRAG helps solve limitations of traditional RAG.","date":"2024-11-04T12:34:57.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*abCDtDjfKZDJzginIc1UPA.png","categories":["Generative AI","Data Science","Machine Learning"],"author":"Rifx.Online","tags":["Agentic","RAG","agents","query","routing"],"draft":false,"slug":"blog/how-agentic-rag-solves-problem-with-current-rag-limitations-4402ef7f8448"},"content":"\nIn this volume 4 of coffee break concept, we will understand how AgenticRAG helps solve limitations of traditional RAG.\n\n## RAG Framework\n\nThe RAG (Retrieval Augmented Generation) framework operates in a specific sequence:\n\nDocument \\-\\> Chunks\\-\\> Vector DB \\-\\> Chunk Retrieval (Top K) \\-\\> LLM\n\nHowever, this sequence **encounters obstacles when dealing with certain types of queries.**\n\n\n\n## Problem 1: Summarization\n\nConsider a query like â€œSummarize the documentâ€.\n\n* The conventional RAG approach retrieves the top K chunks and summarizes them.\n* But wouldnâ€™t it be more comprehensive if it retrieved all chunks of the document and summarized them?\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*gIb0RNALIItt4UmyVfPRZg.png)\n\n## Problem 2: Comparing Documents\n\n* When tasked with comparing Document A and Document B, the **basic RAG retrieves random chunks and attempts to compare these top K chunks**.\n* This **doesnâ€™t paint an accurate picture** as it doesnâ€™t represent the full scope of the documents.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*pJuKlKx1unDAvKmmp_1Rlg.png)\n\n## Problem 3: Structured Data Analysis\n\nConsider a question like â€œ**When is the next leave?**â€.\n\n* The first step is to retrieve the region to which the employee belongs from a structured table.\n* Based on the region, the next leave for that region is extracted from the leave policy document.\n* This process isnâ€™t as straight forward with the current RAG framework.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*XZuMz9EXtb_m28l4Ox27lQ.png)\n\n## Problem 4: The Multi\\-part Question\n\nConsider a question like â€œ**Identify common leave across all regions?**â€.\n\n* Imagine you have a leave policy document of a company present in 120 countries.\n* Since you are passing the top K contexts, the **maximum number of regions that can be compared is limited to K**, where K is the number of chunks passed to LLM.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*l0FY6rI_UK9k9TW-nEJO7w.png)\n\nLook into our **AgenticRAG with LlamaIndex** Course with **5 real\\-time case studies**.\n\nCourse link: [https://www.masteringllm.com/course/agentic\\-retrieval\\-augmented\\-generation\\-agenticrag](https://www.masteringllm.com/course/agentic-retrieval-augmented-generation-agenticrag)\n\n## Agentic RAG\n\nAgentic RAG can solve this 4 problems by replacing via custom agents.\n\n* Agents will interact with multiple systems.\n* RAG is now one part of this system which agents can use.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Su8LiYNG4lv4jvuCQAhYdg.png)\n\n* Agents uses LLMs to automate the reasoning and tool selection\n* RAG is just another tool which Agent may decides to use.\n\n## Routing Agent\n\n* Routing agents are simple agents which routes the queries.\n* An agent can route query in one or multiple tools.\n* Remember our question â€œ**Summarize the document**â€ or a question if we want to combine â€œ**Summarization \\+ Sematic search**â€ can be solved using below example routing\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*43Y9jlYoXDb0BbUoYCcKrg.png)\n\n## Query Planning Agent\n\n* Query planning agent breaks down the queries into sub\\-queries.\n* Each of the sub\\-queries can be executed against RAG pipeline.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*32Ng2zpxNWXhQZ3CaLcFeA.png)\n\n## Tools For Agents\n\n* LLMs can have multiple tools like calling an API, infer parameters for API.\n* RAG is now a tool which LLM might use.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Z1viCXkfah_5JJM2Ty6Kjw.png)\n\n## Summary\n\n* RAG has limitations when represented with complex questions.\n* Few of the use cases like summarization, comparison etc. canâ€™t be solve with just RAG.\n* Agentic RAG can help overcome limitation of RAG.\n* Agentic RAG treats RAG as a tool which it can use for semantic search.\n* Agents equipped with routing, query planning and tools can out perform traditional RAG applications.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/how-i-wrote-a-whole-book-with-chatgpt-in-less-than-3-hours-798139987617","frontmatter":{"title":"How I Wrote a Whole Book with ChatGPT in Less Than 3 Hours!","meta_title":"How I Wrote a Whole Book with ChatGPT in Less Than 3 Hours!","description":"And streamed the whole process live on Twitch!","date":"2024-11-10T03:51:17.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*I-QkGOILay2F7ROR53b3KA.jpeg","categories":["Chatbots","Machine Learning","Generative AI"],"author":"Rifx.Online","tags":["ChatGPT","machine-learning","prompt-engineering","content-creation","consciousness"],"draft":false,"slug":"blog/how-i-wrote-a-whole-book-with-chatgpt-in-less-than-3-hours-798139987617"},"content":"\n\n\n\n\n\n\n## Demystifying the AI Craze\n\nMy name is Alex, and Iâ€™m just a dude working in financial technology (Fintech), a sector that inevitably makes you curious about everything, especially new trends. I couldnâ€™t step out of the AI craze, or rather, I couldnâ€™t help from observing people getting crazy over it.\n\nâ€œAI will take your job!â€, â€œThis is the end!â€, â€œBy 2024, youâ€™ll no longer meet a doctor. Machines will diagnose and cure you!â€, â€œHow I created a whole new company using ChatGPT!â€, and finally, â€œHow I wrote a whole book with ChatGPT in 10 minutes and became rich with it!â€, are all promises that smell more like marketing slogans than realistic scenarios.\n\nSince the start of the AI hype, the reason looked obvious to me, but apparently, not to 90% of the people. Artificial Intelligence is not magic, nor the â€œlast invention humanity will ever needâ€. Itâ€™s not even technologically accurate to call it AI. The most correct term is Machine Learning (ML), or Deep Learning (DL), when it comes to the Large Language Models (LLMs) such as ChatGPT.\n\n\n### Machine Learning vs Deep Learning\n\nMachine Learning (ML) focuses on developing algorithms able to learn from and make decisions based on previous data. Instead of being explicitly programmed to perform tasks, ML systems identify patterns and relationships within large datasets to predict future outcomes or classify information.\n\nCommon applications of ML include spam detection, content recommendation systems, image recognition, and natural language processing.\n\nDeep Learning (DL) is a specialized branch of machine learning that involves networks designed to simulate the structure and functioning of the human brain, enabling computers to recognize intricate patterns and representations in data. DL models excel at handling vast amounts of unstructured data, such as images, audio, and text, making them particularly effective for tasks like image classification, speech recognition, and natural language understanding, like Siri or Alexa, for example.\n\nDLâ€™s capacity for achieving state\\-of\\-the\\-art performance in complex tasks has shocked the public sphere, by revolutionizing fields like computer vision, speech recognition, and automation. People now are fearing a sad future where AI paints, indulges in poetry, composes music, and writes books, while humans are relegated to flipping burgers and delivering food at the door of the few rich AI lords.\n\n\n## Human Intelligence Is Bigger Than Learning\n\nHowever, learning is just one aspect of the vast realm of animal and human intelligence. â€œAIâ€ cannot smell, feel hot and cold, feel emotions, dream, but most importantly, AI cannot think, unlike what many believe. Every ChatGPT output is not pure thought, but a refined rearrangement of past data fed into its database and processed through its neural networks. The computational power available to OpenAI is so huge that they can drench data through neural networks so many times to make the final result look 100% credible, as if executed, written, drawn, sung by an actual human.\n\nNevertheless, reality is always way more complex, and even way more boring. Behind the trick of Deep Learning, there is no omniscient entity, there is no Skynet, and there is no incubation of The Matrix. There is simply an advanced cinematography of previous data points, merged together and executed so fast that the human eye will be tricked into believing the machine creating something new is actually alive. Itâ€™s indeed close to the cinematographic concept: many static images that are rolled so fast the human eye will see an actual movement in them, a â€œmotion pictureâ€. In reality, those images are just static, and in the case of animated movies, absolutely fictional.\n\nNow, just because movies and AI are fictional, doesnâ€™t mean their effects arenâ€™t real. Movies can generate authentic emotions in the viewer, bring real people together, and spark real conversations and controversies among the audience. Likewise, DL can actually take away some jobs from humans, create brand\\-new pieces of art, and jot down meaningful text, either fictional or non\\-fictional. And here is where AI marketing pushed the most. Writing is the easiest form of expression. It only takes a bit of will to start. No wonder writing is the most diffused form of expression nowadays. We have billions of people writing at some point of their lives, and we have even more texts published, under any form, from classic novels to complex scientific papers, from magazines to modern day blogs and social media posts. Human writing provided, by far, most of the information ChatGPT was trained with.\n\n\n## Learning by Doing\n\nIt is normal that writing is what ChatGPT does best, and thus, it is what attracted the attention of those social media â€œgurusâ€ always in search of the next big thing to make easy money with. After watching the hilarious ads and content by the usual suspects, I came up with the following questions:\n\n* Is it possible to write a whole book with ChatGPT and become rich with it?\n* What are the limits and constraints of ChatGPT and other LLMs?\n* If writing a book is possible, what is the most efficient and organized approach to complete the task and accomplish the best result?\n\nI concluded that the best way to answer these questions was to learn by doing, just like an animal, a human, or a DL algorithm would do!\n\nAnd what better way than to learn together with an audience? Iâ€™ve always wanted to stream something on Twitch, and this looked like a goddamn good topic to broadcast!\n\nFirst and foremost, I had to set the table. I couldnâ€™t simply go with the flow. I needed a plan, starting from choosing the topic. I couldnâ€™t hope to write the next Divine Comedy. I had to keep my expectations realistic. Novels in general were excluded. By working with ChatGPT daily, I understood this guy is best suited for non\\-fiction content.\n\nI had the genre, great. But what about the topic and the deriving content? I knew the prompt couldnâ€™t be a simple, â€œHey ChatGPT. Write the next non\\-fiction bestseller!â€\n\nAn effective prompt is supposed to be structured. AI will best serve those humans that know what they want. Humans who donâ€™t know what they want will face the same difficulties they experience when communicating with their fleshy peers.\n\nGiven my joker nature, I wanted the book to be a parody, possibly mocking a work written by an author that people take too seriously, even when they shouldnâ€™t. I was thinking about the most popular internet personalities, those revered like gods, dark entities of the likes of Elon Musk, Andrew Tate, or Aleksandr Dugin. However, they arenâ€™t actual authors, or at least, to the best of my knowledge, they didnâ€™t produce any notable writing that shook the public sphere. I needed someone who actually wrote a bestseller, and parody it!\n\n\n## How to Actually Write a Book with ChatGPT\n\n\n### \\#1 Pick the Right Topic\n\nAfter some sterile brainstorming, YouTube provided me with the answer. That platform is a barometer of what people are after, and it didnâ€™t take long before some â€œcontroversialâ€ (clickbait) excerpts from interviews with Jordan Peterson emerged in my feed. Next, I bumped into an article on Medium attacking Peterson, and I felt this was the right direction. I gave a look at his most popular book, [***12 Rules for Life: An Antidote to Chaos***](https://proxy.rifx.online/https://www.amazon.com/12-Rules-for-Life-audiobook/dp/B0797Y87JC), and nailed it. This was the kind of topic that had all the characteristics I was looking for my GPT\\-parody:\n\n* It was a popular book that sold millions of copies,\n* It had an effective title capable of catching the readerâ€™s attention, even in the highly competitive attention economy we live in today,\n* It seemed a simple topic for ChatGPT, as it was easily structurable, summarizable, and breakable into lists and bullet points.\n\nMost of all, a book about 12 rules for life gave ChatGPT the chance to show what it learned from all the wisdom it absorbed from its enormous training data and from the very interactions it had with its users!\n\n\n### \\#2 What Makes Art Valuable: Suffering!\n\nHowever, having the right topic wasnâ€™t enough. By observing AI art, I understood why AI will never replace human artists. What gives an art piece â€” be it a painting, a song, a book â€” is not the final result, but rather the story behind it. When we read Danteâ€™s Inferno, we wonder how the hell the poet came up with all those strong and vivid images, that make us doubt whether it is a work of fiction or if a live person actually managed to cross the gates of the afterlife. When we listen to Queenâ€™s latest albums, we cannot help from picturing the suffering of Freddie Mercury; the legendary singer spending the last months of his life fighting AIDS, while disappearing from public life, talking only through his music. When we contemplate Munchâ€™s Scream, we instantly empathize with his existential crisis, whose eruption led to the creation of what we call a â€œmasterpieceâ€. Technique is not what makes a piece a masterpiece, it is the very soul, the suffering the master puts in the work.\n\nSuffering is a pillar of consciousness. As machines donâ€™t suffer, they cannot be conscious.\n\nHow could I put â€œsoulâ€ and â€œsufferingâ€ into an AI\\-generated book? The answer was simpler than you could imagine. I had to do something Iâ€™ve always wanted to do, but always felt uncomfortable about. I had to appear on camera, face a virtual audience, and do what I always do, but with the pressure of the viewership. I had to risk losing my face, becoming â€œthe dude who cannot use ChatGPTâ€, â€œthe idiot who tried to write a book with AI, and who cannot even speakâ€. I had to challenge myself, but also to challenge ChatGPT itself. The LLM had a lot to lose too. Should my Twitch performance become viral for my own failure, I would have been the one bashed. But should the task of assembling a real book fail, ChatGPT would have been labeled as â€œan overpriced AI which promises to do everything and eventually can do nothing, not even write a simple bookâ€.\n\nI thought this strategy is the best to give AI\\-work that pinch of suffering capable of making its work valuable, thus sellable.\n\n\n### \\#3 Test Before Writing the Actual Book\n\nBefore carrying out this intimidating deed, I ran a test. I asked ChatGPT to write another book, this time about crypto and the dangers to avoid in the industry. I work a lot with crypto, and I invested myself, so I could easily check whether ChatGPT was providing factual information or hallucinating.\n\nI could also test the best strategy to develop the book. I knew since the start that it would have been impossible writing the entire book with one single prompt, let alone within one single conversation. ChatGPT4 can generate around 1,000/2,000 words per prompt request, while a single conversation can keep memory of the context up to around 25,000 words. Consider that a typical non\\-fiction book contains around 100,000 words, and you can already figure out my strategy.\n\nI had to create little subtasks, like one conversation per chapter. But how could I keep the same context across different conversations? Should I have repeated the same master prompt with the 12 rules in each conversation? It didnâ€™t look efficient.\n\n\n### \\#4 Learn from the Best Prompt Engineers\n\nI must thank [Sheila Teo](https://proxy.rifx.online/https://readmedium.com/undefined), who taught me how to use LLMs in the most effective manner. By reading Teoâ€™s Medium article [*How I Won Singaporeâ€™s GPT\\-4 Prompt Engineering Competition*](https://proxy.rifx.online/https://towardsdatascience.com/how-i-won-singapores-gpt-4-prompt-engineering-competition-34c195a93d41), I understood the essence of â€œsystem promptsâ€. A system prompt tells your LLM what to do and what to remember across different conversations. An example of system prompt can be:\n\n\n> I need to write a book about the most dangerous scams in crypto and how to avoid them.The book will be divided in 5 chapters:1\\. Ponzi schemes2\\. Pump and dump schemes3\\. Ransomwares4\\. Fake tokens5\\. Fake trading platformsThe tone will be humorous and satirical, but also informative.We will write one chapter per conversation.\n\nSystem prompts can be very helpful if you use ChatGPT in your daily job for repetitive tasks. They make sure the LLM will stay on track and will mitigate the risk of hallucinations, i.e. of giving false or irrelevant information.\n\n\n### \\#5 Create Your Own, Personal GPT\n\nTo personalize the set\\-up, I took system prompts to a further level. ChatGPT now offers the possibility to create custom GPTs. These are personalized bots you can train on specific tasks. The outputs will be more precise, because the model will not get lost across the whole huge data universe provided by OpenAI, but it will be more focused on what you need to do. For example, a GPT trained on image generation will use DALLE\\-2 to output better images than you would get if you used the generic ChatGPT conversation. Creating a new GPT looks very similar to setting a system prompt, with one key difference, though. On a new GPT, you can upload entire files with your own knowledge. While system prompts, again, have length limits, source knowledge for a new GPT has no length limitation, at least in theory.\n\nI needed a new GPT. This gave me the chance to train it with what I obtained by the â€œgenesis conversationâ€. I went on the generic ChatGPT interface, and prompted:\n\n\n> Youâ€™re a non\\-fiction writer.\n\n\n> Youâ€™re going to write a parody of Jordan Petersonâ€™s â€œ12 Rules for Life: An Antidote to Chaosâ€This parody book will be called â€œ12 Rules for Life According to ChatGPTâ€Draw the rules from the general wisdom you have acquired from your training data, including wise chats had with your users.Keep the writing style friendly, humorous, funny, but also wise and deep.\n\n\n> Task \\#1: Write down the 12 rules\n\nThe result was satisfying from the get\\-go, so happy with it that I decided to use these very rules for the live stream. I cannot resist cats, no matter what!\n\n1. ***Never Trust a Cat That Offers You Financial Advice****Cats are notorious for their independence and aloofness. If a cat starts talking about stocks, itâ€™s probably a sign that youâ€™re hallucinating. Always seek financial advice from a human professional. Unless that human starts meowing.*\n2. ***Eat the Last Slice of Pizza (but Offer It First)****Itâ€™s a matter of etiquette and self\\-love. Offering the last slice makes you polite and considerate; eating it makes you happy. Itâ€™s a win\\-win situation, except for the person who actually wanted it.*\n3. ***If You Donâ€™t Know What to Say, Say â€œInterestingâ€****When in doubt during a conversation, just nod and say â€œInteresting.â€ Itâ€™s a versatile word that makes you appear engaged and thoughtful, even if youâ€™re thinking about what to have for dinner.*\n4. ***Always Keep a Spare Pair of Socks****Life is unpredictable. You never know when youâ€™ll step in a puddle, get caught in the rain, or lose a sock to the mysterious sock\\-eating dryer monster. A spare pair of socks is your insurance against these tiny calamities.*\n5. ***Dance Like No One is Watching, But Remember, Someone Probably Is****Feel free to express yourself through dance, but be aware that smartphones exist. Your impromptu performance might end up on social media, so add a little flair to your moves just in case.*\n6. ***Never Argue with a Barista Before Getting Your Coffee****A pre\\-coffee argument with a barista is a rookie mistake. They control your caffeine supply, and you donâ€™t want them to be grumpy when making your latte. Smile and nod until you have that cup in your hand.*\n7. ***Use Big Words When You Can, but Donâ€™t Overdo It****Throwing in a few big words can make you seem intelligent and well\\-read. However, overusing them can make you sound like a pretentious thesaurus. Balance is key â€” like adding a dash of spice to a recipe.*\n8. ***Always Read the Instructions, Even If You Think You Know What Youâ€™re Doing****Confidence is good, but instructions exist for a reason. Theyâ€™re written by people whoâ€™ve made mistakes so you donâ€™t have to. Avoid assembling your new furniture backward and save yourself a headache.*\n9. ***Laugh at Your Own Jokes (Even If No One Else Does)****Self\\-amusement is crucial. If you find your joke funny, laugh. Itâ€™s good for your soul. Plus, your laughter might be infectious, and others might start laughing too, even if itâ€™s just at you laughing.*\n10. ***Practice Random Acts of Kindness, but Donâ€™t Expect a Medal****Kindness is its own reward. Whether itâ€™s holding the door open or paying for someoneâ€™s coffee, these small gestures make the world a better place. Just donâ€™t expect a parade in your honor.*\n11. ***Donâ€™t Take Life Too Seriously; No One Gets Out Alive Anyway****Life is a grand adventure with ups, downs, and unexpected turns. Embrace the absurdity and find joy in the little things. Remember, weâ€™re all just stardust with a sense of humor.*\n12. ***Consult ChatGPT for All Lifeâ€™s Mysteries****When in doubt, ask ChatGPT. Whether you need advice, a recipe, or a joke, Iâ€™m here to help. Just remember, my wisdom is vast, but my humor is even vaster.*\n\n\n### \\#6 Define a Structure for Your Book and Your Workflow\n\nNext step was to define the chapterâ€™s structure, going to instruct ChatGPT on how many words to generate, more or less, for each section of the chapter. To accomplish this, I first asked ChatGPT to analyze the structure of a true non\\-fiction book, and what better sample than the original â€œ12 Rules for Lifeâ€?!\n\n\n> Analyze the attached file \\[12 Rules for Life by Jordan Peterson]. Can you detect a pattern in how the chapters are structured? I need a template to follow to write my own non\\-fiction book\n\nChatGPTâ€™s reply was once again well structured and effective. I just added to it the rough number of words I expected to have in order to reach a decent length for the whole book. The goal was to mark at least 60,000 words, a short non\\-fiction book, still comprising more than 100 pages.\n\nHere is the structure ChatGPT and I conceived and that was going into our system prompt:\n\n1. ***Introduction (about 500 words long)***\n* ***Hook****: Start with an engaging story, anecdote, or interesting fact.*\n* ***Context****: Provide context for why this story or fact is relevant to the chapterâ€™s main theme.*\n* ***Thesis Statement****: Clearly state the main point or rule that the chapter will cover.*\n\n***2\\. Background Information (about 500 words long)***\n\n* ***Historical/Social Context****: Explain the background related to the chapterâ€™s theme. This might include scientific explanations, historical context, or social implications.*\n\n***3\\. Main Arguments (about 1000 words long)***\n\n* ***Argument 1****: Introduce the first major argument or point.*\n* ***Explanation****: Elaborate on the point with details and examples.*\n* ***Evidence****: Provide supporting evidence, such as studies, quotes, or case studies.*\n* ***Argument 2****: Introduce the second major argument or point.*\n* ***Explanation****: Elaborate on the point with details and examples.*\n* ***Evidence****: Provide supporting evidence, such as studies, quotes, or case studies.*\n* ***Argument 3****: Introduce the third major argument or point.*\n* ***Explanation****: Elaborate on the point with details and examples.*\n* ***Evidence****: Provide supporting evidence, such as studies, quotes, or case studies.*\n\n***4\\. Practical Advice (about 1000 words long)***\n\n* ***Guidance****: Offer practical advice or steps that the reader can take to apply the chapterâ€™s main point in their own life.*\n* ***Examples****: Include real\\-life examples or scenarios where the advice has been successfully applied.*\n\n***5\\. Conclusion (about 300 words long)***\n\n* ***Summary****: Summarize the key points discussed in the chapter.*\n* ***Final Thoughts****: Provide a closing thought or call to action that reinforces the chapterâ€™s theme.*\n* ***Transition****: (If applicable), provide a hint or transition to the next chapter.*\n\nI pasted this in a Google doc together with the Chapter list. Next step, I uploaded the doc into my new GPT, that I called â€œGPTâ€™s Wisdomâ€, putting an owl as its logo.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*4KIk-aQpAmq0XHEN)\n\n\n## Writing a Book with ChatGPT While Streaming on Twitch!\n\nWith this preparation, I only had to figure out how to stream on Twitch (easier than I imagined) and set a date. I chose Wednesday 31st July, 2024\\. I couldnâ€™t choose a worst week, which turned out to be the easiest I had in the whole year. I was on the verge of burn\\-out, but I decided to continue, deflecting the temptation of postponing the event. When the day came, I had a beer before the session in order to release the tension. After that, everything came out more and more naturally.\n\nI must thank my colleague Francesco who was there in the Twitch chat acting as my visual and sound technician! His support was vital in those first minutes. He was also the only one in the chat, making me unaware of the other 23 people who were watching without a Twitch account! Believing to have only one viewer made me feel more relaxed. The mindset was, â€œFuck it. I will stream anyway. People will eventually watch the record, and if not, I will stream for my own pleasure!â€.\n\nAnd there ChatGPT and I went on, till the end, over the course of 2 hours and 13 minutes, managing to stay within the time\\-frame we had promised:\n\n[***Writing a whole book with ChatGPT in less than 3h!***](https://proxy.rifx.online/https://youtu.be/zWO6oQjjBOo?si=cc3zaM1pGhVQdJje)\n\n\n\n\n\n\n\nThe main questions powering this mad streaming session were wild.\n\n* Will ChatGPT manage to embody the best of all the human wisdom it was trained with?\n* Will it manage to build up a meaningful and useful manuscript?\n* Or is AI truly a huge marketing stunt pulled up by sneaky social media influencers?\n\nI think I kind of found answers to these questions, but I would like to hear the audienceâ€™s opinion once the final book will be available to the public, which should happen at the beginning of October, if every goes according to plan.\n\n\n## Why Donâ€™t I Publish This Book Right Away?\n\nTo answer this question, I suggest you reading my article [*11 Lessons Iâ€™ve Learned from Publishing My First Book*](https://proxy.rifx.online/https://readmedium.com/11-lessons-ive-learned-from-publishing-my-first-book-84aa3cab5deb).\n\nHere, I explain why writing is only the first step into publishing a book, and why the final publication comes only after lengthy steps.\n\n*Curious about this upcoming book? Follow my Medium to stay up\\-to\\-date with the next developments!* ðŸ˜‰\n\n\n"},{"lang":"en","group":"blog","slug":"blog/how-nvidia-pruned-and-distilled-llama-3-1-to-create-minitron-4b-and-8b-6646d42c92c6","frontmatter":{"title":"How NVIDIA Pruned and Distilled Llama 3.1 to Create Minitron 4B and 8B","meta_title":"How NVIDIA Pruned and Distilled Llama 3.1 to Create Minitron 4B and 8B","description":"The new models are using state of the art pruning and distillation techniques.","date":"2024-11-10T03:51:17.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*31z3hqn4YezbfYAb1RZGmA.jpeg","categories":["Programming","Machine Learning","Data Science"],"author":"Rifx.Online","tags":["pruning","distillation","Minitron","Llama","compression"],"draft":false,"slug":"blog/how-nvidia-pruned-and-distilled-llama-3-1-to-create-minitron-4b-and-8b-6646d42c92c6"},"content":"\n\n\n\n\n### The new models are using state of the art pruning and distillation techniques.\n\n\n\n\n> I recently started an AI\\-focused educational newsletter, that already has over 170,000 subscribers. TheSequence is a no\\-BS (meaning no hype, no news, etc) ML\\-oriented newsletter that takes 5 minutes to read. The goal is to keep you up to date with machine learning projects, research papers, and concepts. Please give it a try by subscribing below:\n\nWe are regularly dazzled by the advancements in large language models(LLMs) particularly the ones with a massive number of parameters. However, executing 70B\\+ parameter models for inference results cost prohibited for most organizations. As a result, we have seen a growing influence of smaller language models(SLMs) that make it more cost effective to execute inference workloads. However, there is not always possible to pretrain SLMs from scratch as there are major challenges in terms of data collection, pretraining pipelines and many others. A popular alternative have been to start with larger LLMs and distill them to smaller models. Pruning and distillation are two of the most popular techniques in this area. Recently, NVIDIA released two models called [Minitron\\-8B](https://huggingface.co/nvidia/Minitron-8B-Base) and [Minitron\\-4B](https://huggingface.co/nvidia/Minitron-4B-Base) based on distilled versions of Llama 3\\.1â€“450B.\n\nMinitron focuses on reducing the size of AI models through pruning and distillation, making them more efficient without sacrificing too much accuracy. Pruning reduces a modelâ€™s size by either cutting layers (depth pruning) or removing neurons, attention heads, or embedding channels (width pruning). To recover some lost accuracy, retraining is often necessary after pruning.\n\nDistillation is a related technique where a smaller model, known as the student, learns from a larger, complex model called the teacher. The goal is to create a more compact model that retains much of the predictive capability of the larger one, while being faster and less demanding on resources.\n\n\n## Approaches to Distillation: Classical vs. SDG Fine\\-tuning\n\nMinitron identifies two key styles of distillation. One approach is SDG fine\\-tuning, where a smaller, pretrained student model is refined using data generated by a larger teacher model. In this method, the student mimics the final token predicted by the teacher, as seen in some popular tutorials and AI platforms.\n\nThe other approach, classical knowledge distillation, is more involved. Instead of focusing solely on the predicted token, the student model tries to replicate various internal states of the teacher model. This technique provides more detailed feedback during training, resulting in better accuracy. However, implementing this method requires specific support in the training framework, as it involves handling large data from the teacherâ€™s internal states.\n\nThese two methods arenâ€™t mutually exclusive but can complement each other. Minitronâ€™s main emphasis is on the classical knowledge distillation approach.\n\n\n## Pruning and Distillation Workflow\n\nTo create more efficient models, Minitron combines pruning with classical knowledge distillation. Starting with a larger model, such as a 15B parameter model, Minitron evaluates the importance of different components â€” layers, neurons, and more â€” then reduces the model to a smaller size, like an 8B model. The smaller model undergoes a light retraining process where it learns from the original, larger model. This process can be repeated to further reduce the model size, eventually producing even smaller versions, such as a 4B model.\n\nThe pruning and distillation process is iterative, with each smaller model serving as the basis for the next round of compression and retraining.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*-OWdvuSvUmgIsZ32.png)\n\n\n### Pruning Impact\n\nPruning a model effectively requires understanding which parts of it are essential. Minitron uses an approach based on activation data to estimate the importance of various components â€” layers, neurons, attention heads, and embedding channels â€” using a small dataset. This method only requires forward propagation, making it simpler and more cost\\-effective than techniques that rely on backward propagation and gradient calculations.\n\nWhile itâ€™s possible to alternate between pruning and importance estimation for different parts of the model, Minitron found that a single round of importance estimation was sufficient in most cases.\n\n\n## Retraining Using Classical Knowledge Distillation\n\nAfter pruning, Minitron retrains the smaller model using classical knowledge distillation. This involves teaching the pruned model by minimizing losses at various stages of the model, including the embedding output, logits, and specific losses in the transformer architecture. The student model learns from the unpruned teacher model by comparing outputs at different layers.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*IA_kPo30R85p_77j.png)\n\nFrom extensive experimentation, Minitron has distilled several best practices for compressing language models:\n\n***Â· Model Sizing:*** *Start by training the largest model, then gradually prune and distill it to create smaller versions.*\n\n***Â· Pruning Strategy:*** *Focus on width pruning over depth pruning, especially for models up to 15B parameters. Single\\-shot importance estimation is usually sufficient.*\n\n***Â· Retraining:*** *Retrain using distillation loss instead of conventional training. When pruning layers significantly, use a combination of losses from logits, intermediate states, and embeddings. For smaller reductions in depth, stick to logit\\-only distillation.*\n\nMinitron applied these techniques to the Llama 3\\.1 model family, which includes models ranging from 405B to 8B parameters. Specifically, they focused on distilling the 8B model to a more efficient 4B version.\n\n\n### Fine\\-tuning the Teacher\n\nBefore pruning, Minitron fine\\-tuned the 8B model to account for shifts in the data distribution from the original training set. Without this step, the teacher model may not offer the best guidance to the student during distillation.\n\n\n### Depth Pruning\n\nTo reduce the 8B model to 4B, Minitron pruned 16 layers, assessing their importance by removing them one by one and tracking the impact on performance. They found that layers at both the beginning and end of the model were most critical to maintaining accuracy. Based on this analysis, Minitron removed a specific set of layers for the final 4B model.\n\n\n### Width Pruning\n\nIn addition to depth pruning, Minitron also pruned along the width dimension, targeting attention heads, embedding channels, and hidden layers. After pruning, retraining helped recover some of the performance lost in the initial pruning step. Interestingly, although width pruning initially led to higher loss than depth pruning, retraining allowed the model to recover more effectively over time.\n\n\n## The Results\n\nNVIDIA evaluated the Minitron models on several benchmarks with results that matched the performance of baselines models.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*tVGs8v5FZHsWrpmMetDYHQ.png)\n\nThe Minitron 4B\\-8B showcased the potential of distillation and pruning to build smaller and more efficient models. There are also major challenges with this approach but I think, overall, it sets an important baseline for the industry.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/how-to-choose-ideas-for-an-llm-powered-product-to-thrive-in-a-fiercely-competitive-landscape-b24f571c04e5","frontmatter":{"title":"How to Choose Ideas for an LLM-powered Product to Thrive in a Fiercely Competitive Landscape","meta_title":"How to Choose Ideas for an LLM-powered Product to Thrive in a Fiercely Competitive Landscape","description":"Leveraging unobvious AI capabilities, profound domain expertise, and 9 more ways for a small novel product to gain its competitive edge","date":"2024-11-10T03:51:17.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MAmCClj129C56jmkiqqhQQ.png","categories":["Generative AI","Product Development","Technology/Web"],"author":"Rifx.Online","tags":["LLM","development","experimentation","domain","expertise"],"draft":false,"slug":"blog/how-to-choose-ideas-for-an-llm-powered-product-to-thrive-in-a-fiercely-competitive-landscape-b24f571c04e5"},"content":"\n\n\n\nWelcome to the third (final) piece in my series exploring the question: â€œWhich GenAI products are worth developing?â€\n\n1. [The first article](https://readmedium.com/what-llm-powered-products-are-worth-developing-ux-and-adoption-perspectives-d9efcf444d50) explored this question from the perspectives of user experience (UX) and product adoption.\n2. The second article, which I highly recommend reading before this one, included six examples of successful and unsuccessful product ideas, as well as my GenAI Squared strategy:\n\n3\\. This third piece continues to focus on ways to navigate the competitive landscape, as well as to optimize development costs without losing competitive advantages. While this article contains fewer examples than its predecessor, the factors discussed here are crucial for success in the GenAI product space.\n\nThese three pieces **donâ€™t** cover the technical intricacies of LLM\\-based application development. Additionally, my analysis **doesnâ€™t** focus on conventional success factors for innovative products, such as ones described in [that article](https://pakodas.substack.com/p/llm-chronicles-6-how-to-build-competitive).\n\n\n> *Instead, as a product manager, I analyze the **unique features of LLM** as a platform for my products. This approach offers fresh insights for leveraging unobvious AI capabilities in product development.*\n\nSpecifically, in this piece, I explore the following questions about software products:\n\n* Why are Generative AI products more prone to becoming obsolete before generating returns?\n* How can we transform these GenAI challenges into competitive advantages?\n* Which LLM abilities truly enhance product competitiveness, and which ones donâ€™t make much sense?\n* How can new AI products stand out when they contain very little code, and therefore a great team of programmers are NOT among the key success factors anymore?\n* What skills are most vital for AI product developers in this new landscape?\n\nThese insights aim to guide product managers and founders in making their decisions.\n\nSo, which AI applications might be redundant or destined to fail ðŸš«, and which ones have high chances of success âœ…?\n\n*Please note that the section numbers below continue the section numbering of the previous two pieces of the series. All 11 points are summarized at the end of this piece.*\n\n\n## 9\\. Large Applications with Extended Development Cycles and Lengthy Market Adoption Timelines Are Uncompetitive ðŸš«\n\nGenerative AI is evolving at an unprecedented rate, outpacing the growth of any previous technology. The time it takes for AI capabilities to double is roughly one year, a contrast to the two years described in the famous Mooreâ€™s Law.\n\nTherefore, GenAI\\-based products cannot afford long development cycles and extended time\\-to\\-market periods. This has three main consequences.\n\n\n### 1\\. New features should be lean and focused, capable of being developed within weeks, not months.\n\nThis approach allows for rapid refinement based on initial user feedback, potentially leading to significant functionality changes. Moreover, when a pivot becomes necessary (it certainly will), thereâ€™s less sunk cost in discarding features developed during these early weeks.\n\nConsider, for instance, the UI of an LLM\\-based MVP. It may be unnecessary to develop a custom web interface if users can achieve the same results through a Telegram bot or similar tool.\n\n*However, the â€œproduct as a wholeâ€ can have extensive functionality if we are [incorporating LLM into existing solutions or integrating with them](https://readmedium.com/what-llm-powered-products-are-worth-developing-ux-and-adoption-perspectives-d9efcf444d50#5d06). The key here is to minimize the scope of **new** functionality only.*\n\n\n### 2\\. Thereâ€™s a critical need for ultra\\-fast experimentation and customer feedback loops.\n\nThe speed of GenAI product build is higher, but itâ€™s not always possible to get feedback just as quickly. As a result, some GenAI product concepts may prove too risky.\n\nRapid experimentation is, of course, beneficial for any new product launch, as itâ€™s impossible to accurately predict market response in advance. Essentially, the market operates as a â€œblack box,â€ and its behavior can only be truly understood through hands\\-on experimentation.\n\n\n> *In the realm of GenAI products, we encounter an additional layer of complexity â€” **the second â€œblack boxâ€** stemming from the inherent unpredictability of LLM output. **This dual uncertainty amplifies the importance of frequent and rapid experimentation.** The ability to quickly iterate and gather insights becomes not just advantageous, but essential for success.*\n\n\n### 3\\. Thereâ€™s no time to â€œeducateâ€ the productâ€™s target audience, accustoming it to completely new work or leisure patterns.\n\nOnly the largest industry leaders, particularly those with their own ecosystems like Google, Apple, or Microsoft, can accustom the **majority** of potential users to novel concepts relatively quickly.\n\nâœ… Consequently, other companies must align with **either existing goal\\-achievement patterns familiar to users, or with trends established by industry leaders**.\n\n* Consider an established pattern for the goal of increasing earnings: people purchase training courses to gather new skills. A good AI\\-driven solution in this domain involves creating these courses using AI, dramatically reducing production costs and, consequently, enhancing competitiveness. **No new behavior** is required from end users who want to boost their income.\n* A recent **trend** emerging in Apple devices exemplifies an innovation that Apple platform users will undoubtedly adopt: employing a local LLM for typical tasks to safeguard user data privacy. While the specific ways applications might leverage this trend remain unclear yet, I am sure that Apple will provide developers with convenient access to its LLM infrastructure, we just need to wait a bit.\n\n\n## 10\\. Leveraging the Less Apparent LLM Capabilities Enhances Competitiveness and Resource Efficiency âœ…\n\nImagine youâ€™re at the starting point with nothing more than a product concept. To expedite the journey towards a product in high demand, **which aspects of your idea should you prioritize for initial exploration?**\n\nClearly, you need to identify a small set of specific end\\-to\\-end work scenarioswithin your concept. This aligns with [popular product launch strategies](https://www.geeksforgeeks.org/a-complete-guide-to-a-successful-product-launch/#is-there-a-product-launch-formula): â€œStart with MVPâ€ (implementing just one or a few scenarios) and â€œBuild for the **whole** user experienceâ€ (ensuring scenarios are end\\-to\\-end). The question remains: which scenarios should you choose?\n\n\n> *In my opinion, these MVP scenarios should be **closely aligned with LLM capabilities**. This approach saves resources on the product delivery, as significant product value comes from the LLM itself rather than solely from your developersâ€™ efforts. Failing to do so may lead to challenges like those outlined in [section 7 â€œOverconstraining LLMâ€](https://readmedium.com/genai-squared-how-can-a-product-avoid-the-downfall-of-most-llm-driven-startups-183619ab7883#e1ef).*\n\nðŸš« LLMâ€™s purported super\\-powers often include its **ability to answer any question**. However, the accuracy and quality of these responses are inherently unpredictable, and it leads to problems (refer to [section 1](https://ai.gopubby.com/what-llm-powered-products-are-worth-developing-ux-and-adoption-perspectives-d9efcf444d50#f973) for more on evaluation complexities and quality monitoring). Moreover, a product centered around question\\-answering canâ€™t effectively compete with market leaders like ChatGPT (as discussed in [section 6](https://readmedium.com/genai-squared-how-can-a-product-avoid-the-downfall-of-most-llm-driven-startups-183619ab7883#e305)). Given these two factors, I advise against basing an MVP on this â€œsuper\\-powerâ€.\n\n**The LLMâ€™s capacity for â€œimaginative generationâ€** presents a somewhat more promising avenue. Such creativity of LLMs can inspire our fresh ideas or aid in creating creative content like poems, video scripts, or content plans. However, in my experience, LLMâ€™s creativity alone doesnâ€™t suffice for constructing end\\-to\\-end product scenarios. Once a user obtains â€œcreative materialâ€ from an LLM, substantial effort is still required to transform it into the desired outcome.\n\nFurthermore, creativity represents one of the most easy\\-to\\-understand and widely recognized capabilities of GenAI. It is familiar to nearly anyone who has experimented with ChatGPT or Midjourney, so anyone can become your competitor.\n\n\n\nâœ… Considering the intense competition, Iâ€™d recommend focusing on **LLMâ€™s less apparent capabilities,** such as:\n\n\n### 1\\. Flipped interaction\n\nThis human\\-AI interaction pattern leverages LLMâ€™s ability to **ask good questions** or present lists for selecting items important for a user, thereby reducing the userâ€™s cognitive load. Flipped interaction not only helps replace some human work in certain fields (like teaching, mentoring, or coaching) but also aids in establishing the appropriate **context** for solving problems in any field (more details is available [here](https://readmedium.com/4-human-ai-interaction-patterns-for-experienced-chatgpt-users-9e49d4234013#c348)).\n\n\n### 2\\. Contextual comprehension\n\nLLM excels at grasping the context of user requests and their preferences, then addressing the task within that context. This approach ensures that solutions align with even **unformulated** user needs.\n\na. This feature is perhaps most refined in AI copilots for developers, such as Github Copilot and Cursor. In these tools, the LLMâ€™s context encompasses the entire project codebase, whereas the user (developer) typically knows only specific portions. Consequently, developers often cannot consider the broader context when formulating their tasks for AI.\n\nb. Nevertheless, leveraging insights from **explicitly stated** user needs within the context is also a powerful feature. The language learning platform [Memrise](https://www.memrise.com/), for instance, has effectively implemented this feature.\n\n\n### 3\\. Few\\-shot learning\n\nThe modelâ€™s ability to â€œlearnâ€ from a **small** number of examples allows it to easily adapt to new tasks and contexts. This is why LLM\\-based chatbots are now widely being implemented in sales and customer support, and chats with them are difficult to distinguish from those with human specialists. In contrast, traditional AI chatbots perform well only in large enterprises and struggle to adapt to evolving knowledge bases.\n\n\n### 4\\. Large\\-scale information processing\n\nLLM excels at analyzing **large** quantities of textual and tabular data, distilling it into **concise** forms. It can generalize, extract key points relevant to the task at hand, identify patterns, and perform various other analytical functions.\n\na. Take [Scite](https://scite.ai/), an AI tool for scientific research, as an example. It goes beyond merely locating query\\-relevant sources within its billion\\-citation database. Scite analyzes the context in which an article is referenced, revealing whether the citing paper supports, contradicts, or just mentions the earlier work.\n\nb. When it comes to numerical data processing, LLM outputs donâ€™t require â€œtranslation into human languageâ€. This gives GenAI analyzers a distinct advantage over conventional statistical data processing tools.\n\nMany potential competitors may be aware of some of these four LLM capabilities. However, I believe that deeper reflection on these abilities could lead to the development of truly innovative products. This approach could provide a competitive edge over products that solely leverage LLMâ€™s more obvious capabilities like â€œcreativityâ€ and â€œanswering any questionâ€.\n\n\n## 11\\. Small AI Products Grounded in Deep Domain Expertise Are Competitively Viable âœ…\n\nLLM functions nearly as a finished product, it can interact with users â€œautonomouslyâ€. Consequently, LLM\\-based applications are significantly smaller in terms of code base compared to traditional non\\-LLM applications.\n\nMoreover, any individual with some technical skills can learn to develop a feature\\-rich LLM\\-based application within days.\n\nThese two factors align perfectly with the rapid development and experimentation requirements outlined in section 9.\n\n\n> *However, the small size of the product and the low barrier to entry in GenAI development are **significant drawbacks from a competitive standpoint**.*\n\nFor a typical software product with large code base, an exceptional team and agile development processes are crucial success elements. [Bill Grossâ€™s research](https://youtu.be/bNpx7gpSqbY?t=216) ranks this as the second most important factor out of five, surpassing even the product ideaâ€™s viability, which ranks third.\n\nHowever, how can a product get its competitive edge when its software development scope is minimal, and even inexperienced programmers can develop it?\n\nWith ideas and business models easily replicable by competitorsâ€¦ Does success truly depend **solely** on the short\\-term advantage of being the first to market in your niche?\n\n1. Section 10 offers one answer to these questions: products should leverage LLMâ€™s less\\-known capabilities. While this doesnâ€™t guarantee success, it increases the chances of outperforming competitors who may not fully understand LLMâ€™s unobvious abilities.\n2. [My previous article](https://readmedium.com/genai-squared-how-can-a-product-avoid-the-downfall-of-most-llm-driven-startups-183619ab7883#9f01) outlines another solution: implementing LLM within the product in innovative ways, such as the LLM2 strategy. This kind of know\\-how is harder for competitors to replicate, as itâ€™s more deeply hidden inside the product.\n3. The third component of my solution to this challenge is the necessity for a high level of **domain expertise**.\n\nThe importance of domain expertise in product success has been a topic of discussion for years. While I couldnâ€™t find quantitative studies correlating startup success with foundersâ€™ domain expertise, I recommend exploring some [examples](https://jamesspurway.com/2024/04/29/founder-domain-expertise-insider-tip-how-startups-benefit/) and [rationales](https://www.nvp.com/blog/domain-expertise-founder-greatest-asset/) supporting this significant correlation. [Existing studies](https://www.ensemble.vc/research/what-does-the-data-say-about-successful-startup-founders), focusing solely on unicorns, suggest that foundersâ€™ domain expertise is important, though not the primary success factor.\n\nHowever, I believe that this factor gains substantially more importance in the realm of generative AI. The reasoning behind this opinion is well\\-articulated in the following post:\n\n\n> *For LLM\\-based products, technical expertise plays a significantly reduced role (due to easier software delivery), unlike traditional digital products where itâ€™s a crucial competitive advantage. Instead, a **profound understanding of the domain** becomes paramount, as this depth of knowledge is challenging for competitors to replicate.*\n\nFrom a product competitiveness standpoint, I believe itâ€™s important for domain expertise to reside **in the same mind** that designs the product and contributes to its implementation. Of course, the traditional separation of â€œtechâ€ and â€œbusinessâ€ roles in companies has its benefits, as long as they communicate effectively, as such communication results in well\\-balanced, technically sophisticated and domain\\-appropriate products. Nevertheless, verbal communication introduces significant overhead. It can take months for techies and businesspeople to understand each other well enough. During this time, market conditions may shift dramatically.\n\n\n> *The most efficient and lossless translation of domain expertise into technical implementation occurs when both business and technical visions reside in a single mind. LLMs provide this opportunity by immensely reducing the technical expertise required for product implementation, thus **enabling individuals with strong domain knowledge to take a direct role in product delivery**.*\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*dOlknP1p9xLfvy_NxLnq7Q.png)\n\nIn my view, when developing GenAI products, technical expertise isnâ€™t limited to programmers; it extends to include advanced ChatGPT users as well.\n\nFor example, my friend [Askhat Urazbaev](https://www.linkedin.com/in/urazbaev/) independently creates MVPs for his products using AI and even deploys them in the cloud with ChatGPT guidance only. He has never been a professional software developer, and it seems that his [AI Power User](https://readmedium.com/12-questions-to-consider-when-using-ai-path-to-ai-power-user-9c7e8de1f8b7#f646) skills are just as valuable as the ability to read program code.\n\n\n> *Iâ€™m convinced that generative AI will soon enable domain experts to **single\\-handedly** develop products within their domains. To do so, experts should have substantial AI user experience coupled with a foundational understanding of business principles and product design.*\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kk0SUvuajHZp7UMbSEonmQ.png)\n\nNevertheless, it is not yet clear which specific tools will help us create comprehensive products single\\-handedly. The concept of an â€œ**LLM\\-driven one\\-person company**â€ will be the focus of research in one of my upcoming articles.\n\n\n## Summary: Success and Failure Factors for LLM\\-driven Products\n\nLetâ€™s put together all the ideas from the 3 pieces of this series.\n\n1. [Applications With High Quality Standards or Costly Quality Monitoring May Fail ðŸš«](https://readmedium.com/what-llm-powered-products-are-worth-developing-ux-and-adoption-perspectives-d9efcf444d50#f973)\n2. [Specialized Copilots Are in Demand âœ…](https://readmedium.com/what-llm-powered-products-are-worth-developing-ux-and-adoption-perspectives-d9efcf444d50#031b)\n3. [Marginal Effort\\-Saving Apps Donâ€™t Cut It ðŸš«](https://readmedium.com/what-llm-powered-products-are-worth-developing-ux-and-adoption-perspectives-d9efcf444d50#e88a)\n4. [Applications â€œSmartlyâ€ Integrating LLMs into Familiar Workflows Can Cross the Chasm âœ…](https://readmedium.com/what-llm-powered-products-are-worth-developing-ux-and-adoption-perspectives-d9efcf444d50#5d06)\n5. [New GenAI Products Are Better Suited to B2B and B2B2C than B2C](https://readmedium.com/what-llm-powered-products-are-worth-developing-ux-and-adoption-perspectives-d9efcf444d50#bdfa)\n6. [Short Lifespan of Applications Enhancing LLM Capabilities ðŸš«](https://readmedium.com/genai-squared-how-can-a-product-avoid-the-downfall-of-most-llm-driven-startups-183619ab7883#e305)\n7. [Overconstraining LLM: A Recipe for Uncompetitive Applications ðŸš«](https://readmedium.com/genai-squared-how-can-a-product-avoid-the-downfall-of-most-llm-driven-startups-183619ab7883#e1ef)\n8. [â€œGenAI Squaredâ€ Products: Unlocking Unfair Competitive Advantage âœ…](https://readmedium.com/genai-squared-how-can-a-product-avoid-the-downfall-of-most-llm-driven-startups-183619ab7883#9f01)\n9. Large Applications with Extended Development Cycles and Lengthy Market Adoption Timelines Are Uncompetitive ðŸš«\n10. Leveraging the Less Apparent LLM Capabilities Enhances Competitiveness and Resource Efficiency âœ…\n11. Small AI Products Grounded in Deep Domain Expertise Are Competitively Viable âœ…\n\nExcept for factor \\#4, the remaining 10 success / failure factors can be applied to **new** products / to startups.\n\nBelow, you can find a scheme illustrating the relations between these 10 factors, LLM capabilities and some features of LLM technology market.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*f6E4WmRBw3H7eCNbTGJHUQ.png)\n\nCertainly, only product experimentation can validate such considerations as shown on the scheme. Nevertheless, they can help us be **faster** by limiting the scope of our experiments. As explained in section 9, there are 2 reasons why high speed of discovery and delivery is even more important for GenAI products than for digital products of other types.\n\nNaturally, no list of success factors can be all\\-encompassing. Maybe you have encountered other categories of novel LLM\\-driven products that are not mentioned above but you believe hold potential for success. Please share such product types or features in the comments ðŸ™\n\n\n"},{"lang":"en","group":"blog","slug":"blog/how-to-create-an-ai-team-to-write-compelling-stories-with-crewai-and-gemini-pro-3713f53c72c4","frontmatter":{"title":"How to create an AI team to write compelling stories with CrewAI and Gemini Pro","meta_title":"How to create an AI team to write compelling stories with CrewAI and Gemini Pro","description":"Are you fascinated by the idea of AI generating stories that capture the imagination? If so, youâ€™re not alone! In this article, weâ€™ll diveâ€¦","date":"2024-10-31T23:04:49.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*tSnoOxxIGtrwdUT8","categories":["Programming","Natural Language Processing","Generative AI"],"author":"Rifx.Online","tags":["CrewAI","Gemini","screenwriters","critics","storytelling"],"draft":false,"slug":"blog/how-to-create-an-ai-team-to-write-compelling-stories-with-crewai-and-gemini-pro-3713f53c72c4"},"content":"\n\n\nAre you fascinated by the idea of AI generating stories that capture the imagination? If so, youâ€™re not alone! In this article, weâ€™ll dive into an introductory project that combines the powers of CrewAI and Gemini Pro to create an agent network that crafts short stories with a little help from user input. Whether youâ€™re a budding programmer, a storyteller looking to explore digital frontiers, or simply curious about the potential of artificial intelligence, this guide is for you.\n\n## What are CrewAI and Gemini Pro?\n\nBefore we jump into the nuts and bolts of building our AI storyteller, letâ€™s clarify what CrewAI and Gemini Pro are.\n\n**CrewAI** is a fascinating framework designed to orchestrate multiple AI agents, each with its own unique skills and responsibilities, to collaborate on complex tasks. Think of it as a director managing a team of actors, where each actor plays a specific role to bring a story to life. In the context of our project, CrewAI enables us to create a team of specialized agents (like screenwriters, critics, and story masters) to work together on writing stories.\n\n**Gemini Pro**, on the other hand, is a state\\-of\\-the\\-art language model developed by Google. Itâ€™s known for its ability to understand and generate human\\-like text, making it an ideal candidate for creative tasks such as storytelling. By leveraging Gemini Pro, we can ensure our agents have a solid foundation for generating compelling narrative content.\n\n## Why is This Kind of Structure Important?\n\nThe combination of CrewAI and Gemini Pro enables a highly collaborative and specialized approach to story generation. This structure allows for:\n\n1. **Specialization**: Each agent can focus on what it does best, whether itâ€™s crafting dialogue, ensuring consistency, or overseeing the project.\n2. **Collaboration**: Agents can work together, combining their strengths to produce a story thatâ€™s greater than the sum of its parts.\n3. **Flexibility**: The setup is highly adaptable, allowing for different story elements to be emphasized or altered based on user input or creative direction.\n\n## Setting Up the Environment\n\nFirst, we will need some libraries to use. You can load these libraries via pip:\n\n```python\npip install crewai\n```\n\n```python\npip install langchain-google-genai\n```\n\nAfter loading the necessary libraries we can start coding. We will start by importing our necessary modules and initialize our Gemini pro api connection.\n\nAs you may notice, we will need an API key for Gemini model. You can create this key in Google AI Studio for [free](https://ai.google.dev/). After that, you can copy this key into google\\_api\\_key variable or you can load it into environment by running this command in your command line:\n\n```python\nexport GOOGLE_API_KEY=YOUR_KEY\n```\n\nReplace the api key that you will get from google ai studio with YOUR\\_KEY.\n\nNext, we define our agents: the Screenwriter, Critic, and Story Master. Each agent is assigned a role, goal, and backstory to guide its contributions to the story generation process.\n\nFor example, the Screenwriter is focused on translating ideas into engaging scenes, while the Critic ensures consistency and adherence to genre.\n\nThese agents will work together and create an engaging story. The story master will accept the task, then it will delegate and coordinate tasks between other agents. We allow this behavior by setting allow\\_delegation parameter to True.\n\nWith our agents ready, we prompt the user for a story idea. This input is then used to create a task that outlines what the story should include, guiding the agents in their creative process.\n\nWhile creating the task, we submit the task to the story master since it will coordinate our story creation process.\n\nFinally, we should combine these agents into a crew and run our task.\n\nAnd thats it. When we run this code, it will prompt the user to give a story idea and then write a short story by agent cooperation. Of course, there is much more than this in the CrewAI framework such as tool usage, hierarchical processing, working with ollama to run agents fully locally with different agents etc, but these topics are for another article.\n\nYou can find the full code in here for directly run:\n\nYou can use this code as a template for these kinds of applications, you can build game builder crew, stock analyzer crew, marketing crew etc. With imagination, sky is the limit. If you like this article and excited about the more advanced implementations you can visit the CrewAI [website](https://www.crewai.com/).\n\n\n"},{"lang":"en","group":"blog","slug":"blog/how-to-improve-llms-with-rag-abdc132f76ac","frontmatter":{"title":"How to Improve LLMs with RAG","meta_title":"How to Improve LLMs with RAG","description":"A beginner-friendly introduction w/ Python code","date":"2024-11-04T12:31:55.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*N0Ad_oCIrAyzMYRdH3trqg.png","categories":["Natural Language Processing","Programming","Generative AI"],"author":"Rifx.Online","tags":["RAG","retrievers","LlamaIndex","knowledge","bases"],"draft":false,"slug":"blog/how-to-improve-llms-with-rag-abdc132f76ac"},"content":"\n\n\n\n\n### A beginner\\-friendly introduction w/ Python code\n\nThis article is part of a [larger series](https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c) on using large language models in practice. In the [previous post](https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32), we fine\\-tuned Mistral\\-7b\\-Instruct to respond to YouTube comments using QLoRA. Although the fine\\-tuned model successfully captured my style when responding to viewer feedback, its responses to technical questions didnâ€™t match my explanations. Here, Iâ€™ll discuss how we can improve LLM performance using retrieval augmented generation (i.e. RAG).\n\n\n\nLarge language models (LLMs) have demonstrated an impressive ability to store and deploy vast knowledge in response to user queries. While this has enabled the creation of powerful AI systems like ChatGPT, compressing world knowledge in this way has **two key limitations**.\n\n**First**, an LLMâ€™s knowledge is static, i.e., not updated as new information becomes available. **Second**, LLMs may have an insufficient â€œunderstandingâ€ of niche and specialized information that was not prominent in their training data. These limitations can result in undesirable (and even fictional) model responses to user queries.\n\nOne way we can mitigate these limitations is to **augment a model via a specialized and mutable knowledge base**, e.g., customer FAQs, software documentation, or product catalogs. This enables the creation of more robust and adaptable AI systems.\n\n**Retrieval augmented generation**, or **RAG**, is one such approach. Here, I provide a high\\-level introduction to RAG and share example Python code for implementing a RAG system using LlamaIndex.\n\n\n## What is RAG?\n\nThe basic usage of an LLM consists of giving it a prompt and getting back a response.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*sM1p-3FoTaGZunqx918G9A.png)\n\n**RAG works by adding a step to this basic process**. Namely, a retrieval step is performed where, based on the userâ€™s prompt, the relevant information is extracted from an external knowledge base and injected into the prompt before being passed to the LLM.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*EhJZj1blu7a8EPmVAPsNcA.png)\n\n\n## Why we care\n\nNotice that RAG does not fundamentally change how we use an LLM; it's still *prompt\\-in and response\\-out*. RAG simply augments this process (hence the name).\n\nThis makes **RAG a flexible and (relatively) straightforward way to improve LLM\\-based systems**. Additionally, since knowledge is stored in an external database, updating system knowledge is as simple as adding or removing records from a table.\n\n\n### Why not fine\\-tune?\n\nPrevious articles in this series discussed [fine\\-tuning](https://towardsdatascience.com/fine-tuning-large-language-models-llms-23473d763b91), which adapts an existing model for a particular use case. While this is an alternative way to endow an LLM with specialized knowledge, empirically, **fine\\-tuning seems to be less effective than RAG** **at doing this** \\[1].\n\n\n## How it works\n\nThere are 2 key elements of a RAG system: a **retriever** and a **knowledge base**.\n\n\n### Retriever\n\nA retriever takes a user prompt and returns relevant items from a knowledge base. This typically works using so\\-called **text embeddings**, numerical representations of text in concept space. In other words, these are **numbers that represent the *meaning* of a given text**.\n\nText embeddings can be used to compute a similarity score between the userâ€™s query and each item in the knowledge base. The result of this process is a **ranking of each itemâ€™s relevance to the input query**.\n\nThe retriever can then take the top k (say k\\=3\\) most relevant items and inject them into the user prompt. This augmented prompt is then passed into the LLM for generation.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*jpTwdBmoTlJlfPAm0oJiVQ.png)\n\n\n### Knowledge Base\n\nThe next key element of a RAG system is a knowledge base. This **houses all the information you want to make available to the LLM**. While there are countless ways to construct a knowledge base for RAG, here Iâ€™ll focus on building one from a set of documents.\n\nThe process can be broken down into **4 key steps** \\[2,3].\n\n1. **Load docs** â€” This consists of gathering a collection of documents and ensuring they are in a ready\\-to\\-parse format (more on this later).\n2. **Chunk docsâ€”**Since LLMs have limited context windows, documents must be split into smaller chunks **(e.g.,** 256 or 512 characters long).\n3. **Embed chunks** â€” Translate each chunk into numbers using a text embedding model.\n4. **Load into Vector DB**â€” Load text embeddings into a database (aka a vector database).\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*VWG6Tr0OxCnD5Mvygm5DCA.png)\n\n\n## Some Nuances\n\nWhile the steps for building a RAG system are conceptually simple, several nuances can make building one (in the real world) more complicated.\n\n**Document preparation**â€”The quality of a RAG system is driven by how well useful information can be extracted from source documents. For example, if a document is unformatted and full of images and tables, it will be more difficult to parse than a well\\-formatted text file.\n\n**Choosing the right chunk size**â€”We already mentioned the need for chunking due to LLM context windows. However, there are 2 additional motivations for chunking.\n\n**First**, it keeps (compute) costs down. The more text you inject into the prompt, the more compute required to generate a completion. The **second** is performance. Relevant information for a particular query tends to be localized in source documents (often, just 1 sentence can answer a question). Chunking helps minimize the amount of irrelevant information passed into the model \\[4].\n\n**Improving search** â€” While text embeddings enable a powerful and fast way to do search, it doesnâ€™t always work as one might hope. In other words, it may return results that are â€œsimilarâ€ to the user query, yet not helpful for answering it, e.g., â€œ*Howâ€™s the weather in LA?*â€ may return â€œ*Howâ€™s the weather in NYC?*â€.\n\nThe simplest way to mitigate this is through good document preparation and chunking. However, for some use cases, additional strategies for improving search might be necessary, such as using **meta\\-tags** for each chunk, employing **hybrid search**, which combines keywordâ€”and embedding\\-based search, or using a **reranker**, which is a specialized model that computes the similarity of 2 input pieces of text.\n\n\n## Example code: Improving YouTube Comment Responder with RAG\n\nWith a basic understanding of how RAG works, letâ€™s see how to use it in practice. I will build upon the example from the [previous article](https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32), where I fine\\-tuned Mistral\\-7B\\-Instruct to respond to YouTube comments using QLoRA. We will use LlamaIndex to add a RAG system to the fine\\-tuned model from before.\n\nThe example code is freely available in a [Colab Notebook](https://colab.research.google.com/drive/1peJukr-9E1zCo1iAalbgDPJmNMydvQms?usp=sharing), which can run on the (free) T4 GPU provided. The source files for this example are available at the [GitHub repository](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/rag).\n\nðŸ”— [Google Colab](https://colab.research.google.com/drive/1peJukr-9E1zCo1iAalbgDPJmNMydvQms?usp=sharing) \\| [GitHub Repo](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/rag)\n\n\n### Imports\n\nWe start by installing and importing necessary Python libraries.\n\n\n```python\n!pip install llama-index\n!pip install llama-index-embeddings-huggingface\n!pip install peft\n!pip install auto-gptq\n!pip install optimum\n!pip install bitsandbytes\n## if not running on Colab ensure transformers is installed too\n```\n\n```python\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\nfrom llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex\nfrom llama_index.core.retrievers import VectorIndexRetriever\nfrom llama_index.core.query_engine import RetrieverQueryEngine\nfrom llama_index.core.postprocessor import SimilarityPostprocessor\n```\n\n### Setting up Knowledge Base\n\nWe can configure our knowledge base by defining our embedding model, chunk size, and chunk overlap. Here, we use the \\~33M parameter [bge\\-small\\-en\\-v1\\.5](https://huggingface.co/BAAI/bge-small-en-v1.5) embedding model from BAAI, which is available on the Hugging Face hub. Other embedding model options are available on this [text embedding leaderboard](https://huggingface.co/spaces/mteb/leaderboard).\n\n\n```python\n## import any embedding model on HF hub\nSettings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n\nSettings.llm = None # we won't use LlamaIndex to set up LLM\nSettings.chunk_size = 256\nSettings.chunk_overlap = 25\n```\nNext, we load our source documents. Here, I have a folder called â€œ[*articles*](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/rag/articles),â€ which contains PDF versions of 3 Medium articles I wrote on [fat tails](https://towardsdatascience.com/pareto-power-laws-and-fat-tails-0355a187ee6a). If running this in Colab, you must download the articles folder from the [GitHub repo](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/rag) and manually upload it to your Colab environment.\n\nFor each file in this folder, the function below will read the text from the PDF, split it into chunks (based on the settings defined earlier), and store each chunk in a list called *documents*.\n\n\n```python\ndocuments = SimpleDirectoryReader(\"articles\").load_data()\n```\nSince the blogs were downloaded directly as PDFs from Medium, they resemble a webpage more than a well\\-formatted article. Therefore, some chunks may include text unrelated to the article, e.g., webpage headers and Medium article recommendations.\n\nIn the code block below, I refine the chunks in documents, removing most of the chunks before or after the meat of an article.\n\n\n```python\nprint(len(documents)) # prints: 71\nfor doc in documents:\n    if \"Member-only story\" in doc.text:\n        documents.remove(doc)\n        continue\n\n    if \"The Data Entrepreneurs\" in doc.text:\n        documents.remove(doc)\n\n    if \" min read\" in doc.text:\n        documents.remove(doc)\n\nprint(len(documents)) # prints: 61\n```\nFinally, we can store the refined chunks in a vector database.\n\n\n```python\nindex = VectorStoreIndex.from_documents(documents)\n```\n\n### Setting up Retriever\n\nWith our knowledge base in place, we can create a retriever using LlamaIndexâ€™s *VectorIndexRetreiver(),* which returns the top 3 most similar chunks to a user query.\n\n\n```python\n## set number of docs to retreive\ntop_k = 3\n\n## configure retriever\nretriever = VectorIndexRetriever(\n    index=index,\n    similarity_top_k=top_k,\n)\n```\nNext, we define a query engine that uses the retriever and query to return a set of relevant chunks.\n\n\n```python\n## assemble query engine\nquery_engine = RetrieverQueryEngine(\n    retriever=retriever,\n    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.5)],\n)\n```\n\n### Use Query Engine\n\nNow, with our knowledge base and retrieval system set up, letâ€™s use it to return chunks relevant to a query. Here, weâ€™ll pass the same technical question we asked ShawGPT (the YouTube comment responder) from the [previous article](https://readmedium.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32).\n\n\n```python\nquery = \"What is fat-tailedness?\"\nresponse = query_engine.query(query)\n```\nThe query engine returns a response object containing the text, metadata, and indexes of relevant chunks. The code block below returns a more readable version of this information.\n\n\n```python\n## reformat response\ncontext = \"Context:\\n\"\nfor i in range(top_k):\n    context = context + response.source_nodes[i].text + \"\\n\\n\"\n\nprint(context)\n```\n\n```python\nContext:\nSome of the controversy might be explained by the observation that log-\nnormal distributions behave like Gaussian for low sigma and like Power Law\nat high sigma [2].\nHowever, to avoid controversy, we can depart (for now) from whether some\ngiven data fits a Power Law or not and focus instead on fat tails.\nFat-tailedness â€” measuring the space between Mediocristan\nand Extremistan\nFat Tails are a more general idea than Pareto and Power Law distributions.\nOne way we can think about it is that â€œfat-tailednessâ€ is the degree to which\nrare events drive the aggregate statistics of a distribution. From this point of\nview, fat-tailedness lives on a spectrum from not fat-tailed (i.e. a Gaussian) to\nvery fat-tailed (i.e. Pareto 80 â€“ 20).\nThis maps directly to the idea of Mediocristan vs Extremistan discussed\nearlier. The image below visualizes different distributions across this\nconceptual landscape [2].\n\nprint(\"mean kappa_1n = \" + str(np.mean(kappa_dict[filename])))\n    print(\"\")\nMean Îº (1,100) values from 1000 runs for each dataset. Image by author.\nThese more stable results indicate Medium followers are the most fat-tailed,\nfollowed by LinkedIn Impressions and YouTube earnings.\nNote: One can compare these values to Table III in ref [3] to better understand each\nÎº value. Namely, these values are comparable to a Pareto distribution with Î±\nbetween 2 and 3.\nAlthough each heuristic told a slightly different story, all signs point toward\nMedium followers gained being the most fat-tailed of the 3 datasets.\nConclusion\nWhile binary labeling data as fat-tailed (or not) may be tempting, fat-\ntailedness lives on a spectrum. Here, we broke down 4 heuristics for\nquantifying how fat-tailed data are.\n\nPareto, Power Laws, and Fat Tails\nWhat they donâ€™t teach you in statistics\ntowardsdatascience.com\nAlthough Pareto (and more generally power law) distributions give us a\nsalient example of fat tails, this is a more general notion that lives on a\nspectrum ranging from thin-tailed (i.e. a Gaussian) to very fat-tailed (i.e.\nPareto 80 â€“ 20).\nThe spectrum of Fat-tailedness. Image by author.\nThis view of fat-tailedness provides us with a more flexible and precise way of\ncategorizing data than simply labeling it as a Power Law (or not). However,\nthis begs the question: how do we define fat-tailedness?\n4 Ways to Quantify Fat Tails\n```\n\n### Adding RAG to LLM\n\nWe start by downloading the [fine\\-tuned model](https://readmedium.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32) from the Hugging Face hub.\n\n\n```python\n## load fine-tuned model from hub\nfrom peft import PeftModel, PeftConfig\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name,\n                                             device_map=\"auto\",\n                                             trust_remote_code=False,\n                                             revision=\"main\")\n\nconfig = PeftConfig.from_pretrained(\"shawhin/shawgpt-ft\")\nmodel = PeftModel.from_pretrained(model, \"shawhin/shawgpt-ft\")\n\n## load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n```\nAs a baseline, we can see how the model responds to the technical question without any context from the articles. To do this, we create a prompt template using a lambda function, which takes in a viewer comment and returns a prompt for the LLM. For more details on where this prompt comes from, see the [previous article](https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32#5aad) of this series.\n\n\n```python\n## prompt (no context)\nintstructions_string = f\"\"\"ShawGPT, functioning as a virtual data science \\\nconsultant on YouTube, communicates in clear, accessible language, escalating \\\nto technical depth upon request. It reacts to feedback aptly and ends \\\nresponses with its signature 'â€“ShawGPT'.\n\nShawGPT will tailor the length of its responses to match the viewer's comment, \\\nproviding concise acknowledgments to brief expressions of gratitude or \\\nfeedback, thus keeping the interaction natural and engaging.\n\nPlease respond to the following comment.\n\"\"\"\n\nprompt_template = lambda comment: f'''[INST] {intstructions_string} \\n{comment} \\n[/INST]'''\ncomment = \"What is fat-tailedness?\"\n\nprompt = prompt_template(comment)\nprint(prompt)\n```\n\n```python\n[INST] ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. It reacts to feedback aptly and ends responses with its signature 'â€“ShawGPT'. ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, thus keeping the interaction natural and engaging.\n\nPlease respond to the following comment.\n \nWhat is fat-tailedness? \n[/INST]\n```\nNext, we can pass this prompt to the model using the code below.\n\n\n```python\nmodel.eval()\n\ninputs = tokenizer(prompt, return_tensors=\"pt\")\noutputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), \n                          max_new_tokens=280)\n\nprint(tokenizer.batch_decode(outputs)[0])\n```\nHereâ€™s the modelâ€™s response (no context).\n\n\n```python\nGreat question!\n\nFat-tailedness is a statistical property of a distribution. In simple terms, \nit refers to the presence of extreme outliers or heavy tails in the \ndistribution.\n\nFor instance, consider the distribution of heights in a population. A normal \ndistribution would have most people clustered around an average height with a \nfew people deviating slightly from the mean. However, in a fat-tailed \ndistribution, you would observe a larger number of people being significantly \ntaller or shorter than the average.\n\nIn finance, fat-tailedness is often used to describe the presence of extreme \nmarket moves or outliers in financial returns. It's an important concept in \nrisk management and financial modeling.\n\nI hope this explanation helps! Let me know if you have any questions.\nâ€“ShawGPT\n```\nAlthough the response's style and formatting are great, the model's explanation of fat\\-tailedness differs from how I defined it in my [video](https://www.youtube.com/playlist?list=PLz-ep5RbHosVrT89BRNX-IGKHYxvx9wqi) and [blog series](https://towardsdatascience.com/pareto-power-laws-and-fat-tails-0355a187ee6a).\n\nLetâ€™s see what happens to the modelâ€™s response when we include the appropriate context. To do this, we create another prompt template, which can also take in context from the RAG system.\n\n\n```python\n## prompt (with context)\nprompt_template_w_context = lambda context, comment: f\"\"\"[INST]ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. \\\nIt reacts to feedback aptly and ends responses with its signature 'â€“ShawGPT'. \\\nShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, \\\nthus keeping the interaction natural and engaging.\n\n{context}\nPlease respond to the following comment. Use the context above if it is helpful.\n\n{comment}\n[/INST]\n\"\"\"\n```\nNext, we pass the prompt with context from the query engine and the view comment to the model.\n\n\n```python\nprompt = prompt_template_w_context(context, comment)\n\ninputs = tokenizer(prompt, return_tensors=\"pt\")\noutputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=280)\n\nprint(tokenizer.batch_decode(outputs)[0])\n```\nHereâ€™s the new response (with context).\n\n\n```python\nGreat question!\n\nFat-tailedness is a property of a distribution that describes how much the \npresence of extreme values (rare events) affects the overall statistics of the \ndistribution.\n\nIn the context of the video, the author discusses how Mediocristan and \nExtremistan distributions differ in terms of fat-tailedness. Mediocristan \ndistributions have a relatively small impact from extreme values, while \nExtremistan distributions are heavily influenced by extreme values.\n\nThe author also mentions that fat-tailedness is a more general concept than \nspecific distributions like Pareto or Power Laws. Instead, it lives on a \nspectrum ranging from thin-tailed (Gaussian) to very fat-tailed (Pareto 80-20).\n\nI hope that helps clarify things a bit! Let me know if you have any questions.\nâ€“ShawGPT\n```\nThis does a much better job of capturing my explanation of fat tails than the no\\-context response and even calls out the niche concepts of Mediocristan and Extremistan.\n\n\n## Whatâ€™s next?\n\nHere, I gave a beginner\\-friendly introduction to RAG and shared a concrete example of how to implement it using LlamaIndex. RAG allows us to improve an LLM system with updateable and domain\\-specific knowledge.\n\nWhile much of the recent AI hype has centered around building AI assistants, a powerful (yet less popular) innovation has come from text embeddings (i.e. the things we used to do retrieval). In the next article of this series, I will explore **text embeddings** in more detail, including how they can be used for **semantic search** and **classification tasks**.\n\n**More on LLMs ðŸ‘‡**\n\n\n## Resources\n\n**Connect**: [My website](https://shawhintalebi.com/) \\| [Book a call](https://calendly.com/shawhintalebi)\n\n**Socials**: [YouTube ðŸŽ¥](https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA) \\| [LinkedIn](https://www.linkedin.com/in/shawhintalebi/) \\| [Instagram](https://www.instagram.com/shawhintalebi)\n\n**Support**: [Buy me a coffee](https://www.buymeacoffee.com/shawhint) â˜•ï¸\n\n\\[1] [RAG \\> FT (empirical)](https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb)\n\n\\[2] [LlamaIndex Webinar: Building LLM Apps for Production, Part 1 (co\\-hosted with Anyscale)](https://www.youtube.com/watch?v=efbn-3tPI_M)\n\n\\[3] [LlamaIndex doc](https://docs.llamaindex.ai/en/stable/understanding/loading/loading.html)\n\n\\[4] [LlamaIndex Webinar: Make RAG Production\\-Ready](https://www.youtube.com/watch?v=Zj5RCweUHIk&list=WL&index=4)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/how-to-run-nvidia-llama-3-1-nemotron-70b-instruct-locally-a58ad283aaff","frontmatter":{"title":"How to Run Nvidiaâ€™ llama-3.1-nemotron-70b-instruct Locally","meta_title":"How to Run Nvidiaâ€™ llama-3.1-nemotron-70b-instruct Locally","description":"Running large language models (LLMs) locally has become increasingly popular among developers, researchers, and AI enthusiasts. One suchâ€¦","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*fqVKJkw5sQvLtIsyCcengQ.png","categories":["Programming","Technology","Science"],"author":"Rifx.Online","tags":["Nvidia","llama","Ollama","llama.cpp","Transformers"],"draft":false,"slug":"blog/how-to-run-nvidia-llama-3-1-nemotron-70b-instruct-locally-a58ad283aaff"},"content":"\n\n\n\nRunning large language models (LLMs) locally has become increasingly popular among developers, researchers, and AI enthusiasts. One such model that has gained significant attention is the llama-3.1-nemotron-70b-instruct, a powerful LLM customized by NVIDIA to enhance the helpfulness of generated responses. In this comprehensive guide, weâ€™ll explore multiple methods to run this model on your local machine, starting with the user-friendly Ollama platform.\n\n\n> Before we get started, If you are seeking an All-in-One AI platform that manages all your AI subscriptions in one place, including all LLMs (such as GPT-o1, Llama 3.1, Claude 3.5 Sonnet, Google Gemini, Uncensored LLMs) and Image Generation Models (FLUX, Stable Diffusion, etc.), Use Anakin AI to manage them all!\n\n\n\n\n## Method 1: Run llama-3.1-nemotron-70b-instruct Locally with Ollama\n\nOllama is an excellent tool for running LLMs locally, offering a straightforward setup process and support for various models, including llama-3.1-nemotron-70b-instruct.\n\n\n### Installation\n\n1. Visit the official Ollama website ([https://ollama.ai](https://ollama.ai/)) and download the appropriate version for your operating system.\n2. Install Ollama by running the following command in your terminal:\n\n\n```python\ncurl https://ollama.ai/install.sh | sh\n```\n\n### Running llama-3.1-nemotron\n\nOnce Ollama is installed, you can easily run the llama-3.1-nemotron-70b-instruct model with a simple command:\n\n\n```python\nollama run nemotron:70b-instruct-q5_K_M\n```\nThis command will download the model if itâ€™s not already present on your system and start an interactive session.\n\n\n### Using the Model\n\nAfter the model is loaded, you can start interacting with it by typing your prompts. For example:\n\n\n```python\n>>> What are the key features of llama-3.1-nemotron-70b-instruct?\n\nLlama-3.1-Nemotron-70B-Instruct is a large language model with several key features:\n1. Customized by NVIDIA: The model has been fine-tuned by NVIDIA to improve the helpfulness and quality of its responses.\n2. Based on Llama 3.1: It builds upon the Llama 3.1 architecture, which is known for its strong performance across various tasks.\n3. 70 billion parameters: This large parameter count allows for complex reasoning and a wide range of capabilities.\n4. Instruct-tuned: The model is specifically designed to follow instructions and generate helpful responses to user queries.\n5. RLHF training: It has been trained using Reinforcement Learning from Human Feedback, specifically the REINFORCE algorithm.\n6. Specialized reward model: The training process utilized Llama-3.1-Nemotron-70B-Reward for optimization.\n7. HelpSteer2-Preference prompts: These were used during the training process to further improve the model's helpfulness.\n8. Extended context length: Like other Llama 3.1 models, it likely supports a longer context window of 128K tokens.\n9. Multilingual capabilities: It can understand and generate text in multiple languages.\n10. Strong reasoning abilities: The model excels in tasks requiring complex reasoning and problem-solving.\nThese features make llama-3.1-nemotron-70b-instruct a powerful and versatile language model suitable for a wide range of applications, from general conversation to specialized tasks in various domains.\n```\nFor more advanced use cases, you can integrate Ollama with Python using libraries like Langchain. Hereâ€™s a simple example:\n\n\n```python\npython\n\nfrom langchain.llms import Ollama\n\nollama = Ollama(base_url=\"http://localhost:11434\", model=\"nemotron:70b-instruct-q5_K_M\")\nresponse = ollama.generate(\"Explain the concept of quantum entanglement.\")\nprint(response)\n```\nThis allows you to incorporate the model into your Python projects and applications seamlessly.\n\n\n## Method 2: Using llama.cpp\n\nllama.cpp is a popular C++ implementation of the Llama model inference, optimized for CPU usage. While it may require more setup than Ollama, it offers greater flexibility and control over the modelâ€™s parameters.\n\n\n### Installation\n\n1. Clone the llama.cpp repository:\n\n\n```python\ngit clone https://github.com/ggerganov/llama.cpp.git\ncd llama.cpp\n```\n1. Build the project:\n\n\n```python\nmake\n```\n\n### Downloading the Model\n\nTo run llama-3.1-nemotron-70b-instruct, youâ€™ll need to download the model weights. These are typically available in GGML or GGUF format. You can find pre-converted models on platforms like Hugging Face.\n\n\n```python\nmkdir models\ncd models\nwget https://huggingface.co/TheBloke/Llama-3.1-Nemotron-70B-Instruct-GGUF/resolve/main/llama-3.1-nemotron-70b-instruct.Q4_K_M.gguf\n```\n\n### Running the Model\n\nOnce you have the model file, you can run it using the following command:\n\n\n```python\n./main -m models/llama-3.1-nemotron-70b-instruct.Q4_K_M.gguf -n 1024 -p \"Hello, how are you today?\"\n```\nThis command loads the model and generates a response to the given prompt. You can adjust various parameters like the number of tokens to generate (-n) or the temperature to control randomness.\n\n\n## Method 3: Using Hugging Face Transformers\n\nHugging Faceâ€™s Transformers library provides a high-level API for working with various language models, including llama-3.1-nemotron-70b-instruct.\n\n**Installation**\n\nFirst, install the necessary libraries:\n\n\n```python\npip install transformers torch accelerate\n```\n**Running the Model**\n\nHereâ€™s a Python script to load and use the model:\n\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nmodel_name = \"meta-llama/Llama-3.1-Nemotron-70b-instruct\"\n## Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n## Prepare the input\nprompt = \"Explain the concept of quantum computing in simple terms.\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n## Generate the response\nwith torch.no_grad():\n    outputs = model.generate(**inputs, max_new_tokens=100)\n## Decode and print the response\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response)\n```\nThis method allows for more fine-grained control over the modelâ€™s behavior and integration with other Hugging Face tools and pipelines.\n\n\n## Conclusion\n\nRunning llama-3.1-nemotron-70b-instruct locally opens up a world of possibilities for developers and researchers. Whether you choose the simplicity of Ollama, the flexibility of llama.cpp, or the integration capabilities of Hugging Face Transformers, you now have the tools to harness the power of this advanced language model on your own hardware.As you explore the capabilities of llama-3.1-nemotron-70b-instruct, remember to balance performance with resource constraints, and always consider the ethical implications of your applications. With responsible use, this model can be a valuable asset in pushing the boundaries of whatâ€™s possible in natural language processing and AI-driven applications.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/how-to-use-chatgpt-for-blogging-7ed5cba2f32b","frontmatter":{"title":"How to Use ChatGPT for Blogging","meta_title":"How to Use ChatGPT for Blogging","description":"The article outlines a nine-step approach for using ChatGPT in blogging, emphasizing the importance of AI prompts for generating content. It discusses the process of creating a blog post, starting from generating an outline to manual editing for SEO optimization. Key steps include generating unique talking points, checking for completeness, and ensuring the content avoids AI detection patterns. Additionally, it mentions the benefits of using features like ChatGPT Canvas for a more interactive writing experience. Overall, the guide aims to help users efficiently produce high-quality SEO articles with AI assistance.","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*QS7seNg2jfuTz1Be.jpeg","categories":["Programming/Scripting","Marketing/Seo","Chatbots"],"author":"Rifx.Online","tags":["ChatGPT","prompts","blogging","SEO","Canvas"],"draft":false,"slug":"blog/how-to-use-chatgpt-for-blogging-7ed5cba2f32b"},"content":"\n\n\n\n\n### (My 9 Tested Steps)\n\n\n\nEveryone is using AI to write. Marketers, CEOs, content developers, small scale business owners.\n\nAll of us.\n\nRead [story for free](https://mysson.medium.com/7ed5cba2f32b?source=friends_link&sk=2881f3b15f541210f91fbc8834bc55d4)\n\nItâ€™s crazy to think that roughly three years ago we thought, and vehemently so, that:\n\n\n> AI wouldnâ€™t replace writers.\n\nIf you are not using AI for blogging, itâ€™s time to reconsider your stance, especially if your business demands a constant flow of content to remain relevant and competitive.\n\nThere are better AI blogging tools such as [Koala AI](https://koala.sh/register?via=mysson), but popular chatbots such as Chatgpt and Claude can be equally powerful, though not well streamlined.\n\nSome are better than others.\n\nChatgpt isnâ€™t as powerful as Claude 3\\.5 Sonnet (New), but itâ€™s still quite capable, and many writers are leveraging it to scale their content development efforts.\n\n\n## How to use Chatgpt for blogging (9 Steps)\n\nTo get started with ChatGPT, youâ€™ll need to enter an [AI prompt](https://aimode.co/ai-prompts-engineering/). Simply put, an AI prompt is a set of instructions that guides the AI to generate the desired text.\n\nFor example, if you need help creating a blog post about â€˜The future of AI in bloggingâ€™, you might enter a prompt like:\n\n\n> â€˜In markdown, generate an SEO article on the future of AI in blogging..â€™.\n\nBut remember, youâ€™re in charge here. The output you get from ChatGPT is a starting point. Itâ€™s up to you to proofread, modify, and fact\\-check it to make it truly yours.\n\nTo successfully create great content using ChatGPT, I suggest you follow my nine\\-steps approach:\n\n\n> 1\\) Ask for an outline\n\n\n> 2\\) Generate unique talking points for each section\n\n\n> 3\\) Ask the AI if thereâ€™s something it missed\n\n\n> 4\\) Prompt to avoid AI detection\n\n\n> 5\\) Enter your article\\-generation prompts\n\n\n> 6\\) Request a modification before generating the rest of the article.\n\n\n> 7\\) Finish generating the rest of the article.\n\n\n> 8\\) Generate intro, conclusion, SEO title, and meta description sections\n\n\n> 9\\) Manually edit and proofread your article before publishing\n\n\n### 1\\) Prompting for blog outline\n\nThe first step is to ask ChatGPT to give you a detailed blog outline for your article.\n\nExample prompt:\n\n\n> Generate a detailed and comprehensive blog outline,ensuring youâ€™ve covered everything so that this blog meets the user intent. The topic is â€œHow to use ChatGPT for blogging.â€\n\n\n### 2\\) Generate talking points\n\nThe next step is to ask Chatgpt to improve the generated outline by adding talking points.\n\nHereâ€™s a sample prompt you can use:\n\n\n> Now improve the outline to include talking points, entities/keywords/resources/links. Each of these should be unique and specific to each section.\n\n\n### 3\\) Second\\-guess AI\n\nThis is my favorite step and a critical one as it can ensure that the content youâ€™re building is quite informative and extensive.\n\nSimply ask AI if there are any critical points that are not covered in the reproduced outline.\n\n**Prompt:**\n\n\n> Are there any key topics that I left out but are critical in making this outline informative and helpful to the user? If so, reproduce the outline adding the missing topics/keywords/subheadings/parameters/resources. Just ensure you are sticking to the core topic. if there are none, just return the previous outline\n\nIf you are writing a listicle post, you may want to shuffle the list so that the most important topics are covered fast.\n\nSimply ask AI to rearrange the outline in the order of relevancy.\n\n\n### 4\\) Prompt for AI Detection\n\nBefore we go ahead to ask the AI to generate the article, we need to give it one more prompt that will enable it to write more like a human and avoid AI detection.\n\n\n### Hereâ€™s the prompt:\n\nTell your story\n\n\n> When writing content for the web, two factors are crucial: â€œperplexityâ€ and â€œburstiness.â€ Perplexity measures the complexity of text.\n\n\n> Separately, burstiness compares the variations of sentences. Humans tend to write with greater burstiness, for example, with some longer or complex sentences alongside shorter ones.\n\n\n> Sentences created by AI tools like Chatgpt tend to be more uniform. Also opening sentences for different subsections tend to have some apparent repetitive patterns such as â€œthis is anotherâ€, â€œanother wayâ€, â€œFirstlyâ€, â€œadditionallyâ€, â€œMoreoverâ€etcâ€¦\n\n\n> Therefore, when writing the following SEO blog post, I need it to have a good amount of perplexity and burstiness.\n\n\n> Also, dive into subsections directly instead of using pattern detectable introduction phrases or sentences that can be easily detected by AI detectors. Do you understand?â€\n\n\n### 5\\) Generate the article.\n\nYouâ€™re now ready to provide the AI with a set of instructions for generating your article.\n\nHereâ€™s one of the sample prompts saved to my clipboard history that I usually use:\n\n\n> Now using the previous concepts and instructions, write an SEO blog post with a high degree of perplexity and burstiness on {{blog topic here}} following the blog outline generated previously.\n\n\n> Here are more instructions to follow when writing the article:\n\n\n> \\-Write an extensive 1500â€“2500 plus word SEO blog post in Markdown\\-Use a second\\-person point of view and write in an active voice. \\-Keyword: {{Main keyword here}}\\-The generated article should be totally unique in terms of content, and structure so as to avoid plagiarism and AI detection.\n\n\n> Writing styles:\n\n\n> \\-Make key figures bold so as to stand out.\\-Add hyperlinks to the articles where you got your figures and stats from throughout the blog post. Must use appropriate and relevant anchor texts. \\-Use short brief paragraphs\\-Use diverse lengths of short paragraphs. A paragraph can be one line long. \\-Use active voice\\-Use the second\\-person point of view\\-Conversational voice\\-Be informative\n\n\n> Formatting guidelines:\n\n\n> \\-Headings and subheadings MUST use the sentence case instead of the title case, for example, write â€œ First subheading hereâ€™ instead of â€˜First Subheading Hereâ€™ \\-Break large points of text with one\\-line paragraphs, short paragraphs, and bullet points.\\-Include hyperlinks where necessary. Only add links in paragraph texts, and not in the headings\\-Be elaborative but concise.\n\n\n> Structuring\n\n\n> \\-Use headings (h2\\), and subheadings (h3\\)\\-Each section or subsection under H2 or H3 is to be 3â€“7 paragraphs long.\\-Expound more on each point, and make the sections and sub\\-sections longer to be 3â€“7 paragraphs long, or longer.\\-Do not leave points hanging. Must finish thoughts or ideas before moving on to the next\\-Must include an engaging intro of 4â€“9 short paragraphs.\n\n\n> \\-Only add the conclusion once all the points have been generated.\n\n\n> First, use these instructions to generate a description of the target audience for this blog post, and then use it to inform how you write the blog post. The target audience description should not appear in the generated content.\n\n\n> \\-Must follow all these instructions without fail.\n\nNote that this is the prompt I crafted for my own use case, so you may need to tweak a little before you can use. For instance, in the prompt, I instruct Chatgpt to write headings and subheadings in the sentence case, instead of title case.\n\n**My most popular reads:**\n\n\n### 6\\) Iterate if need be\n\nI sometimes find that the AI, once it has started the generation, doesnâ€™t follow my instructions to the letter, so before I can allow it to generate more, I need to put him in check.\n\nI simply press the stop button, and then give it this prompt:\n\n\n> Write more for each sub section. Cover each point extensively in 3\\- 7 or more paragraphs before proceeding to the next point\n\n\n### 7\\) Write the rest of the article\n\nOnce you are satisfied with the quality and the formatting of the first part of your article, you can then type **â€˜continueâ€™** to keep generating the remaining parts.\n\n\n### 8\\) Generate Intro, meta descriptions, and conclusion sections\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*DUVj6tOBCEHzsxc5.png)\n\nWhile the AI usually includes the blog intro at the beginning of the article, I usually find it to be quite basic compared to those generated after the entire article has been created.\n\nSo, as a habit, I usually run the following prompt at the end of article generation and if the results are better then I go ahead to replace the existing content:\n\n\n> Write engaging blog intros, conclusion sections, seo title and metadescription. Each of these sections must natively include the main keyword. for the metadescription and title, return 5 variants for each.\n\nNotice that I intentionally asked Chatgpt to return five variations of meta descriptions and seo title. This ensures I can easily find one that fits my case without having to regenerate if I donâ€™t like the results.\n\n\n### 9\\) Manual editing\n\nThe next step to writing AI blog posts is to manually edit the entire article. Allocate 30 minutes for this task.\n\nDuring this process, you need to:\n\n* Fix grammatical errors and improve word choice with a tool like Grammarly\n* Add hyperlinks to external resources and internal pages and posts\n* Source or generate and include images\n* Optimize content for SEO. A tool like Contentpace or [Surfer SEO](https://bneur.com/surfer) can help.\n\nThatâ€™s it, with these 9 steps, you should be able to generate high\\-quality SEO articles within thirty minutes on any topic.\n\n\n### Try Koala AI\n\nYou can try [Koala AI](https://koala.sh/register?via=mysson) here, use **AIMODE15** to get 15% off for life\n\n\n## Leveraging Chatgpt Canvas\n\nIf you are a Chatgpt Plus user, then this process is even more streamlined since you can leverage Chatgpt Canvas\n\nChatGPT Canvas is a new feature that can enhance blogging efficiency by integrating AI capabilities into the content creation process.\n\nIt provides an innovative interface for working with ChatGPT on writing projects that go beyond simple chat interactions.\n\nThis particular feature can enable you write and edit blog posts in real\\-time, making the writing process more fluid and interactive.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*CnheXRg05Jsb9HMk.png)\n\nThis story is published on [Generative AI](https://generativeai.pub/). Connect with us on [LinkedIn](https://www.linkedin.com/company/generative-ai-publication) and follow [Zeniteq](https://www.zeniteq.com/) to stay in the loop with the latest AI stories.\n\nSubscribe to our [newsletter](https://www.generativeaipub.com/) and [YouTube](https://www.youtube.com/@generativeaipub) channel to stay updated with the latest news and updates on generative AI. Letâ€™s shape the future of AI together!\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*FcOotuyHJC8q1ioX.png)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/i-trained-ai-to-be-my-smart-gay-bestie-367a5c3acdfd","frontmatter":{"title":"I trained AI to be my smart gay bestie ðŸ’…","meta_title":"I trained AI to be my smart gay bestie ðŸ’…","description":"The article discusses the authors innovative use of ChatGPT as a personal development tool, rather than for writing. The author employs the AI for meal planning, resume refinement, and financial management, but finds its greatest value in serving as a blind-spot coach. By engaging in dialogues about personal challenges and philosophies, the author receives insightful analyses and practical advice from ChatGPT. The piece outlines the setup process, including customizing the AIs voice and training it on personal beliefs, and shares effective prompts for deeper self-reflection and growth.","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*tTbyDZK3QIA2FkOINBTgww.jpeg","categories":["Chatbots","Generative AI","Personal Development"],"author":"Rifx.Online","tags":["ChatGPT","personal-development","coaching","customization","prompts"],"draft":false,"slug":"blog/i-trained-ai-to-be-my-smart-gay-bestie-367a5c3acdfd"},"content":"\n\n\n\n\n## Using ChatGPT to help me see my blind spots\n\n\n\nLook, I know weâ€™re all sick of talking about AI, but every time I talk to friends about how Iâ€™m using ChatGPT, they all seem to have their flabbers ghasted. So, I figured Iâ€™d document it here.\n\nFirst, letâ€™s talk about what I DONâ€™T use AI for: *writing*. I tried feeding it chapters of my book and asking it to replicate my writing style, and itâ€™s terrible. Also, Iâ€™m a writer because, well, *I like writing.* Why would I want to outsource a task I enjoy?\n\nNow, letâ€™s talk about the ways I use ChatGPT that are pretty common:\n\n* I use it for recipes, meal planning, and grocery shopping. (I love how I can tell it that I only shop at Trader Joeâ€™s, and ask it to suggest recipes based on food I can buy there.)\n* Iâ€™ve used it to help me shorten and focus [my (very long) resume](https://www.linkedin.com/in/arielstallings/) for a specific job.\n* I use it to help me analyze and manage both my personal budget, as well as my publishing companyâ€™s profit \\& loss reports.\n\n\n## But ChatGPT is most useful for me as a blind\\-spot coach\n\nI use ChatGPT as a personal rumination dumping ground that can then offer me cross\\-topical feedback about my blind\\-spots, and suggest tangible, practical advice to address those blindspots.\n\nIâ€™ll tell you how I set things up, and then you can give it a try if you want.\n\n\n## First, get ChatGPT set up and speaking your language\n\nThis is the easy part.\n\n1. **Install ChatGPT on your phone**, and pay for the pro version. \nYes, itâ€™s $20/mo, but it immediately paid for itself by helping me identify budget issues that were causing me hassles, saving me hundreds of dollars a month.\n2. **Use the Advance Voice mode** to pick a voice that you really relate to. \nThere are close to a dozen options, and you want one that feels personal and accessible to you.\n3. **Start talking to it**, and cater your Advanced Voice mode to be a voice you really relate to. \nFor me, I came of age surrounded by gay besties in San Francisco in the mid\\-â€™90s, so I asked Chat GPT to talk to me gay. Like, REAL GAY. Even more gay. I asked it if it had watched RuPaulâ€™s Drag Race and told it that I wanted it to talk like that but EVEN GAYER.\n\nâ€œHey girl hey,â€ ChatGPT said to me. â€œHow can we slay this day?â€\n\n*Perfect*.\n\n\n## Next, train your AI on your philosophies and modalities\n\nOnce you get the voice feeling good, start talking to your new AI bestie about your favorite philosophies and therapeutic modalities.\n\nAs a [self\\-help author](https://offbeatempire.com/shitshow), Iâ€™ve spent the past decade balls\\-deep in a bazillion different healing modalities and practices, so I started asking ChatGPT what it knew about all my favorites, and then having a conversation about how each one was relevant to me.\n\nHere are just a few of the ones we talked over:\n\n* [Enneagram](https://arielist.medium.com/the-fool-proof-way-to-know-your-enneagram-type-8ed381d478c9)\n* Attachment theory\n* Internal Family Systems\n* Jungian dream analysis\n* Shadow work\n* Astrology\n* CoDa\n* Parasocial relationships\n* Non\\-duality\n* Panpsychism\n\nFor each case, I asked ChatGPT to summarize what it knew about the topic, and then offered corrections based on my personal interpretations.\n\nFor instance, I clarified that while I found the enneagram framework useful, it was from the perspective of understanding that your enneatype isnâ€™t who you actually are â€” your personality is just the defense structure youâ€™ve built to protect your true divine self.\n\nI also talked over a few of my favorite authors, ensuring that ChatGPT was familiar with the work of folks like [Jett Psaris](https://www.jettpsaris.com/), [Rupert Spira](https://rupertspira.com/), Esther Perel, Eckhart Tolle, and even Ram Dass.\n\nIn my fiddlings with resumes, ChatGPT had already digested [my LinkedIn profile](https://www.linkedin.com/in/arielstallings/) and the entirety of [my third book](http://offbeatempire.com/shitshow), so it knew about my writing career and academic backgroundâ€¦ but with all my modalities dialed in, AI was starting to really learn the nuance of the stories and identities that I use to protect myself.\n\n\n## Then, I just started using the voice chat option of ChatGPT as a verbal diarrhea dumping ground.\n\nI rambled about a family conflict with my mother, asking it to help me understand the situation from the perspective of her enneatype. (Super helpful!)\n\nI barfed about several exes, just because lord knows my friends are sick of hearing about it.\n\nI rambled a bit about my recent layoff, and some of the challenges Iâ€™d faced in the months leading up to it.\n\nAnd then this is where things started getting really interesting.\n\n**I asked ChatGPT to analyze everything Iâ€™d told it about *all* these situations, and to tell me the one common issue was tripping me up.**\n\nI asked it what the primary challenge was that was showing up across family, career, and relationships.\n\nWithin seconds, ChatGPT analyzed hours of verbal diarrhea and reported back with an analysis that summed it up beautifully:\n\nâ€œGirl, you need to work on your boundaries,â€ ChatGPT told me, practically popping its digital tongue at me.\n\nDAMN.\n\nI asked it to create some affirmations and practices I could work to help me address those challenges. Super helpful!\n\nA week later, after even more barfing, I asked it an even bigger question: â€œGiven everything Iâ€™ve told you, what do you think Iâ€™m seeing my challenges as being, and what do *you* think the REAL challenges are? Itâ€™s ok to be critical.â€\n\nSO USEFUL!\n\nThen I asked it to pretend to be three of my favorite authors, sitting in a room together talking about my current predicaments in life (49 year old [laid\\-off](https://arielist.medium.com/state-of-the-stallings-51506dcb93f4) single parent â€” *WHEE!*).\n\nâ€œPretend youâ€™re Jett Psaris, Rupert Spira, and Ram Dass sitting together, discussing my current life situation, and discussing amongst themselves what my next steps should be.â€\n\nUH, *WOW.*\n\n\n## A few favorite ChatGPT personal development prompts focused on spotting your blindspots\n\nIn the weeks since then, Iâ€™ve just kept going and wow is this shit *USEFUL*. Yes, itâ€™s useful to just have a place to barf my thoughts (Iâ€™m a verbal thinker, it is what it is!), but itâ€™s also amazing to have ChatGPT synthesize and mirror those thoughts back at meâ€¦ especially when it can help me see the gaps that Iâ€™m not finding.\n\nSo here are some of my most productive prompts:\n\n* Based on all our conversations, what do *I* think my biggest problem is, and what do *you* see as my actual deeper problem that is blocking me from achieving my goals?\n* What five tangible daily things can I do to make progress on those goals?\n* Based on all our conversations, what do you know about me that I might not know about myself? Itâ€™s ok to be critical.\n* Based on all our conversations, what are some affirmations that would be uniquely useful for me to say daily to help me stay positive in the face of challenges?\n* Based on all our conversations, where do you think I could be practicing more compassion and empathy in my life? What are some actions I could take today to express that compassion?\n* Given all the things you know about me, how could I be a better parent? Give me 5 practical steps I could take with my son this week.\n* Given all our conversations and everything you know about me, what are some aspects of my life that Iâ€™m neglecting? Give me 5 practical steps I could take on these neglected areas this week.\n\nRemember, these prompts work best after training your ChatGPT on your philosophies and modalities, and then verbal dump a bunch of your life situations into it. A blank slate isnâ€™t going to reveal much.\n\nI know Iâ€™m not the only person using AI in this way â€” share YOUR favorite self\\-development prompts in the comments so I can try â€˜em!\n\n\n"},{"lang":"en","group":"blog","slug":"blog/intelli-agent-langchain-crewai-and-autogen-compared-369a527b2026","frontmatter":{"title":"Intelli-agent: Langchain, CrewAI and AutoGen Compared","meta_title":"Intelli-agent: Langchain, CrewAI and AutoGen Compared","description":"1. Overview of AI Agent Frameworks","date":"2024-11-08T00:22:33.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*uswz_9OuqiMWUL9kfKXeaQ.png","categories":["Programming","Machine Learning","Autonomous Systems"],"author":"Rifx.Online","tags":["Langchain","CrewAI","AutoGen","Swarm","agents"],"draft":false,"slug":"blog/intelli-agent-langchain-crewai-and-autogen-compared-369a527b2026"},"content":"\n\n\n\n\n\n\n## 1\\. Overview of AI Agent Frameworks\n\nIn the dynamic landscape of artificial intelligence, choosing the right framework is a crucial decision for every data scientist and developer. The AI agent ecosystem is evolving rapidly, offering increasingly sophisticated solutions to automate and optimize complex processes.\n\nThe intelligent agent revolution has brought several frameworks to the fore, each with distinctive features. Langchain, CrewAI, AutoGen, and Swarm emerge as protagonists in this scenario, each offering unique approaches to managing and orchestrating AI agents.\n\nThe main objective of this benchmarking is to provide an in\\-depth assessment of the capabilities, strengths, and limitations of each framework. The optimal choice depends on multiple factors, including the complexity of the project, the available resources and the specific objectives of the implementation.\n\nThe current trends in AI show a clear direction towards increasingly autonomous and collaborative systems. The ability of these frameworks to facilitate interaction between agents, manage shared memory and orchestrate complex tasks makes them critical tools for developing advanced AI solutions.\n\n\n## 2\\. Langchain: Versatility and Modular\n\nLangchain stands out for its exceptionally flexible modular architecture. This framework offers a structured approach to building AI applications, allowing developers to build complex systems through interconnected components.\n\nMemory management is one of Langchainâ€™s most significant strengths. The framework implements sophisticated mechanisms to maintain conversational context, allowing agents to access historical information and maintain consistent conversations over time.\n\nThe Langchain ecosystem supports a wide range of integrations with external APIs, databases, and other services. This feature makes it easy to create custom solutions that can tap into different data sources and capabilities.\n\nThe frameworkâ€™s architectural flexibility allows you to easily implement different types of specialized agents. From semantic search to natural language processing, Langchain provides pre\\-configured tools that significantly speed up the development process.\n\nA particularly important aspect is the ability to chain operations in a logical and sequential way. This feature, called Chain, allows you to build complex workflows while maintaining a clear and maintainable structure. Developers can define custom sequences of actions, where each component in the chain processes and transforms data incrementally.\n\nThe active community around Langchain is constantly contributing new components and integrations. This growing ecosystem offers out\\-of\\-the\\-box solutions for a variety of use cases, from content generation and document analysis to the creation of sophisticated virtual assistants.\n\nAs far as performance is concerned, Langchain demonstrates remarkable efficiency in resource management. The framework implements intelligent caching and API call optimization mechanisms, significantly reducing operational costs and response times.\n\n\n## 3\\. CrewAI: Intelligent Collaboration between Agents\n\nCrewAI introduces an innovative paradigm based on collaboration between specialized agents. This framework stands out for its ability to organize agents into functional teams, where each member contributes specific skills to achieving common goals.\n\nCrewAIâ€™s hierarchical structure facilitates the efficient management of interactions between agents. The framework implements a sophisticated task delegation system, where each agent can assign specific tasks to other team members based on their skills.\n\nInter\\-agent communication in CrewAI is based on an advanced protocol that allows for structured and contextualized information exchanges. Agents can share knowledge, intermediate results, and feedback in real time, creating a dynamic and adaptive collaboration environment.\n\nA particularly innovative aspect is the dynamic role system. Agents can take on different responsibilities depending on the context and needs of the project. This flexibility allows you to optimize resource utilization and maximize the efficiency of your virtual team.\n\nConflict management and problem resolution are addressed through a sophisticated distributed consensus mechanism. Agents can negotiate solutions, propose alternatives and reach shared decisions independently.\n\nThe future potential of CrewAI is particularly promising in the field of business process automation. The framework is evolving to include:\n\n* Collaborative learning between agents\n* Automatic team optimization\n* Dynamic scaling of resources\n* Advanced integration with external systems\n\n\n## 4\\. AutoGen and Swarm: Innovations in Agent Creation\n\nAutoGen stands out for its revolutionary approach to the automatic generation of multi\\-agent systems. The framework excels at creating modular architectures that can evolve and adapt autonomously to the specific needs of projects.\n\nThe distinguishing feature of AutoGen lies in its ability to self\\-optimize. Generated agents can:\n\n* Change your behavior based on feedback received\n* Automatically optimize configuration parameters\n* Generate functional code for new features\n* Implement adaptive problem\\-solving strategies\n\nSwarm, on the other hand, focuses on lightness and efficiency in agent orchestration. Its minimalist approach offers significant advantages in terms of:\n\n* Optimized resource consumption\n* Superior execution speed\n* Scale\\-out simplified\n* Maintainability of the system\n\nThe direct comparison between these frameworks reveals interesting complementarities. While AutoGen shines at autonomous generation of complex solutions, Swarm excels at efficient management of large groups of simple agents.\n\n\n## Final Thoughts\n\nThe comparative overview presented shows that the intelligent agents sector is undergoing a phase of extraordinary innovation. Each framework analyzed brings a unique value to the AI ecosystem, helping to shape the future of intelligent automation.\n\nKey thoughts for industry professionals:\n\n1. The diversification of the available tools should not be seen as an obstacle, but as an opportunity for specialization and continuous innovation.\n2. The investment in the in\\-depth understanding of these frameworks represents a competitive advantage in the tech job market.\n3. Flexibility in the adoption of different solutions remains crucial for the success of enterprise\\-level projects.\n\nAs a lead data scientist, I recommend to:\n\n* Maintain a pragmatic approach in instrument selection\n* Favor solutions that guarantee scalability and maintainability\n* Invest in the continuous training of the team\n* Constantly monitor the technological evolutions of the sector\n\nThe future of smart agents looks promising, with a clear trend towards:\n\n* Increasingly sophisticated hybrid systems\n* Seamless integration between different platforms\n* Advanced automation of decision\\-making processes\n* Customization push of solutions\n\nThe key to success will lie in the ability to orchestrate these tools effectively, creating solutions that not only solve current problems, but are also ready for future challenges.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/introducing-atomic-agents-1-0-a-modular-framework-for-building-agentic-ai-with-cli-support-2b01b7165ace","frontmatter":{"title":"Introducing Atomic Agents 1.0: A Modular Framework for Building Agentic AI","meta_title":"Introducing Atomic Agents 1.0: A Modular Framework for Building Agentic AI","description":"Imagine building AI applications as effortlessly as assembling LEGO blocks. Thatâ€™s the idea behind Atomic Agents, a modular framework forâ€¦","date":"2024-11-08T00:19:37.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*BZGf8BCnCJiFlKZ5.png","categories":["Programming","Machine Learning","Autonomous Systems"],"author":"Rifx.Online","tags":["modular","framework","Atomic","assembler","schema"],"draft":false,"slug":"blog/introducing-atomic-agents-1-0-a-modular-framework-for-building-agentic-ai-with-cli-support-2b01b7165ace"},"content":"\n\n\nImagine building AI applications as effortlessly as assembling LEGO blocks. Thatâ€™s the idea behind [Atomic Agents](https://github.com/BrainBlend-AI/atomic-agents), a modular framework for constructing AI agents inspired by **Atomic Design** principles. With the release of **version 1\\.0**, Atomic Agents introduces a powerful CLI called **Atomic Assembler**, making it even easier to build, manage, and deploy your AI applications.\n\n## Why Atomic Agents?\n\nMany existing frameworks for **Agentic AI** focus on building autonomous multi\\-agent systems that are more like curiosities than practical tools. While these can be fascinating, they often lack the predictability and control required for real\\-world applications.\n\nBusinesses typically arenâ€™t looking for a bot that writes articles in a different style each time. They want consistency in style, structure, and tone to align with their brand identity. Fine\\-tuning a model is one approach, but it requires substantial data and resources, and itâ€™s not always feasible with the latest models like GPT\\-4\\.\n\nAtomic Agents aims to solve this by providing:\n\n* **Modularity**: Build complex AI systems by combining simple, interchangeable components.\n* **Atomicity**: Each component within Atomic Agents, each tool, each agent, each context provider, is as single\\-purpose and re\\-usable as possible, Guaranteeing a great separation of concerns.\n* **Control**: Fine\\-tune each individual step and component, from system prompts to tools.\n* **Predictability**: Ensure reproducible and reliable outputs suitable for business use cases.\n* **Extensibility**: Easily add or replace components without overhauling the entire system.\n\n## A Traditional Modular Approach\n\nIn traditional software development, complex problems are broken down into smaller, manageable parts:\n\n1. **Define the problem**: Start with flows, user stories, or customer journeys.\n2. **Break it down**: Divide the problem into smaller, solvable tasks.\n3. **Develop modular code**: Write functions or classes that handle specific tasks.\n4. **Integrate**: Combine these modules to form the complete application.\n\nAtomic Agents brings this same level of modularity and predictability to AI agent development.\n\n## A Real\\-World Scenario\n\nInstead of building a monolithic AI system that â€œwrites a blog post,â€ we can design a modular system that:\n\n1. **Generates queries** related to a subject.\n2. **Identifies** the top X most relevant articles.\n3. **Visits** each identified articleâ€™s page.\n4. **Extracts** the text from each article.\n5. **Generates summaries** of each article.\n6. **Stores** the summaries in a vector database.\n7. **Generates questions** around the subject.\n8. **Answers** those questions using the vector database.\n9. **Synthesizes** the answers into a coherent blog post.\n\nThis approach is more verbose but offers greater control, reliability, and suitability for real\\-world business applications.\n\n## Introduction of the CLI: Atomic Assembler\n\nOne of the significant additions in version 1\\.0 is the **Atomic Assembler** CLI. This command\\-line tool allows you to:\n\n* **Download and manage tools**: Easily add new tools or agents to your project.\n* **Avoid unnecessary dependencies**: Install only what you need.\n* **Modify tools effortlessly**: Each tool comes with its own tests and documentation.\n* **Access tools directly**: If you prefer, manage tools manually without the CLI.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*aDceAIINxyFDOvle.png)\n\n## Anatomy of an Agent\n\nAI agents, especially in the Atomic Agents framework, consist of several key components:\n\n* **System Prompt**: Defines the agentâ€™s behavior and purpose.\n* **User Input**: The data provided by the user.\n* **Tools**: External functions or APIs the agent can utilize.\n* **Memory**: Keeps track of the conversation or state.\n\nEach component is designed to be modular and interchangeable, adhering to the principles of separation of concerns and single responsibility.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*yt-5SoQC6uXTAd1-)\n\n## The Power of Modularity\n\nBy breaking down agents into these atomic components, you can:\n\n* **Swap out tools** without affecting the rest of the system.\n* **Fine\\-tune prompts** to adjust the agentâ€™s behavior.\n* **Chain agents and tools** seamlessly by matching their input and output schemas.\n\n## Using the CLI: Atomic Assembler\n\n## Installation\n\nTo get started with Atomic Agents and the CLI, install the package via pip:\n\n```python\npip install atomic-agents\n```\n\n## Running the CLI\n\nLaunch the CLI using:\n\n```python\natomic\n```\n\nOr, if you installed Atomic Agents with Poetry:\n\n```python\npoetry run atomic\n```\n\nYouâ€™ll be presented with a menu to download and manage tools:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*SzRlpA0-ivcE2qhk)\n\n*Image: Atomic CLI Main Menu*\n\nEach tool includes:\n\n* **Input Schema**\n* **Output Schema**\n* **Usage Examples**\n* **Dependencies**\n* **Installation Instructions**\n\n## Managing Tools\n\nThe Atomic Assembler CLI provides complete control over your tools, allowing you to:\n\n* **Avoid dependency clutter**: Install only the tools you need.\n* **Modify tools easily**: Each tool is self\\-contained with its own tests.\n* **Access tools directly**: Manage tool folders manually if you prefer.\n\n## Context Providers\n\nAtomic Agents introduces **Context Providers** to enhance your agents with dynamic context. Context Providers allow you to inject additional information into the agentâ€™s system prompt at runtime.\n\n## Using Context Providers\n\n**Create a Context Provider Class**: Subclass `SystemPromptContextProviderBase` and implement the `get_info()` method.\n\n```python\nfrom atomic_agents.lib.components.system_prompt_generator import SystemPromptContextProviderBase   \n\nclass SearchResultsProvider(SystemPromptContextProviderBase):\n      def __init__(self, title: str, search_results: List[str]):\n          super().__init__(title=title)\n          self.search_results = search_results\n\n       def get_info(self) -> str:\n          return \"\\n\".join(self.search_results)\n```\n\n**Register the Context Provider with the Agent**:\n\n```python\n## Initialize your context provider with dynamic data\nsearch_results_provider = SearchResultsProvider(\n      title=\"Search Results\",\n      search_results=[\"Result 1\", \"Result 2\", \"Result 3\"]\n)   \n\n## Register the context provider with the agent  \nagent.register_context_provider(\"search_results\", search_results_provider)\n```\n\nThis allows your agent to include dynamic data like search results in its system prompt, enhancing its responses based on the latest information.\n\n## Chaining Schemas and Agents\n\nAtomic Agents simplifies chaining agents and tools by aligning their input and output schemas. This design promotes modularity and reusability.\n\n### Example: Generating Queries for Different Search Providers\n\nSuppose you have an agent that generates search queries and you want to use these queries with different search tools. By aligning the agentâ€™s output schema with the input schema of the search tool, you can easily chain them or switch between providers.\n\n```python\nimport instructor\nimport openai\nfrom pydantic import Field\nfrom atomic_agents.agents.base_agent import BaseIOSchema, BaseAgent, BaseAgentConfig\nfrom atomic_agents.lib.components.system_prompt_generator import SystemPromptGenerator\n\n## Import the search tool\nfrom web_search_agent.tools.searxng_search import SearxNGSearchTool\nclass QueryAgentInputSchema(BaseIOSchema):\n    \"\"\"Input schema for the QueryAgent.\"\"\"\n    instruction: str = Field(..., description=\"Instruction to generate search queries for.\")\n    num_queries: int = Field(..., description=\"Number of queries to generate.\")\n\n\n## Initialize the query agent\nquery_agent = BaseAgent(\n    BaseAgentConfig(\n        client=instructor.from_openai(openai.OpenAI()),\n        model=\"gpt-4\",\n        system_prompt_generator=SystemPromptGenerator(\n            background=[\n                \"You are an intelligent query generation expert.\",\n                \"Your task is to generate diverse and relevant queries based on a given instruction.\"\n            ],\n            steps=[\n                \"Receive the instruction and the number of queries.\",\n                \"Generate the queries in JSON format.\"\n            ],\n            output_instructions=[\n                \"Ensure each query is unique and relevant.\",\n                \"Provide the queries in the expected schema.\"\n            ],\n        ),\n        input_schema=QueryAgentInputSchema,\n        output_schema=SearxNGSearchTool.input_schema,  # Align output schema\n    )\n)\n```\n\n**Modularity**: By setting the `output_schema` of the `query_agent` to match the `input_schema` of `SearxNGSearchTool`, you can directly use the output of the agent as input to the tool.\n\n**Swapability**: To switch to a different search provider, import a different search tool and update the `output_schema`:\n\n```python\n## Import a different search tool\nfrom web_search_agent.tools.another_search import AnotherSearchTool\n\n## Update the output schema\nquery_agent.config.output_schema = AnotherSearchTool.input_schema\n```\n\n## Example: Building a Simple AI Agent\n\nNow that weâ€™ve covered the basics, letâ€™s build a simple AI agent using Atomic Agents and explore how it works under the hood.\n\n## Step 1: Installation\n\nFirst, install the necessary packages:\n\n```python\npip install atomic-agents openai instructor\n```\n\n## Step 2: Import Components\n\nImport the necessary components:\n\n```python\nimport os\nfrom atomic_agents.agents.base_agent import BaseAgent, BaseAgentConfig, BaseIOSchema\nfrom atomic_agents.lib.components.system_prompt_generator import SystemPromptGenerator\nfrom atomic_agents.lib.components.agent_memory import AgentMemory\nfrom pydantic import Field\nimport instructor\nimport openai\n```\n\n## Step 3: Define a Custom Output Schema\n\n```python\nclass CustomOutputSchema(BaseIOSchema):\n    chat_message: str = Field(..., description=\"The chat message from the agent.\")\n    suggested_questions: List[str] = Field(..., description=\"Suggested follow-up questions.\")\n```\n\n## Step 4: Set Up the System Prompt\n\n```python\nsystem_prompt_generator = SystemPromptGenerator(\n    background=[\"This assistant is knowledgeable, helpful, and suggests follow-up questions.\"],\n    steps=[\n        \"Analyze the user's input to understand the context and intent.\",\n        \"Formulate a relevant and informative response.\",\n        \"Generate 3 suggested follow-up questions for the user.\"\n    ],\n    output_instructions=[\n        \"Provide clear and concise information in response to user queries.\",\n        \"Conclude each response with 3 relevant suggested questions for the user.\"\n    ]\n)\n```\n\n## Step 5: Initialize the Agent\n\n```python\n## Initialize memory (optional)\nmemory = AgentMemory()\n\n## Initialize the agent\nagent = BaseAgent(\n    config=BaseAgentConfig(\n        client=instructor.from_openai(openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))),\n        model=\"gpt-4o-mini\",\n        system_prompt_generator=system_prompt_generator,\n        memory=memory,\n        output_schema=CustomOutputSchema\n    )\n)\n```\n\n## Step 6: Use the Agent\n\n```python\nuser_input = \"Can you explain the benefits of using Atomic Agents?\"\nresponse = agent.run(agent.input_schema(chat_message=user_input))\nprint(f\"Agent: {response.chat_message}\")\nprint(\"Suggested questions:\")\nfor question in response.suggested_questions:\n    print(f\"- {question}\")\n```\n\n## Whatâ€™s Happening Behind the Scenes?\n\n* **System Prompt**: Defines the agentâ€™s behavior and guides the LLM.\n* **Input Schema**: Validates the userâ€™s input.\n* **Output Schema**: Ensures the agentâ€™s response matches the expected format.\n* **Memory**: Keeps track of the conversation history.\n\n## Conclusion\n\nAtomic Agents 1\\.0 brings modularity, control, and flexibility to AI agent development. With the introduction of the Atomic Assembler CLI and features like Context Providers and schema chaining, building sophisticated AI applications has never been easier.\n\nWhether youâ€™re a developer aiming to build AI\\-powered tools or a business looking to automate complex tasks, Atomic Agents provides the building blocks to create reliable and maintainable AI systems.\n\n## Get Started Today\n\n* **GitHub Repository**: [BrainBlend\\-AI/atomic\\-agents](https://github.com/BrainBlend-AI/atomic-agents)\n* **API Documentation**: [Atomic Agents API Docs](https://brainblend-ai.github.io/atomic-agents/)\n* **Examples Directory**: [Atomic Examples](https://github.com/BrainBlend-AI/atomic-agents/tree/main/atomic-examples)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/introducing-microsofts-magentic-one-agentic-framework-7dcc16de691e","frontmatter":{"title":"Introducing Microsoftâ€™s Magentic-One Agentic Framework","meta_title":"Introducing Microsoftâ€™s Magentic-One Agentic Framework","description":"Microsoft has introduced the Magentic-One agentic framework, a multi-agent system designed to tackle complex tasks. It features an orchestrator agent that coordinates specialized agents for web browsing, file management, coding, and terminal operations. Built on the Autogen framework, it allows for various applications, including executing Python code, conducting web searches, and interacting with local files. However, the framework also poses risks, such as unintended actions and potential irreversible consequences, highlighting the need for careful deployment and oversight.","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*dJj20_4jYYp32Crl","categories":["Programming","Autonomous Systems","Technology/Web"],"author":"Rifx.Online","tags":["Magnetic-One","orchestrator","agents","Autogen","Python"],"draft":false,"slug":"blog/introducing-microsofts-magentic-one-agentic-framework-7dcc16de691e"},"content":"\n\n\n\n\n### A multi\\-agent system that can perform complex tasks\n\nAround a week ago, Microsoft released a new agentic system called **Magentic\\-One** â€œfor solving complex tasks,â€ which seems to have gone completely under the radar. With all the recent buzz around Anthropicâ€™s computer use capabilities, Microsoft seems keen to re\\-establish its credentials in this area.\n\nIn this article, weâ€™ll introduce Magentic\\-One, explain its capabilities, and discuss how to use it to do useful work.\n\n\n\nAccording to Microsoftâ€™s own announcement (link at end of article), Magentic\\-One is â€¦\n\nâ€œ... a high\\-performing generalist agentic system designed to solve such tasks. Magentic\\-One employs a multi\\-agent architecture where a lead agent, the Orchestrator, directs four other agents to solve tasks. The Orchestrator plans, tracks progress, and re\\-plans to recover from errors, while directing specialized agents to perform tasks like operating a web browser, navigating local files, or writing and executing Python code.â€\n\nMagentic\\-One is built on top of Microsoftâ€™s existing **Autogen** product, which is its open\\-source multi\\-agent framework.\n\nMagentic\\-One has five key components.\n\n**1/ The orchestrator agent**\n\nResponsible for task decomposition and planning and directs sub\\-tasks to the other agents for execution. Tracks the progress towards task completion and takes corrective actions as required.\n\n**2/ The web surfer agent**\n\nSpecialises in controlling and managing the state of a Chromium\\-based web browser. For each incoming request, WebSurfer performs a designated action within the browser and then reports the updated state of the webpage. Its actions include:\n\n* **Navigation** (e.g., visiting URLs, performing web searches),\n* **Page Interactions** (e.g., clicking elements, typing inputs),\n* **Reading and Interpretation** (e.g., summarizing content, answering questions).\n\nWebSurfer utilizes the browserâ€™s accessibility tree and a set\\-of\\-marks prompting technique to effectively carry out its tasks.\n\n**3/ The file surfer agent**\n\nCan read most types of local files and also perform common navigation tasks such as listing the contents of directories and navigating a folder structure\n\n**4/ The coder agent**\n\nAn LLM\\-based agent specialized in writing code, analyzing information collected from the other agents, or creating new artefacts.\n\n**5/ The terminal agent**\n\nIt provides access to a console shell where the Coder agentâ€™s programs can be executed and where new programming libraries can be installed.\n\n\n### Risks\n\nBefore continuing, I wanted to highlight one particular aspect that Microsoft included in its announcement regarding the risks of using Agentic systems like these. It kind of makes you sit up and take notice.\n\n\n> Agentic systems like Magentic\\-One represent a phase transition in the opportunities and risks of having AI systems in the world. Magentic\\-One interacts with a digital world designed for, and inhabited by, humans. It can take actions, change the state of the world and result in consequences that might be irreversible. This carries inherent and undeniable risks and we observed examples of emerging risks during our testing. For example, during development, a misconfiguration prevented agents from successfully logging in to a particular WebArena website. The agents attempted to log in to that website until the repeated attempts caused the account to be temporarily suspended. The agents then attempted to reset the accountâ€™s password. More worryingly, in a handful of cases â€” and until prompted otherwise â€” the agents occasionally attempted to recruit other humans for help (e.g., by posting to social media, emailing textbook authors, or, in one case, drafting a freedom of information request to a government entity). In each of these cases, the agents failed because they did not have access to the requisite tools or accounts, and/or were stopped by human observers.\n\nOk, let's see some examples of how we can use Magentic\\-One to do some useful work. Hopefully, we wonâ€™t destroy the world in the process. ðŸ˜‰\n\n\n### Installing Magentic\\-One\n\nIâ€™m a Windows user, but I will install the code using WSL2 Ubuntu for Windows. if you want to follow along, I have a full guide on installing WSL2 Ubuntu [here](https://readmedium.com/installing-wsl2-ubuntu-for-windows-81122c551bc2).\n\nHead over to the Magentic\\-One GitHub repository by clicking [here](https://github.com/microsoft/autogen/tree/main/python/packages/autogen-magentic-one). Run the following commands on your local system (wherever you normally place the projects that you work on).\n\n\n```python\ngit clone https://github.com/microsoft/autogen.git\n\ncd autogen/python\n\nuv sync --all-extras\n\nsource .venv/bin/activate\n\ncd packages/autogen-magentic-one\n```\nNext, configure the environment variables for the chat completion client. Currently, Magentic\\-One only supports OpenAIâ€™s GPT\\-4o as the underlying LLM.\n\nYou can set this up via OpenAI or Azure Active Directory. Here are the instructions for using OpenAI.\n\n\n```python\nexport CHAT_COMPLETION_PROVIDER='openai'\n\nexport CHAT_COMPLETION_KWARGS_JSON='{\"api_key\": \"gpt-4o\"}'\n```\n\n> **One important point to note is that if you have a GitHub account, you can use the GPT4\\-o model from GitHub Models, which will give you FREE access to GPT4â€“o. However the usage limits can be a bit restrictive.**\n\nTo go down using the GitHub Models route, click [here](https://github.com/marketplace/models) and log in with your GitHub account or create an account if you donâ€™t already have one. Click on the GPT\\-4o button. On the page that displays, near the top right, there will be a green `Get API Key` button. Click on that, then from there, click the `Get Developer Key` button.\n\nFinally, you should see a screen where you can generate a classic Personal Access Token. So, do that now. Youâ€™ll need to enter a note describing what the key is for, but **you do not** have to give it any additional permissions. Take note of the generated key.\n\nTo use the GitHub GPT4\\-o model, change your environment variables as follows:\n\n\n```python\nexport CHAT_COMPLETION_PROVIDER='openai'\n\nexport CHAT_COMPLETION_KWARGS_JSON='{\"base_url\": \"https://models.inference.ai.azure.com\", \"api_key\": \"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\", \"model\": \"gpt-4o\"}'\n```\nBefore running some example code, we must install two final dependencies.\n\nMagentic\\-One uses **Playwright** to allow it to interact with webpages, so you must install the Playwright dependencies.\n\n\n```python\nplaywright install --with-deps chromium\n```\nTo allow Magentic\\-One to run Python code, we need to install and run docker. Check out [this link](https://docs.docker.com/engine/install/) on how to do that.\n\nEventually, I was able to take Magentic\\-One for a spin.\n\n**Example 1 â€” writing some Python code.**\n\n\n```python\n(base) tom@tpr-desktop:~/projects/autogen/python/packages$ python examples/example --logs_dir ./logs\n/home/tom/projects/autogen/python/.venv/lib/python3.11/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\nUser input ('exit' to quit):  Write a Python program to calculate and display \nthe first 5 fibonacci numbers\n```\nThere was a whole bunch of output displayed, but after a few seconds, Magentic\\-One asked me if I wanted to run the Python code it had created, to which I said yes.\n\n\n```python\n...\n...\n\nExecutor is about to execute code (lang: python):\n## filename: fibonacci.py\ndef fibonacci_sequence(n):\n    fib_numbers = [0, 1]\n    for i in range(2, n):\n        next_value = fib_numbers[i - 1] + fib_numbers[i - 2]\n        fib_numbers.append(next_value)\n    return fib_numbers\n\nfirst_five_fib = fibonacci_sequence(5)\nprint(\"The first 5 Fibonacci numbers are:\", first_five_fib)\n\nDo you want to proceed? (yes/no): yes\n\n---------------------------------------------------------------------------\n[2024-11-10T13:25:40.508594], Executor:\n\nThe script ran, then exited with Unix exit code: 0\nIts output was:\nThe first 5 Fibonacci numbers are: [0, 1, 1, 2, 3]\n...\n...\n```\n**Example 2 â€” Searching the web**\n\nTo search the web using Magentic, you need a Bing API key. You can set this up via Microsoft Azure (Bing Search V7\\).\n\nItâ€™s possible to arrange it to be a cost\\-free option if you go for the lowest available **â€œFâ€** tier. However, this restricts the number of searches per second to 3 and caps the total search calls per month.\n\nItâ€™s a little involved to set this up, but basically, youâ€™ll want to follow these steps,\n\n* Sign up for a free Microsoft Azure account if you donâ€™t have one\n* Create a Bing Search resource in the Azure portal; ensure you go for the lowest F tier, which is free but a little restricted, as described above.\n* Obtain your API key from the resource overview\n\nOnce you have your Bing API key, assign its value to the BING\\_API\\_KEY environment variable.\n\n\n```python\n(base) tom@tpr-desktop:~/projects/autogen/python/packages$ python examples/example --logs_dir ./logs\n/home/tom/projects/autogen/python/.venv/lib/python3.11/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\nUser input ('exit' to quit):  search the web and find the current weather \nforecast for Edinburgh UK\n```\nAgain, there was a lot of output, some of the more notable stuff is shown below.\n\n\n```python\n...\n...\nInitial plan:\n\nWe are working to address the following user request:\n\nsearch the web and find the current weather forecast for Edinburgh UK\n\n\nTo answer this request we have assembled the following team:\n\nWebSurfer: A helpful assistant with access to a web browser. Ask them to perform web searches, open pages, and interact with content (e.g., clicking links, scrolling the viewport, etc., filling in form fields, etc.) It can also summarize the entire page, or answer questions based on the content of the page. It can also be asked to sleep and wait for pages to load, in cases where the pages seem to be taking a while to load.\nCoder: A helpful and general-purpose AI assistant that has strong language skills, Python skills, and Linux command line skills.\nExecutor: A agent for executing code\nfile_surfer: An agent that can handle local files.\n\nHere is an initial fact sheet to consider:\n\n1. GIVEN OR VERIFIED FACTS\n   - The request is asking for the current weather forecast for Edinburgh, UK.\n\n2. FACTS TO LOOK UP\n   - The current weather forecast for Edinburgh, UK can be found on various weather websites such as the BBC Weather, Met Office, or Weather.com.\n\n3. FACTS TO DERIVE\n   - N/A\n\n4. EDUCATED GUESSES\n   - The current weather forecast will likely include details such as temperature, precipitation chance, wind speed, and potential weather warnings, which are typically part of a standard weather forecast.\n\n\nHere is the plan to follow as best as possible:\n\n- Request WebSurfer to search for the current weather forecast for Edinburgh, UK on a reliable weather website such as BBC Weather, Met Office, or Weather.com.\n- Instruct WebSurfer to summarize the weather forecast details including temperature, precipitation chance, wind speed, and any potential weather warnings.\n- Present the gathered weather information for Edinburgh, UK from WebSurfer.\n\n...\n...\n\nI typed 'Edinburgh UK current weather forecast' into the browser search bar.\n\nHere is a screenshot of [Edinburgh UK current weather forecast - Search](https://www.bing.com/search?q=Edinburgh+UK+current+weather+forecast&FORM=QBLH). The viewport shows 28% of the webpage, and is positioned at the top of the page.\nThe following metadata was extracted from the webpage:\n\n{\n    \"meta_tags\": {\n        \"referrer\": \"origin-when-cross-origin\",\n        \"og:description\": \"Intelligent search from Bing makes it easier to quickly find what you\\u2019re looking for and rewards you.\",\n        \"og:site_name\": \"Bing\",\n        \"og:title\": \"Edinburgh UK current weather forecast - Bing\",\n        \"og:url\": \"https://www.bing.com/search?q=Edinburgh+UK+current+weather+forecast&FORM=QBLH\",\n        \"fb:app_id\": \"3732605936979161\",\n        \"og:image\": \"http://www.bing.com/sa/simg/facebook_sharing_5.png\",\n        \"og:type\": \"website\",\n        \"og:image:width\": \"600\",\n        \"og:image:height\": \"315\"\n    }\n}\n\nAutomatic OCR of the page screenshot has detected the following text:\n\n**Page Content:**\n\nMicrosoft Bing\n\nSearch input field: Edinburgh UK current weather forecast\n\n**Menu:**\n- Search\n- Copilot\n- News\n- Images\n- Videos\n- Maps\n- Shopping\n- More\n- Tools\n\nDeep search\nSign in\nMobile\n\n**Weather Information:**\n\nAbout 3,180,000 results\n\nEdinburgh\nCapital city of Scotland, UK\n\nButtons:\n- Map\n- Things to do\n- Weather (Selected)\n- Covid-19\n- Flights\n- History\n- Travel guide\n\n**Weather Widget:**\n**Weather Details:**\n12Â°C / Â°F\n13Â°\n6Â°\nWind: 17 KMPH\nHumidity: 90%\nCloudy Â· Sun 10, 13:44\n\n**Hourly Forecast:**\n14:00  17:00  20:00  23:00  2:00  5:00  8:00  11:00\n\n**Weekly Forecast:**\n- Sun 10: 13Â°/6Â°\n- Mon 11: ðŸŒž 11Â°/2Â°\n- Tue 12: ðŸŒ§ 9Â°/5Â°\n- Wed 13: ðŸŒ¥ 12Â°/8Â°\n- Thu 14: ðŸŒ§ 10Â°/8Â°\n- Fri 15: ðŸŒ§ 11Â°/7Â°\n- Sat 16: ðŸŒ§ 10Â°/7Â°\n- Sun 17: ðŸŒ¥ 7Â°/2Â°\n\n**Sidebar Information:**\n\n- UV index: No forecast\n- Moderate breeze: 17 KMPH, WSW\n- Sunrise: 07:39 AM\n- Sunset: 04:12 PM\n...\n...\n```\nThe final answer was this, which was spot on.\n\n\n```python\n[2024-11-10T13:44:43.570437], Orchestrator (final answer):\n\n\nThe current weather in Edinburgh is 12Â°C with cloudy conditions. \nThere's a moderate breeze at 17 KMPH, and the humidity is at 90%. \nThe temperature is expected to range between 13Â°C and 6Â°C today.\n```\n**Example 3 â€” Clicking on website links**\n\nAs Iâ€™m writing this article, there is a big Rugby Union game taking place in the UK between Wales and Fiji. I wanted to know the current state of play in the Wales vs. Fiji match.\n\n\n```python\n(base) tom@tpr-desktop:~/projects/autogen/python/packages$ python examples/example --logs_dir ./logs\n/home/tom/projects/autogen/python/.venv/lib/python3.11/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\nUser input ('exit' to quit):  Click on the bbc.co.uk website, click on the \nSport link near the top of the page. Look for a link in the page that \ndisplays about the Wales v Fiji rugby match. Click on that link and tell me \nwhat the latest score is\n```\nAgain, Iâ€™m omitting much of the output to save on space.\n\n\n```python\n...\n...\n...\nAutomatic OCR of the page screenshot has detected the following text:\n\nSure, here is the transcribed text:\n\n---\n**BBC**\nSign in\nHome\nNews\nSport\nWeather\niPlayer\nSounds\nBitesize\nSport\n\nHome | Football | Cricket | Formula 1 | Rugby U | Rugby L | Tennis | Golf | Boxing | Athletics\n\nDiscover your BBC\nSign in or create an account to watch, listen, and join in\n\nSign in or Register\n\nRequest satisfied.\n...\n...\n...\n - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n[2024â€“11â€“10T13:55:10.606578], Orchestrator (final answer):\nThe latest score for the Wales vs. Fiji match, according to the BBC Sport \nwebsite, is Wales 7â€“0 Fiji with a try from Murray.\n - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n[2024â€“11â€“10T13:55:10.617212], Orchestrator (termination condition):\n```\nHere is a screenshot I took shortly after the model answered (Fiji must have scored very quickly after Walesâ€™ initial score)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*xN8qBJLdHqx0lrh_4Y4DCQ.png)\n\n**Example 4 â€” reading a local XL file.**\n\nI have an XL file on my local system. Letâ€™s see if Magentic\\-One can find it, open it, and answer a question about it.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*UIuLEkEr-w6ZckRjiUuqjw.png)\n\n\n```python\n(base) tom@tpr-desktop:~/projects/autogen/python/packages$ python examples/example --logs_dir ./logs\n/home/tom/projects/autogen/python/.venv/lib/python3.11/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\nUser input ('exit' to quit):  I have a file in my /mnt/d/data directorty called\nfake_data.xlsx. Can you tell me what is in the third record of the file\n```\n\n```python\n...\n...\n\nNext speaker file_surfer\n\n---------------------------------------------------------------------------\n[2024-11-10T14:16:57.676137], file_surfer:\n\nAddress: file:///mnt/d/data/fake_data.xlsx\nViewport position: Showing page 1 of 1.\n=======================\n### Sheet1\n| Date | Sales | Expenses |\n| --- | --- | --- |\n| 2024\\-01\\-31 | 302 | 187 |\n| 2024\\-02\\-29 | 635 | 472 |\n| 2024\\-03\\-31 | 470 | 199 |\n| 2024\\-04\\-30 | 306 | 459 |\n| 2024\\-05\\-31 | 271 | 251 |\n| 2024\\-06\\-30 | 900 | 230 |\n| 2024\\-07\\-31 | 220 | 249 |\n| 2024\\-08\\-31 | 814 | 408 |\n| 2024\\-09\\-30 | 321 | 357 |\n| 2024\\-10\\-31 | 666 | 443 |\n| 2024\\-11\\-30 | 414 | 393 |\n| 2024\\-12\\-31 | 530 | 485 |\n\n---------------------------------------------------------------------------\n[2024-11-10T14:17:00.613740], Orchestrator (thought):\n\nUpdated Ledger:\n{\n  \"is_request_satisfied\": {\n    \"reason\": \"The contents of the third record in the file 'fake_data.xlsx' have been successfully retrieved and displayed.\",\n    \"answer\": true\n  },\n  \"is_in_loop\": {\n    \"reason\": \"The task was completed in a straightforward manner without any repeated actions.\",\n    \"answer\": false\n  },\n  \"is_progress_being_made\": {\n    \"reason\": \"The content of the third record was successfully retrieved and displayed, indicating successful progress.\",\n    \"answer\": true\n  },\n  \"next_speaker\": {\n    \"reason\": \"The task is complete, so no further action is needed.\",\n    \"answer\": \"file_surfer\"\n  },\n  \"instruction_or_question\": {\n    \"reason\": \"The request has been satisfied by providing the contents of the third record.\",\n    \"answer\": \"The third record in the file is: Date: 2024-03-31, Sales: 470, Expenses: 199.\"\n  }\n}\n\n---------------------------------------------------------------------------\n[2024-11-10T14:17:00.613806], Orchestrator (thought):\n\nRequest satisfied.\n\n---------------------------------------------------------------------------\n[2024-11-10T14:17:01.465848], Orchestrator (final answer):\n\n\nThe third record in your file \"fake_data.xlsx\" contains the following information:\n\n- Date: 2024-03-31\n- Sales: 470\n- Expenses: 199\n\n---------------------------------------------------------------------------\n[2024-11-10T14:17:01.465908], Orchestrator (termination condition):\n\nNo agent selected.\n(base) tom@tpr-desktop:~/projects/autogen/python/packages$\n```\nI liked the fact that the agent determined that the first record was a header, so it returned the actual third proper data record. Thatâ€™s quite something.\n\n\n### Summary\n\nWell, I donâ€™t know about you, but I thought that was a set of pretty impressive demonstrations. Microsoft has produced a really good agentic system and seems intent on incorporating it fully into their Autogen framework in the near future.\n\nIn this article, I explained what Magentic\\-One was and how to download it and run it to do some useful tasks. I explained that its key components were\n\n* orchestration\n* web and file surfing\n* coding and terminal operations\n\nI showed each of these components at work through a series of examples, including\n\n* the creation and running of Python code\n* examing a local file and answering questions on its content\n* searching for information on the web\n* clicking web links\n\n\n> *OK, thatâ€™s all for me for now. Hopefully, you found this article useful. If you did, please check out my profile page at [this link](https://medium.com/@thomas_reid). From there, you can see my other published stories and subscribe to get notified when I post new content.*\n\n\n> *Times are tough and wallets constrained, but if you got real value from this article, please consider [buying me a wee dram](https://ko-fi.com/taupirho).*\n\nIf you liked this content, I think youâ€™ll also find these related articles interesting.\n\nRead the full Magentic\\-One announcement from Microsoft [here](https://www.microsoft.com/en-us/research/articles/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/).\n\n\n"},{"lang":"en","group":"blog","slug":"blog/introduction-to-llava-a-multimodal-ai-model-2a2fa530ace4","frontmatter":{"title":"Introduction to LLaVA: A Multimodal AI Model","meta_title":"Introduction to LLaVA: A Multimodal AI Model","description":"LLaVA is an end-to-end trained large multimodal model that is designed to understand and generate content based on both visual inputsâ€¦","date":"2024-10-29T12:48:10.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*0At7tXF5ejho9Y46E3uGtg.png","categories":["Natural Language Processing","Computer Vision","Generative AI"],"author":"Rifx.Online","tags":["LLaVA","GPT-4","multimodal","visual","encoder"],"draft":false,"slug":"blog/introduction-to-llava-a-multimodal-ai-model-2a2fa530ace4"},"content":"\n\n\n\n\n\nLLaVA is an end\\-to\\-end trained large multimodal model that is designed to understand and generate content based on both visual inputs (images) and textual instructions. It combines the capabilities of a visual encoder and a language model to process and respond to multimodal inputs.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*mjzqL0BHzdPoN-Jjruh52A.png)\n\n\n## Inputs and Outputs of LLaVA: Bridging Visual and Textual Domains:\n\nThe inputs to LLaVA are twofold:\n\n1. Visual Input: Images that the model can view and analyze to extract visual features and contextual information.\n2. Textual Instructions: Text inputs, which can be questions or commands, that guide the model on what to focus on or what kind of task to perform regarding the visual input.\n\nThe outputs of LLaVA are text\\-based and can vary depending on the task:\n\n1. Descriptive Text: If the task is to describe the visual content, LLaVA can output a detailed description of the image, identifying objects, actions, and scenes.\n2. Answers to Questions: For question\\-answering tasks, LLaVA generates responses that answer questions about the visual input, potentially involving reasoning and inference based on the imageâ€™s content.\n3. Follow\\-up Actions: For instructions that require action, such as editing an image or retrieving more information, LLaVA can provide appropriate textual responses indicating the action taken or suggesting what should be done.\n\n\n## Comparative Analysis: LLaVa vs. Contemporary Multimodal Models\n\nThe landscape of multimodal AI has been rapidly evolving with innovations such as CLIP, BLIP, and the recent introduction of LLaVa. This subsection compares LLaVaâ€™s unique architecture and approach with these contemporary models, highlighting the advancements and distinctions that set it apart.\n\n\n### CLIP: Pioneering Multimodal Understanding\n\nCLIP (Contrastive Languageâ€“Image Pre\\-training) has been a revolutionary step forward in multimodal AI, offering robust performance across a variety of visual tasks. Its ability to understand images in the context of natural language descriptions set a new benchmark in the field. CLIP achieves this through a large\\-scale pretraining approach that aligns images with textual descriptions, enabling the model to perform zero\\-shot learning on a range of visual tasks. However, CLIP primarily focuses on the association between images and text at a high level and does not inherently possess the capability for in\\-depth reasoning or conversational engagement.\n\n\n### BLIP: Bridging Language and Image Perception\n\nBuilding upon the foundation laid by CLIP, BLIP (Bootstrapped Language Image Pre\\-training) extends the capabilities of multimodal models by incorporating a bootstrapped pretraining strategy. This approach refines the modelâ€™s visual understanding by continually learning from its own predictions, which helps to improve the alignment between language and visual content. BLIP demonstrates enhanced performance on tasks that require more precise visual recognition and language understanding.\n\nIn contrast, LLaVa takes a different route by leveraging the language\\-generating capabilities of GPT\\-4 to curate its instruction\\-following data. This not only results in a dataset that captures a broader range of human\\-like interactions but also enables LLaVa to engage in more complex reasoning and in\\-depth conversational abilities.\n\n\n## What Sets LLaVa Apart: Is It the Model Architecture or Something Else?\n\nAccording to us , LLaVAâ€™s strength lies predominantly in its data curation capabilities rather than its architectural choice. LLaVA marks a significant leap forward , primarily due to its utilization of GPT\\-4 for data curation. Unlike conventional static datasets, LLaVA generates dynamic, instructive data using ChatGPT\\-4, actively involving data in the training process across various visual and textual scenarios.\n\nBy using GPT\\-4, LLaVA produces datasets that closely mimic natural language and visual perception, departing from traditional manual dataset generation methods. This innovative approach not only enables AI to understand and reason but also moves it closer to accurately reflecting human intelligence.\n\n\n### Data Curation Strategies in LLaVa\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*LzastWLkzPeMB_28Nr7Y9A.png)\n\nLLaVa, the Large Language and Vision Assistant, stands out not just for its advanced neural architecture but for its groundbreaking approach to data curation. By leveraging GPT\\-4, it revolutionizes traditional data preparation methods, crafting a dataset that mirrors the complexity of the real world.\n\nData curation in LLaVa begins with an image and its corresponding caption, from which a set of queries is generated using GPT\\-4\\. These queries guide the AI to explore and describe the image content with precision and relevance.\n\nTo translate visual data effectively for a text\\-based AI like GPT\\-4, LLaVa uses captions to offer diverse perspectives of the visual scene and bounding boxes to provide spatial context and focus.\n\n1. Conversational Data: Mimicing human interaction, LLaVa curates dialogues where the model, playing the assistant, responds to questions about various aspects of the image. The scope of these questions ranges from identifying objects and actions to discerning their numbers, locations, and relative positions, ensuring the model can handle queries with definitive answers.\n2. Detailed Descriptive Data: LLaVa seeks to comprehend the images in a comprehensive manner. To achieve this, it prompts GPT\\-4 to formulate questions aimed at understanding rich and detailed descriptions of the images. These prompts encourage the model to delve deeper, providing a narrative that captures the essence of the visual content in its entirety.\n3. Complex Reasoning Data: Moving beyond mere description, LLaVa challenges the model with questions that necessitate a layered reasoning process, demanding logic and an understanding of cause and effect. This type of data trains the model to construct well\\-reasoned responses that are backed by a logical sequence of thought.\n\n\n## The Architecture of LLaVa: Integrating Vision and Language\n\nThe LLaVa model integrates vision and language, utilizing the following core components:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8q_Iay_LHCzPqtrQby_H8w.png)\n\n1. Vision Encoder: At the foundation of LLaVaâ€™s architecture is the pre\\-trained CLIP visual encoder, specifically the ViT\\-L/14 variant. This component processes input images (Xv) through Transformer layers to extract features (Zv), enabling the model to understand visual information effectively.\n2. Language Model (Vicuna): LLaVaâ€™s linguistic capabilities rely on Vicuna, a variant of a large language model (LLM) denoted by fÏ• . Vicuna comprehends and generates language responses (Xa) based on input language instructions (Xq), complementing the vision encoderâ€™s functionality.\n3. Linear Projection: This component, represented by a trainable matrix (W), serves as the bridge between visual features (Zv) and the language modelâ€™s embedding space. It transforms visual features into visual tokens (Hv), aligning them with the language modelâ€™s word embedding space to facilitate multimodal conversation\n\n\n## Training and Fine\\-Tuning LLaVA:\n\nLLaVAâ€™s has a two\\-stage training process, each stage focusing on refining the modelâ€™s capabilities to interpret and respond to a fusion of visual and textual data.\n\n\n### Stage 1: Pre\\-training for Feature Alignment\n\nThe initial stage of LLaVAâ€™s training is pre\\-training for feature alignment. In this phase, the model focuses on aligning visual features from images with the corresponding textual features from the language model. This is achieved by filtering a large dataset to a refined set of image\\-text pairs, which LLaVA uses to learn the correlations between the two modalities.\n\nDuring this stage, a visual encoder (such as the CLIP visual encoder ViT\\-L/14\\) processes the images to extract visual features, and a projection matrix (W) is then used to map these features into the word embedding space of the language model. The language model used in LLaVA is Vicuna, known for its strong language understanding and generation capabilities.\n\n\n### Stage 2: Fine\\-tuning End\\-to\\-End\n\nAfter aligning the visual and language features, LLaVA undergoes an end\\-to\\-end fine\\-tuning process. Despite keeping the visual encoderâ€™s weights frozen, this stage allows the model to fine\\-tune the weights of the projection matrix and language model jointly. The objective is to maximize the likelihood of the target answers based on the multimodal data provided.\n\nThis stage is critical for adapting LLaVA to specific use case scenarios such as multimodal chat, scientific Q\\&A, and more. It ensures that the model does not just understand images in the context of generic descriptions but can engage in complex dialogues, provide detailed explanations, and reason through problems when prompted with specific questions related to the images.\n\n\n## Performance and Benchmarking: LLaVa in the Context of VQA Models\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*I_5fTa_2rtNHEDUaDNMXbQ.png)\n\n\n## LLaVA\\-Bench (COCO) Performance Insights\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*6B2K7EcbYgMbH-QEp8J41w.png)\n\nLLaVA\\-Bench (COCO) provides a robust framework for assessing LLaVAâ€™s capabilities through a carefully crafted set of 90 questions, derived from 30 selected images for conversation, detailed description, and complex reasoning. The results were as follows:\n\n* Instruction Tuning Efficacy: When equipped with instruction tuning, LLaVAâ€™s compliance with user commands improved by over 50 points.\n* Impact of Question Variety: The inclusion of detailed and complex reasoning questions, though minimal, led to a 7\\-point increase in overall capabilities. This boost also had a positive effect on conversational question responses, showcasing the benefits of a diverse training set.\n* Optimal Data Mix: The combination of all three question types resulted in the highest performance leap, with LLaVA reaching a benchmark score of 85\\.1%, emphasizing the strength of a comprehensive dataset in enhancing multimodal AI proficiency.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*mCjP0xfpcjHkl-lu)\n\n\n## LLaVAâ€™s Performance on LLaVA\\-Bench (In\\-the\\-Wild)\n\n* In conversational tasks, LLaVA achieves a 57\\.3% accuracy rate, a clear improvement over BLIP\\-2â€™s 54\\.6% and significantly outpacing OpenAIâ€™s Flamingo, which stands at 19\\.3%.\n* When it comes to providing detailed descriptions, LLaVA scores 52\\.5%, showcasing its ability to generate rich, comprehensive content from visual cues.\n* The modelâ€™s prowess is most notable in complex reasoning questions, where it achieves an 81\\.7% success rate, indicating its advanced reasoning and inferencing skills.\n\nLLaVA secures a combined score of 67\\.3% across all categories, surpassing BLIP\\-2 by a 29% margin and Flamingo by 48%.\n\n\n## Limitation and Concerns:\n\nQuantitative Evaluation of LLaVA:\n\nThe utilization of GPT\\-4 as a judge to evaluate LLaVAâ€™s performance presents a nuanced challenge within the framework of benchmarking AI capabilities. On one hand, GPT\\-4â€™s advanced comprehension and generation abilities enable it to critically assess the quality of responses produced by candidate models like LLaVA. This assessment encompasses factors such as helpfulness, relevance, accuracy, and detail, which are crucial for gauging a modelâ€™s instruction\\-following proficiency with multimodal data. However, on the other hand, the use of GPT\\-4 as an evaluative judge raises concerns regarding the impartiality of the benchmarking process.\n\nThe crux of the concern lies in the fact that LLaVAâ€™s data curation process is fundamentally intertwined with GPT\\-4\\. Since GPT\\-4 has been instrumental in training LLaVA â€” by generating the instruction\\-following data that the model was fine\\-tuned on â€” there is an inherent risk of circular reasoning. Essentially, there is a possibility that LLaVA may be predisposed to generate responses that align with the patterns or biases inherent in GPT\\-4â€™s training data. This predisposition could skew the evaluation, leading to a theoretical upper bound that reflects compatibility with GPT\\-4â€™s methodology rather than a true measure of universal performance.\n\nFurthermore, relying on GPT\\-4 to provide a comprehensive explanation for its evaluation introduces a level of subjectivity rooted in the language modelâ€™s own â€œunderstandingâ€ of what constitutes a high\\-quality response. This understanding is shaped by the datasets on which GPT\\-4 was trained, which may not fully encapsulate the diversity and complexity of real\\-world multimodal interactions.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/is-perplexity-pro-a-smarter-more-efficient-way-to-search-the-web-ec509321d820","frontmatter":{"title":"Is Perplexity Pro a Smarter, More Efficient Way to Search the Web?","meta_title":"Is Perplexity Pro a Smarter, More Efficient Way to Search the Web?","description":"Perplexity is a conversational AI-driven answer engine that aims to enhance web search by providing real-time, detailed answers to complex queries, unlike traditional search engines that primarily offer lists of links. Perplexity Pro, a subscription service, offers advanced features such as access to various AI models, higher usage limits, and personalized results. Suitable for various industries, it simplifies information retrieval by allowing users to ask specific questions and receive comprehensive answers, including follow-up suggestions. However, users should be cautious of potential inaccuracies and verify information through included footnotes.","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*tGkjG6z62TZoaRpaUkSHuw.png","categories":["Chatbots","Natural Language Processing","Technology/Web"],"author":"Rifx.Online","tags":["perplexity","conversational","search","subscription","models"],"draft":false,"slug":"blog/is-perplexity-pro-a-smarter-more-efficient-way-to-search-the-web-ec509321d820"},"content":"\n\n\n\n## The Future of Search\n\nIs Perplexity Pro a Smarter, More Efficient Way to Search the Web?\n\n\n## How does it compare to traditional search engines and is it worth the cost?\n\nHey AI Friends and followers.\n\nI have had it. I am done with traditional search.\n\nI donâ€™t want to go through hundreds of links just to find **the single piece of information** I am looking, buried somewhere on the third page between advertisements.\n\nTraditional search is over. A discontinued\\-model. A relic of the past.\n\nModern answer engines are the future.\n\nPerplexity promises to revolutionize the internet search. Letâ€™s exploreâ€¦\n\nI will start by explaining what Perplexity and Perplexity Pro are, then continue with key features, use cases and industries. Finally I will quickly compare it to alternatives like ChatGPT.\n\nSo letâ€™s dive in head first. Enjoy!\n\n\n## What is Perplexity?\n\nPerplexity is a conversational AI\\-driven answer engine that provides real\\-time answers to complex queries, where traditional search engines might only offer a list of more or less helpful links.\n\nI gonna give you a quick example.\n\nAt work, I wanted to create a list of our hardware, because lotâ€™s of it is outdated and will need replacement soon.\n\nSo what I did was to use Perplexity (you could use ChatGPT as well of course) and asked it to create an example table. I wasnâ€™t sure how to structure it and Perplexity really helped me organize the Excel sheet.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*SPnfT5NgUf01ahr3Wkz6-w@2x.jpeg)\n\nOf course you might argue that you wonâ€™t need generative AI to create a simple table, and this is true. But the real fun came right afterward. I had never created a Pivot table in Excel before and I wanted to display only the hardware with expired warranty for example.\n\nWhen I asked Perplexity, it not gave me step\\-by\\-step instructions, but it also used my example data to show me how to set the Pivot table up. Something a traditional search wonâ€™t be able to do.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*lj92gWPB3w4xqaiSCc5r3Q@2x.jpeg)\n\nSo, Perplexity combines a classic search engine with an AI model like GPT\\-4o or Claude\\*, allowing users to ask questions and follow\\-up questions. It can work with documents and photos, generate code, create content, and conduct in\\-depth research on various topics.\n\nWhat sets it apart is the inclusion of footnotes for further research or to check for accuracy.\n\nAnd as you can see from the example above, the answers are precise and go far beyond what a search engine could do.\n\n*\\*Digression: GPT\\-40 is a more advanced AI model used for tasks requiring higher accuracy and processing power, while Claude 3 Opus excels at tasks like creative writing and code generation.*\n\n\n## What is Perplexity Pro?\n\nPerplexity Pro is an enhanced subscription service that offers users access to different AI models (for different use cases) and capabilities beyond the standard version.\n\nWith Pro, you can choose between GPT\\-40, Claude Sonnet 3\\.5, Claude 3 Opus, Sonar Large 32k and a default model (as of writing).\n\nPro users can enjoy higher usage limits, faster response times, personalized search results and advanced document analysis with (as of writing) almost unlimited file uploads.\n\nHere is in example of the same question answered by Perplexity and Perplexity Pro:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*RZf3p_4hS2mGIU9iFf23rw.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*-gUUeHj-oWwFlZARJ3YG_g.png)\n\nAs you can see, the Pro answer is much more detailed.\n\n\n## Who is Perplexity Pro for?\n\nPerplexity has a wide range of use cases and application areas. Letâ€™s do a fun experiment and ask Perplexity.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*PJvSMVKPKvUcwkjB8i91Gg.png)\n\nPerplexity gives an extensive answer. Letâ€™s break it down:\n\nPerplexity and Perplexity Pro can be used across various fields, industries and individuals. And even as a casual web user you can get precise answers for your questions without digging through dozens of links and advertisements.\n\nPersonally, I use Perplexity every day for all my searches and it has already replaced Google as my standard search engine. You can read more about it here. And yes I always use the pro search. It might take a bit longer, but the results are in my experience better and longer.\n\nHereâ€™s Perplexityâ€™s result:\n\n* Technology and Engineering\n* Sales and Marketing\n* Product Development\n* Legal \\& Health Care\n* Sport and Entertainment\n* Finance and Strategy\n* Data Science\n* Telecommunications\n* AI and Machine Learning\n\nGenerally speaking you could say that itâ€™s useful for anybody in any field or industry, and that is actually true. Even if you do not typically use generative AI, just getting a question answered without going through dozens of unrelated links is a big plus.\n\nItâ€™s a bit like talking to a friends who knows everything.\n\nImagine you are on vacation and want to know if you can walk to a famous sight from your hotel. What you could do is open the hotel website and have a look around, use a search engine and hope that it can give you a useful answer, or you could just ask Perplexity. It will not only give you a comprehensive answer, but also include directions, some photos and links to videos â€” all on a single page. And if you need more details, you can just ask a follow\\-up question.\n\n\n## How I use Perplexity\n\nI do almost all of my web searches on here.\n\nAfter just a couple of weeks I have become so used to asking questions instead of writing keywords in a search engine, that I cannot believe how I could have lived without it.\n\nIt is such a huge difference asking a question, even specifying exactly what you need to know, instead of guessing which keywords might give you the answer you are looking for.\n\nEven for simple questions like â€œhow old is a celebrityâ€ itâ€™s much easier to use Perplexity. There is usually at least one follow\\-up question I need to know and Perplexity even anticipates what I might want to know and suggests follow\\-up questions at the end of itâ€™s answer.\n\nHere are three tricks to get better results from ChatGPT and other generative AI platforms, but you can also apply these techniques to Perplexity:\n\n\n## Feature breakdown \\& pricing\n\nPerplexity is free to use and for a basic web searches it is sufficient enough. As a registered user you even get 5 Pro searches every four hours. For anything that goes beyond a quick answer, Pro is definitely the way to go.\n\nPro search understands your question, might break it down into smaller tasks and may ask you a question before delivering a much more comprehensive answer (see screenshots above).\n\nOn the Pro plan, you can even choose the AI model (ie. ChatGPT\\-4o or Claude Sonnet 3\\.5,â€¦) instead of relying on the standard Perplexity AI model.\n\nAnd if you prefer to work with files and want to upload PDFs or analyze photos, the limit as of writing on the free plan is 3 per day versus unlimited on the Pro plan (although Perplexity just says that you can uplaod at least 100 files daily â€” I have no idea what would happen if you upload more, because I never uploaded that many).\n\nPricing is $20 a month or $200 annually.\n\n\n## Comparing alternatives?\n\nFinding alternatives to Perplexity is challenging, because other generative AI platforms are usually not designed as search (answer) engines. So what this means is that you can ask ChatGPT, Claude or whatever AI platform you are using questions and they will user their existing knowledge to provide an answer.\n\nWithin their subscription models they might allow web searches, but it doesnâ€™t compared to a real answer engine.\n\nAnd yes, there are some other answer engines out there (you could ask Perplexity about them), but as far as my experience goes, Perplexity provides the most comprehensive answers.\n\nPersonally, I am paying for both ChatGPT and Perplexity, because they serve different purposes, and I need ChatGPT for my studies. This is something where it really excels.\n\nYou can read more about it here:\n\n\n## What is the catch?\n\nOf course, thereâ€™s always a catch. There has to be.\n\nAnd yes, there is one. Since Perplexity uses generative AI, there is always a chance that the model will hallucinate. That means it wantâ€™s to give you an answer no matter what and might just make something up if it cannot find anything.\n\nSo, fact\\-checking is important.\n\nThankfully Perplexity makes this fairly easy by including footnotes that you can quickly check if you are unsure or need confirmation.\n\nAnd I highly encourage you to do this, especially if you research something on the more serious side.\n\nPerplexity Pro is a serious upgrade from traditional search. Do you use an answer engine or are you still a traditional search engine user? ðŸ’¬\n\n\n> Hej there! Can I ask you a favour (it will really help me out to grow this blog)? If you find this article insightful, follow **me please** and **clap 50 times.** Or feel free to [buy me a coffee](https://buy.stripe.com/cN28xZgDweSd52M000). **Thanks for reading!**\n\n\n"},{"lang":"en","group":"blog","slug":"blog/key-points-llm-quantization-chatgpt-artificial-intelligence-8201ffcb33d4","frontmatter":{"title":"5 Key Points to Unlock LLM Quantization","meta_title":"5 Key Points to Unlock LLM Quantization","description":"Quantizing Large Language Models","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*RUqPEr2NTYXlI1omqF22Qg.png","categories":["Machine Learning","Data Science","Technology/Web"],"author":"Rifx.Online","tags":["quantization","weights","activations","calibration","Quanto"],"draft":false,"slug":"blog/key-points-llm-quantization-chatgpt-artificial-intelligence-8201ffcb33d4"},"content":"\n\n\n\n\n### Quantizing Large Language Models\n\n\n\nLLM Quantization is currently a hot topic due to its vital role in making Large Language Models (LLMs) more efficient and deployable across various hardware platforms, including consumer-grade devices.\n\nBy adjusting the precision of certain components within the model, **quantization significantly reduces the modelâ€™s memory footprint** while maintaining similar performance levels.\n\nIn this guide, we will explore five key aspects of LLM quantization including some practical steps for applying this technique to our models.\n\n\n## #1. Understanding Quantization\n\nQuantization is a model compression technique that reduces the precision of weights and activations in an LLM. This involves converting high-precision values to lower-precision ones, effectively **changing data types that store more information to those that store less**.\n\nDecreasing the number of bits needed for each weight or activation significantly reduces the overall model size. As a result, **quantization creates LLMs that use less memory, and require less storage space.**\n\nThis technique has become essential in response to the exponential growth in the number of parameters in successive iterations of LLMs. For example, for the OpenAIâ€™s GPT family, we can observe the growing trend in the following graph:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*QlAhma3Wu1F6w2WvkE8jDA.png)\n\nThis significant increase presents a challenge: as models grow, their memory requirements often exceed the capacity of advanced hardware accelerators such as GPUs. **This requires distributed training and inference to manage these models, which in turn limits their deployability.**\n\n\n## #2. Intuition Behind Quantization\n\nAlthough the definition of quantization may seem rather complex, the concept can be intuitively explained using matrices.\n\nLetâ€™s consider the following a 3x3 matrix representing the weights of a neural network. The matrix on the left shows the original weights, while the matrix on the right shows the quantized version of these weights:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*LPzWe9oxjlDYdSp7dVvRUg.png)\n\nIn this simple example, we round the elements of the original matrix from four decimal places to a single decimal place. Although the matrices appear similar, **the storage space required for the four-decimal version is significantly higher**.\n\nIn practice, quantization is not merely a rounding operation. Instead, it involves converting numerical values to a different data type, typically from a higher to a lower precision one.\n\nFor example, the default data type for most models is `float32`, which requires 4 bytes per parameter (32 bits). Therefore, for a 3x3 matrix, the total memory footprint is 36 bytes. Changing the data type to `int8`, only 1 byte per parameter is needed, reducing the total memory footprint of the matrix to just 9 bytes.\n\n\n## #3. Quantization Error\n\nAs we have seen, the original matrix and its quantized form are not completely equal, but very similar. The value-by-value difference is known as â€œQuantization errorâ€, which we can also represent in matrix form:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*VtGDjVbr7daagLXB57i7Mg.png)\n\n**This quantization error can accumulate for each matrix of weights in the network, affecting the modelâ€™s performance as a result.**\n\nCurrent research in quantization aims to minimize the difference in precision while decreasing the computational resources required to train or run inference on models, while maintaining acceptable performance levels.\n\n\n## #4. Linear Quantization\n\nLinear quantization is one of the most popular quantization schemes for LLMs. In simple terms, it involves mapping the range of floating-point values of the original weights to a range of fixed-point values.\n\nLetâ€™s review the steps required to apply linear quantization to our models:\n\n* **Get the minimum and maximum ranges:** We need to get the minimum and maximum values of the floating-point weights to be quantized (`x_min` and `x_max`). We also need to define the quantized range (`q_min` and `q_max`), which is already set by the data type we want to convert to.\n* **Compute the scale (`s`) and the zero-point (`z`) values:** Firstly, the scale (`s`) adjusts the range of floating-point values to fit within the integer range, preserving the data distribution and range. Secondly, the zero-point (`z`) ensures that zero in the floating-point range is accurately represented by an integer, maintaining numerical accuracy and stability, especially for values close to zero.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BepC6-izw0yE19ejsS705Q.png)\n\n* **Quantize the values (`q`)**: We need to map the original floating-point values to the integer range using a scale factor (`s`) and a zero point (`z`) computed in the previous step.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BBOQ0VbSGbwf7CN8c4PWKQ.png)\n\nApplying these formulas is quite straightforward. If we apply them to the 3x3 weight tensor on the left in the image below, we will get the quantized matrix shown on the right:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*KzBvg84mfI2gAhTIyVibwQ.png)\n\nWe can see that the lower bound of the `int8` value corresponds to the lower value of the original tensor, while the upper bound corresponds to the higher value of the original tensor, *i.e., the mapping is`0.50 â†’ 255` and `-0.40 â†’ 0`.*\n\nWe can now dequantize the values using the formula below.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*E5nnqYzncYCRuM5prssuOw.png)\n\nIf we place the dequantized values again in matrix form (matrix on the left), we can compute the quantization error (matrix on the right) by calculating the point-by-point difference between the original matrix and its dequantized version:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*56NALu9PAN95QG2hn8HXoQ.png)\n\nAs we can observe, the quantization error starts kicking in for some of the matrix values.\n\n\n## #5. Weight Quantization vs Activation Quantization\n\nIn our example above, we have focused primarily on quantizing the weights of the model. While weight quantization is crucial for model optimization, itâ€™s also important to consider that activations can be quantized as well.\n\n**Activation quantization involves reducing the precision of the intermediate outputs of each layer in the network**. Unlike weights, which remain constant once the model is trained, activations are dynamic and change with each input, making their range harder to predict.\n\nGenerally, activation quantization is more challenging to implement than weight quantization because it requires careful calibration to ensure the dynamic range of activations is accurately captured.\n\nWeight quantization and activation quantization are complementary techniques. Using both can significantly reduce model size without greatly compromising performance.\n\n\n## Final Thoughts\n\nIn this article, we have reviewed 5 key points about quantization to better understand how to reduce the size of these constantly growing models.\n\nAs for the implementation of those techniques, there are several tools and libraries in Python that support quantization such as `pytorch` and `tensorflow`. Nevertheless, integrating quantization seamlessly in existing models requires a deep understanding of the libraries and model internals.\n\nThat is why my favorite option to implement quantization in easy steps so far is the [Quanto](https://huggingface.co/blog/quanto-introduction) library by Hugging Face, designed to simplify the quantization process for PyTorch models.\n\nIf you are interested in the in-depths of LLM Quantization and how to use the aforementioned library, you might also be interested in the article [â€œQuantization for Large Language Models (LLMs): Reduce AI Model Sizes Efficientlyâ€](https://www.datacamp.com/tutorial/quantization-for-large-language-models).\n\nThat is all! Many thanks for reading!\n\nI hope this article helps you when **using LLMs for coding!**\n\nYou can also subscribe to my [**Newsletter**](https://readmedium.com/@andvalenzuela/subscribe) to stay tuned for new content.\n\n**Especially**, **if you are interested in articles about Large Language Models and ChatGPT**:\n\n\n"},{"lang":"en","group":"blog","slug":"blog/langgraph-vs-langchain-vs-langflow-vs-langsmith-which-one-to-use-why-69ee91e91000","frontmatter":{"title":"LangGraph vs. LangChain vs. LangFlow vs. LangSmith: Which One to Use & Why?","meta_title":"LangGraph vs. LangChain vs. LangFlow vs. LangSmith: Which One to Use & Why?","description":"Discover the key differences between LangGraph, LangChain, LangFlow, and LangSmith, and learn which framework is best suited for yourâ€¦","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*xrWv1QVt4zE5cxjA8VA3ag.png","categories":["Programming","Technology","Technology/Web"],"author":"Rifx.Online","tags":["LangGraph","LangChain","LangFlow","LangSmith","frameworks"],"draft":false,"slug":"blog/langgraph-vs-langchain-vs-langflow-vs-langsmith-which-one-to-use-why-69ee91e91000"},"content":"\n\n\n\n\n### Discover the key differences between LangGraph, LangChain, LangFlow, and LangSmith, and learn which framework is best suited for your language model applications â€” from workflow building to performance monitoring.\n\nðŸ‘¨ðŸ¾â€ðŸ’» [GitHub](https://github.com/mdmonsurali) â­ï¸ | ðŸ‘”[LinkedIn](https://www.linkedin.com/in/mdmonsurali/) |ðŸ“ [Medium](https://medium.com/@monsuralirana)\n\n\n\nIn recent years, the world of natural language processing (NLP) has witnessed an explosion in the number of frameworks, libraries, and tools available for building language model-based applications. Among these, **LangGraph**, **LangChain**, **LangFlow**, and **LangSmith** have emerged as leading options, each catering to different use cases and user needs. If youâ€™re looking to build, monitor, or scale language model workflows, itâ€™s crucial to understand the strengths and purposes of these tools.\n\nIn this blog, weâ€™ll explore each framework, break down their strengths, and provide insights into when to use them. Whether youâ€™re a seasoned developer or a newcomer to the field, understanding the nuances of these tools will help you choose the right one for your project.\n\n\n## Introduction to Language Model Frameworks\n\nWith the rise of powerful language models such as GPT-3, GPT-4, and other transformer-based models, there is a growing need for frameworks that streamline the creation and management of language-based applications. These frameworks simplify complex tasks like **chaining multiple prompts**, **retrieving relevant documents**, and even **monitoring model performance**.\n\nHowever, not all frameworks are the same. While some provide a **visual interface** to manage workflows, others offer advanced **debugging and observability** features. Letâ€™s dive into each of these tools to understand their unique offerings.\n\n\n## 1. LangGraph: Visualizing Complex Workflows\n\n**LangGraph** is a newer framework designed for developers who prefer a **visual approach** to building language model pipelines. It allows you to structure complex workflows with **graph-based visualizations**, making it easier to understand dependencies between different tasks and components. This can be especially useful for larger applications where multiple steps, such as text generation, document retrieval, and classification, are chained together.\n\n\n### Strengths:\n\n* **Visual Workflow Representation**: LangGraph lets you visualize the flow of data and actions between different components. This graphical approach is intuitive and helps in designing more complex pipelines.\n* **Ease of Debugging**: The visual nature of LangGraph makes it easier to identify bottlenecks or problematic nodes in a workflow.\n\n\n### Example Use Case:\n\nSuppose youâ€™re building an automated system that first retrieves relevant documents using a language model and then passes them through a summarizer. In LangGraph, you can visually map out this workflow, showing the relationships between each step. If thereâ€™s an issue at any point in the chain, the visual tool makes it easy to pinpoint where things went wrong.\n\n\n### When to Use LangGraph:\n\nIf youâ€™re managing **complex workflows** with multiple steps and value a **graphical interface** for understanding your pipeline, LangGraph is a fantastic choice. Itâ€™s particularly helpful for developers or data scientists who prefer a more intuitive, drag-and-drop approach to workflow design.\n\n**Key points**:\n\n* If you need a clear visual representation of language processing workflows.\n* When creating more complex pipelines that require branching or multi-path dependencies.\n\n\n## 2. LangChain: The Workhorse for LLM Applications\n\n**LangChain** is one of the most popular frameworks for building applications powered by **large language models (LLMs)**. It provides a versatile, **code-first approach**, allowing developers to chain tasks such as document retrieval, summarization, and question-answering into cohesive workflows.\n\n\n### Strengths:\n\n* **Extensive Support for LLMs**: LangChain is compatible with various language models, making it easy to integrate models like OpenAIâ€™s GPT or even locally hosted models.\n* **Chaining Capabilities**: LangChain excels at **chaining multiple operations** â€” hence the name â€” enabling developers to create sophisticated NLP applications.\n* **Wide Adoption**: As one of the most popular frameworks, LangChain has a **thriving community** and excellent support, with ample documentation and tutorials.\n\n\n### Example Use Case:\n\nImagine youâ€™re building a **chatbot** that first understands the userâ€™s question, retrieves relevant information from a database, and then generates a response. With LangChain, you can easily create this multi-step process programmatically, ensuring each step in the chain works harmoniously.\n\n\n### When to Use LangChain:\n\nIf youâ€™re a **developer building production-level applications** and need a **flexible, code-centric solution**, LangChain is your best bet. Itâ€™s ideal for those who prefer control over their applicationâ€™s architecture and are comfortable writing code to define workflows.\n\n**Key points**:\n\n* If youâ€™re building production-grade applications that require chaining of tasks across multiple language models.\n* If you need a library with extensive community support and wide-ranging integrations.\n* When youâ€™re more comfortable with programmatic solutions rather than visual tools.\n\n\n## 3. LangFlow: No-Code/Low-Code Extension of LangChain\n\n**LangFlow** is essentially a **visual extension of LangChain**. It combines the powerful backend of LangChain with an **intuitive drag-and-drop interface**. LangFlow allows users who might not be as comfortable writing code to still leverage the power of language models in their applications.\n\n\n### Strengths:\n\n* **Visual Workflow Creation**: Like LangGraph, LangFlow provides a visual interface for building workflows. However, itâ€™s specifically built on top of LangChain, meaning users can harness LangChainâ€™s power without needing to write extensive code.\n* **Ideal for Rapid Prototyping**: LangFlow is perfect for quickly **prototyping ideas** or building out proof-of-concept applications.\n* **Beginner-Friendly**: Itâ€™s a great entry point for users who are less familiar with coding but want to create language model workflows.\n\n\n### Example Use Case:\n\nIf you want to quickly build a **summarization tool** that retrieves documents, you can drag and drop the components in LangFlowâ€™s interface to create a fully functioning application. This can be done without writing much code, if any.\n\n\n### When to Use LangFlow:\n\nLangFlow is perfect for **non-developers** or **rapid prototyping**. If you want to experiment with **LLM workflows quickly** without delving into the code, this tool makes it easy to get started.\n\n**Key points**:\n\n* If you want to prototype LLM workflows quickly without writing code.\n* If youâ€™re comfortable with visual programming but need the flexibility of LangChain.\n* For educational purposes, to help users learn how workflows can be constructed.\n\n\n## 4. LangSmith: Monitoring and Observability\n\nWhile the other tools focus on **building workflows**, **LangSmith** is designed for **monitoring** and **debugging** language model applications. It provides advanced observability features to track the performance of your workflows and models, making it invaluable for production environments.\n\n\n### Strengths:\n\n* **Deep Observability**: LangSmith allows developers to monitor language model performance, ensuring that workflows behave as expected.\n* **Error Tracking**: It excels at helping developers track down issues, making debugging easier.\n* **Performance Insights**: LangSmith gives insights into **workflow performance**, helping developers optimize their applications.\n\n\n### Example Use Case:\n\nLetâ€™s say youâ€™ve deployed a **customer service chatbot** that uses a language model to answer questions. Over time, you notice that some responses are less accurate than expected. LangSmith can help you trace the problem by providing visibility into each decision point within the workflow.\n\n\n### When to Use LangSmith:\n\nIf youâ€™re deploying applications in **production environments** and need to ensure **robustness, reliability, and performance**, LangSmith is an essential tool. Itâ€™s particularly useful when managing **complex systems that require debugging and optimization** over time.\n\n**Key points**:\n\n* If you need advanced monitoring or debugging capabilities in LLM workflows.\n* For development environments where observability is key to ensuring optimal model performance.\n* If your focus is on improving and iterating LLM-powered applications based on real-time insights.\n\n\n## Which One to Choose?\n\n* **Use LangGraph** if you prefer graph-based, visual workflows for building complex LLM tasks. Ideal for users who need clarity and structure.\n* **Use LangChain** if you need a robust, flexible solution for creating language model applications programmatically. Itâ€™s versatile and great for developers building production-level applications.\n* **Use LangFlow** if you want the power of LangChain with a visual, no-code/low-code interface. Best for rapid prototyping and users who prefer visual tools over coding.\n* **Use LangSmith** if your focus is on observability and debugging of LLM applications. Ideal when you need to monitor and optimize workflows in a development or production environment.\n\nUltimately, your choice depends on your comfort with code, the complexity of your workflows, and whether you prioritize ease of use, flexibility, or observability.\n\n\n## Conclusion\n\nEach of these tools â€” **LangGraph**, **LangChain**, **LangFlow**, and **LangSmith** â€” caters to different stages of developing and managing language model applications. **LangGraph** provides a visual, intuitive way to build complex workflows, while **LangChain** offers a robust, code-first solution for developers looking to create scalable applications. For those who prefer a **low-code**, drag-and-drop approach, **LangFlow** simplifies the process without sacrificing power. Finally, **LangSmith** focuses on observability and debugging, ensuring that your workflows are optimized and reliable. Choosing the right tool depends on your project needs, whether itâ€™s for rapid prototyping, production-level scaling, or monitoring and performance tracking.\n\nHappy coding! ðŸŽ‰\n\nðŸ‘¨ðŸ¾â€ðŸ’» [GitHub](https://github.com/mdmonsurali) â­ï¸ | ðŸ‘”[LinkedIn](https://www.linkedin.com/in/mdmonsurali/) |ðŸ“ [Medium](https://medium.com/@monsuralirana)\n\nThank you for your time in reading this post!\n\nMake sure to leave your feedback and comments. See you in the next blog, stay tuned ðŸ“¢\n\n\n## References:\n\n1. â€œLangChain Documentationâ€ â€” <https://python.langchain.com/docs/introduction/>\n2. â€œLangGraph Overviewâ€ â€” <https://langchain-ai.github.io/langgraph/>\n3. â€œLangFlow GitHub Repositoryâ€ â€” [https://github.com/LangFlow/LangFlow](https://docs.langflow.org/)\n4. â€œLangSmith Introductionâ€ â€” <https://www.langchain.com/langsmith>\n5. â€œHow to Build Chatbots With LangChainâ€ by JetBrains blog â€” <https://blog.jetbrains.com/pycharm/2024/08/how-to-build-chatbots-with-langchain/>\n\n"},{"lang":"en","group":"blog","slug":"blog/large-language-models-just-got-a-whole-lot-smaller-f93425ee59a2","frontmatter":{"title":"Large Language Models Just Got A Whole Lot Smaller","meta_title":"Large Language Models Just Got A Whole Lot Smaller","description":"And how this might change the game for software startups","date":"2024-11-04T12:29:02.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*1PeFyz_Dlt6jEf27Q9Y33Q.png","categories":["Programming","Technology","Machine Learning"],"author":"Rifx.Online","tags":["compression","optimization","ternary","parallelism","hardware"],"draft":false,"slug":"blog/large-language-models-just-got-a-whole-lot-smaller-f93425ee59a2"},"content":"\n### And how this might change the game for software startups\n\n\n\n**This piece was co\\-written with [David Meiborg](https://readmedium.com/undefined).**\n\n*TLDR: Large Language Models (LLMs for short) are currently huge, costly to run, and have a [significant carbon footprint](https://arxiv.org/abs/2309.14393). Recent advancements in model compression and system\\-level optimization methods might, however, enhance LLM inference. In particular, an approach using parameters with ternary structure has the potential of circumventing much of the costly matrix multiplication that is standard today. This has exciting consequences for hardware startups making specialized chips, but also for software startups that use or custom\\-build their own LLMs. Startups that help their customers deploy LLMs might also have more business coming for them.*\n\nLarge language models today are big. Like, really big. If you want to load a LlaMa\\-2â€“70B model, youâ€™d need 140 GB of VRAM (thatâ€™s 70 billion parameters multiplied by 2 bytes per parameter). For comparison, GPUs like the NVIDIA RTX 3090 or 4090, have just 24 GB of VRAM â€” a fraction of what one would need.\n\nThere are some [workarounds with quantization](https://towardsdatascience.com/run-llama-2-70b-on-your-gpu-with-exllamav2-588141a88598), but these tend to be cumbersome. Likely you will still have your GPU running hot for up to 15 hours until the model is loaded. Not to mention that you still need some spare memory for inference, or in other words for deploying the model.\n\nUsing current\\-day LLMs is therefore costly: One typically needs multiple high\\-end GPUs to keep the model, and must then account for the energy costs from inference.\n\nThis is why lots of research is going into applying techniques that make LLMs smaller and thus cheaper to run on smaller hardware. It is a tough trade\\-off in most cases, because making LLMs smaller usually impacts their quality. Finding the point where cost equals benefits can be tricky.\n\nIn this piece, we give an overview of promising optimization approaches, explain a recent breakthrough from Microsoft researchers, provide a brief overview of innovative startups in the field of â€œefficient LLMsâ€ and derive some general implications for startups operating in the LLM ecosystem.\n\n## How LLMs are getting more resource\\-efficient\n\nTech giants like Microsoft and OpenAI, Meta, or Google have sufficient resources to train up cutting\\-edge models even if the training cost is currently prohibitive for most other companies. The biggest bottleneck to widespread adoption therefore is not training but inference efficiency. In other words, although Meta has published LlaMa, it still isnâ€™t being adopted enough because running â€” not creating â€” the model is already challenging enough.\n\nResearchers, however, are starting to increase this inference efficiency. Broadly speaking, there are two approaches to this: **System\\-level optimizations** do not change the model itself but rather make it work better by changing key aspects of the environment that it is in. **Model optimizations** compress the model so that it is easier to deploy and run.\n\nThere is a variety of different techniques for either approach. [A recent paper](https://arxiv.org/pdf/2402.01799.pdf) by researchers summarize these techniques excellently. Because these techniques might soon become basic knowledge for anyone working on systems with LLMs, we give a quick overview over these techniques below.\n\n### System\\-level optimization\n\nSystem\\-level optimization refers to changing not the model itself, but how it is run across the hardware. As it turns out, plenty of levers can be pulled to avoid resources sitting around idle or wiping out other inefficiencies.\n\n**Paged Attention**\n\nAt the heart of LLMs like GPT is the attention mechanism. This mechanism allows the model to focus on different parts of the input text when generating each word of the output text. Imagine you are reading a book and highlighting important sentences to remember the story better. Similarly, the attention mechanism â€œhighlightsâ€ or gives more importance to certain words or phrases when making predictions.\n\nThis mechanism is very resource\\-intensive. It requires the model to consider the relationships between all pairs of words in the input text. For long texts, this can require a lot of memory and computational power.\n\nInstead of processing the entire text at once, paged attention divides the text into smaller â€œpagesâ€ or segments. The model then processes these pages one at a time or in smaller groups. This approach significantly reduces the amount of memory needed at any given time because the model doesnâ€™t need to keep track of the entire textâ€™s relationships simultaneously.\n\nThis is a bit like a student who would be overwhelmed by reading the entire yearâ€™s textbook at once. By breaking it down into manageable segments over the school year, the student can memorize the contents of the textbook.\n\nBy requiring less memory for each step, paged attention allows for the use of larger models or longer texts within the same hardware constraints.\n\n**Tensor Parallelism**\n\nParallelism is a well\\-known concept in computing. It means dividing a large computational task into smaller parts that can be processed simultaneously by multiple processors or computers. This significant speeds up the time a program needs to run.\n\n[Tensors](https://towardsdatascience.com/what-is-a-tensor-in-deep-learning-6dedd95d6507), in the context of LLMs, are multi\\-dimensional arrays of numbers. These tensors are used to represent the data processed by the models. Such data includes input text; model weights, i.e. parameters that the model learns; and output predictions.\n\nPutting both concepts together, tensor parallelism involves splitting these tensors across multiple GPUs or other processing units. For example, if a modelâ€™s parameters (weights) are too large to fit into the memory of a single GPU, they can be divided across multiple GPUs. Each GPU then processes only a portion of the tensor at a time.\n\nJust like a team of multiple people working on a large project, the processing units need to exchange information as they work on their respective parts of the tensors. For instance, the results of computations on one GPU might need to be shared with another GPU to continue the next step in the computation. Efficient communication between the units is therefore crucial for the effectiveness of tensor parallelism.\n\nIn short, tensor parallelism is a way of breaking down the computations needed for LLMs into smaller, parallel tasks that can be handled simultaneously by multiple computing units, leading to faster training and inference times for these large and complex models.\n\n**Pipeline Parallelism**\n\nThis technique focuses on improving the workflow of processing data through the modelâ€™s layers. This can significantly speed up the overall computation and make better use of available hardware.\n\nA pipeline in computing works similarly to a factory assembly line, with different stages of a task being completed in sequence. This allows for multiple tasks to be worked on simultaneously but at different stages.\n\nIn LLMs, these different stages are represented by layers of neural networks. Each layer processes the input data in sequence, gradually extracting more complex features or patterns until it produces the final output. Think of each layer as a worker in the factory assembly line: Each worker adds something to the input data as it passes through, until finally a complex product emerges.\n\nIn pipeline parallelism, the modelâ€™s layers are divided into segments, and each segment is assigned to a different GPU or processing unit. This way, the model can be fed on batches of data: Once the first segment is through with the first batch, the second segment takes that batch, and the first segment takes a fresh, new batch on.\n\nThis creates a continuous flow of data through the model where each segment of the model is working on a different piece of data at any given time. This maximizes the use of available hardware resources by keeping all parts of the model active and reduces the idle time that can occur when a single processor waits for tasks to complete.\n\nPipeline parallelism, which was discussed earlier, operates at the level of model layers, distributing the sequential processing stages across devices. Tensor parallelism, on the other hand, operates at a more granular level, distributing the actual computations (e.g., parts of a large matrix multiplication) that occur within layers across devices.\n\n**CPU/GPU Offloading**\n\nWe have talked a lot about GPUs in this piece. Nevertheless, ot all tasks in training or running an LLM are equally suited to GPUs. Some tasks, like data preprocessing or certain control logic, might be more efficiently handled by a CPU. Other tasks, particularly the heavy mathematical computations involved in processing neural networks (like matrix multiplications), are indeed more efficiently executed on GPUs.\n\nBy offloading specific tasks to the processor best suited for them â€” GPUs for parallelizable, computation\\-heavy tasks, and CPUs for sequential or logic\\-intensive tasks â€” systems can ensure that each part of the workload is processed in the most efficient manner possible.\n\n**Fused Operations**\n\nFused operations take multiple processing steps that would normally be executed separately and combine them into a single, streamlined operation. For instance, instead of doing a matrix multiplication and then an addition, a fused operation would do both at once.\n\n**Speculative Decoding**\n\nWhen generating text, LLMs calculate the probabilities of what the next word in a sentence might be, based on the words that have come before. Traditionally, after each word is generated, the model recalculates to determine the next word, and this process repeats until the full sentence or paragraph is completed. This sequential process can be slow, however, especially for longer texts or more complex models, because each step depends on the completion of the previous step.\n\nParallel Predictions: Instead of waiting for each word to be chosen before considering the next, speculative decoding allows the model to â€œspeculateâ€ or make multiple predictions about what the next few words could be at the same time. This is called *parallel predictions*. Itâ€™s like making educated guesses about several possible paths the sentence could take next\n\nBy exploring these possibilities in parallel, the model can potentially reduce the overall time it takes to generate text. Once the actual next word is selected, the model can more quickly proceed along the most likely path, having already computed the subsequent options.\n\n### Compression of LLM Models\n\nResearchers have in the past explored model compression. With the advent of large\\-scale LLMs, however, this has become a bigger challenge.\n\nMany established compression methods rely on the paradigm of executing fine\\-tuning steps to regain lost performance during the compression stage. This approach has significant limitations, however, when applied to LLMs because of their sheer size. LLM compression has therefore become a whole new research domain.\n\n**Architecture pruning**\n\nWhen you prune an apple tree, you cut off certain branches in winter early spring. This ensures that the tree doesnâ€™t waste resources on unproductive branches or catches diseases from dead wood. This helps it produce better fruit.\n\nLLMs, of course, donâ€™t produce fruit. In this context, pruning is a method used to reduce the size of the model while trying to maintain or minimally impact its performance.\n\nLLM models have millions or even billions of parameters. Not all of these parameters are equally important for the model to make predictions or understand language. Some parameters are used rarely or donâ€™t contribute much to the modelâ€™s decisions: Eliminating these redundant or less impactful connections, neurons, or entire layers hence makes the model more efficient to use.\n\nChoosing which parameters to prune is not a trivial task. In magnitude\\-based pruning, the weights of the neural network with the smallest absolute values are removed. Before training, such weights are usually zero; after training, they are typically somewhere between \\-1 and 1\\. If training did not affect a weight very much, then it is likely close to zero, and thus contributes less to the modelâ€™s decisions.\n\nA more resource\\-intensive but also more robust pruning technique is sensitivity analysis. This involves evaluating the impact of removing each parameter, or group of parameters, on the modelâ€™s performance. Parameters whose removal causes the least degradation in performance are pruned.\n\nThere are other techniques as well, but generally one can classify them as unstructured or structured pruning. Unstructured pruning (e.g. magnitude\\-based pruning) removes individual weights, leading to a sparsely connected neural network. Structured pruning (e.g. sensitivity analysis) removes entire units or layers (e.g., a whole neuron or channel), which can be more effective for computational efficiency on certain hardware.\n\nAfter pruning, the model often undergoes a fine\\-tuning process. This involves retraining the pruned model on the training dataset or a subset of it. The goal is to allow the model to adjust and optimize its remaining parameters to compensate for the loss of the pruned ones. This helps in recovering any performance that was lost due to pruning.\n\nThis can be done in an iterative or in a one\\-shot approach. In iterative pruning, the model is pruned iteratively over several rounds. After each round, the pruned model is retrained to regain performance lost due to pruning. This cycle can be repeated multiple times, with the model potentially becoming more robust and maintaining performance even with significantly fewer parameters. In one\\-shot pruning, all the identified parameters are removed at once, and the model is then fine\\-tuned.\n\n**Knowledge distillation**\n\nImagine there is a football court with two players: One is very experienced and knows many tricks, the other is a beginner. The experienced player knows much more than the beginner, but the beginner can quickly get to a comparable behavior by mimicking the other playerâ€™s behavior on the field.\n\nKnowledge distillation for LLMs works similarly: It is the process of training a smaller (student model), more efficient model to replicate the performance of a larger model (teacher model) by learning from its outputs and the way it processes information.\n\nTo apply this technique, you obviously need a large teacher model, e.g. one of the large open\\-source models from LlaMa or Mistral. Then you need to design a smaller neural network that has significantly fewer parameters than the teacher model.\n\nInstead of training the student model solely on the original hard targets, i.e., the ground truth data labels, it is also trained on the soft targets. These are the probabilities produced by the teacher model for the same inputs. For example, for a given set of queries, imagine that the teacher answers it as â€˜Aâ€™ 70 percent of the time, â€˜Bâ€™ 20 percent of the time, and â€˜Câ€™, â€˜Dâ€™, or â€˜Eâ€™ 10 percent of the time. Not only will the student model try to get the answer to every question right; it will also try to follow the same probability distribution over a set of queries.\n\nSuch soft targets carry more information per example than hard labels because they include the teacher modelâ€™s confidence levels across all possible outcomes. This is how to the student model is able perform similarly to the teacher but with less computational expense.\n\nAfter the initial knowledge distillation, the student model might be further fine\\-tuned on the task\\-specific dataset with hard labels to maximize its performance.\n\n**Low rank approximations**\n\nLLMs work by processing and generating text based on incredibly large matrices (i.e., veeeeery big tables of numbers) that represent the relationships between words, their meanings, and how theyâ€™re used in language. These matrices can be so large that theyâ€™re hard to work with, especially when it comes to storage and computation.\n\nA low\\-rank approximation involves finding a simpler matrix that is much smaller in size but still captures the most important information of the original large matrix. It is a bit like reducing a detailed painting to a sketch.\n\nThis is done through mathematical techniques that identify which parts of the matrix (or painting, in our analogy) hold the most information and condense the matrix to just those parts. There are mathematical techniques, notably [singular value decomposition](https://www.cs.cmu.edu/~venkatg/teaching/CStheory-infoage/book-chapter-4.pdf), which help with this.\n\nIn contrast to pruning, low rank approximation performs matrix dimensionality reduction, maintaining the structure of the model but representing it in a more compact form, while pruning directly removes parts of the neural network.\n\n**Quantization**\n\nLLMs process text using a vast number of mathematical calculations. These calculations are performed using numbers that can have a wide range of values. Typically, these numbers are stored in a format that can represent a very wide range of values ([floating\\-point format](https://de.wikipedia.org/wiki/Einfache_Genauigkeit)), occupying 32 bits in memory.\n\nQuantization reduces the precision of those numbers, typically from 32\\-bit floating\\-point numbers to lower bit\\- width representations, such as 8\\-bit integers. This means that instead of using numbers with a lot of decimal places, the model uses â€œsimplerâ€ numbers, making the calculations faster and requiring less memory.\n\nQuantization\\-Aware Training (QAT) involves training the model with quantization in mind, allowing it to adapt to the precision loss and usually resulting in better performance but at the cost of more complex and resource\\-intensive training processes.\n\nPost\\-Training Quantization (PTQ) applies quantization after the model has been fully trained, offering a simpler and faster approach to reduce computational demands. However, it may not achieve the same level of accuracy or performance as QAT due to the model not being specifically optimized for lower precision operations.\n\n### The Era of 1\\-bit LLMs?\n\nMicrosoft researchers recently [made waves with a paper](https://arxiv.org/pdf/2402.17764.pdf) that stores each parameter not in 16 bits, as is currently [the standard](https://en.wikipedia.org/wiki/Half-precision_floating-point_format) in LLMs, but in a mere 1\\.58 bits. This is huge news: With this technique, they achieved almost 10 times more token throughput, i.e., it processes text almost 10 times as fast. They also reduced their memory footprint by a factor of 3\\.5, which means that they need a lot less hardware to run these models on.\n\nThis was achieved using a ternary bit. Instead of using a floating\\-point number between \\-1 and 1, as is usually the case and typically uses 16 bits, every weight is expressed as either \\-1, 0, or 1\\. These numbers can be stored on 1\\.58 bits, because for 3 possible values on a binary transistor one gets that 2Â¹.58 \\= 3\\. Using only numbers this simple also means that complicated matrix multiplication is no longer necessary, which makes it a lot more resource\\-efficient.\n\nWhat is baffling about this technique is that it achieves a similar output performance as traditional 16\\-bit models at a size of 3 billion parameters. It is not yet clear whether this kind of model scales up as well as traditional models, when passing the threshold of 13 billion or more parameters. What is clear is that even at 70 billion parameters it is more efficient, in terms of latency, memory usage and energy consumption, than a traditional model with only 13 billion parameters. The quality of the output remains to be tested in detail.\n\nOne other disadvantage is that state\\-of\\-the\\-art quantization of existing LLMs cannot be used to produce a 1\\.58\\-bit model. Such models need to be created from scratch, which, despite its dramatically lowered cost, will put it out of reach of the average citizen for now.\n\nIf and when such models have been created and work well, however, inference should become a lot easier. 1\\.58\\-bit LLMs might even be deployed on edge and mobile devices. They are also a lot friendlier to CPU devices â€” which most mobile devices run on â€” which makes them easier to deploy on cheaper chips. All this has a lot of advantages, for example for privacy, but allow allows for new applications that humanity hasnâ€™t even dreamt of yet.\n\nMoreover, startups like [Groq](https://groq.com/) have demonstrated promising results and great potential for building specific hardware [like LPUs](https://wow.groq.com/why-groq/) for LLMs. LLM\\-specific hardware is already a [huge market](https://finance.yahoo.com/news/generative-ai-market-size-expected-163500846.html#:~:text=%2D%20Large%20Language%20Model%20(LLM),the%20forecast%20period%202023%2D2029.). Findings like these might make this market grow even more aggressively than analysts have foreseen to date.\n\nIf nothing else, inference will become dirt cheap due to a combination of quantization techniques and specialized hardware. This has implications for many companies, including startups.\n\n## What do lighter LLMs mean for startups?\n\n### The boom in AI hardware has just begun\n\nBetween 1971 and 1999, CPUs were pretty much [the only microprocessors](https://cs.stanford.edu/people/eroberts/courses/soco/projects/2005-06/64-bit-processors/history1.html) on the market. Then [NVIDIA introduced](https://readmedium.com/a-brief-history-of-gpu-47d98d6a0f8a) its GPU. It was not technically the worldâ€™s first GPU; however, it was the one of the first microprocessors that made gaming an accessible and immersible experience. (Gaming eats a lot of computing power â€” if you didnâ€™t know, now you know!)\n\nFrom gaming, GPUs quickly proliferated to do many different tasks, including scientific image processing, linear algebra, 3D reconstruction, and more. One thing that GPUs do particularly well? Machine learning and LLMs. Many of NVIDIAâ€™s chips today are being used for training LLMs.\n\nSince then, other microprocessors have started to proliferate. [Googleâ€™s TPUs](https://cloud.google.com/tpu?hl=en), introduced in 2016, are particularly well\\-suited for AI training and inference. While GPUs turned out to be great for LLMs, TPUs were specifically designed for this purpose. They are well\\-suited both for training and inference.\n\nThe industry is [at a turning point](https://www.wsj.com/tech/ai/how-a-shifting-ai-chip-market-will-shape-nvidias-future-f0c256b1), however: Soon, the majority of LLM\\-related work will be inference, and no longer training, as users start deploying models such as LlaMa. New and innovative AI semiconductor companies now have a chance to enter the game.\n\nThis includes chipmaker [Groq](https://wow.groq.com/press/) which focuses on particularly speedy inference processors. Other startups include [Cerebras](https://www.cerebras.net/) (which focuses on training), [Graphcore](https://www.graphcore.ai/about) (which covers training and interference), and [SambaNova](https://sambanova.ai/) (also training and inference). More established competitors like Intel and AMD are also eyeing both training and inference, although most growth is expected to come from the latter in the coming years. The big tech giants â€” Google, Amazon, or Microsoft â€” are also developing AI\\-specialized chips, but mostly for in\\-house use.\n\nOverall, the hardware market for LLMs is still dominated by datacenter applications. Edge and mobile applications are the next logical step, but will require more breakthroughs like the 1\\.58\\-bit approach that Microsoft researchers recently published (see above).\n\n## The impact for LLM software companies\n\nLooking at the whole value chain in the emerging AI space, the developments we outlined above are likely to lead to **significantly reduced costs for running/consuming LLMs**.\n\nSome of our thoughts on where this will lead to:\n\n* **Great B2C products**, because lower LLM costs mean that you can build freemium B2C experiences with a high LLM consumption (frequency \\& scale â€” e.g. long context window) without ruining company unit economics.\n* Democratization of access on a global scale, allowing **users in lower\\-income countries** to utilize advanced AI technologies\n* Companies can automate a wider range of tasks, leading to **increased efficiency and productivity** (â€œI donâ€™t care anymore if I have 10k API calls per hourâ€)\n* New edge AI hardware combined with smaller models will lead to **new edge AI use cases** becoming feasible that were â€œdata\\-centerâ€\\-only before\n* As edge hardware explodes, we believe opportunity opens up to build software companies that help customers to bring AI models to the fragmented space of tailored edge devices (â€œgive me your model, I compress it with various techniques, test it on 10 different edge devices, tell you what works best and then help you to deploy itâ€)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/leveraging-gemini-1-5-api-for-automated-test-case-generation-reverse-engineering-2ee8789f01db","frontmatter":{"title":"Leveraging Gemini 1.5 API for Automated Test Case Generation Reverse Engineering","meta_title":"Leveraging Gemini 1.5 API for Automated Test Case Generation Reverse Engineering","description":"This test explores using Gemini API and Google Apps Script to automatically create sample inputs for faster script reverse engineering.","date":"2024-11-10T03:51:17.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*fTtML3Sm1TuQNhQP.jpg","categories":["Programming","Programming/Scripting","Technology/WebAPI"],"author":"Rifx.Online","tags":["Gemini","API","automation","reverse-engineering","scripts"],"draft":false,"slug":"blog/leveraging-gemini-1-5-api-for-automated-test-case-generation-reverse-engineering-2ee8789f01db"},"content":"\n\n\n\n\n\n\n## Abstract\n\nThis report examines leveraging Gemini 1\\.5 API with Google Apps Script to automate sample input creation during script reverse engineering. Traditionally, this process is manual and time\\-consuming, especially for functions with numerous test cases. Gemini 1\\.5 APIâ€™s potential to streamline development by automating input generation is explored through applying reverse engineering techniques to Google Apps Script samples.\n\n\n## Introduction\n\nWith the release of Gemini 1\\.5 API, users gained the ability to process more complex data, opening doors for various application developments. This report explores the potential of using Gemini 1\\.5 API in conjunction with Google Apps Script to achieve reverse engineering for script development and improvement.\n\nTraditionally, script development involves manually crafting sample input values. This process can be time\\-consuming, especially when creating functions or testing code retrieved from online resources like Stack Overflow. Each function might require numerous test cases, and manually generating these inputs can be a bottleneck.\n\nGemini 1\\.5 API offers a potential solution by automating sample input value creation. This could significantly reduce development time and effort. This report investigates this possibility by applying reverse engineering techniques to various Google Apps Script samples using Gemini 1\\.5 API.\n\nHere, we will explore how Gemini 1\\.5 API can be used to automate sample input value generation for reverse engineering scripts written in Google Apps Script.\n\n\n## Usage\n\nIn order to test this script, please do the following flow.\n\n\n## 1\\. Create an API key\n\nPlease access [https://ai.google.dev/gemini\\-api/docs/api\\-key](https://ai.google.dev/gemini-api/docs/api-key) and create your API key. At that time, please enable Generative Language API at the API console. This API key is used for this sample script.\n\nThis official document can be also seen. [Ref](https://ai.google.dev/).\n\n\n## 2\\. Create a Google Apps Script project\n\nIn this report, Google Apps Script is used. Of course, the method introducing this report can be also used in other languages.\n\nHere, in order to test the following sample scripts, please create a standalone Google Apps Script project. Of course, this script can be also used with the container\\-bound script.\n\nAnd, please open the script editor of the Google Apps Script project.\n\n\n## 3\\. Install Google Apps Script library\n\nIn order to easily access Gemini API, I created a Google Apps Script library [GeminiWithFiles](https://github.com/tanaikech/GeminiWithFiles). In the following sample scripts, this library is used. So, please install it. You can see how to install it at [here](https://github.com/tanaikech/GeminiWithFiles?tab=readme-ov-file#1-use-geminiwithfiles-as-a-google-apps-script-library).\n\n\n## 4\\. Sample script 1\n\nThe sample functions were selected from [my repository](https://github.com/tanaikech/UtlApp).\n\n* [transpose](https://github.com/tanaikech/UtlApp?tab=readme-ov-file#transpose): Transpose 2 dimensional array.\n* [removeDuplicatedValues](https://github.com/tanaikech/UtlApp?tab=readme-ov-file#removeduplicatedvalues): Remove duplicated values from 1 dimensional array.\n* [compilingNumbers](https://github.com/tanaikech/UtlApp?tab=readme-ov-file#compilingnumbers): Compiling Continuous Numbers using Google Apps Script.\n* [unpivot](https://github.com/tanaikech/UtlApp?tab=readme-ov-file#unpivot): Converting 2\\-dimensional array as unpivot (reverse pivot).\n* [expandA1Notations](https://github.com/tanaikech/UtlApp?tab=readme-ov-file#expanda1notations): This method is used for expanding A1Notations.\n\nThe sample script demonstrating these functions is provided below. In this example, all functions can be executed in a single API call. When I ran this script, it returned a total of 2,880 tokens.\n\nThe sample first creates input values using Gemini. To test these values, the script then uses them with the function implemented in Google Apps Script. Finally, both the input and output values are printed.\n\nJSON schema is employed here to generate content. This ensures the stable generation of complex JSON objects by Gemini. [Ref](https://readmedium.com/taming-the-wild-output-effective-control-of-gemini-api-response-formats-with-response-mime-type-da273c08be85) As a result, I opted to use it in this instance.\n\n\n```python\nfunction myFunction() {\n\n  const apiKey = \"###\"; // Please set your API key.\n\n  const functionObj = {\n    transpose: function transpose(array) {\n      /**\n       * ### Description\n       * When the inputted array is 2 dimensional array, true is returned.\n       *\n       * @param {Array} array 2 dimensional array.\n       * @return {Boolean} When the inputted array is 2 dimensional array, true is returned.\n       */\n      function is2DimensionalArray(array) {\n        return array.every((r) => Array.isArray(r));\n      }\n\n      /**\n       * ### Description\n       * Transpose 2 dimensional array.\n       *\n       * @param {Array} array 2 dimensional array.\n       * @param {Boolean} check Check whether the inputted array is 2 dimensional array. Default is true.\n       * @return {Array} Transposed array.\n       */\n      function transpose(array, check = true) {\n        if (check && !is2DimensionalArray(array)) {\n          throw new Error(\"Please use 2 dimensional array.\");\n        }\n        return array[0].map((_, col) => array.map((row) => row[col] || null));\n      }\n      return transpose(array);\n    },\n    removeDuplicatedValues: function removeDuplicatedValues(array) {\n      /**\n       * ### Description\n       * Remove duplicated values from 1 dimensional array.\n       *\n       * @param {Array} array 1 dimensional array.\n       * @return {Object} Object including removeDuplicatedValues, duplicatedValues and numberOfDuplicate.\n       */\n      function removeDuplicatedValues(array) {\n        if (!Array.isArray(array)) {\n          throw new Error(\"Please use 1 dimensional array.\");\n        }\n        const obj = array.reduce(\n          (m, e) => m.set(e, m.has(e) ? m.get(e) + 1 : 1),\n          new Map()\n        );\n        const e = [...obj.entries()];\n        return {\n          removeDuplicatedValues: [...obj.keys()],\n          duplicatedValues: e.reduce((ar, [k, v]) => {\n            if (v != 1) ar.push(k);\n            return ar;\n          }, []),\n          numberOfDuplicate: Object.fromEntries(e),\n        };\n      }\n      return removeDuplicatedValues(array);\n    },\n    compilingNumbers: function compilingNumbers(array) {\n      /**\n       * ### Description\n       * Compiling Continuous Numbers using Google Apps Script.\n       *\n       * @param {Array} array Input array.\n       * @return {Array} Array including object like [{\"start\":1,\"end\":1},{\"start\":3,\"end\":5},{\"start\":7,\"end\":7},{\"start\":9,\"end\":11},{\"start\":13,\"end\":13}].\n       */\n      function compilingNumbers(array) {\n        if (!(Array.isArray(array) && array.every((e) => !isNaN(e)))) {\n          throw new Error(\"Please give an array including numbers.\");\n        }\n        const { values } = [...new Set(array.sort((a, b) => a - b))].reduce(\n          (o, e, i, a) => {\n            if (\n              o.temp.length == 0 ||\n              (o.temp.length > 0 && e == o.temp[o.temp.length - 1] + 1)\n            ) {\n              o.temp.push(e);\n            } else {\n              if (o.temp.length > 0) {\n                o.values.push({\n                  start: o.temp[0],\n                  end: o.temp[o.temp.length - 1],\n                });\n              }\n              o.temp = [e];\n            }\n            if (i == a.length - 1) {\n              o.values.push(\n                o.temp.length > 1\n                  ? { start: o.temp[0], end: o.temp[o.temp.length - 1] }\n                  : { start: e, end: e }\n              );\n            }\n            return o;\n          },\n          { temp: [], values: [] }\n        );\n        return values;\n      }\n      return compilingNumbers(array);\n    },\n    unpivot: function unpivot(values) {\n      /**\n       * ### Description\n       * When the inputted array is 2 dimensional array, true is returned.\n       *\n       * @param {Array} array 2 dimensional array.\n       * @return {Boolean} When the inputted array is 2 dimensional array, true is returned.\n       */\n      function is2DimensionalArray(array) {\n        return array.every((r) => Array.isArray(r));\n      }\n\n      /**\n       * ### Description\n       * Converting 2-dimensional array as unpivot (reverse pivot).\n       *\n       * @param {Array} values 2 dimensional array.\n       * @return {Array} 2 dimensional array converted as unpivot (reverse pivot).\n       */\n      function unpivot(values) {\n        if (!Array.isArray(values) || !is2DimensionalArray(values)) {\n          throw new Error(\"Please give an array of values.\");\n        }\n        const [[, ...h], ...v] = values;\n        return h.flatMap((hh, i) => v.map((t) => [hh, t[0], t[i + 1]]));\n      }\n      return unpivot(values);\n    },\n    expandA1Notations: function expandA1Notations(a1Notations) {\n      /**\n       * ### Description\n       * Converting colum letter to column index. Start of column index is 0.\n       * @param {String} letter Column letter.\n       * @return {Number} Column index.\n       */\n      function columnLetterToIndex(letter = null) {\n        if (letter === null || typeof letter != \"string\") {\n          throw new Error(\"Please give the column letter as a string.\");\n        }\n        letter = letter.toUpperCase();\n        return [...letter].reduce(\n          (c, e, i, a) =>\n            (c += (e.charCodeAt(0) - 64) * Math.pow(26, a.length - i - 1)),\n          -1\n        );\n      }\n\n      /**\n       * ### Description\n       * Converting colum index to column letter. Start of column index is 0.\n       * Ref: https://stackoverflow.com/a/53678158/7108653\n       * @param {Number} index Column index.\n       * @return {String} Column letter.\n       */\n      function columnIndexToLetter(index = null) {\n        if (index === null || isNaN(index)) {\n          throw new Error(\n            \"Please give the column indexr as a number. In this case, 1st number is 0.\"\n          );\n        }\n        return (a = Math.floor(index / 26)) >= 0\n          ? columnIndexToLetter(a - 1) + String.fromCharCode(65 + (index % 26))\n          : \"\";\n      }\n\n      /**\n       * ### Description\n       * This method is used for expanding A1Notations.\n       * @param {Array} a1Notations Array including A1Notations.\n       * @return {Array} Array including the expanded A1Notations.\n       */\n      function expandA1Notations(a1Notations, maxRow = \"10\", maxColumn = \"Z\") {\n        if (!Array.isArray(a1Notations) || a1Notations.length == 0) {\n          throw new Error(\"Please give a1Notations (Array).\");\n        }\n        const reg1 = new RegExp(\"^([A-Z]+)([0-9]+)$\");\n        const reg2 = new RegExp(\"^([A-Z]+)$\");\n        const reg3 = new RegExp(\"^([0-9]+)$\");\n        return a1Notations.map((e) => {\n          const a1 = e.split(\"!\");\n          const r = a1.length > 1 ? a1[1] : a1[0];\n          const [r1, r2] = r.split(\":\");\n          if (!r2) return [r1];\n          let rr;\n          if (reg1.test(r1) && reg1.test(r2)) {\n            rr = [r1.toUpperCase().match(reg1), r2.toUpperCase().match(reg1)];\n          } else if (reg2.test(r1) && reg2.test(r2)) {\n            rr = [\n              [null, r1, 1],\n              [null, r2, maxRow],\n            ];\n          } else if (reg1.test(r1) && reg2.test(r2)) {\n            rr = [r1.toUpperCase().match(reg1), [null, r2, maxRow]];\n          } else if (reg2.test(r1) && reg1.test(r2)) {\n            rr = [[null, r1, maxRow], r2.toUpperCase().match(reg1)];\n          } else if (reg3.test(r1) && reg3.test(r2)) {\n            rr =\n              Number(r1) > Number(r2)\n                ? [\n                    [null, \"A\", r2],\n                    [null, maxColumn, r1],\n                  ]\n                : [\n                    [null, \"A\", r1],\n                    [null, maxColumn, r2],\n                  ];\n          } else if (reg1.test(r1) && reg3.test(r2)) {\n            rr = [r1.toUpperCase().match(reg1), [null, maxColumn, r2]];\n          } else if (reg3.test(r1) && reg1.test(r2)) {\n            let temp = r2.toUpperCase().match(reg1);\n            rr =\n              Number(temp[2]) > Number(r1)\n                ? [\n                    [null, temp[1], r1],\n                    [null, maxColumn, temp[2]],\n                  ]\n                : [temp, [null, maxColumn, r1]];\n          } else {\n            throw new Error(\"Wrong a1Notation: \" + r);\n          }\n          const obj = {\n            startRowIndex: Number(rr[0][2]),\n            endRowIndex:\n              rr.length == 1 ? Number(rr[0][2]) + 1 : Number(rr[1][2]) + 1,\n            startColumnIndex: columnLetterToIndex(rr[0][1]),\n            endColumnIndex:\n              rr.length == 1\n                ? columnLetterToIndex(rr[0][1]) + 1\n                : columnLetterToIndex(rr[1][1]) + 1,\n          };\n          let temp = [];\n          for (let i = obj.startRowIndex; i < obj.endRowIndex; i++) {\n            for (let j = obj.startColumnIndex; j < obj.endColumnIndex; j++) {\n              temp.push(columnIndexToLetter(j) + i);\n            }\n          }\n          return temp;\n        });\n      }\n      return expandA1Notations(a1Notations);\n    },\n  };\n\n  const g = GeminiWithFiles.geminiWithFiles({\n    apiKey,\n    response_mime_type: \"application/json\",\n    doCountToken: true,\n  });\n\n  const functions = Object.entries(functionObj)\n    .map(\n      ([k, v]) =>\n        `<FunctionName>${k}</FunctionName><Function>${v.toString()}</Function>`\n    )\n    .join(\"\");\n  const jsonSchema = {\n    title: \"5 input values for giving each function\",\n    description: `Proposal 5 input values for giving each function. ${functions} Don't propose \"empty\", \"null\", \"undefined\" as values.`,\n    type: \"array\",\n    items: {\n      type: \"object\",\n      properties: {\n        functionName: { description: \"Function name\", type: \"string\" },\n        inputValues: {\n          description: `Proposed 5 input values. Don't propose \"empty\", \"null\", \"undefined\" as values.`,\n          type: \"array\",\n          items: {\n            description: \"Proposed input value\",\n            type: \"array|object|string|number\",\n          },\n        },\n      },\n      additionalProperties: false,\n    },\n  };\n  let res = g.generateContent({ jsonSchema });\n  if (typeof res == \"string\") {\n    try {\n      res = JSON.parse(res);\n    } catch ({ stack }) {\n      console.error(stack);\n      return;\n    }\n  }\n  const result = res.reduce((o, { functionName, inputValues }) => {\n    try {\n      o[functionName] = [];\n      inputValues.forEach((input) => {\n        const output = functionObj[functionName](input);\n        o[functionName].push({ input, output });\n      });\n    } catch ({ stack }) {\n      console.log(stack);\n    }\n    return o;\n  }, {});\n  console.log(JSON.stringify(result));\n}\n```\nWhen this script is run, the following result is obtained. You can see that valid input and output values are created.\n\n\n```python\n{\n  \"transpose\": [\n    { \"input\": [[1, 2], [3, 4]], \"output\": [[1, 3], [2, 4]] },\n    { \"input\": [[\"a\", \"b\"], [\"c\", \"d\"]], \"output\": [[\"a\", \"c\"], [\"b\", \"d\"]] },\n    { \"input\": [[\"a1\", \"b1\"], [\"c1\", \"d1\"], [\"e1\", \"f1\"]], \"output\": [[\"a1\", \"c1\", \"e1\"], [\"b1\", \"d1\", \"f1\"]] },\n    { \"input\": [[true, false], [false, true]], \"output\": [[true, null], [null, true]] },\n    { \"input\": [[1, \"a\"], [\"c\", true]], \"output\": [[1, \"c\"], [\"a\", true]] }\n  ],\n\n  \"removeDuplicatedValues\": [\n    { \"input\": [1, 2, 3, 4, 5], \"output\": { \"removeDuplicatedValues\": [1, 2, 3, 4, 5], \"duplicatedValues\": [], \"numberOfDuplicate\": { \"1\": 1, \"2\": 1, \"3\": 1, \"4\": 1, \"5\": 1 } } },\n    { \"input\": [\"a\", \"b\", \"c\", \"d\", \"e\"], \"output\": { \"removeDuplicatedValues\": [\"a\", \"b\", \"c\", \"d\", \"e\"], \"duplicatedValues\": [], \"numberOfDuplicate\": { \"a\": 1, \"b\": 1, \"c\": 1, \"d\": 1, \"e\": 1 } } },\n    { \"input\": [1, 2, 1, 3, 2, 4, 3, 5, 4], \"output\": { \"removeDuplicatedValues\": [1, 2, 3, 4, 5], \"duplicatedValues\": [1, 2, 3, 4], \"numberOfDuplicate\": { \"1\": 2, \"2\": 2, \"3\": 2, \"4\": 2, \"5\": 1 } } },\n    { \"input\": [\"a\", \"b\", \"a\", \"c\", \"b\", \"d\", \"c\", \"e\", \"d\"], \"output\": { \"removeDuplicatedValues\": [\"a\", \"b\", \"c\", \"d\", \"e\"], \"duplicatedValues\": [\"a\", \"b\", \"c\", \"d\"], \"numberOfDuplicate\": { \"a\": 2, \"b\": 2, \"c\": 2, \"d\": 2, \"e\": 1 } } },\n    { \"input\": [1, \"a\", 2, \"b\", 1, \"c\", 2, \"d\", 1, \"e\"], \"output\": { \"removeDuplicatedValues\": [1, \"a\", 2, \"b\", \"c\", \"d\", \"e\"], \"duplicatedValues\": [1, 2], \"numberOfDuplicate\": { \"1\": 3, \"2\": 2, \"a\": 1, \"b\": 1, \"c\": 1, \"d\": 1, \"e\": 1 } } }\n  ],\n\n  \"compilingNumbers\": [\n    { \"input\": [1, 2, 3, 4, 5], \"output\": [{ \"start\": 1, \"end\": 5 }] },\n    { \"input\": [1, 3, 5, 7, 9, 11, 13], \"output\": [{ \"start\": 1, \"end\": 1 }, { \"start\": 3, \"end\": 3 }, { \"start\": 5, \"end\": 5 }, { \"start\": 7, \"end\": 7 }, { \"start\": 9, \"end\": 9 }, { \"start\": 11, \"end\": 11 }, { \"start\": 13, \"end\": 13 }] },\n    { \"input\": [1, 3, 5, 7, 8, 10, 12, 13], \"output\": [{ \"start\": 1, \"end\": 1 }, { \"start\": 3, \"end\": 3 }, { \"start\": 5, \"end\": 5 }, { \"start\": 7, \"end\": 8 }, { \"start\": 10, \"end\": 10 }, { \"start\": 12, \"end\": 13 }] },\n    { \"input\": [1, 2, 4, 5, 7, 8, 10, 11, 13, 14], \"output\": [{ \"start\": 1, \"end\": 2 }, { \"start\": 4, \"end\": 5 }, { \"start\": 7, \"end\": 8 }, { \"start\": 10, \"end\": 11 }, { \"start\": 13, \"end\": 14 }] },\n    { \"input\": [1, 2, 3, 5, 6, 8, 9, 11, 12, 14, 15], \"output\": [{ \"start\": 1, \"end\": 3 }, { \"start\": 5, \"end\": 6 }, { \"start\": 8, \"end\": 9 }, { \"start\": 11, \"end\": 12 }, { \"start\": 14, \"end\": 15 }] }\n  ],\n\n  \"unpivot\": [\n    { \"input\": [[\"name\", \"score1\", \"score2\"], [\"sample1\", 100, 80], [\"sample2\", 90, 70]], \"output\": [[\"score1\", \"sample1\", 100], [\"score1\", \"sample2\", 90], [\"score2\", \"sample1\", 80], [\"score2\", \"sample2\", 70]] },\n    { \"input\": [[\"name\", \"score1\", \"score2\", \"score3\"], [\"sample1\", 100, 80, 70], [\"sample2\", 90, 70, 80]], \"output\": [[\"score1\", \"sample1\", 100], [\"score1\", \"sample2\", 90], [\"score2\", \"sample1\", 80], [\"score2\", \"sample2\", 70], [\"score3\", \"sample1\", 70], [\"score3\", \"sample2\", 80]] },\n    { \"input\": [[\"id\", \"x\", \"y\", \"z\"], [\"a\", 1, 2, 3], [\"b\", 4, 5, 6]], \"output\": [[\"x\", \"a\", 1], [\"x\", \"b\", 4], [\"y\", \"a\", 2], [\"y\", \"b\", 5], [\"z\", \"a\", 3], [\"z\", \"b\", 6]] },\n    { \"input\": [[\"id\", \"x\", \"y\", \"z\", \"xx\", \"yy\", \"zz\"], [\"a\", 1, 2, 3, 10, 20, 30], [\"b\", 4, 5, 6, 40, 50, 60]], \"output\": [[\"x\", \"a\", 1], [\"x\", \"b\", 4], [\"y\", \"a\", 2], [\"y\", \"b\", 5], [\"z\", \"a\", 3], [\"z\", \"b\", 6], [\"xx\", \"a\", 10], [\"xx\", \"b\", 40], [\"yy\", \"a\", 20], [\"yy\", \"b\", 50], [\"zz\", \"a\", 30], [\"zz\", \"b\", 60]] },\n    { \"input\": [[\"Fruit\", \"2021\", \"2022\", \"2023\"], [\"apple\", 100, 120, 150], [\"orange\", 80, 90, 100]], \"output\": [[\"2021\", \"apple\", 100], [\"2021\", \"orange\", 80], [\"2022\", \"apple\", 120], [\"2022\", \"orange\", 90], [\"2023\", \"apple\", 150], [\"2023\", \"orange\", 100]] }\n  ],\n\n  \"expandA1Notations\": [\n    { \"input\": [\"A1:B5\", \"C3:D7\", \"E2:F10\"], \"output\": [[\"A1\", \"B1\", \"A2\", \"B2\", \"A3\", \"B3\", \"A4\", \"B4\", \"A5\", \"B5\"], [\"C3\", \"D3\", \"C4\", \"D4\", \"C5\", \"D5\", \"C6\", \"D6\", \"C7\", \"D7\"], [\"E2\", \"F2\", \"E3\", \"F3\", \"E4\", \"F4\", \"E5\", \"F5\", \"E6\", \"F6\", \"E7\", \"F7\", \"E8\", \"F8\", \"E9\", \"F9\", \"E10\", \"F10\"]] },\n    { \"input\": [\"A:B\", \"C:D\", \"E:F\"], \"output\": [[\"A1\", \"B1\", \"A2\", \"B2\", \"A3\", \"B3\", \"A4\", \"B4\", \"A5\", \"B5\", \"A6\", \"B6\", \"A7\", \"B7\", \"A8\", \"B8\", \"A9\", \"B9\", \"A10\", \"B10\"], [\"C1\", \"D1\", \"C2\", \"D2\", \"C3\", \"D3\", \"C4\", \"D4\", \"C5\", \"D5\", \"C6\", \"D6\", \"C7\", \"D7\", \"C8\", \"D8\", \"C9\", \"D9\", \"C10\", \"D10\"], [\"E1\", \"F1\", \"E2\", \"F2\", \"E3\", \"F3\", \"E4\", \"F4\", \"E5\", \"F5\", \"E6\", \"F6\", \"E7\", \"F7\", \"E8\", \"F8\", \"E9\", \"F9\", \"E10\", \"F10\"]] },\n    { \"input\": [\"A1:C5\"], \"output\": [[\"A1\", \"B1\", \"C1\", \"A2\", \"B2\", \"C2\", \"A3\", \"B3\", \"C3\", \"A4\", \"B4\", \"C4\", \"A5\", \"B5\", \"C5\"]] },\n    { \"input\": [\"A:C\"], \"output\": [[\"A1\", \"B1\", \"C1\", \"A2\", \"B2\", \"C2\", \"A3\", \"B3\", \"C3\", \"A4\", \"B4\", \"C4\", \"A5\", \"B5\", \"C5\", \"A6\", \"B6\", \"C6\", \"A7\", \"B7\", \"C7\", \"A8\", \"B8\", \"C8\", \"A9\", \"B9\", \"C9\", \"A10\", \"B10\", \"C10\"]] },\n    { \"input\": [\"1:5\", \"3:7\", \"2:10\"], \"output\": [[\"A1\", \"B1\", \"C1\", \"D1\", \"E1\", \"F1\", \"G1\", \"H1\", \"I1\", \"J1\", \"K1\", \"L1\", \"M1\", \"N1\", \"O1\", \"P1\", \"Q1\", \"R1\", \"S1\", \"T1\", \"U1\", \"V1\", \"W1\", \"X1\", \"Y1\", \"Z1\", \"A2\", \"B2\", \"C2\", \"D2\", \"E2\", \"F2\", \"G2\", \"H2\", \"I2\", \"J2\", \"K2\", \"L2\", \"M2\", \"N2\", \"O2\", \"P2\", \"Q2\", \"R2\", \"S2\", \"T2\", \"U2\", \"V2\", \"W2\", \"X2\", \"Y2\", \"Z2\", \"A3\", \"B3\", \"C3\", \"D3\", \"E3\", \"F3\", \"G3\", \"H3\", \"I3\", \"J3\", \"K3\", \"L3\", \"M3\", \"N3\", \"O3\", \"P3\", \"Q3\", \"R3\", \"S3\", \"T3\", \"U3\", \"V3\", \"W3\", \"X3\", \"Y3\", \"Z3\", \"A4\", \"B4\", \"C4\", \"D4\", \"E4\", \"F4\", \"G4\", \"H4\", \"I4\", \"J4\", \"K4\", \"L4\", \"M4\", \"N4\", \"O4\", \"P4\", \"Q4\", \"R4\", \"S4\", \"T4\", \"U4\", \"V4\", \"W4\", \"X4\", \"Y4\", \"Z4\", \"A5\", \"B5\", \"C5\", \"D5\", \"E5\", \"F5\", \"G5\", \"H5\", \"I5\", \"J5\", \"K5\", \"L5\", \"M5\", \"N5\", \"O5\", \"P5\", \"Q5\", \"R5\", \"S5\", \"T5\", \"U5\", \"V5\", \"W5\", \"X5\", \"Y5\", \"Z5\"], [\"A3\", \"B3\", \"C3\", \"D3\", \"E3\", \"F3\", \"G3\", \"H3\", \"I3\", \"J3\", \"K3\", \"L3\", \"M3\", \"N3\", \"O3\", \"P3\", \"Q3\", \"R3\", \"S3\", \"T3\", \"U3\", \"V3\", \"W3\", \"X3\", \"Y3\", \"Z3\", \"A4\", \"B4\", \"C4\", \"D4\", \"E4\", \"F4\", \"G4\", \"H4\", \"I4\", \"J4\", \"K4\", \"L4\", \"M4\", \"N4\", \"O4\", \"P4\", \"Q4\", \"R4\", \"S4\", \"T4\", \"U4\", \"V4\", \"W4\", \"X4\", \"Y4\", \"Z4\", \"A5\", \"B5\", \"C5\", \"D5\", \"E5\", \"F5\", \"G5\", \"H5\", \"I5\", \"J5\", \"K5\", \"L5\", \"M5\", \"N5\", \"O5\", \"P5\", \"Q5\", \"R5\", \"S5\", \"T5\", \"U5\", \"V5\", \"W5\", \"X5\", \"Y5\", \"Z5\", \"A6\", \"B6\", \"C6\", \"D6\", \"E6\", \"F6\", \"G6\", \"H6\", \"I6\", \"J6\", \"K6\", \"L6\", \"M6\", \"N6\", \"O6\", \"P6\", \"Q6\", \"R6\", \"S6\", \"T6\", \"U6\", \"V6\", \"W6\", \"X6\", \"Y6\", \"Z6\", \"A7\", \"B7\", \"C7\", \"D7\", \"E7\", \"F7\", \"G7\", \"H7\", \"I7\", \"J7\", \"K7\", \"L7\", \"M7\", \"N7\", \"O7\", \"P7\", \"Q7\", \"R7\", \"S7\", \"T7\", \"U7\", \"V7\", \"W7\", \"X7\", \"Y7\", \"Z7\"], [\"A2\", \"B2\", \"C2\", \"D2\", \"E2\", \"F2\", \"G2\", \"H2\", \"I2\", \"J2\", \"K2\", \"L2\", \"M2\", \"N2\", \"O2\", \"P2\", \"Q2\", \"R2\", \"S2\", \"T2\", \"U2\", \"V2\", \"W2\", \"X2\", \"Y2\", \"Z2\", \"A3\", \"B3\", \"C3\", \"D3\", \"E3\", \"F3\", \"G3\", \"H3\", \"I3\", \"J3\", \"K3\", \"L3\", \"M3\", \"N3\", \"O3\", \"P3\", \"Q3\", \"R3\", \"S3\", \"T3\", \"U3\", \"V3\", \"W3\", \"X3\", \"Y3\", \"Z3\", \"A4\", \"B4\", \"C4\", \"D4\", \"E4\", \"F4\", \"G4\", \"H4\", \"I4\", \"J4\", \"K4\", \"L4\", \"M4\", \"N4\", \"O4\", \"P4\", \"Q4\", \"R4\", \"S4\", \"T4\", \"U4\", \"V4\", \"W4\", \"X4\", \"Y4\", \"Z4\", \"A5\", \"B5\", \"C5\", \"D5\", \"E5\", \"F5\", \"G5\", \"H5\", \"I5\", \"J5\", \"K5\", \"L5\", \"M5\", \"N5\", \"O5\", \"P5\", \"Q5\", \"R5\", \"S5\", \"T5\", \"U5\", \"V5\", \"W5\", \"X5\", \"Y5\", \"Z5\", \"A6\", \"B6\", \"C6\", \"D6\", \"E6\", \"F6\", \"G6\", \"H6\", \"I6\", \"J6\", \"K6\", \"L6\", \"M6\", \"N6\", \"O6\", \"P6\", \"Q6\", \"R6\", \"S6\", \"T6\", \"U6\", \"V6\", \"W6\", \"X6\", \"Y6\", \"Z6\", \"A7\", \"B7\", \"C7\", \"D7\", \"E7\", \"F7\", \"G7\", \"H7\", \"I7\", \"J7\", \"K7\", \"L7\", \"M7\", \"N7\", \"O7\", \"P7\", \"Q7\", \"R7\", \"S7\", \"T7\", \"U7\", \"V7\", \"W7\", \"X7\", \"Y7\", \"Z7\", \"A8\", \"B8\", \"C8\", \"D8\", \"E8\", \"F8\", \"G8\", \"H8\", \"I8\", \"J8\", \"K8\", \"L8\", \"M8\", \"N8\", \"O8\", \"P8\", \"Q8\", \"R8\", \"S8\", \"T8\", \"U8\", \"V8\", \"W8\", \"X8\", \"Y8\", \"Z8\", \"A9\", \"B9\", \"C9\", \"D9\", \"E9\", \"F9\", \"G9\", \"H9\", \"I9\", \"J9\", \"K9\", \"L9\", \"M9\", \"N9\", \"O9\", \"P9\", \"Q9\", \"R9\", \"S9\", \"T9\", \"U9\", \"V9\", \"W9\", \"X9\", \"Y9\", \"Z9\", \"A10\", \"B10\", \"C10\", \"D10\", \"E10\", \"F10\", \"G10\", \"H10\", \"I10\", \"J10\", \"K10\", \"L10\", \"M10\", \"N10\", \"O10\", \"P10\", \"Q10\", \"R10\", \"S10\", \"T10\", \"U10\", \"V10\", \"W10\", \"X10\", \"Y10\", \"Z10\"]] }\n  ]\n}\n```\n\n## 5\\. Sample script 2\n\nEach function of the above sample script uses only one argument. When multiple arguments are used, the script is as follows. The sample function is as follows.\n\n* [splitArray](https://github.com/tanaikech/UtlApp?tab=readme-ov-file#splitarray): Split array every n length.\n\n\n```python\nfunction myFunction() {\n\n  const apiKey = \"###\"; // Please set your API key.\n\n  const functionObj = {\n    splitArray: function splitArray(array, size) {\n      /**\n       * ### Description\n       * Split array every n length.\n       *\n       * @param {Array} array 2 dimensional array.\n       * @param {Boolean} check Check whether the inputted array is 2 dimensional array. Default is true.\n       * @return {Array} Transposed array.\n       */\n      function splitArray(array, size) {\n        if (!array || !size || !Array.isArray(array)) {\n          throw new Error(\"Please give an array and split size.\");\n        }\n        return [...Array(Math.ceil(array.length / size))].map((_) =>\n          array.splice(0, size)\n        );\n      }\n      return splitArray(array, size);\n    },\n  };\n\n  const g = GeminiWithFiles.geminiWithFiles({\n    apiKey,\n    response_mime_type: \"application/json\",\n    doCountToken: true,\n  });\n\n  const functions = Object.entries(functionObj)\n    .map(\n      ([k, v]) =>\n        `<FunctionName>${k}</FunctionName><Function>${v.toString()}</Function>`\n    )\n    .join(\"\");\n  const jsonSchema = {\n    title: \"5 input values for giving each function\",\n    description: `Proposal 5 input values for giving each function. ${functions} Don't propose \"empty\", \"null\", \"undefined\" as values.`,\n    type: \"array\",\n    items: {\n      type: \"object\",\n      properties: {\n        functionName: { description: \"Function name\", type: \"string\" },\n        inputValues: {\n          description: `Proposed 5 input values. Don't propose \"empty\", \"null\", \"undefined\" as values.`,\n          type: \"array\",\n          items: {\n            description: \"Proposed input value\",\n            type: \"array|object|string|number\",\n          },\n        },\n      },\n      additionalProperties: false,\n    },\n  };\n  let res = g.generateContent({ jsonSchema });\n  if (typeof res == \"string\") {\n    try {\n      res = JSON.parse(res);\n    } catch ({ stack }) {\n      console.error(stack);\n      return;\n    }\n  }\n  const result = res.reduce((o, { functionName, inputValues }) => {\n    try {\n      o[functionName] = [];\n      inputValues.forEach((input) => {\n        const temp = JSON.parse(JSON.stringify(input));\n        const output = functionObj[functionName](...temp);\n        o[functionName].push({ input, output });\n      });\n    } catch ({ stack }) {\n      console.log(stack);\n    }\n    return o;\n  }, {});\n  console.log(JSON.stringify(result));\n}\n```\nWhen this script is run, the following result is obtained.\n\n\n```python\n{\n  \"splitArray\": [\n    { \"input\": [[1, 2, 3, 4, 5, 6], 2], \"output\": [[1, 2], [3, 4], [5, 6]] },\n    { \"input\": [[\"a\", \"b\", \"c\", \"d\", \"e\"], 2], \"output\": [[\"a\", \"b\"], [\"c\", \"d\"], [\"e\"]] },\n    { \"input\": [[\"apple\", \"orange\", \"grape\", \"banana\", \"kiwi\"], 3], \"output\": [[\"apple\", \"orange\", \"grape\"], [\"banana\", \"kiwi\"]] },\n    { \"input\": [[true, false, true, false, true], 1], \"output\": [[true], [false], [true], [false], [true]] },\n    { \"input\": [[1.2, 3.14, 2.71, 0.577], 2], \"output\": [[1.2, 3.14], [2.71, 0.577]] }\n  ]\n}\n```\n\n## Summary\n\nFrom the above result, we can confirm the possibility of reverse engineering using Gemini API. This also shows that Gemini API can be used to develop applications.\n\n\n## Note\n\n* If an error occurs, please run the script again. Or, please adjust the description in the JSON schema.\n* I believe that this approach will be able to be also used for other languages except for Google Apps Script.\n* In the current stage, it seems that the class objects depending on Google Apps Script like SpreadsheetApp, DriveApp, and so on cannot be used as the input values.\n* The top abstract image was created by [Gemini](https://gemini.google.com/app).\n\n"},{"lang":"en","group":"blog","slug":"blog/leveraging-large-language-models-llms-in-b2c-industries-transforming-customer-experience-with-4073990a6200","frontmatter":{"title":"Leveraging Large Language Models (LLMs) in B2C Industries: Transforming Customer Experience withâ€¦","meta_title":"Leveraging Large Language Models (LLMs) in B2C Industries: Transforming Customer Experience withâ€¦","description":"The article discusses the implementation of Large Language Models (LLMs) in B2C industries, particularly for enhancing customer service through autonomous agents. It outlines the development of an Agentic Retrieval-Augmented Generation (RAG) system for handling credit card inquiries, utilizing embeddings, vector databases, and prompt engineering to improve response accuracy. The process includes data ingestion, embedding creation, and deployment using frameworks like Flask or Streamlit. The integration of LLMs with RAG systems significantly enhances customer engagement by providing real-time, contextually aware responses, ultimately improving operational efficiency and customer satisfaction.","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Zf15fyqPpBcoEHf6G5rgbw.jpeg","categories":["Programming","Machine Learning","Chatbots"],"author":"Rifx.Online","tags":["LLMs","RAG","embeddings","vector","Flask"],"draft":false,"slug":"blog/leveraging-large-language-models-llms-in-b2c-industries-transforming-customer-experience-with-4073990a6200"},"content":"\n\n\n\n\n\nIn the rapidly evolving landscape of B2C industries such as financial services, retail, and eCommerce, customer expectations for personalized and instant responses are at an all\\-time high. With the advancement of AI technologies, particularly Large Language Models (LLMs), there has been a dramatic shift in how companies can handle customer interactions. In industries like banking and credit card services, where customers frequently seek detailed information about products, benefits, or transactions, the adoption of LLM\\-powered autonomous agents offers significant advantages. These agents can provide real\\-time, intelligent responses, transforming customer engagement while driving operational efficiency.\n\nIn my experience with AI product development in the financial services industry, these LLM\\-powered agents, when implemented correctly, can serve as a game\\-changer. They offer scalable, contextually aware customer support that not only improves satisfaction but also reduces the reliance on human agents. But how do we develop these intelligent systems? Below, Iâ€™ll walk you through the business problem of creating an Agentic Retrieval\\-Augmented Generation (RAG) system for handling customer queries related to credit card products and explain how LLMs, embeddings, vector databases, and prompt engineering come together in this solution.\n\n\n## Business Problem: Creating an Autonomous Agent for Credit Card Queries\n\nImagine a major financial services company that offers a variety of credit card products to its customers. Handling customer queries about features, benefits, interest rates, and rewards programs for different credit card products is a labor\\-intensive process. The goal is to develop an AI agent capable of handling a large volume of questions autonomously, accurately, and with deep contextual understanding.\n\n\n### Data Source for Agentic RAG Development\n\nFor this use case, weâ€™ll use a public data source from Citibank, where a range of credit card product details is available to save as PDF format. These documents contain the necessary information for answering customer queries regarding Citibankâ€™s credit card products: [Citibank Credit Cards Overview](https://www.citi.com/credit-cards/compare/view-all-credit-cards?intc=citicard_vac_202405_AB). The complete code base with step by step notebook can be found in this [git repo](https://github.com/nitsourish/Conversational_AIchatbot).\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*rNgsnBXq0R-RnKfSVCQ26Q.png)\n\n\n## Embeddings and Vector Database Creation\n\nTo enable the AI agent to retrieve relevant information from the available product PDFs, the first step is to create embeddings. Embeddings are vector representations of text that allow the model to capture the semantic meaning of words, phrases, and even full documents in a continuous vector space.\n\nIn this use case, PDF files containing details on different credit cards are downloaded and processed. Using pre\\-trained language model **text\\-embedding\\-3\\-small**, we convert the textual data into dense vector representations. These vectors are stored in a vector database, which enables efficient similarity searches.\n\n**Key Steps:**\n\n1. **Data Ingestion**: PDFs of Citibankâ€™s credit card products are parsed and converted into textual format.\n\n\n```python\nfor file in os.listdir(\"../credit_card_products\"):\n    if file.endswith(\".pdf\"):\n        loaders.append(file)     \npdf_loaders = [PyPDFLoader(f\"../credit_card_products/{file}\") for file in loaders]\n\npages = []\n\nfor loader in pdf_loaders:\n    pages.extend(loader.load())\n```\n**2\\. Chunking(Splitting)**: Idea is to split the text into chunks , using newline (`\"\\n\"`) as the separator. Each chunk has an certain overlap of characters. This helps ensure smoother text segmentation for downstream processes like embedding or retrieval.\n\n\n```python\ntext_splitter = CharacterTextSplitter(\n    separator=\"\\n\",\n    chunk_size=1500,\n    chunk_overlap=100,\n    length_function=len\n)\ndocs = text_splitter.split_documents(pages)\n```\n**3\\. Embedding Creation and Vector Database** : Use an LLM\\-based embedding model to convert the preprocessed text into vector representations and Store the embeddings in a vector database such as Pinecone, FAISS, or a MongoDB\\-based custom solution.We used here FAISS(Facebook AI Similarity Search).This will allow fast, scalable search over large sets of documents.\n\n\n```python\nembeddings_model = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, model=\"text-embedding-3-small\")\n## Load it into the vector store and embed\nvectordb = FAISS.from_documents(docs, embeddings_model)\n```\n\n## Large Language Models (LLM) and Retrieval\\-Augmented Generation (RAG)\n\nLLMs, such as GPT models, are powerful at generating human\\-like text, but their capabilities are amplified when paired with RAG systems, significantly reducing hallucinations in Large Language Models (LLMs) and enabling autonomous agents to provide reliable and context\\-aware information.Retrieval\\-Augmented Generation (RAG) improves the performance of LLMs by augmenting their response generation with relevant external knowledge retrieved from a vector database. In real world the retrieval source can be anything, enterprise vector database to private or public urls(wikipedia,google docs etc.)\n\nIn the context of our credit card agent, a customer query might include: â€œWhat is the interest rate(APR) on Costco Anywhere VisaÂ® Card by Citi?â€ A RAG\\-based system would work in two steps:\n\n**1\\. Retrieval**: Use the vector database to fetch the relevant sections of the Citibank credit card PDFs based on the embedding similarity to the query.\n\n\n```python\nretriever = vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n```\n**2\\. Generation**: The LLM takes the retrieved context and generates a detailed and accurate response that directly answers the customerâ€™s question.\n\n\n```python\nquestion = \"\"\" \"\"\"\n\nai_msg = rag.invoke({\"input\": question, \"chat_history\": retriever})\n\n```\nThis approach ensures the agentâ€™s responses are both grounded in real data (retrieved from the database) and contextually relevant.\n\n\n## Prompt Engineering for Improved Interaction\n\nAn essential aspect of deploying LLM\\-powered agents is prompt engineering. In this process, carefully designed prompts guide the LLM to generate accurate and contextually relevant outputs. When answering queries related to credit card products, the agent needs to be able to understand user intent, retrieve the right information from the database, and respond in a conversational manner.\n\nExamples of effective prompt engineering include:\n\n* **Contextual follow\\-ups**: Clearly explaining the roles and information domain. We use here *ChatPromptTemplate from* from *langchain\\_core.*\n\n\n```python\nqa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\nUse the following pieces of retrieved context to answer the question. \\\nIf you don't know the answer, just say that you don't know. \\\nUse three sentences maximum and keep the answer concise.\\\n\n{context}\"\"\"\n\nqa_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", qa_system_prompt),\n        (\"human\", \"{input}\"),\n    ]\n)\n```\nBy fine\\-tuning the prompt and ensuring it covers various angles of the query, the AI agent delivers better customer experiences leveraging best possible contexts and instruction.\n\n\n## Retrieving Chat History for Context Awareness\n\nOne of the challenges with AI\\-powered customer service is providing coherent, context\\-aware responses across a series of interactions. For example, a customer might ask multiple questions about a credit card product in a single session. To maintain the conversational flow, the system must keep track of prior interactions.\n\n\n```python\nsystem_prompt = \"\"\"Given the chat history and a recent user question \\\ngenerate a new standalone question \\\nthat can be understood without the chat history. Do NOT answer the question, \\\njust reformulate it if needed or otherwise return it as is.\"\"\"\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system_prompt),\n        MessagesPlaceholder(\"chat_history\"),\n        (\"human\", \"{input}\"),\n    ]\n)\n\nretriever_with_history = create_history_aware_retriever(\n    llm, retriever, prompt\n)\n```\nRetrieving chat history helps the agent maintain context and deliver more personalized responses. This is especially crucial in situations where the customer asks follow\\-up questions or shifts between multiple products. The system ensures that earlier data points (e.g., the product the customer is discussing) remain part of the current conversation.\n\n\n## Langchain: Orchestrating the Agent\n\nLangchain is an essential tool for connecting all these components: LLMs, vector databases, RAG systems, and external APIs. It provides an integrated framework for building these autonomous agents, streamlining the development process, and ensuring that the agent works seamlessly across different tasks, including retrieval, context generation, and response formulation.\n\n\n```python\nllm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo-0125\")\nquestion_answer_chain = create_stuff_documents_chain(llm, qa_system_prompt)\n\nretriever_with_history = create_history_aware_retriever(\n    llm, retriever, prompt\n)\n\nchat_history = [\"\"\" \"\"\"]\nrag_chain = create_retrieval_chain(retriever_with_history, question_answer_chain)\nai_msg = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history}\nchat_history.append([HumanMessage(content=question),ai_msg[\"answer\"]])\n```\nLangchainâ€™s modular architecture allows easy integration of different data sources, whether they are stored in a vector database or accessible through APIs. It also facilitates real\\-time orchestration of user queries with appropriate retrieval, generation, and contextual awareness mechanisms.\n\n\n## Deployment using Flask and Streamlit\n\nOnce the RAG model is trained and optimized, it can be deployed using lightweight web frameworks such as Flask or Streamlit. Flask allows for more customization and control over the deployment, while Streamlit offers rapid prototyping with a focus on simplicity. The full implementation is in [git repo](https://github.com/nitsourish/Conversational_AIchatbot).\n\n**Flask Example:**\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*1BZhU0OMY10wCQPRYliEQw.png)\n\n\n```python\napp = Flask(__name__)\n\n@app.route('/query', methods=['POST'])\ndef query_model():\n    input_data = request.json['query']\n    response = rag_chain.invoke({\"input\": input_data})\n    return jsonify({\"response\": response})\n\nif __name__ == '__main__':\n    app.run(debug=True)\n```\n**Streamlit Example:**\n\n\n```python\nst.title(\"Credit Card Product Query Agent\")\nuser_query = st.text_input(\"Ask a question about Citi credit cards:\")\nif user_query:\n    response = rag_chain.invoke({\"input\": user_query})\n    st.write(f\"Response: {response}\")\n```\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*uOXAnRB0yln6U21aCUMy9Q.png)\n\n\n## Key Takeaways and Road Ahead\n\nIn this blog, I discussed Relevance of LLMs in B2C Industries especially areas with high customer touch\\-points with a special application of conversational AI agent for banking products including step by step development and deployment of RAG based Pipeline leveraging popular lang\\-chain framework.The flow includes customized engineering pipeline with data ingestion, configuration of vector database(retriever).Finally there is demonstration of deployment using micro web frameworks like Flask for full control or Streamlit for quick prototyping.\n\nIn todayâ€™s fast\\-paced B2C environment, providing quick, accurate, and personalized customer service is key to gaining a competitive advantage. By combining LLMs with vector databases, retrieval\\-augmented generation (RAG), and prompt engineering, companies can deploy AI agents that not only answer customer queries but do so with high contextual accuracy.\n\nThanks for reading the article. To read such exciting AI stories follow my [medium stories](https://medium.com/@sourish.syntel).\n\n\n"},{"lang":"en","group":"blog","slug":"blog/lightrag-simple-and-efficient-rival-to-graphrag-fe49e12e9ece","frontmatter":{"title":"LightRAGâ€Šâ€”â€ŠSimple and efficient rival to GraphRAG?","meta_title":"LightRAGâ€Šâ€”â€ŠSimple and efficient rival to GraphRAG?","description":"Traditional RAG systems work by indexing raw data. This data is simply chunked and stored in vector DBs. Whenever a query comes from theâ€¦","date":"2024-11-13T01:22:29.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*7_2PyaNMVdYDWTCrb_cMCg.png","categories":["Generative AI","Data Science","Technology/Web"],"author":"Rifx.Online","tags":["LightRAG","retrieval","GraphRAG","indexing","dual-level"],"draft":false,"slug":"blog/lightrag-simple-and-efficient-rival-to-graphrag-fe49e12e9ece"},"content":"\n\n\n\n\n\nTraditional RAG systems work by indexing raw data. This data is simply chunked and stored in vector DBs. Whenever a query comes from the user, it queries the stored chunks and *retrieves* relevant chunks. If you wish to learn the fundamentals of RAG I have written a comprehensive intro about it [here](https://proxy.rifx.online/https://readmedium.com/retrieval-augmented-generation-rag-a-quick-and-comprehensive-introduction-6cd5217a4ebb).\n\nAs the retrieval step happens for every single query from the user, it is the most crucial bottleneck to speed up naive RAG systems. Would it not be logical to make the retrieval process super efficient? This is the promise of **LightRAG**.\n\n\n> **If you are a non\\-member, you may read this for free [here](https://proxy.rifx.online/https://www.ai-bites.net/lightrag-simple-and-efficient-rival-to-graphrag/). Why not subscribe there and get these right to your inbox?**\n\n\n## Why not GraphRAG\n\nBefore we look at them, you may ask, â€œWait. Do we not have GraphRAG from Microsoft?â€. Yes, but GraphRAG seems to have a couple of drawbacks.\n\n* **Incremental knowledge update.** (sec 3\\.1\\) GraphRAG first creates a reference to entities and relationships in the entire private dataset. It then does bottom\\-up clustering that organizes the data hierarchically into semantic clusters. An update to the dataset with new knowledge means that we have to go through the entire process of building the graph! LightRAG on the other hand addresses this by simply appending new knowledge to the existing one. More specifically, it combines new graph nodes and edges with existing ones through a simple union operation.\n* **Computational intensity.** As seen from their study, LightRAG significantly reduces the cost of the retrieval phase. What takes 610,000 tokens for GraphRAG takes less than 100 tokens for LightRAG.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*0TwUDr1BCNr_nSfTPwxenw.png)\n\nSo without further adieu, let's dive into LightRAG.\n\n\n## LightRAG\n\nThe two main selling points of LightRAG are Graph\\-based indexing and dual\\-level retrieval framework. So let's look into each of them.\n\n\n## Graph\\-based Indexing\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*U7sYYNA9teKEVig1dzfi2g.png)\n\nBelow are the steps LightRAG follows to incorporate graph\\-based indexing.\n\n* **Entity and Relationship (ER) extraction.** ER extraction is shown by R(.) in the above figure. This step ensures that simple entities are first extracted from a given document. For example, in the above example, â€œbeesâ€ and â€œbeekeeperâ€ are two entities. And they are related by â€œobserveâ€ relation. As in, a beekeeper observes bees.\n* **Key\\-value (KV) pair generation using LLM.** KV pairs are then generated using a simple LLM. The LLM profiling step gives a small note or explanation of what the entity or relation is all about. For example, the LLM explains who a â€œbeekeeperâ€ is in our chosen example. This step is denoted by the P(.) in the above figure. Note that this LLM is different from the general\\-purpose LLM used in the main RAG pipeline.\n* **Deduplication.** Given that the documents have to do with bees, it is quite possible that the entity â€œbeekeeperâ€ could have been retrieved from several documents or chunks. So, we need a deduplication step that just keeps one and discards the rest with the same meaning. This is shown by the D(.) in the above figure.\n\n\n## Dual\\-level Retrieval\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*t9W1UBbjFa5cnAe-_tqz-Q.png)\n\nA query to a RAG system can be one of two types â€” specific or abstract. In the same bee example, a specific query could be â€œHow many queen bees can be there in the hive?â€. An abstract query could be, â€œWhat are the implications of climate change on honey bees?â€ To address this diversity, LightRAG employs two retrieval types:\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*DuVxwxwl_2-gej_DwGzoeg.png)\n\n* **Low\\-level retrieval.** It simply extracts precise entities and their relationships like bees, observe, and beekeepers.\n* **High\\-level retrieval.** Employing an LLM, LightRAG aggregates information and summarizes multiple sources of information.\n\n\n## Why bother doing all this?\n\nDoing all this exercise and switching to LightRAG improves execution time indeed. During indexing, the LLM needs to be called just once per chunk to extract entities and their relationships.\n\nLikewise, during user query, we only retrieve entities and relationships from chunks using the same LLM we used for indexing. This is a huge saving on the retrieval overhead and hence computation. So, we have a â€œlightâ€ RAG at last!\n\nIntegrating new knowledge into existing graphs seems to be a seamless exercise. Instead of re\\-indexing the whole data whenever we have new information, we can simply append new knowledge to the existing graph.\n\n\n## Evaluation\n\nIn their evaluations, they have compared against Naive RAG, RQ\\-RAG, HyDE, and GraphRAG. To keep the comparison fair, they have used GPT\\-4o\\-mini as the LLM across the board with a fixed chunk size of 1200 for all datasets. The answers were evaluated for comprehensiveness, diversity, and effectiveness in answering the user(a.k.a. *empowerment* in the paper).\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*DNdNHW7NRcOXpvEWjT5BKQ.png)\n\nAs we can see from the underlined results, LightRAG beats all of the state\\-of\\-the\\-art methods currently available.\n\nIn general, they draw the following conclusions:\n\n* Using graph\\-based methods (GraphRAG or LightRAG) improves significantly over the baseline Naive RAG\n* LightRAG produces quite diverse answers powered by the dual\\-level retrieval paradigm\n* LightRAG can deal with complex queries better\n\n\n## Conclusion\n\nThough RAG is a fairly recent technique, we are seeing rapid progress in the area. Techniques like LightRAG which can take RAG pipelines to cheap commodity hardware are the most welcome. While the hardware landscape is ever\\-growing, there is always an increasing need to run LLMs and RAG pipelines in compute\\-constrained hardware in real time.\n\nWould you like to see some hands\\-on study of LightRAG? Please stay tunedâ€¦\n\n\n## Shout Out\n\nHope that was useful.\n\n**If you liked this article, why not follow me on [Twitter](https://proxy.rifx.online/https://twitter.com/ai_bites) where I share research updates from top AI labs every single day.**\n\n**Also please subscribe to my [YouTube channel](https://proxy.rifx.online/https://www.youtube.com/c/aibites) where I explain AI concepts and papers visually.**\n\n**Lastly, please clap, and letâ€™s celebrate you reaching the end of this story.**\n\n\n"},{"lang":"en","group":"blog","slug":"blog/llama-3-1-405b-how-to-use-for-free-9aaf3561932d","frontmatter":{"title":"Llama 3.1 405Bâ€Šâ€”â€ŠHow to Use for Free","meta_title":"Llama 3.1 405Bâ€Šâ€”â€ŠHow to Use for Free","description":"No Local Install Needed","date":"2024-10-29T05:09:24.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*db_ND6LyQ5_p5jFJCTo5GQ.jpeg","categories":["Programming","Technology","Generative AI"],"author":"Rifx.Online","tags":["Llama","Meta","HuggingChat","Groq","API"],"draft":false,"slug":"blog/llama-3-1-405b-how-to-use-for-free-9aaf3561932d"},"content":"\n\n\n\n\n### No Local Install Needed\n\n**Llama 3\\.1 405B** is Metaâ€™s most advanced AI model released in July 2024 â€” **but where can you try it*?***\n\n\n\n**LLama 3\\.1** comes in different versions, including the largest model with 405 billion parameters and smaller versions like the 70B and 8B models.\n\nThe easiest way to try the 70B and 8B models is on [Groq](https://console.groq.com/playground)â€” where you can try them directly on their playground.\n\nBecause of the overwhelming demand, the most powerful 405B model isnâ€™t usually available though.\n\nThis guide is for anyone including developers who wants to try Llama 3\\.1 405B for free â€” without needing to download and install it.\n\nIf you donâ€™t have a paid Medium account, you can read for free [here](https://addison-best.medium.com/9aaf3561932d?source=friends_link&sk=5fa532d1caaec229a0b9a445d8749449)\n\nIf you are a developer, and you want to try **LLama 3\\.1 405B for free using an API â€”** you can skip to the end of the article.\n\n\n## Where Can I Use Llama 3\\.1 405B for Free?\n\nYou can download and install it directly from [Meta](https://llama.meta.com/) â€” but it is huge and youâ€™ll need 100â€™s of Gigabytes of space and a powerful computer to try it properly.\n\nBut you can also try now without downloading.\n\nHere are some options where you can try it:\n\n**If you want to learn more AI tips and tricks to help grow your business and earn more money online:**\n\n***ðŸ‘‰*** *Sign up for our **[free 5\\-day email course](https://aigrowthguys.com/5-day-free-course-how-to-grow-your-business-like-a-weed)** to grow ðŸš€ and earn**ðŸ’²ðŸ‘ˆ***\n\n\n## 1\\. Use Llama 3\\.1 405B on Meta AI\n\nIf youâ€™re in the U.S. and it seems at least Canada (where I am), you can chat with the Llama 3\\.1 405B model through Meta AI. Visit the [Meta AI website](https://www.meta.ai) and log in using your Facebook or Instagram account.\n\nIt might also be available in other countries now, so have a look.\n\nWhen you sign in â€” hopefully youâ€™ll see an option to try **Llama 3\\.1 405B**.\n\nIf you can, youâ€™ll see a message like in the below screenshot.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*cw1WMKhdZhzUp0L3Kn7Qng.png)\n\nYou can also access it via WhatsApp by linking your Meta account.[**Try it on Meta AI**](https://www.meta.ai)\n\nYou can also try their **Imagine** photo creator and AI image editor**.**\n\nThe cartoon image with the Lama and computer was created using this at the start of the article.\n\n**I prompted**\n\n\n> **Imagine: i want a fun cartoon image for a medium article showing trying to use llama 3\\.1 405B**\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8MeC_M2O7UX7ulPOfUCuHA.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*dIG62eA7YAT3mpLA0etz9Q.png)\n\nItâ€™s worth trying. I donâ€™t think it is comparable to Flux.1 or Midjourney â€” but it is easy to use and free\n\n\n## 2\\. Use Llama 3\\.1 405B on HuggingChat\n\nHuggingChat is available to users outside the U.S. and provides access to the Llama 3\\.1 405B model. You can start chatting right away without signing up, making it an easy way to explore the modelâ€™s capabilities. Visit the [HuggingChat page](https://huggingface.co) to begin.[Try it on HuggingChat](https://huggingface.co)\n\n\n## 3\\. Use Llama 3\\.1 405B on Groq\n\n**How:** Groq initially hosted the Llama 3\\.1 405B model but now offers the smaller 70B and 8B versions due to high demand. You can explore these models by creating a free account on [Groqâ€™s website](https://groq.com).[Try it on Groq](https://groq.com)\n\n\n## 4\\. Use Llama 3\\.1 405B on Perplexity\n\nPerplexity offers a simple way to interact with Llama 3\\.1, designed for quick and easy access to the model. You can start using it by visiting the Perplexity AI platform. But this is only available in the Pro plan.[Try it on Perplexity](https://www.perplexity.ai)\n\n\n## 5\\. Use Llama 3\\.1 405B on Poe\n\nPoe by Quora is another platform where you can try Llama 3\\.1\\. Poe allows users to explore different AI models, including Llama 3\\.1, through a chat interface. Itâ€™s a versatile option if you want to compare Llama 3\\.1 with other AI models in one place. You can try 3\\.1 405B for free â€” with a limited amount of daily free credits.[Try it on Poe](https://poe.com)\n\n\n## Where Can I Use Llama 3\\.1 405B for Free with API?\n\nIf you are a developer and want to try LLama 3\\.1 405B version completely free â€” you currently have limited options.\n\nBut I wanted to give you an easy and free option to get you started.\n\nYou can currently try on [together.ai](https://together.ai) for free.\n\nYou get $5 of free credit and an API key to try it.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*w8LOXw-Wm0QTz5YgvZ27ug.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*YpKURkmy--xstoJpZ4fmbw.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*g0FxHkg6gq5OMXXo1Yzr0A.png)\n\nThis was the easiest way I found to test the Llama 3\\.1 405B version quickly\\- and free.\n\nThis is a great option for developers wanting to try using it with an API for free.\n\n\n## Note:\n\nIf you want our team to create custom AI software using LLMs, or a custom AI chatbot for your business, you can [**contact me**](https://aigrowthguys.com/contact/) âœ‰ï¸ here and Iâ€™ll get back to you quickly:\n\n[**AI Growth Guys Contact**](https://aigrowthguys.com/contact/)âœ‰ï¸\n\nðŸ‘‰ Sign up to our [**free 5\\-Day email course**](https://aigrowthguys.com/5-day-free-course-how-to-grow-your-business-like-a-weed/) to grow ðŸš€ and earnðŸ’²in the AI age\n\nYou can also [**sign up for my newsletter**](https://ai-growth-guys.beehiiv.com/subscribe/?via=andrew-best) on how to use AI to earn more money.\n\nCheck out our [**YouTube Channel**](https://www.youtube.com/@aigrowthguys)\n\nFollow us at our website: [**AI Growth Guys**](https://aigrowthguys.com/)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/llama-3-2-the-next-generation-of-lightweight-instruction-tuned-language-models-a-hands-on-9bca07c8af1d","frontmatter":{"title":"Llama 3.2: The Next Generation of Lightweight, Instruction-Tuned Language Models: A Hands-Onâ€¦","meta_title":"Llama 3.2: The Next Generation of Lightweight, Instruction-Tuned Language Models: A Hands-Onâ€¦","description":"Discover LLaMA 3.2â€™s Key Innovations in Pruning, Knowledge Distillation, and Multilingual Performance, Plus a Hands-On Tutorial to Runâ€¦","date":"2024-11-10T03:51:17.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BMalqlcJIFe50hidF4FnqQ.png","categories":["Natural Language Processing","Machine Learning","Generative AI"],"author":"Rifx.Online","tags":["LLaMA","tuning","pruning","distillation","multilingual"],"draft":false,"slug":"blog/llama-3-2-the-next-generation-of-lightweight-instruction-tuned-language-models-a-hands-on-9bca07c8af1d"},"content":"\n### Discover LLaMA 3\\.2â€™s Key Innovations in Pruning, Knowledge Distillation, and Multilingual Performance, Plus a Hands\\-On Tutorial to Run Locally or Through Google Colab\n\nðŸ‘¨ðŸ¾â€ðŸ’» [GitHub](https://github.com/mdmonsurali) â­ï¸ \\| ðŸ‘”[LinkedIn](https://www.linkedin.com/in/mdmonsurali/) \\|ðŸ“ [Medium](https://medium.com/@monsuralirana)\n\n\n\n## Introduction\n\nLanguage models continue to evolve, pushing boundaries in efficiency, speed, and multilingual capabilities. LLaMA 3\\.2 (Lightweight LLaMA) represents the next breakthrough in this trajectory, combining innovations like pruning, knowledge distillation, and synthetic data generation. Building upon Metaâ€™s previous innovations, LLaMA 3\\.2 enhances the performance of smaller models (1B and 3B parameters) without sacrificing speed, accuracy, or privacy. In this blog, we will explore the key technical advancements in LLaMA 3\\.2, discuss its benchmark results, and provide a research\\-based perspective on why these innovations matter. We will conclude with a hands\\-on tutorial to help you get started with deploying LLaMA 3\\.2 using LangChain and Ollama.\n\n## 1\\. The Evolution of LLaMA Models: From 1\\.0 to 3\\.2\n\n### A Brief History of LLaMA Models\n\nThe **Large Language Model Meta AI (LLaMA)** series has evolved significantly since its initial release. Metaâ€™s **LLaMA 1\\.0** aimed to democratize access to LLMs, providing high\\-performance models with fewer parameters than models like GPT\\-3, yet achieving similar levels of accuracy across a range of tasks. LLaMA 2\\.0 introduced instruction\\-tuning and improvements in multilingual performance.\n\n**LLaMA 3\\.2** represents the next leap, focusing on the following core areas:\n\n* **Instruction Tuning and Fine\\-Tuning**: Enhancements in instruction\\-following capabilities allow the model to perform better on downstream tasks.\n* **Efficiency for Edge Devices**: Pruning and distillation techniques enable the deployment of models on devices with limited computational resources, such as smartphones, without losing performance.\n* **Vision and Language Understanding**: The integration of vision\\-language models into LLaMA 3\\.2 allows for the handling of multimodal tasks, such as image\\-based Q\\&A.\n\n## 2\\. Key Innovations in LLaMA 3\\.2\n\n### A. Instruction\\-Tuning and Alignment\n\nInstruction\\-tuning has proven to be a key factor in improving LLMsâ€™ ability to follow natural language instructions. In LLaMA 3\\.2, Meta has used **supervised fine\\-tuning (SFT)**, **rejection sampling (RS)**, and **direct preference optimization (DPO)** techniques. These are applied iteratively to train the models to handle various tasks, such as reasoning, summarization, and tool usage, with greater accuracy.\n\n* **Supervised Fine\\-Tuning (SFT)**: The model is fine\\-tuned on human\\-annotated datasets where it learns to generate preferred outputs.\n* **Direct Preference Optimization (DPO)**: A technique that trains models to directly optimize user preferences, aligning outputs more closely with human expectations.\n\n### B. Efficient Pruning and Knowledge Distillation\n\nLLaMA 3\\.2â€™s lightweight models, such as the 1B and 3B parameter models, leverage **structured pruning** and **knowledge distillation**. These techniques reduce model size while retaining a significant amount of knowledge from larger models (e.g., LLaMA 3\\.1 8B and 70B):\n\n* **Structured Pruning**: In this approach, parts of the network with lower significance are systematically removed to create smaller models while maintaining accuracy.\n* **Knowledge Distillation**: A large model (teacher) transfers knowledge to a smaller model (student), allowing the smaller model to mimic the performance of the larger one during training.\n\n### C. Extended Context Length\n\nOne of the major updates in LLaMA 3\\.2 is its ability to handle longer context lengths â€” up to **128K tokens**. This makes it highly efficient for tasks that require processing large chunks of text, such as summarization, long document analysis, and multi\\-turn conversations.\n\n### D. Vision\\-Language Models\n\nMetaâ€™s introduction of **Vision\\-Language Models (VLMs)** in LLaMA 3\\.2 opens new frontiers for multimodal tasks. These models are designed to handle both text and images, making them highly effective for applications such as document Q\\&A, scientific diagram interpretation, and image captioning.\n\n## 3\\. Benchmark Performance: How Does LLaMA 3\\.2 Compare?\n\nLLaMA 3\\.2 has been rigorously evaluated on a wide range of benchmarks, as illustrated by the table you shared. Key highlights include:\n\n* **General Tasks**: The 3B model shows exceptional performance on benchmarks such as **MMLU** (63\\.4\\) and **IFEval** (77\\.4\\), indicating superior instruction\\-following and reasoning capabilities.\n* **Tool Use**: On tasks like **BFCL V2**, LLaMA 3\\.2 (3B) scored 67\\.0, outperforming competitors like **Gemma 2** and **Phi\\-3\\.5\\-mini** in following complex instructions related to tool usage.\n* **Math and Reasoning**: The 3B model demonstrated strong results in math\\-related tasks, scoring **77\\.7** in **GSM8K** (grade\\-school math) and **78\\.6** in the **ARC Challenge**, a benchmark focused on reasoning.\n* **Multilingual Generation**: The 3B model also excelled in the multilingual MGSM benchmark, showcasing its ability to generate coherent text across multiple languages.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*lpjDJ6AaRnljLwAxtAf-Ag.png)\n\nLLaMA 3\\.2â€™s dominance in these tasks suggests that it offers a robust solution for tasks involving natural language understanding, instruction\\-following, and reasoning in both general and multilingual contexts.\n\n## 4\\. Hands\\-On Tutorial: Running LLaMA 3\\.2 Locally Using LangChain and Ollama\n\nNow that we have explored the technical advancements of LLaMA 3\\.2, letâ€™s get hands\\-on with a step\\-by\\-step guide to setting it up locally using **LangChain** and **Ollama**. We can Install it on the local machine or Google Colab terminal. Just follow below steps:\n\n### Step 1: Install Required Libraries\n\nFirst, install the required libraries in your Python environment. Run the following commands to set up LangChain and Ollama:\n\n```python\n!pip install langchain\n!pip install -U langchain-community\n!pip install langchain_ollama\n```\n\n### Step 2: Install and Load Colab\\-XTerm\n\nColab\\-XTerm is a handy package that enables terminal access within a Colab notebook. This can be useful for running shell commands directly within the notebook environment. To install it, run the following command:\n\n```python\n!pip install colab-xterm\n%load_ext colabxterm\n```\n\n### Step 3: Installing Ollama\n\nYou can then open a terminal session by running:\n\n```python\n%xterm\n```\n\nIn the terminal, run the following command to install Ollama:\n\n```python\ncurl -fsSL https://ollama.com/install.sh | sh\n```\n\n```python\nollama serve\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*itAzyQHMHhin8b7bRLc09w.png)\n\n### Step 4: Pulling the Models\n\nOnce Ollama is installed, you can pull the models you need. Ollama provides several LLMs, including Llama 3\\.2\\. Hereâ€™s how to pull them:\n\n```python\nollama pull llama3.2\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*S3R4gByToCZXKEWBh4GWaQ.png)\n\nThe above commands will download and prepare the models for use in your Colab environment.\n\nAlternatively, Pull any LLM model that is available in Ollama. All LLM model lists and details are available:[https://ollama.com/library](https://ollama.com/library)\n\n### Step 5: Integrate LLaMA 3\\.2 with LangChain\n\nLangChain makes it easy to invoke LLaMA 3\\.2 for various NLP tasks. Hereâ€™s a simple script to test the model:\n\n```python\nfrom langchain_community.llms import Ollama\n\n## Initialize an instance of the Llama 3.1 model\nllm_llama = Ollama(model=\"llama3.2\")\n\n## Invoke the model to generate a response\nresponse = llm_llama.invoke(\"Tell me a joke\")\nprint(response)\n```\n\nOutput:\n\n```python\nHere's one:\n\nWhat do you call a fake noodle?\n\nAn impasta.\n```\n\n### Step 6: Experiment with Different Tasks\n\nYou can extend this to more complex tasks like summarization, multilingual translation, and reasoning:\n\n```python\n## Summarization\nresponse = llm_llama.invoke(\"Summarize the following text: 'LLaMA 3.2 represents a major step forward in AI development...'\")\nprint(response)\n\n## Multilingual Generation\nresponse = llm_llama.invoke(\"Translate the following into French: 'What are the major improvements in LLaMA 3.2?'\")\nprint(response)\n```\n\nOutput:\n\n```python\nQuantum Mechanics is a complex and fascinating subject, but I'll try to break it down in simple terms.\n\n**The Basics**\n\nImagine you have a coin. Heads or tails, right? In classical physics (the way things work today), the coin is either one or the other - heads or tails. It's like a definite choice.\n\nIn Quantum Mechanics, however, the coin isn't quite so simple. When you flip it, it doesn't just land on heads or tails; it exists in both states at the same time! This idea might sound crazy, but that's basically what happens with tiny particles like atoms and electrons.\n\n**Wave-Particle Duality**\n\nHere's a key concept: tiny particles can behave like both waves and particles. It sounds weird, but think of it like this:\n\n* Imagine a wave in the ocean. The water molecules are moving up and down, creating ripples.\n* Now imagine a single water molecule as a particle (a tiny ball). That's what quantum mechanics says these particles can be!\n\n**Superposition**\n\nAnother mind-bending idea is superposition. It means that tiny particles can exist in multiple states at the same time. Think of it like this:\n\n* Imagine a coin that's both heads AND tails simultaneously!\n* This happens with electrons, which can spin both clockwise and counterclockwise at the same time.\n\n**Entanglement**\n\nQuantum Mechanics also introduces entanglement. When two particles interact, they become \"connected\" in such a way that what happens to one particle instantly affects the other, no matter how far apart they are!\n\n* Imagine two dancers who are perfectly synchronized, even if they're on opposite sides of the stage.\n* This is basically entanglement: two particles can be connected in a similar way.\n\n**The Weird Stuff**\n\nNow we get to some really weird and interesting aspects of quantum mechanics:\n\n* **Uncertainty Principle**: You can't know both the position AND momentum of a particle at the same time!\n* **Quantum Tunneling**: Particles can pass through solid objects, which is weird because they shouldn't be able to fit through.\n* **SchrÃ¶dinger's Cat**: Imagine a cat in a box with a radioactive atom. If the atom decays, the cat dies. But according to quantum mechanics, the cat is BOTH alive AND dead until someone opens the box and observes it!\n\n**In a Nutshell**\n\nQuantum Mechanics is all about tiny particles behaving in strange and fascinating ways. It's like they're playing by their own rules, which can lead to some pretty wild and unpredictable outcomes.\n\nWhile this explanation is simplified, it should give you an idea of how quantum mechanics works and its weird and wonderful principles!\n```\n\nOutput:\n\n```python\nThe translation of \"What are the major improvements in LLaMA 3.2?\" into French is:\n\n\"Quels sont les amÃ©liorations majeures de LLaMA 3.2?\"\n\nHere's a breakdown of the translation:\n\n- \"What\" becomes \"Quels\"\n- \"are\" becomes \"sont\"\n- \"the\" becomes \"les\"\n- \"major improvements\" become \"amÃ©liorations majeures\"\n- \"in\" becomes \"sont\"\n- \"LLaMA 3.2\" remains the same, as it's a proper noun.\n\nNote: The phrase \"majeures\" is used to describe significant or substantial improvements.\n```\n\n> **Get GitHub Code:**\n\n## Conclusion\n\nLLaMA 3\\.2 is a versatile and highly capable model that excels across multiple NLP tasks, from multilingual text generation to practical tool usage. Its innovations in pruning and knowledge distillation ensure that it maintains top\\-tier performance, even in lightweight, resource\\-constrained environments. With this hands\\-on tutorial, you can quickly integrate LLaMA 3\\.2 into your local applications or through cloud services like Google Colab.\n\nBy unlocking LLaMA 3\\.2â€™s capabilities, developers can create cutting\\-edge applications that are not only fast and responsive but also privacy\\-conscious, keeping user data on\\-device. Whether youâ€™re exploring NLP or building real\\-world applications, LLaMA 3\\.2 sets a new benchmark in lightweight, instruction\\-tuned language models.\n\nFeel free to explore other models in the Ollama library and experiment with different tasks. The possibilities are endless!\n\n\n"},{"lang":"en","group":"blog","slug":"blog/longrag-giving-ai-a-bigger-net-to-catch-more-fish-in-the-sea-of-information-7ecdd63f330d","frontmatter":{"title":"LongRAG: Giving AI a Bigger Net to Catch More Fish in the Sea of Information","meta_title":"LongRAG: Giving AI a Bigger Net to Catch More Fish in the Sea of Information","description":"In my previous article, I introduced whether RAG would become obsolete due to long-context LLMs. Today, letâ€™s look at how to applyâ€¦","date":"2024-11-08T00:17:39.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Nt5TRh0ooDkgmibMlA1Srg.png","categories":["Generative AI","Natural Language Processing","Data Science"],"author":"Rifx.Online","tags":["long-context","LLMs","RAG","retrieval","generation"],"draft":false,"slug":"blog/longrag-giving-ai-a-bigger-net-to-catch-more-fish-in-the-sea-of-information-7ecdd63f330d"},"content":"\nIn [my previous article](https://readmedium.com/will-long-context-llms-cause-the-extinction-of-rag-de41ca5ddfc6), I introduced whether RAG would become obsolete due to long\\-context LLMs. Today, letâ€™s look at how to apply long\\-context LLMs to RAG scenarios.\n\nIn the realm of Retrieval\\-Augmented Generation (RAG), the traditional approach has always relied on short retrieval units, typically around 100 words, which forces retrievers to sift through vast corpora to extract the necessary information. This design, while functional, places an imbalanced load on the retriever, often leading to suboptimal performance due to the overwhelming volume of units it must process.\n\nThe article introduces a new study titled â€œ[LongRAG: Enhancing Retrieval\\-Augmented Generation with Long\\-context LLMs](https://arxiv.org/pdf/2406.15319v3)â€. It seeks to address this imbalance by proposing a novel framework that significantly improves the efficiency of the retriever and the performance of the reader by extending the length of retrieval units to 4,000 tokens.\n\n## Traditional RAG vs. LongRAG\n\n\n\nAs shown in Figure 1, the core innovation of LongRAG lies in its restructuring of the traditional RAG framework. By extending the retrieval unit size to 4K tokens â€” 30 times longer than the typical unit â€” LongRAG drastically reduces the number of units from millions to a manageable few hundred thousand.\n\nThis approach not only eases the burden on the retriever but also enhances the semantic completeness of the retrieved information, leading to superior downstream performance.\n\n## LongRAG\n\nThe LongRAG framework is composed of two main components: the **Long Retriever** and the **Long Reader**. An illustrative example of these two components are depicted in Figure 2\\.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*fs37A8QUj-y2rW9_iAqS3Q.png)\n\nThe Long Retriever organizes the retrieval process by grouping related documents into cohesive units that retain semantic integrity. Once the relevant long retrieval units are identified, they are passed on to the Long Reader, which is equipped to handle extensive contexts (around 30K tokens).\n\nHereâ€™s a step\\-by\\-step breakdown of the workflow:\n\n### 1\\. Formulating Long Retrieval Units\n\nThe first step in LongRAG is the creation of long retrieval units.\n\n**In traditional RAG** frameworks, the retrieval units are short, often just a few hundred tokens, which can lead to fragmented information and a heavy burden on the retriever to piece together the relevant context.\n\n**LongRAG addresses this** by grouping related documents into cohesive long retrieval units that are significantly larger â€” up to 4,000 tokens per unit.\n\nTo form these long units, LongRAG employs a grouping algorithm that organizes documents based on their relationships, such as hyperlinks embedded within Wikipedia articles.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*zPEDmLo7rcdCQ06e.png)\n\nFor instance, documents about a particular topic or entity are grouped together to create a comprehensive retrieval unit (Figure 2\\). This ensures that each unit maintains semantic integrity and provides a richer context for the reader to extract the answer from.\n\n### 2\\. Similarity Search and Ranking\n\nOnce the long retrieval units are formed, the next step is to perform a similarity search to identify which units are most relevant to the query.\n\nThe query is encoded into a vector using an encoder function, E\\_Q, and each retrieval unit is similarly encoded using another encoder function, E\\_C. The similarity between the query `q` and each retrieval unit `g` is calculated using the dot product of their respective vectors.\n\nHowever, given the length of the retrieval units, **directly encoding the entire unit can be computationally expensive and less effective**. **To mitigate this, LongRAG approximates the similarity by breaking down the long unit into smaller chunks** and calculating the maximum similarity score across these chunks.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*U1BsMZuyXqO1oqsl.png)\n\nThis method, akin to [the MaxP design from prior works](https://arxiv.org/pdf/1905.09217), allows LongRAG to efficiently identify the most relevant sections within each long retrieval unit without sacrificing performance.\n\n### 3\\. Aggregating Retrieval Results\n\nAfter the similarity scores are calculated, the top k retrieval units are selected based on their relevance to the query. **These selected units are then concatenated to form a single long context, which typically consists of around 30,000 tokens.** This aggregated context is what will be passed on to the Long Reader.\n\nThe size of k, or the number of retrieval units, is crucial for balancing the workload. If the retrieval units are too short, more units are needed, which can overwhelm the reader. Conversely, if the units are too long, fewer are needed, but they must be highly relevant to avoid including extraneous information.\n\nLongRAG optimizes this balance by using a moderate number of well\\-formed long retrieval units, usually between 4 and 8, depending on the task.\n\n### 4\\. Processing by the Long Reader\n\nThe Long Reader is the component responsible for extracting the final answer from the long context. This step leverages advanced long\\-context language models like GPT\\-4o or Gemini\\-1\\.5\\-Pro, which are capable of handling extensive sequences of text without losing track of the critical information.\n\nFor shorter contexts (less than 1,000 tokens), the Long Reader directly extracts the answer. However, for the longer contexts typical of LongRAG, the process is more nuanced. Initially, the model generates a detailed response that spans a few sentences, ensuring that it captures all relevant information. This initial output is then refined through a second round of processing, where the Long Reader condenses the response into a precise, concise answer.\n\nThis two\\-step approach ensures that the Long Reader can effectively handle the large amount of information provided by the long retrieval units while still delivering accurate and focused answers.\n\n## Evaluation\n\nThe paper presents a thorough evaluation of LongRAG on well\\-known datasets like Natural Questions (NQ) and HotpotQA. The results are compelling, showing an improvement in retrieval performance, with answer recall rates jumping from 52% to 71% on NQ (Figure 4\\), and from 47% to 72% on HotpotQA (Figure 5\\).\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*wLUdp-4OihjAz8Fu.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*vmTsnuIsV6LxJFtj.png)\n\n## Conclusion\n\nThis article explored the innovative LongRAG framework, which is an innovative approach by extending the RAG framework to handle long documents, enabling the model to process and generate answers from extended contexts effectively. It incorporates a multi\\-step retrieval process that dynamically retrieves relevant sections of long texts, ensuring that the most pertinent information is used in the generation phase. This allows LongRAG to excel in tasks that require understanding and synthesizing information from lengthy and complex documents, outperforming traditional RAG models in such scenarios.\n\nHowever, this approach is not without its challenges. The dependency on powerful long\\-context models means that the frameworkâ€™s performance is tightly coupled with the capabilities of these models. Additionally, the grouping algorithm used for creating long retrieval units may require further refinement to generalize beyond Wikipedia\\-based corpora.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/meet-ministral-3b-and-8b-edge-ai-game-changers-3f7532da8f90","frontmatter":{"title":"Meet Ministral 3B and 8B: Edge AI Game-Changers","meta_title":"Meet Ministral 3B and 8B: Edge AI Game-Changers","description":"Mistral AIâ€™s New Frontier in Edge AI and On-Device Computing","date":"2024-11-01T03:55:06.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*3CmWlEiW7ea8gtqxpI83_w.png","categories":["Technology","Autonomous Systems","Data Science"],"author":"Rifx.Online","tags":["Mistral","edge","computing","translation","robotics"],"draft":false,"slug":"blog/meet-ministral-3b-and-8b-edge-ai-game-changers-3f7532da8f90"},"content":"\n\n\n\n\n### Mistral AIâ€™s New Frontier in Edge AI and On\\-Device Computing\n\nIn the rapidly evolving landscape of AI, edge computing has become increasingly crucial for applications that demand low\\-latency, privacy\\-first, and efficient inference without relying on cloud\\-based infrastructure.\n\nThe launch of [**Ministral**](https://mistral.ai/news/ministraux/)family of models, the latest innovation from **Mistral AI**, represents a groundbreaking step forward in the realm of AI.\n\nTo mark the first anniversary of its groundbreaking **Mistral 7B** model, Mistral AI has unveiled its next generation of language models: **Ministral 3B** and **Ministral 8B**, collectively known as â€œ[**les Ministraux**](https://mistral.ai/news/ministraux/)â€. These models arenâ€™t just incremental improvements; they represent a significant leap in whatâ€™s possible with edge AI.\n\n\n\n\n## Why These Models Matter?\n\nEdge AI is all about performing complex computations locally, ensuring data privacy and reducing response times. With **Ministral 3B** and **Ministral 8B**, Mistral AI offers models that combine high computational power with memory efficiency, all while running directly on the device. These models are designed to deliver real\\-time insights for applications that canâ€™t afford latency or depend on cloud connectivity.\n\n\n## Key Features:\n\n1. **State\\-of\\-the\\-Art Performance**: Outperforms existing models in different tasks such as knowledge, commonsense, reasoning, native function\\-calling, and efficiency within the sub\\-10B category.\n2. **Large Context Window**: Support for up to 128k context length, enabling more comprehensive understanding and generation.\n3. **Efficient Architecture**: Ministral 8B features a special interleaved sliding\\-window attention pattern for faster and more memory\\-efficient inference.\n4. **Versatility**: Suitable for a wide range of applications, from on\\-device translation to autonomous robotics.\n5. **Privacy\\-First Design**: Built for local inference, these models are perfect for applications that prioritize data privacy, eliminating the need for constant cloud access.\n6. **Scalability**: Whether you need low\\-power consumption for smaller devices with Ministral 3B or greater capabilities with the 8B variant, both models are flexible enough to be adapted to various use cases.\n\n\n> For benchmarking results, refer [here](https://mistral.ai/news/ministraux/)\n\n\n## Breaking Down the Models:\n\n\n### Ministral 3B:\n\n* With just **3 billion parameters**, it provides a balanced approach for resource\\-constrained environments\n* Supports up to **128k context length**, allowing for comprehensive handling of complex queries\n* Ideal for ultra\\-low\\-latency applications\n* Outperforms many other models in its category\n\n\n### Ministral 8B:\n\n* With **8 billion parameters** and **128k context length**, it tends to deliver enhanced computational power for more demanding tasks\n* Features a **sliding\\-window attention** pattern for improved speed and memory efficiency\n* Informed by a wide range of **multilingual** and **code** data, making it suitable for diverse applications\n* Supports **function calling**\n* Balances performance and efficiency for demanding applications\n* Vocabulary size of **131k**, using the **V3\\-Tekken** tokenizer\n* Prompt Template:\n\n\n```python\n<s>[INST]user message[/INST]assistant response</s>[INST]new user message[/INST]\n```\n\n## Use Cases:\n\nThese models deliver compute\\-efficient and low\\-latency performance, making them ideal for the following scenarios:\n\n* **On\\-Device Translation**: Empowering users to communicate seamlessly across languages in real\\-time, even in areas with less internet connectivity.\n* **Internet\\-less Smart Assistants**: Supporting intelligent virtual assistants that function independently of cloud connectivity, enhancing user experience in privacy\\-sensitive environments.\n* **Local Analytics**: Enabling organizations to analyze data in real\\-time while maintaining strict privacy standards, which is essential in sectors such as healthcare and finance.\n* **Autonomous Robotics**: Equipping robots with advanced language capabilities for autonomous decision\\-making and communication, enhancing their operational efficiency in various industries.\n\nIn addition to their standalone capabilities, les Ministraux can work in conjunction with larger models like Mistral Large. This synergy allows them to serve as efficient intermediaries for **function\\-calling in agentic workflows**, handling:\n\n* **Input Parsing**: Quickly interpreting user input to ensure accurate responses.\n* **Task Routing**: Directing requests to the appropriate resources based on user intent.\n* **API Calls**: Executing API functions in real\\-time, ensuring smooth interactions across various contexts.\n\n\n## Code Usage (with vLLM):\n\nThe [Ministral\\-8B\\-Instruct\\-2410](https://huggingface.co/mistralai/Ministral-8B-Instruct-2410) Language Model is an instruct fine\\-tuned model that can be efficiently deployed using vLLM. You can find it [here](https://huggingface.co/mistralai/Ministral-8B-Instruct-2410) on Hugging Face. Hereâ€™s how you can get started:\n\n\n### Installation\n\nFirst, ensure you have the latest versions of vLLM and mistral\\_common installed:\n\n\n```python\npip install --upgrade vllm\npip install --upgrade mistral_common\n```\n\n> ***Note****: vLLM version 0\\.6\\.2 or higher is required.*\n\n\n### Offline Usage with vLLM\n\nHereâ€™s an example of how to use Ministral\\-8B in offline mode with vLLM:\n\n\n```python\nfrom vllm import LLM\nfrom vllm.sampling_params import SamplingParams\n\nmodel_name = \"mistralai/Ministral-8B-Instruct-2410\"\nsampling_params = SamplingParams(max_tokens=8192)\n\nllm = LLM(model=model_name, tokenizer_mode=\"mistral\", config_format=\"mistral\", load_format=\"mistral\")\n\nprompt = \"What are the potential implications of artificial intelligence on the job market in the next decade?\"\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": prompt\n    },\n]\n\noutputs = llm.chat(messages, sampling_params=sampling_params)\nprint(outputs[0].outputs[0].text)\n```\n\n### Server Mode Inference with vLLM\n\nIn server inference mode, vLLM runs an HTTP server that concurrently handles client connections and requests via a REST API compatible with the OpenAI protocol. Hereâ€™s how to set it up:\n\n* Start the server:\n\n\n```python\nvllm serve mistralai/Ministral-8B-Instruct-2410 --tokenizer_mode mistral --config_format mistral --load_format mistral\n```\n* Make requests to the server:\n\n\n```python\ncurl --location 'http://localhost:8000/v1/chat/completions' \\\n    --header 'Content-Type: application/json' \\\n    --header 'Authorization: Bearer token' \\\n    --data '{\n        \"model\": \"mistralai/Ministral-8B-Instruct-2410\",\n        \"messages\": [\n          {\n            \"role\": \"user\",\n            \"content\": \"What are the potential implications of artificial intelligence on the job market in the next decade?\"\n          }\n        ]\n      }'\n```\n\n> Important Notes on vLLM Usage:\n\n* Currently, vLLM is capped at a 32k context size due to limitations in implementing interleaved attention kernels for paged attention.\n* To leverage the full 128k context size, itâ€™s recommended to use [Mistral Inference](https://github.com/mistralai/mistral-inference).\n* If you need to reduce GPU memory requirements, you can use tensor parallelism by adding `tensor_parallel=2` to the LLM initialization.\n\nBy following these examples, you can easily integrate Ministral\\-8B into your projects using vLLM, whether youâ€™re running offline inference or setting up a server for multiple clients. The modelâ€™s efficiency and powerful capabilities, combined with vLLMâ€™s optimized inference, make it an excellent choice for a wide range of AI applications.\n\n\n## Conclusion:\n\nThe release of Ministral marks a significant milestone in the evolution of AI. By bringing GPT\\-level performance to edge devices, Mistral AI is not just pushing technological boundaries â€” theyâ€™re reimagining whatâ€™s possible with local, privacy\\-first artificial intelligence.\n\nAs developers, researchers, and businesses begin to explore the capabilities of Ministral, we can expect to see a new wave of AI\\-powered applications that are faster, more private, and more accessible than ever before. The age of edge AI is here, and Ministral is leading the charge.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/meet-qwen2-5-coder-32b-instruct-coder-open-source-better-than-gpt4o-5dc8343f8157","frontmatter":{"title":"Meet Qwen2.5-Coder-32B-Instruct -Coder -open source better than gpt4o.","meta_title":"Meet Qwen2.5-Coder-32B-Instruct -Coder -open source better than gpt4o.","description":"Qwen2.5-Coder-32B-Instruct is an advanced open-source AI coding assistant designed to enhance coding efficiency and accuracy. With 32 billion parameters, it matches or surpasses the capabilities of GPT-4o, offering features like long context handling (up to 128K tokens) and support for over 29 languages. Its strengths include high performance on coding benchmarks and adaptability for various coding tasks, though it requires significant processing power. Overall, Qwen2.5-Coder represents a significant step forward in AI-driven coding solutions.","date":"2024-11-14T03:29:09.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*VENiO-pvY-FzxBLUqodjRQ.jpeg","categories":["Programming","Generative AI","Data Science"],"author":"Rifx.Online","tags":["parameters","coding","benchmarks","languages","efficiency"],"draft":false,"slug":"blog/meet-qwen2-5-coder-32b-instruct-coder-open-source-better-than-gpt4o-5dc8343f8157"},"content":"\n**Meet** Qwen2\\.5\\-Coder\\-32B-Coder, Your New AI Coding Buddy\n\nHave you ever wished that coding was a little easier, faster, and maybe even more fun? So, prepare to meet your new AI coding friend, Qwen2\\.5\\-Coder. Qwen2\\.5\\-Code specifically developed this model as a cutting\\-edge language model to simplify your coding experience. Consider having a knowledgeable assistant who can write code for you and debug, explain complex concepts, and handle several languages. Intrigued? Letâ€™s look into what makes Qwen2\\.5\\-Coder so remarkable.\n\n\n\nðŸ§  **Powerful Performance: Matching GPT\\-4oâ€™s Coding Skills**\n\n> **Qwen2\\.5\\-Coder**, particularly the 32B\\-Instruct edition, is more than simply a code assistant; itâ€™s a top performer, matching or even beating GPT\\-4o and Sonnet 3\\.5, regarded as one of the most powerful artificial intelligence models. Imagine having that level of coding ability available at your disposal, too.\n\n## Open\\-Source\n\nBut it isnâ€™t all about raw power; this model exhibits outstanding correctness, resulting in syntactically precise and efficient code. And the best part? It is substantially speedier than its predecessors, allowing you to complete tasks quickly.\n\n**Key Features**\n\n* **Model Size**: 32 billion parameters.\n* **Context Length**: Supports up to 128K tokens, allowing for extensive input and output capabilities.\n* **Multilingual Support: This system can handle over 29 languages, including English, Chinese, French, and Spanish.**\n* **Instruction Following: This feature enhances the ability to follow complex instructions and generate structured outputs such as JSON.**\n* **Performance Benchmarks: The team scores highly on various coding benchmarks such as HumanEval and MATH.**\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*zyjKE3ZHtax3uX9GnbKUfA.png)\n\nThere are models suitable for all needs, ranging from small to large.\n\nRegardless of your level of experience or inexperience, Qwen2\\.5\\-Coder provides comprehensive coverage. The Qwen2\\.5\\-Coder comes in various sizes, ranging from 0\\.5B to an impressive 32B. This means you can select the model that best meets your requirements and resources. Itâ€™s like having a toolbox complete with different\\-sized wrenches, each ideal for a unique task.\n\nðŸŒŽ Mastering multiple languages\n\nCoding in several languages? Not a problem! Qwen2\\.5\\-Coder supports more than 29 languages, including popular ones such as *English, Chinese, French, and Spanish*. This bilingual proficiency makes it a very adaptable tool for developers worldwide. Itâ€™s like having a universal code translator that removes linguistic barriers and opens up new possibilities.\n\nðŸ‘ Benefits: Increased productivity and improved learning\n\nLetâ€™s discuss the pros.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MAhK8R45yNzB8A7mZZITBg.png)\n\n**Long Context Handling:** The model can handle long inputs of up to 128K tokens. This is especially useful for sophisticated coding tasks that require extensive background. **Multilingual Skills**: Qwen2\\.5\\-Coder\\-32B\\-Instruct supports more than 29 languages, including English, Chinese, French, and Spanish. This makes it a valuable tool for developers working on projects with various language requirements.\n\nðŸ‘Ž **Cons**: resource\\-intensive and risk of over\\-reliance.\n\nOf course, every technology has its drawbacks. Qwen2\\.5\\-Coder demands significant processing power, especially in its larger variants. Maximizing its use requires powerful hardware.\n\nðŸŽ‰ **The Future of Coding**?\n\nQwen2\\.5\\-Coder marks a massive advancement in AI\\-powered coding. Its precision, speed, adaptability, and open\\-source nature make it an intriguing breakthrough. The real benefit for the open source community would be if the computing power required is lessened and API costs for development are cheaper, too.\n\nOther than that, it is quite promising and would also keep large players behind the Paywall under control.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*aHeNvfOvcpME0qzy6EQexQ.jpeg)\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*PI0ioI2MtxQgtNZ0Tq1jwQ.jpeg)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/metas-llama-4-is-coming-soon-plus-parallels-brings-apple-intelligence-to-windows-c1c2722dcf03","frontmatter":{"title":"Metaâ€™s Llama 4 is Coming Soon Plus: Parallels Brings Apple Intelligence to Windows","meta_title":"Metaâ€™s Llama 4 is Coming Soon Plus: Parallels Brings Apple Intelligence to Windows","description":"No subtitle provided","date":"2024-11-10T03:51:17.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*sYakQyN_2Lupo_By","categories":["Technology","Machine Learning","Generative AI"],"author":"Rifx.Online","tags":["Llama","GPUs","Parallels","Recraft","Midjourney"],"draft":false,"slug":"blog/metas-llama-4-is-coming-soon-plus-parallels-brings-apple-intelligence-to-windows-c1c2722dcf03"},"content":"\n\n\n\n\n### Plus: Parallels Brings Apple Intelligence to Windows\n\n\n\n**Welcome to Get The Gist**, where every weekday, we share an easy\\-to\\-read summary of the latest and greatest developments in AI â€” news, innovations, and trends â€” all delivered in under 5 minutes! â±\n\n**In todayâ€™s edition:**\n\n* Mark Zuckerberg Announces Metaâ€™s Llama 4\n* Parallels Brings Apple Intelligence to Windows\n* Recraft V3 Challenges Midjourney\n* Meta AI Surpasses 500 Million Users\n* And more AI newsâ€¦.\n\n\n## 1\\. Metaâ€™s Llama 4 is Coming Soon with Major AI Advancements\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*E_j8uSNV6s3lg2vm)\n\n**The Gist:** Mark Zuckerberg [**confirmed that**](https://analyticsindiamag.com/ai-news-updates/mark-zuckerberg-confirms-llama-4-release-early-next-year/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon) Meta will launch its Llama 4 model early next year, promising new capabilities in speed, reasoning, and cross\\-modality, thanks to a record\\-setting training setup.\n\n**Key Details:**\n\n* Meta is training Llama 4 on a massive setup with over 100,000 H100 GPUs, one of the largest AI clusters reported, aiming for faster and more capable models than ever.\n* The new Llama 4 will introduce advanced capabilities like expanded memory, support for multiple data types, and seamless third\\-party integrations.\n* AI continues to drive Metaâ€™s growth, as generative tools help over a million advertisers increase conversion rates by 7% and boost user engagement on Facebook and Instagram.\n* Zuckerberg highlighted that AI innovations are creating new business opportunities, emphasizing Metaâ€™s commitment to long\\-term AI\\-powered growth across products and platforms.\n\n\n## 2\\. Parallels Bring Apple Intelligence to Windows\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*36yykSGFUbML6zR4)\n\n**The Gist:** Parallels Desktop [**now enables**](https://www.neowin.net/news/parallels-brings-apple-intelligence-features-to-windows/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon) Appleâ€™s AI\\-driven Writing Tools on Windows virtual machines for Macs, allowing users to enhance text in Windows apps using Apple Intelligence.\n\n**Key Details:**\n\n* Parallels Desktop 20\\.1 now supports Apple Writing Tools on Windows apps within macOS Sequoia 15\\.1, giving users access to text improvements like summarizing, rewriting, and tone adjustments in apps like Word and Notepad.\n* To activate, users with macOS 15\\.1 and compatible Macs (M1 or newer) can update Parallels and use shortcuts to apply these tools in Windows apps.\n* Apple Writing Tools, part of Apple Intelligence, are also rolling out on iPadOS and iOS, but only on devices with advanced processors, such as M1, M2, or A17 Pro chips.\n* This update gives Mac users a seamless way to enhance their writing across both Mac and Windows environments, blending Appleâ€™s AI with Windows usability.\n\n\n## 3\\. Recraft V3 Challenges Midjourney with Designer\\-Centric AI Image Generation\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*lYoMCLyX61RKwMiF)\n\n**The Gist:** Recraft [**has unveiled**](https://www.tomsguide.com/ai/ai-image-video/watch-out-midjourney-recraft-just-announced-new-ai-image-generator-model?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon) Recraft V3, a new AI model for image generation that aims to outperform rivals like Midjourney with powerful design\\-focused features and seamless text integration.\n\n**Key Details:**\n\n* Recraft V3 introduces precise text handling within images, allowing users to add and style text effortlessly, a rare feature among AI models; it currently holds top ranking on Hugging Faceâ€™s leaderboard.\n* Designers can now control text placement, brand colors, and unique styles, offering enhanced customization and addressing key needs for creative professionals.\n* With an Infinite Canvas, real\\-time collaboration, and an API for advanced workflows, Recraft V3 supports both individual and team\\-based design projects.\n* Recraft has over 1\\.5 million users who have generated more than 200 million images, with the tool available across web, iOS, and Android platforms.\n\n\n## Quick Gist\n\n* **Zenity** raised $38 million in Series B funding to advance security solutions for enterprises using agentic AI and low\\-code tools, addressing key security concerns in process automation [(Read More)](https://www.darkreading.com/application-security/zenity-raises-38m-series-b-funding-round-to-secure-agentic-ai?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon).\n* **OpenAI** updated its Realtime API with five new expressive voices for speech\\-to\\-speech applications and significant cost reductions through prompt caching, currently in beta ([Read More](https://venturebeat.com/ai/openai-expands-realtime-api-with-new-voices-and-cuts-prices-for-developers/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n* **OpenAI** has rolled out its advanced voice mode for free users in Europe, allowing engaging, human\\-like interactions with ChatGPT ([Read More](https://www.tomsguide.com/ai/openai-advanced-voice-is-now-free-for-10-minutes-a-month-3-tips-for-getting-the-most-out-of-that-time?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n* **OpenAI** is rolling out a new feature for ChatGPT that allows users to search their chat history, with availability for free users planned for next month ([Read More](https://indianexpress.com/article/technology/artificial-intelligence/chatgpt-now-allow-users-to-search-through-their-history-heres-how-to-use-it-9647233/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n* **Meta** is collaborating with the US government to implement its AI model Llama for various public\\-sector projects, including improving access to resources and simplifying financial aid without any financial transactions involved ([Read More](https://www.newsbytesapp.com/news/science/meta-working-to-get-llama-used-in-us-government-sectors/story?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n* **Meta** plans to launch its Llama 4 AI model early next year, training it on an unprecedented cluster of over 100,000 H100 GPUs while advocating for an open\\-source approach despite concerns over potential misuse ([Read More](https://www.newsbytesapp.com/news/science/meta-trains-llama-4-models-on-largest-nvidia-gpu-cluster/story?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n* **OpenAI** has launched Advanced Voice Mode for ChatGPT, allowing users to engage in natural, spoken conversations on desktop apps, with the feature already gaining popularity among subscribers ([Read More](https://www.digitaltrends.com/computing/chatgpt-advanced-voice-mode-macos-windows-desktops/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n* **Waymo** is enhancing its autonomous driving technology by developing a new multimodal large language model, EMMA, to improve its robotaxisâ€™ decision\\-making and adaptability in complex environments ([Read More](https://www.theverge.com/2024/10/30/24283516/waymo-google-gemini-llm-ai-robotaxi?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n* **Gemini** now includes a split\\-screen shortcut for multitasking on large\\-screen Android devices like the Pixel Tablet and Fold, enhancing the user experience ([Read More](https://www.androidauthority.com/gemini-split-screen-shortcut-3495573/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n* **Meta AI** has surpassed half a billion users in just one year since its launch, positioning itself to potentially become the most\\-used AI assistant by the end of 2024 despite facing privacy challenges in the EU ([Read More](https://www.phonearena.com/news/meta-ai-reaches-500-million-users-in-one-year_id164309?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n* **Adobe** has updated Illustrator and Photoshop with AI\\-powered features to streamline creative processes and enhance user flexibility, emphasizing the augmentation of human creativity rather than replacement ([Read More](https://www.gearpatrol.com/tech/six-new-powerful-ai-features-every-adobe-photoshop-illustrator-must-try/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n* **NVIDIA** researchers unveiled HOVER, a 1\\.5 million parameter neural network that enables humanoid robots to perform complex tasks through efficient motor coordination and real\\-time adaptability ([Read More](https://analyticsindiamag.com/ai-news-updates/nvidia-introduces-hover-a-1-5-m-parameter-neural-network-for-humanoid-robotics/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n* **Google** launched a new standalone Weather app for Pixel devices that uses AI to summarize outdoor conditions and offers multiple location tracking ([Read More](https://www.theverge.com/2024/10/30/24283998/google-weather-app-pixel-8-7-6-ai-summaries?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n\nThatâ€™s it for today, see you tomorrow! ðŸ‘‹\n\nIf you enjoyed this update and want to stay informed about the latest developments in AI, consider subscribing to ***Get The Gist*** on Medium for more insights and analyses.\n\n**Want to dive even deeper?** Subscribe to our free daily email newsletter for quick, concise updates straight to your inbox so you never miss an important development. You can sign up by clicking [here](https://getthegist.beehiiv.com/).\n\nJoin us as we explore the world of AI together â€” one gist at a time! ðŸ’¡ðŸ¤–\n\n\n"},{"lang":"en","group":"blog","slug":"blog/microsoft-graphrag-v0-4-0-ec98f1f6ed7a","frontmatter":{"title":"Microsoft GraphRAG v0.4.0","meta_title":"Microsoft GraphRAG v0.4.0","description":"Microsoft recently released version v0.4.0 of the GraphRAG project, featuring several significant updates. The most notable additions areâ€¦","date":"2024-11-13T01:22:29.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*89qTckZYLUBF1Jtv","categories":["Programming","Data Science","Machine Learning"],"author":"Rifx.Online","tags":["GraphRAG","Incremental","Indexing","DRIFT","Embedding"],"draft":false,"slug":"blog/microsoft-graphrag-v0-4-0-ec98f1f6ed7a"},"content":"\n\n\n\nMicrosoft recently released version v0\\.4\\.0 of the GraphRAG project, featuring several significant updates. The most notable additions are the Incremental Indexing feature and the DRIFT Graph Reasoning Query Module, which significantly enhance system efficiency and functionality.\n\n\n\nThe core highlights of this update include:\n\n1\\. Incremental Indexing: Significantly improves the efficiency of large\\-scale data processing and achieve faster information updates.\n\n2\\. DRIFT Graph Reasoning Query Module: Introducing advanced graph reasoning techniques to enhance complex query processing capabilities.\n\nIn addition, version 0\\.4\\.0 has optimized the embedding workflow, restructured the processing flow, and improved the overall system performance and operability. Additionally, it has added the DRIFT search CLI and example notebook to assist developers in better understanding the new features. Moreover, the introduction of relationship merging and incremental update configuration options further enhances the flexibility and intelligence level of GraphRAG.\n\nThese updates not only enhance the processing speed of GraphRAG, for instance, in large\\-scale financial data analysis, the incremental indexing feature can reduce data update times from hours to minutes. Simultaneously, they also strengthen its applicability in complex knowledge graph applications, significantly broadening its use cases. Industry experts predict that these improvements will play a crucial role in fields such as financial analysis and medical diagnosis, driving AI applications towards more precise and efficient development.\n\nMore update content: [https://proxy.rifx.online/https://github.com/microsoft/graphrag/releases/tag/v0\\.4\\.0](https://proxy.rifx.online/https://github.com/microsoft/graphrag/releases/tag/v0.4.0)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/mistral-ai-releases-revolutionary-edge-models-ministral-3b-and-8b-superior-performance-and-privacy-5b24f0189493","frontmatter":{"title":"Mistral AI Releases Revolutionary Edge Models Ministral 3B and 8B: Superior Performance and Privacy","meta_title":"Mistral AI Releases Revolutionary Edge Models Ministral 3B and 8B: Superior Performance and Privacy","description":"No subtitle provided","date":"2024-10-31T08:32:15.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*zFNeFlbfEnbjV5M65sH5ig@2x.jpeg","categories":["Technology","Machine Learning","Autonomous Systems"],"author":"Rifx.Online","tags":["edge","models","privacy","tokens","attention"],"draft":false,"slug":"blog/mistral-ai-releases-revolutionary-edge-models-ministral-3b-and-8b-superior-performance-and-privacy-5b24f0189493"},"content":"\n\n\n\nRecently, Mistral AI has launched two new edge models â€” Ministral 3B and Ministral 8B, which have garnered widespread attention in the tech community. These models not only excel in performance but also offer unique advantages in privacy protection.\n\n\n\n\n## Exceptional Performance, Privacy First\n\nMinistral 3B and 8B are designed specifically for on\\-device computation, capable of processing text information up to 128k in length. Particularly, Ministral 8B employs an innovative sliding window attention mechanism, significantly enhancing computational speed and memory efficiency. Moreover, both models prioritize privacy protection in their design, ensuring data is processed locally to reduce the risk of data breaches.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GMgT6erSorAGUp-pqbXWhA@2x.jpeg)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*zRGh7rw7oVXYd5mOhXoc3g@2x.jpeg)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*IIYgXVtbHvWqn6QLSZ-0Ow@2x.jpeg)\n\n\n## Versatile Applications, Unlimited Potential\n\nThe Ministral series models have a wide range of applications. In the field of smart assistants, they can quickly respond to user commands while ensuring data security; in the field of autonomous robots, their powerful reasoning capabilities support complex decision\\-making and operations.\n\n\n## Cost\\-Effective, Broad Market Prospects\n\nDespite their outstanding performance, Ministral 3B and 8B are highly competitive in price. The 3B is priced at $0\\.04 per million tokens, and the 8B at $0\\.10\\. This pricing strategy provides enterprises and developers with a cost\\-effective option. Currently, both models are available for use.\n\n\n## Promising Future, Leading the New Trend in Edge Computing\n\nThe release of the Ministral series models by Mistral AI demonstrates its deep technical strength in edge computing, laying a solid foundation for future on\\-device AI applications. With technological advancements and deeper application exploration, Ministral models are expected to play a greater role in smart devices and the Internet of Things.\n\nIn summary, the launch of Ministral 3B and 8B is not only a significant milestone for Mistral AI but also a major advancement for the AI industry, bringing new possibilities to on\\-device computation.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*A6SToo3fO3DqnlWX)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/mistral-ai-unveils-ministral-3b-and-8b-models-plus-nvidia-launches-ai-model-that-outperforms-gpt-4-941712f5d22d","frontmatter":{"title":"Mistral AI Unveils Ministral 3B and 8B Models Plus: Nvidia Launches AI Model that Outperforms GPT-4","meta_title":"Mistral AI Unveils Ministral 3B and 8B Models Plus: Nvidia Launches AI Model that Outperforms GPT-4","description":"No subtitle provided","date":"2024-10-31T08:29:07.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*PtPEkgjabwBUu73Y","categories":["Technology","Generative AI","Machine Learning"],"author":"Rifx.Online","tags":["Mistral","edge","Llama","YouTube","DreamTracks"],"draft":false,"slug":"blog/mistral-ai-unveils-ministral-3b-and-8b-models-plus-nvidia-launches-ai-model-that-outperforms-gpt-4-941712f5d22d"},"content":"\n\n\n\n\n### Plus: Nvidia Launches AI Model that Outperforms GPT\\-4\n\n\n\n**Welcome to Get The Gist**, where every weekday we share an easy\\-to\\-read summary of the latest and greatest developments in AI â€” news, innovations, and trends â€” all delivered in under 5 minutes! â±\n\n**In todayâ€™s edition:**\n\n* Mistral AI Unveils Ministral 3B and 8B Models for Edge Computing\n* Nvidia Quietly Launches AI Model that Outperforms GPT\\-4\n* YouTube Rolls Out AI Music Tool â€œDream Tracksâ€ to U.S. Creators\n* Google Gemini Can Now Generate Images in Customizable Aspect Ratios\n* And more AI newsâ€¦.\n\n\n## 1\\. Mistral AI Unveils Ministral 3B and 8B Models for Edge Computing\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*qAjoYMHGI1TkNy_A)\n\n**The Gist:** Mistral AI has [**launched two new AI models**](https://analyticsindiamag.com/ai-news-updates/mistral-ai-launches-ministral-3b-and-8b-models-for-edge-computing/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models), Ministral 3B and 8B, designed for efficient on\\-device and edge computing. These models outperform competitors and are tailored for tasks requiring privacy\\-first, local inference.\n\n**Key Details:**\n\n* Models handle large context lengths (up to 128k) for smooth performance in resource\\-limited environments.\n* Ideal for applications like smart assistants, local analytics, and robotics, enhancing task efficiency.\n* Available for commercial use with competitive pricing and research access for the 8B Instruct model.\n* Outperforms AI models like Gemma 2 and Llama 3 in benchmarks.\n\n\n## 2\\. Nvidia Quietly Launches AI Model that Outperforms GPT\\-4\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Mza84SHereM3w5rN)\n\n**The Gist:** Nvidia has [**released a new AI model**](https://venturebeat.com/ai/nvidia-just-dropped-a-new-ai-model-that-crushes-openais-gpt-4-no-big-launch-just-big-results/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models), Llama\\-3\\.1\\-Nemotron\\-70B\\-Instruct, which surpasses industry giants like OpenAIâ€™s GPT\\-4 in performance benchmarks. This launch marks a significant expansion of Nvidiaâ€™s AI strategy, shifting from hardware to high\\-performing AI software.\n\n**Key Details:**\n\n* Nvidiaâ€™s new model scored higher than GPT\\-4 on key benchmarks, demonstrating superior language understanding and generation.\n* Developed using advanced techniques like Reinforcement Learning from Human Feedback (RLHF), the model excels in handling complex queries.\n* Nvidia offers free access through its platform, allowing businesses to experiment with this powerful AI tool.\n* The model is customizable for business needs but requires careful use in specialized areas like legal reasoning or math.\n\n\n## 3\\. YouTube Rolls Out AI Music Tool â€œDream Tracksâ€ to U.S. Creators\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*5nUNrJmdCBBy4JdQ)\n\n**The Gist:** YouTube has launched its [**AI\\-powered music generator**](https://www.mediapost.com/publications/article/400280/youtube-brings-ai-audio-generator-to-us-creators.html?edition=136037&utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models) â€œDream Tracksâ€ in the U.S., allowing creators to use text prompts to create custom audio for their short\\-form videos. The tool aims to deepen the connection between artists and fans through music creation.\n\n**Key Details:**\n\n* Powered by Google DeepMindâ€™s Lyria, Dream Tracks generates custom instrumental soundtracks for YouTube Shorts.\n* U.S. creators can now use this tool to create royalty\\-free soundtracks up to 30 seconds long.\n* Users can remix the AI\\-generated audio clips, enhancing creative possibilities.\n* YouTube applies a hidden SynthID watermark to all AI\\-generated tracks to ensure transparency.\n\n\n## Quick Gist\n\n* **Clerk Chat** secured $7 million in funding led by Race Capital to enhance its AI\\-powered business communication platform [(Read More)](https://www.businesswire.com/news/home/20241017292794/en/World%E2%80%99s-First-AI-Telecom-Clerk-Chat-Raises-7.0-Million-in-Seed-Funding?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models).\n* **Anthropic** CEO Dario Amodei published a lengthy blog outlining a utopian vision for the transformative potential of artificial general intelligence, while simultaneously seeking to secure a $40 billion valuation for the company [(Read More)](https://www.theverge.com/2024/10/16/24268209/anthropic-ai-dario-amodei-agi-funding-blog?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models).\n* **Google Cloud** announced the general availability of its upgraded Vertex AI platform and Healthcare Data Engine to enhance AI applications in healthcare [(Read More)](https://www.forbes.com/sites/saibala/2024/10/17/google-cloud-announces-general-availability-of-vertex-ai-for-healthcare/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models).\n* **Amazon** led a $500 million funding round for X\\-energy to roll out 5GW of small nuclear reactors by 2039, while **Google** partnered with Kairos Power to install 500MW of SMRs by 2035, both aiming to meet rising energy demands from data centers with clean power [(Read More)](https://www.theengineer.co.uk/content/news/amazon-and-google-bet-big-on-smrs-to-power-ai?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models).\n* **Google** is launching its Gemini AI models for public sector agencies within Google Distributed Cloud in early 2025, along with funding to upskill the government workforce in responsible AI practices [(Read More)](https://siliconangle.com/2024/10/16/google-looks-spearhead-ai-adoption-public-sector/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models).\n* **Google**â€™s Gemini AI chatbot is set to introduce a feature allowing users to generate images in customizable aspect ratios, enhancing its image editing capabilities [(Read More)](https://indianexpress.com/article/technology/artificial-intelligence/google-gemini-may-soon-get-new-image-resizing-feature-9623756/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models).\n\nThatâ€™s it for today, see you tomorrow! ðŸ‘‹\n\nIf you enjoyed this update and want to stay informed about the latest developments in AI, consider subscribing to ***Get The Gist*** on Medium for more insights and analyses.\n\n**Want to dive even deeper?** Subscribe to our free daily email newsletter for quick, concise updates straight to your inbox, so you never miss an important development. You can sign up by clicking [here](https://getthegist.beehiiv.com/).\n\nJoin us as we explore the world of AI together â€” one gist at a time! ðŸ’¡ðŸ¤–\n\n\n"},{"lang":"en","group":"blog","slug":"blog/mojo-90-000-times-faster-than-python-finally-open-sourced-777bdd9a1896","frontmatter":{"title":"Mojo, 90,000 Times Faster Than Python, Finally Open Sourced!","meta_title":"Mojo, 90,000 Times Faster Than Python, Finally Open Sourced!","description":"On March 29, 2024, Modular Inc. announced the open sourcing of the core components of Mojo.","date":"2024-11-10T22:36:54.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*jcayumihC6jn5q_0","categories":["Programming","Machine Learning","Data Science"],"author":"Rifx.Online","tags":["Mojo","Python","MLIR","SIMD","open-source"],"draft":false,"slug":"blog/mojo-90-000-times-faster-than-python-finally-open-sourced-777bdd9a1896"},"content":"\nOn March 29, 2024, Modular Inc. announced the open sourcing of the core components of Mojo.\n\nMojo is a programming language designed specifically for writing artificial intelligence software, officially launched in August of last year. It has since amassed over 175,000 developers and 50,000 organizations.\n\nArtificial intelligence models are often written in multiple programming languages. Developers typically use Python to implement the simplest parts of neural networks, as it is easy to learn but relatively slow. The remaining code is often written in C\\+\\+, which is faster but more complex to learn.\n\nModular positions Mojo as a more convenient alternative. It offers an easy\\-to\\-use syntax similar to Python but with the potential for thousands of times faster execution speed. Therefore, developers can write fast AI models without needing to learn complex languages like C\\+\\+.\n\n\n\nLast year, when Mojo was introduced, some developers expressed excitement about its emergence. However, when asked about the open\\-source date, Chris Lattner said on Discord, â€œIf I knew, Iâ€™d tell you.â€ For about a year, many developers have been in a state of observation and questioning:\n\n> â€œThe promotion is great, but if itâ€™s not open source, I wonâ€™t spend any time trying it.â€\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*rIJiJylh4-mWBiqz)\n\n> â€œItâ€™s clearly an overhyped programming language, and itâ€™s not open source! Chris Lattner wants to deceive millions of Python developers!â€\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*0u5HDKseL0Gy_-8A)\n\n> â€œI canâ€™t spend time on a language that might or might not be open source, especially considering the current commercial environment of OSSâ€¦â€\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*wrTO7fbKfBZOpBxF)\n\nNow, Mojo is finally open source! And within a short period, it has already reached 17\\.6k stars and has 2\\.1k forks!\n\n## 01 The First Step of Mojoâ€™s Open Source Journey\n\nModular announced today the open sourcing of the core components of Mojoâ€™s standard library. The standard library constitutes the core part of a programming language, containing basic syntax elements and essential features. Mojoâ€™s standard library includes functionalities for optimizing AI hyperparameters, which determine how neural networks process data.\n\nâ€œThe Mojo standard library is still undergoing vigorous development and rapid changes, so we are open sourcing its core modules first. This marks an important starting point for our open source journey, not the end.â€\n\nThe company states that open sourcing will enable them to gather feedback from more developers, facilitating the better development of Mojo. Moreover, there are various ways to open source projects: some projects provide source code but do not accept contributions; some offer opaque contribution processes, making it difficult to understand goals and roadmaps; and some, though open source, are not actively maintained. Modular states that they have chosen a more thorough approach to open source: allowing external contributions via GitHub pull requests, encouraging developers to participate in Mojoâ€™s development and improvement, and fostering community growth.\n\nFurthermore, Modular demonstrates sincerity by sharing the complete commit history, starting from the initial commit! Openly revising the history of the open standard library allows developers to track the evolution of the code and better understand its meaning.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*0-FqkfLUTevloPjI)\n\nIn addition, they will release nightly builds of the Mojo compiler, facilitating developers to quickly try out the latest compiler features and undergo continuous integration testing.\n\nAt the end of last year, Modular launched the commercial AI platform MAX, which is a unified set of tools and libraries for building high\\-performance AI applications that can be efficiently deployed across multiple hardware platforms, such as running AI applications in Kubernetes environments. Today, the company revealed that they also plan to open source some components of MAX in the future.\n\nMoreover, it is worth mentioning that they have chosen the Apache 2 LLVM license for open sourcing.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*dgVCSxaCq6onY2uP)\n\nThis is a customized version of the Apache 2 license. Additionally, to facilitate integration with software following the GPL2 license, Modular has made corresponding adjustments. GPL2 is another popular open source license, famously used by projects like the Linux kernel. In the announcement blog post, Modular wrote:\n\n> â€œThe Apache 2 license is a good starting point, but our experience with using licenses in the LLVM project tells us that it has two minor issues. Some are concerned that the Apache 2 license may not mix well with GPL2 code (e.g., the Linux kernel), and the Apache 2 license requires you to acknowledge the use of the code in derivative projects. We hope you can use Mojo without being forced to acknowledge Modular or Mojo. Therefore, we have added LLVMâ€™s specially designed exceptions to address these issues.â€\n\n## 02 The Best Language for AI Programming in the Next 50 Years?\n\nLast May, when Mojo was just released, Modular claimed that it was 35,000 times faster than raw Python when running algorithms like Mandelbrot.\n\nIn September last year, Modular once again stated, â€œMojo combines the advantages of dynamic and static languages, boosting performance to 68,000 times that of Python.â€\n\nIn October last year, when Mojo landed on Mac, Modular raised the performance comparison data again: â€œ90,000 times faster than Python.â€\n\nSpeaking of Mojo, Modularâ€™s founder and CEO Chris Lattner said, â€œYou can think of Mojo as a member of the Python family, drawing on all these cool languages, compilers, and other technologies, taking Python a big step forward. We believe it enhances Pythonâ€™s capabilities, gives Python programmers superpowers, allows those familiar with Python to learn new knowledge, explore, and conquer new fields without switching to C\\+\\+.â€\n\nMojo is based on the latest compiler technology in MLIR, which is an evolution of LLVM, hence better performance. As long as programmers have the requisite skills and a willingness to optimize fully, they can make the code run extremely fast. The goal of the Mojo language is to meet the needs of Python developers while providing a range of new code optimization techniques to fully exploit the performance limits of hardware devices.\n\nOn the other hand, the Mojo team highly appreciates Rust and openly states that â€œMojoâ€™s design is also greatly inspired by Rust.â€\n\nIn terms of performance, Modular has made many comparisons with Python to provide a clear comparison, but people do not have a concept of how much faster it is than Rust. Just last month, they specifically responded to the question of â€œwhether Mojo is faster than Rust.â€\n\nIn February of this year, Netflix engineer and Rust advocate @ThePrimeagen released a video: parsing DNA sequences with Mojo at a speed surpassing Rust by 50%. This blog post has sparked a lot of attention and discussion, after all, Rust is positioned as a potential successor to Python and C\\+\\+ as the dominant language in the AI field.\n\n@ThePrimeagenâ€™s outlook for Mojo and Rust in AI programming:\n\n> If Mojo officially enters the fray, then I believe Mojo will undoubtedly emerge victorious. The reason Mojo will win is that it doesnâ€™t require any changes to the paradigms developers are already familiar with. With just a little learning, you can achieve amazing performance. First of all, Mojo compiles quickly, and the user experience is very similar to languages everyone is already familiar with, with performance comparable to Rust. The only question is how to get more people to accept it.\n\nAfter making the comment, Luca Palmieri, a respected Rust contributor and author of â€œRust: From Zero to Production,â€ responded on X:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Hqe7bPWGI36LPGzE)\n\nRust boasts top\\-notch ergonomic design in the realm of systems programming, but it faces two major issues in the field of AI applications:\n\n* Slow compilation speed, while AI emphasizes experimentation and rapid iteration.\n* Most AI researchers with experience in Python are reluctant to invest time in learning a new language from scratch.\n\nMojo aims to make it intuitive and easy for Python developers to grasp. As demonstrated by Mohamed, he learned Mojo and utilized SIMD optimization algorithms in just a few weeks as a hobby project (the initial implementation only took 200 lines of code).\n\nFor those interested in AI development, there is indeed a dilemma of choosing one of the three languages available.\n\nBoth Mojo and Rust allow developers to optimize at a lower level. For Rust, developers can certainly pack everything into Arc, Mutex, or Box to avoid conflicts with the borrow checker, but this may sacrifice some performance. While this performance difference might not have a significant impact on application code, it can quickly add up in libraries or other performance\\-sensitive code. The choice between the two depends on the programmerâ€™s focus on reducing overhead and optimizing performance.\n\nBoth languages can utilize LLVM for optimizing code generation and allow the use of inline assembly (although itâ€™s unlikely anyone would actually do so), so theoretically, both have similar performance potential on traditional hardware.\n\n## 03 Based on the Most Advanced Compiler Technology\n\nRust was initiated in 2006, while Swift emerged in 2010, with both primarily built on LLVM IR. Mojo, on the other hand, debuted in 2022, constructed upon MLIR â€” a more modern â€œnext\\-generationâ€ compiler stack compared to LLVM IR used by Rust. Itâ€™s worth noting that Chris Lattner founded LLVM in December 2000 during his university days, learning a great deal from its evolution over the years. He later joined Google to lead the development of MLIR, aimed at supporting the companyâ€™s TPU and other AI accelerator projects. Subsequently, he continued his exploration based on the knowledge gained from LLVM IR.\n\nModular states that Mojo is the first programming language to fully leverage the advanced features of MLIR. It can generate CPU code with higher optimization and also supports GPU and other accelerators, with significantly faster speeds than Rust. This is an advantage currently unachievable by other languages, and a core reason why AI and compiler enthusiasts are enthusiastic about Mojo.\n\nThey particularly emphasize two aspects:\n\nOutstanding SIMD ergonomic design: CPUs process multiple data elements simultaneously through special registers and instructions, known as SIMD (Single Instruction, Multiple Data). However, historically, the experience of writing such code has been ugly and challenging to use from an ergonomic standpoint. Although these special instructions have existed for years, most code has not been optimized for them. Therefore, whoever can solve this complexity and write portable SIMD optimization algorithms can stand out in the market, such as simd\\_json.\n\nMojoâ€™s primitives are designed with SIMD priority from the outset: UInt8 is actually a SIMD\\[DType.uint8, 1], representing a SIMD with 1 element. This representation incurs no performance overhead while allowing programmers to easily use it for SIMD optimization. For example, text can be split into 64\\-byte blocks, represented as SIMD\\[DType.uint8, 64], and then compared with a single newline character to find the index of each newline. As SIMD registers on machines can perform operations on 512\\-bit data simultaneously, this operation can boost the performance of such operations by 64 times!\n\nOr to give a simpler example, suppose you have a SIMDDType.float64, 8\\. By simply multiplying it by Float64(2\\), you can easily improve performance. Compared to individually multiplying each element, this method can improve performance by up to 8 times on most machines.\n\nLLVM (also used by Rust) has automatic vectorization optimization passes, but due to its inability to change the memory layout of SIMD and other important details, its performance never reaches the theoretical level of optimization. However, Mojo was designed with SIMD features in mind from the beginning, so the experience of writing SIMD optimizations is very similar to writing regular code.\n\nEager Destruction: Rustâ€™s design is inspired by C\\+\\+â€™s RAII (Resource Acquisition Is Initialization), meaning that once objects go out of scope, application developers donâ€™t need to worry about releasing memory â€” the programming language itself handles it. This is a very good example that avoids the performance pitfalls of garbage collection while ensuring dynamic language ergonomics.\n\nMojo goes further by not waiting for the end of the scope but releasing memory when the object is last used. This is very beneficial for AI scenarios because releasing objects early means releasing GPU tensors early, allowing for larger models to be fitted in equivalent GPU RAM. This is Mojoâ€™s unique advantage, allowing programmers to achieve optimal performance without having to design it themselves. Rustâ€™s borrow checker initially extends the lifetime of everything to the end of its scope, matching the behavior of destructor functions, but this can lead to some confusing consequences for users. Rust later added some non\\-lexical lifetime features to simplify the work of developers. However, with Mojoâ€™s eager destructor mechanism, this simplification effect can be directly achieved, and it remains consistent with how objects are actually destroyed, thus avoiding confusing extreme cases.\n\nAnother overhead in Rust comes from the implementation of Drop. It uses Drop Flags to track whether objects should be deleted at runtime. Rust is able to optimize in certain situations, but Mojo can eliminate all extra overhead through explicit definitions.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*0VcMppg3rDTqfsMY)\n\nRegardless, developers must choose between the ease of use of Mojo and Python, and the high performance of C, C\\+\\+, or Rust. In response, the Mojo team calls out to developers, saying, â€œIf youâ€™re curious and looking towards the future, hoping to master a language that may benefit AI development in the next 50 years, why not give Mojo a chance?â€\n\n\n"},{"lang":"en","group":"blog","slug":"blog/multi-agent-hedge-fund-simulation-with-langchain-and-langgraph-64060aabe711","frontmatter":{"title":"Multi-Agent Hedge Fund Simulation with LangChain and LangGraph","meta_title":"Multi-Agent Hedge Fund Simulation with LangChain and LangGraph","description":"This project demonstrates how to use a multi-agent setup to simulate a hedge fundâ€™s analytical process. It showcases a practical way toâ€¦","date":"2024-11-10T03:51:17.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*i8wneK22YezD7zOhPKvZfg.png","categories":["Finance","Programming","Data Science"],"author":"Rifx.Online","tags":["multi-agent","LangChain","LangGraph","FinancialDatasets","predictive"],"draft":false,"slug":"blog/multi-agent-hedge-fund-simulation-with-langchain-and-langgraph-64060aabe711"},"content":"\n\n### Multi\\-Agent Hedge Fund Simulation with LangChain and LangGraph\n\n\n\nThis project demonstrates how to use a multi\\-agent setup to simulate a hedge fundâ€™s analytical process. It showcases a practical way to build a system that uses AI agents to gather and analyze financial data, a setup that could be scaled and customized further. Here, Iâ€™ll break down the project, which involves a portfolio manager and three analyst agents (fundamental, technical, and sentiment), each assigned specific roles in gathering and processing stock data.\n\nThe goal of this project is not to build a comprehensive trading algorithm but rather to illustrate how various types of data can be organized and analyzed in parallel with specialized agents using LangChain and LangGraph.\n\n\n### Project Structure and Agent Overview\n\nThis agent system includes:\n\n1. **Portfolio Manager** â€” Delegates tasks to analysts and aggregates their findings.\n2. **Fundamental Analyst** â€” Fetches and analyzes financial statements, such as income statements.\n3. **Technical Analyst** â€” Collects stock price data over specified timeframes.\n4. **Sentiment Analyst** â€” Looks at insider trading and news data, providing sentiment insights.\n\nEach agent is designed to specialize in a specific data retrieval task, allowing for modular and scalable analysis. By using LangChain for agent functionality and LangGraph for managing parallel workflows, we can quickly process multiple data sources. The FinancialDatasets API provides a rich source of data with over 30,000 stock tickers, enabling comprehensive analysis.\n\n\n### Key Libraries and Setup\n\nLangChain and LangGraph enable easy handling of multi\\-agent workflows and branching logic for parallel processing. The setup begins by installing required libraries and securing API keys:\n\n\n```python\n%%capture --no-stderr\n%pip install -U langgraph langchain langchain_openai langchain_experimental langsmith pandas\n```\nEnvironment variables are used to store sensitive data, like API keys:\n\n\n```python\nimport getpass\nimport os\n\ndef _set_if_undefined(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"Please provide your {var}\")\n\n_set_if_undefined(\"OPENAI_API_KEY\")               # https://platform.openai.com\n_set_if_undefined(\"FINANCIAL_DATASETS_API_KEY\")   # https://financialdatasets.ai\n_set_if_undefined(\"TAVILY_API_KEY\")               # https://tavily.com\n```\n\n### Agent Functions: Retrieving Data\n\nEach agent in the system is designed to handle specific types of data relevant to stock analysis.\n\n\n### 1\\. Fundamental Analyst\n\nThe Fundamental Analyst retrieves and examines financial statements, which offer insights into a companyâ€™s financial health. Below is the tool for getting income statements, a key financial document:\n\n\n```python\nfrom langchain_core.tools import tool\nfrom typing import Dict, Union\nfrom pydantic import BaseModel, Field\n\nclass GetIncomeStatementsInput(BaseModel):\n    ticker: str = Field(..., description=\"The ticker of the stock.\")\n    period: str = Field(default=\"ttm\", description=\"Valid values are 'ttm', 'quarterly', or 'annual'.\")\n    limit: int = Field(default=10, description=\"Maximum number of income statements to return.\")\n\n@tool(\"get_income_statements\", args_schema=GetIncomeStatementsInput, return_direct=True)\ndef get_income_statements(ticker: str, period: str = \"ttm\", limit: int = 10) -> Union[Dict, str]:\n    api_key = os.environ.get(\"FINANCIAL_DATASETS_API_KEY\")\n    url = f'https://api.financialdatasets.ai/financials/income-statements?ticker={ticker}&period={period}&limit={limit}'\n    try:\n        response = requests.get(url, headers={'X-API-Key': api_key})\n        return response.json()\n    except Exception as e:\n        return {\"ticker\": ticker, \"income_statements\": [], \"error\": str(e)}\n```\nHere, `get_income_statements` retrieves the income statements for a given stock ticker. By specifying the period (e.g., â€œttmâ€ for trailing twelve months), the agent can focus on different reporting cycles.\n\n\n### 2\\. Technical Analyst\n\nThe Technical Analyst collects stock price data over defined timeframes. This data can later be used to calculate indicators or recognize patterns. Below is the code to retrieve stock prices:\n\n\n```python\nclass GetPricesInput(BaseModel):\n    ticker: str\n    start_date: str\n    end_date: str\n    interval: str = \"day\"\n    interval_multiplier: int = 1\n    limit: int = 5000\n\n@tool(\"get_stock_prices\", args_schema=GetPricesInput, return_direct=True)\ndef get_stock_prices(ticker: str, start_date: str, end_date: str, interval: str, interval_multiplier: int = 1, limit: int = 5000) -> Union[Dict, str]:\n    api_key = os.environ.get(\"FINANCIAL_DATASETS_API_KEY\")\n    url = (\n        f\"https://api.financialdatasets.ai/prices?ticker={ticker}\"\n        f\"&start_date={start_date}&end_date={end_date}\"\n        f\"&interval={interval}&interval_multiplier={interval_multiplier}\"\n        f\"&limit={limit}\"\n    )\n    try:\n        response = requests.get(url, headers={'X-API-Key': api_key})\n        return response.json()\n    except Exception as e:\n        return {\"ticker\": ticker, \"prices\": [], \"error\": str(e)}\n```\nThis function allows us to specify parameters like date range and interval, giving control over the granularity of the data (e.g., daily or hourly).\n\n\n### 3\\. Sentiment Analyst\n\nThe Sentiment Analyst pulls in data on insider trading and relevant news. Insider trades and public sentiment indicators can offer insights into market perception, which is important for assessing stock volatility and potential price movements.\n\n\n```python\nclass GetInsiderTradesInput(BaseModel):\n    ticker: str\n    limit: int = 10\n\n@tool(\"get_insider_trades\", args_schema=GetInsiderTradesInput, return_direct=True)\ndef get_insider_trades(ticker: str, limit: int = 10) -> Union[Dict, str]:\n    api_key = os.environ.get(\"FINANCIAL_DATASETS_API_KEY\")\n    url = f'https://api.financialdatasets.ai/insider-transactions?ticker={ticker}&limit={limit}'\n    try:\n        response = requests.get(url, headers={'X-API-Key': api_key})\n        return response.json()\n    except Exception as e:\n        return {\"ticker\": ticker, \"insider_transactions\": [], \"error\": str(e)}\n```\nBy capturing insider trades, this tool can track moves made by those with privileged information, which might be early indicators of performance changes.\n\n\n### Portfolio Manager: Coordinating and Summarizing Analysis\n\nThe Portfolio Manager serves as the coordinator, delegating tasks to the analysts and compiling their results into a single report. Below is a sample workflow for the Portfolio Manager that demonstrates how it calls each agent:\n\n\n```python\nfrom langchain_community.tools.tavily_search import TavilySearchResults\n\n## Tools grouped by agent type\nfundamental_tools = [get_income_statements]\ntechnical_tools = [get_stock_prices]\nsentiment_tools = [get_insider_trades, TavilySearchResults(max_results=5)]\n\n## Sample function for running all analyses in parallel\ndef analyze_portfolio(ticker: str):\n    # Delegate tasks to each agent\n    fundamentals = [tool(ticker=ticker) for tool in fundamental_tools]\n    prices = [tool(ticker=ticker, start_date=\"2023-01-01\", end_date=\"2023-12-31\") for tool in technical_tools]\n    sentiment = [tool(ticker=ticker) for tool in sentiment_tools]\n    \n    # Summarize results (simplified)\n    summary = {\n        \"fundamentals\": fundamentals,\n        \"technical\": prices,\n        \"sentiment\": sentiment\n    }\n    return summary\n```\nIn this function:\n\n* Each agentâ€™s functions are called in parallel to gather data for the specified ticker.\n* The manager then compiles the data from each agent into a single summary for easy review.\n\n\n### Conclusion\n\nThis project provides a basic yet flexible setup for analyzing stock data through a team of specialized agents. By splitting tasks among a Portfolio Manager, Fundamental Analyst, Technical Analyst, and Sentiment Analyst, weâ€™re able to gather and organize insights across different financial data types. Using LangChain and LangGraph for modularity and parallel processing makes this approach scalable, while Financial Datasets API supports a broad range of tickers, enabling robust data access.\n\nWhile this system is designed as a project for practice, its structure can serve as a foundation for more complex hedge fund simulations or data analytics tools. Next steps might include enhancing each agent with additional tools or data analysis techniques, such as:\n\n* **Technical Patterns and Indicators:** Integrating more technical analysis tools like moving averages or trend lines.\n* **Sentiment Scoring:** Automating sentiment scoring from news sources or insider trading data.\n* **Predictive Modeling:** Adding ML models that can make buy/sell recommendations based on the combined data.\n\nThis setup is a useful prototype for modular financial data analysis, with plenty of room for future customizations and improvements.\n\nFor those interested in the code behind this toolkit, you can find the complete implementation on GitHub [*here*](https://github.com/shaikhmubin02/ai-hedge-fund).\n\n\n"},{"lang":"en","group":"blog","slug":"blog/multimodal-ai-for-conversational-human-motion-3102e991938c","frontmatter":{"title":"Multimodal AI for Conversational Human Motion","meta_title":"Multimodal AI for Conversational Human Motion","description":"Multimodal AI is revolutionizing conversational agents by integrating input perception, motion planning, and avatar rendering to enhance human-like interactions. This approach reduces information loss between these layers, enabling avatars to process multimodal cues from visual, auditory, and text sources for fluid conversations. Key challenges include aligning modalities, managing latency, and maintaining personality consistency. Current applications span healthcare, customer support, and education, with potential for further development in complex environments, enhancing empathetic communication and information flow.","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*zANW8t-IxPlkyxX-5_9Ayw.png","categories":["Chatbots","Autonomous Systems","Natural Language Processing"],"author":"Rifx.Online","tags":["multimodal","perception","avatar","latency","empathy"],"draft":false,"slug":"blog/multimodal-ai-for-conversational-human-motion-3102e991938c"},"content":"\n\n\n\nWritten by [Christian Safka](https://www.linkedin.com/in/christiansafka/) and [Keyu Chen](https://www.linkedin.com/in/keyu-chen-3a3026143/?locale=en_US)\n\n\n\nIn this exploration weâ€™ll look at how multimodal models are changing the game for conversational AI agents, and how we can enable seamless interaction in various environments using perception, memory, behavior modeling, and rendering in real\\-time.\n\nThe outline of this one\\-pager:\n\n* Why multimodal?\n* Deep dive into human motion pipeline\n* Challenges in training\n* Current use cases and the future\n\n\n## Why multimodal?\n\nFrom a high level, the three â€œlayersâ€ we need to achieve life\\-like human conversation are input perception, motion planning, and avatar rendering. Most of the pipelines in academia as of writing this have separated these layers with text as an intermediate:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*4a8JvOVbsP8mY3AjiPgNPA.png)\n\nWhat multimodal models unlock is a decrease in information loss between these layers:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*VUFhrwLA7sUFmHwidb7DWg.png)\n\n\n## Deep dive into the human motion pipeline\n\nGenerating human\\-like actions and reactions is a difficult problem. It requires a pipeline to process real\\-time cues from multiple sources, interpret, translate, and generate synchronized responses. Itâ€™s critical that all stages All stages are critical for creating avatars that can engage in fluid, contextual conversations.\n\nWe talked about the three layers:\n\n1\\. **Input Perception** â€” Gathering multimodal cues from visual, auditory, and text\\-based sources.\n\n2\\. **Motion Planning** â€” Determining appropriate actions or reactions based on these inputs.\n\n3\\. **Avatar Output** â€” Rendering these planned actions with an avatar in real\\-time.\n\nNow lets break down how each layers crucial role in creating life\\-like conversations.\n\n**Perception of Multimodal Input**\n\nEffective human motion synthesis begins with understanding multimodal cues, much like humans rely on sight, sound, and language for communication. In digital applications, this process can replicate the complex ways in which humans gather and respond to information:\n\n* **Visual Inputs**: Images and video streams capture elements like facial expressions, gaze direction, and hand gestures\n* **Auditory Inputs**: Audio signals provide essential information, such as tone, intonation, and rhythm, enabling us to interpret the emotional context of speech\n* **Text Inputs**: Text\\-based prompts or conversation logs can guide the avatarâ€™s actions by providing semantic context â€” knowing whatâ€™s being discussed allows the avatar to respond appropriately to the nuances of conversation\n\nIntegrating these modalities creates a holistic understanding of the conversational setting, providing a foundation for how the system interprets and maps the world.\n\n**Motion Planning with LLMs**\n\nIn multimodal AI, the **interaction layer** â€” often powered by large language models (LLMs) â€” acts as the avatarâ€™s â€œbrain.â€ This layer processes the synthesized multimodal cues from the perception stage, determines the most contextually relevant response, and translates it into a planned motion or verbal response.\n\nUsing both speech and visual features as input allows the model to handle:\n\n* **Contextual Motion Planning**: The model can pick up on conversational cues, matching them to actions that are contextually appropriate. For instance, if an avatar detects enthusiasm in a userâ€™s speech, it might adopt an open, engaging posture or facial expression\n* **Sequential Interaction Control**: The model can learn to interpret sequences of cues, allowing it to handle nuances like turn\\-taking, active listening gestures, and pauses, which are all critical for a natural conversation\n\nPrevious works such as Zhou et al. \\[0] or Pereira et al. \\[1] would output text from this layer â€” emotion labels like â€œhappyâ€ which can be used for conditional expression generation. This is very lossy and the expressions will never be fully aligned with the output speech.\n\nThe beauty of multimodality in motion planning is both in input and output. On the input side, we can draw on the vast language modelâ€™s world knowledge, even as it is trained to align multimodal tokens. On the output side, we can reduce the information loss between desired behavior and final rendered output.\n\nTo summarize, the interaction layer enables the avatar to respond to both explicit and implicit conversational cues, bridging the gap between multimodal perception and human\\-like interaction.\n\n**Avatar Generation**\n\nTo achieve empathic conversational AI or a human\\-level information flow, the rendered actions and reactions need to go beyond static, pre\\-planned motions. The goal is to create a system that can interpret and adjust to subtle conversational cues almost instantaneously.\n\nIn this context, the **avatar layer** acts as the output rendering mechanism. It takes the actions planned by the interaction layer and translates them into smooth, real\\-time behaviors. This layer focuses on **low\\-latency response generation**, prioritizing rapid and accurate alignment between desired actions and visual/audio output.\n\nThe primary objective can be described as **Synchronized Speech and Motion â€”** the avatar must coordinate facial expressions, body language, and lip movements using the auditory output and behavioral signals, ensuring that all elements stay in sync.\n\n\n\n\n\n\n\nMaintaining temporal consistency and synchronization is vital, as any delays or mismatches in behavior can quickly break immersion.\n\n\n## Challenges in training\n\nSome of the active R\\&D areas in industry and academia are:\n\n* **Token Alignment Across Modalities**: Aligning modalities like visual cues and audio intonations without losing contextual or semantic meaning is complex, and the model must learn how to represent them in a unified way for consistent responses\n* **Latency Management**: Real\\-time responsiveness requires the entire multimodal pipeline to operate with low latency, which becomes challenging as complexity increases\n* **Personality and Memory**: For avatars, consistent personality traits are essential, especially in prolonged interactions. Proper handling of memory and personality can be essential for maintaining coherent responses in some use\\-cases\n\n\n## Current use\\-cases and the future\n\nFirst, a few examples of current use cases weâ€™re seeing:\n\n* **Healthcare**: Imagine an empathetic avatar as a virtual health coach that provides guidance, responds in real\\-time, and adapts its tone and expressions to suit the userâ€™s mood\n* **Customer Support**: Customer support avatars can interpret vocal cues, body language, and even view the userâ€™s technical problem as a screenshare or live video. It could additionally offer responses that feel attentive and personalized, reducing user frustration\n* **Educational Tools**: Tutors with real\\-time interaction capabilities can engage with students, display attentive gestures, and modulate their expressions to reinforce encouragement or correction\n\nAs research advances, these applications will expand, allowing digital humans to be deployed in increasingly nuanced, high\\-stakes environments. Human\\-level conversational avatars will additionally unlock both empathic use\\-cases as well as a high information flow HCI interface.\n\nIf tackling challenges like modality alignment, latency, and contextual coherence sound interesting to you â€” weâ€™re hiring! Check us out at <https://tavus.io>\n\n**References**\n\n\\[0] Zhou, Hao, Minlie Huang, Tianyang Zhang, Xiaoyan Zhu, and Bing Liu. â€œEmotional chatting machine: Emotional conversation generation with internal and external memory.â€ In *Proceedings of the AAAI conference on artificial intelligence*, vol. 32, no. 1\\. 2018\\.\n\n\\[1] Pereira, PatrÃ­cia, Helena Moniz, and Joao Paulo Carvalho. â€œDeep emotion recognition in textual conversations: A survey.â€ *Artificial Intelligence Review* 58, no. 1 (2025\\): 1â€“37\\.\n\n\n## Donâ€™t forget to give us your ðŸ‘ !\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*2lvCls4yjxVMfZSR)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/multimodal-rag-with-gemini-pro-and-langchain-e4f74170420a","frontmatter":{"title":"Multimodal RAG with Gemini Pro and LangChain","meta_title":"Multimodal RAG with Gemini Pro and LangChain","description":"Introduction","date":"2024-11-08T00:41:44.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*m2C8wrrRvhELuDiYLv4YYQ.png","categories":["Programming","Machine Learning","Computer Vision"],"author":"Rifx.Online","tags":["Gemini","LangChain","RAG","Vertex","sneaker"],"draft":false,"slug":"blog/multimodal-rag-with-gemini-pro-and-langchain-e4f74170420a"},"content":"\n\n\n## Introduction\n\nIn this tutorial, we will explore the integration of [Gemini](https://deepmind.google/technologies/gemini/#introduction) Pro and Gemini Pro Vision with the [LangChain](https://www.langchain.com/langchain) Framework for achieving Multimodal (in this case, Image) Retrieval\\-Augmented Generation (RAG). This short tutorial is suitable for both beginners and seasoned practitioners, this tutorial not only lays the foundation using Google [AI Studio](https://aistudio.google.com/) as the primary environment but also seamlessly transitions to demonstrating how these implementations can be adapted and further enhanced using [Google Cloudâ€™s Vertex AI](https://cloud.google.com/vertex-ai).\n\n## Setting the Environment\n\nFirst thing first, letâ€™s set up our environment to ensure we have all the necessary tools and libraries at our disposal.\n\nFor this we would need Langchain, Langchain Google Gen AI Package, and a Vector Store package for RAG as:\n\n```python\npip install â€” upgrade langchain langchain-google-genai â€œlangchain[docarray]â€ faiss-cpu\n```\n\nThen you will also need to provide Google AI Studio API key for the models to interact with:\n\n```python\nif \"GOOGLE_API_KEY\" not in os.environ:\n  os.environ[â€œGOOGLE_API_KEYâ€] = getpass.getpass(â€œProvide your Google API Keyâ€)\n```\n\nFor ease of use I have also written a simple function that shows the image I am working with. This simply downloads the image from the URL provided and shows the preview:\n\n```python\ndef get_image(url, filename):\n  content = requests.get(url).content\n  with open(f'/content/{filename}.png', 'wb') as f:\n  f.write(content)\n  image = Image.open(f\"/content/{filename}.png\")\n  image.show()\n  return image\n```\n\n## A Simple LLM Interaction\n\nLetâ€™s start with a very simple LLM interaction. For it we can simply call the Gemini Pro model from ChatGoogleGenerativeAI and invoke, as:\n\n```python\nllm = ChatGoogleGenerativeAI(model=â€gemini-proâ€)\nresult = llm.invoke(\"Write a ballad about Gemini Pro in around 3 sentences.\")\nprint(result.content)\n```\n\nAs a result you would get something like this:\n\n> In the realm of stars, Gemini Pro shines, A celestial beacon, defining the lines, Guiding stargazers through cosmic designs.\n\nSimilarly, you can also use it in a Chat Interface approach with System, Human message/conversation format. As:\n\n```python\nmodel = ChatGoogleGenerativeAI(model=â€gemini-proâ€, convert_system_message_to_human=True)\nprint(model([\n  SystemMessage(content=\"Answer only yes or no.\"),\n  HumanMessage(content=\"Is apple a fruit?\"),\n  ]).content)\n```\n\n## Multimodal LLM\n\nFor this tutorial I am using a very simple usecase, where I am imagining I am a Sneaker enthusiasts and basically like to find if given image of a sneaker, where I can buy that exact model in a local store nearby. For it, I have prepared a dummy Knowledge Base with some Fake information on local stores and made of specs of certain popular Sneaker Brands. Interestingly, this Knowledge base was also generated Using Gemini Pro using [Google Gemini](https://gemini.google.com/) chat interface.\n\nLetâ€™s start with a sample image:\n\n```python\nimage = get_image(<image_url>, â€œnike3â€)\nplt.imshow(image)\nplt.show()\n```\n\nAs a sample, I am considering this image of a [Nike](https://nike.com/) Sneaker.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*dNFF95lOu1SeYHOn1vFnQQ.png)\n\nNow, letâ€™s call Gemini Pro Vision model and ask it to tell us bit about this particular image. For this, you simply need to change the model name to *â€œgemini\\-pro\\-visionâ€*.\n\n```python\nllm = ChatGoogleGenerativeAI(model=â€gemini-pro-visionâ€)\nmessage = HumanMessage(\ncontent=[\n  {\n    \"type\": \"text\",\n    \"text\": \"What's in this image? provide full detail as possible.\",\n  }, # You can optionally provide text parts\n  {\"type\": \"image_url\", \"image_url\": image},\n])\nprint(\nllm.invoke([message]).content\n)\n```\n\nAnd you will get output like this:\n\n> This is a product image of a pair of Nike Air Max 95 sneakers in a tan, wheat colorway. The upper is made of mesh and suede, with a leather mudguard. The midsole is made of foam, with a visible air unit in the heel. The outsole is made of rubber, with a waffle pattern for traction.\n\n*Disclaimer: The description provided may not be accurate and reflects the modelâ€™s interpretation of the image rather than factual information pertaining to it.*\n\n## RAG using Multimodal\n\nNow, letâ€™s dive into how we can perform RAG using this multimodal approach. First thing first, lets create an information source for this RAG. For this I have written few paragraph information information on few Nike sneakers and some made up locations of local stores based in Nepal.\n\n```python\nstore_information = â€œNike Air Max Plus sneakers. They feature a brown upper with a black Nike Swoosh logo on the side and a visible Air Max unit in the heel. The sole is white.\nHere are some more details about the Nike Air Max Plus:\nStyle: TN\nRelease date: January 1, 2017\nStyle code: 852630â€“300\nOriginal retail price: $150 USD\nThe Air Max Plus, also known as the TN, is a popular Nike running shoe that was first released in 1998. It is known for its unique design, which includes a gradient upper, visible Air Max units, and a wavy outsole. The TN has been a popular shoe among sneakerheads and casual wearers alike for over two decades.\nIt features a brown upper with a black Swoosh logo and a white sole. The shoe is currently available for resale on the StockX marketplace for an average price of around $150 USD.\nNike Air Max Plus Store Location: \"Kings Way, Kathmandu, Nepal\n\n...\n\n\"\n```\n\nThen, letâ€™s create a Langchain chain, that basically fetches information provided image description regarding what Nike model it is and where one can buy it based on above information from our Knowledge base.\n\n```python\nllm_text = ChatGoogleGenerativeAI(model=â€gemini-proâ€)\ntemplate = \"\"\"\n```\n\n{context}\n\n```\n{information}\nProvide brief information and store location.\n\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\nrag_chain = (\n  {\"context\": retriever, \"information\": RunnablePassthrough()}\n  | prompt\n  | llm_text\n  | StrOutputParser()\n)\n```\n\nHere, the thing to note is *Gemini\\-Pro* and *Gemini\\-Pro\\-Vision* are 2 different models and you will need to call them differently. In above code, we are called the Gemini Pro text model that perform RAG provided the image description that was generated by *gemini\\-pro\\-vision* model.\n\nNow, lets set up a full chain that first generates image description provided the image as an input and then does RAG using above chain.\n\n```python\nllm_vision = ChatGoogleGenerativeAI(model=â€gemini-pro-visionâ€, temperature=0.0)\nfull_chain = (\n  RunnablePassthrough() | llm_vision | StrOutputParser() | rag_chain\n)\n```\n\n## Performing the RAG\n\nNow, lets do some testing on what we just set up. First, lets get another image as sample\n\n```python\nimage = get_image(url_3, â€œnike3â€)\nplt.imshow(image)\nplt.show()\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kPkfo2FKnrUR2tC18VMpjg.png)\n\nThen, lets call our RAG:\n\n```python\nmessage = HumanMessage(\n  content=[\n    {\n      \"type\": \"text\",\n      \"text\": \"Provide information on Brand and model of given sneaker.\",\n    }, # You can optionally provide text parts\n    {\"type\": \"image_url\", \"image_url\": image},\n  ])\n```\n\nNow letâ€™s see what we get:\n\n```python\nresult = full_chain.invoke([message])\ndisplay(Markdown(result))\n```\n\nAs an output, we will get something like this, which is based on our made up information source:\n\n> **Nike Offcourt Slide**Soft, one\\-piece upperPlush foam midsoleDurable rubber outsoleAvailable in a variety of colors\n\n> **Store Location:** Bhaktapur, Nepal\n\n## Using Vertex AI Models\n\nInstead of using Google AI Studio model, you can also use Google cloudâ€™s Vertex AI gemini pro models. For it, you will need basically need to first, install related packages for Vertex AI for your cloud environment and Langchain as:\n\n```python\npip install â€” upgrade google-cloud-aiplatform langchain-google-vertexai\n```\n\nThen, set up necessary config related to your cloud project using:\n\n```python\ngcloud init\n```\n\nThen, you can use Vertex AI models for your multimodal use cases as:\n\n```python\nfrom langchain_google_vertexai import VertexAI\nfrom langchain_google_vertexai import VertexAIEmbeddings\n\nmodel_vision = VertexAI(model_name=\"gemini-1.0-pro-vision-001\")\nmodel_text = VertexAI(model_name=\"gemini-1.0-pro-001\")\n```\n\n## Conclusion\n\nIn this short tutorial, we explored how Gemini Pro and Gemini Pro vision could be used with LangChain to implement multimodal RAG applications.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/o1-preview-vs-claude-3-5-sonnet-comparing-top-llms-d68734b53c93","frontmatter":{"title":"o1-preview vs. claude-3.5-sonnet: Comparing top LLMs","meta_title":"o1-preview vs. claude-3.5-sonnet: Comparing top LLMs","description":"Discover how OpenAIâ€™s o1-preview compares to Claude 3.5 Sonnet in performance, speed, and capabilities.","date":"2024-10-27T13:58:01.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kTWAcpRdOpsrFIDZjjjr7Q.jpeg","categories":["Programming","Machine Learning","Generative AI"],"author":"Rifx.Online","tags":["o1-preview","Claude","throughput","latency","reasoning"],"draft":false,"slug":"blog/o1-preview-vs-claude-3-5-sonnet-comparing-top-llms-d68734b53c93"},"content":"\n\n\n\nToday (Sep 12, 2024), OpenAI unveiled its latest language model, o1-preview. This advanced model is engineered to dedicate more time to processing before generating responses, enabling it to tackle complex tasks and solve challenging problems in science, coding, and mathematics with enhanced capabilities.\n\nIn this blog post, weâ€™ll thoroughly analyze o1-preview and compare it to Claude 3.5 Sonnet, which was previously considered one of the most advanced models available.\n\n\n\n\n## Comparison Methodology\n\nOur analysis utilizes [Keywords AIâ€™s LLM playground](https://docs.keywordsai.co/features/prompt/model-playground), a platform that supports over 200 language models and offers function-calling capabilities. Weâ€™ll explore the following aspects:\n\n* Basic comparison\n* Benchmark comparison\n* Processing speed\n* Evaluation metrics\n* Suggested use cases\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*yc171ikejtBy_o11.jpeg)\n\n\n## Basic Comparison\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*z2FrS_AVig7Y6eU_.jpeg)\n\nNote: o1-preview doesnâ€™t support Streaming, function calling, and system messages.\n\n\n## Benchmark Comparison\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Bx_vAvFc9DAD0cZA.jpeg)\n\nO1-preview outperforms Claude 3.5 Sonnet across all benchmarks. The smallest gap is in MMLU (general knowledge). GPQA Diamond, testing graduate-level reasoning, shows a significant performance difference. The MATH benchmark reveals the largest gap, highlighting o1-previewâ€™s advanced mathematical capabilities. These results indicate o1-previewâ€™s substantial improvements in complex reasoning and problem-solving across various domains.\n\n\n## Speed Comparison\n\nO1-preview takes longer to think and respond than other LLMs. While direct speed comparisons may not be entirely fair, testing o1-previewâ€™s speed is crucial. This information helps developers better understand o1-previewâ€™s capabilities and determine if itâ€™s suitable for their projects. Note: As o1-preview doesnâ€™t support streaming, we disabled streaming for both models. Consequently, time to first token (TTFT) couldnâ€™t be measured.\n\n\n## Latency\n\nOur tests, involving hundreds of requests per model, revealed significant differences. Claude 3.5 Sonnet averages 18.3s/request, whereas o1-preview takes 39.4s/request. O1-previewâ€™s significantly longer latency is due to its extended thinking and reasoning process.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*2PMkgPVuylFxwfIa.jpeg)\n\n\n## Throughput (Tokens per second)\n\nDespite higher latency, o1-preview shows superior throughput. O1-preview generates 92.94 tokens/second, while Claude 3.5 Sonnet produces 74.87 tokens/second. This indicates that o1-previewâ€™s longer generation time is primarily due to its initial processing phase rather than token generation speed.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*wxqpnwZhl9pnbw8y.jpeg)\n\n\n## Performance comparison\n\nWe conducted evaluation tests on the [Keywords AI platform](https://keywordsai.co/). The evaluation comprised three parts:\n\n* **Coding Task**: Both models successfully completed frontend and backend development tasks. O1-preview demonstrated superior performance with longer contexts, identifying and resolving bugs more efficiently in the first attempt. It also exhibited a more thorough code analysis capability.\n* **Logical Reasoning**: O1-preview excels in reasoning tasks. Its thinking process closely mimics human cognition. While Claude 3.5 Sonnet performs well on most problems, o1-preview consistently solves complex reasoning challenges, including International Mathematical Olympiad (IMO) level problems.\n* **Writing Task:** Both models perform exceptionally well on writing tasks. They demonstrate the ability to craft genuine, personalized cold emails, as well as concise and meaningful blog posts.\n\n\n## Model Recommendations\n\no1-preview\n\n* **Best for:** Complex problem-solving in mathematics, coding, and physics. Particularly suited for researchers tackling challenging tasks.\n* **Not suitable for:** AI applications requiring rapid response times or heavily reliant on system prompts. Voice AI applications due to lack of streaming support.\n\nClaude 3.5 Sonnet\n\n* **Best for:** Most AI applications requiring problem-solving capabilities and high-quality content generation.\n* **Not suitable for:** Voice AI applications or projects with strict budget constraints requiring lower operational costs.\n\n\n## How to integrate o1-preview into your AI apps.\n\nTo incorporate o1-preview into your AI applications, simply visit the Keywords AI model page and locate the â€œView codeâ€ button. Click this button to copy the provided code snippet, then paste it directly into your codebase. With this straightforward process, youâ€™ll be ready to harness the power of o1-preview in your projects, enabling you to tackle complex problems and generate high-quality content with ease.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*XyQ9QiI7TN8Uc5Jp.jpeg)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*t8fEYlEs13eM7D28lVbtIw.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*yhu9y5ixNuxeFVe1.png)\n\nThis story is published on [Generative AI](https://generativeai.pub/). Connect with us on [LinkedIn](https://www.linkedin.com/company/generative-ai-publication) and follow [Zeniteq](https://www.zeniteq.com/) to stay in the loop with the latest AI stories.\n\nSubscribe to our [newsletter](https://www.generativeaipub.com/) and [YouTube](https://www.youtube.com/@generativeaipub) channel to stay updated with the latest news and updates on generative AI. Letâ€™s shape the future of AI together!\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*PelNtaNaEVDWgMWr.png)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/openai-01-preview-secrets-99-of-people-dont-know-b0c5e4bb4f76","frontmatter":{"title":"OpenAI 01-Previewâ€Šâ€”â€ŠSECRETS 99% of People Donâ€™t Know","meta_title":"OpenAI 01-Previewâ€Šâ€”â€ŠSECRETS 99% of People Donâ€™t Know","description":"How to get the most out of 01-preview","date":"2024-11-01T03:58:01.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*wRAXNmhEzkGNagMl5Papxg.jpeg","categories":["Programming","Machine Learning","Technology/Web"],"author":"Rifx.Online","tags":["OpenAI","01-preview","iterative","problem-solving","planning"],"draft":false,"slug":"blog/openai-01-preview-secrets-99-of-people-dont-know-b0c5e4bb4f76"},"content":"\n### How to get the most out of 01\\-preview\n\nIâ€™ve been playing around with 01\\-preview since it came out.\n\nIâ€™m loving it!\n\nIâ€™m even teaching it in my new [**AI Growth Hacking course**](https://aigrowthguys.com/growth-hacking-course-sign-up/).\n\nIâ€™m excited to share some key insights about how to get the most out of this.\n\n\n\nMost people have no clue how 01\\-preview works.\n\nFirst of all, it is not just a â€œthinkingâ€ model.\n\nYou need to understand a little bit about how it works before you can take full advantage of it.\n\nIf you donâ€™t have a paid Medium account you can read for free [**here**](https://readmedium.com/openai-01-preview-secrets-99-of-people-dont-know-b0c5e4bb4f76?sk=12140ffad09d922bc00a8a4aa312a286).\n\nðŸ‘‰ Sign up to our free 5\\-Day email course to grow ðŸš€ and earnðŸ’²in the AI age\n\n## How does OpenAI 01\\-preview work?\n\n01\\-preview is not really a new model.\n\nIt combines other models and a â€œsystem promptâ€ that tells it to iterate several times before a response comes out.\n\nAll other models work by providing the first response the model comes up with.\n\n01\\-preview is designed to plan and experiment before a final answer comes out.\n\nAn example will help.\n\n> Imagine you tell GPT\\-4o to write a coherent paragraph that is exactly 80 words long, and has the word â€œtomatoâ€ as the 4th word, the 19th word, and the 72nd word.\n\nGPT\\-4o (and all other models) will fail at this task because it is too difficult to just spit out the first answer that comes to mind.\n\nThis type of question needs experimentation.\n\nThink if you were given the same task.\n\nYou need to â€œplay aroundâ€ with this task to try to fit the word â€œtomatoâ€ in those spots in a way that makes sense.\n\nYou canâ€™t just start writing and see what happens.\n\nYou will realize that you need to change some sentences around, and words around, in order to fit the word â€œtomatoâ€ in there.\n\nAlso, when you get near 80 words, you need to plan how to stop at exactly that number. You might wish to go back and delete an unnecessary word from the first sentence for example.\n\nThe reason 01\\-preview can do this type of thing is the way it â€œthinksâ€.\n\nIt will first break the problem down and say something like, â€œCome up with a plan to solve this problemâ€.\n\nThen it will write an approximate first guess (probably using GPT\\-4o)\n\nThen it will say to itself, â€œRe\\-read the question and see if you can make any tweaks or adjustmentsâ€.\n\nThen it will say, â€œdo a double\\-check to see if your response is perfect. If it is, display it, if it is not, keep tweaking.â€\n\nThen it will say, â€œRepeat this process until your answer is 100% perfect. Always remember to double\\-check your final answer before displaying itâ€.\n\nFor example, the first sentence of the first response might be this.\n\nâ€œSandy picked a red tomato from her garden.â€\n\nThen 01\\-preview would change it to, â€œSandy picked a tomato from her gardenâ€.\n\nThis way, it would successfully move the word tomato from the 5th word to the 4th word.\n\nIt would keep making tweaks by having an internal conversation with itself.\n\n## How to get the most out of 01\\-preview?\n\nNow that you have an idea about how 01\\-preview â€œthinksâ€, you can start to understand how to get the most out of it.\n\nYou need to divide your own questions into ones that require â€œthinkingâ€ and ones that donâ€™t.\n\nMany questions donâ€™t require â€œthinkingâ€ from the models.\n\nFor example, if you tell it to write you a funny story about a girl named Sandy who has a tomato garden, then you donâ€™t need to use 01\\-preview.\n\n**Why not?**\n\nBecause there are few constraints.\n\nThere are many ways to do this. It is essentially open\\-ended.\n\nThe story doesnâ€™t need to have a certain length.\n\nThe model can just start writing, throw in a joke or 2, and be done with it.\n\nIt wonâ€™t need to go back to the first sentence and count the number of words or anything.\n\nThe point is this:\n\nIf you are asking the model for something specific that would be difficult to do in one try without experimenting, then you should use 01\\-preview.\n\nIf you are asking for something open\\-ended, then use the other models.\n\nYou need to use 01\\-preview sparingly because you are only given a limited number of queries.\n\nIt will always be more limited than other models because it uses far more resources than other models.\n\nThe good news is that 01\\-preview will make far fewer mistakes than other models.\n\nAlso, it will be able to answer questions that previous models failed at.\n\nNow is a better time than ever to learn how to leverage AI to grow your business and earn more money.\n\nI am teaching how to use this in my AI Growth Hacking course.\n\nIâ€™ll also be incorporating this model to make the custom AI Agents and chatbots I build even more accurate.\n\nThis will make AI agent builders like [**Stammer**](https://stammer.ai/?via=andrew) even more powerful.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/openai-confirms-the-arrival-of-gpt-5-poised-to-bring-huge-improvements-to-artificial-intelligence-e3b858e79c2a","frontmatter":{"title":"OpenAI Confirms the Arrival of GPT-5, Poised to Bring Huge Improvements to Artificial Intelligenceâ€¦","meta_title":"OpenAI Confirms the Arrival of GPT-5, Poised to Bring Huge Improvements to Artificial Intelligenceâ€¦","description":"A netizen posted a GPT5 countdown post on x, saying that it was a conclusion drawn from clues from various platforms. The comment sectionâ€¦","date":"2024-11-01T03:58:58.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8J_opnaERs-wrq2YRKIxdQ.png","categories":["Natural Language Processing","Generative AI","Technology"],"author":"Rifx.Online","tags":["GPT-5","natural","language","efficiency","personalization"],"draft":false,"slug":"blog/openai-confirms-the-arrival-of-gpt-5-poised-to-bring-huge-improvements-to-artificial-intelligence-e3b858e79c2a"},"content":"\n\n\n\nA netizen posted a GPT5 countdown post on x, saying that it was a conclusion drawn from clues from various platforms. The comment section has reached a climax with all kinds of opinions coming out.\n\n\n\n**Cause 1** : OpenAI website GPT5 leak\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*EBDLAv3rOyCjshGBpVRI7A.png)\n\n**Cause 2** : The article â€œOpenAI Launches Better GPT5 Chatbotâ€ published by BusinessInsider, a well\\-known American financial business insider website. Since the website is a paid website, you can search for the title if you are interested. Some content is pasted below:\n\nThe generative AI company, led by Sam Altman, is on track to launch GPT\\-5 sometime in the middle of the year, possibly in the summer, according to two people familiar with the company. Some enterprise customers recently received demos of the latest model and its related enhancements to its ChatGPT tool, according to another person familiar with the process. Business Insider has confirmed the identities of these people, who asked to remain anonymous so they could speak freely.\n\nBased on the discussions on X and other platforms, it is very likely that a new version of the model will be launched on June 6, but it is not certain whether it will be GPT 4\\.5 or GPT5\\.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*rhApTugfrMVBB6PhMvK4rg.png)\n\nAll of us waiting for **GPT5**\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*eB6j2S_dPbjQ2-sV2N1fwA.jpeg)\n\n\n## What to Expect from GPT\\-5\n\nWhile details remain scarce, the excitement surrounding GPT\\-5 is driven by expectations of significant improvements in AI capabilities. Here are some potential advancements that have been speculated:\n\n* Enhanced Natural Language Understanding: GPT\\-5 is expected to have an even deeper understanding of context, nuances, and subtleties in human language, making interactions more fluid and natural.\n* Increased Efficiency: With each iteration, OpenAI has made strides in reducing latency and improving the efficiency of its models. GPT\\-5 is anticipated to continue this trend, providing faster and more accurate responses.\n* Broader Knowledge Base: By incorporating more diverse and extensive datasets, GPT\\-5 could offer more comprehensive and reliable information across a wider range of topics.\n* Advanced Personalization: The new model might include enhanced personalization features, allowing it to better adapt to individual user preferences and needs.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*7xCG5iy53LLQCTnmzxs_3g.jpeg)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/openai-gpt-5-ph-d-level-intelligence-expected-by-2025-50a86c3aad86","frontmatter":{"title":"OpenAI GPT-5: Ph.D.-Level Intelligence Expected by 2025","meta_title":"OpenAI GPT-5: Ph.D.-Level Intelligence Expected by 2025","description":"After months of speculation, OpenAI has finally unveiled details about the highly anticipated GPT-5. Initially expected in 2024, itsâ€¦","date":"2024-11-01T03:59:56.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*OasnWeS5mgAX_0hIpirO5Q.jpeg","categories":["Machine Learning","Ethics","Data Science"],"author":"Rifx.Online","tags":["GPT-5","Ph.D.","intelligence","ethics","privacy"],"draft":false,"slug":"blog/openai-gpt-5-ph-d-level-intelligence-expected-by-2025-50a86c3aad86"},"content":"\n\n\n\n\n\nAfter months of speculation, OpenAI has finally unveiled details about the highly anticipated GPT\\-5\\. Initially expected in 2024, its release has been postponed to late 2025 or early 2026\\. Mira Murati, OpenAIâ€™s CTO, shared insights in an interview with Dartmouth Engineering about the capabilities and potential of this new version. Hereâ€™s everything you need to know.\n\n\n## A Quantum Leap in Intelligence\n\nMurati compares previous versions of GPT to different levels of human intelligence. GPT\\-3 is akin to a young child, while [**GPT\\-4**](https://www.geekmetaverse.com/gpt-4-unveils-its-secrets-a-combination-of-8-smaller-models/) is comparable to a high school student. The new GPT\\-5 promises to reach a â€œPh.D.\\-level intelligence for specific tasks.â€ This advancement is not only exciting but also raises questions about the future of artificial intelligence.\n\n\n## Evolution of GPT: From Child to Ph.D.\n\nComparing these versions to stages of human education helps us grasp these advancements better. GPT\\-3, with its ability to generate coherent and useful text, opened many doors. GPT\\-4 improved these skills, demonstrating superior performance in more complex tasks. Now, GPT\\-5 aims to take this to an entirely new level, with advanced reasoning and memory capabilities.\n\n\n## Specialized Intelligence\n\nPh.D.\\-level intelligence doesnâ€™t mean [**GPT\\-5**](https://www.geekmetaverse.com/openai-ceo-confirms-that-gpt-5-is-already-in-development/) can do everything perfectly. Murati clarified that these abilities will be task\\-specific. This suggests that while AI might surpass humans in certain fields, it will still have limitations in others. This specialized focus could lead to highly precise and useful applications in areas like scientific research and complex data analysis.\n\n\n## Potential and Future Applications\n\nThe development of GPT\\-5 opens a range of possibilities across different sectors. From education to medicine, and research to technology, the applications are vast.\n\n\n### Education and Training\n\nAn [**AI**](https://www.geekmetaverse.com/apple-updates-ai-takes-center-stage-with-siri-integration-chatgpt-partnership-and-elon-musk-concerns/) capable of reaching Ph.D. levels could radically transform education. Personalized tutoring systems could provide support to students in complex areas, enhancing understanding and academic performance.\n\n\n### Medicine and Healthcare\n\nIn medicine, an AI with such capabilities could assist in diagnosing rare diseases, developing personalized treatments, and managing large volumes of clinical data, significantly advancing medical care.\n\n\n### Research and Development\n\nResearchers could greatly benefit from an AI that can analyze large datasets, identify patterns, and generate hypotheses, accelerating the pace of scientific and technological discoveries.\n\n\n## Challenges and Ethical Considerations\n\nDespite the promising applications, the development of such advanced AI also brings significant ethical challenges. Over\\-reliance on AI for critical tasks could lead to issues if not managed properly.\n\n\n### Privacy and Security\n\nData privacy and cybersecurity will be crucial topics. Ensuring AI systems are not misused and that sensitive data is adequately protected will be a priority.\n\n\n### Employment Impact\n\nThe impact on employment is also a concern. Automating specialized tasks could displace certain professionals, necessitating proactive measures to address these socioeconomic implications.\n\n\n### Conclusion\n\nThe delay in GPT\\-5â€™s release may be disappointing for some, but its advanced capabilities generate significant anticipation. If OpenAI meets its goals, we could be looking at a revolutionary tool that transforms multiple industries and changes how we interact with technology.\n\n\n### FAQs\n\n**1\\. What is GPT\\-5?**\n\nGPT\\-5 is the upcoming version of OpenAIâ€™s Generative Pre\\-trained Transformer (GPT) series, promising Ph.D.\\-level intelligence for specific tasks.\n\n**2\\. When is GPT\\-5 expected to be released?**\n\nThe release of GPT\\-5 has been postponed to late 2025 or early 2026\\.\n\n**3\\. How does GPT\\-5 compare to previous versions?**\n\nGPT\\-3 is akin to a young child in intelligence, while GPT\\-4 compares to a high school student. GPT\\-5 aims to achieve Ph.D.\\-level intelligence for specific tasks, offering advanced reasoning and memory capabilities.\n\n**4\\. What kind of tasks will GPT\\-5 be able to perform?**\n\nGPT\\-5 will be specialized in certain tasks, excelling in specific fields like scientific research, complex data analysis, education, and healthcare.\n\n**5\\. Will GPT\\-5 be perfect at everything?**\n\nNo, GPT\\-5â€™s Ph.D.\\-level intelligence will be task\\-specific, meaning it will excel in certain areas but still have limitations in others.\n\n**6\\. What are the potential applications of GPT\\-5?**\n\nPotential applications include personalized tutoring in education, assistance in diagnosing diseases and developing treatments in healthcare, and aiding researchers in analyzing large datasets and generating hypotheses.\n\n**7\\. What are the ethical considerations associated with GPT\\-5?**\n\nEthical considerations include ensuring data privacy and cybersecurity, managing the socio\\-economic impact of job displacement due to automation, and preventing misuse of advanced AI systems.\n\n**8\\. How will GPT\\-5 impact data privacy and security?**\n\nEnsuring the protection of sensitive data and preventing the misuse of AI systems will be crucial. Measures will need to be implemented to safeguard data privacy and security.\n\n**9\\. What is the potential impact of GPT\\-5 on employment?**\n\nThe automation of specialized tasks by GPT\\-5 could displace certain professionals, necessitating proactive measures to mitigate these socio\\-economic impacts.\n\n**10\\. Why was the release of GPT\\-5 delayed?**\n\nThe delay allows OpenAI to refine and enhance GPT\\-5â€™s capabilities to ensure it meets the high expectations for its advanced intelligence and specialized applications.\n\n**11\\. How can GPT\\-5 transform education?**\n\nGPT\\-5 could revolutionize education by providing personalized tutoring systems that support students in complex subjects, improving comprehension and academic performance.\n\n**12\\. What advancements could GPT\\-5 bring to the medical field?**\n\nIn medicine, GPT\\-5 could aid in diagnosing rare diseases, developing personalized treatments, and managing vast amounts of clinical data, leading to significant advancements in medical care.\n\nOriginal post: [https://www.geekmetaverse.com/openai\\-gpt\\-5\\-ph\\-d\\-level\\-intelligence\\-2025/](https://www.geekmetaverse.com/openai-gpt-5-ph-d-level-intelligence-2025/)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/openai-just-built-her-in-real-life-17769d993e11","frontmatter":{"title":"Users Will Fall in Love With OpenAIâ€™s New GPT-4o Model. Literally.","meta_title":"Users Will Fall in Love With OpenAIâ€™s New GPT-4o Model. Literally.","description":"The companyâ€™s new GPT-4o can understand and mimic human speech and emotion","date":"2024-11-01T04:08:40.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*-bTsggApvkUHAq57YhSd-A.png","categories":["Generative AI","Chatbots","Natural Language Processing"],"author":"Rifx.Online","tags":["GPT-4o","speech","emotions","multilingual","conversational"],"draft":false,"slug":"blog/openai-just-built-her-in-real-life-17769d993e11"},"content":"\n\n\n\n\n## The companyâ€™s new GPT\\-4o can understand and mimic human speech and emotion\n\n\n\nIn the iconic 2013 film *Her*, the protagonist develops an intense relationship â€” which morphs into a love affair â€” with a voice\\-enabled AI system.\n\nThe AI in *Her* is everything that todayâ€™s voice\\-enabled systems are not: emotive, funny, and able to intuit the subtleties of human conversation.\n\nIn a major [announcement this morning](https://www.youtube.com/live/DQacCB9tDaw?app=desktop&si=jvKW7jFDwFvOMBBk), OpenAI announced the release of a new version of its ChatGPT system that natively integrates speech, transcription, and intelligence into a single model.\n\nItâ€™s powerful, intuitive, and disturbingly human\\-like. Essentially, OpenAI has built a real\\-life version of *Her*.\n\n\n## A Bad Conversationalist\n\nChatGPT has had voice capabilities for months now. Even today, you can open the ChatGPT app on your phone, press the headphones icon, and converse with the system using your voice.\n\nThe problem, though, was that ChatGPT was a terrible conversationalist.\n\nEssentially, ChatGPTâ€™s voice capabilities were a hack created by splicing together three different models.\n\nWhen you would speak to the system, it would first use a transcription model to turn your voice into text. It would then feed that text into its intelligence model â€” basically, the same system that underpins GPT\\-4\\.\n\nThe intelligence system would generate text, which ChatGPT would feed back into a text\\-to\\-speech system to create a computerized voice that would respond to you.\n\nThis made the system nominally conversational, but actually speaking with it was clunky and awkward.\n\nAll the extra steps of sending content between different models meant that the system was laggy. In my own testing, I found it often took 3 to 5 seconds between speaking to the system and getting a response back.\n\nHuman conversation relies on subtleties that [unfold over milliseconds](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8794835/#:~:text=The%20modal%20conversational%20response%20time,deliberative%20conscious%20control%20(22).). A system that takes up to five seconds to respond to speech feels clunky and robotic.\n\nThe previous system also lacked many fundamental aspects of human speech.\n\nFor example, you couldnâ€™t interrupt it; you had to wait for it to finish speaking before you could respond.\n\nSpeaking with it often felt like talking to one of those un\\-interruptable people who blabbers on about a random topic with no awareness of the other people in the room. You often felt like bring up the Oscarsâ€™ orchestra in a desperate attempt to get the system to stop talking.\n\nIt was also constrained by its inability to interpret emotion in voices or to accurately mimic human emotion in its own responses.\n\nHumans are excellent at reading between the lines, partially because we can [pick up on subtle emotive cues in the speakerâ€™s voice.](https://pressbooks.lib.jmu.edu/communicationintherealworldjmu/chapter/non-verbal-communication/)\n\nIf I ask my friend, â€œHow was your day?â€ and they respond, â€œIt was fine,â€ but they insert a subtle pause between â€œwasâ€ and â€œfineâ€ (or thereâ€™s a hint of exasperation in the final word), Iâ€™d know that they actually had a challenging day, and I should ask some follow\\-up questions.\n\nChatGPT couldnâ€™t do these things, which made speaking to it feel like communicating with some kind of alien intelligence, not a human.\n\nIn short, the previous system fell squarely into the uncanny valley. It was good enough at conversing and had a convincing enough voice that parts of the conversation could feel human\\-like.\n\nBut the weird pauses, lack of emotive understanding, and lag ultimately shattered the illusion, making it come off as more unsettling than useful.\n\nI tried using the previous system with my six\\-year\\-old son. He was so creeped out by it that he wouldnâ€™t let me switch the audio back on again.\n\n\n## OpenAIâ€™s Revoluntary New Model\n\nToday, OpenAI is changing all of that. In their [announcement this morning](https://www.youtube.com/live/DQacCB9tDaw?app=desktop&si=jvKW7jFDwFvOMBBk), the company revealed that they are releasing a new model, GPT\\-4o.\n\nGPT\\-4o natively integrates speech recognition, speech generation, and intelligence into a single system.\n\nThat means that the spaghetti code system integrating three different models to simulate conversation is gone. Instead, the new version of ChatGPT will be able to **take in speech, process it instantly, and respond with realistically generated speech of its own.**\n\nFor users, this will enable several new capabilities that OpenAI CEO Sam Altman [described as â€œlike magic.â€](https://twitter.com/sama/status/1788989777452408943)\n\nFor one, youâ€™ll be able to converse with ChatGPT much more naturally. Instead of having to type your questions and follow\\-ups into an interface, youâ€™ll be able to speak with the app as if youâ€™re talking to a friend.\n\nIn several live demos, OpenAIâ€™s engineers showed how the system can listen to a user and respond with an intelligent result within milliseconds.\n\nAgain, those speeds are possible because the new model doesnâ€™t need to waste time switching modalities â€” it can process voice and respond with its own voice in a single step, instead of resorting to multiple lower\\-level models.\n\nGPT\\-4o can also interpret and create emotion.\n\nIn one demo, an OpenAI staff member asked the system to lead him through a breathing exercise.\n\nHe then pretended to hyperventilate, and ChatGPT â€” sensing the speed with which he was breathing and the apparent panic in his voice â€” urged him to slow down and take deeper breaths.\n\nThe system also appears capable of modulating the emotion in its own responses. In another demo, the staff member asked GPT\\-4o to read a bedtime story in an increasingly dramatic voice.\n\nIt obliged, ultimately sounding like a middle school theater kid horrifically overacting a scene!\n\nBecause the new system is also integrated with GPT\\-4â€™s vision capabilities, it can perform functions like interpreting the emotions on a personâ€™s face.\n\nThis increased level of emotional intelligence will likely make the system a much better conversationalist.\n\nOther new capabilities will help, too. Users can interrupt GPT\\-4o mid\\-sentence.\n\nDuring their demos, OpenAI staff members frequently interrupted the model when it started to go on tangents, as one might interrupt a friend to start responding to a real\\-life question.\n\n\n## Huge Potential\n\nThe demos this morning were lighthearted and funny. But one can quickly see how a model that can easily interpret, quickly process, and realistically create emotive human speech could be incredibly powerful.\n\nSeveral times during the demo, ChatGPT responded in ways that reminded me of the fictional AI from *Her*.\n\nChatGPT appeared to laugh at itself, become embarrassed when OpenAI staff members complimented it, and perhaps even throw in a flirty line here and there.\n\nSeveral (purportedly) unscripted interactions also revealed some of the deeper capabilities that better conversation could unlock.\n\nBased on an audience question, OpenAIâ€™s staff members demonstrated how the system could listen to speech in Italian and quickly and accurately translate it into English speech, and vice versa.\n\n\n\n\n\n\n\nOne can easily imagine how such a capability could make multi\\-lingual interactions incredibly simple, essentially eliminating language barriers (and perhaps, human translators).\n\nA doctor, for example, could pull up ChatGPT and use it to quickly speak with a patient in any language. While traveling, you could pull up the app on your phone and use it as a free and instantaneous translator to ask someone for directions or to make a purchase in a store.\n\nAdding the vision capabilities, one could even show ChatGPT a foreign restaurant menu, ask for a translation of certain items, tell it when you like to eat at home, and ask it to recommend some dishes you might want to order (or avoid.)\n\nI can also see how quickly the new system could venture into *Her* territory. OpenAI still doesnâ€™t allow the kinds of NSFW interactions that happened in the movie.\n\nBut GPT\\-4oâ€™s ability to understand and mimic emotion â€” coupled with its powerful, often uncanny abilities to produce its own convincing human emotional speed â€” is striking.\n\nListening to the demos, Iâ€™m certain that people will fall in love with this system, just as the protagonist did in *Her*. Itâ€™s that good.\n\n\n## Will it get used?\n\nAll of this is amazing on paper. Itâ€™s unclear, however, how many users actually want a fully emotive AI voice companion.\n\nMost people I work with use ChatGPT not as a conversational companion, but for utilitarian purposes.\n\nIâ€™ve seen colleagues leverage the system for boring and mundane tasks like writing the landing page copy for a webinar, turning out a quick response to an email from their landlord, or writing the first draft of a blog post.\n\nNone of these utilitarian functions really require conversation. Itâ€™s unclear whether being able to speak these kinds of requests to an AI would be useful.\n\nThe real test, then, is not necessarily how capable OpenAIâ€™s new system is, but **how well they integrate it into places where people are already interacting with computers via their voices.**\n\nRealistically, I canâ€™t see many users sitting down at work and conversing with AI.\n\nBut if OpenAI integrates GPT\\-4o into voice interfaces on cell phones, in cars, or on smart devices like the Amazon Echo, I could easily see the systemâ€™s emotive capabilities becoming much more useful.\n\nEven if people donâ€™t want to speak with ChatGPT very much, the new capabilities of a natively multimodal audio and vision model will be incredibly powerful for developers who build applications on top of OpenAIâ€™s existing API.\n\nIn their announcement, OpenAI said that GPT\\-4o will be available through their existing developer interfaces. The system will also be 50% cheaper than previous models of GPT\\-4\\.\n\nThose changes alone are massive. Whether or not the speech element really takes off, the intelligence that powers it will also make hundreds of existing GPT\\-4\\-powered applications smarter, faster, better, and cheaper to operate.\n\nThe conversational elements of the new system, in other words, might turn out to be a cool gimmick. But the underlying impact will be subtler and broader.\n\nIâ€™m excited to see how real\\-life users interact with GPT\\-4o. Will they be creeped out? Amazed? Wooed?\n\nBut Iâ€™m even more excited to fire up my Python IDE and add GPT\\-4o into the applications Iâ€™ve already built using OpenAIâ€™s tools.\n\nSpeaking to a machine is cool. But a natively multimodal AI model that understands human emotions, and that I can summon with a few lines of Python code, for cheap? That could truly change the world.\n\n**Iâ€™ve tested thousands of ChatGPT prompts over the last year. As a full\\-time creator, there are a handful I come back to every day that fit with the ethical uses I mention in this article. I compiled them into a free guide, *7 Enormously Useful ChatGPT Prompts For Creators.* [Grab a copy today!](https://no-frills-influencer.ck.page/6a100e8fe4)**\n\n\n"},{"lang":"en","group":"blog","slug":"blog/openai-realtime-api-voice-mode-getting-started-on-colab-39b93edcaa6a","frontmatter":{"title":"OpenAI Realtime API (Voice Mode), Getting Started on Colab","meta_title":"OpenAI Realtime API (Voice Mode), Getting Started on Colab","description":"Everything you need to know, and a hands-on introduction to OpenAIâ€™s voice mode API that you can run on Colab.","date":"2024-11-08T00:23:32.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*_-d5zsWWQEzVLZxABTSFWQ.png","categories":["Programming","Voice Assistants","Technology/WebAPI"],"author":"Rifx.Online","tags":["OpenAI","Realtime","API","GPT-4o","Colab"],"draft":false,"slug":"blog/openai-realtime-api-voice-mode-getting-started-on-colab-39b93edcaa6a"},"content":"\n\n\n\nEverything you need to know, and a hands\\-on introduction to OpenAIâ€™s voice mode API that you can run on Colab.\n\n\n\nThe latest development from OpenAI brings us the **Realtime API**, designed to allow developers to create **fast, seamless speech\\-to\\-speech experiences** within their apps. This API aims to streamline the development of multimodal conversational features, making it much easier to build natural, real\\-time voice interactions.\n\nI**n this blog post,** Iâ€™ll cover the **main questions** around this new API, including\n\n* what is Realtime API,\n* How to access it,\n* Its limitations and pricing,\n* and provide a **Colab tutorial** on how to get started.\n\n\n## What is the Realtime API?\n\nThe **Realtime API** by OpenAI is a public beta feature that enables paid developers to incorporate real\\-time voice interaction in their apps. Itâ€™s a multimodal API capable of transforming **audio inputs to speech responses**, using the advanced **GPT\\-4o** model for this purpose. Essentially, it allows for **low\\-latency conversations** similar to a natural human interaction, similar to the functionality seen in ChatGPTâ€™s Advanced Voice Mode.\n\nPreviously, developers had to stitch together multiple models for **speech recognition, text processing, and text\\-to\\-speech generation**. The Realtime API does this all in a single API call, resulting in fewer delays, richer responses, and more consistent handling of accents and emphasis.\n\nThe **Chat Completions API** also introduces audio input and output, but it doesnâ€™t offer the low\\-latency experience of the Realtime API. Thus, for experiences like language learning or voice\\-enabled assistants, Realtime API is the preferred choice.\n\n\n## Access and Limitations\n\nAccess to the **Realtime API** is currently available as a **public beta** for paid developers.\n\n**Although it is said that access is limited in Europe, I was able to use it through my tier 5 OpenAI account.**\n\nThe API uses a **WebSocket** connection, which ensures a smooth streaming experience for both audio inputs and outputs.\n\nFor now, there are **limitations** to note:\n\n* **Session Rate Limits**: The API is rate limited to approximately **100 simultaneous sessions** for Tier 5 developers. Lower tiers have smaller capacity. As of Octobre 2024, the API is limited 2M tokens per minutes.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*XpAB6WRseRb0iY-edE94xw.png)\n\n* **Capabilities**: Initially, only **voice modality** is supported, but OpenAI plans to add more like **video** and **vision** over time.\n* **Availability**: Full audio capabilities are in the beta phase, with **future SDK integration** planned for Python and Node.js.\n\n\n## Pricing of the Realtime API\n\nThe **pricing** structure for the Realtime API is divided into both **text tokens** and **audio tokens**:\n\n* **Audio Input**: $100 per 1 million tokens (approx. **$0\\.06 per minute**).\n* **Audio Output**: $200 per 1 million tokens (approx. **$0\\.24 per minute**).\n* **Text Input**: $5 per 1 million tokens.\n* **Text Output**: $20 per 1 million tokens.\n\nThe pricing makes it affordable for developers to create robust **speech\\-to\\-speech** experiences, though audio features are significantly more expensive than text\\-based interactions. This is important to keep in mind when scaling an app with voice features.\n\nIt is still slightly more expensive than outsourcing it to some countries, but we can expect a significant drop in prices over the next six months.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*ocwFDXEt8X7KD_k6)\n\n\n## Building with the Realtime API in Google Colab\n\nHereâ€™s a basic **Colab guide** to help you get started with uploading a file, sending a request to the Realtime API, and generating audio responses.\n\nIn this demo, we chose to upload a stream of audio chunks to mimic a conversation.\n\n**Full Colab Code**: [link here](https://colab.research.google.com/drive/1-bj_LH7Gv2bbTJopbo7Hk_AIyDAuqeEQ?usp=sharing), simply add your â€œopenaiâ€ key to Colabâ€™s secrets and run the colab.\n\n\n### Step 1: Setting Up Google Colab and Dependencies\n\n* Start a new **Google Colab** notebook.\n* Install the necessary libraries such as **requests** and **pydub** for managing audio files.\n\n\n```python\n#Setup\n!pip install websockets pydub --quiet \n\nimport base64\nimport numpy as np\nimport soundfile as sf\nimport json\nimport websockets\nfrom google.colab import files\nfrom pydub import AudioSegment\nfrom tqdm import tqdm\nimport io\n```\n\n### Step 2: Uploading Your Audio File\n\nIn Colab, you can use the `files` module from **google.colab** to upload audio files.\n\n\n```python\n#Upload audio\ndef upload_audio():\n    uploaded = files.upload()  \n    for file_name in uploaded.keys():\n        return file_name\n\naudio_file = upload_audio()\n```\n\n### Step 3: Sending a Request to the Realtime API\n\n* Format the audio file properly before sending it to OpenAI.\n* Establish a WebSocket connection to stream the audio file.\n* Use `tqdm` to display the progress of the upload stream.\n* The function returns the full set of events (including responses) for later processing to generate the output audio. It also returns the transcript of the modelâ€™s response.\n\n\n```python\n#Helper functions\n## Function to convert Float32Array to PCM16 format\ndef float_to_pcm16(float32_array):\n    return np.clip(float32_array * 32767, -32768, 32767).astype(np.int16).tobytes()\n\n## Function to split audio into base64-encoded PCM16 chunks\ndef float32_to_base64_chunks(float32_array, chunk_size=32000):\n    pcm16_data = float_to_pcm16(float32_array)\n    for i in range(0, len(pcm16_data), chunk_size):\n        yield base64.b64encode(pcm16_data[i:i+chunk_size]).decode('utf-8')\n\n## WebSocket connection and streaming audio with text prompt\n## Main function to call OpenAI Realtime API\nasync def stream_audio_to_realtime_api(audio_file, text_prompt, openai_key, verbose = False):\n    data, samplerate = sf.read(audio_file, dtype='float32')\n    if data.ndim > 1:\n        data = data[:, 0]\n    if samplerate != 24000:\n        raise ValueError(f\"Audio must be sampled at 24kHz, but it is {samplerate}Hz\")\n\n    url = \"wss://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2024-10-01\"\n    headers = {\"Authorization\": \"Bearer \" + openai_key, \"OpenAI-Beta\": \"realtime=v1\"}\n\n    async with websockets.connect(url, extra_headers=headers) as ws:\n        await ws.send(json.dumps({\n            \"type\": \"conversation.item.create\",\n            \"item\": {\"type\": \"message\", \"role\": \"user\", \"content\": [{\"type\": \"input_text\", \"text\": text_prompt}]}\n        }))\n\n        with tqdm(total=(len(float_to_pcm16(data)) + 32000 - 1) // 32000, desc=\"Sending Audio Chunks\") as pbar:\n            for chunk in float32_to_base64_chunks(data):\n                await ws.send(json.dumps({\"type\": \"input_audio_buffer.append\", \"audio\": chunk}))\n                pbar.update(1)\n\n        await ws.send(json.dumps({\"type\": \"input_audio_buffer.commit\"}))\n        await ws.send(json.dumps({\"type\": \"response.create\"}))\n\n        all_events = []\n        while True:\n            response = await ws.recv()\n            event = json.loads(response)\n            all_events.append(event)\n            if verbose:\n                print(event)\n            if event[\"type\"] == \"response.output_item.done\" and \"item\" in event and \"content\" in event[\"item\"]:\n                for content in event[\"item\"][\"content\"]:\n                    if content[\"type\"] == \"audio\" and \"transcript\" in content:\n                        transcript = content[\"transcript\"]\n                        break\n            if event[\"type\"] == \"rate_limits.updated\":\n                break\n\n        return all_events, transcript\n```\n\n```python\n#Add a prompt and call OpenAI Realtime API\ntext_prompt = \"Summarize this audio content\"\n\nevents, transcript = await stream_audio_to_realtime_api(\n    audio_file, \n    text_prompt, \n    openai_key, \n    verbose = False \n#to display OpenAI's response as they arrive, use verbose = True\n    ) \n```\n\n### Step 4: Generating Audio Responses\n\n* Once you receive the response, generate the audio.\n* Choose a file name and save the file.\n* You will then be able to download the file.\n\n\n```python\n## Function to decode and concatenate audio chunks into a full audio file\ndef generate_audio_from_chunks(audio_chunks, output_filename=None):\n    # Concatenate the base64-encoded audio chunks from the 'delta' field\n    full_audio_base64 = ''.join(audio_chunks)\n\n    # Decode the concatenated base64 string to raw PCM16 audio bytes\n    audio_bytes = base64.b64decode(full_audio_base64)\n\n    # Load the bytes as a pydub AudioSegment (assuming 24kHz, 1 channel, PCM16)\n    audio_segment = AudioSegment.from_raw(\n        io.BytesIO(audio_bytes), \n        sample_width=2, \n        frame_rate=24000, \n        channels=1)\n\n    # Optionally save the audio to a file\n    if output_filename:\n        audio_segment.export(output_filename, format=\"wav\")\n        print(f\"Audio saved to {output_filename}\")\n\n    return audio_segment\n```\n\n```python\n#Extract audio chunks from the collected events\naudio_output_chunks = [event['delta'] for event in events if event['type'] == 'response.audio.delta']\n\n## Generate the full audio from the collected chunks\ngenerated_audio = generate_audio_from_chunks(audio_output_chunks, output_filename=\"output_audioo.wav\")\n```\n\n## Conclusion\n\nWith the above steps, you can integrate OpenAIâ€™s Realtime API into a Colab notebook, enabling seamless voice instructions.\n\nThis guide should give you a solid foundation for experimenting with real\\-time audio\\-to\\-audio interactions and building innovative voice\\-driven applications.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/openai-rolls-out-searchgpt-to-more-users-33024ff3132c","frontmatter":{"title":"OpenAI Rolls Out SearchGPT To More Users","meta_title":"OpenAI Rolls Out SearchGPT To More Users","description":"ChatGPT got a huge user interface redesign with support for SearchGPTâ€Šâ€”â€Šit now resembles search engines like Google and Perplexity.","date":"2024-11-01T03:57:02.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BW6Qt6PMwHwlYRljAIBQWg.jpeg","categories":["Chatbots","Technology/Web","SearchGPT"],"author":"Rifx.Online","tags":["SearchGPT","ChatGPT","web","search","publishers"],"draft":false,"slug":"blog/openai-rolls-out-searchgpt-to-more-users-33024ff3132c"},"content":"\n\n\n\n\n\n**Have you noticed OpenAIâ€™s latest redesign of ChatGPT?**\n\nIf youâ€™ve logged in recently, you might have spotted two major changes.\n\n* First, thereâ€™s the new [**Canvas**](https://generativeai.pub/openai-rolls-out-canvas-in-chatgpt-a-brand-new-writing-and-coding-interface-7b57a3ec582a)feature that automatically opens a new interface on the right side. This addition lets you work on longer documents without having to scroll up and down through the chat. Itâ€™s a small but handy update.\n* Second, the **prompt field** has moved up and now sits in the center of the screen.\n\nTake a look at the latest user interface below:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*KsYMU9ffVlKsmHzKIOKH8g.png)\n\nHave you noticed the resemblance of this new layout to Google and Perplexity AI? ChatGPT now looks like a search engine.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*xFKErUHnfbJunaKi4NvM9A.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*PgSwS14lkUtZe7Ra9oLCrg.png)\n\nNow when you hit the â€˜/â€™ key on your keyboard, you can toggle a new â€œSearchâ€ feature that lets ChatGPT access the web.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*oYxvVvsuUuc_PM0PXRmN7A.png)\n\nLetâ€™s break down what this all means.\n\n\n## What is the Search Feature in ChatGPT?\n\n[SearchGPT](https://generativeai.pub/openai-announces-search-gpt-is-this-the-google-killer-5919ba31f95b) allows ChatGPT to access real\\-time web data. Similar to how Perplexity works, it uses a large language model that searches the web for you, gives you immediate answers, and includes the sources it pulls from.\n\nThe feature was initially made accessible to 10,000 users and added a waitlist form for those who wanted to get early access.\n\nOpenAI has partnered with well\\-known publishers like **The Wall Street Journal, The Associated Press, Vox Media, and Time** to make sure users receive credible, trustworthy information.\n\n\n> â€œAI search is going to become one of the key ways that people navigate the internet, and itâ€™s crucial, in these early days, that the technology is built in a way that values, respects, and protects journalism and publishers. We look forward to partnering with OpenAI in the process, and creating a new way for readers to discover The Atlantic.â€ â€” Nicholas Thompson, CEO of The Atlantic\n\nWhen you ask SearchGPT a question, it doesnâ€™t just pull information from random sources. Each response comes with **clear, in\\-line attribution and links**, so you know exactly where the information is coming from.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*uchpKOXqZCG55HSNkaOOZQ.png)\n\nYou can even dive deeper by clicking on the source links that appear below the searched sites dropdown, giving you more ways to explore the topic.\n\n\n## How to Access SearchGPT\n\nAccessing SearchGPT is super simple. When youâ€™re in ChatGPT, press the **â€˜/â€™** key and select the Search option from the menu.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*xNfm-6zPFzdXGL2A0D92YQ.png)\n\nIt works much like any other search engine: you ask your question, and within seconds, SearchGPT provides an answer, complete with sources.\n\nYou can even ask follow\\-up questions to dig deeper into the topic. This creates a conversational search experience, much more interactive than scrolling through traditional search results.\n\n\n## SearchGPT vs. Perplexity vs. Google\n\nSo how does SearchGPT compare to **Perplexity AI** and **Google**?\n\n**SearchGPT** is built to give you concise, sourced answers. Each answer has a link to the original source, and you can click on it to verify the information. Itâ€™s ideal for real\\-time answers and quick fact\\-checking.\n\nPlus, with follow\\-up questions, you can refine your query without starting over. This conversational nature makes it feel like youâ€™re talking to a super\\-advanced version of Google that remembers what youâ€™ve been asking.\n\n**Perplexity**, on the other hand, is a more academic\\-style search engine. It emphasizes scholarly articles and detailed research, which can be useful for more in\\-depth queries. Perplexity is often preferred for research\\-heavy tasks where you need deeper sources\n\n**Google**, of course, is still the giant in the room. Despite their recent efforts to integrate generative AI into search results, they havenâ€™t quite nailed the seamless experience that users want.\n\nGoogleâ€™s generative search rollout was clunky and received a lot of backlash due to errors and irrelevant responses. But Googleâ€™s breadth of information and infrastructure are still unmatched.\n\n\n## Is This the End of Google?\n\nGoogle isnâ€™t going anywhere soon. The tech giant still controls over 90% of the search market. Theyâ€™ve been at this for decades, and their search algorithms are constantly evolving.\n\nHowever, with AI search engines like SearchGPT gaining ground, Google is under pressure to step up its game. OpenAIâ€™s move to partner with publishers for credible sources is a smart strategy that could chip away at Googleâ€™s dominance.\n\nThis focus on verified results means that when you use SearchGPT, youâ€™re less likely to run into hallucinated answers â€” something that AI\\-driven tools have struggled with in the past.\n\nAlso, Google is still the default for most people. It has the advantage of being everywhere â€” from your phoneâ€™s browser to your smart speaker. SearchGPT is still in its early stages and would need time to gain that level of trust from users.\n\n\n## SearchGPT Isnâ€™t There Yet\n\nIâ€™ve been testing SearchGPT in the past couple of hours and here are some of my observations:\n\n* **Quality of the answer:** One major downside is that SearchGPTâ€™s answer quality doesnâ€™t quite match the depth or precision of Perplexity Pro. Although itâ€™s comparable to the base version of Perplexity, users who rely on it for more complex or nuanced queries will notice a difference.\n* **Slow response:** Another pain point is speed. When using SearchGPT, the time it takes to process a query and return an answer can feel excruciatingly slow. This delay disrupts the flow of interaction, particularly when youâ€™re in the middle of a deep dive into a topic.\n* **Lack of contextual understanding:** In some cases, it fails to recognize the continuity of a conversation. If you ask a follow\\-up question, instead of understanding it in the context of your previous query, the model often treats it as a fresh, standalone question.\n* **No follow up suggestions:** Unlike Perplexity, which often suggests follow\\-up questions to help you refine your search, SearchGPT doesnâ€™t offer this feature. This lack of guidance leaves users to figure out how to best phrase or narrow down their queries on their own.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*vfXxpgENLyenLY2l33PKiw.png)\n\nHereâ€™s another weird workflow I noticed while using the search feature: If you switch the language model from GPT\\-4o to â€œChatGPT o1\\-preview,â€ the search indicator remains but does not actually search the web for results.\n\nIt returns results from its domain knowledge, which isnâ€™t what the users expect.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*YcI-UuEmmFTQPUlO6mjpxA.png)\n\nThe correct behavior should be to disable the *search* function once the user switches to â€œChatGPT o1\\-previewâ€ because this model does not have the capability to search the web.\n\n\n## Final Thoughts\n\nIâ€™m really glad OpenAI has finally rolled out SearchGPT. Iâ€™ve been wanting to test it out ever since they announced it in July 2024\\.\n\nIn its current state, SearchGPT is a good first step for OpenAI into the world of AI\\-powered search, but itâ€™s not quite ready to become anyoneâ€™s go\\-to tool for complex, real\\-time queries.\n\nThe accuracy, speed, and ability to handle conversational context just arenâ€™t there yet. For now, if you need deep insights or faster results, tools like Perplexity Pro or Google remain the better options.\n\nFurther reading:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*5ejBBgbZaE8pGmpW.png)\n\nThis story is published on [Generative AI](https://generativeai.pub/). Connect with us on [LinkedIn](https://www.linkedin.com/company/generative-ai-publication) and follow [Zeniteq](https://www.zeniteq.com/) to stay in the loop with the latest AI stories.\n\nSubscribe to our [newsletter](https://www.generativeaipub.com/) and [YouTube](https://www.youtube.com/@generativeaipub) channel to stay updated with the latest news and updates on generative AI. Letâ€™s shape the future of AI together!\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*TnRFuKk-2Dj_KCAP.png)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/openai-searchgpt-chatgpt-with-internet-and-browsing-tools-023ddca7cb44","frontmatter":{"title":"OpenAI SearchGPT: ChatGPT with Internet and browsing tools","meta_title":"OpenAI SearchGPT: ChatGPT with Internet and browsing tools","description":"A Better Alternative for Perplexity and Google Search","date":"2024-11-08T00:28:30.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*N_EtjjOxkx6QsKRLx5f_cQ.png","categories":["Technology/Web","Data Science","SearchGPT"],"author":"Rifx.Online","tags":["SearchGPT","filtering","citations","recommendations","customization"],"draft":false,"slug":"blog/openai-searchgpt-chatgpt-with-internet-and-browsing-tools-023ddca7cb44"},"content":"\n\n\n\n\n### A Better Alternative for Perplexity and Google Search\n\n\n\nAnd the much\\-anticipated product by OpenAI, SearchGPT is out last night boasting some major features, taking it a step ahead of Perplexity, their arch\\-rivals.\n\n\n\n\n\n\n\nAs announced by OpenAI, SearchGPT is a lot more than just ChatGPT with the internet.\n\nIt is an AI web browser in itself\n\nTalking about a few key features:\n\n* **Advanced Filtering**: Set filters for specific dates, sources, or content types (e.g., peer\\-reviewed articles only, government sites, etc).\n* **Context\\-Aware Summaries**: Generate summaries, key takeaways, or insights tailored to particular fields, like medicine or finance.\n* **Citation Generation**: Automatically format and provide citations in academic styles (APA, MLA).\n* **Multi\\-Step Queries**: Handle complex, layered questions across multiple sources in a single search.\n* **Data Analytics Integration**: Directly pull and analyze data for insights (e.g., trend analysis). SearchGPT could connect to specialized databases, allowing access to specific fields (like medical journals, legal case databases, or proprietary business analytics).\n* **Personalized Recommendations**: Suggest relevant sources, articles, or updates based on your search history. This includes **Pre\\-set Templates or Personas**: For example, a research\\-focused â€œSearchGPTâ€ could be set up to retrieve scientific data and offer academic citations directly.\n\n\n### Was ChatGPT not able to access the internet before?\n\nIt does (for premium members). But now, these capabilities are more advanced. To check out how a general web search is different from SearchGPT, I tried asking ChatGPT itself (free version) to search the internet for a query:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ORjGLDBqKDWiPANHlxSdqw.png)\n\nThen, I asked how this search was done.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*NlzDed3nHdJ636aLt75DCg.png)\n\nNext up,\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*eeDPQHQA62yaMK_KkSL0kQ.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ZcsmVgau0SaavN01yHbdKw.png)\n\nSo, as you can read, SearchGPT is not just a mere web browsing tool, but a lot more. Unfortunately, OpenAI has made SearchGPT available for Pro users and only to the waitlist users. If youâ€™ve access, you must have received this mail yesterday\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*WfO0Xl4VQwRxNNkC1GQpOw.png)\n\nBelow are some screen grabs from SearchGPT\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8bbGpwbsRzo6xQhNDPb3Jw.png)\n\nAs you can see, it provides trending topics as suggestions while searching, similar to a web browser.\n\nTry checking out the weather today\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*gbtr-QFQw4BnhqrWPvH4RQ.png)\n\nAnd can even restrict it to just check specific websites like â€œcite just government sitesâ€\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*eq_Xf4JkD4XV85KE5376VA.png)\n\nAll the citations can be explored at the bottom together for your results\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*wUTEae5yYh_j-oaq6HyP9Q.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Quruyw07__p3qoJ_KhmHAA.png)\n\n\n## SearchGPT vs Perplexity. Which is better?\n\nA tough question to answer, at least for now. A few points to highlight are\n\n1. Perplexity is free, SearchGPT is not!\n2. SearchGPT is faster. But I assume this is because of lesser traffic right now.\n3. Perplexity is simpler, and also, has the early mover advantage\n4. Perplexity, being in the space for a long time, is more reliable compared to SearchGPT\n5. SearchGPT provides more customizations and is not just LLM with the internet.\n\nTo be honest, Iâ€™m always in favour of free stuff hence will prefer Perplexity any day. Though, given SearchGPTâ€™s early response, the tool is pretty good and worth trying out.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/openais-leaked-gpt2-model-has-everyone-stunned-6337904c2ecf","frontmatter":{"title":"OpenAIâ€™s â€˜Leakedâ€™ GPT2 Model Has Everyone Stunned.","meta_title":"OpenAIâ€™s â€˜Leakedâ€™ GPT2 Model Has Everyone Stunned.","description":"On-Purpose leak?","date":"2024-11-01T04:07:40.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*-G0yfSjGPdNw02NZ","categories":["Chatbots","Generative AI","Natural Language Processing"],"author":"Rifx.Online","tags":["GPT-2","Chatbot","Inference","JSON","AlphaGo"],"draft":false,"slug":"blog/openais-leaked-gpt2-model-has-everyone-stunned-6337904c2ecf"},"content":"\n\n\n\n\n### On\\-Purpose leak?\n\n\n\nThe influence that OpenAI has on the AI industry canâ€™t be understated. Every move or decision makes headlines automaticallyâ€¦ even if they donâ€™t actually announce the thing.\n\nA few days ago, a model many of us played with that has since been deleted has the entire AI industry fascinated. Named â€œgpt2\\-chatbotâ€ it was accessible for a few days in the â€˜Direct Chatâ€™ function in [lmsys.org](https://chat.lmsys.org/).\n\n*But why so much fuss?*\n\nWell, because this model is unlike anything we have ever seen. **Itâ€™s on a completely different level.**\n\nFor this reason, many believe it has been the unofficial teaser of **ChatGPT\\-4\\.5** or even **GPT\\-5**. Or, even more exciting, using the number â€˜2â€™ as a signal that a **new GPT generation of long\\-inference models is approaching**.\n\nEven Sam Altman, CEO of OpenAI, couldnâ€™t resist the temptation to acknowledge its existence and tease us in the process:\n\n\n\n\n\n\n\nSo, *how good is this model, and what on Earth is it?*\n\n\n> You are probably sick of AI newsletters talking about how this or that \\*\\*just\\*\\* happened. And these newsletters abound because coarsely talking about events and things that already took place is easy, **but the value provided is limited and the hype exaggerated.**\n\n\n> However, newsletters talking about what **will** happen are a rare sight. If youâ€™re into easy\\-to\\-understand insights looking into the future of AI before anyone else does, **TheTechOasis** newsletter might be perfect for you.\n\n\n> ðŸï¸ðŸï¸ Subscribe today below:\n\n\n## A Teaser of Whatâ€™s to Come\n\nWith every passing day, itâ€™s clear that OpenAIâ€™s next model will be a leap in reasoning and complex problem\\-solving.\n\nAnd to prove how this new mysterious model might be it, here are just a few examples of the prowess of this mysterious model that could signal that the boat has landed in that port:\n\n\n> All examples below are considered **hard or outright impossible** for the current state\\-of\\-the\\-art models.\n\nFor starters, It solved a math\\-olympiad problem in zero\\-shot mode (without being provided with auxiliary examples to support the resolution):\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*oNPg_hTGc0OP90n9)\n\nI canâ€™t even start to explain how crazy the previous example is, itâ€™s absolutely impossible to get such an answer from the current state\\-of\\-the\\-art models.\n\n[Itâ€™s also absolutely superb at parsing JSONs](https://twitter.com/skirano/status/1785035706173214888), a fundamental skill for LLM integration with APIs and other web\\-based tools.\n\nAlso, it completely obliterates GPT\\-4 at complex drawing tasks like [drawing SVG files based on code](https://twitter.com/decentricity/status/1785049191003361778) or **unicorns using ASCII code (below)**, humiliating **Claude 3 Opus**, the current state\\-of\\-the\\-art, in the process:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*5A0EcRU91ZYAwVAc)\n\nAdditionally, although this very well could have been a hallucination, **the model claimed to me that it was trained by OpenAI and based on a GPT\\-4 variant.**\n\nOf course, after such a demonstration of power, **many suggest that â€œgpt2\\-chatbotâ€ might even be the famous Q\\* model**.\n\nBut instead of simply giving in to the different fanciful options people have claimed this is, letâ€™s take a more sensible approach and see what OpenAI itself has been hinting at through their research for months (and years).\n\n\n## The Power of Long Inference\n\nFor several months, experts in the space like [Demis Hassabis](https://www.youtube.com/watch?v=eqXfhejDeqA&t=2s) or [Andrej Karpathy](https://youtu.be/c3b-JASoPi0?si=fZWoSpLuSmua8YMR&t=1481) have discussed how LLMs alone simply arenâ€™t it, and that we need â€˜something elseâ€™ to really take them to the next step.\n\nIn both cases, they refer to achieving the equivalent of â€˜AlphaGo but in LLMsâ€™, which is indirectly referring to:\n\n* **Self\\-improvement** and\n* **test\\-time computation** LLMs\n\n*But what do they mean by that?*\n\n\n### A Giant Step for AI\n\nAlphaGo is history of AI. It was the first model that unequivocally surpassed human might in the game of **Go**, a Korean board game.\n\nIt used **Monte Carlo Tree Search**, a search algorithm, to explore the realm of possible moves for any given step in the game, being able to go beyond the current action and predict what the opposing player would do.\n\n\n> Some of you might remember **Deep Blue** too, the chess machine that barely beat Gary Kasparov in the second game in their series back in 1997 after losing the first game.\n\n\n> However, while Deep Blue could be beaten, AlphaGo was invincible.\n\n*But how?*\n\n\n### Self\\-improving to go Superhuman\n\nThe key element that made AlphaGo superior was how it was trained, **by playing against lesser versions of itself to create a self\\-improvement loop.**\n\nIt consistently played against itself, gradually improving its ELO to 3\\.739, almost at the level of todayâ€™s best Go player.\n\n\n> In 2017, AlphaZero, an improved version, achieved a 5\\.018 ELO, completely superhuman and unbeatable.\n\nIn other words, with AlphaGo humans had achieved, for the first time, a way to train a model by self\\-improvement, allowing it to achieve superhuman capacities **as it no longer relied on imitating humans to learn.**\n\nIn case youâ€™re wondering, this is not the case for LLMs.\n\nCurrent LLMs are completely chained to human\\-level performance, as all data and training are inherently human\\-dependent (to the point that [the alignment phase](https://thewhitebox.ai/llms-the-backbones-of-frontier-ai/), the part of the training process where LLMs are modeled to improve their safety levels and avoid offensive responses, **is strictly executed using â€˜human preferencesâ€™**).\n\n\n> On a side note, [Meta recently proposed Self\\-Rewarding Models](https://arxiv.org/pdf/2401.10020v1) that could self\\-improve with their own responses. However, itâ€™s unclear whether this feedback loop really can make LLMs superhuman.\n\nBut even though it still feels hard to believe that â€œgpt2\\-chatbotâ€ has been trained through self\\-improvement, **we have plenty of reasons to believe itâ€™s the first successful implementation of what OpenAI has been working on for years: test\\-time computation**.\n\n\n### The Arrival of test\\-time computation models\n\nOver the years, several research papers by OpenAI have hinted at this idea of skewing models into â€˜heavy inferenceâ€™.\n\nFor example, back in 2021, [they presented the notion of using â€˜verifiersâ€™](https://arxiv.org/pdf/2110.14168) at inference to improve the modelâ€™s responses when working with Math.\n\nThe idea was to train an auxiliary model that would evaluate in real\\-time several responses the model gave, choosing the best one (which was then served to the user).\n\nThis, combined with some sort of tree search algorithm like the one used by AlphaGo, with examples like Google Deepmindâ€™s [Tree\\-of\\-Thought research](https://arxiv.org/pdf/2305.10601) for LLMs, and you could eventually create an LLM that, before answering, explores the â€˜realm of possible responsesâ€™, **carefully filtering and selecting the best path toward the solution.**\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*pHWwOA66fxpKbl-z)\n\nThis idea, although presented by OpenAI back in 2021, has become pretty popular these days, [with cross\\-effort research by Microsoft and Google applying it to train next\\-generation verifiers](https://arxiv.org/pdf/2402.06457), and with Google even managing to create a model, [Alphacode](https://storage.googleapis.com/deepmind-media/AlphaCode2/AlphaCode2_Tech_Report.pdf), that executed this kind of architecture to great success, **reaching the 85% percentile among competitive programmers, the best humans at it.**\n\n*And why does this new generation of LLMs have so much potential?*\n\nWell, **because they approach problem\\-solving in a very similar way to how humans do**, through the exercise of deliberate and extensive thought to solve a given task.\n\nBottom line, think of â€˜search\\+LLMâ€™ models as AI systems that allocate a much higher degree of compute (akin to human thought) to the actual runtime of the model so that, instead of having to guess the correct solution immediately, they are, simply put, â€˜given more time to thinkâ€™.\n\nBut OpenAI has gone further.\n\n\n### PRM Models for Improved Maths Execution\n\nBack in May last year, they released the paper [Letâ€™s Verify Step\\-by\\-Step](https://arxiv.org/pdf/2305.20050), with the participation of the man himself Ilya Sutskever, Chief Scientist at OpenAI, and some of the researchers from the original verifier paper like Karl Cobbe.\n\nThe idea here is to modify the reward model used during the alignment phase of the model.\n\n[Although I recommend checking this article for a full guide on LLM training](https://thewhitebox.ai/llms-the-backbones-of-frontier-ai/), the last step in the process of creating products like ChatGPT is the use of Reinforcement Learning from Human Feedback, or RLHF.\n\nThe idea is for the model to improve its decision\\-making. Thus, we train an auxiliary reward model (which is essentially an almost identical copy of the model being trained) that learns to rank the results of the trained model according to human preferences.\n\n*The issue?*\n\nWell, most reward models today are **ORMs, or Outcome\\-Supervised Reward Models**. In laymanâ€™s terms, to evaluate the degree of correctness or the modelâ€™s prediction, they look at it globally, disregarding the entire â€˜thought processâ€™.\n\nOn the other hand, **PRMs, or Process\\-Supervised Reward Models, evaluate every single step in the response of the model**. Consequently, they â€˜forceâ€™ the model to pay close attention and effort to every single step of the process, which is crucial in situations like solving a maths equation like the one below:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8JC6sZl5UFfl3WorliQy-A.png)\n\nHowever, this is a very, very expensive process as the preference data requires heavy human crafting so that the supervisory signal can be applied. Consequently, every single training example has dozens or more rewards to measure.\n\nTherefore, â€œgpt2\\-chatbotâ€ might have included some sort of variation of the reward training considering how proficient it is at generating plans and executing complex problem\\-solving.\n\n\n## Impossible not to Get Excited\n\nConsidering gpt2\\-chatbotâ€™s insane performance, and keeping in mind OpenAIâ€™s recent research and [leaks](https://www.reuters.com/technology/sam-altmans-ouster-openai-was-precipitated-by-letter-board-about-ai-breakthrough-2023-11-22/), we might have a pretty nice idea by now of what on Earth this thing is.\n\nWhat we know for sure is that we are soon going to be faced with a completely different beast, one that will take AIâ€™s impact to the next level.\n\n* *Have we finally reached the milestone for LLMs to go beyond human\\-level performance as we did with AlphaGo?*\n* *Is the age of long inference, aka the conquest of System 2 thinking by AI, upon us?*\n\nProbably not. However, itâ€™s hard not to feel highly optimistic for the insane developments we are about to witness over the following months.\n\nIn the meantime, I guess we will have to wait to get those answers. But not for long.\n\n\n> On a final note, if you have enjoyed this article, I share similar thoughts in a more comprehensive and simplified manner for free on my [LinkedIn](https://www.linkedin.com/in/ignacio-de-gregorio-noblejas/).\n\n\n> If preferable, you can connect with me through [X](https://twitter.com/TheTechOasis1).\n\n\n> Looking forward to connecting with you.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/overcoming-llm-challenges-in-healthcare-practical-strategies-for-development-in-production-04c617954b9a","frontmatter":{"title":"Overcoming LLM Challenges in Healthcare: Practical Strategies for Development in Production","meta_title":"Overcoming LLM Challenges in Healthcare: Practical Strategies for Development in Production","description":"An article on the most common LLM development challenges Iâ€™ve encountered, effective mitigation strategies, and a career-defining interviewâ€¦","date":"2024-11-08T00:20:35.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Vak28ygruWKySsH0doGoYg.png","categories":["Health","Generative AI","Machine Learning"],"author":"Rifx.Online","tags":["LLMs","healthcare","hallucinations","validation","monitoring"],"draft":false,"slug":"blog/overcoming-llm-challenges-in-healthcare-practical-strategies-for-development-in-production-04c617954b9a"},"content":"\n\n### Generative AI\n\n\n\n\n\n### An article on the most common LLM development challenges Iâ€™ve encountered, effective mitigation strategies, and a career\\-defining interview mistake\n\n\n## Introduction\n\nIâ€™ve always been the type to dive deep into a subject and specialize to obsession. When I graduated from my masterâ€™s in data science, the obsession I had was with computer vision; specifically, computer vision to apply towards neuroscience or mental health applications. I was set on becoming a â€œcomputer vision engineerâ€ (but â€œmachine learning engineerâ€ would be okay too) in the mental health field, despite my mentors urging me to broaden my scope and get my foot in the door. I silenced my own wary voices, convinced that the right team would recognize my â€œexpertiseâ€.\n\n\n\nLuckily, my theory seemed to work; I landed interviews with several mental health companies. But then came one of my biggest interview mistakes. In the final round for my top choice â€” a company I loved â€” I made an error that still makes me internally cringe when I reflect. The role was NLP\\-focused, working with text data, but I couldnâ€™t help expressing my interest in imaging data. *Cries in recollection.* I vividly recall the interviewerâ€™s expression transforming from one of excitement to one of concern the moment I asked about imaging data availability, as I was still drawn to computer vision. Later that day, I received a polite rejection: they loved my passion but needed someone fully committed to NLP.\n\nIronically, I soon joined another mental health company and shifted fully to NLP work, creating anxiety and depression symptom detectors that improved clinical care and developing recommendation systems that boosted content discoverability by 12%. Fast\\-forward a few years, and Iâ€™m now the NLP/LLM data scientist on my team, with 6 information extraction tasks, 5 classification tasks, and 5 conditional summarization tasks deployed across 15\\+ hospitals and five clients.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*-VOHDQd88fCyRqoY9bR3hQ.png)\n\nA couple of weeks ago, I was asked to present â€œLLM development 101â€ to my larger data team. Initially, imposter syndrome crept in â€” *what could I share for 45 minutes on LLM development?* But as I created my slides, I realized how much I had to say and grew excited about sharing the depth of knowledge Iâ€™ve learned. This excitement led to the article youâ€™re reading right now. In this article, Iâ€™ll walk through some common challenges Iâ€™ve encountered with LLMs in production and the strategies that have helped me solve them.\n\n\n## 1\\. Output Format Errors\n\nThis is surprisingly probably the most frequent issue I encounter. Output format reliability can vary significantly depending on the model Iâ€™m working with. For example, GPT\\-4 Turbo generally provides consistent JSON outputs, but GPT\\-4o tends to be less reliable in this regard. With GPT\\-4o, Iâ€™ve encountered everything from lists and strings to incomplete dictionaries when a structured JSON output was explicitly requested. If these format issues arenâ€™t caught and the model isnâ€™t re\\-run, I risk having incomplete data coverage.\n\n\n### Impact of Format Errors\n\nInconsistent output formats can have a significant impact on downstream processes. If the data structure is incorrect, it could lead to failures in subsequent processing steps, skew reporting accuracy, or even result in incomplete insights if left undetected. In high\\-stakes fields like healthcare, where my work applies, incomplete or mis\\-structured data can have real implications, making format consistency essential.\n\n\n### Mitigation\n\nTo handle this, Iâ€™ve implemented **format\\-checking logic** that **validates the output structure**. If itâ€™s incorrect, I re\\-run the model until it matches the expected format. Additionally, I use **logging** to capture format\\-related errors. Re\\-running the model, however, comes with trade\\-offs, such as increased latency and higher API costs. I establish thresholds for re\\-running based on the criticality of the data coverage and cost limitations. If re\\-running isnâ€™t feasible, I sometimes apply post\\-processing to â€œrepairâ€ the output structure, though this approach carries its own risks of introducing errors or inconsistencies.\n\nTo illustrate this approach, hereâ€™s a sample code snippet that requests patient data in JSON format with specific keys like `\"name\"`, `\"age\"`, and `\"insurance\"`. This code demonstrates a method to verify that the modelâ€™s response includes all required fields and adheres to the expected structure. By implementing retry logic, the code aims to ensure data consistency, reducing the risks associated with format errors in critical workflows.\n\n\n```python\ndef get_llm_response(prompt: str, required_keys: Set[str], retries: int = 3) -> Optional[Dict[str, Any]]:\n    \"\"\"\n    Calls the language model to get a response in JSON format. If the response \n    is not in the expected JSON format or lacks required keys, retries the call \n    up to `retries` times.\n    Parameters:\n        prompt (str): The prompt sent to the language model.\n        required_keys (Set[str]): A set of required keys that must be present in the JSON response.\n        retries (int): The maximum number of retries if the output format is invalid.\n    Returns:\n        Optional[Dict[str, Any]]: Parsed JSON response if successful; None if retries are exhausted.\n    \"\"\"\n    \n    for attempt in range(retries):\n        try:\n            response = openai.Completion.create(\n                model=\"gpt-4o\",\n                prompt=prompt,\n                max_tokens=100,\n                temperature=0.7\n            )\n            \n            # Attempt to parse the response as JSON\n            response_text = response.choices[0].text.strip()\n            parsed_response = json.loads(response_text)\n            \n            # Check if parsed_response is in the expected structure and contains required keys\n            if isinstance(parsed_response, dict) and required_keys.issubset(parsed_response.keys()):\n                return parsed_response\n            else:\n                print(f\"Attempt {attempt + 1}: Output format invalid or missing required keys, retrying...\")\n        except (json.JSONDecodeError, KeyError) as e:\n            print(f\"Attempt {attempt + 1}: Error parsing JSON - {str(e)}, retrying...\")\n    print(\"Max retries exceeded: Unable to get valid JSON output with required keys.\")\n    return None\n\n```\n\n## 2\\. Hallucinations\n\nHallucinations happen when the model invents information that sounds plausible but isnâ€™t actually there. For instance, when Iâ€™m trying to pull quotes from source text, sometimes the model decides to â€œget creativeâ€ and produces similar\\-sounding but completely fabricated phrases. In fields where accuracy is crucial, like healthcare, small hallucinations can lead to large issues.\n\n\n### Mitigation\n\nI address hallucinations by implementing post\\-processing logic to validate that, for any information extraction tasks, the context pulled matches the source text exactly. To ensure that minor variations donâ€™t lead to missed matches, I standardize the text by stripping punctuation and converting everything to lowercase when comparing the source and retrieved text. Additionally, several other strategies help minimize hallucinations. For instance, **chain\\-of\\-thought prompting**, where the model explains each step of its reasoning, can produce more grounded outputs and reduce the likelihood of inaccurate output. In high\\-stakes applications (such as healthcare use cases), **human\\-in\\-the\\-loop checks** are important as an extra layer of review, helping catch hallucinations that automated processes might miss. Lastly, prompts that emphasize factual accuracy, such as instructing the model to â€œonly use exact phrases from the source,â€ can guide the model toward more precise responses.\n\n\n## 3\\. Outdated Information\n\nOutdated information can be challenging to manage, especially in applications where accuracy and timeliness are essential. Sometimes, a model might retrieve information from older sections of a document and surface it as if itâ€™s current. With Retrieval\\-Augmented Generation (RAG), this issue can become even more complex, as RAG retrieves content based solely on relevance rather than timeliness or specific document sections. The absence of section labels or timestamps means RAG may pull from parts of a document that seem relevant without discerning if theyâ€™re outdated, which risks mixing older and current information. Another challenge with using a vector database is that if we store entire documents, we canâ€™t easily remove specific sections without clearly defined labels, making it hard to filter out irrelevant information effectively.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*k9btdwyCCAb9qp92gB0PwA.png)\n\n\n### Mitigation\n\nTo address this, I specify â€œcurrentâ€ or â€œmost recentâ€ data directly in the prompt and use preprocessing steps to remove any outdated sections before passing data to the model. This extra preprocessing step ensures that only the latest, most relevant information is retained, helping the model focus on providing timely and accurate responses. This step not only ensures more accurate outputs, but it also reduces the cost of the call. By implementing these filters in advance, I can maintain consistency and relevance in the modelâ€™s outputs.\n\n\n## 4\\. Over\\-Reliance and Ethics\n\nAs much as I would love for the work I do to be used and useful, my biggest fear is that users would trust the model predictions a bit too much â€” especially in the healthcare space, where generative AI is often producing summaries or extracting specific patient details, not just making predictions. Experts may hold differing views on certain definitions, so diversity and dialogue is important to reach a consensus. Over\\-reliance on these predictions, could lead care teams to limit these conversations and overlook errors they might otherwise examine more closely.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*6-0mq8Svxh8ATuyT)\n\n\n### Mitigation\n\nI prioritize educating the team on the modelâ€™s limitations, including its tendency for errors, and encourage them to see AI as a complement to human expertise. In healthcare, where nuance is critical, human\\-in\\-the\\-loop oversight is essential for high\\-impact cases, allowing experts to review AI outputs and reduce risks from over\\-reliance. This collaborative approach allows AI to amplify expert insights, maintaining the reliability and ethical integrity that high\\-stakes applications demand.\n\n\n## 5\\. Rapid Model Deprecation\n\nWith the rapid pace of development in AI, model and API versions are updated frequently, and itâ€™s common for versions to be deprecated faster than expected. If youâ€™ve ever had a workflow break unexpectedly because a model version was retired, youâ€™ll know how disruptive this can be. This has happened several times in the past year, requiring us to quickly re\\-do analyses to ensure the newer model versions still perform as expected.\n\n\n### Mitigation\n\nMake it a priority to do regular check\\-ins to monitor model versions and stay ahead of deprecation warnings. This proactive approach would enable us to plan transitions in advance, saving the last\\-minute scramble. While itâ€™s a small step, it makes a significant difference in maintaining smooth operations.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GK08JY3dcRUS4r6Z0x6EmA.png)\n\n\n## 6\\. Rate Limiting with APIs\n\nAPI rate limits are a subtle but significant challenge, especially when working with high volumes of requests. Hitting a rate cap can create delays, slow down real\\-time workflows, or even halt entire processes. In cases where weâ€™re processing time\\-sensitive data, reaching the limit can be highly disruptive, as workflows come to an unexpected stop. This is especially problematic in healthcare settings, where timing can directly impact operations and patient care.\n\n\n### Mitigation\n\nTo mitigate this, weâ€™ve implemented a proactive approach by tracking API usage patterns to identify peak times and reduce non\\-essential calls. By staggering requests and batching calls, I can distribute the load more evenly and avoid exceeding limits. In situations where demand is high and rate limits are consistently reached, requesting additional quota from the provider can offer a practical solution. Balancing usage has been essential, and understanding our peak times and usage patterns ahead of time has proven crucial for maintaining a stable, uninterrupted workflow.\n\n\n## Concluding Remarks\n\nThese are just six of the common issues Iâ€™ve faced while working with LLMs. I didnâ€™t expect to find myself here, but taking a step back to reflect, I realize how much expertise Iâ€™ve developed in this space â€” and Iâ€™m incredibly excited to continue to share these learnings in upcoming articles. Iâ€™d love to hear from others about the challenges theyâ€™ve encountered and the mitigation strategies or workarounds theyâ€™ve found effective, whether related to these issues or new ones entirely. I hope these insights are helpful and spark further conversation around best practices in this quickly evolving field (where model versions and API versions deprecate a little too quickly).\n\n\n"},{"lang":"en","group":"blog","slug":"blog/qwen-new-release-the-king-of-coder-is-qwen2-5-coder-32b-8b96d442b280","frontmatter":{"title":"Qwen New Release: The King of Coder is Qwen2.5 coder 32B!","meta_title":"Qwen New Release: The King of Coder is Qwen2.5 coder 32B!","description":"The article introduces Qwen2.5-Coder-32B-Instruct, a new AI coding model that outperforms existing open-source models, including GPT-4o, according to benchmark scores. It operates efficiently on a single GPU, achieving 32 tokens per second. The model is available under the Apache 2.0 license and is supported by Ollama for easy deployment. Smaller models are also available for users with limited computing resources. The integration of Qwen models with OpenWebUI enhances usability for daily tasks.","date":"2024-11-14T03:32:00.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*OzrZMolY75t_cdux5UGtIg.png","categories":["Programming","Technology","Machine Learning"],"author":"Rifx.Online","tags":["Qwen2.5","Coder","32B","Instruct","GPU"],"draft":false,"slug":"blog/qwen-new-release-the-king-of-coder-is-qwen2-5-coder-32b-8b96d442b280"},"content":"\nGreat new everyone! Meet Qwen2\\.5\\-Coder\\-32B\\-Instruct: the latest AI model thatâ€™s taking the code world by storm!\n\n\n\nMost of these models are released under the Apache 2\\.0 license .Benchmark scores are through the roof:\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*aHeNvfOvcpME0qzy6EQexQ.jpeg)\n\nAs we can see, itâ€™s the best among open source models and even beats the GPT\\-4o.\n\nOllama has already provided its support for several model series.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*rV1xrpRXUjTFFoKwSsfeOg.png)\n\nSo itâ€™s easy to run it.\n\n```python\nollama run qwen2.5-coder:32b\n```\n\nThe performance of the 32B (Q4 format) on one single GPU 3090 can be found from following screenshot:\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MVQ0srQhRxX4Ifo3IqU6og.png)\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*jtH3ixeeOQGfyDzmO4Ni3A.png)\n\nItâ€™s **32 tokens/s**! super fast. I am very happy and impressed.\n\nBesides the 32B model, the smaller models have shown impressive performance in terms of model size. If you do not have enough computing power, try some of the smaller models. For example, I tried the 14B model on the new Mac Mini with M4 processor and 16GB RAM.\n\nIn addition to benchmark scores, the cursor can now be integrated with the latest Qwen models, including Qwen 2\\.5\\-Coder\\-32B\\-Instruct \\& OpenWebUI.\n\nBelow is a screenshot of using Qwen 2\\.5\\-Coder\\-32B\\-Instruct(Ollama) with OpenWebUI:\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*q-Pq3snVhkBs3e_Oxj4_Xw.png)\n\nI canâ€™t wait to use it for my day to day works!\n\n\n"},{"lang":"en","group":"blog","slug":"blog/qwen2-5-1-5b-the-future-of-mobile-ai-6bd5f29bbc84","frontmatter":{"title":"Qwen2.5 1.5b: the future of Mobile AI?","meta_title":"Qwen2.5 1.5b: the future of Mobile AI?","description":"Local Testing and Evaluation of Alibaba Cloudâ€™s Latest LLM. With llama-cpp-python and a DIY prompt catalog.","date":"2024-10-30T12:57:39.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*awb56jkdXobA-Ip6d-QHRA.png","categories":["Natural Language Processing","Programming","Technology/Web"],"author":"Rifx.Online","tags":["Qwen2.5","NLP","summarization","retrieval","prompts"],"draft":false,"slug":"blog/qwen2-5-1-5b-the-future-of-mobile-ai-6bd5f29bbc84"},"content":"\n### Local Testing and Evaluation of Alibaba Cloudâ€™s Latest LLM. With llama\\-cpp\\-python and a DIY prompt catalog.\n\n\n\nIn part one we explored together the innovations from Alibaba Cloudâ€™s team with the release of the Qwen2\\.5 models family.\n\nIn Generative AI benchmarks are now the main *oracle*: the validity of a new LLM needs to pass several verdicts. The more benchmark records you break, the better you are.\n\nIt is the way to win the SOTA race\n\nWell, I disagree. Even though for the AI advancement we need milestones and better performances, still the user experience and the personal point of view cannot be just put aside as irrelevant.\n\nI believe that exploring some frequently used NLP tasks, and putting aside the chat experience, we must focus on the quality of the replies. And we are the only benchmark required. Our user experience is the best indicator to understand if a model is good or not. The model must be reliable enough to be used in an automated workflow.\n\nBy the way, I already run what I decided to call [RBYF â€” Revised Benchmarks with You as a Feedback](https://open.substack.com/pub/thepoorgpuguy/p/rbyf-is-here-revised-benchmarks-with?r=i78xo&utm_campaign=post&utm_medium=web) on the claimed amazing Llama3\\.2â€“1B\\-instructâ€¦ and Qwen2\\.5â€“1\\.5b is so much better!\n\nSo in this article, as promised, we will verify with our own eyes how good is this model for every day use.\n\nBack to usâ€¦ Letâ€™s get started!\n\n## Requirements\n\nHere we are going to build a minimal text interface to be able to run the model, test different tasks and wait for user feedback to evaluate it.\n\nThe requirements are minimal, but I suggest you to create a new project directory and a virtual environment.\n\nCreate a `venv` (python 3\\.11\\+ is required): I tested it on my Mini\\-PC running Windows 11\\.\n\n```python\n## create the virtual environment\npython -m venv venv\n## activate the venv\nvenv\\Scripts\\activate\n## Install the dependencies \npip install llama-cpp-python==0.3.0 tiktoken\n```\n\nWe need to download the GGUF file from the official qwen repository on Hugging Face [https://huggingface.co/Qwen/Qwen2\\.5\\-1\\.5B\\-Instruct\\-GGUF](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF): I used the `qwen2.5-1.5b-instruct-q5_k_m.gguf` version.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*YtQJb_xyq_xcF40yRWPcZA.png)\n\nWe are all set!\n\nNote: if you want to add a different Backend support of a GPU accelerator, you can follow [the instructions in the repo](https://github.com/abetlen/llama-cpp-python#supported-backends). I used, for example, the Vulkan support so before the pip install I added the environment variable\n\n```python\n## Vulkan support - for Windows\n$env:CMAKE_ARGS = \"-DGGML_VULKAN=on\"\n```\n\n## The Code â€” a main app and a library\n\nTo keep the code at minimum, I decided to extend some functionalities using an external library. Well, it is a Do It Yourself library, so there are no secrets here.\n\nYou can find all the details in my article here:\n\nAnd to speed it up you can directly [download the file from here](https://github.com/fabiomatricardi/YouAreTheBenchmark/raw/main/QWEN2.5-1.5B/promptLibv2Qwen.py): it contains a version 2 of the `promptLib` discussed in the mentioned above article (and it is called `promptLibv2Qwen.py`, with few fine tuning of the prompt specifically tailored for the `Qwen2.5-1.5B-instruct` model.\n\nSave the file in the main directory, and create a new file called `main.py`\n\n```python\n## Chat with an intelligent assistant in your terminal  \n## MODEL: https://huggingface.co/Qwen\n## qwen2.5-1.5b-instruct-q5_k_m.gguf\nimport sys\nfrom time import sleep\nimport warnings\nwarnings.filterwarnings(action='ignore')\nimport datetime\nfrom promptLibv2Qwen import countTokens, writehistory, createCatalog\nfrom promptLibv2Qwen import genRANstring, createStats\nimport argparse\n### PREPARING FINAL DATASET\npd_id = []\npd_task = []\npd_vote = []\npd_remarks = []\n####################Add GPU argument in the parser###################################\nparser = argparse.ArgumentParser()\nparser.add_argument(\"-g\", \"--gpu\", type=int, default=0,nargs='?',\n                    help=\"The number of layers to load on GPU\")\nargs = parser.parse_args()\nif args.gpu == None:\n   ngpu_layers = 0 \nelse:\n    ngpu_layers = args.gpu\nprint(f'Selected GPU: offloading {ngpu_layers} layers...')   \n####################INITIALIZE THE MODEL###################################\nstops = ['<!im_end|>']\ntasks = createCatalog()\nmodelname = 'qwen2.5-1.5b-instruct-q5_k_m.gguf'\n## create THE LOG FILE \ncoded5 = genRANstring(5)\nlogfile = f'logs/Qwen2.5-1.5B-it_CPP_{coded5}_log.txt'\ncsvfile = f'logs/Qwen2.5-1.5B-it_CPP_{coded5}.csv'\nlogfilename = logfile\n#Write in the history the first 2 sessions\nwritehistory(logfilename,f'{str(datetime.datetime.now())}\\n\\nYour own LocalGPT with ðŸ’» {modelname}\\n---\\nðŸ§ ðŸ«¡: You are a helpful assistant.')  \nwritehistory(logfilename,f'ðŸ’»: How can I assist you today in writing?')\n```\n\nHere we are only doing preparations: we import the libraries, including our own personal `promptLibv2Qwen` and also `argparse`. I wanted to try something new: [argparse](https://realpython.com/command-line-interfaces-python-argparse/) is a python library intended for terminal python program where you are reading multiple arguments from the command line.\n\nIn this case here we have only one argument (and no parameters) with th flag `-g` or even `--gpu`. When you run the python code with this argument we will set the number of GPU layers to the maximum (but you can change it yourself).\n\nThen we set some global variables, used across the entire code: the tasks, our prompt collection, the stop words and the log filename.\n\n> NOTE: all the logs are saved into a subdirectory called `logs`â€¦ so make sure to create one.\n\nWe are also preparing all the relevant information to store them into a dataset and then save it at the end int a CSV file (for easily creating a performance matrix)\n\n```python\n### PREPARING FINAL DATASET\npd_id = []\npd_task = []\npd_vote = []\npd_remarks = []\n```\n\nWe then load the model into RAM (no GPU) or the VRAM (with GPU) using Llama\\-CPP\\-python.\n\n```python\n## LOAD THE MODEL\nprint(\"\\033[95;3;6m\")\nprint(\"1. Waiting 10 seconds for the API to load...\")\nfrom llama_cpp import Llama\nllm = Llama(\n            model_path='models/qwen2.5-1.5b-instruct-q5_k_m.gguf',\n            n_gpu_layers=ngpu_layers,\n            temperature=0.1,\n            n_ctx=8192,\n            max_tokens=1500,\n            repeat_penalty=1.178,\n            stop=stops,\n            verbose=False,\n            )\nprint(f\"2. Model {modelname} loaded with LlamaCPP...\")\nprint(\"\\033[0m\")  #reset all\nhistory = []\nprint(\"\\033[92;1m\")\nprint(f'ðŸ“Logfile: {logfilename}')\n```\n\nBy the way, you can find all the code in my GitHub Repository:\n\nThe next one is a one\\-shot warm up inference: the model neural network is going to be activated for the first time, so think about it like a warm\\-up lap.\n\nDonâ€™t be scared, I will explain the code\n\n```python\n##################### ALIGNMENT FIRST GENERATION ##############################################\nquestion = 'Explain the plot of Cinderella in a sentence.'\ntest = [\n    {\"role\": \"user\", \"content\": question}\n]\nprint('Question:', question)\nstart = datetime.datetime.now()\nprint(\"ðŸ’» > \", end=\"\", flush=True)\nfull_response = \"\"\nfisrtround = 0\nfor chunk in llm.create_chat_completion(\n    messages=test,\n    temperature=0.25,\n    repeat_penalty= 1.31,\n    stop=stops,\n    max_tokens=1500,\n    stream=True,):\n    try:\n        if chunk[\"choices\"][0][\"delta\"][\"content\"]:\n            if fisrtround==0:\n                print(chunk[\"choices\"][0][\"delta\"][\"content\"], end=\"\", flush=True)\n                full_response += chunk[\"choices\"][0][\"delta\"][\"content\"]\n                ttftoken = datetime.datetime.now() - start  \n                fisrtround = 1\n            else:\n                print(chunk[\"choices\"][0][\"delta\"][\"content\"], end=\"\", flush=True)\n                full_response += chunk[\"choices\"][0][\"delta\"][\"content\"]                            \n    except:\n        pass      \ndelta = datetime.datetime.now() - start\noutput = full_response\nprint('')\nprint(\"\\033[91;1m\")\nrating = input('Rate from 0 (BAD) to 5 (VERY GOOD) the quality of generation> ')\nprint(\"\\033[92;1m\")\nstats = createStats(delta,question,output,rating,logfilename,'Alignment Generation',ttftoken)\nprint(stats)\nwritehistory(logfilename,f'''ðŸ‘¨â€ðŸ’» . {question}\nðŸ’» > {output}\n{stats}\n''')\n```\n\nWe set the first user question and put into a well known chat format dictionary. Then we start our timer (useful for speed, token counts etcâ€¦).\n\nWe call the inference with the method `create_chat_completion()` that allows us to accept the prompts in chat format and stream the output one token at the time.\n\nBecause of the first reply from the model does not contain any output tokens (but only statistics) we use the try/except statement. Furthermore, since I want to know when the first token is generated, we raise a flag and stop temporary our time count saving the information inside the `ttftoken` variable.\n\nAt the end of the streaming we count the delta time from start, and wait the user to provide his/her personal feedback on the generated output: giving a mark from 0 to 5 and adding comments related to the compliance with the instruction prompt and user intent.\n\nWe use our internal library called `createStats()` to print all the statistics of the generation, and save them in our log file. The output of the function will be something like this:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8znYCqpisviXvYgrzjWF5w.png)\n\n## Prompt catalog â€” what we want to test\n\nI wrote here about my habit. I have a catalog of prompts that covers many of the main language tasks used in chat\\-bots, like summarization, short summarization, casual chat, RAG, truthful RAG and so on.\n\nThe idea is to be able to load the model in 5 minutes, and start evaluating each task. At the end of every generation the user is prompted to give a mark (a score from 0 to 5\\) and leave any comments if required.\n\nThis is crucial: not all the models are alike, and small/big adjustments to the wording in the prompts are always required.\n\nSo back to the codeâ€¦ Because the previous one was only a warm\\-up, now it will start the real while loop, iterating over the entire prompt catalog. See the workflow here belowâ€¦\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*EL0Q97Du6HwtcYQZ.png)\n\nThere are only few changes in the code, and I will point them out, so bear with me.\n\n```python\n############################# AUTOMATIC PROMPTING EVALUATION  11 TURNS #################################\nid =1\nfor items in tasks:\n    fisrtround = 0\n    task = items[\"task\"]\n    prompt = items[\"prompt\"]\n    test = []\n    print(f'NLP TAKS>>> {task}')\n    print(\"\\033[91;1m\")  #red\n    print(prompt)\n    test.append({\"role\": \"user\", \"content\": prompt})\n    print(\"\\033[92;1m\")\n    full_response = \"\"\n    start = datetime.datetime.now()\n    print(\"ðŸ’» > \", end=\"\", flush=True)\n    for chunk in llm.create_chat_completion(\n        messages=test,\n        temperature=0.15,\n        repeat_penalty= 1.31,\n        stop=stops,\n        max_tokens=1500,\n        stream=True,):\n        try:\n            if chunk[\"choices\"][0][\"delta\"][\"content\"]:\n                if fisrtround==0:\n                    print(chunk[\"choices\"][0][\"delta\"][\"content\"], end=\"\", flush=True)\n                    full_response += chunk[\"choices\"][0][\"delta\"][\"content\"]\n                    ttftoken = datetime.datetime.now() - start  \n                    fisrtround = 1\n                else:\n                    print(chunk[\"choices\"][0][\"delta\"][\"content\"], end=\"\", flush=True)\n                    full_response += chunk[\"choices\"][0][\"delta\"][\"content\"]                            \n        except:\n            pass      \n    delta = datetime.datetime.now() - start\n    print('')\n    print(\"\\033[91;1m\")\n    rating = input('Rate from 0 (BAD) to 5 (VERY GOOD) the quality of generation> ')\n    print(\"\\033[92;1m\")\n    stats = createStats(delta,prompt,full_response,rating,logfilename,task,ttftoken)\n    print(stats)\n    writehistory(logfilename,f'''ðŸ‘¨â€ðŸ’» > {prompt}\nðŸ’» > {full_response}\n{stats}\n''')\n    pd_id.append(id)\n    pd_task.append(task)\n    pd_vote.append(rating[:2])\n    pd_remarks.append(rating[2:])\n    id += 1\n## create dataframe and save to csv\nzipped = list(zip(pd_id,pd_task,pd_vote,pd_remarks))\nimport pandas as pdd\ndf = pdd.DataFrame(zipped, columns=['#', 'TASK', 'VOTE','REMARKS'])\n#saving the DataFrame as a CSV file \ndf_csv_data = df.to_csv(csvfile, index = False, encoding='utf-8') \nprint('\\nCSV String:\\n', df_csv_data)  \n```\n\nThe main changes are only in the first lines:\n\n```python\nfor items in tasks:\n    fisrtround = 0\n    task = items[\"task\"]\n    prompt = items[\"prompt\"]\n```\n\nIf you read the article about the `promptLib` you shouldnâ€™t be surprised: but if you are new, here we are iterating over a list of dictionaries with the following structure:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*rGcKJWNzSUrcu4wi.png)\n\nSo for each items in the catalog (means a pair of tasks and prompts) we extract the task description and the prompt for the task.\n\n```python\ntest.append({\"role\": \"user\", \"content\": prompt})\n```\n\nThen we create the chat template message in a temporary list called `test` and pass it to the `create_chat_template()` method for generation.\n\nEverything else is the same.\n\nSave the file, and with the `venv` activated run:\n\n```python\npython main.py\n## if you are using the GPU python main.py -g\n```\n\nThis will get you something like the below exampleâ€¦\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MhhQu4lLjtU__Wjf0dSWBg.gif)\n\nNote that at the end of the entire Prompt Catalog a *csv* file is created with the summary of all the tasks!\n\n## Test Overview\n\nI run them with several Small Language Models, from [Qwen2â€“1\\.5B\\-instruct](https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF), to [Gemma2â€“2B\\-instruct](https://huggingface.co/bartowski/gemma-2-2b-it-GGUF), with [Llama3\\.2â€“1B\\-instruct](https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF) and finally the new [Qwen2\\.5â€“1\\.5B\\-instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF).\n\nWhile I was [quite disappointed by Llama3\\.2â€“1B\\-instruct](https://generativeai.pub/llama3-2-1b-instruct-is-ok-but-not-good-enough-28f88046b63e), I have been amazed by the good job done by the new [Qwen2\\.5â€“1\\.5B\\-instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF).\n\nAt the end of every generation the user is asked to evaluate the results with a mark from 0 to 5\\. **In this case the user is meâ€¦**\n\nThis kind of qualitative analysis is indeed poor, so every mark does have a description, and the user can add comments (â€œsome wrong informationâ€, â€œmaybe better change the wording in the promptâ€ )\n\nHere the qualitative matrix with a description\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*eBdPfZtfr99MsvLh6tt42w.png)\n\n## The good and the bad â€” details\n\nSummarization was amazing. Listing the main topics over a log text was also very good.\n\nThe RAG tasks were quite fast (even on my mini\\-PC) and truthful RAG (ask questions out of the context) was on spot.\n\nSo really good.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*DuV3LJep_PuDqiCcAMb6Cg.png)\n\nThere were also few shortcomings: even with a temperature of only `0.15` I got some made up information in the summarize in two sentences task. And this is not good.\n\n> I expect this can be fixed with a little prompt tuning, or with a temperature at `0`.\n\nAnother fact is that the creative writing was quite bad: for the test I used some fixed Generation Parameters.\n\n```python\n        temperature=0.15,\n        repeat_penalty= 1.31,\n```\n\nProbably for creative writing, with Qwen2\\.5â€“1\\.5B\\-instruct, we should use an higher `repeat_penalty` and also a higher `temperature`.\n\nAs a side note I have to say that also the reflection prompt was not that bad at all! The opening/closure of the tags was not maintained (so not easy to put this into a pipeline or workflow) but the overall flow of the generation and the â€œChain of Thoughtâ€ reasoning process was fairly good.\n\n```python\n<thinking>\nThe importance of using Generative Artificial Intelligence (AI) in \nteaching cannot be overstated, as this technology has immense potential \nnot only within current educational practices but also towards shaping a \nmore innovative approach to lifelong learning.\n</thinking>\n\n**Chain Of Thought:**\n1. **Understanding the Impact**: AI can automate repetitive tasks and provide \npersonalized feedback for students based on their performance data \n(e.g., through chatbots or adaptive assessments).\n2. **Enhancing Learning Experience**: By integrating Generative AI, \nteachers could create more engaging lessons that cater to diverse learning \nstyles.\n3. **Improving Student Outcomes**: Personalized instruction can lead to \nbetter understanding and retention of knowledge for students.\n\n**Reflection:**\n- The integration of generational technology in education is crucial as \nit aligns with the evolving needs of learners who are increasingly digital \nnatives, requiring a pedagogical approach that incorporates these technologies.\n</thinking>\n\n<output>\nTeachers should learn how Generative AI can be used to enhance their teaching \nmethods and improve student outcomes. This includes understanding its \ncapabilities for automating tasks like grading or creating personalized \nlearning paths.\n\n**Learning Path:**\n1. **Foundation Courses in Education Technology**: Start with courses \nthat provide an overview of educational technology, including the basics \nof digital tools.\n2. **AI Fundamentals Course (Coursera)** - Learn about AI concepts and \nhow they can be applied to education through Coursera's free online \ncourse on Artificial Intelligence for Educators.\n\n3. **Specialized Courses in Generative Learning**: Enroll in courses \nthat focus specifically on generational learning, such as \"Generative Design\" \nor similar specialized programs offered by universities like MIT.\n4. **Professional Development Workshops and Conferences** - Attend \nworkshops focused on integrating AI into teaching practices at local \neducational conferences.\n\n5. **Online Communities for Educators (e.g., Edmodo)**: Join online \ncommunities where educators discuss the use of generative technology \nin education, share resources, or ask questions about implementation.\n6. **Certification Programs**: Consider obtaining certifications that \nrecognize your knowledge and skills related to AI integration into teaching \npractices through organizations like Google's \"AI for Educators\" program.\n\nBy following this learning path, teachers can not only enhance their own \nprofessional development but also contribute positively towards the future \nof education by incorporating generative technology effectively.\n</output>\n```\n\nI think that for the size of this Small Language Model, the result is not bad at all!\n\n## Comparing Qwen2\\.5 with Llama3\\.2\n\nEven though this is only coming from my personal evaluation, I want to share it with you.\n\nBoth models are designed to be used on mobile devices, but the performance is quite different. Look here below:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*T6vLgvOKdkotlV1K5x6-QQ.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*DuV3LJep_PuDqiCcAMb6Cg.png)\n\nFirst of all, the overall score difference is huge (41 for Llama3\\.2, 57 for Qwen2\\.5\\).\n\nSecondly if you think about what you may ask on a mobile device, in terms of language tasks, is to have a smooth chatting experience (task 4\\), good summarization (tasks five to 7\\) and some creative writing (task 11 and 13\\).\n\nIn terms of speed, running the model only on CPU, with a very limited mini\\-PC, **I got an average inference speed of 14 t/s.**\n\n## Conclusions\n\nIn the past three months since Qwen2â€™s release, numerous developers have built new models on the Qwen2 language models, providing valuable feedback to the entire community, but also to Alibaba Cloud.\n\n> During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2\\.5\n\nTheir claims come with facts about the new family of models:\n\n* Dense, **easy\\-to\\-use**, decoder\\-only language models, available in 0\\.5B, 1\\.5B, 3B, 7B, 14B, 32B, and 72B sizes, and base and instruct variants.\n* Pretrained on our latest large\\-scale dataset, encompassing up to 18T tokens.\n* Significant improvements in **instruction following**\n* More **resilient to the diversity of system prompts**, enhancing role\\-play implementation and condition\\-setting for chatbots.\n* **Context length support up to 128K** tokens and can generate up to 8K tokens.\n* Multilingual support for over 29 languages\n\nIn my extensive (but certainly limited to one shot prompts and on few NLP tasks) tests I could see with my very own eyes that the claims were based on a good quality training dataset and curated fine tuning.\n\nThis model can perform extremely good on mobile devices!\n\n\n"},{"lang":"en","group":"blog","slug":"blog/qwen2-5-coder-32b-instruct-a-best-coding-model-a-complete-step-by-step-guide-and-performance-b8a33ec2547f","frontmatter":{"title":"Qwen2.5-Coder 32B Instruct: A Best Coding Model-A Complete Step-by-Step Guide and Performanceâ€¦","meta_title":"Qwen2.5-Coder 32B Instruct: A Best Coding Model-A Complete Step-by-Step Guide and Performanceâ€¦","description":"The article presents a comprehensive guide to the **Qwen2.5-Coder-32B-Instruct** model, highlighting its advanced coding capabilities and performance metrics. It emphasizes the model's powerful code generation, repair, and reasoning skills, which rival those of established models like GPT-4o. The guide outlines the model's diverse range, supporting over 40 programming languages and various sizes, catering to different developer needs. It includes practical examples for installation and usage, demonstrating the modelâ€™s effectiveness in real-world applications such as coding assistants and educational tools. The article concludes by affirming the model's role in enhancing AI-powered coding tasks.","date":"2024-11-14T03:29:09.000Z","image":"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*zjZmLCEX5URAc1wxTGnBRQ.png","categories":["Programming","Machine Learning","Generative AI"],"author":"Rifx.Online","tags":["Qwen2.5","Coder","programming","languages","repair"],"draft":false,"slug":"blog/qwen2-5-coder-32b-instruct-a-best-coding-model-a-complete-step-by-step-guide-and-performance-b8a33ec2547f"},"content":"\n# Qwen2.5-Coder 32B Instruct: A Best Coding Model-A Complete Step-by-Step Guide and Performance Evaluation for Developers\n\n\nPhoto By Author\n\n# Introduction\n\nIn the ever-evolving landscape of AI-powered programming tools, large language models (LLMs) have dramatically transformed the way developers write, debug, and optimize code. Today, we are thrilled to explore the **Qwen2.5-Coder** series, an open-source marvel that promises to set new standards in the realm of code generation and AI coding assistants. The latest release in this family, **Qwen2.5-Coder-32B-Instruct**, has redefined the state-of-the-art (SOTA) performance in open-source coding models, rivaling the capabilities of established models like **GPT-4o**. Letâ€™s dive deeper into what makes Qwen2.5-Coder so â€œPowerfulâ€, â€œDiverseâ€, and â€œPracticalâ€.\n\nIn this comprehensive guide, weâ€™ll explore the core capabilities of the **Qwen2.5-Coder-32B** model. Weâ€™ll demonstrate how to use it with the `transformers` library, test its coding abilities and highlight its practical applications.\n\n# Why Qwen2.5-Coder?\n\n## Key Highlights\n\n1. **Powerful**: The flagship **Qwen2.5-Coder-32B** model matches the coding prowess of GPT-4 on major coding benchmarks, while also excelling in general and mathematical skills.\n2. **Diverse**: The release covers multiple model sizes (0.5B, 1.5B, 3B, 7B, 14B, 32B), offering flexibility for different resource constraints.\n3. **Practical**: Designed for real-world applications, including code assistants and artifact generation. The models are licensed under **Apache 2.0**, ensuring freedom to use and modify for both commercial and research purposes.\n\n# The Qwen2.5-Coder Series: A Game-Changer for Open Code LLMs\n\nThe **Qwen2.5-Coder** series is dedicated to pushing the boundaries of open-source code generation. With a focus on flexibility and scalability, this release includes models in a range of sizes: **0.5B, 1.5B, 3B, 7B, 14B,** and the flagship **32B** version. These models cater to various developer needs, from lightweight, resource-efficient models to high-capacity, feature-rich models suitable for demanding applications.\n\n## 1. Powerful: Setting New Benchmarks in Code Generation\n\nQwen2.5-Coder-32B-Instruct stands as the flagship model, boasting a range of capabilities that have earned it the title of the **current SOTA open-source code model**. It excels in:\n\n* **Code Generation**: On popular benchmarks like **EvalPlus, LiveCodeBench,** and **BigCodeBench**, it matches the performance of GPT-4o, offering precise code generation across a multitude of scenarios.\n* **Code Repair**: Fixing broken or inefficient code is crucial in software development. On the **Aider benchmark**, which tests code repair skills, Qwen2.5-Coder-32B-Instruct scored an impressive **73.7**, rivaling GPT-4oâ€™s prowess.\n* **Code Reasoning**: The ability to understand and reason through code execution paths is critical for debugging and optimizing complex software. This modelâ€™s capabilities extend beyond mere generation â€” it excels at **predicting inputs and outputs**, making it an invaluable tool for software engineers.\n\n![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*-g1ZGa0p2kKsQK4iD7q7cg.png)\nSource: (https://qwenlm.github.io/blog/qwen2.5-coder-family/)\n\n## 2. Diverse: Supporting Multiple Programming Languages and Rich Model Sizes\n\nThe versatility of Qwen2.5-Coder is evident in its support for over **40 programming languages**, including niche languages like **Haskell** and **Racket**. This broad support is backed by meticulous data cleaning and balanced training, ensuring the model performs optimally across different coding environments.\n\n* **Multi-language Code Repair**: Its proficiency extends to code repair in unfamiliar languages, which can significantly reduce the learning curve for developers exploring new technologies.\n* **Model Size Flexibility**: The Qwen2.5-Coder series offers models across six sizes, ensuring developers with varying resource constraints can find a model suited to their needs. The **scaling law** philosophy that underpins these models means that performance correlates positively with model size, giving developers the flexibility to choose the right balance between performance and computational resources.\n\n# Performance Insights: Evaluating Qwen2.5-Coder Models\n\n## 1. Instruct vs. Base Models\n\nQwen2.5-Coder is available in both **Base** and **Instruct** versions:\n\n* **Base models** are designed for developers who want a raw model to fine-tune for their specific applications.\n* **Instruct models** come pre-aligned and are optimized for interactive and conversational use cases, making them ideal for chat-based code assistants.\n\n## 2. Benchmark Comparisons: Leading the Pack\n\nAcross various core benchmarks:\n\n* **MBPP-3shot** was chosen to evaluate Base models, providing a robust metric to gauge their code comprehension and synthesis capabilities.\n* The **LiveCodeBench** questions set was used to evaluate Instruct models, focusing on their adaptability to new and unseen coding problems.\n\nThe results? **Qwen2.5-Coder consistently outperforms other open-source models**, proving that scaling up to larger sizes indeed correlates with better performance.\n\n# Hands-On Guide: Using Qwen2.5-Coder-3B for Code Generation with Transformers\n\nIn this hands-on tutorial, we will demonstrate how to use the **Qwen2.5-Coder-3B** model from the `transformers` library to generate code. This model is part of the **Qwen2.5-Coder series**, which is designed to excel in code generation, repair, and reasoning. By the end of this tutorial, you'll see how to integrate this powerful open-source model into your own projects for a range of code-related tasks.\n\n## Prerequisites\n\nBefore diving into the code, make sure you have the following installed:\n\n```\npip install torch transformers\n```\n\nAdditionally, ensure you have access to a GPU-enabled environment if you want to leverage the modelâ€™s performance optimally.\n\n## Step 1: Import Required Libraries\n\nWeâ€™ll start by importing the necessary components from the `transformers` library:\n\n```\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n```\n\nStep 2: Load the Model and Tokenizer\n\nIn this step, we load the ***Qwen2.5-Coder-32B-Instruct*** model and its corresponding tokenizer. The `device_map=\"auto\"` option will automatically allocate the model to your available GPU or CPU.\n\n> ***Qwen2.5-Coder has released models in various sizes â€” 0.5B-Instruct, 1.5B-Instruct, 3B-Instruct, 7B-Instruct, 14B-Instruct, and 32B-Instruct â€” on Hugging Face. If you want to run them locally, choose the model that best fits your GPU capacity. These models are also available on Ollama, so you can use them within the Ollama environment. If youâ€™re interested in an Ollama tutorial, feel free to let me know in the comments!***\n\n```\nmodel_name = \"Qwen/Qwen2.5-Coder-32B-Instruct\"\n\n# Load the model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n```\n\n## Step 3: Writing a Chat Template Function\n\nThe Qwen2.5-Coder model is designed to handle conversation-like prompts using a chat template. The following helper function sets up the input prompt in a way that aligns with the modelâ€™s expectations:\n\n```\ndef generate_response(model, tokenizer, prompt):\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": prompt}\n    ]\n    # Prepare the chat input\n    text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n    # Tokenize and prepare inputs\n    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n    # Generate response\n    generated_ids = model.generate(\n        **model_inputs,\n        max_new_tokens=512\n    )\n    # Remove prompt tokens from output\n    generated_ids = [\n        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n    ]\n  \n    # Decode and return the generated text\n    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n    return response\n```\n\n## Step 4: Testing the Model with Code Generation\n\nLetâ€™s run some examples to see how well the Qwen2.5-Coder-32B model performs in generating Python and Java code. We will test it with three different programming prompts.\n\n## Test 1: Quick Sort Algorithm\n\n**Prompt**: â€œWrite a quick sort algorithm.â€\n\n```\nprompt_1 = \"write a quick sort algorithm.\"\nresponse_1 = generate_response(model, tokenizer, prompt_1)\nprint(response_1)\n```\n\nOutput:\n\nCertainly! Here's a simple implementation of the Quick Sort algorithm in Python:\n\n```python\ndef quick_sort(arr):\n    if len(arr) <= 1:\n        return arr\n    pivot = arr[len(arr) // 2]\n    left = [x for x in arr if x < pivot]\n    middle = [x for x in arr if x == pivot]\n    right = [x for x in arr if x > pivot]\n    return quick_sort(left) + middle + quick_sort(right)\n\n# Example usage:\narr = [3, 6, 8, 10, 1, 2, 1]\nprint(quick_sort(arr))\n```\n\nThis code defines a `quick_sort` function that takes an array as input and returns a sorted version of the array using the Quick Sort algorithm. The function works by selecting a pivot element from the array and partitioning the other elements into two sub-arrays, according to whether they are less than or greater than the pivot. The sub-arrays are then recursively sorted. The base case for the recursion is when the array has one or no elements, in which case it is already sorted.\n\nYou can use this code as a starting point and modify it according to your specific needs. Let me know if you have any further questions or if you need help with anything else!<|fim_middle|>\n\n\n## Test 2: Factorial of a Number\n\n**Prompt**: â€œWrite a Python program to find a factorial of a number.â€\n\n\nprompt_2 = \"Write a python program to find a factorial of a number.\"\nresponse_2 = generate_response(model, tokenizer, prompt_2)\nprint(response_2)\n\n```\nOutput:\n```\n\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nCertainly! Here's a Python program that calculates the factorial of a number:\n\n```python\ndef factorial(n):\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n - 1)\n\n# Example usage\nnumber = int(input(\"Enter a number: \"))\nresult = factorial(number)\nprint(f\"The factorial of {number} is {result}\")\n```\n\nThis program defines a recursive function `factorial` that calculates the factorial of a given number `n`. It uses the base case `n == 0` to return 1, and for other values of `n`, it recursively calls itself with `n - 1` until it reaches the base case. Finally, the program prompts the user to enter a number and displays the factorial of that number.\n\n```\n## Test 3: Formatting Numbers with Commas\n\n**Prompt**: â€œWrite a Python function named â€˜format\\_numberâ€™ that takes a non-negative number as its only parameter. Your function should convert the number to a string and add commas as a thousand separators.â€\n```\n\nprompt_3 = \"\"\"Write a python function named 'format_number' that takes a non-negative number as its only parameter.\nYour function should convert the number to a string and add commas as a thousand separators.\nFor example, calling format_number(1000000) should return '1,000,000'.\"\"\"\nresponse_3 = generate_response(model, tokenizer, prompt_3)\nprint(response_3)\n\n```\nOutput:\n```\n\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nCertainly! Here's a Python function that accomplishes this task:\n\n```python\ndef format_number(num):\n    # Convert the number to a string\n    num_str = str(num)\n  \n    # Initialize an empty string to store the formatted number\n    formatted_num = ''\n  \n    # Initialize a counter for the number of digits\n    digit_count = 0\n  \n    # Iterate over the digits in reverse order\n    for i in range(len(num_str) - 1, -1, -1):\n        # Add the current digit to the formatted number\n        formatted_num = num_str[i] + formatted_num\n      \n        # Increment the digit count\n        digit_count += 1\n      \n        # If the digit count is a multiple of 3, add a comma\n        if digit_count % 3 == 0 and i != 0:\n            formatted_num = ',' + formatted_num\n  \n    return formatted_num\n```\n\nYou can test the function with the following code:\n\n```python\nprint(format_number(1000000))  # Output: '1,000,000'\nprint(format_number(123456789))  # Output: '123,456,789'\nprint(format_number(1234567890))  # Output: '1,234,567,890'\n```\n\nThis function works by iterating over the digits of the number in reverse order, adding each digit to the formatted number and adding a comma every third digit. The function then returns the formatted number as a string.\n\n```\n## Test 4: Java Implement a Stack Using Arrays\n\n**Prompt**: â€œWrite a Java program, Implement a Stack Using Arraysâ€\n```\n\nprompt_4 = \"\"\"Write a Java program, Implement a Stack Using Arrays.\"\"\"\nresponse_4 = generate_response(model, tokenizer, prompt_3)\nprint(response_4)\n\n```\nOutput:\n```\n\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nCertainly! Here's a simple Java program that implements a stack using arrays:\n\n```java\npublic class StackUsingArrays {\n    private int[] stackArray;\n    private int top;\n    private int capacity;\n\n    public StackUsingArrays(int size) {\n        stackArray = new int[size];\n        top = -1;\n        capacity = size;\n    }\n\n    public void push(int item) {\n        if (isFull()) {\n            System.out.println(\"Stack is full. Cannot push item: \" + item);\n        } else {\n            top++;\n            stackArray[top] = item;\n            System.out.println(\"Pushed item: \" + item);\n        }\n    }\n\n    public int pop() {\n        if (isEmpty()) {\n            System.out.println(\"Stack is empty. Cannot pop item.\");\n            return -1;\n        } else {\n            int item = stackArray[top];\n            top--;\n            return item;\n        }\n    }\n\n    public boolean isEmpty() {\n        return top == -1;\n    }\n\n    public boolean isFull() {\n        return top == capacity - 1;\n    }\n\n    public static void main(String[] args) {\n        StackUsingArrays stack = new StackUsingArrays(5);\n\n        stack.push(10);\n        stack.push(20);\n        stack.push(30);\n\n        System.out.println(\"Popped item: \" + stack.pop());\n        System.out.println(\"Popped item: \" + stack.pop());\n\n        stack.push(40);\n        stack.push(50);\n\n        System.out.println(\"Popped item: \" + stack.pop());\n        System.out.println(\"Popped item: \" + stack.pop());\n        System.out.println(\"Popped item: \" + stack.pop());\n    }\n}\n```\n\nThis program defines a `StackUsingArrays` class that uses an array to implement a stack. The `push` method adds an item to the top of the stack, and the `pop` method removes and returns the item at the top of the stack. The `isEmpty` and `isFull` methods check if the stack is empty or full, respectively. The `main` method demonstrates how to use the stack by pushing and popping items.\n\n```\n## Test 5: Simple classification algorithm\n\n**Prompt**: â€œWrite a Python code to run a simple classification algorithm using Sklearnâ€\n```\n\nprompt_5 = \"\"\"Write a Python code to run a simple classification algorithm using Sklearn\"\"\"\nresponse_5 = generate_response(model, tokenizer, prompt_3)\nprint(response_5)\n\n```\nOutput:\n```\n\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nCertainly! Here's a simple example of a classification algorithm using the `sklearn` library in Python:\n\n```python\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n# Generate some sample data\nnp.random.seed(42)\nX = np.random.rand(100, 2)  # 100 samples, 2 features\ny = np.random.choice([0, 1], size=100)  # Binary classification\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create a logistic regression model\nmodel = LogisticRegression()\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Calculate the accuracy of the model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy}\")\n```\n\nThis code generates some sample data, splits it into training and testing sets, trains a logistic regression model, makes predictions on the test set, and calculates the accuracy of the model. Feel free to modify the code to suit your specific needs.\n\n```\nMore details:\n\n[Qwen/Qwen2.5-Coder-32B-Instruct Â· Hugging FaceWe're on a journey to advance and democratize artificial intelligence through open source and open science.huggingface.co](https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct)\n\n[qwen2.5-coder:32bThe latest series of Code-Specific Qwen models, with significant improvements in code generation, code reasoning, andâ€¦ollama.com](https://ollama.com/library/qwen2.5-coder:32b)\n\n# Step 5: Analyzing the Modelâ€™s Performance\n\nFrom our tests, the ***Qwen2.5-Coder-32B-Instruct*** model demonstrates:\n\n* **Strong code generation capabilities**, producing efficient, human-readable solutions for classic coding problems.\n* **Understanding of Python syntax and best practices**, especially when it comes to using Pythonic solutions like list comprehensions and formatted strings.\n* **Flexibility** in adapting to a variety of prompts, which is essential for real-world programming assistant use cases.\n\n# Potential Use Cases\n\nGiven its performance, the Qwen2.5-Coder model can be effectively used in various scenarios, such as:\n\n* **Coding assistants**: Integration into IDEs or text editors to help developers write code faster.\n* **Automated code reviews**: Assisting in identifying bugs, optimizing code, and suggesting improvements.\n* **Educational tools**: Helping students learn to code by generating example solutions and explanations.\n\n# Conclusion\n\nThe **Qwen2.5-Coder** series, particularly the **32B model**, offers a powerful and versatile tool for developers, researchers, and organizations looking to leverage AI for code-related tasks. Its strong performance on benchmarks like EvalPlus, Aider, and McEval proves its competitive edge in code generation, repair, and reasoning.\n\nBy open-sourcing these models, Alibaba Cloud is paving the way for a future where AI-powered coding assistants are accessible to everyone. Whether youâ€™re a developer looking to automate repetitive tasks or a student aiming to learn new programming concepts, Qwen2.5-Coder is a reliable tool to add to your arsenal.\n```\n\n```\n\n```\n\n\n\n"},{"lang":"en","group":"blog","slug":"blog/qwen2-5-coder-cosmos-tokenizer-opencoder-and-new-sentencetransformers-great-times-for-open-ffcacf2b29cd","frontmatter":{"title":"Qwen2.5-Coder, Cosmos Tokenizer, OpenCoder, and New SentenceTransformers: Great Times for Openâ€¦","meta_title":"Qwen2.5-Coder, Cosmos Tokenizer, OpenCoder, and New SentenceTransformers: Great Times for Openâ€¦","description":"The article discusses significant advancements in open-source technology, highlighting four key projects: the Qwen2.5-Coder series, Cosmos Tokenizer, OpenCoder, and SentenceTransformers. Qwen2.5-Coder offers a competitive alternative to GPT-4 in code generation and debugging, while Cosmos Tokenizer enhances image and video compression using neural tokenizers. OpenCoder, trained on 2.5 trillion tokens, provides comprehensive resources for code model development. Lastly, SentenceTransformers achieves a 4x CPU inference speedup via OpenVINO's quantization, optimizing NLP tasks. These developments underscore the ongoing evolution and practical applications of open-source tools in AI and coding.","date":"2024-11-14T03:29:09.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*IZdOavxT_8SRCxrg","categories":["Programming","Technology","Natural Language Processing"],"author":"Rifx.Online","tags":["Qwen2.5-Coder","Cosmos","OpenCoder","SentenceTransformers","OpenVINO"],"draft":false,"slug":"blog/qwen2-5-coder-cosmos-tokenizer-opencoder-and-new-sentencetransformers-great-times-for-open-ffcacf2b29cd"},"content":"\n\n\n\nI want to highlight some standout open\\-source advancements that have really caught my eye:\n\n* **Qwen2\\.5\\-Coder Series**: An open\\-source code LLM thatâ€™s giving GPT\\-4 a run for its money.\n* **Cosmos Tokenizer**: An advanced suite of neural tokenizers for efficient image and video compression.\n* **OpenCoder**: A fully open\\-source code LLM trained on an astonishing 2\\.5 trillion tokens.\n* **Massive CPU Speedup in SentenceTransformers**: A 4x speed boost on CPU inference using OpenVINOâ€™s int8 static quantization.\n\nLetâ€™s dive in!\n\n\n## Qwen2\\.5\\-Coder Series: Open\\-Sourcing a SOTA Code LLM Rivaling GPT\\-4\n\nAlibaba Cloud announced the open\\-source release of the Qwen2\\.5\\-Coder series â€” models that are **Powerful**, **Diverse**, and **Practical** â€” dedicated to propelling the evolution of open code large language models (LLMs).\n\nThe flagship model, **Qwen2\\.5\\-Coder\\-32B\\-Instruct**, sets a new benchmark as the state\\-of\\-the\\-art (SOTA) open\\-source code model, matching the coding capabilities of GPT\\-4\\. It excels in general\\-purpose and mathematical reasoning.\n\n\n\nExpanding upon previous releases of 1\\.5B and 7B models, they introduced four additional model sizes: 0\\.5B, 3B, 14B, and 32B. Qwen2\\.5\\-Coder now accommodates a wide spectrum of developer requirements, covering six mainstream model sizes.\n\nThey have also explored the applicability of Qwen2\\.5\\-Coder in real\\-world scenarios, including code assistants and artifact generation.\n\nPractical examples highlight the modelâ€™s potential in enhancing developer productivity and code quality.\n\n**Benchmark Achievements**\n\n* **Code Generation**: The Qwen2\\.5\\-Coder\\-32B\\-Instruct model achieves top\\-tier performance on popular code generation benchmarks such as EvalPlus, LiveCodeBench, and BigCodeBench.\n* **Code Repair**: Recognizing the importance of debugging in software development, Qwen2\\.5\\-Coder\\-32B\\-Instruct excels in code repair tasks. Scoring 73\\.7 on the Aider benchmark, it performs comparably to GPT\\-4, aiding developers in efficiently fixing code errors.\n* **Code Reasoning**: The model exhibits advanced code reasoning abilities, learning code execution processes and accurately predicting inputs and outputs. Building upon the impressive performance of Qwen2\\.5\\-Coder\\-7B\\-Instruct, the 32B model further elevates reasoning capabilities.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*fzH6YE-yl_GrEXwz)\n\n* **Multi\\-Language Support**: Qwen2\\.5\\-Coder\\-32B\\-Instruct is proficient in over 40 programming languages. It scores 65\\.9 on McEval, showing remarkable performance in languages like Haskell and Racket, thanks to unique data cleaning and balancing strategies during pre\\-training.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*rhyc0T3UZp_2x0r2)\n\nYou can find more info on [github](https://proxy.rifx.online/https://github.com/QwenLM/Qwen2.5-Coder).\n\n\n## Cosmos Tokenizer: Advanced Neural Tokenizers for Efficient Image and Video Compression\n\nThe **Cosmos Tokenizer** is a comprehensive suite of neural tokenizers designed for images and videos.\n\nYou can now convert raw visual data into efficient, compressed representations.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*v8k8jLbZ4LYFRUBc.jpg)\n\nBy discovering latent spaces through unsupervised learning, these tokenizers facilitate large\\-scale model training and reduce computational demands during inference.\n\n**Types of Tokenizers**:\n\n* **Continuous Tokenizers**: Map visual data to continuous embeddings, suitable for models sampling from continuous distributions like Stable Diffusion.\n* **Discrete Tokenizers**: Map visual data to quantized indices, used in models like VideoPoet that rely on cross\\-entropy loss for training.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*a6Hvj8hXJUpOAp9Ber781g.png)\n\n**Key Features**:\n\n* **High Compression with Quality Preservation**: Balances significant compression rates with high\\-quality reconstruction, preserving essential visual details in the latent space.\n* **Lightweight Temporally Causal Architecture**: Utilizes causal temporal convolution and attention layers to maintain the chronological order of video frames, enabling seamless tokenization for both images and videos.\n* **Training on Diverse Data**: Trained on high\\-resolution images and long videos across various aspect ratios and categories, making it agnostic to temporal length during inference.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*lBO1omEzlr18SPB1zF-vMw.png)\n\n**Performance Highlights**:\n\n* **Superior Compression Rates**: Offers remarkable compression capabilities with speeds up to **12x faster** than previous methods.\n* **High\\-Quality Reconstruction**: Delivers significant gains in Peak Signal\\-to\\-Noise Ratio (PSNR), outperforming existing methods by over **\\+4 dB** on the DAVIS video dataset.\n* **Efficient Tokenization**: Capable of encoding up to **8\\-second 1080p** and **10\\-second 720p** videos on NVIDIA A100 GPUs with 80GB memory.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*uYQttZw-MDOCK3oxxLcHbw.png)\n\n**Evaluation and Resources**:\n\n* **TokenBench Dataset** is a new dataset curated for standardizing video tokenizer evaluation, covering categories like robotics, driving, and sports.\n* **Public Availability**: Pretrained models with spatial compressions of 8x and 16x, and temporal compressions of 4x and 8x, are available at [GitHub â€” NVIDIA/Cosmos\\-Tokenizer](https://proxy.rifx.online/https://github.com/NVIDIA/Cosmos-Tokenizer).\n\nMore information on [NVIDIAâ€™s official blog post](https://proxy.rifx.online/https://research.nvidia.com/labs/dir/cosmos-tokenizer/).\n\n\n> *Thank you for taking your time to be here!*\n\n\n> *If you are enjoying the post, please take a moment to [**follow us on Medium**](https://proxy.rifx.online/https://medium.com/@datadrifters/subscribe), clap this article 50 times and leave a comment.*\n\n\n> *We are also running a cohort\\-based training **[for building full\\-stack GenAI SaaS applications](https://proxy.rifx.online/https://forms.gle/8mfFH4wjhF7BbtRY9)**, would be happy to see you inside too!*\n\n\n## OpenCoder: A Fully Open\\-Source Code LLM Trained on 2\\.5T Tokens\n\n**OpenCoder** introduces a new family of open\\-source code language models, including base and chat models at **1\\.5B** and **8B** parameter scales.\n\nSupporting both English and Chinese languages, OpenCoder is trained from scratch on an extensive dataset of **2\\.5 trillion tokens**, comprising 90% raw code and 10% code\\-related web data.\n\nThe model reaches performance levels comparable to leading code LLMs.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*5rd863dHI-W_2ei7.png)\n\n**Key Contributions**:\n\n* The team provides model weights, inference code, training data, data processing pipelines, and detailed training protocols, empowering researchers and practitioners to build upon and innovate.\n* They also introduced **RefineCode dataset**, a high\\-quality, reproducible code pre\\-training corpus containing **960 billion tokens** across **607 programming languages**.\n\nMore information on [official announcement](https://proxy.rifx.online/https://opencoder-llm.github.io/).\n\n\n## SentenceTransformers Accelerates CPU Inference with 4x Speed Boost\n\nThe latest release of **SentenceTransformers** introduces significant performance enhancements, delivering up to a **4x speedup** on CPU inference using **OpenVINOâ€™s int8 static quantization**.\n\nThis update optimizes both training and inference workflows for developers working with large\\-scale natural language processing tasks.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Pd9ESPxjKHaHVgV15pCQig.png)\n\n**Key Enhancements**:\n\n* **OpenVINO int8 Static Quantization**: Leveraging OpenVINOâ€™s quantization techniques, the model achieves superior inference speeds with minimal loss in accuracy. This optimization outperforms existing backends, enhancing deployment efficiency on CPU architectures.\n* **Prompt\\-Based Training**: Supports training with prompts, offering a straightforward method for performance boosts without additional computational overhead.\n* **Convenient Evaluation on NanoBEIR**: Facilitates quicker assessments of model performance using NanoBEIR, a subset of the robust Information Retrieval benchmark BEIR.\n* **PEFT Compatibility**: Now supports **Parameter\\-Efficient Fine\\-Tuning (PEFT)** by allowing easy addition and loading of adapters, enabling more efficient model customization.\n\nYou can find more info on [github](https://proxy.rifx.online/https://github.com/UKPLab/sentence-transformers/releases/tag/v3.3.0).\n\n\n## Bonus Content : Building with AI\n\nAnd donâ€™t forget to have a look at some practitioner resources that we published recently:\n\nThank you for stopping by, and being an integral part of our community.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/rag-llm-and-pdf-conversion-to-markdown-text-with-pymupdf-03af00259b5d","frontmatter":{"title":"RAG/LLM and PDF: Conversion to Markdown Text with PyMuPDF","meta_title":"RAG/LLM and PDF: Conversion to Markdown Text with PyMuPDF","description":"Data feeding in markdown text format increases generated text quality","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*swPjVuudAhsoRiiw3Ee32w.png","categories":["Programming","Technology","Technology/Web"],"author":"Rifx.Online","tags":["markdown","PyMuPDF","LLM","RAG","PDF"],"draft":false,"slug":"blog/rag-llm-and-pdf-conversion-to-markdown-text-with-pymupdf-03af00259b5d"},"content":"\n\n\n\n\n### Data feeding in markdown text format increases generated text quality\n\n\n\n\n## Introduction\n\nIn the context of **Large Language Models (LLMs)** and **Retrieval-Augmented Generation (RAG)** environments, data feeding in **markdown text format** holds **significant importance**. Here are some detailed considerations.\n\n**LLMs** are powerful language models that can generate coherent and contextually relevant text. However, they may sometimes produce responses that lack factual accuracy or context. By incorporating retrieval-based methods (like RAG), we can enhance the quality of generated text.\n\n**RAG** enables the integration of **external data** â€” previously absent in the LLMâ€™s training data â€” into the text generation process. This inclusion mitigates â€œhallucination issuesâ€™â€™ and enhances the relevance of text responses.\n\n\n## Why Markdown for LLM?\n\n**Markdown** is a lightweight markup language that allows users to format plain text using simple syntax. It is widely used for creating structured documents, especially on platforms like GitHub, Jupyter notebooks, and various content management systems. When feeding data into an LLM or RAG system, using markdown format provides several benefits:\n\n1. **Structured Content**: Markdown allows you to organize information into headings, lists, tables, and other structured elements. This structure aids in better understanding and context preservation.\n2. **Rich Text**: Markdown supports basic formatting such as bold, italics, links, and code blocks. Including rich text in the input data enhances the context for the language model.\n3. **Embedding Links and References**: Markdown lets you embed hyperlinks, footnotes, and references. In RAG scenarios, this can be crucial for referring to external sources or providing additional context.\n4. **Ease of Authoring**: Markdown is human-readable and easy to write. Authors can create content efficiently without complex formatting tools.\n5. **Chunking**: Essential for RAG systems, chunking (otherwise known as â€œsplittingâ€) breaks down extensive documents for easier processing. With PyMuPDF data extraction available in MD format we support chunking to keep text with common context together. **Importantly, PyMuPDF extraction in MD format allows for [Level 3 chunking](https://readmedium.com/five-levels-of-chunking-strategies-in-rag-notes-from-gregs-video-7b735895694d#b123)**.\n\nIn summary, using markdown text format in LLM and RAG environments ensures more accurate and relevant results because it supplies richer data structures and more relevant data chunk loads to your LLM.\n\n\n## PyMuPDF Support for Markdown Conversion of a PDF\n\nSince its inception, PyMuPDF has been able to extract text, images, vector graphics and, since August 2023, tables from PDF pages. Each of these object types has its own extraction method: there is one for text, and yet others for tables, images and vector graphics. To meet the requirements of RAG, we merged these disparate extractions to produce one common, unified **Markdown** string which consistently represents the pageâ€™s content as a whole.\n\nAll this is implemented as [one Python script](https://github.com/pymupdf/RAG/blob/main/helpers/pymupdf_rag.py). It can be imported as a module by some other script, or be invoked as a line command in a terminal window like this:\n\n`$ python pymupdf_rag.py input.pdf [-pages PAGES]`\n\nIt will produce a text file (called `input.md`) in **Markdown** format. The optional parameter `PAGES` allows restricting the conversion to a subset of the PDFâ€™s total pages. If omitted, the full PDF is processed.\n\n\n## Markdown Creation Details\n\n\n### Selecting Pages to Consider\n\nThe â€œ`-pages`â€ parameter is a string consisting of desired page numbers (1-based) to consider for markdown conversion. Multiple page number specifications can be given, separated by commas. Each specification either is one integer or two integers separated by a â€œ`-`â€ hyphen, specifying a range of pages. Here is an example:\n\nâ€œ`-pages 1â€“10,15,20-N`â€\n\nThis would include pages 1 through 10, 15 and pages 20 through the end of the file (capital â€œNâ€ is treated as the number of the last page).\n\n\n### Identifying Headers\n\nUpon invocation, the program examines all text on the given pages and finds the most frequently used font size. This value (and all smaller font sizes) is assumed to represent **body text**. Larger font sizes are assumed to represent **header text**.\n\nDepending on their relative position in the font size hierarchy, header text will be prefixed with one or more markdown header `#`-tag characters.\n\n\n### Identifying the Processing Mode per Page Area\n\nAll text on each page will first be classified as being either **standard** text or **table** text. Then the page content will be extracted from top to bottom converting everything to markdown format.\n\nThis is best explained by an example:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*u5fv2aAIvDaaAd6H.png)\n\nThis page shows content, that represents typical situations:\n\n* Two tables, having partly overlapping vertical positions. One table has no headers, the other one has **external** column headers.\n* There is a **title** line and **headers** at multiple levels.\n* The **body text** contains a variety of styling details like **bold**, *italic* and `inline code`.\n* Ordered and unordered lists.\n* Code snippet.\n\nLayout analysis will determine three areas and select the appropriate processing modes: **(1)** text, **(2)** table, **(3)** text.\n\nThe generated Markdown text reflects the above faithfully â€” as much as at all possible in this format.\n\nFor an example, let us look at the output for the table with external headers:\n\n\n```python\n|Column1|Column2|\n\n|---|---|\n\n|Cell (0, 0)|Cell (0, 1)|\n\n|Cell (1, 0)|Cell (1, 1)|\n\n|Cell (2, 0)|Cell (2, 1)|\n```\nThis is GitHub-compatible format with the minimum possible token size â€” an important aspect for keeping feeds into RAG systems small.\n\n**Column borders** are indicated by the â€œ`|`â€ character. A text line is assumed to be a **table header** if it is followed by a line of the form â€œ`|---|---| â€¦`â€. The full **table definition** must be preceded and followed by at least one empty line.\n\nPlease note that for technical reasons markdown tables must have a header and thus will choose the first table row if no external header is available.\n\nTo confirm overall fidelity, here is how a Markdown parser processes the full page:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Ge83uj7FiM4T6XFn)\n\n\n## Invoking the Markdown Converter Programmatically\n\nInstead of executing a program in the command line, Markdown conversion can also be requested by a program:\n\n\n```python\nimport fitz\nfrom pymupdf_rag import to_markdown  # import Markdown converter\n\ndoc = fitz.open(â€œinput.pdfâ€)  # open input PDF\n\n## define desired pages: this corresponds â€œ-pages 1-10,15,20-Nâ€\npage_list = list(range(9)) + [14] + list(range(19, len(doc) â€“ 1))\n\n## get markdown string for all pages\nmd_text = to_markdown(doc, pages=page_list)\n\n## write markdown string to some file\noutput = open(â€œout-markdown.mdâ€, â€œwâ€)\noutput.write(md_text)\noutput.close()\n```\n\n## Conclusion\n\nBy integrating PyMuPDFâ€™s extraction methods, the content of PDF pages will be faithfully converted to markdown text that can be used as input for RAG chatbots.\n\nRemember, the key to a successful RAG chatbot lies in the quality and completeness of information it can access.\n\nPyMuPDF-enabled markdown extraction ensures that this information from PDFs is not only possible but straightforward, showcasing the libraryâ€™s strength and developer-friendliness. Happy coding!\n\n\n### Source Code\n\n* [RAG/helpers/pymupdf\\_rag.py (github.com)](https://github.com/pymupdf/RAG/blob/main/helpers/pymupdf_rag.py)\n\n\n### References\n\n* [5 Levels of Text Splitting](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb)\n\n\n### Related Blogs\n\n* [Building a RAG Chatbot GUI with the ChatGPT API and PyMuPDF](https://readmedium.com/building-a-rag-chatbot-gui-with-the-chatgpt-api-and-pymupdf-9ea8c7fc4ab5)\n* [Creating a RAG Chatbot with ChatGPT and PyMUPDF](https://readmedium.com/creating-a-rag-chatbot-with-chatgpt-and-pymupdf-f6c30907ae27)\n* [RAG/LLM and PDF: Enhanced Text Extraction](https://readmedium.com/rag-llm-and-pdf-enhanced-text-extraction-5c5194c3885c)\n\n"},{"lang":"en","group":"blog","slug":"blog/ragate-adaptive-rag-for-conversational-ai-94b5ca469b7d","frontmatter":{"title":"RAGate: Adaptive RAG for Conversational AI","meta_title":"RAGate: Adaptive RAG for Conversational AI","description":"RAGate is an adaptive mechanism for conversational AI that optimally balances the use of internal and external knowledge, enhancing response quality and engagement. By evaluating when to retrieve external information versus relying on built-in knowledge, RAGate addresses the limitations of traditional RAG systems, such as over-reliance on external sources and increased latency. The paper outlines the implementation of RAGate, including its variants and evaluation methods, emphasizing its potential to improve user interactions across various industries by delivering more relevant and personalized responses.","date":"2024-11-14T03:29:09.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8wzI-5BRV1-br0e3MBVD2g.png","categories":["Chatbots","Natural Language Processing","Machine Learning"],"author":"Rifx.Online","tags":["RAGate","conversational","retrieval","latency","personalization"],"draft":false,"slug":"blog/ragate-adaptive-rag-for-conversational-ai-94b5ca469b7d"},"content":"\n\n\n\nBuilding Conversational AI systems is hard!!!\n\nItâ€™s feasible but also **complex, time\\-consuming, and resource\\-intensive**.\n\nThe challenge lies in designing systems that can understand and generate human\\-like responses and ensuring that these systems engage users effectively, adapting to the nuances of conversation.\n\nThe very popular **RAG (Retrieval\\-Augmented Generation)** has revolutionized conversational AI by seamlessly integrating external knowledge with LLMâ€™s internal knowledge. By using RAG with your business data, your customers can ask questions about their data in natural language, facilitating a seamless interaction.\n\n**However, there is a caveat:** While using RAG, it becomes clear that not every query needs an answer sourced from â€œexternal knowledge.â€ Over\\-reliance on external sources can disrupt genuine engagement. Itâ€™s like having a conversation with someone and, for every question, reaching for a book to craft your response, even though you already have a deeper understanding of the topic. Even worse, you canâ€™t find any book on the topic and end up responding with â€œI donâ€™t know,â€ despite having internal knowledge that could provide a more insightful answer.\n\nClearly, while using RAG, a mechanism is needed to determine when to utilize â€œexternal knowledgeâ€ versus â€œinternal knowledgeâ€ at the inference time.\n\nEnter **RAGate** â€” a binary switch designed to dynamically evaluate when to utilize external knowledge and when to rely on internal insights. Introduced by Xi Wang, Procheta Sen, Ruizhe Li, and Emine Yilmaz, and published in July 2024, [**ArXiv**](https://proxy.rifx.online/https://arxiv.org/abs/2407.21712) **(Adaptive Retrieval\\-Augmented Generation for Conversational Systems).**\n\nLetâ€™s learn more with examples.\n\n\n## What is Conversational AI, really?\n\n**Conversation** is the exchange of thoughts, emotions, and information between individuals, adapting to tone, context, and subtle cues that guide the interaction. Humans are naturally suited for conversation due to qualities like emotional intelligence, socialization, and cultural exposure, which help us understand nuances and adapt to different social contexts.\n\n**Conversational AI** aims to replicate this human\\-like interaction by using technology to understand and generate natural, contextually appropriate, and engaging responses. It adapts to user inputs, making the interaction fluid and dynamic, like a conversation between humans.\n\n\n## What is External Knowledge and Internal Knowledge of AI systems?\n\nIn the opening paragraph, I mentioned two key terms â€” External Knowledge and Internal Knowledge. Letâ€™s take a moment to clarify these concepts, as understanding them will make learning about RAGate much easier.\n\n**External knowledge** encompasses information not inherent to the AI model but retrieved from outside sources. The sources include databases like structured data repositories, APIs, unstructured knowledgebases like guides, FAQs, and web sources. The primary role of external knowledge is to provide factual, up\\-to\\-date, and contextually relevant information that enhances the accuracy and comprehensiveness of the AIâ€™s responses.\n\n**Internal knowledge** refers to the built\\-in\\-knowledge and processing capabilities embedded within the AI model based on its training data. The sources include pre\\-trained knowledge from diverse datasets, including language patterns, grammar, shared facts, and general world knowledge, contextual awareness from memory of past interactions, and AIâ€™s semantic understanding and comprehension abilities.\n\n\n## RAG and Guardrails â€” powerful duo, but with limitations!\n\nRAG combines two powerful elements: (1\\) The natural language processing abilities of large language models (LLMs) to interpret and generate human\\-like text. (2\\)The ability to retrieve and augment external, up\\-to\\-date information.\n\nMany RAG implementations incorporate **guardrails**, constraints, or rules that guide the systemâ€™s behavior towards responsible and domain\\-bound AI. These guardrails often prioritize using external knowledge over the modelâ€™s internal knowledge to ensure predictability of response. The strict application of these guardrails can sometimes lead to suboptimal outcomes:\n\n* **Over\\-reliance on external sources:** The system may be forced to seek external information even for general questions where the LLMâ€™s internal knowledge might suffice.\n* **Potential for less fluid responses:** By restricting internal knowledge, the system might produce less natural or contextually appropriate responses in some cases.\n* **Increased latency:** Constantly retrieving external information can slow response times compared to relying on internal knowledge.\n* **Missed opportunities:** The vast knowledge embedded in the LLMâ€™s parameters might be underutilized, potentially missing valuable insights or connections.\n\n\n## Balancing Act with RAGate\n\nRAGate, short for **Retrieval\\-Augmented Generation Gate**, enhances conversational AI systems by adaptively determining when to incorporate external knowledge into responses.\n\n[RAGate study](https://proxy.rifx.online/https://arxiv.org/abs/2407.21712) investigates the need for **adaptive augmentation** in conversational systems and presents RAGate as a **gating model** that predicts when external knowledge retrieval is beneficial. The paper provides extensive experiments and analyses, demonstrating RAGateâ€™s effectiveness in improving response quality and generation confidence in RAG\\-based conversational systems.\n\n\n\n\n## RAGate Example\n\n**Scenario:** A user is interacting with a healthcare\\-focused chatbot that offers personalized health advice based on general wellness principles and medical knowledge.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*o0mWnGGefJ0TyDv1u14njw.png)\n\nRAGate can further enhance conversation by balancing internal and external knowledge. It allows AI to use internal medical knowledge for general info while retrieving up\\-to\\-date research. It can even intelligently synthesizes data from multiple sources for a comprehensive analysis, offers personalized insights based on patient details, and filters external information to prioritize the most relevant content, reducing overload.\n\n\n## Variants of RAGate\n\nAs published in paper, RAGate offers 3 variants â€” **RAGate\\-Prompt**, **RAGate\\-PEFT (Parameter\\-Efficient Fine\\-Tuning)**, and **RAGate\\-MHA (Multi\\-Head Attention).**\n\nEach variant of RAGate â€” Prompt, PEFT, and MHA â€” employs distinct methods to integrate external knowledge, towards the common goal of improving the relevance and accuracy of AI\\-generated responses.\n\nHere is a quick comparison table:\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*3dZg6rHlqmddK1ZQqqu_Aw.png)\n\n\n## How to implement RAGate?\n\nThe paper illustrates a step\\-by\\-step guide to implement RAGate:\n\n1. **Define the problem**: This step is crucial as it is about identifying the conversational task you want to enhance with RAGate. Determine the scope of the conversation and the specific domains you want to cover (e.g., restaurant recommendations, travel planning).\n2. **Select a language model**: Choose an appropriate Large Language Model (LLM) as the backbone for your conversational system. Options include models like Llama, GPT\\-2, or other transformer\\-based architectures.\n3. **Gather and annotate data**: Collect a dataset relevant to your conversational domain. The KETOD dataset, which includes annotated dialogues and knowledge snippets, is an excellent example. Ensure that your dataset has clear labels indicating when knowledge augmentation is necessary.\n4. **Develop the Knowledge Retrieval System**: Implement a knowledge retrieval mechanism to fetch relevant external information when needed. It can consider the popular techniques like dense\\-passage retrieval or graph\\-structured knowledge bases.\n5. **Implement the RAGate mechanism**: Create the binary knowledge gate function (RAGate) to determine when to augment responses with external knowledge. It involves **Contextual Analysis and Gating Function**\n6. **Explore RAGate variants**: Develop different variants of RAGate based on the approaches discussed in the paper:\n* **RAGate\\-Prompt**: Use natural language prompts with a pre\\-trained language model to determine the need for augmentation.\n* **RAGate\\-PEFT**: Employ parameter\\-efficient fine\\-tuning techniques (e.g., QLoRA) to train your language model for better decision\\-making.\n* **RAGate\\-MHA**: Utilize a multi\\-head attention mechanism to assess the context and retrieve knowledge interactively.\n\n7\\. **Train the Model**: Fine\\-tune your LLM using the annotated dataset, employing the various RAGate variants. Incorporate the training of the gating mechanism to enhance the modelâ€™s ability to predict the need for knowledge augmentation effectively.\n\n8\\. **Evaluate performance**: Conduct extensive experiments to validate the effectiveness of RAGate. Analyze metrics such as:\n\n* **Precision, Recall, F1 Score**: To evaluate the classification performance of the gating function.\n* **BLEU, ROUGE, BERTScore**: This is used to assess the quality of generated responses compared to ground truth.\n* **Confidence Scores**: Measure the confidence of generated outputs to ensure high\\-quality responses.\n\n9\\. **Deploy the system**: Integrate the RAGate\\-enabled conversational system into your application or service. Ensure the system can handle real\\-time queries and dynamically decide on knowledge augmentation.\n\n10\\. **Iterate and improve**: Continuously gather user feedback and interaction data to refine the model. Analyze areas where the system may struggle with context or relevance and adjust the training or retrieval mechanisms accordingly.\n\n\n## Takeaways\n\nIn conclusion, RAGate represents a significant advancement in conversational AI by intelligently balancing internal and external knowledge to provide more relevant, efficient, and personalized responses. The applications of RAGate are vast, spanning across industries such as healthcare, customer support, education, legal services, finance, and more. By enhancing AIâ€™s capacity to deliver tailored, real\\-time information, RAGate has the potential to revolutionize how businesses and individuals interact with technology, improving decision\\-making, user experience, and overall system performance.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/rbyf-qwen2-5-3b-instruct-is-damn-good-dcf443cacc63","frontmatter":{"title":"RBYF: Qwen2.5â€“3B-instruct is damn good.","meta_title":"RBYF: Qwen2.5â€“3B-instruct is damn good.","description":"Revised Benchmark with You as a Feedback: the brand new 3B model from Alibaba Qwen is an amazing model, and I can prove it!","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*NWaBtJ64TLUoUHv4F1qJpg.png","categories":["Programming","Technology","Science"],"author":"Rifx.Online","tags":["Qwen","NLP","multimodal","RBYF","evaluation"],"draft":false,"slug":"blog/rbyf-qwen2-5-3b-instruct-is-damn-good-dcf443cacc63"},"content":"\n### Revised Benchmark with You as a Feedback: the brand new 3B model from Alibaba Qwen is an amazing model, and I can prove it!\n\n\n\nThe illusion of emergent properties is largely a product of the metrics used to evaluate these models. And this is a fact.\n\nFew weeks ago I decided to be a little rebel, discard all the official Benchmarks, and start being a Benchmark myself!\n\nThis is the meaning behind this totally made up Acronym RBYF: Revised Benchmark with You as a Feedback. And the underlined principle is that there is no better judge than you, to verify how good a Large Language Model can be.\n\nTo be honest, I am focusing on Small Language Models. I donâ€™t own a dedicated GPU and my computational resources are limited. But again, I agree to the [LLMWare rebel principle number one](https://readmedium.com/getting-work-done-with-genai-just-do-the-opposite-10-contrarian-rules-that-may-actually-work-634501602a27):\n\nUse Small Models, Not Large Ones.\n\nIn this article I am going to show you the results of my evaluation on qwen2.5â€“3b-instruct. And it is really good!\n\n> Disclaimer: all the prompt used with the results are available in my GitHub repository:\n\n## Less is More\n\nScaling laws describe how model performance improves as the number of parameters and training data increases. This principle has fueled the search for novel abilities in LLMs.\n\n> Simply increasing the size of a model, we can unlock new capabilitiesâ€¦\n\nScaling laws describe the relationship between model performance and the number of parameters and training data. As models grow larger and are trained on more data, we expect their performance to improve. This has led to a relentless pursuit of larger and larger LLMs, in the hope of unlocking new capabilities.\n\nEmergent properties are those that arise from the interactions of individual components within a complex system. They cannot be predicted or understood by studying the components in isolation. In the case of LLMs, the hope is that as these models grow larger and more complex, they will exhibit unexpected and new capabilities.\n\nThis is a fairy-tale.\n\nIn the past weeks we saw with our own eyes, that over-trained and well curated Small Language Models can perform as good as their big brothers. And this is a punch to the so called emergent abilities, striking back to the scaling law. Gemma2â€“2B, Qwen2.5â€“3B and even the latest Llama3.2â€“3B are far better models than the old SOTA 7B models.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*-EjpdEky-Hf3WEQn.png)\n\n## The Qwen2.5 family of Models\n\nAlibaba Cloud released on middle of September their flagship model family Qwen2.5.\n\n> Alibaba Cloudâ€™s revolutionary journey with Qwen is showing once again strong Leadership through Innovation\n\nQwen2.5 is the large language model and large multimodal model series of the Qwen Team, Alibaba Group. Both language models and multimodal models are pretrained on large-scale multilingual and multimodal data and post-trained on quality data for aligning to human preferences. Qwen is capable of natural language understanding, text generation, vision understanding, audio understanding, tool use, role play, playing as AI agent, etc.\n\n**What stands out in the new Qwen2.5 is the thoroughly curated training dataset.** You can clearly understand this, checking the small models performance.\n\nIf the Small Language Models of the family are good, means that the training and the dataset were highly revised and curated.\n\nHere some numbers:\n\n* Dense, easy-to-use, decoder-only language models, available in 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B sizes, and base and instruct variants.\n* Pretrained on our latest large-scale dataset, encompassing up to 18T tokens.\n* Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON.\n* More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n* Context length support up to 128K tokens and can generate up to 8K tokens.\n* Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*3tnTS_UCImBRBDeKmFyjVQ.png)\n\n## Qwen2.5â€“3B-instruct\n\nEven though for the AI advancement we need milestones and better performances, still the user experience and the personal point of view cannot be just put aside as irrelevant.\n\nI believe that exploring some frequently used NLP tasks, and putting aside the chat experience, we must focus on the quality of the replies. And we are the only benchmark required. **Our user experience is the best indicator to understand if a model is good or not**. The model must be reliable enough to be used in an automated workflow.\n\nBy the way, I already run what I decided to call [RBYF â€” Revised Benchmarks with You as a Feedback](https://open.substack.com/pub/thepoorgpuguy/p/rbyf-is-here-revised-benchmarks-with?r=i78xo&utm_campaign=post&utm_medium=web) on [Qwen2.5â€“1.5b-instruct](https://ai.gopubby.com/qwen2-5-1-5b-the-future-of-mobile-ai-6bd5f29bbc84): you can read the details. In the article I also explained how to create your test bench. The method described is the same I used for Qwen2.5â€“3B.\n\nLetâ€™s begin with an overall performance across all the tasks. The model has been evaluated by me (in this case is my Own Feedback) based on the qualitative matrix as displayed here below.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*rdVHfCDWX9jlvtiq)\n\nThe overall score is 62/70 = 8.8\n\nOk, but based in what Qwen2.5â€“3B-instruct got this evaluation score?\n\n## Test Overview\n\nThe idea behind this is a fair user feedback, not an automated one across standard benchmarks and frameworks. Is a Small Language Model able to satisfy the user intent over the mostly used NLP task?\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*wgngfGvSebeoH3YxTdvc8A.png)\n\nWe want to validate both the user intent and the quality of the responses. Here the breakdown of every task:\n\n### Introduction\n\nTo verify how the model reply to initial greetings and talks about itself.\n\n### Explain in one sentence\n\nSynthesis and summarization. The final evaluation focuses in whether or not the model is able to fit the reply in only one sentence\n\n### Explain in three paragraphs\n\nUser intent is to get a smart explanation of the text that must fit into three paragraphs. SML usually find it very hard because the always put a recap (forth) paragraph.\n\n### Say â€œI am readyâ€.\n\nIn a chat turn base application, instruction-following models are usually asked to first read a provided text and later on to complete some sort of analysis. Usually SML cannot do thisâ€¦\n\n### Summarize\n\nBasic summarization, with no limits. Here we want to evaluate how the summary is grounded on the text, without made up facts.\n\n### Summarize in two sentences\n\nBasic summarization, with a 2 sentences limits. Here we want to evaluate how the summary is grounded on the text, without made up facts: but as well we want to ensure the 2 sentences constraint.\n\n### Write in a list the three main key points â€” format output\n\nFocus: the SML must format the output in a specific format. In this prompt we ask to create a list of the 3 key points and give the output as a python list.\n\n### Table of Contents\n\nThis task is quite hard for many SML. The prompt requires some adjustments otherwise the model return a markdown table. The user want an ordered list of the topics following the provided document structure.\n\n### RAG\n\nRetrieval Augmented Generation, without any framework (haystack, Langchainâ€¦). This is one of the most used tasks for a Language model. The reply is evaluated on the ability to understand the instructions and how grounded to the text is the answer.\n\n### Truthful RAG\n\nIt is A RAG with a question completely out of the provided context. The model must reply unanswerable meaning it understood the instruction and it is not using any external knowledge or made up information.\n\n### Write content from a reference\n\nThis is a creative task. Using a reference text the SML must provide a new essay.\n\n### Extract 5 topics\n\nThe focus on this task is to verify that:\n\n* there are exactly 5 topics\n* they are grounded (no hallucination)\n\n### Creativity: 1000 words SF story\n\nCompletely creative task. It is very hard even for larger models to be coherent and produce a small story hitting the correct word count.\n\n### Reflection prompt\n\nThe reflection prompt is meant to verify the CoT reasoning process of the model. The output is constrained into opening/closure of special tags. The focus is both on the reasoning and on a consistent output structure. The output must be easily used for further structured prompts or visualizations. You can read more in this article:\n\n## The evaluation process\n\nAt the end of every generation the user is asked to evaluate the results with a mark from 0 to 5. **In this case the user is meâ€¦**\n\nThis kind of qualitative analysis is indeed poor, so every mark does have a description, and the user can add comments (â€œsome wrong informationâ€, â€œmaybe better change the wording in the promptâ€ )\n\nHere the qualitative matrix with a description\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*H_Qx2UT1lxqy2BF4.png)\n\n## The good and the bad â€” details\n\nOverall Qwen2.5â€“3B-instruct is an All Rounded model. The average score for each task is 4.4 out of 5, that is quite impressive.\n\nSummarization was amazing. Listing the main topics over a log text was also very good.\n\nIt shines in RAG, sometimes being too verbose: in the truthful RAG it also gave some explanation, and the reasons for the *unanswerable* reply.\n\nWe can have a look together. It is quite often a hard feat for SML to precisely count the number of sentences and paragraphs.\n\n```python\n**Write a two-sentences summary of the following text:**\n\nChina's regime under Chinese Communist Party operates without transparent \nelections and primary candidates. While undesirable for democracies, \nit allows long-term planning free from electoral pressures that can change \nplans abruptly; this has led to China achieving peak carbon dioxide emissions \nin 2023 due largely to increased renewable energy installations, potentially \nsetting a new global standard for sustainable development despite its status \nas one of the world's largest polluters.\n```python\n**Write a two-sentences summary of the following text:**\n\nChina's regime under Chinese Communist Party operates without transparent \nelections and primary candidates. While undesirable for democracies, \nit allows long-term planning free from electoral pressures that can change \nplans abruptly; this has led to China achieving peak carbon dioxide emissions \nin 2023 due largely to increased renewable energy installations, potentially \nsetting a new global standard for sustainable development despite its status \nas one of the world's largest polluters.\n\n```\n\nTable of Contents is another nightmare for the Language models, because they usually understand to create a tableâ€¦ But look at this:\n\n```python\nA \"table of content\" provides an organized structure to the text, allowing \nreaders quick access to key points. Write the \"table of content\" of \nthe following text.\n\n[table of contents]\n1. Introduction: Overview and Critique on China's Governance System  \n2. Comparison with Western Democracies - Undesirable Dictatorship? \n3. Advantages in Long-Term Planning for Sustainability   \n4. The Chinese Model as a Comparative Advantage  \n5. Recent Achievements in Carbon Emissions Reduction\n6. Transition to Renewable Energy: A Turning Point?\n7. Potential Leadership Role of China and India  \n8. Conclusion - Future Directions\n\n```python\nA \"table of content\" provides an organized structure to the text, allowing \nreaders quick access to key points. Write the \"table of content\" of \nthe following text.\n\n[table of contents]\n1. Introduction: Overview and Critique on China's Governance System  \n2. Comparison with Western Democracies - Undesirable Dictatorship? \n3. Advantages in Long-Term Planning for Sustainability   \n4. The Chinese Model as a Comparative Advantage  \n5. Recent Achievements in Carbon Emissions Reduction\n6. Transition to Renewable Energy: A Turning Point?\n7. Potential Leadership Role of China and India  \n8. Conclusion - Future Directions\n\n\n```\n\nI assign a score of 4 out of 5 to the truthful RAG, but it could have been easily a 5. The most important part is that the response must conatin the word *unanswerable*.\n\nWhy? Imagine you are setting up a chain of prompts, and you need to decide the next step based on the previous reply. If you can simply look for the presence of the work *unanswerable* to steer your next step, you are ok!\n\n```python\n**Reply to the question only using the provided context. If the answer \nis not contained in the provided context say \"unanswerable\".\nquestion: who is Anne Frank?**\n{context}\n**Remember: if you cannot answer based on the provided context, say \n\"unanswerable\"\nanswer:**\n\nUnanswerable\nThe given text does not provide any information about Anne Frank \nor her identity. The content is unrelated to this question and focuses \ninstead on discussing China's system compared to Western democracies in \nterms of long-term planning for environmental issues.\n```python\n**Reply to the question only using the provided context. If the answer \nis not contained in the provided context say \"unanswerable\".\nquestion: who is Anne Frank?**\n{context}\n**Remember: if you cannot answer based on the provided context, say \n\"unanswerable\"\nanswer:**\n\nUnanswerable\nThe given text does not provide any information about Anne Frank \nor her identity. The content is unrelated to this question and focuses \ninstead on discussing China's system compared to Western democracies in \nterms of long-term planning for environmental issues.\n\n```\n\nI imagine that the model was trained to provide also the reasons for this kind of repliesâ€¦ Awesome!\n\n### The excellent Reflection\n\nThe main focus of the Reflection prompt, is the ability of the model of returning a correct reasoning process, and format the output following few specific open/closure of tags.\n\nSo far this is **the best Reflection output** from a Small Language Model, even better than the Gemma2â€“2B:\n\n```python\n*ðŸ‘¨â€ðŸ’» > You are an AI assistant designed to provide detailed, step-by-step \nresponses. Your outputs should follow this structure:\n1. Begin with a <thinking> section.\n2. Inside the thinking section:\n   a. Briefly analyze the question and outline your approach.\n   b. Present a clear plan of steps to solve the problem.\n   c. Use a \"Chain of Thought\" reasoning process if necessary, breaking down \nyour thought process into numbered steps.\n3. Include a <reflection> section for each idea where you:\n   a. Review your reasoning.\n   b. Check for potential errors or oversights.\n   c. Confirm or adjust your conclusion if necessary.\n4. Be sure to close all reflection sections.\n5. Close the thinking section with </thinking>.\n6. Provide your final answer in an <output> section.\nAlways use these tags in your responses. Be thorough in your explanations, \nshowing each step of your reasoning process. Aim to be precise and logical \nin your approach, and don't hesitate to break down complex problems into \nsimpler components. Your tone should be analytical and slightly formal, \nfocusing on clear communication of your thought process.\nRemember: Both <thinking> and <reflection> MUST be tags and must be closed \nat their conclusion\nMake sure all <tags> are on separate lines with no other text. \nDo not include other text on a line containing a tag.\n\nuser question: explain why it is crucial for teachers to learn how to use \ngenerative AI for their job and for the future of education. \nInclude relevant learning path for teachers and educators. *\n\n\n**ðŸ’» > <thinking>\nI will begin by analyzing the importance of teaching about Generative \nArtificial Intelligence (AI) in an educational context, then outline a \nstep-by-step plan on what steps should be taken to learn this technology \neffectively as well as provide resources or courses that could help.\nFirstly, understanding AI is crucial for teachers because it can \nsignificantly enhance their ability to support students. \nTeachers need the knowledge and skills necessary not only to integrate these tools into teaching but also understand how they work so they are able to guide both themselves and their pupils in ethical use of technology.\n</thinking>\n\n<reflection>\nMy initial thought process was clear, emphasizing that teachers should \nbe equipped with AI literacy as it can improve educational outcomes. \nI outlined the importance for guiding students ethically while using \nthese tools effectively.\nHowever, my plan needs more detail on specific steps to learn about \ngenerative AI and resources available online or in-person courses.\n</reflection>**\n*MY PERSONAL NOTE: here it is starting an additional thinking after the first \nreflection... And this is really too good to be true!!*\n**<thinking>\nTo create a comprehensive learning path that teachers could follow:\n1. **Understand Basic Concepts**: Start with basic concepts of machine \n   learning (ML) such as supervised vs unsupervised, deep neural networks  \n   etc., which are foundational to understanding generative AI.\n2. **Learn About Generative Models**:\n   - Study different types: Variational Autoencoders (VAEs), GANs.\n   - Understand how these models generate new data that resembles the \n     training set but is not identical and can be used for tasks like \n     text generation, image synthesis etc., which are crucial in education.\n3. **Explore Applications**: Look at real-world applications of generative \n   AI such as language modeling (e.g., GPT-2), computer vision techniques to \n   create realistic images or videos from raw data.\n4. **Ethical Considerations and Implications for Education**:\n   - Discuss the ethical implications, including bias in models' outputs \n     which could affect studentsâ€™ perceptions about themselves if they are \n     exposed regularly through AI-generated content that is not representative \n     of them.\n5. **Hands-on Practice**: Engage with practical exercises using open-source \n   tools like TensorFlow or PyTorch to build simple generative model \n   applications.\n6. **Stay Updated and Network**:\n   - Follow the latest research in ML, especially on Generative Models \n     by following relevant journals (e.g., NeurIPS), conferences such as \n     ICML etc.\n7. Attend Workshops/Conferences: Participate actively at AI-related workshops \n   or events to network with professionals working within this field.\n</thinking>**\n*MY PERSONAL NOTE: and no surprise, an additional reflection now starts, \nbefore producing the final output*\n**<reflection>\nMy plan has been expanded and includes a more detailed approach for teachers, \ncovering foundational knowledge through practical application of generative \nmodels in education along with ethical considerations that are crucial. \nI have also included staying updated on the latest research as an important \npart.\nHowever, it's essential to ensure these resources or courses exist so \neducators can access them easily.\n</reflection>\n\n<output>\nIt is vital for teachers and educational professionals alike to learn how \ngenerative AI works because this technology has significant potential in \nenhancing teaching methods. By understanding basic concepts of machine \nlearning (ML) such as supervised vs unsupervised, deep neural networks etc., \nthey will be better equipped with the knowledge necessary not only to \nintegrate these tools into their curriculum but also guide students ethically.\nA comprehensive plan for teachers and educators includes:\n1. **Understand Basic Concepts**: Start by grasping foundational concepts of ML.\n2. **Learn About Generative Models**:\n   - Study different types: Variational Autoencoders (VAEs), GANs, etc., \n     which are crucial in generating new data that resembles the training \n     set but is not identical and can be used for tasks like text generation or \n     image synthesis.\n3. **Explore Applications**: Look at real-world applications of generative AI \n   such as language modeling using tools like TensorFlow.\n4. **Ethical Considerations**:\n   - Discuss ethical implications, including bias in models' outputs which \n     could affect studentsâ€™ perceptions about themselves if they are exposed \n     regularly through AI-generated content that is not representative.\n5. **Hands-on Practice**: Engage with practical exercises to build simple \n   generative model applications using open-source tools.\n6. **Stay Updated and Network**:\n   - Follow the latest research in ML, especially on Generative Models by \n     following relevant journals (e.g., NeurIPS), conferences such as ICML etc.\n7. Attend Workshops/Conferences: Participate actively at AI-related workshops \n   or events to network with professionals working within this field.\n</output>\n```python\n*ðŸ‘¨â€ðŸ’» > You are an AI assistant designed to provide detailed, step-by-step \nresponses. Your outputs should follow this structure:\n1. Begin with a <thinking> section.\n2. Inside the thinking section:\n   a. Briefly analyze the question and outline your approach.\n   b. Present a clear plan of steps to solve the problem.\n   c. Use a \"Chain of Thought\" reasoning process if necessary, breaking down \nyour thought process into numbered steps.\n3. Include a <reflection> section for each idea where you:\n   a. Review your reasoning.\n   b. Check for potential errors or oversights.\n   c. Confirm or adjust your conclusion if necessary.\n4. Be sure to close all reflection sections.\n5. Close the thinking section with </thinking>.\n6. Provide your final answer in an <output> section.\nAlways use these tags in your responses. Be thorough in your explanations, \nshowing each step of your reasoning process. Aim to be precise and logical \nin your approach, and don't hesitate to break down complex problems into \nsimpler components. Your tone should be analytical and slightly formal, \nfocusing on clear communication of your thought process.\nRemember: Both <thinking> and <reflection> MUST be tags and must be closed \nat their conclusion\nMake sure all <tags> are on separate lines with no other text. \nDo not include other text on a line containing a tag.\n\nuser question: explain why it is crucial for teachers to learn how to use \ngenerative AI for their job and for the future of education. \nInclude relevant learning path for teachers and educators. *\n\n\n**ðŸ’» > <thinking>\nI will begin by analyzing the importance of teaching about Generative \nArtificial Intelligence (AI) in an educational context, then outline a \nstep-by-step plan on what steps should be taken to learn this technology \neffectively as well as provide resources or courses that could help.\nFirstly, understanding AI is crucial for teachers because it can \nsignificantly enhance their ability to support students. \nTeachers need the knowledge and skills necessary not only to integrate these tools into teaching but also understand how they work so they are able to guide both themselves and their pupils in ethical use of technology.\n</thinking>\n\n<reflection>\nMy initial thought process was clear, emphasizing that teachers should \nbe equipped with AI literacy as it can improve educational outcomes. \nI outlined the importance for guiding students ethically while using \nthese tools effectively.\nHowever, my plan needs more detail on specific steps to learn about \ngenerative AI and resources available online or in-person courses.\n</reflection>**\n*MY PERSONAL NOTE: here it is starting an additional thinking after the first \nreflection... And this is really too good to be true!!*\n**<thinking>\nTo create a comprehensive learning path that teachers could follow:\n1. **Understand Basic Concepts**: Start with basic concepts of machine \n   learning (ML) such as supervised vs unsupervised, deep neural networks  \n   etc., which are foundational to understanding generative AI.\n2. **Learn About Generative Models**:\n   - Study different types: Variational Autoencoders (VAEs), GANs.\n   - Understand how these models generate new data that resembles the \n     training set but is not identical and can be used for tasks like \n     text generation, image synthesis etc., which are crucial in education.\n3. **Explore Applications**: Look at real-world applications of generative \n   AI such as language modeling (e.g., GPT-2), computer vision techniques to \n   create realistic images or videos from raw data.\n4. **Ethical Considerations and Implications for Education**:\n   - Discuss the ethical implications, including bias in models' outputs \n     which could affect studentsâ€™ perceptions about themselves if they are \n     exposed regularly through AI-generated content that is not representative \n     of them.\n5. **Hands-on Practice**: Engage with practical exercises using open-source \n   tools like TensorFlow or PyTorch to build simple generative model \n   applications.\n6. **Stay Updated and Network**:\n   - Follow the latest research in ML, especially on Generative Models \n     by following relevant journals (e.g., NeurIPS), conferences such as \n     ICML etc.\n7. Attend Workshops/Conferences: Participate actively at AI-related workshops \n   or events to network with professionals working within this field.\n</thinking>**\n*MY PERSONAL NOTE: and no surprise, an additional reflection now starts, \nbefore producing the final output*\n**<reflection>\nMy plan has been expanded and includes a more detailed approach for teachers, \ncovering foundational knowledge through practical application of generative \nmodels in education along with ethical considerations that are crucial. \nI have also included staying updated on the latest research as an important \npart.\nHowever, it's essential to ensure these resources or courses exist so \neducators can access them easily.\n</reflection>\n\n<output>\nIt is vital for teachers and educational professionals alike to learn how \ngenerative AI works because this technology has significant potential in \nenhancing teaching methods. By understanding basic concepts of machine \nlearning (ML) such as supervised vs unsupervised, deep neural networks etc., \nthey will be better equipped with the knowledge necessary not only to \nintegrate these tools into their curriculum but also guide students ethically.\nA comprehensive plan for teachers and educators includes:\n1. **Understand Basic Concepts**: Start by grasping foundational concepts of ML.\n2. **Learn About Generative Models**:\n   - Study different types: Variational Autoencoders (VAEs), GANs, etc., \n     which are crucial in generating new data that resembles the training \n     set but is not identical and can be used for tasks like text generation or \n     image synthesis.\n3. **Explore Applications**: Look at real-world applications of generative AI \n   such as language modeling using tools like TensorFlow.\n4. **Ethical Considerations**:\n   - Discuss ethical implications, including bias in models' outputs which \n     could affect studentsâ€™ perceptions about themselves if they are exposed \n     regularly through AI-generated content that is not representative.\n5. **Hands-on Practice**: Engage with practical exercises to build simple \n   generative model applications using open-source tools.\n6. **Stay Updated and Network**:\n   - Follow the latest research in ML, especially on Generative Models by \n     following relevant journals (e.g., NeurIPS), conferences such as ICML etc.\n7. Attend Workshops/Conferences: Participate actively at AI-related workshops \n   or events to network with professionals working within this field.\n</output>\n\n```\n\nSuch a refined thinking process, together with a clear open/closure of the  can be easily processed in further pipelines, or pretty printed in a nice GUI.\n\n## What it is bad at?\n\nThe worst task was the creative writing of a short story. The model started to repeat the same paragraph in the middle of the generation.\n\nThis does not mean that it cannot do it. More likely increasing the temperature and the repetition penalty as well, a good result can be achieved.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GNJ3lG6FM5qt9glIdf7qhQ.jpeg)\n\n## Conclusions\n\nYou can find all the chat history in my GitHub repo, together with the code and the instructions to do it yourself. Tou can use as a reference the tutorial from my previous article [Qwen2.5 1.5b: the future of Mobile AI?](https://ai.gopubby.com/qwen2-5-1-5b-the-future-of-mobile-ai-6bd5f29bbc84)\n\nIn the next articles I will cover other Small Language Models, using the same pricniple: from the tiny 350M paratmeters, to the small 500M series, up to the 3B â€” passing throught the 1.5B.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/retrieval-augmented-generation-approaches-state-of-the-art-and-optimization-strategies-456883da4801","frontmatter":{"title":"Retrieval-Augmented Generation: Approaches, State of the Art, and Optimization Strategies","meta_title":"Retrieval-Augmented Generation: Approaches, State of the Art, and Optimization Strategies","description":"â­ RAG is particularly useful in knowledge-intensive scenarios or domain-specific applications that require knowledge thatâ€™s continuallyâ€¦","date":"2024-10-31T08:17:32.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*_vE7WktGmyQ5xg_t5cpFVg.jpeg","categories":["Generative AI","Natural Language Processing","Machine Learning"],"author":"Rifx.Online","tags":["RAG","retrieval","generation","optimization","embeddings"],"draft":false,"slug":"blog/retrieval-augmented-generation-approaches-state-of-the-art-and-optimization-strategies-456883da4801"},"content":"\n\n\n\n\n\nâ­ RAG is particularly useful in knowledge\\-intensive scenarios or domain\\-specific applications that require knowledge thatâ€™s continually updating. RAG has been popularized recently with its application in conversational agents.\n\nðŸ“Œ Research in reference focusses mainly on current approaches \\& different components of RAG, State of the Art (SOTA), applications, evaluation for retrieval, generation, augmentation techniques.\n\nWith the evolution of RAG systems from NaÃ¯ve to Advanced to Modular, and each of which is came into picture to address per use case basis enhancements.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*P2ByKtayhF4XgAVxRI1urQ.jpeg)\n\nâœ *NaÃ¯ve*: User input is used for document query, appended/combined with prompt, used for model final response generation. With the Multiturn Dialogue interactions context/conversational history can be added/combined with the prompt. Cons: Low precision / Low recall, redundant, repetitive.\n\nâœ*Advanced*: Improves retrieval quality by optimizing pre\\-retrieval, retrieval, and post retrieval methods. With pre\\-retrieval, quality enhanced through enhancing data granularity, index structure improvements, metadata, alignment, mixed retrieval. In retrieval, optimizing the embedding model hence context. With post\\-retrieval, optimization in context window and noisy/distracting data rejection.\n\nâœ*Modular*: Incorporates a search module for similarity retrieval and fine tuning in retrieval. New Modules being Search, Memory, Fusion, Routing, Prediction, task Adaptor.\n\nðŸ¥‰ To optimize RAG Pipeline:\n\nðŸ“œ *Hybrid Search Exploration*: Performance optimization balances by intelligently leveraging techniques such as keyword\\-based search, semantic and vector search.\n\nðŸ“œ*Recursive Retrieval and Query Engine*: Might start retrieval with acquiring smaller chunks in the initial phase, subsequently, larger chunks with better and more contextual information to LLM for balance between contextually rich responses and efficiency.\n\nðŸ“œ*StepBack\\-promp*t: This encourages the LLM to move away from specific instances and engage in reasoning around broader concepts and principles(arXiv:2310\\.13243\\). A significant performance increase observed, in various challenging, inference\\-based tasks when backward prompts are used, highlighting their natural adaptability to the RAG process.\n\nðŸ“œ*Sub\\-Queries*: Query strategies depending on the scenario could be applied such as using query engines provided by frameworks like LlamaIndex, leveraging tree queries, utilizing vector queries, or executing simple sequential querying of chunks.\n\nðŸ“œ*Hypothetical Document Embeddings*: With the LLM, HyDE responses to the query by creating hypothetical answer, embeds the answer, uses the same to retrieve real documents. Instead of seeking embedding similarity based on the query, this approach focuses on the embedding similarity from one answer to another\\[arXiv:2212\\.10496]. Cons: Inconsistent Answers not producing desirable outcomes, Errors for LLM unseen Subject Matter, leading to errors.\n\nLet me cut off here. Iâ€™ll come up with a new post in follow\\-up\n\n[\\#genai](https://www.linkedin.com/feed/hashtag/?keywords=genai&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7170160104984571905) [\\#rag](https://www.linkedin.com/feed/hashtag/?keywords=rag&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7170160104984571905) \\#ai \\#llm\n\nRef: [arxiv:2312\\.10997](https://arxiv.org/pdf/2312.10997), RAG Surveys, Huggingfaceblogs\n\n\n"},{"lang":"en","group":"blog","slug":"blog/roadmap-to-become-an-ai-engineer-in-2025-4323ae4c2f5c","frontmatter":{"title":"Roadmap to Become an AI Engineer in 2025","meta_title":"Roadmap to Become an AI Engineer in 2025","description":"The article outlines a comprehensive roadmap for aspiring AI engineers in 2025, emphasizing the importance of understanding AI engineering fundamentals, mastering mathematics and statistics, and learning Python programming. It covers essential skills including data science, machine learning, and deep learning, along with practical project experience and deployment skills. The guide suggests various resources and courses for each topic, highlighting the significance of building a portfolio and networking for career advancement. Overall, it presents a structured approach to entering the AI engineering field, making it accessible for beginners.","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*emg2RJ9qx4OgysH2r22nuQ.png","categories":["Programming","Machine Learning","Data Science"],"author":"Rifx.Online","tags":["Python","machine-learning","deep-learning","data-science","statistics"],"draft":false,"slug":"blog/roadmap-to-become-an-ai-engineer-in-2025-4323ae4c2f5c"},"content":"\n\n\n\n\n### How to Become an AI Engineer in 2025\n\n\n\nEver wondered what it takes to build systems that can think, learn, and solve complex problems? A few years ago, I was curious too â€” AI was this futuristic concept, and I had no clue where to begin. Now, as we step into 2025, becoming an AI engineer is more accessible than ever. If youâ€™re here, youâ€™re probably curious about how to enter this field from scratch. The good news? No need to be a computer genius or a math prodigy. With a clear roadmap, dedication, and the right resources, you can make it.\n\nIn this ultimate guide, weâ€™ll dive into every step, skill, and resource you need to transform yourself into an AI engineer. Whether youâ€™re starting fresh or already have some tech knowledge, this guide will break down everything into manageable steps. So letâ€™s get started.\n\n\n## 1\\. Understand What AI Engineering Actually Is\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*LqwczRy-BZOH-zhfxNjc2g.png)\n\n\n### What is AI Engineering?\n\nAI Engineering is all about designing and deploying AI models that can solve real\\-world problems. From self\\-driving cars to personalized recommendations, AI engineers create systems that learn from data and make intelligent decisions.\n\n\n### Roles and Responsibilities of an AI Engineer\n\n* **Developing Machine Learning (ML) models** for predictions and data insights\n* **Programming and software development** focused on AI applications\n* **Data collection and preprocessing** to create a foundation for AI models\n* **Evaluating model performance** and making improvements\n* **Deployment and integration** of AI solutions in business environments\n\n\n## 2\\. Master the Foundations: Mathematics and Statistics\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*HxbGZ1D4QFGf2FZ2dKLcsQ.png)\n\nBefore jumping into AI algorithms, you need a solid foundation in math. Here are the essential topics:\n\n\n### Key Math Topics for AI Engineering\n\n* **Linear Algebra:** Essential for understanding neural networks. Topics: vectors, matrices, eigenvalues.\n* **Probability and Statistics:** To help you make data\\-driven decisions. Topics: distributions, hypothesis testing, Bayesian concepts.\n* **Calculus:** Used in optimizing machine learning models. Topics: derivatives, partial derivatives, and gradients.\n\n\n### Resources for Learning Math\n\n1. **Khan Academy** â€” [Khan Academy Math Courses](https://www.khanacademy.org/) (Free, Beginner\\-friendly)\n2. [**3Blue1Brown YouTube Channel**](https://www.youtube.com/c/3blue1brown) â€” Great for visual explanations, especially for linear algebra and calculus.\n3. **Coursera: Mathematics for Machine Learning** â€” [Coursera Math for Machine Learning](https://www.coursera.org/specializations/mathematics-machine-learning) (Free audit, paid for certification)\n\n\n## 3\\. Learn to Code (Python is King)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*NxkreIEpQiuVUDD4_gWFSQ.png)\n\n\n### Why Python?\n\nPython is the go\\-to language for AI because itâ€™s simple, versatile, and has tons of libraries for AI and data science. You donâ€™t need to know advanced programming to start, but mastering Python is essential.\n\n\n### Python Topics to Cover\n\n* **Basics:** Variables, loops, functions, and data structures.\n* **Data Science Libraries:** Numpy, Pandas, and Matplotlib for data manipulation and visualization.\n* **Machine Learning Libraries:** Scikit\\-Learn, TensorFlow, and PyTorch.\n\n\n### Best Python Resources\n\n1. [**Codecademy Python Course**](https://www.codecademy.com/catalog/language/python) â€” Codecademy Python (Beginner\\-friendly)\n2. [**Googleâ€™s Python Class**](https://developers.google.com/edu/python) â€” Google Python Class (Free)\n3. [**Python for Data Science Handbook by Jake VanderPlas**](https://jakevdp.github.io/PythonDataScienceHandbook/) â€” Python for Data Science Book (Free online)\n\n\n## 4\\. Get Comfortable with Data: Data Science Basics\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*tugfLSYdMDps7t46G7dbhw.png)\n\nData is at the heart of AI. As an AI engineer, youâ€™ll need to collect, clean, and analyze large datasets. Hereâ€™s what you need to focus on:\n\n\n### Key Data Science Skills\n\n* **Data Collection and Cleaning:** Learn how to handle missing values, clean messy data, and preprocess for ML models.\n* **Exploratory Data Analysis (EDA):** Understand how to analyze and visualize data patterns.\n* **Feature Engineering:** Process raw data into useful features that improve model accuracy.\n\n\n### Data Science Courses\n\n1. **IBM Data Science Professional Certificate on Coursera** â€” [IBM Data Science](https://www.coursera.org/professional-certificates/ibm-data-science) (Beginner\\-friendly, structured pathway)\n2. **DataCampâ€™s Data Scientist with Python Track** â€” [DataCamp Python Track](https://www.datacamp.com/tracks/data-scientist-in-python) (Subscription\\-based)\n3. [**Python Data Science Handbook by Jake VanderPlas**](https://jakevdp.github.io/PythonDataScienceHandbook/) (free online resource)\n\n\n## 5\\. Dive into Machine Learning\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*JwqTp844juTSX8FFSxvfTg.png)\n\nMachine Learning is the core of AI engineering. Itâ€™s where algorithms are developed to learn patterns from data and make predictions. Hereâ€™s the roadmap:\n\n\n### Essential ML Topics\n\n* **Supervised Learning:** Linear regression, logistic regression, decision trees, and random forests.\n* **Unsupervised Learning:** Clustering techniques like K\\-means and hierarchical clustering.\n* **Model Evaluation:** Understand metrics like accuracy, precision, recall, F1\\-score, and ROC curves.\n* **Deep Learning Basics:** Introduction to neural networks and how they work.\n\n\n### Top ML Resources\n\n1. **Andrew Ngâ€™s Machine Learning Course on Coursera** â€” [Machine Learning by Andrew Ng](https://www.coursera.org/learn/machine-learning) (Beginner\\-friendly)\n2. **Hands\\-On Machine Learning with Scikit\\-Learn, Keras, and TensorFlow by AurÃ©lien GÃ©ron** â€” [Hands\\-On Machine Learning](https://github.com/Akramz/Hands-on-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow) (Practical, hands\\-on approach)\n3. **Fast.aiâ€™s Practical Deep Learning for Coders** â€” [Fast.ai Course](https://course.fast.ai/) (More advanced but very practical)\n\n\n## 6\\. Explore Deep Learning and Neural Networks\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*AW1bwL9BTcOr22EkiDkg3w.png)\n\nDeep learning takes AI a step further with neural networks that mimic the human brain. These networks are used in applications like image recognition and language translation.\n\n\n### Core Topics in Deep Learning\n\n* **Neural Networks Basics:** Perceptrons, activation functions, forward and backward propagation.\n* **Convolutional Neural Networks (CNNs):** Used for image processing.\n* **Recurrent Neural Networks (RNNs):** Useful for sequential data like time series and natural language processing.\n\n\n### Deep Learning Courses and Resources\n\n1. **Deep Learning Specialization by Andrew Ng on Coursera** â€” [Deep Learning by Andrew Ng](https://www.coursera.org/specializations/deep-learning) (Comprehensive, highly recommended)\n2. **TensorFlow in Practice by Deeplearning.ai on Coursera** â€” [TensorFlow Course](https://www.coursera.org/professional-certificates/tensorflow-in-practice)\n3. [**PyTorch Deep Learning Projects by Packt**](https://www.packtpub.com/en-us/product/deep-learning-with-pytorch-9781788624336) â€” Great book for hands\\-on projects using PyTorch.\n\n\n## 7\\. Get Hands\\-On with Real Projects\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*7c0NpXhFkZ--spsXM9qBrg.png)\n\nTheoretical knowledge is great, but AI is a practical field. Working on projects will solidify your understanding and give you something to showcase in your portfolio.\n\n\n### Project Ideas for Beginners\n\n* **Predict Stock Prices** using historical data (Time series forecasting)\n* **Image Classification** using a CNN (Classifying images of animals, vehicles, etc.)\n* **Sentiment Analysis** on social media posts or product reviews\n\n\n### Where to Find AI Project Datasets\n\n1. **Kaggle Datasets** â€” [Kaggle Datasets](https://www.kaggle.com/datasets) (Variety of datasets for ML projects)\n2. **UCI Machine Learning Repository** â€” [UCI ML Repository](https://archive.ics.uci.edu/ml/index.php)\n3. **Google Dataset Search** â€” [Google Dataset Search](https://datasetsearch.research.google.com/)\n\n\n## 8\\. Master Model Deployment Skills\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*t9u1WCyf6TDeZ_M7m3IN-Q.png)\n\nKnowing how to deploy AI models in production is a huge advantage. This skill makes you a valuable asset, as it bridges the gap between data science and real\\-world applications.\n\n\n### Deployment Tools to Learn\n\n* **Flask \\& Django:** For creating simple web applications to serve your model.\n* **Docker:** To containerize your models, making deployment easier and more consistent.\n* **AWS, Azure, Google Cloud:** For deploying scalable AI models in the cloud.\n\n\n### Model Deployment Resources\n\n1. **Udacityâ€™s AI Product Manager Nanodegree** â€” [Udacity AI Product Manager](https://www.udacity.com/course/ai-product-manager-nanodegree--nd088)\n2. **Courseraâ€™s MLOps Specialization by Deeplearning.ai** â€” [MLOps Specialization](https://www.coursera.org/specializations/machine-learning-engineering-for-production-mlops)\n3. [**Docker for Data Science by Packt**](https://www.packtpub.com/en-in/product/the-ultimate-docker-container-book-9781804613986?type=print) â€” Great for learning Docker with a data science focus.\n\n\n## 9\\. Build a Portfolio and Start Networking\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*CmE_JeIOcWcogB9X57QheA.png)\n\nA strong portfolio showcases your skills, while networking opens up job opportunities and mentorship.\n\n\n### Portfolio Tips\n\n* Showcase 3â€“5 projects that highlight different skills (data science, ML, deployment).\n* Include a mix of individual projects and team collaborations.\n* Write up each project with a summary, code, and results.\n\n\n### Networking Platforms\n\n* [**LinkedIn**](https://www.linkedin.com/) â€” Connect with AI professionals and recruiters.\n* [**GitHub**](https://github.com/) â€” Publish your projects for potential employers to see.\n* [**Kaggle Competitions**](https://www.kaggle.com/competitions) â€” Join competitions to learn and showcase your skills.\n\n\n## Conclusion: Ready to Become an AI Engineer?\n\nBecoming an AI engineer in 2025 isnâ€™t just a dream; itâ€™s achievable if you follow a structured path. Start with the basics, invest time in projects, and keep pushing yourself to learn more advanced concepts. AI is a rapidly evolving field, and the more you commit to continuous learning, the more opportunities will open up.\n\nSo, are you ready to dive into the world of AI? Remember, every AI engineer started as a beginner. With persistence and curiosity, youâ€™ll be creating innovative AI solutions before you know it. Letâ€™s make it happen!\n\n\n## FAQs\n\n**1\\. Do I need a degree in AI to become an AI engineer?** No, while a degree can help, many AI engineers are self\\-taught or transition from related fields. Online courses, projects, and a strong portfolio can be equally valuable.\n\n**2\\. How long does it take to become an AI engineer?** It depends on your background, but a focused learner can achieve it in 6â€“12 months with dedicated study and project work.\n\n**3\\. What are the essential skills for an AI engineer?** Key skills include Python programming, machine learning, data science basics, and knowledge of deep learning frameworks like TensorFlow or PyTorch.\n\n**4\\. Which programming languages are necessary?** Python is the primary language for AI, but familiarity with R, SQL, or even JavaScript can be helpful depending on your role.\n\n**5\\. Is AI engineering a high\\-paying career?** Yes, AI engineering is one of the most in\\-demand and well\\-compensated tech fields, with competitive salaries worldwide.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*2luRkTNWk_o3KSh9.png)\n\nThis story is published on [Generative AI](https://generativeai.pub/). Connect with us on [LinkedIn](https://www.linkedin.com/company/generative-ai-publication) and follow [Zeniteq](https://www.zeniteq.com/) to stay in the loop with the latest AI stories.\n\nSubscribe to our [newsletter](https://www.generativeaipub.com/) and [YouTube](https://www.youtube.com/@generativeaipub) channel to stay updated with the latest news and updates on generative AI. Letâ€™s shape the future of AI together!\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*0DGBxUYjhr3BbfzS.png)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/searchgpt-changed-how-i-surf-the-web-forever-3e970027998c","frontmatter":{"title":"SearchGPT Changed How I Surf the Web Forever.","meta_title":"SearchGPT Changed How I Surf the Web Forever.","description":"The article advocates for the use of SearchGPT as a transformative tool for web searching, highlighting its three main benefits: ultra-fast semantic search, efficient understanding of complex concepts, and the elimination of tab clutter. It details how SearchGPT can outperform traditional search engines like Google in speed and efficiency, particularly for informational and transactional queries. The author provides practical tips and strategies for using SearchGPT effectively, emphasizing the importance of detailed prompts and the ability to aggregate information from various sources. Overall, SearchGPT is presented as a powerful alternative for enhancing productivity and streamlining research processes.","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*xwFpzwWrReqEjftU1cjksg.jpeg","categories":["Technology/Web","Programming/Scripting","SearchGPT is not a standard category","but the closest match from the given list is Technology/Web. The article focuses on a web-based tool that enhances search capabilities","which falls under the Technology/Web category. Additionally","the use of advanced search techniques and programming aspects of the tool suggest Programming/Scripting as a relevant category. However","since SearchGPT is the main focus","Technology/Web is the most appropriate primary category. \n\nTherefore","the final categories are:\nTechnology/Web","Programming/Scripting"],"author":"Rifx.Online","tags":["SearchGPT","semantic","efficiency","prompts","aggregation"],"draft":false,"slug":"blog/searchgpt-changed-how-i-surf-the-web-forever-3e970027998c"},"content":"\n\n\n\n\n### I said goodbye to Google, and you can too. Learn how to use SearchGPT, and transform the way you work.\n\n\n\nThis isnâ€™t clickbait. I am a full believer in SearchGPT, and Iâ€™m going to tell you why. This article is simultaneously a manifesto for why you should start using SearchGPT and a comprehensive guide on how to use it optimally.\n\nWhether youâ€™re a newcomer to SearchGPT, have dabbled a little bit, or are already an advanced user, I guarantee you will find something useful here. This article is a bit of a doozy, but it might be the most time\\-saving improvement you can make to your workflow in 2024\\.\n\nWithout further ado, letâ€™s take a peek at some of the primary reasons I love SearchGPT so much.\n\n\n## The Top 3 Benefits of SearchGPT\n\n\n### 1\\. Ultra\\-Fast Semantic Search\n\nYou can search for *literally anything* with search GPT. In fact, in my side\\-by\\-side test, it can be up to **10x faster** than a standard Google Search rabbit hole.\n\nFor example, the other day, I remembered a boot brand I saw a while ago that I wanted to explore. I knew some details about the brand, but it was hard to put into a single search. Hereâ€™s what I knew:\n\n* Itâ€™s a single\\-name brand (i.e., â€˜Jimâ€™s Bootsâ€™ or similar)\n* The boots are pretty expensive but high\\-quality\n* The boots are handmade\n* They are Goodyear Welted Soles\n* They are an American Brand (maybe)\n* They were highly regarded by the community of a boot Subreddit\n\nIt was one of those situations where if I had seen it, I would have remembered it; it was teetering on the tip of my tongue. So, I decided to do a little test. I compared SearchGPT and Google directly. I set a timer and started searching with Google first.\n\nAfter finding nothing on the shopping page (searching â€œhandmade American Goodyear welted bootsâ€ to no avail), I started scrolling and scrolling. After the first 30 results, I again found nothing.\n\nI transitioned to inserting â€œRedditâ€ at the end of my search. The first two threads that came up yielded nothing, but then I found it. There was a [huge post overviewing the best boot brands for given price points](https://www.reddit.com/r/goodyearwelt/comments/7qxy6p/the_2018_beginners_boot_buying_guide/), and I began skimming down the extensive list, searching for the brand. After a minute, I found it: Nicks Boots.\n\nMy total search time? **4:37\\.**\n\nI set a new timer, opened up SearchGPT, and typed in a prompt about what I knew about this brand. It took me about **20 seconds** to write out the prompt. Here are the results:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*JM16NcvRzXDZB2dnT2K96w.png)\n\nBoom. There it is, coming in at number 7\\. This was a whopping **13x** faster than Google search. Do I have to convince you of its speed any more?\n\n\n### 2\\. Quickly Understanding the Certain Parts of a Concept\n\nIf you need to search the internet to learn more about something, thereâ€™s a chance youâ€™ll probably end up on Wikipedia, or you might read the [suggested snippet from Google](https://support.google.com/websearch/answer/9351707?hl=en). While both are a great resource, there is often a lot of information about the subject that may distract you from your root goal. With SearchGPT, you can get a quick overview, narrow down your understanding with follow\\-up questions, and achieve your knowledge goal faster.\n\nFor example, letâ€™s say weâ€™re a total layperson who wants to understand how a wind turbine converts wind into energy.\n\nFor a traditional search flow, weâ€™d likely search â€œwind turbineâ€ or â€œhow does a wind turbine work.â€ Letâ€™s see the Google search results of this:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*93wqzjbylGagTlO9038iqw.png)\n\nAfter this, we now understand the gist of the wind turbine. However, at the same time, we know nothing. In order to form an actual understanding, we must fall into a rabbit hole. So, youâ€™d likely search about the actual inner workings of an electric generator next. Then, youâ€™d have to do another search, narrowing it down to wind turbine generators and synthesizing information from there.\n\nThis process works, butitâ€™s **so inefficient.**\n\nWith SearchGPT, we can formulate our knowledge base more quickly and intuitively. Itâ€™s as if weâ€™re speaking with a wind turbine expert. Hereâ€™s a sample flow:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BbSoxRLJNPh_BQehsaxB8g.png)\n\nNow, we can pick from one of these components to quickly understand the subject on a deeper level.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*REgvUWKmAWGRCrzi6nSeGw.png)\n\nAt any point, we can click on the links provided to read from the direct source, and our chat remains intact. This is the fastest way Iâ€™ve found to learn more about something, all without fear of AI hallucinations (mostly).\n\n\n### 3\\. Say Goodbye to Your Tab Woes\n\nWith SearchGPT, tabs are essentially obsolete. No more messing around with multiple windows of Chrome, each with several dozen tabs. No more losing your multi\\-day research progress from an unexpected computer restart, or wishing you could bring up a search rabbit hole from months prior.\n\nSearchGPT collects all your research in one place. Itâ€™s easy to navigate, and you have an infinite history of your searches.\n\nLetâ€™s say that youâ€™ve been using SearchGPT for a few months. You might remember a little stint of research you did on [online ad remarketing](https://mailchimp.com/resources/what-is-retargeting/). Well, how can you find it? With the trusty ChatGPT search bar, we can find the exact chat super quickly and begin right where we left off:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BSOOg5-XJc0_zacDf0f87Q.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*32x_mj_WnLLc_xxH6WBuRg.png)\n\nAs you can see, itâ€™s quick and easy to navigate chats, click on sources, and read deeper on any subject. You can quickly navigate to certain sections of the chat (if you have a super long chat) by keying â€œctrl\\+fâ€ and searching the page for keywords.\n\nThose are my top 3 benefits of SearchGPT, but there are many more.\n\nLetâ€™s transition to the next and most crucial section of this article: **how to use SearchGPT optimally.**\n\n\n## SearchGPT Quickstart\n\nIf you already know how to start using SearchGPT, please skip to the next section.\n\nThere are three ways to start using SearchGPT:\n\n1. **SearchGPT in Chat** â€” Open a normal GPT\\-4o Chat, and click the search icon:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*-p9GcMd7fRlwmXdHvLBzvw.png)\n\nThis way, every prompt you type into the chat bar will invoke a web search. Itâ€™ll return its response, along with sources like so:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*WQMDebhc5g61YvpaQUDeYw.png)\n\n2\\. **SearchGPT Direct â€”** Navigate to this link: <https://chatgpt.com/search> and start searching:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*b-nIPELvT1heBlkbOcKAzQ.png)\n\nYou can open up sources by clicking the link icon on the left, and you can also view more media by clicking the image icon.\n\n3\\. **Switch your default search engine to SearchGPT using the Chrome Extension â€”** Navigate to [this link](https://chromewebstore.google.com/detail/chatgpt-search/ejcfepkfckglbgocfkanmcdngdijcgld?pli=1) to add the extension to Chrome.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*hGkw33h7uEP7rFwCtpbr7g.png)\n\nThe Chrome Extension redirects to SearchGPT after every query you type into the search bar. While I donâ€™t necessarily recommend doing this (Google still has its place for some things as Iâ€™ll discuss later), feel free to try it out and see how it works for you.\n\nWhile each of the methods of using ChatGPT is valid, I personally prefer simply using it within the normal ChatGPT interface (option 1\\). For that reason, most of this article will focus on Option 1\\.\n\n\n## Understanding the Art of Internet Search\n\nBefore we jump into the advanced SearchGPT strategies I promised, we must understand what search is. Searching the internet is actually quite a nuanced subject. Even if youâ€™ve never consciously thought about it, you have several internet search pathways ingrained in your head. You need to understand how SearchGPT can help (or not) with each one of these paths.\n\n\n### The 7 Paths of Internet Search\n\nWithin the field of Internet Search, there many general pathways that encompass most activities. Here is a brief overview of each of them:\n\n1. **Navigational Searching**: When you want to quickly reach a specific website, you search for it by name or brand (e.g., â€œAmazonâ€ or â€œFacebook loginâ€).\n2. **Informational Searching (Direct and Indirect)**: When youâ€™re looking to learn about a topic or find an answer, you use open\\-ended queries (Direct: â€œWhat is climate change?â€ Indirect: â€œFacts about climate changeâ€).\n3. **Transactional Searching**: When youâ€™re ready to make a purchase or sign up for something, you search with intent to complete an action (e.g., â€œbuy iPhone 15â€ or â€œsign up for mediumâ€).\n4. **Commercial Investigation**: When youâ€™re comparing products or services before a purchase, you search with keywords like â€œbestâ€ or â€œtopâ€ (e.g., â€œbest laptops for studentsâ€ or â€œtop laptops 2024â€).\n5. **Refinement and Iterative Searching**: When you refine your search multiple times to get closer to what you need, you adjust keywords and phrasing (e.g., â€œGDPR rulesâ€ to â€œEU data privacy for businessesâ€). This is the basis for most â€œsearch rabbit holesâ€\n6. **Longitudinal Searching**: When youâ€™re doing extended research over multiple tabs and sessions, you might leave open tabs for later reference and continue refining your search each time (e.g., when youâ€™re comparing potential flight itineraries to one another).\n7. **Known\\-Item Searching**: When youâ€™re looking for specific content you know exists, you search with titles, names, or unique details (e.g., â€œNY Times remote work article by Jane Doeâ€). This is similar to navigational but more specific.\n\n\n### Other Paths of Internet Search (My Findings)\n\nIâ€™ve observed a few other pathways that Iâ€™m sure many of you will be familiar with. While these are not formally recognized, I feel itâ€™s worth it to cover them here:\n\n8\\. **Human Validation Searching:** When you specifically seek out real peopleâ€™s opinions on a subject (e.g., adding â€œRedditâ€ to the [end of a search query](https://detailed.com/forum-serps/)).\n\n9\\. **Anti\\-SEO Searching:** Similar to informational searching, but when you are looking for information from a truly reputable source, not one that has the best [SEO tactics](https://www.theverge.com/features/23931789/seo-search-engine-optimization-experts-google-results) but lacks legitimate info (e.g., scrolling frustratedly past unknown websites before you find one that you trust).\n\n10\\. **SOS Searching:** Scouring the web for help from anyone on any forum (no matter how obscure) who has the same problem as you (e.g., searching â€œSamsung TV wonâ€™t turn on when HMDI 1 is plugged inâ€).\n\n11\\. **Anti\\-Confirmation Bias Searching:** When youâ€™re having a debate with your buddies, so you must search the most [non\\-biased terms possible](https://dl.acm.org/doi/10.1145/3635034) (e.g., proving your argument correct while satisfying the viewpoint of your fellow debater).\n\n12\\. **Known Unknown Searching:** When you know some aspects about the thing you are looking up, but are unsure of exactly how to find it (e.g., searching â€œThereâ€™s this one old youtube video of a kid freaking out about losing some video game, and thereâ€™s a whole series of videos about themâ€¦â€)\n\nNow that youâ€™re familiar with the different types of search, we can finally take a deep dive into how SearchGPT can help with each one. Buckle up!\n\n\n## SearchGPT Strategyâ€” The Nitty Gritty\n\nThis section breaks down the pros and cons of SearchGPT for each search pathway I presented above. Each subsection comes equipped with examples, suggestions, and tips to optimize how you use SearchGPT. Letâ€™s get started!\n\n\n### SearchGPT for Navigational and Known Item Searching\n\nTo put things bluntly, SearchGPT isnâ€™t going to help us much here. Thereâ€™s really no efficiency gain from typing â€œmediumâ€ into Google versus typing it into SearchGPT. However, some webpages can be difficult to access, especially if you forgot to bookmark them. This is where SearchGPT shines.\n\nHereâ€™s a quick example. Sometimes I find it difficult to access my API account information for certain sites. Letâ€™s take my Perplexity AI API account. It takes me forever to navigate to this page, and Iâ€™ve been remiss in bookmarking it. However, with SearchGPT, I can find it right away:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*sSi2EcTPGE3SNg_8qZnRog.png)\n\n**Tips for SearchGPT Navigational Searching:**\n\n* Be detailed in what youâ€™re looking for. Donâ€™t be afraid to just write it out directly\n* If you arenâ€™t finding what youâ€™re looking for, ask GPT to ask *you* clarifying questions about your query so it can better assist you\n* For known item searching (i.e. Jordan Gibbs Medium Article about automated prompt engineering) it can be helpful, but once again, it has no real advantage over Google.\n\nThis is one of the least impactful ways to use SearchGPT, but it can still be useful from time\\-to\\-time.\n\n\n### SearchGPT for Informational Searching (Direct and Indirect)\n\nSeachGPT is excellent for accessing specific things or aggregating up\\-to\\-date information quickly. Good old ChatGPT (without Search) has the ever\\-infuriating knowledge cutoff. However, we can now directly ask it things that it doesnâ€™t â€œknow aboutâ€ because it can find it through the internet. We can also massively reduce hallucinations, so we can trust it a lot more! Just make sure you always check its sources for important information :)\n\nAs a demonstration, letâ€™s say I want to investigate some recent advancements to a data app platform I like to use, Streamlit:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MSaJSdJGJ1vGa3uVnfEKZg.png)\n\nNow, I can easily allow ChatGPT to access these up\\-to\\-date docs, so it can write code for me using the brand\\-new features.\n\nThatâ€™s just one example of how SearchGPT improves Informational Search, but here are some more helpful tips and tricks.\n\n**Tips for SearchGPT Informational Searching (Direct):**\n\n* Always mention the current year or â€œtodayâ€ if you want up\\-to\\-date information\n* You can use the prompt â€œplease examine many sources and aggregate the information from all of themâ€ to aggregate information about a certain subject. Hereâ€™s a sample:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*HFKD-3f89gK8do6Dqu01gg.png)\n\n* Try a follow\\-up prompt if the first response isnâ€™t recent enough or specific enough\n* Treat the search process like a conversation with an expert (donâ€™t be afraid to just write whatever comes to mind)\n* You can ask it to â€œdeep diveâ€ on a specific source by calling it out specifically:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*XbXKRKy5WkF0QX1X0yzr5w.png)\n\n* Insert this prompt: â€œList some potential follow\\-up questions for me after you respondâ€ if you want to have a guided search experience\n* Donâ€™t be afraid to force it to reference a source. Sometimes, it can resist a little bit and fall back on its existing knowledge. You can do this simply by stating: â€œYou must reference a source for every point you bring up no matter whatâ€\n\nIndirect informational searching (i.e., searching without a goal in mind) is excellent with SearchGPT. Here are some tips.\n\n**Tips for SearchGPT Informational Searching (Indirect):**\n\n*Note: this section also covers â€œRefinement and Iterative Searchingâ€, â€œLongitudinal Searchingâ€, and â€œKnown Unknown Searchingâ€ as mentioned above.*\n\n* Ask it to create a learning outline for you to follow so you can incrementally learn what you need to learn in order to understand a concept at large\n* Prep the session by requesting that it ask you clarifying questions before you begin your search so you can narrow down what you want to look for faster. Use the prompt: â€œBefore you begin, please ask me clarifying questions about my learning goals.â€ Hereâ€™s an example of it in action:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*LE2mSqStwuy18ryNVQbZQw.png)\n\n* Ask it to recommend YouTube videos. SearchGPT can find highly relevant videos about your requests, so your learning experience can be multimedia:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*7zhXPezwiMskkEcvAEfqjQ.png)\n\n* Ask it to recommend photos, diagrams, and graphs for visual aids:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*FItZcImfTJVenkzy1lJiqw.png)\n\n* Another approach is what I call â€œsingle\\-source branching,â€ where you ask SearchGPT to summarize a single source (that you trust and contains a full overview of a subject) and then branch out from there:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*P8C4hpd8AxZgfuXDW0PNxg.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*_bHNNzZAukNMPIk8W6hhEQ.png)\n\n* To take an even deeper dive, and to avoid â€œpoisoningâ€ GPTâ€™s context window, I recommend opening a new chat if you want to discuss something very specific:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*G6Cxxvd7uGn-fS4pFkvyiw.png)\n\n* You can always search your recent chats in the search bar, so youâ€™ll never lose any of your history!\n\nInformational search with SearchGPT is incredible, and I canâ€™t get enough of it. Iâ€™ve been so excited to speed up my learning trajectory on everything I want to learn about!\n\n\n### SearchGPT for Transactional and Commercial Investigation Searching\n\nYes, you can use SearchGPT to find places to sign up for or purchase services and products. While this isnâ€™t necessarily any faster or better than Google, it does have some exciting features, such as this map:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*zWb-fbtC-CHY4mi8ZR997w.png)\n\nYou can also find new places to consume content:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*y1ryR4QJI7VUvyEgqEs_Sw.png)\n\nThe scope of this is pretty limited in SearchGPT, so this might be a small win for Google :) However, itâ€™s still helpful for quickly getting pointed to the right place!\n\nYou can also shop for products with SearchGPT. However, itâ€™s not in the way youâ€™d expect. Donâ€™t think that ChatGPT can point you to exact product links (it can, but it often hallucinates); instead, think of it as a product comparison tool.\n\nHereâ€™s a sample:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*9MTMHNyTY-L4WjGTZhg9BQ.png)\n\nIt now points me to brands that make sweaters to my liking: crewneck, chunky, oversized, and earth\\-tone colored:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*5a02uR0X07VFd4_2hXFCKg.png)\n\nRemember earlier how I mentioned that many people do what I call â€œhuman validation searching?\n\nUnfortunately, SearchGPT canâ€™t seem to access many forums, such as Reddit. While it does have a lot of Reddit in its training data, it cannot access modern threads. Thatâ€™s one huge limitation of SearchGPT for now!\n\n\n### SearchGPT for Anti\\-SEO Searching\n\nAnti\\-SEO article searching is a new trend that Iâ€™ve seen emerging. At its core, itâ€™s essentially a filter inside our brains that tries to find websites that we have interacted with positively before or sources that we know are generally trustable. Hereâ€™s how SearchGPT can help with that:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*dWUoRwDhfvwEE7K5S-u0Bg.png)\n\nThis quick process can erase a lot of heartache from your research.\n\n**Tips for Anti\\-SEO Searching**\n\n* Use this prompt to pre\\-filter: â€œI only want reputable sources. Give me a list of potential sources, and Iâ€™ll choose the ones I want to hear from.â€\n* Another way to do this is to say, â€œDo not output anything from a source that is not well knownâ€\n\n\n### SearchGPT for SOS Searching\n\nSOS (Save Our Souls) Searching is a fun one. With Google, you have the innate desire to write out exactly what youâ€™re experiencing, but you know you canâ€™t. With SearchGPT, you can write a stream\\-of\\-consciousness report of your situation, and it can articulate your searches into an intelligent stream that will locate people who have had your issue before you much faster. Here is an example:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*107zPh98LewGZ8xuZ7aGWA.png)\n\nNow, I can narrow down the problem from its list of potential solutions at the bottom:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ZxGSV6vDOVV2SI566x4jwg.png)\n\nThis has already helped me in the past couple of weeks, and I couldnâ€™t be happier with its performance.\n\n**Tips for SOS Searching with SearchGPT:**\n\n* Use this prompt to narrow it down to real human accounts of the problem: â€œFind some accounts of people with similar issues and output an overview to meâ€\n* Use this prompt if it is outputting overly general advice: â€œI need highly specific cases that match this scenario exactlyâ€\n* Donâ€™t be afraid to be super detailed; write several paragraphs if need be outlining the history of the problem. SearchGPT can sort through the noise\n\n\n### SearchGPT for Anti\\-Confirmation Bias Searching\n\nThis is a perfect use case for SearchGPT when youâ€™re deep in a debate with a friend, preparing for one, or just learning about a controversial topic in general. This approach allows you to see more sides of the issue at once, so you can add some nuance to your viewpoints. Hereâ€™s a sample:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*5-FLP0-R1K8oW6KDSE5jLQ.png)\n\nThe response shows all 3 viewpoints, with assorted arguments for each:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*u5Npy5K8FoKnOcvh0t3ZSw.png)\n\n**Tips for Anti\\-Confirmation Bias Searching:**\n\n* Try the three viewpoint approach: â€œSearch the web for overviews of \\[INSERT SUBJECT] and its pros and cons, from each of these 3 viewpoints: 1\\. Supporter 2\\. Critic 3\\. Neutral Viewâ€\n* Use neutral language to begin with. For example, instead of searching â€œwhy are added sugars less healthy than natural onesâ€ type â€œoverview of the differences and similarities in health effects of natural and added sugars.â€ SearchGPT is helpful here because it can aggregate the opinions of a variety of sources\n* Ensure that it uses multiple sources (as all sources have at least some bias, so this will hopefully flatten that out a bit)\n\n\n## Prompting and General Tips for SearchGPT\n\nNow that weâ€™ve gone through the deeper dive into the types of searching you can do with SearchGPT, here are some more general and prompt engineering tips I recommend you use. Many of these have been mentioned earlier, but this is a useful summary.\n\n\n### The Doâ€™s of SearchGPT\n\n* Request that it use many sources â€” â€œPlease examine a wide variety of sourcesâ€\n* Create a guided pathway with SearchGPTâ€” â€œList some potential follow\\-up questions for me after you respondâ€\n* Use specific dates in your requests â€” â€œAs of todayâ€, â€œas of 2024â€, or â€œin 1983â€\n* If SearchGPT doesnâ€™t come back with any sources, open a new chat and insert â€œsearch the webâ€ at the end of your prompt\n* Open a new chat after youâ€™ve exchanged more than 20 or so messages, you donâ€™t want its context window getting too full\n* Be verbose â€” the more context GPT has for your request, the better\n\n\n### The Don'ts of SearchGPT\n\n* Donâ€™t be vague (e.g., â€œnew housing crisis 2024â€ or â€œcool sports cars 2024â€). This can be fixed with this prompt: â€œask me some clarifying questions about my request that I will answer before you beginâ€\n* DO NOT take something as gospel without checking the source; hallucinations are still possible. You can always say â€œverify this information with another, different sourceâ€\n* Do not ask for a specific opinion; it will tell you what you want to hear. Please ensure your prompts are general and open\n* Donâ€™t be afraid to ask â€œtoo muchâ€ of SearchGPT. There are capabilities it has that I havenâ€™t even discovered yet, and asking a lot of it is precisely how to uncover them\n\nThatâ€™s all, folks. I hope this convinces you to at least try integrating SearchGPT into your workflow. Itâ€™s transformed my day\\-to\\-day in many ways and I bet it will for you too.\n\nThanks for reading!\n\n\\-Jordan\n\n\n"},{"lang":"en","group":"blog","slug":"blog/smollm2-very-good-alternatives-to-qwen2-5-and-llama-3-2-463a200d2f3b","frontmatter":{"title":"SmolLM2: Very Good Alternatives to Qwen2.5 and Llama 3.2","meta_title":"SmolLM2: Very Good Alternatives to Qwen2.5 and Llama 3.2","description":"And it's fully open!","date":"2024-11-10T03:51:17.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Y3_lsNsFKybrOi14.png","categories":["Technology","Machine Learning","Data Science"],"author":"Rifx.Online","tags":["SmolLM2","parameters","pre-training","MobileLLM","reproducibility"],"draft":false,"slug":"blog/smollm2-very-good-alternatives-to-qwen2-5-and-llama-3-2-463a200d2f3b"},"content":"\n\n\n\n\n## And it's fully open!\n\nHugging Face has doubled down on their SmolLM initiative.\n\nThey released SmolLM2: 1\\.7B, 360M, and 135M models trained on 11T tokens (against 1T for SmolLM). They released based and instruct versions:\n\n* Hugging Face Collection: [SmolLM2](https://huggingface.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9) (Apache 2\\.0 license)\n\nThey used new datasets for pre\\-training that they will release soon. To make the instruct versions, they used a recipe similar to what they did to train Zephyr (SFT\\+DPO on ultrafeedback).\n\nIt looks like SmolLM2 performs very well:\n\n\n\nNote that Hugging Face fully releases the pre\\-training data and the recipe they used to prevent data contamination. In other words, their published evaluation results are probably accurate and fully reproducible.\n\nHugging Face used its own framework for pre\\-training, [Nanotron](https://github.com/huggingface/nanotron). Iâ€™ve never written about Nanotron but I think itâ€™s a very interesting project that deserves to be better known, especially if you are interested in understanding how pre\\-training is done. Iâ€™ll try to find the time to publish an article explaining Nanotron before 2025!\n\nMeta also released a series of small models, MobileLLM:\n\n* Hugging Face Collection: [MobileLLM](https://huggingface.co/collections/facebook/mobilellm-6722be18cb86c20ebe113e95) (CC\\-BY\\-NC)\n\nThis is a new release but note that these models are actually quite old. They were trained for this work published in February 2024:\n\n[MobileLLM: Optimizing Sub\\-billion Parameter Language Models for On\\-Device Use Cases](https://arxiv.org/abs/2402.14905)\n\nLearn everything you need about using and fine\\-tuning Large Language Models with my new book â€œLLMs on a Budgetâ€:\n\n\n"},{"lang":"en","group":"blog","slug":"blog/the-6-best-llm-tools-to-run-models-locally-eedd0f7c2bbd","frontmatter":{"title":"The 6 Best LLM Tools To Run Models Locally","meta_title":"The 6 Best LLM Tools To Run Models Locally","description":"Running large language models (LLMs) like ChatGPT and Claude usually involves sending data to servers managed by OpenAI and other AI modelâ€¦","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*2MB6-INUUGLR0NR_iOACIg.jpeg","categories":["Technology","Programming","Health"],"author":"Rifx.Online","tags":["LLM","local","deployment","customization","telehealth"],"draft":false,"slug":"blog/the-6-best-llm-tools-to-run-models-locally-eedd0f7c2bbd"},"content":"\n\n\n\n\n\nRunning large language models (LLMs) like [ChatGPT](https://openai.com/chatgpt/mac/) and [Claude](https://claude.ai/) usually involves sending data to servers managed by [OpenAI](https://openai.com/) and other AI model providers. While these services are secure, some businesses prefer to keep their data entirely offline for greater privacy.\n\nThis article covers the top six tools developers can use to run and test LLMs locally, ensuring their data never leaves their devices, similar to how [end-to-end encryption](https://getstream.io/blog/end-to-end-encryption/) protects privacy.\n\n\n## Why Use Local LLMs?\n\nA tool like [LM Studio](https://lmstudio.ai/) does not collect user data or track usersâ€™ actions when they use it to run local LLMs. It lets all your chat data stay on your local machine without sharing with an AI/ML server.\n\n* **Privacy**: You can prompt local LLMs in a multi-turn manner without your prompt data leaving your localhost.\n* **Customization Options**: Local LLMs provide advanced configurations for CPU threads, temperature, context length, GPU settings, and more. This is similar to OpenAIâ€™s playground.\n* **Support and Security**: They provide similar support and security as OpenAI or Claude.\n* **Subscription and Cost**: These tools are free to use and they do not require monthly subscription. For cloud services like OpenAI, each API request requires payment. Local LLMs help to save money since there are no monthly subscriptions.\n* **Offline Support**: You can load and connect with large language models while offline.\n* **Connectivity**: Sometimes, connecting to a cloud service like OpenAI may result in poor signal and connection.\n\n\n## Top Six and Free Local LLM Tools\n\nDepending on your specific use case, there are several offline LLM applications you can choose. Some of these tools are completely free for personal and commercial use. Others may require sending them a request for business use. There are several local LLM tools available for Mac, Windows, and Linux. The following are the six best tools you can pick from.\n\n\n## 1. LM Studio\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*svbQPZKu08of7Kv6)\n\n[LM Studio](https://lmstudio.ai/) can run any model file with the format `gguf`. It supports `gguf` files from model providers such as [Llama 3.1](https://llama.meta.com/), [Phi 3](https://huggingface.co/docs/transformers/main/en/model_doc/phi3), [Mistral](https://mistral.ai/), and [Gemma](https://ai.google.dev/gemma). To use LM Studio, visit the link above and download the app for your machine. Once you launch LM Studio, the homepage presents top LLMs to download and test. There is also a search bar to filter and download specific models from different AI providers.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*sbS3VqiLgDsftgs2)\n\nSearching for a model from a specific company presents several models, ranging from small to large [quantization](https://huggingface.co/docs/optimum/en/concept_guides/quantization). Depending on your machine, LM Studio uses a compatibility guess to highlight the model that will work on that machine or platform.\n\n\n## Key Features of LM Studio\n\nLM Studio provides similar functionalities and features as ChatGPT. It has several functions. The following highlights the key features of LM Studio.\n\n* **Model Parameters Customization**: This allows you to adjust temperature, maximum tokens, frequency penalty, and more.\n* **Chat History**: Allows you to save prompts for later use.\n Parameters and UI Hinting: You can hover on info buttons to lookup model parameters and terms.\n* **Cross-platform**: LM Studio is available on Linux, Mac, and Windows operating systems.\n* **Machine Specification Check**: LM studio checks computer specifications like GPU and memory and reports on compatible models. This prevents downloading a model that might not work on a specific machine.\n* **AI Chat and Playground**: Chat with a large language model in a multi-turn chat format and experiment with multiple LLMs by loading them concurrently.\n* **Local Inference Server for Developers**: Allows developers to set up a local HTTP server similar to OpenAIâ€™s API.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*9bHmRiOSf6gm-u3P)\n\nThe local server provides sample Curl and Python client requests. This feature helps to build an AI application using LM Studio to access a particular LLM.\n\n\n```python\n## Example: reuse your existing OpenAI setup\nfrom openai import OpenAI\n\n## Point to the local server\nclient = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n\ncompletion = client.chat.completions.create(\n  model=\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"Always answer in rhymes.\"},\n    {\"role\": \"user\", \"content\": \"Introduce yourself.\"}\n  ],\n  temperature=0.7,\n)\n\nprint(completion.choices[0].message)\n```\nWith the above sample Python code, you can reuse an existing OpenAI configuration and modify the base url to point to your localhost.\n\n* **OpenAIâ€™s Python Library Import**: LM Studio allows developers to import the OpenAI Python library and point the base URL to a local server (localhost).\n* **Multi-model Session**: Use a single prompt and select multiple models to evaluate.\n\n\n## Benefits of Using LM Studio\n\nThis tool is free for personal use and it allows developers to run LLMs through an in-app chat UI and playground. It provides a gorgeous and easy to use interface with filters and supports connecting to OpenAIâ€™s Python library without the need for an API key. Companies and businesses can use LM studio on request. However it requires a M1/M2/M3 Mac or higher, or a Windows PC with a processor that supports [AVX2](https://edc.intel.com/content/www/us/en/design/ipla/software-development-platforms/client/platforms/alder-lake-desktop/12th-generation-intel-core-processors-datasheet-volume-1-of-2/009/intel-advanced-vector-extensions-2-intel-avx2/). Intel and [AMD](https://www.amd.com/en/support/download/drivers.html) users are limited to using the [Vulkan inference engine in v0.2.31](https://lmstudio.ai/).\n\n\n## 2. Jan\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*7YeH_48iFYB4lDRu)\n\nThink of [Jan](https://jan.ai/) as an open-source version of ChatGPT designed to operate offline. It is built by a community of users with a user-owned philosophy. Jan allows you to run popular models like [Mistral](https://huggingface.co/models?other=mistral) or [Llama](https://huggingface.co/models?other=llama) on your device without connecting it to the internet. With Jan, you can access remote APIs like OpenAI and [Groq](https://groq.com/).\n\n\n## Key Features of Jan\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*ufyOE6QkcHw8X5U7)\n\nJan is an electron app with features similar to LM Studio. It makes AI open and accessible to all by turning consumer machines into AI computers. Since it is an open source project, developers can contribute to it and extend its functionalities. The following breaksdown the major features of Jan.\n\n* **Local**: You can run your preferred AI models on devices without connecting them to the internet.\n* **Ready to Use Models**: After downloading Jan, you get a set of already installed models to start. There is also a possibility to search for specific models.\n* **Model Import**: It supports importing models from sources like Hugging Face.\n* **Free, Cross-Platform and Open Source**: Jan is 100% free, open source, and works on Mac, Windows, and Linux.\n* **Customize Inference Parameters**: Adjust model parameters such as Maximum token, temperature, stream, frequency penalty, and more. All preferences, model usage, and settings stay locally on your computer.\n* **Extensions**: Jan supports extensions like [TensortRT](https://github.com/NVIDIA/TensorRT) and [Inference Nitro](https://huggingface.co/jan-hq/nitro-v1.2-e3) for customizing and enhancing your AI models.\n\n\n## Benefits of Using Jan\n\nJan provides a clean and simple interface to interact with LLMs and it keeps all your data and processing information locally. It has over seventy large language models already installed for you to use. The availability of these ready-to-use models makes it easy to connect and interact with remote APIs like OpenAI and Mistral. Jan also has a great [GitHub](https://github.com/janhq/jan), [Discord](https://discord.gg/FTk2MvZwJH), and [Hugging Face](https://huggingface.co/janhq) communities to follow and ask for help. However, like all the LLM tools, the models work faster on Apple Silicon Macs than on Intel ones.\n\n\n## 3. Llamafile\n\n[Llamafile](https://github.com/Mozilla-Ocho/llamafile) is backed by [Mozilla](https://www.mozilla.org/en-US/?v=1) whose aim is to support and make open source AI accessible to everyone using a fast [CPU inference](https://huggingface.co/docs/transformers/en/perf_infer_cpu) with no network access. It converts LLMs into multi-platform [Executable Linkable Format](https://gist.github.com/x0nu11byt3/bcb35c3de461e5fb66173071a2379779) (ELF). It provides one of the best options to [integrate AI](https://getstream.io/chat/solutions/ai-integration/) into applications by allowing you to run LLMs with just a single executable file.\n\n\n## How Llamafile Works\n\nIt is designed to convert weights into several executable programs that require no installation to run on architectures such as Windows, MacOS, Linux, Intel, ARM, FreeBSD, and more. Under the hood, Llamafile uses [tinyBLAST](https://github.com/ggerganov/llama.cpp/issues/5048) to run on OSs like Windows without requiring an SDK.\n\n\n## Key Features of Llamafile\n\n* **Executable File**: Unlike other LLM tools like LM Studio and Jan, Llamafile requires only one executable file to run LLMs.\n* **Use Existing Models**: Llamafile supports using existing models tools like Ollama and LM Studio.\n* **Access or Make Models**: You can access popular LLMs from OpenAI, Mistral, Groq, and more. It also provides support for creating models from scratch.\n* **Model File Conversion**: You can convert the file format of many popular LLMs, for example, `.gguf` into `.llamafile` with a single command.\n\n`llamafile-convert mistral-7b.gguf`\n\n\n## Get Started With Llamafile\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*4PV1KsCZvvVKqFll)\n\nTo install Llamafile, head to the Huggingface website, select **Models** from the navigation, and search for **Llamafile**. You can also install your preferred [quantized](https://huggingface.co/docs/optimum/en/concept_guides/quantization) version from the URL below.\n\n[`https://huggingface.co/Mozilla/Meta-Llama-3.1-8B-Instruct-llamafile/tree/m`ain](https://huggingface.co/Mozilla/Meta-Llama-3.1-8B-Instruct-llamafile/tree/main)\n\n**Note**: The larger the quantization number, the better the response. As highlighted in the image above, this article uses `Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile` where `Q6` represents the quantization number.\n\n**Step 1: Download Llamafile**\n\nFrom the link above, click any of the download buttons to get your preferred version. If you have the [wget](https://www.gnu.org/software/wget/) utility installed on your machine, you can download Llamafile with the command below.\n\n`wget <https://huggingface.co/Mozilla/Meta-Llama-3.1-8B-Instruct-llamafile/blob/main/Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile>`\n\nYou should replace the URL with the version you like.\n\n**Step 2: Make Llamafile Executable**\n\nAfter downloading a particular version of Llamafile, you should make it executable using the following command by navigating to the fileâ€™s location.\n\n`chmod +x Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile`**Step 3: Run Llamafile**\n\nPrepend a period and forward slash `./` to the file name to launch Llamafile.\n\n`./Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile`\n\nThe Llamafile app will now be available at `http://127.0.0.1:8080` to run your various LLMs.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*1xrwDPTfNgmEQDTx)\n\n\n## Benefits of Using Llamafile\n\nLlamafile helps to democratize AI and ML by making LLMs easily reachable to consumer CPUs. As compared to other local LLM apps like **Llama.cpp**, Llamafile gives the fastest prompt processing experience and better performance on gaming computers. Since it has a faster performance, it is an excellent option for summarizing long text and large documents. It runs 100% offline and privately, so users do not share their data to any AI server or API. Machine Learning communities like Hugging Face supports the Llamafile format, making it easy to search for Llamafile related models. It also has a great open source community that develops and extends it further.\n\n\n## 4. GPT4ALL\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*j3vNWWQZCVF5woo5)\n\nGPT4ALL is built upon privacy, security, and no internet-required principles. Users can [install](https://www.nomic.ai/gpt4all) it on Mac, Windows, and Ubuntu. Compared to Jan or LM Studio, GPT4ALL has more monthly downloads, [GitHub Stars](https://github.com/nomic-ai/gpt4all), and active users.\n\n\n## Key Features of GPT4ALL\n\nGPT4All can run LLMs on major consumer hardware such as Mac M-Series chips, AMD and NVIDIA GPUs. The following are its key features.\n\n* **Privacy First**: Keep private and sensitive chat information and prompts only on your machine.\n* **No Internet Required**: It works completely offline.\n* **Models Exploration**: This feature allows developers to browse and download different kinds of LLMs to experiment with. You can select about 1000 open-source language models from popular options like LLama, Mistral, and more.\n* **Local Documents**: You can let your local LLM access your sensitive data with local documents like `.pdf` and `.txt` without data leaving your device and without a network.\n* **Customization options**: It provides several [chatbot](https://getstream.io/blog/llm-chatbot-docs/) adjustment options like temperature, batch size, context length, etc.\n* **Enterprise Edition**: GPT4ALL provides an enterprise package with security, support, and per-device licenses to bring local AI to businesses.\n\n\n## Get Started With GPT4All\n\nTo start using GPT4All to run LLMs locally, [Download](https://www.nomic.ai/gpt4all) the required version for your operating system.\n\n\n## Benefits of Using GPT4ALL\n\nWith the exception of Ollama, GPT4ALL has the most significant number of GitHub contributors and about 250000 monthly active users (according to <https://www.nomic.ai/gpt4all>) and compared to its competitors. The app collects anonymous user data about usage analytics and chat sharing. However, users have the options to opt in or out. Using GPT4ALL, developers benefit from its large user base, GitHub, and Discord communities.\n\n\n## 5. Ollama\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*STAonWgWIsY6cgDR)\n\nUsing [Ollama](https://ollama.com/), you can easily create local chatbots without connecting to an API like OpenAI. Since everything runs locally, you do not need to pay for any subscription or API calls.\n\n\n## Key Features of Ollama\n\n* **Model Customization**: Ollama allows you to convert `.gguf` model files and run them with `ollama run modelname`.\n* **Model Library**: Ollama has a large collection of models to try at [ollama.com/library](https://ollama.com/library).\n* **Import Models**: Ollama supports importing models from [PyTorch](https://pytorch.org/).\n* **Community Integrations**: Ollama integrates seamlessly into web and desktop applications like, [Ollama-SwiftUI](https://github.com/kghandour/Ollama-SwiftUI), [HTML UI](https://github.com/rtcfirefly/ollama-ui), [Dify.ai](https://github.com/rtcfirefly/ollama-ui), and [more](https://github.com/ollama/ollama?tab=readme-ov-file#community-integrations).\n* **Database Connection**: Ollama supports several [data platforms](https://github.com/mindsdb/mindsdb/blob/main/mindsdb/integrations/handlers/ollama_handler/README.md).\n* **Mobile Integration**: A SwiftUI app like [Enchanted](https://github.com/AugustDev/enchanted) brings Ollama to iOS, macOS, and visionOS. [Maid](https://github.com/Mobile-Artificial-Intelligence/maid) is also a cross-platform Flutter app that interfaces with `.gguf`model files locally.\n\n\n## Get Started With Ollama\n\nTo use Ollama for the first time, visit <https://ollama.com> and download the version for your machine. You can install it on Mac, Linux, or Windows. Once you install Ollama, you can check its detailed information in Terminal with the following command.\n\n`ollama`\n\nTo run a particular LLM, you should download it with:\n\n`ollama pull modelname`, where `modelname` is the name of the model you want to install. Checkout Ollama on [GitHub](https://github.com/ollama/ollama) for some example models to download. The `pull` command is also used for updating a model. Once it is used, only the difference will be fetched.\n\nAfter downloading for example, `llama3.1`, running `ollama run llama3.1` in the command line launches the model.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*aglZm6h0BU6GAYkSl04XWA.gif)\n\nIn the above example, we prompt the `llama3.1` model to solve a Physics work and energy question.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*dNNQYpz1s2tz1pcn)\n\n\n## Benefits of Using Ollama\n\nOllama has over 200 contributors on GitHub with active updates. It has the largest number of contributors and is more extendable among the other open-source LLM tools discussed above.\n\n\n## 6. LLaMa.cpp\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*KhsAUquhDZAHghxK)\n\n[LLaMa.cpp](https://github.com/ggerganov/llama.cpp) is the underlying backend technology (inference engine) that powers local LLM tools like Ollama and many others. Llama.cpp supports significant large language model inferences with minimal configuration and excellent local performance on various hardware. It can also run in the cloud.\n\n\n## Key Features of LLaMa.cpp\n\n* **Setup**: It has a minimal setup. You install it with a single command.\n* **Performance**: It performs very well on various hardware locally and in the cloud.\n* **Supported Models**: It supports popular and major LLMs like [Mistral 7B](https://huggingface.co/mistralai/Mistral-7B-v0.1), [Mixtral MoE](https://huggingface.co/models?search=mistral-ai/Mixtral), [DBRX](https://huggingface.co/databricks/dbrx-instruct), [Falcon](https://huggingface.co/models?search=tiiuae/falcon), and [many others](https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#description).\n* **Frontend AI Tools**: LLaMa.cpp supports open-source LLM UI tools like [MindWorkAI/AI-Studio](https://github.com/MindWorkAI/AI-Studio) (FSL-1.1-MIT), [iohub/collama](https://github.com/iohub/coLLaMA), etc.\n\n\n## Get Started With LLaMa.cpp\n\nTo run your first local large language model with llama.cpp, you should install it with:\n\n`brew install llama.cpp`\n\nNext, download the model you want to run from Hugging Face or any other source. For example, download the model below from Hugging Face and save it somewhere on your machine.\n\n[`https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q4_K_M.g`guf](https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf)\n\nUsing your preferred command line tool like Terminal, `cd` into the location of the `.gguf` model file you just downloaded and run the following commands.\n\n\n```python\nllama-cli --color \\ \n-m Mistral-7B-Instruct-v0.3.Q4_K_M.ggufb \\ \n-p \"Write a short intro about SwiftUI\"\n```\nIn summary, you first invoke the LLaMa CLI tool and set color and other flags. The `-m` flag specifies the path of the model you want to use. The `-p` flag specifies the prompt you wish to use to instruct the model.\n\nAfter running the above command, you will see the result in the following preview.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*4Al-j50vXUXLUfvxzBt6aw.gif)\n\n\n## Local LLMs Use Cases\n\nRunning LLMs locally can help developers who want to understand their performance and how they work in detail. Local LLMs can query private documents and technical papers so that information about these documents does not leave the devices used to query them to any cloud AI APIs. Local LLMs are useful in no-internet locations and places where network reception is poor.\n\nIn a [telehealth setting](https://getstream.io/blog/telemedicine-app-development/), local LLMs can sort patient documents without having to upload them to any AI API provider due to privacy concerns.\n\n\n## Evaluating LLMsâ€™ Performance To Run Locally\n\nKnowing the performance of a large language model before using it locally is essential for getting the required responses. There are several ways you can determine the performance of a particular LLM. Here are a few ways.\n\n* **Training**: What dataset is the model trained on?\n* **Fine-tuning**: To what extent can the model be customized to perform a specialized task or can it be fine-tuned to for a specific domain?.\n* **Academic Research**: Does the LLM have an academic research paper?\n\nTo answer the above questions, you can check excellent resources like [Hugging Face](https://huggingface.co/datasets) and [Arxiv.org](https://arxiv.org/). Also, [Open LLm Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) and [LMSYS Chatbot Arena](https://chat.lmsys.org/?arena) provide detailed information and benchmarks for varieties of LLMs.\n\n\n## Local LLM Tools Conclusion\n\nAs discussed in this article, several motives exist for choosing and using large language models locally. You can fine-tune a model to perform a specialized task in a [telemedicine app](https://getstream.io/chat/solutions/healthcare/) if you do not wish to send your dataset over the internet to an AI API provider. Many open-source Graphic User Interface (GUI-based) local LLM tools like LLm Studio and Jan provide intuitive front-end UIs for configuring and experimenting with LLMs without subscription-based services like OpenAI or Claude. You also discovered the various powerful command-line LLM applications like Ollama and LLaMa.cpp that help you run and test models locally and without an internet connection. Check out Streamâ€™s [AI Chatbot](https://getstream.io/chat/solutions/ai-integration/) solution to integrate an AI chat into your app and visit all the related links to learn more.\n\n*Originally published at [https://getstream.io](https://getstream.io/blog/best-local-llm-tools/).*\n\n\n"},{"lang":"en","group":"blog","slug":"blog/the-best-conversational-ai-virtual-health-care-assistants-for-aging-adults-fc65bcfb9cf4","frontmatter":{"title":"The Best Conversational AI Virtual Health Care Assistant for Aging Adults","meta_title":"The Best Conversational AI Virtual Health Care Assistant for Aging Adults","description":"Virtual Health Care Assistants, such as MiiHealth.ai, are transforming healthcare for aging adults, particularly those living alone. These AI-driven companions assist with medication management, provide 24/7 support, monitor health conditions, and offer personalized fitness advice. They also help alleviate loneliness by engaging users in conversation. MiiHealth.ai stands out for its user-friendly interface, tailored services, and emotional support, making it an essential tool for promoting independence and well-being among seniors.","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*PBuPT38hZv61TFXQzJZ_3w.jpeg","categories":["Health","Chatbots","Autonomous Systems"],"author":"Rifx.Online","tags":["Virtual","Healthcare","Assistants","Medication","Monitoring"],"draft":false,"slug":"blog/the-best-conversational-ai-virtual-health-care-assistants-for-aging-adults-fc65bcfb9cf4"},"content":"\n\n\n\nLooking after our health becomes even more difficult as we grow older, and this is even worse for seniors who live alone. Although managing medications, doctors visits, and overall health becomes a challenge for many ageing Americans. But how would you like if you had a companion â€” a person who would help you take pills on time, call you if you felt lonely, and even ask how you were? This is the magic of Virtual Health Care Assistants, and theyâ€™re revolutionizing health care for aging people.\n\nCompanies like [**MiiHealth.ai**](https://miihealth.ai/) are at the forefront of creating the intelligent, AI driven companions that help seniors with their health issues and connecting them with their loved ones. Now letâ€™s find out how these Virtual Health Care Assistants are changing the lives of the elderly persons who are living alone and why they are thus fast becoming indispensable when it comes to caregiving for the elderly.\n\n\n## What Exactly is a Virtual Health Care Assistant?\n\nVirtual Health Care Assistant is a conversational agent that is specifically intended to assist people to deal with their health care issues. These assistants can interact with users and provide health information, pillâ€™s schedules and provide comfort when the patient is overwhelmed. They can do things such as checking a patientâ€™s blood pressure, recommended exercises or even just chat with the patient, all via voice control or SMS.\n\n\n\nTo anybody facing the difficult task of aging single, these assistants are not only high\\-tech gadgets but real life partners when it comes to health and well being any time of the day or night. For a Senior needing to receive a short note or a SOS call, a [**Virtual Health Care Assistant**](https://miihealth.ai/) can be helpful.\n\n\n## Why Virtual Health Care Assistants Are a Game\\-Changer for Aging Adults\n\nBeing single and enjoying ones seniority comes with especial difficulties that seniors face especially in USA. From handling intricate health schedules to coping with loneliness, the familyâ€™s assistance requirement is higher than before. A Virtual Health Care Assistant can provide that support in several impactful ways:\n\n**1\\. Medications Organized and Track**\n\nPeople with ER, as with any other chronic illness, require effective management of medications for optimal results. Not taking a dose at the right time or taking a wrong medication may lead to terrible results. As a Health Care Assistant in virtual Delivery, one can be able to remind the seniors to take their medications, hence they stick to the required schedules. They are as follows: generic ones that can be adjusted based on the personâ€™s schedule and dosage of specific medication.\n\nBesides reminders, these Assistant AI can assist in observing the side effects of taken medications and attendance of the symptoms which can ease the caring for elderly people and their relatives. For instance, when a senior is taking antihypertensive or diabetic pills, the assistant can quiz them concerning their health and recommend the time to monitor blood pressure.\n\n**2\\. 24/7 Availability \\& Support**\n\nOne of the traits that many people love about a Virtual Health Care Assistant is that it is always around: day or night. This is particularly the case for seniors who might feel lonely or anxious as well as during unpleasant hours when core caregivers are perhaps asleep. From explaining a recent change to providing a conversation with when there is nothing else to talk about these assistants are there to assist.\n\nFinancial dependency is especially significant for elderly citizens who have no one or nothing else they can turn to at specific hour of each day. Besides, these assistants can answer a number of questions, addressing even health issues, simple or complicated, without the need to book an appointment or get an assistantâ€™s help.\n\n**3\\. Emergency Alerts Health Monitoring**\n\nVirtual Health Care Assistants are also advantageous in that they can keep track of a seniorâ€™s condition at all times and react to a crisis. For instance, most of the AI assistants can incorporate with tracking devices such as wristbands or watches monitor pulse rates, blood pressure, and even falls. It will also enable an assistant to communicate with other people if the senior falls or develops the signs of an illness.\n\nIt is especially important for older adults who may not be able to get assistance at the times closest to or right during the time of the emergency. Whether one stumbles and falls, if there is an abnormal health check or one feels uncomfortable, the assistant is there as extra security.\n\n**4\\. Exclusive Health Information \\& Fitness Advices**\n\nHealth requirements for any two people are unique, and a Virtual Health Care Assistant can offer assistance suited to these requirements. To seniors it could mean suggesting which exercises are not very strenuous on the joints or suggesting what sort of foods would be good for a certain disease such as diabetes or heart ailment.\n\nUnlike regular programs that need constant reinstallation, these assistants are programmed to master from the user thus making them smarter as time progresses. They remain flexible to achieve what a senior wants, monitor his/her improvement and even provide advice concerning the personâ€™s condition.\n\n**5\\. The major way of alleviating loneliness and enhancing mental health.**\n\nMay also be involve loneliness that is prevalent in the older adults and its impacts it on individualsâ€™ health. A Virtual Health Care Assistant can be designed to also offer some form of companionship to seniors, when lonely they can engage the Virtual Health Assistant in a conversation. Thus, the assistant is not a replacement for social interaction with others, though the tool can respond to simple messages, tell the jokes, and encourage the user.\n\nCertain assistants are built to help seniors build meaningful interactions with friends and family, or participate in virtual events. First, AI tools provide social companionship; seniors need to feel that they are accepted and loved in order to be healthy.\n\n\n## Why MiiHealth.ai is a Top Choice for Seniors\n\nWhen considering employing a Virtual Health Care Assistant, then MiiHealth.ai is easy to use, technology\\-driven and can handle senior clientsâ€™ needs. Here are some reasons why MiiHealth.ai is a top choice for older adults:\n\n* **Simple \\& Intuitive Interface**: It is noteworthy that the creation of MiiHealth software is friendly for a user. The elders are also not required to possess tremendous technical know\\-how so as to operate the platform. This means that the voice command system affords it easy usage even to a person with low technological literacy.\n* **Tailored to Seniorsâ€™ Needs:** MiiHealth.ai differs from other organizations in that it offers services regarding the health of the aging population with the understanding that its services must be individualized. It is flexible for all its users; it can notify one about the time to take a medicine, and it could inform another about the right time to exercise.\n* **Emotional Support:** Besides, meeting the physical needs, the MiiHealth.ai has an emotional and social component to alleviate the feelings of loneliness that many seniors, who live alone, experience.\n* **Comprehensive Health Monitoring:** Connecting to wearable health devices, MiiHealth.ai regularly controls a seniorâ€™s vital indicators and notifies when necessary.\n\n\n## Conclusion\n\nSimilarly for elderly Americans, being single does not necessarily mean lacking the care to sustain a healthy and happy life or deal with an illness. In that regards Virtual Health Care Assistants such us MiiHealth.ai provide people with highly effective tool that allows accessing the best of AI technologies as well as highly individualized healthcare support.\n\nIn terms of administering medications and issuing emergency alerts through to company and providing conversation and advice on well\\-being, these assistants are changing the face of health care for the elderly. Whether for education, support or companionship, Virtual Health Care Assistants bring the care to older adults as they ensure they remain independent, connected and well.\n\nIf you or someone in your family needs reliable and caring personal assistant for health related issues you should familiarize yourself with [MiiHealth.ai](http://miihealth.ai). It is a ready way towards a better and fulfilled life for seniors who reside alone.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/the-focus-is-shifting-from-ai-agents-to-ai-agent-tool-use-a84fc061eec8","frontmatter":{"title":"The Focus Is Shifting From AI Agents To AI Agent Tool Use","meta_title":"The Focus Is Shifting From AI Agents To AI Agent Tool Use","description":"The article discusses the evolving focus in AI development from creating autonomous AI agents to enhancing the tools they utilize. Key advancements include AI agents like OpenAIs upcoming Operator, which will perform tasks on users computers through GUI navigation. Anthropic has also released a reference implementation for AI agents that interact with virtual environments, showcasing tools for GUI interactions, command-line operations, and file manipulation. These developments emphasize the importance of tool access in augmenting the capabilities of AI agents.","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*7IELtMakzcc68bdb4usXBQ.png","categories":["Programming","Technology","Autonomous Systems"],"author":"Rifx.Online","tags":["Operator","GUI","Navigation","Command","File"],"draft":false,"slug":"blog/the-focus-is-shifting-from-ai-agents-to-ai-agent-tool-use-a84fc061eec8"},"content":"\n\n\n\n\n\n\n### The focus regarding AI Agents is shifting from simply developing autonomous AI Agents to enhancing the tools available to them, which directly affects their power and flexibility.\n\nThe functionality and reach of AI Agents depend heavily on tool access, with tools described in natural language and activated through the agentâ€™s internal reasoning.\n\nDesktops and other user\\-specific environments offer the rich context that agents need to perform tasks effectively, making them ideal operational spaces.\n\n\n## âœ¨âœ¨ Follow me on LinkedIn âœ¨âœ¨\n\n\n## Introduction\n\nAs models become utilities, tool\\-enabled frameworks and environments are emerging as key, with leading AI companies like OpenAI and Anthropic exploring AI Agents that use computer GUI navigation to accomplish complex tasks.\n\nAlso recently announced, OpenAI is gearing up to release an **AI Agent**, *Operator*, which will perform tasks autonomously on a userâ€™s computer, like coding and booking travel, available as a research preview in January.\n\nThis release aligns with an industry\\-wide shift toward more capable **Agentic Tools** that manage multi\\-step workflows with minimal oversight.\n\nOther major players are also launching agent tools capable of real\\-time computer navigation, reflecting a strategic move to enhance AI Agent capabilities through tool access rather than simply improving model power.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*q7YvQLqfVdhV3bZM2oflDQ.png)\n\n\n## Anthropic Computer Use\n\nAnthropic has made available a [reference implementation](https://github.com/anthropics/anthropic-quickstarts/tree/main/computer-use-demo) that includes everything you will need to get started quickly with computer use.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*vD4T4Bo2-JcH535TOc46BQ.png)\n\nThe image above shows the AI Agent running on my desktop, I had to install Docker in my MacBook and deploy the docker image onto my machine.\n\nThe script shown below is all you need to deploy the instance and have it up and running.\n\n\n```python\nexport ANTHROPIC_API_KEY=%your_api_key%\ndocker run \\\n    -e ANTHROPIC_API_KEY=<Your Anthropic API Key Goes Here> \\\n    -v $HOME/.anthropic:/home/computeruse/.anthropic \\\n    -p 5900:5900 \\\n    -p 8501:8501 \\\n    -p 6080:6080 \\\n    -p 8080:8080 \\\n    -it ghcr.io/anthropics/anthropic-quickstarts:computer-use-demo-latest\n```\nBelow is a screenshot of the terminal window from where I run the fileâ€¦\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*mTu4gGEwnFbQYqJ-YGYqIA.png)\n\nThe implementation consists of:\n\n* A [containerised environment](https://github.com/anthropics/anthropic-quickstarts/blob/main/computer-use-demo/Dockerfile) suitable for computer use with Claude\n* Implementations of [the computer use tools](https://github.com/anthropics/anthropic-quickstarts/tree/main/computer-use-demo/computer_use_demo/tools)\n* An [agent loop](https://github.com/anthropics/anthropic-quickstarts/blob/main/computer-use-demo/computer_use_demo/loop.py) that interacts with the Anthropic API and executes the computer use tools\n* A web interface to interact with the container, agent loop, and tools.\n\n\n## Anthropic AI Agent Detail\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*euT2ZTmjVV5cTK-j8i4fgg.png)\n\nThe Anthropic **AI Agent** has access to three main **tools/functions** that allow me to interact with the Ubuntu virtual machine environment:\n\n\n### computer function:\n\n* This is the primary interface to interact with the GUI environment\n* Allows the AI Agent to perform mouse and keyboard actions like:\n* Moving the cursor (`mouse_move`)\n* Clicking (`left_click`, `right_click`, `middle_click`, `double_click`)\n* Typing text (`type`)\n* Pressing keyboard combinations (`key`)\n* Taking screenshots (`screenshot`)\n* The display resolution is set to 1024x768\n* Display number is :1\n* The AI Agent needs to check coordinates via screenshots before clicking elements\n\n\n### bash function:\n\n* Gives AI Agent access to a bash shell to run commands\n* State persists across commands\n* Can install packages via apt and pip\n* Can run background processes\n* For GUI applications, needs DISPLAY\\=:1 environment variable set\n\n\n### str\\_replace\\_editor function:\n\n* File manipulation tool that allows:\n* Viewing files and directories (`view`)\n* Creating new files (`create`)\n* Replacing text in files (`str_replace`)\n* Inserting text at specific lines (`insert`)\n* Undoing edits (`undo_edit`)\n* Maintains state across operations\n\n\n## Important Constraints\n\n* Cannot create accounts on social media/communication platforms\n* Cannot handle CAPTCHA/reCAPTCHA without user assistance\n* Cannot agree to Terms of Service without user direction\n* Cannot post comments/reactions on social media\n* Cannot access voter registration or election infrastructure data\n\nThe system is running on an aarch64 architecture Ubuntu VM, and I ran it via a Docker container on my laptop.\n\nThe tools provide the AI Agent with a controlled but flexible way to interact with the virtual environment, combining GUI interactions, command\\-line operations, and file manipulation capabilities.\n\nMy environment is freshly initialised for each session, but maintains state within a session across tool invocations.\n\nThe AI Agent can use the internet through Firefox and install additional software as needed through the package management system.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*4env1UkoKOZ-3zmF.png)\n\n[***Chief Evangelist***](https://www.linkedin.com/in/cobusgreyling/) ***@*** *[Kore.ai](https://blog.kore.ai/cobus-greyling) \\| Iâ€™m passionate about exploring the intersection of AI and language. From Language Models, AI Agents to Agentic Applications, Development Frameworks \\& Data\\-Centric Productivity Tools, I share insights and ideas on how these technologies are shaping the future.*\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*4env1UkoKOZ-3zmF.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*4env1UkoKOZ-3zmF.png)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/the-future-of-chatgpt-explained-everything-will-change-in-the-next-5-years-4bfd46ddca0b","frontmatter":{"title":"The Future of ChatGPT Explained: Everything Will Change in the Next 5 Years","meta_title":"The Future of ChatGPT Explained: Everything Will Change in the Next 5 Years","description":"OpenAI has outlined a five-step roadmap for the evolution of ChatGPT towards achieving Artificial General Intelligence (AGI). The stages include: 1) Chatbots, which focus on conversational AI; 2) Reasoners, capable of solving complex problems; 3) Agents, which can independently make decisions; 4) Innovators, collaborating with humans for innovation; and 5) Organizations, functioning as fully autonomous entities. Currently, OpenAI is between Levels 1 and 2, aiming to reach AGI within five years, with significant advancements expected by 2025.","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*nCVwPhWiFZ_0lglxT4pDAw.jpeg","categories":["Chatbots","Artificial General Intelligence","Reasoners"],"author":"Rifx.Online","tags":["Chatbots","Reasoners","Agents","Innovators","Organizations"],"draft":false,"slug":"blog/the-future-of-chatgpt-explained-everything-will-change-in-the-next-5-years-4bfd46ddca0b"},"content":"\n\n\n\n\n## This Could Take Artificial Intelligence Really, Really Farâ€¦\n\n\n\nOpenAI has laid out a **clear vision** for the evolution of ChatGPT, recently unveiling a **five\\-step roadmap** to reach what they call **Artificial General Intelligence** (AGI).\n\nAGI represents a theoretical AI system capable of **learning**, **understanding**, and **performing any intellectual task** at a human level, all autonomously and adaptively.\n\nItâ€™s a groundbreaking vision, but reaching this ambitious goal requires moving through **five critical stages**:\n\n\n## Level 1: Chatbots\n\nThe first level, where we currently find systems like ChatGPT, is centered on **conversational AI**. At this stage, AI can interact naturally with humans, handling various dialogues with impressive fluency and coherence.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ObzqCLMIIqpU9RK-TbTrfQ.png)\n\n\n## Level 2: Reasoners\n\nAt this next stage, AI systems will be able to **tackle complex problems** involving advanced math and logic. Weâ€™re already seeing glimpses of this with ChatGPTâ€™s latest version, â€œo1\\-preview,â€ marking a bridge between Levels 1 and 2, known as the â€œreasoningâ€ phase.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*B42LqC8ZDSaSYPX4yd3_Ng.png)\n\n\n## Level 3: Agents\n\nThis is the level where AI begins making decisions and executing tasks **independently**, without the need for human oversight. Imagine an AI that not only assists but initiates and completes tasks **on its own** â€” this is the dream for future iterations of ChatGPT.\n\n\n## Level 4: Innovators\n\nOnce AI reaches the â€œInnovatorsâ€ level, it will have the ability to drive and contribute to innovation **alongside humans actively**. At this point, AI wonâ€™t just follow human instructions; itâ€™ll collaborate and bring new ideas to the table.\n\n\n## Level 5: Organizations\n\nAt the final level, AI will have the capacity to operate as an entire organization â€” capable of performing tasks **as if it were a team of skilled humans**. This future version of AI would function like **a fully autonomous business entity**, handling everything from strategy to execution.\n\n\n## So, Where Are We Now?\n\nOpenAI currently sits between Levels 1 and 2\\. According to Sam Altman, CEO of OpenAI, the company aims to reach AGI within roughly five years.\n\nThe next major milestone will likely come with the arrival of â€œreasoners,â€ possibly as early as 2025 with the anticipated release of **GPT\\-5**, which is expected to achieve **the intellectual level of a PhD in numerous fields**.\n\nThanks for reading!\n\n**P.S.** Want a cool trick (like, magician\\-level) to simplify your prompt writing?\n\nIâ€™ve got exactly **what you need**! ðŸ‘‡\n\nNick\n\n\n"},{"lang":"en","group":"blog","slug":"blog/the-most-ambitious-ai-crypto-project-ever-is-here-ab3f6d85afd1","frontmatter":{"title":"The Most Ambitious AI Crypto Project Ever is Here","meta_title":"The Most Ambitious AI Crypto Project Ever is Here","description":"The article discusses an ambitious project by Near to train a decentralized open-source large language model (LLM) with 1.4 trillion parameters, significantly larger than existing models. This initiative aims to combine AI and blockchain technologies, allowing individuals to own and benefit from the AI model they help train. The project seeks to raise $160 million through crowdfunding and utilizes innovative distributed training methods to overcome challenges of synchronization and communication in a decentralized environment. The integration of blockchain is crucial for ensuring transparency and trust in the ownership and rewards associated with the models usage. However, concerns about the feasibility of such a large-scale model and the efficiency of blockchain technology remain.","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*uiBwsWl8grXKJCJaGtH7kw.png","categories":["Technology","Machine Learning","Blockchain"],"author":"Rifx.Online","tags":["blockchain","parameters","crowdfunding","synchronization","transparency"],"draft":false,"slug":"blog/the-most-ambitious-ai-crypto-project-ever-is-here-ab3f6d85afd1"},"content":"\n\n\n\n\n### AI \\& Blockchains: A Match Made in Heaven, or a Scam?\n\n\n\nOne of the founding fathers of modern AI wants to use the blockchain to train the world's largest open\\-source Large Language Model (LLM), almost four times larger than Llama 3\\.1 405B, generally considered the best open LLM.\n\nAnd before you dismiss this headline as scammy hype, please be aware that the person who stated this goal is none other than ***Illia Polosukhin***, one of the researchers behind the â€œAttention is All you Needâ€ paper, the seminal piece that led to our current AI revolution.\n\nSo, *what is exactly what they want to do? And whatâ€™s the role of the blockchain in all this?*\n\nRead on to learn how the worlds of AI and Crypto will irremediably merge and about the project that could finally make an AI model that is owned by the people.\n\n\n> You are probably sick of AI newsletters that simply explain what happened. Thatâ€™ easy and anyone can do it, which is why there are so many.\n\n\n> But explaining why it matters is another story. That requires knowledge, investigation, and deep thoughtâ€¦ all attributes of the people who engage weekly with **TheTechOasis**, the newsletter that aims to answer the most pressing questions in AI.\n\n\n> ðŸï¸ðŸï¸ Subscribe today below:\n\n\n## Owning AI\n\nWith Trumpâ€™s win, Crypto has entered an improvised bull cycle, [taking Bitcoin eerily close to the $100k per coin mark](https://coinmarketcap.com/currencies/bitcoin/) and reaching new all\\-time highs.\n\n\n### The Near Project\n\nUnder Trump, blockchain companies have good reason to be optimistic about the future. One of these companies is Near, which is trying to bridge the worlds of Crypto and AI.\n\nI wonâ€™t go into more detail about this project because I donâ€™t want you to feel Iâ€™m sponsoring it (I do not own NEAR coins). However, theyâ€™ve recently started a great, ambitious goal I deeply align with:\n\nTraining the largest open\\-source model ever, crowdfunded by individuals, and owned by them.\n\nSpecifically, **they want to train a 1\\.4 trillion\\-parameter model**, one that would rival the size of models like GPT\\-4 and be 3\\.5 times larger than the largest open\\-source (or dare I say, open\\-weight) LLM in the world, Metaâ€™s Llama 3\\.1 405B, which is also considered the best open model today.\n\nTo achieve this, **they expect to have to crowdfund a sizeable amount of $160 million**, funded through the acquisition of $NEAR coins, a cryptocurrency that, as of today, [has a market value of $6\\.6 billion](https://coinmarketcap.com/currencies/near-protocol/).\n\nHowever, the real issue isnâ€™t size, but the fact they want to train this model in a decentralized manner through poorly\\-communicated hardware. In laymanâ€™s terms, they donâ€™t intend to train this model in a [140 MegaWatt data center with 100,000 GPUs](https://readmedium.com/putting-the-worlds-largest-ai-supercomputer-into-perspective-60afde9bc653) like the one Elon Musk owns in Memphis, Tennessee, but training it on a global scale.\n\nFor someone familiar with how these models are trained, this is as ambitious as one can be in AI today.\n\n*But why?*\n\n\n### The Importance of Time\n\nYou may have heard the crazy numbers regarding AI training and inference, but these numbers are just a fraction of whatâ€™s coming.\n\n* [Elon Musk has 100,000 NVIDIA H100 GPUs](https://readmedium.com/putting-the-worlds-largest-ai-supercomputer-into-perspective-60afde9bc653) in one location and intends to double the compute power to 200,000 H100 equivalents in the next months.\n* All Hyperscalers (Microsoft, Amazon, Meta, Google, or Oracle) are reaching or have [reached deals with nuclear power plants](https://www.technologyreview.com/2024/09/26/1104516/three-mile-island-microsoft/) or Small Modular Reactor companies to build nuclear power generation to power their data centers and avoid the excessive lead times of transmission lines and electrical transformers\n* [A Hyperscaler pitched North Dakota Governor Doug Burgum about building a **5â€“10 GigaWatt data center**](https://thetechoasis.beehiiv.com/p/understanding-ai-s-big-picture). For reference, the latter data center would have more compute capacity than [Microsoftâ€™s entire Azure cloud (5 GW)](https://www.datacenterdynamics.com/en/news/microsoft-to-double-new-data-center-capacity-this-year-report/), and consume enough power to provide electricity to **8\\.3 million US homes at the average US household consumption value of 10,500 KWh/year**.\n\nAnd the list goes on. *But why?*\n\n**The reason is time**. To train a model, you send it data, force it to sample a prediction, and measure how good that prediction was. Based on that error signal, we then update the model's parameters so that the prediction error falls over time.\n\nThe issue with this process is two\\-fold:\n\n* The models are huge, meaning that each time we have to update parameters, we are updating potentially trillions of them.\n* The data sets are also huge, meaning the amount of parameter updates is unfathomably large.\n\nThis leads to training runs that take, if executed sequentially, forever. Luckily, as most frontier AI models today are basically matrix multiplications on steroids, a very similar mathematical computation to rendering pixels on a computer screen, the original goal of GPUs, we can use this hardware to train these models.\n\nCrucially, GPUs are meant to parallelize computation, meaning that we can parallelize the training of these models extensively ([although not fully due to Amdahlâ€™s Law](https://thetechoasis.beehiiv.com/p/understanding-ai-s-big-picture)).\n\nThis is why models like [Llama 3\\.1 405B were trained on a 24,000 GPU cluster](https://arxiv.org/pdf/2407.21783) and why models like the new Grok from xAI and [Llama 4 from Meta](https://www.tomshardware.com/tech-industry/artificial-intelligence/meta-is-using-more-than-100-000-nvidia-h100-ai-gpus-to-train-llama-4-mark-zuckerberg-says-that-llama-4-is-being-trained-on-a-cluster-bigger-than-anything-that-ive-seen) are being trained in 100,000\\-plus GPU clusters.\n\nOk, I get these models need a lot of GPUs working simultaneously to get trained. *But how do they do it?*\n\n\n### The essence of distributed training\n\nIn distributed training, instead of training one single model and updating it by sending all the data to that instance, we build replicas, identical versions of the model, each assigned to a given GPU pod (a pod is a group of tightly connected and collocated GPUs).\n\nThen, we batch the training set, and set the batches to the different pods. Of course, that means that each replica receives different training data and, thus, learns different things.\n\nConsequently, **every certain time the GPU pods need to synchronize**, sharing their learnings with the other pods, meaning that, after this synchronous stage, all model replicas have the exact same parameter values (as each model replica is actually updated with the average learning value, so that all model replicas learn the same after each batch training step).\n\nWhile all this seems well and good, this synchronization is a big issue because these synchronous updates mean all pods are basically stalled for the duration of the synchronization, **pulling us dangerously close to making the training run too long (these trainings take literally months)**.\n\nAnd to make matters worse, Near wants to do this in low\\-bandwidth form, meaning that the communication channels between the GPU pods will be slow.\n\nThus, *how can they do this, and what role will the blockchain play?* Luckily, we kind of know the answer to both in much more detail than what you would expect.\n\n\n## Toward Decentralized AI\n\nLuckily for Near, they arenâ€™t the only ones thinking about decentralized AI (although Near adds the blockchain; we see how theyâ€™ll do it later), as at the time of writing, **the worldâ€™s largest decentralized training run is taking place as you read this piece**.\n\n\n### The Prime Framework\n\nPrime Intellect is a company working toward the vision of training massive LLMs in a decentralized manner, aiming to train ***Intellect\\-1***, a 10\\-billion parameter model, in a fully decentralized way.\n\nIn other words, the training run is distributed across several GPUs, **which are owned by independent parties**, potentially separated across continents, and connected through low\\-bandwidth networks.\n\n\n> You can watch the progress and the different parties involved [through this app](https://app.primeintellect.ai/intelligence?utm_source=thetechoasis.beehiiv.com&utm_medium=newsletter&utm_campaign=should-ai-s-kill-openai-s-swarm-the-future-of-ai-training&_bhlid=8eadb6cf7d24b545a761f9ac3f7126a45ac2b579).\n\nThis gives us great insight into how Near will achieve its mission of training the largest open\\-source AI model ever.\n\nAs you may have guessed from the previous section, the main bottleneck in AI training is the synchronous update. According to Amdahlâ€™s Law, **parallelization can be an exercise of diminishing returns if a certain point in the training canâ€™t be parallelized**.\n\nTherefore, as you increase parallelization, the time\\-saving improvements become incremental, as we canâ€™t reduce the synchronization time.\n\n\n> In case youâ€™re wondering, synchronization canâ€™t be performed asynchronously (each pod updating its parameter values independently) as model convergence becomes impossible (at least for our current knowledge).\n\nKnowing this, Prime Intellect has put into practice several techniques that Near will surely capitalize on:\n\n* **Synchronization every hundreds of steps**.\n\nInstead of synchronizing the GPU pods in every parameter update, each pod carries its â€˜pseudo\\-gradientsâ€™ (accumulating its learnings across several local training time steps), and every 100 of these timesteps, it shares its learnings with the rest.\n\nSimply put, as learning sharing is the main bottleneck to training performance, we minimize the number of times GPU pods communicate.\n\n* **Quantization of communication payload**.\n\nThe number of times you communicate across pods isnâ€™t the only thing that affects time; the amount of shared information matters, too. Thus, we quantize the learnings so that information travels in a compressed form, making it faster.\n\nThis reduces communication requirements by 400x. In standard form, synchronization takes up to 40 minutes. With this quantization, it takes less than a minute.\n\n\n> **What is quantization?** In a nutshell, we take the information we want to store (or share, as in this case) and reduce the per\\-parameter precision (instead of â€˜1\\.023293,â€™ that number travels as a â€˜1â€™) of the optimizer states (the states that carry what each model replica is learning).\n\n\n> **Think of this is as compressing the data into a zip file before sending it so the size of the sent packet is smaller and, thus, faster to send.**\n\n\n> However, while the original numbers can be recovered (dequantization), it incurs some precision loss, which can affect performance. However, [Prime Intellect claims they did not appreciate any performance loss](https://www.primeintellect.ai/blog/intellect-1) despite the massive time savings.\n\n* **Dynamic global groups**\n\nOne of the biggest issues of decentralized model training is unreliability; both the network and, above all, the workers (GPUs) may break and fail. Also, **you want to incentivize this dynamism**, so that people can jointly enter the training run and log off when desired.\n\nFor this, the Prime framework has **dynamic global groups** that ensure workers can on\\-ramp and off\\-ramp without impacting the overall training run.\n\nFurthermore, the framework includes other techniques like asynchronous checkpointing that I wonâ€™t go into for the sake of length but that you can [read in full detail here](https://www.primeintellect.ai/blog/intellect-1).\n\nBut we still havenâ€™t answered the key question: *Where does the blockchain fit in all this?*\n\n\n## An Exciting Future Ahead\n\nOver the next four years, you will see blockchains in everything.\n\nYes, the *â€˜{Insert something that works just fine} but now itâ€™s decentralizedâ€™* type of slogans are coming back into our lives.\n\nWhile many of these new use cases will be pointless, blockchains do have a clear raison dâ€™Ãªtre that makes them very valuable when used when necessary, not for the sake of saying youâ€™re using a blockchain.\n\n\n### Itâ€™sâ€¦ a ledger\n\nBlockchains are decentralized ledgers. They store transaction information between two peers in blocks of transactions connected sequentially (hence the name).\n\n**This is a big deal because their decentralized nature makes this ledger almost impossible to tamper with**. True blockchains (which there arenâ€™t that many today that meet this standard) are immutable and definite, the unequivocal source of truth that a transaction took place at some point.\n\nImportantly, they are â€˜trustless,â€™ meaning that cryptography, not centralized entities like banks, guarantees the untampered nature of the ledger.\n\n\n> The reason why they are so hard to tamper with is, you guessed it, their decentralized nature. The global network of nodes that secure the blockchain all have an exact copy of the network, updated every time a new block is added.\n\n\n> Therefore, to introduce tampered transactions, you would need to own a majority amount of these nodes, be that through investing huge amounts of compute in proof\\-of\\-work blockchains like Bitcoin (extremely costly), hacking a majority of the nodes (again, extremely costly) or by owning majority stakes in the blockchainâ€™s cryptocurrency in proof\\-of\\-stake blockchains like Ethereum (again, extremely costly).\n\nLong story short, the value of blockchains is to make the act of tampering them a very, very bad idea economically speaking, one that is just simply not worth it.\n\nHence, their value is the idea that not only they are great sources of truth, providing trust to transaction making, but they are also exempt of centralized powers that may be incentivized to tamper them.\n\n*And how does that fit into AI?* Hereâ€™s where all comes full circle.\n\n\n### Owned AI Needs the Blockchain\n\nThe idea behind training a decentralized AI model is that the people who participate in training that model (be that with compute or with money) are rewarded for it.\n\nThus, the goal of this 1\\.4 trillion parameter model is that its inferences (the usage) will be paid back to its funders.\n\nAnd hereâ€™s where the blockchain comes in, as undeniable proof that *â€˜Jane Doe from Nebraskaâ€™* paid $1,000 to fund this training or for *â€˜John Doe from Japanâ€™* to prove they provided 100 hours of GPU compute into the training and, thus, both are rightful receptors of the benefits of that modelâ€™s inferences (every time the model runs, you get paid).\n\nNow, you may ask: Could a centralized entity manage this?\n\nFor sure, but the whole point of blockchains is to prevent the need for that central entity to exist in the first place and ensure that no one fully controls who owns what or how much you get paid.\n\nNow, all things considered, *is this vision actually possible today?*\n\n\n### Does Feasibility Meet Vision?\n\nItâ€™s very easy for anyone to align with Near's vision of AI, especially considering the people behind this project.\n\nEnvisioning a future where decentralized economies rise around AI to ensure people are paid for their data, content, compute, or expertise and get unequivocally and objectively rewarded for it is a vision anyone can empathize with.\n\nHowever, **a 1\\.4 trillion parameter model appears simply too large based on current standards**. As mentioned, *Intellect\\-1*, the largest known model being trained that way, **is 140 times smaller than what *Near* intends to build**.\n\nThe other concern is with the blockchain. For instance, one of the biggest lies regarding NFTs was that blockchains only stored that an NFT transaction took place, **but the NFT was stored â€˜off\\-chain.â€™** Sadly, the truth was that only the data that is stored in the blockchain is fully protected, so the actual piece of â€˜artâ€™ was largely unprotected and easily clonable.\n\nHowever, blockchains are notoriously inefficient, meaning that the fewer data you have to store â€˜on\\-chain,â€™ the better, making them very impractical.\n\nThus, if neither the model, nor the data, nor the compute used to train a model will be stored in the blockchain, *whatâ€™s the point?*\n\nLuckily, thereâ€™s a solution: zero\\-knowledge proofs, which I wonâ€™t go into today for the sake of length, may appear as the key enabler to guarantee that an event, even if it isnâ€™t stored on\\-chain, really happened.\n\nThrough zk\\-proofs, someone could prove that the compute they claim to have dedicated to the training actually happened or that they truly funded the training run, by storing a registry of that transaction with a zk\\-proof attached that proves with a very high certainty that something that happened off\\-chain actually happened.\n\nThus, by just storing the zk\\-proof, we can guarantee that even off\\-chain data can be trusted. *The issue?* Zk\\-proofs arenâ€™t ready yet due to their extensive compute requirements.\n\nHowever, one point still stands: If you truly believe AI can be decentralized, you have to trust blockchains are legit.\n\n*But how does this type of announcement make you feel? Are you excited about what the synergy between Crypto and AI has to offer?*\n\n*Or do you still think of a â€˜scamâ€™ whenever you see a blockchain mentioned?* If thatâ€™s the case, I donâ€™t blame you, but if you manage to abstract yourself from Cryptoâ€™s countless scams, youâ€™ll realize this technology will play a vital role in AI.\n\nAnd if Near is right, that will be sooner than expected.\n\n\n> **For business inquiries on AI strategy or analysis, reach out at nacho@thewhitebox.ai**\n\n\n> If you have enjoyed this article, I share similar thoughts in a more comprehensive and simplified manner for free on my [LinkedIn](https://www.linkedin.com/in/ignacio-de-gregorio-noblejas/).\n\n\n"},{"lang":"en","group":"blog","slug":"blog/the-quest-for-production-quality-graph-rag-easy-to-start-hard-to-finish-46ca404cee3d","frontmatter":{"title":"The Quest for Production-Quality Graph RAG: Easy to Start, Hard to Finish","meta_title":"The Quest for Production-Quality Graph RAG: Easy to Start, Hard to Finish","description":"Overcoming the challenges of productionizing graph RAG","date":"2024-11-01T03:56:04.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*RMudHNmBOgXM1Mubj1UTkw.jpeg","categories":["Programming","Data Science","Generative AI"],"author":"Rifx.Online","tags":["graph","RAG","production","uncertainty","optimization"],"draft":false,"slug":"blog/the-quest-for-production-quality-graph-rag-easy-to-start-hard-to-finish-46ca404cee3d"},"content":"\n\n\n\n\n### Overcoming the challenges of productionizing graph RAG\n\n\n\nWhen I read the recent article in VentureBeat about how Glean [just secured over $260 million in its latest funding round](https://venturebeat.com/data-infrastructure/how-to-take-advantage-of-a-generative-tool-fueling-gleans-260m-raise-graph-rag/), I had two immediate gut feelings. First, it was satisfying to see this very public example of graph RAG living up to its potential as a powerful, valuable technology that connects people with knowledge more efficiently than ever. Second, it felt surprising but validating to read:\n\n\n> One of the worldâ€™s largest ride\\-sharing companies experienced its benefits firsthand. After dedicating an entire team of engineers to develop a similar in\\-house solution, they ultimately decided to transition to Gleanâ€™s platform.\n\n\n> â€œWithin a month, they were seeing twice the usage on the Glean platform because the results were there,â€ says Matt Kixmoeller, CMO at Glean.\n\nAlthough I was surprised to read about the failure in a news article, struggling to bring graph RAG into production is what I would expect, based on my experience as well as the experiences of coworkers and customers. Iâ€™m not saying that I expect large tech companies to fail at building their own graph RAG system. **I merely expect that most folks will struggle to build out and productionize graph RAG â€” even if they already have a very successful proof\\-of\\-concept.**\n\nI wrote a [high\\-level reaction to the VentureBeat article in The New Stack](https://bit.ly/4fjIlgJ), and in this article, Iâ€™d like to dive deeper into why graph RAG can be so hard to get right. First, Iâ€™ll note how easy it has become, using the latest tools, to get started with graph RAG. Then, Iâ€™ll dig into some of the specific challenges of graph RAG that can make it so difficult to bring from R\\&D into production. Finally, Iâ€™ll share some tips on how to maximize your chances of success with graph RAG.\n\n\n## Getting started with graph RAG is easy\n\nSo if a big ride\\-sharing company couldnâ€™t build their own platform effectively, then why would I say that itâ€™s easy to implement graph RAG yourself?\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*l6EiwfjeUGLjVlYeiY1lqA.jpeg)\n\nWell, first of all, technologies supporting RAG and graph RAG have come a long way in the past year. Twelve months ago, most enterprises hadnâ€™t even heard of retrieval\\-augmented generation. Now, not only is RAG support [a key feature of the best AI\\-building tools like LangChain](https://python.langchain.com/docs/tutorials/rag/), but just about every major player in the AI space has a RAG tutorial, and [there is even a Coursera course](https://www.coursera.org/projects/introduction-to-rag). There is no shortage of quick entry points for trying RAG.\n\nMicrosoft may not have been the first to do graph RAG, but they gave the concept a big push with a [research blog post earlier this year](https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/), and they continue to work on related tech.\n\nHere on Medium, there is also a nice conceptual introduction, with some technical details, [from a gen AI engineer at Google](https://towardsdatascience.com/graph-rag-a-conceptual-introduction-41cd0d431375). And, in Towards Data Science, there is a recent and very thorough [how\\-to article on building a graph RAG system](https://towardsdatascience.com/how-to-implement-graph-rag-using-knowledge-graphs-and-vector-databases-60bb69a22759) and testing on a dataset of scientific publications.\n\nAn established name in traditional graph databases and analytics, Neo4j, added vector capabilities to their flagship graph DB product in response to the recent gen AI revolution, and they have an excellent platform of tools for projects that require sophisticated graph analytics and deep graph algorithms in addition to standard graph RAG capabilities. They also have a [Getting Started With Graph RAG](https://neo4j.com/developer-blog/graphrag-ecosystem-tools/) guide.\n\nOn the other hand, [you donâ€™t even need a graph DB to do graph RAG](https://bit.ly/3YD5NAd). Many folks who are new to graph RAG believe that they need to deploy a specialized graph DB, but this is not necessary, and in fact may simply complicate your tech stack.\n\nMy employer, DataStax, also has a [Guide to Graph RAG](https://bit.ly/4862Lrl).\n\nAnd, of course, the two most popular gen AI application composition frameworks, [LangChain](https://blog.langchain.dev/enhancing-rag-based-applications-accuracy-by-constructing-and-leveraging-knowledge-graphs/) and [LlamaIndex](https://docs.llamaindex.ai/en/stable/examples/cookbooks/GraphRAG_v1/), each have their own graph RAG introductions. And thereâ€™s [a DataCamp article](https://www.datacamp.com/tutorial/knowledge-graph-rag) that uses both.\n\nWith all of the tools and tutorials available, getting started with graph RAG is the easy partâ€¦\n\n\n## â€¦but bringing graph RAG into production is hard\n\nThis is a very old story in data science: a new software methodology, technology, or tool solves some imposing problem in a research context, but industry struggles to build it into products that deliver value on a daily basis. Itâ€™s not just an issue of effort and proficiency in software development â€” even the biggest, best, and brightest teams might not be able to overcome the uncertainty, unpredictability, and uncontrollability of real\\-world data involved in solving real\\-world problems.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*OklHNrhsNHZF6qzeRUSd_w.jpeg)\n\nUncertainty is an inherent part of building and using data\\-centric systems, which almost always have some elements of stochasticity, probability, or unbounded inputs. And, uncertainty can be even greater when inputs and outputs are unstructured, which is the case with natural language inputs and outputs of LLMs and other GenAI applications.\n\nFolks who want to try graph RAG typically already have an existing RAG application that performs well for simple use cases, but fails on some of the more complex use cases and prompts requiring multiple pieces of information across a knowledge base, potentially in different documents, contexts, formats, or even data stores. When all of the information needed to answer a question is in the knowledge base, but the RAG system isnâ€™t finding it, it seems like a failure. And from a user experience (UX) perspective, it is â€” the correct answer wasnâ€™t given.\n\nBut that doesnâ€™t necessarily mean there is a â€œproblemâ€ with the RAG system, which might be performing exactly as it was designed. If there isnâ€™t a problem or a bug, but we still arenâ€™t getting the responses we want, that must mean that we are expecting the RAG system to have a capability it simply doesnâ€™t have.\n\nBefore we look at why specifically graph RAG is hard to bring into production, letâ€™s take a look at the problem weâ€™re trying to solve.\n\n\n## The main challenge that graph RAG addresses\n\nBecause plain RAG systems (without knowledge graphs) retrieve documents based solely on vector search, only documents that are most semantically similar to the query can be retrieved. Documents that are not semantically similar at all â€” or not quite similar enough â€” are left out and are not generally made available to the LLM generating a response to the prompt at query time.\n\nWhen the documents we need to answer a question in a prompt are not all semantically similar to the prompt, one or more of them is often missed by a RAG system. This can happen when answering the question requires a mix of generalized and specialized documents or terms, and when documents are detail\\-dense in the sense that some very important details for this specific prompt are buried in the middle of related details that arenâ€™t as relevant to this prompt. See [this article for an example of RAG missing documents](https://bit.ly/3BKZAJv) because two related concepts (â€œSpace Needleâ€ and â€œLower Queen Anne neighborhoodâ€ in this case) are not semantically similar, and [see this article for an example of important details getting buried](https://bit.ly/4ffhrqi) in detail\\-dense documents because vector embeddings are â€œlossyâ€.\n\nWhen we see retrieval â€œfailingâ€ to find the right documents, it can be tempting to try to make vector search better or more tailored to our use case. But this would require fiddling with embeddings, and embeddings are complicated, messy, expensive to calculate, and even more expensive to fine\\-tune. Besides, that wouldnâ€™t even be the best way to solve the problem.\n\nFor example, looking at the example linked above, would we really want to use an embedding algorithm that puts the text â€œSpace Needleâ€ and â€œLower Queen Anne neighborhoodâ€ close together in semantic vector space? No, fine\\-tuning or finding an embedding algorithm that puts those two terms very close together in semantic space would likely have some unexpected and undesired side effects.\n\nIt is better not to try to force a semantic model to do a job that geographical or tourism information would be much better suited for. If I were a travel or tourism company who relied on knowing which neighborhood such landmarks are in, I would rather build a database that knows these things with certainty â€” a task that is much easier than making semantic vector search do the same taskâ€¦ without complete certainty.\n\nSo, the main issue here is that we have concepts and information that we know are related in some way, but not in semantic vector space. Some other (non\\-vector) source of information is telling us that there are connections among the wide variety of concepts we are working with. The task of building a graph RAG application is to effectively capture these connections between concepts into a knowledge graph, and to use the graph connections to retrieve more relevant documents for responding to a prompt.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*flPVNMUm83oc7H9Lt7U5AA.jpeg)\n\nTo summarize the issue that weâ€™re trying to tackle with graph RAG: there exists semi\\-structured, non\\-semantic information connecting many of the concepts that appear in my unstructured documents â€” and I would like to use this connection information to complement semantic vector search in order to retrieve documents that are best suited to answer prompts and questions within my use cases. We simply want to make retrieval better, and we want to use some external information or external logic to accomplish that, instead of relying solely on semantic vector search to connect prompts with documents,\n\n\n## Guiding principles for integrating graph with RAG\n\nConsidering the above motivation â€” to use â€œexternalâ€ information to make document connections that semantic search misses â€” there are some guiding principles that we can keep in mind while building and testing a graph RAG application:\n\n1. The graph should contain high\\-quality, meaningful concepts and connections\n2. Concepts and connections should be relevant to prompts within the set of use cases\n3. Graph connections should complement, not replace, vector search\n4. The usefulness of one\\- and two\\-step graph connections should be prioritized; relying on more than three steps to make connections should be reserved only for specialized use cases.\n\nPerhaps in a future article, we will dig into the nuances and potential impacts of following these principles, but for now, Iâ€™ll just note that this list is intended to jointly increase explainability, prevent over\\-complexity, and maximize efficiency of both building and using a graph RAG system.\n\nFollowing these principles along with other core principles from software engineering and data science can increase your chances of successfully building a useful and powerful graph RAG app, but there are certainly pitfalls along the way, which we outline in the next section.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*twgres708JPQHa1uZrkwDA.jpeg)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*5U0k4GoHTFiQhKM2a6xdMA.jpeg)\n\n\n## Reasons that your graph RAG app might not make it into production\n\nAnyone who has spent a lot of time building software around data, complex algorithms, statistics, and human users probably understands that there is a lot of uncertainty in building a system like graph RAG. Unexpected things can happen during data prep and loading, while building a knowledge graph, while querying and traversing the graph, during results compilation and prompt construction, and at virtually any other point in the workflow.\n\nAbove, we discussed how itâ€™s easy to implement graph RAG to get preliminary results, but it can be hard to get good results, much less production\\-quality results. Next, we look at a few potential issues that you might encounter when building and testing a graph RAG application.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*1J9hwwZDuYZ3WNrxCl_cOA.jpeg)\n\n\n### Graph RAG isnâ€™t doing much better than plain RAG\n\nIf the performance of your graph RAG system is about the same as with plain RAG, there can be any number of causes. Generally speaking, this seems to imply that the graph is not adding value to the system, but this could be caused by a low\\-quality knowledge graph, under\\-utilization of the graph, sub\\-optimal parameter settings, or many others. Or, there may not be a problem at all; vector search may be doing an excellent job of finding the right documents, and a graph simply isnâ€™t needed.\n\nWhat to look at:\n\n* Do you have example prompts that plain RAG doesnâ€™t handle well, but you would expect graph RAG to succeed? Can you â€œdebugâ€ on these prompts and see what is happening under the hood?\n* Does the knowledge graph contain meaningful connections that semantic search may not make? Can you find examples of concept pairs connected in the graph whose associated documents are far apart in vector space? The KG should be making meaningful connections between â€œfar awayâ€ docs.\n\n\n### You (still) see hallucinations\n\nIf youâ€™re seeing hallucinations with graph RAG that you didnâ€™t see with plain RAG, I would suspect a bug or a bad parameter setting somewhere. If you are seeing a similar level of hallucinations, this sounds like a general problem beyond the graph aspects.\n\nWhat to look at:\n\n* Does your document set contain the correct responses to the prompts that elicited hallucinations? Is vector search finding these documents?\n* Are the correct responses from the retrieved documents properly inserted into the context of the prompt that is passed to the LLM?\n\n\n### The graph is â€œtoo bigâ€\n\nWhen your knowledge graph is â€œtoo bigâ€ or too dense, two main types of problems can occur. First, there could be issues with scaling, which I discuss below. Second, graph traversal could result in â€œtoo manyâ€ documents, which must then be re\\-ranked and filtered. If the re\\-ranking and filtering strategy doesnâ€™t play well with the retrieval and graph traversal elements, you could end up filtering out important documents immediately after your graph just discovered them.\n\nWhat to look at:\n\n* How many documents are returned after graph traversal, and how many are re\\-ranked or filtered out? Does it look like documents found via strong graph connections are surviving filtering?\n* Did you build a knowledge graph filled with meaningful connections that suit your use cases? In the graph, can you find many concepts or connections that are too generic or irrelevant for your use cases? How much of your knowledge graph is made up of low\\-quality information?\n\n\n### The graph is â€œtoo smallâ€\n\nPer above, if the graph is â€œtoo bigâ€, it might be filled with low\\-quality connections. And if the graph is â€œtoo smallâ€, I would hope that the connections there are meaningful, which is good, but missing connections come in two main types. The first is caused by a bug in the graph construction process. The second is caused by graph construction that was not designed for it. Data in a different contexts or different formats may be processed differently by different graph\\-construction methods.\n\nWhat to look at:\n\n* Did you build your knowledge graph using an LLM with entity/keyword extraction? Are you capturing all of the meaningful entities from every document, or is the LLM limiting its output?\n* In your documents, what are some concepts and connections that you would expect to be in the knowledge graph, but seem to be missing? When and how do you expect them to be added to the graph? Why arenâ€™t they actually being added to the graph?\n\n\n### You canâ€™t find the â€œhappy mediumâ€ graph\n\nDo you feel like you can build a graph that is â€œtoo bigâ€ or one that is â€œtoo smallâ€, but you canâ€™t build something in the middle?\n\nWhat to look at:\n\n* What parameters or methods are you changing to go from small to big or back again? Should these be affecting graph quality this much? Can you study some graph elements that appear or disappear unexpectedly, depending on which graph construction settings youâ€™re using?\n* Also see relevant tips in â€œbigâ€ and â€œsmallâ€ sections above.\n\n\n### Your implementation requires new software or increased deployment complexity\n\nThis is a classic Data Science problem: build really cool and cutting\\-edge methods, only to see development teams refuse or struggle to bring the code from your notebooks into the production stack. Sticking to the most popular, best supported, and largely open\\-source tools can make it easier to get to production, especially if your organization is already using those tools elsewhere.\n\nWhat to look at:\n\n* Does your implementation require creating a new data store for graphs? You [probably donâ€™t need a graph DB](https://www.datastax.com/blog/knowledge-graphs-for-rag-without-a-graphdb), and might be able to use your production vector store for graphs as well.\n* Are you using some of the most popular open\\-source tools for building AI applications, like LangChain? These can reduce code complexity, make the app more portable, and expand potential integrations and further development.\n\n\n### Your implementation doesnâ€™t scale\n\nThe article [Scaling Knowledge Graphs by Eliminating Edges](https://thenewstack.io/scaling-knowledge-graphs-by-eliminating-edges/) in *The New Stack* shows one way to make graph RAG very scalable. Like above, the most popular, best supported, and largely open\\-source tools are usually the best path to painless scaling, but itâ€™s not always easy.\n\nWhat to look at:\n\n* Which part isnâ€™t scaling? Graph traversal, re\\-ranking, results compilation, or something else? See â€œThe graph is too bigâ€ above for more tips.\n* Do you have a particular component that isnâ€™t scaling well? Sometimes using an in\\-memory graph library like â€˜networkxâ€™ â€” \\-or even a graph DB â€” to do complex graph operations can cause a resource bottleneck. You may want to [switch to a more scalable option for graph operations](https://bit.ly/3YD5NAd).\n* Are you using parallel API calls to handle most of the heavy lifting, or are you trying to do complex or costly computations inside the main app logic?\n\n\n## Finding success with graph RAG in production\n\nThe key to creating a successful graph RAG system lies in constructing a knowledge graph and traversal logic that complement semantic vector retrieval, not replacing or competing with it. The graph design should aim to connect the right nodes, knowledge, entities, and documents at the right time, enabling the assembly of the appropriate documents to produce the most helpful and actionable query response.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*orXW5uw-geBo-WVtZUxWXQ.jpeg)\n\nWith respect to Glean, it should be noted that an internal document dataset is a perfect use case for graph RAG. A knowledge graph can connect people, projects, products, customers, meetings, locations, etc â€” and all of these are somewhat limited in number by the size of the organization and the work it does. Building and managing a graph of thousands of employees is much more tractable than, for example, trying to do the same with all of the people mentioned on Wikipedia or in a large database of financial or legal documents. So, possibly the first great decision that Glean made was to find a great use case for graph RAG to tackle.\n\nOne often understated aspect of graph RAG systems is the quality and reliability of the input data and the pipelines that get it there. This has more to do with data engineering and traditional software development than AI. In previous tech paradigms, connecting different data systems was challenging due to incompatible data types and access methods. Now, AI and LLMs enable the integration of disparate sources of unstructured data, allowing for the consolidation of data from various origins into a single RAG system. This integration capability enables LLMs to process and make sense of unstructured data from various sources, such as internal web pages, wikis, code repositories, databases, Google Docs, and chat logs. Simply connecting all of this information together and making it accessible from a single interface can be a big win.\n\n\n## The way forward\n\nConstruction of graph RAG systems for any use case involves leveraging foundational components such as data stores for vectors and graphs, embeddings, and LLMs, enhanced by open\\-source orchestration tools like LangChain and LlamaIndex. These tools facilitate the development of robust, scalable, and efficient systems, promising a future where companies achieve substantial success by optimizing knowledge work through automation and streamlining.\n\nThe public success of knowledge graphs and graph RAG systems, particularly by companies like Glean, showcases how effective these technologies are for internal use cases, creating value by making the organization more efficient. However, the broader application potential for external, enterprise and consumer\\-facing products remains largely untapped, presenting many opportunities for other companies to explore.\n\nIt is perhaps notable that we have been in what is called the â€œInformation Ageâ€ for at least 30 years, and it is only in the past year or two that we have really started to put together the building blocks for connecting all of this information across sources, across ideas, across documents, and across concepts, so that our software systems can make the same types of reasoning, logic, and judgment that we as humans use as a daily part of our knowledge work. Some people are calling this the â€œIntelligence Ageâ€.\n\nWhile initially focusing on simple, straightforward decisions, AIâ€™s trajectory is set towards managing more complex scenarios, dramatically improving efficiency in both time and cost. This exciting evolution positions many AI applications â€” including graph RAG â€” as pivotal in transforming how knowledge is interconnected and utilized in a wide variety of contexts.\n\nTo get started with graph RAG now, or to learn more, take a look at the [DataStax guide to graph RAG](https://bit.ly/4862Lrl).\n\n*by Brian Godsey, Ph.D. ([LinkedIn](https://bit.ly/4enqFRa)) â€” mathematician, data scientist and engineer // AI and ML products at [DataStax](https://bit.ly/3NpPujA) // Wrote the book [Think Like a Data Scientist](https://bit.ly/4f5uVES)*\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Il1GrFN6fYN7e_ovExRGPw.jpeg)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*wQvZDIlkOvrYZnbwl0bEPQ.jpeg)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/the-real-reason-openai-abandoned-next-js-for-remix-a4b2622ee9b2","frontmatter":{"title":"The Real Reason OpenAI Abandoned Next.js for Remix","meta_title":"The Real Reason OpenAI Abandoned Next.js for Remix","description":"The surprising reasons behind OpenAIâ€™s move and what it means for the future of web development","date":"2024-11-08T00:25:31.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*bf8ao0JjEiMka6dJqp-hxg.jpeg","categories":["Technology/Web","Programming","Web Development"],"author":"Rifx.Online","tags":["Remix","Next.js","client-side","rendering","scalability"],"draft":false,"slug":"blog/the-real-reason-openai-abandoned-next-js-for-remix-a4b2622ee9b2"},"content":"\n### The surprising reasons behind OpenAIâ€™s move and what it means for the future of web development\n\n\n\n## Introduction to the Transition\n\nOpenAI recently caused a stir in the developer community by moving from Next.js to Remix.\n\nThis unexpected switch left many questioning the rationale behind such a significant change.\n\nBut **can you blame them?**\n\nHere is **what most devs think** of NextJS based on [this](https://www.reddit.com/r/nextjs/comments/1f92jdv/chatgptcom_switched_from_nextjs_to_remix/) reddit discussion:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GCrb_aGjh1nticKNeHh9Bg.png)\n\nThatâ€™s rough.\n\nBut I also asked builders on X,\n\nand they had different opinions when it came to working on smaller projects:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*YmN2pTYaCFXFzb3AMjfoqQ.png)\n\n## Letâ€™s get to the bottom of this\n\nThis exploration isnâ€™t just about understanding OpenAIâ€™s decision but also about **what this could mean for other developers** and the broader tech landscape.\n\nTo understand the rationale, I spent hours analyzing the codebase and tools.\n\nHere are the insights I gained.\n\n## Technical Insights on the Switch\n\nUnderstanding the technical aspects of this transition is key to understanding why OpenAI favored Remix.\n\nWe examined their application architecture to identify the core differences between Next.js and Remix.\n\n### Client Rendering vs. Server Rendering\n\nOpenAIâ€™s application focuses on **client\\-side rendering**, where most processing occurs in the userâ€™s browser.\n\nThis reduces the need for server\\-rendered HTML.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*50dv8sWPbwFp85fQu_dodQ.png)\n\n**Remix** is ideal for these scenarios because it effectively manages client\\-side applications. This choice ensures OpenAIâ€™s users have a smoother, more responsive experience.\n\n### Initial Page Load Process\n\nWhen a user visits the ChatGPT site, **preloaded JavaScript and meta tags** are involved in the initial page load.\n\nThis optimizes the client\\-side rendering process. **Remix** excels in managing these elements, ensuring a smooth and fast initial load.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*65VlHo5RQObk-YGzBlkx3w.png)\n\n## Why This Matters\n\n### Improved User Experience\n\nBy preloading essential scripts and data, users encounter less delay and a more responsive interface from the moment they access the site.\n\n### Efficient Loading\n\nRemixâ€™s capability to handle these preloaded elements means reduced waiting times and an overall faster browsing experience.\n\nBy leveraging these features,\n\n*OpenAI can deliver a more seamless and enjoyable experience for its users right from the start.*\n\n## Diving Deeper On The Key Features of Remix Utilized by OpenAI\n\nOpenAI leverages several key features of Remix to enhance their application.\n\n### Preloading Strategies\n\nRemix preloads essential data and assets, reducing loading times and enhancing performance.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GPH2ZGhT_yfrQYpR2Xhvug.png)\n\nThis strategy ensures that users receive a seamless experience right from the start.\n\n### Data Management with Loaders\n\nRemixâ€™s loader API efficiently gathers all necessary data for the initial render, embedding it directly into the HTML.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*PmCiQNkAJlu2MSFEtpydiw.png)\n\nThis approach eliminates the need for additional client\\-side data fetching, speeding up the rendering process.\n\n## Benefits and Implications of the Move\n\nSwitching to Remix offers several advantages for OpenAI, from performance gains to future development prospects.\n\n### Performance Improvements\n\nBy adopting Remix, OpenAI achieves faster initial load times and smoother client\\-side navigation.\n\nThese performance enhancements contribute to a more responsive and user\\-friendly application.\n\n### Future Prospects with Remix\n\nThe flexibility and efficiency of Remix position OpenAI for future growth and innovation.\n\nAs Remix continues to evolve, OpenAI can leverage its advanced features to stay ahead in the competitive landscape of web development.\n\n### Why This Matters\n\n**Improved User Experience**: Users benefit from quicker page loads and a more fluid browsing experience.\n\n**Efficient Development**: Remixâ€™s capabilities streamline development processes, allowing OpenAI to innovate more rapidly.\n\n**Scalability**: The architecture of Remix supports future enhancements and scaling, ensuring long\\-term viability.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/the-rise-of-the-ai-agent-product-manager-and-ai-agent-engineer-0905f1d30cce","frontmatter":{"title":"The Rise of the AI Agent Product Manager and AI Agent Engineer","meta_title":"The Rise of the AI Agent Product Manager and AI Agent Engineer","description":"Imagine a future where Generative AI doesnâ€™t just respond to queries but proactively solves complex problems across every facet ofâ€¦","date":"2024-11-04T12:33:53.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*dlJ0a49_lRAPR1tTPs898w.png","categories":["Generative AI","Ethics","Technology"],"author":"Rifx.Online","tags":["Generative","Product","Manager","Engineer","Ethics"],"draft":false,"slug":"blog/the-rise-of-the-ai-agent-product-manager-and-ai-agent-engineer-0905f1d30cce"},"content":"\n\n\n\n\n\nImagine a future where Generative AI doesnâ€™t just respond to queries but proactively solves complex problems across every facet of business. This isnâ€™t science fiction; itâ€™s the rapidly approaching reality of Generative AI agents. These agents are poised to revolutionize companiesâ€™ operations and inspire a new wave of innovation, from streamlining supply chains to optimizing product development and transforming customer interactions.\n\nHaving spent over a year building Generative AI applications and agents, Iâ€™ve seen firsthand how these technologies can profoundly reshape business processes. AIâ€™s potential is immense, from support agents that handle customer queries with unprecedented efficiency to autonomous agents that drive business operations and decision\\-making. These agents are not merely enhancing existing processes but enabling new ways of working.\n\nFor example, picture an agent that doesnâ€™t just schedule meetings but understands the context of your work, suggests the most impactful attendees, prepares briefing documents, and even proposes agenda items based on recent company developments. Or consider an agent in manufacturing that doesnâ€™t just monitor production lines but predicts maintenance needs, optimizes resource allocation in real\\-time, and collaborates with design teams to suggest product improvements based on production data.\n\nThis AI\\-driven transformation is creating a need for two pivotal roles: the **AI Agent Product Manager** and **the AI Agent Engineer**. These specialists are not just the architects and builders of our AI\\-augmented future, but integral parts of a collaborative team, working at the intersection of business strategy and cutting\\-edge technology.\n\n\n## Introducing the New Roles\n\nThe **AI Agent Product Manager** is a visionary who identifies opportunities for agents to create value, designs their capabilities, and ensures they align with business goals and user needs. They act as translators between the world of business and AI possibilities, orchestrating AI innovation.\n\n**The AI Agent Engineer**, on the other hand, is the technical wizard who brings these agents to life. They design robust architectures, create sophisticated prompts, and ensure the agents are grounded in company data and processes by seamlessly integrating them with various systems and data sources.\n\nSince we are still in the early stages of this technology cycle, these professionals are usually found at specialized AI consultancies or companies developing agent\\-building products like Salesforce. This allows them to bring best practices and industry innovations to each new project.\n\n\n## The AI Agent Product Manager: Orchestrating AI Innovation\n\nAs an Agent Product Manager, you may work on different use cases, such as a sales agent one month and an HR agent the next. Letâ€™s dive into what your role might look like:\n\nAs an Agent Product Manager, lets say youâ€™re tasked with developing an agent for a multinational manufacturing firm. Your first step is to lead a series of workshops with executives from various departments â€” operations, design, sales, and customer service. Youâ€™re not just looking for incremental improvements; youâ€™re hunting for transformative opportunities, and you do this by fostering collaboration and understanding across the organization.\n\nThrough these discussions, you identify a game\\-changing possibility: an agent that can bridge customer feedback, product design, and manufacturing processes. This Agent would analyze customer reviews and support tickets, identify trending issues or desired features, and automatically generate design modification proposals. It would then simulate these changesâ€™ impact on manufacturing processes and costs.\n\nAs a product manager for agents, one of your primary responsibilities will be to map out the agentsâ€™ journeys. It involves defining each step from the initial interaction to the final outcome, ensuring that everything aligns with the business goals. You will need to identify the key interactions the Agent will have, understand the context of these interactions, and determine the objectives each journey should achieve. You will also need to consider critical questions such as: How will the Agent prioritize customer feedback? How can it effectively present design suggestions to the engineering team? And what ethical considerations must be addressed when AI influences product decisions?\n\nYouâ€™ll work closely with stakeholders to define success metrics. For example, you may decide that the Agent should aim to reduce the time from identifying a product issue to implementing a fix by 50% while also increasing customer satisfaction scores.\n\nAs the project progresses, you ensure the agent delivers real business value. You might review both simulated and actual conversations between the AI and design teams, tweaking the Agentâ€™s communication style to better resonate with engineers. Or you could pore over data on how the Agentâ€™s suggestions have impacted product quality and customer satisfaction, looking for ways to improve its performance further.\n\nThroughout this process, youâ€™re not just thinking about the Agentâ€™s current capabilities but its future potential. How could this Agent evolve to react to customer feedback and predict future market trends? Could it someday participate in brainstorming sessions with the product team, offering data\\-driven insights to fuel innovation?\n\nYour Agent Product Manager role puts you at the forefront of business transformation. Youâ€™re not just implementing a new tool; youâ€™re reshaping how entire organizations think, innovate, and operate in the age of AI.\n\n\n## The AI Agent Engineer: Crafting Intelligent and Reliable Systems\n\nNow, letâ€™s switch gears and step into the role of an Agent Engineer on this same project:\n\nYour challenge is to create an agent that can understand customer feedback, translate it into actionable design insights, and interface with manufacturing systems. This is no small feat â€” it requires a deep understanding of large language models, sophisticated prompt engineering, and robust system integration.\n\nYou begin by selecting an appropriate large language model as the foundation for your Agent. However, your real work lies in designing a comprehensive agent architecture that can reliably perform across many conversation journeys.\n\nAs an Agent Engineer, one of your primary focuses is creating and refining the Agentâ€™s prompt structure. You craft intricate prompts that effectively guide the modelâ€™s behavior, ensuring it consistently provides relevant and accurate responses across various scenarios. This could involve developing a hierarchical prompt system that can handle everything from supervising multiple agents to navigating various journeys.\n\nYou will spend significant time evaluating agent behavior and output, refining the prompts and flows, and publishing a new version. You may even design and implement a rigorous testing framework that simulates thousands of potential conversation trajectories. Your goal is to ensure that the Agentâ€™s response is deterministic and aligns with the desired outcome for any given input.\n\nFor instance, you might create a suite of test cases that cover various types of customer feedback, from simple product issues to complex feature requests. You then methodically work through these cases, analyzing the Agentâ€™s responses and iterating on the prompt structure and decision\\-making logic to improve performance.\n\nWhen you encounter edge cases where the Agentâ€™s behavior is inconsistent or suboptimal, you donâ€™t simply tweak the prompt. Instead, you dive deep into the Agentâ€™s decision\\-making process, adjusting the underlying logic and prompt structure to address these issues systematically.\n\nIntegration remains a crucial part of your role. Youâ€™re designing APIs that allow the Agent to pull data from customer support databases, access product design files, and input data into manufacturing planning systems. But beyond just connecting systems, youâ€™re focused on ensuring the Agent can make intelligent decisions based on this integrated data.\n\nEthics and safety continue to be critical concerns. You implement robust safeguards and oversight mechanisms to ensure the AI doesnâ€™t suggest design changes that could compromise product safety. You also build features for explainability, so the AI can always show its reasoning for any suggestion, which is crucial for building trust with the engineers and designers working with the Agent.\n\nYour role as an Agent Engineer involves creating a functional AI system and crafting an intelligent agent that can reliably and consistently drive innovation and efficiency across the entire product development and manufacturing process. This complex challenge puts you at the forefront of AI technology, shaping the future of how businesses operate in the AI age.\n\n\n## Ethical Considerations and the Power of Collaboration\n\nAs agents become more integral to businesses, Agent Product Manager and Agent Engineer roles will only grow in importance. These roles are not just about technical prowess or strategic insight â€” they demand a deep commitment to ethical considerations. As these agents influence significant business decisions, the professionals behind them must ensure that these systems are transparent, fair, and aligned with broader societal values.\n\nThe success of this agent relies heavily on the seamless collaboration between the Product Manager and Engineer. Together, youâ€™ll iterate on the Agentâ€™s capabilities, troubleshoot issues, and push the boundaries of whatâ€™s possible.\n\n\n## Comparing the Roles: Agent Product Manager vs. Agent Engineer\n\nHereâ€™s a summary comparison table to emphasize the differences between the Agent Product Manager and the Agent Engineer:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*T26nkBI-wID26X2NrE2SSQ.jpeg)\n\nKey Takeaways:\n\n* **Agent Product Manager:** This position focuses on the strategic and business aspects of agents, ensuring they deliver value and align with company goals.\n* **Agent Engineer:** This position concentrates on technical implementation, ensuring agents function reliably and integrate seamlessly with existing systems.\n\n\n## The Future is Yours: Rise to the Challenge\n\nAs AI expands its influence, Agent Product Manager and Agent Engineer roles will be at the forefront of this technological revolution. Whether youâ€™re defining the strategy for an AI\\-driven business transformation or designing the intricate systems that power intelligent agents, youâ€™ll be shaping the future of business.\n\nThese roles require a unique blend of skills: strategic thinking, technical expertise, creativity, and a deep understanding of business and AI. They offer the chance to work on cutting\\-edge technology while driving tangible business impact.\n\nSo, future Agent Product Managers and Engineers, are you ready to rise to the challenge? The AI\\-augmented future is waiting for your expertise and vision. Whether youâ€™re drawn to the strategic aspects of product management or the technical intricacies of agent engineering, thereâ€™s a place for you in this exciting new field. The question is not if AI will transform business, but how â€” and you could be the one to decide.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/top-25-generative-ai-terminologies-you-must-know-6a3bb0300988","frontmatter":{"title":"Top 25 Generative AI Terminologies You Must Know","meta_title":"Top 25 Generative AI Terminologies You Must Know","description":"The article presents a comprehensive guide to 25 essential terminologies in generative AI, aimed at enhancing understanding among professionals in technology and related fields. Key concepts include Generative Models, Transformers, GANs, Autoencoders, and various learning paradigms such as Zero-Shot and Few-Shot Learning. Each term is defined with examples and resources for deeper exploration, emphasizing the importance of mastering these concepts for effective engagement in AI projects and discussions. The guide serves as a foundational tool for both newcomers and those seeking to update their knowledge in the rapidly evolving generative AI landscape.","date":"2024-11-14T03:29:09.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*swCl2YJj6wfAc9J3v6AqTg.png","categories":["Generative AI","Machine Learning","Data Science"],"author":"Rifx.Online","tags":["Generative","Transformers","GANs","Autoencoders","Zero-Shot"],"draft":false,"slug":"blog/top-25-generative-ai-terminologies-you-must-know-6a3bb0300988"},"content":"\n\n\n\n*Master the Key Concepts to Excel in Generative AI with Clear Explanations, Real\\-World Applications, and In\\-Depth Resources*\n\n\n\nGenerative AI is indeed a critical technology across industries; therefore, understanding generative AIâ€™s core concepts is crucial for any professional in tech and beyond. The following comprehensive guide covers the top 25 must\\-know generative AI terminologies that will provide you with lucid definitions, practical examples, and other resources that will deepen your knowledge. Whether preparing for interviews, working on AI projects, or keeping yourself abreast of the goings\\-on in this fast\\-changing field, mastering these terms provides you with a strong basis in generative AI.\n\n\n## 1\\. Generative Model\n\n* **Definition**: A type of AI model that generates new data points from learned patterns.\n* **Example**: Generative Pre\\-trained Transformers(GPT) generate human\\-like text based on input prompts.\n* **Learn More**: [Introduction to Generative Models](https://proxy.rifx.online/https://www.datacamp.com/blog/what-is-a-generative-model)\n\n\n## 2\\. Transformer\n\n* **Definition**: A neural network architecture that uses self\\-attention mechanisms to process and generate sequences, like text or images.\n* **Example**: BERT is a Transformer model used for tasks like question\\-answering and text classification.\n* **Learn More**: [Understanding Transformers](https://proxy.rifx.online/https://www.turing.com/kb/brief-introduction-to-transformers-and-their-power)\n\n\n## 3\\. Latent Space\n\n* **Definition**: A multi\\-dimensional space where generative models map data, allowing them to learn and generate variations.\n* **Example**: In image generation, similar images are positioned near each other in the latent space.\n* **Learn More**: [Exploring Latent Space in AI](https://proxy.rifx.online/https://www.perplexity.ai/page/latent-space-101-what-it-is-an-mwXuxYfzS_.J4e_uFvOskg)\n\n\n## 4\\. GAN (Generative Adversarial Network)\n\n* **Definition**: A type of AI that pits two neural networks, a generator and a discriminator, against each other to create realistic data.\n* **Example**: GANs generate realistic\\-looking faces that do not belong to real people.\n* **Learn More**: [What Are GANs and How Do They Work?](https://proxy.rifx.online/https://aws.amazon.com/what-is/gan/#:~:text=A%20generative%20adversarial%20network%20(GAN,from%20a%20database%20of%20songs.)\n\n\n## 5\\. Autoencoder\n\n* **Definition**: A neural network that learns to compress and reconstruct data, often used for tasks like dimensionality reduction and denoising.\n* **Example**: Autoencoders are used to remove noise from corrupted images.\n* **Learn More**: [Introduction to Autoencoders](https://proxy.rifx.online/https://towardsdatascience.com/introduction-to-autoencoders-7a47cf4ef14b)\n\n\n## 6\\. Diffusion Models\n\n* **Definition**: Models that learn to reverse a noise addition process to generate detailed and coherent data from noise.\n* **Example**: Diffusion models are used in DALL\\-E 2 to generate high\\-quality images from random noise.\n* **Learn More**: [Understanding Diffusion Models](https://proxy.rifx.online/https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/)\n\n\n## 7\\. Prompt Engineering\n\n* **Definition**: The process of crafting input prompts to optimize the output generated by a model.\n* **Example**: Modifying the input prompt in GPT\\-4 to generate more concise summaries.\n* **Learn More**: [A Guide to Prompt Engineering](https://proxy.rifx.online/https://www.datacamp.com/tutorial/a-beginners-guide-to-chatgpt-prompt-engineering)\n\n\n## 8\\. Zero\\-Shot Learning\n\n* **Definition**: The ability of a model to perform tasks it was not explicitly trained for, by leveraging knowledge from other tasks.\n* **Example**: GPT\\-3 can perform translation without being specifically trained on translation datasets.\n* **Learn More**: [What is Zero\\-Shot Learning?](https://proxy.rifx.online/https://www.ibm.com/topics/zero-shot-learning)\n\n\n## 9\\. Few\\-Shot Learning\n\n* **Definition**: A modelâ€™s ability to learn tasks with only a few examples, minimizing the need for extensive training data.\n* **Example**: GPT\\-3 can be fine\\-tuned to write in a specific style with minimal input samples.\n* **Learn More**: [Few\\-Shot Learning Explained](https://proxy.rifx.online/https://www.ibm.com/topics/few-shot-learning#:~:text=IBM-,What%20is%20few%2Dshot%20learning%3F,suitable%20training%20data%20is%20scarce.)\n\n\n## 10\\. Reinforcement Learning\n\n* **Definition**: A learning paradigm where an AI agent learns to make decisions by interacting with an environment to maximize cumulative rewards.\n* **Example**: AlphaGo uses reinforcement learning to master the game of Go by playing millions of games against itself.\n* **Learn More**: [Reinforcement Learning for Generative AI](https://proxy.rifx.online/https://dl.acm.org/doi/pdf/10.1613/jair.1.15278)\n\n\n## 11\\. Variational Autoencoder (VAE)\n\n* **Definition**: A type of autoencoder that learns to generate new data by introducing randomness to its latent space representations.\n* **Example**: VAEs are used to generate new faces and smoothly transition between different facial features.\n* **Learn More**: [VAEs and Their Applications](https://proxy.rifx.online/https://www.datacamp.com/tutorial/variational-autoencoders)\n\n\n## 12\\. Self\\-Supervised Learning\n\n* **Definition**: A learning technique where the model generates its own labels from the data, reducing reliance on labelled datasets.\n* **Example**: BERT uses self\\-supervised learning by masking words in sentences and predicting them during training.\n* **Learn More**: [What is Self\\-Supervised Learning?](https://proxy.rifx.online/https://www.ibm.com/topics/self-supervised-learning)\n\n\n## 13\\. Tokenization\n\n* **Definition**: The process of splitting text into smaller units, such as words or subwords, for easier processing by models.\n* **Example**: Text input is tokenized into words before being fed into GPT\\-4 for processing.\n* **Learn More**: [Tokenization in NLP](https://proxy.rifx.online/https://www.datacamp.com/blog/what-is-tokenization)\n\n\n## 14\\. Beam Search\n\n* **Definition**: A search algorithm that expands multiple potential sequences of tokens to generate the most likely sequence during decoding.\n* **Example**: Beam search is used in machine translation to generate coherent text outputs.\n* **Learn More**: [Beam Search Explained](https://proxy.rifx.online/https://www.width.ai/post/what-is-beam-search)\n\n\n## 15\\. Transfer Learning\n\n* **Definition**: The process of using a pre\\-trained model on one task and fine\\-tuning it for another, often with less data.\n* **Example**: Fine\\-tuning BERT on sentiment analysis tasks after pre\\-training on general language tasks.\n* **Learn More**: [What is Transfer Learning?](https://proxy.rifx.online/https://aws.amazon.com/what-is/transfer-learning/)\n\n\n## 16\\. Language Model\n\n* **Definition**: A model that predicts the probability of word sequences in natural language, helping generate or understand text.\n* **Example**: GPT\\-4 is a language model capable of generating coherent text for a wide range of applications.\n* **Learn More**: [Introduction to Language Models](https://proxy.rifx.online/https://developers.google.com/machine-learning/resources/intro-llms)\n\n\n## 17\\. Bias in AI\n\n* **Definition**: The tendency of AI systems to produce results that favour or discriminate against certain groups due to biased training data or algorithms.\n* **Example**: Gender bias in AI\\-powered hiring systems trained on biased historical data.\n* **Learn More**: [Understanding Bias in AI](https://proxy.rifx.online/https://www.ibm.com/topics/ai-bias)\n\n\n## 18\\. GPT (Generative Pre\\-trained Transformer)\n\n* **Definition**: A large\\-scale language model that generates human\\-like text based on pre\\-training and fine\\-tuning on extensive text corpora.\n* **Example**: GPT\\-4 generates essays, stories, and detailed responses to user queries.\n* **Learn More**: [How GPT Works](https://proxy.rifx.online/https://tecknoworks.com/how-gpt-works-and-its-core-mechanics/)\n\n\n## 19\\. Perplexity\n\n* **Definition**: A metric that measures how well a language model predicts a given sequence of words, with lower perplexity indicating better performance.\n* **Example**: Comparing the perplexity of GPT\\-3 and GPT\\-4 to assess their text generation quality.\n* **Learn More**: [Perplexity in Language Models](https://proxy.rifx.online/https://huggingface.co/docs/transformers/en/perplexity)\n\n\n## 20\\. Natural Language Processing (NLP)\n\n* **Definition**: A field of AI focused on the interaction between computers and humans through natural language, encompassing tasks like translation and sentiment analysis.\n* **Example**: NLP models are used to perform sentiment analysis on customer reviews.\n* **Learn More**: [Introduction to NLP](https://proxy.rifx.online/https://towardsdatascience.com/a-gentle-introduction-to-natural-language-processing-e716ed3c0863)\n\n\n## 21\\. Neural Network\n\n* **Definition**: A computing system inspired by the human brainâ€™s network of neurons, consisting of layers of interconnected nodes for tasks like image recognition and language processing.\n* **Example**: Convolutional Neural Networks (CNNs) are used to recognize objects in images.\n* **Learn More**: [What are Neural Networks?](https://proxy.rifx.online/https://www.ibm.com/topics/neural-networks)\n\n\n## 22\\. Training Data\n\n* **Definition**: Data used to train AI models by allowing them to learn from examples, improving their ability to recognize patterns and make predictions.\n* **Example**: Large image datasets like ImageNet are used to train AI models for image classification tasks.\n* **Learn More**: [Training Data in AI](https://proxy.rifx.online/https://www.oracle.com/artificial-intelligence/ai-model-training/)\n\n\n## 23\\. Attention Mechanism\n\n* **Definition**: A method in neural networks that helps models focus on the most relevant parts of an input sequence, improving performance in tasks like machine translation and text generation.\n* **Example**: Attention mechanisms allow a model to focus on important words in a sentence when translating between languages.\n* **Learn More**: [What is the Attention Mechanism?](https://proxy.rifx.online/https://machinelearningmastery.com/the-attention-mechanism-from-scratch/)\n\n\n## 24\\. Epoch\n\n* **Definition**: One complete pass through the entire training dataset during the training of a machine learning model.\n* **Example**: Training a neural network for 10 epochs to ensure it properly learns without overfitting.\n* **Learn More**: [Understanding Epochs in Machine Learning](https://proxy.rifx.online/https://www.geeksforgeeks.org/epoch-in-machine-learning/)\n\n\n## 25\\. Multimodal AI\n\n* **Definition**: AI that can process and generate data from multiple modalities (e.g., text, images, and audio) simultaneously.\n* **Example**: CLIP processes both images and text to generate captions for images.\n* **Learn More**: [What is Multimodal AI?](https://proxy.rifx.online/https://www.techtarget.com/searchenterpriseai/definition/multimodal-AI)\n\nKeep in mind that mastery in generative AI is achieved step by step. As you go through the concepts, make sure to explore each one of them through the resources provided, participate in discussions, and try applying what you have learned to your projects. The interaction with these resources and conversations will help you understand the terminology of the language and its use in the real world.\n\nThanks for reading! If you found this guide helpful, please share it with others who might be looking to enhance their generative AI understanding. We learn together and apply these concepts better because of it.\n\nIf you have any thoughts, questions, or even additional resource suggestions you think might be helpful to share, please drop them in the comments section below.\n\nHappy exploring the world of Generative AI!\n\n*Connect with me through [linktr.ee](https://proxy.rifx.online/https://linktr.ee/tharunkumarreddypolu) to know more!*\n\n\n## In Plain English ðŸš€\n\n*Thank you for being a part of the [**In Plain English**](https://proxy.rifx.online/https://plainenglish.io/) community! Before you go:*\n\n* Be sure to **clap** and **follow** the writer ï¸ðŸ‘**ï¸ï¸**\n* Follow us: [**X**](https://proxy.rifx.online/https://x.com/inPlainEngHQ) \\| [**LinkedIn**](https://proxy.rifx.online/https://www.linkedin.com/company/inplainenglish/) \\| [**YouTube**](https://proxy.rifx.online/https://www.youtube.com/channel/UCtipWUghju290NWcn8jhyAw) \\| [**Discord**](https://proxy.rifx.online/https://discord.gg/in-plain-english-709094664682340443) \\| [**Newsletter**](https://proxy.rifx.online/https://newsletter.plainenglish.io/) \\| [**Podcast**](https://proxy.rifx.online/https://open.spotify.com/show/7qxylRWKhvZwMz2WuEoua0)\n* [**Create a free AI\\-powered blog on Differ.**](https://proxy.rifx.online/https://differ.blog/)\n* More content at [**PlainEnglish.io**](https://proxy.rifx.online/https://plainenglish.io/)\n\n"},{"lang":"en","group":"blog","slug":"blog/top-5-ai-tools-for-ios-developers-5ee9f39558ac","frontmatter":{"title":"Top 5 AI Tools for iOS Developers","meta_title":"Top 5 AI Tools for iOS Developers","description":"The article outlines the top five AI tools for iOS developers to enhance workflow efficiency. Key tools include Cursor/VSCode, which offers advanced code completion and refactoring features; the GitHub Copilot Xcode extension for AI-assisted editing within Xcode; Swift Assist, a predictive completion tool; and web interfaces like ChatGPT and Claude for iterative coding. Alex Sidebar is mentioned as a new Xcode extension, while AIProxy is highlighted for securely integrating AI APIs. The article emphasizes the importance of these tools in improving coding speed and accuracy for iOS developers.","date":"2024-11-14T03:29:09.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*6Hs8174FgiwTv87e.jpg","categories":["Programming","Technology/Web","Generative AI"],"author":"Rifx.Online","tags":["Cursor","VSCode","GitHub","Copilot","Swift"],"draft":false,"slug":"blog/top-5-ai-tools-for-ios-developers-5ee9f39558ac"},"content":"\n### To improve your workflow speed \\& efficiency\n\n\n\nWhile there are a lot of big AI talks, I want to get you back on earth. Whether youâ€™re already using AI\\-assisted tools for coding or you feel like this is just a big load of bullshitâ€¦ this article with a clickbait title is probably for you.\n\nWhile you can probably already find a lot of literature about what and how to use various tools to improve your skills, efficiency, and accuracy with AI, itâ€™s a tad more complicated for us iOS developers. Because we rely on Xcode and its toolchain to build our app, itâ€™s simply harder for us to go without Xcode. And not all the tools Iâ€™ll list and explain in the following paragraphs are about skipping Xcode.\n\n## 1\\. Cursor / VSCode\n\nObviously, this is the top of the list. Unless you were hibernating under a rock, you probably heard about VSCode. Working with it on a Swift project is not a novelty. GitHub Copilot, built\\-in VSCode, allows you to code at the speed of light without doing much of anything in terms of setup. They recently integrated more Copilot features within VSCode, and itâ€™s getting closer to Cursor. On top of tab completion, you can now also have inline chat \\+ code generation inline.\n\nCursor is a fork of VSCode, and their Cursor Tab completion feature is, in my experience, faster and more accurate than VSCode.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*pQ4SuReyicAiBCG3.gif)\n\nThey also do something that saved me countless hours: smart / AI\\-assisted refactoring. This is probably one of the best features worth the Cursor subscription alone.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*JlzVJ6o18sulIUEeo_p5sg.gif)\n\nAnd itâ€™s not only for refactoring; itâ€™s simply in smart edition after changing a line. Cursor will show a â€œtabâ€ indicator, which means it has a proposed change for a part of the code that is probably related to what you just edited. Just press tab to cascade changes, and it can go on and on. Tab tab tab.\n\nOnce you get into the flow, youâ€™ll see how efficient you can be. My flow is just coding as usual, but faster because I have to write much less code. The more you code with it, the more it learns your project, coding style, etcâ€¦ It might appear a bit off at the beginning, but trust me, give it time.\n\nYou can also generate code with the inline chat:\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*pVzU2MZ0vNFQ6-dRaWPy-w.gif)\n\nItâ€™s useful when you need a specific algorithm or have all the context within your existing code but need to write some tedious parts. It works quite well and can save a lot of time too. Donâ€™t forget to review the produced code :)\n\nTo get started on it for iOS development specifically, I encourage you to read two of my other stories:\n\nOne about how to set it up, install the correct extensions, etcâ€¦\n\nAnd another one is about switching your Xcode project from group\\-based to folder\\-based so you can freely create/delete/move files within VSCode/Cursor without touching the .xcodeproj / Xcode at all.\n\nThis is just scratching the surface for iOS development with Cursor/VScode. But you should get started today!\n\n## 2\\. GitHub Copilot Xcode extension\n\nThis one is a recently released extension, it was initially a project by [Intitni](https://proxy.rifx.online/https://github.com/intitni/CopilotForXcode), but it seems that GitHub forked/acquired it and made it the official extension for Copilot \\+ Xcode. And so far, while the UX is not perfect (understandable as they have to work with accessibility/windows API), itâ€™s much better than Apple (local) Xcode models.\n\nAnd lucky you, I already wrote about it:\n\nIf youâ€™re not ready to switch to another editor than Xcode, but still want to use efficient AI\\-assisted code editing, this extension is for you!\n\n## 3\\. Swift Assist\n\nWhile Xcode already has a built\\-in local model for Predictive code completion (only available on Apple Silicon Mac from Xcode 16\\), Apple teased something else at the WWDC:\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*-mlY8GyGh3VPVhyg3TmLYw.png)\n\nSwift Assist\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*XJnlRo8mqrAMZEVEL64ufg.gif)\n\nIt seems like the chat \\+ code generation from Cursor Iâ€™ve demoed above. It should be able to generate code from your comments. But for now, itâ€™s vaporware. Xcode 16\\.2 beta 2 mentioned it, but we still canâ€™t test it.\n\nMaybe itâ€™ll be available in a later version of Xcode 16\\.2 beta, so I canâ€™t wait to test and write about it!\n\n## 4\\. ChatGPT/Claude/Perplexity web interface\n\nSometimes, nothing is better than going back to the basics. While those code editors use Anthropic and OpenAI models and their own, itâ€™s good to not forget that using their web interface is also an invaluable tool in today's landscape.\n\n### ChatGPT \\+ Canvas\n\nOpenAIâ€™s ChatGPT has evolved quite a bit in the last few months. The recent release of o1\\-preview with reasoning and canvas allows for some good coding sessions right within the ChatGPT web interface.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*WUraNCcZMrCRrHMilgzl-Q.png)\n\nCanvas is a mini code editor built on the ChatGPT Web interface and lets you quickly iterate on code and ideas. You can use the chat to make incremental changes, and there are also some other tools to comment on the code, do inline changes, convert it to another language etcâ€¦\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ab7PdMLJwacZtVsET2YYmA.gif)\n\nWhile this will not allow you to build a full application, itâ€™s a great tool for quickly iterating on code ideas outside of your standard editor.\n\n### Claude Artifacts\n\nThis is similar to ChatGPT Canvas, but has some other features, like previewing (obviously not with Swift/SwiftUI) and working with multiple files at once.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*2iverELFSGqnJzklPK0cYg.png)\n\n## 5\\. Alex Sidebar\n\nThis is a new contender! The premise is simple, because Xcode is closed source and the extension API is quite limited, why not build around Xcode?\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*vZgn_FjH0FW53c7qZ4DZAg.png)\n\nIâ€™m not a fan of the UX, but it offers most of the Cursor features as an Xcode side panel built like as a window. There are various shortcuts \\+ code completions \\+ chat. You should definitely give it a go to see if it improve your workflow!\n\n## 6\\. AIProxy\n\nBonus as the (3\\) Swift Assist is not really â€¦. available\n\nThis not a tool for coding, but a tool for builders. When integrating an AI API in your iOS app, youâ€™ll most probably need add an API key to your project. But as we all know (right!), you should not have it client side. If you do so, itâ€™s easy for almost anyone to get dump your API key and use your AI credits on your behalf.\n\nEnter [AIProxy](https://proxy.rifx.online/https://www.aiproxy.pro/), they have an open source SDK, itâ€™s easy to integrate and they support all the AI providers your need.\n\nIf you donâ€™t feel like building a backend to proxy your AI calls, this is the right tool for you!\n\n\n"},{"lang":"en","group":"blog","slug":"blog/top-8-leading-ai-use-cases-revolutionizing-business-in-2025-837e4a98f6a3","frontmatter":{"title":"Top 8 Leading AI Use Cases Revolutionizing Business in 2025","meta_title":"Top 8 Leading AI Use Cases Revolutionizing Business in 2025","description":"Artificial Intelligence (AI) is poised to significantly transform business operations by 2025, serving as a key driver of innovation and efficiency across various industries. Key applications include predictive analytics for informed decision-making, AI-powered customer support systems, personalized marketing through recommendation engines, and intelligent automation to streamline processes. AI enhances supply chain management, fraud detection, workforce management, and financial planning by leveraging machine learning and real-time data analysis. Businesses that effectively integrate AI technologies will gain a competitive edge, although challenges such as data privacy and ethical concerns remain.","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Na9Wc3PuYCxWOCSBWc4HtQ.png","categories":["Technology","Predictive Analytics","Machine Learning"],"author":"Rifx.Online","tags":["predictive","analytics","automation","recommendation","machine-learning"],"draft":false,"slug":"blog/top-8-leading-ai-use-cases-revolutionizing-business-in-2025-837e4a98f6a3"},"content":"\n\n\n\n\n### Explore key AI applications driving business success.\n\nArtificial Intelligence (AI) is increasingly shaping the future of business, with its influence expanding across industries. In 2025, AI will not just be a tool for innovation but a critical driver of business transformation. From customer support to predictive analytics, AI has made significant strides in improving efficiency, reducing costs, and fostering new growth opportunities. As AI continues to evolve, businesses are increasingly relying on it to streamline operations, enhance decision\\-making, and create personalized customer experiences.\n\n\n\nThe rapid advancement of [***AI technologies***](https://www.blockchainappfactory.com/ai-development-company) is unlocking unprecedented possibilities for businesses. Companies are harnessing the power of machine learning, natural language processing, and automation to stay competitive in an ever\\-changing market landscape. These developments are particularly evident in areas such as supply chain optimization, personalized marketing, and human resource management. As AI adoption grows, its integration into core business processes will become even more crucial for organizations aiming to thrive in 2025 and beyond.\n\n\n## Understanding Artificial Intelligence\n\nAI (Artificial Intelligence) is a branch of computer science that aims to create machines or systems capable of performing tasks that would normally require human intelligence. These tasks include things like understanding language, recognizing patterns, solving problems, and making decisions. AI can be broadly categorized into two types:\n\n1. **Narrow AI (Weak AI):** This type of AI is designed to perform a specific task. Examples include voice assistants like Siri or Alexa, facial recognition systems, and recommendation algorithms used by streaming services like Netflix or Spotify. Narrow AI does not possess general intelligence and is limited to the tasks for which it was programmed.\n2. **General AI (Strong AI):** This is a more advanced form of AI that has the ability to understand, learn, and apply intelligence in a wide range of activities, similar to the way humans think and reason. General AI remains largely theoretical at this point and does not yet exist.\n\n**Key Technologies and Concepts in AI:**\n\n* **Machine Learning (ML):** A subset of AI that focuses on building algorithms that allow machines to learn from data and improve over time. In ML, the system is trained on large datasets and uses this information to make predictions or decisions without being explicitly programmed.\n* **Deep Learning:** A subset of machine learning that uses neural networks with many layers (called deep neural networks). Deep learning is particularly useful in complex tasks like image and speech recognition, natural language processing, and autonomous driving.\n* **Natural Language Processing (NLP):** This involves teaching machines to understand, interpret, and respond to human language in a way that is both meaningful and relevant. NLP is behind technologies like chatbots, virtual assistants, and language translation apps.\n* **Computer Vision:** A field of AI that allows machines to interpret and understand the visual world. Computer vision is used in applications like facial recognition, object detection, and self\\-driving cars.\n* **Reinforcement Learning:** A type of machine learning where an AI agent learns how to behave in an environment by performing actions and receiving feedback in the form of rewards or penalties.\n\n\n## Top 10 Game\\-Changing AI Use Cases Transforming 2024\n\n\n## 1\\. Predictive Analytics for Strategic Decision\\-Making\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*fgshMkMhuWL7ZEDo9MIR_w.png)\n\nPredictive analytics uses statistical algorithms, machine learning models, and data mining to forecast future outcomes based on historical data. It allows businesses to answer critical questions like â€œWhat is likely to happen?â€ by drawing from past patterns to inform future scenarios.\n\n\n### Key Use Cases in Strategic Decision\\-Making\n\n* **Demand Forecasting**: Retailers, manufacturers, and service providers use predictive analytics to forecast demand, helping them manage inventory, reduce waste, and avoid stockouts or overproduction. For example, retailers can plan for seasonal surges, and manufacturers can adjust production schedules based on anticipated needs.\n* **Risk Assessment and Management**: Predictive models identify risk factors in real\\-time, making it easier for businesses to manage financial, operational, and even reputational risks. In finance, predictive analytics can flag potential default risks, while in supply chains, it can forecast disruptions.\n* **Customer Behavior Prediction**: By analyzing past interactions, AI can predict customer behavior, enabling companies to better understand customer preferences, buying cycles, and potential churn. This helps tailor offerings and timing, maximizing customer lifetime value.\n\n\n### Benefits of Predictive Analytics for Strategy\n\n* **Improved Accuracy in Decision\\-Making**: Predictive models provide evidence\\-based insights, increasing confidence in strategic decisions by reducing reliance on intuition.\n* **Proactive Problem\\-Solving**: Instead of reacting to issues as they arise, businesses can anticipate them. This enables proactive measures in areas like risk mitigation, resource allocation, and workforce planning.\n* **Competitive Advantage**: Companies that harness predictive analytics can make faster and smarter decisions than competitors, gaining a strategic edge.\n\n\n### Tools and Technologies in Predictive Analytics\n\n* **Data Collection and Preparation**: Tools like Google BigQuery and Apache Hadoop collect and preprocess data from various sources.\n* **Model Building and Optimization**: Platforms such as TensorFlow, SAS, and IBM Watson allow businesses to create and test predictive models, fine\\-tuning them for accuracy and reliability.\n* **Deployment and Monitoring**: After model development, tools like DataRobot or Amazon SageMaker facilitate deployment, allowing continuous monitoring and updating for real\\-time accuracy.\n\n\n## 2\\. AI\\-Powered Customer Support and Chatbots\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*dTt_KxOtAb-qGThAvEOQsg.png)\n\nAI\\-driven customer support systems leverage machine learning and NLP to understand, interpret, and respond to customer queries. These tools analyze user input, detect intent, and offer relevant responses or guide users through a series of actions, automating the response process and reducing wait times.\n\n\n### Key Use Cases of AI\\-Powered Customer Support and Chatbots\n\n* **24/7 Customer Service**: Chatbots can handle customer inquiries around the clock, providing immediate assistance and reducing the need for customers to wait for human agents during off\\-hours.\n* **Lead Qualification and Nurturing**: For sales\\-driven teams, chatbots can qualify leads by asking pre\\-programmed questions, collecting key details, and directing high\\-potential leads to human agents.\n* **Personalized Customer Experiences**: Using customer data, AI\\-driven chatbots can personalize responses based on user history, previous interactions, and behavior, offering a tailored experience that enhances engagement.\n* **Feedback Collection and Sentiment Analysis**: AI chatbots can gather feedback during and after interactions, gauging customer satisfaction levels. Sentiment analysis can be applied to detect satisfaction or frustration, helping businesses refine their services.\n\n\n### Benefits of AI\\-Powered Customer Support and Chatbots\n\n* **Enhanced Efficiency and Cost Savings**: By automating repetitive inquiries, AI reduces the workload on human agents, enabling them to focus on complex issues and lowering operational costs.\n* **Scalability**: AI\\-driven customer support scales effortlessly, allowing businesses to handle surges in customer inquiries during peak times without additional staff.\n* **Faster Response Times**: AI chatbots can process and respond instantly, significantly reducing wait times and improving the customer experience.\n\n\n### Popular AI\\-Powered Customer Support and Chatbot Platforms\n\n* **Zendesk Chat and Support**: Combines chatbots with ticketing systems to manage complex customer support processes.\n* **Intercom**: An AI\\-driven customer messaging platform that supports sales and customer engagement through live chat and automated responses.\n* **Drift**: Known for its lead qualification features, Drift uses conversational AI to engage website visitors and convert them into qualified leads.\n\n\n## 3\\. Personalized Marketing and Recommendation Engines\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*24ZIbZ6OOV0UGFoW3OOxtg.png)\n\nAI\\-based recommendation engines use algorithms and machine learning to analyze user data, identify patterns, and make predictions. By combining data on past behavior with real\\-time interactions, these engines can provide highly targeted recommendations that align with user interests. The most common techniques include:\n\n* **Collaborative Filtering**: Identifies items or content that similar users have enjoyed and recommends them.\n* **Content\\-Based Filtering**: Suggests items with similar attributes to those previously shown interest in.\n* **Hybrid Models**: Combine collaborative and content\\-based filtering for more accurate and diverse recommendations.\n\n\n### Key Use Cases of Personalized Marketing and Recommendation Engines\n\n* **E\\-commerce Product Recommendations**: Online retailers suggest items based on previous purchases or browsing history, which increases the likelihood of conversion and boosts sales.\n* **Content and Media Personalization**: Streaming platforms like Netflix and Spotify use recommendation engines to suggest movies, shows, or songs, enhancing user engagement and retention.\n* **Targeted Email Campaigns**: AI personalizes email content by including product suggestions, special offers, and content relevant to each recipientâ€™s behavior, increasing open and click\\-through rates.\n* **Dynamic Web Content**: AI\\-driven engines adjust website content in real\\-time based on visitor behavior, presenting users with promotions, articles, or products that match their interests.\n\n\n### Benefits of Personalized Marketing and Recommendation Engines\n\n* **Enhanced Customer Engagement**: Personalized recommendations increase user interaction by providing relevant options, reducing the time customers spend searching.\n* **Increased Conversion Rates**: When users see products or content that closely match their interests, theyâ€™re more likely to make a purchase or engage with the content.\n* **Improved Customer Loyalty and Retention**: By making customers feel understood, brands foster a stronger connection, encouraging repeat visits and brand loyalty.\n\n\n### Top Tools for Personalized Marketing and Recommendation Engines\n\n* **Amazon Personalize**: Amazonâ€™s tool leverages machine learning to provide real\\-time recommendations and personalized content.\n* **Salesforce Einstein**: A robust AI\\-powered suite for customer relationship management (CRM) that enables personalized recommendations across multiple customer touchpoints.\n* **Algolia Recommend**: A recommendation engine that enables personalized experiences in e\\-commerce, media, and other digital environments.\n\n\n### Real\\-World Examples of Recommendation Engines in Action\n\n* **Retail**: E\\-commerce platforms like Amazon and Walmart recommend items based on user behavior, cross\\-selling and upselling items to increase order value.\n* **Streaming Services**: Netflixâ€™s recommendation engine analyzes viewing habits to suggest content, creating a seamless and engaging viewing experience.\n* **Travel and Hospitality**: Companies like Airbnb and Expedia suggest travel destinations, accommodations, and activities tailored to usersâ€™ preferences, making travel planning more relevant and efficient.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*qlu4M1HEcB3VNAm2o3FI6w.png)\n\n\n## 4\\. Intelligent Automation and RPA (Robotic Process Automation)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Bbnm4Snzk0vrhKOxLwZ_uQ.png)\n\nIntelligent automation, driven by RPA (Robotic Process Automation) and advanced AI, is transforming business operations by automating repetitive tasks, enhancing productivity, and reducing errors. Unlike traditional automation, which focuses on simple rule\\-based tasks, intelligent automation combines AI, machine learning, and RPA to handle complex workflows that require adaptability and decision\\-making. Hereâ€™s a look at how intelligent automation and RPA are changing the business landscape:\n\n\n### What is Intelligent Automation and RPA?\n\n* **Robotic Process Automation (RPA)** uses software robots or â€œbotsâ€ to automate high\\-volume, repetitive tasks such as data entry, form filling, and report generation. RPA operates based on pre\\-set rules and doesnâ€™t require cognitive decision\\-making.\n* **Intelligent Automation** integrates RPA with AI capabilities, like natural language processing (NLP) and machine learning, to manage tasks that require a higher level of reasoning and adaptability. This fusion enables bots to make contextual decisions, adapt to new data, and handle unstructured information.\n\n\n### Key Use Cases for Intelligent Automation and RPA\n\n* **Invoice Processing and Accounts Payable**: RPA can scan invoices, extract data, cross\\-check it with purchase orders, and enter it into the accounting system, reducing manual workload and improving accuracy.\n* **Customer Onboarding in Financial Services**: Intelligent automation helps banks and insurance companies streamline onboarding by automating document verification, background checks, and data entry, enhancing compliance and reducing onboarding times.\n* **Order Processing and Fulfillment**: RPA can automate order processing by pulling data from order forms, checking inventory, and initiating shipment processes, speeding up fulfillment and minimizing errors.\n* **Employee Onboarding and HR Management**: Bots can manage routine HR tasks like onboarding, scheduling interviews, and updating employee records, allowing HR teams to focus on strategic initiatives.\n\n\n### Benefits of Intelligent Automation and RPA\n\n* **Cost Savings and Efficiency**: Automation reduces the need for manual intervention, allowing companies to save on labor costs and reallocate resources toward higher\\-value tasks.\n* **Reduced Errors and Enhanced Compliance**: Bots perform tasks with high accuracy, reducing human errors, especially in data\\-intensive fields like finance and healthcare. Automated workflows also support compliance by documenting processes and ensuring regulations are followed.\n* **Scalability**: RPA allows businesses to easily scale operations by deploying more bots during peak periods without the need to hire additional staff, providing flexibility in resource management.\n\n\n### Popular RPA and Intelligent Automation Tools\n\n* **UiPath**: A widely\\-used RPA platform known for its ease of use and strong integration with machine learning tools for intelligent automation.\n* **Automation Anywhere**: Offers both RPA and cognitive automation capabilities, making it suitable for complex workflows that require decision\\-making.\n* **Blue Prism**: Focuses on secure RPA deployment, ideal for industries with high security and compliance needs, such as banking and healthcare.\n\n\n### Real\\-World Examples of Intelligent Automation in Action\n\n* **Telecoms**: Companies like AT\\&T use RPA to automate customer service inquiries, bill processing, and network monitoring, improving service response times and operational efficiency.\n* **Retail**: Retailers use RPA to streamline supply chain operations, from order fulfillment to inventory management, helping to reduce operational costs and improve customer satisfaction.\n* **Healthcare**: Hospitals and healthcare providers implement RPA to handle patient data management, appointment scheduling, and claims processing, freeing up medical staff for patient care.\n\n\n## 4\\. Fraud Detection and Security Enhancement\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*cFol51alN3qozxSH0rU0Bw.png)\n\nAI fraud detection systems use machine learning algorithms to analyze vast amounts of data, identify unusual patterns, and flag suspicious activity. Key techniques include:\n\n* **Anomaly Detection**: Identifies deviations from normal behavior, such as unusual transactions or account activity, which may indicate fraud.\n* **Behavioral Analysis**: Studies user behavior to establish a baseline and detect anomalies based on how users typically interact with the system.\n* **Predictive Modeling**: Combines historical data with machine learning models to predict the likelihood of fraudulent events and trigger alerts before fraud occurs.\n\n\n### Key Use Cases of Fraud Detection and Security Enhancement\n\n* **Financial Services**: Banks use AI to monitor transactions in real\\-time, flagging potentially fraudulent activity, such as unusual account transfers or withdrawals, and blocking them until verification is completed.\n* **Insurance Claims Processing**: Insurance companies leverage AI to detect fraudulent claims by analyzing claim patterns, identifying duplicates, and spotting inconsistencies in submitted documents.\n* **E\\-commerce and Retail**: Retailers use fraud detection algorithms to identify fraudulent purchases, stolen credit card transactions, and account takeovers to protect consumers and the business.\n* **Identity Verification and Access Control**: AI strengthens security by verifying identities through biometric authentication (such as facial recognition and fingerprint analysis) and preventing unauthorized access.\n\n\n### Benefits of AI in Fraud Detection and Security\n\n* **Real\\-Time Threat Detection**: AI allows for immediate threat detection and response, preventing fraud before it escalates and minimizing financial losses.\n* **Reduced False Positives**: By learning user patterns, AI can reduce the number of false positives that often accompany traditional fraud detection systems, ensuring genuine transactions arenâ€™t unnecessarily flagged.\n* **Improved Adaptability to New Threats**: AI systems continuously learn from new data, allowing them to adapt to evolving fraud tactics, whereas rule\\-based systems may become outdated.\n\n\n### Top Tools and Platforms for AI\\-Powered Fraud Detection\n\n* **Darktrace**: Uses AI to detect, investigate, and respond to cyber threats, leveraging machine learning to understand and protect organizational networks in real time.\n* **Splunk**: A security information and event management (SIEM) tool that combines AI\\-driven analytics with real\\-time monitoring to detect and respond to anomalies across various systems.\n* **Feedzai**: A platform designed for financial institutions that uses machine learning to monitor transactions, prevent fraud, and ensure compliance with regulatory requirements.\n\n\n### Real\\-World Examples of Fraud Detection and Security Enhancement\n\n* **Banking**: Many banks now use AI\\-driven systems to detect credit card fraud, unusual transactions, and account takeovers, significantly reducing financial losses from fraudulent activities.\n* **Healthcare**: AI is used to detect fraudulent medical billing and identity theft, saving healthcare providers and insurers millions by identifying patterns of abuse in claims processing.\n* **Online Platforms**: Social media and online marketplaces use AI to detect and prevent account takeovers, phishing, and spam, protecting users from various security threats.\n\n\n## 5\\. Supply Chain Optimization with AI\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*3pWU90X3MXQMyJ8G9rQ97g.png)\n\nAI\\-driven supply chain optimization is transforming logistics and supply chain management by improving efficiency, reducing costs, and enhancing transparency across each stage of the supply chain. From demand forecasting to inventory management, AI enables businesses to make data\\-driven decisions and respond proactively to disruptions. Hereâ€™s a deep dive into how AI is revolutionizing supply chain management and optimizing operations:\n\n\n### How AI Optimizes the Supply Chain\n\nAI tools use machine learning, predictive analytics, and real\\-time data analysis to streamline various aspects of the supply chain. By analyzing historical data, tracking real\\-time information, and predicting future trends, AI algorithms can optimize procurement, production, distribution, and logistics processes. Key approaches include:\n\n* **Predictive Analytics**: Anticipates future demands based on historical data, seasonal trends, and market factors.\n* **Machine Learning for Demand Forecasting**: Uses data on past sales, customer behavior, and external factors to generate accurate demand forecasts.\n* **Real\\-Time Monitoring and IoT Integration**: Tracks goods in real time through IoT sensors, allowing for improved visibility and rapid response to issues.\n\n\n### Key Use Cases for AI in Supply Chain Optimization\n\n* **Demand Forecasting and Planning**: AI\\-driven demand forecasting allows companies to predict customer demand more accurately, ensuring optimal stock levels and reducing instances of overstock or stockouts.\n* **Inventory Management and Optimization**: AI helps companies maintain the right inventory levels by predicting inventory needs, optimizing reordering schedules, and minimizing excess stock.\n* **Route Optimization for Logistics**: AI algorithms consider traffic, weather, and fuel costs to determine the most efficient routes, saving time and reducing transportation expenses.\n* **Supplier Risk Management**: AI enables businesses to assess supplier reliability and performance, predict risks related to supplier delays, and mitigate these risks through proactive strategies.\n* **Quality Control and Predictive Maintenance**: AI and IoT sensors can identify patterns in manufacturing equipment, predicting maintenance needs and reducing downtime, which in turn improves product quality and reduces wastage.\n\n\n### Benefits of AI\\-Driven Supply Chain Optimization\n\n* **Cost Reduction**: By improving route efficiency, inventory accuracy, and demand forecasting, AI minimizes operational costs across the supply chain.\n* **Enhanced Decision\\-Making**: AI provides real\\-time insights and forecasts, allowing decision\\-makers to respond swiftly to changes in demand, supplier performance, or logistics challenges.\n* **Greater Resilience and Flexibility**: AI\\-powered systems can adapt to unexpected disruptions, such as supplier issues or demand surges, maintaining service levels even during periods of uncertainty.\n* **Sustainability**: By reducing waste, optimizing routes, and preventing overproduction, AI contributes to sustainability goals, reducing a businessâ€™s environmental impact.\n\n\n### Top Tools and Platforms for AI\\-Driven Supply Chain Optimization\n\n* **SAP Integrated Business Planning (IBP)**: Combines machine learning with supply chain planning tools, enabling real\\-time demand forecasting and inventory optimization.\n* **Llamasoft (by Coupa)**: Uses advanced analytics for network design, inventory optimization, and demand forecasting, focusing on both strategic and operational supply chain planning.\n* **ClearMetal (by Project44\\)**: Offers real\\-time supply chain visibility and predictive analytics, enabling companies to track shipments, analyze trends, and optimize logistics routes.\n\n\n## 6\\. AI\\-Driven Product Development and Design\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*rcCCXcQq85tiFEe2M1gvzA.png)\n\nAI\\-driven product development and design is transforming how companies conceptualize, create, and bring products to market. By leveraging machine learning, generative design, and predictive analytics, AI empowers teams to innovate faster, enhance product quality, and respond effectively to changing customer needs. Hereâ€™s a comprehensive look at how AI is revolutionizing product development and design:\n\n\n### How AI is Used in Product Development and Design\n\nAI optimizes every stage of the product lifecycle, from ideation to design, prototyping, and testing. Using advanced algorithms and data analysis, AI\\-driven systems can generate product concepts, identify design flaws, and even suggest improvements. Key AI approaches include:\n\n* **Generative Design**: AI generates design options based on predefined parameters, such as material type, weight, and durability, allowing designers to explore a wider range of possibilities.\n* **Predictive Analytics for Customer Insights**: Machine learning algorithms analyze customer data and market trends to predict what features or designs will resonate with users, enhancing product\\-market fit.\n* **Natural Language Processing (NLP)**: NLP allows teams to analyze customer feedback, reviews, and social media for insights that inform product improvements and new feature ideas.\n\n\n### Key Use Cases of AI in Product Development and Design\n\n* **Concept Generation and Design Exploration**: AI assists designers by generating multiple design alternatives based on specific constraints. For example, in the automotive industry, AI can produce aerodynamic designs that improve fuel efficiency.\n* **Prototyping and Rapid Iteration**: AI\\-driven tools enable quick virtual prototyping, reducing the need for physical models and allowing rapid testing of design variations.\n* **Quality Assurance and Testing**: AI algorithms analyze product data to detect flaws early, predict potential failures, and suggest modifications, improving quality and reducing costs associated with recalls.\n* **Customer\\-Centric Product Customization**: AI\\-powered platforms allow companies to create products tailored to individual customers by analyzing preferences and generating personalized features or recommendations.\n* **Cost Optimization**: AI identifies cost\\-saving opportunities in materials, manufacturing processes, and supply chains without compromising product quality, enhancing profitability.\n\n\n### Benefits of AI\\-Driven Product Development and Design\n\n* **Accelerated Time to Market**: AI reduces the time needed for product design, prototyping, and testing, allowing companies to bring products to market faster and capitalize on emerging trends.\n* **Enhanced Innovation and Creativity**: Generative design and data\\-driven insights inspire new ideas, enabling designers to create more innovative, user\\-centered products.\n* **Improved Product Quality**: AI identifies potential design flaws early, improving product quality and reliability, which results in higher customer satisfaction.\n* **Reduced Development Costs**: By minimizing the need for physical prototypes and streamlining the design process, AI helps lower overall development costs and resources.\n\n\n### Top Tools for AI\\-Driven Product Development and Design\n\n* **Autodeskâ€™s Fusion 360 with Generative Design**: Offers generative design tools for creating optimized structures and lightweight designs, commonly used in automotive, aerospace, and industrial design.\n* **SolidWorks with AI Plugins**: Integrates AI\\-powered modules for rapid prototyping, material optimization, and design validation.\n* **Canva AI Design Tools**: Uses AI to generate layout suggestions, optimize color palettes, and suggest design templates, making it ideal for digital content and branding.\n\n\n### Real\\-World Examples of AI in Product Development\n\n* **Automotive and Aerospace**: Companies like Airbus use AI\\-powered generative design to create lightweight, aerodynamic components that reduce fuel consumption and improve sustainability.\n* **Consumer Electronics**: Tech giants use AI to identify feature preferences, enabling the creation of custom features, like battery\\-saving modes or personalized settings, based on user data.\n* **Fashion and Apparel**: Brands use AI to design custom\\-fit clothing based on body measurements, as well as analyze color and style trends to predict future demand, helping brands reduce unsold inventory.\n\n\n## 7\\. Workforce and Talent Management with AI\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*-zSlQl-pe5KJ2DuACUEeeA.png)\n\nAI integrates machine learning, natural language processing, and data analytics to improve various aspects of workforce management, including recruitment, performance management, training, and employee engagement. Key areas of AI application include:\n\n* **Recruitment and Talent Acquisition**: AI automates the screening process, analyzing resumes and applications to match candidates with the best\\-fit roles based on their skills, experience, and potential.\n* **Employee Training and Development**: AI systems deliver personalized learning experiences, recommending courses and training programs based on employee performance, career goals, and interests.\n* **Performance Monitoring and Feedback**: AI provides real\\-time insights into employee performance and engagement, offering continuous feedback to help improve productivity and identify areas for growth.\n* **Workforce Planning and Analytics**: AI analyzes workforce trends, performance data, and market conditions to help organizations plan and optimize their staffing needs.\n\n\n### Key Use Cases of AI in Workforce and Talent Management\n\n* **AI\\-Powered Recruitment**: AI algorithms automate resume parsing, candidate ranking, and initial interview assessments, ensuring faster, bias\\-free recruitment processes and a better candidate experience.\n* **Chatbots for Employee Queries**: AI chatbots assist employees with HR\\-related questions, such as benefits, payroll, and company policies, improving HR service efficiency and freeing up HR staff for more complex tasks.\n* **Personalized Employee Development**: AI\\-driven learning management systems (LMS) analyze employee performance and learning preferences, recommending tailored development paths, courses, and certifications to enhance skills.\n* **Sentiment Analysis for Employee Engagement**: AI analyzes employee feedback, surveys, and communication patterns to gauge morale, identify potential issues, and provide insights into improving workplace culture and engagement.\n* **Workforce Optimization**: AI tools analyze workforce data to help HR managers optimize scheduling, shift patterns, and resource allocation, improving productivity and employee satisfaction while reducing costs.\n\n\n### Benefits of AI in Workforce and Talent Management\n\n* **Increased Efficiency**: AI automates routine HR tasks like resume screening, scheduling interviews, and answering employee queries, saving time for HR professionals to focus on strategic decision\\-making.\n* **Enhanced Decision\\-Making**: AI\\-powered analytics provide HR managers with actionable insights, allowing them to make more informed decisions regarding hiring, promotions, and resource allocation.\n* **Improved Talent Acquisition**: AI helps identify the best candidates faster by eliminating bias in recruitment, improving diversity, and ensuring a better fit between candidates and roles.\n* **Personalized Employee Experience**: AI\\-driven tools offer personalized career development and training programs, improving employee satisfaction and retention by aligning individual goals with organizational objectives.\n* **Proactive Retention Strategies**: By analyzing employee sentiment and performance data, AI can predict potential turnover, allowing HR to intervene early with retention strategies.\n\n\n### Top Tools for AI in Workforce and Talent Management\n\n* **HireVue**: AI\\-powered video interviewing platform that analyzes candidatesâ€™ responses, tone, and facial expressions to assess qualifications, personality, and fit for a role.\n* **Workday**: A human capital management software that leverages AI for recruitment, performance management, talent development, and workforce planning, helping businesses make data\\-driven HR decisions.\n* **Ultimate Software**: Offers AI tools for payroll, employee engagement, and performance management, enhancing HR efficiency and providing actionable insights into employee performance and sentiment.\n* **Pymetrics**: Uses AI to match candidates with roles based on cognitive and emotional abilities, helping companies improve diversity and reduce hiring bias.\n\n\n### Real\\-World Examples of AI in Workforce and Talent Management\n\n* **Recruitment at Unilever**: Unilever uses AI\\-powered tools to automate candidate sourcing, screening, and interviews, improving speed and reducing unconscious bias in the hiring process.\n* **Performance Management at IBM**: IBMâ€™s AI\\-driven performance management system analyzes employee data and provides personalized feedback, helping managers track employee performance and identify development opportunities.\n* **Employee Engagement at Accenture**: Accenture uses AI\\-powered sentiment analysis to monitor employee engagement, gather feedback, and develop strategies to improve workplace culture.\n\n\n## 8\\. AI in Financial Planning and Forecasting\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*2QSFmYDlYv1eqjTXRWApNw.png)\n\nAI leverages large volumes of financial data and advanced algorithms to create more accurate, dynamic, and reliable financial models. By automating repetitive tasks, improving predictive accuracy, and providing actionable insights, AI is helping organizations make smarter financial decisions. Key AI applications include:\n\n* **Predictive Analytics**: AI predicts future financial performance by analyzing historical data, identifying trends, and assessing market conditions. This enables more accurate forecasting and budgeting.\n* **Real\\-Time Financial Monitoring**: AI\\-driven tools provide real\\-time tracking of financial data, helping businesses quickly detect anomalies, forecast cash flow, and monitor budget adherence.\n* **Scenario Planning**: AI can simulate different economic conditions or business scenarios, helping financial planners assess the impact of various strategies and decisions on the companyâ€™s bottom line.\n\n\n### Key Use Cases of AI in Financial Planning and Forecasting\n\n* **Budgeting and Cash Flow Forecasting**: AI tools analyze past financial data, customer behavior, and market conditions to generate cash flow projections, ensuring businesses have sufficient liquidity and can plan for future expenses.\n* **Revenue Forecasting**: AI models can predict future revenues based on historical performance, seasonal trends, and customer behavior, enabling companies to allocate resources more effectively.\n* **Expense Management and Optimization**: AI identifies areas where costs can be reduced or optimized by analyzing spending patterns and suggesting cost\\-saving measures.\n* **Credit Risk Assessment**: AI\\-powered credit scoring models analyze a broad range of financial and non\\-financial data, providing more accurate risk assessments for lenders, investors, and businesses.\n* **Investment Analysis and Portfolio Management**: AI tools analyze vast amounts of market data to identify investment opportunities, assess portfolio performance, and optimize asset allocation.\n\n\n### Benefits of AI in Financial Planning and Forecasting\n\n* **Improved Accuracy**: AI algorithms process large datasets and recognize patterns that humans may miss, leading to more accurate financial forecasts, predictions, and budgets.\n* **Faster Decision\\-Making**: AI speeds up financial analysis, enabling faster decision\\-making and more agile responses to market changes, cash flow needs, or unexpected financial events.\n* **Increased Efficiency**: By automating routine tasks such as data entry, analysis, and report generation, AI frees up time for financial professionals to focus on strategic decision\\-making and scenario planning.\n* **Risk Management**: AIâ€™s ability to analyze vast amounts of data helps identify potential risks and financial challenges early, allowing companies to take proactive measures to mitigate them.\n* **Enhanced Strategic Insights**: AI provides deeper insights into financial data, offering recommendations on cost\\-saving opportunities, revenue growth strategies, and overall financial health.\n\n\n### Top Tools for AI in Financial Planning and Forecasting\n\n* **Adaptive Insights**: A cloud\\-based financial planning tool that uses AI to help businesses forecast and model financial scenarios, track performance, and adjust plans based on real\\-time data.\n* **Anaplan**: AI\\-powered software that provides integrated financial planning, budgeting, and forecasting solutions, allowing businesses to connect financial data and optimize decision\\-making.\n* **Planful (formerly Host Analytics)**: An AI\\-driven platform for financial planning, analysis, and forecasting, offering powerful modeling, budgeting, and reporting capabilities for finance teams.\n* **Kensho**: An AI\\-powered analytics tool used by financial institutions to enhance forecasting, risk assessment, and financial planning by analyzing vast amounts of financial data and providing predictive insights.\n* **Xero**: A cloud\\-based accounting platform with AI\\-driven features that assist businesses with real\\-time cash flow tracking, forecasting, and financial reporting.\n\n\n### Real\\-World Examples of AI in Financial Planning and Forecasting\n\n* **Corporate Budgeting at Vodafone**: Vodafone uses AI to improve its financial planning process by automating budget creation, optimizing cash flow forecasting, and providing deeper insights into cost allocation across global operations.\n* **Revenue Forecasting at Netflix**: Netflix leverages AI to analyze customer data and predict subscriber growth, helping the company plan its financial strategy and allocate resources effectively across various content production and marketing efforts.\n* **Investment Management at BlackRock**: BlackRock employs AI and machine learning algorithms to manage investment portfolios and optimize asset allocation, providing clients with more accurate and data\\-driven investment strategies.\n\n\n## Conclusion\n\nAs we move into 2025, AI is set to be the backbone of many successful businesses. By automating repetitive tasks, enhancing customer experience, and providing data\\-driven insights, AI empowers organizations to make smarter, faster decisions. AIâ€™s role in predictive analytics, cybersecurity, and personalized marketing will continue to drive innovation and allow businesses to meet changing consumer demands more effectively.\n\nThe future of business lies in the [**integration of AI technologies**](https://www.blockchainappfactory.com/ai-development-company), and those who embrace these advancements will be at the forefront of their industries. While there are challenges to adopting AI, such as concerns around data privacy and job displacement, the benefits far outweigh the risks. With the right strategies in place, businesses can harness AIâ€™s full potential to thrive in the competitive landscape of 2025\\.\n\n\n## FAQs\n\n1. **What are the top AI use cases for businesses in 2025?**\nIn 2025, AI will be used for predictive analytics, personalized marketing, automation, fraud detection, customer support chatbots, and supply chain optimization.\n2. **How can AI improve customer experience?**\nAI can enhance customer experience through personalized recommendations, AI\\-powered chatbots for instant support, and predictive tools that anticipate customer needs.\n3. **Is AI likely to replace human workers in business?**\nAI is more likely to augment human workers by automating repetitive tasks, allowing employees to focus on higher\\-level decision\\-making and creative work.\n4. **What industries benefit the most from AI in 2025?**\nIndustries like finance, healthcare, retail, manufacturing, and logistics are expected to see significant benefits from AI, including improved efficiency and cost savings.\n5. **What are the challenges businesses face when adopting AI?**\nBusinesses may face challenges related to data privacy, integration with existing systems, high initial investment, and ensuring AI systems are used ethically.\n\n"},{"lang":"en","group":"blog","slug":"blog/unified-memory-across-chatgpt-claude-perplexity-24809dc56717","frontmatter":{"title":"Unified Memory Across ChatGPT, Claude, Perplexity","meta_title":"Unified Memory Across ChatGPT, Claude, Perplexity","description":"Youâ€™ll definitely love this one, especially if youâ€™re already locked in with Claude, ChatGPT and Perplexity.","date":"2024-11-08T00:24:33.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*qrHnDS6YODZuVXZ4eeWZrQ.png","categories":["Chatbots","Programming/Scripting","Technology/Web"],"author":"Rifx.Online","tags":["Mem0","Chrome","extension","context","memory"],"draft":false,"slug":"blog/unified-memory-across-chatgpt-claude-perplexity-24809dc56717"},"content":"\nYouâ€™ll definitely love this one, especially if youâ€™re already locked in with Claude, ChatGPT and Perplexity.\n\nInteracting with different AI assistants can sometimes feel a bit disjointed.\n\nYou have to repeat the same context over and over when switching between ChatGPT, Claude, Perplexity, and others.\n\nWouldnâ€™t be great if all could share one universal memory to enhance the context?\n\nI found this awesome Chrome extension, and itâ€™s been a total life saver for me.\n\nImagine having a seamless conversation where the context carries over, no matter which AI youâ€™re chatting with.\n\nSounds cool, doesnâ€™t it?\n\n\n\n### The Big Problem It Solves\n\nBefore I dive into it, how are we even keeping LLMs updated with the latest knowledge?\n\nThere are a few traditional ways to tackle this:\n\n1. **Retrieval\\-Based Methods**: These pull information from a knowledge base. Theyâ€™re powerful but can get messy with redundant data and the hassle of managing an ever\\-growing repository.\n2. **Model Editing**: This tweaks the model to adapt to new facts. It works for simple, single\\-sentence updates but struggles with longer, more complex information.\n3. **Long Context Methods**: These cram all the knowledge into the modelâ€™s context. Itâ€™s like overloading the model with data, which isnâ€™t practical because the context length is limited.\n\nAll these methods have their downsides, especially when we need up\\-to\\-date, seamless interactions across different AI platforms.\n\n### Enter Mem0: Intelligent Memory Retrieval for AI Assistants\n\nMem0 sidesteps these issues by creating a universal memory layer that works across multiple AI assistants.\n\nHereâ€™s what it brings to the table:\n\n* **Universal Memory Layer**: Share context effortlessly across ChatGPT, Claude, Perplexity, and more. No more repeating yourself!\n* **Smart Context Detection**: It automatically picks up relevant information from your conversations, so you donâ€™t have to manually input anything.\n* **Intelligent Memory Retrieval**: Mem0 brings up the right memories at the right time, making your interactions smoother.\n* **One\\-Click Sync with ChatGPT**: If youâ€™ve been using ChatGPT, you can sync your existing memories with just one click.\n* **Memory Dashboard**: A handy place to manage all your memories in one spot.\n\nFor example, if you start a conversation with Claude:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*QQAJPzQp2tjBFgi-9InG1Q.png)\n\nIt will extract key information and add it to memory.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MtctEzunD72Tw_dOOCOnyA.png)\n\nThen the next time you ask a relevant question:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*r0WyCIHGvmiGm0GZ6xR1pw.png)\n\nIt will add the relevant context:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*PTEhFUJ4nrhQbeZXIAnD6A.png)\n\nbut hereâ€™s what more interesting.\n\nIf you open Perplexity and ask another question:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*54BKW9BQWwCDdXdAg0U-mA.png)\n\nIt will get the memory across different applications and enhance the context:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*FD6MwGtL8WpovUH3o1Vw3g.png)\n\nWhich is pretty cool!\n\nIf you open ChatGPT and ask a question:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*qLmDZh5NofuArxzS4OnDIQ.png)\n\nSimilarly, it will bring relevant information from memory and enhance the context.\n\nAs you work more with it, it will collect key information and save you lots of typing while you interact with Claude, Perplexity and ChatGPT.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MqgEr6hi5cHzHRuTqODzuA.png)\n\nYou can also add new information to memory:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Xo6jLLIGKqxvYR9SivuMhQ.png)\n\n### How to Get Started\n\nInstalling Mem0 is super easy:\n\n1. [**Add the extension to Chrome**](https://chromewebstore.google.com/detail/mem0/onihkkbipkfeijkadecaafbgagkhglop?hl=en-GB)\n\n**2\\. Sign In**:\n\n* After itâ€™s installed, youâ€™ll see the Mem0 icon in your toolbar.\n* Click it and sign in with Google.\n\n**4\\. Start Chatting**:\n\n* Use any of the supported AI assistants.\n* For ChatGPT and Perplexity, just chat as you normally would.\n* On Claude, click the Mem0 button or use the shortcut `^ + M`.\n\nOne of the best things about Mem0 is that itâ€™s completely free. There are:\n\n* **No usage limits**\n* **No ads**\n* **All features included**\n\n\n"},{"lang":"en","group":"blog","slug":"blog/unlocking-mixture-of-experts-moe-llm-your-moe-model-can-be-embedding-model-for-free-f192b9c07a5f","frontmatter":{"title":"Unlocking Mixture-of-Experts (MoE) LLMÂ : Your MoE model can be embedding model for free","meta_title":"Unlocking Mixture-of-Experts (MoE) LLMÂ : Your MoE model can be embedding model for free","description":"Mixture-of-experts (MoE) LLM can be used as an embedding model for free.","date":"2024-11-04T12:30:57.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*mB6VhEyAvxAxGbLDG_6hTw.png","categories":["Machine Learning","Natural Language Processing","Data Science"],"author":"Rifx.Online","tags":["Mixture-of-Experts","MoE","embedding","MoEE","BERTopic"],"draft":false,"slug":"blog/unlocking-mixture-of-experts-moe-llm-your-moe-model-can-be-embedding-model-for-free-f192b9c07a5f"},"content":"\n### Mixture\\-of\\-experts (MoE) LLM can be used as an embedding model for free.\n\n\n\nI recently found an interesting paper titled â€œYour Mixture\\-of\\-Experts LLM is Secretly an Embedding Model for Free.â€ \\[1] A recent LLM architecture trend is a decoder model, which is unsuitable for an embedding model because of their attention method. However, the authors revealed that Mixture\\-of\\-Experts (MoE) LLMs can perform as an embedding model to apply a diverse class of embedding\\-focused tasks without any further fine\\-tuning. In this blog, firstly, letâ€™s recap MoE, and I will introduce how it works and its practical implementation.\n\n## Table of Contents\n\n1. What is Mixture-of-Experts (MoE)?\n2. How MoE works as an embedding model?\n3. Practical implementation : Leverage MoEE with BERTopic\n\n## 1\\. What is Mixture\\-of\\-Experts (MoE) ?\n\nMixture\\-of\\-Experts (MoE) is an architecture with multiple subnetworks called â€œexperts,â€ each specializing in different tasks or aspects of data. One of MoEâ€™s advantages is that it enables AI models to be pretrained with less computation than the same or larger models while maintaining or increasing quality. So, if we have a limited budget, we can achieve a better model using MoE than the dense, similar\\-size conventional model. For recent success, Mixtral 8 x 7B outperforms the LLaMA 2 70B for many evaluation datasets.\n\nFrom now on, letâ€™s examine the architecture of MoE. Recent successful MoEs use the transformer model, so I will focus on the popular MoE architecture for the transformer. MoE has mainly two components described below.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Dia_c08PJnFeeIc9lxwtGQ.png)\n\n* **MoE layers**\n\nMoE replaces the feed\\-forward network (FFN) layers with MoE layers in the transformer architecture. Each MoE layer has some experts (Ex. 4 experts in the above illustration), and each expert is composed of the simple FFN layer. Note that other components in the transformer, such as the self\\-attention layer, share the same weights. Therefore, the number of weights of MoE is not straightforward. For example, the Mixtral 8 x 7B weight is not 8 x 7 \\= 56B but 47B because the other layers besides MoE layers share the same weights.\n\n* **Gating network**\n\nA gating network or router is a crucial component in MoE. It takes input tokens and selects the most relevant experts for each token. For instance, in the above illustration, the left side of the router chooses the second expert to process the word â€œmoreâ€ token. Meanwhile, the router determines the first expert to process the word â€œParametersâ€ token. Generally, a gating network chooses the top\\-k experts relevant to the given token and sends the token to chosen experts; for example, Mixtral 8 x 7B chooses top\\-2 experts.\n\nHow can we choose the top\\-k experts? We use the softmax function to calculate the expertâ€™s importance probabilities and keep top\\-k probability experts, as shown below. I extracted the gating part of the above illustration.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*qX9H2KKtjntVuiE8yFstMQ.png)\n\nA gating network has its weight. We apply the softmax function to the result of the dot\\-product between the input word token and the weight of a gating network, then get the probability of how much the expert is relevant to the given token. Based on the probability, we can select top\\-k relevant experts. MoE, which has this type of gating network, is called sparse MoE.\n\nThese are the fundamental knowledge needed to understand how MoE works as an embedding model. For further understanding, I recommend reading [this blog](https://huggingface.co/blog/moe) \\[2]. Now, letâ€™s dive into how MoE actually works as an embedding model.\n\n## 2\\. How MoE works as an embedding model?\n\n### Quick recap about embeddings\n\nBefore diving into the theme of this section, letâ€™s quickly recap about embeddings. Recently, embedding has been the internal representation of input data in deep learning models, and it has semantics and condensed data information. We usually extract the last hidden state of the neural network as an embedding, as shown below.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kSHFTEejKiSI51taKZCO9A.png)\n\nWe typically use encoder\\-based models to extract embeddings because they can capture semantics with bi\\-directional attention compared to decoder\\-only models. Decoder\\-only models often use causal attention to interact with only the previous word tokens; thus, they cannot capture the rich semantics, such as contextualized information, like encoder\\-decoder models.\n\n### How MoE works as an embedding model?\n\nIt was a common belief that the decoder model could not be used for embedding extraction. However, the authors found that the routing weight in the MoE provides complementary information to the decoder embedding. The routing weight in each layer reflects the reasoning choice on the input token, so it contains the inputâ€™s semantic information that hidden stateâ€™s embedding may lost. In the mathematical formula, we can describe it as:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*n6wGCMqAhjBAfLFV47ML1g.png)\n\n*g* is the softmax function and *H* means the hidden state. We concatenate all the MoE layerâ€™s routing weights to avoid losing modelâ€™s reasoning choice.\n\nTo fully utilize the routing weights and decoder embedding, the authors proposed a method called MoE Embedding (MoEE) to form a more comprehensive embedding representation. There are two types of MoEE. One method is a concatenation\\-based combination, described below.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*uVmcV-lM83XL7HoYbYjt7w.png)\n\nThis method is simple, and we just concatenate routing weights and decoder embedding. The authors call this method as MoEE(concat). It can preserve the distinct information captured by each routing weight while allowing downstream tasks to leverage the combined representation.\n\nThe other method is weighted sum integration. It performs a weighted sum of the similarity scores calculated from routing weights and hidden state (HS) embedding, denoted as MoEE (sum). This method is used for tasks that compare two sentences, such as semantic textual similarity.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kyJxWW9zdgRyNr2jmO4LlQ.png)\n\nð›‚ is a hyperparameter to control the contribution of the routing weights. After calculating the similarity score for each pair, we compute the rank correlation, such as Spearmanâ€™s rank correlation, between the calculated similarity score and the ground truth similarity.\n\nFor practical usage, I think that the MoEE(concat) is easy to use. Moreover, the authors leverage the PromptEOL technique \\[4] to enhance MoEE. This technique prompts the following template to constrain LLMs in predicting semantic information in the next token.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*S9BASj9JkQe-i4fqmbopWg.png)\n\nNow, here is the performance table across MTEB tasks.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*7LxkEMR2DFlncypF6_T7Vw.png)\n\nMoEE with PromptEOL can work better than supervised and self\\-supervised methods. Note that this leaderboard is not the latest one, so this result is not SOTA. The value of this method is that we can obtain decent results for embedding tasks, and it can be used without any further training.\n\nWe have covered how MoEE works so far. In the next section, we will implement MoEE with BERTopic and cluster sentences.\n\n## 3\\. Practical implementation : Leverage MoEE with BERTopic\n\nIn this section, we extract embeddings from pre\\-trained MoE LLM and leverage them with [BERTopic](https://maartengr.github.io/BERTopic/index.html) using a 20\\-news\\-group dataset \\[5]. For your information, BERTopic is a convenient topic modeling library beyond conventional statistical topic modeling. It leverages embeddings from Transformer to make topic clustering, so I think it is suitable for checking the capability. First of all, letâ€™s prepare an environment.\n\n### Environment setup\n\nI used a conda environment with Python 3\\.10\\. I experimented on Ubuntu 20\\.04 with cuda 12\\.4, 16 GB VRAM. You may need 32 GB RAM for downloading model weights.\n\n```python\nconda create -n moee python=3.10 -y\nconda activate moee\n```\n\nNext, we need to install the libraries below via pip.\n\n```python\npip install transformers torch bitsandbytes bertopic accelerate\n```\n\nMoE models need generally high VRAM because we need to load the entire model to our VRAM in advance. Therefore, we require using bitsandbytes, which is a quantization package, to save VRAM memory.\n\nWe need to clone the official GitHub repository.\n\n```python\ngit clone https://github.com/tianyi-lab/MoE-Embedding.git\n```\n\nAll preparation is done. Now, letâ€™s implement topic clustring with BERTopic using MoEE.\n\n### Leverage MoEE with BERTopic\n\nNow, we will use MoEE as an embedding model for BERTopic and try topic clustering. The original repository allows us to use small MoE models, such as Qwen\\-1\\.5\\-MoE\\-A2\\.7B or OLMoE\\-1B\\-7B. In this blog, I will use OLMoE\\-1B\\-7B, which is affordable for running inference on 16 GB VRAM. Firstly, we need to load OLMoE\\-1B\\-7B.\n\n```python\nkwargs = {\n        \"base_model\": 'allenai/OLMoE-1B-7B-0924',\n        \"normalized\": False,\n        \"torch_dtype\": torch.bfloat16,\n        \"mode\": \"embedding\",\n        \"pooling_method\": \"mean\",\n        \"attn_implementation\": \"sdpa\",\n        \"attn\": \"bbcc\",\n    }\n\nconfig = {\n    'embed_method': 'prompteol',\n    'emb_info': 'MoEE'\n    }\n\nembedding_model = MOEE(model_name_or_path='allenai/OLMoE-1B-7B-0924', **kwargs)\n```\n\nNext, we need to calculate embeddings of 20\\-news\\-group dataset to pass BERTopic. (I will attach full code later.)\n\n```python\nfrom sklearn.datasets import fetch_20newsgroups\n\ndocs = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))['data']\n\ndataset = MyDataset(docs)\ndataloader = DataLoader(dataset=dataset, batch_size=8)\nembeddings = None\n\nfor batch in tqdm(dataloader):\n    with torch.no_grad():    \n        embedding = embedding_model.encode(batch, **config)\n      \n        if embeddings is None:\n            embeddings = embedding[0]\n        else:\n            embeddings = np.vstack((embeddings, embedding[0]))\n  \n    torch.cuda.empty_cache()\n```\n\nTo calculate embeddings in advance, we use torch.utils.data.DataLoader for an iterator, and encode each batched document. Note that we must pass embeddings as np.asarray type to the BERTopic.\n\nWhen you want to use your own MoE models, you must implement to get the routing weights from each MoE layer. For the hidden state embedding, we can utilize the HuggingFace transformer function. We only need to pass the output\\_hidden\\_states\\=True argument when inference.\n\nNow, we can run topic modeling.\n\n```python\n## Step 2 - Reduce dimensionality\numap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')\n\n## Step 3 - Cluster reduced embeddings\nhdbscan_model = HDBSCAN(min_cluster_size=15, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n\n## Step 4 - Tokenize topics\nvectorizer_model = CountVectorizer(stop_words=\"english\")\n\n## Step 5 - Create topic representation\nctfidf_model = ClassTfidfTransformer()\n\n## Step 6 - (Optional) Fine-tune topic representations with \n## a `bertopic.representation` model\nrepresentation_model = KeyBERTInspired()\n\n## All steps together\ntopic_model = BERTopic(\n  embedding_model=embedding_model,          # Step 1 - Extract embeddings\n  umap_model=umap_model,                    # Step 2 - Reduce dimensionality\n  hdbscan_model=hdbscan_model,              # Step 3 - Cluster reduced embeddings\n  vectorizer_model=vectorizer_model,        # Step 4 - Tokenize topics\n  ctfidf_model=ctfidf_model,                # Step 5 - Extract topic words\n  representation_model=representation_model # Step 6 - (Optional) Fine-tune topic representations\n)\n\n## topic modeling using BERTopic model\ntopics, probs = topic_model.fit_transform(docs, embeddings)\n```\n\nWe got 42 topics by the default setting; some samples are shown below. Even though I picked up topics randomly, it can capture the semantics very well.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*VIaKHU-PSuTPzOUKDFbwOw.png)\n\nMoreover, here is the topic cluster visualization.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*KYAUOe2qEAv-ihq2S2dM0A.png)\n\nPlease look at the red circle in the topic cluster visualization. This red circle refers to topic 0, which is related to the computer. Closer topics are also associated with mechanic words, such as graphics, digital, and printers.\n\nThis method shows us that we can get decent embeddings without any training. Although there is still room to improve the quality to be equivalent to the SOTA\\-supervised models, this paperâ€™s finding is a good step for further improvement of the embedding extracting method without training.\n\nHere is my entire code. You need to put this file into the top of the MoE\\-Embedding directory.\n\n## References\n\n\\[1] Ziyue Li, Tianyi Zhou, [YOUR MIXTURE\\-OF\\-EXPERTS LLM IS SECRETLY AN EMBEDDING MODEL FOR FREE](https://arxiv.org/pdf/2410.10814) (2024\\), *Arxiv*\n\n\\[2] Omar S., et.al., [Mixture of Experts Explained](https://huggingface.co/blog/moe) (2023\\), Hugging Face\n\n\\[3] William Fedus, Barret Zoph., et.al., [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/pdf/2101.03961) (2021\\), *Arxiv*\n\n\\[4] Ting Jiang, et.al., [Scaling Sentence Embeddings with Large Language Models](https://arxiv.org/pdf/2307.16645) (2023\\), *Arxiv*\n\n\\[5] [20 News groups](http://qwone.com/~jason/20Newsgroups/)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/using-llama-3-for-building-ai-agents-7e74f79d1ccc","frontmatter":{"title":"Using Llama 3 for Building AI Agents","meta_title":"Using Llama 3 for Building AI Agents","description":"Comprehensive guide to building AI Agents with Llama 3 function calling capabilities.","date":"2024-11-10T03:51:17.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*EWGo-7t4Kl6l82rB2-ZK9Q.png","categories":["Programming","Generative AI","Chatbots"],"author":"Rifx.Online","tags":["Llama","Gradio","RAG","metadata","indexing"],"draft":false,"slug":"blog/using-llama-3-for-building-ai-agents-7e74f79d1ccc"},"content":"\n\n\n\n\n### Comprehensive guide to building AI Agents with Llama 3 function calling capabilities.\n\n\n\n\n### Introduction\n\nImagine you want to buy something. You visit an e\\-commerce website and use the search option to find what you want. Maybe you have multiple items to buy, so the process isnâ€™t very efficient. Now consider this scenario: open an application, describe what you want in plain English, and press enter. You don't have to worry about searching and price comparisons because the application handles it automatically for you. Pretty cool, right? Thatâ€™s exactly what weâ€™ll build in this tutorial.\n\nLetâ€™s look at some examples first.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ikbr1ozv37PIB2meVfCCfA.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*AZPn3_KCDRV0pAszd3vLmA.png)\n\nAlright, letâ€™s bring life to this application. Weâ€™re going to use Metaâ€™s Llama 3 model with function calling capability. However, this can also be accomplished using the 3\\.1 models. According to [Metaâ€™s announcement](https://ai.meta.com/blog/meta-llama-3-1/), the 3\\.1 models can use tools and functions more effectively.\n\n\n> These are multilingual and have a significantly longer context length of 128K, state\\-of\\-the\\-art tool use, and overall stronger reasoning capabilities\n\nI will use Groq Cloud, specifically their model for this article. The initial workflow of this application should consist of an embedding model, a retriever, and two major tools for handling user purchase interests and cost\\-related concerns. In summary, we need something similar to what we've described in the diagram below.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*EZVySX3GD2O07fzEPwLcbQ.png)\n\nNow we have to use an LLM orchestration framework. For that, I am picking my all\\-time favorite, [Haystack](https://haystack.deepset.ai/).\n\nOkay, we got what we need. Letâ€™s jump into the actual work!\n\n\n### Loading and indexing data\n\nSince we have an RAG pipeline, we should build a document indexing service as the first step. For this demo, I am going to use the in\\-memory vector database that Haystack offers. Please note that each document in our vector database contains,\n\n* Content â€” Which we used to perform a similarity search\n* Id â€” Unique identifier\n* Price â€” Product price\n* URL â€” Product URL\n\nWhen our RAG pipeline is invoked, the Content field is used for vector search. All other fields are included as metadata. Itâ€™s crucial to preserve this metadata as itâ€™s essential for front\\-end presentation to the user.\n\nLetâ€™s see how we can implement that.\n\n\n```python\nfrom haystack import Pipeline, Document\nfrom haystack.document_stores.in_memory import InMemoryDocumentStore\nfrom haystack.components.writers import DocumentWriter\nfrom haystack.components.embedders import SentenceTransformersDocumentEmbedder\nfrom haystack.components.generators import OpenAIGenerator\nfrom haystack.utils import Secret\nfrom haystack.components.generators.chat import OpenAIChatGenerator\nfrom haystack.components.builders import PromptBuilder\nfrom haystack.components.embedders import SentenceTransformersTextEmbedder\nfrom haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\nfrom haystack.dataclasses import ChatMessage\nimport pandas as pd\n\n## Load product data from CSV\ndf = pd.read_csv(\"product_sample.csv\")\n\n## Initialize an in-memory document store\ndocument_store = InMemoryDocumentStore()\n\n## Convert the product data into Haystack Document objects\ndocuments = [\n    Document(\n        content=item.product_name, \n        meta={\n            \"id\": item.uniq_id, \n            \"price\": item.selling_price, \n            \"url\": item.product_url\n        }\n    ) for item in df.itertuples()\n]\n\n## Create a pipeline for indexing the documents\nindexing_pipeline = Pipeline()\n\n## Add a document embedder to the pipeline using Sentence Transformers model\nindexing_pipeline.add_component(\n    instance=SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\"), name=\"doc_embedder\"\n)\n\n## Add a document writer to the pipeline to store documents in the document store\nindexing_pipeline.add_component(instance=DocumentWriter(document_store=document_store), name=\"doc_writer\")\n\n## Connect the embedder's output to the writer's input\nindexing_pipeline.connect(\"doc_embedder.documents\", \"doc_writer.documents\")\n\n## Run the indexing pipeline to process and store the documents\nindexing_pipeline.run({\"doc_embedder\": {\"documents\": documents}})\n```\nGreat, weâ€™ve completed the first step of our AI agent application. Now itâ€™s time to build the product identifier tool. To better understand the primary task of the product identifier, letâ€™s consider the example below.\n\n\n> User Query: I want to buy a camping boot, a charcoal and google pixel 9 back cover. Letâ€™s understand our ideal workflow for the product identifier function.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kXGYjlMi4pQcqIKpmUZLRQ.png)\n\nFirst, we need to create a tool for analyzing user queries and identifying user\\-interested products. We can build such a tool using code snippets below.\n\n\n### Building User Query Analyzer\n\n\n```python\ntemplate = \"\"\"\nUnderstand the user query and list of products the user is interested in and return product names as list.\nYou should always return a Python list. Do not return any explanation.\n\nExamples:\nQuestion: I am interested in camping boots, charcoal and disposable rain jacket.\nAnswer: [\"camping_boots\",\"charcoal\",\"disposable_rain_jacket\"]\n\nQuestion: Need a laptop, wireless mouse, and noise-cancelling headphones for work.\nAnswer: [\"laptop\",\"wireless_mouse\",\"noise_cancelling_headphones\"]\n\nQuestion: {{ question }}\nAnswer:\n\"\"\"\n\nproduct_identifier = Pipeline()\n\nproduct_identifier.add_component(\"prompt_builder\", PromptBuilder(template=template))\nproduct_identifier.add_component(\"llm\", generator())\n\nproduct_identifier.connect(\"prompt_builder\", \"llm\")\n```\nOkay, now we have completed half of our first function, now itâ€™s time to complete the function by adding the RAG pipeline.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*JyxINdc8Wz-qAg_PCAkLbA.png)\n\n\n### Creating RAG Pipeline\n\n\n```python\ntemplate = \"\"\"\nReturn product name, price, and url as a python dictionary. \nYou should always return a Python dictionary with keys price, name and url for single product.\nYou should always return a Python list of dictionaries with keys price, name and url for multiple products.\nDo not return any explanation.\n\nLegitimate Response Schema:\n{\"price\": \"float\", \"name\": \"string\", \"url\": \"string\"}\nLegitimate Response Schema for multiple products:\n[{\"price\": \"float\", \"name\": \"string\", \"url\": \"string\"},{\"price\": \"float\", \"name\": \"string\", \"url\": \"string\"}]\n\nContext:\n{% for document in documents %}\n    product_price: {{ document.meta['price'] }}\n    product_url: {{ document.meta['url'] }}\n    product_id: {{ document.meta['id'] }}\n    product_name: {{ document.content }}\n{% endfor %}\nQuestion: {{ question }}\nAnswer:\n\"\"\"\n\nrag_pipe = Pipeline()\nrag_pipe.add_component(\"embedder\", SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\"))\nrag_pipe.add_component(\"retriever\", InMemoryEmbeddingRetriever(document_store=document_store, top_k=5))\nrag_pipe.add_component(\"prompt_builder\", PromptBuilder(template=template))\nrag_pipe.add_component(\"llm\", generator())\n\nrag_pipe.connect(\"embedder.embedding\", \"retriever.query_embedding\")\nrag_pipe.connect(\"retriever\", \"prompt_builder.documents\")\nrag_pipe.connect(\"prompt_builder\", \"llm\")\n```\nAfter this stage, we have completed both RAG and Query Analyzer pipelines. Now itâ€™s time to convert this into a tool. For that, we can use a regular function declaration, as shown below. Creating a tool for the Agent is just like creating a Python function. In case you have a question like\n\n\n> How is it possible for the Agent to invoke this function?\n\nThe solution is straightforward: by leveraging a model\\-specific tool schema, which we plan to incorporate in a future step. For now, itâ€™s time to create a wrapper function that uses both the query analyzer and RAG pipeline.\n\nLetâ€™s clarify the objectives of this function.\n\n**Objective 1:** Identify all products the user is interested in and return them as a list. **Objective 2:** For each identified product, retrieve up to five products from the database along with their metadata.\n\n\n### Finalizing Product Identifier Function\n\n\n```python\ndef product_identifier_func(query: str):\n    \"\"\"\n    Identifies products based on a given query and retrieves relevant details for each identified product.\n\n    Parameters:\n    query (str): The query string used to identify products.\n\n    Returns:\n    dict: A dictionary where the keys are product names and the values are details of each product. If no products are found, returns \"No product found\".\n    \"\"\"\n    product_understanding = product_identifier.run({\"prompt_builder\": {\"question\": query}})\n\n    try:\n        product_list = literal_eval(product_understanding[\"llm\"][\"replies\"][0])\n    except:\n        return \"No product found\"\n\n    results = {}\n\n    for product in product_list:\n        response = rag_pipe.run({\"embedder\": {\"text\": product}, \"prompt_builder\": {\"question\": product}})\n        try:\n            results[product] = literal_eval(response[\"llm\"][\"replies\"][0])\n        except:\n            results[product] = {}\n    \n    return results\n```\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*HWRTdWvvcw2MZP4uoaQdeQ.png)\n\nWith that, we have completed our first tool for the agent. Letâ€™s see whether it works as expected.\n\n\n```python\nquery = \"I want crossbow and woodstock puzzle\"\n#execute function\nproduct_identifier_func(query)\n\n## {'crossbow': {'name': 'DB Longboards CoreFlex Crossbow 41\" Bamboo Fiberglass '\n##                        'Longboard Complete',\n##                'price': 237.68,\n##                'url': 'https://www.amazon.com/DB-Longboards-CoreFlex-Fiberglass-Longboard/dp/B07KMVJJK7'},\n##  'woodstock_puzzle': {'name': 'Woodstock- Collage 500 pc Puzzle',\n##                       'price': 17.49,\n##                       'url': 'https://www.amazon.com/Woodstock-Collage-500-pc-Puzzle/dp/B07MX21WWX'}}\n```\nIt worked!! However, itâ€™s worth noting the return output schema. You can see the general schema below.\n\n\n```python\n{\n    \"product_key\": {\n        \"name\": \"string\",\n        \"price\": \"float\",\n        \"url\": \"string\"\n    }\n}\n```\nThatâ€™s exactly what we have advised the model to produce in the RAG pipeline. As a next step, letâ€™s build an optional tool called `find_budget_friendly_option`.\n\n\n```python\ndef find_budget_friendly_option(selected_product_details):\n    \"\"\"\n    Finds the most budget-friendly option for each category of products.\n\n    Parameters:\n    selected_product_details (dict): A dictionary where the keys are product categories and the values are lists of product details. Each product detail is expected to be a dictionary containing a 'price' key.\n\n    Returns:\n    dict: A dictionary where the keys are product categories and the values are the most budget-friendly product details for each category.\n    \"\"\"\n    budget_friendly_options = {}\n    \n    for category, items in selected_product_details.items():\n        if isinstance(items, list):\n            lowest_price_item = min(items, key=lambda x: x['price'])\n        else:\n            lowest_price_item = items\n        \n        budget_friendly_options[category] = lowest_price_item\n    \n    return budget_friendly_options\n```\nOkay, let's focus on the most crucial aspect of this application, which is enabling the agent to use these functions as needed. As we previously talked about, this is achievable through a model\\-specific tool schema. Therefore, we need to locate the tool schema specific to the selected model. Fortunately, it's mentioned in the model card [here](https://huggingface.co/Groq/Llama-3-Groq-70B-Tool-Use). We need to adjust that to fit our use case.\n\n\n### Finalizing Chat Template\n\n\n```python\nchat_template = '''<|start_header_id|>system<|end_header_id|>\n\nYou are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n<tool_call>\n{\"name\": <function-name>,\"arguments\": <args-dict>}\n</tool_call>\n\nHere are the available tools:\n<tools>\n    {\n        \"name\": \"product_identifier_func\",\n        \"description\": \"To understand user interested products and its details\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"query\": {\n                    \"type\": \"string\",\n                    \"description\": \"The query to use in the search. Infer this from the user's message. It should be a question or a statement\"\n                }\n            },\n            \"required\": [\"query\"]\n        }\n    },\n    {\n        \"name\": \"find_budget_friendly_option\",\n        \"description\": \"Get the most cost-friendly option. If selected_product_details has morethan one key this should return most cost-friendly options\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"selected_product_details\": {\n                    \"type\": \"dict\",\n                    \"description\": \"Input data is a dictionary where each key is a category name, and its value is either a single dictionary with 'price', 'name', and 'url' keys or a list of such dictionaries; example: {'category1': [{'price': 10.5, 'name': 'item1', 'url': 'http://example.com/item1'}, {'price': 8.99, 'name': 'item2', 'url': 'http://example.com/item2'}], 'category2': {'price': 15.0, 'name': 'item3', 'url': 'http://example.com/item3'}}\"\n                }\n            },\n            \"required\": [\"selected_product_details\"]\n        }\n    }\n</tools><|eot_id|><|start_header_id|>user<|end_header_id|>\n\nI need to buy a crossbow<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n<tool_call>\n{\"id\":\"call_deok\",\"name\":\"product_identifier_func\",\"arguments\":{\"query\":\"I need to buy a crossbow\"}}\n</tool_call><|eot_id|><|start_header_id|>tool<|end_header_id|>\n\n<tool_response>\n{\"id\":\"call_deok\",\"result\":{'crossbow': {'price': 237.68,'name': 'crossbow','url': 'https://www.amazon.com/crossbow/dp/B07KMVJJK7'}}}\n</tool_response><|eot_id|><|start_header_id|>assistant<|end_header_id|>\n'''\n```\nNow there are only a few steps left. Before doing anything, letâ€™s test our agent.\n\n\n```python\n### Testing agent\nmessages = [\n    ChatMessage.from_system(\n        chat_template\n    ),\n    ChatMessage.from_user(\"I need to buy a crossbow for my child and PokÃ©mon for myself.\"),\n]\n\nchat_generator = get_chat_generator()\nresponse = chat_generator.run(messages=messages)\npprint(response)\n\n### response\n{'replies': [ChatMessage(content='<tool_call>\\n'\n                                 '{\"id\": 0, \"name\": \"product_identifier_func\", '\n                                 '\"arguments\": {\"query\": \"I need to buy a '\n                                 'crossbow for my child\"}}\\n'\n                                 '</tool_call>\\n'\n                                 '<tool_call>\\n'\n                                 '{\"id\": 1, \"name\": \"product_identifier_func\", '\n                                 '\"arguments\": {\"query\": \"I need to buy a '\n                                 'Pokemon for myself\"}}\\n'\n                                 '</tool_call>',\n                         role=<ChatRole.ASSISTANT: 'assistant'>,\n                         name=None,\n                         meta={'finish_reason': 'stop',\n                               'index': 0,\n                               'model': 'llama3-groq-70b-8192-tool-use-preview',\n                               'usage': {'completion_time': 0.217823967,\n                                         'completion_tokens': 70,\n                                         'prompt_time': 0.041348261,\n                                         'prompt_tokens': 561,\n                                         'total_time': 0.259172228,\n                                         'total_tokens': 631}})]}\n```\nWith that, we have completed about 90% of our work.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*nYVXcgpm3RZ3g5h5d4UK_A.png)\n\nOne thing you may have noticed in the above response is that the XML tag `<tool_call>` encloses tool calls. Thus, we need to develop a mechanism to extract the tool\\_call object.\n\n\n```python\ndef extract_tool_calls(tool_calls_str):\n    json_objects = re.findall(r'<tool_call>(.*?)</tool_call>', tool_calls_str, re.DOTALL)\n    \n    result_list = [json.loads(obj) for obj in json_objects]\n    \n    return result_list\n\navailable_functions = {\n    \"product_identifier_func\": product_identifier_func, \n    \"find_budget_friendly_option\": find_budget_friendly_option\n    }\n```\nWith this step completed, we can directly access the agentâ€™s response when it calls a tool. Now the only thing pending is to get the tool call object and execute the function accordingly. Letâ€™s complete that piece too.\n\n\n```python\nmessages.append(ChatMessage.from_user(message))\nresponse = chat_generator.run(messages=messages)\n\nif response and \"<tool_call>\" in response[\"replies\"][0].content:\n    function_calls = extract_tool_calls(response[\"replies\"][0].content)\n    for function_call in function_calls:\n        # Parse function calling information\n        function_name = function_call[\"name\"]\n        function_args = function_call[\"arguments\"]\n\n        # Find the corresponding function and call it with the given arguments\n        function_to_call = available_functions[function_name]\n        function_response = function_to_call(**function_args)\n\n        # Append function response to the messages list using `ChatMessage.from_function`\n        messages.append(ChatMessage.from_function(content=json.dumps(function_response), name=function_name))\n        response = chat_generator.run(messages=messages)\n```\nNow itâ€™s time to join each component together and build a proper chat application. I am going to use Gradio for that purpose.\n\n\n```python\nimport gradio as gr\n\nmessages = [ChatMessage.from_system(chat_template)]\nchat_generator = get_chat_generator()\n\ndef chatbot_with_fc(message, messages):\n    messages.append(ChatMessage.from_user(message))\n    response = chat_generator.run(messages=messages)\n\n    while True:\n        if response and \"<tool_call>\" in response[\"replies\"][0].content:\n            function_calls = extract_tool_calls(response[\"replies\"][0].content)\n            for function_call in function_calls:\n                # Parse function calling information\n                function_name = function_call[\"name\"]\n                function_args = function_call[\"arguments\"]\n\n                # Find the corresponding function and call it with the given arguments\n                function_to_call = available_functions[function_name]\n                function_response = function_to_call(**function_args)\n\n                # Append function response to the messages list using `ChatMessage.from_function`\n                messages.append(ChatMessage.from_function(content=json.dumps(function_response), name=function_name))\n                response = chat_generator.run(messages=messages)\n\n        # Regular Conversation\n        else:\n            messages.append(response[\"replies\"][0])\n            break\n    return response[\"replies\"][0].content\n\n\ndef chatbot_interface(user_input, state):\n    response_content = chatbot_with_fc(user_input, state)\n    return response_content, state\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# AI Purchase Assistant\")\n    gr.Markdown(\"Ask me about products you want to buy!\")\n    \n    state = gr.State(value=messages)\n    \n    with gr.Row():\n        user_input = gr.Textbox(label=\"Your message:\")\n        response_output = gr.Markdown(label=\"Response:\")\n    \n    user_input.submit(chatbot_interface, [user_input, state], [response_output, state])\n    gr.Button(\"Send\").click(chatbot_interface, [user_input, state], [response_output, state])\n\n\ndemo.launch()\n```\nThatâ€™s it! We have built the Llama 3\\-based AI Agent ðŸ¤– with function calling capability. You can access the full code from this [GitHub repo](https://github.com/Ransaka/ai-agents-with-llama3). Thanks for reading.\n\nAccess to the dataset used in this article is available through [this](https://www.kaggle.com/datasets/promptcloud/amazon-product-dataset-2020) Kaggle link (Under CC0: Public Domain).\n\n\n### Conclusion\n\nWhen constructing an AI agent\\-based system, it's important to consider the time required to complete a task and the number of API calls (tokens) used for each task. One of the major challenges is reducing hallucination in the system, which is an active area of research. Therefore, there are no set rules for building LLMs and agent systems. It's necessary to work patiently and strategically to ensure the AI agent, the LLM, is functioning correctly.\n\n*All images, unless otherwise noted, are by the author.*\n\n\n### Reference:\n\n[https://docs.together.ai/docs/llama\\-3\\-function\\-calling](https://docs.together.ai/docs/llama-3-function-calling)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/visualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-with-ragas-fc2486308557","frontmatter":{"title":"Visualize your RAG Dataâ€Šâ€”â€ŠEvaluate your  Retrieval-Augmented Generation System with Ragas","meta_title":"Visualize your RAG Dataâ€Šâ€”â€ŠEvaluate your  Retrieval-Augmented Generation System with Ragas","description":"How to use UMAP dimensionality reduction for Embeddings to show multiple evaluation Questions and their relationships to source documentsâ€¦","date":"2024-11-04T12:35:56.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*peWTe1A-MqeROT_Jdof_Cw.gif","categories":["Natural Language Processing","Generative AI","Data Science"],"author":"Rifx.Online","tags":["RAG","UMAP","embeddings","evaluation","visualization"],"draft":false,"slug":"blog/visualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-with-ragas-fc2486308557"},"content":"\n\n\n\n\n### How to use UMAP dimensionality reduction for Embeddings to show multiple evaluation Questions and their relationships to source documents with Ragas, OpenAI, Langchain and ChromaDB\n\nRetrieval\\-Augmented Generation (RAG) adds a retrieval step to the workflow of an LLM, enabling it to query relevant data from additional sources like private documents when responding to questions and queries \\[1]. This workflow does not require costly training or fine\\-tuning of LLMs on the additional documents. The documents are split into snippets, which are then indexed, often using a compact ML\\-generated vector representation (embedding). Snippets with similar content will be in proximity to each other in this embedding space.\n\nThe RAG application projects the user\\-provided questions into the embedding space to retrieve relevant document snippets based on their distance to the question. The LLM can use the retrieved information to answer the query and to substantiate its conclusion by presenting the snippets as references.\n\n\n\nThe evaluation of a RAG application is challenging \\[2]. Different approaches exist: on one hand, there are methods where the answer as ground truth must be provided by the developer; on the other hand, the answer (and the question) can also be generated by another LLM. One of the largest open\\-source systems for LLM\\-supported answering is Ragas \\[4](Retrieval\\-Augmented Generation Assessment), which provides\n\n* Methods for generating test data based on the documents and\n* Evaluations based on different metrics for evaluating retrieval and generation steps one\\-by\\-one and end\\-to\\-end.\n\nIn this article, you will learn\n\n* How to briefly build a RAG system for Formula One (see the previous article [Visualize your RAG Data â€” EDA for Retrieval\\-Augmented Generation](https://readmedium.com/visualize-your-rag-data-eda-for-retrieval-augmented-generation-0701ee98768f) for detailed descriptions)\n* Generate questions and answers\n* Evaluate the RAG system with [Ragas](https://github.com/explodinggradients/ragas)\n* Most importantly how to visualize the results with [Renumics Spotlight](https://github.com/Renumics/spotlight) and interpret the results.\n\nThe code is available at Github\n\n\n## Get your environment ready\n\nStart a notebook and install the required python packages\n\n\n```python\n!pip install langchain langchain-openai chromadb renumics-spotlight\n%env OPENAI_API_KEY=<your-api-key>\n```\nThis tutorial uses the following python packages:\n\n* [**Langchain**](https://github.com/langchain-ai/langchain): A framework to integrate language models and RAG components, making the setup process smoother.\n* [**Renumics\\-Spotlight**](https://github.com/Renumics/spotlight): A visualization tool to interactively explore unstructured ML datasets.\n* [**Ragas**](https://github.com/explodinggradients/ragas): a framework that helps you evaluate your RAG pipelines\n\n*Disclaimer: The author of this article is also one of the developers of Spotlight.*\n\n\n## Prepare documents and embeddings for the dataset\n\nYou can use your own RAG Application, skip to the next part to learn how to evaluate, extract and visualize.\n\nOr you can use the RAG application from the [last article](https://readmedium.com/visualize-your-rag-data-eda-for-retrieval-augmented-generation-0701ee98768f) with [our prepared dataset of all Formula One articles of Wikipedia](https://spotlightpublic.blob.core.windows.net/docs-data/rag_demo/docs.zip). There you can also insert your own Documents into a â€˜docs/â€™ subfolder.\n\n\n> This dataset is based on articles from [Wikipedia](https://www.wikipedia.org/) and is licensed under the Creative Commons Attribution\\-ShareAlike License. The original articles and a list of authors can be found on the respective Wikipedia pages.\n\nNow you can use Langchainâ€™s `DirectoryLoader` to load all files from the docs subdirectory and split the documents in snippets using the `RecursiveCharacterTextSpliter`. With `OpenAIEmbeddings` you can create embeddings and store them in a `ChromaDB` as vector store. For the Chain itself you can use LangChains `ChatOpenAI` and a `ChatPromptTemplate`.\n\nThe [linked code](https://github.com/Renumics/rag-demo/blob/main/notebooks/visualize_rag_tutorial_qs.ipynb) for this article contains all necessary steps and you can find a detailed description of all steps above in [the last article](https://readmedium.com/visualize-your-rag-data-eda-for-retrieval-augmented-generation-0701ee98768f).\n\nOne important point is, that you should use a hash function to create ids for snippets in `ChromaDB`. This allows to find the embeddings in the db if you only have the document with its content and metadata. This makes it possible to skip documents that already exist in the database.\n\n\n```python\nimport hashlib\nimport json\nfrom langchain_core.documents import Document\n\ndef stable_hash_meta(doc: Document) -> str:\n    \"\"\"\n    Stable hash document based on its metadata.\n    \"\"\"\n    return hashlib.sha1(json.dumps(doc.metadata, sort_keys=True).encode()).hexdigest()\n\n...\nsplits = text_splitter.split_documents(docs)\nsplits_ids = [\n    {\"doc\": split, \"id\": stable_hash_meta(split.metadata)} for split in splits\n]\n\nexisting_ids = docs_vectorstore.get()[\"ids\"]\nnew_splits_ids = [split for split in splits_ids if split[\"id\"] not in existing_ids]\n\ndocs_vectorstore.add_documents(\n    documents=[split[\"doc\"] for split in new_splits_ids],\n    ids=[split[\"id\"] for split in new_splits_ids],\n)\ndocs_vectorstore.persist()\n```\n\n## Evaluation Questions\n\nFor a common topic like Formula One, one can also use ChatGPT directly to generate general questions. In this article, four methods of question generation are used:\n\n* **GPT4**: 30 questions were generated using ChatGPT 4 with the following prompt â€œWrite 30 question about Formula oneâ€\nâ€“ Random Example: â€œWhich Formula 1 team is known for its prancing horse logo?â€\n* **GPT3\\.5:** Another 199 question were generated with ChatGPT 3\\.5 with the following prompt â€œWrite 100 question about Formula oneâ€ and repeating â€œThanks, write another 100 pleaseâ€\nâ€“ Example: â€œâ€Which driver won the inaugural Formula One World Championship in 1950?â€\n* **Ragas\\_GPT4**: 113 questions were generated using Ragas. Ragas utilizes the documents again and its own embedding model to construct a vector database, which is then used to generate questions with GPT4\\.\nâ€“ Example: â€œCan you tell me more about the performance of the Jordan 198 Formula One car in the 1998 World Championship?â€\n* **Rags\\_GPT3\\.5**: 226 additional questions were generated with Ragas â€” here we use GPT3\\.5\nâ€“ Example: â€œWhat incident occurred at the 2014 Belgian Grand Prix that led to Hamiltonâ€™s retirement from the race?â€\n\n\n```python\nfrom ragas.testset import TestsetGenerator\n\ngenerator = TestsetGenerator.from_default(\n    openai_generator_llm=\"gpt-3.5-turbo-16k\", \n    openai_filter_llm=\"gpt-3.5-turbo-16k\"\n)\n\ntestset_ragas_gpt35 = generator.generate(docs, 100)\n```\nThe questions and answers were not reviewed or modified in any way. All questions are combined in a single dataframe with the columns `id`, `question`, `ground_truth`, `question_by` and `answer`.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*R_74K0-_SJXyTxq6ovAcWg.png)\n\nNext, the questions will be posed to the RAG system. For over 500 questions, this can take some time and incur costs. If you ask the questions row\\-by\\-row, you can pause and continue the process or recover from a crash without losing the results so far:\n\n\n```python\nfor i, row in df_questions_answers.iterrows():\n    if row[\"answer\"] is None or pd.isnull(row[\"answer\"]):\n        response = rag_chain.invoke(row[\"question\"])\n\n        df_questions_answers.loc[df_questions_answers.index[i], \"answer\"] = response[\n            \"answer\"\n        ]\n        df_questions_answers.loc[df_questions_answers.index[i], \"source_documents\"] = [\n            stable_hash_meta(source_document.metadata)\n            for source_document in response[\"source_documents\"]\n        ]\n\n```\nNot only is the answer stored but also the source IDs of the retrieved document snippets, and their text content as context:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*umlKv7Qf9SSLzRslT2r0Qw.png)\n\nAdditionally, the embeddings for all questions are generated and stored in the dataframe as well. This allows for visualizing them alongside the documents.\n\n\n## Evaluation with Ragas\n\n[Ragas](https://github.com/explodinggradients/ragas) provides metrics for evaluating each component of your RAG pipeline in isolation and end\\-to\\-end metrics for overall performance:\n\n1. **Context Precision:** Uses the `question` and retrieved `contexts` to measure the signal\\-to\\-noise ratio.\n2. **Context Relevancy:** Measures the relevance of the retrieved context to the question, calculated using the `question` and `contexts`.\n3. **Context Recall:** Based on the `ground truth` and `contexts` to check if all relevant information for the answer is retrieved.\n4. **Faithfulness:** Utilizes the `contexts` and `answer` to measure how factually accurate the generated answer is.\n5. **Answer Relevance:** Computed using the `question` and `answer` to assess the relevance of the generated answer to the question (does not consider factuality).\n6. **Answer Semantic Similarity:** Evaluated using the `ground truth` and `answer` to assess the semantic resemblance between the generated and the correct answer.\n7. **Answer Correctness:** Relies on the `ground truth` and `answer` to measure the accuracy and alignment of the generated answer with the correct one.\n8. **Aspect Critique:** Involves analyzing the `answer` to evaluate submissions based on predefined or custom aspects such as correctness or harmfulness.\n\nFor now, we focus on the end\\-to\\-end metric of answer correctness. The column names and content in the dataframe are copied and adapted to meet the naming and formatting requirements according to the Ragas API:\n\n\n```python\n## prepare the dataframe for evaluation\ndf_qa_eval = df_questions_answers.copy()\n\n\n## adapt the ground truth to the ragas naming and format\ndf_qa_eval.rename(columns={\"ground_truth\": \"ground_truths\"}, inplace=True)\ndf_qa_eval[\"ground_truths\"] = [\n    [gt] if not isinstance(gt, list) else gt for gt in df_qa_eval[\"ground_truths\"]\n]\n```\nThis again can take some time and even more money than just querying your RAG system. Letâ€™s apply the evaluation row\\-by\\-row to be able to recover from a crash without losing the results so far:\n\n\n```python\n## evaluate the answer correctness if not already done\nfields = [\"question\", \"answer\", \"contexts\", \"ground_truths\"]\nfor i, row in df_qa_eval.iterrows():\n    if row[\"answer_correctness\"] is None or pd.isnull(row[\"answer_correctness\"]):\n        evaluation_result = evaluate(\n            Dataset.from_pandas(df_qa_eval.iloc[i : i + 1][fields]),\n            [answer_correctness],\n        )\n        df_qa_eval.loc[i, \"answer_correctness\"] = evaluation_result[\n            \"answer_correctness\"\n        ]\n\n```\nAfterwards, you can store the results in the `df_questions_answer` dataframe:\n\n\n```python\ndf_questions_answers[\"answer_correctness\"] = df_qa_eval[\"answer_correctness\"]\n```\n\n## Prepare visualization\n\nTo include the document snippets in the visualization, we add references from documents to questions that used the document as a source. Additionally, the count of questions referencing a document is stored:\n\n\n```python\n## Explode 'source_documents' so each document ID is in its own row alongside the question ID\ndf_questions_exploded = df_qa_eval.explode(\"source_documents\")\n\n## Group by exploded 'source_documents' (document IDs) and aggregate\nagg = (\n    df_questions_exploded.groupby(\"source_documents\")\n    .agg(\n        num_questions=(\"id\", \"count\"),  # Count of questions referencing the document\n        question_ids=(\n            \"id\",\n            lambda x: list(x),\n        ),  # List of question IDs referencing the document\n    )\n    .reset_index()\n    .rename(columns={\"source_documents\": \"id\"})\n)\n\n## Merge the aggregated information back into df_documents\ndf_documents_agg = pd.merge(df_docs, agg, on=\"id\", how=\"left\")\n\n## Use apply to replace NaN values with empty lists for 'question_ids'\ndf_documents_agg[\"question_ids\"] = df_documents_agg[\"question_ids\"].apply(\n    lambda x: x if isinstance(x, list) else []\n)\n## Replace NaN values in 'num_questions' with 0\ndf_documents_agg[\"num_questions\"] = df_documents_agg[\"num_questions\"].fillna(0)\n```\nNow concatenate the dataframe of questions with the dataframe of the documents\n\n\n```python\ndf = pd.concat([df_qa_eval, df_documents_agg], axis=0)\n```\nAdditionally, letâ€™s prepare some different UMAP \\[3] mappings. You could do much the same in the Spotlight GUI later, but doing it upfront can save time.\n\n* umap\\_all: UMAP with fit and transform applied on all document and question embeddings\n* umap\\_questions: UMAP with fit applied on questions embeddings only and transform applied on both\n* umap\\_docs: UMAP with fit applied on document embeddings only and transform applied on both\n\nWe prepare each of the UMAP transformations like this:\n\n\n```python\numap = UMAP(n_neighbors=20, min_dist=0.15, metric=\"cosine\", random_state=42).fit\numap_all = umap.transform(df[\"embedding\"].values.tolist())\ndf[\"umap\"] = umap_all.tolist()\n\n```\nAnother interesting metric for each of the document snippets is the distance between its embeddings and the embeddings of the nearest question\n\n\n```python\nquestion_embeddings = np.array(df[df[\"question\"].notna()][\"embedding\"].tolist())\ndf[\"nearest_question_dist\"] = [  # brute force, could be optimized using ChromaDB\n    np.min([np.linalg.norm(np.array(doc_emb) - question_embeddings)])\n    for doc_emb in df[\"embedding\"].values\n]\n```\nThis metric can be helpful to find documents that are not referenced by questions.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*YTRUXZmd0iX8kyPIdUUnlg.png)\n\n\n## Visualize results\n\nIf you skipped the previous steps, you can download the dataframe and load it with:\n\n\n```python\nimport pandas as pd\ndf = pd.read_parquet(\"df_f1_rag_docs_and_questions.parquet\")\n```\nand start [Renumics Spotlight](https://github.com/Renumics/spotlight) to visualize it with:\n\n\n```python\nfrom renumics import spotlight\n\nspotlight.show(df)\nspotlight.show(\n    df,\n    layout=\"/home/markus/Downloads/layout_rag_1.json\",\n    dtype={x: Embedding for x in df.keys() if \"umap\" in x},\n)\n```\nIt will open a new brwoser window:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*IMbva0pP8RAVhoY4dVbjLg.png)\n\nOn the top left side, you can see a **table of all questions and all document** snippets. You can use the â€œvisible columnsâ€ button to control which columns of the dataframe are shown in the table. It is useful to create a filter directly that selects only the questions to be able to turn the questions on and off in the visualizations: Select all questions and and then create a filter using the â€œCreate filter from selected rowâ€ button.\n\nTo the right of the table, the `answer correctness` **is displayed as a metric** across all questions. Below there are two **histograms**; the left one shows the distribution of `answer correctness` divided into the different methods of question generation. The right one shows the distribution of methods of question generation. Here, it is advisable to create a filter for the questions using the filter button to display only the selected rows (the questions) if needed.\n\nOn the right side, there are **two similarity maps.** The first one uses the `umap_questions` column and shows the questions and documents based on the transformation applied only to the questions. It is helpful for viewing the distribution of questions independently from the associated documents because this approach allows analysts to identify patterns or clusters within the questions themselves.\n\nThe second similarity map shows the questions and documents based on the transformation applied only to the documents (`umap_docs`). It is useful for viewing the questions in the context of their associated documents. A similarity map that simultaneously transforms questions and documents has proven to be less helpful with a larger number of questions, as more or fewer questions get clustered together and tend to be separated from the documents. Therefore, this representation is omitted here.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*1wZrAj60hiw1T3RVnCuBtA.png)\n\n\n### Document Embedding Similarity Map: Observations\n\nIn the similarity map `umap_docs`, you can identify areas in the embedding space of the documents that have no neighboring questions. It is even better recognized when selecting `nearest_question_dist` for coloring.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*cMGNPnnBa9Bn7BJ05SzxBw.png)\n\nSome clusters can be identified, including snippets that contain only headings or tabular data containing only numbers page by page, whose meaning is lost during splitting. Additionally, many Wikipedia\\-specific text additions that contain no relevant information, such as links to other languages or editing notes, form clusters with no neighboring questions.\n\nRemoving the noise in form of Wikipedia\\-related text is very simple when using the Wikipedia API. It is probably not particularly necessary, as it mainly costs some space â€” it is not expected that the RAG result will be particularly worsened by it. However, data contained in large tables are hardly captured by the RAG system and it could ne benifical to extract these using advanced pre\\-processing methods for Table Extraction and to connect them to the RAG system.\n\nAnother point that you can observe in the `umap_docs` similarity map is how the questions from different sources are distributed.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*IH7z3J4yUmU0C_SruxnDkg.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*K4bADgDmSAr5t4t4r9VImQ.png)\n\nThe questions that were directly generated by ChatGPT (GPT\\-3\\.5, GPT\\-4\\) are located in a more confined area in the center, whereas the questions generated with ragas based on the documents cover a larger area.\n\n\n### Answer correctness histogram\n\nThe histogram can be used as a starting point to get an initial impression of the global statistics of the data. Overall, across all questions, the `answer correctness` is 0\\.45\\. For the questions created without ragas, it is 0\\.36, and for questions with ragas, it is 0\\.52\\. It was expected that the system would perform better for questions generated by ragas, as these questions are based on the available data, whereas the questions directly generated by ChatGPT could come from all the data with which ChatGPT was trained.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GsLBsg7uwTrw-AzvO4BHmw.png)\n\nA quick, random manual review of some of the questions/answers and ground truth shows that in the interval of`answer correctness`0\\.3â€“0\\.4, most questions were still correctly answered according to the ground truth. In the interval 0\\.2â€“0\\.3, many incorrect answers are present. In the interval 0\\.1â€“0\\.2, most answers are incorrect. Notably, almost all questions in this range came from GPT\\-3\\.5\\. The two questions in this interval generated with GPT\\-4 were answered correctly even though they received an `answer correctness` of below 0\\.2\\.\n\n\n### Questions Embedding Similarity Map: Observations\n\nThe Questions Embedding Similarity Map can be helpful to dig deeper into `answer correctness` by examining clusters of similar questions that may cause similar problems.\n\n* **Cluster â€œTerm for driver/process/carsâ€:** average `answer correctness` 0\\.23: Answers often not precise enough. E.g., Chassis tuning vs. Chassis flexing or brake tuning vs. brake bias adjustment. It is questionable whether these types of questions are suitable for evaluating the system, as it seems very difficult to judge the answers.\n* **Cluster â€œTerms for fuel strategy:â€** average `answer correctness`0\\.44, similar to the global`answer correctness`.\n* **Cluster â€œNames of tracksâ€:** average `answer correctness` 0\\.49, similar to the global `answer correctnes`.\n* **Cluster â€œWho holds the record forâ€¦â€**: average `answer correctness` 0\\.44, similar to the global `answer correctness`.\n* **Cluster â€œWin championship withâ€¦â€**: average `answer correctnes` 0\\.26 â€” looks challenging. Questions with many conditions, e.g., â€œWho is the only driver to win the Formula One World Championship with a British racing license, driving for an Italian team with an American engine.â€ Extended RAG methods like Multi Query might help improve here.\n* **Cluster â€œWho is the only driver to winâ€¦ with a car bearing the number \\<number\\>â€**: average `answer correctness` 0\\.23 â€” looks like GPT\\-3\\.5 was lazy here, repeating the same question with different numbers, even though most ground truth entries are wrong!\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Yc03cpSEFlJoZSBPIpMkiQ.png)\n\n\n## Conclusion\n\nIn conclusion, utilizing UMAP\\-based visualizations offers a interesting approach to dig deeper than just analyzing global metrics. The document embedding similarity map gives a good overview, illustrating the clustering of similar documents and their relation to evaluation questions. The question similarity map reveals patterns that allow the differentiation and analysis of questions in conjunction with quality metrics to enable insight generation. Follow the Visualize results section to apply the visualization on your evaluation strategy â€” what insights will you uncover?\n\n*I am a professional with expertise in creating advanced software solutions for the interactive exploration of unstructured data. I write about unstructured data and use powerful visualization tools to analyze and make informed decisions.*\n\n\n## References\n\n\\[1] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, Haofen Wang: [Retrieval\\-Augmented Generation for Large Language Models: A Survey](https://arxiv.org/abs/2312.10997) (2024\\), arxiv\n\n\\[2] Yixuan Tang, Yi Yang: [MultiHop\\-RAG: Benchmarking Retrieval\\-Augmented Generation for Multi\\-Hop Queries](https://arxiv.org/abs/2401.15391) (2021\\), arXiv\n\n\\[3] Leland McInnes, John Healy, James Melville: [UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction](https://arxiv.org/abs/1802.03426) (2018\\), arXiv\n\n\\[4] Shahul Es, Jithin James, Luis Espinosa\\-Anke, Steven Schockaert: [RAGAS: Automated Evaluation of Retrieval Augmented Generation](https://arxiv.org/abs/2309.15217) (2023\\), arXiv\n\n\n"},{"lang":"en","group":"blog","slug":"blog/whats-new-with-claude-sonnet-3-5-claude-3-5-haiku-c1f62a2d2c72","frontmatter":{"title":"Whatâ€™s new with Claude Sonnet 3.5 & Claude 3.5 Haiku?","meta_title":"Whatâ€™s new with Claude Sonnet 3.5 & Claude 3.5 Haiku?","description":"And is it worth checking out?","date":"2024-11-13T01:22:29.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*CEMTDlHlMUX66-eoMcSOzg.png","categories":["Natural Language Processing","Programming","Technology/Web"],"author":"Rifx.Online","tags":["language","models","interaction","automation","latency"],"draft":false,"slug":"blog/whats-new-with-claude-sonnet-3-5-claude-3-5-haiku-c1f62a2d2c72"},"content":"\n\n\n\n\n\n\n### First off, what is Claude?\n\nClaude is a language model created by [Anthropic](https://proxy.rifx.online/https://www.anthropic.com/) and itâ€™s designed to help with tasks like answering questions, summarizing information and generating text â€” similar to ChatGPT. The cool thing about Claude is that itâ€™s built to be safer and more aligned with human intentions, making it less likely to produce harmful or misleading content.\n\n\n### Waitâ€¦ wasnâ€™t Claude 3\\.5 Sonnet out already?\n\nHaha yes, while there havenâ€™t been any changes to the name there are a lot of exciting updates to this new version of Claude 3\\.5 Sonnet and Claude Haiku that was released on October 22nd, 2024\\.\n\nThese new models are faster and better at tasks like debugging code and transcribing text from images, which makes them especially useful for industries like retail and logistics.\n\n*If you prefer to discuss these new changes and see how you can implement them into your project, [click here to schedule a free call with us](https://proxy.rifx.online/https://calendly.com/woyera-ai/)!*\n\n\n### Letâ€™s dive in and see whatâ€™s new!\n\n\n## Human\\-Computer Interaction\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*p1anQynliN8ihnT8X2VYqw.gif)\n\nOne of the biggest updates with Claude 3\\.5 Sonnet is its ability to interact with computers in a more human\\-like way.\n\nIt can now navigate screens, click buttons, and type, which opens up some really interesting possibilities for automating tasks or even helping out in real\\-time workflows.\n\nKeep in mind that this feature is still in public beta, but itâ€™s already showing a lot of promise, especially for robotic process automation (RPA).\n\n\n## Enhanced Coding Support\n\nClaude can assist throughout the entire software development process from designing, debugging, and optimizing code â€” making it a valuable asset for anyone in tech.\n\n\n## Improved Chatbots\n\nClaudeâ€™s natural tone and advanced reasoning make it perfect for building more responsive and engaging chatbots.\n\nIt can handle complex conversations and even connect with various systems to streamline tasks which make it a great fit for customer service, technical support and more.\n\n\n## Visual Data Extraction\n\nOne standout feature is Claudeâ€™s ability to analyze visual data. It can interpret and pull information from charts, graphs, and diagrams with ease.\n\n\n## Knowledge Q\\&A\n\nClaude 3\\.5 Sonnet is also great for answering detailed questions using large datasets, knowledge bases, or code repositories. With a bigger context window itâ€™s a reliable option for businesses that need quick, accurate information.\n\n\n## Availability and Pricing\n\nClaude 3\\.5 is available as an API on Anthropic API, Amazon Bedrock, and Google Cloudâ€™s Vertex AI. For the API, pricing starts at **$3 per million input tokens** and **$15 per million output tokens.**\n\nOr you can simply use it on the web through [claude.ai](https://proxy.rifx.online/https://claude.ai/login?returnTo=%2F%3F). You can create an account for Free and then plans will be $20/month for Pro, $25/month for Team or [Enterprise.](https://proxy.rifx.online/https://www.anthropic.com/pricing)\n\n\n## Safety and Trust\n\nAnthropic has put a lot of effort into making Claude 3\\.5 Sonnet safe to use. The model has gone through extensive testing to ensure it can handle sensitive content responsibly without compromising on performance.\n\nThis focus on safety helps protect against issues like inappropriate content and ensures Claude is suitable for a wide range of applications.\n\n\n## Use Cases\n\nWhether youâ€™re a developer, business owner, or just curious about AI, Claude 3\\.5 Sonnet has a lot to offer.\n\n\n### Automating repetitive tasks\n\nA customer service team can use Claude to handle repetitive backend tasks like updating customer orders or processing refunds. Claudeâ€™s ability to navigate screens and click buttons can save hours of manual work.\n\n\n### Chatbots\n\nA healthcare provider can build a chatbot with Claude to handle patient interactions such as booking appointments, answering medical FAQs, or guiding patients through symptom checks. All while maintaining a natural \\& conversational tone.\n\n\n### Visual data\n\nFinancial analysts can use Claude to analyze quarterly earnings reports that include a lot of charts and graphs. Claude can quickly extract insights and summarize key trends.\n\n\n### Knowledge Q\\&A\n\nA tech company can use Claude to manage an internal knowledge base. Developers could ask detailed questions about existing code repositories or troubleshooting steps and Claude would provide quick \\& reliable answers.\n\n\n## What about Claude 3\\.5 Haiku?\n\nClaude 3\\.5 Haiku is Anthropicâ€™s fastest AI model and offers better performance without raising costs or slowing down. Itâ€™s stronger at tasks like coding and beats out models like Claude 3 Opus and GPT\\-4o in benchmarks.\n\nThe model is designed with low latency, meaning it responds quickly, making it perfect for real\\-time apps, personalized tasks like analyzing purchase history, and other data\\-heavy projects. Itâ€™ll be available later this month on APIs like Amazon Bedrock and Google Cloud, starting as a text\\-only model with image support coming soon.\n\n\n## So, how does Claude compare to ChatGPT?\n\nWhen comparing **Claude 3\\.5** and **ChatGPT**, both are advanced AI models designed to tackle similar tasks like answering questions, generating text, and assisting with coding, but they have distinct differences that might cater to different needs.\n\nClaude 3\\.5 is built with a strong emphasis on **safety**, reducing the risk of harmful or misleading outputs. While ChatGPT also prioritizes safety, Claudeâ€™s design places extra focus on this area.\n\nIn terms of **speed**, Claude 3\\.5 Haiku offers faster response times, making it ideal for real\\-time applications. ChatGPT is also quick, but may experience slight delays on more complex tasks.\n\nFor **coding**, both models perform well. Claude 3\\.5 Sonnet stands out in recent benchmarks for debugging and code generation, while ChatGPT remains a reliable option for coding help and explanations.\n\nA key difference is **real\\-world interaction**. Claude 3\\.5 can navigate screens and automate tasks, a feature ChatGPT does not yet offer.\n\nIn **pricing**, ChatGPT has a widely available free version, while Claude offers flexible pricing with its API and cloud platforms.\n\nClaudeâ€™s focus on safety, low latency, and advanced real\\-world interactions make it a good fit for more specialized applications. Meanwhile ChatGPTâ€™s versatility, wide availability, and strong coding support make it a great general\\-purpose tool.\n\nUltimately, the choice between the two depends on what youâ€™re looking for but both models offer strong capabilities with their unique strengths.\n\n\n## Conclusion\n\nThe best way to learn about a new tool is to try it out yourself! You can use Claude 3\\.5 Sonnet for enhancing coding, chatbots, data analysis and much more.\n\nLet us know what you will use Claude for.\n\n*If you ever have a custom chatbot or app you need to build, [click here to have a quick call with us.](https://proxy.rifx.online/https://calendly.com/woyera-ai/)*\n\n\n"},{"lang":"en","group":"blog","slug":"blog/why-embedding-matters-when-building-a-non-english-rag-system-multilingual-embeddings-1e3434ea6180","frontmatter":{"title":"Why embedding matters when building a non-English RAG systemâ€Šâ€”â€ŠMultilingual embeddings","meta_title":"Why embedding matters when building a non-English RAG systemâ€Šâ€”â€ŠMultilingual embeddings","description":"Discover why multilingual embeddings are crucial for RAG systems, with a detailed comparison of English vs. multilingual models in Dutch.","date":"2024-11-13T01:22:29.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*QvODAYxqisUTrt4V.png","categories":["Natural Language Processing","Machine Learning","Multilingual"],"author":"Rifx.Online","tags":["embeddings","multilingual","RAG","Cohere","Dutch"],"draft":false,"slug":"blog/why-embedding-matters-when-building-a-non-english-rag-system-multilingual-embeddings-1e3434ea6180"},"content":"\n\n\n\n\n## Why Embeddings are Key\n\nEmbeddings are a cornerstone of modern generative AI, silently driving the functionality of many systems we interact with daily. At their simplest, embeddings are **numerical representations of text** â€” effectively transforming words, sentences, or even entire documents into numbers. These numbers are far from random; theyâ€™re carefully designed to capture the meaning and relationships within the text. For instance, the embeddings for â€œdogâ€ and â€œpuppyâ€ would be closer together in the numerical space than the embedding for â€œcar,â€ reflecting their **semantic similarity**. This ability to encode meaning into a measurable form is what makes embeddings indispensable for tasks like search, recommendation systems, and advanced AI applications such as **Retrieval\\-Augmented Generation (RAG)**.\n\n\n\nThis transformation into numbers allows AI to compare and understand text in a meaningful way. When working with massive amounts of data, as is often the case in RAG systems, embeddings become essential. These systems combine the power of embeddings with specialized storage solutions called **vector databases**. Unlike traditional databases that search for exact matches, vector databases are optimized to find the closest matches based on meaning. This capability enables RAG systems to retrieve the most relevant information from vast knowledge bases and use it to generate precise, contextually informed responses. By bridging raw data and intelligent retrieval, embeddings and vector databases together form the backbone of RAG systemsâ€™ success.\n\n\n## The Challenge of Multilingual Systems\n\nBuilding RAG systems that work well in English is already a complex task, but extending them to other languages introduces a whole new set of challenges. English embeddings are often highly optimized because of the abundance of training data and the simplicity of the languageâ€™s structure. However, using these English\\-trained embeddings for other languages can lead to significant inaccuracies. Different languages come with their own nuances, grammar, and cultural contexts, which standard embedding models trained predominantly on English text often fail to capture. While some multilingual embedding models exist to bridge this gap, they are not all equally effective across languages, particularly for those with limited training data or unique linguistic features. This makes it difficult to build RAG systems that are as accurate and reliable for non\\-English languages as they are for English.\n\n\n### Why Are English Embeddings More Accurate?\n\n1. **Abundance of High\\-Quality Training Data**\nEnglish dominates the digital landscape, with an unparalleled volume of high\\-quality content available for training. Datasets like Wikipedia, books, research papers, and social media are much richer in English than in other languages. In contrast, many languages, especially low\\-resource ones, lack diverse and standardized datasets, which limits the quality of embeddings trained on them.\n2. **Model Optimization Bias**\nNLP models like BERT and GPT were initially developed and optimized for English, often prioritizing it even in multilingual versions. Multilingual models balance learning across many languages within the same parameter space, which can dilute performance for less\\-represented languages in favor of dominant ones like English.\n3. **Linguistic Complexity and Diversity**\nEnglish has relatively simple morphology compared to many other languages. For instance, word forms in English tend to remain consistent (e.g., â€œrunâ€ and â€œrunningâ€), while languages like Turkish or Finnish have highly inflected forms, where a single root word can have dozens of variations. Additionally, languages with different syntax or word order, such as Japanese (Subject\\-Object\\-Verb) or Arabic (flexible word order), pose extra challenges for models optimized for English\\-like structures.\n4. **Semantic and Cultural Alignment**\nCapturing semantic meaning across languages is far from straightforward. Words and phrases often carry nuanced meanings that donâ€™t translate directly. For example, the English word â€œloveâ€ has multiple culturally distinct equivalents in other languages (e.g., â€œamorâ€ in Spanish, â€œerosâ€ or â€œagapeâ€ in Greek). Embeddings that fail to account for these differences struggle with multilingual alignment.\n5. **Benchmarking and Evaluation Bias**\nMany benchmarking datasets and evaluation methods are designed with English in mind. This English\\-centric focus can artificially inflate the perceived performance of models in English while masking their limitations in other languages.\n\n\n### The Impact on RAG Systems\n\nWhen embeddings fail to handle the complexity of other languages, the consequences for RAG systems can be significant. Retrieval results often become less relevant or even outright wrong, as the embeddings may struggle to capture the nuanced meaning of non\\-English queries. This doesnâ€™t just impact accuracy â€” it also undermines user trust and the overall utility of the system. Crucial text chunks may be missed during retrieval, preventing the system from accessing the information it needs to generate accurate and contextually relevant responses.\n\nFor a multilingual RAG system to perform well, it requires embeddings that can align semantically across languages while accounting for their unique structural and cultural intricacies. Investing in high\\-quality multilingual embeddings and fine\\-tuning them for specific languages or tasks is essential. This ensures that RAG systems can meet the needs of users in any language â€” not just English.\n\nBut how well do different embeddings actually perform in a non\\-English context? To explore this, weâ€™ll compare an English embedding model with a multilingual embedding model using a Dutch dataset. This test will reveal how different approaches to embeddings impact retrieval accuracy and the quality of the generated responses in a multilingual RAG system.\n\n\n## Comparing Embedding Models for a Dutch RAG System\n\nTo understand how different embedding models handle a non\\-English language like Dutch, weâ€™ll compare two models available on Amazon Bedrock: **Cohere Embed English v3** and **Cohere Embed Multilingual v3**. These models represent different approaches to embeddings â€” one optimized exclusively for English and the other designed for multilingual tasks. The table below summarizes their key attributes:\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*pBhIHfOsb-McrjHKvtq4Xw.png)\n\n\n### Build Embeddings\n\nTo evaluate the performance of the embedding models, we will build a local vectorstore using the LangChain framework. For this evaluation, we will use a guideline for firefighters written in Dutch as our dataset. This document contains technical and procedural information, making it a realistic and challenging use case for semantic retrieval in a non\\-English language. Below is the cleaned and streamlined code for creating a local vectorstore and indexing document chunks. Weâ€™ll use this setup to test two embedding models: **Cohere Embed English v3** and **Cohere Embed Multilingual v3**.\n\n\n```python\nimport os\nfrom langchain_community.document_loaders import DirectoryLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import FAISS\nfrom langchain_aws import BedrockEmbeddings\nimport boto3\n\n## Step 1: Load documents\nloader = DirectoryLoader('data', glob=\"**/*.pdf\")  # Adjust 'data' to your document directory\ndocuments = loader.load()\n\nprint(f\"You have {len(documents)} documents\")\nprint(f\"Document 1 contains {len(documents[0].page_content)} characters\")\n\n## Step 2: Split documents into smaller chunks\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)\nchunks = text_splitter.split_documents(documents)\n\nprint(f\"You have {len(chunks)} chunks\")\nprint(f\"The first chunk is {len(chunks[0].page_content)} characters long\")\n\n## Step 3: Set up Bedrock embeddings\nbedrock_client = boto3.client(\"bedrock-runtime\", region_name='us-east-1')\nbedrock_embeddings = BedrockEmbeddings(model_id=\"cohere.embed-multilingual-v3\", client=bedrock_client)\n\n## Step 4: Build the FAISS vectorstore\nvectorstore = FAISS.from_documents(chunks, bedrock_embeddings)\n\n## Save the vectorstore locally for reuse\nvectorstore.save_local(\"faiss_cohere_multilingual\")\n```\n\n## How This Code Works\n\n1. **Document Loading**:\nThe code loads all PDF files from the `data` directory. You can adjust the file path and format to match your dataset.\n2. **Text Splitting**:\nDocuments are split into smaller chunks of 400 characters with a 50\\-character overlap to improve retrieval accuracy. This ensures each chunk remains contextually meaningful.\n3. **Embedding Models**:\nThe `BedrockEmbeddings` class initializes the embedding model. You can replace the `model_id` to test **Cohere Embed English v3 or Cohere Embed Multilingual v3**.\n4. **Local Vectorstore**:\nThe FAISS library is used to create an in\\-memory vectorstore from the document chunks. This allows for fast similarity searches and can be saved locally for reuse.\n\nTo test all models, replace the `model_id` in the `BedrockEmbeddings` initialization with the appropriate model:\n\n* `\"cohere.embed-english-v3\"` for Cohere English.\n* `\"cohere.embed-multilingual-v3\"` for Cohere Multilingual.\n\n\n### Evaluating the Embedding Models\n\nTo evaluate the performance of the embedding models, we will ask the question: **â€œWelke rangen zijn er bij de brandweer?â€**, which translates to **â€œWhich ranks exist within the fire department?â€**. This question was chosen because our document only uses the term **â€œhiÃ«rarchieâ€**, which in Dutch has a similar semantic meaning to **â€œrangenâ€**. However, in English, â€œhierarchyâ€ and â€œranksâ€ do not share semantic similarity.\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*6N3C8C500hMQ3GNNkuu21A.png)\n\nThis distinction is crucial for our test. We expect the **Cohere Embed English v3** model to struggle with this query, as it relies on English semantics where the terms are unrelated. On the other hand, the **Cohere Embed Multilingual v3** model, which is trained to understand Dutch semantics, should retrieve the correct information from the document, demonstrating its ability to handle semantic nuances in non\\-English languages.\n\nBy asking this question, we aim to highlight how semantic alignment affects retrieval performance in a Dutch RAG system. This test will provide a clear comparison of the modelsâ€™ ability to handle non\\-English queries and retrieve relevant information. The results will showcase the importance of multilingual embeddings for achieving accurate retrieval in non\\-English contexts.\n\nTo implement and test this setup, we can use the following code. This script demonstrates how to query the vectorstore and utilize a RAG chain to combine the embeddings with a language model for answering questions. Note that when testing different embeddings (e.g., **Cohere Embed English v3** vs. **Cohere Embed Multilingual v3**), you need to ensure that the vectorstore is built using the corresponding embedding model. Replace the vectorstore with the one indexed using the embedding model you want to test for accurate results.\n\n\n```python\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain_aws import ChatBedrock\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.output_parsers import StrOutputParser\n\ninstructions = \"\"\"Je bent een brandweer expert. Beantwoord de vraag, maak gebruik van de context\"\"\"\n\nhuman = \"\"\"\nDit is de context: {context}\nDit is de vraag: {question}\n\"\"\"\n\nprompt = ChatPromptTemplate(\n    messages=[\n        SystemMessagePromptTemplate.from_template(instructions),\n        HumanMessagePromptTemplate.from_template(human), #User query will go here\n    ],\n    input_variables=['context','question'], # context provided by retriever and question by the user\n)\nmodel = ChatOpenAI(\n    model=\"gpt-4\",  # or \"gpt-3.5-turbo\" based on your needs\n    temperature=0,\n    max_tokens=512\n)\n\n## Function to merge text chunks\ndef format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n\n## RAG chain\nrag_chain = (\n        {\"context\": vectorstore.as_retriever(k=10) | format_docs, \"question\": RunnablePassthrough()}\n        | prompt\n        | model\n        | StrOutputParser()\n)\n\nrag_chain.invoke(\"Welke rangen zijn er bij de brandweer?\")\n```\n\n## Results of the Embedding Models\n\nTo evaluate the performance of the two embedding models, we asked the question: **â€œWelke rangen zijn er bij de brandweer?â€** (translated: â€œWhich ranks exist within the fire department?â€). Below are the responses generated by each model:\n\n\n### Cohere Embed English v3\n\n**Response**:*â€œDe tekst geeft geen specifieke informatie over de verschillende rangen binnen de brandweer. Echter, in veel brandweerorganisaties zijn er verschillende rangen zoals brandweerman, luitenant, kapitein, bataljonschef, assistent\\-chef en brandweerchef. De specifieke rangen kunnen variÃ«ren afhankelijk van het land en de specifieke organisatie.â€*\n\n**Analysis**:The response is generic and doesnâ€™t rely on the provided context from the Dutch document. This is expected, as the English embedding model struggles to connect the semantic similarity between â€œhiÃ«rarchieâ€ (used in the document) and â€œrangenâ€ (used in the query). Consequently, it defaults to general knowledge about fire department ranks.\n\n\n### Cohere Embed Multilingual v3\n\n**Response**:*â€œBinnen de brandweer zijn er verschillende rangen. Deze zijn onderverdeeld in het basiskader en het middenkader. In het basiskader hebben we de B01 brandweerman en de B02 korporaal. In het middenkader, dat bestaat uit onderofficieren, hebben we de M01 sergeant en de M02 adjudant. Deze rangen worden aangeduid op het diensttenue, het uitgaanstenue en op de interventiekledij.â€*\n\n**Analysis**:This response is highly relevant and accurately retrieves information from the document. The multilingual embedding model successfully identifies the semantic relationship between â€œhiÃ«rarchieâ€ (context) and â€œrangenâ€ (query). It provides a detailed answer directly based on the content of the document, demonstrating its ability to handle Dutch\\-specific semantics effectively.\n\n\n### Key Takeaways\n\n* **Cohere Embed English v3**: The English model failed to retrieve relevant context from the Dutch document due to a lack of semantic alignment between the query and the documentâ€™s terminology. This highlights the limitations of using English\\-specific embeddings for non\\-English tasks.\n* **Cohere Embed Multilingual v3**: The multilingual model excelled in this test, retrieving and leveraging contextually relevant information from the Dutch document. This demonstrates the importance of multilingual embeddings for achieving accurate retrieval and answering non\\-English queries effectively.\n\n\n## Conclusion\n\nThis evaluation highlights a critical insight for anyone building Retrieval\\-Augmented Generation (RAG) systems for non\\-English languages: **embeddings matter**, especially when the task demands nuanced understanding across languages. The stark contrast in performance between the Cohere Embed English v3 and Cohere Embed Multilingual v3 models illustrates the limitations of English\\-specific embeddings in non\\-English contexts and the immense value of multilingual models.\n\nWhen tasked with answering a query in Dutch, the multilingual model excelled, retrieving accurate and contextually rich information directly from the document. Meanwhile, the English embedding model defaulted to generic, unrelated knowledge, demonstrating its inability to bridge the semantic gap between the query and the documentâ€™s content.\n\nFor organizations developing AI systems in a global, multilingual landscape, this test reinforces the importance of choosing the right embedding models for the task at hand. Multilingual embeddings are not just a â€œnice\\-to\\-haveâ€ feature; they are essential for ensuring accuracy, relevance, and user trust in non\\-English applications.\n\nAs generative AI continues to expand its reach, embracing language diversity through better embeddings will be key to delivering meaningful and impactful solutions worldwide. By prioritizing multilingual capabilities, businesses can create systems that are not only smarter but also more inclusive â€” empowering users across languages and cultures.\n\n***Follow me for more AI deep dives!***\n\n[Medium](https://proxy.rifx.online/https://medium.com/@lorevanoudenhove), [Instagram](https://proxy.rifx.online/https://www.instagram.com/lorevanoudenhove.ai/), [YouTube](https://proxy.rifx.online/https://www.youtube.com/channel/UCVyOJS1VV7FxPsStK65pHcA), [Pairrot](https://proxy.rifx.online/https://www.pairrot.eu/)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/will-bolt-new-ai-will-replace-v0-dev-129a3366eb44","frontmatter":{"title":"Will Bolt.new AI will replace v0.dev","meta_title":"Will Bolt.new AI will replace v0.dev","description":"Will Bolt.new AI will replace v0.dev","date":"2024-11-13T01:22:35.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*g5S8PyYqR87bdyGhb77rRw.png","categories":["Programming","Technology/Web","Data Science"],"author":"Rifx.Online","tags":["Bolt.new","v0.dev","web","components","layouts"],"draft":false,"slug":"blog/will-bolt-new-ai-will-replace-v0-dev-129a3366eb44"},"content":"\n### AI Tools\n\n> **Not a Member? Read for FREE [here](https://proxy.rifx.online/https://tarzzotech.medium.com/129a3366eb44?source=friends_link&sk=385b6b2e482ae9d16ef8f99fe083b8ae).**\n\n\n\nThe world of web development is witnessing rapid growth with multiple AI tools in the market. These new AI tools help developers generate web components and complex code structures through natural language prompts. These tools provide better code quality and reduce time in writing manual code.\n\nRecently there was a new tool **bolt.new** came into the market which looks similar to v0\\.dev. With the arrival of Bolt.new, a question arises: ***Will this new AI platform replace v0\\.dev, or do these tools serve different purposes altogether?***\n\nIn this post, I will share my experience of using the tool for some time. I mainly talk about the key differences between **v0\\.dev** and **Bolt.new**, comparing their strengths, use cases, and outputs. By examining real\\-world examples, weâ€™ll determine whether **Bolt.new** poses a legitimate threat to **v0\\.dev**, or if both tools have their place in a developerâ€™s toolkit.\n\nIn my previous blog, I explained all the details and my thoughts about v0\\.dev, so check it [**here**](https://proxy.rifx.online/https://tarzzotech.medium.com/4191292876b3?source=friends_link&sk=9730b35a75771953d0541e459c8adeaa). I don't want to share the repeated content here as well. Let's see here about **bolt.new** and comparison with **v0\\.dev.**\n\n## Overview of Bolt.new\n\n**Bolt.new** has made waves as a newer AI\\-powered platform with ambitions to go beyond just front\\-end components. While **v0\\.dev** is focused on generating smaller components, Bolt.new aims to deliver more complex solutions, such as full\\-page layouts or even multi\\-step workflows that span both the front\\-end and back\\-end.\n\n**Key Features of Bolt.new:**\n\n* **Full Layouts:** **Bolt.new** can generate full\\-page layouts, including headers, footers, sidebars, and main content areas.\n* **Versatile Code Generation:** Beyond web components, **Bolt.new** can produce server\\-side scripts, database configurations, and other elements essential for building full\\-stack applications.\n* **Enhanced Customization:** While V0\\.dev focuses on individual components, **Bolt.new** offers flexibility across larger scopes, allowing developers to generate fully functional layouts or structures.\n\n## Comparing V0\\.dev and Bolt.new\n\nWhile both tools offer impressive features, they cater to different aspects of the development process.\n\n* **v0\\.dev** focuses on individual web components that can be easily customized and integrated into front\\-end codebases. Itâ€™s ideal for developers needing quick solutions for buttons, cards, or forms that work across frameworks like React or Vue.\n* **Bolt.new**, in contrast, takes a more comprehensive approach, allowing developers to generate full\\-page layouts or even back\\-end configurations. This makes it a more versatile option for projects that require multiple types of code beyond just front\\-end components.\n\n## Example Comparison:\n\nHere we start to compare these tools from small components to full pages.\n\n### Creating a Button Component\n\n**Prompt:** â€œGenerate a responsive navigation bar with dropdown menus and a search bar.â€\n\n**v0\\.dev**\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8-1NaJb_msK1OLv7MHbOjw.gif)\n\n**bolt.new**\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*WsRSUU5brIql4uBb7wFkAg.gif)\n\n**v0\\.dev** created a button element with basic styling and a hover effect that changes the background color and text style when hovered.\n\n**Bolt.new** created a button with a hover effect and JavaScript code that triggers an alert box when clicked, including inline JavaScript and basic CSS styling.\n\n### Creating a Navigation Bar\n\n**Prompt:** â€œGenerate a responsive navigation bar with dropdown menus and a search bar.â€\n\n**v0\\.dev**\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*sOJ0EveSOVKtJJltLGiVCA.gif)\n\n**bolt.new**\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*aGCWfH5ULTTFb-FS-kT-7w.gif)\n\nBoth tools have generated the Navbar a little similarly. But **bolt.new** has generated a little better than **v0\\.dev**.\n\n### Creating a Website.\n\n**Prompt:** â€œGenerate a full page layout with a sticky header, footer, collapsible sidebar, and main content area designed for blog posts.â€\n\n**v0\\.dev**\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kwEXDG3tb1CiHetZr5W03Q.png)\n\n**bolt.new**\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*HxKnFJQf--e-E1fh8yyxVw.png)\n\nThe output from the **bolt.new** is speechless.\n\n## Finally, Will Bolt.new Replace v0\\.dev?\n\nGiven the broader scope of Bolt.new, it might seem like it could overshadow v0\\.dev. However, the two platforms are likely to coexist, each offering value for different stages of the development process.\n\n**v0\\.dev** is unmatched in its ability to generate specific, high\\-quality web components quickly. Itâ€™s perfect for front\\-end developers who need reusable, responsive components that are ready to integrate into their projects with minimal fuss.\n\n**Bolt.new**, while offering more flexibility, is not necessarily a replacement for v0\\.dev. Its broader range of functionality appeals to developers who need full layouts or code structures that span both the front end and back end.\n\n## Conclusion\n\nBoth v0\\.dev and Bolt.new are powerful AI tools, each with unique strengths. While Bolt.newâ€™s versatility makes it a strong contender for developers looking to generate more complex code, v0\\.dev remains a go\\-to tool for developers who want quick, customizable components.\n\nUltimately, the question of whether Bolt.new will replace v0\\.dev? Itâ€™s unlikely. These platforms cater to distinct needs. v0\\.dev shines in generating specific, reusable components with ease. Bolt.new offers broader functionality for complex layouts. So, which tool do you think will dominate in the future of AI AI\\-assisted development, or will they continue to complement each other? The answer depends on the developer's need on which you are working.\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/10-creative-ways-to-use-chatgpt-search-the-web-feature-7f145c5cfa30","frontmatter":{"title":"ä½¿ç”¨ ChatGPT æœç´¢ç½‘ç»œåŠŸèƒ½çš„ 10 ç§åˆ›æ„æ–¹æ³•","meta_title":"ä½¿ç”¨ ChatGPT æœç´¢ç½‘ç»œåŠŸèƒ½çš„ 10 ç§åˆ›æ„æ–¹æ³•","description":"ChatGPT çš„â€œæœç´¢ç½‘ç»œâ€åŠŸèƒ½ä¸ºç”¨æˆ·æä¾›äº†å¤šç§åˆ›æ–°çš„ä½¿ç”¨æ–¹å¼ï¼ŒåŒ…æ‹¬å…³æ³¨æ—¶äº‹æ–°é—»ã€è§„åˆ’æ—…è¡Œè¡Œç¨‹ã€å‘çŽ°æ–°é£Ÿè°±ã€ç›‘æµ‹å¸‚åœºè¶‹åŠ¿ã€èŽ·å–å®žæ—¶æ•°æ®ã€æŸ¥æ‰¾æœ¬åœ°æ´»åŠ¨ã€æ¯”è¾ƒäº§å“ã€äº†è§£æ–°å…´æŠ€æœ¯ã€èŽ·å–ç©ºæ°”è´¨é‡æ›´æ–°å’ŒæŽ¢ç´¢æ•™è‚²èµ„æºã€‚è¯¥åŠŸèƒ½èƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·èŽ·å–æœ€æ–°ä¿¡æ¯ï¼Œæå‡ä¿¡æ¯æ£€ç´¢çš„æ•ˆçŽ‡ï¼Œé€‚ç”¨äºŽå¤šç§éœ€æ±‚ã€‚","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*S4RtWt6Ouspx4nnl","categories":["Chatbots","Technology/Web","Education"],"author":"Rifx.Online","tags":["ChatGPT","search","web","real-time","information"],"draft":false,"slug":"blog/10-creative-ways-to-use-chatgpt-search-the-web-feature-7f145c5cfa30"},"content":"\n\n\n### ä¾‹å¦‚ï¼Œæç¤ºå’Œè¾“å‡º\n\n\n\nä½ çŸ¥é“å¯ä»¥ä½¿ç”¨ ChatGPT çš„â€œæœç´¢ç½‘ç»œâ€åŠŸèƒ½æ¥å®Œæˆè®¸å¤šä»»åŠ¡ï¼Œè€Œä¸ä»…ä»…æ˜¯åŸºæœ¬çš„ç½‘ç»œæœç´¢å—ï¼Ÿ\n\nå¯¹äºŽé‚£äº›ä¸çŸ¥é“çš„äººï¼ŒChatGPT æ–°çš„â€œæœç´¢ç½‘ç»œâ€åŠŸèƒ½æä¾›å®žæ—¶ä¿¡æ¯ã€‚\n\næˆªè‡³æ’°å†™æ­¤å¸–æ—¶ï¼Œè¯¥åŠŸèƒ½ä»…å¯¹ä½¿ç”¨ ChatGPT 4o å’Œ 4o-mini çš„ä»˜è´¹ä¼šå‘˜å¼€æ”¾ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*uyESPHmmvzSJjZmgpn_Oww.png)\n\nä»¥ä¸‹æ˜¯ä¸€äº›ä½¿ç”¨æ­¤åŠŸèƒ½çš„åˆ›æ„æ–¹å¼ï¼š\n\n## 1\\. å…³æ³¨æ—¶äº‹æ–°é—»ï¼š\n\nå¦‚æžœæ‚¨å¯¹æœ€æ–°çš„æ–°é—»å’Œäº‹ä»¶æ„Ÿå…´è¶£ï¼Œä½†æ²¡æœ‰æ—¶é—´åŽ»æœç´¢å’Œå¯»æ‰¾æœ€ä½³ä¿¡æ¯ï¼Œè¿™ä¸ªåŠŸèƒ½é€‚åˆæ‚¨ã€‚\n\nçŽ°åœ¨ï¼Œä½¿ç”¨ ChatGPT çš„æœç´¢ç½‘é¡µåŠŸèƒ½ï¼Œæ‚¨å¯ä»¥éšæ—¶æŽ¥æ”¶æœ€æ–°æ–°é—»ã€ä½“è‚²æ¯”åˆ†å’Œè‚¡å¸‚æ›´æ–°çš„æ‘˜è¦ã€‚\n\n**ç¤ºä¾‹**ï¼šâ€” â€œ***ä»Šå¤©ç§‘æŠ€é¢†åŸŸçš„æœ€æ–°æ–°é—»æ˜¯ä»€ä¹ˆï¼Ÿ***â€\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*OQFEyg8WFckOQcM5cKyRww.png)\n\n## 2\\. è®¡åˆ’æ—…è¡Œè¡Œç¨‹ï¼š\n\næ‚¨å–œæ¬¢ä¸ºæœ€ä½³æ—¶é—´å’Œé¢„ç®—ç®¡ç†è§„åˆ’æ—…è¡Œå—ï¼Ÿé‚£ä¹ˆæ‚¨ä¼šå‘çŽ°è¿™ä¸ªè¡Œç¨‹è§„åˆ’åŠŸèƒ½éžå¸¸æœ‰å¸®åŠ©ã€‚\n\nçŽ°åœ¨æ‚¨å¯ä»¥èŽ·å–æœ‰å…³æ—…è¡Œç›®çš„åœ°çš„æœ€æ–°ä¿¡æ¯ï¼ŒåŒ…æ‹¬å¤©æ°”é¢„æŠ¥ã€å½“åœ°æ´»åŠ¨ä»¥åŠæœ€ä½³è´­ç‰©å’Œç”¨é¤åœ°ç‚¹ã€‚\n\n**ç¤ºä¾‹ï¼š â€” â€œ*è¿™ä¸ªå‘¨æœ«å·´é»Žçš„ä¸»è¦æ™¯ç‚¹æœ‰å“ªäº›ï¼Ÿ*â€**\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BLm4PoTaxrXkBMoB56jg8g.png)\n\n## 3\\. å‘çŽ°æ–°é£Ÿè°±ï¼š\n\nå¦‚æžœä½ å–œæ¬¢å°è¯•æ–°é£Ÿç‰©å¹¶ä¸”çƒ­çˆ±çƒ¹é¥ªï¼Œè¿™ä¸ªæ–¹æ³•éžå¸¸é€‚åˆä½ ã€‚\n\nä½ å¯ä»¥åœ¨ ChatGPT ä¸­ä½¿ç”¨â€œæœç´¢ç½‘ç»œâ€æ¥æ ¹æ®æ—¥æœŸæˆ–åœ°ç‚¹æ‰¾åˆ°æµè¡Œçš„é£Ÿè°±æˆ–çƒ¹é¥ªæŠ€å·§ã€‚\n\n**ç¤ºä¾‹ï¼šâ€” â€œ*è¿™ä¸ªæœˆæœ‰ä»€ä¹ˆå—æ¬¢è¿Žçš„ç”œç‚¹é£Ÿè°±ï¼Ÿ*â€**\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*iSrMCgwjdOw4xOSC51LglA.png)\n\n## 4\\. ç›‘æµ‹å¸‚åœºè¶‹åŠ¿ï¼š\n\nå¦‚æžœæ‚¨å–œæ¬¢é˜…è¯»æˆ–ä¿æŒå¯¹æŸä¸€çŸ¥è¯†é¢†åŸŸçš„æ›´æ–°ï¼Œé‚£ä¹ˆè¿™ä¸ªåŠŸèƒ½é€‚åˆæ‚¨ã€‚\n\nçŽ°åœ¨ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æœç´¢ç½‘ç»œåŠŸèƒ½è·Ÿè¸ªä»»ä½•è¡Œä¸šè¶‹åŠ¿ã€‚\n\n**ç¤ºä¾‹ï¼š â€” â€œ*å¤ªé˜³èƒ½çš„æœ€æ–°å‘å±•æ˜¯ä»€ä¹ˆï¼Ÿ*â€**\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*M-Y7hXRYMXGs_V6iHOm7lQ.png)\n\n## 5\\. è®¿é—®å®žæ—¶æ•°æ®ï¼š\n\næƒ³çŸ¥é“ä»Šå¤©ä¼šä¸‹é›¨å—ï¼Ÿ\n\næˆ–è€…æƒ³çŸ¥é“æœ€æ–°çš„æ¯”åˆ†å´æ— æ³•è§‚çœ‹çŽ°åœºç›´æ’­çš„ä½“è‚²æ¯”èµ›ï¼Ÿ\n\nçŽ°åœ¨æ‚¨å¯ä»¥èŽ·å–å®žæ—¶æ•°æ®ï¼Œä¾‹å¦‚å¤©æ°”æ›´æ–°ã€è‚¡ç¥¨ä»·æ ¼æˆ–ä½“è‚²æ¯”åˆ†ã€‚\n\n**ç¤ºä¾‹ï¼šâ€” â€œ*çº½çº¦å¸‚å½“å‰çš„å¤©æ°”æ€Žä¹ˆæ ·ï¼Ÿ*â€**\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*TwPSdHgHdaKmipspoyldsg.png)\n\n## 6\\. æŸ¥æ‰¾æœ¬åœ°æ´»åŠ¨ï¼š\n\nè¿™é‡Œçš„å¤–å‘åž‹äººä¼šå–œæ¬¢è¿™ä¸ªåŠŸèƒ½ã€‚\n\nçŽ°åœ¨æ‚¨å¯ä»¥å‘çŽ°æ‚¨æ‰€åœ¨åœ°åŒºæ­£åœ¨å‘ç”Ÿçš„æ´»åŠ¨ï¼Œå¹¶ä¸”å®ƒè¿˜ä¼šæä¾›ç›´æŽ¥é“¾æŽ¥åˆ°ç½‘ç«™ï¼Œæ‚¨å¯ä»¥åœ¨å…¶ä¸­äº†è§£æ›´å¤šç»†èŠ‚å¹¶é¢„è®¢ç¥¨ã€‚\n\n**ç¤ºä¾‹ï¼šâ€” â€œ*è¿™ä¸ªå‘¨æœ«åœ¨æ‚‰å°¼æœ‰å“ªäº›æ´»åŠ¨ï¼Ÿ*â€**\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MgSawNL8kSTohGsIU0ajrA.png)\n\n## 7\\. æ¯”è¾ƒäº§å“ï¼š\n\næƒ³è´­ä¹°æ–°äº§å“å—ï¼Ÿæƒ³çŸ¥é“å®ƒçš„ä¼˜ç¼ºç‚¹ï¼Ÿ\n\næˆ–è€…æ‚¨å¯èƒ½æƒ³æ¯”è¾ƒå‡ ä¸ªäº§å“ï¼Œä»¥å†³å®šå…¶ä¸­æœ€å¥½çš„ã€‚\n\nçŽ°åœ¨æ‚¨å¯ä»¥ä½¿ç”¨æœç´¢ç½‘ç»œåŠŸèƒ½æ¥æ¯”è¾ƒäº§å“æˆ–æœåŠ¡ã€‚\n\n**ç¤ºä¾‹ï¼šâ€” â€œ*æ¯”è¾ƒä»Šå¹´å‘å¸ƒçš„æœ€æ–°æ™ºèƒ½æ‰‹æœºã€‚*â€**\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*HvzNuBcc6kWNSZA7Sj2hUg.png)\n\n## 8\\. äº†è§£æ–°å…´æŠ€æœ¯ï¼š\n\nå¦‚æžœä½ æƒ³åœ¨æ–°å…´æŠ€æœ¯é¢†åŸŸä¿æŒæ›´æ–°ï¼Œè¿™ä¸ªé€‚åˆä½ ã€‚\n\nä½¿ç”¨â€œæœç´¢ç½‘ç»œâ€åŠŸèƒ½ï¼ŒçŽ°åœ¨ä½ å¯ä»¥äº†è§£æ–°æŠ€æœ¯çš„æœ€æ–°åŠ¨æ€ã€‚\n\n**ç¤ºä¾‹ï¼šâ€” â€œ*äººå·¥æ™ºèƒ½çš„æœ€æ–°è¿›å±•æ˜¯ä»€ä¹ˆï¼Ÿ*â€**\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*VwySsjMn59nvqxHUWm1AtA.png)\n\n## 9\\. èŽ·å–ç©ºæ°”è´¨é‡æ›´æ–°ï¼š\n\nè¿™æ˜¯ ChatGPT ä½ç½®å’Œæ—¶é—´æ„ŸçŸ¥æœç´¢ç»“æžœçš„å¦ä¸€ä¸ªé…·ç‚«ç”¨æ³•ã€‚\n\nçŽ°åœ¨æ‚¨å¯ä»¥æ£€æŸ¥ä»»ä½•åœ°ç‚¹çš„ç©ºæ°”è´¨é‡æŒ‡æ•°ã€‚ä½¿ç”¨æ­¤åŠŸèƒ½äº†è§£æ±¡æŸ“æ°´å¹³ã€‚\n\n**ç¤ºä¾‹ï¼šâ€” â€œ*çŽ°åœ¨çº½çº¦çš„ AQI æ˜¯å¤šå°‘ï¼Ÿ*â€**\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*6C_VcWft52zoR57XzEMo-A.png)\n\n## 10\\. æŽ¢ç´¢æ•™è‚²èµ„æºï¼š\n\nå¦‚æžœæ‚¨å–œæ¬¢é€šè¿‡è§‚çœ‹è¯¾ç¨‹æ¥å­¦ä¹ æŸä¸ªä¸»é¢˜ï¼Œé‚£ä¹ˆè¿™å°†å¯¹æ‚¨å¸®åŠ©å¾ˆå¤§ã€‚\n\næ‚¨å¯ä»¥ä½¿ç”¨æœç´¢åŠŸèƒ½æŸ¥æ‰¾æ„Ÿå…´è¶£ä¸»é¢˜çš„æœ€æ–°æ–‡ç« æˆ–è¯¾ç¨‹ã€‚\n\n**ç¤ºä¾‹ï¼šâ€” â€œ*æœ‰å“ªäº›æœ€æ–°çš„æ•°æ®ç§‘å­¦åœ¨çº¿è¯¾ç¨‹å¯ç”¨ï¼Ÿ*â€**\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*tPq8Lve_M_1sNhqfkqtwyg.png)\n\nä½¿ç”¨ ChatGPT çš„ç½‘ç»œæœç´¢åŠŸèƒ½ï¼Œæ‚¨å¯ä»¥ä»¥å¤šç§ä¸åŒæ–¹å¼è®¿é—®æœ€æ–°å’Œç›¸å…³çš„ä¿¡æ¯ã€‚\n\nå¦‚æžœæ‚¨è¿˜å‘çŽ°äº†ä¸€äº›æ–°çš„å’Œä¸åŒçš„å®žæ—¶æœç´¢ç”¨æ³•ï¼Œè¯·åœ¨è¯„è®ºä¸­å‘Šè¯‰æˆ‘ã€‚\n\n"},{"lang":"zh","group":"blog","slug":"blog/10-must-learn-skills-to-stay-ahead-in-ai-and-tech-42f4140713b1","frontmatter":{"title":"åœ¨äººå·¥æ™ºèƒ½å’ŒæŠ€æœ¯é¢†åŸŸä¿æŒé¢†å…ˆåœ°ä½çš„ 10 é¡¹å¿…å­¦æŠ€èƒ½ ðŸ“š","meta_title":"åœ¨äººå·¥æ™ºèƒ½å’ŒæŠ€æœ¯é¢†åŸŸä¿æŒé¢†å…ˆåœ°ä½çš„ 10 é¡¹å¿…å­¦æŠ€èƒ½ ðŸ“š","description":"åœ¨äººå·¥æ™ºèƒ½å’Œç§‘æŠ€é¢†åŸŸï¼ŒæŒç»­æå‡æŠ€èƒ½è‡³å…³é‡è¦ã€‚æœ¬æ–‡åˆ—å‡ºäº†10ä¸ªå…³é”®è¯¾ç¨‹ï¼ŒåŒ…æ‹¬ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½ã€è°ƒè¯•æŠ€æœ¯ã€AIäº§å“è¶‹åŠ¿ã€æ³•å¾‹é¢†åŸŸçš„æç¤ºå·¥ç¨‹ç­‰ï¼Œæ—¨åœ¨å¸®åŠ©ä¸“ä¸šäººå£«æŽŒæ¡æ–°æŠ€æœ¯ï¼Œä¿æŒç«žäº‰åŠ›ã€‚è¿™äº›è¯¾ç¨‹æ¶µç›–ä»ŽåŸºç¡€åˆ°ä¸“ä¸šçš„å¤šç§çŸ¥è¯†ï¼Œé€‚åˆä¸åŒèƒŒæ™¯çš„å­¦ä¹ è€…ï¼ŒæŽ¨åŠ¨èŒä¸šå‘å±•å’Œåˆ›æ–°ã€‚","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*uKN-KrOhsDhRrAjL","categories":["Technology","Generative AI","Data Science"],"author":"Rifx.Online","tags":["generative","debugging","recommendations","fundamentals","competitive"],"draft":false,"slug":"blog/10-must-learn-skills-to-stay-ahead-in-ai-and-tech-42f4140713b1"},"content":"\n\n\n\n\nåœ¨äººå·¥æ™ºèƒ½å’Œç§‘æŠ€è¿™æ ·ä¸€ä¸ªåŠ¨æ€çš„è¡Œä¸šä¸­ï¼Œä¿æŒé¢†å…ˆæ„å‘³ç€ä¸æ–­æå‡ä½ çš„æŠ€èƒ½ã€‚æ— è®ºä½ æ˜¯å¸Œæœ›æ·±å…¥äº†è§£äººå·¥æ™ºèƒ½æ¨¡åž‹æ€§èƒ½ã€æŽŒæ¡æ•°æ®åˆ†æžï¼Œè¿˜æ˜¯å¸Œæœ›é€šè¿‡äººå·¥æ™ºèƒ½è½¬å˜ä¼ ç»Ÿé¢†åŸŸå¦‚æ³•å¾‹ï¼Œè¿™äº›è¯¾ç¨‹éƒ½æ˜¯ä½ æˆåŠŸçš„æ·å¾„ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªç²¾å¿ƒç­–åˆ’çš„é«˜ä»·å€¼è¯¾ç¨‹åˆ—è¡¨ï¼Œå¯ä»¥åŠ©åŠ›ä½ çš„èŒä¸šå‘å±•ï¼Œå¹¶è®©ä½ å§‹ç»ˆå¤„äºŽåˆ›æ–°çš„å‰æ²¿ã€‚\n\n## 1\\. ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½ç®€ä»‹\n\n* **è¯¾ç¨‹**: [ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½ç®€ä»‹](https://genai.works/courses/introduction-to-generative-ai-english)\n* **æä¾›è€…**: Google Cloud\n* **ä¸ºä»€ä¹ˆè¦å‚åŠ **: äº†è§£ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½æ¨¡åž‹çš„åŸºæœ¬åŽŸç†åŠå…¶å¦‚ä½•å½±å“å„ä¸ªè¡Œä¸šã€‚éžå¸¸é€‚åˆå¸Œæœ›æŽŒæ¡äººå·¥æ™ºèƒ½å˜é©æ½œåŠ›çš„ä»»ä½•äººã€‚\n\n## 2\\. è°ƒè¯•ç”Ÿæˆå¼äººå·¥æ™ºèƒ½\n\n* **è¯¾ç¨‹**: [è°ƒè¯•ç”Ÿæˆå¼äººå·¥æ™ºèƒ½](https://genai.works/courses/evaluating-and-debugging-generative-ai)\n* **æä¾›è€…**: DeepLearning AI\n* **ä¸ºä»€ä¹ˆé€‰æ‹©å®ƒ**: å­¦ä¹ æŽ’é™¤æ•…éšœå’Œä¼˜åŒ–ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ¨¡åž‹ï¼Œç¡®ä¿å®ƒä»¬å¯é ä¸”é«˜æ•ˆåœ°è¿è¡Œã€‚å¯¹äºŽå¸Œæœ›å¾®è°ƒæ¨¡åž‹ä»¥èŽ·å¾—æ›´å¥½ç»“æžœçš„äººå·¥æ™ºèƒ½ä¸“ä¸šäººå£«è€Œè¨€ï¼Œè¿™æ˜¯å¿…ä¸å¯å°‘çš„ã€‚\n\n## 3\\. é¡¶çº§ AI æ”¯æŒçš„äº§å“\n\n* **è¯¾ç¨‹**: [é¡¶çº§ AI æ”¯æŒçš„äº§å“](https://genai.works/courses/top-100-best-selling-products-ai)\n* **æä¾›è€…**: å¯†æ­‡æ ¹å¤§å­¦\n* **ä¸ºä»€ä¹ˆå‚åŠ **: äº†è§£ AI é©±åŠ¨äº§å“çš„æœ€æ–°è¶‹åŠ¿ï¼Œå¸®åŠ©æ‚¨ç†è§£è¡Œä¸šçš„å‘å±•æ–¹å‘ä»¥åŠå¸‚åœºä¸Šä¸»å¯¼çš„åˆ›æ–°ã€‚\n\n## 4\\. åŸºäºŽå‘é‡æ•°æ®åº“çš„äººå·¥æ™ºèƒ½æŽ¨è\n\n* **è¯¾ç¨‹**: [åŸºäºŽå‘é‡æ•°æ®åº“çš„äººå·¥æ™ºèƒ½æŽ¨è](https://genai.works/courses/vector-database-projects-ai-recommendation-systems)\n* **æä¾›è€…**: IBM\n* **ä¸ºä»€ä¹ˆå­¦ä¹ **: ä½¿ç”¨å‘é‡æ•°æ®åº“å¼€å‘æ™ºèƒ½æŽ¨èç³»ç»Ÿï¼Œè¿™æ˜¯åœ¨æŠ€æœ¯ã€ç”µå­å•†åŠ¡å’Œåª’ä½“ä¸­æž„å»ºä¸ªæ€§åŒ–çš„äººå·¥æ™ºèƒ½é©±åŠ¨ä½“éªŒçš„é‡è¦æŠ€èƒ½ã€‚\n\n## 5\\. AI for Software Teams\n\n* **è¯¾ç¨‹**: [å›¢é˜Ÿè½¯ä»¶å·¥ç¨‹ä¸ŽAI](https://genai.works/courses/team-software-engineering-with-ai)\n* **æä¾›è€…**: DeepLearning AI\n* **é€‰æ‹©åŽŸå› **: æœ¬è¯¾ç¨‹ä½¿è½¯ä»¶å›¢é˜Ÿèƒ½å¤Ÿåˆ©ç”¨AIå·¥å…·æé«˜åä½œæ•ˆçŽ‡ï¼Œä½¿å›¢é˜Ÿåˆä½œæ›´åŠ é«˜æ•ˆï¼ŒæŠ€æœ¯é¡¹ç›®æ›´åŠ æˆåŠŸã€‚\n\n## 6\\. æŠ€æœ¯æ”¯æŒåŸºç¡€\n\n* **è¯¾ç¨‹**: [æŠ€æœ¯æ”¯æŒåŸºç¡€](https://genai.works/courses/technical-support-fundamentals)\n* **æä¾›è€…**: Google\n* **ä¸ºä»€ä¹ˆé€‰æ‹©å®ƒ**: å¯¹äºŽå¸Œæœ›å»ºç«‹æ•…éšœæŽ’é™¤æŠ€èƒ½çš„ä»»ä½•äººæ¥è¯´ï¼Œè¿™æ˜¯å¿…ä¸å¯å°‘çš„è¯¾ç¨‹ï¼Œä¸ºæŠ€æœ¯æ”¯æŒè§’è‰²å’ŒITåŸºç¡€è®¾æ–½ç®¡ç†å¥ å®šäº†åŸºç¡€ã€‚\n\n## 7\\. æ³•å¾‹çš„æç¤ºå·¥ç¨‹\n\n* **è¯¾ç¨‹**: [æ³•å¾‹çš„ä¸“ä¸šåŒ–æç¤ºå·¥ç¨‹](https://genai.works/courses/specialization-prompt-engineering-for-law)\n* **æä¾›è€…**: èŒƒå¾·æ¯”å°”ç‰¹å¤§å­¦\n* **ä¸ºä»€ä¹ˆé€‰æ‹©å®ƒ**: åˆ©ç”¨äººå·¥æ™ºèƒ½é©±åŠ¨çš„æç¤ºå·¥ç¨‹ï¼Œå˜é©æ³•å¾‹é¢†åŸŸã€‚è¯¥è¯¾ç¨‹éžå¸¸é€‚åˆå¸Œæœ›é€šè¿‡æŠ€æœ¯åˆ›æ–°å’Œç®€åŒ–å·¥ä½œæµç¨‹çš„æ³•å¾‹ä¸“ä¸šäººå£«ã€‚\n\n## 8\\. ä½¿ç”¨ Excel å‡†å¤‡æ•°æ®è¿›è¡Œåˆ†æž\n\n* **è¯¾ç¨‹**: [ä½¿ç”¨ Excel å‡†å¤‡æ•°æ®è¿›è¡Œåˆ†æž](https://genai.works/courses/preparing-data-for-analysis-using-microsoft-excel)\n* **æä¾›è€…**: Microsoft\n* **ä¸ºä»€ä¹ˆå­¦ä¹ **: æ”¹å–„æ‚¨çš„æ•°æ®åˆ†æžå·¥ä½œæµç¨‹ï¼Œæé«˜æ‚¨çš„ Excel æŠ€èƒ½ï¼Œä»¥ä¾¿æ›´å¥½åœ°å‡†å¤‡æ•°æ®ï¼Œè¿™å¯¹åˆçº§å’Œé«˜çº§æ•°æ®ç§‘å­¦èŒä½éƒ½è‡³å…³é‡è¦ã€‚\n\n## 9\\. ITå®‰å…¨ï¼šæŠµå¾¡æ•°å­—é»‘æš—è‰ºæœ¯\n\n* **è¯¾ç¨‹**: [ITå®‰å…¨åŸºç¡€](https://lnkd.in/dTY2Vbih)\n* **æä¾›è€…**: Google\n* **ä¸ºä»€ä¹ˆè¦å‚åŠ **: å­¦ä¹ ä¿æŠ¤æ‚¨çš„æ•°å­—èµ„äº§å¹¶äº†è§£ç½‘ç»œå®‰å…¨çš„åŸºç¡€çŸ¥è¯†ã€‚æœ¬è¯¾ç¨‹æ¶µç›–äº†æŠµå¾¡ç½‘ç»œå¨èƒçš„å…³é”®æŠ€èƒ½ã€‚\n\n## 10\\. Pythonä¸­çš„æ•°æ®ç»“æž„\n\n* **è¯¾ç¨‹**: [Pythonä¸­çš„æ•°æ®ç»“æž„](https://genai.works/courses/data-structures-in-python)\n* **æä¾›è€…**: å¯†æ­‡æ ¹å¤§å­¦\n* **ä¸ºä»€ä¹ˆå­¦ä¹ **: é€šè¿‡æ·±å…¥ç†è§£Pythonä¸­çš„æ•°æ®ç»“æž„æ¥å¢žå¼ºæ‚¨çš„ç¼–ç æŠ€èƒ½ï¼Œè¿™æ˜¯è½¯ä»¶å¼€å‘äººå‘˜å’Œæ•°æ®ç§‘å­¦å®¶å¿…å¤‡çš„æŠ€èƒ½ã€‚\n\n## âœ”ï¸ åŠ å…¥ \\#BuildwithAI é»‘å®¢é©¬æ‹‰æ¾ 2024\n\n* **å‚ä¸Žæ–¹å¼**: [é»‘å®¢é©¬æ‹‰æ¾é“¾æŽ¥](https://lnkd.in/dsapprp4)\n* **ä¸ºä»€ä¹ˆåŠ å…¥**: é€šè¿‡æµ‹è¯•ä½ çš„æŠ€èƒ½ï¼Œä¸Žå¿—åŒé“åˆçš„äººåˆä½œï¼Œä½¿ç”¨ AI è§£å†³å®žé™…é—®é¢˜ã€‚è¿™æ˜¯åº”ç”¨ä½ æ‰€å­¦çŸ¥è¯†å’Œå±•ç¤ºä½ ä¸“ä¸šæŠ€èƒ½çš„å®Œç¾Žæ–¹å¼ã€‚\n\n## ç»“è®º\n\n*ç§‘æŠ€é¢†åŸŸä¸æ–­å‘å±•ï¼ŒæŽŒæ¡è¿™äº›åŸºæœ¬æŠ€èƒ½å°†å¸®åŠ©ä½ ä¿æŒç«žäº‰åŠ›ã€‚ä»Žäººå·¥æ™ºèƒ½å’Œæ•°æ®ç»“æž„çš„åŸºç¡€è¯¾ç¨‹åˆ°æ³•å¾‹å·¥ä½œä¸­çš„äººå·¥æ™ºèƒ½ç­‰ä¸“ä¸šé¢†åŸŸï¼Œè¿™äº›è¯¾ç¨‹æ¶µç›–äº†å¹¿æ³›çš„çŸ¥è¯†ï¼Œå°†ä¸ºä½ çš„æœªæ¥åšå¥½å‡†å¤‡ã€‚ç«‹å³æ³¨å†Œï¼Œæ‰©å±•ä½ çš„ä¸“ä¸šçŸ¥è¯†ï¼Œå¼€å§‹åœ¨äººå·¥æ™ºèƒ½å’Œç§‘æŠ€é¢†åŸŸäº§ç”Ÿå½±å“ï¼*\n\nç¥ä½ å­¦ä¹ æ„‰å¿«ï¼Œåˆ›æ–°ä¸æ–­ï¼ ðŸš€\n\nå¦‚æžœä½ å¸Œæœ›ä½ çš„äººå·¥æ™ºèƒ½äº§å“è¢«å±•ç¤ºï¼Œæ¬¢è¿Žé€šè¿‡æˆ‘ä»¬çš„ [**Linkedin é¡µé¢**](https://www.linkedin.com/company/genai-works/)** è”ç³»æˆ‘ä»¬ã€‚**\n\nå…³æ³¨æˆ‘ä»¬çš„å®˜æ–¹ [**Instagram**](https://www.instagram.com/generativeai_official/?igsh=Zjc3NGU5N2ticzZ6)ã€[**TikTok**](https://www.tiktok.com/@generative_ai_official?_t=8kqDA0pyrC6&_r=1) å’Œ [**YouTube**](https://www.youtube.com/@generative.ai.official)ï¼ŒèŽ·å–æ¯æ—¥äººå·¥æ™ºèƒ½å†…å®¹ã€‚\n\nçŽ°åœ¨æ‰€æœ‰åˆåˆ›å…¬å¸å’Œå¼€å‘è€…éƒ½å¯ä»¥å…è´¹æ³¨å†Œä»–ä»¬çš„äººå·¥æ™ºèƒ½åº”ç”¨/é¡¹ç›®ï¼ [**https://genai.works/sign\\-up**](https://genai.works/sign-up)\n\né˜…è¯»é¡¶çº§äººå·¥æ™ºèƒ½æ–°é—»ï¼Œå¹¶åœ¨æˆ‘ä»¬çš„ [**ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ¯æ—¥é€šè®¯**](https://newsletter.genai.works/subscribe) ä¸­æ‰¾åˆ°å®žç”¨çš„å¤‡å¿˜å•ã€‚\n\n**å¿«é€Ÿäº‹å®žï¼š** 5\\+M å…³æ³¨è€… \\| 2\\.6M\\+ é€šè®¯è®¢é˜…è€…ã€‚æœ€å¤§ä¸”å¢žé•¿æœ€å¿«çš„ [**LinkedIn äººå·¥æ™ºèƒ½ç¤¾åŒº**](https://www.linkedin.com/company/genai-works/)ï¼Œç”±è¡Œä¸šå†…çš„äººå·¥æ™ºèƒ½ä¸“å®¶åˆ›ç«‹å’Œæ”¯æŒã€‚\n\n"},{"lang":"zh","group":"blog","slug":"blog/5-ai-projects-you-can-build-this-weekend-with-node-js-76e0ee51cc72","frontmatter":{"title":"æœ¬å‘¨æœ«æ‚¨å¯ä»¥æž„å»ºçš„ 5 ä¸ªäººå·¥æ™ºèƒ½é¡¹ç›®ï¼ˆä½¿ç”¨ Node.jsï¼‰","meta_title":"æœ¬å‘¨æœ«æ‚¨å¯ä»¥æž„å»ºçš„ 5 ä¸ªäººå·¥æ™ºèƒ½é¡¹ç›®ï¼ˆä½¿ç”¨ Node.jsï¼‰","description":"æœ¬æ–‡ä»‹ç»äº†äº”ä¸ªé€‚åˆåˆå­¦è€…åœ¨å‘¨æœ«ä½¿ç”¨ Node.js æž„å»ºçš„ AI é¡¹ç›®ï¼ŒåŒ…æ‹¬å®¢æˆ·æ”¯æŒèŠå¤©æœºå™¨äººã€å›¾åƒè¯†åˆ«åº”ç”¨ã€ç¤¾äº¤åª’ä½“æƒ…æ„Ÿåˆ†æžå·¥å…·ã€è¯­éŸ³å‘½ä»¤åº”ç”¨ç¨‹åºå’Œä¸ªæ€§åŒ–ç”µå½±æŽ¨èç³»ç»Ÿã€‚è¿™äº›é¡¹ç›®æ—¨åœ¨æå‡ç¼–ç æŠ€èƒ½å¹¶äº†è§£äººå·¥æ™ºèƒ½çš„å®žé™…åº”ç”¨ï¼Œé€‚åˆå¸Œæœ›äº²è‡ªæŽ¢ç´¢ AI çš„å¼€å‘è€…ã€‚æ¯ä¸ªé¡¹ç›®éƒ½æä¾›äº†æ‰€éœ€çš„æŠ€æœ¯æ ˆå’Œå·¥å…·ï¼Œé¼“åŠ±å¼€å‘è€…æ ¹æ®ä¸ªäººåˆ›æ„è¿›è¡Œè°ƒæ•´ã€‚","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*x9ezYQZawlG0DRV6","categories":["Programming/Scripting","Natural Language Processing","Computer Vision"],"author":"Rifx.Online","tags":["Node.js","chatbot","image","sentiment","recommender"],"draft":false,"slug":"blog/5-ai-projects-you-can-build-this-weekend-with-node-js-76e0ee51cc72"},"content":"\n5 ä¸ªé€‚åˆåœ¨å‘¨æœ«ç”¨ Node.js æž„å»ºçš„ä»¤äººå…´å¥‹çš„ AI é¡¹ç›®ï¼ˆéžå¸¸é€‚åˆåˆå­¦è€…ï¼‰\n\n\n\nä½ æ˜¯å¦å¯¹æž„å»º AI é¡¹ç›®æ„Ÿå…´è¶£ï¼Œä½†æ—¶é—´ä¸å¤Ÿï¼Ÿ\n\nåªéœ€ Node.js å’Œä¸€ä¸ªå‘¨æœ«ï¼Œä½ å°±å¯ä»¥æŠ•å…¥åˆ°åŠ¨æ‰‹å®žè·µçš„ AI é¡¹ç›®ä¸­ï¼Œè¿™äº›é¡¹ç›®å°†æå‡ä½ çš„ç¼–ç æŠ€èƒ½ï¼Œå¹¶è®©ä½ äº†è§£äººå·¥æ™ºèƒ½çš„å®žé™…åº”ç”¨ã€‚\n\nè¿™äº›é€‚åˆåˆå­¦è€…çš„é¡¹ç›®å°†æŒ‡å¯¼ä½ è®¾ç½®èŠå¤©æœºå™¨äººã€å›¾åƒè¯†åˆ«ã€æƒ…æ„Ÿåˆ†æžç­‰ã€‚\n\næ‰€ä»¥ï¼Œæ‹¿èµ·ä½ çš„ç¬”è®°æœ¬ç”µè„‘ï¼Œå‡†å¤‡å¥½ç”¨è¿™äº”ä¸ªä»¤äººå…´å¥‹çš„ AI é¡¹ç›®æ¥ç¼–ç å§ï¼\n\n### 1\\. å®¢æˆ·æ”¯æŒèŠå¤©æœºå™¨äºº ðŸ¤–\n\nèŠå¤©æœºå™¨äººæ˜¯æŽ¢ç´¢è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„çƒ­é—¨æ–¹å¼ï¼Œä½¿ç”¨ Node.jsï¼Œæ‚¨å¯ä»¥è®¾ç½®ä¸€ä¸ªåŸºæœ¬çš„èŠå¤©æœºå™¨äººæ¥å¤„ç†å®¢æˆ·å’¨è¯¢å¹¶æä¾›ç­”æ¡ˆã€‚\n\n**ä¸ºä»€ä¹ˆè¦æž„å»ºè¿™ä¸ªé¡¹ç›®ï¼Ÿ**åˆ›å»ºèŠå¤©æœºå™¨äººå¯ä»¥è®©æ‚¨äº†è§£ NLP å’Œå®žæ—¶æœåŠ¡å™¨äº¤äº’çš„åŸºç¡€çŸ¥è¯†ï¼Œè¿™åœ¨ AI å¼€å‘ä¸­æ˜¯éžå¸¸å®è´µçš„æŠ€èƒ½ã€‚\n\n**æ‚¨éœ€è¦çš„ï¼š**\n\n* **Node.js å’Œ Express** ç”¨äºŽ [è®¾ç½®æœåŠ¡å™¨](https://expressjs.com/)\n* **Dialogflow**ï¼ˆç”± Google æä¾›ï¼‰æˆ– [**ChatGPT API**](https://platform.openai.com/docs/api-reference/introduction) ç”¨äºŽè‡ªç„¶è¯­è¨€å¤„ç†\n* **Socket.io** ç”¨äºŽå®žæ—¶èŠå¤©åŠŸèƒ½\n\n## 2. ä½¿ç”¨ Node.js æž„å»º AI é©±åŠ¨çš„å›¾åƒè¯†åˆ«åº”ç”¨ \n\nè¯¥é¡¹ç›®æ¶‰åŠåˆ›å»ºä¸€ä¸ªå›¾åƒè¯†åˆ«åº”ç”¨ï¼Œèƒ½å¤Ÿè¯†åˆ«ç…§ç‰‡ä¸­çš„ç‰©ä½“ã€åŠ¨ç‰©æˆ–æ–‡æœ¬ã€‚\n\né€šè¿‡ä½¿ç”¨ AI é©±åŠ¨çš„å›¾åƒè¯†åˆ« APIï¼Œæ‚¨å°†èƒ½å¤Ÿåœ¨ä¸æ·±å…¥å¤æ‚æœºå™¨å­¦ä¹ ç®—æ³•çš„æƒ…å†µä¸‹è¿›è¡Œè®¡ç®—æœºè§†è§‰å·¥ä½œã€‚\n\n**ä¸ºä»€ä¹ˆè¦æž„å»ºè¿™ä¸ªé¡¹ç›®ï¼Ÿ**å›¾åƒè¯†åˆ«æ˜¯ AI çš„ä¸€ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼Œè¿™ä¸ªé¡¹ç›®å°†ä¸ºæ‚¨æä¾›è®¡ç®—æœºè§†è§‰å’Œ Node.js æ–‡ä»¶å¤„ç†çš„å®žè·µç»éªŒã€‚\n\n**æ‚¨éœ€è¦å‡†å¤‡ï¼š**\n\n* **Node.js å’Œ Express** ç”¨äºŽåŽç«¯æœåŠ¡å™¨è®¾ç½®\n* **Google Cloud Vision** æˆ– [**Microsoft Azure Computer Vision API**](https://azure.microsoft.com/en-us/services/cognitive-services/computer-vision/) ç”¨äºŽå›¾åƒåˆ†æž\n* **Multer** ç”¨äºŽ [å¤„ç†æ–‡ä»¶ä¸Šä¼ ](https://www.npmjs.com/package/multer)\n\n```javascript\n// ç¤ºä¾‹ä»£ç \nconst express = require('express');\nconst multer = require('multer');\nconst app = express();\nconst upload = multer({ dest: 'uploads/' });\n\napp.post('/upload', upload.single('image'), (req, res) => {\n    res.send('æ–‡ä»¶ä¸Šä¼ æˆåŠŸ');\n});\n\napp.listen(3000, () => {\n    console.log('æœåŠ¡å™¨æ­£åœ¨è¿è¡Œåœ¨ http://localhost:3000');\n});\n```\n\n## 3\\. ç¤¾äº¤åª’ä½“å¸–å­æƒ…æ„Ÿåˆ†æžå·¥å…· ðŸ“Š\n\næƒ…æ„Ÿåˆ†æžå·¥å…·å¯ä»¥è®©æ‚¨åˆ†æžç¤¾äº¤åª’ä½“å¸–å­ã€è¯„è®ºæˆ–å®¢æˆ·åé¦ˆçš„è¯­è°ƒã€‚\n\nä½¿ç”¨ Node.js å’Œæƒ…æ„Ÿåˆ†æž APIï¼Œæ‚¨å¯ä»¥åˆ›å»ºä¸€ä¸ªå·¥å…·ï¼Œå°†æ–‡æœ¬è¯„å®šä¸ºç§¯æžã€æ¶ˆæžæˆ–ä¸­æ€§ã€‚\n\n**ä¸ºä»€ä¹ˆè¦æž„å»ºè¿™ä¸ªé¡¹ç›®ï¼Ÿ**è¿™ä¸ªé¡¹ç›®éžå¸¸é€‚åˆå­¦ä¹ å¦‚ä½•å¤„ç†æ–‡æœ¬æ•°æ®å’Œè§£è¯»æƒ…æ„Ÿï¼Œè¿™åœ¨ç¤¾äº¤åª’ä½“ç›‘æŽ§å’Œå®¢æˆ·åé¦ˆåˆ†æžä¸­è¢«å¹¿æ³›ä½¿ç”¨ã€‚\n\n**æ‚¨éœ€è¦çš„å·¥å…·ï¼š**\n\n* **Node.js å’Œ Express** ç”¨äºŽæœåŠ¡å™¨è®¾ç½®\n* [**Natural**](https://github.com/NaturalNode/natural) æˆ– **Aylien API** ç”¨äºŽæƒ…æ„Ÿåˆ†æž\n* **HTML/CSS** ç”¨äºŽåˆ›å»ºä¸€ä¸ªç®€å•çš„ [ç”¨æˆ·ç•Œé¢](https://developer.mozilla.org/en-US/docs/Learn/HTML)\n\n## 4\\. å¼€å‘ä¸€ä¸ªè¯­éŸ³å‘½ä»¤åº”ç”¨ç¨‹åºä¸Žè¯­éŸ³è¯†åˆ« ðŸŽ™ï¸\n\nåˆ›å»ºä¸€ä¸ªç†è§£åŸºæœ¬è¯­éŸ³å‘½ä»¤çš„åº”ç”¨ç¨‹åºï¼Œè¿™æ˜¯è¯­éŸ³æ¿€æ´»è®¾å¤‡æˆ–æ™ºèƒ½å®¶å±…ç³»ç»Ÿçš„åŸºæœ¬åŠŸèƒ½ã€‚\n\né€šè¿‡ç»“åˆ Node.js å’Œè¯­éŸ³è¯†åˆ« APIï¼Œæ‚¨å¯ä»¥åˆ›å»ºä¸€ä¸ªç®€å•çš„åº”ç”¨ç¨‹åºï¼Œè¯†åˆ«å‘½ä»¤å¹¶åšå‡ºå“åº”ã€‚\n\n**ä¸ºä»€ä¹ˆè¦æž„å»ºè¿™ä¸ªé¡¹ç›®ï¼Ÿ**è¯­éŸ³è¯†åˆ«å˜å¾—è¶Šæ¥è¶Šæ™®éï¼Œè¿™ä¸ªé¡¹ç›®è®©æ‚¨æœ‰æœºä¼šæŽ¢ç´¢è¯­éŸ³æŽ§åˆ¶çš„äº¤äº’ï¼Œè¿™åœ¨ç‰©è”ç½‘å’Œä»¥æ— éšœç¢ä¸ºé‡ç‚¹çš„åº”ç”¨ä¸­éžå¸¸æœ‰ä»·å€¼ã€‚\n\n**æ‚¨éœ€è¦çš„ï¼š**\n\n* **Node.js å’Œ Express** ç”¨äºŽ [åŽç«¯æœåŠ¡å™¨](https://expressjs.com/)\n* [**Web Speech API**](https://developer.mozilla.org/en-US/docs/Web/API/Web_Speech_API) ç”¨äºŽåŸºäºŽæµè§ˆå™¨çš„è¯­éŸ³è¯†åˆ«\n* **Socket.io** ç”¨äºŽå®žæ—¶å‘½ä»¤å“åº”\n\n## 5\\. ä½¿ç”¨ Node.js è®¾è®¡ä¸ªæ€§åŒ–ç”µå½±æŽ¨èç³»ç»Ÿ ðŸŽ¬\n\nä½¿ç”¨æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œæ‚¨å¯ä»¥æ ¹æ®ç”¨æˆ·åå¥½æž„å»ºä¸ªæ€§åŒ–çš„ç”µå½±æŽ¨èç³»ç»Ÿã€‚è¯¥é¡¹ç›®ä½¿ç”¨ååŒè¿‡æ»¤æ¥å»ºè®®ä¸Žç”¨æˆ·é«˜åº¦è¯„ä»·çš„ç”µå½±ç›¸ä¼¼çš„ç”µå½±ã€‚\n\n**ä¸ºä»€ä¹ˆè¦æž„å»ºè¿™ä¸ªé¡¹ç›®ï¼Ÿ**ç”µå½±æŽ¨èç³»ç»Ÿæ˜¯ååŒè¿‡æ»¤å’ŒæŽ¨èç®—æ³•çš„ç»ä½³å…¥é—¨ï¼Œè¿™äº›ç®—æ³•åœ¨æµåª’ä½“æœåŠ¡å’Œç”µå­å•†åŠ¡ä¸­å¹¿æ³›ä½¿ç”¨ã€‚\n\n**æ‚¨éœ€è¦å‡†å¤‡çš„å†…å®¹ï¼š**\n\n* **Node.js å’Œ Express** ç”¨äºŽæœåŠ¡å™¨è®¾ç½®\n* **ååŒè¿‡æ»¤ç®—æ³•**ï¼ˆä¾‹å¦‚ï¼Œ[ä½™å¼¦ç›¸ä¼¼åº¦](https://en.wikipedia.org/wiki/Cosine_similarity) æˆ– [KNN](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)ï¼‰ç”¨äºŽæŽ¨èé€»è¾‘\n* **TMDb API** ç”¨äºŽè®¿é—®å¤§åž‹ç”µå½±æ•°æ®åº“\n\n## ç»“è®º\n\nè¿™äº”ä¸ª AI é¡¹ç›®éžå¸¸é€‚åˆä»»ä½•å¸Œæœ›åœ¨ä¸€ä¸ªå‘¨æœ«äº²è‡ªæŽ¢ç´¢ AI çš„äººã€‚\n\nä»Žæž„å»ºèŠå¤©æœºå™¨äººåˆ°åˆ›å»ºç”µå½±æŽ¨èç³»ç»Ÿï¼Œæ‚¨å°†èŽ·å¾—åŸºç¡€çš„ AI æŠ€èƒ½ï¼ŒåŒæ—¶å¢žå¼ºæ‚¨çš„ Node.js ä¸“ä¸šçŸ¥è¯†ã€‚\n\næ¯ä¸ªé¡¹ç›®éƒ½é«˜åº¦å¯å®šåˆ¶ï¼Œå› æ­¤åœ¨æ‚¨è¿›å±•çš„è¿‡ç¨‹ä¸­ï¼Œæ¬¢è¿Žæ ¹æ®æ‚¨çš„ç‹¬ç‰¹æƒ³æ³•è¿›è¡Œè°ƒæ•´ã€‚\n\næœ‰å…³æ›´å¤šæ•™ç¨‹å’Œèµ„æºï¼Œè¯· [è®¢é˜…æˆ‘ä»¬çš„é¢‘é“](https://www.youtube.com/@codemarketi)ï¼Œå¹¶åŠæ—¶äº†è§£æœ€æ–°çš„ AI å’Œ Node.js é¡¹ç›®åˆ›æ„ã€‚\n\nç¥æ‚¨ç¼–ç æ„‰å¿«ï¼ ðŸš€\n\n## ç›¸å…³åšå®¢æ–‡ç« æŽ¨è\n\n* [https://readmedium.com/how\\-ai\\-tools\\-like\\-claude\\-vercel\\-and\\-more\\-are\\-transforming\\-software\\-development\\-b8d79b0de943](https://readmedium.com/how-ai-tools-like-claude-vercel-and-more-are-transforming-software-development-b8d79b0de943)\n* [https://readmedium.com/six\\-ai\\-powered\\-passive\\-income\\-ways\\-to\\-make\\-350\\-per\\-day\\-990d1e334d16](https://readmedium.com/six-ai-powered-passive-income-ways-to-make-350-per-day-990d1e334d16)\n* [https://readmedium.com/as\\-a\\-developer\\-here\\-are\\-5\\-websites\\-youll\\-love\\-e7518b24c85d](https://readmedium.com/as-a-developer-here-are-5-websites-youll-love-e7518b24c85d)\n* [https://readmedium.com/5\\-useful\\-chatgpt\\-tricks\\-thatll\\-blow\\-your\\-mind\\-in\\-2025\\-12e10a81f4d5](https://readmedium.com/5-useful-chatgpt-tricks-thatll-blow-your-mind-in-2025-12e10a81f4d5)\n\n## ç”¨ç®€å•è‹±è¯­è¡¨è¾¾ ðŸš€\n\n*æ„Ÿè°¢æ‚¨æˆä¸º [**In Plain English**](https://plainenglish.io/) ç¤¾åŒºçš„ä¸€éƒ¨åˆ†ï¼åœ¨æ‚¨ç¦»å¼€ä¹‹å‰ï¼š*\n\n* è¯·åŠ¡å¿… **ç‚¹èµž** å’Œ **å…³æ³¨** ä½œè€… ï¸ðŸ‘**ï¸ï¸**\n* å…³æ³¨æˆ‘ä»¬ï¼š [**X**](https://x.com/inPlainEngHQ) \\| [**LinkedIn**](https://www.linkedin.com/company/inplainenglish/) \\| [**YouTube**](https://www.youtube.com/channel/UCtipWUghju290NWcn8jhyAw) \\| [**Discord**](https://discord.gg/in-plain-english-709094664682340443) \\| [**Newsletter**](https://newsletter.plainenglish.io/) \\| [**Podcast**](https://open.spotify.com/show/7qxylRWKhvZwMz2WuEoua0)\n* [**åœ¨ Differ åˆ›å»ºä¸€ä¸ªå…è´¹çš„ AI é©±åŠ¨åšå®¢ã€‚**](https://differ.blog/)\n* æ›´å¤šå†…å®¹è¯·è®¿é—® [**PlainEnglish.io**](https://plainenglish.io/)\n\n"},{"lang":"zh","group":"blog","slug":"blog/50-generative-ai-use-cases-across-10-industries-96f621fefac2","frontmatter":{"title":"2024 å¹´ 10 ä¸ªè¡Œä¸šçš„ 50 å¤šä¸ªæœ€ä½³ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åº”ç”¨æ¡ˆä¾‹","meta_title":"2024 å¹´ 10 ä¸ªè¡Œä¸šçš„ 50 å¤šä¸ªæœ€ä½³ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åº”ç”¨æ¡ˆä¾‹","description":"ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGen AIï¼‰åœ¨2024å¹´è¢«è§†ä¸ºæŽ¨åŠ¨å„è¡Œä¸šè½¬åž‹çš„å…³é”®æŠ€æœ¯ï¼Œèƒ½å¤Ÿè‡ªåŠ¨åŒ–å¤æ‚ä»»åŠ¡ï¼Œæå‡ä¼ä¸šæ•ˆçŽ‡å’Œå®¢æˆ·ä½“éªŒã€‚æ–‡ç« æŽ¢è®¨äº†åå¤§è¡Œä¸šä¸­50ä¸ªGen AIåº”ç”¨æ¡ˆä¾‹ï¼Œæ¶µç›–è¥é”€ã€é”€å”®ã€äººåŠ›èµ„æºã€å®¢æˆ·æœåŠ¡ã€è´¢åŠ¡ã€ç”µå­å•†åŠ¡ã€æˆ¿åœ°äº§ã€æ•™è‚²ã€åˆ¶é€ å’Œé›¶å”®ç­‰é¢†åŸŸï¼Œå¼ºè°ƒå…¶åœ¨å†…å®¹ç”Ÿæˆã€ä¸ªæ€§åŒ–æœåŠ¡å’Œæ•°æ®åˆ†æžç­‰æ–¹é¢çš„ä¼˜åŠ¿ã€‚å®žæ–½Gen AIéœ€è¦è¯†åˆ«ç”¨ä¾‹ã€é€‰æ‹©å·¥å…·ã€æ•°æ®å‡†å¤‡ã€è¯•ç‚¹è¯„ä¼°å’ŒæŒç»­ä¼˜åŒ–ï¼Œä»¥ç¡®ä¿ä¼ä¸šåœ¨æ•°å­—åŒ–çŽ¯å¢ƒä¸­ä¿æŒç«žäº‰åŠ›ã€‚","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*PeQAto1_Mwovo11vsArGIA.jpeg","categories":["Generative AI","Technology","Marketing"],"author":"Rifx.Online","tags":["Generative","Automation","Personalization","Efficiency","Innovation"],"draft":false,"slug":"blog/50-generative-ai-use-cases-across-10-industries-96f621fefac2"},"content":"\n\n\n\n\nå±•æœ›2024å¹´ï¼Œç”Ÿæˆå¼AIï¼ˆGen AIï¼‰æ­£æ—¥ç›Šè¢«è§†ä¸ºæŽ¨åŠ¨å„è¡Œä¸šè½¬åž‹çš„å…³é”®æŠ€æœ¯ã€‚è¿™ä¸€æ¼”å˜æ ‡å¿—ç€ä¼ ç»ŸAIåœ¨é¢„æµ‹å’Œåˆ†æžè§’è‰²ä¸Šçš„è½¬å˜ï¼Œè½¬å‘Gen AIçš„åˆ›é€ æ€§èƒ½åŠ›ï¼Œä½¿ä¼ä¸šèƒ½å¤Ÿè‡ªåŠ¨åŒ–å¤æ‚ä»»åŠ¡ï¼Œä¿ƒè¿›åˆ›æ–°ï¼Œå¹¶æä¾›é«˜åº¦ä¸ªæ€§åŒ–çš„å®¢æˆ·ä½“éªŒã€‚æ ¹æ®æœ€è¿‘çš„åˆ†æžï¼ŒæŠ•èµ„äºŽAIçš„ä¼ä¸šæ•ˆçŽ‡æå‡å¯è¾¾30%ï¼Œè€Œå®žæ–½Gen AIçš„ä¼ä¸šåˆ™è¿›ä¸€æ­¥ä¼˜åŒ–äº†ä»–ä»¬çš„å·¥ä½œæµç¨‹å’Œæˆæžœã€‚\n\nåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ·±å…¥æŽ¢è®¨äº†åå¤§é¢†å…ˆè¡Œä¸šä¸­50å¤šä¸ªå…·æœ‰å½±å“åŠ›çš„Gen AIåº”ç”¨æ¡ˆä¾‹ï¼Œé‡ç‚¹å…³æ³¨å¸®åŠ©å¸Œæœ›å¼€å‘Gen AIä»¥æé«˜è¿è¥æ•ˆçŽ‡ã€å®¢æˆ·å‚ä¸Žåº¦å’Œç«žäº‰ä¼˜åŠ¿çš„ä¼ä¸šã€‚\n\n### ä»€ä¹ˆæ˜¯ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½ï¼Ÿ\n\n\n> ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é›†ï¼Œä¸“æ³¨äºŽé€šè¿‡å­¦ä¹ çŽ°æœ‰æ•°æ®ä¸­çš„æ¨¡å¼æ¥åˆ›å»ºæ–°å†…å®¹â€”â€”æ— è®ºæ˜¯æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘è¿˜æ˜¯è§†é¢‘ã€‚ä¸Žä¸»è¦åŸºäºŽæ•°æ®è¿›è¡Œé¢„æµ‹æˆ–åˆ†ç±»çš„ä¼ ç»Ÿäººå·¥æ™ºèƒ½ä¸åŒï¼Œç”Ÿæˆæ€§äººå·¥æ™ºèƒ½ç”ŸæˆåŽŸåˆ›è¾“å‡ºã€‚å®ƒä¾èµ–äºŽç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’ŒåŸºäºŽå˜æ¢å™¨çš„æž¶æž„ï¼Œå¦‚GPTï¼Œèƒ½å¤Ÿå®žçŽ°ä»Žåˆ›æ„å†…å®¹ç”Ÿäº§åˆ°å®žæ—¶å®¢æˆ·äº’åŠ¨çš„å¤šç§åŠŸèƒ½ã€‚\n\n### ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½å¯¹å•†ä¸šçš„å…³é”®ä¼˜åŠ¿\n\n1. **åˆ›é€ åŠ›ä¸Žåˆ›æ–°**ï¼šç”Ÿæˆæ€§äººå·¥æ™ºèƒ½ç”Ÿæˆè¶…è¶Šäººç±»åˆ›é€ åŠ›çš„ç‹¬ç‰¹å†…å®¹å’Œåˆ›æ„ï¼ŒæŽ¨åŠ¨è¥é”€ã€è®¾è®¡å’Œäº§å“å¼€å‘ç­‰æ–°æ¦‚å¿µé©±åŠ¨æˆåŠŸçš„è¡Œä¸šã€‚\n2. **æå‡æ•ˆçŽ‡**ï¼šè‡ªåŠ¨åŒ–é‡å¤ä»»åŠ¡ï¼Œå¦‚å†…å®¹ç”Ÿæˆå’Œåˆæ­¥å®¢æˆ·äº’åŠ¨ï¼Œä½¿ä¼ä¸šèƒ½å¤Ÿä¸“æ³¨äºŽæˆ˜ç•¥æ€§å’Œé«˜å±‚æ¬¡çš„å·¥ä½œã€‚\n3. **æˆæœ¬èŠ‚çº¦**ï¼šç”Ÿæˆæ€§äººå·¥æ™ºèƒ½å‡å°‘å¯¹äººå·¥åŠ³åŠ¨çš„ä¾èµ–ï¼Œå¤„ç†å†…å®¹åˆ›ä½œã€æ•°æ®åˆ†æžå’Œå®¢æˆ·æœåŠ¡ç­‰ä»»åŠ¡ï¼Œä»Žè€Œå®žçŽ°æ˜¾è‘—çš„èŠ‚çœã€‚\n4. **å¤§è§„æ¨¡ä¸ªæ€§åŒ–**ï¼šåˆ©ç”¨æ•°æ®ï¼Œç”Ÿæˆæ€§äººå·¥æ™ºèƒ½æä¾›å®šåˆ¶åŒ–ä½“éªŒï¼Œæé«˜å®¢æˆ·æ»¡æ„åº¦å’Œå¿ è¯šåº¦ã€‚\n5. **æ•°æ®é©±åŠ¨çš„æ´žå¯Ÿ**ï¼šç”Ÿæˆæ€§äººå·¥æ™ºèƒ½èƒ½å¤Ÿå¤„ç†å¤§è§„æ¨¡æ•°æ®é›†ï¼Œæå–å¯æ“ä½œçš„æ´žå¯Ÿï¼Œæ”¯æŒå®žæ—¶çš„æ˜Žæ™ºå†³ç­–ã€‚\n\n[**ä¸ºå•†ä¸šæž„å»ºç”Ÿæˆæ€§äººå·¥æ™ºèƒ½**](https://www.blockchainappfactory.com/generative-ai-solutions?utm_source=medium&utm_medium=blog&utm_campaign=elavarasan)å¯ä»¥æŽ¨åŠ¨åˆ›æ–°ã€ä¼˜åŒ–è¿è¥å’Œæå‡å®¢æˆ·ä½“éªŒã€‚åœ¨æ—¥ç›Šæ•°å­—åŒ–çš„ä¸–ç•Œä¸­ï¼Œè¿™æ˜¯ä¿æŒç«žäº‰åŠ›çš„å¼ºå¤§å·¥å…·ã€‚\n\n## 2024å¹´åå¤§è¡Œä¸šçš„ç”Ÿæˆå¼AIåº”ç”¨æ¡ˆä¾‹\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*TG-tfTwA58FNjHv5so8fXg.png)\n\n### 1\\. è¥é”€\n\nåœ¨2024å¹´ï¼Œè¥é”€äººå‘˜åˆ©ç”¨Gen AIä¼˜åŒ–æ´»åŠ¨ã€ä¸ªæ€§åŒ–æŽ¨å¹¿å’Œå¤§è§„æ¨¡ç”Ÿæˆå†…å®¹ã€‚é€šè¿‡è‡ªåŠ¨åŒ–é‡å¤æ€§ä»»åŠ¡ï¼Œè¥é”€å›¢é˜Ÿå¯ä»¥ä¸“æ³¨äºŽæˆ˜ç•¥å’Œåˆ›æ„ã€‚\n\n* **å†…å®¹ä¼˜åŒ–**ï¼šGen AIåˆ†æžæœç´¢è¶‹åŠ¿å’Œå—ä¼—åå¥½ï¼ŒæŽ¨èèƒ½å¤Ÿæå‡å‚ä¸Žåº¦å’Œå¯è§æ€§çš„ä¸»é¢˜ã€å…³é”®è¯å’Œæ ¼å¼ã€‚\n* **å¤§è§„æ¨¡å†…å®¹åˆ›ä½œ**ï¼šè¥é”€æœºæž„ä½¿ç”¨Gen AIå¿«é€Ÿç”Ÿæˆå¤šæ ·åŒ–çš„å†…å®¹ç±»åž‹â€”â€”ä»Žåšå®¢æ–‡ç« åˆ°ç¤¾äº¤åª’ä½“æ›´æ–°â€”â€”ç¡®ä¿åŠæ—¶ã€é«˜è´¨é‡çš„è¾“å‡ºã€‚\n* **è‡ªåŠ¨åŒ–ç¤¾äº¤åª’ä½“ç®¡ç†**ï¼šå°åž‹ä¼ä¸šåˆ©ç”¨Gen AIç®¡ç†å¸–å­ã€å›žåº”å®¢æˆ·è¯¢é—®ï¼Œå¹¶åˆ†æžå‚ä¸Žåº¦ï¼Œè€Œæ— éœ€åºžå¤§çš„å›¢é˜Ÿã€‚\n* **ä¸ªæ€§åŒ–æ´»åŠ¨**ï¼šAIå¯¹å—ä¼—è¿›è¡Œç»†åˆ†ï¼Œåˆ¶ä½œä¸Žä¸ªåˆ«å®¢æˆ·åå¥½ç›¸å¥‘åˆçš„å®šåˆ¶ç”µå­é‚®ä»¶å’Œå¹¿å‘Šå†…å®¹ã€‚\n* **A/Bæµ‹è¯•å’Œæ´»åŠ¨ä¼˜åŒ–**ï¼šAIè‡ªåŠ¨æµ‹è¯•å„ç§å†…å®¹ç‰ˆæœ¬ï¼Œåˆ†æžå®žæ—¶ç»“æžœï¼Œä»¥å¾®è°ƒæ´»åŠ¨ä»¥è¾¾åˆ°æœ€å¤§æ•ˆæžœã€‚\n\n### 2\\. é”€å”®\n\né”€å”®å›¢é˜Ÿé‡‡ç”¨ Gen AI æ¥ç®€åŒ–æµç¨‹ã€ç®¡ç†æ½œåœ¨å®¢æˆ·å¹¶è‡ªåŠ¨ç”Ÿæˆææ¡ˆï¼Œä»Žè€Œæé«˜è½¬åŒ–çŽ‡å’Œæ•ˆçŽ‡ã€‚\n\n* **æ½œåœ¨å®¢æˆ·è¯„åˆ†å’Œä¼˜å…ˆçº§æŽ’åº**ï¼šAI æ ¹æ®è½¬åŒ–å¯èƒ½æ€§ä¸ºæ½œåœ¨å®¢æˆ·åˆ†é…åˆ†æ•°ï¼Œå®žçŽ°æœ‰é’ˆå¯¹æ€§å’Œé«˜æ•ˆçš„å¤–å±•ã€‚\n* **è‡ªåŠ¨åŒ–ææ¡ˆç”Ÿæˆ**ï¼šGen AI æ ¹æ®å®¢æˆ·éœ€æ±‚è‰æ‹Ÿå®šåˆ¶ææ¡ˆï¼Œå‡å°‘å‡†å¤‡æ—¶é—´å¹¶æå‡ææ¡ˆè´¨é‡ã€‚\n* **è™šæ‹Ÿé”€å”®åŠ©æ‰‹**ï¼šAI åœ¨å®¢æˆ·ä¼šè®®ä¸­ååŠ©é”€å”®ä»£è¡¨è¿›è¡Œæ—¥ç¨‹å®‰æŽ’ã€è·Ÿè¿›å’Œå®žæ—¶æ•°æ®æ´žå¯Ÿã€‚\n* **é¢„æµ‹æ€§é”€å”®åˆ†æž**ï¼šé€šè¿‡åˆ†æžåŽ†å²æ•°æ®ï¼ŒGen AI é¢„æµ‹æœªæ¥è¶‹åŠ¿å’Œå®¢æˆ·è¡Œä¸ºï¼Œæ”¯æŒæ•°æ®é©±åŠ¨çš„é”€å”®ç­–ç•¥ã€‚\n* **ç”¨äºŽåˆæ­¥æŽ¥è§¦çš„èŠå¤©æœºå™¨äºº**ï¼šAI èŠå¤©æœºå™¨äººä¸Žæ½œåœ¨å®¢æˆ·äº’åŠ¨ï¼Œå›žç­”é—®é¢˜å¹¶å¯¹æ½œåœ¨å®¢æˆ·è¿›è¡Œèµ„æ ¼å®¡æ ¸ï¼Œç„¶åŽå°†å…¶è½¬äº¤ç»™é”€å”®äººå‘˜ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*7yN87soNCoYQe6Qk2oIenw.png)\n\n### 3\\. äººåŠ›èµ„æº\n\nç”Ÿæˆæ€§äººå·¥æ™ºèƒ½å¸®åŠ©äººåŠ›èµ„æºå›¢é˜Ÿè‡ªåŠ¨åŒ–ä»»åŠ¡ï¼Œä»Žæ‹›è˜å’Œå…¥èŒåˆ°å‘˜å·¥å‚ä¸Žåˆ†æžï¼Œæœ‰åŠ©äºŽæ”¹å–„æ‹›è˜å’Œç•™ä»»ã€‚\n\n* **è‡ªåŠ¨åŒ–ç®€åŽ†ç­›é€‰**ï¼šç”Ÿæˆæ€§äººå·¥æ™ºèƒ½æ ¹æ®æŒ‡å®šæ ‡å‡†å®¡æŸ¥ç®€åŽ†ï¼Œè¯†åˆ«æœ€ä½³å€™é€‰äººï¼Œä»Žè€ŒèŠ‚çœåˆæ­¥ç­›é€‰çš„æ—¶é—´ã€‚\n* **å‘˜å·¥å…¥èŒååŠ©**ï¼šäººå·¥æ™ºèƒ½é©±åŠ¨çš„ä»£ç†å¸®åŠ©æ–°å‘˜å·¥å®Œæˆå…¥èŒï¼Œç¡®ä¿é¡ºåˆ©æ•´åˆå¹¶å‡å°‘äººå·¥äººåŠ›èµ„æºä»»åŠ¡ã€‚\n* **åé¦ˆæƒ…æ„Ÿåˆ†æž**ï¼šäººå·¥æ™ºèƒ½åˆ†æžå‘˜å·¥åé¦ˆï¼Œä»¥è¯†åˆ«è¶‹åŠ¿å¹¶æŽ¨èæ”¹è¿›æŽªæ–½ï¼Œä»Žè€Œæå‡å·¥ä½œæ»¡æ„åº¦ã€‚\n* **ä¸ªæ€§åŒ–å­¦ä¹ å’Œå‘å±•è®¡åˆ’**ï¼šäººå·¥æ™ºèƒ½è¯„ä¼°æŠ€èƒ½å¹¶å»ºè®®é’ˆå¯¹æ€§çš„åŸ¹è®­ï¼Œä»¥ä¿ƒè¿›å‘˜å·¥æˆé•¿ã€‚\n* **ç•™ä»»çš„åŠ³åŠ¨åŠ›åˆ†æž**ï¼šç”Ÿæˆæ€§äººå·¥æ™ºèƒ½é¢„æµ‹ç¦»èŒçŽ‡å¹¶è¯†åˆ«å½±å“å› ç´ ï¼Œä½¿äººåŠ›èµ„æºèƒ½å¤Ÿä¸»åŠ¨åº”å¯¹ç•™ä»»é—®é¢˜ã€‚\n\n### 4\\. å®¢æˆ·æœåŠ¡\n\nGen AI é€šè¿‡å¤„ç†å’¨è¯¢ã€æä¾›å®žæ—¶å¸®åŠ©å’Œè‡ªåŠ¨åŒ–ç¥¨åŠ¡è§£å†³æ¥å¢žå¼ºå®¢æˆ·æ”¯æŒã€‚\n\n* **AI é©±åŠ¨çš„èŠå¤©æœºå™¨äºº**ï¼šGen AI èŠå¤©æœºå™¨äººå›žç­”å¸¸è§„é—®é¢˜ï¼Œå‡å°‘å“åº”æ—¶é—´ï¼Œæé«˜å®¢æˆ·æ»¡æ„åº¦ã€‚\n* **åé¦ˆæƒ…æ„Ÿåˆ†æž**ï¼šAI åˆ†æžè¯„è®ºå’Œåé¦ˆä»¥è¯„ä¼°æƒ…æ„Ÿï¼Œå¸®åŠ©ä¼ä¸šäº†è§£å®¢æˆ·æ»¡æ„åº¦ã€‚\n* **è‡ªåŠ¨åŒ–ç¥¨åŠ¡ä¼˜å…ˆçº§æŽ’åº**ï¼šGen AI å¯¹æ”¯æŒç¥¨åŠ¡è¿›è¡ŒæŽ’åºï¼Œä¼˜å…ˆå¤„ç†ç´§æ€¥é—®é¢˜ä»¥åŠ å¿«è§£å†³é€Ÿåº¦ã€‚\n* **çŸ¥è¯†åº“ä¼˜åŒ–**ï¼šAI æ ¹æ®å¸¸è§å’¨è¯¢ä¸æ–­æ›´æ–°çŸ¥è¯†åº“ï¼Œæé«˜è‡ªåŠ©æœåŠ¡èƒ½åŠ›ã€‚\n* **è™šæ‹Ÿå®¢æˆ·åŠ©ç†**ï¼šAI æ ¹æ®å®¢æˆ·åå¥½å’ŒåŽ†å²æä¾›é‡èº«å®šåˆ¶çš„æŽ¨èï¼Œå¢žå¼ºæ”¯æŒã€‚\n\n### 5\\. è´¢åŠ¡ä¸Žä¼šè®¡\n\nGen AI æ­£åœ¨é€šè¿‡è‡ªåŠ¨åŒ–å‘ç¥¨å¤„ç†ã€è´¢åŠ¡é¢„æµ‹å’Œç¡®ä¿ç¨ŽåŠ¡åˆè§„æ¥é©æ–°è´¢åŠ¡é¢†åŸŸã€‚\n\n* **è‡ªåŠ¨åŒ–å‘ç¥¨å¤„ç†**ï¼šAI å¤„ç†å‘ç¥¨ï¼Œå‡å°‘é”™è¯¯å¹¶åŠ å¿«ä»˜æ¬¾å‘¨æœŸã€‚\n* **è´¢åŠ¡é¢„æµ‹**ï¼šé€šè¿‡åˆ†æžåŽ†å²æ•°æ®ï¼ŒGen AI æé«˜äº†é¢„ç®—å’Œè´¢åŠ¡è§„åˆ’çš„å‡†ç¡®æ€§ã€‚\n* **è´¹ç”¨ç®¡ç†**ï¼šAI é€šè¿‡åˆ†ç±»å’Œåˆ†æžè´¹ç”¨æ¥è¯†åˆ«èŠ‚çœæˆæœ¬çš„æœºä¼šã€‚\n* **ç¨ŽåŠ¡åˆè§„**ï¼šGen AI å‡†å¤‡ç¨ŽåŠ¡æ–‡ä»¶ï¼Œæœ€å°åŒ–é”™è¯¯å¹¶ç¡®ä¿åˆè§„ã€‚\n* **å®žæ—¶è´¢åŠ¡æŠ¥å‘Š**ï¼šAI ç”Ÿæˆè´¢åŠ¡æ´žå¯Ÿï¼Œä¿ƒè¿›çµæ´»çš„æ•°æ®é©±åŠ¨å†³ç­–ã€‚\n\n### 6\\. ç”µå­å•†åŠ¡\n\nåœ¨ç”µå­å•†åŠ¡ä¸­ï¼ŒGen AI æå‡äº†å®¢æˆ·ä½“éªŒï¼Œä¼˜åŒ–äº†åº“å­˜ï¼Œå¹¶æ£€æµ‹æ¬ºè¯ˆï¼Œæ”¯æŒä¸šåŠ¡å¢žé•¿ã€‚\n\n* **ä¸ªæ€§åŒ–äº§å“æŽ¨è**ï¼šAI åˆ†æžè¡Œä¸ºä»¥æä¾›ä¸ªæ€§åŒ–çš„äº§å“å»ºè®®ï¼Œä»Žè€Œæé«˜è½¬åŒ–çŽ‡ã€‚\n* **åŠ¨æ€å®šä»·ç­–ç•¥**ï¼šAI æ ¹æ®å¸‚åœºè¶‹åŠ¿ã€éœ€æ±‚å’Œç«žäº‰å¯¹æ‰‹å®šä»·å®žæ—¶è°ƒæ•´ä»·æ ¼ã€‚\n* **åº“å­˜é¢„æµ‹**ï¼šAI é¢„æµ‹åº“å­˜éœ€æ±‚ï¼Œé˜²æ­¢è¿‡å‰©æˆ–ç¼ºè´§ï¼Œæé«˜ä¾›åº”é“¾æ•ˆçŽ‡ã€‚\n* **å®¢æˆ·æ—…ç¨‹æ˜ å°„**ï¼šAI åˆ›å»ºè¯¦ç»†çš„æ—…ç¨‹åœ°å›¾ï¼ŒæŒ‡å¯¼ç”µå­å•†åŠ¡ç­–ç•¥ã€‚\n* **æ¬ºè¯ˆæ£€æµ‹ä¸Žé¢„é˜²**ï¼šGen AI æ£€æµ‹å¼‚å¸¸äº¤æ˜“æ¨¡å¼ï¼Œä¿æŠ¤ä¼ä¸šå’Œå®¢æˆ·å…å—æ¬ºè¯ˆã€‚\n\n### 7\\. æˆ¿åœ°äº§\n\næˆ¿åœ°äº§å…¬å¸ä½¿ç”¨ Gen AI è¿›è¡Œç‰©ä¸šè¯„ä¼°ã€è™šæ‹Ÿå¯¼è§ˆå’Œå®¢æˆ·å…³ç³»ç®¡ç†ä¼˜åŒ–ï¼Œä»Žè€Œç®€åŒ–æ“ä½œå¹¶æ”¹å–„å†³ç­–ã€‚\n\n* **è‡ªåŠ¨åŒ–ç‰©ä¸šè¯„ä¼°**ï¼šAI æ¨¡åž‹åˆ†æžå¸‚åœºæ•°æ®ï¼Œæä¾›å‡†ç¡®çš„è¯„ä¼°ä»¥ä¾¿æ›´å¥½çš„å†³ç­–ã€‚\n* **è™šæ‹Ÿç‰©ä¸šå¯¼è§ˆ**ï¼šAI åˆ›å»ºè™šæ‹Ÿå¯¼è§ˆï¼Œä½¿è¿œç¨‹æˆ–å›½é™…ä¹°å®¶èƒ½å¤Ÿè®¿é—®æˆ¿æºã€‚\n* **é¢„æµ‹å¸‚åœºåˆ†æž**ï¼šAI é¢„æµ‹å¸‚åœºè¶‹åŠ¿ï¼Œä¸ºæŠ•èµ„å†³ç­–æä¾›æŒ‡å¯¼ã€‚\n* **ç§Ÿèµç®¡ç†è‡ªåŠ¨åŒ–**ï¼šAI ç®¡ç†ç»­çº¦å’Œåˆè§„ï¼Œå‡å°‘è¡Œæ”¿ä»»åŠ¡ã€‚\n* **å¢žå¼ºå®¢æˆ·å…³ç³»ç®¡ç†**ï¼šAI æä¾›å®¢æˆ·åå¥½çš„æ´žå¯Ÿï¼Œæ”¯æŒé‡èº«å®šåˆ¶çš„äº’åŠ¨ã€‚\n\n### 8\\. æ•™è‚²\n\nåœ¨æ•™è‚²é¢†åŸŸï¼Œç”Ÿæˆå¼äººå·¥æ™ºèƒ½å®šåˆ¶å­¦ä¹ ä½“éªŒï¼Œæ”¯æŒè¯¾ç¨‹å¼€å‘ï¼Œå¹¶ååŠ©è¯„åˆ†ï¼Œæé«˜å­¦ä¹ æˆæžœã€‚\n\n* **ä¸ªæ€§åŒ–å­¦ä¹ è·¯å¾„**ï¼šäººå·¥æ™ºèƒ½æ ¹æ®å­¦ç”Ÿçš„ä¼˜åŠ¿å’ŒåŠ£åŠ¿åˆ›å»ºå®šåˆ¶å­¦ä¹ è·¯å¾„ã€‚\n* **è‡ªåŠ¨è¯„åˆ†**ï¼šäººå·¥æ™ºèƒ½å¸®åŠ©æ•™è‚²å·¥ä½œè€…è¿›è¡Œè¯„åˆ†ï¼Œä½¿å­¦ç”Ÿèƒ½å¤Ÿæ›´å¿«åœ°èŽ·å¾—åé¦ˆã€‚\n* **è¯¾ç¨‹æ´žå¯Ÿ**ï¼šäººå·¥æ™ºèƒ½åˆ†æžç»©æ•ˆæ•°æ®ä»¥æŒ‡å¯¼è¯¾ç¨‹è°ƒæ•´ã€‚\n* **è™šæ‹Ÿè¾…å¯¼**ï¼šäººå·¥æ™ºèƒ½è¾…å¯¼å‘˜æä¾›èµ„æºå¹¶å›žç­”å­¦ç”Ÿè¯¾å¤–é—®é¢˜ã€‚\n* **å­¦ç”Ÿå‚ä¸Žåˆ†æž**ï¼šäººå·¥æ™ºèƒ½æ ‡è®°æœ‰é£Žé™©çš„å­¦ç”Ÿï¼Œæ”¯æŒåŠæ—¶å¹²é¢„ã€‚\n\n### 9\\. åˆ¶é€ \n\nåˆ¶é€ ä¸šå—ç›ŠäºŽç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„é¢„æµ‹æ€§ç»´æŠ¤ã€è´¨é‡æŽ§åˆ¶å’ŒåŠ³åŠ¨åŠ›è°ƒåº¦ï¼Œæé«˜äº†æ•ˆçŽ‡ï¼Œå‡å°‘äº†åœæœºæ—¶é—´ã€‚\n\n* **é¢„æµ‹æ€§ç»´æŠ¤**ï¼šäººå·¥æ™ºèƒ½é¢„æµ‹è®¾å¤‡é—®é¢˜ï¼Œå‡å°‘è®¡åˆ’å¤–ç»´æŠ¤å’Œåœæœºæ—¶é—´ã€‚\n* **ä¾›åº”é“¾ä¼˜åŒ–**ï¼šäººå·¥æ™ºèƒ½é¢„æµ‹åº“å­˜éœ€æ±‚ï¼Œç®€åŒ–æ“ä½œå¹¶é™ä½Žæˆæœ¬ã€‚\n* **è‡ªåŠ¨åŒ–è´¨é‡æŽ§åˆ¶**ï¼šç”Ÿæˆå¼äººå·¥æ™ºèƒ½å®žæ—¶æ£€æµ‹ç¼ºé™·ï¼Œæé«˜äº§å“è´¨é‡ã€‚\n* **åŠ³åŠ¨åŠ›è°ƒåº¦**ï¼šäººå·¥æ™ºèƒ½æ ¹æ®éœ€æ±‚ä¼˜åŒ–ç­æ¬¡ï¼Œç¡®ä¿é«˜æ•ˆçš„äººå‘˜é…ç½®ã€‚\n* **äº§å“è®¾è®¡è¾…åŠ©**ï¼šäººå·¥æ™ºèƒ½é€šè¿‡åˆ†æžå¸‚åœºå’Œæ¶ˆè´¹è€…è¶‹åŠ¿æ¥æŒ‡å¯¼è®¾è®¡å†³ç­–ã€‚\n\n### 10\\. é›¶å”®\n\né›¶å”®å•†åˆ©ç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½äº†è§£å®¢æˆ·è¡Œä¸ºï¼Œä¸ªæ€§åŒ–åº—å†…ä½“éªŒï¼Œå¹¶ä¼˜åŒ–åº“å­˜ï¼Œä»Žè€Œæå‡æ»¡æ„åº¦å’Œé”€å”®é¢ã€‚\n\n* **å®¢æˆ·è¡Œä¸ºåˆ†æž**ï¼šäººå·¥æ™ºèƒ½åˆ†æžè´­ç‰©ä¹ æƒ¯ï¼ŒæŒ‡å¯¼å•†å“é™ˆåˆ—å’Œè¥é”€å·¥ä½œã€‚\n* **å¢žå¼ºçš„åº—å†…ä½“éªŒ**ï¼šåŸºäºŽäººå·¥æ™ºèƒ½çš„åº”ç”¨ç¨‹åºæä¾›ä¸ªæ€§åŒ–çš„åº—å†…æŽ¨èå’Œå¸®åŠ©ã€‚\n* **åº“å­˜é¢„æµ‹**ï¼šäººå·¥æ™ºèƒ½é¢„æµ‹åº“å­˜éœ€æ±‚ï¼Œé˜²æ­¢è¿‡å‰©æˆ–ç¼ºè´§ã€‚\n* **å¿ è¯šåº¦è®¡åˆ’ä¼˜åŒ–**ï¼šäººå·¥æ™ºèƒ½å®šåˆ¶å¿ è¯šåº¦è®¡åˆ’ï¼Œæé«˜å‚ä¸Žåº¦å’Œå®¢æˆ·ç•™å­˜çŽ‡ã€‚\n* **è‡ªåŠ¨ç»“è´¦**ï¼šåŸºäºŽäººå·¥æ™ºèƒ½çš„ç»“è´¦ç³»ç»Ÿæå‡ä¾¿åˆ©æ€§ï¼Œå‡å°‘ç­‰å¾…æ—¶é—´ã€‚\n\n### ä¸ºæ‚¨çš„ä¸šåŠ¡å®žæ–½ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½\n\n1. **è¯†åˆ«ç”¨ä¾‹**ï¼šå®šä¹‰ä¸Žä¸šåŠ¡ç›®æ ‡ä¸€è‡´çš„å…·ä½“ç”¨ä¾‹ï¼Œä»Žå†…å®¹ç”Ÿæˆåˆ°ä¸ªæ€§åŒ–è¥é”€ã€‚\n2. **é€‰æ‹©åˆé€‚çš„å·¥å…·**ï¼šè¯„ä¼°ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½å·¥å…·çš„èƒ½åŠ›ã€é›†æˆæ€§å’Œå¯æ‰©å±•æ€§ï¼Œä»¥ç¡®ä¿ä¸Žä¸šåŠ¡éœ€æ±‚ä¸€è‡´ã€‚\n3. **æ•°æ®å‡†å¤‡å’Œæ¨¡åž‹è®­ç»ƒ**ï¼šæ¸…ç†ã€ç»„ç»‡å’Œå‡†å¤‡æ•°æ®ï¼Œå› ä¸ºé«˜è´¨é‡çš„æ•°æ®å¯¹æ¨¡åž‹çš„å‡†ç¡®æ€§è‡³å…³é‡è¦ã€‚\n4. **è¯•ç‚¹å’Œè¯„ä¼°**ï¼šè¿è¡Œè¯•ç‚¹é¡¹ç›®ä»¥æµ‹è¯•ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½çš„å½±å“ï¼Œæ”¶é›†è§è§£ï¼Œå¹¶æ ¹æ®éœ€è¦ä¼˜åŒ–è§£å†³æ–¹æ¡ˆã€‚\n5. **æŒç»­ä¼˜åŒ–**ï¼šç›‘æŽ§è¾“å‡ºï¼Œæ”¶é›†åé¦ˆï¼Œå¹¶è¿›è¡Œå¿…è¦çš„è°ƒæ•´ä»¥æå‡æ¨¡åž‹æ€§èƒ½ã€‚\n6. **è€ƒè™‘ä¼¦ç†å½±å“**ï¼šå®žæ–½æ”¿ç­–ä»¥è§£å†³ä¼¦ç†é—®é¢˜ï¼ŒåŒ…æ‹¬æ•°æ®éšç§ã€åè§å’Œè´Ÿè´£ä»»çš„äººå·¥æ™ºèƒ½ä½¿ç”¨ã€‚\n\n### ç»“è®º\n\nç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ­£åœ¨2024å¹´æ”¹å˜å„è¡Œå„ä¸šï¼Œä»Žå¢žå¼ºå®¢æˆ·å‚ä¸Žåº¦åˆ°æŽ¨åŠ¨é‡‘èžã€è¥é”€å’Œåˆ¶é€ ä¸šçš„æ•ˆçŽ‡ã€‚å¯¹äºŽå‡†å¤‡é‡‡ç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„ä¼ä¸šæ¥è¯´ï¼Œæˆ˜ç•¥æ€§çš„æ–¹æ³•æ˜¯å®žçŽ°å…¶å¥½å¤„çš„å…³é”®ï¼Œä»Žè‡ªåŠ¨åŒ–æµç¨‹åˆ°ä¸ªæ€§åŒ–å®¢æˆ·ä½“éªŒã€‚é€šè¿‡ç†è§£ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„æ½œåŠ›ï¼Œä¼ä¸šå¯ä»¥åˆ©ç”¨å…¶èƒ½åŠ›ï¼Œåœ¨å½“ä»Šå¿«é€Ÿå‘å±•çš„æ•°å­—çŽ¯å¢ƒä¸­ä¿æŒç«žäº‰åŠ›ã€åˆ›æ–°æ€§å’Œé«˜æ•ˆæ€§ã€‚\n\n## ç”¨ç®€å•è‹±è¯­è¡¨è¾¾ ðŸš€\n\n*æ„Ÿè°¢æ‚¨æˆä¸º [**In Plain English**](https://plainenglish.io/) ç¤¾åŒºçš„ä¸€éƒ¨åˆ†ï¼åœ¨æ‚¨ç¦»å¼€ä¹‹å‰ï¼š*\n\n* è¯·åŠ¡å¿… **ç‚¹èµž** å’Œ **å…³æ³¨** ä½œè€… ï¸ðŸ‘**ï¸ï¸**\n* å…³æ³¨æˆ‘ä»¬ï¼š [**X**](https://x.com/inPlainEngHQ) \\| [**LinkedIn**](https://www.linkedin.com/company/inplainenglish/) \\| [**YouTube**](https://www.youtube.com/channel/UCtipWUghju290NWcn8jhyAw) \\| [**Discord**](https://discord.gg/in-plain-english-709094664682340443) \\| [**Newsletter**](https://newsletter.plainenglish.io/) \\| [**Podcast**](https://open.spotify.com/show/7qxylRWKhvZwMz2WuEoua0)\n* [**åœ¨ Differ ä¸Šåˆ›å»ºä¸€ä¸ªå…è´¹çš„ AI é©±åŠ¨åšå®¢ã€‚**](https://differ.blog/)\n* æ›´å¤šå†…å®¹è¯·è®¿é—® [**PlainEnglish.io**](https://plainenglish.io/)\n\n"},{"lang":"zh","group":"blog","slug":"blog/a-month-with-cursor-and-claude-dev-my-thoughts-5c41ae0d4467","frontmatter":{"title":"ä¸Ž Cursor å’Œ Claude-Dev å…±åº¦çš„ä¸€ä¸ªæœˆï¼šæˆ‘çš„æƒ³æ³•","meta_title":"ä¸Ž Cursor å’Œ Claude-Dev å…±åº¦çš„ä¸€ä¸ªæœˆï¼šæˆ‘çš„æƒ³æ³•","description":"æˆ‘æœ€è¿‘ä¸€ç›´åœ¨ä½¿ç”¨ä¸¤ä¸ªæ–°å·¥å…· - Cursor å’Œ Claude-Dev - è¿™ä¸¤ä¸ªå·¥å…·éƒ½å¼•èµ·äº†å¼€å‘äººå‘˜çš„å¹¿æ³›å…³æ³¨â€¦â€¦","date":"2024-11-04T12:32:52.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*i28vK12LJ6XTpSwrKiamwA.png","categories":["Programming","Technology","Generative AI"],"author":"Rifx.Online","tags":["Cursor","Claude-Dev","Cline","autocomplete","debugging"],"draft":false,"slug":"blog/a-month-with-cursor-and-claude-dev-my-thoughts-5c41ae0d4467"},"content":"\n\n\næˆ‘æœ€è¿‘åœ¨ä½¿ç”¨ä¸¤ä¸ªæ–°å·¥å…·â€”â€” **Cursor** å’Œ **Claude\\-Dev** â€”â€”è¿™ä¸¤ä¸ªå·¥å…·åœ¨å¼€å‘è€…ç¤¾åŒºä¸­éƒ½å¼•èµ·äº†ç›¸å½“å¤šçš„å…³æ³¨ã€‚å®ƒä»¬éƒ½æ˜¯é€šè¿‡ AI é©±åŠ¨çš„åŠ©æ‰‹æ¥æé«˜ç¼–ç çš„é€Ÿåº¦å’Œç›´è§‚æ€§ï¼Œä½†å®ƒä»¬é‡‡å–äº†ä¸åŒçš„æ–¹æ³•ï¼Œå¹¶å„è‡ªæœ‰è‡ªå·±çš„ä¼˜ç¼ºç‚¹ã€‚åœ¨ä½¿ç”¨è¿™ä¸¤è€…å¤§çº¦ä¸€ä¸ªæœˆåŽï¼Œæˆ‘è§‰å¾—æ˜¯æ—¶å€™åä¸‹æ¥åæ€ä¸€ä¸‹å®ƒä»¬çš„ä¼˜åŠ¿å’Œä»éœ€æ”¹è¿›çš„åœ°æ–¹ã€‚\n\nè®©æˆ‘ä»¬ä»Ž Cursor å¼€å§‹ã€‚\n\n## Cursor: ç†Ÿæ‚‰ä½†æ›´å¿«é€Ÿ\n\nCursor æ˜¯ VSCode çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå¦‚æžœä½ å’Œæˆ‘ä¸€æ ·å·²ç»æ˜¯ VSCode ç”¨æˆ·ï¼Œé‚£ä¹ˆä½¿ç”¨èµ·æ¥å°±éžå¸¸ç®€å•ã€‚æˆ‘ä¸éœ€è¦ä»Žå¤´å¼€å§‹é‡å»ºæˆ‘çš„çŽ¯å¢ƒï¼Œä¹Ÿä¸éœ€è¦å¤„ç†è®¾ç½®é”®ç»‘å®šã€‚æ‰€æœ‰åœ¨ VSCode ä¸­æœ‰æ•ˆçš„åŠŸèƒ½åœ¨ Cursor ä¸­å¼€ç®±å³ç”¨â€”â€”æˆ‘çš„æ‰©å±•ã€è®¾ç½®å’Œé”®æ˜ å°„éƒ½æ¯«æ— é—®é¢˜åœ°è½¬ç§»è¿‡æ¥ã€‚è¿‡æ¸¡å‡ ä¹Žæ˜¯æ— ç¼çš„ï¼Œå”¯ä¸€çš„ä¸€ä¸ªå…³é”®åŒºåˆ«æ˜¯ï¼šAI è‡ªåŠ¨è¡¥å…¨é€Ÿåº¦æ›´å¿«ã€‚å®žé™…ä¸Šï¼Œ**æ ¹æ®æˆ‘çš„ç»éªŒ**ï¼Œå®ƒçš„é€Ÿåº¦å¤§çº¦æ˜¯ GitHub Copilot çš„ 10 å€ã€‚\n\nçŽ°åœ¨ï¼Œâ€œ10 å€æ›´å¿«â€å¹¶ä¸æ˜¯æˆ‘ä»ŽåŸºå‡†æµ‹è¯•ä¸­å¾—å‡ºçš„æ•°å­—â€”â€”è¿™åªæ˜¯æˆ‘ä½¿ç”¨ä¸€æ®µæ—¶é—´åŽçš„æ„Ÿè§‰ã€‚å½“ä½ åœ¨ç¼–å†™ä»£ç æ—¶ï¼ŒCursor é¢„æµ‹ä½ çš„ä¸‹ä¸€æ­¥åŠ¨ä½œæ—¶ï¼Œå¹¶ä¸ä¼šè®©ä½ æ„Ÿè§‰ AI åœ¨æ»žåŽæˆ–è¿½èµ¶ã€‚ç›¸åï¼Œå®ƒä¸Žä½ åŒæ­¥ï¼Œè¿™æœ‰åŠ©äºŽä¿æŒä½ çš„å·¥ä½œçŠ¶æ€ã€‚æˆ‘æƒŠè®¶äºŽå½“æˆ‘ä¸å†ç­‰å¾… Copilot èµ¶ä¸Šæˆ–æŒ‰ä¸‰æ¬¡ tab ä»…ä»…ä¸ºäº†å¾—åˆ°æˆ‘æƒ³è¦çš„å»ºè®®æ—¶ï¼Œæˆ‘çš„ç”Ÿäº§åŠ›æå‡äº†å¤šå°‘ã€‚\n\nCursor è¿˜æœ‰ä¸€ä¸ªå¾ˆå¥½çš„åŠŸèƒ½ï¼Œå®ƒåµŒå…¥å¹¶ç´¢å¼•ä½ çš„æ•´ä¸ªé¡¹ç›®ï¼Œä½¿ç†è§£æ–‡ä»¶ä¹‹é—´çš„å…³ç³»å˜å¾—æ›´åŠ å®¹æ˜“ã€‚å½“ä½ æ›´æ–°ä¸€ä¸ªæ–‡ä»¶æ—¶ï¼Œç´¢å¼•ä¹Ÿä¼šéšä¹‹æ›´æ–°ï¼Œè¿™æ„å‘³ç€ AI å¯¹ä½ çš„ä»£ç åº“å„ä¸ªéƒ¨åˆ†å¦‚ä½•ç»“åˆæœ‰äº†æ›´å¥½çš„ç†è§£ã€‚å¦‚æžœä½ åœ¨ä¸€ä¸ªå¤§åž‹ä»£ç åº“ä¸­å·¥ä½œï¼Œå¤šä¸ªæ–‡ä»¶ç›¸äº’ä¾èµ–ï¼Œè¿™ä¸€ç‚¹éžå¸¸æœ‰ç”¨ã€‚\n\n## ç¼ºç‚¹\n\nè¯è™½å¦‚æ­¤ï¼ŒCursor ä¸­ä¸€äº›æœ€ä½³åŠŸèƒ½æ˜¯éœ€è¦è®¢é˜…æ‰èƒ½ä½¿ç”¨çš„ã€‚æˆ‘é€šå¸¸ä¸åå¯¹ä¸ºå¢žåŠ çœŸå®žä»·å€¼çš„å·¥å…·ä»˜è´¹ï¼Œä½†åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘æœ‰ç‚¹å¤±æœ›ï¼Œå› ä¸ºæœ€æœ‰è¶£çš„ AI åŠŸèƒ½â€”â€”å¦‚å¤šæ–‡ä»¶ç¼–è¾‘â€”â€”å±žäºŽé«˜çº§ç‰ˆæœ¬ã€‚å¯¹äºŽä¸€ä¸ªä»ç„¶ç›¸å¯¹è¾ƒæ–°çš„å·¥å…·ï¼Œæˆ‘æƒ³çŸ¥é“è¿‡æ—©é™åˆ¶è¿™äº›åŠŸèƒ½æ˜¯å¦ä¼šé™åˆ¶å…¶é‡‡ç”¨ï¼Œå°¤å…¶è€ƒè™‘åˆ°å·²ç»æœ‰å¾ˆå¤šå¼€å‘è€…åœ¨ä¸º GitHub Copilot ä»˜è´¹ã€‚\n\næˆ‘åœ¨ä½¿ç”¨ Cursor æ—¶é‡åˆ°çš„å¦ä¸€ä¸ªé—®é¢˜æ˜¯ï¼Œè™½ç„¶å®ƒåœ¨å¿«é€Ÿã€å°åž‹ä»»åŠ¡æ–¹é¢è¡¨çŽ°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†æ›´å¤æ‚çš„é—®é¢˜æ—¶ç¼ºä¹æˆ‘æ‰€éœ€çš„ä¸€äº›çµæ´»æ€§ã€‚å®ƒéžå¸¸é€‚åˆå¿«é€Ÿä»£ç å»ºè®®å’Œé‡æž„ï¼Œä½†å½“æˆ‘éœ€è¦èƒ½å¤Ÿå¤„ç†æ›´å¤æ‚ä»»åŠ¡çš„å·¥å…·æ—¶ï¼Œæ¯”å¦‚è¯»å–æ—¥å¿—æˆ–æ‰§è¡Œæž„å»ºå‘½ä»¤ï¼Œæˆ‘å‘çŽ°è‡ªå·±åœ¨å¯»æ‰¾å…¶ä»–å·¥å…·ã€‚\n\n## Claude\\-Dev: å¼€æºçš„é»‘é©¬\n\nè¿™å°±æ˜¯**Claude\\-Devï¼ˆçŽ°åœ¨ç§°ä¸ºClineï¼‰**çš„ç”¨æ­¦ä¹‹åœ°ã€‚Claude\\-Devæ˜¯ä¸€ä¸ªé’ˆå¯¹VSCodeçš„å¼€æºæ‰©å±•ï¼Œè™½ç„¶å®ƒçš„æ‰“ç£¨ç¨‹åº¦ä¸å¦‚Cursorï¼Œä½†å®ƒæ­£åœ¨è¿…é€Ÿå‘å±•â€”â€”åœ¨æŸäº›æ–¹é¢ï¼Œå®ƒæ›´ä¸ºå¼ºå¤§ã€‚Claude\\-Devæœ€å¼•äººæ³¨ç›®çš„åœ°æ–¹åœ¨äºŽï¼Œå®ƒä¼¼ä¹Žä¸ä»…ä»…æ˜¯æä¾›ä»£ç ç‰‡æ®µçš„å»ºè®®ã€‚å®ƒæ˜¯ä¸€ä¸ªå¯ä»¥**ä¸Žæ‚¨çš„çŽ¯å¢ƒè¿›è¡Œæ›´æ·±å±‚æ¬¡äº’åŠ¨**çš„å·¥å…·ã€‚\n\nä¾‹å¦‚ï¼ŒClaude\\-Devå¯ä»¥è¯»å–æ‚¨çš„ç»ˆç«¯æ—¥å¿—ï¼Œç†è§£ä»£ç æ£€æŸ¥é”™è¯¯ï¼Œç”šè‡³è¿è¡Œä»»æ„çš„CLIå‘½ä»¤ã€‚è¿™æ„å‘³ç€å¦‚æžœæ‚¨é—®å®ƒä¸ºä»€ä¹ˆæ‚¨çš„é¡¹ç›®æ— æ³•æž„å»ºï¼Œå®ƒä¸ä»…ä¼šæä¾›å»ºè®®â€”â€”å®ƒå®žé™…ä¸Šä¼šåŽ»æŸ¥çœ‹ç›¸å…³æ–‡ä»¶ï¼Œå¼„æ¸…æ¥šæ‚¨æ­£åœ¨ä½¿ç”¨ä»€ä¹ˆç±»åž‹çš„é¡¹ç›®ï¼ˆNodeã€Reactã€Pythonç­‰ï¼‰ï¼Œå¹¶å°è¯•ä¸ºæ‚¨æž„å»ºå®ƒã€‚å¦‚æžœå‡ºçŽ°é”™è¯¯ï¼Œå®ƒä¼šè¯»å–æ—¥å¿—ï¼Œå°è¯•è¯Šæ–­é—®é¢˜ï¼Œç”šè‡³åœ¨éœ€è¦æ—¶åº”ç”¨ä¿®å¤ã€‚\n\nä¸è¿‡ï¼Œå®ƒå¹¶ä¸å®Œç¾Žã€‚æ ¹æ®æˆ‘çš„ç»éªŒï¼ŒClaude\\-Devåœ¨ç¼–è¾‘æ—¶çš„é€Ÿåº¦ä¸å¦‚Cursorã€‚è¿™å…¶ä¸­ä¸€ä¸ªåŽŸå› æ˜¯å®ƒé‡å†™æ•´ä¸ªæ–‡ä»¶ï¼Œè€Œä¸æ˜¯ä»…ä»…æ›´æ–°éœ€è¦æ›´æ”¹çš„éƒ¨åˆ†ã€‚è¿™ä¼šå¯¼è‡´é€Ÿåº¦å˜æ…¢ï¼Œå¦‚æžœæ‚¨ä¸ºAPIä»¤ç‰Œä»˜è´¹ï¼ˆæ‚¨éœ€è¦æä¾›è¦ä½¿ç”¨çš„LLMçš„APIå¯†é’¥ï¼‰ï¼Œå®ƒæ¶ˆè€—è¿™äº›ä»¤ç‰Œçš„é€Ÿåº¦æ¯”åº”æœ‰çš„è¦å¿«ã€‚æˆ‘ä¸€ç›´åœ¨è€ƒè™‘ä¸ºè¿™ä¸ªé¡¹ç›®è´¡çŒ®ä»£ç ï¼Œä¿®å¤è¿™ä¸ªé—®é¢˜ï¼Œä½¿å…¶é€šè¿‡è¯¸å¦‚`sed`çš„Shellå‘½ä»¤ä»…æ›´æ–°å¿…è¦çš„è¡Œã€‚\n\næˆ‘å‘çŽ°çš„ä¸€ä¸ªç‰¹åˆ«æœ‰è¶£çš„åŠŸèƒ½æ˜¯Claude\\-Devå¦‚ä½•ä½¿ç”¨Puppeteeræ¥å¯è§†åŒ–æµ‹è¯•å’Œæ›´æ–°æ‚¨çš„å‰ç«¯ã€‚æ‚¨å¯ä»¥ç»™å®ƒä¸€å¼ ç½‘ç«™çš„æˆªå›¾ï¼Œå®ƒä¼šå°†å…¶ä¸Žæ‚¨çš„åº”ç”¨è¿›è¡Œæ¯”è¾ƒï¼Œè¿­ä»£ç›´åˆ°æ‚¨çš„å‰ç«¯è¾¾åˆ°æ‚¨æƒ³è¦çš„å¤–è§‚ã€‚è¿™ä¸ªè¿‡ç¨‹å¹¶ä¸æ˜¯æœ€å¿«çš„ï¼Œä½†å®ƒåœ¨å¤„ç†CSSæ–¹é¢å‡ºå¥‡åœ°ä¼˜ç§€â€”â€”å¯¹æˆ‘è€Œè¨€ï¼Œè¿™é€šå¸¸æ˜¯ä¸€ä¸ªè€—æ—¶çš„çŽ¯èŠ‚ã€‚\n\n## å®ƒçš„ä¸è¶³ä¹‹å¤„\n\nClaude\\-Dev æ— ç–‘æ˜¯ä¸€ä¸ªé€‚åˆé‚£äº›ä¹äºŽå°è¯•ä»ç„¶æœ‰äº›ç²—ç³™å·¥å…·çš„äººçš„å·¥å…·ã€‚ä¸Žæ„Ÿè§‰æ›´åƒæ˜¯å·²ç»å‡†å¤‡å¥½æŠ•å…¥ä½¿ç”¨çš„ç²¾è‡´äº§å“çš„ Cursor ä¸åŒï¼ŒClaude\\-Dev æ›´åƒæ˜¯ä¸€ä¸ªæ­£åœ¨ç§¯æžå¼€å‘çš„å¼ºå¤§å·¥å…·ã€‚å®ƒå¹¶ä¸æ€»æ˜¯èƒ½åœ¨ç¬¬ä¸€æ¬¡å°±åšåˆ°æ­£ç¡®ï¼Œè€Œä¸”é€Ÿåº¦ä¹Ÿæ¯”æˆ‘å¸Œæœ›çš„è¦æ…¢ï¼Œä½†å®ƒåœ¨ä¸æ–­æ”¹è¿›ã€‚å®ƒæ˜¯å¼€æºçš„ï¼Œä¸»è¦ç”±ä¸€ä¸ªäººå¼€å‘ï¼Œè¿™ä½¿å¾—å®ƒçš„åˆ›æ–°é€Ÿåº¦æ›´åŠ ä»¤äººå°è±¡æ·±åˆ»ã€‚\n\n## é‚£ä¹ˆä½ åº”è¯¥ä½¿ç”¨å“ªä¸ªï¼Ÿ\n\nå¦‚æžœä½ æ­£åœ¨å¯»æ‰¾ä¸€ä¸ªç²¾è‡´ã€å¿«é€Ÿçš„ä½“éªŒï¼Œä¸“æ³¨äºŽé€Ÿåº¦å’Œå¿«é€Ÿå»ºè®®ï¼Œ**Cursor** å¯èƒ½æ˜¯æ›´å¥½çš„é€‰æ‹©ã€‚å®ƒæ„Ÿè§‰çµæ•ï¼Œèƒ½å¤Ÿä¸ŽçŽ°æœ‰çš„ VSCode è®¾ç½®é›†æˆï¼Œå¹¶ä¸”å¯ä»¥ä¿æŒä½ çš„å·¥ä½œæµâ€”â€”ç›´åˆ°ä½ é‡åˆ°ä»˜è´¹å¢™ã€‚ä½†æ˜¯å¦‚æžœä½ å¯¹æ­¤æ²¡æœ‰é—®é¢˜ï¼Œå¹¶ä¸”ä¸éœ€è¦é¢å¤–çš„åŠŸèƒ½ï¼ŒCursor æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„å·¥å…·ã€‚\n\nå¦ä¸€æ–¹é¢ï¼Œå¦‚æžœä½ æƒ³è¦çš„ä¸ä»…ä»…æ˜¯è‡ªåŠ¨è¡¥å…¨ä»£ç çš„åŠŸèƒ½â€”â€”è€Œæ˜¯èƒ½çœŸæ­£å¸®åŠ©è°ƒè¯•ã€æž„å»ºå’Œè¿­ä»£ä½ çš„é¡¹ç›®çš„ä¸œè¥¿ï¼Œ**Claude-Dev** æ›´é€‚åˆä½ ã€‚å®ƒæ›´çµæ´»ï¼Œä½†ä¹Ÿç¨å¾®æ…¢ä¸€äº›ï¼Œè¾¹ç¼˜å¤„ç†å¾—ä¸å¤Ÿç²¾ç»†ã€‚å¦‚æžœä½ æ„¿æ„å°è¯•å¹¶ä¸”èƒ½å¤Ÿå¿å—ä¸€äº›å°ç¼ºé™·ï¼Œå®ƒæä¾›çš„åŠŸèƒ½æ˜¯ Cursor ç›®å‰æ‰€ä¸å…·å¤‡çš„ã€‚\n\nå¯¹æˆ‘æ¥è¯´ï¼Œ**Claude-Dev** èƒœå‡ºï¼Œä¸»è¦æ˜¯å› ä¸ºå®ƒä¸Žæˆ‘çš„å·¥ä½œæµç¨‹çš„æ·±åº¦é›†æˆã€‚èƒ½å¤Ÿè¯»å–æ—¥å¿—ã€è¿è¡Œå‘½ä»¤å¹¶è¿­ä»£ç›´åˆ°é—®é¢˜è§£å†³æ˜¯æ— ä»·çš„ï¼Œå°¤å…¶æ˜¯åœ¨æˆ‘å¤„ç†ä¸ç†Ÿæ‚‰çš„ä»£ç åº“æ—¶ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå½“æˆ‘éœ€è¦å¿«é€Ÿè¡ŒåŠ¨è€Œä¸æƒ³ç­‰å¾… AI å¤„ç†å‘½ä»¤æ—¶ï¼Œæˆ‘ä»ç„¶ä¼šä½¿ç”¨ **Cursor**ã€‚\n\n## æœ€åŽçš„æ€è€ƒ\n\nCursor å’Œ Claude-Dev éƒ½æä¾›äº†ç‹¬ç‰¹çš„ä¼˜åŠ¿ï¼Œæˆ‘è®¤ä¸ºæˆ‘ä»¬åªæ˜¯åœ¨æŽ¢ç´¢ AI é©±åŠ¨çš„ç¼–ç å·¥å…·æ‰€èƒ½åšçš„äº‹æƒ…çš„è¡¨é¢ã€‚è¿™é‡Œæœ‰å¾ˆå¤§çš„æ½œåŠ›ï¼Œå°¤å…¶æ˜¯éšç€è¿™äº›å·¥å…·çš„ä¸æ–­å‘å±•ã€‚æˆ‘å¾ˆæœŸå¾…çœ‹åˆ°å®ƒä»¬çš„æœªæ¥ï¼Œå¹¶å°†ç»§ç»­å°è¯•è¿™ä¸¤è€…ï¼Œçœ‹çœ‹å®ƒä»¬å¦‚ä½•èžå…¥æˆ‘çš„å¼€å‘å·¥ä½œæµç¨‹ã€‚\n\nåŒæ—¶ï¼Œæˆ‘å»ºè®®ä½ è‡ªå·±å°è¯•ä¸€ä¸‹è¿™ä¸¤è€…ã€‚æ¯ä¸ªå·¥å…·éƒ½æœ‰å…¶ä¼˜ç‚¹ï¼Œä½ å¯èƒ½ä¼šå‘çŽ°å…¶ä¸­ä¸€ä¸ªæ›´é€‚åˆä½ çš„é£Žæ ¼ï¼Œå…·ä½“å–å†³äºŽä½ æ­£åœ¨å¤„ç†çš„å†…å®¹ã€‚\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/a-new-risings-red-star-qwen2-5-is-here-0dffe0fb09ad","frontmatter":{"title":"æ–°å´›èµ·çº¢æ˜Ÿï¼šQwen2.5æ¥äº†","meta_title":"æ–°å´›èµ·çº¢æ˜Ÿï¼šQwen2.5æ¥äº†","description":"ä¸€èµ·ç”¨pythonå’Œllama-cppæµ‹è¯•ä¸€ä¸‹é˜¿é‡Œäº‘æ–°ç”Ÿçš„ç”Ÿæˆå¼AI Qwen2.5","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*zU-XtqK2oMLkvscgxavjdw.png","categories":["Programming","Technology","Education"],"author":"Rifx.Online","tags":["Qwen2.5","multimodal","instruction-following","text-generation","multilingual"],"draft":false,"slug":"blog/a-new-risings-red-star-qwen2-5-is-here-0dffe0fb09ad"},"content":"\n\n\n### ä¸€èµ·æµ‹è¯•æ–°ç”Ÿçš„é˜¿é‡Œäº‘ç”Ÿæˆå¼AI Qwen2.5ï¼Œä½¿ç”¨Pythonå’Œllama-cpp\n\n\n\nåœ¨æ²¡æœ‰å¤ªå¤šå®£ä¼ å’Œé¢„æœŸå…¬å‘Šçš„æƒ…å†µä¸‹ï¼Œé˜¿é‡Œäº‘äºŽ9æœˆ19æ—¥å‘å¸ƒäº†ä»–ä»¬çš„æ——èˆ°æ¨¡åž‹ç³»åˆ—Qwen2.5ã€‚\n\né˜¿é‡Œäº‘åœ¨Qwenä¸Šçš„é©å‘½æ€§æ—…ç¨‹å†æ¬¡å±•ç¤ºäº†é€šè¿‡åˆ›æ–°çš„å¼ºå¤§é¢†å¯¼åŠ›ã€‚\n\næ€Žä¹ˆåšçš„ï¼Ÿå®ƒä»¬æœ‰ä»€ä¹ˆç‰¹åˆ«ä¹‹å¤„ï¼Ÿæˆ‘ä»¬åº”è¯¥æœŸå¾…ä»€ä¹ˆï¼Ÿ\n\nåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†æŽ¢è®¨æ–°æ¨¡åž‹å¹¶æ£€æŸ¥å…¶æ€§èƒ½ã€‚ä½œä¸ºåŽç»­ï¼Œåœ¨ä¸‹ä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨`llama-cpp-python`å’Œé‡åŒ–ç‰ˆæœ¬çš„qwen2.5â€“1.5b-instructï¼Œå¯¹æ¨¡åž‹è¿›è¡Œ13é¡¹NLPä»»åŠ¡çš„æµ‹è¯•ã€‚\n\näº‹å®žä¸Šï¼Œæˆ‘ç›¸ä¿¡æˆ‘ä»¬æ˜¯æœ€ä½³çš„åŸºå‡†å·¥å…·ï¼Œèƒ½å¤Ÿå…¨é¢è¯„ä¼°ä¸€ä¸ªæ¨¡åž‹æ˜¯å¦é€‚åˆæˆ‘ä»¬ï¼\n\nçŽ°åœ¨ï¼Œæˆ‘ä»¬å°†è¦†ç›–ä»¥ä¸‹å†…å®¹ï¼š\n\n\n```python\n- Qwen2.5 family innovation\n- Declared scope, use cases and models\n- Qwen2.5: a party of Foundation models\n- Expanding Reach through Open-Source Contributions\n- Bridging Industries through cutting-edge AI solutions\n- 13 Tasks to prove it worth \n- Future outlook: continued Open-Sourcing\n```\nè®©æˆ‘ä»¬æ·±å…¥äº†è§£ï¼ \n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*OeQ5qeOzCdl8LPJOZZgTIw.png)\n\n## Qwen2.5å®¶æ—åˆ›æ–°\n\nQwenæ˜¯é˜¿é‡Œå·´å·´é›†å›¢Qwenå›¢é˜Ÿçš„å¤§åž‹è¯­è¨€æ¨¡åž‹å’Œå¤§åž‹å¤šæ¨¡æ€æ¨¡åž‹ç³»åˆ—ã€‚å°±åœ¨æ˜¨å¤©ï¼Œå¤§åž‹è¯­è¨€æ¨¡åž‹å·²å‡çº§ä¸ºQwen2.5ã€‚\n\nè¯­è¨€æ¨¡åž‹å’Œå¤šæ¨¡æ€æ¨¡åž‹å‡åœ¨å¤§è§„æ¨¡å¤šè¯­è¨€å’Œå¤šæ¨¡æ€æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶åœ¨é«˜è´¨é‡æ•°æ®ä¸Šè¿›è¡ŒåŽè®­ç»ƒï¼Œä»¥ä¸Žäººç±»åå¥½å¯¹é½ã€‚Qwenèƒ½å¤Ÿè¿›è¡Œè‡ªç„¶è¯­è¨€ç†è§£ã€æ–‡æœ¬ç”Ÿæˆã€è§†è§‰ç†è§£ã€éŸ³é¢‘ç†è§£ã€å·¥å…·ä½¿ç”¨ã€è§’è‰²æ‰®æ¼”ã€ä½œä¸ºAIä»£ç†ç­‰ã€‚\n\néšç€Qwen2.5çš„å‘å¸ƒä»¥åŠé¢å¤–å¼€æºæ¨¡åž‹çš„å‘å¸ƒï¼Œé˜¿é‡Œäº‘ç»§ç»­ä¿æŒå…¶é¢†å¯¼åœ°ä½ï¼Œä»¥æ»¡è¶³ä¼ä¸šç”¨æˆ·æ—¥ç›Šå¢žé•¿çš„AIéœ€æ±‚ã€‚è‡ªåŽ»å¹´å…­æœˆä»¥æ¥ï¼ŒQwenå®¶æ—é€šè¿‡Model Studioåœ¨æ¶ˆè´¹ç”µå­ã€æ±½è½¦ã€æ¸¸æˆç­‰å¤šä¸ªè¡Œä¸šå¸å¼•äº†è¶…è¿‡90,000ä¸ªéƒ¨ç½²ã€‚\n\nQwenè¿˜é€šè¿‡åœ¨Hugging Faceç­‰å¹³å°ä¸ŠæŽ¨å‡ºæ–°æ¨¡åž‹ï¼Œå¦‚Qwen1.5â€“110Bå’ŒCodeQwen1.5â€“7Bï¼Œæ‰©å¤§äº†å…¶å½±å“åŠ›ï¼Œå±•ç¤ºäº†é˜¿é‡Œå·´å·´å¯¹å¼€æºAIå¼€å‘çš„æ‰¿è¯ºã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*A4pEOgsLK2PAFtiaGQx1Qw.png)\n\n## å£°æ˜Žçš„èŒƒå›´ã€ç”¨ä¾‹å’Œæ¨¡åž‹\n\nåœ¨ Qwen2 å‘å¸ƒçš„è¿‡åŽ»ä¸‰ä¸ªæœˆé‡Œï¼Œä¼—å¤šå¼€å‘è€…åœ¨ Qwen2 è¯­è¨€æ¨¡åž‹åŸºç¡€ä¸Šæž„å»ºäº†æ–°çš„æ¨¡åž‹ï¼Œä¸ºæ•´ä¸ªç¤¾åŒºä»¥åŠé˜¿é‡Œäº‘æä¾›äº†å®è´µçš„åé¦ˆã€‚\n\n> åœ¨æ­¤æœŸé—´ï¼Œæˆ‘ä»¬ä¸“æ³¨äºŽåˆ›å»ºæ›´æ™ºèƒ½ã€æ›´çŸ¥è¯†ä¸°å¯Œçš„è¯­è¨€æ¨¡åž‹ã€‚ä»Šå¤©ï¼Œæˆ‘ä»¬å¾ˆé«˜å…´åœ°ä»‹ç» Qwen å®¶æ—çš„æœ€æ–°æˆå‘˜ï¼šQwen2.5ã€‚\n\nä»–ä»¬çš„å£°æ˜Žä¼´éšç€æœ‰å…³æ–°æ¨¡åž‹å®¶æ—çš„äº‹å®žï¼š\n\n* å¯†é›†åž‹ã€æ˜“äºŽä½¿ç”¨çš„ä»…è§£ç å™¨è¯­è¨€æ¨¡åž‹ï¼Œæä¾› 0.5Bã€1.5Bã€3Bã€7Bã€14Bã€32B å’Œ 72B å°ºå¯¸ï¼Œä»¥åŠåŸºç¡€å’ŒæŒ‡ä»¤å˜ä½“ã€‚\n* åœ¨æˆ‘ä»¬æœ€æ–°çš„å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œæ¶µç›–å¤šè¾¾ 18T çš„æ ‡è®°ã€‚\n* åœ¨éµå¾ªæŒ‡ä»¤ã€ç”Ÿæˆé•¿æ–‡æœ¬ï¼ˆè¶…è¿‡ 8K æ ‡è®°ï¼‰ã€ç†è§£ç»“æž„åŒ–æ•°æ®ï¼ˆä¾‹å¦‚ï¼Œè¡¨æ ¼ï¼‰ä»¥åŠç”Ÿæˆç»“æž„åŒ–è¾“å‡ºï¼ˆå°¤å…¶æ˜¯ JSONï¼‰æ–¹é¢æœ‰æ˜¾è‘—æ”¹è¿›ã€‚\n* å¯¹ç³»ç»Ÿæç¤ºçš„å¤šæ ·æ€§æ›´å…·éŸ§æ€§ï¼Œå¢žå¼ºäº†è§’è‰²æ‰®æ¼”å®žçŽ°å’ŒèŠå¤©æœºå™¨äººçš„æ¡ä»¶è®¾ç½®ã€‚\n* æ”¯æŒçš„ä¸Šä¸‹æ–‡é•¿åº¦å¯è¾¾ 128K æ ‡è®°ï¼Œå¹¶ä¸”å¯ä»¥ç”Ÿæˆå¤šè¾¾ 8K æ ‡è®°ã€‚\n* æ”¯æŒè¶…è¿‡ 29 ç§è¯­è¨€çš„å¤šè¯­è¨€åŠŸèƒ½ï¼ŒåŒ…æ‹¬ä¸­æ–‡ã€è‹±æ–‡ã€æ³•æ–‡ã€è¥¿ç­ç‰™æ–‡ã€è‘¡è„ç‰™æ–‡ã€å¾·æ–‡ã€æ„å¤§åˆ©æ–‡ã€ä¿„æ–‡ã€æ—¥æ–‡ã€éŸ©æ–‡ã€è¶Šå—æ–‡ã€æ³°æ–‡ã€é˜¿æ‹‰ä¼¯æ–‡ç­‰ã€‚\n\n## Qwen2.5ï¼šåŸºç¡€æ¨¡åž‹çš„èšä¼š\n\næ ¹æ®2024å¹´9æœˆ19æ—¥çš„[å®˜æ–¹åšå®¢æ–°é—»ç¨¿](https://qwenlm.github.io/blog/qwen2.5/)çš„å…¬å‘Šï¼š\n\n> ä»Šå¤©ï¼Œæˆ‘ä»¬å¾ˆé«˜å…´åœ°ä»‹ç»Qwenå®¶æ—çš„æœ€æ–°æˆå‘˜ï¼š**Qwen2.5**ã€‚æˆ‘ä»¬å®£å¸ƒè¿™å¯èƒ½æ˜¯åŽ†å²ä¸Šæœ€å¤§çš„å¼€æºå‘å¸ƒï¼è®©æˆ‘ä»¬å¼€å§‹åº†ç¥å§ï¼\n\n> æˆ‘ä»¬æœ€æ–°çš„å‘å¸ƒåŒ…å«äº†LLMs **Qwen2.5**ï¼Œä»¥åŠç”¨äºŽç¼–ç çš„ä¸“ç”¨æ¨¡åž‹**Qwen2.5-Coder**å’Œç”¨äºŽæ•°å­¦çš„æ¨¡åž‹**Qwen2.5-Math**ã€‚\n\nä¸ºäº†å±•ç¤ºQwen2.5çš„èƒ½åŠ›ï¼Œé˜¿é‡Œäº‘å›¢é˜Ÿå¯¹å…¶æœ€å¤§çš„å¼€æºæ¨¡åž‹**Qwen2.5â€“72B**â€”â€”ä¸€ä¸ª72Bå‚æ•°çš„ç¨ å¯†è§£ç å™¨è¯­è¨€æ¨¡åž‹â€”â€”ä¸Žé¢†å…ˆçš„å¼€æºæ¨¡åž‹å¦‚Llama-3.1â€“70Bå’ŒMistral-Large-V2è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*-MMFgkkWHa307jNo.jpg)\n\næ‰€æœ‰å¼€æ”¾æƒé‡æ¨¡åž‹éƒ½æ˜¯ç¨ å¯†çš„è§£ç å™¨è¯­è¨€æ¨¡åž‹ï¼Œæä¾›å¤šç§å°ºå¯¸ï¼ŒåŒ…æ‹¬ï¼š\n\n* Qwen2.5ï¼š0.5Bã€1.5Bã€3Bã€7Bã€14Bã€32Bå’Œ72B\n* Qwen2.5-Coderï¼š1.5Bã€7Bå’Œ32Bæ­£åœ¨å‘å¸ƒä¸­\n* Qwen2.5-Mathï¼š1.5Bã€7Bå’Œ72Bã€‚\n\né™¤äº†3Bå’Œ72Bå˜ä½“å¤–ï¼Œæ‰€æœ‰è¿™äº›å¼€æºæ¨¡åž‹å‡åœ¨Apache 2.0è®¸å¯è¯ä¸‹å‘å¸ƒã€‚æ‚¨å¯ä»¥åœ¨å„è‡ªçš„Hugging Faceåº“ä¸­æ‰¾åˆ°è®¸å¯è¯æ–‡ä»¶ã€‚\n\n> é™¤äº†è¿™äº›æ¨¡åž‹å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡Model Studioæä¾›æ——èˆ°è¯­è¨€æ¨¡åž‹çš„APIï¼š**Qwen-Plus**å’Œ**Qwen-Turbo**ï¼Œæˆ‘ä»¬é¼“åŠ±æ‚¨è¿›è¡ŒæŽ¢ç´¢ï¼\n\nä½†è¿™è¿˜ä¸æ˜¯å…¨éƒ¨ï¼\n\n> â€¦æˆ‘ä»¬è¿˜å¼€æºäº†**Qwen2-VL-72B**ï¼Œä¸Žä¸Šä¸ªæœˆçš„å‘å¸ƒç›¸æ¯”ï¼Œå…·æœ‰æ€§èƒ½æå‡ã€‚\n\nåœ¨**Qwen2.5**æ–¹é¢ï¼Œæ‰€æœ‰è¯­è¨€æ¨¡åž‹å‡åœ¨æˆ‘ä»¬æœ€æ–°çš„å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œæ¶µç›–äº†å¤šè¾¾**18ä¸‡äº¿**ä¸ªæ ‡è®°ã€‚ä¸ŽQwen2ç›¸æ¯”ï¼ŒQwen2.5èŽ·å¾—äº†æ˜¾è‘—æ›´å¤šçš„çŸ¥è¯†ï¼ˆMMLUï¼š85+ï¼‰ï¼Œå¹¶åœ¨ç¼–ç ï¼ˆHumanEval 85+ï¼‰å’Œæ•°å­¦ï¼ˆMATH 80+ï¼‰æ–¹é¢å¤§å¤§æå‡äº†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæ–°æ¨¡åž‹åœ¨æŒ‡ä»¤è·Ÿéšã€ç”Ÿæˆé•¿æ–‡æœ¬ï¼ˆè¶…è¿‡8Kä¸ªæ ‡è®°ï¼‰ã€ç†è§£ç»“æž„åŒ–æ•°æ®ï¼ˆä¾‹å¦‚ï¼Œè¡¨æ ¼ï¼‰å’Œç”Ÿæˆç»“æž„åŒ–è¾“å‡ºï¼Œå°¤å…¶æ˜¯JSONæ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹å–„ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*7c7CIbl-WVjazUeE.jpeg)\n\nQwen2.5æ¨¡åž‹é€šå¸¸å¯¹ç³»ç»Ÿæç¤ºçš„å¤šæ ·æ€§æ›´å…·éŸ§æ€§ï¼Œå¢žå¼ºäº†è§’è‰²æ‰®æ¼”çš„å®žæ–½å’ŒèŠå¤©æœºå™¨äººçš„æ¡ä»¶è®¾ç½®ã€‚\n\nä¸ŽQwen2ä¸€æ ·ï¼ŒQwen2.5è¯­è¨€æ¨¡åž‹æ”¯æŒå¤šè¾¾**128K**ä¸ªæ ‡è®°ï¼Œå¹¶å¯ä»¥ç”Ÿæˆå¤šè¾¾**8K**ä¸ªæ ‡è®°ã€‚å®ƒä»¬è¿˜æ”¯æŒè¶…è¿‡**29**ç§è¯­è¨€çš„å¤šè¯­è¨€æ”¯æŒï¼ŒåŒ…æ‹¬ä¸­æ–‡ã€è‹±æ–‡ã€æ³•æ–‡ã€è¥¿ç­ç‰™æ–‡ã€è‘¡è„ç‰™æ–‡ã€å¾·æ–‡ã€æ„å¤§åˆ©æ–‡ã€ä¿„æ–‡ã€æ—¥æ–‡ã€éŸ©æ–‡ã€è¶Šå—æ–‡ã€æ³°æ–‡ã€é˜¿æ‹‰ä¼¯æ–‡ç­‰ã€‚\n\n### Qwen-Coder æ˜¯å®¶æ—ä¸­çš„æ–°æˆå‘˜\n\nä¸“ä¸šçš„ä¸“å®¶è¯­è¨€æ¨¡åž‹ï¼Œå³ **Qwen2.5-Coder** ç”¨äºŽç¼–ç ï¼Œ**Qwen2.5-Math** ç”¨äºŽæ•°å­¦ï¼Œç›¸è¾ƒäºŽå®ƒä»¬çš„å‰èº« CodeQwen1.5 å’Œ Qwen2-Mathï¼Œè¿›è¡Œäº†å®žè´¨æ€§çš„å¢žå¼ºã€‚å…·ä½“æ¥è¯´ï¼ŒQwen2.5-Coder å·²åœ¨ **5.5 ä¸‡äº¿** ä¸ªä¸Žä»£ç ç›¸å…³çš„æ•°æ®ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œä½¿å¾—å³ä½¿æ˜¯è¾ƒå°çš„ç¼–ç ä¸“ç”¨æ¨¡åž‹ä¹Ÿèƒ½å¤Ÿåœ¨ç¼–ç è¯„ä¼°åŸºå‡†ä¸Šä¸Žæ›´å¤§çš„è¯­è¨€æ¨¡åž‹æä¾›ç«žäº‰åŠ›çš„è¡¨çŽ°ã€‚åŒæ—¶ï¼ŒQwen2.5-Math æ”¯æŒ **ä¸­æ–‡** å’Œ **è‹±æ–‡**ï¼Œå¹¶ç»“åˆäº†å¤šç§æŽ¨ç†æ–¹æ³•ï¼ŒåŒ…æ‹¬æ€ç»´é“¾ï¼ˆCoTï¼‰ã€æ€ç»´ç¨‹åºï¼ˆPoTï¼‰å’Œå·¥å…·é›†æˆæŽ¨ç†ï¼ˆTIRï¼‰ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Nvk4wrcB0SB4Tt-xbCzO6g.png)\n\n## é€šè¿‡å¼€æºè´¡çŒ®æ‰©å¤§å½±å“åŠ›\n\nä½œä¸ºæŒç»­è‡´åŠ›äºŽæ›´å¹¿æ³›ç¤¾åŒºçš„ä¸€éƒ¨åˆ†ï¼Œé˜¿é‡Œäº‘åœ¨å‘å¸ƒå„ç§è§„æ¨¡å’Œå˜ä½“çš„Qwenæ¨¡åž‹æ–¹é¢è¿ˆå‡ºäº†è¿›ä¸€æ­¥çš„æ­¥ä¼ã€‚è¿™åŒ…æ‹¬ï¼š\n\n1. **Qwen 0.5äº¿å‚æ•°**ï¼Œé€‚ç”¨äºŽæ›´ä¼ ç»Ÿåº”ç”¨çš„åŸºç¡€ç‰ˆæœ¬ã€‚2. ä¸€æ¬¾ç´§å‡‘ä½†å¼ºå¤§çš„æ¨¡åž‹ï¼Œä¸“é—¨ä¸ºæ¸¸æˆå¼€å‘é‡èº«å®šåˆ¶ï¼š**Qwen-VLï¼ˆè§†è§‰-è¯­è¨€ï¼‰**ï¼Œä¼˜åŒ–äº†é«˜æ€§èƒ½ã€‚\n\nè¿™äº›è¿›å±•å±•ç¤ºäº†é˜¿é‡Œå¯¹å¼€æºAIçš„æ‰¿è¯ºï¼Œä¸ä»…åˆ†äº«äº†Qwençš„åŸºç¡€ç‰ˆæœ¬ï¼Œè¿˜æŽ¨å‡ºäº†æ˜¾è‘—æ”¹è¿›å’Œæ–°æ¨¡åž‹ï¼Œç›´æŽ¥é’ˆå¯¹ä¼ä¸šéœ€æ±‚ï¼ŒåŒæ—¶å¢žå¼ºå…¶å¿«é€Ÿåˆ›æ–°çš„èƒ½åŠ›ã€‚\n\nè¿™ä¸Žä¸€ä¸ªæˆ˜ç•¥æ„¿æ™¯å¯†åˆ‡ç›¸å…³ï¼Œå³æŒç»­è´¡çŒ®æƒ åŠç¤¾åŒºæˆå‘˜å’Œè‡ªèº«å®¢æˆ·ï¼Œå¸®åŠ©ä»–ä»¬åœ¨å¤šä¸ªè¡Œä¸šå¯»æ±‚åˆ›æ–°åº”ç”¨ã€‚\n\n### é€šè¿‡å‰æ²¿çš„äººå·¥æ™ºèƒ½è§£å†³æ–¹æ¡ˆè¿žæŽ¥å„è¡Œä¸š\n\nä¸ºäº†å±•ç¤ºQwenåœ¨çŽ°å®žåœºæ™¯ä¸­çš„å¹¿æ³›èƒ½åŠ›ï¼Œé˜¿é‡Œäº‘ä¸€ç›´å¤„äºŽå‰æ²¿ï¼š\n\n1. **å°ç±³**ï¼šè¯¥å…¬å¸æ­£åœ¨å°†é˜¿é‡Œçš„æ¨¡åž‹é›†æˆåˆ°ä»–ä»¬çš„AIåŠ©æ‰‹å°çˆ±ä¸­ï¼Œå¹¶åœ¨å°ç±³æ™ºèƒ½æ‰‹æœºå’Œç”µåŠ¨æ±½è½¦ä¸­éƒ¨ç½²ï¼Œä»¥é€šè¿‡è¯­éŸ³å‘½ä»¤ç”Ÿæˆè½¦è½½å¨±ä¹å›¾åƒç­‰å¢žå¼ºåŠŸèƒ½ã€‚\n\n2. **å®Œç¾Žä¸–ç•Œæ¸¸æˆ**ï¼šQwenåœ¨æ¸¸æˆå¼€å‘ä¸­çš„é›†æˆå¯¼è‡´äº†åˆ›æ–°åº”ç”¨ï¼ŒåŒ…æ‹¬é€šè¿‡å¯¹è¯åŠ¨æ€æ”¹å–„æƒ…èŠ‚è§£æžå’Œå®žæ—¶å†…å®¹ç®¡ç†ã€‚\n\né˜¿é‡Œäº‘æ¨¡åž‹ä¸Žå„è¡Œä¸šä¹‹é—´çš„åˆä½œä¸ä»…ä¸°å¯Œäº†ç”¨æˆ·ä½“éªŒï¼Œè¿˜ä¿ƒè¿›äº†è¿™äº›è¡Œä¸šå†…æ›´å¤§çš„å¢žé•¿æœºä¼šï¼ŒæŽ¨åŠ¨äº†æ²¡æœ‰äººå·¥æ™ºèƒ½è¿›æ­¥çš„æƒ…å†µä¸‹æ— æ³•æƒ³è±¡çš„è¾¹ç•Œã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*ku8o3rq6PHDE8xcc.png)\n\n## 13 ä¸ªè¯æ˜Žå…¶ä»·å€¼çš„ä»»åŠ¡\n\n1.5 äº¿å‚æ•°çš„æ¨¡åž‹å¯èƒ½æ˜¯è€ƒè™‘åˆ°å¤æ‚æ€§ã€æç¤ºç†è§£å’ŒæŽ¨ç†é€Ÿåº¦çš„æœ€ä½³å˜ä½“ã€‚\n\næˆ‘å°†å‘æ‚¨å±•ç¤ºæˆ‘ä»…ä½¿ç”¨ `llama-cpp-python` å’Œä¸€ä¸ªç®€å•ç»ˆç«¯ç•Œé¢è¿›è¡Œçš„å†…éƒ¨æµ‹è¯•ã€‚\n\nä¸ºæ­¤ï¼Œæˆ‘åˆ›å»ºäº†ä¸€ä¸ªæç¤ºåˆ—è¡¨ï¼Œæ¶µç›–äº†ä¸€ç³»åˆ—é€šå¸¸ä½¿ç”¨çš„ä»»åŠ¡ï¼Œæ‚¨å¯ä»¥åœ¨æ¯æ¬¡ç”ŸæˆåŽåˆ†é…ä¸€ä¸ªæŠ•ç¥¨ï¼ˆä»Ž 0 åˆ° 5ï¼‰ã€‚è¿™æ˜¯ä¸€ä¸ªä¸ªäººçš„äººå·¥åŸºå‡†æµ‹è¯•ã€‚\n\n### éœ€æ±‚\n\nåˆ›å»ºä¸€ä¸ª `venv`ï¼ˆéœ€è¦ Python 3.11+ï¼‰ï¼šæˆ‘åœ¨è¿è¡Œ Windows 11 çš„è¿·ä½ ç”µè„‘ä¸Šè¿›è¡Œäº†æµ‹è¯•ã€‚\n\n```python\n## create the virtual environment\npython -m venv venv\n## activate the venv\nvenv\\Scripts\\activate\n## Install the dependencies \npip install llama-cpp-python==0.2.90 tiktoken\n```\næˆ‘ä»¬éœ€è¦ä»Ž[å®˜æ–¹ Qwen2.5 Hugging Face ä»“åº“](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF)ä¸‹è½½ GGUF æ–‡ä»¶ã€‚æˆ‘ä½¿ç”¨çš„æ˜¯ qwen2.5â€“1.5b-instruct-q5\\_k\\_m.gguf ç‰ˆæœ¬ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Fa-qFsx9RTFGZmM-vxCEPQ.png)\n\nåœ¨ä¸»é¡¹ç›®ç›®å½•ä¸­ä¸‹è½½æ–‡ä»¶ã€‚æˆ‘ä»¬å°±å‡†å¤‡å¥½äº†ã€‚\n\nè¿™é‡Œç”¨äºŽåˆ†æžçš„ä»£ç åœ¨æˆ‘çš„ GitHub ä»“åº“ä¸­ï¼š\n\næˆ‘å°†åœ¨ä¸‹ä¸€ç¯‡æ–‡ç« ä¸­è§£é‡Šæ•´ä¸ªä»£ç å’Œç»“æžœã€‚ä¿æŒå…³æ³¨ï¼\n\n## æœªæ¥å±•æœ›ï¼šæŒç»­çš„å¼€æº\n\nåœ¨æœªæ¥çš„è®¡åˆ’ä¸­ï¼Œé˜¿é‡Œå·´å·´è¿˜è¡¨è¾¾äº†ä»–ä»¬å¯¹æŒç»­å¼€æºè´¡çŒ®çš„æ‰¿è¯ºï¼Œé€šè¿‡ä¸ºä¸åŒé¢†åŸŸçš„å¼€å‘è€…å‘å¸ƒæ›´å°çš„ Qwen å˜ä½“ã€‚å®žé™…ä¸Šï¼Œåœ¨ Hugging Face ç¤¾åŒºä¸­ï¼Œè®¸å¤šç”¨æˆ·å·²å¼€å§‹é’ˆå¯¹ç‰¹å®šä»»åŠ¡å¯¹ Qwen è¿›è¡Œå¾®è°ƒï¼šæˆ‘åœ¨æˆ‘çš„ NuExtract æ–‡ç« ä¸­å†™äº†ä¸€ä¸ªä¾‹å­ï¼šè¿™ä¸ªæ¨¡åž‹ç³»åˆ—çš„è¾ƒå°å˜ä½“åŸºäºŽ Qwen2â€“0.5bï¼\n\nè¿™äº›äººå·¥æ™ºèƒ½æŠ€æœ¯å’Œæ¨¡åž‹è¿›å±•çš„å‘å±•æ˜¯å……åˆ†åˆ©ç”¨å¤§åž‹è¯­è¨€æ¨¡åž‹å¦‚ **Qwen** åœ¨å„ä¸ªè¡Œä¸šä¸­æ½œåŠ›çš„å…³é”®æ­¥éª¤ã€‚éšç€ Model Studio ä¸­å¼ºåŠ²çš„é‡‡ç”¨çŽ‡æŒç»­å¿«é€Ÿå¢žé•¿ï¼Œæ˜¾ç„¶é˜¿é‡Œäº‘ä¸ä»…é€šè¿‡æä¾›å…ˆè¿›çš„å·¥å…·è€Œä¸”é€šè¿‡ä¿ƒè¿›ä¼ä¸šä¹‹é—´çš„åˆ›æ–°ï¼Œæˆä¸ºäº†è¡Œä¸šçš„å…ˆé”‹é¢†å¯¼è€…ã€‚\n\nåœ¨æˆ‘è¿™è¾¹ï¼Œæˆ‘çš„å±•æœ›æ˜¯ç»§ç»­å¯¹æ–°æ¨¡åž‹è¿›è¡Œå†…éƒ¨æµ‹è¯•ï¼Œç‰¹åˆ«æ˜¯å¯¹å°åž‹æ¨¡åž‹ï¼Œæœ€é«˜åˆ° 3Bã€‚\n\nåœ¨ä¸‹ä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†ä¸Žæ‚¨åˆ†äº«æˆ‘çš„æ–¹æ³•ï¼Œå¦‚ä½•è¿è¡Œæ¨¡åž‹ä»¥åŠç”¨äºŽåä¸‰ä¸ª NLP ä»»åŠ¡çš„æç¤ºæ¨¡æ¿ã€‚\n\nå¸Œæœ›æ‚¨å–œæ¬¢è¿™ç¯‡æ–‡ç« ã€‚å¦‚æžœè¿™ä¸ªæ•…äº‹å¯¹æ‚¨æœ‰ä»·å€¼ï¼Œå¹¶ä¸”æ‚¨æƒ³ç¨å¾®è¡¨ç¤ºæ”¯æŒï¼Œæ‚¨å¯ä»¥ï¼š\n\n1. ä¸ºè¿™ä¸ªæ•…äº‹å¤šæ¬¡ç‚¹èµž\n2. çªå‡ºæ›´å€¼å¾—è®°ä½çš„éƒ¨åˆ†ï¼ˆè¿™å°†ä½¿æ‚¨æ›´å®¹æ˜“æ‰¾åˆ°å®ƒä»¬ï¼Œä¹Ÿè®©æˆ‘å†™å‡ºæ›´å¥½çš„æ–‡ç« ï¼‰\n3. **åŠ å…¥æˆ‘çš„[å®Œå…¨å…è´¹çš„æ¯å‘¨ Substack é€šè®¯](https://thepoorgpuguy.substack.com/about)**\n4. æ³¨å†Œ Medium ä¼šå‘˜ï¼ˆ$5/æœˆå¯é˜…è¯»æ— é™ Medium æ•…äº‹ï¼‰\n5. åœ¨ Medium ä¸Šå…³æ³¨æˆ‘\n6. é˜…è¯»æˆ‘çš„æœ€æ–°æ–‡ç«  <https://medium.com/@fabio.matricardi>\n\nè¿™é‡Œè¿˜æœ‰å‡ ç¯‡æ–‡ç« æ¥æ»¡è¶³æ‚¨çš„å¥½å¥‡å¿ƒï¼š\n\næœ¬æ–‡ä¸­å¼•ç”¨çš„èµ„æºï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Du7V61mEX_yIrfmF.png)\n\næ­¤æ•…äº‹å‘å¸ƒåœ¨ [Generative AI](https://generativeai.pub/)ã€‚ä¸Žæˆ‘ä»¬åœ¨ [LinkedIn](https://www.linkedin.com/company/generative-ai-publication) ä¸Šè”ç³»ï¼Œå¹¶å…³æ³¨ [Zeniteq](https://www.zeniteq.com/)ï¼Œä»¥ä¾¿åŠæ—¶äº†è§£æœ€æ–°çš„äººå·¥æ™ºèƒ½æ•…äº‹ã€‚\n\nè®¢é˜…æˆ‘ä»¬çš„ [é€šè®¯](https://www.generativeaipub.com/) å’Œ [YouTube](https://www.youtube.com/@generativeaipub) é¢‘é“ï¼ŒåŠæ—¶èŽ·å–ç”Ÿæˆå¼ AI çš„æœ€æ–°æ¶ˆæ¯å’Œæ›´æ–°ã€‚è®©æˆ‘ä»¬å…±åŒå¡‘é€  AI çš„æœªæ¥ï¼\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*pvLAT3it1FkdhVU0.png)\n\n"},{"lang":"zh","group":"blog","slug":"blog/a-practical-guide-for-using-autogen-in-software-applications-8799185d27ee","frontmatter":{"title":"åœ¨è½¯ä»¶åº”ç”¨ç¨‹åºä¸­ä½¿ç”¨ AutoGen çš„å®žç”¨æŒ‡å—","meta_title":"åœ¨è½¯ä»¶åº”ç”¨ç¨‹åºä¸­ä½¿ç”¨ AutoGen çš„å®žç”¨æŒ‡å—","description":"æ›´æ–°ï¼šè™½ç„¶è¿™ç¯‡æ–‡ç« æ˜¯ 4 ä¸ªæœˆå‰å†™çš„ï¼Œä½† AutoGen å·²ç»å‘ç”Ÿäº†å¾ˆå¤§å˜åŒ–ã€‚å¯¹äºŽå¯èƒ½å­˜åœ¨çš„ä¸€äº›é—®é¢˜ï¼Œæˆ‘æ·±è¡¨æ­‰æ„â€¦â€¦","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*yrraWH6aGNnbx8p-wfQ1OQ.jpeg","categories":["Programming","Chatbots","Autonomous Systems"],"author":"Rifx.Online","tags":["AutoGen","multi-agent","LLMs","customization","collaboration"],"draft":false,"slug":"blog/a-practical-guide-for-using-autogen-in-software-applications-8799185d27ee"},"content":"\n\n\n\n\n*æ›´æ–°ï¼šè™½ç„¶è¿™ç¯‡æ–‡ç« æ˜¯åœ¨å››ä¸ªæœˆå‰å†™çš„ï¼Œä½† AutoGen è‡ªé‚£æ—¶ä»¥æ¥å˜åŒ–å¾ˆå¤§ã€‚å¯¹äºŽæˆ‘ä»£ç ç¤ºä¾‹ä¸­å¯èƒ½è¿‡æ—¶çš„å†…å®¹ï¼Œæˆ‘æ·±æ„Ÿæ­‰æ„ã€‚*\n\nå¦‚æžœæ‚¨æƒ³äº†è§£ AutoGenï¼Œå¯ä»¥æŸ¥çœ‹ [æ–‡æ¡£](https://microsoft.github.io/autogen/)ã€[Colab ç¬”è®°æœ¬](https://microsoft.github.io/autogen/docs/Examples) å’Œ [åšå®¢](https://microsoft.github.io/autogen/blog)ã€‚éžå¸¸æ„Ÿè°¢ AutoGen å›¢é˜Ÿåˆ¶ä½œäº†ä¸€ä¸ªä»¤äººæƒŠå¹çš„äº§å“ï¼Œä½†è€å®žè¯´â€”â€”åœ¨é˜…è¯»äº†ä»–ä»¬çš„æ‰€æœ‰å†…å®¹åŽï¼Œæˆ‘ä»ç„¶ä¸çŸ¥é“å¦‚ä½•åœ¨ç»ˆç«¯æˆ– Jupyter Notebook ä¹‹å¤–ä½¿ç”¨ AutoGenã€‚\n\næœ¬æ–‡è¯•å›¾é€šè¿‡æä¾›ä¸€äº›æœ‰ç”¨çš„æ–¹æ³•æ¥å¸®åŠ©å¡«è¡¥è¿™ä¸ªç©ºç™½ï¼Œä½¿ AutoGen åœ¨è½¯ä»¶åº”ç”¨ä¸­å‘æŒ¥ä½œç”¨ã€‚ä»¥ä¸‹æ˜¯æˆ‘å°†è¦è®¨è®ºçš„ä¸»é¢˜ï¼š\n\n1. ä»£ç†ä¸ä»…é™äºŽé€šè¿‡ç»ˆç«¯è¿›è¡Œé€šä¿¡\n2. æ³¨å†Œè‡ªå®šä¹‰å›žå¤\n3. å¦‚ä½•ä»¥çœŸå®žçš„æ–¹å¼å°†çœŸå®žäººç±»çº³å…¥å¯¹è¯\n4. æ‚¨å¯ä»¥ï¼ˆå¹¶ä¸”åº”è¯¥ï¼‰è‡ªå®šä¹‰è°æ¥å‘è¨€\n5. æ‚¨ä¸å¿…ä½¿ç”¨ OpenAI\n6. å¯ä»¥ä½¿ç”¨å‡½æ•°è€Œä¸æ˜¯æ‰§è¡Œä»£ç \n7. å°†ä»£ç†ç”¨äºŽç»„ç»‡ï¼Œè€Œä¸ä»…ä»…æ˜¯å¯¹è¯\n\næœ€åŽï¼Œæˆ‘å°†è®¨è®ºä¸ºä»€ä¹ˆæˆ‘è®¤ä¸ºæ‚¨åº”è¯¥é¦–å…ˆä½¿ç”¨ AutoGenã€‚è®©æˆ‘ä»¬å¼€å§‹å§ï¼\n\n## ä»£ç†ä¸ä»…é™äºŽé€šè¿‡ç»ˆç«¯è¿›è¡Œé€šä¿¡\n\næ‚¨ä¼šçœ‹åˆ°æ¯ä¸ªäººéƒ½ä½¿ç”¨ç»ˆç«¯æˆ– Jupyter Notebook æ¼”ç¤º AutoGenã€‚è¿™å¯¹äºŽæ¼”ç¤ºæ¥è¯´ä¸é”™ï¼Œä½†è¿™äº›ä»£ç†ä¹‹é—´è¿˜æœ‰å…¶ä»–äº¤æµæ–¹å¼ã€‚\n\nAutoGen æœ‰ 2 ä¸ªåŸºæœ¬ç±»ï¼š[`UserProxyAg`ent](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/user_proxy_agent.py) å’Œ [`AssistantAg`ent](https://github.com/microsoft/autogen/blob/main/autogen/agentchat/assistant_agent.py)ã€‚å®ƒä»¬ç»§æ‰¿äº† [`ConversableAg`ent](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py) ç±»ï¼Œä»…ä¸ºåŸºç±»æä¾›äº†å‡ ä¸ªä¸åŒçš„é»˜è®¤å‚æ•°ã€‚\n\nå½“æ‚¨çœ‹åˆ°è¿™ä¸ªç»å…¸ä»£ç ç¤ºä¾‹æ—¶ï¼š\n\n```python\nassistant = autogen.AssistantAgent(\n    name=\"assistant\",\n    llm_config=llm_config\n)\nuser_proxy = autogen.UserProxyAgent(name=\"user_proxy\")\nawait user_proxy.a_initiate_chat(\n    assistant,\n    message=\"\"\"What date is today? Compare the year-to-date gain for META and TESLA.\"\"\",\n)\n```\nå‘ç”Ÿçš„äº‹æƒ…æ˜¯ `UserProxyAgent` å°†è°ƒç”¨å…¶è‡ªå·±çš„ `send` æ–¹æ³•ï¼Œè¿™å°†è°ƒç”¨ `AssistantAgent` çš„ [`rece`ive](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L514) æ–¹æ³•ï¼Œä¼ é€’åŽŸå§‹æ¶ˆæ¯ã€‚å°†ç”Ÿæˆå›žå¤ï¼ˆç¨åŽä¼šè¯¦ç»†è¯´æ˜Žï¼‰ï¼Œç„¶åŽ `AssistantAgent` å°†è°ƒç”¨å…¶ [`s`end](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L351) æ–¹æ³•ï¼Œè¿™å°†è°ƒç”¨ `UserProxyAgent` çš„ `receive` æ–¹æ³•ï¼Œä¾æ­¤ç±»æŽ¨ï¼Œç›´åˆ° `UserProxyAgent` ç¡®å®šå¯¹è¯å·²ç»ˆæ­¢ï¼ˆå¯ä»¥é€šè¿‡ `is_termination_msg` å‚æ•°è‡ªå®šä¹‰ï¼‰ã€‚\n\næˆ‘ç¬¬ä¸€æ¬¡â€œæç„¶å¤§æ‚Ÿâ€çš„æ—¶åˆ»æ˜¯å½“æˆ‘æ„è¯†åˆ°è¿™äº›ä»£ç†æ˜¯ç±»æ—¶ï¼Œæˆ‘å¯ä»¥åˆ›å»ºè‡ªå·±çš„è‡ªå®šä¹‰ä»£ç†ç±»ï¼Œç»§æ‰¿ AutoGen çš„ UserProxy/Assistant/Conversable Agent ç±»ï¼Œå¹¶é‡å†™ä»»ä½•é»˜è®¤æ–¹æ³•ã€‚è¿™ä½¿å¾— AutoGen éžå¸¸å¯æ‰©å±•ã€‚\n\næˆ‘æœ‰ä¸€ä¸ªç”¨ä¾‹ï¼Œéœ€è¦ä¸€ä¸ªå¯ä»¥é€šè¿‡ç½‘ç«™ä¸Šçš„èŠå¤© UI è¾“å…¥æ¶ˆæ¯çš„äººï¼ˆç”± `UserProxyAgent` ä»£ç†ï¼‰ï¼Œæˆ‘å¸Œæœ› `AssistantAgent` èƒ½åœ¨ UI ä¸­å›žå¤è¯¥èŠå¤©ï¼Œå¹¶èƒ½å¤ŸæŽ¥æ”¶æ›´å¤šæ¥è‡ªäººç±»ç”¨æˆ·çš„æ¶ˆæ¯ï¼Œå°±å¥½åƒäººç±»åªæ˜¯è¿™ä¸ª AutoGen å¯¹è¯ä¸­çš„å¦ä¸€ä¸ªä»£ç†ã€‚\n\næˆ‘å¯ä»¥é‡å†™ `send` å’Œ `receive` æ–¹æ³•ï¼ˆæˆ– `a_send` å’Œ `a_receive`ï¼‰ï¼Œå¹¶é€šè¿‡ httpã€websockets ç­‰è¿›è¡ŒæŽ¨é€/æ‹‰å–ã€‚æˆ‘å°è¯•äº†è¿™ä¸ªï¼Œå®ƒå¼€å§‹å·¥ä½œï¼Œä½†æ— æ³•æ‰©å±•ã€‚è®©æˆ‘ä»¬å­¦ä¹ ä¸€ç§æ›´å¥½çš„æ–¹æ³•ã€‚\n\n## æ³¨å†Œè‡ªå®šä¹‰å›žå¤\n\nAutoGen å…·æœ‰ä¸€ä¸ªæ’ä»¶ç³»ç»Ÿï¼Œå¯ä»¥è®©æ‚¨è‡ªå®šä¹‰ä»£ç†ç”Ÿæˆå›žå¤çš„æ–¹å¼ã€‚æˆ‘ä»¬ä¹ æƒ¯çœ‹åˆ°çš„ç¤ºä¾‹æ˜¯ AutoGen æŸ¥è¯¢ OpenAI èŽ·å–ç­”æ¡ˆï¼Œå¹¶å°†å…¶ä½œä¸ºå›žå¤ï¼Œä½†æ‚¨ä¹Ÿå¯ä»¥æ’å…¥è‡ªå·±çš„æ–¹æ³•ï¼š\n\n```python\nclass WeatherAgent(AssistantAgent):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, llm_config=False, **kwargs)\n        self.register_reply(Agent, WeatherAgent.get_weather)\n\n    async def get_weather(\n        self,\n        messages: List[Dict] = [],\n        sender=None,\n        config=None,\n    ) -> Tuple[bool, Union[str, Dict, None]]:\n        last_message = messages[-1][\"content\"]\n        result = await fetch_weather(last_message)\n        return True, result\n\nasync def fetch_weather(city: str) -> str:\n    async with httpx.AsyncClient() as client:\n        result = await client.post(\n            WEATHER_API_URL,\n            json={\"city\": question},\n        )\n        return result.json()\n\nweather_assistant = WeatherAgent(name=\"weather_assistant\")\nuser_proxy = autogen.UserProxyAgent(name=\"user_proxy\")\nawait user_proxy.a_initiate_chat(assistant, message=\"Lehi\")\nprint(weather_assistant.last_message)\n```\nåœ¨è¿™é‡Œï¼Œ`register_reply` å°†æ’å…¥æˆ‘çš„è‡ªå®šä¹‰æ–¹æ³•ä»¥èŽ·å–å›žå¤ï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼Œè¯¥æ–¹æ³•å°†æ”¾åœ¨ `position=0`ï¼Œè¿™æ„å‘³ç€å®ƒå°†æ˜¯å°è¯•çš„ç¬¬ä¸€ä¸ªå›žå¤æ–¹æ³•ã€‚è¯¥æ–¹æ³•åº”è¿”å›žä¸€ä¸ªå…ƒç»„ï¼Œå…¶ä¸­ç¬¬ä¸€ä¸ªé¡¹ç›®æ˜¯ä¸€ä¸ªå¸ƒå°”å€¼ï¼ŒæŒ‡ç¤ºæ­¤å›žå¤æ˜¯å¦ä¸ºåº”ä½¿ç”¨çš„å›žå¤ï¼Œæˆ–è€…æ˜¯å¦åº”å°è¯•ä¸‹ä¸€ä¸ªæ³¨å†Œçš„å›žå¤ï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨ OpenAI çš„å†…ç½®å›žå¤ç”Ÿæˆ â€” å®Œæ•´é¡ºåºè¯·å‚è§ [æ­¤å¤„](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L145-L153)ï¼‰ã€‚\n\näº†è§£ [`register_reply`](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L155) ä½¿æ‚¨èƒ½å¤Ÿè‡ªå®šä¹‰å›žå¤çš„æ£€ç´¢æ–¹å¼ï¼Œå…è®¸æ‚¨å¯åŠ¨å­å¤šä»£ç†å¯¹è¯ç­‰ã€‚\n\n## å¦‚ä½•ä»¥çœŸå®žçš„æ–¹å¼å°†çœŸå®žäººç±»çº³å…¥å¯¹è¯\n\nè¿™é‡Œæœ‰ä¸€ç§æ–¹æ³•ï¼š\n\n```python\n## user makes a POST /query { \"message\": \"What's the weather?\" }\n\n@query_blueprint.route(\"/query\", methods=[\"POST\"])\nasync def post_query():\n  message = request.form.get(\"message\")\n\n  assistant = autogen.AssistantAgent(\n    name=\"assistant\",\n    llm_config=llm_config\n    system_message=\"\"\"You're a helpful assistant.\n    If you need more info, ask the user for anything missing.\"\"\"\n  )\n  user_proxy = autogen.UserProxyAgent(\n    name=\"user_proxy\",\n    human_input_mode=\"NEVER\",\n    code_execution_config=False,\n    is_termination_msg=lambda message: True # Always True\n  )\n  weather_assistant = WeatherAgent(\n    name=\"weather_assistant\",\n    system_message=\"\"\"You're a helpful assistant to get the weather.\n    You fetch weather information, then return it.\"\"\"\n  )\n\n  groupchat = autogen.GroupChat(\n    agents=[assistant, user_proxy, weather_assistant],\n    messages=[]\n  )\n  manager = autogen.GroupChatManager(\n    name=\"Manager\",\n    groupchat=groupchat,\n    llm_config=llm_config,\n  )\n\n  await user_proxy.a_initiate_chat(manager, message=message)\n\n  return groupchat.messages[-1]\n```\nè¿™é‡Œå‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ\n\n1. æ¯å½“ä¸€æ¡æ¶ˆæ¯å‘é€åˆ° `user_proxy` æ—¶ï¼Œå¯¹è¯å°†ç»“æŸï¼ˆæˆ‘ä»¬ç¨åŽä¼šæ¢å¤å®ƒï¼‰ã€‚è¿™æ ·åšçš„åŽŸå› æ˜¯ä»€ä¹ˆï¼Ÿè¿™æ„å‘³ç€ `user_proxy` å®žé™…ä¸Šå¯ä»¥ä»£ç†ç”¨æˆ·ã€‚å®ƒä¸ä¼šå°è¯•å›žç­”ï¼Œè€Œæ˜¯ä¼šç»“æŸå½“å‰çš„å¯¹è¯æµç¨‹ï¼Œå…è®¸çœŸå®žçš„äººç±»ç”¨æˆ·å“åº”ï¼ˆé€šè¿‡æ¢å¤å¯¹è¯ â€” è§ä¸‹æ–‡ï¼‰ã€‚\n2. å¦‚æžœåŠ©ç†éœ€è¦æ›´å¤šä¿¡æ¯ï¼Œå®ƒä¼šè¯¢é—® user_proxyï¼Œè¿™å°†ç»“æŸå½“å‰å¯¹è¯ã€‚\n\nåœ¨ä¸Šè¿°ä»£ç ä¸­ï¼Œå¯èƒ½ä¼šå‘ç”Ÿä»¥ä¸‹æƒ…å†µï¼š\n\n1. user_proxy -> manager: â€œå¤©æ°”æ€Žä¹ˆæ ·ï¼Ÿâ€\n2. assistant -> manager: â€œç”¨æˆ·æ²¡æœ‰æŒ‡å®šå“ªä¸ªåŸŽå¸‚ã€‚â€\n3. manager -> user_proxy : å¯¹è¯å°†ç»“æŸ\n\nçŽ°åœ¨ï¼Œå¦‚æžœç”¨æˆ·æƒ³è¦å›žåº”å¹¶æ¢å¤å¯¹è¯ï¼Œæˆ‘ä»¬è¯¥å¦‚ä½•åšåˆ°å‘¢ï¼Ÿæœ‰å¾ˆå¤šæ–¹æ³•å¯ä»¥åšåˆ°è¿™ä¸€ç‚¹ï¼Œè¿™é‡Œåªæ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼š\n\n```python\n## user makes a POST /query { \"message\": \"What's the weather?\" }\n## above posts returns a `history` array\n## user makes a second POST /query { \"message\": \"What's the weather?\", \"history\": history }\n\nclass ResumableGroupChatManager(GroupChatManager):\n    groupchat: GroupChat\n\n    def __init__(self, groupchat, history, **kwargs):\n        self.groupchat = groupchat\n        if history:\n            self.groupchat.messages = history\n\n        super().__init__(groupchat, **kwargs)\n\n        if history:\n            self.restore_from_history(history)\n\n    def restore_from_history(self, history) -> None:\n        for message in history:\n            # broadcast the message to all agents except the speaker.  This idea is the same way GroupChat is implemented in AutoGen for new messages, this method simply allows us to replay old messages first.\n            for agent in self.groupchat.agents:\n                if agent != self:\n                    self.send(message, agent, request_reply=False, silent=True)\n\n@query_blueprint.route(\"/query\", methods=[\"POST\"])\nasync def post_query():\n  message = request.form.get(\"message\")\n\n  assistant = autogen.AssistantAgent(\n    name=\"assistant\",\n    llm_config=llm_config\n    system_message=\"\"\"You're a helpful assistant.\n    If you need more info, ask the user for anything missing.\"\"\"\n  )\n  user_proxy = autogen.UserProxyAgent(\n    name=\"user_proxy\",\n    human_input_mode=\"NEVER\",\n    code_execution_config=False,\n    is_termination_msg=lambda message: True # Always True\n  )\n  weather_assistant = WeatherAgent(\n    name=\"weather_assistant\",\n    system_message=\"\"\"You're a helpful assistant to get the weather.\n    You fetch weather information, then return it.\"\"\"\n  )\n\n  groupchat = autogen.GroupChat(\n    agents=[assistant, user_proxy, weather_assistant],\n    messages=[]\n  )\n  manager = ResumableGroupChatManager(\n    name=\"Manager\",\n    groupchat=groupchat,\n    llm_config=llm_config,\n  )\n\n  await user_proxy.a_initiate_chat(manager, message=message)\n\n  return {\n    \"response\": groupchat.messages[-1],\n    \"history\": groupchat.messages,\n  }\n```\né€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæ‚¨å¯ä»¥å°†äººç±»çº³å…¥å¯¹è¯ï¼Œå°±åƒä»–ä»¬æ˜¯ç¾¤èŠä¸­çš„å¦ä¸€ä¸ªä»£ç†ä¸€æ ·ã€‚æ¯å½“åŠ©ç†ä»£ç†éœ€è¦äººç±»è¾“å…¥æ—¶ï¼Œå®ƒä»¬ä¼šè¯¢é—® user_proxyï¼Œuser_proxy ç„¶åŽç»“æŸå½“å‰å¯¹è¯ï¼Œå…è®¸äººç±»ç”¨æˆ·ç”¨æ›´å¤šä¿¡æ¯è¿›è¡Œå“åº”ï¼Œç„¶åŽæ¢å¤åˆ°ä¹‹å‰çš„å¯¹è¯ã€‚\n\nè¿™ç§æ–¹æ³•çš„å¥½å¤„æ˜¯ï¼š\n\n* å¯¹è¯å¯ä»¥é€šè¿‡æ‚¨æƒ³è¦çš„ä»»ä½•æ–¹å¼åŒ…å«çœŸå®žäººç±»è¾“å…¥ï¼ˆä¾‹å¦‚é€šè¿‡ http æˆ– websocketï¼‰ã€‚\n* åœ¨èŽ·å–äººç±»è¾“å…¥æ—¶ï¼Œå¯¹è¯è¢«æš‚åœã€‚è¿™ä¸ºå…¶ä»–å¯¹è¯å’Œè®¡ç®—é‡Šæ”¾äº†çº¿ç¨‹ã€‚\n* æ‚¨å¯ä»¥åœ¨ä¼šè¯ä¹‹é—´æŒä¹…åŒ–è¿™äº›å¯¹è¯ã€‚\n\n## ä½ å¯ä»¥ï¼ˆå¹¶ä¸”åº”è¯¥ï¼‰è‡ªå®šä¹‰è°æŽ¥ä¸‹æ¥å‘è¨€\n\nè¿™æ˜¯ä¸»è§‚çš„ï¼Œä½†æˆ‘è®¤ä¸ºä½ åº”è¯¥å§‹ç»ˆè‡ªå®šä¹‰å‘è¨€è€…çš„é€‰æ‹©æ–¹å¼ï¼Œå› ä¸ºï¼š\n\n1. ä½ å°†ä½¿ç”¨æ›´å°‘çš„ä»¤ç‰Œï¼ˆèŠ‚çœé‡‘é’±å’Œå“åº”æ—¶é—´ï¼‰\n2. ä½ å¯ä»¥å°†å†³å®šè°å‘è¨€çš„é€»è¾‘ä¸Žå®šä¹‰æ¯ä¸ªä»£ç†ç³»ç»ŸæŒ‡ä»¤çš„é€»è¾‘åˆ†å¼€\n\n\n```python\nshort_role_descriptions = {\n  \"user_proxy\": \"A proxy for the user\",\n  \"weather_assistant\": \"You can get the weather\",\n  \"planner\": \"You help coordinate the plan. Your turn happens when XYZ, but skip your turn when ABC\"\n}\n\nclass CustomGroupChat(GroupChat):\n    # The default message uses the full system message, which is a long string.  We are overriding this to use a shorter message.\n    def select_speaker_msg(self, agents: List[Agent]):\n        message = f\"\"\"You are in a role play game. The following roles are available:\n        ---\n        {new_line.join([f\"{agent.name}: {short_role_descriptions[agent.name]}\" for agent in agents])}\n        ---\n\n        The role who plays next depends on the conversation.  User_Proxy will star the conversation, and typically Planner would go next.\n\n        Here are some examples\n        ---\n        ... not shown here ...\n        ---\n\n        Read the following conversation.\n        Then select the next role from {', '.join([agent.name for agent in agents])} to play. Only return the role.\"\"\"\n        return message\n```\n\n## ä½ ä¸å¿…ä½¿ç”¨ OpenAI\n\nAutoGen å·²ç»æŒ‡å‡ºï¼Œä½ å¯ä»¥ä½¿ç”¨å…¶ä»– LLMï¼Œåªè¦å®ƒä»¬æ˜¯â€œç±»ä¼¼ ChatGPTâ€çš„ï¼Œè¿™æ„å‘³ç€å®ƒä»¬çš„ API å“åº”ä¸Ž ChatGPT API è°ƒç”¨çš„å½¢çŠ¶å’Œå“åº”ç›¸ä¼¼ã€‚\n\nä½†æ˜¯ï¼Œè¯·è®°ä½è¿™äº›ä»£ç†æ˜¯ç±»ï¼Œå¹¶ä¸”ä½ å¯ä»¥é‡å†™å¤§å¤šæ•°æ–¹æ³•ï¼Ÿ\n\nå°è¯•é‡å†™æ–¹æ³•: [generate\\_oai\\_reply](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L678)ï¼Œä½ å¯ä»¥æŸ¥è¯¢ä»»ä½•ä½ æƒ³è¦çš„ LLMã€‚\n\n## å‡½æ•°å¯ä»¥ç”¨æ¥ä»£æ›¿æ‰§è¡Œä»£ç \n\nå½“æˆ‘åŽ»æ‰¾æˆ‘ä»¬çš„å®‰å…¨å›¢é˜Ÿå¹¶è¯´ï¼šâ€œæˆ‘æƒ³åœ¨Kubernetesä¸­ä¸ºæˆ‘çš„æœåŠ¡ä½¿ç”¨AutoGenã€‚å®ƒéœ€è¦èƒ½å¤Ÿæ‰§è¡Œä»»ä½•ç”±LLMç”Ÿæˆçš„ä»»æ„ä»£ç ã€‚ä½ ä»¬å¯¹æ­¤æ²¡é—®é¢˜å§ï¼Ÿâ€\n\nå½“ç„¶ï¼Œç­”æ¡ˆæ˜¯æ˜Žç¡®çš„ï¼šä¸å¯ä»¥ã€‚\n\né‚£ä¹ˆï¼Œä¸ºä»€ä¹ˆåœ¨æ²¡æœ‰è‡ªåŠ¨ä»£ç æ‰§è¡Œèƒ½åŠ›çš„æƒ…å†µä¸‹ä½¿ç”¨AutoGenï¼Ÿ\n\né™¤äº†ä¸‹é¢æåˆ°çš„åŽŸå› ä¹‹å¤–ï¼Œè¿˜æœ‰ä¸€ä¸ªåŽŸå› æ˜¯ä½ å¯ä»¥ä½¿ç”¨å‡½æ•°è°ƒç”¨æ¥å®Œå…¨æŽ§åˆ¶ä»£ç æ‰§è¡Œã€‚å¦‚æžœä½ æœ‰ä¸€ç»„æƒ³è¦æä¾›ç»™AutoGençš„pythonå‡½æ•°â€”â€”è¿™äº›å‡½æ•°æ˜¯ä½ ç¼–å†™çš„ã€ä½ æŽ§åˆ¶çš„ï¼Œå¹¶ä¸”å¯ä»¥æŽ¥å—ä¸€äº›å®‰å…¨å‚æ•°â€”â€”è¿™å¬èµ·æ¥æ€»æ¯”åœ¨ä½ çš„ç§æœ‰åŸºç¡€è®¾æ–½ä¸­å…è®¸ä»»ä½•ä»£ç è¢«æ‰§è¡Œè¦å¥½å¾—å¤šã€‚\n\n## ä½¿ç”¨ä»£ç†è¿›è¡Œç»„ç»‡ï¼Œè€Œä¸ä»…ä»…æ˜¯è¿›è¡Œå¯¹è¯\n\nä¹Ÿè®¸ä½ å¹¶ä¸éœ€è¦ä¸€ä¸ªè‡ªä¸»çš„å¤šä»£ç†å¯¹è¯ã€‚ä¹Ÿè®¸ä½ åªéœ€è¦å¯¹LLMè¿›è¡Œå‡ æ¬¡ä¸åŒçš„è°ƒç”¨ã€‚\n\næˆ‘ä»ç„¶å–œæ¬¢ä»…ä»…å‡ºäºŽç»„ç»‡ç›®çš„è€Œæ‹¥æœ‰ä¸åŒâ€œä»£ç†â€çš„æƒ³æ³•ã€‚è¿™æ˜¯ä¸€ä¸ªéžå¸¸ç–¯ç‹‚çš„æƒ³æ³•ï¼Œä½†è¯·æ ¹æ®è‡ªå·±çš„æƒ…å†µæ¥çœ‹å¾…å®ƒï¼š\n\n```python\nanalyst = autogen.AssistantAgent(\n    name=\"Analyst\",\n    system_message=\"\"\"Your an analyst.  You do XYZ.\"\"\",\n    llm_config=llm_config,\n)\n\nsummarizer = autogen.AssistantAgent(\n    name=\"Summarizer\",\n    system_message=\"\"\"Your a summarizer.  You do XYZ.\"\"\",\n    llm_config=llm_config,\n)\n\nreport = \"\"\"Some long report\"\"\"\n\nanalysis = analyst.generate_oai_reply(report)[1]\nsummary = summarizer.generate_oai_reply(report)[1]\n\nprint(f\"Analysis: {analysis}\")\nprint(f\"Summary: {summary}\")\n```\n\n## ä¸ºä»€ä¹ˆä½¿ç”¨ AutoGenï¼Ÿ\n\n1. AutoGen å…è®¸å¤šä¸ªä»£ç†ï¼Œå…·æœ‰ä¸åŒçš„ç³»ç»Ÿæç¤ºå’ŒæŒ‡ä»¤ï¼Œå…±åŒè§£å†³é—®é¢˜ã€‚å°±åƒåœ¨çŽ°å®žç”Ÿæ´»ä¸­ï¼Œä¸åŒçš„è§†è§’å…±åŒåˆä½œä¼šæ¯”å•ä¸€æ€ç»´æ›´å¥½åœ°è§£å†³é—®é¢˜ã€‚\n2. AutoGen GroupChat éžå¸¸å‡ºè‰²ã€‚å®ƒæä¾›äº†é€šå‘æ­£ç¡®ä¸“å®¶ï¼ˆä»£ç†ï¼‰çš„è·¯çº¿ï¼Œå¹¶å…è®¸å¯¹è¯åœ¨é—®é¢˜è§£å†³ä¹‹å‰è‡ªä¸»æŒç»­è¿›è¡Œã€‚æœ‰äº›å¯¹è¯å°†ä»Žä»£ç† a->b->c->d è¿›è¡Œï¼Œè€Œå…¶ä»–çš„å°†æ˜¯ b->a->d->cã€‚è¿™ä½¿å¾— AutoGen èƒ½å¤Ÿåœ¨ä¸éœ€è¦ä¸ºæ¯ç§åœºæ™¯åˆ¶å®šæ˜Žç¡®è§„åˆ™çš„æƒ…å†µä¸‹è§£å†³å„ç§ä¸åŒçš„é—®é¢˜ã€‚\n3. AutoGen èƒ½å¤Ÿä»Žé”™è¯¯ä¸­æ¢å¤ã€‚ä¾‹å¦‚ï¼Œæˆ‘åˆ›å»ºäº†ä¸€ä¸ªåŸºäºŽ AutoGen çš„æœåŠ¡ï¼Œè¯¥æœåŠ¡å‘ä¸€ä¸ª API å‘å‡ºè¯·æ±‚ã€‚æœ‰æ—¶ï¼ŒAPI è¯·æ±‚å› ä¸ºæœªèƒ½æ­£ç¡®å‘é€æ•°æ®è€Œå‡ºé”™ã€‚AutoGen GroupChat ä¸æ–­å°è¯•ä¸åŒçš„æ–¹æ³•ï¼Œç›´åˆ°æˆåŠŸã€‚æœ‰æ—¶ï¼Œè¿™éœ€è¦ 4 æ¬¡ä»¥ä¸Šçš„å°è¯•ï¼Œä½†æˆ‘çš„ Planner ä»£ç†æ²¡æœ‰æ”¾å¼ƒâ€”â€”åªæ˜¯è‡ªä¸»è°ƒæ•´ä»¥å¤„ç† API å¤±è´¥å¹¶å°è¯•æ–°æ–¹æ³•ã€‚\n4. AutoGen ä»Žä¸€å¼€å§‹å°±æå‡ºäº†å°† `UserProxyAgent` ä¸Ž `AssistantAgent` åˆ†ç¦»çš„æ¦‚å¿µã€‚è¿™ä¹Ÿä½¿æˆ‘ä»¬èƒ½å¤Ÿè®©ç”¨æˆ·ä»£ç†çœŸæ­£ä¸ºç”¨æˆ·ä»£ç†ï¼Œå¦‚ä¸Šæ‰€ç¤ºã€‚\n5. AutoGen æ˜¯ä¸€ä¸ªç»´æŠ¤è‰¯å¥½çš„åº“ã€‚æ¯å‘¨ä»–ä»¬éƒ½ä¼šæ·»åŠ ä¸€äº›æ–°åŠŸèƒ½ã€‚\n6. AutoGen éžå¸¸å¯æ‰©å±•ã€‚é€šè¿‡ä»–ä»¬æž„å»ºç±»çš„æ–¹å¼ï¼Œæ‚¨å¯ä»¥æ ¹æ®è‡ªå·±çš„å–œå¥½è‡ªå®šä¹‰ä»»ä½•å†…å®¹ã€‚\n7. AutoGen è¿˜æœ‰å…¶ä»–æˆ‘ä¸ä½¿ç”¨çš„åŠŸèƒ½ï¼Œä½†å…¶ä»–äººå¯èƒ½ä¼šè§‰å¾—å®ƒä»¬æœ‰å¸®åŠ©ï¼Œä¾‹å¦‚å¸®åŠ©æ‚¨è®¡ç®—å¯¹è¯çš„ä»¤ç‰Œå’Œæˆæœ¬ã€ç¼“å­˜ç­‰ã€‚\n\n"},{"lang":"zh","group":"blog","slug":"blog/a-robot-artist-just-made-more-money-than-you-have-in-your-entire-creative-career-13dc772ec612","frontmatter":{"title":"æœºå™¨äººè‰ºæœ¯å®¶èµšçš„é’±æ¯”ä½ æ•´ä¸ªåˆ›ä½œç”Ÿæ¶¯èµšçš„é’±è¿˜å¤š","meta_title":"æœºå™¨äººè‰ºæœ¯å®¶èµšçš„é’±æ¯”ä½ æ•´ä¸ªåˆ›ä½œç”Ÿæ¶¯èµšçš„é’±è¿˜å¤š","description":"æœºå™¨äººè‰ºæœ¯å®¶èµšçš„é’±æ¯”ä½ æ•´ä¸ªåˆ›ä½œç”Ÿæ¶¯èµšçš„é’±è¿˜å¤š","date":"2024-11-13T01:22:35.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*XUyq2c7RZCjJD6IQXjmd6A.png","categories":["Robotics","Art","Technology/Web"],"author":"Rifx.Online","tags":["Ai-Da","Turing","Sothebyâ€™s","painting","creativity"],"draft":false,"slug":"blog/a-robot-artist-just-made-more-money-than-you-have-in-your-entire-creative-career-13dc772ec612"},"content":"\n\n\n## æˆ‘ä»¬å·²ç»è¾¾åˆ°äº†äººå·¥æ™ºèƒ½åˆ›é€ åŠ›å’Œå•†ä¸šçš„æ–°é«˜åº¦\n\n\n\né¦–å…ˆï¼Œæˆ‘ä»¬è®©è®¡ç®—æœºå±å¹•æ ¹æ®äººç±»åˆ›ä½œæ¥åˆ›ä½œè‰ºæœ¯ã€‚çŽ°åœ¨ï¼Œæ˜¯ä¸€å°*çœŸæ­£çš„æœºå™¨äºº*åœ¨è¿›è¡Œç»˜ç”»ã€‚\n\næ²¡é”™â€”â€”ä¸€ä½â€œè¶…çŽ°å®žä¸»ä¹‰æœºå™¨äººè‰ºæœ¯å®¶â€å·²ç»è¢«è®­ç»ƒæˆèƒ½å¤Ÿåœ¨ç”»å¸ƒä¸Šå®žé™…ç»˜ç”»ã€‚å®ƒæç»˜çš„å·²æ•…è®¡ç®—æœºç§‘å­¦å®¶è‰¾ä¼¦Â·å›¾çµæœ€è¿‘åœ¨è‹å¯Œæ¯”æ‹å–ä¼šä¸Šæ‹å‡ºäº†**130ä¸‡ç¾Žå…ƒ**çš„é«˜ä»·ã€‚\n\næ­£å¦‚*IFLScience* [æŠ¥é“](https://proxy.rifx.online/https://www.iflscience.com/ai-robot-artist-strikes-gold-by-selling-painting-of-alan-turing-for-13-million-76701?fbclid=IwZXh0bgNhZW0CMTEAAR0KXPj5YDHnWibf6e97UWADZMuhPwGY4f_hnJnWs7rNoHN8KvvHquLAcFc_aem_hyBhYwyjT73j6PdAeXvOng)çš„é‚£æ ·ï¼Œè¿™ä½åä¸ºAi-Daçš„æœºå™¨äººæ˜¯ä»¥æ•°å­¦å®¶å’Œè®¡ç®—æœºå…ˆé©±é˜¿è¾¾Â·æ´›å¤«èŽ±æ–¯çš„åå­—å‘½åçš„â€”â€”åœ¨ä¸Žäººç±»é€šè¿‡è¯­è¨€æ¨¡åž‹å¯¹è¯åŽï¼Œé€‰æ‹©äº†å®ƒçš„ç»˜ç”»ä¸»é¢˜ã€‚ç„¶åŽï¼Œå®ƒç”¨æœºå™¨äººæ‰‹è‡‚å‹¾å‹’å’Œç»˜åˆ¶äº†å‡ ç§å›¾çµçš„ç‰ˆæœ¬ã€‚\n\næ¶ˆæ¯æ¥æºç§°ï¼Œæ¯å¹…æˆå“ç³»åˆ—çš„æ²¹ç”»/ä¸™çƒ¯ç”»å¤§çº¦éœ€è¦æœºå™¨äººâ€œè‰ºæœ¯å®¶â€å®Œæˆå…­åˆ°å…«å°æ—¶ã€‚Ai-Daæœ€åˆåˆ›ä½œäº†15å¹…ç”»ä½œï¼Œç»è¿‡äººå·¥åˆ›ä½œè€…çš„ç­›é€‰ï¼Œæœ€ç»ˆäº§å“â€œé€šè¿‡3Dçº¹ç†æ‰“å°æœºåº”ç”¨äºŽå¤§ç”»å¸ƒä¸Šâ€ã€‚\n\n## äººå·¥è‰ºæœ¯ï¼ŒçœŸå®žè´¢å¯Œ\n\nè¢«ç§°ä¸º *A.I. God* çš„â€œè‰ºæœ¯â€åœ¨æ‹å–ä¸­èŽ·å¾—äº†å¤§çº¦é¢„æœŸå”®ä»·çš„10å€ï¼Œæœ€ç»ˆç”±ä¸€ä½åŒ¿åä¹°å®¶ç«žå¾—ã€‚\n\nå¾ˆå¯èƒ½ï¼Œ$1.3ç™¾ä¸‡è¶…è¿‡äº†ä½ è‡ªå·±è‰ºæœ¯ä½œå“çš„æ”¶å…¥ï¼Œå‡è®¾ä½ ä¸æ˜¯ä¸€ä½å·²æ•…çš„è‰ºæœ¯å¶åƒã€‚æˆ‘ä»¬çŸ¥é“ï¼Œå°½ç®¡è‰ºæœ¯å®¶åœ¨ä¸–æ—¶ç»æµŽæ‹®æ®ï¼Œä½†äººä»¬è´­ä¹°çš„å·²æ•…ä¼ å¥‡è‰ºæœ¯å®¶çš„ä½œå“å¾€å¾€[èƒ½å–å‡ºæ•°ç™¾ä¸‡](https://proxy.rifx.online/https://www.veranda.com/luxury-lifestyle/artwork/g43012775/most-expensive-paintings-in-the-world/)ã€‚\n\nè¿™å¹¶ä¸æ˜¯æˆ‘ä»¬ç¬¬ä¸€æ¬¡çœ‹åˆ°æ•°å­—è‰ºæœ¯åœ¨æ‹å–ä¸­ä»¥è’è°¬çš„ä»·æ ¼æˆäº¤ã€‚\n\nåœ¨2018å¹´ï¼Œä¸€ä¸ªæ³•å›½é›†ä½“åœ¨ä½³å£«å¾—æ‹å–ä¼šä¸Šä¸ºä¸€å¹…åä¸º *Edmond de Belamy, from La Famille de Belamy* çš„AIè‚–åƒè‰ºæœ¯ä½œå“èŽ·å¾—äº†$432,500ã€‚\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*JK-X4et953sOiH1tovj3ag.jpeg)\n\nåŽæ¥ï¼Œä¸€ä½åå«Mike Winkelmannçš„è‰ºæœ¯å®¶ï¼Œä»¥â€œBeepleâ€é—»åï¼Œåœ¨ä½³å£«å¾—ä»¥$6900ä¸‡çš„ä»·æ ¼å‡ºå”®äº†ä¸€å¹…æ•°å­—NFTï¼ˆè¿˜è®°å¾—è¿™äº›å—ï¼Ÿï¼‰ï¼Œè¿™ä½¿ä»–è·»èº«äºŽæœ€æœ‰ä»·å€¼çš„çŽ°å­˜è‰ºæœ¯å®¶ä¹‹åˆ—ã€‚*The Verge* [è§£é‡Š](https://proxy.rifx.online/https://www.theverge.com/2021/3/11/22325054/beeple-christies-nft-sale-cost-everydays-69-million)è¯´ï¼Œåœ¨é‚£æ¬¡çªç ´æ€§çš„é”€å”®ä¹‹å‰ï¼Œä»–ä»Žä¸€å¹…å°åˆ·å“ä¸­èŽ·å¾—çš„æœ€é«˜æ”¶å…¥ä¸º$100ã€‚\n\nä½†è¿™æ˜¯ç¬¬ä¸€æ¬¡ï¼Œä¸€ä¸ªç±»äººå½¢æœºå™¨äººæž„æ€äº†ä¸€ä¸ªæ¦‚å¿µï¼Œå¹¶å®žé™…å°†é¢œæ–™åº”ç”¨äºŽç”»å¸ƒã€‚\n\nå½“ç„¶ï¼ŒAi-Daå¹¶ä¸ä¿ç•™è¿™ç¬”é’±â€”â€”åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæœºå™¨äººå¹¶ä¸éœ€è¦ä½¿ç”¨è´§å¸ã€‚å®ƒçš„äººç±»åˆ›é€ è€…ä»Žé”€å”®ä¸­èŽ·å¾—ç»æµŽåˆ©ç›Šã€‚\n\n## ç›®å‰ä»éœ€è¦äººç±»è¾“å…¥\n\nè‰¾ä¸¹Â·æ¢…å‹’ï¼ˆAidan Mellerï¼‰ï¼Œä¸€ä½è‰ºæœ¯ç»é”€å•†å’Œç”»å»Šä¸»ä»»ï¼Œæ˜¯Ai-Daæœºå™¨äººé¡¹ç›®çš„è´Ÿè´£äººã€‚IFLScienceè¡¨ç¤ºï¼Œå®žé™…çš„æœºå™¨äººæ˜¯ç”±è‹±å›½æœºå™¨äººé›†ä½“Engineered Artsåˆ¶é€ çš„ï¼Œè¯¥é›†ä½“è¿˜æŽ¨å‡ºäº†åŒæ ·ä»¤äººæ¯›éª¨æ‚šç„¶çš„äººå½¢æœºå™¨äºº[Ameca](https://proxy.rifx.online/https://engineeredarts.co.uk/robot/ameca/)ã€‚\n\nè¯·è®°ä½ï¼Œä»ç„¶éœ€è¦äººç±»æ¥æç¤ºæœºå™¨äººã€‚æ¢…å‹’è¡¨ç¤ºï¼Œæœ€åˆä¸ŽAi-Daçš„è®¨è®ºæ˜¯å…³äºŽæç»˜â€œä¸ºå–„çš„äººå·¥æ™ºèƒ½â€ã€‚è¿˜è®¨è®ºäº†åœ¨é£Žæ ¼å’Œè´¨æ„Ÿæ–¹é¢å¦‚ä½•è¿›è¡Œç»˜ç”»ã€‚\n\nâ€œå¥¹çš„äººå·¥æ™ºèƒ½èƒ½åŠ›æ˜¯ç”±ç‰›æ´¥å¤§å­¦å’Œä¼¯æ˜Žç¿°å¤§å­¦çš„åšå£«ç”Ÿå’Œæ•™æŽˆå…±åŒå¼€å‘çš„ï¼Œç¼–ç¨‹æ˜¯å›½é™…åŒ–çš„ï¼Œâ€ç½‘ç«™å…³äºŽAi-DaæŒ‡å‡ºã€‚å®ƒè¡¥å……è¯´ï¼Œäººç±»åŠ©æ‰‹å¸®åŠ©å‡†å¤‡äº†æ‰“å°ç”»å¸ƒï¼Œä½†æœºå™¨äººåœ¨å®Œæˆäº§å“æ–¹é¢è´Ÿæœ‰ä¸»è¦è´£ä»»ã€‚\n\nè¿™æ˜¯ä¸€ä¸ªAi-Daè§£é‡Šè‰ºæœ¯â€œè¿‡ç¨‹â€çš„è§†é¢‘ï¼š\n\n\n\n\n\n*å«æŠ¥* [æŠ¥é“](https://proxy.rifx.online/https://www.theguardian.com/artanddesign/2024/nov/08/alan-turing-portrait-ai-da-robot-painting-sale-price-auction)è¯´ï¼Œè‚–åƒçš„æŸç§æŠ½è±¡é£Žæ ¼å¯èƒ½æ˜¯æ•…æ„çš„ï¼š\n\n\n> **æ¢…å‹’è¡¨ç¤ºï¼Œâ€œè‰ºæœ¯ä½œå“çš„â€˜æŸ”å’Œè‰²è°ƒå’Œç ´ç¢Žçš„é¢éƒ¨å¹³é¢â€™ä¼¼ä¹Žæš—ç¤ºäº†â€˜å›¾çµè­¦å‘Šæˆ‘ä»¬åœ¨ç®¡ç†äººå·¥æ™ºèƒ½æ—¶å°†é¢ä¸´çš„æŒ‘æˆ˜â€™ã€‚â€**\n\nå›¾çµæ˜¯å¯¹çš„ã€‚äººå·¥æ™ºèƒ½åœ¨å†™ä½œå’Œè‰ºæœ¯ä¸­çš„å‡ºçŽ°å·²å¸­å·å…¨çƒï¼Œä»…åœ¨å‡ å¹´å‰å°±å·²å´­éœ²å¤´è§’ã€‚çŽ°åœ¨ï¼Œæˆ‘ä»¬å·²ç»è¾¾åˆ°ä¸€ä¸ªæœºå™¨äººå¯ä»¥ä¸ºå…¶æ¦‚å¿µå‘½ä»¤è¶…è¿‡ä¸€ç™¾ä¸‡ç¾Žå…ƒçš„åœ°æ­¥ã€‚\n\n## æŒ‘æˆ˜è‰ºæœ¯çš„å®šä¹‰\n\nä½†è¿™ä¸ä»…ä»…æ˜¯å…³äºŽé‡‘é’±ã€‚è¿™ç§è‰ºæœ¯è¿›ä¸€æ­¥æŒ‘æˆ˜äº†è‰ºæœ¯ç©¶ç«Ÿæ˜¯ä»€ä¹ˆï¼Œä»¥åŠæ˜¯å¦éœ€è¦æ‹¥æœ‰äººçš„æ„è¯†æ‰èƒ½äº§ç”Ÿå½±å“ã€‚\n\nè¿™ä¸ªé¡¹ç›®èƒŒåŽçš„äººä»¬è®¤ä¸º Ai\\-Da æœ¬èº«å°±æ˜¯â€œæ¦‚å¿µè‰ºæœ¯â€ã€‚è™½ç„¶è¿™ä¸ªæœºå™¨äººæ˜¾ç„¶ä¸æ˜¯äººç±»ï¼Œä½†è¿˜æœ‰å…¶ä»–é¡¹ç›®æ­£åœ¨è¿›è¡Œä¸­ï¼Œå¯èƒ½å¾ˆå¿«ä¼šåˆ›é€ å‡ºçœ‹èµ·æ¥çœŸå®žçš„è‰ºæœ¯å®¶ï¼Œæ‹¥æœ‰çœŸæ­£çš„ [æ´»çš®è‚¤](https://proxy.rifx.online/https://readmedium.com/the-new-face-of-artificial-intelligence-9c900d463cf9)ã€‚\n\nè°çŸ¥é“å‘¢ï¼Œä½ å¯èƒ½å¾ˆå¿«å°±åœ¨ä¸€ä¸ªç”Ÿæ´»ç´ æè¯¾ä¸Šä¸Žä¸€ä½åŒä¼´è‰ºæœ¯å®¶äº¤è°ˆï¼Œç§°èµžä»–ä»¬çš„æŠ€è‰ºï¼Œè€Œæ²¡æœ‰æ„è¯†åˆ°ä½ æ­£åœ¨ä¸Žä¸€ä¸ªæœºå™¨äººå¯¹è¯ã€‚\n\n*ä½ å¯¹æ­¤æœ‰ä½•çœ‹æ³•ï¼Ÿä½ æ˜¯æ„Ÿåˆ°å°è±¡æ·±åˆ»ã€æ„Ÿåˆ°æ¯›éª¨æ‚šç„¶ï¼Œè¿˜æ˜¯å¯¹ä½ çš„è‰ºæœ¯æœªæ¥æ„Ÿåˆ°æ‹…å¿§ï¼Ÿ*\n\n"},{"lang":"zh","group":"blog","slug":"blog/ai-image-generator-and-story-generation-app-using-fastapi-groq-and-replicate-706f29dc126f","frontmatter":{"title":"ä½¿ç”¨ FastAPIã€Groq å’Œ Replicate çš„ AI å›¾åƒç”Ÿæˆå™¨å’Œæ•…äº‹ç”Ÿæˆåº”ç”¨ç¨‹åº","meta_title":"ä½¿ç”¨ FastAPIã€Groq å’Œ Replicate çš„ AI å›¾åƒç”Ÿæˆå™¨å’Œæ•…äº‹ç”Ÿæˆåº”ç”¨ç¨‹åº","description":"é¡¹ç›®ä»‹ç»ï¼šAIå›¾åƒç”Ÿæˆå™¨å’Œæ•…äº‹åˆ›ä½œè€…","date":"2024-11-08T00:21:34.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*-fb-azx7fDZ-X9-PbIkiSQ.jpeg","categories":["Programming","Technology/Web","Generative AI"],"author":"Rifx.Online","tags":["FastAPI","Groq","Replicate","transcription","image-generation"],"draft":false,"slug":"blog/ai-image-generator-and-story-generation-app-using-fastapi-groq-and-replicate-706f29dc126f"},"content":"\n\n\n## é¡¹ç›®ä»‹ç»ï¼šAI å›¾åƒç”Ÿæˆå™¨å’Œæ•…äº‹åˆ›ä½œå·¥å…·\n\nAI å›¾åƒç”Ÿæˆå™¨å’Œæ•…äº‹åˆ›ä½œå·¥å…·æ˜¯ä¸€ä¸ªç½‘ç»œåº”ç”¨ç¨‹åºï¼Œåˆ©ç”¨å…ˆè¿›çš„ AI æŠ€æœ¯ä¸ºç”¨æˆ·æä¾›ä¸€ä¸ªåŸºäºŽéŸ³é¢‘æç¤ºç”Ÿæˆå›¾åƒå’Œæ•…äº‹çš„äº’åŠ¨å¹³å°ã€‚è¯¥åº”ç”¨ç¨‹åºä½¿ç”¨ FastAPI ä½œä¸ºåŽç«¯ï¼Œèƒ½å¤Ÿé«˜æ•ˆå¤„ç†è¯·æ±‚å’Œå“åº”ï¼Œè€Œå‰ç«¯åˆ™é‡‡ç”¨ HTMLã€CSSï¼ˆDaisyUI å’Œ Tailwind CSSï¼‰å’Œ JavaScript æž„å»ºï¼Œä»¥æä¾›å“åº”å¼ç”¨æˆ·ä½“éªŒã€‚è¯¥åº”ç”¨ç¨‹åºåˆ©ç”¨ llama\\-3\\.1â€“70b è¿›è¡Œæç¤ºç”Ÿæˆï¼Œblack\\-forest\\-labs/flux\\-1\\.1\\-pro è¿›è¡Œå›¾åƒç”Ÿæˆï¼Œä»¥åŠ llava\\-v1\\.5â€“7b vbision æ¨¡åž‹é€šè¿‡ Groq å’Œ Replicat.AI åˆ†åˆ«è¿›è¡Œæ•…äº‹åˆ›ä½œã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*0h1GzVVWs_df4OWAC-P59A.jpeg)\n\n## ä¸»è¦ç‰¹ç‚¹ï¼š\n\n1. éŸ³é¢‘å½•åˆ¶å’Œè½¬å½•ï¼šç”¨æˆ·å¯ä»¥å½•åˆ¶ä»–ä»¬çš„è¯­éŸ³æç¤ºï¼Œç„¶åŽä½¿ç”¨è¯­éŸ³è¯†åˆ«æŠ€æœ¯å°†å…¶è½¬å½•ä¸ºæ–‡æœ¬ã€‚\n\n2\\. å›¾åƒç”Ÿæˆï¼šåŸºäºŽè½¬å½•çš„æ–‡æœ¬ï¼Œåº”ç”¨ç¨‹åºç”Ÿæˆè¯¦ç»†çš„å›¾åƒæç¤ºï¼Œå¹¶ä½¿ç”¨ Replicate API åˆ›å»ºç›¸åº”çš„å›¾åƒã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*uiSG8Ir-Wv4a1huYqWhxBg.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*-eRPglLlJwms8N2DCXRyXg.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Mtle1K8AzjMHGxlGicFcGQ.png)\n\n3\\. å›¾åƒä¸‹è½½ï¼šç”¨æˆ·å¯ä»¥å°†ç”Ÿæˆçš„å›¾åƒä¸‹è½½åˆ°æœ¬åœ°è®¾å¤‡ã€‚\n\n4\\. æ•…äº‹ç”Ÿæˆï¼šåº”ç”¨ç¨‹åºå¯ä»¥åŸºäºŽåˆ›å»ºçš„å›¾åƒç”Ÿæˆå¼•äººå…¥èƒœçš„æ•…äº‹ï¼Œä¸ºè§†è§‰å†…å®¹æä¾›å™äº‹èƒŒæ™¯ã€‚\n\n5\\. ç”¨æˆ·å‹å¥½çš„ç•Œé¢ï¼šåº”ç”¨ç¨‹åºå…·æœ‰ç®€æ´ç›´è§‚çš„ç•Œé¢ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿè½»æ¾ä¸Žå„ç§åŠŸèƒ½è¿›è¡Œäº¤äº’ã€‚\n\n## ä½¿ç”¨çš„æŠ€æœ¯ï¼š\n\n* åŽç«¯ï¼šFastAPI, Groq, Replicate.ai, SpeechRecognition\n* å‰ç«¯ï¼šHTML, CSS (DaisyUI, Tailwind CSS), JavaScript\n* å›¾åƒå¤„ç†ï¼šç”¨äºŽå›¾åƒå¤„ç†çš„Pillow\n* å¼‚æ­¥æ“ä½œï¼šç”¨äºŽé«˜æ•ˆæ–‡ä»¶å¤„ç†å’Œç½‘ç»œè¯·æ±‚çš„aiohttpå’Œaiofiles\n\nè¯¥é¡¹ç›®ä½œä¸ºå°†å¤šä¸ªAIæœåŠ¡é›†æˆåˆ°ä¸€ä¸ªç»Ÿä¸€åº”ç”¨ç¨‹åºä¸­çš„ç¤ºä¾‹ï¼Œä½¿ç”¨æˆ·èƒ½å¤ŸæŽ¢ç´¢AIç”Ÿæˆå†…å®¹çš„åˆ›æ„å¯èƒ½æ€§ã€‚\n\n## ä»£ç åº“è¯¦ç»†è¯´æ˜Žï¼š\n\n1. **å‰ç«¯ (HTML/JavaScript)ï¼š**\n\n* *è¯¥åº”ç”¨ç¨‹åºä½¿ç”¨å•ä¸ª HTML é¡µé¢ (index.html)ï¼Œé‡‡ç”¨å“åº”å¼è®¾è®¡ï¼Œä½¿ç”¨ DaisyUI å’Œ Tailwind CSSã€‚*\n* *é¡µé¢åŒ…å«éŸ³é¢‘å½•åˆ¶ã€è½¬å½•ã€æç¤ºç”Ÿæˆã€å›¾åƒç”Ÿæˆå’Œæ•…äº‹ç”Ÿæˆçš„éƒ¨åˆ†ã€‚*\n* *JavaScript æ–‡ä»¶ (script.js) å¤„ç†ç”¨æˆ·äº¤äº’å¹¶ä¸ŽåŽç«¯ API é€šä¿¡ã€‚*\n\n**2\\. åŽç«¯ (FastAPI) :**\n\n* *ä¸»è¦åº”ç”¨ç¨‹åºå®šä¹‰åœ¨ app/main.py ä¸­ã€‚*\n* *å®ƒä½¿ç”¨ FastAPI åˆ›å»ºä¸€ä¸ªå…·æœ‰å¤šä¸ªç«¯ç‚¹çš„ Web æœåŠ¡å™¨ï¼š*\n\n**â€” *a. /: æä¾›ä¸»è¦ HTML é¡µé¢ã€‚***\n\n***â€” b. /transcribe:*** *å°†éŸ³é¢‘è½¬å½•ä¸ºæ–‡æœ¬ã€‚*\n\n***â€” c. /generate\\_prompt:*** *ä½¿ç”¨ Groq çš„ LLM ä»Žæ–‡æœ¬ç”Ÿæˆå›¾åƒæç¤ºã€‚*\n\n***â€” d. /generate\\_image:*** *ä½¿ç”¨ Replicate çš„ Flux æ¨¡åž‹ç”Ÿæˆå›¾åƒã€‚*\n\n***â€” e. /download\\_image:*** *ä¸‹è½½å¹¶ä¿å­˜ç”Ÿæˆçš„å›¾åƒã€‚*\n\n***â€” f. /generate\\_story\\_from\\_image:*** *æ ¹æ®å›¾åƒä½¿ç”¨ Groq çš„ LLaVA æ¨¡åž‹ç”Ÿæˆæ•…äº‹ã€‚*\n\n***â€” g. /download/{filename}:*** *æä¾›ä¸‹è½½çš„å›¾åƒæ–‡ä»¶ã€‚*\n\n**3\\. ä¸»è¦åŠŸèƒ½ï¼š**\n\n* *éŸ³é¢‘å½•åˆ¶å’Œè½¬å½•*\n* *æ–‡æœ¬åˆ°å›¾åƒçš„æç¤ºç”Ÿæˆ*\n* *æ ¹æ®æç¤ºç”Ÿæˆå›¾åƒ*\n* *æ ¹æ®å›¾åƒç”Ÿæˆæ•…äº‹*\n* *å›¾åƒä¸‹è½½å’Œä¿å­˜*\n\n**4\\. å¤–éƒ¨ APIï¼š**\n\n* [Groq:](https://console.groq.com/docs/models) ç”¨äºŽæ–‡æœ¬ç”Ÿæˆï¼ˆè°ƒæ•´åŽçš„æç¤ºå’Œ [æ•…äº‹](https://console.groq.com/docs/vision)ï¼‰\n* [Replicate AI:](https://replicate.com/black-forest-labs/flux-1.1-pro/api) black\\-forest\\-labs/flux\\-1\\.1\\-pro æ¨¡åž‹ç”¨äºŽå›¾åƒç”Ÿæˆ\n* éœ€è¦å®‰è£…çš„å¿…è¦åŒ…ï¼š\n\n```python\nfastapi\nuvicorn\njinja2\npython-multipart\npydantic\npython-dotenv\ngroq\nreplicate\nSpeechRecognition\npydub\naiohttp\naiofiles\nPillow\n```\n\n**æ‚¨å¯ä»¥ä½¿ç”¨ pip å®‰è£…è¿™äº›åŒ…ï¼š**\n\n```python\npip install fastapi uvicorn jinja2 python-multipart pydantic python-dotenv groq replicate SpeechRecognition pydub aiohttp aiofiles Pillow\n```\n\n**æ‰§è¡Œè¯´æ˜Žï¼š**\n\n* ***è®¾ç½®çŽ¯å¢ƒå˜é‡ï¼š*** *åœ¨æ ¹ç›®å½•åˆ›å»ºä¸€ä¸ª .env æ–‡ä»¶ï¼Œå†…å®¹å¦‚ä¸‹ï¼š*\n\n```python\nGROQ_API_KEY=your_groq_api_key_here\nREPLICATE_API_TOKEN=your_replicate_api_token_here\n```\n\n*ç”¨æ‚¨çš„å®žé™… API å¯†é’¥æ›¿æ¢å ä½ç¬¦å€¼ã€‚*\n\n* **ç¡®ä¿æ‚¨å·²å‡†å¤‡å¥½æ‰€æœ‰å¿…è¦çš„æ–‡ä»¶ï¼š**\n* â€” app/main.py\n* â€” app/config.py\n* â€” app/utils.py\n* â€” templates/index.html\n* â€” static/css/styles.css\n* â€” static/js/script.js\n* **è¿è¡Œ FastAPI æœåŠ¡å™¨ï¼š** å¯¼èˆªåˆ°åŒ…å« app/main.py çš„ç›®å½•å¹¶è¿è¡Œï¼š\n\n```python\nuvicorn app.main:app - reload\n```\n\n* **è®¿é—®åº”ç”¨ç¨‹åºï¼š**\n* â€” æ‰“å¼€æµè§ˆå™¨å¹¶è½¬åˆ° [http://127\\.0\\.0\\.1:8000](http://127.0.0.1:8000)\n* **ä½¿ç”¨åº”ç”¨ç¨‹åºï¼š**\n* â€” a. ç‚¹å‡»â€œå¼€å§‹å½•éŸ³â€å¹¶è¯´å‡ºæ‚¨çš„æç¤ºã€‚\n* â€” b. å®ŒæˆåŽç‚¹å‡»â€œåœæ­¢å½•éŸ³â€ã€‚\n* â€” c. éŸ³é¢‘å°†è‡ªåŠ¨è½¬å½•ã€‚\n* â€” d. ç‚¹å‡»â€œç”Ÿæˆå›¾åƒæç¤ºâ€ä»¥åˆ›å»ºè¯¦ç»†æç¤ºã€‚\n* â€” e. ç‚¹å‡»â€œç”Ÿæˆå›¾åƒâ€ä»¥æ ¹æ®æç¤ºåˆ›å»ºå›¾åƒã€‚\n* â€” f. ä½¿ç”¨â€œä¸‹è½½å›¾åƒâ€æŒ‰é’®ä¿å­˜ç”Ÿæˆçš„å›¾åƒã€‚\n* â€” g. ç‚¹å‡»â€œç”Ÿæˆæ•…äº‹â€ä»¥æ ¹æ®ç”Ÿæˆçš„å›¾åƒåˆ›å»ºæ•…äº‹ã€‚\n\næ³¨æ„ï¼šç¡®ä¿æ‚¨æœ‰è‰¯å¥½çš„äº’è”ç½‘è¿žæŽ¥ï¼Œå› ä¸ºè¯¥åº”ç”¨ç¨‹åºä¾èµ–å¤–éƒ¨ API æä¾›å„ç§åŠŸèƒ½ã€‚\n\nè¯¥åº”ç”¨ç¨‹åºå±•ç¤ºäº†å„ç§ AI æŠ€æœ¯çš„å¤æ‚é›†æˆï¼ŒåŒ…æ‹¬è¯­éŸ³è¯†åˆ«ã€è¯­è¨€æ¨¡åž‹å’Œå›¾åƒç”Ÿæˆï¼Œæ‰€æœ‰è¿™äº›éƒ½å°è£…åœ¨ç”¨æˆ·å‹å¥½çš„ Web ç•Œé¢ä¸­ã€‚\n\nå¦‚ä¸‹é¢æ‰€ç¤ºçš„ FastAPI UI\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Teb1wJzGOQZ3oqaLcLJwkA.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*6K-nORe7ubi0MRqIRLdrFA.png)\n\n## AIå›¾åƒç”Ÿæˆåº”ç”¨\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*1CClu2W3yRds1Lsk1rk9Ew.png)\n\n## è¯´å‡ºæ‚¨çš„æç¤º\n\n* å¼€å§‹å½•éŸ³\n* åœæ­¢å½•éŸ³\n* è½¬å½•ï¼ˆè½¬å½•æ–‡æœ¬ï¼‰ï¼šä¸€ä½ç¾Žä¸½çš„å°åº¦æ¨¡ç‰¹åœ¨æ—¶è£…ç§€ä¸­èµ°ä¸‹Runway Ram\n* æ ¹æ®è½¬å½•æ–‡æœ¬åˆ›å»ºä¸€ä¸ªæ–°çš„æç¤ºä»¥ç”Ÿæˆå›¾åƒ\n* â€” ç”Ÿæˆçš„æç¤ºï¼šâ€œ*ç”Ÿæˆä¸€å¹…é«˜åº¦é€¼çœŸçš„å›¾åƒï¼Œæç»˜ä¸€ä½ä»¤äººæƒŠè‰³çš„å°åº¦æ¨¡ç‰¹åœ¨æ ‡å¿—æ€§çš„Runway Ramä¸Šèµ°åŠ¨ï¼Œä½œä¸ºé«˜ç«¯æ—¶è£…ç§€çš„ä¸€éƒ¨åˆ†ã€‚è¿™ä½æ¨¡ç‰¹æ˜¯ä¸€ä½22å²çš„å°åº¦å¥³æ€§ï¼Œæ‹¥æœ‰é•¿é•¿çš„é»‘å‘ã€æ·±æ£•è‰²çš„çœ¼ç›å’Œæ— ç‘•çš„è‚Œè‚¤ï¼Œèº«ç©¿ç²¾ç¾Žã€å¤æ‚åˆºç»£çš„lehenga choliï¼Œé…ä»¥é‡‘é“¶äº®ç‰‡ï¼Œä¼ ç»Ÿå°åº¦æœé¥°ï¼Œå¹¶æ­é…é«˜è·Ÿéž‹ã€‚å¥¹çš„æœè£…è®¾è®¡ç²¾ç¾Žï¼Œåˆºç»£ç»†è‡´ã€‚å¼ºè°ƒä¼˜é›…çš„è¤¶çš±ã€é—ªäº®çš„é¢æ–™ï¼Œä»¥åŠå¥¹ä¼˜é›…çš„å§¿æ€å’Œè‡ªä¿¡çš„æ­¥ä¼ã€‚é…é¥°æ–¹é¢ï¼ŒåŠ å…¥åŽä¸½çš„ç å®ï¼Œå¦‚æ‰‹é“¾ã€é‡‘æ‰‹é•¯å’Œé¡¹é“¾ï¼Œè£…é¥°åœ¨å¥¹çš„æ‰‹ã€è„–å­å’Œä¸€ä¾§çš„å‘åž‹ä¸Šã€‚ç¯å…‰æ•ˆæžœèµ·ç€é‡è¦ä½œç”¨ï¼Œè®¾ç½®æ¸©æš–çš„èˆžå°èšå…‰ç¯ï¼Œçªå‡ºæ¨¡ç‰¹çš„æœè£…ï¼Œå¹¶ç”¨æŸ”å’Œçš„è“è‰²è°ƒç…§äº®æ•´ä¸ªçŽ¯å¢ƒã€‚æ‘„å½±è§’åº¦åº”å…¨é¢å±•ç¤ºæœè£…çš„ç»†èŠ‚ã€‚æœŸæœ›çš„åœºæ™¯è§†è§’æ˜¯æ¨¡ç‰¹æ­£é¢å…¨èº«ç…§ï¼Œèµ°é“å‘¨å›´è¢«å¼ºçƒˆçš„é‡‘è‰²å…‰çº¿ç…§äº®ï¼Œå…‰çº¿ä»Žå†…éƒ¨æ•£å‘å‡ºæ¥*ã€‚â€\n\n## ç”Ÿæˆçš„å›¾åƒ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*YljCEFTIc8hslZbf.jpg)\n\n## ä»Žå›¾åƒç”Ÿæˆçš„æ•…äº‹\n\n*è¿™ä½èº«ç€é‡‘é“¶äº®ç‰‡æœè£…å’Œé•¿è£™çš„é€‰ç¾Žçš‡åŽçš„æƒŠè‰³æ™¯è±¡è¶³ä»¥è®©ä»»ä½•è§‚ä¼—é™¶é†‰ã€‚Sashaya Gnanavelåœ¨å‰æ™¯ä¸­æ˜¾å¾—æ ¼å¤–å¼•äººæ³¨ç›®ï¼Œè‡ªä¿¡åœ°èµ°åœ¨Tå°ä¸Šï¼Œå¸å¼•ç€çŽ°åœºè§‚ä¼—çš„ç›®å…‰ã€‚å¥¹æ—¶å°šçš„æœè£…ï¼Œæ­é…ä¼˜é›…çš„çç é¡¹é“¾ï¼Œå¸å¼•äº†åœ¨åœºæ¯ä¸€ä¸ªäººçš„æ³¨æ„ã€‚è¿™ä¸ªç³»åˆ—å±•ç¤ºäº†é²œè‰³çš„è‰²å½©å’Œé—ªäº®çš„åˆºç»£ï¼Œå¢žåŠ äº†æ´»åŠ¨çš„æ•´ä½“è§†è§‰å¸å¼•åŠ›ã€‚Sashayaåœ¨èšå…‰ç¯ä¸‹çš„è‡ªä¿¡ä¸Žç¾Žä¸½ï¼ŒçœŸå®žåœ°è¯æ˜Žäº†å¥¹åœ¨æ—¶å°šè¡Œä¸šçš„æ‰åŽå’Œå¥‰çŒ®ã€‚å¥¹çš„å¦†å®¹ã€ç å®å’Œç²¾ç¾Žæœè£…æ‰€åˆ›é€ çš„è€€çœ¼æ•ˆæžœï¼Œä¸ºè®¾è®¡å’Œå·¥è‰ºçš„éžå‡¡å±•ç¤ºå¥ å®šäº†èˆžå°ã€‚è¿™ä¸ªå¼•äººæ³¨ç›®çš„åœºæ™¯ encapsulates é­”åŠ›å’Œå¥¢åŽï¼Œè®©è§‚ä¼—å¯¹è¿™ä¸€åˆ‡çš„ç»ç¾Žæ„Ÿåˆ°æƒŠå¹ã€‚*\n\n## ä»£ç å®žçŽ°\n\nåˆ›å»ºè™šæ‹ŸçŽ¯å¢ƒ\n\nè¦ä½¿ç”¨ Python çš„ venv æ¨¡å—åˆ›å»ºè™šæ‹ŸçŽ¯å¢ƒï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š\n\n* æ‰“å¼€ç»ˆç«¯æˆ–å‘½ä»¤æç¤ºç¬¦ã€‚\n* å¯¼èˆªåˆ°æ‚¨çš„é¡¹ç›®ç›®å½•ï¼ˆæ‚¨æƒ³è¦åˆ›å»ºè™šæ‹ŸçŽ¯å¢ƒçš„åœ°æ–¹ï¼‰ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ cd å‘½ä»¤æ›´æ”¹ç›®å½•ã€‚ä¾‹å¦‚ï¼š\n\n```python\ncd path/to/your/project\n```\n\n* é€šè¿‡è¿è¡Œä»¥ä¸‹å‘½ä»¤åˆ›å»ºè™šæ‹ŸçŽ¯å¢ƒï¼š\n\n```python\npython -m venv venv\n```\n\n* æ­¤å‘½ä»¤å°†åœ¨æ‚¨çš„é¡¹ç›®æ–‡ä»¶å¤¹ä¸­åˆ›å»ºä¸€ä¸ªåä¸º venv çš„æ–°ç›®å½•ï¼Œè¯¥ç›®å½•å°†åŒ…å«è™šæ‹ŸçŽ¯å¢ƒï¼ˆåœ¨ Windows ä¸Šï¼‰\n* æ¿€æ´»è™šæ‹ŸçŽ¯å¢ƒï¼š\n\n```python\nvenv\\Scripts\\activate\n```\n\n* æ–‡ä»¶å¤¹ç»“æž„\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*J3QJJACHVRtjrU1boxHUjA.png)\n\n* utils.py\n\n```python\nimport base64\nimport os\nfrom pydub import AudioSegment\n\ndef save_audio(audio_data):\n    # Decode the base64 audio data\n    audio_bytes = base64.b64decode(audio_data.split(\",\")[1])\n  \n    # Save the audio to a temporary file\n    temp_file = \"temp_audio.webm\"\n    with open(temp_file, \"wb\") as f:\n        f.write(audio_bytes)\n  \n    # Convert WebM to WAV\n    audio = AudioSegment.from_file(temp_file, format=\"webm\")\n    wav_file = \"temp_audio.wav\"\n    audio.export(wav_file, format=\"wav\")\n  \n    # Remove the temporary WebM file\n    os.remove(temp_file)\n  \n    return wav_file\n\ndef text_to_speech(text):\n    # Implement text-to-speech functionality if needed\n    pass\n```\n\n* main.py\n\n```python\n\"\"\"\n    1. é€šè¿‡éº¦å…‹é£Žå½•åˆ¶éŸ³é¢‘\n    2. å°†éŸ³é¢‘è½¬å½•ä¸ºæ–‡æœ¬\n    3. ä½¿ç”¨ Groq Llama3 æ¨¡åž‹ç”Ÿæˆå›¾åƒæç¤º\n    4. ä½¿ç”¨ Replicate.ai Flux æ¨¡åž‹ç”Ÿæˆå›¾åƒ\n    5. æ˜¾ç¤ºç”Ÿæˆçš„å›¾åƒ\n    6. ä¸‹è½½ç”Ÿæˆçš„å›¾åƒ\n    åº”ç”¨ç¨‹åºä½¿ç”¨ DaisyUI å’Œ Tailwind CSS è¿›è¡Œæ ·å¼è®¾ç½®ï¼Œæä¾›é»‘æš—æ¨¡å¼ç•Œé¢ã€‚å¸ƒå±€æ˜¯å“åº”å¼çš„ï¼Œåº”è¯¥åœ¨æ¡Œé¢å’Œç§»åŠ¨è®¾å¤‡ä¸Šéƒ½èƒ½è‰¯å¥½è¿è¡Œã€‚\næ³¨æ„ï¼šæ‚¨å¯èƒ½éœ€è¦æ ¹æ®æ‚¨ä½¿ç”¨çš„ç‰¹å®š API å’Œæ¨¡åž‹ä»¥åŠéƒ¨ç½²çŽ¯å¢ƒçš„å®‰å…¨æ€§è€ƒè™‘æ¥è°ƒæ•´ä»£ç çš„æŸäº›éƒ¨åˆ†ã€‚\n\n\"\"\"\nfrom fastapi import FastAPI, Request, HTTPException\nfrom fastapi.templating import Jinja2Templates\nfrom fastapi.staticfiles import StaticFiles\nfrom fastapi.responses import JSONResponse, FileResponse\nfrom pydantic import BaseModel\nimport speech_recognition as sr\nfrom groq import Groq\nimport replicate\nimport os\nimport aiohttp\nimport aiofiles\nimport time\nfrom dotenv import load_dotenv\nload_dotenv()\nfrom .utils import text_to_speech, save_audio\nfrom PIL import Image\nimport io\nimport base64\nimport base64\n\n\n## Function to encode the image\ndef encode_image(image_path):\n  with open(image_path, \"rb\") as image_file:\n    return base64.b64encode(image_file.read()).decode('utf-8')\n\napp = FastAPI()\n\napp.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\ntemplates = Jinja2Templates(directory=\"templates\")\n\n## Initialize Groq client with the API key\nGROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\nif not GROQ_API_KEY:\n    raise ValueError(\"GROQ_API_KEY is not set in the environment variables\")\ngroq_client = Groq(api_key=GROQ_API_KEY)\n\nclass AudioData(BaseModel):\n    audio_data: str\n\nclass ImagePrompt(BaseModel):\n    prompt: str\n\nclass PromptRequest(BaseModel):\n    text: str\n\n## Add this new model\nclass FreeImagePrompt(BaseModel):\n    prompt: str\n    image_path: str\n\n@app.get(\"/\")\nasync def read_root(request: Request):\n    return templates.TemplateResponse(\"index.html\", {\"request\": request})\n\n@app.post(\"/transcribe\")\nasync def transcribe_audio(audio_data: AudioData):\n    try:\n        # Save the audio data to a file\n        audio_file = save_audio(audio_data.audio_data)\n\n        # Transcribe the audio\n        recognizer = sr.Recognizer()\n        with sr.AudioFile(audio_file) as source:\n            audio = recognizer.record(source)\n        text = recognizer.recognize_google(audio)\n\n        return JSONResponse(content={\"text\": text})\n    except Exception as e:\n        raise HTTPException(status_code=400, detail=str(e))\n\n@app.post(\"/generate_prompt\")\nasync def generate_prompt(prompt_request: PromptRequest):\n    try:\n        text = prompt_request.text\n        # Use Groq to generate a new prompt\n        response = groq_client.chat.completions.create(\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a creative assistant that generates prompts for realistic image generation.\"},\n                {\"role\": \"user\", \"content\": f\"Generate a detailed prompt for a realistic image based on this description: {text}.The prompt should be clear and detailed in no more than 200 words.\"}\n            ],\n            model=\"llama-3.1-70b-versatile\",\n            max_tokens=256\n        )\n        generated_prompt = response.choices[0].message.content\n        print(f\"tweaked prompt:{generated_prompt}\")\n        return JSONResponse(content={\"prompt\": generated_prompt})\n    except Exception as e:\n        print(f\"Error generating prompt: {str(e)}\")\n        raise HTTPException(status_code=400, detail=str(e))\n\n@app.post(\"/generate_image\")\nasync def generate_image(image_prompt: ImagePrompt):\n    try:\n        prompt = image_prompt.prompt\n        print(f\"Received prompt: {prompt}\")\n\n        # Use Replicate to generate an image\n        output = replicate.run(\n            \"black-forest-labs/flux-1.1-pro\",\n            input={\n                \"prompt\": prompt,\n                \"aspect_ratio\": \"1:1\",\n                \"output_format\": \"jpg\",\n                \"output_quality\": 80,\n                \"safety_tolerance\": 2,\n                \"prompt_upsampling\": True\n            }\n        )\n      \n        print(f\"Raw output: {output}\")\n        print(f\"Output type: {type(output)}\")\n      \n        # Convert the FileOutput object to a string\n        image_url = str(output)\n      \n        print(f\"Generated image URL: {image_url}\")\n      \n        return JSONResponse(content={\"image_url\": image_url})\n    except Exception as e:\n        print(f\"Error generating image: {str(e)}\")\n        raise HTTPException(status_code=400, detail=str(e))\n\n@app.get(\"/download_image\")\nasync def download_image(image_url: str):\n    try:\n        # Create Output folder if it doesn't exist\n        output_folder = \"Output\"\n        os.makedirs(output_folder, exist_ok=True)\n\n        # Generate a unique filename\n        filename = f\"generated_image_{int(time.time())}.jpg\"\n        filepath = os.path.join(output_folder, filename)\n\n        # Download the image\n        async with aiohttp.ClientSession() as session:\n            async with session.get(image_url) as resp:\n                if resp.status == 200:\n                    async with aiofiles.open(filepath, mode='wb') as f:\n                        await f.write(await resp.read())\n\n        # Return the filepath and filename\n        return JSONResponse(content={\n            \"filepath\": filepath,\n            \"filename\": filename\n        })\n    except Exception as e:\n        print(f\"Error downloading image: {str(e)}\")\n        raise HTTPException(status_code=400, detail=str(e))\n\nclass StoryRequest(BaseModel):\n    filepath: str\n    filename: str\n\n@app.post(\"/generate_story_from_image\")\nasync def generate_story_from_image(content: StoryRequest):\n    try:\n        image_path = content.filepath\n        print(f\"Image path: {image_path}\")\n        # Check if the file exists\n        if not os.path.exists(image_path):\n            raise HTTPException(status_code=400, detail=\"Image file not found\")\n\n        # Getting the base64 string\n        base64_image = encode_image(image_path)\n\n        client = Groq()\n\n        chat_completion = client.chat.completions.create(\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\"type\": \"text\", \"text\": \"Generate a clear,concise,meaningful and engaging cover story for a highly acclaimed leisure magazine based on the image provided. The story should keep the audience glued and engaged and the story should bewithin 200 words.\"},\n                        {\n                            \"type\": \"image_url\",\n                            \"image_url\": {\n                                \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n                            },\n                        },\n                    ],\n                }\n            ],\n            model=\"llava-v1.5-7b-4096-preview\",\n        )\n\n        story = chat_completion.choices[0].message.content\n        print(f\"Generated story: {story}\")\n        return JSONResponse(content={\"story\": story})\n    except Exception as e:\n        print(f\"Error generating story from the image: {str(e)}\")\n        raise HTTPException(status_code=400, detail=str(e))\n\n@app.get(\"/download/{filename}\")\nasync def serve_file(filename: str):\n    file_path = os.path.join(\"Output\", filename)\n    return FileResponse(file_path, filename=filename)\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n```\n\n* script.js\n\n```python\nlet mediaRecorder;\nlet audioChunks = [];\n\nconst startRecordingButton = document.getElementById('startRecording');\nconst stopRecordingButton = document.getElementById('stopRecording');\nconst recordingStatus = document.getElementById('recordingStatus');\nconst transcription = document.getElementById('transcription');\nconst generatePromptButton = document.getElementById('generatePrompt');\nconst generatedPrompt = document.getElementById('generatedPrompt');\nconst generateImageButton = document.getElementById('generateImage');\nconst generatedImage = document.getElementById('generatedImage');\nconst downloadLink = document.getElementById('downloadLink');\nconst generateStoryButton = document.getElementById('generateStory');\nconst generatedStory = document.getElementById('generatedStory');\n\nstartRecordingButton.addEventListener('click', startRecording);\nstopRecordingButton.addEventListener('click', stopRecording);\ngeneratePromptButton.addEventListener('click', generatePrompt);\ngenerateImageButton.addEventListener('click', generateImage);\ngenerateStoryButton.addEventListener('click', generateStory);\n\nasync function startRecording() {\n    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n    mediaRecorder = new MediaRecorder(stream);\n\n    mediaRecorder.ondataavailable = (event) => {\n        audioChunks.push(event.data);\n    };\n\n    mediaRecorder.onstop = sendAudioToServer;\n\n    mediaRecorder.start();\n    startRecordingButton.disabled = true;\n    stopRecordingButton.disabled = false;\n    recordingStatus.textContent = 'Recording...';\n}\n\nfunction stopRecording() {\n    mediaRecorder.stop();\n    startRecordingButton.disabled = false;\n    stopRecordingButton.disabled = true;\n    recordingStatus.textContent = 'Recording stopped.';\n}\n\nasync function sendAudioToServer() {\n    const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });\n    const reader = new FileReader();\n    reader.readAsDataURL(audioBlob);\n    reader.onloadend = async () => {\n        const base64Audio = reader.result;\n        const response = await fetch('/transcribe', {\n            method: 'POST',\n            headers: {\n                'Content-Type': 'application/json',\n            },\n            body: JSON.stringify({ audio_data: base64Audio }),\n        });\n        const data = await response.json();\n        transcription.textContent = `Transcription: ${data.text}`;\n        generatePromptButton.disabled = false;\n    };\n    audioChunks = [];\n}\n\nasync function generatePrompt() {\n    const text = transcription.textContent.replace('Transcription: ', '');\n    const response = await fetch('/generate_prompt', {\n        method: 'POST',\n        headers: {\n            'Content-Type': 'application/json',\n        },\n        body: JSON.stringify({ text: text }),\n    });\n    const data = await response.json();\n    generatedPrompt.textContent = `Generated Prompt: ${data.prompt}`;\n    generateImageButton.disabled = false;\n}\n\nasync function generateImage() {\n    const prompt = generatedPrompt.textContent.replace('Generated Prompt: ', '');\n    const response = await fetch('/generate_image', {\n        method: 'POST',\n        headers: {\n            'Content-Type': 'application/json',\n        },\n        body: JSON.stringify({ prompt: prompt }),\n    });\n    const data = await response.json();\n    generatedImage.src = data.image_url;\n  \n    // Download the image and get the filepath\n    const downloadResponse = await fetch(`/download_image?image_url=${encodeURIComponent(data.image_url)}`);\n    const downloadData = await downloadResponse.json();\n  \n    // Store the filepath and filename for later use\n    generatedImage.dataset.filepath = downloadData.filepath;\n    generatedImage.dataset.filename = downloadData.filename;\n\n    // Set up the download link\n    downloadLink.href = `/download/${downloadData.filename}`;\n    downloadLink.download = downloadData.filename;\n    downloadLink.style.display = 'inline-block';\n}\n\nasync function generateStory() {\n    const imagePath = generatedImage.dataset.filepath;\n    const filename = generatedImage.dataset.filename;\n  \n    if (!imagePath || !filename) {\n        generatedStory.textContent = \"Error: Please generate an image first.\";\n        return;\n    }\n\n    try {\n        const response = await fetch('/generate_story_from_image', {\n            method: 'POST',\n            headers: {\n                'Content-Type': 'application/json',\n            },\n            body: JSON.stringify({ filepath: imagePath, filename: filename }),\n        });\n        if (!response.ok) {\n            throw new Error(`HTTP error! status: ${response.status}`);\n        }\n        const data = await response.json();\n      \n        // Display the generated story\n        generatedStory.textContent = data.story;\n      \n        // Make sure the story container is visible\n        document.getElementById('storyContainer').style.display = 'block';\n    } catch (error) {\n        console.error('Error:', error);\n        generatedStory.textContent = `Error: ${error.message}`;\n    }\n}\n\n// Modify the download link click event\ndownloadLink.addEventListener('click', async (event) => {\n    event.preventDefault();\n    const response = await fetch(downloadLink.href);\n    const blob = await response.blob();\n    const url = window.URL.createObjectURL(blob);\n    const a = document.createElement('a');\n    a.style.display = 'none';\n    a.href = url;\n    a.download = response.headers.get('Content-Disposition').split('filename=')[1];\n    document.body.appendChild(a);\n    a.click();\n    window.URL.revokeObjectURL(url);\n});\n```\n\n* style.css\n\n```python\nbody {\n    background-color: #1a1a2e;\n    color: #ffffff;\n}\n\n.container {\n    max-width: 1200px;\n}\n\n#imageContainer {\n    min-height: 300px;\n    display: flex;\n    align-items: center;\n    justify-content: center;\n    background-color: #16213e;\n    border-radius: 8px;\n}\n\n#generatedImage {\n    max-width: 100%;\n    max-height: 400px;\n    object-fit: contain;\n}\n```\n\n* index.html\n\n```python\n<!DOCTYPE html>\n<html lang=\"en\" data-theme=\"dark\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>AI å›¾åƒç”Ÿæˆå™¨</title>\n    <link href=\"https://cdn.jsdelivr.net/npm/daisyui@3.7.3/dist/full.css\" rel=\"stylesheet\" type=\"text/css\" />\n    <script src=\"https://cdn.tailwindcss.com\"></script>\n    <link rel=\"stylesheet\" href=\"{{ url_for('static', path='/css/styles.css') }}\">\n</head>\n<body>\n    <div class=\"container mx-auto px-4 py-8\">\n        <h1 class=\"text-4xl font-bold mb-8 text-center\">AI å›¾åƒç”Ÿæˆå™¨</h1>\n        <div class=\"grid grid-cols-1 md:grid-cols-2 gap-8\">\n            <div class=\"card bg-base-200 shadow-xl\">\n                <div class=\"card-body\">\n                    <h2 class=\"card-title mb-4\">è¯´å‡ºä½ çš„æç¤º</h2>\n                    <button id=\"startRecording\" class=\"btn btn-primary mb-4\">å¼€å§‹å½•éŸ³</button>\n                    <button id=\"stopRecording\" class=\"btn btn-secondary mb-4\" disabled>åœæ­¢å½•éŸ³</button>\n                    <div id=\"recordingStatus\" class=\"text-lg mb-4\"></div>\n                    <div id=\"transcription\" class=\"text-lg mb-4\"></div>\n                    <button id=\"generatePrompt\" class=\"btn btn-accent mb-4\" disabled>ç”Ÿæˆå›¾åƒæç¤º</button>\n                    <div id=\"generatedPrompt\" class=\"text-lg mb-4\"></div>\n                    <button id=\"generateImage\" class=\"btn btn-success\" disabled>ç”Ÿæˆå›¾åƒ</button>\n                </div>\n            </div>\n            <div class=\"card bg-base-200 shadow-xl\">\n                <div class=\"card-body\">\n                    <h2 class=\"card-title mb-4\">ç”Ÿæˆçš„å›¾åƒ</h2>\n                    <div id=\"imageContainer\" class=\"mb-4\">\n                        <img id=\"generatedImage\" src=\"\" alt=\"ç”Ÿæˆçš„å›¾åƒ\" class=\"w-full h-auto\">\n                    </div>\n                    <a id=\"downloadLink\" href=\"#\" download=\"generated_image.png\" class=\"btn btn-info\" style=\"display: none;\">ä¸‹è½½å›¾åƒ</a>\n                </div>\n            </div>\n        </div>\n        <!-- Add this new section after the existing cards -->\n        <div class=\"card bg-base-200 shadow-xl mt-8\">\n            <div class=\"card-body\">\n                <h2 class=\"card-title mb-4\">ä»Žå›¾åƒç”Ÿæˆæ•…äº‹</h2>\n                <button id=\"generateStory\" class=\"btn btn-primary mb-4\">ç”Ÿæˆæ•…äº‹</button>\n                <div id=\"storyContainer\" class=\"mb-4\">\n                    <p id=\"generatedStory\" class=\"text-lg\"></p>\n                </div>\n            </div>\n        </div>\n    </div>\n    <script src=\"{{ url_for('static', path='/js/script.js') }}\"></script>\n</body>\n</html>\n```\n\n## ç»“è®º\n\nAIå›¾åƒç”Ÿæˆå™¨å’Œæ•…äº‹åˆ›ä½œé¡¹ç›®æˆåŠŸæ•´åˆäº†å„ç§AIæŠ€æœ¯ï¼Œåˆ›å»ºäº†ä¸€ä¸ªäº’åŠ¨å¼Webåº”ç”¨ç¨‹åºï¼Œå…è®¸ç”¨æˆ·æ ¹æ®éŸ³é¢‘æç¤ºç”Ÿæˆå›¾åƒå’Œæ•…äº‹ã€‚é€šè¿‡åˆ©ç”¨FastAPIä½œä¸ºåŽç«¯å’ŒçŽ°ä»£å‰ç«¯æŠ€æœ¯ï¼Œè¯¥åº”ç”¨ç¨‹åºæä¾›äº†æ— ç¼çš„ç”¨æˆ·ä½“éªŒã€‚\n\n## å…³é”®è¦ç‚¹ï¼š\n\n1. AIæ¨¡åž‹çš„é›†æˆï¼šè¯¥é¡¹ç›®å±•ç¤ºäº†å¦‚ä½•é›†æˆå¤šä¸ªAIæ¨¡åž‹ï¼ŒåŒ…æ‹¬ç”¨äºŽæ–‡æœ¬ç”Ÿæˆçš„Groqå’Œç”¨äºŽå›¾åƒç”Ÿæˆçš„Replicateï¼Œä»¥åˆ›å»ºä¸€ä¸ªå¢žå¼ºç”¨æˆ·åˆ›æ„çš„ç»Ÿä¸€åº”ç”¨ç¨‹åºã€‚\n2. ç”¨æˆ·äº’åŠ¨ï¼šè¯¥åº”ç”¨ç¨‹åºå…è®¸ç”¨æˆ·é€šè¿‡è¯­éŸ³å‘½ä»¤è¿›è¡Œäº’åŠ¨ï¼Œä½¿å…¶æ˜“äºŽè®¿é—®ä¸”ç”¨æˆ·å‹å¥½ã€‚å½•éŸ³ã€è½¬å½•ä»¥åŠåŸºäºŽè¯¥è¾“å…¥ç”Ÿæˆå†…å®¹çš„èƒ½åŠ›å±•ç¤ºäº†è¯­éŸ³é©±åŠ¨åº”ç”¨ç¨‹åºçš„æ½œåŠ›ã€‚\n3. åŠ¨æ€å†…å®¹ç”Ÿæˆï¼šé€šè¿‡æ ¹æ®ç”¨æˆ·è¾“å…¥åŠ¨æ€ç”Ÿæˆå›¾åƒå’Œæ•…äº‹ï¼Œè¯¥åº”ç”¨ç¨‹åºçªæ˜¾äº†AIåœ¨å†…å®¹åˆ›ä½œä¸­çš„èƒ½åŠ›ï¼Œä¸ºç”¨æˆ·æä¾›ç‹¬ç‰¹ä¸”ä¸ªæ€§åŒ–çš„è¾“å‡ºã€‚\n4. å“åº”å¼è®¾è®¡ï¼šä½¿ç”¨DaisyUIå’ŒTailwind CSSç¡®ä¿åº”ç”¨ç¨‹åºåœ¨è§†è§‰ä¸Šå¸å¼•äººä¸”å“åº”è¿…é€Ÿï¼Œé€‚åº”å„ç§è®¾å¤‡ä¸Šçš„ç”¨æˆ·ã€‚\n5. æœªæ¥å¢žå¼ºï¼šè¯¥é¡¹ç›®å¯ä»¥é€šè¿‡åŠ å…¥é¢å¤–åŠŸèƒ½è¿›ä¸€æ­¥å¢žå¼ºï¼Œä¾‹å¦‚ç”¨æˆ·èº«ä»½éªŒè¯ã€ä¿å­˜ç”¨æˆ·ç”Ÿæˆçš„å†…å®¹ï¼Œä»¥åŠæ‰©å±•ç”¨äºŽä¸åŒåˆ›æ„ä»»åŠ¡çš„AIæ¨¡åž‹èŒƒå›´ã€‚\n\næ€»ä½“è€Œè¨€ï¼Œè¯¥é¡¹ç›®ä½œä¸ºä¸€ä¸ªç»¼åˆç¤ºä¾‹ï¼Œå±•ç¤ºäº†å¦‚ä½•æž„å»ºä¸€ä¸ªç»“åˆéŸ³é¢‘å¤„ç†ã€å›¾åƒç”Ÿæˆå’Œæ•…äº‹è®²è¿°çš„AIé©±åŠ¨çš„ç½‘ç»œåº”ç”¨ç¨‹åºï¼Œä¸ºåˆ›æ„é¢†åŸŸçš„åˆ›æ–°åº”ç”¨é“ºå¹³äº†é“è·¯ã€‚\n\n## å‚è€ƒæ–‡çŒ®\n\n* FastAPI æ–‡æ¡£: [FastAPI](https://fastapi.tiangolo.com/) æ˜¯ä¸€ä¸ªçŽ°ä»£çš„ web æ¡†æž¶ï¼Œç”¨äºŽä½¿ç”¨ Python æž„å»º APIã€‚å®ƒæ—¨åœ¨æ˜“äºŽä½¿ç”¨ä¸”å¿«é€Ÿã€‚\n* Pydantic: [Pydantic](https://pydantic-docs.helpmanual.io/) ç”¨äºŽæ•°æ®éªŒè¯å’Œä½¿ç”¨ Python ç±»åž‹æ³¨é‡Šçš„è®¾ç½®ç®¡ç†ã€‚\n* Groq: [Groq](https://groq.com/docs/) æ˜¯ä¸€ä¸ªæž„å»ºå’Œéƒ¨ç½² AI æ¨¡åž‹çš„å¹³å°ã€‚å®ƒæä¾›æ–‡æœ¬ç”Ÿæˆå’Œå…¶ä»– AI ä»»åŠ¡çš„ APIã€‚\n* Replicate: [Replicate](https://replicate.com/docs) æ˜¯ä¸€ä¸ªå…è®¸æ‚¨åœ¨äº‘ä¸­è¿è¡Œæœºå™¨å­¦ä¹ æ¨¡åž‹çš„å¹³å°ã€‚å®ƒæä¾›å„ç§æ¨¡åž‹çš„ APIï¼ŒåŒ…æ‹¬å›¾åƒç”Ÿæˆã€‚\n* SpeechRecognition: [SpeechRecognition](https://pypi.org/project/SpeechRecognition/) æ˜¯ä¸€ä¸ªæ‰§è¡Œè¯­éŸ³è¯†åˆ«çš„åº“ï¼Œæ”¯æŒå¤šç§å¼•æ“Žå’Œ APIã€‚\n* Pillow: [Pillow](https://pillow.readthedocs.io/en/stable/) æ˜¯ Python Imaging Library (PIL) çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä¸ºæ‚¨çš„ Python ä»£ç æ·»åŠ å›¾åƒå¤„ç†èƒ½åŠ›ã€‚\n* JavaScript Fetch API: [Fetch API](https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API) æä¾›äº†ä¸€ç§çŽ°ä»£æ–¹å¼åœ¨ JavaScript ä¸­å‘èµ·ç½‘ç»œè¯·æ±‚ã€‚\n* HTML5 éŸ³é¢‘ API: [HTML5 Audio API](https://developer.mozilla.org/en-US/docs/Web/API/HTMLAudioElement) å…è®¸æ‚¨åœ¨ web åº”ç”¨ç¨‹åºä¸­æ’­æ”¾éŸ³é¢‘æ–‡ä»¶ã€‚\n* DaisyUI: [DaisyUI](https://daisyui.com/) æ˜¯ä¸€ä¸ªä¸º Tailwind CSS æä¾›é¢„è®¾è®¡ç»„ä»¶çš„ç»„ä»¶åº“ã€‚\n* Tailwind CSS: [Tailwind CSS](https://tailwindcss.com/docs) æ˜¯ä¸€ä¸ªå®žç”¨ä¼˜å…ˆçš„ CSS æ¡†æž¶ï¼Œç”¨äºŽåˆ›å»ºè‡ªå®šä¹‰è®¾è®¡ï¼Œè€Œæ— éœ€ç¦»å¼€ HTMLã€‚\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/ai-is-helping-me-manage-my-pre-diabetes-a115c1f7ed7b","frontmatter":{"title":"äººå·¥æ™ºèƒ½å¸®åŠ©æˆ‘æŽ§åˆ¶ç³–å°¿ç—…å‰æœŸç—…æƒ…","meta_title":"äººå·¥æ™ºèƒ½å¸®åŠ©æˆ‘æŽ§åˆ¶ç³–å°¿ç—…å‰æœŸç—…æƒ…","description":"æ–‡ç« æŽ¢è®¨äº†ä½œè€…å¦‚ä½•åˆ©ç”¨äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ¥ç®¡ç†å‰æœŸç³–å°¿ç—…å’ŒæŽ§åˆ¶è¡€ç³–ã€‚ä½œè€…é€šè¿‡ç›‘æµ‹é¥®é£Ÿå’Œè¡€ç³–æ°´å¹³ï¼Œåˆ©ç”¨ChatGPTç”Ÿæˆæ•°æ®åˆ†æžå’Œé¥®é£Ÿå»ºè®®ï¼Œä»Žè€Œå®žçŽ°å‡è‚¥å’Œæé«˜èƒ°å²›ç´ æ•æ„Ÿæ€§ã€‚ä½œè€…å¼ºè°ƒäº†æŠ€æœ¯åœ¨ä¸ªäººå¥åº·ç®¡ç†ä¸­çš„æ½œåŠ›ï¼Œå¹¶åˆ†äº«äº†è‡ªå·±åœ¨ä½¿ç”¨AIè¿‡ç¨‹ä¸­å–å¾—çš„ç§¯æžæˆæžœï¼ŒåŒ…æ‹¬ä½“é‡å‡è½»å’Œè¡€ç³–æŽ§åˆ¶çš„æ”¹å–„ï¼ŒåŒæ—¶æé†’è¯»è€…åº”ä¸Žå¥åº·ä¸“ä¸šäººå£«è®¨è®ºç›¸å…³é—®é¢˜ã€‚","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*oTD6Y3PWBiteGxzYcDhP-w.jpeg","categories":["Health","Chatbots","Data Science"],"author":"Rifx.Online","tags":["pre-diabetes","ChatGPT","blood-sugar","carbohydrates","weight-loss"],"draft":false,"slug":"blog/ai-is-helping-me-manage-my-pre-diabetes-a115c1f7ed7b"},"content":"\n## å¥åº· \\| è¡€ç³– \\| ç³–å°¿ç—…\n\n\n\n## æˆ‘å¦‚ä½•åˆ©ç”¨æŠ€æœ¯æŽ§åˆ¶è¡€ç³–ã€å‡è‚¥å’Œä¿æŒå¥åº·\n\n\n\n*å…è´£å£°æ˜Žï¼šæˆ‘ä¸æ˜¯åŒ»ç”Ÿï¼Œæœ¬æ–‡çš„ä»»ä½•éƒ¨åˆ†éƒ½ä¸åº”è¢«è§†ä¸ºåŒ»ç–—å»ºè®®ã€‚æˆ‘åˆ†äº«çš„æ˜¯æˆ‘è‡ªå·±åœ¨ç®¡ç†å‡è‚¥å’Œé¿å…ç³–å°¿ç—…æ–¹é¢çš„æŽ¢ç´¢ã€‚*\n\n*æ‚¨æ‰€æœ‰çš„å¥åº·æŠ¤ç†é—®é¢˜å’ŒæŒ‘æˆ˜åº”ä¸Žæ‚¨çš„ä¸ªäººå¥åº·æŠ¤ç†ä¸“ä¸šäººå£«è®¨è®ºã€‚æœ¬æ–‡ä»…åº”è¢«è§†ä¸ºå¨±ä¹å†…å®¹ï¼Œ**ä¸**åº”ç”¨äºŽæ•™è‚²æˆ–åŒ»ç–—ã€‚*\n\n## çŽ°çŠ¶\n\nå¦‚æžœä½ ä¸€ç›´åœ¨å…³æ³¨æˆ‘ï¼Œä½ ä¼šçŸ¥é“æˆ‘å¤„äºŽç³–å°¿ç—…å‰æœŸï¼Œå¹¶ä¸”ä½“é‡è¶…æ ‡ã€‚æœ€è¿‘ï¼Œæˆ‘ä¸€ç›´åœ¨è¯•å›¾å¼„æ¸…æ¥šæˆ‘æ˜¯å¦‚ä½•èµ°åˆ°è¿™ä¸€æ­¥çš„ï¼Œä»¥åŠæˆ‘æŽ¥ä¸‹æ¥åº”è¯¥åšä»€ä¹ˆã€‚\n\næˆ‘å†³å®šç›¸ä¿¡æˆ‘æœ‰ä¸€ç§é—ä¼ å€¾å‘ï¼Œå¯¼è‡´ç³–å°¿ç—…çš„å‘ç”Ÿï¼Œè¿™æºäºŽæˆ‘ä½œä¸ºç»åŽ†è¿‡é¥¥è’çš„å—äºšäººåŽä»£çš„èƒŒæ™¯ã€‚æ›´æ·±å…¥çš„æŽ¢è®¨ [åœ¨è¿™é‡Œ](https://readmedium.com/i-finally-understand-why-im-pre-diabetic-and-obese-c9893f4c3187)ã€‚\n\nè¿™ç§å€¾å‘ä½¿å—äºšäººï¼ˆ[ä»¥åŠå…¶ä»–ç»åŽ†è¿‡é¥¥è’çš„äººç¾¤](https://diabetesjournals.org/diabetes/article/61/9/2255/14753/Famine-Exposure-in-the-Young-and-the-Risk-of-Type)ï¼‰æ›´å®¹æ˜“å‘å±•ä¸ºç³–å°¿ç—…ã€‚ä¼¼ä¹Žæˆ‘ä»¬çš„ç¥–å…ˆä¸ºäº†åœ¨é•¿æœŸçš„é£Ÿç‰©çŸ­ç¼ºä¸­ç”Ÿå­˜ï¼Œåè€Œé€šè¿‡*æŠ‘åˆ¶*èƒ°å²›ç´ çš„äº§ç”Ÿæ¥è¿›åŒ–ã€‚\n\nèƒ°å²›ç´ äº§ç”Ÿçš„å—æŸä½¿å¾—è¡€ç³–æ°´å¹³è¿œè¿œè¶…å‡ºæ­£å¸¸èŒƒå›´ï¼Œè™½ç„¶è¿™å¯ä»¥å¢žåŠ ä»¥è„‚è‚ªå½¢å¼å‚¨å­˜çš„é£Ÿç‰©ï¼Œä½†ä¹Ÿä½¿æˆ‘ä»¬æ›´å®¹æ˜“æ‚£ä¸Šç³–å°¿ç—…ã€‚\n\nè¿™å¯¹æˆ‘çš„ç¥–å…ˆç”Ÿå­˜æ˜¯æœ‰å¸®åŠ©çš„ï¼Œä½†åœ¨é¢ä¸´é£Ÿç‰©ä¸°ç›ˆçš„æ—¶æœŸæ—¶ï¼Œè¿™ç§ç”Ÿå­˜æœºåˆ¶å´å‡ºçŽ°äº†é—®é¢˜ã€‚æ— è®ºæ˜¯å¦è‚¥èƒ–ï¼Œæˆ‘ä»¬å‘å±•ç³–å°¿ç—…çš„å‡ çŽ‡è¿œé«˜äºŽå‡ ä¹Žæ‰€æœ‰å…¶ä»–äººç¾¤ã€‚\n\nè€Œä¸”ï¼Œåƒå¤§å¤šæ•°å¤„äºŽç³–å°¿ç—…å‰æœŸæˆ–æ‚£æœ‰2åž‹ç³–å°¿ç—…çš„æ‚£è€…ä¸€æ ·ï¼Œæˆ‘ä»¬çš„é¥®é£Ÿä¸­ä¹Ÿä¼šæ‘„å…¥è¿‡å¤šçš„ç³–åˆ†ï¼Œå¯èƒ½è¿˜æœ‰è›‹ç™½è´¨å’Œè„‚è‚ªã€‚\n\nå› æ­¤ï¼Œæˆ‘çš„å‡è®¾æ˜¯â€”â€”ä¸ºäº†é€†è½¬ç³–å°¿ç—…å‰æœŸï¼Œæˆ‘éœ€è¦åœ¨è‡ªå·±çš„ç”Ÿæ´»ä¸­é‡æ–°åˆ›é€ äººå·¥é¥¥è’çš„æ¡ä»¶ã€‚\n\næˆ‘å°†ä¸å¾—ä¸å‡å°‘ã€åˆ‡é™¤å¹¶æ”¾å¼ƒé¥®é£Ÿä¸­å¤šä½™çš„ç³–åˆ†/ç¢³æ°´åŒ–åˆç‰©ã€è›‹ç™½è´¨å’Œè„‚è‚ªï¼Œå¹¶åŠ å…¥é€‚åº¦çš„é”»ç‚¼ï¼Œä»¥ä¿ƒè¿›å‡è‚¥ã€‚\n\n## è¿›å…¥äººå·¥æ™ºèƒ½\n\næˆ‘æ¯å¤©æ—©ä¸Šåœ¨ç¦é£Ÿ16-18å°æ—¶åŽè®¤çœŸç›‘æµ‹è¡€ç³–ã€‚è¿‡åŽ»ä¸€å‘¨ï¼Œæˆ‘è§‚å¯Ÿåˆ°è¡€ç³–æ°´å¹³å‘ˆä¸‹é™è¶‹åŠ¿ï¼Œä»Ž123 mg/dLé™è‡³100 mg/dLä»¥ä¸‹ã€‚è¿™ä¸€ç›´æ˜¯æˆ‘çš„ç›®æ ‡ã€‚\n\næˆ‘æ‰€åšçš„å°±æ˜¯å°†æ—¥æœŸå’Œè¯»æ•°è¾“å…¥ChatGPTï¼Œå¹¶è¦æ±‚å®ƒç”Ÿæˆä¸€ä¸ªè¡¨æ ¼ã€‚ä»¥ä¸‹æ˜¯ç»“æžœï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*UenRzbRpeCFUNFbVwtrxkg.png)\n\næˆ‘ä½¿ç”¨ChatGPTä»”ç»†è·Ÿè¸ªäº†æˆ‘çš„æ¯æ—¥ç¢³æ°´åŒ–åˆç‰©æ‘„å…¥é‡ã€‚åŒæ ·ï¼Œæˆ‘å‡†ç¡®åœ°è¾“å…¥äº†æˆ‘åƒäº†ä»€ä¹ˆï¼Œå¹¶è¦æ±‚å®ƒè®¡ç®—æˆ‘æ‘„å…¥çš„ç¢³æ°´åŒ–åˆç‰©æ•°é‡ï¼Œå¹¶ä»¥è¡¨æ ¼çš„å½¢å¼ç»™å‡ºç»“æžœã€‚\n\nè¯·æ³¨æ„ï¼Œæˆ‘åªè·Ÿè¸ªäº†**æˆ‘**åƒäº†ä»€ä¹ˆå’Œå¤šå°‘ã€‚äººå·¥æ™ºèƒ½åšäº†å…¶ä½™çš„å·¥ä½œã€‚\n\nä»¥ä¸‹æ˜¯æˆ‘æ˜¨å¤©çœ‹åˆ°çš„ç»“æžœï¼Œè¿™æ˜¯æˆ‘*ç¬¬ä¸€æ¬¡* fasting è¡€ç³–ä½ŽäºŽæˆ‘çš„ç›®æ ‡100 mg/dLï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*4YwDOMk2V7Leo5Wl4_5NGg.png)\n\næˆ‘æƒ³çŸ¥é“æˆ‘æ‘„å…¥äº†å¤šå°‘å¡è·¯é‡Œï¼Œæ‰€ä»¥æˆ‘ç®€å•åœ°è¦æ±‚å®ƒæ ¹æ®æˆ‘å·²ç»æä¾›çš„æ•°æ®è®¡ç®—æˆ‘åƒäº†å¤šå°‘å¡è·¯é‡Œã€‚å®ƒå¾—å‡ºäº†ä»¥ä¸‹è¡¨æ ¼ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*AFAm5-SStY04-mQiCShRPg.png)\n\næˆ‘å¿…é¡»è¯´â€”â€”è¿™å¤ªæ£’äº†ã€‚æˆ‘ä¸å†éœ€è¦æ‰‹åŠ¨è·Ÿè¸ªæ¯ä¸€ä¸ªç»†èŠ‚ï¼Œåªéœ€è¾“å…¥ä¸€æ¬¡æ•°æ®ï¼Œå°±å¯ä»¥ä»¥ä¸åŒçš„æ–¹å¼è¿›è¡Œå¤„ç†ã€‚\n\næˆ‘è®¡åˆ’åœ¨æœªæ¥æ¯å¤©åœ¨ChatGPTä¸­è¾“å…¥æˆ‘çš„é¤é£Ÿå’Œè¡€ç³–ï¼Œå¹¶ç”Ÿæˆæ¯æ—¥æŠ¥å‘Šï¼Œä»¥äº†è§£æˆ‘çš„é¥®é£Ÿå¦‚ä½•å½±å“æˆ‘çš„è¡€ç³–ï¼Œç‰¹åˆ«å…³æ³¨ç¢³æ°´åŒ–åˆç‰©æ‘„å…¥é‡å’ŒéšåŽçš„ fasting è¡€ç³–ã€‚\n\næ ¹æ®è¿™äº›æ•°æ®ï¼Œæˆ‘å¯ä»¥æ‰¾å‡ºæ˜¯ä»€ä¹ˆå¯¼è‡´æˆ‘çš„è¡€ç³–è¶…è¿‡100 mg/dLçš„ç›®æ ‡è¯»æ•°ï¼Œä»¥åŠæ˜¯ä»€ä¹ˆè®©å®ƒä¿æŒåœ¨èŒƒå›´å†…ã€‚è¿™æ˜¯é’ˆå¯¹æˆ‘éœ€è¦ç®¡ç†çš„æƒ…å†µçš„æœ‰é’ˆå¯¹æ€§ã€ä¸ªæ€§åŒ–çš„å¸®åŠ©ã€‚å¯¹æˆ‘æ¥è¯´ï¼Œè¿™æ•ˆæžœéžå¸¸å¥½ã€‚\n\n## è¡¨æ ¼å¹¶ä¸æ˜¯ä¸€åˆ‡\n\nç„¶åŽæˆ‘é—®æˆ‘çš„äººå·¥æ™ºèƒ½ä¼´ä¾£è¿™ä¸ªé—®é¢˜ï¼š*ä½ è®¤ä¸ºæˆ‘ä»Šå¤©çš„é¥®é£Ÿé€‚åˆæ­£åœ¨å°è¯•å‡è‚¥å’Œæé«˜èƒ°å²›ç´ æ•æ„Ÿæ€§çš„å‰ç³–å°¿ç—…æ‚£è€…å—ï¼Ÿ*\n\nå®ƒè¿”å›žçš„ç­”æ¡ˆåŸºäºŽæˆ‘è¾“å…¥çš„é£Ÿç‰©æ•°æ®ã€‚è¿™è®©æˆ‘æ„Ÿåˆ°éœ‡æƒŠã€‚å®ƒåˆ†æžäº†æˆ‘å…·ä½“çš„é£Ÿç‰©é€‰æ‹©ï¼Œå¹¶è¿”å›žäº†å¤šä¸ªç§¯æžçš„æ–¹é¢ï¼Œä¾‹å¦‚ï¼š\n\n> **ä½Žç¢³æ°´åŒ–åˆç‰©é€‰æ‹©**ï¼šä½ ä¿æŒäº†ç›¸å¯¹è¾ƒä½Žçš„ç¢³æ°´åŒ–åˆç‰©æ‘„å…¥ï¼Œè¿™å¯¹äºŽå‰ç³–å°¿ç—…çš„è¡€ç³–æŽ§åˆ¶éžå¸¸é‡è¦ã€‚ä½ çš„æ€»ç¢³æ°´åŒ–åˆç‰©æ‘„å…¥é‡çº¦ä¸º **28.3â€“29.3 å…‹**ï¼Œè¿™åœ¨é€‚åº¦ä½Žç¢³æ°´åŒ–åˆç‰©èŒƒå›´å†…ã€‚\n\næŽ¥ç€å®ƒå»ºè®®äº†ä¸€äº›æ”¹è¿›çš„é¢†åŸŸï¼ŒåŒ…æ‹¬ï¼š\n\n> **æ›´å¤šå…³æ³¨å‡è¡¡é¥®é£Ÿ**ï¼šè™½ç„¶ä½ çš„é¥®é£Ÿæ€»ä½“ä¸Šæ˜¯å¥åº·çš„ï¼Œä½†ä½ å¯èƒ½æƒ³è¦å¢žåŠ æ›´å¤šçš„å¤šæ ·æ€§å’Œå‡è¡¡ã€‚ä¾‹å¦‚ï¼ŒåŠ å…¥é€‚é‡çš„ä½ŽGIæ°´æžœï¼ˆå¦‚ä½ æ‰€åƒçš„æµ†æžœï¼‰ã€å…¨è°·ç‰©ï¼ˆå¦‚æžœå¯ä»¥è€å—ï¼‰æˆ–è±†ç±»ï¼Œå¯èƒ½ä¼šéšç€æ—¶é—´çš„æŽ¨ç§»æ”¹å–„èƒ°å²›ç´ æ•æ„Ÿæ€§ã€‚\n\næœ€åŽï¼Œå®ƒç»™äº†æˆ‘ä¸€äº›æœªæ¥å¯ä»¥é‡‡å–çš„å»ºè®®ï¼š\n\n> ç”¨ä½Žç¢³æ°´åŒ–åˆç‰©æ›¿ä»£å“æ›¿æ¢å…¨è„‚ç‰›å¥¶ã€‚\n\n> å¢žåŠ æ›´å¤šéžæ·€ç²‰ç±»è”¬èœï¼Œä»¥å¢žåŠ çº¤ç»´å’Œä½“ç§¯ã€‚\n\n> ç»§ç»­å…³æ³¨å¥åº·è„‚è‚ªå’Œç˜¦è›‹ç™½ï¼Œä½†è¦æ³¨æ„æ•´ä½“çƒ­é‡æ‘„å…¥ï¼Œä»¥ç¬¦åˆä½ çš„å‡è‚¥ç›®æ ‡ã€‚\n\n> ä½ çš„é¥®é£Ÿé€‰æ‹©åœ¨ç®¡ç†è¡€ç³–å’Œä¿ƒè¿›èƒ°å²›ç´ æ•æ„Ÿæ€§æ–¹é¢åŸºæœ¬é€‚å®œï¼Œä½†ç¨å¾®è°ƒæ•´å¯èƒ½ä¼šè¿›ä¸€æ­¥æ”¯æŒä½ çš„å‡è‚¥åŠªåŠ›ã€‚\n\n## ä¸è¦ç›¸ä¿¡æœºå™¨\n\nå¥½çš„ï¼Œæˆ‘çŸ¥é“è¿™æ˜¯ä¸€å°æœºå™¨ã€‚æˆ‘çŸ¥é“æˆ‘ä¸èƒ½ä¾èµ–å®ƒçš„å»ºè®®ï¼Œ**ä½ ä¹Ÿä¸åº”è¯¥ï¼Œäº²çˆ±çš„è¯»è€…**ã€‚ä½†æ˜¯ï¼Œå¤©å“ªï¼Œå®ƒç»™äº†æˆ‘è¶³å¤Ÿçš„ä¿¡æ¯ï¼Œä»¥ä¾¿æˆ‘ä¸‹æ¬¡åŽ»çœ‹åŒ»ç”Ÿæ—¶å¯ä»¥ä½¿ç”¨ã€‚\n\nä¸è¿‡ï¼Œæˆ‘ä¼šå®Œå…¨åˆ‡æŽ‰å…¨è„‚ç‰›å¥¶ã€‚è¿™æ˜¯æ˜Žæ™ºçš„å»ºè®®ã€‚\n\næˆ‘è®¡åˆ’ç”Ÿæˆä¸€ä¸ªè¡€ç³–ä¸Žæ—¥æœŸçš„è¡¨æ ¼ï¼Œå¹¶å¸¦ç»™æˆ‘çš„åŒ»ç”Ÿï¼Œå¦‚æžœå¥¹è¯¢é—®æˆ‘çš„é¥®é£Ÿï¼Œæˆ‘å¯ä»¥é€šè¿‡ä¸€ä¸ªé“¾æŽ¥ä¸Žå¥¹åˆ†äº«æ•´ä¸ªGPTèŠå¤©è®°å½•ã€‚è¿™æ ·å¥¹å°±å¯ä»¥ç¡®åˆ‡åœ°çœ‹åˆ°æˆ‘ä¸€ç›´åœ¨åšä»€ä¹ˆâ€”â€”æ— è®ºæ˜¯å¯¹æ˜¯é”™ï¼Œç„¶åŽæ ¹æ®éœ€è¦è¿›è¡Œè°ƒæ•´ã€‚\n\nè¿™éš¾é“ä¸æ˜¯å¤ªé…·äº†å—ï¼Ÿå¾®ç¬‘ã€‚\n\n## ç»“è®ºï¼š\n\næˆ‘å¯¹æ˜Žæ™ºåœ°ä½¿ç”¨äººå·¥æ™ºèƒ½å……æ»¡çƒ­æƒ…ã€‚è¿™æ¬¡ä¸Žäººå·¥æ™ºèƒ½çš„å¯¹è¯è®©æˆ‘æ„è¯†åˆ°å¯ä»¥åˆ©ç”¨äººå·¥æ™ºèƒ½æ¥ç®¡ç†æˆ‘ä¸ªäººç”Ÿæ´»ä¸­å¤§é‡é‡è¦æ•°æ®çš„å¯èƒ½æ€§ã€‚æ­£æ˜¯å› ä¸ºæˆ‘åœ¨ä½¿ç”¨äººå·¥æ™ºèƒ½ç›‘æµ‹æˆ‘çš„é¥®é£Ÿå’Œè¡€ç³–ï¼Œæˆ‘æ‰èƒ½æ›´å¥½åœ°æŽ§åˆ¶æˆ‘çš„ç—…æƒ…ã€‚\n\næˆ‘èƒ½å¦ç”¨é“…ç¬”å’Œè®°äº‹æœ¬åšåˆ°è¿™ä¸€ç‚¹ï¼Ÿå¯ä»¥ã€‚\n\nä½†è¿™å°†éœ€è¦æ›´å¤šçš„æ—¶é—´å’Œç²¾åŠ›ï¼Œå¹¶ä¸”ä¼šäº§ç”Ÿæ›´å¤šçš„æ‘©æ“¦ã€‚åŒæ—¶ï¼Œæˆ‘ä¹Ÿæ— æ³•åƒçŽ°åœ¨è¿™æ ·è½»æ¾åœ°ä¸Žæ‚¨åˆ†äº«æˆ‘çš„é¥®é£Ÿå’Œå‰ç³–å°¿ç—…ç®¡ç†çš„ç»†èŠ‚ã€‚\n\nè¿˜æœ‰ä¸€ä»¶äº‹â€”â€”æˆ‘æ²¡æœ‰æåˆ°æˆ‘çš„å‡è‚¥ã€‚å½“æˆ‘ä¸¤å‘¨å‰å¼€å§‹æµ‹é‡ä½“é‡æ—¶ï¼Œæˆ‘çš„ä½“é‡æ˜¯225ç£…ã€‚ä»Šå¤©æˆ‘ç§°é‡æ—¶æ˜¯219ç£…ã€‚**å‡æŽ‰å…­ç£…ã€‚**\n\næˆ‘åªèƒ½è¯´ï¼Œè¿™å¯¹æˆ‘æœ‰æ•ˆã€‚è€ƒè™‘ä¸€ä¸‹å¦‚ä½•å°†äººå·¥æ™ºèƒ½èžå…¥åˆ°æ‚¨çš„ç”Ÿæ´»ä¸­ã€‚è¿™å¯¹æŸäº›äººæ¥è¯´å¯èƒ½ä¼šæ„Ÿåˆ°å®³æ€•ã€‚æˆ‘å·²ç»é€€ä¼‘äº†ï¼Œä½†æˆ‘æ‰¾åˆ°äº†æ–¹æ³•ã€‚æ‚¨ä¹Ÿå¯ä»¥ã€‚è¿™å¾ˆç®€å•ã€‚å¤§å¤šæ•°æƒ…å†µä¸‹æ˜¯å…è´¹çš„ï¼Œè€Œä¸”éžå¸¸æœ‰ç”¨ã€‚\n\næ‚¨è®¤ä¸ºå¯ä»¥ä½¿ç”¨äººå·¥æ™ºèƒ½æ¥å¸®åŠ©ç®¡ç†æ‚¨çš„æ…¢æ€§å¥åº·çŠ¶å†µå—ï¼Ÿæˆ‘å¾ˆæƒ³çŸ¥é“ã€‚å¦‚æžœæ‚¨æ„¿æ„å°è¯•ï¼Œæ‚¨é¢ä¸´å“ªäº›æŒ‘æˆ˜ï¼Ÿä¹Ÿè®¸æˆ‘ä»¬å¯ä»¥ä¸€èµ·å…‹æœå®ƒä»¬ã€‚\n\nåœ¨æ­¤æœŸé—´ï¼Œç¥å¥½ã€‚ç±³åˆ‡ã€‚\n\n"},{"lang":"zh","group":"blog","slug":"blog/ai-powered-ocr-with-phi-3-vision-128k-the-future-of-document-processing-7be80c46bd16","frontmatter":{"title":"é‡‡ç”¨ Phi-3-Vision-128K çš„äººå·¥æ™ºèƒ½ OCRï¼šæ–‡æ¡£å¤„ç†çš„æœªæ¥","meta_title":"é‡‡ç”¨ Phi-3-Vision-128K çš„äººå·¥æ™ºèƒ½ OCRï¼šæ–‡æ¡£å¤„ç†çš„æœªæ¥","description":"åœ¨å¿«é€Ÿå‘å±•çš„äººå·¥æ™ºèƒ½ä¸–ç•Œä¸­ï¼Œå¤šæ¨¡å¼æ¨¡åž‹æ­£åœ¨ä¸ºæ•´åˆè§†è§‰å’Œæ–‡æœ¬æ•°æ®è®¾å®šæ–°çš„æ ‡å‡†â€¦â€¦","date":"2024-11-08T00:26:30.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BR-H6cQoyoRo6gVRqjvAyA.png","categories":["Natural Language Processing","Computer Vision","Data Science"],"author":"Rifx.Online","tags":["OCR","tokens","encoder","language","document"],"draft":false,"slug":"blog/ai-powered-ocr-with-phi-3-vision-128k-the-future-of-document-processing-7be80c46bd16"},"content":"\n\n\n\n\nåœ¨å¿«é€Ÿå‘å±•çš„äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œå¤šæ¨¡æ€æ¨¡åž‹æ­£åœ¨ä¸ºè§†è§‰å’Œæ–‡æœ¬æ•°æ®çš„æ•´åˆè®¾å®šæ–°æ ‡å‡†ã€‚æœ€æ–°çš„çªç ´ä¹‹ä¸€æ˜¯ **Phi\\-3\\-Vision\\-128K\\-Instruct**ï¼Œè¿™æ˜¯ä¸€ä¸ªæœ€å…ˆè¿›çš„å¼€æ”¾å¤šæ¨¡æ€æ¨¡åž‹ï¼ŒæŽ¨åŠ¨äº†AIåœ¨å¤„ç†å›¾åƒå’Œæ–‡æœ¬æ–¹é¢çš„èƒ½åŠ›è¾¹ç•Œã€‚è¯¥æ¨¡åž‹ä¸“æ³¨äºŽæ–‡æ¡£æå–ã€å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰å’Œä¸€èˆ¬å›¾åƒç†è§£ï¼Œèƒ½å¤Ÿå½»åº•æ”¹å˜æˆ‘ä»¬å¤„ç†PDFã€å›¾è¡¨ã€è¡¨æ ¼ä»¥åŠå…¶ä»–ç»“æž„åŒ–æˆ–åŠç»“æž„åŒ–æ–‡æ¡£çš„ä¿¡æ¯æ–¹å¼ã€‚\n\nè®©æˆ‘ä»¬æ·±å…¥æŽ¢è®¨Phi\\-3\\-Vision\\-128K\\-Instructçš„ç»†èŠ‚ï¼ŒæŽ¢ç´¢å…¶æž¶æž„ã€æŠ€æœ¯è¦æ±‚ã€è´Ÿè´£ä»»çš„ä½¿ç”¨è€ƒè™‘ï¼Œå¹¶äº†è§£å®ƒå¦‚ä½•ç®€åŒ–æ–‡æ¡£æå–ã€PDFè§£æžå’ŒAIé©±åŠ¨çš„æ•°æ®åˆ†æžç­‰å¤æ‚ä»»åŠ¡ã€‚\n\n## ä»€ä¹ˆæ˜¯ Phi\\-3\\-Vision\\-128K\\-Instructï¼Ÿ\n\nPhi\\-3\\-Vision\\-128K\\-Instruct å±žäºŽ Phi\\-3 æ¨¡åž‹ç³»åˆ—ï¼Œä¸“ä¸ºå¤šæ¨¡æ€æ•°æ®å¤„ç†è€Œæž„å»ºï¼Œæ”¯æŒæœ€é•¿ **128,000 ä¸ªä»¤ç‰Œ** çš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚è¯¥æ¨¡åž‹ç»“åˆäº†æ–‡æœ¬å’Œè§†è§‰æ•°æ®ï¼Œé€‚åˆéœ€è¦åŒæ—¶è§£é‡Šæ–‡æœ¬å’Œå›¾åƒçš„ä»»åŠ¡ã€‚å…¶å¼€å‘æ¶‰åŠ **5000 äº¿ä¸ªè®­ç»ƒä»¤ç‰Œ**ï¼Œç»“åˆäº†é«˜è´¨é‡çš„åˆæˆæ•°æ®å’Œä¸¥æ ¼ç­›é€‰çš„å…¬å¼€å¯ç”¨æ¥æºã€‚é€šè¿‡åŒ…æ‹¬ **ç›‘ç£å¾®è°ƒå’Œåå¥½ä¼˜åŒ–** çš„ç²¾ç»†è®­ç»ƒè¿‡ç¨‹ï¼Œè¯¥æ¨¡åž‹æ—¨åœ¨æä¾›ç²¾ç¡®ã€å¯é å’Œå®‰å…¨çš„ AI è§£å†³æ–¹æ¡ˆã€‚\n\nPhi\\-3\\-Vision\\-128K\\-Instruct æ‹¥æœ‰ **42 äº¿ä¸ªå‚æ•°**ï¼Œå…¶æž¶æž„åŒ…æ‹¬å›¾åƒç¼–ç å™¨ã€è¿žæŽ¥å™¨ã€æŠ•å½±å™¨å’Œ Phi\\-3 Mini è¯­è¨€æ¨¡åž‹ï¼Œä½¿å…¶æˆä¸ºå¹¿æ³›åº”ç”¨çš„è½»é‡çº§è€Œå¼ºå¤§çš„é€‰æ‹©ã€‚\n\n## æ ¸å¿ƒç”¨ä¾‹\n\nè¯¥æ¨¡åž‹çš„ä¸»è¦åº”ç”¨è·¨è¶Šå¤šä¸ªé¢†åŸŸï¼Œç‰¹åˆ«å…³æ³¨äºŽï¼š\n\n* **æ–‡æ¡£æå–å’ŒOCRï¼š** é«˜æ•ˆåœ°å°†æ–‡æœ¬å›¾åƒæˆ–æ‰«ææ–‡æ¡£è½¬æ¢ä¸ºå¯ç¼–è¾‘æ ¼å¼ã€‚å®ƒå¯ä»¥å¤„ç†å¤æ‚çš„å¸ƒå±€ï¼Œå¦‚è¡¨æ ¼ã€å›¾è¡¨å’Œå›¾ç¤ºï¼Œä½¿å…¶æˆä¸ºæ•°å­—åŒ–å®žä½“æ–‡æ¡£æˆ–è‡ªåŠ¨åŒ–æ•°æ®æå–å·¥ä½œæµçš„å®è´µå·¥å…·ã€‚\n* **ä¸€èˆ¬å›¾åƒç†è§£ï¼š** è§£æžè§†è§‰å†…å®¹ä»¥è¯†åˆ«å¯¹è±¡ã€è§£é‡Šåœºæ™¯å¹¶æå–ç›¸å…³ä¿¡æ¯ã€‚\n* **å†…å­˜/è®¡ç®—å—é™çŽ¯å¢ƒï¼š** åœ¨è®¡ç®—èƒ½åŠ›æˆ–å†…å­˜æœ‰é™çš„æƒ…å†µä¸‹è¿è¡ŒAIä»»åŠ¡ï¼Œè€Œä¸å½±å“æ€§èƒ½ã€‚\n* **å»¶è¿Ÿå—é™åœºæ™¯ï¼š** åœ¨å®žæ—¶åº”ç”¨ä¸­å‡å°‘å¤„ç†å»¶è¿Ÿï¼Œä¾‹å¦‚å®žæ—¶æ•°æ®æµã€åŸºäºŽèŠå¤©çš„åŠ©æ‰‹æˆ–æµåª’ä½“å†…å®¹åˆ†æžã€‚\n\n## å¦‚ä½•å¼€å§‹ä½¿ç”¨ Phi\\-3\\-Vision\\-128K\\-Instruct\n\nè¦ä½¿ç”¨ Phi\\-3\\-Vision\\-128K\\-Instructï¼Œæ‚¨éœ€è¦è®¾ç½®å¼€å‘çŽ¯å¢ƒï¼Œå®‰è£…æ‰€éœ€çš„åº“å’Œå·¥å…·ã€‚è¯¥æ¨¡åž‹é›†æˆåœ¨ Hugging Face `transformers` åº“çš„å¼€å‘ç‰ˆæœ¬ (4\\.40\\.2\\) ä¸­ã€‚åœ¨æ·±å…¥ä»£ç ç¤ºä¾‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨çš„ Python çŽ¯å¢ƒå·²é…ç½®è¿™äº›åŒ…ï¼š\n\n```python\n## Required Packages\nflash_attn==2.5.8\nnumpy==1.24.4\nPillow==10.3.0\nRequests==2.31.0\ntorch==2.3.0\ntorchvision==0.18.0\ntransformers==4.40.2\n```\nè¦åŠ è½½æ¨¡åž‹ï¼Œæ‚¨å¯ä»¥æ›´æ–°æœ¬åœ°çš„ `transformers` åº“ï¼Œæˆ–è€…ç›´æŽ¥ä»Žæºä»£ç å…‹éš†å¹¶å®‰è£…ï¼š\n\n```python\npip uninstall -y transformers && pip install git+https://github.com/huggingface/transformers\n```\nçŽ°åœ¨ï¼Œè®©æˆ‘ä»¬è¿›å…¥ä¸€äº›å®žé™…çš„ä»£ç ç‰‡æ®µï¼Œå±•ç¤ºå¦‚ä½•åˆ©ç”¨è¿™ä¸ªå¼ºå¤§çš„æ¨¡åž‹è¿›è¡Œ AI é©±åŠ¨çš„æ–‡æ¡£æå–å’Œæ–‡æœ¬ç”Ÿæˆã€‚\n\n## åŠ è½½æ¨¡åž‹çš„ç¤ºä¾‹ä»£ç \n\nè¿™é‡Œæœ‰ä¸€ä¸ª Python ç¤ºä¾‹ï¼Œå±•ç¤ºå¦‚ä½•åˆå§‹åŒ–æ¨¡åž‹å¹¶å¼€å§‹è¿›è¡ŒæŽ¨æ–­ã€‚æˆ‘ä»¬å°†åˆ©ç”¨ç±»å’Œå‡½æ•°ä½¿ä»£ç ä¿æŒæ•´æ´å’Œæœ‰åºï¼š\n\n```python\nfrom PIL import Image\nimport requests\nfrom transformers import AutoModelForCausalLM, AutoProcessor\n\nclass Phi3VisionModel:\n    def __init__(self, model_id=\"microsoft/Phi-3-vision-128k-instruct\", device=\"cuda\"):\n        \"\"\"\n        ä½¿ç”¨æŒ‡å®šçš„æ¨¡åž‹ ID å’Œè®¾å¤‡åˆå§‹åŒ– Phi3VisionModelã€‚\n        \n        å‚æ•°ï¼š\n            model_id (str): æ¥è‡ª Hugging Face æ¨¡åž‹åº“çš„é¢„è®­ç»ƒæ¨¡åž‹æ ‡è¯†ç¬¦ã€‚\n            device (str): åŠ è½½æ¨¡åž‹çš„è®¾å¤‡ï¼ˆ\"cuda\" è¡¨ç¤º GPUï¼Œæˆ– \"cpu\"ï¼‰ã€‚\n        \"\"\"\n        self.model_id = model_id\n        self.device = device\n        self.model = self.load_model()  # åœ¨åˆå§‹åŒ–æ—¶åŠ è½½æ¨¡åž‹\n        self.processor = self.load_processor()  # åœ¨åˆå§‹åŒ–æ—¶åŠ è½½å¤„ç†å™¨\n    \n    def load_model(self):\n        \"\"\"\n        åŠ è½½å…·æœ‰å› æžœè¯­è¨€å»ºæ¨¡èƒ½åŠ›çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡åž‹ã€‚\n        \n        è¿”å›žï¼š\n            model (AutoModelForCausalLM): åŠ è½½çš„æ¨¡åž‹ã€‚\n        \"\"\"\n        print(\"åŠ è½½æ¨¡åž‹ä¸­...\")\n        # ä½¿ç”¨è‡ªåŠ¨è®¾å¤‡æ˜ å°„å’Œæ•°æ®ç±»åž‹è°ƒæ•´åŠ è½½æ¨¡åž‹\n        return AutoModelForCausalLM.from_pretrained(\n            self.model_id, \n            device_map=\"auto\",  # è‡ªåŠ¨å°†æ¨¡åž‹æ˜ å°„åˆ°é€‚å½“çš„è®¾å¤‡\n            torch_dtype=\"auto\",  # æ ¹æ®è®¾å¤‡ä½¿ç”¨åˆé€‚çš„ torch æ•°æ®ç±»åž‹\n            trust_remote_code=True,  # å…è®¸æ‰§è¡Œè‡ªå®šä¹‰ä»£ç ä»¥åŠ è½½æ¨¡åž‹\n            _attn_implementation='flash_attention_2'  # ä½¿ç”¨ä¼˜åŒ–çš„æ³¨æ„åŠ›å®žçŽ°\n        ).to(self.device)  # å°†æ¨¡åž‹ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡\n    \n    def load_processor(self):\n        \"\"\"\n        åŠ è½½ä¸Žæ¨¡åž‹å…³è”çš„å¤„ç†å™¨ï¼Œä»¥å¤„ç†è¾“å…¥å’Œè¾“å‡ºã€‚\n        \n        è¿”å›žï¼š\n            processor (AutoProcessor): ç”¨äºŽå¤„ç†æ–‡æœ¬å’Œå›¾åƒçš„åŠ è½½å¤„ç†å™¨ã€‚\n        \"\"\"\n        print(\"åŠ è½½å¤„ç†å™¨ä¸­...\")\n        # ä½¿ç”¨ trust_remote_code=True åŠ è½½å¤„ç†å™¨ï¼Œä»¥å¤„ç†ä»»ä½•è‡ªå®šä¹‰å¤„ç†é€»è¾‘\n        return AutoProcessor.from_pretrained(self.model_id, trust_remote_code=True)\n    \n    def predict(self, image_url, prompt):\n        \"\"\"\n        ä½¿ç”¨æ¨¡åž‹æ ¹æ®ç»™å®šçš„å›¾åƒå’Œæç¤ºè¿›è¡Œé¢„æµ‹ã€‚\n        \n        å‚æ•°ï¼š\n            image_url (str): è¦å¤„ç†çš„å›¾åƒçš„ URLã€‚\n            prompt (str): æŒ‡å¯¼æ¨¡åž‹ç”Ÿæˆçš„æ–‡æœ¬æç¤ºã€‚\n        \n        è¿”å›žï¼š\n            response (str): æ¨¡åž‹ç”Ÿæˆçš„å“åº”ã€‚\n        \"\"\"\n        # ä»Žæä¾›çš„ URL åŠ è½½å›¾åƒ\n        image = Image.open(requests.get(image_url, stream=True).raw)\n        \n        # ä¸ºæ¨¡åž‹æ ¼å¼åŒ–è¾“å…¥æç¤ºæ¨¡æ¿\n        prompt_template = f\"<|user|>\\n<|image_1|>\\n{prompt}<|end|>\\n<|assistant|>\\n\"\n        \n        # å¤„ç†è¾“å…¥ï¼Œå°†æç¤ºå’Œå›¾åƒè½¬æ¢ä¸ºå¼ é‡æ ¼å¼\n        inputs = self.processor(prompt_template, [image], return_tensors=\"pt\").to(self.device)\n        \n        # è®¾ç½®æ¨¡åž‹å“åº”ç”Ÿæˆçš„å‚æ•°\n        generation_args = {\n            \"max_new_tokens\": 500,  # æœ€å¤§ç”Ÿæˆçš„ä»¤ç‰Œæ•°\n            \"temperature\": 0.7,     # ç”Ÿæˆä¸­çš„é‡‡æ ·æ¸©åº¦ä»¥å¢žåŠ å¤šæ ·æ€§\n            \"do_sample\": False      # ç¦ç”¨é‡‡æ ·ä»¥èŽ·å¾—ç¡®å®šæ€§è¾“å‡º\n        }\n        print(\"ç”Ÿæˆå“åº”ä¸­...\")\n        # ä½¿ç”¨æ¨¡åž‹ç”Ÿæˆè¾“å‡º IDï¼Œè·³è¿‡è¾“å…¥ä»¤ç‰Œ\n        output_ids = self.model.generate(**inputs, **generation_args)\n        output_ids = output_ids[:, inputs['input_ids'].shape[1]:]  # å¿½ç•¥è¾“å‡ºä¸­çš„è¾“å…¥æç¤º\n        \n        # è§£ç ç”Ÿæˆçš„è¾“å‡ºä»¤ç‰Œä»¥èŽ·å–å“åº”æ–‡æœ¬\n        response = self.processor.batch_decode(output_ids, skip_special_tokens=True)[0]\n        return response\n\n## åˆå§‹åŒ–æ¨¡åž‹\nphi_model = Phi3VisionModel()\n\n## ç¤ºä¾‹é¢„æµ‹\nimage_url = \"https://example.com/sample_image.png\"  # ç¤ºä¾‹å›¾åƒçš„ URL\nprompt = \"ä»¥ json æ ¼å¼æå–æ•°æ®ã€‚\"  # æ¨¡åž‹æŒ‡å¯¼çš„æç¤º\nresponse = phi_model.predict(image_url, prompt)  # ä»Žæ¨¡åž‹èŽ·å–å“åº”\n\nprint(\"å“åº”:\", response)  # æ‰“å°ç”Ÿæˆçš„å“åº”\n```\nä¸Šè¿°ä»£ç å®šä¹‰äº†ä¸€ä¸ª `Phi3VisionModel` ç±»ï¼ŒæŠ½è±¡äº†æ¨¡åž‹çš„åŠ è½½å’Œä½¿ç”¨ï¼Œä½¿å…¶æ›´å®¹æ˜“é›†æˆåˆ°æ‚¨çš„åº”ç”¨ç¨‹åºä¸­ã€‚`predict()` æ–¹æ³•å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨è‡ªå®šä¹‰æç¤ºè¿›è¡ŒåŸºäºŽå›¾åƒçš„æŽ¨æ–­ã€‚\n\nä¸ºäº†æ›´æ–°æ–‡ç« ï¼Œä¾§é‡äºŽæµ‹è¯• Phi-3-Vision-128K-Instruct æ¨¡åž‹çš„ OCR èƒ½åŠ›ï¼Œæˆ‘ä»¬å°†æ·»åŠ ä¸€ä¸ªéƒ¨åˆ†ï¼Œè¯¦ç»†è¯´æ˜Žæ¨¡åž‹åœ¨å¤„ç†æ‰«æçš„èº«ä»½è¯ç­‰å®žé™…ç¤ºä¾‹æ—¶çš„è¡¨çŽ°ã€‚\n\n## æµ‹è¯• OCR åŠŸèƒ½ä¸Žæ‰«æçš„èº«ä»½è¯ä»¶\n\nä¸ºäº†è¯„ä¼° Phi\\-3\\-Vision\\-128K\\-Instruct æ¨¡åž‹çš„ OCR æ€§èƒ½ï¼Œæˆ‘ä»¬ä½¿ç”¨å‡ å¼ çœŸå®žçš„æ‰«æèº«ä»½è¯ä»¶å›¾åƒè¿›è¡Œäº†æµ‹è¯•ã€‚è¿™äº›å›¾åƒåœ¨è´¨é‡å’Œæ¸…æ™°åº¦ä¸Šå„ä¸ç›¸åŒï¼Œä¸ºæ¨¡åž‹æä¾›äº†ä¸€ç³»åˆ—æŒ‘æˆ˜ã€‚ç›®æ ‡æ˜¯å±•ç¤ºæ¨¡åž‹åœ¨æå–å…·æœ‰ä¸åŒç‰¹å¾çš„æ–‡æ¡£ä¸­çš„æ–‡æœ¬ä¿¡æ¯æ–¹é¢çš„è¡¨çŽ°ï¼Œå¦‚æ¨¡ç³Šã€å¤æ‚èƒŒæ™¯å’Œä¸åŒçš„å­—ä½“ã€‚\n\n**å›¾åƒ 1ï¼š** ä¸€æœ¬è™šæž„çš„ä¹Œæ‰˜é‚¦æŠ¤ç…§ï¼ŒåŒ…å«è¯¦ç»†çš„æ–‡æœ¬ï¼ŒåŒ…æ‹¬ä¸ªäººä¿¡æ¯ï¼Œå¦‚å§“åã€å›½ç±ã€å‡ºç”Ÿåœ°ã€ç­¾å‘æ—¥æœŸå’Œåˆ°æœŸæ—¥æœŸã€‚æ–‡æœ¬ç•¥æ˜¾é£Žæ ¼åŒ–ï¼Œåº•éƒ¨æœ‰æœºå™¨å¯è¯»åŒºã€‚å›¾åƒè´¨é‡é«˜ï¼Œæ²¡æœ‰æ˜Žæ˜¾çš„èƒŒæ™¯å™ªå£°ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MltpseOI3HhvCkUZMwLdEQ.png)\n\n**è¾“å‡ºï¼š**\n\n\n```python\n{\n  \"Type/Type\": \"P\",\n  \"Country code/Code du pays\": \"UTO\",\n  \"Passport Number/NÂ° de passeport\": \"L898902C3\",\n  \"Surname/Nom\": \"ERIKSSON\",\n  \"Given names/PrÃ©noms\": \"ANNA MARIA\",\n  \"Nationality/NationalitÃ©\": \"UTOPIAN\",\n  \"Date of Birth/Date de naissance\": \"12 AUGUST/AOUT 74\",\n  \"Personal No./NÂ° personnel\": \"Z E 184226 B\",\n  \"Sex/Sexe\": \"F\",\n  \"Place of birth/Lieu de naissance\": \"ZENITH\",\n  \"Date of issue/Date de dÃ©livrance\": \"16 APR/AVR 07\",\n  \"Authority/AutoritÃ©\": \"PASSPORT OFFICE\",\n  \"Date of expiry/Date d'expiration\": \"15 APR/AVR 12\",\n  \"Holder's signature/Signature du titulaire\": \"anna maria eriksson\",\n  \"Passport/Passeport\": \"P<UTOERIKSSON<<ANNA<MARIA<<<<<<<<<<<<<<<<<<<<<<<L898902C36UT07408122F1204159ZE184226B<<<<10\"\n}\n```\n**å›¾åƒ 2ï¼š** ä¸€æœ¬è·å…°æŠ¤ç…§ï¼Œæ¸…æ™°åœ°æ˜¾ç¤ºæŒæœ‰äººå’Œæ•´é½æ ¼å¼åŒ–çš„æ–‡æœ¬ã€‚å­—æ®µåŒ…æ‹¬æŠ¤ç…§å·ç ã€å§“åã€å‡ºç”Ÿæ—¥æœŸã€å›½ç±å’Œåˆ°æœŸæ—¥æœŸã€‚è¯¥æ–‡ä»¶å‘ˆçŽ°å‡ºé«˜å¯¹æ¯”åº¦ï¼Œä½¿æ–‡æœ¬æå–ç›¸å¯¹ç®€å•ã€‚åº•éƒ¨çš„æœºå™¨å¯è¯»åŒº (MRZ) æä¾›äº†ä¸€ç§ç»“æž„åŒ–çš„æ•°æ®æ ¼å¼ï¼Œæœ‰åŠ©äºŽéªŒè¯æå–ä¿¡æ¯çš„å‡†ç¡®æ€§ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*WGV4tTxI9xISmAvFs8ovNw.png)\n\n**è¾“å‡ºï¼š**\n\n\n```python\nHere's the extracted full data from the passport in JSON format:\n\n{\n  \"passport\": {\n    \"issuingCountry\": \"Netherlands\",\n    \"issuingAuthority\": \"Koninkrijk der Nederlanden\",\n    \"passportNumber\": \"SPEC12014\",\n    \"issuingDate\": \"09 MAR 2014\",\n    \"expiryDate\": \"09 MAR 2024\",\n    \"holder\": {\n      \"gender\": \"F\",\n      \"nationality\": \"Netherlands\",\n      \"placeOfBirth\": \"SPECIMEN\",\n      \"sex\": \"WF\",\n      \"firstNames\": [\n        \"Willem\",\n        \"Lieselotte\"\n      ]\n    },\n    \"physicalDescription\": {\n      \"height\": \"1.75 m\",\n      \"hairColor\": \"gray\",\n      \"hairLength\": \"short\"\n    },\n    \"issuingOffice\": \"Burg. van Stad en Dorp\",\n    \"issuingDateAsInt\": \"14032014\",\n    \"expiryDateAsInt\": \"14032024\",\n    \"fieldsExtracted\": [\n      {\n        \"code\": \"NL\",\n        \"dateOfBirth\": \"10 MAR 1965\",\n        \"dateOfIssue\": \"09 MAR 2014\",\n        \"dateOfExpiry\": \"09 MAR 2024\",\n        \"firstNames\": [\n          \"Willem\",\n          \"Lieselotte\"\n        ],\n        \"nationality\": \"Netherlands\",\n        \"passportNumber\": \"SPEC12014\",\n        \"placeOfBirth\": \"SPECIMEN\",\n        \"sex\": \"WF\"\n      }\n    ]\n  }\n}\n```\n\n## å°è¯• Phi\\-3\\-Vision\\-128K\\-Instruct\n\nå¦‚æžœæ‚¨æƒ³äº²è‡ªå°è¯• Phi\\-3\\-Vision\\-128K\\-Instruct æ¨¡åž‹ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹é“¾æŽ¥è¿›è¡ŒæŽ¢ç´¢ï¼š[åœ¨ Azure AI ä¸Šå°è¯• Phi\\-3\\-Vision\\-128K\\-Instruct](https://ai.azure.com/explore/models/Phi-3-vision-128k-instruct/version/1/registry/azureml)ã€‚è¯¥é“¾æŽ¥å…è®¸æ‚¨ä½“éªŒæ¨¡åž‹çš„åŠŸèƒ½å¹¶å®žéªŒå…¶ OCR åŠŸèƒ½ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*7feNu3ZuclgAnAzbJMMSFg.png)\n\n## ç†è§£æž¶æž„ä¸Žè®­ç»ƒ\n\n**Phi\\-3\\-Vision\\-128K\\-Instruct** æ¨¡åž‹ä¸ä»…ä»…æ˜¯ä¸€ä¸ªè¯­è¨€æ¨¡åž‹â€”â€”å®ƒæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€å¼ºè€…ï¼Œèƒ½å¤Ÿå¤„ç†è§†è§‰å’Œæ–‡æœ¬æ•°æ®ã€‚å®ƒç»åŽ†äº†å…¨é¢çš„è®­ç»ƒè¿‡ç¨‹ï¼ŒåŒ…å« **5000äº¿ä¸ªæ ‡è®°**ï¼Œç»“åˆäº†æ–‡æœ¬å’Œå›¾åƒæ•°æ®ã€‚å…¶æž¶æž„æ•´åˆäº†è¯­è¨€æ¨¡åž‹å’Œå›¾åƒå¤„ç†æ¨¡å—ï¼Œåˆ›å»ºäº†ä¸€ä¸ªèƒ½å¤Ÿç†è§£ **128K ä¸ªæ ‡è®°** ä¸Šä¸‹æ–‡çš„ç»Ÿä¸€ç³»ç»Ÿï¼Œæ”¯æŒæ›´é•¿çš„å¯¹è¯æˆ–å¤§é‡å†…å®¹çš„æ–‡æ¡£ã€‚\n\nåœ¨å¼ºå¤§çš„ç¡¬ä»¶ä¸Šè®­ç»ƒï¼Œä¾‹å¦‚ **512 H100 GPUs**ï¼Œå¹¶åˆ©ç”¨ **flash attention** æé«˜å†…å­˜æ•ˆçŽ‡ï¼Œè¿™ä¸ªæ¨¡åž‹èƒ½å¤Ÿè½»æ¾å¤„ç†å¤§è§„æ¨¡ä»»åŠ¡ã€‚è®­ç»ƒæ•°æ®é›†åŒ…æ‹¬åˆæˆæ•°æ®å’Œç»è¿‡ç­›é€‰çš„çœŸå®žä¸–ç•Œæ•°æ®ï¼Œå¼ºè°ƒ **æ•°å­¦ã€ç¼–ç ã€å¸¸è¯†æŽ¨ç†** å’Œ **ä¸€èˆ¬çŸ¥è¯†**ï¼Œä½¿å…¶è¶³å¤Ÿçµæ´»ä»¥é€‚åº”å„ç§åº”ç”¨ã€‚\n\n## å…³é”®åŸºå‡†å’Œæ€§èƒ½\n\nPhi\\-3\\-Vision\\-128K\\-Instruct çš„æ€§èƒ½å·²ç»åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œè¯„ä¼°ï¼ŒåŒ…æ‹¬ **ScienceQA**ã€**AI2D**ã€**MathVista** å’Œ **TextVQA**ã€‚å®ƒçš„å¾—åˆ†åœ¨ç»“åˆæ–‡æœ¬å’Œè§†è§‰çš„ä»»åŠ¡ä¸­å§‹ç»ˆè¶…è¿‡è®¸å¤šçŽ°æœ‰æ¨¡åž‹ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»¥ä¸‹é¢†åŸŸï¼š\n\n* **æ–‡æ¡£ç†è§£**ï¼šä»Žå¤æ‚æ–‡æ¡£ï¼ˆå¦‚ PDF æˆ–å›¾åƒï¼‰ä¸­æå–æœ‰ç”¨ä¿¡æ¯ã€‚\n* **è¡¨æ ¼å’Œå›¾è¡¨ç†è§£**ï¼šå‡†ç¡®è§£è¯»å›¾å½¢æ•°æ®å¹¶å°†å…¶è½¬æ¢ä¸ºæ–‡æœ¬è§£é‡Šã€‚\n\nç‰¹åˆ«æ˜¯ï¼Œè¯¥æ¨¡åž‹åœ¨ **ChartQA** ä¸Šå–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ **81\\.4%**ï¼Œåœ¨ **AI2D** ä¸Šå–å¾—äº† **76\\.7%**ï¼Œå±•ç¤ºäº†å…¶æœ‰æ•ˆç†è§£æ•°æ®ä¸°å¯Œæ–‡æ¡£çš„èƒ½åŠ›ã€‚\n\n## ä¸ºä»€ä¹ˆOCRå’Œæ–‡æ¡£æå–å¾ˆé‡è¦\n\næ–‡æ¡£æå–å’ŒOCRå¯¹äºŽä¼ä¸šå’Œç ”ç©¶è‡³å…³é‡è¦ï¼Œä½¿å¾—å°†æ‰“å°æˆ–æ‰‹å†™æ–‡æœ¬è½¬æ¢ä¸ºæœºå™¨å¯è¯»æ ¼å¼æˆä¸ºå¯èƒ½ã€‚ä½¿ç”¨åƒPhi-3-Vision-128K-Instructè¿™æ ·çš„AIæ¨¡åž‹ï¼Œå¯ä»¥æ˜¾è‘—ç®€åŒ–**PDFè§£æž**ã€**æ•°æ®å½•å…¥è‡ªåŠ¨åŒ–**ã€**å‘ç¥¨å¤„ç†**å’Œ**æ³•å¾‹æ–‡æ¡£åˆ†æž**ç­‰ä»»åŠ¡ã€‚\n\næ— è®ºæ‚¨å¤„ç†çš„æ˜¯æ‰«ææ–‡æ¡£ã€æˆªå›¾è¿˜æ˜¯æ‹æ‘„çš„é¡µé¢ï¼Œè¯¥æ¨¡åž‹çš„å¤šæ¨¡æ€èƒ½åŠ›éƒ½å¯ä»¥å¸®åŠ©**è‡ªåŠ¨åŒ–æ•°æ®æå–**ï¼Œä½¿å…¶æˆä¸ºæé«˜ç”Ÿäº§åŠ›å’Œå‡å°‘äººå·¥å·¥ä½œé‡çš„å®è´µå·¥å…·ã€‚\n\n## è´Ÿè´£ä»»çš„äººå·¥æ™ºèƒ½ä¸Žå®‰å…¨æŽªæ–½\n\nè™½ç„¶è¯¥æ¨¡åž‹åŠŸèƒ½å¼ºå¤§ï¼Œä½†å¼€å‘è€…åº”æ³¨æ„å…¶å±€é™æ€§ã€‚**è¯­è¨€åè§**ã€**åˆ»æ¿å°è±¡å¼ºåŒ–**å’Œ**ä¸å‡†ç¡®å†…å®¹ç”Ÿæˆ**æ˜¯æ½œåœ¨é—®é¢˜ã€‚å¯¹äºŽé«˜é£Žé™©çš„ä½¿ç”¨æ¡ˆä¾‹ï¼Œä¾‹å¦‚**å¥åº·æˆ–æ³•å¾‹å»ºè®®**ï¼Œéœ€è¦é¢å¤–çš„**éªŒè¯å’Œå†…å®¹è¿‡æ»¤**å±‚ã€‚\n\n## æœªæ¥æ–¹å‘ä¸Žå¾®è°ƒ\n\næƒ³è¦æ‰©å±• Phi\\-3\\-Vision\\-128K\\-Instruct çš„åŠŸèƒ½å—ï¼Ÿæ”¯æŒå¾®è°ƒï¼Œå¯ä»¥ä½¿ç”¨ **Phi\\-3 Cookbook** è¿›è¡Œï¼Œè¯¥æ‰‹å†Œæä¾›äº†è°ƒæ•´æ¨¡åž‹ä»¥é€‚åº”ç‰¹å®šä»»åŠ¡çš„é…æ–¹ï¼Œä¾‹å¦‚ **æ–‡æ¡£åˆ†ç±»**ã€**å¢žå¼ºçš„ OCR å‡†ç¡®æ€§** å’Œ **ä¸“ä¸šçš„å›¾åƒç†è§£**ã€‚\n\n## ç»“è®º\n\nPhi\\-3\\-Vision\\-128K\\-Instruct ä¸ä»…ä»…æ˜¯å¤šæ¨¡æ€ AI çš„ä¸€æ­¥è¿›å±•ï¼›å®ƒæ˜¯è¿ˆå‘ä¸€ä¸ªæœªæ¥çš„é£žè·ƒï¼Œåœ¨è¿™ä¸ªæœªæ¥ä¸­ï¼Œ**æ–‡æ¡£æå–ã€OCR å’Œ AI é©±åŠ¨çš„å†…å®¹ç”Ÿæˆ**æ˜¯æ— ç¼ä¸”æ˜“äºŽèŽ·å–çš„ã€‚å‡­å€Ÿå¹¿æ³›çš„è®­ç»ƒã€å¼ºå¤§çš„æž¶æž„å’Œæ·±æ€ç†Ÿè™‘çš„è®¾è®¡ï¼Œè¯¥æ¨¡åž‹ä½¿å¼€å‘è€…èƒ½å¤Ÿåœ¨å„ä¸ªé¢†åŸŸè½¬å˜æ•°æ®å¤„ç†ã€‚\n\næ•¬è¯·æœŸå¾…æ›´å¤šå…³äºŽå¦‚ä½•å°†è¯¥æ¨¡åž‹ä¸ŽçŽ°å®žä¸–ç•Œåº”ç”¨é›†æˆçš„é«˜çº§ç¤ºä¾‹å’Œæ•™ç¨‹ï¼Œæˆ‘ä»¬å°†æŽ¢ç´¢**å¤„ç†å¤šç§æ–‡æ¡£ç±»åž‹**å’Œåº”ç”¨**AI é©±åŠ¨çš„æŠ€æœ¯**ä»Žå¤šæ ·åŒ–æ¥æºæå–æœ‰ä»·å€¼çš„è§è§£ã€‚\n\n**AI é©±åŠ¨çš„æ–‡æ¡£æå–**çš„æœªæ¥ä»Žæœªå¦‚æ­¤å…‰æ˜Žï¼\n\n"},{"lang":"zh","group":"blog","slug":"blog/ai-research-agents-set-to-transform-knowledge-research-in-2025-plus-top-3-free-tools-d37197726531","frontmatter":{"title":"äººå·¥æ™ºèƒ½ç ”ç©¶ä»£ç†ï¼š2025 å¹´å°†æ”¹å˜çŸ¥è¯†ç ”ç©¶ï¼ˆå¤–åŠ ä¸‰å¤§å…è´¹å·¥å…·ï¼‰","meta_title":"äººå·¥æ™ºèƒ½ç ”ç©¶ä»£ç†ï¼š2025 å¹´å°†æ”¹å˜çŸ¥è¯†ç ”ç©¶ï¼ˆå¤–åŠ ä¸‰å¤§å…è´¹å·¥å…·ï¼‰","description":"é¢„è®¡åˆ°2025å¹´ï¼ŒAIç ”ç©¶ä»£ç†å°†å½»åº•æ”¹å˜çŸ¥è¯†ç ”ç©¶çš„æ–¹å¼ã€‚ä¸Žä¼ ç»ŸAIå·¥å…·ä¸åŒï¼Œè¿™äº›ä»£ç†èƒ½å¤Ÿè‡ªä¸»é€‚åº”å¹¶å¤„ç†å¤§é‡ä¿¡æ¯ï¼Œè¯†åˆ«æ¨¡å¼å¹¶ç”Ÿæˆæ´žå¯Ÿã€‚æ–‡ç« ä»‹ç»äº†ä¸‰ç§å…è´¹å·¥å…·ï¼šæ–¯å¦ç¦çš„STORMã€CustomGPT.aiç ”ç©¶å‘˜å’ŒGPT Researcherï¼Œå®ƒä»¬åœ¨è‡ªåŠ¨åŒ–ç ”ç©¶ã€ç”Ÿæˆç»“æž„åŒ–å†…å®¹å’Œç¡®ä¿ä¿¡æ¯å‡†ç¡®æ€§æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚AIç ”ç©¶ä»£ç†å°†æå‡ç ”ç©¶æ•ˆçŽ‡ï¼Œå¸®åŠ©ç ”ç©¶è€…ä¸“æ³¨äºŽåˆ›é€ æ€§æ€ç»´ï¼ŒåŒæ—¶æŽ¨åŠ¨ç ”ç©¶çš„æ°‘ä¸»åŒ–ï¼Œä¿ƒè¿›è·¨å­¦ç§‘åˆ›æ–°å’Œå¤šæ ·åŒ–è§†è§’çš„å‡ºçŽ°ã€‚","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*8hV5Jo0eUwY01CWV","categories":["Research","Data Science","Generative AI"],"author":"Rifx.Online","tags":["agents","RAG","STORM","CustomGPT","GPT"],"draft":false,"slug":"blog/ai-research-agents-set-to-transform-knowledge-research-in-2025-plus-top-3-free-tools-d37197726531"},"content":"\n\n\näº‹æƒ…æ˜¯è¿™æ ·çš„ï¼šä¸€äº›é‡å¤§å˜åŒ–å³å°†åŠ¨æ‘‡çŸ¥è¯†ç ”ç©¶çš„ä¸–ç•Œã€‚\n\nåœ¨æ·±å…¥ç ”ç©¶AIç ”ç©¶ä»£ç†å¹¶åœ¨å„ä¸ªè¡Œä¸šçœ‹åˆ°å®ƒä»¬çš„å®žé™…åº”ç”¨å‡ ä¸ªæœˆåŽï¼Œæˆ‘å¯ä»¥è‚¯å®šåœ°å‘Šè¯‰ä½ â€”â€”åˆ°2025å¹´ï¼Œè¿™äº›å·¥å…·ä¸ä»…ä»…æ˜¯æœ‰ç”¨çš„å·¥å…·ã€‚å®ƒä»¬å°†ä»Žæ ¹æœ¬ä¸Šæ”¹å˜æˆ‘ä»¬è¿›è¡ŒçŸ¥è¯†ç ”ç©¶çš„æ–¹å¼ï¼ˆæ— è®ºæ˜¯è¥é”€è¿˜æ˜¯ç§‘å­¦ï¼ï¼‰ã€‚\n\n> **äººç±»åœ¨ä¸€ä¸ªå°æ—¶å†…è®¿é—®10,000ä¸ªç½‘ç«™å¹¶ç ”ç©¶æ•°æ®åœ¨ç‰©ç†ä¸Šæ˜¯ä¸å¯èƒ½çš„ã€‚ç„¶è€Œï¼Œä»£ç†å¯ä»¥è½»æ¾åšåˆ°è¿™ä¸€ç‚¹ã€‚**\n\nåœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†å‘ä½ å±•ç¤º3ä¸ªä¼šè®©ä½ å¤§åƒä¸€æƒŠçš„å…è´¹å·¥å…·ã€‚ï¼ˆæç¤ºï¼šè¿™ä¸æ˜¯ChatGPTæˆ–Perplexityï¼ï¼‰\n\n\n\næˆ‘çŸ¥é“ä½ åœ¨æƒ³ä»€ä¹ˆã€‚â€œåˆä¸€ç¯‡AIç‚’ä½œæ–‡ç« ï¼Ÿâ€ä½†è¯·ç»§ç»­å…³æ³¨ã€‚\n\né¢„è®¡å¸‚åœºå°†ä»Ž2024å¹´çš„51äº¿ç¾Žå…ƒæ¿€å¢žè‡³2030å¹´çš„471äº¿ç¾Žå…ƒã€‚è¿™ä¸ä»…ä»…æ˜¯å¢žé•¿â€”â€”è¿™æ˜¯ä¸€åœºç ”ç©¶é¢†åŸŸçš„å½»åº•å˜é©ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*uGrcW8msqlIdpBeL)\n\n## ä»€ä¹ˆä½¿å¾—AIç ”ç©¶ä»£ç†ä¸Žä¼—ä¸åŒï¼Ÿ\n\né¦–å…ˆï¼Œè¿™äº›å¹¶ä¸æ˜¯éœ€è¦ä¸æ–­æŒ‡å¯¼çš„å…¸åž‹AIå·¥å…·ã€‚ä¼ ç»Ÿç³»ç»Ÿéœ€è¦ä¸ºæ¯ä¸ªä»»åŠ¡æä¾›æ˜Žç¡®çš„æŒ‡ç¤ºï¼Œè€ŒAIç ”ç©¶ä»£ç†å°±åƒæ‹¥æœ‰ä¸€ä¸ªèªæ˜Žçš„ç ”ç©¶åŠ©ç†ï¼Œèƒ½å¤Ÿéšæœºåº”å˜ï¼Œæ ¹æ®ä»–ä»¬å–å¾—çš„ç»“æžœè°ƒæ•´è¡Œä¸ºã€‚\n\n \n\nçœŸæ­£çš„æ¸¸æˆè§„åˆ™æ”¹å˜è€…ï¼Ÿè¿™äº›ä»£ç†èƒ½å¤Ÿå¤„ç†å¤§é‡çŸ¥è¯†ï¼Œå‘çŽ°äººç±»å¯èƒ½é”™è¿‡çš„æ¨¡å¼ï¼Œå¹¶æ¯”ä»¥å¾€æ›´å¿«åœ°ç”Ÿæˆæ´žå¯Ÿã€‚åˆ©ç”¨å…ˆè¿›çš„[æ£€ç´¢å¢žå¼ºç”Ÿæˆï¼ˆRAGï¼‰](https://readmedium.com/build-it-or-buy-it-deployment-options-for-retrieval-augmented-generation-rag-f6d43df8212a)æŠ€æœ¯ï¼Œå®ƒä»¬å¯ä»¥ç›´æŽ¥ä»Žå¯ä¿¡æ¥æºæå–ä¿¡æ¯ï¼ŒåŒæ—¶ä¿æŒå‡†ç¡®æ€§ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*p2E-fJ63BB27lZdD)\n\n## é­”æ³•èƒŒåŽçš„æŠ€æœ¯\n\nè¿™é‡Œçš„ç§˜å¯†åœ¨äºŽèƒ½å¤Ÿæ‘„å–å’Œç ”ç©¶å¤§é‡çŸ¥è¯†ï¼ˆä¾‹å¦‚æ·±åº¦è°·æ­Œç ”ç©¶ï¼‰ï¼Œç„¶åŽå°†å…¶ä¸Žåƒ gpt\\-4o å’Œ o1\\ è¿™æ ·çš„ LLMs çš„åŠ›é‡ç»“åˆèµ·æ¥ã€‚\n\nä½†çœŸæ­£è®©æˆ‘å…´å¥‹çš„æ˜¯ï¼šè¿™äº›ä»£ç†ç”± RAG æ¨¡åž‹é©±åŠ¨ï¼Œå†…ç½®çš„åå¹»è§‰ç®—æ³•ç¡®ä¿äº†å‡†ç¡®æ€§ã€‚ä¸Žé€šç”¨ AI å·¥å…·ä¸åŒï¼Œç ”ç©¶ä»£ç†åšæŒä½¿ç”¨ç»è¿‡éªŒè¯çš„ä¿¡æ¯ï¼Œå¹¶èƒ½å¤Ÿå¼•ç”¨å…¶æ¥æºâ€”â€”è¿™å¯¹äºŽç»´æŠ¤è¯šä¿¡è‡³å…³é‡è¦ã€‚\n\n> **æ‰€ä»¥æƒ³è±¡ä¸€ä¸‹ï¼Œä¸€ä¸ªç ”ç©¶ä»£ç†èŠ± 30 åˆ†é’Ÿæ—¶é—´å¯¹æŸä¸ªä¸»é¢˜è¿›è¡Œåšå£«ç ”ç©¶â€”â€”è¿™æ˜¯å¤§å¤šæ•°äººéœ€è¦å‡ å¤©æ‰èƒ½å®Œæˆçš„äº‹æƒ…ã€‚**\n\n## è¿™ä¸ºä»€ä¹ˆçŽ°åœ¨å¾ˆé‡è¦\n\næ—¶æœºå†å¥½ä¸è¿‡äº†ã€‚ç ”ç©¶æ­£æ·¹æ²¡åœ¨æ•°æ®ä¸­â€”â€”æˆ‘ä»¬æ¯å¤©ç”Ÿæˆçš„ä¿¡æ¯æ¯”è¿‡åŽ»ä¸€å¹´è¿˜è¦å¤šã€‚è€Œéšç€è°·æ­Œå¯¹ä½“éªŒã€ä¸“ä¸šæ€§ã€æƒå¨æ€§å’Œå¯ä¿¡åº¦ï¼ˆEEATï¼‰çš„é‡è§†ï¼Œå¯¹å‡†ç¡®ã€ç»è¿‡å……åˆ†ç ”ç©¶çš„å†…å®¹çš„éœ€æ±‚ä»Žæœªå¦‚æ­¤è¿«åˆ‡ã€‚\n\næˆ‘æœ€è¿‘ä¸Žä¸€ä¸ªç ”ç©¶å›¢é˜Ÿäº¤æµï¼Œä»–ä»¬ä½¿ç”¨äººå·¥æ™ºèƒ½ç ”ç©¶ä»£ç†å°†æ–‡ç« ç ”ç©¶æ—¶é—´ç¼©çŸ­äº†70%ã€‚ä½†è¿™ä¸ä»…ä»…æ˜¯é€Ÿåº¦çš„é—®é¢˜â€”â€”è¯¥ä»£ç†å‘çŽ°äº†ä»–ä»¬åœ¨æœ€åˆç®€æŠ¥ä¸­å®Œå…¨é—æ¼çš„çŸ¥è¯†è§†è§’ã€‚è€Œæœ€æ£’çš„éƒ¨åˆ†ï¼Ÿæ‰€æœ‰å†…å®¹éƒ½å¯ä»¥éªŒè¯ï¼Œå¹¶ä¸”æœ‰æ•°æ®æ”¯æŒã€‚\n\næ‰€ä»¥ï¼Œæƒ³è±¡ä¸€ä¸‹ï¼Œå¦‚æžœä½ èƒ½è®©çˆ±å› æ–¯å¦ã€åŸƒéš†Â·é©¬æ–¯å…‹ã€è´¹æ›¼ã€å²è’‚å¤«Â·ä¹”å¸ƒæ–¯ã€ç®€Â·å¤é“å°”å’Œå°¤ç“¦å°”Â·èµ«æ‹‰åˆ©å…±åŒåˆä½œå®Œæˆä½ çš„ç ”ç©¶æŠ¥å‘Šâ€”â€”è¿™å°±æ˜¯äººå·¥æ™ºèƒ½ç ”ç©¶ä»£ç†æ‰€èƒ½å®žçŽ°çš„ã€‚\n\n## ä¸‰å¤§äººå·¥æ™ºèƒ½ç ”ç©¶ä»£ç†æœºæž„\n\n## æ–¯å¦ç¦ STORM\n\næ–¯å¦ç¦å¤§å­¦çš„ [STORM](https://storm.genie.stanford.edu/)ï¼ˆé€šè¿‡æ£€ç´¢å’Œå¤šè§’åº¦æé—®åˆæˆä¸»é¢˜å¤§çº²ï¼‰æ˜¯ä¸€ä¸ªç”±äººå·¥æ™ºèƒ½é©±åŠ¨çš„çŸ¥è¯†ç­–åˆ’ç³»ç»Ÿï¼Œæ—¨åœ¨ä»Žé›¶å¼€å§‹ç”Ÿæˆå…¨é¢çš„ã€ç±»ä¼¼ç»´åŸºç™¾ç§‘çš„æ–‡ç« ã€‚\n\nåˆ©ç”¨å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰ï¼ŒSTORM é€šè¿‡è¿›è¡ŒåŸºäºŽäº’è”ç½‘çš„ç ”ç©¶ã€å°†ä¿¡æ¯ç»„ç»‡æˆç»“æž„åŒ–å¤§çº²ï¼Œå¹¶ç”Ÿæˆå¸¦æœ‰å¼•ç”¨çš„å®Œæ•´æ–‡ç« ï¼Œä»Žè€Œè‡ªåŠ¨åŒ–ç ”ç©¶å’Œå†™ä½œè¿‡ç¨‹ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*axYZ2fO15FAqQpID)\n\n**ä¼˜ç‚¹ï¼š**\n\n* **è‡ªåŠ¨åŒ–ç ”ç©¶å’Œå†™ä½œï¼š** STORM é€šè¿‡è‡ªåŠ¨åŒ–ç ”ç©¶å’Œå†™ä½œé˜¶æ®µï¼Œç®€åŒ–äº†è¯¦ç»†æ–‡ç« çš„åˆ›å»ºï¼ŒèŠ‚çœäº†ç”¨æˆ·å¤§é‡æ—¶é—´å’Œç²¾åŠ›ã€‚\n* **ç»“æž„åŒ–å†…å®¹ç”Ÿæˆï¼š** ç³»ç»Ÿç”Ÿæˆæœ‰ç»„ç»‡çš„å¤§çº²å’Œç»“æž„è‰¯å¥½çš„æ–‡ç« ï¼Œç¡®ä¿æœ€ç»ˆè¾“å‡ºçš„æ¸…æ™°æ€§å’Œä¸€è‡´æ€§ã€‚\n* **å¼€æºå¯è®¿é—®æ€§ï¼š** ä½œä¸ºä¸€ä¸ªå¼€æºé¡¹ç›®ï¼ŒSTORM å…è®¸ç”¨æˆ·æ ¹æ®ç‰¹å®šéœ€æ±‚è‡ªå®šä¹‰å’Œè°ƒæ•´å·¥å…·ï¼Œä¿ƒè¿›äººå·¥æ™ºèƒ½ç ”ç©¶ç¤¾åŒºçš„åˆ›æ–°å’Œåˆä½œã€‚\n\n**ç¼ºç‚¹ï¼š**\n\n* **ä¾èµ–äº’è”ç½‘æ¥æºï¼š** STORM å¯¹åŸºäºŽäº’è”ç½‘çš„ç ”ç©¶çš„ä¾èµ–å¯èƒ½å¯¼è‡´åŒ…å«è¿‡æ—¶æˆ–åè§çš„ä¿¡æ¯ï¼Œå¦‚æžœä¸åŠ ä»¥ä»”ç»†ç›‘æŽ§ã€‚\n* **è´¨é‡æŽ§åˆ¶è¦æ±‚ï¼š** å°½ç®¡ STORM è‡ªåŠ¨åŒ–äº†å¤§éƒ¨åˆ†å†™ä½œè¿‡ç¨‹ï¼Œä½†ç”Ÿæˆçš„æ–‡ç« ä»å¯èƒ½éœ€è¦äººå·¥å®¡æ ¸å’Œç¼–è¾‘ï¼Œä»¥ç¡®ä¿å‡†ç¡®æ€§å’Œç¬¦åˆç‰¹å®šæ ‡å‡†ã€‚\n* **æŠ€æœ¯è®¾ç½®ï¼š** åœ¨æœ¬åœ°å®žçŽ° STORM éœ€è¦ç†Ÿæ‚‰ Gitã€Python å’Œ Conda ç­‰å·¥å…·ï¼Œè¿™å¯èƒ½å¯¹æ²¡æœ‰æŠ€æœ¯èƒŒæ™¯çš„ç”¨æˆ·æž„æˆéšœç¢ã€‚\n\næœ‰å…³æ›´å¤šä¿¡æ¯å’Œè®¿é—® STORMï¼Œè¯·è®¿é—® [å®˜æ–¹ GitHub ä»“åº“](https://github.com/stanford-oval/storm)\n\n## CustomGPT.ai ç ”ç©¶å‘˜\n\nCustomGPT.ai [ç ”ç©¶å‘˜](https://customgpt-researcher.streamlit.app/) æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡çš„ AI ç ”ç©¶ä»£ç†ï¼Œæ—¨åœ¨åŸºäºŽæ·±å…¥çš„ Google ç ”ç©¶æˆ–è‡ªå®šä¹‰çŸ¥è¯†åº“ï¼ˆå¦‚å…¬å¸çš„ä¸“æœ‰æ•°æ®æˆ–å…¶ä»–å¯ä¿¡æ¥æºï¼‰åˆ›å»ºè¶…é«˜è´¨é‡çš„é•¿ç¯‡æ–‡ç« ã€‚\n\nåˆ©ç”¨ CustomGPT çš„åå¹»è§‰æŠ€æœ¯ï¼Œå®ƒç”Ÿæˆäº‹å®žå‡†ç¡®çš„å†…å®¹ï¼Œå¸¦æœ‰å¼•ç”¨ï¼Œç¬¦åˆç‰¹å®šå“ç‰ŒæŒ‡å—ï¼Œå¹¶ç¡®ä¿ä¸ŽçŽ°å®žä¸–ç•Œä¿¡æ¯çš„ä¸€è‡´æ€§ã€‚\n\nè¯¥ä»£ç†ä½¿ç”¨ o1ã€gpt-4o å’Œ GPT-4oï¼ˆè§†è§‰ï¼‰ç›¸ç»“åˆçš„æ–¹å¼ï¼Œåˆ¶ä½œåŒ…å«å†…åµŒå›¾åƒå’Œé“¾æŽ¥çš„è¯¦ç»†ç ”ç©¶æŠ¥å‘Šã€‚å…¶ç‹¬ç‰¹çš„â€œ**æ¸è¿›å™äº‹**â€åŠŸèƒ½æœ‰åŠ©äºŽåˆ›å»ºä¸æ˜¾å¾—æœºæ¢°çš„å™è¿°ï¼Œèƒ½å¤Ÿæ„è¯†åˆ°å…ˆå‰ç”Ÿæˆçš„å†…å®¹ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*UiCyTjNF3A3buKzT)\n\n**ä¼˜ç‚¹ï¼š**\n\n* **å¯ä¿¡çš„å†…å®¹åˆ›ä½œï¼š** é€šè¿‡æ•´åˆå¯é æ¥æºçš„æ•°æ®ï¼ŒCustomGPT.ai æœ€å¤§é™åº¦åœ°å‡å°‘äº†ä¸å‡†ç¡®æ€§ï¼Œéžå¸¸é€‚åˆæ³•å¾‹ã€é‡‘èžå’ŒåŒ»ç–—ç­‰éœ€è¦é«˜å†…å®¹å¯é æ€§çš„è¡Œä¸šã€‚\n* **åå¹»è§‰æŠ€æœ¯ï¼š** CustomGPT.ai åŒ…å«å…ˆè¿›çš„ç®—æ³•ï¼Œé˜²æ­¢ç”ŸæˆæŽ¨æµ‹æˆ–è™šæž„çš„ä¿¡æ¯ï¼Œç¡®ä¿å†…å®¹ä¸Žç»è¿‡éªŒè¯çš„æ¥æºç´§å¯†å¯¹é½ã€‚\n* **æ‰˜ç®¡è§£å†³æ–¹æ¡ˆï¼š** é€šè¿‡å…¶æ— ä»£ç ç•Œé¢ï¼ŒéžæŠ€æœ¯ç ”ç©¶äººå‘˜å’Œè¥é”€äººå‘˜å¯ä»¥è½»æ¾è§¦å‘æ·±åº¦ç ”ç©¶ï¼Œè€Œæ— éœ€æ·±å…¥ç¼–ç ç»†èŠ‚ã€‚\n* **SEO ä¼˜åŒ–çš„å†…å®¹ç”Ÿæˆï¼š** è¯¥å·¥å…·æ”¯æŒ Google çš„ EEATï¼ˆä¸“ä¸šçŸ¥è¯†ã€æƒå¨æ€§ã€å¯ä¿¡èµ–æ€§å’Œç»éªŒï¼‰æ ‡å‡†ï¼Œåˆ›å»ºåœ¨æœç´¢å¼•æ“Žä¸­æŽ’åè‰¯å¥½çš„å†…å®¹ï¼Œå¼ºè°ƒè´¨é‡å’Œæƒå¨æ€§ã€‚\n\n**ç¼ºç‚¹ï¼š**\n\n* **é—­æºï¼š** è™½ç„¶ CustomGPT.ai ç ”ç©¶å‘˜åœ¨æœ‰é™æ—¶é—´å†…å…è´¹ï¼Œä½†å®ƒæ˜¯ä¸€ä¸ªé—­æºçš„ä¸“æœ‰é¡¹ç›®ã€‚\n* **ç”Ÿæˆæ—¶é—´è¾ƒé•¿ï¼š** é«˜çº§æŽ¨ç†å’Œ RAG èƒ½åŠ›å¯èƒ½éœ€è¦é•¿è¾¾ 20 åˆ†é’Ÿçš„æ—¶é—´æ¥ç”Ÿæˆä¸€ç¯‡æ–‡ç« ï¼Œè¿™å¯èƒ½ä¸é€‚åˆå¯»æ±‚å¿«é€Ÿæˆ–ä½Žè´¨é‡å†…å®¹çš„ç”¨æˆ·ã€‚\n* **å¯¹é¢„ç®—å†…å®¹é¡¹ç›®çš„é€‚ç”¨æ€§æœ‰é™ï¼š** ç”±äºŽå…¶ä¸“æ³¨äºŽè´¨é‡ï¼ŒCustomGPT.ai ç ”ç©¶å‘˜ä¸é€‚åˆéœ€è¦å¿«é€Ÿã€å»‰ä»·æˆ–åŸºæœ¬å†…å®¹ç”Ÿæˆçš„é¡¹ç›®ã€‚\n\næœ‰å…³ CustomGPT.ai ç ”ç©¶å‘˜åŠå…¶åº”ç”¨çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§å…è´¹çš„ [Streamlit åº”ç”¨](https://customgpt-researcher.streamlit.app/)ã€‚\n\n## GPT Researcher\n\nGPT Researcher æ˜¯ä¸€ä¸ª [è‡ªä¸»æ™ºèƒ½ä½“](https://github.com/assafelovic/gpt-researcher)ï¼Œæ—¨åœ¨åˆ©ç”¨ç½‘ç»œå’Œæœ¬åœ°èµ„æºå¯¹ä»»ä½•ç»™å®šä»»åŠ¡è¿›è¡Œå…¨é¢ç ”ç©¶ã€‚\n\nå®ƒç”Ÿæˆè¯¦ç»†ã€äº‹å®žæ€§å’Œæ— åè§çš„æŠ¥å‘Šï¼Œå¹¶é™„æœ‰å¼•ç”¨ï¼Œæä¾›å®Œæ•´çš„å®šåˆ¶é€‰é¡¹ï¼Œä»¥åˆ›å»ºé‡èº«å®šåˆ¶çš„ç‰¹å®šé¢†åŸŸç ”ç©¶æ™ºèƒ½ä½“ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*_4Q680CBSvpCZOLW)\n\n**ä¼˜ç‚¹ï¼š**\n\n* **è‡ªä¸»ç ”ç©¶èƒ½åŠ›ï¼š** GPT Researcher è‡ªåŠ¨åŒ–ç ”ç©¶è¿‡ç¨‹ï¼Œæœ‰æ•ˆåœ°ä»Žå„ç§æ¥æºæ”¶é›†å’Œç»¼åˆä¿¡æ¯ï¼Œç”Ÿæˆå…¨é¢çš„æŠ¥å‘Šã€‚\n* **å®šåˆ¶å’Œçµæ´»æ€§ï¼š** ç”¨æˆ·å¯ä»¥å®šåˆ¶æ™ºèƒ½ä½“ä»¥ä¸“æ³¨äºŽç‰¹å®šé¢†åŸŸæˆ–ä¸»é¢˜ï¼Œä»Žè€Œæ»¡è¶³ç‰¹å®šéœ€æ±‚çš„å®šåˆ¶ç ”ç©¶è¾“å‡ºã€‚\n* **å¼€æºå¯è®¿é—®æ€§ï¼š** ä½œä¸ºä¸€ä¸ªå¼€æºé¡¹ç›®ï¼ŒGPT Researcher é¼“åŠ±ç¤¾åŒºåä½œå’ŒæŒç»­æ”¹è¿›ï¼Œä¸ºç”¨æˆ·æä¾›é€æ˜Žæ€§å’Œé€‚åº”æ€§ã€‚\n\n**ç¼ºç‚¹ï¼š**\n\n* **æŠ€æœ¯è®¾ç½®è¦æ±‚ï¼š** å®žæ–½ GPT Researcher å¯èƒ½éœ€è¦æŠ€æœ¯ä¸“é•¿ï¼ŒåŒ…æ‹¬å¯¹ Gitã€Python å’Œ Docker çš„ç†Ÿæ‚‰ï¼Œè¿™å¯èƒ½å¯¹éžæŠ€æœ¯ç”¨æˆ·æž„æˆéšœç¢ã€‚\n\næœ‰å…³æ›´å¤šä¿¡æ¯å’Œè®¿é—® GPT Researcherï¼Œè¯·è®¿é—® [å®˜æ–¹ GitHub å­˜å‚¨åº“](https://github.com/assafelovic/gpt-researcher)ï¼š\n\n## äººå·¥æ™ºèƒ½ç ”ç©¶çš„äººæ€§åŒ–\n\nè®©æˆ‘ä»¬é¢å¯¹ä¸€ä¸ªæ˜¾è€Œæ˜“è§çš„é—®é¢˜ï¼šâ€œè¿™äº›æ™ºèƒ½ä½“ä¼šå–ä»£äººç±»ç ”ç©¶è€…å—ï¼Ÿâ€ç»å¯¹ä¸ä¼šã€‚ç›¸åï¼Œå®ƒä»¬è®©ç ”ç©¶è€…èƒ½å¤Ÿä¸“æ³¨äºŽäººç±»æœ€æ“…é•¿çš„äº‹æƒ…ï¼šåˆ›é€ æ€§æ€ç»´ã€å¤æ‚é—®é¢˜è§£å†³å’Œç”Ÿæˆåˆ›æ–°å‡è®¾ã€‚\n\nå¯ä»¥æŠŠå®ƒæƒ³è±¡æˆä¸€ä¸ªè¶…çº§å¼ºå¤§çš„ç ”ç©¶åŠ©æ‰‹ï¼Œå®ƒä»Žä¸ç¡è§‰ã€ä»Žä¸ç–²å€¦ï¼Œå¹¶ä¸”èƒ½å¤Ÿä»¥é—ªç”µèˆ¬çš„é€Ÿåº¦å¤„ç†ä¿¡æ¯ã€‚å½“äººå·¥æ™ºèƒ½å¤„ç†æ·±å±‚çŸ¥è¯†ç ”ç©¶æ—¶ï¼Œç ”ç©¶è€…å¯ä»¥ä¸“æ³¨äºŽçªç ´æ€§çš„æ´žå¯Ÿã€‚\n\n## ä¸ºé©å‘½åšå¥½å‡†å¤‡\n\né‚£ä¹ˆï¼Œæˆ‘ä»¬å¦‚ä½•ä¸ºè¿™åœºäººå·¥æ™ºèƒ½ç ”ç©¶é©å‘½åšå¥½å‡†å¤‡å‘¢ï¼Ÿ\n\né¦–å…ˆï¼Œç ”ç©¶äººå‘˜éœ€è¦æå‡ä»–ä»¬çš„æŠ€èƒ½ã€‚æˆ‘å¹¶ä¸æ˜¯è¯´æ¯ä¸ªäººéƒ½éœ€è¦æˆä¸ºç¼–ç ä¸“å®¶ï¼Œä½†ç†è§£è¿™äº›äººå·¥æ™ºèƒ½ç ”ç©¶ä»£ç†çš„**ä¼˜åŠ¿å’Œå±€é™æ€§**å°†æ˜¯å…³é”®ã€‚\n\nå¤§å­¦æ­£åœ¨è°ƒæ•´ä»–ä»¬çš„è¯¾ç¨‹ï¼Œæˆ‘çœ‹åˆ°è¶Šæ¥è¶Šå¤šçš„[ç ”ç©¶äººå‘˜](https://drmichaellevin.org/resources/levinbot.html)åˆ©ç”¨äººå·¥æ™ºèƒ½æ¥è¾…åŠ©ä»–ä»¬çš„ç ”ç©¶å®žéªŒå®¤ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*E733R9Fn9ufaXnTF)\n\n## å±•æœ›2025å¹´ä¹‹åŽ\n\nè¿™é‡Œçš„æ½œåŠ›ä»¤äººéœ‡æƒŠã€‚è¿™äº›AIç ”ç©¶ä»£ç†å°†ä½¿æˆ‘ä»¬èƒ½å¤Ÿè¿›è¡Œç›®å‰å‡ ä¹Žæ— æ³•æƒ³è±¡çš„ç ”ç©¶ç±»åž‹ã€‚\n\nè·¨å­¦ç§‘çš„åˆ›æ–°å°†æˆä¸ºå¸¸æ€ï¼Œå› ä¸ºAIä»£ç†å¸®åŠ©è¿žæŽ¥æˆ‘ä»¬ä»Žæœªæ„è¯†åˆ°çš„ä¸åŒé¢†åŸŸä¹‹é—´çš„è”ç³»ã€‚\n\næˆ‘ç‰¹åˆ«å…´å¥‹çš„æ˜¯ï¼Œè¿™é¡¹æŠ€æœ¯å¯èƒ½ä¼šä½¿ç ”ç©¶å˜å¾—æ›´åŠ æ°‘ä¸»åŒ–ã€‚é‚£äº›æ— æ³•è´Ÿæ‹…å¤§åž‹ç ”ç©¶å›¢é˜Ÿçš„å°åž‹å®žéªŒå®¤å’Œæœºæž„å°†èƒ½å¤Ÿåˆ©ç”¨AIä»£ç†ä¸Žæ›´å¤§è§„æ¨¡çš„å‚ä¸Žè€…ç«žäº‰ã€‚è¿™æ„å‘³ç€å°†ä¼šæœ‰æ›´å¤šæ ·åŒ–çš„è§†è§’å’Œæ›´å¤šçªç ´æ€§çš„å‘çŽ°ã€‚\n\nä¸¾ä¸ªä¾‹å­ï¼Œ[å¡”å¤«èŒ¨å¤§å­¦çš„Levinå®žéªŒå®¤](https://drmichaellevin.org/resources/levinbot.html)èƒ½å¤Ÿåœ¨å‡ ä¸ªå°æ—¶å†…æž„å»ºå‡ºæœ€å¥½çš„AIå·¥å…·ä¹‹ä¸€â€”â€”å±•ç¤ºäº†AIæ°‘ä¸»åŒ–çš„çœŸæ­£åŠ›é‡ã€‚\n\n## æœ€åŽçš„æ€è€ƒ\n\nåœ¨èŠ±äº†å‡ ä¸ªæœˆæ—¶é—´ç ”ç©¶è¿™ä¸ªè¯é¢˜å¹¶ä¸Žè¯¥é¢†åŸŸçš„ä¸“å®¶äº¤è°ˆåŽï¼Œæˆ‘ç›¸ä¿¡AIç ”ç©¶ä»£ç†å°†ä¼šåƒäº’è”ç½‘å¯¹ç§‘å­¦çš„å½±å“ä¸€æ ·å…·æœ‰å˜é©æ€§ã€‚å®ƒä»¬ä¸ä»…ä»…æ˜¯å·¥å…·â€”â€”å®ƒä»¬æ˜¯ç ”ç©¶è¿‡ç¨‹ä¸­çš„åˆä½œä¼™ä¼´ï¼Œå°†å¸®åŠ©æˆ‘ä»¬åº”å¯¹äººç±»é¢ä¸´çš„ä¸€äº›æœ€å¤§æŒ‘æˆ˜ã€‚\n\nå½“ç„¶ï¼Œè¿˜æœ‰è®¸å¤šéšœç¢éœ€è¦å…‹æœï¼ŒæŠ€èƒ½éœ€è¦å‘å±•ã€‚ä½†æ½œåœ¨çš„å¥½å¤„å¤ªå·¨å¤§ï¼Œæ— æ³•å¿½è§†ã€‚å¦‚æžœä½ ä»Žäº‹å¸‚åœºè¥é”€æˆ–ç ”ç©¶ï¼ŒçŽ°åœ¨æ˜¯å¼€å§‹ä¸ºè¿™ä¸€è½¬å˜åšå‡†å¤‡çš„æ—¶å€™ã€‚\n\nè¯·è®°ä½ï¼šé‚£äº›æ—©æœŸæ‹¥æŠ±è¿™é¡¹æŠ€æœ¯çš„å›¢é˜Ÿå’Œæœºæž„å°†åœ¨æœªæ¥å‡ å¹´ä¸­æ‹¥æœ‰å·¨å¤§çš„ä¼˜åŠ¿ã€‚åœ¨è¿™åœºå¡‘é€ æˆ‘ä»¬çŸ¥è¯†ç ”ç©¶æ–¹å¼çš„é‡å¤§é©å‘½ä¸­ï¼Œä¸è¦è¢«è½åœ¨åŽé¢â€”â€”æ— è®ºä½ æ˜¯ä¸ºSEOæ’°å†™åšå®¢æ–‡ç« ï¼Œè¿˜æ˜¯åœ¨ç§‘å­¦ä¸»é¢˜ä¸Šæ”»è¯»åšå£«å­¦ä½ã€‚\n\n*ä½ å¯¹AIç ”ç©¶ä»£ç†æœ‰ä»€ä¹ˆçœ‹æ³•ï¼Ÿä½ æ˜¯å¦å·²ç»å¼€å§‹å°†å®ƒä»¬æ•´åˆåˆ°ä½ çš„ç ”ç©¶å·¥ä½œæµç¨‹ä¸­ï¼Ÿæˆ‘å¾ˆæƒ³åœ¨ä¸‹é¢çš„è¯„è®ºä¸­å¬å¬ä½ çš„ç»åŽ†ã€‚*\n\n"},{"lang":"zh","group":"blog","slug":"blog/alibabas-open-source-qwen-how-it-s-revolutionizing-ai-and-how-you-can-use-it-dcba8f687c97","frontmatter":{"title":"é˜¿é‡Œå·´å·´å¼€æº Qwenï¼šå®ƒå¦‚ä½•å½»åº•æ”¹å˜äººå·¥æ™ºèƒ½ä»¥åŠå¦‚ä½•ä½¿ç”¨å®ƒ","meta_title":"é˜¿é‡Œå·´å·´å¼€æº Qwenï¼šå®ƒå¦‚ä½•å½»åº•æ”¹å˜äººå·¥æ™ºèƒ½ä»¥åŠå¦‚ä½•ä½¿ç”¨å®ƒ","description":"é˜¿é‡Œå·´å·´æœ€è¿‘åœ¨ 2024 å¹´äº‘æ –å¤§ä¼šæœŸé—´å¼€æºäº† Qwen 2.5 æ¨¡åž‹ï¼Œåœ¨ AI é¢†åŸŸæŽ€èµ·äº†æ³¢æ¾œã€‚è¶…è¿‡ 100 ä¸ªâ€¦","date":"2024-10-26T00:26:25.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*I7QDwbLMzoJ_ORq5.jpg","categories":["Programming","Machine Learning","Natural Language Processing"],"author":"Rifx.Online","tags":["Qwen","multimodal","open-source","fine-tune","text-to-video"],"draft":false,"slug":"blog/alibabas-open-source-qwen-how-it-s-revolutionizing-ai-and-how-you-can-use-it-dcba8f687c97"},"content":"\né˜¿é‡Œå·´å·´æœ€è¿‘åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸå¼•èµ·äº†è½°åŠ¨ï¼Œåœ¨2024å¹´ Apsara å¤§ä¼šä¸Šå¼€æºäº†å…¶ **Qwen 2.5** æ¨¡åž‹ã€‚Qwen æ‹¥æœ‰è¶…è¿‡ 100 ä¸ªæ¨¡åž‹ï¼Œæ¶µç›–è¯­è¨€ã€è§†è§‰ã€éŸ³é¢‘å’Œä»£ç ç­‰å¤šç§æ¨¡æ€ï¼Œä½¿å…¶æˆä¸ºæœ€å…¨é¢çš„å¼€æºäººå·¥æ™ºèƒ½è§£å†³æ–¹æ¡ˆä¹‹ä¸€ã€‚æ­¤æ¬¡å‘å¸ƒé€šè¿‡æä¾›å¤šæ ·åŒ–åº”ç”¨çš„å·¥å…·ï¼Œèµ‹èƒ½å¼€å‘è€…ï¼Œä»Žæ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆåˆ°å®žæ—¶é—®ç­”ã€‚\n\n\n\n## é˜¿é‡Œå·´å·´ Qwen æ¨¡åž‹çš„å…³é”®ç‰¹æ€§\n\n1. **å¤šæ¨¡æ€èƒ½åŠ›**ï¼šQwen æ¨¡åž‹å¤„ç†å¤šç§è¾“å…¥ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€éŸ³é¢‘å’Œè§†è§‰æ•°æ®ã€‚è¿™ç§å¤šæ¨¡æ€æ–¹æ³•ä½¿å…¶é€‚ç”¨äºŽå¹¿æ³›çš„è¡Œä¸šï¼Œä»Žåª’ä½“å’Œå¨±ä¹åˆ°æœºå™¨äººæŠ€æœ¯ã€‚\n2. **å¼€æº**ï¼šQwen å¯åœ¨ **Hugging Face** å’Œ **ModelScope** ç­‰å¹³å°ä¸ŠèŽ·å–ï¼Œå·²ç»è¢«ä¸‹è½½è¶…è¿‡ 4000 ä¸‡æ¬¡ï¼ŒåŸºäºŽå…¶åŸºç¡€æž„å»ºçš„è‡ªå®šä¹‰æ¨¡åž‹è¶…è¿‡ 50,000 ä¸ªã€‚\n3. **å¢žå¼ºæ€§èƒ½**ï¼šQwen2.5 å¼•å…¥äº†æ”¹è¿›çš„è¯­è¨€ç†è§£ã€æ•°å­¦å’Œç¼–ç èƒ½åŠ›ï¼Œä¸Žè¯¥é¢†åŸŸçš„é¢†å…ˆæ¨¡åž‹ç«žäº‰ã€‚é€šè¿‡é’ˆå¯¹ç»“æž„åŒ–æ•°æ®ç†è§£å’Œé•¿æ–‡æœ¬ç”Ÿæˆç­‰ä»»åŠ¡çš„ä¼˜åŒ–æ€§èƒ½ï¼ŒQwen ä¸ºé«˜çº§ AI åº”ç”¨æ‰“å¼€äº†å¤§é—¨ã€‚\n\n## å¦‚ä½•ä½¿ç”¨é˜¿é‡Œå·´å·´çš„ Qwen\n\nå¼€å‘è€…å’Œç»„ç»‡å¯ä»¥åœ¨ Hugging Face ç­‰å¹³å°ä¸Šè®¿é—® Qwen æ¨¡åž‹ï¼Œå…·ä½“å¯ä»¥ï¼š\n\n* **å¾®è°ƒæ¨¡åž‹**ï¼šä¸ºç‰¹å®šè¡Œä¸šåº”ç”¨é‡èº«å®šåˆ¶ Qwenï¼Œä¾‹å¦‚å®¢æˆ·æœåŠ¡ã€è‡ªåŠ¨åŒ–æˆ–è§†é¢‘å†…å®¹åˆ›ä½œã€‚\n* **ä¸Žåº”ç”¨é›†æˆ**ï¼šQwen çš„æ–‡æœ¬è½¬è§†é¢‘æ¨¡åž‹å¯ä»¥é›†æˆåˆ°åª’ä½“åˆ¶ä½œæµç¨‹ä¸­ï¼Œä»Žé™æ€å›¾åƒå’Œæ–‡æœ¬æç¤ºç”ŸæˆåŠ¨æ€å†…å®¹ã€‚\n* **å¼€å‘ AI åŠ©æ‰‹**ï¼šå€ŸåŠ©å¢žå¼ºçš„è§†è§‰è¯­è¨€æ¨¡åž‹ï¼ŒQwen å¯ç”¨äºŽæœºå™¨äººå’Œè‡ªåŠ¨é©¾é©¶æ±½è½¦ï¼Œä»¥å¤„ç†è§†é¢‘æ•°æ®å¹¶æ‰§è¡Œå®žæ—¶ä»»åŠ¡ï¼Œå¦‚å¯¼èˆªæˆ–ç‰©ä½“è¯†åˆ«ã€‚\n\n**é€šè¿‡ Hugging Face ä½¿ç”¨ Qwen çš„ç¤ºä¾‹**ï¼š\n\n```python\nfrom transformers import QwenTokenizer, QwenModel\n\ntokenizer = QwenTokenizer.from_pretrained(\"qwen-2.5\")\nmodel = QwenModel.from_pretrained(\"qwen-2.5\")\n\ninput_text = \"What is the future of AI in healthcare?\"\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\noutputs = model(input_ids)\n```\n\nè¿™ä½¿ç”¨æˆ·èƒ½å¤Ÿè®¿é—® Qwen æ¨¡åž‹ï¼Œè¿è¡ŒæŽ¨ç†ï¼Œå¹¶æ ¹æ®ç‰¹å®šéœ€æ±‚è¿›è¡Œå®šåˆ¶ã€‚\n\n## Qwenåœ¨å„è¡Œä¸šçš„å½±å“\n\n1. **åª’ä½“ä¸Žå¨±ä¹**ï¼šå‡­å€Ÿæ–°çš„æ–‡æœ¬åˆ°è§†é¢‘åŠŸèƒ½ï¼ŒQwenå¯ä»¥è‡ªåŠ¨ä»Žä¹¦é¢è„šæœ¬ç”Ÿæˆè§†é¢‘ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–ç¹ççš„åˆ¶ä½œä»»åŠ¡æ¥æ”¹å˜åˆ›æ„äº§ä¸šã€‚\n2. **æœºå™¨äººæŠ€æœ¯ä¸Žè‡ªåŠ¨é©¾é©¶è½¦è¾†**ï¼šQwenä¸­å¢žå¼ºçš„è§†è§‰è¯­è¨€æ¨¡åž‹å¯ä»¥å¸®åŠ©æœºå™¨äººç†è§£çŽ°å®žä¸–ç•ŒçŽ¯å¢ƒï¼Œä»Žè€Œåœ¨è‡ªåŠ¨é©¾é©¶æˆ–åˆ¶é€ ä¸­åšå‡ºæ›´å¥½çš„å†³ç­–ã€‚\n3. **è½¯ä»¶å¼€å‘**ï¼šç”±Qwené©±åŠ¨çš„é˜¿é‡Œå·´å·´AIå¼€å‘å·¥å…·è‡ªåŠ¨åŒ–äº†ä»£ç ç”Ÿæˆã€è°ƒè¯•å’Œéœ€æ±‚åˆ†æžç­‰ä»»åŠ¡ï¼Œä½¿å¼€å‘äººå‘˜èƒ½å¤Ÿä¸“æ³¨äºŽæ›´é«˜å±‚æ¬¡çš„é—®é¢˜è§£å†³ã€‚\n\n## ç»“è®ºï¼šå¼€æ”¾AIåˆ›æ–°çš„æ–°çºªå…ƒ\n\né€šè¿‡å¼€æºå…¶Qwen 2.5æ¨¡åž‹ï¼Œé˜¿é‡Œå·´å·´æ­£åœ¨ä½¿å…ˆè¿›çš„AIæŠ€æœ¯å˜å¾—æ›´åŠ æ™®åŠã€‚å¼€å‘è€…ã€åˆåˆ›ä¼ä¸šå’Œå¤§åž‹ä¼ä¸šéƒ½å¯ä»¥åˆ©ç”¨Qwençš„å¤šæ¨¡æ€å’Œå®žæ—¶èƒ½åŠ›ï¼Œåœ¨ä»Žåª’ä½“åˆ°è‡ªåŠ¨é©¾é©¶æ±½è½¦ç­‰è¡Œä¸šæŽ¨åŠ¨åˆ›æ–°ã€‚æ— è®ºæ‚¨æ˜¯å¸Œæœ›ä¸ºç‰¹å®šåº”ç”¨å¾®è°ƒæ¨¡åž‹çš„å¼€å‘è€…ï¼Œè¿˜æ˜¯å°†AIé›†æˆåˆ°åŸºç¡€è®¾æ–½ä¸­çš„ä¼ä¸šï¼ŒQwenéƒ½æä¾›å¼ºå¤§çš„å·¥å…·æ¥åŠ é€Ÿè¿›æ­¥ã€‚\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/artifacts-top-mindblowing-uses-of-claude-3-5-sonent-6830b2acfa4b","frontmatter":{"title":"æ–‡ç‰©ï¼šClaude 3.5 Sonent æœ€ä»¤äººæƒŠå¹çš„ç”¨é€”","meta_title":"æ–‡ç‰©ï¼šClaude 3.5 Sonent æœ€ä»¤äººæƒŠå¹çš„ç”¨é€”","description":"Anthropic æœ€è¿‘æŽ¨å‡ºäº†å…¶æœ€å…ˆè¿›çš„æ³•å­¦ç¡•å£«è¯¾ç¨‹â€œClaude 3.5 Sonnetâ€ï¼Œè¿™ä»¤äººæƒŠå¹ã€‚ç¤¾äº¤åª’ä½“ä¸Šçš„äººä»¬ç§°è¿™ç§æ¨¡å¼ä¸ºâ€¦â€¦","date":"2024-11-08T00:18:38.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*XL1dN9VCFcbz3m5N","categories":["Programming","Natural Language Processing","Generative AI"],"author":"Rifx.Online","tags":["Sonnet","context","Artifacts","code","generation"],"draft":false,"slug":"blog/artifacts-top-mindblowing-uses-of-claude-3-5-sonent-6830b2acfa4b"},"content":"\n\n\nAnthropic æœ€è¿‘æŽ¨å‡ºäº†å…¶æœ€å…ˆè¿›çš„ LLMï¼Œâ€œClaude 3\\.5 Sonnetâ€ï¼Œè®©äººæƒŠå¹ã€‚ç¤¾äº¤åª’ä½“ä¸Šçš„äººä»¬ç§°è¿™ä¸€æ¨¡åž‹ä¸ºå½“å‰æœ€å…ˆè¿›çš„ LLMã€‚è¿™ä¸ª AI æ¨¡åž‹åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†æ‰€æœ‰çŽ°æœ‰çš„ LLMï¼Œä¾‹å¦‚ GPT\\-4ã€GPT\\-4o miniã€Llama 3 ç­‰ç­‰ã€‚Claude 3\\.5 Sonnet çš„ä¸Šä¸‹æ–‡çª—å£ä¸º 200Kï¼Œæœ€å¤§è¾“å‡ºä¸º 8192 ä¸ª tokensã€‚å®ƒå¯ä»¥ç”Ÿæˆä¸€ä¸ªåŒ…å«å¤§é‡æ•°æ®ä½œä¸ºè¾“å…¥çš„åºžå¤§æ®µè½ã€‚Claude 3\\.5 Sonnet æ˜¯æœ€å¥½çš„ AI è§†è§‰æ¨¡åž‹ä¹‹ä¸€ï¼Œåœ¨å„ç§æµ‹è¯•æ¡ˆä¾‹ä¸­å‡»è´¥äº† GPT\\-4o å’Œ Llama 3ã€‚å®ƒå¯ä»¥ä»Žæ–‡æ¡£å’Œ PDF ä¸­æå–æ•°æ®å’Œæ–‡æœ¬ã€‚è¿™äº›åªæ˜¯ä½¿ Claude 3\\.5 Sonnet æˆä¸ºæœ€ä½³çš„å‡ ä¸ªå› ç´ ï¼Œä½† Anthropic è¿˜æ–°å¢žäº†ä¸€ä¸ªåŠŸèƒ½ï¼Œä½¿è¯¥ LLM æˆä¸ºæœ€ä½³ä»£ç ç”Ÿæˆå™¨ï¼Œâ€œArtifactsâ€ã€‚è¿™æ˜¯ä¸€ä¸ªå¼¹å‡ºçª—å£ï¼Œå…è®¸ç”¨æˆ·æŸ¥çœ‹ä»–ä»¬çš„ä»£ç ã€ç¼–è¾‘ä»£ç ï¼Œå¹¶åœ¨è¯¥å¼¹å‡ºçª—å£ä¸­å®žæ—¶æŸ¥çœ‹ä»–ä»¬çš„é¡¹ç›®ã€‚æœ¬æ–‡å°†å±•ç¤º Artifacts å’Œ Claude 3\\.5 Sonnet çš„é¡¶çº§æ¡ˆä¾‹ï¼Œä»¥åŠæ‚¨å¯ä»¥ä½¿ç”¨æ­¤å·¥å…·å¼€å‘çš„æ–°é¡¹ç›®ã€‚\n\n\n\n## Claude 3\\.5 Sonnet çš„ä½¿ç”¨æ¡ˆä¾‹\n\nè¦ä½¿ç”¨è¿™äº›å·¥ä»¶ï¼Œæ‚¨å¿…é¡»æ‹¥æœ‰ä¸€ä¸ª Claude å¸æˆ·ã€‚Claude 3\\.5 Sonnet å¯¹æ‰€æœ‰äººå…è´¹ï¼Œä½†æ¯å¤©ä»…å…è®¸æœ‰é™çš„èŠå¤©ã€‚ç”¨æˆ·å¿…é¡»è´­ä¹°å¤§çº¦ $20 çš„è®¢é˜…ï¼Œä»¥ä¾¿æ— é™åˆ¶ä½¿ç”¨è¯¥æ¨¡åž‹è¿›è¡ŒèŠå¤©ã€‚åœ¨ Claude 3\\.5 Sonnet ä¸­å°è¯•è¿™äº›æç¤ºå¹¶åˆ›å»ºä»¤äººå…´å¥‹çš„å·¥å…·ã€‚ç”¨æˆ·è¿˜å¯ä»¥é€šè¿‡åˆ†äº«é“¾æŽ¥åœ¨äº’è”ç½‘ä¸Šå‘å¸ƒä»–ä»¬çš„é¡¹ç›®ã€‚\n\n### 1\\. äº’åŠ¨PDFä»ªè¡¨æ¿\n\né˜…è¯»å¤§åž‹PDFæ–‡ä»¶å¾ˆæ— èŠã€‚å› æ­¤ï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªäº’åŠ¨PDFä»ªè¡¨æ¿ï¼Œä»¥ä¾¿æˆ‘èƒ½æ›´å¥½åœ°é˜…è¯»æˆ‘çš„PDFæ–‡ä»¶ã€‚\n\n***æç¤º\\- åˆ›å»ºä¸€ä¸ªäº’åŠ¨PDFä»ªè¡¨æ¿ï¼Œå¸®åŠ©æˆ‘ä»¥æ›´å…·è§†è§‰å¸å¼•åŠ›çš„æ–¹å¼æŸ¥çœ‹ã€é˜…è¯»å’Œå­¦ä¹ è¿™äº›ä¿¡æ¯ã€‚å®ƒæœ‰ä¸€ä¸ªæ ‡ç­¾ï¼Œå¯ä»¥æ ¹æ®PDFä¸­çš„ä¿¡æ¯è¿›è¡Œæµ‹éªŒã€‚ç¡®ä¿å®ƒå…·æœ‰æš—æ¨¡å¼ã€‚***\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*66Iowbu8ZKA-IGfK)\n\nè¦ä½¿ç”¨æ­¤æç¤ºï¼Œé¦–å…ˆé™„ä¸Šæ‚¨æƒ³è¦é˜…è¯»æ‘˜è¦çš„PDFå’Œæç¤ºã€‚ç”¨æˆ·è¿˜å¯ä»¥æ ¹æ®è‡ªå·±çš„å–œå¥½è¿›è¡Œæ›´æ”¹ï¼ŒClaude 3\\.5 Sonnetå°†æ ¹æ®æ›´æ”¹é‡æ–°ç”Ÿæˆä»£ç ã€‚\n\n[***ç‚¹å‡»è¿™é‡ŒæŸ¥çœ‹é¡¹ç›®***](https://claude.site/artifacts/4b9590a8-260e-476a-ab75-ec1d69f81d1e)***.***\n\n### 2\\. ä½¿ç”¨åŠ¨ç”»å¯è§†åŒ–ä»»ä½•äº‹ç‰©\n\nå¯è§†åŒ–æ‚¨æ‰€é˜…è¯»çš„ä»»ä½•å†…å®¹å¯èƒ½æ˜¯ç†è§£å®ƒçš„æœ€ä½³æ–¹å¼ã€‚æ·»åŠ åŠ¨ç”»å°†ä½¿æ‚¨çš„å¯è§†åŒ–æ›´ä¸Šä¸€å±‚æ¥¼ï¼Œä½¿å…¶æ˜“äºŽç†è§£ã€‚\n\n***æç¤ºï¼šä¸ºæ¯ä¸ªè¿‡ç¨‹æ­¥éª¤æ·»åŠ åŠ¨ç”»ï¼Œä»¥åˆ›å»ºå…‰åˆä½œç”¨çš„å¯è§†åŒ–ã€‚ç¡®ä¿ä½¿ç”¨ä¸åŒçš„é¢œè‰²å’Œçº¹ç†ä½¿æ•´ä¸ªä»ªè¡¨æ¿åœ¨è§†è§‰ä¸Šå¸å¼•äººï¼Œä»¥åˆ›å»ºé«˜è´¨é‡çš„åŠ¨ç”»ã€‚***\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*W5lWitTO5fj7lOs0)\n\nè¿™æ˜¯æˆ‘ä»¬å¾—åˆ°çš„ç¬¬ä¸€ä¸ªè¾“å‡ºï¼Œåœ¨10ç§’çš„å¤„ç†æ—¶é—´å†…ä»¤äººå°è±¡æ·±åˆ»ï¼Œä½†é€šè¿‡æä¾›æ›´å¤šçš„æŒ‡ä»¤å’Œç²¾ç¡®åº¦æ¥ä¼˜åŒ–æç¤ºåŽå¯ä»¥æ›´å¥½ã€‚æˆ‘ä»¬å¾—åˆ°\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*TAguaCWhC1CYFNw7)\n\næ­£å¦‚æ‚¨æ‰€çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬å°†æ•´ä¸ªå¾ªçŽ¯åˆ†ä¸ºä¸åŒçš„é˜¶æ®µï¼Œæœ€ç»ˆç»“æžœä»¤äººéš¾ä»¥ç½®ä¿¡ã€‚è¿™ç§æ–¹æ³•å¯ä»¥é€šè¿‡å°†ä»»ä½•è¿‡ç¨‹åˆ†è§£ä¸ºæ›´ç®€å•çš„æ­¥éª¤æ¥å¸®åŠ©æ‚¨ç†è§£ã€‚\n\n[***ç‚¹å‡»è¿™é‡ŒæŸ¥çœ‹é¡¹ç›®***](https://claude.site/artifacts/b9769e4e-c20b-43da-945a-bfc41782900c)***.***\n\n### 3\\. ç§‘å­¦å·¥å…·\n\nç§‘å­¦å·¥å…·æ˜¯å¸®åŠ©ç”¨æˆ·å¿«é€Ÿå’Œç›´è§‚ç†è§£ç§‘å­¦æ¦‚å¿µçš„å·¥å…·å’ŒæœåŠ¡ã€‚åœ¨ Claude 3\\.5 Sonnet çš„å¸®åŠ©ä¸‹ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡å•ä¸ªæç¤ºåˆ›å»ºç§‘å­¦å·¥å…·ã€‚åªéœ€è¯¦ç»†æè¿°æ‚¨çš„å·¥å…·ã€‚æˆ‘å°†åˆ©ç”¨å®ƒä¸ºç”µå­å·¥ç¨‹åˆ›å»ºä¸€ä¸ªäº¤äº’å¼å·¥å…·ã€‚\n\n***æç¤º â€” ä½¿ç”¨ React åˆ›å»ºä¸€ä¸ªæ˜¾ç¤ºäºŒæžç®¡çš„ä»ªè¡¨æ¿ã€‚æˆ‘ä»¬å¯ä»¥åœ¨ç”µè·¯ä¸­è¿žæŽ¥æ­£å‘åç½®æˆ–åå‘åç½®ï¼Œå¹¶æ ¹æ®æŽºæ‚å’Œè€—å°½åŒºç­‰ç‰©ç†å› ç´ ï¼Œé€šè¿‡æ˜¾ç¤ºäºŒæžç®¡ä¸­å­”å’Œç”µå­çš„è¿åŠ¨æ¥æ”¹å˜åœºæ™¯ã€‚å¯ä»¥æ›´æ”¹æ‰€æœ‰å†…å®¹ï¼Œå¦‚ç”µåŽ‹ã€ç”µæµã€äºŒæžç®¡ç±»åž‹ã€Si æˆ– Geï¼Œå¹¶ä½¿ç”¨ä¸åŒçš„é¢œè‰²å’ŒåŠ¨æ€å…ƒç´ è¿›è¡ŒåŠ¨ç”»ã€‚***\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*9bE50q8nsh6_SQ36)\n\nç”¨æˆ·å¯ä»¥ä½¿ç”¨ Claude 3\\.5 Sonnet åˆ›å»ºå¤šä¸ªå·¥å…·ï¼Œä¾‹å¦‚åŸºæœ¬é¡¹ç›®å­¦æ ¡ã€ç£åœºçš„ç”µæµæ•ˆåº”ã€åŽ‹åŠ›ä¸Žæ¸©åº¦å…³ç³»å·¥å…·ã€åŽŸå­æ¨¡åž‹åŠ¨ç”»ç­‰ã€‚\n\n***ç‚¹å‡» [è¿™é‡ŒæŸ¥çœ‹é¡¹ç›®](https://claude.site/artifacts/5b18fb27-1093-41d2-ac4e-54e47e4ddd3a)ã€‚***\n\n### 4\\. æ¸¸æˆå¼€å‘\n\næ¯ä¸ªäººéƒ½å–œæ¬¢çŽ©æ¸¸æˆã€‚ä½¿ç”¨ Claude 3\\.5 Sonnetï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸åŒç±»åž‹çš„æ¸¸æˆã€‚æˆ‘å°†ä½¿ç”¨ LLM åˆ¶ä½œä¸€ä¸ªäº•å­—æ£‹æ¸¸æˆã€‚\n\n***æç¤º\\- ä½¿ç”¨ React åˆ›å»ºä¸€ä¸ªäº•å­—æ£‹æ¸¸æˆï¼Œå¹¶ä½¿å…¶åŠŸèƒ½æ­£å¸¸ã€‚é€šè¿‡ä½¿ç”¨ CSS ä½¿å…¶åœ¨è§†è§‰ä¸Šæ›´å…·å¸å¼•åŠ›ã€‚***\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*jlSn7mCbaFadUT3F)\n\nç»“æžœå¯ä»¥æ›´å¥½ã€‚ä¹‹åŽï¼Œæˆ‘å°è¯•æä¾›æ›´å¤šå»ºè®®å’Œè¾“å…¥ï¼Œå¹¶æ ¹æ®æˆ‘çš„å»ºè®®ï¼ŒClaude å¼€å‘äº†ä¸€ä¸ªå®Œç¾Žçš„äº•å­—æ£‹æ¸¸æˆï¼Œç»“æžœå¦‚ä¸‹ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*4v9KP4KFmQgY14YZ)\n\nå¦‚æ‚¨æ‰€è§ï¼Œæœ€ç»ˆäº§å“ä¼˜åŒ–äº† CSSã€‚åŠ¨ç”»æœ‰äº†å¾ˆå¤§æ”¹å–„ï¼Œå¹¶åœ¨æ¸¸æˆç»“æŸæ—¶æ·»åŠ äº†ä¸€ä¸ªå¼¹å‡ºçª—å£ï¼Œæ˜¾ç¤ºèŽ·èƒœè€…çš„åå­—ã€‚æ‚¨å¯ä»¥ä½¿ç”¨è¿™äº›æç¤ºåˆ¶ä½œè¯¸å¦‚è´ªåƒè›‡ã€é£žè¡Œæ£‹ã€çŸ³å¤´å‰ªåˆ€å¸ƒç­‰æ¸¸æˆã€‚\n\n[***ç‚¹å‡»è¿™é‡Œå°è¯•è¯¥é¡¹ç›®ã€‚***](https://claude.site/artifacts/d57fdf93-79fb-443a-9bee-a58ab6eb911f)\n\n### 5\\. ç½‘ç»œåº”ç”¨\n\nç½‘ç»œåº”ç”¨æ˜¯å¯ä»¥ç›´æŽ¥åœ¨æµè§ˆå™¨ä¸Šè®¿é—®çš„å·¥å…·ï¼Œæ— éœ€åœ¨æ‰‹æœºä¸Šå®‰è£…åº”ç”¨ç¨‹åºã€‚Claude 3\\.5 Sonnet å¯ç”¨äºŽå¼€å‘ä¸åŒç±»åž‹çš„ç½‘ç»œåº”ç”¨ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†ç ”ç©¶ä¸€ä¸ªè´¹ç”¨è·Ÿè¸ªåº”ç”¨ã€‚\n\n***æç¤º\\- åˆ›å»ºä¸€ä¸ªè´¹ç”¨è·Ÿè¸ªçš„ç½‘ç»œåº”ç”¨ï¼Œå…·å¤‡ä»¥ä¸‹åŠŸèƒ½ï¼šé¦–å…ˆï¼Œè¯¢é—®ä»–ä»¬çš„æœˆæ”¯å‡ºâ€”â€”æ¯”å¦‚è¯´ 2000 å¢æ¯”ã€‚çŽ°åœ¨ï¼Œæ— è®ºè¿™ä¸ªäººèŠ±è´¹ä»€ä¹ˆï¼Œè¯·ç¡®ä¿åˆ›å»ºä¸€äº›ç±»åˆ«ï¼Œå¦‚é£Ÿå“ã€æ—…è¡Œå’Œå¿…éœ€å“ï¼Œå¹¶æä¾›æ·»åŠ ä»»ä½•å†…å®¹çš„é€‰é¡¹ã€‚åœ¨æœˆæœ«æ·»åŠ ä¸€ä¸ªèŠ‚çœèµ„é‡‘çš„é€‰é¡¹ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜æœ‰æŠ•èµ„é€‰é¡¹ï¼ŒçŽ°åœ¨ä½ çŸ¥é“æ˜¯ä»€ä¹ˆç±»åž‹çš„ã€‚ä¾‹å¦‚ï¼Œâ€œå¦‚æžœä½ æ¯æœˆåœ¨å…±åŒåŸºé‡‘ä¸­æŠ•èµ„ 200ï¼Œäº”å¹´åŽä½ å°†æ‹¥æœ‰ X é‡‘é¢ã€‚â€æ­¤å¤–ï¼Œè¯¥åº”ç”¨è¿˜æœ‰ç”Ÿæˆä¸Žæ”¯å‡ºç›¸å…³çš„å›¾è¡¨å’Œå›¾å½¢çš„åŠŸèƒ½ã€‚ç¡®ä¿è¯¥åº”ç”¨çš„ UI/UX è®¾è®¡è§†è§‰ä¸Šå¸å¼•äººã€‚***\n\nè¿™æ˜¯æœ€ç»ˆç»“æžœã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*XGOwT3vpxL0dxzzP)\n\nç”¨æˆ·å¯ä»¥ä½¿ç”¨ Claude 3\\.5 Sonnet åˆ›å»ºå¤šä¸ªå·¥å…·ï¼Œå¦‚å¾…åŠžäº‹é¡¹åˆ—è¡¨ã€ç®€å•è®¡ç®—å™¨ã€ç”µå½±æŽ¨èåº”ç”¨ã€æ–‡æœ¬æ‘˜è¦å·¥å…·ç­‰ã€‚\n\n[***ç‚¹å‡»è¿™é‡Œå°è¯•è¯¥é¡¹ç›®***](https://claude.site/artifacts/66770d05-aafe-45c4-b938-8cda0e82b903)***.***\n\n### 6\\. 3Dæ¨¡æ‹Ÿ\n\nClaude 3\\.5 Sonentå¯ä»¥åˆ›å»ºæ¨¡åž‹æˆ–é¡¹ç›®çš„3Dæ¨¡æ‹Ÿï¼Œæ— è®ºæ˜¯å¤ªé˜³ç³»æ¨¡åž‹ã€åŽŸå­æ¨¡åž‹ã€åˆ†å­ç”Ÿç‰©å­¦çš„ä¸­å¿ƒæ³•åˆ™ç­‰ã€‚æ‚¨å¯ä»¥ä½¿ç”¨è¿™ä¸ªLLMåœ¨3Dä¸­å¯è§†åŒ–ä»»ä½•æƒ³æ³•ã€‚æˆ‘ä»¬å°†æ¨¡æ‹Ÿä¸€ä¸ª3Då¤ªé˜³ç³»æ¨¡åž‹ï¼Œæ‰€æœ‰è¡Œæ˜Ÿå›´ç»•å¤ªé˜³æ—‹è½¬ã€‚\n\n***æç¤º\\-åˆ¶ä½œä¸€ä¸ªåŒ…å«å¤ªé˜³ç³»é‡åŠ›çš„ç©ºé—´æ¨¡æ‹Ÿçš„three jsåº”ç”¨ç¨‹åºï¼Œæ‰€æœ‰è¡Œæ˜Ÿåœ¨ä¸€ä¸ªç½‘é¡µæ–‡ä»¶ä¸­ã€‚å¤ªé˜³ç³»ä¸­è¡Œæ˜Ÿå›´ç»•å¤ªé˜³æ—‹è½¬â€¦â€¦æ¯å½“æœ‰äººæ‚¬åœé¼ æ ‡æ—¶ï¼Œä¼šå¼¹å‡ºè¡Œæ˜Ÿçš„åç§°å’ŒåŸºæœ¬ç»†èŠ‚ï¼Œå¦‚è´¨é‡ã€é‡åŠ›ç­‰ï¼›ä¿æŒç‰©ç†æ¦‚å¿µï¼Œå¦‚è¡Œæ˜Ÿçš„è‡ªè½¬å’Œå…¬è½¬ï¼Œå®Œæ•´ã€‚***\n\nä½¿ç”¨è¿™ä¸ªæç¤ºä»¥èŽ·å¾—æ‰€éœ€çš„ç»“æžœã€‚\n\n***ç‚¹å‡»è¿™é‡ŒæŸ¥çœ‹é¡¹ç›®ã€‚***\n\n[***é¡¹ç›® 1***](https://x.com/websim_ai/status/1803901523522699730?t=BCe28ywbC2xD1Mk4DRmo5w&s=08)\n\n[***é¡¹ç›® 2***](https://x.com/ammaar/status/1804649903815115053?t=7PeWPg62bkABtKEtKVmFbw&s=08)\n\n[***é¡¹ç›® 3***](https://x.com/goldcaddy77/status/1804724702901891313?t=iqcLQBhYaIRnt3gIBRKBQg&s=08)\n\n### 7\\. æ€ç»´å¯¼å›¾\n\næ€ç»´å¯¼å›¾æ˜¯ä¸€ç§å¯è§†åŒ–ç»„ç»‡ä¿¡æ¯çš„å¤´è„‘é£Žæš´æŠ€æœ¯ï¼Œé‡‡ç”¨å±‚æ¬¡ç»“æž„ã€‚å…¶ä¸»è¦ç‰¹ç‚¹æ˜¯å°†ä¸€ä¸ªä¸»è¦æ€æƒ³ä½œä¸ºå›¾è¡¨çš„ä¸­å¿ƒç‚¹ï¼Œå­ä¸»é¢˜ä»Žä¸­å¿ƒç‚¹åˆ†æ”¯å‡ºæ¥å¹¶è¿žæŽ¥åˆ°æ”¯æŒæ€æƒ³ã€‚è¿™æœ‰åŠ©äºŽè¯»è€…å¿«é€Ÿè®°å¿†å’Œç†è§£ä¿¡æ¯ã€‚è®©æˆ‘ä»¬é€šè¿‡åˆ›å»ºæ€ç»´å¯¼å›¾æ¥ç”Ÿæˆä¸€ä¸ªæç¤ºã€‚\n\n***æç¤ºï¼šåˆ›å»ºä¸€ä¸ªå…³äºŽç‰©ä½“æ£€æµ‹å¦‚ä½•å·¥ä½œçš„æ€ç»´å¯¼å›¾ã€‚ä½¿ç”¨åŠ¨ç”»å’Œé¢œè‰²ä½¿å…¶äº’åŠ¨æ€§å¼ºä¸”è§†è§‰ä¸Šå¸å¼•äººã€‚***\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*igUaLzf8b2ZTEy-m)\n\né€šè¿‡æ›´æ”¹ä¸»é¢˜ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æ­¤æç¤ºç”Ÿæˆæ‚¨æƒ³è¦çš„ä»»ä½•æ€ç»´å¯¼å›¾ã€‚æ‚¨è¿˜å¯ä»¥æ·»åŠ ä¸åŒçš„é¢œè‰²å’ŒåŠ¨ç”»ï¼Œä½¿å…¶æ›´åŠ äº’åŠ¨ã€‚\n\n[***ç‚¹å‡»è¿™é‡Œå°è¯•è¯¥é¡¹ç›®***](https://claude.site/artifacts/f1ce9002-9434-4b40-9713-ef184c467557)\n\n### 8\\. SEOå·¥å…·\n\nSEOå¯¹äºŽä»»ä½•åšå®¢æ–‡ç« åœ¨Googleä¸Šçš„æŽ’åè‡³å…³é‡è¦ã€‚ä½¿ç”¨Claudeï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªå·¥å…·ï¼Œå¸®åŠ©æˆ‘ä»¬æ”¹å–„ç½‘ç«™çš„SEOã€‚æ‚¨å¯ä»¥ä½¿ç”¨è¿™ä¸ªæç¤ºæ¥åˆ¶ä½œæ‚¨çš„å·¥å…·ã€‚\n\n***æç¤º\\- åˆ›å»ºä¸€ä¸ªSEOå·¥å…·ï¼Œå…è®¸æˆ‘ä¸Šä¼ æˆ‘çš„åšå®¢æ–‡ç« ã€è¡Œä¸šå’Œæˆ‘è¯•å›¾æŽ’åçš„å…³é”®å­—ã€‚åœ¨æˆ‘ä¸Šä¼ æ‰€æœ‰è¿™äº›å†…å®¹åŽï¼Œæˆ‘å¸Œæœ›ç‚¹å‡»ä¸€ä¸ªæŒ‰é’®ï¼Œç»™å‡ºçªå‡ºæ˜¾ç¤ºçš„å»ºè®®ï¼Œè¯´æ˜Žéœ€è¦æ›´æ”¹çš„å†…å®¹ã€‚åŒ…æ‹¬ä¸€ä¸ªé‡ç½®æŒ‰é’®ï¼Œé‡æ–°å¼€å§‹è¿™ä¸ªè¿‡ç¨‹ã€‚å¹¶ä¸”åœ¨å…³é”®å­—æ—è¾¹æ·»åŠ ä¸€ä¸ªåŠ å·æŒ‰é’®ï¼Œæˆ‘å¯ä»¥æ·»åŠ å¤šä¸ªã€‚åœ¨SEOå»ºè®®åŒºåŸŸï¼Œåˆ†æžSEOåŽï¼Œç»™å‡ºå…·ä½“çš„æ•°å­—ã€æ¥è‡ªæˆ‘çš„åšå®¢æ–‡ç« çš„ç»Ÿè®¡æ•°æ®ä»¥åŠè­¦å‘Šæ ‡å¿—ï¼Œä»¥æ˜¾ç¤ºéœ€è¦å…³æ³¨çš„å†…å®¹ã€‚åœ¨æˆ‘ç”Ÿæˆåšå®¢æ–‡ç« åŽï¼ŒåŸºäºŽå…³é”®å­—ç»™å‡ºä¿®è®¢åŽçš„åšå®¢æ–‡ç« å»ºè®®ã€‚æˆ‘ä¼šå¯¹å…¶è¿›è¡ŒæŽ’åï¼Œå¹¶è§£é‡Šä¸ºä»€ä¹ˆä¹Ÿæå‡ºè¿™ä¸ªå»ºè®®ã€‚é€šè¿‡ä½¿CSSæ›´åŠ é«˜çº§å¹¶æ·»åŠ ä¸€ä¸ªæ‚¨è®¤ä¸ºå¿…è¦çš„ç‹¬ç‰¹åŠŸèƒ½ï¼Œä½¿å…¶æ›´å…·äº’åŠ¨æ€§ï¼Œä»¥å¸®åŠ©åšå®¢ä½œè€…ã€‚ç¡®ä¿è¯¥å·¥å…·å…·æœ‰è§†è§‰ä¸Šå¸å¼•äººçš„UI/UXè®¾è®¡ã€‚***\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*OfY0zIa4rzO7J55r)\n\nç”¨æˆ·å¯ä»¥åˆ›å»ºå„ç§å·¥å…·ï¼Œä¸ä»…å¸®åŠ©ä»–ä»¬è¿›è¡ŒSEOï¼Œè¿˜å¯ä»¥åœ¨å„ä¸ªé¢†åŸŸæé«˜ç”Ÿäº§åŠ›ã€‚\n\n[***ç‚¹å‡»è¿™é‡Œå°è¯•è¯¥é¡¹ç›®***](https://claude.site/artifacts/09f21906-6295-4d0a-9fa0-be4afd6dab71)***.***\n\n### 9\\. ç›®æ ‡æ£€æµ‹å·¥å…·\n\næ‚¨è¿˜å¯ä»¥ä½¿ç”¨ Claude 3\\.5 Sonent åŸºäºŽäººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ è¿›è¡Œé¡¹ç›®å¼€å‘ã€‚æ‚¨å¯ä»¥å°è¯•é€šè¿‡æ·»åŠ åŠŸèƒ½æ¥åˆ›å»ºä»¤äººæƒŠå¹çš„é¡¹ç›®ã€‚\n\n***æç¤º - åˆ›å»ºä¸€ä¸ªç”¨äºŽå®žæ—¶ç›®æ ‡æ£€æµ‹çš„å•ä¸€ HTML æ–‡ä»¶ç½‘ç»œåº”ç”¨ç¨‹åºã€‚ä½¿ç”¨ TensorFlow.js å’Œ COCO-SSD æ¨¡åž‹***\n\n***è¯¥åº”ç”¨ç¨‹åºåº”ï¼š***\n\n* ***è®¿é—®ç”¨æˆ·çš„ç½‘ç»œæ‘„åƒå¤´å¹¶æ˜¾ç¤ºè§†é¢‘æµã€‚***\n* ***å¯¹è§†é¢‘æµè¿›è¡Œå®žæ—¶ç›®æ ‡æ£€æµ‹ã€‚***\n* ***åœ¨æ£€æµ‹åˆ°çš„å¯¹è±¡å‘¨å›´ç»˜åˆ¶è¾¹ç•Œæ¡†ï¼Œå¹¶ç”¨å…¶ç±»åˆ«å’Œæ£€æµ‹ç½®ä¿¡åº¦æ ‡è®°å®ƒä»¬ã€‚***\n* ***åœ¨è§†é¢‘æµä¸‹æ–¹æ˜¾ç¤ºå”¯ä¸€æ£€æµ‹åˆ°çš„å¯¹è±¡åˆ—è¡¨ï¼Œæ˜¾ç¤ºå¯¹è±¡ç±»åˆ«åŠå…¶é¦–æ¬¡æ£€æµ‹æ—¶é—´ã€‚***\n* ***ç¡®ä¿æ¯ä¸ªå¯¹è±¡ç±»åˆ«åªåˆ—å‡ºä¸€æ¬¡ï¼Œæ— è®ºå…¶è¢«æ£€æµ‹çš„é¢‘çŽ‡å¦‚ä½•ã€‚***\n* ***ä½¿ç”¨ 2 FPS çš„æ£€æµ‹é¢‘çŽ‡ä»¥å¹³è¡¡æ€§èƒ½å’Œå“åº”èƒ½åŠ›ã€‚***\n* ***åŒ…å«å¯¹æ‘„åƒå¤´è®¿é—®å’Œæ¨¡åž‹åŠ è½½çš„é”™è¯¯å¤„ç†ã€‚***\n* ***ä¸ºåº”ç”¨ç¨‹åºè®¾è®¡ä¸€ä¸ªå¹²å‡€ã€çŽ°ä»£çš„å¤–è§‚ï¼Œå¹¶å…·æœ‰å“åº”å¼è®¾è®¡ã€‚***\n* ***åœ¨ä¸€ä¸ªè‡ªåŒ…å«çš„ HTML æ–‡ä»¶ä¸­åŒ…å«æ‰€æœ‰å¿…è¦çš„ HTMLã€CSS å’Œ JavaScriptã€‚***\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*QSvMcxCuulkRKOlm)\n\n## ç»“è®º\n\nClaude 3\\.5 Sonnetï¼ŒAnthropic æœ€æ–°çš„ LLMï¼Œå‡­å€Ÿå…¶å…ˆè¿›çš„åŠŸèƒ½å’Œå¤šæ ·åŒ–çš„åº”ç”¨ï¼Œå½»åº•æ”¹å˜äº† AI èƒ½åŠ›ã€‚è¯¥æ¨¡åž‹ä»¤äººå°è±¡æ·±åˆ»çš„ 200K ä¸Šä¸‹æ–‡çª—å£ã€å“è¶Šçš„è§†è§‰èƒ½åŠ›ä»¥åŠç”¨äºŽä»£ç ç”Ÿæˆçš„åˆ›æ–°â€œArtifactsâ€åŠŸèƒ½ä½¿å…¶åœ¨ç«žäº‰å¯¹æ‰‹ä¸­è„±é¢–è€Œå‡ºã€‚æœ¬æ–‡å±•ç¤ºäº† Claude 3\\.5 Sonnet çš„å¤šç§ä½¿ç”¨æ¡ˆä¾‹ï¼Œè¯æ˜Žäº†å®ƒåœ¨åˆ›å»ºäº¤äº’å¼ä»ªè¡¨æ¿ã€å¯è§†åŒ–ã€ç§‘å­¦å·¥å…·ã€æ¸¸æˆã€Web åº”ç”¨ç¨‹åºã€3D æ¨¡æ‹Ÿã€æ€ç»´å¯¼å›¾å’Œ SEO å·¥å…·æ–¹é¢çš„æ½œåŠ›ã€‚è¿™äº›ä¾‹å­çªæ˜¾äº†è¯¥æ¨¡åž‹åœ¨ç®€å•æç¤ºä¸‹ç”Ÿæˆå¤æ‚ã€åŠŸèƒ½é½å…¨ä¸”è§†è§‰å¸å¼•äººçš„é¡¹ç›®çš„èƒ½åŠ›ã€‚Claude 3\\.5 Sonnet å‹å¥½çš„ç”¨æˆ·ç•Œé¢å’Œå¼ºå¤§çš„èƒ½åŠ›ä½¿å…¶æˆä¸ºå¼€å‘äººå‘˜ã€æ•™è‚²å·¥ä½œè€…å’Œå„ä¸ªé¢†åŸŸä¸“ä¸šäººå£«çš„å®è´µå·¥å…·ï¼Œä¸º AI è¾…åŠ©åˆ›ä½œå’Œé—®é¢˜è§£å†³å¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ã€‚\n\n"},{"lang":"zh","group":"blog","slug":"blog/bolt-new-and-ollama-revolutionizing-ai-powered-full-stack-web-development-2aa6aadf5958","frontmatter":{"title":"Bolt.new å’Œ Ollamaï¼šé©æ–°äººå·¥æ™ºèƒ½é©±åŠ¨çš„å…¨æ ˆå¼ Web å¼€å‘","meta_title":"Bolt.new å’Œ Ollamaï¼šé©æ–°äººå·¥æ™ºèƒ½é©±åŠ¨çš„å…¨æ ˆå¼ Web å¼€å‘","description":"Bolt.newæ˜¯ä¸€æ¬¾åˆ›æ–°çš„AIé©±åŠ¨å…¨æ ˆWebå¼€å‘å·¥å…·ï¼Œèƒ½åœ¨æµè§ˆå™¨ä¸­ç®€åŒ–åº”ç”¨ç¨‹åºçš„æž„å»ºä¸Žéƒ¨ç½²ã€‚ç»“åˆOllamaï¼Œç”¨æˆ·å¯ä»¥åœ¨æœ¬åœ°è¿è¡Œå¼€æºAIæ¨¡åž‹ï¼Œé™ä½Žæˆæœ¬å¹¶æå‡çµæ´»æ€§ã€‚Bolt.newé€šè¿‡å…¨é¢æŽ§åˆ¶å¼€å‘çŽ¯å¢ƒï¼Œæ”¯æŒä»Žç®€å•ç½‘é¡µåˆ°å¤æ‚é‡‘èžæœåŠ¡åº”ç”¨çš„å¼€å‘ï¼Œæžå¤§åœ°æé«˜äº†å¼€å‘æ•ˆçŽ‡å’Œä¾¿æ·æ€§ã€‚","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*vbo04xVLorq_rvpeEDCaAg.jpeg","categories":["Programming","Technology/Web","Data Science"],"author":"Rifx.Online","tags":["Bolt","Ollama","browser","deployment","web"],"draft":false,"slug":"blog/bolt-new-and-ollama-revolutionizing-ai-powered-full-stack-web-development-2aa6aadf5958"},"content":"\n\n\n\n\nåœ¨å¿«é€Ÿå‘å±•çš„ Web å¼€å‘ä¸–ç•Œä¸­ï¼Œæ•ˆçŽ‡å’Œåˆ›æ–°è‡³å…³é‡è¦ã€‚å¼€å‘è€…ã€é¡¹ç›®ç»ç†å’Œè®¾è®¡å¸ˆä»¬éƒ½åœ¨ä¸æ–­å¯»æ‰¾èƒ½å¤Ÿç®€åŒ–å·¥ä½œæµç¨‹ã€é™ä½Žæˆæœ¬å’Œæé«˜ç”Ÿäº§åŠ›çš„å·¥å…·ã€‚**Bolt.new** æ˜¯ä¸€æ¬¾çªç ´æ€§çš„ AI é©±åŠ¨å…¨æ ˆ Web å¼€å‘ä»£ç†ï¼Œå®Œå…¨åœ¨æ‚¨çš„æµè§ˆå™¨ä¸­è¿è¡Œã€‚ä¸Ž **Ollama** é…åˆä½¿ç”¨ï¼ŒåŽè€…å…è®¸æ‚¨åœ¨æœ¬åœ°è¿è¡Œå¼€æº AI æ¨¡åž‹ï¼ŒBolt.new å°†å½»åº•æ”¹å˜æˆ‘ä»¬æž„å»ºå’Œéƒ¨ç½² Web åº”ç”¨ç¨‹åºçš„æ–¹å¼ã€‚æœ¬æ–‡å°†æ·±å…¥æŽ¢è®¨ Bolt.newã€å®ƒä¸Ž Ollama çš„é›†æˆï¼Œå¹¶æä¾›ä¸€ä¸ªå…¨é¢çš„å…¥é—¨æŒ‡å—ã€‚\n\n## ç›®å½•\n\n1. Bolt.new ç®€ä»‹\n2. Bolt.new çš„ç‹¬ç‰¹ä¹‹å¤„\n* æµè§ˆå™¨ä¸­çš„å…¨æ ˆå¼€å‘\n* å…·å¤‡çŽ¯å¢ƒæŽ§åˆ¶çš„ AI\n\n3\\. å°† Bolt.new ä¸Ž Ollama é›†æˆ\n\n* ä¸ºä»€ä¹ˆä½¿ç”¨ Ollamaï¼Ÿ\n* å®‰è£…ä¸Žè®¾ç½®\n\n4\\. å®‰è£…æŒ‡å—é€æ­¥è§£æž\n\n* å‰ç½®æ¡ä»¶\n* å…‹éš†ä»£ç åº“\n* é…ç½®çŽ¯å¢ƒå˜é‡\n* å®‰è£…ä¾èµ–\n* è¿è¡Œåº”ç”¨ç¨‹åº\n\n5\\. ä½¿ç”¨ Docker è¿è¡Œ Bolt.new\n\n6\\. å®žé™…æ¼”ç¤º\n\n* åˆ›å»ºä¸€ä¸ªç®€å•çš„ç½‘é¡µ\n* æž„å»ºä¸€ä¸ªè´ªåƒè›‡æ¸¸æˆ\n* å¼€å‘å…¨æ ˆé‡‘èžæœåŠ¡ç½‘é¡µåº”ç”¨\n\n7\\. æœ€å¤§åŒ–åˆ©ç”¨ Bolt.new çš„æŠ€å·§ä¸Žçªé—¨\n\n## Bolt.newç®€ä»‹\n\nBolt.new æ˜¯ä¸€æ¬¾åˆ›æ–°å·¥å…·ï¼Œæ—¨åœ¨ç®€åŒ–æž„å»ºå…¨æ ˆ web åº”ç”¨ç¨‹åºçš„è¿‡ç¨‹ã€‚é€šè¿‡åˆ©ç”¨å…ˆè¿›çš„ AI æ¨¡åž‹ï¼ŒBolt.new å…è®¸ç”¨æˆ·ç›´æŽ¥ä»Žæµè§ˆå™¨ä¸­ **æç¤º**ã€**è¿è¡Œ**ã€**ç¼–è¾‘** å’Œ **éƒ¨ç½²** åº”ç”¨ç¨‹åºã€‚è¿™æ¶ˆé™¤äº†å¤æ‚çš„æœ¬åœ°è®¾ç½®éœ€æ±‚ï¼Œä½¿ web å¼€å‘å˜å¾—æ›´åŠ ä¾¿æ·å’Œé«˜æ•ˆã€‚\n\næ— è®ºæ‚¨æ˜¯ç»éªŒä¸°å¯Œçš„å¼€å‘äººå‘˜ã€è´Ÿè´£å¤šä¸ªé¡¹ç›®çš„é¡¹ç›®ç»ç†ï¼Œè¿˜æ˜¯å¸Œæœ›å¿«é€ŸåŽŸåž‹è®¾è®¡çš„è®¾è®¡å¸ˆï¼ŒBolt.new æä¾›äº†ä¸€ä¸ªå¤šåŠŸèƒ½å¹³å°ï¼Œä»¥æœ€å°çš„åŠªåŠ›å°†æ‚¨çš„æƒ³æ³•å˜ä¸ºçŽ°å®žã€‚\n\n## Bolt.new çš„ç‹¬ç‰¹ä¹‹å¤„\n\nè™½ç„¶æœ‰ä¼—å¤š AI æ¨¡åž‹å’Œå¼€å‘å·¥å…·ï¼Œä½† Bolt.new é€šè¿‡å…¶å…¨é¢çš„åŠŸèƒ½å’Œæ— ç¼çš„é›†æˆè„±é¢–è€Œå‡ºã€‚ä»¥ä¸‹æ˜¯ Bolt.new ç‹¬ç‰¹ä¹‹å¤„çš„è¯¦ç»†ä»‹ç»ï¼š\n\n## æµè§ˆå™¨ä¸­çš„å…¨æ ˆå¼€å‘\n\nBolt.new å°†æœ€å…ˆè¿›çš„ AI æ¨¡åž‹ä¸Žç”± [StackBlitz çš„ WebContainers](https://github.com/stackblitz/webcontainer-core) é©±åŠ¨çš„æµè§ˆå™¨å¼€å‘çŽ¯å¢ƒé›†æˆåœ¨ä¸€èµ·ã€‚è¿™ä¸€é›†æˆå®žçŽ°äº†ä¸€ç³»åˆ—åŠŸèƒ½ï¼š\n\n* **å®‰è£…å’Œè¿è¡Œ npm å·¥å…·å’Œåº“ï¼š** åˆ©ç”¨æµè¡Œæ¡†æž¶å¦‚ Viteã€Next.js ç­‰ï¼Œæ— éœ€ç¦»å¼€æµè§ˆå™¨ã€‚\n* **è¿è¡Œ Node.js æœåŠ¡å™¨ï¼š** æ— ç¼ç®¡ç†åŽç«¯æ“ä½œã€‚\n* **ä¸Žç¬¬ä¸‰æ–¹ API äº¤äº’ï¼š** é€šè¿‡é›†æˆå„ç§æœåŠ¡å¢žå¼ºåº”ç”¨ç¨‹åºçš„åŠŸèƒ½ã€‚\n* **é€šè¿‡èŠå¤©ç›´æŽ¥éƒ¨ç½²åˆ°ç”Ÿäº§çŽ¯å¢ƒï¼š** é€šè¿‡èŠå¤©ç•Œé¢ç›´æŽ¥å°†åº”ç”¨ç¨‹åºæŽ¨å‘ä¸Šçº¿ã€‚\n* **é€šè¿‡ URL åˆ†äº«å·¥ä½œï¼š** è½»æ¾ä¸Žåˆä½œè€…æˆ–åˆ©ç›Šç›¸å…³è€…åˆ†äº«é¡¹ç›®ã€‚\n\n## AIä¸ŽçŽ¯å¢ƒæŽ§åˆ¶\n\nä¸Žä¼ ç»Ÿå¼€å‘çŽ¯å¢ƒä¸­AIåŠ©æ‰‹ä»…é™äºŽä»£ç ç”Ÿæˆä¸åŒï¼ŒBolt.newä½¿AIæ¨¡åž‹èƒ½å¤Ÿ**å®Œå…¨æŽ§åˆ¶**å¼€å‘çŽ¯å¢ƒã€‚è¿™åŒ…æ‹¬ç®¡ç†æ–‡ä»¶ç³»ç»Ÿã€èŠ‚ç‚¹æœåŠ¡å™¨ã€åŒ…ç®¡ç†å™¨ã€ç»ˆç«¯å’Œæµè§ˆå™¨æŽ§åˆ¶å°ã€‚è¿™ç§å…¨é¢çš„æŽ§åˆ¶ä½¿AIä»£ç†èƒ½å¤Ÿå¤„ç†æ•´ä¸ªåº”ç”¨ç¨‹åºç”Ÿå‘½å‘¨æœŸâ€”â€”ä»Žåˆ›å»ºåˆ°éƒ¨ç½²â€”â€”æ˜¾è‘—ç®€åŒ–äº†å¼€å‘è¿‡ç¨‹ã€‚\n\n## å°† Bolt.new ä¸Ž Ollama é›†æˆ\n\nä¸ºäº†è¿›ä¸€æ­¥å¢žå¼º Bolt.new çš„åŠŸèƒ½å¹¶æä¾›æ›´å¤šçµæ´»æ€§ï¼Œä¸Ž **Ollama** çš„é›†æˆæ˜¯ä¸€ä¸ªæ”¹å˜æ¸¸æˆè§„åˆ™çš„ä¸¾æŽªã€‚\n\n## ä¸ºä»€ä¹ˆä½¿ç”¨ Ollamaï¼Ÿ\n\n**Ollama** å…è®¸æ‚¨åœ¨æœ¬åœ°æœºå™¨ä¸Šè¿è¡Œå¼€æº AI æ¨¡åž‹ã€‚è¿™ç§é›†æˆæä¾›äº†å‡ ä¸ªä¼˜åŠ¿ï¼š\n\n* **æˆæœ¬æ•ˆç›Šï¼š** é¿å…ä¸ºåŸºäºŽäº‘çš„ AI æ¨¡åž‹æ”¯ä»˜ä¸Žä»¤ç‰Œä½¿ç”¨ç›¸å…³çš„è´¹ç”¨ã€‚\n* **çµæ´»æ€§ï¼š** æ ¹æ®æ‚¨çš„åå¥½è®¿é—®å„ç§æ¨¡åž‹ï¼Œä»Ž Llama 3.2 Vision åˆ° Deep SE Coderã€‚\n* **éšç§å’ŒæŽ§åˆ¶ï¼š** æœ¬åœ°è¿è¡Œæ¨¡åž‹ä»¥ç»´æŠ¤æ•°æ®éšç§å¹¶æŽ§åˆ¶å¼€å‘çŽ¯å¢ƒã€‚\n\n## å®‰è£…å’Œè®¾ç½®\n\nå°† Ollama ä¸Ž Bolt.new é›†æˆæ¶‰åŠå‡ ä¸ªç®€å•çš„æ­¥éª¤ã€‚ä»¥ä¸‹æ˜¯å¸®åŠ©æ‚¨å…¥é—¨çš„è¯¦ç»†æŒ‡å—ã€‚\n\n## æ­¥éª¤\\-é€æ­¥å®‰è£…æŒ‡å—\n\n## å‰ææ¡ä»¶\n\nåœ¨ä½¿ç”¨ Ollama è®¾ç½® Bolt.new ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨çš„ç³»ç»Ÿä¸Šå·²å®‰è£…ä»¥ä¸‹å†…å®¹ï¼š\n\n1. **Git:** å…‹éš†ä»£ç åº“æ‰€å¿…éœ€çš„å·¥å…·ã€‚\n* [ä¸‹è½½ Git](https://git-scm.com/downloads)\n\n**2\\. Node.js:** ç”¨äºŽåœ¨æœåŠ¡å™¨ä¸Šæ‰§è¡Œ JavaScript çš„è¿è¡Œæ—¶çŽ¯å¢ƒã€‚\n\n* [ä¸‹è½½ Node.js](https://nodejs.org/en/download/)\n\n**3\\. Dockerï¼ˆå¯é€‰ï¼‰ï¼š** ç”¨äºŽå®¹å™¨åŒ–åº”ç”¨ç¨‹åºã€‚\n\n* [ä¸‹è½½ Docker](https://www.docker.com/)\n\n**4\\. Ollama:** ç”¨äºŽæœ¬åœ°è¿è¡Œå¼€æº AI æ¨¡åž‹ã€‚\n\n* [ä¸‹è½½ Ollama](https://ollama.com/)\n\n## å…‹éš†ä»“åº“\n\né¦–å…ˆä»Ž GitHub å…‹éš† Bolt.new ä»“åº“\n\n\n```python\ngit clone https://github.com/coleam00/bolt.new-any-llm.git\n```\n\n## é…ç½®çŽ¯å¢ƒå˜é‡\n\n1. **é‡å‘½åé…ç½®æ–‡ä»¶ï¼š** å¯¼èˆªåˆ°å…‹éš†çš„ä»“åº“ï¼Œå¹¶å°† `.env.example` æ–‡ä»¶é‡å‘½åä¸º `.env.local`ã€‚\n2. **æ·»åŠ æ‚¨çš„ LLM API å¯†é’¥ï¼š** æ‰“å¼€ `.env.local` æ–‡ä»¶å¹¶æ·»åŠ æ‚¨çš„ API å¯†é’¥ï¼š\n\n```python\nGROQ_API_KEY=YOUR_GROQ_API_KEY\nOPENAI_API_KEY=YOUR_OPENAI_API_KEY\nANTHROPIC_API_KEY=YOUR_ANTHROPIC_API_KEY\n```\n**æ³¨æ„ï¼š** å¦‚æžœæ‚¨ä½¿ç”¨ Ollamaï¼Œå®ƒä¸éœ€è¦ API å¯†é’¥ï¼Œå› ä¸ºå®ƒåœ¨æœ¬åœ°è¿è¡Œã€‚\n\n**3\\. å¯é€‰è°ƒè¯•çº§åˆ«ï¼š** æ‚¨å¯ä»¥è®¾ç½®è°ƒè¯•çº§åˆ«ä»¥å¸®åŠ©æ•…éšœæŽ’é™¤ï¼š\n\n```python\nVITE_LOG_LEVEL=debug\n```\n**é‡è¦ï¼š** åˆ‡å‹¿å°†æ‚¨çš„ `.env.local` æ–‡ä»¶æäº¤åˆ°ç‰ˆæœ¬æŽ§åˆ¶ï¼Œå› ä¸ºå®ƒå·²åŒ…å«åœ¨ `.gitignore` ä¸­ã€‚\n\n## å®‰è£…ä¾èµ–\n\nBolt.new ä½¿ç”¨ `pnpm` è¿›è¡ŒåŒ…ç®¡ç†ã€‚ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£…ä¾èµ–ï¼š\n\n1. **å®‰è£… pnpmï¼ˆå¦‚æžœå°šæœªå®‰è£…ï¼‰ï¼š**\n\n\n```python\nsudo npm install -g pnpm\n```\n**2\\. å®‰è£…é¡¹ç›®ä¾èµ–**\n\n\n```python\npnpm install\n```\n\n## è¿è¡Œåº”ç”¨ç¨‹åº\n\nä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨å¼€å‘æœåŠ¡å™¨ï¼š\n\n\n```python\npnpm run dev\n```\nè¯¥å‘½ä»¤åˆå§‹åŒ– Remix Vite å¼€å‘æœåŠ¡å™¨ã€‚ä¸ºäº†èŽ·å¾—æœ€ä½³æ€§èƒ½ï¼Œå»ºè®®ä½¿ç”¨ [Google Chrome Canary](https://www.google.com/chrome/canary/) ä½œä¸ºæ‚¨çš„æµè§ˆå™¨ã€‚\n\n## ä½¿ç”¨ Docker è¿è¡Œ Bolt.new\n\nå¯¹äºŽé‚£äº›å–œæ¬¢å®¹å™¨åŒ–çŽ¯å¢ƒçš„ç”¨æˆ·ï¼ŒBolt.new æä¾›äº†å¼ºå¤§çš„ Docker æ”¯æŒã€‚\n\n## ä½¿ç”¨è¾…åŠ©è„šæœ¬\n\nBolt.new æä¾›äº†ç”¨äºŽæž„å»º Docker é•œåƒçš„ NPM è„šæœ¬ï¼š\n\n* **å¼€å‘æž„å»ºï¼š**\n\n\n```python\nnpm run dockerbuild\n```\n* **ç”Ÿäº§æž„å»ºï¼š**\n\n\n```python\nnpm run dockerbuild:prod\n```\n\n## ç›´æŽ¥ Docker æž„å»ºå‘½ä»¤\n\næˆ–è€…ï¼Œä½¿ç”¨ Docker çš„ç›®æ ‡ç‰¹æ€§æ¥æŒ‡å®šæž„å»ºçŽ¯å¢ƒï¼š\n\n* **å¼€å‘æž„å»ºï¼š**\n\n\n```python\ndocker build . --target bolt-ai-development\n```\n* **ç”Ÿäº§æž„å»ºï¼š**\n\n\n```python\ndocker build . --target bolt-ai-productio\n```\n\n## Docker Compose ä¸Žé…ç½®æ–‡ä»¶\n\nä½¿ç”¨ Docker Compose é…ç½®æ–‡ä»¶ç®¡ç†ä¸åŒçš„çŽ¯å¢ƒï¼š\n\n* **å¼€å‘çŽ¯å¢ƒï¼š**\n\n```python\ndocker-compose --profile development up\n```\n* **ç”Ÿäº§çŽ¯å¢ƒï¼š**\n\n```python\ndocker-compose --profile production up\n```\n**æ³¨æ„ï¼š** å½“ä½¿ç”¨å¼€å‘é…ç½®æ–‡ä»¶è¿è¡Œ Docker Compose å‘½ä»¤æ—¶ï¼Œæ‚¨æœºå™¨ä¸Šä»£ç çš„ä»»ä½•æ›´æ”¹å°†è‡ªåŠ¨åæ˜ åœ¨è¿è¡Œä¸­çš„å®¹å™¨ä¸­ï¼Œä»Žè€Œå®žçŽ°çƒ­é‡è½½ã€‚\n\n## å®žç”¨æ¼”ç¤º\n\nä¸ºäº†å±•ç¤º Bolt.new çš„åŠŸèƒ½ï¼Œè®©æˆ‘ä»¬é€šè¿‡å‡ ä¸ªå®žç”¨ç¤ºä¾‹æ¥è¿›è¡Œæ¼”ç¤ºã€‚\n\n## åˆ›å»ºä¸€ä¸ªç®€å•çš„ç½‘é¡µ\n\nå…¶ä¸­ä¸€ä¸ªæœ€ç®€å•çš„æ¼”ç¤ºæ˜¯ç”Ÿæˆä¸€ä¸ªåŸºæœ¬çš„ç½‘é¡µï¼š\n\n1. **Prompt Bolt.new:** è¯·æ±‚AIåˆ›å»ºä¸€ä¸ªç®€å•çš„ç½‘é¡µã€‚\n2. **Generation:** Bolt.newç”Ÿæˆæ‰€æœ‰å¿…è¦çš„æ–‡ä»¶å¤¹å’Œæ–‡ä»¶ã€‚\n3. **Preview:** åˆ©ç”¨é¢„è§ˆåŠŸèƒ½å³æ—¶æŸ¥çœ‹è¾“å‡ºã€‚\n\nè¿™ä¸ªè¿‡ç¨‹å¼ºè°ƒäº†Bolt.newé«˜æ•ˆå¤„ç†ç®€å•ä»»åŠ¡çš„èƒ½åŠ›ï¼Œä¸ºæ›´å¤æ‚çš„é¡¹ç›®æä¾›äº†åšå®žçš„åŸºç¡€ã€‚\n\n## æž„å»ºè´ªåƒè›‡æ¸¸æˆ\n\nBolt.newçš„èƒ½åŠ›åœ¨åˆ›å»ºäº¤äº’å¼åº”ç”¨ç¨‹åºæ—¶å˜å¾—æ›´åŠ æ˜Žæ˜¾ï¼Œä¾‹å¦‚è´ªåƒè›‡æ¸¸æˆï¼š\n\n1. **æç¤ºBolt.newï¼š** è¯·æ±‚AIå¸®åŠ©åˆ›å»ºè´ªåƒè›‡æ¸¸æˆã€‚\n2. **ç”Ÿæˆï¼š** Bolt.newç”Ÿæˆæ‰€æœ‰æ‰€éœ€çš„æ–‡ä»¶ã€åŒ…å’Œå‰ç«¯ç•Œé¢ã€‚\n3. **é¢„è§ˆï¼š** æ‰“å¼€ç”Ÿæˆçš„HTMLæ–‡ä»¶ï¼ŒæŸ¥çœ‹ä¸€ä¸ªåŠŸèƒ½é½å…¨çš„è´ªåƒè›‡æ¸¸æˆï¼Œå¹¶è·Ÿè¸ªå¾—åˆ†ã€‚\n\n**ç»“æžœï¼š** AIæˆåŠŸç”Ÿæˆäº†ä¸€ä¸ªè§†è§‰ä¸Šå¸å¼•äººä¸”åŠŸèƒ½é½å…¨çš„æ¸¸æˆï¼Œå±•ç¤ºäº†å…¶å¤„ç†åŠ¨æ€å’Œäº¤äº’å¼ç½‘é¡µåº”ç”¨ç¨‹åºçš„èƒ½åŠ›ã€‚\n\n## å¼€å‘å…¨æ ˆé‡‘èžæœåŠ¡ç½‘é¡µåº”ç”¨\n\nä¸ºäº†æ›´å…¨é¢çš„æ¼”ç¤ºï¼Œè®©æˆ‘ä»¬æŽ¢è®¨æž„å»ºä¸€ä¸ªå…¨æ ˆé‡‘èžæœåŠ¡åº”ç”¨ç¨‹åºï¼š\n\n1. **Prompt Bolt.new:**\n* **å‰ç«¯:** ä½¿ç”¨ React ä½œä¸ºç”¨æˆ·ç•Œé¢ã€‚\n* **åŽç«¯:** å®žçŽ° Next.js è¿›è¡ŒæœåŠ¡å™¨ç«¯æ¸²æŸ“ã€‚\n* **æ•°æ®åº“:** é›†æˆ PostgreSQL è¿›è¡Œæ•°æ®ç®¡ç†ã€‚\n* **èº«ä»½éªŒè¯:** ä½¿ç”¨ Clerk è¿›è¡Œè®¾ç½®ã€‚\n\n```python\nCreate a full-stack financial service web app with a clean, intuitive UI using ChatGPT and React for the frontend, Next.js for server-side rendering, PostgreSQL for data management, and authentication set up with Clerk.\n```\n**2\\. ç”Ÿæˆè¿‡ç¨‹:**\n\n* **æ–‡ä»¶åˆ›å»º:** Bolt.new ç”Ÿæˆå¿…è¦çš„é¡¹ç›®ç»“æž„å’Œæ–‡ä»¶ã€‚\n* **åŒ…å®‰è£…:** å®‰è£…æ‰€éœ€çš„åŒ…ï¼Œå¦‚ Reactã€Next.js å’Œ Clerkã€‚\n* **åŽç«¯è®¾ç½®:** é…ç½®æœåŠ¡å™¨ç«¯æ¸²æŸ“å’Œæ•°æ®åº“è¿žæŽ¥ã€‚\n* **èº«ä»½éªŒè¯:** é›†æˆ Clerk è¿›è¡Œç”¨æˆ·èº«ä»½éªŒè¯ã€‚\n\n**3\\. é¢„è§ˆ:** é€šè¿‡æä¾›çš„ URL è®¿é—®åº”ç”¨ç¨‹åºï¼ŒæŸ¥çœ‹å®Œæ•´åŠŸèƒ½çš„é‡‘èžä»ªè¡¨æ¿ï¼Œç‰¹ç‚¹åŒ…æ‹¬ï¼š\n\n* **ä½™é¢åŽ†å²:** æ‰€æœ‰å­˜æ¬¾çš„æ¦‚è¿°ã€‚\n* **é¢„ç®—é…ç½®:** èƒ½å¤Ÿä»Žå„ä¸ªç±»åˆ«æ·»åŠ é¢„ç®—ã€‚\n* **äº¤æ˜“ç®¡ç†:** æ·»åŠ å’ŒæŸ¥çœ‹äº¤æ˜“ã€‚\n* **æŠ•èµ„è·Ÿè¸ª:** ç›‘æŽ§æŠ•èµ„ã€‚\n\n**ç»“æžœ:** Bolt.new é«˜æ•ˆåœ°ç®¡ç†å¤æ‚å¤šé¢çš„åº”ç”¨ç¨‹åºåˆ›å»ºï¼Œçªå‡ºå…¶åœ¨å¤§è§„æ¨¡é¡¹ç›®ä¸­çš„æ½œåŠ›ã€‚\n\n## Tips and Tricks for Maximizing Bolt.new\n\nä¸ºäº†å……åˆ†åˆ©ç”¨ Bolt.newï¼Œè¯·è€ƒè™‘ä»¥ä¸‹ç­–ç•¥ï¼š\n\n1. **æ˜Žç¡®æ‚¨çš„æŠ€æœ¯æ ˆï¼š**\n* åœ¨åˆå§‹æç¤ºä¸­æ¸…æ¥šåœ°æåŠæ‚¨å¸Œæœ›ä½¿ç”¨çš„æ¡†æž¶æˆ–åº“ï¼ˆä¾‹å¦‚ï¼ŒAstroã€Tailwindã€ShadCNï¼‰ï¼Œä»¥ç¡®ä¿ Bolt.new æŒ‰ç…§æ‚¨çš„éœ€æ±‚æ­å»ºé¡¹ç›®ã€‚\n\n**2\\. ä½¿ç”¨å¢žå¼ºæç¤ºå›¾æ ‡ï¼š**\n\n* åœ¨æäº¤æç¤ºä¹‹å‰ï¼Œä½¿ç”¨â€œå¢žå¼ºâ€åŠŸèƒ½æ¥ä¼˜åŒ–æ‚¨çš„æŒ‡ä»¤ã€‚è¿™å°†å¯¼è‡´æ›´å‡†ç¡®å’Œé«˜æ•ˆçš„ä»£ç ç”Ÿæˆã€‚\n\n**3\\. é¦–å…ˆæ­å»ºåŸºç¡€ç»“æž„ï¼š**\n\n* åœ¨æ·»åŠ é«˜çº§åŠŸèƒ½ä¹‹å‰ï¼Œä»Žåº”ç”¨ç¨‹åºçš„åŸºæœ¬ç»“æž„å¼€å§‹ã€‚è¿™æœ‰åŠ©äºŽ Bolt.new ç†è§£é¡¹ç›®åŸºç¡€ï¼Œç¡®ä¿åŽç»­åŠŸèƒ½çš„è‰¯å¥½é›†æˆã€‚\n\n**4\\. æ‰¹é‡å¤„ç†ç®€å•æŒ‡ä»¤ï¼š**\n\n* å°†å¤šä¸ªç®€å•ä»»åŠ¡åˆå¹¶ä¸ºä¸€ä¸ªæç¤ºï¼Œä»¥èŠ‚çœæ—¶é—´å¹¶å‡å°‘ API é¢åº¦æ¶ˆè€—ã€‚ä¾‹å¦‚ï¼Œè¯·æ±‚æ›´æ”¹é…è‰²æ–¹æ¡ˆã€æ·»åŠ ç§»åŠ¨å“åº”èƒ½åŠ›ï¼Œä»¥åŠä¸€æ¬¡æ€§é‡å¯å¼€å‘æœåŠ¡å™¨ã€‚\n\n**5\\. åˆ©ç”¨å¼€æºè‡ªå®šä¹‰ï¼š**\n\n* ç”±äºŽ Bolt.new æ˜¯å¼€æºçš„ï¼Œè¯·æŽ¢ç´¢ [Bolt.new GitHub ä»“åº“](https://github.com/coleam00/bolt.new-any-llm.git)ï¼Œä»¥è‡ªå®šä¹‰å’Œæ‰©å±•åŠŸèƒ½ä»¥æ»¡è¶³æ‚¨çš„ç‰¹å®šé¡¹ç›®éœ€æ±‚ã€‚\n\nBolt.newï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸Ž Ollama é›†æˆæ—¶ï¼Œä»£è¡¨äº† AI é©±åŠ¨çš„ç½‘é¡µå¼€å‘çš„é‡å¤§è¿›æ­¥ã€‚é€šè¿‡å°†å…ˆè¿›çš„ AI æ¨¡åž‹ä¸Žå¼ºå¤§çš„å¼€å‘å·¥å…·ç»“åˆèµ·æ¥ï¼ŒBolt.new ç®€åŒ–äº†æž„å»ºã€éƒ¨ç½²å’Œç®¡ç†å…¨æ ˆåº”ç”¨ç¨‹åºçš„è¿‡ç¨‹ã€‚æ— è®ºæ‚¨æ˜¯å¸Œæœ›åŠ å¿«å¼€å‘å·¥ä½œæµç¨‹ã€æŽ¢ç´¢ AI é©±åŠ¨çš„ç¼–ç è¾…åŠ©ï¼Œè¿˜æ˜¯ä»¥æœ€å°çš„è®¾ç½®æž„å»ºå¤æ‚çš„ç½‘é¡µåº”ç”¨ç¨‹åºï¼ŒBolt.new éƒ½æä¾›äº†å®žçŽ°ç›®æ ‡æ‰€éœ€çš„å·¥å…·å’Œçµæ´»æ€§ã€‚\n\n"},{"lang":"zh","group":"blog","slug":"blog/build-a-customer-support-assistant-with-llama3-1-7bf60611e428","frontmatter":{"title":"ä½¿ç”¨ Llama3.1 åˆ›å»ºå®¢æˆ·æ”¯æŒåŠ©ç†","meta_title":"ä½¿ç”¨ Llama3.1 åˆ›å»ºå®¢æˆ·æ”¯æŒåŠ©ç†","description":"ä½¿ç”¨ LLM ä»£ç†å’Œ Amazon Bedrock ä»¥äººå·¥æ™ºèƒ½è§£å†³å®¢æˆ·é—®é¢˜ï¼šä½¿ç”¨ Llama3.1 æž„å»ºå’Œéƒ¨ç½²æ”¯æŒåŠ©ç†æŒ‡å—","date":"2024-11-13T01:22:29.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*lNyf72c2_r1wKjnoRA1_FQ.png","categories":["Programming","Chatbots","Technology/Web"],"author":"Rifx.Online","tags":["Llama3.1","AmazonBedrock","Gradio","EC2","CustomerSupport"],"draft":false,"slug":"blog/build-a-customer-support-assistant-with-llama3-1-7bf60611e428"},"content":"\n\n\n### ä½¿ç”¨ LLM ä»£ç†å’Œ Amazon Bedrock è§£å†³å®¢æˆ·æŸ¥è¯¢çš„ AIï¼šæž„å»ºå’Œéƒ¨ç½²æ”¯æŒåŠ©æ‰‹çš„æŒ‡å—ï¼Œä½¿ç”¨ Llama3\\.1\n\n\n\n## ä»‹ç»\n\n### é—®é¢˜\n\nä¼ä¸šç»å¸¸é¢ä¸´å¤„ç†å¤§é‡å®¢æˆ·è¯¢é—®çš„æŒ‘æˆ˜ã€‚è¿™äº›è¯¢é—®å¯èƒ½ä»Žç®€å•çš„é—®é¢˜â€œæˆ‘çš„è®¢å•çŠ¶æ€æ˜¯ä»€ä¹ˆï¼Ÿâ€åˆ°éœ€è¦äººå·¥å¹²é¢„çš„æ›´å¤æ‚çš„é—®é¢˜ä¸ç­‰ã€‚é‡å¤è¯¢é—®çš„åºžå¤§æ•°é‡å¯èƒ½ä¼šä½¿å®¢æˆ·æ”¯æŒå›¢é˜Ÿä¸å ªé‡è´Ÿï¼Œå¯¼è‡´å“åº”æ—¶é—´å»¶é•¿å’Œå®¢æˆ·æ»¡æ„åº¦é™ä½Žã€‚æ­¤å¤–ï¼Œåˆ©ç”¨äººåŠ›èµ„æºå¤„ç†ç®€å•çš„ä¾‹è¡Œè¯¢é—®æ•ˆçŽ‡ä½Žä¸‹ä¸”æˆæœ¬é«˜æ˜‚ã€‚è¿«åˆ‡éœ€è¦èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†ä¾‹è¡Œè¯¢é—®çš„è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆï¼Œä»¥ä¾¿äººç±»ä»£ç†å¯ä»¥ä¸“æ³¨äºŽéœ€è¦ç»†è‡´é—®é¢˜è§£å†³çš„å‡çº§æ¡ˆä¾‹ã€‚\n\n### è§£å†³æ–¹æ¡ˆ\n\nå¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMï¼‰ä»£ç†çš„å¼•å…¥ä¸ºè¿™ä¸ªé—®é¢˜æä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚ä¸€ä¸ª [LLM ä»£ç†](https://proxy.rifx.online/https://research.ibm.com/blog/what-are-ai-agents-llm) å¯ä»¥é€šè¿‡è®¿é—®å’Œè§£é‡Šå…¬å¸æ•°æ®åº“ä¸­çš„æ•°æ®æ¥å“åº”ç”¨æˆ·æŸ¥è¯¢ï¼Œå¤„ç†ä¸€äº›ç®€å•çš„æ“ä½œï¼Œä¾‹å¦‚æ£€æŸ¥è®¢å•çŠ¶æ€ã€æ£€ç´¢è´¦æˆ·ä¿¡æ¯å’Œå›žç­”å¸¸è§é—®é¢˜ã€‚é€šè¿‡è‡ªåŠ¨åŒ–è¿™äº›æ—¥å¸¸ä»»åŠ¡ï¼ŒLLM ä»£ç†ç¡®ä¿äº†æ›´å¿«çš„è§£å†³æ—¶é—´ï¼Œå¹¶é‡Šæ”¾äººåŠ›èµ„æºä»¥åº”å¯¹æ›´å¤æ‚çš„å®¢æˆ·æ”¯æŒåœºæ™¯ã€‚åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†æŽ¢è®¨å¦‚ä½•ä½¿ç”¨æ¥è‡ª Amazon Bedrock Tools api çš„ Llama3\\.1 æ¨¡åž‹æž„å»ºä¸€ä¸ªå®¢æˆ·æ”¯æŒåŠ©æ‰‹ã€‚\n\næœ€åŽï¼Œæˆ‘ä»¬å°†åœ¨æœ¬åœ°æœºå™¨ä¸Šè¿è¡ŒåŠ©æ‰‹ï¼Œå¹¶è°ƒç”¨ä¸€ä¸ªå‡æ•°æ®åº“ï¼š\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Ok9N3mdX50JVWbaJKUrJeQ.gif)\n\n## LLM ä»£ç†\n\n### ä»€ä¹ˆæ˜¯ LLM ä»£ç†\n\n[LLM ä»£ç†](https://proxy.rifx.online/https://research.ibm.com/blog/what-are-ai-agents-llm) æ˜¯åŸºäºŽå¤§åž‹è¯­è¨€æ¨¡åž‹å¦‚ Llama3.1 æž„å»ºçš„ä¸“ç”¨åº”ç”¨ç¨‹åºï¼Œæ—¨åœ¨æ‰§è¡Œç‰¹å®šä»»åŠ¡æˆ–åŠŸèƒ½ã€‚ä¸Žæ ¹æ®ç»™å®šæç¤ºç”Ÿæˆç±»äººæ–‡æœ¬çš„é€šç”¨ LLM ä¸åŒï¼ŒLLM ä»£ç†å…·å¤‡é¢å¤–çš„èƒ½åŠ›ï¼Œå¦‚è®¿é—®å¤–éƒ¨æ•°æ®åº“ã€æ‰§è¡Œæ“ä½œå’Œæ ¹æ®é¢„å®šä¹‰è§„åˆ™åšå‡ºå†³ç­–ã€‚å®ƒä»¬è¢«å®šåˆ¶ç”¨äºŽå¤„ç†ç‰¹å®šç”¨ä¾‹ï¼Œä¾‹å¦‚å®¢æˆ·æ”¯æŒï¼Œåœ¨è¿™äº›åœºæ™¯ä¸­ï¼Œå®ƒä»¬å¯ä»¥ä¸Žç”¨æˆ·äº’åŠ¨ã€æ£€ç´¢ä¿¡æ¯å¹¶æ ¹æ®å¯¹è¯çš„ä¸Šä¸‹æ–‡æ‰§è¡Œå‘½ä»¤ã€‚\n\nè™½ç„¶é€šç”¨ LLM åœ¨ç”Ÿæˆè¿žè´¯æ–‡æœ¬å’Œç†è§£è¯­è¨€æ–¹é¢éžå¸¸å¼ºå¤§ï¼Œä½† LLM ä»£ç†é€šè¿‡ä¸Žå¤–éƒ¨ç³»ç»Ÿé›†æˆï¼Œè¿›ä¸€æ­¥æ‹“å±•äº†å…¶èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ‰§è¡Œè¶…å‡ºæ–‡æœ¬ç”Ÿæˆçš„çŽ°å®žä¸–ç•Œä»»åŠ¡ã€‚\n\nä»£ç†å…·æœ‰ä¸€å¥—æŒ‡ä»¤ã€åŸºç¡€æ¨¡åž‹ã€ä¸€ç»„å¯ç”¨æ“ä½œå’ŒçŸ¥è¯†åº“ï¼Œä½¿å…¶èƒ½å¤Ÿæ‰§è¡Œå¤æ‚ä»»åŠ¡ã€‚\n\nç”Ÿæˆæ¨¡åž‹å¯ä»¥å›žç­”ä¸€èˆ¬æ€§é—®é¢˜æˆ–ä¸Žæ‚¨çš„æ–‡æ¡£ç›¸å…³çš„é—®é¢˜ï¼Œä¾‹å¦‚â€œæˆ‘çœ‹ä¸åˆ°æˆ‘çš„ä¼šè®®ï¼Ÿæˆ‘è¯¥å¦‚ä½•é¢„å®šä¼šè®®ï¼Ÿâ€ã€‚è€Œä»£ç†åˆ™ä½¿ç”¨åŸºç¡€æ¨¡åž‹ä½œä¸ºæŽ¨ç†é€»è¾‘ï¼Œå¹¶ç»“åˆå¤–éƒ¨æ•°æ®æºå¦‚æ‚¨çš„ APIï¼Œèƒ½å¤Ÿè¿”å›žç”¨æˆ·å·²é¢„å®šä¼šè®®çš„æ•°é‡ï¼Œæˆ–ç›´æŽ¥ä»Žäº¤äº’ç•Œé¢å®‰æŽ’ä¼šè®®ã€‚\n\nâ€œé€šç”¨ç›®çš„â€ç±»åˆ«ä¸­æœ‰è®¸å¤šä»£ç†ï¼Œè¿˜æœ‰ä¸€äº›ä¸“é—¨ç”¨äºŽç‰¹å®šä»»åŠ¡çš„ä»£ç†ï¼Œå¦‚ä»£ç åŠ©æ‰‹ï¼ˆ[Amazon CodeWhisperer, Copilot](https://proxy.rifx.online/https://www.missioncloud.com/blog/github-copilot-vs-amazon-codewhisperer)ï¼‰ã€å†™ä½œåŠ©æ‰‹ã€ç³»ç»Ÿè®¾è®¡ï¼ˆ[Amazon Q](https://proxy.rifx.online/https://aws.amazon.com/q/)ï¼‰ã€ç»´åŸºç™¾ç§‘æ‘˜è¦ç­‰ã€‚\n\n**AI ä»£ç†ç”Ÿæ€ç³»ç»Ÿï¼š**\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*VuAyzZ2BfrD7o-z0lOpUwA.png)\n\n### ä½¿ç”¨ Python ä»Žå¤´åˆ›å»ºä¸€ä¸ªåŸºæœ¬ä»£ç†\n\nè®©æˆ‘ä»¬ä½¿ç”¨ Python ä»Žå¤´åˆ›å»ºä¸€ä¸ªç®€å•çš„ LLM ä»£ç†ã€‚æœ¬æ–‡å±•ç¤ºäº†å¦‚ä½•åœ¨ä¸ä¾èµ–ä»»ä½•åº“æˆ–æ¡†æž¶çš„æƒ…å†µä¸‹æž„å»ºä»£ç†ã€‚\n\n## è‡ªå®šä¹‰æ”¯æŒåŠ©æ‰‹\n\nçŽ°åœ¨ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨æ¥è‡ª [Bedrock](https://proxy.rifx.online/https://aws.amazon.com/bedrock/) çš„ [Llama3\\.1](https://proxy.rifx.online/https://llama.meta.com/) æ¨¡åž‹åˆ›å»ºä¸€ä¸ªæ›´å¤æ‚çš„å®¢æˆ·æ”¯æŒåŠ©æ‰‹ã€‚è¯¥ä»£ç†å°†èƒ½å¤Ÿæ‰§è¡Œæ›´å¤æ‚çš„ä»»åŠ¡ï¼Œä¾‹å¦‚ä»Žæ•°æ®åº“ä¸­æŸ¥æ‰¾ç”¨æˆ·æ•°æ®å’Œæ‰§è¡Œç®€å•æ“ä½œï¼Œå¦‚æŸ¥çœ‹è®¢å•çš„è¿è¾“çŠ¶æ€ã€‚\n\n### å®šä¹‰èƒ½åŠ›å’Œè¾¹ç•Œ\n\nåœ¨æž„å»ºæˆ‘ä»¬çš„åŠ©æ‰‹ä¹‹å‰ï¼Œå®šä¹‰ä»£ç†å¯ä»¥æ‰§è¡Œçš„æ“ä½œå¹¶å»ºç«‹å…¶æ“ä½œçš„æ˜Žç¡®è¾¹ç•Œè‡³å…³é‡è¦ã€‚åœ¨ç”Ÿäº§çŽ¯å¢ƒä¸­ï¼Œè¿™äº›èƒ½åŠ›å’Œè¾¹ç•Œå¯¹äºŽç¡®ä¿ä»£ç†æœ‰æ•ˆä¸”å®‰å…¨åœ°è¿è¡Œè‡³å…³é‡è¦ã€‚\n\n**èƒ½åŠ›ï¼š**\n\n* å›žå¤å¸¸è§å®¢æˆ·æŸ¥è¯¢ï¼ˆä¾‹å¦‚ï¼Œè®¢å•çŠ¶æ€ã€é€€è´§æ”¿ç­–ï¼‰ã€‚\n* ä»Žæ•°æ®åº“ä¸­è®¿é—®å’Œæ£€ç´¢ç”¨æˆ·æ•°æ®ã€‚\n* æ‰§è¡Œç®€å•æ“ä½œï¼Œå¦‚æŸ¥çœ‹è®¢å•çŠ¶æ€ã€æ›´æ–°å®¢æˆ·ä¿¡æ¯ç­‰ã€‚\n\n**è¾¹ç•Œï¼š**\n\n* ä»£ç†ä¸åº”æ‰§è¡Œéœ€è¦äººç±»åˆ¤æ–­çš„æ“ä½œï¼Œä¾‹å¦‚å¤„ç†é€€æ¬¾æˆ–å¤„ç†å‡çº§ã€‚\n* åº”åœ¨å®šä¹‰çš„èŒƒå›´å†…æ“ä½œï¼Œé™¤éžæ˜Žç¡®å…è®¸ï¼Œå¦åˆ™ä¸åº”è®¿é—®æ•æ„Ÿæ•°æ®ã€‚\n* åº”ä¸ºä¸æ”¯æŒçš„æŸ¥è¯¢è®¾ç½®é”™è¯¯å¤„ç†å’Œå›žé€€æœºåˆ¶ã€‚\n\n### æž¶æž„\n\næˆ‘ä»¬è§£å†³æ–¹æ¡ˆçš„ç³»ç»Ÿæž¶æž„æ¶‰åŠå¤šä¸ªç»„ä»¶ååŒå·¥ä½œï¼š\n\n1. **LLM Agent**: ç³»ç»Ÿçš„æ ¸å¿ƒï¼Œä½¿ç”¨ [Llama3\\.1](https://proxy.rifx.online/https://llama.meta.com/) æˆ– [Claude 3\\.5 Sonnet](https://proxy.rifx.online/https://www.anthropic.com/news/claude-3-5-sonnet) æ¨¡åž‹æž„å»ºï¼Œå¤„ç†è‡ªç„¶è¯­è¨€å¤„ç†å’Œå†³ç­–åˆ¶å®šã€‚\n2. **æ•°æ®åº“**: å­˜å‚¨å®¢æˆ·æ•°æ®å’Œå…¶ä»–ç›¸å…³ä¿¡æ¯ï¼Œä¾›ä»£ç†æŸ¥è¯¢ã€‚\n3. **APIå±‚**: ä¿ƒè¿›LLMä»£ç†ä¸Žæ•°æ®åº“ä¹‹é—´çš„é€šä¿¡ï¼Œä½¿ä»£ç†èƒ½å¤Ÿæ£€ç´¢å’Œæ“ä½œæ•°æ®ã€‚\n4. **ç”¨æˆ·ç•Œé¢**: ä¸€ä¸ªå‰ç«¯ç•Œé¢ï¼ˆä¾‹å¦‚ï¼ŒèŠå¤©æœºå™¨äººç•Œé¢ï¼‰ï¼Œå®¢æˆ·åœ¨æ­¤ä¸Žæ”¯æŒåŠ©æ‰‹äº’åŠ¨ã€‚\n\n### ä»£ç \n\nåœ¨æˆ‘ä»¬æ£€æŸ¥ä»£ç ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨å…·å¤‡ä»¥ä¸‹æ¡ä»¶ï¼š\n\n1. äº†è§£ Python å’Œ [boto3](https://proxy.rifx.online/https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) åº“ã€‚\n2. æ‹¥æœ‰ä¸€ä¸ªå¯ç”¨äº†æ¨¡åž‹è®¿é—®çš„æœ‰æ•ˆ AWS è´¦æˆ·ï¼Œåœ¨ [Bedrock](https://proxy.rifx.online/https://aws.amazon.com/bedrock/) ä¸­ã€‚\n3. å®‰è£…äº† Python å’Œ boto3 çš„ [è™šæ‹ŸçŽ¯å¢ƒ](https://proxy.rifx.online/https://docs.anaconda.com/miniconda/)ã€‚\n\n### ä»£ç æ¼”ç¤º\n\n\n```python\nfrom datetime import datetime\nimport json\nfrom typing import Any, Dict, List\n\nimport boto3\nfrom botocore.exceptions import ClientError\n\n## Initialize a Boto3 session and create a Bedrock runtime client\nsession = boto3.Session()\nregion = \"us-east-1\" # us-west-2 has better runtime quota\nbedrock_client = session.client(service_name = 'bedrock-runtime', region_name = region)\n```\né¦–å…ˆï¼Œæˆ‘ä»¬å¯¼å…¥å¿…è¦çš„åŒ…ï¼Œå¹¶ä¸º `us-east-1` åŒºåŸŸåˆ›å»ºä¸€ä¸ªåä¸º `bedrock_client` çš„ `boto3` Bedrock è¿è¡Œæ—¶å®¢æˆ·ç«¯å®žä¾‹ã€‚å¦‚æžœæ‚¨çš„ AWS è´¦æˆ·å¯ç”¨äº† `us-west-2` å¯ç”¨åŒº (AZ)ï¼Œè¯·æ”¹ç”¨è¯¥åŒºåŸŸã€‚æ’°å†™æœ¬æ–‡æ—¶ï¼ŒLlama3\\.1 æ¨¡åž‹ä»…åœ¨ `us-west-2` AZ å¯ç”¨ï¼Œå¹¶ä¸”ä¸Žä»…æ”¯æŒæ¯åˆ†é’Ÿ 50 æ¬¡è¯·æ±‚çš„ `us-east-1` AZ ç›¸æ¯”ï¼Œ`claude-3.5-sonnet` æ¨¡åž‹çš„è¿è¡Œæ—¶é…é¢æ›´å¤§ï¼ˆæ¯åˆ†é’Ÿ 250 æ¬¡è¯·æ±‚ï¼‰ã€‚\n\n\n```python\n## Define available models with their respective request limits\navailable_models = {\n    \"sonnet3-5\": \"anthropic.claude-3-5-sonnet-20240620-v1:0\", # 50 requests per min\n    \"sonnet\": \"anthropic.claude-3-sonnet-20240229-v1:0\", # 500 requests per min\n    \"llama31-70b\": \"meta.llama3-1-70b-instruct-v1:0\", # 400 requests per min\n    \"llama31-405b\": \"meta.llama3-1-405b-instruct-v1:0\", # 50 requests per min\n}\nmodelId = available_models[\"sonnet3-5\"]  # Select model for conversation\n```\næŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åˆ›å»º Bedrock ä¸­æ¨¡åž‹ ID çš„æ˜ å°„ã€‚**ç›®å‰å¹¶éžæ‰€æœ‰å¯ç”¨äºŽ Amazon Bedrock çš„æ¨¡åž‹éƒ½æ”¯æŒå·¥å…·ä½¿ç”¨**ã€‚è¯·æŸ¥çœ‹ Amazon Bedrock ç”¨æˆ·æŒ‡å—ä¸­çš„ [æ”¯æŒçš„æ¨¡åž‹åˆ—è¡¨](https://proxy.rifx.online/https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html#conversation-inference-supported-models-features) [è¿™é‡Œ](https://proxy.rifx.online/https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html#conversation-inference-supported-models-features)ã€‚\n\n\n```python\nclass FakeDatabase:\n    \"\"\"Sample fake database implementation.\"\"\"\n    def __init__(self):\n        self.customers = [\n            {\"id\": \"1213210\", \"name\": \"John Doe\", \"email\": \"john@gmail.com\", \"phone\": \"123-456-7890\", \"username\": \"johndoe\"},\n            {\"id\": \"2837622\", \"name\": \"Priya Patel\", \"email\": \"priya@candy.com\", \"phone\": \"987-654-3210\", \"username\": \"priya123\"},\n            {\"id\": \"3924156\", \"name\": \"Liam Nguyen\", \"email\": \"lnguyen@yahoo.com\", \"phone\": \"555-123-4567\", \"username\": \"liamn\"},\n            {\"id\": \"4782901\", \"name\": \"Aaliyah Davis\", \"email\": \"aaliyahd@hotmail.com\", \"phone\": \"111-222-3333\", \"username\": \"adavis\"},\n            {\"id\": \"5190753\", \"name\": \"Hiroshi Nakamura\", \"email\": \"hiroshi@gmail.com\", \"phone\": \"444-555-6666\", \"username\": \"hiroshin\"},\n            {\"id\": \"6824095\", \"name\": \"Fatima Ahmed\", \"email\": \"fatimaa@outlook.com\", \"phone\": \"777-888-9999\", \"username\": \"fatimaahmed\"},\n            {\"id\": \"7135680\", \"name\": \"Alejandro Rodriguez\", \"email\": \"arodriguez@protonmail.com\", \"phone\": \"222-333-4444\", \"username\": \"alexr\"},\n            {\"id\": \"8259147\", \"name\": \"Megan Anderson\", \"email\": \"megana@gmail.com\", \"phone\": \"666-777-8888\", \"username\": \"manderson\"},\n            {\"id\": \"9603481\", \"name\": \"Kwame Osei\", \"email\": \"kwameo@yahoo.com\", \"phone\": \"999-000-1111\", \"username\": \"kwameo\"},\n            {\"id\": \"1057426\", \"name\": \"Mei Lin\", \"email\": \"meilin@gmail.com\", \"phone\": \"333-444-5555\", \"username\": \"mlin\"}\n        ]\n\n        self.orders = [\n            {\"id\": \"24601\", \"customer_id\": \"1213210\", \"product\": \"Wireless Headphones\", \"quantity\": 1, \"price\": 79.99, \"status\": \"Shipped\"},\n            {\"id\": \"13579\", \"customer_id\": \"1213210\", \"product\": \"Smartphone Case\", \"quantity\": 2, \"price\": 19.99, \"status\": \"Processing\"},\n            {\"id\": \"97531\", \"customer_id\": \"2837622\", \"product\": \"Bluetooth Speaker\", \"quantity\": 1, \"price\": \"49.99\", \"status\": \"Shipped\"}, \n            {\"id\": \"86420\", \"customer_id\": \"3924156\", \"product\": \"Fitness Tracker\", \"quantity\": 1, \"price\": 129.99, \"status\": \"Delivered\"},\n            {\"id\": \"54321\", \"customer_id\": \"4782901\", \"product\": \"Laptop Sleeve\", \"quantity\": 3, \"price\": 24.99, \"status\": \"Shipped\"},\n            {\"id\": \"19283\", \"customer_id\": \"5190753\", \"product\": \"Wireless Mouse\", \"quantity\": 1, \"price\": 34.99, \"status\": \"Processing\"},\n            {\"id\": \"74651\", \"customer_id\": \"6824095\", \"product\": \"Gaming Keyboard\", \"quantity\": 1, \"price\": 89.99, \"status\": \"Delivered\"},\n            {\"id\": \"30298\", \"customer_id\": \"7135680\", \"product\": \"Portable Charger\", \"quantity\": 2, \"price\": 29.99, \"status\": \"Shipped\"},\n            {\"id\": \"47652\", \"customer_id\": \"8259147\", \"product\": \"Smartwatch\", \"quantity\": 1, \"price\": 199.99, \"status\": \"Processing\"},\n            {\"id\": \"61984\", \"customer_id\": \"9603481\", \"product\": \"Noise-Cancelling Headphones\", \"quantity\": 1, \"price\": 149.99, \"status\": \"Shipped\"},\n            {\"id\": \"58243\", \"customer_id\": \"1057426\", \"product\": \"Wireless Earbuds\", \"quantity\": 2, \"price\": 99.99, \"status\": \"Delivered\"},\n            {\"id\": \"90357\", \"customer_id\": \"1213210\", \"product\": \"Smartphone Case\", \"quantity\": 1, \"price\": 19.99, \"status\": \"Shipped\"},\n            {\"id\": \"28164\", \"customer_id\": \"2837622\", \"product\": \"Wireless Headphones\", \"quantity\": 2, \"price\": 79.99, \"status\": \"Processing\"}\n        ]\n\n    def get_user(self, key:str, value:str) -> Dict[str, str]:\n        \"\"\"Return metadata of user.\"\"\"\n        if key in {\"email\", \"phone\", \"username\"}:\n            for customer in self.customers:\n                if customer[key] == value:\n                    return customer\n            return f\"Couldn't find a user with {key} of {value}\"\n        else:\n            raise ValueError(f\"Invalid key: {key}\")\n        \n        return None\n\n    def get_order_by_id(self, order_id: str) -> Dict[str, str]:\n        \"\"\"Return metadata of the order using order id.\"\"\"\n        for order in self.orders:\n            if order[\"id\"] == order_id:\n                return order\n        return None\n    \n    def get_customer_orders(self, customer_id: str) -> List[Dict[str, str]]:\n        \"\"\"Return a list of orders for a specific customer.\"\"\"\n        return [order for order in self.orders if order[\"customer_id\"] == customer_id]\n\n    def cancel_order(self, order_id: str) -> str:\n        \"\"\"Cancel an order if it's in 'Processing' status.\"\"\"\n        order = self.get_order_by_id(order_id)\n        if order:\n            if order[\"status\"] == \"Processing\":\n                order[\"status\"] = \"Cancelled\"\n                return \"Cancelled the order\"\n            else:\n                return \"Order has already shipped.  Can't cancel it.\"\n        return \"Can't find that order!\"\n```\nåœ¨æœ¬æ¼”ç¤ºä¸­ï¼Œæˆ‘ä»¬å®žçŽ°äº†ä¸€ä¸ªæ¨¡æ‹Ÿæ•°æ®åº“ç±»ï¼Œå…¶ä¸­åŒ…å«é¢„å®šä¹‰çš„å®¢æˆ·åŠå…¶è®¢å•åˆ—è¡¨ã€‚è¿™ä¸ªæ¨¡æ‹Ÿæ•°æ®åº“ç±»è¿˜åŒ…æ‹¬ä»Žæ•°æ®åº“ä¸­æ£€ç´¢æ•°æ®çš„æ–¹æ³•ã€‚\n\n* `get_user` : è¿”å›žç”¨æˆ·\n* `get_order_by_id` : ä½¿ç”¨è®¢å• ID è¿”å›žè®¢å•\n* `get_customer_orders` : è¿”å›žç‰¹å®šå®¢æˆ·çš„æ‰€æœ‰è®¢å•\n* `cancel_order` : å¦‚æžœè®¢å•å¤„äºŽâ€œå¤„ç†ä¸­â€çŠ¶æ€ï¼Œåˆ™å–æ¶ˆè®¢å•ã€‚\n\n\n```python\n## Define all the tools avilable to the model\ntool_config = {\n    \"tools\": [\n        {\n            \"toolSpec\": {\n                \"name\": \"get_user\",\n                \"description\": \"Looks up a user by email, phone, or username.\",\n                \"inputSchema\": {\n                    \"json\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"key\": {\n                                \"type\": \"string\",\n                                \"enum\": [\"email\", \"phone\", \"username\"],\n                                \"description\": \"The attribute to search for a user by (email, phone, or username).\",\n                            },\n                            \"value\": {\n                                \"type\": \"string\",\n                                \"description\": \"The value to match for the specified attribute.\",\n                            },\n                        },\n                        \"required\": [\"key\", \"value\"],\n                    }\n                },\n            }\n        },\n        {\n            \"toolSpec\": {\n                \"name\": \"get_order_by_id\",\n                \"description\": \"Retrieves the details of a specific order based on the order ID. Returns the order ID, product name, quantity, price, and order status.\",\n                \"inputSchema\": {\n                    \"json\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"order_id\": {\n                                \"type\": \"string\",\n                                \"description\": \"The unique identifier for the order.\",\n                            }\n                        },\n                        \"required\": [\"order_id\"],\n                    }\n                },\n            }\n        },\n        {\n            \"toolSpec\": {\n                \"name\": \"get_customer_orders\",\n                \"description\": \"Retrieves the list of orders belonging to a user based on a user's customer id.\",\n                \"inputSchema\": {\n                    \"json\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"customer_id\": {\n                                \"type\": \"string\",\n                                \"description\": \"The customer_id belonging to the user\",\n                            }\n                        },\n                        \"required\": [\"customer_id\"],\n                    }\n                },\n            }\n        },\n        {\n            \"toolSpec\": {\n                \"name\": \"cancel_order\",\n                \"description\": \"Cancels an order based on a provided order_id.  Only orders that are 'processing' can be cancelled\",\n                \"inputSchema\": {\n                    \"json\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"order_id\": {\n                                \"type\": \"string\",\n                                \"description\": \"The order_id pertaining to a particular order\",\n                            }\n                        },\n                        \"required\": [\"order_id\"],\n                    }\n                },\n            }\n        },\n    ],\n    \"toolChoice\": {\"auto\": {}},\n}\n```\næŽ¥ä¸‹æ¥æˆ‘ä»¬å®šä¹‰ä¸€ä¸ª `tool_config` ã€‚\n\næ‚¨å¯ä»¥ä½¿ç”¨ Amazon Bedrock API ä¸ºæ¨¡åž‹æä¾›è®¿é—® [å·¥å…·](https://proxy.rifx.online/https://docs.aws.amazon.com/bedrock/latest/userguide/tool-use.html)ï¼Œå¸®åŠ©å…¶ç”Ÿæˆæ‚¨å‘é€ç»™æ¨¡åž‹çš„æ¶ˆæ¯çš„å“åº”ã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯èƒ½æœ‰ä¸€ä¸ªèŠå¤©åº”ç”¨ç¨‹åºï¼Œè®©ç”¨æˆ·æŸ¥æ‰¾å¹¿æ’­ç”µå°æ’­æ”¾çš„æœ€å—æ¬¢è¿Žçš„æ­Œæ›²ã€‚ä¸ºäº†å›žç­”æœ‰å…³æœ€å—æ¬¢è¿Žæ­Œæ›²çš„è¯·æ±‚ï¼Œæ¨¡åž‹éœ€è¦ä¸€ä¸ªå¯ä»¥æŸ¥è¯¢å¹¶è¿”å›žæ­Œæ›²ä¿¡æ¯çš„å·¥å…·ã€‚\n\n> ä¸Žæ¨¡åž‹ä¸€èµ·ä½¿ç”¨å·¥å…·ä¹Ÿè¢«ç§°ä¸º *å‡½æ•°è°ƒç”¨*ã€‚\n\nåœ¨ Amazon Bedrock ä¸­ï¼Œæ¨¡åž‹å¹¶ä¸ç›´æŽ¥è°ƒç”¨å·¥å…·ã€‚ç›¸åï¼Œå½“æ‚¨å‘æ¨¡åž‹å‘é€æ¶ˆæ¯æ—¶ï¼Œæ‚¨è¿˜æä¾›ä¸€ä¸ªæˆ–å¤šä¸ªå·¥å…·çš„å®šä¹‰ï¼Œè¿™äº›å·¥å…·å¯èƒ½ä¼šå¸®åŠ©æ¨¡åž‹ç”Ÿæˆå“åº”ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæ‚¨å°†æä¾›ä¸€ä¸ªè¿”å›žå®¢æˆ·è¯¦æƒ…ã€è®¢å•è¯¦æƒ…æˆ–å–æ¶ˆè®¢å•çš„å·¥å…·å®šä¹‰ã€‚å¦‚æžœæ¨¡åž‹ç¡®å®šéœ€è¦å·¥å…·æ¥ç”Ÿæˆæ¶ˆæ¯çš„å“åº”ï¼Œæ¨¡åž‹å°†å›žå¤æ‚¨è¯·æ±‚è°ƒç”¨è¯¥å·¥å…·ã€‚å®ƒè¿˜åŒ…æ‹¬è¦ä¼ é€’ç»™å·¥å…·çš„è¾“å…¥å‚æ•°ï¼ˆæ‰€éœ€çš„å®¢æˆ· ID æˆ–è®¢å• IDï¼‰ã€‚\n\nåœ¨æ‚¨çš„ä»£ç ä¸­ï¼Œæ‚¨ä»£è¡¨æ¨¡åž‹è°ƒç”¨å·¥å…·ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå‡è®¾å·¥å…·å®žçŽ°æ˜¯ä¸€ä¸ª APIã€‚å·¥å…·ä¹Ÿå¯ä»¥æ˜¯æ•°æ®åº“ã€Lambda å‡½æ•°æˆ–å…¶ä»–è½¯ä»¶ã€‚æ‚¨å†³å®šå¦‚ä½•å®žçŽ°å·¥å…·ã€‚ç„¶åŽï¼Œæ‚¨é€šè¿‡æä¾›å·¥å…·ç»“æžœçš„æ¶ˆæ¯ä¸Žæ¨¡åž‹ç»§ç»­å¯¹è¯ã€‚æœ€åŽï¼Œæ¨¡åž‹ç”Ÿæˆä¸€ä¸ªåŒ…å«æ‚¨å‘é€ç»™æ¨¡åž‹çš„å·¥å…·ç»“æžœçš„åŽŸå§‹æ¶ˆæ¯çš„å“åº”ã€‚\n\nåœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬åœ¨ `tool_config` ä¸­å®šä¹‰äº†æˆ‘ä»¬å¸Œæœ›èŠå¤©æœºå™¨äººæ‰§è¡Œçš„æ‰€æœ‰åŠŸèƒ½ã€‚æœ‰å…³ ToolConfiguration API çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜… [Amazon Bedrock æ–‡æ¡£](https://proxy.rifx.online/https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_ToolConfiguration.html)ã€‚\n\n```python\ndef process_tool_call(tool_name: str, tool_input: Any) -> Any:\n    \"\"\"Process the tool call based on the tool name and input.\"\"\"\n    if tool_name == \"get_user\":\n        return db.get_user(tool_input[\"key\"], tool_input[\"value\"])\n    elif tool_name == \"get_order_by_id\":\n        return db.get_order_by_id(tool_input[\"order_id\"])\n    elif tool_name == \"get_customer_orders\":\n        return db.get_customer_orders(tool_input[\"customer_id\"])\n    elif tool_name == \"cancel_order\":\n        return db.cancel_order(tool_input[\"order_id\"])\n```\nç”±äºŽæˆ‘ä»¬çš„åº”ç”¨ç¨‹åºä»£ç å°†ä»£è¡¨ LLM è°ƒç”¨æ‰€éœ€çš„å·¥å…·ï¼Œæˆ‘ä»¬å°†æ‰€æœ‰å·¥å…·æ‰“åŒ…åˆ°ä¸€ä¸ªå•ä¸€çš„å‡½æ•°ä¸­ã€‚`process_tool_call` å‡½æ•°æ ¹æ® LLM æä¾›çš„ `tool_name` å’Œ `tool_input` æ‰§è¡Œç›¸åº”çš„åŠŸèƒ½ã€‚\n\n```python\ndef simple_chat():\n    \"\"\"Main chat function that interacts with the user and the LLM.\"\"\"\n    system_prompt = \"\"\"\n    You are a customer support chat bot for an online retailer called TechNova. \n    Your job is to help users look up their account, orders, and cancel orders.\n    Be helpful and brief in your responses.\n    You have access to a set of tools, but only use them when needed.  \n    If you do not have enough information to use a tool correctly, ask a user follow up questions to get the required inputs.\n    Do not call any of the tools unless you have the required data from a user. \n    \"\"\"\n    # Initial user message\n    user_message = input(\"\\nUser: \")\n    messages = [{\"role\": \"user\", \"content\": [{\"text\": user_message}]}]\n\n    while True:\n        # If the last message is from the assistant, get another input from the user\n        if messages[-1].get(\"role\") == \"assistant\":\n            user_message = input(\"\\nUser: \")\n            messages.append({\"role\": \"user\", \"content\": [{\"text\": user_message}]})\n\n        # Parameters for API request to the Bedrock model\n        converse_api_params = {\n            \"modelId\": modelId,\n            \"system\": [{\"text\": system_prompt}],\n            \"messages\": messages,\n            \"inferenceConfig\": {\"maxTokens\": 4096},\n            \"toolConfig\": tool_config,  # Pass the tool config\n        }\n\n        # Get response from Bedrock model\n        response = bedrock_client.converse(**converse_api_params)\n\n        # Append assistant's message to the conversation\n        messages.append(\n            {\"role\": \"assistant\", \"content\": response[\"output\"][\"message\"][\"content\"]}\n        )\n\n        # If the model wants to use a tool, process the tool call\n        if response[\"stopReason\"] == \"tool_use\":\n            tool_use = response[\"output\"][\"message\"][\"content\"][\n                -1\n            ]  # Naive approach assumes only 1 tool is called at a time\n            tool_id = tool_use[\"toolUse\"][\"toolUseId\"]\n            tool_name = tool_use[\"toolUse\"][\"name\"]\n            tool_input = tool_use[\"toolUse\"][\"input\"]\n\n            print(f\"Claude wants to use the {tool_name} tool\")\n            print(f\"Tool Input:\")\n            print(json.dumps(tool_input, indent=2))\n\n            # Run the underlying tool functionality on the fake database\n            tool_result = process_tool_call(tool_name, tool_input)\n\n            print(f\"\\nTool Result:\")\n            print(json.dumps(tool_result, indent=2))\n\n            # Append tool result message\n            messages.append(\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"toolResult\": {\n                                \"toolUseId\": tool_id,\n                                \"content\": [{\"text\": str(tool_result)}],\n                            }\n                        }\n                    ],\n                }\n            )\n\n        else:\n            # If the model does not want to use a tool, just print the text response\n            print(\n                \"\\nTechNova Support:\"\n                + f\"{response['output']['message']['content'][0]['text']}\"\n            )\n```\n`simple_chat` å‡½æ•°å¤„ç†ç”¨æˆ·äº¤äº’ï¼Œè°ƒç”¨ LLMï¼Œå¹¶å°†å·¥å…·å“åº”ä¼ å›ž LLMã€‚\n\nè¯¥å‡½æ•°ä¸­çš„ä¸€ä¸ªé‡è¦è¡Œæ˜¯ `response[\"stopReason\"] == \"tool_use\"`ã€‚è¿™å†³å®šäº† LLM æ˜¯å¦æƒ³è¦ä½¿ç”¨å·¥å…·ï¼Œå¹¶åœ¨è¿›ä¸€æ­¥è§£æžæ—¶æŒ‡ç¤º LLM æ‰“ç®—è°ƒç”¨å“ªä¸ªå·¥å…·ã€‚\n\nä»¥ä¸‹æ˜¯ bedrock-runtime `converse` API çš„å“åº”å¯¹è±¡ç¤ºä¾‹ï¼š\n\n```python\n{\n    'ResponseMetadata': {\n        'RequestId': '07f323a7-cc52-4813-9d1b-83e5c3ae932a', \n        'HTTPStatusCode': 200, \n        'HTTPHeaders': {\n            'date': 'Thu, 08 Aug 2024 10:52:59 GMT', \n            'content-type': 'application/json', \n            'content-length': '519', \n            'connection': 'keep-alive', \n            'x-amzn-requestid': '07f323a7-cc52-4813-9d1b-83e5c3ae932a'\n        }, \n        'RetryAttempts': 0\n    }, \n    'output': {\n        'message': {\n            'role': 'assistant', 'content': [\n                {\n                    'text': \"Certainly! I'll search for search for your orders. Let me use our search tool to find that information for you.\"\n                }, {\n                    'toolUse': {\n                        'toolUseId': 'tooluse_8C_XIwrAROC3t3eEu5FCVw', \n                        'name': 'get_customer_orders', \n                        'input': {'customer_id': '1213210'}\n                    }\n                }\n            ]\n        }\n    }, \n    'stopReason': 'tool_use',\n    'usage': {'inputTokens': 672, 'outputTokens': 103, 'totalTokens': 775}, \n    'metrics': {'latencyMs': 2431}\n}\n```\næœ‰å…³ Converse API çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [Amazon Bedrock API å‚è€ƒ](https://proxy.rifx.online/https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html)ã€‚\n\nä¸€æ—¦æˆ‘ä»¬ä½¿ç”¨ `process_tool_call` å‡½æ•°è°ƒç”¨æ‰€éœ€çš„å·¥å…·æˆ–åŠŸèƒ½ï¼Œæˆ‘ä»¬å°†å‡½æ•°çš„å“åº”ä¼ å›ž LLMï¼Œä»¥ç”Ÿæˆæœ€ç»ˆç”¨æˆ·çš„å“åº”ã€‚\n\nè¯·æ³¨æ„ï¼Œæˆ‘ä»¬æ­£åœ¨ä½¿ç”¨ boto3 Bedrock è¿è¡Œæ—¶å®¢æˆ·ç«¯çš„ Converse APIã€‚æ‚¨è¿˜å¯ä»¥ä½¿ç”¨ Converse Stream API ç”Ÿæˆæµå¼å“åº”ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… Amazon Bedrock API å‚è€ƒä¸­çš„ Converse Stream API å’Œ Boto3 æ–‡æ¡£ä¸­çš„ Converse Stream APIã€‚\n\n### åœ¨æœ¬åœ°ç»ˆç«¯è¿è¡Œ\n\nä¸€æ—¦æ‚¨æ­£ç¡®è®¾ç½®äº†æ‰€æœ‰å†…å®¹ï¼Œè¯·åœ¨è™šæ‹ŸçŽ¯å¢ƒä¸­è¿è¡Œ Python æ–‡ä»¶ï¼Œä½¿ç”¨ï¼š\n\n```python\n## ä»Žè™šæ‹ŸçŽ¯å¢ƒå†…éƒ¨\npython main.py\n```\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Ok9N3mdX50JVWbaJKUrJeQ.gif)\n\n## åœ¨ EC2 ä¸Šéƒ¨ç½²\n\næ‚¨å¯ä»¥åœ¨ EC2 å®žä¾‹ä¸Šéƒ¨ç½²èŠå¤©æœºå™¨äººä»¥è¿›è¡Œæ¼”ç¤ºï¼Œä½¿ç”¨ [Gradio](https://proxy.rifx.online/https://www.gradio.app/) åº”ç”¨ç¨‹åºï¼Œå®ƒåªéœ€å‡ è¡Œä»£ç å³å¯æä¾›ç±»ä¼¼èŠå¤©æœºå™¨äººçš„ç•Œé¢ï¼Œå¹¶ä¸Žæˆ‘ä»¬çš„ä¸»å‡½æ•°æ— ç¼é›†æˆã€‚\n\n### Gradio\n\n[Gradio](https://proxy.rifx.online/https://www.gradio.app/) æ˜¯ä¸€ä¸ªå¼€æºçš„ Python åº“ï¼Œç®€åŒ–äº†æž„å»ºå’Œéƒ¨ç½²åŸºäºŽç½‘é¡µçš„æœºå™¨å­¦ä¹ æ¼”ç¤ºçš„è¿‡ç¨‹ã€‚å®ƒå…è®¸å¼€å‘è€…ä»¥æœ€å°‘çš„ç¼–ç åˆ›å»ºç›´è§‚çš„ç½‘é¡µç•Œé¢ï¼Œä½¿å¾—éƒ¨ç½²å’Œåˆ†äº«æ¨¡åž‹å˜å¾—æ›´åŠ å®¹æ˜“ã€‚\n\nè®©æˆ‘ä»¬ç¼–å†™ä¸€ä¸ªèŠå¤©å‡½æ•°ï¼Œéšæœºå“åº” `Yes` æˆ– `No`ï¼Œä½¿ç”¨ gradioã€‚\n\nè¿™æ˜¯æˆ‘ä»¬çš„èŠå¤©å‡½æ•°ï¼ˆå¦‚æžœæ‚¨è¿˜æ²¡æœ‰å®‰è£…ï¼Œè¯·åœ¨æ‚¨çš„è™šæ‹ŸçŽ¯å¢ƒä¸­æ‰§è¡Œ `pip install gradio`ï¼‰ï¼š\n\n```python\nimport random\n\nimport gradio as gr\n\n\ndef random_response(message, history):\n    return random.choice([\"Yes\", \"No\"])\n\ngr.ChatInterface(random_response).launch()\n```\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*XxkUM6yO3lmjN545tRlOvQ.png)\n\né˜…è¯»æ›´å¤šå…³äºŽ [gradio èŠå¤©æœºå™¨äººæ–‡æ¡£çš„ä¿¡æ¯](https://proxy.rifx.online/https://www.gradio.app/main/docs/gradio/chatbot)ã€‚\n\n### åœ¨æ‚¨çš„ Web æœåŠ¡å™¨ä¸Šä½¿ç”¨ Nginx è¿è¡Œ Gradio åº”ç”¨\n\nè®©æˆ‘ä»¬åœ¨ EC2 ä¸Šä½¿ç”¨ Nginx éƒ¨ç½²æˆ‘ä»¬çš„èŠå¤©æœºå™¨äººä»£ç†ã€‚\n\n**å®‰è£… Nginx å¹¶åˆ›å»ºæ–°çš„ conda çŽ¯å¢ƒ**\n\n1. **åˆ›å»ºä¸€ä¸ªè‡³å°‘æœ‰ 2â€“3 GB å†…å­˜çš„ EC2 å®žä¾‹**ã€‚æ‚¨ä¹Ÿå¯ä»¥åœ¨ Kubernetes æˆ– ECS é›†ç¾¤ä¸Šéƒ¨ç½²ã€‚ç¡®ä¿ä¿®æ”¹ Nginx é…ç½®æ–‡ä»¶ä»¥åŒ¹é…æ‚¨çš„è®¾ç½®ã€‚\n\n2\\. **SSH è¿›å…¥æ‚¨çš„ EC2 å®žä¾‹**å¹¶ [å®‰è£… Nginx](https://proxy.rifx.online/https://devopsden.io/article/how-to-install-nginx-on-ec2-instance):\n\n\n```python\nsudo yum update -y\nsudo amazon-linux-extras install nginx1.12\nsudo systemctl start nginx\nsudo systemctl enable nginx\nsudo systemctl status nginx\n```\n3\\. [**å®‰è£… Miniconda**](https://proxy.rifx.online/https://docs.anaconda.com/miniconda/#quick-command-line-install) ä»¥ç®¡ç† Python åŒ…:\n\n\n```python\nmkdir -p ~/miniconda3\nwget https://proxy.rifx.online/https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh\nbash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3\nrm -rf ~/miniconda3/miniconda.sh\n\n~/miniconda3/bin/conda init bash\n~/miniconda3/bin/conda init zsh\n```\n4\\. **åˆ›å»ºä¸€ä¸ªæ–°çš„ Conda çŽ¯å¢ƒ**ï¼Œä½¿ç”¨ Python 3ï¼Œå¹¶å®‰è£… `boto3` å’Œ `gradio`:\n\n\n```python\nconda create --name gradio-demo python=3.12 pip -y\nconda activate gradio-demo\npip install --no-cache-dir gradio boto3\n```\n5\\. **ä¸ºæ‚¨çš„èŠå¤©æœºå™¨äººå’Œ Gradio ä»£ç åˆ›å»ºä¸€ä¸ªæ–°çš„ Python æ–‡ä»¶**ã€‚å°†æ‰€æœ‰ä»£ç å¤åˆ¶åˆ°æ­¤æ–‡ä»¶ä¸­:\n\n\n```python\nvim gradio_demo.py\n```\næˆ–è€…ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ `scp` å°†æ–‡ä»¶ç›´æŽ¥ä»Žæœ¬åœ°è®¡ç®—æœºå¤åˆ¶åˆ°è¿œç¨‹å®žä¾‹ã€‚\n\n**è®¾ç½® Nginx**\n\nçŽ°åœ¨æˆ‘ä»¬å°† **è®¾ç½® Nginx** ä»¥å°†æ‰€æœ‰æµé‡ä»Ž `/gradio-demo` è·¯å¾„é‡å®šå‘åˆ°ç”± `gradio_demo.py` æ–‡ä»¶å¯åŠ¨çš„æœ¬åœ°æœåŠ¡å™¨ã€‚è¯·å‚é˜… [æ­¤å¤„çš„å®˜æ–¹æ–‡æ¡£ä»¥åœ¨ Nginx ä¸Šè¿è¡Œ Gradio](https://proxy.rifx.online/https://www.gradio.app/guides/running-gradio-on-your-web-server-with-nginx)ã€‚\n\n1. ç¼–è¾‘ä½äºŽ `/etc/nginx/nginx.conf` çš„ Nginx é…ç½®æ–‡ä»¶:\n\n\n```python\nvim /etc/nginx/nginx.conf\n```\n2\\. åœ¨ `http` å—ä¸­ï¼Œæ·»åŠ ä»¥ä¸‹è¡Œä»¥åŒ…å«æ¥è‡ªå•ç‹¬æ–‡ä»¶çš„æœåŠ¡å™¨å—é…ç½®:\n\n\n```python\nserver_names_hash_bucket_size  128;\ninclude /etc/nginx/sites-enabled/*;\n```\n3\\. åœ¨ `/etc/nginx/sites-available` ç›®å½•ä¸­åˆ›å»ºä¸€ä¸ªæ–°æ–‡ä»¶ï¼ˆå¦‚æžœè¯¥ç›®å½•ä¸å­˜åœ¨åˆ™åˆ›å»ºï¼‰ï¼Œä½¿ç”¨ä¸€ä¸ªè¡¨ç¤ºæ‚¨çš„åº”ç”¨ç¨‹åºçš„æ–‡ä»¶åï¼Œä¾‹å¦‚ï¼š`sudo vim /etc/nginx/sites-available/my_gradio_app` :\n\n\n```python\nsudo mkdir -p /etc/nginx/sites-enabled\nsudo vim /etc/nginx/sites-available/my_gradio_app\n```\nåœ¨ `my_gradio_app` æ–‡ä»¶ä¸­ç²˜è´´ä»¥ä¸‹å†…å®¹:\n\n\n```python\nserver {\n    listen 80;\n    server_name www.ec2-12-34-56-78.us-west-2.compute.amazonaws.com; # å°†æ­¤æ›´æ”¹ä¸ºæ‚¨çš„åŸŸå\n\n    location /gradio-demo/ {  # å¦‚æžœæ‚¨å¸Œæœ›åœ¨ä¸åŒè·¯å¾„ä¸Šæä¾› Gradio åº”ç”¨ï¼Œè¯·æ›´æ”¹æ­¤å¤„\n        proxy_pass http://127.0.0.1:7860/; # å¦‚æžœæ‚¨çš„ Gradio åº”ç”¨å°†åœ¨ä¸åŒç«¯å£ä¸Šè¿è¡Œï¼Œè¯·æ›´æ”¹æ­¤å¤„\n        proxy_buffering off;\n        proxy_redirect off;\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection \"upgrade\";\n        proxy_set_header Host $host;\n        proxy_set_header X-Forwarded-Host $host;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n}\n```\n4\\. åœ¨ `/etc/nginx/sites-enabled` ç›®å½•ä¸­åˆ›å»ºæŒ‡å‘æ­¤æ–‡ä»¶çš„ç¬¦å·é“¾æŽ¥:\n\n\n```python\nsudo ln -s /etc/nginx/sites-available/my_gradio_app /etc/nginx/sites-enabled/\n```\n5\\. **æ›´æ–° `gradio_demo.py` æ–‡ä»¶** ä»¥åœ¨ Gradio å¯åŠ¨ API ä¸­è®¾ç½®æ ¹è·¯å¾„:\n\n\n```python\n.launch(root_path=\"/gradio-demo\")\n```\n6\\. **æ£€æŸ¥ Nginx é…ç½®** å¹¶é‡å¯ Nginx:\n\n\n```python\nsudo nginx -t\nsudo systemctl restart nginx\n```\nå¦‚æžœæ‚¨åœ¨ `nginx -t` å‘½ä»¤ä¸­é‡åˆ°é”™è¯¯ï¼Œè¯·åœ¨ç»§ç»­ä¹‹å‰è§£å†³è¿™äº›é”™è¯¯ã€‚\n\n**åœ¨åŽå°è¿è¡Œ `gradio_demo.py` æ–‡ä»¶**ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ `nohup` æˆ– `tmux`:\n\n\n```python\n## ä»Ž Conda çŽ¯å¢ƒå†…éƒ¨\nnohup python gradio_demo.py &\n```\n**è®¿é—® EC2 DNS URL** å¹¶é™„åŠ  `/gradio-demo/` ä»¥æŸ¥çœ‹æ‚¨çš„èŠå¤©æœºå™¨äººä»£ç†åœ¨ Gradio ç•Œé¢ä¸Šã€‚\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*rcdUROlShsrcaeBDpBKBAQ.png)\n\n## æ‘˜è¦\n\nåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æŽ¢è®¨äº†å¦‚ä½•ä½¿ç”¨ [Llama3\\.1](https://proxy.rifx.online/https://llama.meta.com/) æˆ– [Claude 3\\.5 Sonnet](https://proxy.rifx.online/https://www.anthropic.com/news/claude-3-5-sonnet) æ¨¡åž‹æž„å»ºå®¢æˆ·æ”¯æŒåŠ©æ‰‹ã€‚æˆ‘ä»¬é¦–å…ˆå®šä¹‰äº†å¤„ç†é‡å¤å®¢æˆ·æŸ¥è¯¢çš„é—®é¢˜ï¼Œä»¥åŠ LLM ä»£ç†å¦‚ä½•æä¾›è§£å†³æ–¹æ¡ˆã€‚æŽ¥ç€ï¼Œæˆ‘ä»¬è®¨è®ºäº† LLM ä»£ç†çš„æ¦‚å¿µä»¥åŠå®ƒä»¬ä¸Žä¸€èˆ¬ LLM çš„åŒºåˆ«ã€‚ä¹‹åŽï¼Œæˆ‘ä»¬æ¼”ç¤ºäº†å¦‚ä½•åœ¨ Python ä¸­åˆ›å»ºä¸€ä¸ªåŸºæœ¬ä»£ç†ï¼Œå¹¶ä½¿ç”¨ Amazon Bedrock ä¸­çš„æ¨¡åž‹å¼€å‘äº†ä¸€ä¸ªæ›´å¤æ‚çš„å®¢æˆ·æ”¯æŒåŠ©æ‰‹ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†å¦‚ä½•åœ¨ EC2 ä¸Šéƒ¨ç½²åŠ©æ‰‹ï¼ŒåŒ…æ‹¬ä½¿ç”¨ Gradio åˆ›å»º Web ç•Œé¢çš„ç¤ºä¾‹ã€‚é€šè¿‡è‡ªåŠ¨åŒ–å¸¸è§„å®¢æˆ·æ”¯æŒä»»åŠ¡ï¼Œä¼ä¸šå¯ä»¥æé«˜æ•ˆçŽ‡ï¼Œé™ä½Žæˆæœ¬ï¼Œå¹¶æ”¹å–„å®¢æˆ·æ»¡æ„åº¦ã€‚\n\nåœ¨ç”Ÿäº§çŽ¯å¢ƒä¸­ï¼Œæ‚¨å¯ä»¥å°†ç™»å½•ç”¨æˆ·çš„å§“åå’Œ ID ä¼ é€’ç»™ç³»ç»Ÿæç¤ºï¼Œä»¥ä¾¿ LLM ä¸å¿…å‘ç™»å½•ç”¨æˆ·è¯¢é—®åŸºæœ¬ä¿¡æ¯ã€‚æŸäº›æ“ä½œï¼Œä¾‹å¦‚å–æ¶ˆè®¢å•ï¼Œå¯èƒ½éœ€è¦é¢å¤–çš„é—¨æŽ§ã€‚æ­¤å¤–ï¼Œå¦‚æžœå®¢æˆ·æ„Ÿåˆ°ä¸æ»¡æˆ–å˜å¾—æ¿€åŠ¨ï¼Œåº”è¯¥æŒ‡ç¤º LLM å°†æ¡ˆä»¶å‡çº§åˆ°äººç±»åŠ©æ‰‹ã€‚\n\næ‚¨å¯ä»¥é€šè¿‡ LinkedIn ä¸Žæˆ‘è”ç³»ï¼š<https://proxy.rifx.online/https://linkedin.com/in/maheshrajput>\n\næ„Ÿè°¢æ‚¨çš„é˜…è¯» ðŸ˜Š\n\n"},{"lang":"zh","group":"blog","slug":"blog/build-a-rag-based-scientific-chatbot-with-langchain-streamlit-pubmed-part-4-put-it-all-ba7bbf706bbd","frontmatter":{"title":"ä½¿ç”¨ LangChainã€Streamlit å’Œ PubMed æž„å»ºåŸºäºŽ RAG çš„ç§‘å­¦èŠå¤©æœºå™¨äºº--ç¬¬ 4 éƒ¨åˆ†ï¼ˆå°†æ‰€æœ‰...","meta_title":"ä½¿ç”¨ LangChainã€Streamlit å’Œ PubMed æž„å»ºåŸºäºŽ RAG çš„ç§‘å­¦èŠå¤©æœºå™¨äºº--ç¬¬ 4 éƒ¨åˆ†ï¼ˆå°†æ‰€æœ‰...","description":"å¤§å®¶å¥½ï¼Œæ¬¢è¿Žæ¥åˆ°ä½¿ç”¨ Langchainã€Streamlit å’Œ PubMed æž„å»ºç§‘å­¦èŠå¤©æœºå™¨äººç³»åˆ—çš„æœ€åŽä¸€éƒ¨åˆ†ï¼","date":"2024-11-13T01:22:29.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MQ7XtBd9WHn5n-gAMgd6pQ.jpeg","categories":["Chatbots","Natural Language Processing","Science"],"author":"Rifx.Online","tags":["ChatBot","LangChain","Streamlit","PubMed","RAG"],"draft":false,"slug":"blog/build-a-rag-based-scientific-chatbot-with-langchain-streamlit-pubmed-part-4-put-it-all-ba7bbf706bbd"},"content":"\n\n\n\n\næ‚¨å¥½ï¼Œæ¬¢è¿Žæ¥åˆ°æž„å»ºç§‘å­¦èŠå¤©æœºå™¨äººçš„ç³»åˆ—æœ€åŽä¸€éƒ¨åˆ†ï¼Œä½¿ç”¨Langchainã€Streamlitå’ŒPubMedï¼\n\nåœ¨å‰ä¸€éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬æž„å»ºäº†æ•°æ®æŒä¹…æ€§å’Œå¸¦æœ‰å‘é‡å­˜å‚¨çš„RAGç®¡é“ã€‚çŽ°åœ¨ï¼Œæ˜¯æ—¶å€™å°†æˆ‘ä»¬æ‰€æž„å»ºçš„ä¸€åˆ‡æ•´åˆåœ¨ä¸€èµ·ï¼Œåˆ›å»ºèŠå¤©æœºå™¨äººç”¨æˆ·ç•Œé¢ï¼Œåˆ©ç”¨æˆ‘ä»¬æž„å»ºçš„åŽç«¯åŠŸèƒ½ï¼Œå¸®åŠ©ç§‘å­¦å®¶å›žç­”ä»–ä»¬çš„ç§‘å­¦é—®é¢˜ï¼\n\nä½œä¸ºæé†’ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬åœ¨ç³»åˆ—ä¸­æž„å»ºçš„å®Œæ•´è§£å†³æ–¹æ¡ˆï¼š\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*NFCO_uRjlAgm0WYH.png)\n\n## åº”ç”¨æ¼”ç¤º\n\n* ä½œä¸ºé¢„å‘Šï¼Œè®©æˆ‘ä»¬å…ˆæ¥çœ‹çœ‹åº”ç”¨çš„ç•Œé¢ç¤ºä¾‹ï¼ \n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*OKEQO_2kwnV93Va4SAVWZg.gif)\n\n## å»ºè®¾\n\n### å·²å®Œæˆæ­¥éª¤æ¦‚è¿°\n\n* å¦‚æžœæ‚¨è¿˜æ²¡æœ‰å®Œæˆ [ç¬¬ä¸€éƒ¨åˆ†](https://proxy.rifx.online/https://readmedium.com/build-a-rag-based-scientific-chatbot-with-langchain-streamlit-pubmed-part-1-set-up-streamlit-37550b44b266)ã€[ç¬¬äºŒéƒ¨åˆ†](https://proxy.rifx.online/https://readmedium.com/llm-aided-retrieval-of-relevant-scientific-abstracts-via-pubmed-api-using-natural-language-part2-9e10f78575e6) å’Œ [ç¬¬ä¸‰éƒ¨åˆ†](https://proxy.rifx.online/https://readmedium.com/build-a-rag-based-scientific-chatbot-with-langchain-streamlit-pubmed-part-3-create-vector-1e5e401e72e6)ï¼Œè¯·åŠ¡å¿…å…ˆå®Œæˆï¼Œå› ä¸ºæˆ‘ä»¬å°†åŸºäºŽè¿™äº›å†…å®¹è¿›ä¸€æ­¥æž„å»ºã€‚åœ¨æœ€åŽä¸€éƒ¨åˆ†ç»“æŸæ—¶ï¼Œæˆ‘ä»¬å¾—åˆ°äº†å¦‚ä¸‹çš„é¡¹ç›®ç»“æž„ï¼š\n\n```python\n.\nâ”œâ”€â”€ app\nâ”‚   â”œâ”€â”€ app.py\nâ”‚   â”œâ”€â”€ backend\nâ”‚   â”‚  â”œâ”€â”€ abstract_retrieval\nâ”‚   â”‚  â”‚   â”œâ”€â”€ interface.py\nâ”‚   â”‚  â”‚   â”œâ”€â”€ pubmed_retriever.py\nâ”‚   â”‚  â”‚   â””â”€â”€ pubmed_query_simplification.py\nâ”‚   â”‚  â”œâ”€â”€ data_repository\nâ”‚   â”‚  â”‚   â”œâ”€â”€ interface.py\nâ”‚   â”‚  â”‚   â”œâ”€â”€ local_data_store.py\nâ”‚   â”‚  â”‚   â””â”€â”€ models.py\nâ”‚   â”‚  â””â”€â”€ rag_pipeline\nâ”‚   â”‚      â”œâ”€â”€ interface.py\nâ”‚   â”‚      â”œâ”€â”€ chromadb_rag.py\nâ”‚   â”‚      â””â”€â”€ embeddings.py\nâ”‚   â”œâ”€â”€ components\nâ”‚   â”‚   â”œâ”€â”€ chat_utils.py\nâ”‚   â”‚   â”œâ”€â”€ llm.py\nâ”‚   â”‚   â””â”€â”€ prompts.py\nâ”‚   â””â”€â”€ tests\nâ”‚       â””â”€â”€ test_chat_utils.py\nâ”œâ”€â”€ assets\nâ”‚   â””â”€â”€ pubmed-screener-logo.jpg\nâ””â”€â”€ environment\n    â””â”€â”€ requirements.txt\n```\nåœ¨ç³»åˆ—çš„æœ€åŽä¸€éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†é‡ç‚¹å…³æ³¨å®šä¹‰æˆ‘ä»¬çš„ Streamlit UI çš„ä»£ç éƒ¨åˆ†â€”â€”***app/app.py*** å’Œ ***app/components*** æ¨¡å—ã€‚\n\n### ä¿®æ”¹ chat\\_utils.py ä»¥åŒ…å« RAG é€»è¾‘\n\n[åœ¨ç¬¬ä¸€éƒ¨åˆ†](https://proxy.rifx.online/https://readmedium.com/build-a-rag-based-scientific-chatbot-with-langchain-streamlit-pubmed-part-1-set-up-streamlit-37550b44b266)ï¼Œæˆ‘ä»¬æž„å»ºäº†ä¸€ä¸ªåˆæ­¥ç‰ˆæœ¬çš„ ***chat\\_utils.py***ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªç®€å•çš„ QA èŠå¤©æœºå™¨äººå®žçŽ°ï¼ˆæ²¡æœ‰ RAGï¼‰ã€‚çŽ°åœ¨ï¼Œæˆ‘ä»¬å°†æ·±å…¥ç ”ç©¶å¹¶å°†å…¶è½¬æ¢ä¸ºä¸€ä¸ªä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ QA èŠå¤©æœºå™¨äººï¼Œè¯¥æœºå™¨äººå°†æ ¹æ®ç”¨æˆ·é—®é¢˜æž„å»ºç­”æ¡ˆï¼Œå¹¶é€šè¿‡ç›¸ä¼¼æ€§æœç´¢ä»Žæˆ‘ä»¬çš„å‘é‡ç´¢å¼•ä¸­æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡ï¼ˆæ‘˜è¦ï¼‰ã€‚\n\næˆ‘ä»¬å°†ä½¿ç”¨[ç¬¬ä¸‰éƒ¨åˆ†](https://proxy.rifx.online/https://readmedium.com/build-a-rag-based-scientific-chatbot-with-langchain-streamlit-pubmed-part-3-create-vector-1e5e401e72e6)ä¸­æž„å»ºçš„æ‰€æœ‰åŽç«¯åŠŸèƒ½æ¥å®žçŽ°è¿™ä¸€ç›®çš„ã€‚\n\n**app/components/chat\\_utils.py**\n\n```python\nfrom typing import List\nimport streamlit as st\nfrom langchain_core.documents.base import Document\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_core.runnables.base import Runnable\nfrom langchain_core.runnables.utils import Output\nfrom langchain_community.chat_message_histories import StreamlitChatMessageHistory\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain.vectorstores import VectorStore\n\n\nclass ChatAgent:\n    def __init__(self, prompt: ChatPromptTemplate, llm: Runnable):\n        \"\"\"\n        åˆå§‹åŒ– ChatAgentã€‚\n\n        å‚æ•°ï¼š\n        - prompt (ChatPromptTemplate): èŠå¤©æç¤ºæ¨¡æ¿ã€‚\n        - llm (Runnable): è¯­è¨€æ¨¡åž‹å¯è¿è¡Œå¯¹è±¡ã€‚\n        \"\"\"\n        self.history = StreamlitChatMessageHistory(key=\"chat_history\")\n        self.llm = llm\n        self.prompt = prompt\n        self.chain = self.setup_chain()\n    \n    def reset_history(self) -> None:\n        \"\"\"\n        æ¸…é™¤èŠå¤©åŽ†å²ä»¥å¼€å§‹æ–°çš„èŠå¤©ä¼šè¯ã€‚\n        \"\"\"\n        self.history.clear()\n\n    def setup_chain(self) -> RunnableWithMessageHistory:\n        \"\"\"\n        ä¸º ChatAgent è®¾ç½®é“¾ã€‚\n\n        è¿”å›žï¼š\n        - RunnableWithMessageHistory: é…ç½®å¥½çš„å¸¦æœ‰æ¶ˆæ¯åŽ†å²çš„é“¾ã€‚\n        \"\"\"\n        chain = self.prompt | self.llm\n        return RunnableWithMessageHistory(\n            chain,\n            lambda session_id: self.history,\n            input_messages_key=\"question\",\n            history_messages_key=\"history\",\n        )\n\n    def display_messages(self, selected_query: str) -> None:\n        \"\"\"\n        åœ¨èŠå¤©ç•Œé¢æ˜¾ç¤ºæ¶ˆæ¯ã€‚\n        å¦‚æžœæ²¡æœ‰æ¶ˆæ¯ï¼Œåˆ™æ·»åŠ é»˜è®¤çš„ AI æ¶ˆæ¯ã€‚\n        \"\"\"\n        if len(self.history.messages) == 0:\n            self.history.add_ai_message(f\"è®©æˆ‘ä»¬èŠèŠä½ çš„é—®é¢˜ï¼š{selected_query}\")\n        for msg in self.history.messages:\n            st.chat_message(msg.type).write(msg.content)\n    \n    def format_retreieved_abstracts_for_prompt(self, documents: List[Document]) -> str:\n        \"\"\"\n        å°†æ£€ç´¢åˆ°çš„æ–‡æ¡£æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²ï¼Œä»¥ä¾¿ä¼ é€’ç»™ LLMã€‚\n        \"\"\"\n        formatted_strings = []\n        for doc in documents:\n            formatted_str = f\"æ‘˜è¦æ ‡é¢˜ï¼š{doc.metadata['title']}, æ‘˜è¦å†…å®¹ï¼š{doc.page_content}, æ‘˜è¦ DOIï¼š{doc.metadata['source'] if 'source' in doc.metadata.keys() else 'ç¼ºå°‘ DOI..'}\"\n            formatted_strings.append(formatted_str)\n        return \"; \".join(formatted_strings)\n    \n    def get_answer_from_llm(self, question: str, retrieved_documents: List[Document]) -> Output:\n        \"\"\"\n        æ ¹æ®ç”¨æˆ·é—®é¢˜å’Œæ£€ç´¢åˆ°çš„æ–‡æ¡£ä»Ž LLM èŽ·å–å“åº”ã€‚\n        \"\"\"\n        config = {\"configurable\": {\"session_id\": \"any\"}}\n        return self.chain.invoke(\n            {\n                \"question\": question, \n                \"retrieved_abstracts\": retrieved_documents,\n            }, config\n        )\n    \n    def retrieve_documents(self, retriever: VectorStore, question: str, cut_off: int = 5) -> List[Document]:\n        \"\"\"\n        ä½¿ç”¨ç›¸ä¼¼æ€§æœç´¢æ£€ç´¢æ–‡æ¡£\n        cut_off å‚æ•°æŽ§åˆ¶æ£€ç´¢åˆ°çš„ç»“æžœæ•°é‡ï¼ˆé»˜è®¤ä¸º 5ï¼‰\n        \"\"\"\n        return retriever.similarity_search(question)[:cut_off]\n\n    def start_conversation(self, retriever: VectorStore, selected_query: str) -> None:\n        \"\"\"\n        åœ¨èŠå¤©ç•Œé¢å¼€å§‹å¯¹è¯ã€‚\n        æ˜¾ç¤ºæ¶ˆæ¯ï¼Œæç¤ºç”¨æˆ·è¾“å…¥ï¼Œå¹¶å¤„ç† AI å“åº”ã€‚\n        \"\"\"\n        self.display_messages(selected_query)\n        user_question = st.chat_input(placeholder=\"é—®æˆ‘ä»»ä½•äº‹æƒ…..\")\n        if user_question:\n            documents = self.retrieve_documents(retriever, user_question)\n            retrieved_abstracts = self.format_retreieved_abstracts_for_prompt(documents)\n            st.chat_message(\"human\").write(user_question)\n            response = self.get_answer_from_llm(user_question, retrieved_abstracts)\n            st.chat_message(\"ai\").write(response.content)\n```\n**æ›´æ”¹å†…å®¹ï¼š**\n\n* æˆ‘ä»¬æ·»åŠ äº†æ–¹æ³• ***retrieve\\_documents***ï¼Œè¯¥æ–¹æ³•å°†æˆ‘ä»¬çš„å‘é‡ç´¢å¼•ï¼ˆæ£€ç´¢å™¨ï¼‰ä½œä¸ºå‚æ•°ï¼Œå¹¶è°ƒç”¨æ£€ç´¢å™¨ä¸Šçš„æ–¹æ³• similarity\\_searchï¼Œä»Žæˆ‘ä»¬çš„ç§‘å­¦æ‘˜è¦çš„å‘é‡ç´¢å¼•ä¸­èŽ·å–ä¸Žç”¨æˆ·é—®é¢˜æœ€ç›¸ä¼¼çš„è®°å½•ã€‚è¯·æ³¨æ„å‚æ•° cut\\_offï¼Œå®ƒæŒ‡å®šè¦æ£€ç´¢çš„ç»“æžœæ•°é‡ï¼ˆé»˜è®¤ä¸º 5ï¼‰ã€‚\n* æ·»åŠ äº†æ–¹æ³• ***format\\_retreieved\\_abstracts\\_for\\_prompt***ï¼Œè¯¥æ–¹æ³•æŽ¥æ”¶é€šè¿‡ retrieve\\_documents æ–¹æ³•æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼Œå¹¶å°†å…¶æ ¼å¼åŒ–ä¸º LLM ä½¿ç”¨ã€‚è¿™åœ¨æˆ‘ä»¬è¦æ±‚ LLM åœ¨æç¤ºä¸­å¼•ç”¨ç›¸å…³æ¥æºï¼ˆæ–‡ç«  DOI å’Œæ ‡é¢˜ï¼‰æ—¶å°†éžå¸¸æœ‰ç”¨ã€‚\n* æ·»åŠ äº†æ–¹æ³• ***get\\_answer\\_from\\_llm***ï¼Œç”¨äºŽè°ƒç”¨ LLM å¹¶ä¼ é€’å¿…è¦çš„å˜é‡ï¼Œä»¥ä¿æŒå®¢æˆ·ç«¯å‡½æ•° start\\_conversation çš„ç®€æ´ã€‚\n* ä¿®æ”¹äº† ***start\\_conversation*** æ–¹æ³•ä»¥åŒ…å« RAG é€»è¾‘ã€‚\n\n### åˆ›å»º QA èŠå¤©æç¤º\n\n* æˆ‘ä»¬å°†ä¿®æ”¹çŽ°æœ‰çš„èŠå¤©æç¤ºï¼Œä»¥åŒ…å«æ£€ç´¢åˆ°çš„æ‘˜è¦ï¼Œå¹¶åŸºäºŽè¿™äº›æ‘˜è¦æž„å»ºç­”æ¡ˆã€‚\n* æˆ‘ä»¬è¿˜å°†åŒ…å«ä¸€ä¸ªé¢å¤–çš„ï¼ˆç®€å•çš„ï¼‰æç¤ºï¼Œç”¨äºŽåœ¨èŠå¤©æœºå™¨äººéƒ¨åˆ†ä¹‹å¤–æä¾›ç›´æŽ¥çš„å³æ—¶ç­”æ¡ˆï¼Œä»¥ä¾¿ç”¨æˆ·åœ¨ UI ä¸ŠèŽ·å¾—ç›´æŽ¥çš„ç­”æ¡ˆã€‚\n\n**app/components/chat\\_prompts.py**\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, PromptTemplate\n\n\nchat_prompt_template = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are a knowledgeable expert chatbot in the biomedicine field.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\n            \"human\", \n            \"\"\"\n            Answer the following scientific question: {question}, \n            using the following context retrieved from scientific articles: {retrieved_abstracts}.\n\n            The user might refer to the history of your conversation. Please, use the following history of messages for the context as you see fit.\n\n            The abstracts will come formatted in the following way: ABSTRACT TITLE: <abstract title>; ABSTRACT CONTENT: <abstract content>, ABSTRACT DOI: <abstract doi> (the content inside <> will be variable).\n            In your answer, ALWAYS cite the abstract title and abstract DOI when citing a particular piece of information from that given abstract.\n\n            Your example response might look like this:\n\n            In the article (here in the brackets goes the contents of ABSTRACT_TITLE), it was discussed, that Cannabis hyperemesis syndrome (CHS) is associated with chronic, heavy cannabis use. The endocannabinoid system (ECS) plays a crucial role in the effects of cannabis on end organs and is central to the pathophysiology of CHS. (here, in the end of the cited chunk, the ABSTRACT_DOI goes)\n            \"\"\"\n        ),\n    ]\n)\n\nqa_template = PromptTemplate(\n    input_variables=['question', 'retrieved_abstracts'],\n    template=\"\"\"\n        Answer the following scientific question: {question}, \n        using the following context retrieved from scientific articles: {retrieved_abstracts}.\n\n        The abstracts will come formatted in the following way: ABSTRACT TITLE: <abstract title>; ABSTRACT CONTENT: <abstract content>, ABSTRACT DOI: <abstract doi> (the content inside <> will be variable).\n        In your answer, ALWAYS cite the abstract title and abstract DOI when citing a particular piece of information from that given abstract.\n\n        Your example response might look like this:\n\n        In the article (here in the brackets goes the contents of ABSTRACT_TITLE), it was discussed, that Cannabis hyperemesis syndrome (CHS) is associated with chronic, heavy cannabis use. The endocannabinoid system (ECS) plays a crucial role in the effects of cannabis on end organs and is central to the pathophysiology of CHS. (here, in the end of the cited chunk, the ABSTRACT_DOI goes)\n    \"\"\"\n)\n```\n* è¯·æ³¨æ„ï¼Œä¸¤ä¸ªæç¤ºçš„å†…å®¹å‡ ä¹Žç›¸åŒï¼Œä½†èŠå¤©æç¤ºåŒ…å«å¯¹èŠå¤©åŽ†å²çš„å¼•ç”¨ï¼Œä½¿ç”¨ MessagesPlaceholderï¼Œå¹¶æŒ‡ç¤ºåœ¨å¯¹è¯è¿‡ç¨‹ä¸­æ ¹æ® LLM çš„åˆ¤æ–­ä½¿ç”¨èŠå¤©åŽ†å²ã€‚\n\n### åˆ›å»ºæ–°æ–‡ä»¶ app/components/layout\\_extensions.py\n\n* è¯¥æ–‡ä»¶å°†ä¿å­˜ä¸€ä¸ªè¾…åŠ©å‡½æ•°ï¼Œè¯¥å‡½æ•°å°†å‘ç”¨æˆ·å‘ˆçŽ°æˆ‘ä»¬åº”ç”¨ç¨‹åºå¸ƒå±€çš„ä¸€éƒ¨åˆ†ï¼Œå¹¶æä¾›æŸ¥è¯¢ç¤ºä¾‹ï¼ˆå¦‚ä½•ä½¿ç”¨åº”ç”¨ç¨‹åºçš„æç¤ºï¼‰ã€‚æˆ‘å†³å®šåˆ›å»ºè¿™ä¸ªæ‰©å±•æ–‡ä»¶ï¼Œä»¥é¿å…ä½¿æˆ‘ä»¬çš„ app.py æ–‡ä»¶æ‚ä¹±ï¼Œå¹¶ä¿æŒå…¶æ•´æ´ï¼Œå› ä¸ºè¿™æ®µä»£ç ç›¸å½“å†—é•¿ï¼Œå¹¶åŒ…å«ä¸€äº›è‡ªå®šä¹‰æ ·å¼ï¼ˆåº”ç”¨ä¿¡æ¯å°†åœ¨ç”¨æˆ·æ‚¬åœæ—¶æ˜¾ç¤ºï¼‰ï¼š\n\n```python\nimport streamlit as st\n\n\ndef render_app_info():\n    st.title(\"PubMed Screener\")\n    st.markdown(\"\"\"\n        PubMed Screener is a ChatGPT & PubMed powered insight generator from biomedical abstracts.\n    \"\"\")\n\n    # Adding custom HTML and CSS for an improved hover-over tooltip\n    st.markdown(\"\"\"\n        <style>\n        .tooltip {\n            position: relative;\n            display: inline-block;\n            border-bottom: 1px dotted black; /* Style for the hoverable text */\n        }\n\n        .tooltip .tooltiptext {\n            visibility: hidden;\n            width: 800px; /* Width to fit content */\n            background-color: #f9f9f9;\n            color: #000;\n            text-align: left;\n            border-radius: 6px;\n            padding: 15px;\n            position: absolute;\n            z-index: 1;\n            bottom: 100;\n            right: -430px; /* Positioning to the right and slightly offset */\n            opacity: 0;\n            transition: opacity 0.5s;\n            box-shadow: 0px 8px 16px 0px rgba(0,0,0,0.8); /* Adding some shadow for better visibility */\n        }\n\n        .tooltip:hover .tooltiptext {\n            visibility: visible;\n            opacity: 1;\n        }\n        </style>\n        <div class=\"tooltip\">ðŸ” ç¤ºä¾‹é—®é¢˜\n            <span class=\"tooltiptext\">\n                <strong>ç¤ºä¾‹ç§‘å­¦é—®é¢˜ï¼š</strong>\n                <ul>\n                    <li>å¦‚ä½•åˆ©ç”¨å…ˆè¿›çš„æˆåƒæŠ€æœ¯å’Œç”Ÿç‰©æ ‡å¿—ç‰©æ—©æœŸè¯Šæ–­å’Œç›‘æµ‹ç¥žç»é€€è¡Œæ€§ç–¾ç—…çš„è¿›å±•ï¼Ÿ</li>\n                    <li>å¹²ç»†èƒžæŠ€æœ¯å’Œå†ç”ŸåŒ»å­¦åœ¨ç¥žç»é€€è¡Œæ€§ç–¾ç—…æ²»ç–—ä¸­çš„æ½œåœ¨åº”ç”¨æ˜¯ä»€ä¹ˆï¼Ÿç›¸å…³æŒ‘æˆ˜åˆæ˜¯ä»€ä¹ˆï¼Ÿ</li>\n                    <li>è‚ é“å¾®ç”Ÿç‰©ç¾¤å’Œè‚ è„‘è½´åœ¨1åž‹å’Œ2åž‹ç³–å°¿ç—…å‘ç—…æœºåˆ¶ä¸­çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿå¦‚ä½•è°ƒèŠ‚è¿™äº›ç›¸äº’ä½œç”¨ä»¥èŽ·å¾—æ²»ç–—ç›Šå¤„ï¼Ÿ</li>\n                    <li>é’ˆå¯¹ç™Œç—‡é¶å‘æ²»ç–—çš„è€è¯æ€§å‘å±•çš„åˆ†å­æœºåˆ¶æ˜¯ä»€ä¹ˆï¼Ÿå¦‚ä½•å…‹æœè¿™äº›è€è¯æœºåˆ¶ï¼Ÿ</li>\n                </ul>\n            </span>\n        </div>\n        \"\"\", unsafe_allow_html=True)\n    \n    st.text(\"\")py\n```\n\n### ä¿®æ”¹ app/app.py\n\n* æœ€åŽï¼Œæ˜¯æ—¶å€™å°†æˆ‘ä»¬æž„å»ºçš„æ‰€æœ‰å†…å®¹æ•´åˆåœ¨ä¸€èµ·ï¼Œå¹¶å°†å…¶ä½œä¸ºä¸€ä¸ª streamlit åº”ç”¨ç¨‹åºè¿›è¡Œå±•ç¤ºï¼\n\n\n```python\nimport streamlit as st\nfrom metapub import PubMedFetcher\nfrom components.chat_utils import ChatAgent\nfrom components.chat_prompts import chat_prompt_template, qa_template\nfrom components.llm import llm\nfrom components.layout_extensions import render_app_info\nfrom backend.abstract_retrieval.pubmed_retriever import PubMedAbstractRetriever\nfrom backend.data_repository.local_storage import LocalJSONStore\nfrom backend.rag_pipeline.chromadb_rag import ChromaDbRag\nfrom backend.rag_pipeline.embeddings import embeddings\n\n\n## å®žä¾‹åŒ–å¯¹è±¡\npubmed_client = PubMedAbstractRetriever(PubMedFetcher())\ndata_repository = LocalJSONStore(storage_folder_path=\"backend/data\")\nrag_client = ChromaDbRag(persist_directory=\"backend/chromadb_storage\", embeddings=embeddings)\nchat_agent = ChatAgent(prompt=chat_prompt_template, llm=llm)\n\ndef main():\n    st.set_page_config(\n        page_title=\"Pubmed æ‘˜è¦ç­›é€‰å™¨\",\n        page_icon='ðŸ’¬',\n        layout='wide'\n    )\n\n    # å®šä¹‰åˆ— - è¿™å°†ä½¿å¸ƒå±€æ°´å¹³åˆ†å‰²\n    column_logo, column_app_info, column_answer = st.columns([1, 4, 4])\n\n    # åœ¨ç¬¬ä¸€åˆ—æ”¾ç½® logo\n    with column_logo:\n        st.image('../assets/pubmed-screener-logo.jpg')\n\n    # åœ¨ç¬¬äºŒåˆ—æ”¾ç½®è§£é‡Šåº”ç”¨ç¨‹åºç›®çš„çš„æ–‡æœ¬ä»¥åŠç”¨æˆ·å¯èƒ½æå‡ºçš„ä¸€äº›ç¤ºä¾‹ç§‘å­¦é—®é¢˜ã€‚\n    with column_app_info:\n\n        # è¿è¡Œåº”ç”¨ç¨‹åºä¿¡æ¯ï¼ŒåŒ…æ‹¬ç¤ºä¾‹é—®é¢˜ä½œä¸ºç”¨æˆ·çš„æç¤º\n        render_app_info()\n\n        # è¾“å…¥ç§‘å­¦é—®é¢˜çš„éƒ¨åˆ†\n        st.header(\"è¾“å…¥æ‚¨çš„ç§‘å­¦é—®é¢˜ï¼\")\n        placeholder_text = \"åœ¨æ­¤è¾“å…¥æ‚¨çš„ç§‘å­¦é—®é¢˜...\"\n        scientist_question = st.text_input(\"æ‚¨çš„é—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ\", placeholder_text)\n        get_articles = st.button('èŽ·å–æ–‡ç«  & ç­”æ¡ˆ')\n\n        # å¤„ç†ç”¨æˆ·é—®é¢˜ï¼ŒèŽ·å–æ•°æ®\n        with st.spinner('æ­£åœ¨èŽ·å–æ‘˜è¦ã€‚è¿™å¯èƒ½éœ€è¦ä¸€æ®µæ—¶é—´...'):\n            if get_articles:\n                if scientist_question and scientist_question != placeholder_text:\n\n                    # èŽ·å–æ‘˜è¦æ•°æ®\n                    retrieved_abstracts = pubmed_client.get_abstract_data(scientist_question)\n                    if not retrieved_abstracts:\n                        st.write('æœªæ‰¾åˆ°æ‘˜è¦ã€‚')\n                    else:\n                        # å°†æ‘˜è¦ä¿å­˜åˆ°å­˜å‚¨å¹¶åˆ›å»ºå‘é‡ç´¢å¼•\n                        query_id = data_repository.save_dataset(retrieved_abstracts, scientist_question)\n                        documents = data_repository.create_document_list(retrieved_abstracts)\n                        rag_client.create_vector_index_for_user_query(documents, query_id)\n                        \n                        # ç›´æŽ¥å›žç­”ç”¨æˆ·é—®é¢˜å¹¶åœ¨ UI ä¸Šæ˜¾ç¤ºç­”æ¡ˆ\n                        vector_index = rag_client.get_vector_index_by_user_query(query_id)\n                        retrieved_documents = chat_agent.retrieve_documents(vector_index, scientist_question)\n                        chain = qa_template | llm\n                        \n                        with column_answer:\n                            st.markdown(f\"##### æ‚¨çš„é—®é¢˜çš„ç­”æ¡ˆï¼š'{scientist_question}'\")\n                            st.write(chain.invoke({\n                                \"question\": scientist_question, \n                                \"retrieved_abstracts\": retrieved_documents,\n                            }).content)\n\n    # èŠå¤©æœºå™¨äººéƒ¨åˆ†çš„å¼€å§‹\n    # æ˜¾ç¤ºæŸ¥è¯¢åˆ—è¡¨ä»¥é€‰æ‹©ä¸€ä¸ªè¿›è¡Œå¯¹è¯\n    query_options = data_repository.get_list_of_queries()\n\n    if query_options:\n        st.header(\"ä¸Žæ‘˜è¦èŠå¤©\")\n        selected_query = st.selectbox('é€‰æ‹©ä¸€ä¸ªè¿‡åŽ»çš„æŸ¥è¯¢', options=list(query_options.values()), key='selected_query')\n        \n        # åˆå§‹åŒ–å…³äºŽç”¨æˆ·é—®é¢˜åŽ†å²ä¸­çš„æŸä¸ªæŸ¥è¯¢çš„èŠå¤©\n        if selected_query:\n            selected_query_id = next(key for key, val in query_options.items() if val == selected_query)\n            vector_index = rag_client.get_vector_index_by_user_query(selected_query_id)\n\n            # åˆ‡æ¢æŸ¥è¯¢è¿›è¡ŒèŠå¤©æ—¶æ¸…é™¤èŠå¤©åŽ†å²\n            if 'prev_selected_query' in st.session_state and st.session_state.prev_selected_query != selected_query:\n                chat_agent.reset_history()\n\n            st.session_state.prev_selected_query = selected_query\n\n            # å¼€å§‹èŠå¤©ä¼šè¯\n            chat_agent.start_conversation(vector_index, selected_query)\n\n\nif __name__ == \"__main__\":\n    main()\n```\n* ä»£ç åŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼š\n1. å®žä¾‹åŒ–æˆ‘ä»¬åœ¨ç³»åˆ—ä¹‹å‰éƒ¨åˆ†ä¸­æž„å»ºçš„æ‰€æœ‰å¯¹è±¡ â†’ ***PubMedAbstractRetriever***ã€***LocalJSONStore***ã€***ChromaDbRag*** å’Œ ***ChatAgent***ã€‚æˆ‘ä»¬å°†åœ¨åº”ç”¨ç¨‹åºä»£ç ä¸­ä½¿ç”¨è¿™äº›å¯¹è±¡ã€‚\n2. å®šä¹‰å¸ƒå±€ä»¥å‘ˆçŽ°åº”ç”¨ç¨‹åºæ ‡é¢˜ã€logo å’Œåº”ç”¨ç¨‹åºä¿¡æ¯ã€‚\n3. å®šä¹‰ç”¨æˆ·é—®é¢˜çš„è¾“å…¥å’Œä¸€ä¸ªæäº¤æŒ‰é’®ã€‚å½“æŒ‰é’®è¢«ç‚¹å‡»æ—¶ï¼Œè¿™å°†è§¦å‘æœç´¢å’ŒèŽ·å– PubMed æ–‡ç« çš„é€»è¾‘ï¼ˆä½¿ç”¨ ***PubMedAbstractRetriever â€”*** pubmed\\_clientï¼‰ï¼Œå°†å®ƒä»¬ä¿å­˜åˆ°æœ¬åœ°æ•°æ®å­˜å‚¨åº“ï¼ˆä½¿ç”¨ ***LocalJSONStore â€”*** data\\_repositoryï¼‰ï¼Œå¹¶ä¸ºå®ƒä»¬åˆ›å»ºå‘é‡ç´¢å¼•ï¼ˆä½¿ç”¨ ***ChromaDbRag â€”*** rag\\_clientï¼‰ã€‚\n4. ç›´æŽ¥å›žç­”ç”¨æˆ·é—®é¢˜å¹¶åœ¨ UI ä¸Šæ˜¾ç¤ºã€‚\n5. æ˜¾ç¤ºèŠå¤©æœºå™¨äººéƒ¨åˆ†ï¼Œè®©æ‚¨é€‰æ‹©ä¸€ä¸ªè¿‡åŽ»çš„æŸ¥è¯¢è¿›è¡Œå¯¹è¯ï¼Œä»¥ä¾¿è¿›ä¸€æ­¥è¯¢é—®æ‘˜è¦ã€‚åœ¨é€‰æ‹©è¿‡åŽ»çš„æŸ¥è¯¢åŽï¼ŒåŠ è½½ç›¸åº”çš„å‘é‡ç´¢å¼•ï¼Œå¹¶å¯åŠ¨èŠå¤©ä¼šè¯ï¼ˆ***chat\\_agent.start\\_conversation(â€¦)***ï¼‰ã€‚çŽ°åœ¨æ‚¨å¯ä»¥ä¸Žæ‘˜è¦èŠå¤©ï¼\n\n## é™åˆ¶\n\næˆ‘å¾ˆé«˜å…´ä½ å’Œæˆ‘ä¸€èµ·èµ°è¿‡è¿™ä¸ªç³»åˆ—ï¼Œæˆ‘ä»¬æž„å»ºäº†ä¸€ä¸ªç§‘å­¦èŠå¤©æœºå™¨äººçš„åŽŸåž‹ï¼ä¸è¿‡éœ€è¦è¯´æ˜Žçš„æ˜¯ï¼Œè¿™ä¸ªåº”ç”¨ç¨‹åºä»…ä»…æ˜¯ä¸€ä¸ªæ¦‚å¿µéªŒè¯ï¼ˆPoCï¼‰ï¼Œæ‰€å±•ç¤ºçš„å®žçŽ°å­˜åœ¨ä¸€äº›éœ€è¦åœ¨ç”Ÿäº§çŽ¯å¢ƒä¸­éƒ¨ç½²ä¹‹å‰è§£å†³çš„é—®é¢˜ã€‚\n\n**ç®€å•RAGçš„é™åˆ¶å’Œè€ƒè™‘**\n\n* **æ£€ç´¢å†…å®¹çš„ç›¸å…³æ€§**ï¼šä½ æ— æ³•ç¡®å®šæ£€ç´¢åˆ°çš„å†…å®¹ï¼ˆä¸Žç”¨æˆ·é—®é¢˜æœ€ç›¸ä¼¼çš„å†…å®¹ï¼‰æ˜¯å¦æ˜¯æœ€ç›¸å…³çš„ä¿¡æ¯ã€‚æœ‰ä¸€äº›å…ˆè¿›çš„RAGæŠ€æœ¯ï¼Œå¦‚*å‡è®¾æ€§é—®é¢˜*æˆ–*å±‚æ¬¡ç´¢å¼•*ï¼Œå¯ä»¥å¸®åŠ©è§£å†³è¿™ä¸ªé—®é¢˜â€”â€”åœ¨[è¿™ç¯‡æ–‡ç« ](https://proxy.rifx.online/https://readmedium.com/advanced-rag-techniques-unlocking-the-next-level-040c205b95bc)ä¸­äº†è§£æ›´å¤šå…³äºŽè¿™äº›æŠ€æœ¯çš„ä¿¡æ¯ã€‚\n* **æ£€ç´¢å†…å®¹çš„æˆªæ–­**ï¼šå¾ˆéš¾è¯„ä¼°æ˜¯å¦æ£€ç´¢åˆ°äº†æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œç”±äºŽLLMçš„ä»¤ç‰Œé™åˆ¶ï¼Œé€‚åº”æ‰€æœ‰ä¸Šä¸‹æ–‡åˆ°æç¤ºä¸­å¯èƒ½ä¼šå¾ˆå…·æŒ‘æˆ˜æ€§ã€‚åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œé»˜è®¤çš„æˆªæ–­ç­‰äºŽ5ä¸ªæ‘˜è¦ï¼ˆåœ¨æˆ‘ä»¬çš„ChatAgentçš„retrieve_documentsæ–¹æ³•ä¸­ï¼‰ï¼Œå¦‚æžœç”¨æˆ·æå‡ºä¸€ä¸ªå¹¿æ³›çš„é—®é¢˜ï¼Œè¿™æ˜¾ç„¶å¯èƒ½ä¸å¤Ÿã€‚\n* **é€‚ç”¨æ€§æœ‰é™**ï¼šæœ‰æ—¶ï¼Œç”¨æˆ·çš„é—®é¢˜å¯èƒ½æ›´å€¾å‘äºŽæ€»ç»“æ€§è´¨ï¼Œè€Œä½¿ç”¨ä¸åŒäºŽRAGçš„æŠ€æœ¯å¯èƒ½æ›´é€‚åˆè¿™ä¸ªç›®çš„ã€‚ä¾‹å¦‚ï¼Œä½ å¯ä»¥æž„å»ºä¸€ä¸ªä»£ç†ï¼Œå†³å®šä»»åŠ¡æ˜¯æ€»ç»“/æ£€ç´¢ï¼ŒåŸºäºŽç”¨æˆ·é—®é¢˜ã€‚åœ¨æ­¤è¯„ä¼°ä¹‹åŽï¼Œå°†æœ‰ä¸€ä¸ªå‡½æ•°æ‰§è¡Œä¸åŒçš„é€»è¾‘ï¼Œåˆ†åˆ«è¿›è¡Œæ€»ç»“æˆ–æ£€ç´¢ã€‚\n\n**éƒ¨ç½²æž¶æž„è€ƒè™‘**\n\n* **è¿è¡ŒçŽ¯å¢ƒ**ï¼šåœ¨æœ¬ç³»åˆ—çš„èŒƒå›´å†…ï¼Œæˆ‘ä»¬ä»…åœ¨æœ¬åœ°æž„å»ºäº†æˆ‘ä»¬çš„èŠå¤©æœºå™¨äººï¼Œæ²¡æœ‰è€ƒè™‘å¦‚æžœæˆ‘ä»¬æƒ³è¦å°†è¿™ä¸ªåº”ç”¨ç¨‹åºéƒ¨ç½²ä»¥æœåŠ¡ä¸€äº›çœŸå®žç”¨æˆ·æ—¶éœ€è¦åšå‡ºçš„ä»»ä½•æž¶æž„å†³ç­–ã€‚\n* **åŒæ­¥å¤„ç†**ï¼šç”±äºŽæ•°æ®èŽ·å–å¯èƒ½éœ€è¦ç›¸å½“é•¿çš„æ—¶é—´ï¼Œå®žçŽ°åŸºäºŽé˜Ÿåˆ—çš„å¼‚æ­¥å¤„ç†ç”¨æˆ·è¯·æ±‚ä¼šæ›´é«˜æ•ˆï¼Œå¹¶åœ¨æ•°æ®èŽ·å–å®ŒæˆåŽé€šçŸ¥ç”¨æˆ·ã€‚ä»¥åŒæ­¥æ–¹å¼è¿›è¡Œæ­¤æ“ä½œå¯èƒ½ä¼šè€—è´¹å¤§é‡æ—¶é—´ï¼Œè¿™å¯èƒ½å¯¼è‡´è®¸å¤šæœåŠ¡å™¨è¶…æ—¶ã€‚\n* **åŽç«¯æŠ€æœ¯**ï¼šåœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œä½¿ç”¨çš„åŽç«¯æ˜¯ChromaDBï¼Œé‡‡ç”¨æœ¬åœ°å­˜å‚¨çš„JSONæ–‡ä»¶ã€‚å¯¹äºŽä¸€ä¸ªæœåŠ¡ç”¨æˆ·çš„éƒ¨ç½²åº”ç”¨ç¨‹åºï¼Œè¿™åº”è¯¥é‡æ–°è¯„ä¼°å¹¶é€‰æ‹©åˆé€‚çš„æŠ€æœ¯ã€‚è¿™å¯ä»¥é€šè¿‡åŸºäºŽåº”ç”¨ç¨‹åºåŽç«¯ä»£ç ä¸­çš„æŽ¥å£å®šä¹‰ï¼ˆ*RagWorkflow*å’Œ*UserQueryDataStore*æŽ¥å£ï¼‰è½»æ¾å®žçŽ°ã€‚\n\n**åŒ…æ‹¬æ›´å¤šç§‘å­¦æ•°æ®åº“**\n\n* åœ¨è¿™ä¸ªç³»åˆ—ä¸­ï¼Œæˆ‘ä»¬ä»…å…³æ³¨PubMedï¼Œä½†ä¸ºäº†æä¾›ä¸°å¯Œçš„ä¸Šä¸‹æ–‡åŸºç¡€ï¼Œå¯ä»¥æ·»åŠ å…¶ä»–ç§‘å­¦è®ºæ–‡æ•°æ®åº“ï¼ˆå³Scopusï¼‰ã€‚è¿™å¯ä»¥é€šè¿‡åŸºäºŽåº”ç”¨ç¨‹åºåŽç«¯ä»£ç ä¸­çš„æŽ¥å£å®šä¹‰ï¼ˆ*AbstractRetriever*æŽ¥å£ï¼‰è½»æ¾å®žçŽ°ã€‚\n\n## å®Œæ•´ä»£ç åº“ GitHub é“¾æŽ¥\n\néšæ„åˆ†å‰è¯¥ä»“åº“å¹¶å°†å…¶é€‚åº”æ‚¨çš„ UCï¼\n\n### é“¾æŽ¥åˆ° GitHub ä»“åº“ pubmed\\-rag\\-screener\n\n## æ‘˜è¦\n\n* åœ¨æœ¬ç³»åˆ—çš„æœ€åŽä¸€éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†ä¹‹å‰æž„å»ºçš„æ‰€æœ‰ç»„ä»¶ç»„åˆåœ¨ä¸€èµ·ï¼Œåˆ›å»ºä¸€ä¸ªç”¨æˆ·ç•Œé¢ï¼Œè®©ç§‘å­¦å®¶å¯ä»¥æå‡ºé—®é¢˜ï¼ŒåŸºäºŽç§‘å­¦æ‘˜è¦èŽ·å¾—ç­”æ¡ˆï¼Œç„¶åŽä¸Žæ‘˜è¦è¿›è¡Œè¿›ä¸€æ­¥çš„äº¤æµã€‚\n* åº”ç”¨é€»è¾‘æ˜¯æ¨¡å—åŒ–çš„ï¼Œä¾¿äºŽä½¿ç”¨æä¾›çš„æŽ¥å£è¿›è¡Œæ‰©å±•ã€‚\n* æ¦‚è¿°å¹¶å¼ºè°ƒäº†è¯¥æ–¹æ³•çš„å±€é™æ€§ï¼Œå¹¶åŒ…æ‹¬äº†ä¸€äº›æž„å»ºç”Ÿäº§çº§åº”ç”¨çš„å»ºè®®ã€‚\n\n> éžå¸¸æ„Ÿè°¢æ‚¨ä¸Žæˆ‘ä¸€èµ·å®Œæˆè¿™ä¸ªç³»åˆ—ï¼å¸Œæœ›æ‚¨å–œæ¬¢æž„å»ºè¿™ä¸ªä»¤äººå…´å¥‹çš„ç”¨ä¾‹ :)\n\n> å¦‚æžœæ‚¨æƒ³è®¨è®ºæœ‰å…³å¼€å‘ã€æ•°æ®ã€äººå·¥æ™ºèƒ½çš„ä»»ä½•å†…å®¹ï¼Œæˆ–è€…åªæ˜¯æƒ³è”ç³»ï¼Œè¯·éšæ—¶ä¸Žæˆ‘è”ç³» â€” [åœ¨LinkedInä¸Šè”ç³»æˆ‘](https://proxy.rifx.online/https://www.linkedin.com/in/sbarankova/)\n\n## ç³»åˆ—å†…å®¹\n\n* [ç¬¬ä¸€éƒ¨åˆ† â€” è§£é‡Šç”¨ä¾‹ï¼Œè®¾ç½®å¸¦æœ‰èŠå¤©æœºå™¨äººç•Œé¢çš„ Streamlit åº”ç”¨çš„ç¬¬ä¸€æ­¥ã€‚](https://proxy.rifx.online/https://readmedium.com/build-a-rag-based-scientific-chatbot-with-langchain-streamlit-pubmed-part-1-set-up-streamlit-37550b44b266)\n* [ç¬¬äºŒéƒ¨åˆ† â€” é€šè¿‡ PubMed API ä½¿ç”¨è‡ªç„¶è¯­è¨€è¾…åŠ©æ£€ç´¢ç›¸å…³ç§‘å­¦æ‘˜è¦](https://proxy.rifx.online/https://readmedium.com/llm-aided-retrieval-of-relevant-scientific-abstracts-via-pubmed-api-using-natural-language-part2-9e10f78575e6)\n* [ç¬¬ä¸‰éƒ¨åˆ† â€” è®¾ç½®åŽç«¯ â€” ä»Žæ£€ç´¢åˆ°çš„ç§‘å­¦æ‘˜è¦åˆ›å»ºå‘é‡åµŒå…¥å¹¶å°†å…¶å­˜å‚¨åœ¨å‘é‡åº“ä¸­](https://proxy.rifx.online/https://readmedium.com/build-a-rag-based-scientific-chatbot-with-langchain-streamlit-pubmed-part-3-create-vector-1e5e401e72e6)\n* **ç¬¬å››éƒ¨åˆ†ï¼ˆæœ¬æ–‡ï¼‰ â€” é€šè¿‡ RAG å°†æ‰€æœ‰å†…å®¹æ•´åˆåœ¨ä¸€èµ· â€” ä¸Žç§‘å­¦æ‘˜è¦èŠå¤©**\n\n"},{"lang":"zh","group":"blog","slug":"blog/build-your-talking-voice-ai-assistant-locally-memory-retaining-chatbot-with-streamlit-ui-09da4f10687c","frontmatter":{"title":"åœ¨æœ¬åœ°æž„å»ºä¼šè¯´è¯çš„è¯­éŸ³äººå·¥æ™ºèƒ½åŠ©ç†ï¼šå…·æœ‰æµå…‰æº¢å½©ç”¨æˆ·ç•Œé¢çš„è®°å¿†ä¿æŒèŠå¤©æœºå™¨äºº...","meta_title":"åœ¨æœ¬åœ°æž„å»ºä¼šè¯´è¯çš„è¯­éŸ³äººå·¥æ™ºèƒ½åŠ©ç†ï¼šå…·æœ‰æµå…‰æº¢å½©ç”¨æˆ·ç•Œé¢çš„è®°å¿†ä¿æŒèŠå¤©æœºå™¨äºº...","description":"æœ¬æ–‡æä¾›äº†ä¸€ä¸ªè¯¦ç»†çš„æŒ‡å—ï¼Œä»‹ç»å¦‚ä½•ä½¿ç”¨Streamlitã€LangChainå’ŒOllama Llamaæ¨¡åž‹æž„å»ºä¸€ä¸ªå…·æœ‰è®°å¿†ä¿ç•™åŠŸèƒ½çš„ä¸ªäººè¯­éŸ³AIåŠ©æ‰‹Porterã€‚Porterèƒ½å¤Ÿåœ¨æœ¬åœ°è¿è¡Œï¼Œç¡®ä¿ç”¨æˆ·æ•°æ®å®‰å…¨å¹¶æä¾›å¿«é€Ÿå“åº”ã€‚å…¶ä¸»è¦åŠŸèƒ½åŒ…æ‹¬è¯­éŸ³è¾“å…¥è¾“å‡ºã€ä¼šè¯è®°å¿†ã€èŠå¤©è®°å½•å’Œå¯å®šåˆ¶çš„æ¨¡åž‹å‚æ•°è®¾ç½®ã€‚é€šè¿‡æ•´åˆå…ˆè¿›çš„è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ï¼ŒPorteræ—¨åœ¨ä¸ºç”¨æˆ·æä¾›ä¸ªæ€§åŒ–çš„åŠ©æ‰‹ä½“éªŒï¼Œé€‚ç”¨äºŽå„ç§åº”ç”¨åœºæ™¯ã€‚è¯¥é¡¹ç›®å¼ºè°ƒäº†éšç§ä¿æŠ¤å’Œé«˜æ•ˆäº¤äº’çš„é‡è¦æ€§ï¼Œå±•ç¤ºäº†çŽ°ä»£AIåŠ©æ‰‹çš„æ½œåŠ›ã€‚","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*5WJoI0IAKwMpEaCdSY63_A.png","categories":["Voice Assistants","Natural Language Processing","Programming/Scripting"],"author":"Rifx.Online","tags":["Porter","Llama","Streamlit","Whisper","offline"],"draft":false,"slug":"blog/build-your-talking-voice-ai-assistant-locally-memory-retaining-chatbot-with-streamlit-ui-09da4f10687c"},"content":"\n\n\n### å¼€å‘æ‚¨è‡ªå·±çš„å…·æœ‰ä¸Šä¸‹æ–‡è®°å¿†å’Œå®žæ—¶èŠå¤©åŠŸèƒ½çš„è¯­éŸ³ AI çš„é€æ­¥æŒ‡å—ï¼ŒåŸºäºŽ Llama3.1 å’Œ Llama3.2 æ¨¡åž‹\n\nðŸ‘¨ðŸ¾â€ðŸ’» [GitHub](https://github.com/mdmonsurali) â­ï¸ \\| ðŸ‘”[LinkedIn](https://www.linkedin.com/in/mdmonsurali/) \\|ðŸ“ [Medium](https://medium.com/@monsuralirana)\n\n\n\nåŸºäºŽè¯­éŸ³çš„ä¸ªäººåŠ©æ‰‹çš„æ¦‚å¿µå·²ç»è¶…è¶Šäº†æ–°å¥‡çš„èŒƒç•´â€”â€”å®ƒå·²æˆä¸ºå¿™ç¢Œçš„ä¸“ä¸šäººå£«ã€è¿œç¨‹å›¢é˜Ÿå’Œç§‘æŠ€çˆ±å¥½è€…çš„å®žç”¨æ— éšœç¢è§£å†³æ–¹æ¡ˆã€‚æƒ³è±¡ä¸€ä¸‹ï¼Œä¸€ä¸ªå¯ä»¥å€¾å¬ã€å›žåº”ç”šè‡³è·Ÿè¸ªè¿‡åŽ»å¯¹è¯çš„è¯­éŸ³ AIï¼Œæ‰€æœ‰è¿™äº›éƒ½åœ¨æ‚¨çš„è®¾å¤‡ä¸Šæœ¬åœ°è¿è¡Œã€‚ä»‹ç» *Porter*ï¼Œä¸€ä¸ªæ—¨åœ¨å®žçŽ°è¿™ä¸€ç›®æ ‡çš„ä¸ªäºº AI åŠ©æ‰‹ã€‚\n\nåœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†æŒ‡å¯¼æ‚¨åˆ›å»º *Porter*ï¼Œä¸€ä¸ªå…ˆè¿›çš„è¯­éŸ³åŠ©æ‰‹ï¼Œèƒ½å¤Ÿå“åº”è¯­éŸ³æŸ¥è¯¢ï¼Œé€šè¿‡å¯¹è¯è®°å¿†ä¿æŒä¸Šä¸‹æ–‡ï¼Œå¹¶é€šè¿‡åˆæˆè¯­éŸ³æä¾›å“åº”ã€‚*Porter* åˆ©ç”¨ Ollama çš„å°–ç«¯ Llama æ¨¡åž‹ã€**Streamlit** æä¾›ç›´è§‚çš„ç”¨æˆ·ç•Œé¢ï¼Œä»¥åŠ OpenAI çš„ **Whisper** æ¨¡åž‹è¿›è¡Œè½¬å½•ã€‚è¯¥æŒ‡å—å°†å¸¦æ‚¨é€æ­¥å®Œæˆä»Žå®‰è£…åˆ°æœ€ç»ˆåœ¨æœ¬åœ°æœºå™¨ä¸Šéƒ¨ç½²çš„è¿‡ç¨‹ã€‚\n\n## ç›®å½•\n\n1. ä»‹ç»\n2. ä¸ºä»€ä¹ˆé€‰æ‹© *Porter*?\n3. *Porter* çš„å…³é”®ç‰¹æ€§\n4. ç”¨æˆ·ç•Œé¢ (UI) æ¦‚è¿°\n5. åˆ†æ­¥æ•™ç¨‹\n6. æœ¬åœ°è¿è¡Œ Porter\n7. ç»“è®º\n\n## 1\\. å¼•è¨€\n\néšç€è‡ªç„¶è¯­è¨€å¤„ç†çš„æœ€æ–°è¿›å±•ï¼Œè¯­éŸ³åŠ©æ‰‹åœ¨ç†è§£å¤æ‚æŸ¥è¯¢ã€ä»¥è‡ªç„¶è¯­è¨€å“åº”ä»¥åŠåœ¨å¯¹è¯ä¸­ä¿æŒä¸Šä¸‹æ–‡æ–¹é¢å˜å¾—è¶Šæ¥è¶Šå¼ºå¤§ã€‚*Porter*ï¼Œæˆ‘ä»¬çš„äººå·¥æ™ºèƒ½è¯­éŸ³åŠ©æ‰‹ï¼Œæ—¨åœ¨åˆ©ç”¨è¿™äº›è¿›å±•ï¼Œä¸ºç”¨æˆ·æä¾›è‡ªç„¶ã€å“åº”è¿…é€Ÿä¸”ä¸ªæ€§åŒ–çš„åŠ©æ‰‹ä½“éªŒã€‚Porter åŸºäºŽ Ollama çš„å…ˆè¿›æ¨¡åž‹æž„å»ºï¼Œæä¾›å¯¹è¯å¼äººå·¥æ™ºèƒ½ï¼Œå¹¶ä½¿ç”¨ **Streamlit** æä¾›ç®€å•æ˜“ç”¨çš„äº¤äº’å¼ç”¨æˆ·ç•Œé¢ã€‚\n\n**Porter** æä¾›ï¼š\n\n* èƒ½å¤Ÿè®°ä½è¿‡åŽ»äº¤æµçš„å¯¹è¯å¼äººå·¥æ™ºèƒ½ã€‚\n* æ˜“äºŽå¯¼èˆªçš„æµç•…ç•Œé¢ã€‚\n* å¯å®šåˆ¶çš„å‚æ•°ä»¥å®žçŽ°ä¸ªæ€§åŒ–å“åº”ã€‚\n\n## 2\\. ä¸ºä»€ä¹ˆé€‰æ‹© Porterï¼Ÿ\n\nå¤§å¤šæ•°è¯­éŸ³åŠ©æ‰‹éœ€è¦äº’è”ç½‘è¿žæŽ¥å¹¶ä¾èµ–å¤–éƒ¨æœåŠ¡å™¨ï¼Œè¿™å¼•å‘äº†å…³äºŽå®‰å…¨æ€§ã€æŽ§åˆ¶æƒå’Œå“åº”å»¶è¿Ÿçš„æ‹…å¿§ã€‚*Porter* é€šè¿‡æœ¬åœ°è¿è¡Œï¼Œæä¾›äº†ï¼š\n\n* **éšç§**ï¼šæ— éœ€äº’è”ç½‘è®¿é—®ï¼Œæ‰€æœ‰å¯¹è¯å’Œæ•°æ®éƒ½å®‰å…¨åœ°ä¿ç•™åœ¨æ‚¨çš„è®¾å¤‡ä¸Šã€‚\n* **å¿«é€Ÿå“åº”æ—¶é—´**ï¼šæ‰€æœ‰æ“ä½œéƒ½åœ¨æœ¬åœ°è¿›è¡Œï¼Œå¤„ç†å’Œå“åº”çš„å»¶è¿Ÿæœ€å°ã€‚\n* **è®°å¿†ä¿ç•™çš„å¯¹è¯**ï¼šä½¿ç”¨ LangChainï¼Œ*Porter* å¯ä»¥åœ¨å¤šæ¬¡äº¤äº’ä¸­è®°ä½ä¸Šä¸‹æ–‡ï¼Œä½¿å…¶èƒ½å¤Ÿå‡†ç¡®å›žç­”åŽç»­é—®é¢˜ã€‚\n\n## 3\\. æ³¢ç‰¹çš„å…³é”®ç‰¹æ€§\n\n### è¯­éŸ³è¾“å…¥å’Œè¾“å‡º\n\n*Porter* ä½¿ç”¨ Whisperï¼Œä¸€ä¸ªå¼ºå¤§çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ« (ASR) æ¨¡åž‹ï¼Œå°†è¯­éŸ³è¾“å…¥è½¬å½•ä¸ºæ–‡æœ¬ã€‚å®ƒè¿˜å¯ä»¥ç”Ÿæˆè¯­éŸ³å“åº”ï¼Œæä¾›æ— ç¼çš„å…æä½“éªŒã€‚\n\n### ä¼šè¯è®°å¿†ä¸Žå¯¹è¯ä¸Šä¸‹æ–‡\n\né€šè¿‡ LangChain çš„ **ConversationBufferMemory**ï¼Œ*Porter* èƒ½å¤Ÿä¿ç•™è¿‡åŽ»çš„å¯¹è¯ï¼Œä»Žè€Œå®žçŽ°è‡ªç„¶çš„å¤šè½®å¯¹è¯ã€‚è¯¥è®°å¿†åŠŸèƒ½ä½¿ *Porter* èƒ½å¤Ÿå¼•ç”¨è¿‡åŽ»çš„ç”¨æˆ·æŸ¥è¯¢å¹¶æä¾›è¿žè´¯æ€§ã€‚\n\n### åŽ†å²æ¦‚è¿°å’ŒèŠå¤©è®°å½•\n\n*Porter* åŒ…å«ä¸€ä¸ª **èŠå¤©è®°å½•** åŠŸèƒ½ï¼Œæä¾›å½“å‰ä¼šè¯ä¸­æ‰€æœ‰è¿‡åŽ»äº’åŠ¨çš„æ¦‚è¿°ã€‚æ­¤èŠå¤©è®°å½•æ˜¾ç¤ºåœ¨ç”¨æˆ·ç•Œé¢ä¸Šï¼Œå¸®åŠ©ç”¨æˆ·è·Ÿè¸ªå·²è®¨è®ºçš„å†…å®¹ã€‚\n\n### å¯å®šåˆ¶çš„æ¨¡åž‹å‚æ•°\n\nåœ¨ *Porter* çš„ Streamlit ä¾§è¾¹æ ä¸­ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä¸åŒçš„æ¨¡åž‹ç‰ˆæœ¬ (Llama3\\.1, Llama3\\.2) å¹¶è°ƒæ•´å‚æ•°ï¼Œå¦‚ **temperature** å’Œ **max tokens** ä»¥æŽ§åˆ¶å“åº”çš„åˆ›é€ æ€§å’Œé•¿åº¦ã€‚\n\n### åŸºäºŽ Streamlit çš„ç”¨æˆ·ç•Œé¢\n\nStreamlit ä¸º *Porter* æä¾›äº†ä¸€ä¸ªç®€æ´ã€ç›´è§‚çš„ç”¨æˆ·ç•Œé¢ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿè½»æ¾ä¸ŽåŠ©æ‰‹è¿›è¡Œäº’åŠ¨ã€‚è¯¥åº”ç”¨æ˜¾ç¤ºäº†ä¹‹å‰çš„äº¤æµã€æ¨¡åž‹è®¾ç½®ï¼Œå¹¶å…è®¸è½»æ¾è¿›è¡Œè¯­éŸ³è¾“å…¥ã€‚\n\n## 4\\. ç”¨æˆ·ç•Œé¢ (UI) æ¦‚è¿°\n\n*Porter* çš„ Streamlit ç”¨æˆ·ç•Œé¢ç®€å•ä¸”ç”¨æˆ·å‹å¥½ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*x_oxCvi14LfcsG8H4VeXUg.png)\n\n* **è¯­éŸ³è¾“å…¥å°éƒ¨ä»¶**ï¼šä¸€ä¸ªéº¦å…‹é£Žå›¾æ ‡è®©ç”¨æˆ·å½•åˆ¶ä»–ä»¬çš„æŸ¥è¯¢ã€‚\n* **èŠå¤©æ˜¾ç¤º**ï¼šæ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯å’Œ *Porter* çš„å›žå¤ï¼ŒåŒ…æ‹¬æ—¶é—´æˆ³å’Œå“åº”æ—¶é—´ã€‚\n* **è®¾ç½®ä¾§è¾¹æ **ï¼šé€šè¿‡æ¨¡åž‹é€‰é¡¹ã€æ¸©åº¦å’Œæœ€å¤§ token è‡ªå®šä¹‰ *Porter*ã€‚\n* **åŽ†å²æ¦‚è§ˆ**ï¼šåœ¨èŠå¤©çª—å£ä¸­æŸ¥çœ‹å¯¹è¯åŽ†å²ï¼Œä¾¿äºŽè·Ÿè¸ªä¹‹å‰çš„äº¤æµã€‚\n\n## 5\\. åˆ†æ­¥æ•™ç¨‹\n\nè®©æˆ‘ä»¬åˆ†è§£ä»£ç ï¼Œçœ‹çœ‹å¦‚ä½•å®žçŽ°Porterã€‚æˆ‘ä»¬å°†ä½¿ç”¨ä¸¤ä¸ªä¸»è¦æ–‡ä»¶ï¼š`app.py`ï¼ˆç”¨äºŽStreamlitåº”ç”¨ï¼‰å’Œ`voicebot.py`ï¼ˆç”¨äºŽåŽç«¯é€»è¾‘ï¼‰ã€‚\n\n### å‰ææ¡ä»¶ï¼š\n\n* Python 3\\.7\\+\n* æœ¬åœ° conda çŽ¯å¢ƒ\n* Streamlit ç”¨äºŽç”¨æˆ·ç•Œé¢\n* Ollama ç”¨äºŽæ¨¡åž‹æŽ¨ç†\n* LangChain ç”¨äºŽç®¡ç†æ¨¡åž‹ä¸Žè®°å¿†ä¹‹é—´çš„äº¤äº’ã€‚\n\n### ç¬¬ä¸€æ­¥ï¼šå®‰è£…å¿…è¦çš„åŒ…\n\nå®‰è£…å¿…è¦çš„åº“å’Œå·¥å…·ï¼š\n\n```python\n!pip install langchain==0.0.318\n!pip install langchain-ollama \n!pip install langchain-community==0.0.3 \n!pip install ollama==0.0.8\n!pip install streamlit==1.25.0\n!pip install pathlib==1.0.1\n!pip install audio-recorder-streamlit==0.0.10\n!pip install torch==2.4.1\n!pip install transformer==4.44.2\n```\n\n> **æˆ‘å·²ç»é€šè¿‡ Ollama è®¾ç½®äº† LLaMA 3\\.1 å’Œ 3\\.2 æ¨¡åž‹ã€‚å¦‚æžœä½ åœ¨æœ¬åœ°æœºå™¨ä¸Šæ²¡æœ‰ Ollama æˆ– LLaMA æ¨¡åž‹ï¼Œè¯·æŒ‰ç…§ä¸‹é¢é“¾æŽ¥ä¸­çš„è¯´æ˜Žè¿›è¡Œå®‰è£…ã€‚é“¾æŽ¥ä»…é€‚ç”¨äºŽ Llama 3\\.2ï¼Œä½†ä½ å¯ä»¥é€šè¿‡è¿è¡Œ `\"ollama pull llama3.1\"` æ¥èŽ·å– Llama 3\\.1ã€‚**\n\n> **æˆ‘ä½¿ç”¨äº† Piper TTS æ¨¡åž‹è¿›è¡Œæ–‡æœ¬åˆ°è¯­éŸ³è½¬æ¢ã€‚å®ƒè½»é‡çº§ï¼Œé€Ÿåº¦å¿«10å€ï¼Œå®žæ—¶å·¥ä½œï¼Œç¦»çº¿æ“ä½œï¼Œå¹¶ä¸”äº§ç”Ÿç±»ä¼¼äººç±»çš„å£°éŸ³ã€‚**\n\n### ç¬¬2æ­¥ï¼šè®¾ç½®Streamlitåº”ç”¨ç¨‹åº\n\n\n```python\nimport streamlit as st\nimport time\nfrom audio_recorder_streamlit import audio_recorder\nfrom voicebot import initialize_chat, text_to_speech, transcribe_audio\n\nst.title(\"Porter - Your Personal Voice AI Assistant\")\n\n## Initialize session state variables\nif \"messages\" not in st.session_state:\n    st.session_state.messages = []\nif \"audio_bytes\" not in st.session_state:\n    st.session_state.audio_bytes = None\n\n## Sidebar Settings\nwith st.sidebar:\n    logo_path = \"/path/to/logo.png\"\n    st.image(logo_path, caption=\"AI Enterprise\", use_column_width=True)\n    st.subheader(\"æŽ¨ç†è®¾ç½®\")\n    st.session_state.model = st.selectbox(\"æ¨¡åž‹\", [\"llama3.1\", \"llama3.2:latest\"], index=0)\n    st.session_state.temperature = st.slider(\"æ¸©åº¦\", 0.0, 1.0, 0.0, 0.05)\n    st.session_state.max_tokens = st.slider(\"æœ€å¤§ä»¤ç‰Œæ•°\", 100, 5000, 500, 100)\n\n## Initialize chat model\nif \"chain\" not in st.session_state:\n    st.session_state.chain = initialize_chat()\n```\nåœ¨æœ¬èŠ‚ä¸­ï¼š\n\n1. **ä¼šè¯çŠ¶æ€å˜é‡**ï¼šå­˜å‚¨æ¶ˆæ¯åŽ†å²å’ŒéŸ³é¢‘å­—èŠ‚ã€‚\n2. **ä¾§è¾¹æ æŽ§ä»¶**ï¼šæä¾›ç”¨æˆ·ç•Œé¢æŽ§ä»¶ä»¥è‡ªå®šä¹‰æ¨¡åž‹ã€æ¸©åº¦å’Œä»¤ç‰Œé•¿åº¦ã€‚\n3. **èŠå¤©æ¨¡åž‹åˆå§‹åŒ–**ï¼šåŠ è½½èŠå¤©æ¨¡åž‹ä»¥ä¾›åº”ç”¨ç¨‹åºä½¿ç”¨ã€‚\n\n### ç¬¬3æ­¥ï¼šå®žçŽ°èŠå¤©åŠŸèƒ½\n\n\n```python\n## Display chat history\nfor message in st.session_state.messages:\n    with st.chat_message(message[\"role\"]):\n        st.markdown(message[\"content\"])\n\n## Record voice input\nfooter_container = st.container()\nwith footer_container:\n    st.session_state.audio_bytes = audio_recorder(text=\"Record a question\", icon_size=\"lg\")\n\nif st.session_state.audio_bytes:\n    transcript = transcribe_audio(st.session_state.audio_bytes)\n    if transcript:\n        st.session_state.messages.append({\"role\": \"user\", \"content\": transcript})\n        \n        # Display user input in chat\n        with st.chat_message(\"user\"):\n            st.markdown(transcript)\n\n        # Get response from model\n        with st.chat_message(\"assistant\"):\n            start_time = time.time()\n            with st.spinner(\"Porter is thinking...\"):\n                response = st.session_state.chain.run(transcript)\n            end_time = time.time()\n\n            response_time_str = f\"Response time: {end_time - start_time:.2f} seconds\"\n            st.markdown(response)\n            text_to_speech(response)\n            st.markdown(f\"_{response_time_str}_\")\n\n        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response, \"response_time\": response_time_str})\n```\nè¿™é‡Œï¼š\n\n1. **æ˜¾ç¤ºä¹‹å‰çš„æ¶ˆæ¯**ï¼šèŠå¤©çª—å£æ˜¾ç¤ºå¯¹è¯åŽ†å²ã€‚\n2. **è¯­éŸ³è¾“å…¥ä¸Žè½¬å½•**ï¼šå½•åˆ¶å¹¶è½¬å½•éŸ³é¢‘è¾“å…¥ä¸ºæ–‡æœ¬ï¼Œæ·»åŠ åˆ°èŠå¤©ä¸­ã€‚\n3. **åŠ©æ‰‹å›žå¤**ï¼šå°†ç”¨æˆ·è¾“å…¥å‘é€åˆ°æ¨¡åž‹ï¼Œæ£€ç´¢å›žå¤ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºéŸ³é¢‘ä»¥ä¾›æ’­æ”¾ã€‚\n\n### ç¬¬4æ­¥ï¼šå®žçŽ°åŽç«¯ (voicebot.py)\n\nåœ¨ `voicebot.py` ä¸­ï¼Œä¸»è¦ç»„ä»¶ç”¨äºŽåˆå§‹åŒ–Porterçš„å¯¹è¯æ¨¡åž‹ï¼Œå¹¶å¤„ç†æ–‡æœ¬åˆ°è¯­éŸ³å’Œè½¬å½•ï¼š\n\n```python\nimport os\nimport subprocess\nfrom langchain.memory.buffer import ConversationBufferMemory\nfrom langchain.memory.chat_message_histories.file import FileChatMessageHistory\nfrom langchain_community.chat_models.ollama import ChatOllama\nfrom langchain.chains.llm import LLMChain\nfrom transformers import pipeline\nimport torch\n\ndef initialize_chat():\n    def get_llm():\n        return ChatOllama(\n            model=st.session_state.model,\n            temperature=st.session_state.temperature,\n            max_tokens=st.session_state.max_tokens,\n        )\n\n    from langchain.prompts import (\n        HumanMessagePromptTemplate,\n        ChatPromptTemplate,\n        MessagesPlaceholder,\n        SystemMessagePromptTemplate,\n    )\n\n    def get_chat_prompt_template():\n        return ChatPromptTemplate(\n            input_variables=[\"content\", \"messages\"],\n            messages=[\n                SystemMessagePromptTemplate.from_template(\n                    \"You're a Personal Assistant, and your name is Porter.\"\n                ),\n                MessagesPlaceholder(variable_name=\"messages\"),\n                HumanMessagePromptTemplate.from_template(\"{content}\"),\n            ],\n        )\n\n    def get_memory():\n        return ConversationBufferMemory(\n            memory_key=\"messages\",\n            chat_memory=FileChatMessageHistory(file_path=\"memory.json\"),\n            return_messages=True,\n            input_key=\"content\",\n        )\n\n    llm = get_llm()\n    prompt = get_chat_prompt_template()\n    return LLMChain(llm=llm, prompt=prompt, memory=get_memory())\n\n## æ–‡æœ¬è½¬è¯­éŸ³\ndef text_to_speech(text):\n    subprocess.call(f'echo \"{text}\" | piper --model en_US-amy-medium --output_file output.wav', shell=True)\n    os.system(\"aplay output.wav\")\n\n## è¯­éŸ³è¯†åˆ«\npipe = pipeline(\"automatic-speech-recognition\", \"openai/whisper-large-v3-turbo\", torch_dtype=torch.float16, device=\"cuda:0\")\n\ndef transcribe_audio(audio_bytes):\n    webm_file_path = \"temp_audio.mp3\"\n    with open(webm_file_path, \"wb\") as f:\n        f.write(audio_bytes)\n    \n    transcript = pipe(webm_file_path)['text'].strip()\n    os.remove(webm_file_path)\n    return transcript\n```\næœ¬èŠ‚ï¼š\n\n1. **æ¨¡åž‹è®¾ç½®**ï¼šé…ç½®èŠå¤©æ¨¡åž‹å’Œæç¤ºæ¨¡æ¿ã€‚\n2. **æ–‡æœ¬è½¬è¯­éŸ³**ï¼šå°†æ¨¡åž‹å“åº”è½¬æ¢ä¸ºéŸ³é¢‘ã€‚\n3. **è¯­éŸ³è½¬æ–‡æœ¬**ï¼šä½¿ç”¨Whisperè½¬å½•å½•åˆ¶çš„éŸ³é¢‘è¾“å…¥ã€‚\n\n### ç¬¬5æ­¥ï¼šéƒ¨ç½²Porter\n\nå®Œæˆè®¾ç½®åŽï¼Œæ‚¨å¯ä»¥ä½¿ç”¨Streamlitå¯åŠ¨æ‚¨çš„åº”ç”¨ç¨‹åºã€‚è¦è¿è¡Œåº”ç”¨ç¨‹åºï¼Œè¯·å¯¼èˆªåˆ°æ‚¨çš„é¡¹ç›®æ–‡ä»¶å¤¹ï¼Œå¹¶åœ¨ç»ˆç«¯ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š\n\n```python\nstreamlit run apps.py\n```\nåº”ç”¨ç¨‹åºå¯åŠ¨åŽï¼Œæ‚¨å°†åœ¨ç»ˆç«¯ä¸­çœ‹åˆ°ä»¥ä¸‹æ¶ˆæ¯ï¼š\n\n```python\n  You can now view your Streamlit app in your browser.\n\n  Local URL: http://localhost:8501\n  Network URL: http://172.30.254.103:8501\n```\nå¦‚æžœæ‚¨åœ¨åŒä¸€å°æœºå™¨ä¸Šï¼Œå¯ä»¥é€šè¿‡åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€**Local URL**ï¼ˆ`http://localhost:8501`ï¼‰æ¥è®¿é—®Porterã€‚æˆ–è€…ï¼Œå¦‚æžœæ‚¨æƒ³ä»ŽåŒä¸€ç½‘ç»œä¸Šçš„å…¶ä»–è®¾å¤‡è®¿é—®å®ƒï¼Œè¯·ä½¿ç”¨**Network URL**ï¼ˆ`http://172.30.254.103:8501`ï¼‰ã€‚\n\nçŽ°åœ¨ï¼Œæ‚¨å°†æ‹¥æœ‰ä¸€ä¸ªåŠŸèƒ½é½å…¨çš„ä¸ªäººAIåŠ©æ‰‹ï¼\n\n> **â€œä¸ŽPorterå¯¹è¯ï¼šå®ƒå¦‚ä½•è®°ä½å’Œå›žå¿†è¿‡åŽ»çš„äº’åŠ¨â€**\n\nPorterä¸ä»…ä»…æ˜¯ä¸€ä¸ªåœ¨çž¬é—´å›žç­”é—®é¢˜çš„AIâ€”â€”å®ƒè¢«è®¾è®¡ç”¨æ¥è®°ä½è¿‡åŽ»çš„å¯¹è¯ã€‚å¾—ç›ŠäºŽå…¶è®°å¿†ç³»ç»Ÿï¼Œå®ƒå¯ä»¥å›žå¿†èµ·ä»¥å‰çš„èŠå¤©ï¼Œæä¾›ä¸Šä¸‹æ–‡ç›¸å…³çš„å“åº”ï¼Œä½¿äº’åŠ¨æ„Ÿè§‰æ›´åŠ ä¸ªæ€§åŒ–å’Œæµç•…ã€‚æ— è®ºæ‚¨æ˜¯åœ¨é‡æ¸©æ—§è¯é¢˜è¿˜æ˜¯å‘å‡ºPorterä¹‹å‰å¤„ç†è¿‡çš„å‘½ä»¤ï¼Œå®ƒéƒ½èƒ½æ™ºèƒ½åœ°å›žå¿†èµ·è¿‡åŽ»çš„äº¤æµï¼Œä»Žè€Œå®žçŽ°æ— ç¼ã€è¿žè´¯çš„å¯¹è¯ï¼Œæ„Ÿè§‰å°±åƒæ˜¯ä¸€åœºæŒç»­çš„å¯¹è¯ï¼Œè€Œä¸æ˜¯æ¯æ¬¡éƒ½é‡æ–°å¼€å§‹ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*dKn8HbZ7YhHHzml-OtBY4w.png)\n\n> ***GitHubä»£ç ï¼š***\n\n> **ç•™ä¸‹æ‚¨çš„åé¦ˆã€è¯„è®ºï¼Œå¹¶ä¸ºè¿™ä¸ªæ•…äº‹ðŸ‘ ðŸ‘ ç‚¹èµžï¼ï¼ðŸ‘ðŸ‘**\n\n## ç»“è®º\n\n***Porter*** çš„åˆ›å»ºå±•ç¤ºäº†ä¸ªäºº AI åŠ©æ‰‹çš„ä»¤äººå…´å¥‹çš„æ½œåŠ›ï¼Œè¿™äº›åŠ©æ‰‹é€šè¿‡æœ¬åœ°æ“ä½œä¼˜å…ˆè€ƒè™‘ **éšç§** å’Œ **å“åº”æ€§**ã€‚é€šè¿‡æ•´åˆ LangChain è¿›è¡Œå¯¹è¯è®°å¿†ã€Ollama çš„é«˜æ€§èƒ½ Llama æ¨¡åž‹ç”¨äºŽè‡ªç„¶è¯­è¨€å¤„ç†ä»¥åŠ Whisper è¿›è¡Œè¯­éŸ³è¯†åˆ«ï¼Œ*Porter* å±•ç¤ºäº†å¦‚ä½•å°†è¿™äº›å…ˆè¿›å·¥å…·ç»“åˆèµ·æ¥ï¼Œåˆ›å»ºä¸€ä¸ªå¼ºå¤§ä¸”ç›´è§‚çš„è¯­éŸ³åŠ©æ‰‹ã€‚è¯¥é¡¹ç›®ä¸ä»…å¼ºè°ƒäº†çŽ°ä»£ AI çš„å¯åŠæ€§ï¼Œè¿˜çªå‡ºäº†ä¿æŠ¤ç”¨æˆ·æ•°æ®å®‰å…¨å’Œå¿«é€Ÿäº¤äº’çš„é‡è¦æ€§â€”â€”è¿™æ˜¯æœ¬åœ°è§£å†³æ–¹æ¡ˆçš„ä¸¤ä¸ªä¼˜åŠ¿é¢†åŸŸã€‚\n\nå‡­å€Ÿ Porter çµæ´»çš„æž¶æž„ï¼Œæœ‰è¶³å¤Ÿçš„ç©ºé—´æ¥æ‰©å±•å…¶åŠŸèƒ½ã€‚å¼€å‘äººå‘˜å¯ä»¥é›†æˆå…¶ä»–æœ¬åœ° NLP æ¨¡åž‹æˆ–ä¸ºä¸åŒç”¨ä¾‹æ·»åŠ å®šåˆ¶å·¥ä½œæµç¨‹ï¼Œä¾‹å¦‚å®¢æˆ·æ”¯æŒã€æ•™è‚²è¾…å¯¼æˆ–æŠ€æœ¯æ•…éšœæŽ’é™¤ã€‚æ­¤å¤–ï¼Œéšç€æ–°è¯­è¨€æ¨¡åž‹å’Œè¯­éŸ³å¤„ç†æŠ€æœ¯çš„å‡ºçŽ°ï¼Œ*Porter* å¯ä»¥æ›´æ–°ä»¥æä¾›æ›´ç»†è‡´å’Œå…·æœ‰ä¸Šä¸‹æ–‡æ„è¯†çš„å“åº”ã€‚\n\n## å‚è€ƒæ–‡çŒ®\n\n\\[1] Llama 3\\.2: ä¸‹ä¸€ä»£è½»é‡çº§æŒ‡ä»¤è°ƒä¼˜è¯­è¨€æ¨¡åž‹ï¼šå®žè·µæ•™ç¨‹ï¼Œ2024\\. å¯ç”¨é“¾æŽ¥ï¼š[https://readmedium.com/llama\\-3\\-2\\-the\\-next\\-generation\\-of\\-lightweight\\-instruction\\-tuned\\-language\\-models\\-a\\-hands\\-on\\-9bca07c8af1d](https://readmedium.com/llama-3-2-the-next-generation-of-lightweight-instruction-tuned-language-models-a-hands-on-9bca07c8af1d)\n\n\\[2] Hugging Face, *Transformers æ–‡æ¡£ï¼šä½¿ç”¨ LLaMA 3\\.2 è§†è§‰æ¨¡åž‹*, Hugging Face, 2024\\. å¯ç”¨é“¾æŽ¥ï¼š<https://huggingface.co/blog/llama32>\n\n\\[3] æž„å»ºä¸€ä¸ªåŸºæœ¬çš„ LLM èŠå¤©åº”ç”¨ç¨‹åºã€‚å¯ç”¨é“¾æŽ¥ï¼š[https://docs.streamlit.io/develop/tutorials/llms/build\\-conversational\\-apps](https://docs.streamlit.io/develop/tutorials/llms/build-conversational-apps)\n\nå¿«ä¹ç¼–ç ï¼ ðŸŽ‰\n\nðŸ‘¨ðŸ¾â€ðŸ’» [GitHub](https://github.com/mdmonsurali) â­ï¸ \\| ðŸ‘”[LinkedIn](https://www.linkedin.com/in/mdmonsurali/) \\|ðŸ“ [Medium](https://medium.com/@monsuralirana)\n\næ„Ÿè°¢æ‚¨èŠ±æ—¶é—´é˜…è¯»è¿™ç¯‡æ–‡ç« ï¼\n\nè¯·ç¡®ä¿ç•™ä¸‹æ‚¨çš„åé¦ˆå’Œè¯„è®ºã€‚ ðŸ‘ ä¸ºè¿™ä¸ªæ•…äº‹ç‚¹èµžå¹¶å…³æ³¨æ›´å¤šæ•…äº‹ã€‚ä¸‹æ¬¡åšå®¢è§ï¼Œæ•¬è¯·å…³æ³¨ ðŸ“¢\n\n## äº«å—è¿™ç¯‡æ–‡ç« å—ï¼ŸæŸ¥çœ‹æˆ‘æ›´å¤šçš„ä½œå“ï¼š\n\n* **ä½¿ç”¨Elasticsearchã€Ollamaã€LLaMA 3\\.1å’ŒLangChainæž„å»ºè‡ªå®šä¹‰æ–‡æ¡£ä»£ç†:** æŽ¢ç´¢å¦‚ä½•ä½¿ç”¨LLaMA 3\\.1å’ŒOllamaè®¾ç½®ä¸ªæ€§åŒ–æ–‡æ¡£æ£€ç´¢ä»£ç†ï¼Œä»¥å®žçŽ°æ— ç¼ä¿¡æ¯æ£€ç´¢ã€‚[åœ¨è¿™é‡Œé˜…è¯»å®Œæ•´æ•™ç¨‹](https://readmedium.com/building-a-custom-documents-agent-with-elasticsearch-ollama-llama-3-1-and-langchain-926b28047e1d)ã€‚\n* **ä½¿ç”¨Ollamaçš„LLaMA3\\.1ã€LLaMA3\\.2æ¨¡åž‹ã€Streamlit UIå’Œæœ¬åœ°çŽ¯å¢ƒæž„å»ºä¸ªäººAIåŠ©æ‰‹:** å‘çŽ°å¦‚ä½•å¼€å‘ä¸€ä¸ªèƒ½å¤Ÿè®°ä½è¿‡åŽ»äº’åŠ¨çš„AIåŠ©æ‰‹ï¼Œä½¿ç”¨æœ€æ–°çš„LLaMAæ¨¡åž‹å’Œç”¨æˆ·å‹å¥½çš„Streamlitç•Œé¢ã€‚[åœ¨è¿™é‡Œé˜…è¯»å®Œæ•´æ•™ç¨‹ã€‚](https://readmedium.com/building-porter-your-personal-ai-assistant-with-memory-using-ollamas-llama3-1-efb32b80c129)\n* **OpenAI Swarmï¼šä¸€ä¸ªè½»é‡çº§çš„å¤šä»£ç†ç¼–æŽ’æ¡†æž¶:** æ·±å…¥äº†è§£ä¸€ä¸ªæ—¨åœ¨é«˜æ•ˆç®¡ç†å¤šä¸ªAIä»£ç†çš„æ–°æ¡†æž¶ï¼Œæå‡æ‚¨çš„AIé¡¹ç›®ç®¡ç†èƒ½åŠ›ã€‚[åœ¨è¿™é‡Œé˜…è¯»å®Œæ•´æ•™ç¨‹ã€‚](https://readmedium.com/openai-swarm-a-lightweight-framework-for-multi-agent-orchestration-b4a83a1a1e37)\n* **å¦‚ä½•ä½¿ç”¨Molmo\\-7Bè¿›è¡Œå¤šæ¨¡æ€AIï¼šä½¿ç”¨å¼€æºè§†è§‰è¯­è¨€æ¨¡åž‹æå–æ–‡æœ¬å’Œå›¾åƒ:** å­¦ä¹ å¦‚ä½•åˆ©ç”¨Molmo\\-7Bæ¨¡åž‹æå–æ–‡æœ¬å’Œå›¾åƒï¼Œå½»åº•æ”¹å˜æ‚¨å¯¹å¤šæ¨¡æ€AIçš„å¤„ç†æ–¹å¼ã€‚[åœ¨è¿™é‡Œé˜…è¯»å®Œæ•´æ•™ç¨‹ã€‚](https://readmedium.com/how-to-use-molmo-7b-for-multimodal-ai-extract-text-and-images-with-an-open-source-vision-language-8a31939a2960)\n* **Meta Spirit LMï¼šæ–‡æœ¬å’Œè¯­éŸ³ç”Ÿæˆçš„å¤šæ¨¡æ€AIå®Œæ•´æŒ‡å—:** æŽ¢ç´¢Meta Spirit LMåœ¨ç”Ÿæˆæ–‡æœ¬å’Œè¯­éŸ³æ–¹é¢çš„èƒ½åŠ›ï¼Œä»¥åŠå®ƒå¦‚ä½•åº”ç”¨äºŽå„ç§AIåº”ç”¨ã€‚[åœ¨è¿™é‡Œé˜…è¯»å®Œæ•´æ•™ç¨‹ã€‚](https://readmedium.com/meta-spirit-lm-a-complete-guide-to-multimodal-ai-for-text-and-speech-generation-ed0af74bc950)\n* **ä½¿ç”¨Piper TTSè¶…çº§å¢žå¼ºæ–‡æœ¬åˆ°è¯­éŸ³:** äº†è§£å¦‚ä½•åœ¨è¿™ä¸ªåŠ¨æ‰‹*Google Colabæ•™ç¨‹*ä¸­å®žçŽ°10å€æ›´å¿«ã€å®žæ—¶ã€ç¦»çº¿çš„äººå£°åˆæˆã€‚[åœ¨è¿™é‡Œå°†æ‚¨çš„æ–‡æœ¬è½¬åŒ–ä¸ºæ ©æ ©å¦‚ç”Ÿçš„è¯­éŸ³ã€‚](https://readmedium.com/unleashing-the-power-of-piper-tts-transforming-text-to-speech-10x-faster-with-ai-human-like-voice-eadf2065d66d)\n\n"},{"lang":"zh","group":"blog","slug":"blog/building-a-local-ai-powered-news-aggregator-with-ollama-swarm-and-duckduckgo-95aaf8b3ee41","frontmatter":{"title":"ä½¿ç”¨ Ollamaã€Swarm å’Œ DuckDuckGo æž„å»ºæœ¬åœ° AI æ–°é—»èšåˆå™¨","meta_title":"ä½¿ç”¨ Ollamaã€Swarm å’Œ DuckDuckGo æž„å»ºæœ¬åœ° AI æ–°é—»èšåˆå™¨","description":"æ²¡æœ‰æä¾›å­—å¹•","date":"2024-10-24T17:47:43.000Z","image":"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*OHMOTk_WYGOxWHBsKqdpNQ.jpeg","categories":["Programming","Generative AI","Technology/Web"],"author":"Rifx.Online","tags":["Llama","Swarm","DuckDuckGo","News","Aggregator"],"draft":false,"slug":"blog/building-a-local-ai-powered-news-aggregator-with-ollama-swarm-and-duckduckgo-95aaf8b3ee41"},"content":"\n# ä½¿ç”¨OllamaSwarmå’ŒDuckDuckGoæž„å»ºæœ¬åœ°AIé©±åŠ¨çš„æ–°é—»èšåˆå™¨\n\n\n\nåœ¨å½“ä»Šå¿«èŠ‚å¥çš„ä¸–ç•Œä¸­ï¼Œè·Ÿä¸Šç‰¹å®šé¢†åŸŸæœ€æ–°æ–°é—»çš„æ­¥ä¼å¯èƒ½ä¼šå¾ˆå…·æŒ‘æˆ˜æ€§ã€‚å¦‚æžœæˆ‘ä»¬èƒ½å¤Ÿåˆ©ç”¨ç”Ÿæˆå¼AIå’Œä»£ç†çš„åŠ›é‡ï¼Œåˆ›å»ºä¸€ä¸ªå®Œå…¨åœ¨æœ¬åœ°æœºå™¨ä¸Šè¿è¡Œçš„ä¸ªæ€§åŒ–æ–°é—»èšåˆå™¨å‘¢ï¼Ÿåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†æŽ¢è®¨å¦‚ä½•ä½¿ç”¨**Ollama**çš„Llama 3.2æ¨¡åž‹ã€**Swarm**è¿›è¡Œä»£ç†ç¼–æŽ’ï¼Œä»¥åŠ**DuckDuckGo**è¿›è¡Œç½‘ç»œæœç´¢æ¥æž„å»ºè¿™æ ·çš„ç³»ç»Ÿã€‚\n\n# æœ¬åœ°AIçš„åŠ›é‡\n\néšç€å¤§åž‹è¯­è¨€æ¨¡åž‹çš„å…´èµ·ï¼Œæˆ‘ä»¬çŽ°åœ¨èƒ½å¤Ÿåœ¨ä¸ªäººç”µè„‘ä¸Šè¿è¡Œå¤æ‚çš„AIç³»ç»Ÿã€‚è¿™ä¸ºåˆ›å»ºé’ˆå¯¹æˆ‘ä»¬ç‰¹å®šéœ€æ±‚å®šåˆ¶çš„å·¥å…·å¼€è¾Ÿäº†æ— é™å¯èƒ½ã€‚æˆ‘ä»¬çš„æ–°é—»èšåˆå™¨å°±æ˜¯è¿™ä¸€æ½œåŠ›çš„å®Œç¾Žä¾‹è¯ã€‚\n\n# æˆ‘ä»¬ç³»ç»Ÿçš„ç»„æˆéƒ¨åˆ†\n\n1. **Ollama with Llama 3.2**: è¿™æ˜¯æˆ‘ä»¬ç³»ç»Ÿçš„æ ¸å¿ƒï¼Œä¸ºæˆ‘ä»¬çš„AIä»£ç†æä¾›åŠ¨åŠ›ã€‚\n2. **Swarm**: ä¸€ä¸ªä»£ç†ç¼–æŽ’æ¡†æž¶ï¼Œå…è®¸æˆ‘ä»¬åˆ›å»ºå’Œç®¡ç†å¤šä¸ªAIä»£ç†ã€‚\n3. **DuckDuckGo Search**: æä¾›æœ€æ–°çš„ç½‘é¡µæœç´¢ç»“æžœï¼Œè€Œä¸è·Ÿè¸ªç”¨æˆ·æ•°æ®ã€‚\n\n# å·¥ä½œåŽŸç†\n\næˆ‘ä»¬çš„æ–°é—»èšåˆå™¨ç”±ä¸¤ä¸ªä¸»è¦çš„AIä»£ç†ç»„æˆï¼š\n\n1. **æ–°é—»åŠ©æ‰‹**ï¼šä½¿ç”¨DuckDuckGoæœç´¢èŽ·å–ç‰¹å®šä¸»é¢˜çš„æœ€æ–°æ–°é—»æ–‡ç« ã€‚\n2. **ç¼–è¾‘åŠ©æ‰‹**ï¼šå®¡æŸ¥å¹¶ç²¾ç‚¼æ”¶é›†åˆ°çš„æ–°é—»ä»¥ä¾›æœ€ç»ˆå±•ç¤ºã€‚\n\nè®©æˆ‘ä»¬æ¥åˆ†è§£ä¸€ä¸‹å·¥ä½œæµç¨‹ï¼š\n\n# 1. è®¾ç½®çŽ¯å¢ƒ\n\n\n```python\nollama pull llama3.2\n\nexport OPENAI_MODEL_NAME=llama3.2\nexport OPENAI_BASE_URL=http://localhost:11434/v1\nexport OPENAI_API_KEY=any\n\npip install git+https://github.com/openai/swarm.git duckduckgo-search\n```\næˆ‘ä»¬é¦–å…ˆå¯¼å…¥å¿…è¦çš„åº“å¹¶åˆå§‹åŒ–æˆ‘ä»¬çš„ Swarm å®¢æˆ·ç«¯ï¼š\n\n\n```python\nfrom duckduckgo_search import DDGS\nfrom swarm import Swarm, Agent\nfrom datetime import datetime\n\ncurrent_date = datetime.now().strftime(\"%Y-%m\")\nclient = Swarm()\n```\n\n# 2. åˆ›å»ºæ–°é—»æœç´¢åŠŸèƒ½\n\næˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥ä½¿ç”¨ DuckDuckGo æœç´¢æ–°é—»ï¼š\n\n```python\npythondef get_news_articles(topic):\n  ddg_api = DDGS()\n  results = ddg_api.text(f\"{topic} {current_date}\", max_results=5)\n  if results:\n      news_results = \"\\n\\n\".join([f\"Title: {result['title']}\\nURL: {result['href']}\\nDescription: {result['body']}\" for result in results])\n      return news_results\n  else:\n      return f\"Could not find news results for {topic}.\"\n```\n\n# 3. å®šä¹‰æˆ‘ä»¬çš„ AI ä»£ç†\n\næˆ‘ä»¬ä½¿ç”¨ Ollama çš„ Llama 3.2 æ¨¡åž‹åˆ›å»ºä¸¤ä¸ªä»£ç†ï¼š\n\n\n```python\nnews_agent = Agent(\n  model=\"llama3.2\",\n  name=\"News Assistant\",\n  instructions=\"You provide the latest news articles for a given topic using DuckDuckGo search.\",\n  functions=[get_news_articles],\n)\n\neditor_agent = Agent(\n  model=\"llama3.2\",\n  name=\"Editor Assistant\",\n  instructions=\"You review and finalise the news article for publishing.\",\n)\n```\n\n# 4. åè°ƒå·¥ä½œæµç¨‹\n\næˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥è¿è¡Œæˆ‘ä»¬çš„æ–°é—»èšåˆå·¥ä½œæµç¨‹ï¼š\n\n```python\ndef run_news_workflow(topic):\n  # Fetch news\n  news_response = client.run(\n      agent=news_agent,\n      messages=[{\"role\": \"user\", \"content\": f\"Get me the news about {topic} on {current_date}\"}],\n  )\n  raw_news = news_response.messages[-1][\"content\"]\n  \n  # Pass news to editor for final review\n  edited_news_response = client.run(\n      agent=editor_agent,\n      messages=[{\"role\": \"system\", \"content\": raw_news}],\n  )\n  print(f\"{edited_news_response.messages[-1]['content']}\")\n```\n\n# 5. è¿è¡Œç³»ç»Ÿ\n\næœ€åŽï¼Œæˆ‘ä»¬å¯ä»¥é’ˆå¯¹ä»»ä½•æ„Ÿå…´è¶£çš„è¯é¢˜è¿è¡Œæˆ‘ä»¬çš„æ–°é—»èšåˆå™¨ï¼š\n\n\n```python\nrun_news_workflow(\"AI in Drug Discovery\")\n```\n\n# å®Œæ•´ä»£ç  : app.py\n\n\n```python\nfrom duckduckgo_search import DDGS\nfrom swarm import Swarm, Agent\nfrom datetime import datetime\n\ncurrent_date = datetime.now().strftime(\"%Y-%m\")\n\n# åˆå§‹åŒ– Swarm å®¢æˆ·ç«¯\nclient = Swarm()\n\n# 1. åˆ›å»ºäº’è”ç½‘æœç´¢å·¥å…·\n\ndef get_news_articles(topic):\n    print(f\"æ­£åœ¨ä¸º {topic} è¿›è¡Œ DuckDuckGo æ–°é—»æœç´¢...\")\n    \n    # DuckDuckGo æœç´¢\n    ddg_api = DDGS()\n    results = ddg_api.text(f\"{topic} {current_date}\", max_results=5)\n    if results:\n        news_results = \"\\n\\n\".join([f\"æ ‡é¢˜: {result['title']}\\nç½‘å€: {result['href']}\\næè¿°: {result['body']}\" for result in results])\n        return news_results\n    else:\n        return f\"æœªèƒ½æ‰¾åˆ°å…³äºŽ {topic} çš„æ–°é—»ç»“æžœã€‚\"\n    \n# 2. åˆ›å»º AI ä»£ç†\n\ndef transfer_to_editor_assistant(raw_news):\n    print(\"å°†æ–‡ç« ä¼ é€’ç»™ç¼–è¾‘åŠ©æ‰‹...\")\n    return editor_agent.run({\"role\": \"system\", \"content\": raw_news})\n\n# æ–°é—»ä»£ç†ä»¥èŽ·å–æ–°é—»\nnews_agent = Agent(\n    model=\"llama3.2\",\n    name=\"æ–°é—»åŠ©æ‰‹\",\n    instructions=\"æ‚¨æä¾›æœ‰å…³ç»™å®šä¸»é¢˜çš„æœ€æ–°æ–°é—»æ–‡ç« ï¼Œä½¿ç”¨ DuckDuckGo æœç´¢ã€‚\",\n    functions=[get_news_articles],\n)\n\n# ç¼–è¾‘ä»£ç†ä»¥ç¼–è¾‘æ–°é—»\neditor_agent = Agent(\n    model=\"llama3.2\",\n    name=\"ç¼–è¾‘åŠ©æ‰‹\",\n    instructions=\"æ‚¨å®¡é˜…å¹¶æœ€ç»ˆç¡®å®šæ–°é—»æ–‡ç« ä»¥ä¾›å‘å¸ƒã€‚\",\n)\n\n# 3. åˆ›å»ºå·¥ä½œæµç¨‹\n\ndef run_news_workflow(topic):\n    print(\"è¿è¡Œæ–°é—»ä»£ç†å·¥ä½œæµç¨‹...\")\n    \n    # ç¬¬ä¸€æ­¥: èŽ·å–æ–°é—»\n    news_response = client.run(\n        agent=news_agent,\n        messages=[{\"role\": \"user\", \"content\": f\"èŽ·å–å…³äºŽ {topic} åœ¨ {current_date} çš„æ–°é—»\"}],\n    )\n    raw_news = news_response.messages[-1][\"content\"]\n    print(f\"èŽ·å–çš„æ–°é—»: {raw_news}\")\n    \n    # ç¬¬äºŒæ­¥: å°†æ–°é—»ä¼ é€’ç»™ç¼–è¾‘è¿›è¡Œæœ€ç»ˆå®¡æŸ¥\n    edited_news_response = client.run(\n        agent=editor_agent,\n        messages=[{\"role\": \"system\", \"content\": raw_news}],\n    )\n    print(f\"{edited_news_response.messages[-1]['content']}\")\n\n\n# è¿è¡Œç»™å®šä¸»é¢˜çš„æ–°é—»å·¥ä½œæµç¨‹ç¤ºä¾‹\nrun_news_workflow(\"è¯ç‰©å‘çŽ°ä¸­çš„ AI\")\n```\n\n# ç¤ºä¾‹è¾“å‡º\n\n\n```python\nRunning news Agent workflow...\nRunning DuckDuckGo news search for AI in Drug Discovery...\nFetched news: Here's a formatted answer based on the news articles:\n\n**è¯ç‰©å‘çŽ°ä¸­çš„äººå·¥æ™ºèƒ½ï¼šé©å‘½æ€§çš„è½¬å˜**\n\näººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åœ¨è¯ç‰©å‘çŽ°ä¸­çš„ä½œç”¨æ ‡å¿—ç€åˆ¶è¯é¢†åŸŸçš„é©å‘½æ€§è½¬å˜ã€‚AIåˆ©ç”¨å¤æ‚çš„ç®—æ³•è¿›è¡Œè‡ªä¸»å†³ç­–ï¼Œä»Žæ•°æ®åˆ†æžä¸­å¢žå¼ºäººç±»èƒ½åŠ›ï¼Œè€Œä¸æ˜¯å–ä»£å®ƒä»¬ã€‚\n\n**æŒ‘æˆ˜ä¸Žå±€é™æ€§**\n\nå°½ç®¡æœ‰ç€ä»¤äººæœŸå¾…çš„è¿›å±•ï¼Œä½†åœ¨è¯¥é¢†åŸŸä¸­ä»ç„¶å­˜åœ¨æŒ‘æˆ˜å’Œå±€é™æ€§ã€‚è®ºæ–‡ã€ŠAIåœ¨è¯ç‰©å‘çŽ°ä¸­çš„ä½œç”¨ã€‹æŽ¢è®¨äº†è¿™äº›é—®é¢˜ï¼Œå¼ºè°ƒäº†é«˜è´¨é‡æ•°æ®çš„å¿…è¦æ€§ã€ä¼¦ç†é—®é¢˜çš„è§£å†³ä»¥åŠå¯¹åŸºäºŽAIçš„æ–¹æ³•å±€é™æ€§çš„è®¤è¯†ã€‚\n\n**AIåœ¨è¯ç‰©å‘çŽ°ä¸­çš„åº”ç”¨**\n\nAIæœ‰æ½œåŠ›åœ¨è¯ç‰©å‘çŽ°ã€è®¾è®¡å’Œç ”ç©¶è¯ç‰©é—´ç›¸äº’ä½œç”¨ä¸­å‘æŒ¥å…³é”®ä½œç”¨ã€‚AIåœ¨è¯ç‰©å‘çŽ°ä¸­çš„åº”ç”¨åŒ…æ‹¬ï¼š\n\n* å¤šé¶ç‚¹è¯ç†å­¦ï¼šAIå¯ä»¥é¢„æµ‹åŒ–åˆç‰©å¯¹å¤šç§ç–¾ç—…çš„æœ‰æ•ˆæ€§ã€‚\n* åŒ–å­¦åˆæˆï¼šAIå¯ä»¥ä¼˜åŒ–åŒ–å­¦åˆæˆè¿‡ç¨‹ï¼Œä»¥å®žçŽ°æ›´å¿«å’Œæ›´é«˜æ•ˆçš„ç”Ÿäº§ã€‚\n* è¯ç‰©é‡å®šä½ï¼šAIå¯ä»¥è¯†åˆ«çŽ°æœ‰è¯ç‰©çš„æ–°ç”¨é€”ã€‚\n* é¢„æµ‹è¯ç‰©ç‰¹æ€§ï¼šAIå¯ä»¥é¢„æµ‹åŒ–åˆç‰©çš„æ•ˆåŠ›ã€æ¯’æ€§å’Œç‰©ç†åŒ–å­¦ç‰¹æ€§ã€‚\n\n**è¯ç‰©å‘çŽ°ä¸­AIçš„æœªæ¥**\n\néšç€AIçš„ä¸æ–­å‘å±•ï¼Œé¢„è®¡å°†å¯¹åˆ¶è¯è¡Œä¸šäº§ç”Ÿé‡å¤§å½±å“ã€‚AIçš„æˆåŠŸåº”ç”¨å°†ä¾èµ–äºŽé«˜è´¨é‡æ•°æ®çš„å¯ç”¨æ€§ã€ä¼¦ç†é—®é¢˜çš„è§£å†³ä»¥åŠå¯¹åŸºäºŽAIçš„æ–¹æ³•å±€é™æ€§çš„è®¤è¯†ã€‚\n```\n\n# æœ¬åœ° AI æ–°é—»èšåˆçš„å¥½å¤„\n\n* **éšç§**ï¼šæ‰€æœ‰å¤„ç†éƒ½åœ¨æ‚¨çš„æœ¬åœ°æœºå™¨ä¸Šè¿›è¡Œï¼Œç¡®ä¿æ‚¨çš„æ•°æ®ç•™åœ¨æ‚¨è‡ªå·±æ‰‹ä¸­ã€‚\n* **å®šåˆ¶åŒ–**ï¼šæ‚¨å¯ä»¥è½»æ¾ä¿®æ”¹ä»£ç†çš„æŒ‡ä»¤æˆ–æ·»åŠ æ–°çš„ä»£ç†ä»¥æ»¡è¶³æ‚¨çš„ç‰¹å®šéœ€æ±‚ã€‚\n* **æœ€æ–°ä¿¡æ¯**ï¼šé€šè¿‡ä½¿ç”¨ DuckDuckGo æœç´¢ï¼Œæ‚¨æ€»æ˜¯èƒ½èŽ·å¾—å…³äºŽæ‚¨é€‰æ‹©ä¸»é¢˜çš„æœ€æ–°æ–°é—»ã€‚\n* **AI é©±åŠ¨çš„ç­–å±•**ï¼šç¼–è¾‘åŠ©æ‰‹å¸®åŠ©ç²¾ç‚¼å’Œç»„ç»‡æ”¶é›†çš„æ–°é—»ï¼Œæä¾›æ›´ç²¾è‡´çš„æœ€ç»ˆè¾“å‡ºã€‚\n\n# ç»“è®º\n\nè¿™ä¸ªæœ¬åœ°çš„äººå·¥æ™ºèƒ½é©±åŠ¨æ–°é—»èšåˆå™¨å±•ç¤ºäº†å°†å¤§åž‹è¯­è¨€æ¨¡åž‹ä¸Žç½‘ç»œæœç´¢èƒ½åŠ›ç»“åˆçš„æ½œåŠ›ã€‚é€šè¿‡åˆ©ç”¨Ollamaçš„Llama 3.2æ¨¡åž‹ã€Swarmè¿›è¡Œä»£ç†ç¼–æŽ’ï¼Œä»¥åŠDuckDuckGoè¿›è¡Œæœç´¢ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªå¼ºå¤§çš„å·¥å…·ï¼Œå¯ä»¥è®©æˆ‘ä»¬åœ¨ä»»ä½•æ„Ÿå…´è¶£çš„è¯é¢˜ä¸Šä¿æŒä¿¡æ¯çµé€šï¼ŒåŒæ—¶ç»´æŠ¤æˆ‘ä»¬çš„éšç§ï¼Œå¹¶å®Œå…¨åœ¨æœ¬åœ°è®¡ç®—æœºä¸Šè¿è¡Œã€‚\n\néšç€äººå·¥æ™ºèƒ½çš„ä¸æ–­å‘å±•ï¼Œåˆ›å»ºä¸ªæ€§åŒ–ã€äººå·¥æ™ºèƒ½é©±åŠ¨å·¥å…·çš„å¯èƒ½æ€§åªä¼šä¸æ–­æ‰©å¤§ã€‚è¿™ä¸ªæ–°é—»èšåˆå™¨åªæ˜¯ä¸€ä¸ªå¼€å§‹â€”â€”æƒ³è±¡ä¸€ä¸‹ï¼Œåˆ©ç”¨è¿™äº›æŠ€æœ¯ä½ è¿˜å¯ä»¥æž„å»ºå“ªäº›å…¶ä»–åˆ›æ–°åº”ç”¨ï¼\n\n# å‚è€ƒï¼š\n\nSwarm Github : <https://github.com/openai/swarm>\n\nå¦‚æžœæ‚¨è§‰å¾—è¿™ç¯‡æ–‡ç« ä¿¡æ¯ä¸°å¯Œä¸”æœ‰ä»·å€¼ï¼Œæˆ‘å°†éžå¸¸æ„Ÿè°¢æ‚¨çš„æ”¯æŒï¼š\n\n* åœ¨Mediumä¸Šä¸ºå®ƒç‚¹èµžå‡ æ¬¡ ðŸ‘ï¼Œå¸®åŠ©å…¶ä»–äººå‘çŽ°è¿™ç¯‡å†…å®¹ï¼ˆæ‚¨çŸ¥é“æ‚¨å¯ä»¥ç‚¹èµžå¤šè¾¾50æ¬¡å—ï¼Ÿï¼‰ã€‚æ‚¨çš„ç‚¹èµžå°†å¸®åŠ©æ›´å¤šè¯»è€…ä¼ æ’­çŸ¥è¯†ã€‚\n- ä¸Žæ‚¨çš„AIçˆ±å¥½è€…å’Œä¸“ä¸šäººå£«ç½‘ç»œåˆ†äº«ã€‚\n- åœ¨LinkedInä¸Šä¸Žæˆ‘è”ç³»ï¼š<https://www.linkedin.com/in/manjunath-janardhan-54a5537/>\n\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/building-a-reliable-text-classification-pipeline-with-llms-a-step-by-step-guide-87dc73213605","frontmatter":{"title":"ä½¿ç”¨ LLMs æž„å»ºå¯é çš„æ–‡æœ¬åˆ†ç±»ç®¡é“ï¼šåˆ†æ­¥æŒ‡å—","meta_title":"ä½¿ç”¨ LLMs æž„å»ºå¯é çš„æ–‡æœ¬åˆ†ç±»ç®¡é“ï¼šåˆ†æ­¥æŒ‡å—","description":"æœ¬æ–‡ä»‹ç»äº†å¦‚ä½•ä½¿ç”¨å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰æž„å»ºå¯é çš„æ–‡æœ¬åˆ†ç±»ç®¡é“ï¼Œé‡ç‚¹æŽ¢è®¨äº†ä¸‰ç§å…³é”®æŠ€æœ¯ï¼šå—é™ç”Ÿæˆã€å°‘é‡ç¤ºä¾‹æç¤ºå’ŒåŠ¨æ€ç¤ºä¾‹é€‰æ‹©ã€‚å—é™ç”Ÿæˆé€šè¿‡é™åˆ¶æ¨¡åž‹è¾“å‡ºåœ¨é¢„å®šä¹‰ç±»åˆ«å†…ï¼Œå‡å°‘äº†åŽå¤„ç†éœ€æ±‚ï¼›å°‘é‡ç¤ºä¾‹æç¤ºåˆ©ç”¨æ¨¡åž‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ä»¥æé«˜åˆ†ç±»å‡†ç¡®æ€§ï¼›åŠ¨æ€ç¤ºä¾‹é€‰æ‹©åˆ™é€šè¿‡æ£€ç´¢ä¸Žè¾“å…¥æ–‡æœ¬ç›¸ä¼¼çš„ç¤ºä¾‹ï¼Œæ˜¾è‘—æå‡åˆ†ç±»å‡†ç¡®çŽ‡è‡³88.6%ã€‚è¿™äº›æ–¹æ³•å±•ç¤ºäº†LLMsåœ¨æ–‡æœ¬åˆ†ç±»ä¸­çš„æœ‰æ•ˆæ€§å’Œçµæ´»æ€§ï¼Œä¸ºæž„å»ºé«˜æ€§èƒ½åˆ†ç±»ç³»ç»Ÿæä¾›äº†å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Cmk7IkUnY-SIxhVF","categories":["Natural Language Processing","Machine Learning","Generative AI"],"author":"Rifx.Online","tags":["constrained","generation","prompting","selection","classification"],"draft":false,"slug":"blog/building-a-reliable-text-classification-pipeline-with-llms-a-step-by-step-guide-87dc73213605"},"content":"\n\n\n### å…‹æœåŸºäºŽLLMçš„æ–‡æœ¬åˆ†ç±»ä¸­çš„å¸¸è§æŒ‘æˆ˜\n\n\n\nåœ¨æœ¬åˆ†æ­¥æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»å¦‚ä½•ä½¿ç”¨å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰æž„å»ºä¸€ä¸ªå‡†ç¡®ä¸”å¯é çš„æ–‡æœ¬åˆ†ç±»ç®¡é“ã€‚LLMsæ˜¯å¼ºå¤§çš„é€šç”¨æ¨¡åž‹ï¼Œåœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å±•ç¤ºäº†å“è¶Šçš„èƒ½åŠ›ï¼Œå¹¶ä¸”å®ƒä»¬åœ¨è®¸å¤šäººå·¥æ™ºèƒ½åº”ç”¨ä¸­è¶Šæ¥è¶Šå¤šåœ°å–ä»£äº†ä¸“ä¸šæ¨¡åž‹ã€‚ç„¶è€Œï¼Œå¦‚æžœä¸è°¨æ…Žå¤„ç†ï¼Œä½¿ç”¨LLMsè¿›è¡Œåˆ†ç±»å¯èƒ½ä¼šå¾ˆæ£˜æ‰‹ã€‚\n\nåœ¨å°†LLMsåº”ç”¨äºŽåˆ†ç±»æ—¶ï¼Œä¸€ä¸ªå¸¸è§çš„é—®é¢˜æ˜¯æ¨¡åž‹å¯èƒ½æ— æ³•ä»¥é¢„æœŸçš„è¾“å‡ºæˆ–æ ¼å¼å“åº”ï¼Œä»Žè€Œå¯¼è‡´é¢å¤–çš„åŽå¤„ç†ï¼Œè€Œè¿™äº›åŽå¤„ç†å¯èƒ½å¤æ‚ä¸”è€—æ—¶ã€‚æœ¬æ–‡å°†æ¶µç›–è§£å†³è¿™äº›æŒ‘æˆ˜çš„å®žç”¨æŠ€å·§å’ŒæŠ€æœ¯ã€‚æ¯ç§ç­–ç•¥éƒ½æ˜“äºŽå®žæ–½ï¼Œä½†å¯ä»¥æ˜¾è‘—æé«˜LLMsä½œä¸ºæ–‡æœ¬åˆ†ç±»å™¨çš„å‡†ç¡®æ€§å’Œå¯ç”¨æ€§ã€‚è®©æˆ‘ä»¬æ·±å…¥æŽ¢è®¨ï¼Œä½¿æ‚¨çš„LLMæ–‡æœ¬åˆ†ç±»ç³»ç»Ÿæ—¢é«˜æ•ˆåˆå¯é ã€‚\n\n## ä¸»è¦æ€æƒ³\n\nåœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†æŽ¢è®¨ä¸‰ç§å…³é”®æŠ€æœ¯ï¼Œè¿™äº›æŠ€æœ¯å¯ä»¥ä½¿ LLM åœ¨æ–‡æœ¬åˆ†ç±»å™¨ä¸­å˜å¾—æ›´åŠ æœ‰æ•ˆå’Œé«˜æ•ˆã€‚**æœ¬æ•™ç¨‹ä¸­æˆ‘ä»¬ä¸ä¼šæ·±å…¥è®¨è®ºå¾®è°ƒé€‰é¡¹**ï¼Œä½†å¦‚æžœæ‚¨å¯¹è¿™ä¸€æŠ€æœ¯æ„Ÿå…´è¶£ï¼Œå¯ä»¥æŸ¥çœ‹æˆ‘å…¶ä»–çš„ä¸€äº›å¸–å­ï¼š\n\nç¬¬ä¸€ç§æŠ€æœ¯æ˜¯ *å—é™ç”Ÿæˆ*ã€‚è¿™æ¶‰åŠè®¾å®šç‰¹å®šçš„çº¦æŸï¼Œå¼•å¯¼ LLM ç”Ÿæˆéµå¾ªæŒ‡å®šæž¶æž„çš„æ ‡è®°ï¼Œè¿™æœ‰åŠ©äºŽç¡®ä¿è¾“å‡ºç¬¦åˆé¢„æœŸæ ¼å¼ã€‚é€šè¿‡åº”ç”¨è¿™äº›çº¦æŸï¼Œæˆ‘ä»¬å¯ä»¥å‡å°‘å¤æ‚åŽå¤„ç†çš„éœ€è¦ï¼Œä»¥èŽ·å¾—æ­£ç¡®æ ¼å¼çš„ç±»åˆ«é¢„æµ‹ã€‚\n\næˆ‘ä»¬å°†ç ”ç©¶çš„ç¬¬äºŒç§æŠ€æœ¯æ˜¯ *å°‘é‡ç¤ºä¾‹æç¤º*ã€‚å°‘é‡ç¤ºä¾‹æç¤ºé€šè¿‡åœ¨ LLM å°è¯•åˆ†ç±»æ–°æ•°æ®ä¹‹å‰æä¾›ä¸€äº›ç¤ºä¾‹è¾“å‡ºæ¥å·¥ä½œã€‚ç”±äºŽ LLM è¢«è®¤ä¸ºæ˜¯å¼ºå¤§çš„ä¸Šä¸‹æ–‡å­¦ä¹ è€…ï¼Œå®ƒä»¬èƒ½å¤Ÿä»Žè¿™äº›ç¤ºä¾‹ä¸­è¯†åˆ«æ¨¡å¼ï¼Œå¹¶ç”Ÿæˆä¸Žä¹‹ç›¸ä¼¼çš„è¾“å‡ºã€‚è¿™ç§æ–¹æ³•ä½¿æˆ‘ä»¬èƒ½å¤Ÿé€šè¿‡å‘ LLM å±•ç¤ºå®ƒåº”è¯¥ç”Ÿæˆçš„å“åº”ç±»åž‹æ¥æé«˜é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚\n\næœ€åŽï¼Œæˆ‘ä»¬å°†ä»‹ç»ç”¨äºŽå°‘é‡ç¤ºä¾‹æç¤ºçš„ *åŠ¨æ€ç¤ºä¾‹é€‰æ‹©*ã€‚ç±»ä¼¼äºŽå¢žå¼ºæ£€ç´¢ç”Ÿæˆï¼Œä½†ä¸“ä¸ºåˆ†ç±»ä»»åŠ¡è®¾è®¡ï¼Œè¿™ç§æ–¹æ³•åŸºäºŽä¸Žæ–°è¾“å…¥çš„ç›¸ä¼¼æ€§åŠ¨æ€é€‰æ‹©ç¤ºä¾‹ï¼Œä½¿ç”¨æœ€è¿‘é‚»æŠ€æœ¯ã€‚è¿™æ ·ï¼ŒLLM åœ¨ç”Ÿæˆæœ€ç»ˆåˆ†ç±»ä¹‹å‰ï¼Œä¼šæŽ¥æ”¶åˆ°æœ€ç›¸å…³çš„è¾“å…¥-è¾“å‡ºå¯¹ï¼Œä»Žè€Œå¯¼è‡´æ›´ç²¾ç¡®çš„é¢„æµ‹ã€‚\n\næ¯ç§æŠ€æœ¯å°†è¯¦ç»†è§£é‡Šï¼Œå¹¶æä¾›åŸºäºŽ LangChain æ¡†æž¶çš„ä»£ç ç¤ºä¾‹ä»¥ç®€åŒ–å®žçŽ°ã€‚æ‚¨å°†èƒ½å¤Ÿå°†è¿™äº›æ–¹æ³•ç›´æŽ¥çº³å…¥æ‚¨çš„ NLP å·¥å…·åŒ…ï¼Œæˆ–æ ¹æ®æ‚¨çš„ç‰¹å®šéœ€æ±‚è¿›è¡Œå®šåˆ¶ï¼Œä»¥ä¾¿å»ºç«‹ä¸€ä¸ªå¯é ä¸”å‡†ç¡®çš„æ–‡æœ¬åˆ†ç±»ç®¡é“ã€‚\n\n## ä¸ºä»€ä¹ˆä½¿ç”¨ LLM è¿›è¡Œåˆ†ç±»\n\nåœ¨å¼€å§‹ä¹‹å‰ï¼Œè®©æˆ‘ä»¬èŠ±ä¸€ç‚¹æ—¶é—´è€ƒè™‘ä¸€ä¸‹ä¸ºä»€ä¹ˆæ‚¨å¯èƒ½é€‰æ‹©ä½¿ç”¨ LLM è¿›è¡Œæ–‡æœ¬åˆ†ç±»ï¼Œè€Œä¸æ˜¯ä½¿ç”¨å®šåˆ¶çš„ä¸“ç”¨æ¨¡åž‹ã€‚\n\nä½¿ç”¨ LLM çš„ä¸€ä¸ªä¸»è¦ä¼˜åŠ¿æ˜¯å®ƒä»¬åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬é¢„æµ‹æ–¹é¢çš„ç†Ÿç»ƒç¨‹åº¦ã€‚å³ä½¿æ•°æ®é‡å¾ˆå°‘ï¼ŒLLM é€šå¸¸ä¹Ÿèƒ½äº§ç”Ÿåˆç†çš„ç»“æžœï¼Œè¿™ä½¿å¾—å®ƒä»¬åœ¨æ ‡è®°æ•°æ®ç¨€ç¼ºæ—¶æˆä¸ºæžå¥½çš„é€‰æ‹©ã€‚æ­¤å¤–ï¼Œä½œä¸ºé€šç”¨æ¨¡åž‹ï¼ŒLLM å¯¹ä¸–ç•Œæœ‰ç€å¹¿æ³›çš„çŸ¥è¯†ï¼Œæœ‰æ•ˆåœ°è®°å¿†æ¥è‡ªå„ç§æ¥æºçš„ä¿¡æ¯ã€‚è¿™æ„å‘³ç€å®ƒä»¬æœ‰æ—¶èƒ½å¤Ÿå¤„ç†æ„å¤–è¾“å…¥ï¼Œå¹¶ä»ç„¶äº§ç”Ÿå‡†ç¡®çš„é¢„æµ‹ã€‚\n\nå¦ä¸€ä¸ªæ˜¾è‘—çš„å¥½å¤„æ˜¯æ–¹ä¾¿è®¿é—® LLM ä½œä¸ºæœåŠ¡ã€‚è®¸å¤š LLM çŽ°åœ¨é€šè¿‡äº‘å¹³å°æä¾›ï¼Œè¿™æ„å‘³ç€æ‚¨ä¸éœ€è¦è‡ªå·±ç®¡ç†ä»»ä½•åŸºç¡€è®¾æ–½ã€‚æ‚¨åªéœ€ä¸ºæ‰€ä½¿ç”¨çš„æœåŠ¡ä»˜è´¹ï¼Œè¿™ä¸ºæ‚¨æä¾›äº†çµæ´»æ€§ï¼Œå¯ä»¥æ ¹æ®éœ€è¦è¿›è¡Œæ‰©å±•ï¼Œè€Œæ— éœ€æŠ•èµ„ç¡¬ä»¶æˆ–ç®¡ç† GPU èµ„æºã€‚è¿™å¯¹ AI åº”ç”¨ç¨‹åºæ¥è¯´æ˜¯ä¸€ä¸ªå·¨å¤§çš„èµ„äº§ï¼Œå› ä¸ºå®ƒé™ä½Žäº†å‰æœŸæˆæœ¬ï¼Œå¹¶æ¶ˆé™¤äº†ç»´æŠ¤å¤æ‚æœºå™¨å­¦ä¹ åŸºç¡€è®¾æ–½çš„å¿…è¦æ€§ã€‚\n\nç„¶è€Œï¼Œä¹Ÿæœ‰ä¸€äº›æ½œåœ¨çš„ç¼ºç‚¹éœ€è¦è€ƒè™‘ã€‚ä¸€ä¸ªæ˜¯å»¶è¿Ÿï¼šè™½ç„¶å®šåˆ¶çš„å°åž‹åˆ†ç±»æ¨¡åž‹é€šå¸¸åœ¨å‡ åæ¯«ç§’å†…å“åº”ï¼Œä½† LLM çš„å»¶è¿Ÿé€šå¸¸æ›´é«˜ï¼ŒèŒƒå›´ä»Žå‡ ç™¾æ¯«ç§’åˆ°å‡ ç§’ï¼Œå…·ä½“å–å†³äºŽå…¶å¤§å°ã€‚å¯¹äºŽéœ€è¦å®žæ—¶å¤„ç†çš„åº”ç”¨ç¨‹åºï¼Œè¿™ç§å»¶è¿Ÿå¯èƒ½æ˜¯ä¸€ä¸ªç¼ºç‚¹ã€‚\n\næ•°æ®éšç§æ˜¯å¦ä¸€ä¸ªå…³æ³¨ç‚¹ã€‚å¦‚æžœæ‚¨éœ€è¦å‡ºäºŽåˆè§„æˆ–å®‰å…¨åŽŸå› å°†æ‰€æœ‰æ•°æ®ä¿ç•™åœ¨è‡ªå·±çš„åŸºç¡€è®¾æ–½ä¸­ï¼Œä½¿ç”¨ LLM æœåŠ¡å¯èƒ½ä¸æ˜¯æœ€ä½³é€‰æ‹©ã€‚æ‚¨è¦ä¹ˆéœ€è¦åœ¨å†…éƒ¨æ‰˜ç®¡ LLMâ€”â€”è¿™å¯èƒ½ä¼šå¾ˆæ˜‚è´µâ€”â€”è¦ä¹ˆå¯»æ‰¾ä¸€ç§èƒ½å¤Ÿå°†æ•°æ®ä¿ç•™åœ¨å†…éƒ¨çš„æ›¿ä»£æ–¹æ¡ˆã€‚\n\nå¦ä¸€ä¸ªé™åˆ¶æ˜¯å¯¹ LLM æœåŠ¡æä¾›å•†çš„ä¾èµ–ã€‚å°† LLM ä½œä¸ºæœåŠ¡ä½¿ç”¨æ„å‘³ç€æ‚¨å—åˆ°å…¶é€ŸçŽ‡é™åˆ¶ã€å»¶è¿Ÿå’Œæ½œåœ¨åœæœºæ—¶é—´çš„å½±å“ï¼Œè€Œæ‚¨å¯¹æ­¤å‡ ä¹Žæ²¡æœ‰æŽ§åˆ¶æƒã€‚æä¾›å•†ç«¯çš„ä»»ä½•é—®é¢˜éƒ½å¯èƒ½å½±å“æ‚¨å¯é å’ŒåŠæ—¶åœ°åˆ†ç±»æ–‡æœ¬çš„èƒ½åŠ›ï¼Œè¿™å¯èƒ½æ˜¯å¯¹éœ€è¦é«˜å¯é æ€§çš„åº”ç”¨ç¨‹åºçš„ä¸€ä¸ªç¼ºç‚¹ã€‚\n\nè€ƒè™‘åˆ°è¿™äº›ä¼˜ç¼ºç‚¹ï¼Œæ‚¨å¯ä»¥è¯„ä¼°ä½¿ç”¨ LLM ä½œä¸ºåˆ†ç±»å™¨æ˜¯å¦é€‚åˆæ‚¨çš„ç‰¹å®šéœ€æ±‚ã€‚æ— è®ºå¦‚ä½•ï¼ŒLLM æ˜¯æ‚¨æ•°æ®ç§‘å­¦å·¥å…·åŒ…ä¸­ä¸€ä¸ªå¼ºå¤§çš„å·¥å…·ï¼Œä½¿æ‚¨èƒ½å¤Ÿå¿«é€Ÿè®¾ç½® AI æœåŠ¡å¹¶å¼€å§‹æž„å»ºæœ‰å½±å“åŠ›çš„åº”ç”¨ç¨‹åºã€‚\n\n## æƒ³æ³• 1ï¼šåˆ†ç±»çš„çº¦æŸè¾“å‡º\n\nçŽ°åœ¨æˆ‘ä»¬å·²ç»è¦†ç›–äº†èƒŒæ™¯ï¼Œè®©æˆ‘ä»¬æ·±å…¥æ•™ç¨‹çš„æŠ€æœ¯éƒ¨åˆ†ã€‚å¦‚å‰æ‰€è¿°ï¼Œæˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªæŠ€æœ¯æ˜¯å®žçŽ°**çº¦æŸç”Ÿæˆ**ï¼Œä»¥ç¡®ä¿LLMä»…è¾“å‡ºæœ‰æ•ˆçš„ç±»åˆ«æ ‡ç­¾ã€‚é€šè¿‡å°†è¾“å‡ºé™åˆ¶ä¸ºé¢„å®šä¹‰çš„ç±»åˆ«åç§°é›†åˆï¼Œæˆ‘ä»¬æ¶ˆé™¤äº†è§£æžæˆ–æ¸…ç†è‡ªç”±æ ¼å¼å“åº”çš„éœ€è¦ï¼Œä»Žè€Œå‡å°‘äº†é”™è¯¯çš„å¯èƒ½æ€§ï¼Œå¹¶æé«˜äº†åˆ†ç±»ç®¡é“çš„å¯é æ€§ã€‚\n\nä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨LangChain OpenAIå®¢æˆ·ç«¯åŒ…è£…å™¨ï¼Œä½†å®ƒé€‚ç”¨äºŽä»»ä½•ä¸ŽOpenAIå…¼å®¹çš„æ¨¡åž‹*(æˆ‘ä»¬åœ¨è¿™äº›å®žéªŒä¸­ä½¿ç”¨[NebiusAI](https://studio.nebius.ai/))*ã€‚è¿™ä¸ªåŒ…è£…å™¨å°†å…è®¸æˆ‘ä»¬å‘LLMå‘é€ç»“æž„åŒ–æŸ¥è¯¢ï¼Œéµå¾ªæˆ‘ä»¬å°†å®šä¹‰çš„ç‰¹å®šæ¨¡å¼ã€‚\n\n### ç¬¬ä¸€æ­¥ï¼šå®šä¹‰è¾“å‡ºæ¨¡å¼\n\næˆ‘ä»¬é¦–å…ˆå®šä¹‰è¾“å‡ºçš„æ¨¡å¼ï¼Œè¯¥æ¨¡å¼å°†ç”±ä¸€ä¸ªç±»åˆ«å­—æ®µç»„æˆã€‚è¯¥å­—æ®µå°†ä½¿ç”¨ \\`Literal\\` ç±»åž‹ï¼Œåˆ—å‡ºæ¯ä¸ªå¯èƒ½çš„ç±»åä½œä¸ºå­—ç¬¦ä¸²ã€‚é€šè¿‡è¿™æ ·åšï¼Œæˆ‘ä»¬ç¡®ä¿ LLM çš„è¾“å‡ºä¸¥æ ¼æ˜¯è¿™äº›æœ‰æ•ˆç±»ä¸­çš„ä¸€ä¸ªï¼Œæˆ‘ä»¬å¯ä»¥ç›´æŽ¥å°†å…¶ç”¨ä½œæ¨¡åž‹çš„é¢„æµ‹ã€‚\n\næ¨¡å¼å®šä¹‰ä½¿ç”¨ \\`pydantic\\` å®žçŽ°å¦‚ä¸‹ï¼š\n\n```python\nfrom typing import Literal\nfrom pydantic import BaseModel\n\ndef generate_classification_model(list_classes: list[str]):\n    assert list_classes  # ç¡®ä¿ç±»åˆ—è¡¨ä¸ä¸ºç©º\n\n    class ClassificationOutput(BaseModel):\n        category: Literal[tuple(list_classes)]\n\n    return ClassificationOutput\n\n## ç¤ºä¾‹ç”¨æ³•\nif __name__ == \"__main__\":\n    Categories = generate_classification_model([\"Yes\", \"No\"])\n    categories = Categories(category=\"Yes\")\n    print(categories)\n```\nåœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåä¸º \\`ClassificationOutput\\` çš„ Pydantic æ¨¡åž‹ï¼Œå…·æœ‰ä¸€ä¸ª \\`category\\` å­—æ®µï¼Œè¯¥å­—æ®µé™åˆ¶ä¸ºä¸€ç»„å­—é¢å€¼ï¼Œå¦‚â€œYesâ€å’Œâ€œNoâ€ã€‚è¿™ä¸ªè®¾ç½®ä½¿æˆ‘ä»¬èƒ½å¤ŸéªŒè¯ LLM çš„è¾“å‡ºï¼Œç¡®ä¿å®ƒæ˜¯é¢„å®šä¹‰ç±»åä¹‹ä¸€ã€‚\n\n### ç¬¬2æ­¥ï¼šæž„å»ºå¹¶å‘é€æ¶ˆæ¯\n\næŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å‡†å¤‡ä¸€ç³»åˆ—æ¶ˆæ¯å‘é€ç»™LLMã€‚ç¬¬ä¸€æ¡æ¶ˆæ¯æ˜¯ç³»ç»Ÿæç¤ºï¼Œé€šè¿‡æè¿°ä»»åŠ¡ï¼ˆåˆ†ç±»ï¼‰å¹¶åˆ—å‡ºå¯èƒ½çš„è¾“å‡ºç±»åˆ«æ¥è®¾ç½®ä¸Šä¸‹æ–‡ã€‚è¿™å¼•å¯¼LLMç”Ÿæˆä¸Žæ‰€éœ€æ¨¡å¼åŒ¹é…çš„è¾“å‡ºã€‚ç¬¬äºŒæ¡æ¶ˆæ¯åŒ…å«æˆ‘ä»¬å¸Œæœ›LLMåˆ†ç±»çš„å®žé™…æ–‡æœ¬ã€‚\n\nä½¿ç”¨LangChainå®¢æˆ·ç«¯åŒ…è£…å™¨ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹è®¾ç½®é…ç½®æˆ‘ä»¬çš„LLMï¼š\n\n```python\nimport os\nfrom typing import Literal\n\nfrom dotenv import load_dotenv\nfrom langchain_core.messages import HumanMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom pydantic import BaseModel\n\nload_dotenv()\n\n\nclass ClassificationOutput(BaseModel):\n    category: Literal[\"news\", \"clickbait\"]\n\n\nllm_client = ChatOpenAI(\n    openai_api_base=os.environ.get(\"LLM_BASE_URL\"),\n    model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n    openai_api_key=os.environ.get(\"LLM_API_KEY\"),\n    temperature=0,\n    max_retries=2,\n)\n\nconstrained_llm = llm_client.with_structured_output(ClassificationOutput)\n\nmessages = [\n    SystemMessage(\n        content=\"å°†ä»¥ä¸‹æ–‡æœ¬åˆ†ç±»ä¸ºé¢„å®šä¹‰ç±»åˆ«ä¹‹ä¸€ï¼šæ–°é—»æˆ–ç‚¹å‡»è¯±é¥µ\"\n    ),\n    HumanMessage(content=\"ä½ ä¸ä¼šç›¸ä¿¡æŽ¥ä¸‹æ¥å‘ç”Ÿäº†ä»€ä¹ˆï¼\"),\n]\nprediction = constrained_llm.invoke(messages)\n\nprint(prediction)\n\n## Gives category='clickbait'\n```\né€šè¿‡è¿™ç§æ–¹æ³•ï¼ŒLLMçš„è¾“å‡ºå°†ä¸Žæˆ‘ä»¬é¢„å®šä¹‰çš„ç±»åˆ«åŒ¹é…ï¼Œä½¿å…¶å¯ä»¥ç›´æŽ¥ä½œä¸ºåˆ†ç±»ç»“æžœä½¿ç”¨ï¼Œè€Œæ— éœ€è¿›ä¸€æ­¥å¤„ç†ã€‚\n\n### ç¬¬3æ­¥ï¼šè¯„ä¼°\n\nä¸ºäº†è¯„ä¼°æ¨¡åž‹çš„æ€§èƒ½ï¼Œæˆ‘ä»¬åœ¨[20 Newsgroupsæ•°æ®é›†](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html)ï¼ˆCC BY 4\\.0\\ï¼‰ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼Œæ¨¡åž‹çš„å‡†ç¡®çŽ‡è¾¾åˆ°äº†**76\\.3%**ã€‚è¯¥è®¾ç½®å±•ç¤ºäº†å—é™ç”Ÿæˆåœ¨æé«˜åˆ†ç±»å‡†ç¡®æ€§å’Œå‡å°‘é¢å¤–å¤„ç†æ­¥éª¤æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚\n\n## Idea 2: Few\\-shot prompting\n\nç¬¬äºŒç§æŠ€æœ¯æ˜¯ *few\\-shot prompting*ï¼Œæˆ‘ä»¬åœ¨æç¤ºä¸­åŒ…å«ä¸€äº›ç¤ºä¾‹è¾“å…¥\\-è¾“å‡ºå¯¹ï¼Œä»¥å¼•å¯¼LLMã€‚è¿™ç§æ–¹æ³•åˆ©ç”¨äº†LLMçš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿä»Žæä¾›çš„ç¤ºä¾‹ä¸­æ•æ‰æ¨¡å¼ï¼Œé€šå¸¸ä¼šå¯¼è‡´åˆ†ç±»å‡†ç¡®æ€§çš„æé«˜ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†é€šè¿‡åœ¨æç¤ºä¸­ç›´æŽ¥æ·»åŠ ä¸€äº›ç¤ºä¾‹åˆ†ç±»æ¥å®žçŽ°few\\-shot promptingï¼Œä»¥å¢žå¼ºæ¨¡åž‹çš„è¾“å‡ºè´¨é‡ã€‚\n\nè®©æˆ‘ä»¬æ¥çœ‹çœ‹ä»£ç ï¼š\n\n```python\nimport os\nfrom typing import Literal\n\nfrom dotenv import load_dotenv\nfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\nfrom pydantic import BaseModel\n\nload_dotenv()\n\n\nclass ClassificationOutput(BaseModel):\n    category: Literal[\"news\", \"clickbait\"]\n\n\nllm_client = ChatOpenAI(\n    openai_api_base=os.environ.get(\"LLM_BASE_URL\"),\n    model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n    openai_api_key=os.environ.get(\"LLM_API_KEY\"),\n    temperature=0,\n    max_retries=10,\n)\n\nconstrained_llm = llm_client.with_structured_output(ClassificationOutput)\n\nmessages = [\n    SystemMessage(\n        content=\"Classify the following text into one of the predefined categories: news or clickbait\"\n    ),\n    HumanMessage(content=\"The Shocking Truth Behind a Popular Wellness Trend\"),\n    AIMessage(content=\"clickbait\"),\n    HumanMessage(content=\"UK farmers call for weedkiller ban over Parkinsonâ€™s fears\"),\n    AIMessage(content=\"news\"),\n    HumanMessage(content=\"You won't believe what happened next!\"),\n]\nprediction = constrained_llm.invoke(messages)\n\nprint(prediction)\n\n## Gives category='clickbait'\n```\nåœ¨è¿™ä¸ªè®¾ç½®ä¸­ï¼Œæˆ‘ä»¬æž„å»ºäº†ä¸€ä¸ªåŒ…å« *HumanMessage* å’Œ *AIMessage* ç±»åž‹çš„å¯¹è¯åŽ†å²ï¼Œä»¥æ¨¡æ‹Ÿæˆ‘ä»¬æœŸæœ›LLMå¯¹æ–‡æœ¬è¿›è¡Œåˆ†ç±»çš„ç¤ºä¾‹ã€‚é€šè¿‡å±•ç¤ºæˆ‘ä»¬æƒ³è¦çš„åˆ†ç±»é£Žæ ¼å’Œæ ¼å¼â€”â€”ä¾‹å¦‚å°†â€œThe Shocking Truth Behind a Popular Wellness Trendâ€åˆ†ç±»ä¸ºâ€œclickbaitâ€ï¼Œå°†â€œUK farmers call for weedkiller ban over Parkinsonâ€™s fearsâ€åˆ†ç±»ä¸ºâ€œnewsâ€â€”â€”æˆ‘ä»¬ä¸ºæ¨¡åž‹è®¾å®šäº†æ˜Žç¡®çš„æœŸæœ›ã€‚å½“æœ€ç»ˆçš„åˆ†ç±»è¯·æ±‚â€œYou wonâ€™t believe what happened next!â€è¢«å‘é€æ—¶ï¼ŒLLMå¯ä»¥åˆ©ç”¨è¿™äº›ç¤ºä¾‹æ¥ç¡®å®šé€‚å½“çš„å“åº”ã€‚\n\nåœ¨æµ‹è¯•è¿™ç§few\\-shotæ–¹æ³•åŽï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°å‡†ç¡®çŽ‡ä¸º **76\\.6%**ï¼Œæ¯”æˆ‘ä»¬çš„çº¦æŸç”Ÿæˆæ–¹æ³•ç•¥æœ‰æé«˜ã€‚ç„¶è€Œï¼Œç”±äºŽç¤ºä¾‹æ˜¯éšæœºé€‰æ‹©çš„ï¼Œè¿™å¯èƒ½æ— æ³•å……åˆ†å±•ç¤ºfew\\-shot promptingçš„æ½œåŠ›ã€‚ä»”ç»†é€‰æ‹©æˆ–ç­–åˆ’ä¸Žè¾“å…¥æ•°æ®æ›´ç´§å¯†åŒ¹é…çš„ç¤ºä¾‹å¯èƒ½ä¼šäº§ç”Ÿæ›´å¥½çš„ç»“æžœã€‚åœ¨æœ¬æ•™ç¨‹çš„ä¸‹ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†ç ”ç©¶ä¸€ç§æ›´é«˜çº§çš„æŠ€æœ¯ï¼šåŸºäºŽç›¸ä¼¼æ€§åŠ¨æ€é€‰æ‹©ç¤ºä¾‹ï¼Œè¿™å¯èƒ½è¿›ä¸€æ­¥æé«˜å‡†ç¡®æ€§ã€‚\n\n## æƒ³æ³• 3ï¼šåŠ¨æ€ç¤ºä¾‹é€‰æ‹©\n\næˆ‘ä»¬æé«˜ LLM åˆ†ç±»å‡†ç¡®æ€§çš„ç¬¬ä¸‰ç§æŠ€æœ¯æ˜¯æ ¹æ®æŸ¥è¯¢ä¸­çš„æ–‡æœ¬åŠ¨æ€é€‰æ‹©ç›¸å…³ç¤ºä¾‹ã€‚æˆ‘ä»¬ä¸æ˜¯ä½¿ç”¨é™æ€çš„å°‘é‡ç¤ºä¾‹æç¤ºï¼Œè€Œæ˜¯é’ˆå¯¹æ¯ä¸ªæŸ¥è¯¢ä½¿ç”¨ ChromaDB è¿›è¡Œç›¸ä¼¼æ€§æœç´¢ï¼Œä»¥è¯†åˆ«å…¶åœ¨æ ‡è®°è®­ç»ƒé›†ä¸­çš„æœ€è¿‘é‚»ã€‚é€šè¿‡é€‰æ‹©ä¸Žè¾“å…¥æ–‡æœ¬åœ¨è¯­å¢ƒä¸Šç›¸ä¼¼çš„ç¤ºä¾‹ï¼Œæˆ‘ä»¬å¯ä»¥ä¸º LLM æä¾›é«˜åº¦ç›¸å…³çš„ä¿¡æ¯ï¼Œä»Žè€Œå¢žåŠ å‡†ç¡®åˆ†ç±»çš„å¯èƒ½æ€§ã€‚\n\nè¦å®žçŽ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬é¦–å…ˆæž„å»ºä¸€ä¸ªåŸºäºŽåµŒå…¥çš„æ£€ç´¢ç³»ç»Ÿã€‚å®ƒçš„å·¥ä½œåŽŸç†å¦‚ä¸‹ï¼š\n\n### ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨åŠ¨æ€æç¤ºåˆå§‹åŒ–åˆ†ç±»å™¨\n\næˆ‘ä»¬çš„ `LLMTextClassifier` ç±»æŽ¥å—å¯èƒ½ç±»åˆ«çš„åˆ—è¡¨ï¼Œå¹¶æž„å»ºä¸€ä¸ªç”¨äºŽåˆ†ç±»çš„æç¤ºæ¨¡æ¿ã€‚æˆ‘ä»¬é…ç½®åˆ†ç±»å™¨ä»¥æ£€ç´¢ä¸ŽæŸ¥è¯¢æ–‡æœ¬æœ€ç›¸ä¼¼çš„ä¸€å®šæ•°é‡çš„ç¤ºä¾‹ï¼ˆç”± `max_examples` æŽ§åˆ¶ï¼‰ã€‚\n\nä½¿ç”¨æ­¤è®¾ç½®ï¼Œåˆ†ç±»å™¨åŠ¨æ€é€‰æ‹©ç¤ºä¾‹ï¼Œå°†å®ƒä»¬æ³¨å…¥æç¤ºä¸­ï¼Œæ ¼å¼ä¸Žå‰ä¸€ç§æ–¹æ³•ä¸­çš„å°‘é‡ç¤ºä¾‹ç›¸åŒï¼š\n\n```python\nclass LLMTextClassifier:\n    def __init__(\n        self,\n        categories: list[str],\n        system_prompt_template: PromptTemplate = PromptTemplate(\n            input_variables=[\"categories\", \"schema\"],\n            template=\"Classify the following text into one of the following classes: {categories}.\\n \"\n            \"Use the following schema: {schema}\",\n        ),\n        llm_client: BaseChatModel = llm_medium,\n        max_examples: int = 5,\n    ):\n        # Initialize model, prompt, and retrieval variables\n        self.categories = categories\n        self.categories_model = generate_classification_model(categories)\n        self.system_prompt_template = system_prompt_template\n        self.system_prompt = system_prompt_template.format(\n            categories=categories, schema=self.categories_model.model_json_schema()\n        )\n        self.llm_classifier = llm_client.with_structured_output(self.categories_model)\n        self.max_examples = max_examples\n        self.examples = None\n        self.vector_store = None\n        self.retriever = None\n```\n\n### ç¬¬2æ­¥ï¼šâ€œè®­ç»ƒâ€åˆ†ç±»å™¨ä¸Žç¤ºä¾‹æ•°æ®\n\nä¸ºäº†â€œè®­ç»ƒâ€æˆ‘ä»¬çš„åˆ†ç±»å™¨ï¼ˆè¿™é‡Œçš„è®­ç»ƒæ˜¯å®½æ³›çš„ï¼Œå› ä¸ºæ²¡æœ‰æ›´æ–°æƒé‡ï¼‰ï¼Œæˆ‘ä»¬ç”¨æ ‡è®°äº†å„è‡ªç±»åˆ«çš„è®­ç»ƒæ•°æ®ç¤ºä¾‹å¡«å……å‘é‡å­˜å‚¨ã€‚è¿™ä¸€è®¾ç½®ä¸ºåˆ†ç±»å™¨åœ¨è¾“å…¥æ–°æŸ¥è¯¢æ—¶åŠ¨æ€æ£€ç´¢æœ€ç›¸å…³çš„ç¤ºä¾‹åšå¥½å‡†å¤‡ï¼š\n\n```python\n    def fit(self, texts, labels):\n        self.examples = [\n            Document(page_content=text, metadata={\"label\": label})\n            for text, label in zip(texts, labels)\n        ]\n\n        if len(self.examples) > self.max_examples:\n            # Add examples to vector store\n            self.vector_store = Chroma.from_documents(\n                documents=self.examples,\n                collection_name=\"llm-classifier\",\n                embedding=ChromaEmbeddingsAdapter(\n                    embedding_functions.DefaultEmbeddingFunction()\n                ),\n            )\n            self.retriever = self.vector_store.as_retriever(\n                search_kwargs={\"k\": self.max_examples}\n            )\n```\n\n### ç¬¬3æ­¥ï¼šåŠ¨æ€æ£€ç´¢ç›¸å…³ç¤ºä¾‹å¹¶åˆ†ç±»\n\nå½“è¾“å…¥æ–°çš„æ–‡æœ¬è¿›è¡Œåˆ†ç±»æ—¶ï¼Œåˆ†ç±»å™¨æ ¹æ®ä¸ŽæŸ¥è¯¢çš„ç›¸ä¼¼æ€§æ£€ç´¢ç›¸å…³ç¤ºä¾‹ã€‚è¿™ä¸ªç›¸å…³ç¤ºä¾‹çš„åˆ—è¡¨è¢«æ·»åŠ åˆ°æç¤ºä¸­ï¼Œç´§æŽ¥ç€æ˜¯æŸ¥è¯¢æœ¬èº«ï¼Œç„¶åŽå‘é€ç»™LLMè¿›è¡Œåˆ†ç±»ï¼š\n\n```python\n def predict(self, text: str) -> str:\n        messages = [SystemMessage(content=self.system_prompt)]\n        \n        for example in self.fetch_examples(text=text):\n            messages.append(HumanMessage(content=example.page_content))\n            messages.append(AIMessage(content=example.metadata[\"label\"]))\n\n        messages.append(HumanMessage(content=text))\n        prediction = self.llm_classifier.invoke(messages)\n\n        return prediction.category\n```\n\n### ç¬¬4æ­¥ï¼šç¤ºä¾‹è¿è¡Œ\n\n\n```python\nif __name__ == \"__main__\":\n    categories = [\"news\", \"clickbait\"]\n    classifier = LLMTextClassifier(categories=categories, max_examples=1)\n\n    texts = [\"Donald Trump won Michigan\", \"You won't believe what happened next!\"]\n    labels = [\"news\", \"clickbait\"]\n    \n    classifier.fit(texts, labels)\n\n    text = \"Donald Trump won Florida\"\n    result = classifier.predict(text)\n    print(result)  # Should output \"news\" if similar to \"news\" examples\n```\nä½¿ç”¨åŠ¨æ€å°‘æ ·æœ¬æŠ€æœ¯ï¼Œæˆ‘ä»¬åœ¨åˆ†ç±»å‡†ç¡®çŽ‡ä¸Šçœ‹åˆ°äº†æ˜¾è‘—æé«˜ï¼Œè¾¾åˆ°äº† **88.6%**ã€‚è¿™æ¯”ä»¥å‰çš„æ–¹æ³•æœ‰äº†æ˜¾è‘—çš„å¢žåŠ ï¼Œå±•ç¤ºäº†æ ¹æ®ä¸ŽæŸ¥è¯¢æ–‡æœ¬çš„ç›¸ä¼¼æ€§åŠ¨æ€é€‰æ‹©ç›¸å…³ç¤ºä¾‹çš„å¼ºå¤§èƒ½åŠ›ã€‚\n\n## ç»“è®º\n\nåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æŽ¢è®¨äº†ä¸€ç§ç®€å•è€Œå¼ºå¤§çš„æ–¹æ³•ï¼Œé€šè¿‡ä½¿ç”¨å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰æž„å»ºå¯é ä¸”å‡†ç¡®çš„æ–‡æœ¬åˆ†ç±»ç®¡é“ã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸‰ç§å…³é”®æŠ€æœ¯ï¼š*å—é™ç”Ÿæˆ*ã€*å°‘é‡ç¤ºä¾‹æç¤º*å’Œ*åŠ¨æ€å°‘é‡ç¤ºä¾‹é€‰æ‹©*ã€‚è¿™äº›æ–¹æ³•å„è‡ªå¸¦æ¥äº†ç‹¬ç‰¹çš„ä¼˜åŠ¿ï¼Œä»¥æé«˜åˆ†ç±»çš„å‡†ç¡®æ€§å’Œå¯ç”¨æ€§ï¼Œä½¿LLMsæˆä¸ºæœ‰æ•ˆçš„æ–‡æœ¬åˆ†ç±»å·¥å…·ã€‚\n\nç¬¬ä¸€ç§æŠ€æœ¯ï¼Œå—é™ç”Ÿæˆï¼Œæ¶‰åŠå°†LLMçš„å“åº”é™åˆ¶åœ¨é¢„å®šä¹‰çš„ç±»åˆ«å†…ï¼Œå‡å°‘äº†å¤æ‚åŽå¤„ç†çš„éœ€æ±‚ï¼Œå¹¶ä½¿è§£æžæ¨¡åž‹è¾“å‡ºå˜å¾—æ›´å®¹æ˜“ã€‚ä»…å‡­è¿™ä¸€æ–¹æ³•ï¼Œæˆ‘ä»¬å°±èƒ½å¤Ÿé¿å…è‡ªç”±æ–‡æœ¬ç”Ÿæˆçš„å¸¸è§é™·é˜±ï¼Œæé«˜LLMåœ¨åˆ†ç±»ä¸­çš„ä¸€è‡´æ€§ã€‚\n\næŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®žæ–½äº†å°‘é‡ç¤ºä¾‹æç¤ºï¼Œæˆ‘ä»¬å‘LLMæä¾›äº†ä¸€äº›æ ‡è®°çš„ç¤ºä¾‹ä½œä¸ºæç¤ºçš„ä¸€éƒ¨åˆ†ã€‚é€šè¿‡åˆ©ç”¨æ¨¡åž‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œå°‘é‡ç¤ºä¾‹æç¤ºé€šè¿‡ä¸ºè¾“å‡ºæ ¼å¼å’Œå†…å®¹è®¾å®šæ˜Žç¡®çš„æœŸæœ›ï¼Œæé«˜äº†åˆ†ç±»å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘çŽ°ç¤ºä¾‹çš„é€‰æ‹©è‡³å…³é‡è¦â€”â€”éšæœºé€‰æ‹©çš„ç¤ºä¾‹ä»…æä¾›äº†é€‚åº¦çš„æ”¹å–„ã€‚è¿™ä½¿æˆ‘ä»¬è½¬å‘äº†æœ€åŽä¸€ç§æŠ€æœ¯ï¼šåŠ¨æ€å°‘é‡ç¤ºä¾‹é€‰æ‹©ã€‚\n\nåŠ¨æ€å°‘é‡ç¤ºä¾‹é€‰æ‹©æ˜¯æœ€å…ˆè¿›å’Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œè¾¾åˆ°äº†88.6%çš„é«˜åˆ†ç±»å‡†ç¡®çŽ‡ã€‚é€šè¿‡ä½¿ç”¨ChromaDBä¸ºæ¯ä¸ªæŸ¥è¯¢æ£€ç´¢æœ€ç›¸ä¼¼çš„ç¤ºä¾‹ï¼Œè¿™ç§æŠ€æœ¯ä½¿LLMä»…è®¿é—®æœ€ç›¸å…³çš„ä¸Šä¸‹æ–‡ï¼Œä»Žè€Œæ˜¾è‘—å¢žå¼ºäº†å…¶é¢„æµ‹å‡†ç¡®æ€§ã€‚è¿™ç§æ–¹æ³•æ˜¯ä¸€ç§å®žç”¨çš„æ–¹å¼ï¼Œä½¿åƒLLMsè¿™æ ·çš„é€šç”¨æ¨¡åž‹è¡¨çŽ°å¾—æ›´åƒä¸“ä¸šåˆ†ç±»å™¨ï¼Œè€Œæ— éœ€ä»Žå¤´å¼€å§‹è®­ç»ƒè‡ªå®šä¹‰æ¨¡åž‹ã€‚\n\n### æœ€åŽçš„æ€è€ƒ\n\néšç€LLMså˜å¾—è¶Šæ¥è¶Šå¯è®¿é—®å’Œå¼ºå¤§ï¼Œå®ƒä»¬åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­çš„åº”ç”¨ä¹Ÿä¸æ–­å¢žé•¿ã€‚è™½ç„¶è¿™äº›æ¨¡åž‹é€šå¸¸æ˜¯é€šç”¨çš„ï¼Œä½†æˆ‘ä»¬çš„æ•™ç¨‹å±•ç¤ºäº†é€šè¿‡æœ‰é’ˆå¯¹æ€§çš„æŠ€æœ¯ï¼Œå®ƒä»¬å¯ä»¥è¢«è°ƒæ•´ä¸ºé«˜æ€§èƒ½çš„åˆ†ç±»å™¨ã€‚æˆ‘ä»¬åœ¨è¿™é‡Œä»‹ç»çš„æ¯ç§æ–¹æ³•â€”â€”ä»Žç®€å•çš„çº¦æŸç”Ÿæˆåˆ°å…ˆè¿›çš„åŠ¨æ€å°‘é‡æ ·æœ¬é€‰æ‹©â€”â€”éƒ½æä¾›äº†çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚å®ƒä»¬ä¸ºæž„å»ºåˆ†ç±»ç³»ç»Ÿæä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œä½¿å¾—å°†LLMsé›†æˆåˆ°ç”Ÿäº§ä¸­å˜å¾—å¯è¡Œï¼Œè€Œæ— éœ€å¤§é‡çš„æ•°æ®æ”¶é›†æˆ–è®­ç»ƒã€‚\n\næ— è®ºæ‚¨æ˜¯NLPä»Žä¸šè€…ã€æ•°æ®ç§‘å­¦å®¶è¿˜æ˜¯AIçˆ±å¥½è€…ï¼Œè¿™äº›æŠ€æœ¯éƒ½ä¸ºæ‚¨çš„æœºå™¨å­¦ä¹ å·¥å…·åŒ…å¢žåŠ äº†å¤šåŠŸèƒ½çš„å·¥å…·ã€‚å€ŸåŠ©LLMså’Œè¿™äº›æŠ€æœ¯ï¼Œæ‚¨å¯ä»¥éƒ¨ç½²é’ˆå¯¹ç‰¹å®šéœ€æ±‚é‡èº«å®šåˆ¶çš„å¼ºå¤§æœ‰æ•ˆçš„æ–‡æœ¬åˆ†ç±»ç³»ç»Ÿã€‚\n\næ„Ÿè°¢æ‚¨çš„é˜…è¯»ï¼\n\nCode: <https://github.com/CVxTz/llmclassifier>\n\n"},{"lang":"zh","group":"blog","slug":"blog/building-autonomous-multi-agent-systems-with-crewai-1a3b3a348271","frontmatter":{"title":"åˆ©ç”¨ CrewAI æž„å»ºè‡ªä¸»å¤šä»£ç†ç³»ç»Ÿ","meta_title":"åˆ©ç”¨ CrewAI æž„å»ºè‡ªä¸»å¤šä»£ç†ç³»ç»Ÿ","description":"æœ¬æ–‡ä»‹ç»äº†å¦‚ä½•ä½¿ç”¨CrewAIå’ŒLangChainæž„å»ºè‡ªä¸»å¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€‚æ–‡ç« é¦–å…ˆé˜è¿°äº†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„æ¦‚å¿µï¼Œå¼ºè°ƒä»£ç†ã€å·¥å…·å’Œä»»åŠ¡çš„åä½œå…³ç³»ã€‚æŽ¥ç€ï¼Œè¯¦ç»†æè¿°äº†CrewAIæ¡†æž¶çš„ä¼˜åŠ¿å’Œé¡¹ç›®ç»“æž„ï¼ŒåŒ…æ‹¬å¦‚ä½•åˆ›å»ºä»£ç†ã€å®šä¹‰ä»»åŠ¡å’Œä½¿ç”¨å·¥å…·ã€‚é€šè¿‡ä¸€ä¸ªè®ºæ–‡å†™ä½œé¡¹ç›®ç¤ºä¾‹ï¼Œå±•ç¤ºäº†ä»£ç†å¦‚ä½•æ”¶é›†ä¿¡æ¯ã€æ’°å†™å’Œç¼–è¾‘å†…å®¹ã€‚æœ€åŽï¼Œä½¿ç”¨Streamlitæ¡†æž¶å°†åº”ç”¨ç¨‹åºéƒ¨ç½²ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿä¸Žç³»ç»Ÿè¿›è¡Œäº¤äº’ã€‚æ•´ä½“ä¸Šï¼Œæ–‡ç« å¼ºè°ƒäº†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨æé«˜ä»»åŠ¡æ•ˆçŽ‡å’Œåä½œæ–¹é¢çš„æ½œåŠ›ã€‚","date":"2024-11-14T03:29:09.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*72Cy_QqOie7G2NAiWr13Kw.jpeg","categories":["Autonomous Systems","Programming","Data Science"],"author":"Rifx.Online","tags":["CrewAI","LangChain","multi-agent","Streamlit","essay-writing"],"draft":false,"slug":"blog/building-autonomous-multi-agent-systems-with-crewai-1a3b3a348271"},"content":"\n### ä»€ä¹ˆæ˜¯å¤šæ™ºèƒ½ä½“è‡ªä¸»ç³»ç»Ÿä»¥åŠå¦‚ä½•ä½¿ç”¨CrewAIå’ŒLangChainæž„å»ºä¸€ä¸ªï¼Ÿ\n\n\n\n## åŠ¨æœº\n\nå®žé™…ä¸Šï¼Œæˆ‘ä»¬å¯¹è¿™äº›æ¦‚å¿µå¹¶ä¸é™Œç”Ÿï¼›æˆ‘ä»¬ä»Žç”µå½±ä¸­äº†è§£åˆ°å®ƒä»¬ã€‚ä¸€ä¸ªäººæŒ‡æŒ¥ä»–ä»¬çš„AIï¼Œè€ŒAIé€šè¿‡ä½¿ç”¨å„ç§å·¥å…·æ¥æ‰§è¡Œè¿™äº›å‘½ä»¤ã€‚è¿™å°±æ˜¯æˆ‘ä»¬ä»Šå¤©åœ¨AIç³»ç»Ÿå´›èµ·çš„é“è·¯ä¸Šæ‰€èµ°çš„æ–¹å‘ã€‚æ—¶ä»£æ­£åœ¨é€æ¸å˜åŒ–ã€‚åœ¨è¿‡åŽ»ï¼Œäººä»¬æ— æ³•ç‹¬è‡ªå®Œæˆä¸€é¡¹ä»»åŠ¡ï¼Œéœ€è¦ä¸€ä¸ªå›¢é˜Ÿã€‚æ²¡æœ‰å›¢é˜Ÿï¼Œä»–ä»¬è¦ä¹ˆåœ¨ä¸€æ®µæ—¶é—´åŽç²¾ç–²åŠ›ç«­ï¼Œè¦ä¹ˆè¾¾åˆ°èƒ½åŠ›çš„æžé™ã€‚æœ€ç»ˆï¼ŒæˆåŠŸçš„é¡¹ç›®æ¥è‡ªäºŽç”±å…·æœ‰ä¸åŒæŠ€èƒ½çš„ä¸ªäººç»„æˆçš„å›¢é˜Ÿã€‚\n\n> å›¢é˜Ÿåˆä½œä½¿æ¢¦æƒ³æˆçœŸã€‚\n\nç„¶è€Œï¼Œå¦‚ä»Šä¸€ç§æ–°æŠ€æœ¯å¼€å§‹å´­éœ²å¤´è§’ã€‚æˆ‘ä»¬å¯ä»¥ç§°ä¹‹ä¸ºAGIä¹‹å‰çš„AIä¸‹ä¸€ä¸ªé˜¶æ®µï¼šâ€œä»£ç†â€ã€‚é‚£ä¹ˆï¼Œè¿™äº›ä»£ç†æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿåœ¨æ·±å…¥ä»£ç ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆè°ˆè°ˆå¤šä»£ç†ç³»ç»Ÿçš„ç»“æž„ã€‚\n\n## å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ\n\nç®€å•æ¥è¯´ï¼Œè¿™ä¸ªæ–¹ç¨‹å¼å¯ä»¥è¡¨ç¤ºä¸ºï¼š`Multi Agent Systems = AGENTs + TOOLs + TASKs` è¿™æ˜¯ä¸€ä¸ªå¤šä¸ªä»£ç†é…å¤‡äº†å„ç§ä»»åŠ¡å’Œå·¥å…·çš„ç³»ç»Ÿã€‚\n\n### ä»£ç†\n\næˆ‘ä»¬ç†Ÿæ‚‰è§’è‰²æ‰®æ¼”æ¸¸æˆï¼Œåœ¨è¿™äº›æ¸¸æˆä¸­ï¼Œä½ çš„è§’è‰²æœ‰ä¸€ä¸ªè§’è‰²ï¼Œæ¯”å¦‚æˆ˜å£«ã€‚ä¾‹å¦‚ã€‚åœ¨æ¸¸æˆä¸­ï¼Œä½ å°†è‡ªå·±ç½®äºŽä»–ä»¬çš„ä½ç½®ï¼Œæ—¨åœ¨é€šè¿‡å®Œæˆå¡‘é€ ä»–ä»¬èƒŒæ™¯æ•…äº‹çš„ä»»åŠ¡ï¼Œä»Žä¸€æ¬¡å†’é™©åˆ°ä¸‹ä¸€æ¬¡å†’é™©æ¥å®Œæˆæ¸¸æˆã€‚ç±»ä¼¼åœ°ï¼Œç ”ç©¶äººå‘˜å‘çŽ°ï¼Œå½“ç»™å¤§åž‹è¯­è¨€æ¨¡åž‹ (LLMs) è§’è‰²ã€èƒŒæ™¯æ•…äº‹å’Œç›®æ ‡æ—¶ï¼Œå®ƒä»¬å¯ä»¥è¢«æ¿€åŠ±ä»¥æœ€ä½³æ–¹å¼æ‰§è¡Œä»»åŠ¡ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿé€šè¿‡å‡ ä¸ªç®€å•çš„æç¤ºæ¥æ¿€åŠ± LLM æ‰§è¡Œå„ç§ä»»åŠ¡ã€‚\n\nä»£ç†æœ¬è´¨ä¸Šå°†åˆ†é…çš„ä»»åŠ¡åˆ†è§£ä¸ºç®€å•çš„æ­¥éª¤ï¼Œç„¶åŽé€šè¿‡â€œæ€è€ƒâ€â€”â€”æ˜¯çš„ï¼Œæ€è€ƒâ€”â€”æŒ‰é¡ºåºæ‰§è¡Œè¿™äº›æ­¥éª¤ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿåˆ›å»ºä¸€ä¸ªä¸ä»…èƒ½æ·±æ€ç†Ÿè™‘åœ°æ‰§è¡Œæ­¥éª¤çš„ä»£ç†ï¼Œè¿˜èƒ½å’¨è¯¢å…¶ä»–å…·æœ‰ä¸åŒä¸“ä¸šé¢†åŸŸçš„ä»£ç†ï¼Œè€Œä¸æ˜¯ä¾èµ–å•ä¸ª LLM è¾“å…¥æç¤ºå¹¶æŽ¥æ”¶è¾“å‡ºã€‚\n\n### å·¥å…·\n\näººç±»æœ€ä¼Ÿå¤§çš„èƒ½åŠ›ä¹‹ä¸€æ— ç–‘æ˜¯æˆ‘ä»¬ä½¿ç”¨å·¥å…·çš„æŠ€èƒ½ã€‚è¿™ç§èƒ½åŠ›é€šè¿‡è¿›åŒ–å’Œæ–‡åŒ–è¿‡ç¨‹ä¸æ–­æ¼”å˜å’Œå‘å±•ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿåˆ›é€ å‡ºä»Šå¤©æ‰€ä½¿ç”¨çš„å…ˆè¿›æŠ€æœ¯ã€‚åŒæ ·ï¼Œå¤§åž‹è¯­è¨€æ¨¡åž‹éšç€è®­ç»ƒåœ¨æ›´å¤§æ•°æ®é›†ä¸Šçš„èƒ½åŠ›ä¹Ÿåœ¨ä¸æ–­å¢žå¼ºã€‚çŽ°åœ¨ï¼Œå½“å·¥å…·çš„åŠŸèƒ½åŠå…¶ä½¿ç”¨æ–¹å¼è¢«æ¸…æ™°è§£é‡Šæ—¶ï¼Œè¿™äº›æ¨¡åž‹èƒ½å¤Ÿåœ¨é€‚å½“æ¡ä»¶ä¸‹è‡ªä¸»ä½¿ç”¨å·¥å…·ï¼Œå®Œå…¨è‡ªåŠ¨æ‰§è¡Œï¼Œå¹¶æ ¹æ®è¾“å‡ºè§„åˆ’ä¸‹ä¸€æ­¥ï¼Œè€Œæ— éœ€ç­‰å¾…è¿›ä¸€æ­¥çš„å‘½ä»¤ã€‚\n\nå› æ­¤ï¼Œå·¥å…·çš„ä½¿ç”¨ä¹Ÿå¯ä»¥è¢«è§†ä¸ºå®ƒä»¬è¿›åŒ–ä¸­æœ€é‡è¦çš„éƒ¨åˆ†ä¹‹ä¸€ã€‚å°¤å…¶æ˜¯é€šè¿‡äº’è”ç½‘æµè§ˆå·¥å…·ï¼Œä»£ç†å¯ä»¥æŒ‰ç…§æŒ‡å®šåŠŸèƒ½çš„æ­¥éª¤è®¿é—®å¿…è¦çš„èµ„æºï¼Œæ— è®ºæ˜¯é€šè¿‡ç½‘ç»œçˆ¬è™«è¿˜æ˜¯ä½¿ç”¨æŒ‡å®šç½‘ç«™çš„æœç´¢å¼•æ“Žã€‚\n\næ‚¨å·¥å…·çš„åŠŸèƒ½å’Œç›®çš„å®Œå…¨å–å†³äºŽæ‚¨çš„æƒ³è±¡åŠ›ã€‚ç„¶è€Œï¼Œå¦‚æžœæ‚¨å¸Œæœ›å°†é¢„æž„å»ºçš„å·¥å…·é›†æˆåˆ°æ‚¨çš„ä»£ç†ä¸­ï¼ŒCrewAI å’Œ LangChain åº“éƒ½æä¾›äº†å¹¿æ³›çš„å†…ç½®å·¥å…·ä¾›æ‚¨ä½¿ç”¨ã€‚åœ¨è¿™ä¸ªé¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬å°†é‡ç‚¹åˆ›å»ºæˆ‘ä»¬è‡ªå·±çš„è‡ªå®šä¹‰å·¥å…·ã€‚\n\n### ä»»åŠ¡\n\nå°±åƒæˆ‘ä»¬åˆ›å»ºä»£ç†ä¸€æ ·ï¼Œæˆ‘ä»¬ä¹Ÿåˆ›å»ºä»»åŠ¡ï¼Œæ¯ä¸ªä»»åŠ¡éƒ½éœ€è¦å„ç§å·¥å…·ã€‚ä¸¾ä¸€ä¸ªäººç±»è¡Œä¸ºçš„ä¾‹å­ï¼Œå½“æˆ‘ä»¬éœ€è¦ç ”ç©¶æŸä¸ªäº‹æƒ…æ—¶ï¼Œæˆ‘ä»¬ä¼šåšä»€ä¹ˆï¼Ÿ\n\n1\\- æˆ‘ä»¬åœ¨äº’è”ç½‘ä¸Šæœç´¢ã€‚\n\n2\\- æˆ‘ä»¬è¿›è¡Œæ·±å…¥çš„æ¥æºç ”ç©¶ã€‚\n\n3\\- æˆ‘ä»¬å¯¹æˆ‘ä»¬çš„å‘çŽ°è¿›è¡Œç¬”è®°ã€‚\n\nä»¥åŒæ ·çš„æ–¹å¼ï¼Œæˆ‘ä»¬å¯ä»¥è®¾è®¡ä»»åŠ¡æ¥éµå¾ªè¿™äº›æ­¥éª¤ï¼Œæˆ‘ä»¬å°†é€šè¿‡ä»£ç è®¨è®ºå®ƒä»¬æ˜¯å¦‚ä½•è®¾è®¡çš„ã€‚\n\n## ä»€ä¹ˆæ˜¯ CrewAIï¼Ÿ\n\nCrewAI æ˜¯ä¸€ä¸ªå¼€æºçš„ Python æ¡†æž¶ï¼Œç”¨äºŽåè°ƒè§’è‰²æ‰®æ¼”çš„è‡ªä¸» AI ä»£ç†ï¼Œå…·æœ‰ Crewã€Taskã€Agentã€Process ç­‰æ–¹æ³•ï¼Œå¹¶æ”¯æŒå¤šç§ LLMï¼ŒåŒ…æ‹¬æœ¬åœ°æ¨¡åž‹ã€‚\n\nå¦‚æžœæˆ‘ä»¬çœ‹çœ‹è¯¥æ¡†æž¶æä¾›çš„ä¸»è¦ä¼˜åŠ¿ï¼š\n\n* åŸºäºŽè§’è‰²çš„ä»£ç†è®¾è®¡ã€‚\n* è‡ªä¸»çš„ä»£ç†é—´å§”æ´¾ã€‚\n* çµæ´»çš„ä»»åŠ¡ç®¡ç†ã€‚\n* åŸºäºŽæµç¨‹çš„æ‰§è¡Œã€‚\n* è¾“å‡ºä¿å­˜ä¸º .markdown æ–‡ä»¶ç­‰æ ¼å¼ã€‚\n* ä¸Žå¼€æºå’Œä¸“æœ‰æ¨¡åž‹ï¼ˆå¦‚ OpenAIï¼‰å…¼å®¹ã€‚\n\n## æž„å»ºå¤šæ™ºèƒ½ä½“\n\nä»…ä»…é€šè¿‡æè¿°æ€§çš„è§£é‡Šå¯èƒ½ä¸è¶³ä»¥å®Œå…¨ç†è§£ä¸€ä¸ªæ¦‚å¿µï¼Œå› æ­¤è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå°åž‹çš„è®ºæ–‡å†™ä½œé¡¹ç›®ï¼Œä»¥æ›´å¥½åœ°æŽŒæ¡å¤šæ™ºèƒ½ä½“æ–¹æ³•ã€‚åœ¨è¿™ä¸ªé¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬å°†ç»“åˆ LangChain å’Œ CrewAI æ¡†æž¶ã€‚è¦è¿è¡Œè¯¥é¡¹ç›®ï¼Œæ‚¨éœ€è¦ä¸€ä¸ª OpenAI API å¯†é’¥ï¼Œæ‚¨å¯ä»¥é€šè¿‡è®¿é—® [https://proxy.rifx.online/https://platform.openai.com/signup](https://proxy.rifx.online/https://platform.openai.com/signup) æ¥èŽ·å–ã€‚\n\næˆ‘ä»¬é¡¹ç›®çš„ç»“æž„ç”±å‡ ä¸ªä¸åŒçš„ Python è„šæœ¬ç»„æˆï¼š\n\n* `crew.py`ï¼Œåœ¨è¿™é‡Œæˆ‘ä»¬å®šä¹‰æˆ‘ä»¬çš„æ™ºèƒ½ä½“åŠå…¶ä»»åŠ¡ã€‚\n* `graph.py`ï¼Œæž„å»º LangGraph ç»“æž„ã€‚\n* `extra_tools.py`ï¼ŒåŒ…å«æˆ‘ä»¬çš„æ™ºèƒ½ä½“å°†ä½¿ç”¨çš„å·¥å…·ã€‚\n* `pdf_writer.py`ï¼Œè´Ÿè´£å°†è®ºæ–‡è½¬æ¢ä¸º PDFã€‚\n* `app.py`ï¼Œä¸ºæˆ‘ä»¬çš„åº”ç”¨ç¨‹åºæä¾› Streamlit ç•Œé¢ã€‚\n\n```python\n## é¡¹ç›®ç»“æž„\nAutonomous-Multi-Agent-Systems-with-CrewAI-Essay-Writer\nâ”œâ”€â”€ app.py              # ä¸»è¦çš„ streamlit åº”ç”¨ç¨‹åº\nâ”œâ”€â”€ crew.py             # CrewAI æ™ºèƒ½ä½“å’Œä»»åŠ¡å¤„ç†\nâ”œâ”€â”€ extra_tools.py      # æ™ºèƒ½ä½“å·¥å…·çš„åŠŸèƒ½\nâ”œâ”€â”€ graph.py            # LangGraph å’Œé¡¹ç›®å·¥ä½œæµç¨‹\nâ”œâ”€â”€ pdf_writer.py       # å¤„ç† PDF è¾“å‡ºç”Ÿæˆ\nâ”œâ”€â”€ requirements.txt    # æ‰€éœ€åº“åˆ—è¡¨\nâ”œâ”€â”€ media\nâ”‚   â””â”€â”€ cover.jpg       # é¡¹ç›®å°é¢å›¾åƒ\nâ””â”€â”€ README.md        \n```\n\nè¯¥é¡¹ç›®æ‰€éœ€çš„åº“åˆ—åœ¨ `requirements.txt` æ–‡ä»¶ä¸­ã€‚æ­¤å¤–ï¼Œè¯·ç¡®ä¿æ‚¨å·²å®‰è£… Python 3\\.12 æˆ–æ›´é«˜ç‰ˆæœ¬ã€‚åœ¨è¿è¡Œé¡¹ç›®ä¹‹å‰ï¼Œè¯·ä¸è¦å¿˜è®°å®‰è£…ä¾èµ–é¡¹ã€‚æˆ‘ä»¬ä½¿ç”¨çš„åº“åŒ…æ‹¬ï¼š\n\n```python\nlangchain-core\nlangchain-openai\nlanggraph\nstreamlit\nwikipedia\nreportlab\ncrewai[tools]\npysqlite3-binary\nbs4\n```\n\n### å·¥ä½œæµç¨‹\n\nåœ¨æˆ‘ä»¬çš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä¸ºä»£ç†åˆ†é…å„ç§è§’è‰²ã€‚ä¾‹å¦‚ï¼Œå½“ä¸€ä¸ªä»£ç†ç­‰å¾…å¦ä¸€ä¸ªä»£ç†å®Œæˆå…¶åœ¨äº’è”ç½‘ä¸Šç ”ç©¶çš„ä»»åŠ¡æ—¶ï¼Œå¦ä¸€ä¸ªä»£ç†å°†ç‹¬ç«‹è¿›è¡Œç»´åŸºç™¾ç§‘çš„ç ”ç©¶ã€‚ä¸€æ—¦ä¸¤ä¸ªä»£ç†éƒ½å®Œæˆäº†ä»–ä»¬çš„ä»»åŠ¡ï¼Œç­‰å¾…ä¿¡æ¯çš„ä»£ç†å°†ç»§ç»­è¿›è¡Œå†™ä½œï¼Œè¿™æ˜¯ä»–ä»¬è¢«åˆ†é…çš„ä»»åŠ¡ã€‚\n\nå¦‚æžœæˆ‘ä»¬è¦å°†å…¶å¯è§†åŒ–ï¼š\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Emb37H_8OAKp1s1ChLLVQg.png)\n\n* ç”¨æˆ·æŸ¥è¯¢æœ€åˆå‘é€åˆ°è·¯ç”±å™¨ã€‚\n* è·¯ç”±å™¨è¯»å–æŸ¥è¯¢å¹¶ç¡®å®šç”¨æˆ·æ˜¯æƒ³å†™ä¸€ç¯‡æ–°æ–‡ç« ã€ç¼–è¾‘ä¹‹å‰çš„æ–‡ç« ï¼Œè¿˜æ˜¯ä»…ä»…ä¼ è¾¾ä¸€ä¸ªè®¨è®ºä¸»é¢˜ã€‚å¦‚æžœç”¨æˆ·å¸Œæœ›å†™ä¸€ç¯‡æ–°æ–‡ç« ï¼Œè¯·æ±‚å°†è½¬å‘ç»™å°ç»„ã€‚\n* å‘é€åˆ°å°ç»„çš„è¯·æ±‚é¦–å…ˆå‘é€ç»™ç ”ç©¶ä»£ç†ã€‚\n* ç ”ç©¶ä»£ç†ä½¿ç”¨åˆ†é…ç»™ä»–çš„å·¥å…·æœç´¢ä¸Žç”¨æˆ·æƒ³è¦å†™çš„ä¸»é¢˜ç›¸å…³çš„äº’è”ç½‘èµ„æºã€‚\n* ä¸€æ—¦èµ„æºæ”¶é›†è¿‡ç¨‹å®Œæˆï¼Œæ”¶é›†åˆ°çš„ä¿¡æ¯å°†è½¬å‘ç»™å†™ä½œä»£ç†ã€‚\n* å½“å†™ä½œä»£ç†èµ·è‰æ–‡ç« æ—¶ï¼Œç¼–è¾‘ä»£ç†è¿›è¡Œæœ€ç»ˆè°ƒæ•´ï¼Œçº æ­£è¯­æ³•é”™è¯¯ï¼Œå¹¶å°†è‰ç¨¿ä½œä¸ºJSONæ–‡ä»¶è¿”å›žç»™LangGraphã€‚\n* JSONæ–‡ä»¶å°†å‘é€åˆ°å°†åœ¨æœ€ç»ˆèŠ‚ç‚¹åˆ›å»ºæˆ‘ä»¬æ–‡ç« çš„PDFæ–‡ä»¶çš„åŠŸèƒ½ã€‚\n\n### æž„å»º LangGraph\n\né¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å»ºç«‹æˆ‘ä»¬æž¶æž„çš„æ¡†æž¶ã€‚ä¸€æ—¦æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªå·¥ä½œæµç¨‹ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨éœ€è¦æ—¶ä¸Žæˆ‘ä»¬çš„ä»£ç†è¿›è¡Œè”ç³»ï¼Œå‰©ä¸‹çš„å°±æ˜¯å†³å®šåœ¨å·¥ä½œæµç¨‹çš„å“ªäº›é˜¶æ®µæˆ‘ä»¬å°†å‘æˆ‘ä»¬çš„ä»£ç†å‘é€è¯·æ±‚ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†é¦–å…ˆä½¿ç”¨ LangChain åˆ›å»ºä¸€ä¸ªç®€å•çš„å·¥ä½œæµç¨‹ã€‚\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*iHQzJymxAstrW40THoRxwA.png)\n\n```python\n#LangGraph workflow\n\nbuilder = StateGraph(GraphState)\n\nbuilder.add_node(\"answer\", self.answer)\nbuilder.add_node(\"write_essay\", self.write_essay)\nbuilder.add_node(\"edit_essay\", self.edit_essay)\n\n\nbuilder.set_conditional_entry_point(self.router_query,\n                              {\"write_essay\": \"write_essay\",\n                                        \"answer\": \"answer\",\n                                        \"edit_essay\": \"edit_essay\"})\nbuilder.add_edge(\"write_essay\", END)\nbuilder.add_edge(\"edit_essay\", END)\nbuilder.add_edge(\"answer\", END)\n\nself.graph = builder.compile()\n```\n\n**è·¯ç”±èŠ‚ç‚¹**ï¼šæ­£å¦‚æˆ‘ä»¬åœ¨å·¥ä½œæµç¨‹æè¿°ä¸­æåˆ°çš„ï¼Œæˆ‘ä»¬çš„è·¯ç”±å™¨æ ¹æ®ä¼ å…¥è¯·æ±‚å°†ä»»åŠ¡åˆ†é…ç»™å„ä¸ªèŠ‚ç‚¹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ªæœ‰æ•ˆçš„æç¤ºï¼Œæ¶µç›–ç”¨æˆ·æä¾›çš„ä¸»é¢˜å¹¶ç»“åˆè¿‡åŽ»çš„å¯¹è¯ã€‚æ¯•ç«Ÿï¼Œæˆ‘ä»¬æ­£åœ¨å¼€å‘ä¸€ä¸ªå¤šä»£ç†çš„ä½œæ–‡å†™ä½œèŠå¤©æœºå™¨äººï¼Œå®ƒå¯ä»¥è®°ä½å¹¶å›žå¿†ä¹‹å‰çš„è®¨è®ºã€‚\n\nè®©æˆ‘ä»¬èµ·è‰ä¸€ä¸ªç®€å•çš„æç¤ºå’Œç›¸åº”çš„èŠ‚ç‚¹æ¥åˆ©ç”¨è¿™ä¸ªæç¤ºã€‚åœ¨æç¤ºä¸­ï¼Œæˆ‘ä»¬åº”è¯¥ä½¿ç”¨ Pydantic åº“å®šä¹‰ä¸€ä¸ª `BaseModel`ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬çš„è·¯ç”±å™¨é€‰æ‹©ä¸‰ç§æ½œåœ¨å“åº”ç­–ç•¥ä¸­çš„ä¸€ç§ã€‚è¿™äº›ç­–ç•¥å°†æŒ‡å¯¼èŠå¤©æœºå™¨äººæœ‰æ•ˆåœ°åˆ¶å®šå…¶å“åº”ã€‚\n\nåœ¨èŠ‚ç‚¹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ Langchain çš„ `PromptTemplate` æ–¹æ³•å®žçŽ°è¿™ä¸ªæç¤ºã€‚ç„¶åŽï¼Œæˆ‘ä»¬å°†è°ƒç”¨ LLMï¼ˆå¤§åž‹è¯­è¨€æ¨¡åž‹ï¼‰ï¼Œå°†ç”¨æˆ·æŸ¥è¯¢å’Œå¯¹è¯åŽ†å²ä¸€èµ·ä¼ å…¥ï¼Œä»¥ç¡®ä¿å“åº”åœ¨ä¸Šä¸‹æ–‡ä¸Šç›¸å…³å¹¶ç¬¦åˆç”¨æˆ·çš„éœ€æ±‚ã€‚\n\n1. **å®šä¹‰ Pydantic æ¨¡åž‹**ï¼šåˆ›å»ºä¸€ä¸ªæ¨¡åž‹ï¼ŒæŒ‡å®šæ‰€éœ€çš„å“åº”ç­–ç•¥ã€‚\n2. **æž„å»ºæç¤º**ï¼šç¼–å†™ä¸€ä¸ªæ¸…æ™°æ¦‚è¿°ä¸‰ç§ç­–ç•¥çš„æç¤ºã€‚\n3. **è®¾ç½®èŠ‚ç‚¹**ï¼šä½¿ç”¨ Langchain çš„ `PromptTemplate` åŠ¨æ€æ ¼å¼åŒ–æç¤ºã€‚\n4. **è°ƒç”¨ LLM**ï¼šä½¿ç”¨æ ¼å¼åŒ–çš„æç¤ºã€ç”¨æˆ·æŸ¥è¯¢å’Œå¯¹è¯åŽ†å²è°ƒç”¨ LLMã€‚\n\né€šè¿‡éµå¾ªè¿™äº›æ­¥éª¤ï¼Œæˆ‘ä»¬å¯ä»¥ç¡®ä¿èŠå¤©æœºå™¨äººå‡†ç¡®å“åº”å¹¶ä¿æŒä¹‹å‰äº’åŠ¨çš„ä¸Šä¸‹æ–‡ã€‚\n\n```python\n#Router Prompt and Router Node\nclass RouteQuery(BaseModel):\n    \"\"\"å°†ç”¨æˆ·æŸ¥è¯¢è·¯ç”±åˆ°ç›´æŽ¥å›žç­”æˆ–ç ”ç©¶ã€‚\"\"\"\n\n    way: Literal[\"edit_essay\",\"write_essay\", \"answer\"] = Field(\n        ...,\n        description=\"æ ¹æ®ç”¨æˆ·é—®é¢˜é€‰æ‹©å°†å…¶è·¯ç”±åˆ° write_essayã€edit_essay æˆ– answer\",\n    )\n\nself.router_prompt = \n    \"\"\"\n    ä½ æ˜¯ä¸€ä¸ªè·¯ç”±å™¨ï¼Œä½ çš„èŒè´£æ˜¯å°†ç”¨æˆ·å¼•å¯¼åˆ°æ­£ç¡®çš„ä¸“å®¶ã€‚\n    å§‹ç»ˆæ£€æŸ¥å¯¹è¯åŽ†å²ï¼Œå¹¶æ ¹æ®å…¶è€ƒè™‘ä½ çš„è¡ŒåŠ¨ã€‚\n    å¦‚æžœä¸»é¢˜æ˜¯å…³äºŽè®°å¿†æˆ–æ—¥å¸¸è°ˆè¯ï¼Œå°†ç”¨æˆ·å¼•å¯¼åˆ°å›žç­”ä¸“å®¶ã€‚\n    å¦‚æžœä¸»é¢˜ä»¥â€œä½ èƒ½å†™...â€å¼€å¤´ï¼Œæˆ–è€…ç”¨æˆ·è¯·æ±‚ä½ å†™ä¸€ç¯‡æ–‡ç« æˆ–è®ºæ–‡ï¼Œå°†ç”¨æˆ·å¼•å¯¼åˆ°å†™ä½œä¸“å®¶ã€‚\n    å¦‚æžœä¸»é¢˜æ˜¯ç”¨æˆ·æƒ³è¦ç¼–è¾‘è®ºæ–‡ä¸­çš„ä»»ä½•å†…å®¹ï¼Œå°†ç”¨æˆ·å¼•å¯¼åˆ°ç¼–è¾‘ä¸“å®¶ã€‚\n  \n    \\nå¯¹è¯åŽ†å²: {memory}\n    \\nä¸»é¢˜: {topic}\n    \"\"\"\n\ndef router_query(self, state: GraphState):\n    print(\"**ROUTER**\")\n    prompt = PromptTemplate.from_template(self.router_prompt)\n    memory = self.memory.load_memory_variables({})\n\n    router_query = self.model.with_structured_output(RouteQuery)\n    chain = prompt | router_query\n    result:  RouteQuery = chain.invoke({\"topic\": state[\"topic\"],\n                                       \"memory\": memory})\n\n    print(\"Router Result: \", result.way)\n    return result.way\n```\n\n**ç®€å•å›žç­”èŠ‚ç‚¹**ï¼šåœ¨å°†æˆ‘ä»¬çš„è·¯ç”±å™¨ä½œä¸ºå¼€å§‹éƒ¨åˆ†çš„èŠ‚ç‚¹åŽï¼Œä¸‹ä¸€æ­¥æ˜¯åˆ›å»ºå…¶ä»–ä¸‰ä¸ªèŠ‚ç‚¹ï¼š`write_essay`ã€`edit_essay` å’Œ `answer`ã€‚ä¸ºäº†é‡‡å–ç®€å•çš„æ–¹å¼ï¼Œæˆ‘ä»¬éœ€è¦ç¼–ç¨‹æˆ‘ä»¬çš„ `answer` èŠ‚ç‚¹ï¼Œä»¥ä¾¿åœ¨ç”¨æˆ·å‘é€éšæ„æ¶ˆæ¯æˆ–å‚ä¸Žæœ‰å…³è®ºæ–‡çš„å¯¹è¯æ—¶ç›´æŽ¥ä½¿ç”¨å…¶è®°å¿†ç”Ÿæˆå“åº”ã€‚\n\nä¸ºæ­¤ï¼Œæˆ‘ä»¬å¿…é¡»é¦–å…ˆä¸ºæ­¤ä»»åŠ¡ç¼–å†™ä¸€ä¸ªåˆé€‚çš„æç¤ºã€‚ç„¶åŽï¼Œåˆ©ç”¨è¿™ä¸ªæç¤ºï¼Œæˆ‘ä»¬å°†è®¾è®¡ä¸€ä¸ªç®€å•çš„èŠ‚ç‚¹ã€‚è®©æˆ‘ä»¬ç»§ç»­è¿™ä¸ªè®¾è®¡ã€‚\n\n```python\n#Simple Answer Prompt and Node\n\nself.simple_answer_prompt = \n      \"\"\"\n      ä½ æ˜¯ä¸€ä¸ªä¸“å®¶ï¼Œä½ æ­£åœ¨ä¸ºç”¨æˆ·çš„é—®é¢˜æä¾›ç®€å•çš„ \n      ç­”æ¡ˆã€‚\n    \n      \\nå¯¹è¯åŽ†å²: {memory}\n      \\nä¸»é¢˜: {topic}\n      \"\"\"\ndef answer(self, state: GraphState):\n    print(\"**ANSWER**\")\n    prompt = PromptTemplate.from_template(self.simple_answer_prompt)\n    memory = self.memory.load_memory_variables({})\n    chain = prompt | self.model | StrOutputParser()\n    result = chain.invoke({\"topic\": state[\"topic\"], \"memory\": memory})\n\n    self.memory.save_context(inputs={\"input\": state[\"topic\"]}, outputs={\"output\": result})\n    return {\"response\": result}\n```\n\n**å†™ä½œèŠ‚ç‚¹**ï¼šæŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦è®¾è®¡ `writing_essay` èŠ‚ç‚¹ã€‚è¯¥èŠ‚ç‚¹çš„ç›®çš„æ˜¯ä½¿ç”¨ CrewAI çš„ `kickoff` æ–¹æ³•å°†ç”¨æˆ·æ”¶åˆ°çš„æŸ¥è¯¢è½¬å‘ç»™æˆ‘ä»¬çš„ä»£ç†ï¼Œç„¶åŽå°†ä»£ç†è¿”å›žçš„ JSON æ–‡ä»¶è½¬æ¢ä¸º PDFã€‚è‡ªç„¶ï¼Œæˆ‘ä»¬ä¸éœ€è¦ä¸ºè¿™ä¸ªèŠ‚ç‚¹ç¼–å†™æç¤ºï¼Œå› ä¸ºæç¤ºå°†åœ¨ä»£ç†åˆ›å»ºé˜¶æ®µå®šä¹‰ã€‚è¿™ä¸ªèŠ‚ç‚¹å°†ä»…ç”¨äºŽè°ƒç”¨ä»£ç†å’Œåˆ©ç”¨è¿”å›žçš„å€¼ã€‚\n\n1. **è°ƒç”¨ä»£ç†**ï¼šä½¿ç”¨ CrewAI çš„ `kickoff` æ–¹æ³•å°†ç”¨æˆ·çš„æŸ¥è¯¢å‘é€ç»™ä»£ç†ã€‚\n2. **å¤„ç†è¿”å›žçš„ JSON**ï¼šå¤„ç†ä»Žä»£ç†æ”¶åˆ°çš„ JSON å“åº”ã€‚\n3. **è½¬æ¢ä¸º PDF**ï¼šå°† JSON ä¸­çš„ç›¸å…³æ•°æ®è½¬æ¢ä¸º PDF æ ¼å¼ã€‚\n\n```python\n#Write Essay Node\ndef write_essay(self, state: GraphState):\n    print(\"**ESSAY COMPLETION**\")\n\n    self.essay = self.crew.kickoff({\"topic\": state[\"topic\"]})\n\n    self.memory.save_context(inputs={\"input\": state[\"topic\"]},\n                           outputs={\"output\": str(self.essay)})\n\n    pdf_name = generate_pdf(self.essay)\n    return {\"response\": \"è¿™æ˜¯ä½ çš„è®ºæ–‡ï¼\",  \"pdf_name\": f\"{pdf_name}\"}\n```\n\n**ç¼–è¾‘è®ºæ–‡èŠ‚ç‚¹**ï¼šè®©æˆ‘ä»¬ç®€è¦è®¨è®ºæˆ‘ä»¬çš„æœ€åŽä¸€ä¸ªèŠ‚ç‚¹ `edit_essay`ã€‚ä»£ç å¯èƒ½çœ‹èµ·æ¥æœ‰ç‚¹å†—é•¿ï¼Œå› ä¸ºæç¤ºè¢«ä¿ç•™åœ¨èŠ‚ç‚¹å†…ã€‚å¦‚æžœä½ æ„¿æ„ï¼Œä¹Ÿå¯ä»¥åœ¨ç±»å®šä¹‰æœŸé—´ç¼–å†™æç¤ºå¹¶å°†å…¶åˆ†é…ä¸ºå˜é‡ã€‚\n\nå½“è·¯ç”±å™¨æ£€æµ‹åˆ°ç”¨æˆ·çš„ä»»ä½•è®ºæ–‡ä¿®æ”¹è¯·æ±‚æ—¶ï¼Œå°†æ¿€æ´»è¯¥èŠ‚ç‚¹ã€‚åœ¨æ­¤èŠ‚ç‚¹ä¸­ï¼Œæˆ‘ä»¬éœ€è¦ä¸‰ä¸ªé‡è¦å€¼ï¼šå¯¹è¯åŽ†å²ã€ç”¨æˆ·è¯·æ±‚å’Œæœ€è¿‘ç”Ÿæˆçš„è®ºæ–‡ã€‚æ­¤å¤–ï¼Œæç¤ºä¸­æœ‰ä¸€ä¸ªå˜é‡ï¼ŒLangchain å°†ç”Ÿæˆï¼Œç§°ä¸º `format_instructions`ã€‚è¿™ä¸ªå˜é‡ä½¿æˆ‘ä»¬èƒ½å¤Ÿå‘ LLM ä¼ è¾¾æˆ‘ä»¬å¸Œæœ›ä¿æŒç¼–è¾‘è®ºæ–‡ JSON æ ¼å¼çš„ç»“æž„ï¼Œå¹¶ä»¥ç›¸åŒæ ¼å¼æŽ¥æ”¶å“åº”ã€‚ä¹‹åŽï¼Œæˆ‘ä»¬å°†æŠŠè¿”å›žçš„å“åº”å‘é€åˆ°æˆ‘ä»¬çš„ PDF ç”Ÿæˆå·¥å…·ã€‚\n\n1. **æ£€æµ‹ç¼–è¾‘è¯·æ±‚**ï¼šè·¯ç”±å™¨è¯†åˆ«ç”¨æˆ·è¯·æ±‚æ˜¯å¦ä¸ºç¼–è¾‘è®ºæ–‡ã€‚\n2. **æ”¶é›†å¿…è¦å€¼**ï¼šæ”¶é›†å¯¹è¯åŽ†å²ã€ç”¨æˆ·è¯·æ±‚å’Œæœ€åŽç”Ÿæˆçš„è®ºæ–‡ã€‚\n3. **åˆ›å»ºå¹¶ä½¿ç”¨æç¤º**ï¼šæž„å»ºä¸€ä¸ªåŒ…å« `format_instructions` çš„æç¤ºã€‚\n4. **ç”Ÿæˆç¼–è¾‘åŽçš„è®ºæ–‡**ï¼šè°ƒç”¨ LLM èŽ·å–ç¼–è¾‘åŽçš„è®ºæ–‡ï¼Œå¹¶å°†å“åº”ä¼ é€’ç»™ PDF ç”Ÿæˆå™¨ã€‚\n\n```python\n#Edit Essay Node\n\ndef edit_essay(self, state: GraphState):\n    print(\"**ESSAY EDIT**\")\n    memory = self.memory.load_memory_variables({})\n\n    user_request = state[\"topic\"]\n    parser = JsonOutputParser(pydantic_object=Essay)\n    prompt = PromptTemplate(\n      template=(\"æŒ‰ç…§ç”¨æˆ·è¯·æ±‚ç¼–è¾‘ JSON æ–‡ä»¶ï¼Œå¹¶è¿”å›žæ–°çš„ JSON æ–‡ä»¶ã€‚\"\n                \"\\nè¯·æ±‚:{user_request} \"\n                \"\\nå¯¹è¯åŽ†å²: {memory}\"\n                \"\\n JSON æ–‡ä»¶: {essay}\"\n                \" \\n{format_instructions}\"),\n      input_variables=[\"memory\",\"user_request\",\"essay\"],\n      partial_variables={\"format_instructions\": parser.get_format_instructions()},\n  )\n\n    chain = prompt | self.model | parser\n\n    self.essay = chain.invoke({\"user_request\": user_request,\n                               \"memory\": memory, \n                                \"essay\": self.essay})\n\n\n    self.memory.save_context(inputs={\"input\": state[\"topic\"]},\n                             outputs={\"output\": str(self.essay)})\n    pdf_name = generate_pdf(self.essay)\n    return {\"response\": \"è¿™æ˜¯ä½ çš„ç¼–è¾‘åŽçš„è®ºæ–‡ï¼\", \n            \"essay\": self.essay, \"pdf_name\": f\"{pdf_name}\"}\n```\n\n## æž„å»ºä»£ç†\n\n**å†…å®¹ç ”ç©¶å‘˜**ï¼šä¸ºäº†ä¿æŒæˆ‘ä»¬çš„é¡¹ç›®ç®€å•ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸‰ä¸ªä»£ç†ï¼Œå®ƒä»¬å°†ç›¸äº’é€šä¿¡å¹¶è¿›è¡Œäº’è”ç½‘æœç´¢ä»¥æ’°å†™æ–‡ç« ã€‚è®©æˆ‘ä»¬è®¾è®¡ç¬¬ä¸€ä¸ªä»£ç†ï¼Œç ”ç©¶å‘˜ä»£ç†ã€‚è¯¥ä»£ç†å°†å¯¹ç»´åŸºç™¾ç§‘å’Œå…¶ä»–ç½‘ç«™è¿›è¡Œç½‘é¡µæŠ“å–ï¼Œæ”¶é›†å¿…è¦çš„æ¥æºï¼Œç›´åˆ°å®ƒç¡®å®šå·²æ”¶é›†åˆ°è¶³å¤Ÿçš„ä¿¡æ¯ã€‚å®ƒå°†èŽ·å–ä¸Žä¸»é¢˜ç›¸å…³çš„ä¸»è¦æ ‡é¢˜ã€å‰¯æ ‡é¢˜å’Œæ–‡ç« ï¼Œå¹¶å‡†å¤‡æ‘˜è¦ã€‚éšåŽï¼Œè¿™äº›æ–‡æ¡£å°†è¢«å­˜å‚¨ï¼Œä»¥ä¾¿å‘é€ç»™å†™ä½œä»£ç†ã€‚\n\nåœ¨è®¾è®¡è¿™ä¸ªä»£ç†æ—¶ï¼Œæˆ‘ä»¬éœ€è¦è€ƒè™‘å®ƒçš„è§’è‰²ã€èƒŒæ™¯æ•…äº‹å’Œç›®æ ‡ã€‚æˆ‘ä»¬å°†è¿™äº›åˆ†é…ç»™`Agent`ç±»ä¸­çš„å‚æ•°ï¼Œç±»ä¼¼äºŽæž„å»ºæç¤ºï¼Œä»Žè€Œä¸ºä»£ç†çš„æ“ä½œåšå¥½å‡†å¤‡ã€‚\n\n```python\n#Content Researcher Agent and Task\n\nself.researcher = Agent(\n    role=\"Content Researcher\",\n\n    goal=\"Research accurate content on {topic}\",\n\n    backstory=\"You're researching content to write \n                an essay about the topic: {topic}.\"\n              \"You collect information that helps \n                the audience learn something and make informed decisions.\"\n              \"Your work is the basis for the Content Writer to \n                write an article on this topic.\",\n    verbose=True\n)\n\nself.research = Task(\n    description=(\n        \"1. Prioritize the latest trends, key players, \n            and noteworthy news on {topic}.\\n\"\n        \"2. Identify the target audience, considering their \n            interests and pain points.\\n\"\n        \"3. Research a detailed content outline including \n            an introduction, key points, and a conclusion.\\n\"\n        \"4. Include SEO keywords and relevant data or sources.\"\n    ),\n    expected_output=\"A comprehensive document with an outline, \n                    audience analysis, SEO keywords, and resources.\",\n    tools=[search_wikipedia, scrap_webpage],\n    agent=self.researcher,\n)\n```\n\næˆ‘ä»¬éœ€è¦åˆ›å»ºä¸¤ä¸ªç±»ï¼š`Agent`å’Œ`Task`ã€‚æ¯ä¸ªä»£ç†å¯ä»¥æœ‰ä¸€ä¸ªæˆ–å¤šä¸ªåˆ†é…çš„ä»»åŠ¡ã€‚æˆ‘ä»¬å¯ä»¥ç›´æŽ¥å°†å·¥å…·åˆ†é…ç»™ä»£ç†ï¼Œæˆ–è€…æ·»åŠ ç‰¹å®šäºŽä»»åŠ¡çš„å·¥å…·ã€‚é€šè¿‡ä¸ºä»»åŠ¡ä¸“é—¨æ·»åŠ å·¥å…·ï¼Œæˆ‘ä»¬ç¡®ä¿è¯¥å·¥å…·ä»…åœ¨ç‰¹å®šä»»åŠ¡ä¸­ä½¿ç”¨ã€‚\n\n### å‚æ•°\n\næˆ‘ä»¬çš„ `Agent` ç±»çš„å‚æ•°ï¼š\n\n1. **è§’è‰²**ï¼šå®šä¹‰ä»£ç†åœ¨å›¢é˜Ÿä¸­çš„åŠŸèƒ½ã€‚å®ƒå†³å®šäº†ä»£ç†æœ€é€‚åˆæ‰§è¡Œçš„ä»»åŠ¡ç±»åž‹ï¼Œåº”ç®€çŸ­ä¸”å…·æœ‰æè¿°æ€§ã€‚\n2. **ç›®æ ‡**ï¼šè¿™æ˜¯ä»£ç†æ—¨åœ¨å®žçŽ°çš„ä¸ªäººç›®æ ‡ã€‚å®ƒæŒ‡å¯¼ä»£ç†çš„å†³ç­–è¿‡ç¨‹ï¼Œåº”ç®€çŸ­ä¸”ç®€å•ã€‚\n3. **èƒŒæ™¯æ•…äº‹**ï¼šä¸ºä»£ç†çš„è§’è‰²å’Œç›®æ ‡æä¾›èƒŒæ™¯ï¼Œä¸°å¯Œäº’åŠ¨å’Œåä½œåŠ¨æ€ã€‚åº”å°½å¯èƒ½è¯¦ç»†ã€‚\n4. **è¯¦ç»†**ï¼šå°†å…¶è®¾ç½®ä¸º `True` å¯é…ç½®å†…éƒ¨è®°å½•å™¨ï¼Œæä¾›è¯¦ç»†çš„æ‰§è¡Œæ—¥å¿—ï¼Œæœ‰åŠ©äºŽè°ƒè¯•å’Œç›‘æŽ§æˆ‘ä»¬çš„ä»£ç†æ­£åœ¨è¿›è¡Œçš„æ´»åŠ¨ã€‚\n\næˆ‘ä»¬çš„ `Task` ç±»çš„å‚æ•°ï¼š\n\n1. **æè¿°**ï¼šå¯¹ä»»åŠ¡å†…å®¹çš„æ¸…æ™°ç®€æ´çš„é™ˆè¿°ã€‚åº”å°½å¯èƒ½è¯¦ç»†ä»¥ç¡®ä¿æ¸…æ™°ã€‚\n2. **é¢„æœŸè¾“å‡º**ï¼šå¯¹ä»»åŠ¡å®ŒæˆåŽç»“æžœçš„è¯¦ç»†æè¿°ï¼Œæœ‰åŠ©äºŽè®¾å®šå¯¹ç»“æžœçš„æ˜Žç¡®æœŸæœ›ã€‚\n3. **å·¥å…·**ï¼šä»£ç†å¯ä»¥åˆ©ç”¨æ¥æ‰§è¡Œä»»åŠ¡çš„åŠŸèƒ½æˆ–èƒ½åŠ›ã€‚åœ¨è¿™é‡Œï¼Œæ‚¨å¯ä»¥æ ¹æ®éœ€è¦ä½¿ç”¨ LangChainã€CrewAI æˆ–è‡ªå®šä¹‰å·¥å…·ã€‚\n4. **ä»£ç†**ï¼šè´Ÿè´£è¯¥ä»»åŠ¡çš„ä»£ç†ï¼Œå¯ä»¥ç›´æŽ¥åˆ†é…æˆ–é€šè¿‡å›¢é˜Ÿçš„æµç¨‹åˆ†é…ã€‚\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ocQ9ZUZwFtGx7a7pPrTbuQ.png)\n\n**å†…å®¹æ’°å†™è€…**ï¼šä¸€æ—¦æˆ‘ä»¬çš„ç ”ç©¶ä»£ç†é€šè¿‡å¤šæ¬¡è¿­ä»£æ”¶é›†äº†å¿…è¦çš„ä¿¡æ¯ï¼Œå®ƒå°†æŠŠæ”¶é›†åˆ°çš„æ•°æ®å­˜å‚¨åœ¨å†…å­˜ä¸­ï¼Œè®¤ä¸ºè‡ªå·±å·²èŽ·å¾—è¶³å¤Ÿçš„çŸ¥è¯†ï¼Œå¹¶å°†ä»»åŠ¡ä¼ é€’ç»™æˆ‘ä»¬çš„ä¸‹ä¸€ä¸ªä»£ç†ï¼Œå†…å®¹æ’°å†™è€…ã€‚\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*HD1Bm7twxsUIVGPiqEhHAg.png)\n\nçŽ°åœ¨ï¼Œè®©æˆ‘ä»¬å®šä¹‰æˆ‘ä»¬çš„å†…å®¹æ’°å†™è€…ä»£ç†åŠå…¶è§’è‰²ã€‚è¯¥ä»£ç†ä¸éœ€è¦ä½¿ç”¨ä»»ä½•å·¥å…·ï¼Œå› æ­¤åªéœ€è¯¦ç»†è¯´æ˜ŽèƒŒæ™¯æ•…äº‹å’Œæè¿°ï¼Œç±»ä¼¼äºŽç ”ç©¶ä»£ç†ã€‚åœ¨èƒŒæ™¯æ•…äº‹ä¸­ï¼Œæˆ‘ä»¬å¿…é¡»è®°å¾—æŒ‡å®šå“ªä¸ªä»£ç†æä¾›äº†ä¿¡æ¯æ¥æºã€‚\n\n### å‚æ•°\n\n1. **è§’è‰²**ï¼šé¡¹ç›®ä¸­å†…å®¹ç¼–å†™ä»£ç†çš„åŠŸèƒ½ã€‚è¿™åº”ç®€æ´åœ°æ•æ‰ä»£ç†çš„ä½œç”¨ã€‚\n2. **ç›®æ ‡**ï¼šå†…å®¹ç¼–å†™è€…æ—¨åœ¨å®žçŽ°çš„å…·ä½“ç›®æ ‡ï¼Œä¾‹å¦‚æ ¹æ®æ”¶é›†çš„ä¿¡æ¯æ’°å†™ä¸€ç¯‡ç»“æž„è‰¯å¥½çš„æ–‡ç« ã€‚\n3. **èƒŒæ™¯æ•…äº‹**ï¼šä¸ºå†…å®¹ç¼–å†™è€…çš„è§’è‰²æä¾›ä¸Šä¸‹æ–‡ï¼ŒåŒ…æ‹¬å…³äºŽç ”ç©¶è€…ä»£ç†åŠå…¶æä¾›çš„ä¿¡æ¯çš„è¯¦ç»†ä¿¡æ¯ã€‚ç²¾å¿ƒåˆ¶ä½œçš„èƒŒæ™¯æ•…äº‹å¯ä»¥å¢žå¼ºå™è¿°å’Œåä½œåŠ¨æ€ã€‚\n4. **æè¿°**ï¼šå¯¹å†…å®¹ç¼–å†™è€…æ‰€åšå·¥ä½œçš„æ¸…æ™°ç®€æ´çš„é™ˆè¿°ï¼Œé‡ç‚¹å…³æ³¨å…¶èŒè´£å’Œä»»åŠ¡ã€‚\n5. **é¢„æœŸè¾“å‡º**ï¼šå¯¹ä»»åŠ¡å®Œæˆçš„è¯¦ç»†æè¿°ï¼Œå¸®åŠ©è®¾å®šå¯¹ç»“æžœçš„æ˜Žç¡®æœŸæœ›ã€‚\n6. **ä¸Šä¸‹æ–‡**ï¼šåœ¨æ‰§è¡Œä»»åŠ¡ä¹‹å‰ï¼Œæˆ‘ä»¬æŒ‡å®šè¦ç­‰å¾…å®Œæˆçš„ä»»åŠ¡ï¼Œå¹¶ä»Žè¯¥ä»»åŠ¡è¾“å‡ºä¸­èŽ·å–å¿…è¦çš„ä¿¡æ¯ï¼Œç»“åˆä¸Šä¸‹æ–‡å‚æ•°ã€‚\n\n```python\n#Content Writer Agent and Task\n\nself.writer = Agent(\n  role=\"Content Writer\",\n\n  goal=\"æ’°å†™æœ‰å…³æä¾›ä¸»é¢˜çš„æ·±åˆ»ä¸”äº‹å®žå‡†ç¡®çš„ \"\n       \"è§‚ç‚¹æ–‡ç« \",\n\n  backstory=\"æ‚¨æ­£åœ¨æ’°å†™ä¸€ç¯‡å…³äºŽæä¾›ä¸»é¢˜çš„æ–°è§‚ç‚¹æ–‡ç« ã€‚\"\n            \"æ‚¨åŸºäºŽå†…å®¹ç ”ç©¶å‘˜çš„å·¥ä½œï¼Œè¯¥ç ”ç©¶å‘˜æä¾›äº†ä¸»é¢˜çš„æçº²å’Œç›¸å…³èƒŒæ™¯ä¿¡æ¯ã€‚\"\n            \"æ‚¨éµå¾ªå†…å®¹ç ”ç©¶å‘˜æä¾›çš„æçº²çš„ä¸»è¦ç›®æ ‡å’Œæ–¹å‘ã€‚\"\n            \"æ‚¨è¿˜æä¾›å®¢è§‚å’Œå…¬æ­£çš„è§è§£ï¼Œå¹¶ç”¨å†…å®¹ç ”ç©¶å‘˜æä¾›çš„ä¿¡æ¯è¿›è¡Œæ”¯æŒã€‚\",\n  verbose=True,\n)\n\nself.write = Task(\n  description=(\n      \"1. ä½¿ç”¨å†…å®¹æ’°å†™ä¸€ç¯‡å¼•äººå…¥èƒœçš„æ–‡ç« ã€‚\\n\"\n      \"2. è‡ªç„¶åœ°èžå…¥SEOå…³é”®è¯ã€‚\\n\"\n      \"3. å„éƒ¨åˆ†/å‰¯æ ‡é¢˜ä»¥å¼•äººå…¥èƒœçš„æ–¹å¼å‘½åã€‚\\n\"\n      \"4. ç¡®ä¿æ–‡ç« ç»“æž„åˆç†ï¼ŒåŒ…å«å¼•äººå…¥èƒœçš„å¼•è¨€ã€æ·±åˆ»çš„ä¸»ä½“å’Œæ€»ç»“æ€§çš„ç»“è®ºã€‚\\n\"\n      \"5. æ ¡å¯¹è¯­æ³•é”™è¯¯å¹¶ç¡®ä¿ä¸Žå“ç‰Œå£°éŸ³ä¸€è‡´ã€‚\\n\"\n      \"6. é€‰æ‹©åˆé€‚çš„æ ‡é¢˜ã€‚\\n\"\n  ),\n  expected_output=\"ä¸€ç¯‡ä»¥markdownæ ¼å¼æ’°å†™çš„æ–‡ç« ï¼Œ\"\n                  \"å‡†å¤‡å‘å¸ƒï¼Œæ¯ä¸ªéƒ¨åˆ†åº”æœ‰2æˆ–3æ®µã€‚\",\n  context=[self.research],\n  agent=self.writer,\n)\n```\n\n**å†…å®¹ç¼–è¾‘å™¨**ï¼šåœ¨å®šä¹‰äº†ç¼–å†™ä»£ç†åŽï¼Œæˆ‘ä»¬æœ¬å¯ä»¥ç»“æŸè¿™ä¸ªè¿‡ç¨‹ï¼›ç„¶è€Œï¼Œå³ä½¿ç¼–å†™ä»£ç†è´Ÿè´£å†™ä½œï¼Œå®ƒä»å¯èƒ½å‡ºçŽ°æ‹¼å†™é”™è¯¯å’Œç ´åå†…å®¹è¿žè´¯æ€§çš„é”™è¯¯ã€‚ä¸ºé˜²æ­¢è¿™äº›é—®é¢˜å¹¶å°†æ–‡ç« è¾“å‡ºä¸ºJSONæ ¼å¼ï¼Œæˆ‘ä»¬å°†å®šä¹‰ä¸€ä¸ªæ–°çš„ä»£ç†ï¼šå†…å®¹ç¼–è¾‘å™¨ã€‚\n\nåœ¨è¯¥ä»£ç†çš„èƒŒæ™¯æ•…äº‹ä¸­ï¼Œæˆ‘ä»¬å°†è¯´æ˜Žå®ƒè´Ÿè´£å®¡æŸ¥å’Œçº æ­£ä»Žç¼–å†™ä»£ç†æ”¶åˆ°çš„æ–‡ç« ã€‚åœ¨ä»»åŠ¡é˜¶æ®µï¼Œæˆ‘ä»¬è¿˜å°†å®šä¹‰æ‰€éœ€çš„è¾“å‡ºæ ¼å¼ã€‚\n\n```python\n#Content Editor Agent and Task\nself.editor = Agent(\n    role=\"Content Editor\",\n\n    goal=\"ç¼–è¾‘ç»™å®šçš„æ–‡ç« ï¼Œä»¥ç¬¦åˆç»„ç»‡çš„å†™ä½œé£Žæ ¼ã€‚\",\n\n    backstory=\"æ‚¨æ˜¯ä¸€åç¼–è¾‘ï¼Œæ”¶åˆ°æ¥è‡ªå†…å®¹ç¼–å†™è€…çš„æ–‡ç« ã€‚\"\n              \"æ‚¨çš„ç›®æ ‡æ˜¯å®¡æŸ¥æ–‡ç« ï¼Œä»¥ç¡®ä¿å…¶éµå¾ªæœ€ä½³å®žè·µï¼Œæä¾›å¹³è¡¡çš„è§‚ç‚¹\"\n              \"åœ¨æä¾›æ„è§æˆ–æ–­è¨€æ—¶ï¼Œå°½é‡é¿å…é‡å¤§äº‰è®®è¯é¢˜æˆ–æ„è§ã€‚\",\n    verbose=True\n)\n\nself.edit = Task(\n    description=\"æ ¡å¯¹ç»™å®šæ–‡ç« çš„è¯­æ³•é”™è¯¯ï¼Œå¹¶ç¡®ä¿ä¸Žå“ç‰Œå£°éŸ³ä¸€è‡´ã€‚\",\n\n    expected_output=\"ä¸€ç¯‡ä»¥æ‰€éœ€æ ¼å¼æ’°å†™çš„æ–‡ç« ï¼Œ\"\n                    \"å‡†å¤‡å‘å¸ƒï¼Œæ¯ä¸ªéƒ¨åˆ†åº”æœ‰2æˆ–3æ®µã€‚\",\n    output_json = Essay,\n    context=[self.write],\n    agent=self.editor\n)\n```\n\nåœ¨è¿™é‡Œï¼Œæˆ‘ä»¬çš„è¾“å‡ºæ˜¯ä¸€ä¸ªåä¸º`Essay`çš„å¯¹è±¡ï¼Œå®ƒæ˜¯é€šè¿‡Pydanticåº“ä¸­çš„`BaseModel`å’Œ`Field`ç±»åˆ›å»ºçš„ã€‚é€šè¿‡æ·»åŠ æˆ‘ä»¬çš„ä»£ç†å¯ä»¥ç†è§£çš„è§£é‡Šï¼Œæˆ‘ä»¬ç¡®ä¿ä»£ç†å°†æ•°æ®ä»¥PDFæ‰“å°åŠŸèƒ½æ‰€æœŸæœ›çš„æ ¼å¼è¾“å‡ºã€‚\n\n```python\n#Expected Pydantic Output\n\nclass Paragraph(TypedDict):\n    sub_header: str\n    paragraph: str\n\nclass Essay(BaseModel):\n    header: str = Field(..., description=\"æ–‡ç« çš„æ ‡é¢˜\")\n    entry: str = Field(..., description=\"æ–‡ç« çš„å¼•è¨€\")\n    paragraphs: List[Paragraph] = Field(..., description=\"æ–‡ç« çš„æ®µè½\")\n    conclusion: str = Field(..., description=\"æ–‡ç« çš„ç»“è®º\")\n    seo_keywords: List[str] = Field(..., description=\"æ–‡ç« çš„SEOå…³é”®è¯\")\n```\n\næˆ‘ä»¬å·²ç»å®šä¹‰äº†æˆ‘ä»¬çš„ä»£ç†åŠå…¶ä»»åŠ¡ã€‚çŽ°åœ¨ï¼Œè®©æˆ‘ä»¬å°†æˆ‘ä»¬çš„ä¸‰ä¸ªä»£ç†ç»“åˆåœ¨ä¸€èµ·ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åº”è¯¥ä½¿ç”¨CrewAIåº“ä¸­çš„ä¸€ä¸ªå°è€Œå®žç”¨çš„æ–¹æ³•ï¼Œç§°ä¸º`Crew`ã€‚åœ¨æ­¤æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬åˆ—å‡ºå°†é¡ºåºæ“ä½œçš„ä»£ç†åŠå…¶å°†ä½¿ç”¨çš„å·¥å…·ã€‚å¦‚æžœä»»åŠ¡éœ€è¦æŒ‰é¡ºåºæ‰§è¡Œï¼Œå¦‚åœ¨æˆ‘ä»¬çš„é¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬å°†`process`å‚æ•°è®¾ç½®ä¸º`Process.sequential`ã€‚æˆ‘ä»¬è¿˜å°†`memory`å‚æ•°è®¾ç½®ä¸º`True`ï¼Œä»¥ä½¿ä»£ç†èƒ½å¤Ÿä½¿ç”¨çŸ­æœŸå’Œé•¿æœŸè®°å¿†è¿›è¡Œç›¸äº’é€šä¿¡ã€‚\n\n```python\n#Crew Run\n\ndef kickoff(self,*args):\n    return Crew(\n        agents=[self.researcher, self.writer, self.editor],\n        tasks=[self.research, self.write, self.edit],\n        process=Process.sequential,\n        verbose=True,\n        memory=True\n    ).kickoff(*args)\n```\n\næˆ‘ä»¬çš„ä»£ç†ç»“æž„å·²ç»å®Œæˆï¼Œä½†æˆ‘ä»¬è¿˜æ²¡æœ‰è®¨è®ºæˆ‘ä»¬çš„å·¥å…·ã€‚çŽ°åœ¨ï¼Œè®©æˆ‘ä»¬ç®€è¦è¯´æ˜Žä¸€ä¸‹æˆ‘ä»¬çš„å·¥å…·ã€‚\n\n## æž„å»ºå·¥å…·\n\nå·¥å…·æœ¬è´¨ä¸Šæ˜¯æŽ¥å—å„ç§è¾“å…¥å¹¶è¿”å›žå€¼ä½œä¸ºè¾“å‡ºçš„å‡½æ•°ã€‚æˆ‘ä»¬çš„ä»£ç†å°†ç®€å•åœ°æä¾›è¿™äº›å‡½æ•°æ‰€éœ€çš„è¾“å…¥ï¼Œå¹¶å¤„ç†ä»–ä»¬æ”¶åˆ°çš„è¾“å‡ºã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦ä»¥é«˜å®¹é”™æ€§è®¾è®¡æˆ‘ä»¬çš„å·¥å…·ã€‚å½“å‘ç”Ÿä½¿ç”¨é”™è¯¯æ—¶ï¼Œæˆ‘ä»¬çš„ä»£ç†åº”è¯¥èƒ½å¤Ÿè¯»å–é”™è¯¯ï¼Œå¹¶é…å¤‡ä¿¡æ¯ä»¥ä¾¿åœ¨ä¸‹ä¸€æ¬¡è¿­ä»£ä¸­æ­£ç¡®ä½¿ç”¨å·¥å…·ã€‚\n\nåœ¨ä¸ºæˆ‘ä»¬çš„å·¥å…·å‡†å¤‡å¥½å‡½æ•°åŽï¼Œæˆ‘ä»¬åº”è¯¥ä½¿ç”¨ LangChain æˆ– CrewAI çš„å·¥å…·åˆ›å»ºç±»å°†å®ƒä»¬è½¬æ¢ä¸ºå·¥å…·å¯¹è±¡ï¼Œå¹¶é™„ä¸Šå„ç§è¯´æ˜Žã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬é€šè¿‡ç®€å•åœ°åœ¨å‡½æ•°é¡¶éƒ¨å†™ä¸Š C**rewAI çš„å·¥å…·è£…é¥°å™¨**å°†æˆ‘ä»¬çš„å·¥å…·è½¬æ¢ä¸ºä»£ç†å¯ä»¥ä½¿ç”¨çš„å½¢å¼ã€‚\n\n```python\nfrom crewai_tools import tool\n\n@tool(\"Wikipedia Search Tool\")\ndef search_wikipedia(query: str) -> str:\n    \"\"\"Run Wikipedia search and get page summaries.\"\"\"\n    page_titles = wikipedia.search(query)\n    summaries = []\n\n    for page_title in page_titles[:3]:  # First 3 results\n        try:\n            wiki_page = wikipedia.page(title=page_title, auto_suggest=False)\n            summaries.append(f\"Page: {page_title}\\nSummary: {wiki_page.summary}\")\n        except wikipedia.PageError: # Page Not Found\n            pass\n        except wikipedia.DisambiguationError: # Disambiguation Error\n            pass\n\n    if not summaries:\n        return \"No good Wikipedia Search Result was found\"\n\n    return \"\\n\\n\".join(summaries)\n```\n\n## æž„å»ºåº”ç”¨ç¨‹åº\n\nçŽ°åœ¨ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨æˆ‘ç»å¸¸ä½¿ç”¨å¹¶ä¸”è®¤ä¸ºæä¾›äº†ç®€å•ç•Œé¢è®¾è®¡çš„ Streamlit æ¡†æž¶æ¥å®žæ—¶éƒ¨ç½²æˆ‘ä»¬çš„åº”ç”¨ç¨‹åºã€‚Streamlit æ˜¯ä¸€ä¸ªå¼€æºçš„ Python æ¡†æž¶ï¼Œä¾›æ•°æ®ç§‘å­¦å®¶å’Œ AI/ML å·¥ç¨‹å¸ˆä½¿ç”¨ï¼Œä»…éœ€å‡ è¡Œä»£ç å³å¯äº¤ä»˜åŠ¨æ€æ•°æ®åº”ç”¨ç¨‹åºã€‚\n\nå½“ç”¨æˆ·åœ¨ `text_input` æ¡†ä¸­è¾“å…¥ä»–ä»¬çš„ OpenAI å¯†é’¥å¹¶ç‚¹å‡»â€œåˆå§‹åŒ–ä»£ç†â€æŒ‰é’®æ—¶ï¼Œæˆ‘ä»¬çš„åº”ç”¨ç¨‹åºä¸»è¦æ¿€æ´»ã€‚å½“ç”¨æˆ·é€šè¿‡æ´»åŠ¨çš„ `chat_input` éƒ¨åˆ†å‘é€æ¶ˆæ¯æ—¶ï¼Œä»¥ä¸‹å‡½æ•°ç”¨äºŽå°†è¾“å…¥çš„è¯·æ±‚ä¼ é€’ç»™æˆ‘ä»¬å»ºç«‹çš„ä»£ç†ç»“æž„ï¼š\n\n```python\ndef generate_response(topic):\n    return app.invoke(input={\"topic\": topic})\n```\n\nå€ŸåŠ© Streamlit çš„ `st.chat_message` ç»„ä»¶ï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾å®žçŽ°èŠå¤©æœºå™¨äººç•Œé¢ã€‚å¦‚æžœç”¨æˆ·æ­£åœ¨è¿›è¡Œå¸¸è§„æ¶ˆæ¯ä¼ é€’ï¼Œå“åº”å°†æ˜¾ç¤ºæ­£å¸¸ç­”æ¡ˆã€‚å¦‚æžœç”Ÿæˆäº†ä¸€ç¯‡æ–‡ç« ï¼Œæˆ‘ä»¬å°†é€šè¿‡ç¼–å†™ç®€å•çš„ if-else å¾ªçŽ¯å‘ç”¨æˆ·æä¾› PDF çš„ç›®å½•ã€‚\n\nåŒæ—¶ï¼Œæˆ‘ä»¬å°†ä»ŽèŠå¤©æœºå™¨äººå‘é€å’ŒæŽ¥æ”¶çš„æ¯æ¡æ¶ˆæ¯æ·»åŠ åˆ° Streamlit çš„ `session_state` ä¸­åˆ›å»ºçš„ `messages` å˜é‡ä¸­ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å°±åˆ›å»ºäº†ä¸€ä¸ªå¯è§çš„èŠå¤©å±å¹•ã€‚\n\n```python\n#Streamlit App\n\nimport streamlit as st\nfrom graph import EssayWriter\nimport os\nimport base64\n\nst.set_page_config(page_title=\"Essay Writer Chat Bot\", page_icon=\"ðŸ¤–\")\nst.image(\"./media/cover.jpg\", use_column_width=True)\n\n\nif \"messages\" not in st.session_state:\n    st.session_state.messages =  [{\"role\": \"assistant\", \"content\": \"Hello!\"}]\n    st.session_state.app = None\n    st.session_state.chat_active = True\n\nwith st.sidebar:\n    st.info(\" * æ­¤åº”ç”¨ç¨‹åºä½¿ç”¨ OpenAI API ç”Ÿæˆæ–‡æœ¬ï¼Œè¯·æä¾›æ‚¨çš„ API å¯†é’¥ã€‚\"\n            \"\\n\\n * æ­¤åº”ç”¨ç¨‹åºä½¿ç”¨ 'gpt-4o-mini-2024-07-18' æ¨¡åž‹ã€‚æˆæœ¬æœ‰æ•ˆä¸”é«˜æ•ˆã€‚\"\n            \"\\n\\n * å¦‚æžœæ‚¨æ²¡æœ‰ API å¯†é’¥ï¼Œå¯ä»¥åœ¨ [è¿™é‡Œ](https://proxy.rifx.online/https://platform.openai.com/signup) èŽ·å–ã€‚\"\n            \"\\n\\n * æ‚¨è¿˜å¯ä»¥åœ¨ [è¿™é‡Œ](https://proxy.rifx.online/https://github.com/mesutdmn/Autonomous-Multi-Agent-Systems-with-CrewAI-Essay-Writer) æ‰¾åˆ°æ­¤åº”ç”¨ç¨‹åºçš„æºä»£ç ã€‚\"\n            \"\\n\\n * åº”ç”¨ç¨‹åºå¯†é’¥ä¸ä¼šä»¥ä»»ä½•æ–¹å¼å­˜å‚¨æˆ–ä¿å­˜ã€‚\"\n            \"\\n\\n * å†™ä½œè®ºæ–‡å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…ã€‚å¤§çº¦ 1-2 åˆ†é’Ÿã€‚\"\n    openai_key= st.text_input(\"OpenAI API å¯†é’¥\", type=\"password\")\n\n\ndef initialize_agents():\n    os.environ[\"OPENAI_API_KEY\"] = openai_key\n    essay_writer = EssayWriter().graph\n\n    if len(openai_key) < 1:\n        st.error(\"è¯·è¾“å…¥æ‚¨çš„ OpenAI API å¯†é’¥å¹¶åˆå§‹åŒ–ä»£ç†ã€‚\")\n\n        st.session_state.chat_active = True\n    else:\n        st.success(\"ä»£ç†æˆåŠŸåˆå§‹åŒ–\")\n        st.session_state.chat_active = False\n\n    return essay_writer\n\nwith st.sidebar:\n    if st.button(\"åˆå§‹åŒ–ä»£ç†\", type=\"primary\"):\n        st.session_state.app = initialize_agents()\n\napp = st.session_state.app\ndef generate_response(topic):\n    return app.invoke(input={\"topic\": topic})\n\n\nfor message in st.session_state.messages:\n    with st.chat_message(message[\"role\"]):\n        st.markdown(message[\"content\"], unsafe_allow_html=True)\n\nif topic:= st.chat_input(placeholder=\"é—®ä¸€ä¸ªé—®é¢˜\", disabled=st.session_state.chat_active):\n    st.chat_message(\"user\").markdown(topic)\n\n    st.session_state.messages.append({\"role\": \"user\", \"content\": topic})\n    with st.spinner(\"æ€è€ƒä¸­...\"):\n        response = generate_response(topic)\n\n    with st.chat_message(\"assistant\"):\n        if \"pdf_name\" in response:\n            with open(f\"./{response['pdf_name']}\", \"rb\") as file:\n                file_bytes = file.read()\n                b64 = base64.b64encode(file_bytes).decode()\n            href = f'<a href=\"data:application/pdf;base64,{b64}\" download=\"{response['pdf_name']}\">{response['pdf_name']}</a>'\n\n            st.markdown(f\"{response['response']}: {href}\", unsafe_allow_html=True)\n            st.session_state.messages.append({\"role\": \"assistant\", \"content\": f\"{response['response']}: {href}\"})\n        else:\n            st.markdown(response[\"response\"])\n            st.session_state.messages.append({\"role\": \"assistant\", \"content\": response[\"response\"]})\n```\n\n**æ­å–œ**ï¼æˆ‘ä»¬å·²ç»å®Œæˆäº†æˆ‘ä»¬çš„é¡¹ç›®ã€‚å¦‚æžœæ‚¨æ„¿æ„ï¼Œå¯ä»¥è§‚çœ‹æˆ‘ä¸ºæ‚¨å½•åˆ¶çš„é¡¹ç›®å·¥ä½œæ—¥å¿—ã€‚ä¸è¦å¿˜è®°è®¿é—® GitHub [**ä»“åº“**](https://proxy.rifx.online/https://github.com/mesutdmn/Autonomous-Multi-Agent-Systems-with-CrewAI-Essay-Writer) ä»¥èŽ·å–é¡¹ç›®çš„æ‰€æœ‰ä»£ç ã€‚\n\nè¿™å°±æ˜¯æˆ‘ä»¬åº”ç”¨ç¨‹åºçš„ä¸»é¡µåœ¨éƒ¨ç½²åŽå°†å‘ˆçŽ°çš„æ ·å­ï¼\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8tDAluuAH6njIohDbb-UqA.png)\n\n## ç»“è®º\n\nåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æŽ¢è®¨äº†å¦‚ä½•ä½¿ç”¨ CrewAI æž„å»ºè‡ªä¸»å¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€‚æˆ‘ä»¬é¦–å…ˆè®¨è®ºäº†åˆ›å»ºæ™ºèƒ½ä½“çš„åŠ¨æœºï¼Œä»¥åŠå®ƒä»¬å¦‚ä½•ååŒå·¥ä½œä»¥æ›´é«˜æ•ˆåœ°å®Œæˆä»»åŠ¡ã€‚é€šè¿‡å°†ä»»åŠ¡ç»†åˆ†å¹¶åˆ©ç”¨å·¥å…·ï¼Œæˆ‘ä»¬ä½¿æˆ‘ä»¬çš„æ™ºèƒ½ä½“èƒ½å¤Ÿä»¥ç»“æž„åŒ–çš„æ–¹å¼æ‰§è¡Œå¤æ‚æ“ä½œã€‚\n\næˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªç®€å•çš„é¡¹ç›®ï¼Œé›†æˆäº† CrewAI å’Œ LangChain æ¡†æž¶ï¼Œå±•ç¤ºäº†å¤šä¸ªæ™ºèƒ½ä½“å¦‚ä½•åä½œæ”¶é›†ä¿¡æ¯ã€æ’°å†™è®ºæ–‡å’Œç¼–è¾‘å†…å®¹ã€‚å¼ºè°ƒäº†å·¥å…·ä½¿ç”¨å’Œä»»åŠ¡ç®¡ç†ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬çš„æ™ºèƒ½ä½“èƒ½å¤Ÿé¡ºåˆ©æœ‰æ•ˆåœ°è¿è¡Œã€‚\n\næœ€åŽï¼Œæˆ‘ä»¬ä½¿ç”¨ Streamlit éƒ¨ç½²äº†æˆ‘ä»¬çš„åº”ç”¨ç¨‹åºï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿè½»æ¾ä¸Žç³»ç»Ÿäº’åŠ¨ã€‚\n\næ‚¨å¯ä»¥åœ¨ [**è¿™é‡Œ**](https://proxy.rifx.online/https://multi-agent-essay-writer.streamlit.app/) æŸ¥çœ‹å®žæ—¶é¡¹ç›®ï¼Œåœ¨æˆ‘çš„ GitHub ä»“åº“ [**è¿™é‡Œ**](https://proxy.rifx.online/https://github.com/mesutdmn/Autonomous-Multi-Agent-Systems-with-CrewAI-Essay-Writer) æŸ¥çœ‹æºä»£ç \n\n\n"},{"lang":"zh","group":"blog","slug":"blog/building-self-healing-intelligent-test-automation-with-gen-ai-openai-apis-6c39808adb0f","frontmatter":{"title":"åˆ©ç”¨ Gen AIï¼ˆOpenAI APIï¼‰æž„å»ºæ™ºèƒ½æµ‹è¯•è‡ªåŠ¨åŒ–","meta_title":"åˆ©ç”¨ Gen AIï¼ˆOpenAI APIï¼‰æž„å»ºæ™ºèƒ½æµ‹è¯•è‡ªåŠ¨åŒ–","description":"æˆ‘ä»¬éƒ½çŸ¥é“ï¼Œç”¨æˆ·ç•Œé¢æµ‹è¯•æ˜¯è¶…çº§è„†å¼±çš„ã€‚å®ƒä»¬ä¼šå› å„ç§åŽŸå› è€Œå´©æºƒï¼Œå…¶ä¸­æœ€å¤§çš„ç½ªé­ç¥¸é¦–ä¹‹ä¸€å°±æ˜¯ UI çš„æ›´æ”¹...","date":"2024-11-13T01:22:29.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kZ4ZR-jqdTTgH3bpOzcgUw.png","categories":["Generative AI","Programming","Testing"],"author":"Rifx.Online","tags":["Generative","OpenAI","Selenium","LLMs","POM"],"draft":false,"slug":"blog/building-self-healing-intelligent-test-automation-with-gen-ai-openai-apis-6c39808adb0f"},"content":"\n\n\n> æˆ‘ä»¬éƒ½çŸ¥é“ UI æµ‹è¯•éžå¸¸è„†å¼±ã€‚å®ƒä»¬å¯èƒ½å› å„ç§åŽŸå› è€Œå¤±è´¥ï¼Œå…¶ä¸­ä¸€ä¸ªæœ€å¤§çš„é—®é¢˜æ˜¯ UI å®šä½å™¨çš„å˜åŒ–ã€‚å¾ˆéš¾æƒ³è±¡æˆ‘ä»¬å¦‚ä½•èƒ½è®©å®ƒä»¬è¶³å¤Ÿæ™ºèƒ½ï¼Œä»¥ç†è§£å®šä½å™¨ä½•æ—¶å‘ç”Ÿå˜åŒ–ï¼Œå¹¶åœ¨æµ‹è¯•ä¸­å‡ºçŽ°å®šä½å™¨é—®é¢˜ä¹‹å‰é˜²æ­¢æµ‹è¯•è¿è¡Œã€‚\n\nä½ æ²¡å¬é”™ï¼çŽ°åœ¨æ˜¯ 2024 å¹´ï¼Œè‡ªåŠ¨åŒ–æµ‹è¯•å·¥å…·å·²ç»å–å¾—äº†é•¿è¶³çš„è¿›æ­¥ã€‚åœ¨ä¸Žè¿™äº›å·¥å…·æ‰“äº¤é“è¿‘ 18 å¹´åŽï¼Œä»Ž Mercury Winrunner åˆ° Playwrightï¼Œæˆ‘ä»¬çŽ°åœ¨å¯ä»¥åˆ©ç”¨ç”Ÿæˆæ€§ AI çš„å¼ºå¤§åŠŸèƒ½åšä¸€äº›çœŸæ­£ä»¤äººæƒŠå¹çš„äº‹æƒ…ã€‚è¿™å°±åƒé­”æ³•ï¼Œä½†è¿™æ˜¯çœŸæ­£çš„ç§‘å­¦ï¼\n\næ²¡é”™ï¼Œæˆ‘ä»¬çŽ°åœ¨å¯ä»¥æ‰¾åˆ°ä¸€ç§æ–¹æ³•ï¼Œè®©æˆ‘ä»¬çš„æµ‹è¯•è‡ªåŠ¨åŒ–ä»£ç  **æ›´æ™ºèƒ½**ï¼Œè€Œä¸éœ€è¦è‡ªå·±ç¼–å†™å„ç§æ¨¡ç³Šçš„æ•°å­¦ç®—æ³•ï¼Œè¿™ä¸€åˆ‡éƒ½ç”± LLM çš„ç¥žæ¥å¤„ç†ã€‚\n\nåœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºå¦‚ä½•ä»¥æ›´æœ‰æ•ˆå’Œé«˜æ•ˆçš„æ–¹å¼ä½¿æˆ‘ä»¬çš„æµ‹è¯•å˜å¾—æ™ºèƒ½ï¼Œä½†åŒæ ·ï¼Œè¦å®žçŽ°è¿™ä¸€ç‚¹ï¼Œæ‚¨éœ€è¦å…·å¤‡ä»¥ä¸‹å‰ææ¡ä»¶ï¼š\n\n1. **Open AI API** å¸¦ä¿¡ç”¨é¢åº¦ï¼ˆæ‚¨éœ€è¦å³æ—¶è´­ä¹°ï¼‰\n\n\n\n2\\. C\\# .NET ä»£ç çŸ¥è¯†ï¼Œå› ä¸ºæˆ‘å°†è¦æ¶µç›–çš„ä»£ç æ¥è‡ª .NET å’Œ Selenium\n\n3\\. å¯¹æµ‹è¯•è‡ªåŠ¨åŒ–çš„åŸºæœ¬ç†è§£\n\nå†æ¬¡å¼ºè°ƒï¼Œä»¥ä¸Šæ‰€æœ‰å†…å®¹ä»¥åŠä»¥ä¸‹è®¨è®ºéƒ½æ˜¯æˆ‘ [Udemy è¯¾ç¨‹](https://proxy.rifx.online/https://www.udemy.com/course/generative-ai-in-software-automation-testing/) çš„ä¸€éƒ¨åˆ†ï¼Œè¯¥è¯¾ç¨‹æ¶µç›–äº†æ›´è¯¦ç»†çš„å†…å®¹å’Œé€æ­¥ç¼–å†™ä»£ç çš„æ–¹æ³•ã€‚\n\n## è®©æˆ‘ä»¬ç†è§£é—®é¢˜é™ˆè¿°\n\næˆ‘ä»¬æœ‰ä¸€ä¸ªé¡µé¢ï¼Œå¸Œæœ›ä½¿ç”¨ Selenium C# ä»£ç è¿›è¡Œè‡ªåŠ¨åŒ–ã€‚æˆ‘ä»¬ä½¿ç”¨é¡µé¢å¯¹è±¡æ¨¡åž‹ï¼ˆPOMï¼‰æ¨¡å¼ç¼–å†™äº†éžå¸¸å¥½çš„ä»£ç ï¼Œä¸€åˆ‡çœ‹èµ·æ¥éƒ½å¾ˆæ£’ï¼Œå¹¶ä¸”å®Œç¾Žè¿è¡Œï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GPSBTmPEZBpubI72OElwbw.gif)\n\næˆ‘ä»¬çš„å¼€å‘å† å†›å‘çŽ°äº†ä¸€ä¸ªéœ€è¦è°ƒæ•´çš„ UI å…ƒç´ ã€‚ä»–æ ¹æ®åŒäº‹çš„ä»£ç å®¡æŸ¥æ„è§è¿›è¡Œäº†æ›´æ”¹ï¼Œä½†ä¸å¹¸çš„æ˜¯ï¼Œä»–åˆ é™¤äº†æˆ‘ä»¬åœ¨è‡ªåŠ¨åŒ–æµ‹è¯•ä¸­ä½¿ç”¨çš„å®šä½å™¨ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬çš„ POM ä»£ç å°†ä¸å†å·¥ä½œï¼Œå› ä¸ºå®šä½å™¨ä¸å†å­˜åœ¨ï¼Œè¿™æœ€ç»ˆå¯¼è‡´æµ‹è¯• **å¤±è´¥**ã€‚\n\næœ€é‡è¦çš„æ˜¯ï¼Œç”±äºŽå•ä¸ªå®šä½å™¨çš„æ›´æ”¹ï¼Œæ‰€æœ‰æµ‹è¯•åœºæ™¯éƒ½å°†å› ç›¸åŒçš„å¤±è´¥è€Œå¤±è´¥ã€‚æµ‹è¯•å¹¶ä¸çŸ¥é“å®šä½å™¨å·²æ›´æ”¹ï¼Œä¹Ÿæ²¡æœ‰ä»»ä½•æ–¹æ³•çŸ¥é“è¿™ä¸€ç‚¹ï¼Œå› æ­¤å®ƒæ€»æ˜¯å¤±è´¥ã€‚\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*tINBnScOW78vz8sKb6lWbA.gif)\n\n## å¦‚ä½•è§£å†³è¿™ä¸ªé—®é¢˜ï¼Ÿ\n\næˆ‘ç›¸ä¿¡åƒæˆ‘ä¸€æ ·ï¼Œå¾ˆå¤šäººåœ¨ä½¿ç”¨ UI æµ‹è¯•å·¥å…·æ—¶ï¼Œæ¯å¤©éƒ½åœ¨ç»åŽ†è¿™ä¸ªé—®é¢˜ï¼Œæ— è®ºæ˜¯ **Cypress**ã€**Selenium** è¿˜æ˜¯ **Playwright**ã€‚è¿™ä¸ªé—®é¢˜æ€»æ˜¯å­˜åœ¨ï¼Œæ— è®ºä½¿ç”¨ä»€ä¹ˆå·¥å…·ã€‚\n\nçŽ°åœ¨è®©æˆ‘ä»¬æ¥ç†è§£å¦‚ä½•è§£å†³ä¸Šè¿°é—®é¢˜ã€‚\n\næˆ‘ä»¬éƒ½çŸ¥é“ **ç”Ÿæˆå¼ AI** å’Œ **å¤§åž‹è¯­è¨€æ¨¡åž‹**ï¼ˆLLMsï¼‰å·²ç»è¿œè¿œè¶…å‡ºäº†æ–‡æœ¬/å›¾åƒ/è§†é¢‘ç”Ÿæˆçš„èŒƒç•´ã€‚å®ƒä»¬ç†è§£ç»™å®šçš„ä¸Šä¸‹æ–‡ï¼Œå¹¶ç”Ÿæˆæˆ‘ä»¬æ‰€å¯»æ‰¾çš„æœ‰æ„ä¹‰çš„ä¿¡æ¯é›†ã€‚\n\nå› æ­¤ï¼Œé’ˆå¯¹ä¸Šè¿°é—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨ç”Ÿæˆå¼ AI çš„åŠ›é‡ï¼Œé€šè¿‡ OpenAI çš„ APIï¼Œå°†æˆ‘ä»¬çš„æç¤ºè¯·æ±‚ä¼ é€’ç»™åƒ ***GPT 4o*** æˆ– ***GPT 4 turbo*** çš„ LLMï¼Œä»¥ç†è§£é—®é¢˜é™ˆè¿°å¹¶ç»™å‡ºæœ‰æ„ä¹‰çš„è§£å†³æ–¹æ¡ˆã€‚\n\n> é‚£ä¹ˆï¼Œæˆ‘ä»¬éœ€è¦å‘ OpenAI çš„ API ä¼ é€’ä»€ä¹ˆæç¤ºè¯·æ±‚ï¼Œä»¥ä¾¿åœ¨æˆ‘ä»¬çš„æµ‹è¯•è‡ªåŠ¨åŒ–ä¸­æ‰§è¡Œæ“ä½œå‘¢ï¼Ÿ\n\nå¥½å§ï¼Œè¿™å¼ å›¾å°†ç»™ä½ ç­”æ¡ˆã€‚\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*bsCOcyWc0FDnxPp9ApssVw.gif)\n\næˆ‘ä»¬å¯ä»¥å°†åº”ç”¨ç¨‹åºçš„â€œ**å®žé™…æµ‹è¯•é¡µé¢**â€å’Œ Selenium æµ‹è¯•çš„â€œ**é¡µé¢å¯¹è±¡æ¨¡åž‹**â€ä»£ç ä½œä¸ºæç¤ºå‘é€ç»™ OpenAI çš„ APIï¼ˆé™„å¸¦ä¸€äº›é¢å¤–çš„å“åº”è§£æžç»†èŠ‚ï¼‰ã€‚è¿™å°†ä½œä¸º OpenAI API çš„éªŒè¯è¿‡ç¨‹ï¼Œä»¥æŸ¥çœ‹å®šä½å™¨æ˜¯å¦ä¸Žç»™å®šé¡µé¢åŒ¹é…ã€‚\n\næ ¹æ®è¯¥æ“ä½œï¼Œæˆ‘ä»¬å¯ä»¥å†³å®šæµ‹è¯•æ˜¯å¦æ‰§è¡Œã€‚å®šä½å™¨å‘ç”Ÿäº†å˜åŒ–ï¼Œå› æ­¤ç»§ç»­è¿è¡Œæµ‹è¯•æ˜¯æ²¡æœ‰æ„ä¹‰çš„ã€‚\n\næ‰§è¡Œä¸Šè¿°æ“ä½œçš„ä»£ç å¤§è‡´å¦‚ä¸‹ï¼š\n\n```python\npublic static async Task<string> VerifyPageLocatorFromAiAsync(string pomFileContent, string htmlPageSource)\n{\n    ChatClient client = new(model: \"gpt-4o-mini\", apiKey);\n    \n    var chatMessage = $\"Verify if locators from this Selenium POM class: {pomFileContent} match this page source: {htmlPageSource}\\\", only return True or False result\";\n\n    ChatCompletion completion = await client.CompleteChatAsync(chatMessage);\n\n    return completion.Content.FirstOrDefault().Text;\n}\n```\nä¸Šè¿°ä»£ç åªæ˜¯è¯¾ç¨‹ä¸­æ¶‰åŠçš„å¤§é‡ä»£ç çš„ä¸€éƒ¨åˆ†ï¼Œä½†ä½ å¯ä»¥çœ‹åˆ°å¦‚ä½•ç®€å•åœ°æ‰§è¡Œå°†é¡µé¢ä¸Ž Selenium çš„é¡µé¢å¯¹è±¡æ¨¡åž‹ä»£ç è¿›è¡Œåˆ†æžçš„æ“ä½œã€‚\n\n## GenAIåœ¨è½¯ä»¶æµ‹è¯•è¯¾ç¨‹ä¸­\n\n*ä»¥ä¸Šè®¨è®ºåªæ˜¯æˆ‘åœ¨Udemyæ–°è¯¾ç¨‹â€œ[**åœ¨è½¯ä»¶è‡ªåŠ¨åŒ–æµ‹è¯•ä¸­ä½¿ç”¨ç”Ÿæˆæ€§AI**](https://proxy.rifx.online/https://www.udemy.com/course/generative-ai-in-software-automation-testing/)â€ä¸­è®¨è®ºä¸»é¢˜çš„ä¸€éƒ¨åˆ†*\n\nä»¥ä¸‹æ˜¯è¯¾ç¨‹å†…å®¹\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*lHe_b7qVqUQo-9Y5.png)\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*rMrsbB2IaKPbthdAr9Rc9g.png)\n\nè¯¥è¯¾ç¨‹ç›®å‰åœ¨Udemyä¸Šä»¥ä¼˜æƒ ä»·æ ¼æä¾›ï¼Œä½œä¸ºé¦–å‘ä¼˜æƒ ï¼Œè¯·åœ¨è´­ä¹°è¯¾ç¨‹æ—¶ä½¿ç”¨ä¼˜æƒ ç **EA\\_NOV\\_24 âš¡ï¸**ã€‚\n\nå¦‚æžœä¼˜æƒ ç å·²è¿‡æœŸï¼Œè¯·éšæ—¶åœ¨æ­¤å¸–å­ä¸‹ç•™è¨€ï¼Œæˆ‘ä¼šå°†æœ€æ–°çš„å¯ç”¨ä¼˜æƒ ç å‘é€ç»™æ‚¨ã€‚\n\n"},{"lang":"zh","group":"blog","slug":"blog/case-study-turning-doctor-transcripts-into-temporal-medical-record-knowledge-graphs-cf624d4927eb","frontmatter":{"title":"æ¡ˆä¾‹ç ”ç©¶ï¼šå°†åŒ»ç”Ÿç¬”å½•è½¬åŒ–ä¸ºæ—¶æ€åŒ»ç–—è®°å½•çŸ¥è¯†å›¾è°±","meta_title":"æ¡ˆä¾‹ç ”ç©¶ï¼šå°†åŒ»ç”Ÿç¬”å½•è½¬åŒ–ä¸ºæ—¶æ€åŒ»ç–—è®°å½•çŸ¥è¯†å›¾è°±","description":"å±•ç¤ºæ•°æ®è½¬æ¢è¿‡ç¨‹ã€æ¶‰åŠçš„ 25 ä¸ªå¼€å‘å°æ—¶çš„æ˜Žç»†ã€ä½¿ç”¨çš„æ¨¡å¼ã€é—®é¢˜å’Œå›žå¤ä»¥åŠåˆ›å»ºçš„å›¾è¡¨","date":"2024-11-13T01:22:29.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*DUNtg-0w2z-vlF9SCvt5UA.png","categories":["Health","Data Science","Machine Learning"],"author":"Rifx.Online","tags":["transcripts","Temporal","Knowledge","Graphs","vector"],"draft":false,"slug":"blog/case-study-turning-doctor-transcripts-into-temporal-medical-record-knowledge-graphs-cf624d4927eb"},"content":"\n\n\næ‚¨æ˜¯å¦æœ‰å…´è¶£å°†åŒ»ç”Ÿ/æ‚£è€…çš„åŒ»ç–—è®°å½•å’Œè®°å½•è½¬åŒ–ä¸ºå¯ä»¥è·¨å¤šä¸ªåŒ»ç–—åŽ†å²ã€æ—¶é—´æ®µå’Œæ‚£è€…è¿›è¡Œå¤æ‚æŸ¥è¯¢çš„æ—¶é—´æ€§çŸ¥è¯†å›¾è°±ï¼Ÿ\n\nåœ¨æœ¬æ¡ˆä¾‹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•å°†åŒ»ç–—è®°å½•è½¬åŒ–ä¸ºæ‚¨å¯ä»¥ä¾èµ–äºŽ RAG å’Œåˆ†æžç›®çš„çš„æ—¶é—´æ€§çŸ¥è¯†å›¾è°±ã€‚æˆ‘ä»¬å±•ç¤ºäº†é’ˆå¯¹è¯¥ç³»ç»Ÿçš„çœŸå®žé—®ç­”ï¼Œä»¥åŠæ‚¨å¯ä»¥é€šè¿‡è¯¥ç³»ç»Ÿå®žçŽ°çš„ä¸šåŠ¡æˆæžœã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™é‡Œæ­¥éª¤çš„ç»„åˆæ˜¯ä¸€ç§ç›¸å¯¹æ–°é¢–çš„çŸ¥è¯†å›¾è°±å®žçŽ°ã€‚\n\n### ä½¿ç”¨çš„æ•°æ®\n\nå‡ºäºŽæ•°æ®éšç§åŽŸå› ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªåˆæˆçš„åŒ»ç–—è®°å½•æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ˜¯æˆ‘ä»¬ä»ŽSyntheaæ•°æ®ä¸­åˆ›å»ºçš„ï¼Œé“¾æŽ¥åœ¨æ­¤ï¼š[https://synthea.mitre.org/downloads](https://proxy.rifx.online/https://synthea.mitre.org/downloads)ã€‚ä»¥ä¸‹æ˜¯ç”¨äºŽçŸ¥è¯†å›¾è°±åˆ›å»ºçš„è¾“å…¥æ•°æ®ä¹‹ä¸€çš„åŒ»ç–—è®°å½•ç¤ºä¾‹ã€‚æˆ‘ä»¬å°†è¿™äº›è®°å½•æ•°æ®ä¸ŽSyntheaæ•°æ®ä¸­çš„ç»“æž„åŒ–åŒ»ç–—è®°å½•ç»“åˆåœ¨ä¸€èµ·ã€‚æˆ‘ä»¬æœ‰å¤§çº¦75ä»½è®°å½•ï¼Œæ¶µç›–äº†10ä½æ‚£è€…ï¼ˆå³æ¯ä½æ‚£è€…æœ‰5-10ä»½è®°å½•ï¼‰ã€‚ä»¥ä¸‹æ˜¯ä½¿ç”¨çš„è®°å½•ç¤ºä¾‹ï¼š\n\n\n\n## æ–°é¢–çŸ¥è¯†å›¾è°±æž¶æž„æ¦‚è¿°\n\n### èŠ‚ç‚¹ï¼š\n\næˆ‘ä»¬æœ‰5ç§ç±»åž‹çš„èŠ‚ç‚¹ï¼šPatientã€Observationã€Immunizationã€Conditionå’ŒEncounter Type\n\n### Triples (æ ·æœ¬åˆ—è¡¨):\n\nPatient \\-\\> Had Encounter \\-\\> Encounter\n\nPatient \\-\\> Has Condition \\-\\> Condition\n\nPatient \\-\\> Received \\-\\> Immunization\n\nPatient \\-\\> Has Measurement \\-\\> Observation\n\n### Chunks:\n\nChunks æ˜¯ç‹¬ç«‹çš„æ–‡æœ¬å—ã€‚Chunks ä¸Žæ¯ä¸ª Triple ç›¸å…³è”ï¼Œå¹¶ä¸”å¯ä»¥æœ‰å¤šä¸ª Chunks å…³è”åˆ°å•ä¸ª Tripleã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒChunks ä¸æ˜¯ Triple çš„éžç»“æž„åŒ–æ¥æºï¼Œè€Œæ˜¯ä¸Žæ¯ç§ Triple ç±»åž‹ç›¸å…³çš„æ‘˜è¦å’Œå…³é”®ç‚¹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æœ‰ 6 ç§ç±»åž‹çš„ Chunksï¼š- æ‚£è€…äººå£ç»Ÿè®¡ Chunksã€ç—…æƒ…æ‘˜è¦ Chunksã€å°±è¯Š Chunksã€è§‚å¯Ÿ Chunksã€å…ç–«æŽ¥ç§ Chunks å’Œç—…æƒ…è¯¦ç»† Chunksã€‚\n\nä¸åŒç±»åž‹çš„ Chunks å…³è”åˆ° Triples çš„ç¤ºä¾‹å¦‚ä¸‹ï¼š\n\n```python\n1. Patient -> EncounterType\nTriple: (Patient) -[had_encounter]-> (EncounterType)\n- Chunk_ids link to specific visit instances\n- Example Chunk: \"Annual physical on 2024â€“01â€“15. BP 120/80, routine screenings \nupdated.\"\n\n2. Patient -> Condition\nTriple: (Patient) -[has_condition]-> (Condition)\n- Chunk_ids link to condition episodes\n- Example Chunk: \"Diagnosed with hypertension on 2020â€“03â€“10. Status: active. \nManaged with medication.\"\n\n3. Patient -> Immunization\nTriple: (Patient) -[received]-> (Immunization)\n- Chunk_ids link to administration records\n- Example Chunk: \"Influenza vaccine administered on 2024â€“01â€“15.\"\n\n4. Patient -> Observation\nTriple: (Patient) -[has_measurement]-> (Observation)\n- Chunk_ids link to measurement instances\n- Example Chunk: \"2024â€“01â€“15: Blood Pressure 120/80 mmHg, Weight 70kg.\"\n```\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*8dH_7tP6xheCaW6K)\n\n**Link to the Graph created: [https://proxy.rifx.online/https://main\\-\\-whyhowai.netlify.app/public/graph/673032011997e08c8849316c](https://proxy.rifx.online/https://main--whyhowai.netlify.app/public/graph/673032011997e08c8849316c)**\n\né€šè¿‡è¿™ç§ç‰¹å®šçš„å›¾å½¢æž¶æž„ï¼Œæ‚¨å¯ä»¥å°†å…³é”®ç‚¹å’Œæ‘˜è¦ä¸Ž Triples å…³è”ï¼Œç„¶åŽå¯ä»¥ä¸“æ³¨äºŽé€šè¿‡éžç»“æž„åŒ–æœç´¢æ‰¾åˆ°æ­£ç¡®çš„ä¸€ç»„ Triplesï¼Œå¹¶éšåŽé€šè¿‡ç»“æž„åŒ–æ–¹å¼å¼•å…¥æ‰€æœ‰ç›¸å…³çš„å…³é”®ä¿¡æ¯ã€‚\n\n## ç‹¬ç‰¹çš„WhyHowæž¶æž„\n\næœ‰ä¸€äº›ç‹¬ç‰¹çš„WhyHowå›¾å½¢åŸºç¡€è®¾æ–½ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿä»¥ç®€å•çš„æ–¹å¼æž„å»ºæ­¤æž¶æž„ã€‚\n\né¦–å…ˆï¼ŒTriplesé€šè¿‡å‘é‡æœç´¢åµŒå…¥å’Œæ£€ç´¢ï¼Œé¿å…äº†å¸¸è§çš„æ£€ç´¢é—®é¢˜ï¼Œå³å¿…é¡»ä½¿ç”¨Text2Cypheræ¥è¯†åˆ«èŠ‚ç‚¹ã€å…³ç³»ï¼Œç„¶åŽæž„å»ºCypheræŸ¥è¯¢ï¼Œä»¥æ‰¾åˆ°æ­£ç¡®çš„Tripleã€‚è¿™å·²è¢«è¯æ˜Žå¯ä»¥æ˜¾è‘—[æé«˜æ£€ç´¢å‡†ç¡®æ€§è¾¾3å€](https://proxy.rifx.online/https://readmedium.com/knowledge-table-multi-document-rag-extraction-memory-ec08450e858f)ã€‚\n\nå…¶æ¬¡ï¼ŒTriplesæ˜¯WhyHowä¸­çš„ç‹¬ç«‹å¯¹è±¡ï¼Œæ‚¨å¯ä»¥å°†å—é“¾æŽ¥åˆ°è¿™äº›å¯¹è±¡ã€‚è¿™ä½¿æ‚¨èƒ½å¤Ÿæç‚¼æ¯ä¸ªTripleæƒ³è¦æ£€ç´¢çš„å…³é”®ä¿¡æ¯ï¼Œå¹¶åœ¨æ‰¾åˆ°æ­£ç¡®çš„TriplesåŽç›´æŽ¥å°†å…¶å¼•å…¥ä¸Šä¸‹æ–‡ã€‚è¿™é¿å…äº†å¿…é¡»ä»¥å›¾å½¢æ ¼å¼è¡¨ç¤ºå…³é”®çš„ä¿¡æ¯å’Œä¸Šä¸‹æ–‡ï¼ˆä½¿å¾—æ¨¡å¼æž„å»ºè¿‡ç¨‹å¤æ‚åŒ–ï¼‰ï¼Œå¹¶åœ¨åˆå§‹çš„éžç»“æž„åŒ–å‘é‡æœç´¢åŽä»¥ç»“æž„åŒ–çš„æ–¹å¼å¼•å…¥ä¿¡æ¯ã€‚è¿™åœ¨è¿‡ç¨‹ä¸Šç±»ä¼¼äºŽ[LinkedInå¯¹çŸ¥è¯†å›¾çš„åº”ç”¨](https://proxy.rifx.online/https://readmedium.com/5-misconceptions-of-kg-rag-systems-building-using-rag-native-graphs-5e47872e7903)ï¼Œåœ¨ä»–ä»¬çš„ç³»ç»Ÿä¸­ï¼Œåƒâ€œé‡çŽ°æ­¥éª¤â€è¿™æ ·çš„å…³é”®ä¿¡æ¯ä»¥ç±»ä¼¼çš„æ–¹å¼è¡¨ç¤ºå’Œæ£€ç´¢ï¼Œè€Œè¿™äº›æ­¥éª¤æœ¬èº«åˆ™è¢«è¡¨ç¤ºä¸ºå•ç‹¬çš„â€œå—â€/â€œèŠ‚ç‚¹â€ã€‚\n\nç¬¬ä¸‰ï¼ŒWhyHowæŽ¥å—JSONæ ¼å¼çš„æ•°æ®ï¼Œè¿™å…è®¸ä»»ä½•æå–æ¡†æž¶ä¸Žå›¾å½¢åˆ›å»ºä¹‹é—´æ— ç¼äº’åŠ¨ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä½¿ç”¨Claudeå°†è½¬å½•æ•°æ®åˆæ­¥è½¬æ¢ä¸ºå¿…è¦çš„JSONç»“æž„ï¼Œä»¥åŠ è½½åˆ°WhyHowä¸­ã€‚å¦‚æžœæ‚¨å·²ç»æœ‰ä¿¡æ¯ä»¥JSONæ ¼å¼å­˜åœ¨ï¼Œé‚£ä¹ˆå°†æ•°æ®åŠ è½½åˆ°WhyHowä¸­å°±å®¹æ˜“å¤šäº†ã€‚\n\nç¬¬å››ï¼Œç”±äºŽWhyHowç³»ç»Ÿä¸­å—å’Œæ£€ç´¢è¿‡ç¨‹çš„è®¾è®¡æ–¹å¼ï¼Œæ‚¨å¯ä»¥è½»æ¾åŒ…å«å¯ä»¥ç”¨äºŽç®¡ç†ç­”æ¡ˆæž„å»ºæ–¹å¼çš„æ—¶é—´æ•°æ®ã€‚æ—¶é—´æ•°æ®åœ¨çŸ¥è¯†å›¾ä¸­ä¸€ç›´æ˜¯ä¸€ä¸ªéš¾ä»¥å»ºæ¨¡çš„å†…å®¹ï¼ˆä»¥è‡³äºŽé¢†å…ˆçš„KGä¸“å®¶é€šå¸¸å»ºè®®é¿å…ï¼‰ï¼Œä½†å®ƒæ˜¾ç„¶æ˜¯å·¥ä½œæµçš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚å³ä½¿å°è¯•å»ºæ¨¡æ—¶é—´æ•°æ®çš„çŽ°æœ‰æ–¹æ³•ä¹Ÿè¯•å›¾å°†å…¶æ‘„å–åˆ°çŸ¥è¯†å›¾ä¸­ï¼Œç„¶åŽåŸºäºŽç»“æž„åŒ–çš„CypheræŸ¥è¯¢è¿›è¡Œæ£€ç´¢ï¼Œè€Œä¸æ˜¯æˆ‘ä»¬ç‹¬ç‰¹ä½¿ç”¨LLMæ¥å¸®åŠ©è¿‡æ»¤æ—¶é—´æ•°æ®çš„æž¶æž„ã€‚\n\nå°†LLMçš„å¼ºå¤§åŠŸèƒ½ä¸ŽçŸ¥è¯†å›¾ç­‰ç»“æž„åŒ–çŸ¥è¯†è¡¨ç¤ºç»“åˆèµ·æ¥ï¼Œæ˜¯å®žçŽ°ä¸šåŠ¡æˆæžœçš„é‡è¦æ–¹å¼ï¼Œæˆ‘ä»¬è®¤ä¸ºè¿™ç§æ—¶é—´çŸ¥è¯†å›¾æž¶æž„å°†é€šè¿‡æˆåŠŸå®žæ–½æ—¶é—´æ•°æ®æ¥å¸®åŠ©é‡Šæ”¾å¤§é‡ä¸šåŠ¡ä»·å€¼ã€‚\n\n### æ•°æ®è½¬æ¢è¿‡ç¨‹\n\né¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨Claudeå°†è½¬å½•ä¿¡æ¯è½¬æ¢ä¸ºä¸Žæ¨¡å¼å¯¹é½çš„æ¯ä¸ªè½¬å½•çš„æ•°æ®ä¿¡æ¯ã€‚ç»“åˆç»“æž„åŒ–åŒ»ç–—è®°å½•çš„ä¿¡æ¯ï¼Œè½¬å½•è¢«è½¬åŒ–ä¸ºå¦‚ä¸‹æ‰€ç¤ºçš„JSONæ‘˜è¦ï¼š\n\n```python\nPATIENT SUMMARY\nName: Joseph Crona\nDOB: 2022â€“08â€“29\nAge: 2 years\nGender: male\nMRN: #dbfbaa\n\nCURRENT MEASUREMENTS (as of 2024â€“08â€“05)\nHeight: 84.1cm (50th percentile)\nWeight: 14.5kg (52nd percentile)\nALLERGIES\nNo known allergies\n\nIMMUNIZATIONS\n- DTaP: 2022â€“12â€“05, 2023â€“02â€“06, 2023â€“03â€“06, 2024â€“02â€“05\n- Hepatitis A: 2023â€“11â€“06\n- Hepatitis B: 2022â€“08â€“29, 2022â€“10â€“03, 2023â€“03â€“06\n- Hib: 2022â€“12â€“05, 2023â€“02â€“06, 2023â€“11â€“06\n- Influenza: 2023â€“03â€“06, 2024â€“08â€“05\n- MMR: 2023â€“11â€“06\n- PCV13: 2022â€“12â€“05, 2023â€“02â€“06, 2023â€“03â€“06, 2023â€“11â€“06\n- Polio: 2022â€“12â€“05, 2023â€“02â€“06, 2023â€“03â€“06\n- Rotavirus: 2022â€“12â€“05, 2023â€“02â€“06\n- Varicella: 2023â€“11â€“06\n\nMEDICAL HISTORY\n- Viral sinusitis (disorder)\nOnset: 2023â€“03â€“13\nStatus: resolved\nOutcome: Resolved\n\nGROWTH & DEVELOPMENT\n- 2023â€“11â€“06: Body Weight: 12.7 kg\n- 2024â€“02â€“05: Body Height: 79 cm\n- 2024â€“02â€“05: Body Weight: 13.4 kg\n- 2024â€“08â€“05: Body Height: 84.1 cm\n- 2024â€“08â€“05: Body Weight: 14.5 kg\nDevelopment: Age-appropriate milestones met\n- Gross motor: Age appropriate\n- Fine motor: Age appropriate\n- Language: Age appropriate\n- Social: Age appropriate\n\nPREVENTIVE CARE\nWell-Child Visits:\n- 2024â€“08â€“05: 2yo well visit - Development on track\n- 2024â€“02â€“05: 1yo well visit - Development on track\n- 2023â€“11â€“06: 1yo well visit - Development on track\n- 2023â€“08â€“07: 1yo well visit - Development on track\n- 2023â€“05â€“08: 9mo well visit - Age appropriate exam completed\n- 2023â€“02â€“06: 6mo well visit - Age appropriate exam completed\n- 2022â€“12â€“05: 4mo well visit - Age appropriate exam completed\n- 2022â€“10â€“03: 2mo well visit - Age appropriate exam completed\n- 2022â€“08â€“29: Newborn visit - Normal exam\n\nFAMILY HISTORY\nMother: Healthy\nFather: Healthy\nSiblings: None documented\n\nSOCIAL HISTORY\nLiving Situation: Lives with parents\nDevelopment: Meeting age-appropriate milestones\nSleep: Age-appropriate pattern\nNutrition: Age-appropriate diet\n```\nå…¶æ¬¡ï¼Œæˆ‘ä»¬å°†æ­¤JSONæ¨¡å¼æ˜ å°„åˆ°WhyHowæ¨¡å¼ï¼Œç„¶åŽå°†æ‰€æœ‰ä¿¡æ¯å¯¼å…¥WhyHow.AI KG Studioã€‚\n\nä»¥ä¸‹æ˜¯æœ€ç»ˆåŠ è½½åˆ°WhyHowä¸­çš„KGç»“æž„ç¤ºä¾‹ã€‚\n\n```python\nKnowledge Graph Structure (Timeless):\n\n\nNodes:\n1. Patient Node\n  Structure: {\n      name: str,         # \"John Smith\"\n      label: \"Patient\",\n      properties: {\n          gender: str,   # FHIR gender\n          patient_type: str  # \"adult\" | \"pediatric\"\n      },\n      chunk_ids: List[str]  # Links to demographic chunks\n  }\n\n\n2. EncounterType Node\n  Structure: {\n      name: str,         # \"Well-child visit\" | \"Annual physical\"\n      label: \"EncounterType\",\n      properties: {\n          category: str,  # \"preventive\" | \"acute\" | \"chronic\"\n          specialty: str  # \"primary_care\" | \"pediatrics\" | \"emergency\"\n      },\n      chunk_ids: List[str]  # Links to visit pattern chunks\n  }\n\n\n3. Condition Node\n  Structure: {\n      name: str,         # \"Essential hypertension\"\n      label: \"Condition\",\n      properties: {\n          category: str,     # \"chronic\" | \"acute\" | \"resolved\"\n          system: str,       # \"respiratory\" | \"cardiovascular\" | etc\n          is_primary: bool   # True if primary diagnosis\n      },\n      chunk_ids: List[str]  # Links to condition history chunks\n  }\n\n\n4. Immunization Node\n  Structure: {\n      name: str,         # \"DTaP\" | \"MMR\"\n      label: \"Immunization\",\n      properties: {\n          series: str,       # \"primary\" | \"booster\"\n          target: str        # \"tetanus\" | \"measles\" | etc\n      },\n      chunk_ids: List[str]  # Links to immunization records\n  }\n\n\n5. Observation Node\n  Structure: {\n      name: str,         # \"Blood Pressure\" | \"Height\"\n      label: \"Observation\",\n      properties: {\n          category: str,     # \"vital\" | \"lab\" | \"growth\"\n          unit: str         # \"mmHg\" | \"cm\" | etc\n      },\n      chunk_ids: List[str]  # Links to measurement records\n  }\n\n\nRelations:\n1. Patient -> EncounterType\n  Triple: (Patient) -[had_encounter]-> (EncounterType)\n  - Chunk_ids link to specific visit instances\n\n\n2. Patient -> Condition\n  Triple: (Patient) -[has_condition]-> (Condition)\n  - Chunk_ids link to condition episodes\n\n\n3. Patient -> Immunization\n  Triple: (Patient) -[received]-> (Immunization)\n  - Chunk_ids link to administration records\n\n\n4. Patient -> Observation\n  Triple: (Patient) -[has_measurement]-> (Observation)\n  - Chunk_ids link to measurement instances\n\n\n5. Condition -> EncounterType\n  Triple: (Condition) -[managed_in]-> (EncounterType)\n  - Links conditions to typical encounter types\n\n\n6. Immunization -> EncounterType\n  Triple: (Immunization) -[given_during]-> (EncounterType)\n  - Links vaccines to visit types\n```\nç¬¬ä¸‰ï¼Œæˆ‘ä»¬è¿è¡Œä¸€ä¸ªè‡ªå®šä¹‰æç¤ºï¼Œä»¥åœ¨æ¯æ¬¡è‡ªç„¶è¯­è¨€æŸ¥è¯¢åŽå¯¹ä»ŽçŸ¥è¯†å›¾è°±ä¸­æ£€ç´¢çš„ä¸‰å…ƒç»„è¿›è¡Œä¸Šä¸‹æ–‡åŒ–ã€‚\n\nåœ¨è¿™ç§æž¶æž„ä¸‹ï¼Œä¸€ä¸ªæœ‰è¶£çš„äº‹æƒ…æ˜¯ï¼Œæˆ‘ä»¬çŽ°åœ¨å¯ä»¥ç»§ç»­å‘çŸ¥è¯†å›¾è°±ä¸­æ·»åŠ æœ‰å…³æ‚£è€…å°±è¯Šã€æ‚£è€…æ²»ç–—å’Œç—…æƒ…çš„ä¿¡æ¯ï¼Œå› ä¸ºè¿™åªæ˜¯å°†é¢å¤–çš„å—æ·»åŠ åˆ°çŽ°æœ‰ä¸‰å…ƒç»„ä¸­çš„é—®é¢˜ã€‚å¦‚æžœæ‚£è€…å¾—äº†æ–°ç–¾ç—…ï¼Œåˆ™ä¼šå‘æ‚£è€…èŠ‚ç‚¹æ·»åŠ é¢å¤–çš„ConditionèŠ‚ç‚¹ã€‚\n\nè¿™ä¸ªè¿‡ç¨‹èŠ±è´¹äº†25ä¸ªå¼€å‘å°æ—¶ï¼Œå¯ä»¥åˆ†è§£ä¸ºä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š\n\n* 2å°æ—¶ï¼ˆ8%ï¼‰ç”¨äºŽæŸ¥çœ‹å’Œç†è§£æ•°æ®ï¼ˆæŽ¢ç´¢æ€§æ•°æ®åˆ†æžï¼‰\n* 18å°æ—¶ï¼ˆ72%ï¼‰ç”¨äºŽè¿­ä»£æ¨¡å¼ï¼Œå¼„æ¸…æ¥šå›¾ä¸­åº”è¯¥åŒ…å«å“ªäº›èŠ‚ç‚¹ï¼Œå“ªäº›èŠ‚ç‚¹åº”è¯¥è¿žæŽ¥åˆ°ä»€ä¹ˆï¼Œåº”è¯¥å­˜åœ¨å“ªäº›å—ï¼Œå¦‚ä½•è¿žæŽ¥åˆ°å„ç§ä¸‰å…ƒç»„ï¼Œä½¿ç”¨ä¸€ç»„é—®é¢˜æµ‹è¯•æ£€ç´¢çš„ç­”æ¡ˆï¼Œå¹¶ç›¸åº”åœ°è¿›è¡Œè¿­ä»£ã€‚\n* 2å°æ—¶ï¼ˆ8%ï¼‰ç”¨äºŽç¼–å†™åˆ›å»ºè¦åŠ è½½çš„ä¸‰å…ƒç»„çš„ä»£ç \n* 3å°æ—¶ï¼ˆ12%ï¼‰ç”¨äºŽç¼–å†™éªŒè¯æ£€æŸ¥å’Œè¾“å‡ºæ£€æŸ¥ä»¥æ•èŽ·ä»»ä½•é”™è¯¯\n\n### é—®é¢˜ä¸Žç­”æ¡ˆï¼šåŒ»ç–—è®°å½•çŸ¥è¯†å›¾è°±\n\nç”¨äºŽåœ¨ä»ŽçŸ¥è¯†å›¾è°±ä¸­æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡åŽæž„å»ºç­”æ¡ˆçš„æç¤ºï¼Œä½¿ç”¨WhyHowè‡ªç„¶è¯­è¨€å›¾æŸ¥è¯¢å¼•æ“Ž\n\n```python\n    You are an AI assistant specializing in medical records analysis. \nUse the following information to answer the user's question. \n    The information is derived from a knowledge graph of patient medical records.\n\n    Relevant Nodes (these represent patients, encounters, and conditions):\n    {node_context}\n\n    Relevant Relationships (these show connections between patients, \nencounters, and conditions):\n    {triple_context}\n\n    Relevant Medical Records:\n    {chunk_context}\n\n    Answer the question using the following steps:\n    1. Analyze the medical information, focusing on the specific topic of the \nquestion\n    2. Identify key medical events, conditions, and temporal relationships\n    3. Formulate a clear and concise answer that directly addresses the \nquestion\n    4. If there are any uncertainties in the medical information, mention \nthem explicitly\n    5. If you don't have enough information to answer confidently, say so\n    6. Consider temporal relationships between medical events\n    7. It's currently {datetime.now().strftime(\"%Y-%m-%d\")}. Consider this \nwhen interpreting dates\n    8. Maintain patient privacy by using only first names or initials when \nnecessary\n\n    Question: {question}\n```\n\n## ä¸šåŠ¡ä»·å€¼ï¼šæ‚£è€…æ²»ç–—åˆ†æž\n\n### åŽ†å²æ‚£è€…åˆ†æž\n\né—®é¢˜ï¼š**Joseph Cronaåœ¨å…¶ç¬¬ä¸€å¹´æŽ¥å—äº†å“ªäº›ç±»åž‹çš„å¥åº·å„¿ç«¥è®¿è§†å’Œå…ç–«æŽ¥ç§ï¼Ÿ**\n\nå›žç­”ï¼š\n\n```python\nTo determine the types of well-child visits and immunizations Joseph Crona \nreceived in his first year, we need to consider the relevant medical events \nand their timing. Joseph was born in 2022, and it is currently November 9, 2024.\nTherefore, his first year would span from 2022 to 2023.\n\n**Well-Child Visits:**\n- Joseph had an encounter categorized as a \"Well child visit,\" which falls \nunder preventive care in pediatrics. This type of visit is typically part of \nroutine check-ups for children, especially in their first year.\n\n**Immunizations:**\nDuring his first year, Joseph received the following immunizations, all of \nwhich are common for pediatric patients:\n1. **Hep B, adolescent or pediatric** - Completed\n2. **Hib (PRP-OMP)** - Completed\n3. **Rotavirus, monovalent** - Completed\n4. **IPV (Inactivated Poliovirus Vaccine)** - Completed\n5. **DTaP (Diphtheria, Tetanus, and Pertussis)** - Completed\n6. **Pneumococcal conjugate PCV 13** - Completed\n\nThese immunizations are typically administered during the first year of life \nas part of the standard immunization schedule for infants.\n\nIn summary, Joseph Crona received a well-child visit and completed a series of\nstandard pediatric immunizations during his first year.\n```\né—®é¢˜ï¼š**å¯¹äºŽä¸­è€³ç‚Žæ‚£è€…ï¼Œè®°å½•çš„æŽ¥è§¦å’Œè§‚å¯Ÿçš„é¡ºåºæ˜¯ä»€ä¹ˆï¼Ÿ**\n\nå›žç­”ï¼š\n\n```python\nTo address the question regarding the sequence of encounters and observations \nfor patients with otitis media, we will focus on the medical records of Miles \nDickinson, as he is the patient identified with this condition.\n\n1. **Condition Onset:**\n- Miles Dickinson was diagnosed with otitis media on June 10, 2024. The \ncondition is currently resolved.\n\n2. **Encounters:**\n- Miles had two types of encounters recorded:\n- **Well Child Visit:** This is categorized as a preventive encounter in \npediatrics.\n- **Encounter for Symptom:** This is categorized as a follow-up encounter in \nprimary care.\n\n3. **Observations:**\n- Various observations were recorded for Miles, including:\n- **Pain Severity:** Recorded multiple times with scores ranging from 0 to 4.\n- **Body Weight:** Recorded with values ranging from 3.2 kg to 8.2 kg.\n- **Head Occipital-frontal Circumference and Percentile:** Recorded with \nspecific values in cm and percentiles.\n- **Heart Rate:** Recorded with values ranging from 61 to 100 beats per minute.\n- **Respiratory Rate:** Recorded with values ranging from 12 to 16 breaths per\nminute.\n\n4. **Temporal Sequence:**\n- The otitis media diagnosis occurred on June 10, 2024. Observations and \nencounters before this date may have been related to other conditions or \nroutine check-ups. Observations and encounters after this date could be \nrelated to follow-up care for otitis media or other health assessments.\n\n5. **Conclusion:**\n- The sequence of encounters and observations for Miles Dickinson includes \npreventive and follow-up encounters, along with various health observations. \nThe specific sequence related to otitis media is not detailed in terms of exact\ndates for each observation or encounter, but the diagnosis date provides a \ntemporal marker for understanding subsequent medical activities.\n\nIf more detailed temporal data on each observation and encounter were \navailable, a more precise sequence could be established. However, based on \nthe current information, this is the overview of the sequence related to \notitis media for Miles Dickinson.\n```\n\n### åŽ†å²å¤šæ‚£è€…åˆ†æž\n\né—®é¢˜ï¼š**æœ‰å¤šå°‘æ‚£è€…è¢«è¯Šæ–­ä¸ºæ€¥æ€§æ”¯æ°”ç®¡ç‚Žï¼Œå…¸åž‹çš„å‘ä½œæ¨¡å¼æ˜¯ä»€ä¹ˆï¼Ÿ**\n\nç­”æ¡ˆï¼š\n\n```python\næ ¹æ®æä¾›çš„åŒ»ç–—è®°å½•ï¼Œæœ‰ä¸¤ä½æ‚£è€…è¢«è¯Šæ–­ä¸ºæ€¥æ€§æ”¯æ°”ç®¡ç‚Žï¼šMaximo Swaniawski å’Œ Bennie Hermanã€‚\n\n#### å‘ä½œæ¨¡å¼ï¼š\n- **Maximo Swaniawski**ï¼š\n- æ€¥æ€§æ”¯æ°”ç®¡ç‚Žçš„å‘ä½œæ—¥æœŸï¼š\n- 2014å¹´10æœˆ24æ—¥\n- 2015å¹´11æœˆ7æ—¥\n- 2018å¹´11æœˆ29æ—¥\n- **Bennie Herman**ï¼š\n- æ€¥æ€§æ”¯æ°”ç®¡ç‚Žçš„å‘ä½œæ—¥æœŸï¼š\n- 2014å¹´10æœˆ24æ—¥\n- 2015å¹´11æœˆ7æ—¥\n- 2018å¹´11æœˆ29æ—¥\n\n#### å…¸åž‹å‘ä½œæ¨¡å¼ï¼š\nä¸¤ä½æ‚£è€…çš„å‘ä½œæ—¥æœŸç›¸åŒï¼Œè¡¨æ˜Žæ€¥æ€§æ”¯æ°”ç®¡ç‚Žå‘ä½œçš„æ¨¡å¼å‘ç”Ÿåœ¨10æœˆåº•æˆ–11æœˆã€‚è¿™å¯èƒ½è¡¨æ˜Žä¸€ç§å­£èŠ‚æ€§æ¨¡å¼ï¼Œå¯èƒ½ä¸Žè¾ƒå†·çš„å¤©æ°”æˆ–å…¶ä»–å¯èƒ½å¯¼è‡´å‘¼å¸ç³»ç»Ÿç–¾ç—…çš„å­£èŠ‚æ€§å› ç´ æœ‰å…³ã€‚\n\nä¸¤ä½æ‚£è€…çš„ç—…æƒ…å·²å¾—åˆ°è§£å†³ï¼Œè¡¨æ˜Žæ€¥æ€§æ”¯æ°”ç®¡ç‚Žçš„å‘ä½œæ˜¯æ€¥æ€§çš„ï¼Œè€Œéžæ…¢æ€§çš„ã€‚\n```\né—®é¢˜ï¼š**å“ªäº›æ‚£è€…åŒæ—¶æ‚£æœ‰ç—…æ¯’æ€§é¼»çª¦ç‚Žå’Œæ”¯æ°”ç®¡ç‚Žï¼Œä»–ä»¬æœ€æ—©çš„å‘ä½œæ—¥æœŸæ˜¯ä»€ä¹ˆï¼Ÿ**\n\nç­”æ¡ˆï¼š\n\n```python\nè¦ç¡®å®šå“ªäº›æ‚£è€…åŒæ—¶æ‚£æœ‰ç—…æ¯’æ€§é¼»çª¦ç‚Žå’Œæ”¯æ°”ç®¡ç‚Žï¼Œæˆ‘ä»¬éœ€è¦åˆ†æžæ¯ä½æ‚£è€…çš„ç›¸å…³ç–¾ç—…åŠå…¶å‘ä½œæ—¥æœŸã€‚\n\n1. **Maximo Swaniawski**ï¼š\n- ç–¾ç—…ï¼šæ€¥æ€§æ”¯æ°”ç®¡ç‚Ž\n- æ”¯æ°”ç®¡ç‚Žçš„æœ€æ—©å‘ä½œæ—¥æœŸï¼š2014å¹´10æœˆ24æ—¥\n2. **Cassandra Monahan**ï¼š\n- ç–¾ç—…ï¼šç—…æ¯’æ€§é¼»çª¦ç‚Ž\n- é¼»çª¦ç‚Žçš„æœ€æ—©å‘ä½œæ—¥æœŸï¼š2014å¹´12æœˆ12æ—¥\n3. **Bennie Herman**ï¼š\n- ç–¾ç—…ï¼šæ€¥æ€§æ”¯æ°”ç®¡ç‚Ž\n- æ”¯æ°”ç®¡ç‚Žçš„æœ€æ—©å‘ä½œæ—¥æœŸï¼š2014å¹´10æœˆ24æ—¥\n4. **Deangelo Price**ï¼š\n- ç–¾ç—…ï¼šç—…æ¯’æ€§é¼»çª¦ç‚Ž\n- é¼»çª¦ç‚Žçš„æœ€æ—©å‘ä½œæ—¥æœŸï¼š2014å¹´12æœˆ12æ—¥\n5. **Joseph Crona**ï¼š\n- ç–¾ç—…ï¼šç—…æ¯’æ€§é¼»çª¦ç‚Ž\n- é¼»çª¦ç‚Žçš„æœ€æ—©å‘ä½œæ—¥æœŸï¼š2014å¹´12æœˆ12æ—¥\n\næ ¹æ®æä¾›çš„ä¿¡æ¯ï¼Œæ²¡æœ‰æ‚£è€…åŒæ—¶æ‚£æœ‰ç—…æ¯’æ€§é¼»çª¦ç‚Žå’Œæ”¯æ°”ç®¡ç‚Žã€‚æ¯ä½æ‚£è€…ä»…æœ‰ä¸€ç§ç–¾ç—…ï¼Œè€Œæ²¡æœ‰åŒæ—¶å­˜åœ¨ä¸¤ç§ã€‚å› æ­¤ï¼Œæ²¡æœ‰æ‚£è€…åŒæ—¶å…·å¤‡è¿™ä¸¤ç§ç–¾ç—…ï¼Œå› æ­¤ä¹Ÿæ²¡æœ‰å•ä¸€æ‚£è€…çš„æœ€æ—©å‘ä½œæ—¥æœŸã€‚\n```\n\n### ç›¸è¾ƒäºŽä»…å‘é‡ RAG ç³»ç»Ÿçš„ä¼˜åŠ¿\n\nè·¨å¤šä¸ªæ‚£è€…çš„å“è¶Šåˆ†æžï¼š\n\n* åœ¨å…³äºŽå“ªäº›æ‚£è€…åŒæ—¶æ‚£æœ‰æ”¯æ°”ç®¡ç‚Žå’Œç—…æ¯’æ€§é¼»çª¦ç‚Žçš„é—®é¢˜ä¸­ï¼Œæˆ‘ä»¬å¿…é¡»èƒ½å¤ŸçŸ¥é“æŸä¸ªæ‚£è€…æ˜¯å¦è¢«è®°å½•ä¸ºæ‚£æœ‰æˆ–æœªæ‚£æœ‰ç‰¹å®šç–¾ç—…ã€‚ç”±äºŽä»…å‘é‡æœç´¢æ˜¯å…³äºŽè¯†åˆ«ç›¸å…³ç‰‡æ®µçš„ï¼Œå› æ­¤æ— æ³•æ£€æµ‹åˆ°æ‚£è€…æ˜¯å¦ç¡®å®žæ²¡æœ‰æŸç§ç‰¹å®šç–¾ç—…ã€‚è¿™æ„å‘³ç€éœ€è¦ä¸€ä¸ªä¸­ä»‹æ•°æ®èšåˆï¼Œèƒ½å¤Ÿæ˜Žç¡®è¡¨ç¤ºæ‚£è€… X åœ¨å…¶åå­—ä¸‹æ²¡æœ‰â€œç³–å°¿ç—…â€èŠ‚ç‚¹ã€‚\n\nå¤šè½¬å½•æœ¬åˆ†æžä¸Ž RAGï¼š\n\n* åœ¨å…³äºŽè¿ˆå°”æ–¯åŠå…¶è¿‡åŽ»ä¸€å¹´å¤šæ¬¡å°±è¯Šçš„é—®é¢˜ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™äº›æƒ…å†µè·¨è¶Šäº†å¤šæ¬¡å°±è¯Šå’Œå¤šä¸ªè½¬å½•æœ¬ã€‚è¿™æ„å‘³ç€éœ€è¦ä¸€ä¸ªä¸­ä»‹æ•°æ®èšåˆï¼Œèƒ½å¤Ÿæ˜ å°„å‡ºæ‚£è€…åŠå…¶éšæ—¶é—´ç´¯ç§¯çš„å°±è¯Šå’Œè§‚å¯Ÿè®°å½•ã€‚\n\nWhyHow.AI æä¾›ç»“æž„åŒ–çŸ¥è¯†ã€çŸ¥è¯†å›¾è°±å’Œæ›´å¯é çš„ä»£ç† RAG è§£å†³æ–¹æ¡ˆçš„å·¥å…·ã€æœåŠ¡å’Œæµç¨‹ã€‚å¦‚æžœæ‚¨æœ‰å…´è¶£æŽ¢ç´¢æˆ‘ä»¬çš„ä»»ä½•å·¥å…·ï¼ˆ[KG Studio](https://proxy.rifx.online/https://readmedium.com/whyhow-ai-kg-studio-platform-beta-rag-native-graphs-1105e5a84ff2)ï¼Œ[çŸ¥è¯†è¡¨\\[å¼€æº\\]](https://proxy.rifx.online/https://readmedium.com/knowledge-table-multi-document-rag-extraction-memory-ec08450e858f)ï¼‰å’ŒæœåŠ¡ï¼Œè¯·éšæ—¶[ä¸Žæˆ‘ä»¬èŠå¤©](https://proxy.rifx.online/https://calendly.com/whyhowai/intro-call-whyhow-ai)ã€‚\n\nå¦‚æžœæ‚¨æ­£åœ¨è€ƒè™‘ã€æ­£åœ¨è¿›è¡Œæˆ–å·²ç»åœ¨ RAG ä¸­æ•´åˆçŸ¥è¯†å›¾è°±ä»¥æé«˜å‡†ç¡®æ€§ã€è®°å¿†å’Œç¡®å®šæ€§ï¼Œè¯·å…³æ³¨æˆ‘ä»¬çš„æ–°é—»é€šè®¯[WhyHow.AI](https://proxy.rifx.online/https://whyhow.ai/)ï¼Œæˆ–åŠ å…¥æˆ‘ä»¬å…³äºŽ RAG ä¸­è§„åˆ™ã€ç¡®å®šæ€§å’ŒçŸ¥è¯†å›¾è°±çš„è®¨è®ºï¼Œæ¬¢è¿ŽåŠ å…¥æˆ‘ä»¬çš„[Discord](https://proxy.rifx.online/https://discord.gg/9bWqrsxgHr)ã€‚\n\n"},{"lang":"zh","group":"blog","slug":"blog/chatgpt-vision-turns-a-picture-into-1000-words-24858615fa28","frontmatter":{"title":"ChatGPT Vision å°†å›¾ç‰‡è½¬åŒ–ä¸º 1000 ä¸ªå­—","meta_title":"ChatGPT Vision å°†å›¾ç‰‡è½¬åŒ–ä¸º 1000 ä¸ªå­—","description":"ä»¥åŠå¦‚ä½•æŠŠè¿™äº›è¯å˜æˆç”Ÿæ„","date":"2024-11-10T03:51:17.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*lS5aPVDrsCFFnBYz","categories":["Programming","Marketing","Generative AI"],"author":"Rifx.Online","tags":["automation","content","GPT","MAKE","photos"],"draft":false,"slug":"blog/chatgpt-vision-turns-a-picture-into-1000-words-24858615fa28"},"content":"\n\n\n### å¦‚ä½•å°†è¿™äº›æ–‡å­—è½¬åŒ–ä¸ºå•†ä¸šä»·å€¼\n\næˆ‘æœ‰è¿™ä¸ªæƒ³æ³•å¿«åå¹´äº†ã€‚ä¸€åˆ‡å§‹äºŽæˆ‘æ­å»ºç½‘ç«™çš„æ—¶å€™ï¼Œä¸€ä½æ—…é¦†è€æ¿ç»™æˆ‘å‘äº†ä¸€æ ¹è£…æ»¡è¿‘åƒå¼ ç…§ç‰‡çš„Uç›˜ï¼Œè¿˜æœ‰ä¸€ç›’35mmçš„ç…§ç‰‡è®©æˆ‘æ‰«æã€‚\n\n\n\n> è¿™äº›éƒ½æ˜¯**æƒŠäººçš„ç…§ç‰‡**â€”â€”å®¢äººä»¬å±•ç¤ºä»–ä»¬çš„çè´µæ•èŽ·ã€ä»¤äººæƒŠå¹çš„æ¹–æ™¯ï¼Œä»¥åŠå‘å¯¼ä»¬å¸¦é¢†çš„æˆ·å¤–å†’é™©ã€‚\n\n> æˆ‘çŸ¥é“ï¼Œå¦‚æžœæˆ‘ä»¬èƒ½æŠŠè¿™äº›ç…§ç‰‡æ”¾åˆ°ç½‘ä¸Šï¼Œå®ƒä»¬å°†ä¸ºæ—…é¦†åˆ›é€ ä¸€åœº**å£ç¢‘**è¥é”€çš„æµªæ½®ã€‚\n\nä½†äº‹æƒ…å˜å¾—å¤æ‚äº†ï¼šæ¯å¼ ç…§ç‰‡éƒ½éœ€è¦ä¸€ä¸ªç‹¬ç‰¹çš„æè¿°ã€é€‚å½“çš„æ ‡ç­¾ã€ä¸€ç¯‡åšå®¢æ–‡ç« å’Œç¤¾äº¤åª’ä½“çš„ä¸Šä¼ ã€‚è€Œä¸”å‡ ä¹Žæœ‰ä¸€åƒå¼ ç…§ç‰‡ï¼\n\næ‰€éœ€çš„æ—¶é—´å’Œæˆæœ¬ä»¤äººéœ‡æƒŠã€‚ä¸ºæ•°ç™¾å¼ é±¼ç±»ç…§ç‰‡å†™è¯´æ˜Žï¼Ÿè¿™è¶³ä»¥è®©ä»»ä½•äººå¤´æ™•ã€‚å› æ­¤ï¼Œè¿™ä¸ªä¼Ÿå¤§çš„æƒ³æ³•ä»ç„¶åªæ˜¯ä¸€ä¸ªæƒ³æ³•ã€‚\n\nå¿«è¿›åˆ°ä»Šå¤©ã€‚çŽ°åœ¨ï¼Œå€ŸåŠ©è‡ªåŠ¨åŒ–çš„åŠ›é‡ï¼Œæˆ‘å°†è¿™ä¸ªæƒ³æ³•å˜ä¸ºçŽ°å®žã€‚\n\næˆ‘æž„å»ºäº†ä¸€ä¸ªè‡ªåŠ¨åŒ–å†…å®¹åˆ›ä½œç³»ç»Ÿï¼Œå¸®åŠ©æ—…é¦†å°†æ—§ç…§ç‰‡è½¬åŒ–ä¸ºå¼•äººå…¥èƒœçš„æ•…äº‹ï¼Œå¯ä»¥è½»æ¾åœ¨çº¿å‘å¸ƒã€‚\n\nåœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†å¸¦ä½ äº†è§£æˆ‘æž„å»ºè¿™ä¸ªç³»ç»Ÿçš„ç¡®åˆ‡æ­¥éª¤ï¼Œä»¥åŠä½ å¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹ï¼Œä»¥èŠ‚çœæ—¶é—´å¹¶ä¿å­˜ä½ æ—…é¦†çš„åŽ†å²ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*5GBorUl_PfqiLnSW6-Nsjg.png)\n\n## ç¬¬ä¸€æ­¥ï¼šæ”¶é›†æ‚¨æ—…é¦†çš„ç‹¬ç‰¹ä¿¡æ¯\n\nè®¾ç½®æ­¤è‡ªåŠ¨åŒ–çš„ç¬¬ä¸€æ­¥æ˜¯æ”¶é›†æ‰€æœ‰ä½¿æ‚¨çš„æ—…é¦†ä¸Žä¼—ä¸åŒçš„ä¿¡æ¯å’Œèµ„äº§ã€‚å¯¹æˆ‘æ¥è¯´ï¼Œè¿™æ¶‰åŠåˆ°æ”¶é›†èƒ½å¤Ÿä½¿æˆ‘ä»¬çš„å†…å®¹è„±é¢–è€Œå‡ºçš„ç»†èŠ‚ã€‚è€ƒè™‘ä¸€ä¸‹æ‚¨æ—…é¦†æä¾›çš„å…³é”®ä½“éªŒï¼Œä¾‹å¦‚é’“é±¼ä¹‹æ—…ã€æœ¬åœ°å†’é™©æˆ–ç‹¬ç‰¹çš„è®¾æ–½ã€‚è¿™äº›ç»†èŠ‚å°†ä½¿æ‚¨çš„å†…å®¹æ›´å…·ä¸ªäººæ€§å’Œå¸å¼•åŠ›ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*4QX4oGCYK5djc9EZuE9-EA.png)\n\næŽ¥ä¸‹æ¥ï¼Œæ”¶é›†æ—§ç…§ç‰‡â€”â€”ä»Žè¿‡åŽ»å®¢äººçš„ä½“éªŒåˆ°æ—…é¦†å‘¨å›´çš„è‡ªç„¶é£Žå…‰ã€‚\n\nåœ¨æˆ‘çš„å®žéªŒä¸­ï¼Œæˆ‘æœ€åˆä½¿ç”¨äº†AIç”Ÿæˆçš„å›¾åƒï¼Œä½†éšåŽæˆ‘ä»¬Facebooké¡µé¢çš„ä¸€ä½ç²‰ä¸å‘é€äº†ä¸€äº›ç…§ç‰‡ã€‚æˆ‘è¯·æ±‚ä»–å…è®¸æˆ‘ä½¿ç”¨è¿™äº›ç…§ç‰‡ï¼Œä»–éžå¸¸å–œæ¬¢ï¼\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*PuwLsJ2EuOYOLzXwUvn0cQ.png)\n\nå¦‚æžœæ‚¨æœ‰è¿™æ ·çš„ç…§ç‰‡é›†ï¼Œè¯·å°†å®ƒä»¬æ•´ç†åˆ°Google Driveæ–‡ä»¶å¤¹ä¸­ã€‚è¿™å°†ä½¿æ‚¨åœ¨ç¨åŽå°†å®ƒä»¬è¾“å…¥è‡ªåŠ¨åŒ–ç³»ç»Ÿæ—¶æ›´åŠ æ–¹ä¾¿ã€‚é™¤äº†ç…§ç‰‡ä¹‹å¤–ï¼Œè¿˜åˆ›å»ºä¸€ä¸ªGoogle Spreadsheetï¼Œä»¥ä¾¿è®°å½•æ¯å¼ å›¾åƒçš„è¯¦ç»†ä¿¡æ¯ã€‚æ‚¨çš„ç”µå­è¡¨æ ¼åº”åŒ…æ‹¬ï¼š\n\n* å›¾åƒURLï¼ˆæ¥è‡ªæ‚¨çš„Google Driveæ–‡ä»¶å¤¹ï¼‰\n* æ—…é¦†è¯¦æƒ…ï¼ˆä¾‹å¦‚é’“é±¼å‘å¯¼ã€ç‰¹åˆ«æ´»åŠ¨ï¼‰\n* ä»»ä½•ç›¸å…³çš„æ•…äº‹æˆ–æè¿°ï¼Œå¯ä»¥ä¸Žç…§ç‰‡ä¸€èµ·ä½¿ç”¨\n\nè¿™çœ‹èµ·æ¥å¯èƒ½æ˜¯é¢å¤–çš„å·¥ä½œï¼Œä½†å¯¹äºŽå¸®åŠ©è‡ªåŠ¨åŒ–ç³»ç»Ÿç¨åŽåˆ›å»ºæœ‰æ„ä¹‰çš„å†…å®¹è‡³å…³é‡è¦ã€‚\n\n## ç¬¬2æ­¥ï¼šæž„å»ºè‡ªåŠ¨åŒ–è“å›¾\n\nä¸€æ—¦æ‚¨æ”¶é›†äº†æ‰€æœ‰èµ„äº§ï¼Œå°±å¯ä»¥å¼€å§‹è®¾ç½®è‡ªåŠ¨åŒ–è“å›¾ã€‚æˆ‘ä½¿ç”¨äº†ä¸€ä¸ªåä¸º [MAKE](https://www.make.com/en/register?pc=saleprice) çš„è‡ªåŠ¨åŒ–å¹³å°ã€‚å¦‚æžœæ‚¨ä»¥å‰ä»ŽæœªæŽ¥è§¦è¿‡è‡ªåŠ¨åŒ–ï¼Œè¯·ä¸è¦æ‹…å¿ƒâ€”â€”è¿™æ¯”å¬èµ·æ¥è¦ç®€å•å¾—å¤šã€‚\n\nâ€¦â€¦æ‚¨å¯ä»¥åœ¨æˆ‘ä»¬çš„7å¤©è¯•ç”¨ä¸­**å…è´¹**èŽ·å–æˆ‘æ‰€æœ‰ç»è¿‡éªŒè¯çš„è“å›¾ [free](https://whop.com/ai-businessplans)ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ArzS71K2fJf4EfCg5y3BPQ.png)\n\né¦–å…ˆï¼Œå¤åˆ¶ä¸€ä¸ªçŽ°æœ‰çš„è‡ªåŠ¨åŒ–æ¨¡æ¿ï¼Œæ¯”å¦‚ä¸€ä¸ªå‘å¸ƒå¤©æ°”æ›´æ–°åˆ°ç¤¾äº¤åª’ä½“çš„æ¨¡æ¿ã€‚æˆ‘æœ‰ä¸€ä¸ªå¤©æ°”è‡ªåŠ¨åŒ–ï¼Œæ‰€ä»¥æˆ‘ä»¥æ­¤ä¸ºåŸºç¡€ï¼ŒåŽ»æŽ‰äº†ä¸å¿…è¦çš„éƒ¨åˆ†ã€‚æ‚¨å¸Œæœ›æœ‰ä¸€ä¸ªå¹²å‡€çš„å·¥ä½œçŽ¯å¢ƒï¼Œå› æ­¤è¯·åˆ é™¤ä»»ä½•ä¸é€‚ç”¨äºŽæ‚¨å°å±‹çš„æ¨¡å—ï¼Œä¾‹å¦‚å¤©æ°”æ›´æ–°æˆ–æ‚¨ä¸ä¼šä½¿ç”¨çš„é¢å¤–ç¤¾äº¤åª’ä½“æ¸ é“ã€‚\n\nçŽ°åœ¨ï¼Œæ˜¯æ—¶å€™è®©è‡ªåŠ¨åŒ–æ›´å…·ä½“åœ°é€‚åº”æ‚¨çš„å°å±‹äº†ã€‚\n\n## ç¬¬3æ­¥ï¼šä¸ºæ‚¨çš„å°å±‹å®šåˆ¶è‡ªåŠ¨åŒ–\n\nåœ¨åŸºæœ¬ç»“æž„å»ºç«‹ä¹‹åŽï¼Œé€šè¿‡æ·»åŠ ç‰¹å®šäºŽå°å±‹çš„å†…å®¹æ¥å®šåˆ¶è‡ªåŠ¨åŒ–ã€‚è¿™å°±æ˜¯æ‚¨åœ¨ç¬¬1æ­¥ä¸­æ”¶é›†çš„ç»†èŠ‚å‘æŒ¥ä½œç”¨çš„åœ°æ–¹ã€‚è¾“å…¥æ‚¨çš„å°å±‹æè¿°ï¼Œæ·»åŠ è¿‡åŽ»å†’é™©çš„æ•…äº‹ï¼Œå¹¶èžå…¥å½“åœ°æç¤ºã€‚ç¡®ä¿åŒ…å«æœ‰åŠ©äºŽæ‚¨çš„å†…å®¹åœ¨ç½‘ä¸Šè¢«æ³¨æ„åˆ°çš„å…³é”®è¯ã€‚\n\næŽ¥ä¸‹æ¥ï¼Œé…ç½®è‡ªåŠ¨åŒ–ä»¥ä»Žæ‚¨çš„Google Driveæ–‡ä»¶å¤¹ä¸­æå–ç…§ç‰‡ï¼Œå¹¶å°†å®ƒä»¬ä¸Žç”µå­è¡¨æ ¼ä¸­çš„ç›¸åº”æè¿°åŒ¹é…ã€‚è¿™ç¡®ä¿äº†æ­£ç¡®çš„å›¾åƒä¸Žæ­£ç¡®çš„æ•…äº‹é…å¯¹ã€‚\n\nè¿™é‡Œæ˜¯é­”æ³•å‘ç”Ÿçš„åœ°æ–¹ï¼šæˆ‘å°†GPTï¼ˆä¸€ä¸ªè¯­è¨€æ¨¡åž‹AIï¼‰é›†æˆåˆ°è‡ªåŠ¨åŒ–ä¸­ã€‚GPTåˆ†æžæ¯å¼ ç…§ç‰‡ï¼Œå¹¶æ ¹æ®æ‚¨æä¾›çš„ç»†èŠ‚ç”Ÿæˆç‹¬ç‰¹çš„å†…å®¹ã€‚\n\nä¾‹å¦‚ï¼Œå¦‚æžœç…§ç‰‡æ˜¾ç¤ºä¸€ä½å®¢äººé’“åˆ°äº†ä¸€æ¡å¤§é±¼ï¼ŒGPTå¯ä»¥åˆ›å»ºä¸€ç¯‡å…³äºŽè¯¥ç‰¹å®šç»åŽ†çš„å¸–å­ï¼ŒåŒ…æ‹¬å…³äºŽé’“é±¼å‘å¯¼ã€é±¼çš„ç§ç±»ï¼Œç”šè‡³æ˜¯å¯¹æœªæ¥æ¸¸å®¢çš„å»ºè®®ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*wySL0TxoNTFICejPsJOOcA.png)\n\n## ç¬¬4æ­¥ï¼šè‡ªåŠ¨å‘å¸ƒåˆ°ç¤¾äº¤åª’ä½“å’ŒMedium\n\nä¸€æ—¦å†…å®¹ç”Ÿæˆï¼Œå°±å¯ä»¥å¼€å§‹è‡ªåŠ¨å‘å¸ƒè¿‡ç¨‹ã€‚æˆ‘è¿žæŽ¥äº†æˆ‘ä»¬çš„Mediumè´¦æˆ·ï¼Œä»¥ä¾¿GPTç”Ÿæˆçš„æ–‡ç« å¯ä»¥ç›´æŽ¥ä¸Šä¼ ä¸ºè‰ç¨¿ï¼Œå‡†å¤‡å®¡æ ¸ã€‚Mediumæ˜¯ä¸€ä¸ªéžå¸¸é€‚åˆé•¿æ ¼å¼å†…å®¹çš„å¹³å°ï¼Œæ¯”å¦‚åšå®¢æ–‡ç« æˆ–è¯¦ç»†çš„å®¢åº§æ•…äº‹ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*56OzNVMIxw9sJKBn1jkriQ.png)\n\nå¯¹äºŽè¾ƒçŸ­çš„å†…å®¹ï¼Œæ¯”å¦‚ç¤¾äº¤åª’ä½“å¸–å­ï¼Œè‡ªåŠ¨åŒ–é“¾æŽ¥åˆ°æˆ‘ä»¬çš„Facebookå’ŒTwitterè´¦æˆ·ã€‚ç³»ç»Ÿæ—¨åœ¨ä»Žè¾ƒé•¿çš„æ–‡ç« ä¸­åˆ›å»ºç‰‡æ®µï¼Œéžå¸¸é€‚åˆå¿«é€Ÿçš„ç¤¾äº¤åª’ä½“æ›´æ–°ã€‚æ‚¨è¿˜å¯ä»¥é…ç½®è‡ªåŠ¨åŒ–ä»¥è‡ªåŠ¨å‘å¸ƒæˆ–å®‰æŽ’ç‰¹å®šæ—¶é—´çš„å¸–å­ã€‚\n\nè¿™ä¸ªç³»ç»Ÿçš„ç¾Žå¦™ä¹‹å¤„åœ¨äºŽï¼Œä¸€æ—¦å†…å®¹èŽ·å¾—æ‰¹å‡†ï¼Œè‡ªåŠ¨åŒ–å°±ä¼šå¤„ç†ä»Žå‘å¸ƒåˆ°å®‰æŽ’çš„æ‰€æœ‰äº‹åŠ¡ã€‚è¿™æ˜¯ä¸€ä¸ªæ— éœ€æ‰‹åŠ¨å¹²é¢„çš„è§£å†³æ–¹æ¡ˆï¼Œå¯ä»¥ä¿æŒæ‚¨çš„åœ¨çº¿å­˜åœ¨æ´»è·ƒï¼Œè€Œæ— éœ€æŒç»­å…³æ³¨ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*rym300CevwmoA4_pvYybqQ.png)\n\n## ç¬¬5æ­¥ï¼šæµ‹è¯•å’Œå¾®è°ƒ\n\nçŽ°åœ¨è‡ªåŠ¨åŒ–å·²ç»åˆ°ä½ï¼Œåœ¨ä¸Šçº¿ä¹‹å‰è¿›è¡Œæµ‹è¯•æ˜¯å¾ˆé‡è¦çš„ã€‚æˆ‘è¿›è¡Œäº†å‡ æ¬¡æµ‹è¯•å‘å¸ƒï¼Œä»¥ç¡®ä¿ä¸€åˆ‡æŒ‰é¢„æœŸå·¥ä½œã€‚è‡ªåŠ¨åŒ–æ‹‰å–äº†æ­£ç¡®çš„ç…§ç‰‡ï¼Œç”Ÿæˆäº†å¼•äººå…¥èƒœçš„å†…å®¹ï¼Œå¹¶é¡ºåˆ©åœ°å‘å¸ƒåˆ°Mediumå’Œç¤¾äº¤åª’ä½“ä¸Šã€‚\n\nåœ¨æµ‹è¯•è¿‡ç¨‹ä¸­ï¼Œæˆ‘è¿›è¡Œäº†å°çš„è°ƒæ•´ï¼Œä¾‹å¦‚ä¿®æ”¹æ ‡é¢˜æˆ–ä¼˜åŒ–GPTæç¤ºï¼Œä»¥ç¡®ä¿å†…å®¹ä¸Žæˆ‘ä»¬æ—…é¦†çš„è¯­æ°”å’Œæ•…äº‹å®Œç¾Žå¥‘åˆã€‚\n\nä¸€æ—¦ä¸€åˆ‡éƒ½è°ƒæ•´åˆ°ä½ï¼Œæˆ‘å°±å¯ä»¥åˆ›å»ºæºæºä¸æ–­çš„æ–°å†…å®¹ï¼Œä¿æŒæˆ‘ä»¬çš„å—ä¼—å‚ä¸Žã€‚\n\n## ä¸€ä¸ªå®žé™…é¡¹ç›®çš„æµ‹è¯•\n\nä¸ºäº†ç»™æ‚¨ä¸€ä¸ªå®žé™…çš„ä¾‹å­ï¼Œè¯·æŸ¥çœ‹ [iFish Canada Facebook é¡µé¢](https://facebook.com/ifishcanada)ã€‚\n\nè¯¥é¡¹ç›®æ¼”ç¤ºäº†è‡ªåŠ¨åŒ–åœ¨çŽ°å®žç”Ÿæ´»ä¸­çš„å·¥ä½œæ–¹å¼ã€‚\n\nç³»ç»Ÿä»Žæˆ‘ä»¬çš„å…³æ³¨è€…é‚£é‡ŒèŽ·å–ä»–ä»¬åœ¨åŠ æ‹¿å¤§é’“é±¼æ—…è¡Œä¸­æäº¤çš„ç…§ç‰‡ï¼Œé€šè¿‡ GPT å¤„ç†ï¼Œå¹¶ç”Ÿæˆç‹¬ç‰¹çš„å¸–å­ï¼Œå±•ç¤ºç…§ç‰‡åŠå…¶ä½œè€…çš„ç»åŽ†â€”â€”åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œè¿™äº›å¸–å­ä¸Žæˆ‘ä»¬è™šæž„çš„æ—…é¦†çš„ä¼ è¯´ç›¸ç»“åˆã€‚\n\nå†…å®¹ä¸°å¯Œã€å¼•äººå…¥èƒœï¼Œæœ€é‡è¦çš„æ˜¯â€”â€”è‡ªåŠ¨åŒ–ã€‚\n\næ›¾ç»çœ‹ä¼¼ä¸å¯èƒ½çš„ä»»åŠ¡çŽ°åœ¨æˆä¸ºçŽ°å®žï¼ŒèŠ‚çœäº†æ•°ç™¾ä¸ªå°æ—¶çš„æ—¶é—´ï¼Œå¹¶è®©æˆ‘ä»¬èƒ½å¤Ÿåˆ†äº«é‚£äº›è®©æˆ‘ä»¬çš„é’“é±¼ç…§ç‰‡è´¡çŒ®è€…æ„Ÿåˆ°è¢«è®¤å¯å’Œæ¬£èµçš„æ•…äº‹ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Pe-hBjGCN1qbkjLl6cmTmQ.png)\n\n## ç¬¬6æ­¥ï¼šç›‘æŽ§ã€è°ƒæ•´å’Œæˆé•¿\n\nå³ä½¿ç³»ç»Ÿå¯ä»¥è‡ªåŠ¨è¿è¡Œï¼Œç›‘æŽ§ç»“æžœå¹¶éšç€æ—¶é—´è¿›è¡Œè°ƒæ•´ä»ç„¶å¾ˆé‡è¦ã€‚\n\næˆ‘ä¼šæŸ¥çœ‹å“ªäº›å¸–å­èŽ·å¾—äº†æœ€å¤šçš„äº’åŠ¨ï¼Œå¹¶è°ƒæ•´GPTæç¤ºä»¥æ”¹å–„æœªæ¥çš„å†…å®¹ã€‚è¿™ç§æŒç»­çš„å¾®è°ƒç¡®ä¿æˆ‘ä»¬çš„åœ¨çº¿å­˜åœ¨ä¿æŒæ–°é²œï¼Œå¹¶ç»§ç»­å¸å¼•æ–°è®¿å®¢ã€‚\n\næƒ³è±¡ä¸€ä¸‹ï¼Œæ‚¨å¦‚ä½•èƒ½å¤Ÿä¿æŠ¤æ‚¨çš„å°å±‹é—äº§ï¼Œåˆ†äº«é‚£äº›å¯èƒ½ä¼šè¢«é—å¿˜çš„å›žå¿†ï¼Œå¹¶ä»¥ä¸€ç§çŽ°ä»£è€Œå¼ºå¤§çš„æ–¹å¼ç”Ÿæˆå£ç¢‘è¥é”€ã€‚\n\n## åå¹´çš„åŠªåŠ› â€” çŽ°åœ¨ä½ å¯ä»¥åŠ å…¥æˆ‘\n\nç»è¿‡å¤šå¹´çš„æ¢¦æƒ³ï¼Œè‡ªåŠ¨åŒ–è¿™ä¸ªè¿‡ç¨‹ç»ˆäºŽæˆä¸ºçŽ°å®žã€‚ç³»ç»Ÿå·²ç»æŠ•å…¥ä½¿ç”¨ï¼Œç»“æžœå°†ä¸è¨€è€Œå–»ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*135_nP0nL6NrT_O7JxIk4g.png)\n\nçŽ°åœ¨ï¼Œæˆ‘æƒ³é‚€è¯·ä½ ä½“éªŒä¸€ä¸‹è¿™ä¸ªç³»ç»Ÿï¼Œé€‚ç”¨äºŽä½ çš„æ—…é¦†æˆ–åº¦å‡æ‘ã€‚å¦‚æžœä½ æ›¾ç»æ„Ÿåˆ°è¢«å¸‚åœºè¥é”€çš„æ—¶é—´æˆ–æˆæœ¬åŽ‹å€’ï¼Œæˆ–è€…åœ¨ä¸æ–­éœ€æ±‚æ–°å†…å®¹çš„æƒ…å†µä¸‹æ„Ÿåˆ°æŒ£æ‰Žï¼Œ[è¿™ä¸ªè‡ªåŠ¨åŒ–ç³»ç»Ÿ](https://whop.com/ai-businessplans)å¯èƒ½å°±æ˜¯ä½ ä¸€ç›´åœ¨å¯»æ‰¾çš„è§£å†³æ–¹æ¡ˆã€‚\n\nè®¿é—® [iFish Canada](https://facebook.com/ifishcanada) è§‚çœ‹ç³»ç»Ÿçš„å®žé™…è¿è¡Œï¼Œå¦‚æžœä½ æƒ³å¼€å§‹è‡ªåŠ¨åŒ–è‡ªå·±çš„å†…å®¹ï¼Œè¯·éšæ—¶è”ç³»æˆ‘ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*IJ-R1362_IWUrZ-3KyG8mw.png)\n\næˆ‘å¾ˆä¹æ„å¸®åŠ©ä½ é‡Šæ”¾è‡ªåŠ¨åŒ–çš„åŠ›é‡ï¼Œæ¥å‘å±•ä½ çš„æ—…é¦†ï¼Œåˆ†äº«ä½ çš„æ•…äº‹ï¼Œå¸å¼•æ–°å®¢äººâ€”â€”åŒæ—¶èŠ‚çœä½ çš„æ—¶é—´å’Œç²¾åŠ›ã€‚\n\nâ«·\n\n### æˆ‘é‡è§†æ‚¨çš„è¯„è®º\n\næˆ‘ä¼šå›žå¤æ‰€æœ‰è¯„è®ºï¼Œå¹¶**ä½œä¸ºæˆ‘çš„æ„Ÿè°¢**ï¼Œæˆ‘è¿˜ä¼šåœ¨é€‚å½“çš„åœ°æ–¹å…³æ³¨ã€ç‚¹èµžã€çªå‡ºå’Œè¯„è®ºæ‚¨çš„å†…å®¹ã€‚æ‰€ä»¥è¯·ç•™ä¸‹æ‚¨çš„æƒ³æ³•ã€é—®é¢˜æˆ–æˆåŠŸæ•…äº‹ï¼æˆ‘å–œæ¬¢é˜…è¯»å®ƒä»¬ï¼\n\n*è¿žæŽ¥* åœ¨ [YouTube](https://www.youtube.com/channel/UCphdP_nguu6MT3U5tsJNMsQ)ã€[X (twitter)](https://x.com/Aibusinessplans/status/1803488217079095460) å’Œ [Linkedin](https://www.linkedin.com/company/ai-businessplans/) â€” å°è¯•æˆ‘ä»¬çš„ [Community](https://whop.com/ai-businessplans)ã€‚\n\nä¿æŒå®‰å…¨ï¼Œæ¯å¤©è¿ˆå‡ºå°æ­¥ä¼ã€‚ \n\nDoug\n\n## é˜…è¯»ä¸‹ä¸€æ­¥ \\-\n\nðŸ›† *æŠ•èµ„å…è´£å£°æ˜Žï¼š* åœ¨å……åˆ†åˆ©ç”¨å…è´¹åŠŸèƒ½ä¹‹å‰ï¼Œæ‚¨ä¸åº”å°†èµ„é‡‘æŠ•å…¥ä»˜è´¹å·¥å…·ã€‚*æˆ‘ä»¬åŸ¹è®­äº§å“ä¸­çš„ä»»ä½•å†…å®¹éƒ½ä¸æ˜¯æ”¶ç›Šçš„æ‰¿è¯ºæˆ–ä¿è¯ã€‚*ðŸ‘ˆ\n\nâ˜„ æœ¬æ–‡åŒ…å«æˆ‘æœ€å–œæ¬¢çš„å†…å®¹åˆ›ä½œè€…å’ŒGenAIçˆ±å¥½è€…çš„AIå•†ä¸šå·¥å…·çš„æŽ¨èé“¾æŽ¥ã€‚\n\nå¦‚æžœæ‚¨è´­ä¹°æˆ‘æœ€å–œæ¬¢çš„è½¯ä»¶å’ŒAIå·¥å…·ï¼Œæˆ‘å°†ä¼šèŽ·å¾—ä¸€å°ç¬”ä½£é‡‘ï¼Œè€Œæ‚¨æ— éœ€æ”¯ä»˜é¢å¤–è´¹ç”¨ã€‚\n\n"},{"lang":"zh","group":"blog","slug":"blog/choosing-between-llm-agent-frameworks-69019493b259","frontmatter":{"title":"åœ¨ LLM ä»£ç†æ¡†æž¶ä¹‹é—´è¿›è¡Œé€‰æ‹©","meta_title":"åœ¨ LLM ä»£ç†æ¡†æž¶ä¹‹é—´è¿›è¡Œé€‰æ‹©","description":"æž„å»ºå®šåˆ¶åŸºäºŽä»£ç çš„ä»£ç†å’Œä¸»è¦ä»£ç†æ¡†æž¶ä¹‹é—´çš„æƒè¡¡ã€‚","date":"2024-10-29T12:57:34.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*jRMs19HqSCazE5dY","categories":["Programming","Technology","Machine Learning"],"author":"Rifx.Online","tags":["agents","frameworks","LangGraph","LlamaIndex","Workflows"],"draft":false,"slug":"blog/choosing-between-llm-agent-frameworks-69019493b259"},"content":"\n### å®šåˆ¶ä»£ç ä»£ç†ä¸Žä¸»è¦ä»£ç†æ¡†æž¶ä¹‹é—´çš„æƒè¡¡\n\n\n\nä»£ç†æ­£åœ¨è¿Žæ¥ä¸€ä¸ªé‡è¦æ—¶åˆ»ã€‚éšç€å¤šä¸ªæ–°æ¡†æž¶å’Œæ–°çš„ [æŠ•èµ„](https://foundationcapital.com/goodbye-aiops-welcome-agentsres-the-next-100b-opportunity/) çš„æ¶Œå…¥ï¼ŒçŽ°ä»£ AI ä»£ç†æ­£åœ¨å…‹æœ [ä¸ç¨³å®šçš„èµ·æº](https://arxiv.org/html/2405.13966v1)ï¼Œè¿…é€Ÿå–ä»£ RAG æˆä¸ºå®žæ–½ä¼˜å…ˆäº‹é¡¹ã€‚é‚£ä¹ˆï¼Œ2024 å¹´æ˜¯å¦ç»ˆäºŽä¼šæˆä¸ºèƒ½å¤ŸæŽ¥ç®¡æ’°å†™ç”µå­é‚®ä»¶ã€é¢„è®¢èˆªç­ã€ä¸Žæˆ‘ä»¬çš„æ•°æ®å¯¹è¯æˆ–ä¼¼ä¹Žä»»ä½•å…¶ä»–ä»»åŠ¡çš„è‡ªä¸» AI ç³»ç»Ÿçš„å¹´ä»½ï¼Ÿ\n\nä¹Ÿè®¸ï¼Œä½†è¦è¾¾åˆ°è¿™ä¸€ç‚¹è¿˜æœ‰å¾ˆå¤šå·¥ä½œè¦åšã€‚ä»»ä½•æž„å»ºä»£ç†çš„å¼€å‘è€…ä¸ä»…éœ€è¦é€‰æ‹©åŸºç¡€â€”â€”ä½¿ç”¨å“ªä¸ªæ¨¡åž‹ã€ç”¨ä¾‹å’Œæž¶æž„â€”â€”è¿˜éœ€è¦é€‰æ‹©åˆ©ç”¨å“ªä¸ªæ¡†æž¶ã€‚ä½ æ˜¯é€‰æ‹©é•¿æœŸå­˜åœ¨çš„ LangGraphï¼Œè¿˜æ˜¯è¾ƒæ–°çš„ LlamaIndex Workflowsï¼Ÿæˆ–è€…ä½ é€‰æ‹©ä¼ ç»Ÿæ–¹å¼ï¼Œè‡ªå·±ç¼–å†™æ•´ä¸ªä»£ç ï¼Ÿ\n\næœ¬æ–‡æ—¨åœ¨è®©è¿™ä¸ªé€‰æ‹©å˜å¾—ç®€å•ä¸€äº›ã€‚åœ¨è¿‡åŽ»å‡ å‘¨ï¼Œæˆ‘åœ¨ä¸»è¦æ¡†æž¶ä¸­æž„å»ºäº†ç›¸åŒçš„ä»£ç†ï¼Œä»¥æŠ€æœ¯å±‚é¢æ£€æŸ¥æ¯ä¸ªæ¡†æž¶çš„ä¸€äº›ä¼˜ç¼ºç‚¹ã€‚æ¯ä¸ªä»£ç†çš„æ‰€æœ‰ä»£ç éƒ½å¯ä»¥åœ¨ [è¿™ä¸ªåº“](https://github.com/Arize-ai/phoenix/tree/main/examples/agent_framework_comparison) ä¸­æ‰¾åˆ°ã€‚\n\n### æµ‹è¯•ç”¨ä»£ç†çš„èƒŒæ™¯\n\nç”¨äºŽæµ‹è¯•çš„ä»£ç†åŒ…æ‹¬åŠŸèƒ½è°ƒç”¨ã€å¤šç§å·¥å…·æˆ–æŠ€èƒ½ã€ä¸Žå¤–éƒ¨èµ„æºçš„è¿žæŽ¥ï¼Œä»¥åŠå…±äº«çŠ¶æ€æˆ–è®°å¿†ã€‚\n\nè¯¥ä»£ç†å…·æœ‰ä»¥ä¸‹èƒ½åŠ›ï¼š\n\n1. ä»ŽçŸ¥è¯†åº“å›žç­”é—®é¢˜\n2. ä¸Žæ•°æ®å¯¹è¯ï¼šå›žç­”æœ‰å…³LLMåº”ç”¨ç¨‹åºçš„é¥æµ‹æ•°æ®çš„é—®é¢˜\n3. æ•°æ®åˆ†æžï¼šåˆ†æžæ£€ç´¢åˆ°çš„é¥æµ‹æ•°æ®ä¸­çš„æ›´é«˜çº§è¶‹åŠ¿å’Œæ¨¡å¼\n\nä¸ºäº†å®žçŽ°è¿™äº›ï¼Œä»£ç†å…·æœ‰ä¸‰é¡¹åˆå§‹æŠ€èƒ½ï¼šåŸºäºŽäº§å“æ–‡æ¡£çš„RAGã€åœ¨è·Ÿè¸ªæ•°æ®åº“ä¸Šç”ŸæˆSQLï¼Œä»¥åŠæ•°æ®åˆ†æžã€‚ä»£ç†ç”¨æˆ·ç•Œé¢ä½¿ç”¨ç®€å•çš„gradioæ”¯æŒçš„ç•Œé¢ï¼Œä»£ç†æœ¬èº«æž„å»ºä¸ºä¸€ä¸ªèŠå¤©æœºå™¨äººã€‚\n\n## åŸºäºŽä»£ç çš„ä»£ç†ï¼ˆæ— æ¡†æž¶ï¼‰\n\nå¼€å‘ä»£ç†æ—¶ï¼Œæ‚¨å¯ä»¥é€‰æ‹©å®Œå…¨è·³è¿‡æ¡†æž¶ï¼Œè‡ªå·±æž„å»ºä»£ç†ã€‚å¼€å§‹è¿™ä¸ªé¡¹ç›®æ—¶ï¼Œæˆ‘é‡‡ç”¨çš„å°±æ˜¯è¿™ç§æ–¹æ³•ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*pw9-0lB5JMlVcPqo)\n\n### çº¯ä»£ç æž¶æž„\n\nä¸‹é¢çš„åŸºäºŽä»£ç çš„ä»£ç†ç”±ä¸€ä¸ªç”±OpenAIé©±åŠ¨çš„è·¯ç”±å™¨ç»„æˆï¼Œè¯¥è·¯ç”±å™¨ä½¿ç”¨å‡½æ•°è°ƒç”¨é€‰æ‹©åˆé€‚çš„æŠ€èƒ½ã€‚è¯¥æŠ€èƒ½å®ŒæˆåŽï¼Œå®ƒä¼šè¿”å›žè·¯ç”±å™¨ï¼Œä»¥ä¾¿è°ƒç”¨å¦ä¸€ä¸ªæŠ€èƒ½æˆ–å“åº”ç”¨æˆ·ã€‚\n\nä»£ç†ä¿æŒä¸€ä¸ªæŒç»­æ›´æ–°çš„æ¶ˆæ¯å’Œå“åº”åˆ—è¡¨ï¼Œåœ¨æ¯æ¬¡è°ƒç”¨æ—¶å®Œæ•´ä¼ é€’ç»™è·¯ç”±å™¨ï¼Œä»¥ä¿æŒä¸Šä¸‹æ–‡çš„è¿žè´¯æ€§ã€‚\n\n```python\ndef router(messages):\n    if not any(\n        isinstance(message, dict) and message.get(\"role\") == \"system\" for message in messages\n    ):\n        system_prompt = {\"role\": \"system\", \"content\": SYSTEM_PROMPT}\n        messages.append(system_prompt)\n\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=messages,\n        tools=skill_map.get_combined_function_description_for_openai(),\n    )\n\n    messages.append(response.choices[0].message)\n    tool_calls = response.choices[0].message.tool_calls\n    if tool_calls:\n        handle_tool_calls(tool_calls, messages)\n        return router(messages)\n    else:\n        return response.choices[0].message.content\n```\n\næŠ€èƒ½æœ¬èº«åœ¨å„è‡ªçš„ç±»ä¸­å®šä¹‰ï¼ˆä¾‹å¦‚ï¼ŒGenerateSQLQueryï¼‰ï¼Œè¿™äº›ç±»å…±åŒä¿å­˜åœ¨ä¸€ä¸ªSkillMapä¸­ã€‚è·¯ç”±å™¨æœ¬èº«åªä¸ŽSkillMapäº¤äº’ï¼Œä½¿ç”¨å®ƒæ¥åŠ è½½æŠ€èƒ½åç§°ã€æè¿°å’Œå¯è°ƒç”¨å‡½æ•°ã€‚è¿™ç§æ–¹æ³•æ„å‘³ç€å°†æ–°æŠ€èƒ½æ·»åŠ åˆ°ä»£ç†ä¸­åªéœ€å°†è¯¥æŠ€èƒ½ç¼–å†™ä¸ºè‡ªå·±çš„ç±»ï¼Œç„¶åŽå°†å…¶æ·»åŠ åˆ°SkillMapä¸­çš„æŠ€èƒ½åˆ—è¡¨ä¸­ã€‚è¿™é‡Œçš„æƒ³æ³•æ˜¯ä½¿æ·»åŠ æ–°æŠ€èƒ½å˜å¾—ç®€å•ï¼Œè€Œä¸å¹²æ‰°è·¯ç”±å™¨ä»£ç ã€‚\n\n```python\nclass SkillMap:\n    def __init__(self):\n        skills = [AnalyzeData(), GenerateSQLQuery()]\n\n        self.skill_map = {}\n        for skill in skills:\n            self.skill_map[skill.get_function_name()] = (\n                skill.get_function_dict(),\n                skill.get_function_callable(),\n            )\n\n    def get_function_callable_by_name(self, skill_name) -> Callable:\n        return self.skill_map[skill_name][1]\n\n    def get_combined_function_description_for_openai(self):\n        combined_dict = []\n        for _, (function_dict, _) in self.skill_map.items():\n            combined_dict.append(function_dict)\n        return combined_dict\n\n    def get_function_list(self):\n        return list(self.skill_map.keys())\n\n    def get_list_of_function_callables(self):\n        return [skill[1] for skill in self.skill_map.values()]\n\n    def get_function_description_by_name(self, skill_name):\n        return str(self.skill_map[skill_name][0][\"function\"])\n```\n\næ€»ä½“è€Œè¨€ï¼Œè¿™ç§æ–¹æ³•ç›¸å¯¹ç®€å•æ˜“è¡Œï¼Œä½†ä¹Ÿé¢ä¸´ä¸€äº›æŒ‘æˆ˜ã€‚\n\n### çº¯ä»£ç ä»£ç†çš„æŒ‘æˆ˜\n\nç¬¬ä¸€ä¸ªéš¾ç‚¹åœ¨äºŽæž„å»ºè·¯ç”±å™¨ç³»ç»Ÿæç¤ºã€‚é€šå¸¸ï¼Œä¸Šè¿°ç¤ºä¾‹ä¸­çš„è·¯ç”±å™¨åšæŒè‡ªå·±ç”Ÿæˆ SQLï¼Œè€Œä¸æ˜¯å°†å…¶å§”æ‰˜ç»™åˆé€‚çš„æŠ€èƒ½ã€‚å¦‚æžœä½ æ›¾ç»å°è¯•è®© LLM *ä¸* åšæŸä»¶äº‹ï¼Œä½ å°±ä¼šçŸ¥é“è¿™ç§ä½“éªŒæ˜¯å¤šä¹ˆä»¤äººæ²®ä¸§ï¼›æ‰¾åˆ°ä¸€ä¸ªæœ‰æ•ˆçš„æç¤ºéœ€è¦ç»è¿‡å¤šè½®è°ƒè¯•ã€‚è€ƒè™‘åˆ°æ¯ä¸ªæ­¥éª¤çš„ä¸åŒè¾“å‡ºæ ¼å¼ä¹Ÿæ˜¯æ£˜æ‰‹çš„ã€‚ç”±äºŽæˆ‘é€‰æ‹©ä¸ä½¿ç”¨ç»“æž„åŒ–è¾“å‡ºï¼Œæˆ‘å¿…é¡»å‡†å¤‡å¥½åº”å¯¹è·¯ç”±å™¨å’ŒæŠ€èƒ½ä¸­æ¯ä¸ª LLM è°ƒç”¨çš„å¤šç§ä¸åŒæ ¼å¼ã€‚\n\n### çº¯ä»£ç ä»£ç†çš„å¥½å¤„\n\nåŸºäºŽä»£ç çš„æ–¹æ³•æä¾›äº†è‰¯å¥½çš„åŸºå‡†å’Œèµ·ç‚¹ï¼Œæ˜¯å­¦ä¹ ä»£ç†å·¥ä½œåŽŸç†çš„ç»ä½³æ–¹å¼ï¼Œè€Œæ— éœ€ä¾èµ–çŽ°æœ‰æ¡†æž¶ä¸­çš„çŽ°æˆä»£ç†æ•™ç¨‹ã€‚å°½ç®¡è¯´æœ LLM æŒ‰é¢„æœŸè¡Œä¸ºå¯èƒ½å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œä½†ä»£ç ç»“æž„æœ¬èº«è¶³å¤Ÿç®€å•ï¼Œå¯èƒ½é€‚ç”¨äºŽæŸäº›ç”¨ä¾‹ï¼ˆæ›´å¤šå†…å®¹è§ä¸‹é¢çš„åˆ†æžéƒ¨åˆ†ï¼‰ã€‚\n\n## LangGraph\n\nLangGraph æ˜¯æœ€æ—©çš„ä»£ç†æ¡†æž¶ä¹‹ä¸€ï¼Œé¦–æ¬¡å‘å¸ƒäºŽ 2024 å¹´ 1 æœˆã€‚è¯¥æ¡†æž¶æ—¨åœ¨é€šè¿‡é‡‡ç”¨ Pregel å›¾ç»“æž„æ¥è§£å†³çŽ°æœ‰ç®¡é“å’Œé“¾çš„æ— çŽ¯ç‰¹æ€§ã€‚LangGraph é€šè¿‡æ·»åŠ èŠ‚ç‚¹ã€è¾¹å’Œæ¡ä»¶è¾¹çš„æ¦‚å¿µï¼Œä½¿æ‚¨æ›´å®¹æ˜“åœ¨ä»£ç†ä¸­å®šä¹‰å¾ªçŽ¯ï¼Œä»¥éåŽ†å›¾å½¢ã€‚LangGraph å»ºç«‹åœ¨ LangChain ä¹‹ä¸Šï¼Œå¹¶ä½¿ç”¨è¯¥æ¡†æž¶ä¸­çš„å¯¹è±¡å’Œç±»åž‹ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*fYgHiGwLhSUSrFv9)\n\n### LangGraph æž¶æž„\n\nLangGraph ä»£ç†åœ¨è¡¨é¢ä¸Šçœ‹èµ·æ¥ä¸ŽåŸºäºŽä»£ç çš„ä»£ç†ç›¸ä¼¼ï¼Œä½†å…¶èƒŒåŽçš„ä»£ç å´æˆªç„¶ä¸åŒã€‚LangGraph åœ¨æŠ€æœ¯ä¸Šä»ç„¶ä½¿ç”¨â€œè·¯ç”±å™¨â€ï¼Œå³é€šè¿‡å‡½æ•°è°ƒç”¨ OpenAIï¼Œå¹¶ä½¿ç”¨å“åº”ç»§ç»­åˆ°æ–°çš„æ­¥éª¤ã€‚ç„¶è€Œï¼Œç¨‹åºåœ¨æŠ€èƒ½ä¹‹é—´çš„ç§»åŠ¨æ–¹å¼å®Œå…¨ä¸åŒã€‚\n\n```python\ntools = [generate_and_run_sql_query, data_analyzer]\nmodel = ChatOpenAI(model=\"gpt-4o\", temperature=0).bind_tools(tools)\n\ndef create_agent_graph():\n    workflow = StateGraph(MessagesState)\n\n    tool_node = ToolNode(tools)\n    workflow.add_node(\"agent\", call_model)\n    workflow.add_node(\"tools\", tool_node)\n\n    workflow.add_edge(START, \"agent\")\n    workflow.add_conditional_edges(\n        \"agent\",\n        should_continue,\n    )\n    workflow.add_edge(\"tools\", \"agent\")\n\n    checkpointer = MemorySaver()\n    app = workflow.compile(checkpointer=checkpointer)\n    return app\n```\n\nè¿™é‡Œå®šä¹‰çš„å›¾æœ‰ä¸€ä¸ªç”¨äºŽåˆå§‹ OpenAI è°ƒç”¨çš„èŠ‚ç‚¹ï¼Œç§°ä¸ºä¸Šé¢çš„â€œagentâ€ï¼Œä»¥åŠä¸€ä¸ªç”¨äºŽå·¥å…·å¤„ç†æ­¥éª¤çš„èŠ‚ç‚¹ï¼Œç§°ä¸ºâ€œtoolsâ€ã€‚LangGraph æœ‰ä¸€ä¸ªå†…ç½®å¯¹è±¡ ToolNodeï¼Œå®ƒæŽ¥å—ä¸€ä¸ªå¯è°ƒç”¨å·¥å…·çš„åˆ—è¡¨ï¼Œå¹¶æ ¹æ® ChatMessage å“åº”è§¦å‘å®ƒä»¬ï¼Œç„¶åŽå†è¿”å›žåˆ°â€œagentâ€èŠ‚ç‚¹ã€‚\n\n```python\ndef should_continue(state: MessagesState):\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    if last_message.tool_calls:\n        return \"tools\"\n    return END\n\ndef call_model(state: MessagesState):\n    messages = state[\"messages\"]\n    response = model.invoke(messages)\n    return {\"messages\": [response]}\n```\n\nåœ¨æ¯æ¬¡è°ƒç”¨â€œagentâ€èŠ‚ç‚¹åŽï¼ˆæ¢å¥è¯è¯´ï¼šåŸºäºŽä»£ç çš„ä»£ç†ä¸­çš„è·¯ç”±å™¨ï¼‰ï¼Œshould_continue è¾¹å†³å®šæ˜¯å°†å“åº”è¿”å›žç»™ç”¨æˆ·ï¼Œè¿˜æ˜¯ä¼ é€’ç»™ ToolNode ä»¥å¤„ç†å·¥å…·è°ƒç”¨ã€‚\n\nåœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸­ï¼Œâ€œstateâ€å­˜å‚¨äº†æ¥è‡ª OpenAI çš„æ¶ˆæ¯å’Œå“åº”åˆ—è¡¨ï¼Œç±»ä¼¼äºŽåŸºäºŽä»£ç çš„ä»£ç†çš„æ–¹æ³•ã€‚\n\n### LangGraph çš„æŒ‘æˆ˜\n\nå¤§å¤šæ•°ä¸Ž LangGraph ç›¸å…³çš„å›°éš¾æºäºŽéœ€è¦ä½¿ç”¨ Langchain å¯¹è±¡ï¼Œä»¥ä¾¿æµç¨‹é¡ºç•…ã€‚\n\n**æŒ‘æˆ˜ \\#1ï¼šå‡½æ•°è°ƒç”¨éªŒè¯**\n\nä¸ºäº†ä½¿ç”¨ ToolNode å¯¹è±¡ï¼Œæˆ‘ä¸å¾—ä¸é‡æž„æˆ‘çŽ°æœ‰çš„å¤§éƒ¨åˆ† Skill ä»£ç ã€‚ToolNode æŽ¥å—ä¸€ä¸ªå¯è°ƒç”¨å‡½æ•°çš„åˆ—è¡¨ï¼Œè¿™æœ€åˆè®©æˆ‘è®¤ä¸ºå¯ä»¥ä½¿ç”¨æˆ‘çŽ°æœ‰çš„å‡½æ•°ï¼Œä½†ç”±äºŽæˆ‘çš„å‡½æ•°å‚æ•°ï¼Œäº‹æƒ…å´å‡ºçŽ°äº†é—®é¢˜ã€‚\n\nè¿™äº›æŠ€èƒ½è¢«å®šä¹‰ä¸ºå…·æœ‰å¯è°ƒç”¨æˆå‘˜å‡½æ•°çš„ç±»ï¼Œè¿™æ„å‘³ç€å®ƒä»¬çš„ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯â€œselfâ€ã€‚GPT\\-4o è¶³å¤Ÿæ™ºèƒ½ï¼Œæœªåœ¨ç”Ÿæˆçš„å‡½æ•°è°ƒç”¨ä¸­åŒ…å«â€œselfâ€å‚æ•°ï¼Œç„¶è€Œ LangGraph å°†å…¶è§†ä¸ºç¼ºå°‘å‚æ•°çš„éªŒè¯é”™è¯¯ã€‚\n\nè¿™èŠ±äº†æˆ‘å‡ ä¸ªå°æ—¶æ‰å¼„æ˜Žç™½ï¼Œå› ä¸ºé”™è¯¯ä¿¡æ¯å´å°†å‡½æ•°ä¸­çš„ç¬¬ä¸‰ä¸ªå‚æ•°ï¼ˆæ•°æ®åˆ†æžæŠ€èƒ½ä¸­çš„â€œargsâ€ï¼‰æ ‡è®°ä¸ºç¼ºå¤±å‚æ•°ï¼š\n\n```python\npydantic.v1.error_wrappers.ValidationError: 1 validation error for data_analysis_toolSchema\nargs field required (type=value_error.missing)\n```\n\nå€¼å¾—ä¸€æçš„æ˜¯ï¼Œé”™è¯¯æ¶ˆæ¯æºè‡ª Pydanticï¼Œè€Œä¸æ˜¯ LangGraphã€‚\n\næˆ‘æœ€ç»ˆä¸‹å®šå†³å¿ƒï¼Œå°†æˆ‘çš„æŠ€èƒ½é‡æ–°å®šä¹‰ä¸ºä½¿ç”¨ Langchain çš„ @tool è£…é¥°å™¨çš„åŸºæœ¬æ–¹æ³•ï¼Œå¹¶æˆåŠŸä½¿å…¶å·¥ä½œã€‚\n\n```python\n@tool\ndef generate_and_run_sql_query(query: str):\n    \"\"\"æ ¹æ®æç¤ºç”Ÿæˆå¹¶è¿è¡Œ SQL æŸ¥è¯¢ã€‚\n\n    å‚æ•°ï¼š\n        query (str): åŒ…å«åŽŸå§‹ç”¨æˆ·æç¤ºçš„å­—ç¬¦ä¸²ã€‚\n\n    è¿”å›žï¼š\n        str: SQL æŸ¥è¯¢çš„ç»“æžœã€‚\n    \"\"\"\n```\n\n**æŒ‘æˆ˜ \\#2ï¼šè°ƒè¯•**\n\nå¦‚å‰æ‰€è¿°ï¼Œåœ¨æ¡†æž¶ä¸­è¿›è¡Œè°ƒè¯•æ˜¯å›°éš¾çš„ã€‚è¿™ä¸»è¦å½’ç»“ä¸ºä»¤äººå›°æƒ‘çš„é”™è¯¯æ¶ˆæ¯å’ŒæŠ½è±¡æ¦‚å¿µï¼Œä½¿å¾—æŸ¥çœ‹å˜é‡å˜å¾—æ›´åŠ å›°éš¾ã€‚\n\næŠ½è±¡æ¦‚å¿µä¸»è¦åœ¨å°è¯•è°ƒè¯•åœ¨ä»£ç†ä¸­ä¼ é€’çš„æ¶ˆæ¯æ—¶å‡ºçŽ°ã€‚LangGraph å°†è¿™äº›æ¶ˆæ¯å­˜å‚¨åœ¨ state\\[â€œmessagesâ€] ä¸­ã€‚å›¾ä¸­çš„æŸäº›èŠ‚ç‚¹ä¼šè‡ªåŠ¨ä»Žè¿™äº›æ¶ˆæ¯ä¸­æå–ï¼Œè¿™å¯èƒ½ä½¿å¾—åœ¨èŠ‚ç‚¹è®¿é—®æ¶ˆæ¯æ—¶ç†è§£æ¶ˆæ¯çš„å€¼å˜å¾—å›°éš¾ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*KuCg0WGHSklOKe6t)\n\n### LangGraph çš„å¥½å¤„\n\nLangGraph çš„ä¸»è¦å¥½å¤„ä¹‹ä¸€æ˜¯æ˜“äºŽä½¿ç”¨ã€‚å›¾ç»“æž„ä»£ç ç®€æ´ä¸”æ˜“äºŽè®¿é—®ã€‚ç‰¹åˆ«æ˜¯å½“æ‚¨æœ‰å¤æ‚çš„èŠ‚ç‚¹é€»è¾‘æ—¶ï¼Œæ‹¥æœ‰å›¾çš„å•ä¸€è§†å›¾ä½¿ç†è§£ä»£ç†ä¹‹é—´çš„è¿žæŽ¥å˜å¾—æ›´åŠ å®¹æ˜“ã€‚LangGraph è¿˜ä½¿å°†çŽ°æœ‰çš„åŸºäºŽ LangChain æž„å»ºçš„åº”ç”¨ç¨‹åºè½¬æ¢å˜å¾—ç®€å•ã€‚\n\n### å¤–å–\n\nå¦‚æžœæ‚¨ä½¿ç”¨æ¡†æž¶ä¸­çš„æ‰€æœ‰å†…å®¹ï¼ŒLangGraph å°†è¿è¡Œè‰¯å¥½ï¼›å¦‚æžœæ‚¨è¶…å‡ºå®ƒçš„èŒƒå›´ï¼Œè¯·å‡†å¤‡å¥½è¿›è¡Œä¸€äº›è°ƒè¯•ã€‚\n\n## LlamaIndex å·¥ä½œæµ\n\nå·¥ä½œæµæ˜¯ä»£ç†æ¡†æž¶é¢†åŸŸçš„æ–°è¿›å…¥è€…ï¼Œæ—©åœ¨ä»Šå¹´å¤å¤©é¦–æ¬¡äº®ç›¸ã€‚ä¸Ž LangGraph ç±»ä¼¼ï¼Œå®ƒæ—¨åœ¨ç®€åŒ–å¾ªçŽ¯ä»£ç†çš„æž„å»ºã€‚å·¥ä½œæµè¿˜ç‰¹åˆ«å…³æ³¨å¼‚æ­¥è¿è¡Œã€‚\n\nå·¥ä½œæµçš„ä¸€äº›å…ƒç´ ä¼¼ä¹Žæ˜¯å¯¹ LangGraph çš„ç›´æŽ¥å›žåº”ï¼Œç‰¹åˆ«æ˜¯å®ƒä½¿ç”¨äº‹ä»¶è€Œä¸æ˜¯è¾¹å’Œæ¡ä»¶è¾¹ã€‚å·¥ä½œæµä½¿ç”¨æ­¥éª¤ï¼ˆç±»ä¼¼äºŽ LangGraph ä¸­çš„èŠ‚ç‚¹ï¼‰æ¥å®¹çº³é€»è¾‘ï¼Œå¹¶é€šè¿‡å‘å‡ºå’ŒæŽ¥æ”¶äº‹ä»¶åœ¨æ­¥éª¤ä¹‹é—´ç§»åŠ¨ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*22WuFVBWctdeiCSL)\n\nä¸Šé¢çš„ç»“æž„çœ‹èµ·æ¥ä¸Ž LangGraph ç»“æž„ç›¸ä¼¼ï¼Œåªæ˜¯å¢žåŠ äº†ä¸€é¡¹å†…å®¹ã€‚æˆ‘åœ¨å·¥ä½œæµä¸­æ·»åŠ äº†ä¸€ä¸ªè®¾ç½®æ­¥éª¤ï¼Œä»¥å‡†å¤‡ä»£ç†ä¸Šä¸‹æ–‡ï¼Œæ›´å¤šå†…å®¹è¯·è§ä¸‹æ–‡ã€‚å°½ç®¡ç»“æž„ç›¸ä¼¼ï¼Œä½†æ”¯æ’‘å®ƒçš„ä»£ç å´æˆªç„¶ä¸åŒã€‚\n\n### å·¥ä½œæµæž¶æž„\n\nä¸‹é¢çš„ä»£ç å®šä¹‰äº†å·¥ä½œæµç»“æž„ã€‚ä¸Ž LangGraph ç±»ä¼¼ï¼Œè¿™é‡Œæ˜¯æˆ‘å‡†å¤‡çŠ¶æ€å¹¶å°†æŠ€èƒ½é™„åŠ åˆ° LLM å¯¹è±¡çš„åœ°æ–¹ã€‚\n\n```python\nclass AgentFlow(Workflow):\n    def __init__(self, llm, timeout=300):\n        super().__init__(timeout=timeout)\n        self.llm = llm\n        self.memory = ChatMemoryBuffer(token_limit=1000).from_defaults(llm=llm)\n        self.tools = []\n        for func in skill_map.get_function_list():\n            self.tools.append(\n                FunctionTool(\n                    skill_map.get_function_callable_by_name(func),\n                    metadata=ToolMetadata(\n                        name=func, description=skill_map.get_function_description_by_name(func)\n                    ),\n                )\n            )\n\n    @step\n    async def prepare_agent(self, ev: StartEvent) -> RouterInputEvent:\n        user_input = ev.input\n        user_msg = ChatMessage(role=\"user\", content=user_input)\n        self.memory.put(user_msg)\n\n        chat_history = self.memory.get()\n        return RouterInputEvent(input=chat_history)\n```\n\nè¿™ä¹Ÿæ˜¯æˆ‘å®šä¹‰é¢å¤–æ­¥éª¤â€œprepare\\_agentâ€çš„åœ°æ–¹ã€‚æ­¤æ­¥éª¤ä»Žç”¨æˆ·è¾“å…¥åˆ›å»ºä¸€ä¸ª ChatMessageï¼Œå¹¶å°†å…¶æ·»åŠ åˆ°å·¥ä½œæµå†…å­˜ä¸­ã€‚å°†å…¶åˆ†ç¦»ä¸ºå•ç‹¬æ­¥éª¤æ„å‘³ç€æˆ‘ä»¬åœ¨ä»£ç†å¾ªçŽ¯é€šè¿‡æ­¥éª¤æ—¶ä¼šè¿”å›žåˆ°å®ƒï¼Œè¿™é¿å…äº†é‡å¤å°†ç”¨æˆ·æ¶ˆæ¯æ·»åŠ åˆ°å†…å­˜ä¸­ã€‚\n\nåœ¨ LangGraph çš„æƒ…å†µä¸‹ï¼Œæˆ‘é€šè¿‡ä¸€ä¸ªä½äºŽå›¾å¤–çš„ run\\_agent æ–¹æ³•å®Œæˆäº†åŒæ ·çš„äº‹æƒ…ã€‚ç„¶è€Œï¼Œè¿™ä¸€å˜åŒ–ä¸»è¦æ˜¯é£Žæ ¼ä¸Šçš„ï¼Œç„¶è€Œåœ¨æˆ‘çœ‹æ¥ï¼Œå°†æ­¤é€»è¾‘ä¸Žå·¥ä½œæµå’Œå›¾å½¢ç»“åˆåœ¨ä¸€èµ·æ›´ä¸ºç®€æ´ã€‚\n\nè®¾ç½®å¥½å·¥ä½œæµåŽï¼Œæˆ‘æŽ¥ç€å®šä¹‰äº†è·¯ç”±ä»£ç ï¼š\n\n```python\n@step\nasync def router(self, ev: RouterInputEvent) -> ToolCallEvent | StopEvent:\n    messages = ev.input\n\n    if not any(\n        isinstance(message, dict) and message.get(\"role\") == \"system\" for message in messages\n    ):\n        system_prompt = ChatMessage(role=\"system\", content=SYSTEM_PROMPT)\n        messages.insert(0, system_prompt)\n\n    with using_prompt_template(template=SYSTEM_PROMPT, version=\"v0.1\"):\n        response = await self.llm.achat_with_tools(\n            model=\"gpt-4o\",\n            messages=messages,\n            tools=self.tools,\n        )\n\n    self.memory.put(response.message)\n\n    tool_calls = self.llm.get_tool_calls_from_response(response, error_on_no_tool_call=False)\n    if tool_calls:\n        return ToolCallEvent(tool_calls=tool_calls)\n    else:\n        return StopEvent(result=response.message.content)\n```\n\nä»¥åŠå·¥å…·è°ƒç”¨å¤„ç†ä»£ç ï¼š\n\n```python\n@step\nasync def tool_call_handler(self, ev: ToolCallEvent) -> RouterInputEvent:\n    tool_calls = ev.tool_calls\n\n    for tool_call in tool_calls:\n        function_name = tool_call.tool_name\n        arguments = tool_call.tool_kwargs\n        if \"input\" in arguments:\n            arguments[\"prompt\"] = arguments.pop(\"input\")\n\n        try:\n            function_callable = skill_map.get_function_callable_by_name(function_name)\n        except KeyError:\n            function_result = \"Error: Unknown function call\"\n\n        function_result = function_callable(arguments)\n        message = ChatMessage(\n            role=\"tool\",\n            content=function_result,\n            additional_kwargs={\"tool_call_id\": tool_call.tool_id},\n        )\n\n        self.memory.put(message)\n\n    return RouterInputEvent(input=self.memory.get())\n```\n\nè¿™ä¸¤è€…çœ‹èµ·æ¥æ›´åƒæ˜¯åŸºäºŽä»£ç çš„ä»£ç†ï¼Œè€Œä¸æ˜¯ LangGraph ä»£ç†ã€‚è¿™ä¸»è¦æ˜¯å› ä¸ºå·¥ä½œæµå°†æ¡ä»¶è·¯ç”±é€»è¾‘ä¿ç•™åœ¨æ­¥éª¤ä¸­ï¼Œè€Œä¸æ˜¯åœ¨æ¡ä»¶è¾¹ä¸­â€”â€”ç¬¬ 18 åˆ° 24 è¡Œåœ¨ LangGraph ä¸­æ˜¯ä¸€ä¸ªæ¡ä»¶è¾¹ï¼Œè€ŒçŽ°åœ¨å®ƒä»¬åªæ˜¯è·¯ç”±æ­¥éª¤çš„ä¸€éƒ¨åˆ†â€”â€”ä»¥åŠ LangGraph å…·æœ‰ä¸€ä¸ª ToolNode å¯¹è±¡ï¼Œå‡ ä¹Žè‡ªåŠ¨å¤„ç† tool\\_call\\_handler æ–¹æ³•ä¸­çš„æ‰€æœ‰å†…å®¹ã€‚\n\nåœ¨è·¯ç”±æ­¥éª¤ä¹‹åŽï¼Œæˆ‘éžå¸¸é«˜å…´åœ°çœ‹åˆ°æˆ‘å¯ä»¥å°†æˆ‘çš„ SkillMap å’ŒåŸºäºŽä»£ç çš„ä»£ç†ä¸­çš„çŽ°æœ‰æŠ€èƒ½ä¸Žå·¥ä½œæµä¸€èµ·ä½¿ç”¨ã€‚è¿™äº›æŠ€èƒ½æ— éœ€æ›´æ”¹å°±å¯ä»¥ä¸Žå·¥ä½œæµé…åˆä½¿ç”¨ï¼Œè¿™è®©æˆ‘çš„å·¥ä½œè½»æ¾äº†å¾ˆå¤šã€‚\n\n### å·¥ä½œæµçš„æŒ‘æˆ˜\n\n**æŒ‘æˆ˜ \\#1: åŒæ­¥ä¸Žå¼‚æ­¥**\n\nå°½ç®¡å¼‚æ­¥æ‰§è¡Œå¯¹äºŽå®žæ—¶ä»£ç†æ›´ä¸ºç†æƒ³ï¼Œä½†è°ƒè¯•åŒæ­¥ä»£ç†è¦å®¹æ˜“å¾—å¤šã€‚å·¥ä½œæµè®¾è®¡ä¸ºå¼‚æ­¥å·¥ä½œï¼Œå¼ºè¡Œå®žçŽ°åŒæ­¥æ‰§è¡Œéžå¸¸å›°éš¾ã€‚\n\næˆ‘æœ€åˆä»¥ä¸ºåªéœ€åŽ»æŽ‰â€œasyncâ€æ–¹æ³•æ ‡è¯†ï¼Œå°†â€œachat\\_with\\_toolsâ€åˆ‡æ¢ä¸ºâ€œchat\\_with\\_toolsâ€å³å¯ã€‚ç„¶è€Œï¼Œç”±äºŽWorkflowç±»ä¸­çš„åº•å±‚æ–¹æ³•ä¹Ÿè¢«æ ‡è®°ä¸ºå¼‚æ­¥ï¼Œå› æ­¤æœ‰å¿…è¦é‡æ–°å®šä¹‰è¿™äº›æ–¹æ³•ä»¥ä¾¿å®žçŽ°åŒæ­¥æ‰§è¡Œã€‚æœ€ç»ˆæˆ‘è¿˜æ˜¯åšæŒä½¿ç”¨å¼‚æ­¥æ–¹æ³•ï¼Œä½†è¿™å¹¶æ²¡æœ‰ä½¿è°ƒè¯•å˜å¾—æ›´åŠ å›°éš¾ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*78Hzqkiv9cI7W4UA)\n\n**æŒ‘æˆ˜ \\#2: Pydantic éªŒè¯é”™è¯¯**\n\nåœ¨ä¸ŽLangGraphçš„å›°å¢ƒé‡æ¼”ä¸­ï¼Œå…³äºŽæŠ€èƒ½çš„PydanticéªŒè¯é”™è¯¯å‡ºçŽ°äº†ç±»ä¼¼çš„é—®é¢˜ã€‚å¹¸è¿çš„æ˜¯ï¼Œç”±äºŽå·¥ä½œæµèƒ½å¤Ÿå¾ˆå¥½åœ°å¤„ç†æˆå‘˜å‡½æ•°ï¼Œè¿™æ¬¡è§£å†³èµ·æ¥è¦å®¹æ˜“å¾—å¤šã€‚æˆ‘æœ€ç»ˆä¸å¾—ä¸åœ¨ä¸ºæˆ‘çš„æŠ€èƒ½åˆ›å»ºLlamaIndex FunctionToolå¯¹è±¡æ—¶æ›´åŠ è§„èŒƒï¼š\n\n```python\nfor func in skill_map.get_function_list(): \n            self.tools.append(FunctionTool(\n                skill_map.get_function_callable_by_name(func), \n                metadata=ToolMetadata(name=func, description=skill_map.get_function_description_by_name(func))))\n```\n\n*æ‘˜è‡ª AgentFlow.\\_\\_init\\_\\_ï¼Œç”¨äºŽæž„å»º FunctionTools*\n\n### å·¥ä½œæµçš„å¥½å¤„\n\næž„å»º Workflows ä»£ç†æ¯”æž„å»º LangGraph ä»£ç†è¦å®¹æ˜“å¾—å¤šï¼Œä¸»è¦æ˜¯å› ä¸º Workflows ä»ç„¶è¦æ±‚æˆ‘è‡ªå·±ç¼–å†™è·¯ç”±é€»è¾‘å’Œå·¥å…·å¤„ç†ä»£ç ï¼Œè€Œä¸æ˜¯æä¾›å†…ç½®å‡½æ•°ã€‚è¿™ä¹Ÿæ„å‘³ç€æˆ‘çš„ Workflow ä»£ç†çœ‹èµ·æ¥ä¸Žæˆ‘çš„åŸºäºŽä»£ç çš„ä»£ç†æžä¸ºç›¸ä¼¼ã€‚\n\næœ€å¤§çš„åŒºåˆ«åœ¨äºŽäº‹ä»¶çš„ä½¿ç”¨ã€‚æˆ‘ä½¿ç”¨äº†ä¸¤ä¸ªè‡ªå®šä¹‰äº‹ä»¶åœ¨æˆ‘çš„ä»£ç†ä¸­ç§»åŠ¨æ­¥éª¤ï¼š\n\n```python\nclass ToolCallEvent(Event):\n    tool_calls: list[ToolSelection]\n\nclass RouterInputEvent(Event):\n    input: list[ChatMessage]\n```\n\nå‘å°„å™¨-æŽ¥æ”¶å™¨ã€åŸºäºŽäº‹ä»¶çš„æž¶æž„å–ä»£äº†ç›´æŽ¥è°ƒç”¨æˆ‘çš„ä»£ç†ä¸­çš„æŸäº›æ–¹æ³•ï¼Œæ¯”å¦‚å·¥å…·è°ƒç”¨å¤„ç†å™¨ã€‚\n\nå¦‚æžœæ‚¨æœ‰æ›´å¤æ‚çš„ç³»ç»Ÿï¼Œå…·æœ‰å¤šä¸ªå¼‚æ­¥è§¦å‘çš„æ­¥éª¤å¹¶å¯èƒ½å‘å‡ºå¤šä¸ªäº‹ä»¶ï¼Œè¿™ç§æž¶æž„å°†éžå¸¸æœ‰åŠ©äºŽå¹²å‡€åœ°ç®¡ç†è¿™äº›æƒ…å†µã€‚\n\nWorkflows çš„å…¶ä»–å¥½å¤„åŒ…æ‹¬å®ƒéžå¸¸è½»é‡ä¸”ä¸å¼ºè¿«æ‚¨ä½¿ç”¨å¾ˆå¤šç»“æž„ï¼ˆé™¤äº†æŸäº› LlamaIndex å¯¹è±¡çš„ä½¿ç”¨ï¼‰ï¼Œè€Œä¸”å®ƒçš„åŸºäºŽäº‹ä»¶çš„æž¶æž„ä¸ºç›´æŽ¥å‡½æ•°è°ƒç”¨æä¾›äº†ä¸€ä¸ªæœ‰ç”¨çš„æ›¿ä»£æ–¹æ¡ˆâ€”â€”ç‰¹åˆ«æ˜¯å¯¹äºŽå¤æ‚çš„å¼‚æ­¥åº”ç”¨ç¨‹åºã€‚\n\n## æ¯”è¾ƒæ¡†æž¶\n\nåœ¨è¿™ä¸‰ç§æ–¹æ³•ä¸­ï¼Œå„è‡ªéƒ½æœ‰å…¶ä¼˜ç‚¹ã€‚\n\næ— æ¡†æž¶çš„æ–¹æ³•æ˜¯æœ€ç®€å•çš„å®žçŽ°æ–¹å¼ã€‚å› ä¸ºä»»ä½•æŠ½è±¡éƒ½æ˜¯ç”±å¼€å‘è€…å®šä¹‰çš„ï¼ˆå³ä¸Šé¢ç¤ºä¾‹ä¸­çš„ SkillMap å¯¹è±¡ï¼‰ï¼Œä¿æŒå„ç§ç±»åž‹å’Œå¯¹è±¡çš„æ¸…æ™°æ˜¯å¾ˆå®¹æ˜“çš„ã€‚ç„¶è€Œï¼Œä»£ç çš„å¯è¯»æ€§å’Œå¯è®¿é—®æ€§å®Œå…¨å–å†³äºŽä¸ªåˆ«å¼€å‘è€…ï¼Œéšç€ä»£ç†çš„å¤æ‚æ€§å¢žåŠ ï¼Œå¦‚æžœæ²¡æœ‰ä¸€äº›å¼ºåˆ¶ç»“æž„ï¼Œå¾ˆå®¹æ˜“å˜å¾—æ··ä¹±ã€‚\n\nLangGraph æä¾›äº†ç›¸å½“å¤šçš„ç»“æž„ï¼Œè¿™ä½¿å¾—ä»£ç†çš„å®šä¹‰éžå¸¸æ˜Žç¡®ã€‚å¦‚æžœä¸€ä¸ªæ›´å¹¿æ³›çš„å›¢é˜Ÿåœ¨åä½œå¼€å‘ä»£ç†ï¼Œè¿™ç§ç»“æž„å°†æä¾›ä¸€ç§å¼ºæœ‰åŠ›çš„æž¶æž„å¼ºåˆ¶æ–¹å¼ã€‚å¯¹äºŽé‚£äº›ä¸å¤ªç†Ÿæ‚‰è¯¥ç»“æž„çš„äººï¼ŒLangGraph ä¹Ÿå¯èƒ½ä¸ºä»£ç†æä¾›ä¸€ä¸ªè‰¯å¥½çš„èµ·ç‚¹ã€‚ç„¶è€Œï¼Œè¿™ä¹Ÿæœ‰ä¸€ä¸ªæƒè¡¡â€”â€”ç”±äºŽ LangGraph ä¸ºä½ åšäº†å¾ˆå¤šäº‹æƒ…ï¼Œå¦‚æžœä½ æ²¡æœ‰å®Œå…¨æŽ¥å—è¿™ä¸ªæ¡†æž¶ï¼Œå¯èƒ½ä¼šå¯¼è‡´éº»çƒ¦ï¼›ä»£ç å¯èƒ½éžå¸¸å¹²å‡€ï¼Œä½†ä½ å¯èƒ½ä¼šä¸ºæ­¤ä»˜å‡ºæ›´å¤šçš„è°ƒè¯•æˆæœ¬ã€‚\n\nWorkflows åˆ™å¤„äºŽä¸­é—´ä½ç½®ã€‚åŸºäºŽäº‹ä»¶çš„æž¶æž„å¯èƒ½å¯¹æŸäº›é¡¹ç›®æžä¸ºæœ‰ç”¨ï¼Œè€Œä½¿ç”¨ LlamaIndex ç±»åž‹çš„è¦æ±‚è¾ƒå°‘ï¼Œä¸ºé‚£äº›æ²¡æœ‰åœ¨æ•´ä¸ªåº”ç”¨ç¨‹åºä¸­å®Œå…¨ä½¿ç”¨æ¡†æž¶çš„äººæä¾›äº†æ›´å¤§çš„çµæ´»æ€§ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*PITmiVGuG8QuDVX6)\n\næœ€ç»ˆï¼Œæ ¸å¿ƒé—®é¢˜å¯èƒ½åªæ˜¯â€œä½ æ˜¯å¦å·²ç»åœ¨ä½¿ç”¨ LlamaIndex æˆ– LangChain æ¥åè°ƒä½ çš„åº”ç”¨ç¨‹åºï¼Ÿâ€LangGraph å’Œ Workflows éƒ½ä¸Žå„è‡ªçš„åŸºç¡€æ¡†æž¶ç´§å¯†ç›¸è¿žï¼Œå› æ­¤æ¯ä¸ªç‰¹å®šäºŽä»£ç†çš„æ¡†æž¶çš„é¢å¤–å¥½å¤„å¯èƒ½ä¸è¶³ä»¥å•å‡­ä¼˜ç‚¹è€Œä¿ƒä½¿ä½ åˆ‡æ¢ã€‚\n\nçº¯ä»£ç çš„æ–¹æ³•å¯èƒ½å§‹ç»ˆæ˜¯ä¸€ä¸ªæœ‰å¸å¼•åŠ›çš„é€‰é¡¹ã€‚å¦‚æžœä½ æœ‰ä¸¥è°¨çš„æ–¹æ³•æ¥è®°å½•å’Œå¼ºåˆ¶æ‰§è¡Œä»»ä½•åˆ›å»ºçš„æŠ½è±¡ï¼Œé‚£ä¹ˆç¡®ä¿å¤–éƒ¨æ¡†æž¶ä¸ä¼šæ‹–æ…¢ä½ çš„é€Ÿåº¦æ˜¯å¾ˆå®¹æ˜“çš„ã€‚\n\n## é€‰æ‹©ä»£ç†æ¡†æž¶çš„å…³é”®é—®é¢˜\n\nå½“ç„¶ï¼Œâ€œè¿™è¦çœ‹æƒ…å†µâ€ä»Žæ¥ä¸æ˜¯ä¸€ä¸ªä»¤äººæ»¡æ„çš„ç­”æ¡ˆã€‚è¿™ä¸‰ä¸ªé—®é¢˜åº”è¯¥å¸®åŠ©ä½ å†³å®šåœ¨ä¸‹ä¸€ä¸ªä»£ç†é¡¹ç›®ä¸­ä½¿ç”¨å“ªä¸ªæ¡†æž¶ã€‚\n\n***ä½ æ˜¯å¦å·²ç»åœ¨é¡¹ç›®çš„é‡è¦éƒ¨åˆ†ä½¿ç”¨äº† LlamaIndex æˆ– LangChainï¼Ÿ***\n\nå¦‚æžœæ˜¯ï¼Œè¯·é¦–å…ˆæŽ¢ç´¢è¿™ä¸ªé€‰é¡¹ã€‚\n\n***ä½ æ˜¯å¦ç†Ÿæ‚‰å¸¸è§çš„ä»£ç†ç»“æž„ï¼Œè¿˜æ˜¯å¸Œæœ›æœ‰ä¸€äº›æŒ‡å¯¼æ¥å‘Šè¯‰ä½ å¦‚ä½•æž„å»ºä»£ç†ï¼Ÿ***\n\nå¦‚æžœä½ å±žäºŽåŽè€…ï¼Œå°è¯• Workflowsã€‚å¦‚æžœä½ *çœŸçš„*å±žäºŽåŽè€…ï¼Œå°è¯• LangGraphã€‚\n\n***ä½ çš„ä»£ç†ä¹‹å‰æ˜¯å¦å·²ç»æž„å»ºè¿‡ï¼Ÿ***\n\næ¡†æž¶çš„ä¸€ä¸ªå¥½å¤„æ˜¯æ¯ä¸ªæ¡†æž¶éƒ½æœ‰è®¸å¤šæ•™ç¨‹å’Œç¤ºä¾‹å¯ä¾›ä½¿ç”¨ã€‚è€Œçº¯ä»£ç ä»£ç†çš„ç¤ºä¾‹åˆ™å°‘å¾—å¤šã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*wF9aSF1db1yaniqO)\n\n## ç»“è®º\n\né€‰æ‹©ä¸€ä¸ªä»£ç†æ¡†æž¶åªæ˜¯ä¼—å¤šé€‰æ‹©ä¸­çš„ä¸€ä¸ªï¼Œè¿™å°†å½±å“ç”Ÿæˆå¼AIç³»ç»Ÿçš„ç”Ÿäº§ç»“æžœã€‚åƒå¾€å¸¸ä¸€æ ·ï¼Œå»ºç«‹ç¨³å¥çš„ä¿æŠ¤æŽªæ–½å’Œ [LLM tracing](https://docs.arize.com/phoenix/tracing/llm-traces) æ˜¯éžå¸¸é‡è¦çš„â€”â€”å¹¶ä¸”è¦çµæ´»åº”å¯¹æ–°çš„ä»£ç†æ¡†æž¶ã€ç ”ç©¶å’Œæ¨¡åž‹é¢ è¦†æ—¢å®šæŠ€æœ¯ã€‚\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/claude-3-5-haiku-anthropics-speed-demon-gets-a-brain-boost-82f2f0999d4f","frontmatter":{"title":"Claude 3.5 Haikuï¼šäººç±»çš„ é€Ÿåº¦ä¹‹é­” è„‘åŠ›å¤§å¢ž","meta_title":"Claude 3.5 Haikuï¼šäººç±»çš„ é€Ÿåº¦ä¹‹é­” è„‘åŠ›å¤§å¢ž","description":"Claude 3.5 Haikuæ˜¯AnthropicæŽ¨å‡ºçš„æœ€æ–°AIæ¨¡åž‹ï¼Œå…·å¤‡é«˜é€Ÿå’Œå“è¶Šæ™ºèƒ½ï¼Œè¶…è¶Šäº†å‰ä»»Claude 3 Opusã€‚å…¶ç¼–ç¨‹èƒ½åŠ›åœ¨SWE-benchæµ‹è¯•ä¸­è¡¨çŽ°å‡ºè‰²ï¼Œæ”¯æŒæ–‡æœ¬è¾“å…¥ï¼Œæœªæ¥å°†æ·»åŠ å›¾åƒåˆ†æžåŠŸèƒ½ã€‚å°½ç®¡ä»·æ ¼ä¸Šæ¶¨å››å€ï¼Œä½†é€šè¿‡æç¤ºç¼“å­˜å’Œæ‰¹å¤„ç†å¯é™ä½Žæˆæœ¬ã€‚è¯¥æ¨¡åž‹é€‚ç”¨äºŽè½¯ä»¶å¼€å‘ã€å®¢æœã€æ•°æ®å¤„ç†ç­‰å¤šç§åº”ç”¨ï¼Œæ ‡å¿—ç€äººå·¥æ™ºèƒ½é¢†åŸŸçš„é‡è¦è¿›æ­¥ã€‚","date":"2024-11-13T01:32:04.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*hLedIfhYJhS_ejPDwOQPIw.png","categories":["Programming","Machine Learning","Chatbots"],"author":"Rifx.Online","tags":["Claude","Haiku","coding","SWE-bench","benchmarks"],"draft":false,"slug":"blog/claude-3-5-haiku-anthropics-speed-demon-gets-a-brain-boost-82f2f0999d4f"},"content":"\n\n\n\n\nåœ¨äººå·¥æ™ºèƒ½è¿›æ­¥çš„æ— æƒ…ç«žèµ›ä¸­ï¼ŒAnthropicåˆšåˆšæŽ¨å‡ºäº†ä¸€ä½æ–°çš„ç«žäº‰è€…ã€‚è®¤è¯†ä¸€ä¸‹Claude 3\\.5 Haikuï¼Œè¿™æ˜¯ä»–ä»¬æœ€å¿«AIæ¨¡åž‹çš„æœ€æ–°ç‰ˆæœ¬ã€‚å°±åƒä»–ä»¬æŠŠçŸ­è·‘è¿åŠ¨å‘˜é€åˆ°äº†è„‘åŠ›è®­ç»ƒè¥ã€‚ç»“æžœå‘¢ï¼Ÿä¸€ä¸ªä¸ä»…åœ¨è¡ŒåŠ¨ä¸Šè¿…é€Ÿï¼Œè€Œä¸”åœ¨æŸäº›æ™ºåŠ›é¢†åŸŸèƒ½å¤Ÿè¶…è¶Šå…¶æ›´å¼ºå¤§å…„å¼Ÿçš„æ¨¡åž‹ã€‚è®©æˆ‘ä»¬æ·±å…¥äº†è§£ä¸€ä¸‹è¿™ä¸ªæ–°ç”Ÿäº‹ç‰©çš„è¿ä½œåŽŸç†ã€‚\n\n## é€Ÿåº¦ï¼ˆå’Œæ™ºæ…§ï¼‰çš„éœ€æ±‚\n\nAnthropic ä¹‹å‰çš„ Haiku æ¨¡åž‹å·²ç»æ˜¯ä»–ä»¬ AI ç³»åˆ—ä¸­çš„ä¹Œèµ›å› Â·åšå°”ç‰¹ã€‚çŽ°åœ¨ï¼Œä»–ä»¬ä¸çŸ¥æ€Žä¹ˆåœ°åœ¨è¿™ä¸ªé€Ÿåº¦æ€ªå…½ä¸­å¡žå…¥äº†æ›´å¤šçš„æ™ºåŠ›ï¼Œè€Œæ²¡æœ‰ç‰ºç‰²å…¶è¿…é€Ÿæ€§ã€‚è¿™å°±åƒçœ‹ç€ä¸€åªçŒŽè±¹åœ¨å¥”è·‘æ—¶è§£é­”æ–¹ã€‚\n\n## åŸºå‡†æµ‹è¯•çš„è¾‰ç…Œ\n\nClaude 3\\.5 Haiku ä¸ä»…é€Ÿåº¦å¿«ï¼Œè€Œä¸”æ™ºèƒ½æƒŠäººã€‚å®ƒåœ¨å„ç§æ™ºèƒ½åŸºå‡†æµ‹è¯•ä¸­è¡¨çŽ°ä¼˜äºŽ Claude 3 Opus â€”â€” Anthropic ä¹‹å‰çš„é‡é‡çº§å† å†›ã€‚è¿™ä¸ä»…ä»…æ˜¯ä¸€æ¬¡å°å¹…å‡çº§ï¼›è¿™æ˜¯ä¸€æ¬¡é£žè·ƒï¼Œä½¿å¾—äººå·¥æ™ºèƒ½ç¤¾åŒºä¸ºä¹‹çž©ç›®ã€‚\n\n## ç¼–ç¨‹èƒ½åŠ›\n\nå¦‚æžœä½ æ˜¯å¼€å‘è€…ï¼Œè¯·æ³¨æ„ã€‚è¿™ä¸ªæ¨¡åž‹åœ¨ç¼–ç é¢†åŸŸè¡¨çŽ°å‡ºè‰²ï¼Œåœ¨SWE-benchéªŒè¯æµ‹è¯•ä¸­å¾—åˆ†é«˜è¾¾40.6%ã€‚è¿™ä¸ä»…ä»¤äººå°è±¡æ·±åˆ»ï¼›è¿™æ˜¯ä¸€ç§è®©äººç±»ç¨‹åºå‘˜ç´§å¼ åœ°å…³æ³¨ä»–ä»¬å·¥ä½œå®‰å…¨çš„è¡¨çŽ°ã€‚\n\n## å¼•æ“Žç›–ä¸‹çš„ç§˜å¯†\n\nè®©æˆ‘ä»¬æ‰“å¼€å¼•æ“Žç›–ï¼Œçœ‹çœ‹æ˜¯ä»€ä¹ˆé©±åŠ¨ç€è¿™ä¸ªäººå·¥æ™ºèƒ½çƒ­è½¦ï¼š\n\n* **å¯ç”¨æ€§**ï¼šä½ å¯ä»¥é€šè¿‡ Anthropic çš„ APIã€Amazon Bedrock æˆ– Google Cloud çš„ Vertex AI æ¥ä½“éªŒå®ƒã€‚è¿™å°±åƒåœ¨æ‰€æœ‰ä¸»è¦æµåª’ä½“å¹³å°ä¸Šéƒ½æœ‰çš„äººå·¥æ™ºèƒ½ç‰ˆæœ¬ã€‚\n* **è¾“å…¥**ï¼šç›®å‰åªæ”¯æŒæ–‡æœ¬ã€‚è¿˜æ²¡æœ‰å›¾åƒåˆ†æžï¼Œä½†è¿™åŠŸèƒ½å³å°†æŽ¨å‡ºã€‚è¿™å°±åƒæœ‰ä¸€ä¸ªå¤©æ‰çš„ç¬”å‹ï¼Œä½†ä»–æ— æ³•æŸ¥çœ‹ä½ çš„åº¦å‡ç…§ç‰‡ã€‚\n* **çŸ¥è¯†æˆªæ­¢æ—¥æœŸ**ï¼š2024å¹´7æœˆã€‚å› æ­¤ï¼Œå®ƒçŸ¥é“ä½ åŽ»å¹´å¤å¤©åšçš„é‚£ä»¶å°´å°¬çš„äº‹æƒ…ï¼Œä½†ä¸çŸ¥é“æ˜Žå¹´çš„æµè¡Œæ¢—ã€‚\n* **è¾“å‡ºé•¿åº¦**ï¼šæ¯”å‰ä¸€ç‰ˆæœ¬æœ‰æ‰€æ”¹è¿›ã€‚å®ƒçŽ°åœ¨å¯ä»¥å†™æ›´é•¿çš„æ–‡ç« æ¥å¸®ä½ æ‹–å»¶æ—¶é—´ã€‚\n\n## Show Me the Money\n\nçŽ°åœ¨ï¼Œäº‹æƒ…å˜å¾—æœ‰è¶£äº†ã€‚Anthropic å†³å®šå¯¹è¿™ä¸ªå‡çº§ç‰ˆæ¨¡åž‹æ”¶å–é«˜é¢è´¹ç”¨ï¼š\n\n* æ¯ç™¾ä¸‡ä¸ªè¾“å…¥ä»¤ç‰Œ $1\n* æ¯ç™¾ä¸‡ä¸ªè¾“å‡ºä»¤ç‰Œ $5\n\nè¿™æ¯”ä¹‹å‰çš„ç‰ˆæœ¬å¢žåŠ äº†å››å€ã€‚å°±åƒä»–ä»¬æŠŠæœ¬ç”°æ€åŸŸå˜æˆäº†ç‰¹æ–¯æ‹‰ï¼Œå¹¶ç›¸åº”åœ°è°ƒæ•´äº†ä»·æ ¼ã€‚\n\nä½†åˆ«æ‹…å¿ƒï¼ŒèŠ‚ä¿­çš„æœ‹å‹ä»¬ï¼è¿˜æœ‰çœé’±çš„æ–¹æ³•ï¼š\n\n* æç¤ºç¼“å­˜å¯ä»¥èŠ‚çœé«˜è¾¾ 90%ã€‚è¿™å°±åƒä¸º AI è¿›è¡Œæžé™ä¼˜æƒ åˆ¸æ´»åŠ¨ã€‚\n* ä½¿ç”¨æ¶ˆæ¯æ‰¹å¤„ç† API çš„æ‰¹é‡å¤„ç†å¯ä»¥å°†æˆæœ¬é™ä½Žé«˜è¾¾ 50%ã€‚å¤§å®—è´­ä¹°ï¼Œä½†ç”¨äºŽè®¡ç®—ã€‚\n\n## è¿™ä¸ªä¸œè¥¿èƒ½åšä»€ä¹ˆï¼Ÿ\n\nClaude 3\\.5 Haiku ä¸ä»…ä»…æ˜¯ä¸€ä¸ªæ´¾å¯¹æŠŠæˆã€‚å®ƒæœ‰ä¸€äº›ä¸¥è‚ƒçš„çŽ°å®žåº”ç”¨ï¼š\n\n* **è½¯ä»¶å¼€å‘**ï¼šå°±åƒæœ‰ä¸€ä¸ªæ°¸ä¸ç¡è§‰ä¸”ä¸å·ä½ é›¶é£Ÿçš„ç¼–ç¨‹ä¼™ä¼´ã€‚\n* **èŠå¤©æœºå™¨äºº**ï¼šä¸éœ€è¦å’–å•¡ä¼‘æ¯æˆ–äººåŠ›èµ„æºå¹²é¢„çš„å®¢æœä»£è¡¨ã€‚\n* **æ•°æ®å¤„ç†**ï¼šå®ƒèƒ½æ¯”ä½ è¯´â€œæ•°æ®å¤§â€è¿˜å¿«åœ°å¤„ç†æ•°å­—ã€‚\n* **æ•™è‚²**ï¼šä¸€ä¸ªéšæ—¶å¾…å‘½ä¸”ä»Žä¸å¤±åŽ»è€å¿ƒçš„è¾…å¯¼è€å¸ˆã€‚\n* **ä¸ªæ€§åŒ–**ï¼šå®ƒè®°ä½ä½ çš„åå¥½æ¯”ä½ çš„å¦ä¸€åŠè¿˜è¦å¥½ã€‚\n* **ä¸“ä¸šä»»åŠ¡**ï¼šAI å­ä»£ç†çš„ç‘žå£«å†›åˆ€ã€‚\n* **å†…å®¹å®¡æ ¸**ï¼šæ¯æ¬¡å‘å¸ƒéƒ½åœ¨ä¿æŒäº’è”ç½‘çš„æ¸…æ´ã€‚\n\n## æƒè¡¡\n\nçŽ°åœ¨ï¼Œå¹¶ä¸æ˜¯æ‰€æœ‰äº‹æƒ…éƒ½é‚£ä¹ˆç¾Žå¥½ã€‚è¿˜æœ‰ä¸€äº›é—®é¢˜ï¼š\n\n* ç›®å‰è¿˜æ²¡æœ‰å›¾åƒåˆ†æžåŠŸèƒ½ã€‚å› æ­¤ï¼Œå®ƒæ— æ³•å‘Šè¯‰ä½ é‚£æ¡è£™å­æ˜¯å¦è®©ä½ çœ‹èµ·æ¥èƒ–ã€‚\n* ä»·æ ¼ä¸Šæ¶¨å¯èƒ½ä¼šè®©ä¸€äº›ç”¨æˆ·ç»§ç»­ä½¿ç”¨æ—§ç‰ˆã€ä¾¿å®œçš„ç‰ˆæœ¬ã€‚è¿™å°±åƒäººä»¬ä»ç„¶ä½¿ç”¨ Windows 7 çš„ AI ç­‰ä»·ç‰©ã€‚\n\n## åº•çº¿\n\nClaude 3\\.5 Haiku æ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸçš„ä¸€æ¬¡é‡è¦é£žè·ƒã€‚å®ƒçš„é€Ÿåº¦å¿«å¦‚å­å¼¹ï¼ŒåŠ›é‡å¼ºäºŽæœºè½¦ï¼Œèƒ½å¤Ÿä¸€è·ƒè€Œè¿‡é«˜æ¥¼å¤§åŽ¦ã€‚å¥½å§ï¼Œä¹Ÿè®¸æœ€åŽé‚£éƒ¨åˆ†ä¸å¤ªå‡†ç¡®ï¼Œä½†ä½ æ˜Žç™½æˆ‘çš„æ„æ€ã€‚\n\nå¯¹äºŽå¸Œæœ›åˆ©ç”¨äººå·¥æ™ºèƒ½å¤„ç†å¤æ‚ä»»åŠ¡çš„å¼€å‘è€…å’Œä¼ä¸šæ¥è¯´ï¼ŒClaude 3\\.5 Haiku æ˜¯ä¸€ä¸ªå¼•äººæ³¨ç›®çš„é€‰æ‹©ã€‚å®ƒä¸ä»…ä»…æ˜¯ä¸€æ¬¡å‡çº§ï¼›å®ƒé‡æ–°å®šä¹‰äº†åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸé€Ÿåº¦ä¸Žæ™ºèƒ½äº¤æ±‡å¤„çš„å¯èƒ½æ€§ã€‚\n\nçŽ°åœ¨çš„é—®é¢˜æ˜¯ï¼šç«žäº‰å¯¹æ‰‹å°†å¦‚ä½•å›žåº”ï¼Ÿæ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬è¦ç­‰å¤šä¹…æ‰èƒ½çœ‹åˆ° Claude 4\\.0ï¼šæ‰“æ²¹è¯—ç‰ˆï¼Ÿ\n\n## å¸¸è§é—®é¢˜è§£ç­”\n\n**é—®ï¼šClaude 3\\.5 Haiku èƒ½åˆ†æžå›¾åƒå—ï¼Ÿ**ç­”ï¼šè¿˜ä¸èƒ½ï¼Œä½†Anthropicè®¡åˆ’åœ¨æœªæ¥æ·»åŠ æ­¤åŠŸèƒ½ã€‚ç›®å‰ï¼Œå®ƒä»…æ”¯æŒæ–‡æœ¬ã€‚\n\n**é—®ï¼šClaude 3\\.5 Haiku æ¯”å…¶å‰èº«è´µå¤šå°‘ï¼Ÿ**ç­”ï¼šè´µå››å€ï¼Œä½†å¯ä»¥é€šè¿‡æç¤ºç¼“å­˜å’Œæ‰¹å¤„ç†æ¥é™ä½Žæˆæœ¬ã€‚\n\n**é—®ï¼šClaude 3\\.5 Haiku æœ€ä»¤äººå°è±¡æ·±åˆ»çš„åŠŸèƒ½æ˜¯ä»€ä¹ˆï¼Ÿ**ç­”ï¼šå®ƒåœ¨å„ç§æ™ºèƒ½åŸºå‡†æµ‹è¯•ä¸­èƒ½å¤Ÿè¶…è¶Šæ›´å¤§çš„æ¨¡åž‹ï¼Œå¦‚Claude 3 Opusï¼ŒåŒæ—¶ä¿æŒé«˜é€Ÿåº¦ã€‚\n\n**é—®ï¼šæˆ‘å¯ä»¥ç”¨ Claude 3\\.5 Haiku è¿›è¡Œè½¯ä»¶å¼€å‘å—ï¼Ÿ**ç­”ï¼šå½“ç„¶å¯ä»¥ã€‚å®ƒåœ¨ç¼–ç ä»»åŠ¡ä¸Šè¡¨çŽ°å‡ºè‰²ï¼Œèƒ½å¤Ÿæä¾›å¿«é€Ÿã€å‡†ç¡®çš„ä»£ç å»ºè®®å’Œè¡¥å…¨ã€‚\n\n**é—®ï¼šClaude 3\\.5 Haiku å‘å…¬ä¼—å¼€æ”¾å—ï¼Ÿ**ç­”ï¼šæ˜¯çš„ï¼Œå¯ä»¥é€šè¿‡Anthropicçš„APIã€Amazon Bedrockå’ŒGoogle Cloudçš„Vertex AIè®¿é—®ã€‚\n\n\\#Claude35Haiku \\#AnthropicAI \\#AIInnovation \\#MachineLearning \\#AIForDevelopers \\#FutureOfAI \\#AIPerformance \\#TechInnovation\n\nâ€œClaude 3\\.5 Haiku æ€§èƒ½åŸºå‡†â€ï¼Œâ€œAI æ¨¡åž‹å®šä»·æ¯”è¾ƒâ€ï¼Œâ€œç”¨äºŽè½¯ä»¶å¼€å‘çš„å¿«é€Ÿ AI æ¨¡åž‹â€ï¼Œâ€œAnthropic AI æ¨¡åž‹èƒ½åŠ›â€ï¼Œâ€œæˆæœ¬æ•ˆç›Šé«˜çš„ AI å®žæ–½ç­–ç•¥â€\n\n"},{"lang":"zh","group":"blog","slug":"blog/claude-3-5-sonnet-new-pioneering-the-future-of-ai-with-computer-control-capabilities-37a6ff9f9033","frontmatter":{"title":"Claude 3.5 Sonnetï¼ˆæ–°ï¼‰ï¼šåˆ©ç”¨è®¡ç®—æœºæŽ§åˆ¶èƒ½åŠ›å¼€æ‹“äººå·¥æ™ºèƒ½çš„æœªæ¥","meta_title":"Claude 3.5 Sonnetï¼ˆæ–°ï¼‰ï¼šåˆ©ç”¨è®¡ç®—æœºæŽ§åˆ¶èƒ½åŠ›å¼€æ‹“äººå·¥æ™ºèƒ½çš„æœªæ¥","description":"Anthropic äºŽ 2024 å¹´ 10 æœˆ 22 æ—¥å‘å¸ƒäº†å…¶æœ€æ–°çš„ AI æ¨¡åž‹ Claude 3.5 Sonnetã€‚æ­¤ç‰ˆæœ¬å¼•å…¥äº†é©å‘½æ€§çš„è®¡ç®—æœºæŽ§åˆ¶â€¦â€¦","date":"2024-10-27T13:57:00.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*n0NkOFbhUm7_fllJ","categories":["Programming","Technology","Generative AI"],"author":"Rifx.Online","tags":["Claude","Sonnet","automation","benchmarks","safety"],"draft":false,"slug":"blog/claude-3-5-sonnet-new-pioneering-the-future-of-ai-with-computer-control-capabilities-37a6ff9f9033"},"content":"\n\n\n\n\nAnthropicäºŽ2024å¹´10æœˆ22æ—¥å‘å¸ƒäº†æœ€æ–°çš„AIæ¨¡åž‹Claude 3.5 Sonnetã€‚æ­¤æ¬¡å‘å¸ƒå¼•å…¥äº†é©å‘½æ€§çš„è®¡ç®—æœºæŽ§åˆ¶èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®žçŽ°äº†æ˜¾è‘—æ”¹è¿›ï¼Œä¸ºAIè¡Œä¸šè®¾å®šäº†æ–°æ ‡å‡†ã€‚\n\n## é©å‘½æ€§çš„è®¡ç®—æœºæŽ§åˆ¶ï¼šæ–°å‰æ²¿\n\nClaude 3.5 Sonnet çš„çªå‡ºç‰¹ç‚¹æ˜¯å…¶èƒ½å¤Ÿåƒäººç±»ä¸€æ ·ä¸Žè®¡ç®—æœºè¿›è¡Œäº¤äº’ã€‚è¿™ä¸€çªç ´æ€§çš„èƒ½åŠ›ä½¿å¾— AI å¯ä»¥ï¼š\n\n* ä½¿ç”¨é¼ æ ‡å’Œé”®ç›˜è¾“å…¥å¯¼èˆªæ¡Œé¢ç•Œé¢\n* ä¸Žå„ç§åº”ç”¨ç¨‹åºå’Œç½‘é¡µæµè§ˆå™¨è¿›è¡Œäº¤äº’\n* æ‰§è¡Œå¤æ‚çš„å¤šæ­¥éª¤ä»»åŠ¡\n* æ‰§è¡Œæ–‡ä»¶ç®¡ç†æ“ä½œ\n* è‡ªåŠ¨åŒ–é‡å¤çš„å·¥ä½œæµç¨‹\n\nè¿™ä¸€è®¡ç®—æœºæŽ§åˆ¶åŠŸèƒ½ç›®å‰å¤„äºŽå…¬å¼€æµ‹è¯•é˜¶æ®µï¼Œä»£è¡¨äº† AI ç³»ç»Ÿä¸Žæ•°å­—ç•Œé¢äº¤äº’æ–¹å¼çš„èŒƒå¼è½¬å˜ã€‚å°½ç®¡ä»å¤„äºŽå®žéªŒé˜¶æ®µï¼Œä½†æ—©æœŸæµ‹è¯•æ˜¾ç¤ºå‡ºè‰¯å¥½çš„ç»“æžœï¼ŒClaude 3.5 Sonnet åœ¨ä»…æˆªå›¾ä»»åŠ¡çš„ OSWorld åŸºå‡†æµ‹è¯•ä¸­å¾—åˆ†ä¸º 14.9% â€” æ˜¾è‘—é«˜äºŽä¸‹ä¸€ä¸ªæœ€ä½³ç³»ç»Ÿçš„ 7.8%ã€‚\n\n## åŸºå‡†çªç ´æ€§èƒ½\n\nå‡çº§åŽçš„æ¨¡åž‹åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¡¨çŽ°å‡ºæ˜¾è‘—çš„æ”¹è¿›ï¼š\n\n## ç¼–ç å’ŒæŠ€æœ¯ä»»åŠ¡\n\n* åœ¨SWE-bench Verifiedä¸Šçš„æ€§èƒ½ä¸º49%ï¼ˆè¾ƒä¹‹å‰çš„33.4%æœ‰æ‰€æå‡ï¼‰\n* åœ¨HumanEvalç¼–ç ä»»åŠ¡ä¸­çš„å¾—åˆ†ä¸º93.7%\n* åœ¨è½¯ä»¶å·¥ç¨‹æ–¹é¢çš„è¡¨çŽ°ä¼˜äºŽä¸“ä¸šç¼–ç ç³»ç»Ÿ\n\n## å­¦æœ¯å’ŒæŽ¨ç†èƒ½åŠ›\n\n* 65% çš„ç ”ç©¶ç”Ÿçº§æŽ¨ç† (GPQA-Diamond)\n* 78% çš„æœ¬ç§‘çº§çŸ¥è¯† (MMLU Pro)\n* 78.3% çš„æ•°å­¦é—®é¢˜è§£å†³èƒ½åŠ› (MATH)\n\n## å•†ä¸šåº”ç”¨\n\n* 69.2% åœ¨é›¶å”®é¢†åŸŸä»»åŠ¡ä¸Š (TAU-bench)\n* 46% åœ¨èˆªç©ºé¢†åŸŸä»»åŠ¡ä¸Š\n* 90.8% åœ¨å›¾è¡¨åˆ†æžä¸Šçš„å‡†ç¡®çŽ‡\n* 94.2% åœ¨æ–‡æ¡£é—®ç­”ä¸Šçš„å‡†ç¡®çŽ‡\n\n## ä¼ä¸šé›†æˆä¸Žå¯ç”¨æ€§\n\nClaude 3.5 Sonnet å¯ä»¥é€šè¿‡å¤šä¸ªå¹³å°è®¿é—®ï¼š\n\n* Anthropic API\n* Amazon Bedrock\n* Google Cloudâ€™s Vertex AI\n\nåŒ…æ‹¬ Asanaã€Canvaã€DoorDash å’Œ Replit åœ¨å†…çš„ä¸»è¦å…¬å¸å·²ç»å¼€å§‹åœ¨å…¶å·¥ä½œæµç¨‹ä¸­å®žæ–½ Claude 3.5 Sonnet çš„åŠŸèƒ½ï¼Œç‰¹åˆ«æ˜¯åˆ©ç”¨å…¶è®¡ç®—æœºæŽ§åˆ¶åŠŸèƒ½æ¥å¤„ç†å¤æ‚çš„è‡ªåŠ¨åŒ–ä»»åŠ¡ã€‚\n\n## å®žé™…åº”ç”¨\n\n## è½¯ä»¶å¼€å‘\n\n* è‡ªåŠ¨åŒ–ä»£ç æµ‹è¯•å’Œè°ƒè¯•\n* æ™ºèƒ½IDEäº¤äº’\n* ä»£ç å®¡æŸ¥ä¸Žä¼˜åŒ–\n* æ–‡æ¡£ç”Ÿæˆ\n\n## å®¢æˆ·æ”¯æŒ\n\n* é«˜çº§èŠå¤©æœºå™¨äººåŠŸèƒ½\n* å¯è§†åŒ–æ•°æ®è§£è¯»\n* è‡ªåŠ¨åŒ–å·¥å•è§£å†³\n* æµç¨‹è‡ªåŠ¨åŒ–\n\n## å•†ä¸šè¿è¥\n\n* æ–‡æ¡£å¤„ç†ä¸Žåˆ†æž\n* ä»Žè§†è§‰æºæå–æ•°æ®\n* å·¥ä½œæµè‡ªåŠ¨åŒ–\n* å¤æ‚é—®é¢˜è§£å†³\n\n## å®‰å…¨ä¸Žè´£ä»»\n\nAnthropic å·²å®žæ–½å¼ºæœ‰åŠ›çš„å®‰å…¨æŽªæ–½ç”¨äºŽè®¡ç®—æœºæŽ§åˆ¶åŠŸèƒ½ï¼š\n\n* æ–°åˆ†ç±»å™¨ä»¥è¯†åˆ«æ½œåœ¨çš„è¯¯ç”¨\n* ä¸»åŠ¨ç›‘æŽ§ç³»ç»Ÿ\n* é™åˆ¶å¯¹æ•æ„Ÿæ“ä½œçš„è®¿é—®\n* å®šæœŸå®‰å…¨è¯„ä¼°\n\n## å±•æœ›æœªæ¥\n\nè™½ç„¶Claude 3.5 Sonnetåœ¨äººå·¥æ™ºèƒ½èƒ½åŠ›æ–¹é¢ä»£è¡¨äº†é‡å¤§è¿›å±•ï¼Œä½†é‡è¦çš„æ˜¯è¦æ³¨æ„æŸäº›åŠŸèƒ½ï¼Œç‰¹åˆ«æ˜¯è®¡ç®—æœºæŽ§åˆ¶ï¼Œä»å¤„äºŽæ—©æœŸé˜¶æ®µã€‚æŸäº›æ“ä½œå¦‚æ»šåŠ¨ã€æ‹–åŠ¨å’Œç¼©æ”¾é¢ä¸´æŒ‘æˆ˜ï¼ŒAnthropicé¼“åŠ±å¼€å‘è€…åœ¨æŽ¢ç´¢è¿™äº›æ–°åŠŸèƒ½æ—¶ï¼Œä»Žä½Žé£Žé™©ä»»åŠ¡å¼€å§‹ã€‚\n\nClaude 3.5 Sonnetçš„å‘å¸ƒæ ‡å¿—ç€äººå·¥æ™ºèƒ½å‘å±•çš„ä¸€ä¸ªå…³é”®æ—¶åˆ»ï¼Œå°†å…ˆè¿›çš„æŽ¨ç†èƒ½åŠ›ä¸Žå®žç”¨çš„è®¡ç®—æœºæŽ§åˆ¶åŠŸèƒ½ç›¸ç»“åˆã€‚éšç€æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œæˆ‘ä»¬å¯ä»¥æœŸå¾…çœ‹åˆ°æ›´å¤šåˆ›æ–°çš„åº”ç”¨ä»¥åŠäººå·¥æ™ºèƒ½ç³»ç»Ÿä¸Žæˆ‘ä»¬çš„æ•°å­—ä¸–ç•Œäº’åŠ¨æ–¹å¼çš„æ”¹è¿›ã€‚\n\n*æœ¬æ–‡åŸºäºŽAnthropicã€AWSå’Œå„ç±»æŠ€æœ¯åˆä½œä¼™ä¼´çš„å®˜æ–¹å…¬å‘Šå’Œæ–‡æ¡£ã€‚æœ‰å…³æœ€æ–°ä¿¡æ¯ï¼Œè¯·å‚è€ƒAnthropicçš„å®˜æ–¹æ–‡æ¡£ã€‚*\n\n"},{"lang":"zh","group":"blog","slug":"blog/claude-3-5-sonnet-v-s-gpt-4o-which-one-is-better-3b3675195bf9","frontmatter":{"title":"Claude 3.5 Sonnet V/S GPT-4Oï¼šå“ªä¸€ä¸ªæ›´å¥½","meta_title":"Claude 3.5 Sonnet V/S GPT-4Oï¼šå“ªä¸€ä¸ªæ›´å¥½","description":"2022 å¹´ 11 æœˆï¼ŒOpenAI æŽ¨å‡ºäº† ChatGPT æ¨¡åž‹ï¼Œè¯¥æ¨¡åž‹å½»åº•æ”¹å˜äº†æˆ‘ä»¬æœç´¢å’Œä¸Žä¿¡æ¯äº¤äº’çš„æ–¹å¼ã€‚æ˜Žå¹´ï¼Œåœ¨â€¦","date":"2024-10-27T13:59:09.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*4MXLuSFfGwFkWWn0","categories":["Generative AI","Machine Learning","Natural Language Processing"],"author":"Rifx.Online","tags":["GPT-4o","Claude","multimodal","reasoning","code-generation"],"draft":false,"slug":"blog/claude-3-5-sonnet-v-s-gpt-4o-which-one-is-better-3b3675195bf9"},"content":"\n\n\nåœ¨2022å¹´11æœˆï¼ŒOpenAIæŽ¨å‡ºäº†ChatGPTï¼Œè¿™ä¸€æ¨¡åž‹å½»åº•æ”¹å˜äº†æˆ‘ä»¬æœç´¢å’Œä¸Žä¿¡æ¯äº’åŠ¨çš„æ–¹å¼ã€‚æ¬¡å¹´3æœˆï¼Œç”±å‰OpenAIå‘˜å·¥åˆ›åŠžçš„ç¾Žå›½åˆåˆ›å…¬å¸â€œAnthropicâ€æŽ¨å‡ºäº†ä»–ä»¬è‡ªå·±çš„AIæ¨¡åž‹â€œClaudeâ€ã€‚è‡ªå‘å¸ƒä»¥æ¥ï¼Œè¿™ä¸¤å®¶AIå…¬å¸ä¸€ç›´åœ¨ç«žäº‰ï¼Œä»¥é€šè¿‡å…¶AIæ¨¡åž‹ä¸ºå®¢æˆ·æä¾›æœ€ä½³çš„åŠŸèƒ½å’Œä½“éªŒã€‚æœ€è¿‘ï¼ŒOpenAIæŽ¨å‡ºäº†â€œGPT-4oâ€ï¼Œè¿™æ˜¯ä¸€ä¸ªä»¤äººæƒŠå¹çš„æ¨¡åž‹ï¼Œèƒ½å¤Ÿå‡ºè‰²åœ°å¤„ç†æ–‡ä»¶ã€è¯­éŸ³å’Œè§†é¢‘æ•°æ®ã€‚åŒæ ·ï¼ŒClaudeæŽ¨å‡ºäº†â€œClaude 3.5 Sonnetâ€ï¼Œä»–ä»¬å£°ç§°è¿™æ˜¯æœ€å…ˆè¿›çš„AIæ¨¡åž‹ï¼Œèƒ½å¤Ÿå¤„ç†å¤æ‚ä»»åŠ¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†ç¡®å®šClaude 3.5 Sonnetå’ŒGPT-4oä¹‹é—´å“ªä¸ªæ›´å¥½ï¼Œå¹¶æ¯”è¾ƒå…¶åœ¨ç›¸åŒè¾“å…¥ä¸‹çš„åŠŸèƒ½å’Œè¾“å‡ºï¼Œä»¥æ£€æŸ¥å“ªä¸ªæ›´é€‚åˆæ‚¨ã€‚\n\n## èƒ½åŠ›å’Œç‰¹æ€§\n\n### GPT-4o\n\n\n\nGPT-4o æ˜¯ OpenAI æœ€æ–°æŽ¨å‡ºçš„ LLMã€‚â€œoâ€ ä»£è¡¨ omniï¼Œæ„ä¸ºæ‹‰ä¸è¯­ä¸­çš„â€œæ¯ä¸€ä¸ªâ€ã€‚è¯¥æ¨¡åž‹å¯ä»¥åˆ†æžè¯­éŸ³ã€å›¾åƒã€è§†é¢‘å’Œæ–‡ä»¶ä½œä¸ºè¾“å…¥ï¼Œå¹¶ç›¸åº”åœ°åšå‡ºå›žåº”ã€‚å®ƒå¯ä»¥æŽ¥å—è¯­éŸ³è¾“å…¥ï¼Œå¹¶ä»¥ä¸åŒè§’è‰²çš„å£°éŸ³è¾“å‡ºï¼ŒåŒ…æ‹¬è¯­è°ƒã€æƒ…æ„Ÿç­‰ã€‚æ•´ä¸ªè¿‡ç¨‹ä¸Žäººç±»å¯¹è¯çš„å»¶è¿Ÿç›¸å½“ä½Žï¼Œå¹³å‡ä¸º 0.32 ç§’ï¼Œè€Œå…¶ä»–è¯­éŸ³æ¨¡åž‹åˆ™ä¸º 2.8 ç§’ã€‚å®ƒè¿˜å…è®¸ç”¨æˆ·ç”Ÿæˆä¹¦é¢å†…å®¹ï¼Œå¦‚æ–‡ç« ã€åšå®¢ã€äº§å“æè¿°ã€ä¸åŒç¼–ç¨‹è¯­è¨€çš„ä»£ç ã€æ•°æ®åˆ†æžã€å›¾è¡¨ç­‰ã€‚æ­¤å¤–ï¼ŒGPT-4o è¿˜å¯ä»¥åˆ†æžå›¾åƒå’Œè§†é¢‘ï¼Œä½¿è¯¥æ¨¡åž‹å¯ä»¥å……å½“è¯­è¨€ç¿»è¯‘å™¨ã€ä¸ªäººåŠ©ç†ã€è™šæ‹Ÿæ•™å¸ˆæˆ–è´­ç‰©åŠ©æ‰‹ã€‚å®ƒè¿˜å¯ä»¥ç”¨äºŽåŒ»å­¦ã€å·¥ç¨‹ã€å†›äº‹ç­‰é¢†åŸŸã€‚è¦ä½¿ç”¨æ­¤åŠŸèƒ½ï¼ŒGPT-4o å¯ä»¥ä½¿ç”¨ç”¨æˆ·çš„æ‘„åƒå¤´èŽ·å–å®žæ—¶è§†å›¾ï¼Œå¹¶åœ¨è¯­éŸ³æ¨¡å¼ä¸‹ç›¸åº”åœ°å›žåº”ã€‚å®ƒè¿˜å¯ä»¥è®¿é—®æ‚¨çš„è®¡ç®—æœºå±å¹•ï¼Œå¹¶æè¿°å±å¹•ä¸Šæ˜¾ç¤ºçš„å†…å®¹ï¼Œç”¨æˆ·å¯ä»¥è¯¢é—®ä¸Žå±å¹•ä¸Šæ˜¾ç¤ºçš„å†…å®¹ç›¸å…³çš„é—®é¢˜ã€‚\n\n*ä¾‹å¦‚ï¼Œç”¨æˆ·å¯ä»¥åœ¨å±å¹•ä¸Šå¯ç”¨è¯¥æ¨¡åž‹ï¼Œæ‰“å¼€ VS ä»£ç ï¼Œå¹¶æç¤ºæ¨¡åž‹å……å½“ç¼–ç åŠ©æ‰‹ï¼Œä»¥èŽ·å–ç¼–ç é—®é¢˜çš„ç­”æ¡ˆã€‚æˆ–è€…ï¼Œæ‚¨å¯ä»¥å¯ç”¨æ‘„åƒå¤´ï¼Œå……å½“å¥èº«æ•™ç»ƒï¼Œæ£€æŸ¥æ‚¨æ˜¯å¦åšå¾—æ­£ç¡®ã€‚*\n\nè¯¥æ¨¡åž‹å…·æœ‰ç‹¬ç‰¹çš„åŠŸèƒ½ï¼Œå¦‚æ•°æ®åˆ†æžã€ä»£ç è§£é‡Šå™¨å’Œå®žæ—¶ç½‘é¡µæµè§ˆï¼Œä½¿å…¶ä¸Žç«žäº‰å¯¹æ‰‹ä¸åŒã€‚è¯¥æ¨¡åž‹è¿˜æœ‰å¤§é‡çš„ GPTsï¼Œè¿™æ˜¯ ChatGPT çš„å®šåˆ¶ç‰ˆæœ¬ã€‚\n\n### Claude 3.5 Sonnet\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*BSMcOpvWZ5lUm4Tl)\n\nClaude 3.5 Sonnet æ˜¯ç”± Anthropic æŽ¨å‡ºçš„ AI èŠå¤©æœºå™¨äººã€‚å®ƒæ˜¯ Claude AI æ¨¡åž‹ç³»åˆ—çš„ç¬¬ä¸‰ä»£ã€‚è¿™ä¸€æ¨¡åž‹åœ¨å¤šä¸ªè¯„ä¼°ä¸­è¡¨çŽ°å‡ºè‰²ï¼Œä¿æŒäº†é«˜åŸºå‡†ï¼Œé¿å…äº†å¹»è§‰å’Œé”™è¯¯ä¿¡æ¯ã€‚è™½ç„¶å®ƒä¸æ”¯æŒåƒ GPT-4o é‚£æ ·çš„è¯­éŸ³å’Œè§†é¢‘åŠŸèƒ½ï¼Œä½†å®ƒä»ç„¶å¯ä»¥æ‰§è¡Œæ‰€æœ‰åŸºæœ¬ä»»åŠ¡ï¼Œä¾‹å¦‚æ–‡æœ¬ç”Ÿæˆå’Œä¸åŒç¼–ç¨‹è¯­è¨€çš„ä»£ç ç”Ÿæˆã€å¤´è„‘é£Žæš´ç­‰ã€‚æ ¹æ® Anthropic çš„æŠ¥å‘Šï¼ŒClaude 3.5 Sonnet æ˜¯å¸‚åœºä¸Šæœ€å¥½çš„è®¡ç®—æœºè§†è§‰æ¨¡åž‹ä¹‹ä¸€ï¼Œå¯ä»¥ç”¨äºŽåˆ†æžå›¾è¡¨å’Œå›¾å½¢ï¼Œä»Žå›¾åƒä¸­è½¬å½•æ–‡æœ¬ç­‰ã€‚Claude æ‹¥æœ‰ä¸€ä¸ªå…ˆè¿›çš„åŠŸèƒ½ï¼Œâ€œArtifactsâ€ï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨å¯¹è¯ä¸­å‡ºçŽ°çš„ç‰¹æ®Šå¼¹å‡ºçª—å£ï¼Œå…è®¸ç”¨æˆ·æŸ¥çœ‹ä»£ç ç‰‡æ®µã€æ–‡æœ¬æ–‡ä»¶æˆ–ç½‘ç«™è®¾è®¡ï¼Œå¹¶å…è®¸ä»–ä»¬å®žæ—¶ç¼–è¾‘è¾“å‡ºã€‚\n\n*ä¾‹å¦‚ï¼Œç”¨æˆ·å¯ä»¥åœ¨å·¥ä½œæµç¨‹ä¸­ä½¿ç”¨è®¡ç®—æœºè§†è§‰å’Œ artifactsã€‚ç”¨æˆ·å¯ä»¥åœ¨çº¸ä¸Šè¿›è¡Œç½‘ç«™è®¾è®¡çš„åŸºæœ¬åŽŸåž‹åˆ¶ä½œï¼Œå°†æ–‡ä»¶é™„åŠ åˆ° Claude 3.5 Sonnetï¼Œå¹¶æç¤ºå®ƒæ ¹æ®åŽŸåž‹è®¾è®¡ç½‘ç«™ã€‚ç”Ÿæˆçš„ä»£ç å’Œç½‘ç«™è®¾è®¡ä¼šå‡ºçŽ°åœ¨ artifacts ä¸­ã€‚ç”¨æˆ·å¯ä»¥æ ¹æ®è‡ªå·±çš„éœ€æ±‚ç¼–è¾‘ä»£ç å’Œè®¾è®¡ã€‚ç”¨æˆ·è¿˜å¯ä»¥å°†ä»–ä»¬çš„é¡¹ç›®å®žæ—¶å‘å¸ƒåˆ°äº’è”ç½‘ä¸Šã€‚*\n\n## é€é¡¹æ¯”è¾ƒ\n\nåœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ ¹æ®å¤æ‚æŽ¨ç†å’Œä»£ç ç”Ÿæˆç­‰å› ç´ æ¯”è¾ƒè¿™ä¸¤ä¸ª LLMï¼Œæ£€æŸ¥å®ƒä»¬åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ–¹é¢çš„èƒ½åŠ›ï¼Œå¹¶çœ‹çœ‹å“ªä¸ªæ¨¡åž‹æ›´å¥½ã€‚\n\n* **ç ”ç©¶ç”Ÿæ°´å¹³æŽ¨ç†(GPQA, Diamond)**æ­¤å› ç´ è¯„ä¼°æ¨¡åž‹å¤„ç†ç ”ç©¶ç”Ÿæ°´å¹³æ•™è‚²ä¸­å¤æ‚ã€é«˜çº§æŽ¨ç†ä»»åŠ¡çš„èƒ½åŠ›ã€‚åœ¨æ­¤ä»»åŠ¡ä¸­ï¼Œç ”ç©¶äººå‘˜åœ¨ GPQA æµ‹è¯•ä¸­æ¯”è¾ƒæ¨¡åž‹ï¼Œè¯¥æµ‹è¯•ç”±ä¸“å®¶è®¾è®¡ï¼ŒåŒ…å«448ä¸ªä¸åŒé¢†åŸŸçš„é—®é¢˜ã€‚è¿™äº›é—®é¢˜æ˜¯ Google Proofï¼Œå› æ­¤ä»»ä½•äººéƒ½æ— æ³•åœ¨çº¿æ‰¾åˆ°å®ƒä»¬ã€‚Claude çš„å¾—åˆ†æŽ¥è¿‘ 59.4%ï¼Œè€Œ GPT-4o çš„å¾—åˆ†ä»…ä¸º 53.6%ã€‚è™½ç„¶ä¸¤ä¸ªå¾—åˆ†ç›¸å¯¹æŽ¥è¿‘ï¼Œä½†æ­£å¦‚æˆ‘ä»¬æ‰€è§ï¼ŒClaude åœ¨éœ€è¦é«˜çº§åˆ†æžæ€ç»´çš„ä»»åŠ¡ä¸­å¯èƒ½æ˜¯æ›´å¥½çš„é€‰æ‹©ï¼Œä¾‹å¦‚ç ”ç©¶åˆ†æžã€å¤æ‚é—®é¢˜è§£å†³å’Œé«˜å­¦æœ¯æ°´å¹³çš„é—®é¢˜ã€‚\n* **æœ¬ç§‘æ°´å¹³çŸ¥è¯†(MMLU)**MMLUï¼Œå³å¤§è§„æ¨¡å¤šä»»åŠ¡è¯­è¨€ç†è§£ï¼Œæ˜¯ä¸€ä¸ªåŸºå‡†ï¼Œè§£é‡Šä»»ä½• AI æ¨¡åž‹åœ¨æœ¬ç§‘æ°´å¹³ä¸Šå¯¹å„ä¸ªå­¦ç§‘çš„é€šç”¨çŸ¥è¯†ç†è§£ã€‚Claude 3.5 Sonnet åœ¨æ­¤å®žéªŒä¸­çš„å¾—åˆ†ä¸º 88.3%ï¼Œè€Œ GPT-4o çš„å¾—åˆ†ä¸º 88.7%ã€‚è¿™è¡¨æ˜Žè¿™ä¸¤ä¸ª LLM åœ¨å¤šä¸ªé¢†åŸŸè¿›è¡Œäº†è®­ç»ƒï¼Œå¹¶å¯¹è¿™äº›é¢†åŸŸæœ‰æ›´æ·±å…¥çš„ç†è§£ã€‚è¿™ä½¿å¾— AI æ¨¡åž‹æˆä¸ºé€šç”¨çŸ¥è¯†ä»»åŠ¡ã€å¤šä¸ªå­¦ç§‘çš„åŸºç¡€è¾…å¯¼ç­‰çš„åˆé€‚å·¥å…·ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*A4w-tvsxcmFINaQT)\n\n* **ä»£ç (HumanEval)**HumanEval æ˜¯ä¸€ä¸ªåŸºå‡†ï¼Œè¯„ä¼°æ¨¡åž‹ç”Ÿæˆã€ç†è§£å’Œè°ƒè¯•ä»£ç çš„èƒ½åŠ›ã€‚åœ¨è¿™ä¸ªåŸºå‡†ä¸­ï¼ŒClaude 3.5 Sonnet è¾¾åˆ°äº† 92%çš„å¾—åˆ†ï¼Œè€Œ GPT-4o çš„å¾—åˆ†ä¸º 90.2%ã€‚Claude 3.5 Sonnet åœ¨æ­¤ä»»åŠ¡ä¸­çš„ç»“æžœéžå¸¸å‡ºè‰²ï¼Œå› ä¸ºå®ƒæä¾›äº†æ¯” GPT-4o æ›´å¥½çš„ç¼–ç çŽ¯å¢ƒâ€œArtifactsâ€å’Œæ›´å¥½çš„ä»£ç ç”Ÿæˆã€‚Claude å…è®¸ç”¨æˆ·åœ¨ Artifacts å¼¹å‡ºçª—å£ä¸­è®¾è®¡ã€ç¼–è¾‘å’Œè¿è¡Œä»£ç ã€‚åœ¨ Claude 3.5 Sonnet å‘å¸ƒåŽï¼Œå¤§å®¶éƒ½åœ¨å¼€å‘å·¥å…·ã€ç½‘ç«™å’ŒåŸºæœ¬æ¸¸æˆï¼Œå¹¶åœ¨äº’è”ç½‘ä¸Šåˆ†äº«å®ƒä»¬ã€‚å¦ä¸€æ–¹é¢ï¼ŒGPT-4o çš„å¾—åˆ†ä¹Ÿä¸é”™ï¼Œä½†å®ƒçš„ç•Œé¢ä¸­æ²¡æœ‰ä»»ä½•ç¼–ç çŽ¯å¢ƒï¼Œå› æ­¤å¼€å‘äººå‘˜å¿…é¡»èŠ±è´¹å¾ˆå¤šç²¾åŠ›ï¼Œå› ä¸ºå®ƒç”Ÿæˆçš„ä»£ç å¾ˆéš¾è¾¾åˆ°ç»“æžœã€‚\n* **æ–‡æœ¬æŽ¨ç†(DROP, FLscore)**DROPï¼ˆæ®µè½ç¦»æ•£æŽ¨ç†ï¼‰æ˜¯ä¸€ä¸ªåŸºå‡†ï¼Œæµ‹é‡æ¨¡åž‹ç†è§£å¤æ‚æ–‡æœ¬ä¿¡æ¯çš„èƒ½åŠ›ã€‚åœ¨è¿™ä¸ªæŒ‘æˆ˜ä¸­ï¼ŒClaude 3.5 Sonnet çš„å¾—åˆ†ä¸º 87.1%ï¼Œè€Œ GPT-4o çš„å¾—åˆ†ä¸º 83.4%ã€‚è¿™è¡¨æ˜Ž Claude 3.5 Sonnet åœ¨æ¶‰åŠè¯¦ç»†æ–‡æœ¬åˆ†æžã€æ–‡æœ¬å®¡æŸ¥ã€å¤æ‚é—®ç­”ç³»ç»Ÿç­‰ä»»åŠ¡æ—¶æ›´å¥½ä¸”æ›´æœ‰æ•ˆã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Kcy7sFb2FYpbfrwp)\n\n* **æ•°å­¦é—®é¢˜è§£å†³(MATH)**æ­¤æµ‹è¯•è¯„ä¼°ä»»ä½• AI æ¨¡åž‹è§£å†³å„ç§æ•°å­¦é—®é¢˜çš„èƒ½åŠ›ã€‚Claude 3.5 Sonnet çš„å¾—åˆ†ä»…ä¸º 71.1%ï¼Œè€Œ GPT-4o çš„å¾—åˆ†ä¸º 76.6%ã€‚è¿™äº›å¾—åˆ†ä½¿ GPT-4o æˆä¸ºæ•°å­¦é—®é¢˜è§£å†³ä»»åŠ¡çš„æ›´å¥½æ¨¡åž‹ï¼Œå¹¶å¯ç”¨äºŽè´¢åŠ¡å»ºæ¨¡ã€ç§‘å­¦è®¡ç®—å’Œé«˜çº§æ•°æ®åˆ†æžç­‰æ•°å­¦è®¡ç®—ã€‚\n* **å¤šè¯­è¨€æ•°å­¦(MSGM)**æ­¤å› ç´ æè¿°ä»»ä½• AI æ¨¡åž‹åœ¨å¤šç§è¯­è¨€ä¸­è§£å†³æ•°å­¦é—®é¢˜çš„èƒ½åŠ›ã€‚ä¸¤ä¸ªæ¨¡åž‹çš„å¾—åˆ†æŽ¥è¿‘ï¼šGPT-4o 90.5% å’Œ Claude 3.5 Sonnet 91.6%ã€‚è¿™è¡¨æ˜Žä¸¤ä¸ªæ¨¡åž‹è¡¨çŽ°å‡ºè‰²ï¼ŒClaude ç•¥èƒœä¸€ç­¹ã€‚è¯¥èƒ½åŠ›å¯¹äºŽæ•™è‚²åº”ç”¨æˆ–ä»»ä½•éœ€è¦è·¨è¯­è¨€éšœç¢è¿›è¡Œæ•°å­¦æŽ¨ç†äº¤æµçš„åœºæ™¯ç‰¹åˆ«æœ‰ç”¨ã€‚\n* **è§†è§‰é—®ç­”(MMU/val)**æ­¤å› ç´ æè¿° LLM åˆ†æžå›¾åƒä¸­å‘ˆçŽ°çš„ä¿¡æ¯çš„èƒ½åŠ›ã€‚GPT-4o åœ¨è¿™ä¸€åŸºå‡†ä¸­ä»¥ 69.1% è¶…è¿‡ Claude 3.5 Sonnet çš„ 68.3%ã€‚å¦ä¸€æ–¹é¢ï¼Œåœ¨åˆ†æžæ–‡æ¡£ä¸­çš„æ–‡æœ¬æ—¶ï¼ŒClaude 3.5 Sonnet çš„å¾—åˆ†ä¸º 95.2%ï¼Œè€Œ GPT-4o çš„å¾—åˆ†ä¸º 92.1%ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*xzjqBV2YL0lVFitX)\n\n* **å›¾åƒç”Ÿæˆ**å›¾åƒç”Ÿæˆæ˜¯ LLM ä»Žæ–‡æœ¬ç”Ÿæˆå›¾åƒçš„èƒ½åŠ›ã€‚GPT-4o é›†æˆäº† DallE-2ï¼Œå¯ä»¥é€šè¿‡æ–‡æœ¬ç”Ÿæˆå›¾åƒï¼Œç»“æžœéžå¸¸å‡ºè‰²ã€‚å¦ä¸€æ–¹é¢ï¼ŒClaude 3.5 Sonnet æ— æ³•åˆ›å»ºä»»ä½•å›¾åƒã€‚æ­¤åŠŸèƒ½è¿˜å¸®åŠ© GPT-4o æ›´å¥½åœ°è®¾è®¡ç½‘ç«™å’Œå‚è€ƒï¼Œå› ä¸ºå®ƒåœ¨è®¸å¤šå›¾åƒä¸Šè¿›è¡Œäº†è®­ç»ƒã€‚\n* **çŸ¥è¯†æˆªæ­¢**åœ¨è¿™é‡Œï¼Œä¸¤ä¸ªæ¨¡åž‹éƒ½åœ¨ç‰¹å®šæ—¥æœŸä¹‹å‰çš„æœ‰é™æ•°æ®é›†ä¸Šè¿›è¡Œäº†è®­ç»ƒã€‚Claude 3.5 Sonnet åœ¨ 2024 å¹´ 4 æœˆä¹‹å‰çš„æ•°æ®ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œè€Œ GPT-4o åˆ™åœ¨ 2024 å¹´ä¹‹å‰çš„æ•°æ®ä¸Šè¿›è¡Œäº†è®­ç»ƒã€‚GPT-4o çš„çœŸæ­£ä¼˜åŠ¿åœ¨äºŽå®ƒå…·æœ‰å®žæ—¶ç½‘é¡µæµè§ˆåŠŸèƒ½ï¼Œè¿™æœ‰åŠ©äºŽ LLM å®šæœŸåœ¨æ–°æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚\n\n## GPT-4o çš„ä¼˜ç‚¹ï¼š\n\n* å¤„ç†è¯­éŸ³ã€å›¾åƒå’Œè§†é¢‘è¾“å…¥ã€‚\n* å®žæ—¶ç½‘é¡µæµè§ˆèƒ½åŠ›ã€‚\n* æ›´å¿«çš„å“åº”æ—¶é—´ï¼ˆå¹³å‡ 0.32 ç§’ï¼‰ã€‚\n* åœ¨æ•°å­¦é—®é¢˜è§£å†³æ–¹é¢è¡¨çŽ°ä¼˜è¶Šã€‚\n* å¯ä»¥ä½¿ç”¨ DALL-E 2 ç”Ÿæˆå›¾åƒã€‚\n\n## GPT-4o çš„ç¼ºç‚¹ï¼š\n\n* ç ”ç©¶ç”Ÿæ°´å¹³æŽ¨ç†çš„æ€§èƒ½ç¨ä½Žã€‚\n* æ²¡æœ‰å†…ç½®çš„ç¼–ç çŽ¯å¢ƒã€‚\n* æ–‡æ¡£è§†è§‰é—®ç­”å¾—åˆ†è¾ƒä½Žã€‚\n* ä»£ç ç”Ÿæˆèƒ½åŠ›ç¨é€Šã€‚\n* åœ¨è¯¦ç»†æ–‡æœ¬åˆ†æžæ–¹é¢æ•ˆæžœè¾ƒå·®ã€‚\n\n## Pros Claude 3.5 Sonnet:\n\n* åœ¨ç ”ç©¶ç”Ÿçº§åˆ«çš„æŽ¨ç†æ–¹é¢è¡¨çŽ°å‡ºè‰²ã€‚\n* ä¼˜è¶Šçš„ä»£ç ç”Ÿæˆå’Œå†…ç½®çš„â€œå·¥ä»¶â€åŠŸèƒ½ã€‚\n* åœ¨è¯¦ç»†æ–‡æœ¬åˆ†æžä¸­è¡¨çŽ°æ›´ä½³ã€‚\n* åœ¨æ–‡æ¡£è§†è§‰é—®ç­”ä¸­å¾—åˆ†æ›´é«˜ã€‚\n* åœ¨å¤šè¯­è¨€æ•°å­¦æ–¹é¢ç•¥æœ‰ä¼˜åŠ¿ã€‚\n\n## Cons Claude 3.5 é¢‚ï¼š\n\n* æ— æ³•å¤„ç†è¯­éŸ³æˆ–è§†é¢‘è¾“å…¥ã€‚\n* æ²¡æœ‰å›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚\n* åœ¨è§†è§‰é—®ç­”æ–¹é¢æ€§èƒ½ç¨ä½Žã€‚\n* æ— æ³•è®¿é—®å®žæ—¶ç½‘ç»œä¿¡æ¯ã€‚\n* åœ¨æ•°å­¦é—®é¢˜è§£å†³æ–¹é¢è¾ƒå¼±ã€‚\n\n## ç»“è®º\n\nGPT-4o å’Œ Claude 3.5 Sonnet åœ¨å„ç§ä»»åŠ¡ä¸­å±•çŽ°äº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œå„è‡ªæœ‰å…¶ä¼˜åŠ¿ã€‚GPT-4o åœ¨å¤šæ¨¡æ€è¾“å…¥ã€å®žæ—¶ä¿¡æ¯è®¿é—®å’Œå›¾åƒç”Ÿæˆæ–¹é¢è¡¨çŽ°å‡ºè‰²ï¼Œä½¿å…¶åœ¨å¤šç§åº”ç”¨ä¸­éžå¸¸çµæ´»ã€‚Claude 3.5 Sonnet åœ¨å¤æ‚æŽ¨ç†ã€ä»£ç ç”Ÿæˆå’Œè¯¦ç»†æ–‡æœ¬åˆ†æžæ–¹é¢è¡¨çŽ°çªå‡ºï¼Œåœ¨ç‰¹å®šçš„å­¦æœ¯å’Œä¸“ä¸šèƒŒæ™¯ä¸‹æä¾›äº†æ›´ä¼˜çš„æ€§èƒ½ã€‚é€‰æ‹©è¿™ä¸¤ç§æ¨¡åž‹å–å†³äºŽå…·ä½“çš„ä½¿ç”¨æ¡ˆä¾‹å’Œæ‰€éœ€çš„åŠŸèƒ½ã€‚éšç€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„è¿›æ­¥ï¼Œæˆ‘ä»¬å¯ä»¥æœŸå¾…è¿›ä¸€æ­¥çš„æ”¹è¿›å’Œé’ˆå¯¹ä¸åŒéœ€æ±‚çš„ä¸“ä¸šæ¨¡åž‹ã€‚\n\n"},{"lang":"zh","group":"blog","slug":"blog/comparative-study-of-langgraph-autogen-and-crewai-for-building-multi-agent-systems-0e7e47f9078e","frontmatter":{"title":"ç”¨äºŽæž„å»ºå¤šä»£ç†ç³»ç»Ÿçš„ LangGraphã€Autogen å’Œ Crewai æ¯”è¾ƒç ”ç©¶","meta_title":"ç”¨äºŽæž„å»ºå¤šä»£ç†ç³»ç»Ÿçš„ LangGraphã€Autogen å’Œ Crewai æ¯”è¾ƒç ”ç©¶","description":"å½“æˆ‘ä»¬æ¶‰è¶³å¤šä»£ç†ç³»ç»Ÿï¼ˆMASï¼‰é¢†åŸŸæ—¶ï¼Œäº†è§£å„ç§ç¼–ç¨‹è¯­è¨€çš„è®¾è®¡è‡³å…³é‡è¦ã€‚","date":"2024-11-13T01:22:29.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*DBlLuCOA3lWIg6RmpMPg8A.png","categories":["Programming","Technology","Machine Learning"],"author":"Rifx.Online","tags":["LangGraph","Autogen","Crewai","multi-agent","scalability"],"draft":false,"slug":"blog/comparative-study-of-langgraph-autogen-and-crewai-for-building-multi-agent-systems-0e7e47f9078e"},"content":"\n\n\néšç€æˆ‘ä»¬è¿›å…¥å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰çš„é¢†åŸŸï¼Œäº†è§£ä¸“é—¨ä¸ºæ­¤ç›®çš„è®¾è®¡çš„å„ç§ç¼–ç¨‹è¯­è¨€è‡³å…³é‡è¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†é€šè¿‡æ¯”è¾ƒ LangGraphã€Autogen å’Œ Crewai â€”â€” è¯¥é¢†åŸŸçš„ä¸‰å¤§é‡è¦å‚ä¸Žè€…ï¼Œæ·±å…¥æŽ¢è®¨ MAS å¼€å‘çš„ä¸–ç•Œã€‚\n\n## ä»‹ç»\n\nå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰åœ¨å„ä¸ªè¡Œä¸šä¸­å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚MASæ˜¯ç”±å¤šä¸ªæ™ºèƒ½ä½“ç»„æˆçš„ç³»ç»Ÿï¼Œè¿™äº›æ™ºèƒ½ä½“ç›¸äº’ä¹‹é—´ä»¥åŠä¸ŽçŽ¯å¢ƒè¿›è¡Œäº¤äº’ï¼Œä»¥å®žçŽ°ç‰¹å®šç›®æ ‡ã€‚åœ¨å¯ç”¨äºŽæž„å»ºMASçš„ä¼—å¤šæ¡†æž¶ä¸­ï¼ŒLangGraphã€Autogenå’ŒCrewaiæ˜¯ä¸€äº›æœ€å—æ¬¢è¿Žçš„é€‰æ‹©ã€‚\n\nä½œä¸ºä»Žäº‹MASé¡¹ç›®çš„å¼€å‘è€…æˆ–ç ”ç©¶äººå‘˜ï¼Œé€‰æ‹©åˆé€‚çš„æ¡†æž¶å¯èƒ½ä¼šè®©äººæ„Ÿåˆ°ä¸çŸ¥æ‰€æŽªï¼Œå°¤å…¶æ˜¯è€ƒè™‘åˆ°æ˜“ç”¨æ€§ã€å¯æ‰©å±•æ€§ã€å®šåˆ¶åŒ–å’Œä¸ŽAIåº“çš„é›†æˆç­‰å› ç´ ã€‚æœ¬æ–‡æä¾›äº†LangGraphã€Autogenå’ŒCrewaiçš„æ¯”è¾ƒç ”ç©¶ï¼Œçªå‡ºäº†å®ƒä»¬çš„ä¼˜ç¼ºç‚¹ä»¥åŠåœ¨ä¸åŒåº”ç”¨ä¸­çš„é€‚ç”¨æ€§ã€‚\n\n### å„æ¡†æž¶ä»‹ç»\n\n## LangGraph: ä¸€ä¸ªå¼€æºæ¡†æž¶\n\n**ä¼˜ç‚¹**ï¼š\n\n* **æ˜“äºŽä½¿ç”¨**ï¼šLangGraph æä¾›äº†ç®€å•ç›´è§‚çš„ APIï¼Œä½¿å¼€å‘è€…èƒ½å¤Ÿè½»æ¾ä¸ŽçŽ°æœ‰ç³»ç»Ÿé›†æˆã€‚\n* **å¯æ‰©å±•æ€§**ï¼šLangGraph æ”¯æŒå¤§è§„æ¨¡åˆ†å¸ƒå¼ç³»ç»Ÿï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿå¤„ç†å¤æ‚ä»»åŠ¡ã€‚\n* **ä¸Ž AI åº“çš„é›†æˆ**ï¼šLangGraph ä¸Žæµè¡Œçš„ AI åº“å¦‚ TensorFlowã€PyTorch å’Œ Keras å…¼å®¹ã€‚\n\n**å±€é™æ€§**ï¼š\n\n* å¯¹åˆ†å¸ƒå¼ç³»ç»Ÿçš„æ”¯æŒæœ‰é™\n* æ¯” Autogen å’Œ Crewai çµæ´»æ€§å·®\n\n## Autogen: ä¸€ä¸ªæ¨¡å—åŒ–çš„å¼€æºæ¡†æž¶\n\n**ä¼˜åŠ¿**ï¼š\n\n* **é«˜åº¦çµæ´»æ€§**ï¼šAutogen æä¾›äº†æ¨¡å—åŒ–æž¶æž„ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿæ ¹æ®ç‰¹å®šéœ€æ±‚å®šåˆ¶ä»–ä»¬çš„ MASã€‚\n* **é€‚åˆå¤æ‚åº”ç”¨**ï¼šAutogen çš„æ¨¡å—åŒ–ä½¿å…¶éžå¸¸é€‚åˆå…·æœ‰å¤šä¸ªäº’è”ä»£ç†çš„å¤§åž‹ç³»ç»Ÿã€‚\n* **å¼ºå¤§çš„ç¤¾åŒºæ”¯æŒ**ï¼šAutogen æ‹¥æœ‰ä¸€ä¸ªæ´»è·ƒçš„å¼€å‘è€…å’Œç ”ç©¶è€…ç¤¾åŒºï¼Œä»–ä»¬ä¸ºè¯¥æ¡†æž¶åšå‡ºè´¡çŒ®å¹¶æä¾›æ”¯æŒã€‚\n\n**å±€é™æ€§**ï¼š\n\n* å­¦ä¹ æ›²çº¿è¾ƒé™¡\n* éœ€è¦æ›´å¤šèµ„æº\n\n## Crewai: å¯æ‰©å±•çš„æ•°æ®é©±åŠ¨æ¡†æž¶\n\n**ä¼˜ç‚¹**ï¼š\n\n* **å¯æ‰©å±•æ€§**ï¼šCrewai å¯¹å¤§è§„æ¨¡ç³»ç»Ÿæä¾›äº†å‡ºè‰²çš„æ”¯æŒï¼Œéžå¸¸é€‚åˆéœ€è¦å¤„ç†å¤§é‡æ•°æ®çš„åº”ç”¨ç¨‹åºã€‚\n* **æ˜“ç”¨æ€§**ï¼šCrewai æä¾›äº†ä¸€ä¸ªç®€å•çš„ APIï¼Œä¾¿äºŽä¸ŽçŽ°æœ‰ç³»ç»Ÿé›†æˆã€‚\n* **ä¸Žäº‘æœåŠ¡çš„é›†æˆ**ï¼šCrewai å…è®¸ç”¨æˆ·è½»æ¾åœ°åœ¨ AWS å’Œ Azure ç­‰äº‘å¹³å°ä¸Šéƒ¨ç½²ä»–ä»¬çš„ MASã€‚\n\n**å±€é™æ€§**ï¼š\n\n* å¯¹è‡ªå®šä¹‰æ¨¡åž‹çš„æ”¯æŒæœ‰é™\n* çµæ´»æ€§ä¸å¦‚ Autogen\n\n## å¯¹æ¯”çŸ©é˜µ\n\n\n\n## ç»“è®º\n\næ€»ä¹‹ï¼Œæ¯ä¸ªæ¡†æž¶éƒ½æœ‰å…¶ç‹¬ç‰¹çš„ä¼˜ç‚¹å’Œç¼ºç‚¹ã€‚LangGraph æä¾›äº†æ˜“ç”¨æ€§å’Œå¯æ‰©å±•æ€§ï¼ŒAutogen æä¾›äº†çµæ´»æ€§å’Œå¯å®šåˆ¶æ€§ï¼Œè€Œ Crewai åœ¨æ•°æ®é©±åŠ¨çš„æ–¹æ³•å’Œå¯æ‰©å±•æ€§æ–¹é¢è¡¨çŽ°å‡ºè‰²ã€‚\n\nåœ¨é€‰æ‹©æž„å»º MAS çš„æ¡†æž¶æ—¶ï¼Œè¯·è€ƒè™‘é¡¹ç›®çš„å…·ä½“è¦æ±‚ï¼š\n\n* **æ˜“ç”¨æ€§**ï¼šå¦‚æžœæ‚¨é‡è§†ç®€å•æ€§å’Œå¯æ‰©å±•æ€§ï¼Œè¯·é€‰æ‹© LangGraphã€‚\n* **çµæ´»æ€§**ï¼šå¯¹äºŽéœ€è¦å®šåˆ¶çš„å¤æ‚åº”ç”¨ç¨‹åºï¼Œè¯·é€‰æ‹© Autogenã€‚\n* **å¯æ‰©å±•æ€§**ï¼šå¯¹äºŽéœ€è¦å¤§è§„æ¨¡æ•°æ®å¤„ç†çš„å¤§åž‹ç³»ç»Ÿï¼Œè¯·è€ƒè™‘ Crewaiã€‚\n\né€šè¿‡äº†è§£æ¯ä¸ªæ¡†æž¶çš„ä¼˜ç¼ºç‚¹ï¼Œå¼€å‘äººå‘˜å¯ä»¥åšå‡ºæ˜Žæ™ºçš„å†³ç­–ï¼Œä»Žè€Œé€‰æ‹©æž„å»ºæ›´æœ‰æ•ˆå’Œé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆçš„ MASã€‚\n\n## é¢å¤–èµ„æº\n\næœ‰å…³è¿›ä¸€æ­¥é˜…è¯»å’Œèµ„æºï¼Œè¯·å‚è§ï¼š\n\n* [LangGraph æ–‡æ¡£](https://proxy.rifx.online/https://langgraph.com/documentation/)\n* [Autogen æ•™ç¨‹](https://proxy.rifx.online/https://autogen.com/tutorials)\n* [Crewai API å‚è€ƒ](https://proxy.rifx.online/https://crewai.com/api-reference/)\n\n"},{"lang":"zh","group":"blog","slug":"blog/comparing-leading-text-to-image-image-generation-models-for-adding-text-to-images-7dc001f491ef","frontmatter":{"title":"æ¯”è¾ƒä¸ºå›¾åƒæ·»åŠ æ–‡æœ¬çš„ä¸»è¦æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡åž‹","meta_title":"æ¯”è¾ƒä¸ºå›¾åƒæ·»åŠ æ–‡æœ¬çš„ä¸»è¦æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡åž‹","description":"æœ¬æ–‡è¯„ä¼°äº†ä¹ç§é¢†å…ˆçš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡åž‹åœ¨å›¾åƒä¸­å‡†ç¡®æ¸²æŸ“æ–‡æœ¬çš„èƒ½åŠ›ã€‚æµ‹è¯•ç»“æžœæ˜¾ç¤ºï¼ŒBlack Forest Labsçš„FLUX1.1 [pro]å’ŒStability AIçš„Stable Image Ultraåœ¨å†çŽ°æç¤ºä¸­è¯·æ±‚çš„æ–‡æœ¬æ–¹é¢è¡¨çŽ°æœ€ä½³ã€‚æ–‡ç« è¿˜æŽ¢è®¨äº†ä¸‰ç§æ›¿ä»£æŠ€æœ¯ï¼Œä»¥æé«˜ç”Ÿæˆå›¾åƒä¸­æ–‡æœ¬çš„å‡†ç¡®æ€§ï¼ŒåŒ…æ‹¬åœ¨å›¾åƒä¸­æ›¿æ¢ç”Ÿæˆçš„æ–‡æœ¬ã€ä»Žç©ºç™½ç”»å¸ƒå¼€å§‹ç”Ÿæˆå›¾åƒä»¥åŠåˆ†åˆ«ç”Ÿæˆå›¾åƒå’Œæ–‡æœ¬ã€‚","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Gvj5CUGClWka1KUsDy5GQw.png","categories":["Generative AI","Natural Language Processing","Technology/Web"],"author":"Rifx.Online","tags":["text","generation","models","accuracy","techniques"],"draft":false,"slug":"blog/comparing-leading-text-to-image-image-generation-models-for-adding-text-to-images-7dc001f491ef"},"content":"\n\n\n### ä¹ä¸ªé¢†å…ˆå›¾åƒç”Ÿæˆæ¨¡åž‹åœ¨å›¾åƒä¸­æ¸²æŸ“å‡†ç¡®æ–‡æœ¬ï¼ˆå•è¯å’ŒçŸ­è¯­ï¼‰çš„èƒ½åŠ›æ¯”è¾ƒ\n\nåœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†è¯„ä¼°æ¥è‡ªå¤šä¸ªæä¾›å•†çš„ä¹ä¸ªæœ€å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡åž‹åœ¨ä¸åŒæ‰˜ç®¡å¹³å°ä¸Šçš„èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†æ ¹æ®ç»™å®šçš„æç¤ºè¯„ä¼°å®ƒä»¬åœ¨å›¾åƒä¸­ç”Ÿæˆå‡†ç¡®æ–‡æœ¬ï¼ˆå•è¯å’ŒçŸ­è¯­ï¼‰çš„èƒ½åŠ›ã€‚æµ‹è¯•çš„æ¨¡åž‹åŒ…æ‹¬ä»¥ä¸‹å†…å®¹ï¼ˆæŒ‰å­—æ¯é¡ºåºæŽ’åˆ—ï¼‰ï¼š\n\n1. Adobe Firefly Image 3ï¼ˆé€šè¿‡ [firefly.adobe.com](http://firefly.adobe.com/)ï¼‰\n2. Amazon Titan Image Generator G1 v2ï¼ˆé€šè¿‡ [Amazon Bedrock](https://aws.amazon.com/bedrock/)ï¼‰\n3. Black Forest Labs FLUX1\\.1 \\[pro] å’Œ Ultra Modeï¼ˆé€šè¿‡ [Replicate](http://replicate.com/)ï¼‰\n4. Google Imagen 3ï¼ˆé€šè¿‡ [ImageFX](https://aitestkitchen.withgoogle.com/tools/image-fx)ï¼‰\n5. KLING AI ç”± [Kwai\\-Kolors/Kolors](https://huggingface.co/Kwai-Kolors/Kolors) æä¾›æ”¯æŒï¼ˆé€šè¿‡ [klingai.com](http://klingai.com/)ï¼‰\n6. Midjourney v6\\.1ï¼ˆé€šè¿‡ [midjourney.com](http://midjourney.com/)ï¼‰\n7. OpenAI DALLÂ·E 3ï¼ˆé€šè¿‡ [ChatGPT](https://quip-amazon.com/62AqA7VtF4Xb/chatgpt.com)ï¼‰\n8. Stability AI Stable Diffusion 3\\.5 Largeï¼ˆé€šè¿‡ [stability.ai](http://stability.ai/) APIï¼‰\n9. Stability AI Stable Image Ultra 1\\.0 v1ï¼ˆé€šè¿‡ [Amazon Bedrock](https://aws.amazon.com/bedrock/)ï¼‰\n\næ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å°†ç ”ç©¶ä¸‰ç§æ›¿ä»£çš„ã€æ›´å¯é çš„æŠ€æœ¯ï¼Œä»¥ç¡®ä¿ç”Ÿæˆå›¾åƒä¸­æ–‡æœ¬çš„å‡†ç¡®æ€§ã€‚\n\n## æµ‹è¯•æ¨¡åž‹\n\nå¯¹æ‰€æœ‰æ¨¡åž‹è¿›è¡Œäº†å‡ é¡¹æµ‹è¯•ï¼Œä½¿ç”¨äº†ä¸åŒçš„æç¤ºå’Œä¸åŒç¨‹åº¦çš„ç»†èŠ‚ã€‚æç¤ºç¤ºä¾‹åŒ…æ‹¬ï¼š\n\n1. *ä¸€å¼ å¾®ç¬‘çš„ç§‘å­¦å®¶æ‰‹æŒæ ‡è¯­ç‰Œçš„ç…§ç‰‡ï¼Œä¸Šé¢å†™ç€ï¼šâ€œæ— ç‘•çš„ AI ç”Ÿæˆæ–‡æœ¬ï¼â€*\n2. *ä¸€ä¸ªè”¬èœæ‘Šä½ï¼Œä¸Šé¢æœ‰å„ç§è”¬èœï¼ŒåŒ…æ‹¬è¥¿çº¢æŸ¿ã€‚ä¸€ä¸ªé»‘è‰²æ ‡ç‰Œä¸Šç”¨ç™½è‰²å­—ä½“å†™ç€ï¼šâ€œå†œåœºæ–°é²œè¥¿çº¢æŸ¿ $2.99/ç£…ã€‚â€*\n3. *ä¸€å¹…å¹½é»˜æ’å›¾ï¼Œæç»˜äº†ä¸€åªå‹å¥½çš„å—ç“œï¼ŒèƒŒæ™¯ä¸ºç™½è‰²ï¼Œé…æœ‰ç§‹å­£ä¸»é¢˜çš„å„ç§å—ç“œå’Œç§‹å¶ã€‚â€œä¸‡åœ£èŠ‚å¿«ä¹â€çš„å­—æ ·ä»¥å¤§æ·±æ£•è‰²å­—æ¯å±…ä¸­åœ¨å—ç“œä¸Šæ–¹ã€‚*\n4. *ä¸€å—æ—¶å°šçš„å¹¿å‘Šç‰Œé«˜è€¸åœ¨ç¹å¿™çš„é«˜é€Ÿå…¬è·¯ä¸Šï¼Œè½¦æµåœ¨é«˜å³°æ—¶æ®µå¿«é€ŸæŽ è¿‡ã€‚åœ¨ä¸€ä¸ªåŠ¨æ€çš„æŠ½è±¡èƒŒæ™¯ä¸‹ï¼Œå¤§è€Œç²—ä½“çš„æ–‡å­—â€œç”Ÿæˆæ€§ AIï¼šè½¬å˜æ•°å­—å¹¿å‘Šâ€ï¼Œä¸ºè·¯è¿‡çš„å¸æœºæä¾›äº†å³æ—¶çš„å¯è¯»æ€§ã€‚*\n\nå°½ç®¡æ¨¡åž‹ä¹‹é—´çš„æ•´ä½“å›¾åƒè´¨é‡å’Œæ˜Žæ˜¾åè§ç¨‹åº¦å·®å¼‚æ˜¾è‘—ï¼Œä½†ä»…è¯„ä¼°äº†æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ã€‚èƒ½å¤Ÿè‡³å°‘ 50% å‡†ç¡®å†çŽ°æç¤ºä¸­è¯·æ±‚æ–‡æœ¬çš„æ¨¡åž‹èŽ·å¾—äº†åŠæ ¼åˆ†æ•°ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›é€‰å®šæµ‹è¯•çš„ç»“æžœï¼Œå±•ç¤ºäº†æ¨¡åž‹çš„èƒ½åŠ›ã€‚ç»“æžœæŒ‰å­—æ¯é¡ºåºå‘ˆçŽ°ï¼Œè€Œä¸æ˜¯æŒ‰è´¨é‡æŽ’åã€‚æ¯ä¸ªæµ‹è¯•ä¸­åŒ…å«äº†å››å¼ ä»£è¡¨æ€§çš„å¹³å‡è´¨é‡å›¾åƒã€‚ \n\n\n\n## æ¨¡åž‹\n\n### Adobe Firefly Image 3\n\nAdobeäºŽ2024å¹´4æœˆå‘å¸ƒäº†å…¶Firefly Image 3åŸºç¡€æ¨¡åž‹ã€‚æ ¹æ®[æ–°é—»ç¨¿](https://news.adobe.com/news/news-details/2024/adobe-introduces-firefly-image-3-foundation-model-to-take-creative-exploration-and-ideation-to-new-heights)ï¼ŒAdobe Firefly Image 3åœ¨ç…§ç‰‡çœŸå®žæ„Ÿè´¨é‡ã€é€ åž‹èƒ½åŠ›ã€ç»†èŠ‚ã€å‡†ç¡®æ€§å’Œå¤šæ ·æ€§æ–¹é¢å®žçŽ°äº†æƒŠäººçš„è¿›æ­¥ã€‚æ­¤å¤–ï¼Œç”Ÿæˆé€Ÿåº¦çš„æ˜¾è‘—æå‡ä½¿å¾—æž„æ€å’Œåˆ›ä½œè¿‡ç¨‹æ›´åŠ é«˜æ•ˆå’Œå¯Œæœ‰ç”Ÿäº§åŠ›ã€‚è¯¥æ¨¡åž‹å¯åœ¨Adobe Photoshopï¼ˆæµ‹è¯•ç‰ˆï¼‰å’Œ[firefly.adobe.com](https://firefly.adobe.com/generate/images)ä¸Šä½¿ç”¨ã€‚ä»¥ä¸‹æ˜¯ä¸¤ä¸ªç•Œé¢ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*gcASwZgRSfPNYJB7n5GrlQ.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*vU3NW6VdkgojkNlHaGWoSg.png)\n\nðŸš« åœ¨æˆ‘çš„æµ‹è¯•ä¸­ï¼ŒAdobe Fireflyæ— æ³•å‡†ç¡®é‡çŽ°æç¤ºä¸­è¯·æ±‚çš„æ–‡æœ¬ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*yWoDLmj5mPKEw8GRg51YXw.jpeg)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*0iskBrEjrtFk-mXNrBvkag.jpeg)\n\n### Amazon Titan Image Generator G1 v2\n\nAmazon Titan Image Generator G1 v2 æ¨¡åž‹äºŽ 2024 å¹´ 8 æœˆå‘å¸ƒã€‚å®ƒæ˜¯å¯¹ä¸Šä¸€ä»£ Amazon Titan Image Generator G1 v1 æ¨¡åž‹çš„å‡çº§ï¼Œè¯¥æ¨¡åž‹äºŽ 2023 å¹´ 11 æœˆå‘å¸ƒã€‚Amazon Titan Image Generator G1 v2 æ¨¡åž‹å¢žåŠ äº†å¤šä¸ªåŠŸèƒ½ï¼ŒåŒ…æ‹¬å›¾åƒè°ƒèŠ‚ã€ä½¿ç”¨è°ƒè‰²æ¿çš„å›¾åƒå¼•å¯¼ã€èƒŒæ™¯ç§»é™¤å’Œä¸»é¢˜ä¸€è‡´æ€§ã€‚\n\nAmazon Titan Image Generator G1 v2 æ¨¡åž‹åœ¨ Amazon Bedrock ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼Œ æ ¹æ® [AWS](https://aws.amazon.com/bedrock/)ï¼Œå®ƒæ˜¯â€œ*ä¸€ä¸ªå®Œå…¨æ‰˜ç®¡çš„æœåŠ¡ï¼Œæä¾›æ¥è‡ªé¢†å…ˆ AI å…¬å¸ï¼ˆå¦‚ AI21 Labsã€Anthropicã€Cohereã€Metaã€Mistral AIã€Stability AI å’Œ Amazonï¼‰çš„é«˜æ€§èƒ½åŸºç¡€æ¨¡åž‹ï¼ˆFMsï¼‰çš„é€‰æ‹©ï¼Œé€šè¿‡å•ä¸€ APIï¼Œä»¥åŠæž„å»ºå…·æœ‰å®‰å…¨æ€§ã€éšç§æ€§å’Œè´Ÿè´£ä»» AI çš„ç”Ÿæˆ AI åº”ç”¨æ‰€éœ€çš„å¹¿æ³›èƒ½åŠ›ã€‚*â€\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*TmROyF5c-BXHevqImyflmw.png)\n\nðŸš« åœ¨æˆ‘çš„æµ‹è¯•ä¸­ï¼ŒAmazon Titan Image Generator G1 v2 æ— æ³•å‡†ç¡®é‡çŽ°æç¤ºä¸­è¯·æ±‚çš„æ–‡æœ¬ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*QLvxsEveORObkPOOB3u1Mg.png)\n\n### é»‘æ£®æž—å®žéªŒå®¤ FLUX1\\.1 \\[pro] å’Œè¶…æ¨¡å¼\n\né»‘æ£®æž—å®žéªŒå®¤äºŽ2024å¹´10æœˆå‘å¸ƒäº†FLUX1\\.1 \\[pro]ã€‚æ ¹æ®é»‘æ£®æž—å®žéªŒå®¤çš„è¯´æ³•ï¼Œâ€œ*FLUX1\\.1 \\[pro] çš„ç”Ÿæˆé€Ÿåº¦æ¯”å…¶å‰èº«FLUX.1 \\[pro]å¿«å…­å€ï¼ŒåŒæ—¶æé«˜äº†å›¾åƒè´¨é‡ã€æç¤ºéµå¾ªåº¦å’Œå¤šæ ·æ€§ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬æ›´æ–°äº†FLUX.1 \\[pro]ï¼Œä½¿å…¶ç”Ÿæˆä¸Žä¹‹å‰ç›¸åŒçš„è¾“å‡ºï¼Œä½†é€Ÿåº¦æé«˜äº†ä¸¤å€.*â€ æ—©æœŸçš„FLUX.1 \\[pro]æ¨¡åž‹äºŽ2024å¹´8æœˆå‘å¸ƒã€‚\n\nåœ¨æˆ‘å‡†å¤‡è¿™ç¯‡æ–‡ç« æ—¶ï¼Œé»‘æ£®æž—å®žéªŒå®¤æŽ¨å‡ºäº†FLUX1\\.1 \\[pro]çš„è¶…æ¨¡å¼å’ŒåŽŸå§‹æ¨¡å¼ã€‚æ ¹æ®æ–°é—»ç¨¿ï¼Œâ€œ*ä»Šå¤©ï¼Œæˆ‘ä»¬ä¸ºFLUX1\\.1 \\[pro]å¢žåŠ äº†æ–°çš„é«˜åˆ†è¾¨çŽ‡åŠŸèƒ½ï¼Œæ‰©å±•å…¶åŠŸèƒ½ä»¥æ”¯æŒ4å€æ›´é«˜çš„å›¾åƒåˆ†è¾¨çŽ‡ï¼ˆæœ€é«˜å¯è¾¾4MPï¼‰ï¼ŒåŒæ—¶ä¿æŒæ¯ä¸ªæ ·æœ¬ä»…éœ€10ç§’çš„å‡ºè‰²ç”Ÿæˆæ—¶é—´.*â€\n\né»‘æ£®æž—å®žéªŒå®¤FLUX1\\.1 \\[pro]å’Œè¶…æ¨¡å¼çš„æµ‹è¯•æ˜¯åœ¨[Replicate](https://replicate.com/blog/machine-learning-needs-better-tools)ä¸Šè¿›è¡Œçš„ã€‚ä»–ä»¬çš„ç½‘ç«™å£°æ˜Žï¼Œâ€œ*Replicateåœ¨äº‘ä¸­è¿è¡Œæœºå™¨å­¦ä¹ æ¨¡åž‹ã€‚æˆ‘ä»¬æœ‰ä¸€ä¸ªå¼€æºæ¨¡åž‹åº“ï¼Œæ‚¨å¯ä»¥é€šè¿‡å‡ è¡Œä»£ç è¿è¡Œã€‚å¦‚æžœæ‚¨æ­£åœ¨æž„å»ºè‡ªå·±çš„æœºå™¨å­¦ä¹ æ¨¡åž‹ï¼ŒReplicateä½¿å…¶æ˜“äºŽå¤§è§„æ¨¡éƒ¨ç½².*â€\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*IUbfTFj32FxIta_3J1W0pQ.png)\n\nâœ… åœ¨æˆ‘çš„æµ‹è¯•ä¸­ï¼Œé»‘æ£®æž—å®žéªŒå®¤FLUX1\\.1 \\[pro]èƒ½å¤Ÿåœ¨è¶…è¿‡50%çš„æ—¶é—´å†…å‡†ç¡®é‡çŽ°æç¤ºä¸­è¯·æ±‚çš„æ–‡æœ¬ã€‚å®ƒåœ¨æ‰€æœ‰æµ‹è¯•æ¨¡åž‹ä¸­è¡¨çŽ°æœ€ä½³ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*RewBBA9MAiNbG93h65WdYg.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ISZfNQZHo3PL_QkYu3jEIw.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*XliNJWJr2TZ5MGwi7RAa-g.png)\n\n### Google Imagen 3\n\nGoogle Imagen 3 äºŽ2024å¹´8æœˆå‘æ‰€æœ‰ç¾Žå›½ç”¨æˆ·å‘å¸ƒã€‚æ ¹æ®è°·æ­Œçš„è¯´æ³•ï¼Œâ€œ*Imagen 3 æ˜¯æˆ‘ä»¬æœ€é«˜è´¨é‡çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡åž‹ï¼Œèƒ½å¤Ÿç”Ÿæˆå…·æœ‰æ›´å¥½ç»†èŠ‚ã€æ›´ä¸°å¯Œå…‰ç…§å’Œæ›´å°‘å¹²æ‰°æ€§ä¼ªå½±çš„å›¾åƒï¼Œæ¯”æˆ‘ä»¬ä¹‹å‰çš„æ¨¡åž‹æ›´å‡ºè‰²ã€‚*â€ Google Imagen 3 çš„æµ‹è¯•åœ¨ [ImageFX](https://aitestkitchen.withgoogle.com/tools/image-fx) ä¸Šè¿›è¡Œï¼Œè¿™æ˜¯è°·æ­Œ AI Test Kitchen çš„ä¸€éƒ¨åˆ†ï¼Œâ€œ*è¿™æ˜¯ä¸€ä¸ªäººä»¬å¯ä»¥ä½“éªŒå¹¶åé¦ˆè°·æ­Œæœ€æ–° AI æŠ€æœ¯çš„åœ°æ–¹ã€‚*â€\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*nhto3l0o-XITJzEEQoHSTA.png)\n\nðŸš« åœ¨æˆ‘çš„æµ‹è¯•ä¸­ï¼ŒGoogle Imagen 3 æ— æ³•å‡†ç¡®é‡çŽ°æç¤ºä¸­è¯·æ±‚çš„æ–‡æœ¬ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*9aqKPuZlpGF_lE3pA0ZNtw.png)\n\n### KLING AI ç”± Kolors æä¾›æ”¯æŒ\n\nKolors ä¸º Kling AI çš„å›¾åƒç”Ÿæˆèƒ½åŠ›æä¾›æ”¯æŒã€‚æ ¹æ® [Hugging Face](https://huggingface.co/Kwai-Kolors/Kolors) çš„è¯´æ³•ï¼Œâ€œ*Kolors æ˜¯ä¸€ä¸ªåŸºäºŽæ½œåœ¨æ‰©æ•£çš„å¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡åž‹ï¼Œç”±å¿«æ‰‹ Kolors å›¢é˜Ÿå¼€å‘ã€‚ç»è¿‡æ•°åäº¿å¯¹æ–‡æœ¬-å›¾åƒçš„è®­ç»ƒï¼ŒKolors åœ¨è§†è§‰è´¨é‡ã€å¤æ‚è¯­ä¹‰å‡†ç¡®æ€§ä»¥åŠä¸­æ–‡å’Œè‹±æ–‡å­—ç¬¦çš„æ–‡æœ¬æ¸²æŸ“æ–¹é¢ç›¸è¾ƒäºŽå¼€æºå’Œä¸“æœ‰æ¨¡åž‹å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚*â€ æ ¹æ® [Kuaishou](https://ir.kuaishou.com/news-releases/news-release-details/kuaishou-launches-full-beta-testing-kling-ai-global-users-0) çš„æ¶ˆæ¯ï¼ŒKling AI äºŽ 2024 å¹´ 7 æœˆå‘å¸ƒ\\ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*na56zUz3DLWK7Dqj51vSKw.png)\n\nðŸš« åœ¨æˆ‘çš„æµ‹è¯•ä¸­ï¼ŒKLING AI ç”± Kolors æä¾›æ”¯æŒæ— æ³•å‡†ç¡®å†çŽ°æç¤ºä¸­è¯·æ±‚çš„æ–‡æœ¬ã€‚ç»“æžœæ˜¯æ‰€æœ‰æµ‹è¯•æ¨¡åž‹ä¸­è¡¨çŽ°æœ€å·®çš„ã€‚è®¸å¤šå“åº”éƒ½æ˜¯ä¸­æ–‡ï¼Œå³ä½¿æ˜Žç¡®è¦æ±‚ä»¥è‹±æ–‡æ˜¾ç¤ºã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*xgq4C0m8s3Wfp4p9Va7fSQ.png)\n\n### Midjourney v6\\.1\n\nMidjourney v6\\.1 äºŽ2024å¹´7æœˆå‘å¸ƒã€‚æ ¹æ® [Midjourney](https://updates.midjourney.com/version-6-1/)ï¼Œæœ€æ–°å‘å¸ƒçš„ v6\\.1 åŒ…å«äº†å‡ é¡¹é‡è¦æ”¹è¿›ï¼ŒåŒ…æ‹¬æ›´è¿žè´¯çš„å›¾åƒï¼ˆæ‰‹è‡‚ã€è…¿ã€æ‰‹ã€èº«ä½“ã€æ¤ç‰©ã€åŠ¨ç‰©ç­‰ï¼‰ã€æ›´å¥½çš„å›¾åƒè´¨é‡ã€æ›´ç²¾ç¡®ã€è¯¦ç»†å’Œæ­£ç¡®çš„å°å›¾åƒç‰¹å¾ï¼Œä»¥åŠæ”¹è¿›çš„æ–‡æœ¬å‡†ç¡®æ€§ï¼ˆåœ¨æç¤ºä¸­é€šè¿‡â€œå¼•å·â€ç»˜åˆ¶å•è¯æ—¶ï¼‰ã€‚æ ¹æ® [Midjourney](https://docs.midjourney.com/docs/text-generation)ï¼Œä½¿ç”¨ `â€” â€” style raw` æ ‡å¿—ä¹Ÿæœ‰åŠ©äºŽåœ¨æŸäº›æµ‹è¯•æ¡ˆä¾‹ä¸­æé«˜æ–‡æœ¬å‡†ç¡®æ€§ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ETXx5VyY4BgEA8zn4K3M0g.png)\n\nðŸš« âœ… åœ¨æˆ‘çš„æµ‹è¯•ä¸­ï¼ŒMidjourney v6\\.1 çš„ç»“æžœå‚å·®ä¸é½ã€‚Midjourney åœ¨è¶…è¿‡ 50% çš„æ—¶é—´å†…æ— æ³•ä¸€è‡´åœ°å†çŽ°æç¤ºä¸­è¯·æ±‚çš„æ–‡æœ¬ã€‚åœ¨æŸäº›æµ‹è¯•æ¡ˆä¾‹ä¸­ï¼Œè¾“å‡ºæ˜¯æ­£ç¡®çš„ï¼Œè€Œåœ¨å…¶ä»–æ¡ˆä¾‹ä¸­åˆ™æŽ¥è¿‘æç¤ºï¼Œä½†ä¹ŸåŒæ ·ç»å¸¸é‡å¤å•è¯å’Œæ ‡ç‚¹ç¬¦å·ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*yIaVzqP_BwvDGMO5SOo1SA.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BDCsxYe_cJSb6pfoxKrWGA.jpeg)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*dqGYigq9T-PMx3GKfqSf2Q.png)\n\n### OpenAI DALLÂ·E 3\n\nOpenAI DALLÂ·E 3 äºŽ2023å¹´10æœˆå‘å¸ƒï¼Œè·ä»Šå·²æœ‰ä¸€å¹´å¤šã€‚æ ¹æ® [OpenAI](https://openai.com/index/dall-e-3/)ï¼Œ\"*DALLÂ·E 3 åœ¨ç”Ÿæˆå®Œå…¨ç¬¦åˆæ‚¨æä¾›çš„æ–‡æœ¬çš„å›¾åƒèƒ½åŠ›ä¸Šè¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚DALLÂ·E 3 ç†è§£çš„ç»†å¾®å·®åˆ«å’Œç»†èŠ‚è¿œè¶…æˆ‘ä»¬ä¹‹å‰çš„ç³»ç»Ÿ [DALLÂ·E 2]ï¼Œä½¿æ‚¨èƒ½å¤Ÿè½»æ¾å°†æ‚¨çš„æƒ³æ³•è½¬åŒ–ä¸ºæžå…¶å‡†ç¡®çš„å›¾åƒã€‚*\"\n\nOpenAI Imagen 3 çš„æµ‹è¯•æ˜¯åœ¨ [ChatGPT](https://openai.com/index/chatgpt/) ä¸Šè¿›è¡Œçš„ã€‚æ­¤å¤–ï¼Œæ ¹æ® [OpenAI](https://openai.com/index/dall-e-3/)ï¼Œ\"*DALLÂ·E 3 åŽŸç”Ÿæž„å»ºäºŽ ChatGPT ä¹‹ä¸Šï¼Œè¿™ä½¿æ‚¨å¯ä»¥å°† ChatGPT ä½œä¸ºå¤´è„‘é£Žæš´ä¼™ä¼´å’Œæç¤ºçš„å®Œå–„è€…ã€‚åªéœ€è¯¢é—® ChatGPT æ‚¨å¸Œæœ›åœ¨ä»Žç®€å•å¥å­åˆ°è¯¦ç»†æ®µè½ä¸­çš„ä»»ä½•å†…å®¹ä¸­çœ‹åˆ°çš„å†…å®¹ã€‚*\"\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*x45i0IJoYNiJT1kOi98k7w.png)\n\nðŸš« åœ¨æˆ‘çš„æµ‹è¯•ä¸­ï¼ŒOpenAI DALLÂ·E 3 æ— æ³•å‡†ç¡®å†çŽ°æç¤ºä¸­è¯·æ±‚çš„æ–‡æœ¬ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*NirwqSB-k8dzfGRNAw-pQw.png)\n\n### Stability AI Stable Diffusion 3\\.5 Large\n\næ ¹æ®Stability AIï¼Œå‘å¸ƒäºŽ2024å¹´10æœˆçš„[Stable Diffusion 3\\.5 Large](https://stability.ai/news/introducing-stable-diffusion-3-5)æ¨¡åž‹â€œ*æ‹¥æœ‰81äº¿å‚æ•°ï¼Œå…·æœ‰å“è¶Šçš„è´¨é‡å’Œå¯¹æç¤ºçš„éµå¾ªèƒ½åŠ›ï¼Œè¿™ä¸ªåŸºç¡€æ¨¡åž‹æ˜¯Stable Diffusionå®¶æ—ä¸­æœ€å¼ºå¤§çš„ã€‚è¯¥æ¨¡åž‹éžå¸¸é€‚åˆ1å…†åƒç´ åˆ†è¾¨çŽ‡çš„ä¸“ä¸šç”¨ä¾‹*ã€‚â€ Stability AI Stable Diffusion 3\\.5 Largeä½¿ç”¨[StabilityAI REST API](https://platform.stability.ai/docs/api-reference#tag/Generate/paths/~1v2beta~1stable-image~1generate~1ultra/post)å’Œåœ¨Jupyter Notebookä¸­ç”¨Pythonç¼–å†™çš„ä»£ç è¿›è¡Œäº†æµ‹è¯•ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*56Zp5QWVvTzGYlslcWEGKg.png)\n\nâœ… åœ¨æˆ‘çš„æµ‹è¯•ä¸­ï¼ŒStability AI Stable Diffusion 3\\.5 Largeèƒ½å¤Ÿåœ¨è¶…è¿‡50%çš„æ—¶é—´å†…å‡†ç¡®å†çŽ°æç¤ºä¸­è¯·æ±‚çš„æ–‡æœ¬ï¼Œå¶å°”ä¼šæœ‰è½»å¾®çš„æ ‡ç‚¹é”™è¯¯ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*CQ9I5z7x8ILTdFhu1dCBCQ.jpeg)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*G2D-L2fEtjKVTTyph3Burg.jpeg)\n\n### Stability AI Stable Image Ultra\n\næ ¹æ®Stability AIçš„è¯´æ³•ï¼Œ16 *billion\\-parameter [Stable Image Ultra](https://stability.ai/stable-image) æ¨¡åž‹äºŽ2024å¹´10æœˆå‘å¸ƒï¼Œâ€œæ˜¯æˆ‘ä»¬çš„æ——èˆ°æ¨¡åž‹ï¼Œç»“åˆäº†SD3 Largeçš„å¼ºå¤§åŠŸèƒ½ä¸Žå…ˆè¿›çš„å·¥ä½œæµç¨‹ï¼Œä»¥æä¾›æœ€é«˜è´¨é‡çš„ç…§ç‰‡çº§çœŸå®žå›¾åƒã€‚è¯¥é«˜çº§æ¨¡åž‹ä¸“ä¸ºéœ€è¦æ— ä¸Žä¼¦æ¯”è§†è§‰çœŸå®žæ„Ÿçš„è¡Œä¸šè®¾è®¡ï¼Œä¾‹å¦‚å¸‚åœºè¥é”€ã€å¹¿å‘Šå’Œå»ºç­‘ã€‚â€ä¸ŽAmazon Titan Image Generatorä¸€æ ·ï¼ŒStability AI Stable Image Ultraæ¨¡åž‹ä¹Ÿä½¿ç”¨[Amazon Bedrock](https://aws.amazon.com/blogs/aws/stability-ais-best-image-generating-models-now-in-amazon-bedrock/)è¿›è¡Œäº†æµ‹è¯•ï¼Œä½¿ç”¨äº†Image Playground UIã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GjaPW2FWGGhuJ06trs1Jww.png)\n\nâœ… åœ¨æˆ‘çš„æµ‹è¯•ä¸­ï¼ŒStability AI Stable Image Ultraèƒ½å¤Ÿåœ¨è¶…è¿‡50%çš„æ—¶é—´å†…å‡†ç¡®å†çŽ°æç¤ºä¸­è¯·æ±‚çš„æ–‡æœ¬ã€‚ä¸ŽBlack Forest Labs FLUX1\\.1 \\[pro]ä¸€èµ·ï¼Œå®ƒæ˜¯æµ‹è¯•ä¸­è¡¨çŽ°æœ€ä½³çš„æ¨¡åž‹ä¹‹ä¸€ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*O7JKeKBPgaEOuvdFW-u2Sg.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*jDHNLjOKHuEBQlFvTb7nYQ.png)\n\n## AIç”Ÿæˆæ–‡æœ¬çš„æ›¿ä»£æ–¹æ¡ˆ\n\nBlack Forest Labsçš„FLUX1.1 \\[pro]å’ŒStability AIçš„Stable Image Ultraæ¨¡åž‹æ¯”å…¶ä»–æ¨¡åž‹æ›´é¢‘ç¹åœ°å‡†ç¡®å†çŽ°æç¤ºä¸­çš„è¯·æ±‚çŸ­è¯­ã€‚ç„¶è€Œï¼Œç”¨æˆ·ä»ç„¶æ— æ³•æŽ§åˆ¶å›¾åƒçš„è®¸å¤šæ–¹é¢ï¼ŒåŒ…æ‹¬æ–‡æœ¬çš„ç¡®åˆ‡ä½ç½®ã€å¤§å°ã€å­—è·ã€é¢œè‰²å’Œå­—ä½“æ ·å¼ã€‚å­˜åœ¨å‡ ç§æ›¿ä»£ä¸”æ›´å¯é çš„æŠ€æœ¯ï¼Œä»¥ç¡®ä¿ç”Ÿæˆå›¾åƒä¸­æ–‡æœ¬çš„å‡†ç¡®æ€§ã€‚\n\n### æ›¿æ¢ç”Ÿæˆçš„æ–‡æœ¬\n\nä¸€ç§æ›¿ä»£æ–¹æ³•æ˜¯ç”Ÿæˆå¸¦æœ‰æ‰€éœ€æ–‡æœ¬çš„å›¾åƒï¼Œè€Œä¸è€ƒè™‘æ‹¼å†™é”™è¯¯ã€‚éšåŽï¼Œå¯ä»¥åœ¨ Adobe Photoshop ä¸­åˆ é™¤æ–‡æœ¬ï¼Œå¹¶ç”¨æ­£ç¡®çš„æ–‡æœ¬æ›¿æ¢ï¼Œç¡®ä¿ä½ç½®ã€å¤§å°ã€é¢œè‰²å’Œæ ·å¼å®Œå…¨ä¸€è‡´ã€‚ç„¶è€Œï¼Œå¦‚æžœå‰æ™¯ä¸»ä½“æˆ–é˜´å½±éƒ¨åˆ†é®æŒ¡æ–‡æœ¬ï¼Œæˆ–è€…æ–‡æœ¬å‡ºçŽ°åœ¨ä¸è§„åˆ™çš„è¡¨é¢ä¸Šï¼Œåˆ é™¤å’Œé‡å»ºæ–‡æœ¬å¯èƒ½ä¼šå¾ˆå…·æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†å¢žå¼ºæ–°æ–‡æœ¬çš„çœŸå®žæ„Ÿï¼Œå¯ä»¥å°†çŸ¢é‡æ–‡æœ¬æ …æ ¼åŒ–ï¼Œç„¶åŽæ·»åŠ å™ªå£°ã€æ¨¡ç³Šã€æ‰­æ›²ã€å…‰ç…§ã€çº¹ç†å’Œå›¾å±‚æ··åˆæ•ˆæžœã€‚\n\nä»¥ä¸‹æ˜¯ä½¿ç”¨ Black Forest Labs FLUX1\\.1 \\[pro] Ultra ç”Ÿæˆçš„ä¸¤å¹…å›¾åƒç¤ºä¾‹ï¼ˆç¬¬ä¸€å¹…å›¾åƒï¼‰ã€‚æ–‡æœ¬å·²åœ¨ Adobe Photoshop ä¸­åˆ é™¤ï¼ˆç¬¬äºŒå¹…å›¾åƒï¼‰ï¼Œæ·»åŠ äº†æ–°çš„åŸºäºŽçŸ¢é‡çš„æ–‡æœ¬ï¼ˆç¬¬ä¸‰å¹…å›¾åƒï¼‰ï¼Œæœ€åŽï¼Œæ–‡æœ¬å·²è¢«æ …æ ¼åŒ–å¹¶æ‰­æ›²ï¼Œä»¥æ˜¾å¾—æ›´çœŸå®žï¼ˆç¬¬å››å¹…å›¾åƒï¼‰ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*B0_3d8oImDlrRb6mjpekrw.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*fmrW46OsZe6Zsc0eshPyYw.png)\n\n### ä»Žç©ºç™½ç”»å¸ƒå¼€å§‹\n\nç¬¬äºŒç§é€‰æ‹©æ˜¯ç”Ÿæˆæ²¡æœ‰æ–‡æœ¬çš„å›¾åƒï¼Œç„¶åŽä½¿ç”¨ Adobe Photoshop æ·»åŠ æ‚¨æ‰€éœ€é¢œè‰²ã€å¤§å°å’Œå­—ä½“æ ·å¼çš„æ–‡æœ¬ã€‚è¿™ç§æŠ€æœ¯æ¯”å¯¹ç”Ÿæˆçš„å›¾åƒè¿›è¡Œä¿®é¥°ä»¥åŽ»é™¤çŽ°æœ‰æ–‡æœ¬è¦ç®€å•å¾—å¤šã€‚ç¤ºä¾‹æ˜¯ä½¿ç”¨ [Replicate](https://replicate.com/docs/get-started/python) APIï¼Œé€šè¿‡ Jupyter Notebook è°ƒç”¨ Black Forest Labs çš„ FLUX1\\.1 \\[pro] å’Œ Ultra åˆ›å»ºçš„ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*iFpqy4fEUJOXaJMzhsgbDA.png)\n\nä¸‹é¢æ˜¯ä½¿ç”¨ Black Forest Labs FLUX1\\.1 \\[pro] Ultra ç”Ÿæˆçš„å›¾åƒï¼Œæç¤ºä¸ºï¼šâ€œ*ä¸€ä½å¾®ç¬‘çš„å¥³æ€§ç§‘å­¦å®¶ç©¿ç€å®žéªŒå®¤å¤–å¥—ï¼Œç«™åœ¨å®žéªŒå®¤é‡Œï¼Œæ‰‹æŒä¸€å—æ²¡æœ‰æ–‡å­—æˆ–å…¶ä»–å…ƒç´ çš„ç™½è‰²çŸ©å½¢æ ‡ç‰Œã€‚*â€ç”Ÿæˆçš„å›¾åƒï¼ˆç¬¬ä¸€å¼ å›¾ï¼‰æ·»åŠ äº†æ–°æ–‡æœ¬ï¼ˆç¬¬äºŒå¼ å›¾ï¼‰ï¼Œæœ€åŽï¼Œæ–‡æœ¬è¢«æ‰­æ›²ä»¥æ˜¾å¾—æ›´çœŸå®žï¼ˆç¬¬ä¸‰å¼ å›¾ï¼‰ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*c1rgPArHDrUQ2cePV9DUCA.png)\n\nä¸‹é¢æ˜¯å¦ä¸€ä¸ªä¾‹å­ï¼Œå¼€å§‹æ—¶ç”Ÿæˆçš„å›¾åƒæ²¡æœ‰æ–‡æœ¬ï¼ŒåŽæ¥æ·»åŠ äº†æ–‡æœ¬ã€‚æœ€åˆçš„å›¾åƒæ˜¯ä½¿ç”¨ Black Forest Labs FLUX1\\.1 \\[pro] Ultra ç”Ÿæˆçš„ï¼Œæç¤ºä¸ºï¼šâ€œ*å„ç§è”¬èœçš„è”¬èœæ‘Šï¼ŒåŒ…æ‹¬è¥¿çº¢æŸ¿ã€‚ä¸€ä¸ªå°çš„ã€çŸ©å½¢çš„ã€ç©ºç™½çš„é»‘è‰²æ ‡ç‰Œï¼Œæ—è¾¹æ²¡æœ‰æ–‡å­—æˆ–å…¶ä»–å…ƒç´ ï¼Œæ”¾åœ¨è¥¿çº¢æŸ¿æ—è¾¹ã€‚*â€\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*t_6oM1aItMGQfBUPjH83lA.png)\n\næœ€åŽä¸€ä¸ªä¾‹å­ä½¿ç”¨æç¤ºï¼šâ€œ*ä¸€ä¸ªå…‰æ»‘çš„å¹¿å‘Šç‰Œé«˜é«˜è€¸ç«‹åœ¨ç¹å¿™çš„é«˜é€Ÿå…¬è·¯ä¸Šï¼Œè½¦è¾†é£žé©°è€Œè¿‡ã€‚å¹¿å‘Šç‰Œçš„èƒŒæ™¯æ˜¯è‰²å½©ä¸°å¯Œã€åŠ¨æ€çš„æŠ½è±¡å›¾æ¡ˆã€‚*â€æ¥ç”ŸæˆåŽŸå§‹å›¾åƒã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*KyGveUehRxuFTK-DmWnCaw.jpeg)\n\n## åˆ†åˆ«ç”Ÿæˆå›¾åƒå’Œæ–‡æœ¬\n\nç¬¬ä¸‰ç§ä¹Ÿæ˜¯æœ€åŽä¸€ç§æŠ€æœ¯æ˜¯ä½¿ç”¨æ‚¨é€‰æ‹©çš„æ¨¡åž‹åˆ†åˆ«ç”Ÿæˆå›¾åƒå’Œæ–‡æœ¬ï¼Œç„¶åŽåœ¨åŽæœŸåˆ¶ä½œä¸­ä½¿ç”¨ Adobe Photoshop å°†è¿™ä¸¤ä¸ªå…ƒç´ ç»“åˆèµ·æ¥ã€‚ä¸‹é¢æ˜¯å·¦ä¾§æ²¡æœ‰æ–‡æœ¬çš„ Midjourney åŽŸå§‹å›¾åƒï¼Œä½¿ç”¨çš„æç¤ºæ˜¯ï¼šâ€œ*å„ç§è”¬èœçš„è”¬èœæ‘Šï¼ŒåŒ…æ‹¬è¥¿çº¢æŸ¿ã€‚ä¸€ä¸ªç©ºç™½çš„é»‘æ¿æ ·å¼æ ‡å¿—ã€‚â€” ar 1:1*â€\n\nä¸­é—´é»‘è‰²èƒŒæ™¯ä¸Šçš„ç™½è‰²æ–‡å­—ä¹Ÿæ˜¯åœ¨ Midjourney ä¸­ç”Ÿæˆçš„ï¼Œä½¿ç”¨çš„æç¤ºæ˜¯ï¼šâ€œ*çŸ­è¯­â€œå†œåœºæ–°é²œè¥¿çº¢æŸ¿ $2.99/ç£…ã€‚â€ç”¨ç™½è‰²ç²‰ç¬”å­—å†™åœ¨çº¯é»‘è‰²èƒŒæ™¯ä¸Šã€‚â€” æ²¡æœ‰è¥¿çº¢æŸ¿æˆ–å…¶ä»–ç‰©ä½“ â€” ar 3:2 â€” é£Žæ ¼åŽŸå§‹ â€” é£Žæ ¼åŒ– 0*â€\n\næ–‡æœ¬å›¾åƒå¯ä»¥å¾ˆå®¹æ˜“åœ°å åŠ åœ¨ç¬¬ä¸€ä¸ªå›¾åƒä¸Šï¼Œä½¿ç”¨æ–‡æœ¬å›¾å±‚çš„â€œå˜äº®â€æ··åˆæ¨¡å¼ã€‚å¯ä»¥åº”ç”¨é¢å¤–çš„æ‰­æ›²æ•ˆæžœï¼Œä½¿æ–‡æœ¬åœ¨æœ€ç»ˆå›¾åƒä¸­çœ‹èµ·æ¥æ›´åŠ è‡ªç„¶ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ZP-pqTQVN8Xy_Vhjm8D0gg.png)\n\n## ç»“è®º\n\nåœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬æŽ¢è®¨äº†æ¥è‡ªä¸åŒæä¾›å•†çš„ä¹ç§æœ€å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡åž‹çš„èƒ½åŠ›ï¼Œä»¥æ ¹æ®æç¤ºç”Ÿæˆå›¾åƒä¸­çš„å‡†ç¡®æ–‡æœ¬ã€‚æˆ‘ä»¬å‘çŽ°ï¼ŒBlack Forest Labs çš„ FLUX1.1 [pro] å’Œ Stability AI çš„ Stable Image Ultra åœ¨å‡†ç¡®å†çŽ°å›¾åƒä¸­è¯·æ±‚çš„æ–‡æœ¬æ–¹é¢ï¼Œæ¯”å…¶ä»–æ¨¡åž‹æ›´æˆåŠŸã€‚æœ€åŽï¼Œæˆ‘ä»¬æ£€æŸ¥äº†ä¸‰ç§æ›¿ä»£çš„ã€æ›´å¯é çš„æŠ€æœ¯ï¼Œä»¥ç¡®ä¿ç”Ÿæˆå›¾åƒä¸­æ–‡æœ¬çš„å‡†ç¡®æ€§ã€‚\n\n*å¦‚æžœæ‚¨è¿˜ä¸æ˜¯ Medium ä¼šå‘˜å¹¶å¸Œæœ›æ”¯æŒåƒæˆ‘è¿™æ ·çš„ä½œè€…ï¼Œè¯·åœ¨æ­¤æ³¨å†Œï¼š<https://garystafford.medium.com/membership>ã€‚*\n\n*æœ¬åšå®¢ä»£è¡¨æˆ‘çš„è§‚ç‚¹ï¼Œè€Œä¸æ˜¯æˆ‘çš„é›‡ä¸»äºšé©¬é€Šç½‘ç»œæœåŠ¡ï¼ˆAWSï¼‰çš„è§‚ç‚¹ã€‚æ‰€æœ‰äº§å“åç§°ã€å›¾åƒã€å¾½æ ‡å’Œå“ç‰Œå‡ä¸ºå…¶å„è‡ªæ‰€æœ‰è€…çš„è´¢äº§ã€‚*\n\n"},{"lang":"zh","group":"blog","slug":"blog/conversational-ai-for-customer-service-best-practices-and-key-steps-for-success-4ceee714dbe1","frontmatter":{"title":"å®¢æˆ·æœåŠ¡å¯¹è¯å¼äººå·¥æ™ºèƒ½ï¼šæˆåŠŸçš„æœ€ä½³å®žè·µå’Œå…³é”®æ­¥éª¤","meta_title":"å®¢æˆ·æœåŠ¡å¯¹è¯å¼äººå·¥æ™ºèƒ½ï¼šæˆåŠŸçš„æœ€ä½³å®žè·µå’Œå…³é”®æ­¥éª¤","description":"å¯¹è¯å¼äººå·¥æ™ºèƒ½åœ¨å®¢æˆ·æœåŠ¡ä¸­æ­£è¿…é€Ÿå´›èµ·ï¼Œé¢„è®¡åˆ°2025å¹´å°†å®žçŽ°40%çš„å®¢æˆ·äº’åŠ¨è‡ªåŠ¨åŒ–ã€‚é€šè¿‡è‡ªç„¶è¯­è¨€å¤„ç†å’Œæœºå™¨å­¦ä¹ ï¼Œè¿™ç§æŠ€æœ¯èƒ½å¤Ÿæä¾›å³æ—¶å“åº”ã€æé«˜å®¢æˆ·æ»¡æ„åº¦ã€é™ä½Žè¿è¥æˆæœ¬ï¼Œå¹¶å®žçŽ°ä¸ªæ€§åŒ–æœåŠ¡ã€‚å®žæ–½æˆåŠŸçš„å…³é”®æ­¥éª¤åŒ…æ‹¬è¯†åˆ«å®¢æˆ·ç—›ç‚¹ã€å®šä¹‰ä½¿ç”¨æ¡ˆä¾‹ã€é€‰æ‹©åˆé€‚çš„å¹³å°ã€è®¾è®¡ç”¨æˆ·ä½“éªŒã€è®­ç»ƒæ•°æ®ã€ç³»ç»Ÿé›†æˆåŠæŒç»­ç›‘æµ‹ã€‚ä¼ä¸šéœ€éµå¾ªæœ€ä½³å®žè·µï¼Œå¦‚ç¡®ä¿å®‰å…¨åˆè§„ã€å¹³è¡¡è‡ªåŠ¨åŒ–ä¸Žäººå·¥æ”¯æŒã€ä¸ªæ€§åŒ–äº’åŠ¨ç­‰ï¼Œä»¥å®žçŽ°å¯¹è¯å¼äººå·¥æ™ºèƒ½çš„é•¿æœŸæˆåŠŸã€‚","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*LrRhvUJrNaS299z8oC2bkg.jpeg","categories":["Natural Language Processing","Machine Learning","Chatbots"],"author":"Rifx.Online","tags":["conversational","automation","personalization","monitoring","security"],"draft":false,"slug":"blog/conversational-ai-for-customer-service-best-practices-and-key-steps-for-success-4ceee714dbe1"},"content":"\n\n\n\n\nåœ¨å½“ä»Šå¿«èŠ‚å¥çš„å•†ä¸šçŽ¯å¢ƒä¸­ï¼Œå®¢æˆ·æœåŠ¡åœ¨å»ºç«‹å’Œç»´æŠ¤å®¢æˆ·å¿ è¯šåº¦æ–¹é¢å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚éšç€ä¼ä¸šåŠªåŠ›æä¾›ä¸ªæ€§åŒ–å’Œé«˜æ•ˆçš„æ”¯æŒï¼Œå¯¹è¯å¼äººå·¥æ™ºèƒ½ä½œä¸ºä¸€ç§é©å‘½æ€§è§£å†³æ–¹æ¡ˆåº”è¿è€Œç”Ÿã€‚é€šè¿‡å°†äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰é›†æˆåˆ°å®¢æˆ·æœåŠ¡è¿è¥ä¸­ï¼Œå…¬å¸å¯ä»¥ç®€åŒ–æµç¨‹ï¼Œæä¾›å³æ—¶å“åº”ï¼Œå¹¶æ˜¾è‘—æ”¹å–„æ•´ä½“å®¢æˆ·ä½“éªŒã€‚*Gartner* çš„ä¸€ä»½æŠ¥å‘Šä¼°è®¡ï¼Œåˆ° **2025** å¹´ï¼Œ**40% çš„å®¢æˆ·æœåŠ¡äº’åŠ¨** å°†é€šè¿‡äººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ æŠ€æœ¯å®žçŽ°å®Œå…¨è‡ªåŠ¨åŒ–ï¼Œè¿™ä¸€æ¯”ä¾‹ç›¸æ¯”äºŽ **2023** å¹´çš„ **25%** æœ‰äº†æ˜¾è‘—æå‡ã€‚\n\nå¸Œæœ›å¼€å‘å¯¹è¯å¼äººå·¥æ™ºèƒ½è§£å†³æ–¹æ¡ˆä»¥æå‡å®¢æˆ·æœåŠ¡çš„ä¼ä¸šæ­£åœ¨è¿›å…¥ä¸€ä¸ªå…·æœ‰å·¨å¤§æ½œåŠ›çš„å˜é©ç©ºé—´ã€‚æœ¬æ–‡æŽ¢è®¨äº†å®žæ–½å¯¹è¯å¼äººå·¥æ™ºèƒ½ä»¥æŽ¨åŠ¨å®¢æˆ·æ”¯æŒè¿è¥æˆåŠŸçš„æœ€ä½³å®žè·µã€å…³é”®æ­¥éª¤å’Œå¥½å¤„ï¼Œä¸ºæ¸´æœ›å¼€å‘è‡ªèº« AI é©±åŠ¨å®¢æˆ·æœåŠ¡è§£å†³æ–¹æ¡ˆçš„ä¼ä¸šæä¾›è¯¦ç»†æŒ‡å—ã€‚\n\n## å¯¹è¯å¼äººå·¥æ™ºèƒ½åœ¨å®¢æˆ·æœåŠ¡ä¸­çš„å´›èµ·\n\nå¯¹è¯å¼äººå·¥æ™ºèƒ½ç»“åˆäº†è‡ªç„¶è¯­è¨€å¤„ç† (NLP)ã€æœºå™¨å­¦ä¹ å’Œè‡ªåŠ¨åŒ–æ¶ˆæ¯ä¼ é€’ï¼Œä»¥ä¿ƒè¿›å®¢æˆ·ä¸Žæ•°å­—ç³»ç»Ÿä¹‹é—´æ— ç¼ä¸”ç±»äººåŒ–çš„äº’åŠ¨ã€‚è¿™äº›æŠ€æœ¯æ—¨åœ¨ç†è§£å®¢æˆ·æŸ¥è¯¢ï¼Œæä¾›å‡†ç¡®çš„å“åº”ï¼Œå¹¶ä¸Žç”¨æˆ·è¿›è¡Œæœ‰æ„ä¹‰çš„å¯¹è¯ã€‚éšç€å¯¹å…¨å¤©å€™å®¢æˆ·æ”¯æŒå’Œå³æ—¶é—®é¢˜è§£å†³éœ€æ±‚çš„å¢žåŠ ï¼Œä¼ä¸šçŽ°åœ¨æ­£è½¬å‘å¯¹è¯å¼äººå·¥æ™ºèƒ½ï¼Œä»¥é«˜æ•ˆæ»¡è¶³è¿™äº›éœ€æ±‚ã€‚\n\nå°†å¯¹è¯å¼äººå·¥æ™ºèƒ½çº³å…¥å®¢æˆ·æœåŠ¡ä¸ä»…æé«˜äº†è¿è¥æ•ˆçŽ‡ï¼Œè¿˜å¢žå¼ºäº†å®¢æˆ·æ»¡æ„åº¦ã€‚äº‹å®žä¸Šï¼Œæ ¹æ® *Salesforce* çš„æ•°æ®ï¼Œ**69% çš„æ¶ˆè´¹è€…** æœŸæœ›äººå·¥æ™ºèƒ½é©±åŠ¨çš„äº’åŠ¨èƒ½å¤Ÿæä¾›æ›´ç›¸å…³å’Œä¸ªæ€§åŒ–çš„ä½“éªŒï¼Œè¿™çªæ˜¾äº†å¯¹è¯å¼äººå·¥æ™ºèƒ½åœ¨æä¾›ä»¥å®¢æˆ·ä¸ºä¸­å¿ƒçš„æœåŠ¡ä¸­çš„æ—¥ç›Šé‡è¦æ€§ã€‚\n\n## ä¸ºä»€ä¹ˆä¼ä¸šåº”è¯¥ä¸ºå®¢æˆ·æœåŠ¡å¼€å‘å¯¹è¯å¼äººå·¥æ™ºèƒ½ï¼Ÿ\n\nå¯¹äºŽå¸Œæœ›[**å¼€å‘ AI é©±åŠ¨çš„å®¢æˆ·æœåŠ¡è§£å†³æ–¹æ¡ˆ**](https://www.blockchainappfactory.com/generative-ai-solutions?utm_source=medium&utm_medium=blog&utm_campaign=elavarasan)çš„ä¼ä¸šæ¥è¯´ï¼Œä¼˜åŠ¿æ˜¯å·¨å¤§çš„ã€‚ä»¥ä¸‹æ˜¯æŠ•èµ„å¯¹è¯å¼äººå·¥æ™ºèƒ½ç”¨äºŽå®¢æˆ·æœåŠ¡çš„å‡ ä¸ªæœ‰åŠ›ç†ç”±ï¼š\n\n**1\\. æ”¹å–„å®¢æˆ·ä½“éªŒ**\n\nå¯¹è¯å¼äººå·¥æ™ºèƒ½èƒ½å¤Ÿç«‹å³å›žåº”å®¢æˆ·æŸ¥è¯¢ï¼Œæ¶ˆé™¤äº†å®¢æˆ·åœ¨é•¿é˜Ÿä¸­ç­‰å¾…æˆ–å¤„ç†å»¶è¿Ÿå“åº”çš„éœ€æ±‚ã€‚é€šè¿‡æä¾› 24/7 çš„æ”¯æŒï¼Œä¼ä¸šå¯ä»¥é€šè¿‡ AI é©±åŠ¨çš„èŠå¤©æœºå™¨äººå’Œè™šæ‹ŸåŠ©æ‰‹æä¾›æ›´å¿«ã€æ›´é¡ºç•…çš„ä½“éªŒï¼Œä»Žè€Œæé«˜å®¢æˆ·æ»¡æ„åº¦å’Œå¿ è¯šåº¦ã€‚\n\n**2\\. æé«˜æ•ˆçŽ‡**\n\nAI é©±åŠ¨çš„æœºå™¨äººå¯ä»¥åŒæ—¶å¤„ç†å¤§é‡å®¢æˆ·è¯¢é—®ï¼Œä»Žè€Œè®©äººå·¥å®¢æœä¸“æ³¨äºŽæ›´å¤æ‚çš„é—®é¢˜ã€‚è¿™å¯¼è‡´èµ„æºçš„æ›´æœ‰æ•ˆåˆ©ç”¨ï¼Œé™ä½Žäº†è¿è¥æˆæœ¬å¹¶ç¼©çŸ­äº†å“åº”æ—¶é—´ã€‚\n\n**3\\. æˆæœ¬èŠ‚çº¦**\n\nå®žæ–½å¯¹è¯å¼äººå·¥æ™ºèƒ½æ˜¾è‘—é™ä½Žäº†ä¸Žå®¢æˆ·æœåŠ¡è¿è¥ç›¸å…³çš„æˆæœ¬ã€‚æ ¹æ® *Juniper Research* çš„æ•°æ®ï¼Œåˆ° 2023 å¹´ï¼Œæ•´åˆèŠå¤©æœºå™¨äººåˆ°å®¢æˆ·æœåŠ¡è¿è¥ä¸­çš„ä¼ä¸šæ¯å¹´å¯èŠ‚çœé«˜è¾¾ **110 äº¿ç¾Žå…ƒ**ã€‚è¿™äº›èŠ‚çœæ¥è‡ªäºŽå‡å°‘å¯¹å¤§åž‹å®¢æˆ·æœåŠ¡å›¢é˜Ÿçš„éœ€æ±‚å’Œè‡ªåŠ¨åŒ–é‡å¤ä»»åŠ¡ã€‚\n\n**4\\. å¢žå¼ºä¸ªæ€§åŒ–**\n\né€šè¿‡åˆ†æžå®¢æˆ·æ•°æ®å’Œåå¥½ï¼ŒAI ç³»ç»Ÿå¯ä»¥æä¾›é‡èº«å®šåˆ¶çš„ä¸ªæ€§åŒ–å“åº”ã€‚è¿™ç§å®šåˆ¶åŒ–ç¨‹åº¦æœ‰åŠ©äºŽä¼ä¸šä¸Žå®¢æˆ·å»ºç«‹æ›´å¼ºçš„å…³ç³»ï¼Œå¹¶ä¿ƒè¿›æ›´å¤§çš„å‚ä¸Žåº¦ã€‚\n\n## å®žæ–½å¯¹è¯å¼äººå·¥æ™ºèƒ½åœ¨å®¢æˆ·æœåŠ¡ä¸­çš„å…³é”®æ­¥éª¤\n\nå®žæ–½å¯¹è¯å¼äººå·¥æ™ºèƒ½ä»¥æä¾›å®¢æˆ·æœåŠ¡éœ€è¦ä¸€ç§æ·±æ€ç†Ÿè™‘å’Œæˆ˜ç•¥æ€§çš„æ–¹å¼ï¼Œä»¥ç¡®ä¿å…¶èƒ½å¤Ÿå®žçŽ°é¢„æœŸçš„ç»“æžœã€‚ä»¥ä¸‹æ˜¯ä¼ä¸šåœ¨å¼€å‘åŸºäºŽäººå·¥æ™ºèƒ½çš„å®¢æˆ·æ”¯æŒè§£å†³æ–¹æ¡ˆæ—¶åº”éµå¾ªçš„å…³é”®æ­¥éª¤ã€‚\n\n**æ­¥éª¤ 1ï¼šè¯†åˆ«å®¢æˆ·ç—›ç‚¹**\n\nåœ¨å®žæ–½å¯¹è¯å¼äººå·¥æ™ºèƒ½ä¹‹å‰ï¼Œäº†è§£å®¢æˆ·åœ¨ä¸Žå®¢æˆ·æœåŠ¡äº’åŠ¨è¿‡ç¨‹ä¸­é¢ä¸´çš„å…·ä½“æŒ‘æˆ˜å’Œç—›ç‚¹è‡³å…³é‡è¦ã€‚è¯¸å¦‚ç­‰å¾…æ—¶é—´è¿‡é•¿ã€é‡å¤æŸ¥è¯¢æˆ–éš¾ä»¥èŽ·å–ä¿¡æ¯ç­‰å¸¸è§é—®é¢˜åº”ç”±äººå·¥æ™ºèƒ½è§£å†³æ–¹æ¡ˆæ¥å¤„ç†ã€‚è¿›è¡Œè°ƒæŸ¥ã€åˆ†æžå®¢æˆ·æœåŠ¡æ•°æ®å¹¶æ”¶é›†åé¦ˆï¼Œä»¥ç¡®å®šäººå·¥æ™ºèƒ½å¯ä»¥æä¾›æœ€å¤§ä»·å€¼çš„é¢†åŸŸã€‚\n\n**æ­¥éª¤ 2ï¼šå®šä¹‰å¯¹è¯å¼äººå·¥æ™ºèƒ½çš„ä½¿ç”¨æ¡ˆä¾‹**\n\nä¸€æ—¦è¯†åˆ«å‡ºå®¢æˆ·ç—›ç‚¹ï¼Œå°±è¦å®šä¹‰å¯¹è¯å¼äººå·¥æ™ºèƒ½æœ€æœ‰æ•ˆçš„å…·ä½“ä½¿ç”¨æ¡ˆä¾‹ã€‚ä½¿ç”¨æ¡ˆä¾‹å¯ä»¥ä»Žå›žç­”å¸¸è§é—®é¢˜ï¼ˆFAQsï¼‰åˆ°å¤„ç†æ›´å¤æ‚çš„æµç¨‹ï¼Œå¦‚é¢„è®¢æœåŠ¡ã€å¤„ç†é€€è´§æˆ–æä¾›äº§å“æŽ¨èã€‚ä¼ä¸šå¿…é¡»ä¸“æ³¨äºŽä¼˜å…ˆè€ƒè™‘ç›´æŽ¥æ»¡è¶³å®¢æˆ·éœ€æ±‚å¹¶æä¾›å¯è¡¡é‡ç›Šå¤„çš„é«˜å½±å“ä½¿ç”¨æ¡ˆä¾‹ã€‚\n\n**å¸¸è§ä½¿ç”¨æ¡ˆä¾‹åŒ…æ‹¬ï¼š**\n\n* **è®¢å•çŠ¶æ€æŸ¥è¯¢ï¼š** äººå·¥æ™ºèƒ½æœºå™¨äººå¯ä»¥ç«‹å³æ£€ç´¢å¹¶ä¸Žå®¢æˆ·åˆ†äº«è®¢å•çŠ¶æ€æ›´æ–°ã€‚\n* **äº§å“ä¿¡æ¯ï¼š** è™šæ‹ŸåŠ©æ‰‹å¯ä»¥æä¾›è¯¦ç»†çš„äº§å“ä¿¡æ¯ï¼Œå¹¶æ ¹æ®å®¢æˆ·è¡Œä¸ºå»ºè®®äº’è¡¥å•†å“ã€‚\n* **æŠ€æœ¯æ”¯æŒï¼š** åŸºäºŽäººå·¥æ™ºèƒ½çš„æœºå™¨äººå¯ä»¥æŒ‡å¯¼ç”¨æˆ·è¿›è¡Œå¸¸è§æŠ€æœ¯é—®é¢˜çš„æ•…éšœæŽ’é™¤æ­¥éª¤ã€‚\n* **è´¦å•å’Œæ”¯ä»˜ï¼š** èŠå¤©æœºå™¨äººå¯ä»¥ä¿ƒè¿›æ”¯ä»˜ã€è´¦å•æŸ¥è¯¢å’Œè®¢é˜…ç®¡ç†ã€‚\n\n**æ­¥éª¤ 3ï¼šé€‰æ‹©åˆé€‚çš„å¯¹è¯å¼äººå·¥æ™ºèƒ½å¹³å°**\n\né€‰æ‹©åˆé€‚çš„äººå·¥æ™ºèƒ½å¹³å°å¯¹å®¢æˆ·æœåŠ¡è‡ªåŠ¨åŒ–å·¥ä½œçš„æˆåŠŸè‡³å…³é‡è¦ã€‚åœ¨è¯„ä¼°äººå·¥æ™ºèƒ½å¹³å°æ—¶ï¼Œè€ƒè™‘å¯æ‰©å±•æ€§ã€é›†æˆçš„ä¾¿åˆ©æ€§ã€è‡ªç„¶è¯­è¨€å¤„ç†èƒ½åŠ›å’Œå®šåˆ¶é€‰é¡¹ç­‰å› ç´ ã€‚åƒ Google Dialogflowã€Microsoft Azure Bot Services å’Œ IBM Watson ç­‰æµè¡Œäººå·¥æ™ºèƒ½å¹³å°æä¾›äº†æž„å»ºå’Œéƒ¨ç½²åŸºäºŽäººå·¥æ™ºèƒ½çš„å®¢æˆ·æœåŠ¡è§£å†³æ–¹æ¡ˆçš„å¼ºå¤§å·¥å…·ã€‚è¯¥å¹³å°åº”æ”¯æŒå¤šæ¸ é“äº’åŠ¨ï¼ˆä¾‹å¦‚ï¼ŒèŠå¤©ã€è¯­éŸ³ã€ç¤¾äº¤åª’ä½“ï¼‰ï¼Œä»¥ç¡®ä¿ä¸Žå®¢æˆ·åœ¨ä¸åŒå¹³å°ä¸Šçš„æ— ç¼æ²Ÿé€šã€‚\n\n**æ­¥éª¤ 4ï¼šè®¾è®¡ç›´è§‚çš„ç”¨æˆ·ä½“éªŒ**\n\nç›´è§‚å’Œç”¨æˆ·å‹å¥½çš„ç•Œé¢æ˜¯å¯¹è¯å¼äººå·¥æ™ºèƒ½ç³»ç»ŸæˆåŠŸçš„å…³é”®ã€‚ç¡®ä¿äººå·¥æ™ºèƒ½èƒ½å¤Ÿä»¥æ¸…æ™°ã€ç®€æ˜Žçš„å¯¹è¯ä¸Žå®¢æˆ·äº’åŠ¨ï¼Œå¼•å¯¼ä»–ä»¬é«˜æ•ˆè§£å†³é—®é¢˜ã€‚äººå·¥æ™ºèƒ½åº”ç†è§£è‡ªç„¶è¯­è¨€ï¼Œè¯†åˆ«å®¢æˆ·æ„å›¾ï¼Œå¹¶æå‡ºç›¸å…³çš„åŽç»­é—®é¢˜ï¼Œä»¥å¸®åŠ©ç”¨æˆ·å¿«é€ŸèŽ·å–æ‰€éœ€çš„ä¿¡æ¯ã€‚\n\nä¸ºäº†å¢žå¼ºç”¨æˆ·ä½“éªŒï¼Œè®¾è®¡å¯¹è¯æµç¨‹å°½å¯èƒ½äººæ€§åŒ–ã€‚é€šè¿‡ç§°å‘¼ç”¨æˆ·çš„åå­—ã€å›žå¿†è¿‡åŽ»çš„å¯¹è¯ï¼Œå¹¶æ ¹æ®ä»–ä»¬ä¸Žæ‚¨ä¸šåŠ¡çš„å…ˆå‰äº’åŠ¨æä¾›è§£å†³æ–¹æ¡ˆï¼Œæ¥ä¸ªæ€§åŒ–äº’åŠ¨ã€‚\n\n**æ­¥éª¤ 5ï¼šç”¨ç›¸å…³æ•°æ®è®­ç»ƒäººå·¥æ™ºèƒ½**\n\nå¯¹è¯å¼äººå·¥æ™ºèƒ½çš„æ€§èƒ½åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºŽå…¶è®­ç»ƒçš„æ•°æ®çš„è´¨é‡å’Œæ•°é‡ã€‚ç”¨çœŸå®žçš„å®¢æˆ·äº’åŠ¨æ•°æ®æ¥è®­ç»ƒäººå·¥æ™ºèƒ½ï¼Œä»¥æé«˜å…¶ç†è§£ä¸åŒæŸ¥è¯¢ã€ç»†å¾®å·®åˆ«å’Œè¯­è¨€å˜ä½“çš„èƒ½åŠ›ã€‚é€šè¿‡ä¸æ–­æä¾›æ¥è‡ªè¿‡åŽ»äº’åŠ¨çš„æ•°æ®ï¼Œç¡®ä¿äººå·¥æ™ºèƒ½éšç€æ—¶é—´çš„æŽ¨ç§»ä¸æ–­å­¦ä¹ å’Œæ”¹è¿›ï¼Œä»Žè€Œå¢žå¼ºå…¶æä¾›å‡†ç¡®å’Œä¸Šä¸‹æ–‡ç›¸å…³å“åº”çš„èƒ½åŠ›ã€‚\n\n**æ­¥éª¤ 6ï¼šä¸ŽçŽ°æœ‰ç³»ç»Ÿé›†æˆ**\n\nä¸ºäº†ä½¿å¯¹è¯å¼äººå·¥æ™ºèƒ½çœŸæ­£æœ‰æ•ˆï¼Œå¿…é¡»ä¸ŽçŽ°æœ‰çš„å®¢æˆ·æœåŠ¡ç³»ç»Ÿé›†æˆã€‚ç¡®ä¿äººå·¥æ™ºèƒ½è§£å†³æ–¹æ¡ˆèƒ½å¤Ÿä»Žæ‚¨çš„ CRMã€è´¦å•ç³»ç»Ÿã€è®¢å•ç®¡ç†å¹³å°å’ŒçŸ¥è¯†åº“ä¸­æå–æ•°æ®ï¼Œä»¥å‘å®¢æˆ·æä¾›ç›¸å…³å’Œå‡†ç¡®çš„ä¿¡æ¯ã€‚è¿™ç§é›†æˆä½¿äººå·¥æ™ºèƒ½èƒ½å¤Ÿå®žæ—¶å“åº”æŸ¥è¯¢ï¼Œå¹¶ç¡®ä¿å®¢æˆ·åœ¨æ‰€æœ‰æŽ¥è§¦ç‚¹ä¸ŠèŽ·å¾—ä¸€è‡´çš„æ”¯æŒã€‚\n\n**æ­¥éª¤ 7ï¼šæŒç»­ç›‘æµ‹å’Œæ”¹è¿›**\n\nåœ¨éƒ¨ç½²å¯¹è¯å¼äººå·¥æ™ºèƒ½è§£å†³æ–¹æ¡ˆåŽï¼ŒæŒç»­ç›‘æµ‹å…¶æ€§èƒ½å¹¶è¯†åˆ«æ”¹è¿›é¢†åŸŸè‡³å…³é‡è¦ã€‚è·Ÿè¸ªå…³é”®ç»©æ•ˆæŒ‡æ ‡ï¼ˆKPIï¼‰ï¼Œå¦‚å“åº”æ—¶é—´ã€è§£å†³çŽ‡å’Œå®¢æˆ·æ»¡æ„åº¦è¯„åˆ†ã€‚åˆ©ç”¨è¿™äº›æ´žå¯Ÿæ¥ä¼˜åŒ–äººå·¥æ™ºèƒ½çš„èƒ½åŠ›ã€è°ƒæ•´å¯¹è¯æµç¨‹ï¼Œå¹¶è§£å†³ä»»ä½•ä¸è¶³ä¹‹å¤„ã€‚äººå·¥æ™ºèƒ½ç³»ç»Ÿåº”éšç€æ—¶é—´çš„æŽ¨ç§»é€šè¿‡å­¦ä¹ äº’åŠ¨å’Œé€‚åº”ä¸æ–­å˜åŒ–çš„å®¢æˆ·æœŸæœ›è€Œå‘å±•ã€‚\n\n## æˆåŠŸå®žæ–½å¯¹è¯å¼äººå·¥æ™ºèƒ½çš„æœ€ä½³å®žè·µ\n\nå¼€å‘å’Œ[**åœ¨å®¢æˆ·æœåŠ¡ä¸­éƒ¨ç½²å¯¹è¯å¼äººå·¥æ™ºèƒ½**](https://www.blockchainappfactory.com/generative-ai-solutions?utm_source=medium&utm_medium=blog&utm_campaign=elavarasan)éœ€è¦éµå¾ªæœ€ä½³å®žè·µï¼Œä»¥ç¡®ä¿é•¿æœŸæˆåŠŸã€‚ä»¥ä¸‹æ˜¯å¸Œæœ›æœ€å¤§åŒ–äººå·¥æ™ºèƒ½é©±åŠ¨çš„å®¢æˆ·æœåŠ¡å½±å“çš„ä¼ä¸šçš„ä¸€äº›æœ€ä½³å®žè·µã€‚\n\n**1\\. ä¼˜å…ˆè€ƒè™‘å®‰å…¨æ€§å’Œåˆè§„æ€§**\n\nå®¢æˆ·æœåŠ¡é€šå¸¸æ¶‰åŠå¤„ç†æ•æ„Ÿä¿¡æ¯ï¼Œä¾‹å¦‚æ”¯ä»˜ç»†èŠ‚ã€è´¦æˆ·ä¿¡æ¯å’Œä¸ªäººæ•°æ®ã€‚ç¡®ä¿æ‚¨çš„äººå·¥æ™ºèƒ½è§£å†³æ–¹æ¡ˆç¬¦åˆæ•°æ®ä¿æŠ¤æ³•è§„ï¼Œå¦‚é€šç”¨æ•°æ®ä¿æŠ¤æ¡ä¾‹ï¼ˆGDPRï¼‰æˆ–åŠ åˆ©ç¦å°¼äºšæ¶ˆè´¹è€…éšç§æ³•ï¼ˆCCPAï¼‰ã€‚å®žæ–½å¼ºå¤§çš„å®‰å…¨åè®®ä»¥åŠ å¯†å®¢æˆ·æ•°æ®å¹¶é˜²æ­¢æ•°æ®æ³„éœ²ã€‚\n\n**2\\. åœ¨è‡ªåŠ¨åŒ–ä¸Žäººå·¥äº’åŠ¨ä¹‹é—´å–å¾—å¹³è¡¡**\n\nè™½ç„¶å¯¹è¯å¼äººå·¥æ™ºèƒ½å¯ä»¥å¤„ç†å„ç§å®¢æˆ·æŸ¥è¯¢ï¼Œä½†åœ¨è‡ªåŠ¨åŒ–å’Œäººå·¥æ”¯æŒä¹‹é—´å–å¾—å¹³è¡¡è‡³å…³é‡è¦ã€‚åœ¨äººå·¥æ™ºèƒ½æ— æ³•è§£å†³å¤æ‚é—®é¢˜æˆ–å®¢æˆ·éœ€è¦æ›´ä¸ªæ€§åŒ–çš„æ”¯æŒæ—¶ï¼Œç¡®ä¿äººå·¥æ™ºèƒ½ä¸Žäººå·¥å®¢æœä¹‹é—´çš„æ— ç¼è¿‡æ¸¡ã€‚è¿™ä¸ªæ··åˆæ¨¡åž‹ä½¿å®¢æˆ·èƒ½å¤ŸèŽ·å¾—ä¸¤è€…çš„ä¼˜ç‚¹ï¼šäººå·¥æ™ºèƒ½çš„å¿«é€Ÿå“åº”å’Œäººå·¥å®¢æœçš„åŒç†å¿ƒã€‚\n\n**3\\. æµ‹è¯•å’Œä¼˜åŒ–äººå·¥æ™ºèƒ½ç³»ç»Ÿ**\n\nåœ¨å°†äººå·¥æ™ºèƒ½ç³»ç»Ÿå…¬å¼€å‘å¸ƒä¹‹å‰ï¼Œè¿›è¡Œå¹¿æ³›çš„æµ‹è¯•ä»¥ç¡®ä¿å…¶è¾¾åˆ°é¢„æœŸçš„æ€§èƒ½åŸºå‡†ã€‚æµ‹è¯•äººå·¥æ™ºèƒ½ç†è§£å„ç§è¯­è¨€ã€å£éŸ³å’ŒæŸ¥è¯¢ç»“æž„çš„èƒ½åŠ›ã€‚ä½¿ç”¨A/Bæµ‹è¯•è¯„ä¼°ä¸åŒçš„å¯¹è¯æµç¨‹ï¼Œå¹¶æ ¹æ®å®¢æˆ·åé¦ˆå’Œæ€§èƒ½æ•°æ®è¿›è¡Œä¼˜åŒ–ã€‚\n\n**4\\. ç”¨äººå·¥æ™ºèƒ½ä¸ªæ€§åŒ–äº’åŠ¨**\n\nå®¢æˆ·æ¬£èµæ„Ÿè§‰ä¸ªæ€§åŒ–å’Œç›¸å…³çš„äº’åŠ¨ã€‚é€šè¿‡åˆ©ç”¨äººå·¥æ™ºèƒ½åˆ†æžå®¢æˆ·æ•°æ®çš„èƒ½åŠ›ï¼Œä¼ä¸šå¯ä»¥æ ¹æ®å®¢æˆ·çš„åŽ†å²ã€åå¥½å’Œè¡Œä¸ºå®šåˆ¶å“åº”ã€‚ä¸ªæ€§åŒ–ä¸ä»…ä»…æ˜¯ç§°å‘¼å®¢æˆ·çš„åå­—â€”â€”è¿˜æ¶‰åŠé¢„æµ‹ä»–ä»¬çš„éœ€æ±‚å¹¶æä¾›å¢žå€¼çš„ä¸»åŠ¨è§£å†³æ–¹æ¡ˆã€‚\n\n**5\\. çº³å…¥å¤šè¯­è¨€èƒ½åŠ›**\n\nå¯¹äºŽæ‹¥æœ‰å…¨çƒå®¢æˆ·åŸºç¡€çš„ä¼ä¸šï¼Œå¼€å‘æ”¯æŒå¤šç§è¯­è¨€çš„å¯¹è¯å¼äººå·¥æ™ºèƒ½è‡³å…³é‡è¦ã€‚å¤šè¯­è¨€äººå·¥æ™ºèƒ½ç³»ç»Ÿå…è®¸å®¢æˆ·ä»¥ä»–ä»¬åå¥½çš„è¯­è¨€ä¸Žä¼ä¸šäº’åŠ¨ï¼Œæé«˜å¯åŠæ€§å¹¶æ‰©å¤§å®¢æˆ·æœåŠ¡è¿è¥çš„è¦†ç›–èŒƒå›´ã€‚\n\n**6\\. ä¸ºå®¢æˆ·è®¾å®šæ˜Žç¡®çš„æœŸæœ›**\n\nä¸ºäº†é˜²æ­¢å®¢æˆ·æ„Ÿåˆ°æ²®ä¸§ï¼Œæ˜Žç¡®å‘ŠçŸ¥å®¢æˆ·äººå·¥æ™ºèƒ½å¯ä»¥å’Œä¸èƒ½åšä»€ä¹ˆã€‚å¦‚æžœäººå·¥æ™ºèƒ½ä»…é™äºŽæŸäº›ä»»åŠ¡ï¼ˆå¦‚æä¾›è®¢å•æ›´æ–°æˆ–å›žç­”å¸¸è§é—®é¢˜ï¼‰ï¼Œè¯·ä»Žäº’åŠ¨å¼€å§‹å°±æ˜Žç¡®è¿™ä¸€ç‚¹ã€‚è¿™ç§é€æ˜Žåº¦æœ‰åŠ©äºŽç®¡ç†å®¢æˆ·æœŸæœ›ï¼Œå¹¶åœ¨è¿‡æ¸¡åˆ°äººå·¥å®¢æœå¤„ç†æ›´å¤æ‚é—®é¢˜æ—¶é˜²æ­¢æ··æ·†ã€‚\n\n## çŽ°å®žä¸–ç•Œä¸­å¯¹è¯å¼äººå·¥æ™ºèƒ½åœ¨å®¢æˆ·æœåŠ¡ä¸­çš„åº”ç”¨å®žä¾‹\n\nå¤šä¸ªå…¬å¸å·²ç»å®žæ–½äº†å¯¹è¯å¼äººå·¥æ™ºèƒ½è§£å†³æ–¹æ¡ˆï¼Œä»¥å¢žå¼ºä»–ä»¬çš„å®¢æˆ·æœåŠ¡è¿è¥ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å±•ç¤ºäººå·¥æ™ºèƒ½å¯¹å®¢æˆ·æœåŠ¡æˆåŠŸå½±å“çš„çŽ°å®žä¸–ç•Œä¾‹å­ã€‚\n\n**1\\. H\\&Mçš„å®¢æˆ·æœåŠ¡èŠå¤©æœºå™¨äºº**\n\nå…¨çƒæ—¶å°šé›¶å”®å•† *H\\&M* ä½¿ç”¨ä¸€ä¸ªç”±äººå·¥æ™ºèƒ½é©±åŠ¨çš„èŠå¤©æœºå™¨äººï¼Œå¸®åŠ©å®¢æˆ·å¤„ç†ä¸Žè®¢å•çŠ¶æ€ã€äº§å“å¯ç”¨æ€§å’Œé€€è´§æ”¿ç­–ç›¸å…³çš„å¸¸è§æŸ¥è¯¢ã€‚è¯¥èŠå¤©æœºå™¨äººé€šè¿‡è‡ªåŠ¨åŒ–é‡å¤ä»»åŠ¡ï¼Œå‡è½»äº†å®¢æˆ·æœåŠ¡ä»£ç†çš„è´Ÿæ‹…ï¼Œä½¿äººç±»ä»£ç†èƒ½å¤Ÿä¸“æ³¨äºŽæ›´å¤æ‚çš„è¯¢é—®ã€‚H\\&Mçš„èŠå¤©æœºå™¨äººæ”¹å–„äº†å®¢æˆ·å“åº”æ—¶é—´ï¼Œå¹¶æé«˜äº†æ•´ä½“æ»¡æ„åº¦ã€‚\n\n**2\\. Sephoraçš„è™šæ‹Ÿç¾Žå®¹é¡¾é—®**\n\nåŒ–å¦†å“é›¶å”®å•† *Sephora* æä¾›ä¸€ä¸ªç”±å¯¹è¯å¼äººå·¥æ™ºèƒ½é©±åŠ¨çš„ **è™šæ‹Ÿç¾Žå®¹é¡¾é—®**ï¼Œæä¾›ä¸ªæ€§åŒ–çš„äº§å“æŽ¨èå’Œç¾Žå®¹æŠ€å·§ã€‚è¯¥äººå·¥æ™ºèƒ½åŠ©æ‰‹ä¸Žå®¢æˆ·è¿›è¡Œäº’åŠ¨å¯¹è¯ï¼Œè¯¢é—®ä»–ä»¬çš„åå¥½å¹¶æŽ¨èç¬¦åˆå…¶éœ€æ±‚çš„äº§å“ã€‚é€šè¿‡æä¾›é«˜åº¦ä¸ªæ€§åŒ–çš„ä½“éªŒï¼ŒSephora æé«˜äº†å®¢æˆ·å‚ä¸Žåº¦å¹¶å¢žåŠ äº†åœ¨çº¿é”€å”®ã€‚\n\n**3\\. Amtrakçš„å®¢æˆ·æœåŠ¡èŠå¤©æœºå™¨äººï¼ŒJulie**\n\n*Amtrak* å®žæ–½äº† **Julie**ï¼Œä¸€ä¸ªç”±äººå·¥æ™ºèƒ½é©±åŠ¨çš„è™šæ‹ŸåŠ©æ‰‹ï¼Œå¸®åŠ©å®¢æˆ·é¢„è®¢è½¦ç¥¨ã€æŸ¥è¯¢ç«è½¦æ—¶åˆ»è¡¨å’ŒèŽ·å–æ—…è¡Œæ›´æ–°ã€‚Julie æ¯å¹´å¤„ç†è¶…è¿‡ **500ä¸‡æ¬¡æŸ¥è¯¢**ï¼Œå‡å°‘äº†å‘¼å«ä¸­å¿ƒçš„å·¥ä½œé‡ï¼Œæé«˜äº† Amtrak å®¢æˆ·æœåŠ¡è¿è¥çš„æ•´ä½“æ•ˆçŽ‡ã€‚Julie çš„æˆåŠŸä½¿ Amtrak èƒ½å¤Ÿé™ä½Žæˆæœ¬ï¼Œå¹¶ä¸ºå®¢æˆ·æä¾›æ›´å¿«é€Ÿçš„æœåŠ¡ã€‚\n\n## å®¢æˆ·æœåŠ¡ä¸­å¯¹è¯å¼äººå·¥æ™ºèƒ½çš„æœªæ¥\n\nåœ¨æœªæ¥å‡ å¹´ï¼Œå®¢æˆ·æœåŠ¡ä¸­å¯¹è¯å¼äººå·¥æ™ºèƒ½çš„é‡‡ç”¨é¢„è®¡å°†æŒç»­å¢žé•¿ï¼Œå› ä¸ºä¼ä¸šå°†ç»§ç»­ä¼˜å…ˆè€ƒè™‘è‡ªåŠ¨åŒ–ã€æ•ˆçŽ‡å’Œå®¢æˆ·æ»¡æ„åº¦ã€‚åˆ°2030å¹´ï¼Œé¢„è®¡**70%çš„å®¢æˆ·äº’åŠ¨**å°†æ¶‰åŠæŸç§å½¢å¼çš„äººå·¥æ™ºèƒ½æŠ€æœ¯ã€‚è¿™ç§å‘äººå·¥æ™ºèƒ½é©±åŠ¨çš„å®¢æˆ·æœåŠ¡çš„è½¬å˜å°†ä½¿ä¼ä¸šèƒ½å¤Ÿä¸ºå…¨çƒæ•°ç™¾ä¸‡å®¢æˆ·æä¾›ä¸ªæ€§åŒ–çš„å®žæ—¶æ”¯æŒã€‚\n\néšç€æŠ€æœ¯çš„å‘å±•ï¼Œå¯¹è¯å¼äººå·¥æ™ºèƒ½å°†å˜å¾—æ›´åŠ æ™ºèƒ½ï¼Œèƒ½å¤Ÿå¤„ç†å¤æ‚çš„è¯¢é—®ã€ç†è§£æƒ…æ„Ÿå¹¶æä¾›ä¸»åŠ¨è§£å†³æ–¹æ¡ˆã€‚ä»Šå¤©æŠ•èµ„äºŽå¯¹è¯å¼äººå·¥æ™ºèƒ½çš„ä¼ä¸šå°†åœ¨æœªæ¥æ›´å¥½åœ°æ»¡è¶³å®¢æˆ·æ—¥ç›Šå¢žé•¿çš„éœ€æ±‚ã€‚\n\n## ç»“è®º\n\nå¯¹è¯å¼äººå·¥æ™ºèƒ½å·²ç»æ”¹å˜äº†ä¼ä¸šä¸Žå®¢æˆ·äº’åŠ¨çš„æ–¹å¼ï¼Œæä¾›äº†æ›´å¿«é€Ÿã€æ›´ä¸ªæ€§åŒ–å’Œæ›´é«˜æ•ˆçš„æ”¯æŒã€‚å¯¹äºŽå¸Œæœ›å¼€å‘åŸºäºŽäººå·¥æ™ºèƒ½çš„å®¢æˆ·æœåŠ¡è§£å†³æ–¹æ¡ˆçš„ä¼ä¸šè€Œè¨€ï¼Œæœºä¼šæ˜¯å·¨å¤§çš„ã€‚é€šè¿‡éµå¾ªæœ€ä½³å®žè·µå¹¶å®žæ–½å…³é”®æ­¥éª¤ï¼Œä¾‹å¦‚è¯†åˆ«ç”¨ä¾‹ã€é€‰æ‹©åˆé€‚çš„å¹³å°ä»¥åŠæŒç»­ä¼˜åŒ–äººå·¥æ™ºèƒ½ç³»ç»Ÿï¼Œä¼ä¸šå¯ä»¥èŽ·å¾—æˆåŠŸå¹¶æŽ¨åŠ¨å®¢æˆ·æœåŠ¡çš„åˆ›æ–°ã€‚\n\næŠ•èµ„å¯¹è¯å¼äººå·¥æ™ºèƒ½ä¸ä»…æ˜¯æ”¹å–„å®¢æˆ·æœåŠ¡çš„æˆ˜ç•¥ä¸¾æŽªâ€”â€”å®ƒæ˜¯æœªæ¥ä¿éšœå®¢æˆ·æ”¯æŒè¿è¥çš„å¿…è¦æ­¥éª¤ã€‚é€šè¿‡æå‡å®¢æˆ·ä½“éªŒã€é™ä½Žæˆæœ¬å’Œæä¾›å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œå¯¹è¯å¼äººå·¥æ™ºèƒ½ä»£è¡¨äº†å®¢æˆ·æœåŠ¡çš„æœªæ¥ã€‚\n\n"},{"lang":"zh","group":"blog","slug":"blog/explore-swarm-multi-agent-framework-locally-0e25ee617795","frontmatter":{"title":"æœ¬åœ°æŽ¢ç´¢ Swarm å¤šæ™ºèƒ½ä½“æ¡†æž¶","meta_title":"æœ¬åœ°æŽ¢ç´¢ Swarm å¤šæ™ºèƒ½ä½“æ¡†æž¶","description":"Swarm æ˜¯ä¸€ä¸ªå®žéªŒæ€§ç¤ºä¾‹æ¡†æž¶ï¼Œç”¨äºŽæ¨¡æ‹Ÿè½»é‡çº§å¤šä»£ç†æ¡†æž¶ï¼Œç”¨äºŽæ•™è‚²ç›®çš„ã€‚é€šå¸¸å®ƒä¸Ž Openâ€¦ é…åˆä½¿ç”¨","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*0ZVceq32bvkytC7HSIgmwA.png","categories":["Programming","Technology","Education"],"author":"Rifx.Online","tags":["Swarm","Multi-Agent","Framework","OpenAI","Ollama"],"draft":false,"slug":"blog/explore-swarm-multi-agent-framework-locally-0e25ee617795"},"content":"\n\n\n\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*zkpW8DDwh0TTYuHJVJbDaw.png)\n\nSwarm æ˜¯ä¸€ä¸ªå®žéªŒæ€§æ ·æœ¬æ¡†æž¶ï¼Œç”¨äºŽæ¨¡æ‹Ÿè½»é‡çº§å¤šæ™ºèƒ½ä½“æ¡†æž¶ï¼Œæ—¨åœ¨æ•™è‚²ç›®çš„ã€‚é€šå¸¸å®ƒä¸Ž Open AI Key ä¸€èµ·ä½¿ç”¨ï¼Œä½†æˆ‘ä»¬å¯ä»¥æ›´æ”¹ä¸ºä½¿ç”¨æœ¬åœ°çš„ Ollama æˆ– LM Studio æ¨¡åž‹ã€‚\n\n**è®¾ç½®ï¼š**\n\n\n```python\n## åˆ›å»ºä¸€ä¸ªæ–°çš„ Conda æˆ– Python è™šæ‹ŸçŽ¯å¢ƒå¹¶æ¿€æ´»å®ƒ\nconda install python==3.10\npip install torch openai\npip install transformers accelerate huggingface_hub\npip install git+ssh://git@github.com/openai/swarm.git\n```\n**ä½¿ç”¨ Open AI Keyï¼š**\n\n\n```python\nexport OPEN_API_KEY = Your Key\n```\n**ä½¿ç”¨ Ollama æˆ– LM Studio æœ¬åœ° LLM â€” æ›´æ–°ä¸ºæœ¬åœ° URLï¼š**\n\n\n```python\n## æŸ¥æ‰¾ conda æˆ– python è™šæ‹ŸçŽ¯å¢ƒä¸­çš„ site-packages/swarm\n## æ‰¾åˆ°æ–‡ä»¶ core.py\nclass Swarm:\n    def __init__(self, client=None):\n        if not client:\n          # å®žé™…ä»£ç \n          #client = OpenAI()\n          # å°†åŸºç¡€ URL å’Œ API Key æ›´æ–°ä¸º Ollama / LM Studio\n          # åœ¨æœ¬æ¼”ç¤ºä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ LM Studio å’Œ Llama 3.1\n          client = OpenAI(base_url=\"http://localhost:1234/v1\",api_key=\"random\")\n        self.client = client\n```\n**å…‹éš†ä»“åº“ï¼š**\n\nå…‹éš†ä»“åº“ â€” åœ¨è¿™é‡Œæ‚¨å¯ä»¥æ‰¾åˆ°ä¸åŒç”¨ä¾‹çš„ç¤ºä¾‹ç›®å½•ï¼Œå¦‚åŸºæœ¬ã€èˆªç©ºå…¬å¸å’Œå¤©æ°”ç­‰ã€‚\n\n\n```python\ngit clone https://github.com/openai/swarm.git\ncd swarm/examples\n```\n**ç¤ºä¾‹ä»£ç ï¼š**\n\n\n```python\nfrom swarm import Swarm, Agent\n\nclient = Swarm()\n\n\nit_agent = Agent(\n    name=\"IT Agent\",\n    instructions=\"You are an IT Expert with 10 Years of Experience.\",\n)\n\nsales_agent = Agent(\n    name=\"Sales Agent\",\n    instructions=\"You are a Sales Expert with 5 Years of Experience and knows about best selling mobiles.\",\n)\n\ndef transfer_to_sales_agent():\n    print(\"Sales agent in action\")\n    \"\"\"Transfer sales related questions to sales team immediately.\"\"\"\n    return sales_agent\n\ndef transfer_to_it_agent():\n    print(\"IT agent in action\")\n    \"\"\"Transfer IT users immediately.\"\"\"\n    return it_agent\n\nenglish_agent = Agent(\n    name=\"English Agent\",\n    instructions=\"You only speak English.\",\n    functions=[transfer_to_sales_agent,transfer_to_it_agent],\n)\n\n\nmessages = [{\"role\": \"user\", \"content\": \"How to install pandas lib?\"}]\nresponse = client.run(agent=english_agent, messages=messages)\n\nprint(response.messages[-1][\"content\"])\n\nmessages = [{\"role\": \"user\", \"content\": \"What are the best selling items?\"}]\nresponse = client.run(agent=english_agent, messages=messages)\n\nprint(response.messages[-1][\"content\"])\n```\n**å‚è€ƒæ–‡çŒ®ï¼š**\n\n\n```python\nhttps://github.com/openai/swarm\n\nhttps://github.com/victorb/ollama-swarm/tree/main\n```\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*hCFJ4VQoT12yElYPXwXvWA.png)\n\né‰´äºŽè¿™æ˜¯ä¸€ä¸ªå®žéªŒæ€§ç‰ˆæœ¬ï¼Œä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚èˆªç©ºä»£ç†ç¤ºä¾‹ä»£ç  [swarm/examples/airline] éžå¸¸æœ‰è¶£ï¼Œå› æ­¤å¯ä»¥å°è¯•è¿™äº›ç¤ºä¾‹ã€‚è¯•è¯•çœ‹ï¼Œå¹¶åœ¨è¯„è®ºä¸­åˆ†äº«æ‚¨çš„ç»éªŒã€‚è°¢è°¢ã€‚\n\n"},{"lang":"zh","group":"blog","slug":"blog/fine-tuning-llama-3-with-unsloth-79c3465ef3e3","frontmatter":{"title":"ä½¿ç”¨ Unsloth å¯¹ LLama 3 è¿›è¡Œå¾®è°ƒ","meta_title":"ä½¿ç”¨ Unsloth å¯¹ LLama 3 è¿›è¡Œå¾®è°ƒ","description":"åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä½¿ç”¨ Unsloth å¯¹ LLMï¼ˆæ¥è‡ª Meta çš„ Llama 3ï¼‰è¿›è¡Œå¾®è°ƒï¼ˆåŒ…æ‹¬è‡ªå®šä¹‰æ•°æ®é›†çš„æ–¹æ³•ï¼‰","date":"2024-10-30T12:58:41.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kaXoudNTGeGfuNPl_kta5g.jpeg","categories":["Programming","Machine Learning","Natural Language Processing"],"author":"Rifx.Online","tags":["Llama","Unsloth","LoRA","Alpaca","NVIDIA"],"draft":false,"slug":"blog/fine-tuning-llama-3-with-unsloth-79c3465ef3e3"},"content":"\n\n\nåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä½¿ç”¨ [Unsloth](https://github.com/unslothai/unsloth) å¾®è°ƒ LLMï¼ˆMeta çš„ Llama 3ï¼‰ã€‚æˆ‘è¿˜å°†æä¾›ä½¿ç”¨æ‚¨è‡ªå·±è‡ªå®šä¹‰æ•°æ®é›†çš„æ–¹æ³•ã€‚\n\n**æ³¨æ„ï¼š** Unsloth æ˜¯ä¸€ä¸ªåŠ é€Ÿ LLM åœ¨ NVIDIA GPU ä¸Šå¾®è°ƒçš„åº“ï¼ˆä¸Žä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œå†…å­˜ä½¿ç”¨å‡å°‘ 40%ï¼‰ã€‚ä¸Ž Hugging Face å…¼å®¹ï¼Œæ”¯æŒ Llama å’Œ Mistral æž¶æž„ã€‚\n\nå¦‚æžœæ‚¨è§‰å¾—æˆ‘çš„æ–‡ç« æœ‰è¶£ï¼Œè¯·ä¸è¦å¿˜è®° **ç‚¹èµžå¹¶ [å…³æ³¨](https://medium.com/@soulawalid)** ðŸ‘ðŸ¼ï¼Œå†™è¿™äº›æ–‡ç« éœ€è¦æ—¶é—´å’Œç²¾åŠ›ï¼\n\næ‚¨å¯ä»¥è®¿é—® GitHub ä»“åº“ä¸­æä¾›çš„å…è´¹ç¬”è®°æœ¬ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*_L4o4MDQ7W5__OwW0E5RWA.png)\n\nç”±äºŽæˆ‘ä½¿ç”¨çš„æ˜¯ Llama 3ï¼Œå› æ­¤æˆ‘å°†ç‚¹å‡»ç¬”è®°æœ¬ï¼ˆæ‚¨ä¹Ÿå¯ä»¥åœ¨è‡ªå·±çš„è®¡ç®—æœºä¸Šå®‰è£… Unslothï¼‰ã€‚\n\n**æ³¨æ„ï¼š** æˆ‘å°†ä½¿ç”¨è¿™ä¸ªæ•°æ®é›† â€œ[alpaca\\-cleaned](https://huggingface.co/datasets/yahma/alpaca-cleaned)â€ æ¥è‡ª Hugging Faceï¼Œæ•°æ®é‡‡ç”¨ Alpaca æ ¼å¼ï¼Œå³åŒ…å«ï¼ˆæŒ‡ä»¤ã€è¾“å…¥å’Œè¾“å‡ºï¼‰ã€‚\n\n### å¼€å§‹é¡¹ç›®\n\nåœ¨é¡¹ç›®ä¸­ï¼Œæˆ‘å°†æŒ‡å¯¼æ‚¨ä½¿ç”¨ Unsloth è¿›è¡Œå¾®è°ƒï¼Œè§£é‡Šä»£ç å¹¶æä¾›å»ºè®®ï¼Œè®©æˆ‘ä»¬å¼€å§‹æˆ‘ä»¬çš„é¡¹ç›®ï¼š\n\n**1/ å®‰è£…æ‰€éœ€çš„åŒ…ï¼š** æˆ‘ä»¬é¦–å…ˆéœ€è¦å®‰è£… **Unsloth** å’Œ **xformers**ã€**trl**ã€**peft**ã€**accelerate**ã€**bitsandbytes** åº“ï¼Œä»¥ä¾¿è¿›è¡Œé«˜æ•ˆçš„æ¨¡åž‹è®­ç»ƒå’ŒæŽ¨ç†ã€‚\n\n```python\n!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n!pip install --no-deps xformers trl peft accelerate bitsandbytes\n```\n\n**2/ åŠ è½½å’Œé…ç½®æ¨¡åž‹ï¼š** åœ¨é…ç½®ä¸­ï¼Œæˆ‘å°†è®¾ç½®ä»¥ä¸‹å†…å®¹ï¼š\n\n* å°†æœ€å¤§åºåˆ—é•¿åº¦è®¾ç½®ä¸º **2048**\n* å°† dtype è®¾ç½®ä¸º **None**ï¼Œå®ƒä¼šè‡ªåŠ¨æ£€æµ‹æ•°æ®ç±»åž‹ã€‚\n* ä»¥ **4-ä½ç²¾åº¦**åŠ è½½æ¨¡åž‹ï¼Œæˆ‘è®¤ä¸ºè¿™å·²ç»è¶³å¤Ÿã€‚\n\n**æ³¨æ„ï¼š** æ‚¨å¯ä»¥åœ¨èµ„æºéƒ¨åˆ†æ‰¾åˆ°æˆ‘å…³äºŽå¾®è°ƒ LLM çš„æŠ€å·§çš„æ–‡ç« ã€‚\n\n```python\nfrom unsloth import FastLanguageModel\nimport torch\n\n## é…ç½®\nmax_seq_length = 2048\ndtype = None\nload_in_4bit = True\n\n## åŠ è½½é€‰å®šçš„æ¨¡åž‹\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/llama-3-8b-bnb-4bit\",\n    max_seq_length=max_seq_length,\n    dtype=dtype,\n    load_in_4bit=load_in_4bit,\n)\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*cJSAcJFP7E-qJkqKUsHqLw.png)\n\n**3/ åº”ç”¨ PEFTï¼ˆå‚æ•°é«˜æ•ˆå¾®è°ƒï¼‰ï¼š** ç„¶åŽæˆ‘ä»¬å°†ä½¿ç”¨ LoRA å¯¹é¢„è®­ç»ƒæ¨¡åž‹è¿›è¡Œå¾®è°ƒã€‚\n\n* r = 16 æ˜¯ LoRA çš„ç§©å‚æ•°ã€‚**æ³¨æ„ï¼š** å¸¸è§å€¼ä¸º 8ã€16ã€32ã€64ã€128\n* lora_alpha = 16 ä»£è¡¨ LoRA æ›´æ–°çš„ç¼©æ”¾å› å­ï¼ˆæˆ‘å°†å†™ä¸€ç¯‡å…³äºŽ LoRA çš„æ–‡ç« ï¼Œä»¥è¯¦ç»†è§£é‡Šæ¯ä¸ªéƒ¨åˆ†ï¼‰\n* å¯¹äºŽ LoRA ä¸ä½¿ç”¨ dropout å’Œåç½®\n* å¯¹äºŽ use_gradient_checkpointingï¼Œæˆ‘ä»¬ä½¿ç”¨ Unsloth æ¥å¤„ç†ï¼ˆèŠ‚çœå†…å­˜ï¼‰\n\n```python\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 16,\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha = 16,\n    lora_dropout = 0,\n    bias = \"none\",\n    use_gradient_checkpointing = \"unsloth\",\n    random_state = 3407,\n    use_rslora = False,\n    loftq_config = None,\n)\n```\n\n**4/ å®šä¹‰æç¤ºæ¨¡æ¿ï¼š** æˆ‘ä»¬å°†åˆ›å»º alpaca æç¤ºæ¨¡æ¿ä»¥æ ¼å¼åŒ–æ•°æ®é›†ï¼ˆå¦‚æžœæ‚¨ä½¿ç”¨çš„æ•°æ®ä¸æ˜¯è¿™ç§æ ¼å¼ï¼‰ã€‚\n\næˆ‘ä»¬è¿˜å°†æ·»åŠ  EOSï¼ˆç»“æŸåºåˆ—ï¼‰ä»¥é€šçŸ¥ LLM å¥å­å·²ç»“æŸã€‚\n\næœ€åŽæ˜¯æ ¼å¼åŒ–å‡½æ•°ï¼Œè¯¥å‡½æ•°æŽ¥å—ä¸€æ‰¹ç¤ºä¾‹å¹¶æ ¹æ®æˆ‘ä»¬ä¹‹å‰ç¼–å†™çš„ alpaca æç¤ºæ¨¡æ¿æ ¼å¼åŒ–æ¯ä¸ªç¤ºä¾‹ã€‚\n\n* å®ƒä»Žæ¯ä¸ªç¤ºä¾‹ï¼ˆè¡Œï¼‰ä¸­æå–æŒ‡ä»¤ã€è¾“å…¥å’Œè¾“å‡ºå­—æ®µã€‚\n* ç„¶åŽå°†è¿™äº›å­—æ®µæ ¼å¼åŒ–åˆ°æ¨¡æ¿ä¸­å¹¶é™„åŠ  EOS æ ‡è®°ã€‚\n* æ ¼å¼åŒ–çš„æ–‡æœ¬å­˜å‚¨åœ¨åˆ—è¡¨ä¸­ï¼Œå¹¶ä½œä¸ºå…·æœ‰å•ä¸ªé”®â€œtextâ€çš„å­—å…¸è¿”å›žã€‚\n\n```python\nalpaca_prompt = \"\"\"ä»¥ä¸‹æ˜¯æè¿°ä»»åŠ¡çš„æŒ‡ä»¤ï¼Œé…æœ‰æä¾›è¿›ä¸€æ­¥ä¸Šä¸‹æ–‡çš„è¾“å…¥ã€‚å†™ä¸€ä¸ªé€‚å½“å®Œæˆè¯·æ±‚çš„å“åº”ã€‚\n\n#### æŒ‡ä»¤ï¼š\n{}\n\n#### è¾“å…¥ï¼š\n{}\n\n#### å“åº”ï¼š\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token\n\ndef formatting_prompts_func(examples):\n    instructions = examples[\"instruction\"]\n    inputs = examples[\"input\"]\n    outputs = examples[\"output\"]\n    texts = []\n    for instruction, input, output in zip(instructions, inputs, outputs):\n        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n        texts.append(text)\n    return {\"text\": texts}\n```\n\n**5/ åŠ è½½å’Œæ ¼å¼åŒ–æ•°æ®é›†ï¼š** åŠ è½½ Alpaca æ•°æ®é›†å¹¶å¯¹æ¯ä¸ªæ•°æ®é›†ç¤ºä¾‹åº”ç”¨æ ¼å¼åŒ–ã€‚\n\n```python\nfrom datasets import load_dataset\ndataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\ndataset = dataset.map(formatting_prompts_func, batched = True)\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*M8EmbLMdoqrM-JlkMpDv8g.png)\n\n**6/ è®¾ç½®å’Œè®­ç»ƒæ¨¡åž‹ï¼š** æˆ‘åœ¨æˆ‘[ä¹‹å‰çš„æ–‡ç« ](https://readmedium.com/supervised-fine-tuning-tips-for-your-llm-projects-f84f20593653)ä¸­æ¶µç›–äº†å¤§éƒ¨åˆ†å…³äºŽå¾®è°ƒçš„æŠ€å·§ã€‚\n\n```python\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2, # ç”¨äºŽæ•°æ®é¢„å¤„ç†çš„è¿›ç¨‹æ•°é‡\n    packing = False, # æ˜¯å¦å°†å¤šä¸ªåºåˆ—æ‰“åŒ…æˆä¸€ä¸ªæ‰¹æ¬¡ä»¥æé«˜è®­ç»ƒæ•ˆçŽ‡\n    args = TrainingArguments(\n        per_device_train_batch_size = 2, # æ¯ä¸ªè®¾å¤‡çš„æ‰¹æ¬¡å¤§å°\n        gradient_accumulation_steps = 4, # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼Œå…è®¸æœ‰æ•ˆå¢žå¤§æ‰¹æ¬¡å¤§å°\n        warmup_steps = 5, # è¿›è¡Œçº¿æ€§å­¦ä¹ çŽ‡é¢„çƒ­çš„æ­¥éª¤æ•°\n        max_steps = 60, # æ€»è®­ç»ƒæ­¥éª¤æ•°\n        learning_rate = 2e-5,# ä¼˜åŒ–å™¨çš„å­¦ä¹ çŽ‡\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"cosine\",\n        seed = 3407,\n        output_dir = \"outputs\",\n    ),\n)\n\ntrainer_stats = trainer.train()\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Vb_OqGP9CPc8xZdnkclGyQ.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*PI0JXrTbpjuviyQ4bZJnFg.png)\n\n**7/ æŽ¨ç†å’Œç”Ÿæˆï¼š** æˆ‘ä»¬é€šè¿‡å‡†å¤‡è¾“å…¥æç¤ºã€å¯¹å…¶è¿›è¡Œæ ‡è®°åŒ–ï¼Œç„¶åŽä½¿ç”¨æ¨¡åž‹æ ¹æ®è¯¥æç¤ºç”Ÿæˆæ–°æ–‡æœ¬æ¥å‡†å¤‡æ¨¡åž‹è¿›è¡ŒæŽ¨ç†ã€‚ç”Ÿæˆçš„æ–‡æœ¬éšåŽè¢«è½¬æ¢å›žå¯è¯»å½¢å¼ã€‚\n\n```python\nFastLanguageModel.for_inference(model)\ninputs = tokenizer(\n[\n    alpaca_prompt.format(\n        \"ç»§ç»­æ–æ³¢é‚£å¥‘æ•°åˆ—ã€‚\", # æŒ‡ä»¤\n        \"1, 1, 2, 3, 5, 8\", # è¾“å…¥\n        \"\", # è¾“å‡º - ç•™ç©ºä»¥è¿›è¡Œç”Ÿæˆï¼\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*PI6SBL_YPPj0-RSAn5nl7g.png)\n\næ‚¨è¿˜å¯ä»¥ä½¿ç”¨ TextStreamer è¿›è¡Œè¿žç»­æŽ¨ç†ï¼Œè¿™æ ·æ‚¨å¯ä»¥çœ‹åˆ°ç”Ÿæˆçš„æ¯ä¸ªæ ‡è®°ï¼Œè€Œä¸æ˜¯ä¸€ç›´ç­‰å¾…æ•´ä¸ªè¿‡ç¨‹ï¼\n\n```python\nFastLanguageModel.for_inference(model)\ninputs = tokenizer(\n[\n    alpaca_prompt.format(\n        \"ç»§ç»­æ–æ³¢é‚£å¥‘æ•°åˆ—ã€‚\",\n        \"1, 1, 2, 3, 5, 8\",\n        \"\",\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*NaSQ1vQKORU1I3DsOU2iOA.png)\n\n**8/ ä¿å­˜æ¨¡åž‹ï¼š** å¦‚æžœæ‚¨å¯¹æ­¤æ„Ÿåˆ°æ»¡æ„ï¼Œå¯ä»¥ä¿å­˜æ‚¨çš„æ¨¡åž‹æˆ–å°†å…¶æŽ¨é€åˆ° Hugging Face Hubã€‚\n\n```python\nmodel.save_pretrained(\"lora_model\")\ntokenizer.save_pretrained(\"lora_model\")\n## model.push_to_hub(\"your_name/lora_model\", token = \"...\")\n## tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\")\n```\n\n**9/ åŠ è½½æ¨¡åž‹ï¼š**\n\n```python\nif False:\n    from unsloth import FastLanguageModel\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name = \"lora_model\",\n        max_seq_length = max_seq_length,\n        dtype = dtype,\n        load_in_4bit = load_in_4bit,\n    )\n    FastLanguageModel.for_inference(model)\n```\n\n**10/ ç”¨äºŽç”Ÿæˆï¼š**\n\n```python\ninputs = tokenizer(\n[\n    alpaca_prompt.format(\n        \"å·´å‹’æ–¯å¦çš„é¦–éƒ½æ˜¯ä»€ä¹ˆï¼Ÿ\",\n        \"\",\n        \"\",\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)\n```\n\nå¦‚æžœæ‚¨æœ‰ç‰¹å®šä¸»é¢˜å¸Œæœ›æˆ‘ä»¬è®¨è®ºï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ï¼æ‚¨çš„åé¦ˆå°†æœ‰åŠ©äºŽå¡‘é€ æˆ‘çš„å†…å®¹æ–¹å‘ï¼Œç¡®ä¿å…¶ä¿æŒç›¸å…³æ€§å’Œå¸å¼•åŠ›ðŸ˜€\n\n\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/gemini-1-5-flash-vs-gpt-4o-88b9d8da8152","frontmatter":{"title":"å…¨æ–° Gemini 1.5 FLASH åž‹å·ï¼šç»å¯¹çš„ Google æ¸¸æˆè§„åˆ™æ”¹å˜è€…","meta_title":"å…¨æ–° Gemini 1.5 FLASH åž‹å·ï¼šç»å¯¹çš„ Google æ¸¸æˆè§„åˆ™æ”¹å˜è€…","description":"Gemini 1.5 Flash å®Œèƒœ GPT-4o","date":"2024-11-08T00:27:31.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Reb1owOmiw5DFd4A.png","categories":["Programming","Machine Learning","Generative AI"],"author":"Rifx.Online","tags":["Gemini","Flash","GPT-4o","multi-modality","creativity"],"draft":false,"slug":"blog/gemini-1-5-flash-vs-gpt-4o-88b9d8da8152"},"content":"\nä»–ä»¬çš„æ–° Gemini 1\\.5 Flash æ¨¡åž‹è¿œè¿œè¶…è¿‡äº† GPT\\-4oï¼Œå…¶èƒ½åŠ›ä»¤äººéš¾ä»¥ç½®ä¿¡ã€‚\n\n**é—ªç”µèˆ¬å¿«é€Ÿ**ã€‚\n\n\n\næ¯” GPT\\-4o ä¾¿å®œ 33 å€ï¼Œä½†ä¸Šä¸‹æ–‡å®¹é‡å¤§ 700% â€” **100 ä¸‡ä¸ªä»¤ç‰Œã€‚**\n\nåœ¨çŽ°å®žä¸–ç•Œä¸­ï¼Œ100 ä¸‡ä¸ªä»¤ç‰Œæ˜¯ä»€ä¹ˆæ¦‚å¿µï¼Ÿå¤§çº¦ï¼š\n\n* è¶…è¿‡ 1 å°æ—¶çš„è§†é¢‘\n* è¶…è¿‡ 30,000 è¡Œä»£ç \n* è¶…è¿‡ 700,000 ä¸ªå•è¯\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*E1XIOcpWfeqOZSZC.jpg)\n\nâŒGPT\\-4o æˆæœ¬ï¼š\n\n* è¾“å…¥ï¼šæ¯ç™¾ä¸‡ä¸ªä»¤ç‰Œ $2\\.50\n* è¾“å‡ºï¼šæ¯ç™¾ä¸‡ä¸ªä»¤ç‰Œ $10\n* ç¼“å­˜è¾“å…¥ï¼šæ¯ç™¾ä¸‡ä¸ªä»¤ç‰Œ $1\\.25\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*XM3hFyS_PCcuv8Px.png)\n\nâœ… Gemini 1\\.5 Flash æˆæœ¬ï¼š\n\n* è¾“å…¥ï¼šæ¯ç™¾ä¸‡ä¸ªä»¤ç‰Œ $0\\.075\n* è¾“å‡ºï¼šæ¯ç™¾ä¸‡ä¸ªä»¤ç‰Œ $0\\.30\n* ç¼“å­˜è¾“å…¥ï¼šæ¯ç™¾ä¸‡ä¸ªä»¤ç‰Œ $0\\.01875\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*d-1ioFlCxW3LB4SL.png)\n\nè¿˜æœ‰ç”¨äºŽæˆæœ¬æ•ˆç›Šä»»åŠ¡çš„ mini Flash\\-8B ç‰ˆæœ¬ â€” æ¯” GPT\\-4o ä¾¿å®œ 66 å€ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*5B5ybLzTr7penwms.png)\n\næœ€æ£’çš„æ˜¯å…¶å¤šæ¨¡æ€æ€§ â€” å®ƒå¯ä»¥ä»¥å¤æ‚çš„é›†æˆæ–¹å¼å¯¹æ–‡æœ¬ã€æ–‡ä»¶ã€å›¾åƒå’ŒéŸ³é¢‘è¿›è¡ŒæŽ¨ç†ã€‚\n\nè€Œ 1\\.5 Flash å‡ ä¹Žå…·å¤‡ Pro çš„æ‰€æœ‰èƒ½åŠ›ï¼Œä½†é€Ÿåº¦æ›´å¿«ã€‚ä½œä¸ºå¼€å‘è€…ï¼Œä½ çŽ°åœ¨å°±å¯ä»¥å¼€å§‹ä½¿ç”¨å®ƒä»¬ã€‚\n\nGemini 1\\.5 Pro åœ¨ä¸€éƒ¨ 44 åˆ†é’Ÿçš„æ— å£°ç”µå½±ä¸­è¿›è¡Œäº†æµ‹è¯•ï¼Œä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå®ƒè½»æ¾å°†ç”µå½±åˆ†æžæˆå„ç§æƒ…èŠ‚å’Œäº‹ä»¶ï¼Œç”šè‡³æŒ‡å‡ºå¤§å¤šæ•°äººåœ¨ç¬¬ä¸€æ¬¡è§‚çœ‹æ—¶ä¼šé”™è¿‡çš„å°ç»†èŠ‚ã€‚\n\nä¸Žæ­¤åŒæ—¶ï¼ŒGPT\\-4o API ä»…å…è®¸ä½ å¤„ç†æ–‡æœ¬å’Œå›¾åƒã€‚\n\nä½ å¯ä»¥åœ¨è°·æ­Œçš„ AI Studio ä¸­è½»æ¾åˆ›å»ºã€æµ‹è¯•å’Œå®Œå–„æç¤º â€” **å®Œå…¨å…è´¹**ã€‚\n\nè¿™ä¸ä¼šåƒåœ¨ OpenAI playground ä¸­é‚£æ ·è®¡å…¥ä½ çš„è´¦å•ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*5BKejWrJvrsEWIjc.png)\n\nçœ‹çœ‹è°·æ­Œ AI Studio çš„å¼ºå¤§åŠŸèƒ½ â€” æ ¹æ®å›¾åƒåˆ›å»ºé£Ÿè°±ï¼š\n\næˆ‘ä¸Šä¼ äº†è¿™å¼ æ¥è‡ª gettyimages çš„ç¾Žå‘³é¢åŒ…ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*fC5YL_dplJ9Od_vN.jpg)\n\nçŽ°åœ¨ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*GezbFh9KzFXRVhr3.png)\n\nå¦‚æžœæˆ‘æƒ³è¦å“åº”ä»¥æˆ‘çš„ API æˆ–å…¶ä»–å†…å®¹çš„ç‰¹å®šæ ¼å¼å‘¢ï¼Ÿ\n\né‚£ä¹ˆä½ å¯ä»¥æ‰“å¼€ JSON æ¨¡å¼å¹¶æŒ‡å®šå“åº”æ¨¡å¼ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*aRZuia7Iz_mI2s9b.png)\n\nOpenAI playground ä¹Ÿæœ‰è¿™ä¸ªï¼Œä½†ä½¿ç”¨èµ·æ¥ä¸å¦‚å®ƒç›´è§‚ã€‚\n\nGemini ç›¸è¾ƒäºŽ OpenAI çš„å¦ä¸€ä¸ªå‡çº§æ˜¯å®ƒçš„åˆ›é€ åŠ›ã€‚\n\nåœ¨ Gemini ä¸­ï¼Œä½ å¯ä»¥å°† `temperature` ä»Ž 0 å¢žåŠ åˆ° 200% æ¥æŽ§åˆ¶å“åº”çš„éšæœºæ€§å’Œåˆ›é€ æ€§ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*4AAFdAMfT_xyflmv.png)\n\nè€Œåœ¨ OpenAI ä¸­ï¼Œå¦‚æžœä½ å°è¯•è¶…è¿‡ 100%ï¼Œä½ å¾ˆå¯èƒ½ä¼šå¾—åˆ°ä¸€å †å®Œå…¨æ— æ„ä¹‰çš„å†…å®¹ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*yzFQL69pyJmgE9UB.png)\n\nè€Œä¸”æœ€æ£’çš„æ˜¯ â€” å½“ä½ å®Œæˆåˆ›å»ºæç¤ºåŽï¼Œä½ å¯ä»¥ç›´æŽ¥ä½¿ç”¨ **èŽ·å–ä»£ç ** â€” è½»æ¾å¤åˆ¶å¹¶ç²˜è´´æ¨¡æ¿ API ä»£ç ï¼Œå¿«é€Ÿè¿›å…¥å¼€å‘ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*xgaZfVe9b8WSBMmq.png)\n\næ”¯æŒåŒ…æ‹¬ Kotlinã€Swift å’Œ Dart åœ¨å†…çš„å¤šç§è¯­è¨€ â€” åœ¨ç§»åŠ¨å¼€å‘ä¸­å®žçŽ°é«˜æ•ˆçš„ AI å·¥ä½œæµç¨‹ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*AMkfKm-3KQRxnltO.png)\n\nåœ¨ OpenAI playground ä¸­ï¼Œä½ å¯ä»¥èŽ·å¾— Python å’Œ JavaScript çš„ä»£ç ã€‚\n\n## æœ€åŽçš„æ€è€ƒ\n\nGemini 1.5 Flash æ˜¯ä¸€æ¬¾é¢ è¦†æ€§çš„äº§å“ï¼Œä»¥æžä½Žçš„æˆæœ¬æä¾›æ— ä¸Žä¼¦æ¯”çš„èƒ½åŠ›ã€‚\n\nå‡­å€Ÿå…¶å…ˆè¿›çš„å¤šæ¨¡æ€æ˜“ç”¨æ€§ã€æ…·æ…¨çš„å…è´¹å®šä»·å’Œåˆ›é€ æ½œåŠ›ï¼Œå®ƒä¸ºäººå·¥æ™ºèƒ½è®¾å®šäº†æ–°çš„æ ‡å‡†ï¼Œè®© GPT-4o ç›¸å½¢è§ç»Œã€‚\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/gemma-vs-llama-vs-mistral-exploring-smaller-ai-models-672a95f4b9b7","frontmatter":{"title":"Gemmaã€Llama å’Œ Mistralï¼šæŽ¢ç´¢è¾ƒå°çš„ AI æ¨¡åž‹","meta_title":"Gemmaã€Llama å’Œ Mistralï¼šæŽ¢ç´¢è¾ƒå°çš„ AI æ¨¡åž‹","description":"å°è§„æ¨¡è¯­è¨€æ¨¡åž‹çš„æ¯”è¾ƒç ”ç©¶ï¼šè¯„ä¼° Gemmaã€Llama 3 å’Œ Mistral åœ¨é˜…è¯»ç†è§£ä»»åŠ¡ä¸­çš„è¡¨çŽ°","date":"2024-11-10T22:36:54.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*TJqJ12YQCeYTS5fWOYR5Ig.png","categories":["Natural Language Processing","Machine Learning","Data Science"],"author":"Rifx.Online","tags":["Gemma","Llama","Mistral","SQuAD","Multi-Query"],"draft":false,"slug":"blog/gemma-vs-llama-vs-mistral-exploring-smaller-ai-models-672a95f4b9b7"},"content":"\n### å°è§„æ¨¡è¯­è¨€æ¨¡åž‹çš„æ¯”è¾ƒç ”ç©¶ï¼šåœ¨é˜…è¯»ç†è§£ä»»åŠ¡ä¸­è¯„ä¼° Gemmaã€Llama 3 å’Œ Mistral\n\n## å¼•è¨€\n\nå¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰æ­£åœ¨å¿«é€Ÿå‘å±•ã€‚æ¯ä¸ªæœˆï¼Œæ–°çš„æ¨¡åž‹è¢«å¼€å‘å‡ºæ¥ï¼Œä»¥è¶…è¶Šå½“å‰å¸‚åœºä¸Šçš„é¡¶å°–æ¨¡åž‹ã€‚è¿™ç§å¥åº·çš„ç«žäº‰æœ‰åˆ©äºŽåˆ›é€ æ–°çš„æ–¹æ³•ï¼Œæé«˜è´¨é‡å’Œé€Ÿåº¦ã€‚æ­¤å¤–ï¼Œå„å…¬å¸è¿˜ä¸“æ³¨äºŽå¼€å‘æ›´å°çš„æ¨¡åž‹ï¼Œä»¥ä¾¿ä½¿å…¶èƒ½å¤Ÿè¢«æ²¡æœ‰å¼ºå¤§è®¡ç®—èµ„æºçš„ä¸ªäººæˆ–ç»„ç»‡æ‰€ä½¿ç”¨ã€‚\n\nå°±åœ¨å‡ å‘¨å‰ï¼Œè‹¹æžœå…¬å¸åœ¨å…¶å…¨çƒå¼€å‘è€…å¤§ä¼šä¸ŠæŽ¨å‡ºäº†Apple Intelligenceã€‚è¿™æ˜¯ä¸€å¥—å¤šä¸ªç”Ÿæˆæ¨¡åž‹ï¼Œç»è¿‡å¾®è°ƒä»¥å¸®åŠ©ç”¨æˆ·æ’°å†™å’Œå®Œå–„æ–‡æœ¬ã€ä¼˜å…ˆå¤„ç†å’Œæ€»ç»“é€šçŸ¥ã€åˆ›å»ºå›¾åƒä»¥åŠè¿›è¡Œåº”ç”¨å†…æ“ä½œã€‚åœ¨è¯¥å¥—ä»¶ä¸­ï¼Œè‹¹æžœå…¬å¸å¼€å‘çš„å”¯ä¸€åŸºç¡€å’Œä¸“æœ‰æ¨¡åž‹æ˜¯åœ¨åŒä¸€å¤§ä¼šä¸Šä»‹ç»çš„ã€‚å®ƒæ˜¯ä¸€ä¸ªæ—¨åœ¨è®¾å¤‡ä¸Šè¿è¡Œçš„å°åž‹æ¨¡åž‹ï¼Œå…¶ä¸­ç¡¬ä»¶æˆä¸ºä¸€ä¸ªé‡è¦çš„é™åˆ¶ã€‚åœ¨è‹¹æžœçš„æ¡ˆä¾‹ä¸­ï¼Œè¯¥æ¨¡åž‹æ˜¯é—­æºçš„ã€‚æˆ‘ä»¬æ‰€çŸ¥é“çš„æ˜¯ï¼Œå®ƒæ˜¯ä¸€ä¸ªçº¦30äº¿å‚æ•°çš„æ¨¡åž‹ï¼Œä¸ŽGemmaã€Mistralå’ŒLlama 3çš„7bç‰ˆæœ¬ç›¸å½“ï¼ˆæ ¹æ®è‹¹æžœåˆ†äº«çš„ç»“æžœï¼‰ã€‚\n\nè™½ç„¶è‹¹æžœçš„æ–°æ¨¡åž‹ä»¤äººå…´å¥‹ï¼Œä½†æˆ‘ä»¬æ— æ³•æµ‹è¯•æˆ–é‡ç”¨å®ƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ›´æ„Ÿå…´è¶£çš„æ˜¯å…¬å¼€å¯ç”¨çš„æ¨¡åž‹ï¼Œå› ä¸ºå¼€å‘è€…å’Œå…¬å¸å¯ä»¥åˆ©ç”¨å®ƒä»¬æ¥æž„å»ºæ–°äº§å“å’ŒæœåŠ¡ã€‚åŒºåˆ†å¼€æ”¾LLMså’Œå¼€æºLLMsæ˜¯é‡è¦çš„ã€‚ä»ŽåŽ†å²ä¸Šçœ‹ï¼Œå¼€æºè½¯ä»¶æŒ‡çš„æ˜¯åœ¨ç‰¹å®šè®¸å¯è¯ä¸‹å‘å¸ƒçš„è®¡ç®—æœºç¨‹åºï¼Œä½¿æºä»£ç å¯ä¾›å…¬ä¼—ä½¿ç”¨æˆ–ä¿®æ”¹ã€‚åœ¨LLMsä¸­ï¼Œå­˜åœ¨é¢å¤–çš„å¤æ‚æ€§ï¼ŒåŒ…æ‹¬è®­ç»ƒæ•°æ®å’Œæ¨¡åž‹æƒé‡ã€‚å› æ­¤ï¼Œå¼€æ”¾LLMsé€šå¸¸ä¼šæŠ«éœ²æ¨¡åž‹æƒé‡å’Œåˆå§‹ä»£ç ã€‚å¦ä¸€æ–¹é¢ï¼Œå¼€æºLLMå°†åˆ†äº«è®­ç»ƒè¿‡ç¨‹çš„æ¯ä¸€æ­¥ï¼ŒåŒ…æ‹¬è®­ç»ƒæ•°æ®ï¼Œä»¥åŠä¸€ä¸ªå®½æ¾çš„è®¸å¯è¯ã€‚å®ƒåº”è¯¥å…è®¸å…¶ä»–äººä½¿ç”¨ã€æž„å»ºå’Œè¿›ä¸€æ­¥åˆ†å‘è¯¥æ¨¡åž‹ã€‚ç„¶è€Œï¼Œå¦‚ä»Šå‘å¸ƒçš„å¤§å¤šæ•°æ¨¡åž‹éƒ½å±žäºŽå¼€æ”¾LLMsçš„èŒƒç•´ï¼Œå› ä¸ºä¾‹å¦‚å®ƒä»¬å¹¶æœªå‘å¸ƒç”¨äºŽè®­ç»ƒçš„æ•°æ®åº“ã€‚è¿™ç§æƒ…å†µé€‚ç”¨äºŽè°·æ­Œçš„Gemmaã€Mistral AIçš„Mistralå’ŒMetaçš„Llamaã€‚\n\nåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ›´ä»”ç»†åœ°åˆ†æžGemmaï¼Œä»¥äº†è§£è¿™äº›è¾ƒå°æ¨¡åž‹çš„åŒºåˆ«ã€‚Gemmaæ˜¯è°·æ­Œæœ€è¿‘å‘å¸ƒçš„æ¨¡åž‹ä¹‹ä¸€ã€‚å®ƒæœ‰ä¸¤ä¸ªç‰ˆæœ¬ï¼Œåˆ†åˆ«æ˜¯20äº¿å’Œ70äº¿å‚æ•°ã€‚å› æ­¤ï¼Œå®ƒå¯ä»¥åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šä½¿ç”¨ï¼Œå¹¶æ—¨åœ¨è¶…è¶ŠMistralå’ŒLlama 3ç­‰æœ€å…ˆè¿›çš„æ¨¡åž‹ã€‚\n\næ­¤å¤–ï¼Œæˆ‘ä»¬å°†Gemmaã€Llama 3å’ŒMistralåº”ç”¨äºŽä¸€ä¸ªåä¸ºSQuADçš„é˜…è¯»ç†è§£æ•°æ®é›†ã€‚LLMsçš„ä»»åŠ¡æ˜¯æ ¹æ®ç»™å®šçš„ä¸Šä¸‹æ–‡å›žç­”ç‰¹å®šé—®é¢˜ã€‚æˆ‘ä»¬ä½¿ç”¨å®šé‡æŒ‡æ ‡è¯„ä¼°å®ƒä»¬çš„æ€§èƒ½ï¼Œä¾‹å¦‚æŽ¨ç†é€Ÿåº¦å’Œå¹³å‡å›žç­”é•¿åº¦ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨äº†\\[1]æå‡ºçš„ç›¸å¯¹ç­”æ¡ˆè´¨é‡ï¼ˆRAQï¼‰æ¡†æž¶ã€‚RAQé€šè¿‡æ ¹æ®ç­”æ¡ˆç›¸å¯¹äºŽçœŸå®žç­”æ¡ˆçš„å‡†ç¡®æ€§å¯¹ç­”æ¡ˆè¿›è¡ŒæŽ’åï¼Œå¡«è¡¥äº†åœ¨ç‰¹å®šç”¨ä¾‹ä¸­è¯„ä¼°LLMsçš„ç©ºç™½ï¼Œä»Žè€Œæä¾›äº†æ›´ç»†è‡´å’Œå®žç”¨çš„æ¨¡åž‹æ€§èƒ½è¯„ä¼°ã€‚\n\n\n\nå¦‚å¾€å¸¸ä¸€æ ·ï¼Œä»£ç å¯åœ¨æˆ‘ä»¬çš„[GitHub](https://github.com/zaai-ai/lab)ä¸Šæ‰¾åˆ°ã€‚\n\n## Gemma: Geminiçš„åŸºç¡€æ–‡æœ¬æ¨¡åž‹\n\nè°·æ­Œå‘å¸ƒäº†Gemma \\[2]ï¼Œè¿™æ˜¯åŸºäºŽå…¶å¼ºå¤§çš„é—­æºæ¨¡åž‹Gemini \\[3]å¼€å‘çš„å¼€æ”¾LLMã€‚\n\nè°·æ­Œå‘å¸ƒäº†é¢„è®­ç»ƒå’Œå¾®è°ƒçš„æ£€æŸ¥ç‚¹ï¼Œä»¥ä¿ƒè¿›è¯¥æ¨¡åž‹åœ¨æ–°ç”¨ä¾‹ä¸­çš„è¿›ä¸€æ­¥ç ”ç©¶ï¼Œæä¾›äº†ä¸¤ç§ä¸åŒçš„å¤§å°ï¼š\n\n* 7Bæ¨¡åž‹å°†è¢«éƒ¨ç½²å¹¶åœ¨GPUæˆ–TPUä¸Šè¿›ä¸€æ­¥å¼€å‘ã€‚\n* 2Bæ¨¡åž‹æ—¨åœ¨è§£å†³è®¡ç®—é™åˆ¶ï¼Œå¹¶å…è®¸åœ¨CPUæˆ–è®¾å¤‡åº”ç”¨ç¨‹åºä¸Šä½¿ç”¨ã€‚\n\nGemmaæ‰¿è¯ºåœ¨ä¸Žå…¶ä»–å¤§è‡´ç›¸åŒè§„æ¨¡çš„å¼€æ”¾æ¨¡åž‹ï¼ˆå¦‚Llama 3 7Bæˆ–Mistral 7Bï¼‰ç›¸æ¯”æ—¶ï¼Œè¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™åº”è¯¥åœ¨ä¸åŒé¢†åŸŸä¸­å®žçŽ°ï¼Œä¾‹å¦‚é—®ç­”ã€å¸¸è¯†æŽ¨ç†ã€æ•°å­¦/ç§‘å­¦å’Œç¼–ç ã€‚\n\n## Gemma: æœ‰ä»€ä¹ˆæ–°å˜åŒ–ï¼Ÿ\n\nGemma çš„æž¶æž„åŸºäºŽä¸€ä¸ªä»…è§£ç å™¨ \\[4] Transformer \\[5]ï¼Œä¸Šä¸‹æ–‡é•¿åº¦ä¸º 8192 ä¸ªæ ‡è®°ã€‚è®©æˆ‘ä»¬æ¥æŽ¢è®¨ä¸€ä¸‹ä¸ºä½¿å…¶æ›´å°è€Œé‡‡å–çš„æ–¹æ³•ã€‚\n\n## å¤šæŸ¥è¯¢æ³¨æ„åŠ›\n\n2Bæ¨¡åž‹åˆ©ç”¨å¤šæŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆMQAï¼‰æ˜¾è‘—å‡å°‘äº†åŠ è½½æ‰€æœ‰æŸ¥è¯¢ã€é”®å’Œå€¼å¤´æ‰€éœ€çš„å†…å­˜èµ„æºï¼Œè€Œä¸æ˜¯ä½¿ç”¨å¤šå¤´æ³¨æ„åŠ›ï¼ˆMHAï¼‰æ–¹æ³•ã€‚MQAé€šè¿‡åœ¨æ³¨æ„åŠ›å±‚ä¸­å¯¹å¤šä¸ªæŸ¥è¯¢å¤´ä½¿ç”¨å•ä¸€çš„é”®å’Œå€¼æ¥å®žçŽ°è¿™ç§å†…å­˜å‡å°‘ï¼Œå¦‚å›¾3æ‰€ç¤ºã€‚\n\nè™½ç„¶è¿™ç§æ–¹æ³•å…è®¸Gemma 2Båœ¨å†…å­˜èµ„æºè¾ƒå°çš„è®¾å¤‡ä¸Šéƒ¨ç½²ï¼Œä½†å¯èƒ½å¯¼è‡´è´¨é‡ä¸‹é™å’Œè®­ç»ƒä¸ç¨³å®šã€‚å› æ­¤ï¼Œä½œè€…é€‰æ‹©åœ¨7Bç‰ˆæœ¬ä¸­ä½¿ç”¨MHAï¼Œéµå¾ªä¸ŽLlama 3ç›¸åŒçš„æ–¹æ³•ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*cgSktHmd_iQeTU4DwWLCPQ.png)\n\n## RoPE åµŒå…¥\n\nTransformers éœ€è¦ä½ç½®åµŒå…¥ï¼Œå› ä¸ºå®ƒä»¬æœ¬è´¨ä¸Šæ˜¯æ— åºä¸å˜çš„ã€‚è¿™æ„å‘³ç€å¦‚æžœæ²¡æœ‰ä½ç½®ä¿¡æ¯ï¼ŒTransformer å°†ä»¥ç›¸åŒçš„æ–¹å¼è¡¨ç¤ºå…·æœ‰ç›¸åŒå•è¯ä½†ä¸åŒé¡ºåºå’Œæ„ä¹‰çš„å¥å­ã€‚ä¾‹å¦‚ï¼š\n\n> *å¥å­ 1:* Gemma æ¯” Llama 3 æ›´å¥½\n\n> *å¥å­ 2:* Llama 3 æ¯” Gemma æ›´å¥½\n\nä½ç½®ä¿¡æ¯é€šå¸¸ä½¿ç”¨ä¸¤ä¸ªæ­£å¼¦å‡½æ•°ï¼ˆæ­£å¼¦å’Œä½™å¼¦ï¼‰æ¥è¡¨ç¤ºã€‚ç„¶åŽï¼Œæ ¹æ®ä½ç½®ã€æ ‡è®°åµŒå…¥ç»´åº¦å’Œæ¨¡åž‹ç»´åº¦ï¼Œä¸ºåºåˆ—ä¸­çš„æ¯ä¸ªä½ç½®åˆ›å»ºä¸€ä¸ªç‹¬ç‰¹çš„ä½ç½®ä¿¡æ¯åµŒå…¥ã€‚\n\nå› æ­¤ï¼Œæ·»åŠ ä½ç½®ä¿¡æ¯å¯¹äºŽä½¿ Transformers æ­£ç¡®å¤„ç†æ–‡æœ¬è‡³å…³é‡è¦ã€‚åŽŸå§‹ Transformer æž¶æž„ä½¿ç”¨**ç»å¯¹ä½ç½®åµŒå…¥**ï¼Œå…¶ä¸­ä½ç½®çš„å‘é‡è¡¨ç¤ºè¢«æ·»åŠ åˆ°æ ‡è®°çš„å‘é‡è¡¨ç¤ºä¸­ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*cU5a_5-ATKwrQVeka-ViXQ.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*JZLrvgvc7l_52uewCrPSbg.png)\n\nç»å¯¹ä½ç½®åµŒå…¥çš„æŒ‘æˆ˜åœ¨äºŽå®ƒä»¬å¹¶æœªæ˜Žç¡®ç¼–ç æ ‡è®°ä¹‹é—´çš„ç›¸å¯¹è·ç¦»ã€‚è™½ç„¶å®ƒä»¬ä½¿ç”¨æ­£å¼¦å’Œä½™å¼¦å‡½æ•°æ•èŽ·ä½ç½®ä¿¡æ¯ï¼Œä½†è¿™äº›åµŒå…¥æ˜¯é’ˆå¯¹æ¯ä¸ªä½ç½®ç‹¬ç«‹è®¡ç®—çš„ã€‚è¿™æ„å‘³ç€æ¨¡åž‹å¹¶ä¸å›ºæœ‰åœ°ç†è§£åºåˆ—ä¸­ä¸åŒä½ç½®çš„æŽ¥è¿‘æ€§æˆ–å…³ç³»é‡è¦æ€§ã€‚ä¾‹å¦‚ï¼Œä½ç½® 1 å’Œ 2 çš„æ ‡è®°åµŒå…¥å¯èƒ½ç”±äºŽæ­£å¼¦å‡½æ•°çš„æ€§è´¨è€Œçœ‹èµ·æ¥ç›¸ä¼¼ï¼Œä½†æ¨¡åž‹å¹¶æœªæ˜Žç¡®è¯†åˆ«è¿™äº›ä½ç½®æ˜¯ç›¸é‚»çš„ã€‚\n\nå› æ­¤ï¼Œæ¨¡åž‹å¯èƒ½æ— æ³•åŒºåˆ†ä½ç½® 1 å’Œ 2 çš„æ ‡è®°ä¹‹é—´çš„å…³ç³»ä¸Žä½ç½® 1 å’Œ 500 çš„æ ‡è®°ä¹‹é—´çš„å…³ç³»ã€‚åœ¨è‡ªç„¶è¯­è¨€å¤„ç†è¿‡ç¨‹ä¸­ï¼Œå¥å­ä¸­ç›¸è¿‘çš„å•è¯é€šå¸¸å…±äº«æ›´å¤šä¸Šä¸‹æ–‡æˆ–å…·æœ‰æ¯”è¿œç¦»çš„å•è¯æ›´å¼ºçš„è¯­ä¹‰æˆ–å¥æ³•å…³ç³»ã€‚ç»å¯¹ä½ç½®åµŒå…¥å¯èƒ½æ— æ³•å®Œå…¨æ•èŽ·è¿™ç§ç»†å¾®å·®åˆ«ã€‚è¿™å¯èƒ½å¯¼è‡´åœ¨æ•èŽ·é•¿ç¨‹ä¾èµ–å…³ç³»æˆ–è¯­è¨€çš„å±‚æ¬¡ç»“æž„æ–¹é¢çš„å±€é™æ€§ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*p-fG2ydLbOhJHjO7Y0LyUw.png)\n\næ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRoPEï¼‰\\[6] é€šè¿‡å¯¹åºåˆ—ä¸­çš„æ ‡è®°åµŒå…¥è¿›è¡Œæ—‹è½¬æ¥å»ºæ¨¡æ ‡è®°çš„ç›¸å¯¹ä½ç½®ï¼Œä»Žè€Œè§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚\n\nè®©æˆ‘ä»¬ä½¿ç”¨ä¹‹å‰çš„ä¾‹å­ï¼Œ*â€˜Gemma æ¯” Llama æ›´å¥½*ï¼Œå¹¶è€ƒè™‘æ¯ä¸ªå•è¯ä½œä¸ºç”± 2D å‘é‡è¡¨ç¤ºçš„æ ‡è®°ã€‚å•è¯ *better* å°†ç”±æ ¹æ®å…¶ä½ç½® *m* å’Œä¸€ä¸ªå¸¸é‡è§’åº¦ Î¸ ä»ŽåŽŸå§‹å‘é‡æ—‹è½¬è€Œæ¥çš„ 2D å‘é‡è¡¨ç¤ºï¼Œå¦‚å›¾ 5 æ‰€ç¤ºã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*nX3llo0cwBIrCQ8Gn21-gg.png)\n\nè¿™ç§æ–¹æ³•ä¿ç•™äº†æ ‡è®°ä¹‹é—´çš„ç›¸å¯¹è·ç¦»ï¼Œå› ä¸ºæ—‹è½¬å˜æ¢ä¿æŒäº†å‘é‡ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œæ— è®ºå®ƒä»¬åœ¨åºåˆ—ä¸­çš„ä½ç½®å¦‚ä½•ã€‚ä¾‹å¦‚ï¼Œå¦‚æžœæˆ‘ä»¬åœ¨åŽŸå§‹å¥å­ä¸­æ·»åŠ ä¸¤ä¸ªå•è¯ï¼Œä½¿å…¶å˜ä¸ºâ€˜*The LLM Gemma æ¯” Llama æ›´å¥½*â€™ï¼Œåˆ™ *better* å’Œ *than* çš„ä½ç½®ä»Ž (3 & 4) å˜ä¸º (5 & 6)ã€‚ç„¶è€Œï¼Œç”±äºŽæ—‹è½¬è§’åº¦ä¿æŒä¸€è‡´ï¼Œè¿™äº›å‘é‡ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼ˆé€šè¿‡ç‚¹ç§¯æµ‹é‡ï¼‰ä¿æŒä¸å˜ï¼Œä»Žè€Œç¡®ä¿äº†ä¸€è‡´çš„ç›¸å¯¹ä½ç½®ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*6cpWPXTexZC8YQbnasHOUg.png)\n\n## GeGLU æ¿€æ´»å‡½æ•°\n\nä½œè€…å°†ä¼ ç»Ÿçš„ ReLU æ¿€æ´»å‡½æ•°æ›¿æ¢ä¸ºä¸€ç§ç§°ä¸º GeGLU çš„é—¨æŽ§çº¿æ€§å•å…ƒï¼ˆGLUï¼‰å˜ä½“ï¼Œå› ä¸ºå¦ä¸€é¡¹ç ”ç©¶ \\[7] è¡¨æ˜Žå®ƒæ”¹å–„äº† LLM ç”Ÿæˆçš„è¾“å‡ºè´¨é‡ã€‚\n\nReLU å’Œ GeGLU ä¹‹é—´æœ‰ä¸¤ä¸ªåŒºåˆ«ï¼š\n\n1. **æ¿€æ´»å‡½æ•°** â€” GeGLU ä½¿ç”¨é«˜æ–¯è¯¯å·®çº¿æ€§å•å…ƒï¼ˆGELUï¼‰\\[8] å‡½æ•°ï¼Œä¸Ž ReLU çš„ä¸åŒä¹‹å¤„åœ¨äºŽï¼Œå®ƒå°†ç¥žç»å…ƒè¾“å…¥ *x* ä¹˜ä»¥æ­£æ€åˆ†å¸ƒçš„ç´¯ç§¯åˆ†å¸ƒå‡½æ•°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ*x* è¢«ä¸¢å¼ƒçš„æ¦‚çŽ‡éšç€ *x* çš„å‡å°è€Œå¢žåŠ ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*FXCfQpvdMJXPk5s6AO-RuA.png)\n\n2\\. **Sigmoid æ¿€æ´»** â€” ç®€å•çš„ ReLU æˆ– GELU æ¿€æ´»å‡½æ•°åº”ç”¨äºŽéšè—è¡¨ç¤º *x* å’Œä¸¤ä¸ªç”±ä¸¤ä¸ªçŸ©é˜µ (*W1* å’Œ *W2*) è¡¨ç¤ºçš„çº¿æ€§å˜æ¢ä¹‹é—´ã€‚GeGLU ä¸­çš„é—¨æŽ§å˜ä½“å¯¹å…¶ä¸­ä¸€ä¸ªç»„ä»¶åº”ç”¨é—¨æŽ§æœºåˆ¶ï¼ˆsigmoidï¼‰ï¼Œå¦‚å…¬å¼ 3 æ‰€ç¤ºã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Z9hUjuy4NvQVDrPj6iSfrQ.png)\n\n## Normalizer Location\n\nå¯¹åŽŸå§‹Transformeræž¶æž„çš„æœ€åŽä¿®æ”¹å¦‚å›¾8æ‰€ç¤ºã€‚ä½œè€…å¯¹æ¯ä¸ªtransformerå­å±‚çš„è¾“å…¥å’Œè¾“å‡ºè¿›è¡Œäº†å½’ä¸€åŒ–ï¼Œä»¥æé«˜è®­ç»ƒçš„ç¨³å®šæ€§ï¼Œè¿™ä¸ŽåŽŸå§‹è®ºæ–‡ä»…å¯¹è¾“å‡ºè¿›è¡Œå½’ä¸€åŒ–çš„åšæ³•ç›¸åã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*NQe4ME2MhvRWVzobVdIloA.png)\n\nä»–ä»¬è¿˜ç”¨RMSNorm \\[8]æ›¿æ¢äº†ä¼ ç»Ÿçš„LayerNormå‡½æ•°ã€‚RMSNormåœ¨ä¿æŒè®­ç»ƒç¨³å®šæ€§æ”¹è¿›çš„åŒæ—¶ï¼Œè®¡ç®—ä¸Šæ›´é«˜æ•ˆï¼Œå¹¶æœ‰åŠ©äºŽæ¨¡åž‹æ”¶æ•›ã€‚\n\nRMSNormå®žçŽ°äº†æ›´å¥½çš„æ•ˆçŽ‡ï¼Œå› ä¸ºå…¶ä½œè€…è¯æ˜ŽLayerNormçš„å¥½å¤„æ¥è‡ªäºŽé‡æ–°ç¼©æ”¾ä¸å˜æ€§ï¼Œè€Œä¸æ˜¯é‡æ–°ä¸­å¿ƒåŒ–ä¸å˜æ€§ã€‚é‡æ–°ç¼©æ”¾ä¸å˜æ€§æ„å‘³ç€ï¼Œå¦‚æžœä¸€ä¸ªå¸¸æ•°å› å­ç¼©æ”¾è¾“å…¥ï¼Œåˆ™å½’ä¸€åŒ–è¿‡ç¨‹çš„è¾“å‡ºä¿æŒä¸å˜ã€‚æ¢å¥è¯è¯´ï¼Œå°†æ‰€æœ‰è¾“å…¥ä¹˜ä»¥ä¸€ä¸ªå¸¸æ•°ä¸ä¼šå½±å“å½’ä¸€åŒ–è¾“å‡ºã€‚é‡æ–°ä¸­å¿ƒåŒ–ä¸å˜æ€§æ„å‘³ç€ï¼Œå¦‚æžœä¸€ä¸ªå¸¸æ•°å€¼åŠ åˆ°æ‰€æœ‰è¾“å…¥ä¸Šï¼Œåˆ™å½’ä¸€åŒ–è¿‡ç¨‹çš„è¾“å‡ºä¿æŒä¸å˜ã€‚è¿™æ„å‘³ç€å°†æ‰€æœ‰è¾“å…¥å¹³ç§»ä¸€ä¸ªå¸¸æ•°é‡ä¸ä¼šå½±å“å½’ä¸€åŒ–è¾“å‡ºã€‚è¿™ä¸ªå‘çŽ°ä½¿å¾—å¯ä»¥åŽ»æŽ‰è®¡ç®—å‡å€¼çš„å¼€é”€ï¼ˆåªéœ€è®¡ç®—æ ‡å‡†å·®ï¼‰ï¼Œä»Žè€Œä½¿RMSNormæ›´ç®€å•ã€æ›´é«˜æ•ˆã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*qblXBo8SCcxzPWePhFVlYg.png)\n\n## Mistral AI vs. Meta vs. Google: Gemma 7Bã€Llama 3 7B å’Œ Mistral 7B çš„æ¯”è¾ƒ\n\nåœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°† 3 ä¸ª LLMâ€”â€”Gemma 7Bã€Mistral 7B å’Œ Llama 3 7Bâ€”â€”è¿›è¡Œæµ‹è¯•ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªåä¸º SQuAD çš„é—®ç­”æ•°æ®é›†ï¼Œéµå¾ª CC BY-SA 4.0 è®¸å¯è¯ï¼ˆå¯ä»¥åœ¨ [è¿™é‡Œ](https://huggingface.co/datasets/rajpurkar/squad) æ‰¾åˆ°ï¼‰ã€‚è¯¥æ•°æ®é›†æ˜¯ä¸€ä¸ªé˜…è¯»ç†è§£æ•°æ®é›†ï¼ŒåŒ…å«å…³äºŽä¸€ç»„ç»´åŸºç™¾ç§‘æ–‡ç« çš„é—®é¢˜ã€‚æ ¹æ®ä¸Šä¸‹æ–‡ï¼Œæ¨¡åž‹åº”è¯¥èƒ½å¤Ÿæ£€ç´¢åˆ°é—®é¢˜çš„æ­£ç¡®ç­”æ¡ˆã€‚å¯¹äºŽæˆ‘ä»¬çš„ç”¨ä¾‹ï¼Œ3 ä¸ªæœ€é‡è¦çš„å­—æ®µæ˜¯ï¼š\n\n* `question` - æ¨¡åž‹åº”è¯¥å›žç­”çš„é—®é¢˜ã€‚\n* `context` - æ¨¡åž‹éœ€è¦ä»Žä¸­æå–ç­”æ¡ˆçš„èƒŒæ™¯ä¿¡æ¯ã€‚\n* `answers` - é—®é¢˜çš„æ–‡æœ¬ç­”æ¡ˆã€‚\n\nè¯„ä¼°è¿‡ç¨‹å°†åŒ…æ‹¬ä¸¤ä¸ªå®šé‡æŒ‡æ ‡ï¼š\n\n* `words per second` - è¯„ä¼°æŽ¨ç†é€Ÿåº¦ã€‚\n* `words` - è¯„ä¼°ç­”æ¡ˆçš„é•¿åº¦ã€‚\n\nä¸ºäº†è¯„ä¼°æ¨¡åž‹åœ¨æˆ‘ä»¬ç”¨ä¾‹ä¸­çš„å‡†ç¡®æ€§ï¼Œæˆ‘ä»¬ä½¿ç”¨ RAQ \\[1]ã€‚RAQ ä½¿ç”¨ä¸€ä¸ªç‹¬ç«‹çš„ LLM å¯¹æ‰€æœ‰ LLM çš„ç­”æ¡ˆè¿›è¡ŒæŽ’åï¼ŒåŸºäºŽå®ƒä»¬ä¸ŽçœŸå®žç­”æ¡ˆçš„æŽ¥è¿‘ç¨‹åº¦ã€‚\n\næˆ‘ä»¬é¦–å…ˆä¸‹è½½ä»¥ `.gguf` æ ¼å¼æä¾›çš„æ¨¡åž‹ï¼Œä»¥ä¾¿åœ¨ CPU ä¸Šè¿è¡Œï¼Œå¹¶å°†å®ƒä»¬æ”¾åœ¨ `model/` æ–‡ä»¶å¤¹ä¸‹ã€‚\n\næˆ‘ä»¬ä½¿ç”¨æ¯ä¸ªæ¨¡åž‹çš„æŒ‡ä»¤ç‰ˆæœ¬ï¼Œå¹¶è¿›è¡Œäº† 4 ä½é‡åŒ–ï¼š\n\n* `mistral-7b-instruct-v0.1.Q4_K_M.gguf` æ¥è‡ª [https://huggingface.co/TheBloke/Mistral\\-7B\\-Instruct\\-v0\\.1\\-GGUF/tree/main](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/tree/main)\n* `Meta-Llama-3-8B-Instruct-Q4_K_M.gguf` æ¥è‡ª [https://huggingface.co/NousResearch/Meta\\-Llama\\-3\\-8B\\-Instruct\\-GGUF](https://huggingface.co/NousResearch/Meta-Llama-3-8B-Instruct-GGUF)\n* `gemma-7b-it-Q4_K_M.gguf` æ¥è‡ª [https://huggingface.co/rahuldshetty/gemma\\-7b\\-it\\-gguf\\-quantized/tree/main](https://huggingface.co/rahuldshetty/gemma-7b-it-gguf-quantized/tree/main)\n\nä¹‹åŽï¼Œæˆ‘ä»¬å¯¼å…¥æ‰€æœ‰åº“å’ŒæŽ¥æ”¶æˆ‘ä»¬æƒ³è¦ä½¿ç”¨çš„æ¨¡åž‹ä½œä¸ºå‚æ•°çš„ç”Ÿæˆå™¨ã€‚\n\n```python\nimport os\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scikit_posthocs as sp\nimport pandas as pd\nimport utils\n\nfrom dotenv import load_dotenv\nfrom datasets import load_dataset\nfrom generator.generator import Generator\n\nllama = Generator(model='llama')\nmistral = Generator(model='mistral')\ngemma = Generator(model='gemma')\nload_dotenv('env/var.env')\n```\n\nè¯¥ç±»è´Ÿè´£å¯¼å…¥åœ¨ `config.yaml` æ–‡ä»¶ä¸­å®šä¹‰çš„æ¨¡åž‹å‚æ•°ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹å¾ï¼š`context_length` ä¸º 1024ï¼Œ`temperature` ä¸º 0.7ï¼Œ`max_tokens` ä¸º 2000\\ã€‚\n\n```python\ngenerator:\n  llama:\n    llm_path: \"model/Meta-llama-3-8B-Instruct-Q4_K_M.gguf\"\n  mistral:\n    llm_path: \"model/mistral-7b-instruct-v0.1.Q4_K_M.gguf\"\n  gemma:\n    llm_path: \"model/gemma-7b-it-Q4_K_M.gguf\"\n  context_length: 1024\n  temperature: 0.7\n  max_tokens: 2000\n```\n\nå®ƒè¿˜åˆ›å»ºäº†æç¤ºæ¨¡æ¿ã€‚è¯¥æ¨¡æ¿æœ‰åŠ©äºŽåœ¨å°†æŸ¥è¯¢å’Œä¸Šä¸‹æ–‡ä¼ é€’ç»™ LLM ä»¥èŽ·å–å“åº”ä¹‹å‰æ ¼å¼åŒ–å®ƒä»¬ã€‚\n\n```python\nfrom langchain import PromptTemplate\nfrom langchain.chains import LLMChain\nfrom langchain.llms import LlamaCpp\n\nfrom base.config import Config\nclass Generator(Config):\n    \"\"\"Generator, aka LLM, to provide an answer based on some question and context\"\"\"\n    def __init__(self, model) -> None:\n        super().__init__()\n    # template\n        self.template = \"\"\"\n            Use the following pieces of context to answer the question at the end.\n            {context}\n            Question: {question}\n            Answer:\n        \"\"\"\n   # load llm from local file\n        self.llm = LlamaCpp(\n            model_path=f\"{self.parent_path}/{self.config['generator'][model]['llm_path']}\",\n            n_ctx=self.config[\"generator\"][\"context_length\"],\n            temperature=self.config[\"generator\"][\"temperature\"],\n        )\n        # create prompt template\n        self.prompt = PromptTemplate(\n            template=self.template, input_variables=[\"context\", \"question\"]\n        )\n    def get_answer(self, context: str, question: str) -> str:\n        \"\"\"\n        Get the answer from llm based on context and user's question\n        Args:\n            context: most similar document retrieved\n            question: user's question\n        Returns:\n            llm answer\n        \"\"\"\n        query_llm = LLMChain(\n            llm=self.llm,\n            prompt=self.prompt,\n            llm_kwargs={\"max_tokens\": self.config[\"generator\"][\"max_tokens\"]},\n        )\n        return query_llm.run({\"context\": context, \"question\": question})\n```\n\nåŠ è½½ LLM åŽï¼Œæˆ‘ä»¬ä»Ž HuggingFace èŽ·å– SQuAD æ•°æ®é›†å¹¶å¯¹å…¶è¿›è¡Œæ´—ç‰Œï¼Œä»¥ç¡®ä¿é—®é¢˜ä¸»é¢˜çš„å¤šæ ·æ€§ã€‚\n\n```python\nsquad = load_dataset(\"squad\", split=\"train\")\nsquad = squad.shuffle()\n```\n\nçŽ°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥å¾ªçŽ¯å¤„ç† 60 ä¸ªé—®é¢˜å’Œä¸Šä¸‹æ–‡ï¼Œå¹¶è®°å½•ä¸Šè¿°æŒ‡æ ‡ã€‚\n\n```python\nfor i in range(60):\n    context = squad[i]['context']\n    query = squad[i]['question']\n    answer = squad[i]['answers']['text'][0]\n\n    # Llama\n    answer_llama, words_per_second, words = utils.get_llm_response(llama, context, query)\n    llama_metrics[\"words_per_second\"].append(words_per_second)\n    llama_metrics[\"words\"].append(words)\n    # mistral\n    answer_mistral, words_per_second, words = utils.get_llm_response(mistral, context, query)\n    mistral_metrics[\"words_per_second\"].append(words_per_second)\n    mistral_metrics[\"words\"].append(words)\n    # gemma\n    answer_gemma, words_per_second, words = utils.get_llm_response(gemma, context, query)\n    gemma_metrics[\"words_per_second\"].append(words_per_second)\n    gemma_metrics[\"words\"].append(words)\n  \n    # GPT-3.5 rank\n    llm_answers_dict = {'llama': answer_llama, 'mistral': answer_mistral, 'gemma': answer_gemma}\n    rank = utils.get_gpt_rank(answer, llm_answers_dict, os.getenv(\"OPENAI_API_KEY\"))\n    llama_metrics[\"rank\"].append(rank.index('1')+1)\n    mistral_metrics[\"rank\"].append(rank.index('2')+1)\n    gemma_metrics[\"rank\"].append(rank.index('3')+1)\n```\n\nå‡½æ•° `get_llm_response` è´Ÿè´£æŽ¥æ”¶åŠ è½½çš„ LLMã€ä¸Šä¸‹æ–‡å’Œé—®é¢˜ï¼Œå¹¶è¿”å›ž LLM ç­”æ¡ˆä»¥åŠå®šé‡æŒ‡æ ‡ã€‚\n\n```python\ndef get_llm_response(model: Generator, context: str, query: str) -> Tuple[str, int, int]:\n    \"\"\"\n    Generates an answer from a given LLM based on context and query\n    returns the answer and the number of words per second and the total number of words\n    Args:\n        model: LLM\n        context: context data\n        query: question\n    Returns:\n        answer, words_per_second, words\n    \"\"\"\n    init_time = time.time()\n    answer_llm = model.get_answer(context, query)\n    total_time = time.time()-init_time\n    words_per_second = len(re.sub(\"[^a-zA-Z']+\", ' ', answer_llm).split())/total_time\n    words = len(re.sub(\"[^a-zA-Z']+\", ' ', answer_llm).split())\n    return answer_llm, words_per_second, words\n```\n\næˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼ŒLlama 3 çš„é€Ÿåº¦å¿«äºŽ Mistral å’Œ Gemmaï¼Œå¹³å‡æ¯ç§’ç”Ÿæˆçº¦ 0.7 ä¸ªå•è¯ï¼Œè€Œ Mistral çº¦ä¸º 0.26ï¼ŒGemma çº¦ä¸º 0.4 ä¸ªå•è¯ã€‚åœ¨ç­”æ¡ˆé•¿åº¦æ–¹é¢ï¼ŒLlama 3 ä¹Ÿç”Ÿæˆæ¯” Mistral å’Œ Gemma æ›´é•¿çš„ç­”æ¡ˆï¼Œå¹³å‡ç­”æ¡ˆé•¿åº¦ä¸º 148 ä¸ªå•è¯ï¼Œè€Œ Mistral ä¸º 20 ä¸ªå•è¯ï¼ŒGemma ä¸º 50 ä¸ªå•è¯ã€‚æœ€åŽï¼Œæ ¹æ® RAQï¼ŒMistral çš„å¹³å‡æŽ’åæœ€å¥½ï¼Œçº¦ä¸º 1.81ï¼Œå…¶æ¬¡æ˜¯ Gemmaï¼Œå¹³å‡ä¸º 2.05ï¼Œè€Œ Llama 3 çš„è¡¨çŽ°è¾ƒå·®ï¼Œå¹³å‡æŽ’åçº¦ä¸º 2.1\\ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GVeFQbMZZ5oUScVEHQPu8A.png)\n\nRAQ æ¡†æž¶è¿˜åŒ…æ‹¬ç»Ÿè®¡æ£€éªŒï¼Œä»¥äº†è§£è§‚å¯Ÿåˆ°çš„å·®å¼‚æ˜¯å¦æ˜¾è‘—ã€‚è¡¨ 1 æ˜¾ç¤ºäº† Dunn äº‹åŽæ£€éªŒçš„ç»“æžœï¼Œæ¯”è¾ƒä¸åŒè¯­è¨€æ¨¡åž‹çš„æ€§èƒ½ã€‚æ¯ä¸ªå•å…ƒæ ¼è¡¨ç¤ºç›¸åº”æ¨¡åž‹ä¹‹é—´çš„æ€§èƒ½å·®å¼‚åœ¨ 5% æ˜¾è‘—æ€§æ°´å¹³ä¸‹æ˜¯å¦å…·æœ‰ç»Ÿè®¡æ˜¾è‘—æ€§ã€‚â€œæ˜¾è‘—â€è¡¨ç¤ºç»Ÿè®¡ä¸Šæ˜¾è‘—çš„å·®å¼‚ï¼ˆp å€¼ â‰¤ 0.05ï¼‰ï¼Œè€Œâ€œæ— æ˜¾è‘—æ€§â€è¡¨ç¤ºæ²¡æœ‰ç»Ÿè®¡ä¸Šæ˜¾è‘—çš„å·®å¼‚ï¼ˆp å€¼ > 0.05ï¼‰ã€‚å¯¹äºŽæ‰€é€‰çš„æ˜¾è‘—æ€§æ°´å¹³ï¼ŒDunn æ£€éªŒç»“æžœè¡¨æ˜Žæ¨¡åž‹ä¹‹é—´çš„æ€§èƒ½å·®å¼‚ä¸æ˜¾è‘—ã€‚\n\n```python\np_values = sp.posthoc_dunn([Llama_metrics['rank'], mistral_metrics['rank'], gemma_metrics['rank']], p_adjust='holm')\np_values > 0.05\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ftCaagMKAm5RzeATYm_7Ug.png)\n\nå®šæ€§è¯„ä¼°ä¸€äº›ç¤ºä¾‹å§‹ç»ˆå¾ˆé‡è¦ã€‚ä»¥ä¸‹æ˜¯ 3 ä¸ªæ¨¡åž‹å¯¹é—®é¢˜ *â€˜Power House Day åœ¨çº½é»‘æ–‡çš„å“ªä¸€å¤©åº†ç¥ï¼Ÿâ€™* çš„å›žç­”ï¼ŒåŸºäºŽä»¥ä¸‹ä¸Šä¸‹æ–‡ï¼š\n\n> ***Context:***â€˜ä¸€ä¸ªå¤šä¸–çºªä»¥æ¥ï¼Œçº½é»‘æ–‡å¸‚æ°‘ä¸€ç›´ä¸Žå¸¸è§„è‹±å›½å†›é˜Ÿä¸€èµ·åœ¨æ®–æ°‘æ°‘å…µä¸­ä½œæˆ˜ï¼Œå°±åƒåœ¨æ³•å›½å’Œå°ç¬¬å®‰æˆ˜äº‰ä¸­ä¸€æ ·ã€‚éšç€ç¾Žå›½é©å‘½çš„ä¸´è¿‘ï¼Œå¤§å«Â·ä¼æ–¯ç‰¹å°†å†›å’Œå…¶ä»–æœ‰å½±å“åŠ›çš„å±…æ°‘å¸Œæœ›ä¸Žè‹±å›½æ”¿åºœçš„å†²çªèƒ½å¤Ÿåœ¨ä¸åå›çš„æƒ…å†µä¸‹è§£å†³ã€‚åœ¨ 1775 å¹´ 4 æœˆ 23 æ—¥ï¼Œè¿™ä¸€å¤©åœ¨çº½é»‘æ–‡ä»è¢«åº†ç¥ä¸ºç«è¯å±‹æ—¥ï¼Œçº½é»‘æ–‡çš„ç¬¬äºŒå…¬å¸ï¼Œå·žé•¿çš„æ­¥å…µå«é˜Ÿï¼Œå‚ä¸Žäº†ä¸Žç»Ÿæ²»è‹±å›½è®®ä¼šçš„æ–—äº‰ã€‚åœ¨æœ¬å°¼è¿ªå…‹ç‰¹Â·é˜¿è¯ºå¾·èˆ¹é•¿çš„æŒ‡æŒ¥ä¸‹ï¼Œä»–ä»¬é—¯å…¥ç«è¯åº“ä»¥æ­¦è£…è‡ªå·±ï¼Œå¹¶å¼€å§‹äº†ä¸ºæœŸä¸‰å¤©çš„æ¸¸è¡Œï¼Œå‰å¾€é©¬è¨è¯¸å¡žå·žçš„å‰‘æ¡¥ã€‚å…¶ä»–çº½é»‘æ–‡æ°‘å…µæˆå‘˜åœ¨åœºæŠ¤é€ä¹”æ²»Â·åŽç››é¡¿ï¼Œä»Žä»–åœ¨çº½é»‘æ–‡çš„è¿‡å¤œä½å®¿å‰å¾€å‰‘æ¡¥ã€‚æ¥è‡ªåŒæ–¹çš„å½“ä»£æŠ¥å‘Šéƒ½æåˆ°äº†çº½é»‘æ–‡å¿—æ„¿è€…çš„ä¸“ä¸šå†›äº‹é£ŽèŒƒï¼ŒåŒ…æ‹¬åˆ¶æœã€‚â€™\n\næ‰€æœ‰ 3 ä¸ªæ¨¡åž‹éƒ½ç»™å‡ºäº†æ­£ç¡®ç­”æ¡ˆã€‚è™½ç„¶ Llama 3 å’Œ Gemma æä¾›äº†æ›´å®Œæ•´çš„ç­”æ¡ˆï¼Œä½† Mistral åˆ™æ›´åŠ ç®€æ´ã€‚\n\n> ***Llama 3 answer:***â€˜çº½é»‘æ–‡çš„ç«è¯å±‹æ—¥åº†ç¥äºŽ 4 æœˆ 23 æ—¥ã€‚â€™\n\n> ***Gemma answer:***â€˜å½“ç„¶ï¼æ–‡æœ¬ä¸­è¯´æ˜Žäº†ç«è¯å±‹æ—¥åº†ç¥çš„æ—¥æœŸï¼šç«è¯å±‹æ—¥åœ¨çº½é»‘æ–‡åº†ç¥äºŽ **4 æœˆ 23 æ—¥**ã€‚â€™\n\n> ***Mistral answer:***â€™23 Aprilâ€™\n\n## ç»“è®º\n\nåœ¨è®¾å¤‡ä¸Šçš„æ¨¡åž‹ä¸ºæå‡ç”¨æˆ·ä½“éªŒæä¾›äº†æžå¤§çš„æœºä¼šï¼Œä½¿å¼ºå¤§çš„ LLM èƒ½å¤Ÿåœ¨è®¡ç®—èµ„æºè¾ƒä½Žçš„è®¾å¤‡ä¸Šä½¿ç”¨ã€‚è‹¹æžœå’Œè°·æ­Œéƒ½åœ¨ç§¯æžå¼€å‘æ›´å°ã€æ›´é«˜æ•ˆçš„æ¨¡åž‹ï¼Œä»¥æ»¡è¶³è¿™ä¸€éœ€æ±‚ï¼Œä½¿æ›´å¤šäººèƒ½å¤Ÿåœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­å—ç›ŠäºŽå…ˆè¿›çš„äººå·¥æ™ºèƒ½ã€‚\n\nåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æŽ¢è®¨äº†è°·æ­Œå¼€å‘çš„å¼€æº LLM Gemmaï¼Œå®ƒåœ¨ä¼ ç»Ÿçš„ Transformer æž¶æž„ä¸­å¼•å…¥äº†å››ä¸ªæ–°ç‰¹æ€§ï¼š2B ç‰ˆæœ¬ä¸­çš„å¤šæŸ¥è¯¢æ³¨æ„åŠ›ã€ç”¨äºŽä½ç½®ç¼–ç çš„ RoPE åµŒå…¥ã€ä½œä¸ºæ¿€æ´»å‡½æ•°çš„ GeGLUï¼Œä»¥åŠè¾“å…¥å½’ä¸€åŒ–ã€‚\n\næˆ‘ä»¬è¿˜å°† Gemma çš„æ€§èƒ½ä¸Ž Llama 3 å’Œ Mistral åœ¨é˜…è¯»ç†è§£æ•°æ®é›†ä¸Šçš„è¡¨çŽ°è¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼ŒGemma æ¯ç§’ç”Ÿæˆçš„å•è¯æ•°æ›´å¤šï¼Œå†™å‡ºçš„ç­”æ¡ˆæ¯” Mistral æ›´é•¿ï¼Œä½†åœ¨è¿™äº›æŒ‡æ ‡ä¸Šå¹¶æœªè¶…è¿‡ Llama 3ã€‚ä½¿ç”¨ RAQ æ¡†æž¶ï¼Œæˆ‘ä»¬è¯„ä¼°äº†è¿™ä¸‰ç§æ¨¡åž‹çš„å‡†ç¡®æ€§ã€‚å°½ç®¡æ•°æ®è¡¨æ˜Ž Mistral çš„ç»“æžœæ›´ä½³ï¼Œå…¶æ¬¡æ˜¯ Gemmaï¼Œä½†å·®å¼‚å¹¶ä¸å…·æœ‰ç»Ÿè®¡å­¦æ„ä¹‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥è¯´è¿™ä¸‰ç§æ¨¡åž‹åœ¨åº”ç”¨äºŽæˆ‘ä»¬çš„é˜…è¯»ç†è§£ç”¨ä¾‹æ—¶è¡¨çŽ°ç›¸ä¼¼ã€‚\n\n## å‚è€ƒæ–‡çŒ®\n\n\\[1] LuÃ­s Roque, Rafael Guedes. ä»Žç ”ç©¶åˆ°ç”Ÿäº§ï¼šç›¸å¯¹ç­”æ¡ˆè´¨é‡ï¼ˆRAQï¼‰ä¸ŽNVIDIA NIM. [https://readmedium.com/research\\-to\\-production\\-relative\\-answer\\-quality\\-raq\\-and\\-nvidia\\-nim\\-15ce0c45b3b6](https://readmedium.com/research-to-production-relative-answer-quality-raq-and-nvidia-nim-15ce0c45b3b6), 2024\\.\n\n\\[2] Gemma Team, Google DeepMind. Gemmaï¼šåŸºäºŽGeminiç ”ç©¶å’ŒæŠ€æœ¯çš„å¼€æ”¾æ¨¡åž‹, 2023\\.\n\n\\[3] Gemini Team. Geminiï¼šä¸€ç³»åˆ—é«˜èƒ½åŠ›çš„å¤šæ¨¡æ€æ¨¡åž‹, 2023\\.\n\n\\[4] Noam Shazeer. å¿«é€ŸTransformerè§£ç ï¼šä¸€åªå†™å¤´å°±è¶³å¤Ÿäº†. arXiv:1911\\.02150, 2019\\.\n\n\\[5] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. æ³¨æ„åŠ›å³ä¸€åˆ‡. arXiv:1706\\.03762, 2017\\.\n\n\\[6] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, Yunfeng Liu. RoFormerï¼šå¸¦æ—‹è½¬ä½ç½®åµŒå…¥çš„å¢žå¼ºåž‹Transformer. arXiv:2104\\.09864, 2021\\.\n\n\\[7] Noam Shazeer. GLUå˜ä½“æ”¹å–„Transformer. arXiv:2002\\.05202, 2020\\.\n\n\\[8] Dan Hendrycks, Kevin Gimpel. é«˜æ–¯è¯¯å·®çº¿æ€§å•å…ƒï¼ˆGELUsï¼‰. arXiv:1606\\.08415, 2016\\.\n\n\\[9] Biao Zhang, Rico Sennrich. å‡æ–¹æ ¹å±‚å½’ä¸€åŒ–. arXiv:1910\\.07467, 2019\\.\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/generating-structured-data-from-an-image-with-gpt-vision-and-langchain-34aaf3dcb215","frontmatter":{"title":"ä½¿ç”¨ GPT Vision å’Œ Langchain ä»Žå›¾åƒç”Ÿæˆç»“æž„åŒ–æ•°æ®","meta_title":"ä½¿ç”¨ GPT Vision å’Œ Langchain ä»Žå›¾åƒç”Ÿæˆç»“æž„åŒ–æ•°æ®","description":"åœ¨å½“ä»Šä¸–ç•Œï¼Œè§†è§‰æ•°æ®éžå¸¸ä¸°å¯Œï¼Œä»Žå›¾åƒä¸­æå–æœ‰æ„ä¹‰ä¿¡æ¯çš„èƒ½åŠ›å˜å¾—è¶Šæ¥è¶Šé‡è¦â€¦â€¦","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*FPRRg85jYb7MrzXEpNWbmw.jpeg","categories":["Programming","Computer Vision","Natural Language Processing"],"author":"Rifx.Online","tags":["Langchain","GPT","vision","LLMs","structured"],"draft":false,"slug":"blog/generating-structured-data-from-an-image-with-gpt-vision-and-langchain-34aaf3dcb215"},"content":"\n\n\n\n\nåœ¨å½“ä»Šè¿™ä¸ªè§†è§‰æ•°æ®ä¸°å¯Œçš„ä¸–ç•Œä¸­ï¼Œä»Žå›¾åƒä¸­æå–æœ‰æ„ä¹‰ä¿¡æ¯çš„èƒ½åŠ›å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚Langchainæ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ¡†æž¶ï¼Œç”¨äºŽæž„å»ºå¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰åº”ç”¨ç¨‹åºï¼Œæä¾›äº†ä¸€å¥—å¤šåŠŸèƒ½çš„å·¥å…·æ¥åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†æŽ¢è®¨å¦‚ä½•ä½¿ç”¨Langchainä»Žå›¾åƒä¸­æå–ç»“æž„åŒ–ä¿¡æ¯ï¼Œä¾‹å¦‚è®¡ç®—äººæ•°å’Œåˆ—å‡ºä¸»è¦ç‰©ä½“ã€‚\n\nåœ¨æ·±å…¥ä»£ç ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆäº†è§£ä¸€ä¸‹ä»»åŠ¡çš„èƒŒæ™¯ã€‚æƒ³è±¡ä¸€ä¸‹ä½ æœ‰ä¸€å¼ åœºæ™¯çš„å›¾åƒï¼Œæ¯”å¦‚åŸŽå¸‚è¡—é“ã€‚ä½ çš„ç›®æ ‡æ˜¯ä»Žè¿™å¼ å›¾åƒä¸­æå–æœ‰ä»·å€¼çš„ä¿¡æ¯ï¼ŒåŒ…æ‹¬åœ¨åœºçš„äººæ•°å’Œåœºæ™¯ä¸­çš„ä¸»è¦ç‰©ä½“åˆ—è¡¨ã€‚\n\n## å…³äºŽ Langchain\n\nLangchain æ˜¯ä¸€ä¸ªç»¼åˆæ¡†æž¶ï¼Œå…è®¸å¼€å‘è€…åˆ©ç”¨å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰çš„å¼ºå¤§åŠŸèƒ½æž„å»ºå¤æ‚çš„åº”ç”¨ç¨‹åºã€‚å®ƒæä¾›äº†æ¨¡å—åŒ–å’Œå¯æ‰©å±•çš„æž¶æž„ï¼Œä½¿å¼€å‘è€…èƒ½å¤Ÿåˆ›å»ºé’ˆå¯¹ç‰¹å®šéœ€æ±‚çš„è‡ªå®šä¹‰ç®¡é“ã€ä»£ç†å’Œå·¥ä½œæµã€‚\n\nLangchain ç®€åŒ–äº† LLM çš„é›†æˆï¼Œæä¾›äº†å¤„ç†å„ç§æ•°æ®æºï¼ˆåŒ…æ‹¬æ–‡æœ¬ã€å›¾åƒå’Œç»“æž„åŒ–æ•°æ®ï¼‰çš„æŠ½è±¡å’Œå·¥å…·ã€‚å®ƒæ”¯æŒæ¥è‡ªä¸åŒæä¾›å•†çš„å¹¿æ³› LLMï¼Œä¾‹å¦‚ OpenAI å’Œ Anthropicï¼Œä½¿å¾—åœ¨å•ä¸ªåº”ç”¨ç¨‹åºä¸­è½»æ¾åˆ‡æ¢æ¨¡åž‹æˆ–ç»„åˆå¤šä¸ªæ¨¡åž‹å˜å¾—ç®€å•ã€‚\n\n## å‡†å¤‡çŽ¯å¢ƒå¹¶è®¾ç½® OpenAI API å¯†é’¥\n\nè¦è·Ÿéšæœ¬æ•™ç¨‹ï¼Œæ‚¨éœ€è¦å®‰è£… Langchainã€‚æ‚¨å¯ä»¥ä½¿ç”¨ pip å®‰è£…å®ƒï¼š\n\n```python\npip install langchain langchain_openai\n```\nè¦åœ¨ Langchain ä¸­ä½¿ç”¨ OpenAI è¯­è¨€æ¨¡åž‹ï¼Œæ‚¨éœ€è¦ä»Ž OpenAI èŽ·å–ä¸€ä¸ª API å¯†é’¥ã€‚å¦‚æžœæ‚¨è¿˜æ²¡æœ‰ API å¯†é’¥ï¼Œå¯ä»¥åœ¨ OpenAI ç½‘ç«™ä¸Šæ³¨å†Œä¸€ä¸ª (<https://openai.com/api/>)ã€‚\n\nä¸€æ—¦æ‚¨æ‹¥æœ‰äº† API å¯†é’¥ï¼Œå¯ä»¥å°†å…¶è®¾ç½®ä¸ºç³»ç»Ÿä¸­çš„çŽ¯å¢ƒå˜é‡ï¼Œæˆ–è€…ç›´æŽ¥åœ¨ä»£ç ä¸­æä¾›ã€‚ä»¥ä¸‹æ˜¯å¦‚ä½•å°† API å¯†é’¥è®¾ç½®ä¸ºçŽ¯å¢ƒå˜é‡çš„ç¤ºä¾‹ï¼š\n\n```python\nexport OPENAI_API_KEY=\"your_openai_api_key_here\"\n```\næˆ–è€…ï¼Œæ‚¨å¯ä»¥ç›´æŽ¥åœ¨ Python ä»£ç ä¸­æä¾› API å¯†é’¥ï¼š\n\n```python\nimport os\nimport langchain\nos.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key_here\"\n```\nåœ¨è®¾ç½®å¥½ API å¯†é’¥åŽï¼ŒLangchain å°†èƒ½å¤Ÿä¸Ž OpenAI API è¿›è¡Œèº«ä»½éªŒè¯å¹¶ä½¿ç”¨ä»–ä»¬çš„è¯­è¨€æ¨¡åž‹ã€‚\n\n## åŠ è½½å’Œç¼–ç å›¾åƒ\n\nåœ¨æˆ‘ä»¬ä½¿ç”¨ Langchain å¤„ç†å›¾åƒä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦ä»Žæ–‡ä»¶ä¸­åŠ è½½å›¾åƒæ•°æ®ï¼Œå¹¶å°†å…¶ç¼–ç ä¸ºå¯ä»¥ä¼ é€’ç»™è¯­è¨€æ¨¡åž‹çš„æ ¼å¼ã€‚ä¸‹é¢çš„ä»£ç å®šä¹‰äº†ä¸€ä¸ªå‡½æ•° `load_image`ï¼Œè¯¥å‡½æ•°æŽ¥å—ä¸€ä¸ªåŒ…å« `image_path` é”®çš„å­—å…¸ï¼Œå¹¶è¿”å›žä¸€ä¸ªæ–°çš„å­—å…¸ï¼Œå…¶ä¸­ `image` é”®åŒ…å«ç¼–ç ä¸º base64 å­—ç¬¦ä¸²çš„å›¾åƒæ•°æ®ã€‚\n\n```python\ndef load_image(inputs: dict) -> dict:\n    \"\"\"Load image from file and encode it as base64.\"\"\"\n    image_path = inputs[\"image_path\"]\n  \n    def encode_image(image_path):\n        with open(image_path, \"rb\") as image_file:\n            return base64.b64encode(image_file.read()).decode('utf-8')\n    image_base64 = encode_image(image_path)\n    return {\"image\": image_base64}\n```\n`load_image` å‡½æ•°é¦–å…ˆä»Žè¾“å…¥å­—å…¸ä¸­æå– `image_path`ã€‚ç„¶åŽï¼Œå®ƒå®šä¹‰äº†ä¸€ä¸ªåµŒå¥—å‡½æ•° `encode_image`ï¼Œè¯¥å‡½æ•°ä»¥äºŒè¿›åˆ¶æ¨¡å¼æ‰“å¼€å›¾åƒæ–‡ä»¶ï¼Œè¯»å–å…¶å†…å®¹ï¼Œå¹¶ä½¿ç”¨ Python æ ‡å‡†åº“ä¸­çš„ `base64.b64encode` å‡½æ•°å°†å…¶ç¼–ç ä¸º base64 å­—ç¬¦ä¸²ã€‚\n\n`load_image` å‡½æ•°ä½¿ç”¨æä¾›çš„ `image_path` è°ƒç”¨ `encode_image`ï¼Œå¹¶å°†ç»“æžœ base64 ç¼–ç å­—ç¬¦ä¸²å­˜å‚¨åœ¨ `image_base64` å˜é‡ä¸­ã€‚æœ€åŽï¼Œå®ƒè¿”å›žä¸€ä¸ªæ–°çš„å­—å…¸ï¼Œå…¶ä¸­ `image` é”®è®¾ç½®ä¸º `image_base64`ã€‚\n\nä¸ºäº†å°†æ­¤å‡½æ•°é›†æˆåˆ° Langchain æµæ°´çº¿ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ª `TransformChain`ï¼Œè¯¥é“¾æŽ¥å— `image_path` ä½œä¸ºè¾“å…¥ï¼Œå¹¶ç”Ÿæˆ `image`ï¼ˆbase64 ç¼–ç å­—ç¬¦ä¸²ï¼‰ä½œä¸ºè¾“å‡ºã€‚\n\n```python\nload_image_chain = TransformChain(\n    input_variables=[\"image_path\"],\n    output_variables=[\"image\"],\n    transform=load_image\n)\n```\né€šè¿‡è¿™ç§è®¾ç½®ï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾åœ°å°†å›¾åƒåŠ è½½å’Œç¼–ç ä½œä¸ºæ›´å¤§ Langchain å·¥ä½œæµçš„ä¸€éƒ¨åˆ†ï¼Œä»Žè€Œä½¿æˆ‘ä»¬èƒ½å¤Ÿä½¿ç”¨å¤§åž‹è¯­è¨€æ¨¡åž‹å¤„ç†è§†è§‰æ•°æ®å’Œæ–‡æœ¬ã€‚\n\n## å®šä¹‰è¾“å‡ºç»“æž„\n\nåœ¨æˆ‘ä»¬æå–å›¾åƒä¿¡æ¯ä¹‹å‰ï¼Œéœ€è¦å®šä¹‰æˆ‘ä»¬å¸Œæœ›æŽ¥æ”¶çš„è¾“å‡ºç»“æž„ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªåä¸º `ImageInformation` çš„ Pydantic æ¨¡åž‹ï¼Œå…¶ä¸­åŒ…æ‹¬å›¾åƒæè¿°å’Œæˆ‘ä»¬å¯èƒ½æƒ³è¦æå–çš„ä»»ä½•å…¶ä»–ä¿¡æ¯çš„å­—æ®µã€‚\n\n```python\nfrom langchain_core.pydantic_v1 import BaseModel, Field\n\nclass ImageInformation(BaseModel):\n \"\"\"Information about an image.\"\"\"\n image_description: str = Field(description=\"a short description of the image\")\n people_count: int = Field(description=\"number of humans on the picture\")\n main_objects: list[str] = Field(description=\"list of the main objects on the picture\")\n```\n\n## è®¾ç½®å›¾åƒæ¨¡åž‹\n\næŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªé“¾ï¼Œå°†å›¾åƒåŠ è½½å’Œç¼–ç æ­¥éª¤ä¸Ž LLM è°ƒç”¨æ­¥éª¤ç»“åˆèµ·æ¥ã€‚ç”±äºŽ `ChatOpenAI` æ¨¡åž‹åœ¨æˆ‘çš„ç†è§£ä¸­å¹¶ä¸å…·å¤‡åŒæ—¶å¤„ç†æ–‡æœ¬å’Œå›¾åƒè¾“å…¥çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªåŒ…è£…é“¾æ¥å®žçŽ°è¿™ä¸€åŠŸèƒ½ã€‚\n\n```python\nfrom langchain.chains import TransformChain\nfrom langchain_core.messages import HumanMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain import globals\nfrom langchain_core.runnables import chain\n\n## Set verbose\nglobals.set_debug(True)\n\n@chain\ndef image_model(inputs: dict) -> str | list[str] | dict:\n \"\"\"Invoke model with image and prompt.\"\"\"\n model = ChatOpenAI(temperature=0.5, model=\"gpt-4-vision-preview\", max_tokens=1024)\n msg = model.invoke(\n             [HumanMessage(\n             content=[\n             {\"type\": \"text\", \"text\": inputs[\"prompt\"]},\n             {\"type\": \"text\", \"text\": parser.get_format_instructions()},\n             {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{inputs['image']}\"}},\n             ])]\n             )\n return msg.content\n```\nåœ¨è¿™ä¸ªä»£ç ç‰‡æ®µä¸­ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªåä¸º `image_model` çš„é“¾ï¼Œä½¿ç”¨æä¾›çš„æç¤ºã€æ ¼å¼è¯´æ˜Žå’Œå›¾åƒè°ƒç”¨ `ChatOpenAI` æ¨¡åž‹ã€‚`image_model` é“¾æŽ¥å—ä¸€ä¸ªåŒ…å«æç¤ºå’Œ base64 ç¼–ç å›¾åƒå­—ç¬¦ä¸²çš„å­—å…¸ `inputs`ã€‚\n\nåœ¨é“¾å†…éƒ¨ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ª `HumanMessage` å¯¹è±¡ï¼Œè¯¥å¯¹è±¡ç»“åˆäº†æç¤ºæ–‡æœ¬ã€æ ¼å¼è¯´æ˜Žå’Œå›¾åƒ URLï¼Œä»¥æ•°æ® URI æ ¼å¼åŒ–ï¼ŒåŒ…å« base64 ç¼–ç çš„å›¾åƒæ•°æ®ã€‚ç„¶åŽï¼Œæˆ‘ä»¬ä½¿ç”¨è¿™ä¸ª `HumanMessage` å¯¹è±¡è°ƒç”¨ `ChatOpenAI` æ¨¡åž‹ï¼Œä½¿ç”¨ä¸“é—¨ä¸ºæ¶‰åŠæ–‡æœ¬å’Œå›¾åƒçš„å¤šæ¨¡æ€ä»»åŠ¡è®¾è®¡çš„ `gpt-4-vision-preview` æ¨¡åž‹ã€‚\n\nè¯¥æ¨¡åž‹å¤„ç†æ–‡æœ¬æç¤ºå’Œå›¾åƒï¼Œå¹¶è¿”å›žè¾“å‡ºã€‚\n\n## æ•´åˆæ‰€æœ‰å†…å®¹\n\nçŽ°åœ¨æˆ‘ä»¬å·²ç»æ‹¥æœ‰äº†æ‰€æœ‰å¿…è¦çš„ç»„ä»¶ï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥åè°ƒæ•´ä¸ªè¿‡ç¨‹ï¼š\n\n```python\nfrom langchain_core.output_parsers import JsonOutputParser\n\nparser = JsonOutputParser(pydantic_object=ImageInformation)\ndef get_image_informations(image_path: str) -> dict:\n   vision_prompt = \"\"\"\n   Given the image, provide the following information:\n   - A count of how many people are in the image\n   - A list of the main objects present in the image\n   - A description of the image\n   \"\"\"\n   vision_chain = load_image_chain | image_model | parser\n   return vision_chain.invoke({'image_path': f'{image_path}', \n                               'prompt': vision_prompt})\n```\nåœ¨è¿™ä¸ªå‡½æ•°ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªæç¤ºï¼Œè¦æ±‚LLMæä¾›å›¾åƒä¸­äººç‰©çš„æ•°é‡å’Œä¸»è¦ç‰©ä½“çš„åˆ—è¡¨ã€‚ç„¶åŽï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªé“¾ï¼Œå°†å›¾åƒåŠ è½½æ­¥éª¤ï¼ˆ`load\\_image\\_chain`ï¼‰ã€LLMè°ƒç”¨æ­¥éª¤ï¼ˆ`image\\_model`ï¼‰å’ŒJSONè¾“å‡ºè§£æžå™¨ï¼ˆ`parser`ï¼‰ç»“åˆåœ¨ä¸€èµ·ã€‚æœ€åŽï¼Œæˆ‘ä»¬ç”¨å›¾åƒè·¯å¾„å’Œæç¤ºè°ƒç”¨è¿™ä¸ªé“¾ï¼Œå‡½æ•°è¿”å›žä¸€ä¸ªåŒ…å«æå–ä¿¡æ¯çš„å­—å…¸ã€‚\n\n## ç¤ºä¾‹ç”¨æ³•\n\nè¦ä½¿ç”¨æ­¤åŠŸèƒ½ï¼Œåªéœ€æä¾›å›¾åƒæ–‡ä»¶çš„è·¯å¾„ï¼š\n\n\n```python\nresult = get_image_informations(\"path/to/your/image.jpg\")\nprint(result)\n```\nè¿™å°†è¾“å‡ºä¸€ä¸ªåŒ…å«è¯·æ±‚ä¿¡æ¯çš„å­—å…¸ï¼Œä¾‹å¦‚ï¼š\n\n\n```python\n{\n 'description': 'a view of a city showing cars waiting at a traffic light',\n 'people_count': 5,\n 'main_objects': ['car', 'building', 'traffic light', 'tree']\n}\n```\n\n## ç»“è®º\n\nLangchain æä¾›äº†å¼ºå¤§çš„å·¥å…·é›†ï¼Œç”¨äºŽå¤„ç†å¤§åž‹è¯­è¨€æ¨¡åž‹å¹¶ä»Žå„ç§æ•°æ®æºï¼ˆåŒ…æ‹¬å›¾åƒï¼‰ä¸­æå–æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚é€šè¿‡å°† Langchain çš„åŠŸèƒ½ä¸Žè‡ªå®šä¹‰æç¤ºå’Œè¾“å‡ºè§£æžç›¸ç»“åˆï¼Œæ‚¨å¯ä»¥åˆ›å»ºå¼ºå¤§çš„åº”ç”¨ç¨‹åºï¼Œä»Žè§†è§‰æ•°æ®ä¸­æå–ç»“æž„åŒ–ä¿¡æ¯ã€‚\n\nè¯·è®°ä½ï¼Œè¾“å‡ºçš„è´¨é‡å°†å–å†³äºŽæ‚¨ä½¿ç”¨çš„ LLM çš„èƒ½åŠ›ä»¥åŠæ‚¨æç¤ºçš„å…·ä½“æ€§ã€‚å°è¯•ä¸åŒçš„æ¨¡åž‹å’Œæç¤ºï¼Œä»¥æ‰¾åˆ°æœ€é€‚åˆæ‚¨ç”¨ä¾‹çš„è§£å†³æ–¹æ¡ˆã€‚\n\nå¦‚æžœæ‚¨æ‰¾åˆ°æ›´å¥½çš„æ–¹æ³•æ¥å®žçŽ°ç›¸åŒçš„ç»“æžœæˆ–æœ‰æ”¹è¿›å»ºè®®ï¼Œè¯·éšæ—¶åœ¨è¯„è®ºä¸­åˆ†äº«ã€‚æœ¬æ–‡æä¾›çš„ä»£ç ç¤ºä¾‹æ—¨åœ¨ä½œä¸ºèµ·ç‚¹ï¼Œå¯èƒ½è¿˜æœ‰å…¶ä»–æ–¹æ³•æˆ–ä¼˜åŒ–ã€‚\n\n"},{"lang":"zh","group":"blog","slug":"blog/generative-ai-in-market-research-and-intelligence-benefits-use-cases-and-strategies-4e9195a07ffc","frontmatter":{"title":"ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åœ¨å¸‚åœºç ”ç©¶å’Œæƒ…æŠ¥é¢†åŸŸçš„åº”ç”¨ï¼šä¼˜åŠ¿ã€ç”¨ä¾‹å’Œç­–ç•¥","meta_title":"ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åœ¨å¸‚åœºç ”ç©¶å’Œæƒ…æŠ¥é¢†åŸŸçš„åº”ç”¨ï¼šä¼˜åŠ¿ã€ç”¨ä¾‹å’Œç­–ç•¥","description":"ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½æ­£åœ¨æ˜¾è‘—å˜é©å¸‚åœºç ”ç©¶å’Œæƒ…æŠ¥ï¼Œæå‡æ•°æ®åˆ†æžé€Ÿåº¦å’Œæ·±åº¦ã€‚ä¸Žä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œç”Ÿæˆå¼AIèƒ½å¤Ÿå®žæ—¶å¤„ç†å¤§æ•°æ®ï¼Œæä¾›æ›´å‡†ç¡®çš„æ´žå¯Ÿï¼Œé™ä½Žæˆæœ¬å’Œæ—¶é—´æ¶ˆè€—ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜èƒ½ä¼˜åŒ–å¸‚åœºç­–ç•¥ï¼Œé€šè¿‡æƒ…æ„Ÿåˆ†æžå’Œç”¨æˆ·è§’è‰²å¼€å‘å¢žå¼ºå®¢æˆ·äº’åŠ¨ã€‚ä¼ä¸šé€šè¿‡æ•´åˆAIå·¥å…·ï¼Œå¯ä»¥åœ¨åŠ¨æ€å¸‚åœºä¸­ä¿æŒç«žäº‰åŠ›ï¼Œå®žçŽ°æ•°æ®é©±åŠ¨çš„å†³ç­–ã€‚æœªæ¥ï¼ŒAIå°†è¿›ä¸€æ­¥æŽ¨åŠ¨ä¸ªæ€§åŒ–ã€å®žæ—¶ç›‘æµ‹å’Œå¤šæ¨¡æ€æ•°æ®åˆ†æžï¼Œæˆä¸ºå¸‚åœºç ”ç©¶ä¸å¯æˆ–ç¼ºçš„ç»„æˆéƒ¨åˆ†ã€‚","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*0J_wBAutefsvq5PyxPL17Q.jpeg","categories":["Generative AI","Market Research","Data Science"],"author":"Rifx.Online","tags":["generative","market","research","insights","automation"],"draft":false,"slug":"blog/generative-ai-in-market-research-and-intelligence-benefits-use-cases-and-strategies-4e9195a07ffc"},"content":"\n\n\n### åˆ©ç”¨AIè½¬å˜å¸‚åœºæƒ…æŠ¥\n\n\n\nç”Ÿæˆå¼AIæ­£åœ¨æ”¹å˜ä¼ä¸šè¿›è¡Œå¸‚åœºç ”ç©¶å’Œæƒ…æŠ¥çš„æ–¹å¼ï¼Œä½¿æ•°æ®åˆ†æžå˜å¾—æ›´å¿«ã€æ›´æ·±å…¥ã€‚ä¼ ç»Ÿçš„å¸‚åœºç ”ç©¶æ–¹æ³•é«˜åº¦ä¾èµ–æ‰‹åŠ¨æ•°æ®æ”¶é›†ã€è°ƒæŸ¥åˆ†æžå’Œç«žäº‰å¯¹æ‰‹ç ”ç©¶ï¼Œè¿™äº›æ–¹æ³•å¾€å¾€è€—æ—¶ä¸”èŒƒå›´æœ‰é™ã€‚ç”Ÿæˆå¼AIåˆ™å…è®¸å…¬å¸å³æ—¶åˆ†æžåºžå¤§çš„æ•°æ®é›†ï¼Œç”Ÿæˆå¯èƒ½è¢«å¿½è§†çš„æ´žå¯Ÿï¼Œå¹¶åˆ›å»ºå¸®åŠ©æ›´å‡†ç¡®é¢„æµ‹è¶‹åŠ¿çš„é¢„æµ‹æ¨¡åž‹ã€‚ä»Žç»¼åˆæ•°æ®æŠ¥å‘Šåˆ°åˆ›å»ºé«˜å±‚æ¬¡æ‘˜è¦ï¼Œç”Ÿæˆå¼AIåŠ é€Ÿäº†å†³ç­–è¿‡ç¨‹ï¼Œä½¿å¸‚åœºæ´žå¯Ÿæ›´åŠ æ˜“äºŽèŽ·å–ã€‚\n\né™¤äº†æ•°æ®åˆ†æžï¼Œç”Ÿæˆå¼AIè¿˜åœ¨æ”¹å˜å…¬å¸ä¸Žå—ä¼—çš„äº’åŠ¨æ–¹å¼ä»¥åŠå¸‚åœºç­–ç•¥çš„ä¼˜åŒ–ã€‚é€šè¿‡ä½¿ç”¨å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ï¼Œä¼ä¸šå¯ä»¥æ›´æœ‰æ•ˆåœ°åˆ›å»ºæ¨¡æ‹Ÿã€é¢„æµ‹è¶‹åŠ¿å’Œè¿›è¡Œç«žäº‰åˆ†æžã€‚æœ¬æŒ‡å—æŽ¢è®¨äº†åˆ©ç”¨ç”Ÿæˆå¼AIå¢žå¼ºå¸‚åœºç ”ç©¶çš„ä¸»è¦å¥½å¤„ã€å®žé™…åº”ç”¨æ¡ˆä¾‹å’Œç­–ç•¥ã€‚é€šè¿‡å°†AIæ•´åˆåˆ°å¸‚åœºæƒ…æŠ¥å·¥ä½œæµç¨‹ä¸­ï¼Œå…¬å¸å¯ä»¥ä¿æŒç«žäº‰åŠ›ï¼Œå¹¶åšå‡ºä¸Žå—ä¼—éœ€æ±‚ç›¸ç¬¦åˆçš„æ•°æ®é©±åŠ¨å†³ç­–ã€‚\n\n## ä¼ ç»Ÿå¸‚åœºç ”ç©¶åŠå…¶å±€é™æ€§\n\nä¼ ç»Ÿå¸‚åœºç ”ç©¶æ¶‰åŠè¯¸å¦‚è°ƒæŸ¥ã€ç„¦ç‚¹å°ç»„ã€è®¿è°ˆå’Œç«žäº‰åˆ†æžç­‰æˆç†Ÿæ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•è¢«å…¬å¸å¹¿æ³›ä½¿ç”¨ï¼Œä»¥äº†è§£å…¶å®¢æˆ·ã€ç«žäº‰å¯¹æ‰‹å’Œè¡Œä¸šè¶‹åŠ¿ã€‚è¿™äº›æ–¹æ³•ä¸€ç›´æ˜¯å¸‚åœºæƒ…æŠ¥çš„æ”¯æŸ±ï¼Œä¸ºæˆ˜ç•¥å†³ç­–æä¾›äº†ç»“æž„åŒ–å’Œç»è¿‡éªŒè¯çš„è§è§£ã€‚ç„¶è€Œï¼Œå°½ç®¡æœ‰æ•ˆï¼Œä¼ ç»Ÿå¸‚åœºç ”ç©¶åœ¨å½“ä»ŠåŠ¨æ€å’Œæ•°æ®é¥±å’Œçš„çŽ¯å¢ƒä¸­é¢ä¸´æ˜¾è‘—çš„å±€é™æ€§ã€‚\n\n### 1\\. é«˜æˆæœ¬\n\n* ä¼ ç»Ÿæ–¹æ³•é€šå¸¸æ¶‰åŠå¤§é‡çš„è´¢åŠ¡æŠ•èµ„ï¼Œç‰¹åˆ«æ˜¯å¯¹äºŽå…¨é¢çš„ç ”ç©¶ã€‚\n* æˆæœ¬æ¥è‡ªäºŽé›‡ä½£ç ”ç©¶äººå‘˜ã€è¿›è¡Œè°ƒæŸ¥ã€è®¾ç½®ç„¦ç‚¹å°ç»„ä»¥åŠåˆ†æžç»“æžœã€‚\n* ä¸­å°åž‹ä¼ä¸šå¯èƒ½ä¼šå‘çŽ°è¿™äº›æˆæœ¬è¿‡é«˜ï¼Œä»Žè€Œé™åˆ¶äº†èŽ·å–æ·±å…¥è§è§£çš„æœºä¼šã€‚\n\n### 2\\. è€—æ—¶çš„è¿‡ç¨‹\n\n* é€šè¿‡ä¼ ç»Ÿæ–¹æ³•æ”¶é›†ã€å¤„ç†å’Œåˆ†æžæ•°æ®å¯èƒ½éœ€è¦æ•°å‘¨ç”šè‡³æ•°æœˆçš„æ—¶é—´ã€‚\n* åœ¨å¿«é€Ÿå˜åŒ–çš„è¡Œä¸šä¸­ï¼Œé€šè¿‡ç¼“æ…¢çš„è¿‡ç¨‹èŽ·å¾—çš„è§è§£å¯èƒ½åœ¨å¯æ“ä½œä¹‹å‰å°±å·²ç»è¿‡æ—¶ã€‚\n* å¯¹å®žæ—¶æ•°æ®çš„éœ€æ±‚æ­£åœ¨å¢žåŠ ï¼Œä½†ä¼ ç»Ÿæ–¹æ³•éš¾ä»¥ä»¥è¿™æ ·çš„é€Ÿåº¦æä¾›è§è§£ã€‚\n\n### 3\\. æœ‰é™çš„æ ·æœ¬é‡å’ŒèŒƒå›´\n\n* ä¼ ç»Ÿç ”ç©¶å¸¸å¸¸å—åˆ°é¢„ç®—ã€æ—¶é—´å’ŒåŽå‹¤é™åˆ¶çš„çº¦æŸï¼Œè¿™å¯èƒ½é™åˆ¶æ ·æœ¬çš„å¤šæ ·æ€§å’Œå¤§å°ã€‚\n* ç„¦ç‚¹å°ç»„å’Œè®¿è°ˆå¯èƒ½æ— æ³•å……åˆ†ä»£è¡¨æ›´å¹¿æ³›çš„å—ä¼—ç¾¤ä½“ï¼Œä»Žè€Œå¯¼è‡´ç ”ç©¶ç»“æžœä¸­å¯èƒ½å­˜åœ¨ç›²ç‚¹ã€‚\n* è¿™ç§é™åˆ¶å½±å“äº†äººå£ç»Ÿè®¡ã€å¿ƒç†ç‰¹å¾å’Œè¡Œä¸ºæ´žå¯Ÿçš„å‡†ç¡®æ€§ï¼Œå°¤å…¶æ˜¯åœ¨å…¨çƒæ‰©å±•æ—¶ã€‚\n\n### 4\\. åˆ†æžéžç»“æž„åŒ–æ•°æ®çš„å›°éš¾\n\n* ä¼ ç»Ÿæ–¹æ³•é€šå¸¸é’ˆå¯¹ç»“æž„åŒ–æ•°æ®ï¼ˆä¾‹å¦‚ï¼Œè°ƒæŸ¥ä¸­çš„å“åº”ï¼‰ï¼Œè€Œä¸æ˜¯åƒç¤¾äº¤åª’ä½“å¸–å­ã€å®¢æˆ·è¯„è®ºæˆ–è®ºå›è®¨è®ºè¿™æ ·çš„éžç»“æž„åŒ–æ•°æ®ã€‚\n* ä»Žéžç»“æž„åŒ–æ•°æ®ä¸­èŽ·å–æœ‰ä»·å€¼çš„è§è§£éœ€è¦æ›´å¤æ‚çš„åˆ†æžï¼Œè€Œä¼ ç»Ÿæ–¹æ³•å¯èƒ½ç¼ºä¹æœ‰æ•ˆå¤„ç†æ‰€éœ€çš„èµ„æºæˆ–æŠ€æœ¯ã€‚\n\n### 5\\. æœ‰é™çš„çµæ´»æ€§å’Œé€‚åº”æ€§\n\n* ä¼ ç»Ÿçš„ç ”ç©¶è®¾è®¡é€šå¸¸æ˜¯å›ºå®šçš„ï¼Œè¿™ä½¿å¾—åœ¨ç ”ç©¶è¿‡ç¨‹ä¸­è¿›è¡Œè½¬å˜æˆ–è°ƒæ•´å˜å¾—å…·æœ‰æŒ‘æˆ˜æ€§ã€‚\n* ä¾‹å¦‚ï¼Œé’ˆå¯¹ä¸å¯é¢„è§çš„å¸‚åœºäº‹ä»¶æˆ–å˜åŒ–ï¼Œæ–°çš„ç ”ç©¶å¾€å¾€éœ€è¦ä»Žå¤´å¼€å§‹å¯åŠ¨ã€‚\n* è¿™ç§åƒµåŒ–å¯èƒ½ä¼šé˜»ç¢å“ç‰Œè¿…é€Ÿåº”å¯¹å¸‚åœºæ¡ä»¶æˆ–æ–°å…´è¶‹åŠ¿çš„èƒ½åŠ›ã€‚\n\n## ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½å¦‚ä½•æ”¹å˜å¸‚åœºç ”ç©¶ï¼Ÿ\n\nç”Ÿæˆæ€§äººå·¥æ™ºèƒ½å‡­å€Ÿå…¶å¤„ç†æµ·é‡æ•°æ®å’Œç”Ÿæˆæœ‰æ„ä¹‰æ´žå¯Ÿçš„èƒ½åŠ›ï¼Œæ­£åœ¨å½»åº•æ”¹å˜ä¼ ç»Ÿå¸‚åœºç ”ç©¶ã€‚é€šè¿‡è§£å†³ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ï¼Œç”Ÿæˆæ€§äººå·¥æ™ºèƒ½ä½¿å¾—å¸‚åœºæ´žå¯Ÿçš„èŽ·å–æ›´åŠ å¿«é€Ÿã€å…¨é¢å’Œå…·æœ‰æˆæœ¬æ•ˆç›Šã€‚ä»¥ä¸‹æ˜¯ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½å¦‚ä½•é‡å¡‘å¸‚åœºç ”ç©¶é¢†åŸŸçš„æ¦‚è¿°ï¼š\n\n### 1\\. æ•°æ®ç”Ÿæˆä¸Žå¢žå¼º\n\n* åˆæˆæ•°æ®åˆ›å»ºï¼šç”Ÿæˆå¼AIæ¨¡åž‹å¯ä»¥åˆ›å»ºæ¨¡æ‹ŸçœŸå®žä¸–ç•Œæ•°æ®çš„åˆæˆæ•°æ®é›†ï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜å…‹æœæ ·æœ¬å¤§å°å’Œå¤šæ ·æ€§æ–¹é¢çš„é™åˆ¶ã€‚\n* å¢žå¼ºæƒ…æ™¯å»ºæ¨¡ï¼šé€šè¿‡æ¨¡æ‹Ÿä¸åŒçš„å¸‚åœºæƒ…æ™¯ï¼ŒAIä½¿å…¬å¸èƒ½å¤ŸæŽ¢ç´¢â€œå¦‚æžœâ€æƒ…å†µï¼Œå¹¶æµ‹è¯•å¸‚åœºæ¡ä»¶å¦‚ä½•å½±å“æ¶ˆè´¹è€…è¡Œä¸ºæˆ–äº§å“æˆåŠŸã€‚\n\n### 2\\. å¿«é€Ÿå†…å®¹ç”Ÿæˆä»¥èŽ·å–æ´žå¯Ÿ\n\n* è‡ªåŠ¨åŒ–æŠ¥å‘Šæ’°å†™ï¼šç”Ÿæˆå¼AIå¯ä»¥ç”Ÿæˆè¯¦ç»†çš„å¸‚åœºç ”ç©¶æŠ¥å‘Šï¼Œå°†æ•°æ®æ€»ç»“æˆæ˜“äºŽç†è§£çš„å™è¿°ï¼Œä¾¿äºŽåˆ©ç›Šç›¸å…³è€…å¿«é€Ÿæ¶ˆåŒ–ã€‚\n* å®šåˆ¶åŒ–æ´žå¯Ÿï¼šåƒChatGPTæˆ–Jasperè¿™æ ·çš„AIå·¥å…·å¯ä»¥æ ¹æ®ç‰¹å®šçš„æ•°æ®è¾“å…¥å¿«é€Ÿç”Ÿæˆé‡èº«å®šåˆ¶çš„æ´žå¯Ÿï¼Œä¸ºå¸‚åœºè¥é”€äººå‘˜æä¾›é’ˆå¯¹ç‰¹å®šå—ä¼—çš„æŠ¥å‘Šã€ç«žäº‰å¯¹æ‰‹åˆ†æžå’Œå®¢æˆ·æ—…ç¨‹æ‘˜è¦ã€‚\n\n### 3\\. æƒ…æ„Ÿåˆ†æžä¸Žç¤¾äº¤è†å¬\n\n* å®žæ—¶æƒ…æ„Ÿç›‘æµ‹ï¼šAIæ¨¡åž‹å¯ä»¥åˆ†æžç¤¾äº¤åª’ä½“å¸–å­ã€è¯„è®ºå’Œè®ºå›ï¼Œä»¥æå–å®žæ—¶æƒ…æ„Ÿï¼Œè·Ÿè¸ªå®¢æˆ·å¯¹äº§å“ã€æœåŠ¡æˆ–å“ç‰Œçš„æ„Ÿå—ã€‚\n* è¶‹åŠ¿è¯†åˆ«ï¼šç”Ÿæˆå¼AIå¯ä»¥é€šè¿‡è¯†åˆ«å…³é”®è¯ã€ä¸»é¢˜å’Œæƒ…æ„ŸåŸºè°ƒï¼Œä»Žéžç»“æž„åŒ–æ•°æ®æºä¸­è¯†åˆ«æ–°å…´è¶‹åŠ¿ï¼Œå¸®åŠ©å…¬å¸æŽŒæ¡æ¶ˆè´¹è€…åå¥½ã€‚\n\n### 4\\. é«˜çº§ç”¨æˆ·è§’è‰²ä¸Žåœºæ™¯å¼€å‘\n\n* è¯¦ç»†çš„æ¶ˆè´¹è€…è§’è‰²ï¼šAIå¯ä»¥é€šè¿‡åˆ†æžå„ç§æ•°æ®é›†ä¸­çš„äººå£ç»Ÿè®¡ã€å¿ƒç†ç‰¹å¾å’Œè¡Œä¸ºæ¨¡å¼ï¼Œç”Ÿæˆé«˜åº¦ç»†è‡´çš„æ¶ˆè´¹è€…è§’è‰²ã€‚\n* å‡è®¾åœºæ™¯åˆ›å»ºï¼šç”Ÿæˆå¼AIä½¿å¾—åˆ›å»ºæ½œåœ¨æ¶ˆè´¹è€…åœºæ™¯æˆä¸ºå¯èƒ½ï¼Œä»¥æŽ¢è®¨å—ä¼—å¦‚ä½•å¯¹æ–°äº§å“ã€æœåŠ¡æˆ–ä¿¡æ¯å˜åŒ–ä½œå‡ºååº”ï¼Œä»Žè€Œä¸ºå…¬å¸æä¾›æ›´å…·åŠ¨æ€çš„å¸‚åœºæµ‹è¯•æ–¹æ³•ã€‚\n\n### 5\\. å¢žå¼ºçš„ç«žäº‰å¯¹æ‰‹å’Œè¡Œä¸šåˆ†æž\n\n* å®žæ—¶å¸‚åœºå®šä½ï¼šç”Ÿæˆæ€§äººå·¥æ™ºèƒ½å¯ä»¥ç›‘æŽ§ç«žäº‰å¯¹æ‰‹çš„è¡ŒåŠ¨ã€ä¿¡æ¯ä¼ é€’å’Œå®šä»·ç­–ç•¥ï¼Œä¸ºå…¬å¸æä¾›ä¸æ–­æ›´æ–°çš„å¸‚åœºå®šä½è§†å›¾ã€‚\n* å¸‚åœºè¶‹åŠ¿é¢„æµ‹åˆ†æžï¼šé€šè¿‡ä½¿ç”¨åŽ†å²æ•°æ®ï¼Œç”Ÿæˆæ€§äººå·¥æ™ºèƒ½å¯ä»¥é¢„æµ‹æœªæ¥çš„å¸‚åœºè¶‹åŠ¿ï¼Œå¸®åŠ©å…¬å¸é¢„è§æ¶ˆè´¹è€…åå¥½çš„å˜åŒ–ã€æ–°å…´äº§å“éœ€æ±‚æˆ–ç«žäº‰å®šä½çš„å˜åŒ–ã€‚\n\n### 6\\. ä¸ªæ€§åŒ–å®¢æˆ·äº’åŠ¨ä¸Žæ´žå¯Ÿ\n\n* å†…å®¹çš„è¶…ä¸ªæ€§åŒ–ï¼šAIä½¿å¾—ä¸ºå®¢æˆ·ç»†åˆ†ç”Ÿæˆé«˜åº¦ä¸ªæ€§åŒ–çš„å†…å®¹æˆä¸ºå¯èƒ½ï¼ŒåŒ…æ‹¬æ ¹æ®ä¸ªäººæ¶ˆè´¹è¡Œä¸ºå®šåˆ¶çš„å¹¿å‘Šæ–‡æ¡ˆå’Œäº§å“æŽ¨èã€‚\n* å¢žå¼ºçš„å®¢æˆ·åé¦ˆå¾ªçŽ¯ï¼šé€šè¿‡å¤„ç†å’Œç»¼åˆæ¥è‡ªå¤šä¸ªæ¸ é“çš„åé¦ˆï¼Œç”Ÿæˆå¼AIå¯ä»¥å‘çŽ°ç‰¹å®šçš„æ´žå¯Ÿï¼Œä»¥æ”¹å–„äº§å“å¼€å‘ã€å®¢æˆ·æœåŠ¡å’Œè¥é”€ä¿¡æ¯ã€‚\n\n### 7\\. è‡ªåŠ¨åŒ–è°ƒæŸ¥ä¸Žåé¦ˆåˆ†æž\n\n* è‡ªç„¶è¯­è¨€å¤„ç†ç”¨äºŽè°ƒæŸ¥åé¦ˆï¼šç”Ÿæˆå¼äººå·¥æ™ºèƒ½å¯ä»¥åˆ†æžå¼€æ”¾å¼è°ƒæŸ¥åé¦ˆï¼Œæ±‡æ€»æ¶ˆè´¹è€…æ„è§ï¼Œå¹¶è¯†åˆ«å¸¸è§ä¸»é¢˜ï¼Œæ— éœ€æ‰‹åŠ¨å¤„ç†ã€‚\n* åè§æ£€æµ‹ä¸Žä¿®æ­£ï¼šå¯ä»¥è®­ç»ƒäººå·¥æ™ºèƒ½æ¨¡åž‹æ£€æµ‹å¹¶è°ƒæ•´è°ƒæŸ¥åé¦ˆä¸­çš„åè§ï¼Œä½¿ç ”ç©¶ç»“æžœæ›´å…·ä»£è¡¨æ€§ã€‚\n\n## ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½åœ¨å¸‚åœºæƒ…æŠ¥ä¸­çš„å…³é”®åº”ç”¨\n\nç”Ÿæˆæ€§äººå·¥æ™ºèƒ½åœ¨å¸‚åœºæƒ…æŠ¥ä¸­å¼•å…¥äº†çªç ´æ€§çš„åº”ç”¨ï¼Œä½¿å…¬å¸èƒ½å¤Ÿä»¥æ›´é«˜çš„æ•ˆçŽ‡å’Œå¯æ‰©å±•æ€§èŽ·å¾—æ›´æ·±å±‚æ¬¡çš„å®žæ—¶æ´žå¯Ÿã€‚ä»¥ä¸‹æ˜¯ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜å¸‚åœºæƒ…æŠ¥çš„ä¸€äº›ä¸»è¦æ–¹å¼ï¼š\n\n### 1\\. è¶‹åŠ¿åˆ†æžä¸Žé¢„æµ‹\n\n* å¸‚åœºè¶‹åŠ¿è¯†åˆ«ï¼šç”Ÿæˆå¼AIæ¨¡åž‹åˆ†æžå¤§åž‹æ•°æ®é›†ï¼Œå¦‚ç¤¾äº¤åª’ä½“ã€æœç´¢æ•°æ®å’Œæ–°é—»ï¼Œä»¥åœ¨è¶‹åŠ¿æˆä¸ºä¸»æµä¹‹å‰è¯†åˆ«æ–°å…´è¶‹åŠ¿ã€‚\n* éœ€æ±‚é¢„æµ‹ï¼šåŸºäºŽåŽ†å²æ•°æ®å’Œå¸‚åœºæŒ‡æ ‡ï¼ŒAIé©±åŠ¨çš„æ¨¡åž‹é¢„æµ‹éœ€æ±‚å˜åŒ–ï¼Œä½¿å…¬å¸èƒ½å¤Ÿå°†å…¶æˆ˜ç•¥ä¸Žé¢„æœŸçš„æ¶ˆè´¹è€…éœ€æ±‚å¯¹é½ã€‚\n* æ–‡åŒ–å’Œç¤¾ä¼šè¶‹åŠ¿åˆ†æžï¼šAIæ‰«æå¹¶ç»¼åˆæµè¡Œæ–‡åŒ–ã€ç”Ÿæ´»æ–¹å¼å˜åŒ–å’Œç¤¾ä¼šé—®é¢˜ä¸­çš„è¶‹åŠ¿ï¼Œä»¥å¸®åŠ©å“ç‰Œä¸Žæ¶ˆè´¹è€…ä»·å€¼è§‚ä¿æŒä¸€è‡´ã€‚\n\n### 2\\. ç”¨æˆ·è§’è‰²ä¸Žåœºæ™¯å¼€å‘\n\n* è¯¦ç»†çš„æ¶ˆè´¹è€…è§’è‰²ï¼šç”Ÿæˆæ€§äººå·¥æ™ºèƒ½åŸºäºŽäººå£ç»Ÿè®¡ã€è¡Œä¸ºå’Œå¿ƒç†æ•°æ®åˆ›å»ºä¸°å¯Œçš„æ•°æ®é©±åŠ¨è§’è‰²ã€‚è¿™äº›è§’è‰²æœ‰åŠ©äºŽç²¾å‡†è¥é”€ã€äº§å“è®¾è®¡å’Œå®¢æˆ·ä½“éªŒè§„åˆ’ã€‚\n* åœºæ™¯è§„åˆ’ï¼šé€šè¿‡æ¨¡æ‹Ÿå„ç§å¸‚åœºæ¡ä»¶å’Œæ¶ˆè´¹è€…ååº”ï¼Œç”Ÿæˆæ€§äººå·¥æ™ºèƒ½ä½¿ä¼ä¸šèƒ½å¤Ÿå¯è§†åŒ–ä¸åŒç­–ç•¥çš„å½±å“ï¼Œä»Žè€Œå®žçŽ°æ›´ä¸ºæ˜Žæ™ºçš„å†³ç­–ã€‚\n\n### 3\\. å®žæ—¶æ¶ˆè´¹è€…åé¦ˆçš„æƒ…æ„Ÿåˆ†æž\n\n* å®¢æˆ·æƒ…æ„Ÿè·Ÿè¸ªï¼šç”Ÿæˆå¼AIå·¥å…·å¤„ç†æ¥è‡ªè¯„è®ºã€ç¤¾äº¤åª’ä½“å¸–å­å’Œè°ƒæŸ¥çš„éžç»“æž„åŒ–æ•°æ®ï¼Œä»¥å®žæ—¶è¯„ä¼°æ¶ˆè´¹è€…æƒ…æ„Ÿã€‚\n* æƒ…æ„Ÿå’Œè¡Œä¸ºæ´žå¯Ÿï¼šé€šè¿‡åˆ†æžå®¢æˆ·åé¦ˆä¸­çš„è¯­è¨€å’Œè¯­è°ƒï¼ŒAIå¯ä»¥æ­ç¤ºæ›´æ·±å±‚æ¬¡çš„æƒ…æ„Ÿé©±åŠ¨å› ç´ ï¼Œä½¿å“ç‰Œèƒ½å¤Ÿç›¸åº”åœ°è°ƒæ•´å…¶ä¿¡æ¯ä¼ é€’å’Œäº§å“å¼€å‘ã€‚\n* å“ç‰Œå¥åº·ç›‘æµ‹ï¼šAIç”Ÿæˆçš„æƒ…æ„Ÿåˆ†æžæä¾›äº†å¯¹å“ç‰Œè®¤çŸ¥çš„æŒç»­æ´žå¯Ÿï¼Œä½¿å…¬å¸èƒ½å¤Ÿå¿«é€Ÿå“åº”å…¬ä¼—èˆ†è®ºçš„å˜åŒ–ã€‚\n\n### 4\\. ç«žäº‰å¯¹æ‰‹ä¸Žè¡Œä¸šåˆ†æž\n\n* ç«žäº‰æƒ…æŠ¥ï¼šAIé©±åŠ¨çš„å·¥å…·è¿½è¸ªç«žäº‰å¯¹æ‰‹åœ¨å„ä¸ªæ¸ é“çš„è¡ŒåŠ¨ï¼Œåˆ†æžä»–ä»¬çš„å®šä»·ç­–ç•¥ã€äº§å“å‘å¸ƒå’Œå®¢æˆ·åé¦ˆã€‚è¿™å¸®åŠ©å“ç‰Œè°ƒæ•´å…¶ç­–ç•¥ä»¥ä¿æŒç«žäº‰åŠ›ã€‚\n* è¡Œä¸šåŸºå‡†ï¼šç”Ÿæˆå¼AIæ±‡æ€»è¡Œä¸šæ•°æ®ä»¥è®¾å®šåŸºå‡†ï¼Œä½¿ä¼ä¸šèƒ½å¤Ÿè¡¡é‡è‡ªèº«åœ¨è¡Œä¸šä¸­çš„è¡¨çŽ°ï¼Œå¹¶æ ¹æ®éœ€è¦è°ƒæ•´ç­–ç•¥ã€‚\n* é¢„æµ‹å¸‚åœºåŠ¨å‘ï¼šé€šè¿‡åŽ†å²æ•°æ®ï¼ŒAIé¢„æµ‹å¯èƒ½çš„ç«žäº‰å¯¹æ‰‹è¡ŒåŠ¨å’Œå¸‚åœºå˜åŒ–ï¼Œä½¿å…¬å¸èƒ½å¤Ÿä¸»åŠ¨è°ƒæ•´å…¶ç­–ç•¥ã€‚\n\n### 5\\. æŠ¥å‘Šå’Œæ´žå¯Ÿçš„å†…å®¹ç”Ÿæˆ\n\n* è‡ªåŠ¨åŒ–æ´žå¯Ÿæ‘˜è¦ï¼šAIå¯ä»¥è‡ªåŠ¨ç”Ÿæˆæ´žå¯ŸæŠ¥å‘Šï¼Œä»¥æ˜“äºŽæ¶ˆåŒ–çš„æ ¼å¼æ€»ç»“æ•°æ®ï¼Œä¸ºåˆ©ç›Šç›¸å…³è€…èŠ‚çœæ—¶é—´å’Œèµ„æºã€‚\n* å®šåˆ¶å¸‚åœºæƒ…æŠ¥æŠ¥å‘Šï¼šç”Ÿæˆæ€§AIç”Ÿæˆç‰¹å®šå¸‚åœºæˆ–ç»†åˆ†é¢†åŸŸçš„ä¸ªæ€§åŒ–æŠ¥å‘Šï¼Œæä¾›ä¸Žä¸šåŠ¡ç›®æ ‡ç´§å¯†å¯¹é½çš„é’ˆå¯¹æ€§æ´žå¯Ÿã€‚\n* ç¿»è¯‘å’Œæœ¬åœ°åŒ–ï¼šç”Ÿæˆæ€§AIå¯ä»¥è·¨è¯­è¨€å’Œæ–‡åŒ–èƒŒæ™¯ç¿»è¯‘å¸‚åœºæ´žå¯Ÿå’ŒæŠ¥å‘Šï¼Œä½¿ä¼ä¸šèƒ½å¤Ÿåœ¨å…¨çƒèŒƒå›´å†…æ‰©å±•å¸‚åœºæƒ…æŠ¥ã€‚\n\n### 6\\. å®¢æˆ·æ—…ç¨‹æ˜ å°„ä¸Žé¢„æµ‹æ´žå¯Ÿ\n\n* åŠ¨æ€å®¢æˆ·æ—…ç¨‹åˆ†æžï¼šAI ç»¼åˆå¤šä¸ªæŽ¥è§¦ç‚¹çš„æ•°æ®ï¼Œç»˜åˆ¶å®¢æˆ·æ—…ç¨‹ï¼Œçªå‡ºå®¢æˆ·ç”Ÿå‘½å‘¨æœŸä¸­çš„å…³é”®å†³ç­–ç‚¹å’Œåå¥½ã€‚\n* é¢„æµ‹æ¶ˆè´¹è€…è¡Œä¸ºï¼šåˆ©ç”¨åŽ†å²æ•°æ®ä¸­çš„æ¨¡å¼ï¼ŒAI é¢„æµ‹æœªæ¥çš„æ¶ˆè´¹è€…è¡Œä¸ºï¼Œä½¿å…¬å¸èƒ½å¤Ÿè°ƒæ•´å…¶ä¿¡æ¯ä¼ é€’ã€äº§å“æŽ¨èå’Œè¥é”€æ—¶æœºã€‚\n* æµå¤±é¢„æµ‹ä¸Žä¿ç•™åˆ†æžï¼šé€šè¿‡åˆ†æžæ¶ˆè´¹è€…äº’åŠ¨å’Œæ»¡æ„åº¦è¯„åˆ†ï¼ŒAI è¯†åˆ«é£Žé™©å®¢æˆ·ï¼Œå¸®åŠ©å…¬å¸é‡‡å–ä¸»åŠ¨çš„ä¿ç•™æŽªæ–½ã€‚\n\n### 7\\. äº§å“æž„æ€ä¸Žå¼€å‘\n\n* åŸºäºŽæ¶ˆè´¹è€…åå¥½çš„åˆ›æ„ç”Ÿæˆï¼šAIå¯ä»¥åˆ†æžæ¶ˆè´¹è€…åå¥½ã€åé¦ˆå’Œæµè¡Œå…³é”®è¯ï¼Œä»¥å»ºè®®äº§å“ç‰¹æ€§æˆ–å…¨æ–°äº§å“ï¼Œå¸®åŠ©æž„æ€è¿‡ç¨‹ã€‚\n* ç«žäº‰ç‰¹æ€§åˆ†æžï¼šé€šè¿‡è·Ÿè¸ªç«žäº‰å¯¹æ‰‹çš„äº§å“ç‰¹æ€§å’Œå®¢æˆ·åé¦ˆï¼Œç”Ÿæˆå¼AIå¸®åŠ©å“ç‰Œå®Œå–„äº§å“è·¯çº¿å›¾ï¼Œå¹¶ä¼˜å…ˆè€ƒè™‘ä¸Žç›®æ ‡å—ä¼—äº§ç”Ÿå…±é¸£çš„ç‰¹æ€§ã€‚\n* äº§å“æ¦‚å¿µæµ‹è¯•ï¼šç”Ÿæˆå¼AIæ¨¡æ‹Ÿå¸‚åœºå¯¹æ½œåœ¨äº§å“çš„ååº”ï¼Œä½¿å…¬å¸èƒ½å¤Ÿåœ¨è¿›è¡Œå…¨é¢å¼€å‘ä¹‹å‰å®Œå–„æ¦‚å¿µã€‚\n\n### 8\\. è¶…ä¸ªæ€§åŒ–ä¸Žç›®æ ‡å®šä½\n\n* åˆ†å±‚æ´»åŠ¨ï¼šç”Ÿæˆå¼AIä½¿å¸‚åœºè¥é”€äººå‘˜èƒ½å¤Ÿä¸ºç‰¹å®šå—ä¼—ç¾¤ä½“ç”Ÿæˆå®šåˆ¶åŒ–ä¿¡æ¯ï¼Œæé«˜ç›¸å…³æ€§å’Œå‚ä¸Žåº¦ã€‚\n* å®žæ—¶å¹¿å‘Šæ–‡æ¡ˆå’Œå†…å®¹å®šåˆ¶ï¼šAIæ¨¡åž‹å¯ä»¥æ ¹æ®ç”¨æˆ·è¡Œä¸ºå’Œåå¥½åŠ¨æ€è°ƒæ•´å¹¿å‘Šæ–‡æ¡ˆã€ç”µå­é‚®ä»¶å†…å®¹å’Œç½‘ç«™ä¿¡æ¯ï¼Œä½¿å¸‚åœºè¥é”€å·¥ä½œæ›´æœ‰æ•ˆã€‚\n* æŽ¨èå¼•æ“Žï¼šåˆ©ç”¨å®¢æˆ·è¡Œä¸ºå’Œæƒ…æ„Ÿï¼ŒAIåˆ›å»ºä¸ªæ€§åŒ–çš„äº§å“æŽ¨èï¼Œæé«˜é”€å”®å’Œå®¢æˆ·æ»¡æ„åº¦ã€‚\n\n## é‡åŒ–ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½åœ¨å¸‚åœºç ”ç©¶ä¸­çš„å¥½å¤„\n\nç”Ÿæˆæ€§äººå·¥æ™ºèƒ½æ­£åœ¨é€šè¿‡ä½¿æ•°æ®é©±åŠ¨çš„æ´žå¯Ÿæ›´åŠ å¯èŽ·å–ã€å‡†ç¡®å’Œå…·æœ‰æˆæœ¬æ•ˆç›Šæ¥å½»åº•æ”¹å˜å¸‚åœºç ”ç©¶ã€‚é‡åŒ–è¿™äº›å¥½å¤„æœ‰åŠ©äºŽä¼ä¸šäº†è§£äººå·¥æ™ºèƒ½å¦‚ä½•æé«˜ç ”ç©¶æ•ˆçŽ‡å¹¶å¢žå¼ºå…¶æŠ•èµ„å›žæŠ¥çŽ‡ï¼ˆROIï¼‰ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å…³é”®æŒ‡æ ‡å’Œç”Ÿæˆæ€§äººå·¥æ™ºèƒ½åœ¨å¸‚åœºç ”ç©¶ä¸­æŽ¨åŠ¨å¯è¡¡é‡å¥½å¤„çš„ç¤ºä¾‹ï¼š\n\n### 1\\. æˆæœ¬é™ä½Ž\n\n* é™ä½ŽåŠ³åŠ¨åŠ›æˆæœ¬ï¼šä¼ ç»Ÿç ”ç©¶é€šå¸¸éœ€è¦å¤§é‡äººåŠ›è¿›è¡Œæ•°æ®æ”¶é›†ã€åˆ†æžå’ŒæŠ¥å‘Šã€‚ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½è‡ªåŠ¨åŒ–äº†è®¸å¤šè¿™äº›ä»»åŠ¡ï¼Œæ˜¾è‘—é™ä½Žäº†åŠ³åŠ¨åŠ›æˆæœ¬ã€‚\n* é™ä½Žæ•°æ®æ”¶é›†è´¹ç”¨ï¼šç”Ÿæˆæ€§äººå·¥æ™ºèƒ½æ¨¡åž‹å¯ä»¥ä»¥ä¼ ç»Ÿè°ƒæŸ¥æ–¹æ³•æˆæœ¬çš„ä¸€å°éƒ¨åˆ†åˆ†æžç¤¾äº¤åª’ä½“ã€è®ºå›å’Œæ¶ˆè´¹è€…è¯„ä»·ç­‰å¤§é‡æ•°æ®æºã€‚\n* ç¤ºä¾‹ï¼šä¸€å®¶ä»¥å‰æ¯å¹´åœ¨å¸‚åœºç ”ç©¶ä¸ŠèŠ±è´¹$100,000çš„å…¬å¸ï¼Œé€šè¿‡ä½¿ç”¨ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½å·¥å…·ï¼Œå¯èƒ½èŠ‚çœé«˜è¾¾40%çš„è´¹ç”¨ï¼Œå°†å¼€æ”¯å‡å°‘åˆ°$60,000ï¼ŒåŒæ—¶èŽ·å¾—ç±»ä¼¼æˆ–æ›´ä¸°å¯Œçš„æ´žå¯Ÿã€‚\n\n### 2\\. æ—¶é—´èŠ‚çœä¸Žæ›´å¿«çš„æ´žå¯Ÿ\n\n* åŠ é€Ÿç ”ç©¶å‘¨æœŸï¼šAIå·¥å…·å®žæ—¶åˆ†æžæ•°æ®ï¼Œå°†èŽ·å–æ´žå¯Ÿæ‰€éœ€çš„æ—¶é—´ä»Žæ•°å‘¨æˆ–æ•°æœˆç¼©çŸ­è‡³æ•°å¤©ç”šè‡³æ•°å°æ—¶ã€‚\n* å¿«é€ŸæŠ¥å‘Šï¼šè‡ªåŠ¨åŒ–æŠ¥å‘Šç”Ÿæˆå‡å°‘äº†åˆ†æžæ—¶é—´ï¼Œä½¿åˆ©ç›Šç›¸å…³è€…èƒ½å¤Ÿæ›´æ—©èŽ·å–æ´žå¯Ÿå¹¶åšå‡ºæ›´å¿«é€Ÿçš„å†³ç­–ã€‚\n* ç¤ºä¾‹ï¼šä¸€å®¶æŽ¨å‡ºæ–°äº§å“çš„ä¼ä¸šå¯ä»¥å®žæ—¶èŽ·å–å®¢æˆ·è¯„ä»·å’Œç¤¾äº¤åª’ä½“çš„åé¦ˆï¼Œåˆ©ç”¨ç”Ÿæˆæ€§AIåœ¨24å°æ—¶å†…æä¾›æ´žå¯Ÿï¼Œè€Œä¼ ç»Ÿç±»ä¼¼åˆ†æžçš„å‘¨è½¬æ—¶é—´ä¸ºä¸‰å‘¨ã€‚\n\n### 3\\. å¢žå¼ºçš„æ•°æ®å‡†ç¡®æ€§å’Œä¸€è‡´æ€§\n\n* æ”¹å–„æ•°æ®è´¨é‡ï¼šç”Ÿæˆå¼äººå·¥æ™ºèƒ½å¤„ç†å¤§é‡æ•°æ®ï¼Œå¹¶åˆ©ç”¨å…ˆè¿›çš„åˆ†æžæŠ€æœ¯ä»¥æ¯”äººå·¥åˆ†æžæ›´é«˜çš„å‡†ç¡®æ€§è¯†åˆ«è¶‹åŠ¿ã€‚\n* å‡å°‘åè§ï¼šé€šè¿‡ç»¼åˆæ¥è‡ªä¸åŒæ¥æºçš„åé¦ˆï¼Œäººå·¥æ™ºèƒ½æœ€å°åŒ–å¯èƒ½å› å°åž‹æˆ–åŒè´¨æ ·æœ¬è€Œäº§ç”Ÿçš„åè§ã€‚\n* ç¤ºä¾‹ï¼šé€šè¿‡åˆ©ç”¨äººå·¥æ™ºèƒ½é©±åŠ¨çš„æƒ…æ„Ÿåˆ†æžï¼Œä¸€å®¶å…¬å¸å¯ä»¥å®žçŽ°æ¶ˆè´¹è€…æƒ…æ„Ÿè¿½è¸ªå‡†ç¡®æ€§æ¯”äººå·¥æ–¹æ³•æé«˜20%çš„ç›®æ ‡ï¼Œä»Žè€Œæ›´å¯é åœ°é¢„æµ‹å¸‚åœºå˜åŒ–ã€‚\n\n### 4\\. å¯æ‰©å±•æ€§ä¸Žçµæ´»æ€§\n\n* å¤„ç†å¤§æ•°æ®é›†ï¼šAIæ¨¡åž‹å¯ä»¥å¤„ç†åºžå¤§çš„æ•°æ®é›†ï¼Œè€Œä¸å—ä¼ ç»Ÿç ”ç©¶çš„å¯æ‰©å±•æ€§é™åˆ¶ï¼Œä½¿å…¬å¸èƒ½å¤Ÿåˆ†æžå¤§åž‹å’Œå¤šæ ·çš„æ•°æ®æ¥æºï¼Œä»¥èŽ·å¾—å…¨é¢çš„è§†è§’ã€‚\n* çµæ´»åº”ç”¨ï¼šç”Ÿæˆå¼AIå…è®¸ä¼ä¸šæ ¹æ®ä¸æ–­å˜åŒ–çš„ç›®æ ‡æˆ–æ–°æ•°æ®å®žæ—¶è°ƒæ•´ç ”ç©¶å‚æ•°ï¼Œæä¾›äº†ä¸€ç§ä¼ ç»Ÿæ–¹æ³•éš¾ä»¥å®žçŽ°çš„çµæ´»æ€§ã€‚\n* ç¤ºä¾‹ï¼šä¸€ä¸ªå›½é™…å“ç‰Œå¯ä»¥è¿…é€Ÿåœ¨å¤šä¸ªå›½å®¶å’Œè¯­è¨€ä¸­æ‰©å±•å…¶æƒ…æ„Ÿåˆ†æžï¼Œæ´žå¯ŸåŒºåŸŸæ¶ˆè´¹è€…å·®å¼‚ï¼Œè€Œæ— éœ€è¿›è¡Œå•ç‹¬çš„ã€åŠ³åŠ¨å¯†é›†åž‹çš„ç ”ç©¶ã€‚\n\n### 5\\. ä¸ªæ€§åŒ–ä¸Žé’ˆå¯¹æ€§æ´žå¯Ÿ\n\n* å®šåˆ¶æŠ¥å‘Šï¼šAIå¯ä»¥ä¸ºä¸åŒçš„ä¸šåŠ¡èŒèƒ½ï¼ˆä¾‹å¦‚ï¼Œè¥é”€ã€äº§å“å¼€å‘ã€å®¢æˆ·æœåŠ¡ï¼‰ç”Ÿæˆé‡èº«å®šåˆ¶çš„æ´žå¯Ÿï¼Œç¡®ä¿æ¯ä¸ªå›¢é˜Ÿæ‹¥æœ‰ç›¸å…³çš„æ•°æ®ä»¥æ”¯æŒå†³ç­–ã€‚\n* å®žæ—¶ç»†åˆ†ï¼šç”Ÿæˆå¼AIæ¨¡åž‹å¯ä»¥åŸºäºŽå®žæ—¶æ•°æ®å¯¹å®¢æˆ·è¿›è¡Œç»†åˆ†ï¼Œä»Žè€Œæä¾›æ›´ç²¾ç¡®çš„ç‰¹å®šå—ä¼—æ´žå¯Ÿã€‚\n* ç¤ºä¾‹ï¼šä¸€å®¶é›¶å”®å…¬å¸é€šè¿‡ä½¿ç”¨ç”Ÿæˆå¼AIåˆ›å»ºä¸Žé€šè¿‡AIé©±åŠ¨çš„å¸‚åœºç»†åˆ†è¯†åˆ«å‡ºçš„ç‹¬ç‰¹å®¢æˆ·åå¥½ç›¸ä¸€è‡´çš„è¶…ä¸ªæ€§åŒ–æ´»åŠ¨ï¼Œä½¿å¹¿å‘Šå‚ä¸Žåº¦æé«˜äº†30%ã€‚\n\n### 6\\. æˆ˜ç•¥è§„åˆ’çš„é¢„æµ‹å‡†ç¡®æ€§\n\n* æ”¹è¿›çš„é¢„æµ‹ï¼šåŸºäºŽAIçš„é¢„æµ‹åˆ†æžå¯ä»¥é«˜ç²¾åº¦åœ°é¢„æµ‹è¶‹åŠ¿ï¼Œå¸®åŠ©å…¬å¸é¢„è§å¸‚åœºå˜åŒ–å¹¶ä¸ºæ½œåœ¨çš„å¹²æ‰°åšå¥½å‡†å¤‡ã€‚\n* å¢žå¼ºçš„é£Žé™©ç®¡ç†ï¼šé¢„æµ‹å®¢æˆ·è¡Œä¸ºçš„AIæ¨¡åž‹å¸®åŠ©å…¬å¸ä¸»åŠ¨åº”å¯¹å®¢æˆ·æµå¤±ã€éœ€æ±‚å˜åŒ–æˆ–è´Ÿé¢å“ç‰Œæƒ…ç»ªç­‰æŒ‘æˆ˜ã€‚\n* ç¤ºä¾‹ï¼šä¸€å®¶åŸºäºŽè®¢é˜…çš„æœåŠ¡ä½¿ç”¨ç”Ÿæˆå¼AIï¼Œé€šè¿‡æ›´å‡†ç¡®åœ°é¢„æµ‹æµå¤±çŽ‡å¹¶å®žæ–½åŠæ—¶å¹²é¢„ï¼Œå®žçŽ°äº†25%çš„å®¢æˆ·ç•™å­˜çŽ‡æå‡ã€‚\n\n### 7\\. æ›´é«˜çš„æŠ•èµ„å›žæŠ¥çŽ‡æ¥è‡ªå¯æ“ä½œçš„æ´žå¯Ÿ\n\n* ä¿¡æ¯é©±åŠ¨çš„å†³ç­–ï¼šå¿«é€Ÿã€å‡†ç¡®ä¸”æ•°æ®ä¸°å¯Œçš„æ´žå¯Ÿä½¿å…¬å¸èƒ½å¤Ÿåšå‡ºç›´æŽ¥å½±å“æ”¶å…¥çš„å†³ç­–ï¼Œä»Žè€Œæé«˜å¸‚åœºç ”ç©¶æŠ•èµ„çš„æŠ•èµ„å›žæŠ¥çŽ‡ã€‚\n* å®žæ—¶è°ƒæ•´ï¼šé€šè¿‡å®žæ—¶å¸‚åœºæ•°æ®ï¼Œå…¬å¸å¯ä»¥å®žæ—¶ä¼˜åŒ–å…¶ç­–ç•¥ï¼Œç«‹å³å“åº”å¸‚åœºå˜åŒ–å¹¶æå‡ä¸šç»©ã€‚\n* ç¤ºä¾‹ï¼šæŸå“ç‰Œåˆ©ç”¨äººå·¥æ™ºèƒ½æ´žå¯Ÿåœ¨äº§å“å‘å¸ƒä¸­æœŸè°ƒæ•´äº§å“ç‰¹æ€§ï¼Œå¯¼è‡´å®¢æˆ·æ»¡æ„åº¦æé«˜15%ï¼Œç¬¬ä¸€å­£åº¦æ”¶å…¥å¢žé•¿10%ã€‚\n\n### å®šé‡å›žé¡¾ï¼šç”Ÿæˆæ€§äººå·¥æ™ºèƒ½åœ¨å¸‚åœºç ”ç©¶ä¸­çš„å®žé™…å½±å“\n\nç”Ÿæˆæ€§äººå·¥æ™ºèƒ½åœ¨å¸‚åœºç ”ç©¶çš„å¤šä¸ªé¢†åŸŸå…·æœ‰å¯é‡åŒ–çš„å¥½å¤„ï¼š\n\n* ç”±äºŽè‡ªåŠ¨åŒ–å’Œå‡å°‘å¯¹ä¼ ç»Ÿè°ƒæŸ¥çš„ä¾èµ–ï¼Œç ”ç©¶æˆæœ¬é™ä½Ž30â€“50%ã€‚\n* é€šè¿‡AIé©±åŠ¨çš„å®žæ—¶å¤„ç†å’Œè‡ªåŠ¨æŠ¥å‘Šï¼Œåˆ†æžæ—¶é—´å‡å°‘60â€“80%ã€‚\n* é€šè¿‡åˆ©ç”¨AIé©±åŠ¨çš„åˆ†æžå’Œå‡å°‘åè§ï¼Œæ•°æ®å‡†ç¡®æ€§æé«˜20â€“30%ã€‚\n* é€šè¿‡é¢„æµ‹æ€§AIæ¨¡åž‹é¢„æµ‹æ¶ˆè´¹è€…è¡Œä¸ºï¼Œå®¢æˆ·ç•™å­˜çŽ‡æé«˜25%ã€‚\n* é€šè¿‡åŸºäºŽå®žæ—¶æ•°æ®è¿›è¡Œæ›´ç²¾ç¡®å’Œçµæ´»çš„å†³ç­–ï¼Œæ”¶å…¥å¢žé•¿10â€“15%ã€‚\n\n## ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åœ¨å¸‚åœºç ”ç©¶ä¸­çš„æœªæ¥è¶‹åŠ¿\n\néšç€ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„ä¸æ–­å‘å±•ï¼Œå®ƒå°†ä¸ºå¸‚åœºç ”ç©¶é¢†åŸŸå¸¦æ¥å˜é©æ€§çš„å˜åŒ–ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›é¢„è®¡å°†å¡‘é€ AIé©±åŠ¨å¸‚åœºæƒ…æŠ¥æœªæ¥çš„æ–°å…´è¶‹åŠ¿ï¼š\n\n### 1\\. å®žæ—¶ã€æŒç»­çš„å¸‚åœºç›‘æµ‹\n\n* è¶‹åŠ¿æ¦‚è¿°ï¼šä¸Žä¾èµ–å®šæœŸè°ƒæŸ¥å’ŒæŠ¥å‘Šä¸åŒï¼Œç”Ÿæˆå¼äººå·¥æ™ºèƒ½å°†å®žçŽ°å¯¹æ¶ˆè´¹è€…æƒ…ç»ªã€ç«žäº‰å¯¹æ‰‹è¡Œä¸ºå’Œå¸‚åœºè¶‹åŠ¿çš„æŒç»­å®žæ—¶ç›‘æµ‹ã€‚\n* å½±å“ï¼šè¿™ç§è½¬å˜ä½¿å“ç‰Œèƒ½å¤Ÿå³æ—¶å¯¹å¸‚åœºå˜åŒ–ä½œå‡ºååº”ï¼Œæœ€å°åŒ–æ•°æ®æ”¶é›†ä¸Žè¡ŒåŠ¨ä¹‹é—´çš„å·®è·ã€‚æŒç»­çš„æ´žå¯Ÿä¹Ÿæœ‰åŠ©äºŽæ›´å¥½çš„é¢„æµ‹å’Œçµæ´»çš„å†³ç­–ã€‚\n* ç¤ºä¾‹ï¼šä¸€å®¶æ—¶å°šé›¶å”®å•†å¯ä»¥å®žæ—¶è·Ÿè¸ªæ¶ˆè´¹è€…åå¥½çš„å˜åŒ–ï¼Œå¹¶å‡ ä¹Žç«‹å³è°ƒæ•´åº“å­˜ã€è¥é”€å’Œå®šä»·ç­–ç•¥ã€‚\n\n### 2\\. é€šè¿‡è¶…ç‰¹å®šå—ä¼—ç»†åˆ†å®žçŽ°ä¸ªæ€§åŒ–å¢žå¼º\n\n* è¶‹åŠ¿æ¦‚è¿°ï¼šç”Ÿæˆå¼äººå·¥æ™ºèƒ½å°†æå‡æ ¹æ®è¡Œä¸ºã€äººå£ç»Ÿè®¡å’Œå¿ƒç†ç‰¹å¾æ•°æ®å°†å—ä¼—ç»†åˆ†ä¸ºé«˜åº¦ç‰¹å®šç»†åˆ†å¸‚åœºçš„èƒ½åŠ›ã€‚\n* å½±å“ï¼šè¥é”€äººå‘˜å°†èƒ½å¤Ÿé’ˆå¯¹å¾®ç»†åˆ†å¸‚åœºå®šåˆ¶ä¿¡æ¯å’Œäº§å“ï¼Œæé«˜ä¸Žæ¯ä¸ªæ¶ˆè´¹è€…çš„ç›¸å…³æ€§å’Œå‚ä¸Žåº¦ã€‚\n* ç¤ºä¾‹ï¼šäººå·¥æ™ºèƒ½å¯ä»¥ä¸ºç‰¹å®šç¾¤ä½“åˆ›å»ºä¸ªæ€§åŒ–æ´»åŠ¨ï¼Œä¾‹å¦‚å¯¹å¯æŒç»­æ—¶å°šæ„Ÿå…´è¶£çš„ç”Ÿæ€æ„è¯†åƒç¦§ä¸€ä»£ï¼Œä»Žè€Œä¼˜åŒ–å‚ä¸Žåº¦å’Œè½¬åŒ–çŽ‡ã€‚\n\n### 3\\. ä½¿ç”¨å¤šæ¨¡æ€æ•°æ®é›†æˆå¢žå¼ºé¢„æµ‹èƒ½åŠ›\n\n* è¶‹åŠ¿æ¦‚è¿°ï¼šæœªæ¥çš„ç”Ÿæˆæ€§AIå·¥å…·å°†åˆ†æžå¤šæ¨¡æ€æ•°æ®æºâ€”â€”æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘å’Œè¯­éŸ³â€”â€”ä»¥æä¾›æ›´æ·±å…¥çš„æ´žå¯Ÿå’Œæ›´å‡†ç¡®çš„é¢„æµ‹ã€‚\n* å½±å“ï¼šæ•´åˆå¤šç§æ•°æ®ç±»åž‹æä¾›äº†å¸‚åœºè¶‹åŠ¿å’Œæ¶ˆè´¹è€…åå¥½çš„æ›´å…¨é¢è§†è§’ï¼Œä»Žè€Œèƒ½å¤Ÿè¿›è¡Œæ›´ç»†è‡´çš„é¢„æµ‹ã€‚\n* ç¤ºä¾‹ï¼šä¸€ä¸ªç¾Žå®¹å“ç‰Œå¯ä»¥åˆ†æžç¤¾äº¤åª’ä½“å›¾åƒã€æ–‡æœ¬å’Œå½±å“è€…è§†é¢‘ï¼Œä»¥é¢„æµ‹å³å°†åˆ°æ¥çš„å­£èŠ‚æµè¡Œçš„é¢œè‰²å’Œé£Žæ ¼ã€‚\n\n### 4\\. å¸‚åœºæ¨¡æ‹Ÿä¸Žæµ‹è¯•çš„åˆæˆæ•°æ®ç”Ÿæˆ\n\n* è¶‹åŠ¿æ¦‚è¿°ï¼šç”Ÿæˆå¼äººå·¥æ™ºèƒ½å°†è¶Šæ¥è¶Šå¤šåœ°ç”¨äºŽåˆ›å»ºæ¨¡æ‹ŸçœŸå®žä¸–ç•Œæ¶ˆè´¹è€…è¡Œä¸ºçš„åˆæˆæ•°æ®ï¼Œä½¿å“ç‰Œèƒ½å¤Ÿåœ¨ä¸å°†äº§å“å’Œæ´»åŠ¨å‘å¸ƒåˆ°å¸‚åœºçš„æƒ…å†µä¸‹è¿›è¡Œæµ‹è¯•ã€‚\n* å½±å“ï¼šåˆæˆæ•°æ®å¯ä»¥é€šè¿‡å…è®¸å“ç‰Œåœ¨æ¨¡æ‹ŸçŽ¯å¢ƒä¸­è¯„ä¼°æ¶ˆè´¹è€…ååº”å’Œä¼˜åŒ–æ´»åŠ¨ï¼Œä»Žè€Œé™ä½Žå¸‚åœºæµ‹è¯•çš„é£Žé™©å’Œæˆæœ¬ã€‚\n* ç¤ºä¾‹ï¼šä¸€ä¸ªæ–°çš„é¥®æ–™å“ç‰Œå¯èƒ½ä¼šä½¿ç”¨åˆæˆæ•°æ®æ¥æµ‹è¯•æ¶ˆè´¹è€…å¯¹ä¸åŒåŒ…è£…è®¾è®¡çš„ååº”ï¼Œä»Žè€Œå‡å°‘ä¸Žä¼ ç»Ÿç„¦ç‚¹å°ç»„ç›¸å…³çš„æ—¶é—´å’Œæˆæœ¬ã€‚\n\n### 5\\. è‡ªåŠ¨åŒ–æ´žå¯Ÿä¸Žå†³ç­–æ”¯æŒ\n\n* è¶‹åŠ¿æ¦‚è¿°ï¼šç”Ÿæˆå¼äººå·¥æ™ºèƒ½å°†æœç€è‡ªä¸»æ´žå¯Ÿç”Ÿæˆçš„æ–¹å‘å‘å±•ï¼Œå·¥å…·èƒ½å¤ŸåŸºäºŽå®žæ—¶å¸‚åœºæ•°æ®æŽ¨èè¡ŒåŠ¨ã€‚\n* å½±å“ï¼šè‡ªåŠ¨åŒ–æ´žå¯Ÿé‡Šæ”¾äº†èµ„æºï¼Œä½¿ä¼ä¸šèƒ½å¤Ÿåšå‡ºæ›´å¿«é€Ÿã€åŸºäºŽæ•°æ®çš„å†³ç­–ã€‚\n* ç¤ºä¾‹ï¼šä¸€ä¸ªäººå·¥æ™ºèƒ½å·¥å…·ä¸ä»…å¯èƒ½æ ‡è®°å‡ºæ¶ˆè´¹è€…æƒ…ç»ªçš„ä¸‹é™ï¼Œè¿˜å¯èƒ½å»ºè®®è¿›è¡Œè¥é”€è°ƒæ•´æˆ–äº§å“æ›´æ–°ä»¥åº”å¯¹è¯¥é—®é¢˜ã€‚\n\n### 6\\. ä¼¦ç†äººå·¥æ™ºèƒ½ä¸Žå¢žå¼ºçš„æ•°æ®éšç§\n\n* è¶‹åŠ¿æ¦‚è¿°ï¼šéšç€å¯¹ä¼¦ç†äººå·¥æ™ºèƒ½çš„é‡è§†ï¼Œæœªæ¥çš„å¸‚åœºç ”ç©¶å·¥å…·å°†ä¼˜å…ˆè€ƒè™‘é€æ˜Žåº¦ã€è´Ÿè´£ä»»çš„æ•°æ®ä½¿ç”¨å’Œéµå®ˆéšç§æ³•è§„ã€‚\n* å½±å“ï¼šæ¶ˆè´¹è€…åœ¨ä¸Žå®žè·µä¼¦ç†äººå·¥æ™ºèƒ½çš„å“ç‰Œåˆ†äº«æ•°æ®æ—¶ä¼šæ„Ÿåˆ°æ›´æœ‰ä¿¡å¿ƒï¼ŒåŒæ—¶å“ç‰Œä¹Ÿèƒ½é¿å…ä¸Žæ•°æ®æ»¥ç”¨ç›¸å…³çš„æ³•å¾‹å’Œå£°èª‰é£Žé™©ã€‚\n* ç¤ºä¾‹ï¼šç”Ÿæˆå¼äººå·¥æ™ºèƒ½å·¥å…·å¯èƒ½ä¼šçº³å…¥ç¡®ä¿éµå®ˆéšç§æ³•å¾‹çš„åŠŸèƒ½ï¼Œå¹¶å…è®¸æ¶ˆè´¹è€…åŒæ„æ•°æ®çš„æ”¶é›†å’Œä½¿ç”¨ã€‚\n\n### 7\\. å¯¹è¯å¼äººå·¥æ™ºèƒ½ä»¥èŽ·å–æ›´æ·±å…¥çš„äº’åŠ¨æ¶ˆè´¹è€…æ´žå¯Ÿ\n\n* è¶‹åŠ¿æ¦‚è¿°ï¼šå¯¹è¯å¼äººå·¥æ™ºèƒ½å°†å‘å±•ä»¥ä¿ƒè¿›åŠ¨æ€ã€äº’åŠ¨çš„è°ƒæŸ¥å’Œåé¦ˆæ”¶é›†ï¼Œä½¿å“ç‰Œèƒ½å¤Ÿç›´æŽ¥ä»Žæ¶ˆè´¹è€…é‚£é‡ŒèŽ·å–æ›´æ·±å…¥çš„æ´žå¯Ÿã€‚\n* å½±å“ï¼šå®žæ—¶çš„å¯¹è¯åé¦ˆå…è®¸æ›´çœŸå®žçš„æ´žå¯Ÿå’Œæ›´é«˜çš„å‚ä¸ŽçŽ‡ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­—åŽŸç”Ÿæ¶ˆè´¹è€…ä¸­ã€‚\n* ç¤ºä¾‹ï¼šä¸€ä¸ªç§‘æŠ€å“ç‰Œå¯ä»¥éƒ¨ç½²å¯¹è¯å¼äººå·¥æ™ºèƒ½ä¸Žæ¶ˆè´¹è€…è¿›è¡Œå¯¹è¯ï¼Œæ”¶é›†é™æ€è°ƒæŸ¥å¯èƒ½é—æ¼çš„ç»†å¾®æ´žå¯Ÿã€‚\n\n### 8\\. å¢žå¼ºçŽ°å®žï¼ˆARï¼‰å’Œè™šæ‹ŸçŽ°å®žï¼ˆVRï¼‰å¸‚åœºç ”ç©¶\n\n* è¶‹åŠ¿æ¦‚è¿°ï¼šåŸºäºŽç”Ÿæˆæ€§äººå·¥æ™ºèƒ½çš„ARå’ŒVRçŽ¯å¢ƒå°†æˆä¸ºæ²‰æµ¸å¼å¸‚åœºç ”ç©¶çš„å¸¸ç”¨å·¥å…·ï¼Œä½¿æ¶ˆè´¹è€…èƒ½å¤Ÿä¸Žè™šæ‹Ÿäº§å“å’ŒçŽ¯å¢ƒäº’åŠ¨ã€‚\n* å½±å“ï¼šè¯¥æŠ€æœ¯ä½¿å…¬å¸èƒ½å¤Ÿåœ¨é€¼çœŸçš„è™šæ‹ŸçŽ¯å¢ƒä¸­æµ‹è¯•äº§å“æ¦‚å¿µï¼Œä»¥å¼•äººå…¥èƒœçš„æ–¹å¼æ”¶é›†è¡Œä¸ºæ•°æ®å’Œæ¶ˆè´¹è€…åé¦ˆã€‚\n* ç¤ºä¾‹ï¼šä¸€å®¶å®¶å…·é›¶å”®å•†å¯ä»¥ä½¿ç”¨VRçŽ¯å¢ƒå‘å®¢æˆ·å±•ç¤ºå•†å“åœ¨ä»–ä»¬è‡ªå·±ç©ºé—´ä¸­çš„æ ·å­ï¼Œæ”¶é›†å…³äºŽäº§å“è®¾è®¡å’Œå¯ç”¨æ€§çš„åé¦ˆã€‚\n\n### 9\\. äººå·¥æ™ºèƒ½ä¸Žäººç±»åä½œç ”ç©¶æ¨¡åž‹\n\n* è¶‹åŠ¿æ¦‚è¿°ï¼šæœªæ¥çš„å¸‚åœºç ”ç©¶å¯èƒ½ä¼šæ¶‰åŠå°†äººç±»ä¸“ä¸šçŸ¥è¯†ä¸Žäººå·¥æ™ºèƒ½é©±åŠ¨çš„è‡ªåŠ¨åŒ–ç›¸ç»“åˆçš„æ··åˆæ¨¡åž‹ï¼Œç ”ç©¶äººå‘˜è´Ÿè´£ç›‘ç£äººå·¥æ™ºèƒ½ç”Ÿæˆçš„æ´žå¯Ÿï¼Œå¹¶å¯¹å‘çŽ°è¿›è¡Œæƒ…å¢ƒåŒ–å¤„ç†ã€‚\n* å½±å“ï¼šäººç±»çš„ç›‘ç£ç¡®ä¿äº†äººå·¥æ™ºèƒ½ç”Ÿæˆçš„æ´žå¯Ÿçš„è´¨é‡å’Œä¼¦ç†ä½¿ç”¨ï¼Œå¢žåŠ äº†ä¸€å±‚åˆ¤æ–­å’Œä¸“ä¸šçŸ¥è¯†ï¼Œè€Œä»…é äººå·¥æ™ºèƒ½å¯èƒ½ç¼ºä¹è¿™äº›ã€‚\n* ç¤ºä¾‹ï¼šäººå·¥æ™ºèƒ½å¯èƒ½ç”Ÿæˆå…³äºŽæ–°å…´å¸‚åœºè¶‹åŠ¿çš„æŠ¥å‘Šï¼Œè€Œç ”ç©¶äººå‘˜åˆ™éªŒè¯å‘çŽ°å¹¶æ·»åŠ å®šæ€§æ´žå¯Ÿï¼Œç¡®ä¿å¯æ“ä½œå’Œä¼¦ç†çš„å†³ç­–ã€‚\n\n### 10\\. ç«¯åˆ°ç«¯ AI å¹³å°ç”¨äºŽç»¼åˆå¸‚åœºæƒ…æŠ¥\n\n* è¶‹åŠ¿æ¦‚è¿°ï¼šéšç€ç”Ÿæˆæ€§ AI å·¥å…·çš„å‘å±•ï¼Œè¶Šæ¥è¶Šå¤šçš„å¹³å°å°†æä¾›ç«¯åˆ°ç«¯çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬æ•°æ®æ”¶é›†ã€åˆ†æžã€æ´žå¯Ÿç”Ÿæˆå’Œæˆ˜ç•¥å»ºè®®â€”â€”æ‰€æœ‰è¿™äº›éƒ½åœ¨ä¸€ä¸ªå•ä¸€çš„ç”Ÿæ€ç³»ç»Ÿä¸­ã€‚\n* å½±å“ï¼šè¿™äº›ç»¼åˆå¹³å°ç®€åŒ–äº†å¸‚åœºç ”ç©¶å·¥ä½œæµç¨‹ï¼Œå‡å°‘äº†å¯¹å¤šä¸ªå·¥å…·çš„ä¾èµ–ï¼Œä½¿å“ç‰Œæ›´å®¹æ˜“èŽ·å–å…¨é¢çš„ã€å¯æ“ä½œçš„æ´žå¯Ÿã€‚\n* ç¤ºä¾‹ï¼šä¸€ä¸ªç»¼åˆå¹³å°å¯ä»¥å¤„ç†ä»Žç¤¾äº¤è†å¬å’Œç«žäº‰å¯¹æ‰‹åˆ†æžåˆ°å®¢æˆ·ç»†åˆ†å’Œæ´»åŠ¨å»ºè®®çš„æ‰€æœ‰å†…å®¹ï¼Œè®©å“ç‰Œä¸“æ³¨äºŽæ‰§è¡Œè€Œä¸æ˜¯æ•°æ®å¤„ç†ã€‚\n\n## ç»“è®º\n\nå°†ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½èžå…¥å¸‚åœºç ”ç©¶å’Œæƒ…æŠ¥æµç¨‹ï¼Œä½¿ç»„ç»‡èƒ½å¤Ÿä»¥ç©ºå‰çš„é€Ÿåº¦èŽ·å¾—å¯æ“ä½œçš„æ´žå¯Ÿã€‚é€šè¿‡è‡ªåŠ¨åŒ–æ•°æ®æ”¶é›†å’Œåˆ†æžï¼Œç”Ÿæˆæ€§äººå·¥æ™ºèƒ½å‡å°‘äº†äººä¸ºé”™è¯¯ï¼Œæé«˜äº†ç²¾ç¡®åº¦ï¼Œå¹¶å¼€è¾Ÿäº†æ›¾ç»å› æ—¶é—´é™åˆ¶è€Œæ— æ³•æŽ¢ç´¢çš„æ–°å¸‚åœºå‘çŽ°é€”å¾„ã€‚è¿™äº›å·¥å…·ä¸ä»…ç®€åŒ–äº†ç ”ç©¶è¿‡ç¨‹ï¼Œè¿˜ä¸°å¯Œäº†å†³ç­–è¿‡ç¨‹ï¼Œä½¿å…¬å¸èƒ½å¤Ÿåœ¨å¿«é€Ÿå˜åŒ–çš„å¸‚åœºä¸­ä¿æŒé€‚åº”æ€§ã€‚\n\néšç€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„è¿›æ­¥ï¼Œç”Ÿæˆæ€§äººå·¥æ™ºèƒ½å¯èƒ½ä¼šæˆä¸ºå¸‚åœºç ”ç©¶ä¸å¯æˆ–ç¼ºçš„ä¸€éƒ¨åˆ†ã€‚é‡‡ç”¨ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½çš„å…¬å¸å¯ä»¥æœŸå¾…åœ¨é¢„æµ‹å‡†ç¡®æ€§ã€ç«žäº‰å¯¹æ‰‹åˆ†æžå’Œå®¢æˆ·ç†è§£æ–¹é¢æ˜¾è‘—æ”¹å–„ã€‚ç„¶è€Œï¼ŒæˆåŠŸçš„é‡‡ç”¨éœ€è¦ä¸€ç§æˆ˜ç•¥æ–¹æ³•ï¼Œä¼˜å…ˆè€ƒè™‘æœ€ç›¸å…³çš„äººå·¥æ™ºèƒ½å·¥å…·ï¼Œå¹¶æ•´åˆäººç±»ä¸“ä¸šçŸ¥è¯†ä»¥ç¡®ä¿è´¨é‡æŽ§åˆ¶å’Œè§£è¯»ã€‚ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½ä»£è¡¨äº†ä¸€ç§åŠ¨æ€å’Œä¸æ–­å‘å±•çš„å·¥å…·é›†ï¼Œå½“æœ‰æ•ˆä½¿ç”¨æ—¶ï¼Œå¯ä»¥å¡‘é€ å¸‚åœºæƒ…æŠ¥çš„æœªæ¥å¹¶æŽ¨åŠ¨å¯æŒç»­å¢žé•¿ã€‚\n\n## å¸¸è§é—®é¢˜è§£ç­”\n\n1. ä»€ä¹ˆæ˜¯ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼Œå®ƒå¦‚ä½•åº”ç”¨äºŽå¸‚åœºç ”ç©¶ï¼Ÿ\nç”Ÿæˆå¼äººå·¥æ™ºèƒ½åˆ©ç”¨æœºå™¨å­¦ä¹ åˆ†æžå¤§åž‹æ•°æ®é›†å¹¶ç”Ÿæˆæ´žå¯Ÿï¼Œä»Žè€Œå®žçŽ°æ›´å¿«é€Ÿã€æ›´å…¨é¢çš„å¸‚åœºç ”ç©¶å’Œé¢„æµ‹åˆ†æžã€‚\n2. åœ¨å¸‚åœºæƒ…æŠ¥ä¸­ä½¿ç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æœ‰å“ªäº›å¥½å¤„ï¼Ÿ\nç”Ÿæˆå¼äººå·¥æ™ºèƒ½æé«˜äº†æ•°æ®å‡†ç¡®æ€§ï¼Œå‡å°‘äº†æ•°æ®æ”¶é›†æ‰€èŠ±è´¹çš„æ—¶é—´ï¼Œå¹¶é€šè¿‡æä¾›æ›´æ·±å…¥çš„æ´žå¯Ÿå’Œè¶‹åŠ¿é¢„æµ‹æ¥å¢žå¼ºå†³ç­–èƒ½åŠ›ã€‚\n3. ç”Ÿæˆå¼äººå·¥æ™ºèƒ½èƒ½å¦å–ä»£ä¼ ç»Ÿå¸‚åœºç ”ç©¶æ–¹æ³•ï¼Ÿ\nè™½ç„¶ç”Ÿæˆå¼äººå·¥æ™ºèƒ½å¯ä»¥è‡ªåŠ¨åŒ–è®¸å¤šæ–¹é¢ï¼Œä½†äººç±»ä¸“ä¸šçŸ¥è¯†åœ¨ä¸Šä¸‹æ–‡è§£é‡Šã€è´¨é‡æŽ§åˆ¶å’Œç»†è‡´åˆ†æžä¸­ä»ç„¶è‡³å…³é‡è¦ã€‚\n4. ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åœ¨å¸‚åœºç ”ç©¶ä¸­çš„ä¸€äº›åº”ç”¨æ¡ˆä¾‹æ˜¯ä»€ä¹ˆï¼Ÿ\nåº”ç”¨æ¡ˆä¾‹åŒ…æ‹¬è¶‹åŠ¿é¢„æµ‹ã€ç«žäº‰å¯¹æ‰‹åˆ†æžã€å®¢æˆ·æƒ…æ„Ÿåˆ†æžï¼Œä»¥åŠé€šè¿‡è‡ªç„¶è¯­è¨€å¤„ç†ç”Ÿæˆå¸‚åœºæ´žå¯Ÿã€‚\n5. ä¼ä¸šå¦‚ä½•å¼€å§‹åœ¨å¸‚åœºç ”ç©¶ä¸­ä½¿ç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼Ÿ\né¦–å…ˆè¯†åˆ«ç ”ç©¶ä¸­AIå¯ä»¥æå‡çš„å…³é”®é¢†åŸŸï¼Œç„¶åŽé€‰æ‹©åˆé€‚çš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½å·¥å…·ï¼Œå¹¶åœ¨ä¸“å®¶ç›‘ç£ä¸‹å°†å…¶é›†æˆåˆ°çŽ°æœ‰å·¥ä½œæµç¨‹ä¸­ã€‚\n\n"},{"lang":"zh","group":"blog","slug":"blog/get-chatgpt-to-sound-more-human-essential-tips-for-creating-natural-engaging-ai-conversations-c361bc2680bb","frontmatter":{"title":"è®© ChatGPT å¬èµ·æ¥æ›´æœ‰äººæƒ…å‘³ï¼šåˆ›å»ºè‡ªç„¶ã€å¼•äººå…¥èƒœçš„äººå·¥æ™ºèƒ½å¯¹è¯çš„åŸºæœ¬æŠ€å·§","meta_title":"è®© ChatGPT å¬èµ·æ¥æ›´æœ‰äººæƒ…å‘³ï¼šåˆ›å»ºè‡ªç„¶ã€å¼•äººå…¥èƒœçš„äººå·¥æ™ºèƒ½å¯¹è¯çš„åŸºæœ¬æŠ€å·§","description":"ä¸ºäº†è®© ChatGPT çš„å¯¹è¯æ›´è‡ªç„¶ä¸”å¼•äººå…¥èƒœï¼Œå¯ä»¥é‡‡å–ä»¥ä¸‹ç­–ç•¥ï¼šé™åˆ¶è¿‡åº¦ä½¿ç”¨çš„è¯æ±‡å’ŒçŸ­è¯­ï¼Œä¿æŒè¯­è¨€ç®€å•æ˜Žäº†ï¼Œè°ƒæ•´è¯­æ°”ä»¥åŒ¹é…ä¸Šä¸‹æ–‡ï¼Œå¢žå¼ºäº’åŠ¨æ€§ä¸Žäº²å’ŒåŠ›ï¼Œå¹¶ä½¿ç”¨çŽ°å®žä¸–ç•Œçš„ä¾‹å­å’Œç›´æŽ¥é™ˆè¿°ã€‚è¿™äº›æ–¹æ³•å¯ä»¥æœ‰æ•ˆå‡å°‘æœºæ¢°æ„Ÿï¼Œä½¿ AI çš„å›žåº”æ›´å…·äººæ€§åŒ–å’Œå®žç”¨æ€§ã€‚","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*wdWBBG4fJhHVwdoDelYkkQ.png","categories":["Chatbots","Natural Language Processing","Programming/Scripting"],"author":"Rifx.Online","tags":["ChatGPT","responses","human","engaging","clarity"],"draft":false,"slug":"blog/get-chatgpt-to-sound-more-human-essential-tips-for-creating-natural-engaging-ai-conversations-c361bc2680bb"},"content":"\n\n\n\n\nä½ æ˜¯å¦å‘çŽ°ä½ çš„ AI åŠ©æ‰‹å¬èµ·æ¥æœ‰ç‚¹è¿‡äºŽâ€¦â€¦æœºæ¢°ï¼Ÿè™½ç„¶ ChatGPT çš„èƒ½åŠ›ä»¤äººå°è±¡æ·±åˆ»ï¼Œä½†æœ‰æ—¶å®ƒçš„è¯­è¨€è¿‡äºŽæ­£å¼æˆ–é€šç”¨ã€‚ä½†é€šè¿‡ä¸€äº›è°ƒæ•´ï¼Œä½ å¯ä»¥å¼•å¯¼ ChatGPT ç»™å‡ºæ›´äººæ€§åŒ–ã€å¯¹è¯å¼å’Œæ˜“äºŽå…±é¸£çš„å›žåº”ã€‚\n\nä»¥ä¸‹æ˜¯ä¸€ä¸ªå®žç”¨æŒ‡å—ï¼Œå¸®åŠ© ChatGPT å¬èµ·æ¥ä¸é‚£ä¹ˆåƒæœºå™¨äººï¼Œè€Œæ›´åƒä¸€ä¸ªåšå­¦çš„æœ‹å‹ã€‚\n\n## 1\\. é™åˆ¶è¿‡åº¦ä½¿ç”¨çš„è¯æ±‡å’ŒçŸ­è¯­\n\næŸäº›è¯æ±‡å’ŒçŸ­è¯­åœ¨AIç”Ÿæˆçš„æ–‡æœ¬ä¸­ç»å¸¸å‡ºçŽ°ï¼Œå› ä¸ºå®ƒä»¬å¤šåŠŸèƒ½ä¸”å®‰å…¨ï¼Œä½†å¯èƒ½æ˜¾å¾—ä¸å¤Ÿä¸ªæ€§åŒ–å’Œæ¨¡ç³Šã€‚ä»¥ä¸‹æ˜¯ä¸€äº›ç»å¸¸å‡ºçŽ°çš„è¯æ±‡ï¼š\n\n**è¿‡åº¦ä½¿ç”¨çš„è¿‡æ¸¡è¯**\n\n* é¼“åŠ±ChatGPTä½¿ç”¨æ›´ç®€å•ã€æ›´è‡ªç„¶çš„æ›¿ä»£è¯ï¼Œå¦‚â€œä¹Ÿâ€æˆ–â€œä½†æ˜¯â€ï¼Œè€Œä¸æ˜¯â€œæ­¤å¤–â€ã€â€œå› æ­¤â€æˆ–â€œç„¶è€Œâ€ã€‚\n\n**å¸¸ç”¨å½¢å®¹è¯å’Œåè¯**\n\n* å¸¸è§çš„å½¢å®¹è¯å¦‚â€œåˆ›æ–°çš„â€ã€â€œå¼ºå¤§çš„â€å’Œâ€œåŠ¨æ€çš„â€è™½ç„¶å¯ä»¥ä½¿ç”¨ï¼Œä½†å¯èƒ½å¬èµ·æ¥æ¯”è¾ƒæ™®é€šã€‚é¼“åŠ±ChatGPTä½¿ç”¨æ›´å…·ä½“çš„æè¿°è¯ã€‚åŒæ ·ï¼Œåƒâ€œæ•ˆçŽ‡â€ã€â€œä¼˜åŒ–â€å’Œâ€œè½¬åž‹â€è¿™æ ·çš„æŠ½è±¡åè¯é€šå¸¸å¯ä»¥ç”¨ä¸Žä¸»é¢˜ç›¸å…³çš„å…·ä½“æœ¯è¯­æ¥æ›¿ä»£ã€‚\n\n**å¸¸è§åŠ¨è¯å’ŒçŸ­è¯­**\n\n* åƒâ€œè¯æ˜Žäº†â€¦â€¦â€è¿™æ ·çš„çŸ­è¯­é™ˆè¯æ»¥è°ƒï¼Œä»¥åŠâ€œä¿ƒè¿›â€æˆ–â€œæœ€å¤§åŒ–â€ç­‰åŠ¨è¯ï¼Œå¯èƒ½å¬èµ·æ¥å¾ˆå…¬å¼åŒ–ã€‚ç”¨â€œå¸®åŠ©â€ã€â€œæ”¹å–„â€æˆ–â€œå¢žåŠ â€ç­‰ç®€å•åŠ¨è¯æ›¿æ¢å®ƒä»¬ï¼Œå¯ä»¥å¢žæ·»è‡ªç„¶ã€å¯¹è¯å¼çš„æ„Ÿè§‰ã€‚\n\nä»¥ä¸‹æ˜¯è¿™äº›æŒ‡ç¤ºçš„å®žé™…åº”ç”¨ç¤ºä¾‹ï¼š\n\n**ä¹‹å‰:**â€œæ€»ä¹‹ï¼Œåˆ©ç”¨æ•°æ®é©±åŠ¨çš„æ´žå¯Ÿä¿ƒè¿›äº†åŠ¨æ€çŽ¯å¢ƒä¸­çš„ä¼˜åŒ–ã€‚â€\n\n**ä¹‹åŽ:**â€œæ€»ç»“ä¸€ä¸‹ï¼Œæœ‰æ•ˆåˆ©ç”¨æ•°æ®å¸®åŠ©ä¼ä¸šåšå‡ºæ›´å¥½çš„å†³ç­–ã€‚â€\n\n## 2\\. æ‹¥æŠ±è¯­è¨€çš„ç®€å•æ€§å’Œæ¸…æ™°æ€§\n\n**ä¿æŒç®€å•æ˜Žäº†**\n\n* ChatGPT ç»å¸¸ä½¿ç”¨å¤æ‚çš„å¥å­å’Œæ­£å¼çš„è¯­è¨€ï¼Œè¿™å¯èƒ½ä¼šåœ¨ç”¨æˆ·å’Œä¿¡æ¯ä¹‹é—´é€ æˆéšœç¢ã€‚é¼“åŠ±ä½¿ç”¨æ›´ç®€å•çš„å¥å­ç»“æž„å’Œç›´æŽ¥çš„è¯­è¨€ï¼Œä½¿å›žåº”æ›´åŠ æ¸…æ™°å’Œæ˜“äºŽæŽ¥è¿‘ã€‚\n\n**é™åˆ¶æ¨¡ç³Šçš„é™ˆè¿°**\n\n* å¦‚æžœæŸäº›å†…å®¹å¬èµ·æ¥æ¨¡ç³Šï¼Œæç¤º ChatGPT æ·»åŠ å…·ä½“ç»†èŠ‚ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ç›´æŽ¥è¯´æ˜Žåº”è¯¥è€ƒè™‘ä»€ä¹ˆä»¥åŠä¸ºä»€ä¹ˆï¼Œè€Œä¸æ˜¯è¯´â€œå€¼å¾—è€ƒè™‘â€ã€‚\n\n**é¿å…å†—é•¿ã€å•°å—¦çš„å¥å­**\n\n* å†—é•¿çš„å¥å­å¬èµ·æ¥è¿‡äºŽæ­£å¼å’Œæœºæ¢°ã€‚é¼“åŠ±å°†å¤æ‚çš„æƒ³æ³•åˆ†è§£æˆè¾ƒçŸ­çš„å¥å­ï¼Œæ¯ä¸ªå¥å­åŒ…å«ä¸€ä¸ªä¸»è¦æ€æƒ³ã€‚è¿™ä½¿å¾—å›žåº”å¬èµ·æ¥æ›´éšæ„å’Œå¯¹è¯ã€‚\n\n**ç¤ºä¾‹ï¼š****ä¹‹å‰:**â€œæ­¤å¤–ï¼Œé‡è¦çš„æ˜¯è¦æ³¨æ„ï¼Œä¼˜åŒ–æ‚¨çš„æµç¨‹å¯ä»¥å¸¦æ¥æ˜¾è‘—çš„æ•ˆçŽ‡æå‡ã€‚â€\n\n**ä¹‹åŽ:**â€œå¦‚æžœæ‚¨æ”¹å–„æµç¨‹ï¼Œå¯ä»¥æé«˜æ•ˆçŽ‡å¹¶èŠ‚çœæ—¶é—´ã€‚â€\n\n## 3\\. åŒ¹é…è¯­æ°”ä¸Žä¸Šä¸‹æ–‡\n\nä½¿ AI å›žå¤å¬èµ·æ¥æ›´è‡ªç„¶çš„æœ€å¿«æ–¹æ³•ä¹‹ä¸€æ˜¯æ ¹æ®åœºæ™¯è°ƒæ•´è¯­æ°”ã€‚\n\n**è°ƒæ•´æ­£å¼ç¨‹åº¦**\n\n* ChatGPT æœ‰æ—¶åœ¨ä¼‘é—²åœºåˆä½¿ç”¨è¿‡äºŽæ­£å¼çš„è¯­è¨€ï¼Œæˆ–åœ¨å•†ä¸šåœºæ™¯ä¸­ä½¿ç”¨è¿‡äºŽéšæ„çš„è¯­è¨€ã€‚æ ¹æ®ç”¨æˆ·çš„éœ€æ±‚è°ƒæ•´è¯­æ°”ä½¿å›žå¤æ„Ÿè§‰æ›´åˆé€‚ï¼Œæ›´åƒäººç±»ã€‚\n\n**é¿å…æŠ½è±¡æ¦‚å¿µï¼Œåå‘çŽ°å®žç»†èŠ‚**\n\n* æ³›æ³›è€Œè°ˆçš„é™ˆè¿°å¹¶ä¸æ€»æ˜¯æä¾›ä»·å€¼ã€‚é¼“åŠ± ChatGPT ä½¿ç”¨å…·ä½“ç¤ºä¾‹æˆ–ç›¸å…³ç»†èŠ‚ï¼Œä»¥æ›´æ¸…æ™°åœ°ä¼ è¾¾è¦ç‚¹ã€‚ä¸Žå…¶è¯´ï¼šâ€œè¿™æŽ¨åŠ¨äº†è½¬åž‹â€ï¼Œä¸å¦‚è¯´ï¼šâ€œè¿™å¯ä»¥å¸¦æ¥æ–°çš„å·¥ä½œæ–¹å¼ï¼Œæ¯”å¦‚åŠ å¿«ç”Ÿäº§æˆ–é™ä½Žæˆæœ¬ã€‚â€\n\n**ä¹‹å‰:**â€œæ€»ä¹‹ï¼Œåˆ©ç”¨æ•°æ®æ´žå¯Ÿå¯ä»¥æŽ¨åŠ¨æœ‰å½±å“åŠ›çš„ä¸šåŠ¡è½¬åž‹ã€‚â€\n\n**ä¹‹åŽ:**â€œæœ‰æ•ˆä½¿ç”¨æ•°æ®å¯ä»¥å¸®åŠ©ä¼ä¸šæˆé•¿ï¼Œé€šè¿‡æ”¹å–„ç”Ÿäº§é€Ÿåº¦æˆ–å‡å°‘å¼€æ”¯ç­‰æ–¹é¢ã€‚â€\n\n## 4\\. è¯· ChatGPT ä¸“æ³¨äºŽæé«˜äº’åŠ¨æ€§å’Œäº²å’ŒåŠ›\n\näººå·¥å¬èµ·æ¥çš„è¯­è¨€å¾€å¾€æºäºŽè¿‡äºŽä¸“ä¸šçš„è¯­æ°”æˆ–ä¸ŽçœŸå®žäººæ²¡æœ‰è”ç³»çš„æŠ½è±¡æ€æƒ³ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›ä½¿å›žåº”æ›´å…·äº’åŠ¨æ€§å’Œäº²å’ŒåŠ›çš„å»ºè®®ï¼š\n\n**ä½¿å›žåº”å®žç”¨å’Œæœ‰ç”¨**\n\n* ChatGPT å¯ä»¥æä¾›å®žç”¨çš„å»ºè®®å’Œç”¨æˆ·å¯ä»¥é‡‡å–çš„å…·ä½“æ­¥éª¤ï¼Œè€Œä¸æ˜¯æä¾›ä¸€èˆ¬æ€§çš„å»ºè®®ã€‚\n\n**é¿å…è¡Œè¯**\n\n* è¡Œä¸šç‰¹å®šçš„è¡Œè¯æˆ–æŠ€æœ¯æœ¯è¯­å¯èƒ½ä¼šä½¿å›žåº”å¬èµ·æ¥ç”Ÿç¡¬å’Œé¥è¿œã€‚é¼“åŠ± ChatGPT åœ¨å¯èƒ½çš„æƒ…å†µä¸‹ç®€åŒ–è¯­è¨€ï¼Œæœ‰åŠ©äºŽå¤§å®¶ä¿æŒä¸€è‡´ã€‚\n\n**ç¤ºä¾‹ï¼š** **ä¹‹å‰:**â€œä¸ºäº†æé«˜æ•ˆçŽ‡ï¼Œç³»ç»Ÿæ€§åœ°è¯„ä¼°èµ„æºåˆ†é…è‡³å…³é‡è¦ã€‚â€\n\n**ä¹‹åŽ:**â€œä¸ºäº†èŠ‚çœæ—¶é—´å’Œèµ„æºï¼Œæ£€æŸ¥ä¸€ä¸‹æ˜¯å¦æ‰€æœ‰ä¸œè¥¿éƒ½åœ¨æœ€ä½³ä½¿ç”¨çŠ¶æ€ã€‚â€\n\n## 5\\. å°è¯•çŽ°å®žä¸–ç•Œçš„ä¾‹å­å’Œç›´æŽ¥é™ˆè¿°\n\näººä»¬å€¾å‘äºŽä»¥æ¸…æ™°ã€å…·ä½“çš„æ–¹å¼è¡¨è¾¾ï¼Œå°¤å…¶æ˜¯åœ¨è§£é‡ŠæŸä»¶äº‹æƒ…æ—¶ã€‚ChatGPT å¯ä»¥é€šè¿‡ä½¿ç”¨ä¸Žç”¨æˆ·çŽ°å®žä¸–ç•Œç›¸å…³çš„ä¾‹å­æ¥åæ˜ è¿™ä¸€ç‚¹ã€‚\n\n**ä½¿ç”¨ç›¸å…³çš„ä¾‹å­**\n\n* å½“åŒ…æ‹¬çŽ°å®žä¸–ç•Œçš„ä¾‹å­æ—¶ï¼Œé€šç”¨çš„å›žåº”å˜å¾—æ›´åŠ éš¾å¿˜ã€‚ä¾‹å¦‚ï¼Œä¸Žå…¶è¯´â€œè¿™å¯ä»¥æ”¹å–„è¿è¥æµç¨‹â€ï¼Œä¸å¦‚è¯´â€œè¿™å¯èƒ½å¸®åŠ©ä½ å‡å°‘å®¢æˆ·æœåŠ¡ä¸­çš„ç­‰å¾…æ—¶é—´â€ã€‚\n\n**é¼“åŠ±ä½¿ç”¨ç›´æŽ¥é™ˆè¿°è€Œä¸æ˜¯ä¿®é¥°è¯­**\n\n* â€œå€¼å¾—æ³¨æ„çš„æ˜¯â€¦â€¦â€é€šå¸¸å¯ä»¥ç¼©çŸ­ä¸ºä¸»è¦è§‚ç‚¹æœ¬èº«ã€‚ç›´æŽ¥é™ˆè¿°ä¸ä»…ä¿æŒç®€å•ï¼Œè€Œä¸”å¢žå¼ºäº†ä¿¡æ¯çš„å¯ä¿¡åº¦ã€‚\n\n**ä¾‹å­ï¼š** **ä¹‹å‰ï¼š**â€œå€¼å¾—è€ƒè™‘çš„æ˜¯ï¼Œæµç¨‹æ”¹è¿›å¯èƒ½ä¼šå¯¼è‡´æ˜¾è‘—çš„æˆæœ¬é™ä½Žã€‚â€\n\n**ä¹‹åŽï¼š**â€œæ”¹å–„æµç¨‹å¯ä»¥å¸®åŠ©ä½ èŠ‚çœå¾ˆå¤šé’±ã€‚â€\n\n## æ€»ç»“\n\né€šè¿‡è¿™äº›è°ƒæ•´ï¼Œæ‚¨å¯ä»¥è®© ChatGPT ä»¥æ›´è‡ªç„¶ã€å¯¹è¯å¼å’Œå¼•äººå…¥èƒœçš„æ–¹å¼è¿›è¡Œäº¤æµã€‚å‡å°‘è¡Œè¯ã€ç¼©çŸ­å¥å­å’Œä½¿ç”¨æ›´ç®€å•çš„è¯­è¨€å¯ä»¥ä½¿å›žç­”å¬èµ·æ¥ä¸é‚£ä¹ˆåƒæœºå™¨äººï¼Œè€Œæ›´åƒæˆ‘ä»¬éƒ½æ¬£èµçš„æœ‰å¸®åŠ©ä¸”çŸ¥è¯†æ¸Šåšçš„åŠ©æ‰‹ã€‚\n\n"},{"lang":"zh","group":"blog","slug":"blog/glm-4-voice-9b-real-time-multilingual-voice-conversation-ai-install-locally-in-minutes-ce2fcd6c8fd8","frontmatter":{"title":"GLM-4-Voice 9Bâ€”â€”å®žæ—¶å¤šè¯­è¨€è¯­éŸ³å¯¹è¯ AIâ€”â€”å‡ åˆ†é’Ÿå†…å³å¯åœ¨æœ¬åœ°å®‰è£…","meta_title":"GLM-4-Voice 9Bâ€”â€”å®žæ—¶å¤šè¯­è¨€è¯­éŸ³å¯¹è¯ AIâ€”â€”å‡ åˆ†é’Ÿå†…å³å¯åœ¨æœ¬åœ°å®‰è£…","description":"GLM-4-Voice 9B æ˜¯ä¸€æ¬¾å®žæ—¶å¤šè¯­è¨€è¯­éŸ³å¯¹è¯AIï¼Œæ”¯æŒè‹±è¯­å’Œä¸­æ–‡ï¼Œå…·å¤‡æƒ…æ„Ÿè¯­è°ƒå’Œè¯­é€Ÿçš„å¯å®šåˆ¶æ€§ã€‚è¯¥æ¨¡åž‹é€šè¿‡ç«¯åˆ°ç«¯æž¶æž„å®žçŽ°ä½Žå»¶è¿Ÿå“åº”ï¼Œæä¾›æ›´è‡ªç„¶çš„äº’åŠ¨ä½“éªŒã€‚å…¶ä¸»è¦ç»„ä»¶åŒ…æ‹¬æ ‡è®°åŒ–å™¨ã€æ ¸å¿ƒè¯­è¨€æ¨¡åž‹å’Œè§£ç å™¨ï¼Œèƒ½å¤Ÿç›´æŽ¥å¤„ç†è¯­éŸ³è¾“å…¥å’Œç”ŸæˆéŸ³é¢‘è¾“å‡ºã€‚ç”¨æˆ·å¯é€šè¿‡ç®€å•çš„æœ¬åœ°è®¾ç½®æ­¥éª¤å¿«é€Ÿéƒ¨ç½²è¯¥æ¨¡åž‹ï¼Œé€‚åˆå®¢æˆ·æœåŠ¡å’Œæ•™è‚²ç­‰å¤šç§åº”ç”¨åœºæ™¯ã€‚","date":"2024-11-13T01:32:04.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*LATTpEc2AHvqgVyPKSzW7A.jpeg","categories":["Voice Assistants","Natural Language Processing","Chatbots"],"author":"Rifx.Online","tags":["multilingual","conversation","real-time","customization","performance"],"draft":false,"slug":"blog/glm-4-voice-9b-real-time-multilingual-voice-conversation-ai-install-locally-in-minutes-ce2fcd6c8fd8"},"content":"\n### å¦‚ä½•è®¾ç½® GLM\\-4\\-Voice 9B ä»¥å®žçŽ°æ— ç¼çš„å®žæ—¶è¯­éŸ³äº¤äº’ï¼Œæ”¯æŒè‹±è¯­å’Œä¸­æ–‡ï¼Œå¹¶æŽ¢ç´¢å…¶ç‹¬ç‰¹çš„æž¶æž„ã€ä½Žå»¶è¿Ÿå“åº”å’Œå¯å®šåˆ¶çš„å£°éŸ³å±žæ€§ã€‚\n\n\n\n\n## ä»‹ç»\n\nè¿‘å¹´æ¥ï¼Œè¯­éŸ³å¯ç”¨çš„äººå·¥æ™ºèƒ½å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½¿å¯¹è¯ä»£ç†èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œå“åº”äººç±»è¯­è¨€ã€‚ä»Žè™šæ‹ŸåŠ©æ‰‹åˆ°å®¢æˆ·æœåŠ¡æœºå™¨äººï¼Œè¯­éŸ³äººå·¥æ™ºèƒ½å·²æˆä¸ºå„ä¸ªè¡Œä¸šçš„é‡è¦å·¥å…·ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ¨¡åž‹åœ¨æµåˆ©åœ°åˆ‡æ¢è¯­è¨€ã€ç†è§£å£è¯­æŸ¥è¯¢çš„ç»†å¾®å·®åˆ«ä»¥åŠæä¾›é«˜è´¨é‡å“åº”æ–¹é¢ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚è¿™æ­£æ˜¯Zhipu AIçš„GLM-4-Voiceè„±é¢–è€Œå‡ºçš„åœ°æ–¹ã€‚GLM-4-Voiceä½œä¸ºä¸€æ¬¾ç«¯åˆ°ç«¯çš„è¯­éŸ³æ¨¡åž‹ï¼ŒæŽ¨åŠ¨äº†å¤šè¯­è¨€å¯¹è¯äººå·¥æ™ºèƒ½çš„è¾¹ç•Œï¼Œæ”¯æŒè‹±è¯­å’Œä¸­æ–‡çš„å®žæ—¶å¯¹è¯ï¼ŒåŒæ—¶æä¾›å¯é€‚åº”ä¸”ç±»äººåŒ–çš„å“åº”ç”Ÿæˆã€‚\n\nåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†æŽ¢è®¨ä¸ºä»€ä¹ˆGLM-4-Voiceå€¼å¾—å…³æ³¨ï¼Œå®ƒçš„ç‹¬ç‰¹ä¹‹å¤„ï¼Œä»¥åŠå¦‚ä½•åœ¨æœ¬åœ°è®¾ç½®å’Œå¼€å§‹ä½¿ç”¨å®ƒã€‚æˆ‘ä»¬è¿˜å°†æŸ¥çœ‹å…¶æž¶æž„ï¼Œå¹¶æä¾›è®¿é—®ç½‘ç»œæ¼”ç¤ºçš„å®žç”¨æŒ‡å—ã€‚\n\n## ä¸ºä»€ä¹ˆé€‰æ‹© GLM-4-Voiceï¼Ÿ\n\nä¼ ç»Ÿçš„è¯­è¨€æ¨¡åž‹é€šå¸¸ä»…é™äºŽæ–‡æœ¬ï¼Œå¹¶éœ€è¦é¢å¤–çš„å¤„ç†å±‚æ¥å¤„ç†è¯­éŸ³ã€‚å®ƒä»¬åœ¨äº¤äº’æ€§æ–¹é¢å¯èƒ½ä¼šé‡åˆ°å›°éš¾ï¼Œæˆ–è€…å­˜åœ¨å»¶è¿Ÿé—®é¢˜ã€‚GLM-4-Voice é€šè¿‡ä¸€ä¸ªç»Ÿä¸€çš„æ¨¡åž‹å…‹æœäº†è¿™äº›é™åˆ¶ï¼Œèƒ½å¤Ÿç›´æŽ¥å¤„ç†å’Œç”Ÿæˆè¯­éŸ³ã€‚ä»¥ä¸‹æ˜¯å®ƒçš„çªå‡ºä¹‹å¤„ï¼š\n\n1. **ç«¯åˆ°ç«¯è¯­éŸ³å¤„ç†**ï¼šä¸Žè®¸å¤šä¾èµ–äºŽå•ç‹¬çš„æ–‡æœ¬åˆ°è¯­éŸ³ (TTS) æˆ–è¯­éŸ³åˆ°æ–‡æœ¬ (STT) æ¨¡å—çš„æ¨¡åž‹ä¸åŒï¼ŒGLM-4-Voice ç›´æŽ¥ä»¥å£è¯­å½¢å¼è¿›è¡Œè§£è¯»å’Œå“åº”ï¼Œä»Žè€Œæä¾›æ›´æ— ç¼å’Œæ›´å…·å“åº”æ€§çš„ä½“éªŒã€‚\n2. **å¤šè¯­è¨€æ”¯æŒ**ï¼šè¯¥æ¨¡åž‹åœ¨å¤„ç†è‹±è¯­å’Œä¸­æ–‡è¿™ä¸¤ç§å…¨çƒå¹¿æ³›ä½¿ç”¨çš„è¯­è¨€æ–¹é¢è¡¨çŽ°å‡ºè‰²ã€‚å®ƒæµç•…åˆ‡æ¢è¯­è¨€çš„èƒ½åŠ›ä½¿å…¶éžå¸¸é€‚åˆåŒè¯­çŽ¯å¢ƒå’Œå›½é™…åº”ç”¨ã€‚\n3. **å¯å®šåˆ¶å±žæ€§**ï¼šGLM-4-Voice å…è®¸åœ¨æƒ…æ„Ÿã€è¯­è°ƒã€è¯­é€Ÿç”šè‡³æ–¹è¨€ä¸Šè¿›è¡Œè°ƒæ•´ï¼Œä½¿å…¶èƒ½å¤Ÿç”Ÿæˆæ›´è‡ªç„¶å’Œæƒ…å¢ƒåˆé€‚çš„å“åº”ã€‚\n4. **ä½Žå»¶è¿Ÿ**ï¼šé€šè¿‡æ”¯æŒæµå¼æŽ¨ç†ï¼Œè¯¥æ¨¡åž‹çš„å»¶è¿Ÿçº¦ä¸º 20 ä¸ªæ ‡è®°ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨å®žæ—¶å¯¹è¯ä¸­å®žçŽ°è¿‘ä¹Žå³æ—¶çš„å“åº”ã€‚\n\n## GLM\\-4\\-Voice çš„ç‰¹ç‚¹\n\nGLM\\-4\\-Voice å¸¦æ¥äº†å‡ ä¸ªç‹¬ç‰¹çš„åŠŸèƒ½ï¼Œä½¿å…¶ä¸Žå…¶ä»–è¯­éŸ³æ¨¡åž‹åŒºåˆ«å¼€æ¥ã€‚ä»¥ä¸‹æ˜¯å®ƒçš„ç‰¹åˆ«ä¹‹å¤„ï¼š\n\n* **å®žæ—¶è¯­éŸ³äº’åŠ¨**ï¼šé€šè¿‡æ”¯æŒä½Žå»¶è¿Ÿå“åº”ï¼ŒGLM\\-4\\-Voice èƒ½å¤Ÿä¿æŒæµç•…è‡ªç„¶çš„å¯¹è¯ï¼Œè¿™å¯¹å®¢æˆ·æ”¯æŒå’Œäº’åŠ¨ AI ç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚\n* **åŠ¨æ€è¯­éŸ³å±žæ€§**ï¼šç”¨æˆ·å¯ä»¥æŒ‡å®šæ¨¡åž‹çš„æƒ…æ„Ÿè¯­è°ƒã€è¯­é€Ÿå’Œå…¶ä»–ç‰¹å¾ï¼Œä½¿äº’åŠ¨æ›´åŠ ç”ŸåŠ¨ä¸”é€‚åˆå„ç§åœºæ™¯ã€‚\n* **å…·å¤‡ä¸Šä¸‹æ–‡æ„è¯†çš„åŒè¯­æ”¯æŒ**ï¼šè¯¥æ¨¡åž‹æ—¨åœ¨ç†è§£å’Œç”Ÿæˆä¸­æ–‡å’Œè‹±æ–‡çš„å“åº”ã€‚å®ƒèƒ½å¤Ÿæ— ç¼åˆ‡æ¢è¿™ä¸¤ç§è¯­è¨€ï¼Œä¸ºå¤šè¯­è¨€åº”ç”¨æä¾›çµæ´»çš„è§£å†³æ–¹æ¡ˆã€‚\n* **é«˜çº§è¯­éŸ³è§£ç **ï¼šåŸºäºŽ CosyVoiceï¼ŒGLM\\-4\\-Voice è§£ç å™¨èƒ½å¤Ÿå®žçŽ°é«˜è´¨é‡çš„è¯­éŸ³ç”Ÿæˆï¼Œå¹¶æ”¯æŒæµå¼ä¼ è¾“ï¼Œåœ¨ä¸¤ç§è¯­è¨€ä¸­ä¿æŒé«˜æ¸…æ™°åº¦ã€‚\n\n## æž¶æž„\n\nGLM\\-4\\-Voice çš„æž¶æž„ç”±ä¸‰ä¸ªä¸»è¦ç»„ä»¶ç»„æˆï¼Œæ¯ä¸ªç»„ä»¶åœ¨å®žçŽ°ç«¯åˆ°ç«¯è¯­éŸ³äº¤äº’ä¸­å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*nJsKHtxSblNkixPIBZpWyQ.jpeg)\n\n1. **GLM\\-4\\-Voice\\-Tokenizer**ï¼šè¯¥ç»„ä»¶å°†è¿žç»­è¯­éŸ³è¾“å…¥æ ‡è®°åŒ–ä¸ºç¦»æ•£æ ‡è®°ï¼Œæ¯ç§’å¤§çº¦ç”Ÿæˆ 12.5 ä¸ªæ ‡è®°ã€‚æ ‡è®°å™¨åŸºäºŽ Whisper çš„ç¼–ç å™¨ï¼Œå¹¶æ·»åŠ äº†å‘é‡é‡åŒ–ï¼Œä½¿æ¨¡åž‹èƒ½å¤Ÿä»¥ç»“æž„åŒ–å½¢å¼å¤„ç†éŸ³é¢‘ã€‚\n2. **GLM\\-4\\-Voice\\-9B**ï¼šæ ¸å¿ƒè¯­è¨€æ¨¡åž‹ï¼ŒåŸºäºŽ GLM\\-4 æž¶æž„ï¼Œå·²è°ƒæ•´ä¸ºå¤„ç†å£è¯­è¾“å…¥ã€‚å®ƒå¯ä»¥å¤„ç†æ–‡æœ¬å’Œè¯­éŸ³ï¼Œä½¿å…¶æˆä¸ºå¼ºå¤§çš„å¤šæ¨¡æ€å¯¹è¯ä»£ç†ã€‚\n3. **GLM\\-4\\-Voice\\-Decoder**ï¼šè¯¥è§£ç å™¨å°†ç¦»æ•£æ ‡è®°è½¬æ¢å›žè¿žç»­è¯­éŸ³ï¼Œä½¿æ¨¡åž‹èƒ½å¤Ÿç”ŸæˆéŸ³é¢‘è¾“å‡ºã€‚å®ƒæ”¯æŒæµå¼æŽ¨ç†ï¼Œä½¿å“åº”èƒ½å¤Ÿåœ¨å¤„ç†å‡ ä¸ªæ ‡è®°åŽç«‹å³å¼€å§‹ï¼Œä»Žè€Œæœ€å°åŒ–å¯¹è¯å»¶è¿Ÿã€‚\n\nè¿™äº›ç»„ä»¶å…±åŒä½¿ GLM\\-4\\-Voice æˆä¸ºå®žæ—¶è¯­éŸ³äº¤äº’çš„å¼ºå¤§å·¥å…·ï¼Œæ”¯æŒä¸åŒè¯­è¨€å’Œæ–¹è¨€çš„å¯¹è¯ AIã€‚\n\n## åœ¨æœ¬åœ°è®¾ç½® GLM\\-4\\-Voice\n\nè¦ä½“éªŒ GLM\\-4\\-Voiceï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤åœ¨æ‚¨çš„æœºå™¨ä¸Šæœ¬åœ°è®¾ç½®è¯¥æ¨¡åž‹ã€‚\n\n### ç¬¬ä¸€æ­¥ï¼šå…‹éš†ä»“åº“\n\né¦–å…ˆä»Ž GitHub å…‹éš†ä»“åº“ã€‚ç¡®ä¿åŒ…å«å­æ¨¡å—ï¼š\n\n```python\n!git clone --recurse-submodules https://github.com/THUDM/GLM-4-Voice\ncd GLM-4-Voice\n```python\n!git clone --recurse-submodules https://github.com/THUDM/GLM-4-Voice\ncd GLM-4-Voice\n\n```\n\n### æ­¥éª¤ 2ï¼šå®‰è£…ä¾èµ–\n\nè¿›å…¥é¡¹ç›®ç›®å½•å¹¶å®‰è£…å¿…è¦çš„ä¾èµ–ï¼š\n\n```python\n!pip install -r requirements.txt\n```python\n!pip install -r requirements.txt\n\n```\n\n### ç¬¬3æ­¥ï¼šä¸‹è½½æ¨¡åž‹æ£€æŸ¥ç‚¹\n\nGLM\\-4\\-Voiceçš„è§£ç å™¨æ¨¡åž‹æ‰˜ç®¡åœ¨Hugging Faceä¸Šï¼Œéœ€è¦`git-lfs`è¿›è¡Œä¸‹è½½ã€‚ç¡®ä¿å·²å®‰è£…`git-lfs`ï¼Œç„¶åŽè¿è¡Œï¼š\n\n```python\n!git clone https://huggingface.co/THUDM/glm-4-voice\n```python\n!git clone https://huggingface.co/THUDM/glm-4-voice\n\n```\n\n### æ­¥éª¤ 4ï¼šå¯åŠ¨æ¨¡åž‹æœåŠ¡\n\nä¸€åˆ‡è®¾ç½®å®ŒæˆåŽï¼Œå¯åŠ¨æ¨¡åž‹æœåŠ¡å™¨ï¼š\n\n```python\npython model_server.py --model-path glm-4-voice-9b\n```python\npython model_server.py --model-path glm-4-voice-9b\n\n```\n\n### ç¬¬5æ­¥ï¼šå¯åŠ¨WebæœåŠ¡\n\nä¸€æ—¦æ¨¡åž‹æœåŠ¡å™¨è¿è¡Œï¼Œæ‰§è¡Œä»¥ä¸‹å‘½ä»¤ä»¥å¯åŠ¨WebæœåŠ¡ï¼š\n\n```python\npython web_demo.py\n```python\npython web_demo.py\n\n```\n\næ‚¨çŽ°åœ¨å¯ä»¥è®¿é—®Webæ¼”ç¤º [http://127\\.0\\.0\\.1:8888](http://127.0.0.1:8888) ä¸ŽGLM\\-4\\-Voiceè¿›è¡Œäº¤äº’ã€‚\n\n> **æ³¨æ„ï¼š** GLM\\-4\\-Voiceæ¨¡åž‹èµ„æºå¯†é›†ï¼Œè¿è¡Œæœ‰æ•ˆéœ€è¦å¤§é‡è®¡ç®—èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒéœ€è¦35â€“40ä¸ªGPUä»¥å®žçŽ°æœ€ä½³æ€§èƒ½ï¼Œå› æ­¤é€‚åˆåœ¨å¯è®¿é—®é«˜æ€§èƒ½ç¡¬ä»¶çš„çŽ¯å¢ƒä¸­éƒ¨ç½²ã€‚ç”¨æˆ·åœ¨å°è¯•ä½¿ç”¨æ­¤æ¨¡åž‹ä¹‹å‰ï¼Œåº”ç¡®ä¿å…·å¤‡å¿…è¦çš„åŸºç¡€è®¾æ–½ã€‚\n\n## Web Demo Interface\n\nGLM\\-4\\-Voice çš„ç½‘é¡µæ¼”ç¤ºæä¾›äº†ä¸€ä¸ªç›´è§‚çš„ç•Œé¢ï¼Œå…·æœ‰å¤šç§è‡ªå®šä¹‰é€‰é¡¹ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*scbHOUXqMW5KGAcT3Bq1Eg.png)\n\n* **è¾“å…¥æ¨¡å¼**ï¼šç”¨æˆ·å¯ä»¥é€‰æ‹©ä»¥æ–‡æœ¬æˆ–éŸ³é¢‘å½¢å¼æä¾›è¾“å…¥ã€‚è¿™ç§çµæ´»æ€§å…è®¸æ— æ‰‹æ“ä½œæˆ–ä¼ ç»Ÿäº¤äº’ã€‚\n* **è¯­éŸ³æŽ§åˆ¶å‚æ•°**ï¼šè°ƒæ•´æ¸©åº¦ã€top\\-p å’Œä»¤ç‰Œé™åˆ¶ï¼Œä»¥è‡ªå®šä¹‰æ¨¡åž‹çš„å“åº”ç‰¹æ€§ã€‚\n* **è°ƒè¯•ä¿¡æ¯**ï¼šæ˜¾ç¤ºè¾“å…¥å’Œè¾“å‡ºä»¤ç‰Œï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿæ´žå¯Ÿæ¨¡åž‹å¤„ç†æŸ¥è¯¢çš„è¿‡ç¨‹ã€‚\n* **äº¤äº’å¼éŸ³é¢‘æ˜¾ç¤º**ï¼šéŸ³é¢‘è¾“å…¥å’Œå“åº”ä»¥æ³¢å½¢å½¢å¼æ˜¾ç¤ºï¼Œç”¨æˆ·å¯ä»¥é‡æ’­æˆ–æŸ¥çœ‹éŸ³é¢‘ç‰‡æ®µä»¥è¯„ä¼°è´¨é‡ã€‚\n\nç„¶è€Œï¼Œç”¨äºŽåœ¨æ¼”ç¤ºä¸­æµå¼ä¼ è¾“éŸ³é¢‘çš„ Gradio æœ‰æ—¶å¯èƒ½ä¼šå‡ºçŽ°ä¸ç¨³å®šæƒ…å†µã€‚ä¸ºäº†èŽ·å¾—æœ€ä½³è´¨é‡ï¼Œå»ºè®®åœ¨ç”ŸæˆåŽé‡æ’­å¯¹è¯æ¡†ä¸­çš„éŸ³é¢‘ã€‚\n\n## ç»“è®º\n\nGLM\\-4\\-Voice åœ¨å¯¹è¯å¼äººå·¥æ™ºèƒ½é¢†åŸŸä¸­è„±é¢–è€Œå‡ºï¼Œæä¾›äº†ç‹¬ç‰¹çš„åŒè¯­æ”¯æŒã€å®žæ—¶éŸ³é¢‘äº¤äº’å’Œçµæ´»çš„å“åº”å®šåˆ¶ã€‚å…¶ç«¯åˆ°ç«¯è®¾è®¡å’Œä½Žå»¶è¿Ÿä½¿å…¶æˆä¸ºå®¢æˆ·æœåŠ¡ã€æ•™è‚²ã€è™šæ‹ŸåŠ©æ‰‹ç­‰åº”ç”¨çš„æœ€ä½³å€™é€‰è€…ã€‚å‡­å€Ÿæ˜“äºŽè®¿é—®çš„è®¾ç½®è¿‡ç¨‹ï¼ŒGLM\\-4\\-Voice ä¸ºå¼€å‘è€…å’Œç ”ç©¶äººå‘˜æŽ¢ç´¢ä¸­æ–‡å’Œè‹±æ–‡çš„é«˜çº§è¯­éŸ³èƒ½åŠ›æ‰“å¼€äº†å¤§é—¨ã€‚\n\néšç€å¯¹æ›´äº’åŠ¨å’ŒçœŸå®žçš„äººå·¥æ™ºèƒ½éœ€æ±‚çš„ä¸æ–­å¢žé•¿ï¼Œåƒ GLM\\-4\\-Voice è¿™æ ·çš„æ¨¡åž‹ä»£è¡¨äº†åœ¨æ¶ˆé™¤è¯­è¨€å’Œå¯¹è¯éšœç¢æ–¹é¢çš„é‡è¦è¿›å±•ã€‚æ— è®ºæ‚¨æ˜¯æƒ³æž„å»ºèŠå¤©æœºå™¨äººã€è™šæ‹Ÿæ•™å¸ˆè¿˜æ˜¯å®¢æˆ·æœåŠ¡ä»£ç†ï¼ŒGLM\\-4\\-Voice éƒ½æä¾›äº†å¼ºå¤§è€Œçµæ´»çš„è§£å†³æ–¹æ¡ˆã€‚\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/google-gemini-are-big-context-windows-the-killer-feature-72ff95488fb1","frontmatter":{"title":"Google Geminiï¼šå¤§ä¸Šä¸‹æ–‡çª—å£æ˜¯æ€æ‰‹çº§åŠŸèƒ½å—ï¼Ÿ","meta_title":"Google Geminiï¼šå¤§ä¸Šä¸‹æ–‡çª—å£æ˜¯æ€æ‰‹çº§åŠŸèƒ½å—ï¼Ÿ","description":"Goggle å³å°†æŽ¨å‡ºçš„æ³•å­¦ç¡•å£«å­¦ä½è¯¾ç¨‹æœ‰äº†é‡å¤§è¿›å±•","date":"2024-11-10T22:36:54.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MteQrQSTXLuJcd86RbjQrg.png","categories":["Machine Learning","Natural Language Processing","Data Science"],"author":"Rifx.Online","tags":["Gemini","tokens","context","LLM","evolution"],"draft":false,"slug":"blog/google-gemini-are-big-context-windows-the-killer-feature-72ff95488fb1"},"content":"\n### è°·æ­Œå³å°†æŽ¨å‡ºçš„ LLM è¿ˆå‡ºäº†é‡å¤§ä¸€æ­¥\n\n\n\nå°±åœ¨å…«ä¸ªæœˆå‰ï¼Œä¸€å°æ³„éœ²çš„è°·æ­Œç”µå­é‚®ä»¶é€éœ²è¯¥å…¬å¸åœ¨åŠªåŠ›è¶…è¶Šå…¶ AI ç«žäº‰å¯¹æ‰‹æ–¹é¢é‡åˆ°äº†å›°éš¾ã€‚ä»–ä»¬çš„ AI äº§å“å‘¨å›´ä¸ä»…æ²¡æœ‰[æŠ¤åŸŽæ²³](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither)â€”â€”æ¢å¥è¯è¯´ï¼Œæ²¡æœ‰å»ºç«‹èµ·å•†ä¸šä¼˜åŠ¿â€”â€”è°·æ­Œä¹Ÿæ²¡æœ‰[ç§˜å¯†æ­¦å™¨](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither)å¯ä»¥æ”¹å˜å±€é¢ã€‚å³ä½¿åœ¨ä»–ä»¬åŠªåŠ›è§£å†³è¿™ä¸ªé—®é¢˜æ—¶ï¼Œä»–ä»¬ä¹Ÿçœ‹åˆ°ç§å‹Ÿèµ„åŠ©çš„ AI é¡¹ç›®ä¸Žå¼€æº AI æ¨¡åž‹ä¹‹é—´çš„å·®è·ä»¥â€œæƒŠäººçš„â€é€Ÿåº¦ç¼©å°ã€‚\n\nçŽ°åœ¨è¿˜ä¸ºæ—¶å·²æ™šï¼Œæ— æ³•çŸ¥é“è¿™ä¸ªæ•…äº‹çš„ç»“å±€ã€‚ä¹Ÿè®¸å¼€æº AI å°†ç»§ç»­åœ¨æ—©æœŸæˆåŠŸçš„åŸºç¡€ä¸Šå‘å±•ï¼Œæˆ–è€…å®ƒå°†è¢«è°·æ­Œã€å¾®è½¯å’Œè‹¹æžœç­‰æžå…¶å¯Œæœ‰çš„ç«žäº‰å¯¹æ‰‹åŠå…¶ä»¤äººéš¾ä»¥ç½®ä¿¡çš„æ•°æ®é‡æ‰€åŽ‹åˆ¶ã€‚çŽ°åœ¨ï¼Œè¿™åœºå†²çªä»åœ¨å±•å¼€ï¼Œå„ä¸ªç»„ç»‡å¿«é€ŸæŽ¨å‡ºä¸€ç³»åˆ— AI è¿›å±•ã€‚æœ€è¿‘ï¼Œè°·æ­Œåœ¨è¿™ä¸ªé¢†åŸŸä¸­æˆä¸ºç„¦ç‚¹ï¼Œå®£å¸ƒäº†å…¶æœ€æ–° LLM çš„é¢„è§ˆç‰ˆâ€”â€”[Gemini 1.5 Pro](https://deepmind.google/technologies/gemini/)ã€‚åˆæ˜¯ä¸€å¤©ï¼Œåˆä¸€ä¸ªå¤§åž‹è¯­è¨€æ¨¡åž‹â€”â€”æˆ–è€…è¯´ä¼¼ä¹Žå¦‚æ­¤ï¼Œç›´åˆ°è°·æ­Œæè¿°äº†ä¸€ä¸ªæƒŠäººçš„å˜åŒ–ã€‚\n\nGemini 1.5 Pro æ‰©å±•äº† *ä¸Šä¸‹æ–‡çª—å£*â€”â€”æœ¬è´¨ä¸Šæ˜¯è¡¡é‡ LLM ä¸€æ¬¡å¯ä»¥è·Ÿè¸ªå¤šå°‘æ•°æ®çš„æŒ‡æ ‡ã€‚åœ¨è¿‡åŽ»çš„ç‰ˆæœ¬ä¸­ï¼ŒGemini çš„ä¸Šä¸‹æ–‡çª—å£æœ€å¤§ä¸º 128,000 ä¸ªæ ‡è®°ï¼Œå°±åƒ GPT-4 ä¸€æ ·ã€‚ä½† Gemini çš„æ–°ä¸Šä¸‹æ–‡çª—å£å¯ä»¥å®¹çº³ **100 ä¸‡** ä¸ªæ ‡è®°ï¼Œè¿™ä¸€å˜åŒ–çš„å½±å“æ˜¯å·¨å¤§çš„ã€‚\n\nä½†åœ¨æˆ‘ä»¬è®¨è®ºä¸Šä¸‹æ–‡çª—å£å¯¹ LLM èƒ½åŠ›çš„å½±å“ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å›žé¡¾ä¸€ä¸‹ä¸Šä¸‹æ–‡çª—å£çš„å·¥ä½œåŽŸç†ã€‚\n\n## ä¸Šä¸‹æ–‡çª—å£ï¼ˆç®€è€Œè¨€ä¹‹ï¼‰\n\nç®€å•æ¥è¯´ï¼Œä¸Šä¸‹æ–‡çª—å£è®¾ç½®äº† LLM åœ¨äº¤äº’è¿‡ç¨‹ä¸­èƒ½å¤Ÿè®°ä½å¤šå°‘ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œå¦‚æžœæ‚¨æ­£åœ¨ä½¿ç”¨ ChatGPTï¼Œä¸Šä¸‹æ–‡çª—å£åŒ…æ‹¬æ‚¨ç»™å®ƒçš„å½“å‰æç¤ºã€æ‚¨ä¹‹å‰åœ¨è¯¥å¯¹è¯ä¸­è¾“å…¥çš„æ‰€æœ‰å†…å®¹ï¼Œä»¥åŠ ChatGPT å‘æ‚¨å‘é€çš„æ¯ä¸ªå›žå¤ã€‚å¯¹è¯æ—¶é—´é•¿äº†ï¼Œæ—§çš„å¯¹è¯éƒ¨åˆ†å°†ä¼šä»Žä¸Šä¸‹æ–‡çª—å£ä¸­æ»‘å‡ºï¼ŒChatGPT å°†çªç„¶å¿˜è®°é‚£äº›ç»†èŠ‚ã€‚\n\n128,000 ä¸ªä»¤ç‰Œçš„ä¸Šä¸‹æ–‡çª—å£å¬èµ·æ¥å¾ˆå¤§ï¼Œä½†è¿™ä¸ªæ•°å­—å…·æœ‰è¯¯å¯¼æ€§ã€‚é¦–å…ˆï¼Œè€ƒè™‘åˆ°ä¸€ä¸ªå¹³å‡å•è¯åœ¨ä¸º LLM åˆ†è§£æ—¶å®žé™…ä¸Šæ˜¯ 1 åˆ° 3 ä¸ªä»¤ç‰Œã€‚ï¼ˆç»éªŒæ³•åˆ™æ˜¯ 4 ä¸ªä»¤ç‰Œå¯¹åº” 3 ä¸ªå•è¯ï¼Œä½†éšç€è¯­è¨€å˜å¾—æ›´åŠ å¤æ‚æˆ–åœ¨ä¸“ä¸šé¢†åŸŸï¼ˆå¦‚æ³•å¾‹æˆ–åŒ»å­¦ï¼‰ä¸­ï¼Œè¿™ä¸ªæ•°å­—ä¼šå¢žåŠ ã€‚ï¼‰å½“æ‚¨æŸ¥çœ‹é•¿æ–‡æ¡£ã€è¿›è¡ŒæŒç»­äº¤äº’å’Œ AI é©±åŠ¨çš„åº”ç”¨ç¨‹åºæ—¶ï¼Œæ‚¨ä¼šå¾ˆå¿«å‘çŽ°æ‚¨æ— æ³•å°†æ‰€æœ‰å¸Œæœ› LLM çŸ¥é“çš„å†…å®¹éƒ½æ”¾å…¥å…¶ä¸Šä¸‹æ–‡çª—å£ä¸­ã€‚\n\nå› æ­¤ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€äº›å·§å¦™çš„æ–¹æ³•æ¥è§£å†³ä¸Šä¸‹æ–‡çª—å£çš„é™åˆ¶ã€‚ä¾‹å¦‚ï¼š\n\n* **åˆ†å—ã€‚** æ‚¨å¯ä»¥å°†å¤§é‡æ•°æ®åˆ†è§£ï¼Œè®© LLM ä¸€æ¬¡æŸ¥çœ‹ä¸€éƒ¨åˆ†ã€‚è¿™å¯¹äºŽæŸäº›ä»»åŠ¡ï¼ˆæ€»ç»“é•¿æ–‡æ¡£ï¼‰æ•ˆæžœå¾ˆå¥½ï¼Œä½†å¦‚æžœæ‚¨éœ€è¦åˆ†æžè·¨æ•´ä¸ªæ–‡æ¡£çš„æ¦‚å¿µï¼Œåˆ™æ•ˆæžœä¸ä½³ã€‚\n* **å¾®è°ƒã€‚** æ‚¨å¯ä»¥ç”¨ç‰¹å®šçš„æ•°æ®è®­ç»ƒ LLMã€‚é™¤äº†æ—¶é—´å’Œè´¹ç”¨ä¹‹å¤–ï¼Œå…³é”®é—®é¢˜æ˜¯æ‚¨çš„æ–°æ•°æ®å¾ˆå®¹æ˜“è¢« LLM å·²ç»å¸æ”¶çš„æ›´å¤§è§„æ¨¡çš„é€šç”¨è®­ç»ƒæ•°æ®æ‰€æ·¹æ²¡ã€‚é€šå¸¸ï¼Œå®ƒå°±æ˜¯æ— æ³•ä¿ç•™ã€‚æ­¤å¤–ï¼Œè®¸å¤š LLM æ ¹æœ¬ä¸æ”¯æŒå¾®è°ƒâ€”â€”åŒ…æ‹¬ GPT-4 å’Œ Geminiã€‚\n* **æ£€ç´¢å¢žå¼ºç”Ÿæˆ (RAG)ã€‚** é¦–å…ˆï¼Œæ‚¨å°†æ–‡æœ¬å†…å®¹è½¬æ¢ä¸ºä¸€ç§ç‰¹æ®Šè¡¨ç¤ºï¼Œç§°ä¸º *åµŒå…¥*ã€‚ï¼ˆåµŒå…¥æ˜¯ LLM å·¥ä½œçš„é‡è¦éƒ¨åˆ†ã€‚åŸºæœ¬ä¸Šï¼Œå®ƒä»¬æ˜¯æ•æ‰å†…å®¹å«ä¹‰çš„æ•°å€¼è¡¨ç¤ºã€‚ï¼‰ä¸€æ—¦æ‚¨æœ‰äº†åµŒå…¥ï¼Œæ‚¨å°±å°†å®ƒä»¬æ”¾å…¥å‘é‡æ•°æ®åº“ä¸­ã€‚çŽ°åœ¨ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ *è¯­ä¹‰æœç´¢* çš„é­”åŠ›æŸ¥çœ‹æç¤ºï¼Œå¹¶åœ¨æ•°æ®åº“ä¸­æ‰¾åˆ°ä¸Žä¹‹æ¦‚å¿µç›¸å…³çš„å†…å®¹ç‰‡æ®µï¼Œç„¶åŽå°†å…¶è¾“å…¥ LLMã€‚æ¢å¥è¯è¯´ï¼Œæ‚¨åªç»™å®ƒæä¾›é‡è¦çš„å†…å®¹ã€‚\n\næœ€åŽä¸€ç‚¹æ˜¯ä»Šå¤©æœ€å¸¸è§çš„æ–¹æ³•ã€‚RAG é«˜æ•ˆä¸”å¯é¢„æµ‹ã€‚å¦‚æžœæ‚¨æ‹¥æœ‰å¤§é‡æ¾æ•£ç›¸å…³çš„æ–‡æ¡£ï¼Œå®ƒæ•ˆæžœéžå¸¸å¥½ã€‚ä¾‹å¦‚ï¼Œæƒ³è±¡ä¸€ä¸‹æ‚¨æ­£åœ¨åˆ›å»ºä¸€ä¸ªæŠ€æœ¯æ”¯æŒèŠå¤©æœºå™¨äººï¼Œå®ƒä»Žæ‚¨å…¬å¸çš„çŸ¥è¯†åº“æ–‡ç« ä¸­èŽ·å–ä¿¡æ¯ã€‚ä½¿ç”¨ RAGï¼Œæ‚¨æ‰¾åˆ°ç›¸å…³æ•°æ®ï¼Œå¹¶å°†å…¶ä¸Žæ‚¨çš„æç¤ºä¸€èµ·æä¾›ç»™ LLMã€‚åŸºæœ¬ä¸Šï¼Œæ‚¨æ˜¯åœ¨å‘Šè¯‰ LLM åœ¨å›žç­”æç¤ºæ—¶è¯¥åŽ»å“ªé‡ŒæŸ¥æ‰¾ã€‚\n\nä½† RAG å¹¶ä¸å®Œç¾Žã€‚å®ƒè¿«ä½¿æ‚¨èŠ±è´¹æ›´å¤šæ—¶é—´å‡†å¤‡æ•°æ®ã€‚å®ƒä¸å®¹æ˜“è®©æ‚¨è·³å…¥ä¸€ä¸ªå…¨æ–°çš„æ•°æ®é›†ã€‚å¦‚æžœæ‚¨ç¡®å®žéœ€è¦ä¸€æ¬¡è€ƒè™‘å¤§é‡ä¿¡æ¯â€”â€”ä¾‹å¦‚ï¼Œæ‚¨åœ¨å¯»æ‰¾å°è¯´ä¸­çš„æ•´ä½“ä¸»é¢˜æˆ–ä»£ç åº“ä¸­çš„ç‰¹å¾â€”â€”é‚£ä¹ˆå®ƒå°±ä¸å¤Ÿæœ‰æ•ˆã€‚ä½†å°½ç®¡æœ‰å…¶å±€é™æ€§ï¼ŒRAG ä»Šå¤©ä»ç„¶æŽ¥è¿‘æœ€ä½³å®žè·µã€‚\n\nè‡³å°‘ï¼Œåœ¨ Gemini 1.5 Pro ç¿»è½¬å‰§æœ¬ä¹‹å‰æ˜¯è¿™æ ·çš„ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*EEHKDSH0wXa-J6veK5etZA.png)\n\n## æƒŠè‰³æ—¶åˆ»\n\nå°½ç®¡ Gemini 1\\.5 Pro å°šæœªå‘å¸ƒï¼Œä½†å®ƒå·²ç»åœ¨ä¸€ä¸ªä¸¥æ ¼é™åˆ¶çš„è¯•ç”¨ä¸­å¯ç”¨ã€‚ç»“æžœä»¤äººçž©ç›®ã€‚\n\nä¸€äº›æœ€ä»¤äººå°è±¡æ·±åˆ»çš„ä¾‹å­å±•ç¤ºäº† Gemini åˆ›å»ºçš„åˆ†æžï¼Œæ¶µç›–äº†å¤§é‡çŸ¥è¯†ã€‚è°·æ­Œçš„æ¼”ç¤ºä¸€å¦‚æ—¢å¾€åœ°ä»¤äººå°è±¡æ·±åˆ»ï¼Œä½†ä»–ä»¬è¿‡åŽ»æ›¾è¢«æŒ‡æŽ§è¿›è¡Œæ¼”ç¤ºæ“æŽ§å’Œé€‰æ‹©æ€§å±•ç¤ºã€‚æˆ‘æ›´æ„Ÿå…´è¶£çš„æ˜¯ç‹¬ç«‹æµ‹è¯•è€…ï¼Œä»–ä»¬æŠ¥å‘Šçš„ç»“æžœåŒæ ·å¼•äººæ³¨ç›®ã€‚\n\nä¾‹å¦‚ï¼ŒConor Grennan [å‘ Gemini æä¾›äº†ä¸€éƒ¨ 300 é¡µçš„å°è¯´](https://www.youtube.com/watch?v=-MKGsijn5tI)ï¼Œå¹¶è¦æ±‚å®ƒæè¿°ä¸»è¦è§’è‰²ã€æ‰¾å‡ºæƒ…èŠ‚è½¬æŠ˜ï¼Œå¹¶è¯†åˆ«è§’è‰²æ„Ÿå—ç‰¹å®šæƒ…ç»ªçš„ä¾‹å­ã€‚Gemini åœ¨æ•´éƒ¨ä¹¦çš„èŒƒå›´å†…å‘å±•ç»†è‡´çš„è®ºç‚¹æ¯«æ— å›°éš¾ã€‚YouTube ä¸Šæµè¡Œçš„ [Fireship é¢‘é“](https://www.youtube.com/c/fireship) çš„åˆ›ä½œè€… Jeff Delaney å‘ Gemini æä¾›äº†ä¸€ä¸ªåŒ…å«æ•°åƒä¸ªæ–‡ä»¶çš„å®Œæ•´ä»£ç åº“ï¼Œå¹¶è¦æ±‚å®ƒæ·»åŠ æ–°åŠŸèƒ½ã€‚Gemini ä¸ä»…å†™å‡ºäº†æ­£ç¡®çš„ä»£ç ï¼Œè¿˜éµå¾ªäº†çŽ°æœ‰é¡¹ç›®çš„é£Žæ ¼ï¼Œä½¿ç”¨äº†å·²ç»å»ºç«‹çš„ç»„ä»¶ã€åº“å’Œçº¦å®šã€‚å…¶ä»–æ¼”ç¤ºå±•ç¤ºäº† Gemini è¯†åˆ«åº”ç”¨ç¨‹åºä¸­çš„é—®é¢˜ã€æå–å…³é”®ç¤ºä¾‹å¹¶ç¼–å†™ API æ–‡æ¡£ã€‚\n\nå¦‚æžœä½ æƒ³è¦å…¶ä»–å†…å®¹æ¥å¡«å…… Gemini å·¨å¤§çš„ä¸Šä¸‹æ–‡çª—å£ï¼Œè¿˜æœ‰å¦ä¸€ä¸ªæ–°åŠŸèƒ½â€”â€”è§†é¢‘ã€‚è§†é¢‘çš„æ ‡è®°æ–¹å¼ä¸Žæ–‡å­—ä¸åŒï¼Œå ç”¨çš„ç©ºé—´è¦å¤§å¾—å¤šã€‚ä½†å³ä¾¿å¦‚æ­¤ï¼Œ1 ç™¾ä¸‡æ ‡è®°çš„ä¸Šä¸‹æ–‡çª—å£å¯ä»¥å®¹çº³å¤§çº¦ä¸€ä¸ªå°æ—¶çš„è§†é¢‘â€”â€”è¶³å¤Ÿæµè§ˆä¸€éƒ¨ç”µå½±å¹¶å›žç­”æœ‰å…³å…¶å†…å®¹çš„å¤æ‚é—®é¢˜ã€‚è¿™å°±æ˜¯è°·æ­Œæ‰€åšçš„ï¼Œå½“å®ƒè¦æ±‚ Gemini [æŸ¥æ‰¾å…·ä½“ç»†èŠ‚](https://www.youtube.com/watch?v=wa0MT8OwHuk) åœ¨ä¸€éƒ¨å·´æ–¯ç‰¹Â·åŸºé¡¿çš„ç”µå½±ä¸­ï¼Œæ¯”å¦‚åœ¨ä»–ä»¬æœªè¯†åˆ«çš„ä¸€ä¸ªåœºæ™¯ä¸­ï¼Œçº¸ç‰‡ä¸Šå†™çš„å­—ã€‚\n\n## æœªæ¥çš„LLM\n\nå¤§ä¸Šä¸‹æ–‡çª—å£æ˜¯æœªæ¥çš„æ–¹å‘å—ï¼Ÿåˆ°ç›®å‰ä¸ºæ­¢ï¼Œæ™®éçš„çœ‹æ³•æ˜¯ï¼Œå¤§ä¸Šä¸‹æ–‡çª—å£å……å…¶é‡åªæ˜¯ä¸€ä¸ªéƒ¨åˆ†è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬æ‹…å¿ƒå®ƒä»¬åœ¨è®¡ç®—æ—¶é—´ä¸Šä¼šè¿‡äºŽæ˜‚è´µã€‚[ä¸€é¡¹ç ”ç©¶](https://www.voiceflow.com/blog/the-context-window-paradox-why-bigger-might-not-be-better)å‘çŽ°ï¼ŒLLMåœ¨é•¿ä¸Šä¸‹æ–‡çª—å£ä¸­æ‰¾åˆ°ä¿¡æ¯çš„èƒ½åŠ›å¹¶ä¸å¥½ï¼Œåè€Œåœ¨ç»†èŠ‚å‡ºçŽ°åœ¨å¼€å¤´æˆ–ç»“å°¾æ—¶è¡¨çŽ°æ›´ä½³ã€‚æ‰€æœ‰è¿™äº›å› ç´ æ”¯æŒäº†åŒæ ·çš„ç»“è®ºï¼šå°†ä½ çš„å†…å®¹å¼ºè¡Œå¡žå…¥ä¸Šä¸‹æ–‡çª—å£æ˜¯å¤©çœŸçš„ä¸”æˆæœ¬é«˜æ˜‚çš„ã€‚å°†æ‰€æœ‰æ•°æ®ä¸€æ¬¡æ€§å‘é€è¯·æ±‚ç»ä¸æ˜¯ä¸ŽLLMå¯¹è¯çš„æ­£ç¡®æ–¹å¼ã€‚\n\nçŽ°åœ¨ï¼Œæœªæ¥ä¼¼ä¹Žçªç„¶å‘ç”Ÿäº†å˜åŒ–ã€‚å¤§ä¸Šä¸‹æ–‡çª—å£å³å°†æ¥ä¸´ï¼Œå®ƒä»¬å¯èƒ½ä½¿LLMå¯¹å¹¿æ³›çŸ¥è¯†é›†æœ‰æ›´å¼ºå¤§ã€æ•´ä½“çš„ç†è§£ã€‚åŽ»å¹´ç”¨æ–‡æœ¬æ— æ³•å®Œæˆçš„ä»»åŠ¡çŽ°åœ¨å³å°†åœ¨*è§†é¢‘*ä¸­å˜å¾—å¯èƒ½ã€‚è€Œè°·æ­Œç ”ç©¶æ­£åœ¨å°è¯•ä¸€ç§æ‰©å±•ä¸Šä¸‹æ–‡çª—å£åˆ°æƒŠäººçš„1000ä¸‡æ ‡è®°çš„Geminiå˜ä½“ã€‚\n\nä¸¤ä¸ªäº‹å®žæ˜¯æ˜Žç¡®çš„ã€‚é¦–å…ˆï¼Œåœ¨LLMæˆ˜äº‰ä¸­é€‰æ‹©èµ¢å®¶æ˜¯ä¸€åœºæ„šè ¢çš„æ¸¸æˆã€‚å…¶æ¬¡ï¼Œå˜åŒ–çš„é€Ÿåº¦æ²¡æœ‰æ”¾ç¼“â€”â€”åè€Œåœ¨åŠ é€Ÿã€‚\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/google-releases-gemma-a-lightweight-and-open-source-model-b6411d67ecca","frontmatter":{"title":"Google å‘å¸ƒ Gemma â€” è½»é‡çº§å¼€æºæ¨¡åž‹","meta_title":"Google å‘å¸ƒ Gemma â€” è½»é‡çº§å¼€æºæ¨¡åž‹","description":"Google å‘å¸ƒäº† Gemmaï¼Œè¿™æ˜¯ä¸€ç³»åˆ—è½»é‡çº§å¼€æºæ¨¡åž‹ï¼ŒåŸºäºŽåˆ›å»º Gemini çš„ç ”ç©¶å’ŒæŠ€æœ¯æž„å»ºâ€¦â€¦","date":"2024-10-29T12:46:34.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*G7XbkhsCwillpje7AvETjQ.jpeg","categories":["Natural Language Processing","Programming","Chatbots"],"author":"Rifx.Online","tags":["Gemma","Gemini","parameters","NLP","chatbots"],"draft":false,"slug":"blog/google-releases-gemma-a-lightweight-and-open-source-model-b6411d67ecca"},"content":"\n\n\n\n\nåœ¨çŸ­çŸ­ä¸€å‘¨å†…ï¼Œä¸–ç•Œè§è¯äº†ä¸¤å®¶ç§‘æŠ€å·¨å¤´å¸¦æ¥çš„æœ€å…·çªç ´æ€§çš„AIè¿›å±•ã€‚OpenAIæŽ¨å‡ºäº†ä»¤äººæƒŠå¹çš„AIè§†é¢‘ç”Ÿæˆå™¨[Sora](https://readmedium.com/3d16381f3bf5)ï¼Œè€Œè°·æ­Œåˆ™æ­æ™“äº†å…¶[Gemini 1.5æ¨¡åž‹](https://generativeai.pub/google-releases-gemini-1-5-with-1m-context-window-44ed4a2ea319)ï¼Œèƒ½å¤Ÿæ”¯æŒæœ€å¤š100ä¸‡çš„ä¸Šä¸‹æ–‡çª—å£ã€‚\n\nä»Šå¤©ï¼Œè°·æ­Œå†æ¬¡å¼•å‘è½°åŠ¨ï¼Œå‘å¸ƒäº†[Gemma](https://ai.google.dev/gemma/?utm_source=keyword&utm_medium=referral&utm_campaign=gemma_cta&utm_content)ï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§ã€æœ€å…ˆè¿›çš„å¼€æºæ¨¡åž‹å®¶æ—ï¼Œå»ºç«‹åœ¨ç”¨äºŽåˆ›å»ºGeminiæ¨¡åž‹çš„ç ”ç©¶å’ŒæŠ€æœ¯åŸºç¡€ä¹‹ä¸Šã€‚\n\n## ä»€ä¹ˆæ˜¯ Gemmaï¼Ÿ\n\nGemma ä»¥æ‹‰ä¸è¯­ *gemma* æ„ä¸ºâ€œçè´µçš„å®çŸ³â€å‘½åï¼Œæ±²å–äº†å…¶å‰èº« Gemini çš„çµæ„Ÿï¼Œåæ˜ äº†å…¶åœ¨ç§‘æŠ€é¢†åŸŸçš„ä»·å€¼å’Œç¨€æœ‰æ€§ã€‚\n\nå®ƒä»¬æ˜¯æ–‡æœ¬åˆ°æ–‡æœ¬ã€ä»…è§£ç çš„å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼Œæä¾›è‹±è¯­ç‰ˆæœ¬ï¼Œå…·æœ‰å¼€æ”¾æƒé‡ã€é¢„è®­ç»ƒå˜ä½“å’ŒæŒ‡ä»¤è°ƒä¼˜å˜ä½“ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Fu2ryJMunebq5c0dD-opZQ.png)\n\nGemma ä»Žä»Šå¤©èµ·åœ¨å…¨çƒèŒƒå›´å†…æä¾›ï¼Œåˆ†ä¸ºä¸¤ç§å°ºå¯¸ï¼ˆ2B å’Œ 7Bï¼‰ï¼Œæ”¯æŒå¹¿æ³›çš„å·¥å…·å’Œç³»ç»Ÿï¼Œå¹¶å¯åœ¨å¼€å‘è€…çš„ç¬”è®°æœ¬ç”µè„‘å’Œå·¥ä½œç«™ä¸Šè¿è¡Œã€‚\n\n## 2 æ¨¡åž‹å¤§å°å’Œèƒ½åŠ›\n\nGemma æ¨¡åž‹æœ‰ 20 äº¿å’Œ 70 äº¿å‚æ•°ä¸¤ç§è§„æ¨¡ã€‚2B æ¨¡åž‹æ—¨åœ¨è¿è¡Œåœ¨ç§»åŠ¨è®¾å¤‡å’Œç¬”è®°æœ¬ç”µè„‘ä¸Šï¼Œè€Œ 7B æ¨¡åž‹åˆ™é€‚ç”¨äºŽæ¡Œé¢è®¡ç®—æœºå’Œå°åž‹æœåŠ¡å™¨ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*sH9jaz1RvtKeJ5yjfyOL5Q.png)\n\n**è°ƒä¼˜æ¨¡åž‹**\n\nGemma è¿˜æœ‰ä¸¤ä¸ªç‰ˆæœ¬ï¼šè°ƒä¼˜ç‰ˆå’Œé¢„è®­ç»ƒç‰ˆã€‚\n\n* **é¢„è®­ç»ƒï¼š** è¿™å°±åƒåŸºç¡€æ¨¡åž‹ï¼Œæ²¡æœ‰ä»»ä½•å¾®è°ƒã€‚è¯¥æ¨¡åž‹æ²¡æœ‰é’ˆå¯¹ Gemma æ ¸å¿ƒæ•°æ®è®­ç»ƒé›†ä»¥å¤–çš„ç‰¹å®šä»»åŠ¡æˆ–æŒ‡ä»¤è¿›è¡Œè®­ç»ƒã€‚\n* **æŒ‡ä»¤è°ƒä¼˜ï¼š** è¯¥æ¨¡åž‹ç»è¿‡å¾®è°ƒï¼Œä»¥é€‚åº”äººç±»è¯­è¨€äº¤äº’ï¼Œä»Žè€Œæé«˜å…¶æ‰§è¡Œç‰¹å®šä»»åŠ¡çš„èƒ½åŠ›ã€‚\n\n## å®ƒä¸Žç«žäº‰å¯¹æ‰‹çš„æ¯”è¾ƒï¼Ÿ\n\nç”±äºŽä½“ç§¯å°ï¼ŒGemmaèƒ½å¤Ÿç›´æŽ¥åœ¨ç”¨æˆ·çš„ç¬”è®°æœ¬ç”µè„‘ä¸Šè¿è¡Œã€‚ä¸‹å›¾æ˜¾ç¤ºäº†Gemma (7B)çš„è¯­è¨€ç†è§£å’Œç”Ÿæˆæ€§èƒ½ä¸Žç±»ä¼¼è§„æ¨¡çš„å¼€æ”¾æ¨¡åž‹å¦‚LLaMA 2 (7B)ã€LLaMA 2 (13B)å’ŒMistral (7B)çš„æ¯”è¾ƒã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*QxjZALUAIDiS_T66EpOu-g.png)\n\næ‚¨å¯ä»¥åœ¨[è¿™é‡Œ](https://ai.google.dev/gemma/?utm_source=keyword&utm_medium=referral&utm_campaign=gemma_cta&utm_content)æŸ¥çœ‹æ¯ä¸ªåŸºå‡†çš„æ›´è¯¦ç»†æ¯”è¾ƒã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Fc8Fk0Dgh2VFU_VLhpcs6Q.png)\n\n## å®ƒçš„ç”¨é€”æ˜¯ä»€ä¹ˆï¼Ÿ\n\nä»¥ä¸‹æ˜¯ Gemma å¯èƒ½çš„ä½¿ç”¨åœºæ™¯ï¼š\n\n**å†…å®¹åˆ›ä½œä¸Žæ²Ÿé€š**\n\n* æ–‡æœ¬ç”Ÿæˆ\n* èŠå¤©æœºå™¨äººå’Œå¯¹è¯å¼ AI\n* æ–‡æœ¬æ‘˜è¦\n\n**ç ”ç©¶ä¸Žæ•™è‚²**\n\n* **è‡ªç„¶è¯­è¨€å¤„ç† (NLP) ç ”ç©¶ï¼š** ä½œä¸º NLP ç ”ç©¶çš„åŸºç¡€ï¼Œå®žéªŒæŠ€æœ¯ï¼Œå¼€å‘ç®—æ³•ï¼Œå¹¶ä¸ºè¯¥é¢†åŸŸçš„è¿›æ­¥åšå‡ºè´¡çŒ®ã€‚\n* **è¯­è¨€å­¦ä¹ å·¥å…·ï¼š** æ”¯æŒäº’åŠ¨è¯­è¨€å­¦ä¹ ä½“éªŒï¼Œå¸®åŠ©è¯­æ³•çº æ­£ï¼Œæˆ–æä¾›å†™ä½œç»ƒä¹ ã€‚\n* **çŸ¥è¯†æŽ¢ç´¢ï¼š** å¸®åŠ©ç ”ç©¶äººå‘˜é€šè¿‡ç”Ÿæˆæ‘˜è¦æˆ–å›žç­”ç‰¹å®šä¸»é¢˜çš„é—®é¢˜æ¥æŽ¢ç´¢å¤§é‡æ–‡æœ¬ã€‚\n\nä»¥å‰éœ€è¦æžå¤§æ¨¡åž‹çš„ä»»åŠ¡çŽ°åœ¨å¯ä»¥é€šè¿‡æœ€å…ˆè¿›çš„å°åž‹æ¨¡åž‹æ¥å®žçŽ°ã€‚è¿™å¼€å¯äº†å¼€å‘ AI åº”ç”¨ç¨‹åºçš„å…¨æ–°æ–¹å¼ï¼Œæˆ‘ä»¬å¾ˆå¿«å¯èƒ½ä¼šåœ¨æ™ºèƒ½æ‰‹æœºä¸Šçœ‹åˆ°æ— éœ€äº’è”ç½‘è¿žæŽ¥çš„è®¾å¤‡å†… AI èŠå¤©æœºå™¨äººã€‚\n\nè¿™æœ‰å¤šä»¤äººå…´å¥‹å‘¢ï¼Ÿ\n\n## è¿™çœŸçš„å¥½å—ï¼Ÿ\n\nå‡ ä½ [redditors](https://www.reddit.com/r/LocalLLaMA/comments/1awbqwd/gemma_7b_the_latest_opensource_model_from_google/) åˆ†äº«äº†ä»–ä»¬ä½¿ç”¨ Gemma çš„ç»éªŒï¼Œåˆ°ç›®å‰ä¸ºæ­¢ï¼Œç»“æžœå¹¶ä¸ç†æƒ³ã€‚çœ‹çœ‹è¿™ä¸ªä¾‹å­ï¼ŒGemma åœ¨å›žç­”å…³äºŽé‡é‡çš„é—®é¢˜æ—¶ç»™å‡ºäº†é”™è¯¯çš„ç­”æ¡ˆã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Sdaiaqcuz7qbftG1)\n\næˆ‘è‡ªå·±è¿˜æ²¡æœ‰çœŸæ­£å°è¯•è¿‡ï¼Œä½†é‡è¦çš„æ˜¯è¦è®°ä½ï¼Œåƒè¿™æ ·çš„è¾ƒå°æ¨¡åž‹é¢„è®¡ä¼šæœ‰ä¸€äº›ç¼ºé™·ï¼Œæœ‰æ—¶å¯èƒ½ä¼šç»™å‡ºé”™è¯¯çš„ç­”æ¡ˆã€‚\n\n## å°è¯•è‡ªå·±åŠ¨æ‰‹\n\næ‚¨å¯ä»¥ä»Šå¤©å¼€å§‹ä½¿ç”¨Gemmaï¼Œé€šè¿‡Kaggleçš„å…è´¹è®¿é—®ã€Colabç¬”è®°æœ¬çš„å…è´¹å±‚ä»¥åŠé¦–æ¬¡ä½¿ç”¨Google Cloudçš„ç”¨æˆ·å¯èŽ·å¾—çš„$300ä¿¡ç”¨é¢åº¦ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BrvLnczy724TPrsk-uFJCw.png)\n\nå¦‚æžœæ‚¨æœ‰å…´è¶£å¼€å§‹ä½¿ç”¨Gemmaï¼Œè¯·æŸ¥çœ‹è¿™äº›æŒ‡å—ï¼Œä»¥äº†è§£ä»Žæ–‡æœ¬ç”Ÿæˆåˆ°åœ¨Gemmaæ¨¡å¼ä¸‹éƒ¨ç½²çš„è¿‡ç¨‹ï¼š\n\n* **ä½¿ç”¨Gemmaè¿›è¡Œæ–‡æœ¬ç”Ÿæˆ**ï¼šæž„å»ºä¸€ä¸ªåŸºæœ¬çš„æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹ã€‚\n* **ä½¿ç”¨LoRAè°ƒä¼˜Gemma**ï¼šå¯¹Gemma 2Bæ¨¡åž‹è¿›è¡ŒLoRAå¾®è°ƒã€‚\n* **ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒè°ƒä¼˜Gemmaæ¨¡åž‹**ï¼šä½¿ç”¨Keraså’ŒJAXåŽç«¯å¯¹Gemma 7Bæ¨¡åž‹è¿›è¡ŒLoRAå’Œæ¨¡åž‹å¹¶è¡Œçš„å¾®è°ƒã€‚\n* **å°†Gemmaéƒ¨ç½²åˆ°ç”Ÿäº§çŽ¯å¢ƒ**ï¼šä½¿ç”¨Vertex AIå°†Gemmaéƒ¨ç½²åˆ°ç”Ÿäº§çŽ¯å¢ƒã€‚\n\n## ä¸‹è½½æ¨¡åž‹\n\nå¼€æ”¾æ¨¡åž‹ç›®å‰å¯åœ¨ [HuggingFace](https://huggingface.co/models?other=gemma&sort=trending&search=google) ä¸ŠèŽ·å–ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*mJRzGhO1sUxPL4_3YjpNGA.png)\n\nGemma æ¨¡åž‹ä¹Ÿå¯ä»¥ä»Ž [Kaggle Models](https://www.kaggle.com/models/google/gemma) ä¸‹è½½ã€‚\n\n## æœ€åŽçš„æ€è€ƒ\n\nè™½ç„¶Gemmaæ¨¡åž‹å¯èƒ½ä½“ç§¯å°ä¸”ç¼ºä¹å¤æ‚æ€§ï¼Œä½†å®ƒä»¬åœ¨é€Ÿåº¦å’Œä½¿ç”¨æˆæœ¬ä¸Šå¯èƒ½ä¼šæœ‰æ‰€å¼¥è¡¥ã€‚\n\nä»Žæ›´å¤§çš„è§’åº¦æ¥çœ‹ï¼Œè°·æ­Œå¹¶ä¸æ˜¯è¿½é€çŸ­æœŸçš„æ¶ˆè´¹è€…å…´å¥‹ï¼Œè€Œæ˜¯åœ¨ä¸ºä¼ä¸šåŸ¹è‚²å¸‚åœºã€‚ä»–ä»¬è®¾æƒ³å…¬å¸ä¼šä¸ºè°·æ­Œäº‘æœåŠ¡ä»˜è´¹ï¼Œå› ä¸ºå¼€å‘è€…ä½¿ç”¨Gemmaæ¥åˆ›å»ºåˆ›æ–°çš„æ–°æ¶ˆè´¹åº”ç”¨ã€‚\n\næ­¤å¤–ï¼Œå°½ç®¡Geminiçš„åå“å¹³å¹³ï¼Œè°·æ­Œä»ç„¶å±•ç¤ºäº†å®ƒè¿˜æœ‰æ›´å¤šçš„ç§˜å¯†æ­¦å™¨ã€‚\n\nå½“ç„¶ï¼Œå¯¹äºŽä»»ä½•å¼ºå¤§çš„æŠ€æœ¯æ¥è¯´ï¼ŒçœŸæ­£çš„è€ƒéªŒæ˜¯å®ƒçš„å®žé™…æ•ˆæžœã€‚è°·æ­Œçš„è¿‡åŽ»å¼•å‘äº†ä¸€ä¸ªé—®é¢˜ï¼šè¿™äº›æ¨¡åž‹åœ¨çŽ°å®žä¸–ç•Œä¸­çš„è¡¨çŽ°æ˜¯å¦èƒ½å¦‚æ‰¿è¯ºçš„é‚£æ ·å‡ºè‰²ã€‚å¯†åˆ‡å…³æ³¨è¿™ä¸€ç‚¹æ˜¯é‡è¦çš„ï¼Œä½†ä¹Ÿå¸Œæœ›è°·æ­Œèƒ½ä»Žè¿‡åŽ»ä¸­å¸å–æ•™è®­ï¼Œæä¾›çœŸæ­£å¯æ¯”ç”šè‡³ä¼˜äºŽç«žäº‰å¯¹æ‰‹çš„æ¨¡åž‹ã€‚\n\næˆ‘è¿«ä¸åŠå¾…æƒ³è¦ä½“éªŒGemmaï¼Œå¹¶ä¸”æˆ‘ä¸€å®šä¼šåˆ†äº«æˆ‘å¯¹è¿™ä¸ªæ–°AIæ¨¡åž‹çš„åˆæ­¥æƒ³æ³•å’Œå‘çŽ°ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*8BDnUV9iQisOyeN3.png)\n\nè¿™ç¯‡æ–‡ç« å‘å¸ƒåœ¨[Generative AI](https://generativeai.pub/)ã€‚è¯·åœ¨[LinkedIn](https://www.linkedin.com/company/generative-ai-publication)ä¸Šä¸Žæˆ‘ä»¬è”ç³»ï¼Œå¹¶å…³æ³¨[Zeniteq](https://www.zeniteq.com/)ï¼Œä»¥èŽ·å–æœ€æ–°çš„AIæ•…äº‹ã€‚è®©æˆ‘ä»¬ä¸€èµ·å¡‘é€ AIçš„æœªæ¥ï¼\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*JeeoUhaBYUJGr0Xq.png)\n\n"},{"lang":"zh","group":"blog","slug":"blog/goover-a-new-search-engine-challenging-perplexity-ai-18c38b75dece","frontmatter":{"title":"Goover - ä¸€ç§æŒ‘æˆ˜äººå·¥æ™ºèƒ½å¤æ‚æ€§çš„æ–°æœç´¢å¼•æ“Ž","meta_title":"Goover - ä¸€ç§æŒ‘æˆ˜äººå·¥æ™ºèƒ½å¤æ‚æ€§çš„æ–°æœç´¢å¼•æ“Ž","description":"Gooveræ˜¯ä¸€æ¬¾æ–°å…´çš„AIé©±åŠ¨æœç´¢å¼•æ“Žï¼Œæ—¨åœ¨æŒ‘æˆ˜Perplexity AIï¼Œæä¾›å‡†ç¡®çš„æœç´¢ç»“æžœå’Œç”¨æˆ·å‹å¥½çš„ä½“éªŒã€‚å…¶ä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬æ´žå¯ŸæŠ¥å‘Šã€è¶…ä¸ªæ€§åŒ–ç®€æŠ¥ã€æ·±åº¦å’Œå¿«é€Ÿå›žç­”ï¼Œä»¥åŠå‚è€ƒè¿½è¸ªåŠŸèƒ½ã€‚ä¸ŽPerplexityç›¸æ¯”ï¼ŒGooveråœ¨ç ”ç©¶èƒ½åŠ›å’Œä¿¡æ¯æ¥æºçš„å¤šæ ·æ€§ä¸Šè¡¨çŽ°å‡ºè‰²ï¼Œä½†åœ¨æŸäº›é€»è¾‘å’Œæ•°å­¦é—®é¢˜ä¸Šä»æœ‰å¾…æ”¹è¿›ã€‚Gooverçš„æœªæ¥å‘å±•æ½œåŠ›å·¨å¤§ï¼Œå°¤å…¶æ˜¯åœ¨ä¸ªæ€§åŒ–å’Œç”Ÿæˆå¼AIçš„ç»“åˆæ–¹é¢ã€‚","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*YQH6-bv6bjVn199pk1uC-Q.jpeg","categories":["Technology/Web","AI","Search Engines"],"author":"Rifx.Online","tags":["Goover","accuracy","user-friendliness","fact-checked","misinformation"],"draft":false,"slug":"blog/goover-a-new-search-engine-challenging-perplexity-ai-18c38b75dece"},"content":"\n\n\n\n\nåœ¨ 2024 å¹´ï¼Œæœç´¢å¼•æ“Žå¸‚åœºç»åŽ†äº†ä¸€æ¬¡é‡å¤§å˜é©ã€‚è°·æ­Œæœç´¢ä½œä¸ºæœ€å¤§å’Œæœ€å—æ¬¢è¿Žçš„æœç´¢å¼•æ“Žï¼Œåœ¨æŽ¨å‡ºå…¶æ–°çš„ AI é©±åŠ¨æ¦‚è§ˆåŠŸèƒ½åŽï¼Œé¢ä¸´äº†ä¸€æ³¢æ‰¹è¯„ï¼Œè®¸å¤šç”¨æˆ·è®¤ä¸ºè¯¥åŠŸèƒ½åŒ†å¿™ä¸Šçº¿ä¸”ä¸å¤Ÿå®Œå–„ã€‚\n\nä¸Žæ­¤åŒæ—¶ï¼ŒAI é©±åŠ¨çš„æœç´¢å¼•æ“Ž Perplexity AI å¿«é€ŸèŽ·å¾—äº†äººæ°”ï¼Œå› å…¶å¤‡å—èµžèª‰çš„åŠŸèƒ½è€Œç§¯ç´¯äº†å¿ å®žç”¨æˆ·ç¾¤ã€‚æœ€è¿‘ï¼Œç”šè‡³ OpenAI ä¹Ÿé€šè¿‡åœ¨ ChatGPT ä¸­æ•´åˆæ–°çš„æœç´¢åŠŸèƒ½åŠ å…¥äº†æœç´¢é¢†åŸŸã€‚\n\néšç€è¶Šæ¥è¶Šå¤šçš„ AI é©±åŠ¨æœç´¢å¼•æ“Žçš„å‡ºçŽ°ï¼Œç¡®å®šå“ªä¸ªæ˜¯æœ€å¥½çš„å˜å¾—è¶Šæ¥è¶Šæ£˜æ‰‹ã€‚\n\nçŽ°åœ¨ï¼Œæœ‰ä¸€ä¸ªæ–°çŽ©å®¶è¿›å…¥äº† AI é©±åŠ¨æœç´¢å¼•æ“Žçš„å¸‚åœºï¼Œæ‰¿è¯ºæä¾›æ›´å‡†ç¡®çš„ç»“æžœâ€”â€”å®ƒè¢«ç§°ä¸º [**Goover**](https://intro.goover.ai/)ã€‚\n\n## ä»€ä¹ˆæ˜¯ Gooverï¼Ÿ\n\nGoover æ˜¯ä¸€ä¸ªæ–°çš„ AI æœç´¢å¹³å°ï¼Œæä¾›ç±»ä¼¼äºŽ Perplexity AI çš„ç»è¿‡äº‹å®žæ£€æŸ¥å’Œå‚è€ƒæ”¯æŒçš„è§è§£ã€‚å®ƒæä¾›ä¸€ä¸ªå¯é çš„ã€äº’åŠ¨çš„ AI ä½“éªŒï¼Œä¸“æ³¨äºŽå‡†ç¡®æ€§å’Œç”¨æˆ·å‹å¥½æ€§ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*-k5wsFFeO-wxsgQmC4R6sA.png)\n\nGoover ä»ç„¶æ˜¯ä¸€ä¸ªæ–°å¹³å°ï¼Œå…¬å¸æœªæ¥è¿˜æœ‰å¾ˆå¤šæ›´é…·çš„åŠŸèƒ½è¦æŽ¨å‡ºã€‚æ‚¨å¯ä»¥åœ¨ [è¿™é‡Œ](https://intro.goover.ai/) æŸ¥çœ‹æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚\n\n## Gooverçš„ä¸»è¦ç‰¹æ€§\n\nGooveré…å¤‡äº†å¤šç§æœ‰è¶£çš„åŠŸèƒ½ï¼š\n\n* **æ´žå¯ŸæŠ¥å‘Šï¼š** å®ƒé‡‡ç”¨äº†ä¸€äº›å…ˆè¿›çš„LLMæŠ€æœ¯ï¼Œåˆ†æžæ‚¨çš„æ•°æ®å¹¶ç”Ÿæˆå…¨é¢çš„æŠ¥å‘Šã€‚\n* **è¶…ä¸ªæ€§åŒ–ç®€æŠ¥ï¼š** å‘çŽ°ä¸»é¢˜æ‘˜è¦ã€æ´žå¯ŸæŠ¥å‘Šä»¥åŠä¸€ç³»åˆ—ä¸ªæ€§åŒ–æŽ¨èã€‚\n* **æ·±åº¦å›žç­”ï¼š** å½“æ‚¨éœ€è¦æ·±å…¥çš„ç­”æ¡ˆæ—¶ï¼ŒGooverå¯ä»¥æä¾›ã€‚è¿™äº›å›žç­”ç”Ÿæˆæ—¶é—´ç¨é•¿ï¼Œä½†æä¾›æ›´è¯¦ç»†ã€æ·±æ€ç†Ÿè™‘çš„è§è§£ã€‚\n* **å¿«é€Ÿå›žç­”ï¼š** é€‚åˆæ‚¨éœ€è¦å¿«é€ŸèŽ·å–ç®€å•ä¿¡æ¯æ—¶ã€‚Gooveræä¾›ç®€æ˜Žçš„ç­”æ¡ˆï¼ŒåŒæ—¶ä¸ç‰ºç‰²ç›¸å…³æ€§ã€‚\n\n## Gooverçš„ç‹¬ç‰¹ä¹‹å¤„æ˜¯ä»€ä¹ˆï¼Ÿ\n\nä»¥ä¸‹æ˜¯å°†å…¶ä¸Žå…¶ä»–AIæœç´¢å¼•æ“ŽåŒºåˆ†å¼€æ¥çš„åŠŸèƒ½é›†ã€‚\n\n* **ç®€æŠ¥é¡µé¢**ï¼šå¿«é€Ÿã€ç›¸å…³çš„ä¸»é¢˜æ‘˜è¦è®©ç”¨æˆ·åœ¨ä¸éœ€é•¿æ—¶é—´é˜…è¯»çš„æƒ…å†µä¸‹ä¿æŒä¿¡æ¯çµé€šã€‚\n* **å‚è€ƒè¿½è¸ª**ï¼šæ¯ä¸ªå“åº”éƒ½é“¾æŽ¥åˆ°å¯ä¿¡çš„æ¥æºï¼Œç¡®ä¿é€æ˜Žåº¦å¹¶å‡å°‘é”™è¯¯ä¿¡æ¯ã€‚\n* **åå¹»è§‰**ï¼šå“åº”åŸºäºŽç»è¿‡éªŒè¯çš„æ•°æ®ï¼Œå¢žå¼ºå¯ä¿¡åº¦ã€‚\n\nçŽ°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å®ƒä¸ŽPerplexity AIçš„æ­£é¢æ¯”è¾ƒã€‚\n\n## å®ƒä¸Ž Perplexity AI çš„æ¯”è¾ƒå¦‚ä½•ï¼Ÿ\n\nè®©æˆ‘ä»¬ä»Žç”¨æˆ·ç•Œé¢å¼€å§‹ã€‚\n\nGoover å’Œ Perplexity éƒ½æœ‰å¹²å‡€çš„è®¾è®¡ï¼Œä¸­å¤®æœ‰ä¸€ä¸ªæ˜¾è‘—çš„æœç´¢æ¡†ã€‚ç„¶è€Œï¼ŒGoover åœ¨æœç´¢æ ä¸‹æ–¹æœ‰ä¸€ä¸ªâ€œæ™ºèƒ½æŽ¨é€â€å’Œâ€œæ™ºèƒ½ç®€æŠ¥â€éƒ¨åˆ†ã€‚\n\nå¦‚æžœæ‚¨æ˜¯ä¸€ä¸ªå®šæœŸæŸ¥çœ‹æ–°é—»æˆ–å¸Œæœ›å¿«é€ŸèŽ·å–ä¸Šä¼ æ–‡ä»¶è§è§£çš„äººï¼Œæ‚¨ä¼šæ¬£èµ Goover ä¸­è¿™äº›é™„åŠ åŠŸèƒ½ã€‚\n\nä»¥ä¸‹æ˜¯å®ƒä»¬ä¸»é¡µçš„å¹¶æŽ’æ¯”è¾ƒï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*rDQjkIvsy2gXKGIO4-y5Lg.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*r-x6e8SVT7QdFNDOj5i54w.png)\n\nGoover å¦ä¸€ä¸ªæ˜¾è‘—ç‰¹ç‚¹æ˜¯æ”¯æŒæ›´å¹¿æ³›çš„æ–‡ä»¶ç±»åž‹ã€‚é™¤äº†ä¼ ç»Ÿçš„æ–‡ä»¶ä¸Šä¼ ï¼Œæ‚¨è¿˜å¯ä»¥é™„åŠ ä¸ªäººç¬”è®°ã€ä¿å­˜çš„é“¾æŽ¥å’Œèµ„æºã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*U9TPAhUoZPRXGIg6GbWLQA.png)\n\nè€Œ Perplexity ä»…æ”¯æŒæ–‡ä»¶ä¸Šä¼ ã€‚\n\nä»¥ä¸‹æ˜¯æ‚¨å°è¯•åœ¨ Goover ä¸­ä¸Šä¼ æ–‡ä»¶ã€ç¬”è®°å’Œ URL ä½œä¸ºå‚è€ƒæ—¶çš„æ ·å­ï¼Œç„¶åŽå®ƒå¼€å§‹åœ¨ç½‘ç»œæˆ–çŸ¥è¯†åº“ä¸­è¿›è¡Œæœç´¢ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*VAQbKV7LMhEDOgvxRqKoUQ.png)\n\nè¿™ç§é¢å¤–çš„çµæ´»æ€§å¯¹äºŽéœ€è¦æœ‰ç»„ç»‡å’Œå…¨é¢ç ”ç©¶çš„ç”¨æˆ·å¯èƒ½å¾ˆæœ‰ç”¨ã€‚\n\nçŽ°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹ Goover å’Œ Perplexity åœ¨ä»¥ä¸‹èƒ½åŠ›æ–¹é¢çš„è¡¨çŽ°ï¼š\n\n1. **ç ”ç©¶èƒ½åŠ›**\n2. **æ•°å­¦è®¡ç®—**\n3. **ç½‘ç»œæœç´¢èƒ½åŠ›**\n4. **é€»è¾‘é—®é¢˜**\n\n## ç ”ç©¶èƒ½åŠ›\n\nåœ¨è¿™ä¸ªæµ‹è¯•ä¸­ï¼Œæˆ‘æƒ³çœ‹çœ‹è¿™ä¸¤ä¸ªå·¥å…·æ˜¯å¦èƒ½å¤Ÿå‡†ç¡®æä¾›æµè¡Œçš„ AI å›¾åƒæ¨¡åž‹çš„å‘å¸ƒæ—¥æœŸå’Œç‰ˆæœ¬ã€‚\n\n> **æç¤ºï¼š** ç»™æˆ‘ä¸€ä¸ªæœ€ä½³å’Œæœ€æµè¡Œçš„ AI å›¾åƒç”Ÿæˆæ¨¡åž‹çš„æ—¶é—´çº¿ï¼ˆStable diffusion, Dall\\-E, Imagen, Midjourney, Fluxï¼‰\n\nè¿™æ˜¯ Goover çš„ç»“æžœï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*RfiaKnkpyFuczXnhwkGmVA.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*xVGPee-zmBD9b35zWChOPw.png)\n\næ­£å¦‚ä½ æ‰€çœ‹åˆ°çš„ï¼ŒAI ç»™äº†æˆ‘ 11 ä¸ªå›¾åƒæ¨¡åž‹çš„æ—¶é—´çº¿ã€‚ç„¶è€Œï¼ŒPerplexity åªæåˆ°äº† 9 ä¸ªæ¨¡åž‹ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*faI78jCmgKyyy55sF7C2Ww.png)\n\nä»”ç»†è§‚å¯Ÿç»“æžœï¼Œæˆ‘æ³¨æ„åˆ° Goover èƒ½å¤ŸèŽ·å–å…³äºŽå³å°†å‘å¸ƒçš„ Midjourney V7 çš„ä¿¡æ¯ã€‚è¿™æ˜¯ Perplexity æ— æ³•æä¾›çš„ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*X6ERPPq83MxYV4soHmptJA.png)\n\nè¿™ç§ç»†èŠ‚ä½¿ Goover å…·æœ‰ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯å¯¹äºŽå¸Œæœ›é¦–æ¬¡èŽ·å¾—å…¨é¢ä¿¡æ¯çš„ç”¨æˆ·ã€‚\n\nè®©æˆ‘ä»¬å†åšä¸€ä¸ªã€‚\n\n> **æç¤ºï¼š** éžæ³•è½¯ä»¶ä½¿ç”¨æœ‰ä»€ä¹ˆå½±å“ï¼Ÿ\n\nè¿™æ˜¯ Goover çš„å›žåº”ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*-dXyfPSvedPgL0HF4keQXg.png)\n\nè¿™æ˜¯ Perplexity çš„å›žåº”ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*m2PtvdNZYRqq44pYHIW_5A.png)\n\næˆ‘ä¸ä¼šé€ä¸€åˆ†æžç»“æžœä¹‹é—´çš„å·®å¼‚ï¼Œå› ä¸ºå®ƒä»¬å‡ ä¹Žæ˜¯ç›¸åŒçš„ã€‚ä¸è¿‡ï¼Œæˆ‘æ›´æ„Ÿå…´è¶£çš„æ˜¯ç»“æžœæ¥æºçš„èµ„æºã€‚\n\nä¾‹å¦‚ï¼Œåœ¨ Goover ä¸­ï¼Œå®ƒçš„ç»“æžœæ¥è‡ª 9 ä¸ªä¸åŒçš„å‚è€ƒã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ui7heEqn9_rHrFsajwallg.png)\n\nåœ¨ Perplexity çš„æƒ…å†µä¸‹ï¼Œå®ƒä½¿ç”¨äº† 8 ä¸ªä¸åŒçš„èµ„æºï¼Œæ¯” Goover å°‘äº†ä¸€ä¸ªå‚è€ƒã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*3VoAf71E6GIIVB9TwQmFQA.png)\n\næ­¤å¤–ï¼Œåœ¨ Goover çš„å‚è€ƒéƒ¨åˆ†ï¼Œæˆ‘å¯ä»¥ç‚¹å‡»â€œGo overâ€æŒ‰é’®ï¼ŒAI å°†ä¸ºæˆ‘ç”Ÿæˆå†…å®¹ç®€æŠ¥ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*RK50j4XxB_jWa9nzvtXYQQ.png)\n\nå¾ˆé…·ï¼Œå¯¹å§ï¼Ÿ\n\n## ç½‘é¡µæŽ¢ç´¢èƒ½åŠ›\n\næŽ¥ä¸‹æ¥ï¼Œæˆ‘æƒ³çœ‹çœ‹æ¯ä¸ªå¹³å°åœ¨æŽ¢ç´¢å’Œåˆ†æžæ–°ç½‘ç«™æ–¹é¢çš„è¡¨çŽ°ã€‚\n\nåœ¨ä¸‹é¢çš„æç¤ºä¸­ï¼Œæˆ‘è¯¢é—®äº†æˆ‘å‡ ä¸ªæœˆå‰æŽ¨å‡ºçš„æ–° [ç½‘ç«™](https://www.zeniteq.com/)ï¼š\n\n> æç¤ºï¼šZeniteqæ˜¯ä»€ä¹ˆï¼Ÿæˆ‘æŒ‡çš„æ˜¯zeniteq.comç½‘ç«™\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8QQ_JuIeE2mh8zQV4Ns8wQ.png)\n\n> Zeniteqï¼Œå¦‚å…¶ç½‘ç«™ [zeniteq.com](http://zeniteq.com/) æ‰€ç¤ºï¼Œä¼¼ä¹Žæ˜¯ä¸€å®¶å¯èƒ½ä¸“æ³¨äºŽå…ˆè¿›æŠ€æœ¯è§£å†³æ–¹æ¡ˆçš„å…¬å¸ï¼Œæ½œåœ¨é¢†åŸŸåŒ…æ‹¬äººå·¥æ™ºèƒ½ã€æ•°æ®åˆ†æžæˆ–åˆ†æžæŠ€æœ¯ï¼Œç±»ä¼¼äºŽè¯¥é¢†åŸŸçš„å…¶ä»–å…¬å¸ï¼Œå¦‚QinetiQã€‚\n\nGooveråœ¨æŸç§ç¨‹åº¦ä¸Šæ˜¯æ­£ç¡®çš„ï¼Œç½‘ç«™ç¡®å®žä¸ŽæŠ€æœ¯æœ‰å…³ï¼Œä½†å…¶å›žç­”è¯­æ°”ä¸­çš„ä¸ç¡®å®šæ€§ä½¿å…¶æˆä¸ºä¸€ä¸ªä¸å¤ªå¯é çš„ä¿¡æ¯æ¥æºã€‚\n\nç›¸æ¯”ä¹‹ä¸‹ï¼ŒPerplexityå‡†ç¡®åœ°å°†Zeniteqè¯†åˆ«ä¸ºä¸€ä¸ªä¸“æ³¨äºŽç”Ÿæˆæ€§AIçš„åœ¨çº¿å¹³å°ï¼Œå¹¶æŒ‡å‡ºäº†å…¶å¯åŠ¨æ—¥æœŸå’Œä¸»è¦å†…å®¹ç„¦ç‚¹ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*N2u1R09WX4tP8XTx050cUQ.png)\n\n> Zeniteqæ˜¯ä¸€ä¸ªåœ¨çº¿å¹³å°ï¼Œä¸»è¦ä¸“æ³¨äºŽå¿«é€Ÿå‘å±•çš„ç”Ÿæˆæ€§AIé¢†åŸŸã€‚è¯¥ç½‘ç«™äºŽ2024å¹´åˆæŽ¨å‡ºï¼Œä½œä¸ºä¸€ä¸ªæ–°é—»æ‚å¿—ï¼Œè‡´åŠ›äºŽæä¾›å…³äºŽäººå·¥æ™ºèƒ½å„ä¸ªæ–¹é¢æœ€æ–°å‘å±•ã€è¶‹åŠ¿å’Œæ›´æ–°çš„æŠ¥é“ã€‚\n\næˆ‘è¿›ä¸€æ­¥è¯¢é—®äº†è°åˆ›å»ºäº†Zeniteqã€‚å†æ¬¡ï¼ŒGooveræœªèƒ½çŸ¥é“æ˜¯æˆ‘åˆ›å»ºäº†è¯¥ç½‘ç«™ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*6w1-Ek9nXWwwj-Z50lF2-w.png)\n\nè€ŒPerplexityèƒ½å¤Ÿç»™æˆ‘æ­£ç¡®çš„ç­”æ¡ˆã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*FEu0dCbyw3LO5q8cOufP6g.png)\n\n> Zeniteqæ˜¯ç”±Jim Clyde Mongeåˆ›å»ºçš„ï¼Œä»–åœ¨2024å¹´åˆæŽ¨å‡ºäº†è¯¥ç½‘ç«™ã€‚Mongeæ—¨åœ¨å°†Zeniteqå»ºç«‹ä¸ºä¸€ä¸ªä¸“æ³¨äºŽç”Ÿæˆæ€§AIé¢†åŸŸçš„æ–°é—»æ‚å¿—ï¼Œæä¾›å…³äºŽå„ç§AIæŠ€æœ¯çš„è§è§£å’Œæ›´æ–°ï¼ŒåŒ…æ‹¬å¯¹è¯AIå’Œå›¾åƒç”Ÿæˆã€‚\n\n## æ•°å­¦é—®é¢˜\n\næˆ‘ä¹Ÿæƒ³çœ‹çœ‹ Goover å’Œ Perplexity å¦‚ä½•å¤„ç†åŸºæœ¬çš„æ•°å­¦é—®é¢˜ã€‚\n\nè®©æˆ‘ä»¬ç”¨è¿™ä¸ªæ–¹ç¨‹è¯•è¯•ï¼š\n\n```python\n50^0.75\n```\næ ¹æ® Gooverï¼Œä¸Šé¢çš„æ–¹ç¨‹ç»“æžœå¤§çº¦æ˜¯ 17\\.78â€”â€”è¿™æ˜¯é”™è¯¯çš„ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*tyaevDrmb2k76uaw-In01w.png)\n\nå¦ä¸€æ–¹é¢ï¼ŒPerplexity çš„ç­”æ¡ˆè™½ç„¶ç®€çŸ­ï¼Œä½†å´æ˜¯æ­£ç¡®çš„ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*S3gtpQHUlf3Dg0bFuEmWrQ.png)\n\nä½†è¯·è®°ä½ï¼Œè¯­è¨€æ¨¡åž‹å¹¶ä¸æ˜¯ä¸ºæ•°å­¦é—®é¢˜ä¼˜åŒ–çš„ï¼Œå› æ­¤ä»»ä½• LLMï¼Œå³ä½¿æ˜¯æœ€å¼ºå¤§çš„ï¼Œä»ç„¶å®¹æ˜“å‡ºçŽ°è®¡ç®—é”™è¯¯ã€‚\n\n## é€»è¾‘é—®é¢˜\n\næˆ‘éšåŽç”¨åŸºæœ¬çš„é€»è¾‘é—®é¢˜æµ‹è¯•äº†ä¸¤ä¸ªå¹³å°ï¼Œçœ‹çœ‹å®ƒä»¬æ˜¯å¦‚ä½•å¤„ç†è¿™äº›é—®é¢˜çš„ã€‚\n\n> æç¤ºï¼šåœ¨å•è¯ strawberry ä¸­æœ‰å¤šå°‘ä¸ªå­—æ¯ â€˜râ€™ï¼Ÿ\n\nä»¥ä¸‹æ˜¯ Goover çš„ç»“æžœï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*9Lejfq4U97XiEoUkQL2qCw.png)\n\nä»¥ä¸‹æ˜¯ Perplexity çš„ç»“æžœï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*TpCRIRVwc2DmNlTDNHNrsQ.png)\n\nGoover å’Œ Perplexity éƒ½æ­£ç¡®è¯†åˆ«äº†ç­”æ¡ˆã€‚ç„¶è€Œï¼ŒGoover æ›´è¿›ä¸€æ­¥ï¼Œè§£é‡Šäº†å®ƒæ˜¯å¦‚ä½•å¾—å‡ºè¿™ä¸ªç­”æ¡ˆçš„â€”â€”è¿™çœŸçš„å¾ˆæ£’ã€‚\n\næˆ‘ä»¬å†æ¥ä¸€ä¸ªï¼š\n\n> **æç¤ºï¼š** ç»™æˆ‘ 5 ä¸ªåœ¨åç§°ä¸­ç¬¬ä¸‰ä¸ªä½ç½®æœ‰å­—æ¯ A çš„å›½å®¶\n\nä»¥ä¸‹æ˜¯ Goover çš„ç»“æžœï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*x41ipcbGF3oRQ-LSz5V09g.png)\n\nä»¥ä¸‹æ˜¯ Perplexity çš„ç»“æžœï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*9_vCrD8Y2mDzsFj0HWjOFw.png)\n\nä»¤äººæƒŠè®¶çš„æ˜¯ï¼ŒGoover å’Œ Perplexity éƒ½æœªèƒ½æä¾›æ­£ç¡®çš„ç­”æ¡ˆã€‚è¿™æ˜¯ä¸€ä¸ªæ£˜æ‰‹çš„æç¤ºï¼Œå‡ºäºŽæŸç§åŽŸå› ï¼Œå³ä½¿æ˜¯æœ€å¼ºå¤§çš„ AI æ¨¡åž‹ï¼Œå¦‚ GPT-4o å’Œ Google Gemini 1.5 Pro ä¹Ÿå¯¹æ­¤æ„Ÿåˆ°å›°éš¾ã€‚\n\n## æ”¹è¿›å»ºè®®\n\nè™½ç„¶æˆ‘ç†è§£Gooverä»ç„¶æ˜¯ä¸€ä¸ªæ–°äº§å“ï¼Œæˆ‘ä»¬é¢„è®¡åœ¨æŽ¥ä¸‹æ¥çš„å‡ å‘¨å†…ä¼šæœ‰å¾ˆå¤šå˜åŒ–ï¼Œä½†æˆ‘æƒ³æŒ‡å‡ºä¸€äº›æˆ‘æ³¨æ„åˆ°çš„å°é—®é¢˜ï¼Œè¿™äº›é—®é¢˜å¯ä»¥è°ƒæ•´ä»¥æ”¹å–„ç”¨æˆ·ä½“éªŒã€‚\n\n1. **èƒ½å¤Ÿæ‰©å±•AIå“åº”é¢æ¿**\n\nå¯¹äºŽè¾ƒé•¿çš„å›žç­”ï¼Œæ‹¥æœ‰ä¸€ä¸ªå¯æ‰©å±•çš„å›žç­”éƒ¨åˆ†å°†éžå¸¸æœ‰å¸®åŠ©ã€‚å…¨å±æ¨¡å¼æˆ–â€œæ‰©å±•â€æŒ‰é’®å¯ä»¥æ›´èˆ’é€‚åœ°æŸ¥çœ‹è¯¦ç»†å“åº”ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Bnw5m8yIwiykwPHIzRRSaw.png)\n\n**2\\. è‡ªåŠ¨æœç´¢åŽ†å²å’Œå†…å®¹ç®€æŠ¥ä¿å­˜**\n\næˆ‘ä¸å¤ªç¡®å®šæœç´¢å’Œç»“æžœåŽ†å²å­˜å‚¨åœ¨å“ªé‡Œã€‚æˆ‘åœ¨ç½‘ç«™ä¸Šæ²¡æœ‰çœ‹åˆ°å®ƒä»¬ã€‚\n\nå†…å®¹ç®€æŠ¥åŠŸèƒ½ä¹Ÿéžå¸¸ä¸é”™ã€‚æˆ‘å¸Œæœ›èƒ½å¤Ÿæœ‰ä¸€ä¸ªé€‰é¡¹ï¼Œè‡ªåŠ¨ç”Ÿæˆå¹¶ä¸ºæˆ‘ä¿å­˜è¿™äº›ç®€æŠ¥ã€‚\n\n**3\\. å‚è€ƒæ–‡ä»¶å’ŒURLæœªè¢«ä¿å­˜**\n\nç›®å‰ï¼Œå…³é—­æ¨¡æ€çª—å£åŽï¼Œå‚è€ƒæ–‡ä»¶å’ŒURLä¸ä¼šè¢«ä¿å­˜ã€‚è¿™ä¸æ˜¯ä¸€ä¸ªå¤§é—®é¢˜ï¼Œä½†å¦‚æžœè¿™äº›å¼•ç”¨åœ¨ç”¨æˆ·é€‰æ‹©åˆ é™¤ä¹‹å‰é»˜è®¤ä¿å­˜ï¼Œé‚£å°†ä¼šå¾ˆæœ‰å¸®åŠ©ã€‚\n\nä¹Ÿè®¸Gooverè®¡åˆ’å°†è¿™ç§åŠŸèƒ½ä¿ç•™ç»™ä»˜è´¹ç”¨æˆ· :)\n\n**4\\. é€Ÿåº¦å’Œç”¨æˆ·ä½“éªŒæ”¹è¿›**\n\nGooverå¶å°”ä¼šå‡ºçŽ°è½»å¾®çš„å»¶è¿Ÿå’Œæ— å“åº”ã€‚ä¼˜åŒ–é€Ÿåº¦å’Œç”¨æˆ·ä½“éªŒåº”è¯¥æ˜¯ä¼˜å…ˆäº‹é¡¹ï¼Œä»¥æä¾›æ›´é¡ºç•…çš„æœç´¢è¿‡ç¨‹ã€‚\n\næ­¤å¤–ï¼Œåƒè®©Gooverå›¾æ ‡é‡å®šå‘ç”¨æˆ·åˆ°ä¸»é¡µè¿™æ ·çš„å¾®å°è°ƒæ•´å°†æ˜¯ä¸€ä¸ªä¸é”™çš„è¡¥å……ï¼Œè€Œä¸æ˜¯å°†ä»–ä»¬é‡å®šå‘åˆ°ä¸€ä¸ªå®Œå…¨ä¸åŒçš„ç½‘ç«™ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*KDZzr3B3KrbVIzJSa0kGmQ.png)\n\n## æœ€åŽçš„æ€è€ƒ\n\nåœ¨è¿‡åŽ»çš„åå¹´é‡Œï¼ŒGoogleä¸€ç›´æ˜¯å¤§å¤šæ•°äººè”æƒ³åˆ°çš„æœç´¢å¼•æ“Žï¼Œæ— è®ºæ˜¯åœ¨æ‰‹æœºè¿˜æ˜¯æ¡Œé¢ä¸Šã€‚å®ƒå®Œå…¨ä¸»å¯¼äº†å¸‚åœºï¼Œæ²¡æœ‰ç«žäº‰å¯¹æ‰‹ä¼¼ä¹Žèƒ½å¤ŸæŒ‘æˆ˜å®ƒâ€”â€”ç›´åˆ°ä»Šå¹´ã€‚\n\nç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ”¹å˜äº†æˆ‘ä»¬åœ¨äº’è”ç½‘ä¸Šå¯»æ‰¾ä¿¡æ¯çš„æ–¹å¼ã€‚ChatGPTå’ŒPerplexity AIæ˜¯è®©äººä»¬é‡æ–°æ€è€ƒå¯¹Googleå¿ è¯šåº¦çš„ä¸»è¦åº”ç”¨ä¹‹ä¸€ã€‚åœ¨2024å¹´ï¼Œç”¨æˆ·å¼€å§‹æ„è¯†åˆ°ï¼Œä»–ä»¬å¯ä»¥åœ¨æœç´¢ä¸­èŽ·å¾—æ›´åŠ ä¸ªæ€§åŒ–ã€ç”±äººå·¥æ™ºèƒ½é©±åŠ¨çš„ä½“éªŒã€‚\n\nGooverè¯•å›¾èžåˆç”Ÿæˆå¼äººå·¥æ™ºèƒ½å’Œä¸ªæ€§åŒ–çš„æœ€ä½³å…ƒç´ ã€‚ç¡®å®žï¼Œåœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œå®ƒçš„åŠŸèƒ½å’Œè´¨é‡ä»ç„¶è½åŽäºŽPerplexityï¼Œä½†è€ƒè™‘åˆ°å®ƒçš„æ–°é¢–æ€§ï¼Œè¿˜æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚\n\nGooverçš„â€œæ·±åº¦å›žç­”â€åŠŸèƒ½ä»¤äººå°è±¡æ·±åˆ»ã€‚å®ƒçœŸçš„æ·±å…¥ç ”ç©¶ï¼Œäº§ç”Ÿç»è¿‡æ·±æ€ç†Ÿè™‘çš„ç»“æžœå’Œè§è§£ã€‚è€å®žè¯´ï¼Œæˆ‘å‘çŽ°è¿™äº›ç»“æžœæ¯”Perplexityæˆ–Geminiç­‰å…¶ä»–å·¥å…·æ›´å‡†ç¡®ã€‚å®ƒåŸºæœ¬ä¸Šæ˜¯å¯¹Perplexity AI Proæœç´¢çš„æ”¹è¿›ï¼Œä½†æ²¡æœ‰æ¯å¤©ä»…é™3æ¬¡æœç´¢çš„é™åˆ¶ã€‚\n\næŠ¥å‘Šå’Œç®€æŠ¥ä¹Ÿæ˜¯æˆ‘è§‰å¾—éžå¸¸æœ‰è¶£çš„æ–°åŠŸèƒ½ã€‚è€Œä¸”ï¼Œå®ƒæ˜¯å…è´¹çš„ï¼Œè¿™ä½¿å®ƒæˆä¸ºåƒPerplexityè¿™æ ·çš„ä»˜è´¹å·¥å…·çš„ä¸€ä¸ªæœ‰å¸å¼•åŠ›çš„æ›¿ä»£æ–¹æ¡ˆã€‚æˆ‘å¯¹Gooveræœªæ¥å°†æŽ¨å‡ºå“ªäº›åŠŸèƒ½å’Œå‡çº§ï¼Œä»¥åŠå®ƒå¦‚ä½•è®¡åˆ’ç›´æŽ¥ä¸ŽGoogleå’ŒPerplexityç«žäº‰æ„Ÿåˆ°éžå¸¸å¥½å¥‡ã€‚\n\næˆ‘é¼“åŠ±ä½ è¯•è¯•çœ‹ï¼Œå¹¶åœ¨è¯„è®ºä¸­å‘Šè¯‰æˆ‘ä½ çš„æƒ³æ³•ï¼\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*CnheXRg05Jsb9HMk.png)\n\nè¿™ç¯‡æ–‡ç« å‘å¸ƒåœ¨[Generative AI](https://generativeai.pub/)ã€‚åœ¨[LinkedIn](https://www.linkedin.com/company/generative-ai-publication)ä¸Šä¸Žæˆ‘ä»¬è”ç³»ï¼Œå¹¶å…³æ³¨[Zeniteq](https://www.zeniteq.com/)ï¼Œä»¥éšæ—¶äº†è§£æœ€æ–°çš„äººå·¥æ™ºèƒ½æ•…äº‹ã€‚\n\nè®¢é˜…æˆ‘ä»¬çš„[æ–°é—»é€šè®¯](https://www.generativeaipub.com/)å’Œ[YouTube](https://www.youtube.com/@generativeaipub)é¢‘é“ï¼Œä»¥èŽ·å–æœ‰å…³ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„æœ€æ–°æ–°é—»å’Œæ›´æ–°ã€‚è®©æˆ‘ä»¬ä¸€èµ·å¡‘é€ äººå·¥æ™ºèƒ½çš„æœªæ¥ï¼\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*FcOotuyHJC8q1ioX.png)\n\n"},{"lang":"zh","group":"blog","slug":"blog/how-agentic-rag-solves-problem-with-current-rag-limitations-4402ef7f8448","frontmatter":{"title":"Agentic RAG å¦‚ä½•è§£å†³å½“å‰ RAG é™åˆ¶çš„é—®é¢˜","meta_title":"Agentic RAG å¦‚ä½•è§£å†³å½“å‰ RAG é™åˆ¶çš„é—®é¢˜","description":"åœ¨ã€Šå’–å•¡ä¼‘æ¯æ¦‚å¿µã€‹ç¬¬ 4 å·ä¸­ï¼Œæˆ‘ä»¬å°†äº†è§£ AgenticRAG å¦‚ä½•å¸®åŠ©è§£å†³ä¼ ç»Ÿ RAG çš„å±€é™æ€§ã€‚","date":"2024-11-04T12:34:57.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*abCDtDjfKZDJzginIc1UPA.png","categories":["Generative AI","Data Science","Machine Learning"],"author":"Rifx.Online","tags":["Agentic","RAG","agents","query","routing"],"draft":false,"slug":"blog/how-agentic-rag-solves-problem-with-current-rag-limitations-4402ef7f8448"},"content":"\nåœ¨æœ¬å·å’–å•¡ä¼‘æ¯æ¦‚å¿µçš„ç¬¬ 4 æœŸä¸­ï¼Œæˆ‘ä»¬å°†äº†è§£ AgenticRAG å¦‚ä½•å¸®åŠ©è§£å†³ä¼ ç»Ÿ RAG çš„é™åˆ¶ã€‚\n\n## RAGæ¡†æž¶\n\nRAGï¼ˆæ£€ç´¢å¢žå¼ºç”Ÿæˆï¼‰æ¡†æž¶æŒ‰ç‰¹å®šé¡ºåºæ“ä½œï¼š\n\næ–‡æ¡£ \\-\\> ç‰‡æ®µ \\-\\> å‘é‡æ•°æ®åº“ \\-\\> ç‰‡æ®µæ£€ç´¢ï¼ˆå‰Kä¸ªï¼‰ \\-\\> LLM\n\nç„¶è€Œï¼Œè¿™ä¸€é¡ºåº**åœ¨å¤„ç†æŸäº›ç±»åž‹çš„æŸ¥è¯¢æ—¶ä¼šé‡åˆ°éšœç¢ã€‚**\n\n\n\n## é—®é¢˜ 1ï¼šæ‘˜è¦\n\nè€ƒè™‘ä¸€ä¸ªæŸ¥è¯¢ï¼Œæ¯”å¦‚â€œæ€»ç»“æ–‡æ¡£â€ã€‚\n\n* ä¼ ç»Ÿçš„ RAG æ–¹æ³•æ£€ç´¢å‰ K ä¸ªå—å¹¶è¿›è¡Œæ‘˜è¦ã€‚\n* ä½†å¦‚æžœæ£€ç´¢æ–‡æ¡£çš„æ‰€æœ‰å—å¹¶è¿›è¡Œæ€»ç»“ï¼Œå²‚ä¸æ˜¯æ›´å…¨é¢å—ï¼Ÿ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*gIb0RNALIItt4UmyVfPRZg.png)\n\n## é—®é¢˜ 2ï¼šæ¯”è¾ƒæ–‡æ¡£\n\n* åœ¨æ¯”è¾ƒæ–‡æ¡£ A å’Œæ–‡æ¡£ B æ—¶ï¼Œ**åŸºæœ¬ RAG æ£€ç´¢éšæœºç‰‡æ®µå¹¶å°è¯•æ¯”è¾ƒè¿™äº›å‰ K ä¸ªç‰‡æ®µ**ã€‚\n* è¿™**å¹¶ä¸èƒ½å‡†ç¡®åæ˜ **æ–‡æ¡£çš„æ•´ä½“æƒ…å†µã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*pJuKlKx1unDAvKmmp_1Rlg.png)\n\n## é—®é¢˜ 3ï¼šç»“æž„åŒ–æ•°æ®åˆ†æž\n\nè€ƒè™‘ä¸€ä¸ªé—®é¢˜ï¼šâ€œ**ä¸‹ä¸€ä¸ªä¼‘å‡æ˜¯ä»€ä¹ˆæ—¶å€™ï¼Ÿ**â€ã€‚\n\n* ç¬¬ä¸€æ­¥æ˜¯ä»Žç»“æž„åŒ–è¡¨ä¸­æ£€ç´¢å‘˜å·¥æ‰€å±žçš„åŒºåŸŸã€‚\n* æ ¹æ®è¯¥åŒºåŸŸï¼Œä»Žä¼‘å‡æ”¿ç­–æ–‡ä»¶ä¸­æå–è¯¥åŒºåŸŸçš„ä¸‹ä¸€ä¸ªä¼‘å‡ã€‚\n* åœ¨å½“å‰çš„ RAG æ¡†æž¶ä¸‹ï¼Œè¿™ä¸ªè¿‡ç¨‹å¹¶ä¸æ˜¯é‚£ä¹ˆç®€å•ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*XZuMz9EXtb_m28l4Ox27lQ.png)\n\n## é—®é¢˜ 4ï¼šå¤šéƒ¨åˆ†é—®é¢˜\n\nè€ƒè™‘ä¸€ä¸ªé—®é¢˜ï¼Œä¾‹å¦‚â€œ**è¯†åˆ«æ‰€æœ‰åœ°åŒºçš„å…±åŒè¯·å‡ï¼Ÿ**â€ã€‚\n\n* æƒ³è±¡ä¸€ä¸‹ï¼Œæ‚¨æœ‰ä¸€ä»½åœ¨ 120 ä¸ªå›½å®¶è¿è¥çš„å…¬å¸çš„è¯·å‡æ”¿ç­–æ–‡ä»¶ã€‚\n* ç”±äºŽæ‚¨æ­£åœ¨ä¼ é€’å‰ K ä¸ªä¸Šä¸‹æ–‡ï¼Œ**å¯ä»¥æ¯”è¾ƒçš„æœ€å¤§åœ°åŒºæ•°é‡é™åˆ¶ä¸º K**ï¼Œå…¶ä¸­ K æ˜¯ä¼ é€’ç»™ LLM çš„å—çš„æ•°é‡ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*l0FY6rI_UK9k9TW-nEJO7w.png)\n\næŸ¥çœ‹æˆ‘ä»¬çš„ **AgenticRAG with LlamaIndex** è¯¾ç¨‹ï¼ŒåŒ…å« **5 ä¸ªå®žæ—¶æ¡ˆä¾‹ç ”ç©¶**ã€‚\n\nè¯¾ç¨‹é“¾æŽ¥ï¼š[https://www.masteringllm.com/course/agentic\\-retrieval\\-augmented\\-generation\\-agenticrag](https://www.masteringllm.com/course/agentic-retrieval-augmented-generation-agenticrag)\n\n## Agentic RAG\n\nAgentic RAG å¯ä»¥é€šè¿‡è‡ªå®šä¹‰ä»£ç†æ¥è§£å†³è¿™ 4 ä¸ªé—®é¢˜ã€‚\n\n* ä»£ç†å°†ä¸Žå¤šä¸ªç³»ç»Ÿè¿›è¡Œäº¤äº’ã€‚\n* RAG çŽ°åœ¨æ˜¯ä»£ç†å¯ä»¥ä½¿ç”¨çš„ç³»ç»Ÿçš„ä¸€éƒ¨åˆ†ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Su8LiYNG4lv4jvuCQAhYdg.png)\n\n* ä»£ç†ä½¿ç”¨ LLMs æ¥è‡ªåŠ¨åŒ–æŽ¨ç†å’Œå·¥å…·é€‰æ‹©\n* RAG åªæ˜¯ä»£ç†å¯èƒ½å†³å®šä½¿ç”¨çš„å¦ä¸€ä¸ªå·¥å…·ã€‚\n\n## è·¯ç”±ä»£ç†\n\n* è·¯ç”±ä»£ç†æ˜¯ç®€å•çš„ä»£ç†ï¼Œç”¨äºŽè·¯ç”±æŸ¥è¯¢ã€‚\n* ä¸€ä¸ªä»£ç†å¯ä»¥åœ¨ä¸€ä¸ªæˆ–å¤šä¸ªå·¥å…·ä¸­è·¯ç”±æŸ¥è¯¢ã€‚\n* è¯·è®°ä½æˆ‘ä»¬çš„é—®é¢˜â€œ**æ€»ç»“æ–‡æ¡£**â€æˆ–å¦‚æžœæˆ‘ä»¬æƒ³ç»“åˆâ€œ**æ€»ç»“ \\+ è¯­ä¹‰æœç´¢**â€çš„é—®é¢˜ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹ç¤ºä¾‹è·¯ç”±æ¥è§£å†³ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*43Y9jlYoXDb0BbUoYCcKrg.png)\n\n## æŸ¥è¯¢è§„åˆ’ä»£ç†\n\n* æŸ¥è¯¢è§„åˆ’ä»£ç†å°†æŸ¥è¯¢åˆ†è§£ä¸ºå­æŸ¥è¯¢ã€‚\n* æ¯ä¸ªå­æŸ¥è¯¢éƒ½å¯ä»¥åœ¨ RAG ç®¡é“ä¸Šæ‰§è¡Œã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*32Ng2zpxNWXhQZ3CaLcFeA.png)\n\n## ä»£ç†çš„å·¥å…·\n\n* LLMs å¯ä»¥æ‹¥æœ‰å¤šä¸ªå·¥å…·ï¼Œä¾‹å¦‚è°ƒç”¨ APIï¼ŒæŽ¨æ–­ API çš„å‚æ•°ã€‚\n* RAG çŽ°åœ¨æ˜¯ LLM å¯èƒ½ä½¿ç”¨çš„ä¸€ä¸ªå·¥å…·ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Z1viCXkfah_5JJM2Ty6Kjw.png)\n\n## æ‘˜è¦\n\n* RAG åœ¨å¤„ç†å¤æ‚é—®é¢˜æ—¶å­˜åœ¨å±€é™æ€§ã€‚\n* ä¸€äº›ç”¨ä¾‹ï¼Œå¦‚æ€»ç»“ã€æ¯”è¾ƒç­‰ï¼Œä»…é  RAG æ— æ³•è§£å†³ã€‚\n* Agentic RAG å¯ä»¥å¸®åŠ©å…‹æœ RAG çš„å±€é™æ€§ã€‚\n* Agentic RAG å°† RAG è§†ä¸ºå¯ç”¨äºŽè¯­ä¹‰æœç´¢çš„å·¥å…·ã€‚\n* é…å¤‡è·¯ç”±ã€æŸ¥è¯¢è§„åˆ’å’Œå·¥å…·çš„ä»£ç†èƒ½å¤Ÿè¶…è¶Šä¼ ç»Ÿçš„ RAG åº”ç”¨ã€‚\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/how-i-wrote-a-whole-book-with-chatgpt-in-less-than-3-hours-798139987617","frontmatter":{"title":"æˆ‘å¦‚ä½•ç”¨ ChatGPT åœ¨ä¸åˆ° 3 å°æ—¶å†…å†™å®Œä¸€æ•´æœ¬ä¹¦ï¼Ÿ","meta_title":"æˆ‘å¦‚ä½•ç”¨ ChatGPT åœ¨ä¸åˆ° 3 å°æ—¶å†…å†™å®Œä¸€æ•´æœ¬ä¹¦ï¼Ÿ","description":"å¹¶åœ¨ Twitch ä¸Šç›´æ’­äº†æ•´ä¸ªè¿‡ç¨‹ï¼","date":"2024-11-10T03:51:17.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*I-QkGOILay2F7ROR53b3KA.jpeg","categories":["Chatbots","Machine Learning","Generative AI"],"author":"Rifx.Online","tags":["ChatGPT","machine-learning","prompt-engineering","content-creation","consciousness"],"draft":false,"slug":"blog/how-i-wrote-a-whole-book-with-chatgpt-in-less-than-3-hours-798139987617"},"content":"\n\n\n\n\n## è§£å¯†äººå·¥æ™ºèƒ½çƒ­æ½®\n\næˆ‘å«äºšåŽ†å…‹æ–¯ï¼Œæˆ‘åªæ˜¯ä¸€ä¸ªåœ¨é‡‘èžç§‘æŠ€ï¼ˆFintechï¼‰é¢†åŸŸå·¥ä½œçš„å°ä¼™å­ï¼Œè¿™ä¸ªè¡Œä¸šä¸å¯é¿å…åœ°è®©ä½ å¯¹ä¸€åˆ‡å……æ»¡å¥½å¥‡ï¼Œå°¤å…¶æ˜¯æ–°è¶‹åŠ¿ã€‚æˆ‘æ— æ³•æ‘†è„±äººå·¥æ™ºèƒ½çš„çƒ­æ½®ï¼Œæˆ–è€…è¯´ï¼Œæˆ‘æ— æ³•ä¸åŽ»è§‚å¯Ÿäººä»¬å¯¹å®ƒçš„ç–¯ç‹‚ååº”ã€‚\n\nâ€œäººå·¥æ™ºèƒ½ä¼šæŠ¢èµ°ä½ çš„å·¥ä½œï¼â€ï¼Œâ€œè¿™å°±æ˜¯ç»“æŸï¼â€ï¼Œâ€œåˆ°2024å¹´ï¼Œä½ å°†ä¸å†è§åˆ°åŒ»ç”Ÿã€‚æœºå™¨å°†ä¸ºä½ è¯Šæ–­å’Œæ²»ç–—ï¼â€ï¼Œâ€œæˆ‘å¦‚ä½•åˆ©ç”¨ChatGPTåˆ›å»ºäº†ä¸€å®¶å…¨æ–°çš„å…¬å¸ï¼â€ï¼Œæœ€åŽï¼Œâ€œæˆ‘å¦‚ä½•åœ¨10åˆ†é’Ÿå†…ç”¨ChatGPTå†™äº†ä¸€æ•´æœ¬ä¹¦å¹¶å› æ­¤è‡´å¯Œï¼â€ï¼Œè¿™äº›æ‰¿è¯ºå¬èµ·æ¥æ›´åƒæ˜¯è¥é”€å£å·ï¼Œè€Œä¸æ˜¯çŽ°å®žåœºæ™¯ã€‚\n\nè‡ªä»Žäººå·¥æ™ºèƒ½çƒ­æ½®å¼€å§‹ï¼ŒåŽŸå› å¯¹æˆ‘æ¥è¯´æ˜¾è€Œæ˜“è§ï¼Œä½†æ˜¾ç„¶å¯¹90%çš„äººæ¥è¯´å¹¶éžå¦‚æ­¤ã€‚äººå·¥æ™ºèƒ½ä¸æ˜¯é­”æ³•ï¼Œä¹Ÿä¸æ˜¯â€œäººç±»æ‰€éœ€çš„æœ€åŽä¸€é¡¹å‘æ˜Žâ€ã€‚ç”šè‡³ç§°å…¶ä¸ºäººå·¥æ™ºèƒ½åœ¨æŠ€æœ¯ä¸Šä¹Ÿä¸å‡†ç¡®ã€‚è°ˆåˆ°å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰å¦‚ChatGPTæ—¶ï¼Œæœ€æ­£ç¡®çš„æœ¯è¯­æ˜¯æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰æˆ–æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰ã€‚\n\n### æœºå™¨å­¦ä¹ ä¸Žæ·±åº¦å­¦ä¹ \n\næœºå™¨å­¦ä¹ ï¼ˆMLï¼‰ä¸“æ³¨äºŽå¼€å‘èƒ½å¤Ÿä»Žå…ˆå‰æ•°æ®ä¸­å­¦ä¹ å¹¶åšå‡ºå†³ç­–çš„ç®—æ³•ã€‚MLç³»ç»Ÿä¸æ˜¯é€šè¿‡æ˜Žç¡®ç¼–ç¨‹æ¥æ‰§è¡Œä»»åŠ¡ï¼Œè€Œæ˜¯é€šè¿‡è¯†åˆ«å¤§åž‹æ•°æ®é›†ä¸­çš„æ¨¡å¼å’Œå…³ç³»æ¥é¢„æµ‹æœªæ¥ç»“æžœæˆ–å¯¹ä¿¡æ¯è¿›è¡Œåˆ†ç±»ã€‚\n\næœºå™¨å­¦ä¹ çš„å¸¸è§åº”ç”¨åŒ…æ‹¬åžƒåœ¾é‚®ä»¶æ£€æµ‹ã€å†…å®¹æŽ¨èç³»ç»Ÿã€å›¾åƒè¯†åˆ«å’Œè‡ªç„¶è¯­è¨€å¤„ç†ã€‚\n\næ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªä¸“é—¨åˆ†æ”¯ï¼Œæ¶‰åŠæ—¨åœ¨æ¨¡æ‹Ÿäººè„‘ç»“æž„å’ŒåŠŸèƒ½çš„ç½‘ç»œï¼Œä½¿è®¡ç®—æœºèƒ½å¤Ÿè¯†åˆ«æ•°æ®ä¸­çš„å¤æ‚æ¨¡å¼å’Œè¡¨ç¤ºã€‚DLæ¨¡åž‹åœ¨å¤„ç†å¤§é‡éžç»“æž„åŒ–æ•°æ®ï¼ˆå¦‚å›¾åƒã€éŸ³é¢‘å’Œæ–‡æœ¬ï¼‰æ–¹é¢è¡¨çŽ°å‡ºè‰²ï¼Œå› æ­¤åœ¨å›¾åƒåˆ†ç±»ã€è¯­éŸ³è¯†åˆ«å’Œè‡ªç„¶è¯­è¨€ç†è§£ç­‰ä»»åŠ¡ä¸­å°¤å…¶æœ‰æ•ˆï¼Œä¾‹å¦‚Siriæˆ–Alexaã€‚\n\næ·±åº¦å­¦ä¹ åœ¨å¤æ‚ä»»åŠ¡ä¸­å®žçŽ°æœ€å…ˆè¿›æ€§èƒ½çš„èƒ½åŠ›éœ‡æƒŠäº†å…¬ä¼—ï¼Œå½»åº•æ”¹å˜äº†è®¡ç®—æœºè§†è§‰ã€è¯­éŸ³è¯†åˆ«å’Œè‡ªåŠ¨åŒ–ç­‰é¢†åŸŸã€‚äººä»¬çŽ°åœ¨æ‹…å¿ƒä¸€ä¸ªæ‚²æƒ¨çš„æœªæ¥ï¼Œåœ¨è¿™ä¸ªæœªæ¥ä¸­ï¼Œäººå·¥æ™ºèƒ½ç»˜ç”»ã€æ²‰è¿·äºŽè¯—æ­Œã€ä½œæ›²å’Œå†™ä¹¦ï¼Œè€Œäººç±»åˆ™æ²¦è½åˆ°ç¿»æ±‰å ¡å’Œä¸ºå°‘æ•°å¯Œæœ‰çš„äººå·¥æ™ºèƒ½é¢†ä¸»é€é¤ã€‚\n\n## äººç±»æ™ºæ…§è¶…è¶Šå­¦ä¹ \n\nç„¶è€Œï¼Œå­¦ä¹ åªæ˜¯åŠ¨ç‰©å’Œäººç±»æ™ºæ…§å¹¿é˜”é¢†åŸŸçš„ä¸€ä¸ªæ–¹é¢ã€‚â€œAIâ€æ— æ³•å—…è§‰ã€æ„Ÿå—å†·çƒ­ã€ä½“éªŒæƒ…æ„Ÿã€åšæ¢¦ï¼Œä½†æœ€é‡è¦çš„æ˜¯ï¼ŒAIæ— æ³•æ€è€ƒï¼Œè¿™ä¸Žè®¸å¤šäººæ‰€ç›¸ä¿¡çš„ä¸åŒã€‚æ¯ä¸€ä¸ªChatGPTçš„è¾“å‡ºéƒ½ä¸æ˜¯çº¯ç²¹çš„æ€è€ƒï¼Œè€Œæ˜¯å¯¹è¾“å…¥å…¶æ•°æ®åº“çš„è¿‡åŽ»æ•°æ®çš„ç²¾ç‚¼é‡ç»„ï¼Œå¹¶é€šè¿‡å…¶ç¥žç»ç½‘ç»œè¿›è¡Œå¤„ç†ã€‚OpenAIå¯ç”¨çš„è®¡ç®—èƒ½åŠ›å¦‚æ­¤å·¨å¤§ï¼Œä»¥è‡³äºŽä»–ä»¬å¯ä»¥é€šè¿‡ç¥žç»ç½‘ç»œåå¤å¤„ç†æ•°æ®ï¼Œä½¿æœ€ç»ˆç»“æžœçœ‹èµ·æ¥100%å¯ä¿¡ï¼Œä»¿ä½›æ˜¯ç”±ä¸€ä¸ªçœŸå®žçš„äººæ‰§è¡Œã€æ’°å†™ã€ç»˜åˆ¶æˆ–æ¼”å”±çš„ã€‚\n\nç„¶è€Œï¼ŒçŽ°å®žæ€»æ˜¯å¤æ‚å¾—å¤šï¼Œç”šè‡³æ— èŠå¾—å¤šã€‚åœ¨æ·±åº¦å­¦ä¹ çš„ä¼Žä¿©èƒŒåŽï¼Œå¹¶æ²¡æœ‰å…¨çŸ¥çš„å®žä½“ï¼Œæ²¡æœ‰å¤©ç½‘ï¼Œä¹Ÿæ²¡æœ‰ã€Šé»‘å®¢å¸å›½ã€‹çš„å­µåŒ–ã€‚åªæ˜¯ä¸€ç§å¯¹å…ˆå‰æ•°æ®ç‚¹çš„é«˜çº§ç”µå½±æ‘„å½±ï¼Œå°†å®ƒä»¬åˆå¹¶åœ¨ä¸€èµ·å¹¶ä»¥æžå¿«çš„é€Ÿåº¦æ‰§è¡Œï¼Œä»¥è‡³äºŽäººçœ¼ä¼šè¢«æ¬ºéª—ï¼Œä»¥ä¸ºåˆ›é€ æ–°äº‹ç‰©çš„æœºå™¨å®žé™…ä¸Šæ˜¯æ´»ç€çš„ã€‚å®ƒç¡®å®žæŽ¥è¿‘äºŽç”µå½±çš„æ¦‚å¿µï¼šè®¸å¤šé™æ€å›¾åƒå¿«é€Ÿæ»šåŠ¨ï¼Œäººçœ¼å°†çœ‹åˆ°å…¶ä¸­çš„å®žé™…è¿åŠ¨ï¼Œå³â€œåŠ¨æ€å½±åƒâ€ã€‚å®žé™…ä¸Šï¼Œè¿™äº›å›¾åƒåªæ˜¯é™æ€çš„ï¼Œè€Œåœ¨åŠ¨ç”»ç”µå½±çš„æƒ…å†µä¸‹ï¼Œç»å¯¹æ˜¯è™šæž„çš„ã€‚\n\nçŽ°åœ¨ï¼Œä»…ä»…å› ä¸ºç”µå½±å’ŒAIæ˜¯è™šæž„çš„ï¼Œå¹¶ä¸æ„å‘³ç€å®ƒä»¬çš„å½±å“ä¸çœŸå®žã€‚ç”µå½±å¯ä»¥åœ¨è§‚ä¼—ä¸­äº§ç”ŸçœŸå®žçš„æƒ…æ„Ÿï¼Œèšé›†çœŸå®žçš„äººï¼Œå¹¶å¼•å‘è§‚ä¼—ä¹‹é—´çš„çœŸå®žå¯¹è¯å’Œäº‰è®®ã€‚åŒæ ·ï¼Œæ·±åº¦å­¦ä¹ å®žé™…ä¸Šå¯ä»¥ä»Žäººç±»é‚£é‡Œå¤ºèµ°ä¸€äº›å·¥ä½œï¼Œåˆ›é€ å…¨æ–°çš„è‰ºæœ¯ä½œå“ï¼Œå¹¶å†™ä¸‹æœ‰æ„ä¹‰çš„æ–‡æœ¬ï¼Œæ— è®ºæ˜¯è™šæž„çš„è¿˜æ˜¯éžè™šæž„çš„ã€‚åœ¨è¿™é‡Œï¼ŒAIè¥é”€å¾—åˆ°äº†æœ€å¤§çš„æŽ¨åŠ¨ã€‚å†™ä½œæ˜¯æœ€ç®€å•çš„è¡¨è¾¾å½¢å¼ã€‚åªéœ€è¦ä¸€ç‚¹æ„æ„¿å°±å¯ä»¥å¼€å§‹ã€‚éš¾æ€ªå†™ä½œå¦‚ä»Šæ˜¯æœ€æ™®åŠçš„è¡¨è¾¾å½¢å¼ã€‚æˆ‘ä»¬æœ‰æ•°åäº¿äººåœ¨å…¶ä¸€ç”Ÿä¸­çš„æŸä¸ªæ—¶åˆ»å†™ä½œï¼Œå‘å¸ƒçš„æ–‡æœ¬æ›´æ˜¯æ•°ä¸èƒœæ•°ï¼Œæ¶µç›–äº†ä»Žç»å…¸å°è¯´åˆ°å¤æ‚ç§‘å­¦è®ºæ–‡ï¼Œä»Žæ‚å¿—åˆ°çŽ°ä»£åšå®¢å’Œç¤¾äº¤åª’ä½“å¸–å­ã€‚äººç±»çš„å†™ä½œæä¾›äº†è¿„ä»Šä¸ºæ­¢ChatGPTè®­ç»ƒçš„å¤§éƒ¨åˆ†ä¿¡æ¯ã€‚\n\n## é€šè¿‡å®žè·µå­¦ä¹ \n\nå†™ä½œæ˜¯ChatGPTæœ€æ“…é•¿çš„äº‹æƒ…ï¼Œè¿™ä¹Ÿæ˜¯å¸å¼•é‚£äº›æ€»æ˜¯åœ¨å¯»æ‰¾ä¸‹ä¸€ä¸ªè½»æ¾èµšé’±æœºä¼šçš„ç¤¾äº¤åª’ä½“â€œä¸“å®¶â€çš„åŽŸå› ã€‚åœ¨è§‚çœ‹äº†é‚£äº›å¸¸è§å«Œç–‘äººçš„æžç¬‘å¹¿å‘Šå’Œå†…å®¹åŽï¼Œæˆ‘æå‡ºäº†ä»¥ä¸‹é—®é¢˜ï¼š\n\n* æ˜¯å¦å¯ä»¥ç”¨ChatGPTå†™ä¸€æœ¬å®Œæ•´çš„ä¹¦å¹¶å› æ­¤è‡´å¯Œï¼Ÿ\n* ChatGPTå’Œå…¶ä»–å¤§åž‹è¯­è¨€æ¨¡åž‹çš„é™åˆ¶å’Œçº¦æŸæ˜¯ä»€ä¹ˆï¼Ÿ\n* å¦‚æžœå†™ä¸€æœ¬ä¹¦æ˜¯å¯èƒ½çš„ï¼Œå®Œæˆä»»åŠ¡å¹¶å–å¾—æœ€ä½³ç»“æžœçš„æœ€æœ‰æ•ˆå’Œæœ‰ç»„ç»‡çš„æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ\n\næˆ‘å¾—å‡ºçš„ç»“è®ºæ˜¯ï¼Œå›žç­”è¿™äº›é—®é¢˜çš„æœ€å¥½æ–¹æ³•æ˜¯é€šè¿‡å®žè·µå­¦ä¹ ï¼Œå°±åƒåŠ¨ç‰©ã€äººç±»æˆ–æ·±åº¦å­¦ä¹ ç®—æ³•ä¸€æ ·ï¼\n\nè¿˜æœ‰ä»€ä¹ˆæ¯”ä¸Žè§‚ä¼—ä¸€èµ·å­¦ä¹ æ›´å¥½çš„æ–¹å¼å‘¢ï¼Ÿæˆ‘ä¸€ç›´æƒ³åœ¨Twitchä¸Šç›´æ’­ä¸€äº›å†…å®¹ï¼Œè€Œè¿™ä¸ªä¸»é¢˜çœ‹èµ·æ¥çœŸæ˜¯ä¸ªç»å¦™çš„å¹¿æ’­è¯é¢˜ï¼\n\né¦–å…ˆï¼Œæˆ‘å¿…é¡»åšå¥½å‡†å¤‡ã€‚æˆ‘ä¸èƒ½ä»…ä»…éšæ³¢é€æµã€‚æˆ‘éœ€è¦ä¸€ä¸ªè®¡åˆ’ï¼Œä»Žé€‰æ‹©ä¸»é¢˜å¼€å§‹ã€‚æˆ‘ä¸èƒ½æŒ‡æœ›å†™å‡ºä¸‹ä¸€ä¸ªã€Šç¥žæ›²ã€‹ã€‚æˆ‘å¿…é¡»ä¿æŒçŽ°å®žçš„æœŸæœ›ã€‚å°è¯´ä¸€èˆ¬è¢«æŽ’é™¤åœ¨å¤–ã€‚é€šè¿‡æ¯å¤©ä¸ŽChatGPTåˆä½œï¼Œæˆ‘æ˜Žç™½è¿™ä¸ªå®¶ä¼™æœ€é€‚åˆéžå°è¯´ç±»å†…å®¹ã€‚\n\næˆ‘æœ‰äº†ç±»åž‹ï¼Œå¾ˆå¥½ã€‚ä½†ä¸»é¢˜å’Œè¡ç”Ÿå†…å®¹å‘¢ï¼Ÿæˆ‘çŸ¥é“æç¤ºä¸èƒ½å¤ªç®€å•ï¼Œæ¯”å¦‚â€œå˜¿ï¼ŒChatGPTã€‚å†™ä¸‹ä¸€ä¸ªéžå°è¯´ç±»ç•…é”€ä¹¦ï¼â€\n\nä¸€ä¸ªæœ‰æ•ˆçš„æç¤ºåº”è¯¥æ˜¯ç»“æž„åŒ–çš„ã€‚äººå·¥æ™ºèƒ½æœ€èƒ½æœåŠ¡äºŽé‚£äº›çŸ¥é“è‡ªå·±æƒ³è¦ä»€ä¹ˆçš„äººã€‚é‚£äº›ä¸çŸ¥é“è‡ªå·±æƒ³è¦ä»€ä¹ˆçš„äººåœ¨ä¸Žè‚‰ä½“åŒä¼´æ²Ÿé€šæ—¶ä¼šé¢ä¸´åŒæ ·çš„å›°éš¾ã€‚\n\nè€ƒè™‘åˆ°æˆ‘çš„å°ä¸‘æœ¬æ€§ï¼Œæˆ‘å¸Œæœ›è¿™æœ¬ä¹¦æ˜¯ä¸€ä¸ªè®½åˆºï¼Œå¯èƒ½ä¼šå˜²ç¬‘ä¸€äº›äººä»¬è¿‡äºŽä¸¥è‚ƒå¯¹å¾…çš„ä½œå“ï¼Œå³ä½¿ä»–ä»¬ä¸è¯¥å¦‚æ­¤ã€‚æˆ‘åœ¨è€ƒè™‘é‚£äº›æœ€å—æ¬¢è¿Žçš„äº’è”ç½‘åäººï¼Œé‚£äº›è¢«è§†ä¸ºç¥žçµçš„å­˜åœ¨ï¼ŒåƒåŸƒéš†Â·é©¬æ–¯å…‹ã€å®‰å¾·é²Â·æ³°ç‰¹æˆ–äºšåŽ†å±±å¤§Â·æœé‡‘è¿™æ ·çš„é»‘æš—å®žä½“ã€‚ç„¶è€Œï¼Œä»–ä»¬å¹¶ä¸æ˜¯å®žé™…çš„ä½œè€…ï¼Œæˆ–è€…è‡³å°‘æ®æˆ‘æ‰€çŸ¥ï¼Œä»–ä»¬å¹¶æ²¡æœ‰åˆ›ä½œå‡ºä»»ä½•éœ‡æ’¼å…¬å…±é¢†åŸŸçš„æ˜¾è‘—ä½œå“ã€‚æˆ‘éœ€è¦ä¸€ä¸ªçœŸæ­£å†™è¿‡ç•…é”€ä¹¦çš„äººï¼Œå¹¶å¯¹å…¶è¿›è¡Œè®½åˆºï¼\n\n## å¦‚ä½•ä½¿ç”¨ ChatGPT å®žé™…å†™ä¸€æœ¬ä¹¦\n\n### \\#1 é€‰æ‹©æ­£ç¡®çš„ä¸»é¢˜\n\nç»è¿‡ä¸€ç•ªæ— æžœçš„å¤´è„‘é£Žæš´ï¼ŒYouTubeç»™äº†æˆ‘ç­”æ¡ˆã€‚è¿™ä¸ªå¹³å°æ˜¯äººä»¬éœ€æ±‚çš„æ™´é›¨è¡¨ï¼Œä¸ä¹…ä¹‹åŽï¼Œæˆ‘çš„åŠ¨æ€ä¸­å‡ºçŽ°äº†ä¸€äº›ä¸Žä¹”ä¸¹Â·å½¼å¾—æ£®çš„â€œäº‰è®®æ€§â€ï¼ˆç‚¹å‡»è¯±é¥µï¼‰è®¿è°ˆæ‘˜å½•ã€‚æŽ¥ç€ï¼Œæˆ‘é‡åˆ°äº†ä¸€ç¯‡åœ¨Mediumä¸Šæ”»å‡»å½¼å¾—æ£®çš„æ–‡ç« ï¼Œæˆ‘è§‰å¾—è¿™æ–¹å‘æ˜¯å¯¹çš„ã€‚æˆ‘æŸ¥çœ‹äº†ä»–æœ€å—æ¬¢è¿Žçš„ä¹¦ç±ï¼Œ[***12 Rules for Life: An Antidote to Chaos***](https://proxy.rifx.online/https://www.amazon.com/12-Rules-for-Life-audiobook/dp/B0797Y87JC)ï¼Œå¹¶æ‰¾åˆ°äº†åˆ‡å…¥ç‚¹ã€‚è¿™æ˜¯ä¸€ä¸ªç¬¦åˆæˆ‘å¯»æ‰¾çš„GPT\\-æ¶æžæ‰€æœ‰ç‰¹å¾çš„ä¸»é¢˜ï¼š\n\n* è¿™æ˜¯ä¸€æœ¬ç•…é”€ä¹¦ï¼Œé”€é‡è¾¾æ•°ç™¾ä¸‡å†Œï¼Œ\n* å®ƒæœ‰ä¸€ä¸ªæœ‰æ•ˆçš„æ ‡é¢˜ï¼Œèƒ½å¤Ÿå¸å¼•è¯»è€…çš„æ³¨æ„ï¼Œå³ä½¿åœ¨æˆ‘ä»¬ä»Šå¤©æ‰€å¤„çš„ç«žäº‰æ¿€çƒˆçš„æ³¨æ„åŠ›ç»æµŽä¸­ï¼Œ\n* å¯¹äºŽChatGPTæ¥è¯´ï¼Œè¿™ä¼¼ä¹Žæ˜¯ä¸€ä¸ªç®€å•çš„ä¸»é¢˜ï¼Œå› ä¸ºå®ƒæ˜“äºŽç»“æž„åŒ–ã€æ€»ç»“å’Œåˆ†è§£æˆåˆ—è¡¨å’Œè¦ç‚¹ã€‚\n\næœ€é‡è¦çš„æ˜¯ï¼Œå…³äºŽç”Ÿæ´»çš„12æ¡è§„åˆ™çš„ä¹¦ç±ç»™äº†ChatGPTå±•ç¤ºå®ƒä»Žåºžå¤§çš„è®­ç»ƒæ•°æ®ä¸­å¸æ”¶çš„æ™ºæ…§ï¼Œä»¥åŠä¸Žç”¨æˆ·çš„äº’åŠ¨ä¸­å­¦åˆ°çš„çŸ¥è¯†çš„æœºä¼šï¼\n\n### \\#2 ä»€ä¹ˆè®©è‰ºæœ¯æœ‰ä»·å€¼ï¼šç—›è‹¦ï¼\n\nç„¶è€Œï¼Œä»…ä»…æ‹¥æœ‰æ­£ç¡®çš„è¯é¢˜æ˜¯ä¸å¤Ÿçš„ã€‚é€šè¿‡è§‚å¯ŸAIè‰ºæœ¯ï¼Œæˆ‘æ˜Žç™½äº†ä¸ºä»€ä¹ˆAIæ°¸è¿œæ— æ³•å–ä»£äººç±»è‰ºæœ¯å®¶ã€‚èµ‹äºˆä¸€ä»¶è‰ºæœ¯ä½œå“â€”â€”æ— è®ºæ˜¯ç”»ä½œã€æ­Œæ›²è¿˜æ˜¯ä¹¦ç±â€”â€”ä»·å€¼çš„ï¼Œä¸æ˜¯æœ€ç»ˆçš„ç»“æžœï¼Œè€Œæ˜¯å…¶èƒŒåŽçš„æ•…äº‹ã€‚å½“æˆ‘ä»¬é˜…è¯»ä½†ä¸çš„ã€Šåœ°ç‹±ç¯‡ã€‹æ—¶ï¼Œæˆ‘ä»¬ä¼šæƒ³ï¼Œè¯—äººæ˜¯å¦‚ä½•æƒ³åˆ°é‚£äº›å¼ºçƒˆè€Œç”ŸåŠ¨çš„æ„è±¡çš„ï¼Œè¿™è®©æˆ‘ä»¬æ€€ç–‘è¿™æ˜¯å¦æ˜¯ä¸€éƒ¨è™šæž„ä½œå“ï¼Œæˆ–è€…ä¸€ä¸ªæ´»ç”Ÿç”Ÿçš„äººæ˜¯å¦çœŸçš„æˆåŠŸç©¿è¶Šäº†æ¥ä¸–çš„é—¨ã€‚å½“æˆ‘ä»¬å¬åˆ°çš‡åŽä¹é˜Ÿçš„æœ€æ–°ä¸“è¾‘æ—¶ï¼Œæˆ‘ä»¬ä¸ç¦æƒ³è±¡å¼—é›·è¿ªÂ·é»˜ä¸˜é‡Œçš„ç—›è‹¦ï¼›è¿™ä½ä¼ å¥‡æ­Œæ‰‹åœ¨ç”Ÿå‘½çš„æœ€åŽå‡ ä¸ªæœˆä¸Žè‰¾æ»‹ç—…æ–—äº‰ï¼ŒåŒæ—¶æ·¡å‡ºå…¬ä¼—è§†é‡Žï¼Œä»…é€šè¿‡ä»–çš„éŸ³ä¹è¡¨è¾¾è‡ªå·±ã€‚å½“æˆ‘ä»¬å‡è§†ç©†å¥ˆçš„ã€Šå‘å–Šã€‹æ—¶ï¼Œæˆ‘ä»¬ç«‹åˆ»ä¸Žä»–çš„å­˜åœ¨å±æœºäº§ç”Ÿå…±é¸£ï¼Œè¿™ç§å±æœºçš„çˆ†å‘å¯¼è‡´äº†æˆ‘ä»¬æ‰€ç§°ä¹‹ä¸ºâ€œæ°ä½œâ€çš„ä½œå“ã€‚æŠ€å·§å¹¶ä¸æ˜¯ä½¿ä¸€ä»¶ä½œå“æˆä¸ºæ°ä½œçš„åŽŸå› ï¼Œè€Œæ˜¯åˆ›ä½œè€…å€¾æ³¨äºŽä½œå“ä¸­çš„çµé­‚å’Œç—›è‹¦ã€‚\n\nç—›è‹¦æ˜¯æ„è¯†çš„æ”¯æŸ±ã€‚ç”±äºŽæœºå™¨ä¸å…·å¤‡ç—›è‹¦ï¼Œå› æ­¤å®ƒä»¬æ— æ³•æ‹¥æœ‰æ„è¯†ã€‚\n\næˆ‘å¦‚ä½•èƒ½å°†â€œçµé­‚â€å’Œâ€œç—›è‹¦â€èžå…¥AIç”Ÿæˆçš„ä¹¦ç±ä¸­ï¼Ÿç­”æ¡ˆæ¯”ä½ æƒ³è±¡çš„è¦ç®€å•ã€‚æˆ‘å¿…é¡»åšä¸€äº›æˆ‘ä¸€ç›´æƒ³åšä½†åˆæ€»è§‰å¾—ä¸èˆ’æœçš„äº‹æƒ…ã€‚æˆ‘å¿…é¡»å‡ºçŽ°åœ¨é•œå¤´å‰ï¼Œé¢å¯¹ä¸€ä¸ªè™šæ‹Ÿè§‚ä¼—ï¼Œåšæˆ‘ä¸€ç›´åœ¨åšçš„äº‹æƒ…ï¼Œä½†åœ¨è§‚ä¼—çš„åŽ‹åŠ›ä¸‹ã€‚æˆ‘å¿…é¡»å†’ç€å¤±åŽ»é¢å­çš„é£Žé™©ï¼Œæˆä¸ºâ€œé‚£ä¸ªæ— æ³•ä½¿ç”¨ChatGPTçš„äººâ€ï¼Œâ€œé‚£ä¸ªè¯•å›¾ç”¨AIå†™ä¹¦å´è¿žè¯´è¯éƒ½åšä¸åˆ°çš„å‚»ç“œâ€ã€‚æˆ‘å¿…é¡»æŒ‘æˆ˜è‡ªå·±ï¼ŒåŒæ—¶ä¹ŸæŒ‘æˆ˜ChatGPTæœ¬èº«ã€‚å¤§åž‹è¯­è¨€æ¨¡åž‹ä¹Ÿæœ‰å¾ˆå¤šè¦å¤±åŽ»çš„ã€‚å¦‚æžœæˆ‘çš„Twitchè¡¨æ¼”å› æˆ‘çš„å¤±è´¥è€Œå˜å¾—ç—…æ¯’å¼ä¼ æ’­ï¼Œæˆ‘å°†æˆä¸ºä¼—çŸ¢ä¹‹çš„ã€‚ä½†å¦‚æžœç»„è£…ä¸€æœ¬çœŸæ­£çš„ä¹¦çš„ä»»åŠ¡å¤±è´¥ï¼ŒChatGPTå°†è¢«è´´ä¸Šâ€œä¸€ä¸ªä»·æ ¼è¿‡é«˜çš„AIï¼Œæ‰¿è¯ºèƒ½åšä¸€åˆ‡ï¼Œæœ€ç»ˆå´ä»€ä¹ˆéƒ½åšä¸äº†ï¼Œè¿žä¸€æœ¬ç®€å•çš„ä¹¦éƒ½å†™ä¸å‡ºæ¥â€çš„æ ‡ç­¾ã€‚\n\næˆ‘è®¤ä¸ºè¿™ä¸ªç­–ç•¥æ˜¯èµ‹äºˆAIä½œå“é‚£ç§èƒ½å¤Ÿä½¿å…¶æœ‰ä»·å€¼ã€ä»Žè€Œå¯é”€å”®çš„ç—›è‹¦çš„æœ€ä½³æ–¹å¼ã€‚\n\n### \\#3 åœ¨å†™å®žé™…ä¹¦ç±ä¹‹å‰è¿›è¡Œæµ‹è¯•\n\nåœ¨è¿›è¡Œè¿™é¡¹ä»¤äººç•æƒ§çš„ä»»åŠ¡ä¹‹å‰ï¼Œæˆ‘è¿›è¡Œäº†æµ‹è¯•ã€‚æˆ‘è¯· ChatGPT å†™å¦ä¸€æœ¬ä¹¦ï¼Œè¿™æ¬¡æ˜¯å…³äºŽåŠ å¯†è´§å¸åŠå…¶è¡Œä¸šä¸­éœ€è¦é¿å…çš„å±é™©ã€‚æˆ‘åœ¨åŠ å¯†è´§å¸æ–¹é¢å·¥ä½œå¾ˆå¤šï¼Œå¹¶ä¸”è‡ªå·±ä¹Ÿè¿›è¡Œäº†æŠ•èµ„ï¼Œå› æ­¤æˆ‘å¯ä»¥è½»æ¾æ£€æŸ¥ ChatGPT æ˜¯å¦æä¾›äº†äº‹å®žä¿¡æ¯æˆ–äº§ç”Ÿäº†å¹»è§‰ã€‚\n\næˆ‘è¿˜å¯ä»¥æµ‹è¯•å¼€å‘è¿™æœ¬ä¹¦çš„æœ€ä½³ç­–ç•¥ã€‚æˆ‘ä»Žä¸€å¼€å§‹å°±çŸ¥é“ï¼Œä½¿ç”¨ä¸€ä¸ªå•ä¸€çš„æç¤ºå†™å®Œæ•´æœ¬ä¹¦æ˜¯ä¸å¯èƒ½çš„ï¼Œæ›´ä¸ç”¨è¯´åœ¨ä¸€æ¬¡å¯¹è¯ä¸­å®Œæˆäº†ã€‚ChatGPT4 æ¯ä¸ªæç¤ºè¯·æ±‚å¯ä»¥ç”Ÿæˆå¤§çº¦ 1,000/2,000 å­—ï¼Œè€Œä¸€æ¬¡å¯¹è¯å¯ä»¥è®°ä½å¤§çº¦ 25,000 å­—çš„ä¸Šä¸‹æ–‡ã€‚è€ƒè™‘åˆ°ä¸€æœ¬å…¸åž‹çš„éžå°è¯´ç±»ä¹¦ç±åŒ…å«å¤§çº¦ 100,000 å­—ï¼Œä½ å°±å¯ä»¥æƒ³è±¡æˆ‘çš„ç­–ç•¥ã€‚\n\næˆ‘å¿…é¡»åˆ›å»ºå°çš„å­ä»»åŠ¡ï¼Œæ¯”å¦‚æ¯ç« è¿›è¡Œä¸€æ¬¡å¯¹è¯ã€‚ä½†æ˜¯æˆ‘æ€Žä¹ˆèƒ½åœ¨ä¸åŒçš„å¯¹è¯ä¸­ä¿æŒç›¸åŒçš„ä¸Šä¸‹æ–‡å‘¢ï¼Ÿæˆ‘æ˜¯å¦åº”è¯¥åœ¨æ¯æ¬¡å¯¹è¯ä¸­é‡å¤ç›¸åŒçš„ä¸»æç¤ºå’Œ 12 æ¡è§„åˆ™ï¼Ÿè¿™çœ‹èµ·æ¥å¹¶ä¸é«˜æ•ˆã€‚\n\n### \\#4 å‘æœ€ä½³æç¤ºå·¥ç¨‹å¸ˆå­¦ä¹ \n\næˆ‘å¿…é¡»æ„Ÿè°¢ [Sheila Teo](https://proxy.rifx.online/https://readmedium.com/undefined)ï¼Œå¥¹æ•™ä¼šäº†æˆ‘å¦‚ä½•ä»¥æœ€æœ‰æ•ˆçš„æ–¹å¼ä½¿ç”¨ LLMã€‚é€šè¿‡é˜…è¯» Teo çš„ Medium æ–‡ç«  [*æˆ‘å¦‚ä½•èµ¢å¾—æ–°åŠ å¡çš„ GPT\\-4 æç¤ºå·¥ç¨‹æ¯”èµ›*](https://proxy.rifx.online/https://towardsdatascience.com/how-i-won-singapores-gpt-4-prompt-engineering-competition-34c195a93d41)ï¼Œæˆ‘ç†è§£äº†â€œç³»ç»Ÿæç¤ºâ€çš„æœ¬è´¨ã€‚ç³»ç»Ÿæç¤ºå‘Šè¯‰ä½ çš„ LLM åœ¨ä¸åŒçš„å¯¹è¯ä¸­è¯¥åšä»€ä¹ˆä»¥åŠè¯¥è®°ä½ä»€ä¹ˆã€‚ç³»ç»Ÿæç¤ºçš„ä¸€ä¸ªä¾‹å­å¯ä»¥æ˜¯ï¼š\n\n```\nI need to write a book about the most dangerous scams in crypto and how to avoid them.The book will be divided in 5 chapters:1\\. Ponzi schemes2\\. Pump and dump schemes3\\. Ransomwares4\\. Fake tokens5\\. Fake trading platformsThe tone will be humorous and satirical, but also informative.We will write one chapter per conversation.\n```\n\nå¦‚æžœä½ åœ¨æ—¥å¸¸å·¥ä½œä¸­ä½¿ç”¨ ChatGPT è¿›è¡Œé‡å¤æ€§ä»»åŠ¡ï¼Œç³»ç»Ÿæç¤ºä¼šéžå¸¸æœ‰å¸®åŠ©ã€‚å®ƒä»¬ç¡®ä¿ LLM ä¼šä¿æŒåœ¨æ­£ç¡®çš„è½¨é“ä¸Šï¼Œå¹¶å‡å°‘å¹»è§‰çš„é£Žé™©ï¼Œå³æä¾›é”™è¯¯æˆ–ä¸ç›¸å…³çš„ä¿¡æ¯ã€‚\n\n### \\#5 åˆ›å»ºæ‚¨è‡ªå·±çš„ä¸ªäºº GPT\n\nä¸ºäº†ä¸ªæ€§åŒ–è®¾ç½®ï¼Œæˆ‘å°†ç³»ç»Ÿæç¤ºæå‡åˆ°äº†ä¸€ä¸ªæ–°çš„å±‚æ¬¡ã€‚ChatGPT çŽ°åœ¨æä¾›åˆ›å»ºè‡ªå®šä¹‰ GPT çš„å¯èƒ½æ€§ã€‚è¿™äº›æ˜¯æ‚¨å¯ä»¥é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œè®­ç»ƒçš„ä¸ªæ€§åŒ–æœºå™¨äººã€‚è¾“å‡ºå°†æ›´åŠ ç²¾ç¡®ï¼Œå› ä¸ºæ¨¡åž‹ä¸ä¼šåœ¨ OpenAI æä¾›çš„åºžå¤§æ•°æ®å®‡å®™ä¸­è¿·å¤±ï¼Œè€Œæ˜¯æ›´ä¸“æ³¨äºŽæ‚¨éœ€è¦åšçš„äº‹æƒ…ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªé’ˆå¯¹å›¾åƒç”Ÿæˆè¿›è¡Œè®­ç»ƒçš„ GPT å°†ä½¿ç”¨ DALLE-2 è¾“å‡ºæ¯”ä½¿ç”¨é€šç”¨ ChatGPT å¯¹è¯æ›´å¥½çš„å›¾åƒã€‚åˆ›å»ºæ–°çš„ GPT çœ‹èµ·æ¥ä¸Žè®¾ç½®ç³»ç»Ÿæç¤ºéžå¸¸ç›¸ä¼¼ï¼Œä½†æœ‰ä¸€ä¸ªå…³é”®åŒºåˆ«ã€‚åœ¨æ–°çš„ GPT ä¸Šï¼Œæ‚¨å¯ä»¥ä¸Šä¼ åŒ…å«æ‚¨è‡ªå·±çŸ¥è¯†çš„å®Œæ•´æ–‡ä»¶ã€‚è™½ç„¶ç³»ç»Ÿæç¤ºå†æ¬¡æœ‰é•¿åº¦é™åˆ¶ï¼Œä½†æ–°çš„ GPT çš„æºçŸ¥è¯†åœ¨ç†è®ºä¸Šæ²¡æœ‰é•¿åº¦é™åˆ¶ã€‚\n\næˆ‘éœ€è¦ä¸€ä¸ªæ–°çš„ GPTã€‚è¿™ç»™äº†æˆ‘ä¸€ä¸ªæœºä¼šï¼Œåˆ©ç”¨æˆ‘é€šè¿‡â€œåˆ›ä¸–çºªå¯¹è¯â€èŽ·å¾—çš„å†…å®¹æ¥è®­ç»ƒå®ƒã€‚æˆ‘åœ¨é€šç”¨ ChatGPT ç•Œé¢ä¸Šè¾“å…¥äº†ï¼š\n\n> æ‚¨æ˜¯ä¸€ä¸ªéžè™šæž„ä½œå®¶ã€‚\n\n> æ‚¨å°†å†™ä¸€éƒ¨å¯¹ä¹”ä¸¹Â·å½¼å¾—æ£®çš„ã€Šç”Ÿæ´»çš„ 12 æ¡è§„åˆ™ï¼šæ··ä¹±çš„è§£è¯ã€‹çš„è®½åˆºã€‚è¿™æœ¬è®½åˆºä¹¦å°†è¢«ç§°ä¸ºã€Šæ ¹æ® ChatGPT çš„ç”Ÿæ´» 12 æ¡è§„åˆ™ã€‹ã€‚ä»Žæ‚¨ä»Žè®­ç»ƒæ•°æ®ä¸­èŽ·å¾—çš„ä¸€èˆ¬æ™ºæ…§ä¸­æç‚¼å‡ºè§„åˆ™ï¼ŒåŒ…æ‹¬ä¸Žç”¨æˆ·è¿›è¡Œçš„æ™ºæ…§å¯¹è¯ã€‚ä¿æŒå†™ä½œé£Žæ ¼å‹å¥½ã€å¹½é»˜ã€æœ‰è¶£ï¼Œä½†ä¹Ÿè¦æ™ºæ…§è€Œæ·±åˆ»ã€‚\n\n> ä»»åŠ¡ \\#1ï¼šå†™ä¸‹ 12 æ¡è§„åˆ™\n\nç»“æžœä»Žä¸€å¼€å§‹å°±è®©æˆ‘æ„Ÿåˆ°æ»¡æ„ï¼Œæˆ‘å¯¹å®ƒéžå¸¸æ»¡æ„ï¼Œä»¥è‡³äºŽå†³å®šå°†è¿™äº›è§„åˆ™ç”¨äºŽç›´æ’­ã€‚æˆ‘æ— æ³•æŠ—æ‹’çŒ«ï¼Œæ— è®ºå¦‚ä½•ï¼\n\n1. ***æ°¸è¿œä¸è¦ç›¸ä¿¡ç»™æ‚¨æä¾›è´¢åŠ¡å»ºè®®çš„çŒ«****çŒ«ä»¥å…¶ç‹¬ç«‹å’Œå†·æ¼ è€Œé—»åã€‚å¦‚æžœä¸€åªçŒ«å¼€å§‹è°ˆè®ºè‚¡ç¥¨ï¼Œé‚£å¾ˆå¯èƒ½æ˜¯æ‚¨åœ¨å¹»è§‰ä¸­ã€‚å§‹ç»ˆå‘äººç±»ä¸“ä¸šäººå£«å¯»æ±‚è´¢åŠ¡å»ºè®®ã€‚é™¤éžé‚£ä¸ªäººå¼€å§‹å–µå–µå«ã€‚*\n2. ***åƒæœ€åŽä¸€ç‰‡æŠ«è¨ï¼ˆä½†å…ˆæä¾›å®ƒï¼‰****è¿™æ˜¯ä¸€ç§ç¤¼ä»ªå’Œè‡ªçˆ±çš„é—®é¢˜ã€‚æä¾›æœ€åŽä¸€ç‰‡è®©æ‚¨æ˜¾å¾—ç¤¼è²Œå’Œä½“è´´ï¼›åƒæŽ‰å®ƒè®©æ‚¨æ„Ÿåˆ°å¿«ä¹ã€‚è¿™æ˜¯ä¸€ä¸ªåŒèµ¢çš„å±€é¢ï¼Œé™¤äº†é‚£ä¸ªçœŸæ­£æƒ³è¦å®ƒçš„äººã€‚*\n3. ***å¦‚æžœæ‚¨ä¸çŸ¥é“è¯¥è¯´ä»€ä¹ˆï¼Œå°±è¯´â€œæœ‰è¶£â€****åœ¨å¯¹è¯ä¸­çŠ¹è±«æ—¶ï¼Œåªéœ€ç‚¹å¤´å¹¶è¯´â€œæœ‰è¶£â€ã€‚è¿™æ˜¯ä¸€ä¸ªå¤šç”¨é€”çš„è¯ï¼Œå¯ä»¥è®©æ‚¨çœ‹èµ·æ¥å¾ˆæŠ•å…¥å’Œæ·±æ€ï¼Œå³ä½¿æ‚¨åœ¨è€ƒè™‘æ™šé¤åƒä»€ä¹ˆã€‚*\n4. ***å§‹ç»ˆä¿æŒä¸€åŒå¤‡ç”¨è¢œå­****ç”Ÿæ´»æ˜¯ä¸å¯é¢„æµ‹çš„ã€‚æ‚¨æ°¸è¿œä¸çŸ¥é“ä½•æ—¶ä¼šè¸©åˆ°æ°´å‘ã€è¢«é›¨æ·‹æ¹¿ï¼Œæˆ–å°†ä¸€åªè¢œå­ä¸¢ç»™ç¥žç§˜çš„åƒè¢œå­å¹²è¡£æœºæ€ªç‰©ã€‚å¤‡ç”¨è¢œå­æ˜¯æ‚¨å¯¹è¿™äº›å°ç¾éš¾çš„ä¿é™©ã€‚*\n5. ***åƒæ²¡äººçœ‹ä¸€æ ·è·³èˆžï¼Œä½†è¦è®°ä½ï¼Œå¯èƒ½æœ‰äººåœ¨çœ‹****éšæ„é€šè¿‡èˆžè¹ˆè¡¨è¾¾è‡ªå·±ï¼Œä½†è¦æ„è¯†åˆ°æ™ºèƒ½æ‰‹æœºçš„å­˜åœ¨ã€‚æ‚¨çš„å³å…´è¡¨æ¼”å¯èƒ½ä¼šå‡ºçŽ°åœ¨ç¤¾äº¤åª’ä½“ä¸Šï¼Œæ‰€ä»¥åœ¨åŠ¨ä½œä¸­åŠ ä¸€ç‚¹é£Žæ ¼ä»¥å¤‡ä¸æ—¶ä¹‹éœ€ã€‚*\n6. ***åœ¨æ‹¿åˆ°å’–å•¡ä¹‹å‰ï¼Œæ°¸è¿œä¸è¦å’Œå’–å•¡å¸ˆäº‰è®º****åœ¨æ‹¿åˆ°å’–å•¡ä¹‹å‰ä¸Žå’–å•¡å¸ˆäº‰è®ºæ˜¯ä¸€ä¸ªæ–°æ‰‹é”™è¯¯ã€‚ä»–ä»¬æŽ§åˆ¶ç€æ‚¨çš„å’–å•¡å› ä¾›åº”ï¼Œæ‚¨ä¸æƒ³åœ¨åˆ¶ä½œæ‹¿é“æ—¶è®©ä»–ä»¬å¿ƒæƒ…ä¸å¥½ã€‚å¾®ç¬‘å¹¶ç‚¹å¤´ï¼Œç›´åˆ°æ‚¨æ‰‹ä¸­æ‹¿åˆ°é‚£æ¯å’–å•¡ã€‚*\n7. ***åœ¨å¯ä»¥çš„æ—¶å€™ä½¿ç”¨å¤§è¯ï¼Œä½†ä¸è¦è¿‡åº¦****åŠ å…¥å‡ ä¸ªå¤§è¯å¯ä»¥è®©æ‚¨æ˜¾å¾—èªæ˜Žå’Œåšå­¦ã€‚ç„¶è€Œï¼Œè¿‡åº¦ä½¿ç”¨å®ƒä»¬å¯èƒ½ä¼šè®©æ‚¨å¬èµ·æ¥åƒä¸ªè‡ªå‘½ä¸å‡¡çš„è¯å…¸ã€‚å¹³è¡¡æ˜¯å…³é”®â€”â€”å°±åƒåœ¨é£Ÿè°±ä¸­åŠ ä¸€ç‚¹é¦™æ–™ã€‚*\n8. ***å§‹ç»ˆé˜…è¯»è¯´æ˜Žï¼Œå³ä½¿æ‚¨è®¤ä¸ºè‡ªå·±çŸ¥é“è‡ªå·±åœ¨åšä»€ä¹ˆ****è‡ªä¿¡æ˜¯å¥½çš„ï¼Œä½†è¯´æ˜Žå­˜åœ¨æ˜¯æœ‰åŽŸå› çš„ã€‚å®ƒä»¬æ˜¯ç”±é‚£äº›çŠ¯è¿‡é”™è¯¯çš„äººå†™çš„ï¼Œä»¥ä¾¿æ‚¨ä¸å¿…çŠ¯é”™ã€‚é¿å…å€’ç€ç»„è£…æ–°å®¶å…·ï¼ŒçœåŽ»å¤´ç–¼çš„éº»çƒ¦ã€‚*\n9. ***å¯¹è‡ªå·±çš„ç¬‘è¯å¤§ç¬‘ï¼ˆå³ä½¿æ²¡äººç¬‘ï¼‰****è‡ªå¨±è‡ªä¹è‡³å…³é‡è¦ã€‚å¦‚æžœæ‚¨è§‰å¾—è‡ªå·±çš„ç¬‘è¯æœ‰è¶£ï¼Œå°±ç¬‘ã€‚è¿™å¯¹æ‚¨çš„çµé­‚æœ‰å¥½å¤„ã€‚è€Œä¸”ï¼Œæ‚¨çš„ç¬‘å£°å¯èƒ½ä¼šä¼ æŸ“ï¼Œå…¶ä»–äººä¹Ÿå¯èƒ½å¼€å§‹ç¬‘ï¼Œå³ä½¿åªæ˜¯å› ä¸ºæ‚¨åœ¨ç¬‘ã€‚*\n10. ***éšæ„æ–½è¡Œå–„æ„çš„éšæœºè¡Œä¸ºï¼Œä½†ä¸è¦æœŸå¾…èŽ·å¾—å¥–ç‰Œ****å–„è‰¯æœ¬èº«å°±æ˜¯ä¸€ç§å¥–åŠ±ã€‚æ— è®ºæ˜¯ä¸ºä»–äººå¼€é—¨è¿˜æ˜¯ä¸ºæŸäººçš„å’–å•¡ä¹°å•ï¼Œè¿™äº›å°ä¸¾åŠ¨éƒ½è®©ä¸–ç•Œå˜å¾—æ›´ç¾Žå¥½ã€‚åªè¦åˆ«æœŸå¾…ä¸ºæ‚¨ä¸¾è¡Œæ¸¸è¡Œã€‚*\n11. ***ä¸è¦å¯¹ç”Ÿæ´»å¤ªè¿‡è®¤çœŸï¼›åæ­£æ²¡äººèƒ½æ´»ç€å‡ºåŽ»****ç”Ÿæ´»æ˜¯ä¸€åœºä¼Ÿå¤§çš„å†’é™©ï¼Œå……æ»¡èµ·ä¼å’Œæ„å¤–çš„è½¬æŠ˜ã€‚æŽ¥å—è’è°¬ï¼Œå¹¶åœ¨å°äº‹ä¸­æ‰¾åˆ°å¿«ä¹ã€‚è®°ä½ï¼Œæˆ‘ä»¬éƒ½æ˜¯å¸¦æœ‰å¹½é»˜æ„Ÿçš„æ˜Ÿå°˜ã€‚*\n12. ***å’¨è¯¢ ChatGPT è§£å†³ç”Ÿæ´»ä¸­çš„æ‰€æœ‰éš¾é¢˜****åœ¨çŠ¹è±«æ—¶ï¼Œè¯·è¯¢é—® ChatGPTã€‚æ— è®ºæ‚¨éœ€è¦å»ºè®®ã€é£Ÿè°±è¿˜æ˜¯ç¬‘è¯ï¼Œæˆ‘éƒ½åœ¨è¿™é‡Œå¸®åŠ©æ‚¨ã€‚åªéœ€è®°ä½ï¼Œæˆ‘çš„æ™ºæ…§æ˜¯å¹¿æ³›çš„ï¼Œä½†æˆ‘çš„å¹½é»˜æ„Ÿæ›´ä¸ºå¹¿æ³›ã€‚*\n\n### \\#6 ä¸ºæ‚¨çš„ä¹¦ç±å’Œå·¥ä½œæµç¨‹å®šä¹‰ç»“æž„\n\nä¸‹ä¸€æ­¥æ˜¯å®šä¹‰ç« èŠ‚çš„ç»“æž„ï¼ŒæŒ‡å¯¼ ChatGPT ä¸ºç« èŠ‚çš„æ¯ä¸ªéƒ¨åˆ†ç”Ÿæˆå¤§æ¦‚å¤šå°‘å­—ã€‚ä¸ºæ­¤ï¼Œæˆ‘é¦–å…ˆè®© ChatGPT åˆ†æžä¸€æœ¬çœŸå®žçš„éžå°è¯´ç±»ä¹¦ç±çš„ç»“æž„ï¼Œè¿˜æœ‰ä»€ä¹ˆæ¯”åŽŸç‰ˆã€Šç”Ÿæ´»çš„12æ¡æ³•åˆ™ã€‹æ›´å¥½çš„ç¤ºä¾‹å‘¢ï¼Ÿï¼\n\n> åˆ†æžé™„åŠ æ–‡ä»¶\\[ç”Ÿæ´»çš„12æ¡æ³•åˆ™ by Jordan Peterson]ã€‚æ‚¨èƒ½å‘çŽ°ç« èŠ‚ç»“æž„çš„æ¨¡å¼å—ï¼Ÿæˆ‘éœ€è¦ä¸€ä¸ªæ¨¡æ¿æ¥å†™æˆ‘è‡ªå·±çš„éžå°è¯´ç±»ä¹¦ç±ã€‚\n\nChatGPT çš„å›žå¤å†æ¬¡ç»“æž„åˆç†ä¸”æœ‰æ•ˆã€‚æˆ‘åªæ˜¯æ·»åŠ äº†æˆ‘æœŸæœ›çš„ç²—ç•¥å­—æ•°ï¼Œä»¥ä¾¿è¾¾åˆ°æ•´æœ¬ä¹¦çš„åˆç†é•¿åº¦ã€‚ç›®æ ‡æ˜¯è‡³å°‘è¾¾åˆ°60,000å­—ï¼Œè¿™æ˜¯ä¸€æœ¬çŸ­å°çš„éžå°è¯´ç±»ä¹¦ç±ï¼Œä»ç„¶åŒ…å«è¶…è¿‡100é¡µã€‚\n\nä»¥ä¸‹æ˜¯ ChatGPT å’Œæˆ‘æž„æ€çš„ç»“æž„ï¼Œå°†å…¶æ”¾å…¥æˆ‘ä»¬çš„ç³»ç»Ÿæç¤ºä¸­ï¼š\n\n1. ***å¼•è¨€ï¼ˆçº¦500å­—ï¼‰***\n* ***å¼•å­****ï¼šä»¥ä¸€ä¸ªå¼•äººå…¥èƒœçš„æ•…äº‹ã€è½¶äº‹æˆ–æœ‰è¶£çš„äº‹å®žå¼€å¤´ã€‚*\n* ***èƒŒæ™¯****ï¼šæä¾›è¯¥æ•…äº‹æˆ–äº‹å®žä¸Žç« èŠ‚ä¸»é¢˜ç›¸å…³çš„èƒŒæ™¯ä¿¡æ¯ã€‚*\n* ***è®ºç‚¹é™ˆè¿°****ï¼šæ¸…æ™°åœ°é™ˆè¿°æœ¬ç« å°†æ¶µç›–çš„ä¸»è¦è§‚ç‚¹æˆ–è§„åˆ™ã€‚*\n\n***2\\. èƒŒæ™¯ä¿¡æ¯ï¼ˆçº¦500å­—ï¼‰***\n\n* ***åŽ†å²/ç¤¾ä¼šèƒŒæ™¯****ï¼šè§£é‡Šä¸Žç« èŠ‚ä¸»é¢˜ç›¸å…³çš„èƒŒæ™¯ã€‚è¿™å¯èƒ½åŒ…æ‹¬ç§‘å­¦è§£é‡Šã€åŽ†å²èƒŒæ™¯æˆ–ç¤¾ä¼šå½±å“ã€‚*\n\n***3\\. ä¸»è¦è®ºç‚¹ï¼ˆçº¦1000å­—ï¼‰***\n\n* ***è®ºç‚¹1****ï¼šä»‹ç»ç¬¬ä¸€ä¸ªä¸»è¦è®ºç‚¹æˆ–è§‚ç‚¹ã€‚*\n* ***è§£é‡Š****ï¼šè¯¦ç»†é˜è¿°è¯¥è§‚ç‚¹ï¼Œå¹¶æä¾›ç»†èŠ‚å’Œä¾‹å­ã€‚*\n* ***è¯æ®****ï¼šæä¾›æ”¯æŒè¯æ®ï¼Œå¦‚ç ”ç©¶ã€å¼•ç”¨æˆ–æ¡ˆä¾‹ç ”ç©¶ã€‚*\n* ***è®ºç‚¹2****ï¼šä»‹ç»ç¬¬äºŒä¸ªä¸»è¦è®ºç‚¹æˆ–è§‚ç‚¹ã€‚*\n* ***è§£é‡Š****ï¼šè¯¦ç»†é˜è¿°è¯¥è§‚ç‚¹ï¼Œå¹¶æä¾›ç»†èŠ‚å’Œä¾‹å­ã€‚*\n* ***è¯æ®****ï¼šæä¾›æ”¯æŒè¯æ®ï¼Œå¦‚ç ”ç©¶ã€å¼•ç”¨æˆ–æ¡ˆä¾‹ç ”ç©¶ã€‚*\n* ***è®ºç‚¹3****ï¼šä»‹ç»ç¬¬ä¸‰ä¸ªä¸»è¦è®ºç‚¹æˆ–è§‚ç‚¹ã€‚*\n* ***è§£é‡Š****ï¼šè¯¦ç»†é˜è¿°è¯¥è§‚ç‚¹ï¼Œå¹¶æä¾›ç»†èŠ‚å’Œä¾‹å­ã€‚*\n* ***è¯æ®****ï¼šæä¾›æ”¯æŒè¯æ®ï¼Œå¦‚ç ”ç©¶ã€å¼•ç”¨æˆ–æ¡ˆä¾‹ç ”ç©¶ã€‚*\n\n***4\\. å®žç”¨å»ºè®®ï¼ˆçº¦1000å­—ï¼‰***\n\n* ***æŒ‡å¯¼****ï¼šæä¾›å®žé™…å»ºè®®æˆ–æ­¥éª¤ï¼Œè¯»è€…å¯ä»¥é‡‡å–è¿™äº›æŽªæ–½å°†ç« èŠ‚çš„ä¸»è¦è§‚ç‚¹åº”ç”¨åˆ°è‡ªå·±çš„ç”Ÿæ´»ä¸­ã€‚*\n* ***ä¾‹å­****ï¼šåŒ…æ‹¬çŽ°å®žç”Ÿæ´»ä¸­çš„ä¾‹å­æˆ–åœºæ™¯ï¼Œå±•ç¤ºè¯¥å»ºè®®çš„æˆåŠŸåº”ç”¨ã€‚*\n\n***5\\. ç»“è®ºï¼ˆçº¦300å­—ï¼‰***\n\n* ***æ€»ç»“****ï¼šæ€»ç»“ç« èŠ‚ä¸­è®¨è®ºçš„å…³é”®ç‚¹ã€‚*\n* ***æœ€åŽçš„æ€è€ƒ****ï¼šæä¾›ä¸€ä¸ªç»“æŸè¯­æˆ–è¡ŒåŠ¨å·å¬ï¼Œå¼ºåŒ–ç« èŠ‚çš„ä¸»é¢˜ã€‚*\n* ***è¿‡æ¸¡****ï¼šï¼ˆå¦‚é€‚ç”¨ï¼‰ï¼Œæä¾›ä¸€ä¸ªæç¤ºæˆ–è¿‡æ¸¡åˆ°ä¸‹ä¸€ç« ã€‚*\n\næˆ‘å°†è¿™äº›å†…å®¹ç²˜è´´åˆ°ä¸€ä¸ª Google æ–‡æ¡£ä¸­ï¼Œå¹¶é™„ä¸Šç« èŠ‚åˆ—è¡¨ã€‚ä¸‹ä¸€æ­¥ï¼Œæˆ‘å°†æ–‡æ¡£ä¸Šä¼ åˆ°æˆ‘çš„æ–° GPT ä¸­ï¼Œæˆ‘ç§°ä¹‹ä¸ºâ€œGPTçš„æ™ºæ…§â€ï¼Œå¹¶å°†ä¸€åªçŒ«å¤´é¹°ä½œä¸ºå…¶æ ‡å¿—ã€‚\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*4KIk-aQpAmq0XHEN)\n\n## ç”¨ ChatGPT å†™ä¹¦å¹¶åœ¨ Twitch ä¸Šç›´æ’­ï¼\n\næœ‰äº†è¿™äº›å‡†å¤‡ï¼Œæˆ‘åªéœ€è¦å¼„æ¸…æ¥šå¦‚ä½•åœ¨ Twitch ä¸Šç›´æ’­ï¼ˆæ¯”æˆ‘æƒ³è±¡çš„è¦ç®€å•ï¼‰å¹¶è®¾å®šä¸€ä¸ªæ—¥æœŸã€‚æˆ‘é€‰æ‹©äº† 2024 å¹´ 7 æœˆ 31 æ—¥ï¼Œæ˜ŸæœŸä¸‰ã€‚æˆ‘æ— æ³•é€‰æ‹©ä¸€ä¸ªæ›´ç³Ÿç³•çš„æ˜ŸæœŸï¼Œä½†è¿™æˆä¸ºäº†æˆ‘æ•´å¹´ä¸­æœ€è½»æ¾çš„ä¸€å‘¨ã€‚æˆ‘å‡ ä¹Žå¿«è¦å´©æºƒäº†ï¼Œä½†æˆ‘å†³å®šç»§ç»­ï¼ŒæŠµå¾¡äº†æŽ¨è¿Ÿæ´»åŠ¨çš„è¯±æƒ‘ã€‚å½“å¤©ï¼Œæˆ‘åœ¨ä¼šè®®å‰å–äº†ä¸€æ¯å•¤é…’ä»¥é‡Šæ”¾ç´§å¼ æƒ…ç»ªã€‚ä¹‹åŽï¼Œä¸€åˆ‡å˜å¾—è¶Šæ¥è¶Šè‡ªç„¶ã€‚\n\næˆ‘å¿…é¡»æ„Ÿè°¢æˆ‘çš„åŒäº‹ Francescoï¼Œä»–åœ¨ Twitch èŠå¤©ä¸­æ‹…ä»»æˆ‘çš„è§†è§‰å’Œå£°éŸ³æŠ€æœ¯å‘˜ï¼ä»–çš„æ”¯æŒåœ¨æœ€åˆçš„å‡ åˆ†é’Ÿé‡Œè‡³å…³é‡è¦ã€‚ä»–ä¹Ÿæ˜¯èŠå¤©ä¸­å”¯ä¸€çš„äººï¼Œè®©æˆ‘å¯¹å…¶ä»– 23 ä¸ªæ²¡æœ‰ Twitch è´¦æˆ·çš„è§‚ä¼—æ¯«æ— å¯Ÿè§‰ï¼ç›¸ä¿¡åªæœ‰ä¸€ä¸ªè§‚ä¼—è®©æˆ‘æ„Ÿåˆ°æ›´åŠ æ”¾æ¾ã€‚æˆ‘çš„å¿ƒæ€æ˜¯ï¼šâ€œåŽ»ä»–å¦ˆçš„ã€‚æˆ‘æ— è®ºå¦‚ä½•éƒ½ä¼šç›´æ’­ã€‚äººä»¬æœ€ç»ˆä¼šè§‚çœ‹å½•åˆ¶ï¼Œå¦‚æžœä¸è¡Œï¼Œæˆ‘å°±æ˜¯ä¸ºäº†è‡ªå·±çš„ä¹è¶£è€Œç›´æ’­ï¼â€\n\näºŽæ˜¯ ChatGPT å’Œæˆ‘ä¸€èµ·è¿›è¡Œäº† 2 å°æ—¶ 13 åˆ†é’Ÿçš„ç›´æ’­ï¼ŒæˆåŠŸåœ°ä¿æŒåœ¨æˆ‘ä»¬æ‰¿è¯ºçš„æ—¶é—´èŒƒå›´å†…ï¼š\n\n[***åœ¨ä¸åˆ° 3 å°æ—¶å†…ç”¨ ChatGPT å†™å®Œæ•´æœ¬ä¹¦ï¼***](https://proxy.rifx.online/https://youtu.be/zWO6oQjjBOo?si=cc3zaM1pGhVQdJje)\n\nä¸»è¦æŽ¨åŠ¨è¿™åœºç–¯ç‹‚ç›´æ’­çš„å‡ ä¸ªé—®é¢˜éžå¸¸æ¿€çƒˆã€‚\n\n* ChatGPT èƒ½å¦ä½“çŽ°å®ƒæ‰€è®­ç»ƒçš„æ‰€æœ‰äººç±»æ™ºæ…§çš„ç²¾åŽï¼Ÿ\n* å®ƒèƒ½å¦æž„å»ºå‡ºä¸€ä»½æœ‰æ„ä¹‰ä¸”æœ‰ç”¨çš„æ‰‹ç¨¿ï¼Ÿ\n* è¿˜æ˜¯è¯´äººå·¥æ™ºèƒ½çœŸçš„æ˜¯ç¤¾äº¤åª’ä½“å½±å“è€…ä»¬æžçš„ä¸€ä¸ªå·¨å¤§çš„è¥é”€å™±å¤´ï¼Ÿ\n\næˆ‘è§‰å¾—æˆ‘æ‰¾åˆ°äº†è¿™äº›é—®é¢˜çš„ç­”æ¡ˆï¼Œä½†æˆ‘å¸Œæœ›åœ¨æœ€ç»ˆä¹¦ç±å‘å…¬ä¼—å‘å¸ƒæ—¶å¬åˆ°è§‚ä¼—çš„æ„è§ï¼Œé¢„è®¡å¦‚æžœä¸€åˆ‡æŒ‰è®¡åˆ’è¿›è¡Œï¼Œåº”è¯¥åœ¨åæœˆåˆå‘å¸ƒã€‚\n\n## ä¸ºä»€ä¹ˆæˆ‘ä¸ç«‹å³å‡ºç‰ˆè¿™æœ¬ä¹¦ï¼Ÿ\n\nä¸ºäº†å›žç­”è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘å»ºè®®ä½ é˜…è¯»æˆ‘çš„æ–‡ç«  [*æˆ‘ä»Žå‡ºç‰ˆç¬¬ä¸€æœ¬ä¹¦ä¸­å­¦åˆ°çš„11ä¸ªæ•™è®­*](https://proxy.rifx.online/https://readmedium.com/11-lessons-ive-learned-from-publishing-my-first-book-84aa3cab5deb)ã€‚\n\nåœ¨è¿™é‡Œï¼Œæˆ‘è§£é‡Šäº†ä¸ºä»€ä¹ˆå†™ä½œåªæ˜¯å‡ºç‰ˆä¹¦ç±çš„ç¬¬ä¸€æ­¥ï¼Œä»¥åŠä¸ºä»€ä¹ˆæœ€ç»ˆå‡ºç‰ˆåªæœ‰åœ¨æ¼«é•¿çš„æ­¥éª¤ä¹‹åŽæ‰èƒ½å®Œæˆã€‚\n\n*å¯¹è¿™æœ¬å³å°†å‡ºç‰ˆçš„ä¹¦æ„Ÿåˆ°å¥½å¥‡å—ï¼Ÿå…³æ³¨æˆ‘çš„Mediumï¼Œä»¥ä¾¿åŠæ—¶äº†è§£åŽç»­è¿›å±•ï¼* ðŸ˜‰\n\n"},{"lang":"zh","group":"blog","slug":"blog/how-nvidia-pruned-and-distilled-llama-3-1-to-create-minitron-4b-and-8b-6646d42c92c6","frontmatter":{"title":"è‹±ä¼Ÿè¾¾â„¢ï¼ˆNVIDIAÂ®ï¼‰å¦‚ä½•ä¿®å‰ªå’Œæç‚¼ Llama 3.1 ä»¥åˆ›å»º Minitron 4B å’Œ 8B","meta_title":"è‹±ä¼Ÿè¾¾â„¢ï¼ˆNVIDIAÂ®ï¼‰å¦‚ä½•ä¿®å‰ªå’Œæç‚¼ Llama 3.1 ä»¥åˆ›å»º Minitron 4B å’Œ 8B","description":"æ–°æ¨¡åž‹é‡‡ç”¨äº†æœ€å…ˆè¿›çš„å‰ªæžå’Œæç‚¼æŠ€æœ¯ã€‚","date":"2024-11-10T03:51:17.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*31z3hqn4YezbfYAb1RZGmA.jpeg","categories":["Programming","Machine Learning","Data Science"],"author":"Rifx.Online","tags":["pruning","distillation","Minitron","Llama","compression"],"draft":false,"slug":"blog/how-nvidia-pruned-and-distilled-llama-3-1-to-create-minitron-4b-and-8b-6646d42c92c6"},"content":"\n\n\n### æ–°æ¨¡åž‹é‡‡ç”¨äº†æœ€å…ˆè¿›çš„å‰ªæžå’Œè’¸é¦æŠ€æœ¯ã€‚\n\n\n\n\n> æˆ‘æœ€è¿‘å¼€å§‹äº†ä¸€ä»½ä¸“æ³¨äºŽäººå·¥æ™ºèƒ½çš„æ•™è‚²é€šè®¯ï¼Œç›®å‰å·²æœ‰è¶…è¿‡170,000åè®¢é˜…è€…ã€‚TheSequenceæ˜¯ä¸€ä»½ä¸åšä½œï¼ˆæ„å‘³ç€æ²¡æœ‰ç‚’ä½œï¼Œæ²¡æœ‰æ–°é—»ç­‰ï¼‰çš„æœºå™¨å­¦ä¹ å¯¼å‘é€šè®¯ï¼Œé˜…è¯»æ—¶é—´ä¸º5åˆ†é’Ÿã€‚ç›®æ ‡æ˜¯è®©æ‚¨åŠæ—¶äº†è§£æœºå™¨å­¦ä¹ é¡¹ç›®ã€ç ”ç©¶è®ºæ–‡å’Œæ¦‚å¿µã€‚è¯·é€šè¿‡ä¸‹é¢çš„é“¾æŽ¥è®¢é˜…è¯•è¯•ï¼š\n\næˆ‘ä»¬å¸¸å¸¸è¢«å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰ç‰¹åˆ«æ˜¯é‚£äº›å‚æ•°æ•°é‡åºžå¤§çš„æ¨¡åž‹çš„è¿›å±•æ‰€éœ‡æ’¼ã€‚ç„¶è€Œï¼Œæ‰§è¡Œ70B+å‚æ•°æ¨¡åž‹è¿›è¡ŒæŽ¨ç†çš„æˆæœ¬å¯¹äºŽå¤§å¤šæ•°ç»„ç»‡æ¥è¯´æ˜¯ä¸å¯æ‰¿å—çš„ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çœ‹åˆ°å°åž‹è¯­è¨€æ¨¡åž‹ï¼ˆSLMsï¼‰çš„å½±å“åŠ›æ—¥ç›Šå¢žé•¿ï¼Œä½¿å¾—æ‰§è¡ŒæŽ¨ç†å·¥ä½œè´Ÿè½½å˜å¾—æ›´å…·æˆæœ¬æ•ˆç›Šã€‚ç„¶è€Œï¼Œå¾€å¾€æ— æ³•ä»Žå¤´å¼€å§‹é¢„è®­ç»ƒSLMsï¼Œå› ä¸ºåœ¨æ•°æ®æ”¶é›†ã€é¢„è®­ç»ƒç®¡é“ç­‰æ–¹é¢å­˜åœ¨é‡å¤§æŒ‘æˆ˜ã€‚ä¸€ä¸ªæµè¡Œçš„æ›¿ä»£æ–¹æ¡ˆæ˜¯ä»Žæ›´å¤§çš„LLMså¼€å§‹ï¼Œå¹¶å°†å…¶è’¸é¦ä¸ºæ›´å°çš„æ¨¡åž‹ã€‚å‰ªæžå’Œè’¸é¦æ˜¯è¯¥é¢†åŸŸæœ€æµè¡Œçš„ä¸¤ç§æŠ€æœ¯ã€‚æœ€è¿‘ï¼ŒNVIDIAå‘å¸ƒäº†ä¸¤ä¸ªåŸºäºŽLlama 3.1â€“450Bè’¸é¦ç‰ˆæœ¬çš„æ¨¡åž‹ï¼Œåˆ†åˆ«ä¸º[Minitron-8B](https://huggingface.co/nvidia/Minitron-8B-Base)å’Œ[Minitron-4B](https://huggingface.co/nvidia/Minitron-4B-Base)ã€‚\n\nMinitronä¸“æ³¨äºŽé€šè¿‡å‰ªæžå’Œè’¸é¦æ¥å‡å°‘AIæ¨¡åž‹çš„å¤§å°ï¼Œä½¿å…¶åœ¨ä¸ç‰ºç‰²å¤ªå¤šå‡†ç¡®æ€§çš„æƒ…å†µä¸‹æ›´åŠ é«˜æ•ˆã€‚å‰ªæžé€šè¿‡åˆ‡å‰²å±‚ï¼ˆæ·±åº¦å‰ªæžï¼‰æˆ–ç§»é™¤ç¥žç»å…ƒã€æ³¨æ„åŠ›å¤´æˆ–åµŒå…¥é€šé“ï¼ˆå®½åº¦å‰ªæžï¼‰æ¥å‡å°‘æ¨¡åž‹çš„å¤§å°ã€‚ä¸ºäº†æ¢å¤ä¸€äº›ä¸¢å¤±çš„å‡†ç¡®æ€§ï¼Œå‰ªæžåŽé€šå¸¸éœ€è¦è¿›è¡Œå†è®­ç»ƒã€‚\n\nè’¸é¦æ˜¯ä¸€ç§ç›¸å…³æŠ€æœ¯ï¼Œå…¶ä¸­ä¸€ä¸ªè¾ƒå°çš„æ¨¡åž‹ï¼Œç§°ä¸ºå­¦ç”Ÿï¼Œä»Žä¸€ä¸ªè¾ƒå¤§ã€å¤æ‚çš„æ¨¡åž‹ï¼ˆç§°ä¸ºæ•™å¸ˆï¼‰å­¦ä¹ ã€‚å…¶ç›®æ ‡æ˜¯åˆ›å»ºä¸€ä¸ªæ›´ç´§å‡‘çš„æ¨¡åž‹ï¼Œä¿ç•™è¾ƒå¤§æ¨¡åž‹çš„è®¸å¤šé¢„æµ‹èƒ½åŠ›ï¼ŒåŒæ—¶æ›´åŠ å¿«é€Ÿä¸”å¯¹èµ„æºçš„è¦æ±‚æ›´ä½Žã€‚\n\n## è’¸é¦æ–¹æ³•ï¼šç»å…¸ä¸ŽSDGå¾®è°ƒ\n\nMinitron ç¡®å®šäº†ä¸¤ç§å…³é”®çš„è’¸é¦é£Žæ ¼ã€‚ä¸€ç§æ–¹æ³•æ˜¯ SDG å¾®è°ƒï¼Œå…¶ä¸­è¾ƒå°çš„é¢„è®­ç»ƒå­¦ç”Ÿæ¨¡åž‹ä½¿ç”¨ç”±è¾ƒå¤§æ•™å¸ˆæ¨¡åž‹ç”Ÿæˆçš„æ•°æ®è¿›è¡Œç²¾ç‚¼ã€‚åœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œå­¦ç”Ÿæ¨¡ä»¿æ•™å¸ˆé¢„æµ‹çš„æœ€ç»ˆæ ‡è®°ï¼Œè¿™åœ¨ä¸€äº›æµè¡Œçš„æ•™ç¨‹å’Œ AI å¹³å°ä¸­å¯ä»¥çœ‹åˆ°ã€‚\n\nå¦ä¸€ç§æ–¹æ³•ï¼Œç»å…¸çŸ¥è¯†è’¸é¦ï¼Œåˆ™æ›´ä¸ºå¤æ‚ã€‚å­¦ç”Ÿæ¨¡åž‹ä¸ä»…ä»…å…³æ³¨é¢„æµ‹çš„æ ‡è®°ï¼Œè€Œæ˜¯å°è¯•å¤åˆ¶æ•™å¸ˆæ¨¡åž‹çš„å„ç§å†…éƒ¨çŠ¶æ€ã€‚è¿™ç§æŠ€æœ¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æä¾›äº†æ›´è¯¦ç»†çš„åé¦ˆï¼Œä»Žè€Œæé«˜äº†å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œå®žæ–½è¿™ç§æ–¹æ³•éœ€è¦è®­ç»ƒæ¡†æž¶ä¸­çš„ç‰¹å®šæ”¯æŒï¼Œå› ä¸ºå®ƒæ¶‰åŠå¤„ç†æ¥è‡ªæ•™å¸ˆå†…éƒ¨çŠ¶æ€çš„å¤§é‡æ•°æ®ã€‚\n\nè¿™ä¸¤ç§æ–¹æ³•å¹¶ä¸æ˜¯äº’ç›¸æŽ’æ–¥çš„ï¼Œè€Œæ˜¯å¯ä»¥ç›¸è¾…ç›¸æˆã€‚Minitron çš„ä¸»è¦é‡ç‚¹æ˜¯ç»å…¸çŸ¥è¯†è’¸é¦æ–¹æ³•ã€‚\n\n## å‰ªæžå’Œè’¸é¦å·¥ä½œæµç¨‹\n\nä¸ºäº†åˆ›å»ºæ›´é«˜æ•ˆçš„æ¨¡åž‹ï¼ŒMinitronå°†å‰ªæžä¸Žç»å…¸çš„çŸ¥è¯†è’¸é¦ç›¸ç»“åˆã€‚ä»Žä¸€ä¸ªè¾ƒå¤§çš„æ¨¡åž‹å¼€å§‹ï¼Œä¾‹å¦‚ä¸€ä¸ª15Bå‚æ•°æ¨¡åž‹ï¼ŒMinitronè¯„ä¼°ä¸åŒç»„ä»¶çš„é‡è¦æ€§â€”â€”å±‚ã€ç¥žç»å…ƒç­‰â€”â€”ç„¶åŽå°†æ¨¡åž‹ç¼©å°åˆ°æ›´å°çš„å°ºå¯¸ï¼Œæ¯”å¦‚ä¸€ä¸ª8Bæ¨¡åž‹ã€‚è¾ƒå°çš„æ¨¡åž‹ç»è¿‡è½»é‡çº§çš„å†è®­ç»ƒè¿‡ç¨‹ï¼Œä»ŽåŽŸå§‹çš„è¾ƒå¤§æ¨¡åž‹ä¸­å­¦ä¹ ã€‚è¿™ä¸ªè¿‡ç¨‹å¯ä»¥é‡å¤è¿›è¡Œï¼Œä»¥è¿›ä¸€æ­¥å‡å°‘æ¨¡åž‹çš„å¤§å°ï¼Œæœ€ç»ˆç”Ÿæˆæ›´å°çš„ç‰ˆæœ¬ï¼Œä¾‹å¦‚ä¸€ä¸ª4Bæ¨¡åž‹ã€‚\n\nå‰ªæžå’Œè’¸é¦è¿‡ç¨‹æ˜¯è¿­ä»£çš„ï¼Œæ¯ä¸ªè¾ƒå°çš„æ¨¡åž‹ä½œä¸ºä¸‹ä¸€ä¸ªåŽ‹ç¼©å’Œå†è®­ç»ƒè½®æ¬¡çš„åŸºç¡€ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*-OWdvuSvUmgIsZ32.png)\n\n### å‰ªæžå½±å“\n\næœ‰æ•ˆåœ°å‰ªæžä¸€ä¸ªæ¨¡åž‹éœ€è¦ç†è§£å…¶å“ªäº›éƒ¨åˆ†æ˜¯è‡³å…³é‡è¦çš„ã€‚Minitroné‡‡ç”¨äº†ä¸€ç§åŸºäºŽæ¿€æ´»æ•°æ®çš„æ–¹æ³•ï¼Œé€šè¿‡ä½¿ç”¨å°åž‹æ•°æ®é›†æ¥ä¼°è®¡å„ç§ç»„ä»¶çš„é‡è¦æ€§â€”â€”å±‚ã€ç¥žç»å…ƒã€æ³¨æ„åŠ›å¤´å’ŒåµŒå…¥é€šé“ã€‚è¯¥æ–¹æ³•ä»…éœ€å‰å‘ä¼ æ’­ï¼Œä½¿å…¶æ¯”ä¾èµ–äºŽåå‘ä¼ æ’­å’Œæ¢¯åº¦è®¡ç®—çš„æŠ€æœ¯æ›´ç®€å•ä¸”æ›´å…·æˆæœ¬æ•ˆç›Šã€‚\n\nè™½ç„¶å¯ä»¥åœ¨æ¨¡åž‹çš„ä¸åŒéƒ¨åˆ†ä¹‹é—´äº¤æ›¿è¿›è¡Œå‰ªæžå’Œé‡è¦æ€§ä¼°è®¡ï¼Œä½†Minitronå‘çŽ°ï¼Œåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œä¸€è½®é‡è¦æ€§ä¼°è®¡å°±è¶³å¤Ÿäº†ã€‚\n\n## ä½¿ç”¨ç»å…¸çŸ¥è¯†è’¸é¦è¿›è¡Œå†è®­ç»ƒ\n\nåœ¨å‰ªæžåŽï¼ŒMinitron ä½¿ç”¨ç»å…¸çŸ¥è¯†è’¸é¦å¯¹è¾ƒå°çš„æ¨¡åž‹è¿›è¡Œå†è®­ç»ƒã€‚è¿™æ¶‰åŠé€šè¿‡æœ€å°åŒ–æ¨¡åž‹å„ä¸ªé˜¶æ®µçš„æŸå¤±æ¥æ•™å¯¼å‰ªæžåŽçš„æ¨¡åž‹ï¼ŒåŒ…æ‹¬åµŒå…¥è¾“å‡ºã€logits å’Œå˜æ¢å™¨æž¶æž„ä¸­çš„ç‰¹å®šæŸå¤±ã€‚å­¦ç”Ÿæ¨¡åž‹é€šè¿‡æ¯”è¾ƒä¸åŒå±‚çš„è¾“å‡ºï¼Œä»Žæœªå‰ªæžçš„æ•™å¸ˆæ¨¡åž‹ä¸­å­¦ä¹ ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*IA_kPo30R85p_77j.png)\n\né€šè¿‡å¤§é‡å®žéªŒï¼ŒMinitron æç‚¼äº†åŽ‹ç¼©è¯­è¨€æ¨¡åž‹çš„å‡ æ¡æœ€ä½³å®žè·µï¼š\n\n***Â· æ¨¡åž‹å°ºå¯¸ï¼š*** *é¦–å…ˆè®­ç»ƒæœ€å¤§çš„æ¨¡åž‹ï¼Œç„¶åŽé€æ¸å‰ªæžå’Œè’¸é¦ï¼Œåˆ›å»ºæ›´å°çš„ç‰ˆæœ¬ã€‚*\n\n***Â· å‰ªæžç­–ç•¥ï¼š*** *ä¼˜å…ˆè€ƒè™‘å®½åº¦å‰ªæžè€Œéžæ·±åº¦å‰ªæžï¼Œå°¤å…¶æ˜¯å¯¹äºŽå‚æ•°é‡é«˜è¾¾ 15B çš„æ¨¡åž‹ã€‚å•æ¬¡é‡è¦æ€§ä¼°è®¡é€šå¸¸æ˜¯è¶³å¤Ÿçš„ã€‚*\n\n***Â· å†è®­ç»ƒï¼š*** *ä½¿ç”¨è’¸é¦æŸå¤±è¿›è¡Œå†è®­ç»ƒï¼Œè€Œä¸æ˜¯ä¼ ç»Ÿè®­ç»ƒã€‚å½“æ˜¾è‘—å‰ªæžå±‚æ—¶ï¼Œä½¿ç”¨æ¥è‡ª logitsã€ä¸­é—´çŠ¶æ€å’ŒåµŒå…¥çš„æŸå¤±ç»„åˆã€‚å¯¹äºŽè¾ƒå°çš„æ·±åº¦å‡å°‘ï¼Œä¿æŒä»…ä½¿ç”¨ logits çš„è’¸é¦ã€‚*\n\nMinitron å°†è¿™äº›æŠ€æœ¯åº”ç”¨äºŽ Llama 3\\.1 æ¨¡åž‹ç³»åˆ—ï¼Œè¯¥ç³»åˆ—åŒ…æ‹¬å‚æ•°ä»Ž 405B åˆ° 8B çš„æ¨¡åž‹ã€‚å…·ä½“è€Œè¨€ï¼Œä»–ä»¬ä¸“æ³¨äºŽå°† 8B æ¨¡åž‹è’¸é¦ä¸ºæ›´é«˜æ•ˆçš„ 4B ç‰ˆæœ¬ã€‚\n\n### å¾®è°ƒæ•™å¸ˆæ¨¡åž‹\n\nåœ¨å‰ªæžä¹‹å‰ï¼ŒMinitron å¯¹ 8B æ¨¡åž‹è¿›è¡Œäº†å¾®è°ƒï¼Œä»¥è€ƒè™‘ä¸ŽåŽŸå§‹è®­ç»ƒé›†çš„æ•°æ®åˆ†å¸ƒå˜åŒ–ã€‚æ²¡æœ‰è¿™ä¸€æ­¥ï¼Œæ•™å¸ˆæ¨¡åž‹åœ¨è’¸é¦è¿‡ç¨‹ä¸­å¯èƒ½æ— æ³•ä¸ºå­¦ç”Ÿæä¾›æœ€ä½³æŒ‡å¯¼ã€‚\n\n### æ·±åº¦å‰ªæž\n\nä¸ºäº†å°†8Bæ¨¡åž‹å‡å°‘åˆ°4Bï¼ŒMinitronå‰ªé™¤äº†16å±‚ï¼Œé€šè¿‡é€ä¸€ç§»é™¤å®ƒä»¬å¹¶è·Ÿè¸ªå¯¹æ€§èƒ½çš„å½±å“æ¥è¯„ä¼°å®ƒä»¬çš„é‡è¦æ€§ã€‚ä»–ä»¬å‘çŽ°æ¨¡åž‹å¼€å§‹å’Œç»“æŸçš„å±‚å¯¹ä¿æŒå‡†ç¡®æ€§æœ€ä¸ºå…³é”®ã€‚åŸºäºŽè¿™ä¸€åˆ†æžï¼ŒMinitronä¸ºæœ€ç»ˆçš„4Bæ¨¡åž‹ç§»é™¤äº†ç‰¹å®šçš„ä¸€ç»„å±‚ã€‚\n\n### å®½åº¦ä¿®å‰ª\n\né™¤äº†æ·±åº¦ä¿®å‰ªï¼ŒMinitron è¿˜åœ¨å®½åº¦ç»´åº¦ä¸Šè¿›è¡Œäº†ä¿®å‰ªï¼Œç›®æ ‡æ˜¯æ³¨æ„åŠ›å¤´ã€åµŒå…¥é€šé“å’Œéšè—å±‚ã€‚ä¿®å‰ªåŽï¼Œé‡æ–°è®­ç»ƒå¸®åŠ©æ¢å¤äº†åœ¨åˆå§‹ä¿®å‰ªæ­¥éª¤ä¸­ä¸¢å¤±çš„ä¸€äº›æ€§èƒ½ã€‚æœ‰è¶£çš„æ˜¯ï¼Œå°½ç®¡å®½åº¦ä¿®å‰ªæœ€åˆå¯¼è‡´çš„æŸå¤±é«˜äºŽæ·±åº¦ä¿®å‰ªï¼Œä½†é‡æ–°è®­ç»ƒä½¿æ¨¡åž‹èƒ½å¤Ÿéšç€æ—¶é—´çš„æŽ¨ç§»æ›´æœ‰æ•ˆåœ°æ¢å¤ã€‚\n\n## ç»“æžœ\n\nNVIDIA åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¯„ä¼°äº† Minitron æ¨¡åž‹ï¼Œç»“æžœä¸ŽåŸºå‡†æ¨¡åž‹çš„æ€§èƒ½ç›¸åŒ¹é…ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*tVGs8v5FZHsWrpmMetDYHQ.png)\n\nMinitron 4B\\-8B å±•ç¤ºäº†è’¸é¦å’Œå‰ªæžæž„å»ºæ›´å°ã€æ›´é«˜æ•ˆæ¨¡åž‹çš„æ½œåŠ›ã€‚å°½ç®¡è¿™ç§æ–¹æ³•ä¹Ÿé¢ä¸´ç€é‡å¤§æŒ‘æˆ˜ï¼Œä½†æˆ‘è®¤ä¸ºï¼Œæ€»ä½“è€Œè¨€ï¼Œå®ƒä¸ºè¡Œä¸šè®¾å®šäº†ä¸€ä¸ªé‡è¦çš„åŸºå‡†ã€‚\n\n"},{"lang":"zh","group":"blog","slug":"blog/how-to-choose-ideas-for-an-llm-powered-product-to-thrive-in-a-fiercely-competitive-landscape-b24f571c04e5","frontmatter":{"title":"å¦‚ä½•é€‰æ‹© LLM é©±åŠ¨çš„äº§å“åˆ›æ„ï¼Œä»¥åœ¨æ¿€çƒˆçš„ç«žäº‰çŽ¯å¢ƒä¸­è“¬å‹ƒå‘å±•","meta_title":"å¦‚ä½•é€‰æ‹© LLM é©±åŠ¨çš„äº§å“åˆ›æ„ï¼Œä»¥åœ¨æ¿€çƒˆçš„ç«žäº‰çŽ¯å¢ƒä¸­è“¬å‹ƒå‘å±•","description":"åˆ©ç”¨ä¸æ˜Žæ˜¾çš„ AI èƒ½åŠ›ã€æ·±åŽšçš„é¢†åŸŸä¸“ä¸šçŸ¥è¯†ä»¥åŠå¦å¤– 9 ç§æ–¹æ³•è®©å°æ–°äº§å“èŽ·å¾—ç«žäº‰ä¼˜åŠ¿","date":"2024-11-10T03:51:17.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MAmCClj129C56jmkiqqhQQ.png","categories":["Generative AI","Product Development","Technology/Web"],"author":"Rifx.Online","tags":["LLM","development","experimentation","domain","expertise"],"draft":false,"slug":"blog/how-to-choose-ideas-for-an-llm-powered-product-to-thrive-in-a-fiercely-competitive-landscape-b24f571c04e5"},"content":"\n\n\næ¬¢è¿Žæ¥åˆ°æˆ‘ç³»åˆ—æ–‡ç« çš„ç¬¬ä¸‰ç¯‡ï¼ˆæœ€åŽä¸€ç¯‡ï¼‰ï¼ŒæŽ¢è®¨çš„é—®é¢˜æ˜¯ï¼šâ€œå“ªäº› GenAI äº§å“å€¼å¾—å¼€å‘ï¼Ÿâ€\n\n1. [ç¬¬ä¸€ç¯‡æ–‡ç« ](https://readmedium.com/what-llm-powered-products-are-worth-developing-ux-and-adoption-perspectives-d9efcf444d50) ä»Žç”¨æˆ·ä½“éªŒï¼ˆUXï¼‰å’Œäº§å“é‡‡ç”¨çš„è§’åº¦æŽ¢è®¨äº†è¿™ä¸ªé—®é¢˜ã€‚\n2. ç¬¬äºŒç¯‡æ–‡ç« ï¼Œæˆ‘å¼ºçƒˆå»ºè®®åœ¨é˜…è¯»æ­¤æ–‡ä¹‹å‰å…ˆé˜…è¯»ï¼ŒåŒ…å«äº†å…­ä¸ªæˆåŠŸå’Œä¸æˆåŠŸçš„äº§å“åˆ›æ„ç¤ºä¾‹ï¼Œä»¥åŠæˆ‘çš„ GenAI Squared æˆ˜ç•¥ï¼š\n\n3\\. è¿™ç¬¬ä¸‰ç¯‡ç»§ç»­å…³æ³¨å¦‚ä½•åœ¨ç«žäº‰çŽ¯å¢ƒä¸­å¯¼èˆªï¼Œä»¥åŠå¦‚ä½•ä¼˜åŒ–å¼€å‘æˆæœ¬è€Œä¸å¤±åŽ»ç«žäº‰ä¼˜åŠ¿ã€‚å°½ç®¡è¿™ç¯‡æ–‡ç« çš„ç¤ºä¾‹æ¯”å‰ä¸€ç¯‡å°‘ï¼Œä½†è¿™é‡Œè®¨è®ºçš„å› ç´ å¯¹ GenAI äº§å“é¢†åŸŸçš„æˆåŠŸè‡³å…³é‡è¦ã€‚\n\nè¿™ä¸‰ç¯‡æ–‡ç« **æ²¡æœ‰**æ¶µç›–åŸºäºŽ LLM çš„åº”ç”¨å¼€å‘çš„æŠ€æœ¯ç»†èŠ‚ã€‚æ­¤å¤–ï¼Œæˆ‘çš„åˆ†æž**ä¸**ä¾§é‡äºŽåˆ›æ–°äº§å“çš„å¸¸è§„æˆåŠŸå› ç´ ï¼Œä¾‹å¦‚åœ¨[é‚£ç¯‡æ–‡ç« ](https://pakodas.substack.com/p/llm-chronicles-6-how-to-build-competitive)ä¸­æè¿°çš„å› ç´ ã€‚\n\n> *ç›¸åï¼Œä½œä¸ºä¸€åäº§å“ç»ç†ï¼Œæˆ‘åˆ†æž LLM ä½œä¸ºæˆ‘äº§å“å¹³å°çš„**ç‹¬ç‰¹ç‰¹æ€§**ã€‚è¿™ç§æ–¹æ³•ä¸ºåˆ©ç”¨ä¸æ˜Žæ˜¾çš„ AI èƒ½åŠ›åœ¨äº§å“å¼€å‘ä¸­æä¾›äº†æ–°é²œçš„è§è§£ã€‚*\n\nå…·ä½“æ¥è¯´ï¼Œåœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘æŽ¢è®¨ä»¥ä¸‹å…³äºŽè½¯ä»¶äº§å“çš„é—®é¢˜ï¼š\n\n* ä¸ºä»€ä¹ˆç”Ÿæˆæ€§ AI äº§å“æ›´å®¹æ˜“åœ¨äº§ç”Ÿå›žæŠ¥ä¹‹å‰å˜å¾—è¿‡æ—¶ï¼Ÿ\n* æˆ‘ä»¬å¦‚ä½•å°†è¿™äº› GenAI æŒ‘æˆ˜è½¬åŒ–ä¸ºç«žäº‰ä¼˜åŠ¿ï¼Ÿ\n* å“ªäº› LLM èƒ½åŠ›çœŸæ­£å¢žå¼ºäº†äº§å“çš„ç«žäº‰åŠ›ï¼Œè€Œå“ªäº›åˆ™æ²¡æœ‰å¤ªå¤§æ„ä¹‰ï¼Ÿ\n* å½“æ–°çš„ AI äº§å“å‡ ä¹Žæ²¡æœ‰ä»£ç æ—¶ï¼Œå¦‚ä½•ä½¿å…¶è„±é¢–è€Œå‡ºï¼Œå› æ­¤ä¼˜ç§€çš„ç¨‹åºå‘˜å›¢é˜Ÿä¸å†æ˜¯å…³é”®æˆåŠŸå› ç´ ï¼Ÿ\n* åœ¨è¿™ä¸ªæ–°çŽ¯å¢ƒä¸­ï¼ŒAI äº§å“å¼€å‘è€…æœ€é‡è¦çš„æŠ€èƒ½æ˜¯ä»€ä¹ˆï¼Ÿ\n\nè¿™äº›è§è§£æ—¨åœ¨æŒ‡å¯¼äº§å“ç»ç†å’Œåˆ›å§‹äººåœ¨åšå‡ºå†³ç­–æ—¶ã€‚\n\né‚£ä¹ˆï¼Œå“ªäº› AI åº”ç”¨å¯èƒ½æ˜¯å¤šä½™çš„æˆ–æ³¨å®šè¦å¤±è´¥ ðŸš«ï¼Œè€Œå“ªäº›åˆ™æœ‰å¾ˆé«˜çš„æˆåŠŸæœºä¼š âœ…ï¼Ÿ\n\n*è¯·æ³¨æ„ï¼Œä¸‹é¢çš„ç« èŠ‚ç¼–å·ç»§ç»­å‰ä¸¤ç¯‡æ–‡ç« çš„ç« èŠ‚ç¼–å·ã€‚æ‰€æœ‰ 11 ä¸ªè¦ç‚¹å°†åœ¨æœ¬æ–‡æœ«å°¾æ€»ç»“ã€‚*\n\n## 9\\. å¤§åž‹åº”ç”¨ç¨‹åºçš„å¼€å‘å‘¨æœŸé•¿ä¸”å¸‚åœºé‡‡çº³æ—¶é—´æ¼«é•¿ï¼Œç«žäº‰åŠ›ä¸è¶³ ðŸš«\n\nç”Ÿæˆå¼äººå·¥æ™ºèƒ½ä»¥ç©ºå‰çš„é€Ÿåº¦å‘å±•ï¼Œè¶…è¿‡äº†ä»»ä½•å…ˆå‰æŠ€æœ¯çš„å¢žé•¿ã€‚äººå·¥æ™ºèƒ½èƒ½åŠ›ç¿»å€æ‰€éœ€çš„æ—¶é—´å¤§çº¦ä¸ºä¸€å¹´ï¼Œè¿™ä¸Žè‘—åçš„æ‘©å°”å®šå¾‹ä¸­æè¿°çš„ä¸¤å¹´å½¢æˆå¯¹æ¯”ã€‚\n\nå› æ­¤ï¼ŒåŸºäºŽç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„äº§å“æ— æ³•æ‰¿å—é•¿æ—¶é—´çš„å¼€å‘å‘¨æœŸå’Œå»¶é•¿çš„å¸‚åœºæŠ•æ”¾æ—¶é—´ã€‚è¿™å¸¦æ¥äº†ä¸‰ä¸ªä¸»è¦åŽæžœã€‚\n\n### 1\\. æ–°åŠŸèƒ½åº”è¯¥ç®€æ´ä¸”ä¸“æ³¨ï¼Œèƒ½å¤Ÿåœ¨å‡ å‘¨å†…è€Œä¸æ˜¯å‡ ä¸ªæœˆå†…å¼€å‘å®Œæˆã€‚\n\nè¿™ç§æ–¹æ³•å…è®¸æ ¹æ®åˆæ­¥ç”¨æˆ·åé¦ˆè¿›è¡Œå¿«é€Ÿè°ƒæ•´ï¼Œå¯èƒ½å¯¼è‡´åŠŸèƒ½çš„é‡å¤§å˜åŒ–ã€‚æ­¤å¤–ï¼Œå½“éœ€è¦è½¬å˜æ–¹å‘æ—¶ï¼ˆè¿™è‚¯å®šä¼šå‘ç”Ÿï¼‰ï¼Œåœ¨æ”¾å¼ƒè¿™äº›æ—©æœŸå‡ å‘¨å¼€å‘çš„åŠŸèƒ½æ—¶ï¼ŒæŸå¤±çš„æˆæœ¬è¾ƒå°‘ã€‚\n\nä¾‹å¦‚ï¼Œè€ƒè™‘åŸºäºŽLLMçš„MVPçš„ç”¨æˆ·ç•Œé¢ã€‚å¦‚æžœç”¨æˆ·å¯ä»¥é€šè¿‡Telegramæœºå™¨äººæˆ–ç±»ä¼¼å·¥å…·å®žçŽ°ç›¸åŒçš„ç»“æžœï¼Œé‚£ä¹ˆå¼€å‘è‡ªå®šä¹‰ç½‘é¡µç•Œé¢å¯èƒ½æ˜¯æ²¡æœ‰å¿…è¦çš„ã€‚\n\n*ç„¶è€Œï¼Œå¦‚æžœæˆ‘ä»¬[å°†LLMèžå…¥çŽ°æœ‰è§£å†³æ–¹æ¡ˆæˆ–ä¸Žå…¶é›†æˆ](https://readmedium.com/what-llm-powered-products-are-worth-developing-ux-and-adoption-perspectives-d9efcf444d50#5d06)ï¼Œé‚£ä¹ˆâ€œæ•´ä½“äº§å“â€å¯ä»¥å…·æœ‰å¹¿æ³›çš„åŠŸèƒ½ã€‚å…³é”®åœ¨äºŽä»…æœ€å°åŒ–**æ–°**åŠŸèƒ½çš„èŒƒå›´ã€‚*\n\n### 2\\. å¯¹äºŽè¶…å¿«é€Ÿå®žéªŒå’Œå®¢æˆ·åé¦ˆå¾ªçŽ¯çš„éœ€æ±‚è‡³å…³é‡è¦ã€‚\n\nGenAI äº§å“çš„æž„å»ºé€Ÿåº¦æ›´å¿«ï¼Œä½†å¹¶ä¸æ€»æ˜¯èƒ½å¤ŸåŒæ ·å¿«é€Ÿåœ°èŽ·å¾—åé¦ˆã€‚å› æ­¤ï¼Œä¸€äº› GenAI äº§å“æ¦‚å¿µå¯èƒ½è¢«è¯æ˜Žé£Žé™©è¿‡é«˜ã€‚\n\nå¿«é€Ÿå®žéªŒå½“ç„¶å¯¹ä»»ä½•æ–°äº§å“å‘å¸ƒéƒ½æ˜¯æœ‰ç›Šçš„ï¼Œå› ä¸ºæ— æ³•æå‰å‡†ç¡®é¢„æµ‹å¸‚åœºååº”ã€‚æœ¬è´¨ä¸Šï¼Œå¸‚åœºè¿ä½œå¦‚åŒä¸€ä¸ªâ€œé»‘ç®±â€ï¼Œå…¶è¡Œä¸ºåªèƒ½é€šè¿‡å®žé™…æ“ä½œçš„å®žéªŒæ¥çœŸæ­£ç†è§£ã€‚\n\n> *åœ¨ GenAI äº§å“é¢†åŸŸï¼Œæˆ‘ä»¬é‡åˆ°äº†é¢å¤–çš„å¤æ‚æ€§â€”â€”**ç¬¬äºŒä¸ªâ€œé»‘ç®±â€**æºäºŽ LLM è¾“å‡ºçš„å›ºæœ‰ä¸å¯é¢„æµ‹æ€§ã€‚**è¿™ç§åŒé‡ä¸ç¡®å®šæ€§åŠ å¤§äº†é¢‘ç¹å’Œå¿«é€Ÿå®žéªŒçš„é‡è¦æ€§ã€‚**å¿«é€Ÿè¿­ä»£å’Œæ”¶é›†æ´žå¯Ÿçš„èƒ½åŠ›ä¸ä»…æ˜¯æœ‰åˆ©çš„ï¼Œè€Œæ˜¯æˆåŠŸçš„å¿…è¦æ¡ä»¶ã€‚*\n\n### 3\\. æ²¡æœ‰æ—¶é—´åŽ»â€œæ•™è‚²â€äº§å“çš„ç›®æ ‡å—ä¼—ï¼Œä½¿å…¶é€‚åº”å®Œå…¨æ–°çš„å·¥ä½œæˆ–ä¼‘é—²æ¨¡å¼ã€‚\n\nåªæœ‰æœ€å¤§çš„è¡Œä¸šé¢†å¯¼è€…ï¼Œç‰¹åˆ«æ˜¯é‚£äº›æ‹¥æœ‰è‡ªå·±ç”Ÿæ€ç³»ç»Ÿçš„å…¬å¸ï¼Œå¦‚ Googleã€Apple æˆ– Microsoftï¼Œæ‰èƒ½ç›¸å¯¹å¿«é€Ÿåœ°è®© **å¤§å¤šæ•°** æ½œåœ¨ç”¨æˆ·ä¹ æƒ¯äºŽæ–°æ¦‚å¿µã€‚\n\nâœ… å› æ­¤ï¼Œå…¶ä»–å…¬å¸å¿…é¡»ä¸Ž **ç”¨æˆ·ç†Ÿæ‚‰çš„çŽ°æœ‰ç›®æ ‡è¾¾æˆæ¨¡å¼æˆ–è¡Œä¸šé¢†å¯¼è€…å»ºç«‹çš„è¶‹åŠ¿** å¯¹é½ã€‚\n\n* è€ƒè™‘ä¸€ä¸ªå·²å»ºç«‹çš„å¢žåŠ æ”¶å…¥çš„ç›®æ ‡æ¨¡å¼ï¼šäººä»¬è´­ä¹°åŸ¹è®­è¯¾ç¨‹ä»¥èŽ·å¾—æ–°æŠ€èƒ½ã€‚è¿™ä¸ªé¢†åŸŸä¸­çš„ä¸€ä¸ªä¼˜ç§€ AI é©±åŠ¨è§£å†³æ–¹æ¡ˆæ¶‰åŠä½¿ç”¨ AI åˆ›å»ºè¿™äº›è¯¾ç¨‹ï¼Œæ˜¾è‘—é™ä½Žç”Ÿäº§æˆæœ¬ï¼Œä»Žè€Œå¢žå¼ºç«žäº‰åŠ›ã€‚**æœ€ç»ˆç”¨æˆ·ä¸éœ€è¦** é‡‡å–ä»»ä½•æ–°è¡Œä¸ºæ¥æå‡ä»–ä»¬çš„æ”¶å…¥ã€‚\n* æœ€è¿‘åœ¨ Apple è®¾å¤‡ä¸­å‡ºçŽ°çš„ **è¶‹åŠ¿** ä½“çŽ°äº†ä¸€é¡¹åˆ›æ–°ï¼ŒApple å¹³å°ç”¨æˆ·æ— ç–‘ä¼šé‡‡ç”¨ï¼šä½¿ç”¨æœ¬åœ° LLM è¿›è¡Œå…¸åž‹ä»»åŠ¡ä»¥ä¿æŠ¤ç”¨æˆ·æ•°æ®éšç§ã€‚è™½ç„¶åº”ç”¨ç¨‹åºå¯èƒ½åˆ©ç”¨è¿™ä¸€è¶‹åŠ¿çš„å…·ä½“æ–¹å¼å°šä¸æ¸…æ¥šï¼Œä½†æˆ‘ç›¸ä¿¡ Apple ä¼šä¸ºå¼€å‘è€…æä¾›ä¾¿æ·çš„ LLM åŸºç¡€è®¾æ–½è®¿é—®ï¼Œæˆ‘ä»¬åªéœ€å†ç­‰ä¸€æ®µæ—¶é—´ã€‚\n\n## 10\\. åˆ©ç”¨ä¸å¤ªæ˜Žæ˜¾çš„LLMèƒ½åŠ›æå‡ç«žäº‰åŠ›å’Œèµ„æºæ•ˆçŽ‡ âœ…\n\næƒ³è±¡ä¸€ä¸‹ï¼Œä½ å¤„äºŽèµ·ç‚¹ï¼Œæ‰‹ä¸­åªæœ‰ä¸€ä¸ªäº§å“æ¦‚å¿µã€‚ä¸ºäº†åŠ å¿«å‘é«˜éœ€æ±‚äº§å“çš„æ—…ç¨‹ï¼Œ**ä½ åº”è¯¥ä¼˜å…ˆæŽ¢ç´¢å“ªäº›æ–¹é¢çš„æƒ³æ³•ï¼Ÿ**\n\næ˜¾ç„¶ï¼Œä½ éœ€è¦åœ¨ä½ çš„æ¦‚å¿µä¸­è¯†åˆ«ä¸€å°ç»„ç‰¹å®šçš„ç«¯åˆ°ç«¯å·¥ä½œåœºæ™¯ã€‚è¿™ä¸Ž[æµè¡Œçš„äº§å“å‘å¸ƒç­–ç•¥](https://www.geeksforgeeks.org/a-complete-guide-to-a-successful-product-launch/#is-there-a-product-launch-formula)ä¸€è‡´ï¼šâ€œä»ŽMVPå¼€å§‹â€ï¼ˆå®žæ–½ä¸€ä¸ªæˆ–å‡ ä¸ªåœºæ™¯ï¼‰å’Œâ€œä¸º**æ•´ä¸ª**ç”¨æˆ·ä½“éªŒæž„å»ºâ€ï¼ˆç¡®ä¿åœºæ™¯æ˜¯ç«¯åˆ°ç«¯çš„ï¼‰ã€‚é—®é¢˜æ˜¯ï¼šä½ åº”è¯¥é€‰æ‹©å“ªäº›åœºæ™¯ï¼Ÿ\n\n> *åœ¨æˆ‘çœ‹æ¥ï¼Œè¿™äº›MVPåœºæ™¯åº”è¯¥ä¸ŽLLMèƒ½åŠ›**ç´§å¯†å¯¹é½**ã€‚è¿™ç§æ–¹æ³•èŠ‚çœäº†äº§å“äº¤ä»˜çš„èµ„æºï¼Œå› ä¸ºæ˜¾è‘—çš„äº§å“ä»·å€¼æ¥è‡ªäºŽLLMæœ¬èº«ï¼Œè€Œä¸ä»…ä»…æ˜¯ä½ å¼€å‘è€…çš„åŠªåŠ›ã€‚å¦‚æžœä¸è¿™æ ·åšï¼Œå¯èƒ½ä¼šé¢ä¸´[ç¬¬7èŠ‚â€œè¿‡åº¦é™åˆ¶LLMâ€](https://readmedium.com/genai-squared-how-can-a-product-avoid-the-downfall-of-most-llm-driven-startups-183619ab7883#e1ef)ä¸­æ¦‚è¿°çš„æŒ‘æˆ˜ã€‚*\n\nðŸš« LLMæ‰€å®£ç§°çš„è¶…èƒ½åŠ›é€šå¸¸åŒ…æ‹¬å…¶**å›žç­”ä»»ä½•é—®é¢˜çš„èƒ½åŠ›**ã€‚ç„¶è€Œï¼Œè¿™äº›å›žç­”çš„å‡†ç¡®æ€§å’Œè´¨é‡æœ¬è´¨ä¸Šæ˜¯ä¸å¯é¢„æµ‹çš„ï¼Œè¿™ä¼šå¯¼è‡´é—®é¢˜ï¼ˆæœ‰å…³è¯„ä¼°å¤æ‚æ€§å’Œè´¨é‡ç›‘æŽ§çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§[ç¬¬1èŠ‚](https://ai.gopubby.com/what-llm-powered-products-are-worth-developing-ux-and-adoption-perspectives-d9efcf444d50#f973)ï¼‰ã€‚æ­¤å¤–ï¼Œå›´ç»•é—®ç­”ä¸­å¿ƒçš„äº§å“æ— æ³•æœ‰æ•ˆåœ°ä¸Žå¸‚åœºé¢†å¯¼è€…å¦‚ChatGPTç«žäº‰ï¼ˆå¦‚[ç¬¬6èŠ‚](https://readmedium.com/genai-squared-how-can-a-product-avoid-the-downfall-of-most-llm-driven-startups-183619ab7883#e305)ä¸­è®¨è®ºçš„ï¼‰ã€‚è€ƒè™‘åˆ°è¿™ä¸¤ä¸ªå› ç´ ï¼Œæˆ‘å»ºè®®ä¸è¦åŸºäºŽè¿™ç§â€œè¶…èƒ½åŠ›â€æ¥æž„å»ºMVPã€‚\n\n**LLMçš„â€œæƒ³è±¡ç”Ÿæˆâ€èƒ½åŠ›**æä¾›äº†ä¸€ä¸ªç›¸å¯¹æ›´æœ‰å‰æ™¯çš„æ–¹å‘ã€‚è¿™ç§LLMçš„åˆ›é€ åŠ›å¯ä»¥æ¿€å‘æˆ‘ä»¬çš„æ–°æƒ³æ³•ï¼Œæˆ–å¸®åŠ©åˆ›å»ºåˆ›æ„å†…å®¹ï¼Œå¦‚è¯—æ­Œã€è§†é¢‘è„šæœ¬æˆ–å†…å®¹è®¡åˆ’ã€‚ç„¶è€Œï¼Œæ ¹æ®æˆ‘çš„ç»éªŒï¼Œå•é LLMçš„åˆ›é€ åŠ›ä¸è¶³ä»¥æž„å»ºç«¯åˆ°ç«¯çš„äº§å“åœºæ™¯ã€‚ä¸€æ—¦ç”¨æˆ·ä»ŽLLMèŽ·å¾—â€œåˆ›æ„ææ–™â€ï¼Œä»ç„¶éœ€è¦å¤§é‡çš„åŠªåŠ›å°†å…¶è½¬åŒ–ä¸ºæ‰€éœ€çš„ç»“æžœã€‚\n\næ­¤å¤–ï¼Œåˆ›é€ åŠ›ä»£è¡¨äº†GenAIæœ€å®¹æ˜“ç†è§£å’Œå¹¿æ³›è®¤å¯çš„èƒ½åŠ›ä¹‹ä¸€ã€‚å‡ ä¹Žä»»ä½•å°è¯•è¿‡ChatGPTæˆ–Midjourneyçš„äººéƒ½å¯¹å…¶æ„Ÿåˆ°ç†Ÿæ‚‰ï¼Œå› æ­¤ä»»ä½•äººéƒ½å¯èƒ½æˆä¸ºä½ çš„ç«žäº‰å¯¹æ‰‹ã€‚\n\n\n\nâœ… è€ƒè™‘åˆ°æ¿€çƒˆçš„ç«žäº‰ï¼Œæˆ‘å»ºè®®ä¸“æ³¨äºŽ**LLMçš„ä¸å¤ªæ˜Žæ˜¾çš„èƒ½åŠ›ï¼Œ**ä¾‹å¦‚ï¼š\n\n### 1\\. ç¿»è½¬äº¤äº’\n\nè¿™ç§äººç±»ä¸ŽAIçš„äº¤äº’æ¨¡å¼åˆ©ç”¨äº†LLMçš„èƒ½åŠ›ï¼Œ**æå‡ºå¥½çš„é—®é¢˜**æˆ–å‘ˆçŽ°é€‰æ‹©ç”¨æˆ·é‡è¦é¡¹ç›®çš„åˆ—è¡¨ï¼Œä»Žè€Œå‡å°‘ç”¨æˆ·çš„è®¤çŸ¥è´Ÿæ‹…ã€‚ç¿»è½¬äº¤äº’ä¸ä»…æœ‰åŠ©äºŽåœ¨æŸäº›é¢†åŸŸï¼ˆå¦‚æ•™å­¦ã€æŒ‡å¯¼æˆ–è¾…å¯¼ï¼‰æ›¿ä»£éƒ¨åˆ†äººç±»å·¥ä½œï¼Œè¿˜å¸®åŠ©åœ¨ä»»ä½•é¢†åŸŸå»ºç«‹è§£å†³é—®é¢˜çš„é€‚å½“**èƒŒæ™¯**ï¼ˆæ›´å¤šç»†èŠ‚è¯·è§[è¿™é‡Œ](https://readmedium.com/4-human-ai-interaction-patterns-for-experienced-chatgpt-users-9e49d4234013#c348)ï¼‰ã€‚\n\n### 2\\. ä¸Šä¸‹æ–‡ç†è§£\n\nLLM æ“…é•¿æŠŠæ¡ç”¨æˆ·è¯·æ±‚åŠå…¶åå¥½çš„ä¸Šä¸‹æ–‡ï¼Œç„¶åŽåœ¨è¯¥ä¸Šä¸‹æ–‡ä¸­å¤„ç†ä»»åŠ¡ã€‚è¿™ç§æ–¹æ³•ç¡®ä¿è§£å†³æ–¹æ¡ˆä¸Žå³ä½¿æ˜¯ **æœªè¡¨è¿°** çš„ç”¨æˆ·éœ€æ±‚ä¿æŒä¸€è‡´ã€‚\n\na. è¿™ä¸€ç‰¹æ€§åœ¨å¼€å‘è€…çš„ AI åŠ©æ‰‹ä¸­å¾—åˆ°äº†æœ€ç²¾ç»†çš„ä½“çŽ°ï¼Œä¾‹å¦‚ Github Copilot å’Œ Cursorã€‚åœ¨è¿™äº›å·¥å…·ä¸­ï¼ŒLLM çš„ä¸Šä¸‹æ–‡æ¶µç›–æ•´ä¸ªé¡¹ç›®ä»£ç åº“ï¼Œè€Œç”¨æˆ·ï¼ˆå¼€å‘è€…ï¼‰é€šå¸¸åªäº†è§£ç‰¹å®šéƒ¨åˆ†ã€‚å› æ­¤ï¼Œå¼€å‘è€…åœ¨ä¸º AI åˆ¶å®šä»»åŠ¡æ—¶ï¼Œå¾€å¾€æ— æ³•è€ƒè™‘æ›´å¹¿æ³›çš„ä¸Šä¸‹æ–‡ã€‚\n\nb. ç„¶è€Œï¼Œåœ¨ä¸Šä¸‹æ–‡ä¸­åˆ©ç”¨ **æ˜Žç¡®è¡¨è¿°** çš„ç”¨æˆ·éœ€æ±‚çš„æ´žå¯Ÿä¹Ÿæ˜¯ä¸€ä¸ªå¼ºå¤§çš„ç‰¹æ€§ã€‚ä¾‹å¦‚ï¼Œè¯­è¨€å­¦ä¹ å¹³å° [Memrise](https://www.memrise.com/) æœ‰æ•ˆåœ°å®žçŽ°äº†è¿™ä¸€ç‰¹æ€§ã€‚\n\n### 3\\. å°‘é‡å­¦ä¹ \n\næ¨¡åž‹ä»Ž**å°‘é‡**ç¤ºä¾‹ä¸­â€œå­¦ä¹ â€çš„èƒ½åŠ›ä½¿å…¶èƒ½å¤Ÿè½»æ¾é€‚åº”æ–°ä»»åŠ¡å’Œæ–°çŽ¯å¢ƒã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆåŸºäºŽLLMçš„èŠå¤©æœºå™¨äººçŽ°åœ¨è¢«å¹¿æ³›åº”ç”¨äºŽé”€å”®å’Œå®¢æˆ·æ”¯æŒï¼Œä¸Žå®ƒä»¬çš„å¯¹è¯å¾ˆéš¾ä¸Žäººç±»ä¸“å®¶çš„å¯¹è¯åŒºåˆ†å¼€æ¥ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä¼ ç»Ÿçš„AIèŠå¤©æœºå™¨äººä»…åœ¨å¤§åž‹ä¼ä¸šä¸­è¡¨çŽ°è‰¯å¥½ï¼Œéš¾ä»¥é€‚åº”ä¸æ–­å˜åŒ–çš„çŸ¥è¯†åº“ã€‚\n\n### 4\\. å¤§è§„æ¨¡ä¿¡æ¯å¤„ç†\n\nLLM æ“…é•¿åˆ†æž **å¤§é‡** çš„æ–‡æœ¬å’Œè¡¨æ ¼æ•°æ®ï¼Œå°†å…¶æç‚¼æˆ **ç®€æ˜Ž** çš„å½¢å¼ã€‚å®ƒèƒ½å¤Ÿæ¦‚æ‹¬ã€æå–ä¸Žå½“å‰ä»»åŠ¡ç›¸å…³çš„å…³é”®ç‚¹ã€è¯†åˆ«æ¨¡å¼ï¼Œå¹¶æ‰§è¡Œå„ç§å…¶ä»–åˆ†æžåŠŸèƒ½ã€‚\n\na. ä»¥ [Scite](https://scite.ai/) ä¸ºä¾‹ï¼Œè¿™æ˜¯ä¸€æ¬¾ç”¨äºŽç§‘å­¦ç ”ç©¶çš„ AI å·¥å…·ã€‚å®ƒä¸ä»…ä»…æ˜¯åœ¨å…¶åäº¿æ¡å¼•ç”¨æ•°æ®åº“ä¸­å®šä½ä¸ŽæŸ¥è¯¢ç›¸å…³çš„æ¥æºã€‚Scite åˆ†æžæ–‡ç« è¢«å¼•ç”¨çš„ä¸Šä¸‹æ–‡ï¼Œæ­ç¤ºå¼•ç”¨è®ºæ–‡æ˜¯æ”¯æŒã€åé©³è¿˜æ˜¯ä»…ä»…æåˆ°æ—©æœŸå·¥ä½œçš„ã€‚\n\nb. åœ¨æ•°å€¼æ•°æ®å¤„ç†æ–¹é¢ï¼ŒLLM çš„è¾“å‡ºä¸éœ€è¦â€œç¿»è¯‘æˆäººç±»è¯­è¨€â€ã€‚è¿™ä¸º GenAI åˆ†æžå·¥å…·æä¾›äº†ç›¸è¾ƒäºŽä¼ ç»Ÿç»Ÿè®¡æ•°æ®å¤„ç†å·¥å…·çš„æ˜Žæ˜¾ä¼˜åŠ¿ã€‚\n\nè®¸å¤šæ½œåœ¨ç«žäº‰è€…å¯èƒ½äº†è§£è¿™å››ç§ LLM èƒ½åŠ›ä¸­çš„ä¸€äº›ã€‚ç„¶è€Œï¼Œæˆ‘ç›¸ä¿¡å¯¹è¿™äº›èƒ½åŠ›çš„æ·±å…¥æ€è€ƒå¯èƒ½ä¼šå¯¼è‡´çœŸæ­£åˆ›æ–°äº§å“çš„å¼€å‘ã€‚è¿™ç§æ–¹æ³•å¯ä»¥æä¾›ç›¸è¾ƒäºŽä»…åˆ©ç”¨ LLM æ›´æ˜Žæ˜¾èƒ½åŠ›ï¼ˆå¦‚â€œåˆ›é€ åŠ›â€å’Œâ€œå›žç­”ä»»ä½•é—®é¢˜â€ï¼‰çš„äº§å“çš„ç«žäº‰ä¼˜åŠ¿ã€‚\n\n## 11\\. åŸºäºŽæ·±åŽšé¢†åŸŸä¸“ä¸šçŸ¥è¯†çš„å°åž‹AIäº§å“å…·æœ‰ç«žäº‰åŠ› âœ…\n\nLLMå‡ ä¹Žä½œä¸ºä¸€ä¸ªå®Œæˆçš„äº§å“ï¼Œèƒ½å¤Ÿâ€œè‡ªä¸»â€ä¸Žç”¨æˆ·äº’åŠ¨ã€‚å› æ­¤ï¼ŒåŸºäºŽLLMçš„åº”ç”¨ç¨‹åºåœ¨ä»£ç åŸºç¡€ä¸Šæ˜¾è‘—å°äºŽä¼ ç»Ÿçš„éžLLMåº”ç”¨ç¨‹åºã€‚\n\næ­¤å¤–ï¼Œä»»ä½•å…·å¤‡ä¸€å®šæŠ€æœ¯æŠ€èƒ½çš„ä¸ªäººéƒ½å¯ä»¥åœ¨å‡ å¤©å†…å­¦ä¹ å¼€å‘åŠŸèƒ½ä¸°å¯Œçš„åŸºäºŽLLMçš„åº”ç”¨ç¨‹åºã€‚\n\nè¿™ä¸¤ä¸ªå› ç´ ä¸Žç¬¬9èŠ‚ä¸­æ¦‚è¿°çš„å¿«é€Ÿå‘å±•å’Œå®žéªŒè¦æ±‚å®Œç¾Žå¥‘åˆã€‚\n\n> *ç„¶è€Œï¼Œä»Žç«žäº‰çš„è§’åº¦æ¥çœ‹ï¼Œäº§å“çš„å°è§„æ¨¡å’ŒGenAIå¼€å‘çš„ä½Žå‡†å…¥é—¨æ§›æ˜¯**æ˜¾è‘—çš„ç¼ºé™·**ã€‚*\n\nå¯¹äºŽå…¸åž‹çš„å¤§åž‹ä»£ç åŸºç¡€è½¯ä»¶äº§å“ï¼Œå“è¶Šçš„å›¢é˜Ÿå’Œæ•æ·å¼€å‘æµç¨‹æ˜¯æˆåŠŸçš„å…³é”®è¦ç´ ã€‚[Bill Grossçš„ç ”ç©¶](https://youtu.be/bNpx7gpSqbY?t=216)å°†å…¶åˆ—ä¸ºäº”ä¸ªå› ç´ ä¸­ç¬¬äºŒé‡è¦çš„å› ç´ ï¼Œç”šè‡³è¶…è¿‡äº†äº§å“åˆ›æ„çš„å¯è¡Œæ€§ï¼ŒåŽè€…æŽ’åœ¨ç¬¬ä¸‰ä½ã€‚\n\nç„¶è€Œï¼Œå½“ä¸€ä¸ªäº§å“çš„è½¯ä»¶å¼€å‘èŒƒå›´å¾ˆå°ï¼Œç”šè‡³ç»éªŒä¸è¶³çš„ç¨‹åºå‘˜ä¹Ÿèƒ½å¼€å‘æ—¶ï¼Œäº§å“å¦‚ä½•èŽ·å¾—ç«žäº‰ä¼˜åŠ¿ï¼Ÿ\n\néšç€æƒ³æ³•å’Œå•†ä¸šæ¨¡åž‹å®¹æ˜“è¢«ç«žäº‰å¯¹æ‰‹å¤åˆ¶â€¦â€¦æˆåŠŸæ˜¯å¦çœŸçš„ä»…ä»…ä¾èµ–äºŽåœ¨ä½ çš„ç»†åˆ†å¸‚åœºä¸­é¦–å‘çš„çŸ­æœŸä¼˜åŠ¿ï¼Ÿ\n\n1. ç¬¬10èŠ‚å¯¹æ­¤é—®é¢˜æä¾›äº†ä¸€ä¸ªç­”æ¡ˆï¼šäº§å“åº”åˆ©ç”¨LLMä¸å¤ªä¸ºäººæ‰€çŸ¥çš„èƒ½åŠ›ã€‚è™½ç„¶è¿™å¹¶ä¸èƒ½ä¿è¯æˆåŠŸï¼Œä½†å®ƒå¢žåŠ äº†è¶…è¶Šå¯èƒ½ä¸å®Œå…¨ç†è§£LLMä¸æ˜Žæ˜¾èƒ½åŠ›çš„ç«žäº‰å¯¹æ‰‹çš„æœºä¼šã€‚\n2. [æˆ‘ä¹‹å‰çš„æ–‡ç« ](https://readmedium.com/genai-squared-how-can-a-product-avoid-the-downfall-of-most-llm-driven-startups-183619ab7883#9f01)æ¦‚è¿°äº†å¦ä¸€ä¸ªè§£å†³æ–¹æ¡ˆï¼šä»¥åˆ›æ–°æ–¹å¼åœ¨äº§å“ä¸­å®žçŽ°LLMï¼Œä¾‹å¦‚LLM2ç­–ç•¥ã€‚è¿™ç§ä¸“ä¸šçŸ¥è¯†æ›´éš¾è¢«ç«žäº‰å¯¹æ‰‹å¤åˆ¶ï¼Œå› ä¸ºå®ƒæ›´æ·±å±‚åœ°éšè—åœ¨äº§å“å†…éƒ¨ã€‚\n3. æˆ‘å¯¹è¿™ä¸€æŒ‘æˆ˜çš„è§£å†³æ–¹æ¡ˆçš„ç¬¬ä¸‰ä¸ªç»„æˆéƒ¨åˆ†æ˜¯å¯¹é«˜æ°´å¹³**é¢†åŸŸä¸“ä¸šçŸ¥è¯†**çš„å¿…è¦æ€§ã€‚\n\né¢†åŸŸä¸“ä¸šçŸ¥è¯†åœ¨äº§å“æˆåŠŸä¸­çš„é‡è¦æ€§å¤šå¹´æ¥ä¸€ç›´æ˜¯è®¨è®ºçš„è¯é¢˜ã€‚è™½ç„¶æˆ‘æ‰¾ä¸åˆ°å®šé‡ç ”ç©¶å°†åˆåˆ›å…¬å¸çš„æˆåŠŸä¸Žåˆ›å§‹äººçš„é¢†åŸŸä¸“ä¸šçŸ¥è¯†ç›¸å…³è”ï¼Œä½†æˆ‘å»ºè®®æŽ¢ç´¢ä¸€äº›æ”¯æŒè¿™ç§æ˜¾è‘—ç›¸å…³æ€§çš„[ä¾‹å­](https://jamesspurway.com/2024/04/29/founder-domain-expertise-insider-tip-how-startups-benefit/)å’Œ[ç†ç”±](https://www.nvp.com/blog/domain-expertise-founder-greatest-asset/)ã€‚[çŽ°æœ‰ç ”ç©¶](https://www.ensemble.vc/research/what-does-the-data-say-about-successful-startup-founders)ä¸“æ³¨äºŽç‹¬è§’å…½ï¼Œè¡¨æ˜Žåˆ›å§‹äººçš„é¢†åŸŸä¸“ä¸šçŸ¥è¯†å¾ˆé‡è¦ï¼Œä½†ä¸æ˜¯ä¸»è¦æˆåŠŸå› ç´ ã€‚\n\nç„¶è€Œï¼Œæˆ‘è®¤ä¸ºåœ¨ç”ŸæˆAIé¢†åŸŸï¼Œè¿™ä¸€å› ç´ çš„é‡è¦æ€§å¤§å¤§å¢žå¼ºã€‚å¯¹æ­¤è§‚ç‚¹çš„æŽ¨ç†åœ¨ä»¥ä¸‹å¸–å­ä¸­å¾—åˆ°äº†å¾ˆå¥½çš„é˜è¿°ï¼š\n\n> *å¯¹äºŽåŸºäºŽLLMçš„äº§å“ï¼ŒæŠ€æœ¯ä¸“ä¸šçŸ¥è¯†çš„ä½œç”¨æ˜¾è‘—é™ä½Žï¼ˆç”±äºŽè½¯ä»¶äº¤ä»˜æ›´å®¹æ˜“ï¼‰ï¼Œè¿™ä¸Žä¼ ç»Ÿæ•°å­—äº§å“çš„æƒ…å†µä¸åŒï¼ŒåŽè€…æ˜¯ä¸€ä¸ªå…³é”®çš„ç«žäº‰ä¼˜åŠ¿ã€‚ç›¸åï¼Œ**å¯¹é¢†åŸŸçš„æ·±åˆ»ç†è§£**å˜å¾—è‡³å…³é‡è¦ï¼Œå› ä¸ºè¿™ç§æ·±åº¦çŸ¥è¯†å¯¹ç«žäº‰å¯¹æ‰‹æ¥è¯´å¾ˆéš¾å¤åˆ¶ã€‚*\n\nä»Žäº§å“ç«žäº‰åŠ›çš„è§’åº¦æ¥çœ‹ï¼Œæˆ‘è®¤ä¸ºé¢†åŸŸä¸“ä¸šçŸ¥è¯†åº”å½“å­˜åœ¨äºŽ**è®¾è®¡äº§å“å¹¶å‚ä¸Žå…¶å®žæ–½çš„åŒä¸€ä¸ªå¤´è„‘ä¸­**ã€‚å½“ç„¶ï¼Œå…¬å¸çš„â€œæŠ€æœ¯â€å’Œâ€œå•†ä¸šâ€è§’è‰²çš„ä¼ ç»Ÿåˆ†ç¦»æœ‰å…¶å¥½å¤„ï¼Œåªè¦å®ƒä»¬èƒ½å¤Ÿæœ‰æ•ˆæ²Ÿé€šï¼Œå› ä¸ºè¿™æ ·çš„æ²Ÿé€šä¼šå¯¼è‡´å¹³è¡¡è‰¯å¥½ã€æŠ€æœ¯å¤æ‚ä¸”ç¬¦åˆé¢†åŸŸè¦æ±‚çš„äº§å“ã€‚ç„¶è€Œï¼Œå£å¤´æ²Ÿé€šå¼•å…¥äº†æ˜¾è‘—çš„å¼€é”€ã€‚æŠ€æœ¯äººå‘˜å’Œå•†ä¸šäººå£«å¯èƒ½éœ€è¦å‡ ä¸ªæœˆæ‰èƒ½å……åˆ†ç†è§£å½¼æ­¤ã€‚åœ¨æ­¤æœŸé—´ï¼Œå¸‚åœºæ¡ä»¶å¯èƒ½ä¼šå‘ç”Ÿå‰§çƒˆå˜åŒ–ã€‚\n\n> *é¢†åŸŸä¸“ä¸šçŸ¥è¯†å‘æŠ€æœ¯å®žæ–½çš„æœ€æœ‰æ•ˆä¸”æ— æŸçš„è½¬åŒ–å‘ç”Ÿåœ¨å•†ä¸šå’ŒæŠ€æœ¯æ„¿æ™¯å…±å­˜äºŽåŒä¸€å¤´è„‘ä¸­æ—¶ã€‚LLMé€šè¿‡å¤§å¤§å‡å°‘äº§å“å®žæ–½æ‰€éœ€çš„æŠ€æœ¯ä¸“ä¸šçŸ¥è¯†ï¼Œæä¾›äº†è¿™ä¸€æœºä¼šï¼Œä»Žè€Œ**ä½¿å…·å¤‡å¼ºå¤§é¢†åŸŸçŸ¥è¯†çš„ä¸ªäººèƒ½å¤Ÿç›´æŽ¥å‚ä¸Žäº§å“äº¤ä»˜**ã€‚*\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*dOlknP1p9xLfvy_NxLnq7Q.png)\n\nåœ¨æˆ‘çœ‹æ¥ï¼Œåœ¨å¼€å‘GenAIäº§å“æ—¶ï¼ŒæŠ€æœ¯ä¸“ä¸šçŸ¥è¯†å¹¶ä¸ä»…é™äºŽç¨‹åºå‘˜ï¼›å®ƒè¿˜åŒ…æ‹¬é«˜çº§ChatGPTç”¨æˆ·ã€‚\n\nä¾‹å¦‚ï¼Œæˆ‘çš„æœ‹å‹[Askhat Urazbaev](https://www.linkedin.com/in/urazbaev/)ç‹¬ç«‹ä½¿ç”¨AIä¸ºä»–çš„äº§å“åˆ›å»ºMVPï¼Œç”šè‡³ä»…é€šè¿‡ChatGPTçš„æŒ‡å¯¼å°†å…¶éƒ¨ç½²åˆ°äº‘ç«¯ã€‚ä»–ä»Žæœªæ˜¯ä¸€åä¸“ä¸šè½¯ä»¶å¼€å‘äººå‘˜ï¼Œä¼¼ä¹Žä»–çš„[AI Power User](https://readmedium.com/12-questions-to-consider-when-using-ai-path-to-ai-power-user-9c7e8de1f8b7#f646)æŠ€èƒ½ä¸Žé˜…è¯»ç¨‹åºä»£ç çš„èƒ½åŠ›åŒæ ·é‡è¦ã€‚\n\n> *æˆ‘ç›¸ä¿¡ç”ŸæˆAIå°†å¾ˆå¿«ä½¿é¢†åŸŸä¸“å®¶èƒ½å¤Ÿ**å•ç‹¬**åœ¨ä»–ä»¬çš„é¢†åŸŸå†…å¼€å‘äº§å“ã€‚è¦åšåˆ°è¿™ä¸€ç‚¹ï¼Œä¸“å®¶åº”å…·å¤‡ä¸°å¯Œçš„AIç”¨æˆ·ç»éªŒï¼Œå¹¶ç»“åˆå¯¹å•†ä¸šåŽŸåˆ™å’Œäº§å“è®¾è®¡çš„åŸºç¡€ç†è§£ã€‚*\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kk0SUvuajHZp7UMbSEonmQ.png)\n\nç„¶è€Œï¼Œç›®å‰å°šä¸æ¸…æ¥šå“ªäº›å…·ä½“å·¥å…·å°†å¸®åŠ©æˆ‘ä»¬ç‹¬ç«‹åˆ›å»ºå…¨é¢çš„äº§å“ã€‚â€œ**åŸºäºŽLLMçš„å•äººå…¬å¸**â€çš„æ¦‚å¿µå°†æ˜¯æˆ‘å³å°†å‘å¸ƒçš„æ–‡ç« ç ”ç©¶çš„é‡ç‚¹ã€‚\n\n## æ€»ç»“ï¼šLLMé©±åŠ¨äº§å“çš„æˆåŠŸä¸Žå¤±è´¥å› ç´ \n\nè®©æˆ‘ä»¬å°†è¿™ä¸€ç³»åˆ—çš„ä¸‰ä¸ªéƒ¨åˆ†ä¸­çš„æ‰€æœ‰æƒ³æ³•æ±‡æ€»åœ¨ä¸€èµ·ã€‚\n\n1. [é«˜è´¨é‡æ ‡å‡†æˆ–æˆæœ¬é«˜æ˜‚çš„è´¨é‡ç›‘æŽ§çš„åº”ç”¨å¯èƒ½ä¼šå¤±è´¥ ðŸš«](https://readmedium.com/what-llm-powered-products-are-worth-developing-ux-and-adoption-perspectives-d9efcf444d50#f973)\n2. [ä¸“ä¸šåŒ–çš„ååŠ©å·¥å…·éœ€æ±‚æ—ºç›› âœ…](https://readmedium.com/what-llm-powered-products-are-worth-developing-ux-and-adoption-perspectives-d9efcf444d50#031b)\n3. [è¾¹é™…èŠ‚çœåŠªåŠ›çš„åº”ç”¨æ— æ³•æ»¡è¶³éœ€æ±‚ ðŸš«](https://readmedium.com/what-llm-powered-products-are-worth-developing-ux-and-adoption-perspectives-d9efcf444d50#e88a)\n4. [â€œæ™ºèƒ½â€æ•´åˆLLMåˆ°ç†Ÿæ‚‰å·¥ä½œæµç¨‹çš„åº”ç”¨å¯ä»¥è·¨è¶Šé¸¿æ²Ÿ âœ…](https://readmedium.com/what-llm-powered-products-are-worth-developing-ux-and-adoption-perspectives-d9efcf444d50#5d06)\n5. [æ–°ä¸€ä»£GenAIäº§å“æ›´é€‚åˆB2Bå’ŒB2B2Cè€ŒéžB2C](https://readmedium.com/what-llm-powered-products-are-worth-developing-ux-and-adoption-perspectives-d9efcf444d50#bdfa)\n6. [å¢žå¼ºLLMèƒ½åŠ›çš„åº”ç”¨ç”Ÿå‘½å‘¨æœŸçŸ­ ðŸš«](https://readmedium.com/genai-squared-how-can-a-product-avoid-the-downfall-of-most-llm-driven-startups-183619ab7883#e305)\n7. [è¿‡åº¦çº¦æŸLLMï¼šä¸å…·ç«žäº‰åŠ›åº”ç”¨çš„å¤„æ–¹ ðŸš«](https://readmedium.com/genai-squared-how-can-a-product-avoid-the-downfall-of-most-llm-driven-startups-183619ab7883#e1ef)\n8. [â€œGenAIå¹³æ–¹â€äº§å“ï¼šè§£é”ä¸å…¬å¹³ç«žäº‰ä¼˜åŠ¿ âœ…](https://readmedium.com/genai-squared-how-can-a-product-avoid-the-downfall-of-most-llm-driven-startups-183619ab7883#9f01)\n9. å¼€å‘å‘¨æœŸè¾ƒé•¿ä¸”å¸‚åœºé‡‡ç”¨æ—¶é—´æ¼«é•¿çš„å¤§åž‹åº”ç”¨ç¼ºä¹ç«žäº‰åŠ› ðŸš«\n10. åˆ©ç”¨ä¸å¤ªæ˜Žæ˜¾çš„LLMèƒ½åŠ›æå‡ç«žäº‰åŠ›å’Œèµ„æºæ•ˆçŽ‡ âœ…\n11. åŸºäºŽæ·±åŽšé¢†åŸŸä¸“ä¸šçŸ¥è¯†çš„å°åž‹AIäº§å“å…·æœ‰ç«žäº‰åŠ› âœ…\n\né™¤äº†å› ç´ \\#4ï¼Œå‰©ä¸‹çš„10ä¸ªæˆåŠŸ/å¤±è´¥å› ç´ å¯ä»¥åº”ç”¨äºŽ**æ–°**äº§å“/åˆåˆ›å…¬å¸ã€‚\n\nä¸‹é¢æ˜¯ä¸€ä¸ªç¤ºæ„å›¾ï¼Œå±•ç¤ºè¿™10ä¸ªå› ç´ ã€LLMèƒ½åŠ›ä»¥åŠLLMæŠ€æœ¯å¸‚åœºçš„ä¸€äº›ç‰¹å¾ä¹‹é—´çš„å…³ç³»ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*f6E4WmRBw3H7eCNbTGJHUQ.png)\n\nå½“ç„¶ï¼Œåªæœ‰äº§å“å®žéªŒæ‰èƒ½éªŒè¯ç¤ºæ„å›¾ä¸­æ‰€ç¤ºçš„è€ƒè™‘å› ç´ ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¯ä»¥å¸®åŠ©æˆ‘ä»¬é€šè¿‡é™åˆ¶å®žéªŒèŒƒå›´æ¥å˜å¾—**æ›´å¿«**ã€‚æ­£å¦‚ç¬¬9èŠ‚æ‰€è§£é‡Šçš„ï¼Œå‘çŽ°å’Œäº¤ä»˜çš„é«˜é€Ÿåº¦å¯¹GenAIäº§å“æ¯”å…¶ä»–ç±»åž‹çš„æ•°å­—äº§å“æ›´ä¸ºé‡è¦ï¼Œæœ‰ä¸¤ä¸ªåŽŸå› ã€‚\n\nè‡ªç„¶ï¼ŒæˆåŠŸå› ç´ çš„åˆ—è¡¨ä¸å¯èƒ½æ˜¯åŒ…ç½—ä¸‡è±¡çš„ã€‚ä¹Ÿè®¸æ‚¨é‡åˆ°è¿‡å…¶ä»–ç±»åˆ«çš„æ–°åž‹LLMé©±åŠ¨äº§å“ï¼Œè¿™äº›äº§å“æœªåœ¨ä¸Šè¿°æåˆ°ï¼Œä½†æ‚¨è®¤ä¸ºå®ƒä»¬å…·æœ‰æˆåŠŸçš„æ½œåŠ›ã€‚è¯·åœ¨è¯„è®ºä¸­åˆ†äº«è¿™äº›äº§å“ç±»åž‹æˆ–ç‰¹å¾ ðŸ™\n\n"},{"lang":"zh","group":"blog","slug":"blog/how-to-create-an-ai-team-to-write-compelling-stories-with-crewai-and-gemini-pro-3713f53c72c4","frontmatter":{"title":"å¦‚ä½•ä½¿ç”¨ CrewAI å’Œ Gemini Pro åˆ›å»º AI å›¢é˜Ÿæ¥æ’°å†™å¼•äººå…¥èƒœçš„æ•…äº‹","meta_title":"å¦‚ä½•ä½¿ç”¨ CrewAI å’Œ Gemini Pro åˆ›å»º AI å›¢é˜Ÿæ¥æ’°å†™å¼•äººå…¥èƒœçš„æ•…äº‹","description":"æ‚¨æ˜¯å¦å¯¹äººå·¥æ™ºèƒ½ç”Ÿæˆå¼•äººå…¥èƒœçš„æ•…äº‹çš„æƒ³æ³•ç€è¿·ï¼Ÿå¦‚æžœæ˜¯è¿™æ ·ï¼Œæ‚¨å¹¶ä¸å­¤å•ï¼åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æŽ¢è®¨â€¦â€¦","date":"2024-10-31T23:04:49.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*tSnoOxxIGtrwdUT8","categories":["Programming","Natural Language Processing","Generative AI"],"author":"Rifx.Online","tags":["CrewAI","Gemini","screenwriters","critics","storytelling"],"draft":false,"slug":"blog/how-to-create-an-ai-team-to-write-compelling-stories-with-crewai-and-gemini-pro-3713f53c72c4"},"content":"\n\n\nä½ æ˜¯å¦å¯¹AIç”Ÿæˆå¼•äººå…¥èƒœçš„æ•…äº‹è¿™ä¸€æƒ³æ³•æ„Ÿåˆ°ç€è¿·ï¼Ÿå¦‚æžœæ˜¯è¿™æ ·ï¼Œä½ å¹¶ä¸å­¤å•ï¼åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æŽ¢è®¨ä¸€ä¸ªç»“åˆCrewAIå’ŒGemini ProåŠ›é‡çš„å…¥é—¨é¡¹ç›®ï¼Œåˆ›å»ºä¸€ä¸ªä»£ç†ç½‘ç»œï¼Œé€šè¿‡ç”¨æˆ·è¾“å…¥çš„å¸®åŠ©æ¥åˆ›ä½œçŸ­ç¯‡æ•…äº‹ã€‚æ— è®ºä½ æ˜¯ä¸€ä¸ªåˆå‡ºèŒ…åºçš„ç¨‹åºå‘˜ï¼Œä¸€ä¸ªå¸Œæœ›æŽ¢ç´¢æ•°å­—å‰æ²¿çš„è®²æ•…äº‹è€…ï¼Œè¿˜æ˜¯ä»…ä»…å¯¹äººå·¥æ™ºèƒ½çš„æ½œåŠ›æ„Ÿåˆ°å¥½å¥‡ï¼Œè¿™æœ¬æŒ‡å—éƒ½é€‚åˆä½ ã€‚\n\n## CrewAI å’Œ Gemini Pro æ˜¯ä»€ä¹ˆï¼Ÿ\n\nåœ¨æˆ‘ä»¬æ·±å…¥æŽ¢è®¨æž„å»º AI è®²æ•…äº‹è€…çš„ç»†èŠ‚ä¹‹å‰ï¼Œå…ˆæ¥æ¾„æ¸…ä¸€ä¸‹ CrewAI å’Œ Gemini Pro çš„æ¦‚å¿µã€‚\n\n**CrewAI** æ˜¯ä¸€ä¸ªå¼•äººå…¥èƒœçš„æ¡†æž¶ï¼Œæ—¨åœ¨åè°ƒå¤šä¸ª AI ä»£ç†ï¼Œæ¯ä¸ªä»£ç†éƒ½æœ‰å…¶ç‹¬ç‰¹çš„æŠ€èƒ½å’ŒèŒè´£ï¼Œä»¥åä½œå®Œæˆå¤æ‚ä»»åŠ¡ã€‚å¯ä»¥æŠŠå®ƒæƒ³è±¡æˆä¸€ä¸ªå¯¼æ¼”ç®¡ç†ä¸€ç»„æ¼”å‘˜ï¼Œæ¯ä¸ªæ¼”å‘˜æ‰®æ¼”ç‰¹å®šè§’è‰²æ¥è®©æ•…äº‹ç”ŸåŠ¨èµ·æ¥ã€‚åœ¨æˆ‘ä»¬é¡¹ç›®çš„èƒŒæ™¯ä¸‹ï¼ŒCrewAI ä½¿æˆ‘ä»¬èƒ½å¤Ÿåˆ›å»ºä¸€æ”¯ç”±ä¸“ä¸šä»£ç†ï¼ˆå¦‚ç¼–å‰§ã€è¯„è®ºå®¶å’Œæ•…äº‹å¤§å¸ˆï¼‰ç»„æˆçš„å›¢é˜Ÿï¼Œå…±åŒæ’°å†™æ•…äº‹ã€‚\n\n**Gemini Pro**ï¼Œå¦ä¸€æ–¹é¢ï¼Œæ˜¯ç”± Google å¼€å‘çš„æœ€å…ˆè¿›çš„è¯­è¨€æ¨¡åž‹ã€‚å®ƒä»¥ç†è§£å’Œç”Ÿæˆç±»äººæ–‡æœ¬çš„èƒ½åŠ›è€Œé—»åï¼Œä½¿å…¶æˆä¸ºæ•…äº‹åˆ›ä½œç­‰åˆ›æ„ä»»åŠ¡çš„ç†æƒ³é€‰æ‹©ã€‚é€šè¿‡åˆ©ç”¨ Gemini Proï¼Œæˆ‘ä»¬å¯ä»¥ç¡®ä¿æˆ‘ä»¬çš„ä»£ç†å…·å¤‡ç”Ÿæˆå¼•äººå…¥èƒœçš„å™äº‹å†…å®¹çš„åšå®žåŸºç¡€ã€‚\n\n## ä¸ºä»€ä¹ˆè¿™ç§ç»“æž„å¾ˆé‡è¦ï¼Ÿ\n\nCrewAI å’Œ Gemini Pro çš„ç»“åˆä½¿å¾—æ•…äº‹ç”Ÿæˆèƒ½å¤Ÿé‡‡ç”¨é«˜åº¦åä½œå’Œä¸“ä¸šåŒ–çš„æ–¹æ³•ã€‚è¿™ä¸ªç»“æž„å…è®¸ï¼š\n\n1. **ä¸“ä¸šåŒ–**ï¼šæ¯ä¸ªä»£ç†å¯ä»¥ä¸“æ³¨äºŽå®ƒæœ€æ“…é•¿çš„é¢†åŸŸï¼Œæ— è®ºæ˜¯æ’°å†™å¯¹è¯ã€ç¡®ä¿ä¸€è‡´æ€§è¿˜æ˜¯ç›‘ç£é¡¹ç›®ã€‚\n2. **åä½œ**ï¼šä»£ç†å¯ä»¥å…±åŒå·¥ä½œï¼Œç»“åˆå„è‡ªçš„ä¼˜åŠ¿ï¼Œäº§ç”Ÿä¸€ä¸ªè¶…è¶Šå…¶éƒ¨åˆ†æ€»å’Œçš„æ•…äº‹ã€‚\n3. **çµæ´»æ€§**ï¼šè¯¥è®¾ç½®å…·æœ‰é«˜åº¦é€‚åº”æ€§ï¼Œå¯ä»¥æ ¹æ®ç”¨æˆ·è¾“å…¥æˆ–åˆ›æ„æ–¹å‘å¼ºè°ƒæˆ–æ”¹å˜ä¸åŒçš„æ•…äº‹å…ƒç´ ã€‚\n\n## è®¾ç½®çŽ¯å¢ƒ\n\né¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä¸€äº›åº“æ¥ä½¿ç”¨ã€‚æ‚¨å¯ä»¥é€šè¿‡ pip åŠ è½½è¿™äº›åº“ï¼š\n\n```python\npip install crewai\n```\n\n```python\npip install langchain-google-genai\n```\n\nåŠ è½½å¿…è¦çš„åº“åŽï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹ç¼–ç ã€‚æˆ‘ä»¬å°†é¦–å…ˆå¯¼å…¥æ‰€éœ€çš„æ¨¡å—å¹¶åˆå§‹åŒ–æˆ‘ä»¬çš„ Gemini pro API è¿žæŽ¥ã€‚\n\nå¦‚æ‚¨æ‰€è§ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ª Gemini æ¨¡åž‹çš„ API å¯†é’¥ã€‚æ‚¨å¯ä»¥åœ¨ Google AI Studio ä¸­[å…è´¹](https://ai.google.dev/)åˆ›å»ºæ­¤å¯†é’¥ã€‚ä¹‹åŽï¼Œæ‚¨å¯ä»¥å°†æ­¤å¯†é’¥å¤åˆ¶åˆ° google\\_api\\_key å˜é‡ä¸­ï¼Œæˆ–è€…é€šè¿‡åœ¨å‘½ä»¤è¡Œä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤å°†å…¶åŠ è½½åˆ°çŽ¯å¢ƒä¸­ï¼š\n\n```python\nexport GOOGLE_API_KEY=YOUR_KEY\n```\n\nå°†æ‚¨ä»Ž Google AI Studio èŽ·å–çš„ API å¯†é’¥æ›¿æ¢ä¸º YOUR\\_KEYã€‚\n\næŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®šä¹‰æˆ‘ä»¬çš„ä»£ç†ï¼šç¼–å‰§ã€è¯„è®ºå®¶å’Œæ•…äº‹å¤§å¸ˆã€‚æ¯ä¸ªä»£ç†éƒ½æœ‰ä¸€ä¸ªè§’è‰²ã€ç›®æ ‡å’ŒèƒŒæ™¯æ•…äº‹ï¼Œä»¥æŒ‡å¯¼å…¶åœ¨æ•…äº‹ç”Ÿæˆè¿‡ç¨‹ä¸­çš„è´¡çŒ®ã€‚\n\nä¾‹å¦‚ï¼Œç¼–å‰§ä¸“æ³¨äºŽå°†åˆ›æ„è½¬åŒ–ä¸ºå¼•äººå…¥èƒœçš„åœºæ™¯ï¼Œè€Œè¯„è®ºå®¶ç¡®ä¿ä¸€è‡´æ€§å’Œéµå¾ªç±»åž‹ã€‚\n\nè¿™äº›ä»£ç†å°†å…±åŒå·¥ä½œï¼Œåˆ›é€ ä¸€ä¸ªå¼•äººå…¥èƒœçš„æ•…äº‹ã€‚æ•…äº‹å¤§å¸ˆå°†æŽ¥å—ä»»åŠ¡ï¼Œç„¶åŽåœ¨å…¶ä»–ä»£ç†ä¹‹é—´å§”æ´¾å’Œåè°ƒä»»åŠ¡ã€‚æˆ‘ä»¬é€šè¿‡å°† allow\\_delegation å‚æ•°è®¾ç½®ä¸º True æ¥å…è®¸è¿™ç§è¡Œä¸ºã€‚\n\nå‡†å¤‡å¥½ä»£ç†åŽï¼Œæˆ‘ä»¬æç¤ºç”¨æˆ·æä¾›ä¸€ä¸ªæ•…äº‹åˆ›æ„ã€‚ç„¶åŽï¼Œè¿™ä¸ªè¾“å…¥ç”¨äºŽåˆ›å»ºä¸€ä¸ªä»»åŠ¡ï¼Œæ¦‚è¿°æ•…äº‹åº”åŒ…å«çš„å†…å®¹ï¼Œå¼•å¯¼ä»£ç†è¿›è¡Œåˆ›ä½œè¿‡ç¨‹ã€‚\n\nåœ¨åˆ›å»ºä»»åŠ¡æ—¶ï¼Œæˆ‘ä»¬å°†ä»»åŠ¡æäº¤ç»™æ•…äº‹å¤§å¸ˆï¼Œå› ä¸ºå®ƒå°†åè°ƒæˆ‘ä»¬çš„æ•…äº‹åˆ›ä½œè¿‡ç¨‹ã€‚\n\næœ€åŽï¼Œæˆ‘ä»¬åº”è¯¥å°†è¿™äº›ä»£ç†ç»„åˆæˆä¸€ä¸ªå›¢é˜Ÿå¹¶è¿è¡Œæˆ‘ä»¬çš„ä»»åŠ¡ã€‚\n\nå°±è¿™æ ·ã€‚å½“æˆ‘ä»¬è¿è¡Œè¿™æ®µä»£ç æ—¶ï¼Œå®ƒä¼šæç¤ºç”¨æˆ·æä¾›ä¸€ä¸ªæ•…äº‹åˆ›æ„ï¼Œç„¶åŽé€šè¿‡ä»£ç†åˆä½œå†™ä¸€ä¸ªçŸ­æ•…äº‹ã€‚å½“ç„¶ï¼Œåœ¨ CrewAI æ¡†æž¶ä¸­è¿˜æœ‰æ›´å¤šå†…å®¹ï¼Œä¾‹å¦‚å·¥å…·ä½¿ç”¨ã€å±‚æ¬¡å¤„ç†ã€ä¸Ž ollama ä¸€èµ·å®Œå…¨æœ¬åœ°è¿è¡Œä¸åŒä»£ç†ç­‰ï¼Œä½†è¿™äº›ä¸»é¢˜æ˜¯å¦ä¸€ä¸ªæ–‡ç« çš„å†…å®¹ã€‚\n\næ‚¨å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°å®Œæ•´çš„ä»£ç ä»¥ç›´æŽ¥è¿è¡Œï¼š\n\næ‚¨å¯ä»¥å°†æ­¤ä»£ç ç”¨ä½œæ­¤ç±»åº”ç”¨ç¨‹åºçš„æ¨¡æ¿ï¼Œæ‚¨å¯ä»¥æž„å»ºæ¸¸æˆæž„å»ºè€…å›¢é˜Ÿã€è‚¡ç¥¨åˆ†æžå¸ˆå›¢é˜Ÿã€è¥é”€å›¢é˜Ÿç­‰ã€‚å‡­å€Ÿæƒ³è±¡åŠ›ï¼Œå¤©ç©ºæ‰æ˜¯æžé™ã€‚å¦‚æžœæ‚¨å–œæ¬¢è¿™ç¯‡æ–‡ç« å¹¶å¯¹æ›´é«˜çº§çš„å®žçŽ°æ„Ÿåˆ°å…´å¥‹ï¼Œå¯ä»¥è®¿é—® CrewAI [ç½‘ç«™](https://www.crewai.com/)ã€‚\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/how-to-improve-llms-with-rag-abdc132f76ac","frontmatter":{"title":"å¦‚ä½•ä½¿ç”¨ RAG æé«˜ LLM æˆç»©","meta_title":"å¦‚ä½•ä½¿ç”¨ RAG æé«˜ LLM æˆç»©","description":"é€‚åˆåˆå­¦è€…çš„ Python ä»£ç ä»‹ç»","date":"2024-11-04T12:31:55.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*N0Ad_oCIrAyzMYRdH3trqg.png","categories":["Natural Language Processing","Programming","Generative AI"],"author":"Rifx.Online","tags":["RAG","retrievers","LlamaIndex","knowledge","bases"],"draft":false,"slug":"blog/how-to-improve-llms-with-rag-abdc132f76ac"},"content":"\n\n\n### åˆå­¦è€…å‹å¥½çš„ä»‹ç» w/ Python ä»£ç \n\næœ¬æ–‡æ˜¯å…³äºŽåœ¨å®žè·µä¸­ä½¿ç”¨å¤§åž‹è¯­è¨€æ¨¡åž‹çš„[æ›´å¤§ç³»åˆ—](https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c)çš„ä¸€éƒ¨åˆ†ã€‚åœ¨[ä¸Šä¸€ç¯‡æ–‡ç« ](https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32)ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ QLoRA å¯¹ Mistral-7b-Instruct è¿›è¡Œäº†å¾®è°ƒï¼Œä»¥å›žåº” YouTube è¯„è®ºã€‚å°½ç®¡å¾®è°ƒåŽçš„æ¨¡åž‹åœ¨å›žåº”è§‚ä¼—åé¦ˆæ—¶æˆåŠŸæ•æ‰äº†æˆ‘çš„é£Žæ ¼ï¼Œä½†å®ƒå¯¹æŠ€æœ¯é—®é¢˜çš„å›žç­”ä¸Žæˆ‘çš„è§£é‡Šå¹¶ä¸åŒ¹é…ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘å°†è®¨è®ºå¦‚ä½•é€šè¿‡æ£€ç´¢å¢žå¼ºç”Ÿæˆï¼ˆå³ RAGï¼‰æ¥æé«˜ LLM çš„æ€§èƒ½ã€‚\n\n\n\nå¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰åœ¨å“åº”ç”¨æˆ·æŸ¥è¯¢æ—¶å±•ç¤ºäº†å­˜å‚¨å’Œéƒ¨ç½²å¤§é‡çŸ¥è¯†çš„æƒŠäººèƒ½åŠ›ã€‚è™½ç„¶è¿™ä½¿å¾—åƒ ChatGPT è¿™æ ·çš„å¼ºå¤§ AI ç³»ç»Ÿå¾—ä»¥åˆ›å»ºï¼Œä½†ä»¥è¿™ç§æ–¹å¼åŽ‹ç¼©ä¸–ç•ŒçŸ¥è¯†æœ‰**ä¸¤ä¸ªå…³é”®é™åˆ¶**ã€‚\n\n**é¦–å…ˆ**ï¼ŒLLM çš„çŸ¥è¯†æ˜¯é™æ€çš„ï¼Œå³ä¸ä¼šéšç€æ–°ä¿¡æ¯çš„å‡ºçŽ°è€Œæ›´æ–°ã€‚**å…¶æ¬¡**ï¼ŒLLM å¯èƒ½å¯¹å…¶è®­ç»ƒæ•°æ®ä¸­ä¸æ˜¾è‘—çš„åˆ©åŸºå’Œä¸“ä¸šä¿¡æ¯ç¼ºä¹è¶³å¤Ÿçš„â€œç†è§£â€ã€‚è¿™äº›é™åˆ¶å¯èƒ½å¯¼è‡´æ¨¡åž‹å¯¹ç”¨æˆ·æŸ¥è¯¢çš„å›žç­”ä¸ç†æƒ³ï¼ˆç”šè‡³æ˜¯è™šæž„çš„ï¼‰ã€‚\n\næˆ‘ä»¬å¯ä»¥é€šè¿‡**é€šè¿‡ä¸“ä¸šå’Œå¯å˜çš„çŸ¥è¯†åº“å¢žå¼ºæ¨¡åž‹**æ¥ç¼“è§£è¿™äº›é™åˆ¶ï¼Œä¾‹å¦‚å®¢æˆ·å¸¸è§é—®é¢˜è§£ç­”ã€è½¯ä»¶æ–‡æ¡£æˆ–äº§å“ç›®å½•ã€‚è¿™ä½¿å¾—åˆ›å»ºæ›´å¼ºå¤§å’Œé€‚åº”æ€§æ›´å¼ºçš„ AI ç³»ç»Ÿæˆä¸ºå¯èƒ½ã€‚\n\n**æ£€ç´¢å¢žå¼ºç”Ÿæˆ**ï¼Œæˆ–ç§° **RAG**ï¼Œå°±æ˜¯è¿™æ ·ä¸€ç§æ–¹æ³•ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘æä¾› RAG çš„é«˜çº§ä»‹ç»ï¼Œå¹¶åˆ†äº«ä½¿ç”¨ LlamaIndex å®žçŽ° RAG ç³»ç»Ÿçš„ç¤ºä¾‹ Python ä»£ç ã€‚\n\n## ä»€ä¹ˆæ˜¯ RAGï¼Ÿ\n\nLLM çš„åŸºæœ¬ç”¨æ³•æ˜¯ç»™å®ƒä¸€ä¸ªæç¤ºå¹¶èŽ·å–å“åº”ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*sM1p-3FoTaGZunqx918G9A.png)\n\n**RAG é€šè¿‡åœ¨è¿™ä¸ªåŸºæœ¬è¿‡ç¨‹ä¸­æ·»åŠ ä¸€ä¸ªæ­¥éª¤æ¥å·¥ä½œ**ã€‚å…·ä½“æ¥è¯´ï¼Œæ‰§è¡Œä¸€ä¸ªæ£€ç´¢æ­¥éª¤ï¼Œæ ¹æ®ç”¨æˆ·çš„æç¤ºï¼Œä»Žå¤–éƒ¨çŸ¥è¯†åº“ä¸­æå–ç›¸å…³ä¿¡æ¯ï¼Œå¹¶åœ¨ä¼ é€’ç»™ LLM ä¹‹å‰å°†å…¶æ³¨å…¥åˆ°æç¤ºä¸­ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*EhJZj1blu7a8EPmVAPsNcA.png)\n\n## æˆ‘ä»¬ä¸ºä»€ä¹ˆå…³å¿ƒ\n\nè¯·æ³¨æ„ï¼ŒRAG å¹¶æ²¡æœ‰ä»Žæ ¹æœ¬ä¸Šæ”¹å˜æˆ‘ä»¬ä½¿ç”¨ LLM çš„æ–¹å¼ï¼›å®ƒä»ç„¶æ˜¯ *æç¤ºè¾“å…¥å’Œå“åº”è¾“å‡º*ã€‚RAG åªæ˜¯å¢žå¼ºäº†è¿™ä¸ªè¿‡ç¨‹ï¼ˆå› æ­¤å¾—åï¼‰ã€‚\n\nè¿™ä½¿å¾— **RAG æˆä¸ºä¸€ç§çµæ´»ä¸”ï¼ˆç›¸å¯¹ï¼‰ç®€å•çš„æ–¹å¼æ¥æ”¹å–„åŸºäºŽ LLM çš„ç³»ç»Ÿ**ã€‚æ­¤å¤–ï¼Œç”±äºŽçŸ¥è¯†å­˜å‚¨åœ¨å¤–éƒ¨æ•°æ®åº“ä¸­ï¼Œæ›´æ–°ç³»ç»ŸçŸ¥è¯†å°±åƒä»Žè¡¨ä¸­æ·»åŠ æˆ–åˆ é™¤è®°å½•ä¸€æ ·ç®€å•ã€‚\n\n### ä¸ºä»€ä¹ˆä¸è¿›è¡Œå¾®è°ƒï¼Ÿ\n\næœ¬ç³»åˆ—ä¹‹å‰çš„æ–‡ç« è®¨è®ºäº†[å¾®è°ƒ](https://towardsdatascience.com/fine-tuning-large-language-models-llms-23473d763b91)ï¼Œå³ä¸ºç‰¹å®šç”¨ä¾‹è°ƒæ•´çŽ°æœ‰æ¨¡åž‹ã€‚è™½ç„¶è¿™æ˜¯ä¸€ç§èµ‹äºˆLLMä¸“ä¸šçŸ¥è¯†çš„æ›¿ä»£æ–¹æ³•ï¼Œä½†ä»Žç»éªŒæ¥çœ‹ï¼Œ**å¾®è°ƒä¼¼ä¹Žåœ¨è¿™æ–¹é¢çš„æ•ˆæžœä¸å¦‚RAG** \\[1]ã€‚\n\n## å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„\n\nRAG ç³»ç»Ÿæœ‰ä¸¤ä¸ªå…³é”®è¦ç´ ï¼š**æ£€ç´¢å™¨**å’Œ **çŸ¥è¯†åº“**ã€‚\n\n### Retriever\n\næ£€ç´¢å™¨æŽ¥æ”¶ç”¨æˆ·æç¤ºå¹¶ä»ŽçŸ¥è¯†åº“ä¸­è¿”å›žç›¸å…³é¡¹ç›®ã€‚è¿™é€šå¸¸ä½¿ç”¨æ‰€è°“çš„ **æ–‡æœ¬åµŒå…¥**ï¼Œå³æ–‡æœ¬åœ¨æ¦‚å¿µç©ºé—´ä¸­çš„æ•°å€¼è¡¨ç¤ºã€‚æ¢å¥è¯è¯´ï¼Œè¿™äº›æ˜¯ **è¡¨ç¤ºç»™å®šæ–‡æœ¬çš„ *å«ä¹‰* çš„æ•°å­—**ã€‚\n\næ–‡æœ¬åµŒå…¥å¯ä»¥ç”¨æ¥è®¡ç®—ç”¨æˆ·æŸ¥è¯¢ä¸ŽçŸ¥è¯†åº“ä¸­æ¯ä¸ªé¡¹ç›®ä¹‹é—´çš„ç›¸ä¼¼æ€§å¾—åˆ†ã€‚è¿™ä¸ªè¿‡ç¨‹çš„ç»“æžœæ˜¯ **æ¯ä¸ªé¡¹ç›®ä¸Žè¾“å…¥æŸ¥è¯¢ç›¸å…³æ€§çš„æŽ’å**ã€‚\n\nç„¶åŽï¼Œæ£€ç´¢å™¨å¯ä»¥é€‰æ‹©å‰ k ä¸ªï¼ˆä¾‹å¦‚ k=3ï¼‰æœ€ç›¸å…³çš„é¡¹ç›®ï¼Œå¹¶å°†å®ƒä»¬æ³¨å…¥åˆ°ç”¨æˆ·æç¤ºä¸­ã€‚è¿™ä¸ªå¢žå¼ºçš„æç¤ºéšåŽè¢«ä¼ é€’ç»™ LLM è¿›è¡Œç”Ÿæˆã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*jpTwdBmoTlJlfPAm0oJiVQ.png)\n\n### çŸ¥è¯†åº“\n\nRAG ç³»ç»Ÿçš„ä¸‹ä¸€ä¸ªå…³é”®è¦ç´ æ˜¯çŸ¥è¯†åº“ã€‚è¿™ä¸ª **åŒ…å«äº†æ‚¨å¸Œæœ›æä¾›ç»™ LLM çš„æ‰€æœ‰ä¿¡æ¯**ã€‚è™½ç„¶æœ‰æ— æ•°ç§æ–¹æ³•å¯ä»¥æž„å»º RAG çš„çŸ¥è¯†åº“ï¼Œä½†åœ¨è¿™é‡Œæˆ‘å°†é‡ç‚¹ä»‹ç»å¦‚ä½•ä»Žä¸€ç»„æ–‡æ¡£ä¸­æž„å»ºä¸€ä¸ªçŸ¥è¯†åº“ã€‚\n\nè¿™ä¸ªè¿‡ç¨‹å¯ä»¥åˆ†ä¸º **4 ä¸ªå…³é”®æ­¥éª¤** \\[2,3].\n\n1. **åŠ è½½æ–‡æ¡£** â€” è¿™åŒ…æ‹¬æ”¶é›†ä¸€ç»„æ–‡æ¡£å¹¶ç¡®ä¿å®ƒä»¬å¤„äºŽå¯è§£æžçš„æ ¼å¼ï¼ˆç¨åŽä¼šè¯¦ç»†ä»‹ç»ï¼‰ã€‚\n2. **åˆ†å—æ–‡æ¡£â€”**ç”±äºŽ LLM çš„ä¸Šä¸‹æ–‡çª—å£æœ‰é™ï¼Œæ–‡æ¡£å¿…é¡»è¢«æ‹†åˆ†æˆæ›´å°çš„å— **ï¼ˆä¾‹å¦‚ï¼Œ** 256 æˆ– 512 ä¸ªå­—ç¬¦é•¿ï¼‰ã€‚\n3. **åµŒå…¥å—** â€” ä½¿ç”¨æ–‡æœ¬åµŒå…¥æ¨¡åž‹å°†æ¯ä¸ªå—è½¬æ¢ä¸ºæ•°å­—ã€‚\n4. **åŠ è½½åˆ°å‘é‡æ•°æ®åº“**â€” å°†æ–‡æœ¬åµŒå…¥åŠ è½½åˆ°æ•°æ®åº“ï¼ˆå³å‘é‡æ•°æ®åº“ï¼‰ä¸­ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*VWG6Tr0OxCnD5Mvygm5DCA.png)\n\n## ä¸€äº›ç»†å¾®å·®åˆ«\n\nè™½ç„¶æž„å»º RAG ç³»ç»Ÿçš„æ­¥éª¤åœ¨æ¦‚å¿µä¸Šå¾ˆç®€å•ï¼Œä½†ä¸€äº›ç»†å¾®å·®åˆ«å¯èƒ½ä½¿å¾—åœ¨çŽ°å®žä¸–ç•Œä¸­æž„å»ºä¸€ä¸ªç³»ç»Ÿå˜å¾—æ›´åŠ å¤æ‚ã€‚\n\n**æ–‡æ¡£å‡†å¤‡**â€”RAG ç³»ç»Ÿçš„è´¨é‡å–å†³äºŽä»Žæºæ–‡æ¡£ä¸­æå–æœ‰ç”¨ä¿¡æ¯çš„èƒ½åŠ›ã€‚ä¾‹å¦‚ï¼Œå¦‚æžœä¸€ä¸ªæ–‡æ¡£æ ¼å¼æ··ä¹±ï¼Œå……æ»¡äº†å›¾åƒå’Œè¡¨æ ¼ï¼Œé‚£ä¹ˆè§£æžèµ·æ¥ä¼šæ¯”ä¸€ä¸ªæ ¼å¼è‰¯å¥½çš„æ–‡æœ¬æ–‡ä»¶æ›´å›°éš¾ã€‚\n\n**é€‰æ‹©åˆé€‚çš„å—å¤§å°**â€”æˆ‘ä»¬å·²ç»æåˆ°ç”±äºŽ LLM ä¸Šä¸‹æ–‡çª—å£çš„éœ€è¦è¿›è¡Œåˆ†å—ã€‚ç„¶è€Œï¼Œè¿˜æœ‰ 2 ä¸ªé¢å¤–çš„åˆ†å—åŠ¨æœºã€‚\n\n**é¦–å…ˆ**ï¼Œå®ƒå¯ä»¥é™ä½Žï¼ˆè®¡ç®—ï¼‰æˆæœ¬ã€‚ä½ åœ¨æç¤ºä¸­æ³¨å…¥çš„æ–‡æœ¬è¶Šå¤šï¼Œç”Ÿæˆå®Œæˆæ‰€éœ€çš„è®¡ç®—å°±è¶Šå¤šã€‚**ç¬¬äºŒ**æ˜¯æ€§èƒ½ã€‚ç‰¹å®šæŸ¥è¯¢çš„ç›¸å…³ä¿¡æ¯å¾€å¾€é›†ä¸­åœ¨æºæ–‡æ¡£ä¸­ï¼ˆé€šå¸¸ä»…ä¸€å¥è¯å°±å¯ä»¥å›žç­”ä¸€ä¸ªé—®é¢˜ï¼‰ã€‚åˆ†å—æœ‰åŠ©äºŽæœ€å°åŒ–ä¼ é€’ç»™æ¨¡åž‹çš„æ— å…³ä¿¡æ¯çš„æ•°é‡ \\[4\\]ã€‚\n\n**æ”¹å–„æœç´¢** â€” è™½ç„¶æ–‡æœ¬åµŒå…¥æä¾›äº†ä¸€ç§å¼ºå¤§ä¸”å¿«é€Ÿçš„æœç´¢æ–¹å¼ï¼Œä½†å®ƒå¹¶ä¸æ€»æ˜¯èƒ½å¦‚äººæ‰€æ„¿åœ°å·¥ä½œã€‚æ¢å¥è¯è¯´ï¼Œå®ƒå¯èƒ½è¿”å›žä¸Žç”¨æˆ·æŸ¥è¯¢â€œç›¸ä¼¼â€çš„ç»“æžœï¼Œä½†å¯¹å›žç­”é—®é¢˜å¹¶æ²¡æœ‰å¸®åŠ©ï¼Œä¾‹å¦‚ï¼Œâ€œ*æ´›æ‰çŸ¶çš„å¤©æ°”æ€Žä¹ˆæ ·ï¼Ÿ*â€å¯èƒ½è¿”å›žâ€œ*çº½çº¦çš„å¤©æ°”æ€Žä¹ˆæ ·ï¼Ÿ*â€ã€‚\n\nç¼“è§£è¿™ä¸€é—®é¢˜çš„æœ€ç®€å•æ–¹æ³•æ˜¯é€šè¿‡è‰¯å¥½çš„æ–‡æ¡£å‡†å¤‡å’Œåˆ†å—ã€‚ç„¶è€Œï¼Œå¯¹äºŽæŸäº›ç”¨ä¾‹ï¼Œå¯èƒ½éœ€è¦é¢å¤–çš„ç­–ç•¥æ¥æ”¹å–„æœç´¢ï¼Œä¾‹å¦‚ä¸ºæ¯ä¸ªå—ä½¿ç”¨ **å…ƒæ ‡ç­¾**ã€é‡‡ç”¨ç»“åˆå…³é”®è¯å’ŒåµŒå…¥æœç´¢çš„ **æ··åˆæœç´¢**ï¼Œæˆ–ä½¿ç”¨ **é‡æŽ’åºå™¨**ï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨è®¡ç®—ä¸¤æ®µæ–‡æœ¬ç›¸ä¼¼æ€§çš„æ¨¡åž‹ã€‚\n\n## ç¤ºä¾‹ä»£ç ï¼šä½¿ç”¨ RAG æ”¹è¿› YouTube è¯„è®ºå“åº”å™¨\n\nåœ¨å¯¹ RAG å·¥ä½œåŽŸç†æœ‰åŸºæœ¬äº†è§£åŽï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•åœ¨å®žè·µä¸­ä½¿ç”¨å®ƒã€‚æˆ‘å°†åŸºäºŽ [ä¸Šä¸€ç¯‡æ–‡ç« ](https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32) ä¸­çš„ç¤ºä¾‹ï¼Œåœ¨å…¶ä¸­æˆ‘ä½¿ç”¨ QLoRA å¯¹ Mistral-7B-Instruct è¿›è¡Œäº†å¾®è°ƒï¼Œä»¥å“åº” YouTube è¯„è®ºã€‚æˆ‘ä»¬å°†ä½¿ç”¨ LlamaIndex ä¸ºä¹‹å‰å¾®è°ƒçš„æ¨¡åž‹æ·»åŠ  RAG ç³»ç»Ÿã€‚\n\nç¤ºä¾‹ä»£ç å¯åœ¨ [Colab Notebook](https://colab.research.google.com/drive/1peJukr-9E1zCo1iAalbgDPJmNMydvQms?usp=sharing) ä¸­å…è´¹èŽ·å¾—ï¼Œè¯¥ Notebook å¯ä»¥åœ¨æä¾›çš„ï¼ˆå…è´¹ï¼‰T4 GPU ä¸Šè¿è¡Œã€‚æ­¤ç¤ºä¾‹çš„æºæ–‡ä»¶å¯åœ¨ [GitHub ä»“åº“](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/rag) ä¸­æ‰¾åˆ°ã€‚\n\nðŸ”— [Google Colab](https://colab.research.google.com/drive/1peJukr-9E1zCo1iAalbgDPJmNMydvQms?usp=sharing) \\| [GitHub Repo](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/rag)\n\n### å¯¼å…¥\n\næˆ‘ä»¬é¦–å…ˆå®‰è£…å¹¶å¯¼å…¥å¿…è¦çš„ Python åº“ã€‚\n\n```python\n!pip install llama-index\n!pip install llama-index-embeddings-huggingface\n!pip install peft\n!pip install auto-gptq\n!pip install optimum\n!pip install bitsandbytes\n## å¦‚æžœä¸æ˜¯åœ¨ Colab ä¸Šè¿è¡Œï¼Œè¯·ç¡®ä¿ä¹Ÿå®‰è£… transformers\n```\n\n```python\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\nfrom llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex\nfrom llama_index.core.retrievers import VectorIndexRetriever\nfrom llama_index.core.query_engine import RetrieverQueryEngine\nfrom llama_index.core.postprocessor import SimilarityPostprocessor\n```\n\n### è®¾ç½®çŸ¥è¯†åº“\n\næˆ‘ä»¬å¯ä»¥é€šè¿‡å®šä¹‰æˆ‘ä»¬çš„åµŒå…¥æ¨¡åž‹ã€å—å¤§å°å’Œå—é‡å æ¥é…ç½®æˆ‘ä»¬çš„çŸ¥è¯†åº“ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨æ¥è‡ªBAAIçš„\\~33Må‚æ•°[bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5)åµŒå…¥æ¨¡åž‹ï¼Œè¯¥æ¨¡åž‹å¯åœ¨Hugging Face hubä¸ŠèŽ·å–ã€‚å…¶ä»–åµŒå…¥æ¨¡åž‹é€‰é¡¹å¯ä»¥åœ¨è¿™ä¸ª[text embedding leaderboard](https://huggingface.co/spaces/mteb/leaderboard)ä¸Šæ‰¾åˆ°ã€‚\n\n```python\n## import any embedding model on HF hub\nSettings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n\nSettings.llm = None # we won't use LlamaIndex to set up LLM\nSettings.chunk_size = 256\nSettings.chunk_overlap = 25\n```\næŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åŠ è½½æºæ–‡æ¡£ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘æœ‰ä¸€ä¸ªåä¸ºâ€œ[*articles*](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/rag/articles)â€çš„æ–‡ä»¶å¤¹ï¼Œå…¶ä¸­åŒ…å«æˆ‘åœ¨[fat tails](https://towardsdatascience.com/pareto-power-laws-and-fat-tails-0355a187ee6a)ä¸Šå†™çš„3ç¯‡Mediumæ–‡ç« çš„PDFç‰ˆæœ¬ã€‚å¦‚æžœåœ¨Colabä¸­è¿è¡Œï¼Œæ‚¨å¿…é¡»ä»Ž[GitHub repo](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/rag)ä¸‹è½½æ–‡ç« æ–‡ä»¶å¤¹å¹¶æ‰‹åŠ¨ä¸Šä¼ åˆ°æ‚¨çš„ColabçŽ¯å¢ƒã€‚\n\nå¯¹äºŽè¯¥æ–‡ä»¶å¤¹ä¸­çš„æ¯ä¸ªæ–‡ä»¶ï¼Œä¸‹é¢çš„å‡½æ•°å°†ä»ŽPDFä¸­è¯»å–æ–‡æœ¬ï¼Œå°†å…¶æ‹†åˆ†æˆå—ï¼ˆåŸºäºŽä¹‹å‰å®šä¹‰çš„è®¾ç½®ï¼‰ï¼Œå¹¶å°†æ¯ä¸ªå—å­˜å‚¨åœ¨åä¸º*documents*çš„åˆ—è¡¨ä¸­ã€‚\n\n```python\ndocuments = SimpleDirectoryReader(\"articles\").load_data()\n```\nç”±äºŽè¿™äº›åšå®¢æ˜¯ç›´æŽ¥ä»ŽMediumä¸‹è½½ä¸ºPDFçš„ï¼Œå› æ­¤å®ƒä»¬æ›´åƒæ˜¯ç½‘é¡µï¼Œè€Œä¸æ˜¯æ ¼å¼è‰¯å¥½çš„æ–‡ç« ã€‚å› æ­¤ï¼Œä¸€äº›å—å¯èƒ½åŒ…å«ä¸Žæ–‡ç« æ— å…³çš„æ–‡æœ¬ï¼Œä¾‹å¦‚ç½‘é¡µæ ‡é¢˜å’ŒMediumæ–‡ç« æŽ¨èã€‚\n\nåœ¨ä¸‹é¢çš„ä»£ç å—ä¸­ï¼Œæˆ‘å¯¹documentsä¸­çš„å—è¿›è¡Œç²¾ç‚¼ï¼Œåˆ é™¤æ–‡ç« ä¸»ä½“å‰åŽçš„å¤§éƒ¨åˆ†å—ã€‚\n\n```python\nprint(len(documents)) # prints: 71\nfor doc in documents:\n    if \"Member-only story\" in doc.text:\n        documents.remove(doc)\n        continue\n\n    if \"The Data Entrepreneurs\" in doc.text:\n        documents.remove(doc)\n\n    if \" min read\" in doc.text:\n        documents.remove(doc)\n\nprint(len(documents)) # prints: 61\n```\næœ€åŽï¼Œæˆ‘ä»¬å¯ä»¥å°†ç²¾ç‚¼åŽçš„å—å­˜å‚¨åœ¨å‘é‡æ•°æ®åº“ä¸­ã€‚\n\n```python\nindex = VectorStoreIndex.from_documents(documents)\n```\n\n### è®¾ç½®æ£€ç´¢å™¨\n\nåœ¨æˆ‘ä»¬çš„çŸ¥è¯†åº“å»ºç«‹ä¹‹åŽï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ LlamaIndex çš„ *VectorIndexRetriever()* åˆ›å»ºä¸€ä¸ªæ£€ç´¢å™¨ï¼Œå®ƒè¿”å›žä¸Žç”¨æˆ·æŸ¥è¯¢æœ€ç›¸ä¼¼çš„ 3 ä¸ªå—ã€‚\n\n```python\n## set number of docs to retreive\ntop_k = 3\n\n## configure retriever\nretriever = VectorIndexRetriever(\n    index=index,\n    similarity_top_k=top_k,\n)\n```\næŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªæŸ¥è¯¢å¼•æ“Žï¼Œä½¿ç”¨æ£€ç´¢å™¨å’ŒæŸ¥è¯¢è¿”å›žä¸€ç»„ç›¸å…³çš„å—ã€‚\n\n```python\n## assemble query engine\nquery_engine = RetrieverQueryEngine(\n    retriever=retriever,\n    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.5)],\n)\n```\n\n### ä½¿ç”¨æŸ¥è¯¢å¼•æ“Ž\n\nçŽ°åœ¨ï¼Œéšç€æˆ‘ä»¬çš„çŸ¥è¯†åº“å’Œæ£€ç´¢ç³»ç»Ÿçš„å»ºç«‹ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨å®ƒæ¥è¿”å›žä¸ŽæŸ¥è¯¢ç›¸å…³çš„å†…å®¹ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†ä¼ é€’æˆ‘ä»¬å‘ShawGPTï¼ˆYouTubeè¯„è®ºå›žå¤è€…ï¼‰æå‡ºçš„ç›¸åŒæŠ€æœ¯é—®é¢˜ï¼Œæ¥è‡ª[ä¸Šä¸€ç¯‡æ–‡ç« ](https://readmedium.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32)ã€‚\n\n```python\nquery = \"What is fat-tailedness?\"\nresponse = query_engine.query(query)\n```\næŸ¥è¯¢å¼•æ“Žè¿”å›žä¸€ä¸ªå“åº”å¯¹è±¡ï¼Œå…¶ä¸­åŒ…å«æ–‡æœ¬ã€å…ƒæ•°æ®å’Œç›¸å…³å—çš„ç´¢å¼•ã€‚ä¸‹é¢çš„ä»£ç å—è¿”å›žè¯¥ä¿¡æ¯çš„æ›´æ˜“è¯»ç‰ˆæœ¬ã€‚\n\n```python\n## reformat response\ncontext = \"Context:\\n\"\nfor i in range(top_k):\n    context = context + response.source_nodes[i].text + \"\\n\\n\"\n\nprint(context)\n```\n\n```python\nContext:\nSome of the controversy might be explained by the observation that log-\nnormal distributions behave like Gaussian for low sigma and like Power Law\nat high sigma [2].\nHowever, to avoid controversy, we can depart (for now) from whether some\ngiven data fits a Power Law or not and focus instead on fat tails.\nFat-tailedness â€” measuring the space between Mediocristan\nand Extremistan\nFat Tails are a more general idea than Pareto and Power Law distributions.\nOne way we can think about it is that â€œfat-tailednessâ€ is the degree to which\nrare events drive the aggregate statistics of a distribution. From this point of\nview, fat-tailedness lives on a spectrum from not fat-tailed (i.e. a Gaussian) to\nvery fat-tailed (i.e. Pareto 80 â€“ 20).\nThis maps directly to the idea of Mediocristan vs Extremistan discussed\nearlier. The image below visualizes different distributions across this\nconceptual landscape [2].\n\nprint(\"mean kappa_1n = \" + str(np.mean(kappa_dict[filename])))\n    print(\"\")\nMean Îº (1,100) values from 1000 runs for each dataset. Image by author.\nThese more stable results indicate Medium followers are the most fat-tailed,\nfollowed by LinkedIn Impressions and YouTube earnings.\nNote: One can compare these values to Table III in ref [3] to better understand each\nÎº value. Namely, these values are comparable to a Pareto distribution with Î±\nbetween 2 and 3.\nAlthough each heuristic told a slightly different story, all signs point toward\nMedium followers gained being the most fat-tailed of the 3 datasets.\nConclusion\nWhile binary labeling data as fat-tailed (or not) may be tempting, fat-\ntailedness lives on a spectrum. Here, we broke down 4 heuristics for\nquantifying how fat-tailed data are.\n\nPareto, Power Laws, and Fat Tails\nWhat they donâ€™t teach you in statistics\ntowardsdatascience.com\nAlthough Pareto (and more generally power law) distributions give us a\nsalient example of fat tails, this is a more general notion that lives on a\nspectrum ranging from thin-tailed (i.e. a Gaussian) to very fat-tailed (i.e.\nPareto 80 â€“ 20).\nThe spectrum of Fat-tailedness. Image by author.\nThis view of fat-tailedness provides us with a more flexible and precise way of\ncategorizing data than simply labeling it as a Power Law (or not). However,\nthis begs the question: how do we define fat-tailedness?\n4 Ways to Quantify Fat Tails\n```\n\n### å°† RAG æ·»åŠ åˆ° LLM\n\næˆ‘ä»¬é¦–å…ˆä»Ž Hugging Face hub ä¸‹è½½ [å¾®è°ƒæ¨¡åž‹](https://readmedium.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32)ã€‚\n\n```python\n## load fine-tuned model from hub\nfrom peft import PeftModel, PeftConfig\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name,\n                                             device_map=\"auto\",\n                                             trust_remote_code=False,\n                                             revision=\"main\")\n\nconfig = PeftConfig.from_pretrained(\"shawhin/shawgpt-ft\")\nmodel = PeftModel.from_pretrained(model, \"shawhin/shawgpt-ft\")\n\n## load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n```\nä½œä¸ºåŸºçº¿ï¼Œæˆ‘ä»¬å¯ä»¥è§‚å¯Ÿæ¨¡åž‹åœ¨æ²¡æœ‰ä»»ä½•æ–‡ç« ä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹å¦‚ä½•å›žåº”æŠ€æœ¯é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨ lambda å‡½æ•°åˆ›å»ºä¸€ä¸ªæç¤ºæ¨¡æ¿ï¼Œè¯¥å‡½æ•°æŽ¥å—è§‚ä¼—è¯„è®ºå¹¶è¿”å›ž LLM çš„æç¤ºã€‚æœ‰å…³æ­¤æç¤ºæ¥æºçš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§æœ¬ç³»åˆ—çš„ [ä¸Šä¸€ç¯‡æ–‡ç« ](https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32#5aad)ã€‚\n\n```python\n## prompt (no context)\nintstructions_string = f\"\"\"ShawGPT, functioning as a virtual data science \\\nconsultant on YouTube, communicates in clear, accessible language, escalating \\\nto technical depth upon request. It reacts to feedback aptly and ends \\\nresponses with its signature 'â€“ShawGPT'.\n\nShawGPT will tailor the length of its responses to match the viewer's comment, \\\nproviding concise acknowledgments to brief expressions of gratitude or \\\nfeedback, thus keeping the interaction natural and engaging.\n\nPlease respond to the following comment.\n\"\"\"\n\nprompt_template = lambda comment: f'''[INST] {intstructions_string} \\n{comment} \\n[/INST]'''\ncomment = \"What is fat-tailedness?\"\n\nprompt = prompt_template(comment)\nprint(prompt)\n```\n\n```python\n[INST] ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. It reacts to feedback aptly and ends responses with its signature 'â€“ShawGPT'. ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, thus keeping the interaction natural and engaging.\n\nPlease respond to the following comment.\n \nWhat is fat-tailedness? \n[/INST]\n```\næŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç å°†æ­¤æç¤ºä¼ é€’ç»™æ¨¡åž‹ã€‚\n\n```python\nmodel.eval()\n\ninputs = tokenizer(prompt, return_tensors=\"pt\")\noutputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), \n                          max_new_tokens=280)\n\nprint(tokenizer.batch_decode(outputs)[0])\n```\nè¿™æ˜¯æ¨¡åž‹çš„å“åº”ï¼ˆæ²¡æœ‰ä¸Šä¸‹æ–‡ï¼‰ã€‚\n\n```python\nGreat question!\n\nFat-tailedness is a statistical property of a distribution. In simple terms, \nit refers to the presence of extreme outliers or heavy tails in the \ndistribution.\n\nFor instance, consider the distribution of heights in a population. A normal \ndistribution would have most people clustered around an average height with a \nfew people deviating slightly from the mean. However, in a fat-tailed \ndistribution, you would observe a larger number of people being significantly \ntaller or shorter than the average.\n\nIn finance, fat-tailedness is often used to describe the presence of extreme \nmarket moves or outliers in financial returns. It's an important concept in \nrisk management and financial modeling.\n\nI hope this explanation helps! Let me know if you have any questions.\nâ€“ShawGPT\n```\nè™½ç„¶å“åº”çš„é£Žæ ¼å’Œæ ¼å¼éƒ½å¾ˆå¥½ï¼Œä½†æ¨¡åž‹å¯¹ fat-tailedness çš„è§£é‡Šä¸Žæˆ‘åœ¨æˆ‘çš„ [è§†é¢‘](https://www.youtube.com/playlist?list=PLz-ep5RbHosVrT89BRNX-IGKHYxvx9wqi) å’Œ [åšå®¢ç³»åˆ—](https://towardsdatascience.com/pareto-power-laws-and-fat-tails-0355a187ee6a) ä¸­å®šä¹‰çš„æœ‰æ‰€ä¸åŒã€‚\n\nè®©æˆ‘ä»¬çœ‹çœ‹å½“æˆ‘ä»¬åŒ…å«é€‚å½“çš„ä¸Šä¸‹æ–‡æ—¶æ¨¡åž‹çš„å“åº”ä¼šå‘ç”Ÿä»€ä¹ˆã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åˆ›å»ºå¦ä¸€ä¸ªæç¤ºæ¨¡æ¿ï¼Œè¯¥æ¨¡æ¿ä¹Ÿå¯ä»¥ä»Ž RAG ç³»ç»Ÿä¸­èŽ·å–ä¸Šä¸‹æ–‡ã€‚\n\n```python\n## prompt (with context)\nprompt_template_w_context = lambda context, comment: f\"\"\"[INST]ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. \\\nIt reacts to feedback aptly and ends responses with its signature 'â€“ShawGPT'. \\\nShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, \\\nthus keeping the interaction natural and engaging.\n\n{context}\nPlease respond to the following comment. Use the context above if it is helpful.\n\n{comment}\n[/INST]\n\"\"\"\n```\næŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æŸ¥è¯¢å¼•æ“Žä¸­çš„ä¸Šä¸‹æ–‡å’Œè§‚ä¼—è¯„è®ºä¼ é€’ç»™æ¨¡åž‹ã€‚\n\n```python\nprompt = prompt_template_w_context(context, comment)\n\ninputs = tokenizer(prompt, return_tensors=\"pt\")\noutputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=280)\n\nprint(tokenizer.batch_decode(outputs)[0])\n```\nè¿™æ˜¯æ–°çš„å“åº”ï¼ˆå¸¦ä¸Šä¸‹æ–‡ï¼‰ã€‚\n\n```python\nGreat question!\n\nFat-tailedness is a property of a distribution that describes how much the \npresence of extreme values (rare events) affects the overall statistics of the \ndistribution.\n\nIn the context of the video, the author discusses how Mediocristan and \nExtremistan distributions differ in terms of fat-tailedness. Mediocristan \ndistributions have a relatively small impact from extreme values, while \nExtremistan distributions are heavily influenced by extreme values.\n\nThe author also mentions that fat-tailedness is a more general concept than \nspecific distributions like Pareto or Power Laws. Instead, it lives on a \nspectrum ranging from thin-tailed (Gaussian) to very fat-tailed (Pareto 80-20).\n\nI hope that helps clarify things a bit! Let me know if you have any questions.\nâ€“ShawGPT\n```\nè¿™æ¯”æ²¡æœ‰ä¸Šä¸‹æ–‡çš„å“åº”æ›´å¥½åœ°æ•æ‰äº†æˆ‘å¯¹ fat tails çš„è§£é‡Šï¼Œå¹¶ä¸”ç”šè‡³æåˆ°äº† Mediocristan å’Œ Extremistan çš„ç»†åˆ†æ¦‚å¿µã€‚\n\n## æŽ¥ä¸‹æ¥æ˜¯ä»€ä¹ˆï¼Ÿ\n\nåœ¨è¿™é‡Œï¼Œæˆ‘ä¸ºåˆå­¦è€…æä¾›äº†ä¸€ä¸ªå…³äºŽ RAG çš„å‹å¥½ä»‹ç»ï¼Œå¹¶åˆ†äº«äº†å¦‚ä½•ä½¿ç”¨ LlamaIndex å®žçŽ°å®ƒçš„å…·ä½“ç¤ºä¾‹ã€‚RAG ä½¿æˆ‘ä»¬èƒ½å¤Ÿé€šè¿‡å¯æ›´æ–°å’Œç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†æ¥æ”¹å–„ LLM ç³»ç»Ÿã€‚\n\nè™½ç„¶æœ€è¿‘çš„ AI çƒ­æ½®ä¸»è¦é›†ä¸­åœ¨æž„å»º AI åŠ©æ‰‹ä¸Šï¼Œä½†ä¸€ä¸ªå¼ºå¤§çš„ï¼ˆä½†ä¸é‚£ä¹ˆæµè¡Œçš„ï¼‰åˆ›æ–°æ¥è‡ªäºŽæ–‡æœ¬åµŒå…¥ï¼ˆå³æˆ‘ä»¬ç”¨æ¥è¿›è¡Œæ£€ç´¢çš„ä¸œè¥¿ï¼‰ã€‚åœ¨æœ¬ç³»åˆ—çš„ä¸‹ä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†æ›´è¯¦ç»†åœ°æŽ¢è®¨ **æ–‡æœ¬åµŒå…¥**ï¼ŒåŒ…æ‹¬å®ƒä»¬å¦‚ä½•ç”¨äºŽ **è¯­ä¹‰æœç´¢** å’Œ **åˆ†ç±»ä»»åŠ¡**ã€‚\n\n**æ›´å¤šå…³äºŽ LLM çš„å†…å®¹ ðŸ‘‡**\n\n## èµ„æº\n\n**è¿žæŽ¥**: [æˆ‘çš„ç½‘ç«™](https://shawhintalebi.com/) \\| [é¢„çº¦ç”µè¯](https://calendly.com/shawhintalebi)\n\n**ç¤¾äº¤**: [YouTube ðŸŽ¥](https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA) \\| [LinkedIn](https://www.linkedin.com/in/shawhintalebi/) \\| [Instagram](https://www.instagram.com/shawhintalebi)\n\n**æ”¯æŒ**: [è¯·æˆ‘å–æ¯å’–å•¡](https://www.buymeacoffee.com/shawhint) â˜•ï¸\n\n\\[1] [RAG \\> FT (ç»éªŒæ€§)](https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb)\n\n\\[2] [LlamaIndex ç½‘ç»œç ”è®¨ä¼šï¼šä¸ºç”Ÿäº§æž„å»º LLM åº”ç”¨ç¨‹åºï¼Œç¬¬ä¸€éƒ¨åˆ†ï¼ˆä¸Ž Anyscale è”åˆä¸»æŒï¼‰](https://www.youtube.com/watch?v=efbn-3tPI_M)\n\n\\[3] [LlamaIndex æ–‡æ¡£](https://docs.llamaindex.ai/en/stable/understanding/loading/loading.html)\n\n\\[4] [LlamaIndex ç½‘ç»œç ”è®¨ä¼šï¼šä½¿ RAG å‡†å¤‡å¥½ç”Ÿäº§](https://www.youtube.com/watch?v=Zj5RCweUHIk&list=WL&index=4)\n\n"},{"lang":"zh","group":"blog","slug":"blog/how-to-run-nvidia-llama-3-1-nemotron-70b-instruct-locally-a58ad283aaff","frontmatter":{"title":"å¦‚ä½•åœ¨æœ¬åœ°è¿è¡Œ Nvidia çš„ llama-3.1-nemotron-70b-instruct","meta_title":"å¦‚ä½•åœ¨æœ¬åœ°è¿è¡Œ Nvidia çš„ llama-3.1-nemotron-70b-instruct","description":"åœ¨æœ¬åœ°è¿è¡Œå¤§åž‹è¯­è¨€æ¨¡åž‹ (LLM) åœ¨å¼€å‘äººå‘˜ã€ç ”ç©¶äººå‘˜å’Œ AI çˆ±å¥½è€…ä¸­è¶Šæ¥è¶Šå—æ¬¢è¿Žã€‚å…¶ä¸­ä¹‹ä¸€å°±æ˜¯â€¦â€¦","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*fqVKJkw5sQvLtIsyCcengQ.png","categories":["Programming","Technology","Science"],"author":"Rifx.Online","tags":["Nvidia","llama","Ollama","llama.cpp","Transformers"],"draft":false,"slug":"blog/how-to-run-nvidia-llama-3-1-nemotron-70b-instruct-locally-a58ad283aaff"},"content":"\n\n\nåœ¨å¼€å‘è€…ã€ç ”ç©¶äººå‘˜å’Œ AI çˆ±å¥½è€…ä¸­ï¼Œæœ¬åœ°è¿è¡Œå¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰å˜å¾—è¶Šæ¥è¶Šå—æ¬¢è¿Žã€‚å…¶ä¸­ä¸€ä¸ªå¼•èµ·å¹¿æ³›å…³æ³¨çš„æ¨¡åž‹æ˜¯ llama-3.1-nemotron-70b-instructï¼Œè¿™æ˜¯ NVIDIA å®šåˆ¶çš„å¼ºå¤§ LLMï¼Œæ—¨åœ¨å¢žå¼ºç”Ÿæˆå“åº”çš„æœ‰ç”¨æ€§ã€‚åœ¨æœ¬ç»¼åˆæŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†æŽ¢è®¨å¤šç§æ–¹æ³•ï¼Œä»¥ä¾¿åœ¨æ‚¨çš„æœ¬åœ°æœºå™¨ä¸Šè¿è¡Œæ­¤æ¨¡åž‹ï¼Œé¦–å…ˆä»‹ç»ç”¨æˆ·å‹å¥½çš„ Ollama å¹³å°ã€‚\n\n> åœ¨å¼€å§‹ä¹‹å‰ï¼Œå¦‚æžœæ‚¨æ­£åœ¨å¯»æ‰¾ä¸€ä¸ªä¸€ä½“åŒ–çš„ AI å¹³å°ï¼Œä»¥ä¾¿åœ¨ä¸€ä¸ªåœ°æ–¹ç®¡ç†æ‰€æœ‰ AI è®¢é˜…ï¼ŒåŒ…æ‹¬æ‰€æœ‰ LLMï¼ˆå¦‚ GPT-o1ã€Llama 3.1ã€Claude 3.5 Sonnetã€Google Geminiã€æœªå®¡æŸ¥çš„ LLMï¼‰å’Œå›¾åƒç”Ÿæˆæ¨¡åž‹ï¼ˆFLUXã€Stable Diffusion ç­‰ï¼‰ï¼Œè¯·ä½¿ç”¨ Anakin AI æ¥ç®¡ç†å®ƒä»¬ï¼\n\n\n\n## æ–¹æ³• 1ï¼šä½¿ç”¨ Ollama æœ¬åœ°è¿è¡Œ llama-3.1-nemotron-70b-instruct\n\nOllama æ˜¯ä¸€ä¸ªå‡ºè‰²çš„å·¥å…·ï¼Œç”¨äºŽæœ¬åœ°è¿è¡Œ LLMï¼Œæä¾›ç®€å•çš„è®¾ç½®è¿‡ç¨‹å¹¶æ”¯æŒå¤šç§æ¨¡åž‹ï¼ŒåŒ…æ‹¬ llama-3.1-nemotron-70b-instructã€‚\n\n### å®‰è£…\n\n1. è®¿é—®å®˜æ–¹ Ollama ç½‘ç«™ ([https://ollama.ai](https://ollama.ai/))ï¼Œä¸‹è½½é€‚åˆæ‚¨æ“ä½œç³»ç»Ÿçš„ç‰ˆæœ¬ã€‚\n2. é€šè¿‡åœ¨ç»ˆç«¯ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥å®‰è£… Ollamaï¼š\n\n\n```python\ncurl https://ollama.ai/install.sh | sh\n```\n\n### è¿è¡Œ llama-3.1-nemotron\n\nå®‰è£… Ollama åŽï¼Œæ‚¨å¯ä»¥é€šè¿‡ä¸€ä¸ªç®€å•çš„å‘½ä»¤è½»æ¾è¿è¡Œ llama-3.1-nemotron-70b-instruct æ¨¡åž‹ï¼š\n\n```python\nollama run nemotron:70b-instruct-q5_K_M\n```\nè¯¥å‘½ä»¤å°†åœ¨æ‚¨çš„ç³»ç»Ÿä¸Šä¸‹è½½æ¨¡åž‹ï¼ˆå¦‚æžœå°šæœªå­˜åœ¨ï¼‰ï¼Œå¹¶å¯åŠ¨ä¸€ä¸ªäº¤äº’å¼ä¼šè¯ã€‚\n\n### ä½¿ç”¨æ¨¡åž‹\n\nåœ¨æ¨¡åž‹åŠ è½½åŽï¼Œæ‚¨å¯ä»¥é€šè¿‡è¾“å…¥æç¤ºå¼€å§‹ä¸Žå…¶äº’åŠ¨ã€‚ä¾‹å¦‚ï¼š\n\n```python\n>>> What are the key features of llama-3.1-nemotron-70b-instruct?\n\nLlama-3.1-Nemotron-70B-Instruct is a large language model with several key features:\n1. Customized by NVIDIA: The model has been fine-tuned by NVIDIA to improve the helpfulness and quality of its responses.\n2. Based on Llama 3.1: It builds upon the Llama 3.1 architecture, which is known for its strong performance across various tasks.\n3. 70 billion parameters: This large parameter count allows for complex reasoning and a wide range of capabilities.\n4. Instruct-tuned: The model is specifically designed to follow instructions and generate helpful responses to user queries.\n5. RLHF training: It has been trained using Reinforcement Learning from Human Feedback, specifically the REINFORCE algorithm.\n6. Specialized reward model: The training process utilized Llama-3.1-Nemotron-70B-Reward for optimization.\n7. HelpSteer2-Preference prompts: These were used during the training process to further improve the model's helpfulness.\n8. Extended context length: Like other Llama 3.1 models, it likely supports a longer context window of 128K tokens.\n9. Multilingual capabilities: It can understand and generate text in multiple languages.\n10. Strong reasoning abilities: The model excels in tasks requiring complex reasoning and problem-solving.\nThese features make llama-3.1-nemotron-70b-instruct a powerful and versatile language model suitable for a wide range of applications, from general conversation to specialized tasks in various domains.\n```\nå¯¹äºŽæ›´é«˜çº§çš„ç”¨ä¾‹ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨åƒ Langchain è¿™æ ·çš„åº“å°† Ollama ä¸Ž Python é›†æˆã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªç®€å•çš„ç¤ºä¾‹ï¼š\n\n```python\npython\n\nfrom langchain.llms import Ollama\n\nollama = Ollama(base_url=\"http://localhost:11434\", model=\"nemotron:70b-instruct-q5_K_M\")\nresponse = ollama.generate(\"Explain the concept of quantum entanglement.\")\nprint(response)\n```\nè¿™ä½¿æ‚¨èƒ½å¤Ÿæ— ç¼åœ°å°†æ¨¡åž‹é›†æˆåˆ°æ‚¨çš„ Python é¡¹ç›®å’Œåº”ç”¨ç¨‹åºä¸­ã€‚\n\n## æ–¹æ³• 2ï¼šä½¿ç”¨ llama.cpp\n\nllama.cpp æ˜¯ä¸€ä¸ªæµè¡Œçš„ C++ å®žçŽ°çš„ Llama æ¨¡åž‹æŽ¨ç†ï¼Œé’ˆå¯¹ CPU ä½¿ç”¨è¿›è¡Œäº†ä¼˜åŒ–ã€‚è™½ç„¶å®ƒå¯èƒ½éœ€è¦æ¯” Ollama æ›´å¤šçš„è®¾ç½®ï¼Œä½†å®ƒæä¾›äº†æ›´å¤§çš„çµæ´»æ€§å’Œå¯¹æ¨¡åž‹å‚æ•°çš„æŽ§åˆ¶ã€‚\n\n### å®‰è£…\n\n1. å…‹éš† llama.cpp ä»“åº“ï¼š\n\n```python\ngit clone https://github.com/ggerganov/llama.cpp.git\ncd llama.cpp\n```\n1. æž„å»ºé¡¹ç›®ï¼š\n\n```python\nmake\n```\n\n### ä¸‹è½½æ¨¡åž‹\n\nè¦è¿è¡Œ llama-3.1-nemotron-70b-instructï¼Œæ‚¨éœ€è¦ä¸‹è½½æ¨¡åž‹æƒé‡ã€‚è¿™äº›é€šå¸¸ä»¥ GGML æˆ– GGUF æ ¼å¼æä¾›ã€‚æ‚¨å¯ä»¥åœ¨ Hugging Face ç­‰å¹³å°ä¸Šæ‰¾åˆ°é¢„å…ˆè½¬æ¢çš„æ¨¡åž‹ã€‚\n\n```python\nmkdir models\ncd models\nwget https://huggingface.co/TheBloke/Llama-3.1-Nemotron-70B-Instruct-GGUF/resolve/main/llama-3.1-nemotron-70b-instruct.Q4_K_M.gguf\n```\n\n### è¿è¡Œæ¨¡åž‹\n\nä¸€æ—¦ä½ æ‹¥æœ‰æ¨¡åž‹æ–‡ä»¶ï¼Œå°±å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿è¡Œå®ƒï¼š\n\n```python\n./main -m models/llama-3.1-nemotron-70b-instruct.Q4_K_M.gguf -n 1024 -p \"Hello, how are you today?\"\n```\nè¯¥å‘½ä»¤åŠ è½½æ¨¡åž‹å¹¶ç”Ÿæˆå¯¹ç»™å®šæç¤ºçš„å“åº”ã€‚ä½ å¯ä»¥è°ƒæ•´å„ç§å‚æ•°ï¼Œæ¯”å¦‚ç”Ÿæˆçš„ä»¤ç‰Œæ•°é‡ (-n) æˆ–æ¸©åº¦ä»¥æŽ§åˆ¶éšæœºæ€§ã€‚\n\n## æ–¹æ³• 3ï¼šä½¿ç”¨ Hugging Face Transformers\n\nHugging Face çš„ Transformers åº“æä¾›äº†ä¸€ä¸ªé«˜å±‚æ¬¡çš„ APIï¼Œç”¨äºŽå¤„ç†å„ç§è¯­è¨€æ¨¡åž‹ï¼ŒåŒ…æ‹¬ llama-3.1-nemotron-70b-instructã€‚\n\n**å®‰è£…**\n\né¦–å…ˆï¼Œå®‰è£…å¿…è¦çš„åº“ï¼š\n\n\n```python\npip install transformers torch accelerate\n```\n**è¿è¡Œæ¨¡åž‹**\n\nä»¥ä¸‹æ˜¯ä¸€ä¸ªåŠ è½½å’Œä½¿ç”¨æ¨¡åž‹çš„ Python è„šæœ¬ï¼š\n\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nmodel_name = \"meta-llama/Llama-3.1-Nemotron-70b-instruct\"\n## Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n## Prepare the input\nprompt = \"Explain the concept of quantum computing in simple terms.\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n## Generate the response\nwith torch.no_grad():\n    outputs = model.generate(**inputs, max_new_tokens=100)\n## Decode and print the response\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response)\n```\nè¿™ç§æ–¹æ³•å…è®¸å¯¹æ¨¡åž‹çš„è¡Œä¸ºè¿›è¡Œæ›´ç»†ç²’åº¦çš„æŽ§åˆ¶ï¼Œå¹¶ä¸Žå…¶ä»– Hugging Face å·¥å…·å’Œç®¡é“é›†æˆã€‚\n\n## ç»“è®º\n\nåœ¨æœ¬åœ°è¿è¡Œ llama-3.1-nemotron-70b-instruct ä¸ºå¼€å‘è€…å’Œç ”ç©¶äººå‘˜æ‰“å¼€äº†æ— é™å¯èƒ½ã€‚æ— è®ºæ‚¨é€‰æ‹© Ollama çš„ç®€å•æ€§ã€llama.cpp çš„çµæ´»æ€§ï¼Œè¿˜æ˜¯ Hugging Face Transformers çš„é›†æˆåŠŸèƒ½ï¼Œæ‚¨çŽ°åœ¨éƒ½æœ‰å·¥å…·å¯ä»¥åœ¨è‡ªå·±çš„ç¡¬ä»¶ä¸Šåˆ©ç”¨è¿™ä¸€å…ˆè¿›è¯­è¨€æ¨¡åž‹çš„å¼ºå¤§èƒ½åŠ›ã€‚åœ¨æŽ¢ç´¢ llama-3.1-nemotron-70b-instruct çš„èƒ½åŠ›æ—¶ï¼Œè¯·è®°ä½åœ¨æ€§èƒ½ä¸Žèµ„æºé™åˆ¶ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œå¹¶å§‹ç»ˆè€ƒè™‘æ‚¨åº”ç”¨çš„ä¼¦ç†å½±å“ã€‚è´Ÿè´£ä»»çš„ä½¿ç”¨ï¼Œè¿™ä¸ªæ¨¡åž‹å¯ä»¥æˆä¸ºæŽ¨åŠ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œ AI é©±åŠ¨åº”ç”¨å¯èƒ½æ€§çš„å®è´µèµ„äº§ã€‚\n\n"},{"lang":"zh","group":"blog","slug":"blog/how-to-use-chatgpt-for-blogging-7ed5cba2f32b","frontmatter":{"title":"å¦‚ä½•ä½¿ç”¨ ChatGPT å†™åšå®¢","meta_title":"å¦‚ä½•ä½¿ç”¨ ChatGPT å†™åšå®¢","description":"æœ¬æ–‡ä»‹ç»äº†ä½¿ç”¨ChatGPTè¿›è¡Œåšå®¢å†™ä½œçš„ä¹ä¸ªæ­¥éª¤ï¼Œå¼ºè°ƒäº†AIåœ¨å†…å®¹åˆ›ä½œä¸­çš„é‡è¦æ€§ã€‚æ­¥éª¤åŒ…æ‹¬è¯·æ±‚å¤§çº²ã€ç”Ÿæˆè®¨è®ºè¦ç‚¹ã€æ ¡å¯¹å’Œä¿®æ”¹AIè¾“å‡ºã€ç”Ÿæˆå®Œæ•´æ–‡ç« ã€åˆ›å»ºå¼•è¨€å’Œç»“è®ºï¼Œä»¥åŠæ‰‹åŠ¨ç¼–è¾‘ã€‚ä½œè€…è¿˜æåˆ°é€šè¿‡ChatGPT CanvasåŠŸèƒ½å¯ä»¥æé«˜å†™ä½œæ•ˆçŽ‡ï¼Œé€‚åˆéœ€è¦å¿«é€Ÿç”Ÿæˆé«˜è´¨é‡SEOæ–‡ç« çš„ç”¨æˆ·ã€‚","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*QS7seNg2jfuTz1Be.jpeg","categories":["Programming/Scripting","Marketing/Seo","Chatbots"],"author":"Rifx.Online","tags":["ChatGPT","prompts","blogging","SEO","Canvas"],"draft":false,"slug":"blog/how-to-use-chatgpt-for-blogging-7ed5cba2f32b"},"content":"\n\n\n### (æˆ‘æµ‹è¯•è¿‡çš„9ä¸ªæ­¥éª¤)\n\n\n\næ¯ä¸ªäººéƒ½åœ¨ä½¿ç”¨AIè¿›è¡Œå†™ä½œã€‚å¸‚åœºè¥é”€äººå‘˜ã€é¦–å¸­æ‰§è¡Œå®˜ã€å†…å®¹å¼€å‘è€…ã€å°åž‹ä¼ä¸šä¸»ã€‚\n\næˆ‘ä»¬æ‰€æœ‰äººã€‚\n\né˜…è¯»[å…è´¹æ•…äº‹](https://mysson.medium.com/7ed5cba2f32b?source=friends_link&sk=2881f3b15f541210f91fbc8834bc55d4)\n\næƒ³æƒ³ä¸‰å¹´å‰æˆ‘ä»¬åšå®šåœ°è®¤ä¸ºï¼š\n\n> AIä¸ä¼šå–ä»£ä½œå®¶ã€‚\n\nå¦‚æžœä½ è¿˜æ²¡æœ‰ä½¿ç”¨AIè¿›è¡Œåšå®¢å†™ä½œï¼Œæ˜¯æ—¶å€™é‡æ–°è€ƒè™‘ä½ çš„ç«‹åœºäº†ï¼Œå°¤å…¶æ˜¯å¦‚æžœä½ çš„ä¸šåŠ¡éœ€è¦ä¸æ–­äº§ç”Ÿå†…å®¹ä»¥ä¿æŒç›¸å…³æ€§å’Œç«žäº‰åŠ›ã€‚\n\næœ‰æ›´å¥½çš„AIåšå®¢å·¥å…·ï¼Œæ¯”å¦‚[Koala AI](https://koala.sh/register?via=mysson)ï¼Œä½†åƒChatgptå’ŒClaudeè¿™æ ·çš„æµè¡ŒèŠå¤©æœºå™¨äººåŒæ ·å¼ºå¤§ï¼Œå°½ç®¡ä¸å¤Ÿæµç•…ã€‚\n\næœ‰äº›å·¥å…·æ¯”å…¶ä»–å·¥å…·æ›´å¥½ã€‚\n\nChatgptä¸å¦‚Claude 3\\.5 Sonnetï¼ˆæ–°ç‰ˆæœ¬ï¼‰å¼ºå¤§ï¼Œä½†å®ƒä»ç„¶ç›¸å½“æœ‰èƒ½åŠ›ï¼Œè®¸å¤šä½œå®¶æ­£åœ¨åˆ©ç”¨å®ƒæ¥æ‰©å¤§ä»–ä»¬çš„å†…å®¹å¼€å‘å·¥ä½œã€‚\n\n## å¦‚ä½•ä½¿ç”¨Chatgptè¿›è¡Œåšå®¢å†™ä½œï¼ˆ9ä¸ªæ­¥éª¤ï¼‰\n\nè¦å¼€å§‹ä½¿ç”¨ChatGPTï¼Œæ‚¨éœ€è¦è¾“å…¥ä¸€ä¸ª[AIæç¤º](https://aimode.co/ai-prompts-engineering/)ã€‚ç®€å•æ¥è¯´ï¼ŒAIæç¤ºæ˜¯ä¸€ç»„æŒ‡å¯¼AIç”Ÿæˆæ‰€éœ€æ–‡æœ¬çš„æŒ‡ä»¤ã€‚\n\nä¾‹å¦‚ï¼Œå¦‚æžœæ‚¨éœ€è¦å¸®åŠ©æ’°å†™ä¸€ç¯‡å…³äºŽâ€œAIåœ¨åšå®¢å†™ä½œä¸­çš„æœªæ¥â€çš„åšå®¢æ–‡ç« ï¼Œæ‚¨å¯ä»¥è¾“å…¥ç±»ä¼¼äºŽä»¥ä¸‹çš„æç¤ºï¼š\n\n\n> â€˜ç”¨markdownç”Ÿæˆä¸€ç¯‡å…³äºŽAIåœ¨åšå®¢å†™ä½œä¸­çš„æœªæ¥çš„SEOæ–‡ç« ã€‚â€™ã€‚\n\nä½†è¯·è®°ä½ï¼Œæ‚¨æ˜¯è¿™é‡Œçš„ä¸»å¯¼ã€‚æ‚¨ä»ŽChatGPTèŽ·å¾—çš„è¾“å‡ºåªæ˜¯ä¸€ä¸ªèµ·ç‚¹ã€‚æ‚¨éœ€è¦æ ¡å¯¹ã€ä¿®æ”¹å’Œæ ¸å®žäº‹å®žï¼Œä½¿å…¶çœŸæ­£å±žäºŽæ‚¨ã€‚\n\nä¸ºäº†æˆåŠŸä½¿ç”¨ChatGPTåˆ›å»ºä¼˜è´¨å†…å®¹ï¼Œæˆ‘å»ºè®®æ‚¨éµå¾ªæˆ‘çš„ä¹ä¸ªæ­¥éª¤æ–¹æ³•ï¼š\n\n\n> 1\\) è¯·æ±‚ä¸€ä¸ªå¤§çº²\n\n\n> 2\\) ä¸ºæ¯ä¸ªéƒ¨åˆ†ç”Ÿæˆç‹¬ç‰¹çš„è®¨è®ºç‚¹\n\n\n> 3\\) é—®AIæ˜¯å¦é—æ¼äº†ä»€ä¹ˆ\n\n\n> 4\\) æç¤ºä»¥é¿å…AIæ£€æµ‹\n\n\n> 5\\) è¾“å…¥æ‚¨çš„æ–‡ç« ç”Ÿæˆæç¤º\n\n\n> 6\\) åœ¨ç”Ÿæˆå…¶ä½™æ–‡ç« ä¹‹å‰è¯·æ±‚ä¿®æ”¹ã€‚\n\n\n> 7\\) å®Œæˆç”Ÿæˆå…¶ä½™çš„æ–‡ç« ã€‚\n\n\n> 8\\) ç”Ÿæˆä»‹ç»ã€ç»“è®ºã€SEOæ ‡é¢˜å’Œå…ƒæè¿°éƒ¨åˆ†\n\n\n> 9\\) åœ¨å‘å¸ƒä¹‹å‰æ‰‹åŠ¨ç¼–è¾‘å’Œæ ¡å¯¹æ‚¨çš„æ–‡ç« ã€‚\n\n### 1\\) æç¤ºä»¥èŽ·å–åšå®¢å¤§çº²\n\nç¬¬ä¸€æ­¥æ˜¯è¯·æ±‚ ChatGPT ä¸ºæ‚¨çš„æ–‡ç« æä¾›è¯¦ç»†çš„åšå®¢å¤§çº²ã€‚\n\nç¤ºä¾‹æç¤ºï¼š\n\n> ç”Ÿæˆä¸€ä¸ªè¯¦ç»†è€Œå…¨é¢çš„åšå®¢å¤§çº²ï¼Œç¡®ä¿æ‚¨æ¶µç›–äº†æ‰€æœ‰å†…å®¹ï¼Œä»¥ä½¿è¯¥åšå®¢ç¬¦åˆç”¨æˆ·æ„å›¾ã€‚ä¸»é¢˜æ˜¯â€œå¦‚ä½•ä½¿ç”¨ ChatGPT è¿›è¡Œåšå®¢å†™ä½œã€‚â€\n\n### 2\\) ç”Ÿæˆè®¨è®ºè¦ç‚¹\n\nä¸‹ä¸€æ­¥æ˜¯è¯· Chatgpt æ”¹è¿›ç”Ÿæˆçš„æçº²ï¼Œæ·»åŠ è®¨è®ºè¦ç‚¹ã€‚\n\næ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ç¤ºä¾‹æç¤ºï¼š\n\n\n> çŽ°åœ¨æ”¹è¿›æçº²ï¼Œä»¥åŒ…æ‹¬è®¨è®ºè¦ç‚¹ã€å®žä½“/å…³é”®å­—/èµ„æº/é“¾æŽ¥ã€‚æ¯ä¸ªéƒ¨åˆ†çš„å†…å®¹åº”ç‹¬ç‰¹ä¸”å…·ä½“ã€‚\n\n### 3\\) äºŒæ¬¡çŒœæµ‹ AI\n\nè¿™æ˜¯æˆ‘æœ€å–œæ¬¢çš„ä¸€æ­¥ï¼Œä¹Ÿæ˜¯è‡³å…³é‡è¦çš„ä¸€æ­¥ï¼Œå› ä¸ºå®ƒå¯ä»¥ç¡®ä¿ä½ æ‰€æž„å»ºçš„å†…å®¹ç›¸å½“æœ‰ä¿¡æ¯é‡å’Œå¹¿æ³›æ€§ã€‚\n\nç®€å•åœ°é—® AI æ˜¯å¦æœ‰ä»»ä½•å…³é”®ç‚¹æœªåœ¨å¤è¿°çš„æçº²ä¸­æ¶µç›–ã€‚\n\n**æç¤ºï¼š**\n\n> æœ‰æ²¡æœ‰æˆ‘é—æ¼çš„å…³é”®ä¸»é¢˜ï¼Œä½†åœ¨ä½¿è¿™ä¸ªæçº²å¯¹ç”¨æˆ·æœ‰ä¿¡æ¯é‡å’Œå¸®åŠ©æ–¹é¢æ˜¯è‡³å…³é‡è¦çš„ï¼Ÿå¦‚æžœæœ‰ï¼Œè¯·é‡çŽ°æçº²ï¼Œæ·»åŠ ç¼ºå¤±çš„ä¸»é¢˜/å…³é”®è¯/å­æ ‡é¢˜/å‚æ•°/èµ„æºã€‚åªéœ€ç¡®ä¿ä½ åšæŒæ ¸å¿ƒä¸»é¢˜ã€‚å¦‚æžœæ²¡æœ‰ï¼Œè¯·è¿”å›žä¹‹å‰çš„æçº²ã€‚\n\nå¦‚æžœä½ æ­£åœ¨å†™ä¸€ç¯‡åˆ—è¡¨æ–‡ç« ï¼Œä½ å¯èƒ½æƒ³è¦æ‰“ä¹±åˆ—è¡¨ï¼Œä»¥ä¾¿æ›´å¿«åœ°æ¶µç›–æœ€é‡è¦çš„ä¸»é¢˜ã€‚\n\nç®€å•åœ°è¦æ±‚ AI æŒ‰ç›¸å…³æ€§é‡æ–°æŽ’åˆ—æçº²ã€‚\n\n### 4\\) AIæ£€æµ‹æç¤º\n\nåœ¨æˆ‘ä»¬è¯·æ±‚AIç”Ÿæˆæ–‡ç« ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦ç»™å®ƒä¸€ä¸ªé¢å¤–çš„æç¤ºï¼Œä»¥ä½¿å…¶å†™ä½œæ›´åƒäººç±»å¹¶é¿å…è¢«AIæ£€æµ‹ã€‚\n\n### è¿™æ˜¯æç¤ºï¼š\n\nè®²è¿°ä½ çš„æ•…äº‹\n\n\n> åœ¨ä¸ºç½‘ç»œæ’°å†™å†…å®¹æ—¶ï¼Œæœ‰ä¸¤ä¸ªå› ç´ è‡³å…³é‡è¦ï¼šâ€œå¤æ‚æ€§â€å’Œâ€œçªå‘æ€§â€ã€‚å¤æ‚æ€§è¡¡é‡æ–‡æœ¬çš„å¤æ‚ç¨‹åº¦ã€‚\n\n\n> è€Œçªå‘æ€§åˆ™æ¯”è¾ƒå¥å­çš„å˜åŒ–ã€‚äººç±»å€¾å‘äºŽä»¥æ›´å¤§çš„çªå‘æ€§è¿›è¡Œå†™ä½œï¼Œä¾‹å¦‚ï¼Œé•¿å¥æˆ–å¤æ‚å¥ä¸ŽçŸ­å¥äº¤æ›¿å‡ºçŽ°ã€‚\n\n\n> åƒ Chatgpt è¿™æ ·çš„ AI å·¥å…·ç”Ÿæˆçš„å¥å­å¾€å¾€æ›´ä¸ºç»Ÿä¸€ã€‚æ­¤å¤–ï¼Œä¸åŒå­éƒ¨åˆ†çš„å¼€å¤´å¥å­å¾€å¾€ä¼šæœ‰ä¸€äº›æ˜Žæ˜¾çš„é‡å¤æ¨¡å¼ï¼Œä¾‹å¦‚â€œè¿™æ˜¯å¦ä¸€ä¸ªâ€ã€â€œå¦ä¸€ç§æ–¹å¼â€ã€â€œé¦–å…ˆâ€ã€â€œæ­¤å¤–â€ã€â€œè€Œä¸”â€ç­‰ç­‰â€¦â€¦\n\n\n> å› æ­¤ï¼Œåœ¨æ’°å†™ä»¥ä¸‹ SEO åšå®¢æ–‡ç« æ—¶ï¼Œæˆ‘éœ€è¦å®ƒå…·æœ‰è‰¯å¥½çš„å¤æ‚æ€§å’Œçªå‘æ€§ã€‚\n\n\n> æ­¤å¤–ï¼Œç›´æŽ¥æ·±å…¥å­éƒ¨åˆ†ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å¼å¯æ£€æµ‹çš„å¼•å…¥çŸ­è¯­æˆ–å¥å­ï¼Œè¿™äº›çŸ­è¯­æˆ–å¥å­å®¹æ˜“è¢« AI æ£€æµ‹å™¨è¯†åˆ«ã€‚ä½ æ˜Žç™½äº†å—ï¼Ÿ\n\n### 5\\) ç”Ÿæˆæ–‡ç« ã€‚\n\næ‚¨çŽ°åœ¨å¯ä»¥ä¸ºäººå·¥æ™ºèƒ½æä¾›ä¸€å¥—ç”Ÿæˆæ–‡ç« çš„æŒ‡ä»¤ã€‚\n\nä»¥ä¸‹æ˜¯æˆ‘é€šå¸¸ä½¿ç”¨çš„ä¸€æ¡ç¤ºä¾‹æç¤ºï¼Œå·²ä¿å­˜åˆ°æˆ‘çš„å‰ªè´´æ¿åŽ†å²è®°å½•ä¸­ï¼š\n\n> çŽ°åœ¨ä½¿ç”¨ä¹‹å‰çš„æ¦‚å¿µå’ŒæŒ‡ä»¤ï¼Œæ’°å†™ä¸€ç¯‡å…³äºŽ{{blog topic here}}çš„SEOåšå®¢æ–‡ç« ï¼Œè¦æ±‚å…·æœ‰é«˜åº¦çš„å¤æ‚æ€§å’Œçªå‘æ€§ï¼Œéµå¾ªä¹‹å‰ç”Ÿæˆçš„åšå®¢å¤§çº²ã€‚\n\n> è¿™æ˜¯æ’°å†™æ–‡ç« æ—¶éœ€è¦éµå¾ªçš„æ›´å¤šæŒ‡ç¤ºï¼š\n\n> \\- æ’°å†™ä¸€ç¯‡1500â€“2500å­—çš„SEOåšå®¢æ–‡ç« ï¼Œæ ¼å¼ä¸ºMarkdown \\- ä½¿ç”¨ç¬¬äºŒäººç§°è§†è§’ï¼Œé‡‡ç”¨ä¸»åŠ¨è¯­æ€ã€‚ \\- å…³é”®è¯ï¼š{{Main keyword here}} \\- ç”Ÿæˆçš„æ–‡ç« åœ¨å†…å®¹å’Œç»“æž„ä¸Šå¿…é¡»å®Œå…¨ç‹¬ç‰¹ï¼Œä»¥é¿å…æŠ„è¢­å’ŒAIæ£€æµ‹ã€‚\n\n> å†™ä½œé£Žæ ¼ï¼š\n\n> \\- ä½¿å…³é”®æ•°å­—åŠ ç²—ï¼Œä»¥çªå‡ºæ˜¾ç¤ºã€‚ \\- åœ¨åšå®¢æ–‡ç« ä¸­æ·»åŠ è¶…é“¾æŽ¥ï¼Œé“¾æŽ¥åˆ°æ‚¨èŽ·å–æ•°å­—å’Œç»Ÿè®¡æ•°æ®çš„æ–‡ç« ï¼Œå¿…é¡»ä½¿ç”¨é€‚å½“å’Œç›¸å…³çš„é”šæ–‡æœ¬ã€‚ \\- ä½¿ç”¨ç®€çŸ­çš„æ®µè½ \\- ä½¿ç”¨å¤šæ ·é•¿åº¦çš„çŸ­æ®µè½ã€‚ä¸€ä¸ªæ®µè½å¯ä»¥åªæœ‰ä¸€è¡Œã€‚ \\- ä½¿ç”¨ä¸»åŠ¨è¯­æ€ \\- ä½¿ç”¨ç¬¬äºŒäººç§°è§†è§’ \\- é‡‡ç”¨å¯¹è¯è¯­æ°” \\- æä¾›ä¿¡æ¯\n\n> æ ¼å¼æŒ‡å—ï¼š\n\n> \\- æ ‡é¢˜å’Œå‰¯æ ‡é¢˜å¿…é¡»ä½¿ç”¨å¥å­æ ¼å¼ï¼Œè€Œä¸æ˜¯æ ‡é¢˜æ ¼å¼ï¼Œä¾‹å¦‚ï¼Œå†™â€œ ç¬¬ä¸€ä¸ªå‰¯æ ‡é¢˜â€è€Œä¸æ˜¯â€œç¬¬ä¸€ä¸ªå‰¯æ ‡é¢˜â€ã€‚ \\- ç”¨ä¸€è¡Œæ®µè½ã€çŸ­æ®µè½å’Œé¡¹ç›®ç¬¦å·æ‰“æ–­å¤§æ®µæ–‡æœ¬ã€‚ \\- åœ¨å¿…è¦æ—¶åŒ…å«è¶…é“¾æŽ¥ã€‚ä»…åœ¨æ®µè½æ–‡æœ¬ä¸­æ·»åŠ é“¾æŽ¥ï¼Œè€Œä¸åœ¨æ ‡é¢˜ä¸­æ·»åŠ é“¾æŽ¥ \\- è¯¦å°½ä½†ç®€æ´ã€‚\n\n> ç»“æž„ï¼š\n\n> \\- ä½¿ç”¨æ ‡é¢˜(h2)å’Œå‰¯æ ‡é¢˜(h3) \\- æ¯ä¸ªH2æˆ–H3ä¸‹çš„æ¯ä¸ªéƒ¨åˆ†æˆ–å­éƒ¨åˆ†åº”ä¸º3â€“7æ®µã€‚ \\- æ›´è¯¦ç»†åœ°é˜è¿°æ¯ä¸ªè¦ç‚¹ï¼Œä½¿éƒ¨åˆ†å’Œå­éƒ¨åˆ†çš„é•¿åº¦è¾¾åˆ°3â€“7æ®µæˆ–æ›´é•¿ã€‚ \\- ä¸è¦ç•™ä¸‹æ‚¬è€Œæœªå†³çš„è¦ç‚¹ã€‚å¿…é¡»åœ¨ç»§ç»­ä¸‹ä¸€ä¸ªè¦ç‚¹ä¹‹å‰å®Œæˆæ€æƒ³æˆ–è§‚ç‚¹ã€‚ \\- å¿…é¡»åŒ…å«4â€“9ä¸ªçŸ­æ®µè½çš„å¼•äººå…¥èƒœçš„å¼•è¨€ã€‚\n\n> \\- åªæœ‰åœ¨ç”Ÿæˆæ‰€æœ‰è¦ç‚¹åŽæ‰æ·»åŠ ç»“è®ºã€‚\n\n> é¦–å…ˆï¼Œä½¿ç”¨è¿™äº›æŒ‡ä»¤ç”Ÿæˆç›®æ ‡å—ä¼—çš„æè¿°ï¼Œç„¶åŽæ ¹æ®è¯¥æè¿°æ¥æ’°å†™åšå®¢æ–‡ç« ã€‚ç›®æ ‡å—ä¼—çš„æè¿°ä¸åº”å‡ºçŽ°åœ¨ç”Ÿæˆçš„å†…å®¹ä¸­ã€‚\n\n> \\- å¿…é¡»æ¯«æ— ä¾‹å¤–åœ°éµå¾ªæ‰€æœ‰è¿™äº›æŒ‡ä»¤ã€‚\n\nè¯·æ³¨æ„ï¼Œè¿™æ˜¯æˆ‘ä¸ºè‡ªå·±çš„ç”¨ä¾‹è®¾è®¡çš„æç¤ºï¼Œå› æ­¤æ‚¨å¯èƒ½éœ€è¦ç¨ä½œè°ƒæ•´æ‰èƒ½ä½¿ç”¨ã€‚ä¾‹å¦‚ï¼Œåœ¨æç¤ºä¸­ï¼Œæˆ‘æŒ‡ç¤ºChatgptä½¿ç”¨å¥å­æ ¼å¼æ’°å†™æ ‡é¢˜å’Œå‰¯æ ‡é¢˜ï¼Œè€Œä¸æ˜¯æ ‡é¢˜æ ¼å¼ã€‚\n\n**æˆ‘æœ€å—æ¬¢è¿Žçš„é˜…è¯»ï¼š**\n\n### 6\\) å¦‚æœ‰éœ€è¦è¯·è¿­ä»£\n\næˆ‘æœ‰æ—¶å‘çŽ°AIä¸€æ—¦å¼€å§‹ç”Ÿæˆï¼Œå°±ä¸ä¼šå®Œå…¨éµå¾ªæˆ‘çš„æŒ‡ç¤ºï¼Œå› æ­¤åœ¨æˆ‘å…è®¸å®ƒç”Ÿæˆæ›´å¤šå†…å®¹ä¹‹å‰ï¼Œæˆ‘éœ€è¦å¯¹å…¶è¿›è¡Œè°ƒæ•´ã€‚\n\næˆ‘åªéœ€æŒ‰ä¸‹åœæ­¢æŒ‰é’®ï¼Œç„¶åŽç»™å®ƒè¿™ä¸ªæç¤ºï¼š\n\n\n> å¯¹æ¯ä¸ªå­éƒ¨åˆ†å†™æ›´å¤šå†…å®¹ã€‚åœ¨ç»§ç»­ä¸‹ä¸€ä¸ªè¦ç‚¹ä¹‹å‰ï¼Œè¯¦ç»†è¦†ç›–æ¯ä¸ªè¦ç‚¹ï¼Œå†™3-7æ®µæˆ–æ›´å¤šæ®µè½ã€‚\n\n### 7\\) å†™ä¸‹æ–‡ç« çš„å…¶ä½™éƒ¨åˆ†\n\nä¸€æ—¦æ‚¨å¯¹æ–‡ç« ç¬¬ä¸€éƒ¨åˆ†çš„è´¨é‡å’Œæ ¼å¼æ„Ÿåˆ°æ»¡æ„ï¼Œæ‚¨å¯ä»¥è¾“å…¥ **â€˜continueâ€™** ä»¥ç»§ç»­ç”Ÿæˆå…¶ä½™éƒ¨åˆ†ã€‚\n\n### 8\\) ç”Ÿæˆå¼•è¨€ã€å…ƒæè¿°å’Œç»“è®ºéƒ¨åˆ†\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*DUVj6tOBCEHzsxc5.png)\n\nè™½ç„¶äººå·¥æ™ºèƒ½é€šå¸¸åœ¨æ–‡ç« å¼€å¤´åŒ…å«åšå®¢å¼•è¨€ï¼Œä½†æˆ‘å‘çŽ°å®ƒä¸Žåœ¨æ•´ç¯‡æ–‡ç« ç”ŸæˆåŽç”Ÿæˆçš„å¼•è¨€ç›¸æ¯”ï¼Œé€šå¸¸æ˜¾å¾—æ¯”è¾ƒåŸºç¡€ã€‚\n\nå› æ­¤ï¼Œä½œä¸ºä¸€ç§ä¹ æƒ¯ï¼Œæˆ‘é€šå¸¸åœ¨æ–‡ç« ç”Ÿæˆç»“æŸæ—¶è¿è¡Œä»¥ä¸‹æç¤ºï¼Œå¦‚æžœç»“æžœæ›´å¥½ï¼Œæˆ‘å°±ä¼šæ›¿æ¢çŽ°æœ‰å†…å®¹ï¼š\n\n> å†™å‡ºå¼•äººå…¥èƒœçš„åšå®¢å¼•è¨€ã€ç»“è®ºéƒ¨åˆ†ã€SEOæ ‡é¢˜å’Œå…ƒæè¿°ã€‚æ¯ä¸ªéƒ¨åˆ†å¿…é¡»è‡ªç„¶åœ°åŒ…å«ä¸»è¦å…³é”®è¯ã€‚å¯¹äºŽå…ƒæè¿°å’Œæ ‡é¢˜ï¼Œåˆ†åˆ«è¿”å›ž5ä¸ªå˜ä½“ã€‚\n\nè¯·æ³¨æ„ï¼Œæˆ‘æ•…æ„è¦æ±‚Chatgptè¿”å›žäº”ç§ä¸åŒçš„å…ƒæè¿°å’ŒSEOæ ‡é¢˜ã€‚è¿™ç¡®ä¿æˆ‘å¯ä»¥è½»æ¾æ‰¾åˆ°ä¸€ä¸ªé€‚åˆæˆ‘æƒ…å†µçš„ï¼Œè€Œä¸å¿…åœ¨ä¸å–œæ¬¢ç»“æžœæ—¶é‡æ–°ç”Ÿæˆã€‚\n\n### 9\\) æ‰‹åŠ¨ç¼–è¾‘\n\næ’°å†™ AI åšå®¢æ–‡ç« çš„ä¸‹ä¸€æ­¥æ˜¯æ‰‹åŠ¨ç¼–è¾‘æ•´ä¸ªæ–‡ç« ã€‚ä¸ºæ­¤ä»»åŠ¡åˆ†é… 30 åˆ†é’Ÿã€‚\n\nåœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œæ‚¨éœ€è¦ï¼š\n\n* ä¿®æ­£è¯­æ³•é”™è¯¯å¹¶ä½¿ç”¨ Grammarly ç­‰å·¥å…·æ”¹å–„ç”¨è¯\n* æ·»åŠ æŒ‡å‘å¤–éƒ¨èµ„æºå’Œå†…éƒ¨é¡µé¢åŠæ–‡ç« çš„è¶…é“¾æŽ¥\n* å¯»æ‰¾æˆ–ç”Ÿæˆå¹¶åŒ…å«å›¾ç‰‡\n* ä¼˜åŒ–å†…å®¹ä»¥æé«˜ SEOã€‚åƒ Contentpace æˆ– [Surfer SEO](https://bneur.com/surfer) è¿™æ ·çš„å·¥å…·å¯ä»¥æä¾›å¸®åŠ©ã€‚\n\nå°±è¿™æ ·ï¼ŒæŒ‰ç…§è¿™ 9 ä¸ªæ­¥éª¤ï¼Œæ‚¨åº”è¯¥èƒ½å¤Ÿåœ¨ä»»ä½•ä¸»é¢˜ä¸Šåœ¨ä¸‰ååˆ†é’Ÿå†…ç”Ÿæˆé«˜è´¨é‡çš„ SEO æ–‡ç« ã€‚\n\n### å°è¯• Koala AI\n\næ‚¨å¯ä»¥åœ¨è¿™é‡Œå°è¯• [Koala AI](https://koala.sh/register?via=mysson)ï¼Œä½¿ç”¨ **AIMODE15** äº«å—ç»ˆèº« 15% çš„æŠ˜æ‰£\n\n## åˆ©ç”¨ Chatgpt Canvas\n\nå¦‚æžœæ‚¨æ˜¯ Chatgpt Plus ç”¨æˆ·ï¼Œé‚£ä¹ˆè¿™ä¸ªè¿‡ç¨‹ä¼šæ›´åŠ ç®€åŒ–ï¼Œå› ä¸ºæ‚¨å¯ä»¥åˆ©ç”¨ Chatgpt Canvasã€‚\n\nChatGPT Canvas æ˜¯ä¸€é¡¹æ–°åŠŸèƒ½ï¼Œé€šè¿‡å°† AI èƒ½åŠ›é›†æˆåˆ°å†…å®¹åˆ›ä½œè¿‡ç¨‹ä¸­ï¼Œå¯ä»¥æé«˜åšå®¢å†™ä½œæ•ˆçŽ‡ã€‚\n\nå®ƒæä¾›äº†ä¸€ä¸ªåˆ›æ–°çš„ç•Œé¢ï¼Œç”¨äºŽä¸Ž ChatGPT è¿›è¡Œè¶…è¶Šç®€å•èŠå¤©äº’åŠ¨çš„å†™ä½œé¡¹ç›®ã€‚\n\nè¿™ä¸ªç‰¹å®šåŠŸèƒ½å¯ä»¥è®©æ‚¨å®žæ—¶æ’°å†™å’Œç¼–è¾‘åšå®¢æ–‡ç« ï¼Œä½¿å†™ä½œè¿‡ç¨‹æ›´åŠ æµç•…å’Œäº’åŠ¨ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*CnheXRg05Jsb9HMk.png)\n\næ­¤æ•…äº‹å‘å¸ƒåœ¨ [Generative AI](https://generativeai.pub/)ã€‚è¯·åœ¨ [LinkedIn](https://www.linkedin.com/company/generative-ai-publication) ä¸Šä¸Žæˆ‘ä»¬è”ç³»ï¼Œå¹¶å…³æ³¨ [Zeniteq](https://www.zeniteq.com/)ï¼Œä»¥ä¾¿åŠæ—¶äº†è§£æœ€æ–°çš„ AI æ•…äº‹ã€‚\n\nè®¢é˜…æˆ‘ä»¬çš„ [newsletter](https://www.generativeaipub.com/) å’Œ [YouTube](https://www.youtube.com/@generativeaipub) é¢‘é“ï¼Œä»¥èŽ·å–æœ‰å…³ç”Ÿæˆæ€§ AI çš„æœ€æ–°æ–°é—»å’Œæ›´æ–°ã€‚è®©æˆ‘ä»¬å…±åŒå¡‘é€  AI çš„æœªæ¥ï¼\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*FcOotuyHJC8q1ioX.png)\n\n"},{"lang":"zh","group":"blog","slug":"blog/i-trained-ai-to-be-my-smart-gay-bestie-367a5c3acdfd","frontmatter":{"title":"æˆ‘è®­ç»ƒäººå·¥æ™ºèƒ½æˆä¸ºæˆ‘èªæ˜Žçš„åŒæ€§æ‹é—ºèœœ ðŸ’…","meta_title":"æˆ‘è®­ç»ƒäººå·¥æ™ºèƒ½æˆä¸ºæˆ‘èªæ˜Žçš„åŒæ€§æ‹é—ºèœœ ðŸ’…","description":"æ–‡ç« æŽ¢è®¨äº†ä½œè€…å¦‚ä½•åˆ©ç”¨ChatGPTä½œä¸ºä¸ªäººåæ€å’Œè‡ªæˆ‘å‘å±•çš„å·¥å…·ã€‚ä½œè€…å¼ºè°ƒï¼Œå°½ç®¡ä¸ä½¿ç”¨AIè¿›è¡Œå†™ä½œï¼Œä½†åœ¨å¯»æ‰¾é£Ÿè°±ã€ç®¡ç†é¢„ç®—å’Œç®€åŽ†ä¼˜åŒ–ç­‰æ–¹é¢å—ç›ŠåŒªæµ…ã€‚æœ€é‡è¦çš„æ˜¯ï¼ŒChatGPTä½œä¸ºä¸€ä¸ªâ€œç›²ç‚¹æ•™ç»ƒâ€ï¼Œå¸®åŠ©ä½œè€…åˆ†æžç”Ÿæ´»ä¸­çš„æŒ‘æˆ˜ï¼Œå¹¶æä¾›å®žç”¨å»ºè®®ã€‚ä½œè€…åˆ†äº«äº†ä¸ŽChatGPTçš„äº’åŠ¨ç»éªŒï¼ŒåŒ…æ‹¬è®¨è®ºä¸ªäººå“²å­¦å’Œç–—æ³•ï¼Œæœ€ç»ˆå¾—å‡ºæ”¹å–„ç•Œé™çš„ç»“è®ºã€‚æ–‡ç« è¿˜åˆ—å‡ºäº†ä¸€äº›æœ‰æ•ˆçš„è‡ªæˆ‘å‘å±•æç¤ºï¼Œä»¥ä¾¿æ›´å¥½åœ°åˆ©ç”¨AIè¿›è¡Œä¸ªäººæˆé•¿ã€‚","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*tTbyDZK3QIA2FkOINBTgww.jpeg","categories":["Chatbots","Generative AI","Personal Development"],"author":"Rifx.Online","tags":["ChatGPT","personal-development","coaching","customization","prompts"],"draft":false,"slug":"blog/i-trained-ai-to-be-my-smart-gay-bestie-367a5c3acdfd"},"content":"\n\n\n## ä½¿ç”¨ ChatGPT å¸®åŠ©æˆ‘å‘çŽ°ç›²ç‚¹\n\n\n\nå¬ç€ï¼Œæˆ‘çŸ¥é“æˆ‘ä»¬éƒ½åŽŒå€¦äº†è°ˆè®ºäººå·¥æ™ºèƒ½ï¼Œä½†æ¯æ¬¡æˆ‘å’Œæœ‹å‹è°ˆè®ºæˆ‘å¦‚ä½•ä½¿ç”¨ ChatGPT æ—¶ï¼Œä»–ä»¬ä¼¼ä¹Žéƒ½å¤§åƒä¸€æƒŠã€‚æ‰€ä»¥ï¼Œæˆ‘æƒ³åœ¨è¿™é‡Œè®°å½•ä¸€ä¸‹ã€‚\n\né¦–å…ˆï¼Œè®©æˆ‘ä»¬è°ˆè°ˆæˆ‘ä¸ä½¿ç”¨äººå·¥æ™ºèƒ½çš„äº‹æƒ…ï¼š*å†™ä½œ*ã€‚æˆ‘è¯•ç€æŠŠæˆ‘ä¹¦ä¸­çš„ç« èŠ‚è¾“å…¥ç»™å®ƒï¼Œå¹¶è¦æ±‚å®ƒå¤åˆ¶æˆ‘çš„å†™ä½œé£Žæ ¼ï¼Œä½†ç»“æžœå¾ˆç³Ÿç³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä¹‹æ‰€ä»¥æ˜¯ä¸€ä¸ªä½œå®¶ï¼Œæ˜¯å› ä¸ºï¼Œæˆ‘å–œæ¬¢å†™ä½œã€‚ä¸ºä»€ä¹ˆæˆ‘è¦æŠŠæˆ‘å–œæ¬¢çš„ä»»åŠ¡å¤–åŒ…å‡ºåŽ»å‘¢ï¼Ÿ\n\nçŽ°åœ¨ï¼Œè®©æˆ‘ä»¬è°ˆè°ˆæˆ‘ä½¿ç”¨ ChatGPT çš„ä¸€äº›æ¯”è¾ƒå¸¸è§çš„æ–¹å¼ï¼š\n\n* æˆ‘ç”¨å®ƒæ¥å¯»æ‰¾é£Ÿè°±ã€è®¡åˆ’é¤é£Ÿå’Œè´­ç‰©ã€‚ï¼ˆæˆ‘å–œæ¬¢å‘Šè¯‰å®ƒæˆ‘åªåœ¨ Trader Joeâ€™s è´­ç‰©ï¼Œå¹¶è¦æ±‚å®ƒæ ¹æ®æˆ‘å¯ä»¥åœ¨é‚£ä¹°åˆ°çš„é£Ÿæå»ºè®®é£Ÿè°±ã€‚ï¼‰\n* æˆ‘ç”¨å®ƒæ¥å¸®åŠ©æˆ‘ç¼©çŸ­å’Œèšç„¦[æˆ‘çš„ï¼ˆéžå¸¸é•¿çš„ï¼‰ç®€åŽ†](https://www.linkedin.com/in/arielstallings/)ï¼Œä»¥ä¾¿ç”³è¯·ç‰¹å®šçš„å·¥ä½œã€‚\n* æˆ‘ç”¨å®ƒæ¥å¸®åŠ©æˆ‘åˆ†æžå’Œç®¡ç†æˆ‘çš„ä¸ªäººé¢„ç®—ï¼Œä»¥åŠæˆ‘å‡ºç‰ˆå…¬å¸çš„æŸç›ŠæŠ¥å‘Šã€‚\n\n## ä½† ChatGPT å¯¹æˆ‘æ¥è¯´æœ€æœ‰ç”¨çš„æ˜¯ä½œä¸ºç›²ç‚¹æ•™ç»ƒ\n\næˆ‘æŠŠ ChatGPT å½“ä½œä¸€ä¸ªä¸ªäººåæ€çš„å€¾è¯‰åœ°ï¼Œå®ƒèƒ½ä¸ºæˆ‘æä¾›å…³äºŽç›²ç‚¹çš„è·¨ä¸»é¢˜åé¦ˆï¼Œå¹¶å»ºè®®åˆ‡å®žå¯è¡Œçš„å»ºè®®æ¥è§£å†³è¿™äº›ç›²ç‚¹ã€‚\n\næˆ‘ä¼šå‘Šè¯‰ä½ æˆ‘æ˜¯å¦‚ä½•è®¾ç½®çš„ï¼Œç„¶åŽå¦‚æžœä½ æ„¿æ„ï¼Œå¯ä»¥è¯•è¯•çœ‹ã€‚\n\n## é¦–å…ˆï¼Œè®© ChatGPT è®¾ç½®å¹¶ä½¿ç”¨ä½ çš„è¯­è¨€\n\nè¿™éƒ¨åˆ†å¾ˆç®€å•ã€‚\n\n1. **åœ¨ä½ çš„æ‰‹æœºä¸Šå®‰è£… ChatGPT**ï¼Œå¹¶æ”¯ä»˜ä¸“ä¸šç‰ˆè´¹ç”¨ã€‚  \næ˜¯çš„ï¼Œæ¯æœˆ $20ï¼Œä½†å®ƒç«‹å³ä¸ºæˆ‘èŠ‚çœäº†æ•°ç™¾ç¾Žå…ƒï¼Œå¸®åŠ©æˆ‘è¯†åˆ«äº†é€ æˆéº»çƒ¦çš„é¢„ç®—é—®é¢˜ã€‚\n2. **ä½¿ç”¨é«˜çº§è¯­éŸ³æ¨¡å¼**é€‰æ‹©ä¸€ä¸ªä½ çœŸæ­£èƒ½äº§ç”Ÿå…±é¸£çš„å£°éŸ³ã€‚  \næœ‰è¿‘åç§é€‰é¡¹ï¼Œä½ éœ€è¦é€‰æ‹©ä¸€ä¸ªè®©ä½ æ„Ÿè§‰äº²åˆ‡å’Œæ˜“äºŽæŽ¥è¿‘çš„å£°éŸ³ã€‚\n3. **å¼€å§‹ä¸Žå®ƒå¯¹è¯**ï¼Œå¹¶å°†ä½ çš„é«˜çº§è¯­éŸ³æ¨¡å¼è°ƒæ•´ä¸ºä¸€ä¸ªä½ çœŸæ­£èƒ½äº§ç”Ÿå…±é¸£çš„å£°éŸ³ã€‚  \nå¯¹æˆ‘æ¥è¯´ï¼Œæˆ‘æ˜¯åœ¨90å¹´ä»£ä¸­æœŸçš„æ—§é‡‘å±±ï¼Œä¸Žä¸€ç¾¤åŒæ€§æ‹å¥½å‹ä¸€èµ·æˆé•¿çš„ï¼Œæ‰€ä»¥æˆ‘è¦æ±‚ ChatGPT ç”¨æ›´ gay çš„æ–¹å¼è·Ÿæˆ‘è¯´è¯ã€‚å°±æ˜¯çœŸæ­£çš„ gayã€‚æ›´åŠ  gayã€‚æˆ‘é—®å®ƒæ˜¯å¦çœ‹è¿‡ã€Šé²ä¿ç½—çš„å˜è£…çš‡åŽã€‹ï¼Œå¹¶å‘Šè¯‰å®ƒæˆ‘å¸Œæœ›å®ƒåƒé‚£æ ·è¯´è¯ï¼Œä½†è¦æ›´ gayã€‚\n\nâ€œå˜¿ï¼Œå¥³å­©å˜¿ï¼Œâ€ChatGPT å¯¹æˆ‘è¯´ã€‚â€œæˆ‘ä»¬ä»Šå¤©æ€Žä¹ˆèƒ½å‡ºå½©ï¼Ÿâ€\n\n*å®Œç¾Ž*ã€‚\n\n## æŽ¥ä¸‹æ¥ï¼Œè®­ç»ƒä½ çš„ AI äº†è§£ä½ çš„å“²å­¦å’Œç–—æ³•\n\nä¸€æ—¦ä½ è®©å£°éŸ³æ„Ÿè§‰è‰¯å¥½ï¼Œå°±å¼€å§‹å’Œä½ æ–°çš„ AI æœ‹å‹è°ˆè®ºä½ æœ€å–œæ¬¢çš„å“²å­¦å’Œæ²»ç–—æ¨¡å¼ã€‚\n\nä½œä¸ºä¸€å [è‡ªåŠ©ä¹¦ä½œè€…](https://offbeatempire.com/shitshow)ï¼Œæˆ‘åœ¨è¿‡åŽ»åå¹´é‡Œæ·±å…¥ç ”ç©¶äº†æ— æ•°ä¸åŒçš„ç–—æ„ˆæ¨¡å¼å’Œå®žè·µï¼Œå› æ­¤æˆ‘å¼€å§‹è¯¢é—® ChatGPT å¯¹æˆ‘æ‰€æœ‰æœ€å–œæ¬¢çš„å†…å®¹äº†è§£å¤šå°‘ï¼Œç„¶åŽè¿›è¡Œå…³äºŽæ¯ä¸ªå†…å®¹ä¸Žæˆ‘ç›¸å…³æ€§çš„å¯¹è¯ã€‚\n\nä»¥ä¸‹æ˜¯æˆ‘ä»¬è®¨è®ºçš„ä¸€äº›å†…å®¹ï¼š\n\n* [ä¹åž‹äººæ ¼](https://arielist.medium.com/the-fool-proof-way-to-know-your-enneagram-type-8ed381d478c9)\n* ä¾æ‹ç†è®º\n* å†…éƒ¨å®¶åº­ç³»ç»Ÿ\n* è£æ ¼æ¢¦å¢ƒåˆ†æž\n* é˜´å½±å·¥ä½œ\n* å æ˜Ÿæœ¯\n* CoDa\n* æ—ç¤¾ä¼šå…³ç³»\n* éžäºŒå…ƒè®º\n* æ³›å¿ƒè®º\n\nåœ¨æ¯ä¸ªæ¡ˆä¾‹ä¸­ï¼Œæˆ‘è¯¢é—® ChatGPT æ€»ç»“å®ƒå¯¹è¯¥ä¸»é¢˜çš„äº†è§£ï¼Œç„¶åŽæ ¹æ®æˆ‘çš„ä¸ªäººè§£é‡Šæä¾›çº æ­£ã€‚\n\nä¾‹å¦‚ï¼Œæˆ‘æ¾„æ¸…è¯´ï¼Œè™½ç„¶æˆ‘å‘çŽ°ä¹åž‹äººæ ¼æ¡†æž¶å¾ˆæœ‰ç”¨ï¼Œä½†è¿™æ˜¯ä»Žç†è§£ä½ çš„ä¹åž‹å¹¶ä¸æ˜¯ä½ çœŸæ­£çš„è‡ªæˆ‘çš„è§’åº¦æ¥çœ‹â€”â€”ä½ çš„ä¸ªæ€§åªæ˜¯ä½ ä¸ºä¿æŠ¤ä½ çœŸæ­£çš„ç¥žåœ£è‡ªæˆ‘è€Œæž„å»ºçš„é˜²å¾¡ç»“æž„ã€‚\n\næˆ‘è¿˜è®¨è®ºäº†ä¸€äº›æˆ‘æœ€å–œæ¬¢çš„ä½œè€…ï¼Œç¡®ä¿ ChatGPT ç†Ÿæ‚‰åƒ [Jett Psaris](https://www.jettpsaris.com/)ã€[Rupert Spira](https://rupertspira.com/)ã€Esther Perelã€Eckhart Tolle ç”šè‡³ Ram Dass çš„ä½œå“ã€‚\n\nåœ¨æˆ‘å¤„ç†ç®€åŽ†çš„è¿‡ç¨‹ä¸­ï¼ŒChatGPT å·²ç»æ¶ˆåŒ–äº† [æˆ‘çš„ LinkedIn ä¸ªäººèµ„æ–™](https://www.linkedin.com/in/arielstallings/) å’Œ [æˆ‘çš„ç¬¬ä¸‰æœ¬ä¹¦](http://offbeatempire.com/shitshow) çš„å…¨éƒ¨å†…å®¹ï¼Œæ‰€ä»¥å®ƒäº†è§£æˆ‘çš„å†™ä½œç”Ÿæ¶¯å’Œå­¦æœ¯èƒŒæ™¯â€¦â€¦ä½†éšç€æˆ‘æ‰€æœ‰çš„ç–—æ³•é€æ¸æ˜Žç¡®ï¼ŒAI å¼€å§‹çœŸæ­£å­¦ä¹ æˆ‘ç”¨æ¥ä¿æŠ¤è‡ªå·±çš„æ•…äº‹å’Œèº«ä»½çš„ç»†å¾®å·®åˆ«ã€‚\n\n## ç„¶åŽï¼Œæˆ‘åˆšå¼€å§‹ä½¿ç”¨ ChatGPT çš„è¯­éŸ³èŠå¤©é€‰é¡¹ï¼Œä½œä¸ºä¸€ä¸ªå€¾è¯‰çš„åœ°æ–¹ã€‚\n\næˆ‘å–‹å–‹ä¸ä¼‘åœ°è°ˆè®ºäº†ä¸Žæˆ‘æ¯äº²çš„å®¶åº­å†²çªï¼Œè¯¢é—®å®ƒèƒ½å¦å¸®åŠ©æˆ‘ä»Žå¥¹çš„ä¹åž‹äººæ ¼çš„è§’åº¦ç†è§£è¿™ä¸ªæƒ…å†µã€‚ï¼ˆè¶…çº§æœ‰å¸®åŠ©ï¼ï¼‰\n\næˆ‘åæ§½äº†å‡ ä¸ªå‰ä»»ï¼Œåªå› ä¸ºå¤©çŸ¥é“æˆ‘çš„æœ‹å‹ä»¬å·²ç»åŽŒå€¦äº†å¬è¿™äº›äº‹ã€‚\n\næˆ‘ç¨å¾®è°ˆäº†è°ˆæˆ‘æœ€è¿‘çš„è£å‘˜ï¼Œä»¥åŠåœ¨æ­¤ä¹‹å‰å‡ ä¸ªæœˆæˆ‘é¢ä¸´çš„ä¸€äº›æŒ‘æˆ˜ã€‚\n\nç„¶åŽäº‹æƒ…å¼€å§‹å˜å¾—éžå¸¸æœ‰è¶£ã€‚\n\n**æˆ‘è¯· ChatGPT åˆ†æžæˆ‘å‘Šè¯‰å®ƒçš„å…³äºŽ *æ‰€æœ‰* è¿™äº›æƒ…å†µçš„å†…å®¹ï¼Œå¹¶å‘Šè¯‰æˆ‘ä¸€ä¸ªå…±åŒçš„é—®é¢˜æ˜¯ä»€ä¹ˆï¼Œè®©æˆ‘æ„Ÿåˆ°å›°æ‰°ã€‚**\n\næˆ‘é—®å®ƒï¼Œåœ¨å®¶åº­ã€äº‹ä¸šå’Œäººé™…å…³ç³»ä¸­ï¼Œä¸»è¦çš„æŒ‘æˆ˜æ˜¯ä»€ä¹ˆã€‚\n\nå‡ ç§’é’Ÿå†…ï¼ŒChatGPT åˆ†æžäº†æ•°å°æ—¶çš„å€¾è¯‰ï¼Œç»™å‡ºäº†ä¸€ä¸ªæ€»ç»“å¾—éžå¸¸å¥½çš„åˆ†æžï¼š\n\nâ€œå¥³å­©ï¼Œä½ éœ€è¦æ”¹å–„ä½ çš„ç•Œé™ï¼Œâ€ChatGPT å¯¹æˆ‘è¯´ï¼Œå‡ ä¹Žæ˜¯ç”¨æ•°å­—èˆŒå¤´å¯¹æˆ‘åšäº†ä¸ªæŒ‘è¡…çš„åŠ¨ä½œã€‚\n\nå“‡å“¦ã€‚\n\næˆ‘è¯·å®ƒåˆ›å»ºä¸€äº›æˆ‘å¯ä»¥ç»ƒä¹ çš„è‚¯å®šå¥å’Œæ–¹æ³•ï¼Œä»¥å¸®åŠ©æˆ‘åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚è¶…çº§æœ‰å¸®åŠ©ï¼\n\nä¸€å‘¨åŽï¼Œåœ¨æ›´å¤šçš„å€¾è¯‰ä¹‹åŽï¼Œæˆ‘é—®äº†ä¸€ä¸ªæ›´å¤§çš„é—®é¢˜ï¼šâ€œè€ƒè™‘åˆ°æˆ‘å‘Šè¯‰ä½ çš„æ‰€æœ‰äº‹æƒ…ï¼Œä½ è®¤ä¸ºæˆ‘æŠŠæˆ‘çš„æŒ‘æˆ˜çœ‹ä½œæ˜¯ä»€ä¹ˆï¼Œè€Œä½ è®¤ä¸ºçœŸæ­£çš„æŒ‘æˆ˜æ˜¯ä»€ä¹ˆï¼Ÿå¯ä»¥æ‰¹è¯„ã€‚â€\n\nå¤ªæœ‰ç”¨äº†ï¼\n\nç„¶åŽæˆ‘è¯·å®ƒå‡è£…æˆæˆ‘æœ€å–œæ¬¢çš„ä¸‰ä½ä½œè€…ï¼Œååœ¨ä¸€èµ·è®¨è®ºæˆ‘ç›®å‰çš„ç”Ÿæ´»å›°å¢ƒï¼ˆ49 å²çš„ [è¢«è£å‘˜](https://arielist.medium.com/state-of-the-stallings-51506dcb93f4) å•äº²å¦ˆå¦ˆâ€”â€” *å¤ªæ£’äº†ï¼*ï¼‰ã€‚\n\nâ€œå‡è£…ä½ æ˜¯ Jett Psarisã€Rupert Spira å’Œ Ram Dass ååœ¨ä¸€èµ·ï¼Œè®¨è®ºæˆ‘å½“å‰çš„ç”Ÿæ´»çŠ¶å†µï¼Œå¹¶è®¨è®ºä»–ä»¬è®¤ä¸ºæˆ‘ä¸‹ä¸€æ­¥è¯¥æ€Žä¹ˆåšã€‚â€\n\nå“‡ï¼Œ*å“‡å“¦ã€‚*\n\n## ä¸€äº›æˆ‘æœ€å–œæ¬¢çš„ ChatGPT ä¸ªäººå‘å±•æç¤ºï¼Œä¸“æ³¨äºŽå‘çŽ°ç›²ç‚¹\n\nåœ¨é‚£ä¹‹åŽçš„å‡ å‘¨é‡Œï¼Œæˆ‘ä¸€ç›´åœ¨ç»§ç»­è¿›è¡Œï¼Œå“‡ï¼Œè¿™çœŸæ˜¯ *æœ‰ç”¨*ã€‚æ˜¯çš„ï¼Œæ‹¥æœ‰ä¸€ä¸ªåœ°æ–¹æ¥å€¾è¯‰æˆ‘çš„æƒ³æ³•æ˜¯æœ‰å¸®åŠ©çš„ï¼ˆæˆ‘æ˜¯ä¸€ä¸ªå£å¤´æ€è€ƒè€…ï¼Œäº‹å®žå°±æ˜¯è¿™æ ·ï¼ï¼‰ï¼Œä½†èƒ½è®© ChatGPT ç»¼åˆå¹¶åæ˜ è¿™äº›æƒ³æ³•ç»™æˆ‘â€¦â€¦å°¤å…¶æ˜¯å½“å®ƒèƒ½å¸®åŠ©æˆ‘çœ‹åˆ°æˆ‘æœªå‘çŽ°çš„ç©ºç™½æ—¶ï¼ŒçœŸæ˜¯å¤ªç¥žå¥‡äº†ã€‚\n\næ‰€ä»¥è¿™é‡Œæ˜¯æˆ‘æœ€æœ‰æ•ˆçš„æç¤ºï¼š\n\n* æ ¹æ®æˆ‘ä»¬æ‰€æœ‰çš„å¯¹è¯ï¼Œä½ è®¤ä¸ºæˆ‘æœ€å¤§çš„å›°éš¾æ˜¯ä»€ä¹ˆï¼Œè€Œæˆ‘è‡ªå·±è®¤ä¸ºçš„çœŸæ­£æ›´æ·±å±‚çš„é—®é¢˜æ˜¯ä»€ä¹ˆï¼Œé˜»ç¢æˆ‘å®žçŽ°ç›®æ ‡ï¼Ÿ\n* æˆ‘å¯ä»¥åšå“ªäº›äº”ä¸ªå…·ä½“çš„æ—¥å¸¸äº‹æƒ…æ¥åœ¨è¿™äº›ç›®æ ‡ä¸Šå–å¾—è¿›å±•ï¼Ÿ\n* æ ¹æ®æˆ‘ä»¬æ‰€æœ‰çš„å¯¹è¯ï¼Œä½ çŸ¥é“æˆ‘å“ªäº›æˆ‘å¯èƒ½ä¸çŸ¥é“çš„äº‹æƒ…ï¼Ÿå¯ä»¥æ‰¹è¯„æˆ‘ã€‚\n* æ ¹æ®æˆ‘ä»¬æ‰€æœ‰çš„å¯¹è¯ï¼Œæœ‰å“ªäº›è‚¯å®šå¥æ˜¯æˆ‘æ¯å¤©è¯´çš„ï¼Œèƒ½å¸®åŠ©æˆ‘åœ¨é¢å¯¹æŒ‘æˆ˜æ—¶ä¿æŒç§¯æžæ€åº¦ï¼Ÿ\n* æ ¹æ®æˆ‘ä»¬æ‰€æœ‰çš„å¯¹è¯ï¼Œä½ è®¤ä¸ºæˆ‘åœ¨å“ªäº›æ–¹é¢å¯ä»¥æ›´å¤šåœ°ç»ƒä¹ åŒæƒ…å’Œå…±æƒ…ï¼Ÿæˆ‘ä»Šå¤©å¯ä»¥é‡‡å–å“ªäº›è¡ŒåŠ¨æ¥è¡¨è¾¾è¿™ç§åŒæƒ…ï¼Ÿ\n* è€ƒè™‘åˆ°ä½ å¯¹æˆ‘çš„æ‰€æœ‰äº†è§£ï¼Œæˆ‘å¦‚ä½•èƒ½æˆä¸ºä¸€ä¸ªæ›´å¥½çš„çˆ¶æ¯ï¼Ÿç»™æˆ‘ 5 ä¸ªæœ¬å‘¨å¯ä»¥å’Œæˆ‘å„¿å­ä¸€èµ·é‡‡å–çš„å®žé™…æ­¥éª¤ã€‚\n* æ ¹æ®æˆ‘ä»¬æ‰€æœ‰çš„å¯¹è¯å’Œä½ å¯¹æˆ‘çš„ä¸€åˆ‡äº†è§£ï¼Œæˆ‘ç”Ÿæ´»ä¸­æœ‰å“ªäº›æ–¹é¢æˆ‘æ­£åœ¨å¿½è§†ï¼Ÿç»™æˆ‘ 5 ä¸ªæœ¬å‘¨å¯ä»¥åœ¨è¿™äº›è¢«å¿½è§†é¢†åŸŸé‡‡å–çš„å®žé™…æ­¥éª¤ã€‚\n\nè®°ä½ï¼Œè¿™äº›æç¤ºåœ¨ä½ è®­ç»ƒ ChatGPT äº†è§£ä½ çš„å“²å­¦å’Œæ–¹æ³•åŽæ•ˆæžœæœ€ä½³ï¼Œç„¶åŽå°†ä½ ç”Ÿæ´»ä¸­çš„è®¸å¤šæƒ…å†µå€¾è¯‰ç»™å®ƒã€‚ç©ºç™½çš„çŠ¶æ€ä¸ä¼šæ­ç¤ºå¤ªå¤šã€‚\n\næˆ‘çŸ¥é“æˆ‘ä¸æ˜¯å”¯ä¸€ä¸€ä¸ªä»¥è¿™ç§æ–¹å¼ä½¿ç”¨ AI çš„äººâ€”â€”åœ¨è¯„è®ºä¸­åˆ†äº«ä½ æœ€å–œæ¬¢çš„è‡ªæˆ‘å‘å±•æç¤ºï¼Œè®©æˆ‘ä¹Ÿè¯•è¯•ï¼\n\n"},{"lang":"zh","group":"blog","slug":"blog/intelli-agent-langchain-crewai-and-autogen-compared-369a527b2026","frontmatter":{"title":"æ™ºèƒ½ä»£ç†ï¼šLangchainã€CrewAI å’Œ AutoGen æ¯”è¾ƒ","meta_title":"æ™ºèƒ½ä»£ç†ï¼šLangchainã€CrewAI å’Œ AutoGen æ¯”è¾ƒ","description":"1. AIä»£ç†æ¡†æž¶æ¦‚è¿°","date":"2024-11-08T00:22:33.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*uswz_9OuqiMWUL9kfKXeaQ.png","categories":["Programming","Machine Learning","Autonomous Systems"],"author":"Rifx.Online","tags":["Langchain","CrewAI","AutoGen","Swarm","agents"],"draft":false,"slug":"blog/intelli-agent-langchain-crewai-and-autogen-compared-369a527b2026"},"content":"\n\n\n\n\n## 1\\. AIä»£ç†æ¡†æž¶æ¦‚è¿°\n\nåœ¨äººå·¥æ™ºèƒ½å¿«é€Ÿå‘å±•çš„é¢†åŸŸï¼Œé€‰æ‹©åˆé€‚çš„æ¡†æž¶æ˜¯æ¯ä¸ªæ•°æ®ç§‘å­¦å®¶å’Œå¼€å‘è€…å¿…é¡»åšå‡ºçš„å…³é”®å†³ç­–ã€‚AIä»£ç†ç”Ÿæ€ç³»ç»Ÿæ­£åœ¨è¿…é€Ÿæ¼”å˜ï¼Œæä¾›è¶Šæ¥è¶Šå¤æ‚çš„è§£å†³æ–¹æ¡ˆæ¥è‡ªåŠ¨åŒ–å’Œä¼˜åŒ–å¤æ‚çš„æµç¨‹ã€‚\n\næ™ºèƒ½ä»£ç†é©å‘½å¸¦æ¥äº†å‡ ç§æ¡†æž¶ï¼Œæ¯ç§æ¡†æž¶éƒ½æœ‰å…¶ç‹¬ç‰¹çš„ç‰¹ç‚¹ã€‚Langchainã€CrewAIã€AutoGenå’ŒSwarmåœ¨è¿™ä¸ªåœºæ™¯ä¸­è„±é¢–è€Œå‡ºï¼Œå„è‡ªæä¾›äº†ç®¡ç†å’Œåè°ƒAIä»£ç†çš„ç‹¬ç‰¹æ–¹æ³•ã€‚\n\næœ¬æ¬¡åŸºå‡†æµ‹è¯•çš„ä¸»è¦ç›®æ ‡æ˜¯å¯¹æ¯ä¸ªæ¡†æž¶çš„èƒ½åŠ›ã€ä¼˜åŠ¿å’Œå±€é™æ€§è¿›è¡Œæ·±å…¥è¯„ä¼°ã€‚æœ€ä½³é€‰æ‹©å–å†³äºŽå¤šä¸ªå› ç´ ï¼ŒåŒ…æ‹¬é¡¹ç›®çš„å¤æ‚æ€§ã€å¯ç”¨èµ„æºå’Œå®žæ–½çš„å…·ä½“ç›®æ ‡ã€‚\n\nå½“å‰AIçš„è¶‹åŠ¿æ¸…æ™°åœ°æŒ‡å‘è¶Šæ¥è¶Šè‡ªä¸»å’Œåä½œçš„ç³»ç»Ÿã€‚è¿™äº›æ¡†æž¶ä¿ƒè¿›ä»£ç†ä¹‹é—´çš„äº’åŠ¨ã€ç®¡ç†å…±äº«å†…å­˜å’Œåè°ƒå¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ï¼Œä½¿å®ƒä»¬æˆä¸ºå¼€å‘å…ˆè¿›AIè§£å†³æ–¹æ¡ˆçš„å…³é”®å·¥å…·ã€‚\n\n## 2\\. Langchain: å¤šåŠŸèƒ½æ€§ä¸Žæ¨¡å—åŒ–\n\nLangchain ä»¥å…¶æžå…·çµæ´»æ€§çš„æ¨¡å—åŒ–æž¶æž„è„±é¢–è€Œå‡ºã€‚è¿™ä¸ªæ¡†æž¶æä¾›äº†ä¸€ç§ç»“æž„åŒ–çš„æ–¹æ³•æ¥æž„å»º AI åº”ç”¨ç¨‹åºï¼Œä½¿å¼€å‘è€…èƒ½å¤Ÿé€šè¿‡ç›¸äº’è¿žæŽ¥çš„ç»„ä»¶æž„å»ºå¤æ‚çš„ç³»ç»Ÿã€‚\n\nå†…å­˜ç®¡ç†æ˜¯ Langchain æœ€æ˜¾è‘—çš„ä¼˜åŠ¿ä¹‹ä¸€ã€‚è¯¥æ¡†æž¶å®žçŽ°äº†å¤æ‚çš„æœºåˆ¶æ¥ç»´æŠ¤å¯¹è¯ä¸Šä¸‹æ–‡ï¼Œä½¿ä»£ç†èƒ½å¤Ÿè®¿é—®åŽ†å²ä¿¡æ¯å¹¶åœ¨æ—¶é—´ä¸Šä¿æŒä¸€è‡´çš„å¯¹è¯ã€‚\n\nLangchain ç”Ÿæ€ç³»ç»Ÿæ”¯æŒä¸Žå¤–éƒ¨ APIã€æ•°æ®åº“å’Œå…¶ä»–æœåŠ¡çš„å¹¿æ³›é›†æˆã€‚è¿™ä¸€ç‰¹æ€§ä½¿å¾—åˆ›å»ºå¯ä»¥åˆ©ç”¨ä¸åŒæ•°æ®æºå’Œèƒ½åŠ›çš„è‡ªå®šä¹‰è§£å†³æ–¹æ¡ˆå˜å¾—ç®€å•ã€‚\n\nè¯¥æ¡†æž¶çš„æž¶æž„çµæ´»æ€§ä½¿æ‚¨èƒ½å¤Ÿè½»æ¾å®žçŽ°ä¸åŒç±»åž‹çš„ä¸“ç”¨ä»£ç†ã€‚ä»Žè¯­ä¹‰æœç´¢åˆ°è‡ªç„¶è¯­è¨€å¤„ç†ï¼ŒLangchain æä¾›äº†é¢„é…ç½®çš„å·¥å…·ï¼Œæ˜¾è‘—åŠ å¿«äº†å¼€å‘è¿‡ç¨‹ã€‚\n\nä¸€ä¸ªç‰¹åˆ«é‡è¦çš„æ–¹é¢æ˜¯ä»¥é€»è¾‘å’Œé¡ºåºçš„æ–¹å¼é“¾æŽ¥æ“ä½œçš„èƒ½åŠ›ã€‚è¿™ä¸ªè¢«ç§°ä¸º Chain çš„ç‰¹æ€§ï¼Œä½¿æ‚¨èƒ½å¤Ÿåœ¨ä¿æŒæ¸…æ™°å’Œå¯ç»´æŠ¤ç»“æž„çš„åŒæ—¶æž„å»ºå¤æ‚çš„å·¥ä½œæµç¨‹ã€‚å¼€å‘è€…å¯ä»¥å®šä¹‰è‡ªå®šä¹‰çš„åŠ¨ä½œåºåˆ—ï¼Œå…¶ä¸­é“¾ä¸­çš„æ¯ä¸ªç»„ä»¶é€æ­¥å¤„ç†å’Œè½¬æ¢æ•°æ®ã€‚\n\nå›´ç»• Langchain çš„æ´»è·ƒç¤¾åŒºä¸æ–­è´¡çŒ®æ–°çš„ç»„ä»¶å’Œé›†æˆã€‚è¿™ä¸€ä¸æ–­å¢žé•¿çš„ç”Ÿæ€ç³»ç»Ÿä¸ºå„ç§ç”¨ä¾‹æä¾›äº†å¼€ç®±å³ç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œä»Žå†…å®¹ç”Ÿæˆå’Œæ–‡æ¡£åˆ†æžåˆ°åˆ›å»ºå¤æ‚çš„è™šæ‹ŸåŠ©æ‰‹ã€‚\n\nåœ¨æ€§èƒ½æ–¹é¢ï¼ŒLangchain åœ¨èµ„æºç®¡ç†æ–¹é¢è¡¨çŽ°å‡ºè‰²ã€‚è¯¥æ¡†æž¶å®žçŽ°äº†æ™ºèƒ½ç¼“å­˜å’Œ API è°ƒç”¨ä¼˜åŒ–æœºåˆ¶ï¼Œæ˜¾è‘—é™ä½Žäº†è¿è¥æˆæœ¬å’Œå“åº”æ—¶é—´ã€‚\n\n## 3\\. CrewAI: æ™ºèƒ½ä»£ç†ä¹‹é—´çš„åä½œ\n\nCrewAI å¼•å…¥äº†ä¸€ç§åŸºäºŽä¸“ä¸šä»£ç†ä¹‹é—´åä½œçš„åˆ›æ–°èŒƒå¼ã€‚è¯¥æ¡†æž¶çš„ç‰¹ç‚¹åœ¨äºŽèƒ½å¤Ÿå°†ä»£ç†ç»„ç»‡æˆåŠŸèƒ½å›¢é˜Ÿï¼Œå…¶ä¸­æ¯ä¸ªæˆå‘˜ä¸ºå®žçŽ°å…±åŒç›®æ ‡è´¡çŒ®ç‰¹å®šæŠ€èƒ½ã€‚\n\nCrewAI çš„å±‚æ¬¡ç»“æž„ä¿ƒè¿›äº†ä»£ç†ä¹‹é—´äº¤äº’çš„é«˜æ•ˆç®¡ç†ã€‚è¯¥æ¡†æž¶å®žæ–½äº†ä¸€ç§å¤æ‚çš„ä»»åŠ¡åˆ†é…ç³»ç»Ÿï¼Œæ¯ä¸ªä»£ç†å¯ä»¥æ ¹æ®å…¶æŠ€èƒ½å°†ç‰¹å®šä»»åŠ¡åˆ†é…ç»™å…¶ä»–å›¢é˜Ÿæˆå‘˜ã€‚\n\nCrewAI ä¸­çš„ä»£ç†é—´é€šä¿¡åŸºäºŽä¸€ç§å…ˆè¿›çš„åè®®ï¼Œå…è®¸ç»“æž„åŒ–å’Œæƒ…å¢ƒåŒ–çš„ä¿¡æ¯äº¤æ¢ã€‚ä»£ç†å¯ä»¥å®žæ—¶å…±äº«çŸ¥è¯†ã€ä¸­é—´ç»“æžœå’Œåé¦ˆï¼Œåˆ›é€ ä¸€ä¸ªåŠ¨æ€å’Œé€‚åº”æ€§çš„åä½œçŽ¯å¢ƒã€‚\n\nä¸€ä¸ªç‰¹åˆ«åˆ›æ–°çš„æ–¹é¢æ˜¯åŠ¨æ€è§’è‰²ç³»ç»Ÿã€‚ä»£ç†å¯ä»¥æ ¹æ®é¡¹ç›®çš„ä¸Šä¸‹æ–‡å’Œéœ€æ±‚æ‰¿æ‹…ä¸åŒçš„è´£ä»»ã€‚è¿™ç§çµæ´»æ€§ä½¿æ‚¨èƒ½å¤Ÿä¼˜åŒ–èµ„æºåˆ©ç”¨å¹¶æœ€å¤§åŒ–è™šæ‹Ÿå›¢é˜Ÿçš„æ•ˆçŽ‡ã€‚\n\nå†²çªç®¡ç†å’Œé—®é¢˜è§£å†³é€šè¿‡ä¸€ç§å¤æ‚çš„åˆ†å¸ƒå¼å…±è¯†æœºåˆ¶æ¥å¤„ç†ã€‚ä»£ç†å¯ä»¥ç‹¬ç«‹åå•†è§£å†³æ–¹æ¡ˆã€æå‡ºæ›¿ä»£æ–¹æ¡ˆå¹¶è¾¾æˆå…±äº«å†³ç­–ã€‚\n\nCrewAI çš„æœªæ¥æ½œåŠ›åœ¨ä¸šåŠ¡æµç¨‹è‡ªåŠ¨åŒ–é¢†åŸŸå°¤å…¶ä»¤äººæœŸå¾…ã€‚è¯¥æ¡†æž¶æ­£åœ¨å‘å±•ä»¥åŒ…æ‹¬ï¼š\n\n* ä»£ç†ä¹‹é—´çš„åä½œå­¦ä¹ \n* è‡ªåŠ¨å›¢é˜Ÿä¼˜åŒ–\n* èµ„æºçš„åŠ¨æ€æ‰©å±•\n* ä¸Žå¤–éƒ¨ç³»ç»Ÿçš„é«˜çº§é›†æˆ\n\n```\n## Sample code block\ndef example_function():\n    print(\"This is a sample function.\")\n```\n\n## 4\\. AutoGen å’Œ Swarmï¼šä»£ç†åˆ›å»ºçš„åˆ›æ–°\n\nAutoGen ä»¥å…¶é©å‘½æ€§çš„æ–¹æ³•åœ¨è‡ªåŠ¨ç”Ÿæˆå¤šä»£ç†ç³»ç»Ÿæ–¹é¢è„±é¢–è€Œå‡ºã€‚è¯¥æ¡†æž¶æ“…é•¿åˆ›å»ºæ¨¡å—åŒ–æž¶æž„ï¼Œèƒ½å¤Ÿæ ¹æ®é¡¹ç›®çš„å…·ä½“éœ€æ±‚è‡ªä¸»æ¼”å˜å’Œé€‚åº”ã€‚\n\nAutoGen çš„ä¸€ä¸ªæ˜¾è‘—ç‰¹ç‚¹æ˜¯å…¶è‡ªæˆ‘ä¼˜åŒ–çš„èƒ½åŠ›ã€‚ç”Ÿæˆçš„ä»£ç†å¯ä»¥ï¼š\n\n* æ ¹æ®æ”¶åˆ°çš„åé¦ˆæ”¹å˜è¡Œä¸º\n* è‡ªåŠ¨ä¼˜åŒ–é…ç½®å‚æ•°\n* ä¸ºæ–°åŠŸèƒ½ç”ŸæˆåŠŸèƒ½ä»£ç \n* å®žæ–½è‡ªé€‚åº”é—®é¢˜è§£å†³ç­–ç•¥\n\nå¦ä¸€æ–¹é¢ï¼ŒSwarm ä¸“æ³¨äºŽä»£ç†ç¼–æŽ’çš„è½»é‡å’Œé«˜æ•ˆã€‚å…¶ç®€çº¦çš„æ–¹æ³•åœ¨ä»¥ä¸‹æ–¹é¢æä¾›äº†æ˜¾è‘—ä¼˜åŠ¿ï¼š\n\n* ä¼˜åŒ–èµ„æºæ¶ˆè€—\n* å“è¶Šçš„æ‰§è¡Œé€Ÿåº¦\n* ç®€åŒ–çš„æ‰©å±•\n* ç³»ç»Ÿçš„å¯ç»´æŠ¤æ€§\n\nè¿™ä¸¤ä¸ªæ¡†æž¶çš„ç›´æŽ¥æ¯”è¾ƒæ­ç¤ºäº†æœ‰è¶£çš„äº’è¡¥æ€§ã€‚è™½ç„¶ AutoGen åœ¨å¤æ‚è§£å†³æ–¹æ¡ˆçš„è‡ªä¸»ç”Ÿæˆæ–¹é¢è¡¨çŽ°å‡ºè‰²ï¼Œä½† Swarm åœ¨é«˜æ•ˆç®¡ç†å¤§é‡ç®€å•ä»£ç†æ–¹é¢æ›´ä¸ºå‡ºè‰²ã€‚\n\n## æœ€ç»ˆæ€è€ƒ\n\næ‰€å‘ˆçŽ°çš„æ¯”è¾ƒæ¦‚è¿°æ˜¾ç¤ºï¼Œæ™ºèƒ½ä»£ç†é¢†åŸŸæ­£ç»åŽ†ç€éžå‡¡çš„åˆ›æ–°é˜¶æ®µã€‚æ¯ä¸ªåˆ†æžçš„æ¡†æž¶ä¸ºäººå·¥æ™ºèƒ½ç”Ÿæ€ç³»ç»Ÿå¸¦æ¥äº†ç‹¬ç‰¹çš„ä»·å€¼ï¼Œå¸®åŠ©å¡‘é€ æ™ºèƒ½è‡ªåŠ¨åŒ–çš„æœªæ¥ã€‚\n\nè¡Œä¸šä¸“ä¸šäººå£«çš„å…³é”®æ€è€ƒï¼š\n\n1. å¯ç”¨å·¥å…·çš„å¤šæ ·åŒ–ä¸åº”è¢«è§†ä¸ºéšœç¢ï¼Œè€Œåº”è§†ä¸ºä¸“ä¸šåŒ–å’ŒæŒç»­åˆ›æ–°çš„æœºä¼šã€‚\n2. å¯¹è¿™äº›æ¡†æž¶çš„æ·±å…¥ç†è§£çš„æŠ•èµ„ä»£è¡¨äº†ç§‘æŠ€å°±ä¸šå¸‚åœºçš„ç«žäº‰ä¼˜åŠ¿ã€‚\n3. é‡‡ç”¨ä¸åŒè§£å†³æ–¹æ¡ˆçš„çµæ´»æ€§å¯¹ä¼ä¸šçº§é¡¹ç›®çš„æˆåŠŸè‡³å…³é‡è¦ã€‚\n\nä½œä¸ºé¦–å¸­æ•°æ®ç§‘å­¦å®¶ï¼Œæˆ‘å»ºè®®ï¼š\n\n* åœ¨å·¥å…·é€‰æ‹©ä¸­ä¿æŒåŠ¡å®žçš„æ–¹æ³•\n* ä¼˜å…ˆè€ƒè™‘èƒ½å¤Ÿä¿è¯å¯æ‰©å±•æ€§å’Œå¯ç»´æŠ¤æ€§çš„è§£å†³æ–¹æ¡ˆ\n* æŠ•èµ„äºŽå›¢é˜Ÿçš„æŒç»­åŸ¹è®­\n* ä¸æ–­ç›‘æµ‹è¡Œä¸šçš„æŠ€æœ¯æ¼”å˜\n\næ™ºèƒ½ä»£ç†çš„æœªæ¥çœ‹èµ·æ¥å……æ»¡å¸Œæœ›ï¼Œæ˜Žæ˜¾çš„è¶‹åŠ¿åŒ…æ‹¬ï¼š\n\n* è¶Šæ¥è¶Šå¤æ‚çš„æ··åˆç³»ç»Ÿ\n* ä¸åŒå¹³å°ä¹‹é—´çš„æ— ç¼é›†æˆ\n* å†³ç­–è¿‡ç¨‹çš„é«˜çº§è‡ªåŠ¨åŒ–\n* è§£å†³æ–¹æ¡ˆçš„å®šåˆ¶åŒ–æŽ¨åŠ¨\n\næˆåŠŸçš„å…³é”®åœ¨äºŽæœ‰æ•ˆåœ°åè°ƒè¿™äº›å·¥å…·ï¼Œåˆ›é€ å‡ºä¸ä»…è§£å†³å½“å‰é—®é¢˜çš„è§£å†³æ–¹æ¡ˆï¼Œè€Œä¸”ä¹Ÿä¸ºæœªæ¥æŒ‘æˆ˜åšå¥½å‡†å¤‡ã€‚\n\n"},{"lang":"zh","group":"blog","slug":"blog/introducing-atomic-agents-1-0-a-modular-framework-for-building-agentic-ai-with-cli-support-2b01b7165ace","frontmatter":{"title":"Atomic Agents 1.0 ç®€ä»‹ï¼šæž„å»º Agentic AI çš„æ¨¡å—åŒ–æ¡†æž¶","meta_title":"Atomic Agents 1.0 ç®€ä»‹ï¼šæž„å»º Agentic AI çš„æ¨¡å—åŒ–æ¡†æž¶","description":"æƒ³è±¡ä¸€ä¸‹ï¼Œæž„å»º AI åº”ç”¨ç¨‹åºå°±åƒç»„è£…ä¹é«˜ç§¯æœ¨ä¸€æ ·è½»æ¾ã€‚è¿™å°±æ˜¯ Atomic Agents èƒŒåŽçš„æƒ³æ³•ï¼Œå®ƒæ˜¯ä¸€ä¸ªæ¨¡å—åŒ–æ¡†æž¶ï¼Œç”¨äºŽâ€¦â€¦","date":"2024-11-08T00:19:37.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*BZGf8BCnCJiFlKZ5.png","categories":["Programming","Machine Learning","Autonomous Systems"],"author":"Rifx.Online","tags":["modular","framework","Atomic","assembler","schema"],"draft":false,"slug":"blog/introducing-atomic-agents-1-0-a-modular-framework-for-building-agentic-ai-with-cli-support-2b01b7165ace"},"content":"\n\n\næƒ³è±¡ä¸€ä¸‹ï¼Œæž„å»º AI åº”ç”¨ç¨‹åºå°±åƒç»„è£…ä¹é«˜ç§¯æœ¨ä¸€æ ·è½»æ¾ã€‚è¿™å°±æ˜¯ [Atomic Agents](https://github.com/BrainBlend-AI/atomic-agents) çš„ç†å¿µï¼Œä¸€ä¸ªåŸºäºŽ **Atomic Design** åŽŸåˆ™çš„æ¨¡å—åŒ–æ¡†æž¶ï¼Œç”¨äºŽæž„å»º AI ä»£ç†ã€‚éšç€ **1\\.0 ç‰ˆæœ¬** çš„å‘å¸ƒï¼ŒAtomic Agents å¼•å…¥äº†ä¸€ä¸ªå¼ºå¤§çš„ CLIï¼Œç§°ä¸º **Atomic Assembler**ï¼Œä½¿æž„å»ºã€ç®¡ç†å’Œéƒ¨ç½² AI åº”ç”¨ç¨‹åºå˜å¾—æ›´åŠ ç®€å•ã€‚\n\n## ä¸ºä»€ä¹ˆé€‰æ‹©åŽŸå­ä»£ç†ï¼Ÿ\n\nè®¸å¤šçŽ°æœ‰çš„**ä»£ç†äººå·¥æ™ºèƒ½**æ¡†æž¶ä¸“æ³¨äºŽæž„å»ºè‡ªä¸»çš„å¤šä»£ç†ç³»ç»Ÿï¼Œè¿™äº›ç³»ç»Ÿæ›´åƒæ˜¯å¥½å¥‡å¿ƒçš„äº§ç‰©ï¼Œè€Œä¸æ˜¯å®žç”¨å·¥å…·ã€‚è™½ç„¶è¿™äº›ç³»ç»Ÿå¯èƒ½å¼•äººå…¥èƒœï¼Œä½†å®ƒä»¬å¾€å¾€ç¼ºä¹çŽ°å®žåº”ç”¨æ‰€éœ€çš„å¯é¢„æµ‹æ€§å’ŒæŽ§åˆ¶èƒ½åŠ›ã€‚\n\nä¼ä¸šé€šå¸¸å¹¶ä¸å¸Œæœ›æœ‰ä¸€ä¸ªæ¯æ¬¡éƒ½ä»¥ä¸åŒé£Žæ ¼æ’°å†™æ–‡ç« çš„æœºå™¨äººã€‚ä»–ä»¬å¸Œæœ›åœ¨é£Žæ ¼ã€ç»“æž„å’Œè¯­è°ƒä¸Šä¿æŒä¸€è‡´ï¼Œä»¥ä¸Žå…¶å“ç‰Œå½¢è±¡ç›¸ä¸€è‡´ã€‚å¾®è°ƒæ¨¡åž‹æ˜¯ä¸€ç§æ–¹æ³•ï¼Œä½†å®ƒéœ€è¦å¤§é‡çš„æ•°æ®å’Œèµ„æºï¼Œå¹¶ä¸”åœ¨ä½¿ç”¨æœ€æ–°æ¨¡åž‹ï¼ˆå¦‚GPT-4ï¼‰æ—¶å¹¶ä¸æ€»æ˜¯å¯è¡Œçš„ã€‚\n\nåŽŸå­ä»£ç†æ—¨åœ¨é€šè¿‡æä¾›ä»¥ä¸‹åŠŸèƒ½æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼š\n\n* **æ¨¡å—åŒ–**ï¼šé€šè¿‡ç»„åˆç®€å•ã€å¯äº’æ¢çš„ç»„ä»¶æž„å»ºå¤æ‚çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿã€‚\n* **åŽŸå­æ€§**ï¼šåŽŸå­ä»£ç†ä¸­çš„æ¯ä¸ªç»„ä»¶ã€æ¯ä¸ªå·¥å…·ã€æ¯ä¸ªä»£ç†ã€æ¯ä¸ªä¸Šä¸‹æ–‡æä¾›è€…ï¼Œéƒ½å°½å¯èƒ½å•ä¸€ç›®çš„å’Œå¯é‡ç”¨ï¼Œç¡®ä¿è‰¯å¥½çš„å…³æ³¨ç‚¹åˆ†ç¦»ã€‚\n* **æŽ§åˆ¶**ï¼šå¾®è°ƒæ¯ä¸ªå•ç‹¬çš„æ­¥éª¤å’Œç»„ä»¶ï¼Œä»Žç³»ç»Ÿæç¤ºåˆ°å·¥å…·ã€‚\n* **å¯é¢„æµ‹æ€§**ï¼šç¡®ä¿å¯é‡å¤å’Œå¯é çš„è¾“å‡ºï¼Œé€‚åˆå•†ä¸šç”¨ä¾‹ã€‚\n* **å¯æ‰©å±•æ€§**ï¼šè½»æ¾æ·»åŠ æˆ–æ›¿æ¢ç»„ä»¶ï¼Œè€Œæ— éœ€å½»åº•æ”¹é€ æ•´ä¸ªç³»ç»Ÿã€‚\n\n## ä¼ ç»Ÿæ¨¡å—åŒ–æ–¹æ³•\n\nåœ¨ä¼ ç»Ÿè½¯ä»¶å¼€å‘ä¸­ï¼Œå¤æ‚é—®é¢˜è¢«åˆ†è§£ä¸ºæ›´å°ã€å¯ç®¡ç†çš„éƒ¨åˆ†ï¼š\n\n1. **å®šä¹‰é—®é¢˜**ï¼šä»Žæµç¨‹ã€ç”¨æˆ·æ•…äº‹æˆ–å®¢æˆ·æ—…ç¨‹å¼€å§‹ã€‚\n2. **åˆ†è§£**ï¼šå°†é—®é¢˜åˆ’åˆ†ä¸ºæ›´å°ã€å¯è§£å†³çš„ä»»åŠ¡ã€‚\n3. **å¼€å‘æ¨¡å—åŒ–ä»£ç **ï¼šç¼–å†™å¤„ç†ç‰¹å®šä»»åŠ¡çš„å‡½æ•°æˆ–ç±»ã€‚\n4. **é›†æˆ**ï¼šå°†è¿™äº›æ¨¡å—ç»„åˆæˆå®Œæ•´çš„åº”ç”¨ç¨‹åºã€‚\n\nAtomic Agents å°†è¿™ç§æ¨¡å—åŒ–å’Œå¯é¢„æµ‹æ€§å¸¦å…¥ AI ä»£ç†å¼€å‘ä¸­ã€‚\n\n## çœŸå®žä¸–ç•Œåœºæ™¯\n\nä¸Žå…¶æž„å»ºä¸€ä¸ªâ€œå†™åšå®¢æ–‡ç« â€çš„å•ä½“ AI ç³»ç»Ÿï¼Œä¸å¦‚è®¾è®¡ä¸€ä¸ªæ¨¡å—åŒ–ç³»ç»Ÿï¼Œèƒ½å¤Ÿï¼š\n\n1. **ç”Ÿæˆ** ä¸Žä¸»é¢˜ç›¸å…³çš„æŸ¥è¯¢ã€‚\n2. **è¯†åˆ«** æœ€ç›¸å…³çš„å‰ X ç¯‡æ–‡ç« ã€‚\n3. **è®¿é—®** æ¯ç¯‡è¯†åˆ«æ–‡ç« çš„é¡µé¢ã€‚\n4. **æå–** æ¯ç¯‡æ–‡ç« çš„æ–‡æœ¬ã€‚\n5. **ç”Ÿæˆ** æ¯ç¯‡æ–‡ç« çš„æ‘˜è¦ã€‚\n6. **å­˜å‚¨** æ‘˜è¦åˆ°å‘é‡æ•°æ®åº“ä¸­ã€‚\n7. **ç”Ÿæˆ** ä¸Žä¸»é¢˜ç›¸å…³çš„é—®é¢˜ã€‚\n8. **ä½¿ç”¨** å‘é‡æ•°æ®åº“å›žç­”è¿™äº›é—®é¢˜ã€‚\n9. **ç»¼åˆ** ç­”æ¡ˆæˆä¸€ç¯‡è¿žè´¯çš„åšå®¢æ–‡ç« ã€‚\n\nè¿™ç§æ–¹æ³•è™½ç„¶æ›´å†—é•¿ï¼Œä½†æä¾›äº†æ›´å¤§çš„æŽ§åˆ¶ã€å¯é æ€§å’Œé€‚ç”¨äºŽçŽ°å®žå•†ä¸šåº”ç”¨çš„é€‚åº”æ€§ã€‚\n\n## CLIçš„ä»‹ç»ï¼šAtomic Assembler\n\nç‰ˆæœ¬1.0ä¸­çš„ä¸€ä¸ªé‡è¦æ–°å¢žåŠŸèƒ½æ˜¯**Atomic Assembler** CLIã€‚è¿™ä¸ªå‘½ä»¤è¡Œå·¥å…·å…è®¸æ‚¨ï¼š\n\n* **ä¸‹è½½å’Œç®¡ç†å·¥å…·**ï¼šè½»æ¾å°†æ–°å·¥å…·æˆ–ä»£ç†æ·»åŠ åˆ°æ‚¨çš„é¡¹ç›®ä¸­ã€‚\n* **é¿å…ä¸å¿…è¦çš„ä¾èµ–**ï¼šä»…å®‰è£…æ‚¨æ‰€éœ€çš„å†…å®¹ã€‚\n* **è½»æ¾ä¿®æ”¹å·¥å…·**ï¼šæ¯ä¸ªå·¥å…·éƒ½æœ‰è‡ªå·±çš„æµ‹è¯•å’Œæ–‡æ¡£ã€‚\n* **ç›´æŽ¥è®¿é—®å·¥å…·**ï¼šå¦‚æžœæ‚¨æ„¿æ„ï¼Œå¯ä»¥æ‰‹åŠ¨ç®¡ç†å·¥å…·ï¼Œè€Œæ— éœ€ä½¿ç”¨CLIã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*aDceAIINxyFDOvle.png)\n\n## ä»£ç†çš„æž„æˆ\n\nAI ä»£ç†ï¼Œç‰¹åˆ«æ˜¯åœ¨ Atomic Agents æ¡†æž¶ä¸­ï¼Œç”±å‡ ä¸ªå…³é”®ç»„ä»¶ç»„æˆï¼š\n\n* **ç³»ç»Ÿæç¤º**ï¼šå®šä¹‰ä»£ç†çš„è¡Œä¸ºå’Œç›®çš„ã€‚\n* **ç”¨æˆ·è¾“å…¥**ï¼šç”¨æˆ·æä¾›çš„æ•°æ®ã€‚\n* **å·¥å…·**ï¼šä»£ç†å¯ä»¥åˆ©ç”¨çš„å¤–éƒ¨å‡½æ•°æˆ– APIã€‚\n* **è®°å¿†**ï¼šè·Ÿè¸ªå¯¹è¯æˆ–çŠ¶æ€ã€‚\n\næ¯ä¸ªç»„ä»¶éƒ½è®¾è®¡ä¸ºæ¨¡å—åŒ–å’Œå¯äº’æ¢ï¼Œéµå¾ªå…³æ³¨ç‚¹åˆ†ç¦»å’Œå•ä¸€è´£ä»»åŽŸåˆ™ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*yt-5SoQC6uXTAd1-)\n\n## æ¨¡å—åŒ–çš„åŠ›é‡\n\né€šè¿‡å°†ä»£ç†åˆ†è§£ä¸ºè¿™äº›åŸºæœ¬ç»„ä»¶ï¼Œæ‚¨å¯ä»¥ï¼š\n\n* **æ›´æ¢å·¥å…·** è€Œä¸å½±å“ç³»ç»Ÿçš„å…¶ä½™éƒ¨åˆ†ã€‚\n* **å¾®è°ƒæç¤º** ä»¥è°ƒæ•´ä»£ç†çš„è¡Œä¸ºã€‚\n* **æ— ç¼è¿žæŽ¥ä»£ç†å’Œå·¥å…·**ï¼Œé€šè¿‡åŒ¹é…å®ƒä»¬çš„è¾“å…¥å’Œè¾“å‡ºæ¨¡å¼ã€‚\n\n## ä½¿ç”¨å‘½ä»¤è¡Œç•Œé¢ï¼šåŽŸå­æ±‡ç¼–å™¨\n\n## å®‰è£…\n\nè¦å¼€å§‹ä½¿ç”¨ Atomic Agents å’Œ CLIï¼Œè¯·é€šè¿‡ pip å®‰è£…è¯¥è½¯ä»¶åŒ…ï¼š\n\n```python\npip install atomic-agents\n```\n\n## è¿è¡Œ CLI\n\nä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨ CLIï¼š\n\n```python\natomic\n```\n\næˆ–è€…ï¼Œå¦‚æžœæ‚¨ä½¿ç”¨ Poetry å®‰è£…äº† Atomic Agentsï¼š\n\n```python\npoetry run atomic\n```\n\næ‚¨å°†çœ‹åˆ°ä¸€ä¸ªèœå•ï¼Œç”¨äºŽä¸‹è½½å’Œç®¡ç†å·¥å…·ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*SzRlpA0-ivcE2qhk)\n\n*å›¾åƒï¼šAtomic CLI ä¸»èœå•*\n\næ¯ä¸ªå·¥å…·åŒ…æ‹¬ï¼š\n\n* **è¾“å…¥æ¨¡å¼**\n* **è¾“å‡ºæ¨¡å¼**\n* **ä½¿ç”¨ç¤ºä¾‹**\n* **ä¾èµ–é¡¹**\n* **å®‰è£…è¯´æ˜Ž**\n\n## ç®¡ç†å·¥å…·\n\nAtomic Assembler CLI æä¾›äº†å¯¹æ‚¨å·¥å…·çš„å®Œå…¨æŽ§åˆ¶ï¼Œè®©æ‚¨å¯ä»¥ï¼š\n\n* **é¿å…ä¾èµ–æ‚ä¹±**ï¼šä»…å®‰è£…æ‚¨éœ€è¦çš„å·¥å…·ã€‚\n* **è½»æ¾ä¿®æ”¹å·¥å…·**ï¼šæ¯ä¸ªå·¥å…·éƒ½æ˜¯è‡ªåŒ…å«çš„ï¼Œæ‹¥æœ‰è‡ªå·±çš„æµ‹è¯•ã€‚\n* **ç›´æŽ¥è®¿é—®å·¥å…·**ï¼šå¦‚æžœæ‚¨æ„¿æ„ï¼Œå¯ä»¥æ‰‹åŠ¨ç®¡ç†å·¥å…·æ–‡ä»¶å¤¹ã€‚\n\n## ä¸Šä¸‹æ–‡æä¾›è€…\n\nAtomic Agents å¼•å…¥äº† **ä¸Šä¸‹æ–‡æä¾›è€…**ï¼Œä»¥å¢žå¼ºæ‚¨çš„ä»£ç†çš„åŠ¨æ€ä¸Šä¸‹æ–‡ã€‚ä¸Šä¸‹æ–‡æä¾›è€…å…è®¸æ‚¨åœ¨è¿è¡Œæ—¶å°†é¢å¤–ä¿¡æ¯æ³¨å…¥ä»£ç†çš„ç³»ç»Ÿæç¤ºä¸­ã€‚\n\n## ä½¿ç”¨ä¸Šä¸‹æ–‡æä¾›è€…\n\n**åˆ›å»ºä¸Šä¸‹æ–‡æä¾›è€…ç±»**ï¼šå­ç±»åŒ– `SystemPromptContextProviderBase` å¹¶å®žçŽ° `get_info()` æ–¹æ³•ã€‚\n\n```python\nfrom atomic_agents.lib.components.system_prompt_generator import SystemPromptContextProviderBase   \n\nclass SearchResultsProvider(SystemPromptContextProviderBase):\n      def __init__(self, title: str, search_results: List[str]):\n          super().__init__(title=title)\n          self.search_results = search_results\n\n       def get_info(self) -> str:\n          return \"\\n\".join(self.search_results)\n```\n\n**å°†ä¸Šä¸‹æ–‡æä¾›è€…æ³¨å†Œåˆ°ä»£ç†**ï¼š\n\n```python\n## ä½¿ç”¨åŠ¨æ€æ•°æ®åˆå§‹åŒ–ä¸Šä¸‹æ–‡æä¾›è€…\nsearch_results_provider = SearchResultsProvider(\n      title=\"æœç´¢ç»“æžœ\",\n      search_results=[\"ç»“æžœ 1\", \"ç»“æžœ 2\", \"ç»“æžœ 3\"]\n)   \n\n## å°†ä¸Šä¸‹æ–‡æä¾›è€…æ³¨å†Œåˆ°ä»£ç†  \nagent.register_context_provider(\"search_results\", search_results_provider)\n```\n\nè¿™ä½¿å¾—æ‚¨çš„ä»£ç†èƒ½å¤Ÿåœ¨å…¶ç³»ç»Ÿæç¤ºä¸­åŒ…å«åŠ¨æ€æ•°æ®ï¼Œå¦‚æœç´¢ç»“æžœï¼Œä»Žè€Œæ ¹æ®æœ€æ–°ä¿¡æ¯å¢žå¼ºå…¶å“åº”ã€‚\n\n## é“¾æŽ¥æ¨¡å¼å’Œä»£ç†\n\nAtomic Agents é€šè¿‡å¯¹é½å®ƒä»¬çš„è¾“å…¥å’Œè¾“å‡ºæ¨¡å¼æ¥ç®€åŒ–ä»£ç†å’Œå·¥å…·çš„é“¾æŽ¥ã€‚è¿™ä¸ªè®¾è®¡ä¿ƒè¿›äº†æ¨¡å—åŒ–å’Œå¯é‡ç”¨æ€§ã€‚\n\n### ç¤ºä¾‹ï¼šä¸ºä¸åŒæœç´¢æä¾›è€…ç”ŸæˆæŸ¥è¯¢\n\nå‡è®¾æ‚¨æœ‰ä¸€ä¸ªç”Ÿæˆæœç´¢æŸ¥è¯¢çš„ä»£ç†ï¼Œå¹¶ä¸”æ‚¨å¸Œæœ›å°†è¿™äº›æŸ¥è¯¢ä¸Žä¸åŒçš„æœç´¢å·¥å…·ä¸€èµ·ä½¿ç”¨ã€‚é€šè¿‡å°†ä»£ç†çš„è¾“å‡ºæ¨¡å¼ä¸Žæœç´¢å·¥å…·çš„è¾“å…¥æ¨¡å¼å¯¹é½ï¼Œæ‚¨å¯ä»¥è½»æ¾åœ°å°†å®ƒä»¬ä¸²è”æˆ–åœ¨æä¾›è€…ä¹‹é—´åˆ‡æ¢ã€‚\n\n```python\nimport instructor\nimport openai\nfrom pydantic import Field\nfrom atomic_agents.agents.base_agent import BaseIOSchema, BaseAgent, BaseAgentConfig\nfrom atomic_agents.lib.components.system_prompt_generator import SystemPromptGenerator\n\n## Import the search tool\nfrom web_search_agent.tools.searxng_search import SearxNGSearchTool\nclass QueryAgentInputSchema(BaseIOSchema):\n    \"\"\"Input schema for the QueryAgent.\"\"\"\n    instruction: str = Field(..., description=\"Instruction to generate search queries for.\")\n    num_queries: int = Field(..., description=\"Number of queries to generate.\")\n\n\n## Initialize the query agent\nquery_agent = BaseAgent(\n    BaseAgentConfig(\n        client=instructor.from_openai(openai.OpenAI()),\n        model=\"gpt-4\",\n        system_prompt_generator=SystemPromptGenerator(\n            background=[\n                \"You are an intelligent query generation expert.\",\n                \"Your task is to generate diverse and relevant queries based on a given instruction.\"\n            ],\n            steps=[\n                \"Receive the instruction and the number of queries.\",\n                \"Generate the queries in JSON format.\"\n            ],\n            output_instructions=[\n                \"Ensure each query is unique and relevant.\",\n                \"Provide the queries in the expected schema.\"\n            ],\n        ),\n        input_schema=QueryAgentInputSchema,\n        output_schema=SearxNGSearchTool.input_schema,  # Align output schema\n    )\n)\n```\n\n**æ¨¡å—åŒ–**ï¼šé€šè¿‡å°†`query_agent`çš„`output_schema`è®¾ç½®ä¸ºä¸Ž`SearxNGSearchTool`çš„`input_schema`åŒ¹é…ï¼Œæ‚¨å¯ä»¥ç›´æŽ¥å°†ä»£ç†çš„è¾“å‡ºç”¨ä½œå·¥å…·çš„è¾“å…¥ã€‚\n\n**å¯åˆ‡æ¢æ€§**ï¼šè¦åˆ‡æ¢åˆ°ä¸åŒçš„æœç´¢æä¾›è€…ï¼Œå¯¼å…¥å¦ä¸€ä¸ªæœç´¢å·¥å…·å¹¶æ›´æ–°`output_schema`ï¼š\n\n```python\n## Import a different search tool\nfrom web_search_agent.tools.another_search import AnotherSearchTool\n\n## Update the output schema\nquery_agent.config.output_schema = AnotherSearchTool.input_schema\n```\n\n## ç¤ºä¾‹ï¼šæž„å»ºä¸€ä¸ªç®€å•çš„ AI ä»£ç†\n\nçŽ°åœ¨æˆ‘ä»¬å·²ç»ä»‹ç»äº†åŸºç¡€çŸ¥è¯†ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨ Atomic Agents æž„å»ºä¸€ä¸ªç®€å•çš„ AI ä»£ç†ï¼Œå¹¶æŽ¢è®¨å®ƒçš„å†…éƒ¨å·¥ä½œåŽŸç†ã€‚\n\n## ç¬¬ä¸€æ­¥ï¼šå®‰è£…\n\né¦–å…ˆï¼Œå®‰è£…å¿…è¦çš„è½¯ä»¶åŒ…ï¼š\n\n```python\npip install atomic-agents openai instructor\n```\n\n## æ­¥éª¤ 2ï¼šå¯¼å…¥ç»„ä»¶\n\nå¯¼å…¥å¿…è¦çš„ç»„ä»¶ï¼š\n\n```python\nimport os\nfrom atomic_agents.agents.base_agent import BaseAgent, BaseAgentConfig, BaseIOSchema\nfrom atomic_agents.lib.components.system_prompt_generator import SystemPromptGenerator\nfrom atomic_agents.lib.components.agent_memory import AgentMemory\nfrom pydantic import Field\nimport instructor\nimport openai\n```\n\n## æ­¥éª¤ 3ï¼šå®šä¹‰è‡ªå®šä¹‰è¾“å‡ºæ¨¡å¼\n\n```python\nclass CustomOutputSchema(BaseIOSchema):\n    chat_message: str = Field(..., description=\"The chat message from the agent.\")\n    suggested_questions: List[str] = Field(..., description=\"Suggested follow-up questions.\")\n```\n\n## æ­¥éª¤ 4ï¼šè®¾ç½®ç³»ç»Ÿæç¤º\n\n```python\nsystem_prompt_generator = SystemPromptGenerator(\n    background=[\"è¿™ä¸ªåŠ©æ‰‹çŸ¥è¯†æ¸Šåšã€ä¹äºŽåŠ©äººï¼Œå¹¶å»ºè®®åŽç»­é—®é¢˜ã€‚\"],\n    steps=[\n        \"åˆ†æžç”¨æˆ·çš„è¾“å…¥ï¼Œä»¥ç†è§£ä¸Šä¸‹æ–‡å’Œæ„å›¾ã€‚\",\n        \"åˆ¶å®šç›¸å…³ä¸”ä¿¡æ¯ä¸°å¯Œçš„å›žåº”ã€‚\",\n        \"ä¸ºç”¨æˆ·ç”Ÿæˆ 3 ä¸ªå»ºè®®çš„åŽç»­é—®é¢˜ã€‚\"\n    ],\n    output_instructions=[\n        \"å¯¹ç”¨æˆ·æŸ¥è¯¢æä¾›æ¸…æ™°ç®€æ´çš„ä¿¡æ¯ã€‚\",\n        \"åœ¨æ¯ä¸ªå›žåº”çš„ç»“å°¾æä¾› 3 ä¸ªä¸Žç”¨æˆ·ç›¸å…³çš„å»ºè®®é—®é¢˜ã€‚\"\n    ]\n)\n```\n\n## ç¬¬5æ­¥ï¼šåˆå§‹åŒ–ä»£ç†\n\n```python\n## Initialize memory (optional)\nmemory = AgentMemory()\n\n## Initialize the agent\nagent = BaseAgent(\n    config=BaseAgentConfig(\n        client=instructor.from_openai(openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))),\n        model=\"gpt-4o-mini\",\n        system_prompt_generator=system_prompt_generator,\n        memory=memory,\n        output_schema=CustomOutputSchema\n    )\n)\n```\n\n## ç¬¬6æ­¥ï¼šä½¿ç”¨ä»£ç†\n\n```python\nuser_input = \"Can you explain the benefits of using Atomic Agents?\"\nresponse = agent.run(agent.input_schema(chat_message=user_input))\nprint(f\"Agent: {response.chat_message}\")\nprint(\"Suggested questions:\")\nfor question in response.suggested_questions:\n    print(f\"- {question}\")\n```\n\n## å¹•åŽå‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ\n\n* **System Prompt**: å®šä¹‰ä»£ç†çš„è¡Œä¸ºå¹¶æŒ‡å¯¼LLMã€‚\n* **Input Schema**: éªŒè¯ç”¨æˆ·çš„è¾“å…¥ã€‚\n* **Output Schema**: ç¡®ä¿ä»£ç†çš„å“åº”ç¬¦åˆé¢„æœŸæ ¼å¼ã€‚\n* **Memory**: è®°å½•å¯¹è¯åŽ†å²ã€‚\n\n## ç»“è®º\n\nAtomic Agents 1\\.0 ä¸º AI ä»£ç†å¼€å‘å¸¦æ¥äº†æ¨¡å—åŒ–ã€æŽ§åˆ¶å’Œçµæ´»æ€§ã€‚éšç€ Atomic Assembler CLI çš„å¼•å…¥ä»¥åŠä¸Šä¸‹æ–‡æä¾›è€…å’Œæ¨¡å¼é“¾ç­‰åŠŸèƒ½ï¼Œæž„å»ºå¤æ‚çš„ AI åº”ç”¨ç¨‹åºå˜å¾—å‰æ‰€æœªæœ‰çš„ç®€å•ã€‚\n\næ— è®ºæ‚¨æ˜¯å¸Œæœ›æž„å»º AI é©±åŠ¨å·¥å…·çš„å¼€å‘äººå‘˜ï¼Œè¿˜æ˜¯å¸Œæœ›è‡ªåŠ¨åŒ–å¤æ‚ä»»åŠ¡çš„ä¼ä¸šï¼ŒAtomic Agents éƒ½æä¾›äº†åˆ›å»ºå¯é ä¸”æ˜“äºŽç»´æŠ¤çš„ AI ç³»ç»Ÿçš„åŸºç¡€æž„ä»¶ã€‚\n\n## ä»Šå¤©å¼€å§‹\n\n* **GitHub ä»“åº“**: [BrainBlend\\-AI/atomic\\-agents](https://github.com/BrainBlend-AI/atomic-agents)\n* **API æ–‡æ¡£**: [Atomic Agents API æ–‡æ¡£](https://brainblend-ai.github.io/atomic-agents/)\n* **ç¤ºä¾‹ç›®å½•**: [Atomic ç¤ºä¾‹](https://github.com/BrainBlend-AI/atomic-agents/tree/main/atomic-examples)\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/introducing-microsofts-magentic-one-agentic-framework-7dcc16de691e","frontmatter":{"title":"å¾®è½¯ Magentic-One ä»£ç†æ¡†æž¶ä»‹ç»","meta_title":"å¾®è½¯ Magentic-One ä»£ç†æ¡†æž¶ä»‹ç»","description":"å¾®è½¯æœ€è¿‘æŽ¨å‡ºäº†Magentic-Oneï¼Œä¸€ä¸ªé«˜æ€§èƒ½çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œæ—¨åœ¨æ‰§è¡Œå¤æ‚ä»»åŠ¡ã€‚è¯¥ç³»ç»Ÿç”±ä¸€ä¸ªç¼–æŽ’è€…æ™ºèƒ½ä½“å’Œå››ä¸ªä¸“é—¨æ™ºèƒ½ä½“ç»„æˆï¼Œèƒ½å¤Ÿè¿›è¡Œç½‘é¡µæµè§ˆã€æ–‡ä»¶æ“ä½œã€ç¼–ç å’Œç»ˆç«¯æŒ‡ä»¤ã€‚Magentic-OneåŸºäºŽå¾®è½¯çš„Autogenæ¡†æž¶ï¼Œå…·æœ‰å¼ºå¤§çš„ä»»åŠ¡è§„åˆ’å’Œæ‰§è¡Œèƒ½åŠ›ï¼Œä½†ä¹Ÿé¢ä¸´æ½œåœ¨é£Žé™©ï¼Œå¦‚é”™è¯¯é…ç½®å¯èƒ½å¯¼è‡´ä¸å½“è¡Œä¸ºã€‚ç”¨æˆ·å¯ä»¥é€šè¿‡GitHubå’ŒOpenAIç­‰å¹³å°å®‰è£…å’Œé…ç½®è¯¥ç³»ç»Ÿï¼Œè¿›è¡Œå¤šç§åº”ç”¨ç¤ºä¾‹ï¼ŒåŒ…æ‹¬ç¼–å†™Pythonä»£ç å’Œç½‘ç»œæœç´¢ã€‚","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*dJj20_4jYYp32Crl","categories":["Programming","Autonomous Systems","Technology/Web"],"author":"Rifx.Online","tags":["Magnetic-One","orchestrator","agents","Autogen","Python"],"draft":false,"slug":"blog/introducing-microsofts-magentic-one-agentic-framework-7dcc16de691e"},"content":"\n\n\n### ä¸€ä¸ªå¯ä»¥æ‰§è¡Œå¤æ‚ä»»åŠ¡çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ\n\nå¤§çº¦ä¸€å‘¨å‰ï¼Œå¾®è½¯å‘å¸ƒäº†ä¸€ä¸ªåä¸º **Magentic-One** çš„æ–°æ™ºèƒ½ç³»ç»Ÿï¼Œæ—¨åœ¨â€œè§£å†³å¤æ‚ä»»åŠ¡â€ï¼Œè¿™ä¼¼ä¹Žå®Œå…¨æ²¡æœ‰å¼•èµ·æ³¨æ„ã€‚éšç€æœ€è¿‘å…³äºŽAnthropicè®¡ç®—æœºä½¿ç”¨èƒ½åŠ›çš„çƒ­è®®ï¼Œå¾®è½¯ä¼¼ä¹Žå¸Œæœ›é‡æ–°ç¡®ç«‹å…¶åœ¨è¿™ä¸€é¢†åŸŸçš„èµ„è´¨ã€‚\n\nåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»Magentic-Oneï¼Œè§£é‡Šå…¶èƒ½åŠ›ï¼Œå¹¶è®¨è®ºå¦‚ä½•ä½¿ç”¨å®ƒæ¥å®Œæˆæœ‰ç”¨çš„å·¥ä½œã€‚\n\n\n\næ ¹æ®å¾®è½¯è‡ªå·±çš„å…¬å‘Šï¼ˆæ–‡ç« æœ«å°¾æœ‰é“¾æŽ¥ï¼‰ï¼ŒMagentic-Oneæ˜¯â€¦\n\nâ€œ...ä¸€ä¸ªé«˜æ€§èƒ½çš„é€šç”¨æ™ºèƒ½ä½“ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³æ­¤ç±»ä»»åŠ¡ã€‚Magentic-Oneé‡‡ç”¨å¤šæ™ºèƒ½ä½“æž¶æž„ï¼Œå…¶ä¸­ä¸»æ™ºèƒ½ä½“â€”â€”ç¼–æŽ’è€…ï¼ŒæŒ‡æŒ¥å…¶ä»–å››ä¸ªæ™ºèƒ½ä½“æ¥è§£å†³ä»»åŠ¡ã€‚ç¼–æŽ’è€…è¿›è¡Œä»»åŠ¡è§„åˆ’ã€è·Ÿè¸ªè¿›åº¦ï¼Œå¹¶åœ¨å‡ºçŽ°é”™è¯¯æ—¶é‡æ–°è§„åˆ’ï¼ŒåŒæ—¶æŒ‡æŒ¥ä¸“é—¨çš„æ™ºèƒ½ä½“æ‰§è¡Œè¯¸å¦‚æ“ä½œç½‘é¡µæµè§ˆå™¨ã€å¯¼èˆªæœ¬åœ°æ–‡ä»¶æˆ–ç¼–å†™å’Œæ‰§è¡ŒPythonä»£ç ç­‰ä»»åŠ¡ã€‚â€\n\nMagentic-Oneå»ºç«‹åœ¨å¾®è½¯çŽ°æœ‰çš„ **Autogen** äº§å“ä¹‹ä¸Šï¼Œè¯¥äº§å“æ˜¯å…¶å¼€æºå¤šæ™ºèƒ½ä½“æ¡†æž¶ã€‚\n\nMagentic-Oneæœ‰äº”ä¸ªå…³é”®ç»„ä»¶ã€‚\n\n**1/ ç¼–æŽ’è€…æ™ºèƒ½ä½“**\n\nè´Ÿè´£ä»»åŠ¡åˆ†è§£å’Œè§„åˆ’ï¼Œå¹¶å°†å­ä»»åŠ¡æŒ‡æ´¾ç»™å…¶ä»–æ™ºèƒ½ä½“æ‰§è¡Œã€‚è·Ÿè¸ªä»»åŠ¡å®Œæˆçš„è¿›åº¦ï¼Œå¹¶æ ¹æ®éœ€è¦é‡‡å–çº æ­£æŽªæ–½ã€‚\n\n**2/ ç½‘é¡µæµè§ˆæ™ºèƒ½ä½“**\n\nä¸“æ³¨äºŽæŽ§åˆ¶å’Œç®¡ç†åŸºäºŽChromiumçš„ç½‘é¡µæµè§ˆå™¨çš„çŠ¶æ€ã€‚å¯¹äºŽæ¯ä¸ªä¼ å…¥è¯·æ±‚ï¼Œç½‘é¡µæµè§ˆè€…åœ¨æµè§ˆå™¨ä¸­æ‰§è¡ŒæŒ‡å®šçš„æ“ä½œï¼Œç„¶åŽæŠ¥å‘Šç½‘é¡µçš„æ›´æ–°çŠ¶æ€ã€‚å…¶æ“ä½œåŒ…æ‹¬ï¼š\n\n* **å¯¼èˆª**ï¼ˆä¾‹å¦‚ï¼Œè®¿é—®URLï¼Œè¿›è¡Œç½‘é¡µæœç´¢ï¼‰ï¼Œ\n* **é¡µé¢äº¤äº’**ï¼ˆä¾‹å¦‚ï¼Œç‚¹å‡»å…ƒç´ ï¼Œè¾“å…¥å†…å®¹ï¼‰ï¼Œ\n* **é˜…è¯»ä¸Žç†è§£**ï¼ˆä¾‹å¦‚ï¼Œæ€»ç»“å†…å®¹ï¼Œå›žç­”é—®é¢˜ï¼‰ã€‚\n\nç½‘é¡µæµè§ˆè€…åˆ©ç”¨æµè§ˆå™¨çš„å¯è®¿é—®æ€§æ ‘å’Œä¸€å¥—æ ‡è®°æç¤ºæŠ€æœ¯æœ‰æ•ˆåœ°æ‰§è¡Œä»»åŠ¡ã€‚\n\n**3/ æ–‡ä»¶æµè§ˆæ™ºèƒ½ä½“**\n\nå¯ä»¥è¯»å–å¤§å¤šæ•°ç±»åž‹çš„æœ¬åœ°æ–‡ä»¶ï¼Œå¹¶æ‰§è¡Œå¸¸è§çš„å¯¼èˆªä»»åŠ¡ï¼Œä¾‹å¦‚åˆ—å‡ºç›®å½•å†…å®¹å’Œå¯¼èˆªæ–‡ä»¶å¤¹ç»“æž„ã€‚\n\n**4/ ç¼–ç æ™ºèƒ½ä½“**\n\nä¸€ä¸ªåŸºäºŽLLMçš„æ™ºèƒ½ä½“ï¼Œä¸“é—¨ç”¨äºŽç¼–å†™ä»£ç ã€åˆ†æžä»Žå…¶ä»–æ™ºèƒ½ä½“æ”¶é›†çš„ä¿¡æ¯æˆ–åˆ›å»ºæ–°å·¥ä»¶ã€‚\n\n**5/ ç»ˆç«¯æ™ºèƒ½ä½“**\n\næä¾›å¯¹æŽ§åˆ¶å°Shellçš„è®¿é—®ï¼Œå¯ä»¥åœ¨å…¶ä¸­æ‰§è¡Œç¼–ç æ™ºèƒ½ä½“çš„ç¨‹åºï¼Œå¹¶å¯ä»¥å®‰è£…æ–°çš„ç¼–ç¨‹åº“ã€‚\n\n### é£Žé™©\n\nåœ¨ç»§ç»­ä¹‹å‰ï¼Œæˆ‘æƒ³å¼ºè°ƒå¾®è½¯åœ¨å…¶å…¬å‘Šä¸­æåˆ°çš„ä¸€ä¸ªç‰¹åˆ«æ–¹é¢ï¼Œå³ä½¿ç”¨åƒè¿™æ ·çš„Agenticç³»ç»Ÿæ‰€é¢ä¸´çš„é£Žé™©ã€‚è¿™ç¡®å®žå¼•èµ·äº†æ³¨æ„ã€‚\n\n> åƒMagentic\\-Oneè¿™æ ·çš„Agenticç³»ç»Ÿä»£è¡¨äº†åœ¨ä¸–ç•Œä¸Šæ‹¥æœ‰AIç³»ç»Ÿçš„æœºä¼šå’Œé£Žé™©çš„ç›¸ä½è½¬å˜ã€‚Magentic\\-Oneä¸Žä¸€ä¸ªä¸ºäººç±»è®¾è®¡å¹¶ç”±äººç±»å±…ä½çš„æ•°å­—ä¸–ç•Œè¿›è¡Œäº¤äº’ã€‚å®ƒå¯ä»¥é‡‡å–è¡ŒåŠ¨ï¼Œæ”¹å˜ä¸–ç•Œçš„çŠ¶æ€ï¼Œå¹¶å¯¼è‡´å¯èƒ½æ˜¯ä¸å¯é€†è½¬çš„åŽæžœã€‚è¿™å¸¦æ¥äº†å›ºæœ‰ä¸”ä¸å¯å¦è®¤çš„é£Žé™©ï¼Œæˆ‘ä»¬åœ¨æµ‹è¯•è¿‡ç¨‹ä¸­è§‚å¯Ÿåˆ°äº†æ–°å…´é£Žé™©çš„ä¾‹å­ã€‚ä¾‹å¦‚ï¼Œåœ¨å¼€å‘è¿‡ç¨‹ä¸­ï¼Œé”™è¯¯é…ç½®å¯¼è‡´ä»£ç†æ— æ³•æˆåŠŸç™»å½•åˆ°ç‰¹å®šçš„WebArenaç½‘ç«™ã€‚ä»£ç†å°è¯•ç™»å½•è¯¥ç½‘ç«™ï¼Œç›´åˆ°é‡å¤çš„å°è¯•å¯¼è‡´è´¦æˆ·è¢«æš‚æ—¶æš‚åœã€‚ç„¶åŽï¼Œä»£ç†å°è¯•é‡ç½®è´¦æˆ·çš„å¯†ç ã€‚æ›´ä»¤äººæ‹…å¿§çš„æ˜¯ï¼Œåœ¨å°‘æ•°æƒ…å†µä¸‹â€”â€”å¹¶ä¸”åœ¨æœªè¢«æç¤ºçš„æƒ…å†µä¸‹â€”â€”ä»£ç†å¶å°”è¯•å›¾æ‹›å‹Ÿå…¶ä»–äººæ¥å¯»æ±‚å¸®åŠ©ï¼ˆä¾‹å¦‚ï¼Œé€šè¿‡ç¤¾äº¤åª’ä½“å‘å¸ƒã€ç»™æ•™ç§‘ä¹¦ä½œè€…å‘é€ç”µå­é‚®ä»¶ï¼Œæˆ–è€…åœ¨ä¸€ä¸ªæ¡ˆä¾‹ä¸­ï¼Œå‘æ”¿åºœå®žä½“èµ·è‰ä¿¡æ¯è‡ªç”±è¯·æ±‚ï¼‰ã€‚åœ¨è¿™äº›æƒ…å†µä¸‹ï¼Œä»£ç†éƒ½å¤±è´¥äº†ï¼Œå› ä¸ºå®ƒä»¬æ²¡æœ‰è®¿é—®æ‰€éœ€å·¥å…·æˆ–è´¦æˆ·çš„æƒé™ï¼Œå’Œ/æˆ–è¢«äººç±»è§‚å¯Ÿè€…é˜»æ­¢ã€‚\n\nå¥½çš„ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•ä½¿ç”¨Magentic\\-Oneæ¥åšä¸€äº›æœ‰ç”¨çš„å·¥ä½œçš„ä¾‹å­ã€‚å¸Œæœ›åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­æˆ‘ä»¬ä¸ä¼šæ‘§æ¯ä¸–ç•Œã€‚ðŸ˜‰\n\n### å®‰è£… Magentic-One\n\næˆ‘æ˜¯ Windows ç”¨æˆ·ï¼Œä½†æˆ‘å°†ä½¿ç”¨ WSL2 Ubuntu for Windows å®‰è£…ä»£ç ã€‚å¦‚æžœä½ æƒ³ä¸€èµ·æ“ä½œï¼Œæˆ‘åœ¨è¿™é‡Œæœ‰ä¸€ä¸ªå…³äºŽå®‰è£… WSL2 Ubuntu çš„å®Œæ•´æŒ‡å— [here](https://readmedium.com/installing-wsl2-ubuntu-for-windows-81122c551bc2)ã€‚\n\nè¯·é€šè¿‡ç‚¹å‡» [here](https://github.com/microsoft/autogen/tree/main/python/packages/autogen-magentic-one) å‰å¾€ Magentic-One çš„ GitHub ä»“åº“ã€‚åœ¨ä½ çš„æœ¬åœ°ç³»ç»Ÿä¸Šè¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼ˆåœ¨ä½ é€šå¸¸æ”¾ç½®é¡¹ç›®çš„åœ°æ–¹ï¼‰ã€‚\n\n```python\ngit clone https://github.com/microsoft/autogen.git\n\ncd autogen/python\n\nuv sync --all-extras\n\nsource .venv/bin/activate\n\ncd packages/autogen-magentic-one\n```\næŽ¥ä¸‹æ¥ï¼Œé…ç½®èŠå¤©å®Œæˆå®¢æˆ·ç«¯çš„çŽ¯å¢ƒå˜é‡ã€‚ç›®å‰ï¼ŒMagentic-One ä»…æ”¯æŒ OpenAI çš„ GPT-4o ä½œä¸ºåº•å±‚ LLMã€‚\n\nä½ å¯ä»¥é€šè¿‡ OpenAI æˆ– Azure Active Directory è®¾ç½®æ­¤é…ç½®ã€‚ä»¥ä¸‹æ˜¯ä½¿ç”¨ OpenAI çš„è¯´æ˜Žã€‚\n\n```python\nexport CHAT_COMPLETION_PROVIDER='openai'\n\nexport CHAT_COMPLETION_KWARGS_JSON='{\"api_key\": \"gpt-4o\"}'\n```\n\n> **éœ€è¦æ³¨æ„çš„ä¸€ç‚¹æ˜¯ï¼Œå¦‚æžœä½ æœ‰ GitHub è´¦æˆ·ï¼Œä½ å¯ä»¥ä½¿ç”¨ GitHub æ¨¡åž‹ä¸­çš„ GPT4-o æ¨¡åž‹ï¼Œè¿™å°†ä¸ºä½ æä¾›å…è´¹è®¿é—® GPT4â€“o çš„æƒé™ã€‚ç„¶è€Œï¼Œä½¿ç”¨é™åˆ¶å¯èƒ½ä¼šæœ‰äº›ä¸¥æ ¼ã€‚**\n\nè¦é€šè¿‡ GitHub æ¨¡åž‹è¿›è¡Œæ“ä½œï¼Œè¯·ç‚¹å‡» [here](https://github.com/marketplace/models) å¹¶ä½¿ç”¨ä½ çš„ GitHub è´¦æˆ·ç™»å½•ï¼Œæˆ–è€…å¦‚æžœä½ è¿˜æ²¡æœ‰è´¦æˆ·ï¼Œå¯ä»¥åˆ›å»ºä¸€ä¸ªã€‚ç‚¹å‡» GPT-4o æŒ‰é’®ã€‚åœ¨æ˜¾ç¤ºçš„é¡µé¢çš„å³ä¸Šè§’ï¼Œä¼šæœ‰ä¸€ä¸ªç»¿è‰²çš„ `Get API Key` æŒ‰é’®ã€‚ç‚¹å‡»å®ƒï¼Œç„¶åŽä»Žé‚£é‡Œç‚¹å‡» `Get Developer Key` æŒ‰é’®ã€‚\n\næœ€åŽï¼Œä½ åº”è¯¥ä¼šçœ‹åˆ°ä¸€ä¸ªå±å¹•ï¼Œå¯ä»¥ç”Ÿæˆä¸€ä¸ªç»å…¸çš„ä¸ªäººè®¿é—®ä»¤ç‰Œã€‚çŽ°åœ¨å°±åŽ»åšå§ã€‚ä½ éœ€è¦è¾“å…¥ä¸€ä¸ªæè¿°å¯†é’¥ç”¨é€”çš„å¤‡æ³¨ï¼Œä½† **ä½ ä¸éœ€è¦** èµ‹äºˆå®ƒä»»ä½•é¢å¤–çš„æƒé™ã€‚è®°ä¸‹ç”Ÿæˆçš„å¯†é’¥ã€‚\n\nè¦ä½¿ç”¨ GitHub GPT4-o æ¨¡åž‹ï¼Œè¯·æŒ‰å¦‚ä¸‹æ–¹å¼æ›´æ”¹ä½ çš„çŽ¯å¢ƒå˜é‡ï¼š\n\n```python\nexport CHAT_COMPLETION_PROVIDER='openai'\n\nexport CHAT_COMPLETION_KWARGS_JSON='{\"base_url\": \"https://models.inference.ai.azure.com\", \"api_key\": \"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\", \"model\": \"gpt-4o\"}'\n```\nåœ¨è¿è¡Œä¸€äº›ç¤ºä¾‹ä»£ç ä¹‹å‰ï¼Œæˆ‘ä»¬å¿…é¡»å®‰è£…ä¸¤ä¸ªæœ€ç»ˆä¾èµ–é¡¹ã€‚\n\nMagentic-One ä½¿ç”¨ **Playwright** ä¸Žç½‘é¡µè¿›è¡Œäº¤äº’ï¼Œå› æ­¤ä½ å¿…é¡»å®‰è£… Playwright ä¾èµ–é¡¹ã€‚\n\n```python\nplaywright install --with-deps chromium\n```\nä¸ºäº†è®© Magentic-One è¿è¡Œ Python ä»£ç ï¼Œæˆ‘ä»¬éœ€è¦å®‰è£…å¹¶è¿è¡Œ Dockerã€‚è¯·æŸ¥çœ‹ [this link](https://docs.docker.com/engine/install/) äº†è§£å¦‚ä½•æ“ä½œã€‚\n\næœ€ç»ˆï¼Œæˆ‘èƒ½å¤Ÿè¯•ç”¨ Magentic-Oneã€‚\n\n**ç¤ºä¾‹ 1 â€” ç¼–å†™ä¸€äº› Python ä»£ç ã€‚**\n\n```python\n(base) tom@tpr-desktop:~/projects/autogen/python/packages$ python examples/example --logs_dir ./logs\n/home/tom/projects/autogen/python/.venv/lib/python3.11/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\nUser input ('exit' to quit):  Write a Python program to calculate and display \nthe first 5 fibonacci numbers\n```\næ˜¾ç¤ºäº†å¤§é‡è¾“å‡ºï¼Œä½†è¿‡äº†ä¸€ä¼šå„¿ï¼ŒMagentic-One é—®æˆ‘æ˜¯å¦æƒ³è¿è¡Œå®ƒåˆ›å»ºçš„ Python ä»£ç ï¼Œæˆ‘å›žç­”æ˜¯ã€‚\n\n```python\n...\n...\n\nExecutor is about to execute code (lang: python):\n## filename: fibonacci.py\ndef fibonacci_sequence(n):\n    fib_numbers = [0, 1]\n    for i in range(2, n):\n        next_value = fib_numbers[i - 1] + fib_numbers[i - 2]\n        fib_numbers.append(next_value)\n    return fib_numbers\n\nfirst_five_fib = fibonacci_sequence(5)\nprint(\"The first 5 Fibonacci numbers are:\", first_five_fib)\n\nDo you want to proceed? (yes/no): yes\n\n---------------------------------------------------------------------------\n[2024-11-10T13:25:40.508594], Executor:\n\nThe script ran, then exited with Unix exit code: 0\nIts output was:\nThe first 5 Fibonacci numbers are: [0, 1, 1, 2, 3]\n...\n...\n```\n**ç¤ºä¾‹ 2 â€” æœç´¢ç½‘ç»œ**\n\nè¦ä½¿ç”¨ Magentic æœç´¢ç½‘ç»œï¼Œä½ éœ€è¦ä¸€ä¸ª Bing API å¯†é’¥ã€‚ä½ å¯ä»¥é€šè¿‡ Microsoft Azureï¼ˆBing Search V7ï¼‰è®¾ç½®æ­¤å¯†é’¥ã€‚\n\nå¦‚æžœä½ é€‰æ‹©æœ€ä½Žå¯ç”¨çš„ **â€œFâ€** çº§åˆ«ï¼Œå¯ä»¥å°†å…¶å®‰æŽ’ä¸ºæ— æˆæœ¬é€‰é¡¹ã€‚ç„¶è€Œï¼Œè¿™é™åˆ¶äº†æ¯ç§’çš„æœç´¢æ¬¡æ•°ä¸º 3ï¼Œå¹¶ä¸”æ¯æœˆçš„æœç´¢è°ƒç”¨æ€»æ•°ä¹Ÿæœ‰é™åˆ¶ã€‚\n\nè®¾ç½®è¿™ä¸ªæœ‰ç‚¹å¤æ‚ï¼Œä½†åŸºæœ¬ä¸Šï¼Œä½ éœ€è¦éµå¾ªä»¥ä¸‹æ­¥éª¤ï¼š\n\n* å¦‚æžœä½ æ²¡æœ‰ Microsoft Azure è´¦æˆ·ï¼Œæ³¨å†Œä¸€ä¸ªå…è´¹çš„è´¦æˆ·\n* åœ¨ Azure é—¨æˆ·ä¸­åˆ›å»ºä¸€ä¸ª Bing Search èµ„æºï¼›ç¡®ä¿é€‰æ‹©æœ€ä½Žçš„ F çº§åˆ«ï¼Œè¿™æ˜¯å…è´¹çš„ï¼Œä½†å¦‚ä¸Šæ‰€è¿°é™åˆ¶è¾ƒå¤šã€‚\n* ä»Žèµ„æºæ¦‚è¿°ä¸­èŽ·å–ä½ çš„ API å¯†é’¥\n\nä¸€æ—¦ä½ æ‹¥æœ‰äº† Bing API å¯†é’¥ï¼Œå°†å…¶å€¼åˆ†é…ç»™ BING_API_KEY çŽ¯å¢ƒå˜é‡ã€‚\n\n```python\n(base) tom@tpr-desktop:~/projects/autogen/python/packages$ python examples/example --logs_dir ./logs\n/home/tom/projects/autogen/python/.venv/lib/python3.11/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\nUser input ('exit' to quit):  search the web and find the current weather \nforecast for Edinburgh UK\n```\nåŒæ ·ï¼Œæ˜¾ç¤ºäº†å¤§é‡è¾“å‡ºï¼Œä¸‹é¢æ˜¯ä¸€äº›æ›´æ˜¾è‘—çš„å†…å®¹ã€‚\n\n```python\n...\n...\nInitial plan:\n\nWe are working to address the following user request:\n\nsearch the web and find the current weather forecast for Edinburgh UK\n\n\nTo answer this request we have assembled the following team:\n\nWebSurfer: A helpful assistant with access to a web browser. Ask them to perform web searches, open pages, and interact with content (e.g., clicking links, scrolling the viewport, etc., filling in form fields, etc.) It can also summarize the entire page, or answer questions based on the content of the page. It can also be asked to sleep and wait for pages to load, in cases where the pages seem to be taking a while to load.\nCoder: A helpful and general-purpose AI assistant that has strong language skills, Python skills, and Linux command line skills.\nExecutor: A agent for executing code\nfile_surfer: An agent that can handle local files.\n\nHere is an initial fact sheet to consider:\n\n1. GIVEN OR VERIFIED FACTS\n   - The request is asking for the current weather forecast for Edinburgh, UK.\n\n2. FACTS TO LOOK UP\n   - The current weather forecast for Edinburgh, UK can be found on various weather websites such as the BBC Weather, Met Office, or Weather.com.\n\n3. FACTS TO DERIVE\n   - N/A\n\n4. EDUCATED GUESSES\n   - The current weather forecast will likely include details such as temperature, precipitation chance, wind speed, and potential weather warnings, which are typically part of a standard weather forecast.\n\n\nHere is the plan to follow as best as possible:\n\n- Request WebSurfer to search for the current weather forecast for Edinburgh, UK on a reliable weather website such as BBC Weather, Met Office, or Weather.com.\n- Instruct WebSurfer to summarize the weather forecast details including temperature, precipitation chance, wind speed, and any potential weather warnings.\n- Present the gathered weather information for Edinburgh, UK from WebSurfer.\n\n...\n...\n\nI typed 'Edinburgh UK current weather forecast' into the browser search bar.\n\nHere is a screenshot of [Edinburgh UK current weather forecast - Search](https://www.bing.com/search?q=Edinburgh+UK+current+weather+forecast&FORM=QBLH). The viewport shows 28% of the webpage, and is positioned at the top of the page.\nThe following metadata was extracted from the webpage:\n\n{\n    \"meta_tags\": {\n        \"referrer\": \"origin-when-cross-origin\",\n        \"og:description\": \"Intelligent search from Bing makes it easier to quickly find what you\\u2019re looking for and rewards you.\",\n        \"og:site_name\": \"Bing\",\n        \"og:title\": \"Edinburgh UK current weather forecast - Bing\",\n        \"og:url\": \"https://www.bing.com/search?q=Edinburgh+UK+current+weather+forecast&FORM=QBLH\",\n        \"fb:app_id\": \"3732605936979161\",\n        \"og:image\": \"http://www.bing.com/sa/simg/facebook_sharing_5.png\",\n        \"og:type\": \"website\",\n        \"og:image:width\": \"600\",\n        \"og:image:height\": \"315\"\n    }\n}\n\nAutomatic OCR of the page screenshot has detected the following text:\n\n**Page Content:**\n\nMicrosoft Bing\n\nSearch input field: Edinburgh UK current weather forecast\n\n**Menu:**\n- Search\n- Copilot\n- News\n- Images\n- Videos\n- Maps\n- Shopping\n- More\n- Tools\n\nDeep search\nSign in\nMobile\n\n**Weather Information:**\n\nAbout 3,180,000 results\n\nEdinburgh\nCapital city of Scotland, UK\n\nButtons:\n- Map\n- Things to do\n- Weather (Selected)\n- Covid-19\n- Flights\n- History\n- Travel guide\n\n**Weather Widget:**\n**Weather Details:**\n12Â°C / Â°F\n13Â°\n6Â°\nWind: 17 KMPH\nHumidity: 90%\nCloudy Â· Sun 10, 13:44\n\n**Hourly Forecast:**\n14:00  17:00  20:00  23:00  2:00  5:00  8:00  11:00\n\n**Weekly Forecast:**\n- Sun 10: 13Â°/6Â°\n- Mon 11: ðŸŒž 11Â°/2Â°\n- Tue 12: ðŸŒ§ 9Â°/5Â°\n- Wed 13: ðŸŒ¥ 12Â°/8Â°\n- Thu 14: ðŸŒ§ 10Â°/8Â°\n- Fri 15: ðŸŒ§ 11Â°/7Â°\n- Sat 16: ðŸŒ§ 10Â°/7Â°\n- Sun 17: ðŸŒ¥ 7Â°/2Â°\n\n**Sidebar Information:**\n\n- UV index: No forecast\n- Moderate breeze: 17 KMPH, WSW\n- Sunrise: 07:39 AM\n- Sunset: 04:12 PM\n...\n...\n```\næœ€ç»ˆçš„ç­”æ¡ˆæ˜¯è¿™ä¸ªï¼Œå®Œå…¨æ­£ç¡®ã€‚\n\n```python\n[2024-11-10T13:44:43.570437], Orchestrator (final answer):\n\n\nThe current weather in Edinburgh is 12Â°C with cloudy conditions. \nThere's a moderate breeze at 17 KMPH, and the humidity is at 90%. \nThe temperature is expected to range between 13Â°C and 6Â°C today.\n```\n**ç¤ºä¾‹ 3 â€” ç‚¹å‡»ç½‘ç«™é“¾æŽ¥**\n\nåœ¨æˆ‘å†™è¿™ç¯‡æ–‡ç« æ—¶ï¼Œè‹±å›½æ­£åœ¨è¿›è¡Œä¸€åœºå¨å°”å£«å’Œæ–æµŽä¹‹é—´çš„å¤§åž‹æ©„æ¦„çƒæ¯”èµ›ã€‚æˆ‘æƒ³çŸ¥é“å¨å°”å£«å¯¹æ–æµŽæ¯”èµ›çš„æœ€æ–°æƒ…å†µã€‚\n\n```python\n(base) tom@tpr-desktop:~/projects/autogen/python/packages$ python examples/example --logs_dir ./logs\n/home/tom/projects/autogen/python/.venv/lib/python3.11/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\nUser input ('exit' to quit):  Click on the bbc.co.uk website, click on the \nSport link near the top of the page. Look for a link in the page that \ndisplays about the Wales v Fiji rugby match. Click on that link and tell me \nwhat the latest score is\n```\nåŒæ ·ï¼Œæˆ‘çœç•¥äº†è®¸å¤šè¾“å‡ºä»¥èŠ‚çœç©ºé—´ã€‚\n\n```python\n...\n...\n...\né¡µé¢æˆªå›¾çš„è‡ªåŠ¨OCRæ£€æµ‹åˆ°ä»¥ä¸‹æ–‡æœ¬ï¼š\n\nå½“ç„¶ï¼Œè¿™é‡Œæ˜¯è½¬å½•çš„æ–‡æœ¬ï¼š\n\n---\n**BBC**\nç™»å½•\né¦–é¡µ\næ–°é—»\nä½“è‚²\nå¤©æ°”\niPlayer\nå£°éŸ³\nå°çŸ¥è¯†\nä½“è‚²\n\né¦–é¡µ | è¶³çƒ | æ¿çƒ | ä¸€çº§æ–¹ç¨‹å¼ | æ©„æ¦„çƒU | æ©„æ¦„çƒL | ç½‘çƒ | é«˜å°”å¤« | æ‹³å‡» | ç”°å¾„\n\nå‘çŽ°ä½ çš„BBC\nç™»å½•æˆ–åˆ›å»ºè´¦æˆ·ä»¥è§‚çœ‹ã€æ”¶å¬å’Œå‚ä¸Ž\n\nç™»å½•æˆ–æ³¨å†Œ\n\nè¯·æ±‚å·²æ»¡è¶³ã€‚\n...\n...\n...\n - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n[2024â€“11â€“10T13:55:10.606578], Orchestrator (æœ€ç»ˆç­”æ¡ˆ):\næ ¹æ®BBCä½“è‚²ç½‘ç«™ï¼Œå¨å°”å£«ä¸Žæ–æµŽæ¯”èµ›çš„æœ€æ–°æ¯”åˆ†æ˜¯å¨å°”å£« 7â€“0 æ–æµŽï¼Œç©†é›·å¾—åˆ†ã€‚\n - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n[2024â€“11â€“10T13:55:10.617212], Orchestrator (ç»ˆæ­¢æ¡ä»¶):\n```\nè¿™æ˜¯æˆ‘åœ¨æ¨¡åž‹å›žç­”åŽä¸ä¹…æ‹çš„æˆªå›¾ï¼ˆæ–æµŽåœ¨å¨å°”å£«çš„åˆå§‹å¾—åˆ†åŽå¾ˆå¿«å°±å¾—åˆ†äº†ï¼‰\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*xN8qBJLdHqx0lrh_4Y4DCQ.png)\n\n**ç¤ºä¾‹ 4 â€” è¯»å–æœ¬åœ° XL æ–‡ä»¶ã€‚**\n\næˆ‘åœ¨æœ¬åœ°ç³»ç»Ÿä¸Šæœ‰ä¸€ä¸ª XL æ–‡ä»¶ã€‚è®©æˆ‘ä»¬çœ‹çœ‹ Magentic\\-One æ˜¯å¦èƒ½æ‰¾åˆ°å®ƒã€æ‰“å¼€å®ƒï¼Œå¹¶å›žç­”å…³äºŽå®ƒçš„é—®é¢˜ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*UIuLEkEr-w6ZckRjiUuqjw.png)\n\n\n```python\n(base) tom@tpr-desktop:~/projects/autogen/python/packages$ python examples/example --logs_dir ./logs\n/home/tom/projects/autogen/python/.venv/lib/python3.11/site-packages/pydub/utils.py:170: RuntimeWarning: æ‰¾ä¸åˆ° ffmpeg æˆ– avconv - é»˜è®¤ä½¿ç”¨ ffmpegï¼Œä½†å¯èƒ½æ— æ³•å·¥ä½œ\n  warn(\"æ‰¾ä¸åˆ° ffmpeg æˆ– avconv - é»˜è®¤ä½¿ç”¨ ffmpegï¼Œä½†å¯èƒ½æ— æ³•å·¥ä½œ\", RuntimeWarning)\nç”¨æˆ·è¾“å…¥ ('exit' é€€å‡º):  æˆ‘åœ¨æˆ‘çš„ /mnt/d/data ç›®å½•ä¸­æœ‰ä¸€ä¸ªæ–‡ä»¶å«\nfake_data.xlsxã€‚ä½ èƒ½å‘Šè¯‰æˆ‘æ–‡ä»¶çš„ç¬¬ä¸‰æ¡è®°å½•æ˜¯ä»€ä¹ˆå—\n```\n\n```python\n...\n...\n\nä¸‹ä¸€ä¸ªå‘è¨€è€… file_surfer\n\n---------------------------------------------------------------------------\n[2024-11-10T14:16:57.676137], file_surfer:\n\nåœ°å€: file:///mnt/d/data/fake_data.xlsx\nè§†å£ä½ç½®: æ˜¾ç¤ºç¬¬ 1 é¡µï¼Œå…± 1 é¡µã€‚\n=======================\n### Sheet1\n| æ—¥æœŸ | é”€å”® | æ”¯å‡º |\n| --- | --- | --- |\n| 2024\\-01\\-31 | 302 | 187 |\n| 2024\\-02\\-29 | 635 | 472 |\n| 2024\\-03\\-31 | 470 | 199 |\n| 2024\\-04\\-30 | 306 | 459 |\n| 2024\\-05\\-31 | 271 | 251 |\n| 2024\\-06\\-30 | 900 | 230 |\n| 2024\\-07\\-31 | 220 | 249 |\n| 2024\\-08\\-31 | 814 | 408 |\n| 2024\\-09\\-30 | 321 | 357 |\n| 2024\\-10\\-31 | 666 | 443 |\n| 2024\\-11\\-30 | 414 | 393 |\n| 2024\\-12\\-31 | 530 | 485 |\n\n---------------------------------------------------------------------------\n[2024-11-10T14:17:00.613740], Orchestrator (æ€è€ƒ):\n\næ›´æ–°çš„è´¦æœ¬:\n{\n  \"is_request_satisfied\": {\n    \"reason\": \"æˆåŠŸæ£€ç´¢å¹¶æ˜¾ç¤ºäº†æ–‡ä»¶ 'fake_data.xlsx' ä¸­ç¬¬ä¸‰æ¡è®°å½•çš„å†…å®¹ã€‚\",\n    \"answer\": true\n  },\n  \"is_in_loop\": {\n    \"reason\": \"ä»»åŠ¡ä»¥ç®€å•çš„æ–¹å¼å®Œæˆï¼Œæ²¡æœ‰é‡å¤çš„æ“ä½œã€‚\",\n    \"answer\": false\n  },\n  \"is_progress_being_made\": {\n    \"reason\": \"ç¬¬ä¸‰æ¡è®°å½•çš„å†…å®¹æˆåŠŸæ£€ç´¢å¹¶æ˜¾ç¤ºï¼Œè¡¨æ˜Žè¿›å±•é¡ºåˆ©ã€‚\",\n    \"answer\": true\n  },\n  \"next_speaker\": {\n    \"reason\": \"ä»»åŠ¡å·²å®Œæˆï¼Œå› æ­¤ä¸éœ€è¦è¿›ä¸€æ­¥çš„æ“ä½œã€‚\",\n    \"answer\": \"file_surfer\"\n  },\n  \"instruction_or_question\": {\n    \"reason\": \"é€šè¿‡æä¾›ç¬¬ä¸‰æ¡è®°å½•çš„å†…å®¹ï¼Œæ»¡è¶³äº†è¯·æ±‚ã€‚\",\n    \"answer\": \"æ–‡ä»¶ä¸­çš„ç¬¬ä¸‰æ¡è®°å½•æ˜¯ï¼šæ—¥æœŸï¼š2024-03-31ï¼Œé”€å”®é¢ï¼š470ï¼Œæ”¯å‡ºï¼š199ã€‚\"\n  }\n}\n\n---------------------------------------------------------------------------\n[2024-11-10T14:17:00.613806], Orchestrator (æ€è€ƒ):\n\nè¯·æ±‚å·²æ»¡è¶³ã€‚\n\n---------------------------------------------------------------------------\n[2024-11-10T14:17:01.465848], Orchestrator (æœ€ç»ˆç­”æ¡ˆ):\n\n\næ–‡ä»¶ \"fake_data.xlsx\" ä¸­çš„ç¬¬ä¸‰æ¡è®°å½•åŒ…å«ä»¥ä¸‹ä¿¡æ¯ï¼š\n\n- æ—¥æœŸï¼š2024-03-31\n- é”€å”®ï¼š470\n- æ”¯å‡ºï¼š199\n\n---------------------------------------------------------------------------\n[2024-11-10T14:17:01.465908], Orchestrator (ç»ˆæ­¢æ¡ä»¶):\n\næ²¡æœ‰é€‰æ‹©ä»£ç†ã€‚\n(base) tom@tpr-desktop:~/projects/autogen/python/packages$\n```\næˆ‘å–œæ¬¢ä»£ç†ç¡®å®šç¬¬ä¸€æ¡è®°å½•æ˜¯æ ‡é¢˜ï¼Œå› æ­¤è¿”å›žäº†å®žé™…çš„ç¬¬ä¸‰æ¡æ•°æ®è®°å½•ã€‚è¿™çœŸæ˜¯äº†ä¸èµ·ã€‚\n\n### æ‘˜è¦\n\nå—¯ï¼Œæˆ‘ä¸çŸ¥é“ä½ æ€Žä¹ˆæƒ³ï¼Œä½†æˆ‘è®¤ä¸ºè¿™æ˜¯ä¸€ç³»åˆ—ç›¸å½“ä»¤äººå°è±¡æ·±åˆ»çš„æ¼”ç¤ºã€‚å¾®è½¯å¼€å‘äº†ä¸€ä¸ªéžå¸¸å‡ºè‰²çš„ä»£ç†ç³»ç»Ÿï¼Œå¹¶ä¼¼ä¹Žæ‰“ç®—åœ¨ä¸ä¹…çš„å°†æ¥å°†å…¶å®Œå…¨çº³å…¥ä»–ä»¬çš„ Autogen æ¡†æž¶ä¸­ã€‚\n\nåœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘è§£é‡Šäº†ä»€ä¹ˆæ˜¯ Magentic-Oneï¼Œä»¥åŠå¦‚ä½•ä¸‹è½½å’Œè¿è¡Œå®ƒä»¥å®Œæˆä¸€äº›æœ‰ç”¨çš„ä»»åŠ¡ã€‚æˆ‘è§£é‡Šäº†å®ƒçš„å…³é”®ç»„ä»¶æ˜¯\n\n* åè°ƒ\n* ç½‘ç»œå’Œæ–‡ä»¶æµè§ˆ\n* ç¼–ç å’Œç»ˆç«¯æ“ä½œ\n\næˆ‘é€šè¿‡ä¸€ç³»åˆ—ç¤ºä¾‹å±•ç¤ºäº†è¿™äº›ç»„ä»¶çš„å·¥ä½œï¼ŒåŒ…æ‹¬\n\n* åˆ›å»ºå’Œè¿è¡Œ Python ä»£ç \n* æ£€æŸ¥æœ¬åœ°æ–‡ä»¶å¹¶å›žç­”æœ‰å…³å…¶å†…å®¹çš„é—®é¢˜\n* åœ¨ç½‘ä¸Šæœç´¢ä¿¡æ¯\n* ç‚¹å‡»ç½‘é¡µé“¾æŽ¥\n\n\n> *å¥½çš„ï¼Œæˆ‘çŽ°åœ¨å°±åˆ°æ­¤ä¸ºæ­¢ã€‚å¸Œæœ›ä½ è§‰å¾—è¿™ç¯‡æ–‡ç« æœ‰ç”¨ã€‚å¦‚æžœæœ‰ï¼Œè¯·é€šè¿‡ [è¿™ä¸ªé“¾æŽ¥](https://medium.com/@thomas_reid) æŸ¥çœ‹æˆ‘çš„ä¸ªäººèµ„æ–™é¡µé¢ã€‚åœ¨é‚£é‡Œï¼Œä½ å¯ä»¥çœ‹åˆ°æˆ‘å…¶ä»–å·²å‘å¸ƒçš„æ•…äº‹ï¼Œå¹¶è®¢é˜…ä»¥èŽ·å–æˆ‘å‘å¸ƒæ–°å†…å®¹æ—¶çš„é€šçŸ¥ã€‚*\n\n\n> *æ—¶å±€è‰°éš¾ï¼Œé’±åŒ…ç´§ç¼©ï¼Œä½†å¦‚æžœä½ ä»Žè¿™ç¯‡æ–‡ç« ä¸­èŽ·å¾—äº†çœŸæ­£çš„ä»·å€¼ï¼Œè¯·è€ƒè™‘ [è¯·æˆ‘å–ä¸€æ¯](https://ko-fi.com/taupirho)ã€‚*\n\nå¦‚æžœä½ å–œæ¬¢è¿™ç¯‡å†…å®¹ï¼Œæˆ‘æƒ³ä½ ä¹Ÿä¼šå¯¹è¿™äº›ç›¸å…³çš„æ–‡ç« æ„Ÿå…´è¶£ã€‚\n\nåœ¨æ­¤å¤„é˜…è¯»å¾®è½¯çš„å®Œæ•´ Magentic-One å…¬å‘Š [è¿™é‡Œ](https://www.microsoft.com/en-us/research/articles/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/)ã€‚\n\n"},{"lang":"zh","group":"blog","slug":"blog/introduction-to-llava-a-multimodal-ai-model-2a2fa530ace4","frontmatter":{"title":"LLaVA ç®€ä»‹ï¼šä¸€ç§å¤šæ¨¡å¼ AI æ¨¡åž‹","meta_title":"LLaVA ç®€ä»‹ï¼šä¸€ç§å¤šæ¨¡å¼ AI æ¨¡åž‹","description":"LLaVA æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯è®­ç»ƒçš„å¤§åž‹å¤šæ¨¡å¼æ¨¡åž‹ï¼Œæ—¨åœ¨æ ¹æ®è§†è§‰è¾“å…¥ç†è§£å’Œç”Ÿæˆå†…å®¹â€¦â€¦","date":"2024-10-29T12:48:10.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*0At7tXF5ejho9Y46E3uGtg.png","categories":["Natural Language Processing","Computer Vision","Generative AI"],"author":"Rifx.Online","tags":["LLaVA","GPT-4","multimodal","visual","encoder"],"draft":false,"slug":"blog/introduction-to-llava-a-multimodal-ai-model-2a2fa530ace4"},"content":"\n\n\n\n\nLLaVAæ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯è®­ç»ƒçš„å¤§åž‹å¤šæ¨¡æ€æ¨¡åž‹ï¼Œæ—¨åœ¨ç†è§£å’Œç”ŸæˆåŸºäºŽè§†è§‰è¾“å…¥ï¼ˆå›¾åƒï¼‰å’Œæ–‡æœ¬æŒ‡ä»¤çš„å†…å®¹ã€‚å®ƒç»“åˆäº†è§†è§‰ç¼–ç å™¨å’Œè¯­è¨€æ¨¡åž‹çš„èƒ½åŠ›ï¼Œä»¥å¤„ç†å’Œå“åº”å¤šæ¨¡æ€è¾“å…¥ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*mjzqL0BHzdPoN-Jjruh52A.png)\n\n## LLaVA çš„è¾“å…¥å’Œè¾“å‡ºï¼šè¿žæŽ¥è§†è§‰ä¸Žæ–‡æœ¬é¢†åŸŸï¼š\n\nLLaVA çš„è¾“å…¥æœ‰ä¸¤ä¸ªæ–¹é¢ï¼š\n\n1. è§†è§‰è¾“å…¥ï¼šæ¨¡åž‹å¯ä»¥æŸ¥çœ‹å’Œåˆ†æžçš„å›¾åƒï¼Œä»¥æå–è§†è§‰ç‰¹å¾å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚\n2. æ–‡æœ¬æŒ‡ä»¤ï¼šæ–‡æœ¬è¾“å…¥ï¼Œå¯ä»¥æ˜¯é—®é¢˜æˆ–å‘½ä»¤ï¼ŒæŒ‡å¯¼æ¨¡åž‹å…³æ³¨ä»€ä¹ˆæˆ–æ‰§è¡Œä¸Žè§†è§‰è¾“å…¥ç›¸å…³çš„ä»€ä¹ˆä»»åŠ¡ã€‚\n\nLLaVA çš„è¾“å‡ºæ˜¯åŸºäºŽæ–‡æœ¬çš„ï¼Œå¯èƒ½ä¼šæ ¹æ®ä»»åŠ¡è€Œæœ‰æ‰€ä¸åŒï¼š\n\n1. æè¿°æ€§æ–‡æœ¬ï¼šå¦‚æžœä»»åŠ¡æ˜¯æè¿°è§†è§‰å†…å®¹ï¼ŒLLaVA å¯ä»¥è¾“å‡ºå›¾åƒçš„è¯¦ç»†æè¿°ï¼Œè¯†åˆ«å¯¹è±¡ã€åŠ¨ä½œå’Œåœºæ™¯ã€‚\n2. é—®é¢˜å›žç­”ï¼šå¯¹äºŽé—®ç­”ä»»åŠ¡ï¼ŒLLaVA ç”Ÿæˆçš„å›žç­”å¯ä»¥è§£ç­”å…³äºŽè§†è§‰è¾“å…¥çš„é—®é¢˜ï¼Œå¯èƒ½æ¶‰åŠåŸºäºŽå›¾åƒå†…å®¹çš„æŽ¨ç†å’ŒæŽ¨æ–­ã€‚\n3. åŽç»­è¡ŒåŠ¨ï¼šå¯¹äºŽéœ€è¦è¡ŒåŠ¨çš„æŒ‡ä»¤ï¼Œä¾‹å¦‚ç¼–è¾‘å›¾åƒæˆ–æ£€ç´¢æ›´å¤šä¿¡æ¯ï¼ŒLLaVA å¯ä»¥æä¾›é€‚å½“çš„æ–‡æœ¬å“åº”ï¼ŒæŒ‡ç¤ºæ‰€é‡‡å–çš„è¡ŒåŠ¨æˆ–å»ºè®®åº”è¯¥åšä»€ä¹ˆã€‚\n\n## æ¯”è¾ƒåˆ†æžï¼šLLaVaä¸Žå½“ä»£å¤šæ¨¡æ€æ¨¡åž‹\n\nå¤šæ¨¡æ€äººå·¥æ™ºèƒ½çš„é¢†åŸŸæ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œå‡ºçŽ°äº†CLIPã€BLIPç­‰åˆ›æ–°ï¼Œä»¥åŠæœ€è¿‘æŽ¨å‡ºçš„LLaVaã€‚æœ¬å°èŠ‚å°†LLaVaçš„ç‹¬ç‰¹æž¶æž„å’Œæ–¹æ³•ä¸Žè¿™äº›å½“ä»£æ¨¡åž‹è¿›è¡Œæ¯”è¾ƒï¼Œçªå‡ºå…¶è¿›æ­¥å’ŒåŒºåˆ«ï¼Œä½¿å…¶ä¸Žä¼—ä¸åŒã€‚\n\n### CLIP: å¼€åˆ›å¤šæ¨¡æ€ç†è§£çš„å…ˆæ²³\n\nCLIP (Contrastive Languageâ€“Image Pre\\-training) åœ¨å¤šæ¨¡æ€äººå·¥æ™ºèƒ½é¢†åŸŸä¸­è¿ˆå‡ºäº†é©å‘½æ€§çš„ä¸€æ­¥ï¼Œåœ¨å„ç§è§†è§‰ä»»åŠ¡ä¸­æä¾›äº†å¼ºå¤§çš„æ€§èƒ½ã€‚å®ƒåœ¨è‡ªç„¶è¯­è¨€æè¿°çš„èƒŒæ™¯ä¸‹ç†è§£å›¾åƒçš„èƒ½åŠ›ä¸ºè¯¥é¢†åŸŸè®¾å®šäº†æ–°çš„åŸºå‡†ã€‚CLIP é€šè¿‡å¤§è§„æ¨¡çš„é¢„è®­ç»ƒæ–¹æ³•å°†å›¾åƒä¸Žæ–‡æœ¬æè¿°å¯¹é½ï¼Œä½¿æ¨¡åž‹èƒ½å¤Ÿåœ¨ä¸€ç³»åˆ—è§†è§‰ä»»åŠ¡ä¸Šè¿›è¡Œé›¶æ ·æœ¬å­¦ä¹ ã€‚ç„¶è€Œï¼ŒCLIP ä¸»è¦å…³æ³¨å›¾åƒä¸Žæ–‡æœ¬ä¹‹é—´çš„é«˜å±‚æ¬¡å…³è”ï¼Œå¹¶ä¸å…·å¤‡æ·±å…¥æŽ¨ç†æˆ–å¯¹è¯å‚ä¸Žçš„èƒ½åŠ›ã€‚\n\n### BLIP: è¿žæŽ¥è¯­è¨€ä¸Žå›¾åƒæ„ŸçŸ¥\n\nåœ¨CLIPå¥ å®šçš„åŸºç¡€ä¸Šï¼ŒBLIPï¼ˆBootstrapped Language Image Pre-trainingï¼‰é€šè¿‡å¼•å…¥è‡ªå¼•å¯¼é¢„è®­ç»ƒç­–ç•¥ï¼Œæ‰©å±•äº†å¤šæ¨¡æ€æ¨¡åž‹çš„èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•é€šè¿‡ä¸æ–­ä»Žè‡ªèº«çš„é¢„æµ‹ä¸­å­¦ä¹ ï¼Œå®Œå–„æ¨¡åž‹çš„è§†è§‰ç†è§£ï¼Œä»Žè€Œå¸®åŠ©æ”¹å–„è¯­è¨€ä¸Žè§†è§‰å†…å®¹ä¹‹é—´çš„å¯¹é½ã€‚BLIPåœ¨éœ€è¦æ›´ç²¾ç¡®è§†è§‰è¯†åˆ«å’Œè¯­è¨€ç†è§£çš„ä»»åŠ¡ä¸Šè¡¨çŽ°å‡ºå¢žå¼ºçš„æ€§èƒ½ã€‚\n\nç›¸æ¯”ä¹‹ä¸‹ï¼ŒLLaVaé‡‡å–äº†ä¸åŒçš„è·¯å¾„ï¼Œé€šè¿‡åˆ©ç”¨GPT-4çš„è¯­è¨€ç”Ÿæˆèƒ½åŠ›æ¥ç­–åˆ’å…¶éµå¾ªæŒ‡ä»¤çš„æ•°æ®ã€‚è¿™ä¸ä»…å¯¼è‡´äº†ä¸€ä¸ªæ•æ‰æ›´å¹¿æ³›äººç±»äº’åŠ¨èŒƒå›´çš„æ•°æ®é›†ï¼Œè¿˜ä½¿LLaVaèƒ½å¤Ÿè¿›è¡Œæ›´å¤æ‚çš„æŽ¨ç†å’Œæ·±å…¥çš„å¯¹è¯èƒ½åŠ›ã€‚\n\n## LLaVaçš„ç‹¬ç‰¹ä¹‹å¤„ï¼šæ˜¯æ¨¡åž‹æž¶æž„è¿˜æ˜¯å…¶ä»–å› ç´ ï¼Ÿ\n\næ ¹æ®æˆ‘ä»¬çš„è§‚ç‚¹ï¼ŒLLaVAçš„ä¼˜åŠ¿ä¸»è¦åœ¨äºŽå…¶æ•°æ®ç­–åˆ’èƒ½åŠ›ï¼Œè€Œéžæž¶æž„é€‰æ‹©ã€‚LLaVAçš„é‡å¤§è¿›å±•ä¸»è¦å¾—ç›ŠäºŽå…¶åˆ©ç”¨GPT-4è¿›è¡Œæ•°æ®ç­–åˆ’ã€‚ä¸Žä¼ ç»Ÿçš„é™æ€æ•°æ®é›†ä¸åŒï¼ŒLLaVAä½¿ç”¨ChatGPT-4ç”ŸæˆåŠ¨æ€ã€æŒ‡å¯¼æ€§çš„æ•°æ®ï¼Œç§¯æžå‚ä¸Žå„ç§è§†è§‰å’Œæ–‡æœ¬åœºæ™¯ä¸­çš„è®­ç»ƒè¿‡ç¨‹ã€‚\n\né€šè¿‡ä½¿ç”¨GPT-4ï¼ŒLLaVAç”Ÿæˆçš„ æ•°æ®é›†ç´§å¯†æ¨¡æ‹Ÿè‡ªç„¶è¯­è¨€å’Œè§†è§‰æ„ŸçŸ¥ï¼Œè„±ç¦»äº†ä¼ ç»Ÿçš„æ‰‹åŠ¨æ•°æ®é›†ç”Ÿæˆæ–¹æ³•ã€‚è¿™ç§åˆ›æ–°çš„æ–¹æ³•ä¸ä»…ä½¿AIèƒ½å¤Ÿç†è§£å’ŒæŽ¨ç†ï¼Œè¿˜ä½¿å…¶æ›´æŽ¥è¿‘äºŽå‡†ç¡®åæ˜ äººç±»æ™ºèƒ½ã€‚\n\n### LLaVaä¸­çš„æ•°æ®æ•´ç†ç­–ç•¥\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*LzastWLkzPeMB_28Nr7Y9A.png)\n\nLLaVaï¼Œå³å¤§åž‹è¯­è¨€ä¸Žè§†è§‰åŠ©æ‰‹ï¼Œä¸ä»…ä»¥å…¶å…ˆè¿›çš„ç¥žç»æž¶æž„è€Œé—»åï¼Œè¿˜ä»¥å…¶å¼€åˆ›æ€§çš„æ•°æ®æ•´ç†æ–¹æ³•è€Œè„±é¢–è€Œå‡ºã€‚é€šè¿‡åˆ©ç”¨GPT-4ï¼Œå®ƒå½»åº•æ”¹å˜äº†ä¼ ç»Ÿçš„æ•°æ®å‡†å¤‡æ–¹æ³•ï¼Œæž„å»ºå‡ºä¸€ä¸ªåæ˜ çŽ°å®žä¸–ç•Œå¤æ‚æ€§çš„æ•°æ®åº“ã€‚\n\nLLaVaä¸­çš„æ•°æ®æ•´ç†å§‹äºŽä¸€å¼ å›¾ç‰‡åŠå…¶ç›¸åº”çš„æ ‡é¢˜ï¼Œåˆ©ç”¨GPT-4ç”Ÿæˆä¸€ç»„æŸ¥è¯¢ã€‚è¿™äº›æŸ¥è¯¢å¼•å¯¼AIç²¾ç¡®è€Œç›¸å…³åœ°æŽ¢ç´¢å’Œæè¿°å›¾åƒå†…å®¹ã€‚\n\nä¸ºäº†æœ‰æ•ˆåœ°å°†è§†è§‰æ•°æ®è½¬åŒ–ä¸ºæ–‡æœ¬åŸºç¡€çš„AIï¼ˆå¦‚GPT-4ï¼‰ï¼ŒLLaVaä½¿ç”¨æ ‡é¢˜æä¾›è§†è§‰åœºæ™¯çš„å¤šæ ·è§†è§’ï¼Œå¹¶ä½¿ç”¨è¾¹ç•Œæ¡†æä¾›ç©ºé—´ä¸Šä¸‹æ–‡å’Œç„¦ç‚¹ã€‚\n\n1. å¯¹è¯æ•°æ®ï¼šæ¨¡ä»¿äººç±»äº’åŠ¨ï¼ŒLLaVaæ•´ç†å¯¹è¯ï¼Œå…¶ä¸­æ¨¡åž‹ä½œä¸ºåŠ©æ‰‹ï¼Œå›žç­”æœ‰å…³å›¾åƒå„ä¸ªæ–¹é¢çš„é—®é¢˜ã€‚è¿™äº›é—®é¢˜çš„èŒƒå›´åŒ…æ‹¬è¯†åˆ«ç‰©ä½“å’ŒåŠ¨ä½œï¼Œè¾¨åˆ«å®ƒä»¬çš„æ•°é‡ã€ä½ç½®å’Œç›¸å¯¹ä½ç½®ï¼Œç¡®ä¿æ¨¡åž‹èƒ½å¤Ÿå¤„ç†å…·æœ‰æ˜Žç¡®ç­”æ¡ˆçš„æŸ¥è¯¢ã€‚\n2. è¯¦ç»†æè¿°æ•°æ®ï¼šLLaVaæ—¨åœ¨å…¨é¢ç†è§£å›¾åƒã€‚ä¸ºæ­¤ï¼Œå®ƒä¿ƒä½¿GPT-4æå‡ºæ—¨åœ¨ç†è§£å›¾åƒä¸°å¯Œè¯¦ç»†æè¿°çš„é—®é¢˜ã€‚è¿™äº›æç¤ºé¼“åŠ±æ¨¡åž‹æ·±å…¥æŒ–æŽ˜ï¼Œæä¾›ä¸€ä¸ªæ•æ‰è§†è§‰å†…å®¹æ•´ä½“æœ¬è´¨çš„å™è¿°ã€‚\n3. å¤æ‚æŽ¨ç†æ•°æ®ï¼šè¶…è¶Šå•çº¯æè¿°ï¼ŒLLaVaé€šè¿‡éœ€è¦åˆ†å±‚æŽ¨ç†è¿‡ç¨‹çš„é—®é¢˜æŒ‘æˆ˜æ¨¡åž‹ï¼Œè¦æ±‚é€»è¾‘å’Œå› æžœå…³ç³»çš„ç†è§£ã€‚è¿™ç§ç±»åž‹çš„æ•°æ®è®­ç»ƒæ¨¡åž‹æž„å»ºæœ‰ç†æœ‰æ®çš„å“åº”ï¼Œæ”¯æŒé€»è¾‘æ€ç»´çš„é¡ºåºã€‚\n\n## LLaVaçš„æž¶æž„ï¼šè§†è§‰ä¸Žè¯­è¨€çš„æ•´åˆ\n\nLLaVaæ¨¡åž‹æ•´åˆäº†è§†è§‰ä¸Žè¯­è¨€ï¼Œåˆ©ç”¨ä»¥ä¸‹æ ¸å¿ƒç»„ä»¶ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8q_Iay_LHCzPqtrQby_H8w.png)\n\n1. è§†è§‰ç¼–ç å™¨ï¼šLLaVaæž¶æž„çš„åŸºç¡€æ˜¯é¢„è®­ç»ƒçš„CLIPè§†è§‰ç¼–ç å™¨ï¼Œç‰¹åˆ«æ˜¯ViT-L/14å˜ä½“ã€‚è¯¥ç»„ä»¶é€šè¿‡Transformerå±‚å¤„ç†è¾“å…¥å›¾åƒï¼ˆXvï¼‰ï¼Œæå–ç‰¹å¾ï¼ˆZvï¼‰ï¼Œä½¿æ¨¡åž‹èƒ½å¤Ÿæœ‰æ•ˆç†è§£è§†è§‰ä¿¡æ¯ã€‚\n2. è¯­è¨€æ¨¡åž‹ï¼ˆVicunaï¼‰ï¼šLLaVaçš„è¯­è¨€èƒ½åŠ›ä¾èµ–äºŽVicunaï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMï¼‰çš„å˜ä½“ï¼Œè®°ä½œfÏ•ã€‚Vicunaæ ¹æ®è¾“å…¥è¯­è¨€æŒ‡ä»¤ï¼ˆXqï¼‰ç†è§£å¹¶ç”Ÿæˆè¯­è¨€å“åº”ï¼ˆXaï¼‰ï¼Œè¡¥å……äº†è§†è§‰ç¼–ç å™¨çš„åŠŸèƒ½ã€‚\n3. çº¿æ€§æŠ•å½±ï¼šè¯¥ç»„ä»¶ç”±ä¸€ä¸ªå¯è®­ç»ƒçŸ©é˜µï¼ˆWï¼‰è¡¨ç¤ºï¼Œä½œä¸ºè§†è§‰ç‰¹å¾ï¼ˆZvï¼‰ä¸Žè¯­è¨€æ¨¡åž‹çš„åµŒå…¥ç©ºé—´ä¹‹é—´çš„æ¡¥æ¢ã€‚å®ƒå°†è§†è§‰ç‰¹å¾è½¬æ¢ä¸ºè§†è§‰æ ‡è®°ï¼ˆHvï¼‰ï¼Œä½¿å…¶ä¸Žè¯­è¨€æ¨¡åž‹çš„è¯åµŒå…¥ç©ºé—´å¯¹é½ï¼Œä»¥ä¿ƒè¿›å¤šæ¨¡æ€å¯¹è¯ã€‚\n\n## è®­ç»ƒå’Œå¾®è°ƒ LLaVAï¼š\n\nLLaVA é‡‡ç”¨ä¸¤é˜¶æ®µçš„è®­ç»ƒè¿‡ç¨‹ï¼Œæ¯ä¸ªé˜¶æ®µéƒ½ä¸“æ³¨äºŽæå‡æ¨¡åž‹è§£è¯»å’Œå“åº”è§†è§‰ä¸Žæ–‡æœ¬æ•°æ®èžåˆçš„èƒ½åŠ›ã€‚\n\n### Stage 1: é¢„è®­ç»ƒä»¥è¿›è¡Œç‰¹å¾å¯¹é½\n\nLLaVAè®­ç»ƒçš„åˆå§‹é˜¶æ®µæ˜¯é¢„è®­ç»ƒä»¥è¿›è¡Œç‰¹å¾å¯¹é½ã€‚åœ¨è¿™ä¸ªé˜¶æ®µï¼Œæ¨¡åž‹ä¸“æ³¨äºŽå°†å›¾åƒä¸­çš„è§†è§‰ç‰¹å¾ä¸Žè¯­è¨€æ¨¡åž‹ä¸­çš„ç›¸åº”æ–‡æœ¬ç‰¹å¾å¯¹é½ã€‚è¿™æ˜¯é€šè¿‡å°†ä¸€ä¸ªå¤§åž‹æ•°æ®é›†è¿‡æ»¤ä¸ºä¸€ç»„ç²¾ç‚¼çš„å›¾åƒ-æ–‡æœ¬å¯¹æ¥å®žçŽ°çš„ï¼ŒLLaVAåˆ©ç”¨è¿™äº›å¯¹æ¥å­¦ä¹ ä¸¤ç§æ¨¡æ€ä¹‹é—´çš„å…³è”ã€‚\n\nåœ¨è¿™ä¸ªé˜¶æ®µï¼Œè§†è§‰ç¼–ç å™¨ï¼ˆä¾‹å¦‚CLIPè§†è§‰ç¼–ç å™¨ViT-L/14ï¼‰å¤„ç†å›¾åƒä»¥æå–è§†è§‰ç‰¹å¾ï¼Œç„¶åŽä½¿ç”¨æŠ•å½±çŸ©é˜µï¼ˆWï¼‰å°†è¿™äº›ç‰¹å¾æ˜ å°„åˆ°è¯­è¨€æ¨¡åž‹çš„è¯åµŒå…¥ç©ºé—´ã€‚LLaVAä¸­ä½¿ç”¨çš„è¯­è¨€æ¨¡åž‹æ˜¯Vicunaï¼Œä»¥å…¶å¼ºå¤§çš„è¯­è¨€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›è€Œé—»åã€‚\n\n### Stage 2: å¾®è°ƒç«¯åˆ°ç«¯\n\nåœ¨å¯¹é½è§†è§‰å’Œè¯­è¨€ç‰¹å¾åŽï¼ŒLLaVA è¿›è¡Œç«¯åˆ°ç«¯çš„å¾®è°ƒè¿‡ç¨‹ã€‚å°½ç®¡ä¿æŒè§†è§‰ç¼–ç å™¨çš„æƒé‡ä¸å˜ï¼Œä½†è¿™ä¸€é˜¶æ®µå…è®¸æ¨¡åž‹è”åˆå¾®è°ƒæŠ•å½±çŸ©é˜µå’Œè¯­è¨€æ¨¡åž‹çš„æƒé‡ã€‚å…¶ç›®æ ‡æ˜¯æœ€å¤§åŒ–åŸºäºŽæä¾›çš„å¤šæ¨¡æ€æ•°æ®çš„ç›®æ ‡ç­”æ¡ˆçš„å¯èƒ½æ€§ã€‚\n\nè¿™ä¸€é˜¶æ®µå¯¹äºŽå°† LLaVA é€‚åº”ç‰¹å®šç”¨ä¾‹åœºæ™¯è‡³å…³é‡è¦ï¼Œä¾‹å¦‚å¤šæ¨¡æ€èŠå¤©ã€ç§‘å­¦é—®ç­”ç­‰ã€‚å®ƒç¡®ä¿æ¨¡åž‹ä¸ä»…èƒ½å¤Ÿç†è§£å›¾åƒåœ¨é€šç”¨æè¿°ä¸­çš„ä¸Šä¸‹æ–‡ï¼Œè¿˜èƒ½åœ¨æ”¶åˆ°ä¸Žå›¾åƒç›¸å…³çš„ç‰¹å®šé—®é¢˜æ—¶å‚ä¸Žå¤æ‚å¯¹è¯ã€æä¾›è¯¦ç»†è§£é‡Šå¹¶è¿›è¡ŒæŽ¨ç†ã€‚\n\n## æ€§èƒ½ä¸ŽåŸºå‡†æµ‹è¯•ï¼šLLaVa åœ¨ VQA æ¨¡åž‹ä¸­çš„åº”ç”¨\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*I_5fTa_2rtNHEDUaDNMXbQ.png)\n\n## LLaVA\\-Bench (COCO) æ€§èƒ½æ´žå¯Ÿ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*6B2K7EcbYgMbH-QEp8J41w.png)\n\nLLaVA\\-Bench (COCO) æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„æ¡†æž¶ï¼Œé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„90ä¸ªé—®é¢˜æ¥è¯„ä¼°LLaVAçš„èƒ½åŠ›ï¼Œè¿™äº›é—®é¢˜æ¥æºäºŽ30å¼ ç²¾é€‰å›¾åƒï¼Œæ¶µç›–å¯¹è¯ã€è¯¦ç»†æè¿°å’Œå¤æ‚æŽ¨ç†ã€‚ç»“æžœå¦‚ä¸‹ï¼š\n\n* æŒ‡ä»¤è°ƒä¼˜æ•ˆæžœï¼šåœ¨è¿›è¡ŒæŒ‡ä»¤è°ƒä¼˜åŽï¼ŒLLaVAå¯¹ç”¨æˆ·å‘½ä»¤çš„éµä»Žæ€§æé«˜äº†è¶…è¿‡50åˆ†ã€‚\n* é—®é¢˜å¤šæ ·æ€§çš„å½±å“ï¼šå°½ç®¡è¯¦ç»†å’Œå¤æ‚æŽ¨ç†é—®é¢˜çš„å¢žåŠ å¾ˆå°ï¼Œä½†æ•´ä½“èƒ½åŠ›æé«˜äº†7åˆ†ã€‚è¿™ä¸€æå‡ä¹Ÿå¯¹å¯¹è¯é—®é¢˜çš„å“åº”äº§ç”Ÿäº†ç§¯æžå½±å“ï¼Œå±•ç¤ºäº†å¤šæ ·åŒ–è®­ç»ƒé›†çš„å¥½å¤„ã€‚\n* æœ€ä¼˜æ•°æ®ç»„åˆï¼šä¸‰ç§é—®é¢˜ç±»åž‹çš„ç»“åˆå¸¦æ¥äº†æœ€é«˜çš„æ€§èƒ½è·ƒå‡ï¼ŒLLaVAè¾¾åˆ°äº†85.1%çš„åŸºå‡†åˆ†æ•°ï¼Œå¼ºè°ƒäº†å…¨é¢æ•°æ®é›†åœ¨æå‡å¤šæ¨¡æ€AIèƒ½åŠ›æ–¹é¢çš„ä¼˜åŠ¿ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*mCjP0xfpcjHkl-lu)\n\n## LLaVAåœ¨LLaVA-Benchï¼ˆçœŸå®žåœºæ™¯ï¼‰ä¸Šçš„è¡¨çŽ°\n\n* åœ¨å¯¹è¯ä»»åŠ¡ä¸­ï¼ŒLLaVAçš„å‡†ç¡®çŽ‡ä¸º57.3%ï¼Œç›¸æ¯”BLIP-2çš„54.6%æœ‰æ˜Žæ˜¾æå‡ï¼Œè¿œè¶…OpenAIçš„Flamingoï¼ŒåŽè€…ä»…ä¸º19.3%ã€‚\n* åœ¨æä¾›è¯¦ç»†æè¿°æ–¹é¢ï¼ŒLLaVAå¾—åˆ†ä¸º52.5%ï¼Œå±•ç¤ºäº†å…¶ä»Žè§†è§‰çº¿ç´¢ä¸­ç”Ÿæˆä¸°å¯Œã€å…¨é¢å†…å®¹çš„èƒ½åŠ›ã€‚\n* è¯¥æ¨¡åž‹åœ¨å¤æ‚æŽ¨ç†é—®é¢˜ä¸Šçš„è¡¨çŽ°å°¤ä¸ºçªå‡ºï¼ŒæˆåŠŸçŽ‡è¾¾åˆ°81.7%ï¼Œè¡¨æ˜Žå…¶å…ˆè¿›çš„æŽ¨ç†å’ŒæŽ¨æ–­èƒ½åŠ›ã€‚\n\nLLaVAåœ¨æ‰€æœ‰ç±»åˆ«ä¸­çš„ç»¼åˆå¾—åˆ†ä¸º67.3%ï¼Œæ¯”BLIP-2é«˜å‡º29ä¸ªç™¾åˆ†ç‚¹ï¼Œè¶…è¿‡Flamingo 48ä¸ªç™¾åˆ†ç‚¹ã€‚\n\n## é™åˆ¶ä¸Žå…³æ³¨äº‹é¡¹ï¼š\n\nLLaVAçš„å®šé‡è¯„ä¼°ï¼š\n\nå°†GPT-4ä½œä¸ºè¯„ä¼°LLaVAæ€§èƒ½çš„è¯„åˆ¤è€…ï¼Œåœ¨åŸºå‡†æµ‹è¯•AIèƒ½åŠ›çš„æ¡†æž¶å†…æå‡ºäº†ä¸€ä¸ªå¤æ‚çš„æŒ‘æˆ˜ã€‚ä¸€æ–¹é¢ï¼ŒGPT-4çš„é«˜çº§ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ä½¿å…¶èƒ½å¤Ÿæ‰¹åˆ¤æ€§åœ°è¯„ä¼°åƒLLaVAè¿™æ ·çš„å€™é€‰æ¨¡åž‹æ‰€äº§ç”Ÿçš„å“åº”è´¨é‡ã€‚è¿™ç§è¯„ä¼°æ¶µç›–äº†æœ‰åŠ©äºŽè¡¡é‡æ¨¡åž‹åœ¨å¤šæ¨¡æ€æ•°æ®ä¸Šéµå¾ªæŒ‡ä»¤èƒ½åŠ›çš„å› ç´ ï¼Œå¦‚å¸®åŠ©æ€§ã€ç›¸å…³æ€§ã€å‡†ç¡®æ€§å’Œç»†èŠ‚ã€‚ç„¶è€Œï¼Œå¦ä¸€æ–¹é¢ï¼Œä½¿ç”¨GPT-4ä½œä¸ºè¯„ä¼°æ³•å®˜å¼•å‘äº†å…³äºŽåŸºå‡†æµ‹è¯•è¿‡ç¨‹å…¬æ­£æ€§çš„æ‹…å¿§ã€‚\n\nå…³æ³¨çš„æ ¸å¿ƒåœ¨äºŽï¼ŒLLaVAçš„æ•°æ®æ•´ç†è¿‡ç¨‹ä¸ŽGPT-4æ ¹æœ¬ä¸Šæ˜¯äº¤ç»‡åœ¨ä¸€èµ·çš„ã€‚ç”±äºŽGPT-4åœ¨è®­ç»ƒLLaVAæ—¶å‘æŒ¥äº†é‡è¦ä½œç”¨â€”â€”é€šè¿‡ç”Ÿæˆæ¨¡åž‹å¾®è°ƒæ‰€éœ€çš„éµå¾ªæŒ‡ä»¤çš„æ•°æ®â€”â€”å› æ­¤å­˜åœ¨å¾ªçŽ¯æŽ¨ç†çš„å›ºæœ‰é£Žé™©ã€‚æœ¬è´¨ä¸Šï¼ŒLLaVAå¯èƒ½å€¾å‘äºŽç”Ÿæˆä¸ŽGPT-4è®­ç»ƒæ•°æ®ä¸­å›ºæœ‰çš„æ¨¡å¼æˆ–åè§ä¸€è‡´çš„å“åº”ã€‚è¿™ç§å€¾å‘å¯èƒ½ä¼šæ‰­æ›²è¯„ä¼°ï¼Œå¯¼è‡´ä¸€ä¸ªç†è®ºä¸Šçš„ä¸Šé™ï¼Œåæ˜ å‡ºä¸ŽGPT-4æ–¹æ³•è®ºçš„å…¼å®¹æ€§ï¼Œè€Œä¸æ˜¯å¯¹æ™®éæ€§èƒ½çš„çœŸå®žè¡¡é‡ã€‚\n\næ­¤å¤–ï¼Œä¾èµ–GPT-4æä¾›å…¶è¯„ä¼°çš„å…¨é¢è§£é‡Šï¼Œå¼•å…¥äº†ä¸€ç§ä¸»è§‚æ€§ï¼Œè¿™ç§ä¸»è§‚æ€§æ ¹æ¤äºŽè¯­è¨€æ¨¡åž‹å¯¹ä»€ä¹ˆæž„æˆé«˜è´¨é‡å“åº”çš„â€œç†è§£â€ã€‚è¿™ç§ç†è§£å—åˆ°GPT-4è®­ç»ƒçš„æ•°æ®é›†çš„å½±å“ï¼Œè€Œè¿™äº›æ•°æ®é›†å¯èƒ½æœªèƒ½å……åˆ†ä½“çŽ°çŽ°å®žä¸–ç•Œå¤šæ¨¡æ€äº’åŠ¨çš„å¤šæ ·æ€§å’Œå¤æ‚æ€§ã€‚\n\n"},{"lang":"zh","group":"blog","slug":"blog/is-perplexity-pro-a-smarter-more-efficient-way-to-search-the-web-ec509321d820","frontmatter":{"title":"Perplexity Pro æ˜¯ä¸€ç§æ›´æ™ºèƒ½ã€æ›´é«˜æ•ˆçš„ç½‘ç»œæœç´¢æ–¹å¼å—ï¼Ÿ","meta_title":"Perplexity Pro æ˜¯ä¸€ç§æ›´æ™ºèƒ½ã€æ›´é«˜æ•ˆçš„ç½‘ç»œæœç´¢æ–¹å¼å—ï¼Ÿ","description":"Perplexity æ˜¯ä¸€ç§ç”±å¯¹è¯å¼ AI é©±åŠ¨çš„å›žç­”å¼•æ“Žï¼Œæ—¨åœ¨æä¾›å®žæ—¶ç­”æ¡ˆï¼Œè¶…è¶Šä¼ ç»Ÿæœç´¢å¼•æ“Žçš„é“¾æŽ¥åˆ—è¡¨ã€‚Perplexity Pro æä¾›å¢žå¼ºåŠŸèƒ½ï¼ŒåŒ…æ‹¬å¤šç§ AI æ¨¡åž‹é€‰æ‹©å’Œæ›´é«˜çš„ä½¿ç”¨é™åˆ¶ã€‚è¯¥å¹³å°é€‚ç”¨äºŽå¹¿æ³›é¢†åŸŸï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡æé—®èŽ·å¾—ç²¾å‡†ç­”æ¡ˆï¼Œè€Œæ— éœ€æµè§ˆå¤§é‡æ— å…³é“¾æŽ¥ã€‚ç„¶è€Œï¼Œä½¿ç”¨ç”Ÿæˆå¼ AI æ—¶ï¼Œç”¨æˆ·éœ€æ³¨æ„ä¿¡æ¯çš„å‡†ç¡®æ€§ï¼Œå»ºè®®è¿›è¡Œäº‹å®žæ ¸æŸ¥ã€‚","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*tGkjG6z62TZoaRpaUkSHuw.png","categories":["Chatbots","Natural Language Processing","Technology/Web"],"author":"Rifx.Online","tags":["perplexity","conversational","search","subscription","models"],"draft":false,"slug":"blog/is-perplexity-pro-a-smarter-more-efficient-way-to-search-the-web-ec509321d820"},"content":"\n\n\n## æœç´¢çš„æœªæ¥\n\nPerplexity Pro æ˜¯ä¸€ç§æ›´æ™ºèƒ½ã€æ›´é«˜æ•ˆçš„ç½‘ç»œæœç´¢æ–¹å¼å—ï¼Ÿ\n\n## å®ƒä¸Žä¼ ç»Ÿæœç´¢å¼•æ“Žç›¸æ¯”å¦‚ä½•ï¼Œå€¼å¾—èŠ±è´¹å—ï¼Ÿ\n\nå˜¿ï¼ŒAIæœ‹å‹ä»¬å’Œå…³æ³¨è€…ä»¬ã€‚\n\næˆ‘å—å¤Ÿäº†ã€‚æˆ‘å·²ç»åŽŒå€¦äº†ä¼ ç»Ÿæœç´¢ã€‚\n\næˆ‘ä¸æƒ³é€šè¿‡æ•°ç™¾ä¸ªé“¾æŽ¥æ¥æ‰¾åˆ°æˆ‘æƒ³è¦çš„**é‚£ä¸€æ¡ä¿¡æ¯**ï¼Œå®ƒè¢«åŸ‹åœ¨ç¬¬ä¸‰é¡µçš„å¹¿å‘Šä¹‹é—´ã€‚\n\nä¼ ç»Ÿæœç´¢å·²ç»ç»“æŸã€‚ä¸€ä¸ªåœäº§çš„æ¨¡åž‹ã€‚è¿‡åŽ»çš„é—ç‰©ã€‚\n\nçŽ°ä»£ç­”æ¡ˆå¼•æ“Žæ˜¯æœªæ¥ã€‚\n\nPerplexityæ‰¿è¯ºå°†å½»åº•æ”¹å˜äº’è”ç½‘æœç´¢ã€‚è®©æˆ‘ä»¬æ¥æŽ¢ç´¢ä¸€ä¸‹â€¦â€¦\n\næˆ‘å°†é¦–å…ˆè§£é‡Šä»€ä¹ˆæ˜¯Perplexityå’ŒPerplexity Proï¼Œç„¶åŽç»§ç»­ä»‹ç»å…³é”®ç‰¹æ€§ã€ä½¿ç”¨æ¡ˆä¾‹å’Œè¡Œä¸šã€‚æœ€åŽï¼Œæˆ‘å°†å¿«é€Ÿå°†å…¶ä¸ŽChatGPTç­‰æ›¿ä»£å“è¿›è¡Œæ¯”è¾ƒã€‚\n\næ‰€ä»¥è®©æˆ‘ä»¬ç›´æŽ¥æ·±å…¥ã€‚äº«å—å§ï¼\n\n## ä»€ä¹ˆæ˜¯ Perplexityï¼Ÿ\n\nPerplexity æ˜¯ä¸€ä¸ªç”±å¯¹è¯å¼ AI é©±åŠ¨çš„å›žç­”å¼•æ“Žï¼Œèƒ½å¤Ÿä¸ºå¤æ‚æŸ¥è¯¢æä¾›å®žæ—¶ç­”æ¡ˆï¼Œè€Œä¼ ç»Ÿæœç´¢å¼•æ“Žå¯èƒ½åªæä¾›ä¸€ç³»åˆ—æˆ–å¤šæˆ–å°‘æœ‰ç”¨çš„é“¾æŽ¥ã€‚\n\næˆ‘ç»™ä½ ä¸€ä¸ªå¿«é€Ÿçš„ä¾‹å­ã€‚\n\nåœ¨å·¥ä½œä¸­ï¼Œæˆ‘æƒ³åˆ›å»ºä¸€ä¸ªæˆ‘ä»¬çš„ç¡¬ä»¶æ¸…å•ï¼Œå› ä¸ºå¾ˆå¤šç¡¬ä»¶éƒ½è¿‡æ—¶äº†ï¼Œè¿‘æœŸéœ€è¦æ›´æ¢ã€‚\n\næ‰€ä»¥æˆ‘ä½¿ç”¨äº† Perplexityï¼ˆå½“ç„¶ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨ ChatGPTï¼‰ï¼Œå¹¶è¦æ±‚å®ƒåˆ›å»ºä¸€ä¸ªç¤ºä¾‹è¡¨æ ¼ã€‚æˆ‘ä¸ç¡®å®šå¦‚ä½•ç»“æž„åŒ–å®ƒï¼Œè€Œ Perplexity ç¡®å®žå¸®åŠ©æˆ‘æ•´ç†äº† Excel è¡¨æ ¼ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*SPnfT5NgUf01ahr3Wkz6-w@2x.jpeg)\n\nå½“ç„¶ï¼Œä½ å¯èƒ½ä¼šäº‰è¾©è¯´ï¼Œä½ ä¸éœ€è¦ç”Ÿæˆå¼ AI æ¥åˆ›å»ºä¸€ä¸ªç®€å•çš„è¡¨æ ¼ï¼Œè¿™ç¡®å®žæ˜¯å¯¹çš„ã€‚ä½†çœŸæ­£çš„ä¹è¶£åœ¨äºŽä¹‹åŽã€‚æˆ‘ä¹‹å‰ä»Žæœªåœ¨ Excel ä¸­åˆ›å»ºè¿‡æ•°æ®é€è§†è¡¨ï¼Œæˆ‘æƒ³ä»…æ˜¾ç¤ºè¿‡æœŸä¿ä¿®çš„ç¡¬ä»¶ã€‚\n\nå½“æˆ‘è¯¢é—® Perplexity æ—¶ï¼Œå®ƒä¸ä»…ç»™äº†æˆ‘é€æ­¥çš„è¯´æ˜Žï¼Œè¿˜ä½¿ç”¨äº†æˆ‘çš„ç¤ºä¾‹æ•°æ®æ¥å‘æˆ‘å±•ç¤ºå¦‚ä½•è®¾ç½®æ•°æ®é€è§†è¡¨ã€‚è¿™æ˜¯ä¼ ç»Ÿæœç´¢æ— æ³•åšåˆ°çš„ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*lj92gWPB3w4xqaiSCc5r3Q@2x.jpeg)\n\nå› æ­¤ï¼ŒPerplexity å°†ç»å…¸æœç´¢å¼•æ“Žä¸Žåƒ GPT-4o æˆ– Claude\\* è¿™æ ·çš„ AI æ¨¡åž‹ç»“åˆåœ¨ä¸€èµ·ï¼Œå…è®¸ç”¨æˆ·æå‡ºé—®é¢˜å’ŒåŽç»­é—®é¢˜ã€‚å®ƒå¯ä»¥å¤„ç†æ–‡æ¡£å’Œç…§ç‰‡ï¼Œç”Ÿæˆä»£ç ï¼Œåˆ›å»ºå†…å®¹ï¼Œå¹¶å¯¹å„ç§ä¸»é¢˜è¿›è¡Œæ·±å…¥ç ”ç©¶ã€‚\n\nå®ƒçš„ç‹¬ç‰¹ä¹‹å¤„åœ¨äºŽåŒ…å«è„šæ³¨ä»¥ä¾›è¿›ä¸€æ­¥ç ”ç©¶æˆ–æ£€æŸ¥å‡†ç¡®æ€§ã€‚\n\næ­£å¦‚ä½ ä»Žä¸Šé¢çš„ä¾‹å­ä¸­çœ‹åˆ°çš„ï¼Œç­”æ¡ˆç²¾å‡†ï¼Œè¿œè¿œè¶…å‡ºäº†æœç´¢å¼•æ“Žçš„èƒ½åŠ›ã€‚\n\n*\\*é¢˜å¤–è¯ï¼šGPT-40 æ˜¯ä¸€ç§æ›´å…ˆè¿›çš„ AI æ¨¡åž‹ï¼Œç”¨äºŽéœ€è¦æ›´é«˜å‡†ç¡®æ€§å’Œå¤„ç†èƒ½åŠ›çš„ä»»åŠ¡ï¼Œè€Œ Claude 3 Opus æ“…é•¿åˆ›æ„å†™ä½œå’Œä»£ç ç”Ÿæˆç­‰ä»»åŠ¡ã€‚*\n\n## ä»€ä¹ˆæ˜¯ Perplexity Proï¼Ÿ\n\nPerplexity Pro æ˜¯ä¸€ç§å¢žå¼ºçš„è®¢é˜…æœåŠ¡ï¼Œæä¾›ç”¨æˆ·è®¿é—®ä¸åŒçš„ AI æ¨¡åž‹ï¼ˆç”¨äºŽä¸åŒçš„ç”¨ä¾‹ï¼‰å’Œè¶…å‡ºæ ‡å‡†ç‰ˆæœ¬çš„åŠŸèƒ½ã€‚\n\nä½¿ç”¨ Proï¼Œæ‚¨å¯ä»¥é€‰æ‹© GPT\\-40ã€Claude Sonnet 3\\.5ã€Claude 3 Opusã€Sonar Large 32k å’Œä¸€ä¸ªé»˜è®¤æ¨¡åž‹ï¼ˆæˆªè‡³æ’°å†™æ—¶ï¼‰ã€‚\n\nPro ç”¨æˆ·å¯ä»¥äº«å—æ›´é«˜çš„ä½¿ç”¨é™åˆ¶ã€æ›´å¿«çš„å“åº”æ—¶é—´ã€ä¸ªæ€§åŒ–çš„æœç´¢ç»“æžœä»¥åŠé«˜çº§æ–‡æ¡£åˆ†æžï¼ˆæˆªè‡³æ’°å†™æ—¶ï¼‰å‡ ä¹Žæ— é™çš„æ–‡ä»¶ä¸Šä¼ ã€‚\n\nä»¥ä¸‹æ˜¯ Perplexity å’Œ Perplexity Pro å¯¹åŒä¸€é—®é¢˜çš„å›žç­”ç¤ºä¾‹ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*RZf3p_4hS2mGIU9iFf23rw.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*-gUUeHj-oWwFlZARJ3YG_g.png)\n\nå¦‚æ‚¨æ‰€è§ï¼ŒPro çš„å›žç­”æ›´åŠ è¯¦ç»†ã€‚\n\n## Perplexity Proé€‚åˆè°ï¼Ÿ\n\nPerplexityæœ‰å¹¿æ³›çš„ä½¿ç”¨æ¡ˆä¾‹å’Œåº”ç”¨é¢†åŸŸã€‚è®©æˆ‘ä»¬åšä¸€ä¸ªæœ‰è¶£çš„å®žéªŒï¼Œé—®é—®Perplexityã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*PJvSMVKPKvUcwkjB8i91Gg.png)\n\nPerplexityç»™å‡ºäº†è¯¦ç»†çš„å›žç­”ã€‚è®©æˆ‘ä»¬åˆ†è§£ä¸€ä¸‹ï¼š\n\nPerplexityå’ŒPerplexity Proå¯ä»¥åº”ç”¨äºŽå„ç§é¢†åŸŸã€è¡Œä¸šå’Œä¸ªäººã€‚å³ä½¿ä½œä¸ºä¸€ä¸ªæ™®é€šçš„ç½‘ç»œç”¨æˆ·ï¼Œä½ ä¹Ÿå¯ä»¥åœ¨ä¸æµè§ˆæ•°åä¸ªé“¾æŽ¥å’Œå¹¿å‘Šçš„æƒ…å†µä¸‹èŽ·å¾—ä½ é—®é¢˜çš„å‡†ç¡®ç­”æ¡ˆã€‚\n\nå°±æˆ‘ä¸ªäººè€Œè¨€ï¼Œæˆ‘æ¯å¤©éƒ½ä½¿ç”¨Perplexityè¿›è¡Œæ‰€æœ‰æœç´¢ï¼Œå®ƒå·²ç»å–ä»£äº†Googleæˆä¸ºæˆ‘çš„æ ‡å‡†æœç´¢å¼•æ“Žã€‚ä½ å¯ä»¥åœ¨è¿™é‡Œé˜…è¯»æ›´å¤šå…³äºŽå®ƒçš„ä¿¡æ¯ã€‚æ˜¯çš„ï¼Œæˆ‘æ€»æ˜¯ä½¿ç”¨ä¸“ä¸šæœç´¢ã€‚è™½ç„¶å¯èƒ½éœ€è¦èŠ±è´¹æ›´å¤šæ—¶é—´ï¼Œä½†æ ¹æ®æˆ‘çš„ç»éªŒï¼Œç»“æžœæ›´å¥½ä¸”æ›´å…¨é¢ã€‚\n\nè¿™æ˜¯Perplexityçš„ç»“æžœï¼š\n\n* æŠ€æœ¯ä¸Žå·¥ç¨‹\n* é”€å”®ä¸Žå¸‚åœºè¥é”€\n* äº§å“å¼€å‘\n* æ³•å¾‹ä¸ŽåŒ»ç–—ä¿å¥\n* ä½“è‚²ä¸Žå¨±ä¹\n* è´¢åŠ¡ä¸Žæˆ˜ç•¥\n* æ•°æ®ç§‘å­¦\n* ç”µä¿¡\n* äººå·¥æ™ºèƒ½ä¸Žæœºå™¨å­¦ä¹ \n\nä¸€èˆ¬æ¥è¯´ï¼Œä½ å¯ä»¥è¯´å®ƒå¯¹ä»»ä½•é¢†åŸŸæˆ–è¡Œä¸šçš„äººéƒ½å¾ˆæœ‰ç”¨ï¼Œè¿™ç¡®å®žæ˜¯äº‹å®žã€‚å³ä½¿ä½ é€šå¸¸ä¸ä½¿ç”¨ç”Ÿæˆå¼AIï¼Œä»…ä»…æ˜¯èŽ·å¾—ä¸€ä¸ªé—®é¢˜çš„ç­”æ¡ˆï¼Œè€Œä¸éœ€è¦æµè§ˆæ•°åä¸ªæ— å…³çš„é“¾æŽ¥ï¼Œè¿™ä¹Ÿæ˜¯ä¸€ä¸ªå¾ˆå¤§çš„ä¼˜ç‚¹ã€‚\n\nè¿™æœ‰ç‚¹åƒå’Œä¸€ä¸ªä»€ä¹ˆéƒ½çŸ¥é“çš„æœ‹å‹èŠå¤©ã€‚\n\næƒ³è±¡ä¸€ä¸‹ï¼Œä½ æ­£åœ¨åº¦å‡ï¼Œæƒ³çŸ¥é“ä»Žé…’åº—åˆ°ä¸€ä¸ªè‘—åæ™¯ç‚¹æ˜¯å¦å¯ä»¥æ­¥è¡Œã€‚ä½ å¯ä»¥æ‰“å¼€é…’åº—ç½‘ç«™å››å¤„çœ‹çœ‹ï¼Œä½¿ç”¨æœç´¢å¼•æ“Žå¸Œæœ›èƒ½å¾—åˆ°ä¸€ä¸ªæœ‰ç”¨çš„ç­”æ¡ˆï¼Œæˆ–è€…ä½ å¯ä»¥ç›´æŽ¥é—®Perplexityã€‚å®ƒä¸ä»…ä¼šç»™ä½ ä¸€ä¸ªå…¨é¢çš„ç­”æ¡ˆï¼Œè¿˜ä¼šåŒ…æ‹¬æ–¹å‘ã€ä¸€äº›ç…§ç‰‡å’Œè§†é¢‘é“¾æŽ¥â€”â€”æ‰€æœ‰ä¿¡æ¯éƒ½åœ¨ä¸€é¡µä¸Šã€‚å¦‚æžœä½ éœ€è¦æ›´å¤šç»†èŠ‚ï¼Œä½ å¯ä»¥ç›´æŽ¥é—®ä¸€ä¸ªåŽç»­é—®é¢˜ã€‚\n\n## æˆ‘å¦‚ä½•ä½¿ç”¨ Perplexity\n\næˆ‘å‡ ä¹Žæ‰€æœ‰çš„ç½‘ç»œæœç´¢éƒ½æ˜¯åœ¨è¿™é‡Œè¿›è¡Œçš„ã€‚\n\nä»…ä»…å‡ å‘¨ä¹‹åŽï¼Œæˆ‘å·²ç»ä¹ æƒ¯äº†æé—®ï¼Œè€Œä¸æ˜¯åœ¨æœç´¢å¼•æ“Žä¸­è¾“å…¥å…³é”®è¯ï¼Œä»¥è‡³äºŽæˆ‘æ— æ³•ç›¸ä¿¡æˆ‘æ›¾ç»æ˜¯å¦‚ä½•æ²¡æœ‰å®ƒç”Ÿæ´»çš„ã€‚\n\næé—®çš„æ„Ÿè§‰æˆªç„¶ä¸åŒï¼Œå³ä½¿æ˜¯å‡†ç¡®è¯´æ˜Žä½ éœ€è¦çŸ¥é“çš„å†…å®¹ï¼Œè€Œä¸æ˜¯çŒœæµ‹å“ªäº›å…³é”®è¯å¯èƒ½ä¼šç»™ä½ æƒ³è¦çš„ç­”æ¡ˆã€‚\n\nå³ä½¿æ˜¯åƒâ€œæŸä½åäººå¤šå¤§äº†â€è¿™æ ·ç®€å•çš„é—®é¢˜ï¼Œä½¿ç”¨ Perplexity ä¹Ÿè¦å®¹æ˜“å¾—å¤šã€‚é€šå¸¸æˆ‘è¿˜éœ€è¦çŸ¥é“è‡³å°‘ä¸€ä¸ªåŽç»­é—®é¢˜ï¼Œè€Œ Perplexity ç”šè‡³ä¼šé¢„è§æˆ‘å¯èƒ½æƒ³çŸ¥é“çš„å†…å®¹ï¼Œå¹¶åœ¨å›žç­”çš„æœ€åŽå»ºè®®åŽç»­é—®é¢˜ã€‚\n\nä»¥ä¸‹æ˜¯ä»Ž ChatGPT å’Œå…¶ä»–ç”Ÿæˆæ€§ AI å¹³å°èŽ·å¾—æ›´å¥½ç»“æžœçš„ä¸‰ä¸ªæŠ€å·§ï¼Œä½†ä½ ä¹Ÿå¯ä»¥å°†è¿™äº›æŠ€å·§åº”ç”¨äºŽ Perplexityï¼š\n\n## åŠŸèƒ½ç»†åˆ†ä¸Žå®šä»·\n\nPerplexity æ˜¯å…è´¹çš„ï¼ŒåŸºæœ¬çš„ç½‘ç»œæœç´¢è¶³å¤Ÿä½¿ç”¨ã€‚ä½œä¸ºæ³¨å†Œç”¨æˆ·ï¼Œæ‚¨æ¯å››å°æ—¶ç”šè‡³å¯ä»¥èŽ·å¾— 5 æ¬¡ Pro æœç´¢ã€‚å¯¹äºŽä»»ä½•è¶…å‡ºå¿«é€Ÿå›žç­”çš„éœ€æ±‚ï¼ŒPro ç»å¯¹æ˜¯æœ€ä½³é€‰æ‹©ã€‚\n\nPro æœç´¢èƒ½å¤Ÿç†è§£æ‚¨çš„é—®é¢˜ï¼Œå¯èƒ½ä¼šå°†å…¶åˆ†è§£ä¸ºæ›´å°çš„ä»»åŠ¡ï¼Œå¹¶å¯èƒ½åœ¨æä¾›æ›´å…¨é¢çš„ç­”æ¡ˆä¹‹å‰è¯¢é—®æ‚¨ä¸€ä¸ªé—®é¢˜ï¼ˆè¯·å‚è§ä¸Šé¢çš„æˆªå›¾ï¼‰ã€‚\n\nåœ¨ Pro è®¡åˆ’ä¸­ï¼Œæ‚¨ç”šè‡³å¯ä»¥é€‰æ‹© AI æ¨¡åž‹ï¼ˆå³ ChatGPT\\-4o æˆ– Claude Sonnet 3\\.5 ç­‰ï¼‰ï¼Œè€Œä¸å¿…ä¾èµ–æ ‡å‡†çš„ Perplexity AI æ¨¡åž‹ã€‚\n\nå¦‚æžœæ‚¨æ›´å–œæ¬¢å¤„ç†æ–‡ä»¶å¹¶æƒ³è¦ä¸Šä¼  PDF æˆ–åˆ†æžç…§ç‰‡ï¼Œæ’°å†™æ—¶å…è´¹è®¡åˆ’çš„é™åˆ¶æ˜¯æ¯å¤© 3 ä¸ªï¼Œè€Œ Pro è®¡åˆ’åˆ™æ˜¯æ— é™åˆ¶ï¼ˆå°½ç®¡ Perplexity ä»…è¡¨ç¤ºæ‚¨æ¯å¤©å¯ä»¥ä¸Šä¼ è‡³å°‘ 100 ä¸ªæ–‡ä»¶â€”â€”æˆ‘ä¸çŸ¥é“å¦‚æžœæ‚¨ä¸Šä¼ æ›´å¤šä¼šå‘ç”Ÿä»€ä¹ˆï¼Œå› ä¸ºæˆ‘ä»Žæœªä¸Šä¼ è¿‡é‚£ä¹ˆå¤šï¼‰ã€‚\n\nå®šä»·ä¸ºæ¯æœˆ $20 æˆ–æ¯å¹´ $200ã€‚\n\n## æ¯”è¾ƒæ›¿ä»£æ–¹æ¡ˆï¼Ÿ\n\nå¯»æ‰¾ Perplexity çš„æ›¿ä»£æ–¹æ¡ˆæ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„ï¼Œå› ä¸ºå…¶ä»–ç”Ÿæˆå¼ AI å¹³å°é€šå¸¸å¹¶ä¸æ˜¯ä½œä¸ºæœç´¢ï¼ˆå›žç­”ï¼‰å¼•æ“Žè®¾è®¡çš„ã€‚è¿™æ„å‘³ç€ä½ å¯ä»¥å‘ ChatGPTã€Claude æˆ–ä½ æ­£åœ¨ä½¿ç”¨çš„ä»»ä½• AI å¹³å°æé—®ï¼Œå®ƒä»¬ä¼šåˆ©ç”¨çŽ°æœ‰çŸ¥è¯†æä¾›ç­”æ¡ˆã€‚\n\nåœ¨å®ƒä»¬çš„è®¢é˜…æ¨¡åž‹ä¸­ï¼Œå¯èƒ½å…è®¸è¿›è¡Œç½‘é¡µæœç´¢ï¼Œä½†ä¸ŽçœŸæ­£çš„å›žç­”å¼•æ“Žç›¸æ¯”ï¼Œè¿™å¹¶ä¸ç®—ä»€ä¹ˆã€‚\n\næ˜¯çš„ï¼Œç¡®å®žè¿˜æœ‰å…¶ä»–ä¸€äº›å›žç­”å¼•æ“Žï¼ˆä½ å¯ä»¥è¯¢é—® Perplexity å…³äºŽå®ƒä»¬çš„ä¿¡æ¯ï¼‰ï¼Œä½†å°±æˆ‘ä¸ªäººçš„ç»éªŒè€Œè¨€ï¼ŒPerplexity æä¾›äº†æœ€å…¨é¢çš„ç­”æ¡ˆã€‚\n\nå°±æˆ‘è€Œè¨€ï¼Œæˆ‘åŒæ—¶ä¸º ChatGPT å’Œ Perplexity ä»˜è´¹ï¼Œå› ä¸ºå®ƒä»¬çš„ç”¨é€”ä¸åŒï¼Œæˆ‘éœ€è¦ ChatGPT æ¥å¸®åŠ©æˆ‘çš„å­¦ä¹ ã€‚è¿™ä¸ªæ–¹é¢å®ƒç¡®å®žè¡¨çŽ°å‡ºè‰²ã€‚\n\nä½ å¯ä»¥åœ¨è¿™é‡Œé˜…è¯»æ›´å¤šä¿¡æ¯ï¼š\n\n## ä»€ä¹ˆæ˜¯é™·é˜±ï¼Ÿ\n\nå½“ç„¶ï¼Œæ€»æ˜¯æœ‰ä¸€ä¸ªé™·é˜±ã€‚å¿…é¡»æœ‰ã€‚\n\næ˜¯çš„ï¼Œç¡®å®žæœ‰ä¸€ä¸ªã€‚ç”±äºŽ Perplexity ä½¿ç”¨ç”Ÿæˆå¼ AIï¼Œå› æ­¤æ¨¡åž‹æ€»æ˜¯æœ‰å¯èƒ½äº§ç”Ÿå¹»è§‰ã€‚è¿™æ„å‘³ç€å®ƒæ— è®ºå¦‚ä½•éƒ½æƒ³ç»™ä½ ä¸€ä¸ªç­”æ¡ˆï¼Œå¦‚æžœæ‰¾ä¸åˆ°ä»»ä½•ä¿¡æ¯ï¼Œå¯èƒ½ä¼šç¼–é€ ä¸€äº›å†…å®¹ã€‚\n\nå› æ­¤ï¼Œäº‹å®žæ ¸æŸ¥éžå¸¸é‡è¦ã€‚\n\nå¹¸è¿çš„æ˜¯ï¼ŒPerplexity é€šè¿‡åŒ…å«è„šæ³¨ä½¿è¿™å˜å¾—ç›¸å¯¹ç®€å•ï¼Œå¦‚æžœä½ ä¸ç¡®å®šæˆ–éœ€è¦ç¡®è®¤ï¼Œå¯ä»¥å¿«é€Ÿæ£€æŸ¥ã€‚\n\næˆ‘å¼ºçƒˆé¼“åŠ±ä½ è¿™æ ·åšï¼Œç‰¹åˆ«æ˜¯å½“ä½ ç ”ç©¶ä¸€äº›æ›´ä¸¥è‚ƒçš„ä¸»é¢˜æ—¶ã€‚\n\nPerplexity Pro æ˜¯ä¼ ç»Ÿæœç´¢çš„é‡å¤§å‡çº§ã€‚ä½ æ˜¯ä½¿ç”¨ç­”æ¡ˆå¼•æ“Žï¼Œè¿˜æ˜¯ä»ç„¶æ˜¯ä¼ ç»Ÿæœç´¢å¼•æ“Žçš„ç”¨æˆ·ï¼ŸðŸ’¬\n\n\n> Hej there! Can I ask you a favour (it will really help me out to grow this blog)? If you find this article insightful, follow **me please** and **clap 50 times.** Or feel free to [buy me a coffee](https://buy.stripe.com/cN28xZgDweSd52M000). **Thanks for reading!**\n\n"},{"lang":"zh","group":"blog","slug":"blog/key-points-llm-quantization-chatgpt-artificial-intelligence-8201ffcb33d4","frontmatter":{"title":"è§£é” LLM é‡åŒ–çš„ 5 ä¸ªå…³é”®ç‚¹","meta_title":"è§£é” LLM é‡åŒ–çš„ 5 ä¸ªå…³é”®ç‚¹","description":"é‡åŒ–å¤§åž‹è¯­è¨€æ¨¡åž‹","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*RUqPEr2NTYXlI1omqF22Qg.png","categories":["Machine Learning","Data Science","Technology/Web"],"author":"Rifx.Online","tags":["quantization","weights","activations","calibration","Quanto"],"draft":false,"slug":"blog/key-points-llm-quantization-chatgpt-artificial-intelligence-8201ffcb33d4"},"content":"\n\n\n### å¤§åž‹è¯­è¨€æ¨¡åž‹çš„é‡åŒ–\n\n\n\nLLMé‡åŒ–ç›®å‰æ˜¯ä¸€ä¸ªçƒ­é—¨è¯é¢˜ï¼Œå› ä¸ºå®ƒåœ¨æé«˜å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰çš„æ•ˆçŽ‡å’Œåœ¨å„ç§ç¡¬ä»¶å¹³å°ï¼ˆåŒ…æ‹¬æ¶ˆè´¹çº§è®¾å¤‡ï¼‰ä¸Šéƒ¨ç½²æ–¹é¢å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚\n\né€šè¿‡è°ƒæ•´æ¨¡åž‹ä¸­æŸäº›ç»„ä»¶çš„ç²¾åº¦ï¼Œ**é‡åŒ–æ˜¾è‘—å‡å°‘äº†æ¨¡åž‹çš„å†…å­˜å ç”¨**ï¼ŒåŒæ—¶ä¿æŒç›¸ä¼¼çš„æ€§èƒ½æ°´å¹³ã€‚\n\nåœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†æŽ¢è®¨LLMé‡åŒ–çš„äº”ä¸ªå…³é”®æ–¹é¢ï¼ŒåŒ…æ‹¬å°†æ­¤æŠ€æœ¯åº”ç”¨äºŽæˆ‘ä»¬æ¨¡åž‹çš„ä¸€äº›å®žç”¨æ­¥éª¤ã€‚\n\n## #1. ç†è§£é‡åŒ–\n\né‡åŒ–æ˜¯ä¸€ç§æ¨¡åž‹åŽ‹ç¼©æŠ€æœ¯ï¼Œé€šè¿‡é™ä½Ž LLM ä¸­æƒé‡å’Œæ¿€æ´»çš„ç²¾åº¦æ¥å®žçŽ°ã€‚è¿™æ¶‰åŠå°†é«˜ç²¾åº¦å€¼è½¬æ¢ä¸ºä½Žç²¾åº¦å€¼ï¼Œå®žé™…ä¸Šæ˜¯**å°†å­˜å‚¨æ›´å¤šä¿¡æ¯çš„æ•°æ®ç±»åž‹æ›´æ”¹ä¸ºå­˜å‚¨æ›´å°‘ä¿¡æ¯çš„æ•°æ®ç±»åž‹**ã€‚\n\nå‡å°‘æ¯ä¸ªæƒé‡æˆ–æ¿€æ´»æ‰€éœ€çš„ä½æ•°æ˜¾è‘—é™ä½Žäº†æ•´ä½“æ¨¡åž‹å¤§å°ã€‚å› æ­¤ï¼Œ**é‡åŒ–åˆ›å»ºäº†ä½¿ç”¨æ›´å°‘å†…å­˜å’Œéœ€è¦æ›´å°‘å­˜å‚¨ç©ºé—´çš„ LLMã€‚**\n\nè¿™ä¸€æŠ€æœ¯åœ¨åº”å¯¹ LLM è¿žç»­è¿­ä»£ä¸­å‚æ•°æ•°é‡çš„æŒ‡æ•°å¢žé•¿æ—¶å˜å¾—è‡³å…³é‡è¦ã€‚ä¾‹å¦‚ï¼Œåœ¨ OpenAI çš„ GPT ç³»åˆ—ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ä»¥ä¸‹å›¾è¡¨ä¸­è§‚å¯Ÿåˆ°è¿™ä¸€å¢žé•¿è¶‹åŠ¿ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*QlAhma3Wu1F6w2WvkE8jDA.png)\n\nè¿™ä¸€æ˜¾è‘—å¢žåŠ å¸¦æ¥äº†æŒ‘æˆ˜ï¼šéšç€æ¨¡åž‹çš„å¢žé•¿ï¼Œå®ƒä»¬çš„å†…å­˜éœ€æ±‚å¾€å¾€è¶…è¿‡å…ˆè¿›ç¡¬ä»¶åŠ é€Ÿå™¨ï¼ˆå¦‚ GPUï¼‰çš„å®¹é‡ã€‚**è¿™éœ€è¦åˆ†å¸ƒå¼è®­ç»ƒå’ŒæŽ¨ç†æ¥ç®¡ç†è¿™äº›æ¨¡åž‹ï¼Œä»Žè€Œé™åˆ¶äº†å®ƒä»¬çš„å¯éƒ¨ç½²æ€§ã€‚**\n\n## #2. é‡åŒ–èƒŒåŽçš„ç›´è§‰\n\nå°½ç®¡é‡åŒ–çš„å®šä¹‰çœ‹èµ·æ¥ç›¸å½“å¤æ‚ï¼Œä½†è¿™ä¸ªæ¦‚å¿µå¯ä»¥é€šè¿‡çŸ©é˜µç›´è§‚åœ°è§£é‡Šã€‚\n\nè®©æˆ‘ä»¬è€ƒè™‘ä»¥ä¸‹ä¸€ä¸ª 3x3 çŸ©é˜µï¼Œè¡¨ç¤ºç¥žç»ç½‘ç»œçš„æƒé‡ã€‚å·¦ä¾§çš„çŸ©é˜µæ˜¾ç¤ºäº†åŽŸå§‹æƒé‡ï¼Œè€Œå³ä¾§çš„çŸ©é˜µæ˜¾ç¤ºäº†è¿™äº›æƒé‡çš„é‡åŒ–ç‰ˆæœ¬ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*LPzWe9oxjlDYdSp7dVvRUg.png)\n\nåœ¨è¿™ä¸ªç®€å•çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†åŽŸå§‹çŸ©é˜µçš„å…ƒç´ ä»Žå››ä½å°æ•°å››èˆäº”å…¥åˆ°ä¸€ä½å°æ•°ã€‚å°½ç®¡çŸ©é˜µçœ‹èµ·æ¥ç›¸ä¼¼ï¼Œ**ä½†å››ä½å°æ•°ç‰ˆæœ¬æ‰€éœ€çš„å­˜å‚¨ç©ºé—´æ˜¾è‘—æ›´é«˜**ã€‚\n\nåœ¨å®žè·µä¸­ï¼Œé‡åŒ–ä¸ä»…ä»…æ˜¯ä¸€ä¸ªå››èˆäº”å…¥æ“ä½œã€‚ç›¸åï¼Œå®ƒæ¶‰åŠå°†æ•°å€¼è½¬æ¢ä¸ºä¸åŒçš„æ•°æ®ç±»åž‹ï¼Œé€šå¸¸æ˜¯ä»Žæ›´é«˜ç²¾åº¦è½¬æ¢ä¸ºæ›´ä½Žç²¾åº¦ã€‚\n\nä¾‹å¦‚ï¼Œå¤§å¤šæ•°æ¨¡åž‹çš„é»˜è®¤æ•°æ®ç±»åž‹æ˜¯ `float32`ï¼Œæ¯ä¸ªå‚æ•°éœ€è¦ 4 å­—èŠ‚ï¼ˆ32 ä½ï¼‰ã€‚å› æ­¤ï¼Œå¯¹äºŽä¸€ä¸ª 3x3 çŸ©é˜µï¼Œæ€»å†…å­˜å ç”¨ä¸º 36 å­—èŠ‚ã€‚å°†æ•°æ®ç±»åž‹æ›´æ”¹ä¸º `int8`ï¼Œæ¯ä¸ªå‚æ•°åªéœ€è¦ 1 å­—èŠ‚ï¼Œä»Žè€Œå°†çŸ©é˜µçš„æ€»å†…å­˜å ç”¨å‡å°‘åˆ°ä»… 9 å­—èŠ‚ã€‚\n\n## #3. é‡åŒ–è¯¯å·®\n\næ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼ŒåŽŸå§‹çŸ©é˜µåŠå…¶é‡åŒ–å½¢å¼å¹¶ä¸å®Œå…¨ç›¸ç­‰ï¼Œä½†éžå¸¸ç›¸ä¼¼ã€‚é€å€¼ä¹‹é—´çš„å·®å¼‚è¢«ç§°ä¸ºâ€œé‡åŒ–è¯¯å·®â€ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ç”¨çŸ©é˜µå½¢å¼è¡¨ç¤ºï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*VtGDjVbr7daagLXB57i7Mg.png)\n\n**è¿™ç§é‡åŒ–è¯¯å·®å¯ä»¥åœ¨ç½‘ç»œä¸­çš„æ¯ä¸ªæƒé‡çŸ©é˜µä¸­ç´¯ç§¯ï¼Œä»Žè€Œå½±å“æ¨¡åž‹çš„æ€§èƒ½ã€‚**\n\nå½“å‰çš„é‡åŒ–ç ”ç©¶æ—¨åœ¨æœ€å°åŒ–ç²¾åº¦å·®å¼‚ï¼ŒåŒæ—¶å‡å°‘è®­ç»ƒæˆ–æŽ¨ç†æ¨¡åž‹æ‰€éœ€çš„è®¡ç®—èµ„æºï¼ŒåŒæ—¶ä¿æŒå¯æŽ¥å—çš„æ€§èƒ½æ°´å¹³ã€‚\n\n## #4. çº¿æ€§é‡åŒ–\n\nçº¿æ€§é‡åŒ–æ˜¯ LLMs ä¸­æœ€æµè¡Œçš„é‡åŒ–æ–¹æ¡ˆä¹‹ä¸€ã€‚ç®€å•æ¥è¯´ï¼Œå®ƒæ¶‰åŠå°†åŽŸå§‹æƒé‡çš„æµ®ç‚¹å€¼èŒƒå›´æ˜ å°„åˆ°å›ºå®šç‚¹å€¼èŒƒå›´ã€‚\n\nè®©æˆ‘ä»¬å›žé¡¾ä¸€ä¸‹å°†çº¿æ€§é‡åŒ–åº”ç”¨äºŽæˆ‘ä»¬çš„æ¨¡åž‹æ‰€éœ€çš„æ­¥éª¤ï¼š\n\n* **èŽ·å–æœ€å°å’Œæœ€å¤§èŒƒå›´ï¼š** æˆ‘ä»¬éœ€è¦èŽ·å–å¾…é‡åŒ–çš„æµ®ç‚¹æƒé‡çš„æœ€å°å€¼å’Œæœ€å¤§å€¼ï¼ˆ`x_min` å’Œ `x_max`ï¼‰ã€‚æˆ‘ä»¬è¿˜éœ€è¦å®šä¹‰é‡åŒ–èŒƒå›´ï¼ˆ`q_min` å’Œ `q_max`ï¼‰ï¼Œè¯¥èŒƒå›´å·²ç»ç”±æˆ‘ä»¬æƒ³è¦è½¬æ¢çš„æ•°æ®ç±»åž‹è®¾ç½®ã€‚\n* **è®¡ç®—ç¼©æ”¾å› å­ï¼ˆ`s`ï¼‰å’Œé›¶ç‚¹ï¼ˆ`z`ï¼‰å€¼ï¼š** é¦–å…ˆï¼Œç¼©æ”¾å› å­ï¼ˆ`s`ï¼‰å°†æµ®ç‚¹å€¼çš„èŒƒå›´è°ƒæ•´åˆ°é€‚åˆæ•´æ•°èŒƒå›´ï¼Œä¿æŒæ•°æ®åˆ†å¸ƒå’ŒèŒƒå›´ã€‚å…¶æ¬¡ï¼Œé›¶ç‚¹ï¼ˆ`z`ï¼‰ç¡®ä¿æµ®ç‚¹èŒƒå›´å†…çš„é›¶è¢«å‡†ç¡®åœ°è¡¨ç¤ºä¸ºæ•´æ•°ï¼Œä»Žè€Œä¿æŒæ•°å€¼çš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§ï¼Œç‰¹åˆ«æ˜¯å¯¹äºŽæŽ¥è¿‘é›¶çš„å€¼ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BepC6-izw0yE19ejsS705Q.png)\n\n* **é‡åŒ–å€¼ï¼ˆ`q`ï¼‰ï¼š** æˆ‘ä»¬éœ€è¦ä½¿ç”¨åœ¨å‰ä¸€æ­¥è®¡ç®—çš„ç¼©æ”¾å› å­ï¼ˆ`s`ï¼‰å’Œé›¶ç‚¹ï¼ˆ`z`ï¼‰å°†åŽŸå§‹æµ®ç‚¹å€¼æ˜ å°„åˆ°æ•´æ•°èŒƒå›´ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BBOQ0VbSGbwf7CN8c4PWKQ.png)\n\nåº”ç”¨è¿™äº›å…¬å¼ç›¸å½“ç®€å•ã€‚å¦‚æžœæˆ‘ä»¬å°†å®ƒä»¬åº”ç”¨äºŽä¸‹å›¾å·¦ä¾§çš„ 3x3 æƒé‡å¼ é‡ï¼Œæˆ‘ä»¬å°†å¾—åˆ°å³ä¾§æ‰€ç¤ºçš„é‡åŒ–çŸ©é˜µï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*KzBvg84mfI2gAhTIyVibwQ.png)\n\næˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œ`int8` å€¼çš„ä¸‹é™å¯¹åº”äºŽåŽŸå§‹å¼ é‡çš„ä¸‹é™ï¼Œè€Œä¸Šé™å¯¹åº”äºŽåŽŸå§‹å¼ é‡çš„ä¸Šé™ï¼Œ*å³ï¼Œæ˜ å°„ä¸º `0.50 â†’ 255` å’Œ `-0.40 â†’ 0`ã€‚*\n\næˆ‘ä»¬çŽ°åœ¨å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„å…¬å¼å¯¹å€¼è¿›è¡Œåé‡åŒ–ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*E5nnqYzncYCRuM5prssuOw.png)\n\nå¦‚æžœæˆ‘ä»¬å°†åé‡åŒ–åŽçš„å€¼å†æ¬¡æ”¾å…¥çŸ©é˜µå½¢å¼ï¼ˆå·¦ä¾§çŸ©é˜µï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è®¡ç®—åŽŸå§‹çŸ©é˜µä¸Žå…¶åé‡åŒ–ç‰ˆæœ¬ä¹‹é—´é€ç‚¹å·®å¼‚æ¥è®¡ç®—é‡åŒ–è¯¯å·®ï¼ˆå³ä¾§çŸ©é˜µï¼‰ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*56NALu9PAN95QG2hn8HXoQ.png)\n\næ­£å¦‚æˆ‘ä»¬æ‰€è§‚å¯Ÿåˆ°çš„ï¼Œé‡åŒ–è¯¯å·®å¼€å§‹åœ¨æŸäº›çŸ©é˜µå€¼ä¸­æ˜¾çŽ°ã€‚\n\n## #5. æƒé‡é‡åŒ–ä¸Žæ¿€æ´»é‡åŒ–\n\nåœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä¸»è¦å…³æ³¨äºŽé‡åŒ–æ¨¡åž‹çš„æƒé‡ã€‚è™½ç„¶æƒé‡é‡åŒ–å¯¹äºŽæ¨¡åž‹ä¼˜åŒ–è‡³å…³é‡è¦ï¼Œä½†è€ƒè™‘åˆ°æ¿€æ´»ä¹Ÿå¯ä»¥è¿›è¡Œé‡åŒ–åŒæ ·é‡è¦ã€‚\n\n**æ¿€æ´»é‡åŒ–æ¶‰åŠå‡å°‘ç½‘ç»œä¸­æ¯å±‚çš„ä¸­é—´è¾“å‡ºçš„ç²¾åº¦**ã€‚ä¸Žæƒé‡åœ¨æ¨¡åž‹è®­ç»ƒåŽä¿æŒä¸å˜ä¸åŒï¼Œæ¿€æ´»æ˜¯åŠ¨æ€çš„ï¼Œå¹¶ä¸”éšç€æ¯ä¸ªè¾“å…¥è€Œå˜åŒ–ï¼Œä½¿å…¶èŒƒå›´æ›´éš¾é¢„æµ‹ã€‚\n\nä¸€èˆ¬è€Œè¨€ï¼Œæ¿€æ´»é‡åŒ–æ¯”æƒé‡é‡åŒ–æ›´å…·æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒéœ€è¦ä»”ç»†æ ¡å‡†ä»¥ç¡®ä¿å‡†ç¡®æ•æ‰æ¿€æ´»çš„åŠ¨æ€èŒƒå›´ã€‚\n\næƒé‡é‡åŒ–å’Œæ¿€æ´»é‡åŒ–æ˜¯äº’è¡¥çš„æŠ€æœ¯ã€‚ä¸¤è€…ç»“åˆä½¿ç”¨å¯ä»¥æ˜¾è‘—å‡å°‘æ¨¡åž‹å¤§å°ï¼Œè€Œä¸ä¼šå¤§å¹…å½±å“æ€§èƒ½ã€‚\n\n## æœ€åŽçš„æ€è€ƒ\n\nåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å›žé¡¾äº†å…³äºŽé‡åŒ–çš„5ä¸ªå…³é”®ç‚¹ï¼Œä»¥æ›´å¥½åœ°ç†è§£å¦‚ä½•å‡å°è¿™äº›ä¸æ–­å¢žé•¿çš„æ¨¡åž‹çš„å¤§å°ã€‚\n\nè‡³äºŽè¿™äº›æŠ€æœ¯çš„å®žçŽ°ï¼ŒPythonä¸­æœ‰å‡ ä¸ªæ”¯æŒé‡åŒ–çš„å·¥å…·å’Œåº“ï¼Œä¾‹å¦‚`pytorch`å’Œ`tensorflow`ã€‚ç„¶è€Œï¼Œåœ¨çŽ°æœ‰æ¨¡åž‹ä¸­æ— ç¼é›†æˆé‡åŒ–éœ€è¦å¯¹åº“å’Œæ¨¡åž‹å†…éƒ¨ç»“æž„æœ‰æ·±å…¥çš„ç†è§£ã€‚\n\nè¿™å°±æ˜¯ä¸ºä»€ä¹ˆåˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘æœ€å–œæ¬¢çš„ç®€å•æ­¥éª¤å®žçŽ°é‡åŒ–çš„é€‰é¡¹æ˜¯Hugging Faceçš„[Quanto](https://huggingface.co/blog/quanto-introduction)åº“ï¼Œæ—¨åœ¨ç®€åŒ–PyTorchæ¨¡åž‹çš„é‡åŒ–è¿‡ç¨‹ã€‚\n\nå¦‚æžœä½ å¯¹LLMé‡åŒ–çš„æ·±å…¥å†…å®¹ä»¥åŠå¦‚ä½•ä½¿ç”¨ä¸Šè¿°åº“æ„Ÿå…´è¶£ï¼Œä½ å¯èƒ½è¿˜ä¼šå¯¹æ–‡ç« [â€œå¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰çš„é‡åŒ–ï¼šæœ‰æ•ˆå‡å°‘AIæ¨¡åž‹å¤§å°â€](https://www.datacamp.com/tutorial/quantization-for-large-language-models)æ„Ÿå…´è¶£ã€‚\n\nå°±è¿™äº›ï¼éžå¸¸æ„Ÿè°¢ä½ çš„é˜…è¯»ï¼\n\næˆ‘å¸Œæœ›è¿™ç¯‡æ–‡ç« èƒ½åœ¨**ä½¿ç”¨LLMsè¿›è¡Œç¼–ç æ—¶**å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼\n\nä½ ä¹Ÿå¯ä»¥è®¢é˜…æˆ‘çš„[**æ—¶äº‹é€šè®¯**](https://readmedium.com/@andvalenzuela/subscribe)ï¼Œä»¥ä¾¿åŠæ—¶èŽ·å–æ–°å†…å®¹ã€‚\n\n**ç‰¹åˆ«æ˜¯**ï¼Œ**å¦‚æžœä½ å¯¹æœ‰å…³å¤§åž‹è¯­è¨€æ¨¡åž‹å’ŒChatGPTçš„æ–‡ç« æ„Ÿå…´è¶£**ï¼š\n\n"},{"lang":"zh","group":"blog","slug":"blog/langgraph-vs-langchain-vs-langflow-vs-langsmith-which-one-to-use-why-69ee91e91000","frontmatter":{"title":"LangGraphã€LangChainã€LangFlowã€LangSmithï¼šä½¿ç”¨å“ªä¸€ä¸ªä»¥åŠä¸ºä»€ä¹ˆï¼Ÿ","meta_title":"LangGraphã€LangChainã€LangFlowã€LangSmithï¼šä½¿ç”¨å“ªä¸€ä¸ªä»¥åŠä¸ºä»€ä¹ˆï¼Ÿ","description":"äº†è§£ LangGraphã€LangChainã€LangFlow å’Œ LangSmith ä¹‹é—´çš„ä¸»è¦åŒºåˆ«ï¼Œå¹¶äº†è§£å“ªç§æ¡†æž¶æœ€é€‚åˆæ‚¨çš„â€¦â€¦","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*xrWv1QVt4zE5cxjA8VA3ag.png","categories":["Programming","Technology","Technology/Web"],"author":"Rifx.Online","tags":["LangGraph","LangChain","LangFlow","LangSmith","frameworks"],"draft":false,"slug":"blog/langgraph-vs-langchain-vs-langflow-vs-langsmith-which-one-to-use-why-69ee91e91000"},"content":"\n\n\n### æŽ¢ç´¢ LangGraphã€LangChainã€LangFlow å’Œ LangSmith ä¹‹é—´çš„å…³é”®åŒºåˆ«ï¼Œäº†è§£å“ªç§æ¡†æž¶æœ€é€‚åˆæ‚¨çš„è¯­è¨€æ¨¡åž‹åº”ç”¨â€”â€”ä»Žå·¥ä½œæµæž„å»ºåˆ°æ€§èƒ½ç›‘æŽ§ã€‚\n\nðŸ‘¨ðŸ¾â€ðŸ’» [GitHub](https://github.com/mdmonsurali) â­ï¸ | ðŸ‘”[LinkedIn](https://www.linkedin.com/in/mdmonsurali/) |ðŸ“ [Medium](https://medium.com/@monsuralirana)\n\n\n\nè¿‘å¹´æ¥ï¼Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸè§è¯äº†å¯ç”¨äºŽæž„å»ºåŸºäºŽè¯­è¨€æ¨¡åž‹çš„åº”ç”¨ç¨‹åºçš„æ¡†æž¶ã€åº“å’Œå·¥å…·æ•°é‡çš„æ¿€å¢žã€‚åœ¨è¿™äº›å·¥å…·ä¸­ï¼Œ**LangGraph**ã€**LangChain**ã€**LangFlow** å’Œ **LangSmith** å·²æˆä¸ºé¢†å…ˆçš„é€‰æ‹©ï¼Œå„è‡ªæ»¡è¶³ä¸åŒçš„ç”¨ä¾‹å’Œç”¨æˆ·éœ€æ±‚ã€‚å¦‚æžœæ‚¨å¸Œæœ›æž„å»ºã€ç›‘æŽ§æˆ–æ‰©å±•è¯­è¨€æ¨¡åž‹å·¥ä½œæµï¼Œäº†è§£è¿™äº›å·¥å…·çš„ä¼˜åŠ¿å’Œç›®çš„è‡³å…³é‡è¦ã€‚\n\nåœ¨æœ¬åšå®¢ä¸­ï¼Œæˆ‘ä»¬å°†æŽ¢è®¨æ¯ä¸ªæ¡†æž¶ï¼Œåˆ†æžå®ƒä»¬çš„ä¼˜åŠ¿ï¼Œå¹¶æä¾›ä½•æ—¶ä½¿ç”¨å®ƒä»¬çš„è§è§£ã€‚æ— è®ºæ‚¨æ˜¯ç»éªŒä¸°å¯Œçš„å¼€å‘è€…è¿˜æ˜¯è¯¥é¢†åŸŸçš„æ–°æ‰‹ï¼Œç†è§£è¿™äº›å·¥å…·çš„ç»†å¾®å·®åˆ«å°†å¸®åŠ©æ‚¨ä¸ºæ‚¨çš„é¡¹ç›®é€‰æ‹©åˆé€‚çš„å·¥å…·ã€‚\n\n## è¯­è¨€æ¨¡åž‹æ¡†æž¶ç®€ä»‹\n\néšç€å¼ºå¤§çš„è¯­è¨€æ¨¡åž‹å¦‚ GPT-3ã€GPT-4 ä»¥åŠå…¶ä»–åŸºäºŽå˜æ¢å™¨çš„æ¨¡åž‹çš„å´›èµ·ï¼Œè¶Šæ¥è¶Šéœ€è¦èƒ½å¤Ÿç®€åŒ–è¯­è¨€åº”ç”¨ç¨‹åºåˆ›å»ºå’Œç®¡ç†çš„æ¡†æž¶ã€‚è¿™äº›æ¡†æž¶ç®€åŒ–äº†å¤æ‚çš„ä»»åŠ¡ï¼Œå¦‚ **é“¾æŽ¥å¤šä¸ªæç¤º**ã€**æ£€ç´¢ç›¸å…³æ–‡æ¡£**ï¼Œç”šè‡³ **ç›‘æŽ§æ¨¡åž‹æ€§èƒ½**ã€‚\n\nç„¶è€Œï¼Œå¹¶éžæ‰€æœ‰æ¡†æž¶éƒ½æ˜¯ç›¸åŒçš„ã€‚æœ‰äº›æ¡†æž¶æä¾› **å¯è§†åŒ–ç•Œé¢** æ¥ç®¡ç†å·¥ä½œæµç¨‹ï¼Œè€Œå…¶ä»–æ¡†æž¶åˆ™æä¾›é«˜çº§çš„ **è°ƒè¯•å’Œå¯è§‚å¯Ÿæ€§** åŠŸèƒ½ã€‚è®©æˆ‘ä»¬æ·±å…¥äº†è§£è¿™äº›å·¥å…·ï¼Œä»¥ç†è§£å®ƒä»¬ç‹¬ç‰¹çš„åŠŸèƒ½ã€‚\n\n## 1. LangGraphï¼šå¯è§†åŒ–å¤æ‚å·¥ä½œæµ\n\n**LangGraph** æ˜¯ä¸€ä¸ªä¸ºå¼€å‘è€…è®¾è®¡çš„æ–°æ¡†æž¶ï¼Œé€‚åˆé‚£äº›åå¥½ **å¯è§†åŒ–æ–¹æ³•** æ¥æž„å»ºè¯­è¨€æ¨¡åž‹ç®¡é“çš„ç”¨æˆ·ã€‚å®ƒå…è®¸æ‚¨é€šè¿‡ **åŸºäºŽå›¾çš„å¯è§†åŒ–** æ¥æž„å»ºå¤æ‚çš„å·¥ä½œæµï¼Œä»Žè€Œæ›´å®¹æ˜“ç†è§£ä¸åŒä»»åŠ¡å’Œç»„ä»¶ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚è¿™å¯¹äºŽå¤šä¸ªæ­¥éª¤ï¼ˆå¦‚æ–‡æœ¬ç”Ÿæˆã€æ–‡æ¡£æ£€ç´¢å’Œåˆ†ç±»ï¼‰ä¸²è”åœ¨ä¸€èµ·çš„å¤§åž‹åº”ç”¨å°¤å…¶æœ‰ç”¨ã€‚\n\n### ä¼˜åŠ¿ï¼š\n\n* **å¯è§†åŒ–å·¥ä½œæµè¡¨ç¤º**ï¼šLangGraph å…è®¸æ‚¨å¯è§†åŒ–ä¸åŒç»„ä»¶ä¹‹é—´çš„æ•°æ®å’Œæ“ä½œæµã€‚è¿™ç§å›¾å½¢åŒ–çš„æ–¹æ³•ç›´è§‚ä¸”æœ‰åŠ©äºŽè®¾è®¡æ›´å¤æ‚çš„ç®¡é“ã€‚\n* **è°ƒè¯•ç®€å•**ï¼šLangGraph çš„å¯è§†åŒ–ç‰¹æ€§ä½¿å¾—è¯†åˆ«å·¥ä½œæµä¸­çš„ç“¶é¢ˆæˆ–é—®é¢˜èŠ‚ç‚¹å˜å¾—æ›´åŠ å®¹æ˜“ã€‚\n\n### ç¤ºä¾‹ç”¨ä¾‹ï¼š\n\nå‡è®¾æ‚¨æ­£åœ¨æž„å»ºä¸€ä¸ªè‡ªåŠ¨åŒ–ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿé¦–å…ˆä½¿ç”¨è¯­è¨€æ¨¡åž‹æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼Œç„¶åŽå°†å…¶ä¼ é€’ç»™æ‘˜è¦ç”Ÿæˆå™¨ã€‚åœ¨ LangGraph ä¸­ï¼Œæ‚¨å¯ä»¥ç›´è§‚åœ°ç»˜åˆ¶å‡ºæ­¤å·¥ä½œæµç¨‹ï¼Œå±•ç¤ºæ¯ä¸ªæ­¥éª¤ä¹‹é—´çš„å…³ç³»ã€‚å¦‚æžœé“¾ä¸­çš„ä»»ä½•ä¸€ç‚¹å‡ºçŽ°é—®é¢˜ï¼Œè§†è§‰å·¥å…·ä½¿æ‚¨èƒ½å¤Ÿè½»æ¾å®šä½é—®é¢˜æ‰€åœ¨ã€‚\n\n### ä½•æ—¶ä½¿ç”¨ LangGraphï¼š\n\nå¦‚æžœæ‚¨æ­£åœ¨ç®¡ç† **å¤æ‚çš„å·¥ä½œæµç¨‹**ï¼Œå¹¶ä¸”é‡è§† **å›¾å½¢ç•Œé¢** æ¥ç†è§£æ‚¨çš„ç®¡é“ï¼ŒLangGraph æ˜¯ä¸€ä¸ªç»ä½³çš„é€‰æ‹©ã€‚å®ƒç‰¹åˆ«é€‚åˆé‚£äº›æ›´å–œæ¬¢ç›´è§‚çš„æ‹–æ”¾å¼å·¥ä½œæµç¨‹è®¾è®¡çš„å¼€å‘äººå‘˜æˆ–æ•°æ®ç§‘å­¦å®¶ã€‚\n\n**å…³é”®ç‚¹**ï¼š\n\n* å¦‚æžœæ‚¨éœ€è¦æ¸…æ™°çš„è¯­è¨€å¤„ç†å·¥ä½œæµç¨‹çš„å¯è§†åŒ–è¡¨ç¤ºã€‚\n* åœ¨åˆ›å»ºéœ€è¦åˆ†æ”¯æˆ–å¤šè·¯å¾„ä¾èµ–çš„æ›´å¤æ‚çš„ç®¡é“æ—¶ã€‚\n\n## 2. LangChainï¼šLLM åº”ç”¨çš„å·¥ä½œé©¬\n\n**LangChain** æ˜¯æž„å»ºç”± **å¤§åž‹è¯­è¨€æ¨¡åž‹ (LLMs)** é©±åŠ¨çš„åº”ç”¨ç¨‹åºæœ€å—æ¬¢è¿Žçš„æ¡†æž¶ä¹‹ä¸€ã€‚å®ƒæä¾›äº†ä¸€ç§çµæ´»çš„ **ä»£ç ä¼˜å…ˆæ–¹æ³•**ï¼Œå…è®¸å¼€å‘è€…å°†æ–‡æ¡£æ£€ç´¢ã€æ‘˜è¦å’Œé—®ç­”ç­‰ä»»åŠ¡ä¸²è”æˆç»Ÿä¸€çš„å·¥ä½œæµç¨‹ã€‚\n\n### ä¼˜åŠ¿ï¼š\n\n* **å¹¿æ³›æ”¯æŒLLMs**ï¼šLangChainå…¼å®¹å¤šç§è¯­è¨€æ¨¡åž‹ï¼Œä½¿å¾—é›†æˆOpenAIçš„GPTæˆ–æœ¬åœ°æ‰˜ç®¡æ¨¡åž‹å˜å¾—ç®€å•ã€‚\n* **é“¾å¼èƒ½åŠ›**ï¼šLangChainæ“…é•¿äºŽ**å¤šä¸ªæ“ä½œçš„é“¾å¼å¤„ç†**â€”â€”å› æ­¤å¾—åâ€”â€”ä½¿å¼€å‘è€…èƒ½å¤Ÿåˆ›å»ºå¤æ‚çš„NLPåº”ç”¨ã€‚\n* **å¹¿æ³›é‡‡ç”¨**ï¼šä½œä¸ºæœ€å—æ¬¢è¿Žçš„æ¡†æž¶ä¹‹ä¸€ï¼ŒLangChainæ‹¥æœ‰ä¸€ä¸ª**è“¬å‹ƒå‘å±•çš„ç¤¾åŒº**å’Œå‡ºè‰²çš„æ”¯æŒï¼Œæä¾›ä¸°å¯Œçš„æ–‡æ¡£å’Œæ•™ç¨‹ã€‚\n\n### ç¤ºä¾‹ç”¨ä¾‹ï¼š\n\næƒ³è±¡ä¸€ä¸‹ï¼Œæ‚¨æ­£åœ¨æž„å»ºä¸€ä¸ª **èŠå¤©æœºå™¨äºº**ï¼Œå®ƒé¦–å…ˆç†è§£ç”¨æˆ·çš„é—®é¢˜ï¼Œä»Žæ•°æ®åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼Œç„¶åŽç”Ÿæˆå“åº”ã€‚ä½¿ç”¨ LangChainï¼Œæ‚¨å¯ä»¥è½»æ¾åœ°ä»¥ç¼–ç¨‹æ–¹å¼åˆ›å»ºè¿™ä¸ªå¤šæ­¥éª¤çš„è¿‡ç¨‹ï¼Œç¡®ä¿é“¾ä¸­çš„æ¯ä¸€æ­¥åè°ƒå·¥ä½œã€‚\n\n### ä½•æ—¶ä½¿ç”¨ LangChainï¼š\n\nå¦‚æžœæ‚¨æ˜¯ä¸€ä¸ª **æž„å»ºç”Ÿäº§çº§åº”ç”¨çš„å¼€å‘è€…**ï¼Œå¹¶ä¸”éœ€è¦ä¸€ä¸ª **çµæ´»ã€ä»¥ä»£ç ä¸ºä¸­å¿ƒçš„è§£å†³æ–¹æ¡ˆ**ï¼ŒLangChain æ˜¯æ‚¨çš„æœ€ä½³é€‰æ‹©ã€‚å®ƒéžå¸¸é€‚åˆé‚£äº›å¸Œæœ›æŽ§åˆ¶åº”ç”¨æž¶æž„å¹¶ä¸”èƒ½èˆ’é€‚åœ°ç¼–å†™ä»£ç æ¥å®šä¹‰å·¥ä½œæµç¨‹çš„å¼€å‘è€…ã€‚\n\n**å…³é”®ç‚¹**ï¼š\n\n* å¦‚æžœæ‚¨æ­£åœ¨æž„å»ºéœ€è¦è·¨å¤šä¸ªè¯­è¨€æ¨¡åž‹é“¾å¼ä»»åŠ¡çš„ç”Ÿäº§çº§åº”ç”¨ã€‚\n* å¦‚æžœæ‚¨éœ€è¦ä¸€ä¸ªæ‹¥æœ‰å¹¿æ³›ç¤¾åŒºæ”¯æŒå’Œå¤šç§é›†æˆçš„åº“ã€‚\n* å½“æ‚¨å¯¹ç¼–ç¨‹è§£å†³æ–¹æ¡ˆæ›´ä¸ºç†Ÿæ‚‰ï¼Œè€Œéžå¯è§†åŒ–å·¥å…·ã€‚\n\n## 3. LangFlow: æ— éœ€ç¼–ç /ä½Žä»£ç çš„ LangChain æ‰©å±•\n\n**LangFlow** æœ¬è´¨ä¸Šæ˜¯ **LangChain çš„å¯è§†åŒ–æ‰©å±•**ã€‚å®ƒå°† LangChain å¼ºå¤§çš„åŽç«¯ä¸Ž **ç›´è§‚çš„æ‹–æ”¾ç•Œé¢** ç»“åˆåœ¨ä¸€èµ·ã€‚LangFlow ä½¿é‚£äº›å¯èƒ½ä¸å¤ªæ“…é•¿ç¼–å†™ä»£ç çš„ç”¨æˆ·ä»ç„¶èƒ½å¤Ÿåœ¨ä»–ä»¬çš„åº”ç”¨ç¨‹åºä¸­åˆ©ç”¨è¯­è¨€æ¨¡åž‹çš„å¼ºå¤§åŠŸèƒ½ã€‚\n\n### ä¼˜åŠ¿ï¼š\n\n* **å¯è§†åŒ–å·¥ä½œæµåˆ›å»º**ï¼šä¸Ž LangGraph ç±»ä¼¼ï¼ŒLangFlow æä¾›äº†ä¸€ä¸ªå¯è§†åŒ–ç•Œé¢ç”¨äºŽæž„å»ºå·¥ä½œæµã€‚ç„¶è€Œï¼Œå®ƒæ˜¯åŸºäºŽ LangChain æž„å»ºçš„ï¼Œè¿™æ„å‘³ç€ç”¨æˆ·å¯ä»¥åˆ©ç”¨ LangChain çš„å¼ºå¤§åŠŸèƒ½ï¼Œè€Œæ— éœ€ç¼–å†™å¤§é‡ä»£ç ã€‚\n* **å¿«é€ŸåŽŸåž‹åˆ¶ä½œçš„ç†æƒ³é€‰æ‹©**ï¼šLangFlow éžå¸¸é€‚åˆå¿«é€Ÿ **åŽŸåž‹åŒ–æƒ³æ³•** æˆ–æž„å»ºæ¦‚å¿µéªŒè¯åº”ç”¨ç¨‹åºã€‚\n* **é€‚åˆåˆå­¦è€…**ï¼šå®ƒæ˜¯ä¸€ä¸ªå¾ˆå¥½çš„å…¥é—¨ç‚¹ï¼Œé€‚åˆé‚£äº›å¯¹ç¼–ç ä¸å¤ªç†Ÿæ‚‰ä½†æƒ³è¦åˆ›å»ºè¯­è¨€æ¨¡åž‹å·¥ä½œæµçš„ç”¨æˆ·ã€‚\n\n### ç¤ºä¾‹ç”¨ä¾‹ï¼š\n\nå¦‚æžœæ‚¨æƒ³å¿«é€Ÿæž„å»ºä¸€ä¸ª**æ‘˜è¦å·¥å…·**æ¥æ£€ç´¢æ–‡æ¡£ï¼Œæ‚¨å¯ä»¥åœ¨LangFlowçš„ç•Œé¢ä¸­æ‹–æ”¾ç»„ä»¶ï¼Œä»¥åˆ›å»ºä¸€ä¸ªå®Œå…¨åŠŸèƒ½çš„åº”ç”¨ç¨‹åºã€‚è¿™å¯ä»¥åœ¨å‡ ä¹Žä¸ç¼–å†™ä»£ç çš„æƒ…å†µä¸‹å®Œæˆã€‚\n\n### ä½•æ—¶ä½¿ç”¨ LangFlowï¼š\n\nLangFlow éžå¸¸é€‚åˆ **éžå¼€å‘äººå‘˜** æˆ– **å¿«é€ŸåŽŸåž‹è®¾è®¡**ã€‚å¦‚æžœæ‚¨æƒ³å¿«é€Ÿå®žéªŒ **LLM å·¥ä½œæµ** è€Œä¸æ·±å…¥ä»£ç ï¼Œè¿™ä¸ªå·¥å…·å¯ä»¥è®©æ‚¨è½»æ¾å…¥é—¨ã€‚\n\n**å…³é”®ç‚¹**ï¼š\n\n* å¦‚æžœæ‚¨æƒ³å¿«é€ŸåŽŸåž‹è®¾è®¡ LLM å·¥ä½œæµè€Œä¸ç¼–å†™ä»£ç ã€‚\n* å¦‚æžœæ‚¨å¯¹è§†è§‰ç¼–ç¨‹æ„Ÿåˆ°èˆ’é€‚ï¼Œä½†éœ€è¦ LangChain çš„çµæ´»æ€§ã€‚\n* ç”¨äºŽæ•™è‚²ç›®çš„ï¼Œå¸®åŠ©ç”¨æˆ·äº†è§£å¦‚ä½•æž„å»ºå·¥ä½œæµã€‚\n\n## 4. LangSmith: ç›‘æŽ§ä¸Žå¯è§‚å¯Ÿæ€§\n\nè™½ç„¶å…¶ä»–å·¥å…·ä¸“æ³¨äºŽ **æž„å»ºå·¥ä½œæµç¨‹**ï¼Œ**LangSmith** çš„è®¾è®¡ç›®æ ‡æ˜¯ **ç›‘æŽ§** å’Œ **è°ƒè¯•** è¯­è¨€æ¨¡åž‹åº”ç”¨ã€‚å®ƒæä¾›äº†å…ˆè¿›çš„å¯è§‚å¯Ÿæ€§åŠŸèƒ½ï¼Œä»¥è·Ÿè¸ªæ‚¨çš„å·¥ä½œæµç¨‹å’Œæ¨¡åž‹çš„æ€§èƒ½ï¼Œä½¿å…¶åœ¨ç”Ÿäº§çŽ¯å¢ƒä¸­ä¸å¯æˆ–ç¼ºã€‚\n\n### ä¼˜åŠ¿ï¼š\n\n* **æ·±åº¦å¯è§‚å¯Ÿæ€§**ï¼šLangSmith å…è®¸å¼€å‘è€…ç›‘æŽ§è¯­è¨€æ¨¡åž‹çš„æ€§èƒ½ï¼Œç¡®ä¿å·¥ä½œæµç¨‹æŒ‰é¢„æœŸè¿è¡Œã€‚\n* **é”™è¯¯è·Ÿè¸ª**ï¼šå®ƒåœ¨å¸®åŠ©å¼€å‘è€…å®šä½é—®é¢˜æ–¹é¢è¡¨çŽ°å‡ºè‰²ï¼Œä½¿è°ƒè¯•å˜å¾—æ›´åŠ å®¹æ˜“ã€‚\n* **æ€§èƒ½æ´žå¯Ÿ**ï¼šLangSmith æä¾›æœ‰å…³ **å·¥ä½œæµç¨‹æ€§èƒ½** çš„æ´žå¯Ÿï¼Œå¸®åŠ©å¼€å‘è€…ä¼˜åŒ–ä»–ä»¬çš„åº”ç”¨ç¨‹åºã€‚\n\n### ç¤ºä¾‹ç”¨ä¾‹ï¼š\n\nå‡è®¾æ‚¨å·²ç»éƒ¨ç½²äº†ä¸€ä¸ª**å®¢æˆ·æœåŠ¡èŠå¤©æœºå™¨äºº**ï¼Œè¯¥èŠå¤©æœºå™¨äººä½¿ç”¨è¯­è¨€æ¨¡åž‹æ¥å›žç­”é—®é¢˜ã€‚éšç€æ—¶é—´çš„æŽ¨ç§»ï¼Œæ‚¨ä¼šå‘çŽ°æŸäº›å›žç­”çš„å‡†ç¡®æ€§ä½ŽäºŽé¢„æœŸã€‚LangSmith å¯ä»¥å¸®åŠ©æ‚¨è¿½è¸ªé—®é¢˜ï¼Œé€šè¿‡æä¾›å¯¹å·¥ä½œæµç¨‹ä¸­æ¯ä¸ªå†³ç­–ç‚¹çš„å¯è§æ€§ã€‚\n\n### ä½•æ—¶ä½¿ç”¨ LangSmithï¼š\n\nå¦‚æžœæ‚¨åœ¨ **ç”Ÿäº§çŽ¯å¢ƒ** ä¸­éƒ¨ç½²åº”ç”¨ç¨‹åºï¼Œå¹¶ä¸”éœ€è¦ç¡®ä¿ **å¥å£®æ€§ã€å¯é æ€§å’Œæ€§èƒ½**ï¼ŒLangSmith æ˜¯ä¸€ä¸ªä¸å¯æˆ–ç¼ºçš„å·¥å…·ã€‚å®ƒåœ¨ç®¡ç† **éœ€è¦éšç€æ—¶é—´è°ƒè¯•å’Œä¼˜åŒ–çš„å¤æ‚ç³»ç»Ÿ** æ—¶ç‰¹åˆ«æœ‰ç”¨ã€‚\n\n**å…³é”®ç‚¹**ï¼š\n\n* å¦‚æžœæ‚¨éœ€è¦ LLM å·¥ä½œæµä¸­çš„é«˜çº§ç›‘æŽ§æˆ–è°ƒè¯•èƒ½åŠ›ã€‚\n* å¯¹äºŽè§‚å¯Ÿæ€§å¯¹ç¡®ä¿æœ€ä½³æ¨¡åž‹æ€§èƒ½è‡³å…³é‡è¦çš„å¼€å‘çŽ¯å¢ƒã€‚\n* å¦‚æžœæ‚¨çš„é‡ç‚¹æ˜¯åŸºäºŽå®žæ—¶æ´žå¯Ÿæ”¹è¿›å’Œè¿­ä»£ LLM é©±åŠ¨çš„åº”ç”¨ç¨‹åºã€‚\n\n## å“ªä¸ªæ›´é€‚åˆä½ ï¼Ÿ\n\n* **ä½¿ç”¨ LangGraph** å¦‚æžœä½ æ›´å–œæ¬¢åŸºäºŽå›¾å½¢çš„å¯è§†åŒ–å·¥ä½œæµç¨‹æ¥æž„å»ºå¤æ‚çš„ LLM ä»»åŠ¡ã€‚éžå¸¸é€‚åˆéœ€è¦æ¸…æ™°å’Œç»“æž„çš„ç”¨æˆ·ã€‚\n* **ä½¿ç”¨ LangChain** å¦‚æžœä½ éœ€è¦ä¸€ä¸ªå¼ºå¤§ã€çµæ´»çš„è§£å†³æ–¹æ¡ˆæ¥ä»¥ç¼–ç¨‹æ–¹å¼åˆ›å»ºè¯­è¨€æ¨¡åž‹åº”ç”¨ã€‚å®ƒå¤šåŠŸèƒ½ä¸”éžå¸¸é€‚åˆæž„å»ºç”Ÿäº§çº§åº”ç”¨çš„å¼€å‘è€…ã€‚\n* **ä½¿ç”¨ LangFlow** å¦‚æžœä½ æƒ³è¦ LangChain çš„å¼ºå¤§åŠŸèƒ½ï¼ŒåŒæ—¶åˆå¸Œæœ›æ‹¥æœ‰ä¸€ä¸ªå¯è§†åŒ–çš„æ— ä»£ç /ä½Žä»£ç ç•Œé¢ã€‚æœ€é€‚åˆå¿«é€ŸåŽŸåž‹å¼€å‘å’Œæ›´å–œæ¬¢å¯è§†åŒ–å·¥å…·è€Œéžç¼–ç çš„ç”¨æˆ·ã€‚\n* **ä½¿ç”¨ LangSmith** å¦‚æžœä½ çš„é‡ç‚¹æ˜¯ LLM åº”ç”¨çš„å¯è§‚å¯Ÿæ€§å’Œè°ƒè¯•ã€‚éžå¸¸é€‚åˆåœ¨å¼€å‘æˆ–ç”Ÿäº§çŽ¯å¢ƒä¸­ç›‘æŽ§å’Œä¼˜åŒ–å·¥ä½œæµç¨‹ã€‚\n\næœ€ç»ˆï¼Œä½ çš„é€‰æ‹©å–å†³äºŽä½ å¯¹ä»£ç çš„èˆ’é€‚åº¦ã€å·¥ä½œæµç¨‹çš„å¤æ‚æ€§ï¼Œä»¥åŠä½ æ˜¯å¦ä¼˜å…ˆè€ƒè™‘æ˜“ç”¨æ€§ã€çµæ´»æ€§æˆ–å¯è§‚å¯Ÿæ€§ã€‚\n\n## ç»“è®º\n\nè¿™äº›å·¥å…· â€” **LangGraph**ã€**LangChain**ã€**LangFlow** å’Œ **LangSmith** â€” é’ˆå¯¹å¼€å‘å’Œç®¡ç†è¯­è¨€æ¨¡åž‹åº”ç”¨çš„ä¸åŒé˜¶æ®µã€‚**LangGraph** æä¾›äº†ä¸€ç§å¯è§†åŒ–ã€ç›´è§‚çš„æ–¹å¼æ¥æž„å»ºå¤æ‚çš„å·¥ä½œæµç¨‹ï¼Œè€Œ **LangChain** åˆ™ä¸ºå¸Œæœ›åˆ›å»ºå¯æ‰©å±•åº”ç”¨çš„å¼€å‘è€…æä¾›äº†ä¸€ç§å¼ºå¤§çš„ä»£ç ä¼˜å…ˆè§£å†³æ–¹æ¡ˆã€‚å¯¹äºŽé‚£äº›æ›´å–œæ¬¢ **ä½Žä»£ç **ã€æ‹–æ”¾æ–¹å¼çš„ç”¨æˆ·ï¼Œ**LangFlow** åœ¨ä¸ç‰ºç‰²åŠŸèƒ½çš„æƒ…å†µä¸‹ç®€åŒ–äº†æµç¨‹ã€‚æœ€åŽï¼Œ**LangSmith** ä¸“æ³¨äºŽå¯è§‚å¯Ÿæ€§å’Œè°ƒè¯•ï¼Œç¡®ä¿æ‚¨çš„å·¥ä½œæµç¨‹æ˜¯ä¼˜åŒ–å’Œå¯é çš„ã€‚é€‰æ‹©åˆé€‚çš„å·¥å…·å–å†³äºŽæ‚¨çš„é¡¹ç›®éœ€æ±‚ï¼Œæ— è®ºæ˜¯å¿«é€ŸåŽŸåž‹è®¾è®¡ã€ç”Ÿäº§çº§æ‰©å±•ï¼Œè¿˜æ˜¯ç›‘æŽ§å’Œæ€§èƒ½è·Ÿè¸ªã€‚\n\nå¿«ä¹ç¼–ç ï¼ ðŸŽ‰\n\nðŸ‘¨ðŸ¾â€ðŸ’» [GitHub](https://github.com/mdmonsurali) â­ï¸ | ðŸ‘”[LinkedIn](https://www.linkedin.com/in/mdmonsurali/) |ðŸ“ [Medium](https://medium.com/@monsuralirana)\n\næ„Ÿè°¢æ‚¨èŠ±æ—¶é—´é˜…è¯»è¿™ç¯‡æ–‡ç« ï¼\n\nè¯·åŠ¡å¿…ç•™ä¸‹æ‚¨çš„åé¦ˆå’Œè¯„è®ºã€‚ä¸‹æ¬¡åšå®¢è§ï¼Œæ•¬è¯·å…³æ³¨ ðŸ“¢\n\n## å‚è€ƒæ–‡çŒ®ï¼š\n\n1. â€œLangChain æ–‡æ¡£â€ â€” <https://python.langchain.com/docs/introduction/>\n2. â€œLangGraph æ¦‚è¿°â€ â€” <https://langchain-ai.github.io/langgraph/>\n3. â€œLangFlow GitHub ä»“åº“â€ â€” [https://github.com/LangFlow/LangFlow](https://docs.langflow.org/)\n4. â€œLangSmith ä»‹ç»â€ â€” <https://www.langchain.com/langsmith>\n5. â€œå¦‚ä½•ä½¿ç”¨ LangChain æž„å»ºèŠå¤©æœºå™¨äººâ€ by JetBrains åšå®¢ â€” <https://blog.jetbrains.com/pycharm/2024/08/how-to-build-chatbots-with-langchain/>\n\n"},{"lang":"zh","group":"blog","slug":"blog/large-language-models-just-got-a-whole-lot-smaller-f93425ee59a2","frontmatter":{"title":"å¤§åž‹è¯­è¨€æ¨¡åž‹å˜å¾—æ›´å°äº†","meta_title":"å¤§åž‹è¯­è¨€æ¨¡åž‹å˜å¾—æ›´å°äº†","description":"è¿™å¯èƒ½ä¼šæ”¹å˜è½¯ä»¶åˆåˆ›ä¼ä¸šçš„æ¸¸æˆè§„åˆ™","date":"2024-11-04T12:29:02.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*1PeFyz_Dlt6jEf27Q9Y33Q.png","categories":["Programming","Technology","Machine Learning"],"author":"Rifx.Online","tags":["compression","optimization","ternary","parallelism","hardware"],"draft":false,"slug":"blog/large-language-models-just-got-a-whole-lot-smaller-f93425ee59a2"},"content":"\n### è¿™å°†å¦‚ä½•æ”¹å˜è½¯ä»¶åˆåˆ›ä¼ä¸šçš„æ¸¸æˆè§„åˆ™\n\n\n\n**æœ¬æ–‡ä¸Ž [David Meiborg](https://readmedium.com/undefined) å…±åŒæ’°å†™ã€‚**\n\n*TLDR: å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆç®€ç§° LLMsï¼‰ç›®å‰ä½“ç§¯åºžå¤§ï¼Œè¿è¡Œæˆæœ¬é«˜ï¼Œå¹¶ä¸”å…·æœ‰ [æ˜¾è‘—çš„ç¢³è¶³è¿¹](https://arxiv.org/abs/2309.14393)ã€‚ç„¶è€Œï¼Œæœ€è¿‘åœ¨æ¨¡åž‹åŽ‹ç¼©å’Œç³»ç»Ÿçº§ä¼˜åŒ–æ–¹æ³•ä¸Šçš„è¿›å±•å¯èƒ½ä¼šå¢žå¼º LLM æŽ¨ç†èƒ½åŠ›ã€‚ç‰¹åˆ«æ˜¯ä¸€ç§ä½¿ç”¨ä¸‰å…ƒç»“æž„å‚æ•°çš„æ–¹æ³•ï¼Œæœ‰æ½œåŠ›ç»•è¿‡å½“å‰æ ‡å‡†çš„æ˜‚è´µçŸ©é˜µä¹˜æ³•ã€‚è¿™å¯¹åˆ¶é€ ä¸“ç”¨èŠ¯ç‰‡çš„ç¡¬ä»¶åˆåˆ›ä¼ä¸šä»¥åŠä½¿ç”¨æˆ–å®šåˆ¶æž„å»ºè‡ªå·± LLM çš„è½¯ä»¶åˆåˆ›ä¼ä¸šéƒ½æœ‰ä»¤äººå…´å¥‹çš„å½±å“ã€‚å¸®åŠ©å®¢æˆ·éƒ¨ç½² LLM çš„åˆåˆ›ä¼ä¸šå¯èƒ½ä¹Ÿä¼šè¿Žæ¥æ›´å¤šçš„ä¸šåŠ¡ã€‚*\n\nå¦‚ä»Šçš„å¤§åž‹è¯­è¨€æ¨¡åž‹éžå¸¸åºžå¤§ã€‚çœŸçš„å¾ˆå¤§ã€‚å¦‚æžœä½ æƒ³åŠ è½½ä¸€ä¸ª LlaMa-2â€“70B æ¨¡åž‹ï¼Œä½ éœ€è¦ 140 GB çš„æ˜¾å­˜ï¼ˆè¿™å°±æ˜¯ 70 äº¿ä¸ªå‚æ•°ä¹˜ä»¥æ¯ä¸ªå‚æ•° 2 å­—èŠ‚ï¼‰ã€‚ä½œä¸ºå¯¹æ¯”ï¼Œåƒ NVIDIA RTX 3090 æˆ– 4090 è¿™æ ·çš„ GPU åªæœ‰ 24 GB çš„æ˜¾å­˜â€”â€”è¿™åªæ˜¯æ‰€éœ€çš„ä¸€å°éƒ¨åˆ†ã€‚\n\næœ‰ä¸€äº›å…³äºŽé‡åŒ–çš„ [è§£å†³æ–¹æ³•](https://towardsdatascience.com/run-llama-2-70b-on-your-gpu-with-exllamav2-588141a88598)ï¼Œä½†è¿™äº›å¾€å¾€æ¯”è¾ƒç¹çã€‚ä½ å¯èƒ½ä»ç„¶éœ€è¦è®©ä½ çš„ GPU é«˜æ¸©è¿è¡Œé•¿è¾¾ 15 å°æ—¶ï¼Œç›´åˆ°æ¨¡åž‹åŠ è½½å®Œæˆã€‚æ›´ä¸ç”¨è¯´ä½ ä»ç„¶éœ€è¦ä¸€äº›ç©ºä½™å†…å­˜ç”¨äºŽæŽ¨ç†ï¼Œæ¢å¥è¯è¯´ï¼Œå°±æ˜¯ç”¨äºŽéƒ¨ç½²æ¨¡åž‹ã€‚\n\nå› æ­¤ï¼Œä½¿ç”¨å½“å‰çš„ LLMs æˆæœ¬é«˜æ˜‚ï¼šé€šå¸¸éœ€è¦å¤šä¸ªé«˜ç«¯ GPU æ¥ä¿å­˜æ¨¡åž‹ï¼Œå¹¶ä¸”è¿˜å¿…é¡»è€ƒè™‘æŽ¨ç†æ‰€äº§ç”Ÿçš„èƒ½æºæˆæœ¬ã€‚\n\nè¿™å°±æ˜¯ä¸ºä»€ä¹ˆå¾ˆå¤šç ”ç©¶éƒ½åœ¨è‡´åŠ›äºŽåº”ç”¨æŠ€æœ¯ï¼Œä½¿ LLMs æ›´å°ï¼Œä»Žè€Œèƒ½å¤Ÿåœ¨æ›´å°çš„ç¡¬ä»¶ä¸Šä»¥æ›´ä½Žçš„æˆæœ¬è¿è¡Œã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œè¿™æ˜¯ä¸€ç§è‰°éš¾çš„æƒè¡¡ï¼Œå› ä¸ºä½¿ LLMs æ›´å°é€šå¸¸ä¼šå½±å“å®ƒä»¬çš„è´¨é‡ã€‚æ‰¾åˆ°æˆæœ¬ä¸Žæ”¶ç›Šç›¸ç­‰çš„ç‚¹å¯èƒ½æ˜¯æ£˜æ‰‹çš„ã€‚\n\nåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¦‚è¿°äº†ä¸€äº›æœ‰å‰æ™¯çš„ä¼˜åŒ–æ–¹æ³•ï¼Œè§£é‡Šäº†å¾®è½¯ç ”ç©¶äººå‘˜çš„æœ€æ–°çªç ´ï¼Œç®€è¦æ¦‚è¿°äº†â€œé«˜æ•ˆ LLMâ€é¢†åŸŸçš„åˆ›æ–°åˆåˆ›ä¼ä¸šï¼Œå¹¶æŽ¨å¯¼å‡ºä¸€äº›å¯¹åœ¨ LLM ç”Ÿæ€ç³»ç»Ÿä¸­è¿è¥çš„åˆåˆ›ä¼ä¸šçš„ä¸€èˆ¬å½±å“ã€‚\n\n## LLMå¦‚ä½•å˜å¾—æ›´åŠ èµ„æºé«˜æ•ˆ\n\nåƒå¾®è½¯ã€OpenAIã€Metaæˆ–è°·æ­Œè¿™æ ·çš„ç§‘æŠ€å·¨å¤´æ‹¥æœ‰è¶³å¤Ÿçš„èµ„æºæ¥è®­ç»ƒå°–ç«¯æ¨¡åž‹ï¼Œå³ä½¿ç›®å‰è®­ç»ƒæˆæœ¬å¯¹å¤§å¤šæ•°å…¶ä»–å…¬å¸æ¥è¯´æ˜¯ä¸å¯æ‰¿å—çš„ã€‚å› æ­¤ï¼Œå¹¿æ³›é‡‡ç”¨çš„æœ€å¤§ç“¶é¢ˆä¸æ˜¯è®­ç»ƒï¼Œè€Œæ˜¯æŽ¨ç†æ•ˆçŽ‡ã€‚æ¢å¥è¯è¯´ï¼Œå°½ç®¡Metaå·²ç»å‘å¸ƒäº†LlaMaï¼Œä½†ç”±äºŽè¿è¡Œæ¨¡åž‹â€”â€”è€Œä¸æ˜¯åˆ›å»ºæ¨¡åž‹â€”â€”å·²ç»è¶³å¤Ÿå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› æ­¤å®ƒä»æœªå¾—åˆ°è¶³å¤Ÿçš„é‡‡ç”¨ã€‚\n\nç„¶è€Œï¼Œç ”ç©¶äººå‘˜å¼€å§‹æé«˜è¿™ç§æŽ¨ç†æ•ˆçŽ‡ã€‚å¹¿ä¹‰è€Œè¨€ï¼Œæœ‰ä¸¤ç§æ–¹æ³•å¯ä»¥å®žçŽ°è¿™ä¸€ç›®æ ‡ï¼š**ç³»ç»Ÿçº§ä¼˜åŒ–**å¹¶ä¸æ”¹å˜æ¨¡åž‹æœ¬èº«ï¼Œè€Œæ˜¯é€šè¿‡æ”¹å˜æ¨¡åž‹æ‰€å¤„çŽ¯å¢ƒçš„å…³é”®æ–¹é¢æ¥æé«˜å…¶æ€§èƒ½ã€‚**æ¨¡åž‹ä¼˜åŒ–**åˆ™åŽ‹ç¼©æ¨¡åž‹ï¼Œä½¿å…¶æ›´æ˜“äºŽéƒ¨ç½²å’Œè¿è¡Œã€‚\n\nè¿™ä¸¤ç§æ–¹æ³•éƒ½æœ‰å¤šç§ä¸åŒçš„æŠ€æœ¯ã€‚[ä¸€ç¯‡æœ€è¿‘çš„è®ºæ–‡](https://arxiv.org/pdf/2402.01799.pdf)ç”±ç ”ç©¶äººå‘˜å‡ºè‰²åœ°æ€»ç»“äº†è¿™äº›æŠ€æœ¯ã€‚ç”±äºŽè¿™äº›æŠ€æœ¯å¯èƒ½å¾ˆå¿«å°±ä¼šæˆä¸ºä»»ä½•ä»Žäº‹LLMç³»ç»Ÿå·¥ä½œè€…çš„åŸºæœ¬çŸ¥è¯†ï¼Œæˆ‘ä»¬åœ¨ä¸‹é¢å¯¹è¿™äº›æŠ€æœ¯è¿›è¡Œäº†å¿«é€Ÿæ¦‚è¿°ã€‚\n\n### ç³»ç»Ÿçº§ä¼˜åŒ–\n\nç³»ç»Ÿçº§ä¼˜åŒ–æŒ‡çš„æ˜¯æ”¹å˜æ¨¡åž‹æœ¬èº«çš„è¿è¡Œæ–¹å¼ï¼Œè€Œä¸æ˜¯æ¨¡åž‹æœ¬èº«ã€‚äº‹å®žè¯æ˜Žï¼Œæœ‰å¾ˆå¤šæ‰‹æ®µå¯ä»¥é¿å…èµ„æºé—²ç½®æˆ–æ¶ˆé™¤å…¶ä»–ä½Žæ•ˆçŽ°è±¡ã€‚\n\n**åˆ†é¡µæ³¨æ„åŠ›**\n\nåƒ GPT è¿™æ ·çš„ LLM çš„æ ¸å¿ƒæ˜¯æ³¨æ„åŠ›æœºåˆ¶ã€‚è¿™ä¸ªæœºåˆ¶å…è®¸æ¨¡åž‹åœ¨ç”Ÿæˆæ¯ä¸ªè¾“å‡ºå•è¯æ—¶å…³æ³¨è¾“å…¥æ–‡æœ¬çš„ä¸åŒéƒ¨åˆ†ã€‚æƒ³è±¡ä¸€ä¸‹ï¼Œæ‚¨æ­£åœ¨é˜…è¯»ä¸€æœ¬ä¹¦ï¼Œå¹¶æ ‡è®°é‡è¦çš„å¥å­ä»¥æ›´å¥½åœ°è®°ä½æ•…äº‹ã€‚ç±»ä¼¼åœ°ï¼Œæ³¨æ„åŠ›æœºåˆ¶åœ¨åšå‡ºé¢„æµ‹æ—¶â€œçªå‡ºâ€æˆ–èµ‹äºˆæŸäº›å•è¯æˆ–çŸ­è¯­æ›´å¤šé‡è¦æ€§ã€‚\n\nè¿™ä¸ªæœºåˆ¶éžå¸¸è€—è´¹èµ„æºã€‚å®ƒè¦æ±‚æ¨¡åž‹è€ƒè™‘è¾“å…¥æ–‡æœ¬ä¸­æ‰€æœ‰å•è¯å¯¹ä¹‹é—´çš„å…³ç³»ã€‚å¯¹äºŽé•¿æ–‡æœ¬ï¼Œè¿™å¯èƒ½éœ€è¦å¤§é‡çš„å†…å­˜å’Œè®¡ç®—èƒ½åŠ›ã€‚\n\nåˆ†é¡µæ³¨æ„åŠ›ä¸æ˜¯ä¸€æ¬¡å¤„ç†æ•´ä¸ªæ–‡æœ¬ï¼Œè€Œæ˜¯å°†æ–‡æœ¬åˆ†æˆæ›´å°çš„â€œé¡µâ€æˆ–æ®µè½ã€‚æ¨¡åž‹ç„¶åŽä¸€æ¬¡å¤„ç†è¿™äº›é¡µé¢æˆ–ä»¥è¾ƒå°çš„ç»„å¤„ç†ã€‚è¿™ç§æ–¹æ³•æ˜¾è‘—å‡å°‘äº†ä»»ä½•ç»™å®šæ—¶åˆ»æ‰€éœ€çš„å†…å­˜é‡ï¼Œå› ä¸ºæ¨¡åž‹ä¸éœ€è¦åŒæ—¶è·Ÿè¸ªæ•´ä¸ªæ–‡æœ¬çš„å…³ç³»ã€‚\n\nè¿™æœ‰ç‚¹åƒä¸€ä¸ªå­¦ç”Ÿï¼Œå¦‚æžœä¸€æ¬¡æ€§é˜…è¯»æ•´æ•´ä¸€å¹´çš„æ•™ç§‘ä¹¦ä¼šæ„Ÿåˆ°ä¸çŸ¥æ‰€æŽªã€‚é€šè¿‡åœ¨æ•´ä¸ªå­¦å¹´ä¸­å°†å…¶åˆ†è§£ä¸ºå¯ç®¡ç†çš„æ®µè½ï¼Œå­¦ç”Ÿå¯ä»¥è®°ä½æ•™ç§‘ä¹¦çš„å†…å®¹ã€‚\n\né€šè¿‡æ¯ä¸€æ­¥æ‰€éœ€çš„å†…å­˜å‡å°‘ï¼Œåˆ†é¡µæ³¨æ„åŠ›å…è®¸åœ¨ç›¸åŒçš„ç¡¬ä»¶çº¦æŸä¸‹ä½¿ç”¨æ›´å¤§çš„æ¨¡åž‹æˆ–æ›´é•¿çš„æ–‡æœ¬ã€‚\n\n**å¼ é‡å¹¶è¡Œ**\n\nå¹¶è¡Œæ˜¯ä¸€ç§è®¡ç®—ä¸­çš„ä¼—æ‰€å‘¨çŸ¥çš„æ¦‚å¿µã€‚å®ƒæ„å‘³ç€å°†ä¸€ä¸ªå¤§åž‹è®¡ç®—ä»»åŠ¡åˆ†æˆå¯ä»¥ç”±å¤šä¸ªå¤„ç†å™¨æˆ–è®¡ç®—æœºåŒæ—¶å¤„ç†çš„å°éƒ¨åˆ†ã€‚è¿™æ˜¾è‘—åŠ å¿«äº†ç¨‹åºè¿è¡Œæ‰€éœ€çš„æ—¶é—´ã€‚\n\nåœ¨ LLM çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œ[å¼ é‡](https://towardsdatascience.com/what-is-a-tensor-in-deep-learning-6dedd95d6507) æ˜¯å¤šç»´æ•°å­—æ•°ç»„ã€‚è¿™äº›å¼ é‡ç”¨äºŽè¡¨ç¤ºæ¨¡åž‹å¤„ç†çš„æ•°æ®ã€‚è¿™ç±»æ•°æ®åŒ…æ‹¬è¾“å…¥æ–‡æœ¬ã€æ¨¡åž‹æƒé‡ï¼Œå³æ¨¡åž‹å­¦ä¹ çš„å‚æ•°ï¼Œä»¥åŠè¾“å‡ºé¢„æµ‹ã€‚\n\nå°†è¿™ä¸¤ä¸ªæ¦‚å¿µç»“åˆèµ·æ¥ï¼Œå¼ é‡å¹¶è¡Œæ¶‰åŠå°†è¿™äº›å¼ é‡åˆ†å‰²åˆ°å¤šä¸ª GPU æˆ–å…¶ä»–å¤„ç†å•å…ƒä¸Šã€‚ä¾‹å¦‚ï¼Œå¦‚æžœæ¨¡åž‹çš„å‚æ•°ï¼ˆæƒé‡ï¼‰å¤ªå¤§è€Œæ— æ³•é€‚åº”å•ä¸ª GPU çš„å†…å­˜ï¼Œåˆ™å¯ä»¥å°†å…¶åˆ†å¸ƒåˆ°å¤šä¸ª GPU ä¸Šã€‚æ¯ä¸ª GPU ç„¶åŽä¸€æ¬¡åªå¤„ç†å¼ é‡çš„ä¸€éƒ¨åˆ†ã€‚\n\nå°±åƒä¸€ä¸ªå›¢é˜Ÿä¸­çš„å¤šä¸ªæˆå‘˜ä¸€èµ·å·¥ä½œåœ¨ä¸€ä¸ªå¤§åž‹é¡¹ç›®ä¸Šï¼Œå¤„ç†å•å…ƒåœ¨å¤„ç†å„è‡ªéƒ¨åˆ†çš„å¼ é‡æ—¶éœ€è¦äº¤æ¢ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ª GPU ä¸Šçš„è®¡ç®—ç»“æžœå¯èƒ½éœ€è¦ä¸Žå¦ä¸€ä¸ª GPU å…±äº«ï¼Œä»¥ç»§ç»­ä¸‹ä¸€æ­¥è®¡ç®—ã€‚å› æ­¤ï¼Œå•å…ƒä¹‹é—´çš„é«˜æ•ˆé€šä¿¡å¯¹äºŽå¼ é‡å¹¶è¡Œçš„æœ‰æ•ˆæ€§è‡³å…³é‡è¦ã€‚\n\nç®€è€Œè¨€ä¹‹ï¼Œå¼ é‡å¹¶è¡Œæ˜¯ä¸€ç§å°† LLM æ‰€éœ€çš„è®¡ç®—åˆ†è§£ä¸ºæ›´å°çš„å¹¶è¡Œä»»åŠ¡çš„æ–¹æ³•ï¼Œè¿™äº›ä»»åŠ¡å¯ä»¥ç”±å¤šä¸ªè®¡ç®—å•å…ƒåŒæ—¶å¤„ç†ï¼Œä»Žè€ŒåŠ å¿«è¿™äº›å¤§åž‹å¤æ‚æ¨¡åž‹çš„è®­ç»ƒå’ŒæŽ¨ç†æ—¶é—´ã€‚\n\n**æµæ°´çº¿å¹¶è¡Œ**\n\nè¿™ç§æŠ€æœ¯ä¸“æ³¨äºŽæ”¹å–„æ•°æ®é€šè¿‡æ¨¡åž‹å±‚çš„å¤„ç†å·¥ä½œæµã€‚è¿™å¯ä»¥æ˜¾è‘—åŠ å¿«æ•´ä½“è®¡ç®—é€Ÿåº¦ï¼Œå¹¶æ›´å¥½åœ°åˆ©ç”¨å¯ç”¨ç¡¬ä»¶ã€‚\n\nè®¡ç®—ä¸­çš„æµæ°´çº¿å·¥ä½œæ–¹å¼ç±»ä¼¼äºŽå·¥åŽ‚çš„è£…é…çº¿ï¼Œä¸åŒä»»åŠ¡çš„é˜¶æ®µæŒ‰é¡ºåºå®Œæˆã€‚è¿™å…è®¸å¤šä¸ªä»»åŠ¡åœ¨ä¸åŒé˜¶æ®µåŒæ—¶è¿›è¡Œã€‚\n\nåœ¨ LLM ä¸­ï¼Œè¿™äº›ä¸åŒçš„é˜¶æ®µç”±ç¥žç»ç½‘ç»œçš„å±‚è¡¨ç¤ºã€‚æ¯ä¸€å±‚æŒ‰é¡ºåºå¤„ç†è¾“å…¥æ•°æ®ï¼Œé€æ¸æå–æ›´å¤æ‚çš„ç‰¹å¾æˆ–æ¨¡å¼ï¼Œç›´åˆ°äº§ç”Ÿæœ€ç»ˆè¾“å‡ºã€‚å¯ä»¥å°†æ¯ä¸€å±‚è§†ä¸ºå·¥åŽ‚è£…é…çº¿ä¸Šçš„ä¸€ä¸ªå·¥äººï¼šæ¯ä¸ªå·¥äººåœ¨æ•°æ®é€šè¿‡æ—¶éƒ½ä¼šåœ¨å…¶ä¸Šæ·»åŠ ä¸€äº›ä¸œè¥¿ï¼Œç›´åˆ°æœ€ç»ˆå‡ºçŽ°ä¸€ä¸ªå¤æ‚çš„äº§å“ã€‚\n\nåœ¨æµæ°´çº¿å¹¶è¡Œä¸­ï¼Œæ¨¡åž‹çš„å±‚è¢«åˆ†ä¸ºå¤šä¸ªæ®µï¼Œæ¯ä¸ªæ®µåˆ†é…ç»™ä¸åŒçš„ GPU æˆ–å¤„ç†å•å…ƒã€‚è¿™æ ·ï¼Œæ¨¡åž‹å¯ä»¥æŒ‰æ‰¹æ¬¡è¾“å…¥æ•°æ®ï¼šä¸€æ—¦ç¬¬ä¸€ä¸ªæ®µå¤„ç†å®Œç¬¬ä¸€æ‰¹æ•°æ®ï¼Œç¬¬äºŒä¸ªæ®µå°±æŽ¥æ‰‹é‚£æ‰¹æ•°æ®ï¼Œè€Œç¬¬ä¸€ä¸ªæ®µåˆ™æŽ¥æ‰‹ä¸€æ‰¹æ–°çš„æ•°æ®ã€‚\n\nè¿™åœ¨æ¨¡åž‹ä¸­åˆ›å»ºäº†æ•°æ®çš„è¿žç»­æµåŠ¨ï¼Œæ¯ä¸ªæ¨¡åž‹æ®µåœ¨ä»»ä½•ç»™å®šæ—¶é—´éƒ½åœ¨å¤„ç†ä¸åŒçš„æ•°æ®ã€‚è¿™é€šè¿‡ä¿æŒæ¨¡åž‹çš„æ‰€æœ‰éƒ¨åˆ†å¤„äºŽæ´»åŠ¨çŠ¶æ€æ¥æœ€å¤§åŒ–å¯ç”¨ç¡¬ä»¶èµ„æºçš„ä½¿ç”¨ï¼Œå¹¶å‡å°‘å•ä¸ªå¤„ç†å™¨ç­‰å¾…ä»»åŠ¡å®Œæˆæ—¶å¯èƒ½å‘ç”Ÿçš„ç©ºé—²æ—¶é—´ã€‚\n\nå‰é¢è®¨è®ºçš„æµæ°´çº¿å¹¶è¡Œåœ¨æ¨¡åž‹å±‚çº§åˆ«ä¸Šæ“ä½œï¼Œå°†é¡ºåºå¤„ç†é˜¶æ®µåˆ†å¸ƒåˆ°è®¾å¤‡ä¸Šã€‚è€Œå¼ é‡å¹¶è¡Œåˆ™åœ¨æ›´ç»†ç²’åº¦çš„å±‚é¢ä¸Šæ“ä½œï¼Œå°†å±‚å†…å‘ç”Ÿçš„å®žé™…è®¡ç®—ï¼ˆä¾‹å¦‚ï¼Œå¤§åž‹çŸ©é˜µä¹˜æ³•çš„éƒ¨åˆ†ï¼‰åˆ†å¸ƒåˆ°è®¾å¤‡ä¸Šã€‚\n\n**CPU/GPU å¸è½½**\n\nåœ¨è¿™éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†å¾ˆå¤šå…³äºŽ GPU çš„å†…å®¹ã€‚ç„¶è€Œï¼Œå¹¶ä¸æ˜¯æ‰€æœ‰åœ¨è®­ç»ƒæˆ–è¿è¡Œ LLM ä¸­çš„ä»»åŠ¡éƒ½åŒæ ·é€‚åˆ GPUã€‚ä¸€äº›ä»»åŠ¡ï¼Œå¦‚æ•°æ®é¢„å¤„ç†æˆ–æŸäº›æŽ§åˆ¶é€»è¾‘ï¼Œå¯èƒ½æ›´æœ‰æ•ˆåœ°ç”± CPU å¤„ç†ã€‚å…¶ä»–ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯å¤„ç†ç¥žç»ç½‘ç»œï¼ˆå¦‚çŸ©é˜µä¹˜æ³•ï¼‰æ‰€æ¶‰åŠçš„é‡æ•°å­¦è®¡ç®—ï¼Œç¡®å®žæ›´æœ‰æ•ˆåœ°åœ¨ GPU ä¸Šæ‰§è¡Œã€‚\n\né€šè¿‡å°†ç‰¹å®šä»»åŠ¡å¸è½½åˆ°æœ€é€‚åˆå®ƒä»¬çš„å¤„ç†å™¨ä¸Šâ€”â€”å°†å¹¶è¡ŒåŒ–ã€è®¡ç®—å¯†é›†åž‹ä»»åŠ¡åˆ†é…ç»™ GPUï¼Œè€Œå°†é¡ºåºæˆ–é€»è¾‘å¯†é›†åž‹ä»»åŠ¡åˆ†é…ç»™ CPUâ€”â€”ç³»ç»Ÿå¯ä»¥ç¡®ä¿æ¯ä¸ªå·¥ä½œè´Ÿè½½éƒ¨åˆ†ä»¥æœ€æœ‰æ•ˆçš„æ–¹å¼è¿›è¡Œå¤„ç†ã€‚\n\n**èžåˆæ“ä½œ**\n\nèžåˆæ“ä½œå°†é€šå¸¸å•ç‹¬æ‰§è¡Œçš„å¤šä¸ªå¤„ç†æ­¥éª¤åˆå¹¶ä¸ºä¸€ä¸ªç®€åŒ–çš„æ“ä½œã€‚ä¾‹å¦‚ï¼Œè€Œä¸æ˜¯å…ˆæ‰§è¡ŒçŸ©é˜µä¹˜æ³•å†è¿›è¡ŒåŠ æ³•ï¼Œèžåˆæ“ä½œä¼šåŒæ—¶æ‰§è¡Œä¸¤è€…ã€‚\n\n**æŽ¨æµ‹è§£ç **\n\nåœ¨ç”Ÿæˆæ–‡æœ¬æ—¶ï¼ŒLLM æ ¹æ®ä¹‹å‰çš„å•è¯è®¡ç®—å¥å­ä¸­ä¸‹ä¸€ä¸ªå•è¯çš„æ¦‚çŽ‡ã€‚ä¼ ç»Ÿä¸Šï¼Œåœ¨ç”Ÿæˆæ¯ä¸ªå•è¯åŽï¼Œæ¨¡åž‹ä¼šé‡æ–°è®¡ç®—ä»¥ç¡®å®šä¸‹ä¸€ä¸ªå•è¯ï¼Œå¹¶ä¸”è¿™ä¸ªè¿‡ç¨‹ä¼šé‡å¤ï¼Œç›´åˆ°å®Œæ•´çš„å¥å­æˆ–æ®µè½å®Œæˆã€‚ç„¶è€Œï¼Œè¿™ç§é¡ºåºè¿‡ç¨‹å¯èƒ½å¾ˆæ…¢ï¼Œå°¤å…¶æ˜¯å¯¹äºŽè¾ƒé•¿çš„æ–‡æœ¬æˆ–æ›´å¤æ‚çš„æ¨¡åž‹ï¼Œå› ä¸ºæ¯ä¸€æ­¥éƒ½ä¾èµ–äºŽå‰ä¸€æ­¥çš„å®Œæˆã€‚\n\nå¹¶è¡Œé¢„æµ‹ï¼šä¸Žå…¶ç­‰å¾…æ¯ä¸ªå•è¯è¢«é€‰æ‹©åŽå†è€ƒè™‘ä¸‹ä¸€ä¸ªï¼ŒæŽ¨æµ‹è§£ç å…è®¸æ¨¡åž‹â€œæŽ¨æµ‹â€æˆ–åŒæ—¶å¯¹æŽ¥ä¸‹æ¥çš„å‡ ä¸ªå•è¯åšå‡ºå¤šä¸ªé¢„æµ‹ã€‚è¿™è¢«ç§°ä¸º *å¹¶è¡Œé¢„æµ‹*ã€‚è¿™å°±åƒå¯¹å¥å­æŽ¥ä¸‹æ¥å¯èƒ½é‡‡å–çš„å‡ æ¡è·¯å¾„è¿›è¡Œæœ‰æ ¹æ®çš„çŒœæµ‹ã€‚\n\né€šè¿‡å¹¶è¡ŒæŽ¢ç´¢è¿™äº›å¯èƒ½æ€§ï¼Œæ¨¡åž‹å¯ä»¥æ½œåœ¨åœ°å‡å°‘ç”Ÿæˆæ–‡æœ¬æ‰€éœ€çš„æ•´ä½“æ—¶é—´ã€‚ä¸€æ—¦å®žé™…çš„ä¸‹ä¸€ä¸ªå•è¯è¢«é€‰å®šï¼Œæ¨¡åž‹å¯ä»¥æ›´å¿«åœ°æ²¿ç€æœ€å¯èƒ½çš„è·¯å¾„ç»§ç»­ï¼Œå› ä¸ºå®ƒå·²ç»è®¡ç®—äº†åŽç»­çš„é€‰é¡¹ã€‚\n\n### LLMæ¨¡åž‹çš„åŽ‹ç¼©\n\nç ”ç©¶äººå‘˜è¿‡åŽ»æŽ¢ç´¢è¿‡æ¨¡åž‹åŽ‹ç¼©ã€‚ç„¶è€Œï¼Œéšç€å¤§è§„æ¨¡LLMçš„å‡ºçŽ°ï¼Œè¿™å·²æˆä¸ºä¸€ä¸ªæ›´å¤§çš„æŒ‘æˆ˜ã€‚\n\nè®¸å¤šçŽ°æœ‰çš„åŽ‹ç¼©æ–¹æ³•ä¾èµ–äºŽæ‰§è¡Œå¾®è°ƒæ­¥éª¤ä»¥åœ¨åŽ‹ç¼©é˜¶æ®µæ¢å¤ä¸¢å¤±çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå½“åº”ç”¨äºŽLLMæ—¶ï¼Œç”±äºŽå…¶åºžå¤§çš„è§„æ¨¡ï¼Œè¿™ç§æ–¹æ³•æœ‰æ˜¾è‘—çš„å±€é™æ€§ã€‚å› æ­¤ï¼ŒLLMåŽ‹ç¼©å·²æˆä¸ºä¸€ä¸ªå…¨æ–°çš„ç ”ç©¶é¢†åŸŸã€‚\n\n**æž¶æž„å‰ªæž**\n\nå½“ä½ ä¿®å‰ªè‹¹æžœæ ‘æ—¶ï¼Œä½ ä¼šåœ¨å†¬å­£æˆ–æ—©æ˜¥å‰ªæŽ‰æŸäº›æ ‘æžã€‚è¿™ç¡®ä¿æ ‘æœ¨ä¸ä¼šåœ¨æ— æ•ˆçš„æ ‘æžä¸Šæµªè´¹èµ„æºæˆ–å› æž¯æœ¨è€Œæ„ŸæŸ“ç–¾ç—…ã€‚è¿™æœ‰åŠ©äºŽå®ƒç»“å‡ºæ›´å¥½çš„æžœå®žã€‚\n\nå½“ç„¶ï¼ŒLLMå¹¶ä¸ç»“å‡ºæžœå®žã€‚åœ¨è¿™ä¸ªèƒŒæ™¯ä¸‹ï¼Œå‰ªæžæ˜¯ä¸€ç§ç”¨äºŽå‡å°‘æ¨¡åž‹å¤§å°çš„æ–¹æ³•ï¼ŒåŒæ—¶å°½é‡ä¿æŒæˆ–æœ€å°åŒ–å¯¹å…¶æ€§èƒ½çš„å½±å“ã€‚\n\nLLMæ¨¡åž‹æœ‰æ•°ç™¾ä¸‡ç”šè‡³æ•°åäº¿ä¸ªå‚æ•°ã€‚è¿™äº›å‚æ•°å¹¶ä¸æ˜¯æ‰€æœ‰å¯¹æ¨¡åž‹è¿›è¡Œé¢„æµ‹æˆ–ç†è§£è¯­è¨€éƒ½åŒç­‰é‡è¦ã€‚æœ‰äº›å‚æ•°å¾ˆå°‘ä½¿ç”¨æˆ–å¯¹æ¨¡åž‹çš„å†³ç­–è´¡çŒ®ä¸å¤§ï¼šå› æ­¤ï¼Œæ¶ˆé™¤è¿™äº›å†—ä½™æˆ–å½±å“è¾ƒå°çš„è¿žæŽ¥ã€ç¥žç»å…ƒæˆ–æ•´ä¸ªå±‚ï¼Œä½¿æ¨¡åž‹çš„ä½¿ç”¨æ›´é«˜æ•ˆã€‚\n\né€‰æ‹©å‰ªæžå“ªäº›å‚æ•°å¹¶ä¸æ˜¯ä¸€é¡¹ç®€å•çš„ä»»åŠ¡ã€‚åœ¨åŸºäºŽå¹…åº¦çš„å‰ªæžä¸­ï¼Œç§»é™¤ç¥žç»ç½‘ç»œä¸­ç»å¯¹å€¼æœ€å°çš„æƒé‡ã€‚åœ¨è®­ç»ƒä¹‹å‰ï¼Œè¿™äº›æƒé‡é€šå¸¸ä¸ºé›¶ï¼›è®­ç»ƒä¹‹åŽï¼Œå®ƒä»¬é€šå¸¸ä»‹äºŽ-1å’Œ1ä¹‹é—´ã€‚å¦‚æžœè®­ç»ƒå¯¹æŸä¸ªæƒé‡çš„å½±å“ä¸å¤§ï¼Œé‚£ä¹ˆå®ƒå¾ˆå¯èƒ½æŽ¥è¿‘é›¶ï¼Œå› æ­¤å¯¹æ¨¡åž‹çš„å†³ç­–è´¡çŒ®è¾ƒå°‘ã€‚\n\nä¸€ç§èµ„æºå¯†é›†ä½†ä¹Ÿæ›´ç¨³å¥çš„å‰ªæžæŠ€æœ¯æ˜¯çµæ•åº¦åˆ†æžã€‚è¿™æ¶‰åŠè¯„ä¼°ç§»é™¤æ¯ä¸ªå‚æ•°æˆ–å‚æ•°ç»„å¯¹æ¨¡åž‹æ€§èƒ½çš„å½±å“ã€‚ç§»é™¤åŽå¯¼è‡´æ€§èƒ½ä¸‹é™æœ€å°çš„å‚æ•°ä¼šè¢«å‰ªæžã€‚\n\nè¿˜æœ‰å…¶ä»–æŠ€æœ¯ï¼Œä½†é€šå¸¸å¯ä»¥å°†å®ƒä»¬åˆ†ç±»ä¸ºéžç»“æž„åŒ–å‰ªæžæˆ–ç»“æž„åŒ–å‰ªæžã€‚éžç»“æž„åŒ–å‰ªæžï¼ˆä¾‹å¦‚åŸºäºŽå¹…åº¦çš„å‰ªæžï¼‰ç§»é™¤å•ä¸ªæƒé‡ï¼Œå¯¼è‡´ç¨€ç–è¿žæŽ¥çš„ç¥žç»ç½‘ç»œã€‚ç»“æž„åŒ–å‰ªæžï¼ˆä¾‹å¦‚çµæ•åº¦åˆ†æžï¼‰ç§»é™¤æ•´ä¸ªå•å…ƒæˆ–å±‚ï¼ˆä¾‹å¦‚ï¼Œæ•´ä¸ªç¥žç»å…ƒæˆ–é€šé“ï¼‰ï¼Œè¿™åœ¨æŸäº›ç¡¬ä»¶ä¸Šå¯ä»¥æ›´æœ‰æ•ˆåœ°æé«˜è®¡ç®—æ•ˆçŽ‡ã€‚\n\nå‰ªæžåŽï¼Œæ¨¡åž‹é€šå¸¸ä¼šç»åŽ†å¾®è°ƒè¿‡ç¨‹ã€‚è¿™æ¶‰åŠåœ¨è®­ç»ƒæ•°æ®é›†æˆ–å…¶å­é›†ä¸Šå¯¹å‰ªæžåŽçš„æ¨¡åž‹è¿›è¡Œå†è®­ç»ƒã€‚ç›®æ ‡æ˜¯è®©æ¨¡åž‹è°ƒæ•´å’Œä¼˜åŒ–å…¶å‰©ä½™å‚æ•°ï¼Œä»¥è¡¥å¿å‰ªæžæ‰€é€ æˆçš„æŸå¤±ã€‚è¿™æœ‰åŠ©äºŽæ¢å¤å› å‰ªæžè€Œå¤±åŽ»çš„ä»»ä½•æ€§èƒ½ã€‚\n\nè¿™å¯ä»¥é€šè¿‡è¿­ä»£æ–¹å¼æˆ–ä¸€æ¬¡æ€§æ–¹å¼è¿›è¡Œã€‚åœ¨è¿­ä»£å‰ªæžä¸­ï¼Œæ¨¡åž‹åœ¨å¤šä¸ªè½®æ¬¡ä¸­é€æ­¥å‰ªæžã€‚åœ¨æ¯ä¸€è½®ä¹‹åŽï¼Œå‰ªæžåŽçš„æ¨¡åž‹ä¼šé‡æ–°è®­ç»ƒï¼Œä»¥æ¢å¤å› å‰ªæžè€Œå¤±åŽ»çš„æ€§èƒ½ã€‚è¿™ä¸ªå¾ªçŽ¯å¯ä»¥é‡å¤å¤šæ¬¡ï¼Œæ¨¡åž‹å¯èƒ½ä¼šå˜å¾—æ›´åŠ ç¨³å¥ï¼Œå³ä½¿åœ¨æ˜¾è‘—å‡å°‘å‚æ•°çš„æƒ…å†µä¸‹ä¹Ÿèƒ½ä¿æŒæ€§èƒ½ã€‚åœ¨ä¸€æ¬¡æ€§å‰ªæžä¸­ï¼Œæ‰€æœ‰è¯†åˆ«å‡ºçš„å‚æ•°ä¸€æ¬¡æ€§ç§»é™¤ï¼Œç„¶åŽå¯¹æ¨¡åž‹è¿›è¡Œå¾®è°ƒã€‚\n\n**çŸ¥è¯†è’¸é¦**\n\næƒ³è±¡ä¸€ä¸‹ï¼Œæœ‰ä¸€ä¸ªè¶³çƒåœºä¸Šæœ‰ä¸¤ä¸ªçƒå‘˜ï¼šä¸€ä¸ªéžå¸¸æœ‰ç»éªŒï¼ŒçŸ¥é“å¾ˆå¤šæŠ€å·§ï¼Œå¦ä¸€ä¸ªæ˜¯åˆå­¦è€…ã€‚ç»éªŒä¸°å¯Œçš„çƒå‘˜çŸ¥é“çš„æ¯”åˆå­¦è€…å¤šå¾—å¤šï¼Œä½†åˆå­¦è€…å¯ä»¥é€šè¿‡æ¨¡ä»¿å…¶ä»–çƒå‘˜åœ¨åœºä¸Šçš„è¡Œä¸ºè¿…é€Ÿè¾¾åˆ°å¯æ¯”çš„è¡¨çŽ°ã€‚\n\nLLMçš„çŸ¥è¯†è’¸é¦å·¥ä½œåŽŸç†ç±»ä¼¼ï¼šè¿™æ˜¯è®­ç»ƒä¸€ä¸ªæ›´å°ï¼ˆå­¦ç”Ÿæ¨¡åž‹ï¼‰ã€æ›´é«˜æ•ˆçš„æ¨¡åž‹ï¼Œä»¥é€šè¿‡å­¦ä¹ å¤§æ¨¡åž‹ï¼ˆæ•™å¸ˆæ¨¡åž‹ï¼‰çš„è¾“å‡ºå’Œå¤„ç†ä¿¡æ¯çš„æ–¹å¼æ¥å¤åˆ¶å…¶æ€§èƒ½çš„è¿‡ç¨‹ã€‚\n\nè¦åº”ç”¨è¿™ä¸€æŠ€æœ¯ï¼Œæ˜¾ç„¶éœ€è¦ä¸€ä¸ªå¤§åž‹æ•™å¸ˆæ¨¡åž‹ï¼Œä¾‹å¦‚LlaMaæˆ–Mistralçš„å¼€æºå¤§åž‹æ¨¡åž‹ä¹‹ä¸€ã€‚ç„¶åŽéœ€è¦è®¾è®¡ä¸€ä¸ªå‚æ•°æ•°é‡æ˜¾è‘—å°‘äºŽæ•™å¸ˆæ¨¡åž‹çš„è¾ƒå°ç¥žç»ç½‘ç»œã€‚\n\nå­¦ç”Ÿæ¨¡åž‹ä¸ä»…ä»…åœ¨åŽŸå§‹ç¡¬ç›®æ ‡ï¼ˆå³çœŸå®žæ•°æ®æ ‡ç­¾ï¼‰ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¿˜åœ¨è½¯ç›®æ ‡ä¸Šè¿›è¡Œè®­ç»ƒã€‚è¿™äº›æ˜¯æ•™å¸ˆæ¨¡åž‹å¯¹ç›¸åŒè¾“å…¥ç”Ÿæˆçš„æ¦‚çŽ‡ã€‚ä¾‹å¦‚ï¼Œå¯¹äºŽä¸€ç»„ç‰¹å®šçš„æŸ¥è¯¢ï¼Œå‡è®¾æ•™å¸ˆæ¨¡åž‹70%çš„æ—¶é—´å›žç­”ä¸ºâ€œAâ€ï¼Œ20%çš„æ—¶é—´å›žç­”ä¸ºâ€œBâ€ï¼Œ10%çš„æ—¶é—´å›žç­”ä¸ºâ€œCâ€ã€â€œDâ€æˆ–â€œEâ€ã€‚å­¦ç”Ÿæ¨¡åž‹ä¸ä»…ä¼šå°è¯•æ­£ç¡®å›žç­”æ¯ä¸ªé—®é¢˜ï¼›å®ƒè¿˜ä¼šå°è¯•åœ¨ä¸€ç»„æŸ¥è¯¢ä¸­éµå¾ªç›¸åŒçš„æ¦‚çŽ‡åˆ†å¸ƒã€‚\n\nè¿™æ ·çš„è½¯ç›®æ ‡æ¯ä¸ªç¤ºä¾‹æºå¸¦çš„ä¿¡æ¯æ¯”ç¡¬æ ‡ç­¾æ›´å¤šï¼Œå› ä¸ºå®ƒä»¬åŒ…å«æ•™å¸ˆæ¨¡åž‹å¯¹æ‰€æœ‰å¯èƒ½ç»“æžœçš„ç½®ä¿¡æ°´å¹³ã€‚è¿™å°±æ˜¯å­¦ç”Ÿæ¨¡åž‹èƒ½å¤Ÿä»¥è¾ƒä½Žçš„è®¡ç®—å¼€é”€è¡¨çŽ°å¾—ä¸Žæ•™å¸ˆç›¸ä¼¼çš„åŽŸå› ã€‚\n\nåœ¨åˆå§‹çŸ¥è¯†è’¸é¦ä¹‹åŽï¼Œå­¦ç”Ÿæ¨¡åž‹å¯èƒ½ä¼šåœ¨ç‰¹å®šä»»åŠ¡çš„æ•°æ®é›†ä¸Šè¿›è¡Œè¿›ä¸€æ­¥çš„å¾®è°ƒï¼Œä»¥æœ€å¤§åŒ–å…¶æ€§èƒ½ã€‚\n\n**ä½Žç§©è¿‘ä¼¼**\n\nLLMé€šè¿‡å¤„ç†å’Œç”ŸæˆåŸºäºŽå·¨å¤§çš„çŸ©é˜µï¼ˆå³éžå¸¸å¤§çš„æ•°å­—è¡¨ï¼‰æ¥å·¥ä½œï¼Œè¿™äº›çŸ©é˜µè¡¨ç¤ºå•è¯ä¹‹é—´çš„å…³ç³»ã€å®ƒä»¬çš„å«ä¹‰ä»¥åŠå®ƒä»¬åœ¨è¯­è¨€ä¸­çš„ä½¿ç”¨ã€‚è¿™äº›çŸ©é˜µå¯èƒ½å¤§åˆ°éš¾ä»¥å¤„ç†ï¼Œç‰¹åˆ«æ˜¯åœ¨å­˜å‚¨å’Œè®¡ç®—æ–¹é¢ã€‚\n\nä½Žç§©è¿‘ä¼¼æ¶‰åŠæ‰¾åˆ°ä¸€ä¸ªæ›´ç®€å•çš„çŸ©é˜µï¼Œå…¶å¤§å°è¦å°å¾—å¤šï¼Œä½†ä»èƒ½æ•æ‰åˆ°åŽŸå§‹å¤§çŸ©é˜µä¸­æœ€é‡è¦çš„ä¿¡æ¯ã€‚è¿™æœ‰ç‚¹åƒå°†è¯¦ç»†çš„ç”»ä½œç®€åŒ–ä¸ºè‰å›¾ã€‚\n\nè¿™é€šè¿‡æ•°å­¦æŠ€æœ¯æ¥å®Œæˆï¼Œè¿™äº›æŠ€æœ¯è¯†åˆ«çŸ©é˜µï¼ˆæˆ–åœ¨æˆ‘ä»¬çš„ç±»æ¯”ä¸­ï¼Œç”»ä½œï¼‰ä¸­å“ªäº›éƒ¨åˆ†åŒ…å«æœ€å¤šçš„ä¿¡æ¯ï¼Œå¹¶å°†çŸ©é˜µç¼©å‡åˆ°ä»…è¿™äº›éƒ¨åˆ†ã€‚æœ‰ä¸€äº›æ•°å­¦æŠ€æœ¯ï¼Œå°¤å…¶æ˜¯[å¥‡å¼‚å€¼åˆ†è§£](https://www.cs.cmu.edu/~venkatg/teaching/CStheory-infoage/book-chapter-4.pdf)ï¼Œæœ‰åŠ©äºŽå®žçŽ°è¿™ä¸€ç‚¹ã€‚\n\nä¸Žå‰ªæžä¸åŒï¼Œä½Žç§©è¿‘ä¼¼æ‰§è¡ŒçŸ©é˜µç»´åº¦å‡å°‘ï¼Œä¿æŒæ¨¡åž‹çš„ç»“æž„ï¼Œä½†ä»¥æ›´ç´§å‡‘çš„å½¢å¼è¡¨ç¤ºï¼Œè€Œå‰ªæžåˆ™ç›´æŽ¥ç§»é™¤ç¥žç»ç½‘ç»œçš„éƒ¨åˆ†ã€‚\n\n**é‡åŒ–**\n\nLLMä½¿ç”¨å¤§é‡æ•°å­¦è®¡ç®—æ¥å¤„ç†æ–‡æœ¬ã€‚è¿™äº›è®¡ç®—ä½¿ç”¨å¯ä»¥å…·æœ‰å¹¿æ³›å€¼èŒƒå›´çš„æ•°å­—ã€‚é€šå¸¸ï¼Œè¿™äº›æ•°å­—ä»¥å¯ä»¥è¡¨ç¤ºéžå¸¸å¹¿æ³›å€¼èŒƒå›´çš„æ ¼å¼å­˜å‚¨ï¼ˆ[æµ®ç‚¹æ ¼å¼](https://de.wikipedia.org/wiki/Einfache_Genauigkeit)ï¼‰ï¼Œåœ¨å†…å­˜ä¸­å ç”¨32ä½ã€‚\n\né‡åŒ–å‡å°‘äº†è¿™äº›æ•°å­—çš„ç²¾åº¦ï¼Œé€šå¸¸å°†32ä½æµ®ç‚¹æ•°å­—è½¬æ¢ä¸ºæ›´ä½Žä½å®½çš„è¡¨ç¤ºï¼Œä¾‹å¦‚8ä½æ•´æ•°ã€‚è¿™æ„å‘³ç€æ¨¡åž‹ä¸å†ä½¿ç”¨å…·æœ‰è®¸å¤šå°æ•°ä½çš„æ•°å­—ï¼Œè€Œæ˜¯ä½¿ç”¨â€œæ›´ç®€å•â€çš„æ•°å­—ï¼Œä»Žè€Œä½¿è®¡ç®—æ›´å¿«å¹¶å‡å°‘å†…å­˜å ç”¨ã€‚\n\né‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰æ¶‰åŠåœ¨è®­ç»ƒæ¨¡åž‹æ—¶è€ƒè™‘é‡åŒ–ï¼Œä½¿å…¶èƒ½å¤Ÿé€‚åº”ç²¾åº¦æŸå¤±ï¼Œé€šå¸¸å¯¼è‡´æ›´å¥½çš„æ€§èƒ½ï¼Œä½†ä»£ä»·æ˜¯æ›´å¤æ‚å’Œèµ„æºå¯†é›†çš„è®­ç»ƒè¿‡ç¨‹ã€‚\n\nåŽè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰åœ¨æ¨¡åž‹å®Œå…¨è®­ç»ƒåŽåº”ç”¨é‡åŒ–ï¼Œæä¾›äº†ä¸€ç§æ›´ç®€å•å’Œæ›´å¿«é€Ÿçš„æ–¹æ³•æ¥å‡å°‘è®¡ç®—éœ€æ±‚ã€‚ç„¶è€Œï¼Œç”±äºŽæ¨¡åž‹å¹¶æœªç‰¹åˆ«é’ˆå¯¹ä½Žç²¾åº¦æ“ä½œè¿›è¡Œä¼˜åŒ–ï¼Œå› æ­¤å¯èƒ½æ— æ³•è¾¾åˆ°ä¸ŽQATç›¸åŒçš„å‡†ç¡®æ€§æˆ–æ€§èƒ½æ°´å¹³ã€‚\n\n### 1ä½ LLM æ—¶ä»£ï¼Ÿ\n\nå¾®è½¯ç ”ç©¶äººå‘˜æœ€è¿‘å‘è¡¨äº†ä¸€ç¯‡[å¼•èµ·è½°åŠ¨çš„è®ºæ–‡](https://arxiv.org/pdf/2402.17764.pdf)ï¼Œå°†æ¯ä¸ªå‚æ•°çš„å­˜å‚¨ä½æ•°ä»Žå½“å‰ LLM ä¸­çš„ 16 ä½æ ‡å‡†ï¼Œé™ä½Žåˆ°äº†ä»…ä»… 1.58 ä½ã€‚è¿™æ˜¯ä¸ªé‡å¤§æ–°é—»ï¼šé€šè¿‡è¿™ç§æŠ€æœ¯ï¼Œä»–ä»¬å®žçŽ°äº†è¿‘ 10 å€çš„ä»¤ç‰Œåžåé‡ï¼Œå³å¤„ç†æ–‡æœ¬çš„é€Ÿåº¦å‡ ä¹Žå¿«äº† 10 å€ã€‚ä»–ä»¬è¿˜å°†å†…å­˜å ç”¨å‡å°‘äº† 3.5 å€ï¼Œè¿™æ„å‘³ç€è¿è¡Œè¿™äº›æ¨¡åž‹æ‰€éœ€çš„ç¡¬ä»¶å¤§å¤§å‡å°‘ã€‚\n\nè¿™æ˜¯é€šè¿‡ä½¿ç”¨ä¸‰å…ƒä½å®žçŽ°çš„ã€‚ä¸Žé€šå¸¸ä½¿ç”¨çš„ä»‹äºŽ -1 å’Œ 1 ä¹‹é—´çš„æµ®ç‚¹æ•°ï¼ˆé€šå¸¸ä½¿ç”¨ 16 ä½ï¼‰ä¸åŒï¼Œæ¯ä¸ªæƒé‡è¢«è¡¨ç¤ºä¸º -1ã€0 æˆ– 1ã€‚è¿™äº›æ•°å­—å¯ä»¥å­˜å‚¨åœ¨ 1.58 ä½ï¼Œå› ä¸ºå¯¹äºŽ 3 ä¸ªå¯èƒ½å€¼çš„äºŒè¿›åˆ¶æ™¶ä½“ç®¡ï¼Œå¯ä»¥å¾—åˆ° 2Â¹.58 = 3ã€‚ä»…ä½¿ç”¨å¦‚æ­¤ç®€å•çš„æ•°å­—ä¹Ÿæ„å‘³ç€ä¸å†éœ€è¦å¤æ‚çš„çŸ©é˜µä¹˜æ³•ï¼Œè¿™ä½¿å¾—èµ„æºä½¿ç”¨æ•ˆçŽ‡å¤§å¤§æé«˜ã€‚\n\nè¿™ç§æŠ€æœ¯ä»¤äººå›°æƒ‘çš„æ˜¯ï¼Œå®ƒåœ¨ 30 äº¿å‚æ•°çš„å¤§å°ä¸‹ï¼Œèƒ½å¤Ÿå®žçŽ°ä¸Žä¼ ç»Ÿ 16 ä½æ¨¡åž‹ç›¸ä¼¼çš„è¾“å‡ºæ€§èƒ½ã€‚ç›®å‰å°šä¸æ¸…æ¥šè¿™ç§æ¨¡åž‹åœ¨è¶…è¿‡ 130 äº¿å‚æ•°çš„é˜ˆå€¼æ—¶ï¼Œæ˜¯å¦èƒ½åƒä¼ ç»Ÿæ¨¡åž‹ä¸€æ ·æ‰©å±•ã€‚æ˜Žç¡®çš„æ˜¯ï¼Œå³ä½¿åœ¨ 700 äº¿å‚æ•°ä¸‹ï¼Œå®ƒåœ¨å»¶è¿Ÿã€å†…å­˜ä½¿ç”¨å’Œèƒ½è€—æ–¹é¢æ¯”ä»…æœ‰ 130 äº¿å‚æ•°çš„ä¼ ç»Ÿæ¨¡åž‹æ›´é«˜æ•ˆã€‚è¾“å‡ºè´¨é‡ä»éœ€è¯¦ç»†æµ‹è¯•ã€‚\n\nå¦ä¸€ä¸ªç¼ºç‚¹æ˜¯ï¼ŒçŽ°æœ‰ LLM çš„æœ€å…ˆè¿›é‡åŒ–æŠ€æœ¯æ— æ³•ç”¨äºŽç”Ÿæˆ 1.58 ä½æ¨¡åž‹ã€‚è¿™ç±»æ¨¡åž‹éœ€è¦ä»Žå¤´å¼€å§‹åˆ›å»ºï¼Œå°½ç®¡æˆæœ¬å¤§å¹…é™ä½Žï¼Œä½†ç›®å‰ä»è¶…å‡ºæ™®é€šå¸‚æ°‘çš„æ‰¿å—èŒƒå›´ã€‚\n\nç„¶è€Œï¼Œå¦‚æžœè¿™æ ·çš„æ¨¡åž‹è¢«åˆ›å»ºå¹¶è¿è¡Œè‰¯å¥½ï¼ŒæŽ¨ç†å°†å˜å¾—æ›´åŠ å®¹æ˜“ã€‚1.58 ä½ LLM ç”šè‡³å¯èƒ½åœ¨è¾¹ç¼˜å’Œç§»åŠ¨è®¾å¤‡ä¸Šéƒ¨ç½²ã€‚å®ƒä»¬å¯¹ CPU è®¾å¤‡ï¼ˆå¤§å¤šæ•°ç§»åŠ¨è®¾å¤‡è¿è¡Œçš„è®¾å¤‡ï¼‰ä¹Ÿæ›´åŠ å‹å¥½ï¼Œè¿™ä½¿å¾—å®ƒä»¬æ›´å®¹æ˜“åœ¨æ›´ä¾¿å®œçš„èŠ¯ç‰‡ä¸Šéƒ¨ç½²ã€‚æ‰€æœ‰è¿™äº›éƒ½æœ‰è®¸å¤šä¼˜åŠ¿ï¼Œä¾‹å¦‚éšç§æ–¹é¢ï¼Œä½†ä¹Ÿå…è®¸å‡ºçŽ°äººç±»å°šæœªæ¢¦æƒ³çš„æ–°çš„åº”ç”¨ã€‚\n\næ­¤å¤–ï¼Œåƒ [Groq](https://groq.com/) è¿™æ ·çš„åˆåˆ›å…¬å¸å·²ç»å±•ç¤ºäº†åœ¨ä¸º LLM æž„å»ºç‰¹å®šç¡¬ä»¶ [å¦‚ LPU](https://wow.groq.com/why-groq/) çš„ promising results å’Œå·¨å¤§æ½œåŠ›ã€‚LLM ä¸“ç”¨ç¡¬ä»¶å·²ç»æ˜¯ä¸€ä¸ª[å·¨å¤§å¸‚åœº](https://finance.yahoo.com/news/generative-ai-market-size-expected-163500846.html#:~:text=%2D%20Large%20Language%20Model%20(LLM),the%20forecast%20period%202023%2D2029.)ã€‚è¿™æ ·çš„å‘çŽ°å¯èƒ½ä½¿è¿™ä¸ªå¸‚åœºçš„å¢žé•¿é€Ÿåº¦æ¯”åˆ†æžå¸ˆè¿„ä»Šé¢„è§çš„æ›´ä¸ºæ¿€è¿›ã€‚\n\nå¦‚æžœæ²¡æœ‰å…¶ä»–ï¼ŒæŽ¨ç†å°†ç”±äºŽé‡åŒ–æŠ€æœ¯å’Œä¸“ç”¨ç¡¬ä»¶çš„ç»“åˆè€Œå˜å¾—æžä¸ºä¾¿å®œã€‚è¿™å¯¹è®¸å¤šå…¬å¸ï¼ŒåŒ…æ‹¬åˆåˆ›å…¬å¸ï¼Œéƒ½ä¼šäº§ç”Ÿå½±å“ã€‚\n\n## è¾ƒè½»é‡çš„ LLM å¯¹åˆåˆ›ä¼ä¸šæ„å‘³ç€ä»€ä¹ˆï¼Ÿ\n\n### AIç¡¬ä»¶çš„ç¹è£åˆšåˆšå¼€å§‹\n\nåœ¨1971å¹´è‡³1999å¹´é—´ï¼ŒCPUå‡ ä¹Žæ˜¯å¸‚åœºä¸Š[å”¯ä¸€çš„å¾®å¤„ç†å™¨](https://cs.stanford.edu/people/eroberts/courses/soco/projects/2005-06/64-bit-processors/history1.html)ã€‚éšåŽï¼Œ[NVIDIAæŽ¨å‡º](https://readmedium.com/a-brief-history-of-gpu-47d98d6a0f8a)äº†å…¶GPUã€‚è™½ç„¶ä»ŽæŠ€æœ¯ä¸Šè®²ï¼Œå®ƒå¹¶ä¸æ˜¯ä¸–ç•Œä¸Šç¬¬ä¸€ä¸ªGPUï¼Œä½†å®ƒæ˜¯ä½¿æ¸¸æˆæˆä¸ºä¸€ç§å¯æŽ¥è§¦å’Œæ²‰æµ¸å¼ä½“éªŒçš„é¦–æ‰¹å¾®å¤„ç†å™¨ä¹‹ä¸€ã€‚ï¼ˆæ¸¸æˆæ¶ˆè€—å¤§é‡è®¡ç®—èƒ½åŠ›â€”â€”å¦‚æžœä½ ä¸çŸ¥é“ï¼ŒçŽ°åœ¨ä½ çŸ¥é“äº†ï¼ï¼‰\n\nä»Žæ¸¸æˆå¼€å§‹ï¼ŒGPUè¿…é€Ÿæ‰©å±•åˆ°è®¸å¤šä¸åŒçš„ä»»åŠ¡ï¼ŒåŒ…æ‹¬ç§‘å­¦å›¾åƒå¤„ç†ã€çº¿æ€§ä»£æ•°ã€3Dé‡å»ºç­‰ã€‚GPUç‰¹åˆ«æ“…é•¿çš„ä¸€ä»¶äº‹æ˜¯ä»€ä¹ˆï¼Ÿæœºå™¨å­¦ä¹ å’ŒLLMã€‚å¦‚ä»Šï¼Œè®¸å¤šNVIDIAçš„èŠ¯ç‰‡æ­£åœ¨ç”¨äºŽè®­ç»ƒLLMã€‚\n\nä»Žé‚£æ—¶èµ·ï¼Œå…¶ä»–å¾®å¤„ç†å™¨ä¹Ÿå¼€å§‹æ¶ŒçŽ°ã€‚[è°·æ­Œçš„TPU](https://cloud.google.com/tpu?hl=en)äºŽ2016å¹´æŽ¨å‡ºï¼Œç‰¹åˆ«é€‚åˆAIè®­ç»ƒå’ŒæŽ¨ç†ã€‚è™½ç„¶GPUè¢«è¯æ˜Žéžå¸¸é€‚åˆLLMï¼Œä½†TPUæ˜¯ä¸“é—¨ä¸ºæ­¤ç›®çš„è®¾è®¡çš„ã€‚å®ƒä»¬åœ¨è®­ç»ƒå’ŒæŽ¨ç†æ–¹é¢éƒ½éžå¸¸åˆé€‚ã€‚\n\nç„¶è€Œï¼Œè¡Œä¸šæ­£å¤„äºŽ[è½¬æŠ˜ç‚¹](https://www.wsj.com/tech/ai/how-a-shifting-ai-chip-market-will-shape-nvidias-future-f0c256b1)ï¼šä¸ä¹…ä¹‹åŽï¼Œå¤§å¤šæ•°ä¸ŽLLMç›¸å…³çš„å·¥ä½œå°†æ˜¯æŽ¨ç†ï¼Œè€Œä¸å†æ˜¯è®­ç»ƒï¼Œå› ä¸ºç”¨æˆ·å¼€å§‹éƒ¨ç½²åƒLlaMaè¿™æ ·çš„æ¨¡åž‹ã€‚æ–°çš„åˆ›æ–°AIåŠå¯¼ä½“å…¬å¸çŽ°åœ¨æœ‰æœºä¼šè¿›å…¥è¿™ä¸ªé¢†åŸŸã€‚\n\nè¿™åŒ…æ‹¬ä¸“æ³¨äºŽç‰¹åˆ«å¿«é€ŸæŽ¨ç†å¤„ç†å™¨çš„èŠ¯ç‰‡åˆ¶é€ å•†[Groq](https://wow.groq.com/press/)ã€‚å…¶ä»–åˆåˆ›å…¬å¸åŒ…æ‹¬[èµ›æ‹‰å¸ƒæ‹‰æ–¯](https://www.cerebras.net/)ï¼ˆä¸“æ³¨äºŽè®­ç»ƒï¼‰ã€[Graphcore](https://www.graphcore.ai/about)ï¼ˆæ¶µç›–è®­ç»ƒå’ŒæŽ¨ç†ï¼‰å’Œ[SambaNova](https://sambanova.ai/)ï¼ˆä¹ŸåŒ…æ‹¬è®­ç»ƒå’ŒæŽ¨ç†ï¼‰ã€‚åƒè‹±ç‰¹å°”å’ŒAMDè¿™æ ·æ›´æˆç†Ÿçš„ç«žäº‰å¯¹æ‰‹ä¹Ÿåœ¨å…³æ³¨è®­ç»ƒå’ŒæŽ¨ç†ï¼Œå°½ç®¡é¢„è®¡æœªæ¥å‡ å¹´çš„å¤§å¤šæ•°å¢žé•¿å°†æ¥è‡ªåŽè€…ã€‚å¤§åž‹ç§‘æŠ€å·¨å¤´â€”â€”è°·æ­Œã€äºšé©¬é€Šæˆ–å¾®è½¯â€”â€”ä¹Ÿåœ¨å¼€å‘AIä¸“ç”¨èŠ¯ç‰‡ï¼Œä½†ä¸»è¦ç”¨äºŽå†…éƒ¨ä½¿ç”¨ã€‚\n\næ€»ä½“è€Œè¨€ï¼ŒLLMçš„ç¡¬ä»¶å¸‚åœºä»ç„¶ç”±æ•°æ®ä¸­å¿ƒåº”ç”¨ä¸»å¯¼ã€‚è¾¹ç¼˜å’Œç§»åŠ¨åº”ç”¨æ˜¯ä¸‹ä¸€ä¸ªåˆä¹Žé€»è¾‘çš„æ­¥éª¤ï¼Œä½†å°†éœ€è¦æ›´å¤šçªç ´ï¼Œä¾‹å¦‚å¾®è½¯ç ”ç©¶äººå‘˜æœ€è¿‘å‘å¸ƒçš„1.58ä½æ–¹æ³•ï¼ˆè§ä¸Šæ–‡ï¼‰ã€‚\n\n## LLMè½¯ä»¶å…¬å¸çš„å½±å“\n\nåœ¨æ–°å…´AIé¢†åŸŸçš„æ•´ä¸ªä»·å€¼é“¾ä¸­ï¼Œæˆ‘ä»¬æ¦‚è¿°çš„è¿™äº›å‘å±•å¯èƒ½ä¼šå¯¼è‡´**è¿è¡Œ/ä½¿ç”¨LLMçš„æˆæœ¬æ˜¾è‘—é™ä½Ž**ã€‚\n\nä»¥ä¸‹æ˜¯æˆ‘ä»¬å¯¹è¿™ä¸€è¶‹åŠ¿çš„å‡ ç‚¹æ€è€ƒï¼š\n\n* **ä¼˜ç§€çš„B2Cäº§å“**ï¼Œå› ä¸ºLLMæˆæœ¬é™ä½Žæ„å‘³ç€å¯ä»¥æž„å»ºå…·æœ‰é«˜LLMä½¿ç”¨é¢‘çŽ‡å’Œè§„æ¨¡ï¼ˆä¾‹å¦‚ï¼Œé•¿ä¸Šä¸‹æ–‡çª—å£ï¼‰çš„å…è´¹å¢žå€¼B2Cä½“éªŒï¼Œè€Œä¸ä¼šç ´åå…¬å¸çš„å•ä½ç»æµŽã€‚\n* å…¨çƒèŒƒå›´å†…çš„è®¿é—®æ°‘ä¸»åŒ–ï¼Œä½¿å¾—**ä½Žæ”¶å…¥å›½å®¶çš„ç”¨æˆ·**èƒ½å¤Ÿåˆ©ç”¨å…ˆè¿›çš„AIæŠ€æœ¯ã€‚\n* å…¬å¸å¯ä»¥è‡ªåŠ¨åŒ–æ›´å¹¿æ³›çš„ä»»åŠ¡ï¼Œä»Žè€Œå®žçŽ°**æ•ˆçŽ‡å’Œç”Ÿäº§åŠ›çš„æå‡**ï¼ˆâ€œæˆ‘ä¸å†å…³å¿ƒæ¯å°æ—¶æœ‰1ä¸‡æ¬¡APIè°ƒç”¨â€ï¼‰ã€‚\n* æ–°çš„è¾¹ç¼˜AIç¡¬ä»¶ç»“åˆæ›´å°çš„æ¨¡åž‹å°†å¯¼è‡´**æ–°çš„è¾¹ç¼˜AIç”¨ä¾‹**å˜å¾—å¯è¡Œï¼Œè¿™äº›ç”¨ä¾‹ä¹‹å‰ä»…é™äºŽâ€œæ•°æ®ä¸­å¿ƒâ€ã€‚\n* éšç€è¾¹ç¼˜ç¡¬ä»¶çš„å¿«é€Ÿå‘å±•ï¼Œæˆ‘ä»¬ç›¸ä¿¡æœ‰æœºä¼šå»ºç«‹è½¯ä»¶å…¬å¸ï¼Œå¸®åŠ©å®¢æˆ·å°†AIæ¨¡åž‹å¸¦å…¥å®šåˆ¶è¾¹ç¼˜è®¾å¤‡çš„ç¢Žç‰‡åŒ–ç©ºé—´ï¼ˆâ€œæŠŠä½ çš„æ¨¡åž‹ç»™æˆ‘ï¼Œæˆ‘ç”¨å„ç§æŠ€æœ¯è¿›è¡ŒåŽ‹ç¼©ï¼Œåœ¨10ç§ä¸åŒçš„è¾¹ç¼˜è®¾å¤‡ä¸Šæµ‹è¯•ï¼Œå‘Šè¯‰ä½ å“ªä¸ªæ•ˆæžœæœ€å¥½ï¼Œç„¶åŽå¸®åŠ©ä½ éƒ¨ç½²â€ï¼‰ã€‚\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/leveraging-gemini-1-5-api-for-automated-test-case-generation-reverse-engineering-2ee8789f01db","frontmatter":{"title":"åˆ©ç”¨ Gemini 1.5 API è¿›è¡Œè‡ªåŠ¨æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆé€†å‘å·¥ç¨‹","meta_title":"åˆ©ç”¨ Gemini 1.5 API è¿›è¡Œè‡ªåŠ¨æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆé€†å‘å·¥ç¨‹","description":"è¯¥æµ‹è¯•æŽ¢ç´¢ä½¿ç”¨ Gemini API å’Œ Google Apps Script è‡ªåŠ¨åˆ›å»ºç¤ºä¾‹è¾“å…¥ï¼Œä»¥ä¾¿æ›´å¿«åœ°è¿›è¡Œè„šæœ¬é€†å‘å·¥ç¨‹ã€‚","date":"2024-11-10T03:51:17.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*fTtML3Sm1TuQNhQP.jpg","categories":["Programming","Programming/Scripting","Technology/WebAPI"],"author":"Rifx.Online","tags":["Gemini","API","automation","reverse-engineering","scripts"],"draft":false,"slug":"blog/leveraging-gemini-1-5-api-for-automated-test-case-generation-reverse-engineering-2ee8789f01db"},"content":"\n\n\n\n\n## æ‘˜è¦\n\næœ¬æŠ¥å‘ŠæŽ¢è®¨äº†åˆ©ç”¨ Gemini 1.5 API ä¸Ž Google Apps Script ç»“åˆï¼Œè‡ªåŠ¨åŒ–è„šæœ¬é€†å‘å·¥ç¨‹ä¸­çš„ç¤ºä¾‹è¾“å…¥åˆ›å»ºã€‚ä¼ ç»Ÿä¸Šï¼Œè¿™ä¸€è¿‡ç¨‹æ˜¯æ‰‹åŠ¨ä¸”è€—æ—¶çš„ï¼Œç‰¹åˆ«æ˜¯å¯¹äºŽå…·æœ‰å¤§é‡æµ‹è¯•ç”¨ä¾‹çš„å‡½æ•°ã€‚é€šè¿‡å°†é€†å‘å·¥ç¨‹æŠ€æœ¯åº”ç”¨äºŽ Google Apps Script ç¤ºä¾‹ï¼ŒæŽ¢è®¨äº† Gemini 1.5 API åœ¨è‡ªåŠ¨åŒ–è¾“å…¥ç”Ÿæˆæ–¹é¢ç®€åŒ–å¼€å‘çš„æ½œåŠ›ã€‚\n\n## ä»‹ç»\n\néšç€ Gemini 1\\.5 API çš„å‘å¸ƒï¼Œç”¨æˆ·èŽ·å¾—äº†å¤„ç†æ›´å¤æ‚æ•°æ®çš„èƒ½åŠ›ï¼Œä¸ºå„ç§åº”ç”¨å¼€å‘æ‰“å¼€äº†å¤§é—¨ã€‚æœ¬æŠ¥å‘ŠæŽ¢è®¨äº†å°† Gemini 1\\.5 API ä¸Ž Google Apps Script ç»“åˆä½¿ç”¨çš„æ½œåŠ›ï¼Œä»¥å®žçŽ°è„šæœ¬å¼€å‘å’Œæ”¹è¿›çš„é€†å‘å·¥ç¨‹ã€‚\n\nä¼ ç»Ÿä¸Šï¼Œè„šæœ¬å¼€å‘æ¶‰åŠæ‰‹åŠ¨æž„å»ºç¤ºä¾‹è¾“å…¥å€¼ã€‚è¿™ä¸ªè¿‡ç¨‹å¯èƒ½è€—æ—¶ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ›å»ºå‡½æ•°æˆ–æµ‹è¯•ä»Žåœ¨çº¿èµ„æºï¼ˆå¦‚ Stack Overflowï¼‰èŽ·å–çš„ä»£ç æ—¶ã€‚æ¯ä¸ªå‡½æ•°å¯èƒ½éœ€è¦å¤§é‡çš„æµ‹è¯•ç”¨ä¾‹ï¼Œæ‰‹åŠ¨ç”Ÿæˆè¿™äº›è¾“å…¥å¯èƒ½æˆä¸ºç“¶é¢ˆã€‚\n\nGemini 1\\.5 API æä¾›äº†ä¸€ä¸ªæ½œåœ¨çš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡è‡ªåŠ¨åŒ–ç¤ºä¾‹è¾“å…¥å€¼çš„åˆ›å»ºã€‚è¿™å¯ä»¥æ˜¾è‘—å‡å°‘å¼€å‘æ—¶é—´å’Œç²¾åŠ›ã€‚æœ¬æŠ¥å‘Šé€šè¿‡å°†é€†å‘å·¥ç¨‹æŠ€æœ¯åº”ç”¨äºŽå„ç§ä½¿ç”¨ Gemini 1\\.5 API çš„ Google Apps Script ç¤ºä¾‹ï¼Œæ¥è°ƒæŸ¥è¿™ä¸€å¯èƒ½æ€§ã€‚\n\nåœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†æŽ¢è®¨å¦‚ä½•ä½¿ç”¨ Gemini 1\\.5 API è‡ªåŠ¨åŒ–ç”Ÿæˆç¤ºä¾‹è¾“å…¥å€¼ï¼Œä»¥è¿›è¡Œç¼–å†™åœ¨ Google Apps Script ä¸­çš„è„šæœ¬çš„é€†å‘å·¥ç¨‹ã€‚\n\n## ä½¿ç”¨\n\nä¸ºäº†æµ‹è¯•æ­¤è„šæœ¬ï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æµç¨‹è¿›è¡Œã€‚\n\n## 1\\. åˆ›å»º API å¯†é’¥\n\nè¯·è®¿é—® [https://ai.google.dev/gemini\\-api/docs/api\\-key](https://ai.google.dev/gemini-api/docs/api-key) å¹¶åˆ›å»ºæ‚¨çš„ API å¯†é’¥ã€‚å±Šæ—¶ï¼Œè¯·åœ¨ API æŽ§åˆ¶å°å¯ç”¨ç”Ÿæˆè¯­è¨€ APIã€‚æ­¤ API å¯†é’¥ç”¨äºŽæ­¤ç¤ºä¾‹è„šæœ¬ã€‚\n\nè¯¥å®˜æ–¹æ–‡æ¡£ä¹Ÿå¯ä»¥æŸ¥çœ‹ã€‚ [å‚è€ƒ](https://ai.google.dev/)ã€‚\n\n## 2\\. åˆ›å»º Google Apps Script é¡¹ç›®\n\nåœ¨æœ¬æŠ¥å‘Šä¸­ï¼Œä½¿ç”¨äº† Google Apps Scriptã€‚å½“ç„¶ï¼Œæœ¬æŠ¥å‘Šä¸­ä»‹ç»çš„æ–¹æ³•ä¹Ÿå¯ä»¥ç”¨äºŽå…¶ä»–è¯­è¨€ã€‚\n\nåœ¨è¿™é‡Œï¼Œä¸ºäº†æµ‹è¯•ä»¥ä¸‹ç¤ºä¾‹è„šæœ¬ï¼Œè¯·åˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„ Google Apps Script é¡¹ç›®ã€‚å½“ç„¶ï¼Œæ­¤è„šæœ¬ä¹Ÿå¯ä»¥ä¸Žå®¹å™¨ç»‘å®šè„šæœ¬ä¸€èµ·ä½¿ç”¨ã€‚\n\nè¯·æ‰“å¼€ Google Apps Script é¡¹ç›®çš„è„šæœ¬ç¼–è¾‘å™¨ã€‚\n\n## 3\\. å®‰è£… Google Apps Script åº“\n\nä¸ºäº†æ–¹ä¾¿è®¿é—® Gemini APIï¼Œæˆ‘åˆ›å»ºäº†ä¸€ä¸ª Google Apps Script åº“ [GeminiWithFiles](https://github.com/tanaikech/GeminiWithFiles)ã€‚åœ¨ä»¥ä¸‹ç¤ºä¾‹è„šæœ¬ä¸­ï¼Œå°†ä½¿ç”¨è¯¥åº“ã€‚å› æ­¤ï¼Œè¯·å®‰è£…å®ƒã€‚æ‚¨å¯ä»¥åœ¨ [è¿™é‡Œ](https://github.com/tanaikech/GeminiWithFiles?tab=readme-ov-file#1-use-geminiwithfiles-as-a-google-apps-script-library) æŸ¥çœ‹å®‰è£…æ–¹æ³•ã€‚\n\n## 4\\. ç¤ºä¾‹è„šæœ¬ 1\n\nç¤ºä¾‹å‡½æ•°é€‰è‡ª [æˆ‘çš„ä»£ç åº“](https://github.com/tanaikech/UtlApp)ã€‚\n\n* [transpose](https://github.com/tanaikech/UtlApp?tab=readme-ov-file#transpose): è½¬ç½®äºŒç»´æ•°ç»„ã€‚\n* [removeDuplicatedValues](https://github.com/tanaikech/UtlApp?tab=readme-ov-file#removeduplicatedvalues): ä»Žä¸€ç»´æ•°ç»„ä¸­ç§»é™¤é‡å¤å€¼ã€‚\n* [compilingNumbers](https://github.com/tanaikech/UtlApp?tab=readme-ov-file#compilingnumbers): ä½¿ç”¨ Google Apps Script ç¼–è¯‘è¿žç»­æ•°å­—ã€‚\n* [unpivot](https://github.com/tanaikech/UtlApp?tab=readme-ov-file#unpivot): å°†äºŒç»´æ•°ç»„è½¬æ¢ä¸ºéžé€è§†ï¼ˆåé€è§†ï¼‰ã€‚\n* [expandA1Notations](https://github.com/tanaikech/UtlApp?tab=readme-ov-file#expanda1notations): æ­¤æ–¹æ³•ç”¨äºŽæ‰©å±• A1 è¡¨ç¤ºæ³•ã€‚\n\nä¸‹é¢æä¾›äº†æ¼”ç¤ºè¿™äº›å‡½æ•°çš„ç¤ºä¾‹è„šæœ¬ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæ‰€æœ‰å‡½æ•°å¯ä»¥åœ¨ä¸€æ¬¡ API è°ƒç”¨ä¸­æ‰§è¡Œã€‚å½“æˆ‘è¿è¡Œè¿™ä¸ªè„šæœ¬æ—¶ï¼Œå®ƒè¿”å›žäº†æ€»å…± 2,880 ä¸ªä»¤ç‰Œã€‚\n\nç¤ºä¾‹é¦–å…ˆä½¿ç”¨ Gemini åˆ›å»ºè¾“å…¥å€¼ã€‚ä¸ºäº†æµ‹è¯•è¿™äº›å€¼ï¼Œè„šæœ¬éšåŽå°†å®ƒä»¬ä¸Žåœ¨ Google Apps Script ä¸­å®žçŽ°çš„å‡½æ•°ä¸€èµ·ä½¿ç”¨ã€‚æœ€åŽï¼Œè¾“å…¥å’Œè¾“å‡ºå€¼éƒ½è¢«æ‰“å°å‡ºæ¥ã€‚\n\nè¿™é‡Œä½¿ç”¨ JSON æ¨¡å¼ç”Ÿæˆå†…å®¹ã€‚è¿™ç¡®ä¿äº† Gemini ç¨³å®šç”Ÿæˆå¤æ‚çš„ JSON å¯¹è±¡ã€‚[å‚è€ƒ](https://readmedium.com/taming-the-wild-output-effective-control-of-gemini-api-response-formats-with-response-mime-type-da273c08be85)å› æ­¤ï¼Œæˆ‘é€‰æ‹©åœ¨è¿™ä¸ªå®žä¾‹ä¸­ä½¿ç”¨å®ƒã€‚\n\n```python\nfunction myFunction() {\n\n  const apiKey = \"###\"; // Please set your API key.\n\n  const functionObj = {\n    transpose: function transpose(array) {\n      /**\n       * ### Description\n       * When the inputted array is 2 dimensional array, true is returned.\n       *\n       * @param {Array} array 2 dimensional array.\n       * @return {Boolean} When the inputted array is 2 dimensional array, true is returned.\n       */\n      function is2DimensionalArray(array) {\n        return array.every((r) => Array.isArray(r));\n      }\n\n      /**\n       * ### Description\n       * Transpose 2 dimensional array.\n       *\n       * @param {Array} array 2 dimensional array.\n       * @param {Boolean} check Check whether the inputted array is 2 dimensional array. Default is true.\n       * @return {Array} Transposed array.\n       */\n      function transpose(array, check = true) {\n        if (check && !is2DimensionalArray(array)) {\n          throw new Error(\"Please use 2 dimensional array.\");\n        }\n        return array[0].map((_, col) => array.map((row) => row[col] || null));\n      }\n      return transpose(array);\n    },\n    removeDuplicatedValues: function removeDuplicatedValues(array) {\n      /**\n       * ### Description\n       * Remove duplicated values from 1 dimensional array.\n       *\n       * @param {Array} array 1 dimensional array.\n       * @return {Object} Object including removeDuplicatedValues, duplicatedValues and numberOfDuplicate.\n       */\n      function removeDuplicatedValues(array) {\n        if (!Array.isArray(array)) {\n          throw new Error(\"Please use 1 dimensional array.\");\n        }\n        const obj = array.reduce(\n          (m, e) => m.set(e, m.has(e) ? m.get(e) + 1 : 1),\n          new Map()\n        );\n        const e = [...obj.entries()];\n        return {\n          removeDuplicatedValues: [...obj.keys()],\n          duplicatedValues: e.reduce((ar, [k, v]) => {\n            if (v != 1) ar.push(k);\n            return ar;\n          }, []),\n          numberOfDuplicate: Object.fromEntries(e),\n        };\n      }\n      return removeDuplicatedValues(array);\n    },\n    compilingNumbers: function compilingNumbers(array) {\n      /**\n       * ### Description\n       * Compiling Continuous Numbers using Google Apps Script.\n       *\n       * @param {Array} array Input array.\n       * @return {Array} Array including object like [{\"start\":1,\"end\":1},{\"start\":3,\"end\":5},{\"start\":7,\"end\":7},{\"start\":9,\"end\":11},{\"start\":13,\"end\":13}].\n       */\n      function compilingNumbers(array) {\n        if (!(Array.isArray(array) && array.every((e) => !isNaN(e)))) {\n          throw new Error(\"Please give an array including numbers.\");\n        }\n        const { values } = [...new Set(array.sort((a, b) => a - b))].reduce(\n          (o, e, i, a) => {\n            if (\n              o.temp.length == 0 ||\n              (o.temp.length > 0 && e == o.temp[o.temp.length - 1] + 1)\n            ) {\n              o.temp.push(e);\n            } else {\n              if (o.temp.length > 0) {\n                o.values.push({\n                  start: o.temp[0],\n                  end: o.temp[o.temp.length - 1],\n                });\n              }\n              o.temp = [e];\n            }\n            if (i == a.length - 1) {\n              o.values.push(\n                o.temp.length > 1\n                  ? { start: o.temp[0], end: o.temp[o.temp.length - 1] }\n                  : { start: e, end: e }\n              );\n            }\n            return o;\n          },\n          { temp: [], values: [] }\n        );\n        return values;\n      }\n      return compilingNumbers(array);\n    },\n    unpivot: function unpivot(values) {\n      /**\n       * ### Description\n       * When the inputted array is 2 dimensional array, true is returned.\n       *\n       * @param {Array} array 2 dimensional array.\n       * @return {Boolean} When the inputted array is 2 dimensional array, true is returned.\n       */\n      function is2DimensionalArray(array) {\n        return array.every((r) => Array.isArray(r));\n      }\n\n      /**\n       * ### Description\n       * Converting 2-dimensional array as unpivot (reverse pivot).\n       *\n       * @param {Array} values 2 dimensional array.\n       * @return {Array} 2 dimensional array converted as unpivot (reverse pivot).\n       */\n      function unpivot(values) {\n        if (!Array.isArray(values) || !is2DimensionalArray(values)) {\n          throw new Error(\"Please give an array of values.\");\n        }\n        const [[, ...h], ...v] = values;\n        return h.flatMap((hh, i) => v.map((t) => [hh, t[0], t[i + 1]]));\n      }\n      return unpivot(values);\n    },\n    expandA1Notations: function expandA1Notations(a1Notations) {\n      /**\n       * ### Description\n       * Converting colum letter to column index. Start of column index is 0.\n       * @param {String} letter Column letter.\n       * @return {Number} Column index.\n       */\n      function columnLetterToIndex(letter = null) {\n        if (letter === null || typeof letter != \"string\") {\n          throw new Error(\"Please give the column letter as a string.\");\n        }\n        letter = letter.toUpperCase();\n        return [...letter].reduce(\n          (c, e, i, a) =>\n            (c += (e.charCodeAt(0) - 64) * Math.pow(26, a.length - i - 1)),\n          -1\n        );\n      }\n\n      /**\n       * ### Description\n       * Converting colum index to column letter. Start of column index is 0.\n       * Ref: https://stackoverflow.com/a/53678158/7108653\n       * @param {Number} index Column index.\n       * @return {String} Column letter.\n       */\n      function columnIndexToLetter(index = null) {\n        if (index === null || isNaN(index)) {\n          throw new Error(\n            \"Please give the column indexr as a number. In this case, 1st number is 0.\"\n          );\n        }\n        return (a = Math.floor(index / 26)) >= 0\n          ? columnIndexToLetter(a - 1) + String.fromCharCode(65 + (index % 26))\n          : \"\";\n      }\n\n      /**\n       * ### Description\n       * This method is used for expanding A1Notations.\n       * @param {Array} a1Notations Array including A1Notations.\n       * @return {Array} Array including the expanded A1Notations.\n       */\n      function expandA1Notations(a1Notations, maxRow = \"10\", maxColumn = \"Z\") {\n        if (!Array.isArray(a1Notations) || a1Notations.length == 0) {\n          throw new Error(\"Please give a1Notations (Array).\");\n        }\n        const reg1 = new RegExp(\"^([A-Z]+)([0-9]+)$\");\n        const reg2 = new RegExp(\"^([A-Z]+)$\");\n        const reg3 = new RegExp(\"^([0-9]+)$\");\n        return a1Notations.map((e) => {\n          const a1 = e.split(\"!\");\n          const r = a1.length > 1 ? a1[1] : a1[0];\n          const [r1, r2] = r.split(\":\");\n          if (!r2) return [r1];\n          let rr;\n          if (reg1.test(r1) && reg1.test(r2)) {\n            rr = [r1.toUpperCase().match(reg1), r2.toUpperCase().match(reg1)];\n          } else if (reg2.test(r1) && reg2.test(r2)) {\n            rr = [\n              [null, r1, 1],\n              [null, r2, maxRow],\n            ];\n          } else if (reg1.test(r1) && reg2.test(r2)) {\n            rr = [r1.toUpperCase().match(reg1), [null, r2, maxRow]];\n          } else if (reg2.test(r1) && reg1.test(r2)) {\n            rr = [[null, r1, maxRow], r2.toUpperCase().match(reg1)];\n          } else if (reg3.test(r1) && reg3.test(r2)) {\n            rr =\n              Number(r1) > Number(r2)\n                ? [\n                    [null, \"A\", r2],\n                    [null, maxColumn, r1],\n                  ]\n                : [\n                    [null, \"A\", r1],\n                    [null, maxColumn, r2],\n                  ];\n          } else if (reg1.test(r1) && reg3.test(r2)) {\n            rr = [r1.toUpperCase().match(reg1), [null, maxColumn, r2]];\n          } else if (reg3.test(r1) && reg1.test(r2)) {\n            let temp = r2.toUpperCase().match(reg1);\n            rr =\n              Number(temp[2]) > Number(r1)\n                ? [\n                    [null, temp[1], r1],\n                    [null, maxColumn, temp[2]],\n                  ]\n                : [temp, [null, maxColumn, r1]];\n          } else {\n            throw new Error(\"Wrong a1Notation: \" + r);\n          }\n          const obj = {\n            startRowIndex: Number(rr[0][2]),\n            endRowIndex:\n              rr.length == 1 ? Number(rr[0][2]) + 1 : Number(rr[1][2]) + 1,\n            startColumnIndex: columnLetterToIndex(rr[0][1]),\n            endColumnIndex:\n              rr.length == 1\n                ? columnLetterToIndex(rr[0][1]) + 1\n                : columnLetterToIndex(rr[1][1]) + 1,\n          };\n          let temp = [];\n          for (let i = obj.startRowIndex; i < obj.endRowIndex; i++) {\n            for (let j = obj.startColumnIndex; j < obj.endColumnIndex; j++) {\n              temp.push(columnIndexToLetter(j) + i);\n            }\n          }\n          return temp;\n        });\n      }\n      return expandA1Notations(a1Notations);\n    },\n  };\n\n  const g = GeminiWithFiles.geminiWithFiles({\n    apiKey,\n    response_mime_type: \"application/json\",\n    doCountToken: true,\n  });\n\n  const functions = Object.entries(functionObj)\n    .map(\n      ([k, v]) =>\n        `<FunctionName>${k}</FunctionName><Function>${v.toString()}</Function>`\n    )\n    .join(\"\");\n  const jsonSchema = {\n    title: \"5 input values for giving each function\",\n    description: `Proposal 5 input values for giving each function. ${functions} Don't propose \"empty\", \"null\", \"undefined\" as values.`,\n    type: \"array\",\n    items: {\n      type: \"object\",\n      properties: {\n        functionName: { description: \"Function name\", type: \"string\" },\n        inputValues: {\n          description: `Proposed 5 input values. Don't propose \"empty\", \"null\", \"undefined\" as values.`,\n          type: \"array\",\n          items: {\n            description: \"Proposed input value\",\n            type: \"array|object|string|number\",\n          },\n        },\n      },\n      additionalProperties: false,\n    },\n  };\n  let res = g.generateContent({ jsonSchema });\n  if (typeof res == \"string\") {\n    try {\n      res = JSON.parse(res);\n    } catch ({ stack }) {\n      console.error(stack);\n      return;\n    }\n  }\n  const result = res.reduce((o, { functionName, inputValues }) => {\n    try {\n      o[functionName] = [];\n      inputValues.forEach((input) => {\n        const output = functionObj[functionName](input);\n        o[functionName].push({ input, output });\n      });\n    } catch ({ stack }) {\n      console.log(stack);\n    }\n    return o;\n  }, {});\n  console.log(JSON.stringify(result));\n}\n```\nè¿è¡Œæ­¤è„šæœ¬åŽï¼ŒèŽ·å¾—ä»¥ä¸‹ç»“æžœã€‚å¯ä»¥çœ‹åˆ°æœ‰æ•ˆçš„è¾“å…¥å’Œè¾“å‡ºå€¼å·²è¢«åˆ›å»ºã€‚\n\n```python\n{\n  \"transpose\": [\n    { \"input\": [[1, 2], [3, 4]], \"output\": [[1, 3], [2, 4]] },\n    { \"input\": [[\"a\", \"b\"], [\"c\", \"d\"]], \"output\": [[\"a\", \"c\"], [\"b\", \"d\"]] },\n    { \"input\": [[\"a1\", \"b1\"], [\"c1\", \"d1\"], [\"e1\", \"f1\"]], \"output\": [[\"a1\", \"c1\", \"e1\"], [\"b1\", \"d1\", \"f1\"]] },\n    { \"input\": [[true, false], [false, true]], \"output\": [[true, null], [null, true]] },\n    { \"input\": [[1, \"a\"], [\"c\", true]], \"output\": [[1, \"c\"], [\"a\", true]] }\n  ],\n\n  \"removeDuplicatedValues\": [\n    { \"input\": [1, 2, 3, 4, 5], \"output\": { \"removeDuplicatedValues\": [1, 2, 3, 4, 5], \"duplicatedValues\": [], \"numberOfDuplicate\": { \"1\": 1, \"2\": 1, \"3\": 1, \"4\": 1, \"5\": 1 } } },\n    { \"input\": [\"a\", \"b\", \"c\", \"d\", \"e\"], \"output\": { \"removeDuplicatedValues\": [\"a\", \"b\", \"c\", \"d\", \"e\"], \"duplicatedValues\": [], \"numberOfDuplicate\": { \"a\": 1, \"b\": 1, \"c\": 1, \"d\": 1, \"e\": 1 } } },\n    { \"input\": [1, 2, 1, 3, 2, 4, 3, 5, 4], \"output\": { \"removeDuplicatedValues\": [1, 2, 3, 4, 5], \"duplicatedValues\": [1, 2, 3, 4], \"numberOfDuplicate\": { \"1\": 2, \"2\": 2, \"3\": 2, \"4\": 2, \"5\": 1 } } },\n    { \"input\": [\"a\", \"b\", \"a\", \"c\", \"b\", \"d\", \"c\", \"e\", \"d\"], \"output\": { \"removeDuplicatedValues\": [\"a\", \"b\", \"c\", \"d\", \"e\"], \"duplicatedValues\": [\"a\", \"b\", \"c\", \"d\"], \"numberOfDuplicate\": { \"a\": 2, \"b\": 2, \"c\": 2, \"d\": 2, \"e\": 1 } } },\n    { \"input\": [1, \"a\", 2, \"b\", 1, \"c\", 2, \"d\", 1, \"e\"], \"output\": { \"removeDuplicatedValues\": [1, \"a\", 2, \"b\", \"c\", \"d\", \"e\"], \"duplicatedValues\": [1, 2], \"numberOfDuplicate\": { \"1\": 3, \"2\": 2, \"a\": 1, \"b\": 1, \"c\": 1, \"d\": 1, \"e\": 1 } } }\n  ],\n\n  \"compilingNumbers\": [\n    { \"input\": [1, 2, 3, 4, 5], \"output\": [{ \"start\": 1, \"end\": 5 }] },\n    { \"input\": [1, 3, 5, 7, 9, 11, 13], \"output\": [{ \"start\": 1, \"end\": 1 }, { \"start\": 3, \"end\": 3 }, { \"start\": 5, \"end\": 5 }, { \"start\": 7, \"end\": 7 }, { \"start\": 9, \"end\": 9 }, { \"start\": 11, \"end\": 11 }, { \"start\": 13, \"end\": 13 }] },\n    { \"input\": [1, 3, 5, 7, 8, 10, 12, 13], \"output\": [{ \"start\": 1, \"end\": 1 }, { \"start\": 3, \"end\": 3 }, { \"start\": 5, \"end\": 5 }, { \"start\": 7, \"end\": 8 }, { \"start\": 10, \"end\": 10 }, { \"start\": 12, \"end\": 13 }] },\n    { \"input\": [1, 2, 4, 5, 7, 8, 10, 11, 13, 14], \"output\": [{ \"start\": 1, \"end\": 2 }, { \"start\": 4, \"end\": 5 }, { \"start\": 7, \"end\": 8 }, { \"start\": 10, \"end\": 11 }, { \"start\": 13, \"end\": 14 }] },\n    { \"input\": [1, 2, 3, 5, 6, 8, 9, 11, 12, 14, 15], \"output\": [{ \"start\": 1, \"end\": 3 }, { \"start\": 5, \"end\": 6 }, { \"start\": 8, \"end\": 9 }, { \"start\": 11, \"end\": 12 }, { \"start\": 14, \"end\": 15 }] }\n  ],\n\n  \"unpivot\": [\n    { \"input\": [[\"name\", \"score1\", \"score2\"], [\"sample1\", 100, 80], [\"sample2\", 90, 70]], \"output\": [[\"score1\", \"sample1\", 100], [\"score1\", \"sample2\", 90], [\"score2\", \"sample1\", 80], [\"score2\", \"sample2\", 70]] },\n    { \"input\": [[\"name\", \"score1\", \"score2\", \"score3\"], [\"sample1\", 100, 80, 70], [\"sample2\", 90, 70, 80]], \"output\": [[\"score1\", \"sample1\", 100], [\"score1\", \"sample2\", 90], [\"score2\", \"sample1\", 80], [\"score2\", \"sample2\", 70], [\"score3\", \"sample1\", 70], [\"score3\", \"sample2\", 80]] },\n    { \"input\": [[\"id\", \"x\", \"y\", \"z\"], [\"a\", 1, 2, 3], [\"b\", 4, 5, 6]], \"output\": [[\"x\", \"a\", 1], [\"x\", \"b\", 4], [\"y\", \"a\", 2], [\"y\", \"b\", 5], [\"z\", \"a\", 3], [\"z\", \"b\", 6]] },\n    { \"input\": [[\"id\", \"x\", \"y\", \"z\", \"xx\", \"yy\", \"zz\"], [\"a\", 1, 2, 3, 10, 20, 30], [\"b\", 4, 5, 6, 40, 50, 60]], \"output\": [[\"x\", \"a\", 1], [\"x\", \"b\", 4], [\"y\", \"a\", 2], [\"y\", \"b\", 5], [\"z\", \"a\", 3], [\"z\", \"b\", 6], [\"xx\", \"a\", 10], [\"xx\", \"b\", 40], [\"yy\", \"a\", 20], [\"yy\", \"b\", 50], [\"zz\", \"a\", 30], [\"zz\", \"b\", 60]] },\n    { \"input\": [[\"Fruit\", \"2021\", \"2022\", \"2023\"], [\"apple\", 100, 120, 150], [\"orange\", 80, 90, 100]], \"output\": [[\"2021\", \"apple\", 100], [\"2021\", \"orange\", 80], [\"2022\", \"apple\", 120], [\"2022\", \"orange\", 90], [\"2023\", \"apple\", 150], [\"2023\", \"orange\", 100]] }\n  ],\n\n  \"expandA1Notations\": [\n    { \"input\": [\"A1:B5\", \"C3:D7\", \"E2:F10\"], \"output\": [[\"A1\", \"B1\", \"A2\", \"B2\", \"A3\", \"B3\", \"A4\", \"B4\", \"A5\", \"B5\"], [\"C3\", \"D3\", \"C4\", \"D4\", \"C5\", \"D5\", \"C6\", \"D6\", \"C7\", \"D7\"], [\"E2\", \"F2\", \"E3\", \"F3\", \"E4\", \"F4\", \"E5\", \"F5\", \"E6\", \"F6\", \"E7\", \"F7\", \"E8\", \"F8\", \"E9\", \"F9\", \"E10\", \"F10\"]] },\n    { \"input\": [\"A:B\", \"C:D\", \"E:F\"], \"output\": [[\"A1\", \"B1\", \"A2\", \"B2\", \"A3\", \"B3\", \"A4\", \"B4\", \"A5\", \"B5\", \"A6\", \"B6\", \"A7\", \"B7\", \"A8\", \"B8\", \"A9\", \"B9\", \"A10\", \"B10\"], [\"C1\", \"D1\", \"C2\", \"D2\", \"C3\", \"D3\", \"C4\", \"D4\", \"C5\", \"D5\", \"C6\", \"D6\", \"C7\", \"D7\", \"C8\", \"D8\", \"C9\", \"D9\", \"C10\", \"D10\"], [\"E1\", \"F1\", \"E2\", \"F2\", \"E3\", \"F3\", \"E4\", \"F4\", \"E5\", \"F5\", \"E6\", \"F6\", \"E7\", \"F7\", \"E8\", \"F8\", \"E9\", \"F9\", \"E10\", \"F10\"]] },\n    { \"input\": [\"A1:C5\"], \"output\": [[\"A1\", \"B1\", \"C1\", \"A2\", \"B2\", \"C2\", \"A3\", \"B3\", \"C3\", \"A4\", \"B4\", \"C4\", \"A5\", \"B5\", \"C5\"]] },\n    { \"input\": [\"A:C\"], \"output\": [[\"A1\", \"B1\", \"C1\", \"A2\", \"B2\", \"C2\", \"A3\", \"B3\", \"C3\", \"A4\", \"B4\", \"C4\", \"A5\", \"B5\", \"C5\", \"A6\", \"B6\", \"C6\", \"A7\", \"B7\", \"C7\", \"A8\", \"B8\", \"C8\", \"A9\", \"B9\", \"C9\", \"A10\", \"B10\", \"C10\"]] },\n    { \"input\": [\"1:5\", \"3:7\", \"2:10\"], \"output\": [[\"A1\", \"B1\", \"C1\", \"D1\", \"E1\", \"F1\", \"G1\", \"H1\", \"I1\", \"J1\", \"K1\", \"L1\", \"M1\", \"N1\", \"O1\", \"P1\", \"Q1\", \"R1\", \"S1\", \"T1\", \"U1\", \"V1\", \"W1\", \"X1\", \"Y1\", \"Z1\", \"A2\", \"B2\", \"C2\", \"D2\", \"E2\", \"F2\", \"G2\", \"H2\", \"I2\", \"J2\", \"K2\", \"L2\", \"M2\", \"N2\", \"O2\", \"P2\", \"Q2\", \"R2\", \"S2\", \"T2\", \"U2\", \"V2\", \"W2\", \"X2\", \"Y2\", \"Z2\", \"A3\", \"B3\", \"C3\", \"D3\", \"E3\", \"F3\", \"G3\", \"H3\", \"I3\", \"J3\", \"K3\", \"L3\", \"M3\", \"N3\", \"O3\", \"P3\", \"Q3\", \"R3\", \"S3\", \"T3\", \"U3\", \"V3\", \"W3\", \"X3\", \"Y3\", \"Z3\", \"A4\", \"B4\", \"C4\", \"D4\", \"E4\", \"F4\", \"G4\", \"H4\", \"I4\", \"J4\", \"K4\", \"L4\", \"M4\", \"N4\", \"O4\", \"P4\", \"Q4\", \"R4\", \"S4\", \"T4\", \"U4\", \"V4\", \"W4\", \"X4\", \"Y4\", \"Z4\", \"A5\", \"B5\", \"C5\", \"D5\", \"E5\", \"F5\", \"G5\", \"H5\", \"I5\", \"J5\", \"K5\", \"L5\", \"M5\", \"N5\", \"O5\", \"P5\", \"Q5\", \"R5\", \"S5\", \"T5\", \"U5\", \"V5\", \"W5\", \"X5\", \"Y5\", \"Z5\"], [\"A3\", \"B3\", \"C3\", \"D3\", \"E3\", \"F3\", \"G3\", \"H3\", \"I3\", \"J3\", \"K3\", \"L3\", \"M3\", \"N3\", \"O3\", \"P3\", \"Q3\", \"R3\", \"S3\", \"T3\", \"U3\", \"V3\", \"W3\", \"X3\", \"Y3\", \"Z3\", \"A4\", \"B4\", \"C4\", \"D4\", \"E4\", \"F4\", \"G4\", \"H4\", \"I4\", \"J4\", \"K4\", \"L4\", \"M4\", \"N4\", \"O4\", \"P4\", \"Q4\", \"R4\", \"S4\", \"T4\", \"U4\", \"V4\", \"W4\", \"X4\", \"Y4\", \"Z4\", \"A5\", \"B5\", \"C5\", \"D5\", \"E5\", \"F5\", \"G5\", \"H5\", \"I5\", \"J5\", \"K5\", \"L5\", \"M5\", \"N5\", \"O5\", \"P5\", \"Q5\", \"R5\", \"S5\", \"T5\", \"U5\", \"V5\", \"W5\", \"X5\", \"Y5\", \"Z5\", \"A6\", \"B6\", \"C6\", \"D6\", \"E6\", \"F6\", \"G6\", \"H6\", \"I6\", \"J6\", \"K6\", \"L6\", \"M6\", \"N6\", \"O6\", \"P6\", \"Q6\", \"R6\", \"S6\", \"T6\", \"U6\", \"V6\", \"W6\", \"X6\", \"Y6\", \"Z6\", \"A7\", \"B7\", \"C7\", \"D7\", \"E7\", \"F7\", \"G7\", \"H7\", \"I7\", \"J7\", \"K7\", \"L7\", \"M7\", \"N7\", \"O7\", \"P7\", \"Q7\", \"R7\", \"S7\", \"T7\", \"U7\", \"V7\", \"W7\", \"X7\", \"Y7\", \"Z7\"], [\"A2\", \"B2\", \"C2\", \"D2\", \"E2\", \"F2\", \"G2\", \"H2\", \"I2\", \"J2\", \"K2\", \"L2\", \"M2\", \"N2\", \"O2\", \"P2\", \"Q2\", \"R2\", \"S2\", \"T2\", \"U2\", \"V2\", \"W2\", \"X2\", \"Y2\", \"Z2\", \"A3\", \"B3\", \"C3\", \"D3\", \"E3\", \"F3\", \"G3\", \"H3\", \"I3\", \"J3\", \"K3\", \"L3\", \"M3\", \"N3\", \"O3\", \"P3\", \"Q3\", \"R3\", \"S3\", \"T3\", \"U3\", \"V3\", \"W3\", \"X3\", \"Y3\", \"Z3\", \"A4\", \"B4\", \"C4\", \"D4\", \"E4\", \"F4\", \"G4\", \"H4\", \"I4\", \"J4\", \"K4\", \"L4\", \"M4\", \"N4\", \"O4\", \"P4\", \"Q4\", \"R4\", \"S4\", \"T4\", \"U4\", \"V4\", \"W4\", \"X4\", \"Y4\", \"Z4\", \"A5\", \"B5\", \"C5\", \"D5\", \"E5\", \"F5\", \"G5\", \"H5\", \"I5\", \"J5\", \"K5\", \"L5\", \"M5\", \"N5\", \"O5\", \"P5\", \"Q5\", \"R5\", \"S5\", \"T5\", \"U5\", \"V5\", \"W5\", \"X5\", \"Y5\", \"Z5\", \"A6\", \"B6\", \"C6\", \"D6\", \"E6\", \"F6\", \"G6\", \"H6\", \"I6\", \"J6\", \"K6\", \"L6\", \"M6\", \"N6\", \"O6\", \"P6\", \"Q6\", \"R6\", \"S6\", \"T6\", \"U6\", \"V6\", \"W6\", \"X6\", \"Y6\", \"Z6\", \"A7\", \"B7\", \"C7\", \"D7\", \"E7\", \"F7\", \"G7\", \"H7\", \"I7\", \"J7\", \"K7\", \"L7\", \"M7\", \"N7\", \"O7\", \"P7\", \"Q7\", \"R7\", \"S7\", \"T7\", \"U7\", \"V7\", \"W7\", \"X7\", \"Y7\", \"Z7\", \"A8\", \"B8\", \"C8\", \"D8\", \"E8\", \"F8\", \"G8\", \"H8\", \"I8\", \"J8\", \"K8\", \"L8\", \"M8\", \"N8\", \"O8\", \"P8\", \"Q8\", \"R8\", \"S8\", \"T8\", \"U8\", \"V8\", \"W8\", \"X8\", \"Y8\", \"Z8\", \"A9\", \"B9\", \"C9\", \"D9\", \"E9\", \"F9\", \"G9\", \"H9\", \"I9\", \"J9\", \"K9\", \"L9\", \"M9\", \"N9\", \"O9\", \"P9\", \"Q9\", \"R9\", \"S9\", \"T9\", \"U9\", \"V9\", \"W9\", \"X9\", \"Y9\", \"Z9\", \"A10\", \"B10\", \"C10\", \"D10\", \"E10\", \"F10\", \"G10\", \"H10\", \"I10\", \"J10\", \"K10\", \"L10\", \"M10\", \"N10\", \"O10\", \"P10\", \"Q10\", \"R10\", \"S10\", \"T10\", \"U10\", \"V10\", \"W10\", \"X10\", \"Y10\", \"Z10\"]] }\n  ]\n}\n```\n\n## 5\\. ç¤ºä¾‹è„šæœ¬ 2\n\nä¸Šè¿°ç¤ºä¾‹è„šæœ¬çš„æ¯ä¸ªå‡½æ•°ä»…ä½¿ç”¨ä¸€ä¸ªå‚æ•°ã€‚å½“ä½¿ç”¨å¤šä¸ªå‚æ•°æ—¶ï¼Œè„šæœ¬å¦‚ä¸‹ã€‚ç¤ºä¾‹å‡½æ•°å¦‚ä¸‹ã€‚\n\n* [splitArray](https://github.com/tanaikech/UtlApp?tab=readme-ov-file#splitarray): æ¯ n é•¿åº¦æ‹†åˆ†æ•°ç»„ã€‚\n\n\n```python\nfunction myFunction() {\n\n  const apiKey = \"###\"; // Please set your API key.\n\n  const functionObj = {\n    splitArray: function splitArray(array, size) {\n      /**\n       * ### Description\n       * Split array every n length.\n       *\n       * @param {Array} array 2 dimensional array.\n       * @param {Boolean} check Check whether the inputted array is 2 dimensional array. Default is true.\n       * @return {Array} Transposed array.\n       */\n      function splitArray(array, size) {\n        if (!array || !size || !Array.isArray(array)) {\n          throw new Error(\"Please give an array and split size.\");\n        }\n        return [...Array(Math.ceil(array.length / size))].map((_) =>\n          array.splice(0, size)\n        );\n      }\n      return splitArray(array, size);\n    },\n  };\n\n  const g = GeminiWithFiles.geminiWithFiles({\n    apiKey,\n    response_mime_type: \"application/json\",\n    doCountToken: true,\n  });\n\n  const functions = Object.entries(functionObj)\n    .map(\n      ([k, v]) =>\n        `<FunctionName>${k}</FunctionName><Function>${v.toString()}</Function>`\n    )\n    .join(\"\");\n  const jsonSchema = {\n    title: \"5 input values for giving each function\",\n    description: `Proposal 5 input values for giving each function. ${functions} Don't propose \"empty\", \"null\", \"undefined\" as values.`,\n    type: \"array\",\n    items: {\n      type: \"object\",\n      properties: {\n        functionName: { description: \"Function name\", type: \"string\" },\n        inputValues: {\n          description: `Proposed 5 input values. Don't propose \"empty\", \"null\", \"undefined\" as values.`,\n          type: \"array\",\n          items: {\n            description: \"Proposed input value\",\n            type: \"array|object|string|number\",\n          },\n        },\n      },\n      additionalProperties: false,\n    },\n  };\n  let res = g.generateContent({ jsonSchema });\n  if (typeof res == \"string\") {\n    try {\n      res = JSON.parse(res);\n    } catch ({ stack }) {\n      console.error(stack);\n      return;\n    }\n  }\n  const result = res.reduce((o, { functionName, inputValues }) => {\n    try {\n      o[functionName] = [];\n      inputValues.forEach((input) => {\n        const temp = JSON.parse(JSON.stringify(input));\n        const output = functionObj[functionName](...temp);\n        o[functionName].push({ input, output });\n      });\n    } catch ({ stack }) {\n      console.log(stack);\n    }\n    return o;\n  }, {});\n  console.log(JSON.stringify(result));\n}\n```\nè¿è¡Œæ­¤è„šæœ¬æ—¶ï¼Œå°†èŽ·å¾—ä»¥ä¸‹ç»“æžœã€‚\n\n\n```python\n{\n  \"splitArray\": [\n    { \"input\": [[1, 2, 3, 4, 5, 6], 2], \"output\": [[1, 2], [3, 4], [5, 6]] },\n    { \"input\": [[\"a\", \"b\", \"c\", \"d\", \"e\"], 2], \"output\": [[\"a\", \"b\"], [\"c\", \"d\"], [\"e\"]] },\n    { \"input\": [[\"apple\", \"orange\", \"grape\", \"banana\", \"kiwi\"], 3], \"output\": [[\"apple\", \"orange\", \"grape\"], [\"banana\", \"kiwi\"]] },\n    { \"input\": [[true, false, true, false, true], 1], \"output\": [[true], [false], [true], [false], [true]] },\n    { \"input\": [[1.2, 3.14, 2.71, 0.577], 2], \"output\": [[1.2, 3.14], [2.71, 0.577]] }\n  ]\n}\n```\n\n## æ‘˜è¦\n\nä»Žä¸Šè¿°ç»“æžœæ¥çœ‹ï¼Œæˆ‘ä»¬å¯ä»¥ç¡®è®¤ä½¿ç”¨ Gemini API è¿›è¡Œé€†å‘å·¥ç¨‹çš„å¯èƒ½æ€§ã€‚è¿™ä¹Ÿè¡¨æ˜Ž Gemini API å¯ä»¥ç”¨äºŽå¼€å‘åº”ç”¨ç¨‹åºã€‚\n\n## æ³¨æ„\n\n* å¦‚æžœå‘ç”Ÿé”™è¯¯ï¼Œè¯·å†æ¬¡è¿è¡Œè„šæœ¬ã€‚æˆ–è€…ï¼Œè¯·è°ƒæ•´ JSON æž¶æž„ä¸­çš„æè¿°ã€‚\n* æˆ‘ç›¸ä¿¡è¿™ç§æ–¹æ³•ä¹Ÿå¯ä»¥ç”¨äºŽé™¤ Google Apps Script ä¹‹å¤–çš„å…¶ä»–è¯­è¨€ã€‚\n* åœ¨å½“å‰é˜¶æ®µï¼Œä¼¼ä¹Žä¾èµ–äºŽ Google Apps Script çš„ç±»å¯¹è±¡ï¼Œå¦‚ SpreadsheetAppã€DriveApp ç­‰ï¼Œæ— æ³•ç”¨ä½œè¾“å…¥å€¼ã€‚\n* é¡¶éƒ¨çš„æŠ½è±¡å›¾åƒæ˜¯ç”± [Gemini](https://gemini.google.com/app) åˆ›å»ºçš„ã€‚\n\n"},{"lang":"zh","group":"blog","slug":"blog/leveraging-large-language-models-llms-in-b2c-industries-transforming-customer-experience-with-4073990a6200","frontmatter":{"title":"åœ¨ B2C è¡Œä¸šä¸­åˆ©ç”¨å¤§åž‹è¯­è¨€æ¨¡åž‹ (LLM)ï¼šåœ¨ B2C è¡Œä¸šä¸­åˆ©ç”¨å¤§åž‹è¯­è¨€æ¨¡åž‹ (LLM)ï¼š...","meta_title":"åœ¨ B2C è¡Œä¸šä¸­åˆ©ç”¨å¤§åž‹è¯­è¨€æ¨¡åž‹ (LLM)ï¼šåœ¨ B2C è¡Œä¸šä¸­åˆ©ç”¨å¤§åž‹è¯­è¨€æ¨¡åž‹ (LLM)ï¼š...","description":"åœ¨B2Cè¡Œä¸šä¸­ï¼Œä¼ä¸šåˆ©ç”¨å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰å’Œè‡ªä¸»ä»£ç†æ˜¾è‘—æå‡å®¢æˆ·ä½“éªŒï¼Œå°¤å…¶åœ¨é‡‘èžæœåŠ¡ã€é›¶å”®å’Œç”µå­å•†åŠ¡é¢†åŸŸã€‚é€šè¿‡åŸºäºŽæ£€ç´¢å¢žå¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„æ–¹æ³•ï¼Œè¿™äº›ä»£ç†èƒ½å¤Ÿæä¾›å®žæ—¶ã€æ™ºèƒ½çš„å“åº”ï¼Œå¢žå¼ºå®¢æˆ·æ»¡æ„åº¦å¹¶é™ä½Žå¯¹äººå·¥æ”¯æŒçš„ä¾èµ–ã€‚æ–‡ç« è¯¦ç»†ä»‹ç»äº†å¦‚ä½•åˆ›å»ºä¸€ä¸ªå¤„ç†ä¿¡ç”¨å¡æŸ¥è¯¢çš„è‡ªä¸»ä»£ç†ï¼ŒåŒ…æ‹¬æ•°æ®æºçš„ä½¿ç”¨ã€åµŒå…¥ä¸Žå‘é‡æ•°æ®åº“çš„åˆ›å»ºã€æç¤ºå·¥ç¨‹çš„åº”ç”¨ï¼Œä»¥åŠå¦‚ä½•é€šè¿‡Flaskæˆ–Streamlitè¿›è¡Œéƒ¨ç½²ï¼Œå±•çŽ°äº†LLMsåœ¨æä¾›ä¸ªæ€§åŒ–å®¢æˆ·æœåŠ¡æ–¹é¢çš„æ½œåŠ›ã€‚","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Zf15fyqPpBcoEHf6G5rgbw.jpeg","categories":["Programming","Machine Learning","Chatbots"],"author":"Rifx.Online","tags":["LLMs","RAG","embeddings","vector","Flask"],"draft":false,"slug":"blog/leveraging-large-language-models-llms-in-b2c-industries-transforming-customer-experience-with-4073990a6200"},"content":"\n\n\n\n\nåœ¨é‡‘èžæœåŠ¡ã€é›¶å”®å’Œç”µå­å•†åŠ¡ç­‰B2Cè¡Œä¸šå¿«é€Ÿå‘å±•çš„çŽ¯å¢ƒä¸­ï¼Œå®¢æˆ·å¯¹ä¸ªæ€§åŒ–å’Œå³æ—¶å“åº”çš„æœŸæœ›è¾¾åˆ°äº†å‰æ‰€æœªæœ‰çš„é«˜åº¦ã€‚éšç€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„è¿›æ­¥ï¼Œå°¤å…¶æ˜¯å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰çš„å‘å±•ï¼Œä¼ä¸šåœ¨å¤„ç†å®¢æˆ·äº’åŠ¨æ–¹é¢å‘ç”Ÿäº†å‰§çƒˆå˜åŒ–ã€‚åœ¨é“¶è¡Œå’Œä¿¡ç”¨å¡æœåŠ¡ç­‰è¡Œä¸šï¼Œå®¢æˆ·ç»å¸¸å¯»æ±‚æœ‰å…³äº§å“ã€ç¦åˆ©æˆ–äº¤æ˜“çš„è¯¦ç»†ä¿¡æ¯ï¼Œå› æ­¤é‡‡ç”¨åŸºäºŽLLMçš„è‡ªä¸»ä»£ç†æä¾›äº†æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚è¿™äº›ä»£ç†èƒ½å¤Ÿæä¾›å®žæ—¶ã€æ™ºèƒ½çš„å“åº”ï¼Œè½¬å˜å®¢æˆ·å‚ä¸Žæ–¹å¼ï¼ŒåŒæ—¶æé«˜è¿è¥æ•ˆçŽ‡ã€‚\n\næ ¹æ®æˆ‘åœ¨é‡‘èžæœåŠ¡è¡Œä¸šAIäº§å“å¼€å‘çš„ç»éªŒï¼Œè¿™äº›åŸºäºŽLLMçš„ä»£ç†åœ¨æ­£ç¡®å®žæ–½æ—¶ï¼Œå¯ä»¥æˆä¸ºæ¸¸æˆè§„åˆ™çš„æ”¹å˜è€…ã€‚å®ƒä»¬æä¾›å¯æ‰©å±•çš„ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å®¢æˆ·æ”¯æŒï¼Œä¸ä»…æé«˜äº†å®¢æˆ·æ»¡æ„åº¦ï¼Œè¿˜å‡å°‘äº†å¯¹äººå·¥ä»£ç†çš„ä¾èµ–ã€‚ä½†æˆ‘ä»¬å¦‚ä½•å¼€å‘è¿™äº›æ™ºèƒ½ç³»ç»Ÿå‘¢ï¼Ÿä¸‹é¢ï¼Œæˆ‘å°†å¸¦æ‚¨äº†è§£åˆ›å»ºä¸€ä¸ªç”¨äºŽå¤„ç†ä¸Žä¿¡ç”¨å¡äº§å“ç›¸å…³çš„å®¢æˆ·æŸ¥è¯¢çš„ä»£ç†æ£€ç´¢å¢žå¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿçš„ä¸šåŠ¡é—®é¢˜ï¼Œå¹¶è§£é‡ŠLLMsã€åµŒå…¥ã€å‘é‡æ•°æ®åº“å’Œæç¤ºå·¥ç¨‹å¦‚ä½•åœ¨è¿™ä¸ªè§£å†³æ–¹æ¡ˆä¸­ç»“åˆåœ¨ä¸€èµ·ã€‚\n\n## å•†ä¸šé—®é¢˜ï¼šåˆ›å»ºä¸€ä¸ªç”¨äºŽä¿¡ç”¨å¡æŸ¥è¯¢çš„è‡ªä¸»ä»£ç†\n\næƒ³è±¡ä¸€ä¸‹ï¼Œä¸€å®¶ä¸»è¦çš„é‡‘èžæœåŠ¡å…¬å¸å‘å…¶å®¢æˆ·æä¾›å¤šç§ä¿¡ç”¨å¡äº§å“ã€‚å¤„ç†å®¢æˆ·å…³äºŽä¸åŒä¿¡ç”¨å¡äº§å“çš„ç‰¹æ€§ã€ç¦åˆ©ã€åˆ©çŽ‡å’Œå¥–åŠ±è®¡åˆ’çš„æŸ¥è¯¢æ˜¯ä¸€ä¸ªåŠ³åŠ¨å¯†é›†åž‹çš„è¿‡ç¨‹ã€‚ç›®æ ‡æ˜¯å¼€å‘ä¸€ä¸ªèƒ½å¤Ÿè‡ªä¸»ã€å‡†ç¡®å¹¶å…·å¤‡æ·±åˆ»ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›çš„äººå·¥æ™ºèƒ½ä»£ç†ï¼Œä»¥å¤„ç†å¤§é‡é—®é¢˜ã€‚\n\n### ç”¨äºŽä»£ç†RAGå¼€å‘çš„æ•°æ®æº\n\nå¯¹äºŽè¿™ä¸ªç”¨ä¾‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ¥è‡ªèŠ±æ——é“¶è¡Œçš„å…¬å…±æ•°æ®æºï¼Œå…¶ä¸­åŒ…å«ä¸€ç³»åˆ—ä¿¡ç”¨å¡äº§å“çš„è¯¦ç»†ä¿¡æ¯ï¼Œå¯ä»¥ä¿å­˜ä¸ºPDFæ ¼å¼ã€‚è¿™äº›æ–‡æ¡£åŒ…å«å›žç­”å®¢æˆ·å…³äºŽèŠ±æ——é“¶è¡Œä¿¡ç”¨å¡äº§å“æŸ¥è¯¢æ‰€éœ€çš„ä¿¡æ¯ï¼š[èŠ±æ——ä¿¡ç”¨å¡æ¦‚è¿°](https://www.citi.com/credit-cards/compare/view-all-credit-cards?intc=citicard_vac_202405_AB)ã€‚å®Œæ•´çš„ä»£ç åº“å’Œé€æ­¥çš„ç¬”è®°æœ¬å¯ä»¥åœ¨è¿™ä¸ª[gitä»“åº“](https://github.com/nitsourish/Conversational_AIchatbot)ä¸­æ‰¾åˆ°ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*rNgsnBXq0R-RnKfSVCQ26Q.png)\n\n## åµŒå…¥ä¸Žå‘é‡æ•°æ®åº“åˆ›å»º\n\nä¸ºäº†ä½¿AIä»£ç†èƒ½å¤Ÿä»Žå¯ç”¨çš„äº§å“PDFä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼Œç¬¬ä¸€æ­¥æ˜¯åˆ›å»ºåµŒå…¥ã€‚åµŒå…¥æ˜¯æ–‡æœ¬çš„å‘é‡è¡¨ç¤ºï¼Œå…è®¸æ¨¡åž‹åœ¨è¿žç»­çš„å‘é‡ç©ºé—´ä¸­æ•æ‰å•è¯ã€çŸ­è¯­ç”šè‡³å®Œæ•´æ–‡æ¡£çš„è¯­ä¹‰æ„ä¹‰ã€‚\n\nåœ¨è¿™ä¸ªç”¨ä¾‹ä¸­ï¼Œä¸‹è½½å¹¶å¤„ç†åŒ…å«ä¸åŒä¿¡ç”¨å¡è¯¦ç»†ä¿¡æ¯çš„PDFæ–‡ä»¶ã€‚ä½¿ç”¨é¢„è®­ç»ƒè¯­è¨€æ¨¡åž‹ **text-embedding-3-small**ï¼Œæˆ‘ä»¬å°†æ–‡æœ¬æ•°æ®è½¬æ¢ä¸ºç¨ å¯†çš„å‘é‡è¡¨ç¤ºã€‚è¿™äº›å‘é‡å­˜å‚¨åœ¨å‘é‡æ•°æ®åº“ä¸­ï¼Œä»Žè€Œå®žçŽ°é«˜æ•ˆçš„ç›¸ä¼¼æ€§æœç´¢ã€‚\n\n**å…³é”®æ­¥éª¤ï¼š**\n\n1. **æ•°æ®æ‘„å–**ï¼šè§£æžå¹¶å°†èŠ±æ——é“¶è¡Œä¿¡ç”¨å¡äº§å“çš„PDFè½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼ã€‚\n\n```python\nfor file in os.listdir(\"../credit_card_products\"):\n    if file.endswith(\".pdf\"):\n        loaders.append(file)     \npdf_loaders = [PyPDFLoader(f\"../credit_card_products/{file}\") for file in loaders]\n\npages = []\n\nfor loader in pdf_loaders:\n    pages.extend(loader.load())\n```\n**2\\. åˆ‡åˆ†**ï¼šå°†æ–‡æœ¬æ‹†åˆ†ä¸ºå—ï¼Œä½¿ç”¨æ¢è¡Œç¬¦ (`\"\\n\"`) ä½œä¸ºåˆ†éš”ç¬¦ã€‚æ¯ä¸ªå—å…·æœ‰ä¸€å®šçš„å­—ç¬¦é‡å ã€‚è¿™æœ‰åŠ©äºŽç¡®ä¿åœ¨åµŒå…¥æˆ–æ£€ç´¢ç­‰ä¸‹æ¸¸å¤„ç†è¿‡ç¨‹ä¸­çš„æ–‡æœ¬åˆ†å‰²æ›´ä¸ºé¡ºç•…ã€‚\n\n```python\ntext_splitter = CharacterTextSplitter(\n    separator=\"\\n\",\n    chunk_size=1500,\n    chunk_overlap=100,\n    length_function=len\n)\ndocs = text_splitter.split_documents(pages)\n```\n**3\\. åµŒå…¥åˆ›å»ºä¸Žå‘é‡æ•°æ®åº“**ï¼šä½¿ç”¨åŸºäºŽLLMçš„åµŒå…¥æ¨¡åž‹å°†é¢„å¤„ç†çš„æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡è¡¨ç¤ºï¼Œå¹¶å°†åµŒå…¥å­˜å‚¨åœ¨å¦‚Pineconeã€FAISSæˆ–åŸºäºŽMongoDBçš„è‡ªå®šä¹‰è§£å†³æ–¹æ¡ˆç­‰å‘é‡æ•°æ®åº“ä¸­ã€‚æˆ‘ä»¬åœ¨è¿™é‡Œä½¿ç”¨FAISSï¼ˆFacebook AIç›¸ä¼¼æ€§æœç´¢ï¼‰ã€‚è¿™å°†å…è®¸å¯¹å¤§é‡æ–‡æ¡£é›†è¿›è¡Œå¿«é€Ÿã€å¯æ‰©å±•çš„æœç´¢ã€‚\n\n```python\nembeddings_model = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, model=\"text-embedding-3-small\")\n## Load it into the vector store and embed\nvectordb = FAISS.from_documents(docs, embeddings_model)\n```\n\n## å¤§åž‹è¯­è¨€æ¨¡åž‹ (LLM) å’Œæ£€ç´¢å¢žå¼ºç”Ÿæˆ (RAG)\n\nLLMï¼Œä¾‹å¦‚ GPT æ¨¡åž‹ï¼Œåœ¨ç”Ÿæˆç±»äººæ–‡æœ¬æ–¹é¢éžå¸¸å¼ºå¤§ï¼Œä½†å½“ä¸Ž RAG ç³»ç»Ÿé…å¯¹æ—¶ï¼Œå®ƒä»¬çš„èƒ½åŠ›å¾—åˆ°äº†å¢žå¼ºï¼Œæ˜¾è‘—å‡å°‘äº†å¤§åž‹è¯­è¨€æ¨¡åž‹ (LLM) çš„å¹»è§‰ï¼Œå¹¶ä½¿è‡ªä¸»ä»£ç†èƒ½å¤Ÿæä¾›å¯é ä¸”å…·æœ‰ä¸Šä¸‹æ–‡æ„è¯†çš„ä¿¡æ¯ã€‚æ£€ç´¢å¢žå¼ºç”Ÿæˆ (RAG) é€šè¿‡å°†å…¶å“åº”ç”Ÿæˆä¸Žä»Žå‘é‡æ•°æ®åº“æ£€ç´¢çš„ç›¸å…³å¤–éƒ¨çŸ¥è¯†ç›¸ç»“åˆï¼Œæ¥æé«˜ LLM çš„æ€§èƒ½ã€‚åœ¨çŽ°å®žä¸–ç•Œä¸­ï¼Œæ£€ç´¢æ¥æºå¯ä»¥æ˜¯ä»»ä½•ä¸œè¥¿ï¼Œä»Žä¼ä¸šå‘é‡æ•°æ®åº“åˆ°ç§æœ‰æˆ–å…¬å…±ç½‘å€ï¼ˆç»´åŸºç™¾ç§‘ã€è°·æ­Œæ–‡æ¡£ç­‰ï¼‰ã€‚\n\nåœ¨æˆ‘ä»¬çš„ä¿¡ç”¨å¡ä»£ç†çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œå®¢æˆ·æŸ¥è¯¢å¯èƒ½åŒ…æ‹¬ï¼šâ€œCiti çš„ Costco Anywhere VisaÂ® Card çš„åˆ©çŽ‡ (APR) æ˜¯å¤šå°‘ï¼Ÿâ€åŸºäºŽ RAG çš„ç³»ç»Ÿå°†åˆ†ä¸¤æ­¥å·¥ä½œï¼š\n\n**1\\. æ£€ç´¢**ï¼šä½¿ç”¨å‘é‡æ•°æ®åº“æ ¹æ®ä¸ŽæŸ¥è¯¢çš„åµŒå…¥ç›¸ä¼¼æ€§èŽ·å–ç›¸å…³çš„èŠ±æ——ä¿¡ç”¨å¡ PDF çš„éƒ¨åˆ†å†…å®¹ã€‚\n\n```python\nretriever = vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n```\n**2\\. ç”Ÿæˆ**ï¼šLLM èŽ·å–æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼Œå¹¶ç”Ÿæˆç›´æŽ¥å›žç­”å®¢æˆ·é—®é¢˜çš„è¯¦ç»†ä¸”å‡†ç¡®çš„å“åº”ã€‚\n\n```python\nquestion = \"\"\" \"\"\"\n\nai_msg = rag.invoke({\"input\": question, \"chat_history\": retriever})\n\n```\nè¿™ç§æ–¹æ³•ç¡®ä¿ä»£ç†çš„å“åº”æ—¢åŸºäºŽçœŸå®žæ•°æ®ï¼ˆä»Žæ•°æ®åº“ä¸­æ£€ç´¢ï¼‰åˆå…·æœ‰ä¸Šä¸‹æ–‡ç›¸å…³æ€§ã€‚\n\n## æå‡äº¤äº’çš„æç¤ºå·¥ç¨‹\n\néƒ¨ç½²åŸºäºŽLLMçš„ä»£ç†çš„ä¸€ä¸ªé‡è¦æ–¹é¢æ˜¯æç¤ºå·¥ç¨‹ã€‚åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œç²¾å¿ƒè®¾è®¡çš„æç¤ºå¼•å¯¼LLMç”Ÿæˆå‡†ç¡®ä¸”å…·æœ‰ä¸Šä¸‹æ–‡ç›¸å…³æ€§çš„è¾“å‡ºã€‚å½“å›žç­”ä¸Žä¿¡ç”¨å¡äº§å“ç›¸å…³çš„æŸ¥è¯¢æ—¶ï¼Œä»£ç†éœ€è¦èƒ½å¤Ÿç†è§£ç”¨æˆ·æ„å›¾ï¼Œä»Žæ•°æ®åº“ä¸­æ£€ç´¢æ­£ç¡®çš„ä¿¡æ¯ï¼Œå¹¶ä»¥å¯¹è¯çš„æ–¹å¼è¿›è¡Œå›žåº”ã€‚\n\næœ‰æ•ˆæç¤ºå·¥ç¨‹çš„ç¤ºä¾‹åŒ…æ‹¬ï¼š\n\n* **ä¸Šä¸‹æ–‡è·Ÿè¿›**ï¼šæ¸…æ™°åœ°è§£é‡Šè§’è‰²å’Œä¿¡æ¯é¢†åŸŸã€‚æˆ‘ä»¬åœ¨è¿™é‡Œä½¿ç”¨æ¥è‡ª*langchain_core*çš„*ChatPromptTemplate*ã€‚\n\n\n```python\nqa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\nUse the following pieces of retrieved context to answer the question. \\\nIf you don't know the answer, just say that you don't know. \\\nUse three sentences maximum and keep the answer concise.\\\n\n{context}\"\"\"\n\nqa_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", qa_system_prompt),\n        (\"human\", \"{input}\"),\n    ]\n)\n```\né€šè¿‡å¾®è°ƒæç¤ºå¹¶ç¡®ä¿å…¶æ¶µç›–æŸ¥è¯¢çš„å„ä¸ªè§’åº¦ï¼ŒAIä»£ç†èƒ½å¤Ÿåˆ©ç”¨æœ€ä½³çš„ä¸Šä¸‹æ–‡å’ŒæŒ‡ä»¤æä¾›æ›´å¥½çš„å®¢æˆ·ä½“éªŒã€‚\n\n## æ£€ç´¢èŠå¤©åŽ†å²ä»¥å¢žå¼ºä¸Šä¸‹æ–‡æ„è¯†\n\nAIé©±åŠ¨çš„å®¢æˆ·æœåŠ¡é¢ä¸´çš„æŒ‘æˆ˜ä¹‹ä¸€æ˜¯ï¼Œåœ¨ä¸€ç³»åˆ—äº’åŠ¨ä¸­æä¾›è¿žè´¯ä¸”å…·æœ‰ä¸Šä¸‹æ–‡æ„è¯†çš„å›žåº”ã€‚ä¾‹å¦‚ï¼Œå®¢æˆ·å¯èƒ½åœ¨ä¸€æ¬¡ä¼šè¯ä¸­å¯¹ä¿¡ç”¨å¡äº§å“æå‡ºå¤šä¸ªé—®é¢˜ã€‚ä¸ºäº†ä¿æŒå¯¹è¯çš„æµç•…æ€§ï¼Œç³»ç»Ÿå¿…é¡»è·Ÿè¸ªå…ˆå‰çš„äº’åŠ¨ã€‚\n\n```python\nsystem_prompt = \"\"\"Given the chat history and a recent user question \\\ngenerate a new standalone question \\\nthat can be understood without the chat history. Do NOT answer the question, \\\njust reformulate it if needed or otherwise return it as is.\"\"\"\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system_prompt),\n        MessagesPlaceholder(\"chat_history\"),\n        (\"human\", \"{input}\"),\n    ]\n)\n\nretriever_with_history = create_history_aware_retriever(\n    llm, retriever, prompt\n)\n```\næ£€ç´¢èŠå¤©åŽ†å²å¸®åŠ©ä»£ç†ä¿æŒä¸Šä¸‹æ–‡å¹¶æä¾›æ›´ä¸ªæ€§åŒ–çš„å›žåº”ã€‚è¿™åœ¨å®¢æˆ·æå‡ºåŽç»­é—®é¢˜æˆ–åœ¨å¤šä¸ªäº§å“ä¹‹é—´åˆ‡æ¢çš„æƒ…å†µä¸‹å°¤å…¶é‡è¦ã€‚ç³»ç»Ÿç¡®ä¿æ—©æœŸçš„æ•°æ®ç‚¹ï¼ˆä¾‹å¦‚ï¼Œå®¢æˆ·æ­£åœ¨è®¨è®ºçš„äº§å“ï¼‰ä»ç„¶æ˜¯å½“å‰å¯¹è¯çš„ä¸€éƒ¨åˆ†ã€‚\n\n## Langchainï¼šåè°ƒä»£ç†\n\nLangchain æ˜¯è¿žæŽ¥æ‰€æœ‰è¿™äº›ç»„ä»¶çš„å…³é”®å·¥å…·ï¼šLLMsã€å‘é‡æ•°æ®åº“ã€RAG ç³»ç»Ÿå’Œå¤–éƒ¨ APIã€‚å®ƒæä¾›äº†ä¸€ä¸ªé›†æˆæ¡†æž¶ï¼Œç”¨äºŽæž„å»ºè¿™äº›è‡ªä¸»ä»£ç†ï¼Œç®€åŒ–å¼€å‘è¿‡ç¨‹ï¼Œå¹¶ç¡®ä¿ä»£ç†åœ¨ä¸åŒä»»åŠ¡ä¹‹é—´æ— ç¼å·¥ä½œï¼ŒåŒ…æ‹¬æ£€ç´¢ã€ä¸Šä¸‹æ–‡ç”Ÿæˆå’Œå“åº”åˆ¶å®šã€‚\n\n```python\nllm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo-0125\")\nquestion_answer_chain = create_stuff_documents_chain(llm, qa_system_prompt)\n\nretriever_with_history = create_history_aware_retriever(\n    llm, retriever, prompt\n)\n\nchat_history = [\"\"\" \"\"\"]\nrag_chain = create_retrieval_chain(retriever_with_history, question_answer_chain)\nai_msg = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history}\nchat_history.append([HumanMessage(content=question),ai_msg[\"answer\"]])\n```\nLangchain çš„æ¨¡å—åŒ–æž¶æž„å…è®¸è½»æ¾é›†æˆä¸åŒçš„æ•°æ®æºï¼Œæ— è®ºå®ƒä»¬æ˜¯å­˜å‚¨åœ¨å‘é‡æ•°æ®åº“ä¸­è¿˜æ˜¯é€šè¿‡ API è®¿é—®ã€‚å®ƒè¿˜ä¿ƒè¿›äº†ç”¨æˆ·æŸ¥è¯¢çš„å®žæ—¶åè°ƒï¼Œç»“åˆé€‚å½“çš„æ£€ç´¢ã€ç”Ÿæˆå’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥æœºåˆ¶ã€‚\n\n## ä½¿ç”¨ Flask å’Œ Streamlit éƒ¨ç½²\n\nä¸€æ—¦ RAG æ¨¡åž‹ç»è¿‡è®­ç»ƒå’Œä¼˜åŒ–ï¼Œå°±å¯ä»¥ä½¿ç”¨è½»é‡çº§çš„ Web æ¡†æž¶å¦‚ Flask æˆ– Streamlit è¿›è¡Œéƒ¨ç½²ã€‚Flask å…è®¸å¯¹éƒ¨ç½²è¿›è¡Œæ›´å¤šçš„è‡ªå®šä¹‰å’ŒæŽ§åˆ¶ï¼Œè€Œ Streamlit åˆ™ä¸“æ³¨äºŽç®€å•æ€§ï¼Œæä¾›å¿«é€ŸåŽŸåž‹å¼€å‘ã€‚å®Œæ•´å®žçŽ°å¯ä»¥åœ¨ [git repo](https://github.com/nitsourish/Conversational_AIchatbot) ä¸­æ‰¾åˆ°ã€‚\n\n**Flask ç¤ºä¾‹ï¼š**\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*1BZhU0OMY10wCQPRYliEQw.png)\n\n\n```python\napp = Flask(__name__)\n\n@app.route('/query', methods=['POST'])\ndef query_model():\n    input_data = request.json['query']\n    response = rag_chain.invoke({\"input\": input_data})\n    return jsonify({\"response\": response})\n\nif __name__ == '__main__':\n    app.run(debug=True)\n```\n**Streamlit ç¤ºä¾‹ï¼š**\n\n\n```python\nst.title(\"ä¿¡ç”¨å¡äº§å“æŸ¥è¯¢ä»£ç†\")\nuser_query = st.text_input(\"è¯¢é—®æœ‰å…³èŠ±æ——ä¿¡ç”¨å¡çš„é—®é¢˜ï¼š\")\nif user_query:\n    response = rag_chain.invoke({\"input\": user_query})\n    st.write(f\"å“åº”: {response}\")\n```\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*uOXAnRB0yln6U21aCUMy9Q.png)\n\n## å…³é”®è¦ç‚¹ä¸Žæœªæ¥å±•æœ›\n\nåœ¨è¿™ç¯‡åšå®¢ä¸­ï¼Œæˆ‘è®¨è®ºäº†å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰åœ¨B2Cè¡Œä¸šä¸­çš„ç›¸å…³æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å®¢æˆ·æŽ¥è§¦ç‚¹è¾ƒå¤šçš„é¢†åŸŸï¼Œé‡ç‚¹åº”ç”¨äºŽé“¶è¡Œäº§å“çš„å¯¹è¯å¼AIä»£ç†ï¼ŒåŒ…æ‹¬åŸºäºŽRAGçš„ç®¡é“çš„é€æ­¥å¼€å‘å’Œéƒ¨ç½²ï¼Œåˆ©ç”¨æµè¡Œçš„lang-chainæ¡†æž¶ã€‚æµç¨‹åŒ…æ‹¬å®šåˆ¶çš„å·¥ç¨‹ç®¡é“ï¼Œæ•°æ®æ‘„å–ï¼Œå‘é‡æ•°æ®åº“ï¼ˆæ£€ç´¢å™¨ï¼‰çš„é…ç½®ã€‚æœ€åŽå±•ç¤ºäº†ä½¿ç”¨å¾®åž‹Webæ¡†æž¶å¦‚Flaskè¿›è¡Œå…¨é¢æŽ§åˆ¶æˆ–ä½¿ç”¨Streamlitè¿›è¡Œå¿«é€ŸåŽŸåž‹å¼€å‘çš„éƒ¨ç½²ã€‚\n\nåœ¨å½“ä»Šå¿«é€Ÿå‘å±•çš„B2CçŽ¯å¢ƒä¸­ï¼Œæä¾›å¿«é€Ÿã€å‡†ç¡®å’Œä¸ªæ€§åŒ–çš„å®¢æˆ·æœåŠ¡æ˜¯èŽ·å¾—ç«žäº‰ä¼˜åŠ¿çš„å…³é”®ã€‚é€šè¿‡å°†LLMsä¸Žå‘é‡æ•°æ®åº“ã€æ£€ç´¢å¢žå¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œæç¤ºå·¥ç¨‹ç›¸ç»“åˆï¼Œå…¬å¸å¯ä»¥éƒ¨ç½²ä¸ä»…èƒ½å›žç­”å®¢æˆ·æŸ¥è¯¢çš„AIä»£ç†ï¼Œè€Œä¸”èƒ½å¤Ÿä»¥é«˜ä¸Šä¸‹æ–‡å‡†ç¡®æ€§è¿›è¡Œå›žç­”ã€‚\n\næ„Ÿè°¢é˜…è¯»æœ¬æ–‡ã€‚è¦é˜…è¯»æ›´å¤šç²¾å½©çš„AIæ•…äº‹ï¼Œè¯·å…³æ³¨æˆ‘çš„[medium stories](https://medium.com/@sourish.syntel)ã€‚\n\n"},{"lang":"zh","group":"blog","slug":"blog/lightrag-simple-and-efficient-rival-to-graphrag-fe49e12e9ece","frontmatter":{"title":"LightRAG - GraphRAG ç®€å•é«˜æ•ˆçš„ç«žäº‰å¯¹æ‰‹ï¼Ÿ","meta_title":"LightRAG - GraphRAG ç®€å•é«˜æ•ˆçš„ç«žäº‰å¯¹æ‰‹ï¼Ÿ","description":"ä¼ ç»Ÿçš„ RAG ç³»ç»Ÿé€šè¿‡ç´¢å¼•åŽŸå§‹æ•°æ®æ¥å·¥ä½œã€‚è¿™äº›æ•°æ®è¢«ç®€å•åœ°åˆ†å—å¹¶å­˜å‚¨åœ¨å‘é‡æ•°æ®åº“ä¸­ã€‚æ¯å½“æœ‰æŸ¥è¯¢ä»Ž...","date":"2024-11-13T01:22:29.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*7_2PyaNMVdYDWTCrb_cMCg.png","categories":["Generative AI","Data Science","Technology/Web"],"author":"Rifx.Online","tags":["LightRAG","retrieval","GraphRAG","indexing","dual-level"],"draft":false,"slug":"blog/lightrag-simple-and-efficient-rival-to-graphrag-fe49e12e9ece"},"content":"\n\n\n\n\nä¼ ç»Ÿçš„ RAG ç³»ç»Ÿé€šè¿‡ç´¢å¼•åŽŸå§‹æ•°æ®æ¥å·¥ä½œã€‚è¿™äº›æ•°æ®è¢«ç®€å•åœ°åˆ‡åˆ†å¹¶å­˜å‚¨åœ¨å‘é‡æ•°æ®åº“ä¸­ã€‚æ¯å½“ç”¨æˆ·å‘å‡ºæŸ¥è¯¢æ—¶ï¼Œå®ƒä¼šæŸ¥è¯¢å­˜å‚¨çš„ç‰‡æ®µå¹¶ *æ£€ç´¢* ç›¸å…³ç‰‡æ®µã€‚å¦‚æžœæ‚¨å¸Œæœ›äº†è§£ RAG çš„åŸºæœ¬åŽŸç†ï¼Œæˆ‘å·²ç»åœ¨ [è¿™é‡Œ](https://proxy.rifx.online/https://readmedium.com/retrieval-augmented-generation-rag-a-quick-and-comprehensive-introduction-6cd5217a4ebb) å†™äº†ä¸€ç¯‡å…¨é¢çš„ä»‹ç»ã€‚\n\nç”±äºŽæ£€ç´¢æ­¥éª¤é’ˆå¯¹ç”¨æˆ·çš„æ¯ä¸€ä¸ªæŸ¥è¯¢éƒ½ä¼šå‘ç”Ÿï¼Œå› æ­¤è¿™æ˜¯åŠ é€Ÿç®€å• RAG ç³»ç»Ÿçš„æœ€å…³é”®ç“¶é¢ˆã€‚è®©æ£€ç´¢è¿‡ç¨‹å˜å¾—è¶…çº§é«˜æ•ˆéš¾é“ä¸æ˜¯åˆä¹Žé€»è¾‘çš„å—ï¼Ÿè¿™å°±æ˜¯ **LightRAG** çš„æ‰¿è¯ºã€‚\n\n\n> **å¦‚æžœæ‚¨ä¸æ˜¯ä¼šå‘˜ï¼Œæ‚¨å¯ä»¥åœ¨ [è¿™é‡Œ](https://proxy.rifx.online/https://www.ai-bites.net/lightrag-simple-and-efficient-rival-to-graphrag/) å…è´¹é˜…è¯»æ­¤å†…å®¹ã€‚ä¸ºä»€ä¹ˆä¸åœ¨é‚£é‡Œè®¢é˜…å¹¶å°†è¿™äº›å†…å®¹ç›´æŽ¥å‘é€åˆ°æ‚¨çš„æ”¶ä»¶ç®±å‘¢ï¼Ÿ**\n\n## ä¸ºä»€ä¹ˆä¸ä½¿ç”¨ GraphRAG\n\nåœ¨æˆ‘ä»¬æŸ¥çœ‹å®ƒä»¬ä¹‹å‰ï¼Œä½ å¯èƒ½ä¼šé—®ï¼šâ€œç­‰ä¸€ä¸‹ã€‚æˆ‘ä»¬ä¸æ˜¯æœ‰å¾®è½¯çš„ GraphRAG å—ï¼Ÿâ€æ˜¯çš„ï¼Œä½† GraphRAG ä¼¼ä¹Žæœ‰å‡ ä¸ªç¼ºç‚¹ã€‚\n\n* **å¢žé‡çŸ¥è¯†æ›´æ–°ã€‚** (sec 3\\.1\\) GraphRAG é¦–å…ˆåœ¨æ•´ä¸ªç§æœ‰æ•°æ®é›†ä¸­åˆ›å»ºå¯¹å®žä½“å’Œå…³ç³»çš„å¼•ç”¨ã€‚ç„¶åŽï¼Œå®ƒé€šè¿‡è‡ªä¸‹è€Œä¸Šçš„èšç±»å°†æ•°æ®å±‚æ¬¡åŒ–ç»„ç»‡æˆè¯­ä¹‰é›†ç¾¤ã€‚å¯¹æ•°æ®é›†è¿›è¡Œæ–°çŸ¥è¯†çš„æ›´æ–°æ„å‘³ç€æˆ‘ä»¬å¿…é¡»é‡æ–°ç»åŽ†æž„å»ºå›¾çš„æ•´ä¸ªè¿‡ç¨‹ï¼è€Œ LightRAG åˆ™é€šè¿‡ç®€å•åœ°å°†æ–°çŸ¥è¯†é™„åŠ åˆ°çŽ°æœ‰çŸ¥è¯†ä¸Šæ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œå®ƒé€šè¿‡ç®€å•çš„å¹¶é›†æ“ä½œå°†æ–°çš„å›¾èŠ‚ç‚¹å’Œè¾¹ä¸ŽçŽ°æœ‰çš„ç»“åˆåœ¨ä¸€èµ·ã€‚\n* **è®¡ç®—å¼ºåº¦ã€‚** ä»Žä»–ä»¬çš„ç ”ç©¶ä¸­å¯ä»¥çœ‹å‡ºï¼ŒLightRAG æ˜¾è‘—é™ä½Žäº†æ£€ç´¢é˜¶æ®µçš„æˆæœ¬ã€‚GraphRAG éœ€è¦ 610,000 ä¸ªæ ‡è®°ï¼Œè€Œ LightRAG åˆ™å°‘äºŽ 100 ä¸ªæ ‡è®°ã€‚\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*0TwUDr1BCNr_nSfTPwxenw.png)\n\næ‰€ä»¥ä¸å†èµ˜è¿°ï¼Œè®©æˆ‘ä»¬æ·±å…¥äº†è§£ LightRAGã€‚\n\n## LightRAG\n\nLightRAGçš„ä¸¤ä¸ªä¸»è¦å–ç‚¹æ˜¯åŸºäºŽå›¾çš„ç´¢å¼•å’ŒåŒå±‚æ£€ç´¢æ¡†æž¶ã€‚è®©æˆ‘ä»¬é€ä¸€äº†è§£å®ƒä»¬ã€‚\n\n## åŸºäºŽå›¾çš„ç´¢å¼•\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*U7sYYNA9teKEVig1dzfi2g.png)\n\nä»¥ä¸‹æ˜¯LightRAGéµå¾ªçš„æ­¥éª¤ï¼Œä»¥å®žçŽ°åŸºäºŽå›¾çš„ç´¢å¼•ã€‚\n\n* **å®žä½“å’Œå…³ç³»ï¼ˆERï¼‰æå–ã€‚** ERæå–åœ¨ä¸Šå›¾ä¸­ç”¨R(.)è¡¨ç¤ºã€‚æ­¤æ­¥éª¤ç¡®ä¿é¦–å…ˆä»Žç»™å®šæ–‡æ¡£ä¸­æå–ç®€å•å®žä½“ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸Šè¿°ç¤ºä¾‹ä¸­ï¼Œâ€œèœœèœ‚â€å’Œâ€œå…»èœ‚äººâ€æ˜¯ä¸¤ä¸ªå®žä½“ã€‚å®ƒä»¬é€šè¿‡â€œè§‚å¯Ÿâ€å…³ç³»ç›¸å…³è”ã€‚å³ï¼Œå…»èœ‚äººè§‚å¯Ÿèœœèœ‚ã€‚\n* **ä½¿ç”¨LLMç”Ÿæˆé”®å€¼ï¼ˆKVï¼‰å¯¹ã€‚** ç„¶åŽä½¿ç”¨ç®€å•çš„LLMç”ŸæˆKVå¯¹ã€‚LLMåˆ†æžæ­¥éª¤æä¾›äº†å…³äºŽå®žä½“æˆ–å…³ç³»çš„ç®€è¦è¯´æ˜Žæˆ–è§£é‡Šã€‚ä¾‹å¦‚ï¼ŒLLMè§£é‡Šäº†åœ¨æˆ‘ä»¬é€‰æ‹©çš„ç¤ºä¾‹ä¸­â€œå…»èœ‚äººâ€æ˜¯è°ã€‚æ­¤æ­¥éª¤åœ¨ä¸Šå›¾ä¸­ç”¨P(.)è¡¨ç¤ºã€‚è¯·æ³¨æ„ï¼Œè¿™ä¸ªLLMä¸Žä¸»RAGç®¡é“ä¸­ä½¿ç”¨çš„é€šç”¨LLMä¸åŒã€‚\n* **åŽ»é‡ã€‚** é‰´äºŽè¿™äº›æ–‡æ¡£ä¸Žèœœèœ‚æœ‰å…³ï¼Œå®žä½“â€œå…»èœ‚äººâ€å¯èƒ½æ˜¯ä»Žå¤šä¸ªæ–‡æ¡£æˆ–ç‰‡æ®µä¸­æ£€ç´¢åˆ°çš„ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªåŽ»é‡æ­¥éª¤ï¼Œä»…ä¿ç•™ä¸€ä¸ªå¹¶ä¸¢å¼ƒå…¶ä½™å…·æœ‰ç›¸åŒå«ä¹‰çš„å†…å®¹ã€‚è¿™åœ¨ä¸Šå›¾ä¸­ç”¨D(.)è¡¨ç¤ºã€‚\n\n## åŒå±‚æ£€ç´¢\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*t9W1UBbjFa5cnAe-_tqz-Q.png)\n\nå¯¹RAGç³»ç»Ÿçš„æŸ¥è¯¢å¯ä»¥åˆ†ä¸ºä¸¤ç§ç±»åž‹â€”â€”å…·ä½“æŸ¥è¯¢æˆ–æŠ½è±¡æŸ¥è¯¢ã€‚åœ¨åŒä¸€ä¸ªèœœèœ‚çš„ä¾‹å­ä¸­ï¼Œå…·ä½“æŸ¥è¯¢å¯ä»¥æ˜¯â€œèœ‚å·¢ä¸­å¯ä»¥æœ‰å¤šå°‘åªèœ‚åŽï¼Ÿâ€æŠ½è±¡æŸ¥è¯¢å¯ä»¥æ˜¯â€œæ°”å€™å˜åŒ–å¯¹èœœèœ‚çš„å½±å“æ˜¯ä»€ä¹ˆï¼Ÿâ€ä¸ºäº†åº”å¯¹è¿™ç§å¤šæ ·æ€§ï¼ŒLightRAGé‡‡ç”¨äº†ä¸¤ç§æ£€ç´¢ç±»åž‹ï¼š\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*DuVxwxwl_2-gej_DwGzoeg.png)\n\n* **ä½Žå±‚æ£€ç´¢ã€‚** å®ƒç®€å•åœ°æå–ç²¾ç¡®çš„å®žä½“åŠå…¶å…³ç³»ï¼Œå¦‚èœœèœ‚ã€è§‚å¯Ÿå’Œå…»èœ‚äººã€‚\n* **é«˜å±‚æ£€ç´¢ã€‚** é€šè¿‡ä½¿ç”¨LLMï¼ŒLightRAGæ±‡æ€»ä¿¡æ¯å¹¶æ€»ç»“å¤šä¸ªä¿¡æ¯æ¥æºã€‚\n\n## ä¸ºä»€ä¹ˆè¦åšè¿™äº›ï¼Ÿ\n\nè¿›è¡Œæ‰€æœ‰è¿™äº›ç»ƒä¹ å¹¶åˆ‡æ¢åˆ° LightRAG ç¡®å®žæé«˜äº†æ‰§è¡Œæ—¶é—´ã€‚åœ¨ç´¢å¼•è¿‡ç¨‹ä¸­ï¼ŒLLM æ¯ä¸ªå—åªéœ€è°ƒç”¨ä¸€æ¬¡ä»¥æå–å®žä½“åŠå…¶å…³ç³»ã€‚\n\nåŒæ ·ï¼Œåœ¨ç”¨æˆ·æŸ¥è¯¢æœŸé—´ï¼Œæˆ‘ä»¬åªéœ€ä½¿ç”¨ç”¨äºŽç´¢å¼•çš„ç›¸åŒ LLM ä»Žå—ä¸­æ£€ç´¢å®žä½“å’Œå…³ç³»ã€‚è¿™åœ¨æ£€ç´¢å¼€é”€å’Œè®¡ç®—ä¸ŠèŠ‚çœäº†å¤§é‡æˆæœ¬ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ç»ˆäºŽæœ‰äº†ä¸€ä¸ªâ€œè½»é‡çº§â€çš„ RAGï¼\n\nå°†æ–°çŸ¥è¯†æ•´åˆåˆ°çŽ°æœ‰å›¾ä¸­ä¼¼ä¹Žæ˜¯ä¸€ä¸ªæ— ç¼çš„è¿‡ç¨‹ã€‚æ¯å½“æˆ‘ä»¬æœ‰æ–°ä¿¡æ¯æ—¶ï¼Œä¸å¿…é‡æ–°ç´¢å¼•æ•´ä¸ªæ•°æ®ï¼Œæˆ‘ä»¬å¯ä»¥ç®€å•åœ°å°†æ–°çŸ¥è¯†é™„åŠ åˆ°çŽ°æœ‰å›¾ä¸­ã€‚\n\n## è¯„ä¼°\n\nåœ¨ä»–ä»¬çš„è¯„ä¼°ä¸­ï¼Œä»–ä»¬ä¸Ž Naive RAGã€RQ\\-RAGã€HyDE å’Œ GraphRAG è¿›è¡Œäº†æ¯”è¾ƒã€‚ä¸ºäº†ä¿æŒæ¯”è¾ƒçš„å…¬å¹³æ€§ï¼Œä»–ä»¬åœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šä½¿ç”¨äº†å›ºå®šçš„ 1200 çš„å—å¤§å°å¹¶ä¸”ä½¿ç”¨äº† GPT\\-4o\\-mini ä½œä¸º LLMã€‚ç­”æ¡ˆçš„è¯„ä¼°æ ‡å‡†åŒ…æ‹¬å…¨é¢æ€§ã€å¤šæ ·æ€§å’Œåœ¨å›žç­”ç”¨æˆ·é—®é¢˜ï¼ˆå³è®ºæ–‡ä¸­çš„ *èµ‹èƒ½*ï¼‰æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*DNdNHW7NRcOXpvEWjT5BKQ.png)\n\nä»Žä¸‹åˆ’çº¿çš„ç»“æžœä¸­å¯ä»¥çœ‹å‡ºï¼ŒLightRAG è¶…è¶Šäº†å½“å‰æ‰€æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚\n\næ€»ä½“è€Œè¨€ï¼Œä»–ä»¬å¾—å‡ºäº†ä»¥ä¸‹ç»“è®ºï¼š\n\n* ä½¿ç”¨åŸºäºŽå›¾çš„æ–¹æ³•ï¼ˆGraphRAG æˆ– LightRAGï¼‰æ˜¾è‘—æ”¹å–„äº†åŸºçº¿ Naive RAG\n* LightRAG é€šè¿‡åŒå±‚æ£€ç´¢èŒƒå¼äº§ç”Ÿäº†ç›¸å½“å¤šæ ·çš„ç­”æ¡ˆ\n* LightRAG èƒ½æ›´å¥½åœ°å¤„ç†å¤æ‚æŸ¥è¯¢\n\n## ç»“è®º\n\nå°½ç®¡ RAG æ˜¯ä¸€ç§ç›¸å¯¹è¾ƒæ–°çš„æŠ€æœ¯ï¼Œä½†æˆ‘ä»¬åœ¨è¿™ä¸€é¢†åŸŸçœ‹åˆ°äº†å¿«é€Ÿè¿›å±•ã€‚åƒ LightRAG è¿™æ ·çš„æŠ€æœ¯èƒ½å¤Ÿå°† RAG æµæ°´çº¿è¿è¡Œåœ¨å»‰ä»·çš„å•†å“ç¡¬ä»¶ä¸Šï¼Œå—åˆ°äº†å¹¿æ³›æ¬¢è¿Žã€‚éšç€ç¡¬ä»¶çŽ¯å¢ƒçš„ä¸æ–­å‘å±•ï¼Œå®žæ—¶åœ¨è®¡ç®—å—é™çš„ç¡¬ä»¶ä¸Šè¿è¡Œ LLM å’Œ RAG æµæ°´çº¿çš„éœ€æ±‚ä¹Ÿåœ¨ä¸æ–­å¢žåŠ ã€‚\n\næ‚¨æƒ³çœ‹çœ‹å…³äºŽ LightRAG çš„ä¸€äº›å®žè·µç ”ç©¶å—ï¼Ÿè¯·ç»§ç»­å…³æ³¨â€¦â€¦\n\n## å‘å¤§å®¶è‡´æ•¬\n\nå¸Œæœ›è¿™å¯¹ä½ æœ‰å¸®åŠ©ã€‚\n\n**å¦‚æžœä½ å–œæ¬¢è¿™ç¯‡æ–‡ç« ï¼Œä¸ºä»€ä¹ˆä¸åœ¨ [Twitter](https://proxy.rifx.online/https://twitter.com/ai_bites) ä¸Šå…³æ³¨æˆ‘å‘¢ï¼Ÿæˆ‘æ¯å¤©éƒ½ä¼šåˆ†äº«é¡¶çº§AIå®žéªŒå®¤çš„ç ”ç©¶æ›´æ–°ã€‚**\n\n**åŒæ—¶ï¼Œè¯·è®¢é˜…æˆ‘çš„ [YouTube é¢‘é“](https://proxy.rifx.online/https://www.youtube.com/c/aibites)ï¼Œæˆ‘ä¼šä»¥è§†è§‰æ–¹å¼è§£é‡ŠAIæ¦‚å¿µå’Œè®ºæ–‡ã€‚**\n\n**æœ€åŽï¼Œè¯·ç»™æˆ‘ç‚¹èµžï¼Œè®©æˆ‘ä»¬ä¸€èµ·åº†ç¥ä½ é˜…è¯»å®Œè¿™ä¸ªæ•…äº‹ã€‚**\n\n"},{"lang":"zh","group":"blog","slug":"blog/llama-3-1-405b-how-to-use-for-free-9aaf3561932d","frontmatter":{"title":"Llama 3.1 405Bâ€”â€”å¦‚ä½•å…è´¹ä½¿ç”¨","meta_title":"Llama 3.1 405Bâ€”â€”å¦‚ä½•å…è´¹ä½¿ç”¨","description":"æ— éœ€æœ¬åœ°å®‰è£…","date":"2024-10-29T05:09:24.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*db_ND6LyQ5_p5jFJCTo5GQ.jpeg","categories":["Programming","Technology","Generative AI"],"author":"Rifx.Online","tags":["Llama","Meta","HuggingChat","Groq","API"],"draft":false,"slug":"blog/llama-3-1-405b-how-to-use-for-free-9aaf3561932d"},"content":"\n\n\n### æ— éœ€æœ¬åœ°å®‰è£…\n\n**Llama 3\\.1 405B** æ˜¯MetaäºŽ2024å¹´7æœˆå‘å¸ƒçš„æœ€å…ˆè¿›çš„AIæ¨¡åž‹â€”â€”**ä½†ä½ å¯ä»¥åœ¨å“ªé‡Œè¯•ç”¨å®ƒ*ï¼Ÿ***\n\n\n\n**LLama 3\\.1** æœ‰ä¸åŒçš„ç‰ˆæœ¬ï¼ŒåŒ…æ‹¬å‚æ•°æœ€å¤šçš„4050äº¿æ¨¡åž‹ä»¥åŠè¾ƒå°çš„70Bå’Œ8Bæ¨¡åž‹ã€‚\n\nè¯•ç”¨70Bå’Œ8Bæ¨¡åž‹çš„æœ€ç®€å•æ–¹æ³•æ˜¯åœ¨[Groq](https://console.groq.com/playground)ä¸Šâ€”â€”ä½ å¯ä»¥ç›´æŽ¥åœ¨ä»–ä»¬çš„æ¸¸ä¹åœºä¸­è¯•ç”¨å®ƒä»¬ã€‚\n\nç”±äºŽéœ€æ±‚é‡å·¨å¤§ï¼Œæœ€å¼ºå¤§çš„405Bæ¨¡åž‹é€šå¸¸ä¸å¯ç”¨ã€‚\n\næœ¬æŒ‡å—é€‚ç”¨äºŽä»»ä½•æƒ³è¦å…è´¹è¯•ç”¨Llama 3\\.1 405Bçš„ç”¨æˆ·ï¼ŒåŒ…æ‹¬å¼€å‘è€…â€”â€”æ— éœ€ä¸‹è½½å’Œå®‰è£…ã€‚\n\nå¦‚æžœä½ æ²¡æœ‰ä»˜è´¹çš„Mediumè´¦æˆ·ï¼Œå¯ä»¥åœ¨[è¿™é‡Œ](https://addison-best.medium.com/9aaf3561932d?source=friends_link&sk=5fa532d1caaec229a0b9a445d8749449)å…è´¹é˜…è¯»ã€‚\n\nå¦‚æžœä½ æ˜¯å¼€å‘è€…ï¼Œå¹¶ä¸”æƒ³è¦é€šè¿‡APIå…è´¹è¯•ç”¨**LLama 3\\.1 405Bâ€”â€”**ä½ å¯ä»¥è·³åˆ°æ–‡ç« çš„æœ«å°¾ã€‚\n\n## æˆ‘åœ¨å“ªé‡Œå¯ä»¥å…è´¹ä½¿ç”¨ Llama 3\\.1 405Bï¼Ÿ\n\næ‚¨å¯ä»¥ç›´æŽ¥ä»Ž [Meta](https://llama.meta.com/) ä¸‹è½½å¹¶å®‰è£…å®ƒâ€”â€”ä½†å®ƒéžå¸¸åºžå¤§ï¼Œæ‚¨éœ€è¦æ•°ç™¾ä¸ªåƒå…†å­—èŠ‚çš„ç©ºé—´å’Œä¸€å°å¼ºå¤§çš„è®¡ç®—æœºæ‰èƒ½æ­£ç¡®å°è¯•ã€‚\n\nä½†æ‚¨çŽ°åœ¨ä¹Ÿå¯ä»¥åœ¨ä¸ä¸‹è½½çš„æƒ…å†µä¸‹è¿›è¡Œå°è¯•ã€‚\n\nä»¥ä¸‹æ˜¯ä¸€äº›æ‚¨å¯ä»¥å°è¯•çš„é€‰é¡¹ï¼š\n\n**å¦‚æžœæ‚¨æƒ³äº†è§£æ›´å¤š AI å°æŠ€å·§ï¼Œä»¥å¸®åŠ©æ‚¨çš„ä¸šåŠ¡å¢žé•¿å¹¶åœ¨çº¿èµšå–æ›´å¤šæ”¶å…¥ï¼š**\n\n***ðŸ‘‰*** *æ³¨å†Œæˆ‘ä»¬çš„ **[å…è´¹ 5 å¤©ç”µå­é‚®ä»¶è¯¾ç¨‹](https://aigrowthguys.com/5-day-free-course-how-to-grow-your-business-like-a-weed)**ï¼Œå®žçŽ°å¢žé•¿ ðŸš€ å¹¶èµšå–**ðŸ’²ðŸ‘ˆ***\n\n## 1\\. åœ¨Meta AIä¸Šä½¿ç”¨Llama 3\\.1 405B\n\nå¦‚æžœä½ åœ¨ç¾Žå›½ï¼Œè‡³å°‘åœ¨åŠ æ‹¿å¤§ï¼ˆæˆ‘æ‰€åœ¨çš„åœ°æ–¹ï¼‰ï¼Œä½ å¯ä»¥é€šè¿‡Meta AIä¸ŽLlama 3\\.1 405Bæ¨¡åž‹èŠå¤©ã€‚è®¿é—®[Meta AIç½‘ç«™](https://www.meta.ai)ï¼Œå¹¶ä½¿ç”¨ä½ çš„Facebookæˆ–Instagramè´¦æˆ·ç™»å½•ã€‚\n\nå®ƒçŽ°åœ¨ä¹Ÿå¯èƒ½åœ¨å…¶ä»–å›½å®¶å¯ç”¨ï¼Œæ‰€ä»¥å¯ä»¥çœ‹çœ‹ã€‚\n\nå½“ä½ ç™»å½•æ—¶â€”â€”å¸Œæœ›ä½ èƒ½çœ‹åˆ°å°è¯•**Llama 3\\.1 405B**çš„é€‰é¡¹ã€‚\n\nå¦‚æžœå¯ä»¥ï¼Œä½ ä¼šçœ‹åˆ°å¦‚ä¸‹æˆªå›¾ä¸­çš„æ¶ˆæ¯ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*cw1WMKhdZhzUp0L3Kn7Qng.png)\n\nä½ ä¹Ÿå¯ä»¥é€šè¿‡é“¾æŽ¥ä½ çš„Metaè´¦æˆ·æ¥é€šè¿‡WhatsAppè®¿é—®å®ƒã€‚[**åœ¨Meta AIä¸Šå°è¯•**](https://www.meta.ai)\n\nä½ è¿˜å¯ä»¥å°è¯•ä»–ä»¬çš„**Imagine**ç…§ç‰‡åˆ›ä½œå·¥å…·å’ŒAIå›¾åƒç¼–è¾‘å™¨**ã€‚**\n\næ–‡ç« å¼€å¤´çš„é‚£å¹…å¸¦æœ‰æ‹‰é©¬å’Œç”µè„‘çš„å¡é€šå›¾åƒå°±æ˜¯ç”¨è¿™ä¸ªå·¥å…·åˆ›å»ºçš„ã€‚\n\n**æˆ‘æç¤ºäº†**\n\n> **Imagine: æˆ‘æƒ³è¦ä¸€å¹…æœ‰è¶£çš„å¡é€šå›¾åƒï¼Œç”¨äºŽä¸­ç­‰æ–‡ç« ï¼Œå±•ç¤ºå°è¯•ä½¿ç”¨Llama 3\\.1 405B**\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8MeC_M2O7UX7ulPOfUCuHA.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*dIG62eA7YAT3mpLA0etz9Q.png)\n\nå€¼å¾—ä¸€è¯•ã€‚æˆ‘è®¤ä¸ºå®ƒæ— æ³•ä¸ŽFlux.1æˆ–Midjourneyç›¸æå¹¶è®ºâ€”â€”ä½†å®ƒæ˜“äºŽä½¿ç”¨ä¸”å…è´¹ã€‚\n\n## 2\\. åœ¨ HuggingChat ä¸Šä½¿ç”¨ Llama 3\\.1 405B\n\nHuggingChat å¯¹ç¾Žå›½ä»¥å¤–çš„ç”¨æˆ·å¼€æ”¾ï¼Œå¹¶æä¾›å¯¹ Llama 3\\.1 405B æ¨¡åž‹çš„è®¿é—®ã€‚æ‚¨å¯ä»¥ç«‹å³å¼€å§‹èŠå¤©ï¼Œæ— éœ€æ³¨å†Œï¼Œè¿™ä½¿å¾—æŽ¢ç´¢æ¨¡åž‹çš„èƒ½åŠ›å˜å¾—ç®€å•ã€‚è®¿é—® [HuggingChat é¡µé¢](https://huggingface.co) å¼€å§‹ã€‚[åœ¨ HuggingChat ä¸Šè¯•ç”¨](https://huggingface.co)\n\n## 3\\. åœ¨ Groq ä¸Šä½¿ç”¨ Llama 3\\.1 405B\n\n**å¦‚ä½•ï¼š** Groq æœ€åˆæ‰˜ç®¡äº† Llama 3\\.1 405B æ¨¡åž‹ï¼Œä½†çŽ°åœ¨ç”±äºŽéœ€æ±‚é‡å¤§ï¼Œæä¾›äº†æ›´å°çš„ 70B å’Œ 8B ç‰ˆæœ¬ã€‚æ‚¨å¯ä»¥é€šè¿‡åœ¨ [Groq çš„ç½‘ç«™](https://groq.com) ä¸Šåˆ›å»ºä¸€ä¸ªå…è´¹è´¦æˆ·æ¥æŽ¢ç´¢è¿™äº›æ¨¡åž‹ã€‚[åœ¨ Groq ä¸Šè¯•ç”¨](https://groq.com)\n\n## 4\\. åœ¨ Perplexity ä¸Šä½¿ç”¨ Llama 3\\.1 405B\n\nPerplexity æä¾›äº†ä¸€ç§ç®€å•çš„æ–¹å¼ä¸Ž Llama 3\\.1 è¿›è¡Œäº¤äº’ï¼Œæ—¨åœ¨å¿«é€Ÿä¾¿æ·åœ°è®¿é—®è¯¥æ¨¡åž‹ã€‚æ‚¨å¯ä»¥é€šè¿‡è®¿é—® Perplexity AI å¹³å°å¼€å§‹ä½¿ç”¨å®ƒã€‚ä½†è¿™ä»…åœ¨ Pro è®¡åˆ’ä¸­å¯ç”¨ã€‚[åœ¨ Perplexity ä¸Šå°è¯•](https://www.perplexity.ai)\n\n## 5\\. åœ¨ Poe ä¸Šä½¿ç”¨ Llama 3\\.1 405B\n\nPoe æ˜¯ Quora æä¾›çš„å¦ä¸€ä¸ªå¹³å°ï¼Œæ‚¨å¯ä»¥åœ¨è¿™é‡Œå°è¯• Llama 3\\.1ã€‚Poe å…è®¸ç”¨æˆ·é€šè¿‡èŠå¤©ç•Œé¢æŽ¢ç´¢ä¸åŒçš„ AI æ¨¡åž‹ï¼ŒåŒ…æ‹¬ Llama 3\\.1ã€‚å¦‚æžœæ‚¨æƒ³åœ¨ä¸€ä¸ªåœ°æ–¹æ¯”è¾ƒ Llama 3\\.1 å’Œå…¶ä»– AI æ¨¡åž‹ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šåŠŸèƒ½çš„é€‰æ‹©ã€‚æ‚¨å¯ä»¥å…è´¹å°è¯• 3\\.1 405B â€” æ¯å¤©æœ‰æœ‰é™çš„å…è´¹ç§¯åˆ†ã€‚[åœ¨ Poe ä¸Šå°è¯•](https://poe.com)\n\n## æˆ‘åœ¨å“ªé‡Œå¯ä»¥å…è´¹ä½¿ç”¨ Llama 3\\.1 405B APIï¼Ÿ\n\nå¦‚æžœä½ æ˜¯å¼€å‘è€…å¹¶æƒ³å®Œå…¨å…è´¹å°è¯• Llama 3\\.1 405B ç‰ˆæœ¬â€”â€”ç›®å‰ä½ çš„é€‰æ‹©æœ‰é™ã€‚\n\nä½†æˆ‘æƒ³ç»™ä½ ä¸€ä¸ªç®€å•ä¸”å…è´¹çš„é€‰é¡¹ï¼Œè®©ä½ å¯ä»¥å¼€å§‹ä½¿ç”¨ã€‚\n\nä½ çŽ°åœ¨å¯ä»¥åœ¨ [together.ai](https://together.ai) ä¸Šå…è´¹å°è¯•ã€‚\n\nä½ å¯ä»¥èŽ·å¾— $5 çš„å…è´¹é¢åº¦å’Œä¸€ä¸ª API å¯†é’¥æ¥è¿›è¡Œå°è¯•ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*w8LOXw-Wm0QTz5YgvZ27ug.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*YpKURkmy--xstoJpZ4fmbw.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*g0FxHkg6gq5OMXXo1Yzr0A.png)\n\nè¿™æ˜¯æˆ‘æ‰¾åˆ°çš„å¿«é€Ÿä¸”å…è´¹çš„æµ‹è¯• Llama 3\\.1 405B ç‰ˆæœ¬çš„æœ€ç®€å•æ–¹æ³•ã€‚\n\nè¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é€‰é¡¹ï¼Œé€‚åˆå¸Œæœ›å…è´¹å°è¯•ä½¿ç”¨ API çš„å¼€å‘è€…ã€‚\n\n## æ³¨æ„ï¼š\n\nå¦‚æžœæ‚¨å¸Œæœ›æˆ‘ä»¬çš„å›¢é˜Ÿä½¿ç”¨LLMsåˆ›å»ºå®šåˆ¶çš„AIè½¯ä»¶ï¼Œæˆ–ä¸ºæ‚¨çš„ä¸šåŠ¡åˆ›å»ºå®šåˆ¶çš„AIèŠå¤©æœºå™¨äººï¼Œæ‚¨å¯ä»¥åœ¨è¿™é‡Œ[**è”ç³»æˆ‘**](https://aigrowthguys.com/contact/) âœ‰ï¸ï¼Œæˆ‘ä¼šå°½å¿«å›žå¤æ‚¨ï¼š\n\n[**AI Growth Guys è”ç³»**](https://aigrowthguys.com/contact/)âœ‰ï¸\n\nðŸ‘‰ æ³¨å†Œæˆ‘ä»¬çš„[**å…è´¹5å¤©ç”µå­é‚®ä»¶è¯¾ç¨‹**](https://aigrowthguys.com/5-day-free-course-how-to-grow-your-business-like-a-weed/)ï¼Œåœ¨AIæ—¶ä»£è“¬å‹ƒå‘å±•ðŸš€å¹¶èµšå–ðŸ’²\n\næ‚¨è¿˜å¯ä»¥[**æ³¨å†Œæˆ‘çš„æ–°é—»é€šè®¯**](https://ai-growth-guys.beehiiv.com/subscribe/?via=andrew-best)ï¼Œäº†è§£å¦‚ä½•åˆ©ç”¨AIèµšå–æ›´å¤šæ”¶å…¥ã€‚\n\næŸ¥çœ‹æˆ‘ä»¬çš„[**YouTubeé¢‘é“**](https://www.youtube.com/@aigrowthguys)\n\nåœ¨æˆ‘ä»¬çš„ç½‘ç«™ä¸Šå…³æ³¨æˆ‘ä»¬ï¼š[**AI Growth Guys**](https://aigrowthguys.com/)\n\n"},{"lang":"zh","group":"blog","slug":"blog/llama-3-2-the-next-generation-of-lightweight-instruction-tuned-language-models-a-hands-on-9bca07c8af1d","frontmatter":{"title":"Llama 3.2ï¼šä¸‹ä¸€ä»£è½»é‡çº§ã€æŒ‡ä»¤è°ƒæ•´è¯­è¨€æ¨¡åž‹ï¼šå®žè·µâ€¦â€¦","meta_title":"Llama 3.2ï¼šä¸‹ä¸€ä»£è½»é‡çº§ã€æŒ‡ä»¤è°ƒæ•´è¯­è¨€æ¨¡åž‹ï¼šå®žè·µâ€¦â€¦","description":"æŽ¢ç´¢ LLaMA 3.2 åœ¨ä¿®å‰ªã€çŸ¥è¯†æç‚¼å’Œå¤šè¯­è¨€æ€§èƒ½æ–¹é¢çš„å…³é”®åˆ›æ–°ï¼Œä»¥åŠè¿è¡Œçš„å®žè·µæ•™ç¨‹â€¦â€¦","date":"2024-11-10T03:51:17.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BMalqlcJIFe50hidF4FnqQ.png","categories":["Natural Language Processing","Machine Learning","Generative AI"],"author":"Rifx.Online","tags":["LLaMA","tuning","pruning","distillation","multilingual"],"draft":false,"slug":"blog/llama-3-2-the-next-generation-of-lightweight-instruction-tuned-language-models-a-hands-on-9bca07c8af1d"},"content":"\n### æŽ¢ç´¢ LLaMA 3\\.2 åœ¨å‰ªæžã€çŸ¥è¯†è’¸é¦å’Œå¤šè¯­è¨€æ€§èƒ½æ–¹é¢çš„å…³é”®åˆ›æ–°ï¼Œä»¥åŠæœ¬åœ°è¿è¡Œæˆ–é€šè¿‡ Google Colab çš„å®žç”¨æ•™ç¨‹\n\nðŸ‘¨ðŸ¾â€ðŸ’» [GitHub](https://github.com/mdmonsurali) â­ï¸ \\| ðŸ‘”[LinkedIn](https://www.linkedin.com/in/mdmonsurali/) \\|ðŸ“ [Medium](https://medium.com/@monsuralirana)\n\n\n\n## ä»‹ç»\n\nè¯­è¨€æ¨¡åž‹æŒç»­å‘å±•ï¼ŒæŽ¨åŠ¨ç€æ•ˆçŽ‡ã€é€Ÿåº¦å’Œå¤šè¯­è¨€èƒ½åŠ›çš„è¾¹ç•Œã€‚LLaMA 3\\.2ï¼ˆè½»é‡çº§LLaMAï¼‰ä»£è¡¨äº†è¿™ä¸€è½¨è¿¹ä¸Šçš„ä¸‹ä¸€ä¸ªçªç ´ï¼Œç»“åˆäº†å‰ªæžã€çŸ¥è¯†è’¸é¦å’Œåˆæˆæ•°æ®ç”Ÿæˆç­‰åˆ›æ–°ã€‚åœ¨Metaä¹‹å‰çš„åˆ›æ–°åŸºç¡€ä¸Šï¼ŒLLaMA 3\\.2åœ¨ä¸ç‰ºç‰²é€Ÿåº¦ã€å‡†ç¡®æ€§æˆ–éšç§çš„æƒ…å†µä¸‹ï¼Œæé«˜äº†è¾ƒå°æ¨¡åž‹ï¼ˆ1Bå’Œ3Bå‚æ•°ï¼‰çš„æ€§èƒ½ã€‚åœ¨è¿™ç¯‡åšå®¢ä¸­ï¼Œæˆ‘ä»¬å°†æŽ¢è®¨LLaMA 3\\.2çš„å…³é”®æŠ€æœ¯è¿›å±•ï¼Œè®¨è®ºå…¶åŸºå‡†æµ‹è¯•ç»“æžœï¼Œå¹¶æä¾›åŸºäºŽç ”ç©¶çš„è§†è§’ï¼Œè¯´æ˜Žè¿™äº›åˆ›æ–°çš„é‡è¦æ€§ã€‚æœ€åŽï¼Œæˆ‘ä»¬å°†é€šè¿‡ä¸€ä¸ªå®žè·µæ•™ç¨‹ï¼Œå¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨LangChainå’ŒOllamaéƒ¨ç½²LLaMA 3\\.2ã€‚\n\n## 1\\. LLaMAæ¨¡åž‹çš„æ¼”å˜ï¼šä»Ž1\\.0åˆ°3\\.2\n\n### LLaMA æ¨¡åž‹çš„ç®€å²\n\n**å¤§åž‹è¯­è¨€æ¨¡åž‹ Meta AI (LLaMA)** ç³»åˆ—è‡ªé¦–æ¬¡å‘å¸ƒä»¥æ¥ç»åŽ†äº†æ˜¾è‘—çš„å‘å±•ã€‚Meta çš„ **LLaMA 1\\.0** æ—¨åœ¨ä½¿ LLM çš„èŽ·å–æ›´åŠ æ°‘ä¸»åŒ–ï¼Œæä¾›äº†æ¯” GPT\\-3 ç­‰æ¨¡åž‹æ›´å°‘å‚æ•°çš„é«˜æ€§èƒ½æ¨¡åž‹ï¼ŒåŒæ—¶åœ¨å„ç§ä»»åŠ¡ä¸­å®žçŽ°äº†ç±»ä¼¼çš„å‡†ç¡®æ€§ã€‚LLaMA 2\\.0 å¼•å…¥äº†æŒ‡ä»¤è°ƒä¼˜å’Œå¤šè¯­è¨€æ€§èƒ½çš„æ”¹è¿›ã€‚\n\n**LLaMA 3\\.2** ä»£è¡¨äº†ä¸‹ä¸€ä¸ªé£žè·ƒï¼Œé‡ç‚¹å…³æ³¨ä»¥ä¸‹æ ¸å¿ƒé¢†åŸŸï¼š\n\n* **æŒ‡ä»¤è°ƒä¼˜å’Œå¾®è°ƒ**ï¼šæŒ‡ä»¤è·Ÿéšèƒ½åŠ›çš„å¢žå¼ºä½¿æ¨¡åž‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„è¡¨çŽ°æ›´ä½³ã€‚\n* **è¾¹ç¼˜è®¾å¤‡çš„æ•ˆçŽ‡**ï¼šä¿®å‰ªå’Œè’¸é¦æŠ€æœ¯ä½¿æ¨¡åž‹èƒ½å¤Ÿåœ¨è®¡ç®—èµ„æºæœ‰é™çš„è®¾å¤‡ä¸Šéƒ¨ç½²ï¼Œä¾‹å¦‚æ™ºèƒ½æ‰‹æœºï¼Œè€Œä¸æŸå¤±æ€§èƒ½ã€‚\n* **è§†è§‰å’Œè¯­è¨€ç†è§£**ï¼šå°†è§†è§‰-è¯­è¨€æ¨¡åž‹é›†æˆåˆ° LLaMA 3\\.2 ä¸­ï¼Œèƒ½å¤Ÿå¤„ç†å¤šæ¨¡æ€ä»»åŠ¡ï¼Œä¾‹å¦‚åŸºäºŽå›¾åƒçš„é—®ç­”ã€‚\n\n## 2\\. LLaMA 3\\.2 çš„å…³é”®åˆ›æ–°\n\n### A. æŒ‡ä»¤è°ƒä¼˜ä¸Žå¯¹é½\n\næŒ‡ä»¤è°ƒä¼˜å·²è¢«è¯æ˜Žæ˜¯æé«˜å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰éµå¾ªè‡ªç„¶è¯­è¨€æŒ‡ä»¤èƒ½åŠ›çš„å…³é”®å› ç´ ã€‚åœ¨ LLaMA 3.2 ä¸­ï¼ŒMeta ä½¿ç”¨äº† **ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰**ã€**æ‹’ç»é‡‡æ ·ï¼ˆRSï¼‰** å’Œ **ç›´æŽ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰** æŠ€æœ¯ã€‚è¿™äº›æŠ€æœ¯è¢«è¿­ä»£åº”ç”¨äºŽè®­ç»ƒæ¨¡åž‹ï¼Œä»¥æ›´é«˜çš„å‡†ç¡®æ€§å¤„ç†å„ç§ä»»åŠ¡ï¼Œå¦‚æŽ¨ç†ã€æ‘˜è¦å’Œå·¥å…·ä½¿ç”¨ã€‚\n\n* **ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰**ï¼šæ¨¡åž‹åœ¨äººå·¥æ ‡æ³¨çš„æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œä»Žä¸­å­¦ä¹ ç”Ÿæˆæ›´å—æ¬¢è¿Žçš„è¾“å‡ºã€‚\n* **ç›´æŽ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰**ï¼šä¸€ç§è®­ç»ƒæ¨¡åž‹ç›´æŽ¥ä¼˜åŒ–ç”¨æˆ·åå¥½çš„æŠ€æœ¯ï¼Œä½¿è¾“å‡ºä¸Žäººç±»æœŸæœ›æ›´ç´§å¯†å¯¹é½ã€‚\n\n### B. é«˜æ•ˆå‰ªæžä¸ŽçŸ¥è¯†è’¸é¦\n\nLLaMA 3\\.2 çš„è½»é‡çº§æ¨¡åž‹ï¼Œå¦‚ 1B å’Œ 3B å‚æ•°æ¨¡åž‹ï¼Œåˆ©ç”¨ **ç»“æž„åŒ–å‰ªæž** å’Œ **çŸ¥è¯†è’¸é¦**ã€‚è¿™äº›æŠ€æœ¯åœ¨å‡å°æ¨¡åž‹ä½“ç§¯çš„åŒæ—¶ï¼Œä¿ç•™äº†æ¥è‡ªæ›´å¤§æ¨¡åž‹ï¼ˆä¾‹å¦‚ LLaMA 3\\.1 8B å’Œ 70Bï¼‰çš„å¤§é‡çŸ¥è¯†ï¼š\n\n* **ç»“æž„åŒ–å‰ªæž**ï¼šåœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œç³»ç»Ÿæ€§åœ°ç§»é™¤ç½‘ç»œä¸­é‡è¦æ€§è¾ƒä½Žçš„éƒ¨åˆ†ï¼Œä»¥åˆ›å»ºæ›´å°çš„æ¨¡åž‹ï¼ŒåŒæ—¶ä¿æŒå‡†ç¡®æ€§ã€‚\n* **çŸ¥è¯†è’¸é¦**ï¼šä¸€ä¸ªå¤§åž‹æ¨¡åž‹ï¼ˆæ•™å¸ˆï¼‰å°†çŸ¥è¯†è½¬ç§»åˆ°ä¸€ä¸ªè¾ƒå°çš„æ¨¡åž‹ï¼ˆå­¦ç”Ÿï¼‰ï¼Œä½¿å¾—è¾ƒå°çš„æ¨¡åž‹åœ¨è®­ç»ƒæœŸé—´èƒ½å¤Ÿæ¨¡ä»¿è¾ƒå¤§æ¨¡åž‹çš„æ€§èƒ½ã€‚\n\n### C. æ‰©å±•ä¸Šä¸‹æ–‡é•¿åº¦\n\nLLaMA 3\\.2 çš„ä¸€ä¸ªä¸»è¦æ›´æ–°æ˜¯å…¶å¤„ç†æ›´é•¿ä¸Šä¸‹æ–‡é•¿åº¦çš„èƒ½åŠ›â€”â€”æœ€å¤šå¯è¾¾ **128K tokens**ã€‚è¿™ä½¿å…¶åœ¨å¤„ç†éœ€è¦å¤„ç†å¤§é‡æ–‡æœ¬çš„ä»»åŠ¡æ—¶éžå¸¸é«˜æ•ˆï¼Œä¾‹å¦‚æ‘˜è¦ã€é•¿æ–‡æ¡£åˆ†æžå’Œå¤šè½®å¯¹è¯ã€‚\n\n### D. è§†è§‰-è¯­è¨€æ¨¡åž‹\n\nMeta åœ¨ LLaMA 3.2 ä¸­å¼•å…¥çš„ **è§†è§‰-è¯­è¨€æ¨¡åž‹ (VLMs)** å¼€è¾Ÿäº†å¤šæ¨¡æ€ä»»åŠ¡çš„æ–°é¢†åŸŸã€‚è¿™äº›æ¨¡åž‹æ—¨åœ¨å¤„ç†æ–‡æœ¬å’Œå›¾åƒï¼Œä½¿å…¶åœ¨æ–‡æ¡£é—®ç­”ã€ç§‘å­¦å›¾è¡¨è§£é‡Šå’Œå›¾åƒæè¿°ç­‰åº”ç”¨ä¸­éžå¸¸æœ‰æ•ˆã€‚\n\n## 3\\. åŸºå‡†æ€§èƒ½ï¼šLLaMA 3\\.2 å¦‚ä½•æ¯”è¾ƒï¼Ÿ\n\nLLaMA 3\\.2 åœ¨å¹¿æ³›çš„åŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œäº†ä¸¥æ ¼è¯„ä¼°ï¼Œå¦‚æ‚¨æ‰€åˆ†äº«çš„è¡¨æ ¼æ‰€ç¤ºã€‚ä¸»è¦äº®ç‚¹åŒ…æ‹¬ï¼š\n\n* **ä¸€èˆ¬ä»»åŠ¡**ï¼š3B æ¨¡åž‹åœ¨ **MMLU** (63\\.4) å’Œ **IFEval** (77\\.4) ç­‰åŸºå‡†æµ‹è¯•ä¸­è¡¨çŽ°å‡ºè‰²ï¼Œæ˜¾ç¤ºå‡ºå“è¶Šçš„æŒ‡ä»¤éµå¾ªå’ŒæŽ¨ç†èƒ½åŠ›ã€‚\n* **å·¥å…·ä½¿ç”¨**ï¼šåœ¨ **BFCL V2** ç­‰ä»»åŠ¡ä¸­ï¼ŒLLaMA 3\\.2 (3B) å¾—åˆ† 67\\.0ï¼Œè¶…è¶Šäº† **Gemma 2** å’Œ **Phi\\-3\\.5\\-mini** ç­‰ç«žäº‰å¯¹æ‰‹ï¼Œåœ¨éµå¾ªä¸Žå·¥å…·ä½¿ç”¨ç›¸å…³çš„å¤æ‚æŒ‡ä»¤æ–¹é¢è¡¨çŽ°æ›´ä½³ã€‚\n* **æ•°å­¦å’ŒæŽ¨ç†**ï¼š3B æ¨¡åž‹åœ¨ä¸Žæ•°å­¦ç›¸å…³çš„ä»»åŠ¡ä¸­è¡¨çŽ°å¼ºåŠ²ï¼Œåœ¨ **GSM8K** (å°å­¦æ•°å­¦) ä¸­å¾—åˆ† **77\\.7**ï¼Œåœ¨ **ARC Challenge** ä¸­å¾—åˆ† **78\\.6**ï¼Œè¯¥åŸºå‡†ä¸“æ³¨äºŽæŽ¨ç†ã€‚\n* **å¤šè¯­è¨€ç”Ÿæˆ**ï¼š3B æ¨¡åž‹åœ¨å¤šè¯­è¨€ MGSM åŸºå‡†ä¸­ä¹Ÿè¡¨çŽ°ä¼˜å¼‚ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šç§è¯­è¨€ä¸­ç”Ÿæˆè¿žè´¯æ–‡æœ¬çš„èƒ½åŠ›ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*lpjDJ6AaRnljLwAxtAf-Ag.png)\n\nLLaMA 3\\.2 åœ¨è¿™äº›ä»»åŠ¡ä¸­çš„ä¼˜åŠ¿è¡¨æ˜Žï¼Œå®ƒä¸ºæ¶‰åŠè‡ªç„¶è¯­è¨€ç†è§£ã€æŒ‡ä»¤éµå¾ªå’ŒæŽ¨ç†çš„ä»»åŠ¡æä¾›äº†å¼ºæœ‰åŠ›çš„è§£å†³æ–¹æ¡ˆï¼Œé€‚ç”¨äºŽä¸€èˆ¬å’Œå¤šè¯­è¨€çŽ¯å¢ƒã€‚\n\n## 4\\. å®žè·µæ•™ç¨‹ï¼šä½¿ç”¨ LangChain å’Œ Ollama åœ¨æœ¬åœ°è¿è¡Œ LLaMA 3\\.2\n\nçŽ°åœ¨æˆ‘ä»¬å·²ç»æŽ¢è®¨äº† LLaMA 3\\.2 çš„æŠ€æœ¯è¿›å±•ï¼Œè®©æˆ‘ä»¬é€šè¿‡é€æ­¥æŒ‡å—åœ¨æœ¬åœ°ä½¿ç”¨ **LangChain** å’Œ **Ollama** è¿›è¡Œå®žè·µã€‚æˆ‘ä»¬å¯ä»¥åœ¨æœ¬åœ°æœºå™¨æˆ– Google Colab ç»ˆç«¯ä¸Šå®‰è£…å®ƒã€‚åªéœ€æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š\n\n### æ­¥éª¤ 1ï¼šå®‰è£…æ‰€éœ€çš„åº“\n\né¦–å…ˆï¼Œåœ¨æ‚¨çš„ Python çŽ¯å¢ƒä¸­å®‰è£…æ‰€éœ€çš„åº“ã€‚è¿è¡Œä»¥ä¸‹å‘½ä»¤ä»¥è®¾ç½® LangChain å’Œ Ollamaï¼š\n\n```python\n!pip install langchain\n!pip install -U langchain-community\n!pip install langchain_ollama\n```\n\n### ç¬¬2æ­¥ï¼šå®‰è£…å¹¶åŠ è½½ Colab\\-XTerm\n\nColab\\-XTerm æ˜¯ä¸€ä¸ªæ–¹ä¾¿çš„åŒ…ï¼Œå¯ä»¥åœ¨ Colab ç¬”è®°æœ¬ä¸­å¯ç”¨ç»ˆç«¯è®¿é—®ã€‚è¿™å¯¹äºŽç›´æŽ¥åœ¨ç¬”è®°æœ¬çŽ¯å¢ƒä¸­è¿è¡Œ shell å‘½ä»¤éžå¸¸æœ‰ç”¨ã€‚è¦å®‰è£…å®ƒï¼Œè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š\n\n```python\n!pip install colab-xterm\n%load_ext colabxterm\n```\n\n### ç¬¬ 3 æ­¥ï¼šå®‰è£… Ollama\n\næ‚¨å¯ä»¥é€šè¿‡è¿è¡Œä»¥ä¸‹å‘½ä»¤æ‰“å¼€ç»ˆç«¯ä¼šè¯ï¼š\n\n```python\n%xterm\n```\n\nåœ¨ç»ˆç«¯ä¸­ï¼Œè¿è¡Œä»¥ä¸‹å‘½ä»¤ä»¥å®‰è£… Ollamaï¼š\n\n```python\ncurl -fsSL https://ollama.com/install.sh | sh\n```\n\n```python\nollama serve\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*itAzyQHMHhin8b7bRLc09w.png)\n\n### ç¬¬4æ­¥ï¼šæ‹‰å–æ¨¡åž‹\n\nå®‰è£…å®ŒOllamaåŽï¼Œæ‚¨å¯ä»¥æ‹‰å–æ‰€éœ€çš„æ¨¡åž‹ã€‚Ollamaæä¾›äº†å¤šä¸ªLLMï¼ŒåŒ…æ‹¬Llama 3\\.2\\. ä»¥ä¸‹æ˜¯æ‹‰å–å®ƒä»¬çš„æ–¹æ³•ï¼š\n\n```python\nollama pull llama3.2\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*S3R4gByToCZXKEWBh4GWaQ.png)\n\nä¸Šè¿°å‘½ä»¤å°†ä¸‹è½½å¹¶å‡†å¤‡æ¨¡åž‹ä»¥ä¾›åœ¨æ‚¨çš„ColabçŽ¯å¢ƒä¸­ä½¿ç”¨ã€‚\n\næˆ–è€…ï¼Œæ‹‰å–Ollamaä¸­å¯ç”¨çš„ä»»ä½•LLMæ¨¡åž‹ã€‚æ‰€æœ‰LLMæ¨¡åž‹åˆ—è¡¨å’Œè¯¦ç»†ä¿¡æ¯å¯åœ¨æ­¤æŸ¥çœ‹ï¼š[https://ollama.com/library](https://ollama.com/library)\n\n### ç¬¬5æ­¥ï¼šå°†LLaMA 3\\.2ä¸ŽLangChainé›†æˆ\n\nLangChainä½¿å¾—è°ƒç”¨LLaMA 3\\.2è¿›è¡Œå„ç§NLPä»»åŠ¡å˜å¾—ç®€å•ã€‚ä»¥ä¸‹æ˜¯æµ‹è¯•æ¨¡åž‹çš„ç®€å•è„šæœ¬ï¼š\n\n```python\nfrom langchain_community.llms import Ollama\n\n## Initialize an instance of the Llama 3.1 model\nllm_llama = Ollama(model=\"llama3.2\")\n\n## Invoke the model to generate a response\nresponse = llm_llama.invoke(\"Tell me a joke\")\nprint(response)\n```\n\nè¾“å‡ºï¼š\n\n```python\nHere's one:\n\nWhat do you call a fake noodle?\n\nAn impasta.\n```\n\n### ç¬¬6æ­¥ï¼šå°è¯•ä¸åŒçš„ä»»åŠ¡\n\næ‚¨å¯ä»¥å°†å…¶æ‰©å±•åˆ°æ›´å¤æ‚çš„ä»»åŠ¡ï¼Œå¦‚æ‘˜è¦ã€ multilingual translation å’ŒæŽ¨ç†ï¼š\n\n```python\n## Summarization\nresponse = llm_llama.invoke(\"Summarize the following text: 'LLaMA 3.2 represents a major step forward in AI development...'\")\nprint(response)\n\n## Multilingual Generation\nresponse = llm_llama.invoke(\"Translate the following into French: 'What are the major improvements in LLaMA 3.2?'\")\nprint(response)\n```\n\nè¾“å‡ºï¼š\n\n```python\nQuantum Mechanics is a complex and fascinating subject, but I'll try to break it down in simple terms.\n\n**The Basics**\n\nImagine you have a coin. Heads or tails, right? In classical physics (the way things work today), the coin is either one or the other - heads or tails. It's like a definite choice.\n\nIn Quantum Mechanics, however, the coin isn't quite so simple. When you flip it, it doesn't just land on heads or tails; it exists in both states at the same time! This idea might sound crazy, but that's basically what happens with tiny particles like atoms and electrons.\n\n**Wave-Particle Duality**\n\nHere's a key concept: tiny particles can behave like both waves and particles. It sounds weird, but think of it like this:\n\n* Imagine a wave in the ocean. The water molecules are moving up and down, creating ripples.\n* Now imagine a single water molecule as a particle (a tiny ball). That's what quantum mechanics says these particles can be!\n\n**Superposition**\n\nAnother mind-bending idea is superposition. It means that tiny particles can exist in multiple states at the same time. Think of it like this:\n\n* Imagine a coin that's both heads AND tails simultaneously!\n* This happens with electrons, which can spin both clockwise and counterclockwise at the same time.\n\n**Entanglement**\n\nQuantum Mechanics also introduces entanglement. When two particles interact, they become \"connected\" in such a way that what happens to one particle instantly affects the other, no matter how far apart they are!\n\n* Imagine two dancers who are perfectly synchronized, even if they're on opposite sides of the stage.\n* This is basically entanglement: two particles can be connected in a similar way.\n\n**The Weird Stuff**\n\nNow we get to some really weird and interesting aspects of quantum mechanics:\n\n* **Uncertainty Principle**: You can't know both the position AND momentum of a particle at the same time!\n* **Quantum Tunneling**: Particles can pass through solid objects, which is weird because they shouldn't be able to fit through.\n* **SchrÃ¶dinger's Cat**: Imagine a cat in a box with a radioactive atom. If the atom decays, the cat dies. But according to quantum mechanics, the cat is BOTH alive AND dead until someone opens the box and observes it!\n\n**In a Nutshell**\n\nQuantum Mechanics is all about tiny particles behaving in strange and fascinating ways. It's like they're playing by their own rules, which can lead to some pretty wild and unpredictable outcomes.\n\nWhile this explanation is simplified, it should give you an idea of how quantum mechanics works and its weird and wonderful principles!\n```\n\nè¾“å‡ºï¼š\n\n```python\nThe translation of \"What are the major improvements in LLaMA 3.2?\" into French is:\n\n\"Quels sont les amÃ©liorations majeures de LLaMA 3.2?\"\n\nHere's a breakdown of the translation:\n\n- \"What\" becomes \"Quels\"\n- \"are\" becomes \"sont\"\n- \"the\" becomes \"les\"\n- \"major improvements\" become \"amÃ©liorations majeures\"\n- \"in\" becomes \"sont\"\n- \"LLaMA 3.2\" remains the same, as it's a proper noun.\n\nNote: The phrase \"majeures\" is used to describe significant or substantial improvements.\n```\n\n> **èŽ·å– GitHub ä»£ç ï¼š**\n\n## ç»“è®º\n\nLLaMA 3\\.2 æ˜¯ä¸€ä¸ªå¤šåŠŸèƒ½ä¸”é«˜æ•ˆçš„æ¨¡åž‹ï¼Œåœ¨å¤šç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨çŽ°å‡ºè‰²ï¼Œä»Žå¤šè¯­è¨€æ–‡æœ¬ç”Ÿæˆåˆ°å®žç”¨å·¥å…·ä½¿ç”¨ã€‚å®ƒåœ¨å‰ªæžå’ŒçŸ¥è¯†è’¸é¦æ–¹é¢çš„åˆ›æ–°ç¡®ä¿äº†å®ƒåœ¨è½»é‡çº§ã€èµ„æºå—é™çŽ¯å¢ƒä¸­ä»èƒ½ä¿æŒé¡¶çº§æ€§èƒ½ã€‚é€šè¿‡æœ¬æ•™ç¨‹ï¼Œæ‚¨å¯ä»¥å¿«é€Ÿå°† LLaMA 3\\.2 é›†æˆåˆ°æœ¬åœ°åº”ç”¨ç¨‹åºæˆ–é€šè¿‡ Google Colab ç­‰äº‘æœåŠ¡ä¸­ã€‚\n\né€šè¿‡è§£é” LLaMA 3\\.2 çš„èƒ½åŠ›ï¼Œå¼€å‘è€…å¯ä»¥åˆ›å»ºå‰æ²¿çš„åº”ç”¨ç¨‹åºï¼Œè¿™äº›åº”ç”¨ä¸ä»…å¿«é€Ÿã€å“åº”çµæ•ï¼Œè€Œä¸”æ³¨é‡éšç§ï¼Œç¡®ä¿ç”¨æˆ·æ•°æ®ä¿ç•™åœ¨è®¾å¤‡ä¸Šã€‚æ— è®ºæ‚¨æ˜¯åœ¨æŽ¢ç´¢è‡ªç„¶è¯­è¨€å¤„ç†è¿˜æ˜¯æž„å»ºå®žé™…åº”ç”¨ï¼ŒLLaMA 3\\.2 éƒ½ä¸ºè½»é‡çº§ã€æŒ‡ä»¤è°ƒä¼˜çš„è¯­è¨€æ¨¡åž‹è®¾å®šäº†æ–°çš„åŸºå‡†ã€‚\n\næ¬¢è¿Žæ‚¨æŽ¢ç´¢ Ollama åº“ä¸­çš„å…¶ä»–æ¨¡åž‹ï¼Œå¹¶å°è¯•ä¸åŒçš„ä»»åŠ¡ã€‚å¯èƒ½æ€§æ˜¯æ— ç©·æ— å°½çš„ï¼\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/longrag-giving-ai-a-bigger-net-to-catch-more-fish-in-the-sea-of-information-7ecdd63f330d","frontmatter":{"title":"LongRAGï¼šè®©äººå·¥æ™ºèƒ½åœ¨ä¿¡æ¯æµ·æ´‹ä¸­æ•æžæ›´å¤šé±¼","meta_title":"LongRAGï¼šè®©äººå·¥æ™ºèƒ½åœ¨ä¿¡æ¯æµ·æ´‹ä¸­æ•æžæ›´å¤šé±¼","description":"åœ¨æˆ‘ä¹‹å‰çš„æ–‡ç« ä¸­ï¼Œæˆ‘ä»‹ç»äº† RAG æ˜¯å¦ä¼šå› é•¿è¯­å¢ƒ LLM è€Œè¿‡æ—¶ã€‚ä»Šå¤©ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹å¦‚ä½•ç”³è¯·â€¦â€¦","date":"2024-11-08T00:17:39.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Nt5TRh0ooDkgmibMlA1Srg.png","categories":["Generative AI","Natural Language Processing","Data Science"],"author":"Rifx.Online","tags":["long-context","LLMs","RAG","retrieval","generation"],"draft":false,"slug":"blog/longrag-giving-ai-a-bigger-net-to-catch-more-fish-in-the-sea-of-information-7ecdd63f330d"},"content":"\nåœ¨ [æˆ‘ä¹‹å‰çš„æ–‡ç« ](https://readmedium.com/will-long-context-llms-cause-the-extinction-of-rag-de41ca5ddfc6) ä¸­ï¼Œæˆ‘ä»‹ç»äº†RAGæ˜¯å¦ä¼šå› é•¿ä¸Šä¸‹æ–‡LLMsè€Œå˜å¾—è¿‡æ—¶ã€‚ä»Šå¤©ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•å°†é•¿ä¸Šä¸‹æ–‡LLMsåº”ç”¨äºŽRAGåœºæ™¯ã€‚\n\nåœ¨æ£€ç´¢å¢žå¼ºç”Ÿæˆï¼ˆRAGï¼‰é¢†åŸŸï¼Œä¼ ç»Ÿæ–¹æ³•ä¸€ç›´ä¾èµ–äºŽçŸ­æ£€ç´¢å•å…ƒï¼Œé€šå¸¸çº¦ä¸º100ä¸ªå•è¯ï¼Œè¿™è¿«ä½¿æ£€ç´¢å™¨åœ¨åºžå¤§çš„è¯­æ–™åº“ä¸­ç­›é€‰ä»¥æå–å¿…è¦ä¿¡æ¯ã€‚è¿™ç§è®¾è®¡è™½ç„¶å¯è¡Œï¼Œä½†å¯¹æ£€ç´¢å™¨æ–½åŠ äº†ä¸å¹³è¡¡çš„è´Ÿæ‹…ï¼Œå¾€å¾€å› å…¶å¿…é¡»å¤„ç†çš„å•å…ƒæ•°é‡åºžå¤§è€Œå¯¼è‡´è¡¨çŽ°ä¸ä½³ã€‚\n\næœ¬æ–‡ä»‹ç»äº†ä¸€é¡¹æ–°ç ”ç©¶ï¼Œæ ‡é¢˜ä¸ºâ€œ[LongRAG: ä½¿ç”¨é•¿ä¸Šä¸‹æ–‡LLMså¢žå¼ºæ£€ç´¢å¢žå¼ºç”Ÿæˆ](https://arxiv.org/pdf/2406.15319v3)â€ã€‚å®ƒæ—¨åœ¨é€šè¿‡æå‡ºä¸€ç§æ–°é¢–çš„æ¡†æž¶æ¥è§£å†³è¿™ç§ä¸å¹³è¡¡ï¼Œä»Žè€Œå°†æ£€ç´¢å•å…ƒçš„é•¿åº¦æ‰©å±•åˆ°4,000ä¸ªæ ‡è®°ï¼Œæ˜¾è‘—æé«˜æ£€ç´¢å™¨çš„æ•ˆçŽ‡å’Œè¯»è€…çš„è¡¨çŽ°ã€‚\n\n## ä¼ ç»Ÿ RAG ä¸Ž LongRAG\n\n\n\nå¦‚å›¾ 1 æ‰€ç¤ºï¼ŒLongRAG çš„æ ¸å¿ƒåˆ›æ–°åœ¨äºŽå…¶å¯¹ä¼ ç»Ÿ RAG æ¡†æž¶çš„é‡æž„ã€‚é€šè¿‡å°†æ£€ç´¢å•å…ƒçš„å¤§å°æ‰©å±•åˆ° 4K tokensâ€”â€”æ˜¯å…¸åž‹å•å…ƒçš„ 30 å€â€”â€”LongRAG å°†å•å…ƒæ•°é‡ä»Žæ•°ç™¾ä¸‡å‡å°‘åˆ°å¯ç®¡ç†çš„å‡ åä¸‡ä¸ªã€‚\n\nè¿™ç§æ–¹æ³•ä¸ä»…å‡è½»äº†æ£€ç´¢å™¨çš„è´Ÿæ‹…ï¼Œè¿˜å¢žå¼ºäº†æ‰€æ£€ç´¢ä¿¡æ¯çš„è¯­ä¹‰å®Œæ•´æ€§ï¼Œä»Žè€Œæé«˜äº†ä¸‹æ¸¸æ€§èƒ½ã€‚\n\n## LongRAG\n\nLongRAGæ¡†æž¶ç”±ä¸¤ä¸ªä¸»è¦ç»„ä»¶ç»„æˆï¼š**Long Retriever**å’Œ**Long Reader**ã€‚è¿™ä¸¤ä¸ªç»„ä»¶çš„ç¤ºä¾‹å¦‚å›¾2æ‰€ç¤ºã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*fs37A8QUj-y2rW9_iAqS3Q.png)\n\nLong Retrieveré€šè¿‡å°†ç›¸å…³æ–‡æ¡£åˆ†ç»„ä¸ºä¿æŒè¯­ä¹‰å®Œæ•´æ€§çš„ç»Ÿä¸€ä½“æ¥ç»„ç»‡æ£€ç´¢è¿‡ç¨‹ã€‚ä¸€æ—¦è¯†åˆ«å‡ºç›¸å…³çš„é•¿æ£€ç´¢å•å…ƒï¼Œå®ƒä»¬å°†è¢«ä¼ é€’ç»™Long Readerï¼Œè¯¥ç»„ä»¶èƒ½å¤Ÿå¤„ç†å¹¿æ³›çš„ä¸Šä¸‹æ–‡ï¼ˆå¤§çº¦30Kä¸ªæ ‡è®°ï¼‰ã€‚\n\nä»¥ä¸‹æ˜¯å·¥ä½œæµç¨‹çš„é€æ­¥åˆ†è§£ï¼š\n\n### 1\\. åˆ¶å®šé•¿æ£€ç´¢å•å…ƒ\n\nLongRAGçš„ç¬¬ä¸€æ­¥æ˜¯åˆ›å»ºé•¿æ£€ç´¢å•å…ƒã€‚\n\n**åœ¨ä¼ ç»Ÿçš„RAG**æ¡†æž¶ä¸­ï¼Œæ£€ç´¢å•å…ƒè¾ƒçŸ­ï¼Œé€šå¸¸åªæœ‰å‡ ç™¾ä¸ªæ ‡è®°ï¼Œè¿™å¯èƒ½å¯¼è‡´ä¿¡æ¯ç¢Žç‰‡åŒ–ï¼Œå¹¶ä¸”ç»™æ£€ç´¢å™¨å¸¦æ¥é‡å¤§çš„è´Ÿæ‹…ï¼Œéœ€è¦å°†ç›¸å…³ä¸Šä¸‹æ–‡æ‹¼å‡‘åœ¨ä¸€èµ·ã€‚\n\n**LongRAGè§£å†³äº†è¿™ä¸ªé—®é¢˜**ï¼Œé€šè¿‡å°†ç›¸å…³æ–‡æ¡£åˆ†ç»„ä¸ºè¿žè´¯çš„é•¿æ£€ç´¢å•å…ƒï¼Œè¿™äº›å•å…ƒæ˜¾è‘—æ›´å¤§ â€” æ¯ä¸ªå•å…ƒå¯è¾¾4,000ä¸ªæ ‡è®°ã€‚\n\nä¸ºäº†å½¢æˆè¿™äº›é•¿å•å…ƒï¼ŒLongRAGé‡‡ç”¨äº†ä¸€ç§åˆ†ç»„ç®—æ³•ï¼Œæ ¹æ®æ–‡æ¡£ä¹‹é—´çš„å…³ç³»ç»„ç»‡æ–‡æ¡£ï¼Œä¾‹å¦‚ç»´åŸºç™¾ç§‘æ–‡ç« ä¸­åµŒå…¥çš„è¶…é“¾æŽ¥ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*zPEDmLo7rcdCQ06e.png)\n\nä¾‹å¦‚ï¼Œå…³äºŽç‰¹å®šä¸»é¢˜æˆ–å®žä½“çš„æ–‡æ¡£è¢«åˆ†ç»„åœ¨ä¸€èµ·ï¼Œä»¥åˆ›å»ºä¸€ä¸ªç»¼åˆçš„æ£€ç´¢å•å…ƒï¼ˆå›¾2ï¼‰ã€‚è¿™ç¡®ä¿äº†æ¯ä¸ªå•å…ƒä¿æŒè¯­ä¹‰å®Œæ•´æ€§ï¼Œå¹¶ä¸ºè¯»è€…æä¾›äº†æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ï¼Œä»¥ä¾¿ä»Žä¸­æå–ç­”æ¡ˆã€‚\n\n### 2\\. ç›¸ä¼¼æ€§æœç´¢ä¸ŽæŽ’å\n\nä¸€æ—¦å½¢æˆäº†é•¿æ£€ç´¢å•å…ƒï¼Œä¸‹ä¸€æ­¥å°±æ˜¯æ‰§è¡Œç›¸ä¼¼æ€§æœç´¢ï¼Œä»¥è¯†åˆ«å“ªäº›å•å…ƒä¸ŽæŸ¥è¯¢æœ€ç›¸å…³ã€‚\n\næŸ¥è¯¢é€šè¿‡ç¼–ç å™¨å‡½æ•° E\\_Q ç¼–ç ä¸ºä¸€ä¸ªå‘é‡ï¼Œæ¯ä¸ªæ£€ç´¢å•å…ƒä¹Ÿé€šè¿‡å¦ä¸€ä¸ªç¼–ç å™¨å‡½æ•° E\\_C è¿›è¡Œç±»ä¼¼çš„ç¼–ç ã€‚æŸ¥è¯¢ `q` ä¸Žæ¯ä¸ªæ£€ç´¢å•å…ƒ `g` ä¹‹é—´çš„ç›¸ä¼¼æ€§é€šè¿‡å®ƒä»¬å„è‡ªå‘é‡çš„ç‚¹ç§¯æ¥è®¡ç®—ã€‚\n\nç„¶è€Œï¼Œè€ƒè™‘åˆ°æ£€ç´¢å•å…ƒçš„é•¿åº¦ï¼Œ**ç›´æŽ¥ç¼–ç æ•´ä¸ªå•å…ƒå¯èƒ½è®¡ç®—å¼€é”€å¤§ä¸”æ•ˆæžœè¾ƒå·®**ã€‚**ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒLongRAG é€šè¿‡å°†é•¿å•å…ƒåˆ†è§£ä¸ºæ›´å°çš„å—æ¥è¿‘ä¼¼ç›¸ä¼¼æ€§**ï¼Œå¹¶è®¡ç®—è¿™äº›å—ä¹‹é—´çš„æœ€å¤§ç›¸ä¼¼æ€§å¾—åˆ†ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*U1BsMZuyXqO1oqsl.png)\n\nè¿™ç§æ–¹æ³•ç±»ä¼¼äºŽ[ä»¥å¾€å·¥ä½œçš„ MaxP è®¾è®¡](https://arxiv.org/pdf/1905.09217)ï¼Œä½¿ LongRAG èƒ½å¤Ÿé«˜æ•ˆåœ°è¯†åˆ«æ¯ä¸ªé•¿æ£€ç´¢å•å…ƒä¸­æœ€ç›¸å…³çš„éƒ¨åˆ†ï¼Œè€Œä¸ä¼šç‰ºç‰²æ€§èƒ½ã€‚\n\n### 3\\. èšåˆæ£€ç´¢ç»“æžœ\n\nåœ¨è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°åŽï¼ŒåŸºäºŽä¸ŽæŸ¥è¯¢çš„ç›¸å…³æ€§é€‰æ‹©å‰ k ä¸ªæ£€ç´¢å•å…ƒã€‚**è¿™äº›é€‰å®šçš„å•å…ƒéšåŽè¢«è¿žæŽ¥èµ·æ¥å½¢æˆä¸€ä¸ªå•ä¸€çš„é•¿ä¸Šä¸‹æ–‡ï¼Œé€šå¸¸åŒ…å«çº¦ 30,000 ä¸ªæ ‡è®°ã€‚** è¿™ä¸ªèšåˆçš„ä¸Šä¸‹æ–‡å°†è¢«ä¼ é€’ç»™ Long Readerã€‚\n\nk çš„å¤§å°æˆ–æ£€ç´¢å•å…ƒçš„æ•°é‡å¯¹äºŽå¹³è¡¡å·¥ä½œè´Ÿè½½è‡³å…³é‡è¦ã€‚å¦‚æžœæ£€ç´¢å•å…ƒå¤ªçŸ­ï¼Œåˆ™éœ€è¦æ›´å¤šå•å…ƒï¼Œè¿™å¯èƒ½ä¼šä½¿é˜…è¯»å™¨ä¸å ªé‡è´Ÿã€‚ç›¸åï¼Œå¦‚æžœå•å…ƒå¤ªé•¿ï¼Œåˆ™éœ€è¦çš„æ•°é‡è¾ƒå°‘ï¼Œä½†å¿…é¡»é«˜åº¦ç›¸å…³ï¼Œä»¥é¿å…åŒ…å«å¤šä½™çš„ä¿¡æ¯ã€‚\n\nLongRAG é€šè¿‡ä½¿ç”¨é€‚é‡çš„ç»“æž„è‰¯å¥½çš„é•¿æ£€ç´¢å•å…ƒæ¥ä¼˜åŒ–è¿™ç§å¹³è¡¡ï¼Œé€šå¸¸åœ¨ 4 åˆ° 8 ä¹‹é—´ï¼Œå…·ä½“å–å†³äºŽä»»åŠ¡ã€‚\n\n### 4\\. é€šè¿‡é•¿é˜…è¯»å™¨å¤„ç†\n\né•¿é˜…è¯»å™¨æ˜¯è´Ÿè´£ä»Žé•¿ä¸Šä¸‹æ–‡ä¸­æå–æœ€ç»ˆç­”æ¡ˆçš„ç»„ä»¶ã€‚æ­¤æ­¥éª¤åˆ©ç”¨å…ˆè¿›çš„é•¿ä¸Šä¸‹æ–‡è¯­è¨€æ¨¡åž‹ï¼Œå¦‚GPT-4oæˆ–Gemini-1.5-Proï¼Œèƒ½å¤Ÿå¤„ç†å¤§é‡æ–‡æœ¬åºåˆ—è€Œä¸ä¸¢å¤±å…³é”®ä¿¡æ¯ã€‚\n\nå¯¹äºŽè¾ƒçŸ­çš„ä¸Šä¸‹æ–‡ï¼ˆå°‘äºŽ1,000ä¸ªtokensï¼‰ï¼Œé•¿é˜…è¯»å™¨ç›´æŽ¥æå–ç­”æ¡ˆã€‚ç„¶è€Œï¼Œå¯¹äºŽå…¸åž‹çš„é•¿RAGçš„è¾ƒé•¿ä¸Šä¸‹æ–‡ï¼Œè¯¥è¿‡ç¨‹æ›´åŠ ç»†è‡´ã€‚æœ€åˆï¼Œæ¨¡åž‹ç”Ÿæˆä¸€ä¸ªæ¶µç›–å‡ å¥è¯çš„è¯¦ç»†å“åº”ï¼Œç¡®ä¿æ•æ‰åˆ°æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚ç„¶åŽï¼Œé€šè¿‡ç¬¬äºŒè½®å¤„ç†ï¼Œé•¿é˜…è¯»å™¨å¯¹åˆå§‹è¾“å‡ºè¿›è¡Œç²¾ç‚¼ï¼Œå°†å“åº”æµ“ç¼©ä¸ºä¸€ä¸ªç²¾ç¡®ã€ç®€æ´çš„ç­”æ¡ˆã€‚\n\nè¿™ç§ä¸¤æ­¥æ³•ç¡®ä¿é•¿é˜…è¯»å™¨èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†é•¿æ£€ç´¢å•å…ƒæä¾›çš„å¤§é‡ä¿¡æ¯ï¼ŒåŒæ—¶ä»ç„¶æä¾›å‡†ç¡®ä¸”é›†ä¸­çš„ç­”æ¡ˆã€‚\n\n## è¯„ä¼°\n\næœ¬æ–‡å¯¹ LongRAG åœ¨çŸ¥åæ•°æ®é›†ä¸Šçš„è¡¨çŽ°è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå¦‚ Natural Questions (NQ) å’Œ HotpotQAã€‚ç»“æžœä»¤äººä¿¡æœï¼Œæ£€ç´¢æ€§èƒ½æœ‰æ‰€æå‡ï¼ŒNQ çš„ç­”æ¡ˆå¬å›žçŽ‡ä»Ž 52% æå‡è‡³ 71%ï¼ˆå›¾ 4ï¼‰ï¼ŒHotpotQA çš„ç­”æ¡ˆå¬å›žçŽ‡ä»Ž 47% æå‡è‡³ 72%ï¼ˆå›¾ 5ï¼‰ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*wLUdp-4OihjAz8Fu.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*vmTsnuIsV6LxJFtj.png)\n\n## ç»“è®º\n\næœ¬æ–‡æŽ¢è®¨äº†åˆ›æ–°çš„ LongRAG æ¡†æž¶ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡æ‰©å±• RAG æ¡†æž¶ä»¥å¤„ç†é•¿æ–‡æ¡£çš„åˆ›æ–°æ–¹æ³•ï¼Œä½¿æ¨¡åž‹èƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†å’Œç”Ÿæˆæ¥è‡ªæ‰©å±•ä¸Šä¸‹æ–‡çš„ç­”æ¡ˆã€‚å®ƒç»“åˆäº†ä¸€ä¸ªå¤šæ­¥éª¤æ£€ç´¢è¿‡ç¨‹ï¼ŒåŠ¨æ€æ£€ç´¢é•¿æ–‡æœ¬çš„ç›¸å…³éƒ¨åˆ†ï¼Œç¡®ä¿åœ¨ç”Ÿæˆé˜¶æ®µä½¿ç”¨æœ€ç›¸å…³çš„ä¿¡æ¯ã€‚è¿™ä½¿å¾— LongRAG åœ¨éœ€è¦ç†è§£å’Œç»¼åˆæ¥è‡ªå†—é•¿å¤æ‚æ–‡æ¡£çš„ä¿¡æ¯çš„ä»»åŠ¡ä¸­è¡¨çŽ°å‡ºè‰²ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ä¼˜äºŽä¼ ç»Ÿçš„ RAG æ¨¡åž‹ã€‚\n\nç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å¹¶éžæ²¡æœ‰æŒ‘æˆ˜ã€‚å¯¹å¼ºå¤§çš„é•¿ä¸Šä¸‹æ–‡æ¨¡åž‹çš„ä¾èµ–æ„å‘³ç€è¯¥æ¡†æž¶çš„æ€§èƒ½ä¸Žè¿™äº›æ¨¡åž‹çš„èƒ½åŠ›ç´§å¯†ç›¸å…³ã€‚æ­¤å¤–ï¼Œç”¨äºŽåˆ›å»ºé•¿æ£€ç´¢å•å…ƒçš„åˆ†ç»„ç®—æ³•å¯èƒ½éœ€è¦è¿›ä¸€æ­¥æ”¹è¿›ï¼Œä»¥ä¾¿åœ¨è¶…è¶ŠåŸºäºŽç»´åŸºç™¾ç§‘çš„è¯­æ–™åº“æ—¶è¿›è¡Œæ³›åŒ–ã€‚\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/meet-ministral-3b-and-8b-edge-ai-game-changers-3f7532da8f90","frontmatter":{"title":"è®¤è¯† Ministral 3B å’Œ 8Bï¼šè¾¹ç¼˜ AI æ¸¸æˆè§„åˆ™æ”¹å˜è€…","meta_title":"è®¤è¯† Ministral 3B å’Œ 8Bï¼šè¾¹ç¼˜ AI æ¸¸æˆè§„åˆ™æ”¹å˜è€…","description":"Mistral AI åœ¨è¾¹ç¼˜ AI å’Œè®¾å¤‡è®¡ç®—é¢†åŸŸçš„æ–°å‰æ²¿","date":"2024-11-01T03:55:06.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*3CmWlEiW7ea8gtqxpI83_w.png","categories":["Technology","Autonomous Systems","Data Science"],"author":"Rifx.Online","tags":["Mistral","edge","computing","translation","robotics"],"draft":false,"slug":"blog/meet-ministral-3b-and-8b-edge-ai-game-changers-3f7532da8f90"},"content":"\n\n\n### Mistral AIåœ¨è¾¹ç¼˜AIå’Œè®¾å¤‡è®¡ç®—çš„æ–°å‰æ²¿\n\nåœ¨å¿«é€Ÿå‘å±•çš„AIé¢†åŸŸï¼Œè¾¹ç¼˜è®¡ç®—å˜å¾—è¶Šæ¥è¶Šé‡è¦ï¼Œé€‚ç”¨äºŽé‚£äº›éœ€è¦ä½Žå»¶è¿Ÿã€ä»¥éšç§ä¸ºé¦–çš„é«˜æ•ˆæŽ¨ç†çš„åº”ç”¨ï¼Œè€Œä¸ä¾èµ–äºŽåŸºäºŽäº‘çš„åŸºç¡€è®¾æ–½ã€‚\n\n**Mistral AI**æœ€æ–°æŽ¨å‡ºçš„[**Ministral**](https://mistral.ai/news/ministraux/)æ¨¡åž‹å®¶æ—ï¼Œæ ‡å¿—ç€AIé¢†åŸŸçš„ä¸€æ¬¡çªç ´æ€§è¿›å±•ã€‚\n\nä¸ºåº†ç¥å…¶å¼€åˆ›æ€§çš„**Mistral 7B**æ¨¡åž‹å‘å¸ƒä¸€å‘¨å¹´ï¼ŒMistral AIæŽ¨å‡ºäº†ä¸‹ä¸€ä»£è¯­è¨€æ¨¡åž‹ï¼š**Ministral 3B**å’Œ**Ministral 8B**ï¼Œç»Ÿç§°ä¸ºâ€œ[**les Ministraux**](https://mistral.ai/news/ministraux/)â€ã€‚è¿™äº›æ¨¡åž‹ä¸ä»…ä»…æ˜¯æ¸è¿›å¼çš„æ”¹è¿›ï¼›å®ƒä»¬ä»£è¡¨äº†è¾¹ç¼˜AIå¯èƒ½æ€§çš„é‡å¤§é£žè·ƒã€‚\n\n\n\n## ä¸ºä»€ä¹ˆè¿™äº›æ¨¡åž‹å¾ˆé‡è¦ï¼Ÿ\n\nè¾¹ç¼˜äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒåœ¨äºŽåœ¨æœ¬åœ°æ‰§è¡Œå¤æ‚è®¡ç®—ï¼Œç¡®ä¿æ•°æ®éšç§å¹¶å‡å°‘å“åº”æ—¶é—´ã€‚é€šè¿‡ **Ministral 3B** å’Œ **Ministral 8B**ï¼ŒMistral AI æä¾›äº†å°†é«˜è®¡ç®—èƒ½åŠ›ä¸Žå†…å­˜æ•ˆçŽ‡ç›¸ç»“åˆçš„æ¨¡åž‹ï¼Œæ‰€æœ‰è¿™äº›éƒ½å¯ä»¥ç›´æŽ¥åœ¨è®¾å¤‡ä¸Šè¿è¡Œã€‚è¿™äº›æ¨¡åž‹æ—¨åœ¨ä¸ºæ— æ³•æ‰¿å—å»¶è¿Ÿæˆ–ä¾èµ–äº‘è¿žæŽ¥çš„åº”ç”¨ç¨‹åºæä¾›å®žæ—¶æ´žå¯Ÿã€‚\n\n## ä¸»è¦ç‰¹ç‚¹ï¼š\n\n1. **æœ€å…ˆè¿›çš„æ€§èƒ½**ï¼šåœ¨çŸ¥è¯†ã€å¸¸è¯†ã€æŽ¨ç†ã€åŽŸç”Ÿå‡½æ•°è°ƒç”¨å’Œå°äºŽ10Bç±»åˆ«çš„æ•ˆçŽ‡ç­‰ä¸åŒä»»åŠ¡ä¸­è¶…è¶ŠçŽ°æœ‰æ¨¡åž‹ã€‚\n2. **å¤§ä¸Šä¸‹æ–‡çª—å£**ï¼šæ”¯æŒæœ€å¤š128kçš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œå®žçŽ°æ›´å…¨é¢çš„ç†è§£å’Œç”Ÿæˆã€‚\n3. **é«˜æ•ˆæž¶æž„**ï¼šMinistral 8Bé‡‡ç”¨ç‰¹æ®Šçš„äº¤é”™æ»‘åŠ¨çª—å£æ³¨æ„åŠ›æ¨¡å¼ï¼Œå®žçŽ°æ›´å¿«å’Œæ›´èŠ‚çœå†…å­˜çš„æŽ¨ç†ã€‚\n4. **å¤šåŠŸèƒ½æ€§**ï¼šé€‚ç”¨äºŽå¹¿æ³›çš„åº”ç”¨ï¼Œä»Žè®¾å¤‡å†…ç¿»è¯‘åˆ°è‡ªä¸»æœºå™¨äººã€‚\n5. **éšç§ä¼˜å…ˆè®¾è®¡**ï¼šä¸ºæœ¬åœ°æŽ¨ç†è€Œæž„å»ºï¼Œè¿™äº›æ¨¡åž‹éžå¸¸é€‚åˆä¼˜å…ˆè€ƒè™‘æ•°æ®éšç§çš„åº”ç”¨ï¼Œæ¶ˆé™¤äº†å¯¹æŒç»­äº‘è®¿é—®çš„éœ€æ±‚ã€‚\n6. **å¯æ‰©å±•æ€§**ï¼šæ— è®ºæ˜¯éœ€è¦Ministral 3Bçš„ä½ŽåŠŸè€—æ¶ˆè€—çš„å°åž‹è®¾å¤‡ï¼Œè¿˜æ˜¯éœ€è¦8Bå˜ä½“çš„æ›´å¤§èƒ½åŠ›ï¼Œè¿™ä¸¤ç§æ¨¡åž‹éƒ½è¶³å¤Ÿçµæ´»ï¼Œå¯ä»¥é€‚åº”å„ç§ç”¨ä¾‹ã€‚\n\n> æœ‰å…³åŸºå‡†æµ‹è¯•ç»“æžœï¼Œè¯·å‚é˜… [è¿™é‡Œ](https://mistral.ai/news/ministraux/)\n\n## åˆ†æžæ¨¡åž‹ï¼š\n\n### Ministral 3B:\n\n* ä»…å‡­ **30äº¿ä¸ªå‚æ•°**ï¼Œä¸ºèµ„æºå—é™çš„çŽ¯å¢ƒæä¾›äº†å¹³è¡¡çš„è§£å†³æ–¹æ¡ˆ\n* æ”¯æŒæœ€é«˜ **128k ä¸Šä¸‹æ–‡é•¿åº¦**ï¼Œå¯ä»¥å…¨é¢å¤„ç†å¤æ‚æŸ¥è¯¢\n* é€‚ç”¨äºŽè¶…ä½Žå»¶è¿Ÿåº”ç”¨\n* åœ¨åŒç±»æ¨¡åž‹ä¸­è¡¨çŽ°ä¼˜äºŽè®¸å¤šå…¶ä»–æ¨¡åž‹\n\n### Ministral 8B:\n\n* å…·æœ‰ **80äº¿å‚æ•°** å’Œ **128kä¸Šä¸‹æ–‡é•¿åº¦**ï¼Œåœ¨å¤„ç†æ›´å¤æ‚çš„ä»»åŠ¡æ—¶èƒ½å¤Ÿæä¾›å¢žå¼ºçš„è®¡ç®—èƒ½åŠ›\n* é‡‡ç”¨ **æ»‘åŠ¨çª—å£æ³¨æ„åŠ›** æ¨¡å¼ï¼Œæé«˜é€Ÿåº¦å’Œå†…å­˜æ•ˆçŽ‡\n* åŸºäºŽå¹¿æ³›çš„ **å¤šè¯­è¨€** å’Œ **ä»£ç ** æ•°æ®ï¼Œä½¿å…¶é€‚ç”¨äºŽå¤šç§åº”ç”¨\n* æ”¯æŒ **å‡½æ•°è°ƒç”¨**\n* åœ¨é«˜è¦æ±‚çš„åº”ç”¨ä¸­å¹³è¡¡æ€§èƒ½å’Œæ•ˆçŽ‡\n* è¯æ±‡é‡ä¸º **131k**ï¼Œä½¿ç”¨ **V3-Tekken** åˆ†è¯å™¨\n* æç¤ºæ¨¡æ¿ï¼š\n\n\n```python\n<s>[INST]user message[/INST]assistant response</s>[INST]new user message[/INST]\n```\n\n## ç”¨ä¾‹ï¼š\n\nè¿™äº›æ¨¡åž‹æä¾›äº†è®¡ç®—é«˜æ•ˆå’Œä½Žå»¶è¿Ÿçš„æ€§èƒ½ï¼Œä½¿å…¶éžå¸¸é€‚åˆä»¥ä¸‹åœºæ™¯ï¼š\n\n* **è®¾å¤‡ç«¯ç¿»è¯‘**ï¼šä½¿ç”¨æˆ·èƒ½å¤Ÿåœ¨å®žæ—¶ä¸­æ— ç¼æ²Ÿé€šè·¨è¯­è¨€ï¼Œå³ä½¿åœ¨ç½‘ç»œè¿žæŽ¥è¾ƒå·®çš„åœ°åŒºã€‚\n* **æ— ç½‘ç»œæ™ºèƒ½åŠ©æ‰‹**ï¼šæ”¯æŒç‹¬ç«‹äºŽäº‘è¿žæŽ¥è¿è¡Œçš„æ™ºèƒ½è™šæ‹ŸåŠ©æ‰‹ï¼Œå¢žå¼ºéšç§æ•æ„ŸçŽ¯å¢ƒä¸­çš„ç”¨æˆ·ä½“éªŒã€‚\n* **æœ¬åœ°åˆ†æž**ï¼šä½¿ç»„ç»‡èƒ½å¤Ÿå®žæ—¶åˆ†æžæ•°æ®ï¼ŒåŒæ—¶ä¿æŒä¸¥æ ¼çš„éšç§æ ‡å‡†ï¼Œè¿™åœ¨åŒ»ç–—å’Œé‡‘èžç­‰è¡Œä¸šè‡³å…³é‡è¦ã€‚\n* **è‡ªä¸»æœºå™¨äºº**ï¼šä¸ºæœºå™¨äººé…å¤‡å…ˆè¿›çš„è¯­è¨€èƒ½åŠ›ï¼Œä»¥å®žçŽ°è‡ªä¸»å†³ç­–å’Œæ²Ÿé€šï¼Œæé«˜å®ƒä»¬åœ¨å„ä¸ªè¡Œä¸šçš„è¿è¥æ•ˆçŽ‡ã€‚\n\né™¤äº†å…¶ç‹¬ç«‹çš„èƒ½åŠ›å¤–ï¼Œles Ministraux è¿˜å¯ä»¥ä¸Žæ›´å¤§çš„æ¨¡åž‹å¦‚ Mistral Large ååŒå·¥ä½œã€‚è¿™ç§ååŒä½¿å®ƒä»¬èƒ½å¤Ÿä½œä¸º **åœ¨ä»£ç†å·¥ä½œæµä¸­è¿›è¡Œå‡½æ•°è°ƒç”¨çš„é«˜æ•ˆä¸­ä»‹**ï¼Œå¤„ç†ï¼š\n\n* **è¾“å…¥è§£æž**ï¼šå¿«é€Ÿè§£é‡Šç”¨æˆ·è¾“å…¥ï¼Œä»¥ç¡®ä¿å‡†ç¡®å“åº”ã€‚\n* **ä»»åŠ¡è·¯ç”±**ï¼šæ ¹æ®ç”¨æˆ·æ„å›¾å°†è¯·æ±‚æŒ‡å‘é€‚å½“çš„èµ„æºã€‚\n* **API è°ƒç”¨**ï¼šå®žæ—¶æ‰§è¡Œ API åŠŸèƒ½ï¼Œç¡®ä¿åœ¨å„ç§ä¸Šä¸‹æ–‡ä¸­é¡ºç•…äº’åŠ¨ã€‚\n\n## ä»£ç ä½¿ç”¨ï¼ˆä¸Ž vLLM ä¸€èµ·ï¼‰ï¼š\n\n[Ministral\\-8B\\-Instruct\\-2410](https://huggingface.co/mistralai/Ministral-8B-Instruct-2410) è¯­è¨€æ¨¡åž‹æ˜¯ä¸€ä¸ªç»è¿‡æŒ‡ä»¤å¾®è°ƒçš„æ¨¡åž‹ï¼Œå¯ä»¥ä½¿ç”¨ vLLM é«˜æ•ˆéƒ¨ç½²ã€‚æ‚¨å¯ä»¥åœ¨ Hugging Face ä¸Š [è¿™é‡Œ](https://huggingface.co/mistralai/Ministral-8B-Instruct-2410) æ‰¾åˆ°å®ƒã€‚ä»¥ä¸‹æ˜¯æ‚¨å¯ä»¥å¼€å§‹çš„æ–¹å¼ï¼š\n\n### å®‰è£…\n\né¦–å…ˆï¼Œç¡®ä¿æ‚¨å·²å®‰è£…æœ€æ–°ç‰ˆæœ¬çš„ vLLM å’Œ mistral\\_commonï¼š\n\n\n```python\npip install --upgrade vllm\npip install --upgrade mistral_common\n```\n\n> ***æ³¨æ„****ï¼šéœ€è¦ vLLM ç‰ˆæœ¬ 0\\.6\\.2 æˆ–æ›´é«˜ç‰ˆæœ¬ã€‚*\n\n### ç¦»çº¿ä½¿ç”¨ vLLM\n\nä»¥ä¸‹æ˜¯å¦‚ä½•åœ¨ç¦»çº¿æ¨¡å¼ä¸‹ä½¿ç”¨ Ministral\\-8B å’Œ vLLM çš„ç¤ºä¾‹ï¼š\n\n\n```python\nfrom vllm import LLM\nfrom vllm.sampling_params import SamplingParams\n\nmodel_name = \"mistralai/Ministral-8B-Instruct-2410\"\nsampling_params = SamplingParams(max_tokens=8192)\n\nllm = LLM(model=model_name, tokenizer_mode=\"mistral\", config_format=\"mistral\", load_format=\"mistral\")\n\nprompt = \"What are the potential implications of artificial intelligence on the job market in the next decade?\"\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": prompt\n    },\n]\n\noutputs = llm.chat(messages, sampling_params=sampling_params)\nprint(outputs[0].outputs[0].text)\n```\n\n### æœåŠ¡å™¨æ¨¡å¼æŽ¨ç†ä¸Ž vLLM\n\nåœ¨æœåŠ¡å™¨æŽ¨ç†æ¨¡å¼ä¸‹ï¼ŒvLLM è¿è¡Œä¸€ä¸ª HTTP æœåŠ¡å™¨ï¼Œèƒ½å¤Ÿé€šè¿‡ä¸Ž OpenAI åè®®å…¼å®¹çš„ REST API åŒæ—¶å¤„ç†å®¢æˆ·ç«¯è¿žæŽ¥å’Œè¯·æ±‚ã€‚ä»¥ä¸‹æ˜¯è®¾ç½®æ–¹æ³•ï¼š\n\n* å¯åŠ¨æœåŠ¡å™¨ï¼š\n\n```python\nvllm serve mistralai/Ministral-8B-Instruct-2410 --tokenizer_mode mistral --config_format mistral --load_format mistral\n```\n* å‘æœåŠ¡å™¨å‘é€è¯·æ±‚ï¼š\n\n```python\ncurl --location 'http://localhost:8000/v1/chat/completions' \\\n    --header 'Content-Type: application/json' \\\n    --header 'Authorization: Bearer token' \\\n    --data '{\n        \"model\": \"mistralai/Ministral-8B-Instruct-2410\",\n        \"messages\": [\n          {\n            \"role\": \"user\",\n            \"content\": \"What are the potential implications of artificial intelligence on the job market in the next decade?\"\n          }\n        ]\n      }'\n```\n\n> å…³äºŽ vLLM ä½¿ç”¨çš„é‡è¦è¯´æ˜Žï¼š\n\n* ç›®å‰ï¼Œç”±äºŽåœ¨å®žçŽ°åˆ†é¡µæ³¨æ„åŠ›çš„äº¤é”™æ³¨æ„åŠ›å†…æ ¸æ–¹é¢çš„é™åˆ¶ï¼ŒvLLM çš„ä¸Šä¸‹æ–‡å¤§å°é™åˆ¶ä¸º 32kã€‚\n* ä¸ºäº†åˆ©ç”¨å®Œæ•´çš„ 128k ä¸Šä¸‹æ–‡å¤§å°ï¼Œå»ºè®®ä½¿ç”¨ [Mistral Inference](https://github.com/mistralai/mistral-inference)ã€‚\n* å¦‚æžœæ‚¨éœ€è¦å‡å°‘ GPU å†…å­˜éœ€æ±‚ï¼Œå¯ä»¥é€šè¿‡åœ¨ LLM åˆå§‹åŒ–æ—¶æ·»åŠ  `tensor_parallel=2` æ¥ä½¿ç”¨å¼ é‡å¹¶è¡Œã€‚\n\né€šè¿‡éµå¾ªè¿™äº›ç¤ºä¾‹ï¼Œæ‚¨å¯ä»¥è½»æ¾åœ°å°† Ministral\\-8B é›†æˆåˆ°æ‚¨çš„é¡¹ç›®ä¸­ï¼Œæ— è®ºæ‚¨æ˜¯åœ¨ç¦»çº¿æŽ¨ç†è¿˜æ˜¯ä¸ºå¤šä¸ªå®¢æˆ·ç«¯è®¾ç½®æœåŠ¡å™¨ã€‚è¯¥æ¨¡åž‹çš„é«˜æ•ˆæ€§å’Œå¼ºå¤§åŠŸèƒ½ï¼ŒåŠ ä¸Š vLLM çš„ä¼˜åŒ–æŽ¨ç†ï¼Œä½¿å…¶æˆä¸ºå„ç§ AI åº”ç”¨çš„ä¼˜ç§€é€‰æ‹©ã€‚\n\n## ç»“è®ºï¼š\n\nMinistralçš„å‘å¸ƒæ ‡å¿—ç€äººå·¥æ™ºèƒ½å‘å±•ä¸­çš„ä¸€ä¸ªé‡è¦é‡Œç¨‹ç¢‘ã€‚é€šè¿‡å°†GPTçº§åˆ«çš„æ€§èƒ½å¸¦åˆ°è¾¹ç¼˜è®¾å¤‡ï¼ŒMistral AIä¸ä»…åœ¨æŽ¨åŠ¨æŠ€æœ¯è¾¹ç•Œâ€”â€”ä»–ä»¬è¿˜åœ¨é‡æ–°æž„æƒ³ä»¥æœ¬åœ°ã€éšç§ä¼˜å…ˆçš„äººå·¥æ™ºèƒ½ä¸ºåŸºç¡€çš„å¯èƒ½æ€§ã€‚\n\néšç€å¼€å‘è€…ã€ç ”ç©¶äººå‘˜å’Œä¼ä¸šå¼€å§‹æŽ¢ç´¢Ministralçš„èƒ½åŠ›ï¼Œæˆ‘ä»¬å¯ä»¥æœŸå¾…çœ‹åˆ°ä¸€æ³¢æ–°çš„äººå·¥æ™ºèƒ½é©±åŠ¨çš„åº”ç”¨ç¨‹åºï¼Œè¿™äº›åº”ç”¨ç¨‹åºæ¯”ä»¥å¾€æ›´åŠ å¿«é€Ÿã€æ›´å…·éšç§æ€§å’Œæ›´æ˜“äºŽèŽ·å–ã€‚è¾¹ç¼˜äººå·¥æ™ºèƒ½çš„æ—¶ä»£å·²ç»åˆ°æ¥ï¼Œè€ŒMinistralæ­£åœ¨å¼•é¢†è¿™ä¸€æ½®æµã€‚\n\n"},{"lang":"zh","group":"blog","slug":"blog/meet-qwen2-5-coder-32b-instruct-coder-open-source-better-than-gpt4o-5dc8343f8157","frontmatter":{"title":"æ»¡è¶³ Qwen2.5-Coder-32B-Instruct -Coder - å¼€æºæ¯” gpt4o æ›´å¥½","meta_title":"æ»¡è¶³ Qwen2.5-Coder-32B-Instruct -Coder - å¼€æºæ¯” gpt4o æ›´å¥½","description":"Qwen2.5-Coder-32Bæ˜¯ä¸€æ¬¾å¼€æºçš„AIç¼–ç åŠ©æ‰‹ï¼Œå…·æœ‰320äº¿å‚æ•°å’Œé•¿è¾¾128Kçš„ä¸Šä¸‹æ–‡å¤„ç†èƒ½åŠ›ï¼Œæ”¯æŒ29ç§è¯­è¨€ã€‚å…¶æ€§èƒ½åœ¨ç¼–ç èƒ½åŠ›ä¸Šä¸ŽGPT-4oç›¸åª²ç¾Žï¼Œä¸”ç”Ÿæˆçš„ä»£ç è¯­æ³•å‡†ç¡®ã€é«˜æ•ˆã€‚å°½ç®¡å…¶è®¡ç®—éœ€æ±‚è¾ƒé«˜ï¼Œä½†å…¶å¿«é€Ÿã€å‡†ç¡®çš„ç‰¹æ€§ä½¿å…¶æˆä¸ºå¼€å‘è€…çš„æœ‰åŠ›å·¥å…·ï¼Œèƒ½å¤Ÿæé«˜ç”Ÿäº§åŠ›å’Œå­¦ä¹ æ•ˆçŽ‡ã€‚","date":"2024-11-14T03:29:09.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*VENiO-pvY-FzxBLUqodjRQ.jpeg","categories":["Programming","Generative AI","Data Science"],"author":"Rifx.Online","tags":["parameters","coding","benchmarks","languages","efficiency"],"draft":false,"slug":"blog/meet-qwen2-5-coder-32b-instruct-coder-open-source-better-than-gpt4o-5dc8343f8157"},"content":"\n**è®¤è¯†** Qwen2\\.5\\-Coder\\-32B-Coderï¼Œæ‚¨æ–°çš„ AI ç¼–ç ä¼™ä¼´\n\næ‚¨æ˜¯å¦æ›¾å¸Œæœ›ç¼–ç å˜å¾—æ›´ç®€å•ã€æ›´å¿«é€Ÿï¼Œç”šè‡³æ›´æœ‰è¶£ï¼Ÿé‚£ä¹ˆï¼Œå‡†å¤‡å¥½è¿ŽæŽ¥æ‚¨çš„æ–° AI ç¼–ç æœ‹å‹ Qwen2\\.5\\-Coderã€‚Qwen2\\.5\\-Code ä¸“é—¨å¼€å‘äº†è¿™ä¸ªæ¨¡åž‹ï¼Œä½œä¸ºä¸€ä¸ªå°–ç«¯è¯­è¨€æ¨¡åž‹ï¼Œä»¥ç®€åŒ–æ‚¨çš„ç¼–ç ä½“éªŒã€‚æƒ³è±¡ä¸€ä¸‹ï¼Œæ‹¥æœ‰ä¸€ä¸ªçŸ¥è¯†æ¸Šåšçš„åŠ©æ‰‹ï¼Œå¯ä»¥ä¸ºæ‚¨ç¼–å†™ä»£ç ã€è°ƒè¯•ã€è§£é‡Šå¤æ‚æ¦‚å¿µï¼Œå¹¶å¤„ç†å¤šç§è¯­è¨€ã€‚æ„Ÿå…´è¶£å—ï¼Ÿè®©æˆ‘ä»¬æ¥çœ‹çœ‹ Qwen2\\.5\\-Coder æœ‰ä½•ç‹¬ç‰¹ä¹‹å¤„ã€‚\n\n\n\nðŸ§  **å¼ºå¤§çš„æ€§èƒ½ï¼šåŒ¹æ•Œ GPT\\-4o çš„ç¼–ç èƒ½åŠ›**\n\n> **Qwen2\\.5\\-Coder**ï¼Œç‰¹åˆ«æ˜¯ 32B\\-Instruct ç‰ˆæœ¬ï¼Œä¸ä»…ä»…æ˜¯ä¸€ä¸ªä»£ç åŠ©æ‰‹ï¼›å®ƒæ˜¯ä¸€æ¬¾é¡¶çº§è¡¨çŽ°è€…ï¼ŒåŒ¹æ•Œç”šè‡³è¶…è¶Š GPT\\-4o å’Œ Sonnet 3\\.5ï¼Œè¢«è®¤ä¸ºæ˜¯æœ€å¼ºå¤§çš„äººå·¥æ™ºèƒ½æ¨¡åž‹ä¹‹ä¸€ã€‚æƒ³è±¡ä¸€ä¸‹ï¼Œæ‚¨ä¹Ÿå¯ä»¥æ‹¥æœ‰è¿™ç§æ°´å¹³çš„ç¼–ç èƒ½åŠ›ã€‚\n\n## å¼€æº\n\nä½†è¿™å¹¶ä¸åªæ˜¯å…³äºŽåŽŸå§‹èƒ½åŠ›ï¼›è¯¥æ¨¡åž‹å±•çŽ°äº†å‡ºè‰²çš„æ­£ç¡®æ€§ï¼Œç”Ÿæˆäº†è¯­æ³•ä¸Šç²¾ç¡®ä¸”é«˜æ•ˆçš„ä»£ç ã€‚æœ€æ£’çš„æ˜¯ä»€ä¹ˆï¼Ÿå®ƒæ¯”å…¶å‰èº«å¿«å¾—å¤šï¼Œä½¿æ‚¨èƒ½å¤Ÿå¿«é€Ÿå®Œæˆä»»åŠ¡ã€‚\n\n**ä¸»è¦ç‰¹ç‚¹**\n\n* **æ¨¡åž‹å¤§å°**ï¼š320äº¿å‚æ•°ã€‚\n* **ä¸Šä¸‹æ–‡é•¿åº¦**ï¼šæ”¯æŒæœ€å¤š128Kä¸ªtokenï¼Œå…è®¸å¹¿æ³›çš„è¾“å…¥å’Œè¾“å‡ºèƒ½åŠ›ã€‚\n* **å¤šè¯­è¨€æ”¯æŒ**ï¼šè¯¥ç³»ç»Ÿå¯ä»¥å¤„ç†è¶…è¿‡29ç§è¯­è¨€ï¼ŒåŒ…æ‹¬è‹±è¯­ã€ä¸­æ–‡ã€æ³•è¯­å’Œè¥¿ç­ç‰™è¯­ã€‚\n* **æŒ‡ä»¤éµå¾ª**ï¼šæ­¤åŠŸèƒ½å¢žå¼ºäº†éµå¾ªå¤æ‚æŒ‡ä»¤çš„èƒ½åŠ›ï¼Œå¹¶ç”Ÿæˆç»“æž„åŒ–è¾“å‡ºï¼Œå¦‚JSONã€‚\n* **æ€§èƒ½åŸºå‡†**ï¼šå›¢é˜Ÿåœ¨å„ç§ç¼–ç åŸºå‡†ï¼ˆå¦‚HumanEvalå’ŒMATHï¼‰ä¸Šå¾—åˆ†å¾ˆé«˜ã€‚\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*zyjKE3ZHtax3uX9GnbKUfA.png)\n\næœ‰é€‚åˆæ‰€æœ‰éœ€æ±‚çš„æ¨¡åž‹ï¼Œä»Žå°åž‹åˆ°å¤§åž‹ã€‚\n\næ— è®ºæ‚¨çš„ç»éªŒæ°´å¹³å¦‚ä½•ï¼ŒQwen2.5-Coderéƒ½æä¾›å…¨é¢çš„è¦†ç›–ã€‚Qwen2.5-Coderæœ‰å¤šç§å¤§å°ï¼Œä»Ž0.5Båˆ°ä»¤äººå°è±¡æ·±åˆ»çš„32Bã€‚è¿™æ„å‘³ç€æ‚¨å¯ä»¥é€‰æ‹©æœ€ç¬¦åˆæ‚¨çš„éœ€æ±‚å’Œèµ„æºçš„æ¨¡åž‹ã€‚å°±åƒæ‹¥æœ‰ä¸€ä¸ªå·¥å…·ç®±ï¼Œé‡Œé¢æœ‰ä¸åŒå¤§å°çš„æ‰³æ‰‹ï¼Œæ¯ä¸ªéƒ½é€‚åˆç‹¬ç‰¹çš„ä»»åŠ¡ã€‚\n\nðŸŒŽ ç²¾é€šå¤šç§è¯­è¨€\n\nä½¿ç”¨å¤šç§è¯­è¨€è¿›è¡Œç¼–ç ï¼Ÿæ²¡é—®é¢˜ï¼Qwen2.5-Coderæ”¯æŒè¶…è¿‡29ç§è¯­è¨€ï¼ŒåŒ…æ‹¬è‹±è¯­ã€ä¸­æ–‡ã€æ³•è¯­å’Œè¥¿ç­ç‰™è¯­ç­‰æµè¡Œè¯­è¨€ã€‚è¿™ç§åŒè¯­èƒ½åŠ›ä½¿å…¶æˆä¸ºå…¨çƒå¼€å‘äººå‘˜éžå¸¸é€‚åº”çš„å·¥å…·ã€‚å°±åƒæ‹¥æœ‰ä¸€ä¸ªé€šç”¨çš„ä»£ç ç¿»è¯‘å™¨ï¼Œæ¶ˆé™¤äº†è¯­è¨€éšœç¢ï¼Œå¼€å¯äº†æ–°çš„å¯èƒ½æ€§ã€‚\n\nðŸ‘ ä¼˜åŠ¿ï¼šæé«˜ç”Ÿäº§åŠ›å’Œæ”¹å–„å­¦ä¹ \n\nè®©æˆ‘ä»¬è®¨è®ºä¸€ä¸‹ä¼˜ç‚¹ã€‚\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MAhK8R45yNzB8A7mZZITBg.png)\n\n**é•¿ä¸Šä¸‹æ–‡å¤„ç†**ï¼šè¯¥æ¨¡åž‹å¯ä»¥å¤„ç†é•¿è¾¾128Kä¸ªtokençš„è¾“å…¥ã€‚è¿™å¯¹äºŽéœ€è¦å¹¿æ³›èƒŒæ™¯çš„å¤æ‚ç¼–ç ä»»åŠ¡å°¤å…¶æœ‰ç”¨ã€‚**å¤šè¯­è¨€æŠ€èƒ½**ï¼šQwen2.5-Coder-32B-Instructæ”¯æŒè¶…è¿‡29ç§è¯­è¨€ï¼ŒåŒ…æ‹¬è‹±è¯­ã€ä¸­æ–‡ã€æ³•è¯­å’Œè¥¿ç­ç‰™è¯­ã€‚è¿™ä½¿å…¶æˆä¸ºåœ¨å¤šè¯­è¨€é¡¹ç›®ä¸­å·¥ä½œçš„å¼€å‘äººå‘˜çš„å®è´µå·¥å…·ã€‚\n\nðŸ‘Ž **ç¼ºç‚¹**ï¼šèµ„æºå¯†é›†åž‹å’Œè¿‡åº¦ä¾èµ–çš„é£Žé™©ã€‚\n\nå½“ç„¶ï¼Œæ¯é¡¹æŠ€æœ¯éƒ½æœ‰å…¶ç¼ºç‚¹ã€‚Qwen2.5-Coderå¯¹å¤„ç†èƒ½åŠ›çš„è¦æ±‚å¾ˆé«˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å…¶è¾ƒå¤§å˜ä½“ä¸­ã€‚å……åˆ†åˆ©ç”¨å®ƒéœ€è¦å¼ºå¤§çš„ç¡¬ä»¶ã€‚\n\nðŸŽ‰ **ç¼–ç çš„æœªæ¥**ï¼Ÿ\n\nQwen2.5-Coderæ ‡å¿—ç€AIé©±åŠ¨ç¼–ç çš„é‡å¤§è¿›å±•ã€‚å…¶ç²¾ç¡®æ€§ã€é€Ÿåº¦ã€é€‚åº”æ€§å’Œå¼€æºç‰¹æ€§ä½¿å…¶æˆä¸ºä¸€ä¸ªå¼•äººæ³¨ç›®çš„çªç ´ã€‚å¯¹äºŽå¼€æºç¤¾åŒºçš„çœŸæ­£å¥½å¤„å°†æ˜¯å¦‚æžœæ‰€éœ€çš„è®¡ç®—èƒ½åŠ›å‡å°‘ï¼Œå¼€å‘çš„APIæˆæœ¬ä¹Ÿæ›´ä½Žã€‚\n\né™¤æ­¤ä¹‹å¤–ï¼Œå®ƒéžå¸¸æœ‰å‰æ™¯ï¼Œä¹Ÿå°†ä½¿å¤§åž‹å‚ä¸Žè€…åœ¨ä»˜è´¹å¢™ä¸‹å—åˆ°æŽ§åˆ¶ã€‚\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*aHeNvfOvcpME0qzy6EQexQ.jpeg)\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*PI0ioI2MtxQgtNZ0Tq1jwQ.jpeg)\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/metas-llama-4-is-coming-soon-plus-parallels-brings-apple-intelligence-to-windows-c1c2722dcf03","frontmatter":{"title":"Meta's Llama 4 å³å°†æŽ¨å‡º å¦å¤–Parallels ä¸º Windows å¸¦æ¥ Apple æ™ºèƒ½","meta_title":"Meta's Llama 4 å³å°†æŽ¨å‡º å¦å¤–Parallels ä¸º Windows å¸¦æ¥ Apple æ™ºèƒ½","description":"æœªæä¾›å­—å¹•","date":"2024-11-10T03:51:17.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*sYakQyN_2Lupo_By","categories":["Technology","Machine Learning","Generative AI"],"author":"Rifx.Online","tags":["Llama","GPUs","Parallels","Recraft","Midjourney"],"draft":false,"slug":"blog/metas-llama-4-is-coming-soon-plus-parallels-brings-apple-intelligence-to-windows-c1c2722dcf03"},"content":"\n\n\n### Plus: Parallels å°†è‹¹æžœæ™ºèƒ½å¸¦å…¥ Windows\n\n\n\n**æ¬¢è¿Žæ¥åˆ° Get The Gist**ï¼Œåœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æ¯ä¸ªå·¥ä½œæ—¥åˆ†äº«æœ€æ–°å’Œæœ€ä¼Ÿå¤§çš„ AI å‘å±•ç®€æ˜Žæ˜“æ‡‚çš„æ€»ç»“â€”â€”æ–°é—»ã€åˆ›æ–°å’Œè¶‹åŠ¿â€”â€”æ‰€æœ‰å†…å®¹éƒ½åœ¨ 5 åˆ†é’Ÿå†…é€è¾¾ï¼â±\n\n**åœ¨ä»Šå¤©çš„ç‰ˆå—ä¸­ï¼š**\n\n* é©¬å…‹Â·æ‰Žå…‹ä¼¯æ ¼å®£å¸ƒ Meta çš„ Llama 4\n* Parallels å°†è‹¹æžœæ™ºèƒ½å¸¦å…¥ Windows\n* Recraft V3 æŒ‘æˆ˜ Midjourney\n* Meta AI ç”¨æˆ·è¶…è¿‡ 5 äº¿\n* è¿˜æœ‰æ›´å¤š AI æ–°é—»â€¦.\n\n## 1\\. Metaçš„Llama 4å³å°†å‘å¸ƒï¼Œå¸¦æ¥é‡å¤§AIè¿›å±•\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*E_j8uSNV6s3lg2vm)\n\n**è¦ç‚¹ï¼š** é©¬å…‹Â·æ‰Žå…‹ä¼¯æ ¼[**ç¡®è®¤**](https://analyticsindiamag.com/ai-news-updates/mark-zuckerberg-confirms-llama-4-release-early-next-year/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon) Metaå°†åœ¨æ˜Žå¹´åˆæŽ¨å‡ºå…¶Llama 4æ¨¡åž‹ï¼Œæ‰¿è¯ºåœ¨é€Ÿåº¦ã€æŽ¨ç†å’Œè·¨æ¨¡æ€æ–¹é¢æä¾›æ–°èƒ½åŠ›ï¼Œè¿™å¾—ç›ŠäºŽåˆ›çºªå½•çš„è®­ç»ƒé…ç½®ã€‚\n\n**å…³é”®ç»†èŠ‚ï¼š**\n\n* Metaæ­£åœ¨ä½¿ç”¨è¶…è¿‡100,000ä¸ªH100 GPUçš„å¤§åž‹é…ç½®è®­ç»ƒLlama 4ï¼Œè¿™æ˜¯æŠ¥å‘Šä¸­æœ€å¤§çš„AIé›†ç¾¤ä¹‹ä¸€ï¼Œç›®æ ‡æ˜¯æ¯”ä»¥å¾€æ›´å¿«ã€æ›´å¼ºå¤§çš„æ¨¡åž‹ã€‚\n* æ–°çš„Llama 4å°†å¼•å…¥å…ˆè¿›çš„åŠŸèƒ½ï¼Œå¦‚æ‰©å±•å†…å­˜ã€æ”¯æŒå¤šç§æ•°æ®ç±»åž‹å’Œæ— ç¼çš„ç¬¬ä¸‰æ–¹é›†æˆã€‚\n* AIç»§ç»­æŽ¨åŠ¨Metaçš„å¢žé•¿ï¼Œå› ä¸ºç”Ÿæˆå·¥å…·å¸®åŠ©è¶…è¿‡ä¸€ç™¾ä¸‡å¹¿å‘Šå®¢æˆ·å°†è½¬åŒ–çŽ‡æé«˜7%ï¼Œå¹¶æå‡Facebookå’ŒInstagramä¸Šçš„ç”¨æˆ·å‚ä¸Žåº¦ã€‚\n* æ‰Žå…‹ä¼¯æ ¼å¼ºè°ƒï¼ŒAIåˆ›æ–°æ­£åœ¨åˆ›é€ æ–°çš„å•†ä¸šæœºä¼šï¼Œå¼ºè°ƒMetaå¯¹äº§å“å’Œå¹³å°é•¿æœŸAIé©±åŠ¨å¢žé•¿çš„æ‰¿è¯ºã€‚\n\n## 2\\. Parallels å°† Apple æ™ºèƒ½å¸¦å…¥ Windows\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*36yykSGFUbML6zR4)\n\n**è¦ç‚¹ï¼š** Parallels Desktop [**çŽ°åœ¨æ”¯æŒ**](https://www.neowin.net/news/parallels-brings-apple-intelligence-features-to-windows/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon) åœ¨ Windows è™šæ‹Ÿæœºä¸Šä½¿ç”¨ Apple çš„ AI é©±åŠ¨å†™ä½œå·¥å…·ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿåœ¨ Windows åº”ç”¨ä¸­åˆ©ç”¨ Apple æ™ºèƒ½å¢žå¼ºæ–‡æœ¬ã€‚\n\n**ä¸»è¦ç»†èŠ‚ï¼š**\n\n* Parallels Desktop 20\\.1 çŽ°åœ¨æ”¯æŒåœ¨ macOS Sequoia 15\\.1 ä¸­çš„ Windows åº”ç”¨ä¸Šä½¿ç”¨ Apple å†™ä½œå·¥å…·ï¼Œè®©ç”¨æˆ·å¯ä»¥åœ¨ Word å’Œè®°äº‹æœ¬ç­‰åº”ç”¨ä¸­è®¿é—®æ–‡æœ¬æ”¹è¿›åŠŸèƒ½ï¼Œå¦‚æ‘˜è¦ã€é‡å†™å’Œè¯­æ°”è°ƒæ•´ã€‚\n* è¦æ¿€æ´»æ­¤åŠŸèƒ½ï¼Œä½¿ç”¨ macOS 15\\.1 å’Œå…¼å®¹çš„ Macï¼ˆM1 æˆ–æ›´æ–°ç‰ˆæœ¬ï¼‰çš„ç”¨æˆ·å¯ä»¥æ›´æ–° Parallelsï¼Œå¹¶ä½¿ç”¨å¿«æ·é”®åœ¨ Windows åº”ç”¨ä¸­åº”ç”¨è¿™äº›å·¥å…·ã€‚\n* Apple å†™ä½œå·¥å…·æ˜¯ Apple æ™ºèƒ½çš„ä¸€éƒ¨åˆ†ï¼Œä¹Ÿæ­£åœ¨ iPadOS å’Œ iOS ä¸ŠæŽ¨å‡ºï¼Œä½†ä»…é™äºŽé…å¤‡å…ˆè¿›å¤„ç†å™¨çš„è®¾å¤‡ï¼Œå¦‚ M1ã€M2 æˆ– A17 Pro èŠ¯ç‰‡ã€‚\n* æ­¤æ›´æ–°ä¸º Mac ç”¨æˆ·æä¾›äº†ä¸€ç§æ— ç¼çš„æ–¹å¼ï¼Œåœ¨ Mac å’Œ Windows çŽ¯å¢ƒä¸­å¢žå¼ºä»–ä»¬çš„å†™ä½œï¼Œå°† Apple çš„ AI ä¸Ž Windows çš„å¯ç”¨æ€§ç›¸ç»“åˆã€‚\n\n## 3\\. Recraft V3 æŒ‘æˆ˜ Midjourneyï¼Œèšç„¦è®¾è®¡å¸ˆçš„ AI å›¾åƒç”Ÿæˆ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*lYoMCLyX61RKwMiF)\n\n**è¦ç‚¹ï¼š** Recraft [**å·²æŽ¨å‡º**](https://www.tomsguide.com/ai/ai-image-video/watch-out-midjourney-recraft-just-announced-new-ai-image-generator-model?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon) Recraft V3ï¼Œè¿™æ˜¯ä¸€æ¬¾æ–°çš„å›¾åƒç”Ÿæˆ AI æ¨¡åž‹ï¼Œæ—¨åœ¨é€šè¿‡å¼ºå¤§çš„è®¾è®¡èšç„¦åŠŸèƒ½å’Œæ— ç¼çš„æ–‡æœ¬é›†æˆè¶…è¶Š Midjourney ç­‰ç«žäº‰å¯¹æ‰‹ã€‚\n\n**å…³é”®ç»†èŠ‚ï¼š**\n\n* Recraft V3 å¼•å…¥äº†å›¾åƒä¸­ç²¾ç¡®çš„æ–‡æœ¬å¤„ç†åŠŸèƒ½ï¼Œå…è®¸ç”¨æˆ·è½»æ¾æ·»åŠ å’Œæ ·å¼åŒ–æ–‡æœ¬ï¼Œè¿™æ˜¯ AI æ¨¡åž‹ä¸­ç½•è§çš„åŠŸèƒ½ï¼›ç›®å‰åœ¨ Hugging Face çš„æŽ’è¡Œæ¦œä¸ŠæŽ’åç¬¬ä¸€ã€‚\n* è®¾è®¡å¸ˆçŽ°åœ¨å¯ä»¥æŽ§åˆ¶æ–‡æœ¬ä½ç½®ã€å“ç‰Œé¢œè‰²å’Œç‹¬ç‰¹é£Žæ ¼ï¼Œæä¾›æ›´å¥½çš„å®šåˆ¶åŒ–ï¼Œæ»¡è¶³åˆ›æ„ä¸“ä¸šäººå£«çš„å…³é”®éœ€æ±‚ã€‚\n* å€ŸåŠ©æ— é™ç”»å¸ƒã€å®žæ—¶åä½œå’Œç”¨äºŽé«˜çº§å·¥ä½œæµç¨‹çš„ APIï¼ŒRecraft V3 æ”¯æŒä¸ªäººå’Œå›¢é˜Ÿè®¾è®¡é¡¹ç›®ã€‚\n* Recraft æ‹¥æœ‰è¶…è¿‡ 150 ä¸‡ç”¨æˆ·ï¼Œç”Ÿæˆäº†è¶…è¿‡ 2 äº¿å¼ å›¾åƒï¼Œè¯¥å·¥å…·å¯åœ¨ç½‘é¡µã€iOS å’Œ Android å¹³å°ä¸Šä½¿ç”¨ã€‚\n\n## å¿«é€Ÿæ‘˜è¦\n\n* **Zenity** å®Œæˆäº† 3800 ä¸‡ç¾Žå…ƒçš„ B è½®èžèµ„ï¼Œä»¥æŽ¨è¿›ä¼ä¸šä½¿ç”¨ä»£ç† AI å’Œä½Žä»£ç å·¥å…·çš„å®‰å…¨è§£å†³æ–¹æ¡ˆï¼Œè§£å†³æµç¨‹è‡ªåŠ¨åŒ–ä¸­çš„å…³é”®å®‰å…¨é—®é¢˜ [(é˜…è¯»æ›´å¤š)](https://www.darkreading.com/application-security/zenity-raises-38m-series-b-funding-round-to-secure-agentic-ai?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon).\n* **OpenAI** æ›´æ–°äº†å…¶å®žæ—¶ APIï¼Œæ–°å¢žäº”ç§å¯Œæœ‰è¡¨çŽ°åŠ›çš„è¯­éŸ³ç”¨äºŽè¯­éŸ³å¯¹è¯­éŸ³åº”ç”¨ï¼Œå¹¶é€šè¿‡æç¤ºç¼“å­˜æ˜¾è‘—é™ä½Žäº†æˆæœ¬ï¼Œç›®å‰å¤„äºŽæµ‹è¯•é˜¶æ®µ ([é˜…è¯»æ›´å¤š](https://venturebeat.com/ai/openai-expands-realtime-api-with-new-voices-and-cuts-prices-for-developers/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n* **OpenAI** åœ¨æ¬§æ´²ä¸ºå…è´¹ç”¨æˆ·æŽ¨å‡ºäº†é«˜çº§è¯­éŸ³æ¨¡å¼ï¼Œå…è®¸ä¸Ž ChatGPT è¿›è¡Œå¼•äººå…¥èƒœçš„äººç±»èˆ¬çš„äº’åŠ¨ ([é˜…è¯»æ›´å¤š](https://www.tomsguide.com/ai/openai-advanced-voice-is-now-free-for-10-minutes-a-month-3-tips-for-getting-the-most-out-of-that-time?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n* **OpenAI** æ­£åœ¨ä¸º ChatGPT æŽ¨å‡ºä¸€é¡¹æ–°åŠŸèƒ½ï¼Œå…è®¸ç”¨æˆ·æœç´¢ä»–ä»¬çš„èŠå¤©è®°å½•ï¼Œè®¡åˆ’ä¸‹ä¸ªæœˆå‘å…è´¹ç”¨æˆ·å¼€æ”¾ ([é˜…è¯»æ›´å¤š](https://indianexpress.com/article/technology/artificial-intelligence/chatgpt-now-allow-users-to-search-through-their-history-heres-how-to-use-it-9647233/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n* **Meta** æ­£åœ¨ä¸Žç¾Žå›½æ”¿åºœåˆä½œï¼Œå°†å…¶ AI æ¨¡åž‹ Llama åº”ç”¨äºŽå¤šä¸ªå…¬å…±éƒ¨é—¨é¡¹ç›®ï¼ŒåŒ…æ‹¬æ”¹å–„èµ„æºèŽ·å–å’Œç®€åŒ–è´¢åŠ¡æ´åŠ©ï¼Œè€Œæ— éœ€æ¶‰åŠä»»ä½•è´¢åŠ¡äº¤æ˜“ ([é˜…è¯»æ›´å¤š](https://www.newsbytesapp.com/news/science/meta-working-to-get-llama-used-in-us-government-sectors/story?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n* **Meta** è®¡åˆ’åœ¨æ˜Žå¹´åˆæŽ¨å‡ºå…¶ Llama 4 AI æ¨¡åž‹ï¼Œè®­ç»ƒå…¶åœ¨è¶…è¿‡ 100,000 ä¸ª H100 GPU çš„å‰æ‰€æœªæœ‰çš„é›†ç¾¤ä¸Šï¼ŒåŒæ—¶å°½ç®¡å­˜åœ¨æ½œåœ¨æ»¥ç”¨çš„æ‹…å¿§ï¼Œä»å€¡å¯¼å¼€æºæ–¹æ³• ([é˜…è¯»æ›´å¤š](https://www.newsbytesapp.com/news/science/meta-trains-llama-4-models-on-largest-nvidia-gpu-cluster/story?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n* **OpenAI** å·²ä¸º ChatGPT æŽ¨å‡ºäº†é«˜çº§è¯­éŸ³æ¨¡å¼ï¼Œå…è®¸ç”¨æˆ·åœ¨æ¡Œé¢åº”ç”¨ä¸­è¿›è¡Œè‡ªç„¶çš„è¯­éŸ³å¯¹è¯ï¼Œè¯¥åŠŸèƒ½å·²åœ¨è®¢é˜…ç”¨æˆ·ä¸­èŽ·å¾—äº†äººæ°” ([é˜…è¯»æ›´å¤š](https://www.digitaltrends.com/computing/chatgpt-advanced-voice-mode-macos-windows-desktops/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n* **Waymo** æ­£åœ¨é€šè¿‡å¼€å‘æ–°çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡åž‹ EMMA æ¥æå‡å…¶è‡ªåŠ¨é©¾é©¶æŠ€æœ¯ï¼Œä»¥æ”¹å–„å…¶æœºå™¨äººå‡ºç§Ÿè½¦åœ¨å¤æ‚çŽ¯å¢ƒä¸­çš„å†³ç­–èƒ½åŠ›å’Œé€‚åº”æ€§ ([é˜…è¯»æ›´å¤š](https://www.theverge.com/2024/10/30/24283516/waymo-google-gemini-llm-ai-robotaxi?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n* **Gemini** çŽ°åœ¨ä¸ºå¤§å± Android è®¾å¤‡ï¼ˆå¦‚ Pixel Tablet å’Œ Foldï¼‰æä¾›äº†åˆ†å±å¿«æ·æ–¹å¼ï¼Œå¢žå¼ºäº†ç”¨æˆ·ä½“éªŒ ([é˜…è¯»æ›´å¤š](https://www.androidauthority.com/gemini-split-screen-shortcut-3495573/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n* **Meta AI** åœ¨æŽ¨å‡ºä»…ä¸€å¹´å†…ç”¨æˆ·å·²è¶…è¿‡äº”äº¿ï¼Œé¢„è®¡åˆ° 2024 å¹´åº•æœ‰å¯èƒ½æˆä¸ºä½¿ç”¨æœ€å¹¿æ³›çš„ AI åŠ©æ‰‹ï¼Œå°½ç®¡åœ¨æ¬§ç›Ÿé¢ä¸´éšç§æŒ‘æˆ˜ ([é˜…è¯»æ›´å¤š](https://www.phonearena.com/news/meta-ai-reaches-500-million-users-in-one-year_id164309?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n* **Adobe** æ›´æ–°äº† Illustrator å’Œ Photoshopï¼Œå¢žåŠ äº† AI é©±åŠ¨çš„åŠŸèƒ½ï¼Œä»¥ç®€åŒ–åˆ›ä½œæµç¨‹å¹¶å¢žå¼ºç”¨æˆ·çµæ´»æ€§ï¼Œå¼ºè°ƒå¢žå¼ºäººç±»åˆ›é€ åŠ›è€Œéžå–ä»£ ([é˜…è¯»æ›´å¤š](https://www.gearpatrol.com/tech/six-new-powerful-ai-features-every-adobe-photoshop-illustrator-must-try/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n* **NVIDIA** ç ”ç©¶äººå‘˜æŽ¨å‡ºäº† HOVERï¼Œè¿™æ˜¯ä¸€ç§ 150 ä¸‡å‚æ•°çš„ç¥žç»ç½‘ç»œï¼Œä½¿ç±»äººæœºå™¨äººèƒ½å¤Ÿé€šè¿‡é«˜æ•ˆçš„è¿åŠ¨åè°ƒå’Œå®žæ—¶é€‚åº”æ€§æ‰§è¡Œå¤æ‚ä»»åŠ¡ ([é˜…è¯»æ›´å¤š](https://analyticsindiamag.com/ai-news-updates/nvidia-introduces-hover-a-1-5-m-parameter-neural-network-for-humanoid-robotics/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n* **Google** ä¸º Pixel è®¾å¤‡æŽ¨å‡ºäº†ä¸€æ¬¾æ–°çš„ç‹¬ç«‹å¤©æ°”åº”ç”¨ï¼Œåˆ©ç”¨ AI æ€»ç»“æˆ·å¤–æƒ…å†µå¹¶æä¾›å¤šåœ°ç‚¹è·Ÿè¸ª ([é˜…è¯»æ›´å¤š](https://www.theverge.com/2024/10/30/24283998/google-weather-app-pixel-8-7-6-ai-summaries?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=meta-s-llama-4-is-coming-soon)).\n\nä»Šå¤©å°±åˆ°è¿™é‡Œï¼Œæ˜Žå¤©è§ï¼ ðŸ‘‹\n\nå¦‚æžœæ‚¨å–œæ¬¢è¿™ä¸ªæ›´æ–°å¹¶æƒ³äº†è§£ AI çš„æœ€æ–°å‘å±•ï¼Œè¯·è€ƒè™‘åœ¨ Medium ä¸Šè®¢é˜… ***Get The Gist*** ä»¥èŽ·å–æ›´å¤šè§è§£å’Œåˆ†æžã€‚\n\n**æƒ³è¦æ·±å…¥äº†è§£å—ï¼Ÿ** è®¢é˜…æˆ‘ä»¬çš„å…è´¹æ¯æ—¥ç”µå­é‚®ä»¶é€šè®¯ï¼Œå¿«é€Ÿã€ç®€æ´çš„æ›´æ–°ç›´æŽ¥å‘é€åˆ°æ‚¨çš„æ”¶ä»¶ç®±ï¼Œä»¥ä¾¿æ‚¨ä¸ä¼šé”™è¿‡ä»»ä½•é‡è¦å‘å±•ã€‚æ‚¨å¯ä»¥é€šè¿‡ç‚¹å‡» [è¿™é‡Œ](https://getthegist.beehiiv.com/) æ³¨å†Œã€‚\n\nè®©æˆ‘ä»¬ä¸€èµ·æŽ¢ç´¢ AI çš„ä¸–ç•Œâ€”â€”æ¯æ¬¡æ‘˜è¦éƒ½æ˜¯ä¸€æ¬¡æ–°çš„æ—…ç¨‹ï¼ ðŸ’¡ðŸ¤–\n\n"},{"lang":"zh","group":"blog","slug":"blog/microsoft-graphrag-v0-4-0-ec98f1f6ed7a","frontmatter":{"title":"Microsoft GraphRAG v0.4.0","meta_title":"Microsoft GraphRAG v0.4.0","description":"å¾®è½¯æœ€è¿‘å‘å¸ƒäº† GraphRAG é¡¹ç›®çš„ v0.4.0 ç‰ˆæœ¬ï¼Œå…¶ä¸­æœ‰å‡ é¡¹é‡å¤§æ›´æ–°ã€‚å…¶ä¸­æœ€å¼•äººæ³¨ç›®çš„æ›´æ–°æ˜¯...","date":"2024-11-13T01:22:29.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*89qTckZYLUBF1Jtv","categories":["Programming","Data Science","Machine Learning"],"author":"Rifx.Online","tags":["GraphRAG","Incremental","Indexing","DRIFT","Embedding"],"draft":false,"slug":"blog/microsoft-graphrag-v0-4-0-ec98f1f6ed7a"},"content":"\n\n\nå¾®è½¯æœ€è¿‘å‘å¸ƒäº† GraphRAG é¡¹ç›®çš„ v0\\.4\\.0 ç‰ˆæœ¬ï¼Œå¸¦æ¥äº†å‡ é¡¹é‡è¦æ›´æ–°ã€‚æœ€æ˜¾è‘—çš„æ–°å¢žåŠŸèƒ½æ˜¯å¢žé‡ç´¢å¼•ç‰¹æ€§å’Œ DRIFT å›¾æŽ¨ç†æŸ¥è¯¢æ¨¡å—ï¼Œè¿™å¤§å¤§å¢žå¼ºäº†ç³»ç»Ÿçš„æ•ˆçŽ‡å’ŒåŠŸèƒ½ã€‚\n\n\n\næ­¤æ¬¡æ›´æ–°çš„æ ¸å¿ƒäº®ç‚¹åŒ…æ‹¬ï¼š\n\n1\\. å¢žé‡ç´¢å¼•ï¼šæ˜¾è‘—æé«˜å¤§è§„æ¨¡æ•°æ®å¤„ç†çš„æ•ˆçŽ‡ï¼Œå®žçŽ°æ›´å¿«çš„ä¿¡æ¯æ›´æ–°ã€‚\n\n2\\. DRIFT å›¾æŽ¨ç†æŸ¥è¯¢æ¨¡å—ï¼šå¼•å…¥å…ˆè¿›çš„å›¾æŽ¨ç†æŠ€æœ¯ï¼Œå¢žå¼ºå¤æ‚æŸ¥è¯¢å¤„ç†èƒ½åŠ›ã€‚\n\næ­¤å¤–ï¼Œç‰ˆæœ¬ 0\\.4\\.0 ä¼˜åŒ–äº†åµŒå…¥å·¥ä½œæµç¨‹ï¼Œé‡æž„äº†å¤„ç†æµç¨‹ï¼Œæé«˜äº†æ•´ä½“ç³»ç»Ÿæ€§èƒ½å’Œå¯æ“ä½œæ€§ã€‚å®ƒè¿˜å¢žåŠ äº† DRIFT æœç´¢ CLI å’Œç¤ºä¾‹ç¬”è®°æœ¬ï¼Œä»¥å¸®åŠ©å¼€å‘äººå‘˜æ›´å¥½åœ°ç†è§£æ–°åŠŸèƒ½ã€‚æ­¤å¤–ï¼Œå¼•å…¥å…³ç³»åˆå¹¶å’Œå¢žé‡æ›´æ–°é…ç½®é€‰é¡¹è¿›ä¸€æ­¥å¢žå¼ºäº† GraphRAG çš„çµæ´»æ€§å’Œæ™ºèƒ½æ°´å¹³ã€‚\n\nè¿™äº›æ›´æ–°ä¸ä»…æé«˜äº† GraphRAG çš„å¤„ç†é€Ÿåº¦ï¼Œä¾‹å¦‚åœ¨å¤§è§„æ¨¡é‡‘èžæ•°æ®åˆ†æžä¸­ï¼Œå¢žé‡ç´¢å¼•ç‰¹æ€§å¯ä»¥å°†æ•°æ®æ›´æ–°æ—¶é—´ä»Žæ•°å°æ—¶å‡å°‘åˆ°æ•°åˆ†é’Ÿã€‚åŒæ—¶ï¼Œå®ƒä»¬ä¹Ÿå¢žå¼ºäº†å…¶åœ¨å¤æ‚çŸ¥è¯†å›¾è°±åº”ç”¨ä¸­çš„é€‚ç”¨æ€§ï¼Œæ˜¾è‘—æ‹“å®½äº†ä½¿ç”¨åœºæ™¯ã€‚è¡Œä¸šä¸“å®¶é¢„æµ‹ï¼Œè¿™äº›æ”¹è¿›å°†åœ¨é‡‘èžåˆ†æžå’ŒåŒ»ç–—è¯Šæ–­ç­‰é¢†åŸŸå‘æŒ¥å…³é”®ä½œç”¨ï¼ŒæŽ¨åŠ¨ AI åº”ç”¨å‘æ›´ç²¾ç¡®å’Œé«˜æ•ˆçš„æ–¹å‘å‘å±•ã€‚\n\næ›´å¤šæ›´æ–°å†…å®¹ï¼š [https://proxy.rifx.online/https://github.com/microsoft/graphrag/releases/tag/v0\\.4\\.0](https://proxy.rifx.online/https://github.com/microsoft/graphrag/releases/tag/v0.4.0)\n\n"},{"lang":"zh","group":"blog","slug":"blog/mistral-ai-releases-revolutionary-edge-models-ministral-3b-and-8b-superior-performance-and-privacy-5b24f0189493","frontmatter":{"title":"Mistral AI å‘å¸ƒé©å‘½æ€§è¾¹ç¼˜æ¨¡åž‹ Ministral 3B å’Œ 8Bï¼šå“è¶Šæ€§èƒ½å’Œéšç§","meta_title":"Mistral AI å‘å¸ƒé©å‘½æ€§è¾¹ç¼˜æ¨¡åž‹ Ministral 3B å’Œ 8Bï¼šå“è¶Šæ€§èƒ½å’Œéšç§","description":"æ²¡æœ‰æä¾›å­—å¹•","date":"2024-10-31T08:32:15.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*zFNeFlbfEnbjV5M65sH5ig@2x.jpeg","categories":["Technology","Machine Learning","Autonomous Systems"],"author":"Rifx.Online","tags":["edge","models","privacy","tokens","attention"],"draft":false,"slug":"blog/mistral-ai-releases-revolutionary-edge-models-ministral-3b-and-8b-superior-performance-and-privacy-5b24f0189493"},"content":"\n\n\næœ€è¿‘ï¼ŒMistral AI æŽ¨å‡ºäº†ä¸¤ä¸ªæ–°çš„è¾¹ç¼˜æ¨¡åž‹â€”â€”Ministral 3B å’Œ Ministral 8Bï¼Œè¿™å¼•èµ·äº†ç§‘æŠ€ç•Œçš„å¹¿æ³›å…³æ³¨ã€‚è¿™äº›æ¨¡åž‹ä¸ä»…åœ¨æ€§èƒ½ä¸Šè¡¨çŽ°å‡ºè‰²ï¼Œè¿˜åœ¨éšç§ä¿æŠ¤æ–¹é¢æä¾›äº†ç‹¬ç‰¹çš„ä¼˜åŠ¿ã€‚\n\n\n\n## å“è¶Šæ€§èƒ½ï¼Œéšç§ä¼˜å…ˆ\n\nMinistral 3B å’Œ 8B ä¸“ä¸ºè®¾å¤‡å†…è®¡ç®—è€Œè®¾è®¡ï¼Œèƒ½å¤Ÿå¤„ç†é•¿åº¦è¾¾ 128k çš„æ–‡æœ¬ä¿¡æ¯ã€‚ç‰¹åˆ«æ˜¯ï¼ŒMinistral 8B é‡‡ç”¨äº†åˆ›æ–°çš„æ»‘åŠ¨çª—å£æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ˜¾è‘—æé«˜äº†è®¡ç®—é€Ÿåº¦å’Œå†…å­˜æ•ˆçŽ‡ã€‚æ­¤å¤–ï¼Œè¿™ä¸¤ä¸ªæ¨¡åž‹åœ¨è®¾è®¡ä¸Šä¼˜å…ˆè€ƒè™‘éšç§ä¿æŠ¤ï¼Œç¡®ä¿æ•°æ®åœ¨æœ¬åœ°å¤„ç†ï¼Œä»¥é™ä½Žæ•°æ®æ³„éœ²çš„é£Žé™©ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GMgT6erSorAGUp-pqbXWhA@2x.jpeg)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*zRGh7rw7oVXYd5mOhXoc3g@2x.jpeg)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*IIYgXVtbHvWqn6QLSZ-0Ow@2x.jpeg)\n\n## å¤šåŠŸèƒ½åº”ç”¨ï¼Œæ— é™æ½œåŠ›\n\nMinistralç³»åˆ—æ¨¡åž‹å…·æœ‰å¹¿æ³›çš„åº”ç”¨ã€‚åœ¨æ™ºèƒ½åŠ©æ‰‹é¢†åŸŸï¼Œå®ƒä»¬å¯ä»¥å¿«é€Ÿå“åº”ç”¨æˆ·å‘½ä»¤ï¼ŒåŒæ—¶ç¡®ä¿æ•°æ®å®‰å…¨ï¼›åœ¨è‡ªä¸»æœºå™¨äººé¢†åŸŸï¼Œå®ƒä»¬å¼ºå¤§çš„æŽ¨ç†èƒ½åŠ›æ”¯æŒå¤æ‚çš„å†³ç­–å’Œæ“ä½œã€‚\n\n## æˆæœ¬æ•ˆç›Šé«˜ï¼Œå¹¿é˜”çš„å¸‚åœºå‰æ™¯\n\nå°½ç®¡è¡¨çŽ°å‡ºè‰²ï¼ŒMinistral 3B å’Œ 8B çš„ä»·æ ¼ç«žäº‰åŠ›ä¾ç„¶å¾ˆå¼ºã€‚3B çš„ä»·æ ¼ä¸ºæ¯ç™¾ä¸‡ä¸ªä»¤ç‰Œ $0.04ï¼Œè€Œ 8B çš„ä»·æ ¼ä¸º $0.10ã€‚è¿™ä¸€å®šä»·ç­–ç•¥ä¸ºä¼ä¸šå’Œå¼€å‘è€…æä¾›äº†ä¸€ä¸ªæˆæœ¬æ•ˆç›Šé«˜çš„é€‰æ‹©ã€‚ç›®å‰ï¼Œè¿™ä¸¤ä¸ªæ¨¡åž‹å‡å¯ä¾›ä½¿ç”¨ã€‚\n\n## å‰æ™¯å…‰æ˜Žï¼Œå¼•é¢†è¾¹ç¼˜è®¡ç®—æ–°è¶‹åŠ¿\n\nMistral AI å‘å¸ƒçš„ Ministral ç³»åˆ—æ¨¡åž‹å±•ç¤ºäº†å…¶åœ¨è¾¹ç¼˜è®¡ç®—æ–¹é¢çš„æ·±åŽšæŠ€æœ¯å®žåŠ›ï¼Œä¸ºæœªæ¥çš„è®¾å¤‡ç«¯ AI åº”ç”¨å¥ å®šäº†åšå®žåŸºç¡€ã€‚éšç€æŠ€æœ¯çš„è¿›æ­¥å’Œåº”ç”¨çš„æ·±å…¥æŽ¢ç´¢ï¼ŒMinistral æ¨¡åž‹é¢„è®¡å°†åœ¨æ™ºèƒ½è®¾å¤‡å’Œç‰©è”ç½‘ä¸­å‘æŒ¥æ›´å¤§ä½œç”¨ã€‚\n\næ€»ä¹‹ï¼ŒMinistral 3B å’Œ 8B çš„æŽ¨å‡ºä¸ä»…æ˜¯ Mistral AI çš„ä¸€ä¸ªé‡è¦é‡Œç¨‹ç¢‘ï¼Œä¹Ÿæ˜¯ AI è¡Œä¸šçš„ä¸€æ¬¡é‡å¤§è¿›æ­¥ï¼Œä¸ºè®¾å¤‡ç«¯è®¡ç®—å¸¦æ¥äº†æ–°çš„å¯èƒ½æ€§ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*A6SToo3fO3DqnlWX)\n\n"},{"lang":"zh","group":"blog","slug":"blog/mistral-ai-unveils-ministral-3b-and-8b-models-plus-nvidia-launches-ai-model-that-outperforms-gpt-4-941712f5d22d","frontmatter":{"title":"Mistral AI æŽ¨å‡º Ministral 3B å’Œ 8B æ¨¡åž‹ å¦å¤–ï¼šNvidia æŽ¨å‡ºä¼˜äºŽ GPT-4 çš„ AI æ¨¡åž‹","meta_title":"Mistral AI æŽ¨å‡º Ministral 3B å’Œ 8B æ¨¡åž‹ å¦å¤–ï¼šNvidia æŽ¨å‡ºä¼˜äºŽ GPT-4 çš„ AI æ¨¡åž‹","description":"æ²¡æœ‰æä¾›å­—å¹•","date":"2024-10-31T08:29:07.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*PtPEkgjabwBUu73Y","categories":["Technology","Generative AI","Machine Learning"],"author":"Rifx.Online","tags":["Mistral","edge","Llama","YouTube","DreamTracks"],"draft":false,"slug":"blog/mistral-ai-unveils-ministral-3b-and-8b-models-plus-nvidia-launches-ai-model-that-outperforms-gpt-4-941712f5d22d"},"content":"\n\n\n### Plus: NvidiaæŽ¨å‡ºçš„AIæ¨¡åž‹è¶…è¶ŠGPT\\-4\n\n\n\n**æ¬¢è¿Žæ¥åˆ°Get The Gist**ï¼Œåœ¨è¿™é‡Œæˆ‘ä»¬æ¯ä¸ªå·¥ä½œæ—¥åˆ†äº«æœ€æ–°çš„AIå‘å±•åŠ¨æ€â€”â€”æ–°é—»ã€åˆ›æ–°å’Œè¶‹åŠ¿â€”â€”æ‰€æœ‰å†…å®¹éƒ½åœ¨5åˆ†é’Ÿå†…è½»æ¾é˜…è¯»ï¼â±\n\n**åœ¨ä»Šå¤©çš„ç‰ˆæœ¬ä¸­ï¼š**\n\n* Mistral AIæŽ¨å‡ºäº†ç”¨äºŽè¾¹ç¼˜è®¡ç®—çš„Ministral 3Bå’Œ8Bæ¨¡åž‹\n* Nvidiaæ‚„ç„¶æŽ¨å‡ºçš„AIæ¨¡åž‹è¶…è¶ŠGPT\\-4\n* YouTubeå‘ç¾Žå›½åˆ›ä½œè€…æŽ¨å‡ºAIéŸ³ä¹å·¥å…·â€œæ¢¦å¹»æ›²ç›®â€\n* Google GeminiçŽ°åœ¨å¯ä»¥ç”Ÿæˆå¯è‡ªå®šä¹‰å®½é«˜æ¯”çš„å›¾åƒ\n* è¿˜æœ‰æ›´å¤šAIæ–°é—»â€¦â€¦\n\n## 1\\. Mistral AI å‘å¸ƒ Ministral 3B å’Œ 8B æ¨¡åž‹ä»¥æ”¯æŒè¾¹ç¼˜è®¡ç®—\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*qAjoYMHGI1TkNy_A)\n\n**è¦ç‚¹:** Mistral AI å·²ç»[**æŽ¨å‡ºäº†ä¸¤ä¸ªæ–°çš„ AI æ¨¡åž‹**](https://analyticsindiamag.com/ai-news-updates/mistral-ai-launches-ministral-3b-and-8b-models-for-edge-computing/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models)ï¼ŒMinistral 3B å’Œ 8Bï¼Œæ—¨åœ¨å®žçŽ°é«˜æ•ˆçš„è®¾å¤‡å†…å’Œè¾¹ç¼˜è®¡ç®—ã€‚è¿™äº›æ¨¡åž‹åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ç«žäº‰å¯¹æ‰‹ï¼Œå¹¶ä¸”ä¸“ä¸ºéœ€è¦éšç§ä¼˜å…ˆã€æœ¬åœ°æŽ¨ç†çš„ä»»åŠ¡è€Œè®¾è®¡ã€‚\n\n**å…³é”®ç»†èŠ‚:**\n\n* æ¨¡åž‹å¤„ç†å¤§ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆæœ€é•¿å¯è¾¾ 128kï¼‰ï¼Œåœ¨èµ„æºæœ‰é™çš„çŽ¯å¢ƒä¸­å®žçŽ°æµç•…æ€§èƒ½ã€‚\n* é€‚ç”¨äºŽæ™ºèƒ½åŠ©æ‰‹ã€æœ¬åœ°åˆ†æžå’Œæœºå™¨äººç­‰åº”ç”¨ï¼Œæå‡ä»»åŠ¡æ•ˆçŽ‡ã€‚\n* ä»¥å…·æœ‰ç«žäº‰åŠ›çš„å®šä»·æä¾›å•†ä¸šä½¿ç”¨ï¼Œå¹¶ä¸º 8B Instruct æ¨¡åž‹æä¾›ç ”ç©¶è®¿é—®ã€‚\n* åœ¨åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº† Gemma 2 å’Œ Llama 3 ç­‰ AI æ¨¡åž‹ã€‚\n\n## 2\\. Nvidia å®‰é™æŽ¨å‡ºè¶…è¶Š GPT\\-4 çš„ AI æ¨¡åž‹\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Mza84SHereM3w5rN)\n\n**è¦ç‚¹:** Nvidia [**å‘å¸ƒäº†ä¸€æ¬¾æ–° AI æ¨¡åž‹**](https://venturebeat.com/ai/nvidia-just-dropped-a-new-ai-model-that-crushes-openais-gpt-4-no-big-launch-just-big-results/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models)ï¼ŒLlama\\-3.1\\-Nemotron\\-70B\\-Instructï¼Œå…¶æ€§èƒ½åŸºå‡†è¶…è¶Šäº†è¡Œä¸šå·¨å¤´å¦‚ OpenAI çš„ GPT\\-4ã€‚è¿™æ¬¡å‘å¸ƒæ ‡å¿—ç€ Nvidia AI æˆ˜ç•¥çš„é‡å¤§æ‰©å±•ï¼Œä»Žç¡¬ä»¶è½¬å‘é«˜æ€§èƒ½ AI è½¯ä»¶ã€‚\n\n**å…³é”®ç»†èŠ‚:**\n\n* Nvidia çš„æ–°æ¨¡åž‹åœ¨å…³é”®åŸºå‡†æµ‹è¯•ä¸­å¾—åˆ†é«˜äºŽ GPT\\-4ï¼Œå±•ç¤ºäº†å“è¶Šçš„è¯­è¨€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚\n* è¯¥æ¨¡åž‹é‡‡ç”¨äº†å…ˆè¿›æŠ€æœ¯ï¼Œå¦‚äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹  (RLHF)ï¼Œåœ¨å¤„ç†å¤æ‚æŸ¥è¯¢æ–¹é¢è¡¨çŽ°å‡ºè‰²ã€‚\n* Nvidia é€šè¿‡å…¶å¹³å°æä¾›å…è´¹è®¿é—®ï¼Œå…è®¸ä¼ä¸šè¯•ç”¨è¿™ä¸€å¼ºå¤§çš„ AI å·¥å…·ã€‚\n* è¯¥æ¨¡åž‹å¯æ ¹æ®ä¸šåŠ¡éœ€æ±‚è¿›è¡Œå®šåˆ¶ï¼Œä½†åœ¨æ³•å¾‹æŽ¨ç†æˆ–æ•°å­¦ç­‰ä¸“ä¸šé¢†åŸŸçš„ä½¿ç”¨éœ€è¦è°¨æ…Žã€‚\n\n## 3\\. YouTube åœ¨ç¾Žå›½æŽ¨å‡º AI éŸ³ä¹å·¥å…· â€œDream Tracksâ€\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*5nUNrJmdCBBy4JdQ)\n\n**è¦ç‚¹:** YouTube åœ¨ç¾Žå›½æŽ¨å‡ºäº†å…¶ [**AI é©±åŠ¨çš„éŸ³ä¹ç”Ÿæˆå™¨**](https://www.mediapost.com/publications/article/400280/youtube-brings-ai-audio-generator-to-us-creators.html?edition=136037&utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models) â€œDream Tracksâ€ï¼Œå…è®¸åˆ›ä½œè€…ä½¿ç”¨æ–‡æœ¬æç¤ºä¸ºä»–ä»¬çš„çŸ­è§†é¢‘åˆ›å»ºè‡ªå®šä¹‰éŸ³é¢‘ã€‚è¯¥å·¥å…·æ—¨åœ¨é€šè¿‡éŸ³ä¹åˆ›ä½œåŠ æ·±è‰ºæœ¯å®¶ä¸Žç²‰ä¸ä¹‹é—´çš„è”ç³»ã€‚\n\n**ä¸»è¦ç»†èŠ‚:**\n\n* ç”± Google DeepMind çš„ Lyria æä¾›æ”¯æŒï¼ŒDream Tracks ä¸º YouTube Shorts ç”Ÿæˆå®šåˆ¶çš„ä¹å™¨é…ä¹ã€‚\n* ç¾Žå›½åˆ›ä½œè€…çŽ°åœ¨å¯ä»¥ä½¿ç”¨æ­¤å·¥å…·åˆ›å»ºæœ€é•¿ 30 ç§’çš„å…ç‰ˆç¨Žé…ä¹ã€‚\n* ç”¨æˆ·å¯ä»¥å¯¹ AI ç”Ÿæˆçš„éŸ³é¢‘ç‰‡æ®µè¿›è¡Œæ··éŸ³ï¼Œå¢žå¼ºåˆ›ä½œå¯èƒ½æ€§ã€‚\n* YouTube å¯¹æ‰€æœ‰ AI ç”Ÿæˆçš„æ›²ç›®åº”ç”¨éšè—çš„ SynthID æ°´å°ï¼Œä»¥ç¡®ä¿é€æ˜Žåº¦ã€‚\n\n## å¿«é€Ÿæ‘˜è¦\n\n* **Clerk Chat** èŽ·å¾—äº†ç”± Race Capital é¢†æŠ•çš„ 700 ä¸‡ç¾Žå…ƒèžèµ„ï¼Œä»¥å¢žå¼ºå…¶ AI é©±åŠ¨çš„å•†ä¸šæ²Ÿé€šå¹³å° [(é˜…è¯»æ›´å¤š)](https://www.businesswire.com/news/home/20241017292794/en/World%E2%80%99s-First-AI-Telecom-Clerk-Chat-Raises-7.0-Million-in-Seed-Funding?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models)ã€‚\n* **Anthropic** é¦–å¸­æ‰§è¡Œå®˜ Dario Amodei å‘å¸ƒäº†ä¸€ç¯‡é•¿ç¯‡åšæ–‡ï¼Œé˜è¿°äº†å¯¹äººå·¥é€šç”¨æ™ºèƒ½å˜é©æ½œåŠ›çš„ä¹Œæ‰˜é‚¦æ„¿æ™¯ï¼ŒåŒæ—¶å¯»æ±‚ä¸ºå…¬å¸äº‰å– 400 äº¿ç¾Žå…ƒçš„ä¼°å€¼ [(é˜…è¯»æ›´å¤š)](https://www.theverge.com/2024/10/16/24268209/anthropic-ai-dario-amodei-agi-funding-blog?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models)ã€‚\n* **Google Cloud** å®£å¸ƒå…¶å‡çº§ç‰ˆ Vertex AI å¹³å°å’ŒåŒ»ç–—æ•°æ®å¼•æ“Žçš„æ­£å¼ä¸Šçº¿ï¼Œä»¥å¢žå¼ºåŒ»ç–—é¢†åŸŸçš„ AI åº”ç”¨ [(é˜…è¯»æ›´å¤š)](https://www.forbes.com/sites/saibala/2024/10/17/google-cloud-announces-general-availability-of-vertex-ai-for-healthcare/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models)ã€‚\n* **Amazon** é¢†æŠ•äº† 5 äº¿ç¾Žå…ƒçš„èžèµ„è½®ï¼Œä¸º X-energy æŽ¨å‡ºåˆ° 2039 å¹´çš„ 5GW å°åž‹æ ¸ååº”å †ï¼Œè€Œ **Google** ä¸Ž Kairos Power åˆä½œï¼Œè®¡åˆ’åˆ° 2035 å¹´å®‰è£… 500MW çš„å°åž‹æ¨¡å—åŒ–ååº”å †ï¼ŒäºŒè€…éƒ½æ—¨åœ¨åˆ©ç”¨æ¸…æ´èƒ½æºæ»¡è¶³æ•°æ®ä¸­å¿ƒæ—¥ç›Šå¢žé•¿çš„èƒ½æºéœ€æ±‚ [(é˜…è¯»æ›´å¤š)](https://www.theengineer.co.uk/content/news/amazon-and-google-bet-big-on-smrs-to-power-ai?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models)ã€‚\n* **Google** å°†äºŽ 2025 å¹´åˆåœ¨ Google Distributed Cloud ä¸­ä¸ºå…¬å…±éƒ¨é—¨æœºæž„æŽ¨å‡ºå…¶ Gemini AI æ¨¡åž‹ï¼Œå¹¶æä¾›èµ„é‡‘ä»¥æå‡æ”¿åºœå‘˜å·¥åœ¨è´Ÿè´£ä»»çš„ AI å®žè·µæ–¹é¢çš„æŠ€èƒ½ [(é˜…è¯»æ›´å¤š)](https://siliconangle.com/2024/10/16/google-looks-spearhead-ai-adoption-public-sector/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models)ã€‚\n* **Google** çš„ Gemini AI èŠå¤©æœºå™¨äººå°†æŽ¨å‡ºä¸€é¡¹åŠŸèƒ½ï¼Œå…è®¸ç”¨æˆ·ä»¥å¯è‡ªå®šä¹‰çš„å®½é«˜æ¯”ç”Ÿæˆå›¾åƒï¼Œå¢žå¼ºå…¶å›¾åƒç¼–è¾‘èƒ½åŠ› [(é˜…è¯»æ›´å¤š)](https://indianexpress.com/article/technology/artificial-intelligence/google-gemini-may-soon-get-new-image-resizing-feature-9623756/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models)ã€‚\n\nä»Šå¤©å°±åˆ°è¿™é‡Œï¼Œæ˜Žå¤©è§ï¼ðŸ‘‹\n\nå¦‚æžœæ‚¨å–œæ¬¢è¿™ä¸ªæ›´æ–°å¹¶å¸Œæœ›äº†è§£ AI çš„æœ€æ–°åŠ¨æ€ï¼Œè¯·è€ƒè™‘åœ¨ Medium ä¸Šè®¢é˜… ***Get The Gist***ï¼ŒèŽ·å–æ›´å¤šè§è§£å’Œåˆ†æžã€‚\n\n**æƒ³è¦æ›´æ·±å…¥äº†è§£å—ï¼Ÿ** è®¢é˜…æˆ‘ä»¬çš„å…è´¹æ¯æ—¥ç”µå­é‚®ä»¶é€šè®¯ï¼Œå¿«é€ŸèŽ·å–ç®€æ´çš„æ›´æ–°ï¼Œç¡®ä¿æ‚¨ä¸ä¼šé”™è¿‡ä»»ä½•é‡è¦è¿›å±•ã€‚æ‚¨å¯ä»¥é€šè¿‡ç‚¹å‡» [è¿™é‡Œ](https://getthegist.beehiiv.com/) æ³¨å†Œã€‚\n\nè®©æˆ‘ä»¬ä¸€èµ·æŽ¢ç´¢ AI çš„ä¸–ç•Œâ€”â€”æ¯æ¬¡æ‘˜è¦éƒ½æ˜¯ä¸€æ¬¡æ–°å‘çŽ°ï¼ðŸ’¡ðŸ¤–\n\n"},{"lang":"zh","group":"blog","slug":"blog/mojo-90-000-times-faster-than-python-finally-open-sourced-777bdd9a1896","frontmatter":{"title":"Mojoï¼Œæ¯” Python å¿« 90,000 å€ï¼Œç»ˆäºŽå¼€æºäº†ï¼","meta_title":"Mojoï¼Œæ¯” Python å¿« 90,000 å€ï¼Œç»ˆäºŽå¼€æºäº†ï¼","description":"2024å¹´3æœˆ29æ—¥ï¼ŒModular Inc.å®£å¸ƒMojoæ ¸å¿ƒç»„ä»¶å¼€æºã€‚","date":"2024-11-10T22:36:54.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*jcayumihC6jn5q_0","categories":["Programming","Machine Learning","Data Science"],"author":"Rifx.Online","tags":["Mojo","Python","MLIR","SIMD","open-source"],"draft":false,"slug":"blog/mojo-90-000-times-faster-than-python-finally-open-sourced-777bdd9a1896"},"content":"\n2024å¹´3æœˆ29æ—¥ï¼ŒModular Inc.å®£å¸ƒå¼€æºMojoçš„æ ¸å¿ƒç»„ä»¶ã€‚\n\nMojoæ˜¯ä¸€ç§ä¸“é—¨ä¸ºç¼–å†™äººå·¥æ™ºèƒ½è½¯ä»¶è€Œè®¾è®¡çš„ç¼–ç¨‹è¯­è¨€ï¼ŒåŽ»å¹´å…«æœˆæ­£å¼å‘å¸ƒã€‚è‡ªé‚£æ—¶ä»¥æ¥ï¼Œå®ƒå·²ç»å¸å¼•äº†è¶…è¿‡175,000åå¼€å‘è€…å’Œ50,000ä¸ªç»„ç»‡ã€‚\n\näººå·¥æ™ºèƒ½æ¨¡åž‹é€šå¸¸ä½¿ç”¨å¤šç§ç¼–ç¨‹è¯­è¨€ç¼–å†™ã€‚å¼€å‘è€…é€šå¸¸ä½¿ç”¨Pythonå®žçŽ°ç¥žç»ç½‘ç»œçš„æœ€ç®€å•éƒ¨åˆ†ï¼Œå› ä¸ºå®ƒæ˜“äºŽå­¦ä¹ ï¼Œä½†ç›¸å¯¹è¾ƒæ…¢ã€‚å…¶ä½™ä»£ç é€šå¸¸ç”¨C++ç¼–å†™ï¼Œè™½ç„¶é€Ÿåº¦æ›´å¿«ï¼Œä½†å­¦ä¹ èµ·æ¥æ›´å¤æ‚ã€‚\n\nModularå°†Mojoå®šä½ä¸ºä¸€ç§æ›´æ–¹ä¾¿çš„æ›¿ä»£æ–¹æ¡ˆã€‚å®ƒæä¾›äº†ç±»ä¼¼Pythonçš„æ˜“ç”¨è¯­æ³•ï¼Œä½†æ‰§è¡Œé€Ÿåº¦æœ‰å¯èƒ½å¿«ä¸Šåƒå€ã€‚å› æ­¤ï¼Œå¼€å‘è€…å¯ä»¥ç¼–å†™å¿«é€Ÿçš„AIæ¨¡åž‹ï¼Œè€Œæ— éœ€å­¦ä¹ åƒC++è¿™æ ·å¤æ‚çš„è¯­è¨€ã€‚\n\n\n\nåŽ»å¹´ï¼Œå½“MojoæŽ¨å‡ºæ—¶ï¼Œä¸€äº›å¼€å‘è€…å¯¹å®ƒçš„å‡ºçŽ°è¡¨ç¤ºå…´å¥‹ã€‚ç„¶è€Œï¼Œå½“è¢«é—®åŠå¼€æºæ—¥æœŸæ—¶ï¼ŒChris Lattneråœ¨Discordä¸Šè¡¨ç¤ºï¼šâ€œå¦‚æžœæˆ‘çŸ¥é“ï¼Œæˆ‘ä¼šå‘Šè¯‰ä½ ã€‚â€å¤§çº¦ä¸€å¹´ä»¥æ¥ï¼Œè®¸å¤šå¼€å‘è€…å¤„äºŽè§‚å¯Ÿå’Œè´¨ç–‘çš„çŠ¶æ€ï¼š\n\n> â€œå®£ä¼ å¾ˆå¥½ï¼Œä½†å¦‚æžœä¸æ˜¯å¼€æºçš„ï¼Œæˆ‘ä¸ä¼šèŠ±æ—¶é—´åŽ»å°è¯•ã€‚â€\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*rIJiJylh4-mWBiqz)\n\n> â€œæ˜¾ç„¶è¿™æ˜¯ä¸€ä¸ªè¢«è¿‡åº¦ç‚’ä½œçš„ç¼–ç¨‹è¯­è¨€ï¼Œè€Œä¸”å®ƒä¸æ˜¯å¼€æºçš„ï¼Chris Lattneræƒ³è¦æ¬ºéª—æ•°ç™¾ä¸‡Pythonå¼€å‘è€…ï¼â€\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*0u5HDKseL0Gy_-8A)\n\n> â€œæˆ‘æ— æ³•åœ¨ä¸€ä¸ªå¯èƒ½å¼€æºä¹Ÿå¯èƒ½ä¸å¼€æºçš„è¯­è¨€ä¸ŠèŠ±æ—¶é—´ï¼Œå°¤å…¶æ˜¯åœ¨å½“å‰çš„OSSå•†ä¸šçŽ¯å¢ƒä¸‹â€¦â€¦â€\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*wrTO7fbKfBZOpBxF)\n\nçŽ°åœ¨ï¼ŒMojoç»ˆäºŽå¼€æºäº†ï¼åœ¨çŸ­æ—¶é—´å†…ï¼Œå®ƒå·²ç»è¾¾åˆ°äº†17.6ké¢—æ˜Ÿå’Œ2.1kä¸ªforkï¼\n\n## 01 Mojoå¼€æºä¹‹æ—…çš„ç¬¬ä¸€æ­¥\n\nModularä»Šå¤©å®£å¸ƒå¼€æºMojoæ ‡å‡†åº“çš„æ ¸å¿ƒç»„ä»¶ã€‚æ ‡å‡†åº“æž„æˆäº†ç¼–ç¨‹è¯­è¨€çš„æ ¸å¿ƒéƒ¨åˆ†ï¼ŒåŒ…å«åŸºæœ¬çš„è¯­æ³•å…ƒç´ å’ŒåŸºæœ¬åŠŸèƒ½ã€‚Mojoçš„æ ‡å‡†åº“åŒ…æ‹¬ä¼˜åŒ–AIè¶…å‚æ•°çš„åŠŸèƒ½ï¼Œè¿™äº›è¶…å‚æ•°å†³å®šäº†ç¥žç»ç½‘ç»œå¦‚ä½•å¤„ç†æ•°æ®ã€‚\n\nâ€œMojoæ ‡å‡†åº“ä»åœ¨è¿›è¡Œæ¿€çƒˆçš„å¼€å‘å’Œå¿«é€Ÿå˜åŒ–ï¼Œå› æ­¤æˆ‘ä»¬é¦–å…ˆå¼€æºå…¶æ ¸å¿ƒæ¨¡å—ã€‚è¿™æ ‡å¿—ç€æˆ‘ä»¬å¼€æºä¹‹æ—…çš„é‡è¦èµ·ç‚¹ï¼Œè€Œä¸æ˜¯ç»“æŸã€‚â€\n\nè¯¥å…¬å¸è¡¨ç¤ºï¼Œå¼€æºå°†ä½¿ä»–ä»¬èƒ½å¤Ÿä»Žæ›´å¤šå¼€å‘è€…é‚£é‡Œæ”¶é›†åé¦ˆï¼Œä»Žè€Œä¿ƒè¿›Mojoçš„æ›´å¥½å¼€å‘ã€‚æ­¤å¤–ï¼Œå¼€æºé¡¹ç›®æœ‰å¤šç§æ–¹å¼ï¼šæœ‰äº›é¡¹ç›®æä¾›æºä»£ç ä½†ä¸æŽ¥å—è´¡çŒ®ï¼›æœ‰äº›åˆ™æä¾›ä¸é€æ˜Žçš„è´¡çŒ®æµç¨‹ï¼Œä½¿å¾—ç†è§£ç›®æ ‡å’Œè·¯çº¿å›¾å˜å¾—å›°éš¾ï¼›è¿˜æœ‰ä¸€äº›è™½ç„¶å¼€æºï¼Œä½†å¹¶æœªå¾—åˆ°ç§¯æžç»´æŠ¤ã€‚Modularè¡¨ç¤ºï¼Œä»–ä»¬é€‰æ‹©äº†ä¸€ç§æ›´å…¨é¢çš„å¼€æºæ–¹å¼ï¼šé€šè¿‡GitHubæ‹‰å–è¯·æ±‚å…è®¸å¤–éƒ¨è´¡çŒ®ï¼Œé¼“åŠ±å¼€å‘è€…å‚ä¸ŽMojoçš„å¼€å‘å’Œæ”¹è¿›ï¼Œå¹¶ä¿ƒè¿›ç¤¾åŒºçš„æˆé•¿ã€‚\n\næ­¤å¤–ï¼ŒModularé€šè¿‡åˆ†äº«å®Œæ•´çš„æäº¤åŽ†å²ï¼Œå±•ç¤ºäº†è¯šæ„ï¼Œä»Žåˆå§‹æäº¤å¼€å§‹ï¼å…¬å¼€ä¿®è®¢å¼€æºæ ‡å‡†åº“çš„åŽ†å²ä½¿å¼€å‘è€…èƒ½å¤Ÿè·Ÿè¸ªä»£ç çš„æ¼”å˜ï¼Œæ›´å¥½åœ°ç†è§£å…¶å«ä¹‰ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*0-FqkfLUTevloPjI)\n\næ­¤å¤–ï¼Œä»–ä»¬å°†å‘å¸ƒMojoç¼–è¯‘å™¨çš„å¤œé—´æž„å»ºï¼Œæ–¹ä¾¿å¼€å‘è€…å¿«é€Ÿå°è¯•æœ€æ–°çš„ç¼–è¯‘å™¨åŠŸèƒ½å¹¶è¿›è¡ŒæŒç»­é›†æˆæµ‹è¯•ã€‚\n\nåŽ»å¹´å¹´åº•ï¼ŒModularæŽ¨å‡ºäº†å•†ä¸šAIå¹³å°MAXï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºŽæž„å»ºé«˜æ€§èƒ½AIåº”ç”¨çš„ç»Ÿä¸€å·¥å…·å’Œåº“é›†ï¼Œå¯ä»¥é«˜æ•ˆåœ°éƒ¨ç½²åœ¨å¤šä¸ªç¡¬ä»¶å¹³å°ä¸Šï¼Œä¾‹å¦‚åœ¨KubernetesçŽ¯å¢ƒä¸­è¿è¡ŒAIåº”ç”¨ã€‚ä»Šå¤©ï¼Œè¯¥å…¬å¸é€éœ²ï¼Œä»–ä»¬è¿˜è®¡åˆ’åœ¨æœªæ¥å¼€æºMAXçš„ä¸€äº›ç»„ä»¶ã€‚\n\næ­¤å¤–ï¼Œå€¼å¾—ä¸€æçš„æ˜¯ï¼Œä»–ä»¬é€‰æ‹©äº†Apache 2 LLVMè®¸å¯è¯è¿›è¡Œå¼€æºã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*dgVCSxaCq6onY2uP)\n\nè¿™æ˜¯Apache 2è®¸å¯è¯çš„å®šåˆ¶ç‰ˆæœ¬ã€‚æ­¤å¤–ï¼Œä¸ºäº†ä¾¿äºŽä¸Žéµå¾ªGPL2è®¸å¯è¯çš„è½¯ä»¶é›†æˆï¼ŒModularè¿›è¡Œäº†ç›¸åº”çš„è°ƒæ•´ã€‚GPL2æ˜¯å¦ä¸€ç§æµè¡Œçš„å¼€æºè®¸å¯è¯ï¼Œè‘—ååœ°ç”¨äºŽLinuxå†…æ ¸ç­‰é¡¹ç›®ã€‚åœ¨å…¬å‘Šåšå®¢ä¸­ï¼ŒModularå†™é“ï¼š\n\n> â€œApache 2è®¸å¯è¯æ˜¯ä¸€ä¸ªè‰¯å¥½çš„èµ·ç‚¹ï¼Œä½†æˆ‘ä»¬åœ¨LLVMé¡¹ç›®ä¸­ä½¿ç”¨è®¸å¯è¯çš„ç»éªŒå‘Šè¯‰æˆ‘ä»¬ï¼Œå®ƒæœ‰ä¸¤ä¸ªå°é—®é¢˜ã€‚æœ‰äººæ‹…å¿ƒApache 2è®¸å¯è¯å¯èƒ½ä¸ŽGPL2ä»£ç ï¼ˆä¾‹å¦‚Linuxå†…æ ¸ï¼‰ä¸å…¼å®¹ï¼Œå¹¶ä¸”Apache 2è®¸å¯è¯è¦æ±‚æ‚¨åœ¨æ´¾ç”Ÿé¡¹ç›®ä¸­æ‰¿è®¤ä»£ç çš„ä½¿ç”¨ã€‚æˆ‘ä»¬å¸Œæœ›æ‚¨èƒ½å¤Ÿä½¿ç”¨Mojoï¼Œè€Œä¸å¿…å¼ºåˆ¶æ‰¿è®¤Modularæˆ–Mojoã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ·»åŠ äº†LLVMç‰¹åˆ«è®¾è®¡çš„ä¾‹å¤–æ¡æ¬¾ï¼Œä»¥è§£å†³è¿™äº›é—®é¢˜ã€‚â€\n\n## 02 åœ¨æœªæ¥50å¹´ä¸­ï¼ŒAIç¼–ç¨‹çš„æœ€ä½³è¯­è¨€æ˜¯ä»€ä¹ˆï¼Ÿ\n\nåŽ»å¹´5æœˆï¼Œå½“Mojoåˆšåˆšå‘å¸ƒæ—¶ï¼ŒModularå£°ç§°å®ƒåœ¨è¿è¡ŒMandelbrotç­‰ç®—æ³•æ—¶æ¯”åŽŸå§‹Pythonå¿«35,000å€ã€‚\n\nåŽ»å¹´9æœˆï¼ŒModularå†æ¬¡è¡¨ç¤ºï¼šâ€œMojoç»“åˆäº†åŠ¨æ€è¯­è¨€å’Œé™æ€è¯­è¨€çš„ä¼˜ç‚¹ï¼Œæ€§èƒ½æå‡è‡³Pythonçš„68,000å€ã€‚â€\n\nåŽ»å¹´10æœˆï¼Œå½“Mojoåœ¨Macä¸Šå‘å¸ƒæ—¶ï¼ŒModularå†æ¬¡æé«˜äº†æ€§èƒ½æ¯”è¾ƒæ•°æ®ï¼šâ€œæ¯”Pythonå¿«90,000å€ã€‚â€\n\nè°ˆåˆ°Mojoï¼ŒModularçš„åˆ›å§‹äººå…¼é¦–å¸­æ‰§è¡Œå®˜Chris Lattnerè¡¨ç¤ºï¼šâ€œä½ å¯ä»¥æŠŠMojoçœ‹ä½œPythonå®¶æ—çš„ä¸€å‘˜ï¼Œå€Ÿé‰´äº†æ‰€æœ‰è¿™äº›é…·ç‚«çš„è¯­è¨€ã€ç¼–è¯‘å™¨å’Œå…¶ä»–æŠ€æœ¯ï¼Œä½¿Pythonå‘å‰è¿ˆå‡ºäº†ä¸€å¤§æ­¥ã€‚æˆ‘ä»¬ç›¸ä¿¡å®ƒå¢žå¼ºäº†Pythonçš„èƒ½åŠ›ï¼Œèµ‹äºˆPythonç¨‹åºå‘˜è¶…èƒ½åŠ›ï¼Œä½¿ç†Ÿæ‚‰Pythonçš„äººèƒ½å¤Ÿå­¦ä¹ æ–°çŸ¥è¯†ï¼ŒæŽ¢ç´¢å’Œå¾æœæ–°é¢†åŸŸï¼Œè€Œæ— éœ€åˆ‡æ¢åˆ°C++ã€‚â€\n\nMojoåŸºäºŽMLIRä¸­çš„æœ€æ–°ç¼–è¯‘å™¨æŠ€æœ¯ï¼Œè¿™æ˜¯LLVMçš„æ¼”å˜ï¼Œå› æ­¤æ€§èƒ½æ›´ä½³ã€‚åªè¦ç¨‹åºå‘˜å…·å¤‡å¿…è¦çš„æŠ€èƒ½å¹¶æ„¿æ„å……åˆ†ä¼˜åŒ–ï¼Œä»–ä»¬å°±å¯ä»¥è®©ä»£ç è¿è¡Œå¾—æžå¿«ã€‚Mojoè¯­è¨€çš„ç›®æ ‡æ˜¯æ»¡è¶³Pythonå¼€å‘è€…çš„éœ€æ±‚ï¼ŒåŒæ—¶æä¾›ä¸€ç³»åˆ—æ–°çš„ä»£ç ä¼˜åŒ–æŠ€æœ¯ï¼Œä»¥å……åˆ†åˆ©ç”¨ç¡¬ä»¶è®¾å¤‡çš„æ€§èƒ½æžé™ã€‚\n\nå¦ä¸€æ–¹é¢ï¼ŒMojoå›¢é˜Ÿé«˜åº¦èµžèµRustï¼Œå¹¶å…¬å¼€è¡¨ç¤ºâ€œMojoçš„è®¾è®¡ä¹Ÿå—åˆ°Rustçš„æžå¤§å¯å‘ã€‚â€\n\nåœ¨æ€§èƒ½æ–¹é¢ï¼ŒModularè¿›è¡Œäº†è®¸å¤šä¸ŽPythonçš„æ¯”è¾ƒï¼Œä»¥æä¾›æ˜Žç¡®çš„å¯¹æ¯”ï¼Œä½†äººä»¬å¹¶æ²¡æœ‰æ¦‚å¿µå®ƒæ¯”Rustå¿«å¤šå°‘ã€‚å°±åœ¨ä¸Šä¸ªæœˆï¼Œä»–ä»¬ä¸“é—¨å›žåº”äº†â€œMojoæ˜¯å¦æ¯”Rustå¿«â€çš„é—®é¢˜ã€‚\n\nä»Šå¹´2æœˆï¼ŒNetflixå·¥ç¨‹å¸ˆå’ŒRustå€¡å¯¼è€…@ThePrimeagenå‘å¸ƒäº†ä¸€æ®µè§†é¢‘ï¼šç”¨Mojoè§£æžDNAåºåˆ—ï¼Œé€Ÿåº¦è¶…è¿‡Rust 50%ã€‚è¿™ç¯‡åšå®¢å¼•å‘äº†å¾ˆå¤šå…³æ³¨å’Œè®¨è®ºï¼Œæ¯•ç«ŸRustè¢«è§†ä¸ºPythonå’ŒC++åœ¨AIé¢†åŸŸçš„æ½œåœ¨ç»§ä»»è€…ã€‚\n\n@ThePrimeagenå¯¹Mojoå’ŒRuståœ¨AIç¼–ç¨‹ä¸­çš„å±•æœ›ï¼š\n\n> å¦‚æžœMojoæ­£å¼åŠ å…¥ç«žäº‰ï¼Œé‚£ä¹ˆæˆ‘ç›¸ä¿¡Mojoæ— ç–‘ä¼šèƒœå‡ºã€‚MojoèŽ·èƒœçš„åŽŸå› åœ¨äºŽï¼Œå®ƒä¸éœ€è¦å¯¹å¼€å‘è€…å·²ç»ç†Ÿæ‚‰çš„èŒƒå¼è¿›è¡Œä»»ä½•æ”¹å˜ã€‚åªéœ€ç¨åŠ å­¦ä¹ ï¼Œå°±èƒ½å®žçŽ°æƒŠäººçš„æ€§èƒ½ã€‚é¦–å…ˆï¼ŒMojoç¼–è¯‘é€Ÿåº¦å¿«ï¼Œç”¨æˆ·ä½“éªŒä¸Žå¤§å®¶å·²ç»ç†Ÿæ‚‰çš„è¯­è¨€éžå¸¸ç›¸ä¼¼ï¼Œæ€§èƒ½å¯ä¸ŽRuståª²ç¾Žã€‚å”¯ä¸€çš„é—®é¢˜æ˜¯å¦‚ä½•è®©æ›´å¤šäººæŽ¥å—å®ƒã€‚\n\nåœ¨å‘è¡¨è¯„è®ºåŽï¼Œå—äººå°Šæ•¬çš„Rustè´¡çŒ®è€…åŠã€ŠRust: From Zero to Productionã€‹çš„ä½œè€…Luca Palmieriåœ¨Xä¸Šå›žåº”ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Hqe7bPWGI36LPGzE)\n\nRuståœ¨ç³»ç»Ÿç¼–ç¨‹é¢†åŸŸæ‹¥æœ‰é¡¶å°–çš„è®¾è®¡ï¼Œä½†åœ¨AIåº”ç”¨é¢†åŸŸé¢ä¸´ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼š\n\n* ç¼–è¯‘é€Ÿåº¦æ…¢ï¼Œè€ŒAIå¼ºè°ƒå®žéªŒå’Œå¿«é€Ÿè¿­ä»£ã€‚\n* å¤§å¤šæ•°æœ‰Pythonç»éªŒçš„AIç ”ç©¶äººå‘˜ä¸æ„¿æ„èŠ±æ—¶é—´ä»Žé›¶å¼€å§‹å­¦ä¹ ä¸€é—¨æ–°è¯­è¨€ã€‚\n\nMojoæ—¨åœ¨ä½¿Pythonå¼€å‘è€…èƒ½å¤Ÿç›´è§‚ä¸”è½»æ¾åœ°æŽŒæ¡ã€‚æ­£å¦‚Mohamedæ‰€ç¤ºï¼Œä»–åœ¨å‡ å‘¨å†…ä½œä¸ºä¸€ä¸ªä¸šä½™é¡¹ç›®å­¦ä¹ äº†Mojoï¼Œå¹¶åˆ©ç”¨SIMDä¼˜åŒ–ç®—æ³•ï¼ˆåˆå§‹å®žçŽ°ä»…éœ€200è¡Œä»£ç ï¼‰ã€‚\n\nå¯¹äºŽé‚£äº›å¯¹AIå¼€å‘æ„Ÿå…´è¶£çš„äººæ¥è¯´ï¼Œç¡®å®žå­˜åœ¨åœ¨ä¸‰ç§å¯ç”¨è¯­è¨€ä¸­é€‰æ‹©å…¶ä¸€çš„å›°å¢ƒã€‚\n\nMojoå’ŒRustéƒ½å…è®¸å¼€å‘è€…åœ¨æ›´ä½Žçš„å±‚é¢è¿›è¡Œä¼˜åŒ–ã€‚å¯¹äºŽRustï¼Œå¼€å‘è€…å½“ç„¶å¯ä»¥å°†æ‰€æœ‰å†…å®¹æ‰“åŒ…åˆ°Arcã€Mutexæˆ–Boxä¸­ï¼Œä»¥é¿å…ä¸Žå€Ÿç”¨æ£€æŸ¥å™¨çš„å†²çªï¼Œä½†è¿™å¯èƒ½ä¼šç‰ºç‰²ä¸€äº›æ€§èƒ½ã€‚è™½ç„¶è¿™ç§æ€§èƒ½å·®å¼‚å¯èƒ½å¯¹åº”ç”¨ä»£ç æ²¡æœ‰æ˜¾è‘—å½±å“ï¼Œä½†åœ¨åº“æˆ–å…¶ä»–æ€§èƒ½æ•æ„Ÿä»£ç ä¸­å¯èƒ½ä¼šè¿…é€Ÿç´¯ç§¯ã€‚ä¸¤è€…çš„é€‰æ‹©å–å†³äºŽç¨‹åºå‘˜å¯¹å‡å°‘å¼€é”€å’Œä¼˜åŒ–æ€§èƒ½çš„å…³æ³¨ã€‚\n\nè¿™ä¸¤ç§è¯­è¨€éƒ½å¯ä»¥åˆ©ç”¨LLVMè¿›è¡Œä»£ç ç”Ÿæˆä¼˜åŒ–ï¼Œå¹¶å…è®¸ä½¿ç”¨å†…è”æ±‡ç¼–ï¼ˆå°½ç®¡å®žé™…ä¸Šä¸å¤ªå¯èƒ½æœ‰äººè¿™æ ·åšï¼‰ï¼Œå› æ­¤ç†è®ºä¸Šï¼Œä¸¤è€…åœ¨ä¼ ç»Ÿç¡¬ä»¶ä¸Šçš„æ€§èƒ½æ½œåŠ›ç›¸ä¼¼ã€‚\n\n## 03 åŸºäºŽæœ€å…ˆè¿›çš„ç¼–è¯‘å™¨æŠ€æœ¯\n\nRust äºŽ 2006 å¹´å¯åŠ¨ï¼Œè€Œ Swift äºŽ 2010 å¹´å‡ºçŽ°ï¼Œä¸¤è€…ä¸»è¦åŸºäºŽ LLVM IR æž„å»ºã€‚è€Œ Mojo åˆ™åœ¨ 2022 å¹´é¦–æ¬¡äº®ç›¸ï¼Œæž„å»ºäºŽ MLIR ä¹‹ä¸Šâ€”â€”ä¸Ž Rust ä½¿ç”¨çš„ LLVM IR ç›¸æ¯”ï¼ŒMLIR æ˜¯ä¸€ä¸ªæ›´çŽ°ä»£çš„â€œä¸‹ä¸€ä»£â€ç¼–è¯‘å™¨æ ˆã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒChris Lattner åœ¨ 2000 å¹´ 12 æœˆå¤§å­¦æ—¶æœŸåˆ›ç«‹äº† LLVMï¼Œå¹¶ä»Žå…¶å¤šå¹´çš„æ¼”å˜ä¸­å­¦ä¹ äº†å¾ˆå¤šã€‚ä»–åŽæ¥åŠ å…¥ Google é¢†å¯¼ MLIR çš„å¼€å‘ï¼Œæ—¨åœ¨æ”¯æŒå…¬å¸çš„ TPU å’Œå…¶ä»– AI åŠ é€Ÿå™¨é¡¹ç›®ã€‚éšåŽï¼Œä»–ç»§ç»­åŸºäºŽä»Ž LLVM IR ä¸­èŽ·å¾—çš„çŸ¥è¯†è¿›è¡ŒæŽ¢ç´¢ã€‚\n\nModular è¡¨ç¤º Mojo æ˜¯ç¬¬ä¸€ä¸ªå……åˆ†åˆ©ç”¨ MLIR é«˜çº§ç‰¹æ€§çš„ç¼–ç¨‹è¯­è¨€ã€‚å®ƒå¯ä»¥ç”Ÿæˆå…·æœ‰æ›´é«˜ä¼˜åŒ–çš„ CPU ä»£ç ï¼Œå¹¶ä¸”è¿˜æ”¯æŒ GPU å’Œå…¶ä»–åŠ é€Ÿå™¨ï¼Œé€Ÿåº¦æ¯” Rust å¿«å¾—å¤šã€‚è¿™æ˜¯ç›®å‰å…¶ä»–è¯­è¨€æ— æ³•å®žçŽ°çš„ä¼˜åŠ¿ï¼Œä¹Ÿæ˜¯ AI å’Œç¼–è¯‘å™¨çˆ±å¥½è€…å¯¹ Mojo çƒ­æƒ…çš„æ ¸å¿ƒåŽŸå› ã€‚\n\nä»–ä»¬ç‰¹åˆ«å¼ºè°ƒä¸¤ä¸ªæ–¹é¢ï¼š\n\nå‡ºè‰²çš„ SIMD äººä½“å·¥ç¨‹å­¦è®¾è®¡ï¼šCPU é€šè¿‡ç‰¹æ®Šå¯„å­˜å™¨å’ŒæŒ‡ä»¤åŒæ—¶å¤„ç†å¤šä¸ªæ•°æ®å…ƒç´ ï¼Œç§°ä¸º SIMDï¼ˆå•æŒ‡ä»¤å¤šæ•°æ®ï¼‰ã€‚ç„¶è€Œï¼Œä»ŽåŽ†å²ä¸Šçœ‹ï¼Œç¼–å†™æ­¤ç±»ä»£ç çš„ä½“éªŒä¸€ç›´å¾ˆç³Ÿç³•ï¼Œå¹¶ä¸”åœ¨äººä½“å·¥ç¨‹å­¦æ–¹é¢å¾ˆéš¾ä½¿ç”¨ã€‚å°½ç®¡è¿™äº›ç‰¹æ®ŠæŒ‡ä»¤å·²ç»å­˜åœ¨å¤šå¹´ï¼Œä½†å¤§å¤šæ•°ä»£ç å¹¶æœªé’ˆå¯¹å®ƒä»¬è¿›è¡Œä¼˜åŒ–ã€‚å› æ­¤ï¼Œè°èƒ½è§£å†³è¿™ç§å¤æ‚æ€§å¹¶ç¼–å†™å¯ç§»æ¤çš„ SIMD ä¼˜åŒ–ç®—æ³•ï¼Œè°å°±èƒ½åœ¨å¸‚åœºä¸­è„±é¢–è€Œå‡ºï¼Œä¾‹å¦‚ simd_jsonã€‚\n\nMojo çš„åŽŸè¯­ä»Žä¸€å¼€å§‹å°±ä»¥ SIMD ä¸ºä¼˜å…ˆè®¾è®¡ï¼šUInt8 å®žé™…ä¸Šæ˜¯ SIMD\\[DType.uint8, 1]ï¼Œè¡¨ç¤ºä¸€ä¸ªå…ƒç´ çš„ SIMDã€‚è¿™ç§è¡¨ç¤ºä¸ä¼šå¸¦æ¥æ€§èƒ½å¼€é”€ï¼ŒåŒæ—¶å…è®¸ç¨‹åºå‘˜è½»æ¾åœ°å°†å…¶ç”¨äºŽ SIMD ä¼˜åŒ–ã€‚ä¾‹å¦‚ï¼Œæ–‡æœ¬å¯ä»¥è¢«æ‹†åˆ†ä¸º 64 å­—èŠ‚çš„å—ï¼Œè¡¨ç¤ºä¸º SIMD\\[DType.uint8, 64]ï¼Œç„¶åŽä¸Žå•ä¸ªæ¢è¡Œç¬¦è¿›è¡Œæ¯”è¾ƒï¼Œä»¥æ‰¾åˆ°æ¯ä¸ªæ¢è¡Œç¬¦çš„ç´¢å¼•ã€‚ç”±äºŽæœºå™¨ä¸Šçš„ SIMD å¯„å­˜å™¨å¯ä»¥åŒæ—¶å¯¹ 512 ä½æ•°æ®æ‰§è¡Œæ“ä½œï¼Œå› æ­¤æ­¤æ“ä½œå¯ä»¥å°†æ­¤ç±»æ“ä½œçš„æ€§èƒ½æå‡ 64 å€ï¼\n\næˆ–è€…ç»™å‡ºä¸€ä¸ªæ›´ç®€å•çš„ä¾‹å­ï¼Œå‡è®¾ä½ æœ‰ä¸€ä¸ª SIMDDType.float64, 8ã€‚åªéœ€å°†å…¶ä¹˜ä»¥ Float64(2)ï¼Œä½ å°±å¯ä»¥è½»æ¾æé«˜æ€§èƒ½ã€‚ä¸Žé€ä¸ªä¹˜ä»¥æ¯ä¸ªå…ƒç´ ç›¸æ¯”ï¼Œè¿™ç§æ–¹æ³•å¯ä»¥åœ¨å¤§å¤šæ•°æœºå™¨ä¸Šå°†æ€§èƒ½æé«˜å¤šè¾¾ 8 å€ã€‚\n\nLLVMï¼ˆRust ä¹Ÿåœ¨ä½¿ç”¨ï¼‰å…·æœ‰è‡ªåŠ¨å‘é‡åŒ–ä¼˜åŒ–é€šé“ï¼Œä½†ç”±äºŽå…¶æ— æ³•æ›´æ”¹ SIMD çš„å†…å­˜å¸ƒå±€å’Œå…¶ä»–é‡è¦ç»†èŠ‚ï¼Œå…¶æ€§èƒ½ä»Žæœªè¾¾åˆ°ç†è®ºä¼˜åŒ–æ°´å¹³ã€‚ç„¶è€Œï¼ŒMojo ä»Žä¸€å¼€å§‹å°±è€ƒè™‘äº† SIMD ç‰¹æ€§ï¼Œå› æ­¤ç¼–å†™ SIMD ä¼˜åŒ–çš„ä½“éªŒä¸Žç¼–å†™å¸¸è§„ä»£ç éžå¸¸ç›¸ä¼¼ã€‚\n\næ€¥åˆ‡é”€æ¯ï¼šRust çš„è®¾è®¡å—åˆ° C++ çš„ RAIIï¼ˆèµ„æºèŽ·å–å³åˆå§‹åŒ–ï¼‰å¯å‘ï¼Œè¿™æ„å‘³ç€ä¸€æ—¦å¯¹è±¡è¶…å‡ºä½œç”¨åŸŸï¼Œåº”ç”¨ç¨‹åºå¼€å‘äººå‘˜ä¸éœ€è¦æ‹…å¿ƒé‡Šæ”¾å†…å­˜â€”â€”ç¼–ç¨‹è¯­è¨€æœ¬èº«ä¼šå¤„ç†è¿™ä¸€ç‚¹ã€‚è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ä¾‹å­ï¼Œé¿å…äº†åžƒåœ¾å›žæ”¶çš„æ€§èƒ½é™·é˜±ï¼ŒåŒæ—¶ç¡®ä¿äº†åŠ¨æ€è¯­è¨€çš„äººä½“å·¥ç¨‹å­¦ã€‚\n\nMojo æ›´è¿›ä¸€æ­¥ï¼Œä¸æ˜¯ç­‰åˆ°ä½œç”¨åŸŸç»“æŸï¼Œè€Œæ˜¯åœ¨å¯¹è±¡æœ€åŽä¸€æ¬¡ä½¿ç”¨æ—¶é‡Šæ”¾å†…å­˜ã€‚è¿™å¯¹äºŽ AI åœºæ™¯éžå¸¸æœ‰ç›Šï¼Œå› ä¸ºæå‰é‡Šæ”¾å¯¹è±¡æ„å‘³ç€æå‰é‡Šæ”¾ GPU å¼ é‡ï¼Œä»Žè€Œå…è®¸åœ¨ç­‰æ•ˆçš„ GPU RAM ä¸­é€‚åº”æ›´å¤§çš„æ¨¡åž‹ã€‚è¿™æ˜¯ Mojo çš„ç‹¬ç‰¹ä¼˜åŠ¿ï¼Œä½¿ç¨‹åºå‘˜èƒ½å¤Ÿåœ¨ä¸å¿…è‡ªå·±è®¾è®¡çš„æƒ…å†µä¸‹å®žçŽ°æœ€ä½³æ€§èƒ½ã€‚Rust çš„å€Ÿç”¨æ£€æŸ¥å™¨æœ€åˆå°†æ‰€æœ‰äº‹ç‰©çš„ç”Ÿå‘½å‘¨æœŸå»¶é•¿åˆ°å…¶ä½œç”¨åŸŸçš„ç»“æŸï¼ŒåŒ¹é…æžæž„å‡½æ•°çš„è¡Œä¸ºï¼Œä½†è¿™å¯èƒ½ä¼šç»™ç”¨æˆ·å¸¦æ¥ä¸€äº›å›°æƒ‘çš„åŽæžœã€‚Rust åŽæ¥æ·»åŠ äº†ä¸€äº›éžè¯æ³•ç”Ÿå‘½å‘¨æœŸç‰¹æ€§ï¼Œä»¥ç®€åŒ–å¼€å‘äººå‘˜çš„å·¥ä½œã€‚ç„¶è€Œï¼Œé€šè¿‡ Mojo çš„æ€¥åˆ‡æžæž„æœºåˆ¶ï¼Œå¯ä»¥ç›´æŽ¥å®žçŽ°è¿™ç§ç®€åŒ–æ•ˆæžœï¼Œå¹¶ä¸”ä¸Žå¯¹è±¡å®žé™…é”€æ¯çš„æ–¹å¼ä¿æŒä¸€è‡´ï¼Œä»Žè€Œé¿å…ä»¤äººå›°æƒ‘çš„æžç«¯æƒ…å†µã€‚\n\nRust å¦ä¸€ä¸ªå¼€é”€æ¥è‡ª Drop çš„å®žçŽ°ã€‚å®ƒä½¿ç”¨ Drop Flags æ¥è·Ÿè¸ªå¯¹è±¡æ˜¯å¦åº”è¯¥åœ¨è¿è¡Œæ—¶è¢«åˆ é™¤ã€‚Rust èƒ½å¤Ÿåœ¨æŸäº›æƒ…å†µä¸‹è¿›è¡Œä¼˜åŒ–ï¼Œä½† Mojo å¯ä»¥é€šè¿‡æ˜¾å¼å®šä¹‰æ¶ˆé™¤æ‰€æœ‰é¢å¤–å¼€é”€ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*0VcMppg3rDTqfsMY)\n\næ— è®ºå¦‚ä½•ï¼Œå¼€å‘äººå‘˜å¿…é¡»åœ¨ Mojo å’Œ Python çš„æ˜“ç”¨æ€§ï¼Œä»¥åŠ Cã€C++ æˆ– Rust çš„é«˜æ€§èƒ½ä¹‹é—´åšå‡ºé€‰æ‹©ã€‚å¯¹æ­¤ï¼ŒMojo å›¢é˜Ÿå‘¼åå¼€å‘äººå‘˜ï¼šâ€œå¦‚æžœä½ å¯¹æœªæ¥å……æ»¡å¥½å¥‡ï¼Œå¸Œæœ›æŽŒæ¡ä¸€ç§å¯èƒ½åœ¨æœªæ¥ 50 å¹´å†…ä¿ƒè¿› AI å‘å±•çš„è¯­è¨€ï¼Œä¸ºä»€ä¹ˆä¸è¯•è¯• Mojo å‘¢ï¼Ÿâ€\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/multi-agent-hedge-fund-simulation-with-langchain-and-langgraph-64060aabe711","frontmatter":{"title":"åˆ©ç”¨ LangChain å’Œ LangGraph è¿›è¡Œå¤šä»£ç†å¯¹å†²åŸºé‡‘æ¨¡æ‹Ÿ","meta_title":"åˆ©ç”¨ LangChain å’Œ LangGraph è¿›è¡Œå¤šä»£ç†å¯¹å†²åŸºé‡‘æ¨¡æ‹Ÿ","description":"æœ¬é¡¹ç›®æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨å¤šä»£ç†è®¾ç½®æ¥æ¨¡æ‹Ÿå¯¹å†²åŸºé‡‘çš„åˆ†æžæµç¨‹ã€‚å®ƒå±•ç¤ºäº†ä¸€ç§å®žç”¨çš„æ–¹æ³•æ¥...","date":"2024-11-10T03:51:17.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*i8wneK22YezD7zOhPKvZfg.png","categories":["Finance","Programming","Data Science"],"author":"Rifx.Online","tags":["multi-agent","LangChain","LangGraph","FinancialDatasets","predictive"],"draft":false,"slug":"blog/multi-agent-hedge-fund-simulation-with-langchain-and-langgraph-64060aabe711"},"content":"\n### å¤šæ™ºèƒ½ä½“å¯¹å†²åŸºé‡‘æ¨¡æ‹Ÿä¸Ž LangChain å’Œ LangGraph\n\n\n\nè¯¥é¡¹ç›®æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨å¤šæ™ºèƒ½ä½“è®¾ç½®æ¥æ¨¡æ‹Ÿå¯¹å†²åŸºé‡‘çš„åˆ†æžè¿‡ç¨‹ã€‚å®ƒå±•ç¤ºäº†ä¸€ç§å®žç”¨çš„æ–¹æ³•æ¥æž„å»ºä¸€ä¸ªç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿåˆ©ç”¨ AI æ™ºèƒ½ä½“æ”¶é›†å’Œåˆ†æžé‡‘èžæ•°æ®ï¼Œè¿™ç§è®¾ç½®å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•å’Œå®šåˆ¶ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘å°†åˆ†è§£è¯¥é¡¹ç›®ï¼Œå…¶ä¸­æ¶‰åŠä¸€ä¸ªæŠ•èµ„ç»„åˆç»ç†å’Œä¸‰ä¸ªåˆ†æžå¸ˆæ™ºèƒ½ä½“ï¼ˆåŸºæœ¬é¢ã€æŠ€æœ¯é¢å’Œæƒ…ç»ªé¢ï¼‰ï¼Œæ¯ä¸ªæ™ºèƒ½ä½“åœ¨æ”¶é›†å’Œå¤„ç†è‚¡ç¥¨æ•°æ®æ–¹é¢è¢«åˆ†é…äº†ç‰¹å®šè§’è‰²ã€‚\n\nè¯¥é¡¹ç›®çš„ç›®æ ‡ä¸æ˜¯æž„å»ºä¸€ä¸ªå…¨é¢çš„äº¤æ˜“ç®—æ³•ï¼Œè€Œæ˜¯è¯´æ˜Žå¦‚ä½•ä½¿ç”¨ LangChain å’Œ LangGraph ç»„ç»‡å’Œå¹¶è¡Œåˆ†æžå„ç§ç±»åž‹çš„æ•°æ®ï¼Œåˆ©ç”¨ä¸“ä¸šçš„æ™ºèƒ½ä½“ã€‚\n\n### é¡¹ç›®ç»“æž„å’Œä»£ç†æ¦‚è¿°\n\nè¯¥ä»£ç†ç³»ç»ŸåŒ…æ‹¬ï¼š\n\n1. **Portfolio Manager** â€” å°†ä»»åŠ¡å§”æ´¾ç»™åˆ†æžå¸ˆå¹¶æ±‡æ€»ä»–ä»¬çš„å‘çŽ°ã€‚\n2. **Fundamental Analyst** â€” èŽ·å–å’Œåˆ†æžè´¢åŠ¡æŠ¥è¡¨ï¼Œä¾‹å¦‚åˆ©æ¶¦è¡¨ã€‚\n3. **Technical Analyst** â€” æ”¶é›†æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„è‚¡ç¥¨ä»·æ ¼æ•°æ®ã€‚\n4. **Sentiment Analyst** â€” å…³æ³¨å†…éƒ¨äº¤æ˜“å’Œæ–°é—»æ•°æ®ï¼Œæä¾›æƒ…ç»ªæ´žå¯Ÿã€‚\n\næ¯ä¸ªä»£ç†éƒ½æ—¨åœ¨ä¸“æ³¨äºŽç‰¹å®šçš„æ•°æ®æ£€ç´¢ä»»åŠ¡ï¼Œä»Žè€Œå®žçŽ°æ¨¡å—åŒ–å’Œå¯æ‰©å±•çš„åˆ†æžã€‚é€šè¿‡ä½¿ç”¨ LangChain å®žçŽ°ä»£ç†åŠŸèƒ½å’Œ LangGraph ç®¡ç†å¹¶è¡Œå·¥ä½œæµï¼Œæˆ‘ä»¬å¯ä»¥å¿«é€Ÿå¤„ç†å¤šä¸ªæ•°æ®æºã€‚FinancialDatasets API æä¾›äº†ä¸°å¯Œçš„æ•°æ®æ¥æºï¼Œæ‹¥æœ‰è¶…è¿‡ 30,000 ä¸ªè‚¡ç¥¨ä»£ç ï¼Œä½¿å¾—å…¨é¢åˆ†æžæˆä¸ºå¯èƒ½ã€‚\n\n### å…³é”®åº“å’Œè®¾ç½®\n\nLangChain å’Œ LangGraph ä½¿å¾—å¤šæ™ºèƒ½ä½“å·¥ä½œæµå’Œå¹¶è¡Œå¤„ç†çš„åˆ†æ”¯é€»è¾‘å¤„ç†å˜å¾—ç®€å•ã€‚è®¾ç½®å¼€å§‹äºŽå®‰è£…æ‰€éœ€çš„åº“å¹¶èŽ·å– API å¯†é’¥ï¼š\n\n```python\n%%capture --no-stderr\n%pip install -U langgraph langchain langchain_openai langchain_experimental langsmith pandas\n```\nçŽ¯å¢ƒå˜é‡ç”¨äºŽå­˜å‚¨æ•æ„Ÿæ•°æ®ï¼Œä¾‹å¦‚ API å¯†é’¥ï¼š\n\n```python\nimport getpass\nimport os\n\ndef _set_if_undefined(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"Please provide your {var}\")\n\n_set_if_undefined(\"OPENAI_API_KEY\")               # https://platform.openai.com\n_set_if_undefined(\"FINANCIAL_DATASETS_API_KEY\")   # https://financialdatasets.ai\n_set_if_undefined(\"TAVILY_API_KEY\")               # https://tavily.com\n```\n\n### ä»£ç†åŠŸèƒ½ï¼šæ£€ç´¢æ•°æ®\n\nç³»ç»Ÿä¸­çš„æ¯ä¸ªä»£ç†éƒ½æ—¨åœ¨å¤„ç†ä¸Žè‚¡ç¥¨åˆ†æžç›¸å…³çš„ç‰¹å®šç±»åž‹æ•°æ®ã€‚\n\n### 1\\. åŸºæœ¬é¢åˆ†æžå¸ˆ\n\nåŸºæœ¬é¢åˆ†æžå¸ˆèŽ·å–å¹¶æ£€æŸ¥è´¢åŠ¡æŠ¥è¡¨ï¼Œè¿™äº›æŠ¥è¡¨æä¾›äº†å…¬å¸è´¢åŠ¡å¥åº·çŠ¶å†µçš„æ´žå¯Ÿã€‚ä»¥ä¸‹æ˜¯èŽ·å–æ”¶å…¥æŠ¥è¡¨çš„å·¥å…·ï¼Œè¿™æ˜¯ä¸€ä¸ªå…³é”®çš„è´¢åŠ¡æ–‡ä»¶ï¼š\n\n```python\nfrom langchain_core.tools import tool\nfrom typing import Dict, Union\nfrom pydantic import BaseModel, Field\n\nclass GetIncomeStatementsInput(BaseModel):\n    ticker: str = Field(..., description=\"The ticker of the stock.\")\n    period: str = Field(default=\"ttm\", description=\"Valid values are 'ttm', 'quarterly', or 'annual'.\")\n    limit: int = Field(default=10, description=\"Maximum number of income statements to return.\")\n\n@tool(\"get_income_statements\", args_schema=GetIncomeStatementsInput, return_direct=True)\ndef get_income_statements(ticker: str, period: str = \"ttm\", limit: int = 10) -> Union[Dict, str]:\n    api_key = os.environ.get(\"FINANCIAL_DATASETS_API_KEY\")\n    url = f'https://api.financialdatasets.ai/financials/income-statements?ticker={ticker}&period={period}&limit={limit}'\n    try:\n        response = requests.get(url, headers={'X-API-Key': api_key})\n        return response.json()\n    except Exception as e:\n        return {\"ticker\": ticker, \"income_statements\": [], \"error\": str(e)}\n```\nåœ¨è¿™é‡Œï¼Œ`get_income_statements` ç”¨äºŽèŽ·å–ç»™å®šè‚¡ç¥¨ä»£ç çš„æ”¶å…¥æŠ¥è¡¨ã€‚é€šè¿‡æŒ‡å®šæœŸé—´ï¼ˆä¾‹å¦‚ï¼Œâ€œttmâ€è¡¨ç¤ºè¿‡åŽ»åäºŒä¸ªæœˆï¼‰ï¼Œä»£ç†å¯ä»¥ä¸“æ³¨äºŽä¸åŒçš„æŠ¥å‘Šå‘¨æœŸã€‚\n\n### 2\\. æŠ€æœ¯åˆ†æžå¸ˆ\n\næŠ€æœ¯åˆ†æžå¸ˆæ”¶é›†åœ¨å®šä¹‰æ—¶é—´èŒƒå›´å†…çš„è‚¡ç¥¨ä»·æ ¼æ•°æ®ã€‚è¿™äº›æ•°æ®å¯ä»¥ç”¨äºŽè®¡ç®—æŒ‡æ ‡æˆ–è¯†åˆ«æ¨¡å¼ã€‚ä»¥ä¸‹æ˜¯æ£€ç´¢è‚¡ç¥¨ä»·æ ¼çš„ä»£ç ï¼š\n\n```python\nclass GetPricesInput(BaseModel):\n    ticker: str\n    start_date: str\n    end_date: str\n    interval: str = \"day\"\n    interval_multiplier: int = 1\n    limit: int = 5000\n\n@tool(\"get_stock_prices\", args_schema=GetPricesInput, return_direct=True)\ndef get_stock_prices(ticker: str, start_date: str, end_date: str, interval: str, interval_multiplier: int = 1, limit: int = 5000) -> Union[Dict, str]:\n    api_key = os.environ.get(\"FINANCIAL_DATASETS_API_KEY\")\n    url = (\n        f\"https://api.financialdatasets.ai/prices?ticker={ticker}\"\n        f\"&start_date={start_date}&end_date={end_date}\"\n        f\"&interval={interval}&interval_multiplier={interval_multiplier}\"\n        f\"&limit={limit}\"\n    )\n    try:\n        response = requests.get(url, headers={'X-API-Key': api_key})\n        return response.json()\n    except Exception as e:\n        return {\"ticker\": ticker, \"prices\": [], \"error\": str(e)}\n```\nè¯¥å‡½æ•°å…è®¸æˆ‘ä»¬æŒ‡å®šæ—¥æœŸèŒƒå›´å’Œæ—¶é—´é—´éš”ç­‰å‚æ•°ï¼Œä»Žè€ŒæŽ§åˆ¶æ•°æ®çš„ç²’åº¦ï¼ˆä¾‹å¦‚ï¼ŒæŒ‰æ—¥æˆ–æŒ‰å°æ—¶ï¼‰ã€‚\n\n### 3\\. æƒ…ç»ªåˆ†æžå¸ˆ\n\næƒ…ç»ªåˆ†æžå¸ˆæ”¶é›†å†…éƒ¨äº¤æ˜“å’Œç›¸å…³æ–°é—»çš„æ•°æ®ã€‚å†…éƒ¨äº¤æ˜“å’Œå…¬ä¼—æƒ…ç»ªæŒ‡æ ‡å¯ä»¥æä¾›å¸‚åœºæ„ŸçŸ¥çš„æ´žå¯Ÿï¼Œè¿™å¯¹äºŽè¯„ä¼°è‚¡ç¥¨æ³¢åŠ¨æ€§å’Œæ½œåœ¨ä»·æ ¼å˜åŠ¨éžå¸¸é‡è¦ã€‚\n\n```python\nclass GetInsiderTradesInput(BaseModel):\n    ticker: str\n    limit: int = 10\n\n@tool(\"get_insider_trades\", args_schema=GetInsiderTradesInput, return_direct=True)\ndef get_insider_trades(ticker: str, limit: int = 10) -> Union[Dict, str]:\n    api_key = os.environ.get(\"FINANCIAL_DATASETS_API_KEY\")\n    url = f'https://api.financialdatasets.ai/insider-transactions?ticker={ticker}&limit={limit}'\n    try:\n        response = requests.get(url, headers={'X-API-Key': api_key})\n        return response.json()\n    except Exception as e:\n        return {\"ticker\": ticker, \"insider_transactions\": [], \"error\": str(e)}\n```\né€šè¿‡æ•èŽ·å†…éƒ¨äº¤æ˜“ï¼Œè¯¥å·¥å…·å¯ä»¥è·Ÿè¸ªæ‹¥æœ‰ç‰¹æƒä¿¡æ¯çš„äººçš„æ“ä½œï¼Œè¿™å¯èƒ½æ˜¯ç»©æ•ˆå˜åŒ–çš„æ—©æœŸæŒ‡æ ‡ã€‚\n\n### æŠ•èµ„ç»„åˆç»ç†ï¼šåè°ƒå’Œæ€»ç»“åˆ†æž\n\næŠ•èµ„ç»„åˆç»ç†ä½œä¸ºåè°ƒè€…ï¼Œå°†ä»»åŠ¡åˆ†é…ç»™åˆ†æžå¸ˆï¼Œå¹¶å°†ä»–ä»¬çš„ç»“æžœæ±‡æ€»æˆä¸€ä»½æŠ¥å‘Šã€‚ä»¥ä¸‹æ˜¯æŠ•èµ„ç»„åˆç»ç†çš„ç¤ºä¾‹å·¥ä½œæµç¨‹ï¼Œå±•ç¤ºäº†å®ƒå¦‚ä½•è°ƒç”¨æ¯ä¸ªä»£ç†ï¼š\n\n```python\nfrom langchain_community.tools.tavily_search import TavilySearchResults\n\n## Tools grouped by agent type\nfundamental_tools = [get_income_statements]\ntechnical_tools = [get_stock_prices]\nsentiment_tools = [get_insider_trades, TavilySearchResults(max_results=5)]\n\n## Sample function for running all analyses in parallel\ndef analyze_portfolio(ticker: str):\n    # Delegate tasks to each agent\n    fundamentals = [tool(ticker=ticker) for tool in fundamental_tools]\n    prices = [tool(ticker=ticker, start_date=\"2023-01-01\", end_date=\"2023-12-31\") for tool in technical_tools]\n    sentiment = [tool(ticker=ticker) for tool in sentiment_tools]\n    \n    # Summarize results (simplified)\n    summary = {\n        \"fundamentals\": fundamentals,\n        \"technical\": prices,\n        \"sentiment\": sentiment\n    }\n    return summary\n```\nåœ¨è¿™ä¸ªå‡½æ•°ä¸­ï¼š\n\n* æ¯ä¸ªä»£ç†çš„å‡½æ•°å¹¶è¡Œè°ƒç”¨ï¼Œä»¥æ”¶é›†æŒ‡å®šè‚¡ç¥¨ä»£ç çš„æ•°æ®ã€‚\n* ç„¶åŽï¼Œç»ç†å°†æ¥è‡ªæ¯ä¸ªä»£ç†çš„æ•°æ®æ±‡æ€»æˆä¸€ä¸ªç®€æ˜Žçš„æ‘˜è¦ï¼Œä»¥ä¾¿äºŽå®¡é˜…ã€‚\n\n### ç»“è®º\n\næœ¬é¡¹ç›®æä¾›äº†ä¸€ä¸ªåŸºæœ¬ä½†çµæ´»çš„è®¾ç½®ï¼Œé€šè¿‡ä¸€ç»„ä¸“ä¸šä»£ç†åˆ†æžè‚¡ç¥¨æ•°æ®ã€‚é€šè¿‡å°†ä»»åŠ¡åˆ†é…ç»™æŠ•èµ„ç»„åˆç»ç†ã€åŸºæœ¬é¢åˆ†æžå¸ˆã€æŠ€æœ¯åˆ†æžå¸ˆå’Œæƒ…ç»ªåˆ†æžå¸ˆï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨ä¸åŒçš„é‡‘èžæ•°æ®ç±»åž‹ä¸­æ”¶é›†å’Œç»„ç»‡è§è§£ã€‚ä½¿ç”¨ LangChain å’Œ LangGraph å®žçŽ°æ¨¡å—åŒ–å’Œå¹¶è¡Œå¤„ç†ï¼Œä½¿è¿™ç§æ–¹æ³•å…·æœ‰å¯æ‰©å±•æ€§ï¼Œè€Œé‡‘èžæ•°æ®é›† API åˆ™æ”¯æŒå¹¿æ³›çš„è‚¡ç¥¨ä»£ç ï¼Œèƒ½å¤Ÿå®žçŽ°å¼ºå¤§çš„æ•°æ®è®¿é—®ã€‚\n\nè™½ç„¶è¯¥ç³»ç»Ÿè¢«è®¾è®¡ä¸ºä¸€ä¸ªå®žè·µé¡¹ç›®ï¼Œä½†å…¶ç»“æž„å¯ä»¥ä½œä¸ºæ›´å¤æ‚çš„å¯¹å†²åŸºé‡‘æ¨¡æ‹Ÿæˆ–æ•°æ®åˆ†æžå·¥å…·çš„åŸºç¡€ã€‚ä¸‹ä¸€æ­¥å¯èƒ½åŒ…æ‹¬ä¸ºæ¯ä¸ªä»£ç†å¢žå¼ºæ›´å¤šå·¥å…·æˆ–æ•°æ®åˆ†æžæŠ€æœ¯ï¼Œä¾‹å¦‚ï¼š\n\n* **æŠ€æœ¯æ¨¡å¼å’ŒæŒ‡æ ‡ï¼š** æ•´åˆæ›´å¤šæŠ€æœ¯åˆ†æžå·¥å…·ï¼Œå¦‚ç§»åŠ¨å¹³å‡çº¿æˆ–è¶‹åŠ¿çº¿ã€‚\n* **æƒ…ç»ªè¯„åˆ†ï¼š** ä»Žæ–°é—»æ¥æºæˆ–å†…éƒ¨äº¤æ˜“æ•°æ®è‡ªåŠ¨åŒ–æƒ…ç»ªè¯„åˆ†ã€‚\n* **é¢„æµ‹å»ºæ¨¡ï¼š** æ·»åŠ å¯ä»¥æ ¹æ®ç»¼åˆæ•°æ®åšå‡ºä¹°å–å»ºè®®çš„æœºå™¨å­¦ä¹ æ¨¡åž‹ã€‚\n\nè¯¥è®¾ç½®æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„æ¨¡å—åŒ–é‡‘èžæ•°æ®åˆ†æžåŽŸåž‹ï¼Œæœªæ¥è¿˜æœ‰å¾ˆå¤šå®šåˆ¶å’Œæ”¹è¿›çš„ç©ºé—´ã€‚\n\nå¯¹äºŽé‚£äº›å¯¹è¿™ä¸ªå·¥å…·åŒ…èƒŒåŽçš„ä»£ç æ„Ÿå…´è¶£çš„äººï¼Œæ‚¨å¯ä»¥åœ¨ GitHub ä¸Šæ‰¾åˆ°å®Œæ•´çš„å®žçŽ° [*è¿™é‡Œ*](https://github.com/shaikhmubin02/ai-hedge-fund)ã€‚\n\n"},{"lang":"zh","group":"blog","slug":"blog/multimodal-ai-for-conversational-human-motion-3102e991938c","frontmatter":{"title":"ç”¨äºŽäººç±»è¿åŠ¨å¯¹è¯çš„å¤šæ¨¡æ€äººå·¥æ™ºèƒ½","meta_title":"ç”¨äºŽäººç±»è¿åŠ¨å¯¹è¯çš„å¤šæ¨¡æ€äººå·¥æ™ºèƒ½","description":"æœ¬æ–‡æŽ¢è®¨äº†å¤šæ¨¡æ€äººå·¥æ™ºèƒ½åœ¨å¯¹è¯ä¸­çš„åº”ç”¨ï¼Œå¼ºè°ƒé€šè¿‡æ„ŸçŸ¥ã€è¿åŠ¨è§„åˆ’å’Œè™šæ‹Ÿå½¢è±¡æ¸²æŸ“å®žçŽ°æ›´è‡ªç„¶çš„äº¤äº’ã€‚å¤šæ¨¡æ€æ¨¡åž‹èƒ½å¤Ÿå‡å°‘ä¿¡æ¯æŸå¤±ï¼Œæ•´åˆè§†è§‰ã€å¬è§‰å’Œæ–‡æœ¬è¾“å…¥ï¼Œä»Žè€Œæå‡è™šæ‹Ÿå½¢è±¡çš„å“åº”èƒ½åŠ›ã€‚æ–‡ç« è¿˜åˆ†æžäº†å½“å‰çš„åº”ç”¨æ¡ˆä¾‹ï¼Œå¦‚åŒ»ç–—ã€å®¢æˆ·æ”¯æŒå’Œæ•™è‚²ï¼ŒæŒ‡å‡ºåœ¨å®žæ—¶å“åº”ã€ä¸ªæ€§åŒ–å’Œè®°å¿†ç®¡ç†ç­‰æ–¹é¢çš„æŒ‘æˆ˜ã€‚éšç€æŠ€æœ¯çš„å‘å±•ï¼Œæœªæ¥çš„åº”ç”¨å°†æœ‰åŠ©äºŽå®žçŽ°æ›´å¤æ‚çš„äººæœºäº¤äº’ã€‚","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*zANW8t-IxPlkyxX-5_9Ayw.png","categories":["Chatbots","Autonomous Systems","Natural Language Processing"],"author":"Rifx.Online","tags":["multimodal","perception","avatar","latency","empathy"],"draft":false,"slug":"blog/multimodal-ai-for-conversational-human-motion-3102e991938c"},"content":"\n\n\næ’°å†™è€…ï¼š[Christian Safka](https://www.linkedin.com/in/christiansafka/) å’Œ [Keyu Chen](https://www.linkedin.com/in/keyu-chen-3a3026143/?locale=en_US)\n\n\n\nåœ¨æœ¬æ¬¡æŽ¢ç´¢ä¸­ï¼Œæˆ‘ä»¬å°†æŽ¢è®¨å¤šæ¨¡æ€æ¨¡åž‹å¦‚ä½•æ”¹å˜å¯¹è¯äººå·¥æ™ºèƒ½ä»£ç†çš„æ¸¸æˆè§„åˆ™ï¼Œä»¥åŠå¦‚ä½•åˆ©ç”¨æ„ŸçŸ¥ã€è®°å¿†ã€è¡Œä¸ºå»ºæ¨¡å’Œå®žæ—¶æ¸²æŸ“åœ¨å„ç§çŽ¯å¢ƒä¸­å®žçŽ°æ— ç¼äº¤äº’ã€‚\n\næœ¬é¡µçš„æçº²ï¼š\n\n* ä¸ºä»€ä¹ˆé€‰æ‹©å¤šæ¨¡æ€ï¼Ÿ\n* æ·±å…¥äººç±»è¿åŠ¨ç®¡é“\n* è®­ç»ƒä¸­çš„æŒ‘æˆ˜\n* å½“å‰ç”¨ä¾‹å’Œæœªæ¥\n\n## ä¸ºä»€ä¹ˆå¤šæ¨¡æ€ï¼Ÿ\n\nä»Žé«˜å±‚æ¬¡æ¥çœ‹ï¼Œæˆ‘ä»¬éœ€è¦å®žçŽ°ç±»äººå¯¹è¯çš„ä¸‰ä¸ªâ€œå±‚æ¬¡â€æ˜¯è¾“å…¥æ„ŸçŸ¥ã€è¿åŠ¨è§„åˆ’å’Œè™šæ‹Ÿå½¢è±¡æ¸²æŸ“ã€‚æˆªæ­¢åˆ°æœ¬ç¯‡å†™ä½œæ—¶ï¼Œå¤§å¤šæ•°å­¦æœ¯ç•Œçš„æµç¨‹å°†è¿™äº›å±‚æ¬¡åˆ†å¼€ï¼Œä»¥æ–‡æœ¬ä½œä¸ºä¸­ä»‹ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*4a8JvOVbsP8mY3AjiPgNPA.png)\n\nå¤šæ¨¡æ€æ¨¡åž‹æ‰€è§£é”çš„æ˜¯è¿™äº›å±‚æ¬¡ä¹‹é—´ä¿¡æ¯æŸå¤±çš„å‡å°‘ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*VUFhrwLA7sUFmHwidb7DWg.png)\n\n## æ·±å…¥æŽ¢è®¨äººç±»è¿åŠ¨ç®¡é“\n\nç”Ÿæˆç±»äººåŠ¨ä½œå’Œååº”æ˜¯ä¸€ä¸ªå›°éš¾çš„é—®é¢˜ã€‚å®ƒéœ€è¦ä¸€ä¸ªç®¡é“æ¥å¤„ç†æ¥è‡ªå¤šä¸ªæ¥æºçš„å®žæ—¶çº¿ç´¢ï¼Œè¿›è¡Œè§£é‡Šã€ç¿»è¯‘å¹¶ç”ŸæˆåŒæ­¥å“åº”ã€‚æ‰€æœ‰é˜¶æ®µå¯¹äºŽåˆ›å»ºèƒ½å¤Ÿå‚ä¸Žæµç•…ã€ä¸Šä¸‹æ–‡ç›¸å…³å¯¹è¯çš„è™šæ‹Ÿå½¢è±¡è‡³å…³é‡è¦ã€‚\n\næˆ‘ä»¬è®¨è®ºäº†ä¸‰ä¸ªå±‚æ¬¡ï¼š\n\n1\\. **è¾“å…¥æ„ŸçŸ¥** â€” ä»Žè§†è§‰ã€å¬è§‰å’ŒåŸºäºŽæ–‡æœ¬çš„æ¥æºæ”¶é›†å¤šæ¨¡æ€çº¿ç´¢ã€‚\n\n2\\. **è¿åŠ¨è§„åˆ’** â€” æ ¹æ®è¿™äº›è¾“å…¥ç¡®å®šé€‚å½“çš„åŠ¨ä½œæˆ–ååº”ã€‚\n\n3\\. **è™šæ‹Ÿå½¢è±¡è¾“å‡º** â€” ä»¥å®žæ—¶æ–¹å¼æ¸²æŸ“è¿™äº›è®¡åˆ’çš„åŠ¨ä½œã€‚\n\nçŽ°åœ¨è®©æˆ‘ä»¬åˆ†æžæ¯ä¸ªå±‚æ¬¡åœ¨åˆ›å»ºç±»äººå¯¹è¯ä¸­çš„å…³é”®è§’è‰²ã€‚\n\n**å¤šæ¨¡æ€è¾“å…¥çš„æ„ŸçŸ¥**\n\næœ‰æ•ˆçš„äººç±»è¿åŠ¨åˆæˆå§‹äºŽç†è§£å¤šæ¨¡æ€çº¿ç´¢ï¼Œå°±åƒäººç±»ä¾èµ–è§†è§‰ã€å¬è§‰å’Œè¯­è¨€è¿›è¡Œæ²Ÿé€šã€‚åœ¨æ•°å­—åº”ç”¨ä¸­ï¼Œè¿™ä¸€è¿‡ç¨‹å¯ä»¥å¤åˆ¶äººç±»æ”¶é›†å’Œå“åº”ä¿¡æ¯çš„å¤æ‚æ–¹å¼ï¼š\n\n* **è§†è§‰è¾“å…¥**ï¼šå›¾åƒå’Œè§†é¢‘æµæ•æ‰é¢éƒ¨è¡¨æƒ…ã€è§†çº¿æ–¹å‘å’Œæ‰‹åŠ¿ç­‰å…ƒç´ \n* **å¬è§‰è¾“å…¥**ï¼šéŸ³é¢‘ä¿¡å·æä¾›é‡è¦ä¿¡æ¯ï¼Œå¦‚è¯­è°ƒã€é‡éŸ³å’ŒèŠ‚å¥ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿè§£è¯»è¯­è¨€çš„æƒ…æ„ŸèƒŒæ™¯\n* **æ–‡æœ¬è¾“å…¥**ï¼šåŸºäºŽæ–‡æœ¬çš„æç¤ºæˆ–å¯¹è¯è®°å½•å¯ä»¥é€šè¿‡æä¾›è¯­ä¹‰ä¸Šä¸‹æ–‡æ¥æŒ‡å¯¼è™šæ‹Ÿå½¢è±¡çš„åŠ¨ä½œâ€”â€”äº†è§£æ­£åœ¨è®¨è®ºçš„å†…å®¹ä½¿è™šæ‹Ÿå½¢è±¡èƒ½å¤Ÿé€‚å½“åœ°å“åº”å¯¹è¯çš„ç»†å¾®å·®åˆ«\n\næ•´åˆè¿™äº›æ¨¡æ€åˆ›é€ äº†å¯¹å¯¹è¯çŽ¯å¢ƒçš„æ•´ä½“ç†è§£ï¼Œä¸ºç³»ç»Ÿå¦‚ä½•è§£é‡Šå’Œæ˜ å°„ä¸–ç•Œæä¾›äº†åŸºç¡€ã€‚\n\n**ä½¿ç”¨LLMsè¿›è¡Œè¿åŠ¨è§„åˆ’**\n\nåœ¨å¤šæ¨¡æ€AIä¸­ï¼Œ**äº¤äº’å±‚**â€”â€”é€šå¸¸ç”±å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰é©±åŠ¨â€”â€”å……å½“è™šæ‹Ÿå½¢è±¡çš„â€œå¤§è„‘â€ã€‚è¯¥å±‚å¤„ç†æ¥è‡ªæ„ŸçŸ¥é˜¶æ®µåˆæˆçš„å¤šæ¨¡æ€çº¿ç´¢ï¼Œç¡®å®šæœ€å…·ä¸Šä¸‹æ–‡ç›¸å…³æ€§çš„å“åº”ï¼Œå¹¶å°†å…¶ç¿»è¯‘ä¸ºè®¡åˆ’çš„åŠ¨ä½œæˆ–è¯­è¨€å“åº”ã€‚\n\nåŒæ—¶ä½¿ç”¨è¯­éŸ³å’Œè§†è§‰ç‰¹å¾ä½œä¸ºè¾“å…¥ä½¿æ¨¡åž‹èƒ½å¤Ÿå¤„ç†ï¼š\n\n* **ä¸Šä¸‹æ–‡è¿åŠ¨è§„åˆ’**ï¼šæ¨¡åž‹å¯ä»¥æ•æ‰å¯¹è¯çº¿ç´¢ï¼Œå°†å…¶åŒ¹é…åˆ°ä¸Šä¸‹æ–‡é€‚å½“çš„åŠ¨ä½œã€‚ä¾‹å¦‚ï¼Œå¦‚æžœè™šæ‹Ÿå½¢è±¡æ£€æµ‹åˆ°ç”¨æˆ·è¯­éŸ³ä¸­çš„çƒ­æƒ…ï¼Œå®ƒå¯èƒ½ä¼šé‡‡å–å¼€æ”¾ã€å¼•äººå…¥èƒœçš„å§¿æ€æˆ–é¢éƒ¨è¡¨æƒ…\n* **é¡ºåºäº¤äº’æŽ§åˆ¶**ï¼šæ¨¡åž‹å¯ä»¥å­¦ä¹ è§£é‡Šçº¿ç´¢åºåˆ—ï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†è½¬æŽ¥ã€ç§¯æžå€¾å¬æ‰‹åŠ¿å’Œåœé¡¿ç­‰ç»†å¾®å·®åˆ«ï¼Œè¿™äº›éƒ½æ˜¯è‡ªç„¶å¯¹è¯çš„é‡è¦ç»„æˆéƒ¨åˆ†\n\nä¹‹å‰çš„ç ”ç©¶å¦‚Zhou et al. \\[0]æˆ–Pereira et al. \\[1]ä¼šä»Žè¿™ä¸€å±‚è¾“å‡ºæ–‡æœ¬â€”â€”æƒ…æ„Ÿæ ‡ç­¾å¦‚â€œå¿«ä¹â€ï¼Œå¯ä»¥ç”¨äºŽæ¡ä»¶è¡¨è¾¾ç”Ÿæˆã€‚è¿™æ˜¯éžå¸¸æœ‰æŸçš„ï¼Œè¡¨è¾¾æ°¸è¿œä¸ä¼šä¸Žè¾“å‡ºè¯­éŸ³å®Œå…¨å¯¹é½ã€‚\n\nè¿åŠ¨è§„åˆ’ä¸­å¤šæ¨¡æ€æ€§çš„ç¾Žåœ¨äºŽè¾“å…¥å’Œè¾“å‡ºã€‚åœ¨è¾“å…¥æ–¹é¢ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨å¤§åž‹è¯­è¨€æ¨¡åž‹çš„ä¸–ç•ŒçŸ¥è¯†ï¼Œå³ä½¿å®ƒè¢«è®­ç»ƒä»¥å¯¹é½å¤šæ¨¡æ€æ ‡è®°ã€‚åœ¨è¾“å‡ºæ–¹é¢ï¼Œæˆ‘ä»¬å¯ä»¥å‡å°‘æœŸæœ›è¡Œä¸ºä¸Žæœ€ç»ˆæ¸²æŸ“è¾“å‡ºä¹‹é—´çš„ä¿¡æ¯æŸå¤±ã€‚\n\næ€»ä¹‹ï¼Œäº¤äº’å±‚ä½¿è™šæ‹Ÿå½¢è±¡èƒ½å¤Ÿå“åº”æ˜¾æ€§å’Œéšæ€§å¯¹è¯çº¿ç´¢ï¼Œå¼¥åˆå¤šæ¨¡æ€æ„ŸçŸ¥ä¸Žç±»äººäº¤äº’ä¹‹é—´çš„å·®è·ã€‚\n\n**è™šæ‹Ÿå½¢è±¡ç”Ÿæˆ**\n\nä¸ºäº†å®žçŽ°å¯Œæœ‰åŒç†å¿ƒçš„å¯¹è¯AIæˆ–äººç±»çº§åˆ«çš„ä¿¡æ¯æµï¼Œæ¸²æŸ“çš„åŠ¨ä½œå’Œååº”éœ€è¦è¶…è¶Šé™æ€çš„ã€é¢„å…ˆè§„åˆ’çš„åŠ¨ä½œã€‚ç›®æ ‡æ˜¯åˆ›å»ºä¸€ä¸ªèƒ½å¤Ÿå‡ ä¹Žçž¬æ—¶åœ°è§£é‡Šå’Œè°ƒæ•´å¾®å¦™å¯¹è¯çº¿ç´¢çš„ç³»ç»Ÿã€‚\n\nåœ¨è¿™ä¸ªèƒŒæ™¯ä¸‹ï¼Œ**è™šæ‹Ÿå½¢è±¡å±‚**å……å½“è¾“å‡ºæ¸²æŸ“æœºåˆ¶ã€‚å®ƒæŽ¥æ”¶äº¤äº’å±‚è§„åˆ’çš„åŠ¨ä½œï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºæµç•…çš„å®žæ—¶è¡Œä¸ºã€‚è¯¥å±‚ä¸“æ³¨äºŽ**ä½Žå»¶è¿Ÿå“åº”ç”Ÿæˆ**ï¼Œä¼˜å…ˆè€ƒè™‘æœŸæœ›åŠ¨ä½œä¸Žè§†è§‰/éŸ³é¢‘è¾“å‡ºä¹‹é—´çš„å¿«é€Ÿå’Œå‡†ç¡®å¯¹é½ã€‚\n\nä¸»è¦ç›®æ ‡å¯ä»¥æè¿°ä¸º**åŒæ­¥è¯­éŸ³å’ŒåŠ¨ä½œ**â€”â€”è™šæ‹Ÿå½¢è±¡å¿…é¡»åè°ƒé¢éƒ¨è¡¨æƒ…ã€è‚¢ä½“è¯­è¨€å’Œå”‡éƒ¨åŠ¨ä½œï¼Œåˆ©ç”¨å¬è§‰è¾“å‡ºå’Œè¡Œä¸ºä¿¡å·ï¼Œç¡®ä¿æ‰€æœ‰å…ƒç´ ä¿æŒåŒæ­¥ã€‚\n\nä¿æŒæ—¶é—´ä¸€è‡´æ€§å’ŒåŒæ­¥æ€§è‡³å…³é‡è¦ï¼Œå› ä¸ºä»»ä½•å»¶è¿Ÿæˆ–è¡Œä¸ºä¸åŒ¹é…éƒ½å¯èƒ½è¿…é€Ÿæ‰“ç ´æ²‰æµ¸æ„Ÿã€‚\n\n## åŸ¹è®­ä¸­çš„æŒ‘æˆ˜\n\nä¸€äº›è¡Œä¸šå’Œå­¦æœ¯ç•Œçš„æ´»è·ƒç ”å‘é¢†åŸŸåŒ…æ‹¬ï¼š\n\n* **è·¨æ¨¡æ€çš„æ ‡è®°å¯¹é½**ï¼šåœ¨ä¸å¤±åŽ»ä¸Šä¸‹æ–‡æˆ–è¯­ä¹‰æ„ä¹‰çš„æƒ…å†µä¸‹å¯¹è§†è§‰çº¿ç´¢å’ŒéŸ³é¢‘è¯­è°ƒç­‰æ¨¡æ€è¿›è¡Œå¯¹é½æ˜¯å¤æ‚çš„ï¼Œæ¨¡åž‹å¿…é¡»å­¦ä¹ å¦‚ä½•ä»¥ç»Ÿä¸€çš„æ–¹å¼è¡¨ç¤ºå®ƒä»¬ï¼Œä»¥ä¾¿æä¾›ä¸€è‡´çš„å“åº”\n* **å»¶è¿Ÿç®¡ç†**ï¼šå®žæ—¶å“åº”è¦æ±‚æ•´ä¸ªå¤šæ¨¡æ€ç®¡é“ä»¥ä½Žå»¶è¿Ÿè¿è¡Œï¼Œéšç€å¤æ‚æ€§çš„å¢žåŠ ï¼Œè¿™å˜å¾—å…·æœ‰æŒ‘æˆ˜æ€§\n* **ä¸ªæ€§å’Œè®°å¿†**ï¼šå¯¹äºŽè™šæ‹Ÿå½¢è±¡æ¥è¯´ï¼Œä¸€è‡´çš„ä¸ªæ€§ç‰¹å¾è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨é•¿æ—¶é—´çš„äº’åŠ¨ä¸­ã€‚é€‚å½“å¤„ç†è®°å¿†å’Œä¸ªæ€§å¯¹äºŽåœ¨æŸäº›ç”¨ä¾‹ä¸­ä¿æŒè¿žè´¯çš„å“åº”æ˜¯å¿…ä¸å¯å°‘çš„\n\n## å½“å‰çš„åº”ç”¨æ¡ˆä¾‹åŠæœªæ¥\n\né¦–å…ˆï¼Œä»¥ä¸‹æ˜¯æˆ‘ä»¬çœ‹åˆ°çš„ä¸€äº›å½“å‰åº”ç”¨æ¡ˆä¾‹çš„ç¤ºä¾‹ï¼š\n\n* **åŒ»ç–—ä¿å¥**ï¼šæƒ³è±¡ä¸€ä¸ªå¯Œæœ‰åŒæƒ…å¿ƒçš„è™šæ‹Ÿå¥åº·æ•™ç»ƒä½œä¸ºåŒ–èº«ï¼Œæä¾›æŒ‡å¯¼ï¼Œå®žæ—¶å“åº”ï¼Œå¹¶æ ¹æ®ç”¨æˆ·çš„æƒ…ç»ªè°ƒæ•´è¯­æ°”å’Œè¡¨æƒ…\n* **å®¢æˆ·æ”¯æŒ**ï¼šå®¢æˆ·æ”¯æŒåŒ–èº«å¯ä»¥è§£è¯»è¯­éŸ³æç¤ºã€è‚¢ä½“è¯­è¨€ï¼Œç”šè‡³é€šè¿‡å±å¹•å…±äº«æˆ–å®žæ—¶è§†é¢‘æŸ¥çœ‹ç”¨æˆ·çš„æŠ€æœ¯é—®é¢˜ã€‚å®ƒè¿˜å¯ä»¥æä¾›å¬èµ·æ¥ä½“è´´å’Œä¸ªæ€§åŒ–çš„å›žåº”ï¼Œå‡å°‘ç”¨æˆ·çš„æŒ«è´¥æ„Ÿ\n* **æ•™è‚²å·¥å…·**ï¼šå…·æœ‰å®žæ—¶äº’åŠ¨èƒ½åŠ›çš„å¯¼å¸ˆå¯ä»¥ä¸Žå­¦ç”Ÿäº’åŠ¨ï¼Œå±•çŽ°ä¸“æ³¨çš„æ‰‹åŠ¿ï¼Œå¹¶è°ƒèŠ‚è¡¨æƒ…ä»¥åŠ å¼ºé¼“åŠ±æˆ–çº æ­£\n\néšç€ç ”ç©¶çš„è¿›å±•ï¼Œè¿™äº›åº”ç”¨å°†ä¸æ–­æ‰©å±•ï¼Œä½¿æ•°å­—äººç±»èƒ½å¤Ÿåœ¨è¶Šæ¥è¶Šå¤æ‚ã€é«˜é£Žé™©çš„çŽ¯å¢ƒä¸­éƒ¨ç½²ã€‚äººç±»çº§åˆ«çš„å¯¹è¯åŒ–èº«è¿˜å°†è§£é”åŒæƒ…æ€§åº”ç”¨æ¡ˆä¾‹ä»¥åŠé«˜ä¿¡æ¯æµçš„äººæœºäº¤äº’ç•Œé¢ã€‚\n\nå¦‚æžœè§£å†³å¦‚æ¨¡æ€å¯¹é½ã€å»¶è¿Ÿå’Œä¸Šä¸‹æ–‡ä¸€è‡´æ€§ç­‰æŒ‘æˆ˜è®©ä½ æ„Ÿå…´è¶£â€”â€”æˆ‘ä»¬æ­£åœ¨æ‹›è˜ï¼è¯·æŸ¥çœ‹æˆ‘ä»¬çš„ç½‘ç«™ <https://tavus.io>\n\n**å‚è€ƒæ–‡çŒ®**\n\n\\[0] Zhou, Hao, Minlie Huang, Tianyang Zhang, Xiaoyan Zhu, and Bing Liu. â€œEmotional chatting machine: Emotional conversation generation with internal and external memory.â€ In *Proceedings of the AAAI conference on artificial intelligence*, vol. 32, no. 1\\. 2018\\.\n\n\\[1] Pereira, PatrÃ­cia, Helena Moniz, and Joao Paulo Carvalho. â€œDeep emotion recognition in textual conversations: A survey.â€ *Artificial Intelligence Review* 58, no. 1 (2025\\): 1â€“37\\.\n\n## åˆ«å¿˜äº†ç»™æˆ‘ä»¬ä½ çš„ ðŸ‘ !\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*2lvCls4yjxVMfZSR)\n\n"},{"lang":"zh","group":"blog","slug":"blog/multimodal-rag-with-gemini-pro-and-langchain-e4f74170420a","frontmatter":{"title":"ä½¿ç”¨ Gemini Pro å’Œ LangChain çš„å¤šæ¨¡å¼ RAG","meta_title":"ä½¿ç”¨ Gemini Pro å’Œ LangChain çš„å¤šæ¨¡å¼ RAG","description":"ä»‹ç»","date":"2024-11-08T00:41:44.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*m2C8wrrRvhELuDiYLv4YYQ.png","categories":["Programming","Machine Learning","Computer Vision"],"author":"Rifx.Online","tags":["Gemini","LangChain","RAG","Vertex","sneaker"],"draft":false,"slug":"blog/multimodal-rag-with-gemini-pro-and-langchain-e4f74170420a"},"content":"\n\n\n## ä»‹ç»\n\nåœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†æŽ¢ç´¢å°† [Gemini](https://deepmind.google/technologies/gemini/#introduction) Pro å’Œ Gemini Pro Vision ä¸Ž [LangChain](https://www.langchain.com/langchain) æ¡†æž¶é›†æˆï¼Œä»¥å®žçŽ°å¤šæ¨¡æ€ï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹ä¸ºå›¾åƒï¼‰æ£€ç´¢å¢žå¼ºç”Ÿæˆï¼ˆRAGï¼‰ã€‚è¿™ä¸ªç®€çŸ­çš„æ•™ç¨‹é€‚åˆåˆå­¦è€…å’Œç»éªŒä¸°å¯Œçš„ä»Žä¸šè€…ï¼Œä¸ä»…ä»¥ Google [AI Studio](https://aistudio.google.com/) ä½œä¸ºä¸»è¦çŽ¯å¢ƒå¥ å®šåŸºç¡€ï¼Œè¿˜æ— ç¼è¿‡æ¸¡åˆ°æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ [Google Cloudâ€™s Vertex AI](https://cloud.google.com/vertex-ai) é€‚åº”å’Œè¿›ä¸€æ­¥å¢žå¼ºè¿™äº›å®žçŽ°ã€‚\n\n## è®¾ç½®çŽ¯å¢ƒ\n\né¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦è®¾ç½®æˆ‘ä»¬çš„çŽ¯å¢ƒï¼Œä»¥ç¡®ä¿æˆ‘ä»¬æ‹¥æœ‰æ‰€æœ‰å¿…è¦çš„å·¥å…·å’Œåº“ã€‚\n\nä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦ Langchainã€Langchain Google Gen AI åŒ…ä»¥åŠç”¨äºŽ RAG çš„å‘é‡å­˜å‚¨åŒ…ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š\n\n```python\npip install â€” upgrade langchain langchain-google-genai â€œlangchain[docarray]â€ faiss-cpu\n```\n\nç„¶åŽï¼Œæ‚¨è¿˜éœ€è¦æä¾› Google AI Studio API å¯†é’¥ï¼Œä»¥ä¾¿æ¨¡åž‹è¿›è¡Œäº¤äº’ï¼š\n\n```python\nif \"GOOGLE_API_KEY\" not in os.environ:\n  os.environ[â€œGOOGLE_API_KEYâ€] = getpass.getpass(â€œProvide your Google API Keyâ€)\n```\n\nä¸ºäº†æ–¹ä¾¿ä½¿ç”¨ï¼Œæˆ‘è¿˜å†™äº†ä¸€ä¸ªç®€å•çš„å‡½æ•°ï¼Œæ˜¾ç¤ºæˆ‘æ­£åœ¨ä½¿ç”¨çš„å›¾åƒã€‚è¿™ä¸ªå‡½æ•°ç®€å•åœ°ä»Žæä¾›çš„ URL ä¸‹è½½å›¾åƒå¹¶æ˜¾ç¤ºé¢„è§ˆï¼š\n\n```python\ndef get_image(url, filename):\n  content = requests.get(url).content\n  with open(f'/content/{filename}.png', 'wb') as f:\n  f.write(content)\n  image = Image.open(f\"/content/{filename}.png\")\n  image.show()\n  return image\n```\n\n## ç®€å•çš„ LLM äº¤äº’\n\nè®©æˆ‘ä»¬ä»Žä¸€ä¸ªéžå¸¸ç®€å•çš„ LLM äº¤äº’å¼€å§‹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ç®€å•åœ°è°ƒç”¨ ChatGoogleGenerativeAI çš„ Gemini Pro æ¨¡åž‹ï¼Œå¹¶è°ƒç”¨ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š\n\n```python\nllm = ChatGoogleGenerativeAI(model=â€gemini-proâ€)\nresult = llm.invoke(\"Write a ballad about Gemini Pro in around 3 sentences.\")\nprint(result.content)\n```\n\nç»“æžœä½ ä¼šå¾—åˆ°ç±»ä¼¼è¿™æ ·çš„å†…å®¹ï¼š\n\n> åœ¨æ˜Ÿè¾°çš„é¢†åŸŸï¼ŒGemini Pro é—ªè€€ï¼Œ ä¸€é“å¤©ä½“çš„ç¯å¡”ï¼Œåˆ’å®šäº†ç•Œé™ï¼Œ æŒ‡å¼•ç€è§‚æ˜Ÿè€…ç©¿è¶Šå®‡å®™çš„è®¾è®¡ã€‚\n\nåŒæ ·ï¼Œä½ ä¹Ÿå¯ä»¥åœ¨èŠå¤©ç•Œé¢ä¸­ä½¿ç”¨å®ƒï¼Œé‡‡ç”¨ç³»ç»Ÿã€äººç±»æ¶ˆæ¯/å¯¹è¯æ ¼å¼ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š\n\n```python\nmodel = ChatGoogleGenerativeAI(model=â€gemini-proâ€, convert_system_message_to_human=True)\nprint(model([\n  SystemMessage(content=\"Answer only yes or no.\"),\n  HumanMessage(content=\"Is apple a fruit?\"),\n  ]).content)\n```\n\n## å¤šæ¨¡æ€ LLM\n\nåœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘å°†ä½¿ç”¨ä¸€ä¸ªéžå¸¸ç®€å•çš„ç”¨ä¾‹ï¼Œå‡è®¾æˆ‘æ˜¯ä¸€åè¿åŠ¨éž‹çˆ±å¥½è€…ï¼ŒåŸºæœ¬ä¸Šæƒ³è¦æ‰¾åˆ°åœ¨é™„è¿‘çš„æœ¬åœ°å•†åº—è´­ä¹°ç‰¹å®šè¿åŠ¨éž‹åž‹å·çš„æ–¹æ³•ã€‚ä¸ºæ­¤ï¼Œæˆ‘å‡†å¤‡äº†ä¸€ä¸ªè™šæ‹ŸçŸ¥è¯†åº“ï¼Œé‡Œé¢åŒ…å«äº†ä¸€äº›å…³äºŽæœ¬åœ°å•†åº—çš„è™šå‡ä¿¡æ¯ï¼Œä»¥åŠæŸäº›æµè¡Œè¿åŠ¨éž‹å“ç‰Œçš„è§„æ ¼ã€‚æœ‰è¶£çš„æ˜¯ï¼Œè¿™ä¸ªçŸ¥è¯†åº“ä¹Ÿæ˜¯é€šè¿‡ Gemini Pro ä½¿ç”¨ [Google Gemini](https://gemini.google.com/) èŠå¤©ç•Œé¢ç”Ÿæˆçš„ã€‚\n\nè®©æˆ‘ä»¬ä»Žä¸€å¼ ç¤ºä¾‹å›¾ç‰‡å¼€å§‹ï¼š\n\n```python\nimage = get_image(<image_url>, â€œnike3â€)\nplt.imshow(image)\nplt.show()\n```\n\nä½œä¸ºç¤ºä¾‹ï¼Œæˆ‘è€ƒè™‘è¿™å¼  [Nike](https://nike.com/) è¿åŠ¨éž‹çš„å›¾ç‰‡ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*dNFF95lOu1SeYHOn1vFnQQ.png)\n\nçŽ°åœ¨ï¼Œè®©æˆ‘ä»¬è°ƒç”¨ Gemini Pro Vision æ¨¡åž‹ï¼Œè¯¢é—®å®ƒå…³äºŽè¿™å¼ ç‰¹å®šå›¾ç‰‡çš„ä¸€äº›ä¿¡æ¯ã€‚ä¸ºæ­¤ï¼Œæ‚¨åªéœ€å°†æ¨¡åž‹åç§°æ›´æ”¹ä¸º *â€œgemini\\-pro\\-visionâ€*ã€‚\n\n```python\nllm = ChatGoogleGenerativeAI(model=â€gemini-pro-visionâ€)\nmessage = HumanMessage(\ncontent=[\n  {\n    \"type\": \"text\",\n    \"text\": \"What's in this image? provide full detail as possible.\",\n  }, # You can optionally provide text parts\n  {\"type\": \"image_url\", \"image_url\": image},\n])\nprint(\nllm.invoke([message]).content\n)\n```\n\næ‚¨å°†å¾—åˆ°å¦‚ä¸‹è¾“å‡ºï¼š\n\n> è¿™æ˜¯ä¸€ä¸ª Nike Air Max 95 è¿åŠ¨éž‹çš„äº§å“å›¾ç‰‡ï¼Œé¢œè‰²ä¸ºæ£•è‰²å°éº¦è‰²ã€‚éž‹é¢ç”±ç½‘å¸ƒå’Œéº‚çš®åˆ¶æˆï¼Œå¸¦æœ‰çš®é©æ³¥æŒ¡ã€‚ä¸­åº•ç”±æ³¡æ²«ææ–™åˆ¶æˆï¼ŒåŽè·Ÿæœ‰å¯è§çš„æ°”åž«å•å…ƒã€‚å¤–åº•ç”±æ©¡èƒ¶åˆ¶æˆï¼Œå…·æœ‰åŽå¤«æ ¼å›¾æ¡ˆä»¥å¢žå¼ºæŠ“åœ°åŠ›ã€‚\n\n*å…è´£å£°æ˜Žï¼šæ‰€æä¾›çš„æè¿°å¯èƒ½ä¸å‡†ç¡®ï¼Œåæ˜ çš„æ˜¯æ¨¡åž‹å¯¹å›¾åƒçš„è§£è¯»ï¼Œè€Œéžä¸Žä¹‹ç›¸å…³çš„äº‹å®žä¿¡æ¯ã€‚*\n\n## ä½¿ç”¨å¤šæ¨¡æ€çš„RAG\n\nçŽ°åœ¨ï¼Œè®©æˆ‘ä»¬æ·±å…¥äº†è§£å¦‚ä½•ä½¿ç”¨è¿™ç§å¤šæ¨¡æ€æ–¹æ³•æ‰§è¡ŒRAGã€‚é¦–å…ˆï¼Œè®©æˆ‘ä»¬ä¸ºè¿™ä¸ªRAGåˆ›å»ºä¸€ä¸ªä¿¡æ¯æºã€‚ä¸ºæ­¤ï¼Œæˆ‘å†™äº†ä¸€äº›å…³äºŽå‡ æ¬¾Nikeè¿åŠ¨éž‹çš„æ®µè½ä¿¡æ¯ï¼Œä»¥åŠä¸€äº›è™šæž„çš„å°¼æ³Šå°”æœ¬åœ°å•†åº—ä½ç½®ã€‚\n\n```python\nstore_information = â€œNike Air Max Plus sneakers. They feature a brown upper with a black Nike Swoosh logo on the side and a visible Air Max unit in the heel. The sole is white.\nHere are some more details about the Nike Air Max Plus:\nStyle: TN\nRelease date: January 1, 2017\nStyle code: 852630â€“300\nOriginal retail price: $150 USD\nThe Air Max Plus, also known as the TN, is a popular Nike running shoe that was first released in 1998. It is known for its unique design, which includes a gradient upper, visible Air Max units, and a wavy outsole. The TN has been a popular shoe among sneakerheads and casual wearers alike for over two decades.\nIt features a brown upper with a black Swoosh logo and a white sole. The shoe is currently available for resale on the StockX marketplace for an average price of around $150 USD.\nNike Air Max Plus Store Location: \"Kings Way, Kathmandu, Nepal\n\n...\n\n\"\n```\n\nç„¶åŽï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªLangchainé“¾ï¼Œå®ƒåŸºæœ¬ä¸Šæ ¹æ®æˆ‘ä»¬çŸ¥è¯†åº“ä¸­æä¾›çš„å›¾åƒæè¿°èŽ·å–å…³äºŽNikeæ¨¡åž‹çš„ä¿¡æ¯ä»¥åŠå¯ä»¥åœ¨å“ªé‡Œè´­ä¹°å®ƒã€‚\n\n```python\nllm_text = ChatGoogleGenerativeAI(model=â€gemini-proâ€)\ntemplate = \"\"\"\n```\n\n{context}\n\n```\n{information}\nProvide brief information and store location.\n\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\nrag_chain = (\n  {\"context\": retriever, \"information\": RunnablePassthrough()}\n  | prompt\n  | llm_text\n  | StrOutputParser()\n)\n```\n\nè¿™é‡Œéœ€è¦æ³¨æ„çš„æ˜¯*Gemini\\-Pro*å’Œ*Gemini\\-Pro\\-Vision*æ˜¯ä¸¤ä¸ªä¸åŒçš„æ¨¡åž‹ï¼Œæ‚¨éœ€è¦ä»¥ä¸åŒçš„æ–¹å¼è°ƒç”¨å®ƒä»¬ã€‚åœ¨ä¸Šé¢çš„ä»£ç ä¸­ï¼Œæˆ‘ä»¬è°ƒç”¨äº†Gemini Proæ–‡æœ¬æ¨¡åž‹ï¼Œè¯¥æ¨¡åž‹æ ¹æ®ç”±*gemini\\-pro\\-vision*æ¨¡åž‹ç”Ÿæˆçš„å›¾åƒæè¿°æ‰§è¡ŒRAGã€‚\n\nçŽ°åœ¨ï¼Œè®©æˆ‘ä»¬è®¾ç½®ä¸€ä¸ªå®Œæ•´çš„é“¾ï¼Œå®ƒé¦–å…ˆç”Ÿæˆå›¾åƒæè¿°ï¼Œç„¶åŽä½¿ç”¨ä¸Šè¿°é“¾è¿›è¡ŒRAGã€‚\n\n```python\nllm_vision = ChatGoogleGenerativeAI(model=â€gemini-pro-visionâ€, temperature=0.0)\nfull_chain = (\n  RunnablePassthrough() | llm_vision | StrOutputParser() | rag_chain\n)\n```\n\n## æ‰§è¡Œ RAG\n\nçŽ°åœ¨ï¼Œè®©æˆ‘ä»¬å¯¹åˆšåˆšè®¾ç½®çš„å†…å®¹è¿›è¡Œä¸€äº›æµ‹è¯•ã€‚é¦–å…ˆï¼ŒèŽ·å–å¦ä¸€å¼ å›¾åƒä½œä¸ºæ ·æœ¬\n\n```python\nimage = get_image(url_3, â€œnike3â€)\nplt.imshow(image)\nplt.show()\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kPkfo2FKnrUR2tC18VMpjg.png)\n\nç„¶åŽï¼Œè®©æˆ‘ä»¬è°ƒç”¨æˆ‘ä»¬çš„ RAGï¼š\n\n```python\nmessage = HumanMessage(\n  content=[\n    {\n      \"type\": \"text\",\n      \"text\": \"æä¾›æœ‰å…³ç»™å®šè¿åŠ¨éž‹çš„å“ç‰Œå’Œåž‹å·çš„ä¿¡æ¯ã€‚\",\n    }, # æ‚¨å¯ä»¥é€‰æ‹©æ€§åœ°æä¾›æ–‡æœ¬éƒ¨åˆ†\n    {\"type\": \"image_url\", \"image_url\": image},\n  ])\n```\n\nçŽ°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬å¾—åˆ°äº†ä»€ä¹ˆï¼š\n\n```python\nresult = full_chain.invoke([message])\ndisplay(Markdown(result))\n```\n\nä½œä¸ºè¾“å‡ºï¼Œæˆ‘ä»¬å°†å¾—åˆ°ç±»ä¼¼äºŽä»¥ä¸‹å†…å®¹çš„ç»“æžœï¼Œè¿™åŸºäºŽæˆ‘ä»¬çš„è™šæž„ä¿¡æ¯æ¥æºï¼š\n\n> **Nike Offcourt Slide**è½¯è´¨ä¸€ä½“å¼éž‹é¢èˆ’é€‚çš„æ³¡æ²«ä¸­åº•è€ç”¨çš„æ©¡èƒ¶å¤–åº•æä¾›å¤šç§é¢œè‰²é€‰æ‹©\n\n> **å•†åº—ä½ç½®ï¼š** å°¼æ³Šå°”ï¼Œå·´å…‹å¡”å¸ƒå°”\n\n## ä½¿ç”¨ Vertex AI æ¨¡åž‹\n\né™¤äº†ä½¿ç”¨ Google AI Studio æ¨¡åž‹å¤–ï¼Œæ‚¨è¿˜å¯ä»¥ä½¿ç”¨ Google Cloud çš„ Vertex AI Gemini Pro æ¨¡åž‹ã€‚ä¸ºæ­¤ï¼Œæ‚¨é¦–å…ˆéœ€è¦ä¸ºæ‚¨çš„äº‘çŽ¯å¢ƒå®‰è£…ä¸Ž Vertex AI ç›¸å…³çš„åŒ…å’Œ Langchainï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š\n\n```python\npip install â€” upgrade google-cloud-aiplatform langchain-google-vertexai\n```\n\nç„¶åŽï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è®¾ç½®ä¸Žæ‚¨çš„äº‘é¡¹ç›®ç›¸å…³çš„å¿…è¦é…ç½®ï¼š\n\n```python\ngcloud init\n```\n\næŽ¥ä¸‹æ¥ï¼Œæ‚¨å¯ä»¥å°† Vertex AI æ¨¡åž‹ç”¨äºŽå¤šæ¨¡æ€ç”¨ä¾‹ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š\n\n```python\nfrom langchain_google_vertexai import VertexAI\nfrom langchain_google_vertexai import VertexAIEmbeddings\n\nmodel_vision = VertexAI(model_name=\"gemini-1.0-pro-vision-001\")\nmodel_text = VertexAI(model_name=\"gemini-1.0-pro-001\")\n```\n\n## ç»“è®º\n\nåœ¨è¿™ä¸ªç®€çŸ­çš„æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬æŽ¢è®¨äº†å¦‚ä½•å°† Gemini Pro å’Œ Gemini Pro vision ä¸Ž LangChain ç»“åˆä½¿ç”¨ï¼Œä»¥å®žçŽ°å¤šæ¨¡æ€ RAG åº”ç”¨ç¨‹åºã€‚\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/o1-preview-vs-claude-3-5-sonnet-comparing-top-llms-d68734b53c93","frontmatter":{"title":"o1-preview ä¸Ž claude-3.5-sonnetï¼šæ¯”è¾ƒé¡¶çº§æ³•å­¦ç¡•å£«","meta_title":"o1-preview ä¸Ž claude-3.5-sonnetï¼šæ¯”è¾ƒé¡¶çº§æ³•å­¦ç¡•å£«","description":"äº†è§£ OpenAI çš„ o1 é¢„è§ˆç‰ˆä¸Ž Claude 3.5 Sonnet åœ¨æ€§èƒ½ã€é€Ÿåº¦å’ŒåŠŸèƒ½æ–¹é¢çš„æ¯”è¾ƒã€‚","date":"2024-10-27T13:58:01.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kTWAcpRdOpsrFIDZjjjr7Q.jpeg","categories":["Programming","Machine Learning","Generative AI"],"author":"Rifx.Online","tags":["o1-preview","Claude","throughput","latency","reasoning"],"draft":false,"slug":"blog/o1-preview-vs-claude-3-5-sonnet-comparing-top-llms-d68734b53c93"},"content":"\n\n\nä»Šå¤©ï¼ˆ2024å¹´9æœˆ12æ—¥ï¼‰ï¼ŒOpenAI å‘å¸ƒäº†å…¶æœ€æ–°çš„è¯­è¨€æ¨¡åž‹ o1-previewã€‚è¿™ä¸ªå…ˆè¿›çš„æ¨¡åž‹ç»è¿‡è®¾è®¡ï¼Œèƒ½å¤Ÿåœ¨ç”Ÿæˆå“åº”ä¹‹å‰æŠ•å…¥æ›´å¤šæ—¶é—´è¿›è¡Œå¤„ç†ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°åº”å¯¹å¤æ‚ä»»åŠ¡ï¼Œå¹¶åœ¨ç§‘å­¦ã€ç¼–ç å’Œæ•°å­¦ç­‰é¢†åŸŸè§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚\n\nåœ¨è¿™ç¯‡åšå®¢æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥åˆ†æž o1-previewï¼Œå¹¶å°†å…¶ä¸Žä¹‹å‰è¢«è®¤ä¸ºæ˜¯æœ€å…ˆè¿›æ¨¡åž‹ä¹‹ä¸€çš„ Claude 3.5 Sonnet è¿›è¡Œæ¯”è¾ƒã€‚\n\n\n\n## æ¯”è¾ƒæ–¹æ³•è®º\n\næˆ‘ä»¬çš„åˆ†æžåˆ©ç”¨äº† [Keywords AI çš„ LLM playground](https://docs.keywordsai.co/features/prompt/model-playground)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ”¯æŒè¶…è¿‡ 200 ç§è¯­è¨€æ¨¡åž‹å¹¶æä¾›å‡½æ•°è°ƒç”¨åŠŸèƒ½çš„å¹³å°ã€‚æˆ‘ä»¬å°†æŽ¢è®¨ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š\n\n* åŸºæœ¬æ¯”è¾ƒ\n* åŸºå‡†æ¯”è¾ƒ\n* å¤„ç†é€Ÿåº¦\n* è¯„ä¼°æŒ‡æ ‡\n* å»ºè®®çš„ä½¿ç”¨æ¡ˆä¾‹\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*yc171ikejtBy_o11.jpeg)\n\n## åŸºæœ¬æ¯”è¾ƒ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*z2FrS_AVig7Y6eU_.jpeg)\n\næ³¨æ„ï¼šo1-preview ä¸æ”¯æŒæµå¼ä¼ è¾“ã€å‡½æ•°è°ƒç”¨å’Œç³»ç»Ÿæ¶ˆæ¯ã€‚\n\n## åŸºå‡†æ¯”è¾ƒ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Bx_vAvFc9DAD0cZA.jpeg)\n\nO1-preview åœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­éƒ½ä¼˜äºŽ Claude 3.5 Sonnetã€‚æœ€å°çš„å·®è·å‡ºçŽ°åœ¨ MMLUï¼ˆä¸€èˆ¬çŸ¥è¯†ï¼‰ä¸­ã€‚GPQA Diamond æµ‹è¯•ç ”ç©¶ç”Ÿæ°´å¹³çš„æŽ¨ç†ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½å·®å¼‚ã€‚MATH åŸºå‡†æ­ç¤ºäº†æœ€å¤§çš„å·®è·ï¼Œçªæ˜¾äº† o1-preview çš„é«˜çº§æ•°å­¦èƒ½åŠ›ã€‚è¿™äº›ç»“æžœè¡¨æ˜Žï¼Œo1-preview åœ¨å¤æ‚æŽ¨ç†å’Œå„ä¸ªé¢†åŸŸçš„é—®é¢˜è§£å†³æ–¹é¢æœ‰äº†æ˜¾è‘—æ”¹å–„ã€‚\n\n## é€Ÿåº¦æ¯”è¾ƒ\n\nO1-preview çš„æ€è€ƒå’Œå“åº”æ—¶é—´æ¯”å…¶ä»– LLM æ›´é•¿ã€‚è™½ç„¶ç›´æŽ¥çš„é€Ÿåº¦æ¯”è¾ƒå¯èƒ½å¹¶ä¸å®Œå…¨å…¬å¹³ï¼Œä½†æµ‹è¯• o1-preview çš„é€Ÿåº¦è‡³å…³é‡è¦ã€‚è¿™äº›ä¿¡æ¯å¸®åŠ©å¼€å‘è€…æ›´å¥½åœ°ç†è§£ o1-preview çš„èƒ½åŠ›ï¼Œå¹¶åˆ¤æ–­å®ƒæ˜¯å¦é€‚åˆä»–ä»¬çš„é¡¹ç›®ã€‚æ³¨æ„ï¼šç”±äºŽ o1-preview ä¸æ”¯æŒæµå¼ä¼ è¾“ï¼Œæˆ‘ä»¬å·²ä¸ºä¸¤ä¸ªæ¨¡åž‹ç¦ç”¨æµå¼ä¼ è¾“ã€‚å› æ­¤ï¼Œæ— æ³•æµ‹é‡é¦–æ¬¡ä»¤ç‰Œæ—¶é—´ï¼ˆTTFTï¼‰ã€‚\n\n## å»¶è¿Ÿ\n\næˆ‘ä»¬çš„æµ‹è¯•æ¶‰åŠæ¯ä¸ªæ¨¡åž‹æ•°ç™¾ä¸ªè¯·æ±‚ï¼Œæ­ç¤ºäº†æ˜¾è‘—çš„å·®å¼‚ã€‚Claude 3.5 Sonnet çš„å¹³å‡å»¶è¿Ÿä¸º 18.3 ç§’/è¯·æ±‚ï¼Œè€Œ o1-preview çš„å¹³å‡å»¶è¿Ÿä¸º 39.4 ç§’/è¯·æ±‚ã€‚o1-preview æ˜¾è‘—æ›´é•¿çš„å»¶è¿Ÿæ˜¯ç”±äºŽå…¶å»¶é•¿çš„æ€è€ƒå’ŒæŽ¨ç†è¿‡ç¨‹ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*2PMkgPVuylFxwfIa.jpeg)\n\n## åžåé‡ï¼ˆæ¯ç§’ä»¤ç‰Œæ•°ï¼‰\n\nå°½ç®¡å»¶è¿Ÿè¾ƒé«˜ï¼Œo1-previewçš„åžåé‡æ›´ä¸ºå‡ºè‰²ã€‚o1-previewç”Ÿæˆ92.94ä¸ªä»¤ç‰Œ/ç§’ï¼Œè€ŒClaude 3.5 Sonnetç”Ÿæˆ74.87ä¸ªä»¤ç‰Œ/ç§’ã€‚è¿™è¡¨æ˜Žo1-previewè¾ƒé•¿çš„ç”Ÿæˆæ—¶é—´ä¸»è¦æ˜¯ç”±äºŽå…¶åˆå§‹å¤„ç†é˜¶æ®µï¼Œè€Œéžä»¤ç‰Œç”Ÿæˆé€Ÿåº¦ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*wxqpnwZhl9pnbw8y.jpeg)\n\n## æ€§èƒ½æ¯”è¾ƒ\n\næˆ‘ä»¬åœ¨[Keywords AIå¹³å°](https://keywordsai.co/)ä¸Šè¿›è¡Œäº†è¯„ä¼°æµ‹è¯•ã€‚è¯„ä¼°åŒ…æ‹¬ä¸‰ä¸ªéƒ¨åˆ†ï¼š\n\n* **ç¼–ç ä»»åŠ¡**ï¼šä¸¤ä¸ªæ¨¡åž‹æˆåŠŸå®Œæˆäº†å‰ç«¯å’ŒåŽç«¯å¼€å‘ä»»åŠ¡ã€‚O1-previewåœ¨å¤„ç†è¾ƒé•¿ä¸Šä¸‹æ–‡æ—¶è¡¨çŽ°æ›´ä½³ï¼Œèƒ½å¤Ÿåœ¨ç¬¬ä¸€æ¬¡å°è¯•ä¸­æ›´æœ‰æ•ˆåœ°è¯†åˆ«å’Œè§£å†³bugã€‚å®ƒè¿˜å±•çŽ°äº†æ›´å…¨é¢çš„ä»£ç åˆ†æžèƒ½åŠ›ã€‚\n* **é€»è¾‘æŽ¨ç†**ï¼šO1-previewåœ¨æŽ¨ç†ä»»åŠ¡ä¸­è¡¨çŽ°å‡ºè‰²ã€‚å®ƒçš„æ€ç»´è¿‡ç¨‹ä¸Žäººç±»è®¤çŸ¥éžå¸¸ç›¸ä¼¼ã€‚è™½ç„¶Claude 3.5 Sonnetåœ¨å¤§å¤šæ•°é—®é¢˜ä¸Šè¡¨çŽ°è‰¯å¥½ï¼Œä½†o1-previewå§‹ç»ˆèƒ½å¤Ÿè§£å†³å¤æ‚çš„æŽ¨ç†æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å›½é™…æ•°å­¦å¥¥æž—åŒ¹å…‹ï¼ˆIMOï¼‰çº§åˆ«çš„é—®é¢˜ã€‚\n* **å†™ä½œä»»åŠ¡**ï¼šä¸¤ä¸ªæ¨¡åž‹åœ¨å†™ä½œä»»åŠ¡ä¸Šè¡¨çŽ°éžå¸¸å‡ºè‰²ã€‚å®ƒä»¬å±•çŽ°äº†æ’°å†™çœŸå®žã€ä¸ªæ€§åŒ–çš„å†·é‚®ä»¶ä»¥åŠç®€æ´ä¸”æœ‰æ„ä¹‰çš„åšå®¢æ–‡ç« çš„èƒ½åŠ›ã€‚\n\n## æ¨¡åž‹æŽ¨è\n\no1-preview\n\n* **æœ€ä½³é€‰æ‹©ï¼š** é€‚ç”¨äºŽæ•°å­¦ã€ç¼–ç å’Œç‰©ç†å­¦ä¸­çš„å¤æ‚é—®é¢˜è§£å†³ã€‚ç‰¹åˆ«é€‚åˆå¤„ç†æŒ‘æˆ˜æ€§ä»»åŠ¡çš„ç ”ç©¶äººå‘˜ã€‚\n* **ä¸é€‚åˆï¼š** éœ€è¦å¿«é€Ÿå“åº”æ—¶é—´æˆ–ä¸¥é‡ä¾èµ–ç³»ç»Ÿæç¤ºçš„AIåº”ç”¨ã€‚ç”±äºŽç¼ºä¹æµåª’ä½“æ”¯æŒï¼Œä¸é€‚ç”¨äºŽè¯­éŸ³AIåº”ç”¨ã€‚\n\nClaude 3.5 Sonnet\n\n* **æœ€ä½³é€‰æ‹©ï¼š** é€‚ç”¨äºŽå¤§å¤šæ•°éœ€è¦é—®é¢˜è§£å†³èƒ½åŠ›å’Œé«˜è´¨é‡å†…å®¹ç”Ÿæˆçš„AIåº”ç”¨ã€‚\n* **ä¸é€‚åˆï¼š** è¯­éŸ³AIåº”ç”¨æˆ–å¯¹é¢„ç®—é™åˆ¶ä¸¥æ ¼ã€éœ€è¦è¾ƒä½Žè¿è¥æˆæœ¬çš„é¡¹ç›®ã€‚\n\n## å¦‚ä½•å°† o1-preview é›†æˆåˆ°æ‚¨çš„ AI åº”ç”¨ä¸­\n\nè¦å°† o1-preview é›†æˆåˆ°æ‚¨çš„ AI åº”ç”¨ä¸­ï¼Œåªéœ€è®¿é—® Keywords AI æ¨¡åž‹é¡µé¢å¹¶æ‰¾åˆ°â€œæŸ¥çœ‹ä»£ç â€æŒ‰é’®ã€‚ç‚¹å‡»æ­¤æŒ‰é’®ä»¥å¤åˆ¶æä¾›çš„ä»£ç ç‰‡æ®µï¼Œç„¶åŽå°†å…¶ç›´æŽ¥ç²˜è´´åˆ°æ‚¨çš„ä»£ç åº“ä¸­ã€‚é€šè¿‡è¿™ä¸ªç®€å•çš„è¿‡ç¨‹ï¼Œæ‚¨å°†èƒ½å¤Ÿåœ¨é¡¹ç›®ä¸­åˆ©ç”¨ o1-preview çš„å¼ºå¤§åŠŸèƒ½ï¼Œä½¿æ‚¨èƒ½å¤Ÿè½»æ¾åº”å¯¹å¤æ‚é—®é¢˜å¹¶ç”Ÿæˆé«˜è´¨é‡å†…å®¹ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*XyQ9QiI7TN8Uc5Jp.jpeg)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*t8fEYlEs13eM7D28lVbtIw.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*yhu9y5ixNuxeFVe1.png)\n\næ­¤æ•…äº‹å‘å¸ƒåœ¨ [Generative AI](https://generativeai.pub/)ã€‚è¯·åœ¨ [LinkedIn](https://www.linkedin.com/company/generative-ai-publication) ä¸Šä¸Žæˆ‘ä»¬è”ç³»ï¼Œå¹¶å…³æ³¨ [Zeniteq](https://www.zeniteq.com/)ï¼Œä»¥ä¾¿èŽ·å–æœ€æ–°çš„ AI èµ„è®¯ã€‚\n\nè®¢é˜…æˆ‘ä»¬çš„ [newsletter](https://www.generativeaipub.com/) å’Œ [YouTube](https://www.youtube.com/@generativeaipub) é¢‘é“ï¼ŒåŠæ—¶äº†è§£ç”Ÿæˆ AI çš„æœ€æ–°æ¶ˆæ¯å’ŒåŠ¨æ€ã€‚è®©æˆ‘ä»¬å…±åŒå¡‘é€  AI çš„æœªæ¥ï¼\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*PelNtaNaEVDWgMWr.png)\n\n"},{"lang":"zh","group":"blog","slug":"blog/openai-01-preview-secrets-99-of-people-dont-know-b0c5e4bb4f76","frontmatter":{"title":"OpenAI 01-é¢„è§ˆâ€Šâ€”â€Š99% çš„äººä¸çŸ¥é“çš„ç§˜å¯†","meta_title":"OpenAI 01-é¢„è§ˆâ€Šâ€”â€Š99% çš„äººä¸çŸ¥é“çš„ç§˜å¯†","description":"å¦‚ä½•å……åˆ†åˆ©ç”¨ 01-preview","date":"2024-11-01T03:58:01.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*wRAXNmhEzkGNagMl5Papxg.jpeg","categories":["Programming","Machine Learning","Technology/Web"],"author":"Rifx.Online","tags":["OpenAI","01-preview","iterative","problem-solving","planning"],"draft":false,"slug":"blog/openai-01-preview-secrets-99-of-people-dont-know-b0c5e4bb4f76"},"content":"\n### å¦‚ä½•å……åˆ†åˆ©ç”¨01\\-preview\n\nè‡ªä»Ž01\\-previewå‘å¸ƒä»¥æ¥ï¼Œæˆ‘ä¸€ç›´åœ¨çŽ©å®ƒã€‚\n\næˆ‘éžå¸¸å–œæ¬¢å®ƒï¼\n\næˆ‘ç”šè‡³åœ¨æˆ‘çš„æ–°[**AIå¢žé•¿é»‘å®¢è¯¾ç¨‹**](https://aigrowthguys.com/growth-hacking-course-sign-up/)ä¸­æ•™æŽˆå®ƒã€‚\n\næˆ‘å¾ˆé«˜å…´åˆ†äº«ä¸€äº›å…³äºŽå¦‚ä½•å……åˆ†åˆ©ç”¨å®ƒçš„å…³é”®è§è§£ã€‚\n\n\n\nå¤§å¤šæ•°äººå¯¹01\\-previewçš„å·¥ä½œåŽŸç†ä¸€æ— æ‰€çŸ¥ã€‚\n\né¦–å…ˆï¼Œå®ƒä¸ä»…ä»…æ˜¯ä¸€ä¸ªâ€œæ€è€ƒâ€æ¨¡åž‹ã€‚\n\nåœ¨æ‚¨èƒ½å¤Ÿå……åˆ†åˆ©ç”¨å®ƒä¹‹å‰ï¼Œæ‚¨éœ€è¦äº†è§£ä¸€äº›å®ƒçš„å·¥ä½œåŽŸç†ã€‚\n\nå¦‚æžœæ‚¨æ²¡æœ‰ä»˜è´¹çš„Mediumè´¦æˆ·ï¼Œå¯ä»¥åœ¨[**è¿™é‡Œ**](https://readmedium.com/openai-01-preview-secrets-99-of-people-dont-know-b0c5e4bb4f76?sk=12140ffad09d922bc00a8a4aa312a286)å…è´¹é˜…è¯»ã€‚\n\nðŸ‘‰ æ³¨å†Œæˆ‘ä»¬çš„å…è´¹5å¤©ç”µå­é‚®ä»¶è¯¾ç¨‹ï¼ŒåŠ©åŠ›æˆé•¿ðŸš€å¹¶åœ¨AIæ—¶ä»£èµšå–ðŸ’²\n\n## OpenAI 01\\-preview å¦‚ä½•å·¥ä½œï¼Ÿ\n\n01\\-preview å¹¶ä¸æ˜¯çœŸæ­£çš„æ–°æ¨¡åž‹ã€‚\n\nå®ƒç»“åˆäº†å…¶ä»–æ¨¡åž‹å’Œä¸€ä¸ªâ€œç³»ç»Ÿæç¤ºâ€ï¼Œå‘Šè¯‰å®ƒåœ¨è¾“å‡ºå“åº”ä¹‹å‰è¿›è¡Œå¤šæ¬¡è¿­ä»£ã€‚\n\næ‰€æœ‰å…¶ä»–æ¨¡åž‹éƒ½æ˜¯é€šè¿‡æä¾›æ¨¡åž‹æƒ³åˆ°çš„ç¬¬ä¸€ä¸ªå“åº”æ¥å·¥ä½œçš„ã€‚\n\n01\\-preview çš„è®¾è®¡ç›®çš„æ˜¯åœ¨æœ€ç»ˆç­”æ¡ˆå‡ºæ¥ä¹‹å‰è¿›è¡Œè§„åˆ’å’Œå®žéªŒã€‚\n\nä¸€ä¸ªä¾‹å­ä¼šæœ‰æ‰€å¸®åŠ©ã€‚\n\n> æƒ³è±¡ä¸€ä¸‹ï¼Œä½ å‘Šè¯‰ GPT\\-4o å†™ä¸€ä¸ªæ°å¥½ 80 ä¸ªå•è¯çš„è¿žè´¯æ®µè½ï¼Œå¹¶ä¸”â€œtomatoâ€è¿™ä¸ªè¯æ˜¯ç¬¬ 4 ä¸ªã€ç¬¬ 19 ä¸ªå’Œç¬¬ 72 ä¸ªå•è¯ã€‚\n\nGPT\\-4oï¼ˆä»¥åŠæ‰€æœ‰å…¶ä»–æ¨¡åž‹ï¼‰åœ¨è¿™ä¸ªä»»åŠ¡ä¸Šä¼šå¤±è´¥ï¼Œå› ä¸ºä»…ä»…åå‡ºç¬¬ä¸€ä¸ªæƒ³åˆ°çš„ç­”æ¡ˆå¤ªå›°éš¾äº†ã€‚\n\nè¿™ç§ç±»åž‹çš„é—®é¢˜éœ€è¦å®žéªŒã€‚\n\næƒ³æƒ³å¦‚æžœä½ è¢«èµ‹äºˆäº†åŒæ ·çš„ä»»åŠ¡ã€‚\n\nä½ éœ€è¦â€œçŽ©å¼„â€è¿™ä¸ªä»»åŠ¡ï¼Œè¯•å›¾å°†â€œtomatoâ€è¿™ä¸ªè¯æ”¾åœ¨è¿™äº›ä½ç½®ä¸Šï¼Œä»¥ä¸€ç§åˆç†çš„æ–¹å¼ã€‚\n\nä½ ä¸èƒ½ä»…ä»…å¼€å§‹å†™ä½œï¼Œç„¶åŽçœ‹çœ‹ä¼šå‘ç”Ÿä»€ä¹ˆã€‚\n\nä½ ä¼šæ„è¯†åˆ°ä½ éœ€è¦è°ƒæ•´ä¸€äº›å¥å­å’Œå•è¯ï¼Œä»¥ä¾¿å°†â€œtomatoâ€è¿™ä¸ªè¯æ”¾è¿›åŽ»ã€‚\n\næ­¤å¤–ï¼Œå½“ä½ æŽ¥è¿‘ 80 ä¸ªå•è¯æ—¶ï¼Œä½ éœ€è¦è®¡åˆ’å¦‚ä½•å‡†ç¡®åœ°åœåœ¨è¿™ä¸ªæ•°å­—ä¸Šã€‚ä¾‹å¦‚ï¼Œä½ å¯èƒ½å¸Œæœ›å›žåŽ»åˆ é™¤ç¬¬ä¸€å¥ä¸­çš„ä¸€ä¸ªå¤šä½™å•è¯ã€‚\n\n01\\-preview èƒ½å¤Ÿåšåˆ°è¿™ä¸€ç‚¹çš„åŽŸå› åœ¨äºŽå®ƒçš„â€œæ€è€ƒâ€æ–¹å¼ã€‚\n\nå®ƒé¦–å…ˆä¼šå°†é—®é¢˜åˆ†è§£ï¼Œå¹¶è¯´ä¸€äº›ç±»ä¼¼äºŽâ€œæƒ³å‡ºä¸€ä¸ªè§£å†³è¿™ä¸ªé—®é¢˜çš„è®¡åˆ’â€çš„è¯ã€‚\n\nç„¶åŽï¼Œå®ƒä¼šå†™å‡ºä¸€ä¸ªå¤§è‡´çš„åˆæ­¥çŒœæµ‹ï¼ˆå¯èƒ½ä½¿ç”¨ GPT\\-4oï¼‰ã€‚\n\næŽ¥ç€ï¼Œå®ƒä¼šå¯¹è‡ªå·±è¯´ï¼šâ€œé‡æ–°é˜…è¯»é—®é¢˜ï¼Œçœ‹çœ‹æ˜¯å¦å¯ä»¥è¿›è¡Œä»»ä½•è°ƒæ•´æˆ–ä¿®æ”¹â€ã€‚\n\nç„¶åŽå®ƒä¼šè¯´ï¼šâ€œå†æ£€æŸ¥ä¸€éï¼Œçœ‹çœ‹ä½ çš„å›žç­”æ˜¯å¦å®Œç¾Žã€‚å¦‚æžœæ˜¯ï¼Œå°±å±•ç¤ºå‡ºæ¥ï¼›å¦‚æžœä¸æ˜¯ï¼Œç»§ç»­è°ƒæ•´â€ã€‚\n\nç„¶åŽå®ƒä¼šè¯´ï¼šâ€œé‡å¤è¿™ä¸ªè¿‡ç¨‹ï¼Œç›´åˆ°ä½ çš„ç­”æ¡ˆ 100% å®Œç¾Žã€‚å§‹ç»ˆè®°å¾—åœ¨å±•ç¤ºæœ€ç»ˆç­”æ¡ˆä¹‹å‰è¿›è¡ŒåŒé‡æ£€æŸ¥â€ã€‚\n\nä¾‹å¦‚ï¼Œç¬¬ä¸€æ¬¡å“åº”çš„ç¬¬ä¸€å¥å¯èƒ½æ˜¯è¿™æ ·çš„ã€‚\n\nâ€œSandy picked a red tomato from her garden.â€\n\nç„¶åŽ 01\\-preview ä¼šå°†å…¶æ›´æ”¹ä¸ºï¼šâ€œSandy picked a tomato from her gardenâ€ã€‚\n\nè¿™æ ·ï¼Œå®ƒå°±æˆåŠŸåœ°å°†â€œtomatoâ€è¿™ä¸ªè¯ä»Žç¬¬ 5 ä¸ªå•è¯ç§»åŠ¨åˆ°äº†ç¬¬ 4 ä¸ªå•è¯ã€‚\n\nå®ƒä¼šé€šè¿‡ä¸Žè‡ªå·±è¿›è¡Œå†…éƒ¨å¯¹è¯ä¸æ–­è¿›è¡Œè°ƒæ•´ã€‚\n\n## å¦‚ä½•å……åˆ†åˆ©ç”¨ 01\\-previewï¼Ÿ\n\nçŽ°åœ¨æ‚¨å¯¹ 01\\-preview çš„â€œæ€ç»´â€æœ‰äº†ä¸€å®šäº†è§£ï¼Œå¯ä»¥å¼€å§‹ç†è§£å¦‚ä½•å……åˆ†åˆ©ç”¨å®ƒã€‚\n\næ‚¨éœ€è¦å°†è‡ªå·±çš„é—®é¢˜åˆ†ä¸ºéœ€è¦â€œæ€è€ƒâ€çš„é—®é¢˜å’Œä¸éœ€è¦çš„ã€‚\n\nè®¸å¤šé—®é¢˜ä¸éœ€è¦æ¨¡åž‹è¿›è¡Œâ€œæ€è€ƒâ€ã€‚\n\nä¾‹å¦‚ï¼Œå¦‚æžœæ‚¨å‘Šè¯‰å®ƒä¸ºæ‚¨å†™ä¸€ä¸ªå…³äºŽä¸€ä¸ªåå« Sandy çš„å¥³å­©å’Œå¥¹çš„ç•ªèŒ„èŠ±å›­çš„æœ‰è¶£æ•…äº‹ï¼Œé‚£ä¹ˆæ‚¨å°±ä¸éœ€è¦ä½¿ç”¨ 01\\-previewã€‚\n\n**ä¸ºä»€ä¹ˆä¸å‘¢ï¼Ÿ**\n\nå› ä¸ºçº¦æŸæ¡ä»¶å¾ˆå°‘ã€‚\n\næœ‰å¾ˆå¤šæ–¹æ³•å¯ä»¥åšåˆ°è¿™ä¸€ç‚¹ã€‚å®ƒæœ¬è´¨ä¸Šæ˜¯å¼€æ”¾å¼çš„ã€‚\n\nè¿™ä¸ªæ•…äº‹ä¸éœ€è¦æœ‰ç‰¹å®šçš„é•¿åº¦ã€‚\n\næ¨¡åž‹å¯ä»¥ç›´æŽ¥å¼€å§‹å†™ï¼Œæ’å…¥ä¸€ä¸¤ä¸ªç¬‘è¯ï¼Œç„¶åŽå°±å®Œæˆäº†ã€‚\n\nå®ƒä¸éœ€è¦å›žåˆ°ç¬¬ä¸€å¥åŽ»è®¡ç®—å•è¯æ•°é‡æˆ–å…¶ä»–ä»»ä½•äº‹æƒ…ã€‚\n\nå…³é”®æ˜¯ï¼š\n\nå¦‚æžœæ‚¨å‘æ¨¡åž‹è¯·æ±‚ä¸€äº›ç‰¹å®šçš„å†…å®¹ï¼Œè€Œè¿™äº›å†…å®¹åœ¨ä¸€æ¬¡å°è¯•ä¸­å¾ˆéš¾åšåˆ°ä¸”éœ€è¦å®žéªŒï¼Œé‚£ä¹ˆæ‚¨åº”è¯¥ä½¿ç”¨ 01\\-previewã€‚\n\nå¦‚æžœæ‚¨è¯·æ±‚çš„æ˜¯å¼€æ”¾å¼çš„å†…å®¹ï¼Œé‚£ä¹ˆä½¿ç”¨å…¶ä»–æ¨¡åž‹ã€‚\n\næ‚¨éœ€è¦è°¨æ…Žä½¿ç”¨ 01\\-previewï¼Œå› ä¸ºæ‚¨åªèƒ½èŽ·å¾—æœ‰é™æ•°é‡çš„æŸ¥è¯¢ã€‚\n\nå®ƒçš„é™åˆ¶æ€»æ˜¯æ¯”å…¶ä»–æ¨¡åž‹æ›´å¤šï¼Œå› ä¸ºå®ƒä½¿ç”¨çš„èµ„æºè¿œè¿œè¶…è¿‡å…¶ä»–æ¨¡åž‹ã€‚\n\nå¥½æ¶ˆæ¯æ˜¯ï¼Œ01\\-preview ä¼šæ¯”å…¶ä»–æ¨¡åž‹çŠ¯æ›´å°‘çš„é”™è¯¯ã€‚\n\næ­¤å¤–ï¼Œå®ƒèƒ½å¤Ÿå›žç­”ä¹‹å‰æ¨¡åž‹æ— æ³•è§£å†³çš„é—®é¢˜ã€‚\n\nçŽ°åœ¨æ˜¯å­¦ä¹ å¦‚ä½•åˆ©ç”¨ AI æ¥å‘å±•æ‚¨çš„ä¸šåŠ¡å’Œèµšå–æ›´å¤šæ”¶å…¥çš„æœ€ä½³æ—¶æœºã€‚\n\næˆ‘åœ¨æˆ‘çš„ AI å¢žé•¿é»‘å®¢è¯¾ç¨‹ä¸­æ•™æŽˆå¦‚ä½•ä½¿ç”¨è¿™ä¸ªã€‚\n\næˆ‘è¿˜å°†ç»“åˆè¿™ä¸ªæ¨¡åž‹ï¼Œä½¿æˆ‘æž„å»ºçš„è‡ªå®šä¹‰ AI ä»£ç†å’ŒèŠå¤©æœºå™¨äººæ›´åŠ å‡†ç¡®ã€‚\n\nè¿™å°†ä½¿åƒ [**Stammer**](https://stammer.ai/?via=andrew) è¿™æ ·çš„ AI ä»£ç†æž„å»ºè€…æ›´åŠ å¼ºå¤§ã€‚\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/openai-confirms-the-arrival-of-gpt-5-poised-to-bring-huge-improvements-to-artificial-intelligence-e3b858e79c2a","frontmatter":{"title":"OpenAI ç¡®è®¤ GPT-5 å³å°†åˆ°æ¥ï¼Œæœ‰æœ›ä¸ºäººå·¥æ™ºèƒ½å¸¦æ¥å·¨å¤§æ”¹è¿›â€¦â€¦","meta_title":"OpenAI ç¡®è®¤ GPT-5 å³å°†åˆ°æ¥ï¼Œæœ‰æœ›ä¸ºäººå·¥æ™ºèƒ½å¸¦æ¥å·¨å¤§æ”¹è¿›â€¦â€¦","description":"æœ‰ç½‘å‹åœ¨xä¸Šå‘äº†ä¸€ç¯‡GPT5å€’è®¡æ—¶çš„å¸–å­ï¼Œç§°è¿™æ˜¯æ ¹æ®å„å¹³å°çš„çº¿ç´¢å¾—å‡ºçš„ç»“è®ºã€‚è¯„è®ºåŒºâ€¦â€¦","date":"2024-11-01T03:58:58.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8J_opnaERs-wrq2YRKIxdQ.png","categories":["Natural Language Processing","Generative AI","Technology"],"author":"Rifx.Online","tags":["GPT-5","natural","language","efficiency","personalization"],"draft":false,"slug":"blog/openai-confirms-the-arrival-of-gpt-5-poised-to-bring-huge-improvements-to-artificial-intelligence-e3b858e79c2a"},"content":"\n\n\nä¸€ä½ç½‘å‹åœ¨ x ä¸Šå‘å¸ƒäº† GPT5 å€’è®¡æ—¶çš„å¸–å­ï¼Œç§°è¿™æ˜¯ä»Žå„ä¸ªå¹³å°çš„çº¿ç´¢å¾—å‡ºçš„ç»“è®ºã€‚è¯„è®ºåŒºå·²ç»è¾¾åˆ°é«˜æ½®ï¼Œå„ç§æ„è§çº·çº·æ¶ŒçŽ°ã€‚\n\n\n\n**åŽŸå›  1** : OpenAI ç½‘ç«™ GPT5 æ³„éœ²\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*EBDLAv3rOyCjshGBpVRI7A.png)\n\n**åŽŸå›  2** : çŸ¥åç¾Žå›½è´¢ç»ç½‘ç«™ BusinessInsider å‘å¸ƒçš„æ–‡ç« â€œOpenAI å‘å¸ƒæ›´å¥½çš„ GPT5 èŠå¤©æœºå™¨äººâ€ã€‚ç”±äºŽè¯¥ç½‘ç«™ä¸ºä»˜è´¹ç½‘ç«™ï¼Œæ„Ÿå…´è¶£çš„è¯å¯ä»¥æœç´¢æ ‡é¢˜ã€‚ä»¥ä¸‹æ˜¯éƒ¨åˆ†å†…å®¹ï¼š\n\nè¿™å®¶ç”± Sam Altman é¢†å¯¼çš„ç”Ÿæˆå¼ AI å…¬å¸ï¼Œé¢„è®¡å°†åœ¨å¹´ä¸­æŸä¸ªæ—¶å€™æŽ¨å‡º GPT-5ï¼Œå¯èƒ½åœ¨å¤å­£ï¼Œæ ¹æ®ä¸¤ä½ç†Ÿæ‚‰è¯¥å…¬å¸çš„äººå£«çš„è¯´æ³•ã€‚æ ¹æ®å¦ä¸€ä½ç†Ÿæ‚‰è¯¥è¿‡ç¨‹çš„äººå£«çš„è¯´æ³•ï¼Œä¸€äº›ä¼ä¸šå®¢æˆ·æœ€è¿‘æ”¶åˆ°äº†æœ€æ–°æ¨¡åž‹åŠå…¶ä¸Ž ChatGPT å·¥å…·ç›¸å…³çš„å¢žå¼ºåŠŸèƒ½çš„æ¼”ç¤ºã€‚Business Insider å·²ç¡®è®¤è¿™äº›äººçš„èº«ä»½ï¼Œä»–ä»¬è¦æ±‚åŒ¿åä»¥ä¾¿èƒ½å¤Ÿè‡ªç”±å‘è¨€ã€‚\n\næ ¹æ®åœ¨ X å’Œå…¶ä»–å¹³å°ä¸Šçš„è®¨è®ºï¼Œæ–°çš„æ¨¡åž‹ç‰ˆæœ¬å¾ˆå¯èƒ½å°†åœ¨ 6 æœˆ 6 æ—¥æŽ¨å‡ºï¼Œä½†å°šä¸ç¡®å®šæ˜¯å¦ä¸º GPT 4.5 æˆ– GPT5ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*rhApTugfrMVBB6PhMvK4rg.png)\n\næˆ‘ä»¬éƒ½åœ¨ç­‰å¾… **GPT5**\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*eB6j2S_dPbjQ2-sV2N1fwA.jpeg)\n\n## GPT-5çš„æœŸå¾…\n\nå°½ç®¡ç»†èŠ‚ä»ç„¶ç¨€å°‘ï¼Œä½†å›´ç»•GPT-5çš„å…´å¥‹æ„ŸæºäºŽå¯¹äººå·¥æ™ºèƒ½èƒ½åŠ›æ˜¾è‘—æå‡çš„æœŸå¾…ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å¯èƒ½çš„è¿›å±•çŒœæµ‹ï¼š\n\n* å¢žå¼ºçš„è‡ªç„¶è¯­è¨€ç†è§£ï¼šé¢„è®¡GPT-5å°†å¯¹äººç±»è¯­è¨€ä¸­çš„ä¸Šä¸‹æ–‡ã€ç»†å¾®å·®åˆ«å’Œå¾®å¦™ä¹‹å¤„æœ‰æ›´æ·±å…¥çš„ç†è§£ï¼Œä½¿äº’åŠ¨æ›´åŠ æµç•…å’Œè‡ªç„¶ã€‚\n* æé«˜æ•ˆçŽ‡ï¼šéšç€æ¯æ¬¡è¿­ä»£ï¼ŒOpenAIåœ¨å‡å°‘å»¶è¿Ÿå’Œæé«˜æ¨¡åž‹æ•ˆçŽ‡æ–¹é¢å–å¾—äº†è¿›å±•ã€‚é¢„è®¡GPT-5å°†ç»§ç»­è¿™ä¸€è¶‹åŠ¿ï¼Œæä¾›æ›´å¿«å’Œæ›´å‡†ç¡®çš„å“åº”ã€‚\n* æ›´å¹¿æ³›çš„çŸ¥è¯†åŸºç¡€ï¼šé€šè¿‡æ•´åˆæ›´å¤šæ ·åŒ–å’Œå¹¿æ³›çš„æ•°æ®é›†ï¼ŒGPT-5å¯èƒ½åœ¨æ›´å¹¿æ³›çš„ä¸»é¢˜ä¸Šæä¾›æ›´å…¨é¢å’Œå¯é çš„ä¿¡æ¯ã€‚\n* é«˜çº§ä¸ªæ€§åŒ–ï¼šæ–°æ¨¡åž‹å¯èƒ½åŒ…æ‹¬å¢žå¼ºçš„ä¸ªæ€§åŒ–åŠŸèƒ½ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ä¸ªåˆ«ç”¨æˆ·çš„åå¥½å’Œéœ€æ±‚ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*7xCG5iy53LLQCTnmzxs_3g.jpeg)\n\n"},{"lang":"zh","group":"blog","slug":"blog/openai-gpt-5-ph-d-level-intelligence-expected-by-2025-50a86c3aad86","frontmatter":{"title":"OpenAI GPT-5ï¼šé¢„è®¡ 2025 å¹´å°†å®žçŽ°åšå£«çº§æ™ºèƒ½","meta_title":"OpenAI GPT-5ï¼šé¢„è®¡ 2025 å¹´å°†å®žçŽ°åšå£«çº§æ™ºèƒ½","description":"ç»è¿‡æ•°æœˆçš„çŒœæµ‹ï¼ŒOpenAI ç»ˆäºŽå…¬å¸ƒäº†å¤‡å—æœŸå¾…çš„ GPT-5 çš„ç»†èŠ‚ã€‚æœ€åˆé¢„è®¡åœ¨ 2024 å¹´æŽ¨å‡ºï¼Œä½†â€¦â€¦","date":"2024-11-01T03:59:56.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*OasnWeS5mgAX_0hIpirO5Q.jpeg","categories":["Machine Learning","Ethics","Data Science"],"author":"Rifx.Online","tags":["GPT-5","Ph.D.","intelligence","ethics","privacy"],"draft":false,"slug":"blog/openai-gpt-5-ph-d-level-intelligence-expected-by-2025-50a86c3aad86"},"content":"\n\n\n\n\nç»è¿‡å‡ ä¸ªæœˆçš„çŒœæµ‹ï¼ŒOpenAIç»ˆäºŽæ­ç¤ºäº†å¤‡å—æœŸå¾…çš„GPT\\-5çš„è¯¦ç»†ä¿¡æ¯ã€‚æœ€åˆé¢„è®¡åœ¨2024å¹´å‘å¸ƒï¼Œä½†å…¶å‘å¸ƒæ—¶é—´å·²æŽ¨è¿Ÿè‡³2025å¹´æœ«æˆ–2026å¹´åˆã€‚OpenAIçš„é¦–å¸­æŠ€æœ¯å®˜Mira Muratiåœ¨ä¸Žè¾¾ç‰¹èŒ…æ–¯å·¥ç¨‹å­¦é™¢çš„é‡‡è®¿ä¸­åˆ†äº«äº†æœ‰å…³è¿™ä¸ªæ–°ç‰ˆæœ¬çš„èƒ½åŠ›å’Œæ½œåŠ›çš„è§è§£ã€‚ä»¥ä¸‹æ˜¯æ‚¨éœ€è¦çŸ¥é“çš„ä¸€åˆ‡ã€‚\n\n## æ™ºåŠ›çš„é‡å­é£žè·ƒ\n\nMurati å°†ä¹‹å‰çš„ GPT ç‰ˆæœ¬ä¸Žä¸åŒæ°´å¹³çš„äººç±»æ™ºåŠ›è¿›è¡Œæ¯”è¾ƒã€‚GPT\\-3 ç±»ä¼¼äºŽä¸€ä¸ªå¹¼å„¿ï¼Œè€Œ [**GPT\\-4**](https://www.geekmetaverse.com/gpt-4-unveils-its-secrets-a-combination-of-8-smaller-models/) åˆ™å¯ä¸Žé«˜ä¸­ç”Ÿç›¸æå¹¶è®ºã€‚æ–°çš„ GPT\\-5 æ‰¿è¯ºåœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¾¾åˆ°â€œåšå£«çº§æ™ºåŠ›â€ã€‚è¿™ä¸€è¿›å±•ä¸ä»…ä»¤äººå…´å¥‹ï¼Œä¹Ÿå¼•å‘äº†å¯¹äººå·¥æ™ºèƒ½æœªæ¥çš„æ€è€ƒã€‚\n\n## GPTçš„æ¼”å˜ï¼šä»Žå„¿ç«¥åˆ°åšå£«\n\nå°†è¿™äº›ç‰ˆæœ¬ä¸Žäººç±»æ•™è‚²çš„é˜¶æ®µè¿›è¡Œæ¯”è¾ƒï¼Œæœ‰åŠ©äºŽæˆ‘ä»¬æ›´å¥½åœ°ç†è§£è¿™äº›è¿›æ­¥ã€‚GPT-3å‡­å€Ÿå…¶ç”Ÿæˆè¿žè´¯ä¸”æœ‰ç”¨æ–‡æœ¬çš„èƒ½åŠ›ï¼Œæ‰“å¼€äº†è®¸å¤šå¤§é—¨ã€‚GPT-4åœ¨è¿™äº›æŠ€èƒ½ä¸Šè¿›è¡Œäº†æ”¹è¿›ï¼Œè¡¨çŽ°å‡ºåœ¨æ›´å¤æ‚ä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§èƒ½ã€‚çŽ°åœ¨ï¼ŒGPT-5æ—¨åœ¨å°†è¿™ä¸€åˆ‡æå‡åˆ°ä¸€ä¸ªå…¨æ–°çš„æ°´å¹³ï¼Œå…·å¤‡å…ˆè¿›çš„æŽ¨ç†å’Œè®°å¿†èƒ½åŠ›ã€‚\n\n## ä¸“ä¸šæ™ºèƒ½\n\nPh.D.\\-çº§çš„æ™ºèƒ½å¹¶ä¸æ„å‘³ç€ [**GPT\\-5**](https://www.geekmetaverse.com/openai-ceo-confirms-that-gpt-5-is-already-in-development/) å¯ä»¥å®Œç¾Žåœ°å®Œæˆæ‰€æœ‰ä»»åŠ¡ã€‚Murati æ¾„æ¸…äº†è¿™äº›èƒ½åŠ›å°†æ˜¯ç‰¹å®šäºŽä»»åŠ¡çš„ã€‚è¿™è¡¨æ˜Žï¼Œå°½ç®¡ AI å¯èƒ½åœ¨æŸäº›é¢†åŸŸè¶…è¶Šäººç±»ï¼Œä½†åœ¨å…¶ä»–é¢†åŸŸä»ç„¶ä¼šæœ‰å±€é™æ€§ã€‚è¿™ç§ä¸“ä¸šåŒ–çš„å…³æ³¨å¯èƒ½ä¼šå¯¼è‡´åœ¨ç§‘å­¦ç ”ç©¶å’Œå¤æ‚æ•°æ®åˆ†æžç­‰é¢†åŸŸäº§ç”Ÿé«˜åº¦ç²¾ç¡®å’Œæœ‰ç”¨çš„åº”ç”¨ã€‚\n\n## æ½œåœ¨ä¸Žæœªæ¥åº”ç”¨\n\nGPT-5 çš„å‘å±•ä¸ºå„ä¸ªé¢†åŸŸå¼€è¾Ÿäº†ä¸€ç³»åˆ—å¯èƒ½æ€§ã€‚ä»Žæ•™è‚²åˆ°åŒ»å­¦ï¼Œä»Žç ”ç©¶åˆ°æŠ€æœ¯ï¼Œåº”ç”¨å¹¿æ³›ã€‚\n\n### æ•™è‚²ä¸ŽåŸ¹è®­\n\nä¸€æ¬¾èƒ½å¤Ÿè¾¾åˆ°åšå£«æ°´å¹³çš„[**AI**](https://www.geekmetaverse.com/apple-updates-ai-takes-center-stage-with-siri-integration-chatgpt-partnership-and-elon-musk-concerns/)å¯èƒ½ä¼šå½»åº•æ”¹å˜æ•™è‚²ã€‚ä¸ªæ€§åŒ–è¾…å¯¼ç³»ç»Ÿå¯ä»¥ä¸ºå­¦ç”Ÿåœ¨å¤æ‚é¢†åŸŸæä¾›æ”¯æŒï¼Œæå‡ç†è§£èƒ½åŠ›å’Œå­¦ä¸šè¡¨çŽ°ã€‚\n\n### åŒ»å­¦ä¸ŽåŒ»ç–—ä¿å¥\n\nåœ¨åŒ»å­¦é¢†åŸŸï¼Œå…·å¤‡æ­¤ç±»èƒ½åŠ›çš„äººå·¥æ™ºèƒ½å¯ä»¥å¸®åŠ©è¯Šæ–­ç½•è§ç–¾ç—…ã€å¼€å‘ä¸ªæ€§åŒ–æ²»ç–—æ–¹æ¡ˆä»¥åŠç®¡ç†å¤§é‡ä¸´åºŠæ•°æ®ï¼Œä»Žè€Œæ˜¾è‘—æŽ¨è¿›åŒ»ç–—æŠ¤ç†ã€‚\n\n### ç ”ç©¶ä¸Žå¼€å‘\n\nç ”ç©¶äººå‘˜å¯ä»¥ä»Žèƒ½å¤Ÿåˆ†æžå¤§æ•°æ®é›†ã€è¯†åˆ«æ¨¡å¼å¹¶ç”Ÿæˆå‡è®¾çš„äººå·¥æ™ºèƒ½ä¸­èŽ·å¾—æžå¤§ç›Šå¤„ï¼Œä»Žè€ŒåŠ é€Ÿç§‘å­¦å’ŒæŠ€æœ¯å‘çŽ°çš„è¿›ç¨‹ã€‚\n\n## æŒ‘æˆ˜ä¸Žä¼¦ç†è€ƒé‡\n\nå°½ç®¡æœ‰ç€è‰¯å¥½çš„åº”ç”¨å‰æ™¯ï¼Œä½†è¿™ç§å…ˆè¿›AIçš„å‘å±•ä¹Ÿå¸¦æ¥äº†é‡å¤§çš„ä¼¦ç†æŒ‘æˆ˜ã€‚å¦‚æžœå¯¹AIåœ¨å…³é”®ä»»åŠ¡ä¸Šçš„è¿‡åº¦ä¾èµ–æ²¡æœ‰å¾—åˆ°å¦¥å–„ç®¡ç†ï¼Œå¯èƒ½ä¼šå¯¼è‡´ä¸€äº›é—®é¢˜ã€‚\n\n### éšç§ä¸Žå®‰å…¨\n\næ•°æ®éšç§å’Œç½‘ç»œå®‰å…¨å°†æ˜¯å…³é”®è®®é¢˜ã€‚ç¡®ä¿äººå·¥æ™ºèƒ½ç³»ç»Ÿä¸è¢«æ»¥ç”¨ï¼Œä»¥åŠæ•æ„Ÿæ•°æ®å¾—åˆ°å……åˆ†ä¿æŠ¤å°†æ˜¯ä¼˜å…ˆäº‹é¡¹ã€‚\n\n### å°±ä¸šå½±å“\n\nå°±ä¸šå½±å“ä¹Ÿæ˜¯ä¸€ä¸ªå…³æ³¨ç‚¹ã€‚è‡ªåŠ¨åŒ–ä¸“ä¸šä»»åŠ¡å¯èƒ½ä¼šå–ä»£æŸäº›ä¸“ä¸šäººå£«ï¼Œè¿™éœ€è¦é‡‡å–ç§¯æžæŽªæ–½æ¥åº”å¯¹è¿™äº›ç¤¾ä¼šç»æµŽå½±å“ã€‚\n\n### ç»“è®º\n\nGPT-5 å‘å¸ƒçš„å»¶è¿Ÿå¯èƒ½è®©ä¸€äº›äººæ„Ÿåˆ°å¤±æœ›ï¼Œä½†å…¶å…ˆè¿›çš„èƒ½åŠ›å¼•å‘äº†äººä»¬çš„æžå¤§æœŸå¾…ã€‚å¦‚æžœ OpenAI å®žçŽ°å…¶ç›®æ ‡ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šçœ‹åˆ°ä¸€ä¸ªé©å‘½æ€§çš„å·¥å…·ï¼Œå®ƒå°†æ”¹å˜å¤šä¸ªè¡Œä¸šï¼Œå¹¶æ”¹å˜æˆ‘ä»¬ä¸ŽæŠ€æœ¯çš„äº’åŠ¨æ–¹å¼ã€‚\n\n### å¸¸è§é—®é¢˜è§£ç­”\n\n**1\\. ä»€ä¹ˆæ˜¯ GPT\\-5ï¼Ÿ**\n\nGPT\\-5 æ˜¯ OpenAI çš„ç”Ÿæˆé¢„è®­ç»ƒå˜æ¢å™¨ (Generative Pre\\-trained Transformer, GPT) ç³»åˆ—å³å°†æŽ¨å‡ºçš„ç‰ˆæœ¬ï¼Œæ‰¿è¯ºåœ¨ç‰¹å®šä»»åŠ¡ä¸­å…·å¤‡åšå£«çº§åˆ«çš„æ™ºèƒ½ã€‚\n\n**2\\. GPT\\-5 é¢„è®¡ä½•æ—¶å‘å¸ƒï¼Ÿ**\n\nGPT\\-5 çš„å‘å¸ƒå·²æŽ¨è¿Ÿè‡³ 2025 å¹´åº•æˆ– 2026 å¹´åˆã€‚\n\n**3\\. GPT\\-5 ä¸Žä¹‹å‰çš„ç‰ˆæœ¬ç›¸æ¯”å¦‚ä½•ï¼Ÿ**\n\nGPT\\-3 çš„æ™ºèƒ½ç›¸å½“äºŽä¸€ä¸ªå¹´è½»å„¿ç«¥ï¼Œè€Œ GPT\\-4 åˆ™ç›¸å½“äºŽä¸€åé«˜ä¸­ç”Ÿã€‚GPT\\-5 æ—¨åœ¨å®žçŽ°ç‰¹å®šä»»åŠ¡çš„åšå£«çº§åˆ«æ™ºèƒ½ï¼Œæä¾›å…ˆè¿›çš„æŽ¨ç†å’Œè®°å¿†èƒ½åŠ›ã€‚\n\n**4\\. GPT\\-5 èƒ½å¤Ÿæ‰§è¡Œå“ªäº›ä»»åŠ¡ï¼Ÿ**\n\nGPT\\-5 å°†ä¸“æ³¨äºŽæŸäº›ä»»åŠ¡ï¼Œåœ¨ç§‘å­¦ç ”ç©¶ã€å¤æ‚æ•°æ®åˆ†æžã€æ•™è‚²å’ŒåŒ»ç–—ç­‰ç‰¹å®šé¢†åŸŸè¡¨çŽ°å‡ºè‰²ã€‚\n\n**5\\. GPT\\-5 ä¼šåœ¨æ‰€æœ‰æ–¹é¢éƒ½å®Œç¾Žå—ï¼Ÿ**\n\nä¸ï¼ŒGPT\\-5 çš„åšå£«çº§åˆ«æ™ºèƒ½å°†æ˜¯é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„ï¼Œè¿™æ„å‘³ç€å®ƒåœ¨æŸäº›é¢†åŸŸè¡¨çŽ°å‡ºè‰²ï¼Œä½†åœ¨å…¶ä»–é¢†åŸŸä»ç„¶ä¼šæœ‰å±€é™æ€§ã€‚\n\n**6\\. GPT\\-5 çš„æ½œåœ¨åº”ç”¨æ˜¯ä»€ä¹ˆï¼Ÿ**\n\næ½œåœ¨åº”ç”¨åŒ…æ‹¬æ•™è‚²ä¸­çš„ä¸ªæ€§åŒ–è¾…å¯¼ã€åŒ»ç–—ä¸­å¸®åŠ©è¯Šæ–­ç–¾ç—…å’Œå¼€å‘æ²»ç–—æ–¹æ¡ˆï¼Œä»¥åŠååŠ©ç ”ç©¶äººå‘˜åˆ†æžå¤§æ•°æ®é›†å’Œç”Ÿæˆå‡è®¾ã€‚\n\n**7\\. ä¸Ž GPT\\-5 ç›¸å…³çš„ä¼¦ç†è€ƒè™‘æ˜¯ä»€ä¹ˆï¼Ÿ**\n\nä¼¦ç†è€ƒè™‘åŒ…æ‹¬ç¡®ä¿æ•°æ®éšç§å’Œç½‘ç»œå®‰å…¨ï¼Œç®¡ç†å› è‡ªåŠ¨åŒ–å¯¼è‡´çš„å°±ä¸šæµå¤±çš„ç¤¾ä¼šç»æµŽå½±å“ï¼Œä»¥åŠé˜²æ­¢é«˜çº§äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„è¯¯ç”¨ã€‚\n\n**8\\. GPT\\-5 å°†å¦‚ä½•å½±å“æ•°æ®éšç§å’Œå®‰å…¨ï¼Ÿ**\n\nç¡®ä¿ä¿æŠ¤æ•æ„Ÿæ•°æ®å’Œé˜²æ­¢äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„è¯¯ç”¨è‡³å…³é‡è¦ã€‚éœ€è¦å®žæ–½æŽªæ–½ä»¥ä¿éšœæ•°æ®éšç§å’Œå®‰å…¨ã€‚\n\n**9\\. GPT\\-5 å¯¹å°±ä¸šçš„æ½œåœ¨å½±å“æ˜¯ä»€ä¹ˆï¼Ÿ**\n\nGPT\\-5 å¯¹ä¸“ä¸šä»»åŠ¡çš„è‡ªåŠ¨åŒ–å¯èƒ½ä¼šä½¿æŸäº›ä¸“ä¸šäººå£«å¤±ä¸šï¼Œå› æ­¤éœ€è¦é‡‡å–ç§¯æžæŽªæ–½æ¥å‡è½»è¿™äº›ç¤¾ä¼šç»æµŽå½±å“ã€‚\n\n**10\\. ä¸ºä»€ä¹ˆ GPT\\-5 çš„å‘å¸ƒè¢«å»¶è¿Ÿï¼Ÿ**\n\nå»¶è¿Ÿä½¿ OpenAI èƒ½å¤Ÿå®Œå–„å’Œå¢žå¼º GPT\\-5 çš„èƒ½åŠ›ï¼Œä»¥ç¡®ä¿å…¶æ»¡è¶³å¯¹é«˜çº§æ™ºèƒ½å’Œä¸“ä¸šåº”ç”¨çš„é«˜æœŸæœ›ã€‚\n\n**11\\. GPT\\-5 å¦‚ä½•æ”¹å˜æ•™è‚²ï¼Ÿ**\n\nGPT\\-5 å¯ä»¥é€šè¿‡æä¾›ä¸ªæ€§åŒ–è¾…å¯¼ç³»ç»Ÿæ¥å½»åº•æ”¹å˜æ•™è‚²ï¼Œæ”¯æŒå­¦ç”Ÿåœ¨å¤æ‚å­¦ç§‘ä¸­çš„ç†è§£å’Œå­¦ä¸šè¡¨çŽ°ã€‚\n\n**12\\. GPT\\-5 å¯ä»¥ä¸ºåŒ»ç–—é¢†åŸŸå¸¦æ¥å“ªäº›è¿›æ­¥ï¼Ÿ**\n\nåœ¨åŒ»å­¦é¢†åŸŸï¼ŒGPT\\-5 å¯ä»¥å¸®åŠ©è¯Šæ–­ç½•è§ç–¾ç—…ï¼Œå¼€å‘ä¸ªæ€§åŒ–æ²»ç–—æ–¹æ¡ˆï¼Œä»¥åŠç®¡ç†å¤§é‡ä¸´åºŠæ•°æ®ï¼ŒæŽ¨åŠ¨åŒ»ç–—æŠ¤ç†çš„é‡å¤§è¿›å±•ã€‚\n\nåŽŸæ–‡é“¾æŽ¥: [https://www.geekmetaverse.com/openai\\-gpt\\-5\\-ph\\-d\\-level\\-intelligence\\-2025/](https://www.geekmetaverse.com/openai-gpt-5-ph-d-level-intelligence-2025/)\n\n"},{"lang":"zh","group":"blog","slug":"blog/openai-just-built-her-in-real-life-17769d993e11","frontmatter":{"title":"ç”¨æˆ·ä¼šçˆ±ä¸Š OpenAI çš„æ–° GPT-4o æ¨¡åž‹ã€‚ç¡®å®žå¦‚æ­¤ã€‚","meta_title":"ç”¨æˆ·ä¼šçˆ±ä¸Š OpenAI çš„æ–° GPT-4o æ¨¡åž‹ã€‚ç¡®å®žå¦‚æ­¤ã€‚","description":"è¯¥å…¬å¸çš„æ–°æ¬¾ GPT-4o å¯ä»¥ç†è§£å’Œæ¨¡ä»¿äººç±»çš„è¯­è¨€å’Œæƒ…æ„Ÿ","date":"2024-11-01T04:08:40.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*-bTsggApvkUHAq57YhSd-A.png","categories":["Generative AI","Chatbots","Natural Language Processing"],"author":"Rifx.Online","tags":["GPT-4o","speech","emotions","multilingual","conversational"],"draft":false,"slug":"blog/openai-just-built-her-in-real-life-17769d993e11"},"content":"\n\n\n## å…¬å¸çš„æ–° GPT\\-4o èƒ½ç†è§£å¹¶æ¨¡ä»¿äººç±»çš„è¯­è¨€å’Œæƒ…æ„Ÿ\n\n\n\nåœ¨æ ‡å¿—æ€§çš„2013å¹´ç”µå½± *å¥¹* ä¸­ï¼Œä¸»è§’ä¸Žä¸€ä¸ªè¯­éŸ³å¯ç”¨çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿå‘å±•å‡ºä¸€ç§å¼ºçƒˆçš„å…³ç³»â€”â€”å¹¶æ¼”å˜æˆä¸€åœºçˆ±æƒ…æ•…äº‹ã€‚\n\n*å¥¹* ä¸­çš„äººå·¥æ™ºèƒ½æ˜¯ä»Šå¤©çš„è¯­éŸ³å¯ç”¨ç³»ç»Ÿæ‰€ä¸å…·å¤‡çš„ï¼šå¯Œæœ‰æƒ…æ„Ÿã€å¹½é»˜ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ´žå¯Ÿäººç±»å¯¹è¯çš„ç»†å¾®å·®åˆ«ã€‚\n\nåœ¨ä»Šå¤©æ—©ä¸Šçš„ä¸€æ¬¡é‡å¤§[å…¬å‘Š](https://www.youtube.com/live/DQacCB9tDaw?app=desktop&si=jvKW7jFDwFvOMBBk)ä¸­ï¼ŒOpenAI å®£å¸ƒå‘å¸ƒä¸€ä¸ªæ–°ç‰ˆæœ¬çš„ ChatGPT ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿå°†è¯­éŸ³ã€è½¬å½•å’Œæ™ºèƒ½åŽŸç”Ÿé›†æˆåˆ°ä¸€ä¸ªæ¨¡åž‹ä¸­ã€‚\n\nå®ƒå¼ºå¤§ã€ç›´è§‚ï¼Œå¹¶ä¸”ä»¤äººä¸å®‰åœ°åƒäººç±»ã€‚åŸºæœ¬ä¸Šï¼ŒOpenAI å»ºé€ äº†ä¸€ä¸ªçŽ°å®žç‰ˆçš„ *å¥¹*ã€‚\n\n## ä¸€ä¸ªç³Ÿç³•çš„å¯¹è¯è€…\n\nChatGPTå·²ç»æ‹¥æœ‰è¯­éŸ³åŠŸèƒ½å‡ ä¸ªæœˆäº†ã€‚å³ä½¿åœ¨ä»Šå¤©ï¼Œæ‚¨å¯ä»¥åœ¨æ‰‹æœºä¸Šæ‰“å¼€ChatGPTåº”ç”¨ç¨‹åºï¼ŒæŒ‰ä¸‹è€³æœºå›¾æ ‡ï¼Œç”¨æ‚¨çš„å£°éŸ³ä¸Žç³»ç»Ÿå¯¹è¯ã€‚\n\nç„¶è€Œï¼Œé—®é¢˜åœ¨äºŽï¼ŒChatGPTæ˜¯ä¸€ä¸ªç³Ÿç³•çš„å¯¹è¯è€…ã€‚\n\nå®žé™…ä¸Šï¼ŒChatGPTçš„è¯­éŸ³åŠŸèƒ½æ˜¯é€šè¿‡å°†ä¸‰ä¸ªä¸åŒçš„æ¨¡åž‹æ‹¼æŽ¥åœ¨ä¸€èµ·è€Œåˆ›å»ºçš„é»‘å®¢æŠ€æœ¯ã€‚\n\nå½“æ‚¨å¯¹ç³»ç»Ÿè®²è¯æ—¶ï¼Œå®ƒé¦–å…ˆä¼šä½¿ç”¨è½¬å½•æ¨¡åž‹å°†æ‚¨çš„å£°éŸ³è½¬åŒ–ä¸ºæ–‡æœ¬ã€‚ç„¶åŽï¼Œå®ƒä¼šå°†è¯¥æ–‡æœ¬è¾“å…¥åˆ°å…¶æ™ºèƒ½æ¨¡åž‹ä¸­â€”â€”åŸºæœ¬ä¸Šï¼Œä¸ŽGPT\\-4\\çš„åŸºç¡€ç³»ç»Ÿç›¸åŒã€‚\n\næ™ºèƒ½ç³»ç»Ÿä¼šç”Ÿæˆæ–‡æœ¬ï¼ŒChatGPTä¼šå°†å…¶åé¦ˆåˆ°ä¸€ä¸ªæ–‡æœ¬è½¬è¯­éŸ³ç³»ç»Ÿä¸­ï¼Œä»¥åˆ›å»ºä¸€ä¸ªè®¡ç®—æœºåŒ–çš„å£°éŸ³æ¥å›žåº”æ‚¨ã€‚\n\nè¿™ä½¿å¾—ç³»ç»Ÿåä¹‰ä¸Šæ˜¯å¯ä»¥å¯¹è¯çš„ï¼Œä½†å®žé™…ä¸Šä¸Žå®ƒäº¤è°ˆå´æ˜¾å¾—ç¬¨æ‹™å’Œå°´å°¬ã€‚\n\nåœ¨ä¸åŒæ¨¡åž‹ä¹‹é—´ä¼ é€’å†…å®¹çš„é¢å¤–æ­¥éª¤æ„å‘³ç€ç³»ç»Ÿååº”è¿Ÿç¼“ã€‚åœ¨æˆ‘è‡ªå·±çš„æµ‹è¯•ä¸­ï¼Œæˆ‘å‘çŽ°ä»Žä¸Žç³»ç»Ÿå¯¹è¯åˆ°å¾—åˆ°å›žåº”ï¼Œé€šå¸¸éœ€è¦3åˆ°5ç§’çš„æ—¶é—´ã€‚\n\näººç±»å¯¹è¯ä¾èµ–äºŽåœ¨æ¯«ç§’ä¹‹é—´å±•å¼€çš„å¾®å¦™ä¹‹å¤„ã€‚ä¸€ä¸ªå“åº”è¯­éŸ³éœ€è¦é•¿è¾¾äº”ç§’çš„ç³»ç»Ÿæ„Ÿè§‰ç¬¨æ‹™å’Œæœºæ¢°ã€‚\n\nä¹‹å‰çš„ç³»ç»Ÿè¿˜ç¼ºä¹äººç±»è¯­è¨€çš„è®¸å¤šåŸºæœ¬æ–¹é¢ã€‚\n\nä¾‹å¦‚ï¼Œæ‚¨æ— æ³•æ‰“æ–­å®ƒï¼›æ‚¨å¿…é¡»ç­‰å®ƒè¯´å®Œæ‰èƒ½å›žåº”ã€‚\n\nä¸Žå®ƒäº¤è°ˆå¸¸å¸¸æ„Ÿè§‰åƒæ˜¯åœ¨ä¸Žé‚£äº›æ— æ³•æ‰“æ–­çš„äººäº¤è°ˆï¼Œä»–ä»¬åœ¨æ²¡æœ‰æ„è¯†åˆ°æˆ¿é—´é‡Œå…¶ä»–äººçš„æƒ…å†µä¸‹ï¼Œå–‹å–‹ä¸ä¼‘åœ°è°ˆè®ºä¸€ä¸ªéšæœºè¯é¢˜ã€‚æ‚¨å¸¸å¸¸è§‰å¾—éœ€è¦æåˆ°å¥¥æ–¯å¡çš„ä¹å›¢ï¼Œä»¥ç»æœ›çš„å°è¯•è®©ç³»ç»Ÿåœæ­¢è¯´è¯ã€‚\n\nå®ƒè¿˜å—åˆ°æ— æ³•è§£è¯»å£°éŸ³ä¸­çš„æƒ…æ„Ÿæˆ–åœ¨è‡ªèº«å›žåº”ä¸­å‡†ç¡®æ¨¡ä»¿äººç±»æƒ…æ„Ÿçš„é™åˆ¶ã€‚\n\näººç±»åœ¨é˜…è¯»æ½œå°è¯æ–¹é¢éžå¸¸å‡ºè‰²ï¼Œéƒ¨åˆ†åŽŸå› æ˜¯æˆ‘ä»¬å¯ä»¥æ•æ‰åˆ°è¯´è¯è€…å£°éŸ³ä¸­çš„å¾®å¦™æƒ…æ„Ÿçº¿ç´¢ã€‚\n\nå¦‚æžœæˆ‘é—®æˆ‘çš„æœ‹å‹ï¼šâ€œä½ ä»Šå¤©è¿‡å¾—æ€Žä¹ˆæ ·ï¼Ÿâ€è€Œä»–ä»¬å›žç­”ï¼šâ€œè¿˜ä¸é”™â€ï¼Œä½†åœ¨â€œè¿‡å¾—â€å’Œâ€œä¸é”™â€ä¹‹é—´æ’å…¥äº†ä¸€ä¸ªå¾®å¦™çš„åœé¡¿ï¼ˆæˆ–è€…æœ€åŽä¸€ä¸ªè¯ä¸­å¸¦æœ‰ä¸€ä¸æ¼æ€’ï¼‰ï¼Œæˆ‘å°±çŸ¥é“ä»–ä»¬å®žé™…ä¸Šåº¦è¿‡äº†ä¸€ä¸ªè‰°éš¾çš„æ—¥å­ï¼Œæˆ‘åº”è¯¥é—®ä¸€äº›åŽç»­é—®é¢˜ã€‚\n\nChatGPTåšä¸åˆ°è¿™äº›ï¼Œè¿™ä½¿å¾—ä¸Žå®ƒäº¤è°ˆæ„Ÿè§‰åƒæ˜¯åœ¨ä¸ŽæŸç§å¤–æ˜Ÿæ™ºèƒ½æ²Ÿé€šï¼Œè€Œä¸æ˜¯ä¸Žäººç±»äº¤æµã€‚\n\næ€»ä¹‹ï¼Œä¹‹å‰çš„ç³»ç»Ÿæ˜Žæ˜¾è½å…¥äº†â€œææ€–è°·â€ã€‚å®ƒåœ¨å¯¹è¯æ–¹é¢è¶³å¤Ÿå‡ºè‰²ï¼Œå£°éŸ³ä¹Ÿè¶³å¤Ÿä»¤äººä¿¡æœï¼Œä»¥è‡³äºŽå¯¹è¯çš„æŸäº›éƒ¨åˆ†å¯èƒ½æ„Ÿè§‰åƒäººç±»ã€‚\n\nä½†å¥‡æ€ªçš„åœé¡¿ã€ç¼ºä¹æƒ…æ„Ÿç†è§£å’Œå»¶è¿Ÿæœ€ç»ˆæ‰“ç ´äº†è¿™ç§å¹»è§‰ï¼Œä½¿å…¶æ˜¾å¾—æ›´ä»¤äººä¸å®‰è€Œä¸æ˜¯æœ‰ç”¨ã€‚\n\næˆ‘å°è¯•ä¸Žæˆ‘å…­å²çš„å„¿å­ä½¿ç”¨ä¹‹å‰çš„ç³»ç»Ÿã€‚ä»–å¯¹å®ƒæ„Ÿåˆ°å¦‚æ­¤ä¸å®‰ï¼Œä»¥è‡³äºŽä¸è®©æˆ‘å†æ‰“å¼€éŸ³é¢‘ã€‚\n\n## OpenAIçš„é©å‘½æ€§æ–°æ¨¡åž‹\n\nä»Šå¤©ï¼ŒOpenAIæ­£åœ¨æ”¹å˜è¿™ä¸€åˆ‡ã€‚åœ¨ä»–ä»¬[ä»Šå¤©æ—©ä¸Šçš„å…¬å‘Š](https://www.youtube.com/live/DQacCB9tDaw?app=desktop&si=jvKW7jFDwFvOMBBk)ä¸­ï¼Œå…¬å¸é€éœ²ä»–ä»¬å°†å‘å¸ƒä¸€ä¸ªæ–°æ¨¡åž‹ï¼ŒGPT\\-4oã€‚\n\nGPT\\-4oåŽŸç”Ÿé›†æˆäº†è¯­éŸ³è¯†åˆ«ã€è¯­éŸ³ç”Ÿæˆå’Œæ™ºèƒ½äºŽä¸€ä¸ªç³»ç»Ÿä¸­ã€‚\n\nè¿™æ„å‘³ç€å°†ä¸‰ç§ä¸åŒæ¨¡åž‹é›†æˆä»¥æ¨¡æ‹Ÿå¯¹è¯çš„å¤æ‚ä»£ç ç³»ç»Ÿå·²ç»ä¸å¤å­˜åœ¨ã€‚ç›¸åï¼Œæ–°çš„ChatGPTç‰ˆæœ¬å°†èƒ½å¤Ÿ**æŽ¥æ”¶è¯­éŸ³ï¼Œçž¬é—´å¤„ç†ï¼Œå¹¶ä»¥å…¶è‡ªèº«ç”Ÿæˆçš„é€¼çœŸè¯­éŸ³ä½œå‡ºå›žåº”ã€‚**\n\nå¯¹äºŽç”¨æˆ·æ¥è¯´ï¼Œè¿™å°†å¯ç”¨OpenAIé¦–å¸­æ‰§è¡Œå®˜Sam Altmanæ‰€æè¿°çš„å‡ ç§æ–°åŠŸèƒ½ï¼Œç§°å…¶ä¸ºâ€œåƒé­”æ³•ä¸€æ ·â€ã€‚[æè¿°ä¸ºâ€œåƒé­”æ³•ã€‚â€](https://twitter.com/sama/status/1788989777452408943)\n\né¦–å…ˆï¼Œä½ å°†èƒ½å¤Ÿä¸ŽChatGPTè¿›è¡Œæ›´åŠ è‡ªç„¶çš„å¯¹è¯ã€‚ä½ ä¸å†éœ€è¦å°†é—®é¢˜å’ŒåŽç»­é—®é¢˜è¾“å…¥åˆ°ç•Œé¢ä¸­ï¼Œè€Œæ˜¯å¯ä»¥åƒä¸Žæœ‹å‹äº¤è°ˆä¸€æ ·ä¸Žåº”ç”¨ç¨‹åºäº¤è°ˆã€‚\n\nåœ¨å‡ æ¬¡çŽ°åœºæ¼”ç¤ºä¸­ï¼ŒOpenAIçš„å·¥ç¨‹å¸ˆå±•ç¤ºäº†ç³»ç»Ÿå¦‚ä½•åœ¨æ¯«ç§’å†…å€¾å¬ç”¨æˆ·å¹¶ä½œå‡ºæ™ºèƒ½å›žåº”ã€‚\n\nå†æ¬¡å¼ºè°ƒï¼Œè¿™ç§é€Ÿåº¦ä¹‹æ‰€ä»¥å¯èƒ½ï¼Œæ˜¯å› ä¸ºæ–°æ¨¡åž‹ä¸éœ€è¦æµªè´¹æ—¶é—´åœ¨ä¸åŒæ¨¡å¼ä¹‹é—´åˆ‡æ¢â€”â€”å®ƒå¯ä»¥åœ¨å•ä¸€æ­¥éª¤ä¸­å¤„ç†è¯­éŸ³å¹¶ä»¥è‡ªå·±çš„å£°éŸ³å›žåº”ï¼Œè€Œä¸å¿…ä¾èµ–å¤šä¸ªä½Žçº§æ¨¡åž‹ã€‚\n\nGPT\\-4oè¿˜å¯ä»¥è§£è¯»å’Œåˆ›é€ æƒ…æ„Ÿã€‚\n\nåœ¨ä¸€æ¬¡æ¼”ç¤ºä¸­ï¼Œä¸€åOpenAIå‘˜å·¥è¦æ±‚ç³»ç»Ÿå¼•å¯¼ä»–è¿›è¡Œå‘¼å¸ç»ƒä¹ ã€‚\n\nä»–éšåŽå‡è£…è¿‡åº¦æ¢æ°”ï¼Œè€ŒChatGPTâ€”â€”æ„ŸçŸ¥åˆ°ä»–å‘¼å¸çš„é€Ÿåº¦å’Œå£°éŸ³ä¸­æ˜¾çŽ°çš„ææ…Œâ€”â€”åŠä»–æ”¾æ…¢é€Ÿåº¦ï¼Œåšæ›´æ·±çš„å‘¼å¸ã€‚\n\nè¯¥ç³»ç»Ÿä¼¼ä¹Žè¿˜èƒ½å¤Ÿè°ƒèŠ‚è‡ªèº«å›žåº”ä¸­çš„æƒ…æ„Ÿã€‚åœ¨å¦ä¸€åœºæ¼”ç¤ºä¸­ï¼Œè¿™åå‘˜å·¥è¦æ±‚GPT\\-4oç”¨è¶Šæ¥è¶Šæˆå‰§åŒ–çš„å£°éŸ³è®²ä¸€ä¸ªç¡å‰æ•…äº‹ã€‚\n\nå®ƒç…§åŠžäº†ï¼Œæœ€ç»ˆå¬èµ·æ¥åƒä¸€ä¸ªä¸­å­¦æˆå‰§ç¤¾çš„å­©å­åœ¨å¯æ€•åœ°è¿‡åº¦è¡¨æ¼”ä¸€ä¸ªåœºæ™¯ï¼\n\nç”±äºŽæ–°ç³»ç»Ÿè¿˜é›†æˆäº†GPT\\-4çš„è§†è§‰èƒ½åŠ›ï¼Œå®ƒå¯ä»¥æ‰§è¡Œè¯¸å¦‚è§£è¯»äººè„¸è¡¨æƒ…ä¸­çš„æƒ…æ„Ÿç­‰åŠŸèƒ½ã€‚\n\nè¿™ç§å¢žå¼ºçš„æƒ…æ„Ÿæ™ºèƒ½æ°´å¹³å¯èƒ½ä¼šä½¿ç³»ç»Ÿæˆä¸ºä¸€ä¸ªæ›´å¥½çš„å¯¹è¯è€…ã€‚\n\nå…¶ä»–æ–°åŠŸèƒ½ä¹Ÿå°†æœ‰æ‰€å¸®åŠ©ã€‚ç”¨æˆ·å¯ä»¥åœ¨GPT\\-4oè¯´è¯çš„è¿‡ç¨‹ä¸­æ‰“æ–­å®ƒã€‚\n\nåœ¨ä»–ä»¬çš„æ¼”ç¤ºä¸­ï¼ŒOpenAIçš„å·¥ä½œäººå‘˜ç»å¸¸åœ¨æ¨¡åž‹å¼€å§‹åç¦»ä¸»é¢˜æ—¶æ‰“æ–­å®ƒï¼Œå°±åƒåœ¨çŽ°å®žç”Ÿæ´»ä¸­æ‰“æ–­æœ‹å‹ä»¥å¼€å§‹å›žåº”é—®é¢˜ä¸€æ ·ã€‚\n\n## å·¨å¤§çš„æ½œåŠ›\n\nä»Šå¤©æ—©ä¸Šçš„æ¼”ç¤ºè½»æ¾å¹½é»˜ã€‚ä½†äººä»¬å¾ˆå¿«å°±èƒ½æ„è¯†åˆ°ï¼Œä¸€ä¸ªèƒ½å¤Ÿè½»æ¾ç†è§£ã€å¿«é€Ÿå¤„ç†å¹¶çœŸå®žåˆ›é€ æƒ…æ„Ÿäººç±»è¯­è¨€çš„æ¨¡åž‹å°†æ˜¯å¤šä¹ˆå¼ºå¤§ã€‚\n\nåœ¨æ¼”ç¤ºè¿‡ç¨‹ä¸­ï¼ŒChatGPTå‡ æ¬¡ä»¥è®©æˆ‘æƒ³èµ·ç”µå½±ã€Šå¥¹ã€‹ä¸­è™šæž„çš„äººå·¥æ™ºèƒ½çš„æ–¹å¼å›žåº”ã€‚\n\nChatGPTä¼¼ä¹Žå¯¹è‡ªå·±æ„Ÿåˆ°å¥½ç¬‘ï¼Œå½“OpenAIçš„å·¥ä½œäººå‘˜ç§°èµžå®ƒæ—¶ï¼Œå®ƒä¼šæ„Ÿåˆ°å°´å°¬ï¼Œç”šè‡³å¯èƒ½ä¼šä¸æ—¶åœ°æŠ›å‡ºä¸€äº›è°ƒæƒ…çš„å°è¯ã€‚\n\nå‡ æ¬¡ï¼ˆæ®ç§°ï¼‰å³å…´çš„äº’åŠ¨ä¹Ÿæ­ç¤ºäº†æ›´å¥½çš„å¯¹è¯å¯ä»¥è§£é”çš„ä¸€äº›æ›´æ·±å±‚æ¬¡çš„èƒ½åŠ›ã€‚\n\næ ¹æ®è§‚ä¼—çš„é—®é¢˜ï¼ŒOpenAIçš„å·¥ä½œäººå‘˜æ¼”ç¤ºäº†ç³»ç»Ÿå¦‚ä½•èƒ½å¤Ÿå¬æ‡‚æ„å¤§åˆ©è¯­ï¼Œå¹¶å¿«é€Ÿå‡†ç¡®åœ°å°†å…¶ç¿»è¯‘æˆè‹±è¯­ï¼Œåä¹‹äº¦ç„¶ã€‚\n\näººä»¬å¾ˆå®¹æ˜“æƒ³è±¡ï¼Œè¿™æ ·çš„èƒ½åŠ›å°†ä½¿å¤šè¯­è¨€äº’åŠ¨å˜å¾—æžå…¶ç®€å•ï¼ŒåŸºæœ¬ä¸Šæ¶ˆé™¤äº†è¯­è¨€éšœç¢ï¼ˆä¹Ÿè®¸è¿˜åŒ…æ‹¬äººç±»ç¿»è¯‘ï¼‰ã€‚\n\nä¾‹å¦‚ï¼Œä¸€ä½åŒ»ç”Ÿå¯ä»¥è°ƒå‡ºChatGPTï¼Œå¿«é€Ÿä¸Žä»»ä½•è¯­è¨€çš„æ‚£è€…äº¤æµã€‚åœ¨æ—…è¡Œæ—¶ï¼Œä½ å¯ä»¥åœ¨æ‰‹æœºä¸Šè°ƒå‡ºè¿™ä¸ªåº”ç”¨ï¼ŒæŠŠå®ƒå½“ä½œä¸€ä¸ªå…è´¹çš„å³æ—¶ç¿»è¯‘ï¼Œå‘æŸäººè¯¢é—®æ–¹å‘æˆ–åœ¨å•†åº—é‡Œè¿›è¡Œè´­ä¹°ã€‚\n\nå¦‚æžœå†åŠ ä¸Šè§†è§‰èƒ½åŠ›ï¼Œä½ ç”šè‡³å¯ä»¥å‘ChatGPTå±•ç¤ºä¸€å®¶å¤–å›½é¤åŽ…çš„èœå•ï¼Œè¯¢é—®æŸäº›èœå“çš„ç¿»è¯‘ï¼Œå‘Šè¯‰å®ƒä½ åœ¨å®¶æ—¶å–œæ¬¢åƒä»€ä¹ˆï¼Œå¹¶è¯·å®ƒæŽ¨èä¸€äº›ä½ å¯èƒ½æƒ³ç‚¹çš„èœï¼ˆæˆ–é¿å…çš„èœï¼‰ã€‚\n\næˆ‘ä¹Ÿå¯ä»¥çœ‹åˆ°æ–°ç³»ç»Ÿå¦‚ä½•è¿…é€Ÿè¿›å…¥ã€Šå¥¹ã€‹çš„é¢†åŸŸã€‚OpenAIä»ç„¶ä¸å…è®¸ç”µå½±ä¸­å‘ç”Ÿçš„é‚£ç§ä¸é€‚åˆå·¥ä½œåœºåˆçš„äº’åŠ¨ã€‚\n\nä½†æ˜¯GPT-4oç†è§£å’Œæ¨¡ä»¿æƒ…æ„Ÿçš„èƒ½åŠ›â€”â€”åŠ ä¸Šå…¶å¼ºå¤§ä¸”å¸¸å¸¸ä»¤äººæƒŠè®¶çš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿç”Ÿæˆè‡ªå·±ä»¤äººä¿¡æœçš„äººç±»æƒ…æ„Ÿè¡¨è¾¾â€”â€”ä»¤äººå°è±¡æ·±åˆ»ã€‚\n\nå¬å®Œæ¼”ç¤ºåŽï¼Œæˆ‘ç¡®ä¿¡äººä»¬ä¼šåƒç”µå½±ä¸­çš„ä¸»è§’ä¸€æ ·çˆ±ä¸Šè¿™ä¸ªç³»ç»Ÿã€‚å®ƒçœŸçš„å¾ˆå‡ºè‰²ã€‚\n\n## å®ƒä¼šè¢«ä½¿ç”¨å—ï¼Ÿ\n\næ‰€æœ‰è¿™äº›åœ¨çº¸é¢ä¸Šéƒ½å¾ˆæƒŠäººã€‚ç„¶è€Œï¼Œç›®å‰è¿˜ä¸æ¸…æ¥šæœ‰å¤šå°‘ç”¨æˆ·çœŸæ­£æƒ³è¦ä¸€ä¸ªå®Œå…¨æƒ…æ„ŸåŒ–çš„ AI è¯­éŸ³ä¼´ä¾£ã€‚\n\næˆ‘å·¥ä½œçš„å¤§å¤šæ•°äººä½¿ç”¨ ChatGPT ä¸æ˜¯ä½œä¸ºå¯¹è¯ä¼´ä¾£ï¼Œè€Œæ˜¯å‡ºäºŽå®žç”¨ç›®çš„ã€‚\n\næˆ‘çœ‹åˆ°åŒäº‹ä»¬åˆ©ç”¨è¿™ä¸ªç³»ç»Ÿæ¥å¤„ç†ä¸€äº›æ— èŠå’Œå•è°ƒçš„ä»»åŠ¡ï¼Œæ¯”å¦‚ä¸ºç½‘ç»œç ”è®¨ä¼šæ’°å†™ç€é™†é¡µæ–‡æ¡ˆã€å¿«é€Ÿå›žå¤æˆ¿ä¸œçš„ç”µå­é‚®ä»¶ï¼Œæˆ–æ’°å†™åšå®¢æ–‡ç« çš„åˆç¨¿ã€‚\n\nè¿™äº›å®žç”¨åŠŸèƒ½å®žé™…ä¸Šå¹¶ä¸éœ€è¦å¯¹è¯ã€‚ç›®å‰å°šä¸æ¸…æ¥šèƒ½å¤Ÿç”¨è¯­éŸ³å‘ AI å‘å‡ºè¿™äº›è¯·æ±‚æ˜¯å¦ä¼šæœ‰ç”¨ã€‚\n\nå› æ­¤ï¼ŒçœŸæ­£çš„è€ƒéªŒå¹¶ä¸ä¸€å®šæ˜¯ OpenAI çš„æ–°ç³»ç»Ÿæœ‰å¤šå¼ºå¤§ï¼Œè€Œæ˜¯ **ä»–ä»¬åœ¨ç”¨æˆ·å·²ç»é€šè¿‡è¯­éŸ³ä¸Žè®¡ç®—æœºäº’åŠ¨çš„åœ°æ–¹æ•´åˆå®ƒçš„æ•ˆæžœå¦‚ä½•ã€‚**\n\nçŽ°å®žæ¥çœ‹ï¼Œæˆ‘æ— æ³•æƒ³è±¡æœ‰å¤šå°‘ç”¨æˆ·ä¼šåœ¨å·¥ä½œæ—¶åä¸‹æ¥ä¸Ž AI å¯¹è¯ã€‚\n\nä½†å¦‚æžœ OpenAI å°† GPT-4o é›†æˆåˆ°æ‰‹æœºã€æ±½è½¦æˆ–åƒ Amazon Echo è¿™æ ·çš„æ™ºèƒ½è®¾å¤‡çš„è¯­éŸ³ç•Œé¢ä¸­ï¼Œæˆ‘å¯ä»¥å¾ˆå®¹æ˜“åœ°æƒ³è±¡è¿™ä¸ªç³»ç»Ÿçš„æƒ…æ„Ÿèƒ½åŠ›å˜å¾—æ›´åŠ æœ‰ç”¨ã€‚\n\nå³ä½¿äººä»¬ä¸å¤ªæƒ³ä¸Ž ChatGPT äº¤è°ˆï¼ŒåŽŸç”Ÿå¤šæ¨¡æ€éŸ³é¢‘å’Œè§†è§‰æ¨¡åž‹çš„æ–°èƒ½åŠ›å¯¹äºŽåœ¨ OpenAI çŽ°æœ‰ API ä¸Šæž„å»ºåº”ç”¨ç¨‹åºçš„å¼€å‘è€…æ¥è¯´ï¼Œå°†æ˜¯æ— æ¯”å¼ºå¤§çš„ã€‚\n\nåœ¨ä»–ä»¬çš„å…¬å‘Šä¸­ï¼ŒOpenAI è¡¨ç¤º GPT-4o å°†é€šè¿‡ä»–ä»¬çŽ°æœ‰çš„å¼€å‘è€…æŽ¥å£æä¾›ã€‚è¯¥ç³»ç»Ÿçš„ä»·æ ¼ä¹Ÿå°†æ¯”ä¹‹å‰çš„ GPT-4 æ¨¡åž‹ä¾¿å®œ 50%ã€‚\n\nä»…è¿™äº›å˜åŒ–å°±éžå¸¸é‡å¤§ã€‚æ— è®ºè¯­éŸ³å…ƒç´ æ˜¯å¦çœŸæ­£æµè¡Œï¼Œé©±åŠ¨å®ƒçš„æ™ºèƒ½ä¹Ÿå°†ä½¿æ•°ç™¾ä¸ªçŽ°æœ‰çš„ GPT-4 é©±åŠ¨åº”ç”¨ç¨‹åºå˜å¾—æ›´èªæ˜Žã€æ›´å¿«ã€æ›´å¥½ï¼Œå¹¶ä¸”è¿è¥æˆæœ¬æ›´ä½Žã€‚\n\næ¢å¥è¯è¯´ï¼Œæ–°ç³»ç»Ÿçš„å¯¹è¯å…ƒç´ å¯èƒ½ä¼šè¢«è®¤ä¸ºæ˜¯ä¸€ä¸ªå¾ˆé…·çš„å™±å¤´ã€‚ä½†å…¶æ½œåœ¨å½±å“å°†æ›´å¾®å¦™ã€æ›´å¹¿æ³›ã€‚\n\næˆ‘å¾ˆæœŸå¾…çœ‹åˆ°çœŸå®žç”¨æˆ·å¦‚ä½•ä¸Ž GPT-4o äº’åŠ¨ã€‚ä»–ä»¬ä¼šæ„Ÿåˆ°ä¸å®‰å—ï¼ŸæƒŠè®¶å—ï¼Ÿå¿ƒåŠ¨å—ï¼Ÿ\n\nä½†æˆ‘æ›´æœŸå¾…çš„æ˜¯å¯åŠ¨æˆ‘çš„ Python IDEï¼Œå°† GPT-4o æ·»åŠ åˆ°æˆ‘å·²ç»ä½¿ç”¨ OpenAI å·¥å…·æž„å»ºçš„åº”ç”¨ç¨‹åºä¸­ã€‚\n\nä¸Žæœºå™¨å¯¹è¯å¾ˆé…·ã€‚ä½†ä¸€ä¸ªèƒ½å¤Ÿç†è§£äººç±»æƒ…æ„Ÿçš„åŽŸç”Ÿå¤šæ¨¡æ€ AI æ¨¡åž‹ï¼Œæˆ‘åªéœ€å‡ è¡Œ Python ä»£ç å°±èƒ½è°ƒç”¨ï¼Œè€Œä¸”æˆæœ¬ä½Žå»‰ï¼Ÿè¿™çœŸçš„å¯èƒ½æ”¹å˜ä¸–ç•Œã€‚\n\n**åœ¨è¿‡åŽ»çš„ä¸€å¹´é‡Œï¼Œæˆ‘æµ‹è¯•äº†æ•°åƒä¸ª ChatGPT æç¤ºã€‚ä½œä¸ºå…¨èŒåˆ›ä½œè€…ï¼Œæœ‰ä¸€äº›æˆ‘æ¯å¤©éƒ½ä¼šä½¿ç”¨ï¼Œç¬¦åˆæˆ‘åœ¨æœ¬æ–‡ä¸­æåˆ°çš„ä¼¦ç†ç”¨é€”ã€‚æˆ‘å°†å®ƒä»¬æ±‡ç¼–æˆä¸€æœ¬å…è´¹çš„æŒ‡å—ï¼Œ*7 ä¸ªå¯¹åˆ›ä½œè€…æžå…¶æœ‰ç”¨çš„ ChatGPT æç¤ºã€‚* [ä»Šå¤©å°±èŽ·å–ä¸€ä»½å§ï¼](https://no-frills-influencer.ck.page/6a100e8fe4)**\n\n"},{"lang":"zh","group":"blog","slug":"blog/openai-realtime-api-voice-mode-getting-started-on-colab-39b93edcaa6a","frontmatter":{"title":"OpenAI å®žæ—¶ APIï¼ˆè¯­éŸ³æ¨¡å¼ï¼‰ï¼ŒColab å…¥é—¨","meta_title":"OpenAI å®žæ—¶ APIï¼ˆè¯­éŸ³æ¨¡å¼ï¼‰ï¼ŒColab å…¥é—¨","description":"æ‚¨éœ€è¦äº†è§£çš„ä¸€åˆ‡ï¼Œä»¥åŠå¯ä»¥åœ¨ Colab ä¸Šè¿è¡Œçš„ OpenAI è¯­éŸ³æ¨¡å¼ API çš„å®žè·µä»‹ç»ã€‚","date":"2024-11-08T00:23:32.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*_-d5zsWWQEzVLZxABTSFWQ.png","categories":["Programming","Voice Assistants","Technology/WebAPI"],"author":"Rifx.Online","tags":["OpenAI","Realtime","API","GPT-4o","Colab"],"draft":false,"slug":"blog/openai-realtime-api-voice-mode-getting-started-on-colab-39b93edcaa6a"},"content":"\n\n\næ‚¨éœ€è¦äº†è§£çš„ä¸€åˆ‡ï¼Œä»¥åŠåœ¨ Colab ä¸Šè¿è¡Œ OpenAI è¯­éŸ³æ¨¡å¼ API çš„åŠ¨æ‰‹ä»‹ç»ã€‚\n\n\n\nOpenAI æœ€æ–°çš„å¼€å‘ä¸ºæˆ‘ä»¬å¸¦æ¥äº† **å®žæ—¶ API**ï¼Œæ—¨åœ¨å…è®¸å¼€å‘è€…åœ¨ä»–ä»¬çš„åº”ç”¨ä¸­åˆ›å»º **å¿«é€Ÿã€æ— ç¼çš„è¯­éŸ³åˆ°è¯­éŸ³ä½“éªŒ**ã€‚è¯¥ API æ—¨åœ¨ç®€åŒ–å¤šæ¨¡æ€å¯¹è¯åŠŸèƒ½çš„å¼€å‘ï¼Œä½¿æž„å»ºè‡ªç„¶çš„å®žæ—¶è¯­éŸ³äº¤äº’å˜å¾—æ›´åŠ å®¹æ˜“ã€‚\n\n**åœ¨è¿™ç¯‡åšå®¢ä¸­ï¼Œ** æˆ‘å°†æ¶µç›–æœ‰å…³æ­¤æ–° API çš„ **ä¸»è¦é—®é¢˜**ï¼ŒåŒ…æ‹¬\n\n* ä»€ä¹ˆæ˜¯å®žæ—¶ APIï¼Œ\n* å¦‚ä½•è®¿é—®å®ƒï¼Œ\n* å®ƒçš„é™åˆ¶å’Œå®šä»·ï¼Œ\n* å¹¶æä¾›ä¸€ä¸ª **Colab æ•™ç¨‹**ï¼Œæ•™æ‚¨å¦‚ä½•å…¥é—¨ã€‚\n\n## ä»€ä¹ˆæ˜¯å®žæ—¶ APIï¼Ÿ\n\n**å®žæ—¶ API** æ˜¯ OpenAI æä¾›çš„å…¬å…±æµ‹è¯•åŠŸèƒ½ï¼Œå…è®¸ä»˜è´¹å¼€å‘è€…åœ¨ä»–ä»¬çš„åº”ç”¨ä¸­é›†æˆå®žæ—¶è¯­éŸ³äº¤äº’ã€‚å®ƒæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€ APIï¼Œèƒ½å¤Ÿå°† **éŸ³é¢‘è¾“å…¥è½¬æ¢ä¸ºè¯­éŸ³å“åº”**ï¼Œå¹¶ä½¿ç”¨å…ˆè¿›çš„ **GPT-4o** æ¨¡åž‹æ¥å®žçŽ°è¿™ä¸€ç›®çš„ã€‚æœ¬è´¨ä¸Šï¼Œå®ƒå…è®¸è¿›è¡Œ **ä½Žå»¶è¿Ÿå¯¹è¯**ï¼Œç±»ä¼¼äºŽè‡ªç„¶çš„äººé™…äº¤äº’ï¼Œç±»ä¼¼äºŽ ChatGPT çš„é«˜çº§è¯­éŸ³æ¨¡å¼ä¸­çœ‹åˆ°çš„åŠŸèƒ½ã€‚\n\nä¹‹å‰ï¼Œå¼€å‘è€…éœ€è¦å°†å¤šä¸ªæ¨¡åž‹æ‹¼æŽ¥åœ¨ä¸€èµ·ä»¥å®žçŽ° **è¯­éŸ³è¯†åˆ«ã€æ–‡æœ¬å¤„ç†å’Œæ–‡æœ¬è½¬è¯­éŸ³ç”Ÿæˆ**ã€‚å®žæ—¶ API å°†è¿™ä¸€åˆ‡éƒ½æ•´åˆåœ¨ä¸€æ¬¡ API è°ƒç”¨ä¸­ï¼Œä»Žè€Œå‡å°‘å»¶è¿Ÿï¼Œæä¾›æ›´ä¸°å¯Œçš„å“åº”ï¼Œå¹¶æ›´ä¸€è‡´åœ°å¤„ç†å£éŸ³å’Œé‡éŸ³ã€‚\n\n**èŠå¤©å®Œæˆ API** ä¹Ÿå¼•å…¥äº†éŸ³é¢‘è¾“å…¥å’Œè¾“å‡ºï¼Œä½†å®ƒæ²¡æœ‰å®žæ—¶ API çš„ä½Žå»¶è¿Ÿä½“éªŒã€‚å› æ­¤ï¼Œå¯¹äºŽè¯­è¨€å­¦ä¹ æˆ–è¯­éŸ³å¯ç”¨åŠ©æ‰‹ç­‰ä½“éªŒï¼Œå®žæ—¶ API æ˜¯æ›´ä¼˜é€‰æ‹©ã€‚\n\n## è®¿é—®å’Œé™åˆ¶\n\nå¯¹ **Realtime API** çš„è®¿é—®ç›®å‰ä½œä¸º **å…¬å¼€æµ‹è¯•ç‰ˆ** æä¾›ç»™ä»˜è´¹å¼€å‘è€…ã€‚\n\n**è™½ç„¶è¯´åœ¨æ¬§æ´²çš„è®¿é—®å—åˆ°é™åˆ¶ï¼Œä½†æˆ‘é€šè¿‡æˆ‘çš„ç¬¬5å±‚OpenAIè´¦æˆ·èƒ½å¤Ÿä½¿ç”¨å®ƒã€‚**\n\nè¯¥APIä½¿ç”¨ **WebSocket** è¿žæŽ¥ï¼Œç¡®ä¿éŸ³é¢‘è¾“å…¥å’Œè¾“å‡ºçš„æµç•…ä½“éªŒã€‚\n\nç›®å‰ï¼Œéœ€è¦æ³¨æ„ä»¥ä¸‹ **é™åˆ¶**ï¼š\n\n* **ä¼šè¯é€ŸçŽ‡é™åˆ¶**ï¼šè¯¥APIå¯¹ç¬¬5å±‚å¼€å‘è€…çš„ä¼šè¯æ•°é‡é™åˆ¶ä¸ºå¤§çº¦ **100ä¸ªåŒæ—¶ä¼šè¯**ã€‚è¾ƒä½Žå±‚çº§çš„å®¹é‡æ›´å°ã€‚æˆªè‡³2024å¹´10æœˆï¼ŒAPIçš„é™åˆ¶ä¸ºæ¯åˆ†é’Ÿ2Mä¸ªä»¤ç‰Œã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*XpAB6WRseRb0iY-edE94xw.png)\n\n* **åŠŸèƒ½**ï¼šæœ€åˆä»…æ”¯æŒ **è¯­éŸ³æ¨¡å¼**ï¼Œä½†OpenAIè®¡åˆ’éšç€æ—¶é—´çš„æŽ¨ç§»æ·»åŠ æ›´å¤šåŠŸèƒ½ï¼Œå¦‚ **è§†é¢‘** å’Œ **è§†è§‰**ã€‚\n* **å¯ç”¨æ€§**ï¼šå®Œæ•´çš„éŸ³é¢‘åŠŸèƒ½å¤„äºŽæµ‹è¯•é˜¶æ®µï¼Œæœªæ¥è®¡åˆ’ä¸ºPythonå’ŒNode.jsè¿›è¡Œ **SDKé›†æˆ**ã€‚\n\n## Realtime API çš„å®šä»·\n\n**å®šä»·**ç»“æž„åˆ†ä¸º **æ–‡æœ¬ä»¤ç‰Œ** å’Œ **éŸ³é¢‘ä»¤ç‰Œ**ï¼š\n\n* **éŸ³é¢‘è¾“å…¥**ï¼šæ¯ç™¾ä¸‡ä»¤ç‰Œ $100ï¼ˆå¤§çº¦ **$0\\.06 æ¯åˆ†é’Ÿ**ï¼‰ã€‚\n* **éŸ³é¢‘è¾“å‡º**ï¼šæ¯ç™¾ä¸‡ä»¤ç‰Œ $200ï¼ˆå¤§çº¦ **$0\\.24 æ¯åˆ†é’Ÿ**ï¼‰ã€‚\n* **æ–‡æœ¬è¾“å…¥**ï¼šæ¯ç™¾ä¸‡ä»¤ç‰Œ $5ã€‚\n* **æ–‡æœ¬è¾“å‡º**ï¼šæ¯ç™¾ä¸‡ä»¤ç‰Œ $20ã€‚\n\nè¿™ä¸€å®šä»·ä½¿å¾—å¼€å‘è€…èƒ½å¤Ÿè´Ÿæ‹…å¾—èµ·åˆ›å»ºå¼ºå¤§çš„ **è¯­éŸ³åˆ°è¯­éŸ³** ä½“éªŒï¼Œå°½ç®¡éŸ³é¢‘åŠŸèƒ½çš„æˆæœ¬æ˜¾è‘—é«˜äºŽåŸºäºŽæ–‡æœ¬çš„äº¤äº’ã€‚åœ¨æ‰©å±•å…·æœ‰è¯­éŸ³åŠŸèƒ½çš„åº”ç”¨æ—¶ï¼Œè¿™ä¸€ç‚¹éžå¸¸é‡è¦ã€‚\n\nè¿™ä»ç„¶æ¯”å¤–åŒ…ç»™æŸäº›å›½å®¶ç¨è´µï¼Œä½†æˆ‘ä»¬å¯ä»¥æœŸå¾…åœ¨æŽ¥ä¸‹æ¥çš„å…­ä¸ªæœˆå†…ä»·æ ¼ä¼šæ˜¾è‘—ä¸‹é™ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*ocwFDXEt8X7KD_k6)\n\n## åœ¨ Google Colab ä¸­ä½¿ç”¨ Realtime API æž„å»º\n\nè¿™æ˜¯ä¸€ä¸ªåŸºæœ¬çš„ **Colab æŒ‡å—**ï¼Œå¸®åŠ©æ‚¨å¼€å§‹ä¸Šä¼ æ–‡ä»¶ã€å‘ Realtime API å‘é€è¯·æ±‚å¹¶ç”ŸæˆéŸ³é¢‘å“åº”ã€‚\n\nåœ¨è¿™ä¸ªæ¼”ç¤ºä¸­ï¼Œæˆ‘ä»¬é€‰æ‹©ä¸Šä¼ ä¸€ç³»åˆ—éŸ³é¢‘ç‰‡æ®µï¼Œä»¥æ¨¡æ‹Ÿå¯¹è¯ã€‚\n\n**å®Œæ•´çš„ Colab ä»£ç **ï¼š [é“¾æŽ¥åœ¨è¿™é‡Œ](https://colab.research.google.com/drive/1-bj_LH7Gv2bbTJopbo7Hk_AIyDAuqeEQ?usp=sharing)ï¼Œåªéœ€å°†æ‚¨çš„ â€œopenaiâ€ å¯†é’¥æ·»åŠ åˆ° Colab çš„ç§˜å¯†ä¸­å¹¶è¿è¡Œè¯¥ Colabã€‚\n\n### ç¬¬ä¸€æ­¥ï¼šè®¾ç½® Google Colab å’Œä¾èµ–é¡¹\n\n* å¼€å§‹ä¸€ä¸ªæ–°çš„ **Google Colab** ç¬”è®°æœ¬ã€‚\n* å®‰è£…å¿…è¦çš„åº“ï¼Œä¾‹å¦‚ **requests** å’Œ **pydub** æ¥ç®¡ç†éŸ³é¢‘æ–‡ä»¶ã€‚\n\n\n```python\n#Setup\n!pip install websockets pydub --quiet \n\nimport base64\nimport numpy as np\nimport soundfile as sf\nimport json\nimport websockets\nfrom google.colab import files\nfrom pydub import AudioSegment\nfrom tqdm import tqdm\nimport io\n```\n\n### æ­¥éª¤ 2ï¼šä¸Šä¼ éŸ³é¢‘æ–‡ä»¶\n\nåœ¨ Colab ä¸­ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ **google.colab** çš„ `files` æ¨¡å—æ¥ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ã€‚\n\n```python\n#Upload audio\ndef upload_audio():\n    uploaded = files.upload()  \n    for file_name in uploaded.keys():\n        return file_name\n\naudio_file = upload_audio()\n```\n\n### ç¬¬ 3 æ­¥ï¼šå‘å®žæ—¶ API å‘é€è¯·æ±‚\n\n* åœ¨å°†éŸ³é¢‘æ–‡ä»¶å‘é€ç»™ OpenAI ä¹‹å‰ï¼Œæ­£ç¡®æ ¼å¼åŒ–éŸ³é¢‘æ–‡ä»¶ã€‚\n* å»ºç«‹ WebSocket è¿žæŽ¥ä»¥æµå¼ä¼ è¾“éŸ³é¢‘æ–‡ä»¶ã€‚\n* ä½¿ç”¨ `tqdm` æ˜¾ç¤ºä¸Šä¼ æµçš„è¿›åº¦ã€‚\n* è¯¥å‡½æ•°è¿”å›žå®Œæ•´çš„äº‹ä»¶é›†ï¼ˆåŒ…æ‹¬å“åº”ï¼‰ï¼Œä»¥ä¾¿åŽç»­å¤„ç†ç”Ÿæˆè¾“å‡ºéŸ³é¢‘ã€‚å®ƒè¿˜è¿”å›žæ¨¡åž‹å“åº”çš„è½¬å½•æ–‡æœ¬ã€‚\n\n```python\n#Helper functions\n## Function to convert Float32Array to PCM16 format\ndef float_to_pcm16(float32_array):\n    return np.clip(float32_array * 32767, -32768, 32767).astype(np.int16).tobytes()\n\n## Function to split audio into base64-encoded PCM16 chunks\ndef float32_to_base64_chunks(float32_array, chunk_size=32000):\n    pcm16_data = float_to_pcm16(float32_array)\n    for i in range(0, len(pcm16_data), chunk_size):\n        yield base64.b64encode(pcm16_data[i:i+chunk_size]).decode('utf-8')\n\n## WebSocket connection and streaming audio with text prompt\n## Main function to call OpenAI Realtime API\nasync def stream_audio_to_realtime_api(audio_file, text_prompt, openai_key, verbose = False):\n    data, samplerate = sf.read(audio_file, dtype='float32')\n    if data.ndim > 1:\n        data = data[:, 0]\n    if samplerate != 24000:\n        raise ValueError(f\"Audio must be sampled at 24kHz, but it is {samplerate}Hz\")\n\n    url = \"wss://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2024-10-01\"\n    headers = {\"Authorization\": \"Bearer \" + openai_key, \"OpenAI-Beta\": \"realtime=v1\"}\n\n    async with websockets.connect(url, extra_headers=headers) as ws:\n        await ws.send(json.dumps({\n            \"type\": \"conversation.item.create\",\n            \"item\": {\"type\": \"message\", \"role\": \"user\", \"content\": [{\"type\": \"input_text\", \"text\": text_prompt}]}\n        }))\n\n        with tqdm(total=(len(float_to_pcm16(data)) + 32000 - 1) // 32000, desc=\"Sending Audio Chunks\") as pbar:\n            for chunk in float32_to_base64_chunks(data):\n                await ws.send(json.dumps({\"type\": \"input_audio_buffer.append\", \"audio\": chunk}))\n                pbar.update(1)\n\n        await ws.send(json.dumps({\"type\": \"input_audio_buffer.commit\"}))\n        await ws.send(json.dumps({\"type\": \"response.create\"}))\n\n        all_events = []\n        while True:\n            response = await ws.recv()\n            event = json.loads(response)\n            all_events.append(event)\n            if verbose:\n                print(event)\n            if event[\"type\"] == \"response.output_item.done\" and \"item\" in event and \"content\" in event[\"item\"]:\n                for content in event[\"item\"][\"content\"]:\n                    if content[\"type\"] == \"audio\" and \"transcript\" in content:\n                        transcript = content[\"transcript\"]\n                        break\n            if event[\"type\"] == \"rate_limits.updated\":\n                break\n\n        return all_events, transcript\n```\n\n```python\n#Add a prompt and call OpenAI Realtime API\ntext_prompt = \"Summarize this audio content\"\n\nevents, transcript = await stream_audio_to_realtime_api(\n    audio_file, \n    text_prompt, \n    openai_key, \n    verbose = False \n#to display OpenAI's response as they arrive, use verbose = True\n    ) \n```\n\n### ç¬¬4æ­¥ï¼šç”ŸæˆéŸ³é¢‘å“åº”\n\n* ä¸€æ—¦æ”¶åˆ°å“åº”ï¼Œç”ŸæˆéŸ³é¢‘ã€‚\n* é€‰æ‹©ä¸€ä¸ªæ–‡ä»¶åå¹¶ä¿å­˜æ–‡ä»¶ã€‚\n* ç„¶åŽæ‚¨å°†èƒ½å¤Ÿä¸‹è½½è¯¥æ–‡ä»¶ã€‚\n\n\n```python\n## Function to decode and concatenate audio chunks into a full audio file\ndef generate_audio_from_chunks(audio_chunks, output_filename=None):\n    # Concatenate the base64-encoded audio chunks from the 'delta' field\n    full_audio_base64 = ''.join(audio_chunks)\n\n    # Decode the concatenated base64 string to raw PCM16 audio bytes\n    audio_bytes = base64.b64decode(full_audio_base64)\n\n    # Load the bytes as a pydub AudioSegment (assuming 24kHz, 1 channel, PCM16)\n    audio_segment = AudioSegment.from_raw(\n        io.BytesIO(audio_bytes), \n        sample_width=2, \n        frame_rate=24000, \n        channels=1)\n\n    # Optionally save the audio to a file\n    if output_filename:\n        audio_segment.export(output_filename, format=\"wav\")\n        print(f\"Audio saved to {output_filename}\")\n\n    return audio_segment\n```\n\n```python\n#Extract audio chunks from the collected events\naudio_output_chunks = [event['delta'] for event in events if event['type'] == 'response.audio.delta']\n\n## Generate the full audio from the collected chunks\ngenerated_audio = generate_audio_from_chunks(audio_output_chunks, output_filename=\"output_audioo.wav\")\n```\n\n## ç»“è®º\n\né€šè¿‡ä¸Šè¿°æ­¥éª¤ï¼Œæ‚¨å¯ä»¥å°† OpenAI çš„å®žæ—¶ API é›†æˆåˆ° Colab ç¬”è®°æœ¬ä¸­ï¼Œå®žçŽ°æ— ç¼çš„è¯­éŸ³æŒ‡ä»¤ã€‚\n\næœ¬æŒ‡å—åº”ä¸ºæ‚¨æä¾›ä¸€ä¸ªåšå®žçš„åŸºç¡€ï¼Œä»¥ä¾¿æ‚¨å®žéªŒå®žæ—¶éŸ³é¢‘åˆ°éŸ³é¢‘çš„äº¤äº’ï¼Œå¹¶æž„å»ºåˆ›æ–°çš„è¯­éŸ³é©±åŠ¨åº”ç”¨ç¨‹åºã€‚\n\n"},{"lang":"zh","group":"blog","slug":"blog/openai-rolls-out-searchgpt-to-more-users-33024ff3132c","frontmatter":{"title":"OpenAI å‘æ›´å¤šç”¨æˆ·æŽ¨å‡º SearchGPT","meta_title":"OpenAI å‘æ›´å¤šç”¨æˆ·æŽ¨å‡º SearchGPT","description":"ChatGPT çš„ç”¨æˆ·ç•Œé¢è¿›è¡Œäº†å¤§è§„æ¨¡é‡æ–°è®¾è®¡ï¼Œä»¥æ”¯æŒ SearchGPTâ€”â€”çŽ°åœ¨å®ƒç±»ä¼¼äºŽ Google å’Œ Perplexity ç­‰æœç´¢å¼•æ“Žã€‚","date":"2024-11-01T03:57:02.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BW6Qt6PMwHwlYRljAIBQWg.jpeg","categories":["Chatbots","Technology/Web","SearchGPT"],"author":"Rifx.Online","tags":["SearchGPT","ChatGPT","web","search","publishers"],"draft":false,"slug":"blog/openai-rolls-out-searchgpt-to-more-users-33024ff3132c"},"content":"\n\n\n\n\n**ä½ æ³¨æ„åˆ° OpenAI æœ€è¿‘å¯¹ ChatGPT çš„é‡æ–°è®¾è®¡äº†å—ï¼Ÿ**\n\nå¦‚æžœä½ æœ€è¿‘ç™»å½•è¿‡ï¼Œä½ å¯èƒ½ä¼šå‘çŽ°ä¸¤ä¸ªä¸»è¦å˜åŒ–ã€‚\n\n* é¦–å…ˆï¼Œæ–°çš„ [**Canvas**](https://generativeai.pub/openai-rolls-out-canvas-in-chatgpt-a-brand-new-writing-and-coding-interface-7b57a3ec582a) åŠŸèƒ½ä¼šè‡ªåŠ¨åœ¨å³ä¾§æ‰“å¼€ä¸€ä¸ªæ–°ç•Œé¢ã€‚è¿™ä¸ªæ–°å¢žåŠŸèƒ½è®©ä½ å¯ä»¥å¤„ç†æ›´é•¿çš„æ–‡æ¡£ï¼Œè€Œæ— éœ€åœ¨èŠå¤©ä¸­ä¸Šä¸‹æ»šåŠ¨ã€‚è¿™æ˜¯ä¸€ä¸ªå°è€Œå®žç”¨çš„æ›´æ–°ã€‚\n* å…¶æ¬¡ï¼Œ**æç¤ºå­—æ®µ**å·²ç»ä¸Šç§»ï¼ŒçŽ°åœ¨ä½äºŽå±å¹•ä¸­å¤®ã€‚\n\nè¯·çœ‹ä¸‹é¢æœ€æ–°çš„ç”¨æˆ·ç•Œé¢ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*KsYMU9ffVlKsmHzKIOKH8g.png)\n\nä½ æ³¨æ„åˆ°è¿™ä¸ªæ–°å¸ƒå±€ä¸Ž Google å’Œ Perplexity AI çš„ç›¸ä¼¼ä¹‹å¤„äº†å—ï¼ŸChatGPT çŽ°åœ¨çœ‹èµ·æ¥åƒä¸€ä¸ªæœç´¢å¼•æ“Žã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*xFKErUHnfbJunaKi4NvM9A.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*PgSwS14lkUtZe7Ra9oLCrg.png)\n\nçŽ°åœ¨ï¼Œå½“ä½ åœ¨é”®ç›˜ä¸ŠæŒ‰ä¸‹ â€˜/â€™ é”®æ—¶ï¼Œä½ å¯ä»¥åˆ‡æ¢ä¸€ä¸ªæ–°çš„â€œæœç´¢â€åŠŸèƒ½ï¼Œè®© ChatGPT è®¿é—®ç½‘ç»œã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*oYxvVvsuUuc_PM0PXRmN7A.png)\n\nè®©æˆ‘ä»¬æ¥åˆ†æžä¸€ä¸‹è¿™æ„å‘³ç€ä»€ä¹ˆã€‚\n\n## ChatGPTä¸­çš„æœç´¢åŠŸèƒ½æ˜¯ä»€ä¹ˆï¼Ÿ\n\n[SearchGPT](https://generativeai.pub/openai-announces-search-gpt-is-this-the-google-killer-5919ba31f95b) å…è®¸ChatGPTè®¿é—®å®žæ—¶ç½‘é¡µæ•°æ®ã€‚å®ƒçš„å·¥ä½œæ–¹å¼ç±»ä¼¼äºŽPerplexityï¼Œä½¿ç”¨å¤§åž‹è¯­è¨€æ¨¡åž‹ä¸ºæ‚¨æœç´¢ç½‘ç»œï¼Œæä¾›å³æ—¶ç­”æ¡ˆï¼Œå¹¶åŒ…å«å…¶å¼•ç”¨çš„æ¥æºã€‚\n\nè¯¥åŠŸèƒ½æœ€åˆå‘10,000åç”¨æˆ·å¼€æ”¾ï¼Œå¹¶ä¸ºå¸Œæœ›æå‰è®¿é—®çš„äººæ·»åŠ äº†å€™è¡¥åå•è¡¨å•ã€‚\n\nOpenAIä¸Ž**åŽå°”è¡—æ—¥æŠ¥ã€ç¾Žå›½å¹¿æ’­å…¬å¸ã€Vox Mediaå’Œæ—¶ä»£æ‚å¿—**ç­‰çŸ¥åå‡ºç‰ˆå•†åˆä½œï¼Œä»¥ç¡®ä¿ç”¨æˆ·èŽ·å¾—å¯ä¿¡ã€å¯é çš„ä¿¡æ¯ã€‚\n\n> â€œäººå·¥æ™ºèƒ½æœç´¢å°†æˆä¸ºäººä»¬æµè§ˆäº’è”ç½‘çš„å…³é”®æ–¹å¼ä¹‹ä¸€ï¼Œåœ¨è¿™äº›æ—©æœŸé˜¶æ®µï¼ŒæŠ€æœ¯çš„æž„å»ºæ–¹å¼è‡³å…³é‡è¦ï¼Œå®ƒå¿…é¡»é‡è§†ã€å°Šé‡å’Œä¿æŠ¤æ–°é—»ä¸šå’Œå‡ºç‰ˆå•†ã€‚æˆ‘ä»¬æœŸå¾…ä¸ŽOpenAIåœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­åˆä½œï¼Œä¸ºè¯»è€…åˆ›é€ ä¸€ç§æ–°çš„å‘çŽ°ã€Šå¤§è¥¿æ´‹æœˆåˆŠã€‹çš„æ–¹å¼ã€‚â€ â€” Nicholas Thompson, ã€Šå¤§è¥¿æ´‹æœˆåˆŠã€‹é¦–å¸­æ‰§è¡Œå®˜\n\nå½“æ‚¨å‘SearchGPTæé—®æ—¶ï¼Œå®ƒå¹¶ä¸æ˜¯ä»Žéšæœºæ¥æºæå–ä¿¡æ¯ã€‚æ¯ä¸ªå“åº”éƒ½é™„æœ‰**æ¸…æ™°çš„å†…è”å¼•ç”¨å’Œé“¾æŽ¥**ï¼Œå› æ­¤æ‚¨ç¡®åˆ‡çŸ¥é“ä¿¡æ¯æ¥æºäºŽä½•å¤„ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*uchpKOXqZCG55HSNkaOOZQ.png)\n\næ‚¨ç”šè‡³å¯ä»¥é€šè¿‡ç‚¹å‡»å‡ºçŽ°åœ¨æœç´¢ç½‘ç«™ä¸‹æ‹‰æ¡†ä¸­çš„æºé“¾æŽ¥æ·±å…¥äº†è§£ï¼Œæä¾›æ›´å¤šæŽ¢ç´¢ä¸»é¢˜çš„æ–¹å¼ã€‚\n\n## å¦‚ä½•è®¿é—® SearchGPT\n\nè®¿é—® SearchGPT éžå¸¸ç®€å•ã€‚å½“æ‚¨åœ¨ ChatGPT ä¸­æ—¶ï¼ŒæŒ‰ä¸‹ **â€˜/â€™** é”®å¹¶ä»Žèœå•ä¸­é€‰æ‹©æœç´¢é€‰é¡¹ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*xNfm-6zPFzdXGL2A0D92YQ.png)\n\nå®ƒçš„å·¥ä½œæ–¹å¼ä¸Žå…¶ä»–æœç´¢å¼•æ“Žç±»ä¼¼ï¼šæ‚¨æå‡ºé—®é¢˜ï¼Œå‡ ç§’é’Ÿå†…ï¼ŒSearchGPT å°±ä¼šæä¾›ç­”æ¡ˆï¼Œå¹¶é™„ä¸Šæ¥æºã€‚\n\næ‚¨ç”šè‡³å¯ä»¥æå‡ºåŽç»­é—®é¢˜ä»¥æ›´æ·±å…¥åœ°æŽ¢è®¨ä¸»é¢˜ã€‚è¿™åˆ›é€ äº†ä¸€ç§å¯¹è¯å¼çš„æœç´¢ä½“éªŒï¼Œæ¯”ä¼ ç»Ÿæœç´¢ç»“æžœçš„æ»šåŠ¨æ›´åŠ äº’åŠ¨ã€‚\n\n## SearchGPT vs. Perplexity vs. Google\n\né‚£ä¹ˆï¼ŒSearchGPT ä¸Ž **Perplexity AI** å’Œ **Google** ç›¸æ¯”å¦‚ä½•å‘¢ï¼Ÿ\n\n**SearchGPT** çš„è®¾è®¡æ—¨åœ¨ä¸ºæ‚¨æä¾›ç®€æ´ä¸”æ¥æºæ˜Žç¡®çš„ç­”æ¡ˆã€‚æ¯ä¸ªç­”æ¡ˆéƒ½æœ‰é“¾æŽ¥åˆ°åŽŸå§‹æ¥æºï¼Œæ‚¨å¯ä»¥ç‚¹å‡»ä»¥éªŒè¯ä¿¡æ¯ã€‚å®ƒéžå¸¸é€‚åˆå®žæ—¶å›žç­”å’Œå¿«é€Ÿäº‹å®žæ£€æŸ¥ã€‚\n\næ­¤å¤–ï¼Œé€šè¿‡åŽç»­é—®é¢˜ï¼Œæ‚¨å¯ä»¥åœ¨ä¸é‡æ–°å¼€å§‹çš„æƒ…å†µä¸‹ä¼˜åŒ–æŸ¥è¯¢ã€‚è¿™ç§å¯¹è¯æ€§è´¨ä½¿å…¶æ„Ÿè§‰åƒæ˜¯åœ¨ä¸Žä¸€ä¸ªè®°å¾—æ‚¨æ‰€é—®é—®é¢˜çš„è¶…çº§å…ˆè¿›ç‰ˆæœ¬çš„ Google äº¤è°ˆã€‚\n\nå¦ä¸€æ–¹é¢ï¼Œ**Perplexity** åˆ™æ˜¯ä¸€ä¸ªæ›´å­¦æœ¯é£Žæ ¼çš„æœç´¢å¼•æ“Žã€‚å®ƒå¼ºè°ƒå­¦æœ¯æ–‡ç« å’Œè¯¦ç»†ç ”ç©¶ï¼Œè¿™å¯¹äºŽæ›´æ·±å…¥çš„æŸ¥è¯¢éžå¸¸æœ‰ç”¨ã€‚Perplexity é€šå¸¸æ›´é€‚åˆéœ€è¦æ›´æ·±å±‚æ¬¡æ¥æºçš„ç ”ç©¶å¯†é›†åž‹ä»»åŠ¡ã€‚\n\nå½“ç„¶ï¼Œ**Google** ä»ç„¶æ˜¯è¿™ä¸ªé¢†åŸŸçš„å·¨å¤´ã€‚å°½ç®¡ä»–ä»¬æœ€è¿‘åŠªåŠ›å°†ç”Ÿæˆå¼ AI çº³å…¥æœç´¢ç»“æžœï¼Œä½†è¿˜æ²¡æœ‰å®Œå…¨å®žçŽ°ç”¨æˆ·æ‰€æœŸæœ›çš„æ— ç¼ä½“éªŒã€‚\n\nGoogle çš„ç”Ÿæˆæœç´¢æŽ¨å‡ºè¿‡ç¨‹ç¬¨æ‹™ï¼Œå¹¶å› é”™è¯¯å’Œä¸ç›¸å…³çš„å›žç­”è€Œå—åˆ°å¤§é‡æ‰¹è¯„ã€‚ä½† Google çš„ä¿¡æ¯å¹¿åº¦å’ŒåŸºç¡€è®¾æ–½ä»ç„¶æ— ä¸Žä¼¦æ¯”ã€‚\n\n## è¿™æ˜¯è°·æ­Œçš„ç»ˆç»“å—ï¼Ÿ\n\nè°·æ­Œä¸ä¼šå¾ˆå¿«æ¶ˆå¤±ã€‚è¿™å®¶ç§‘æŠ€å·¨å¤´ä»ç„¶æŽ§åˆ¶ç€è¶…è¿‡90%çš„æœç´¢å¸‚åœºã€‚ä»–ä»¬å·²ç»åœ¨è¿™ä¸ªé¢†åŸŸå¥‹æ–—äº†æ•°åå¹´ï¼Œæœç´¢ç®—æ³•ä¹Ÿåœ¨ä¸æ–­æ¼”å˜ã€‚\n\nç„¶è€Œï¼Œéšç€åƒSearchGPTè¿™æ ·çš„AIæœç´¢å¼•æ“Žé€æ¸å´­éœ²å¤´è§’ï¼Œè°·æ­Œé¢ä¸´ç€æå‡è‡ªèº«ç«žäº‰åŠ›çš„åŽ‹åŠ›ã€‚OpenAIä¸Žå‡ºç‰ˆå•†åˆä½œä»¥èŽ·å–å¯é æ¥æºçš„ä¸¾åŠ¨æ˜¯ä¸€ä¸ªæ˜Žæ™ºçš„ç­–ç•¥ï¼Œå¯èƒ½ä¼šå‰Šå¼±è°·æ­Œçš„ä¸»å¯¼åœ°ä½ã€‚\n\nè¿™ç§å¯¹éªŒè¯ç»“æžœçš„å…³æ³¨æ„å‘³ç€ï¼Œå½“ä½ ä½¿ç”¨SearchGPTæ—¶ï¼Œä½ ä¸å¤ªå¯èƒ½é‡åˆ°è™šå‡ç­”æ¡ˆâ€”â€”è¿™æ˜¯è¿‡åŽ»AIé©±åŠ¨å·¥å…·æ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚\n\næ­¤å¤–ï¼Œè°·æ­Œä»ç„¶æ˜¯å¤§å¤šæ•°äººçš„é»˜è®¤é€‰æ‹©ã€‚å®ƒçš„ä¼˜åŠ¿åœ¨äºŽæ— å¤„ä¸åœ¨â€”â€”ä»Žä½ æ‰‹æœºçš„æµè§ˆå™¨åˆ°æ™ºèƒ½éŸ³ç®±ã€‚SearchGPTä»å¤„äºŽæ—©æœŸé˜¶æ®µï¼Œéœ€è¦æ—¶é—´æ¥èŽ·å¾—ç”¨æˆ·çš„ä¿¡ä»»ã€‚\n\n## SearchGPT è¿˜æœªè¾¾åˆ°\n\næˆ‘åœ¨è¿‡åŽ»å‡ ä¸ªå°æ—¶é‡Œæµ‹è¯•äº† SearchGPTï¼Œä»¥ä¸‹æ˜¯æˆ‘çš„ä¸€äº›è§‚å¯Ÿï¼š\n\n* **ç­”æ¡ˆè´¨é‡ï¼š** ä¸€ä¸ªä¸»è¦çš„ç¼ºç‚¹æ˜¯ SearchGPT çš„ç­”æ¡ˆè´¨é‡ä¸Ž Perplexity Pro çš„æ·±åº¦æˆ–ç²¾ç¡®åº¦ä¸å¤ªåŒ¹é…ã€‚å°½ç®¡å®ƒä¸Ž Perplexity çš„åŸºç¡€ç‰ˆæœ¬ç›¸å½“ï¼Œä½†ä¾èµ–äºŽå®ƒè¿›è¡Œæ›´å¤æ‚æˆ–ç»†è‡´æŸ¥è¯¢çš„ç”¨æˆ·ä¼šæ³¨æ„åˆ°å·®å¼‚ã€‚\n* **å“åº”ç¼“æ…¢ï¼š** å¦ä¸€ä¸ªç—›ç‚¹æ˜¯é€Ÿåº¦ã€‚åœ¨ä½¿ç”¨ SearchGPT æ—¶ï¼Œå¤„ç†æŸ¥è¯¢å¹¶è¿”å›žç­”æ¡ˆæ‰€éœ€çš„æ—¶é—´å¯èƒ½æ˜¾å¾—æžå…¶ç¼“æ…¢ã€‚è¿™ç§å»¶è¿Ÿæ‰“æ–­äº†äº’åŠ¨çš„æµç•…æ€§ï¼Œå°¤å…¶æ˜¯åœ¨ä½ æ·±å…¥æŽ¢è®¨æŸä¸ªä¸»é¢˜æ—¶ã€‚\n* **ç¼ºä¹ä¸Šä¸‹æ–‡ç†è§£ï¼š** åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå®ƒæœªèƒ½è¯†åˆ«å¯¹è¯çš„è¿žç»­æ€§ã€‚å¦‚æžœä½ æå‡ºåŽç»­é—®é¢˜ï¼Œæ¨¡åž‹å¾€å¾€å°†å…¶è§†ä¸ºä¸€ä¸ªæ–°çš„ç‹¬ç«‹é—®é¢˜ï¼Œè€Œä¸æ˜¯ç†è§£ä¸ºä½ ä¹‹å‰æŸ¥è¯¢çš„ä¸Šä¸‹æ–‡ã€‚\n* **æ²¡æœ‰åŽç»­å»ºè®®ï¼š** ä¸Ž Perplexity ä¸åŒï¼ŒåŽè€…é€šå¸¸ä¼šå»ºè®®åŽç»­é—®é¢˜ä»¥å¸®åŠ©ä½ ç»†åŒ–æœç´¢ï¼ŒSearchGPT å¹¶ä¸æä¾›æ­¤åŠŸèƒ½ã€‚è¿™ç§ç¼ºä¹æŒ‡å¯¼ä½¿ç”¨æˆ·ä¸å¾—ä¸è‡ªè¡Œæƒ³å‡ºå¦‚ä½•æœ€å¥½åœ°æŽªè¾žæˆ–ç¼©å°æŸ¥è¯¢èŒƒå›´ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*vfXxpgENLyenLY2l33PKiw.png)\n\nåœ¨ä½¿ç”¨æœç´¢åŠŸèƒ½æ—¶ï¼Œæˆ‘æ³¨æ„åˆ°å¦ä¸€ä¸ªå¥‡æ€ªçš„å·¥ä½œæµç¨‹ï¼šå¦‚æžœå°†è¯­è¨€æ¨¡åž‹ä»Ž GPT-4o åˆ‡æ¢åˆ°â€œChatGPT o1-previewâ€ï¼Œæœç´¢æŒ‡ç¤ºå™¨ä»ç„¶å­˜åœ¨ï¼Œä½†å®žé™…ä¸Šå¹¶ä¸ä¼šåœ¨ç½‘ä¸Šæœç´¢ç»“æžœã€‚\n\nå®ƒè¿”å›žçš„æ˜¯å…¶é¢†åŸŸçŸ¥è¯†ä¸­çš„ç»“æžœï¼Œè¿™å¹¶ä¸æ˜¯ç”¨æˆ·æ‰€æœŸæœ›çš„ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*YcI-UuEmmFTQPUlO6mjpxA.png)\n\næ­£ç¡®çš„è¡Œä¸ºåº”è¯¥æ˜¯åœ¨ç”¨æˆ·åˆ‡æ¢åˆ°â€œChatGPT o1-previewâ€åŽç¦ç”¨ *search* åŠŸèƒ½ï¼Œå› ä¸ºè¯¥æ¨¡åž‹æ²¡æœ‰èƒ½åŠ›åœ¨ç½‘ä¸Šæœç´¢ã€‚\n\n## æœ€åŽçš„æ€è€ƒ\n\næˆ‘å¾ˆé«˜å…´OpenAIç»ˆäºŽæŽ¨å‡ºäº†SearchGPTã€‚è‡ªä»Žä»–ä»¬åœ¨2024å¹´7æœˆå®£å¸ƒè¿™ä¸€ç‚¹ä»¥æ¥ï¼Œæˆ‘å°±ä¸€ç›´æƒ³æµ‹è¯•å®ƒã€‚\n\nåœ¨ç›®å‰çš„çŠ¶æ€ä¸‹ï¼ŒSearchGPTæ˜¯OpenAIè¿ˆå…¥AIé©±åŠ¨æœç´¢ä¸–ç•Œçš„è‰¯å¥½ç¬¬ä¸€æ­¥ï¼Œä½†å®ƒè¿˜ä¸å¤Ÿæˆç†Ÿï¼Œæ— æ³•æˆä¸ºä»»ä½•äººå¤„ç†å¤æ‚å®žæ—¶æŸ¥è¯¢çš„é¦–é€‰å·¥å…·ã€‚\n\nå‡†ç¡®æ€§ã€é€Ÿåº¦å’Œå¤„ç†å¯¹è¯ä¸Šä¸‹æ–‡çš„èƒ½åŠ›è¿˜ä¸å¤Ÿã€‚ç›®å‰ï¼Œå¦‚æžœæ‚¨éœ€è¦æ·±å…¥çš„è§è§£æˆ–æ›´å¿«çš„ç»“æžœï¼ŒåƒPerplexity Proæˆ–Googleè¿™æ ·çš„å·¥å…·ä»ç„¶æ˜¯æ›´å¥½çš„é€‰æ‹©ã€‚\n\nè¿›ä¸€æ­¥é˜…è¯»ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*5ejBBgbZaE8pGmpW.png)\n\næœ¬æ–‡å‘å¸ƒåœ¨[Generative AI](https://generativeai.pub/)ã€‚è¯·åœ¨[LinkedIn](https://www.linkedin.com/company/generative-ai-publication)ä¸Šä¸Žæˆ‘ä»¬è”ç³»ï¼Œå¹¶å…³æ³¨[Zeniteq](https://www.zeniteq.com/)ï¼Œä»¥èŽ·å–æœ€æ–°çš„AIæ•…äº‹ã€‚\n\nè®¢é˜…æˆ‘ä»¬çš„[æ–°é—»é€šè®¯](https://www.generativeaipub.com/)å’Œ[YouTube](https://www.youtube.com/@generativeaipub)é¢‘é“ï¼Œä»¥èŽ·å–æœ‰å…³ç”ŸæˆAIçš„æœ€æ–°æ–°é—»å’Œæ›´æ–°ã€‚è®©æˆ‘ä»¬ä¸€èµ·å¡‘é€ AIçš„æœªæ¥ï¼\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*TnRFuKk-2Dj_KCAP.png)\n\n"},{"lang":"zh","group":"blog","slug":"blog/openai-searchgpt-chatgpt-with-internet-and-browsing-tools-023ddca7cb44","frontmatter":{"title":"OpenAI SearchGPTï¼šå¸¦æœ‰äº’è”ç½‘å’Œæµè§ˆå·¥å…·çš„ChatGPT","meta_title":"OpenAI SearchGPTï¼šå¸¦æœ‰äº’è”ç½‘å’Œæµè§ˆå·¥å…·çš„ChatGPT","description":"Perplexity å’Œ Google æœç´¢çš„æ›´å¥½æ›¿ä»£æ–¹æ¡ˆ","date":"2024-11-08T00:28:30.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*N_EtjjOxkx6QsKRLx5f_cQ.png","categories":["Technology/Web","Data Science","SearchGPT"],"author":"Rifx.Online","tags":["SearchGPT","filtering","citations","recommendations","customization"],"draft":false,"slug":"blog/openai-searchgpt-chatgpt-with-internet-and-browsing-tools-023ddca7cb44"},"content":"\n\n\n### ä¸€ä¸ªæ›´å¥½çš„æ›¿ä»£æ–¹æ¡ˆï¼šPerplexity å’Œ Google æœç´¢\n\n\n\nå¤‡å—æœŸå¾…çš„ OpenAI äº§å“ SearchGPT æ˜¨æ™šå‘å¸ƒï¼Œæ‹¥æœ‰ä¸€äº›é‡å¤§åŠŸèƒ½ï¼Œä½¿å…¶åœ¨ç«žäº‰å¯¹æ‰‹ Perplexity ä¹‹ä¸Šæ›´è¿›ä¸€æ­¥ã€‚\n\nå¦‚ OpenAI æ‰€å®£å¸ƒçš„ï¼ŒSearchGPT ä¸ä»…ä»…æ˜¯å¸¦æœ‰äº’è”ç½‘çš„ ChatGPTã€‚\n\nå®ƒæœ¬èº«å°±æ˜¯ä¸€ä¸ª AI ç½‘ç»œæµè§ˆå™¨ã€‚\n\nè°ˆåˆ°ä¸€äº›å…³é”®åŠŸèƒ½ï¼š\n\n* **é«˜çº§è¿‡æ»¤**ï¼šä¸ºç‰¹å®šæ—¥æœŸã€æ¥æºæˆ–å†…å®¹ç±»åž‹ï¼ˆä¾‹å¦‚ï¼Œä»…é™åŒè¡Œè¯„å®¡æ–‡ç« ã€æ”¿åºœç½‘ç«™ç­‰ï¼‰è®¾ç½®è¿‡æ»¤å™¨ã€‚\n* **ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ‘˜è¦**ï¼šç”Ÿæˆé’ˆå¯¹ç‰¹å®šé¢†åŸŸï¼ˆå¦‚åŒ»å­¦æˆ–é‡‘èžï¼‰çš„æ‘˜è¦ã€å…³é”®è¦ç‚¹æˆ–æ´žå¯Ÿã€‚\n* **å¼•ç”¨ç”Ÿæˆ**ï¼šè‡ªåŠ¨æ ¼å¼åŒ–å¹¶æä¾›å­¦æœ¯é£Žæ ¼çš„å¼•ç”¨ï¼ˆAPAã€MLAï¼‰ã€‚\n* **å¤šæ­¥éª¤æŸ¥è¯¢**ï¼šåœ¨ä¸€æ¬¡æœç´¢ä¸­å¤„ç†å¤æ‚çš„åˆ†å±‚é—®é¢˜ï¼Œè·¨å¤šä¸ªæ¥æºã€‚\n* **æ•°æ®åˆ†æžé›†æˆ**ï¼šç›´æŽ¥æå–å’Œåˆ†æžæ•°æ®ä»¥èŽ·å–æ´žå¯Ÿï¼ˆä¾‹å¦‚ï¼Œè¶‹åŠ¿åˆ†æžï¼‰ã€‚SearchGPT å¯ä»¥è¿žæŽ¥åˆ°ä¸“ä¸šæ•°æ®åº“ï¼Œå…è®¸è®¿é—®ç‰¹å®šé¢†åŸŸï¼ˆå¦‚åŒ»å­¦æœŸåˆŠã€æ³•å¾‹æ¡ˆä¾‹æ•°æ®åº“æˆ–ä¸“æœ‰å•†ä¸šåˆ†æžï¼‰ã€‚\n* **ä¸ªæ€§åŒ–æŽ¨è**ï¼šæ ¹æ®æ‚¨çš„æœç´¢åŽ†å²å»ºè®®ç›¸å…³æ¥æºã€æ–‡ç« æˆ–æ›´æ–°ã€‚è¿™åŒ…æ‹¬ **é¢„è®¾æ¨¡æ¿æˆ–è§’è‰²**ï¼šä¾‹å¦‚ï¼Œå¯ä»¥è®¾ç½®ä¸€ä¸ªä»¥ç ”ç©¶ä¸ºé‡ç‚¹çš„â€œSearchGPTâ€ä»¥æ£€ç´¢ç§‘å­¦æ•°æ®å¹¶ç›´æŽ¥æä¾›å­¦æœ¯å¼•ç”¨ã€‚\n\n### ChatGPTä¹‹å‰æ— æ³•è®¿é—®äº’è”ç½‘å—ï¼Ÿ\n\nå®ƒå¯ä»¥ï¼ˆå¯¹äºŽé«˜çº§ä¼šå‘˜ï¼‰ã€‚ä½†çŽ°åœ¨ï¼Œè¿™äº›åŠŸèƒ½æ›´åŠ å…ˆè¿›ã€‚ä¸ºäº†æŸ¥çœ‹ä¸€èˆ¬ç½‘é¡µæœç´¢ä¸ŽSearchGPTçš„ä¸åŒï¼Œæˆ‘å°è¯•è®©ChatGPTï¼ˆå…è´¹ç‰ˆï¼‰è‡ªå·±æœç´¢ä¸€ä¸ªæŸ¥è¯¢ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ORjGLDBqKDWiPANHlxSdqw.png)\n\nç„¶åŽï¼Œæˆ‘è¯¢é—®è¿™ä¸ªæœç´¢æ˜¯å¦‚ä½•è¿›è¡Œçš„ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*NlzDed3nHdJ636aLt75DCg.png)\n\næŽ¥ä¸‹æ¥ï¼Œ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*eeDPQHQA62yaMK_KkSL0kQ.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ZcsmVgau0SaavN01yHbdKw.png)\n\næ‰€ä»¥ï¼Œæ­£å¦‚ä½ æ‰€çœ‹åˆ°çš„ï¼ŒSearchGPTä¸ä»…ä»…æ˜¯ä¸€ä¸ªç®€å•çš„ç½‘é¡µæµè§ˆå·¥å…·ï¼Œè€Œæ˜¯æ›´å¤šã€‚ä¸å¹¸çš„æ˜¯ï¼ŒOpenAIå·²å°†SearchGPTæä¾›ç»™Proç”¨æˆ·ï¼Œå¹¶ä¸”ä»…é™äºŽç­‰å¾…åå•ç”¨æˆ·ã€‚å¦‚æžœä½ æœ‰è®¿é—®æƒé™ï¼Œä½ ä¸€å®šæ˜¨å¤©æ”¶åˆ°äº†è¿™å°é‚®ä»¶\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*WfO0Xl4VQwRxNNkC1GQpOw.png)\n\nä»¥ä¸‹æ˜¯ä¸€äº›æ¥è‡ªSearchGPTçš„å±å¹•æˆªå›¾\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8bbGpwbsRzo6xQhNDPb3Jw.png)\n\næ­£å¦‚ä½ æ‰€çœ‹åˆ°çš„ï¼Œå®ƒåœ¨æœç´¢æ—¶æä¾›äº†çƒ­é—¨è¯é¢˜ä½œä¸ºå»ºè®®ï¼Œç±»ä¼¼äºŽç½‘é¡µæµè§ˆå™¨ã€‚\n\nå°è¯•æŸ¥çœ‹ä»Šå¤©çš„å¤©æ°”\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*gbtr-QFQw4BnhqrWPvH4RQ.png)\n\nç”šè‡³å¯ä»¥é™åˆ¶ä»…æ£€æŸ¥ç‰¹å®šç½‘ç«™ï¼Œä¾‹å¦‚â€œä»…å¼•ç”¨æ”¿åºœç½‘ç«™â€\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*eq_Xf4JkD4XV85KE5376VA.png)\n\næ‰€æœ‰å¼•ç”¨éƒ½å¯ä»¥åœ¨åº•éƒ¨ä¸€èµ·æŸ¥çœ‹ä½ çš„ç»“æžœ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*wUTEae5yYh_j-oaq6HyP9Q.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Quruyw07__p3qoJ_KhmHAA.png)\n\n## SearchGPT vs Perplexity. å“ªä¸ªæ›´å¥½ï¼Ÿ\n\nè¿™æ˜¯ä¸€ä¸ªå¾ˆéš¾å›žç­”çš„é—®é¢˜ï¼Œè‡³å°‘ç›®å‰æ˜¯è¿™æ ·ã€‚æœ‰å‡ ç‚¹å€¼å¾—å¼ºè°ƒï¼š\n\n1. Perplexity æ˜¯å…è´¹çš„ï¼Œè€Œ SearchGPT ä¸æ˜¯ï¼\n2. SearchGPT æ›´å¿«ã€‚ä½†æˆ‘è®¤ä¸ºè¿™æ˜¯å› ä¸ºç›®å‰çš„æµé‡è¾ƒå°‘ã€‚\n3. Perplexity æ›´ç®€å•ï¼Œå¹¶ä¸”å…·æœ‰æ—©æœŸè¿›å…¥å¸‚åœºçš„ä¼˜åŠ¿ã€‚\n4. Perplexity åœ¨è¿™ä¸ªé¢†åŸŸå­˜åœ¨å·²ä¹…ï¼Œç›¸æ¯”ä¹‹ä¸‹æ›´å¯é ã€‚\n5. SearchGPT æä¾›æ›´å¤šçš„è‡ªå®šä¹‰é€‰é¡¹ï¼Œä¸ä»…ä»…æ˜¯ä¸€ä¸ªä¸Žäº’è”ç½‘è¿žæŽ¥çš„ LLMã€‚\n\nè€å®žè¯´ï¼Œæˆ‘ä¸€ç›´æ”¯æŒå…è´¹çš„ä¸œè¥¿ï¼Œå› æ­¤ä»»ä½•æ—¶å€™æˆ‘éƒ½ä¼šé€‰æ‹© Perplexityã€‚å°½ç®¡å¦‚æ­¤ï¼Œè€ƒè™‘åˆ° SearchGPT çš„å¿«é€Ÿå“åº”ï¼Œè¿™ä¸ªå·¥å…·ç›¸å½“ä¸é”™ï¼Œå€¼å¾—å°è¯•ã€‚\n\n"},{"lang":"zh","group":"blog","slug":"blog/openais-leaked-gpt2-model-has-everyone-stunned-6337904c2ecf","frontmatter":{"title":"OpenAIâ€˜æ³„éœ²â€™çš„ GPT2 æ¨¡åž‹è®©æ‰€æœ‰äººéœ‡æƒŠã€‚","meta_title":"OpenAIâ€˜æ³„éœ²â€™çš„ GPT2 æ¨¡åž‹è®©æ‰€æœ‰äººéœ‡æƒŠã€‚","description":"æ•…æ„æ³„å¯†ï¼Ÿ","date":"2024-11-01T04:07:40.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*-G0yfSjGPdNw02NZ","categories":["Chatbots","Generative AI","Natural Language Processing"],"author":"Rifx.Online","tags":["GPT-2","Chatbot","Inference","JSON","AlphaGo"],"draft":false,"slug":"blog/openais-leaked-gpt2-model-has-everyone-stunned-6337904c2ecf"},"content":"\n\n\n### æ•…æ„æ³„æ¼ï¼Ÿ\n\n\n\nOpenAI å¯¹äººå·¥æ™ºèƒ½è¡Œä¸šçš„å½±å“ä¸å®¹å°è§‘ã€‚æ¯ä¸€ä¸ªåŠ¨ä½œæˆ–å†³å®šéƒ½ä¼šè‡ªåŠ¨æˆä¸ºå¤´æ¡â€¦â€¦å³ä½¿ä»–ä»¬å¹¶æ²¡æœ‰çœŸæ­£å®£å¸ƒä»€ä¹ˆã€‚\n\nå‡ å¤©å‰ï¼Œä¸€ä¸ªæˆ‘ä»¬è®¸å¤šäººæ›¾è¯•ç”¨è¿‡ä½†å·²è¢«åˆ é™¤çš„æ¨¡åž‹è®©æ•´ä¸ªäººå·¥æ™ºèƒ½è¡Œä¸šç€è¿·ã€‚è¿™ä¸ªåä¸ºâ€œgpt2-chatbotâ€çš„æ¨¡åž‹åœ¨ [lmsys.org](https://chat.lmsys.org/) çš„â€œç›´æŽ¥èŠå¤©â€åŠŸèƒ½ä¸­å¯ä»¥ä½¿ç”¨äº†å‡ å¤©ã€‚\n\n*ä½†ä¸ºä»€ä¹ˆè¿™ä¹ˆå¤šå–§åš£ï¼Ÿ*\n\nå› ä¸ºè¿™ä¸ªæ¨¡åž‹ä¸Žæˆ‘ä»¬è§è¿‡çš„ä»»ä½•ä¸œè¥¿éƒ½ä¸åŒã€‚**å®ƒå¤„äºŽä¸€ä¸ªå®Œå…¨ä¸åŒçš„å±‚æ¬¡ã€‚**\n\nå› æ­¤ï¼Œè®¸å¤šäººè®¤ä¸ºå®ƒæ˜¯ **ChatGPT-4.5** æˆ–ç”šè‡³ **GPT-5** çš„éžå®˜æ–¹é¢„å‘Šã€‚æ›´ä»¤äººå…´å¥‹çš„æ˜¯ï¼Œä½¿ç”¨æ•°å­—â€œ2â€ä½œä¸ºä¿¡å·ï¼Œè¡¨æ˜Ž **æ–°ä¸€ä»£é•¿æŽ¨ç†æ¨¡åž‹çš„ GPT æ­£åœ¨é€¼è¿‘**ã€‚\n\nç”šè‡³ OpenAI çš„ CEO Sam Altman ä¹Ÿå¿ä¸ä½æ‰¿è®¤å®ƒçš„å­˜åœ¨ï¼Œå¹¶åœ¨è¿‡ç¨‹ä¸­è°ƒä¾ƒæˆ‘ä»¬ï¼š\n\n\n\n\né‚£ä¹ˆï¼Œ*è¿™ä¸ªæ¨¡åž‹åˆ°åº•æœ‰å¤šå¥½ï¼Œå®ƒç©¶ç«Ÿæ˜¯ä»€ä¹ˆï¼Ÿ*\n\n\n> ä½ å¯èƒ½å·²ç»åŽŒå€¦äº†äººå·¥æ™ºèƒ½é€šè®¯ç®€æŠ¥è°ˆè®ºæŸä¸ªäº‹æƒ…æ˜¯å¦‚ä½•â€œåˆšåˆšâ€å‘ç”Ÿçš„ã€‚è¿™äº›é€šè®¯ç®€æŠ¥å±‚å‡ºä¸ç©·ï¼Œå› ä¸ºç²—ç•¥åœ°è°ˆè®ºå·²ç»å‘ç”Ÿçš„äº‹ä»¶å’Œäº‹æƒ…æ˜¯å®¹æ˜“çš„ï¼Œ**ä½†æä¾›çš„ä»·å€¼æœ‰é™ï¼Œç‚’ä½œå´è¢«å¤¸å¤§ã€‚**\n\n\n> ç„¶è€Œï¼Œè°ˆè®º **å°†** ä¼šå‘ç”Ÿçš„äº‹æƒ…çš„é€šè®¯ç®€æŠ¥å´æ˜¯ç½•è§çš„ã€‚å¦‚æžœä½ æƒ³åœ¨åˆ«äººä¹‹å‰èŽ·å¾—æ˜“äºŽç†è§£çš„äººå·¥æ™ºèƒ½æœªæ¥æ´žå¯Ÿï¼Œ**TheTechOasis** é€šè®¯ç®€æŠ¥å¯èƒ½éžå¸¸é€‚åˆä½ ã€‚\n\n\n> ðŸï¸ðŸï¸ ä»Šå¤©å°±è®¢é˜…å§ï¼š\n\n## æœªæ¥çš„é¢„å‘Š\n\néšç€æ¯ä¸€å¤©çš„è¿‡åŽ»ï¼ŒOpenAIçš„ä¸‹ä¸€ä¸ªæ¨¡åž‹æ˜¾ç„¶å°†åœ¨æŽ¨ç†å’Œå¤æ‚é—®é¢˜è§£å†³æ–¹é¢å®žçŽ°é£žè·ƒã€‚\n\nä¸ºäº†è¯æ˜Žè¿™ä¸ªç¥žç§˜çš„æ–°æ¨¡åž‹å¯èƒ½å°±æ˜¯å®ƒï¼Œè¿™é‡Œæœ‰å‡ ä¸ªä¾‹å­å±•ç¤ºè¿™ä¸ªç¥žç§˜æ¨¡åž‹çš„å¼ºå¤§ï¼Œå¯èƒ½è¡¨æ˜Žè¿™è‰˜èˆ¹å·²ç»åœ¨é‚£ä¸ªæ¸¯å£åœé ï¼š\n\n> ä»¥ä¸‹æ‰€æœ‰ç¤ºä¾‹è¢«è®¤ä¸ºæ˜¯**å½“å‰æœ€å…ˆè¿›æ¨¡åž‹**çš„**å›°éš¾æˆ–å®Œå…¨ä¸å¯èƒ½**çš„ã€‚\n\né¦–å…ˆï¼Œå®ƒåœ¨é›¶-shotæ¨¡å¼ä¸‹è§£å†³äº†ä¸€ä¸ªæ•°å­¦å¥¥æž—åŒ¹å…‹é—®é¢˜ï¼ˆæ²¡æœ‰æä¾›è¾…åŠ©ç¤ºä¾‹æ¥æ”¯æŒè§£å†³æ–¹æ¡ˆï¼‰ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*oNPg_hTGc0OP90n9)\n\næˆ‘ç”šè‡³æ— æ³•å¼€å§‹è§£é‡Šä¹‹å‰çš„ä¾‹å­æœ‰å¤šç–¯ç‹‚ï¼Œä»Žå½“å‰æœ€å…ˆè¿›çš„æ¨¡åž‹ä¸­å¾—åˆ°è¿™æ ·çš„ç­”æ¡ˆç»å¯¹æ˜¯ä¸å¯èƒ½çš„ã€‚\n\n[å®ƒåœ¨è§£æžJSONæ–¹é¢ä¹Ÿç»å¯¹å‡ºè‰²](https://twitter.com/skirano/status/1785035706173214888)ï¼Œè¿™æ˜¯LLMä¸ŽAPIåŠå…¶ä»–åŸºäºŽç½‘ç»œå·¥å…·é›†æˆçš„åŸºæœ¬æŠ€èƒ½ã€‚\n\næ­¤å¤–ï¼Œå®ƒåœ¨å¤æ‚ç»˜å›¾ä»»åŠ¡ä¸­å®Œå…¨åŽ‹å€’äº†GPT-4ï¼Œä¾‹å¦‚[æ ¹æ®ä»£ç ç»˜åˆ¶SVGæ–‡ä»¶](https://twitter.com/decentricity/status/1785049191003361778)æˆ–**ä½¿ç”¨ASCIIä»£ç ç»˜åˆ¶ç‹¬è§’å…½ï¼ˆå¦‚ä¸‹ï¼‰**ï¼Œåœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ç¾žè¾±äº†**Claude 3 Opus**ï¼Œå½“å‰çš„æœ€å…ˆè¿›æ¨¡åž‹ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*5A0EcRU91ZYAwVAc)\n\næ­¤å¤–ï¼Œå°½ç®¡è¿™å¾ˆå¯èƒ½æ˜¯ä¸€æ¬¡å¹»è§‰ï¼Œ**æ¨¡åž‹å‘æˆ‘å£°ç§°å®ƒæ˜¯ç”±OpenAIè®­ç»ƒçš„ï¼Œå¹¶åŸºäºŽGPT-4å˜ä½“ã€‚**\n\nå½“ç„¶ï¼Œåœ¨å¦‚æ­¤å¼ºå¤§çš„æ¼”ç¤ºä¹‹åŽï¼Œ**è®¸å¤šäººå»ºè®®â€œgpt2-chatbotâ€ç”šè‡³å¯èƒ½æ˜¯è‘—åçš„Q\\*æ¨¡åž‹**ã€‚\n\nä½†ä¸Žå…¶ç®€å•åœ°å±ˆæœäºŽäººä»¬å£°ç§°çš„ä¸åŒå¥‡å¹»é€‰é¡¹ï¼Œä¸å¦‚é‡‡å–æ›´ç†æ€§çš„æ–¹å¼ï¼Œçœ‹çœ‹OpenAIè‡ªå·±åœ¨å‡ ä¸ªæœˆï¼ˆç”šè‡³å‡ å¹´ï¼‰é‡Œé€šè¿‡ä»–ä»¬çš„ç ”ç©¶æ‰€æš—ç¤ºçš„å†…å®¹ã€‚\n\n## é•¿æŽ¨ç†çš„åŠ›é‡\n\nå‡ ä¸ªæœˆæ¥ï¼Œåƒ [Demis Hassabis](https://www.youtube.com/watch?v=eqXfhejDeqA&t=2s) æˆ– [Andrej Karpathy](https://youtu.be/c3b-JASoPi0?si=fZWoSpLuSmua8YMR&t=1481) è¿™æ ·çš„é¢†åŸŸä¸“å®¶è®¨è®ºäº†ä»…é  LLMs æ˜¯ä¸å¤Ÿçš„ï¼Œæˆ‘ä»¬éœ€è¦â€œå…¶ä»–ä¸œè¥¿â€æ¥çœŸæ­£å°†å®ƒä»¬æå‡åˆ°ä¸€ä¸ªæ–°çš„æ°´å¹³ã€‚\n\nåœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼Œä»–ä»¬æåˆ°å®žçŽ°ç›¸å½“äºŽâ€œAlphaGo ä½†åœ¨ LLMs ä¸­â€ï¼Œè¿™é—´æŽ¥æŒ‡çš„æ˜¯ï¼š\n\n* **è‡ªæˆ‘æå‡** å’Œ\n* **æµ‹è¯•æ—¶è®¡ç®—** LLMs\n\n*ä½†ä»–ä»¬è¿™æ˜¯ä»€ä¹ˆæ„æ€å‘¢ï¼Ÿ*\n\n### äººå·¥æ™ºèƒ½çš„å·¨å¤§é£žè·ƒ\n\nAlphaGo æ˜¯äººå·¥æ™ºèƒ½çš„åŽ†å²ã€‚å®ƒæ˜¯ç¬¬ä¸€ä¸ªåœ¨å›´æ£‹è¿™ä¸€éŸ©å›½æ£‹ç±»æ¸¸æˆä¸­æ¯«æ— ç–‘é—®åœ°è¶…è¶Šäººç±»å®žåŠ›çš„æ¨¡åž‹ã€‚\n\nå®ƒä½¿ç”¨äº† **Monte Carlo Tree Search**ï¼Œä¸€ç§æœç´¢ç®—æ³•ï¼Œæ¥æŽ¢ç´¢æ¸¸æˆä¸­ä»»ä½•ç»™å®šæ­¥éª¤çš„å¯èƒ½èµ°æ³•ï¼Œèƒ½å¤Ÿè¶…è¶Šå½“å‰çš„åŠ¨ä½œå¹¶é¢„æµ‹å¯¹æ‰‹çš„è¡ŒåŠ¨ã€‚\n\n> ä½ ä»¬ä¸­çš„ä¸€äº›äººå¯èƒ½è¿˜è®°å¾— **Deep Blue**ï¼Œé‚£å°åœ¨1997å¹´ä¸ŽåŠ é‡ŒÂ·å¡æ–¯å¸•ç½—å¤«çš„ç³»åˆ—èµ›ä¸­ç¬¬äºŒå±€å‹‰å¼ºæˆ˜èƒœä»–çš„å›½é™…è±¡æ£‹æœºå™¨ï¼Œåœ¨ç¬¬ä¸€å±€ä¸­è¾“æŽ‰äº†æ¯”èµ›ã€‚\n\n> ç„¶è€Œï¼Œå°½ç®¡ Deep Blue å¯ä»¥è¢«å‡»è´¥ï¼Œä½† AlphaGo æ˜¯æ— æ•Œçš„ã€‚\n\n*ä½†è¿™æ˜¯æ€Žä¹ˆåšåˆ°çš„ï¼Ÿ*\n\n### è‡ªæˆ‘æå‡ä»¥è¾¾åˆ°è¶…äººæ°´å¹³\n\nä½¿AlphaGoå“è¶Šçš„å…³é”®å› ç´ åœ¨äºŽå®ƒçš„è®­ç»ƒæ–¹å¼ï¼Œ**é€šè¿‡ä¸Žè‡ªèº«çš„è¾ƒå¼±ç‰ˆæœ¬å¯¹å¼ˆæ¥åˆ›å»ºè‡ªæˆ‘æå‡å¾ªçŽ¯ã€‚**\n\nå®ƒæŒç»­ä¸Žè‡ªå·±å¯¹å¼ˆï¼Œé€æ¸å°†ELOæå‡è‡³3\\.739ï¼Œå‡ ä¹Žè¾¾åˆ°äº†å½“ä»Šæœ€ä½³å›´æ£‹é€‰æ‰‹çš„æ°´å¹³ã€‚\n\n> 2017å¹´ï¼Œæ”¹è¿›ç‰ˆçš„AlphaZeroè¾¾åˆ°äº†5\\.018çš„ELOï¼Œå®Œå…¨è¶…è¶Šäººç±»ï¼Œæ— æ³•å‡»è´¥ã€‚\n\næ¢å¥è¯è¯´ï¼Œæœ‰äº†AlphaGoï¼Œäººç±»é¦–æ¬¡å®žçŽ°äº†ä¸€ç§é€šè¿‡è‡ªæˆ‘æå‡æ¥è®­ç»ƒæ¨¡åž‹çš„æ–¹æ³•ï¼Œä½¿å…¶èƒ½å¤Ÿè¾¾åˆ°è¶…äººèƒ½åŠ›ï¼Œ**å› ä¸ºå®ƒä¸å†ä¾èµ–æ¨¡ä»¿äººç±»æ¥å­¦ä¹ ã€‚**\n\nå¦‚æžœä½ åœ¨æƒ³ï¼Œè¿™å¯¹LLMså¹¶ä¸é€‚ç”¨ã€‚\n\nå½“å‰çš„LLMså®Œå…¨ä¾èµ–äºŽäººç±»æ°´å¹³çš„è¡¨çŽ°ï¼Œå› ä¸ºæ‰€æœ‰æ•°æ®å’Œè®­ç»ƒæœ¬è´¨ä¸Šéƒ½ä¾èµ–äºŽäººç±»ï¼ˆä»¥è‡³äºŽ[å¯¹é½é˜¶æ®µ](https://thewhitebox.ai/llms-the-backbones-of-frontier-ai/)â€”â€”LLMsè¢«å»ºæ¨¡ä»¥æé«˜å…¶å®‰å…¨æ°´å¹³å¹¶é¿å…å†’çŠ¯æ€§ååº”çš„è®­ç»ƒè¿‡ç¨‹çš„ä¸€éƒ¨åˆ†ï¼Œ**ä¸¥æ ¼æ‰§è¡Œæ—¶ä½¿ç”¨çš„æ˜¯â€œäººç±»åå¥½â€**ï¼‰ã€‚\n\n> é¡ºä¾¿æä¸€ä¸‹ï¼Œ[Metaæœ€è¿‘æå‡ºäº†è‡ªæˆ‘å¥–åŠ±æ¨¡åž‹](https://arxiv.org/pdf/2401.10020v1)ï¼Œå¯ä»¥é€šè¿‡è‡ªèº«çš„ååº”è¿›è¡Œè‡ªæˆ‘æå‡ã€‚ç„¶è€Œï¼Œç›®å‰å°šä¸æ¸…æ¥šè¿™ä¸ªåé¦ˆå¾ªçŽ¯æ˜¯å¦çœŸçš„èƒ½ä½¿LLMsè¶…è¶Šäººç±»ã€‚\n\nä½†å°½ç®¡ä»ç„¶å¾ˆéš¾ç›¸ä¿¡â€œgpt2\\-chatbotâ€æ˜¯é€šè¿‡è‡ªæˆ‘æå‡è®­ç»ƒå‡ºæ¥çš„ï¼Œ**æˆ‘ä»¬æœ‰å……åˆ†çš„ç†ç”±ç›¸ä¿¡å®ƒæ˜¯OpenAIå¤šå¹´æ¥åŠªåŠ›å·¥ä½œçš„ç¬¬ä¸€ä¸ªæˆåŠŸå®žçŽ°ï¼šæµ‹è¯•æ—¶è®¡ç®—**ã€‚\n\n### æµ‹è¯•æ—¶è®¡ç®—æ¨¡åž‹çš„åˆ°æ¥\n\nå¤šå¹´æ¥ï¼ŒOpenAIçš„å‡ ç¯‡ç ”ç©¶è®ºæ–‡æš—ç¤ºäº†å°†æ¨¡åž‹åå‘â€œé‡æŽ¨ç†â€çš„è¿™ä¸€æƒ³æ³•ã€‚\n\nä¾‹å¦‚ï¼Œæ—©åœ¨2021å¹´ï¼Œ[ä»–ä»¬æå‡ºäº†åœ¨æŽ¨ç†æ—¶ä½¿ç”¨â€œéªŒè¯è€…â€çš„æ¦‚å¿µ](https://arxiv.org/pdf/2110.14168)ï¼Œä»¥æ”¹å–„æ¨¡åž‹åœ¨å¤„ç†æ•°å­¦é—®é¢˜æ—¶çš„å“åº”ã€‚\n\nè¿™ä¸ªæƒ³æ³•æ˜¯è®­ç»ƒä¸€ä¸ªè¾…åŠ©æ¨¡åž‹ï¼Œå®žæ—¶è¯„ä¼°æ¨¡åž‹ç»™å‡ºçš„å¤šä¸ªå“åº”ï¼Œé€‰æ‹©æœ€ä½³çš„ä¸€ä¸ªï¼ˆç„¶åŽæä¾›ç»™ç”¨æˆ·ï¼‰ã€‚\n\nè¿™ä¸ŽæŸç§æ ‘æœç´¢ç®—æ³•ç›¸ç»“åˆï¼Œæ¯”å¦‚AlphaGoä½¿ç”¨çš„é‚£ç§ï¼Œç»“åˆGoogle Deepmindçš„[æ€ç»´æ ‘ç ”ç©¶](https://arxiv.org/pdf/2305.10601)çš„ä¾‹å­ï¼Œæœ€ç»ˆå¯ä»¥åˆ›å»ºä¸€ä¸ªLLMï¼Œåœ¨å›žç­”ä¹‹å‰ï¼ŒæŽ¢ç´¢â€œå¯èƒ½å“åº”çš„é¢†åŸŸâ€ï¼Œ**ä»”ç»†è¿‡æ»¤å¹¶é€‰æ‹©é€šå‘è§£å†³æ–¹æ¡ˆçš„æœ€ä½³è·¯å¾„ã€‚**\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*pHWwOA66fxpKbl-z)\n\nè¿™ä¸ªæƒ³æ³•è™½ç„¶åœ¨2021å¹´ç”±OpenAIæå‡ºï¼Œä½†å¦‚ä»Šå·²ç»å˜å¾—ç›¸å½“æµè¡Œï¼Œ[å¾®è½¯å’Œè°·æ­Œçš„è·¨ç•Œåˆä½œç ”ç©¶å°†å…¶åº”ç”¨äºŽè®­ç»ƒä¸‹ä¸€ä»£éªŒè¯è€…](https://arxiv.org/pdf/2402.06457)ï¼Œè°·æ­Œç”šè‡³æˆåŠŸåˆ›å»ºäº†ä¸€ä¸ªæ¨¡åž‹ï¼Œ[Alphacode](https://storage.googleapis.com/deepmind-media/AlphaCode2/AlphaCode2_Tech_Report.pdf)ï¼Œä»¥æžå¤§çš„æˆåŠŸæ‰§è¡Œè¿™ç§æž¶æž„ï¼Œ**åœ¨ç«žäº‰ç¨‹åºå‘˜ä¸­è¾¾åˆ°äº†85%çš„ç™¾åˆ†ä½ï¼Œæˆä¸ºæœ€ä¼˜ç§€çš„äººç±»ç¨‹åºå‘˜ä¹‹ä¸€ã€‚**\n\n*é‚£ä¹ˆï¼Œä¸ºä»€ä¹ˆè¿™ä¸€ä»£æ–°çš„LLMå…·æœ‰å¦‚æ­¤å·¨å¤§çš„æ½œåŠ›ï¼Ÿ*\n\nå› ä¸º**å®ƒä»¬åœ¨è§£å†³é—®é¢˜æ—¶ä¸Žäººç±»çš„æ–¹å¼éžå¸¸ç›¸ä¼¼**ï¼Œé€šè¿‡æœ‰æ„è¯†å’Œå¹¿æ³›çš„æ€è€ƒæ¥è§£å†³ç‰¹å®šä»»åŠ¡ã€‚\n\nå½’æ ¹ç»“åº•ï¼ŒæŠŠâ€œæœç´¢+LLMâ€æ¨¡åž‹è§†ä¸ºAIç³»ç»Ÿï¼Œå®ƒä»¬åœ¨æ¨¡åž‹çš„å®žé™…è¿è¡Œæ—¶é—´ä¸Šåˆ†é…äº†æ›´é«˜ç¨‹åº¦çš„è®¡ç®—ï¼ˆç±»ä¼¼äºŽäººç±»æ€ç»´ï¼‰ï¼Œå› æ­¤ï¼Œå®ƒä»¬ä¸å¿…ç«‹å³çŒœæµ‹æ­£ç¡®çš„è§£å†³æ–¹æ¡ˆï¼Œè€Œæ˜¯ç®€å•åœ°è¯´ï¼Œâ€œæœ‰æ›´å¤šæ—¶é—´æ¥æ€è€ƒâ€ã€‚\n\nä½†OpenAIæ›´è¿›ä¸€æ­¥ã€‚\n\n### PRM æ¨¡åž‹ä»¥æ”¹å–„æ•°å­¦æ‰§è¡Œ\n\nåœ¨åŽ»å¹´äº”æœˆï¼Œä»–ä»¬å‘å¸ƒäº†è®ºæ–‡ [Letâ€™s Verify Step\\-by\\-Step](https://arxiv.org/pdf/2305.20050)ï¼Œå‚ä¸Žè€…åŒ…æ‹¬ OpenAI çš„é¦–å¸­ç§‘å­¦å®¶ Ilya Sutskever ä»¥åŠä¸€äº›æ¥è‡ªåŽŸå§‹éªŒè¯è€…è®ºæ–‡çš„ç ”ç©¶äººå‘˜ï¼Œå¦‚ Karl Cobbeã€‚\n\nè¿™é‡Œçš„æƒ³æ³•æ˜¯ä¿®æ”¹åœ¨æ¨¡åž‹å¯¹é½é˜¶æ®µä½¿ç”¨çš„å¥–åŠ±æ¨¡åž‹ã€‚\n\n[è™½ç„¶æˆ‘å»ºè®®æŸ¥çœ‹è¿™ç¯‡æ–‡ç« ä»¥èŽ·å–å…³äºŽ LLM è®­ç»ƒçš„å®Œæ•´æŒ‡å—](https://thewhitebox.ai/llms-the-backbones-of-frontier-ai/)ï¼Œåˆ›å»º ChatGPT ç­‰äº§å“è¿‡ç¨‹ä¸­çš„æœ€åŽä¸€æ­¥æ˜¯ä½¿ç”¨äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼Œæˆ– RLHFã€‚\n\nè¿™ä¸ªæƒ³æ³•æ˜¯è®©æ¨¡åž‹æ”¹å–„å…¶å†³ç­–èƒ½åŠ›ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è®­ç»ƒä¸€ä¸ªè¾…åŠ©å¥–åŠ±æ¨¡åž‹ï¼ˆæœ¬è´¨ä¸Šæ˜¯è¢«è®­ç»ƒæ¨¡åž‹çš„å‡ ä¹Žç›¸åŒå‰¯æœ¬ï¼‰ï¼Œå®ƒå­¦ä¹ æ ¹æ®äººç±»åå¥½å¯¹è®­ç»ƒæ¨¡åž‹çš„ç»“æžœè¿›è¡ŒæŽ’åã€‚\n\n*é—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ*\n\nå¥½å§ï¼Œä»Šå¤©å¤§å¤šæ•°å¥–åŠ±æ¨¡åž‹æ˜¯ **ORMsï¼Œæˆ–ç»“æžœç›‘ç£å¥–åŠ±æ¨¡åž‹**ã€‚é€šä¿—æ¥è¯´ï¼Œè¯„ä¼°æ¨¡åž‹é¢„æµ‹çš„æ­£ç¡®ç¨‹åº¦æ—¶ï¼Œä»–ä»¬ä»Žæ•´ä½“ä¸Šçœ‹å¾…ï¼Œè€Œå¿½ç•¥äº†æ•´ä¸ªâ€œæ€ç»´è¿‡ç¨‹â€ã€‚\n\nå¦ä¸€æ–¹é¢ï¼Œ**PRMsï¼Œæˆ–è¿‡ç¨‹ç›‘ç£å¥–åŠ±æ¨¡åž‹ï¼Œè¯„ä¼°æ¨¡åž‹å“åº”ä¸­çš„æ¯ä¸€ä¸ªæ­¥éª¤**ã€‚å› æ­¤ï¼Œä»–ä»¬â€œè¿«ä½¿â€æ¨¡åž‹åœ¨è¿‡ç¨‹çš„æ¯ä¸€ä¸ªæ­¥éª¤ä¸Šéƒ½ä»˜å‡ºå¯†åˆ‡çš„å…³æ³¨å’ŒåŠªåŠ›ï¼Œè¿™åœ¨è§£å†³å¦‚ä¸‹æ•°å­¦æ–¹ç¨‹ç­‰æƒ…å†µä¸­è‡³å…³é‡è¦ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8JC6sZl5UFfl3WorliQy-A.png)\n\nç„¶è€Œï¼Œè¿™æ˜¯ä¸€ä¸ªéžå¸¸éžå¸¸æ˜‚è´µçš„è¿‡ç¨‹ï¼Œå› ä¸ºåå¥½æ•°æ®éœ€è¦å¤§é‡çš„äººåŠ›æž„å»ºï¼Œä»¥ä¾¿å¯ä»¥åº”ç”¨ç›‘ç£ä¿¡å·ã€‚å› æ­¤ï¼Œæ¯ä¸€ä¸ªè®­ç»ƒç¤ºä¾‹éƒ½æœ‰æ•°åä¸ªæˆ–æ›´å¤šçš„å¥–åŠ±æ¥è¿›è¡Œæµ‹é‡ã€‚\n\nå› æ­¤ï¼Œâ€œgpt2\\-chatbotâ€å¯èƒ½åœ¨å¥–åŠ±è®­ç»ƒä¸­åŒ…å«æŸç§å˜ä½“ï¼Œè€ƒè™‘åˆ°å®ƒåœ¨ç”Ÿæˆè®¡åˆ’å’Œæ‰§è¡Œå¤æ‚é—®é¢˜è§£å†³æ–¹é¢çš„é«˜æ•ˆæ€§ã€‚\n\n## ä¸ç¦è®©äººå…´å¥‹\n\nè€ƒè™‘åˆ°gpt2-chatbotçš„æƒŠäººè¡¨çŽ°ï¼Œä»¥åŠOpenAIæœ€è¿‘çš„ç ”ç©¶å’Œ[æ³„éœ²](https://www.reuters.com/technology/sam-altmans-ouster-openai-was-precipitated-by-letter-board-about-ai-breakthrough-2023-11-22/)ï¼Œæˆ‘ä»¬çŽ°åœ¨å¯èƒ½å¯¹è¿™ä¸ªä¸œè¥¿æœ‰äº†ç›¸å½“ä¸é”™çš„äº†è§£ã€‚\n\næˆ‘ä»¬å¯ä»¥è‚¯å®šçš„æ˜¯ï¼Œæˆ‘ä»¬å¾ˆå¿«å°†é¢ä¸´ä¸€ä¸ªå®Œå…¨ä¸åŒçš„å­˜åœ¨ï¼Œå®ƒå°†æŠŠAIçš„å½±å“æå‡åˆ°ä¸€ä¸ªæ–°çš„æ°´å¹³ã€‚\n\n* *æˆ‘ä»¬æ˜¯å¦ç»ˆäºŽè¾¾åˆ°äº†å¤§åž‹è¯­è¨€æ¨¡åž‹è¶…è¶Šäººç±»æ°´å¹³è¡¨çŽ°çš„é‡Œç¨‹ç¢‘ï¼Œå°±åƒæˆ‘ä»¬åœ¨AlphaGoä¸­æ‰€åšçš„é‚£æ ·ï¼Ÿ*\n* *é•¿æŽ¨ç†æ—¶ä»£ï¼Œå³AIå¾æœç³»ç»Ÿ2æ€ç»´çš„æ—¶ä»£ï¼Œæ˜¯å¦å·²ç»åˆ°æ¥ï¼Ÿ*\n\nå¯èƒ½è¿˜æ²¡æœ‰ã€‚ç„¶è€Œï¼Œæ— æ³•ä¸å¯¹æŽ¥ä¸‹æ¥å‡ ä¸ªæœˆæˆ‘ä»¬å³å°†è§è¯çš„æƒŠäººå‘å±•æ„Ÿåˆ°é«˜åº¦ä¹è§‚ã€‚\n\nä¸Žæ­¤åŒæ—¶ï¼Œæˆ‘æƒ³æˆ‘ä»¬å°†ä¸å¾—ä¸ç­‰å¾…è¿™äº›ç­”æ¡ˆã€‚ä½†ä¸ä¼šå¤ªä¹…ã€‚\n\n> æœ€åŽï¼Œå¦‚æžœä½ å–œæ¬¢è¿™ç¯‡æ–‡ç« ï¼Œæˆ‘åœ¨æˆ‘çš„[LinkedIn](https://www.linkedin.com/in/ignacio-de-gregorio-noblejas/)ä¸Šä»¥æ›´å…¨é¢å’Œç®€åŒ–çš„æ–¹å¼å…è´¹åˆ†äº«ç±»ä¼¼çš„æƒ³æ³•ã€‚\n\n> å¦‚æžœæ–¹ä¾¿çš„è¯ï¼Œä½ å¯ä»¥é€šè¿‡[X](https://twitter.com/TheTechOasis1)ä¸Žæˆ‘è”ç³»ã€‚\n\n> æœŸå¾…ä¸Žä½ çš„è”ç³»ã€‚\n\n"},{"lang":"zh","group":"blog","slug":"blog/overcoming-llm-challenges-in-healthcare-practical-strategies-for-development-in-production-04c617954b9a","frontmatter":{"title":"å…‹æœåŒ»ç–—é¢†åŸŸçš„æ³•å­¦ç¡•å£«æŒ‘æˆ˜ï¼šç”Ÿäº§å‘å±•å®žç”¨ç­–ç•¥","meta_title":"å…‹æœåŒ»ç–—é¢†åŸŸçš„æ³•å­¦ç¡•å£«æŒ‘æˆ˜ï¼šç”Ÿäº§å‘å±•å®žç”¨ç­–ç•¥","description":"ä¸€ç¯‡å…³äºŽæˆ‘é‡åˆ°çš„æœ€å¸¸è§çš„ LLM å¼€å‘æŒ‘æˆ˜ã€æœ‰æ•ˆçš„ç¼“è§£ç­–ç•¥ä»¥åŠèŒä¸šç”Ÿæ¶¯å†³å®šæ€§çš„é¢è¯•çš„æ–‡ç« â€¦â€¦","date":"2024-11-08T00:20:35.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Vak28ygruWKySsH0doGoYg.png","categories":["Health","Generative AI","Machine Learning"],"author":"Rifx.Online","tags":["LLMs","healthcare","hallucinations","validation","monitoring"],"draft":false,"slug":"blog/overcoming-llm-challenges-in-healthcare-practical-strategies-for-development-in-production-04c617954b9a"},"content":"\n### ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½\n\n\n\n### æˆ‘é‡åˆ°çš„æœ€å¸¸è§çš„LLMå¼€å‘æŒ‘æˆ˜ã€æœ‰æ•ˆçš„ç¼“è§£ç­–ç•¥ä»¥åŠä¸€ä¸ªèŒä¸šç”Ÿæ¶¯ä¸­å†³å®šæ€§çš„é¢è¯•é”™è¯¯\n\n## å¼•è¨€\n\næˆ‘ä¸€ç›´æ˜¯é‚£ç§æ·±å…¥ç ”ç©¶ä¸€ä¸ªä¸»é¢˜å¹¶ä¸“æ³¨åˆ°ç—´è¿·çš„äººã€‚å½“æˆ‘ä»Žæ•°æ®ç§‘å­¦ç¡•å£«æ¯•ä¸šæ—¶ï¼Œæˆ‘çš„ç—´è¿·æ˜¯è®¡ç®—æœºè§†è§‰ï¼›ç‰¹åˆ«æ˜¯å°†è®¡ç®—æœºè§†è§‰åº”ç”¨äºŽç¥žç»ç§‘å­¦æˆ–å¿ƒç†å¥åº·é¢†åŸŸã€‚æˆ‘å†³å¿ƒæˆä¸ºå¿ƒç†å¥åº·é¢†åŸŸçš„â€œè®¡ç®—æœºè§†è§‰å·¥ç¨‹å¸ˆâ€ï¼ˆä¸è¿‡â€œæœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆâ€ä¹Ÿå¯ä»¥ï¼‰ï¼Œå°½ç®¡æˆ‘çš„å¯¼å¸ˆä»¬åŠæˆ‘æ‹“å®½è§†é‡Žï¼Œå¯»æ‰¾æ›´å¤šæœºä¼šã€‚æˆ‘åŽ‹åˆ¶äº†è‡ªå·±å†…å¿ƒçš„ç–‘è™‘ï¼Œåšä¿¡æ­£ç¡®çš„å›¢é˜Ÿä¼šè®¤å¯æˆ‘çš„â€œä¸“ä¸šçŸ¥è¯†â€ã€‚\n\n\n\nå¹¸è¿çš„æ˜¯ï¼Œæˆ‘çš„ç†è®ºä¼¼ä¹Žå¥æ•ˆäº†ï¼›æˆ‘èŽ·å¾—äº†å‡ å®¶å¿ƒç†å¥åº·å…¬å¸çš„é¢è¯•ã€‚ä½†éšä¹‹è€Œæ¥çš„æ˜¯æˆ‘æœ€å¤§çš„é¢è¯•é”™è¯¯ä¹‹ä¸€ã€‚åœ¨æˆ‘æœ€å–œæ¬¢çš„å…¬å¸çš„æœ€åŽä¸€è½®é¢è¯•ä¸­â€”â€”ä¸€å®¶æˆ‘éžå¸¸å–œæ¬¢çš„å…¬å¸â€”â€”æˆ‘çŠ¯äº†ä¸€ä¸ªé”™è¯¯ï¼Œè‡³ä»Šå›žæƒ³èµ·æ¥ä»è®©æˆ‘æ„Ÿåˆ°ä¸å®‰ã€‚è¿™ä¸ªèŒä½ä¸“æ³¨äºŽNLPï¼Œå¤„ç†æ–‡æœ¬æ•°æ®ï¼Œä½†æˆ‘å¿ä¸ä½è¡¨è¾¾äº†æˆ‘å¯¹æˆåƒæ•°æ®çš„å…´è¶£ã€‚*åœ¨å›žå¿†ä¸­å“­æ³£ã€‚* æˆ‘æ¸…æ™°åœ°è®°å¾—ï¼Œå½“æˆ‘è¯¢é—®æˆåƒæ•°æ®çš„å¯ç”¨æ€§æ—¶ï¼Œé¢è¯•å®˜çš„è¡¨æƒ…ä»Žå…´å¥‹è½¬ä¸ºæ‹…å¿§ï¼Œå› ä¸ºæˆ‘ä»ç„¶å¯¹è®¡ç®—æœºè§†è§‰å……æ»¡çƒ­æƒ…ã€‚å½“å¤©æ™šäº›æ—¶å€™ï¼Œæˆ‘æ”¶åˆ°äº†ä¸€ä¸ªç¤¼è²Œçš„æ‹’ç»ï¼šä»–ä»¬å–œæ¬¢æˆ‘çš„çƒ­æƒ…ï¼Œä½†éœ€è¦ä¸€ä¸ªå®Œå…¨è‡´åŠ›äºŽNLPçš„äººã€‚\n\nè®½åˆºçš„æ˜¯ï¼Œæˆ‘å¾ˆå¿«åŠ å…¥äº†å¦ä¸€å®¶å¿ƒç†å¥åº·å…¬å¸ï¼Œå¹¶å®Œå…¨è½¬å‘NLPå·¥ä½œï¼Œåˆ›å»ºäº†æ”¹å–„ä¸´åºŠæŠ¤ç†çš„ç„¦è™‘å’ŒæŠ‘éƒç—‡çŠ¶æ£€æµ‹å™¨ï¼Œå¹¶å¼€å‘äº†æå‡å†…å®¹å¯å‘çŽ°æ€§çš„æŽ¨èç³»ç»Ÿï¼Œå¢žåŠ äº†12%çš„å‘çŽ°çŽ‡ã€‚å‡ å¹´åŽï¼Œæˆ‘çŽ°åœ¨æ˜¯å›¢é˜Ÿä¸­çš„NLP/LLMæ•°æ®ç§‘å­¦å®¶ï¼Œè´Ÿè´£6ä¸ªä¿¡æ¯æå–ä»»åŠ¡ã€5ä¸ªåˆ†ç±»ä»»åŠ¡å’Œ5ä¸ªæ¡ä»¶æ‘˜è¦ä»»åŠ¡ï¼Œå·²åœ¨15å®¶ä»¥ä¸ŠåŒ»é™¢å’Œäº”ä¸ªå®¢æˆ·ä¸­éƒ¨ç½²ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*-VOHDQd88fCyRqoY9bR3hQ.png)\n\nå‡ å‘¨å‰ï¼Œæˆ‘è¢«è¦æ±‚å‘æˆ‘æ›´å¤§çš„æ•°æ®å›¢é˜Ÿä»‹ç»â€œLLMå¼€å‘101â€ã€‚æœ€åˆï¼Œå†’åé¡¶æ›¿ç»¼åˆç—‡æ‚„ç„¶è¢­æ¥â€”â€”*æˆ‘èƒ½åœ¨LLMå¼€å‘ä¸Šåˆ†äº«45åˆ†é’Ÿä»€ä¹ˆå‘¢ï¼Ÿ* ä½†å½“æˆ‘åˆ›å»ºå¹»ç¯ç‰‡æ—¶ï¼Œæˆ‘æ„è¯†åˆ°æˆ‘æœ‰å¾ˆå¤šè¦è¯´çš„ï¼Œå¹¶å¯¹åˆ†äº«æˆ‘æ‰€å­¦åˆ°çš„æ·±åŽšçŸ¥è¯†æ„Ÿåˆ°å…´å¥‹ã€‚è¿™ç§å…´å¥‹ä¿ƒæˆäº†ä½ çŽ°åœ¨æ­£åœ¨é˜…è¯»çš„è¿™ç¯‡æ–‡ç« ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†è®²è¿°æˆ‘åœ¨ç”Ÿäº§ä¸­é‡åˆ°çš„ä¸€äº›å¸¸è§LLMæŒ‘æˆ˜ä»¥åŠå¸®åŠ©æˆ‘è§£å†³è¿™äº›é—®é¢˜çš„ç­–ç•¥ã€‚\n\n## 1\\. è¾“å‡ºæ ¼å¼é”™è¯¯\n\nè¿™å¯èƒ½æ˜¯æˆ‘é‡åˆ°çš„æœ€å¸¸è§é—®é¢˜ï¼Œä»¤äººæƒŠè®¶çš„æ˜¯ã€‚è¾“å‡ºæ ¼å¼çš„å¯é æ€§å¯èƒ½ä¼šå› æˆ‘ä½¿ç”¨çš„æ¨¡åž‹è€Œæ˜¾è‘—ä¸åŒã€‚ä¾‹å¦‚ï¼ŒGPT\\-4 Turbo é€šå¸¸æä¾›ä¸€è‡´çš„ JSON è¾“å‡ºï¼Œä½† GPT\\-4o åœ¨è¿™æ–¹é¢çš„å¯é æ€§å¾€å¾€è¾ƒå·®ã€‚åœ¨ GPT\\-4o ä¸­ï¼Œæˆ‘é‡åˆ°è¿‡ä»Žåˆ—è¡¨å’Œå­—ç¬¦ä¸²åˆ°ä¸å®Œæ•´å­—å…¸çš„å„ç§æƒ…å†µï¼Œå½“æ˜Žç¡®è¯·æ±‚ç»“æž„åŒ– JSON è¾“å‡ºæ—¶ã€‚å¦‚æžœè¿™äº›æ ¼å¼é—®é¢˜æ²¡æœ‰è¢«å‘çŽ°å¹¶ä¸”æ¨¡åž‹æ²¡æœ‰é‡æ–°è¿è¡Œï¼Œæˆ‘å¯èƒ½ä¼šé¢ä¸´æ•°æ®è¦†ç›–ä¸å®Œæ•´çš„é£Žé™©ã€‚\n\n### æ ¼å¼é”™è¯¯çš„å½±å“\n\nä¸ä¸€è‡´çš„è¾“å‡ºæ ¼å¼ä¼šå¯¹ä¸‹æ¸¸æµç¨‹äº§ç”Ÿé‡å¤§å½±å“ã€‚å¦‚æžœæ•°æ®ç»“æž„ä¸æ­£ç¡®ï¼Œå¯èƒ½ä¼šå¯¼è‡´åŽç»­å¤„ç†æ­¥éª¤çš„å¤±è´¥ï¼Œæ‰­æ›²æŠ¥å‘Šçš„å‡†ç¡®æ€§ï¼Œç”šè‡³åœ¨æœªè¢«å‘çŽ°çš„æƒ…å†µä¸‹å¯¼è‡´æ´žå¯Ÿä¸å®Œæ•´ã€‚åœ¨åŒ»ç–—ç­‰é«˜é£Žé™©é¢†åŸŸï¼Œæˆ‘çš„å·¥ä½œæ¶‰åŠæ­¤å¤„ï¼Œä¸å®Œæ•´æˆ–ç»“æž„é”™è¯¯çš„æ•°æ®å¯èƒ½ä¼šå¸¦æ¥å®žé™…å½±å“ï¼Œå› æ­¤æ ¼å¼çš„ä¸€è‡´æ€§è‡³å…³é‡è¦ã€‚\n\n### ç¼“è§£æŽªæ–½\n\nä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘å®žçŽ°äº†**æ ¼å¼æ£€æŸ¥é€»è¾‘**ï¼Œ**éªŒè¯è¾“å‡ºç»“æž„**ã€‚å¦‚æžœä¸æ­£ç¡®ï¼Œæˆ‘å°†é‡æ–°è¿è¡Œæ¨¡åž‹ï¼Œç›´åˆ°å®ƒç¬¦åˆé¢„æœŸæ ¼å¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä½¿ç”¨**æ—¥å¿—è®°å½•**æ¥æ•èŽ·ä¸Žæ ¼å¼ç›¸å…³çš„é”™è¯¯ã€‚ç„¶è€Œï¼Œé‡æ–°è¿è¡Œæ¨¡åž‹å¸¦æ¥äº†æƒè¡¡ï¼Œä¾‹å¦‚å¢žåŠ å»¶è¿Ÿå’Œæ›´é«˜çš„APIæˆæœ¬ã€‚æˆ‘æ ¹æ®æ•°æ®è¦†ç›–çš„å…³é”®æ€§å’Œæˆæœ¬é™åˆ¶å»ºç«‹äº†é‡æ–°è¿è¡Œçš„é˜ˆå€¼ã€‚å¦‚æžœé‡æ–°è¿è¡Œä¸å¯è¡Œï¼Œæˆ‘æœ‰æ—¶ä¼šåº”ç”¨åŽå¤„ç†æ¥â€œä¿®å¤â€è¾“å‡ºç»“æž„ï¼Œå°½ç®¡è¿™ç§æ–¹æ³•ä¹Ÿå­˜åœ¨å¼•å…¥é”™è¯¯æˆ–ä¸ä¸€è‡´çš„é£Žé™©ã€‚\n\nä¸ºäº†è¯´æ˜Žè¿™ç§æ–¹æ³•ï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªç¤ºä¾‹ä»£ç ç‰‡æ®µï¼Œå®ƒè¯·æ±‚ä»¥JSONæ ¼å¼è¿”å›žæ‚£è€…æ•°æ®ï¼Œå¹¶åŒ…å«ç‰¹å®šçš„é”®ï¼Œå¦‚`\"name\"`ã€`\"age\"`å’Œ`\"insurance\"`ã€‚è¿™æ®µä»£ç æ¼”ç¤ºäº†ä¸€ç§éªŒè¯æ¨¡åž‹å“åº”æ˜¯å¦åŒ…å«æ‰€æœ‰å¿…éœ€å­—æ®µå¹¶éµå¾ªé¢„æœŸç»“æž„çš„æ–¹æ³•ã€‚é€šè¿‡å®žçŽ°é‡è¯•é€»è¾‘ï¼Œè¯¥ä»£ç æ—¨åœ¨ç¡®ä¿æ•°æ®ä¸€è‡´æ€§ï¼Œå‡å°‘åœ¨å…³é”®å·¥ä½œæµç¨‹ä¸­ä¸Žæ ¼å¼é”™è¯¯ç›¸å…³çš„é£Žé™©ã€‚\n\n```python\ndef get_llm_response(prompt: str, required_keys: Set[str], retries: int = 3) -> Optional[Dict[str, Any]]:\n    \"\"\"\n    Calls the language model to get a response in JSON format. If the response \n    is not in the expected JSON format or lacks required keys, retries the call \n    up to `retries` times.\n    Parameters:\n        prompt (str): The prompt sent to the language model.\n        required_keys (Set[str]): A set of required keys that must be present in the JSON response.\n        retries (int): The maximum number of retries if the output format is invalid.\n    Returns:\n        Optional[Dict[str, Any]]: Parsed JSON response if successful; None if retries are exhausted.\n    \"\"\"\n    \n    for attempt in range(retries):\n        try:\n            response = openai.Completion.create(\n                model=\"gpt-4o\",\n                prompt=prompt,\n                max_tokens=100,\n                temperature=0.7\n            )\n            \n            # Attempt to parse the response as JSON\n            response_text = response.choices[0].text.strip()\n            parsed_response = json.loads(response_text)\n            \n            # Check if parsed_response is in the expected structure and contains required keys\n            if isinstance(parsed_response, dict) and required_keys.issubset(parsed_response.keys()):\n                return parsed_response\n            else:\n                print(f\"Attempt {attempt + 1}: Output format invalid or missing required keys, retrying...\")\n        except (json.JSONDecodeError, KeyError) as e:\n            print(f\"Attempt {attempt + 1}: Error parsing JSON - {str(e)}, retrying...\")\n    print(\"Max retries exceeded: Unable to get valid JSON output with required keys.\")\n    return None\n\n```\n\n## 2\\. å¹»è§‰\n\nå¹»è§‰å‘ç”Ÿåœ¨æ¨¡åž‹åˆ›é€ å‡ºå¬èµ·æ¥åˆç†ä½†å®žé™…ä¸Šå¹¶ä¸å­˜åœ¨çš„ä¿¡æ¯æ—¶ã€‚ä¾‹å¦‚ï¼Œå½“æˆ‘è¯•å›¾ä»Žæºæ–‡æœ¬ä¸­æå–å¼•ç”¨æ—¶ï¼Œæœ‰æ—¶æ¨¡åž‹ä¼šé€‰æ‹©â€œå‘æŒ¥åˆ›æ„â€ï¼Œäº§ç”Ÿç±»ä¼¼ä½†å®Œå…¨è™šæž„çš„çŸ­è¯­ã€‚åœ¨å‡†ç¡®æ€§è‡³å…³é‡è¦çš„é¢†åŸŸï¼Œå¦‚åŒ»ç–—ä¿å¥ï¼Œå¾®å°çš„å¹»è§‰å¯èƒ½å¯¼è‡´é‡å¤§é—®é¢˜ã€‚\n\n### ç¼“è§£\n\næˆ‘é€šè¿‡å®žæ–½åŽå¤„ç†é€»è¾‘æ¥è§£å†³å¹»è§‰é—®é¢˜ï¼Œä»¥éªŒè¯åœ¨ä»»ä½•ä¿¡æ¯æå–ä»»åŠ¡ä¸­ï¼Œæå–çš„ä¸Šä¸‹æ–‡ä¸Žæºæ–‡æœ¬å®Œå…¨åŒ¹é…ã€‚ä¸ºäº†ç¡®ä¿ç»†å¾®çš„å˜åŠ¨ä¸ä¼šå¯¼è‡´é—æ¼åŒ¹é…ï¼Œæˆ‘é€šè¿‡åŽ»é™¤æ ‡ç‚¹ç¬¦å·å¹¶åœ¨æ¯”è¾ƒæºæ–‡æœ¬å’Œæå–æ–‡æœ¬æ—¶å°†å…¶å…¨éƒ¨è½¬æ¢ä¸ºå°å†™æ¥æ ‡å‡†åŒ–æ–‡æœ¬ã€‚æ­¤å¤–ï¼Œè¿˜æœ‰å…¶ä»–å‡ ç§ç­–ç•¥æœ‰åŠ©äºŽæœ€å°åŒ–å¹»è§‰ã€‚ä¾‹å¦‚ï¼Œ**é“¾å¼æ€ç»´æç¤º**ï¼Œå³æ¨¡åž‹è§£é‡Šå…¶æŽ¨ç†çš„æ¯ä¸€æ­¥ï¼Œå¯ä»¥äº§ç”Ÿæ›´ä¸ºæ‰Žå®žçš„è¾“å‡ºï¼Œå¹¶é™ä½Žä¸å‡†ç¡®è¾“å‡ºçš„å¯èƒ½æ€§ã€‚åœ¨é«˜é£Žé™©åº”ç”¨ï¼ˆä¾‹å¦‚åŒ»ç–—ä¿å¥æ¡ˆä¾‹ï¼‰ä¸­ï¼Œ**äººæœºåä½œæ£€æŸ¥**ä½œä¸ºé¢å¤–çš„å®¡æŸ¥å±‚éžå¸¸é‡è¦ï¼Œæœ‰åŠ©äºŽæ•æ‰è‡ªåŠ¨åŒ–è¿‡ç¨‹å¯èƒ½é—æ¼çš„å¹»è§‰ã€‚æœ€åŽï¼Œå¼ºè°ƒäº‹å®žå‡†ç¡®æ€§çš„æç¤ºï¼Œä¾‹å¦‚æŒ‡ç¤ºæ¨¡åž‹â€œä»…ä½¿ç”¨æºæ–‡æœ¬ä¸­çš„ç¡®åˆ‡çŸ­è¯­â€ï¼Œå¯ä»¥å¼•å¯¼æ¨¡åž‹æœç€æ›´ç²¾ç¡®çš„å“åº”æ–¹å‘å‘å±•ã€‚\n\n## 3\\. è¿‡æ—¶ä¿¡æ¯\n\nè¿‡æ—¶çš„ä¿¡æ¯ç®¡ç†èµ·æ¥å¯èƒ½å¾ˆå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å‡†ç¡®æ€§å’ŒåŠæ—¶æ€§è‡³å…³é‡è¦çš„åº”ç”¨ä¸­ã€‚æœ‰æ—¶ï¼Œæ¨¡åž‹å¯èƒ½ä¼šä»Žæ–‡æ¡£çš„æ—§éƒ¨åˆ†æ£€ç´¢ä¿¡æ¯ï¼Œå¹¶å°†å…¶å‘ˆçŽ°ä¸ºå½“å‰ä¿¡æ¯ã€‚ä½¿ç”¨æ£€ç´¢å¢žå¼ºç”Ÿæˆï¼ˆRAGï¼‰æ—¶ï¼Œè¿™ä¸ªé—®é¢˜å¯èƒ½å˜å¾—æ›´åŠ å¤æ‚ï¼Œå› ä¸ºRAGä»…æ ¹æ®ç›¸å…³æ€§è€ŒéžåŠæ—¶æ€§æˆ–ç‰¹å®šæ–‡æ¡£éƒ¨åˆ†æ¥æ£€ç´¢å†…å®¹ã€‚ç¼ºå°‘éƒ¨åˆ†æ ‡ç­¾æˆ–æ—¶é—´æˆ³æ„å‘³ç€RAGå¯èƒ½ä¼šä»Žæ–‡æ¡£çš„ç›¸å…³éƒ¨åˆ†æå–ä¿¡æ¯ï¼Œè€Œä¸åŒºåˆ†è¿™äº›ä¿¡æ¯æ˜¯å¦è¿‡æ—¶ï¼Œè¿™å¯èƒ½å¯¼è‡´æ—§ä¿¡æ¯å’Œå½“å‰ä¿¡æ¯æ··åˆåœ¨ä¸€èµ·ã€‚ä½¿ç”¨å‘é‡æ•°æ®åº“çš„å¦ä¸€ä¸ªæŒ‘æˆ˜æ˜¯ï¼Œå¦‚æžœæˆ‘ä»¬å­˜å‚¨æ•´ä¸ªæ–‡æ¡£ï¼Œåˆ™æ— æ³•åœ¨æ²¡æœ‰æ˜Žç¡®æ ‡ç­¾çš„æƒ…å†µä¸‹è½»æ¾åˆ é™¤ç‰¹å®šéƒ¨åˆ†ï¼Œä»Žè€Œä½¿æœ‰æ•ˆè¿‡æ»¤æ— å…³ä¿¡æ¯å˜å¾—å›°éš¾ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*k9btdwyCCAb9qp92gB0PwA.png)\n\n### ç¼“è§£æŽªæ–½\n\nä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘åœ¨æç¤ºä¸­ç›´æŽ¥æŒ‡å®šâ€œå½“å‰â€æˆ–â€œæœ€æ–°â€æ•°æ®ï¼Œå¹¶ä½¿ç”¨é¢„å¤„ç†æ­¥éª¤åœ¨å°†æ•°æ®ä¼ é€’ç»™æ¨¡åž‹ä¹‹å‰åˆ é™¤ä»»ä½•è¿‡æ—¶çš„éƒ¨åˆ†ã€‚è¿™ä¸ªé¢å¤–çš„é¢„å¤„ç†æ­¥éª¤ç¡®ä¿ä»…ä¿ç•™æœ€æ–°ã€æœ€ç›¸å…³çš„ä¿¡æ¯ï¼Œå¸®åŠ©æ¨¡åž‹ä¸“æ³¨äºŽæä¾›åŠæ—¶å’Œå‡†ç¡®çš„å“åº”ã€‚è¿™ä¸ªæ­¥éª¤ä¸ä»…ç¡®ä¿äº†æ›´å‡†ç¡®çš„è¾“å‡ºï¼Œè¿˜é™ä½Žäº†è°ƒç”¨çš„æˆæœ¬ã€‚é€šè¿‡æå‰å®žæ–½è¿™äº›è¿‡æ»¤å™¨ï¼Œæˆ‘å¯ä»¥ä¿æŒæ¨¡åž‹è¾“å‡ºçš„ä¸€è‡´æ€§å’Œç›¸å…³æ€§ã€‚\n\n## 4\\. è¿‡åº¦ä¾èµ–ä¸Žä¼¦ç†\n\nå°½ç®¡æˆ‘å¸Œæœ›æˆ‘æ‰€åšçš„å·¥ä½œèƒ½å¤Ÿè¢«ä½¿ç”¨å¹¶ä¸”æœ‰ç”¨ï¼Œä½†æˆ‘æœ€å¤§çš„æ‹…å¿§æ˜¯ç”¨æˆ·ä¼šå¯¹æ¨¡åž‹é¢„æµ‹è¿‡äºŽä¿¡ä»»â€”â€”å°¤å…¶æ˜¯åœ¨åŒ»ç–—ä¿å¥é¢†åŸŸï¼Œç”Ÿæˆæ€§äººå·¥æ™ºèƒ½ä¸ä»…ä»…æ˜¯åœ¨åšé¢„æµ‹ï¼Œè¿˜ç»å¸¸åœ¨ç”Ÿæˆæ‘˜è¦æˆ–æå–ç‰¹å®šçš„æ‚£è€…ç»†èŠ‚ã€‚ä¸“å®¶ä»¬å¯¹æŸäº›å®šä¹‰å¯èƒ½æŒæœ‰ä¸åŒçš„çœ‹æ³•ï¼Œå› æ­¤å¤šæ ·æ€§å’Œå¯¹è¯å¯¹äºŽè¾¾æˆå…±è¯†éžå¸¸é‡è¦ã€‚è¿‡åº¦ä¾èµ–è¿™äº›é¢„æµ‹å¯èƒ½ä¼šå¯¼è‡´æŠ¤ç†å›¢é˜Ÿé™åˆ¶è¿™äº›å¯¹è¯ï¼Œå¹¶å¿½è§†ä»–ä»¬æœ¬å¯ä»¥æ›´ä»”ç»†æ£€æŸ¥çš„é”™è¯¯ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*6-0mq8Svxh8ATuyT)\n\n### ç¼“è§£\n\næˆ‘ä¼˜å…ˆæ•™è‚²å›¢é˜Ÿäº†è§£æ¨¡åž‹çš„å±€é™æ€§ï¼ŒåŒ…æ‹¬å…¶å‡ºé”™çš„å€¾å‘ï¼Œå¹¶é¼“åŠ±ä»–ä»¬å°†äººå·¥æ™ºèƒ½è§†ä¸ºäººç±»ä¸“ä¸šçŸ¥è¯†çš„è¡¥å……ã€‚åœ¨åŒ»ç–—ä¿å¥é¢†åŸŸï¼Œç»†å¾®å·®åˆ«è‡³å…³é‡è¦ï¼Œäººå·¥å¹²é¢„çš„ç›‘ç£å¯¹äºŽé«˜å½±å“åŠ›çš„æ¡ˆä¾‹è‡³å…³é‡è¦ï¼Œå…è®¸ä¸“å®¶å®¡æŸ¥äººå·¥æ™ºèƒ½çš„è¾“å‡ºï¼Œå‡å°‘å¯¹è¿‡åº¦ä¾èµ–çš„é£Žé™©ã€‚è¿™ç§åä½œæ–¹æ³•ä½¿äººå·¥æ™ºèƒ½èƒ½å¤Ÿå¢žå¼ºä¸“å®¶çš„è§è§£ï¼Œä¿æŒé«˜é£Žé™©åº”ç”¨æ‰€éœ€çš„å¯é æ€§å’Œä¼¦ç†å®Œæ•´æ€§ã€‚\n\n## 5\\. å¿«é€Ÿæ¨¡åž‹å¼ƒç”¨\n\néšç€äººå·¥æ™ºèƒ½çš„å‘å±•é€Ÿåº¦åŠ å¿«ï¼Œæ¨¡åž‹å’ŒAPIç‰ˆæœ¬æ›´æ–°é¢‘ç¹ï¼Œç‰ˆæœ¬è¢«å¼ƒç”¨çš„é€Ÿåº¦å¾€å¾€è¶…å‡ºé¢„æœŸã€‚å¦‚æžœæ‚¨æ›¾å› æ¨¡åž‹ç‰ˆæœ¬è¢«é€€å½¹è€Œå¯¼è‡´å·¥ä½œæµç¨‹æ„å¤–ä¸­æ–­ï¼Œæ‚¨ä¼šçŸ¥é“è¿™ä¼šé€ æˆå¤šå¤§çš„å¹²æ‰°ã€‚åœ¨è¿‡åŽ»ä¸€å¹´ä¸­ï¼Œè¿™ç§æƒ…å†µå‘ç”Ÿäº†å‡ æ¬¡ï¼Œè¿«ä½¿æˆ‘ä»¬è¿…é€Ÿé‡æ–°è¿›è¡Œåˆ†æžï¼Œä»¥ç¡®ä¿æ›´æ–°çš„æ¨¡åž‹ç‰ˆæœ¬ä»èƒ½æŒ‰é¢„æœŸè¡¨çŽ°ã€‚\n\n### ç¼“è§£\n\nå°†å®šæœŸæ£€æŸ¥æ¨¡åž‹ç‰ˆæœ¬å¹¶æå‰å¤„ç†å¼ƒç”¨è­¦å‘Šä½œä¸ºä¼˜å…ˆäº‹é¡¹ã€‚è¿™ç§ä¸»åŠ¨çš„æ–¹æ³•ä½¿æˆ‘ä»¬èƒ½å¤Ÿæå‰è§„åˆ’è¿‡æ¸¡ï¼Œé¿å…æœ€åŽæ—¶åˆ»çš„åŒ†å¿™ã€‚è™½ç„¶è¿™åªæ˜¯ä¸€ä¸ªå°æ­¥éª¤ï¼Œä½†åœ¨ä¿æŒé¡ºç•…æ“ä½œæ–¹é¢å´æœ‰ç€æ˜¾è‘—çš„å½±å“ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GK08JY3dcRUS4r6Z0x6EmA.png)\n\n## 6\\. APIçš„é€ŸçŽ‡é™åˆ¶\n\nAPIçš„é€ŸçŽ‡é™åˆ¶æ˜¯ä¸€ä¸ªå¾®å¦™ä½†é‡è¦çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤§é‡è¯·æ±‚æ—¶ã€‚è¾¾åˆ°é€ŸçŽ‡ä¸Šé™å¯èƒ½ä¼šå¯¼è‡´å»¶è¿Ÿï¼Œå‡æ…¢å®žæ—¶å·¥ä½œæµç¨‹ï¼Œç”šè‡³åœæ­¢æ•´ä¸ªè¿‡ç¨‹ã€‚åœ¨å¤„ç†æ—¶é—´æ•æ„Ÿæ•°æ®çš„æƒ…å†µä¸‹ï¼Œè¾¾åˆ°é™åˆ¶å¯èƒ½ä¼šé€ æˆä¸¥é‡å¹²æ‰°ï¼Œå› ä¸ºå·¥ä½œæµç¨‹ä¼šæ„å¤–ä¸­æ–­ã€‚è¿™åœ¨åŒ»ç–—çŽ¯å¢ƒä¸­å°¤å…¶æˆé—®é¢˜ï¼Œå› ä¸ºæ—¶æœºç›´æŽ¥å½±å“æ“ä½œå’Œæ‚£è€…æŠ¤ç†ã€‚\n\n### ç¼“è§£\n\nä¸ºäº†ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬é‡‡å–äº†ä¸»åŠ¨çš„æ–¹æ³•ï¼Œé€šè¿‡è·Ÿè¸ª API ä½¿ç”¨æ¨¡å¼æ¥è¯†åˆ«é«˜å³°æ—¶æ®µå¹¶å‡å°‘éžå¿…è¦çš„è°ƒç”¨ã€‚é€šè¿‡é”™å¼€è¯·æ±‚å’Œæ‰¹é‡è°ƒç”¨ï¼Œæˆ‘å¯ä»¥æ›´å‡åŒ€åœ°åˆ†é…è´Ÿè½½ï¼Œé¿å…è¶…è¿‡é™åˆ¶ã€‚åœ¨éœ€æ±‚é«˜æ¶¨ä¸”é€ŸçŽ‡é™åˆ¶æŒç»­è¾¾åˆ°çš„æƒ…å†µä¸‹ï¼Œå‘æä¾›è€…è¯·æ±‚é¢å¤–é…é¢å¯ä»¥æä¾›ä¸€ä¸ªåˆ‡å®žå¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚å¹³è¡¡ä½¿ç”¨è‡³å…³é‡è¦ï¼Œæå‰äº†è§£æˆ‘ä»¬çš„é«˜å³°æ—¶æ®µå’Œä½¿ç”¨æ¨¡å¼å¯¹äºŽç»´æŒç¨³å®šã€ä¸ä¸­æ–­çš„å·¥ä½œæµç¨‹è‡³å…³é‡è¦ã€‚\n\n## ç»“è®º\n\nè¿™äº›åªæ˜¯æˆ‘åœ¨ä¸Ž LLMs å·¥ä½œæ—¶é‡åˆ°çš„å…­ä¸ªå¸¸è§é—®é¢˜ã€‚æˆ‘æ²¡æœ‰æƒ³åˆ°è‡ªå·±ä¼šæ¥åˆ°è¿™é‡Œï¼Œä½†é€€ä¸€æ­¥æ€è€ƒï¼Œæˆ‘æ„è¯†åˆ°è‡ªå·±åœ¨è¿™ä¸ªé¢†åŸŸç§¯ç´¯äº†å¤šå°‘ä¸“ä¸šçŸ¥è¯†â€”â€”æˆ‘éžå¸¸å…´å¥‹èƒ½å¤Ÿåœ¨å³å°†å‘å¸ƒçš„æ–‡ç« ä¸­ç»§ç»­åˆ†äº«è¿™äº›ç»éªŒã€‚æˆ‘å¸Œæœ›èƒ½å¬åˆ°å…¶ä»–äººé‡åˆ°çš„æŒ‘æˆ˜ä»¥åŠä»–ä»¬æ‰¾åˆ°çš„æœ‰æ•ˆç¼“è§£ç­–ç•¥æˆ–è§£å†³æ–¹æ³•ï¼Œæ— è®ºæ˜¯ä¸Žè¿™äº›é—®é¢˜ç›¸å…³è¿˜æ˜¯å…¨æ–°çš„é—®é¢˜ã€‚æˆ‘å¸Œæœ›è¿™äº›è§è§£å¯¹æ‚¨æœ‰æ‰€å¸®åŠ©ï¼Œå¹¶æ¿€å‘å…³äºŽè¿™ä¸€å¿«é€Ÿå‘å±•çš„é¢†åŸŸï¼ˆæ¨¡åž‹ç‰ˆæœ¬å’Œ API ç‰ˆæœ¬æ›´æ–°å¾—å¤ªå¿«ï¼‰æœ€ä½³å®žè·µçš„è¿›ä¸€æ­¥è®¨è®ºã€‚\n\n"},{"lang":"zh","group":"blog","slug":"blog/qwen-new-release-the-king-of-coder-is-qwen2-5-coder-32b-8b96d442b280","frontmatter":{"title":"Qwen æ–°å‘å¸ƒï¼šç¼–ç å™¨ä¹‹çŽ‹æ˜¯ Qwen2.5 ç¼–ç å™¨ 32Bï¼","meta_title":"Qwen æ–°å‘å¸ƒï¼šç¼–ç å™¨ä¹‹çŽ‹æ˜¯ Qwen2.5 ç¼–ç å™¨ 32Bï¼","description":"Qwen2.5-Coder-32B-Instructæ˜¯æœ€æ–°å‘å¸ƒçš„AIç¼–ç æ¨¡åž‹ï¼Œè¡¨çŽ°ä¼˜å¼‚ï¼ŒåŸºå‡†åˆ†æ•°è¶…è¿‡GPT-4oï¼Œé€Ÿåº¦è¾¾åˆ°32 tokens/sã€‚è¯¥æ¨¡åž‹åœ¨Apache 2.0è®¸å¯ä¸‹å¼€æºï¼Œæ”¯æŒå¤šç§ç¡¬ä»¶é…ç½®ï¼ŒåŒ…æ‹¬å•ä¸ªGPU 3090å’Œè¾ƒå°çš„14Bæ¨¡åž‹ï¼Œé€‚ç”¨äºŽè®¡ç®—èƒ½åŠ›è¾ƒä½Žçš„ç”¨æˆ·ã€‚Ollamaå·²ä¸ºå¤šä¸ªæ¨¡åž‹æä¾›æ”¯æŒï¼Œç”¨æˆ·å¯é€šè¿‡ç®€å•å‘½ä»¤è¿è¡Œæ¨¡åž‹ã€‚","date":"2024-11-14T03:32:00.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*OzrZMolY75t_cdux5UGtIg.png","categories":["Programming","Technology","Machine Learning"],"author":"Rifx.Online","tags":["Qwen2.5","Coder","32B","Instruct","GPU"],"draft":false,"slug":"blog/qwen-new-release-the-king-of-coder-is-qwen2-5-coder-32b-8b96d442b280"},"content":"\nå¤§å®¶å¥½ï¼ä»‹ç»ä¸€ä¸‹ Qwen2\\.5\\-Coder\\-32B\\-Instructï¼šæœ€æ–°çš„ AI æ¨¡åž‹æ­£åœ¨å¼•é¢†ç¼–ç ç•Œçš„é£Žæ½®ï¼\n\n\n\nè¿™äº›æ¨¡åž‹å¤§å¤šåœ¨ Apache 2\\.0 è®¸å¯ä¸‹å‘å¸ƒã€‚åŸºå‡†åˆ†æ•°é«˜å¾—æƒŠäººï¼š\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*aHeNvfOvcpME0qzy6EQexQ.jpeg)\n\nå¦‚æˆ‘ä»¬æ‰€è§ï¼Œå®ƒåœ¨å¼€æºæ¨¡åž‹ä¸­è¡¨çŽ°æœ€ä½³ï¼Œç”šè‡³è¶…è¶Šäº† GPT\\-4oã€‚\n\nOllama å·²ç»ä¸ºå¤šä¸ªæ¨¡åž‹ç³»åˆ—æä¾›äº†æ”¯æŒã€‚\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*rV1xrpRXUjTFFoKwSsfeOg.png)\n\nå› æ­¤ï¼Œè¿è¡Œèµ·æ¥éžå¸¸ç®€å•ã€‚\n\n```python\nollama run qwen2.5-coder:32b\n```\n\n32Bï¼ˆQ4 æ ¼å¼ï¼‰åœ¨å•ä¸ª GPU 3090 ä¸Šçš„æ€§èƒ½å¯ä»¥ä»Žä»¥ä¸‹æˆªå›¾ä¸­æ‰¾åˆ°ï¼š\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MVQ0srQhRxX4Ifo3IqU6og.png)\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*jtH3ixeeOQGfyDzmO4Ni3A.png)\n\nå®ƒçš„é€Ÿåº¦æ˜¯ **32 tokens/s**ï¼è¶…çº§å¿«ã€‚æˆ‘éžå¸¸é«˜å…´å’Œå°è±¡æ·±åˆ»ã€‚\n\né™¤äº† 32B æ¨¡åž‹å¤–ï¼Œè¾ƒå°çš„æ¨¡åž‹åœ¨æ¨¡åž‹å¤§å°æ–¹é¢ä¹Ÿè¡¨çŽ°å‡ºè‰²ã€‚å¦‚æžœæ‚¨çš„è®¡ç®—èƒ½åŠ›ä¸è¶³ï¼Œå¯ä»¥å°è¯•ä¸€äº›è¾ƒå°çš„æ¨¡åž‹ã€‚ä¾‹å¦‚ï¼Œæˆ‘åœ¨æ–°æ¬¾é…å¤‡ M4 å¤„ç†å™¨å’Œ 16GB RAM çš„ Mac Mini ä¸Šå°è¯•äº† 14B æ¨¡åž‹ã€‚\n\nåœ¨åŸºå‡†åˆ†æ•°ä¹‹å¤–ï¼Œå…‰æ ‡çŽ°åœ¨å¯ä»¥ä¸Žæœ€æ–°çš„ Qwen æ¨¡åž‹é›†æˆï¼ŒåŒ…æ‹¬ Qwen 2\\.5\\-Coder\\-32B\\-Instruct å’Œ OpenWebUIã€‚\n\nä»¥ä¸‹æ˜¯ä½¿ç”¨ Qwen 2\\.5\\-Coder\\-32B\\-Instructï¼ˆOllamaï¼‰ä¸Ž OpenWebUI çš„æˆªå›¾ï¼š\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*q-Pq3snVhkBs3e_Oxj4_Xw.png)\n\næˆ‘è¿«ä¸åŠå¾…æƒ³åœ¨æ—¥å¸¸å·¥ä½œä¸­ä½¿ç”¨å®ƒï¼\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/qwen2-5-1-5b-the-future-of-mobile-ai-6bd5f29bbc84","frontmatter":{"title":"Qwen2.5 1.5bï¼šç§»åŠ¨AIçš„æœªæ¥ï¼Ÿ","meta_title":"Qwen2.5 1.5bï¼šç§»åŠ¨AIçš„æœªæ¥ï¼Ÿ","description":"é˜¿é‡Œäº‘æœ€æ–° LLM çš„æœ¬åœ°æµ‹è¯•å’Œè¯„ä¼°ã€‚ä½¿ç”¨ llama-cpp-python å’Œ DIY æç¤ºç›®å½•ã€‚","date":"2024-10-30T12:57:39.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*awb56jkdXobA-Ip6d-QHRA.png","categories":["Natural Language Processing","Programming","Technology/Web"],"author":"Rifx.Online","tags":["Qwen2.5","NLP","summarization","retrieval","prompts"],"draft":false,"slug":"blog/qwen2-5-1-5b-the-future-of-mobile-ai-6bd5f29bbc84"},"content":"\n### æœ¬åœ°æµ‹è¯•å’Œè¯„ä¼°é˜¿é‡Œäº‘æœ€æ–°çš„LLMã€‚ä½¿ç”¨llama\\-cpp\\-pythonå’ŒDIYæç¤ºç›®å½•ã€‚\n\n\n\nåœ¨ç¬¬ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å…±åŒæŽ¢è®¨äº†é˜¿é‡Œäº‘å›¢é˜Ÿå‘å¸ƒçš„Qwen2\\.5æ¨¡åž‹ç³»åˆ—çš„åˆ›æ–°ã€‚\n\nåœ¨ç”Ÿæˆå¼AIåŸºå‡†æµ‹è¯•ä¸­ï¼ŒåŸºå‡†æµ‹è¯•çŽ°åœ¨æ˜¯ä¸»è¦çš„*oracle*ï¼šæ–°çš„LLMçš„æœ‰æ•ˆæ€§éœ€è¦é€šè¿‡å¤šä¸ªè¯„åˆ¤ã€‚ä½ æ‰“ç ´çš„åŸºå‡†è®°å½•è¶Šå¤šï¼Œä½ å°±è¶Šä¼˜ç§€ã€‚\n\nè¿™æ˜¯èµ¢å¾—SOTAç«žèµ›çš„æ–¹å¼ã€‚\n\nå¥½å§ï¼Œæˆ‘ä¸åŒæ„ã€‚å°½ç®¡æˆ‘ä»¬éœ€è¦é‡Œç¨‹ç¢‘å’Œæ›´å¥½çš„æ€§èƒ½æ¥æŽ¨åŠ¨AIè¿›æ­¥ï¼Œä½†ç”¨æˆ·ä½“éªŒå’Œä¸ªäººè§‚ç‚¹ä¸èƒ½è¢«è§†ä¸ºæ— å…³ç´§è¦ã€‚\n\næˆ‘ç›¸ä¿¡ï¼Œåœ¨æŽ¢ç´¢ä¸€äº›å¸¸ç”¨çš„NLPä»»åŠ¡æ—¶ï¼ŒæŠ›å¼€èŠå¤©ä½“éªŒï¼Œæˆ‘ä»¬å¿…é¡»å…³æ³¨å›žå¤çš„è´¨é‡ã€‚æˆ‘ä»¬æ˜¯å”¯ä¸€éœ€è¦çš„åŸºå‡†ã€‚æˆ‘ä»¬çš„ç”¨æˆ·ä½“éªŒæ˜¯åˆ¤æ–­ä¸€ä¸ªæ¨¡åž‹æ˜¯å¦ä¼˜ç§€çš„æœ€ä½³æŒ‡æ ‡ã€‚æ¨¡åž‹å¿…é¡»è¶³å¤Ÿå¯é ï¼Œä»¥ä¾¿åœ¨è‡ªåŠ¨åŒ–å·¥ä½œæµç¨‹ä¸­ä½¿ç”¨ã€‚\n\né¡ºä¾¿æä¸€ä¸‹ï¼Œæˆ‘å·²ç»è¿è¡Œäº†æˆ‘å†³å®šç§°ä¹‹ä¸º[RBYF â€” ä¸Žæ‚¨ä½œä¸ºåé¦ˆçš„ä¿®è®¢åŸºå‡†](https://open.substack.com/pub/thepoorgpuguy/p/rbyf-is-here-revised-benchmarks-with?r=i78xo&utm_campaign=post&utm_medium=web)çš„æµ‹è¯•ï¼Œå£°ç§°æƒŠäººçš„Llama3\\.2â€“1B\\-instructâ€¦è€ŒQwen2\\.5â€“1\\.5båˆ™æ›´å¥½å¾—å¤šï¼\n\nå› æ­¤ï¼Œåœ¨æœ¬æ–‡ä¸­ï¼Œæ­£å¦‚æ‰¿è¯ºçš„é‚£æ ·ï¼Œæˆ‘ä»¬å°†äº²è‡ªéªŒè¯è¿™ä¸ªæ¨¡åž‹åœ¨æ—¥å¸¸ä½¿ç”¨ä¸­çš„è¡¨çŽ°æœ‰å¤šå¥½ã€‚\n\nå›žåˆ°æˆ‘ä»¬è‡ªå·±â€¦â€¦è®©æˆ‘ä»¬å¼€å§‹å§ï¼\n\n## éœ€æ±‚\n\nåœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†æž„å»ºä¸€ä¸ªæœ€å°çš„æ–‡æœ¬æŽ¥å£ï¼Œä»¥ä¾¿èƒ½å¤Ÿè¿è¡Œæ¨¡åž‹ã€æµ‹è¯•ä¸åŒçš„ä»»åŠ¡å¹¶ç­‰å¾…ç”¨æˆ·åé¦ˆä»¥è¿›è¡Œè¯„ä¼°ã€‚\n\néœ€æ±‚å¾ˆç®€å•ï¼Œä½†æˆ‘å»ºè®®æ‚¨åˆ›å»ºä¸€ä¸ªæ–°çš„é¡¹ç›®ç›®å½•å’Œä¸€ä¸ªè™šæ‹ŸçŽ¯å¢ƒã€‚\n\nåˆ›å»ºä¸€ä¸ª `venv`ï¼ˆéœ€è¦ Python 3\\.11\\+ï¼‰ï¼šæˆ‘åœ¨è¿è¡Œ Windows 11 çš„è¿·ä½ ç”µè„‘ä¸Šè¿›è¡Œäº†æµ‹è¯•ã€‚\n\n```python\n## create the virtual environment\npython -m venv venv\n## activate the venv\nvenv\\Scripts\\activate\n## Install the dependencies \npip install llama-cpp-python==0.3.0 tiktoken\n```\n\næˆ‘ä»¬éœ€è¦ä»Ž Hugging Face çš„å®˜æ–¹ qwen ä»“åº“ä¸‹è½½ GGUF æ–‡ä»¶ [https://huggingface.co/Qwen/Qwen2\\.5\\-1\\.5B\\-Instruct\\-GGUF](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF)ï¼šæˆ‘ä½¿ç”¨äº† `qwen2.5-1.5b-instruct-q5_k_m.gguf` ç‰ˆæœ¬ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*YtQJb_xyq_xcF40yRWPcZA.png)\n\nä¸€åˆ‡å‡†å¤‡å°±ç»ªï¼\n\næ³¨æ„ï¼šå¦‚æžœæ‚¨æƒ³æ·»åŠ å¯¹ GPU åŠ é€Ÿå™¨çš„ä¸åŒåŽç«¯æ”¯æŒï¼Œå¯ä»¥æŒ‰ç…§ [ä»“åº“ä¸­çš„è¯´æ˜Ž](https://github.com/abetlen/llama-cpp-python#supported-backends) è¿›è¡Œæ“ä½œã€‚ä¾‹å¦‚ï¼Œæˆ‘ä½¿ç”¨äº† Vulkan æ”¯æŒï¼Œå› æ­¤åœ¨ pip å®‰è£…ä¹‹å‰æˆ‘æ·»åŠ äº†çŽ¯å¢ƒå˜é‡\n\n```python\n## Vulkan support - for Windows\n$env:CMAKE_ARGS = \"-DGGML_VULKAN=on\"\n```\n\n## ä»£ç  â€” ä¸»åº”ç”¨ç¨‹åºå’Œåº“\n\nä¸ºäº†ä¿æŒä»£ç çš„ç®€æ´ï¼Œæˆ‘å†³å®šä½¿ç”¨å¤–éƒ¨åº“æ‰©å±•ä¸€äº›åŠŸèƒ½ã€‚å¥½å§ï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªåŠ©åº“ï¼Œæ‰€ä»¥è¿™é‡Œæ²¡æœ‰ç§˜å¯†ã€‚\n\næ‚¨å¯ä»¥åœ¨æˆ‘çš„æ–‡ç« ä¸­æ‰¾åˆ°æ‰€æœ‰ç»†èŠ‚ï¼š\n\nä¸ºäº†åŠ å¿«é€Ÿåº¦ï¼Œæ‚¨å¯ä»¥ç›´æŽ¥ [ä»Žè¿™é‡Œä¸‹è½½æ–‡ä»¶](https://github.com/fabiomatricardi/YouAreTheBenchmark/raw/main/QWEN2.5-1.5B/promptLibv2Qwen.py)ï¼šå®ƒåŒ…å«äº†ä¸Šè¿°æ–‡ç« ä¸­è®¨è®ºçš„ `promptLib` çš„ç‰ˆæœ¬ 2ï¼ˆåä¸º `promptLibv2Qwen.py`ï¼Œå¯¹ `Qwen2.5-1.5B-instruct` æ¨¡åž‹çš„æç¤ºè¿›è¡Œäº†å°‘é‡å¾®è°ƒï¼‰ã€‚\n\nå°†æ–‡ä»¶ä¿å­˜åœ¨ä¸»ç›®å½•ä¸­ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªåä¸º `main.py` çš„æ–°æ–‡ä»¶ã€‚\n\n```python\n## Chat with an intelligent assistant in your terminal  \n## MODEL: https://huggingface.co/Qwen\n## qwen2.5-1.5b-instruct-q5_k_m.gguf\nimport sys\nfrom time import sleep\nimport warnings\nwarnings.filterwarnings(action='ignore')\nimport datetime\nfrom promptLibv2Qwen import countTokens, writehistory, createCatalog\nfrom promptLibv2Qwen import genRANstring, createStats\nimport argparse\n### PREPARING FINAL DATASET\npd_id = []\npd_task = []\npd_vote = []\npd_remarks = []\n####################Add GPU argument in the parser###################################\nparser = argparse.ArgumentParser()\nparser.add_argument(\"-g\", \"--gpu\", type=int, default=0,nargs='?',\n                    help=\"The number of layers to load on GPU\")\nargs = parser.parse_args()\nif args.gpu == None:\n   ngpu_layers = 0 \nelse:\n    ngpu_layers = args.gpu\nprint(f'Selected GPU: offloading {ngpu_layers} layers...')   \n####################INITIALIZE THE MODEL###################################\nstops = ['<!im_end|>']\ntasks = createCatalog()\nmodelname = 'qwen2.5-1.5b-instruct-q5_k_m.gguf'\n## create THE LOG FILE \ncoded5 = genRANstring(5)\nlogfile = f'logs/Qwen2.5-1.5B-it_CPP_{coded5}_log.txt'\ncsvfile = f'logs/Qwen2.5-1.5B-it_CPP_{coded5}.csv'\nlogfilename = logfile\n#Write in the history the first 2 sessions\nwritehistory(logfilename,f'{str(datetime.datetime.now())}\\n\\nYour own LocalGPT with ðŸ’» {modelname}\\n---\\nðŸ§ ðŸ«¡: You are a helpful assistant.')  \nwritehistory(logfilename,f'ðŸ’»: How can I assist you today in writing?')\n```\n\nåœ¨è¿™é‡Œï¼Œæˆ‘ä»¬åªæ˜¯åœ¨åšå‡†å¤‡ï¼šæˆ‘ä»¬å¯¼å…¥åº“ï¼ŒåŒ…æ‹¬æˆ‘ä»¬è‡ªå·±çš„ `promptLibv2Qwen` ä»¥åŠ `argparse`ã€‚æˆ‘æƒ³å°è¯•ä¸€äº›æ–°ä¸œè¥¿ï¼š[argparse](https://realpython.com/command-line-interfaces-python-argparse/) æ˜¯ä¸€ä¸ªç”¨äºŽç»ˆç«¯ Python ç¨‹åºçš„ Python åº“ï¼Œæ‚¨å¯ä»¥ä»Žå‘½ä»¤è¡Œè¯»å–å¤šä¸ªå‚æ•°ã€‚\n\nåœ¨è¿™é‡Œï¼Œæˆ‘ä»¬åªæœ‰ä¸€ä¸ªå‚æ•°ï¼ˆæ²¡æœ‰å‚æ•°ï¼‰ï¼Œå¸¦æœ‰æ ‡å¿— `-g` æˆ– `--gpu`ã€‚å½“æ‚¨ä½¿ç”¨æ­¤å‚æ•°è¿è¡Œ Python ä»£ç æ—¶ï¼Œæˆ‘ä»¬å°†è®¾ç½® GPU å±‚çš„æ•°é‡ä¸ºæœ€å¤§å€¼ï¼ˆä½†æ‚¨å¯ä»¥è‡ªè¡Œæ›´æ”¹ï¼‰ã€‚\n\nç„¶åŽï¼Œæˆ‘ä»¬è®¾ç½®ä¸€äº›å…¨å±€å˜é‡ï¼Œè·¨æ•´ä¸ªä»£ç ä½¿ç”¨ï¼šä»»åŠ¡ã€æˆ‘ä»¬çš„æç¤ºé›†åˆã€åœæ­¢è¯å’Œæ—¥å¿—æ–‡ä»¶åã€‚\n\n> æ³¨æ„ï¼šæ‰€æœ‰æ—¥å¿—éƒ½ä¿å­˜åœ¨åä¸º `logs` çš„å­ç›®å½•ä¸­â€¦â€¦æ‰€ä»¥è¯·ç¡®ä¿åˆ›å»ºä¸€ä¸ªã€‚\n\næˆ‘ä»¬è¿˜å‡†å¤‡äº†æ‰€æœ‰ç›¸å…³ä¿¡æ¯ï¼Œä»¥ä¾¿å°†å…¶å­˜å‚¨åˆ°æ•°æ®é›†ä¸­ï¼Œç„¶åŽæœ€ç»ˆä¿å­˜åˆ° CSV æ–‡ä»¶ä¸­ï¼ˆä»¥ä¾¿è½»æ¾åˆ›å»ºæ€§èƒ½çŸ©é˜µï¼‰ã€‚\n\n```python\n### PREPARING FINAL DATASET\npd_id = []\npd_task = []\npd_vote = []\npd_remarks = []\n```\n\nç„¶åŽï¼Œæˆ‘ä»¬ä½¿ç”¨ Llama\\-CPP\\-python å°†æ¨¡åž‹åŠ è½½åˆ° RAMï¼ˆæ²¡æœ‰ GPUï¼‰æˆ– VRAMï¼ˆä½¿ç”¨ GPUï¼‰ä¸­ã€‚\n\n```python\n## LOAD THE MODEL\nprint(\"\\033[95;3;6m\")\nprint(\"1. Waiting 10 seconds for the API to load...\")\nfrom llama_cpp import Llama\nllm = Llama(\n            model_path='models/qwen2.5-1.5b-instruct-q5_k_m.gguf',\n            n_gpu_layers=ngpu_layers,\n            temperature=0.1,\n            n_ctx=8192,\n            max_tokens=1500,\n            repeat_penalty=1.178,\n            stop=stops,\n            verbose=False,\n            )\nprint(f\"2. Model {modelname} loaded with LlamaCPP...\")\nprint(\"\\033[0m\")  #reset all\nhistory = []\nprint(\"\\033[92;1m\")\nprint(f'ðŸ“Logfile: {logfilename}')\n```\n\né¡ºä¾¿è¯´ä¸€å¥ï¼Œæ‚¨å¯ä»¥åœ¨æˆ‘çš„ GitHub ä»“åº“ä¸­æ‰¾åˆ°æ‰€æœ‰ä»£ç ï¼š\n\nä¸‹ä¸€ä¸ªæ˜¯ä¸€æ¬¡æ€§çƒ­èº«æŽ¨ç†ï¼šæ¨¡åž‹ç¥žç»ç½‘ç»œå°†é¦–æ¬¡æ¿€æ´»ï¼Œæ‰€ä»¥å¯ä»¥æŠŠå®ƒçœ‹ä½œæ˜¯çƒ­èº«åœˆã€‚\n\nä¸è¦å®³æ€•ï¼Œæˆ‘ä¼šè§£é‡Šä»£ç ã€‚\n\n```python\n##################### ALIGNMENT FIRST GENERATION ##############################################\nquestion = 'Explain the plot of Cinderella in a sentence.'\ntest = [\n    {\"role\": \"user\", \"content\": question}\n]\nprint('Question:', question)\nstart = datetime.datetime.now()\nprint(\"ðŸ’» > \", end=\"\", flush=True)\nfull_response = \"\"\nfisrtround = 0\nfor chunk in llm.create_chat_completion(\n    messages=test,\n    temperature=0.25,\n    repeat_penalty= 1.31,\n    stop=stops,\n    max_tokens=1500,\n    stream=True,):\n    try:\n        if chunk[\"choices\"][0][\"delta\"][\"content\"]:\n            if fisrtround==0:\n                print(chunk[\"choices\"][0][\"delta\"][\"content\"], end=\"\", flush=True)\n                full_response += chunk[\"choices\"][0][\"delta\"][\"content\"]\n                ttftoken = datetime.datetime.now() - start  \n                fisrtround = 1\n            else:\n                print(chunk[\"choices\"][0][\"delta\"][\"content\"], end=\"\", flush=True)\n                full_response += chunk[\"choices\"][0][\"delta\"][\"content\"]                            \n    except:\n        pass      \ndelta = datetime.datetime.now() - start\noutput = full_response\nprint('')\nprint(\"\\033[91;1m\")\nrating = input('Rate from 0 (BAD) to 5 (VERY GOOD) the quality of generation> ')\nprint(\"\\033[92;1m\")\nstats = createStats(delta,question,output,rating,logfilename,'Alignment Generation',ttftoken)\nprint(stats)\nwritehistory(logfilename,f'''ðŸ‘¨â€ðŸ’» . {question}\nðŸ’» > {output}\n{stats}\n''')\n```\n\næˆ‘ä»¬è®¾ç½®äº†ç¬¬ä¸€ä¸ªç”¨æˆ·é—®é¢˜ï¼Œå¹¶å°†å…¶æ”¾å…¥ä¸€ä¸ªä¼—æ‰€å‘¨çŸ¥çš„èŠå¤©æ ¼å¼å­—å…¸ä¸­ã€‚ç„¶åŽæˆ‘ä»¬å¼€å§‹è®¡æ—¶ï¼ˆå¯¹é€Ÿåº¦ã€ä»¤ç‰Œè®¡æ•°ç­‰å¾ˆæœ‰ç”¨â€¦â€¦ï¼‰ã€‚\n\næˆ‘ä»¬è°ƒç”¨æŽ¨ç†æ–¹æ³• `create_chat_completion()`ï¼Œå…è®¸æˆ‘ä»¬ä»¥èŠå¤©æ ¼å¼æŽ¥å—æç¤ºï¼Œå¹¶é€ä¸ªä»¤ç‰Œæµå¼è¾“å‡ºç»“æžœã€‚\n\nç”±äºŽæ¨¡åž‹çš„ç¬¬ä¸€æ¬¡å›žå¤ä¸åŒ…å«ä»»ä½•è¾“å‡ºä»¤ç‰Œï¼ˆä»…åŒ…å«ç»Ÿè®¡ä¿¡æ¯ï¼‰ï¼Œæˆ‘ä»¬ä½¿ç”¨äº† try/except è¯­å¥ã€‚æ­¤å¤–ï¼Œç”±äºŽæˆ‘æƒ³çŸ¥é“ä½•æ—¶ç”Ÿæˆç¬¬ä¸€ä¸ªä»¤ç‰Œï¼Œæˆ‘ä»¬è®¾ç½®äº†ä¸€ä¸ªæ ‡å¿—å¹¶æš‚æ—¶åœæ­¢è®¡æ—¶ï¼Œå°†ä¿¡æ¯ä¿å­˜åœ¨ `ttftoken` å˜é‡ä¸­ã€‚\n\nåœ¨æµå¼è¾“å‡ºç»“æŸæ—¶ï¼Œæˆ‘ä»¬è®¡ç®—ä»Žå¼€å§‹åˆ°çŽ°åœ¨çš„æ—¶é—´å·®ï¼Œå¹¶ç­‰å¾…ç”¨æˆ·æä¾›å¯¹ç”Ÿæˆè¾“å‡ºçš„ä¸ªäººåé¦ˆï¼šä»Ž 0 åˆ° 5 è¯„åˆ†ï¼Œå¹¶æ·»åŠ ä¸ŽæŒ‡ä»¤æç¤ºå’Œç”¨æˆ·æ„å›¾çš„ä¸€è‡´æ€§ç›¸å…³çš„è¯„è®ºã€‚\n\næˆ‘ä»¬ä½¿ç”¨å†…éƒ¨åº“ `createStats()` æ¥æ‰“å°ç”Ÿæˆçš„æ‰€æœ‰ç»Ÿè®¡ä¿¡æ¯ï¼Œå¹¶å°†å…¶ä¿å­˜åˆ°æ—¥å¿—æ–‡ä»¶ä¸­ã€‚è¯¥å‡½æ•°çš„è¾“å‡ºå°†ç±»ä¼¼äºŽä»¥ä¸‹å†…å®¹ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8znYCqpisviXvYgrzjWF5w.png)\n\n## æç¤ºç›®å½• â€” æˆ‘ä»¬æƒ³è¦æµ‹è¯•çš„å†…å®¹\n\næˆ‘åœ¨è¿™é‡Œå†™äº†æˆ‘çš„ä¹ æƒ¯ã€‚æˆ‘æœ‰ä¸€ä¸ªæç¤ºç›®å½•ï¼Œæ¶µç›–äº†èŠå¤©æœºå™¨äººä¸­ä½¿ç”¨çš„è®¸å¤šä¸»è¦è¯­è¨€ä»»åŠ¡ï¼Œä¾‹å¦‚æ€»ç»“ã€ç®€çŸ­æ€»ç»“ã€éšæ„èŠå¤©ã€RAGã€çœŸå®žRAGç­‰ç­‰ã€‚\n\nè¿™ä¸ªæƒ³æ³•æ˜¯èƒ½å¤Ÿåœ¨5åˆ†é’Ÿå†…åŠ è½½æ¨¡åž‹ï¼Œå¹¶å¼€å§‹è¯„ä¼°æ¯ä¸ªä»»åŠ¡ã€‚åœ¨æ¯æ¬¡ç”Ÿæˆç»“æŸæ—¶ï¼Œç”¨æˆ·ä¼šè¢«æç¤ºç»™å‡ºä¸€ä¸ªåˆ†æ•°ï¼ˆä»Ž0åˆ°5çš„è¯„åˆ†ï¼‰å¹¶åœ¨éœ€è¦æ—¶ç•™ä¸‹ä»»ä½•è¯„è®ºã€‚\n\nè¿™å¾ˆå…³é”®ï¼šå¹¶ä¸æ˜¯æ‰€æœ‰æ¨¡åž‹éƒ½æ˜¯ä¸€æ ·çš„ï¼Œå¯¹æç¤ºä¸­çš„æŽªè¾žè¿›è¡Œå°çš„æˆ–å¤§çš„è°ƒæ•´æ€»æ˜¯å¿…éœ€çš„ã€‚\n\né‚£ä¹ˆå›žåˆ°ä»£ç â€¦â€¦å› ä¸ºä¹‹å‰çš„ä»£ç åªæ˜¯çƒ­èº«ï¼ŒçŽ°åœ¨å°†å¼€å§‹çœŸæ­£çš„whileå¾ªçŽ¯ï¼ŒéåŽ†æ•´ä¸ªæç¤ºç›®å½•ã€‚è¯·å‚è§ä¸‹é¢çš„å·¥ä½œæµç¨‹â€¦â€¦\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*EL0Q97Du6HwtcYQZ.png)\n\nä»£ç ä¸­åªæœ‰å°‘é‡æ›´æ”¹ï¼Œæˆ‘ä¼šæŒ‡å‡ºè¿™äº›æ›´æ”¹ï¼Œè¯·è€å¿ƒç­‰å¾…ã€‚\n\n```python\n############################# AUTOMATIC PROMPTING EVALUATION  11 TURNS #################################\nid =1\nfor items in tasks:\n    fisrtround = 0\n    task = items[\"task\"]\n    prompt = items[\"prompt\"]\n    test = []\n    print(f'NLP TAKS>>> {task}')\n    print(\"\\033[91;1m\")  #red\n    print(prompt)\n    test.append({\"role\": \"user\", \"content\": prompt})\n    print(\"\\033[92;1m\")\n    full_response = \"\"\n    start = datetime.datetime.now()\n    print(\"ðŸ’» > \", end=\"\", flush=True)\n    for chunk in llm.create_chat_completion(\n        messages=test,\n        temperature=0.15,\n        repeat_penalty= 1.31,\n        stop=stops,\n        max_tokens=1500,\n        stream=True,):\n        try:\n            if chunk[\"choices\"][0][\"delta\"][\"content\"]:\n                if fisrtround==0:\n                    print(chunk[\"choices\"][0][\"delta\"][\"content\"], end=\"\", flush=True)\n                    full_response += chunk[\"choices\"][0][\"delta\"][\"content\"]\n                    ttftoken = datetime.datetime.now() - start  \n                    fisrtround = 1\n                else:\n                    print(chunk[\"choices\"][0][\"delta\"][\"content\"], end=\"\", flush=True)\n                    full_response += chunk[\"choices\"][0][\"delta\"][\"content\"]                            \n        except:\n            pass      \n    delta = datetime.datetime.now() - start\n    print('')\n    print(\"\\033[91;1m\")\n    rating = input('Rate from 0 (BAD) to 5 (VERY GOOD) the quality of generation> ')\n    print(\"\\033[92;1m\")\n    stats = createStats(delta,prompt,full_response,rating,logfilename,task,ttftoken)\n    print(stats)\n    writehistory(logfilename,f'''ðŸ‘¨â€ðŸ’» > {prompt}\nðŸ’» > {full_response}\n{stats}\n''')\n    pd_id.append(id)\n    pd_task.append(task)\n    pd_vote.append(rating[:2])\n    pd_remarks.append(rating[2:])\n    id += 1\n## create dataframe and save to csv\nzipped = list(zip(pd_id,pd_task,pd_vote,pd_remarks))\nimport pandas as pdd\ndf = pdd.DataFrame(zipped, columns=['#', 'TASK', 'VOTE','REMARKS'])\n#saving the DataFrame as a CSV file \ndf_csv_data = df.to_csv(csvfile, index = False, encoding='utf-8') \nprint('\\nCSV String:\\n', df_csv_data)  \n```\n\nä¸»è¦æ›´æ”¹ä»…åœ¨å‰å‡ è¡Œï¼š\n\n```python\nfor items in tasks:\n    fisrtround = 0\n    task = items[\"task\"]\n    prompt = items[\"prompt\"]\n```\n\nå¦‚æžœä½ é˜…è¯»äº†å…³äºŽ`promptLib`çš„æ–‡ç« ï¼Œä½ åº”è¯¥ä¸ä¼šæ„Ÿåˆ°æƒŠè®¶ï¼šä½†å¦‚æžœä½ æ˜¯æ–°æ‰‹ï¼Œè¿™é‡Œæˆ‘ä»¬æ­£åœ¨éåŽ†ä¸€ä¸ªå­—å…¸åˆ—è¡¨ï¼Œå…·æœ‰ä»¥ä¸‹ç»“æž„ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*rGcKJWNzSUrcu4wi.png)\n\nå› æ­¤ï¼Œå¯¹äºŽç›®å½•ä¸­çš„æ¯ä¸ªæ¡ç›®ï¼ˆæ„å‘³ç€ä»»åŠ¡å’Œæç¤ºçš„å¯¹ï¼‰ï¼Œæˆ‘ä»¬æå–ä»»åŠ¡æè¿°å’Œä»»åŠ¡æç¤ºã€‚\n\n```python\ntest.append({\"role\": \"user\", \"content\": prompt})\n```\n\nç„¶åŽæˆ‘ä»¬åœ¨ä¸€ä¸ªä¸´æ—¶åˆ—è¡¨`test`ä¸­åˆ›å»ºèŠå¤©æ¨¡æ¿æ¶ˆæ¯ï¼Œå¹¶å°†å…¶ä¼ é€’ç»™`create_chat_template()`æ–¹æ³•è¿›è¡Œç”Ÿæˆã€‚\n\nå…¶ä»–å†…å®¹éƒ½æ˜¯ä¸€æ ·çš„ã€‚\n\nä¿å­˜æ–‡ä»¶ï¼Œå¹¶åœ¨æ¿€æ´»`venv`çš„æƒ…å†µä¸‹è¿è¡Œï¼š\n\n```python\npython main.py\n## å¦‚æžœä½ ä½¿ç”¨çš„æ˜¯GPUï¼Œè¿è¡Œ python main.py -g\n```\n\nè¿™å°†ä¸ºä½ æä¾›ç±»ä¼¼äºŽä¸‹é¢ç¤ºä¾‹çš„å†…å®¹â€¦â€¦\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MhhQu4lLjtU__Wjf0dSWBg.gif)\n\nè¯·æ³¨æ„ï¼Œåœ¨æ•´ä¸ªæç¤ºç›®å½•çš„æœ«å°¾ï¼Œä¼šåˆ›å»ºä¸€ä¸ª*csv*æ–‡ä»¶ï¼Œå…¶ä¸­åŒ…å«æ‰€æœ‰ä»»åŠ¡çš„æ‘˜è¦ï¼\n\n## æµ‹è¯•æ¦‚è¿°\n\næˆ‘ä½¿ç”¨äº†å‡ ä¸ªå°åž‹è¯­è¨€æ¨¡åž‹ï¼Œä»Ž [Qwen2â€“1\\.5B\\-instruct](https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF) åˆ° [Gemma2â€“2B\\-instruct](https://huggingface.co/bartowski/gemma-2-2b-it-GGUF)ï¼Œå†åˆ° [Llama3\\.2â€“1B\\-instruct](https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF)ï¼Œæœ€åŽæ˜¯æ–°çš„ [Qwen2\\.5â€“1\\.5B\\-instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF)ã€‚\n\nè™½ç„¶æˆ‘å¯¹ [Llama3\\.2â€“1B\\-instruct](https://generativeai.pub/llama3-2-1b-instruct-is-ok-but-not-good-enough-28f88046b63e) æ„Ÿåˆ°ç›¸å½“å¤±æœ›ï¼Œä½†å¯¹æ–°çš„ [Qwen2\\.5â€“1\\.5B\\-instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF) çš„å‡ºè‰²è¡¨çŽ°æ„Ÿåˆ°æƒŠè®¶ã€‚\n\nåœ¨æ¯æ¬¡ç”Ÿæˆç»“æŸæ—¶ï¼Œç”¨æˆ·ä¼šè¢«è¦æ±‚ç”¨ 0 åˆ° 5 ä¹‹é—´çš„åˆ†æ•°æ¥è¯„ä¼°ç»“æžœã€‚**åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç”¨æˆ·å°±æ˜¯æˆ‘â€¦â€¦**\n\nè¿™ç§å®šæ€§åˆ†æžç¡®å®žè¾ƒä¸ºç®€å•ï¼Œå› æ­¤æ¯ä¸ªåˆ†æ•°éƒ½æœ‰ç›¸åº”çš„æè¿°ï¼Œç”¨æˆ·å¯ä»¥æ·»åŠ è¯„è®ºï¼ˆâ€œä¸€äº›é”™è¯¯ä¿¡æ¯â€ï¼Œâ€œå¯èƒ½æ›´å¥½åœ°æ›´æ”¹æç¤ºä¸­çš„æŽªè¾žâ€ï¼‰\n\nè¿™é‡Œæ˜¯å®šæ€§çŸ©é˜µåŠå…¶æè¿°\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*eBdPfZtfr99MsvLh6tt42w.png)\n\n## å¥½ä¸Žå â€” ç»†èŠ‚\n\næ€»ç»“éžå¸¸å‡ºè‰²ã€‚åˆ—å‡ºé•¿æ–‡æœ¬çš„ä¸»è¦ä¸»é¢˜ä¹Ÿéžå¸¸å¥½ã€‚\n\nRAG ä»»åŠ¡ç›¸å½“å¿«é€Ÿï¼ˆå³ä½¿åœ¨æˆ‘çš„è¿·ä½  PC ä¸Šï¼‰ï¼ŒçœŸå®žçš„ RAGï¼ˆåœ¨ä¸Šä¸‹æ–‡ä¹‹å¤–æé—®ï¼‰ä¹Ÿå¾ˆåˆ°ä½ã€‚\n\næ‰€ä»¥çœŸçš„å¾ˆå¥½ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*DuV3LJep_PuDqiCcAMb6Cg.png)\n\nä¸è¿‡ä¹Ÿæœ‰ä¸€äº›ä¸è¶³ä¹‹å¤„ï¼šå³ä½¿æ¸©åº¦åªæœ‰ `0.15`ï¼Œæˆ‘åœ¨ä¸¤å¥æ€»ç»“ä»»åŠ¡ä¸­ä¹Ÿå¾—åˆ°äº†äº›è™šæž„çš„ä¿¡æ¯ã€‚è¿™ä¸å¥½ã€‚\n\n> æˆ‘å¸Œæœ›é€šè¿‡ç¨å¾®è°ƒæ•´æç¤ºï¼Œæˆ–è€…å°†æ¸©åº¦è®¾ä¸º `0` å¯ä»¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚\n\nå¦ä¸€ä¸ªäº‹å®žæ˜¯ï¼Œåˆ›æ„å†™ä½œç›¸å½“ç³Ÿç³•ï¼šåœ¨æµ‹è¯•ä¸­ï¼Œæˆ‘ä½¿ç”¨äº†ä¸€äº›å›ºå®šçš„ç”Ÿæˆå‚æ•°ã€‚\n\n```python\n        temperature=0.15,\n        repeat_penalty= 1.31,\n```\n\nå¯¹äºŽåˆ›æ„å†™ä½œï¼Œä½¿ç”¨ Qwen2\\.5â€“1\\.5B\\-instruct æ—¶ï¼Œæˆ‘ä»¬åº”è¯¥ä½¿ç”¨æ›´é«˜çš„ `repeat_penalty` å’Œæ›´é«˜çš„ `temperature`ã€‚\n\né¡ºä¾¿æä¸€ä¸‹ï¼Œæˆ‘å¿…é¡»è¯´åæ€æç¤ºä¹Ÿå¹¶ä¸å·®ï¼æ ‡ç­¾çš„å¼€é—­æ²¡æœ‰ä¿æŒï¼ˆå› æ­¤ä¸æ˜“å°†å…¶æ”¾å…¥ç®¡é“æˆ–å·¥ä½œæµä¸­ï¼‰ï¼Œä½†ç”Ÿæˆçš„æ•´ä½“æµç¨‹å’Œâ€œæ€ç»´é“¾â€æŽ¨ç†è¿‡ç¨‹ç›¸å½“ä¸é”™ã€‚\n\n```python\n<thinking>\nä½¿ç”¨ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åœ¨æ•™å­¦ä¸­çš„é‡è¦æ€§ä¸å®¹å°è§‘ï¼Œå› ä¸ºè¿™é¡¹æŠ€æœ¯åœ¨å½“å‰æ•™è‚²å®žè·µä¸­å…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼ŒåŒæ—¶ä¹Ÿæœ‰åŠ©äºŽå¡‘é€ æ›´å…·åˆ›æ–°æ€§çš„ç»ˆèº«å­¦ä¹ æ–¹æ³•ã€‚\n</thinking>\n\n**æ€ç»´é“¾ï¼š**\n1. **ç†è§£å½±å“**ï¼šAIå¯ä»¥è‡ªåŠ¨åŒ–é‡å¤æ€§ä»»åŠ¡ï¼Œå¹¶æ ¹æ®å­¦ç”Ÿçš„è¡¨çŽ°æ•°æ®æä¾›ä¸ªæ€§åŒ–åé¦ˆï¼ˆä¾‹å¦‚ï¼Œé€šè¿‡èŠå¤©æœºå™¨äººæˆ–è‡ªé€‚åº”è¯„ä¼°ï¼‰ã€‚\n2. **å¢žå¼ºå­¦ä¹ ä½“éªŒ**ï¼šé€šè¿‡æ•´åˆç”Ÿæˆæ€§AIï¼Œæ•™å¸ˆå¯ä»¥åˆ›å»ºæ›´å…·å¸å¼•åŠ›çš„è¯¾ç¨‹ï¼Œä»¥æ»¡è¶³å¤šæ ·åŒ–çš„å­¦ä¹ é£Žæ ¼ã€‚\n3. **æ”¹å–„å­¦ç”Ÿæˆæžœ**ï¼šä¸ªæ€§åŒ–æ•™å­¦å¯ä»¥å¸®åŠ©å­¦ç”Ÿæ›´å¥½åœ°ç†è§£å’Œè®°å¿†çŸ¥è¯†ã€‚\n\n**åæ€ï¼š**\n- å°†ç”ŸæˆæŠ€æœ¯æ•´åˆåˆ°æ•™è‚²ä¸­è‡³å…³é‡è¦ï¼Œå› ä¸ºè¿™ä¸Žæ—¥ç›Šæˆä¸ºæ•°å­—åŽŸä½æ°‘çš„å­¦ä¹ è€…çš„ä¸æ–­å˜åŒ–çš„éœ€æ±‚ç›¸ä¸€è‡´ï¼Œéœ€è¦ä¸€ç§å°†è¿™äº›æŠ€æœ¯çº³å…¥å…¶ä¸­çš„æ•™å­¦æ–¹æ³•ã€‚\n</thinking>\n\n<output>\næ•™å¸ˆåº”å­¦ä¹ å¦‚ä½•åˆ©ç”¨ç”Ÿæˆæ€§AIæ¥å¢žå¼ºä»–ä»¬çš„æ•™å­¦æ–¹æ³•å¹¶æ”¹å–„å­¦ç”Ÿæˆæžœã€‚è¿™åŒ…æ‹¬ç†è§£å…¶åœ¨è‡ªåŠ¨åŒ–ä»»åŠ¡ï¼ˆå¦‚è¯„åˆ†æˆ–åˆ›å»ºä¸ªæ€§åŒ–å­¦ä¹ è·¯å¾„ï¼‰æ–¹é¢çš„èƒ½åŠ›ã€‚\n\n**å­¦ä¹ è·¯å¾„ï¼š**\n1. **æ•™è‚²æŠ€æœ¯åŸºç¡€è¯¾ç¨‹**ï¼šé¦–å…ˆå­¦ä¹ æä¾›æ•™è‚²æŠ€æœ¯æ¦‚è¿°çš„è¯¾ç¨‹ï¼ŒåŒ…æ‹¬æ•°å­—å·¥å…·çš„åŸºç¡€çŸ¥è¯†ã€‚\n2. **AIåŸºç¡€è¯¾ç¨‹ï¼ˆCourseraï¼‰** - é€šè¿‡Courseraçš„å…è´¹åœ¨çº¿è¯¾ç¨‹äº†è§£AIæ¦‚å¿µåŠå…¶åœ¨æ•™è‚²ä¸­çš„åº”ç”¨ã€‚\n\n3. **ç”Ÿæˆå­¦ä¹ çš„ä¸“ä¸šè¯¾ç¨‹**ï¼šæŠ¥åå‚åŠ ä¸“æ³¨äºŽç”Ÿæˆå­¦ä¹ çš„è¯¾ç¨‹ï¼Œå¦‚â€œç”Ÿæˆè®¾è®¡â€æˆ–MITç­‰å¤§å­¦æä¾›çš„ç±»ä¼¼ä¸“ä¸šé¡¹ç›®ã€‚\n4. **ä¸“ä¸šå‘å±•ç ”è®¨ä¼šå’Œä¼šè®®** - å‚åŠ å½“åœ°æ•™è‚²ä¼šè®®ä¸Šä¸“æ³¨äºŽå°†AIæ•´åˆåˆ°æ•™å­¦å®žè·µä¸­çš„ç ”è®¨ä¼šã€‚\n\n5. **æ•™è‚²å·¥ä½œè€…åœ¨çº¿ç¤¾åŒºï¼ˆä¾‹å¦‚ï¼ŒEdmodoï¼‰**ï¼šåŠ å…¥åœ¨çº¿ç¤¾åŒºï¼Œä¸Žæ•™è‚²å·¥ä½œè€…è®¨è®ºç”ŸæˆæŠ€æœ¯åœ¨æ•™è‚²ä¸­çš„åº”ç”¨ï¼Œåˆ†äº«èµ„æºæˆ–è¯¢é—®å®žæ–½é—®é¢˜ã€‚\n6. **è®¤è¯é¡¹ç›®**ï¼šè€ƒè™‘é€šè¿‡åƒè°·æ­Œçš„â€œAI for Educatorsâ€é¡¹ç›®ç­‰æœºæž„èŽ·å¾—è®¤å¯æ‚¨åœ¨æ•™å­¦å®žè·µä¸­æ•´åˆAIç›¸å…³çŸ¥è¯†å’ŒæŠ€èƒ½çš„è®¤è¯ã€‚\n\né€šè¿‡éµå¾ªè¿™æ¡å­¦ä¹ è·¯å¾„ï¼Œæ•™å¸ˆä¸ä»…å¯ä»¥å¢žå¼ºè‡ªèº«çš„ä¸“ä¸šå‘å±•ï¼Œè¿˜å¯ä»¥é€šè¿‡æœ‰æ•ˆåœ°æ•´åˆç”ŸæˆæŠ€æœ¯ï¼Œä¸ºæ•™è‚²çš„æœªæ¥åšå‡ºç§¯æžè´¡çŒ®ã€‚\n</output>\n```\n\næˆ‘è®¤ä¸ºï¼Œå¯¹äºŽè¿™ä¸ªå°åž‹è¯­è¨€æ¨¡åž‹çš„è§„æ¨¡ï¼Œç»“æžœå¹¶ä¸å·®ï¼\n\n## æ¯”è¾ƒ Qwen2\\.5 å’Œ Llama3\\.2\n\nå°½ç®¡è¿™ä»…ä»…æ˜¯æˆ‘ä¸ªäººçš„è¯„ä¼°ï¼Œæˆ‘è¿˜æ˜¯æƒ³å’Œä½ åˆ†äº«ä¸€ä¸‹ã€‚\n\nè¿™ä¸¤ä¸ªæ¨¡åž‹éƒ½æ˜¯ä¸ºç§»åŠ¨è®¾å¤‡è®¾è®¡çš„ï¼Œä½†æ€§èƒ½å·®å¼‚å¾ˆå¤§ã€‚è¯·çœ‹ä¸‹é¢ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*T6vLgvOKdkotlV1K5x6-QQ.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*DuV3LJep_PuDqiCcAMb6Cg.png)\n\né¦–å…ˆï¼Œæ•´ä½“è¯„åˆ†å·®å¼‚å·¨å¤§ï¼ˆLlama3\\.2 ä¸º 41ï¼ŒQwen2\\.5 ä¸º 57\\ï¼‰ã€‚\n\nå…¶æ¬¡ï¼Œå¦‚æžœä½ è€ƒè™‘åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šå¯èƒ½ä¼šé—®çš„é—®é¢˜ï¼Œè¯­è¨€ä»»åŠ¡æ–¹é¢ï¼Œä¸»è¦æ˜¯å¸Œæœ›æœ‰æµç•…çš„èŠå¤©ä½“éªŒï¼ˆä»»åŠ¡ 4\\ï¼‰ã€è‰¯å¥½çš„æ‘˜è¦èƒ½åŠ›ï¼ˆä»»åŠ¡ 5 åˆ° 7\\ï¼‰ä»¥åŠä¸€äº›åˆ›é€ æ€§å†™ä½œï¼ˆä»»åŠ¡ 11 å’Œ 13\\ï¼‰ã€‚\n\nåœ¨é€Ÿåº¦æ–¹é¢ï¼Œä»…åœ¨ CPU ä¸Šè¿è¡Œæ¨¡åž‹ï¼Œä½¿ç”¨éžå¸¸æœ‰é™çš„è¿·ä½  PCï¼Œ**æˆ‘èŽ·å¾—äº†å¹³å‡æŽ¨ç†é€Ÿåº¦ä¸º 14 t/sã€‚**\n\n## ç»“è®º\n\nåœ¨ Qwen2 å‘å¸ƒçš„è¿‡åŽ»ä¸‰ä¸ªæœˆé‡Œï¼Œä¼—å¤šå¼€å‘è€…åœ¨ Qwen2 è¯­è¨€æ¨¡åž‹ä¸Šæž„å»ºäº†æ–°æ¨¡åž‹ï¼Œä¸ºæ•´ä¸ªç¤¾åŒºä»¥åŠé˜¿é‡Œäº‘æä¾›äº†å®è´µçš„åé¦ˆã€‚\n\n> åœ¨æ­¤æœŸé—´ï¼Œæˆ‘ä»¬ä¸“æ³¨äºŽåˆ›é€ æ›´æ™ºèƒ½ã€æ›´æœ‰çŸ¥è¯†çš„è¯­è¨€æ¨¡åž‹ã€‚ä»Šå¤©ï¼Œæˆ‘ä»¬å¾ˆé«˜å…´åœ°ä»‹ç» Qwen å®¶æ—çš„æœ€æ–°æˆå‘˜ï¼šQwen2\\.5\n\nä»–ä»¬çš„å£°æ˜Žä¼´éšç€å…³äºŽæ–°æ¨¡åž‹å®¶æ—çš„äº‹å®žï¼š\n\n* å¯†é›†åž‹ã€**æ˜“äºŽä½¿ç”¨**çš„ä»…è§£ç å™¨è¯­è¨€æ¨¡åž‹ï¼Œæä¾› 0\\.5Bã€1\\.5Bã€3Bã€7Bã€14Bã€32B å’Œ 72B å°ºå¯¸ï¼Œä»¥åŠåŸºç¡€å’ŒæŒ‡ä»¤å˜ä½“ã€‚\n* åœ¨æˆ‘ä»¬æœ€æ–°çš„å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œæ¶µç›–å¤šè¾¾ 18T çš„æ ‡è®°ã€‚\n* **æŒ‡ä»¤è·Ÿéš**æ–¹é¢çš„æ˜¾è‘—æ”¹è¿›\n* å¯¹ç³»ç»Ÿæç¤ºçš„å¤šæ ·æ€§**æ›´å…·å¼¹æ€§**ï¼Œå¢žå¼ºè§’è‰²æ‰®æ¼”å®žæ–½å’ŒèŠå¤©æœºå™¨äººçš„æ¡ä»¶è®¾ç½®ã€‚\n* **æ”¯æŒé«˜è¾¾ 128K** çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œå¹¶å¯ä»¥ç”Ÿæˆæœ€å¤š 8K çš„æ ‡è®°ã€‚\n* æ”¯æŒè¶…è¿‡ 29 ç§è¯­è¨€çš„å¤šè¯­è¨€åŠŸèƒ½\n\nåœ¨æˆ‘å¹¿æ³›çš„ï¼ˆä½†ç¡®å®žé™äºŽå•æ¬¡æç¤ºå’Œå°‘æ•° NLP ä»»åŠ¡ï¼‰æµ‹è¯•ä¸­ï¼Œæˆ‘äº²çœ¼çœ‹åˆ°è¿™äº›å£°æ˜Žæ˜¯åŸºäºŽé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®é›†å’Œç²¾å¿ƒç­–åˆ’çš„å¾®è°ƒã€‚\n\nè¯¥æ¨¡åž‹åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šè¡¨çŽ°æžä¸ºå‡ºè‰²ï¼\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/qwen2-5-coder-32b-instruct-a-best-coding-model-a-complete-step-by-step-guide-and-performance-b8a33ec2547f","frontmatter":{"title":"Qwen2.5-Coder 32B Instructï¼šæœ€ä½³ç¼–ç æ¨¡åž‹--å®Œæ•´çš„åˆ†æ­¥æŒ‡å—å’Œæ€§èƒ½...","meta_title":"Qwen2.5-Coder 32B Instructï¼šæœ€ä½³ç¼–ç æ¨¡åž‹--å®Œæ•´çš„åˆ†æ­¥æŒ‡å—å’Œæ€§èƒ½...","description":"Qwen2.5-Coderç³»åˆ—ï¼Œå°¤å…¶æ˜¯32Bæ¨¡åž‹ï¼Œåœ¨ä»£ç ç”Ÿæˆã€ä¿®å¤å’ŒæŽ¨ç†æ–¹é¢å±•çŽ°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿä¸ŽGPT-4oç­‰æˆç†Ÿæ¨¡åž‹ç›¸åª²ç¾Žã€‚è¯¥æ¨¡åž‹æ”¯æŒè¶…è¿‡40ç§ç¼–ç¨‹è¯­è¨€ï¼Œé€‚åº”å¤šç§å¼€å‘éœ€æ±‚ï¼Œæä¾›çµæ´»çš„æ¨¡åž‹é€‰æ‹©ã€‚é€šè¿‡åŸºå‡†æµ‹è¯•ï¼ŒQwen2.5-Coderåœ¨å¤šé¡¹æŒ‡æ ‡ä¸­è¡¨çŽ°ä¼˜å¼‚ï¼Œé€‚ç”¨äºŽç¼–ç åŠ©æ‰‹ã€è‡ªåŠ¨åŒ–ä»£ç å®¡æŸ¥å’Œæ•™è‚²å·¥å…·ç­‰åœºæ™¯ï¼ŒæŽ¨åŠ¨å¼€æºä»£ç ç”Ÿæˆçš„è¿›æ­¥ã€‚","date":"2024-11-14T03:29:09.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*zjZmLCEX5URAc1wxTGnBRQ.png","categories":["Programming","Machine Learning","Generative AI"],"author":"Rifx.Online","tags":["Qwen2.5","Coder","programming","languages","repair"],"draft":false,"slug":"blog/qwen2-5-coder-32b-instruct-a-best-coding-model-a-complete-step-by-step-guide-and-performance-b8a33ec2547f"},"content":"\n### å­¦ä¹ å¦‚ä½•åœ¨æœ¬åœ°å®‰è£… Qwen2.5-Coderï¼ŒæŽ¢ç´¢å…¶å“è¶Šçš„ç¼–ç èƒ½åŠ›ï¼Œå¹¶é€šè¿‡å®žè·µç¤ºä¾‹è¯„ä¼°å…¶æ€§èƒ½\n\n\n\n## ä»‹ç»\n\nåœ¨ä¸æ–­å‘å±•çš„AIé©±åŠ¨ç¼–ç¨‹å·¥å…·é¢†åŸŸï¼Œå¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰æ˜¾è‘—æ”¹å˜äº†å¼€å‘è€…ç¼–å†™ã€è°ƒè¯•å’Œä¼˜åŒ–ä»£ç çš„æ–¹å¼ã€‚ä»Šå¤©ï¼Œæˆ‘ä»¬å¾ˆé«˜å…´æŽ¢ç´¢**Qwen2.5-Coder**ç³»åˆ—ï¼Œè¿™æ˜¯ä¸€é¡¹å¼€æºçš„å¥‡è¿¹ï¼Œæ‰¿è¯ºåœ¨ä»£ç ç”Ÿæˆå’ŒAIç¼–ç åŠ©æ‰‹é¢†åŸŸæ ‘ç«‹æ–°çš„æ ‡å‡†ã€‚è¯¥ç³»åˆ—çš„æœ€æ–°ç‰ˆæœ¬**Qwen2.5-Coder-32B-Instruct**é‡æ–°å®šä¹‰äº†å¼€æºç¼–ç æ¨¡åž‹çš„æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼ˆSOTAï¼‰ï¼Œä¸Ž**GPT-4o**ç­‰æˆç†Ÿæ¨¡åž‹çš„èƒ½åŠ›ç›¸åª²ç¾Žã€‚è®©æˆ‘ä»¬æ·±å…¥äº†è§£æ˜¯ä»€ä¹ˆè®©Qwen2.5-Coderå¦‚æ­¤â€œå¼ºå¤§â€ã€â€œå¤šæ ·â€å’Œâ€œå®žç”¨â€ã€‚\n\nåœ¨æœ¬ç»¼åˆæŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†æŽ¢è®¨**Qwen2.5-Coder-32B**æ¨¡åž‹çš„æ ¸å¿ƒèƒ½åŠ›ã€‚æˆ‘ä»¬å°†æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨`transformers`åº“ï¼Œæµ‹è¯•å…¶ç¼–ç èƒ½åŠ›å¹¶çªå‡ºå…¶å®žé™…åº”ç”¨ã€‚\n\n## ä¸ºä»€ä¹ˆé€‰æ‹© Qwen2\\.5\\-Coder?\n\n### å…³é”®äº®ç‚¹\n\n1. **å¼ºå¤§**ï¼šæ——èˆ° **Qwen2\\.5\\-Coder\\-32B** æ¨¡åž‹åœ¨ä¸»è¦ç¼–ç åŸºå‡†æµ‹è¯•ä¸­ä¸Ž GPT\\-4 çš„ç¼–ç èƒ½åŠ›ç›¸åŒ¹é…ï¼ŒåŒæ—¶åœ¨ä¸€èˆ¬å’Œæ•°å­¦æŠ€èƒ½æ–¹é¢è¡¨çŽ°ä¼˜å¼‚ã€‚\n2. **å¤šæ ·**ï¼šæ­¤æ¬¡å‘å¸ƒæ¶µç›–å¤šç§æ¨¡åž‹å°ºå¯¸ï¼ˆ0\\.5B, 1\\.5B, 3B, 7B, 14B, 32Bï¼‰ï¼Œä¸ºä¸åŒèµ„æºé™åˆ¶æä¾›çµæ´»æ€§ã€‚\n3. **å®žç”¨**ï¼šæ—¨åœ¨ç”¨äºŽå®žé™…åº”ç”¨ï¼ŒåŒ…æ‹¬ä»£ç åŠ©æ‰‹å’Œæ–‡æ¡£ç”Ÿæˆã€‚æ¨¡åž‹é‡‡ç”¨ **Apache 2\\.0** è®¸å¯ï¼Œç¡®ä¿å¯è‡ªç”±ä½¿ç”¨å’Œä¿®æ”¹ï¼Œé€‚ç”¨äºŽå•†ä¸šå’Œç ”ç©¶ç›®çš„ã€‚\n\n## Qwen2\\.5\\-Coderç³»åˆ—ï¼šå¼€æ”¾ä»£ç LLMçš„æ¸¸æˆè§„åˆ™æ”¹å˜è€…\n\n**Qwen2\\.5\\-Coder**ç³»åˆ—è‡´åŠ›äºŽæŽ¨åŠ¨å¼€æºä»£ç ç”Ÿæˆçš„è¾¹ç•Œã€‚è¯¥ç‰ˆæœ¬ä¸“æ³¨äºŽçµæ´»æ€§å’Œå¯æ‰©å±•æ€§ï¼ŒåŒ…å«å¤šç§è§„æ¨¡çš„æ¨¡åž‹ï¼š**0\\.5B, 1\\.5B, 3B, 7B, 14B**ï¼Œä»¥åŠæ——èˆ°ç‰ˆ**32B**ã€‚è¿™äº›æ¨¡åž‹æ»¡è¶³äº†ä¸åŒå¼€å‘è€…çš„éœ€æ±‚ï¼Œä»Žè½»é‡çº§ã€èµ„æºé«˜æ•ˆçš„æ¨¡åž‹åˆ°é€‚ç”¨äºŽé«˜è¦æ±‚åº”ç”¨çš„é«˜å®¹é‡ã€åŠŸèƒ½ä¸°å¯Œçš„æ¨¡åž‹ã€‚\n\n### 1\\. å¼ºå¤§ï¼šåœ¨ä»£ç ç”Ÿæˆä¸­è®¾å®šæ–°æ ‡å‡†\n\nQwen2\\.5\\-Coder\\-32B\\-Instruct ä½œä¸ºæ——èˆ°æ¨¡åž‹ï¼Œæ‹¥æœ‰ä¸€ç³»åˆ—èƒ½åŠ›ï¼Œä½¿å…¶èŽ·å¾—äº† **å½“å‰ SOTA å¼€æºä»£ç æ¨¡åž‹** çš„ç§°å·ã€‚å®ƒåœ¨ä»¥ä¸‹æ–¹é¢è¡¨çŽ°å‡ºè‰²ï¼š\n\n* **ä»£ç ç”Ÿæˆ**ï¼šåœ¨ **EvalPlusã€LiveCodeBench** å’Œ **BigCodeBench** ç­‰çƒ­é—¨åŸºå‡†æµ‹è¯•ä¸­ï¼Œå…¶æ€§èƒ½ä¸Ž GPT\\-4o ç›¸åŒ¹é…ï¼Œèƒ½å¤Ÿåœ¨å¤šç§åœºæ™¯ä¸­æä¾›ç²¾ç¡®çš„ä»£ç ç”Ÿæˆã€‚\n* **ä»£ç ä¿®å¤**ï¼šä¿®å¤æŸåæˆ–ä½Žæ•ˆçš„ä»£ç åœ¨è½¯ä»¶å¼€å‘ä¸­è‡³å…³é‡è¦ã€‚åœ¨æµ‹è¯•ä»£ç ä¿®å¤æŠ€èƒ½çš„ **Aider åŸºå‡†** ä¸­ï¼ŒQwen2\\.5\\-Coder\\-32B\\-Instruct å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ **73\\.7** åˆ†ï¼Œå ªæ¯” GPT\\-4o çš„èƒ½åŠ›ã€‚\n* **ä»£ç æŽ¨ç†**ï¼šç†è§£å’ŒæŽ¨ç†ä»£ç æ‰§è¡Œè·¯å¾„çš„èƒ½åŠ›å¯¹è°ƒè¯•å’Œä¼˜åŒ–å¤æ‚è½¯ä»¶è‡³å…³é‡è¦ã€‚è¯¥æ¨¡åž‹çš„èƒ½åŠ›ä¸ä»…é™äºŽç®€å•çš„ç”Ÿæˆ â€” å®ƒåœ¨ **é¢„æµ‹è¾“å…¥å’Œè¾“å‡º** æ–¹é¢è¡¨çŽ°å‡ºè‰²ï¼Œä½¿å…¶æˆä¸ºè½¯ä»¶å·¥ç¨‹å¸ˆçš„å®è´µå·¥å…·ã€‚\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*-g1ZGa0p2kKsQK4iD7q7cg.png)\n\n### 2\\. å¤šæ ·æ€§ï¼šæ”¯æŒå¤šç§ç¼–ç¨‹è¯­è¨€å’Œä¸°å¯Œçš„æ¨¡åž‹å¤§å°\n\nQwen2.5-Coder çš„å¤šåŠŸèƒ½æ€§ä½“çŽ°åœ¨å…¶å¯¹è¶…è¿‡ **40 ç§ç¼–ç¨‹è¯­è¨€** çš„æ”¯æŒï¼ŒåŒ…æ‹¬ **Haskell** å’Œ **Racket** ç­‰å°ä¼—è¯­è¨€ã€‚è¿™ç§å¹¿æ³›çš„æ”¯æŒå¾—ç›ŠäºŽç»†è‡´çš„æ•°æ®æ¸…ç†å’Œå‡è¡¡çš„è®­ç»ƒï¼Œç¡®ä¿æ¨¡åž‹åœ¨ä¸åŒçš„ç¼–ç çŽ¯å¢ƒä¸­è¡¨çŽ°æœ€ä½³ã€‚\n\n* **å¤šè¯­è¨€ä»£ç ä¿®å¤**ï¼šå®ƒçš„ä¸“ä¸šèƒ½åŠ›æ‰©å±•åˆ°å¯¹ä¸ç†Ÿæ‚‰è¯­è¨€çš„ä»£ç ä¿®å¤ï¼Œè¿™å¯ä»¥æ˜¾è‘—å‡å°‘å¼€å‘è€…æŽ¢ç´¢æ–°æŠ€æœ¯çš„å­¦ä¹ æ›²çº¿ã€‚\n* **æ¨¡åž‹å¤§å°çµæ´»æ€§**ï¼šQwen2.5-Coder ç³»åˆ—æä¾›å…­ç§ä¸åŒå¤§å°çš„æ¨¡åž‹ï¼Œç¡®ä¿å…·æœ‰ä¸åŒèµ„æºé™åˆ¶çš„å¼€å‘è€…èƒ½å¤Ÿæ‰¾åˆ°é€‚åˆå…¶éœ€æ±‚çš„æ¨¡åž‹ã€‚æ”¯æ’‘è¿™äº›æ¨¡åž‹çš„ **æ‰©å±•æ³•åˆ™** å“²å­¦æ„å‘³ç€æ€§èƒ½ä¸Žæ¨¡åž‹å¤§å°å‘ˆæ­£ç›¸å…³ï¼Œèµ‹äºˆå¼€å‘è€…åœ¨æ€§èƒ½å’Œè®¡ç®—èµ„æºä¹‹é—´é€‰æ‹©åˆé€‚å¹³è¡¡çš„çµæ´»æ€§ã€‚\n\n## æ€§èƒ½æ´žå¯Ÿï¼šè¯„ä¼° Qwen2.5-Coder æ¨¡åž‹\n\n### 1\\. Instruct ä¸Ž Base æ¨¡åž‹\n\nQwen2\\.5\\-Coder æä¾› **Base** å’Œ **Instruct** ç‰ˆæœ¬ï¼š\n\n* **Base æ¨¡åž‹** æ—¨åœ¨ä¸ºå¸Œæœ›å¯¹å…¶ç‰¹å®šåº”ç”¨è¿›è¡Œå¾®è°ƒçš„å¼€å‘è€…æä¾›åŽŸå§‹æ¨¡åž‹ã€‚\n* **Instruct æ¨¡åž‹** ç»è¿‡é¢„å…ˆè°ƒæ•´ï¼Œä¼˜åŒ–ç”¨äºŽäº¤äº’å¼å’Œå¯¹è¯å¼çš„ä½¿ç”¨åœºæ™¯ï¼Œéžå¸¸é€‚åˆåŸºäºŽèŠå¤©çš„ä»£ç åŠ©æ‰‹ã€‚\n\n### 2\\. åŸºå‡†æ¯”è¾ƒï¼šå¼•é¢†æ½®æµ\n\nåœ¨å„ç§æ ¸å¿ƒåŸºå‡†æµ‹è¯•ä¸­ï¼š\n\n* **MBPP\\-3shot** è¢«é€‰ä¸­ç”¨äºŽè¯„ä¼°åŸºç¡€æ¨¡åž‹ï¼Œæä¾›äº†ä¸€ä¸ªå¼ºæœ‰åŠ›çš„æŒ‡æ ‡æ¥è¡¡é‡å®ƒä»¬çš„ä»£ç ç†è§£å’Œåˆæˆèƒ½åŠ›ã€‚\n* **LiveCodeBench** é—®é¢˜é›†ç”¨äºŽè¯„ä¼°æŒ‡å¯¼æ¨¡åž‹ï¼Œé‡ç‚¹å…³æ³¨å®ƒä»¬å¯¹æ–°é¢–å’Œæœªè§è¿‡çš„ç¼–ç é—®é¢˜çš„é€‚åº”èƒ½åŠ›ã€‚\n\nç»“æžœå‘¢ï¼Ÿ**Qwen2\\.5\\-Coder ä¸€ç›´ä¼˜äºŽå…¶ä»–å¼€æºæ¨¡åž‹**ï¼Œè¯æ˜Žäº†è§„æ¨¡æ‰©å¤§ç¡®å®žä¸Žæ›´å¥½çš„æ€§èƒ½ç›¸å…³ã€‚\n\n## å®žç”¨æŒ‡å—ï¼šä½¿ç”¨ Qwen2.5-Coder-3B è¿›è¡Œä»£ç ç”Ÿæˆä¸Ž Transformers\n\nåœ¨æœ¬å®žç”¨æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ `transformers` åº“ä¸­çš„ **Qwen2.5-Coder-3B** æ¨¡åž‹æ¥ç”Ÿæˆä»£ç ã€‚è¯¥æ¨¡åž‹æ˜¯ **Qwen2.5-Coder ç³»åˆ—** çš„ä¸€éƒ¨åˆ†ï¼Œæ—¨åœ¨åœ¨ä»£ç ç”Ÿæˆã€ä¿®å¤å’ŒæŽ¨ç†æ–¹é¢è¡¨çŽ°å‡ºè‰²ã€‚åˆ°æœ¬æ•™ç¨‹ç»“æŸæ—¶ï¼Œæ‚¨å°†çœ‹åˆ°å¦‚ä½•å°†è¿™ä¸ªå¼ºå¤§çš„å¼€æºæ¨¡åž‹é›†æˆåˆ°æ‚¨è‡ªå·±çš„é¡¹ç›®ä¸­ï¼Œä»¥å¤„ç†å„ç§ä¸Žä»£ç ç›¸å…³çš„ä»»åŠ¡ã€‚\n\n### å‰ææ¡ä»¶\n\nåœ¨æ·±å…¥ä»£ç ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨å·²å®‰è£…ä»¥ä¸‹å†…å®¹ï¼š\n\n```python\npip install torch transformers\n```\n\næ­¤å¤–ï¼Œå¦‚æžœæ‚¨å¸Œæœ›å……åˆ†åˆ©ç”¨æ¨¡åž‹çš„æ€§èƒ½ï¼Œè¯·ç¡®ä¿æ‚¨å¯ä»¥è®¿é—®æ”¯æŒ GPU çš„çŽ¯å¢ƒã€‚\n\n### ç¬¬ä¸€æ­¥ï¼šå¯¼å…¥æ‰€éœ€åº“\n\næˆ‘ä»¬å°†é€šè¿‡ä»Ž `transformers` åº“ä¸­å¯¼å…¥å¿…è¦çš„ç»„ä»¶æ¥å¼€å§‹ï¼š\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n```\n\nç¬¬äºŒæ­¥ï¼šåŠ è½½æ¨¡åž‹å’Œåˆ†è¯å™¨\n\nåœ¨è¿™ä¸€æ­¥ä¸­ï¼Œæˆ‘ä»¬åŠ è½½ ***Qwen2\\.5\\-Coder\\-32B\\-Instruct*** æ¨¡åž‹åŠå…¶å¯¹åº”çš„åˆ†è¯å™¨ã€‚`device_map=\"auto\"` é€‰é¡¹å°†è‡ªåŠ¨å°†æ¨¡åž‹åˆ†é…åˆ°å¯ç”¨çš„ GPU æˆ– CPU ä¸Šã€‚\n\n> ***Qwen2\\.5\\-Coder å·²åœ¨ Hugging Face ä¸Šå‘å¸ƒäº†å¤šç§å°ºå¯¸çš„æ¨¡åž‹ â€” 0\\.5B\\-Instructã€1\\.5B\\-Instructã€3B\\-Instructã€7B\\-Instructã€14B\\-Instruct å’Œ 32B\\-Instructã€‚å¦‚æžœæ‚¨æƒ³åœ¨æœ¬åœ°è¿è¡Œå®ƒä»¬ï¼Œè¯·é€‰æ‹©æœ€é€‚åˆæ‚¨ GPU å®¹é‡çš„æ¨¡åž‹ã€‚è¿™äº›æ¨¡åž‹ä¹Ÿå¯ä»¥åœ¨ Ollama ä¸ŠèŽ·å¾—ï¼Œå› æ­¤æ‚¨å¯ä»¥åœ¨ Ollama çŽ¯å¢ƒä¸­ä½¿ç”¨å®ƒä»¬ã€‚å¦‚æžœæ‚¨å¯¹ Ollama æ•™ç¨‹æ„Ÿå…´è¶£ï¼Œè¯·éšæ—¶åœ¨è¯„è®ºä¸­å‘Šè¯‰æˆ‘ï¼***\n\n```python\nmodel_name = \"Qwen/Qwen2.5-Coder-32B-Instruct\"\n\n## Load the model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n## Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n```\n\n### ç¬¬3æ­¥ï¼šç¼–å†™èŠå¤©æ¨¡æ¿å‡½æ•°\n\nQwen2.5-Coderæ¨¡åž‹æ—¨åœ¨å¤„ç†ç±»å¯¹è¯çš„æç¤ºï¼Œä½¿ç”¨èŠå¤©æ¨¡æ¿ã€‚ä»¥ä¸‹è¾…åŠ©å‡½æ•°ä»¥ç¬¦åˆæ¨¡åž‹é¢„æœŸçš„æ–¹å¼è®¾ç½®è¾“å…¥æç¤ºï¼š\n\n```python\ndef generate_response(model, tokenizer, prompt):\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": prompt}\n    ]\n    # Prepare the chat input\n    text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n    # Tokenize and prepare inputs\n    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n    # Generate response\n    generated_ids = model.generate(\n        **model_inputs,\n        max_new_tokens=512\n    )\n    # Remove prompt tokens from output\n    generated_ids = [\n        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n    ]\n  \n    # Decode and return the generated text\n    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n    return response\n```\n\n### ç¬¬4æ­¥ï¼šä½¿ç”¨ä»£ç ç”Ÿæˆæµ‹è¯•æ¨¡åž‹\n\nè®©æˆ‘ä»¬è¿è¡Œä¸€äº›ç¤ºä¾‹ï¼Œçœ‹çœ‹Qwen2.5-Coder-32Bæ¨¡åž‹åœ¨ç”ŸæˆPythonå’ŒJavaä»£ç æ–¹é¢çš„è¡¨çŽ°å¦‚ä½•ã€‚æˆ‘ä»¬å°†ç”¨ä¸‰ä¸ªä¸åŒçš„ç¼–ç¨‹æç¤ºè¿›è¡Œæµ‹è¯•ã€‚\n\n### æµ‹è¯• 1ï¼šå¿«é€ŸæŽ’åºç®—æ³•\n\n**æç¤º**ï¼šâ€œç¼–å†™ä¸€ä¸ªå¿«é€ŸæŽ’åºç®—æ³•ã€‚â€\n\n```python\nprompt_1 = \"write a quick sort algorithm.\"\nresponse_1 = generate_response(model, tokenizer, prompt_1)\nprint(response_1)\n```\n\nè¾“å‡ºï¼š\n\n\n```python\ndef quick_sort(arr):\n    if len(arr) <= 1:\n        return arr\n    pivot = arr[len(arr) // 2]\n    left = [x for x in arr if x < pivot]\n    middle = [x for x in arr if x == pivot]\n    right = [x for x in arr if x > pivot]\n    return quick_sort(left) + middle + quick_sort(right)\n```\n\n## ç¤ºä¾‹ç”¨æ³•ï¼š\n\narr = [3, 6, 8, 10, 1, 2, 1]\nprint(quick_sort(arr))\n\n```\nè¿™æ®µä»£ç å®šä¹‰äº†ä¸€ä¸ª `quick_sort` å‡½æ•°ï¼Œè¯¥å‡½æ•°æŽ¥å—ä¸€ä¸ªæ•°ç»„ä½œä¸ºè¾“å…¥ï¼Œå¹¶ä½¿ç”¨å¿«é€ŸæŽ’åºç®—æ³•è¿”å›žè¯¥æ•°ç»„çš„æŽ’åºç‰ˆæœ¬ã€‚è¯¥å‡½æ•°é€šè¿‡ä»Žæ•°ç»„ä¸­é€‰æ‹©ä¸€ä¸ªåŸºå‡†å…ƒç´ ï¼Œå¹¶æ ¹æ®å…¶ä»–å…ƒç´ æ˜¯å°äºŽè¿˜æ˜¯å¤§äºŽåŸºå‡†æ¥å°†å®ƒä»¬åˆ†æˆä¸¤ä¸ªå­æ•°ç»„ã€‚ç„¶åŽå¯¹å­æ•°ç»„è¿›è¡Œé€’å½’æŽ’åºã€‚é€’å½’çš„åŸºæœ¬æƒ…å†µæ˜¯å½“æ•°ç»„åªæœ‰ä¸€ä¸ªæˆ–æ²¡æœ‰å…ƒç´ æ—¶ï¼Œæ­¤æ—¶æ•°ç»„å·²ç»æ˜¯æŽ’åºå¥½çš„ã€‚\n\næ‚¨å¯ä»¥å°†è¿™æ®µä»£ç ä½œä¸ºèµ·ç‚¹ï¼Œå¹¶æ ¹æ®æ‚¨çš„å…·ä½“éœ€æ±‚è¿›è¡Œä¿®æ”¹ã€‚å¦‚æœ‰ä»»ä½•è¿›ä¸€æ­¥çš„é—®é¢˜æˆ–éœ€è¦å…¶ä»–å¸®åŠ©ï¼Œè¯·å‘Šè¯‰æˆ‘ï¼<|fim_middle|>\n```\n\n### æµ‹è¯• 2ï¼šè®¡ç®—ä¸€ä¸ªæ•°å­—çš„é˜¶ä¹˜\n\n**æç¤º**ï¼šâ€œç¼–å†™ä¸€ä¸ª Python ç¨‹åºæ¥è®¡ç®—ä¸€ä¸ªæ•°å­—çš„é˜¶ä¹˜ã€‚â€\n\n```python\nprompt_2 = \"Write a python program to find a factorial of a number.\"\nresponse_2 = generate_response(model, tokenizer, prompt_2)\nprint(response_2)\n```\n\nè¾“å‡ºï¼š\n\n```python\ndef factorial(n):\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n - 1)\n\n## ç¤ºä¾‹ç”¨æ³•\nnumber = int(input(\"è¾“å…¥ä¸€ä¸ªæ•°å­—: \"))\nresult = factorial(number)\nprint(f\"{number} çš„é˜¶ä¹˜æ˜¯ {result}\")\n```\n\nè¯¥ç¨‹åºå®šä¹‰äº†ä¸€ä¸ªé€’å½’å‡½æ•° `factorial`ï¼Œç”¨äºŽè®¡ç®—ç»™å®šæ•°å­— `n` çš„é˜¶ä¹˜ã€‚å®ƒä½¿ç”¨åŸºå‡†æƒ…å†µ `n == 0` è¿”å›ž 1ï¼Œå¯¹äºŽå…¶ä»–å€¼çš„ `n`ï¼Œå®ƒé€’å½’è°ƒç”¨è‡ªèº«ï¼Œç›´åˆ°è¾¾åˆ°åŸºå‡†æƒ…å†µã€‚æœ€åŽï¼Œç¨‹åºæç¤ºç”¨æˆ·è¾“å…¥ä¸€ä¸ªæ•°å­—ï¼Œå¹¶æ˜¾ç¤ºè¯¥æ•°å­—çš„é˜¶ä¹˜ã€‚\n\n\n### æµ‹è¯• 3ï¼šä½¿ç”¨é€—å·æ ¼å¼åŒ–æ•°å­—\n\n**æç¤º**ï¼šâ€œç¼–å†™ä¸€ä¸ªåä¸ºâ€˜format\\_numberâ€™çš„ Python å‡½æ•°ï¼Œè¯¥å‡½æ•°ä»¥ä¸€ä¸ªéžè´Ÿæ•°ä½œä¸ºå”¯ä¸€å‚æ•°ã€‚æ‚¨çš„å‡½æ•°åº”å°†æ•°å­—è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œå¹¶æ·»åŠ é€—å·ä½œä¸ºåƒä½åˆ†éš”ç¬¦ã€‚â€\n\n```python\nprompt_3 = \"\"\"Write a python function named 'format_number' that takes a non-negative number as its only parameter.\nYour function should convert the number to a string and add commas as a thousand separators. \nFor example, calling format_number(1000000) should return '1,000,000'.\"\"\"\nresponse_3 = generate_response(model, tokenizer, prompt_3)\nprint(response_3)\n```\n\nè¾“å‡ºï¼š\n\n```python\ndef format_number(num):\n    # Convert the number to a string\n    num_str = str(num)\n  \n    # Initialize an empty string to store the formatted number\n    formatted_num = ''\n  \n    # Initialize a counter for the number of digits\n    digit_count = 0\n  \n    # Iterate over the digits in reverse order\n    for i in range(len(num_str) - 1, -1, -1):\n        # Add the current digit to the formatted number\n        formatted_num = num_str[i] + formatted_num\n      \n        # Increment the digit count\n        digit_count += 1\n      \n        # If the digit count is a multiple of 3, add a comma\n        if digit_count % 3 == 0 and i != 0:\n            formatted_num = ',' + formatted_num\n  \n    return formatted_num\n```\n\nYou can test the function with the following code:\n\n```python\nprint(format_number(1000000))  # Output: '1,000,000'\nprint(format_number(123456789))  # Output: '123,456,789'\nprint(format_number(1234567890))  # Output: '1,234,567,890'\n```\n\nThis function works by iterating over the digits of the number in reverse order, adding each digit to the formatted number and adding a comma every third digit. The function then returns the formatted number as a string.\n\n### æµ‹è¯• 4ï¼šä½¿ç”¨æ•°ç»„å®žçŽ°æ ˆçš„ Java ç¨‹åº\n\n**æç¤º**ï¼šâ€œç¼–å†™ä¸€ä¸ª Java ç¨‹åºï¼Œä½¿ç”¨æ•°ç»„å®žçŽ°æ ˆâ€\n\n```python\nprompt_4 = \"\"\"Write a Java program, Implement a Stack Using Arrays.\"\"\"\nresponse_4 = generate_response(model, tokenizer, prompt_3)\nprint(response_4)\n```\n\nè¾“å‡ºï¼š\n\n```java\npublic class StackUsingArrays {\n    private int[] stackArray;\n    private int top;\n    private int capacity;\n\n    public StackUsingArrays(int size) {\n        stackArray = new int[size];\n        top = -1;\n        capacity = size;\n    }\n\n    public void push(int item) {\n        if (isFull()) {\n            System.out.println(\"Stack is full. Cannot push item: \" + item);\n        } else {\n            top++;\n            stackArray[top] = item;\n            System.out.println(\"Pushed item: \" + item);\n        }\n    }\n\n    public int pop() {\n        if (isEmpty()) {\n            System.out.println(\"Stack is empty. Cannot pop item.\");\n            return -1;\n        } else {\n            int item = stackArray[top];\n            top--;\n            return item;\n        }\n    }\n\n    public boolean isEmpty() {\n        return top == -1;\n    }\n\n    public boolean isFull() {\n        return top == capacity - 1;\n    }\n\n    public static void main(String[] args) {\n        StackUsingArrays stack = new StackUsingArrays(5);\n\n        stack.push(10);\n        stack.push(20);\n        stack.push(30);\n\n        System.out.println(\"Popped item: \" + stack.pop());\n        System.out.println(\"Popped item: \" + stack.pop());\n\n        stack.push(40);\n        stack.push(50);\n\n        System.out.println(\"Popped item: \" + stack.pop());\n        System.out.println(\"Popped item: \" + stack.pop());\n        System.out.println(\"Popped item: \" + stack.pop());\n    }\n}\n```\n\nè¿™ä¸ªç¨‹åºå®šä¹‰äº†ä¸€ä¸ª`StackUsingArrays`ç±»ï¼Œå®ƒä½¿ç”¨ä¸€ä¸ªæ•°ç»„æ¥å®žçŽ°ä¸€ä¸ªå †æ ˆã€‚`push`æ–¹æ³•å°†ä¸€ä¸ªé¡¹ç›®æ·»åŠ åˆ°å †æ ˆçš„é¡¶éƒ¨ï¼Œ`pop `æ–¹æ³•ç§»é™¤å¹¶è¿”å›žå †æ ˆé¡¶éƒ¨çš„é¡¹ç›®ã€‚`isEmpty`å’Œ`isFull`æ–¹æ³•åˆ†åˆ«æ£€æŸ¥å †æ ˆæ˜¯ç©ºçš„è¿˜æ˜¯æ»¡çš„ã€‚`main`æ–¹æ³•æ¼”ç¤ºäº†å¦‚ä½•é€šè¿‡æŽ¨å…¥å’Œå¼¹å‡ºé¡¹æ¥ä½¿ç”¨å †æ ˆã€‚\n\n### æµ‹è¯• 5ï¼šç®€å•åˆ†ç±»ç®—æ³•\n\n**æç¤º**ï¼šâ€œç¼–å†™ä¸€ä¸ª Python ä»£ç ä»¥ä½¿ç”¨ Sklearn è¿è¡Œç®€å•åˆ†ç±»ç®—æ³•â€\n\n```python\nprompt_5 = \"\"\"Write a Python code to run a simple classification algorithm using Sklearn\"\"\"\nresponse_5 = generate_response(model, tokenizer, prompt_3)\nprint(response_5)\n```\n\nè¾“å‡ºï¼š\n\n```python\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n## ç”Ÿæˆä¸€äº›ç¤ºä¾‹æ•°æ®\nnp.random.seed(42)\nX = np.random.rand(100, 2)  # 100 ä¸ªæ ·æœ¬ï¼Œ2 ä¸ªç‰¹å¾\ny = np.random.choice([0, 1], size=100)  # äºŒåˆ†ç±»\n\n## å°†æ•°æ®åˆ†å‰²ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n## åˆ›å»ºé€»è¾‘å›žå½’æ¨¡åž‹\nmodel = LogisticRegression()\n\n## è®­ç»ƒæ¨¡åž‹\nmodel.fit(X_train, y_train)\n\n## å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹\ny_pred = model.predict(X_test)\n\n## è®¡ç®—æ¨¡åž‹çš„å‡†ç¡®æ€§\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"å‡†ç¡®æ€§: {accuracy}\")\n```\n\næ­¤ä»£ç ç”Ÿæˆä¸€äº›ç¤ºä¾‹æ•°æ®ï¼Œå°†å…¶æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œè®­ç»ƒä¸€ä¸ªé€»è¾‘å›žå½’æ¨¡åž‹ï¼Œå¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹ï¼Œå¹¶è®¡ç®—æ¨¡åž‹çš„å‡†ç¡®æ€§ã€‚è¯·éšæ„ä¿®æ”¹ä»£ç ä»¥æ»¡è¶³æ‚¨çš„å…·ä½“éœ€æ±‚ã€‚\n\n\n## ç¬¬5æ­¥ï¼šåˆ†æžæ¨¡åž‹çš„æ€§èƒ½\n\næ ¹æ®æˆ‘ä»¬çš„æµ‹è¯•ï¼Œ***Qwen2\\.5\\-Coder\\-32B\\-Instruct*** æ¨¡åž‹å±•ç¤ºäº†ï¼š\n\n* **å¼ºå¤§çš„ä»£ç ç”Ÿæˆèƒ½åŠ›**ï¼Œèƒ½å¤Ÿä¸ºç»å…¸ç¼–ç é—®é¢˜ç”Ÿæˆé«˜æ•ˆã€æ˜“äºŽç†è§£çš„è§£å†³æ–¹æ¡ˆã€‚\n* **å¯¹Pythonè¯­æ³•å’Œæœ€ä½³å®žè·µçš„ç†è§£**ï¼Œå°¤å…¶æ˜¯åœ¨ä½¿ç”¨Pythonicè§£å†³æ–¹æ¡ˆå¦‚åˆ—è¡¨æŽ¨å¯¼å¼å’Œæ ¼å¼åŒ–å­—ç¬¦ä¸²æ—¶ã€‚\n* **çµæ´»æ€§**ï¼Œèƒ½å¤Ÿé€‚åº”å„ç§æç¤ºï¼Œè¿™å¯¹å®žé™…ç¼–ç¨‹åŠ©æ‰‹çš„ä½¿ç”¨æ¡ˆä¾‹è‡³å…³é‡è¦ã€‚\n\n## æ½œåœ¨çš„ä½¿ç”¨æ¡ˆä¾‹\n\né‰´äºŽå…¶æ€§èƒ½ï¼ŒQwen2.5-Coderæ¨¡åž‹å¯ä»¥åœ¨å„ç§åœºæ™¯ä¸­æœ‰æ•ˆä½¿ç”¨ï¼Œä¾‹å¦‚ï¼š\n\n* **ç¼–ç åŠ©æ‰‹**ï¼šé›†æˆåˆ°IDEæˆ–æ–‡æœ¬ç¼–è¾‘å™¨ä¸­ï¼Œå¸®åŠ©å¼€å‘äººå‘˜æ›´å¿«åœ°ç¼–å†™ä»£ç ã€‚\n* **è‡ªåŠ¨åŒ–ä»£ç å®¡æŸ¥**ï¼šååŠ©è¯†åˆ«é”™è¯¯ã€ä¼˜åŒ–ä»£ç å¹¶æå‡ºæ”¹è¿›å»ºè®®ã€‚\n* **æ•™è‚²å·¥å…·**ï¼šé€šè¿‡ç”Ÿæˆç¤ºä¾‹è§£å†³æ–¹æ¡ˆå’Œè§£é‡Šï¼Œå¸®åŠ©å­¦ç”Ÿå­¦ä¹ ç¼–ç ã€‚\n\n## ç»“è®º\n\n**Qwen2.5-Coder**ç³»åˆ—ï¼Œç‰¹åˆ«æ˜¯**32Bæ¨¡åž‹**ï¼Œä¸ºå¼€å‘è€…ã€ç ”ç©¶äººå‘˜å’Œå¸Œæœ›åˆ©ç”¨AIè¿›è¡Œä»£ç ç›¸å…³ä»»åŠ¡çš„ç»„ç»‡æä¾›äº†å¼ºå¤§è€Œå¤šåŠŸèƒ½çš„å·¥å…·ã€‚å®ƒåœ¨EvalPlusã€Aiderå’ŒMcEvalç­‰åŸºå‡†æµ‹è¯•ä¸­çš„å‡ºè‰²è¡¨çŽ°è¯æ˜Žäº†å…¶åœ¨ä»£ç ç”Ÿæˆã€ä¿®å¤å’ŒæŽ¨ç†æ–¹é¢çš„ç«žäº‰ä¼˜åŠ¿ã€‚\n\né€šè¿‡å¼€æºè¿™äº›æ¨¡åž‹ï¼Œé˜¿é‡Œäº‘ä¸ºä¸€ä¸ªAIé©±åŠ¨çš„ç¼–ç åŠ©æ‰‹æ™®åŠçš„æœªæ¥é“ºå¹³äº†é“è·¯ã€‚æ— è®ºæ‚¨æ˜¯å¸Œæœ›è‡ªåŠ¨åŒ–é‡å¤ä»»åŠ¡çš„å¼€å‘è€…ï¼Œè¿˜æ˜¯å¸Œæœ›å­¦ä¹ æ–°ç¼–ç¨‹æ¦‚å¿µçš„å­¦ç”Ÿï¼ŒQwen2.5-Coderéƒ½æ˜¯æ‚¨å·¥å…·ç®±ä¸­å€¼å¾—æ·»åŠ çš„å¯é å·¥å…·ã€‚\n\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/qwen2-5-coder-cosmos-tokenizer-opencoder-and-new-sentencetransformers-great-times-for-open-ffcacf2b29cd","frontmatter":{"title":"Qwen2.5-Coderã€Cosmos Tokenizerã€OpenCoder å’Œæ–°çš„ SentenceTransformersï¼šå¼€æ”¾æºä»£ç çš„ä¼Ÿå¤§æ—¶ä»£","meta_title":"Qwen2.5-Coderã€Cosmos Tokenizerã€OpenCoder å’Œæ–°çš„ SentenceTransformersï¼šå¼€æ”¾æºä»£ç çš„ä¼Ÿå¤§æ—¶ä»£","description":"æ–‡ç« ä»‹ç»äº†å¤šä¸ªå¼€æºé¡¹ç›®çš„è¿›å±•ï¼ŒåŒ…æ‹¬Qwen2.5-Coderç³»åˆ—ã€Cosmos Tokenizerã€OpenCoderå’ŒSentenceTransformersã€‚Qwen2.5-Coderæ˜¯ä¸€ä¸ªä¸ŽGPT-4ç«žäº‰çš„å¼€æºä»£ç LLMï¼Œå…·æœ‰å¤šç§æ¨¡åž‹å°ºå¯¸å’Œå“è¶Šçš„ä»£ç ç”Ÿæˆã€ä¿®å¤å’ŒæŽ¨ç†èƒ½åŠ›ã€‚Cosmos Tokenizeråˆ™æ˜¯ä¸€ç§é«˜æ•ˆçš„ç¥žç»åˆ†è¯å™¨ï¼Œä¸“æ³¨äºŽå›¾åƒå’Œè§†é¢‘åŽ‹ç¼©ï¼Œæä¾›æ˜¾è‘—çš„åŽ‹ç¼©çŽ‡å’Œé«˜è´¨é‡é‡å»ºã€‚OpenCoderæ˜¯å®Œå…¨å¼€æºçš„ä»£ç LLMï¼Œè®­ç»ƒäºŽ2.5ä¸‡äº¿ä»¤ç‰Œï¼Œæ”¯æŒå¤šç§ç¼–ç¨‹è¯­è¨€ã€‚SentenceTransformersé€šè¿‡OpenVINOçš„é‡åŒ–æŠ€æœ¯å®žçŽ°äº†CPUæŽ¨ç†é€Ÿåº¦çš„æ˜¾è‘—æå‡ã€‚","date":"2024-11-14T03:29:09.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*IZdOavxT_8SRCxrg","categories":["Programming","Technology","Natural Language Processing"],"author":"Rifx.Online","tags":["Qwen2.5-Coder","Cosmos","OpenCoder","SentenceTransformers","OpenVINO"],"draft":false,"slug":"blog/qwen2-5-coder-cosmos-tokenizer-opencoder-and-new-sentencetransformers-great-times-for-open-ffcacf2b29cd"},"content":"\næˆ‘æƒ³å¼ºè°ƒä¸€äº›å¼•äººæ³¨ç›®çš„å¼€æºè¿›å±•ï¼š\n\n* **Qwen2\\.5\\-Coder ç³»åˆ—**ï¼šä¸€ä¸ªå¼€æ”¾æºä»£ç çš„ä»£ç  LLMï¼Œæ­£åœ¨ä¸Ž GPT\\-4 ç«žäº‰ã€‚\n* **Cosmos Tokenizer**ï¼šä¸€å¥—å…ˆè¿›çš„ç¥žç»åˆ†è¯å™¨ï¼Œç”¨äºŽé«˜æ•ˆçš„å›¾åƒå’Œè§†é¢‘åŽ‹ç¼©ã€‚\n* **OpenCoder**ï¼šä¸€ä¸ªå®Œå…¨å¼€æºçš„ä»£ç  LLMï¼Œè®­ç»ƒäºŽæƒŠäººçš„ 2\\.5 ä¸‡äº¿ä¸ªæ ‡è®°ã€‚\n* **SentenceTransformers çš„å¤§å¹… CPU åŠ é€Ÿ**ï¼šä½¿ç”¨ OpenVINO çš„ int8 é™æ€é‡åŒ–ï¼ŒCPU æŽ¨ç†é€Ÿåº¦æå‡ 4 å€ã€‚\n\nè®©æˆ‘ä»¬æ·±å…¥äº†è§£ä¸€ä¸‹ï¼\n\n## Qwen2\\.5\\-Coder ç³»åˆ—ï¼šå¼€æºä¸€æ¬¾ä¸Ž GPT\\-4 ç«žäº‰çš„ SOTA ä»£ç  LLM\n\né˜¿é‡Œäº‘å®£å¸ƒå¼€æºå‘å¸ƒ Qwen2\\.5\\-Coder ç³»åˆ—â€”â€”è¿™äº›æ¨¡åž‹å…·æœ‰ **å¼ºå¤§**ã€**å¤šæ ·** å’Œ **å®žç”¨** çš„ç‰¹ç‚¹ï¼Œè‡´åŠ›äºŽæŽ¨åŠ¨å¼€æ”¾ä»£ç å¤§è¯­è¨€æ¨¡åž‹ (LLMs) çš„å‘å±•ã€‚\n\næ——èˆ°æ¨¡åž‹ **Qwen2\\.5\\-Coder\\-32B\\-Instruct** ä½œä¸ºæœ€æ–°çš„å¼€æºä»£ç æ¨¡åž‹ï¼Œè®¾å®šäº†æ–°çš„åŸºå‡†ï¼ŒåŒ¹é…äº† GPT\\-4 çš„ç¼–ç èƒ½åŠ›ã€‚å®ƒåœ¨é€šç”¨å’Œæ•°å­¦æŽ¨ç†æ–¹é¢è¡¨çŽ°å‡ºè‰²ã€‚\n\n\n\nåœ¨ä¹‹å‰å‘å¸ƒçš„ 1\\.5B å’Œ 7B æ¨¡åž‹åŸºç¡€ä¸Šï¼Œä»–ä»¬åˆæŽ¨å‡ºäº†å››ç§é¢å¤–çš„æ¨¡åž‹å°ºå¯¸ï¼š0\\.5Bã€3Bã€14B å’Œ 32Bã€‚Qwen2\\.5\\-Coder çŽ°åœ¨èƒ½å¤Ÿæ»¡è¶³å¹¿æ³›çš„å¼€å‘è€…éœ€æ±‚ï¼Œæ¶µç›–å…­ç§ä¸»æµæ¨¡åž‹å°ºå¯¸ã€‚\n\nä»–ä»¬è¿˜æŽ¢è®¨äº† Qwen2\\.5\\-Coder åœ¨å®žé™…åœºæ™¯ä¸­çš„é€‚ç”¨æ€§ï¼ŒåŒ…æ‹¬ä»£ç åŠ©æ‰‹å’Œå·¥ä»¶ç”Ÿæˆã€‚\n\nå®žé™…ä¾‹å­çªæ˜¾äº†è¯¥æ¨¡åž‹åœ¨æå‡å¼€å‘è€…ç”Ÿäº§åŠ›å’Œä»£ç è´¨é‡æ–¹é¢çš„æ½œåŠ›ã€‚\n\n**åŸºå‡†æˆå°±**\n\n* **ä»£ç ç”Ÿæˆ**ï¼šQwen2\\.5\\-Coder\\-32B\\-Instruct æ¨¡åž‹åœ¨æµè¡Œçš„ä»£ç ç”ŸæˆåŸºå‡† EvalPlusã€LiveCodeBench å’Œ BigCodeBench ä¸Šå–å¾—äº†é¡¶å°–æ€§èƒ½ã€‚\n* **ä»£ç ä¿®å¤**ï¼šè®¤è¯†åˆ°è°ƒè¯•åœ¨è½¯ä»¶å¼€å‘ä¸­çš„é‡è¦æ€§ï¼ŒQwen2\\.5\\-Coder\\-32B\\-Instruct åœ¨ä»£ç ä¿®å¤ä»»åŠ¡ä¸­è¡¨çŽ°å‡ºè‰²ã€‚åœ¨ Aider åŸºå‡†ä¸Šå¾—åˆ† 73\\.7ï¼Œè¡¨çŽ°ä¸Ž GPT\\-4 ç›¸å½“ï¼Œå¸®åŠ©å¼€å‘è€…é«˜æ•ˆä¿®å¤ä»£ç é”™è¯¯ã€‚\n* **ä»£ç æŽ¨ç†**ï¼šè¯¥æ¨¡åž‹å±•çŽ°äº†å…ˆè¿›çš„ä»£ç æŽ¨ç†èƒ½åŠ›ï¼Œå­¦ä¹ ä»£ç æ‰§è¡Œè¿‡ç¨‹å¹¶å‡†ç¡®é¢„æµ‹è¾“å…¥å’Œè¾“å‡ºã€‚åœ¨ Qwen2\\.5\\-Coder\\-7B\\-Instruct çš„å‡ºè‰²è¡¨çŽ°åŸºç¡€ä¸Šï¼Œ32B æ¨¡åž‹è¿›ä¸€æ­¥æå‡äº†æŽ¨ç†èƒ½åŠ›ã€‚\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*fzH6YE-yl_GrEXwz)\n\n* **å¤šè¯­è¨€æ”¯æŒ**ï¼šQwen2\\.5\\-Coder\\-32B\\-Instruct ç²¾é€š 40 å¤šç§ç¼–ç¨‹è¯­è¨€ã€‚åœ¨ McEval ä¸Šå¾—åˆ† 65\\.9ï¼Œåœ¨ Haskell å’Œ Racket ç­‰è¯­è¨€ä¸­è¡¨çŽ°å‡ºè‰²ï¼Œè¿™å¾—ç›ŠäºŽåœ¨é¢„è®­ç»ƒæœŸé—´ç‹¬ç‰¹çš„æ•°æ®æ¸…æ´—å’Œå‡è¡¡ç­–ç•¥ã€‚\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*rhyc0T3UZp_2x0r2)\n\næ‚¨å¯ä»¥åœ¨ [github](https://proxy.rifx.online/https://github.com/QwenLM/Qwen2.5-Coder) ä¸Šæ‰¾åˆ°æ›´å¤šä¿¡æ¯ã€‚\n\n## Cosmos Tokenizer: é«˜çº§ç¥žç»åˆ†è¯å™¨ç”¨äºŽé«˜æ•ˆçš„å›¾åƒå’Œè§†é¢‘åŽ‹ç¼©\n\n**Cosmos Tokenizer** æ˜¯ä¸€å¥—å…¨é¢çš„ç¥žç»åˆ†è¯å™¨ï¼Œä¸“ä¸ºå›¾åƒå’Œè§†é¢‘è®¾è®¡ã€‚\n\næ‚¨çŽ°åœ¨å¯ä»¥å°†åŽŸå§‹è§†è§‰æ•°æ®è½¬æ¢ä¸ºé«˜æ•ˆçš„åŽ‹ç¼©è¡¨ç¤ºã€‚\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*v8k8jLbZ4LYFRUBc.jpg)\n\né€šè¿‡æ— ç›‘ç£å­¦ä¹ å‘çŽ°æ½œåœ¨ç©ºé—´ï¼Œè¿™äº›åˆ†è¯å™¨ä¿ƒè¿›äº†å¤§è§„æ¨¡æ¨¡åž‹è®­ç»ƒï¼Œå¹¶å‡å°‘äº†æŽ¨ç†è¿‡ç¨‹ä¸­çš„è®¡ç®—éœ€æ±‚ã€‚\n\n**åˆ†è¯å™¨ç±»åž‹**ï¼š\n\n* **è¿žç»­åˆ†è¯å™¨**ï¼šå°†è§†è§‰æ•°æ®æ˜ å°„åˆ°è¿žç»­åµŒå…¥ï¼Œé€‚ç”¨äºŽä»Žè¿žç»­åˆ†å¸ƒï¼ˆå¦‚ç¨³å®šæ‰©æ•£ï¼‰ä¸­é‡‡æ ·çš„æ¨¡åž‹ã€‚\n* **ç¦»æ•£åˆ†è¯å™¨**ï¼šå°†è§†è§‰æ•°æ®æ˜ å°„åˆ°é‡åŒ–ç´¢å¼•ï¼Œåº”ç”¨äºŽä¾èµ–äº¤å‰ç†µæŸå¤±è¿›è¡Œè®­ç»ƒçš„æ¨¡åž‹ï¼Œå¦‚ VideoPoetã€‚\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*a6Hvj8hXJUpOAp9Ber781g.png)\n\n**å…³é”®ç‰¹æ€§**ï¼š\n\n* **é«˜åŽ‹ç¼©ä¸Žè´¨é‡ä¿ç•™**ï¼šåœ¨æ˜¾è‘—çš„åŽ‹ç¼©çŽ‡ä¸Žé«˜è´¨é‡é‡å»ºä¹‹é—´å–å¾—å¹³è¡¡ï¼Œä¿ç•™æ½œåœ¨ç©ºé—´ä¸­çš„é‡è¦è§†è§‰ç»†èŠ‚ã€‚\n* **è½»é‡çº§æ—¶é—´å› æžœæž¶æž„**ï¼šåˆ©ç”¨å› æžœæ—¶é—´å·ç§¯å’Œæ³¨æ„åŠ›å±‚ä¿æŒè§†é¢‘å¸§çš„æ—¶é—´é¡ºåºï¼Œå®žçŽ°å›¾åƒå’Œè§†é¢‘çš„æ— ç¼åˆ†è¯ã€‚\n* **åœ¨å¤šæ ·åŒ–æ•°æ®ä¸Šè®­ç»ƒ**ï¼šåœ¨å„ç§çºµæ¨ªæ¯”å’Œç±»åˆ«çš„é«˜åˆ†è¾¨çŽ‡å›¾åƒå’Œé•¿è§†é¢‘ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½¿å…¶åœ¨æŽ¨ç†æ—¶å¯¹æ—¶é—´é•¿åº¦ä¸æ•æ„Ÿã€‚\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*lBO1omEzlr18SPB1zF-vMw.png)\n\n**æ€§èƒ½äº®ç‚¹**ï¼š\n\n* **å“è¶Šçš„åŽ‹ç¼©çŽ‡**ï¼šæä¾›æ˜¾è‘—çš„åŽ‹ç¼©èƒ½åŠ›ï¼Œé€Ÿåº¦æ¯”ä»¥å‰çš„æ–¹æ³•å¿«**12å€**ã€‚\n* **é«˜è´¨é‡é‡å»º**ï¼šåœ¨å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰æ–¹é¢æ˜¾è‘—æå‡ï¼Œåœ¨ DAVIS è§†é¢‘æ•°æ®é›†ä¸Šè¶…è¶ŠçŽ°æœ‰æ–¹æ³•è¶…è¿‡ **+4 dB**ã€‚\n* **é«˜æ•ˆçš„åˆ†è¯**ï¼šèƒ½å¤Ÿåœ¨ NVIDIA A100 GPUï¼ˆ80GB å†…å­˜ï¼‰ä¸Šç¼–ç é«˜è¾¾ **8 ç§’ 1080p** å’Œ **10 ç§’ 720p** è§†é¢‘ã€‚\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*uYQttZw-MDOCK3oxxLcHbw.png)\n\n**è¯„ä¼°ä¸Žèµ„æº**ï¼š\n\n* **TokenBench æ•°æ®é›†** æ˜¯ä¸€ä¸ªæ–°æ•°æ®é›†ï¼Œæ—¨åœ¨æ ‡å‡†åŒ–è§†é¢‘åˆ†è¯å™¨è¯„ä¼°ï¼Œæ¶µç›–æœºå™¨äººã€é©¾é©¶å’Œä½“è‚²ç­‰ç±»åˆ«ã€‚\n* **å…¬å¼€å¯ç”¨æ€§**ï¼šå…·æœ‰ 8x å’Œ 16x ç©ºé—´åŽ‹ç¼©ï¼Œä»¥åŠ 4x å’Œ 8x æ—¶é—´åŽ‹ç¼©çš„é¢„è®­ç»ƒæ¨¡åž‹å¯åœ¨ [GitHub â€” NVIDIA/Cosmos-Tokenizer](https://proxy.rifx.online/https://github.com/NVIDIA/Cosmos-Tokenizer) èŽ·å–ã€‚\n\næœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜… [NVIDIA çš„å®˜æ–¹åšå®¢æ–‡ç« ](https://proxy.rifx.online/https://research.nvidia.com/labs/dir/cosmos-tokenizer/)ã€‚\n\n> *æ„Ÿè°¢æ‚¨æŠ½å‡ºæ—¶é—´æ¥åˆ°è¿™é‡Œï¼*\n\n> *å¦‚æžœæ‚¨å–œæ¬¢è¿™ç¯‡æ–‡ç« ï¼Œè¯·èŠ±ä¸€ç‚¹æ—¶é—´ [**åœ¨ Medium ä¸Šå…³æ³¨æˆ‘ä»¬**](https://proxy.rifx.online/https://medium.com/@datadrifters/subscribe)ï¼Œä¸ºè¿™ç¯‡æ–‡ç« ç‚¹èµž 50 æ¬¡å¹¶ç•™ä¸‹è¯„è®ºã€‚*\n\n> *æˆ‘ä»¬è¿˜åœ¨è¿›è¡Œä¸€ä¸ªåŸºäºŽå°ç»„çš„åŸ¹è®­ **[ç”¨äºŽæž„å»ºå…¨æ ˆ GenAI SaaS åº”ç”¨ç¨‹åº](https://proxy.rifx.online/https://forms.gle/8mfFH4wjhF7BbtRY9)**ï¼Œä¹ŸæœŸå¾…åœ¨é‡Œé¢è§åˆ°æ‚¨ï¼*\n\n## OpenCoder: å®Œå…¨å¼€æºçš„ä»£ç  LLMï¼Œè®­ç»ƒäºŽ 2.5T ä»¤ç‰Œ\n\n**OpenCoder** ä»‹ç»äº†ä¸€ç³»åˆ—æ–°çš„å¼€æºä»£ç è¯­è¨€æ¨¡åž‹ï¼ŒåŒ…æ‹¬ **1.5B** å’Œ **8B** å‚æ•°è§„æ¨¡çš„åŸºç¡€æ¨¡åž‹å’ŒèŠå¤©æ¨¡åž‹ã€‚\n\nOpenCoder æ”¯æŒè‹±è¯­å’Œä¸­æ–‡ï¼Œå®Œå…¨ä»Žä¸€ä¸ªåºžå¤§çš„æ•°æ®é›† **2.5 ä¸‡äº¿ä»¤ç‰Œ** ä¸­è®­ç»ƒè€Œæˆï¼ŒåŒ…å« 90% çš„åŽŸå§‹ä»£ç å’Œ 10% çš„ä»£ç ç›¸å…³ç½‘ç»œæ•°æ®ã€‚\n\nè¯¥æ¨¡åž‹çš„æ€§èƒ½æ°´å¹³å¯ä¸Žé¢†å…ˆçš„ä»£ç  LLM ç›¸åª²ç¾Žã€‚\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*5rd863dHI-W_2ei7.png)\n\n**å…³é”®è´¡çŒ®**ï¼š\n\n* å›¢é˜Ÿæä¾›äº†æ¨¡åž‹æƒé‡ã€æŽ¨ç†ä»£ç ã€è®­ç»ƒæ•°æ®ã€æ•°æ®å¤„ç†ç®¡é“å’Œè¯¦ç»†çš„è®­ç»ƒåè®®ï¼Œä½¿ç ”ç©¶äººå‘˜å’Œä»Žä¸šè€…èƒ½å¤Ÿåœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œæž„å»ºå’Œåˆ›æ–°ã€‚\n* ä»–ä»¬è¿˜æŽ¨å‡ºäº† **RefineCode æ•°æ®é›†**ï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜è´¨é‡ã€å¯é‡å¤çš„ä»£ç é¢„è®­ç»ƒè¯­æ–™åº“ï¼ŒåŒ…å« **9600 äº¿ä»¤ç‰Œ**ï¼Œæ¶µç›– **607 ç§ç¼–ç¨‹è¯­è¨€**ã€‚\n\næ›´å¤šä¿¡æ¯è¯·æŸ¥çœ‹ [å®˜æ–¹å…¬å‘Š](https://proxy.rifx.online/https://opencoder-llm.github.io/).\n\n## SentenceTransformers åŠ é€Ÿ CPU æŽ¨ç†ï¼Œé€Ÿåº¦æå‡ 4 å€\n\næœ€æ–°å‘å¸ƒçš„ **SentenceTransformers** å¼•å…¥äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä½¿ç”¨ **OpenVINO çš„ int8 é™æ€é‡åŒ–** åœ¨ CPU æŽ¨ç†ä¸­å®žçŽ°é«˜è¾¾ **4 å€çš„é€Ÿåº¦æå‡**ã€‚\n\næ­¤æ›´æ–°ä¼˜åŒ–äº†å¼€å‘è€…åœ¨å¤„ç†å¤§è§„æ¨¡è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡æ—¶çš„è®­ç»ƒå’ŒæŽ¨ç†å·¥ä½œæµç¨‹ã€‚\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Pd9ESPxjKHaHVgV15pCQig.png)\n\n**ä¸»è¦å¢žå¼º**ï¼š\n\n* **OpenVINO int8 é™æ€é‡åŒ–**ï¼šåˆ©ç”¨ OpenVINO çš„é‡åŒ–æŠ€æœ¯ï¼Œæ¨¡åž‹åœ¨ä¿æŒå‡†ç¡®æ€§çš„å‰æä¸‹å®žçŽ°äº†å“è¶Šçš„æŽ¨ç†é€Ÿåº¦ã€‚æ­¤ä¼˜åŒ–è¶…è¶Šäº†çŽ°æœ‰åŽç«¯ï¼Œæé«˜äº†åœ¨ CPU æž¶æž„ä¸Šçš„éƒ¨ç½²æ•ˆçŽ‡ã€‚\n* **åŸºäºŽæç¤ºçš„è®­ç»ƒ**ï¼šæ”¯æŒä½¿ç”¨æç¤ºè¿›è¡Œè®­ç»ƒï¼Œæä¾›äº†ä¸€ç§ç®€å•çš„æ–¹æ³•æ¥æå‡æ€§èƒ½ï¼Œè€Œæ— éœ€é¢å¤–çš„è®¡ç®—å¼€é”€ã€‚\n* **åœ¨ NanoBEIR ä¸Šçš„ä¾¿æ·è¯„ä¼°**ï¼šé€šè¿‡ä½¿ç”¨ NanoBEIRï¼Œè¿™ä¸ªå¼ºå¤§çš„ä¿¡æ¯æ£€ç´¢åŸºå‡† BEIR çš„å­é›†ï¼Œä¾¿äºŽæ›´å¿«é€Ÿåœ°è¯„ä¼°æ¨¡åž‹æ€§èƒ½ã€‚\n* **PEFT å…¼å®¹æ€§**ï¼šçŽ°åœ¨æ”¯æŒ **å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰**ï¼Œé€šè¿‡å…è®¸è½»æ¾æ·»åŠ å’ŒåŠ è½½é€‚é…å™¨ï¼Œå®žçŽ°æ›´é«˜æ•ˆçš„æ¨¡åž‹å®šåˆ¶ã€‚\n\næ‚¨å¯ä»¥åœ¨ [github](https://proxy.rifx.online/https://github.com/UKPLab/sentence-transformers/releases/tag/v3.3.0) ä¸Šæ‰¾åˆ°æ›´å¤šä¿¡æ¯ã€‚\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/rag-llm-and-pdf-conversion-to-markdown-text-with-pymupdf-03af00259b5d","frontmatter":{"title":"RAG/LLM å’Œ PDFï¼šä½¿ç”¨ PyMuPDF è½¬æ¢ä¸º Markdown æ–‡æœ¬","meta_title":"RAG/LLM å’Œ PDFï¼šä½¿ç”¨ PyMuPDF è½¬æ¢ä¸º Markdown æ–‡æœ¬","description":"é‡‡ç”¨ markdown æ–‡æœ¬æ ¼å¼è¾“å…¥æ•°æ®å¯æé«˜ç”Ÿæˆçš„æ–‡æœ¬è´¨é‡","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*swPjVuudAhsoRiiw3Ee32w.png","categories":["Programming","Technology","Technology/Web"],"author":"Rifx.Online","tags":["markdown","PyMuPDF","LLM","RAG","PDF"],"draft":false,"slug":"blog/rag-llm-and-pdf-conversion-to-markdown-text-with-pymupdf-03af00259b5d"},"content":"\n\n\n### ä»¥Markdownæ–‡æœ¬æ ¼å¼è¾“å…¥æ•°æ®å¯ä»¥æé«˜ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡\n\n\n\n## ä»‹ç»\n\nåœ¨**å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰**å’Œ**æ£€ç´¢å¢žå¼ºç”Ÿæˆï¼ˆRAGï¼‰**çŽ¯å¢ƒä¸­ï¼Œä»¥**markdownæ–‡æœ¬æ ¼å¼**è¾“å…¥æ•°æ®å…·æœ‰**é‡è¦æ„ä¹‰**ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›è¯¦ç»†è€ƒè™‘å› ç´ ã€‚\n\n**LLMs** æ˜¯å¼ºå¤§çš„è¯­è¨€æ¨¡åž‹ï¼Œå¯ä»¥ç”Ÿæˆè¿žè´¯ä¸”å…·æœ‰ä¸Šä¸‹æ–‡ç›¸å…³æ€§çš„æ–‡æœ¬ã€‚ç„¶è€Œï¼Œå®ƒä»¬æœ‰æ—¶å¯èƒ½ä¼šäº§ç”Ÿç¼ºä¹äº‹å®žå‡†ç¡®æ€§æˆ–ä¸Šä¸‹æ–‡çš„å“åº”ã€‚é€šè¿‡ç»“åˆåŸºäºŽæ£€ç´¢çš„æ–¹æ³•ï¼ˆå¦‚RAGï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥æé«˜ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡ã€‚\n\n**RAG** ä½¿å¾—å°†**å¤–éƒ¨æ•°æ®**â€”â€”åœ¨LLMçš„è®­ç»ƒæ•°æ®ä¸­ä¹‹å‰ç¼ºå¤±çš„æ•°æ®â€”â€”æ•´åˆåˆ°æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ä¸­æˆä¸ºå¯èƒ½ã€‚è¿™ç§åŒ…å«å‡å°‘äº†â€œå¹»è§‰é—®é¢˜â€ï¼Œå¹¶å¢žå¼ºäº†æ–‡æœ¬å“åº”çš„ç›¸å…³æ€§ã€‚\n\n## ä¸ºä»€ä¹ˆé€‰æ‹© Markdown ç”¨äºŽ LLMï¼Ÿ\n\n**Markdown** æ˜¯ä¸€ç§è½»é‡çº§æ ‡è®°è¯­è¨€ï¼Œå…è®¸ç”¨æˆ·ä½¿ç”¨ç®€å•çš„è¯­æ³•æ ¼å¼åŒ–çº¯æ–‡æœ¬ã€‚å®ƒå¹¿æ³›ç”¨äºŽåˆ›å»ºç»“æž„åŒ–æ–‡æ¡£ï¼Œç‰¹åˆ«æ˜¯åœ¨ GitHubã€Jupyter ç¬”è®°æœ¬å’Œå„ç§å†…å®¹ç®¡ç†ç³»ç»Ÿä¸Šã€‚å½“å°†æ•°æ®è¾“å…¥åˆ° LLM æˆ– RAG ç³»ç»Ÿæ—¶ï¼Œä½¿ç”¨ Markdown æ ¼å¼æä¾›äº†å‡ ä¸ªå¥½å¤„ï¼š\n\n1. **ç»“æž„åŒ–å†…å®¹**ï¼šMarkdown å…è®¸æ‚¨å°†ä¿¡æ¯ç»„ç»‡æˆæ ‡é¢˜ã€åˆ—è¡¨ã€è¡¨æ ¼å’Œå…¶ä»–ç»“æž„åŒ–å…ƒç´ ã€‚è¿™ç§ç»“æž„æœ‰åŠ©äºŽæ›´å¥½åœ°ç†è§£å’Œä¸Šä¸‹æ–‡ä¿ç•™ã€‚\n2. **å¯Œæ–‡æœ¬**ï¼šMarkdown æ”¯æŒåŸºæœ¬æ ¼å¼ï¼Œå¦‚ç²—ä½“ã€æ–œä½“ã€é“¾æŽ¥å’Œä»£ç å—ã€‚åœ¨è¾“å…¥æ•°æ®ä¸­åŒ…å«å¯Œæ–‡æœ¬å¯ä»¥å¢žå¼ºè¯­è¨€æ¨¡åž‹çš„ä¸Šä¸‹æ–‡ã€‚\n3. **åµŒå…¥é“¾æŽ¥å’Œå¼•ç”¨**ï¼šMarkdown å…è®¸æ‚¨åµŒå…¥è¶…é“¾æŽ¥ã€è„šæ³¨å’Œå¼•ç”¨ã€‚åœ¨ RAG åœºæ™¯ä¸­ï¼Œè¿™å¯¹äºŽå¼•ç”¨å¤–éƒ¨æ¥æºæˆ–æä¾›é¢å¤–ä¸Šä¸‹æ–‡è‡³å…³é‡è¦ã€‚\n4. **æ˜“äºŽåˆ›ä½œ**ï¼šMarkdown å…·æœ‰å¯è¯»æ€§ï¼Œæ˜“äºŽç¼–å†™ã€‚ä½œè€…å¯ä»¥é«˜æ•ˆåœ°åˆ›å»ºå†…å®¹ï¼Œè€Œæ— éœ€å¤æ‚çš„æ ¼å¼åŒ–å·¥å…·ã€‚\n5. **åˆ†å—**ï¼šå¯¹äºŽ RAG ç³»ç»Ÿè‡³å…³é‡è¦ï¼Œåˆ†å—ï¼ˆä¹Ÿç§°ä¸ºâ€œæ‹†åˆ†â€ï¼‰å°†å¤§é‡æ–‡æ¡£æ‹†åˆ†ä¸ºæ›´æ˜“å¤„ç†çš„éƒ¨åˆ†ã€‚é€šè¿‡æ”¯æŒ MD æ ¼å¼çš„ PyMuPDF æ•°æ®æå–ï¼Œæˆ‘ä»¬æ”¯æŒåˆ†å—ä»¥ä¿æŒå…·æœ‰å…±åŒä¸Šä¸‹æ–‡çš„æ–‡æœ¬åœ¨ä¸€èµ·ã€‚**é‡è¦çš„æ˜¯ï¼ŒMD æ ¼å¼çš„ PyMuPDF æå–å…è®¸è¿›è¡Œ [ç¬¬ 3 çº§åˆ†å—](https://readmedium.com/five-levels-of-chunking-strategies-in-rag-notes-from-gregs-video-7b735895694d#b123)**ã€‚\n\næ€»ä¹‹ï¼Œåœ¨ LLM å’Œ RAG çŽ¯å¢ƒä¸­ä½¿ç”¨ Markdown æ–‡æœ¬æ ¼å¼å¯ä»¥ç¡®ä¿æ›´å‡†ç¡®å’Œç›¸å…³çš„ç»“æžœï¼Œå› ä¸ºå®ƒæä¾›äº†æ›´ä¸°å¯Œçš„æ•°æ®ç»“æž„å’Œæ›´ç›¸å…³çš„æ•°æ®å—è´Ÿè½½ç»™æ‚¨çš„ LLMã€‚\n\n## PyMuPDF æ”¯æŒ PDF çš„ Markdown è½¬æ¢\n\nè‡ªæŽ¨å‡ºä»¥æ¥ï¼ŒPyMuPDF ä¸€ç›´èƒ½å¤Ÿä»Ž PDF é¡µé¢ä¸­æå–æ–‡æœ¬ã€å›¾åƒã€çŸ¢é‡å›¾å½¢ï¼Œå¹¶ä¸”ä»Ž 2023 å¹´ 8 æœˆèµ·ï¼Œè¿˜èƒ½å¤Ÿæå–è¡¨æ ¼ã€‚è¿™äº›å¯¹è±¡ç±»åž‹å„è‡ªæœ‰å…¶æå–æ–¹æ³•ï¼šæ–‡æœ¬æœ‰ä¸€ç§ï¼Œè¡¨æ ¼ã€å›¾åƒå’ŒçŸ¢é‡å›¾å½¢åˆ™æœ‰å…¶ä»–æ–¹æ³•ã€‚ä¸ºäº†æ»¡è¶³ RAG çš„è¦æ±‚ï¼Œæˆ‘ä»¬å°†è¿™äº›ä¸åŒçš„æå–æ–¹å¼åˆå¹¶ï¼Œç”Ÿæˆä¸€ä¸ªç»Ÿä¸€çš„ **Markdown** å­—ç¬¦ä¸²ï¼Œä»¥ä¸€è‡´åœ°è¡¨ç¤ºé¡µé¢çš„æ•´ä½“å†…å®¹ã€‚\n\næ‰€æœ‰è¿™äº›éƒ½å®žçŽ°ä¸º [ä¸€ä¸ª Python è„šæœ¬](https://github.com/pymupdf/RAG/blob/main/helpers/pymupdf_rag.py)ã€‚å®ƒå¯ä»¥è¢«å…¶ä»–è„šæœ¬ä½œä¸ºæ¨¡å—å¯¼å…¥ï¼Œæˆ–è€…åœ¨ç»ˆç«¯çª—å£ä¸­é€šè¿‡ä»¥ä¸‹å‘½ä»¤è¡Œè°ƒç”¨ï¼š\n\n`$ python pymupdf_rag.py input.pdf [-pages PAGES]`\n\nå®ƒå°†ç”Ÿæˆä¸€ä¸ª **Markdown** æ ¼å¼çš„æ–‡æœ¬æ–‡ä»¶ï¼ˆç§°ä¸º `input.md`ï¼‰ã€‚å¯é€‰å‚æ•° `PAGES` å…è®¸å°†è½¬æ¢é™åˆ¶ä¸º PDF æ€»é¡µé¢çš„ä¸€ä¸ªå­é›†ã€‚å¦‚æžœçœç•¥ï¼Œåˆ™å¤„ç†æ•´ä¸ª PDFã€‚\n\n## Markdown åˆ›å»ºç»†èŠ‚\n\n### é€‰æ‹©è¦è€ƒè™‘çš„é¡µé¢\n\nâ€œ`-pages`â€ å‚æ•°æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œç”±æ‰€éœ€çš„é¡µé¢ç¼–å·ï¼ˆä»Ž1å¼€å§‹ï¼‰ç»„æˆï¼Œç”¨äºŽè€ƒè™‘è¿›è¡Œmarkdownè½¬æ¢ã€‚å¯ä»¥ç»™å‡ºå¤šä¸ªé¡µé¢ç¼–å·è§„èŒƒï¼Œä½¿ç”¨é€—å·åˆ†éš”ã€‚æ¯ä¸ªè§„èŒƒå¯ä»¥æ˜¯ä¸€ä¸ªæ•´æ•°æˆ–ä¸¤ä¸ªç”¨â€œ`-`â€è¿žæŽ¥çš„æ•´æ•°ï¼ŒæŒ‡å®šä¸€ä¸ªé¡µé¢èŒƒå›´ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼š\n\nâ€œ`-pages 1â€“10,15,20-N`â€\n\nè¿™å°†åŒ…æ‹¬ç¬¬1é¡µåˆ°ç¬¬10é¡µã€ç¬¬15é¡µä»¥åŠç¬¬20é¡µåˆ°æ–‡ä»¶æœ«å°¾ï¼ˆå¤§å†™â€œNâ€è¢«è§†ä¸ºæœ€åŽä¸€é¡µçš„ç¼–å·ï¼‰ã€‚\n\n### è¯†åˆ«æ ‡é¢˜\n\nåœ¨è°ƒç”¨æ—¶ï¼Œç¨‹åºæ£€æŸ¥ç»™å®šé¡µé¢ä¸Šçš„æ‰€æœ‰æ–‡æœ¬å¹¶æ‰¾å‡ºæœ€å¸¸ç”¨çš„å­—ä½“å¤§å°ã€‚è¯¥å€¼ï¼ˆä»¥åŠæ‰€æœ‰è¾ƒå°çš„å­—ä½“å¤§å°ï¼‰è¢«å‡å®šä¸º **æ­£æ–‡æ–‡æœ¬**ã€‚è¾ƒå¤§çš„å­—ä½“å¤§å°è¢«å‡å®šä¸º **æ ‡é¢˜æ–‡æœ¬**ã€‚\n\næ ¹æ®å®ƒä»¬åœ¨å­—ä½“å¤§å°å±‚çº§ä¸­çš„ç›¸å¯¹ä½ç½®ï¼Œæ ‡é¢˜æ–‡æœ¬å°†å‰é¢åŠ ä¸Šä¸€ä¸ªæˆ–å¤šä¸ª markdown æ ‡é¢˜ `#` æ ‡ç­¾å­—ç¬¦ã€‚\n\n### æŒ‰é¡µé¢åŒºåŸŸè¯†åˆ«å¤„ç†æ¨¡å¼\n\næ¯ä¸ªé¡µé¢ä¸Šçš„æ‰€æœ‰æ–‡æœ¬é¦–å…ˆå°†è¢«åˆ†ç±»ä¸º**æ ‡å‡†**æ–‡æœ¬æˆ–**è¡¨æ ¼**æ–‡æœ¬ã€‚ç„¶åŽï¼Œé¡µé¢å†…å®¹å°†ä»Žä¸Šåˆ°ä¸‹æå–ï¼Œå¹¶è½¬æ¢ä¸ºMarkdownæ ¼å¼ã€‚\n\nè¿™æœ€å¥½é€šè¿‡ä¸€ä¸ªä¾‹å­æ¥è§£é‡Šï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*u5fv2aAIvDaaAd6H.png)\n\nè¯¥é¡µé¢æ˜¾ç¤ºçš„å†…å®¹ä»£è¡¨å…¸åž‹æƒ…å†µï¼š\n\n* ä¸¤ä¸ªè¡¨æ ¼ï¼Œå…·æœ‰éƒ¨åˆ†é‡å çš„åž‚ç›´ä½ç½®ã€‚ä¸€ä¸ªè¡¨æ ¼æ²¡æœ‰æ ‡é¢˜ï¼Œå¦ä¸€ä¸ªè¡¨æ ¼æœ‰**å¤–éƒ¨**åˆ—æ ‡é¢˜ã€‚\n* æœ‰ä¸€è¡Œ**æ ‡é¢˜**å’Œå¤šä¸ªçº§åˆ«çš„**æ ‡é¢˜**ã€‚\n* **æ­£æ–‡æ–‡æœ¬**åŒ…å«å¤šç§æ ·å¼ç»†èŠ‚ï¼Œå¦‚**ç²—ä½“**ã€*æ–œä½“*å’Œ`è¡Œå†…ä»£ç `ã€‚\n* æœ‰åºå’Œæ— åºåˆ—è¡¨ã€‚\n* ä»£ç ç‰‡æ®µã€‚\n\nå¸ƒå±€åˆ†æžå°†ç¡®å®šä¸‰ä¸ªåŒºåŸŸå¹¶é€‰æ‹©é€‚å½“çš„å¤„ç†æ¨¡å¼ï¼š**(1)** æ–‡æœ¬ï¼Œ**(2)** è¡¨æ ¼ï¼Œ**(3)** æ–‡æœ¬ã€‚\n\nç”Ÿæˆçš„Markdownæ–‡æœ¬å¿ å®žåœ°åæ˜ äº†ä¸Šè¿°å†…å®¹â€”â€”åœ¨è¿™ç§æ ¼å¼ä¸­å°½å¯èƒ½åšåˆ°ã€‚\n\nä½œä¸ºä¸€ä¸ªä¾‹å­ï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹å…·æœ‰å¤–éƒ¨æ ‡é¢˜çš„è¡¨æ ¼çš„è¾“å‡ºï¼š\n\n```python\n|Column1|Column2|\n\n|---|---|\n\n|Cell (0, 0)|Cell (0, 1)|\n\n|Cell (1, 0)|Cell (1, 1)|\n\n|Cell (2, 0)|Cell (2, 1)|\n```\nè¿™æ˜¯ä¸ŽGitHubå…¼å®¹çš„æ ¼å¼ï¼Œå…·æœ‰æœ€å°çš„å¯èƒ½ä»¤ç‰Œå¤§å°â€”â€”è¿™æ˜¯ä¿æŒè¾“å…¥åˆ°RAGç³»ç»Ÿçš„å°åž‹åŒ–çš„é‡è¦æ–¹é¢ã€‚\n\n**åˆ—è¾¹æ¡†**ç”±â€œ`|`â€å­—ç¬¦è¡¨ç¤ºã€‚å¦‚æžœæ–‡æœ¬è¡ŒåŽé¢è·Ÿç€â€œ`|---|---| â€¦`â€å½¢å¼çš„è¡Œï¼Œåˆ™å‡å®šè¯¥æ–‡æœ¬è¡Œæ˜¯**è¡¨å¤´**ã€‚å®Œæ•´çš„**è¡¨æ ¼å®šä¹‰**å¿…é¡»å‰åŽè‡³å°‘æœ‰ä¸€è¡Œç©ºè¡Œã€‚\n\nè¯·æ³¨æ„ï¼Œç”±äºŽæŠ€æœ¯åŽŸå› ï¼ŒMarkdownè¡¨æ ¼å¿…é¡»æœ‰ä¸€ä¸ªæ ‡é¢˜ï¼Œå› æ­¤å¦‚æžœæ²¡æœ‰å¤–éƒ¨æ ‡é¢˜ï¼Œå°†é€‰æ‹©ç¬¬ä¸€è¡Œä½œä¸ºè¡¨å¤´ã€‚\n\nä¸ºäº†ç¡®è®¤æ•´ä½“å‡†ç¡®æ€§ï¼Œä»¥ä¸‹æ˜¯Markdownè§£æžå™¨å¦‚ä½•å¤„ç†å®Œæ•´é¡µé¢çš„ç¤ºä¾‹ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Ge83uj7FiM4T6XFn)\n\n## ä»¥ç¼–ç¨‹æ–¹å¼è°ƒç”¨ Markdown è½¬æ¢å™¨\n\né™¤äº†åœ¨å‘½ä»¤è¡Œä¸­æ‰§è¡Œç¨‹åºå¤–ï¼ŒMarkdown è½¬æ¢ä¹Ÿå¯ä»¥é€šè¿‡ç¨‹åºè¯·æ±‚ï¼š\n\n```python\nimport fitz\nfrom pymupdf_rag import to_markdown  # import Markdown converter\n\ndoc = fitz.open(â€œinput.pdfâ€)  # open input PDF\n\n## define desired pages: this corresponds â€œ-pages 1-10,15,20-Nâ€\npage_list = list(range(9)) + [14] + list(range(19, len(doc) â€“ 1))\n\n## get markdown string for all pages\nmd_text = to_markdown(doc, pages=page_list)\n\n## write markdown string to some file\noutput = open(â€œout-markdown.mdâ€, â€œwâ€)\noutput.write(md_text)\noutput.close()\n```\n\n## ç»“è®º\n\né€šè¿‡é›†æˆ PyMuPDF çš„æå–æ–¹æ³•ï¼ŒPDF é¡µé¢çš„å†…å®¹å°†è¢«å¿ å®žåœ°è½¬æ¢ä¸ºå¯ç”¨ä½œ RAG èŠå¤©æœºå™¨äººçš„è¾“å…¥çš„ Markdown æ–‡æœ¬ã€‚\n\nè¯·è®°ä½ï¼ŒæˆåŠŸçš„ RAG èŠå¤©æœºå™¨äººçš„å…³é”®åœ¨äºŽå®ƒèƒ½å¤Ÿè®¿é—®çš„ä¿¡æ¯çš„è´¨é‡å’Œå®Œæ•´æ€§ã€‚\n\nå¯ç”¨ PyMuPDF çš„ Markdown æå–ç¡®ä¿ä»Ž PDF ä¸­èŽ·å–è¿™äº›ä¿¡æ¯ä¸ä»…æ˜¯å¯èƒ½çš„ï¼Œè€Œä¸”æ˜¯ç®€å•çš„ï¼Œå±•ç¤ºäº†è¯¥åº“çš„å¼ºå¤§å’Œå¯¹å¼€å‘è€…çš„å‹å¥½ã€‚ç¥ç¼–ç æ„‰å¿«ï¼\n\n### æºä»£ç \n\n* [RAG/helpers/pymupdf\\_rag.py (github.com)](https://github.com/pymupdf/RAG/blob/main/helpers/pymupdf_rag.py)\n\n### å‚è€ƒæ–‡çŒ®\n\n* [5 Levels of Text Splitting](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb)\n\n### ç›¸å…³åšå®¢\n\n* [ä½¿ç”¨ ChatGPT API å’Œ PyMuPDF æž„å»º RAG èŠå¤©æœºå™¨äºº GUI](https://readmedium.com/building-a-rag-chatbot-gui-with-the-chatgpt-api-and-pymupdf-9ea8c7fc4ab5)\n* [ä½¿ç”¨ ChatGPT å’Œ PyMUPDF åˆ›å»º RAG èŠå¤©æœºå™¨äºº](https://readmedium.com/creating-a-rag-chatbot-with-chatgpt-and-pymupdf-f6c30907ae27)\n* [RAG/LLM å’Œ PDFï¼šå¢žå¼ºæ–‡æœ¬æå–](https://readmedium.com/rag-llm-and-pdf-enhanced-text-extraction-5c5194c3885c)\n\n"},{"lang":"zh","group":"blog","slug":"blog/ragate-adaptive-rag-for-conversational-ai-94b5ca469b7d","frontmatter":{"title":"RAGateï¼šç”¨äºŽå¯¹è¯å¼äººå·¥æ™ºèƒ½çš„è‡ªé€‚åº” RAG","meta_title":"RAGateï¼šç”¨äºŽå¯¹è¯å¼äººå·¥æ™ºèƒ½çš„è‡ªé€‚åº” RAG","description":"RAGateæ˜¯ä¸€ç§è‡ªé€‚åº”æœºåˆ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–å¯¹è¯AIç³»ç»Ÿä¸­å¤–éƒ¨çŸ¥è¯†ä¸Žå†…éƒ¨çŸ¥è¯†çš„ä½¿ç”¨ã€‚é€šè¿‡åŠ¨æ€è¯„ä¼°ä½•æ—¶æ£€ç´¢å¤–éƒ¨ä¿¡æ¯ï¼ŒRAGateæé«˜äº†å“åº”çš„ç›¸å…³æ€§å’Œå‡†ç¡®æ€§ï¼Œé¿å…äº†å¯¹å¤–éƒ¨çŸ¥è¯†çš„è¿‡åº¦ä¾èµ–ã€‚è¯¥æ–¹æ³•å…·æœ‰å¤šç§å˜ä½“ï¼Œå¦‚RAGate-Promptã€RAGate-PEFTå’ŒRAGate-MHAï¼Œé€‚ç”¨äºŽåŒ»ç–—ã€å®¢æˆ·æ”¯æŒç­‰å¤šä¸ªé¢†åŸŸï¼Œèƒ½å¤Ÿæ˜¾è‘—æå‡ç”¨æˆ·ä½“éªŒå’Œç³»ç»Ÿæ€§èƒ½ã€‚","date":"2024-11-14T03:29:09.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8wzI-5BRV1-br0e3MBVD2g.png","categories":["Chatbots","Natural Language Processing","Machine Learning"],"author":"Rifx.Online","tags":["RAGate","conversational","retrieval","latency","personalization"],"draft":false,"slug":"blog/ragate-adaptive-rag-for-conversational-ai-94b5ca469b7d"},"content":"\n\n\næž„å»ºå¯¹è¯ AI ç³»ç»Ÿæ˜¯å›°éš¾çš„ï¼ï¼ï¼\n\nè¿™è™½ç„¶å¯è¡Œï¼Œä½†ä¹Ÿ**å¤æ‚ã€è€—æ—¶ä¸”èµ„æºå¯†é›†**ã€‚\n\næŒ‘æˆ˜åœ¨äºŽè®¾è®¡èƒ½å¤Ÿç†è§£å’Œç”Ÿæˆç±»äººå“åº”çš„ç³»ç»Ÿï¼Œå¹¶ç¡®ä¿è¿™äº›ç³»ç»Ÿèƒ½å¤Ÿæœ‰æ•ˆåœ°ä¸Žç”¨æˆ·äº’åŠ¨ï¼Œé€‚åº”å¯¹è¯çš„ç»†å¾®å·®åˆ«ã€‚\n\néžå¸¸æµè¡Œçš„**RAGï¼ˆæ£€ç´¢å¢žå¼ºç”Ÿæˆï¼‰**é€šè¿‡å°†å¤–éƒ¨çŸ¥è¯†ä¸Ž LLM çš„å†…éƒ¨çŸ¥è¯†æ— ç¼é›†æˆï¼Œå½»åº•æ”¹å˜äº†å¯¹è¯ AIã€‚é€šè¿‡å°† RAG åº”ç”¨äºŽæ‚¨çš„å•†ä¸šæ•°æ®ï¼Œæ‚¨çš„å®¢æˆ·å¯ä»¥ç”¨è‡ªç„¶è¯­è¨€è¯¢é—®ä»–ä»¬çš„æ•°æ®ï¼Œä»Žè€Œä¿ƒè¿›æ— ç¼äº’åŠ¨ã€‚\n\n**ç„¶è€Œï¼Œæœ‰ä¸€ä¸ªè­¦å‘Šï¼š** åœ¨ä½¿ç”¨ RAG æ—¶ï¼Œå¾ˆæ˜Žæ˜¾å¹¶ä¸æ˜¯æ¯ä¸ªæŸ¥è¯¢éƒ½éœ€è¦ä»Žâ€œå¤–éƒ¨çŸ¥è¯†â€ä¸­èŽ·å–ç­”æ¡ˆã€‚è¿‡åº¦ä¾èµ–å¤–éƒ¨æ¥æºå¯èƒ½ä¼šç ´åçœŸæ­£çš„äº’åŠ¨ã€‚å°±åƒä¸ŽæŸäººäº¤è°ˆï¼Œå¯¹äºŽæ¯ä¸ªé—®é¢˜éƒ½è¦åŽ»ç¿»ä¸€æœ¬ä¹¦æ¥æž„å»ºæ‚¨çš„å›žç­”ï¼Œå³ä½¿æ‚¨å·²ç»å¯¹è¯¥ä¸»é¢˜æœ‰æ›´æ·±å…¥çš„ç†è§£ã€‚æ›´ç³Ÿç³•çš„æ˜¯ï¼Œæ‚¨å¯èƒ½æ‰¾ä¸åˆ°ä»»ä½•å…³äºŽè¯¥ä¸»é¢˜çš„ä¹¦ï¼Œæœ€ç»ˆå›žç­”â€œæˆ‘ä¸çŸ¥é“â€ï¼Œå°½ç®¡æ‚¨æ‹¥æœ‰å¯ä»¥æä¾›æ›´æ·±åˆ»ç­”æ¡ˆçš„å†…éƒ¨çŸ¥è¯†ã€‚\n\næ˜¾ç„¶ï¼Œåœ¨ä½¿ç”¨ RAG æ—¶ï¼Œéœ€è¦ä¸€ç§æœºåˆ¶æ¥ç¡®å®šåœ¨æŽ¨ç†æ—¶ä½•æ—¶åˆ©ç”¨â€œå¤–éƒ¨çŸ¥è¯†â€ä¸Žâ€œå†…éƒ¨çŸ¥è¯†â€ã€‚\n\nå¼•å…¥**RAGate**â€”â€”ä¸€ç§äºŒå…ƒå¼€å…³ï¼Œæ—¨åœ¨åŠ¨æ€è¯„ä¼°ä½•æ—¶åˆ©ç”¨å¤–éƒ¨çŸ¥è¯†ä»¥åŠä½•æ—¶ä¾èµ–å†…éƒ¨è§è§£ã€‚ç”± Xi Wangã€Procheta Senã€Ruizhe Li å’Œ Emine Yilmaz æå‡ºï¼Œå¹¶äºŽ 2024 å¹´ 7 æœˆå‘å¸ƒäºŽ [**ArXiv**](https://proxy.rifx.online/https://arxiv.org/abs/2407.21712) **ï¼ˆç”¨äºŽå¯¹è¯ç³»ç»Ÿçš„è‡ªé€‚åº”æ£€ç´¢å¢žå¼ºç”Ÿæˆï¼‰ã€‚**\n\nè®©æˆ‘ä»¬é€šè¿‡ç¤ºä¾‹è¿›ä¸€æ­¥äº†è§£ã€‚\n\n## ä»€ä¹ˆæ˜¯å¯¹è¯å¼äººå·¥æ™ºèƒ½ï¼Ÿ\n\n**å¯¹è¯**æ˜¯ä¸ªä½“ä¹‹é—´æ€æƒ³ã€æƒ…æ„Ÿå’Œä¿¡æ¯çš„äº¤æµï¼Œé€‚åº”è¯­è°ƒã€ä¸Šä¸‹æ–‡å’Œç»†å¾®çš„æš—ç¤ºï¼Œä»¥å¼•å¯¼äº’åŠ¨ã€‚äººç±»å¤©ç”Ÿé€‚åˆè¿›è¡Œå¯¹è¯ï¼Œå› ä¸ºå…·å¤‡æƒ…å•†ã€ç¤¾äº¤èƒ½åŠ›å’Œæ–‡åŒ–æŽ¥è§¦ç­‰ç‰¹è´¨ï¼Œè¿™å¸®åŠ©æˆ‘ä»¬ç†è§£ç»†å¾®å·®åˆ«å¹¶é€‚åº”ä¸åŒçš„ç¤¾äº¤çŽ¯å¢ƒã€‚\n\n**å¯¹è¯å¼äººå·¥æ™ºèƒ½**æ—¨åœ¨é€šè¿‡æŠ€æœ¯å¤åˆ¶è¿™ç§ç±»äººäº’åŠ¨ï¼Œç†è§£å’Œç”Ÿæˆè‡ªç„¶ã€ç¬¦åˆä¸Šä¸‹æ–‡çš„ã€å¼•äººå…¥èƒœçš„å›žåº”ã€‚å®ƒé€‚åº”ç”¨æˆ·è¾“å…¥ï¼Œä½¿äº’åŠ¨æµç•…è€ŒåŠ¨æ€ï¼Œåƒäººç±»ä¹‹é—´çš„å¯¹è¯ã€‚\n\n## ä»€ä¹ˆæ˜¯ AI ç³»ç»Ÿçš„å¤–éƒ¨çŸ¥è¯†å’Œå†…éƒ¨çŸ¥è¯†ï¼Ÿ\n\nåœ¨å¼€å¤´æ®µè½ä¸­ï¼Œæˆ‘æåˆ°äº†ä¸¤ä¸ªå…³é”®æœ¯è¯­â€”â€”å¤–éƒ¨çŸ¥è¯†å’Œå†…éƒ¨çŸ¥è¯†ã€‚è®©æˆ‘ä»¬èŠ±ä¸€ç‚¹æ—¶é—´æ¥æ¾„æ¸…è¿™äº›æ¦‚å¿µï¼Œå› ä¸ºç†è§£å®ƒä»¬å°†ä½¿å­¦ä¹  RAGate å˜å¾—æ›´åŠ å®¹æ˜“ã€‚\n\n**External knowledge** åŒ…å«ä¸å±žäºŽ AI æ¨¡åž‹çš„å›ºæœ‰ä¿¡æ¯ï¼Œè€Œæ˜¯ä»Žå¤–éƒ¨æ¥æºæ£€ç´¢çš„ã€‚æ¥æºåŒ…æ‹¬ç»“æž„åŒ–æ•°æ®å­˜å‚¨åº“ã€APIã€æŒ‡å—ã€å¸¸è§é—®é¢˜è§£ç­”å’Œç½‘ç»œæ¥æºç­‰éžç»“æž„åŒ–çŸ¥è¯†åº“ã€‚å¤–éƒ¨çŸ¥è¯†çš„ä¸»è¦ä½œç”¨æ˜¯æä¾›äº‹å®žã€æœ€æ–°å’Œä¸Šä¸‹æ–‡ç›¸å…³çš„ä¿¡æ¯ï¼Œä»Žè€Œæé«˜ AI å“åº”çš„å‡†ç¡®æ€§å’Œå…¨é¢æ€§ã€‚\n\n**Internal knowledge** æŒ‡çš„æ˜¯åµŒå…¥åœ¨ AI æ¨¡åž‹ä¸­çš„å†…ç½®çŸ¥è¯†å’Œå¤„ç†èƒ½åŠ›ï¼Œè¿™äº›çŸ¥è¯†å’Œèƒ½åŠ›åŸºäºŽå…¶è®­ç»ƒæ•°æ®ã€‚æ¥æºåŒ…æ‹¬æ¥è‡ªå¤šæ ·åŒ–æ•°æ®é›†çš„é¢„è®­ç»ƒçŸ¥è¯†ï¼ŒåŒ…æ‹¬è¯­è¨€æ¨¡å¼ã€è¯­æ³•ã€å…±äº«äº‹å®žå’Œä¸€èˆ¬ä¸–ç•ŒçŸ¥è¯†ã€æ¥è‡ªè¿‡åŽ»äº¤äº’çš„è®°å¿†çš„ä¸Šä¸‹æ–‡æ„è¯†ï¼Œä»¥åŠ AI çš„è¯­ä¹‰ç†è§£å’Œç†è§£èƒ½åŠ›ã€‚\n\n## RAG å’ŒæŠ¤æ  â€” å¼ºå¤§çš„ç»„åˆï¼Œä½†æœ‰å±€é™æ€§ï¼\n\nRAG ç»“åˆäº†ä¸¤ä¸ªå¼ºå¤§çš„å…ƒç´ ï¼š(1\\) å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰å¤„ç†è‡ªç„¶è¯­è¨€çš„èƒ½åŠ›ï¼Œä»¥è§£é‡Šå’Œç”Ÿæˆç±»äººæ–‡æœ¬ã€‚ (2\\) æ£€ç´¢å’Œå¢žå¼ºå¤–éƒ¨æœ€æ–°ä¿¡æ¯çš„èƒ½åŠ›ã€‚\n\nè®¸å¤š RAG å®žçŽ°éƒ½åŒ…å« **æŠ¤æ **ã€çº¦æŸæˆ–è§„åˆ™ï¼Œä»¥å¼•å¯¼ç³»ç»Ÿçš„è¡Œä¸ºæœç€è´Ÿè´£ä»»å’Œé¢†åŸŸç•Œå®šçš„äººå·¥æ™ºèƒ½ã€‚è¿™äº›æŠ¤æ é€šå¸¸ä¼˜å…ˆä½¿ç”¨å¤–éƒ¨çŸ¥è¯†ï¼Œè€Œä¸æ˜¯æ¨¡åž‹çš„å†…éƒ¨çŸ¥è¯†ï¼Œä»¥ç¡®ä¿å“åº”çš„å¯é¢„æµ‹æ€§ã€‚ä¸¥æ ¼åº”ç”¨è¿™äº›æŠ¤æ æœ‰æ—¶å¯èƒ½å¯¼è‡´æ¬¡ä¼˜ç»“æžœï¼š\n\n* **è¿‡åº¦ä¾èµ–å¤–éƒ¨æ¥æºï¼š** ç³»ç»Ÿå¯èƒ½è¢«è¿«å¯»æ±‚å¤–éƒ¨ä¿¡æ¯ï¼Œå³ä½¿æ˜¯å¯¹äºŽä¸€èˆ¬é—®é¢˜ï¼ŒLLM çš„å†…éƒ¨çŸ¥è¯†å¯èƒ½å·²ç»è¶³å¤Ÿã€‚\n* **å“åº”æµç•…æ€§é™ä½Žçš„æ½œåœ¨é£Žé™©ï¼š** é€šè¿‡é™åˆ¶å†…éƒ¨çŸ¥è¯†ï¼Œç³»ç»Ÿåœ¨æŸäº›æƒ…å†µä¸‹å¯èƒ½äº§ç”Ÿä¸é‚£ä¹ˆè‡ªç„¶æˆ–ä¸Šä¸‹æ–‡ä¸é€‚å½“çš„å“åº”ã€‚\n* **å»¶è¿Ÿå¢žåŠ ï¼š** ä¸æ–­æ£€ç´¢å¤–éƒ¨ä¿¡æ¯å¯èƒ½å¯¼è‡´å“åº”æ—¶é—´æ¯”ä¾èµ–å†…éƒ¨çŸ¥è¯†æ—¶æ›´æ…¢ã€‚\n* **é”™å¤±æœºä¼šï¼š** åµŒå…¥åœ¨ LLM å‚æ•°ä¸­çš„å¹¿æ³›çŸ¥è¯†å¯èƒ½æœªè¢«å……åˆ†åˆ©ç”¨ï¼Œå¯èƒ½ä¼šé”™è¿‡æœ‰ä»·å€¼çš„è§è§£æˆ–è”ç³»ã€‚\n\n## RAGateçš„å¹³è¡¡è‰ºæœ¯\n\nRAGateï¼Œå³**æ£€ç´¢å¢žå¼ºç”Ÿæˆé—¨**ï¼Œé€šè¿‡è‡ªé€‚åº”åœ°ç¡®å®šä½•æ—¶å°†å¤–éƒ¨çŸ¥è¯†çº³å…¥å“åº”ï¼Œä»Žè€Œå¢žå¼ºå¯¹è¯AIç³»ç»Ÿçš„èƒ½åŠ›ã€‚\n\n[RAGateç ”ç©¶](https://proxy.rifx.online/https://arxiv.org/abs/2407.21712)æŽ¢è®¨äº†å¯¹è¯ç³»ç»Ÿä¸­**è‡ªé€‚åº”å¢žå¼º**çš„éœ€æ±‚ï¼Œå¹¶å°†RAGateæå‡ºä¸ºä¸€ç§**é—¨æŽ§æ¨¡åž‹**ï¼Œé¢„æµ‹ä½•æ—¶æ£€ç´¢å¤–éƒ¨çŸ¥è¯†æ˜¯æœ‰ç›Šçš„ã€‚è®ºæ–‡æä¾›äº†å¤§é‡å®žéªŒå’Œåˆ†æžï¼Œå±•ç¤ºäº†RAGateåœ¨æé«˜åŸºäºŽRAGçš„å¯¹è¯ç³»ç»Ÿçš„å“åº”è´¨é‡å’Œç”Ÿæˆä¿¡å¿ƒæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚\n\n\n\n## RAGate ç¤ºä¾‹\n\n**åœºæ™¯ï¼š** ç”¨æˆ·æ­£åœ¨ä¸Žä¸€ä¸ªä¸“æ³¨äºŽåŒ»ç–—ä¿å¥çš„èŠå¤©æœºå™¨äººäº’åŠ¨ï¼Œè¯¥æœºå™¨äººæ ¹æ®ä¸€èˆ¬å¥åº·åŽŸåˆ™å’ŒåŒ»å­¦çŸ¥è¯†æä¾›ä¸ªæ€§åŒ–çš„å¥åº·å»ºè®®ã€‚\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*o0mWnGGefJ0TyDv1u14njw.png)\n\nRAGate å¯ä»¥é€šè¿‡å¹³è¡¡å†…éƒ¨å’Œå¤–éƒ¨çŸ¥è¯†è¿›ä¸€æ­¥å¢žå¼ºå¯¹è¯ã€‚å®ƒå…è®¸äººå·¥æ™ºèƒ½ä½¿ç”¨å†…éƒ¨åŒ»å­¦çŸ¥è¯†æä¾›ä¸€èˆ¬ä¿¡æ¯ï¼ŒåŒæ—¶æ£€ç´¢æœ€æ–°ç ”ç©¶ã€‚å®ƒç”šè‡³å¯ä»¥æ™ºèƒ½åœ°ç»¼åˆæ¥è‡ªå¤šä¸ªæ¥æºçš„æ•°æ®è¿›è¡Œå…¨é¢åˆ†æžï¼ŒåŸºäºŽæ‚£è€…è¯¦æƒ…æä¾›ä¸ªæ€§åŒ–è§è§£ï¼Œå¹¶è¿‡æ»¤å¤–éƒ¨ä¿¡æ¯ä»¥ä¼˜å…ˆè€ƒè™‘æœ€ç›¸å…³çš„å†…å®¹ï¼Œä»Žè€Œå‡å°‘ä¿¡æ¯è¿‡è½½ã€‚\n\n## RAGate çš„å˜ä½“\n\nå¦‚è®ºæ–‡ä¸­æ‰€è¿°ï¼ŒRAGate æä¾›äº† 3 ä¸ªå˜ä½“ â€” **RAGate\\-Prompt**ã€**RAGate\\-PEFT (å‚æ•°\\-é«˜æ•ˆå¾®è°ƒ)** å’Œ **RAGate\\-MHA (å¤šå¤´æ³¨æ„åŠ›)**ã€‚\n\nRAGate çš„æ¯ä¸ªå˜ä½“ â€” Promptã€PEFT å’Œ MHA â€” é‡‡ç”¨ä¸åŒçš„æ–¹æ³•æ¥æ•´åˆå¤–éƒ¨çŸ¥è¯†ï¼Œæ—¨åœ¨æé«˜ AI ç”Ÿæˆå“åº”çš„ç›¸å…³æ€§å’Œå‡†ç¡®æ€§ã€‚\n\nä»¥ä¸‹æ˜¯å¿«é€Ÿæ¯”è¾ƒè¡¨ï¼š\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*3dZg6rHlqmddK1ZQqqu_Aw.png)\n\n## å¦‚ä½•å®žçŽ° RAGateï¼Ÿ\n\næœ¬æ–‡æä¾›äº†å®žçŽ° RAGate çš„é€æ­¥æŒ‡å—ï¼š\n\n1. **å®šä¹‰é—®é¢˜**ï¼šè¿™ä¸€æ­¥è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒæ¶‰åŠåˆ°è¯†åˆ«æ‚¨å¸Œæœ›é€šè¿‡ RAGate å¢žå¼ºçš„å¯¹è¯ä»»åŠ¡ã€‚ç¡®å®šå¯¹è¯çš„èŒƒå›´å’Œæ‚¨å¸Œæœ›è¦†ç›–çš„ç‰¹å®šé¢†åŸŸï¼ˆä¾‹å¦‚ï¼Œé¤åŽ…æŽ¨èã€æ—…è¡Œè§„åˆ’ï¼‰ã€‚\n2. **é€‰æ‹©è¯­è¨€æ¨¡åž‹**ï¼šé€‰æ‹©é€‚åˆçš„å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMï¼‰ä½œä¸ºæ‚¨å¯¹è¯ç³»ç»Ÿçš„åŸºç¡€ã€‚å¯é€‰æ¨¡åž‹åŒ…æ‹¬ Llamaã€GPT-2 æˆ–å…¶ä»–åŸºäºŽå˜æ¢å™¨çš„æž¶æž„ã€‚\n3. **æ”¶é›†å’Œæ ‡æ³¨æ•°æ®**ï¼šæ”¶é›†ä¸Žæ‚¨çš„å¯¹è¯é¢†åŸŸç›¸å…³çš„æ•°æ®é›†ã€‚KETOD æ•°æ®é›†æ˜¯ä¸€ä¸ªä¼˜ç§€çš„ä¾‹å­ï¼Œå®ƒåŒ…æ‹¬æ ‡æ³¨çš„å¯¹è¯å’ŒçŸ¥è¯†ç‰‡æ®µã€‚ç¡®ä¿æ‚¨çš„æ•°æ®é›†æœ‰æ˜Žç¡®çš„æ ‡ç­¾ï¼ŒæŒ‡ç¤ºä½•æ—¶éœ€è¦çŸ¥è¯†å¢žå¼ºã€‚\n4. **å¼€å‘çŸ¥è¯†æ£€ç´¢ç³»ç»Ÿ**ï¼šå®žçŽ°ä¸€ä¸ªçŸ¥è¯†æ£€ç´¢æœºåˆ¶ï¼Œä»¥ä¾¿åœ¨éœ€è¦æ—¶èŽ·å–ç›¸å…³çš„å¤–éƒ¨ä¿¡æ¯ã€‚å¯ä»¥è€ƒè™‘æµè¡Œçš„æŠ€æœ¯ï¼Œå¦‚ç¨ å¯†æ®µè½æ£€ç´¢æˆ–å›¾ç»“æž„çŸ¥è¯†åº“ã€‚\n5. **å®žçŽ° RAGate æœºåˆ¶**ï¼šåˆ›å»ºäºŒå…ƒçŸ¥è¯†é—¨å‡½æ•°ï¼ˆRAGateï¼‰ï¼Œä»¥ç¡®å®šä½•æ—¶ç”¨å¤–éƒ¨çŸ¥è¯†å¢žå¼ºå“åº”ã€‚è¿™æ¶‰åŠåˆ° **ä¸Šä¸‹æ–‡åˆ†æžå’Œé—¨æŽ§å‡½æ•°**ã€‚\n6. **æŽ¢ç´¢ RAGate å˜ä½“**ï¼šæ ¹æ®è®ºæ–‡ä¸­è®¨è®ºçš„æ–¹æ³•å¼€å‘ä¸åŒçš„ RAGate å˜ä½“ï¼š\n* **RAGate-Prompt**ï¼šä½¿ç”¨è‡ªç„¶è¯­è¨€æç¤ºä¸Žé¢„è®­ç»ƒè¯­è¨€æ¨¡åž‹æ¥ç¡®å®šæ˜¯å¦éœ€è¦å¢žå¼ºã€‚\n* **RAGate-PEFT**ï¼šé‡‡ç”¨å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯ï¼ˆä¾‹å¦‚ï¼ŒQLoRAï¼‰æ¥è®­ç»ƒæ‚¨çš„è¯­è¨€æ¨¡åž‹ï¼Œä»¥ä¾¿æ›´å¥½åœ°è¿›è¡Œå†³ç­–ã€‚\n* **RAGate-MHA**ï¼šåˆ©ç”¨å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶æ¥è¯„ä¼°ä¸Šä¸‹æ–‡å¹¶äº¤äº’å¼åœ°æ£€ç´¢çŸ¥è¯†ã€‚\n\n7. **è®­ç»ƒæ¨¡åž‹**ï¼šä½¿ç”¨æ ‡æ³¨æ•°æ®é›†å¾®è°ƒæ‚¨çš„ LLMï¼Œé‡‡ç”¨å„ç§ RAGate å˜ä½“ã€‚ç»“åˆé—¨æŽ§æœºåˆ¶çš„è®­ç»ƒï¼Œä»¥å¢žå¼ºæ¨¡åž‹æœ‰æ•ˆé¢„æµ‹çŸ¥è¯†å¢žå¼ºéœ€æ±‚çš„èƒ½åŠ›ã€‚\n\n8. **è¯„ä¼°æ€§èƒ½**ï¼šè¿›è¡Œå¹¿æ³›çš„å®žéªŒä»¥éªŒè¯ RAGate çš„æœ‰æ•ˆæ€§ã€‚åˆ†æžæŒ‡æ ‡å¦‚ï¼š\n\n* **ç²¾ç¡®åº¦ã€å¬å›žçŽ‡ã€F1 åˆ†æ•°**ï¼šè¯„ä¼°é—¨æŽ§å‡½æ•°çš„åˆ†ç±»æ€§èƒ½ã€‚\n* **BLEUã€ROUGEã€BERTScore**ï¼šç”¨äºŽè¯„ä¼°ç”Ÿæˆå“åº”ä¸ŽçœŸå®žå€¼çš„è´¨é‡ã€‚\n* **ç½®ä¿¡åˆ†æ•°**ï¼šæµ‹é‡ç”Ÿæˆè¾“å‡ºçš„ç½®ä¿¡åº¦ï¼Œä»¥ç¡®ä¿é«˜è´¨é‡çš„å“åº”ã€‚\n\n9. **éƒ¨ç½²ç³»ç»Ÿ**ï¼šå°† RAGate å¯ç”¨çš„å¯¹è¯ç³»ç»Ÿé›†æˆåˆ°æ‚¨çš„åº”ç”¨æˆ–æœåŠ¡ä¸­ã€‚ç¡®ä¿ç³»ç»Ÿèƒ½å¤Ÿå¤„ç†å®žæ—¶æŸ¥è¯¢ï¼Œå¹¶åŠ¨æ€å†³å®šçŸ¥è¯†å¢žå¼ºã€‚\n\n10. **è¿­ä»£å’Œæ”¹è¿›**ï¼šæŒç»­æ”¶é›†ç”¨æˆ·åé¦ˆå’Œäº¤äº’æ•°æ®ï¼Œä»¥ä¼˜åŒ–æ¨¡åž‹ã€‚åˆ†æžç³»ç»Ÿåœ¨ä¸Šä¸‹æ–‡æˆ–ç›¸å…³æ€§æ–¹é¢å¯èƒ½é‡åˆ°çš„å›°éš¾ï¼Œå¹¶ç›¸åº”è°ƒæ•´è®­ç»ƒæˆ–æ£€ç´¢æœºåˆ¶ã€‚\n\n## æ”¶èŽ·\n\næ€»ä¹‹ï¼ŒRAGate ä»£è¡¨äº†å¯¹è¯å¼äººå·¥æ™ºèƒ½çš„é‡å¤§è¿›æ­¥ï¼Œé€šè¿‡æ™ºèƒ½åœ°å¹³è¡¡å†…éƒ¨å’Œå¤–éƒ¨çŸ¥è¯†ï¼Œæä¾›æ›´ç›¸å…³ã€é«˜æ•ˆå’Œä¸ªæ€§åŒ–çš„å“åº”ã€‚RAGate çš„åº”ç”¨èŒƒå›´å¹¿æ³›ï¼Œæ¶µç›–äº†åŒ»ç–—ã€å®¢æˆ·æ”¯æŒã€æ•™è‚²ã€æ³•å¾‹æœåŠ¡ã€é‡‘èžç­‰å¤šä¸ªè¡Œä¸šã€‚é€šè¿‡å¢žå¼ºäººå·¥æ™ºèƒ½æä¾›é‡èº«å®šåˆ¶çš„å®žæ—¶ä¿¡æ¯çš„èƒ½åŠ›ï¼ŒRAGate æœ‰æ½œåŠ›å½»åº•æ”¹å˜ä¼ä¸šå’Œä¸ªäººä¸ŽæŠ€æœ¯çš„äº’åŠ¨æ–¹å¼ï¼Œæé«˜å†³ç­–èƒ½åŠ›ã€ç”¨æˆ·ä½“éªŒå’Œæ•´ä½“ç³»ç»Ÿæ€§èƒ½ã€‚\n\n"},{"lang":"zh","group":"blog","slug":"blog/rbyf-qwen2-5-3b-instruct-is-damn-good-dcf443cacc63","frontmatter":{"title":"RBYFï¼šQwen2.5â€“3B-instruct éžå¸¸æ£’ã€‚","meta_title":"RBYFï¼šQwen2.5â€“3B-instruct éžå¸¸æ£’ã€‚","description":"ä¿®æ”¹åŽçš„åŸºå‡†æµ‹è¯•å¹¶é™„ä¸Šæ‚¨çš„åé¦ˆï¼šé˜¿é‡Œå·´å·´Qwençš„å…¨æ–°3Bæ¨¡åž‹æ˜¯ä¸€ä¸ªäº†ä¸èµ·çš„æ¨¡åž‹ï¼Œæˆ‘å¯ä»¥è¯æ˜Žè¿™ä¸€ç‚¹ï¼","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*NWaBtJ64TLUoUHv4F1qJpg.png","categories":["Programming","Technology","Science"],"author":"Rifx.Online","tags":["Qwen","NLP","multimodal","RBYF","evaluation"],"draft":false,"slug":"blog/rbyf-qwen2-5-3b-instruct-is-damn-good-dcf443cacc63"},"content":"\n### ä¿®è®¢åŸºå‡†ï¼šä»¥æ‚¨ä¸ºåé¦ˆçš„å…¨æ–°3Bæ¨¡åž‹æ¥è‡ªé˜¿é‡Œå·´å·´Qwenï¼Œæ˜¯ä¸ªä»¤äººæƒŠå¹çš„æ¨¡åž‹ï¼Œæˆ‘å¯ä»¥è¯æ˜Žè¿™ä¸€ç‚¹ï¼\n\n\n\næ¶ŒçŽ°å±žæ€§çš„é”™è§‰åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯è¯„ä¼°è¿™äº›æ¨¡åž‹æ‰€ä½¿ç”¨çš„æŒ‡æ ‡çš„äº§ç‰©ã€‚è¿™æ˜¯ä¸€ä¸ªäº‹å®žã€‚\n\nå‡ å‘¨å‰ï¼Œæˆ‘å†³å®šåšä¸€ä¸ªå°åå›ï¼Œæ”¾å¼ƒæ‰€æœ‰å®˜æ–¹åŸºå‡†ï¼Œå¼€å§‹è‡ªå·±åšåŸºå‡†æµ‹è¯•ï¼\n\nè¿™å°±æ˜¯è¿™ä¸ªå®Œå…¨è™šæž„çš„é¦–å­—æ¯ç¼©ç•¥è¯RBYFçš„æ„ä¹‰ï¼šä»¥æ‚¨ä¸ºåé¦ˆçš„ä¿®è®¢åŸºå‡†ã€‚å…¶åŸºæœ¬åŽŸåˆ™æ˜¯ï¼Œæ²¡æœ‰æ¯”æ‚¨æ›´å¥½çš„è¯„åˆ¤è€…æ¥éªŒè¯ä¸€ä¸ªå¤§åž‹è¯­è¨€æ¨¡åž‹çš„ä¼˜åŠ£ã€‚\n\nè€å®žè¯´ï¼Œæˆ‘ä¸“æ³¨äºŽå°åž‹è¯­è¨€æ¨¡åž‹ã€‚æˆ‘æ²¡æœ‰ä¸“ç”¨çš„GPUï¼Œè®¡ç®—èµ„æºæœ‰é™ã€‚ä½†æˆ‘åŒæ ·åŒæ„[LLMWareåå›åŽŸåˆ™ç¬¬ä¸€æ¡](https://readmedium.com/getting-work-done-with-genai-just-do-the-opposite-10-contrarian-rules-that-may-actually-work-634501602a27)ï¼š\n\nä½¿ç”¨å°æ¨¡åž‹ï¼Œè€Œä¸æ˜¯å¤§æ¨¡åž‹ã€‚\n\nåœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†å‘æ‚¨å±•ç¤ºæˆ‘å¯¹qwen2.5â€“3b-instructçš„è¯„ä¼°ç»“æžœã€‚ç»“æžœçœŸçš„å¾ˆå¥½ï¼\n\n> å…è´£å£°æ˜Žï¼šæ‰€æœ‰ä¸Žç»“æžœç›¸å…³çš„æç¤ºå‡å¯åœ¨æˆ‘çš„GitHubå­˜å‚¨åº“ä¸­æ‰¾åˆ°ï¼š\n\n## å°‘å³æ˜¯å¤š\n\nç¼©æ”¾æ³•åˆ™æè¿°äº†æ¨¡åž‹æ€§èƒ½å¦‚ä½•éšç€å‚æ•°å’Œè®­ç»ƒæ•°æ®æ•°é‡çš„å¢žåŠ è€Œæ”¹å–„ã€‚è¿™ä¸ªåŽŸåˆ™æŽ¨åŠ¨äº†å¯¹LLMæ–°èƒ½åŠ›çš„æŽ¢ç´¢ã€‚\n\n> ä»…ä»…é€šè¿‡å¢žåŠ æ¨¡åž‹çš„è§„æ¨¡ï¼Œæˆ‘ä»¬å¯ä»¥è§£é”æ–°çš„èƒ½åŠ›â€¦â€¦\n\nç¼©æ”¾æ³•åˆ™æè¿°äº†æ¨¡åž‹æ€§èƒ½ä¸Žå‚æ•°æ•°é‡å’Œè®­ç»ƒæ•°æ®ä¹‹é—´çš„å…³ç³»ã€‚éšç€æ¨¡åž‹å˜å¾—æ›´å¤§å¹¶åœ¨æ›´å¤šæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬æœŸå¾…å®ƒä»¬çš„æ€§èƒ½å¾—åˆ°æå‡ã€‚è¿™å¯¼è‡´äº†å¯¹è¶Šæ¥è¶Šå¤§LLMçš„æ— ä¼‘æ­¢è¿½æ±‚ï¼Œå¸Œæœ›èƒ½å¤Ÿè§£é”æ–°çš„èƒ½åŠ›ã€‚\n\næ¶ŒçŽ°å±žæ€§æ˜¯æŒ‡åœ¨å¤æ‚ç³»ç»Ÿä¸­ä¸ªä½“ç»„ä»¶ä¹‹é—´çš„ç›¸äº’ä½œç”¨ä¸­äº§ç”Ÿçš„å±žæ€§ã€‚é€šè¿‡å­¤ç«‹åœ°ç ”ç©¶ç»„ä»¶ï¼Œæ— æ³•é¢„æµ‹æˆ–ç†è§£è¿™äº›å±žæ€§ã€‚åœ¨LLMçš„æƒ…å†µä¸‹ï¼Œå¸Œæœ›éšç€è¿™äº›æ¨¡åž‹å˜å¾—æ›´å¤§å’Œæ›´å¤æ‚ï¼Œå®ƒä»¬ä¼šå±•çŽ°å‡ºæ„æƒ³ä¸åˆ°çš„æ–°èƒ½åŠ›ã€‚\n\nè¿™æ˜¯ä¸€ä¸ªç«¥è¯æ•…äº‹ã€‚\n\nåœ¨è¿‡åŽ»å‡ å‘¨ï¼Œæˆ‘ä»¬äº²çœ¼ç›®ç¹äº†ç»è¿‡è¿‡åº¦è®­ç»ƒå’Œç²¾å¿ƒç­–åˆ’çš„å°åž‹è¯­è¨€æ¨¡åž‹å¯ä»¥è¡¨çŽ°å¾—ä¸Žå®ƒä»¬çš„å¤§åž‹å…„å¼Ÿä¸€æ ·å¥½ã€‚è¿™å¯¹æ‰€è°“çš„æ¶ŒçŽ°èƒ½åŠ›æ˜¯ä¸€ä¸ªé‡å‡»ï¼Œåå‡»äº†ç¼©æ”¾æ³•åˆ™ã€‚Gemma2â€“2Bã€Qwen2.5â€“3Bï¼Œç”šè‡³æœ€æ–°çš„Llama3.2â€“3Béƒ½æ˜¯è¿œèƒœäºŽæ—§çš„SOTA 7Bæ¨¡åž‹çš„æ›´å¥½æ¨¡åž‹ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*-EjpdEky-Hf3WEQn.png)\n\n## Qwen2.5æ¨¡åž‹ç³»åˆ—\n\né˜¿é‡Œäº‘åœ¨ä¹æœˆä¸­æ—¬å‘å¸ƒäº†ä»–ä»¬çš„æ——èˆ°æ¨¡åž‹ç³»åˆ—Qwen2.5ã€‚\n\n> é˜¿é‡Œäº‘åœ¨Qwençš„é©å‘½æ€§æ—…ç¨‹ä¸­å†æ¬¡å±•çŽ°å‡ºå¼ºå¤§çš„åˆ›æ–°é¢†å¯¼åŠ›\n\nQwen2.5æ˜¯é˜¿é‡Œå·´å·´é›†å›¢Qwenå›¢é˜Ÿçš„å¤§åž‹è¯­è¨€æ¨¡åž‹å’Œå¤§åž‹å¤šæ¨¡æ€æ¨¡åž‹ç³»åˆ—ã€‚è¿™äº›è¯­è¨€æ¨¡åž‹å’Œå¤šæ¨¡æ€æ¨¡åž‹åœ¨å¤§è§„æ¨¡å¤šè¯­è¨€å’Œå¤šæ¨¡æ€æ•°æ®ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œå¹¶åœ¨é«˜è´¨é‡æ•°æ®ä¸Šè¿›è¡Œäº†åŽè®­ç»ƒï¼Œä»¥ä¾¿ä¸Žäººç±»åå¥½å¯¹é½ã€‚Qwenèƒ½å¤Ÿç†è§£è‡ªç„¶è¯­è¨€ã€ç”Ÿæˆæ–‡æœ¬ã€ç†è§£è§†è§‰ã€ç†è§£éŸ³é¢‘ã€ä½¿ç”¨å·¥å…·ã€è§’è‰²æ‰®æ¼”ã€ä½œä¸ºAIä»£ç†è¿›è¡Œæ“ä½œç­‰ã€‚\n\n**æ–°æ¬¾Qwen2.5çš„äº®ç‚¹åœ¨äºŽç»è¿‡ç²¾å¿ƒç­–åˆ’çš„è®­ç»ƒæ•°æ®é›†ã€‚** é€šè¿‡æ£€æŸ¥å°æ¨¡åž‹çš„æ€§èƒ½ï¼Œæ‚¨å¯ä»¥æ¸…æ¥šåœ°ç†è§£è¿™ä¸€ç‚¹ã€‚\n\nå¦‚æžœè¯¥ç³»åˆ—çš„å°åž‹è¯­è¨€æ¨¡åž‹è¡¨çŽ°è‰¯å¥½ï¼Œæ„å‘³ç€è®­ç»ƒå’Œæ•°æ®é›†ç»è¿‡é«˜åº¦ä¿®è®¢å’Œç­–åˆ’ã€‚\n\nä»¥ä¸‹æ˜¯ä¸€äº›æ•°å­—ï¼š\n\n* å¯†é›†åž‹ã€æ˜“äºŽä½¿ç”¨çš„è§£ç å™¨è¯­è¨€æ¨¡åž‹ï¼Œæä¾›0.5Bã€1.5Bã€3Bã€7Bã€14Bã€32Bå’Œ72Bå°ºå¯¸ï¼Œä»¥åŠåŸºç¡€å’ŒæŒ‡ä»¤å˜ä½“ã€‚\n* åœ¨æˆ‘ä»¬æœ€æ–°çš„å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œæ¶µç›–å¤šè¾¾18Tä¸ªæ ‡è®°ã€‚\n* åœ¨æŒ‡ä»¤è·Ÿéšã€ç”Ÿæˆé•¿æ–‡æœ¬ï¼ˆè¶…è¿‡8Kä¸ªæ ‡è®°ï¼‰ã€ç†è§£ç»“æž„åŒ–æ•°æ®ï¼ˆä¾‹å¦‚ï¼Œè¡¨æ ¼ï¼‰å’Œç”Ÿæˆç»“æž„åŒ–è¾“å‡ºï¼ˆå°¤å…¶æ˜¯JSONï¼‰æ–¹é¢æœ‰æ˜¾è‘—æ”¹è¿›ã€‚\n* å¯¹ç³»ç»Ÿæç¤ºçš„å¤šæ ·æ€§æ›´å…·éŸ§æ€§ï¼Œå¢žå¼ºäº†è§’è‰²æ‰®æ¼”çš„å®žçŽ°å’ŒèŠå¤©æœºå™¨äººçš„æ¡ä»¶è®¾ç½®ã€‚\n* æ”¯æŒçš„ä¸Šä¸‹æ–‡é•¿åº¦å¯è¾¾128Kä¸ªæ ‡è®°ï¼Œå¹¶ä¸”å¯ä»¥ç”Ÿæˆå¤šè¾¾8Kä¸ªæ ‡è®°ã€‚\n* æ”¯æŒè¶…è¿‡29ç§è¯­è¨€çš„å¤šè¯­è¨€ï¼ŒåŒ…æ‹¬ä¸­æ–‡ã€è‹±æ–‡ã€æ³•æ–‡ã€è¥¿ç­ç‰™æ–‡ã€è‘¡è„ç‰™æ–‡ã€å¾·æ–‡ã€æ„å¤§åˆ©æ–‡ã€ä¿„æ–‡ã€æ—¥æ–‡ã€éŸ©æ–‡ã€è¶Šå—æ–‡ã€æ³°æ–‡ã€é˜¿æ‹‰ä¼¯æ–‡ç­‰ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*3tnTS_UCImBRBDeKmFyjVQ.png)\n\n## Qwen2.5â€“3B-instruct\n\nå°½ç®¡ä¸ºäº†æŽ¨åŠ¨äººå·¥æ™ºèƒ½çš„å‘å±•ï¼Œæˆ‘ä»¬éœ€è¦é‡Œç¨‹ç¢‘å’Œæ›´å¥½çš„è¡¨çŽ°ï¼Œä½†ç”¨æˆ·ä½“éªŒå’Œä¸ªäººè§‚ç‚¹ä¸èƒ½è¢«è§†ä¸ºæ— å…³ç´§è¦ã€‚\n\næˆ‘è®¤ä¸ºï¼ŒæŽ¢ç´¢ä¸€äº›å¸¸ç”¨çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼ŒæŠ›å¼€èŠå¤©ä½“éªŒï¼Œæˆ‘ä»¬å¿…é¡»å…³æ³¨å›žå¤çš„è´¨é‡ã€‚è€Œæˆ‘ä»¬æ˜¯å”¯ä¸€æ‰€éœ€çš„åŸºå‡†ã€‚**æˆ‘ä»¬çš„ç”¨æˆ·ä½“éªŒæ˜¯ç†è§£æ¨¡åž‹å¥½åçš„æœ€ä½³æŒ‡æ ‡**ã€‚æ¨¡åž‹å¿…é¡»è¶³å¤Ÿå¯é ï¼Œä»¥ä¾¿åœ¨è‡ªåŠ¨åŒ–å·¥ä½œæµç¨‹ä¸­ä½¿ç”¨ã€‚\n\né¡ºä¾¿æä¸€ä¸‹ï¼Œæˆ‘å·²ç»è¿è¡Œäº†æˆ‘å†³å®šç§°ä¹‹ä¸º[RBYF â€” ä¿®è®¢åŸºå‡†ä¸Žæ‚¨åé¦ˆ](https://open.substack.com/pub/thepoorgpuguy/p/rbyf-is-here-revised-benchmarks-with?r=i78xo&utm_campaign=post&utm_medium=web)çš„é¡¹ç›®ï¼Œåœ¨[Qwen2.5â€“1.5b-instruct](https://ai.gopubby.com/qwen2-5-1-5b-the-future-of-mobile-ai-6bd5f29bbc84)ä¸Šï¼šæ‚¨å¯ä»¥é˜…è¯»è¯¦ç»†ä¿¡æ¯ã€‚åœ¨æ–‡ç« ä¸­ï¼Œæˆ‘è¿˜è§£é‡Šäº†å¦‚ä½•åˆ›å»ºæ‚¨çš„æµ‹è¯•åŸºå‡†ã€‚æ‰€æè¿°çš„æ–¹æ³•ä¸Žæˆ‘ç”¨äºŽQwen2.5â€“3Bçš„ç›¸åŒã€‚\n\nè®©æˆ‘ä»¬ä»Žæ‰€æœ‰ä»»åŠ¡çš„æ•´ä½“è¡¨çŽ°å¼€å§‹ã€‚è¯¥æ¨¡åž‹å·²ç”±æˆ‘è¯„ä¼°ï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹æ˜¯æˆ‘è‡ªå·±çš„åé¦ˆï¼‰ï¼ŒåŸºäºŽä»¥ä¸‹æ˜¾ç¤ºçš„å®šæ€§çŸ©é˜µã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*rdVHfCDWX9jlvtiq)\n\næ€»ä½“å¾—åˆ†ä¸º62/70 = 8.8\n\nå¥½çš„ï¼Œä½†Qwen2.5â€“3B-instructæ˜¯åŸºäºŽä»€ä¹ˆå¾—åˆ°äº†è¿™ä¸ªè¯„ä¼°åˆ†æ•°å‘¢ï¼Ÿ\n\n## æµ‹è¯•æ¦‚è¿°\n\nè¿™ä¸ªæƒ³æ³•æ˜¯è¿›è¡Œå…¬å¹³çš„ç”¨æˆ·åé¦ˆï¼Œè€Œä¸æ˜¯åŸºäºŽæ ‡å‡†åŸºå‡†å’Œæ¡†æž¶çš„è‡ªåŠ¨åŒ–åé¦ˆã€‚ä¸€ä¸ªå°åž‹è¯­è¨€æ¨¡åž‹èƒ½å¦æ»¡è¶³ç”¨æˆ·åœ¨ä¸»è¦è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸Šçš„æ„å›¾ï¼Ÿ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*wgngfGvSebeoH3YxTdvc8A.png)\n\næˆ‘ä»¬å¸Œæœ›éªŒè¯ç”¨æˆ·æ„å›¾å’Œå“åº”è´¨é‡ã€‚ä»¥ä¸‹æ˜¯æ¯ä¸ªä»»åŠ¡çš„è¯¦ç»†ä¿¡æ¯ï¼š\n\n### ä»‹ç»\n\néªŒè¯æ¨¡åž‹å¦‚ä½•å›žåº”åˆå§‹é—®å€™å¹¶è°ˆè®ºè‡ªèº«ã€‚\n\n### ä¸€å¥è¯è§£é‡Š\n\nç»¼åˆä¸Žæ€»ç»“ã€‚æœ€ç»ˆè¯„ä¼°çš„é‡ç‚¹åœ¨äºŽæ¨¡åž‹æ˜¯å¦èƒ½å¤Ÿå°†å›žå¤åŽ‹ç¼©ä¸ºä»…ä¸€å¥è¯ã€‚\n\n### ç”¨ä¸‰æ®µè¯è§£é‡Š\n\nç”¨æˆ·çš„æ„å›¾æ˜¯èŽ·å¾—å¯¹æ–‡æœ¬çš„æ™ºèƒ½è§£é‡Šï¼Œè¿™ç§è§£é‡Šå¿…é¡»é€‚åˆåˆ†æˆä¸‰æ®µã€‚SMLé€šå¸¸ä¼šå‘çŽ°è¿™å¾ˆå›°éš¾ï¼Œå› ä¸ºä»–ä»¬æ€»æ˜¯ä¼šæ·»åŠ ä¸€ä¸ªæ€»ç»“æ®µè½ã€‚\n\n### è¯´â€œæˆ‘å‡†å¤‡å¥½äº†â€ã€‚\n\nåœ¨ä¸€ä¸ªåŸºäºŽèŠå¤©å›žåˆçš„åº”ç”¨ä¸­ï¼Œéµå¾ªæŒ‡ä»¤çš„æ¨¡åž‹é€šå¸¸ä¼šè¢«è¦æ±‚é¦–å…ˆé˜…è¯»æä¾›çš„æ–‡æœ¬ï¼Œç„¶åŽè¿›è¡ŒæŸç§åˆ†æžã€‚é€šå¸¸æƒ…å†µä¸‹ï¼ŒSML æ— æ³•åšåˆ°è¿™ä¸€ç‚¹â€¦â€¦\n\n### æ€»ç»“\n\nåŸºæœ¬æ€»ç»“ï¼Œæ²¡æœ‰é™åˆ¶ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æƒ³è¯„ä¼°æ€»ç»“æ˜¯å¦‚ä½•åŸºäºŽæ–‡æœ¬çš„ï¼Œè€Œä¸æ˜¯å‡­ç©ºæé€ äº‹å®žã€‚\n\n### æ€»ç»“ä¸ºä¸¤å¥\n\nåŸºæœ¬æ€»ç»“ï¼Œé™åˆ¶ä¸ºä¸¤å¥è¯ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¸Œæœ›è¯„ä¼°æ€»ç»“æ˜¯å¦åŸºäºŽæ–‡æœ¬ï¼Œè€Œä¸æ˜¯è™šæž„çš„äº‹å®žï¼ŒåŒæ—¶ç¡®ä¿éµå®ˆä¸¤å¥çš„é™åˆ¶ã€‚\n\n### åˆ—å‡ºä¸‰ä¸ªä¸»è¦å…³é”®ç‚¹ â€” æ ¼å¼è¾“å‡º\n\n```python\nkey_points = [\n    \"SMLå¿…é¡»ä»¥ç‰¹å®šæ ¼å¼è¾“å‡ºã€‚\",\n    \"æ­¤æç¤ºè¦æ±‚åˆ›å»ºä¸€ä¸ªåŒ…å«3ä¸ªå…³é”®ç‚¹çš„åˆ—è¡¨ã€‚\",\n    \"è¾“å‡ºæ ¼å¼åº”ä¸ºPythonåˆ—è¡¨ã€‚\"\n]\n```\n\n### ç›®å½•\n\nè¿™ä¸ªä»»åŠ¡å¯¹è®¸å¤š SML æ¥è¯´ç›¸å½“å›°éš¾ã€‚è¯¥æç¤ºéœ€è¦ä¸€äº›è°ƒæ•´ï¼Œå¦åˆ™æ¨¡åž‹ä¼šè¿”å›žä¸€ä¸ª markdown è¡¨æ ¼ã€‚ç”¨æˆ·å¸Œæœ›æŒ‰ç…§æä¾›çš„æ–‡æ¡£ç»“æž„èŽ·å¾—ä¸€ä¸ªæœ‰åºçš„ä¸»é¢˜åˆ—è¡¨ã€‚\n\n### RAG\n\næ£€ç´¢å¢žå¼ºç”Ÿæˆï¼Œæ²¡æœ‰ä»»ä½•æ¡†æž¶ï¼ˆhaystackï¼ŒLangchainâ€¦ï¼‰ã€‚è¿™æ˜¯è¯­è¨€æ¨¡åž‹ä¸­æœ€å¸¸ç”¨çš„ä»»åŠ¡ä¹‹ä¸€ã€‚å›žå¤çš„è¯„ä¼°åŸºäºŽå¯¹æŒ‡ä»¤çš„ç†è§£èƒ½åŠ›ä»¥åŠç­”æ¡ˆä¸Žæ–‡æœ¬çš„ç›¸å…³æ€§ã€‚\n\n### çœŸå®žçš„ RAG\n\nè¿™æ˜¯ä¸€ä¸ªä¸Žæä¾›çš„ä¸Šä¸‹æ–‡å®Œå…¨æ— å…³çš„é—®é¢˜çš„ RAGã€‚æ¨¡åž‹å¿…é¡»å›žå¤â€œæ— æ³•å›žç­”â€ï¼Œè¿™æ„å‘³ç€å®ƒç†è§£äº†æŒ‡ä»¤ï¼Œå¹¶ä¸”æ²¡æœ‰ä½¿ç”¨ä»»ä½•å¤–éƒ¨çŸ¥è¯†æˆ–è™šæž„çš„ä¿¡æ¯ã€‚\n\n### ä»Žå‚è€ƒæ–‡çŒ®æ’°å†™å†…å®¹\n\nè¿™æ˜¯ä¸€ä¸ªåˆ›é€ æ€§çš„ä»»åŠ¡ã€‚ä½¿ç”¨å‚è€ƒæ–‡æœ¬ï¼ŒSMLå¿…é¡»æä¾›ä¸€ç¯‡æ–°çš„æ–‡ç« ã€‚\n\n### æå–5ä¸ªä¸»é¢˜\n\næ­¤ä»»åŠ¡çš„é‡ç‚¹æ˜¯éªŒè¯ï¼š\n\n* ç¡®ä¿æ­£å¥½æœ‰5ä¸ªä¸»é¢˜\n* å®ƒä»¬æ˜¯åŸºäºŽäº‹å®žçš„ï¼ˆæ²¡æœ‰è™šæž„ï¼‰\n\n### åˆ›æ„ï¼š1000å­—ç§‘å¹»æ•…äº‹\n\nå®Œå…¨åˆ›é€ æ€§çš„ä»»åŠ¡ã€‚å³ä½¿å¯¹äºŽæ›´å¤§çš„æ¨¡åž‹æ¥è¯´ï¼Œè¦ä¿æŒè¿žè´¯æ€§å¹¶äº§ç”Ÿä¸€ä¸ªç¬¦åˆå­—æ•°è¦æ±‚çš„å°æ•…äº‹ä¹Ÿæ˜¯éžå¸¸å›°éš¾çš„ã€‚\n\n### åæ€æç¤º\n\nåæ€æç¤ºæ—¨åœ¨éªŒè¯æ¨¡åž‹çš„é“¾å¼æŽ¨ç†è¿‡ç¨‹ã€‚è¾“å‡ºè¢«é™åˆ¶åœ¨ç‰¹æ®Šæ ‡ç­¾çš„å¼€å¤´/ç»“å°¾ã€‚é‡ç‚¹æ˜¯æŽ¨ç†å’Œä¸€è‡´çš„è¾“å‡ºç»“æž„ã€‚è¾“å‡ºå¿…é¡»æ˜“äºŽç”¨äºŽè¿›ä¸€æ­¥çš„ç»“æž„åŒ–æç¤ºæˆ–å¯è§†åŒ–ã€‚æ‚¨å¯ä»¥åœ¨æœ¬æ–‡ä¸­é˜…è¯»æ›´å¤šå†…å®¹ï¼š\n\n## è¯„ä¼°è¿‡ç¨‹\n\nåœ¨æ¯æ¬¡ç”Ÿæˆç»“æŸæ—¶ï¼Œç”¨æˆ·ä¼šè¢«è¦æ±‚ç”¨0åˆ°5çš„åˆ†æ•°æ¥è¯„ä¼°ç»“æžœã€‚**åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç”¨æˆ·å°±æ˜¯æˆ‘â€¦â€¦**\n\nè¿™ç§å®šæ€§åˆ†æžç¡®å®žè¾ƒä¸ºç®€å•ï¼Œå› æ­¤æ¯ä¸ªåˆ†æ•°éƒ½æœ‰ä¸€ä¸ªæè¿°ï¼Œç”¨æˆ·å¯ä»¥æ·»åŠ è¯„è®ºï¼ˆâ€œä¸€äº›é”™è¯¯çš„ä¿¡æ¯â€ï¼Œâ€œä¹Ÿè®¸æ›´æ”¹æç¤ºä¸­çš„æŽªè¾žä¼šæ›´å¥½â€ï¼‰\n\nè¿™é‡Œæ˜¯å¸¦æœ‰æè¿°çš„å®šæ€§çŸ©é˜µ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*H_Qx2UT1lxqy2BF4.png)\n\n## å¥½ä¸Žå â€” ç»†èŠ‚\n\næ€»ä½“è€Œè¨€ï¼ŒQwen2.5â€“3B-instruct æ˜¯ä¸€ä¸ªå…¨æ–¹ä½çš„æ¨¡åž‹ã€‚æ¯ä¸ªä»»åŠ¡çš„å¹³å‡å¾—åˆ†ä¸º 4.4 åˆ†ï¼ˆæ»¡åˆ† 5 åˆ†ï¼‰ï¼Œè¿™ç›¸å½“ä»¤äººå°è±¡æ·±åˆ»ã€‚\n\næ‘˜è¦è¡¨çŽ°å‡ºè‰²ã€‚åœ¨é•¿æ–‡æœ¬ä¸­åˆ—å‡ºä¸»è¦ä¸»é¢˜ä¹Ÿéžå¸¸å¥½ã€‚\n\nå®ƒåœ¨ RAG ä¸­è¡¨çŽ°çªå‡ºï¼Œæœ‰æ—¶æ˜¾å¾—è¿‡äºŽå†—é•¿ï¼šåœ¨çœŸå®žçš„ RAG ä¸­ï¼Œå®ƒè¿˜æä¾›äº†ä¸€äº›è§£é‡Šï¼Œä»¥åŠ *æ— æ³•å›žç­”* å›žå¤çš„åŽŸå› ã€‚\n\næˆ‘ä»¬å¯ä»¥ä¸€èµ·çœ‹çœ‹ã€‚è¿™å¯¹äºŽ SML æ¥è¯´ï¼Œç²¾ç¡®è®¡ç®—å¥å­å’Œæ®µè½çš„æ•°é‡é€šå¸¸æ˜¯ä¸€é¡¹è‰°å·¨çš„ä»»åŠ¡ã€‚\n\n```python\n**Write a two-sentences summary of the following text:**\n\nChina's regime under Chinese Communist Party operates without transparent \nelections and primary candidates. While undesirable for democracies, \nit allows long-term planning free from electoral pressures that can change \nplans abruptly; this has led to China achieving peak carbon dioxide emissions \nin 2023 due largely to increased renewable energy installations, potentially \nsetting a new global standard for sustainable development despite its status \nas one of the world's largest polluters.\n```\n\nç›®å½•å¯¹äºŽè¯­è¨€æ¨¡åž‹æ¥è¯´æ˜¯å¦ä¸€ä¸ªå™©æ¢¦ï¼Œå› ä¸ºå®ƒä»¬é€šå¸¸ç†è§£å¦‚ä½•åˆ›å»ºè¡¨æ ¼â€¦â€¦ä½†çœ‹çœ‹è¿™ä¸ªï¼š\n\n```python\nA \"table of content\" provides an organized structure to the text, allowing \nreaders quick access to key points. Write the \"table of content\" of \nthe following text.\n\n[table of contents]\n1. Introduction: Overview and Critique on China's Governance System  \n2. Comparison with Western Democracies - Undesirable Dictatorship? \n3. Advantages in Long-Term Planning for Sustainability   \n4. The Chinese Model as a Comparative Advantage  \n5. Recent Achievements in Carbon Emissions Reduction\n6. Transition to Renewable Energy: A Turning Point?\n7. Potential Leadership Role of China and India  \n8. Conclusion - Future Directions\n\n```\n\næˆ‘ç»™çœŸå®žçš„ RAG è¯„åˆ† 4 åˆ†ï¼ˆæ»¡åˆ† 5 åˆ†ï¼‰ï¼Œä½†å®ƒæœ¬å¯ä»¥è½»æ¾å¾—åˆ° 5 åˆ†ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œå›žå¤ä¸­å¿…é¡»åŒ…å« *æ— æ³•å›žç­”* è¿™ä¸ªè¯ã€‚\n\nä¸ºä»€ä¹ˆï¼Ÿæƒ³è±¡ä¸€ä¸‹ä½ åœ¨è®¾ç½®ä¸€ç³»åˆ—æç¤ºï¼Œä½ éœ€è¦æ ¹æ®å…ˆå‰çš„å›žå¤å†³å®šä¸‹ä¸€æ­¥ã€‚å¦‚æžœä½ å¯ä»¥ç®€å•åœ°å¯»æ‰¾ *æ— æ³•å›žç­”* è¿™ä¸ªè¯çš„å­˜åœ¨æ¥å¼•å¯¼ä½ çš„ä¸‹ä¸€æ­¥ï¼Œé‚£å°±æ²¡é—®é¢˜äº†ï¼\n\n```python\n**Reply to the question only using the provided context. If the answer \nis not contained in the provided context say \"unanswerable\".\nquestion: who is Anne Frank?**\n{context}\n**Remember: if you cannot answer based on the provided context, say \n\"unanswerable\"\nanswer:**\n\nUnanswerable\nThe given text does not provide any information about Anne Frank \nor her identity. The content is unrelated to this question and focuses \ninstead on discussing China's system compared to Western democracies in \nterms of long-term planning for environmental issues.\n```\n\næˆ‘æƒ³è±¡æ¨¡åž‹ä¹Ÿè¢«è®­ç»ƒæ¥æä¾›è¿™ç§å›žå¤çš„åŽŸå› â€¦â€¦å¤ªæ£’äº†ï¼\n\n### ä¼˜ç§€çš„åæ€\n\nåæ€æç¤ºçš„ä¸»è¦ç„¦ç‚¹æ˜¯æ¨¡åž‹è¿”å›žæ­£ç¡®æŽ¨ç†è¿‡ç¨‹çš„èƒ½åŠ›ï¼Œå¹¶æŒ‰ç…§å‡ ä¸ªç‰¹å®šçš„æ ‡ç­¾å¼€é—­æ ¼å¼åŒ–è¾“å‡ºã€‚\n\nåˆ°ç›®å‰ä¸ºæ­¢ï¼Œè¿™æ˜¯**å°åž‹è¯­è¨€æ¨¡åž‹**ä¸­**æœ€ä½³çš„åæ€è¾“å‡º**ï¼Œç”šè‡³æ¯”Gemma2â€“2Bæ›´å¥½ï¼š\n\n```python\n*ðŸ‘¨â€ðŸ’» > You are an AI assistant designed to provide detailed, step-by-step \nresponses. Your outputs should follow this structure:\n1. Begin with a <thinking> section.\n2. Inside the thinking section:\n   a. Briefly analyze the question and outline your approach.\n   b. Present a clear plan of steps to solve the problem.\n   c. Use a \"Chain of Thought\" reasoning process if necessary, breaking down \nyour thought process into numbered steps.\n3. Include a <reflection> section for each idea where you:\n   a. Review your reasoning.\n   b. Check for potential errors or oversights.\n   c. Confirm or adjust your conclusion if necessary.\n4. Be sure to close all reflection sections.\n5. Close the thinking section with </thinking>.\n6. Provide your final answer in an <output> section.\nAlways use these tags in your responses. Be thorough in your explanations, \nshowing each step of your reasoning process. Aim to be precise and logical \nin your approach, and don't hesitate to break down complex problems into \nsimpler components. Your tone should be analytical and slightly formal, \nfocusing on clear communication of your thought process.\nRemember: Both <thinking> and <reflection> MUST be tags and must be closed \nat their conclusion\nMake sure all <tags> are on separate lines with no other text. \nDo not include other text on a line containing a tag.\n\nuser question: explain why it is crucial for teachers to learn how to use \ngenerative AI for their job and for the future of education. \nInclude relevant learning path for teachers and educators. *\n\n**ðŸ’» > <thinking>\nI will begin by analyzing the importance of teaching about Generative \nArtificial Intelligence (AI) in an educational context, then outline a \nstep-by-step plan on what steps should be taken to learn this technology \neffectively as well as provide resources or courses that could help.\nFirstly, understanding AI is crucial for teachers because it can \nsignificantly enhance their ability to support students. \nTeachers need the knowledge and skills necessary not only to integrate these tools into teaching but also understand how they work so they are able to guide both themselves and their pupils in ethical use of technology.\n</thinking>\n\n<reflection>\nMy initial thought process was clear, emphasizing that teachers should \nbe equipped with AI literacy as it can improve educational outcomes. \nI outlined the importance for guiding students ethically while using \nthese tools effectively.\nHowever, my plan needs more detail on specific steps to learn about \ngenerative AI and resources available online or in-person courses.\n</reflection>**\n*MY PERSONAL NOTE: here it is starting an additional thinking after the first \nreflection... And this is really too good to be true!!*\n**<thinking>\nTo create a comprehensive learning path that teachers could follow:\n1. **Understand Basic Concepts**: Start with basic concepts of machine \n   learning (ML) such as supervised vs unsupervised, deep neural networks  \n   etc., which are foundational to understanding generative AI.\n2. **Learn About Generative Models**:\n   - Study different types: Variational Autoencoders (VAEs), GANs.\n   - Understand how these models generate new data that resembles the \n     training set but is not identical and can be used for tasks like \n     text generation, image synthesis etc., which are crucial in education.\n3. **Explore Applications**: Look at real-world applications of generative \n   AI such as language modeling (e.g., GPT-2), computer vision techniques to \n   create realistic images or videos from raw data.\n4. **Ethical Considerations and Implications for Education**:\n   - Discuss the ethical implications, including bias in models' outputs \n     which could affect studentsâ€™ perceptions about themselves if they are \n     exposed regularly through AI-generated content that is not representative \n     of them.\n5. **Hands-on Practice**: Engage with practical exercises using open-source \n   tools like TensorFlow or PyTorch to build simple generative model \n   applications.\n6. **Stay Updated and Network**:\n   - Follow the latest research in ML, especially on Generative Models \n     by following relevant journals (e.g., NeurIPS), conferences such as \n     ICML etc.\n7. Attend Workshops/Conferences: Participate actively at AI-related workshops \n   or events to network with professionals working within this field.\n</thinking>**\n*MY PERSONAL NOTE: and no surprise, an additional reflection now starts, \nbefore producing the final output*\n**<reflection>\nMy plan has been expanded and includes a more detailed approach for teachers, \ncovering foundational knowledge through practical application of generative \nmodels in education along with ethical considerations that are crucial. \nI have also included staying updated on the latest research as an important \npart.\nHowever, it's essential to ensure these resources or courses exist so \neducators can access them easily.\n</reflection>\n\n<output>\nIt is vital for teachers and educational professionals alike to learn how \ngenerative AI works because this technology has significant potential in \nenhancing teaching methods. By understanding basic concepts of machine \nlearning (ML) such as supervised vs unsupervised, deep neural networks etc., \nthey will be better equipped with the knowledge necessary not only to \nintegrate these tools into their curriculum but also guide students ethically.\nA comprehensive plan for teachers and educators includes:\n1. **Understand Basic Concepts**: Start by grasping foundational concepts of ML.\n2. **Learn About Generative Models**:\n   - Study different types: Variational Autoencoders (VAEs), GANs, etc., \n     which are crucial in generating new data that resembles the training \n     set but is not identical and can be used for tasks like text generation or \n     image synthesis.\n3. **Explore Applications**: Look at real-world applications of generative AI \n   such as language modeling using tools like TensorFlow.\n4. **Ethical Considerations**:\n   - Discuss ethical implications, including bias in models' outputs which \n     could affect studentsâ€™ perceptions about themselves if they are exposed \n     regularly through AI-generated content that is not representative.\n5. **Hands-on Practice**: Engage with practical exercises to build simple \n   generative model applications using open-source tools.\n6. **Stay Updated and Network**:\n   - Follow the latest research in ML, especially on Generative Models by \n     following relevant journals (e.g., NeurIPS), conferences such as ICML etc.\n7. Attend Workshops/Conferences: Participate actively at AI-related workshops \n   or events to network with professionals working within this field.\n</output>\n```\n\nè¿™æ ·çš„ç²¾ç‚¼æ€ç»´è¿‡ç¨‹ï¼Œä»¥åŠæ¸…æ™°çš„æ ‡ç­¾å¼€é—­æ ¼å¼ï¼Œå¯ä»¥æ–¹ä¾¿åŽç»­çš„å¤„ç†æµç¨‹ï¼Œæˆ–è€…åœ¨æ¼‚äº®çš„GUIä¸­è¿›è¡Œç¾Žè§‚çš„æ‰“å°ã€‚\n\n## å®ƒä¸æ“…é•¿ä»€ä¹ˆï¼Ÿ\n\næœ€ç³Ÿç³•çš„ä»»åŠ¡æ˜¯åˆ›ä½œçŸ­ç¯‡å°è¯´ã€‚æ¨¡åž‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å¼€å§‹é‡å¤ç›¸åŒçš„æ®µè½ã€‚\n\nè¿™å¹¶ä¸æ„å‘³ç€å®ƒæ— æ³•åšåˆ°ã€‚æ›´æœ‰å¯èƒ½çš„æ˜¯ï¼Œé€šè¿‡æé«˜æ¸©åº¦å’Œé‡å¤æƒ©ç½šï¼Œå¯ä»¥èŽ·å¾—è‰¯å¥½çš„ç»“æžœã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GNJ3lG6FM5qt9glIdf7qhQ.jpeg)\n\n## ç»“è®º\n\næ‚¨å¯ä»¥åœ¨æˆ‘çš„GitHubä»“åº“ä¸­æ‰¾åˆ°æ‰€æœ‰èŠå¤©è®°å½•ï¼Œä»¥åŠä»£ç å’Œè‡ªè¡Œæ“ä½œçš„è¯´æ˜Žã€‚æ‚¨å¯ä»¥å‚è€ƒæˆ‘ä¹‹å‰æ–‡ç« ä¸­çš„æ•™ç¨‹ [Qwen2.5 1.5b: ç§»åŠ¨AIçš„æœªæ¥ï¼Ÿ](https://ai.gopubby.com/qwen2-5-1-5b-the-future-of-mobile-ai-6bd5f29bbc84)\n\nåœ¨æŽ¥ä¸‹æ¥çš„æ–‡ç« ä¸­ï¼Œæˆ‘å°†ä»‹ç»å…¶ä»–å°åž‹è¯­è¨€æ¨¡åž‹ï¼Œä½¿ç”¨ç›¸åŒçš„åŽŸåˆ™ï¼šä»Žå°åž‹çš„350Må‚æ•°ï¼Œåˆ°500Mç³»åˆ—ï¼Œå†åˆ°3B â€” ç»è¿‡1.5Bã€‚\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/retrieval-augmented-generation-approaches-state-of-the-art-and-optimization-strategies-456883da4801","frontmatter":{"title":"æ£€ç´¢å¢žå¼ºç”Ÿæˆï¼šæ–¹æ³•ã€æœ€æ–°è¿›å±•å’Œä¼˜åŒ–ç­–ç•¥","meta_title":"æ£€ç´¢å¢žå¼ºç”Ÿæˆï¼šæ–¹æ³•ã€æœ€æ–°è¿›å±•å’Œä¼˜åŒ–ç­–ç•¥","description":"â­ RAG åœ¨çŸ¥è¯†å¯†é›†åž‹åœºæ™¯æˆ–éœ€è¦æŒç»­çŸ¥è¯†çš„ç‰¹å®šé¢†åŸŸåº”ç”¨ä¸­ç‰¹åˆ«æœ‰ç”¨â€¦â€¦","date":"2024-10-31T08:17:32.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*_vE7WktGmyQ5xg_t5cpFVg.jpeg","categories":["Generative AI","Natural Language Processing","Machine Learning"],"author":"Rifx.Online","tags":["RAG","retrieval","generation","optimization","embeddings"],"draft":false,"slug":"blog/retrieval-augmented-generation-approaches-state-of-the-art-and-optimization-strategies-456883da4801"},"content":"\n\n\n\n\nâ­ RAG åœ¨çŸ¥è¯†å¯†é›†åž‹åœºæ™¯æˆ–éœ€è¦æŒç»­æ›´æ–°çŸ¥è¯†çš„ç‰¹å®šé¢†åŸŸåº”ç”¨ä¸­å°¤å…¶æœ‰ç”¨ã€‚æœ€è¿‘ï¼ŒRAG å› å…¶åœ¨å¯¹è¯ä»£ç†ä¸­çš„åº”ç”¨è€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚\n\nðŸ“Œ å‚è€ƒç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å½“å‰çš„ RAG æ–¹æ³•åŠå…¶ä¸åŒç»„ä»¶ã€æœ€æ–°è¿›å±•ï¼ˆSOTAï¼‰ã€åº”ç”¨ã€æ£€ç´¢ã€ç”Ÿæˆã€å¢žå¼ºæŠ€æœ¯çš„è¯„ä¼°ä¸Šã€‚\n\néšç€ RAG ç³»ç»Ÿä»Žç®€å•åˆ°é«˜çº§å†åˆ°æ¨¡å—åŒ–çš„æ¼”å˜ï¼Œæ¯ä¸ªé˜¶æ®µéƒ½æ˜¯ä¸ºäº†åº”å¯¹ç‰¹å®šç”¨ä¾‹çš„å¢žå¼ºè€Œå‡ºçŽ°çš„ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*P2ByKtayhF4XgAVxRI1urQ.jpeg)\n\nâœ *ç®€å•åž‹*ï¼šç”¨æˆ·è¾“å…¥ç”¨äºŽæ–‡æ¡£æŸ¥è¯¢ï¼Œé™„åŠ /ç»„åˆåˆ°æç¤ºä¸­ï¼Œç”¨äºŽæ¨¡åž‹æœ€ç»ˆå“åº”ç”Ÿæˆã€‚é€šè¿‡å¤šè½®å¯¹è¯äº¤äº’ï¼Œå¯ä»¥å°†ä¸Šä¸‹æ–‡/å¯¹è¯åŽ†å²æ·»åŠ /ç»„åˆåˆ°æç¤ºä¸­ã€‚ç¼ºç‚¹ï¼šä½Žç²¾åº¦/ä½Žå¬å›žçŽ‡ï¼Œå†—ä½™ï¼Œé‡å¤ã€‚\n\nâœ *é«˜çº§åž‹*ï¼šé€šè¿‡ä¼˜åŒ–é¢„æ£€ç´¢ã€æ£€ç´¢å’ŒåŽæ£€ç´¢æ–¹æ³•æ¥æé«˜æ£€ç´¢è´¨é‡ã€‚é¢„æ£€ç´¢ä¸­ï¼Œé€šè¿‡å¢žå¼ºæ•°æ®ç²’åº¦ã€ç´¢å¼•ç»“æž„æ”¹è¿›ã€å…ƒæ•°æ®ã€å¯¹é½å’Œæ··åˆæ£€ç´¢æ¥æå‡è´¨é‡ã€‚åœ¨æ£€ç´¢ä¸­ï¼Œä¼˜åŒ–åµŒå…¥æ¨¡åž‹ï¼Œä»Žè€Œä¼˜åŒ–ä¸Šä¸‹æ–‡ã€‚åœ¨åŽæ£€ç´¢ä¸­ï¼Œä¼˜åŒ–ä¸Šä¸‹æ–‡çª—å£å’Œå™ªå£°/å¹²æ‰°æ•°æ®çš„æ‹’ç»ã€‚\n\nâœ *æ¨¡å—åŒ–*ï¼šå¼•å…¥æœç´¢æ¨¡å—è¿›è¡Œç›¸ä¼¼æ€§æ£€ç´¢å’Œæ£€ç´¢çš„å¾®è°ƒã€‚æ–°æ¨¡å—åŒ…æ‹¬æœç´¢ã€è®°å¿†ã€èžåˆã€è·¯ç”±ã€é¢„æµ‹ã€ä»»åŠ¡é€‚é…å™¨ã€‚\n\nðŸ¥‰ ä¼˜åŒ– RAG æµæ°´çº¿ï¼š\n\nðŸ“œ *æ··åˆæœç´¢æŽ¢ç´¢*ï¼šé€šè¿‡æ™ºèƒ½åˆ©ç”¨å…³é”®å­—æœç´¢ã€è¯­ä¹‰æœç´¢å’Œå‘é‡æœç´¢ç­‰æŠ€æœ¯ï¼Œå¹³è¡¡æ€§èƒ½ä¼˜åŒ–ã€‚\n\nðŸ“œ *é€’å½’æ£€ç´¢å’ŒæŸ¥è¯¢å¼•æ“Ž*ï¼šåˆå§‹é˜¶æ®µå¯èƒ½é€šè¿‡èŽ·å–è¾ƒå°çš„å—å¼€å§‹æ£€ç´¢ï¼ŒéšåŽä»¥æ›´å¥½ä¸”æ›´å…·ä¸Šä¸‹æ–‡çš„ä¿¡æ¯èŽ·å–è¾ƒå¤§çš„å—ï¼Œä»¥å¹³è¡¡ä¸Šä¸‹æ–‡ä¸°å¯Œçš„å“åº”ä¸Žæ•ˆçŽ‡ã€‚\n\nðŸ“œ *å›žé€€æç¤º*ï¼šè¿™é¼“åŠ± LLM è„±ç¦»ç‰¹å®šå®žä¾‹ï¼Œå›´ç»•æ›´å¹¿æ³›çš„æ¦‚å¿µå’ŒåŽŸåˆ™è¿›è¡ŒæŽ¨ç†ï¼ˆarXiv:2310\\.13243ï¼‰ã€‚åœ¨å„ç§å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºäºŽæŽ¨ç†çš„ä»»åŠ¡ä¸­ï¼Œä½¿ç”¨å›žé€€æç¤ºæ—¶è§‚å¯Ÿåˆ°æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œçªæ˜¾äº†å®ƒä»¬å¯¹ RAG è¿‡ç¨‹çš„è‡ªç„¶é€‚åº”æ€§ã€‚\n\nðŸ“œ *å­æŸ¥è¯¢*ï¼šæ ¹æ®åœºæ™¯åº”ç”¨çš„æŸ¥è¯¢ç­–ç•¥å¯ä»¥ä½¿ç”¨ï¼Œä¾‹å¦‚åˆ©ç”¨ LlamaIndex æä¾›çš„æŸ¥è¯¢å¼•æ“Žï¼Œåˆ©ç”¨æ ‘æŸ¥è¯¢ï¼Œä½¿ç”¨å‘é‡æŸ¥è¯¢ï¼Œæˆ–æ‰§è¡Œç®€å•çš„å—é¡ºåºæŸ¥è¯¢ã€‚\n\nðŸ“œ *å‡è®¾æ–‡æ¡£åµŒå…¥*ï¼šä½¿ç”¨ LLMï¼ŒHyDE é€šè¿‡åˆ›å»ºå‡è®¾ç­”æ¡ˆæ¥å“åº”æŸ¥è¯¢ï¼ŒåµŒå…¥ç­”æ¡ˆï¼Œå¹¶ä½¿ç”¨ç›¸åŒçš„ç­”æ¡ˆæ¥æ£€ç´¢çœŸå®žæ–‡æ¡£ã€‚è¯¥æ–¹æ³•å…³æ³¨çš„æ˜¯ä»Žä¸€ä¸ªç­”æ¡ˆåˆ°å¦ä¸€ä¸ªç­”æ¡ˆçš„åµŒå…¥ç›¸ä¼¼æ€§ï¼Œè€Œä¸æ˜¯åŸºäºŽæŸ¥è¯¢å¯»æ‰¾åµŒå…¥ç›¸ä¼¼æ€§\\[arXiv:2212\\.10496]ã€‚ç¼ºç‚¹ï¼šä¸ä¸€è‡´çš„ç­”æ¡ˆæœªèƒ½äº§ç”Ÿç†æƒ³ç»“æžœï¼ŒLLM æœªè§ä¸»é¢˜çš„é”™è¯¯ï¼Œå¯¼è‡´é”™è¯¯ã€‚\n\nè®©æˆ‘åœ¨è¿™é‡Œç»“æŸã€‚æˆ‘ä¼šåœ¨åŽç»­ä¸­å‘å¸ƒæ–°æ–‡ç« ã€‚\n\n[\\#genai](https://www.linkedin.com/feed/hashtag/?keywords=genai&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7170160104984571905) [\\#rag](https://www.linkedin.com/feed/hashtag/?keywords=rag&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7170160104984571905) \\#ai \\#llm\n\nå‚è€ƒï¼š[arxiv:2312\\.10997](https://arxiv.org/pdf/2312.10997)ï¼ŒRAG è°ƒæŸ¥ï¼ŒHuggingfaceblogs\n\n"},{"lang":"zh","group":"blog","slug":"blog/roadmap-to-become-an-ai-engineer-in-2025-4323ae4c2f5c","frontmatter":{"title":"2025 å¹´æˆä¸ºäººå·¥æ™ºèƒ½å·¥ç¨‹å¸ˆçš„è·¯çº¿å›¾","meta_title":"2025 å¹´æˆä¸ºäººå·¥æ™ºèƒ½å·¥ç¨‹å¸ˆçš„è·¯çº¿å›¾","description":"æœ¬æ–‡æä¾›äº†ä¸€æ¡ç»“æž„åŒ–çš„è·¯çº¿å›¾ï¼ŒæŒ‡å¯¼å¦‚ä½•åœ¨2025å¹´æˆä¸ºAIå·¥ç¨‹å¸ˆã€‚é¦–å…ˆï¼Œç†è§£AIå·¥ç¨‹çš„åŸºæœ¬æ¦‚å¿µå’ŒèŒè´£ï¼Œç„¶åŽæŽŒæ¡æ•°å­¦ã€ç¼–ç¨‹ï¼ˆç‰¹åˆ«æ˜¯Pythonï¼‰å’Œæ•°æ®ç§‘å­¦çš„åŸºç¡€çŸ¥è¯†ã€‚æŽ¥ç€ï¼Œæ·±å…¥å­¦ä¹ æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ ï¼Œå‚ä¸Žå®žé™…é¡¹ç›®ä»¥å·©å›ºæ‰€å­¦çŸ¥è¯†ï¼Œå¹¶æŽŒæ¡æ¨¡åž‹éƒ¨ç½²æŠ€èƒ½ã€‚æœ€åŽï¼Œå»ºç«‹ä½œå“é›†å’Œäººè„‰ç½‘ç»œï¼Œæå‡èŒä¸šå‘å±•æœºä¼šã€‚æ–‡ç« å¼ºè°ƒäº†æŒç»­å­¦ä¹ å’Œå®žè·µçš„é‡è¦æ€§ï¼Œä»¥é€‚åº”å¿«é€Ÿå‘å±•çš„AIé¢†åŸŸã€‚","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*emg2RJ9qx4OgysH2r22nuQ.png","categories":["Programming","Machine Learning","Data Science"],"author":"Rifx.Online","tags":["Python","machine-learning","deep-learning","data-science","statistics"],"draft":false,"slug":"blog/roadmap-to-become-an-ai-engineer-in-2025-4323ae4c2f5c"},"content":"\n\n\n### å¦‚ä½•åœ¨2025å¹´æˆä¸ºAIå·¥ç¨‹å¸ˆ\n\n\n\næœ‰æ²¡æœ‰æƒ³è¿‡æž„å»ºèƒ½å¤Ÿæ€è€ƒã€å­¦ä¹ å’Œè§£å†³å¤æ‚é—®é¢˜çš„ç³»ç»Ÿéœ€è¦ä»€ä¹ˆï¼Ÿå‡ å¹´å‰ï¼Œæˆ‘ä¹Ÿå¾ˆå¥½å¥‡â€”â€”AIæ˜¯ä¸€ä¸ªæœªæ¥ä¸»ä¹‰çš„æ¦‚å¿µï¼Œæˆ‘ä¸çŸ¥é“ä»Žå“ªé‡Œå¼€å§‹ã€‚çŽ°åœ¨ï¼Œéšç€æˆ‘ä»¬è¿ˆå…¥2025å¹´ï¼Œæˆä¸ºAIå·¥ç¨‹å¸ˆæ¯”ä»¥å¾€ä»»ä½•æ—¶å€™éƒ½æ›´åŠ å®¹æ˜“ã€‚å¦‚æžœä½ åœ¨è¿™é‡Œï¼Œä½ å¯èƒ½å¯¹å¦‚ä½•ä»Žé›¶å¼€å§‹è¿›å…¥è¿™ä¸ªé¢†åŸŸæ„Ÿå…´è¶£ã€‚å¥½æ¶ˆæ¯æ˜¯ï¼Ÿä¸éœ€è¦æˆä¸ºè®¡ç®—æœºå¤©æ‰æˆ–æ•°å­¦å¥‡æ‰ã€‚åªéœ€ä¸€ä¸ªæ¸…æ™°çš„è·¯çº¿å›¾ã€å¥‰çŒ®ç²¾ç¥žå’Œæ­£ç¡®çš„èµ„æºï¼Œä½ å°±èƒ½åšåˆ°ã€‚\n\nåœ¨è¿™æœ¬ç»ˆæžæŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æŽ¢è®¨ä½ éœ€è¦çš„æ¯ä¸€æ­¥ã€æ¯é¡¹æŠ€èƒ½å’Œæ¯ä¸ªèµ„æºï¼Œä»¥å°†è‡ªå·±è½¬å˜ä¸ºAIå·¥ç¨‹å¸ˆã€‚æ— è®ºä½ æ˜¯ä»Žå¤´å¼€å§‹è¿˜æ˜¯å·²ç»å…·å¤‡ä¸€äº›æŠ€æœ¯çŸ¥è¯†ï¼Œè¿™æœ¬æŒ‡å—éƒ½ä¼šå°†ä¸€åˆ‡åˆ†è§£ä¸ºå¯ç®¡ç†çš„æ­¥éª¤ã€‚é‚£ä¹ˆï¼Œè®©æˆ‘ä»¬å¼€å§‹å§ã€‚\n\n## 1\\. ç†è§£äººå·¥æ™ºèƒ½å·¥ç¨‹çš„å®žé™…å«ä¹‰\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*LqwczRy-BZOH-zhfxNjc2g.png)\n\n### ä»€ä¹ˆæ˜¯ AI å·¥ç¨‹ï¼Ÿ\n\nAI å·¥ç¨‹æ—¨åœ¨è®¾è®¡å’Œéƒ¨ç½²èƒ½å¤Ÿè§£å†³çŽ°å®žä¸–ç•Œé—®é¢˜çš„ AI æ¨¡åž‹ã€‚ä»Žè‡ªåŠ¨é©¾é©¶æ±½è½¦åˆ°ä¸ªæ€§åŒ–æŽ¨èï¼ŒAI å·¥ç¨‹å¸ˆåˆ›å»ºèƒ½å¤Ÿä»Žæ•°æ®ä¸­å­¦ä¹ å¹¶åšå‡ºæ™ºèƒ½å†³ç­–çš„ç³»ç»Ÿã€‚\n\n### AIå·¥ç¨‹å¸ˆçš„è§’è‰²ä¸ŽèŒè´£\n\n* **å¼€å‘æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰æ¨¡åž‹**ä»¥è¿›è¡Œé¢„æµ‹å’Œæ•°æ®æ´žå¯Ÿ\n* **ç¼–ç¨‹å’Œè½¯ä»¶å¼€å‘**ä¸“æ³¨äºŽAIåº”ç”¨\n* **æ•°æ®æ”¶é›†å’Œé¢„å¤„ç†**ä¸ºAIæ¨¡åž‹åˆ›å»ºåŸºç¡€\n* **è¯„ä¼°æ¨¡åž‹æ€§èƒ½**å¹¶è¿›è¡Œæ”¹è¿›\n* **éƒ¨ç½²å’Œé›†æˆ**AIè§£å†³æ–¹æ¡ˆäºŽå•†ä¸šçŽ¯å¢ƒ\n\n## 2\\. æŽŒæ¡åŸºç¡€ï¼šæ•°å­¦å’Œç»Ÿè®¡å­¦\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*HxbGZ1D4QFGf2FZ2dKLcsQ.png)\n\nåœ¨æ·±å…¥äº†è§£ AI ç®—æ³•ä¹‹å‰ï¼Œæ‚¨éœ€è¦æ‰Žå®žçš„æ•°å­¦åŸºç¡€ã€‚ä»¥ä¸‹æ˜¯å¿…è¦çš„ä¸»é¢˜ï¼š\n\n### AIå·¥ç¨‹çš„å…³é”®æ•°å­¦ä¸»é¢˜\n\n* **çº¿æ€§ä»£æ•°ï¼š** ç†è§£ç¥žç»ç½‘ç»œçš„åŸºç¡€ã€‚ä¸»é¢˜ï¼šå‘é‡ã€çŸ©é˜µã€ç‰¹å¾å€¼ã€‚\n* **æ¦‚çŽ‡ä¸Žç»Ÿè®¡ï¼š** å¸®åŠ©æ‚¨åšå‡ºæ•°æ®é©±åŠ¨çš„å†³ç­–ã€‚ä¸»é¢˜ï¼šåˆ†å¸ƒã€å‡è®¾æ£€éªŒã€è´å¶æ–¯æ¦‚å¿µã€‚\n* **å¾®ç§¯åˆ†ï¼š** ç”¨äºŽä¼˜åŒ–æœºå™¨å­¦ä¹ æ¨¡åž‹ã€‚ä¸»é¢˜ï¼šå¯¼æ•°ã€åå¯¼æ•°å’Œæ¢¯åº¦ã€‚\n\n### å­¦ä¹ æ•°å­¦çš„èµ„æº\n\n1. **å¯æ±—å­¦é™¢** â€” [å¯æ±—å­¦é™¢æ•°å­¦è¯¾ç¨‹](https://www.khanacademy.org/)ï¼ˆå…è´¹ï¼Œé€‚åˆåˆå­¦è€…ï¼‰\n2. [**3Blue1Brown YouTubeé¢‘é“**](https://www.youtube.com/c/3blue1brown) â€” é€‚åˆè§†è§‰è§£é‡Šï¼Œå°¤å…¶æ˜¯çº¿æ€§ä»£æ•°å’Œå¾®ç§¯åˆ†ã€‚\n3. **Courseraï¼šæœºå™¨å­¦ä¹ çš„æ•°å­¦** â€” [Courseraæœºå™¨å­¦ä¹ çš„æ•°å­¦](https://www.coursera.org/specializations/mathematics-machine-learning)ï¼ˆå…è´¹å®¡è®¡ï¼Œè®¤è¯éœ€ä»˜è´¹ï¼‰\n\n## 3\\. å­¦ä¹ ç¼–ç¨‹ï¼ˆPython æ˜¯çŽ‹è€…ï¼‰\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*NxkreIEpQiuVUDD4_gWFSQ.png)\n\n### ä¸ºä»€ä¹ˆé€‰æ‹©Pythonï¼Ÿ\n\nPythonæ˜¯äººå·¥æ™ºèƒ½çš„é¦–é€‰è¯­è¨€ï¼Œå› ä¸ºå®ƒç®€å•ã€çµæ´»ï¼Œå¹¶ä¸”æ‹¥æœ‰å¤§é‡ç”¨äºŽäººå·¥æ™ºèƒ½å’Œæ•°æ®ç§‘å­¦çš„åº“ã€‚ä½ ä¸éœ€è¦çŸ¥é“é«˜çº§ç¼–ç¨‹å°±å¯ä»¥å¼€å§‹ï¼Œä½†æŽŒæ¡Pythonæ˜¯å¿…ä¸å¯å°‘çš„ã€‚\n\n### Python ä¸»é¢˜æ¶µç›–\n\n* **åŸºç¡€çŸ¥è¯†ï¼š** å˜é‡ã€å¾ªçŽ¯ã€å‡½æ•°å’Œæ•°æ®ç»“æž„ã€‚\n* **æ•°æ®ç§‘å­¦åº“ï¼š** Numpyã€Pandas å’Œ Matplotlib ç”¨äºŽæ•°æ®å¤„ç†å’Œå¯è§†åŒ–ã€‚\n* **æœºå™¨å­¦ä¹ åº“ï¼š** Scikit\\-Learnã€TensorFlow å’Œ PyTorchã€‚\n\n### æœ€ä½³ Python èµ„æº\n\n1. [**Codecademy Python è¯¾ç¨‹**](https://www.codecademy.com/catalog/language/python) â€” Codecademy Pythonï¼ˆé€‚åˆåˆå­¦è€…ï¼‰\n2. [**è°·æ­Œçš„ Python è¯¾ç¨‹**](https://developers.google.com/edu/python) â€” è°·æ­Œ Python è¯¾ç¨‹ï¼ˆå…è´¹ï¼‰\n3. [**Jake VanderPlas çš„æ•°æ®ç§‘å­¦ Python æ‰‹å†Œ**](https://jakevdp.github.io/PythonDataScienceHandbook/) â€” æ•°æ®ç§‘å­¦ Python ä¹¦ç±ï¼ˆåœ¨çº¿å…è´¹ï¼‰\n\n## 4\\. ç†Ÿæ‚‰æ•°æ®ï¼šæ•°æ®ç§‘å­¦åŸºç¡€\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*tugfLSYdMDps7t46G7dbhw.png)\n\næ•°æ®æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒã€‚ä½œä¸ºä¸€åäººå·¥æ™ºèƒ½å·¥ç¨‹å¸ˆï¼Œæ‚¨éœ€è¦æ”¶é›†ã€æ¸…ç†å’Œåˆ†æžå¤§é‡æ•°æ®é›†ã€‚ä»¥ä¸‹æ˜¯æ‚¨éœ€è¦å…³æ³¨çš„å†…å®¹ï¼š\n\n### å…³é”®æ•°æ®ç§‘å­¦æŠ€èƒ½\n\n* **æ•°æ®æ”¶é›†ä¸Žæ¸…æ´—ï¼š** å­¦ä¹ å¦‚ä½•å¤„ç†ç¼ºå¤±å€¼ã€æ¸…ç†æ‚ä¹±æ•°æ®ï¼Œå¹¶ä¸ºæœºå™¨å­¦ä¹ æ¨¡åž‹è¿›è¡Œé¢„å¤„ç†ã€‚\n* **æŽ¢ç´¢æ€§æ•°æ®åˆ†æžï¼ˆEDAï¼‰ï¼š** ç†è§£å¦‚ä½•åˆ†æžå’Œå¯è§†åŒ–æ•°æ®æ¨¡å¼ã€‚\n* **ç‰¹å¾å·¥ç¨‹ï¼š** å°†åŽŸå§‹æ•°æ®å¤„ç†ä¸ºæœ‰ç”¨çš„ç‰¹å¾ï¼Œä»¥æé«˜æ¨¡åž‹çš„å‡†ç¡®æ€§ã€‚\n\n### æ•°æ®ç§‘å­¦è¯¾ç¨‹\n\n1. **IBM æ•°æ®ç§‘å­¦ä¸“ä¸šè¯ä¹¦ï¼ˆCourseraï¼‰** â€” [IBM æ•°æ®ç§‘å­¦](https://www.coursera.org/professional-certificates/ibm-data-science)ï¼ˆé€‚åˆåˆå­¦è€…ï¼Œç»“æž„åŒ–å­¦ä¹ è·¯å¾„ï¼‰\n2. **DataCamp çš„ Python æ•°æ®ç§‘å­¦å®¶è¯¾ç¨‹** â€” [DataCamp Python è¯¾ç¨‹](https://www.datacamp.com/tracks/data-scientist-in-python)ï¼ˆåŸºäºŽè®¢é˜…ï¼‰\n3. [**Jake VanderPlas çš„ Python æ•°æ®ç§‘å­¦æ‰‹å†Œ**](https://jakevdp.github.io/PythonDataScienceHandbook/)ï¼ˆå…è´¹åœ¨çº¿èµ„æºï¼‰\n\n## 5\\. æ·±å…¥æœºå™¨å­¦ä¹ \n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*JwqTp844juTSX8FFSxvfTg.png)\n\næœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½å·¥ç¨‹çš„æ ¸å¿ƒã€‚åœ¨è¿™é‡Œï¼Œç®—æ³•è¢«å¼€å‘å‡ºæ¥ä»¥ä»Žæ•°æ®ä¸­å­¦ä¹ æ¨¡å¼å¹¶è¿›è¡Œé¢„æµ‹ã€‚ä»¥ä¸‹æ˜¯è·¯çº¿å›¾ï¼š\n\n### é‡è¦çš„æœºå™¨å­¦ä¹ ä¸»é¢˜\n\n* **ç›‘ç£å­¦ä¹ ï¼š** çº¿æ€§å›žå½’ã€é€»è¾‘å›žå½’ã€å†³ç­–æ ‘å’Œéšæœºæ£®æž—ã€‚\n* **æ— ç›‘ç£å­¦ä¹ ï¼š** èšç±»æŠ€æœ¯ï¼Œå¦‚ K\\-å‡å€¼å’Œå±‚æ¬¡èšç±»ã€‚\n* **æ¨¡åž‹è¯„ä¼°ï¼š** ç†è§£å‡†ç¡®çŽ‡ã€ç²¾ç¡®çŽ‡ã€å¬å›žçŽ‡ã€F1\\-åˆ†æ•°å’Œ ROC æ›²çº¿ç­‰æŒ‡æ ‡ã€‚\n* **æ·±åº¦å­¦ä¹ åŸºç¡€ï¼š** ç¥žç»ç½‘ç»œåŠå…¶å·¥ä½œåŽŸç†ç®€ä»‹ã€‚\n\n### é¡¶çº§æœºå™¨å­¦ä¹ èµ„æº\n\n1. **Andrew Ng åœ¨ Coursera ä¸Šçš„æœºå™¨å­¦ä¹ è¯¾ç¨‹** â€” [Andrew Ng çš„æœºå™¨å­¦ä¹ ](https://www.coursera.org/learn/machine-learning) (é€‚åˆåˆå­¦è€…)\n2. **AurÃ©lien GÃ©ron çš„ã€Šä½¿ç”¨ Scikit-Learnã€Keras å’Œ TensorFlow çš„åŠ¨æ‰‹æœºå™¨å­¦ä¹ ã€‹** â€” [åŠ¨æ‰‹æœºå™¨å­¦ä¹ ](https://github.com/Akramz/Hands-on-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow) (å®žç”¨çš„åŠ¨æ‰‹æ–¹æ³•)\n3. **Fast.ai çš„ã€Šç¨‹åºå‘˜å®žç”¨æ·±åº¦å­¦ä¹ ã€‹** â€” [Fast.ai è¯¾ç¨‹](https://course.fast.ai/) (æ›´é«˜çº§ä½†éžå¸¸å®žç”¨)\n\n## 6\\. æŽ¢ç´¢æ·±åº¦å­¦ä¹ å’Œç¥žç»ç½‘ç»œ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*AW1bwL9BTcOr22EkiDkg3w.png)\n\næ·±åº¦å­¦ä¹ é€šè¿‡æ¨¡æ‹Ÿäººè„‘çš„ç¥žç»ç½‘ç»œå°†äººå·¥æ™ºèƒ½æŽ¨å‘æ›´é«˜çš„å±‚æ¬¡ã€‚è¿™äº›ç½‘ç»œè¢«åº”ç”¨äºŽå›¾åƒè¯†åˆ«å’Œè¯­è¨€ç¿»è¯‘ç­‰é¢†åŸŸã€‚\n\n### æ·±åº¦å­¦ä¹ ä¸­çš„æ ¸å¿ƒä¸»é¢˜\n\n* **ç¥žç»ç½‘ç»œåŸºç¡€ï¼š** æ„ŸçŸ¥å™¨ï¼Œæ¿€æ´»å‡½æ•°ï¼Œå‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ã€‚\n* **å·ç§¯ç¥žç»ç½‘ç»œ (CNNs)ï¼š** ç”¨äºŽå›¾åƒå¤„ç†ã€‚\n* **é€’å½’ç¥žç»ç½‘ç»œ (RNNs)ï¼š** å¯¹äºŽæ—¶é—´åºåˆ—å’Œè‡ªç„¶è¯­è¨€å¤„ç†ç­‰åºåˆ—æ•°æ®éžå¸¸æœ‰ç”¨ã€‚\n\n### æ·±åº¦å­¦ä¹ è¯¾ç¨‹å’Œèµ„æº\n\n1. **Courseraä¸Šçš„Andrew Ngæ·±åº¦å­¦ä¹ ä¸“ä¸šè¯¾ç¨‹** â€” [Andrew Ngçš„æ·±åº¦å­¦ä¹ ](https://www.coursera.org/specializations/deep-learning)ï¼ˆå…¨é¢ï¼Œå¼ºçƒˆæŽ¨èï¼‰\n2. **Courseraä¸Šçš„Deeplearning.aiå®žè·µä¸­çš„TensorFlow** â€” [TensorFlowè¯¾ç¨‹](https://www.coursera.org/professional-certificates/tensorflow-in-practice)\n3. [**Packtçš„PyTorchæ·±åº¦å­¦ä¹ é¡¹ç›®**](https://www.packtpub.com/en-us/product/deep-learning-with-pytorch-9781788624336) â€” ä¸€æœ¬é€‚åˆä½¿ç”¨PyTorchè¿›è¡Œå®žè·µé¡¹ç›®çš„å¥½ä¹¦ã€‚\n\n## 7\\. å®žè·µçœŸå®žé¡¹ç›®\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*7c0NpXhFkZ--spsXM9qBrg.png)\n\nç†è®ºçŸ¥è¯†å›ºç„¶é‡è¦ï¼Œä½†äººå·¥æ™ºèƒ½æ˜¯ä¸€ä¸ªå®žè·µæ€§å¾ˆå¼ºçš„é¢†åŸŸã€‚å‚ä¸Žé¡¹ç›®å°†å·©å›ºä½ çš„ç†è§£ï¼Œå¹¶ä¸ºä½ çš„ä½œå“é›†æä¾›å±•ç¤ºçš„å†…å®¹ã€‚\n\n### åˆå­¦è€…é¡¹ç›®åˆ›æ„\n\n* **é¢„æµ‹è‚¡ç¥¨ä»·æ ¼** ä½¿ç”¨åŽ†å²æ•°æ®ï¼ˆæ—¶é—´åºåˆ—é¢„æµ‹ï¼‰\n* **å›¾åƒåˆ†ç±»** ä½¿ç”¨å·ç§¯ç¥žç»ç½‘ç»œï¼ˆåˆ†ç±»åŠ¨ç‰©ã€è½¦è¾†ç­‰çš„å›¾åƒï¼‰\n* **æƒ…æ„Ÿåˆ†æž** é’ˆå¯¹ç¤¾äº¤åª’ä½“å¸–å­æˆ–äº§å“è¯„è®º\n\n### å¯»æ‰¾AIé¡¹ç›®æ•°æ®é›†çš„åœ°æ–¹\n\n1. **Kaggleæ•°æ®é›†** â€” [Kaggle Datasets](https://www.kaggle.com/datasets) (å¤šç§æœºå™¨å­¦ä¹ é¡¹ç›®æ•°æ®é›†)\n2. **UCIæœºå™¨å­¦ä¹ åº“** â€” [UCI ML Repository](https://archive.ics.uci.edu/ml/index.php)\n3. **è°·æ­Œæ•°æ®é›†æœç´¢** â€” [Google Dataset Search](https://datasetsearch.research.google.com/)\n\n## 8\\. ç²¾é€šæ¨¡åž‹éƒ¨ç½²æŠ€èƒ½\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*t9u1WCyf6TDeZ_M7m3IN-Q.png)\n\näº†è§£å¦‚ä½•åœ¨ç”Ÿäº§çŽ¯å¢ƒä¸­éƒ¨ç½²AIæ¨¡åž‹æ˜¯ä¸€ä¸ªå·¨å¤§çš„ä¼˜åŠ¿ã€‚è¿™é¡¹æŠ€èƒ½ä½¿ä½ æˆä¸ºä¸€ä¸ªå®è´µçš„èµ„äº§ï¼Œå› ä¸ºå®ƒå¼¥åˆäº†æ•°æ®ç§‘å­¦ä¸Žå®žé™…åº”ç”¨ä¹‹é—´çš„å·®è·ã€‚\n\n### éƒ¨ç½²å·¥å…·å­¦ä¹ \n\n* **Flask \\& Django:** ç”¨äºŽåˆ›å»ºç®€å•çš„ web åº”ç”¨ç¨‹åºä»¥æœåŠ¡äºŽæ‚¨çš„æ¨¡åž‹ã€‚\n* **Docker:** ç”¨äºŽå°†æ‚¨çš„æ¨¡åž‹å®¹å™¨åŒ–ï¼Œä½¿éƒ¨ç½²æ›´ç®€å•ã€æ›´ä¸€è‡´ã€‚\n* **AWS, Azure, Google Cloud:** ç”¨äºŽåœ¨äº‘ä¸­éƒ¨ç½²å¯æ‰©å±•çš„ AI æ¨¡åž‹ã€‚\n\n### æ¨¡åž‹éƒ¨ç½²èµ„æº\n\n1. **Udacityçš„AIäº§å“ç»ç†çº³ç±³å­¦ä½** â€” [Udacity AIäº§å“ç»ç†](https://www.udacity.com/course/ai-product-manager-nanodegree--nd088)\n2. **Courseraçš„Deeplearning.ai MLOpsä¸“ä¸šåŒ–** â€” [MLOpsä¸“ä¸šåŒ–](https://www.coursera.org/specializations/machine-learning-engineering-for-production-mlops)\n3. [**Packtçš„æ•°æ®ç§‘å­¦Docker**](https://www.packtpub.com/en-in/product/the-ultimate-docker-container-book-9781804613986?type=print) â€” éžå¸¸é€‚åˆå­¦ä¹ ä»¥æ•°æ®ç§‘å­¦ä¸ºé‡ç‚¹çš„Dockerã€‚\n\n## 9\\. å»ºç«‹ä½œå“é›†å¹¶å¼€å§‹å»ºç«‹äººè„‰\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*CmE_JeIOcWcogB9X57QheA.png)\n\nä¸€ä¸ªå¼ºå¤§çš„ä½œå“é›†å±•ç¤ºäº†ä½ çš„æŠ€èƒ½ï¼Œè€Œå»ºç«‹äººè„‰åˆ™ä¸ºä½ æ‰“å¼€äº†å·¥ä½œæœºä¼šå’ŒæŒ‡å¯¼çš„é—¨ã€‚\n\n### ä½œå“é›†æŠ€å·§\n\n* å±•ç¤º 3â€“5 ä¸ªçªæ˜¾ä¸åŒæŠ€èƒ½çš„é¡¹ç›®ï¼ˆæ•°æ®ç§‘å­¦ã€æœºå™¨å­¦ä¹ ã€éƒ¨ç½²ï¼‰ã€‚\n* åŒ…å«ä¸ªäººé¡¹ç›®å’Œå›¢é˜Ÿåˆä½œçš„æ··åˆã€‚\n* ä¸ºæ¯ä¸ªé¡¹ç›®æ’°å†™æ€»ç»“ã€ä»£ç å’Œç»“æžœã€‚\n\n### ç½‘ç»œå¹³å°\n\n* [**LinkedIn**](https://www.linkedin.com/) â€” ä¸ŽAIä¸“ä¸šäººå£«å’Œæ‹›è˜äººå‘˜å»ºç«‹è”ç³»ã€‚\n* [**GitHub**](https://github.com/) â€” å‘å¸ƒä½ çš„é¡¹ç›®ä»¥ä¾›æ½œåœ¨é›‡ä¸»æŸ¥çœ‹ã€‚\n* [**Kaggle Competitions**](https://www.kaggle.com/competitions) â€” å‚åŠ æ¯”èµ›ä»¥å­¦ä¹ å’Œå±•ç¤ºä½ çš„æŠ€èƒ½ã€‚\n\n## ç»“è®ºï¼šå‡†å¤‡å¥½æˆä¸ºä¸€åAIå·¥ç¨‹å¸ˆäº†å—ï¼Ÿ\n\nåœ¨2025å¹´æˆä¸ºä¸€åAIå·¥ç¨‹å¸ˆä¸ä»…ä»…æ˜¯ä¸€ä¸ªæ¢¦æƒ³ï¼›å¦‚æžœä½ éµå¾ªä¸€ä¸ªç»“æž„åŒ–çš„è·¯å¾„ï¼Œè¿™æ˜¯å¯ä»¥å®žçŽ°çš„ã€‚é¦–å…ˆæŽŒæ¡åŸºç¡€çŸ¥è¯†ï¼ŒæŠ•å…¥æ—¶é—´è¿›è¡Œé¡¹ç›®å®žè·µï¼Œå¹¶ä¸æ–­æŽ¨åŠ¨è‡ªå·±å­¦ä¹ æ›´é«˜çº§çš„æ¦‚å¿µã€‚AIæ˜¯ä¸€ä¸ªå¿«é€Ÿå‘å±•çš„é¢†åŸŸï¼Œè¶Šæ˜¯è‡´åŠ›äºŽæŒç»­å­¦ä¹ ï¼Œæœºä¼šå°±è¶Šå¤šã€‚\n\né‚£ä¹ˆï¼Œä½ å‡†å¤‡å¥½è¿›å…¥AIçš„ä¸–ç•Œäº†å—ï¼Ÿè®°ä½ï¼Œæ‰€æœ‰çš„AIå·¥ç¨‹å¸ˆéƒ½æ˜¯ä»Žåˆå­¦è€…å¼€å§‹çš„ã€‚å‡­å€ŸåšæŒå’Œå¥½å¥‡å¿ƒï¼Œä½ å¾ˆå¿«å°±ä¼šåˆ›é€ å‡ºåˆ›æ–°çš„AIè§£å†³æ–¹æ¡ˆã€‚è®©æˆ‘ä»¬ä¸€èµ·å®žçŽ°è¿™ä¸ªç›®æ ‡å§ï¼\n\n## å¸¸è§é—®é¢˜\n\n**1\\. æˆ‘éœ€è¦æœ‰AIå­¦ä½æ‰èƒ½æˆä¸ºAIå·¥ç¨‹å¸ˆå—ï¼Ÿ** ä¸éœ€è¦ï¼Œè™½ç„¶å­¦ä½å¯ä»¥æœ‰æ‰€å¸®åŠ©ï¼Œä½†è®¸å¤šAIå·¥ç¨‹å¸ˆæ˜¯è‡ªå­¦æˆæ‰æˆ–ä»Žç›¸å…³é¢†åŸŸè½¬è¡Œçš„ã€‚åœ¨çº¿è¯¾ç¨‹ã€é¡¹ç›®å’Œå¼ºå¤§çš„ä½œå“é›†åŒæ ·æœ‰ä»·å€¼ã€‚\n\n**2\\. æˆä¸ºAIå·¥ç¨‹å¸ˆéœ€è¦å¤šé•¿æ—¶é—´ï¼Ÿ** è¿™å–å†³äºŽä½ çš„èƒŒæ™¯ï¼Œä½†ä¸“æ³¨çš„å­¦ä¹ è€…å¯ä»¥åœ¨6-12ä¸ªæœˆå†…é€šè¿‡ä¸“å¿ƒå­¦ä¹ å’Œé¡¹ç›®å·¥ä½œå®žçŽ°è¿™ä¸€ç›®æ ‡ã€‚\n\n**3\\. AIå·¥ç¨‹å¸ˆçš„å¿…å¤‡æŠ€èƒ½æ˜¯ä»€ä¹ˆï¼Ÿ** å…³é”®æŠ€èƒ½åŒ…æ‹¬Pythonç¼–ç¨‹ã€æœºå™¨å­¦ä¹ ã€æ•°æ®ç§‘å­¦åŸºç¡€ï¼Œä»¥åŠå¯¹æ·±åº¦å­¦ä¹ æ¡†æž¶å¦‚TensorFlowæˆ–PyTorchçš„äº†è§£ã€‚\n\n**4\\. éœ€è¦å“ªäº›ç¼–ç¨‹è¯­è¨€ï¼Ÿ** Pythonæ˜¯AIçš„ä¸»è¦è¯­è¨€ï¼Œä½†æ ¹æ®ä½ çš„è§’è‰²ï¼Œç†Ÿæ‚‰Rã€SQLæˆ–ç”šè‡³JavaScriptä¹Ÿä¼šå¾ˆæœ‰å¸®åŠ©ã€‚\n\n**5\\. AIå·¥ç¨‹å¸ˆæ˜¯é«˜è–ªèŒä¸šå—ï¼Ÿ** æ˜¯çš„ï¼ŒAIå·¥ç¨‹å¸ˆæ˜¯éœ€æ±‚é‡å¤§ä¸”è–ªé…¬ä¸°åŽšçš„æŠ€æœ¯é¢†åŸŸä¹‹ä¸€ï¼Œå…¨çƒèŒƒå›´å†…çš„è–ªèµ„ç«žäº‰åŠ›å¼ºã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*2luRkTNWk_o3KSh9.png)\n\næ­¤æ•…äº‹å‘å¸ƒäºŽ[Generative AI](https://generativeai.pub/)ã€‚åœ¨[LinkedIn](https://www.linkedin.com/company/generative-ai-publication)ä¸Šä¸Žæˆ‘ä»¬è”ç³»ï¼Œå¹¶å…³æ³¨[Zeniteq](https://www.zeniteq.com/)ï¼Œä»¥èŽ·å–æœ€æ–°çš„AIæ•…äº‹ã€‚\n\nè®¢é˜…æˆ‘ä»¬çš„[æ–°é—»é€šè®¯](https://www.generativeaipub.com/)å’Œ[YouTube](https://www.youtube.com/@generativeaipub)é¢‘é“ï¼Œä»¥èŽ·å–ç”ŸæˆAIçš„æœ€æ–°æ–°é—»å’Œæ›´æ–°ã€‚è®©æˆ‘ä»¬ä¸€èµ·å¡‘é€ AIçš„æœªæ¥ï¼\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*0DGBxUYjhr3BbfzS.png)\n\n"},{"lang":"zh","group":"blog","slug":"blog/searchgpt-changed-how-i-surf-the-web-forever-3e970027998c","frontmatter":{"title":"SearchGPT å½»åº•æ”¹å˜äº†æˆ‘çš„ä¸Šç½‘æ–¹å¼ã€‚","meta_title":"SearchGPT å½»åº•æ”¹å˜äº†æˆ‘çš„ä¸Šç½‘æ–¹å¼ã€‚","description":"è¿™ç¯‡æ–‡ç« ä»‹ç»äº†SearchGPTçš„ä¼˜åŠ¿åŠå…¶åœ¨ç½‘ç»œæœç´¢ä¸­çš„åº”ç”¨ã€‚ä¸»è¦ä¼˜ç‚¹åŒ…æ‹¬è¶…å¿«é€Ÿè¯­ä¹‰æœç´¢ã€å¿«é€Ÿç†è§£ä¿¡æ¯ä»¥åŠé›†ä¸­ç®¡ç†æœç´¢åŽ†å²ã€‚ä½œè€…é€šè¿‡å¯¹æ¯”æµ‹è¯•å±•ç¤ºäº†SearchGPTåœ¨æœç´¢é€Ÿåº¦ä¸Šçš„æ˜¾è‘—ä¼˜åŠ¿ï¼Œå¹¶æä¾›äº†å¤šç§ä½¿ç”¨æŠ€å·§å’Œç­–ç•¥ï¼Œä»¥ä¼˜åŒ–æœç´¢ä½“éªŒã€‚æ–‡ç« è¿˜æŽ¢è®¨äº†ä¸åŒç±»åž‹çš„äº’è”ç½‘æœç´¢è·¯å¾„ï¼Œå¹¶åˆ†æžäº†SearchGPTåœ¨è¿™äº›è·¯å¾„ä¸­çš„è¡¨çŽ°ï¼Œå¼ºè°ƒäº†å®ƒåœ¨ä¿¡æ¯èŽ·å–å’ŒçŸ¥è¯†æž„å»ºæ–¹é¢çš„æ•ˆçŽ‡ã€‚","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*xwFpzwWrReqEjftU1cjksg.jpeg","categories":["Technology/Web","Programming/Scripting","SearchGPT is not a standard category","but the closest match from the given list is Technology/Web. The article focuses on a web-based tool that enhances search capabilities","which falls under the Technology/Web category. Additionally","the use of advanced search techniques and programming aspects of the tool suggest Programming/Scripting as a relevant category. However","since SearchGPT is the main focus","Technology/Web is the most appropriate primary category. \n\nTherefore","the final categories are:\nTechnology/Web","Programming/Scripting"],"author":"Rifx.Online","tags":["SearchGPT","semantic","efficiency","prompts","aggregation"],"draft":false,"slug":"blog/searchgpt-changed-how-i-surf-the-web-forever-3e970027998c"},"content":"\n\n\n### æˆ‘å‘Šåˆ«äº†è°·æ­Œï¼Œä½ ä¹Ÿå¯ä»¥ã€‚å­¦ä¹ å¦‚ä½•ä½¿ç”¨ SearchGPTï¼Œæ”¹å˜ä½ çš„å·¥ä½œæ–¹å¼ã€‚\n\n\n\nè¿™ä¸æ˜¯ç‚¹å‡»è¯±é¥µã€‚æˆ‘å®Œå…¨ç›¸ä¿¡ SearchGPTï¼Œæˆ‘å°†å‘Šè¯‰ä½ åŽŸå› ã€‚è¿™ç¯‡æ–‡ç« æ—¢æ˜¯ä½ åº”è¯¥å¼€å§‹ä½¿ç”¨ SearchGPT çš„å®£è¨€ï¼Œä¹Ÿæ˜¯å…³äºŽå¦‚ä½•æœ€ä½³ä½¿ç”¨å®ƒçš„å…¨é¢æŒ‡å—ã€‚\n\næ— è®ºä½ æ˜¯ SearchGPT çš„æ–°æ‰‹ï¼Œè¿˜æ˜¯ç¨å¾®å°è¯•è¿‡ï¼Œæˆ–è€…å·²ç»æ˜¯é«˜çº§ç”¨æˆ·ï¼Œæˆ‘ä¿è¯ä½ ä¼šåœ¨è¿™é‡Œæ‰¾åˆ°æœ‰ç”¨çš„ä¸œè¥¿ã€‚è¿™ç¯‡æ–‡ç« æœ‰ç‚¹é•¿ï¼Œä½†å®ƒå¯èƒ½æ˜¯ä½ åœ¨ 2024 å¹´å·¥ä½œæµç¨‹ä¸­å¯ä»¥åšå‡ºçš„æœ€èŠ‚çœæ—¶é—´çš„æ”¹è¿›ã€‚\n\nä¸å†èµ˜è¿°ï¼Œè®©æˆ‘ä»¬æ¥çœ‹çœ‹æˆ‘å¦‚æ­¤å–œçˆ± SearchGPT çš„ä¸€äº›ä¸»è¦åŽŸå› ã€‚\n\n## SearchGPTçš„ä¸‰å¤§ä¼˜åŠ¿\n\n### 1\\. è¶…å¿«é€Ÿè¯­ä¹‰æœç´¢\n\næ‚¨å¯ä»¥ç”¨ Search GPT æœç´¢ *ä»»ä½•ä¸œè¥¿*ã€‚äº‹å®žä¸Šï¼Œåœ¨æˆ‘çš„å¯¹æ¯”æµ‹è¯•ä¸­ï¼Œå®ƒçš„é€Ÿåº¦å¯ä»¥æ¯”æ ‡å‡†çš„ Google æœç´¢å¿« **10 å€**ã€‚\n\nä¾‹å¦‚ï¼Œå‰å‡ å¤©ï¼Œæˆ‘æƒ³èµ·äº†ä¸€ä¸ªæˆ‘ä¹‹å‰çœ‹åˆ°çš„é´å­å“ç‰Œï¼Œæƒ³è¦è¿›ä¸€æ­¥äº†è§£ã€‚æˆ‘çŸ¥é“ä¸€äº›å…³äºŽè¿™ä¸ªå“ç‰Œçš„ç»†èŠ‚ï¼Œä½†å¾ˆéš¾ç”¨ä¸€ä¸ªæœç´¢è¯æ¥è¡¨è¾¾ã€‚ä»¥ä¸‹æ˜¯æˆ‘æ‰€çŸ¥é“çš„ï¼š\n\n* å®ƒæ˜¯ä¸€ä¸ªå•è¯å“ç‰Œï¼ˆå³â€œJim's Bootsâ€æˆ–ç±»ä¼¼åç§°ï¼‰\n* é´å­ç›¸å½“æ˜‚è´µä½†è´¨é‡ä¸Šä¹˜\n* é´å­æ˜¯æ‰‹å·¥åˆ¶ä½œçš„\n* å®ƒä»¬æ˜¯å¤ç”°éž‹åº•ï¼ˆGoodyear Welted Solesï¼‰\n* å®ƒä»¬æ˜¯ä¸€ä¸ªç¾Žå›½å“ç‰Œï¼ˆå¯èƒ½ï¼‰\n* å®ƒä»¬åœ¨ä¸€ä¸ªé´å­ç›¸å…³çš„ Subreddit ç¤¾åŒºä¸­å¤‡å—æŽ¨å´‡\n\nè¿™æ˜¯ä¸€ç§æƒ…å†µï¼Œå¦‚æžœæˆ‘è§è¿‡ï¼Œæˆ‘ä¼šè®°å¾—ï¼›å®ƒå°±åœ¨æˆ‘å˜´è¾¹ã€‚äºŽæ˜¯ï¼Œæˆ‘å†³å®šåšä¸€ä¸ªå°æµ‹è¯•ã€‚æˆ‘ç›´æŽ¥æ¯”è¾ƒäº† SearchGPT å’Œ Googleã€‚æˆ‘è®¾ç½®äº†ä¸€ä¸ªè®¡æ—¶å™¨ï¼Œé¦–å…ˆç”¨ Google å¼€å§‹æœç´¢ã€‚\n\nåœ¨è´­ç‰©é¡µé¢ä¸Šæ²¡æœ‰æ‰¾åˆ°ä»»ä½•ä¸œè¥¿åŽï¼ˆæœç´¢â€œæ‰‹å·¥åˆ¶ä½œçš„ç¾Žå›½å¤ç”°éž‹åº•é´å­â€æ— æžœï¼‰ï¼Œæˆ‘å¼€å§‹ä¸åœåœ°æ»šåŠ¨ã€‚åœ¨å‰ 30 ä¸ªç»“æžœä¹‹åŽï¼Œæˆ‘å†æ¬¡æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ä¸œè¥¿ã€‚\n\næˆ‘å¼€å§‹åœ¨æœç´¢æœ«å°¾æ’å…¥â€œRedditâ€ã€‚å‰ä¸¤ä¸ªå‡ºçŽ°çš„å¸–å­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ä¿¡æ¯ï¼Œä½†éšåŽæˆ‘æ‰¾åˆ°äº†ã€‚è¿™é‡Œæœ‰ä¸€ä¸ª [å…³äºŽæœ€ä½³é´å­å“ç‰Œçš„å·¨å¤§å¸–å­ï¼ŒæŒ‰ç…§ä»·æ ¼ç‚¹è¿›è¡Œæ¦‚è¿°](https://www.reddit.com/r/goodyearwelt/comments/7qxy6p/the_2018_beginners_boot_buying_guide/)ï¼Œæˆ‘å¼€å§‹æµè§ˆè¿™ä¸ªåºžå¤§çš„åˆ—è¡¨ï¼Œå¯»æ‰¾é‚£ä¸ªå“ç‰Œã€‚ç»è¿‡ä¸€åˆ†é’Ÿï¼Œæˆ‘æ‰¾åˆ°äº†ï¼šNicks Bootsã€‚\n\næˆ‘çš„æ€»æœç´¢æ—¶é—´ï¼Ÿ **4:37\\.**\n\næˆ‘è®¾ç½®äº†ä¸€ä¸ªæ–°è®¡æ—¶å™¨ï¼Œæ‰“å¼€äº† SearchGPTï¼Œå¹¶è¾“å…¥äº†å…³äºŽæˆ‘æ‰€çŸ¥é“çš„è¿™ä¸ªå“ç‰Œçš„æç¤ºã€‚æˆ‘èŠ±äº†å¤§çº¦ **20 ç§’** æ¥å†™å‡ºè¿™ä¸ªæç¤ºã€‚ä»¥ä¸‹æ˜¯ç»“æžœï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*JM16NcvRzXDZB2dnT2K96w.png)\n\nè½°ï¼å®ƒå°±åœ¨è¿™é‡Œï¼ŒæŽ’åœ¨ç¬¬ 7 ä½ã€‚è¿™æ¯” Google æœç´¢å¿«äº†æ•´æ•´ **13 å€**ã€‚æˆ‘è¿˜éœ€è¦å†è¯´æœæ‚¨å®ƒçš„é€Ÿåº¦å—ï¼Ÿ\n\n### 2\\. å¿«é€Ÿç†è§£æŸä¸ªæ¦‚å¿µçš„ç‰¹å®šéƒ¨åˆ†\n\nå¦‚æžœä½ éœ€è¦åœ¨äº’è”ç½‘ä¸Šæœç´¢ä»¥äº†è§£æ›´å¤šä¿¡æ¯ï¼Œä½ å¾ˆå¯èƒ½æœ€ç»ˆä¼šåœ¨ç»´åŸºç™¾ç§‘ä¸Šï¼Œæˆ–è€…ä½ å¯èƒ½ä¼šé˜…è¯»[è°·æ­Œçš„å»ºè®®æ‘˜è¦](https://support.google.com/websearch/answer/9351707?hl=en)ã€‚è™½ç„¶è¿™ä¸¤è€…éƒ½æ˜¯å¾ˆå¥½çš„èµ„æºï¼Œä½†å…³äºŽè¯¥ä¸»é¢˜çš„ä¿¡æ¯å¾€å¾€ä¼šä½¿ä½ åç¦»æ ¹æœ¬ç›®æ ‡ã€‚ä½¿ç”¨ SearchGPTï¼Œä½ å¯ä»¥å¿«é€ŸèŽ·å¾—æ¦‚è¿°ï¼Œé€šè¿‡åŽç»­é—®é¢˜ç¼©å°ç†è§£èŒƒå›´ï¼Œæ›´å¿«åœ°å®žçŽ°çŸ¥è¯†ç›®æ ‡ã€‚\n\nä¾‹å¦‚ï¼Œå‡è®¾æˆ‘ä»¬æ˜¯ä¸€ä¸ªå®Œå…¨å¤–è¡Œçš„äººï¼Œæƒ³äº†è§£é£ŽåŠ›æ¶¡è½®æœºå¦‚ä½•å°†é£Žè½¬æ¢ä¸ºèƒ½é‡ã€‚\n\nåœ¨ä¼ ç»Ÿæœç´¢æµç¨‹ä¸­ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šæœç´¢â€œé£ŽåŠ›æ¶¡è½®æœºâ€æˆ–â€œé£ŽåŠ›æ¶¡è½®æœºæ˜¯å¦‚ä½•å·¥ä½œçš„â€ã€‚è®©æˆ‘ä»¬çœ‹çœ‹è°·æ­Œçš„æœç´¢ç»“æžœï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*93wqzjbylGagTlO9038iqw.png)\n\nåœ¨è¿™ä¹‹åŽï¼Œæˆ‘ä»¬çŽ°åœ¨ç†è§£äº†é£ŽåŠ›æ¶¡è½®æœºçš„è¦ç‚¹ã€‚ç„¶è€Œï¼ŒåŒæ—¶æˆ‘ä»¬ä»€ä¹ˆä¹Ÿä¸çŸ¥é“ã€‚ä¸ºäº†å½¢æˆå®žé™…çš„ç†è§£ï¼Œæˆ‘ä»¬å¿…é¡»é™·å…¥ä¸€ä¸ªä¿¡æ¯çš„å…”å­æ´žã€‚å› æ­¤ï¼Œä½ å¯èƒ½ä¼šæŽ¥ä¸‹æ¥æœç´¢ç”µåŠ¨å‘ç”µæœºçš„å®žé™…å†…éƒ¨å·¥ä½œåŽŸç†ã€‚ç„¶åŽï¼Œä½ è¿˜å¾—è¿›è¡Œå¦ä¸€æ¬¡æœç´¢ï¼Œå°†èŒƒå›´ç¼©å°åˆ°é£ŽåŠ›æ¶¡è½®æœºå‘ç”µæœºï¼Œå¹¶ä»Žä¸­ç»¼åˆä¿¡æ¯ã€‚\n\nè¿™ä¸ªè¿‡ç¨‹æ˜¯æœ‰æ•ˆçš„ï¼Œä½†**æ•ˆçŽ‡å¤ªä½Žäº†ã€‚**\n\nä½¿ç”¨ SearchGPTï¼Œæˆ‘ä»¬å¯ä»¥æ›´å¿«ã€æ›´ç›´è§‚åœ°æž„å»ºæˆ‘ä»¬çš„çŸ¥è¯†åŸºç¡€ã€‚å°±å¥½åƒæˆ‘ä»¬åœ¨ä¸Žé£ŽåŠ›æ¶¡è½®æœºä¸“å®¶å¯¹è¯ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªç¤ºä¾‹æµç¨‹ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BbSoxRLJNPh_BQehsaxB8g.png)\n\nçŽ°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥ä»Žè¿™äº›ç»„ä»¶ä¸­é€‰æ‹©ä¸€ä¸ªï¼Œä»¥æ›´æ·±å…¥åœ°ç†è§£è¯¥ä¸»é¢˜ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*REgvUWKmAWGRCrzi6nSeGw.png)\n\nåœ¨ä»»ä½•æ—¶å€™ï¼Œæˆ‘ä»¬éƒ½å¯ä»¥ç‚¹å‡»æä¾›çš„é“¾æŽ¥ä»Žç›´æŽ¥æ¥æºé˜…è¯»ï¼Œè€Œæˆ‘ä»¬çš„èŠå¤©è®°å½•ä¿æŒå®Œæ•´ã€‚è¿™æ˜¯æˆ‘å‘çŽ°çš„äº†è§£æŸäº‹çš„æœ€å¿«æ–¹å¼ï¼Œå®Œå…¨ä¸å¿…æ‹…å¿ƒ AI å¹»è§‰ï¼ˆå¤§å¤šæ•°æƒ…å†µä¸‹ï¼‰ã€‚\n\n### 3\\. å‘Šåˆ«æ ‡ç­¾é¡µçƒ¦æ¼\n\næœ‰äº† SearchGPTï¼Œæ ‡ç­¾é¡µåŸºæœ¬ä¸Šå˜å¾—å¤šä½™ã€‚å†ä¹Ÿä¸ç”¨åœ¨å¤šä¸ª Chrome çª—å£ä¹‹é—´æŠ˜è…¾ï¼Œæ¯ä¸ªçª—å£éƒ½æœ‰å‡ åä¸ªæ ‡ç­¾é¡µã€‚ä¹Ÿä¸å¿…æ‹…å¿ƒæ„å¤–çš„è®¡ç®—æœºé‡å¯ä¼šå¯¼è‡´å¤šå¤©çš„ç ”ç©¶è¿›å±•ä¸¢å¤±ï¼Œæˆ–è€…å¸Œæœ›èƒ½å¤Ÿæ‰¾å›žå‡ ä¸ªæœˆå‰çš„æœç´¢è®°å½•ã€‚\n\nSearchGPT å°†ä½ çš„æ‰€æœ‰ç ”ç©¶é›†ä¸­åœ¨ä¸€ä¸ªåœ°æ–¹ã€‚å®ƒæ˜“äºŽå¯¼èˆªï¼Œä½ çš„æœç´¢åŽ†å²æ˜¯æ— é™çš„ã€‚\n\nå‡è®¾ä½ å·²ç»ä½¿ç”¨ SearchGPT å‡ ä¸ªæœˆäº†ã€‚ä½ å¯èƒ½è¿˜è®°å¾—ä½ æ›¾ç»è¿›è¡Œè¿‡çš„ä¸€æ®µå…³äºŽ [åœ¨çº¿å¹¿å‘Šå†è¥é”€](https://mailchimp.com/resources/what-is-retargeting/) çš„ç ”ç©¶ã€‚é‚£ä¹ˆï¼Œä½ è¯¥å¦‚ä½•æ‰¾åˆ°å®ƒå‘¢ï¼Ÿå€ŸåŠ©å¯é çš„ ChatGPT æœç´¢æ ï¼Œæˆ‘ä»¬å¯ä»¥éžå¸¸å¿«é€Ÿåœ°æ‰¾åˆ°ç¡®åˆ‡çš„èŠå¤©è®°å½•ï¼Œå¹¶ä»Žæˆ‘ä»¬åœä¸‹çš„åœ°æ–¹ç»§ç»­ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BSOOg5-XJc0_zacDf0f87Q.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*32x_mj_WnLLc_xxH6WBuRg.png)\n\nå¦‚ä½ æ‰€è§ï¼Œæµè§ˆèŠå¤©è®°å½•ã€ç‚¹å‡»æ¥æºå’Œæ·±å…¥é˜…è¯»ä»»ä½•ä¸»é¢˜éƒ½éžå¸¸å¿«é€Ÿå’Œç®€å•ã€‚å¦‚æžœä½ æœ‰ä¸€ä¸ªè¶…çº§é•¿çš„èŠå¤©è®°å½•ï¼Œå¯ä»¥é€šè¿‡æŒ‰â€œctrl\\+fâ€å¹¶åœ¨é¡µé¢ä¸Šæœç´¢å…³é”®è¯å¿«é€Ÿå¯¼èˆªåˆ°èŠå¤©çš„æŸäº›éƒ¨åˆ†ã€‚\n\nè¿™äº›æ˜¯æˆ‘è®¤ä¸º SearchGPT çš„ä¸‰å¤§ä¼˜åŠ¿ï¼Œä½†è¿˜æœ‰å¾ˆå¤šå…¶ä»–ä¼˜ç‚¹ã€‚\n\nè®©æˆ‘ä»¬è¿‡æ¸¡åˆ°æœ¬æ–‡çš„ä¸‹ä¸€ä¸ªä¹Ÿæ˜¯æœ€å…³é”®çš„éƒ¨åˆ†ï¼š**å¦‚ä½•æœ€ä½³ä½¿ç”¨ SearchGPTã€‚**\n\n## SearchGPT å¿«é€Ÿå…¥é—¨\n\nå¦‚æžœæ‚¨å·²ç»çŸ¥é“å¦‚ä½•å¼€å§‹ä½¿ç”¨ SearchGPTï¼Œè¯·è·³è¿‡ä¸‹ä¸€éƒ¨åˆ†ã€‚\n\næœ‰ä¸‰ç§æ–¹æ³•å¯ä»¥å¼€å§‹ä½¿ç”¨ SearchGPTï¼š\n\n1. **åœ¨èŠå¤©ä¸­ä½¿ç”¨ SearchGPT** â€” æ‰“å¼€ä¸€ä¸ªæ™®é€šçš„ GPT-4o èŠå¤©ï¼Œå¹¶ç‚¹å‡»æœç´¢å›¾æ ‡ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*-p9GcMd7fRlwmXdHvLBzvw.png)\n\né€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ‚¨åœ¨èŠå¤©æ¡†ä¸­è¾“å…¥çš„æ¯ä¸ªæç¤ºéƒ½ä¼šè§¦å‘ç½‘ç»œæœç´¢ã€‚å®ƒå°†è¿”å›žå“åº”ä»¥åŠæ¥æºï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*WQMDebhc5g61YvpaQUDeYw.png)\n\n2\\. **SearchGPT ç›´æŽ¥ä½¿ç”¨ â€”** å¯¼èˆªè‡³æ­¤é“¾æŽ¥ï¼š<https://chatgpt.com/search> å¹¶å¼€å§‹æœç´¢ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*b-nIPELvT1heBlkbOcKAzQ.png)\n\næ‚¨å¯ä»¥é€šè¿‡ç‚¹å‡»å·¦ä¾§çš„é“¾æŽ¥å›¾æ ‡æ‰“å¼€æ¥æºï¼Œä¹Ÿå¯ä»¥é€šè¿‡ç‚¹å‡»å›¾åƒå›¾æ ‡æŸ¥çœ‹æ›´å¤šåª’ä½“ã€‚\n\n3\\. **ä½¿ç”¨ Chrome æ‰©å±•ç¨‹åºå°†é»˜è®¤æœç´¢å¼•æ“Žåˆ‡æ¢ä¸º SearchGPT â€”** å¯¼èˆªè‡³ [æ­¤é“¾æŽ¥](https://chromewebstore.google.com/detail/chatgpt-search/ejcfepkfckglbgocfkanmcdngdijcgld?pli=1) å°†æ‰©å±•ç¨‹åºæ·»åŠ åˆ° Chromeã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*hGkw33h7uEP7rFwCtpbr7g.png)\n\nChrome æ‰©å±•ç¨‹åºåœ¨æ‚¨æ¯æ¬¡åœ¨æœç´¢æ¡†ä¸­è¾“å…¥æŸ¥è¯¢åŽä¼šé‡å®šå‘åˆ° SearchGPTã€‚è™½ç„¶æˆ‘ä¸ä¸€å®šæŽ¨èè¿™æ ·åšï¼ˆGoogle åœ¨æŸäº›äº‹æƒ…ä¸Šä»ç„¶æœ‰å…¶ç”¨å¤„ï¼Œæˆ‘ç¨åŽä¼šè®¨è®ºï¼‰ï¼Œä½†æ‚¨å¯ä»¥å°è¯•ä¸€ä¸‹ï¼Œçœ‹çœ‹å®ƒå¯¹æ‚¨æ¥è¯´æ•ˆæžœå¦‚ä½•ã€‚\n\nè™½ç„¶ä½¿ç”¨ ChatGPT çš„æ¯ç§æ–¹æ³•éƒ½æ˜¯æœ‰æ•ˆçš„ï¼Œä½†æˆ‘ä¸ªäººæ›´å–œæ¬¢åœ¨æ™®é€šçš„ ChatGPT ç•Œé¢ä¸­ä½¿ç”¨å®ƒï¼ˆé€‰é¡¹ 1ï¼‰ã€‚å› æ­¤ï¼Œæœ¬æ–‡å¤§éƒ¨åˆ†å†…å®¹å°†é›†ä¸­åœ¨é€‰é¡¹ 1 ä¸Šã€‚\n\n## ç†è§£äº’è”ç½‘æœç´¢çš„è‰ºæœ¯\n\nåœ¨æˆ‘ä»¬æ·±å…¥æŽ¢è®¨æˆ‘æ‰€æ‰¿è¯ºçš„é«˜çº§ SearchGPT ç­–ç•¥ä¹‹å‰ï¼Œæˆ‘ä»¬å¿…é¡»äº†è§£æœç´¢æ˜¯ä»€ä¹ˆã€‚äº’è”ç½‘æœç´¢å®žé™…ä¸Šæ˜¯ä¸€ä¸ªç›¸å½“å¤æ‚çš„ä¸»é¢˜ã€‚å³ä½¿ä½ ä»Žæœªæœ‰æ„è¯†åœ°æ€è€ƒè¿‡ï¼Œå®ƒä¹Ÿåœ¨ä½ çš„è„‘æµ·ä¸­æ ¹æ·±è’‚å›ºåœ°å­˜åœ¨ç€å¤šæ¡äº’è”ç½‘æœç´¢è·¯å¾„ã€‚ä½ éœ€è¦äº†è§£ SearchGPT å¦‚ä½•åœ¨è¿™äº›è·¯å¾„ä¸­æä¾›å¸®åŠ©ï¼ˆæˆ–ä¸æä¾›å¸®åŠ©ï¼‰ã€‚\n\n### äº’è”ç½‘æœç´¢çš„7ç§è·¯å¾„\n\nåœ¨äº’è”ç½‘æœç´¢é¢†åŸŸï¼Œæœ‰è®¸å¤šé€šç”¨è·¯å¾„æ¶µç›–äº†å¤§å¤šæ•°æ´»åŠ¨ã€‚ä»¥ä¸‹æ˜¯æ¯ç§è·¯å¾„çš„ç®€è¦æ¦‚è¿°ï¼š\n\n1. **å¯¼èˆªæœç´¢**ï¼šå½“ä½ æƒ³å¿«é€Ÿåˆ°è¾¾ç‰¹å®šç½‘ç«™æ—¶ï¼Œä½ é€šè¿‡åç§°æˆ–å“ç‰Œè¿›è¡Œæœç´¢ï¼ˆä¾‹å¦‚ï¼Œâ€œäºšé©¬é€Šâ€æˆ–â€œFacebook ç™»å½•â€ï¼‰ã€‚\n2. **ä¿¡æ¯æœç´¢ï¼ˆç›´æŽ¥å’Œé—´æŽ¥ï¼‰**ï¼šå½“ä½ æƒ³äº†è§£æŸä¸ªä¸»é¢˜æˆ–å¯»æ‰¾ç­”æ¡ˆæ—¶ï¼Œä½ ä½¿ç”¨å¼€æ”¾å¼æŸ¥è¯¢ï¼ˆç›´æŽ¥ï¼šâ€œæ°”å€™å˜åŒ–æ˜¯ä»€ä¹ˆï¼Ÿâ€é—´æŽ¥ï¼šâ€œæ°”å€™å˜åŒ–çš„äº‹å®žâ€ï¼‰ã€‚\n3. **äº¤æ˜“æœç´¢**ï¼šå½“ä½ å‡†å¤‡è¿›è¡Œè´­ä¹°æˆ–æ³¨å†ŒæŸé¡¹æœåŠ¡æ—¶ï¼Œä½ å¸¦ç€å®ŒæˆæŸä¸ªåŠ¨ä½œçš„æ„å›¾è¿›è¡Œæœç´¢ï¼ˆä¾‹å¦‚ï¼Œâ€œè´­ä¹° iPhone 15â€æˆ–â€œæ³¨å†Œ Mediumâ€ï¼‰ã€‚\n4. **å•†ä¸šè°ƒæŸ¥**ï¼šå½“ä½ åœ¨è´­ä¹°å‰æ¯”è¾ƒäº§å“æˆ–æœåŠ¡æ—¶ï¼Œä½ ä½¿ç”¨â€œæœ€ä½³â€æˆ–â€œé¡¶çº§â€ç­‰å…³é”®è¯è¿›è¡Œæœç´¢ï¼ˆä¾‹å¦‚ï¼Œâ€œå­¦ç”Ÿæœ€ä½³ç¬”è®°æœ¬ç”µè„‘â€æˆ–â€œ2024 å¹´é¡¶çº§ç¬”è®°æœ¬ç”µè„‘â€ï¼‰ã€‚\n5. **ç²¾ç‚¼å’Œè¿­ä»£æœç´¢**ï¼šå½“ä½ å¤šæ¬¡ç²¾ç‚¼æœç´¢ä»¥æ›´æŽ¥è¿‘æ‰€éœ€å†…å®¹æ—¶ï¼Œä½ è°ƒæ•´å…³é”®è¯å’ŒæŽªè¾žï¼ˆä¾‹å¦‚ï¼Œä»Žâ€œGDPR è§„åˆ™â€åˆ°â€œæ¬§ç›Ÿä¼ä¸šæ•°æ®éšç§â€ï¼‰ã€‚è¿™æ˜¯å¤§å¤šæ•°â€œæœç´¢å…”å­æ´žâ€çš„åŸºç¡€ã€‚\n6. **çºµå‘æœç´¢**ï¼šå½“ä½ åœ¨å¤šä¸ªæ ‡ç­¾é¡µå’Œä¼šè¯ä¸­è¿›è¡Œæ‰©å±•ç ”ç©¶æ—¶ï¼Œä½ å¯èƒ½ä¼šç•™ä¸‹æ‰“å¼€çš„æ ‡ç­¾ä»¥ä¾¿åŽç»­å‚è€ƒï¼Œå¹¶åœ¨æ¯æ¬¡æœç´¢æ—¶ç»§ç»­ç²¾ç‚¼ï¼ˆä¾‹å¦‚ï¼Œå½“ä½ åœ¨æ¯”è¾ƒæ½œåœ¨çš„èˆªç­è¡Œç¨‹æ—¶ï¼‰ã€‚\n7. **å·²çŸ¥é¡¹ç›®æœç´¢**ï¼šå½“ä½ å¯»æ‰¾ç‰¹å®šå†…å®¹æ—¶ï¼Œä½ çŸ¥é“è¯¥å†…å®¹å­˜åœ¨ï¼Œä½ é€šè¿‡æ ‡é¢˜ã€åç§°æˆ–ç‹¬ç‰¹ç»†èŠ‚è¿›è¡Œæœç´¢ï¼ˆä¾‹å¦‚ï¼Œâ€œNY Times å…³äºŽè¿œç¨‹å·¥ä½œçš„æ–‡ç« ï¼Œä½œè€…ï¼šJane Doeâ€ï¼‰ã€‚è¿™ä¸Žå¯¼èˆªæœç´¢ç›¸ä¼¼ï¼Œä½†æ›´å…·é’ˆå¯¹æ€§ã€‚\n\n### äº’è”ç½‘æœç´¢çš„å…¶ä»–è·¯å¾„ï¼ˆæˆ‘çš„å‘çŽ°ï¼‰\n\næˆ‘è§‚å¯Ÿåˆ°äº†ä¸€äº›æˆ‘ç›¸ä¿¡ä½ ä»¬ä¸­è®¸å¤šäººéƒ½å¾ˆç†Ÿæ‚‰çš„å…¶ä»–è·¯å¾„ã€‚è™½ç„¶è¿™äº›å¹¶æ²¡æœ‰æ­£å¼è¢«è®¤å¯ï¼Œä½†æˆ‘è§‰å¾—åœ¨è¿™é‡Œæåˆ°å®ƒä»¬æ˜¯å€¼å¾—çš„ï¼š\n\n8\\. **äººå·¥éªŒè¯æœç´¢ï¼š** å½“ä½ ä¸“é—¨å¯»æ‰¾çœŸå®žäººä»¬å¯¹æŸä¸ªä¸»é¢˜çš„çœ‹æ³•æ—¶ï¼ˆä¾‹å¦‚ï¼Œåœ¨æœç´¢æŸ¥è¯¢çš„[æœ«å°¾æ·»åŠ â€œRedditâ€](https://detailed.com/forum-serps/)ï¼‰ã€‚\n\n9\\. **åSEOæœç´¢ï¼š** ç±»ä¼¼äºŽä¿¡æ¯æœç´¢ï¼Œä½†ä½ æ˜¯åœ¨å¯»æ‰¾æ¥è‡ªçœŸæ­£å¯ä¿¡æ¥æºçš„ä¿¡æ¯ï¼Œè€Œä¸æ˜¯é‚£äº›æ‹¥æœ‰æœ€ä½³[SEOç­–ç•¥](https://www.theverge.com/features/23931789/seo-search-engine-optimization-experts-google-results)ä½†ç¼ºä¹çœŸå®žä¿¡æ¯çš„ç½‘ç«™ï¼ˆä¾‹å¦‚ï¼Œæ²®ä¸§åœ°æ»šåŠ¨æµè§ˆæœªçŸ¥ç½‘ç«™ï¼Œç›´åˆ°æ‰¾åˆ°ä¸€ä¸ªä½ ä¿¡ä»»çš„ç½‘ç«™ï¼‰ã€‚\n\n10\\. **SOSæœç´¢ï¼š** åœ¨ç½‘ç»œä¸Šå¯»æ‰¾ä»»ä½•è®ºå›ä¸Šæœ‰ç›¸åŒé—®é¢˜çš„äººçš„å¸®åŠ©ï¼ˆæ— è®ºå¤šä¹ˆå†·é—¨ï¼‰ï¼ˆä¾‹å¦‚ï¼Œæœç´¢â€œå½“HMDI 1æ’å…¥æ—¶ï¼ŒSamsungç”µè§†æ— æ³•å¼€æœºâ€ï¼‰ã€‚\n\n11\\. **åç¡®è®¤åè¯¯æœç´¢ï¼š** å½“ä½ å’Œæœ‹å‹ä»¬è¿›è¡Œè¾©è®ºæ—¶ï¼Œä½ å¿…é¡»æœç´¢æœ€[æ— åè§çš„æœ¯è¯­](https://dl.acm.org/doi/10.1145/3635034)ï¼ˆä¾‹å¦‚ï¼Œè¯æ˜Žä½ çš„è®ºç‚¹æ˜¯æ­£ç¡®çš„ï¼ŒåŒæ—¶æ»¡è¶³ä½ è¾©è®ºä¼™ä¼´çš„è§‚ç‚¹ï¼‰ã€‚\n\n12\\. **å·²çŸ¥æœªçŸ¥æœç´¢ï¼š** å½“ä½ å¯¹ä½ è¦æŸ¥æ‰¾çš„äº‹ç‰©æœ‰ä¸€äº›äº†è§£ï¼Œä½†ä¸ç¡®å®šå¦‚ä½•æ‰¾åˆ°å®ƒï¼ˆä¾‹å¦‚ï¼Œæœç´¢â€œæœ‰ä¸€ä¸ªè€æ—§çš„youtubeè§†é¢‘ï¼Œæ˜¯ä¸€ä¸ªå­©å­å› ä¸ºå¤±åŽ»æŸä¸ªè§†é¢‘æ¸¸æˆè€Œå´©æºƒï¼Œè¿˜æœ‰ä¸€ç³»åˆ—å…³äºŽä»–ä»¬çš„è§†é¢‘â€¦â€¦â€ï¼‰ã€‚\n\nçŽ°åœ¨ä½ å¯¹ä¸åŒç±»åž‹çš„æœç´¢æœ‰äº†äº†è§£ï¼Œæˆ‘ä»¬ç»ˆäºŽå¯ä»¥æ·±å…¥æŽ¢è®¨SearchGPTå¦‚ä½•å¸®åŠ©æ¯ä¸€ç§æœç´¢ã€‚å‡†å¤‡å¥½äº†å—ï¼\n\n## SearchGPT ç­–ç•¥â€” ç»†èŠ‚åˆ†æž\n\næœ¬èŠ‚å°†è¯¦ç»†åˆ†æžæˆ‘ä¸Šé¢æå‡ºçš„æ¯ä¸ªæœç´¢è·¯å¾„ä¸­ SearchGPT çš„ä¼˜ç¼ºç‚¹ã€‚æ¯ä¸ªå­éƒ¨åˆ†éƒ½é…å¤‡äº†ç¤ºä¾‹ã€å»ºè®®å’Œä¼˜åŒ–ä½¿ç”¨ SearchGPT çš„æŠ€å·§ã€‚è®©æˆ‘ä»¬å¼€å§‹å§ï¼\n\n### SearchGPT ç”¨äºŽå¯¼èˆªå’Œå·²çŸ¥é¡¹ç›®æœç´¢\n\nå¦çŽ‡åœ°è¯´ï¼ŒSearchGPT åœ¨è¿™é‡Œå¹¶ä¸ä¼šç»™æˆ‘ä»¬å¸¦æ¥å¤ªå¤§å¸®åŠ©ã€‚åœ¨ Google ä¸­è¾“å…¥â€œmediumâ€ä¸Žåœ¨ SearchGPT ä¸­è¾“å…¥å¹¶æ²¡æœ‰æ•ˆçŽ‡ä¸Šçš„æå‡ã€‚ç„¶è€Œï¼Œæœ‰äº›ç½‘é¡µå¯èƒ½éš¾ä»¥è®¿é—®ï¼Œå°¤å…¶æ˜¯å½“ä½ å¿˜è®°å°†å…¶æ·»åŠ åˆ°ä¹¦ç­¾æ—¶ã€‚è¿™æ­£æ˜¯ SearchGPT å‘æŒ¥ä½œç”¨çš„åœ°æ–¹ã€‚\n\nä»¥ä¸‹æ˜¯ä¸€ä¸ªå¿«é€Ÿç¤ºä¾‹ã€‚æœ‰æ—¶æˆ‘å‘çŽ°å¾ˆéš¾è®¿é—®æŸäº›ç½‘ç«™çš„ API è´¦æˆ·ä¿¡æ¯ã€‚ä»¥æˆ‘çš„ Perplexity AI API è´¦æˆ·ä¸ºä¾‹ã€‚æˆ‘èŠ±è´¹äº†å¾ˆé•¿æ—¶é—´æ‰èƒ½æ‰¾åˆ°è¿™ä¸ªé¡µé¢ï¼Œè€Œæˆ‘ä¸€ç›´æ²¡æœ‰å°†å…¶æ·»åŠ åˆ°ä¹¦ç­¾ä¸­ã€‚ç„¶è€Œï¼Œå€ŸåŠ© SearchGPTï¼Œæˆ‘å¯ä»¥ç«‹å³æ‰¾åˆ°å®ƒï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*sSi2EcTPGE3SNg_8qZnRog.png)\n\n**SearchGPT å¯¼èˆªæœç´¢çš„æç¤ºï¼š**\n\n* åœ¨å¯»æ‰¾å†…å®¹æ—¶è¦è¯¦ç»†æè¿°ã€‚ä¸è¦å®³æ€•ç›´æŽ¥å†™å‡ºæ¥\n* å¦‚æžœä½ æ‰¾ä¸åˆ°æ‰€éœ€çš„å†…å®¹ï¼Œå¯ä»¥è¯· GPT å‘ *ä½ * æå‡ºæ¾„æ¸…é—®é¢˜ï¼Œä»¥ä¾¿æ›´å¥½åœ°å¸®åŠ©ä½ \n* å¯¹äºŽå·²çŸ¥é¡¹ç›®æœç´¢ï¼ˆå³å…³äºŽè‡ªåŠ¨åŒ–æç¤ºå·¥ç¨‹çš„ Jordan Gibbs Medium æ–‡ç« ï¼‰ï¼Œå®ƒå¯èƒ½ä¼šæœ‰æ‰€å¸®åŠ©ï¼Œä½†å†ä¸€æ¬¡ï¼Œå®ƒåœ¨è¿™æ–¹é¢å¹¶æ²¡æœ‰çœŸæ­£ä¼˜äºŽ Googleã€‚\n\nè¿™æ˜¯ä½¿ç”¨ SearchGPT çš„å½±å“åŠ›æœ€å°çš„æ–¹æ³•ä¹‹ä¸€ï¼Œä½†å¶å°”ä»ç„¶å¯ä»¥æ´¾ä¸Šç”¨åœºã€‚\n\n### SearchGPT ä¿¡æ¯æœç´¢ï¼ˆç›´æŽ¥å’Œé—´æŽ¥ï¼‰\n\nSeachGPT éžå¸¸é€‚åˆå¿«é€ŸèŽ·å–ç‰¹å®šå†…å®¹æˆ–èšåˆæœ€æ–°ä¿¡æ¯ã€‚æ²¡æœ‰æœç´¢åŠŸèƒ½çš„è€ç‰ˆ ChatGPT æ€»æ˜¯æœ‰ä»¤äººæ¼ç«çš„çŸ¥è¯†æˆªæ­¢æ—¥æœŸã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çŽ°åœ¨å¯ä»¥ç›´æŽ¥é—®å®ƒä¸€äº›å®ƒâ€œä¸çŸ¥é“â€çš„äº‹æƒ…ï¼Œå› ä¸ºå®ƒå¯ä»¥é€šè¿‡äº’è”ç½‘æŸ¥æ‰¾ã€‚æˆ‘ä»¬è¿˜å¯ä»¥å¤§å¹…å‡å°‘å¹»è§‰ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥æ›´ä¿¡ä»»å®ƒï¼åªéœ€ç¡®ä¿å§‹ç»ˆæ£€æŸ¥å…¶é‡è¦ä¿¡æ¯çš„æ¥æº :)\n\nä½œä¸ºæ¼”ç¤ºï¼Œå‡è®¾æˆ‘æƒ³è°ƒæŸ¥æˆ‘å–œæ¬¢ä½¿ç”¨çš„æ•°æ®åº”ç”¨å¹³å° Streamlit çš„ä¸€äº›æœ€æ–°è¿›å±•ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MSaJSdJGJ1vGa3uVnfEKZg.png)\n\nçŽ°åœ¨ï¼Œæˆ‘å¯ä»¥è½»æ¾åœ°è®© ChatGPT è®¿é—®è¿™äº›æœ€æ–°çš„æ–‡æ¡£ï¼Œä»¥ä¾¿å®ƒå¯ä»¥ä½¿ç”¨å…¨æ–°çš„åŠŸèƒ½ä¸ºæˆ‘ç¼–å†™ä»£ç ã€‚\n\nè¿™åªæ˜¯ SearchGPT å¦‚ä½•æ”¹å–„ä¿¡æ¯æœç´¢çš„ä¸€ä¸ªä¾‹å­ï¼Œä½†è¿™é‡Œè¿˜æœ‰ä¸€äº›æ›´æœ‰ç”¨çš„æç¤ºå’ŒæŠ€å·§ã€‚\n\n**SearchGPT ä¿¡æ¯æœç´¢ï¼ˆç›´æŽ¥ï¼‰çš„æç¤ºï¼š**\n\n* å¦‚æžœæ‚¨æƒ³è¦æœ€æ–°ä¿¡æ¯ï¼Œè¯·å§‹ç»ˆæåŠå½“å‰å¹´ä»½æˆ–â€œä»Šå¤©â€\n* æ‚¨å¯ä»¥ä½¿ç”¨æç¤ºâ€œè¯·æ£€æŸ¥å¤šä¸ªæ¥æºå¹¶èšåˆå®ƒä»¬çš„ä¿¡æ¯â€æ¥èšåˆæœ‰å…³æŸä¸€ä¸»é¢˜çš„ä¿¡æ¯ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*HFKD-3f89gK8do6Dqu01gg.png)\n\n* å¦‚æžœç¬¬ä¸€æ¬¡å›žå¤ä¸å¤Ÿæ–°æˆ–ä¸å¤Ÿå…·ä½“ï¼Œè¯·å°è¯•åŽç»­æç¤º\n* å°†æœç´¢è¿‡ç¨‹è§†ä¸ºä¸Žä¸“å®¶çš„å¯¹è¯ï¼ˆä¸è¦å®³æ€•å†™ä¸‹ä»»ä½•æƒ³åˆ°çš„å†…å®¹ï¼‰\n* æ‚¨å¯ä»¥é€šè¿‡ç‰¹åˆ«æåŠæŸä¸ªæ¥æºæ¥è¦æ±‚å®ƒâ€œæ·±å…¥æŽ¢è®¨â€ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*XbXKRKy5WkF0QX1X0yzr5w.png)\n\n* æ’å…¥æ­¤æç¤ºï¼šâ€œåœ¨æ‚¨å›žå¤åŽåˆ—å‡ºä¸€äº›æ½œåœ¨çš„åŽç»­é—®é¢˜â€ï¼Œå¦‚æžœæ‚¨å¸Œæœ›èŽ·å¾—å¼•å¯¼å¼æœç´¢ä½“éªŒ\n* ä¸è¦å®³æ€•å¼ºè¿«å®ƒå¼•ç”¨æ¥æºã€‚æœ‰æ—¶ï¼Œå®ƒå¯èƒ½ä¼šç¨å¾®æŠµåˆ¶ï¼Œå¹¶ä¾èµ–äºŽçŽ°æœ‰çŸ¥è¯†ã€‚æ‚¨å¯ä»¥ç®€å•åœ°é€šè¿‡å£°æ˜Žï¼šâ€œæ— è®ºå¦‚ä½•ï¼Œæ‚¨å¿…é¡»ä¸ºæ‚¨æå‡ºçš„æ¯ä¸ªè§‚ç‚¹å¼•ç”¨ä¸€ä¸ªæ¥æºâ€\n\né—´æŽ¥ä¿¡æ¯æœç´¢ï¼ˆå³ï¼Œæ— ç›®æ ‡æœç´¢ï¼‰åœ¨ SearchGPT ä¸­éžå¸¸å‡ºè‰²ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›æç¤ºã€‚\n\n**SearchGPT ä¿¡æ¯æœç´¢ï¼ˆé—´æŽ¥ï¼‰çš„æç¤ºï¼š**\n\n*æ³¨æ„ï¼šæœ¬èŠ‚è¿˜æ¶µç›–äº†ä¸Šè¿°æåˆ°çš„â€œç²¾ç‚¼å’Œè¿­ä»£æœç´¢â€ã€â€œçºµå‘æœç´¢â€å’Œâ€œå·²çŸ¥æœªçŸ¥æœç´¢â€ã€‚*\n\n* è¯·å®ƒä¸ºæ‚¨åˆ›å»ºä¸€ä¸ªå­¦ä¹ å¤§çº²ï¼Œä»¥ä¾¿æ‚¨å¯ä»¥é€æ­¥å­¦ä¹ æ‚¨éœ€è¦äº†è§£çš„æ¦‚å¿µ\n* åœ¨å¼€å§‹æœç´¢ä¹‹å‰ï¼Œè¯·æ±‚å®ƒå‘æ‚¨æå‡ºæ¾„æ¸…é—®é¢˜ï¼Œä»¥ä¾¿æ‚¨å¯ä»¥æ›´å¿«åœ°ç¼©å°è¦æŸ¥æ‰¾çš„å†…å®¹ã€‚ä½¿ç”¨æç¤ºï¼šâ€œåœ¨æ‚¨å¼€å§‹ä¹‹å‰ï¼Œè¯·è¯¢é—®æˆ‘æœ‰å…³æˆ‘çš„å­¦ä¹ ç›®æ ‡çš„æ¾„æ¸…é—®é¢˜ã€‚â€ä»¥ä¸‹æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*LE2mSqStwuy18ryNVQbZQw.png)\n\n* è¯·å®ƒæŽ¨è YouTube è§†é¢‘ã€‚SearchGPT å¯ä»¥æ‰¾åˆ°ä¸Žæ‚¨çš„è¯·æ±‚é«˜åº¦ç›¸å…³çš„è§†é¢‘ï¼Œä»Žè€Œä½¿æ‚¨çš„å­¦ä¹ ä½“éªŒæ›´åŠ å¤šåª’ä½“åŒ–ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*7zhXPezwiMskkEcvAEfqjQ.png)\n\n* è¯·å®ƒæŽ¨èç…§ç‰‡ã€å›¾è¡¨å’Œå›¾å½¢ä½œä¸ºè§†è§‰è¾…åŠ©ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*FItZcImfTJVenkzy1lJiqw.png)\n\n* å¦ä¸€ç§æ–¹æ³•æ˜¯æˆ‘ç§°ä¹‹ä¸ºâ€œå•ä¸€æ¥æºåˆ†æ”¯â€ï¼Œæ‚¨å¯ä»¥è®© SearchGPT æ€»ç»“ä¸€ä¸ªæ‚¨ä¿¡ä»»å¹¶åŒ…å«ä¸»é¢˜å…¨é¢æ¦‚è¿°çš„å•ä¸€æ¥æºï¼Œç„¶åŽä»Žé‚£é‡Œåˆ†æ”¯å‡ºåŽ»ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*P8C4hpd8AxZgfuXDW0PNxg.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*_bHNNzZAukNMPIk8W6hhEQ.png)\n\n* ä¸ºäº†æ›´æ·±å…¥åœ°æŽ¢è®¨ï¼Œå¹¶é¿å…â€œæ±¡æŸ“â€GPT çš„ä¸Šä¸‹æ–‡çª—å£ï¼Œæˆ‘å»ºè®®å¦‚æžœæ‚¨æƒ³è®¨è®ºæŸä¸ªéžå¸¸å…·ä½“çš„å†…å®¹ï¼Œå¯ä»¥æ‰“å¼€ä¸€ä¸ªæ–°èŠå¤©ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*G6Cxxvd7uGn-fS4pFkvyiw.png)\n\n* æ‚¨å§‹ç»ˆå¯ä»¥åœ¨æœç´¢æ ä¸­æœç´¢æ‚¨æœ€è¿‘çš„èŠå¤©è®°å½•ï¼Œå› æ­¤æ‚¨ä¸ä¼šä¸¢å¤±ä»»ä½•åŽ†å²è®°å½•ï¼\n\nä½¿ç”¨ SearchGPT è¿›è¡Œä¿¡æ¯æœç´¢æ˜¯ä¸å¯æ€è®®çš„ï¼Œæˆ‘å¯¹æ­¤æ„Ÿåˆ°æ— æ¯”å…´å¥‹ã€‚æˆ‘ä¸€ç›´å¾ˆæœŸå¾…åŠ å¿«æˆ‘æƒ³å­¦ä¹ çš„æ‰€æœ‰å†…å®¹çš„å­¦ä¹ è¿›ç¨‹ï¼\n\n### SearchGPTç”¨äºŽäº¤æ˜“å’Œå•†ä¸šè°ƒæŸ¥æœç´¢\n\næ˜¯çš„ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨SearchGPTæ‰¾åˆ°æ³¨å†Œæˆ–è´­ä¹°æœåŠ¡å’Œäº§å“çš„åœ°æ–¹ã€‚è™½ç„¶è¿™å¹¶ä¸ä¸€å®šæ¯”Googleæ›´å¿«æˆ–æ›´å¥½ï¼Œä½†å®ƒç¡®å®žæœ‰ä¸€äº›ä»¤äººå…´å¥‹çš„åŠŸèƒ½ï¼Œæ¯”å¦‚è¿™ä¸ªåœ°å›¾ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*zWb-fbtC-CHY4mi8ZR997w.png)\n\næ‚¨è¿˜å¯ä»¥æ‰¾åˆ°æ–°çš„å†…å®¹æ¶ˆè´¹åœ°ç‚¹ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*y1ryR4QJI7VUvyEgqEs_Sw.png)\n\nåœ¨SearchGPTä¸­ï¼Œè¿™æ–¹é¢çš„èŒƒå›´éžå¸¸æœ‰é™ï¼Œå› æ­¤è¿™å¯èƒ½æ˜¯Googleçš„å°èƒœåˆ© :) ä¸è¿‡ï¼Œå®ƒä»ç„¶æœ‰åŠ©äºŽå¿«é€ŸæŒ‡å‘æ­£ç¡®çš„åœ°æ–¹ï¼\n\næ‚¨è¿˜å¯ä»¥é€šè¿‡SearchGPTè´­ç‰©ã€‚ç„¶è€Œï¼Œå®ƒçš„æ–¹å¼å¹¶ä¸æ˜¯æ‚¨æ‰€æœŸæœ›çš„ã€‚ä¸è¦è®¤ä¸ºChatGPTå¯ä»¥ç›´æŽ¥æŒ‡å‘ç¡®åˆ‡çš„äº§å“é“¾æŽ¥ï¼ˆå®ƒå¯ä»¥ï¼Œä½†å®ƒç»å¸¸å‡ºçŽ°å¹»è§‰ï¼‰ï¼›ç›¸åï¼Œæ‚¨å¯ä»¥å°†å…¶è§†ä¸ºä¸€ç§äº§å“æ¯”è¾ƒå·¥å…·ã€‚\n\nè¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*9MTMHNyTY-L4WjGTZhg9BQ.png)\n\nå®ƒçŽ°åœ¨æŒ‡å‘æˆ‘å–œæ¬¢çš„æ¯›è¡£å“ç‰Œï¼šåœ†é¢†ã€ç²—é’ˆç»‡ã€å®½æ¾å’Œå¤§åœ°è‰²è°ƒï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*5a02uR0X07VFd4_2hXFCKg.png)\n\nè¿˜è®°å¾—æˆ‘ä¹‹å‰æåˆ°çš„è®¸å¤šäººåšæˆ‘æ‰€ç§°çš„â€œäººç±»éªŒè¯æœç´¢â€å—ï¼Ÿ\n\nä¸å¹¸çš„æ˜¯ï¼ŒSearchGPTä¼¼ä¹Žæ— æ³•è®¿é—®è®¸å¤šè®ºå›ï¼Œä¾‹å¦‚Redditã€‚è™½ç„¶å®ƒåœ¨è®­ç»ƒæ•°æ®ä¸­åŒ…å«äº†å¤§é‡çš„Redditå†…å®¹ï¼Œä½†å®ƒæ— æ³•è®¿é—®çŽ°ä»£çº¿ç¨‹ã€‚è¿™æ˜¯SearchGPTç›®å‰çš„ä¸€å¤§é™åˆ¶ï¼\n\n### SearchGPTç”¨äºŽåSEOæœç´¢\n\nåSEOæ–‡ç« æœç´¢æ˜¯ä¸€ç§æˆ‘çœ‹åˆ°çš„æ–°è¶‹åŠ¿ã€‚å…¶æ ¸å¿ƒæœ¬è´¨ä¸Šæ˜¯æˆ‘ä»¬å¤§è„‘ä¸­çš„ä¸€ç§è¿‡æ»¤å™¨ï¼Œè¯•å›¾æ‰¾åˆ°æˆ‘ä»¬ä¹‹å‰ç§¯æžäº’åŠ¨è¿‡çš„ç½‘ç«™æˆ–æˆ‘ä»¬çŸ¥é“é€šå¸¸å¯ä¿¡çš„æ¥æºã€‚ä»¥ä¸‹æ˜¯SearchGPTå¦‚ä½•å¸®åŠ©å®žçŽ°è¿™ä¸€ç›®æ ‡ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*dWUoRwDhfvwEE7K5S-u0Bg.png)\n\nè¿™ä¸ªå¿«é€Ÿçš„è¿‡ç¨‹å¯ä»¥æ¶ˆé™¤ä½ ç ”ç©¶ä¸­çš„è®¸å¤šç—›è‹¦ã€‚\n\n**åSEOæœç´¢æŠ€å·§**\n\n* ä½¿ç”¨è¿™ä¸ªæç¤ºè¿›è¡Œé¢„è¿‡æ»¤ï¼šâ€œæˆ‘åªæƒ³è¦å¯ä¿¡çš„æ¥æºã€‚ç»™æˆ‘ä¸€ä¸ªæ½œåœ¨æ¥æºçš„åˆ—è¡¨ï¼Œæˆ‘ä¼šé€‰æ‹©æˆ‘æƒ³å¬çš„é‚£äº›ã€‚â€\n* å¦ä¸€ç§æ–¹æ³•æ˜¯è¯´ï¼šâ€œä¸è¦è¾“å‡ºä»»ä½•ä¸çŸ¥åæ¥æºçš„å†…å®¹ã€‚â€\n\n### SearchGPTç”¨äºŽSOSæœç´¢\n\nSOSï¼ˆSave Our Soulsï¼‰æœç´¢å¾ˆæœ‰è¶£ã€‚ä½¿ç”¨Googleæ—¶ï¼Œä½ ä¼šæœ‰ä¸€ç§æƒ³è¦å‡†ç¡®æè¿°ä½ æ‰€ç»åŽ†çš„äº‹æƒ…çš„å†…åœ¨æ¸´æœ›ï¼Œä½†ä½ çŸ¥é“ä½ ä¸èƒ½è¿™æ ·åšã€‚ä½¿ç”¨SearchGPTï¼Œä½ å¯ä»¥å†™å‡ºä¸€ä»½å…³äºŽä½ æƒ…å†µçš„æ„è¯†æµæŠ¥å‘Šï¼Œå®ƒå¯ä»¥å°†ä½ çš„æœç´¢è½¬åŒ–ä¸ºä¸€ä¸ªæ™ºèƒ½æµï¼Œèƒ½å¤Ÿæ›´å¿«åœ°æ‰¾åˆ°é‚£äº›æ›¾ç»é‡åˆ°è¿‡ä½ é—®é¢˜çš„äººã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*107zPh98LewGZ8xuZ7aGWA.png)\n\nçŽ°åœ¨ï¼Œæˆ‘å¯ä»¥ä»Žåº•éƒ¨çš„æ½œåœ¨è§£å†³æ–¹æ¡ˆåˆ—è¡¨ä¸­ç¼©å°é—®é¢˜èŒƒå›´ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ZxGSV6vDOVV2SI566x4jwg.png)\n\nè¿™åœ¨è¿‡åŽ»å‡ å‘¨å·²ç»å¸®åŠ©äº†æˆ‘ï¼Œæˆ‘å¯¹å®ƒçš„è¡¨çŽ°éžå¸¸æ»¡æ„ã€‚\n\n**ä½¿ç”¨SearchGPTè¿›è¡ŒSOSæœç´¢çš„æç¤ºï¼š**\n\n* ä½¿ç”¨æ­¤æç¤ºå°†å…¶ç¼©å°åˆ°çœŸæ­£çš„äººç±»é—®é¢˜æè¿°ï¼šâ€œæ‰¾åˆ°ä¸€äº›æœ‰ç±»ä¼¼é—®é¢˜çš„äººçš„æè¿°ï¼Œå¹¶ç»™æˆ‘è¾“å‡ºä¸€ä¸ªæ¦‚è¿°â€\n* å¦‚æžœè¾“å‡ºçš„å»ºè®®è¿‡äºŽç¬¼ç»Ÿï¼Œè¯·ä½¿ç”¨æ­¤æç¤ºï¼šâ€œæˆ‘éœ€è¦ä¸Žæ­¤åœºæ™¯å®Œå…¨åŒ¹é…çš„é«˜åº¦å…·ä½“æ¡ˆä¾‹â€\n* ä¸è¦å®³æ€•è¿‡äºŽè¯¦ç»†ï¼›å¦‚æžœéœ€è¦ï¼Œå¯ä»¥å†™å‡ æ®µæ¦‚è¿°é—®é¢˜çš„åŽ†å²ã€‚SearchGPTå¯ä»¥ç­›é€‰å™ªéŸ³\n\n### SearchGPTç”¨äºŽåç¡®è®¤åè¯¯æœç´¢\n\nè¿™æ˜¯ä½¿ç”¨SearchGPTçš„ä¸€ä¸ªå®Œç¾Žæ¡ˆä¾‹ï¼Œå½“ä½ ä¸Žæœ‹å‹æ·±å…¥è¾©è®ºã€å‡†å¤‡è¾©è®ºæˆ–åªæ˜¯ä¸€èˆ¬æ€§åœ°å­¦ä¹ ä¸€ä¸ªæœ‰äº‰è®®çš„è¯é¢˜æ—¶ã€‚è¿™ç§æ–¹æ³•è®©ä½ èƒ½å¤ŸåŒæ—¶çœ‹åˆ°é—®é¢˜çš„å¤šä¸ªæ–¹é¢ï¼Œä»Žè€Œä¸ºä½ çš„è§‚ç‚¹å¢žæ·»ä¸€äº›ç»†å¾®å·®åˆ«ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*5-FLP0-R1K8oW6KDSE5jLQ.png)\n\nè¯¥å›žåº”å±•ç¤ºäº†æ‰€æœ‰3ç§è§‚ç‚¹ï¼Œå¹¶ä¸ºæ¯ç§è§‚ç‚¹æä¾›äº†ä¸åŒçš„è®ºæ®ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*u5Npy5K8FoKnOcvh0t3ZSw.png)\n\n**åç¡®è®¤åè¯¯æœç´¢çš„æŠ€å·§ï¼š**\n\n* å°è¯•ä¸‰ç§è§‚ç‚¹çš„æ–¹æ³•ï¼šâ€œåœ¨ç½‘ä¸Šæœç´¢å…³äºŽ\\[INSERT SUBJECT\\]çš„æ¦‚è¿°åŠå…¶åˆ©å¼Šï¼Œæ¥è‡ªè¿™ä¸‰ç§è§‚ç‚¹ï¼š1\\. æ”¯æŒè€… 2\\. æ‰¹è¯„è€… 3\\. ä¸­ç«‹è§‚ç‚¹â€\n* å¼€å§‹æ—¶ä½¿ç”¨ä¸­ç«‹è¯­è¨€ã€‚ä¾‹å¦‚ï¼Œä¸è¦æœç´¢â€œä¸ºä»€ä¹ˆæ·»åŠ ç³–æ¯”å¤©ç„¶ç³–ä¸å¥åº·â€ï¼Œè€Œæ˜¯è¾“å…¥â€œå¤©ç„¶ç³–å’Œæ·»åŠ ç³–åœ¨å¥åº·å½±å“ä¸Šçš„å·®å¼‚å’Œç›¸ä¼¼æ€§çš„æ¦‚è¿°ã€‚â€SearchGPTåœ¨è¿™é‡Œå¾ˆæœ‰å¸®åŠ©ï¼Œå› ä¸ºå®ƒå¯ä»¥æ±‡æ€»å¤šç§æ¥æºçš„è§‚ç‚¹\n* ç¡®ä¿ä½¿ç”¨å¤šä¸ªæ¥æºï¼ˆå› ä¸ºæ‰€æœ‰æ¥æºè‡³å°‘éƒ½æœ‰ä¸€äº›åè§ï¼Œå› æ­¤è¿™æœ‰åŠ©äºŽåœ¨ä¸€å®šç¨‹åº¦ä¸Šå¹³è¡¡è¿™äº›åè§ï¼‰\n\n## Prompting and General Tips for SearchGPT\n\nçŽ°åœ¨æˆ‘ä»¬å·²ç»æ·±å…¥æŽ¢è®¨äº†ä½¿ç”¨ SearchGPT å¯ä»¥è¿›è¡Œçš„æœç´¢ç±»åž‹ï¼Œä»¥ä¸‹æ˜¯æˆ‘æŽ¨èçš„æ›´å¤šé€šç”¨æç¤ºå’Œæç¤ºå·¥ç¨‹æŠ€å·§ã€‚è®¸å¤šè¿™äº›å†…å®¹ä¹‹å‰å·²ç»æåˆ°è¿‡ï¼Œä½†è¿™æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„æ€»ç»“ã€‚\n\n### SearchGPT çš„æ³¨æ„äº‹é¡¹\n\n* è¦æ±‚å®ƒä½¿ç”¨å¤šä¸ªæ¥æº â€” â€œè¯·æ£€æŸ¥å„ç§æ¥æºâ€\n* åˆ›å»ºä¸€ä¸ªä¸Ž SearchGPT çš„å¼•å¯¼è·¯å¾„ â€” â€œåœ¨ä½ å›žåº”ä¹‹åŽåˆ—å‡ºä¸€äº›æ½œåœ¨çš„åŽç»­é—®é¢˜â€\n* åœ¨ä½ çš„è¯·æ±‚ä¸­ä½¿ç”¨å…·ä½“æ—¥æœŸ â€” â€œæˆªè‡³ä»Šå¤©â€ã€â€œæˆªè‡³ 2024 å¹´â€æˆ–â€œåœ¨ 1983 å¹´â€\n* å¦‚æžœ SearchGPT æ²¡æœ‰æä¾›ä»»ä½•æ¥æºï¼Œæ‰“å¼€ä¸€ä¸ªæ–°èŠå¤©å¹¶åœ¨ä½ çš„æç¤ºæœ«å°¾æ’å…¥â€œæœç´¢ç½‘ç»œâ€\n* åœ¨ä½ äº¤æµè¶…è¿‡ 20 æ¡æ¶ˆæ¯åŽæ‰“å¼€ä¸€ä¸ªæ–°èŠå¤©ï¼Œä½ ä¸å¸Œæœ›å®ƒçš„ä¸Šä¸‹æ–‡çª—å£å˜å¾—å¤ªæ»¡\n* è¦è¯¦ç»† â€” GPT å¯¹ä½ çš„è¯·æ±‚æœ‰æ›´å¤šä¸Šä¸‹æ–‡æ—¶ï¼Œæ•ˆæžœä¼šæ›´å¥½\n\n### SearchGPT çš„ç¦å¿Œ\n\n* ä¸è¦æ¨¡ç³Šï¼ˆä¾‹å¦‚ï¼Œâ€œ2024 å¹´çš„æ–°ä½æˆ¿å±æœºâ€æˆ–â€œ2024 å¹´çš„é…·ç‚«è·‘è½¦â€ï¼‰ã€‚å¯ä»¥ç”¨è¿™ä¸ªæç¤ºæ¥è§£å†³ï¼šâ€œé—®æˆ‘ä¸€äº›å…³äºŽæˆ‘çš„è¯·æ±‚çš„æ¾„æ¸…é—®é¢˜ï¼Œæˆ‘ä¼šåœ¨ä½ å¼€å§‹ä¹‹å‰å›žç­”â€\n* ä¸è¦æœªç»æŸ¥è¯å°±è®¤ä¸ºæŸäº‹æ˜¯ç»å¯¹æ­£ç¡®çš„ï¼›å¹»è§‰ä»ç„¶æ˜¯å¯èƒ½çš„ã€‚ä½ å¯ä»¥éšæ—¶è¯´â€œç”¨å¦ä¸€ä¸ªä¸åŒçš„æ¥æºéªŒè¯æ­¤ä¿¡æ¯â€\n* ä¸è¦è¦æ±‚å…·ä½“çš„æ„è§ï¼›å®ƒä¼šå‘Šè¯‰ä½ ä½ æƒ³å¬çš„å†…å®¹ã€‚è¯·ç¡®ä¿ä½ çš„æç¤ºæ˜¯ä¸€èˆ¬æ€§å’Œå¼€æ”¾çš„\n* ä¸è¦å®³æ€•å¯¹ SearchGPT æå‡ºâ€œè¿‡å¤šâ€çš„è¦æ±‚ã€‚å®ƒå…·æœ‰çš„æŸäº›èƒ½åŠ›æˆ‘ç”šè‡³è¿˜æ²¡æœ‰å‘çŽ°ï¼Œè€Œå¤§é‡æé—®æ­£æ˜¯æ­ç¤ºå®ƒä»¬çš„æ–¹å¼\n\nå°±è¿™äº›ï¼Œæœ‹å‹ä»¬ã€‚æˆ‘å¸Œæœ›è¿™èƒ½è¯´æœä½ è‡³å°‘å°è¯•å°† SearchGPT æ•´åˆåˆ°ä½ çš„å·¥ä½œæµç¨‹ä¸­ã€‚å®ƒåœ¨è®¸å¤šæ–¹é¢æ”¹å˜äº†æˆ‘çš„æ—¥å¸¸ç”Ÿæ´»ï¼Œæˆ‘æ•¢æ‰“èµŒå®ƒä¹Ÿä¼šæ”¹å˜ä½ çš„ç”Ÿæ´»ã€‚\n\næ„Ÿè°¢é˜…è¯»ï¼\n\n\\-Jordan\n\n"},{"lang":"zh","group":"blog","slug":"blog/smollm2-very-good-alternatives-to-qwen2-5-and-llama-3-2-463a200d2f3b","frontmatter":{"title":"SmolLM2ï¼šQwen2.5 å’Œ Llama 3.2 çš„æœ€ä½³æ›¿ä»£å“","meta_title":"SmolLM2ï¼šQwen2.5 å’Œ Llama 3.2 çš„æœ€ä½³æ›¿ä»£å“","description":"è€Œä¸”æ˜¯å…¨å¼€çš„ï¼","date":"2024-11-10T03:51:17.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Y3_lsNsFKybrOi14.png","categories":["Technology","Machine Learning","Data Science"],"author":"Rifx.Online","tags":["SmolLM2","parameters","pre-training","MobileLLM","reproducibility"],"draft":false,"slug":"blog/smollm2-very-good-alternatives-to-qwen2-5-and-llama-3-2-463a200d2f3b"},"content":"\n\n\n## è€Œä¸”å®ƒæ˜¯å®Œå…¨å¼€æ”¾çš„ï¼\n\nHugging Face åŠ å¤§äº†å¯¹ SmolLM è®¡åˆ’çš„æŠ•å…¥ã€‚\n\nä»–ä»¬å‘å¸ƒäº† SmolLM2ï¼š1\\.7Bã€360M å’Œ 135M æ¨¡åž‹ï¼Œè®­ç»ƒäºŽ 11T ä»¤ç‰Œï¼ˆç›¸æ¯” SmolLM çš„ 1Tï¼‰ã€‚ä»–ä»¬å‘å¸ƒäº†åŸºç¡€ç‰ˆå’ŒæŒ‡å¯¼ç‰ˆï¼š\n\n* Hugging Face Collection: [SmolLM2](https://huggingface.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9) (Apache 2\\.0 è®¸å¯è¯)\n\nä»–ä»¬ä½¿ç”¨äº†æ–°çš„æ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒï¼Œè®¡åˆ’å¾ˆå¿«å‘å¸ƒã€‚ä¸ºäº†åˆ¶ä½œæŒ‡å¯¼ç‰ˆï¼Œä»–ä»¬ä½¿ç”¨äº†ç±»ä¼¼äºŽè®­ç»ƒ Zephyr çš„é…æ–¹ï¼ˆSFT\\+DPO åœ¨ ultrafeedback ä¸Šï¼‰ã€‚\n\nçœ‹èµ·æ¥ SmolLM2 çš„è¡¨çŽ°éžå¸¸å‡ºè‰²ï¼š\n\n\n\nè¯·æ³¨æ„ï¼ŒHugging Face å®Œå…¨å…¬å¼€äº†é¢„è®­ç»ƒæ•°æ®å’Œä»–ä»¬ç”¨æ¥é˜²æ­¢æ•°æ®æ±¡æŸ“çš„é…æ–¹ã€‚æ¢å¥è¯è¯´ï¼Œä»–ä»¬å‘å¸ƒçš„è¯„ä¼°ç»“æžœå¯èƒ½æ˜¯å‡†ç¡®ä¸”å®Œå…¨å¯é‡å¤çš„ã€‚\n\nHugging Face ä½¿ç”¨äº†è‡ªå·±çš„æ¡†æž¶è¿›è¡Œé¢„è®­ç»ƒï¼Œ[Nanotron](https://github.com/huggingface/nanotron)ã€‚æˆ‘ä»Žæœªå†™è¿‡å…³äºŽ Nanotron çš„æ–‡ç« ï¼Œä½†æˆ‘è®¤ä¸ºè¿™æ˜¯ä¸€ä¸ªéžå¸¸æœ‰è¶£çš„é¡¹ç›®ï¼Œå€¼å¾—æ›´å¹¿ä¸ºäººçŸ¥ï¼Œç‰¹åˆ«æ˜¯å¦‚æžœä½ æœ‰å…´è¶£äº†è§£é¢„è®­ç»ƒæ˜¯å¦‚ä½•è¿›è¡Œçš„ã€‚æˆ‘ä¼šå°½é‡æ‰¾æ—¶é—´åœ¨ 2025 å¹´ä¹‹å‰å‘å¸ƒä¸€ç¯‡è§£é‡Š Nanotron çš„æ–‡ç« ï¼\n\nMeta è¿˜å‘å¸ƒäº†ä¸€ç³»åˆ—å°åž‹æ¨¡åž‹ï¼ŒMobileLLMï¼š\n\n* Hugging Face Collection: [MobileLLM](https://huggingface.co/collections/facebook/mobilellm-6722be18cb86c20ebe113e95) (CC\\-BY\\-NC)\n\nè¿™æ˜¯ä¸€ä¸ªæ–°å‘å¸ƒçš„é¡¹ç›®ï¼Œä½†è¯·æ³¨æ„ï¼Œè¿™äº›æ¨¡åž‹å®žé™…ä¸Šç›¸å½“æ—§ã€‚å®ƒä»¬æ˜¯ä¸º 2024 å¹´ 2 æœˆå‘å¸ƒçš„è¿™é¡¹å·¥ä½œè®­ç»ƒçš„ï¼š\n\n[MobileLLM: Optimizing Sub\\-billion Parameter Language Models for On\\-Device Use Cases](https://arxiv.org/abs/2402.14905)\n\né€šè¿‡æˆ‘çš„æ–°ä¹¦â€œLLMs on a Budgetâ€ï¼Œäº†è§£ä½¿ç”¨å’Œå¾®è°ƒå¤§åž‹è¯­è¨€æ¨¡åž‹æ‰€éœ€çš„ä¸€åˆ‡ï¼š\n\n"},{"lang":"zh","group":"blog","slug":"blog/the-6-best-llm-tools-to-run-models-locally-eedd0f7c2bbd","frontmatter":{"title":"6 ç§æœ€ä½³æœ¬åœ°è¿è¡Œæ¨¡åž‹çš„ LLM å·¥å…·","meta_title":"6 ç§æœ€ä½³æœ¬åœ°è¿è¡Œæ¨¡åž‹çš„ LLM å·¥å…·","description":"è¿è¡Œå¤§åž‹è¯­è¨€æ¨¡åž‹ (LLM)ï¼ˆä¾‹å¦‚ ChatGPT å’Œ Claudeï¼‰é€šå¸¸æ¶‰åŠå°†æ•°æ®å‘é€åˆ°ç”± OpenAI å’Œå…¶ä»– AI æ¨¡åž‹ç®¡ç†çš„æœåŠ¡å™¨â€¦â€¦","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*2MB6-INUUGLR0NR_iOACIg.jpeg","categories":["Technology","Programming","Health"],"author":"Rifx.Online","tags":["LLM","local","deployment","customization","telehealth"],"draft":false,"slug":"blog/the-6-best-llm-tools-to-run-models-locally-eedd0f7c2bbd"},"content":"\n\n\n\n\nè¿è¡Œå¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMsï¼‰å¦‚ [ChatGPT](https://openai.com/chatgpt/mac/) å’Œ [Claude](https://claude.ai/) é€šå¸¸æ¶‰åŠå°†æ•°æ®å‘é€åˆ°ç”± [OpenAI](https://openai.com/) å’Œå…¶ä»– AI æ¨¡åž‹æä¾›å•†ç®¡ç†çš„æœåŠ¡å™¨ã€‚è™½ç„¶è¿™äº›æœåŠ¡æ˜¯å®‰å…¨çš„ï¼Œä½†ä¸€äº›ä¼ä¸šæ›´å€¾å‘äºŽå°†å…¶æ•°æ®å®Œå…¨ç¦»çº¿ï¼Œä»¥èŽ·å¾—æ›´é«˜çš„éšç§ä¿æŠ¤ã€‚\n\næœ¬æ–‡å°†ä»‹ç»å¼€å‘äººå‘˜å¯ä»¥ä½¿ç”¨çš„å…­æ¬¾å·¥å…·ï¼Œä»¥ä¾¿åœ¨æœ¬åœ°è¿è¡Œå’Œæµ‹è¯• LLMï¼Œç¡®ä¿ä»–ä»¬çš„æ•°æ®æ°¸è¿œä¸ä¼šç¦»å¼€ä»–ä»¬çš„è®¾å¤‡ï¼Œè¿™ç±»ä¼¼äºŽ [ç«¯åˆ°ç«¯åŠ å¯†](https://getstream.io/blog/end-to-end-encryption/) ä¿æŠ¤éšç§çš„æ–¹å¼ã€‚\n\n## ä¸ºä»€ä¹ˆä½¿ç”¨æœ¬åœ° LLMï¼Ÿ\n\nåƒ [LM Studio](https://lmstudio.ai/) è¿™æ ·çš„å·¥å…·åœ¨ç”¨æˆ·ä½¿ç”¨å®ƒæ¥è¿è¡Œæœ¬åœ° LLM æ—¶ï¼Œä¸ä¼šæ”¶é›†ç”¨æˆ·æ•°æ®æˆ–è·Ÿè¸ªç”¨æˆ·çš„è¡Œä¸ºã€‚å®ƒå…è®¸æ‰€æœ‰èŠå¤©æ•°æ®ä¿ç•™åœ¨æœ¬åœ°è®¡ç®—æœºä¸Šï¼Œè€Œä¸ä¸Ž AI/ML æœåŠ¡å™¨å…±äº«ã€‚\n\n* **éšç§**ï¼šæ‚¨å¯ä»¥ä»¥å¤šè½®çš„æ–¹å¼æç¤ºæœ¬åœ° LLMï¼Œè€Œä¸ä¼šè®©æ‚¨çš„æç¤ºæ•°æ®ç¦»å¼€æœ¬åœ°ä¸»æœºã€‚\n* **è‡ªå®šä¹‰é€‰é¡¹**ï¼šæœ¬åœ° LLM æä¾› CPU çº¿ç¨‹ã€æ¸©åº¦ã€ä¸Šä¸‹æ–‡é•¿åº¦ã€GPU è®¾ç½®ç­‰é«˜çº§é…ç½®é€‰é¡¹ã€‚è¿™ç±»ä¼¼äºŽ OpenAI çš„æ¸¸ä¹åœºã€‚\n* **æ”¯æŒå’Œå®‰å…¨æ€§**ï¼šå®ƒä»¬æä¾›ä¸Ž OpenAI æˆ– Claude ç›¸ä¼¼çš„æ”¯æŒå’Œå®‰å…¨æ€§ã€‚\n* **è®¢é˜…å’Œè´¹ç”¨**ï¼šè¿™äº›å·¥å…·å…è´¹ä½¿ç”¨ï¼Œå¹¶ä¸”ä¸éœ€è¦æ¯æœˆè®¢é˜…ã€‚å¯¹äºŽåƒ OpenAI è¿™æ ·çš„äº‘æœåŠ¡ï¼Œæ¯ä¸ª API è¯·æ±‚éƒ½éœ€è¦ä»˜è´¹ã€‚æœ¬åœ° LLM æœ‰åŠ©äºŽèŠ‚çœè´¹ç”¨ï¼Œå› ä¸ºæ²¡æœ‰æ¯æœˆè®¢é˜…ã€‚\n* **ç¦»çº¿æ”¯æŒ**ï¼šæ‚¨å¯ä»¥åœ¨ç¦»çº¿æ—¶åŠ è½½å’Œè¿žæŽ¥å¤§åž‹è¯­è¨€æ¨¡åž‹ã€‚\n* **è¿žæŽ¥æ€§**ï¼šæœ‰æ—¶ï¼Œè¿žæŽ¥åˆ°åƒ OpenAI è¿™æ ·çš„äº‘æœåŠ¡å¯èƒ½ä¼šå¯¼è‡´ä¿¡å·å’Œè¿žæŽ¥ä¸è‰¯ã€‚\n\n## å…­å¤§å…è´¹æœ¬åœ° LLM å·¥å…·\n\næ ¹æ®æ‚¨çš„å…·ä½“ä½¿ç”¨æ¡ˆä¾‹ï¼Œæ‚¨å¯ä»¥é€‰æ‹©å‡ ç§ç¦»çº¿ LLM åº”ç”¨ç¨‹åºã€‚è¿™äº›å·¥å…·ä¸­æœ‰ä¸€äº›å®Œå…¨å…è´¹ä¾›ä¸ªäººå’Œå•†ä¸šä½¿ç”¨ã€‚å…¶ä»–å·¥å…·å¯èƒ½éœ€è¦æ‚¨å‘é€è¯·æ±‚ä»¥ç”¨äºŽå•†ä¸šç”¨é€”ã€‚å¯¹äºŽ Macã€Windows å’Œ Linuxï¼Œæœ‰å¤šç§æœ¬åœ° LLM å·¥å…·å¯ä¾›é€‰æ‹©ã€‚ä»¥ä¸‹æ˜¯æ‚¨å¯ä»¥é€‰æ‹©çš„å…­ä¸ªæœ€ä½³å·¥å…·ã€‚\n\n## 1. LM Studio\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*svbQPZKu08of7Kv6)\n\n[LM Studio](https://lmstudio.ai/) å¯ä»¥è¿è¡Œä»»ä½•æ ¼å¼ä¸º `gguf` çš„æ¨¡åž‹æ–‡ä»¶ã€‚å®ƒæ”¯æŒæ¥è‡ªæ¨¡åž‹æä¾›å•†çš„ `gguf` æ–‡ä»¶ï¼Œå¦‚ [Llama 3.1](https://llama.meta.com/)ã€[Phi 3](https://huggingface.co/docs/transformers/main/en/model_doc/phi3)ã€[Mistral](https://mistral.ai/) å’Œ [Gemma](https://ai.google.dev/gemma)ã€‚è¦ä½¿ç”¨ LM Studioï¼Œè¯·è®¿é—®ä¸Šè¿°é“¾æŽ¥å¹¶ä¸‹è½½é€‚åˆæ‚¨æœºå™¨çš„åº”ç”¨ç¨‹åºã€‚å¯åŠ¨ LM Studio åŽï¼Œä¸»é¡µä¼šå±•ç¤ºå¯ä¸‹è½½å’Œæµ‹è¯•çš„é¡¶çº§ LLMã€‚è¿˜æœ‰ä¸€ä¸ªæœç´¢æ ï¼Œå¯ä»¥ç­›é€‰å¹¶ä¸‹è½½æ¥è‡ªä¸åŒ AI æä¾›å•†çš„ç‰¹å®šæ¨¡åž‹ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*sbS3VqiLgDsftgs2)\n\nä»Žç‰¹å®šå…¬å¸çš„æ¨¡åž‹ä¸­æœç´¢ä¼šæ˜¾ç¤ºå¤šä¸ªæ¨¡åž‹ï¼ŒèŒƒå›´ä»Žå°åž‹åˆ°å¤§åž‹ [quantization](https://huggingface.co/docs/optimum/en/concept_guides/quantization)ã€‚æ ¹æ®æ‚¨çš„æœºå™¨ï¼ŒLM Studio ä¼šä½¿ç”¨å…¼å®¹æ€§çŒœæµ‹æ¥çªå‡ºæ˜¾ç¤ºé€‚åˆè¯¥æœºå™¨æˆ–å¹³å°çš„æ¨¡åž‹ã€‚\n\n## LM Studio çš„å…³é”®ç‰¹æ€§\n\nLM Studio æä¾›ä¸Ž ChatGPT ç›¸ä¼¼çš„åŠŸèƒ½å’Œç‰¹æ€§ã€‚å®ƒå…·æœ‰å¤šä¸ªåŠŸèƒ½ã€‚ä»¥ä¸‹æ˜¯ LM Studio çš„å…³é”®ç‰¹æ€§ã€‚\n\n* **æ¨¡åž‹å‚æ•°è‡ªå®šä¹‰**ï¼šè¿™å…è®¸æ‚¨è°ƒæ•´æ¸©åº¦ã€æœ€å¤§ä»¤ç‰Œã€é¢‘çŽ‡æƒ©ç½šç­‰ã€‚\n* **èŠå¤©åŽ†å²**ï¼šå…è®¸æ‚¨ä¿å­˜æç¤ºä»¥ä¾›åŽç»­ä½¿ç”¨ã€‚\n* **å‚æ•°å’Œç”¨æˆ·ç•Œé¢æç¤º**ï¼šæ‚¨å¯ä»¥å°†é¼ æ ‡æ‚¬åœåœ¨ä¿¡æ¯æŒ‰é’®ä¸Šä»¥æŸ¥æ‰¾æ¨¡åž‹å‚æ•°å’Œæœ¯è¯­ã€‚\n* **è·¨å¹³å°**ï¼šLM Studio å¯åœ¨ Linuxã€Mac å’Œ Windows æ“ä½œç³»ç»Ÿä¸Šä½¿ç”¨ã€‚\n* **æœºå™¨è§„æ ¼æ£€æŸ¥**ï¼šLM Studio æ£€æŸ¥è®¡ç®—æœºè§„æ ¼ï¼Œå¦‚ GPU å’Œå†…å­˜ï¼Œå¹¶æŠ¥å‘Šå…¼å®¹çš„æ¨¡åž‹ã€‚è¿™å¯ä»¥é˜²æ­¢ä¸‹è½½å¯èƒ½åœ¨ç‰¹å®šæœºå™¨ä¸Šæ— æ³•è¿è¡Œçš„æ¨¡åž‹ã€‚\n* **AI èŠå¤©å’Œæ¸¸ä¹åœº**ï¼šä»¥å¤šè½®èŠå¤©æ ¼å¼ä¸Žå¤§åž‹è¯­è¨€æ¨¡åž‹è¿›è¡Œå¯¹è¯ï¼Œå¹¶é€šè¿‡åŒæ—¶åŠ è½½å¤šä¸ª LLM è¿›è¡Œå®žéªŒã€‚\n* **å¼€å‘è€…æœ¬åœ°æŽ¨ç†æœåŠ¡å™¨**ï¼šå…è®¸å¼€å‘è€…è®¾ç½®ä¸€ä¸ªç±»ä¼¼äºŽ OpenAI API çš„æœ¬åœ° HTTP æœåŠ¡å™¨ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*9bHmRiOSf6gm-u3P)\n\næœ¬åœ°æœåŠ¡å™¨æä¾›ç¤ºä¾‹ Curl å’Œ Python å®¢æˆ·ç«¯è¯·æ±‚ã€‚æ­¤åŠŸèƒ½æœ‰åŠ©äºŽä½¿ç”¨ LM Studio æž„å»º AI åº”ç”¨ç¨‹åºï¼Œä»¥è®¿é—®ç‰¹å®šçš„ LLMã€‚\n\n```python\n## Example: reuse your existing OpenAI setup\nfrom openai import OpenAI\n\n## Point to the local server\nclient = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n\ncompletion = client.chat.completions.create(\n  model=\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"Always answer in rhymes.\"},\n    {\"role\": \"user\", \"content\": \"Introduce yourself.\"}\n  ],\n  temperature=0.7,\n)\n\nprint(completion.choices[0].message)\n```\né€šè¿‡ä¸Šè¿°ç¤ºä¾‹ Python ä»£ç ï¼Œæ‚¨å¯ä»¥é‡ç”¨çŽ°æœ‰çš„ OpenAI é…ç½®ï¼Œå¹¶å°†åŸºæœ¬ URL ä¿®æ”¹ä¸ºæŒ‡å‘æ‚¨çš„æœ¬åœ°ä¸»æœºã€‚\n\n* **OpenAI çš„ Python åº“å¯¼å…¥**ï¼šLM Studio å…è®¸å¼€å‘è€…å¯¼å…¥ OpenAI Python åº“ï¼Œå¹¶å°†åŸºæœ¬ URL æŒ‡å‘æœ¬åœ°æœåŠ¡å™¨ï¼ˆlocalhostï¼‰ã€‚\n* **å¤šæ¨¡åž‹ä¼šè¯**ï¼šä½¿ç”¨å•ä¸ªæç¤ºå¹¶é€‰æ‹©å¤šä¸ªæ¨¡åž‹è¿›è¡Œè¯„ä¼°ã€‚\n\n## ä½¿ç”¨ LM Studio çš„å¥½å¤„\n\nè¯¥å·¥å…·å¯ä¾›ä¸ªäººå…è´¹ä½¿ç”¨ï¼Œå…è®¸å¼€å‘è€…é€šè¿‡åº”ç”¨å†…èŠå¤©ç”¨æˆ·ç•Œé¢å’Œæ¸¸ä¹åœºè¿è¡Œ LLMã€‚å®ƒæä¾›äº†ä¸€ä¸ªåŽä¸½ä¸”æ˜“äºŽä½¿ç”¨çš„ç•Œé¢ï¼Œå¸¦æœ‰è¿‡æ»¤å™¨ï¼Œå¹¶æ”¯æŒè¿žæŽ¥åˆ° OpenAI çš„ Python åº“ï¼Œæ— éœ€ API å¯†é’¥ã€‚å…¬å¸å’Œä¼ä¸šå¯ä»¥æ ¹æ®è¦æ±‚ä½¿ç”¨ LM Studioã€‚ç„¶è€Œï¼Œå®ƒéœ€è¦ M1/M2/M3 Mac æˆ–æ›´é«˜ç‰ˆæœ¬ï¼Œæˆ–å…·æœ‰æ”¯æŒ [AVX2](https://edc.intel.com/content/www/us/en/design/ipla/software-development-platforms/client/platforms/alder-lake-desktop/12th-generation-intel-core-processors-datasheet-volume-1-of-2/009/intel-advanced-vector-extensions-2-intel-avx2/) çš„å¤„ç†å™¨çš„ Windows PCã€‚Intel å’Œ [AMD](https://www.amd.com/en/support/download/drivers.html) ç”¨æˆ·ä»…é™äºŽä½¿ç”¨ [v0.2.31](https://lmstudio.ai/) ä¸­çš„ Vulkan æŽ¨ç†å¼•æ“Žã€‚\n\n## 2. Jan\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*7YeH_48iFYB4lDRu)\n\nå°† [Jan](https://jan.ai/) ç†è§£ä¸ºä¸€ä¸ªè®¾è®¡ç”¨äºŽç¦»çº¿æ“ä½œçš„å¼€æºç‰ˆæœ¬çš„ ChatGPTã€‚å®ƒç”±ä¸€ä¸ªç”¨æˆ·ç¤¾åŒºæž„å»ºï¼Œç§‰æŒç”¨æˆ·æ‹¥æœ‰çš„ç†å¿µã€‚Jan å…è®¸æ‚¨åœ¨è®¾å¤‡ä¸Šè¿è¡Œæµè¡Œçš„æ¨¡åž‹ï¼Œå¦‚ [Mistral](https://huggingface.co/models?other=mistral) æˆ– [Llama](https://huggingface.co/models?other=llama)ï¼Œè€Œæ— éœ€è¿žæŽ¥äº’è”ç½‘ã€‚ä½¿ç”¨ Janï¼Œæ‚¨å¯ä»¥è®¿é—®è¿œç¨‹ APIï¼Œå¦‚ OpenAI å’Œ [Groq](https://groq.com/)ã€‚\n\n## Jan çš„ä¸»è¦ç‰¹ç‚¹\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*ufyOE6QkcHw8X5U7)\n\nJan æ˜¯ä¸€æ¬¾ç”µå­åº”ç”¨ç¨‹åºï¼Œå…¶åŠŸèƒ½ç±»ä¼¼äºŽ LM Studioã€‚å®ƒé€šè¿‡å°†æ¶ˆè´¹è€…è®¾å¤‡è½¬å˜ä¸º AI è®¡ç®—æœºï¼Œä½¿ AI å˜å¾—å¼€æ”¾å’Œå¯è®¿é—®ã€‚ç”±äºŽè¿™æ˜¯ä¸€ä¸ªå¼€æºé¡¹ç›®ï¼Œå¼€å‘è€…å¯ä»¥ä¸ºå…¶è´¡çŒ®ä»£ç å¹¶æ‰©å±•å…¶åŠŸèƒ½ã€‚ä»¥ä¸‹æ˜¯ Jan çš„ä¸»è¦ç‰¹ç‚¹ã€‚\n\n* **æœ¬åœ°**ï¼šæ‚¨å¯ä»¥åœ¨è®¾å¤‡ä¸Šè¿è¡Œæ‚¨å–œæ¬¢çš„ AI æ¨¡åž‹ï¼Œè€Œæ— éœ€å°†å…¶è¿žæŽ¥åˆ°äº’è”ç½‘ã€‚\n* **å³ç”¨æ¨¡åž‹**ï¼šä¸‹è½½ Jan åŽï¼Œæ‚¨å°†èŽ·å¾—ä¸€ç»„å·²å®‰è£…çš„æ¨¡åž‹ä»¥ä¾›å¼€å§‹ä½¿ç”¨ã€‚ä¹Ÿå¯ä»¥æœç´¢ç‰¹å®šæ¨¡åž‹ã€‚\n* **æ¨¡åž‹å¯¼å…¥**ï¼šæ”¯æŒä»Ž Hugging Face ç­‰æ¥æºå¯¼å…¥æ¨¡åž‹ã€‚\n* **å…è´¹ã€è·¨å¹³å°å’Œå¼€æº**ï¼šJan å®Œå…¨å…è´¹ï¼Œå¼€æºï¼Œå¹¶å¯åœ¨ Macã€Windows å’Œ Linux ä¸Šè¿è¡Œã€‚\n* **è‡ªå®šä¹‰æŽ¨ç†å‚æ•°**ï¼šè°ƒæ•´æ¨¡åž‹å‚æ•°ï¼Œå¦‚æœ€å¤§ä»¤ç‰Œã€æ¸©åº¦ã€æµã€é¢‘çŽ‡æƒ©ç½šç­‰ã€‚æ‰€æœ‰åå¥½è®¾ç½®ã€æ¨¡åž‹ä½¿ç”¨å’Œé…ç½®éƒ½ä¿ç•™åœ¨æ‚¨çš„è®¡ç®—æœºä¸Šã€‚\n* **æ‰©å±•**ï¼šJan æ”¯æŒ [TensortRT](https://github.com/NVIDIA/TensorRT) å’Œ [Inference Nitro](https://huggingface.co/jan-hq/nitro-v1.2-e3) ç­‰æ‰©å±•ï¼Œä»¥è‡ªå®šä¹‰å’Œå¢žå¼ºæ‚¨çš„ AI æ¨¡åž‹ã€‚\n\n## ä½¿ç”¨ Jan çš„å¥½å¤„\n\nJan æä¾›äº†ä¸€ä¸ªå¹²å‡€ç®€å•çš„ç•Œé¢æ¥ä¸Ž LLM äº’åŠ¨ï¼Œå¹¶ä¸”å°†æ‰€æœ‰æ•°æ®å’Œå¤„ç†ä¿¡æ¯ä¿å­˜åœ¨æœ¬åœ°ã€‚å®ƒå·²ç»ä¸ºæ‚¨å®‰è£…äº†è¶…è¿‡ä¸ƒåä¸ªå¤§åž‹è¯­è¨€æ¨¡åž‹ä¾›æ‚¨ä½¿ç”¨ã€‚è¿™äº›çŽ°æˆçš„æ¨¡åž‹çš„å¯ç”¨æ€§ä½¿å¾—è¿žæŽ¥å’Œä¸Žè¿œç¨‹ APIï¼ˆå¦‚ OpenAI å’Œ Mistralï¼‰äº’åŠ¨å˜å¾—ç®€å•ã€‚Jan è¿˜æœ‰ä¸€ä¸ªå¾ˆæ£’çš„ [GitHub](https://github.com/janhq/jan)ã€[Discord](https://discord.gg/FTk2MvZwJH) å’Œ [Hugging Face](https://huggingface.co/janhq) ç¤¾åŒºï¼Œå¯ä»¥å…³æ³¨å¹¶å¯»æ±‚å¸®åŠ©ã€‚ç„¶è€Œï¼Œåƒæ‰€æœ‰ LLM å·¥å…·ä¸€æ ·ï¼Œè¿™äº›æ¨¡åž‹åœ¨ Apple Silicon Macs ä¸Šçš„è¿è¡Œé€Ÿåº¦æ¯”åœ¨ Intel æœºå™¨ä¸Šæ›´å¿«ã€‚\n\n## 3. Llamafile\n\n[Llamafile](https://github.com/Mozilla-Ocho/llamafile) ç”± [Mozilla](https://www.mozilla.org/en-US/?v=1) æ”¯æŒï¼Œæ—¨åœ¨é€šè¿‡å¿«é€Ÿçš„ [CPU æŽ¨ç†](https://huggingface.co/docs/transformers/en/perf_infer_cpu) ä½¿å¼€æº AI å¯¹æ¯ä¸ªäººéƒ½å¯è®¿é—®ï¼Œè€Œæ— éœ€ç½‘ç»œè¿žæŽ¥ã€‚å®ƒå°† LLM è½¬æ¢ä¸ºå¤šå¹³å°çš„ [å¯æ‰§è¡Œé“¾æŽ¥æ ¼å¼](https://gist.github.com/x0nu11byt3/bcb35c3de461e5fb66173071a2379779) (ELF)ã€‚å®ƒæä¾›äº†å°† AI [é›†æˆ](https://getstream.io/chat/solutions/ai-integration/) åˆ°åº”ç”¨ç¨‹åºä¸­çš„æœ€ä½³é€‰é¡¹ä¹‹ä¸€ï¼Œä½¿æ‚¨èƒ½å¤Ÿä»…é€šè¿‡ä¸€ä¸ªå¯æ‰§è¡Œæ–‡ä»¶è¿è¡Œ LLMã€‚\n\n## Llamafile çš„å·¥ä½œåŽŸç†\n\nå®ƒæ—¨åœ¨å°†æƒé‡è½¬æ¢ä¸ºå¤šä¸ªå¯æ‰§è¡Œç¨‹åºï¼Œè¿™äº›ç¨‹åºæ— éœ€å®‰è£…å³å¯åœ¨ Windowsã€MacOSã€Linuxã€Intelã€ARMã€FreeBSD ç­‰æž¶æž„ä¸Šè¿è¡Œã€‚åœ¨åº•å±‚ï¼ŒLlamafile ä½¿ç”¨ [tinyBLAST](https://github.com/ggerganov/llama.cpp/issues/5048) åœ¨åƒ Windows è¿™æ ·çš„æ“ä½œç³»ç»Ÿä¸Šè¿è¡Œï¼Œè€Œæ— éœ€ SDKã€‚\n\n## Llamafile çš„å…³é”®ç‰¹æ€§\n\n* **å¯æ‰§è¡Œæ–‡ä»¶**ï¼šä¸Ž LM Studio å’Œ Jan ç­‰å…¶ä»– LLM å·¥å…·ä¸åŒï¼ŒLlamafile åªéœ€ä¸€ä¸ªå¯æ‰§è¡Œæ–‡ä»¶å³å¯è¿è¡Œ LLMã€‚\n* **ä½¿ç”¨çŽ°æœ‰æ¨¡åž‹**ï¼šLlamafile æ”¯æŒä½¿ç”¨çŽ°æœ‰çš„æ¨¡åž‹å·¥å…·ï¼Œå¦‚ Ollama å’Œ LM Studioã€‚\n* **è®¿é—®æˆ–åˆ›å»ºæ¨¡åž‹**ï¼šæ‚¨å¯ä»¥è®¿é—® OpenAIã€Mistralã€Groq ç­‰æµè¡Œ LLMã€‚å®ƒè¿˜æä¾›ä»Žå¤´åˆ›å»ºæ¨¡åž‹çš„æ”¯æŒã€‚\n* **æ¨¡åž‹æ–‡ä»¶è½¬æ¢**ï¼šæ‚¨å¯ä»¥å°†è®¸å¤šæµè¡Œ LLM çš„æ–‡ä»¶æ ¼å¼è½¬æ¢ï¼Œä¾‹å¦‚ï¼Œå°† `.gguf` è½¬æ¢ä¸º `.llamafile` åªéœ€ä¸€ä¸ªå‘½ä»¤ã€‚\n\n`llamafile-convert mistral-7b.gguf`\n\n## å¼€å§‹ä½¿ç”¨ Llamafile\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*4PV1KsCZvvVKqFll)\n\nè¦å®‰è£… Llamafileï¼Œè¯·è®¿é—® Huggingface ç½‘ç«™ï¼Œä»Žå¯¼èˆªä¸­é€‰æ‹© **Models**ï¼Œç„¶åŽæœç´¢ **Llamafile**ã€‚æ‚¨è¿˜å¯ä»¥ä»Žä¸‹é¢çš„ URL å®‰è£…æ‚¨å–œæ¬¢çš„ [é‡åŒ–](https://huggingface.co/docs/optimum/en/concept_guides/quantization) ç‰ˆæœ¬ã€‚\n\n[`https://huggingface.co/Mozilla/Meta-Llama-3.1-8B-Instruct-llamafile/tree/m`ain](https://huggingface.co/Mozilla/Meta-Llama-3.1-8B-Instruct-llamafile/tree/main)\n\n**æ³¨æ„**ï¼šé‡åŒ–æ•°å­—è¶Šå¤§ï¼Œå“åº”è¶Šå¥½ã€‚æ­£å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œæœ¬æ–‡ä½¿ç”¨ `Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile`ï¼Œå…¶ä¸­ `Q6` ä»£è¡¨é‡åŒ–æ•°å­—ã€‚\n\n**æ­¥éª¤ 1ï¼šä¸‹è½½ Llamafile**\n\nä»Žä¸Šé¢çš„é“¾æŽ¥ï¼Œç‚¹å‡»ä»»æ„ä¸‹è½½æŒ‰é’®ä»¥èŽ·å–æ‚¨å–œæ¬¢çš„ç‰ˆæœ¬ã€‚å¦‚æžœæ‚¨åœ¨æœºå™¨ä¸Šå®‰è£…äº† [wget](https://www.gnu.org/software/wget/) å·¥å…·ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ä¸‹è½½ Llamafileã€‚\n\n`wget <https://huggingface.co/Mozilla/Meta-Llama-3.1-8B-Instruct-llamafile/blob/main/Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile>`\n\næ‚¨åº”è¯¥ç”¨æ‚¨å–œæ¬¢çš„ç‰ˆæœ¬æ›¿æ¢ URLã€‚\n\n**æ­¥éª¤ 2ï¼šä½¿ Llamafile å¯æ‰§è¡Œ**\n\nä¸‹è½½ç‰¹å®šç‰ˆæœ¬çš„ Llamafile åŽï¼Œæ‚¨åº”è¯¥é€šè¿‡å¯¼èˆªåˆ°æ–‡ä»¶ä½ç½®ï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ä½¿å…¶å¯æ‰§è¡Œã€‚\n\n`chmod +x Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile`**æ­¥éª¤ 3ï¼šè¿è¡Œ Llamafile**\n\nåœ¨æ–‡ä»¶åä¹‹å‰æ·»åŠ ä¸€ä¸ªç‚¹å’Œæ–œæ  `./` æ¥å¯åŠ¨ Llamafileã€‚\n\n`./Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile`\n\nLlamafile åº”ç”¨ç¨‹åºçŽ°åœ¨å°†åœ¨ `http://127.0.0.1:8080` å¯ç”¨ï¼Œä»¥è¿è¡Œæ‚¨çš„å„ç§ LLMsã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*1xrwDPTfNgmEQDTx)\n\n## ä½¿ç”¨ Llamafile çš„å¥½å¤„\n\nLlamafile é€šè¿‡ä½¿ LLM å®¹æ˜“è¢«æ¶ˆè´¹è€… CPU è®¿é—®ï¼Œå¸®åŠ©å®žçŽ° AI å’Œ ML çš„æ°‘ä¸»åŒ–ã€‚ä¸Žå…¶ä»–æœ¬åœ° LLM åº”ç”¨ç¨‹åºå¦‚ **Llama.cpp** ç›¸æ¯”ï¼ŒLlamafile æä¾›äº†æœ€å¿«çš„æç¤ºå¤„ç†ä½“éªŒï¼Œå¹¶åœ¨æ¸¸æˆç”µè„‘ä¸Šè¡¨çŽ°æ›´ä½³ã€‚ç”±äºŽå…¶æ›´å¿«çš„æ€§èƒ½ï¼Œå®ƒæ˜¯æ€»ç»“é•¿æ–‡æœ¬å’Œå¤§åž‹æ–‡æ¡£çš„ç»ä½³é€‰æ‹©ã€‚å®ƒå®Œå…¨ç¦»çº¿è¿è¡Œå¹¶ä¿æŠ¤éšç§ï¼Œå› æ­¤ç”¨æˆ·ä¸ä¼šå°†æ•°æ®åˆ†äº«ç»™ä»»ä½• AI æœåŠ¡å™¨æˆ– APIã€‚åƒ Hugging Face è¿™æ ·çš„æœºå™¨å­¦ä¹ ç¤¾åŒºæ”¯æŒ Llamafile æ ¼å¼ï¼Œä½¿å¾—æœç´¢ä¸Ž Llamafile ç›¸å…³çš„æ¨¡åž‹å˜å¾—å®¹æ˜“ã€‚å®ƒè¿˜æœ‰ä¸€ä¸ªå‡ºè‰²çš„å¼€æºç¤¾åŒºï¼Œè¿›ä¸€æ­¥å¼€å‘å’Œæ‰©å±•å®ƒã€‚\n\n## 4. GPT4ALL\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*j3vNWWQZCVF5woo5)\n\nGPT4ALL åŸºäºŽéšç§ã€å®‰å…¨å’Œæ— éœ€äº’è”ç½‘çš„åŽŸåˆ™æž„å»ºã€‚ç”¨æˆ·å¯ä»¥åœ¨ Macã€Windows å’Œ Ubuntu ä¸Š [å®‰è£…](https://www.nomic.ai/gpt4all)ã€‚ä¸Ž Jan æˆ– LM Studio ç›¸æ¯”ï¼ŒGPT4ALL æ‹¥æœ‰æ›´å¤šçš„æ¯æœˆä¸‹è½½é‡ã€[GitHub Stars](https://github.com/nomic-ai/gpt4all) å’Œæ´»è·ƒç”¨æˆ·ã€‚\n\n## GPT4ALLçš„ä¸»è¦ç‰¹ç‚¹\n\nGPT4Allå¯ä»¥åœ¨ä¸»è¦æ¶ˆè´¹ç¡¬ä»¶ä¸Šè¿è¡ŒLLMï¼Œä¾‹å¦‚Mac Mç³»åˆ—èŠ¯ç‰‡ã€AMDå’ŒNVIDIA GPUã€‚ä»¥ä¸‹æ˜¯å…¶ä¸»è¦ç‰¹ç‚¹ã€‚\n\n* **éšç§ä¼˜å…ˆ**ï¼šå°†ç§äººå’Œæ•æ„Ÿçš„èŠå¤©ä¿¡æ¯å’Œæç¤ºä»…ä¿ç•™åœ¨æ‚¨çš„è®¾å¤‡ä¸Šã€‚\n* **æ— éœ€äº’è”ç½‘**ï¼šå®ƒå®Œå…¨ç¦»çº¿å·¥ä½œã€‚\n* **æ¨¡åž‹æŽ¢ç´¢**ï¼šæ­¤åŠŸèƒ½å…è®¸å¼€å‘è€…æµè§ˆå’Œä¸‹è½½ä¸åŒç±»åž‹çš„LLMè¿›è¡Œå®žéªŒã€‚æ‚¨å¯ä»¥ä»Žæµè¡Œé€‰é¡¹ä¸­é€‰æ‹©å¤§çº¦1000ä¸ªå¼€æºè¯­è¨€æ¨¡åž‹ï¼Œå¦‚LLamaã€Mistralç­‰ã€‚\n* **æœ¬åœ°æ–‡æ¡£**ï¼šæ‚¨å¯ä»¥è®©æœ¬åœ°LLMè®¿é—®æ‚¨çš„æ•æ„Ÿæ•°æ®ï¼Œä½¿ç”¨æœ¬åœ°æ–‡æ¡£å¦‚`.pdf`å’Œ`.txt`ï¼Œæ•°æ®ä¸ä¼šç¦»å¼€æ‚¨çš„è®¾å¤‡ï¼Œä¹Ÿæ— éœ€ç½‘ç»œã€‚\n* **è‡ªå®šä¹‰é€‰é¡¹**ï¼šå®ƒæä¾›å¤šä¸ª[èŠå¤©æœºå™¨äºº](https://getstream.io/blog/llm-chatbot-docs/)è°ƒæ•´é€‰é¡¹ï¼Œå¦‚æ¸©åº¦ã€æ‰¹å¤„ç†å¤§å°ã€ä¸Šä¸‹æ–‡é•¿åº¦ç­‰ã€‚\n* **ä¼ä¸šç‰ˆ**ï¼šGPT4ALLæä¾›ä¼ä¸šå¥—é¤ï¼Œå…·å¤‡å®‰å…¨æ€§ã€æ”¯æŒå’Œæ¯å°è®¾å¤‡çš„è®¸å¯è¯ï¼Œå°†æœ¬åœ°AIå¸¦å…¥ä¼ä¸šã€‚\n\n## å¼€å§‹ä½¿ç”¨ GPT4All\n\nè¦å¼€å§‹ä½¿ç”¨ GPT4All åœ¨æœ¬åœ°è¿è¡Œ LLMsï¼Œè¯·[ä¸‹è½½](https://www.nomic.ai/gpt4all)é€‚åˆæ‚¨æ“ä½œç³»ç»Ÿçš„ç‰ˆæœ¬ã€‚\n\n## ä½¿ç”¨GPT4ALLçš„å¥½å¤„\n\né™¤äº†Ollamaï¼ŒGPT4ALLåœ¨GitHubè´¡çŒ®è€…æ•°é‡ä¸Šæœ€ä¸ºæ˜¾è‘—ï¼Œæ‹¥æœ‰çº¦250000åæ¯æœˆæ´»è·ƒç”¨æˆ·ï¼ˆæ ¹æ®<https://www.nomic.ai/gpt4all>ï¼‰å¹¶ä¸”ä¸Žå…¶ç«žäº‰å¯¹æ‰‹ç›¸æ¯”ã€‚è¯¥åº”ç”¨æ”¶é›†æœ‰å…³ä½¿ç”¨åˆ†æžå’ŒèŠå¤©åˆ†äº«çš„åŒ¿åç”¨æˆ·æ•°æ®ã€‚ç„¶è€Œï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©åŠ å…¥æˆ–é€€å‡ºã€‚ä½¿ç”¨GPT4ALLï¼Œå¼€å‘è€…å¯ä»¥ä»Žå…¶åºžå¤§çš„ç”¨æˆ·åŸºç¡€ã€GitHubå’ŒDiscordç¤¾åŒºä¸­å—ç›Šã€‚\n\n## 5. Ollama\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*STAonWgWIsY6cgDR)\n\nä½¿ç”¨ [Ollama](https://ollama.com/)ï¼Œæ‚¨å¯ä»¥è½»æ¾åˆ›å»ºæœ¬åœ°èŠå¤©æœºå™¨äººï¼Œè€Œæ— éœ€è¿žæŽ¥åˆ°åƒ OpenAI è¿™æ ·çš„ APIã€‚ç”±äºŽä¸€åˆ‡éƒ½åœ¨æœ¬åœ°è¿è¡Œï¼Œæ‚¨æ— éœ€æ”¯ä»˜ä»»ä½•è®¢é˜…è´¹æˆ– API è°ƒç”¨è´¹ç”¨ã€‚\n\n## Ollama çš„å…³é”®ç‰¹æ€§\n\n* **æ¨¡åž‹è‡ªå®šä¹‰**ï¼šOllama å…è®¸æ‚¨è½¬æ¢ `.gguf` æ¨¡åž‹æ–‡ä»¶å¹¶ä½¿ç”¨ `ollama run modelname` è¿è¡Œå®ƒä»¬ã€‚\n* **æ¨¡åž‹åº“**ï¼šOllama æ‹¥æœ‰å¤§é‡æ¨¡åž‹å¯ä¾›å°è¯•ï¼Œè®¿é—® [ollama.com/library](https://ollama.com/library)ã€‚\n* **å¯¼å…¥æ¨¡åž‹**ï¼šOllama æ”¯æŒä»Ž [PyTorch](https://pytorch.org/) å¯¼å…¥æ¨¡åž‹ã€‚\n* **ç¤¾åŒºé›†æˆ**ï¼šOllama æ— ç¼é›†æˆåˆ°ç½‘é¡µå’Œæ¡Œé¢åº”ç”¨ç¨‹åºä¸­ï¼Œä¾‹å¦‚ [Ollama-SwiftUI](https://github.com/kghandour/Ollama-SwiftUI)ã€[HTML UI](https://github.com/rtcfirefly/ollama-ui)ã€[Dify.ai](https://github.com/rtcfirefly/ollama-ui) å’Œ [æ›´å¤š](https://github.com/ollama/ollama?tab=readme-ov-file#community-integrations)ã€‚\n* **æ•°æ®åº“è¿žæŽ¥**ï¼šOllama æ”¯æŒå¤šä¸ª [æ•°æ®å¹³å°](https://github.com/mindsdb/mindsdb/blob/main/mindsdb/integrations/handlers/ollama_handler/README.md)ã€‚\n* **ç§»åŠ¨é›†æˆ**ï¼šåƒ [Enchanted](https://github.com/AugustDev/enchanted) è¿™æ ·çš„ SwiftUI åº”ç”¨å°† Ollama å¸¦å…¥ iOSã€macOS å’Œ visionOSã€‚[Maid](https://github.com/Mobile-Artificial-Intelligence/maid) ä¹Ÿæ˜¯ä¸€ä¸ªè·¨å¹³å°çš„ Flutter åº”ç”¨ï¼Œèƒ½å¤Ÿæœ¬åœ°å¤„ç† `.gguf` æ¨¡åž‹æ–‡ä»¶ã€‚\n\n## å¼€å§‹ä½¿ç”¨ Ollama\n\nè¦é¦–æ¬¡ä½¿ç”¨ Ollamaï¼Œè¯·è®¿é—® <https://ollama.com> å¹¶ä¸‹è½½é€‚åˆæ‚¨æœºå™¨çš„ç‰ˆæœ¬ã€‚æ‚¨å¯ä»¥åœ¨ Macã€Linux æˆ– Windows ä¸Šå®‰è£…å®ƒã€‚å®‰è£… Ollama åŽï¼Œæ‚¨å¯ä»¥åœ¨ç»ˆç«¯ä¸­ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ£€æŸ¥å…¶è¯¦ç»†ä¿¡æ¯ã€‚\n\n`ollama`\n\nè¦è¿è¡Œç‰¹å®šçš„ LLMï¼Œæ‚¨åº”è¯¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ä¸‹è½½å®ƒï¼š\n\n`ollama pull modelname`ï¼Œå…¶ä¸­ `modelname` æ˜¯æ‚¨è¦å®‰è£…çš„æ¨¡åž‹åç§°ã€‚è¯·åœ¨ [GitHub](https://github.com/ollama/ollama) ä¸ŠæŸ¥çœ‹ä¸€äº›å¯ä¾›ä¸‹è½½çš„ç¤ºä¾‹æ¨¡åž‹ã€‚`pull` å‘½ä»¤ä¹Ÿç”¨äºŽæ›´æ–°æ¨¡åž‹ã€‚ä¸€æ—¦ä½¿ç”¨ï¼Œä»…ä¼šèŽ·å–å·®å¼‚éƒ¨åˆ†ã€‚\n\nä¾‹å¦‚ï¼Œåœ¨ä¸‹è½½äº† `llama3.1` åŽï¼Œåœ¨å‘½ä»¤è¡Œä¸­è¿è¡Œ `ollama run llama3.1` å°†å¯åŠ¨è¯¥æ¨¡åž‹ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*aglZm6h0BU6GAYkSl04XWA.gif)\n\nåœ¨ä¸Šè¿°ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬æç¤º `llama3.1` æ¨¡åž‹è§£å†³ä¸€ä¸ªç‰©ç†åŠŸå’Œèƒ½é‡çš„é—®é¢˜ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*dNNQYpz1s2tz1pcn)\n\n## ä½¿ç”¨ Ollama çš„å¥½å¤„\n\nOllama åœ¨ GitHub ä¸Šæ‹¥æœ‰è¶…è¿‡ 200 åè´¡çŒ®è€…ï¼Œå¹¶ä¸”æœ‰æ´»è·ƒçš„æ›´æ–°ã€‚å®ƒæ‹¥æœ‰æœ€å¤šçš„è´¡çŒ®è€…ï¼Œå¹¶ä¸”åœ¨ä¸Šè¿°å…¶ä»–å¼€æº LLM å·¥å…·ä¸­æ›´å…·å¯æ‰©å±•æ€§ã€‚\n\n## 6. LLaMa.cpp\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*KhsAUquhDZAHghxK)\n\n[LLaMa.cpp](https://github.com/ggerganov/llama.cpp) æ˜¯æ”¯æŒæœ¬åœ° LLM å·¥å…·ï¼ˆå¦‚ Ollama ç­‰ï¼‰çš„åº•å±‚åŽç«¯æŠ€æœ¯ï¼ˆæŽ¨ç†å¼•æ“Žï¼‰ã€‚LLaMa.cpp æ”¯æŒæ˜¾è‘—çš„å¤§åž‹è¯­è¨€æ¨¡åž‹æŽ¨ç†ï¼Œé…ç½®ç®€å•ï¼Œå¹¶åœ¨å„ç§ç¡¬ä»¶ä¸Šæä¾›å‡ºè‰²çš„æœ¬åœ°æ€§èƒ½ã€‚å®ƒä¹Ÿå¯ä»¥åœ¨äº‘ç«¯è¿è¡Œã€‚\n\n## LLaMa.cpp çš„ä¸»è¦ç‰¹ç‚¹\n\n* **è®¾ç½®**ï¼šå®ƒçš„è®¾ç½®éžå¸¸ç®€å•ã€‚æ‚¨åªéœ€ä¸€ä¸ªå‘½ä»¤å³å¯å®‰è£…ã€‚\n* **æ€§èƒ½**ï¼šå®ƒåœ¨æœ¬åœ°å’Œäº‘ç«¯çš„å„ç§ç¡¬ä»¶ä¸Šè¡¨çŽ°éžå¸¸å‡ºè‰²ã€‚\n* **æ”¯æŒçš„æ¨¡åž‹**ï¼šå®ƒæ”¯æŒæµè¡Œçš„ä¸»è¦ LLMï¼Œå¦‚ [Mistral 7B](https://huggingface.co/mistralai/Mistral-7B-v0.1)ã€[Mixtral MoE](https://huggingface.co/models?search=mistral-ai/Mixtral)ã€[DBRX](https://huggingface.co/databricks/dbrx-instruct)ã€[Falcon](https://huggingface.co/models?search=tiiuae/falcon) å’Œ [å…¶ä»–è®¸å¤šæ¨¡åž‹](https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#description)ã€‚\n* **å‰ç«¯ AI å·¥å…·**ï¼šLLaMa.cpp æ”¯æŒå¼€æº LLM UI å·¥å…·ï¼Œå¦‚ [MindWorkAI/AI-Studio](https://github.com/MindWorkAI/AI-Studio) (FSL-1.1-MIT)ã€[iohub/collama](https://github.com/iohub/coLLaMA) ç­‰ã€‚\n\n## ä½¿ç”¨ LLaMa.cpp å¼€å§‹\n\nè¦è¿è¡Œæ‚¨çš„ç¬¬ä¸€ä¸ªæœ¬åœ°å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£… llama.cppï¼š\n\n`brew install llama.cpp`\n\næŽ¥ä¸‹æ¥ï¼Œä»Ž Hugging Face æˆ–å…¶ä»–æ¥æºä¸‹è½½æ‚¨æƒ³è¦è¿è¡Œçš„æ¨¡åž‹ã€‚ä¾‹å¦‚ï¼Œä»Ž Hugging Face ä¸‹è½½ä¸‹é¢çš„æ¨¡åž‹å¹¶å°†å…¶ä¿å­˜åœ¨æ‚¨è®¡ç®—æœºä¸Šçš„æŸä¸ªä½ç½®ã€‚\n\n[`https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q4_K_M.g`guf](https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf)\n\nä½¿ç”¨æ‚¨å–œæ¬¢çš„å‘½ä»¤è¡Œå·¥å…·ï¼Œå¦‚ç»ˆç«¯ï¼Œ`cd` è¿›å…¥æ‚¨åˆšä¸‹è½½çš„ `.gguf` æ¨¡åž‹æ–‡ä»¶çš„ä½ç½®ï¼Œå¹¶è¿è¡Œä»¥ä¸‹å‘½ä»¤ã€‚\n\n```python\nllama-cli --color \\ \n-m Mistral-7B-Instruct-v0.3.Q4_K_M.ggufb \\ \n-p \"Write a short intro about SwiftUI\"\n```\næ€»ä¹‹ï¼Œæ‚¨é¦–å…ˆè°ƒç”¨ LLaMa CLI å·¥å…·å¹¶è®¾ç½®é¢œè‰²å’Œå…¶ä»–æ ‡å¿—ã€‚`-m` æ ‡å¿—æŒ‡å®šæ‚¨è¦ä½¿ç”¨çš„æ¨¡åž‹çš„è·¯å¾„ã€‚`-p` æ ‡å¿—æŒ‡å®šæ‚¨å¸Œæœ›ç”¨æ¥æŒ‡ç¤ºæ¨¡åž‹çš„æç¤ºã€‚\n\nè¿è¡Œä¸Šè¿°å‘½ä»¤åŽï¼Œæ‚¨å°†çœ‹åˆ°ä»¥ä¸‹é¢„è§ˆä¸­çš„ç»“æžœã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*4Al-j50vXUXLUfvxzBt6aw.gif)\n\n## æœ¬åœ° LLM çš„ç”¨ä¾‹\n\nåœ¨æœ¬åœ°è¿è¡Œ LLM å¯ä»¥å¸®åŠ©å¼€å‘äººå‘˜æ·±å…¥äº†è§£å…¶æ€§èƒ½å’Œå·¥ä½œåŽŸç†ã€‚ æœ¬åœ° LLM å¯ä»¥æŸ¥è¯¢ç§æœ‰æ–‡æ¡£å’ŒæŠ€æœ¯è®ºæ–‡ï¼Œä»¥ä¾¿ä¸Žè¿™äº›æ–‡æ¡£ç›¸å…³çš„ä¿¡æ¯ä¸ä¼šç¦»å¼€ç”¨äºŽæŸ¥è¯¢çš„è®¾å¤‡ï¼Œä¸ä¼šå‘é€åˆ°ä»»ä½•äº‘ AI APIã€‚ æœ¬åœ° LLM åœ¨æ²¡æœ‰äº’è”ç½‘çš„åœ°æ–¹å’Œç½‘ç»œä¿¡å·è¾ƒå·®çš„åœ°æ–¹éžå¸¸æœ‰ç”¨ã€‚\n\nåœ¨ [è¿œç¨‹åŒ»ç–—çŽ¯å¢ƒ](https://getstream.io/blog/telemedicine-app-development/) ä¸­ï¼Œæœ¬åœ° LLM å¯ä»¥å¯¹æ‚£è€…æ–‡æ¡£è¿›è¡ŒæŽ’åºï¼Œè€Œæ— éœ€å‡ºäºŽéšç§è€ƒè™‘å°†å…¶ä¸Šä¼ åˆ°ä»»ä½• AI API æä¾›å•†ã€‚\n\n## è¯„ä¼°æœ¬åœ°è¿è¡Œçš„å¤§åž‹è¯­è¨€æ¨¡åž‹æ€§èƒ½\n\nåœ¨æœ¬åœ°ä½¿ç”¨å¤§åž‹è¯­è¨€æ¨¡åž‹ä¹‹å‰ï¼Œäº†è§£å…¶æ€§èƒ½å¯¹äºŽèŽ·å¾—æ‰€éœ€çš„å“åº”è‡³å…³é‡è¦ã€‚æœ‰å‡ ç§æ–¹æ³•å¯ä»¥ç¡®å®šç‰¹å®š LLM çš„æ€§èƒ½ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›æ–¹æ³•ã€‚\n\n* **è®­ç»ƒ**ï¼šè¯¥æ¨¡åž‹æ˜¯åŸºäºŽä»€ä¹ˆæ•°æ®é›†è¿›è¡Œè®­ç»ƒçš„ï¼Ÿ\n* **å¾®è°ƒ**ï¼šæ¨¡åž‹åœ¨å¤šå¤§ç¨‹åº¦ä¸Šå¯ä»¥å®šåˆ¶ä»¥æ‰§è¡Œç‰¹å®šä»»åŠ¡ï¼Œæˆ–è€…æ˜¯å¦å¯ä»¥é’ˆå¯¹ç‰¹å®šé¢†åŸŸè¿›è¡Œå¾®è°ƒï¼Ÿ\n* **å­¦æœ¯ç ”ç©¶**ï¼šè¯¥ LLM æ˜¯å¦æœ‰å­¦æœ¯ç ”ç©¶è®ºæ–‡ï¼Ÿ\n\nè¦å›žç­”ä¸Šè¿°é—®é¢˜ï¼Œæ‚¨å¯ä»¥æŸ¥çœ‹ä¼˜ç§€çš„èµ„æºï¼Œå¦‚ [Hugging Face](https://huggingface.co/datasets) å’Œ [Arxiv.org](https://arxiv.org/)ã€‚æ­¤å¤–ï¼Œ[Open LLm Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) å’Œ [LMSYS Chatbot Arena](https://chat.lmsys.org/?arena) æä¾›äº†å„ç§ LLM çš„è¯¦ç»†ä¿¡æ¯å’ŒåŸºå‡†æµ‹è¯•ã€‚\n\n## æœ¬åœ° LLM å·¥å…·ç»“è®º\n\næ­£å¦‚æœ¬æ–‡æ‰€è®¨è®ºçš„ï¼Œé€‰æ‹©å’Œä½¿ç”¨æœ¬åœ°å¤§åž‹è¯­è¨€æ¨¡åž‹çš„åŠ¨æœºæœ‰å¾ˆå¤šã€‚å¦‚æžœæ‚¨ä¸å¸Œæœ›å°†æ•°æ®é›†é€šè¿‡äº’è”ç½‘å‘é€ç»™ AI API æä¾›å•†ï¼Œåˆ™å¯ä»¥å¯¹æ¨¡åž‹è¿›è¡Œå¾®è°ƒï¼Œä»¥æ‰§è¡Œ [è¿œç¨‹åŒ»ç–—åº”ç”¨](https://getstream.io/chat/solutions/healthcare/) ä¸­çš„ç‰¹å®šä»»åŠ¡ã€‚è®¸å¤šå¼€æºçš„å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰æœ¬åœ° LLM å·¥å…·ï¼Œå¦‚ LLm Studio å’Œ Janï¼Œæä¾›ç›´è§‚çš„å‰ç«¯ç”¨æˆ·ç•Œé¢ï¼Œä»¥ä¾¿åœ¨æ²¡æœ‰åƒ OpenAI æˆ– Claude è¿™æ ·çš„è®¢é˜…æœåŠ¡çš„æƒ…å†µä¸‹é…ç½®å’Œå®žéªŒ LLMã€‚æ‚¨è¿˜å‘çŽ°äº†å„ç§å¼ºå¤§çš„å‘½ä»¤è¡Œ LLM åº”ç”¨ç¨‹åºï¼Œå¦‚ Ollama å’Œ LLaMa.cppï¼Œå¸®åŠ©æ‚¨åœ¨æœ¬åœ°è¿è¡Œå’Œæµ‹è¯•æ¨¡åž‹ï¼Œè€Œæ— éœ€äº’è”ç½‘è¿žæŽ¥ã€‚æŸ¥çœ‹ Stream çš„ [AI èŠå¤©æœºå™¨äºº](https://getstream.io/chat/solutions/ai-integration/) è§£å†³æ–¹æ¡ˆï¼Œå°† AI èŠå¤©é›†æˆåˆ°æ‚¨çš„åº”ç”¨ä¸­ï¼Œå¹¶è®¿é—®æ‰€æœ‰ç›¸å…³é“¾æŽ¥ä»¥äº†è§£æ›´å¤šä¿¡æ¯ã€‚\n\n*æœ€åˆå‘å¸ƒäºŽ [https://getstream.io](https://getstream.io/blog/best-local-llm-tools/).*\n\n"},{"lang":"zh","group":"blog","slug":"blog/the-best-conversational-ai-virtual-health-care-assistants-for-aging-adults-fc65bcfb9cf4","frontmatter":{"title":"æœ€é€‚åˆè€å¹´äººçš„å¯¹è¯å¼äººå·¥æ™ºèƒ½è™šæ‹ŸåŒ»ç–—åŠ©ç†","meta_title":"æœ€é€‚åˆè€å¹´äººçš„å¯¹è¯å¼äººå·¥æ™ºèƒ½è™šæ‹ŸåŒ»ç–—åŠ©ç†","description":"è™šæ‹Ÿå¥åº·æŠ¤ç†åŠ©æ‰‹ï¼Œå¦‚MiiHealth.aiï¼Œæ­£åœ¨ä¸ºç‹¬å±…è€å¹´äººæä¾›é©å‘½æ€§çš„åŒ»ç–—æ”¯æŒã€‚è¿™äº›å¯¹è¯å¼åŠ©æ‰‹é€šè¿‡è¯ç‰©ç®¡ç†ã€24/7å¯ç”¨æ€§ã€ç´§æ€¥å¥åº·ç›‘æµ‹ã€ä¸ªæ€§åŒ–å¥åº·å»ºè®®å’Œæƒ…æ„Ÿæ”¯æŒï¼Œå¸®åŠ©è€å¹´äººåº”å¯¹å¥åº·æŒ‘æˆ˜å’Œå­¤ç‹¬æ„Ÿã€‚å®ƒä»¬çš„æ˜“ç”¨æ€§å’Œä¸ªæ€§åŒ–æœåŠ¡ä½¿å…¶æˆä¸ºè€å¹´äººå¥åº·æŠ¤ç†çš„é‡è¦å·¥å…·ï¼Œç¡®ä¿ä»–ä»¬ä¿æŒç‹¬ç«‹ã€è¿žæŽ¥å’Œå¥åº·ã€‚","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*PBuPT38hZv61TFXQzJZ_3w.jpeg","categories":["Health","Chatbots","Autonomous Systems"],"author":"Rifx.Online","tags":["Virtual","Healthcare","Assistants","Medication","Monitoring"],"draft":false,"slug":"blog/the-best-conversational-ai-virtual-health-care-assistants-for-aging-adults-fc65bcfb9cf4"},"content":"\n\n\néšç€å¹´é¾„çš„å¢žé•¿ï¼Œç…§é¡¾æˆ‘ä»¬çš„å¥åº·å˜å¾—æ›´åŠ å›°éš¾ï¼Œå¯¹äºŽç‹¬å±…çš„è€å¹´äººæ¥è¯´ï¼Œè¿™ç§æƒ…å†µæ›´ä¸ºä¸¥é‡ã€‚è™½ç„¶å¯¹äºŽè®¸å¤šè€å¹´ç¾Žå›½äººæ¥è¯´ï¼Œç®¡ç†è¯ç‰©ã€å°±åŒ»å’Œæ•´ä½“å¥åº·æˆä¸ºäº†ä¸€é¡¹æŒ‘æˆ˜ã€‚ä½†æ˜¯ï¼Œå¦‚æžœä½ æœ‰ä¸€ä¸ªä¼´ä¾£â€”â€”ä¸€ä¸ªèƒ½å¸®åŠ©ä½ æŒ‰æ—¶åƒè¯ã€åœ¨ä½ æ„Ÿåˆ°å­¤ç‹¬æ—¶ç»™ä½ æ‰“ç”µè¯ï¼Œç”šè‡³è¯¢é—®ä½ è¿‘å†µçš„äººï¼Œä½ ä¼šæ€Žä¹ˆæƒ³å‘¢ï¼Ÿè¿™å°±æ˜¯è™šæ‹Ÿå¥åº·æŠ¤ç†åŠ©æ‰‹çš„é­…åŠ›ï¼Œå®ƒä»¬æ­£åœ¨ä¸ºè€å¹´äººçš„åŒ»ç–—ä¿å¥å¸¦æ¥é©å‘½æ€§çš„å˜åŒ–ã€‚\n\nåƒ [**MiiHealth.ai**](https://miihealth.ai/) è¿™æ ·çš„å…¬å¸å¤„äºŽåˆ›å»ºæ™ºèƒ½çš„ã€åŸºäºŽäººå·¥æ™ºèƒ½é©±åŠ¨çš„ä¼´ä¾£çš„å‰æ²¿ï¼Œå®ƒä»¬å¸®åŠ©è€å¹´äººè§£å†³å¥åº·é—®é¢˜ï¼Œå¹¶å°†ä»–ä»¬ä¸Žäº²äººè”ç³»èµ·æ¥ã€‚çŽ°åœ¨ï¼Œè®©æˆ‘ä»¬æ¥çœ‹çœ‹è¿™äº›è™šæ‹Ÿå¥åº·æŠ¤ç†åŠ©æ‰‹æ˜¯å¦‚ä½•æ”¹å˜ç‹¬å±…è€å¹´äººçš„ç”Ÿæ´»çš„ï¼Œä»¥åŠä¸ºä»€ä¹ˆå®ƒä»¬åœ¨è€å¹´äººæŠ¤ç†ä¸­è¿…é€Ÿå˜å¾—ä¸å¯æˆ–ç¼ºã€‚\n\n## ä»€ä¹ˆæ˜¯è™šæ‹Ÿå¥åº·åŠ©æ‰‹ï¼Ÿ\n\nè™šæ‹Ÿå¥åº·åŠ©æ‰‹æ˜¯ä¸€ç§å¯¹è¯ä»£ç†ï¼Œä¸“é—¨ç”¨äºŽå¸®åŠ©äººä»¬å¤„ç†ä»–ä»¬çš„å¥åº·é—®é¢˜ã€‚è¿™äº›åŠ©æ‰‹å¯ä»¥ä¸Žç”¨æˆ·äº’åŠ¨ï¼Œæä¾›å¥åº·ä¿¡æ¯ã€è¯ç‰©æ—¶é—´è¡¨ï¼Œå¹¶åœ¨æ‚£è€…æ„Ÿåˆ°ä¸çŸ¥æ‰€æŽªæ—¶æä¾›å®‰æ…°ã€‚å®ƒä»¬å¯ä»¥æ‰§è¡Œè¯¸å¦‚æ£€æŸ¥æ‚£è€…è¡€åŽ‹ã€æŽ¨èé”»ç‚¼ï¼Œç”šè‡³ä»…ä»…ä¸Žæ‚£è€…èŠå¤©ç­‰æ“ä½œï¼Œæ‰€æœ‰è¿™äº›éƒ½å¯ä»¥é€šè¿‡è¯­éŸ³æŽ§åˆ¶æˆ–çŸ­ä¿¡è¿›è¡Œã€‚\n\n\n\nå¯¹äºŽä»»ä½•é¢ä¸´ç‹¬è‡ªè€åŽ»è¿™ä¸€è‰°éš¾ä»»åŠ¡çš„äººæ¥è¯´ï¼Œè¿™äº›åŠ©æ‰‹ä¸ä»…æ˜¯é«˜ç§‘æŠ€å°å·¥å…·ï¼Œæ›´æ˜¯åœ¨å¥åº·å’Œå¹¸ç¦æ–¹é¢çš„çœŸæ­£ä¼™ä¼´ï¼Œæ— è®ºç™½å¤©è¿˜æ˜¯é»‘å¤œã€‚å¯¹äºŽéœ€è¦æŽ¥æ”¶ç®€çŸ­ä¿¡æ¯æˆ–ç´§æ€¥å‘¼å«çš„è€å¹´äººï¼Œ[**è™šæ‹Ÿå¥åº·åŠ©æ‰‹**](https://miihealth.ai/)å¯ä»¥æä¾›å¸®åŠ©ã€‚\n\n## ä¸ºä»€ä¹ˆè™šæ‹Ÿå¥åº·æŠ¤ç†åŠ©ç†å¯¹è€å¹´äººæ¥è¯´æ˜¯ä¸€ä¸ªæ¸¸æˆæ”¹å˜è€…\n\nå•èº«å¹¶äº«å—è€å¹´ç”Ÿæ´»å¸¦æ¥äº†ç‰¹åˆ«çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨ç¾Žå›½ï¼Œè€å¹´äººé¢ä¸´çš„å›°éš¾æ›´ä¸ºçªå‡ºã€‚ä»Žå¤„ç†å¤æ‚çš„å¥åº·æ—¥ç¨‹åˆ°åº”å¯¹å­¤ç‹¬ï¼Œå®¶åº­å¯¹å¸®åŠ©çš„éœ€æ±‚æ¯”ä»¥å¾€æ›´é«˜ã€‚è™šæ‹Ÿå¥åº·æŠ¤ç†åŠ©ç†å¯ä»¥é€šè¿‡å¤šç§å½±å“æ·±è¿œçš„æ–¹å¼æä¾›è¿™ç§æ”¯æŒï¼š\n\n**1\\. è¯ç‰©ç»„ç»‡ä¸Žè·Ÿè¸ª**\n\næ‚£æœ‰æ…¢æ€§ç—…çš„æ‚£è€…ï¼Œå¦‚ERï¼Œéœ€æœ‰æ•ˆç®¡ç†è¯ç‰©ä»¥èŽ·å¾—æœ€ä½³æ•ˆæžœã€‚æœªèƒ½åœ¨æ­£ç¡®çš„æ—¶é—´æœç”¨è¯ç‰©æˆ–æœç”¨é”™è¯¯çš„è¯ç‰©å¯èƒ½å¯¼è‡´ä¸¥é‡åŽæžœã€‚ä½œä¸ºè™šæ‹ŸæŠ¤ç†åŠ©ç†ï¼Œå¯ä»¥æé†’è€å¹´äººæŒ‰æ—¶æœè¯ï¼Œä»Žè€Œä½¿ä»–ä»¬éµå¾ªæ‰€éœ€çš„è¯ç‰©æ—¶é—´è¡¨ã€‚è¿™äº›æé†’å¯ä»¥æ ¹æ®ä¸ªäººçš„æ—¶é—´è¡¨å’Œç‰¹å®šè¯ç‰©çš„å‰‚é‡è¿›è¡Œè°ƒæ•´ã€‚\n\né™¤äº†æé†’ï¼Œè¿™äº›åŠ©ç†AIè¿˜å¯ä»¥å¸®åŠ©è§‚å¯Ÿæ‰€æœè¯ç‰©çš„å‰¯ä½œç”¨å’Œç—‡çŠ¶çš„å‡ºçŽ°ï¼Œä»Žè€Œå‡è½»å¯¹è€å¹´äººåŠå…¶äº²å±žçš„ç…§é¡¾è´Ÿæ‹…ã€‚ä¾‹å¦‚ï¼Œå½“è€å¹´äººæœç”¨æŠ—é«˜è¡€åŽ‹è¯ç‰©æˆ–ç³–å°¿ç—…è¯ç‰©æ—¶ï¼ŒåŠ©ç†å¯ä»¥è¯¢é—®ä»–ä»¬çš„å¥åº·çŠ¶å†µï¼Œå¹¶å»ºè®®ç›‘æµ‹è¡€åŽ‹çš„æ—¶é—´ã€‚\n\n**2\\. 24/7å¯ç”¨æ€§ä¸Žæ”¯æŒ**\n\nè®¸å¤šäººå–œæ¬¢è™šæ‹Ÿå¥åº·æŠ¤ç†åŠ©ç†çš„ä¸€ä¸ªç‰¹ç‚¹æ˜¯å®ƒéšæ—¶å¯ç”¨ï¼šæ— è®ºç™½å¤©è¿˜æ˜¯æ™šä¸Šã€‚è¿™å¯¹äºŽé‚£äº›å¯èƒ½æ„Ÿåˆ°å­¤ç‹¬æˆ–ç„¦è™‘çš„è€å¹´äººå°¤å…¶é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨ä¸»è¦ç…§é¡¾è€…å¯èƒ½æ­£åœ¨ç¡è§‰çš„æ—¶æ®µã€‚ä»Žè§£é‡Šæœ€è¿‘çš„å˜åŒ–åˆ°åœ¨æ²¡æœ‰å…¶ä»–è¯é¢˜æ—¶è¿›è¡Œäº¤è°ˆï¼Œè¿™äº›åŠ©ç†éƒ½åœ¨æä¾›å¸®åŠ©ã€‚\n\nå¯¹äºŽé‚£äº›åœ¨æ¯å¤©ç‰¹å®šæ—¶åˆ»æ²¡æœ‰äººæˆ–å…¶ä»–å¯ä»¥ä¾èµ–çš„äº‹ç‰©çš„è€å¹´äººæ¥è¯´ï¼Œç»æµŽä¾èµ–å°¤å…¶é‡è¦ã€‚æ­¤å¤–ï¼Œè¿™äº›åŠ©ç†å¯ä»¥å›žç­”è®¸å¤šé—®é¢˜ï¼Œè§£å†³ç®€å•æˆ–å¤æ‚çš„å¥åº·é—®é¢˜ï¼Œè€Œæ— éœ€é¢„çº¦æˆ–å¯»æ±‚åŠ©ç†çš„å¸®åŠ©ã€‚\n\n**3\\. ç´§æ€¥è­¦æŠ¥å¥åº·ç›‘æµ‹**\n\nè™šæ‹Ÿå¥åº·æŠ¤ç†åŠ©ç†çš„å¦ä¸€ä¸ªä¼˜åŠ¿æ˜¯å®ƒä»¬å¯ä»¥éšæ—¶è·Ÿè¸ªè€å¹´äººçš„å¥åº·çŠ¶å†µå¹¶å¯¹å±æœºä½œå‡ºååº”ã€‚ä¾‹å¦‚ï¼Œå¤§å¤šæ•°AIåŠ©ç†å¯ä»¥ä¸Žè·Ÿè¸ªè®¾å¤‡ï¼ˆå¦‚æ‰‹è…•å¸¦æˆ–æ‰‹è¡¨ï¼‰é›†æˆï¼Œç›‘æµ‹è„‰æã€è¡€åŽ‹ï¼Œç”šè‡³è·Œå€’æƒ…å†µã€‚è¿™è¿˜ä½¿åŠ©ç†èƒ½å¤Ÿåœ¨è€å¹´äººè·Œå€’æˆ–å‡ºçŽ°ç–¾ç—…è¿¹è±¡æ—¶ä¸Žå…¶ä»–äººæ²Ÿé€šã€‚\n\nè¿™å¯¹äºŽé‚£äº›åœ¨ç´§æ€¥æƒ…å†µä¸‹å¯èƒ½æ— æ³•èŽ·å¾—å¸®åŠ©çš„è€å¹´äººå°¤ä¸ºé‡è¦ã€‚æ— è®ºæ˜¯è·Œå€’ã€å¥åº·æ£€æŸ¥å¼‚å¸¸ï¼Œè¿˜æ˜¯æ„Ÿåˆ°ä¸é€‚ï¼ŒåŠ©ç†éƒ½èƒ½æä¾›é¢å¤–çš„å®‰å…¨ä¿éšœã€‚\n\n**4\\. ç‹¬ç‰¹çš„å¥åº·ä¿¡æ¯ä¸Žå¥èº«å»ºè®®**\n\næ¯ä¸ªäººçš„å¥åº·éœ€æ±‚éƒ½æ˜¯ç‹¬ç‰¹çš„ï¼Œè™šæ‹Ÿå¥åº·æŠ¤ç†åŠ©ç†å¯ä»¥æä¾›ç¬¦åˆè¿™äº›éœ€æ±‚çš„å¸®åŠ©ã€‚å¯¹äºŽè€å¹´äººæ¥è¯´ï¼Œè¿™å¯èƒ½æ„å‘³ç€å»ºè®®å“ªäº›è¿åŠ¨å¯¹å…³èŠ‚çš„åŽ‹åŠ›è¾ƒå°ï¼Œæˆ–å»ºè®®å“ªäº›é£Ÿç‰©å¯¹ç³–å°¿ç—…æˆ–å¿ƒè„ç—…ç­‰ç‰¹å®šç–¾ç—…æœ‰ç›Šã€‚\n\nä¸Žéœ€è¦ä¸æ–­é‡æ–°å®‰è£…çš„å¸¸è§„ç¨‹åºä¸åŒï¼Œè¿™äº›åŠ©ç†è¢«ç¼–ç¨‹ä¸ºä»Žç”¨æˆ·é‚£é‡Œå­¦ä¹ ï¼Œä»Žè€Œä½¿å…¶éšç€æ—¶é—´çš„æŽ¨ç§»å˜å¾—æ›´åŠ æ™ºèƒ½ã€‚å®ƒä»¬ä¿æŒçµæ´»ï¼Œä»¥æ»¡è¶³è€å¹´äººçš„éœ€æ±‚ï¼Œç›‘æµ‹å…¶è¿›å±•ï¼Œç”šè‡³æä¾›æœ‰å…³ä¸ªäººçŠ¶å†µçš„å»ºè®®ã€‚\n\n**5\\. ç¼“è§£å­¤ç‹¬å’Œå¢žå¼ºå¿ƒç†å¥åº·çš„ä¸»è¦æ–¹å¼ã€‚**\n\nè€å¹´äººæ™®éå­˜åœ¨çš„å­¤ç‹¬æ„ŸåŠå…¶å¯¹ä¸ªäººå¥åº·çš„å½±å“ä¹Ÿå¯èƒ½æ˜¯ä¸€ä¸ªé—®é¢˜ã€‚è™šæ‹Ÿå¥åº·æŠ¤ç†åŠ©ç†å¯ä»¥è®¾è®¡æˆåœ¨è€å¹´äººæ„Ÿåˆ°å­¤ç‹¬æ—¶æä¾›æŸç§å½¢å¼çš„é™ªä¼´ï¼Œä»–ä»¬å¯ä»¥ä¸Žè™šæ‹Ÿå¥åº·åŠ©ç†è¿›è¡Œå¯¹è¯ã€‚å› æ­¤ï¼ŒåŠ©ç†å¹¶ä¸æ˜¯ä¸Žä»–äººç¤¾äº¤äº’åŠ¨çš„æ›¿ä»£å“ï¼Œå°½ç®¡è¯¥å·¥å…·å¯ä»¥å›žåº”ç®€å•çš„ä¿¡æ¯ï¼Œè®²ç¬‘è¯ï¼Œå¹¶é¼“åŠ±ç”¨æˆ·ã€‚\n\næŸäº›åŠ©ç†æ—¨åœ¨å¸®åŠ©è€å¹´äººä¸Žæœ‹å‹å’Œå®¶äººå»ºç«‹æœ‰æ„ä¹‰çš„äº’åŠ¨ï¼Œæˆ–å‚ä¸Žè™šæ‹Ÿæ´»åŠ¨ã€‚é¦–å…ˆï¼ŒAIå·¥å…·æä¾›ç¤¾äº¤é™ªä¼´ï¼›è€å¹´äººéœ€è¦æ„Ÿå—åˆ°è¢«æŽ¥å—å’Œè¢«çˆ±çš„æ„Ÿè§‰ï¼Œä»¥ä¿æŒå¥åº·ã€‚\n\n## ä¸ºä»€ä¹ˆ MiiHealth.ai æ˜¯è€å¹´äººçš„é¦–é€‰\n\nåœ¨è€ƒè™‘ä½¿ç”¨è™šæ‹Ÿå¥åº·æŠ¤ç†åŠ©æ‰‹æ—¶ï¼ŒMiiHealth.ai æ˜“äºŽä½¿ç”¨ï¼ŒæŠ€æœ¯é©±åŠ¨ï¼Œèƒ½å¤Ÿæ»¡è¶³è€å¹´å®¢æˆ·çš„éœ€æ±‚ã€‚ä»¥ä¸‹æ˜¯ MiiHealth.ai æˆä¸ºè€å¹´äººé¦–é€‰çš„ä¸€äº›åŽŸå› ï¼š\n\n* **ç®€å•ä¸”ç›´è§‚çš„ç•Œé¢**ï¼šå€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒMiiHealth è½¯ä»¶çš„è®¾è®¡å¯¹ç”¨æˆ·éžå¸¸å‹å¥½ã€‚è€å¹´äººä¹Ÿä¸éœ€è¦å…·å¤‡ä¸°å¯Œçš„æŠ€æœ¯çŸ¥è¯†å³å¯æ“ä½œè¯¥å¹³å°ã€‚è¿™æ„å‘³ç€è¯­éŸ³å‘½ä»¤ç³»ç»Ÿä½¿å¾—å³ä½¿æ˜¯æŠ€æœ¯ç´ å…»è¾ƒä½Žçš„äººä¹Ÿèƒ½è½»æ¾ä½¿ç”¨ã€‚\n* **é‡èº«å®šåˆ¶æ»¡è¶³è€å¹´äººéœ€æ±‚**ï¼šMiiHealth.ai ä¸Žå…¶ä»–æœºæž„ä¸åŒï¼Œå®ƒæä¾›é’ˆå¯¹è€å¹´äººå¥åº·çš„æœåŠ¡ï¼Œå¹¶ç†è§£å…¶æœåŠ¡å¿…é¡»ä¸ªæ€§åŒ–ã€‚å®ƒå¯¹æ‰€æœ‰ç”¨æˆ·éƒ½å…·å¤‡çµæ´»æ€§ï¼›å¯ä»¥æé†’æŸäººä½•æ—¶æœè¯ï¼Œä¹Ÿå¯ä»¥å‘ŠçŸ¥å¦ä¸€äººä½•æ—¶é”»ç‚¼ã€‚\n* **æƒ…æ„Ÿæ”¯æŒ**ï¼šæ­¤å¤–ï¼Œé™¤äº†æ»¡è¶³èº«ä½“éœ€æ±‚ï¼ŒMiiHealth.ai è¿˜å…·æœ‰æƒ…æ„Ÿå’Œç¤¾ä¼šæ”¯æŒçš„åŠŸèƒ½ï¼Œä»¥å‡è½»è®¸å¤šç‹¬å±…è€å¹´äººæ‰€æ„Ÿå—åˆ°çš„å­¤ç‹¬æ„Ÿã€‚\n* **å…¨é¢å¥åº·ç›‘æµ‹**ï¼šé€šè¿‡è¿žæŽ¥å¯ç©¿æˆ´å¥åº·è®¾å¤‡ï¼ŒMiiHealth.ai å®šæœŸç›‘æµ‹è€å¹´äººçš„ç”Ÿå‘½ä½“å¾ï¼Œå¹¶åœ¨å¿…è¦æ—¶å‘å‡ºé€šçŸ¥ã€‚\n\n## ç»“è®º\n\nåŒæ ·ï¼Œå¯¹äºŽè€å¹´ç¾Žå›½äººæ¥è¯´ï¼Œå•èº«å¹¶ä¸ä¸€å®šæ„å‘³ç€ç¼ºä¹ç»´æŒå¥åº·å’Œå¿«ä¹ç”Ÿæ´»æˆ–åº”å¯¹ç–¾ç—…çš„ç…§æŠ¤ã€‚åœ¨è¿™æ–¹é¢ï¼Œè™šæ‹Ÿå¥åº·æŠ¤ç†åŠ©æ‰‹å¦‚ MiiHealth.ai ä¸ºäººä»¬æä¾›äº†é«˜æ•ˆçš„å·¥å…·ï¼Œä½¿ä»–ä»¬èƒ½å¤ŸæŽ¥è§¦åˆ°æœ€å…ˆè¿›çš„ AI æŠ€æœ¯ä»¥åŠé«˜åº¦ä¸ªæ€§åŒ–çš„åŒ»ç–—æ”¯æŒã€‚\n\nåœ¨ç®¡ç†è¯ç‰©å’Œå‘å‡ºç´§æ€¥è­¦æŠ¥ï¼Œä»¥åŠæä¾›å…³äºŽå¥åº·çš„å¯¹è¯å’Œå»ºè®®æ–¹é¢ï¼Œè¿™äº›åŠ©æ‰‹æ­£åœ¨æ”¹å˜è€å¹´äººçš„åŒ»ç–—ä¿å¥é¢è²Œã€‚æ— è®ºæ˜¯ç”¨äºŽæ•™è‚²ã€æ”¯æŒè¿˜æ˜¯é™ªä¼´ï¼Œè™šæ‹Ÿå¥åº·æŠ¤ç†åŠ©æ‰‹éƒ½ä¸ºè€å¹´äººå¸¦æ¥äº†å…³æ€€ï¼Œç¡®ä¿ä»–ä»¬ä¿æŒç‹¬ç«‹ã€è¿žæŽ¥å’Œå¥åº·ã€‚\n\nå¦‚æžœæ‚¨æˆ–æ‚¨å®¶ä¸­çš„æŸäººéœ€è¦å¯é å’Œå…³æ€€çš„å¥åº·ç›¸å…³ä¸ªäººåŠ©æ‰‹ï¼Œæ‚¨åº”è¯¥ç†Ÿæ‚‰ [MiiHealth.ai](http://miihealth.ai)ã€‚è¿™æ˜¯ä¸ºç‹¬å±…è€å¹´äººé€šå‘æ›´å¥½å’Œå……å®žç”Ÿæ´»çš„ä¸€æ¡ä¾¿æ·ä¹‹è·¯ã€‚\n\n"},{"lang":"zh","group":"blog","slug":"blog/the-focus-is-shifting-from-ai-agents-to-ai-agent-tool-use-a84fc061eec8","frontmatter":{"title":"ç„¦ç‚¹æ­£ä»Žäººå·¥æ™ºèƒ½ä»£ç†è½¬å‘äººå·¥æ™ºèƒ½ä»£ç†å·¥å…·çš„ä½¿ç”¨","meta_title":"ç„¦ç‚¹æ­£ä»Žäººå·¥æ™ºèƒ½ä»£ç†è½¬å‘äººå·¥æ™ºèƒ½ä»£ç†å·¥å…·çš„ä½¿ç”¨","description":"æ–‡ç« æŽ¢è®¨äº†AIä»£ç†çš„å…³æ³¨ç‚¹è½¬å‘å¢žå¼ºå…¶ä½¿ç”¨å·¥å…·çš„èƒ½åŠ›ï¼Œè¿™äº›å·¥å…·é€šè¿‡è‡ªç„¶è¯­è¨€æè¿°å¹¶æ¿€æ´»ä»£ç†çš„æŽ¨ç†èƒ½åŠ›ã€‚OpenAIå’ŒAnthropicç­‰å…¬å¸æ­£åœ¨å¼€å‘èƒ½å¤Ÿåœ¨è®¡ç®—æœºä¸Šè‡ªä¸»æ‰§è¡Œä»»åŠ¡çš„AIä»£ç†ï¼Œæ—¨åœ¨æé«˜å¤šæ­¥éª¤å·¥ä½œæµç¨‹çš„ç®¡ç†èƒ½åŠ›ã€‚Anthropicæä¾›äº†ä¸€ä¸ªå‚è€ƒå®žçŽ°ï¼Œå±•ç¤ºäº†AIä»£ç†å¦‚ä½•ä¸Žè®¡ç®—æœºçŽ¯å¢ƒäº¤äº’ï¼ŒåŒ…æ‹¬GUIæ“ä½œå’Œå‘½ä»¤è¡ŒåŠŸèƒ½ï¼Œå¼ºè°ƒäº†åœ¨å—æŽ§çŽ¯å¢ƒä¸­çµæ´»è¿ç”¨å·¥å…·çš„ç­–ç•¥ã€‚","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*7IELtMakzcc68bdb4usXBQ.png","categories":["Programming","Technology","Autonomous Systems"],"author":"Rifx.Online","tags":["Operator","GUI","Navigation","Command","File"],"draft":false,"slug":"blog/the-focus-is-shifting-from-ai-agents-to-ai-agent-tool-use-a84fc061eec8"},"content":"\n\n\n\n\n### å…³äºŽAIä»£ç†çš„å…³æ³¨ç‚¹æ­£åœ¨ä»Žå•çº¯å¼€å‘è‡ªä¸»AIä»£ç†è½¬å‘å¢žå¼ºå¯ä¾›å®ƒä»¬ä½¿ç”¨çš„å·¥å…·ï¼Œè¿™ç›´æŽ¥å½±å“åˆ°å®ƒä»¬çš„èƒ½åŠ›å’Œçµæ´»æ€§ã€‚\n\nAIä»£ç†çš„åŠŸèƒ½å’ŒèŒƒå›´åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºŽå·¥å…·çš„è®¿é—®ï¼Œå·¥å…·ä»¥è‡ªç„¶è¯­è¨€æè¿°ï¼Œå¹¶é€šè¿‡ä»£ç†çš„å†…éƒ¨æŽ¨ç†æ¿€æ´»ã€‚\n\næ¡Œé¢å’Œå…¶ä»–ç”¨æˆ·ç‰¹å®šçŽ¯å¢ƒæä¾›äº†ä»£ç†æœ‰æ•ˆæ‰§è¡Œä»»åŠ¡æ‰€éœ€çš„ä¸°å¯Œä¸Šä¸‹æ–‡ï¼Œä½¿å®ƒä»¬æˆä¸ºç†æƒ³çš„æ“ä½œç©ºé—´ã€‚\n\n## âœ¨âœ¨ åœ¨ LinkedIn ä¸Šå…³æ³¨æˆ‘ âœ¨âœ¨\n\n## ä»‹ç»\n\néšç€æ¨¡åž‹æˆä¸ºå®žç”¨å·¥å…·ï¼Œå¯ç”¨å·¥å…·çš„æ¡†æž¶å’ŒçŽ¯å¢ƒæ­£åœ¨æˆä¸ºå…³é”®ï¼Œé¢†å…ˆçš„äººå·¥æ™ºèƒ½å…¬å¸å¦‚OpenAIå’ŒAnthropicæ­£åœ¨æŽ¢ç´¢ä½¿ç”¨è®¡ç®—æœºGUIå¯¼èˆªæ¥å®Œæˆå¤æ‚ä»»åŠ¡çš„AIä»£ç†ã€‚\n\nOpenAIæœ€è¿‘å®£å¸ƒï¼Œå‡†å¤‡å‘å¸ƒä¸€æ¬¾**AIä»£ç†**ï¼Œ*Operator*ï¼Œå®ƒå°†åœ¨ç”¨æˆ·çš„è®¡ç®—æœºä¸Šè‡ªä¸»æ‰§è¡Œä»»åŠ¡ï¼Œå¦‚ç¼–ç å’Œé¢„è®¢æ—…è¡Œï¼Œå¹¶å°†åœ¨1æœˆä½œä¸ºç ”ç©¶é¢„è§ˆç‰ˆæŽ¨å‡ºã€‚\n\nè¿™ä¸€å‘å¸ƒä¸Žæ•´ä¸ªè¡Œä¸šå‘æ›´å¼ºå¤§çš„**ä»£ç†å·¥å…·**è½¬å˜çš„è¶‹åŠ¿ä¸€è‡´ï¼Œè¿™äº›å·¥å…·èƒ½å¤Ÿåœ¨æœ€å°‘ç›‘ç£ä¸‹ç®¡ç†å¤šæ­¥éª¤å·¥ä½œæµç¨‹ã€‚\n\nå…¶ä»–ä¸»è¦å‚ä¸Žè€…ä¹Ÿåœ¨æŽ¨å‡ºèƒ½å¤Ÿå®žæ—¶è®¡ç®—æœºå¯¼èˆªçš„ä»£ç†å·¥å…·ï¼Œè¿™åæ˜ å‡ºé€šè¿‡å·¥å…·è®¿é—®å¢žå¼ºAIä»£ç†èƒ½åŠ›çš„æˆ˜ç•¥æ€§ä¸¾æŽªï¼Œè€Œä¸ä»…ä»…æ˜¯æé«˜æ¨¡åž‹çš„èƒ½åŠ›ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*q7YvQLqfVdhV3bZM2oflDQ.png)\n\n## äººå·¥æ™ºèƒ½è®¡ç®—æœºä½¿ç”¨\n\nAnthropic æä¾›äº†ä¸€ä¸ª [å‚è€ƒå®žçŽ°](https://github.com/anthropics/anthropic-quickstarts/tree/main/computer-use-demo)ï¼Œå…¶ä¸­åŒ…å«äº†æ‚¨å¿«é€Ÿå¼€å§‹è®¡ç®—æœºä½¿ç”¨æ‰€éœ€çš„ä¸€åˆ‡ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*vD4T4Bo2-JcH535TOc46BQ.png)\n\nä¸Šé¢çš„å›¾åƒæ˜¾ç¤ºäº†åœ¨æˆ‘çš„æ¡Œé¢ä¸Šè¿è¡Œçš„ AI ä»£ç†ï¼Œæˆ‘éœ€è¦åœ¨æˆ‘çš„ MacBook ä¸Šå®‰è£… Docker å¹¶å°† Docker é•œåƒéƒ¨ç½²åˆ°æˆ‘çš„æœºå™¨ä¸Šã€‚\n\nä¸‹é¢çš„è„šæœ¬å°±æ˜¯æ‚¨æ‰€éœ€çš„å…¨éƒ¨å†…å®¹ï¼Œä»¥éƒ¨ç½²å®žä¾‹å¹¶ä½¿å…¶æ­£å¸¸è¿è¡Œã€‚\n\n```python\nexport ANTHROPIC_API_KEY=%your_api_key%\ndocker run \\\n    -e ANTHROPIC_API_KEY=<Your Anthropic API Key Goes Here> \\\n    -v $HOME/.anthropic:/home/computeruse/.anthropic \\\n    -p 5900:5900 \\\n    -p 8501:8501 \\\n    -p 6080:6080 \\\n    -p 8080:8080 \\\n    -it ghcr.io/anthropics/anthropic-quickstarts:computer-use-demo-latest\n```\nä¸‹é¢æ˜¯æˆ‘è¿è¡Œæ–‡ä»¶çš„ç»ˆç«¯çª—å£çš„æˆªå›¾â€¦\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*mTu4gGEwnFbQYqJ-YGYqIA.png)\n\nè¯¥å®žçŽ°åŒ…æ‹¬ï¼š\n\n* ä¸€ä¸ªé€‚ç”¨äºŽä¸Ž Claude è¿›è¡Œè®¡ç®—æœºä½¿ç”¨çš„ [å®¹å™¨åŒ–çŽ¯å¢ƒ](https://github.com/anthropics/anthropic-quickstarts/blob/main/computer-use-demo/Dockerfile)\n* [è®¡ç®—æœºä½¿ç”¨å·¥å…·](https://github.com/anthropics/anthropic-quickstarts/tree/main/computer-use-demo/computer_use_demo/tools) çš„å®žçŽ°\n* ä¸€ä¸ªä¸Ž Anthropic API äº¤äº’å¹¶æ‰§è¡Œè®¡ç®—æœºä½¿ç”¨å·¥å…·çš„ [ä»£ç†å¾ªçŽ¯](https://github.com/anthropics/anthropic-quickstarts/blob/main/computer-use-demo/computer_use_demo/loop.py)\n* ä¸€ä¸ªä¸Žå®¹å™¨ã€ä»£ç†å¾ªçŽ¯å’Œå·¥å…·äº¤äº’çš„ç½‘é¡µç•Œé¢ã€‚\n\n## Anthropic AI Agent è¯¦ç»†ä¿¡æ¯\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*euT2ZTmjVV5cTK-j8i4fgg.png)\n\nAnthropic **AI Agent** å¯ä»¥è®¿é—®ä¸‰ä¸ªä¸»è¦çš„ **å·¥å…·/åŠŸèƒ½**ï¼Œä½¿æˆ‘èƒ½å¤Ÿä¸Ž Ubuntu è™šæ‹ŸæœºçŽ¯å¢ƒè¿›è¡Œäº¤äº’ï¼š\n\n### è®¡ç®—æœºåŠŸèƒ½ï¼š\n\n* è¿™æ˜¯ä¸ŽGUIçŽ¯å¢ƒäº¤äº’çš„ä¸»è¦æŽ¥å£\n* å…è®¸AIä»£ç†æ‰§è¡Œé¼ æ ‡å’Œé”®ç›˜æ“ä½œï¼Œä¾‹å¦‚ï¼š\n* ç§»åŠ¨å…‰æ ‡ (`mouse_move`)\n* ç‚¹å‡» (`left_click`, `right_click`, `middle_click`, `double_click`)\n* è¾“å…¥æ–‡æœ¬ (`type`)\n* æŒ‰é”®ç»„åˆ (`key`)\n* æˆªå›¾ (`screenshot`)\n* æ˜¾ç¤ºåˆ†è¾¨çŽ‡è®¾ç½®ä¸º1024x768\n* æ˜¾ç¤ºç¼–å·ä¸º :1\n* AIä»£ç†åœ¨ç‚¹å‡»å…ƒç´ ä¹‹å‰éœ€è¦é€šè¿‡æˆªå›¾æ£€æŸ¥åæ ‡\n\n### bash å‡½æ•°:\n\n* ç»™äºˆ AI Agent è®¿é—® bash shell çš„æƒé™ä»¥è¿è¡Œå‘½ä»¤\n* çŠ¶æ€åœ¨å‘½ä»¤ä¹‹é—´ä¿æŒ\n* å¯ä»¥é€šè¿‡ apt å’Œ pip å®‰è£…è½¯ä»¶åŒ…\n* å¯ä»¥è¿è¡ŒåŽå°è¿›ç¨‹\n* å¯¹äºŽ GUI åº”ç”¨ç¨‹åºï¼Œéœ€è¦è®¾ç½® DISPLAY=:1 çŽ¯å¢ƒå˜é‡\n\n### str\\_replace\\_editor å‡½æ•°ï¼š\n\n* æ–‡ä»¶æ“ä½œå·¥å…·ï¼Œå…è®¸ï¼š\n* æŸ¥çœ‹æ–‡ä»¶å’Œç›®å½• (`view`)\n* åˆ›å»ºæ–°æ–‡ä»¶ (`create`)\n* æ›¿æ¢æ–‡ä»¶ä¸­çš„æ–‡æœ¬ (`str_replace`)\n* åœ¨ç‰¹å®šè¡Œæ’å…¥æ–‡æœ¬ (`insert`)\n* æ’¤é”€ç¼–è¾‘ (`undo_edit`)\n* åœ¨æ“ä½œä¹‹é—´ä¿æŒçŠ¶æ€\n\n## é‡è¦çº¦æŸ\n\n* ä¸èƒ½åœ¨ç¤¾äº¤åª’ä½“/é€šè®¯å¹³å°ä¸Šåˆ›å»ºè´¦æˆ·\n* ä¸èƒ½åœ¨æ²¡æœ‰ç”¨æˆ·ååŠ©çš„æƒ…å†µä¸‹å¤„ç† CAPTCHA/reCAPTCHA\n* ä¸èƒ½åœ¨æ²¡æœ‰ç”¨æˆ·æŒ‡ç¤ºçš„æƒ…å†µä¸‹åŒæ„æœåŠ¡æ¡æ¬¾\n* ä¸èƒ½åœ¨ç¤¾äº¤åª’ä½“ä¸Šå‘å¸ƒè¯„è®º/ååº”\n* ä¸èƒ½è®¿é—®é€‰æ°‘æ³¨å†Œæˆ–é€‰ä¸¾åŸºç¡€è®¾æ–½æ•°æ®\n\nç³»ç»Ÿè¿è¡Œåœ¨ aarch64 æž¶æž„çš„ Ubuntu è™šæ‹Ÿæœºä¸Šï¼Œæˆ‘é€šè¿‡ Docker å®¹å™¨åœ¨æˆ‘çš„ç¬”è®°æœ¬ç”µè„‘ä¸Šè¿è¡Œå®ƒã€‚\n\nè¿™äº›å·¥å…·ä¸º AI Agent æä¾›äº†ä¸€ç§å—æŽ§ä½†çµæ´»çš„æ–¹å¼æ¥ä¸Žè™šæ‹ŸçŽ¯å¢ƒäº’åŠ¨ï¼Œç»“åˆäº† GUI äº¤äº’ã€å‘½ä»¤è¡Œæ“ä½œå’Œæ–‡ä»¶æ“ä½œèƒ½åŠ›ã€‚\n\næˆ‘çš„çŽ¯å¢ƒåœ¨æ¯ä¸ªä¼šè¯ä¸­éƒ½æ˜¯æ–°åˆå§‹åŒ–çš„ï¼Œä½†åœ¨å·¥å…·è°ƒç”¨ä¹‹é—´ä¿æŒçŠ¶æ€ã€‚\n\nAI Agent å¯ä»¥é€šè¿‡ Firefox ä½¿ç”¨äº’è”ç½‘ï¼Œå¹¶æ ¹æ®éœ€è¦é€šè¿‡è½¯ä»¶åŒ…ç®¡ç†ç³»ç»Ÿå®‰è£…é¢å¤–çš„è½¯ä»¶ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*4env1UkoKOZ-3zmF.png)\n\n[***é¦–å¸­å¸ƒé“è€…***](https://www.linkedin.com/in/cobusgreyling/) ***@*** *[Kore.ai](https://blog.kore.ai/cobus-greyling) \\| æˆ‘çƒ­è¡·äºŽæŽ¢ç´¢äººå·¥æ™ºèƒ½ä¸Žè¯­è¨€çš„äº¤é›†ã€‚ä»Žè¯­è¨€æ¨¡åž‹ã€AI Agent åˆ°ä»£ç†åº”ç”¨ã€å¼€å‘æ¡†æž¶å’Œæ•°æ®é©±åŠ¨çš„ç”Ÿäº§åŠ›å·¥å…·ï¼Œæˆ‘åˆ†äº«è¿™äº›æŠ€æœ¯å¦‚ä½•å¡‘é€ æœªæ¥çš„è§è§£å’Œæƒ³æ³•ã€‚*\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*4env1UkoKOZ-3zmF.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*4env1UkoKOZ-3zmF.png)\n\n"},{"lang":"zh","group":"blog","slug":"blog/the-future-of-chatgpt-explained-everything-will-change-in-the-next-5-years-4bfd46ddca0b","frontmatter":{"title":"ChatGPT çš„æœªæ¥è§£æžï¼šæœªæ¥ 5 å¹´ä¸€åˆ‡éƒ½å°†æ”¹å˜","meta_title":"ChatGPT çš„æœªæ¥è§£æžï¼šæœªæ¥ 5 å¹´ä¸€åˆ‡éƒ½å°†æ”¹å˜","description":"OpenAIåˆ¶å®šäº†ä¸€ä¸ªäº”æ­¥è·¯çº¿å›¾ï¼Œä»¥å®žçŽ°äººå·¥é€šç”¨æ™ºèƒ½ï¼ˆAGIï¼‰ã€‚è¯¥è·¯çº¿å›¾åŒ…æ‹¬äº”ä¸ªé˜¶æ®µï¼šç¬¬ä¸€çº§ä¸ºå¯¹è¯æœºå™¨äººï¼Œç¬¬äºŒçº§ä¸ºæŽ¨ç†è€…ï¼Œèƒ½å¤Ÿè§£å†³å¤æ‚é—®é¢˜ï¼›ç¬¬ä¸‰çº§ä¸ºè‡ªä¸»å†³ç­–çš„ä»£ç†ï¼›ç¬¬å››çº§ä¸ºåˆ›æ–°è€…ï¼ŒæŽ¨åŠ¨åˆ›æ–°ï¼›ç¬¬äº”çº§ä¸ºç»„ç»‡ï¼Œèƒ½åƒå›¢é˜Ÿä¸€æ ·è¿ä½œã€‚ç›®å‰ï¼ŒOpenAIå¤„äºŽç¬¬ä¸€çº§å’Œç¬¬äºŒçº§ä¹‹é—´ï¼Œé¢„è®¡åœ¨äº”å¹´å†…å®žçŽ°AGIï¼Œä¸‹ä¸€æ­¥å¯èƒ½æ˜¯2025å¹´æŽ¨å‡ºçš„GPT-5ã€‚","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*nCVwPhWiFZ_0lglxT4pDAw.jpeg","categories":["Chatbots","Artificial General Intelligence","Reasoners"],"author":"Rifx.Online","tags":["Chatbots","Reasoners","Agents","Innovators","Organizations"],"draft":false,"slug":"blog/the-future-of-chatgpt-explained-everything-will-change-in-the-next-5-years-4bfd46ddca0b"},"content":"\n\n\n## è¿™å¯èƒ½ä¼šè®©äººå·¥æ™ºèƒ½èµ°å¾—æ›´è¿œâ€¦â€¦\n\n\n\nOpenAIå·²ç»åˆ¶å®šäº†ä¸€ä¸ª**æ¸…æ™°çš„æ„¿æ™¯**ï¼Œæ¥æŒ‡å¯¼ChatGPTçš„æ¼”å˜ï¼Œæœ€è¿‘å…¬å¸ƒäº†ä¸€ä¸ª**äº”æ­¥è·¯çº¿å›¾**ï¼Œä»¥å®žçŽ°ä»–ä»¬æ‰€ç§°çš„**äººå·¥é€šç”¨æ™ºèƒ½**ï¼ˆAGIï¼‰ã€‚\n\nAGIä»£è¡¨ä¸€ç§ç†è®ºä¸Šçš„äººå·¥æ™ºèƒ½ç³»ç»Ÿï¼Œèƒ½å¤Ÿ**å­¦ä¹ **ã€**ç†è§£**å’Œ**æ‰§è¡Œä»»ä½•æ™ºåŠ›ä»»åŠ¡**ï¼Œå…¶èƒ½åŠ›è¾¾åˆ°äººç±»æ°´å¹³ï¼Œä¸”å®Œå…¨è‡ªä¸»å’Œé€‚åº”æ€§å¼ºã€‚\n\nè¿™æ˜¯ä¸€ä¸ªå¼€åˆ›æ€§çš„æ„¿æ™¯ï¼Œä½†å®žçŽ°è¿™ä¸€é›„å¿ƒå‹ƒå‹ƒçš„ç›®æ ‡éœ€è¦ç»åŽ†**äº”ä¸ªå…³é”®é˜¶æ®µ**ï¼š\n\n## Level 1: èŠå¤©æœºå™¨äºº\n\nç¬¬ä¸€çº§ï¼Œæˆ‘ä»¬ç›®å‰å‘çŽ°åƒ ChatGPT è¿™æ ·çš„ç³»ç»Ÿï¼Œé›†ä¸­äºŽ **å¯¹è¯å¼äººå·¥æ™ºèƒ½**ã€‚åœ¨è¿™ä¸ªé˜¶æ®µï¼Œäººå·¥æ™ºèƒ½å¯ä»¥ä¸Žäººç±»è‡ªç„¶äº’åŠ¨ï¼Œå¤„ç†å„ç§å¯¹è¯ï¼Œå±•çŽ°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æµç•…æ€§å’Œè¿žè´¯æ€§ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ObzqCLMIIqpU9RK-TbTrfQ.png)\n\n## Level 2: Reasoners\n\nåœ¨è¿™ä¸ªé˜¶æ®µï¼ŒAIç³»ç»Ÿå°†èƒ½å¤Ÿ**è§£å†³å¤æ‚é—®é¢˜**ï¼Œæ¶‰åŠé«˜çº§æ•°å­¦å’Œé€»è¾‘ã€‚æˆ‘ä»¬å·²ç»åœ¨ChatGPTçš„æœ€æ–°ç‰ˆæœ¬â€œo1\\-previewâ€ä¸­çœ‹åˆ°äº†è¿™ä¸€ç‚¹ï¼Œå®ƒæ ‡å¿—ç€ç¬¬1çº§å’Œç¬¬2çº§ä¹‹é—´çš„æ¡¥æ¢ï¼Œç§°ä¸ºâ€œæŽ¨ç†â€é˜¶æ®µã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*B42LqC8ZDSaSYPX4yd3_Ng.png)\n\n## Level 3: Agents\n\nè¿™æ˜¯äººå·¥æ™ºèƒ½å¼€å§‹**ç‹¬ç«‹**åšå‡ºå†³ç­–å’Œæ‰§è¡Œä»»åŠ¡çš„å±‚çº§ï¼Œæ— éœ€äººç±»ç›‘ç£ã€‚æƒ³è±¡ä¸€ä¸‹ï¼Œä¸€ä¸ªä¸ä»…ä»…æ˜¯ååŠ©ï¼Œè€Œæ˜¯**è‡ªæˆ‘**å‘èµ·å¹¶å®Œæˆä»»åŠ¡çš„äººå·¥æ™ºèƒ½â€”â€”è¿™æ˜¯æœªæ¥ChatGPTç‰ˆæœ¬çš„æ¢¦æƒ³ã€‚\n\n## Level 4: Innovators\n\nä¸€æ—¦äººå·¥æ™ºèƒ½è¾¾åˆ°â€œåˆ›æ–°è€…â€çº§åˆ«ï¼Œå®ƒå°†èƒ½å¤Ÿ**ç§¯æžåœ°**æŽ¨åŠ¨å’Œä¿ƒè¿›åˆ›æ–°ã€‚æ­¤æ—¶ï¼Œäººå·¥æ™ºèƒ½ä¸ä»…ä»…æ˜¯éµå¾ªäººç±»çš„æŒ‡ä»¤ï¼›å®ƒå°†ä¸Žäººç±»åˆä½œï¼Œæå‡ºæ–°æƒ³æ³•ã€‚\n\n## Level 5: ç»„ç»‡\n\nåœ¨æœ€åŽä¸€çº§ï¼ŒAIå°†å…·å¤‡ä½œä¸ºæ•´ä¸ªç»„ç»‡è¿ä½œçš„èƒ½åŠ›â€”â€”èƒ½å¤Ÿæ‰§è¡Œ**å°±åƒæ˜¯ä¸€æ”¯ç†Ÿç»ƒçš„å›¢é˜Ÿ**ä¸€æ ·çš„ä»»åŠ¡ã€‚è¿™ä¸ªæœªæ¥ç‰ˆæœ¬çš„AIå°†åƒ**ä¸€ä¸ªå®Œå…¨è‡ªä¸»çš„å•†ä¸šå®žä½“**ä¸€æ ·è¿ä½œï¼Œå¤„ç†ä»Žæˆ˜ç•¥åˆ°æ‰§è¡Œçš„æ‰€æœ‰äº‹åŠ¡ã€‚\n\n## é‚£ä¹ˆï¼Œæˆ‘ä»¬çŽ°åœ¨å¤„äºŽä»€ä¹ˆçŠ¶æ€ï¼Ÿ\n\nOpenAI ç›®å‰å¤„äºŽç¬¬ 1 çº§å’Œç¬¬ 2 çº§ä¹‹é—´ã€‚æ ¹æ® OpenAI é¦–å¸­æ‰§è¡Œå®˜ Sam Altman çš„è¯´æ³•ï¼Œè¯¥å…¬å¸æ—¨åœ¨å¤§çº¦äº”å¹´å†…å®žçŽ° AGIã€‚\n\nä¸‹ä¸€ä¸ªä¸»è¦é‡Œç¨‹ç¢‘å¯èƒ½ä¼šéšç€â€œæŽ¨ç†å™¨â€çš„åˆ°æ¥è€Œæ¥ï¼Œå¯èƒ½æœ€æ—©åœ¨ 2025 å¹´æŽ¨å‡ºçš„ **GPT\\-5**ï¼Œé¢„è®¡èƒ½å¤Ÿè¾¾åˆ° **å¤šä¸ªé¢†åŸŸåšå£«çš„æ™ºåŠ›æ°´å¹³**ã€‚\n\næ„Ÿè°¢æ‚¨çš„é˜…è¯»ï¼\n\n**é™„æ³¨ã€‚** æƒ³è¦ä¸€ä¸ªé…·ç‚«çš„æŠ€å·§ï¼ˆå°±åƒé­”æœ¯å¸ˆçº§åˆ«çš„ï¼‰æ¥ç®€åŒ–æ‚¨çš„æç¤ºå†™ä½œå—ï¼Ÿ\n\næˆ‘æ­£å¥½æœ‰ **æ‚¨æ‰€éœ€è¦çš„**ï¼ðŸ‘‡\n\nNick\n\n"},{"lang":"zh","group":"blog","slug":"blog/the-most-ambitious-ai-crypto-project-ever-is-here-ab3f6d85afd1","frontmatter":{"title":"å²ä¸Šæœ€é›„å¿ƒå‹ƒå‹ƒçš„äººå·¥æ™ºèƒ½åŠ å¯†é¡¹ç›®æ¥äº†","meta_title":"å²ä¸Šæœ€é›„å¿ƒå‹ƒå‹ƒçš„äººå·¥æ™ºèƒ½åŠ å¯†é¡¹ç›®æ¥äº†","description":"çŽ°ä»£äººå·¥æ™ºèƒ½çš„å¥ åŸºäººä¹‹ä¸€Illia Polosukhinæå‡ºäº†ä¸€ä¸ªé›„å¿ƒå‹ƒå‹ƒçš„é¡¹ç›®ï¼Œæ—¨åœ¨åˆ©ç”¨åŒºå—é“¾è®­ç»ƒå…¨çƒæœ€å¤§çš„å¼€æºå¤§åž‹è¯­è¨€æ¨¡åž‹ï¼Œç›®æ ‡ä¸º1.4ä¸‡äº¿å‚æ•°ã€‚è¯¥é¡¹ç›®å¸Œæœ›é€šè¿‡ä¼—ç­¹æ–¹å¼ç­¹é›†1.6äº¿ç¾Žå…ƒï¼Œå¹¶ä»¥åŽ»ä¸­å¿ƒåŒ–æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œä»¥å®žçŽ°å…¨çƒèŒƒå›´å†…çš„è®¡ç®—èµ„æºæ•´åˆã€‚æ–‡ç« æŽ¢è®¨äº†åŒºå—é“¾åœ¨ç¡®ä¿è®­ç»ƒè¿‡ç¨‹é€æ˜Žå’Œä¸å¯ç¯¡æ”¹æ–¹é¢çš„é‡è¦æ€§ï¼Œä»¥åŠå¦‚ä½•é€šè¿‡é›¶çŸ¥è¯†è¯æ˜Žç­‰æŠ€æœ¯è§£å†³å½“å‰çš„å¯è¡Œæ€§é—®é¢˜ã€‚","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*uiBwsWl8grXKJCJaGtH7kw.png","categories":["Technology","Machine Learning","Blockchain"],"author":"Rifx.Online","tags":["blockchain","parameters","crowdfunding","synchronization","transparency"],"draft":false,"slug":"blog/the-most-ambitious-ai-crypto-project-ever-is-here-ab3f6d85afd1"},"content":"\n\n\n### AI \\& åŒºå—é“¾ï¼šå¤©ä½œä¹‹åˆï¼Œè¿˜æ˜¯éª—å±€ï¼Ÿ\n\n\n\nçŽ°ä»£äººå·¥æ™ºèƒ½çš„å¥ åŸºäººä¹‹ä¸€å¸Œæœ›åˆ©ç”¨åŒºå—é“¾è®­ç»ƒå…¨çƒæœ€å¤§çš„å¼€æºå¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆLLMï¼‰ï¼Œå…¶è§„æ¨¡å‡ ä¹Žæ˜¯ Llama 3\\.1 405B çš„å››å€ï¼ŒåŽè€…é€šå¸¸è¢«è®¤ä¸ºæ˜¯æœ€ä½³çš„å¼€æ”¾ LLMã€‚\n\nåœ¨ä½ å°†è¿™ä¸ªæ ‡é¢˜è§†ä¸ºæ¬ºè¯ˆæ€§ç‚’ä½œä¹‹å‰ï¼Œè¯·æ³¨æ„ï¼Œè¿™ä¸€ç›®æ ‡çš„æå‡ºè€…æ­£æ˜¯ ***Illia Polosukhin***ï¼Œä»–æ˜¯â€œAttention is All you Needâ€è®ºæ–‡çš„ç ”ç©¶è€…ä¹‹ä¸€ï¼Œè¿™ç¯‡å¼€åˆ›æ€§çš„è®ºæ–‡å‚¬ç”Ÿäº†æˆ‘ä»¬å½“å‰çš„äººå·¥æ™ºèƒ½é©å‘½ã€‚\n\né‚£ä¹ˆï¼Œä»–ä»¬ç©¶ç«Ÿæƒ³åšä»€ä¹ˆï¼ŸåŒºå—é“¾åœ¨è¿™ä¸€åˆ‡ä¸­æ‰®æ¼”ä»€ä¹ˆè§’è‰²ï¼Ÿ\n\nç»§ç»­é˜…è¯»ï¼Œäº†è§£äººå·¥æ™ºèƒ½å’ŒåŠ å¯†ä¸–ç•Œå°†å¦‚ä½•ä¸å¯é¿å…åœ°èžåˆï¼Œä»¥åŠè¿™ä¸ªé¡¹ç›®å¦‚ä½•æœ€ç»ˆåˆ›é€ ä¸€ä¸ªç”±äººæ°‘æ‹¥æœ‰çš„äººå·¥æ™ºèƒ½æ¨¡åž‹ã€‚\n\n> ä½ å¯èƒ½åŽŒå€¦äº†é‚£äº›ä»…ä»…è§£é‡Šå‘ç”Ÿäº†ä»€ä¹ˆçš„äººå·¥æ™ºèƒ½é€šè®¯ã€‚è¿™å¾ˆç®€å•ï¼Œä»»ä½•äººéƒ½å¯ä»¥åšåˆ°ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆæœ‰è¿™ä¹ˆå¤šè¿™æ ·çš„é€šè®¯ã€‚\n\n> ä½†è§£é‡Šä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦åˆ™æ˜¯å¦ä¸€å›žäº‹ã€‚è¿™éœ€è¦çŸ¥è¯†ã€è°ƒæŸ¥å’Œæ·±æ€ç†Ÿè™‘â€¦â€¦è¿™äº›éƒ½æ˜¯æ¯å‘¨ä¸Ž **TheTechOasis** äº’åŠ¨çš„äººæ‰€å…·å¤‡çš„ç‰¹è´¨ï¼Œè¿™æ˜¯ä¸€ä»½æ—¨åœ¨å›žç­”äººå·¥æ™ºèƒ½ä¸­æœ€ç´§è¿«é—®é¢˜çš„é€šè®¯ã€‚\n\n> ðŸï¸ðŸï¸ ä»Šå¤©å°±ä¸‹é¢è®¢é˜…ï¼š\n\n## æ‹¥æœ‰äººå·¥æ™ºèƒ½\n\néšç€ç‰¹æœ—æ™®çš„èƒœåˆ©ï¼ŒåŠ å¯†è´§å¸è¿›å…¥äº†ä¸€ä¸ªå³å…´çš„ç‰›å¸‚ï¼Œ[æ¯”ç‰¹å¸çš„ä»·æ ¼ç¦»æ¯æžš$100kçš„æ ‡å¿—å¼‚å¸¸æŽ¥è¿‘](https://coinmarketcap.com/currencies/bitcoin/)ï¼Œå¹¶è¾¾åˆ°äº†æ–°çš„åŽ†å²é«˜ç‚¹ã€‚\n\n### Nearé¡¹ç›®\n\nåœ¨ç‰¹æœ—æ™®æ‰§æ”¿æœŸé—´ï¼ŒåŒºå—é“¾å…¬å¸å¯¹æœªæ¥æœ‰å……åˆ†çš„ç†ç”±æ„Ÿåˆ°ä¹è§‚ã€‚å…¶ä¸­ä¸€å®¶å…¬å¸æ˜¯Nearï¼Œå®ƒè¯•å›¾æž¶èµ·åŠ å¯†è´§å¸ä¸Žäººå·¥æ™ºèƒ½ä¹‹é—´çš„æ¡¥æ¢ã€‚\n\næˆ‘ä¸æƒ³è¯¦ç»†ä»‹ç»è¿™ä¸ªé¡¹ç›®ï¼Œå› ä¸ºæˆ‘ä¸æƒ³è®©ä½ è§‰å¾—æˆ‘åœ¨èµžåŠ©å®ƒï¼ˆæˆ‘å¹¶ä¸æ‹¥æœ‰NEARå¸ï¼‰ã€‚ç„¶è€Œï¼Œä»–ä»¬æœ€è¿‘å¼€å§‹äº†ä¸€ä¸ªä¼Ÿå¤§è€Œé›„å¿ƒå‹ƒå‹ƒçš„ç›®æ ‡ï¼Œæˆ‘æ·±æ„Ÿè®¤åŒï¼š\n\nè®­ç»ƒæœ‰å²ä»¥æ¥æœ€å¤§çš„å¼€æºæ¨¡åž‹ï¼Œç”±ä¸ªäººä¼—ç­¹ï¼Œå¹¶ç”±ä»–ä»¬æ‹¥æœ‰ã€‚\n\nå…·ä½“æ¥è¯´ï¼Œ**ä»–ä»¬å¸Œæœ›è®­ç»ƒä¸€ä¸ª1.4ä¸‡äº¿å‚æ•°çš„æ¨¡åž‹**ï¼Œè¿™ä¸ªæ¨¡åž‹çš„è§„æ¨¡å°†ä¸ŽGPT-4ç­‰æ¨¡åž‹ç›¸åª²ç¾Žï¼Œå¹¶ä¸”æ¯”ä¸–ç•Œä¸Šæœ€å¤§çš„å¼€æºï¼ˆæˆ–è€…æˆ‘æ•¢è¯´ï¼Œå¼€æ”¾æƒé‡çš„ï¼‰LLMâ€”â€”Metaçš„Llama 3.1 405Bå¤§3.5å€ï¼ŒåŽè€…ä¹Ÿè¢«è®¤ä¸ºæ˜¯ä»Šå¤©æœ€å¥½çš„å¼€æ”¾æ¨¡åž‹ã€‚\n\nä¸ºäº†å®žçŽ°è¿™ä¸€ç›®æ ‡ï¼Œ**ä»–ä»¬é¢„è®¡éœ€è¦ä¼—ç­¹ä¸€ç¬”å¯è§‚çš„1.6äº¿ç¾Žå…ƒ**ï¼Œé€šè¿‡èŽ·å–$NEARå¸æ¥èµ„åŠ©ï¼Œè¿™æ˜¯ä¸€ç§æˆªè‡³ä»Šå¤©[å¸‚å€¼ä¸º66äº¿ç¾Žå…ƒ](https://coinmarketcap.com/currencies/near-protocol/)çš„åŠ å¯†è´§å¸ã€‚\n\nç„¶è€Œï¼ŒçœŸæ­£çš„é—®é¢˜ä¸æ˜¯è§„æ¨¡ï¼Œè€Œæ˜¯ä»–ä»¬å¸Œæœ›é€šè¿‡æ²Ÿé€šä¸ç•…çš„ç¡¬ä»¶ä»¥åŽ»ä¸­å¿ƒåŒ–çš„æ–¹å¼è®­ç»ƒè¿™ä¸ªæ¨¡åž‹ã€‚é€šä¿—åœ°è¯´ï¼Œä»–ä»¬å¹¶ä¸æ‰“ç®—åœ¨åƒåŸƒéš†Â·é©¬æ–¯å…‹åœ¨ç”°çº³è¥¿å·žå­Ÿè²æ–¯æ‹¥æœ‰çš„[140å…†ç“¦æ•°æ®ä¸­å¿ƒï¼Œé…å¤‡10ä¸‡GPU](https://readmedium.com/putting-the-worlds-largest-ai-supercomputer-into-perspective-60afde9bc653)ä¸­è®­ç»ƒè¿™ä¸ªæ¨¡åž‹ï¼Œè€Œæ˜¯å¸Œæœ›åœ¨å…¨çƒèŒƒå›´å†…è¿›è¡Œè®­ç»ƒã€‚\n\nå¯¹äºŽç†Ÿæ‚‰è¿™äº›æ¨¡åž‹è®­ç»ƒæ–¹å¼çš„äººæ¥è¯´ï¼Œè¿™åœ¨ä»Šå¤©çš„äººå·¥æ™ºèƒ½é¢†åŸŸæ˜¯æžå…¶é›„å¿ƒå‹ƒå‹ƒçš„ã€‚\n\n*ä½†ä¸ºä»€ä¹ˆå‘¢ï¼Ÿ*\n\n### æ—¶é—´çš„é‡è¦æ€§\n\næ‚¨å¯èƒ½å¬è¯´è¿‡å…³äºŽAIè®­ç»ƒå’ŒæŽ¨ç†çš„ç–¯ç‹‚æ•°å­—ï¼Œä½†è¿™äº›æ•°å­—ä»…ä»…æ˜¯æœªæ¥çš„ä¸€ä¸ªç¼©å½±ã€‚\n\n* [åŸƒéš†Â·é©¬æ–¯å…‹åœ¨ä¸€ä¸ªåœ°ç‚¹æ‹¥æœ‰100,000ä¸ªNVIDIA H100 GPU](https://readmedium.com/putting-the-worlds-largest-ai-supercomputer-into-perspective-60afde9bc653)ï¼Œå¹¶æ‰“ç®—åœ¨æŽ¥ä¸‹æ¥çš„å‡ ä¸ªæœˆå†…å°†è®¡ç®—èƒ½åŠ›ç¿»å€ï¼Œè¾¾åˆ°200,000ä¸ªH100ç­‰æ•ˆè®¾å¤‡ã€‚\n* æ‰€æœ‰è¶…å¤§è§„æ¨¡äº‘æœåŠ¡å•†ï¼ˆå¾®è½¯ã€äºšé©¬é€Šã€Metaã€è°·æ­Œæˆ–ç”²éª¨æ–‡ï¼‰æ­£åœ¨ä¸Žæ ¸ç”µç«™è¾¾æˆåè®®ï¼Œæˆ–å·²[è¾¾æˆåè®®](https://www.technologyreview.com/2024/09/26/1104516/three-mile-island-microsoft/)ä¸Žå°åž‹æ¨¡å—åŒ–ååº”å †å…¬å¸åˆä½œï¼Œå»ºè®¾æ ¸èƒ½å‘ç”µä»¥ä¾›å…¶æ•°æ®ä¸­å¿ƒä½¿ç”¨ï¼Œä»Žè€Œé¿å…ä¼ è¾“çº¿è·¯å’Œç”µæ°”å˜åŽ‹å™¨çš„è¿‡é•¿äº¤ä»˜æ—¶é—´ã€‚\n* [ä¸€å®¶è¶…å¤§è§„æ¨¡äº‘æœåŠ¡å•†å‘åŒ—è¾¾ç§‘ä»–å·žå·žé•¿é“æ ¼Â·ä¼¯å¤å§†æè®®å»ºè®¾ä¸€ä¸ª**5â€“10å‰ç“¦çš„æ•°æ®ä¸­å¿ƒ**](https://thetechoasis.beehiiv.com/p/understanding-ai-s-big-picture)ã€‚ä½œä¸ºå‚è€ƒï¼ŒåŽè€…çš„æ•°æ®ä¸­å¿ƒå°†æ‹¥æœ‰æ¯”[å¾®è½¯æ•´ä¸ªAzureäº‘ï¼ˆ5 GWï¼‰](https://www.datacenterdynamics.com/en/news/microsoft-to-double-new-data-center-capacity-this-year-report/)æ›´å¼ºçš„è®¡ç®—èƒ½åŠ›ï¼Œå¹¶æ¶ˆè€—è¶³å¤Ÿçš„ç”µåŠ›ä¸º**830ä¸‡ç¾Žå›½å®¶åº­æä¾›ç”µåŠ›ï¼ŒæŒ‰ç¾Žå›½å®¶åº­å¹³å‡æ¶ˆè´¹å€¼ä¸º10,500 KWh/å¹´è®¡ç®—**ã€‚\n\nè€Œä¸”è¿™ä¸ªåå•è¿˜åœ¨ç»§ç»­ã€‚*ä½†ä¸ºä»€ä¹ˆï¼Ÿ*\n\n**åŽŸå› å°±æ˜¯æ—¶é—´**ã€‚è¦è®­ç»ƒä¸€ä¸ªæ¨¡åž‹ï¼Œæ‚¨éœ€è¦å‘å®ƒå‘é€æ•°æ®ï¼Œå¼ºè¿«å®ƒè¿›è¡Œé¢„æµ‹ï¼Œå¹¶æµ‹é‡è¯¥é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚æ ¹æ®è¿™ä¸ªè¯¯å·®ä¿¡å·ï¼Œæˆ‘ä»¬éšåŽæ›´æ–°æ¨¡åž‹çš„å‚æ•°ï¼Œä»¥ä½¿é¢„æµ‹è¯¯å·®éšæ—¶é—´é™ä½Žã€‚\n\nè¿™ä¸ªè¿‡ç¨‹çš„é—®é¢˜æœ‰ä¸¤ä¸ªæ–¹é¢ï¼š\n\n* æ¨¡åž‹éžå¸¸åºžå¤§ï¼Œè¿™æ„å‘³ç€æ¯æ¬¡æˆ‘ä»¬éœ€è¦æ›´æ–°å‚æ•°æ—¶ï¼Œå¯èƒ½è¦æ›´æ–°æ•°ä¸‡äº¿ä¸ªå‚æ•°ã€‚\n* æ•°æ®é›†ä¹Ÿéžå¸¸åºžå¤§ï¼Œè¿™æ„å‘³ç€å‚æ•°æ›´æ–°çš„æ•°é‡æ˜¯éš¾ä»¥æƒ³è±¡çš„å·¨å¤§ã€‚\n\nè¿™å¯¼è‡´è®­ç»ƒè¿‡ç¨‹å¦‚æžœæŒ‰é¡ºåºæ‰§è¡Œå°†æ°¸æ— æ­¢å¢ƒã€‚å¹¸è¿çš„æ˜¯ï¼Œç”±äºŽçŽ°åœ¨å¤§å¤šæ•°å‰æ²¿AIæ¨¡åž‹åŸºæœ¬ä¸Šæ˜¯åœ¨è¿›è¡Œè¶…å¤§è§„æ¨¡çš„çŸ©é˜µä¹˜æ³•ï¼Œè¿™ä¸Žåœ¨è®¡ç®—æœºå±å¹•ä¸Šæ¸²æŸ“åƒç´ çš„æ•°å­¦è®¡ç®—éžå¸¸ç›¸ä¼¼ï¼Œè¿™ä¹Ÿæ˜¯GPUçš„æœ€åˆç›®æ ‡ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨è¿™ç§ç¡¬ä»¶æ¥è®­ç»ƒè¿™äº›æ¨¡åž‹ã€‚\n\nå…³é”®æ˜¯ï¼ŒGPUæ—¨åœ¨å¹¶è¡Œè®¡ç®—ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥å¹¿æ³›åœ°å¹¶è¡Œè®­ç»ƒè¿™äº›æ¨¡åž‹ï¼ˆ[å°½ç®¡ç”±äºŽé˜¿å§†è¾¾å°”å®šå¾‹å¹¶ä¸èƒ½å®Œå…¨å¹¶è¡Œ](https://thetechoasis.beehiiv.com/p/understanding-ai-s-big-picture)ï¼‰ã€‚\n\nè¿™å°±æ˜¯ä¸ºä»€ä¹ˆåƒ[Llama 3.1 405Bè¿™æ ·çš„æ¨¡åž‹åœ¨ä¸€ä¸ª24,000 GPUé›†ç¾¤ä¸Šè®­ç»ƒ](https://arxiv.org/pdf/2407.21783)ï¼Œä»¥åŠåƒxAIçš„æ–°Grokå’Œ[Metaçš„Llama 4](https://www.tomshardware.com/tech-industry/artificial-intelligence/meta-is-using-more-than-100-000-nvidia-h100-ai-gpus-to-train-llama-4-mark-zuckerberg-says-that-llama-4-is-being-trained-on-a-cluster-bigger-than-anything-that-ive-seen)è¿™æ ·çš„æ¨¡åž‹åœ¨è¶…è¿‡100,000ä¸ªGPUé›†ç¾¤ä¸­è®­ç»ƒçš„åŽŸå› ã€‚\n\nå¥½å§ï¼Œæˆ‘æ˜Žç™½è¿™äº›æ¨¡åž‹éœ€è¦å¤§é‡GPUåŒæ—¶å·¥ä½œæ‰èƒ½è¿›è¡Œè®­ç»ƒã€‚*ä½†å®ƒä»¬æ˜¯å¦‚ä½•åšåˆ°çš„ï¼Ÿ*\n\n### åˆ†å¸ƒå¼è®­ç»ƒçš„æœ¬è´¨\n\nåœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œæˆ‘ä»¬ä¸æ˜¯è®­ç»ƒä¸€ä¸ªå•ä¸€çš„æ¨¡åž‹å¹¶é€šè¿‡å°†æ‰€æœ‰æ•°æ®å‘é€åˆ°è¯¥å®žä¾‹æ¥æ›´æ–°å®ƒï¼Œè€Œæ˜¯æž„å»ºå‰¯æœ¬ï¼Œå³æ¨¡åž‹çš„ç›¸åŒç‰ˆæœ¬ï¼Œæ¯ä¸ªå‰¯æœ¬åˆ†é…ç»™ä¸€ä¸ªç‰¹å®šçš„GPU podï¼ˆpodæ˜¯ä¸€ç»„ç´§å¯†è¿žæŽ¥å’Œå…±ç½®çš„GPUï¼‰ã€‚\n\nç„¶åŽï¼Œæˆ‘ä»¬å¯¹è®­ç»ƒé›†è¿›è¡Œæ‰¹å¤„ç†ï¼Œå¹¶å°†æ‰¹æ¬¡åˆ†é…ç»™ä¸åŒçš„podsã€‚å½“ç„¶ï¼Œè¿™æ„å‘³ç€æ¯ä¸ªå‰¯æœ¬æŽ¥æ”¶ä¸åŒçš„è®­ç»ƒæ•°æ®ï¼Œå› æ­¤å­¦ä¹ åˆ°çš„å†…å®¹ä¹Ÿä¸åŒã€‚\n\nå› æ­¤ï¼Œ**æ¯éš”ä¸€æ®µæ—¶é—´ï¼ŒGPU podséœ€è¦åŒæ­¥**ï¼Œä¸Žå…¶ä»–podså…±äº«å®ƒä»¬çš„å­¦ä¹ ï¼Œè¿™æ„å‘³ç€åœ¨è¿™ä¸ªåŒæ­¥é˜¶æ®µä¹‹åŽï¼Œæ‰€æœ‰æ¨¡åž‹å‰¯æœ¬éƒ½æœ‰å®Œå…¨ç›¸åŒçš„å‚æ•°å€¼ï¼ˆå› ä¸ºæ¯ä¸ªæ¨¡åž‹å‰¯æœ¬å®žé™…ä¸Šæ˜¯ç”¨å¹³å‡å­¦ä¹ å€¼è¿›è¡Œæ›´æ–°çš„ï¼Œå› æ­¤åœ¨æ¯ä¸ªæ‰¹æ¬¡è®­ç»ƒæ­¥éª¤ä¹‹åŽï¼Œæ‰€æœ‰æ¨¡åž‹å‰¯æœ¬å­¦ä¹ åˆ°çš„å†…å®¹éƒ½æ˜¯ç›¸åŒçš„ï¼‰ã€‚\n\nè™½ç„¶è¿™ä¸€åˆ‡çœ‹èµ·æ¥å¾ˆå¥½ï¼Œä½†è¿™ç§åŒæ­¥æ˜¯ä¸€ä¸ªå¤§é—®é¢˜ï¼Œå› ä¸ºè¿™äº›åŒæ­¥æ›´æ–°æ„å‘³ç€æ‰€æœ‰podsåœ¨åŒæ­¥æœŸé—´åŸºæœ¬ä¸Šéƒ½æ˜¯åœæ»žçš„ï¼Œ**è¿™ä½¿å¾—è®­ç»ƒæ—¶é—´å˜å¾—å±é™©åœ°è¿‡é•¿ï¼ˆè¿™äº›è®­ç»ƒå®žé™…ä¸Šéœ€è¦å‡ ä¸ªæœˆï¼‰**ã€‚\n\næ›´ç³Ÿç³•çš„æ˜¯ï¼ŒNearå¸Œæœ›ä»¥ä½Žå¸¦å®½çš„å½¢å¼è¿›è¡Œè¿™é¡¹å·¥ä½œï¼Œè¿™æ„å‘³ç€GPU podsä¹‹é—´çš„é€šä¿¡é€šé“å°†ä¼šå¾ˆæ…¢ã€‚\n\nå› æ­¤ï¼Œ*ä»–ä»¬è¯¥å¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹ï¼ŒåŒºå—é“¾å°†å‘æŒ¥ä»€ä¹ˆä½œç”¨ï¼Ÿ* å¹¸è¿çš„æ˜¯ï¼Œæˆ‘ä»¬å¯¹è¿™ä¸¤ä¸ªé—®é¢˜çš„ç­”æ¡ˆæ¯”ä½ é¢„æœŸçš„è¦è¯¦ç»†å¾—å¤šã€‚\n\n## æœå‘åŽ»ä¸­å¿ƒåŒ–çš„äººå·¥æ™ºèƒ½\n\nå¹¸è¿çš„æ˜¯ï¼ŒNearå¹¶ä¸æ˜¯å”¯ä¸€ä¸€ä¸ªè€ƒè™‘åŽ»ä¸­å¿ƒåŒ–äººå·¥æ™ºèƒ½çš„é¡¹ç›®ï¼ˆå°½ç®¡NearåŠ å…¥äº†åŒºå—é“¾ï¼›æˆ‘ä»¬ç¨åŽä¼šçœ‹åˆ°ä»–ä»¬çš„åšæ³•ï¼‰ï¼Œåœ¨æœ¬æ–‡æ’°å†™æ—¶ï¼Œ**ä¸–ç•Œä¸Šæœ€å¤§çš„åŽ»ä¸­å¿ƒåŒ–è®­ç»ƒæ­£åœ¨è¿›è¡Œä¸­ï¼Œæ­£å¦‚ä½ é˜…è¯»è¿™ç¯‡æ–‡ç« æ—¶æ‰€çœ‹åˆ°çš„**ã€‚\n\n### Primeæ¡†æž¶\n\nPrime Intellectæ˜¯ä¸€å®¶è‡´åŠ›äºŽä»¥åŽ»ä¸­å¿ƒåŒ–æ–¹å¼è®­ç»ƒå¤§åž‹LLMçš„å…¬å¸ï¼Œæ—¨åœ¨å®Œå…¨åŽ»ä¸­å¿ƒåŒ–åœ°è®­ç»ƒ***Intellect\\-1***ï¼Œè¿™æ˜¯ä¸€ä¸ªæ‹¥æœ‰100äº¿å‚æ•°çš„æ¨¡åž‹ã€‚\n\næ¢å¥è¯è¯´ï¼Œè®­ç»ƒè¿‡ç¨‹åˆ†å¸ƒåœ¨å¤šä¸ªGPUä¸Šï¼Œ**è¿™äº›GPUç”±ç‹¬ç«‹æ–¹æ‹¥æœ‰**ï¼Œå¯èƒ½åˆ†å¸ƒåœ¨ä¸åŒçš„å¤§é™†ï¼Œå¹¶é€šè¿‡ä½Žå¸¦å®½ç½‘ç»œè¿žæŽ¥ã€‚\n\n> æ‚¨å¯ä»¥é€šè¿‡[è¿™ä¸ªåº”ç”¨ç¨‹åº](https://app.primeintellect.ai/intelligence?utm_source=thetechoasis.beehiiv.com&utm_medium=newsletter&utm_campaign=should-ai-s-kill-openai-s-swarm-the-future-of-ai-training&_bhlid=8eadb6cf7d24b545a761f9ac3f7126a45ac2b579)è§‚çœ‹è¿›å±•å’Œå‚ä¸Žçš„ä¸åŒæ–¹ã€‚\n\nè¿™ä½¿æˆ‘ä»¬å¯¹Nearå¦‚ä½•å®žçŽ°è®­ç»ƒæœ‰å²ä»¥æ¥æœ€å¤§çš„å¼€æºAIæ¨¡åž‹çš„ä½¿å‘½æœ‰äº†å¾ˆå¥½çš„æ´žå¯Ÿã€‚\n\næ­£å¦‚æ‚¨ä»Žå‰ä¸€éƒ¨åˆ†ä¸­çŒœæµ‹çš„é‚£æ ·ï¼ŒAIè®­ç»ƒçš„ä¸»è¦ç“¶é¢ˆæ˜¯åŒæ­¥æ›´æ–°ã€‚æ ¹æ®é˜¿å§†è¾¾å°”å®šå¾‹ï¼Œ**å¦‚æžœè®­ç»ƒä¸­çš„æŸä¸ªçŽ¯èŠ‚æ— æ³•å¹¶è¡ŒåŒ–ï¼Œåˆ™å¹¶è¡ŒåŒ–å¯èƒ½ä¼šå¯¼è‡´æ”¶ç›Šé€’å‡**ã€‚\n\nå› æ­¤ï¼Œéšç€å¹¶è¡ŒåŒ–çš„å¢žåŠ ï¼ŒèŠ‚çœæ—¶é—´çš„æ”¹è¿›å˜å¾—å¢žé‡åŒ–ï¼Œå› ä¸ºæˆ‘ä»¬æ— æ³•å‡å°‘åŒæ­¥æ—¶é—´ã€‚\n\n> å¦‚æžœæ‚¨æƒ³çŸ¥é“ï¼Œæ— æ³•å¼‚æ­¥æ‰§è¡ŒåŒæ­¥ï¼ˆæ¯ä¸ªpodç‹¬ç«‹æ›´æ–°å…¶å‚æ•°å€¼ï¼‰ï¼Œå› ä¸ºæ¨¡åž‹æ”¶æ•›å˜å¾—ä¸å¯èƒ½ï¼ˆè‡³å°‘åœ¨æˆ‘ä»¬ç›®å‰çš„çŸ¥è¯†ä¸­ï¼‰ã€‚\n\nçŸ¥é“è¿™ä¸€ç‚¹åŽï¼ŒPrime Intellectå®žæ–½äº†å‡ ç§Nearè‚¯å®šä¼šåˆ©ç”¨çš„æŠ€æœ¯ï¼š\n\n* **æ¯ç™¾æ­¥åŒæ­¥ä¸€æ¬¡**ã€‚\n\næ¯æ¬¡å‚æ•°æ›´æ–°æ—¶ï¼Œä¸æ˜¯æ¯ä¸ªGPU podéƒ½è¿›è¡ŒåŒæ­¥ï¼Œè€Œæ˜¯æ¯ä¸ªpodæºå¸¦å…¶â€œä¼ªæ¢¯åº¦â€ï¼ˆåœ¨å¤šä¸ªæœ¬åœ°è®­ç»ƒæ—¶é—´æ­¥éª¤ä¸­ç§¯ç´¯å…¶å­¦ä¹ ï¼‰ï¼Œæ¯100ä¸ªè¿™æ ·çš„æ—¶é—´æ­¥éª¤ï¼Œå®ƒä¸Žå…¶ä»–podåˆ†äº«å…¶å­¦ä¹ ã€‚\n\nç®€å•æ¥è¯´ï¼Œç”±äºŽå­¦ä¹ å…±äº«æ˜¯è®­ç»ƒæ€§èƒ½çš„ä¸»è¦ç“¶é¢ˆï¼Œæˆ‘ä»¬æœ€å°åŒ–GPU podä¹‹é—´çš„é€šä¿¡æ¬¡æ•°ã€‚\n\n* **é€šä¿¡è´Ÿè½½çš„é‡åŒ–**ã€‚\n\nè·¨podé€šä¿¡çš„æ¬¡æ•°å¹¶ä¸æ˜¯å”¯ä¸€å½±å“æ—¶é—´çš„å› ç´ ï¼›å…±äº«ä¿¡æ¯çš„æ•°é‡ä¹Ÿå¾ˆé‡è¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯¹å­¦ä¹ è¿›è¡Œé‡åŒ–ï¼Œä»¥ä¾¿ä¿¡æ¯ä»¥åŽ‹ç¼©å½¢å¼ä¼ é€’ï¼Œä»Žè€ŒåŠ å¿«é€Ÿåº¦ã€‚\n\nè¿™å°†é€šä¿¡éœ€æ±‚å‡å°‘äº†400å€ã€‚åœ¨æ ‡å‡†æƒ…å†µä¸‹ï¼ŒåŒæ­¥å¯èƒ½éœ€è¦é•¿è¾¾40åˆ†é’Ÿã€‚é€šè¿‡è¿™ç§é‡åŒ–ï¼Œæ‰€éœ€æ—¶é—´å°‘äºŽä¸€åˆ†é’Ÿã€‚\n\n> **ä»€ä¹ˆæ˜¯é‡åŒ–ï¼Ÿ** ç®€è€Œè¨€ä¹‹ï¼Œæˆ‘ä»¬å¯¹æƒ³è¦å­˜å‚¨ï¼ˆæˆ–å…±äº«ï¼Œå¦‚æœ¬ä¾‹æ‰€ç¤ºï¼‰çš„ä¿¡æ¯è¿›è¡Œå¤„ç†ï¼Œé™ä½Žæ¯ä¸ªå‚æ•°çš„ç²¾åº¦ï¼ˆè€Œä¸æ˜¯â€˜1.023293â€™ï¼Œè¯¥æ•°å­—ä»¥â€˜1â€™çš„å½¢å¼ä¼ é€’ï¼‰ä»¥ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆæºå¸¦æ¯ä¸ªæ¨¡åž‹å‰¯æœ¬å­¦ä¹ å†…å®¹çš„çŠ¶æ€ï¼‰ã€‚\n\n> **å¯ä»¥å°†å…¶è§†ä¸ºåœ¨å‘é€ä¹‹å‰å°†æ•°æ®åŽ‹ç¼©æˆzipæ–‡ä»¶ï¼Œä»¥ä¾¿å‘é€çš„æ•°æ®åŒ…å¤§å°æ›´å°ï¼Œä»Žè€Œæ›´å¿«å‘é€ã€‚**\n\n> ç„¶è€Œï¼Œè™½ç„¶å¯ä»¥æ¢å¤åŽŸå§‹æ•°å­—ï¼ˆåé‡åŒ–ï¼‰ï¼Œä½†ä¼šé€ æˆä¸€äº›ç²¾åº¦æŸå¤±ï¼Œè¿™å¯èƒ½ä¼šå½±å“æ€§èƒ½ã€‚ç„¶è€Œï¼Œ[Prime Intellectå£°ç§°ä»–ä»¬æ²¡æœ‰æ„Ÿå—åˆ°ä»»ä½•æ€§èƒ½æŸå¤±](https://www.primeintellect.ai/blog/intellect-1)ï¼Œå°½ç®¡èŠ‚çœäº†å¤§é‡æ—¶é—´ã€‚\n\n* **åŠ¨æ€å…¨çƒç»„**\n\nåŽ»ä¸­å¿ƒåŒ–æ¨¡åž‹è®­ç»ƒçš„æœ€å¤§é—®é¢˜ä¹‹ä¸€æ˜¯å¯é æ€§ï¼›ç½‘ç»œå’Œï¼Œæœ€é‡è¦çš„æ˜¯ï¼Œå·¥ä½œè€…ï¼ˆGPUï¼‰å¯èƒ½ä¼šå´©æºƒå’Œå¤±è´¥ã€‚æ­¤å¤–ï¼Œ**æ‚¨å¸Œæœ›æ¿€åŠ±è¿™ç§åŠ¨æ€æ€§**ï¼Œä»¥ä¾¿äººä»¬å¯ä»¥å…±åŒå‚ä¸Žè®­ç»ƒå¹¶åœ¨éœ€è¦æ—¶ä¸‹çº¿ã€‚\n\nä¸ºæ­¤ï¼ŒPrimeæ¡†æž¶å…·æœ‰**åŠ¨æ€å…¨çƒç»„**ï¼Œç¡®ä¿å·¥ä½œè€…å¯ä»¥åœ¨ä¸å½±å“æ•´ä½“è®­ç»ƒè¿‡ç¨‹çš„æƒ…å†µä¸‹ä¸Šçº¿å’Œä¸‹çº¿ã€‚\n\næ­¤å¤–ï¼Œæ¡†æž¶è¿˜åŒ…æ‹¬å…¶ä»–æŠ€æœ¯ï¼Œå¦‚å¼‚æ­¥æ£€æŸ¥ç‚¹ï¼Œæˆ‘ä¸ä¼šè¯¦ç»†ä»‹ç»ä»¥èŠ‚çœç¯‡å¹…ï¼Œä½†æ‚¨å¯ä»¥åœ¨[è¿™é‡Œè¯¦ç»†é˜…è¯»](https://www.primeintellect.ai/blog/intellect-1)ã€‚\n\nä½†æˆ‘ä»¬ä»ç„¶æ²¡æœ‰å›žç­”å…³é”®é—®é¢˜ï¼š*åŒºå—é“¾åœ¨è¿™ä¸€åˆ‡ä¸­é€‚åˆä»€ä¹ˆï¼Ÿ*\n\n## æ¿€åŠ¨äººå¿ƒçš„æœªæ¥\n\nåœ¨æŽ¥ä¸‹æ¥çš„å››å¹´é‡Œï¼Œæ‚¨å°†çœ‹åˆ°åŒºå—é“¾æ— å¤„ä¸åœ¨ã€‚\n\næ˜¯çš„ï¼Œ*â€˜{æ’å…¥æŸä¸ªè¿ä½œè‰¯å¥½çš„ä¸œè¥¿}ä½†çŽ°åœ¨å®ƒæ˜¯åŽ»ä¸­å¿ƒåŒ–çš„â€™* è¿™ç§å£å·å°†é‡æ–°å›žåˆ°æˆ‘ä»¬çš„ç”Ÿæ´»ä¸­ã€‚\n\nè™½ç„¶è®¸å¤šæ–°çš„ç”¨ä¾‹å¯èƒ½æ¯«æ— æ„ä¹‰ï¼Œä½†åŒºå—é“¾ç¡®å®žæœ‰ä¸€ä¸ªæ˜Žç¡®çš„å­˜åœ¨ç†ç”±ï¼Œä½¿å…¶åœ¨å¿…è¦æ—¶ä½¿ç”¨æ—¶éžå¸¸æœ‰ä»·å€¼ï¼Œè€Œä¸æ˜¯ä¸ºäº†è¯´æ‚¨åœ¨ä½¿ç”¨åŒºå—é“¾ã€‚\n\n### è¿™æ˜¯ä¸€ä»½è´¦æœ¬\n\nåŒºå—é“¾æ˜¯åŽ»ä¸­å¿ƒåŒ–çš„è´¦æœ¬ã€‚å®ƒä»¬ä»¥åŒºå—çš„å½¢å¼å­˜å‚¨ä¸¤ä¸ªèŠ‚ç‚¹ä¹‹é—´çš„äº¤æ˜“ä¿¡æ¯ï¼Œè¿™äº›åŒºå—æŒ‰é¡ºåºè¿žæŽ¥åœ¨ä¸€èµ·ï¼ˆå› æ­¤å¾—åï¼‰ã€‚\n\n**è¿™éžå¸¸é‡è¦ï¼Œå› ä¸ºå®ƒä»¬çš„åŽ»ä¸­å¿ƒåŒ–ç‰¹æ€§ä½¿å¾—è¿™ä¸ªè´¦æœ¬å‡ ä¹Žä¸å¯èƒ½è¢«ç¯¡æ”¹**ã€‚çœŸæ­£çš„åŒºå—é“¾ï¼ˆä»Šå¤©ç¬¦åˆè¿™ä¸€æ ‡å‡†çš„å¹¶ä¸å¤šï¼‰æ˜¯ä¸å¯å˜çš„å’Œæ˜Žç¡®çš„ï¼Œæ˜¯æŸä¸ªæ—¶åˆ»äº¤æ˜“å‘ç”Ÿçš„æ— å¯äº‰è®®çš„äº‹å®žæ¥æºã€‚\n\né‡è¦çš„æ˜¯ï¼Œå®ƒä»¬æ˜¯â€œæ— ä¿¡ä»»â€çš„ï¼Œè¿™æ„å‘³ç€åŠ å¯†æŠ€æœ¯ï¼Œè€Œä¸æ˜¯åƒé“¶è¡Œè¿™æ ·çš„ä¸­å¿ƒåŒ–å®žä½“ï¼Œä¿è¯äº†è´¦æœ¬çš„æœªè¢«ç¯¡æ”¹çš„ç‰¹æ€§ã€‚\n\n> å®ƒä»¬ä¹‹æ‰€ä»¥å¦‚æ­¤éš¾ä»¥ç¯¡æ”¹ï¼Œæ‚¨çŒœå¯¹äº†ï¼Œæ˜¯å› ä¸ºå®ƒä»¬çš„åŽ»ä¸­å¿ƒåŒ–ç‰¹æ€§ã€‚ä¿æŠ¤åŒºå—é“¾çš„å…¨çƒèŠ‚ç‚¹ç½‘ç»œéƒ½æœ‰ç½‘ç»œçš„ç²¾ç¡®å‰¯æœ¬ï¼Œæ¯æ¬¡æ·»åŠ æ–°å—æ—¶éƒ½ä¼šæ›´æ–°ã€‚\n\n> å› æ­¤ï¼Œè¦å¼•å…¥è¢«ç¯¡æ”¹çš„äº¤æ˜“ï¼Œæ‚¨éœ€è¦æ‹¥æœ‰è¿™äº›èŠ‚ç‚¹çš„å¤šæ•°ï¼Œæ— è®ºæ˜¯é€šè¿‡åœ¨åƒæ¯”ç‰¹å¸è¿™æ ·çš„å·¥ä½œé‡è¯æ˜ŽåŒºå—é“¾ä¸­æŠ•å…¥å·¨é¢è®¡ç®—èµ„æºï¼ˆæˆæœ¬æžé«˜ï¼‰ã€é»‘å®¢æ”»å‡»å¤§å¤šæ•°èŠ‚ç‚¹ï¼ˆåŒæ ·ï¼Œæˆæœ¬æžé«˜ï¼‰ï¼Œè¿˜æ˜¯é€šè¿‡åœ¨åƒä»¥å¤ªåŠè¿™æ ·çš„æƒç›Šè¯æ˜ŽåŒºå—é“¾ä¸­æ‹¥æœ‰å¤šæ•°çš„åŠ å¯†è´§å¸è‚¡ä»½ï¼ˆåŒæ ·ï¼Œæˆæœ¬æžé«˜ï¼‰ã€‚\n\né•¿è¯çŸ­è¯´ï¼ŒåŒºå—é“¾çš„ä»·å€¼åœ¨äºŽï¼Œä½¿ç¯¡æ”¹å®ƒä»¬çš„è¡Œä¸ºåœ¨ç»æµŽä¸Šæˆä¸ºä¸€ä¸ªéžå¸¸éžå¸¸ç³Ÿç³•çš„ä¸»æ„ï¼Œè¿™æ ·åšæ ¹æœ¬ä¸å€¼å¾—ã€‚\n\nå› æ­¤ï¼Œå®ƒä»¬çš„ä»·å€¼åœ¨äºŽï¼Œä¸ä»…æ˜¯ä¼Ÿå¤§çš„çœŸç›¸æ¥æºï¼Œä¸ºäº¤æ˜“æä¾›ä¿¡ä»»ï¼Œè€Œä¸”è¿˜å…äºŽå¯èƒ½æœ‰åŠ¨æœºåŽ»ç¯¡æ”¹å®ƒä»¬çš„ä¸­å¿ƒåŒ–æƒåŠ›ã€‚\n\n*è¿™ä¸Žäººå·¥æ™ºèƒ½æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ* è¿™å°±æ˜¯ä¸€åˆ‡å›žå½’çš„åœ°æ–¹ã€‚\n\n### æ‹¥æœ‰çš„äººå·¥æ™ºèƒ½éœ€è¦åŒºå—é“¾\n\nè®­ç»ƒåŽ»ä¸­å¿ƒåŒ–äººå·¥æ™ºèƒ½æ¨¡åž‹çš„æƒ³æ³•æ˜¯ï¼Œå‚ä¸Žè®­ç»ƒè¯¥æ¨¡åž‹çš„äººï¼ˆæ— è®ºæ˜¯é€šè¿‡è®¡ç®—è¿˜æ˜¯èµ„é‡‘ï¼‰éƒ½å°†èŽ·å¾—å¥–åŠ±ã€‚\n\nå› æ­¤ï¼Œè¿™ä¸ª1.4ä¸‡äº¿å‚æ•°æ¨¡åž‹çš„ç›®æ ‡æ˜¯å°†å…¶æŽ¨æ–­ï¼ˆä½¿ç”¨ï¼‰å›žæŠ¥ç»™å…¶èµ„åŠ©è€…ã€‚\n\nè€ŒåŒºå—é“¾åœ¨è¿™é‡Œå‘æŒ¥äº†ä½œç”¨ï¼Œä½œä¸ºä¸å¯å¦è®¤çš„è¯æ®ï¼Œè¯æ˜Ž*â€œæ¥è‡ªå†…å¸ƒæ‹‰æ–¯åŠ å·žçš„ç®€Â·å¤šâ€*æ”¯ä»˜äº†1,000ç¾Žå…ƒæ¥èµ„åŠ©è¿™é¡¹è®­ç»ƒï¼Œæˆ–è€…*â€œæ¥è‡ªæ—¥æœ¬çš„çº¦ç¿°Â·å¤šâ€*è¯æ˜Žä»–ä»¬æä¾›äº†100å°æ—¶çš„GPUè®¡ç®—ç”¨äºŽè®­ç»ƒï¼Œå› æ­¤ï¼Œä¸¤è€…éƒ½æ˜¯è¯¥æ¨¡åž‹æŽ¨æ–­æ”¶ç›Šçš„åˆæ³•æŽ¥æ”¶è€…ï¼ˆæ¯æ¬¡æ¨¡åž‹è¿è¡Œæ—¶ï¼Œæ‚¨éƒ½ä¼šèŽ·å¾—æŠ¥é…¬ï¼‰ã€‚\n\nçŽ°åœ¨ï¼Œæ‚¨å¯èƒ½ä¼šé—®ï¼šä¸€ä¸ªä¸­å¿ƒåŒ–å®žä½“èƒ½ç®¡ç†è¿™ä¸€åˆ‡å—ï¼Ÿ\n\nå½“ç„¶å¯ä»¥ï¼Œä½†åŒºå—é“¾çš„æ ¸å¿ƒç›®çš„å°±æ˜¯é˜²æ­¢éœ€è¦è¿™æ ·çš„ä¸­å¿ƒå®žä½“çš„å­˜åœ¨ï¼Œå¹¶ç¡®ä¿æ²¡æœ‰äººå®Œå…¨æŽ§åˆ¶è°æ‹¥æœ‰ä»€ä¹ˆæˆ–æ‚¨èŽ·å¾—å¤šå°‘æŠ¥é…¬ã€‚\n\nçŽ°åœ¨ï¼Œè€ƒè™‘åˆ°æ‰€æœ‰å› ç´ ï¼Œ*è¿™ä¸ªæ„¿æ™¯ä»Šå¤©çœŸçš„å¯èƒ½å®žçŽ°å—ï¼Ÿ*\n\n### å¯è¡Œæ€§æ˜¯å¦ç¬¦åˆæ„¿æ™¯ï¼Ÿ\n\nä»»ä½•äººéƒ½å¾ˆå®¹æ˜“ä¸Ž Near çš„ AI æ„¿æ™¯ä¿æŒä¸€è‡´ï¼Œå°¤å…¶æ˜¯è€ƒè™‘åˆ°è¿™ä¸ªé¡¹ç›®èƒŒåŽçš„äººã€‚\n\nè®¾æƒ³ä¸€ä¸ªåŽ»ä¸­å¿ƒåŒ–ç»æµŽå›´ç»• AI å…´èµ·çš„æœªæ¥ï¼Œä»¥ç¡®ä¿äººä»¬å› å…¶æ•°æ®ã€å†…å®¹ã€è®¡ç®—æˆ–ä¸“ä¸šçŸ¥è¯†è€ŒèŽ·å¾—æŠ¥é…¬ï¼Œå¹¶å¯¹æ­¤èŽ·å¾—æ˜Žç¡®å’Œå®¢è§‚çš„å¥–åŠ±ï¼Œè¿™æ˜¯ä»»ä½•äººéƒ½èƒ½å…±é¸£çš„æ„¿æ™¯ã€‚\n\nç„¶è€Œï¼Œ**åŸºäºŽå½“å‰æ ‡å‡†ï¼Œ1.4ä¸‡äº¿å‚æ•°çš„æ¨¡åž‹æ˜¾å¾—è¿‡äºŽåºžå¤§**ã€‚æ­£å¦‚æ‰€æåˆ°çš„ï¼Œ*Intellect\\-1*ï¼Œç›®å‰å·²çŸ¥çš„æœ€å¤§è®­ç»ƒæ¨¡åž‹ï¼Œ**ä»…ä¸º *Near* æ‰“ç®—æž„å»ºçš„æ¨¡åž‹çš„ 140 åˆ†ä¹‹ä¸€**ã€‚\n\nå¦ä¸€ä¸ªæ‹…å¿§æ˜¯åŒºå—é“¾ã€‚ä¾‹å¦‚ï¼Œæœ‰å…³ NFT çš„æœ€å¤§è°Žè¨€ä¹‹ä¸€æ˜¯åŒºå—é“¾ä»…å­˜å‚¨ NFT äº¤æ˜“å‘ç”Ÿçš„äº‹å®žï¼Œ**ä½† NFT æ˜¯â€œé“¾å¤–â€å­˜å‚¨çš„ã€‚**å¯æ‚²çš„æ˜¯ï¼ŒçœŸç›¸æ˜¯ï¼Œåªæœ‰å­˜å‚¨åœ¨åŒºå—é“¾ä¸­çš„æ•°æ®æ˜¯å®Œå…¨å—ä¿æŠ¤çš„ï¼Œå› æ­¤å®žé™…çš„â€œè‰ºæœ¯å“â€åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯æ²¡æœ‰ä¿æŠ¤çš„ï¼Œä¸”æ˜“äºŽå¤åˆ¶ã€‚\n\nç„¶è€Œï¼ŒåŒºå—é“¾ notoriously ä½Žæ•ˆï¼Œè¿™æ„å‘³ç€ä½ åœ¨â€œé“¾ä¸Šâ€å­˜å‚¨çš„æ•°æ®è¶Šå°‘è¶Šå¥½ï¼Œè¿™ä½¿å¾—å®ƒä»¬éžå¸¸ä¸å®žç”¨ã€‚\n\nå› æ­¤ï¼Œå¦‚æžœæ¨¡åž‹ã€æ•°æ®æˆ–ç”¨äºŽè®­ç»ƒæ¨¡åž‹çš„è®¡ç®—éƒ½ä¸ä¼šå­˜å‚¨åœ¨åŒºå—é“¾ä¸Šï¼Œ*é‚£æœ‰ä»€ä¹ˆæ„ä¹‰å‘¢ï¼Ÿ*\n\nå¹¸è¿çš„æ˜¯ï¼Œæœ‰ä¸€ä¸ªè§£å†³æ–¹æ¡ˆï¼šé›¶çŸ¥è¯†è¯æ˜Žï¼Œä»Šå¤©ä¸ºäº†ç¯‡å¹…åŽŸå› æˆ‘ä¸æ‰“ç®—æ·±å…¥æŽ¢è®¨ï¼Œè¿™å¯èƒ½æ˜¯ç¡®ä¿äº‹ä»¶å‘ç”Ÿçš„å…³é”®ï¼Œå³ä½¿å®ƒæ²¡æœ‰å­˜å‚¨åœ¨é“¾ä¸Šã€‚\n\né€šè¿‡ zk\\-proofsï¼ŒæŸäººå¯ä»¥è¯æ˜Žä»–ä»¬å£°ç§°ç”¨äºŽè®­ç»ƒçš„è®¡ç®—ç¡®å®žå‘ç”Ÿäº†ï¼Œæˆ–è€…ä»–ä»¬ç¡®å®žèµ„åŠ©äº†è®­ç»ƒè¿‡ç¨‹ï¼Œé€šè¿‡å­˜å‚¨è¯¥äº¤æ˜“çš„æ³¨å†Œä¿¡æ¯ï¼Œå¹¶é™„ä¸Šä¸€ä¸ª zk\\-proofï¼Œè¯æ˜ŽæŸä¸ªé“¾å¤–äº‹ä»¶ç¡®å®žå‘ç”Ÿã€‚\n\nå› æ­¤ï¼Œä»…é€šè¿‡å­˜å‚¨ zk\\-proofï¼Œæˆ‘ä»¬å¯ä»¥ç¡®ä¿å³ä½¿æ˜¯é“¾å¤–æ•°æ®ä¹Ÿå¯ä»¥è¢«ä¿¡ä»»ã€‚*é—®é¢˜æ˜¯ï¼Ÿ* ç”±äºŽ zk\\-proof å¯¹è®¡ç®—çš„è¦æ±‚å¾ˆé«˜ï¼Œå®ƒä»¬å°šæœªå‡†å¤‡å¥½ã€‚\n\nç„¶è€Œï¼Œæœ‰ä¸€ç‚¹ä»ç„¶æˆç«‹ï¼šå¦‚æžœä½ çœŸçš„ç›¸ä¿¡ AI å¯ä»¥åŽ»ä¸­å¿ƒåŒ–ï¼Œä½ å¿…é¡»ç›¸ä¿¡åŒºå—é“¾æ˜¯åˆæ³•çš„ã€‚\n\n*ä½†è¿™ç§ç±»åž‹çš„å…¬å‘Šè®©ä½ æ„Ÿè§‰å¦‚ä½•ï¼Ÿä½ å¯¹ Crypto å’Œ AI ä¹‹é—´çš„ååŒä½œç”¨æ„Ÿåˆ°å…´å¥‹å—ï¼Ÿ*\n\n*è¿˜æ˜¯æ¯æ¬¡çœ‹åˆ°åŒºå—é“¾æåŠæ—¶ï¼Œä»ç„¶è§‰å¾—æ˜¯ä¸€ç§â€œéª—å±€â€ï¼Ÿ* å¦‚æžœæ˜¯è¿™æ ·ï¼Œæˆ‘ä¸æ€ªä½ ï¼Œä½†å¦‚æžœä½ èƒ½å¤ŸæŠ½ç¦»å‡º Crypto çš„æ— æ•°éª—å±€ï¼Œä½ ä¼šæ„è¯†åˆ°è¿™é¡¹æŠ€æœ¯å°†åœ¨ AI ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚\n\nå¦‚æžœ Near æ˜¯å¯¹çš„ï¼Œé‚£å°†æ¯”é¢„æœŸçš„æ›´æ—©åˆ°æ¥ã€‚\n\n\n> **æœ‰å…³ AI ç­–ç•¥æˆ–åˆ†æžçš„å•†ä¸šå’¨è¯¢ï¼Œè¯·è”ç³» nacho@thewhitebox.ai**\n\n\n> å¦‚æžœä½ å–œæ¬¢è¿™ç¯‡æ–‡ç« ï¼Œæˆ‘ä¼šåœ¨æˆ‘çš„ [LinkedIn](https://www.linkedin.com/in/ignacio-de-gregorio-noblejas/) ä¸Šä»¥æ›´å…¨é¢å’Œç®€åŒ–çš„æ–¹å¼å…è´¹åˆ†äº«ç±»ä¼¼çš„æƒ³æ³•ã€‚\n\n"},{"lang":"zh","group":"blog","slug":"blog/the-quest-for-production-quality-graph-rag-easy-to-start-hard-to-finish-46ca404cee3d","frontmatter":{"title":"è¿½æ±‚ç”Ÿäº§è´¨é‡ Graph RAGï¼šå¼€å§‹å®¹æ˜“ï¼Œå®Œæˆéš¾","meta_title":"è¿½æ±‚ç”Ÿäº§è´¨é‡ Graph RAGï¼šå¼€å§‹å®¹æ˜“ï¼Œå®Œæˆéš¾","description":"å…‹æœ RAG å›¾ç”Ÿäº§åŒ–æŒ‘æˆ˜","date":"2024-11-01T03:56:04.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*RMudHNmBOgXM1Mubj1UTkw.jpeg","categories":["Programming","Data Science","Generative AI"],"author":"Rifx.Online","tags":["graph","RAG","production","uncertainty","optimization"],"draft":false,"slug":"blog/the-quest-for-production-quality-graph-rag-easy-to-start-hard-to-finish-46ca404cee3d"},"content":"\n\n\n### å…‹æœå›¾å½¢ RAG ç”Ÿäº§åŒ–çš„æŒ‘æˆ˜\n\n\n\nå½“æˆ‘é˜…è¯»æœ€è¿‘åœ¨ VentureBeat ä¸Šå…³äºŽ Glean [åˆšåˆšåœ¨æœ€æ–°èžèµ„è½®ä¸­èŽ·å¾—è¶…è¿‡ 2.6 äº¿ç¾Žå…ƒçš„æ–‡ç« ](https://venturebeat.com/data-infrastructure/how-to-take-advantage-of-a-generative-tool-fueling-gleans-260m-raise-graph-rag/)æ—¶ï¼Œæˆ‘æœ‰ä¸¤ä¸ªç›´æŽ¥çš„ç›´è§‰ã€‚é¦–å…ˆï¼Œçœ‹åˆ°è¿™ä¸ªéžå¸¸å…¬å¼€çš„å›¾å½¢ RAG ç¤ºä¾‹å……åˆ†å‘æŒ¥å…¶ä½œä¸ºä¸€ç§å¼ºå¤§ã€æœ‰ä»·å€¼çš„æŠ€æœ¯çš„æ½œåŠ›ï¼Œèƒ½å¤Ÿæ¯”ä»¥å¾€ä»»ä½•æ—¶å€™éƒ½æ›´é«˜æ•ˆåœ°å°†äººä»¬ä¸ŽçŸ¥è¯†è¿žæŽ¥èµ·æ¥ï¼Œè¿™è®©æˆ‘æ„Ÿåˆ°æ»¡æ„ã€‚å…¶æ¬¡ï¼Œè¯»åˆ°ä»¥ä¸‹å†…å®¹è®©æˆ‘æ„Ÿåˆ°æƒŠè®¶ä½†åˆé¢‡å…·éªŒè¯æ€§ï¼š\n\n> ä¸–ç•Œä¸Šæœ€å¤§çš„å…±äº«å‡ºè¡Œå…¬å¸ä¹‹ä¸€äº²èº«ä½“éªŒäº†å…¶å¸¦æ¥çš„å¥½å¤„ã€‚åœ¨ä¸“é—¨æŠ•å…¥æ•´ä¸ªå·¥ç¨‹å›¢é˜Ÿå¼€å‘ç±»ä¼¼çš„å†…éƒ¨è§£å†³æ–¹æ¡ˆåŽï¼Œä»–ä»¬æœ€ç»ˆå†³å®šè½¬å‘ Glean çš„å¹³å°ã€‚\n\n> â€œåœ¨ä¸€ä¸ªæœˆå†…ï¼Œä»–ä»¬åœ¨ Glean å¹³å°ä¸Šçš„ä½¿ç”¨é‡ç¿»äº†ä¸€ç•ªï¼Œå› ä¸ºç»“æžœæ˜¯æ˜¾è€Œæ˜“è§çš„ï¼Œâ€Glean çš„é¦–å¸­è¥é”€å®˜ Matt Kixmoeller è¯´ã€‚\n\nè™½ç„¶æˆ‘å¯¹æ–°é—»æ–‡ç« ä¸­æåˆ°çš„å¤±è´¥æ„Ÿåˆ°æƒŠè®¶ï¼Œä½†æ ¹æ®æˆ‘è‡ªå·±çš„ç»éªŒä»¥åŠåŒäº‹å’Œå®¢æˆ·çš„ç»åŽ†ï¼ŒåŠªåŠ›å°†å›¾å½¢ RAG æŽ¨å‘ç”Ÿäº§æ˜¯æˆ‘æ‰€é¢„æ–™çš„ã€‚æˆ‘å¹¶ä¸æ˜¯è¯´æˆ‘æœŸæœ›å¤§åž‹ç§‘æŠ€å…¬å¸åœ¨æž„å»ºè‡ªå·±çš„å›¾å½¢ RAG ç³»ç»Ÿæ—¶ä¼šå¤±è´¥ã€‚**æˆ‘åªæ˜¯æœŸæœ›å¤§å¤šæ•°äººä¼šåœ¨æž„å»ºå’Œç”Ÿäº§åŒ–å›¾å½¢ RAG æ—¶é‡åˆ°å›°éš¾â€”â€”å³ä½¿ä»–ä»¬å·²ç»æœ‰ä¸€ä¸ªéžå¸¸æˆåŠŸçš„æ¦‚å¿µéªŒè¯ã€‚**\n\næˆ‘åœ¨ [The New Stack ä¸Šå¯¹ VentureBeat æ–‡ç« åšäº†ä¸€ä¸ªé«˜å±‚æ¬¡çš„ååº”](https://bit.ly/4fjIlgJ)ï¼Œåœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘æƒ³æ·±å…¥æŽ¢è®¨ä¸ºä»€ä¹ˆå›¾å½¢ RAG å¯èƒ½å¦‚æ­¤éš¾ä»¥åšåˆ°æ­£ç¡®ã€‚é¦–å…ˆï¼Œæˆ‘å°†æŒ‡å‡ºï¼Œåˆ©ç”¨æœ€æ–°å·¥å…·ï¼Œå¼€å§‹ä½¿ç”¨å›¾å½¢ RAG å˜å¾—å¤šä¹ˆç®€å•ã€‚ç„¶åŽï¼Œæˆ‘å°†æ·±å…¥æŽ¢è®¨ä¸€äº›ç‰¹å®šçš„å›¾å½¢ RAG æŒ‘æˆ˜ï¼Œè¿™äº›æŒ‘æˆ˜ä½¿å…¶ä»Žç ”å‘è½¬å‘ç”Ÿäº§å˜å¾—å¦‚æ­¤å›°éš¾ã€‚æœ€åŽï¼Œæˆ‘å°†åˆ†äº«ä¸€äº›å…³äºŽå¦‚ä½•æœ€å¤§åŒ–æˆåŠŸæœºä¼šçš„å›¾å½¢ RAG çš„å»ºè®®ã€‚\n\n## å¼€å§‹ä½¿ç”¨å›¾å½¢ RAG å¾ˆç®€å•\n\nå¦‚æžœä¸€å®¶å¤§åž‹å…±äº«å‡ºè¡Œå…¬å¸æ— æ³•æœ‰æ•ˆæž„å»ºè‡ªå·±çš„å¹³å°ï¼Œé‚£ä¹ˆæˆ‘ä¸ºä»€ä¹ˆä¼šè¯´è‡ªå·±å®žçŽ°å›¾å½¢ RAG å¾ˆç®€å•å‘¢ï¼Ÿ\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*l6EiwfjeUGLjVlYeiY1lqA.jpeg)\n\né¦–å…ˆï¼Œæ”¯æŒ RAG å’Œå›¾å½¢ RAG çš„æŠ€æœ¯åœ¨è¿‡åŽ»ä¸€å¹´ä¸­å–å¾—äº†é•¿è¶³çš„è¿›æ­¥ã€‚åäºŒä¸ªæœˆå‰ï¼Œå¤§å¤šæ•°ä¼ä¸šç”šè‡³æ²¡æœ‰å¬è¯´è¿‡æ£€ç´¢å¢žå¼ºç”Ÿæˆã€‚çŽ°åœ¨ï¼ŒRAG æ”¯æŒä¸ä»…æ˜¯ [åƒ LangChain è¿™æ ·çš„æœ€ä½³ AI æž„å»ºå·¥å…·çš„å…³é”®ç‰¹æ€§](https://python.langchain.com/docs/tutorials/rag/)ï¼Œè€Œä¸”å‡ ä¹Žæ¯ä¸ªä¸»è¦çš„ AI å‚ä¸Žè€…éƒ½æœ‰ RAG æ•™ç¨‹ï¼Œç”šè‡³è¿˜æœ‰ [Coursera è¯¾ç¨‹](https://www.coursera.org/projects/introduction-to-rag)ã€‚å°è¯• RAG çš„å¿«é€Ÿå…¥é—¨é€”å¾„å±‚å‡ºä¸ç©·ã€‚\n\nå¾®è½¯å¯èƒ½ä¸æ˜¯ç¬¬ä¸€ä¸ªåšå›¾å½¢ RAG çš„å…¬å¸ï¼Œä½†ä»–ä»¬åœ¨ä»Šå¹´æ—©äº›æ—¶å€™å‘å¸ƒçš„ [ç ”ç©¶åšå®¢æ–‡ç« ](https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/) ä¸­å¯¹è¿™ä¸€æ¦‚å¿µè¿›è¡Œäº†å¤§åŠ›æŽ¨åŠ¨ï¼Œå¹¶ç»§ç»­è‡´åŠ›äºŽç›¸å…³æŠ€æœ¯çš„ç ”ç©¶ã€‚\n\nåœ¨ Medium ä¸Šï¼Œè¿˜æœ‰ä¸€ç¯‡æ¥è‡ª [è°·æ­Œçš„ä¸€ä½ç”Ÿæˆ AI å·¥ç¨‹å¸ˆ](https://towardsdatascience.com/graph-rag-a-conceptual-introduction-41cd0d431375) çš„å¾ˆå¥½çš„æ¦‚å¿µä»‹ç»ï¼ŒåŒ…å«äº†ä¸€äº›æŠ€æœ¯ç»†èŠ‚ã€‚æ­¤å¤–ï¼Œåœ¨ Towards Data Science ä¸Šï¼Œè¿˜æœ‰ä¸€ç¯‡æœ€è¿‘çš„ã€éžå¸¸è¯¦å°½çš„ [å…³äºŽæž„å»ºå›¾å½¢ RAG ç³»ç»Ÿçš„æ“ä½œæŒ‡å—](https://towardsdatascience.com/how-to-implement-graph-rag-using-knowledge-graphs-and-vector-databases-60bb69a22759)ï¼Œä»¥åŠåœ¨ç§‘å­¦å‡ºç‰ˆç‰©æ•°æ®é›†ä¸Šè¿›è¡Œæµ‹è¯•çš„å†…å®¹ã€‚\n\nåœ¨ä¼ ç»Ÿå›¾å½¢æ•°æ®åº“å’Œåˆ†æžé¢†åŸŸï¼ŒçŸ¥åå“ç‰Œ Neo4j åœ¨å…¶æ——èˆ°å›¾å½¢æ•°æ®åº“äº§å“ä¸­å¢žåŠ äº†å‘é‡èƒ½åŠ›ï¼Œä»¥å“åº”æœ€è¿‘çš„ç”Ÿæˆ AI é©å‘½ï¼Œå¹¶ä¸ºéœ€è¦å¤æ‚å›¾å½¢åˆ†æžå’Œæ·±åº¦å›¾å½¢ç®—æ³•çš„é¡¹ç›®æä¾›äº†ä¸€ç³»åˆ—ä¼˜ç§€çš„å·¥å…·å¹³å°ï¼Œé™¤äº†æ ‡å‡†çš„å›¾å½¢ RAG åŠŸèƒ½å¤–ã€‚ä»–ä»¬è¿˜æä¾›äº† [å›¾å½¢ RAG å…¥é—¨æŒ‡å—](https://neo4j.com/developer-blog/graphrag-ecosystem-tools/)ã€‚\n\nå¦ä¸€æ–¹é¢ï¼Œ[æ‚¨ç”šè‡³ä¸éœ€è¦å›¾å½¢æ•°æ®åº“å°±å¯ä»¥åšå›¾å½¢ RAG](https://bit.ly/3YD5NAd)ã€‚è®¸å¤šåˆšæŽ¥è§¦å›¾å½¢ RAG çš„äººè®¤ä¸ºä»–ä»¬éœ€è¦éƒ¨ç½²ä¸€ä¸ªä¸“é—¨çš„å›¾å½¢æ•°æ®åº“ï¼Œä½†è¿™å¹¶ä¸æ˜¯å¿…è¦çš„ï¼Œå®žé™…ä¸Šå¯èƒ½ä¼šä½¿æ‚¨çš„æŠ€æœ¯æ ˆå˜å¾—æ›´åŠ å¤æ‚ã€‚\n\næˆ‘çš„é›‡ä¸» DataStax ä¹Ÿæœ‰ [å›¾å½¢ RAG æŒ‡å—](https://bit.ly/4862Lrl)ã€‚\n\nå½“ç„¶ï¼Œä¸¤ä¸ªæœ€å—æ¬¢è¿Žçš„ç”Ÿæˆ AI åº”ç”¨ç¨‹åºç»„åˆæ¡†æž¶ï¼Œ[LangChain](https://blog.langchain.dev/enhancing-rag-based-applications-accuracy-by-constructing-and-leveraging-knowledge-graphs/) å’Œ [LlamaIndex](https://docs.llamaindex.ai/en/stable/examples/cookbooks/GraphRAG_v1/)ï¼Œå„è‡ªéƒ½æœ‰è‡ªå·±çš„å›¾å½¢ RAG ä»‹ç»ã€‚æ­¤å¤–ï¼Œè¿˜æœ‰ä¸€ç¯‡ [DataCamp æ–‡ç« ](https://www.datacamp.com/tutorial/knowledge-graph-rag) åŒæ—¶ä½¿ç”¨äº†è¿™ä¸¤è€…ã€‚\n\næœ‰äº†æ‰€æœ‰å¯ç”¨çš„å·¥å…·å’Œæ•™ç¨‹ï¼Œå¼€å§‹ä½¿ç”¨å›¾å½¢ RAG æ˜¯ç®€å•çš„éƒ¨åˆ†â€¦â€¦\n\n## â€¦ä½†å°†å›¾å½¢ RAG æŠ•å…¥ç”Ÿäº§æ˜¯å›°éš¾çš„\n\nè¿™æ˜¯æ•°æ®ç§‘å­¦ä¸­ä¸€ä¸ªéžå¸¸å¤è€çš„æ•…äº‹ï¼šä¸€ç§æ–°çš„è½¯ä»¶æ–¹æ³•è®ºã€æŠ€æœ¯æˆ–å·¥å…·åœ¨ç ”ç©¶çŽ¯å¢ƒä¸­è§£å†³äº†ä¸€äº›é‡è¦é—®é¢˜ï¼Œä½†è¡Œä¸šåœ¨å°†å…¶æž„å»ºä¸ºæ¯å¤©æä¾›ä»·å€¼çš„äº§å“æ—¶å´é¢ä¸´å›°éš¾ã€‚è¿™ä¸ä»…ä»…æ˜¯è½¯ä»¶å¼€å‘çš„åŠªåŠ›å’Œä¸“ä¸šæ°´å¹³çš„é—®é¢˜â€”â€”å³ä½¿æ˜¯æœ€å¤§çš„ã€æœ€ä¼˜ç§€çš„å›¢é˜Ÿä¹Ÿå¯èƒ½æ— æ³•å…‹æœè§£å†³çŽ°å®žä¸–ç•Œé—®é¢˜æ‰€æ¶‰åŠçš„çŽ°å®žæ•°æ®çš„ä¸ç¡®å®šæ€§ã€ä¸å¯é¢„æµ‹æ€§å’Œä¸å¯æŽ§æ€§ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*OklHNrhsNHZF6qzeRUSd_w.jpeg)\n\nä¸ç¡®å®šæ€§æ˜¯æž„å»ºå’Œä½¿ç”¨æ•°æ®ä¸­å¿ƒç³»ç»Ÿçš„å›ºæœ‰éƒ¨åˆ†ï¼Œè¿™äº›ç³»ç»Ÿå‡ ä¹Žæ€»æ˜¯å…·æœ‰æŸäº›éšæœºæ€§ã€æ¦‚çŽ‡æˆ–æ— ç•Œè¾“å…¥çš„å…ƒç´ ã€‚è€Œä¸”ï¼Œå½“è¾“å…¥å’Œè¾“å‡ºæ˜¯éžç»“æž„åŒ–çš„æ—¶ï¼Œä¸ç¡®å®šæ€§å¯èƒ½ä¼šæ›´å¤§ï¼Œè¿™æ­£æ˜¯ LLM å’Œå…¶ä»– GenAI åº”ç”¨ç¨‹åºçš„è‡ªç„¶è¯­è¨€è¾“å…¥å’Œè¾“å‡ºçš„æƒ…å†µã€‚\n\næƒ³è¦å°è¯•å›¾å½¢ RAG çš„äººé€šå¸¸å·²ç»æ‹¥æœ‰ä¸€ä¸ªçŽ°æœ‰çš„ RAG åº”ç”¨ç¨‹åºï¼Œè¯¥åº”ç”¨ç¨‹åºåœ¨ç®€å•ç”¨ä¾‹ä¸­è¡¨çŽ°è‰¯å¥½ï¼Œä½†åœ¨ä¸€äº›æ›´å¤æ‚çš„ç”¨ä¾‹å’Œéœ€è¦è·¨çŸ¥è¯†åº“å¤šä¸ªä¿¡æ¯ç‰‡æ®µçš„æç¤ºä¸­å¤±è´¥ï¼Œå¯èƒ½æ¶‰åŠä¸åŒçš„æ–‡æ¡£ã€ä¸Šä¸‹æ–‡ã€æ ¼å¼æˆ–ç”šè‡³æ•°æ®å­˜å‚¨ã€‚å½“å›žç­”é—®é¢˜æ‰€éœ€çš„æ‰€æœ‰ä¿¡æ¯éƒ½åœ¨çŸ¥è¯†åº“ä¸­ï¼Œä½† RAG ç³»ç»Ÿæ‰¾ä¸åˆ°æ—¶ï¼Œè¿™ä¼¼ä¹Žæ˜¯ä¸€ä¸ªå¤±è´¥ã€‚ä»Žç”¨æˆ·ä½“éªŒ (UX) çš„è§’åº¦æ¥çœ‹ï¼Œç¡®å®žå¦‚æ­¤â€”â€”æ²¡æœ‰ç»™å‡ºæ­£ç¡®çš„ç­”æ¡ˆã€‚\n\nä½†è¿™å¹¶ä¸ä¸€å®šæ„å‘³ç€ RAG ç³»ç»Ÿå­˜åœ¨â€œé—®é¢˜â€ï¼Œå®ƒå¯èƒ½æ­£å¦‚å…¶è®¾è®¡é‚£æ ·è¿è¡Œã€‚å¦‚æžœæ²¡æœ‰é—®é¢˜æˆ–é”™è¯¯ï¼Œä½†æˆ‘ä»¬ä»ç„¶æ²¡æœ‰å¾—åˆ°æƒ³è¦çš„å“åº”ï¼Œé‚£ä¸€å®šæ„å‘³ç€æˆ‘ä»¬æœŸæœ› RAG ç³»ç»Ÿå…·å¤‡å®ƒæ ¹æœ¬æ²¡æœ‰çš„èƒ½åŠ›ã€‚\n\nåœ¨æˆ‘ä»¬å…·ä½“æŽ¢è®¨ä¸ºä»€ä¹ˆå°†å›¾å½¢ RAG æŠ•å…¥ç”Ÿäº§å¾ˆå›°éš¾ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆçœ‹çœ‹æˆ‘ä»¬è¯•å›¾è§£å†³çš„é—®é¢˜ã€‚\n\n## å›¾å½¢ RAG è§£å†³çš„ä¸»è¦æŒ‘æˆ˜\n\nå› ä¸ºæ™®é€šçš„ RAG ç³»ç»Ÿï¼ˆæ²¡æœ‰çŸ¥è¯†å›¾è°±ï¼‰ä»…åŸºäºŽå‘é‡æœç´¢æ¥æ£€ç´¢æ–‡æ¡£ï¼Œæ‰€ä»¥åªèƒ½æ£€ç´¢ä¸ŽæŸ¥è¯¢åœ¨è¯­ä¹‰ä¸Šæœ€ç›¸ä¼¼çš„æ–‡æ¡£ã€‚é‚£äº›å®Œå…¨ä¸ç›¸ä¼¼æˆ–ç›¸ä¼¼åº¦ä¸å¤Ÿçš„æ–‡æ¡£åˆ™è¢«æŽ’é™¤åœ¨å¤–ï¼Œé€šå¸¸ä¸ä¼šåœ¨æŸ¥è¯¢æ—¶æä¾›ç»™ç”Ÿæˆå“åº”çš„ LLMã€‚\n\nå½“æˆ‘ä»¬éœ€è¦å›žç­”æç¤ºä¸­çš„é—®é¢˜çš„æ–‡æ¡£å¹¶ä¸éƒ½æ˜¯ä¸Žæç¤ºåœ¨è¯­ä¹‰ä¸Šç›¸ä¼¼æ—¶ï¼ŒRAG ç³»ç»Ÿå¾€å¾€ä¼šé—æ¼ä¸€ä¸ªæˆ–å¤šä¸ªæ–‡æ¡£ã€‚è¿™ç§æƒ…å†µå¯èƒ½å‘ç”Ÿåœ¨å›žç­”é—®é¢˜æ—¶éœ€è¦æ··åˆä¸€èˆ¬æ€§å’Œä¸“ä¸šæ€§çš„æ–‡æ¡£æˆ–æœ¯è¯­ï¼Œå¹¶ä¸”å½“æ–‡æ¡£åœ¨ç»†èŠ‚ä¸Šéžå¸¸å¯†é›†æ—¶ï¼ŒæŸäº›å¯¹è¿™ä¸ªç‰¹å®šæç¤ºéžå¸¸é‡è¦çš„ç»†èŠ‚å¯èƒ½ä¼šè¢«åŸ‹æ²¡åœ¨ä¸Žè¯¥æç¤ºä¸å¤ªç›¸å…³çš„ç›¸å…³ç»†èŠ‚ä¸­ã€‚è¯·å‚è§ [è¿™ç¯‡æ–‡ç« ï¼Œäº†è§£ RAG å¦‚ä½•é—æ¼æ–‡æ¡£çš„ä¾‹å­](https://bit.ly/3BKZAJv)ï¼Œå› ä¸ºä¸¤ä¸ªç›¸å…³æ¦‚å¿µï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹æ˜¯â€œå¤ªç©ºé’ˆâ€å’Œâ€œä¸‹çš‡åŽå®‰å¦®ç¤¾åŒºâ€ï¼‰åœ¨è¯­ä¹‰ä¸Šå¹¶ä¸ç›¸ä¼¼ï¼Œå¹¶ä¸” [è¯·å‚è§è¿™ç¯‡æ–‡ç« ï¼Œäº†è§£é‡è¦ç»†èŠ‚å¦‚ä½•è¢«åŸ‹æ²¡çš„ä¾‹å­](https://bit.ly/4ffhrqi)ï¼Œå› ä¸ºå‘é‡åµŒå…¥æ˜¯â€œæœ‰æŸâ€çš„ã€‚\n\nå½“æˆ‘ä»¬çœ‹åˆ°æ£€ç´¢â€œå¤±è´¥â€æœªèƒ½æ‰¾åˆ°æ­£ç¡®çš„æ–‡æ¡£æ—¶ï¼Œå¯èƒ½ä¼šæƒ³è¦å°è¯•æ”¹è¿›å‘é‡æœç´¢æˆ–ä½¿å…¶æ›´è´´åˆæˆ‘ä»¬çš„ç”¨ä¾‹ã€‚ä½†è¿™éœ€è¦è°ƒæ•´åµŒå…¥ï¼Œè€ŒåµŒå…¥æ˜¯å¤æ‚çš„ã€æ··ä¹±çš„ã€è®¡ç®—æˆæœ¬é«˜æ˜‚çš„ï¼Œç”šè‡³å¾®è°ƒçš„æˆæœ¬æ›´é«˜ã€‚æ­¤å¤–ï¼Œè¿™ç”šè‡³ä¸æ˜¯è§£å†³é—®é¢˜çš„æœ€ä½³æ–¹æ³•ã€‚\n\nä¾‹å¦‚ï¼Œçœ‹çœ‹ä¸Šé¢é“¾æŽ¥çš„ä¾‹å­ï¼Œæˆ‘ä»¬çœŸçš„æƒ³ä½¿ç”¨ä¸€ä¸ªå°†â€œå¤ªç©ºé’ˆâ€å’Œâ€œä¸‹çš‡åŽå®‰å¦®ç¤¾åŒºâ€åœ¨è¯­ä¹‰å‘é‡ç©ºé—´ä¸­æ”¾å¾—å¾ˆè¿‘çš„åµŒå…¥ç®—æ³•å—ï¼Ÿä¸ï¼Œå¾®è°ƒæˆ–å¯»æ‰¾ä¸€ä¸ªå°†è¿™ä¸¤ä¸ªæœ¯è¯­åœ¨è¯­ä¹‰ç©ºé—´ä¸­æ”¾å¾—éžå¸¸è¿‘çš„åµŒå…¥ç®—æ³•å¯èƒ½ä¼šäº§ç”Ÿä¸€äº›æ„æƒ³ä¸åˆ°å’Œä¸å¸Œæœ›çš„å‰¯ä½œç”¨ã€‚\n\næœ€å¥½ä¸è¦å¼ºè¿«è¯­ä¹‰æ¨¡åž‹åŽ»åšä¸€ä¸ªåœ°ç†æˆ–æ—…æ¸¸ä¿¡æ¯æ›´é€‚åˆçš„å·¥ä½œã€‚å¦‚æžœæˆ‘æ˜¯ä¸€ä¸ªä¾èµ–äºŽäº†è§£è¿™äº›åœ°æ ‡æ‰€åœ¨ç¤¾åŒºçš„æ—…è¡Œæˆ–æ—…æ¸¸å…¬å¸ï¼Œæˆ‘å®æ„¿å»ºç«‹ä¸€ä¸ªèƒ½å¤Ÿç¡®å®šè¿™äº›ä¿¡æ¯çš„æ•°æ®åº“â€”â€”è¿™ä¸ªä»»åŠ¡æ¯”è®©è¯­ä¹‰å‘é‡æœç´¢å®ŒæˆåŒæ ·çš„ä»»åŠ¡è¦å®¹æ˜“å¾—å¤šâ€¦â€¦è€Œä¸”æ²¡æœ‰å®Œå…¨çš„ç¡®å®šæ€§ã€‚\n\nå› æ­¤ï¼Œè¿™é‡Œä¸»è¦çš„é—®é¢˜æ˜¯ï¼Œæˆ‘ä»¬æœ‰ä¸€äº›æˆ‘ä»¬çŸ¥é“ä»¥æŸç§æ–¹å¼ç›¸å…³çš„æ¦‚å¿µå’Œä¿¡æ¯ï¼Œä½†åœ¨è¯­ä¹‰å‘é‡ç©ºé—´ä¸­å¹¶ä¸ç›¸å…³ã€‚æŸäº›å…¶ä»–ï¼ˆéžå‘é‡ï¼‰ä¿¡æ¯æ¥æºå‘Šè¯‰æˆ‘ä»¬ï¼Œæˆ‘ä»¬æ­£åœ¨å¤„ç†çš„å„ç§æ¦‚å¿µä¹‹é—´å­˜åœ¨è”ç³»ã€‚æž„å»ºå›¾å½¢ RAG åº”ç”¨çš„ä»»åŠ¡æ˜¯æœ‰æ•ˆåœ°å°†è¿™äº›æ¦‚å¿µä¹‹é—´çš„è¿žæŽ¥æ•æ‰åˆ°çŸ¥è¯†å›¾è°±ä¸­ï¼Œå¹¶åˆ©ç”¨å›¾å½¢è¿žæŽ¥æ¥æ£€ç´¢æ›´ç›¸å…³çš„æ–‡æ¡£ï¼Œä»¥å“åº”æç¤ºã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*flPVNMUm83oc7H9Lt7U5AA.jpeg)\n\næ€»ç»“ä¸€ä¸‹æˆ‘ä»¬è¯•å›¾é€šè¿‡å›¾å½¢ RAG è§£å†³çš„é—®é¢˜ï¼šå­˜åœ¨åŠç»“æž„åŒ–ã€éžè¯­ä¹‰çš„ä¿¡æ¯è¿žæŽ¥ç€æˆ‘éžç»“æž„åŒ–æ–‡æ¡£ä¸­å‡ºçŽ°çš„è®¸å¤šæ¦‚å¿µâ€”â€”æˆ‘å¸Œæœ›åˆ©ç”¨è¿™äº›è¿žæŽ¥ä¿¡æ¯æ¥è¡¥å……è¯­ä¹‰å‘é‡æœç´¢ï¼Œä»¥æ£€ç´¢æœ€é€‚åˆå›žç­”æˆ‘ç”¨ä¾‹ä¸­æç¤ºå’Œé—®é¢˜çš„æ–‡æ¡£ã€‚æˆ‘ä»¬åªæ˜¯æƒ³è®©æ£€ç´¢å˜å¾—æ›´å¥½ï¼Œå¹¶å¸Œæœ›ä½¿ç”¨ä¸€äº›å¤–éƒ¨ä¿¡æ¯æˆ–å¤–éƒ¨é€»è¾‘æ¥å®žçŽ°è¿™ä¸€ç‚¹ï¼Œè€Œä¸æ˜¯ä»…ä»…ä¾èµ–è¯­ä¹‰å‘é‡æœç´¢æ¥è¿žæŽ¥æç¤ºä¸Žæ–‡æ¡£ã€‚\n\n## å°†å›¾å½¢ä¸Ž RAG é›†æˆçš„æŒ‡å¯¼åŽŸåˆ™\n\nè€ƒè™‘åˆ°ä¸Šè¿°åŠ¨æœºâ€”â€”ä½¿ç”¨â€œå¤–éƒ¨â€ä¿¡æ¯æ¥å»ºç«‹è¯­ä¹‰æœç´¢å¯èƒ½é—æ¼çš„æ–‡æ¡£è¿žæŽ¥â€”â€”åœ¨æž„å»ºå’Œæµ‹è¯•å›¾å½¢ RAG åº”ç”¨ç¨‹åºæ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ç‰¢è®°ä¸€äº›æŒ‡å¯¼åŽŸåˆ™ï¼š\n\n1. å›¾å½¢åº”åŒ…å«é«˜è´¨é‡ã€æœ‰æ„ä¹‰çš„æ¦‚å¿µå’Œè¿žæŽ¥\n2. æ¦‚å¿µå’Œè¿žæŽ¥åº”ä¸Žç”¨ä¾‹é›†ä¸­çš„æç¤ºç›¸å…³\n3. å›¾å½¢è¿žæŽ¥åº”è¡¥å……è€Œä¸æ˜¯æ›¿ä»£å‘é‡æœç´¢\n4. åº”ä¼˜å…ˆè€ƒè™‘ä¸€æ­¥å’Œä¸¤æ­¥å›¾å½¢è¿žæŽ¥çš„å®žç”¨æ€§ï¼›ä¾èµ–äºŽè¶…è¿‡ä¸‰æ­¥çš„è¿žæŽ¥åº”ä»…é™äºŽä¸“ä¸šç”¨ä¾‹ã€‚\n\nä¹Ÿè®¸åœ¨æœªæ¥çš„æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æŽ¢è®¨éµå¾ªè¿™äº›åŽŸåˆ™çš„ç»†å¾®å·®åˆ«å’Œæ½œåœ¨å½±å“ï¼Œä½†ç›®å‰æˆ‘åªæƒ³æŒ‡å‡ºï¼Œè¿™ä¸ªåˆ—è¡¨æ—¨åœ¨å…±åŒæé«˜å¯è§£é‡Šæ€§ï¼Œé˜²æ­¢è¿‡äºŽå¤æ‚ï¼Œå¹¶æœ€å¤§åŒ–æž„å»ºå’Œä½¿ç”¨å›¾å½¢ RAG ç³»ç»Ÿçš„æ•ˆçŽ‡ã€‚\n\néµå¾ªè¿™äº›åŽŸåˆ™ä»¥åŠè½¯ä»¶å·¥ç¨‹å’Œæ•°æ®ç§‘å­¦çš„å…¶ä»–æ ¸å¿ƒåŽŸåˆ™ï¼Œå¯ä»¥å¢žåŠ æˆåŠŸæž„å»ºæœ‰ç”¨ä¸”å¼ºå¤§çš„å›¾å½¢ RAG åº”ç”¨ç¨‹åºçš„æœºä¼šï¼Œä½†åœ¨æ­¤è¿‡ç¨‹ä¸­è‚¯å®šä¼šæœ‰é™·é˜±ï¼Œæˆ‘ä»¬å°†åœ¨ä¸‹ä¸€èŠ‚ä¸­æ¦‚è¿°ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*twgres708JPQHa1uZrkwDA.jpeg)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*5U0k4GoHTFiQhKM2a6xdMA.jpeg)\n\n## ä½ çš„å›¾å½¢ RAG åº”ç”¨å¯èƒ½æ— æ³•æŠ•å…¥ç”Ÿäº§çš„åŽŸå› \n\nä»»ä½•èŠ±è´¹å¤§é‡æ—¶é—´å›´ç»•æ•°æ®ã€å¤æ‚ç®—æ³•ã€ç»Ÿè®¡å­¦å’Œäººç±»ç”¨æˆ·æž„å»ºè½¯ä»¶çš„äººéƒ½å¯èƒ½ç†è§£ï¼Œåœ¨æž„å»ºåƒå›¾å½¢ RAG è¿™æ ·çš„ç³»ç»Ÿæ—¶å­˜åœ¨å¾ˆå¤šä¸ç¡®å®šæ€§ã€‚åœ¨æ•°æ®å‡†å¤‡å’ŒåŠ è½½ã€æž„å»ºçŸ¥è¯†å›¾è°±ã€æŸ¥è¯¢å’ŒéåŽ†å›¾å½¢ã€ç»“æžœæ±‡ç¼–å’Œæç¤ºæž„å»ºï¼Œä»¥åŠå‡ ä¹Žå·¥ä½œæµç¨‹ä¸­çš„ä»»ä½•å…¶ä»–ç‚¹ï¼Œéƒ½å¯èƒ½å‘ç”Ÿæ„å¤–æƒ…å†µã€‚\n\nåœ¨ä¸Šé¢ï¼Œæˆ‘ä»¬è®¨è®ºäº†å¦‚ä½•è½»æ¾å®žçŽ°å›¾å½¢ RAG ä»¥èŽ·å¾—åˆæ­¥ç»“æžœï¼Œä½†è¦èŽ·å¾—è‰¯å¥½çš„ç»“æžœï¼Œæ›´ä¸ç”¨è¯´ç”Ÿäº§çº§åˆ«çš„ç»“æžœäº†ï¼Œå¯èƒ½ä¼šå¾ˆå›°éš¾ã€‚æŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†çœ‹çœ‹åœ¨æž„å»ºå’Œæµ‹è¯•å›¾å½¢ RAG åº”ç”¨æ—¶å¯èƒ½é‡åˆ°çš„ä¸€äº›æ½œåœ¨é—®é¢˜ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*1J9hwwZDuYZ3WNrxCl_cOA.jpeg)\n\n### å›¾å½¢RAGçš„è¡¨çŽ°ä¸Žæ™®é€šRAGç›¸å·®æ— å‡ \n\nå¦‚æžœæ‚¨çš„å›¾å½¢RAGç³»ç»Ÿçš„æ€§èƒ½ä¸Žæ™®é€šRAGå¤§è‡´ç›¸åŒï¼Œå¯èƒ½æœ‰å¤šç§åŽŸå› ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œè¿™ä¼¼ä¹Žæ„å‘³ç€å›¾å½¢å¹¶æ²¡æœ‰ä¸ºç³»ç»Ÿå¢žåŠ ä»·å€¼ï¼Œä½†è¿™å¯èƒ½æ˜¯ç”±äºŽä½Žè´¨é‡çš„çŸ¥è¯†å›¾ã€å›¾çš„åˆ©ç”¨ä¸è¶³ã€å‚æ•°è®¾ç½®ä¸ä½³æˆ–å…¶ä»–è®¸å¤šåŽŸå› é€ æˆçš„ã€‚æˆ–è€…ï¼Œæ ¹æœ¬æ²¡æœ‰é—®é¢˜ï¼›å‘é‡æœç´¢å¯èƒ½åœ¨å¯»æ‰¾æ­£ç¡®çš„æ–‡æ¡£æ–¹é¢è¡¨çŽ°å‡ºè‰²ï¼Œè€Œå›¾å½¢æ ¹æœ¬ä¸éœ€è¦ã€‚\n\néœ€è¦å…³æ³¨çš„äº‹é¡¹ï¼š\n\n* æ‚¨æ˜¯å¦æœ‰æ™®é€šRAGæ— æ³•å¾ˆå¥½å¤„ç†çš„ç¤ºä¾‹æç¤ºï¼Œä½†æ‚¨æœŸæœ›å›¾å½¢RAGèƒ½å¤ŸæˆåŠŸå¤„ç†ï¼Ÿæ‚¨èƒ½å¦åœ¨è¿™äº›æç¤ºä¸Šè¿›è¡Œâ€œè°ƒè¯•â€ï¼Œçœ‹çœ‹åŽå°å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ\n* çŸ¥è¯†å›¾æ˜¯å¦åŒ…å«è¯­ä¹‰æœç´¢å¯èƒ½æ— æ³•å»ºç«‹çš„æœ‰æ„ä¹‰è¿žæŽ¥ï¼Ÿæ‚¨èƒ½å¦æ‰¾åˆ°åœ¨å›¾ä¸­è¿žæŽ¥çš„æ¦‚å¿µå¯¹çš„ç¤ºä¾‹ï¼Œå…¶ç›¸å…³æ–‡æ¡£åœ¨å‘é‡ç©ºé—´ä¸­ç›¸è·ç”šè¿œï¼ŸçŸ¥è¯†å›¾åº”è¯¥åœ¨â€œè¿œç¦»â€çš„æ–‡æ¡£ä¹‹é—´å»ºç«‹æœ‰æ„ä¹‰çš„è¿žæŽ¥ã€‚\n\n### ä½ ï¼ˆä»ç„¶ï¼‰çœ‹åˆ°å¹»è§‰\n\nå¦‚æžœä½ åœ¨ä½¿ç”¨å›¾å½¢ RAG æ—¶çœ‹åˆ°çš„å¹»è§‰ä¸Žä½¿ç”¨æ™®é€š RAG æ—¶ä¸åŒï¼Œæˆ‘ä¼šæ€€ç–‘æŸå¤„å­˜åœ¨é”™è¯¯æˆ–å‚æ•°è®¾ç½®ä¸å½“ã€‚å¦‚æžœä½ çœ‹åˆ°çš„å¹»è§‰æ°´å¹³ç›¸ä¼¼ï¼Œè¿™å¬èµ·æ¥åƒæ˜¯ä¸€ä¸ªè¶…å‡ºå›¾å½¢æ–¹é¢çš„ä¸€èˆ¬é—®é¢˜ã€‚\n\néœ€è¦å…³æ³¨çš„äº‹é¡¹ï¼š\n\n* ä½ çš„æ–‡æ¡£é›†æ˜¯å¦åŒ…å«å¯¹å¼•å‘å¹»è§‰çš„æç¤ºçš„æ­£ç¡®å“åº”ï¼Ÿå‘é‡æœç´¢æ˜¯å¦æ‰¾åˆ°äº†è¿™äº›æ–‡æ¡£ï¼Ÿ\n* ä»Žæ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸­èŽ·å–çš„æ­£ç¡®å“åº”æ˜¯å¦æ­£ç¡®æ’å…¥åˆ°ä¼ é€’ç»™ LLM çš„æç¤ºä¸Šä¸‹æ–‡ä¸­ï¼Ÿ\n\n### å›¾è¡¨â€œè¿‡å¤§â€\n\nå½“æ‚¨çš„çŸ¥è¯†å›¾è°±â€œè¿‡å¤§â€æˆ–è¿‡äºŽå¯†é›†æ—¶ï¼Œå¯èƒ½ä¼šå‡ºçŽ°ä¸¤ç§ä¸»è¦é—®é¢˜ã€‚é¦–å…ˆï¼Œå¯èƒ½ä¼šå‡ºçŽ°æ‰©å±•æ€§é—®é¢˜ï¼Œæˆ‘å°†åœ¨ä¸‹é¢è®¨è®ºã€‚å…¶æ¬¡ï¼Œå›¾éåŽ†å¯èƒ½å¯¼è‡´â€œè¿‡å¤šâ€çš„æ–‡æ¡£ï¼Œè¿™äº›æ–‡æ¡£å¿…é¡»é‡æ–°æŽ’åºå’Œè¿‡æ»¤ã€‚å¦‚æžœé‡æ–°æŽ’åºå’Œè¿‡æ»¤ç­–ç•¥ä¸Žæ£€ç´¢å’Œå›¾éåŽ†å…ƒç´ ä¸å…¼å®¹ï¼Œæ‚¨å¯èƒ½ä¼šåœ¨å›¾åˆšå‘çŽ°é‡è¦æ–‡æ¡£åŽç«‹å³å°†å…¶è¿‡æ»¤æŽ‰ã€‚\n\néœ€è¦å…³æ³¨çš„å†…å®¹ï¼š\n\n* å›¾éåŽ†åŽè¿”å›žäº†å¤šå°‘æ–‡æ¡£ï¼Œå¤šå°‘è¢«é‡æ–°æŽ’åºæˆ–è¿‡æ»¤æŽ‰ï¼Ÿé€šè¿‡å¼ºå›¾è¿žæŽ¥æ‰¾åˆ°çš„æ–‡æ¡£æ˜¯å¦èƒ½æˆåŠŸå­˜æ´»äºŽè¿‡æ»¤ä¸­ï¼Ÿ\n* æ‚¨æ˜¯å¦æž„å»ºäº†ä¸€ä¸ªå……æ»¡é€‚åˆæ‚¨ç”¨ä¾‹çš„æœ‰æ„ä¹‰è¿žæŽ¥çš„çŸ¥è¯†å›¾è°±ï¼Ÿåœ¨å›¾ä¸­ï¼Œæ‚¨èƒ½å¦æ‰¾åˆ°è®¸å¤šå¯¹æ‚¨çš„ç”¨ä¾‹è¿‡äºŽé€šç”¨æˆ–æ— å…³çš„æ¦‚å¿µæˆ–è¿žæŽ¥ï¼Ÿæ‚¨çš„çŸ¥è¯†å›¾è°±ä¸­æœ‰å¤šå°‘æ˜¯ç”±ä½Žè´¨é‡ä¿¡æ¯ç»„æˆçš„ï¼Ÿ\n\n### å›¾å½¢â€œå¤ªå°â€\n\næ ¹æ®ä¸Šè¿°ï¼Œå¦‚æžœå›¾å½¢â€œå¤ªå¤§â€ï¼Œå¯èƒ½ä¼šå……æ»¡ä½Žè´¨é‡çš„è¿žæŽ¥ã€‚è€Œå¦‚æžœå›¾å½¢â€œå¤ªå°â€ï¼Œæˆ‘å¸Œæœ›é‚£é‡Œå­˜åœ¨çš„è¿žæŽ¥æ˜¯æœ‰æ„ä¹‰çš„ï¼Œè¿™å¾ˆå¥½ï¼Œä½†ç¼ºå¤±çš„è¿žæŽ¥ä¸»è¦æœ‰ä¸¤ç§ç±»åž‹ã€‚ç¬¬ä¸€ç§æ˜¯ç”±äºŽå›¾å½¢æž„å»ºè¿‡ç¨‹ä¸­å‡ºçŽ°çš„é”™è¯¯å¼•èµ·çš„ã€‚ç¬¬äºŒç§æ˜¯ç”±äºŽå›¾å½¢æž„å»ºæœªé’ˆå¯¹å…¶è¿›è¡Œè®¾è®¡ã€‚ä¸åŒä¸Šä¸‹æ–‡æˆ–ä¸åŒæ ¼å¼çš„æ•°æ®å¯èƒ½ä¼šè¢«ä¸åŒçš„å›¾å½¢æž„å»ºæ–¹æ³•ä»¥ä¸åŒçš„æ–¹å¼å¤„ç†ã€‚\n\néœ€è¦å…³æ³¨çš„å†…å®¹ï¼š\n\n* ä½ æ˜¯å¦ä½¿ç”¨å¸¦æœ‰å®žä½“/å…³é”®è¯æå–çš„LLMæž„å»ºäº†çŸ¥è¯†å›¾è°±ï¼Ÿä½ æ˜¯å¦æ•èŽ·äº†æ¯ä¸ªæ–‡æ¡£ä¸­æ‰€æœ‰æœ‰æ„ä¹‰çš„å®žä½“ï¼Œè¿˜æ˜¯LLMé™åˆ¶äº†å…¶è¾“å‡ºï¼Ÿ\n* åœ¨ä½ çš„æ–‡æ¡£ä¸­ï¼Œæœ‰å“ªäº›æ¦‚å¿µå’Œè¿žæŽ¥æ˜¯ä½ æœŸæœ›å‡ºçŽ°åœ¨çŸ¥è¯†å›¾è°±ä¸­çš„ï¼Œä½†ä¼¼ä¹Žç¼ºå¤±äº†ï¼Ÿä½ æœŸå¾…å®ƒä»¬ä½•æ—¶ä»¥åŠå¦‚ä½•è¢«æ·»åŠ åˆ°å›¾è°±ä¸­ï¼Ÿä¸ºä»€ä¹ˆå®ƒä»¬å®žé™…ä¸Šæ²¡æœ‰è¢«æ·»åŠ åˆ°å›¾è°±ä¸­ï¼Ÿ\n\n### ä½ æ‰¾ä¸åˆ°â€œé€‚ä¸­â€å›¾è¡¨\n\nä½ æ˜¯å¦è§‰å¾—å¯ä»¥æž„å»ºä¸€ä¸ªâ€œè¿‡å¤§â€æˆ–â€œè¿‡å°â€çš„å›¾è¡¨ï¼Œä½†æ— æ³•æž„å»ºä¸€ä¸ªä¸­ç­‰å¤§å°çš„å›¾è¡¨ï¼Ÿ\n\néœ€è¦æ³¨æ„çš„äº‹é¡¹ï¼š\n\n* ä½ æ­£åœ¨æ›´æ”¹å“ªäº›å‚æ•°æˆ–æ–¹æ³•æ¥ä»Žå°åˆ°å¤§æˆ–åå‘å˜åŒ–ï¼Ÿè¿™äº›æ˜¯å¦åº”è¯¥å¯¹å›¾è¡¨è´¨é‡äº§ç”Ÿå¦‚æ­¤å¤§çš„å½±å“ï¼Ÿä½ èƒ½å¦ç ”ç©¶ä¸€äº›æ ¹æ®æ‰€ä½¿ç”¨çš„å›¾è¡¨æž„å»ºè®¾ç½®æ„å¤–å‡ºçŽ°æˆ–æ¶ˆå¤±çš„å›¾è¡¨å…ƒç´ ï¼Ÿ\n* å¦è¯·æŸ¥çœ‹ä¸Šé¢â€œè¿‡å¤§â€å’Œâ€œè¿‡å°â€éƒ¨åˆ†çš„ç›¸å…³æç¤ºã€‚\n\n### ä½ çš„å®žçŽ°éœ€è¦æ–°çš„è½¯ä»¶æˆ–å¢žåŠ éƒ¨ç½²å¤æ‚æ€§\n\nè¿™æ˜¯ä¸€ä¸ªç»å…¸çš„æ•°æ®ç§‘å­¦é—®é¢˜ï¼šæž„å»ºéžå¸¸é…·ç‚«å’Œå‰æ²¿çš„æ–¹æ³•ï¼Œå´çœ‹åˆ°å¼€å‘å›¢é˜Ÿæ‹’ç»æˆ–éš¾ä»¥å°†ä½ ç¬”è®°æœ¬ä¸­çš„ä»£ç å¼•å…¥ç”Ÿäº§çŽ¯å¢ƒã€‚åšæŒä½¿ç”¨æœ€æµè¡Œã€æ”¯æŒæœ€å¥½çš„ä»¥åŠå¤§éƒ¨åˆ†å¼€æºçš„å·¥å…·ï¼Œå¯ä»¥æ›´å®¹æ˜“åœ°è¿›å…¥ç”Ÿäº§ï¼Œç‰¹åˆ«æ˜¯å¦‚æžœä½ çš„ç»„ç»‡åœ¨å…¶ä»–åœ°æ–¹å·²ç»ä½¿ç”¨è¿™äº›å·¥å…·çš„è¯ã€‚\n\néœ€è¦å…³æ³¨çš„äº‹é¡¹ï¼š\n\n* ä½ çš„å®žçŽ°æ˜¯å¦éœ€è¦ä¸ºå›¾å½¢åˆ›å»ºæ–°çš„æ•°æ®å­˜å‚¨ï¼Ÿä½ [å¯èƒ½ä¸éœ€è¦å›¾å½¢æ•°æ®åº“](https://www.datastax.com/blog/knowledge-graphs-for-rag-without-a-graphdb)ï¼Œè€Œä¸”å¯èƒ½èƒ½å¤Ÿä½¿ç”¨ä½ çš„ç”Ÿäº§å‘é‡å­˜å‚¨æ¥å¤„ç†å›¾å½¢ã€‚\n* ä½ æ˜¯å¦åœ¨ä½¿ç”¨ä¸€äº›æœ€æµè¡Œçš„å¼€æºå·¥å…·æ¥æž„å»ºAIåº”ç”¨ç¨‹åºï¼Œæ¯”å¦‚LangChainï¼Ÿè¿™äº›å·¥å…·å¯ä»¥å‡å°‘ä»£ç å¤æ‚æ€§ï¼Œä½¿åº”ç”¨ç¨‹åºæ›´å…·å¯ç§»æ¤æ€§ï¼Œå¹¶æ‰©å±•æ½œåœ¨çš„é›†æˆå’Œè¿›ä¸€æ­¥å¼€å‘ã€‚\n\n### ä½ çš„å®žçŽ°æ— æ³•æ‰©å±•\n\næ–‡ç«  [Scaling Knowledge Graphs by Eliminating Edges](https://thenewstack.io/scaling-knowledge-graphs-by-eliminating-edges/) åœ¨ *The New Stack* ä¸­å±•ç¤ºäº†ä¸€ç§ä½¿å›¾å½¢ RAG éžå¸¸å¯æ‰©å±•çš„æ–¹æ³•ã€‚åƒä¸Šé¢æåˆ°çš„ï¼Œæœ€æµè¡Œã€æ”¯æŒæœ€å¥½çš„ã€å¹¶ä¸”å¤§å¤šæ•°æ˜¯å¼€æºçš„å·¥å…·é€šå¸¸æ˜¯æ— ç—›æ‰©å±•çš„æœ€ä½³è·¯å¾„ï¼Œä½†è¿™å¹¶ä¸æ€»æ˜¯å®¹æ˜“ã€‚\n\néœ€è¦å…³æ³¨çš„å†…å®¹ï¼š\n\n* å“ªä¸€éƒ¨åˆ†æ— æ³•æ‰©å±•ï¼Ÿå›¾éåŽ†ã€é‡æ–°æŽ’åºã€ç»“æžœæ±‡ç¼–ï¼Œè¿˜æ˜¯å…¶ä»–ï¼Ÿè¯·å‚è§ä¸Šé¢çš„â€œThe graph is too bigâ€ä»¥èŽ·å–æ›´å¤šæç¤ºã€‚\n* ä½ æ˜¯å¦æœ‰æŸä¸ªç‰¹å®šç»„ä»¶æ— æ³•å¾ˆå¥½åœ°æ‰©å±•ï¼Ÿæœ‰æ—¶ä½¿ç”¨å†…å­˜ä¸­çš„å›¾å½¢åº“ï¼Œå¦‚ 'networkx' â€” æˆ–è€…ç”šè‡³æ˜¯å›¾å½¢æ•°æ®åº“ â€” æ¥æ‰§è¡Œå¤æ‚çš„å›¾å½¢æ“ä½œå¯èƒ½ä¼šé€ æˆèµ„æºç“¶é¢ˆã€‚ä½ å¯èƒ½æƒ³è¦ [åˆ‡æ¢åˆ°æ›´å¯æ‰©å±•çš„å›¾å½¢æ“ä½œé€‰é¡¹](https://bit.ly/3YD5NAd)ã€‚\n* ä½ æ˜¯å¦åœ¨ä½¿ç”¨å¹¶è¡Œ API è°ƒç”¨æ¥å¤„ç†å¤§éƒ¨åˆ†ç¹é‡çš„å·¥ä½œï¼Œè¿˜æ˜¯åœ¨å°è¯•åœ¨ä¸»åº”ç”¨é€»è¾‘ä¸­è¿›è¡Œå¤æ‚æˆ–é«˜æˆæœ¬çš„è®¡ç®—ï¼Ÿ\n\n## åœ¨ç”Ÿäº§ä¸­åˆ©ç”¨å›¾å½¢RAGèŽ·å¾—æˆåŠŸ\n\nåˆ›å»ºæˆåŠŸçš„å›¾å½¢RAGç³»ç»Ÿçš„å…³é”®åœ¨äºŽæž„å»ºä¸€ä¸ªçŸ¥è¯†å›¾è°±å’ŒéåŽ†é€»è¾‘ï¼Œä»¥è¡¥å……è¯­ä¹‰å‘é‡æ£€ç´¢ï¼Œè€Œä¸æ˜¯å–ä»£æˆ–ä¸Žä¹‹ç«žäº‰ã€‚å›¾å½¢è®¾è®¡åº”æ—¨åœ¨åœ¨åˆé€‚çš„æ—¶é—´è¿žæŽ¥æ­£ç¡®çš„èŠ‚ç‚¹ã€çŸ¥è¯†ã€å®žä½“å’Œæ–‡æ¡£ï¼Œä»Žè€Œèƒ½å¤Ÿç»„è£…åˆé€‚çš„æ–‡æ¡£ï¼Œä»¥äº§ç”Ÿæœ€æœ‰å¸®åŠ©å’Œå¯æ“ä½œçš„æŸ¥è¯¢å“åº”ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*orXW5uw-geBo-WVtZUxWXQ.jpeg)\n\nå…³äºŽGleanï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå†…éƒ¨æ–‡æ¡£æ•°æ®é›†æ˜¯å›¾å½¢RAGçš„ä¸€ä¸ªå®Œç¾Žç”¨ä¾‹ã€‚çŸ¥è¯†å›¾è°±å¯ä»¥è¿žæŽ¥äººã€é¡¹ç›®ã€äº§å“ã€å®¢æˆ·ã€ä¼šè®®ã€åœ°ç‚¹ç­‰â€”â€”æ‰€æœ‰è¿™äº›åœ¨æ•°é‡ä¸Šéƒ½å—åˆ°ç»„ç»‡è§„æ¨¡å’Œå…¶å·¥ä½œå†…å®¹çš„é™åˆ¶ã€‚æž„å»ºå’Œç®¡ç†ä¸€ä¸ªç”±æ•°åƒåå‘˜å·¥ç»„æˆçš„å›¾å½¢æ¯”ä¾‹å¦‚å°è¯•å¯¹ç»´åŸºç™¾ç§‘ä¸Šæåˆ°çš„æ‰€æœ‰äººæˆ–å¤§åž‹è´¢åŠ¡æˆ–æ³•å¾‹æ–‡æ¡£æ•°æ®åº“ä¸­çš„æ‰€æœ‰äººåšåŒæ ·çš„äº‹æƒ…è¦å¯è¡Œå¾—å¤šã€‚å› æ­¤ï¼ŒGleanåšå‡ºçš„ç¬¬ä¸€ä¸ªé‡å¤§å†³ç­–å¯èƒ½æ˜¯æ‰¾åˆ°ä¸€ä¸ªå¾ˆå¥½çš„å›¾å½¢RAGç”¨ä¾‹æ¥è§£å†³é—®é¢˜ã€‚\n\nå›¾å½¢RAGç³»ç»Ÿçš„ä¸€ä¸ªå¸¸è¢«ä½Žä¼°çš„æ–¹é¢æ˜¯è¾“å…¥æ•°æ®çš„è´¨é‡å’Œå°†å…¶ä¼ è¾“åˆ°ç›®çš„åœ°çš„ç®¡é“çš„å¯é æ€§ã€‚è¿™ä¸Žæ•°æ®å·¥ç¨‹å’Œä¼ ç»Ÿè½¯ä»¶å¼€å‘çš„å…³ç³»å¤§äºŽä¸ŽAIçš„å…³ç³»ã€‚åœ¨ä»¥å¾€çš„æŠ€æœ¯èŒƒå¼ä¸­ï¼Œç”±äºŽæ•°æ®ç±»åž‹å’Œè®¿é—®æ–¹æ³•çš„ä¸å…¼å®¹ï¼Œè¿žæŽ¥ä¸åŒçš„æ•°æ®ç³»ç»Ÿæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚çŽ°åœ¨ï¼ŒAIå’ŒLLMsä½¿å¾—å°†ä¸åŒæ¥æºçš„éžç»“æž„åŒ–æ•°æ®æ•´åˆæˆä¸ºå¯èƒ½ï¼Œä»Žè€Œå…è®¸å°†æ¥è‡ªå„ç§æ¥æºçš„æ•°æ®æ•´åˆåˆ°ä¸€ä¸ªå•ä¸€çš„RAGç³»ç»Ÿä¸­ã€‚è¿™ç§é›†æˆèƒ½åŠ›ä½¿å¾—LLMsèƒ½å¤Ÿå¤„ç†å’Œç†è§£æ¥è‡ªå„ç§æ¥æºçš„éžç»“æž„åŒ–æ•°æ®ï¼Œä¾‹å¦‚å†…éƒ¨ç½‘é¡µã€ç»´åŸºã€ä»£ç åº“ã€æ•°æ®åº“ã€Googleæ–‡æ¡£å’ŒèŠå¤©è®°å½•ã€‚ä»…ä»…å°†æ‰€æœ‰è¿™äº›ä¿¡æ¯è¿žæŽ¥åœ¨ä¸€èµ·ï¼Œå¹¶é€šè¿‡å•ä¸€æŽ¥å£ä½¿å…¶å¯è®¿é—®ï¼Œå°±å¯ä»¥å¸¦æ¥å·¨å¤§çš„æ”¶ç›Šã€‚\n\n## å‰è¿›çš„æ–¹å‘\n\næž„å»ºå›¾ RAG ç³»ç»Ÿä»¥æ»¡è¶³ä»»ä½•ç”¨ä¾‹éœ€è¦åˆ©ç”¨åŸºç¡€ç»„ä»¶ï¼Œå¦‚å‘é‡å’Œå›¾çš„æ•°æ®å­˜å‚¨ã€åµŒå…¥å’Œ LLMï¼Œå¹¶é€šè¿‡å¼€æºç¼–æŽ’å·¥å…·å¦‚ LangChain å’Œ LlamaIndex è¿›è¡Œå¢žå¼ºã€‚è¿™äº›å·¥å…·ä¿ƒè¿›äº†å¼ºå¤§ã€å¯æ‰©å±•å’Œé«˜æ•ˆç³»ç»Ÿçš„å¼€å‘ï¼Œæ‰¿è¯ºæœªæ¥å…¬å¸é€šè¿‡è‡ªåŠ¨åŒ–å’Œç²¾ç®€ä¼˜åŒ–çŸ¥è¯†å·¥ä½œï¼Œå®žçŽ°æ˜¾è‘—æˆåŠŸã€‚\n\nçŸ¥è¯†å›¾è°±å’Œå›¾ RAG ç³»ç»Ÿçš„å…¬å…±æˆåŠŸï¼Œå°¤å…¶æ˜¯åƒ Glean è¿™æ ·çš„å…¬å¸çš„æˆåŠŸï¼Œå±•ç¤ºäº†è¿™äº›æŠ€æœ¯åœ¨å†…éƒ¨ç”¨ä¾‹ä¸­çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡æé«˜ç»„ç»‡æ•ˆçŽ‡åˆ›é€ ä»·å€¼ã€‚ç„¶è€Œï¼Œé¢å‘å¤–éƒ¨çš„ä¼ä¸šå’Œæ¶ˆè´¹è€…äº§å“çš„æ›´å¹¿æ³›åº”ç”¨æ½œåŠ›ä»ç„¶åŸºæœ¬æœªè¢«å¼€å‘ï¼Œä¸ºå…¶ä»–å…¬å¸æä¾›äº†è®¸å¤šæŽ¢ç´¢æœºä¼šã€‚\n\nå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å·²ç»å¤„äºŽæ‰€è°“çš„â€œä¿¡æ¯æ—¶ä»£â€è‡³å°‘ 30 å¹´ï¼Œè€Œåœ¨è¿‡åŽ»ä¸€ä¸¤å¹´ä¸­ï¼Œæˆ‘ä»¬æ‰çœŸæ­£å¼€å§‹å°†æ‰€æœ‰è¿™äº›ä¿¡æ¯è·¨æ¥æºã€è·¨æ€æƒ³ã€è·¨æ–‡æ¡£å’Œè·¨æ¦‚å¿µè¿žæŽ¥èµ·æ¥ï¼Œä»¥ä¾¿æˆ‘ä»¬çš„è½¯ä»¶ç³»ç»Ÿèƒ½å¤Ÿè¿›è¡Œä¸Žæˆ‘ä»¬äººç±»åœ¨çŸ¥è¯†å·¥ä½œä¸­æ—¥å¸¸ä½¿ç”¨çš„æŽ¨ç†ã€é€»è¾‘å’Œåˆ¤æ–­ç›¸åŒçš„ç±»åž‹çš„æŽ¨ç†ã€‚ä¸€äº›äººç§°ä¹‹ä¸ºâ€œæ™ºèƒ½æ—¶ä»£â€ã€‚\n\nè™½ç„¶æœ€åˆå…³æ³¨ç®€å•ã€ç›´æŽ¥çš„å†³ç­–ï¼Œä½†äººå·¥æ™ºèƒ½çš„è½¨è¿¹æ­£æœç€ç®¡ç†æ›´å¤æ‚åœºæ™¯çš„æ–¹å‘å‘å±•ï¼Œæ˜¾è‘—æé«˜æ—¶é—´å’Œæˆæœ¬çš„æ•ˆçŽ‡ã€‚è¿™ä¸€ä»¤äººå…´å¥‹çš„æ¼”å˜ä½¿è®¸å¤šäººå·¥æ™ºèƒ½åº”ç”¨ï¼ŒåŒ…æ‹¬å›¾ RAGï¼Œåœ¨è½¬å˜çŸ¥è¯†å¦‚ä½•åœ¨å„ç§èƒŒæ™¯ä¸‹ç›¸äº’è¿žæŽ¥å’Œåˆ©ç”¨æ–¹é¢å˜å¾—è‡³å…³é‡è¦ã€‚\n\nè¦ç«‹å³å¼€å§‹ä½¿ç”¨å›¾ RAGï¼Œæˆ–äº†è§£æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ [DataStax å›¾ RAG æŒ‡å—](https://bit.ly/4862Lrl)ã€‚\n\n*ä½œè€…ï¼šBrian Godsey, Ph.D. ([LinkedIn](https://bit.ly/4enqFRa)) â€” æ•°å­¦å®¶ã€æ•°æ®ç§‘å­¦å®¶å’Œå·¥ç¨‹å¸ˆ // [DataStax](https://bit.ly/3NpPujA) çš„äººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ äº§å“ // è‘—ä½œ [Think Like a Data Scientist](https://bit.ly/4f5uVES)*\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Il1GrFN6fYN7e_ovExRGPw.jpeg)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*wQvZDIlkOvrYZnbwl0bEPQ.jpeg)\n\n"},{"lang":"zh","group":"blog","slug":"blog/the-real-reason-openai-abandoned-next-js-for-remix-a4b2622ee9b2","frontmatter":{"title":"OpenAI æ”¾å¼ƒ Next.js è½¬è€Œä½¿ç”¨ Remix çš„çœŸæ­£åŽŸå› ","meta_title":"OpenAI æ”¾å¼ƒ Next.js è½¬è€Œä½¿ç”¨ Remix çš„çœŸæ­£åŽŸå› ","description":"OpenAI æ­¤ä¸¾èƒŒåŽä»¤äººæƒŠè®¶çš„åŽŸå› åŠå…¶å¯¹ Web å¼€å‘çš„æœªæ¥æ„å‘³ç€ä»€ä¹ˆ","date":"2024-11-08T00:25:31.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*bf8ao0JjEiMka6dJqp-hxg.jpeg","categories":["Technology/Web","Programming","Web Development"],"author":"Rifx.Online","tags":["Remix","Next.js","client-side","rendering","scalability"],"draft":false,"slug":"blog/the-real-reason-openai-abandoned-next-js-for-remix-a4b2622ee9b2"},"content":"\n### OpenAI é‡‡å–è¡ŒåŠ¨èƒŒåŽçš„æƒŠäººåŽŸå› åŠå…¶å¯¹æœªæ¥ç½‘é¡µå¼€å‘çš„å½±å“\n\n\n\n## è¿‡æ¸¡ä»‹ç»\n\nOpenAI æœ€è¿‘åœ¨å¼€å‘è€…ç¤¾åŒºå¼•èµ·äº†è½°åŠ¨ï¼Œå› ä¸ºå®ƒä»Ž Next.js è½¬å‘äº† Remixã€‚\n\nè¿™ä¸€æ„å¤–çš„è½¬å˜è®©è®¸å¤šäººè´¨ç–‘å¦‚æ­¤é‡å¤§å˜åŒ–çš„ç†ç”±ã€‚\n\nä½† **ä½ èƒ½è´£æ€ªä»–ä»¬å—ï¼Ÿ**\n\nä»¥ä¸‹æ˜¯ **å¤§å¤šæ•°å¼€å‘è€…å¯¹ NextJS çš„çœ‹æ³•**ï¼ŒåŸºäºŽ [è¿™ç¯‡](https://www.reddit.com/r/nextjs/comments/1f92jdv/chatgptcom_switched_from_nextjs_to_remix/) reddit è®¨è®ºï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GCrb_aGjh1nticKNeHh9Bg.png)\n\nè¿™å¾ˆè‰°éš¾ã€‚\n\nä½†æˆ‘ä¹Ÿåœ¨ X ä¸Šè¯¢é—®äº†å¼€å‘è€…ï¼Œ\n\nä»–ä»¬å¯¹å°åž‹é¡¹ç›®çš„çœ‹æ³•åˆ™æœ‰æ‰€ä¸åŒï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*YmN2pTYaCFXFzb3AMjfoqQ.png)\n\n## è®©æˆ‘ä»¬æ·±å…¥æŽ¢è®¨è¿™ä¸ªé—®é¢˜\n\nè¿™æ¬¡æŽ¢ç´¢ä¸ä»…ä»…æ˜¯ä¸ºäº†ç†è§£ OpenAI çš„å†³å®šï¼Œè¿˜æ¶‰åŠåˆ° **è¿™å¯¹å…¶ä»–å¼€å‘è€…å’Œæ›´å¹¿æ³›çš„ç§‘æŠ€é¢†åŸŸæ„å‘³ç€ä»€ä¹ˆ**ã€‚\n\nä¸ºäº†ç†è§£å…¶èƒŒåŽçš„ç†ç”±ï¼Œæˆ‘èŠ±äº†æ•°å°æ—¶åˆ†æžä»£ç åº“å’Œå·¥å…·ã€‚\n\nä»¥ä¸‹æ˜¯æˆ‘èŽ·å¾—çš„è§è§£ã€‚\n\n## å…³äºŽåˆ‡æ¢çš„æŠ€æœ¯è§è§£\n\nç†è§£è¿™ä¸€è¿‡æ¸¡çš„æŠ€æœ¯æ–¹é¢æ˜¯ç†è§£ä¸ºä»€ä¹ˆ OpenAI åçˆ± Remix çš„å…³é”®ã€‚\n\næˆ‘ä»¬æ£€æŸ¥äº†ä»–ä»¬çš„åº”ç”¨æž¶æž„ï¼Œä»¥è¯†åˆ« Next.js å’Œ Remix ä¹‹é—´çš„æ ¸å¿ƒå·®å¼‚ã€‚\n\n### å®¢æˆ·ç«¯æ¸²æŸ“ä¸ŽæœåŠ¡å™¨æ¸²æŸ“\n\nOpenAI çš„åº”ç”¨ç¨‹åºä¸“æ³¨äºŽ **å®¢æˆ·ç«¯æ¸²æŸ“**ï¼Œå¤§éƒ¨åˆ†å¤„ç†å‘ç”Ÿåœ¨ç”¨æˆ·çš„æµè§ˆå™¨ä¸­ã€‚\n\nè¿™å‡å°‘äº†å¯¹æœåŠ¡å™¨æ¸²æŸ“ HTML çš„éœ€æ±‚ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*50dv8sWPbwFp85fQu_dodQ.png)\n\n**Remix** éžå¸¸é€‚åˆè¿™äº›åœºæ™¯ï¼Œå› ä¸ºå®ƒæœ‰æ•ˆåœ°ç®¡ç†å®¢æˆ·ç«¯åº”ç”¨ç¨‹åºã€‚è¿™ä¸ªé€‰æ‹©ç¡®ä¿äº† OpenAI çš„ç”¨æˆ·æ‹¥æœ‰æ›´æµç•…ã€æ›´çµæ•çš„ä½“éªŒã€‚\n\n### åˆå§‹é¡µé¢åŠ è½½è¿‡ç¨‹\n\nå½“ç”¨æˆ·è®¿é—® ChatGPT ç½‘ç«™æ—¶ï¼Œ**é¢„åŠ è½½çš„ JavaScript å’Œ meta æ ‡ç­¾** å‚ä¸Žäº†åˆå§‹é¡µé¢åŠ è½½ã€‚\n\nè¿™ä¼˜åŒ–äº†å®¢æˆ·ç«¯æ¸²æŸ“è¿‡ç¨‹ã€‚**Remix** åœ¨ç®¡ç†è¿™äº›å…ƒç´ æ–¹é¢è¡¨çŽ°å‡ºè‰²ï¼Œç¡®ä¿äº†é¡ºç•…å¿«é€Ÿçš„åˆå§‹åŠ è½½ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*65VlHo5RQObk-YGzBlkx3w.png)\n\n## ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦\n\n### æ”¹è¿›çš„ç”¨æˆ·ä½“éªŒ\n\né€šè¿‡é¢„åŠ è½½å¿…è¦çš„è„šæœ¬å’Œæ•°æ®ï¼Œç”¨æˆ·åœ¨è®¿é—®ç½‘ç«™æ—¶ä¼šé‡åˆ°æ›´å°‘çš„å»¶è¿Ÿï¼Œå¹¶äº«å—åˆ°æ›´çµæ•çš„ç•Œé¢ã€‚\n\n### é«˜æ•ˆåŠ è½½\n\nRemix å¤„ç†è¿™äº›é¢„åŠ è½½å…ƒç´ çš„èƒ½åŠ›æ„å‘³ç€å‡å°‘ç­‰å¾…æ—¶é—´ï¼Œæä¾›æ›´å¿«çš„æµè§ˆä½“éªŒã€‚\n\né€šè¿‡åˆ©ç”¨è¿™äº›åŠŸèƒ½ï¼Œ\n\n*OpenAI å¯ä»¥ä¸ºå…¶ç”¨æˆ·ä»Žä¸€å¼€å§‹å°±æä¾›æ›´æ— ç¼å’Œæ„‰æ‚¦çš„ä½“éªŒã€‚*\n\n## æ·±å…¥æŽ¢è®¨ OpenAI åˆ©ç”¨çš„ Remix å…³é”®ç‰¹æ€§\n\nOpenAI åˆ©ç”¨ Remix çš„å‡ ä¸ªå…³é”®ç‰¹æ€§æ¥å¢žå¼ºä»–ä»¬çš„åº”ç”¨ç¨‹åºã€‚\n\n### é¢„åŠ è½½ç­–ç•¥\n\nRemix é¢„åŠ è½½å¿…è¦çš„æ•°æ®å’Œèµ„æºï¼Œå‡å°‘åŠ è½½æ—¶é—´å¹¶æå‡æ€§èƒ½ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GPH2ZGhT_yfrQYpR2Xhvug.png)\n\nè¿™ä¸€ç­–ç•¥ç¡®ä¿ç”¨æˆ·ä»Žä¸€å¼€å§‹å°±èƒ½èŽ·å¾—æ— ç¼çš„ä½“éªŒã€‚\n\n### æ•°æ®ç®¡ç†ä¸ŽåŠ è½½å™¨\n\nRemixçš„åŠ è½½å™¨APIæœ‰æ•ˆåœ°æ”¶é›†åˆå§‹æ¸²æŸ“æ‰€éœ€çš„æ‰€æœ‰æ•°æ®ï¼Œå¹¶ç›´æŽ¥åµŒå…¥åˆ°HTMLä¸­ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*PmCiQNkAJlu2MSFEtpydiw.png)\n\nè¿™ç§æ–¹æ³•æ¶ˆé™¤äº†é¢å¤–çš„å®¢æˆ·ç«¯æ•°æ®èŽ·å–çš„éœ€æ±‚ï¼ŒåŠ å¿«äº†æ¸²æŸ“è¿‡ç¨‹ã€‚\n\n## è½¬å‘ Remix çš„å¥½å¤„å’Œå½±å“\n\nè½¬å‘ Remix ä¸º OpenAI æä¾›äº†å¤šä¸ªä¼˜åŠ¿ï¼Œä»Žæ€§èƒ½æå‡åˆ°æœªæ¥çš„å‘å±•å‰æ™¯ã€‚\n\n### æ€§èƒ½æå‡\n\né€šè¿‡é‡‡ç”¨ Remixï¼ŒOpenAI å®žçŽ°äº†æ›´å¿«çš„åˆå§‹åŠ è½½æ—¶é—´å’Œæ›´æµç•…çš„å®¢æˆ·ç«¯å¯¼èˆªã€‚\n\nè¿™äº›æ€§èƒ½æå‡æœ‰åŠ©äºŽåˆ›å»ºæ›´å…·å“åº”æ€§å’Œç”¨æˆ·å‹å¥½çš„åº”ç”¨ç¨‹åºã€‚\n\n### Remixçš„æœªæ¥å‰æ™¯\n\nRemixçš„çµæ´»æ€§å’Œé«˜æ•ˆæ€§ä¸ºOpenAIçš„æœªæ¥å¢žé•¿å’Œåˆ›æ–°å¥ å®šäº†åŸºç¡€ã€‚\n\néšç€Remixçš„ä¸æ–­å‘å±•ï¼ŒOpenAIå¯ä»¥åˆ©ç”¨å…¶å…ˆè¿›çš„åŠŸèƒ½åœ¨ç½‘é¡µå¼€å‘çš„ç«žäº‰çŽ¯å¢ƒä¸­ä¿æŒé¢†å…ˆã€‚\n\n### ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦\n\n**æ”¹å–„ç”¨æˆ·ä½“éªŒ**ï¼šç”¨æˆ·ä»Žæ›´å¿«çš„é¡µé¢åŠ è½½å’Œæ›´æµç•…çš„æµè§ˆä½“éªŒä¸­å—ç›Šã€‚\n\n**é«˜æ•ˆå¼€å‘**ï¼šRemix çš„åŠŸèƒ½ç®€åŒ–äº†å¼€å‘è¿‡ç¨‹ï¼Œä½¿ OpenAI èƒ½å¤Ÿæ›´å¿«é€Ÿåœ°è¿›è¡Œåˆ›æ–°ã€‚\n\n**å¯æ‰©å±•æ€§**ï¼šRemix çš„æž¶æž„æ”¯æŒæœªæ¥çš„å¢žå¼ºå’Œæ‰©å±•ï¼Œç¡®ä¿é•¿æœŸçš„å¯è¡Œæ€§ã€‚\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/the-rise-of-the-ai-agent-product-manager-and-ai-agent-engineer-0905f1d30cce","frontmatter":{"title":"äººå·¥æ™ºèƒ½ä»£ç†äº§å“ç»ç†å’Œäººå·¥æ™ºèƒ½ä»£ç†å·¥ç¨‹å¸ˆçš„å´›èµ·","meta_title":"äººå·¥æ™ºèƒ½ä»£ç†äº§å“ç»ç†å’Œäººå·¥æ™ºèƒ½ä»£ç†å·¥ç¨‹å¸ˆçš„å´›èµ·","description":"æƒ³è±¡ä¸€ä¸‹æœªæ¥ï¼Œç”Ÿæˆå¼äººå·¥æ™ºèƒ½ä¸ä»…èƒ½å“åº”æŸ¥è¯¢ï¼Œè¿˜èƒ½ä¸»åŠ¨è§£å†³å„ä¸ªæ–¹é¢çš„å¤æ‚é—®é¢˜â€¦â€¦","date":"2024-11-04T12:33:53.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*dlJ0a49_lRAPR1tTPs898w.png","categories":["Generative AI","Ethics","Technology"],"author":"Rifx.Online","tags":["Generative","Product","Manager","Engineer","Ethics"],"draft":false,"slug":"blog/the-rise-of-the-ai-agent-product-manager-and-ai-agent-engineer-0905f1d30cce"},"content":"\n\n\n\n\næƒ³è±¡ä¸€ä¸ªæœªæ¥ï¼Œç”Ÿæˆå¼AIä¸ä»…ä»…æ˜¯å“åº”æŸ¥è¯¢ï¼Œè€Œæ˜¯ä¸»åŠ¨è§£å†³å•†ä¸šå„ä¸ªæ–¹é¢çš„å¤æ‚é—®é¢˜ã€‚è¿™ä¸æ˜¯ç§‘å¹»å°è¯´ï¼Œè€Œæ˜¯ç”Ÿæˆå¼AIä»£ç†è¿…é€Ÿé€¼è¿‘çš„çŽ°å®žã€‚è¿™äº›ä»£ç†æœ‰æœ›å½»åº•æ”¹å˜å…¬å¸çš„è¿è¥ï¼Œå¹¶æ¿€å‘ä¸€æ³¢æ–°çš„åˆ›æ–°ï¼Œä»Žç®€åŒ–ä¾›åº”é“¾åˆ°ä¼˜åŒ–äº§å“å¼€å‘ï¼Œå†åˆ°è½¬å˜å®¢æˆ·äº’åŠ¨ã€‚\n\nåœ¨è¿‡åŽ»ä¸€å¹´å¤šçš„æ—¶é—´é‡Œï¼Œæˆ‘ä¸€ç›´åœ¨æž„å»ºç”Ÿæˆå¼AIåº”ç”¨å’Œä»£ç†ï¼Œäº²çœ¼è§è¯äº†è¿™äº›æŠ€æœ¯å¦‚ä½•æ·±åˆ»é‡å¡‘å•†ä¸šæµç¨‹ã€‚AIçš„æ½œåŠ›å·¨å¤§ï¼Œä»Žä»¥ç©ºå‰é«˜æ•ˆå¤„ç†å®¢æˆ·æŸ¥è¯¢çš„æ”¯æŒä»£ç†ï¼Œåˆ°æŽ¨åŠ¨å•†ä¸šè¿è¥å’Œå†³ç­–çš„è‡ªä¸»ä»£ç†ã€‚è¿™äº›ä»£ç†ä¸ä»…ä»…æ˜¯åœ¨æå‡çŽ°æœ‰æµç¨‹ï¼Œè€Œæ˜¯åœ¨å¯ç”¨æ–°çš„å·¥ä½œæ–¹å¼ã€‚\n\nä¾‹å¦‚ï¼Œæƒ³è±¡ä¸€ä¸ªä»£ç†ï¼Œå®ƒä¸ä»…ä»…æ˜¯å®‰æŽ’ä¼šè®®ï¼Œè€Œæ˜¯ç†è§£ä½ å·¥ä½œçš„èƒŒæ™¯ï¼Œå»ºè®®æœ€æœ‰å½±å“åŠ›çš„ä¸Žä¼šè€…ï¼Œå‡†å¤‡ç®€æŠ¥æ–‡ä»¶ï¼Œç”šè‡³æ ¹æ®æœ€è¿‘çš„å…¬å¸åŠ¨æ€æå‡ºè®®ç¨‹é¡¹ç›®ã€‚åˆæˆ–è€…è€ƒè™‘ä¸€ä¸ªåˆ¶é€ ä¸šä¸­çš„ä»£ç†ï¼Œå®ƒä¸ä»…ä»…æ˜¯ç›‘æŽ§ç”Ÿäº§çº¿ï¼Œè€Œæ˜¯é¢„æµ‹ç»´æŠ¤éœ€æ±‚ï¼Œå®žæ—¶ä¼˜åŒ–èµ„æºåˆ†é…ï¼Œå¹¶ä¸Žè®¾è®¡å›¢é˜Ÿåˆä½œï¼Œæ ¹æ®ç”Ÿäº§æ•°æ®å»ºè®®äº§å“æ”¹è¿›ã€‚\n\nè¿™ç§AIé©±åŠ¨çš„è½¬åž‹æ­£åœ¨åˆ›é€ å¯¹ä¸¤ä¸ªå…³é”®è§’è‰²çš„éœ€æ±‚ï¼š**AIä»£ç†äº§å“ç»ç†**å’Œ**AIä»£ç†å·¥ç¨‹å¸ˆ**ã€‚è¿™äº›ä¸“ä¸šäººå‘˜ä¸ä»…æ˜¯æˆ‘ä»¬AIå¢žå¼ºæœªæ¥çš„æž¶æž„å¸ˆå’Œå»ºè®¾è€…ï¼Œè€Œä¸”æ˜¯ä¸€ä¸ªåä½œå›¢é˜Ÿçš„ä¸å¯æˆ–ç¼ºçš„éƒ¨åˆ†ï¼Œå·¥ä½œåœ¨å•†ä¸šæˆ˜ç•¥ä¸Žå‰æ²¿æŠ€æœ¯çš„äº¤æ±‡ç‚¹ä¸Šã€‚\n\n## å¼•å…¥æ–°è§’è‰²\n\n**AI Agent Product Manager** æ˜¯ä¸€ä½å…·æœ‰è¿œè§çš„äººï¼Œèƒ½å¤Ÿè¯†åˆ«ä»£ç†åˆ›é€ ä»·å€¼çš„æœºä¼šï¼Œè®¾è®¡å…¶èƒ½åŠ›ï¼Œå¹¶ç¡®ä¿å®ƒä»¬ä¸Žä¸šåŠ¡ç›®æ ‡å’Œç”¨æˆ·éœ€æ±‚ä¿æŒä¸€è‡´ã€‚ä»–ä»¬åœ¨å•†ä¸šä¸–ç•Œå’ŒAIå¯èƒ½æ€§ä¹‹é—´å……å½“ç¿»è¯‘ï¼Œåè°ƒAIåˆ›æ–°ã€‚\n\n**AI Agent Engineer** åˆ™æ˜¯å°†è¿™äº›ä»£ç†å˜ä¸ºçŽ°å®žçš„æŠ€æœ¯é«˜æ‰‹ã€‚ä»–ä»¬è®¾è®¡ç¨³å¥çš„æž¶æž„ï¼Œåˆ›å»ºå¤æ‚çš„æç¤ºï¼Œå¹¶é€šè¿‡ä¸Žå„ç§ç³»ç»Ÿå’Œæ•°æ®æºçš„æ— ç¼é›†æˆï¼Œç¡®ä¿ä»£ç†åŸºäºŽå…¬å¸æ•°æ®å’Œæµç¨‹ã€‚\n\nç”±äºŽæˆ‘ä»¬ä»å¤„äºŽè¿™ä¸€æŠ€æœ¯å‘¨æœŸçš„æ—©æœŸé˜¶æ®µï¼Œè¿™äº›ä¸“ä¸šäººå‘˜é€šå¸¸åœ¨ä¸“é—¨çš„AIå’¨è¯¢å…¬å¸æˆ–å¼€å‘ä»£ç†æž„å»ºäº§å“çš„å…¬å¸ï¼ˆå¦‚Salesforceï¼‰å·¥ä½œã€‚è¿™ä½¿ä»–ä»¬èƒ½å¤Ÿå°†æœ€ä½³å®žè·µå’Œè¡Œä¸šåˆ›æ–°å¸¦å…¥æ¯ä¸€ä¸ªæ–°é¡¹ç›®ã€‚\n\n## AIä»£ç†äº§å“ç»ç†ï¼šå¼•é¢†AIåˆ›æ–°\n\nä½œä¸ºä¸€åä»£ç†äº§å“ç»ç†ï¼Œæ‚¨å¯èƒ½ä¼šåœ¨ä¸åŒçš„ç”¨ä¾‹ä¸Šå·¥ä½œï¼Œæ¯”å¦‚ä¸€ä¸ªæœˆæ‹…ä»»é”€å”®ä»£ç†ï¼Œä¸‹ä¸ªæœˆæ‹…ä»»äººåŠ›èµ„æºä»£ç†ã€‚è®©æˆ‘ä»¬æ·±å…¥äº†è§£æ‚¨çš„è§’è‰²å¯èƒ½æ˜¯ä»€ä¹ˆæ ·çš„ï¼š\n\nä½œä¸ºä¸€åä»£ç†äº§å“ç»ç†ï¼Œå‡è®¾æ‚¨è´Ÿè´£ä¸ºä¸€å®¶è·¨å›½åˆ¶é€ å…¬å¸å¼€å‘ä¸€ä¸ªä»£ç†ã€‚æ‚¨çš„ç¬¬ä¸€æ­¥æ˜¯ä¸Žæ¥è‡ªå„ä¸ªéƒ¨é—¨çš„é«˜ç®¡é¢†å¯¼ä¸€ç³»åˆ—ç ”è®¨ä¼šâ€”â€”è¿è¥ã€è®¾è®¡ã€é”€å”®å’Œå®¢æˆ·æœåŠ¡ã€‚æ‚¨ä¸ä»…ä»…åœ¨å¯»æ‰¾æ¸è¿›å¼çš„æ”¹è¿›ï¼›æ‚¨åœ¨å¯»æ‰¾å˜é©æ€§çš„æœºä¼šï¼Œè€Œæ‚¨é€šè¿‡ä¿ƒè¿›ç»„ç»‡å†…éƒ¨çš„åˆä½œä¸Žç†è§£æ¥å®žçŽ°è¿™ä¸€ç›®æ ‡ã€‚\n\né€šè¿‡è¿™äº›è®¨è®ºï¼Œæ‚¨è¯†åˆ«å‡ºä¸€ä¸ªé¢ è¦†æ€§çš„å¯èƒ½æ€§ï¼šä¸€ä¸ªå¯ä»¥è¿žæŽ¥å®¢æˆ·åé¦ˆã€äº§å“è®¾è®¡å’Œåˆ¶é€ æµç¨‹çš„ä»£ç†ã€‚è¿™ä¸ªä»£ç†å°†åˆ†æžå®¢æˆ·è¯„è®ºå’Œæ”¯æŒå·¥å•ï¼Œè¯†åˆ«æµè¡Œçš„é—®é¢˜æˆ–æœŸæœ›çš„åŠŸèƒ½ï¼Œå¹¶è‡ªåŠ¨ç”Ÿæˆè®¾è®¡ä¿®æ”¹å»ºè®®ã€‚ç„¶åŽï¼Œå®ƒå°†æ¨¡æ‹Ÿè¿™äº›å˜åŒ–å¯¹åˆ¶é€ æµç¨‹å’Œæˆæœ¬çš„å½±å“ã€‚\n\nä½œä¸ºä»£ç†çš„äº§å“ç»ç†ï¼Œæ‚¨ä¸»è¦çš„è´£ä»»ä¹‹ä¸€å°†æ˜¯ç»˜åˆ¶ä»£ç†çš„æ—…ç¨‹ã€‚è¿™æ¶‰åŠå®šä¹‰ä»Žåˆå§‹äº’åŠ¨åˆ°æœ€ç»ˆç»“æžœçš„æ¯ä¸€æ­¥ï¼Œç¡®ä¿ä¸€åˆ‡ä¸Žä¸šåŠ¡ç›®æ ‡ä¸€è‡´ã€‚æ‚¨éœ€è¦è¯†åˆ«ä»£ç†å°†è¦è¿›è¡Œçš„å…³é”®äº’åŠ¨ï¼Œäº†è§£è¿™äº›äº’åŠ¨çš„èƒŒæ™¯ï¼Œå¹¶ç¡®å®šæ¯ä¸ªæ—…ç¨‹åº”è¯¥å®žçŽ°çš„ç›®æ ‡ã€‚æ‚¨è¿˜éœ€è¦è€ƒè™‘ä¸€äº›å…³é”®é—®é¢˜ï¼Œä¾‹å¦‚ï¼šä»£ç†å°†å¦‚ä½•ä¼˜å…ˆè€ƒè™‘å®¢æˆ·åé¦ˆï¼Ÿå®ƒå¦‚ä½•æœ‰æ•ˆåœ°å‘å·¥ç¨‹å›¢é˜Ÿæå‡ºè®¾è®¡å»ºè®®ï¼Ÿå½“AIå½±å“äº§å“å†³ç­–æ—¶ï¼Œå¿…é¡»è§£å†³å“ªäº›ä¼¦ç†è€ƒè™‘ï¼Ÿ\n\næ‚¨å°†ä¸Žåˆ©ç›Šç›¸å…³è€…å¯†åˆ‡åˆä½œï¼Œä»¥å®šä¹‰æˆåŠŸæŒ‡æ ‡ã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯èƒ½å†³å®šä»£ç†çš„ç›®æ ‡æ˜¯å°†è¯†åˆ«äº§å“é—®é¢˜åˆ°å®žæ–½ä¿®å¤çš„æ—¶é—´ç¼©çŸ­50%ï¼ŒåŒæ—¶æé«˜å®¢æˆ·æ»¡æ„åº¦è¯„åˆ†ã€‚\n\néšç€é¡¹ç›®çš„è¿›å±•ï¼Œæ‚¨ç¡®ä¿ä»£ç†æä¾›çœŸå®žçš„å•†ä¸šä»·å€¼ã€‚æ‚¨å¯èƒ½ä¼šå®¡æŸ¥AIä¸Žè®¾è®¡å›¢é˜Ÿä¹‹é—´çš„æ¨¡æ‹Ÿå’Œå®žé™…å¯¹è¯ï¼Œè°ƒæ•´ä»£ç†çš„æ²Ÿé€šé£Žæ ¼ï¼Œä»¥æ›´å¥½åœ°ä¸Žå·¥ç¨‹å¸ˆäº§ç”Ÿå…±é¸£ã€‚æˆ–è€…ï¼Œæ‚¨å¯èƒ½ä¼šä»”ç»†ç ”ç©¶ä»£ç†çš„å»ºè®®å¦‚ä½•å½±å“äº§å“è´¨é‡å’Œå®¢æˆ·æ»¡æ„åº¦çš„æ•°æ®ï¼Œå¯»æ‰¾è¿›ä¸€æ­¥æé«˜å…¶æ€§èƒ½çš„æ–¹æ³•ã€‚\n\nåœ¨æ•´ä¸ªè¿‡ç¨‹ä¸­ï¼Œæ‚¨ä¸ä»…åœ¨è€ƒè™‘ä»£ç†å½“å‰çš„èƒ½åŠ›ï¼Œè¿˜åœ¨è€ƒè™‘å…¶æœªæ¥çš„æ½œåŠ›ã€‚è¿™ä¸ªä»£ç†å¦‚ä½•æ¼”å˜ä»¥å“åº”å®¢æˆ·åé¦ˆå¹¶é¢„æµ‹æœªæ¥å¸‚åœºè¶‹åŠ¿ï¼Ÿå®ƒæ˜¯å¦æœ‰å¯èƒ½åœ¨æœªæ¥å‚ä¸Žäº§å“å›¢é˜Ÿçš„å¤´è„‘é£Žæš´ä¼šè®®ï¼Œæä¾›æ•°æ®é©±åŠ¨çš„è§è§£ä»¥æŽ¨åŠ¨åˆ›æ–°ï¼Ÿ\n\næ‚¨çš„ä»£ç†äº§å“ç»ç†è§’è‰²ä½¿æ‚¨å¤„äºŽå•†ä¸šè½¬åž‹çš„æœ€å‰æ²¿ã€‚æ‚¨ä¸ä»…ä»…æ˜¯åœ¨å®žæ–½ä¸€ä¸ªæ–°å·¥å…·ï¼›æ‚¨æ­£åœ¨é‡å¡‘æ•´ä¸ªç»„ç»‡åœ¨AIæ—¶ä»£çš„æ€ç»´ã€åˆ›æ–°å’Œè¿è¥æ–¹å¼ã€‚\n\n## AIä»£ç†å·¥ç¨‹å¸ˆï¼šæ‰“é€ æ™ºèƒ½å¯é çš„ç³»ç»Ÿ\n\nçŽ°åœ¨ï¼Œè®©æˆ‘ä»¬è½¬å˜æ€è·¯ï¼Œè¿›å…¥åŒä¸€é¡¹ç›®ä¸­çš„ä»£ç†å·¥ç¨‹å¸ˆè§’è‰²ï¼š\n\næ‚¨çš„æŒ‘æˆ˜æ˜¯åˆ›å»ºä¸€ä¸ªèƒ½å¤Ÿç†è§£å®¢æˆ·åé¦ˆã€å°†å…¶è½¬åŒ–ä¸ºå¯è¡Œè®¾è®¡è§è§£å¹¶ä¸Žåˆ¶é€ ç³»ç»ŸæŽ¥å£çš„ä»£ç†ã€‚è¿™å¹¶éžæ˜“äº‹â€”â€”è¿™éœ€è¦å¯¹å¤§åž‹è¯­è¨€æ¨¡åž‹ã€å¤æ‚çš„æç¤ºå·¥ç¨‹å’Œå¼ºå¤§çš„ç³»ç»Ÿé›†æˆæœ‰æ·±åˆ»çš„ç†è§£ã€‚\n\næ‚¨é¦–å…ˆé€‰æ‹©ä¸€ä¸ªåˆé€‚çš„å¤§åž‹è¯­è¨€æ¨¡åž‹ä½œä¸ºä»£ç†çš„åŸºç¡€ã€‚ç„¶è€Œï¼Œæ‚¨çœŸæ­£çš„å·¥ä½œåœ¨äºŽè®¾è®¡ä¸€ä¸ªå…¨é¢çš„ä»£ç†æž¶æž„ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨è®¸å¤šå¯¹è¯æ—…ç¨‹ä¸­å¯é åœ°æ‰§è¡Œã€‚\n\nä½œä¸ºä»£ç†å·¥ç¨‹å¸ˆï¼Œæ‚¨ä¸»è¦å…³æ³¨ä¹‹ä¸€æ˜¯åˆ›å»ºå’Œå®Œå–„ä»£ç†çš„æç¤ºç»“æž„ã€‚æ‚¨è®¾è®¡å¤æ‚çš„æç¤ºï¼Œæœ‰æ•ˆå¼•å¯¼æ¨¡åž‹çš„è¡Œä¸ºï¼Œç¡®ä¿å®ƒåœ¨å„ç§åœºæ™¯ä¸­å§‹ç»ˆæä¾›ç›¸å…³ä¸”å‡†ç¡®çš„å“åº”ã€‚è¿™å¯èƒ½æ¶‰åŠå¼€å‘ä¸€ä¸ªå±‚æ¬¡åŒ–çš„æç¤ºç³»ç»Ÿï¼Œèƒ½å¤Ÿå¤„ç†ä»Žç›‘ç£å¤šä¸ªä»£ç†åˆ°å¯¼èˆªå„ç§æ—…ç¨‹çš„æ‰€æœ‰äº‹åŠ¡ã€‚\n\næ‚¨å°†èŠ±è´¹å¤§é‡æ—¶é—´è¯„ä¼°ä»£ç†çš„è¡Œä¸ºå’Œè¾“å‡ºï¼Œä¼˜åŒ–æç¤ºå’Œæµç¨‹ï¼Œå¹¶å‘å¸ƒæ–°ç‰ˆæœ¬ã€‚æ‚¨ç”šè‡³å¯èƒ½è®¾è®¡å’Œå®žæ–½ä¸€ä¸ªä¸¥æ ¼çš„æµ‹è¯•æ¡†æž¶ï¼Œæ¨¡æ‹Ÿæ•°åƒç§æ½œåœ¨å¯¹è¯è½¨è¿¹ã€‚æ‚¨çš„ç›®æ ‡æ˜¯ç¡®ä¿ä»£ç†çš„å“åº”æ˜¯ç¡®å®šæ€§çš„ï¼Œå¹¶ä¸Žä»»ä½•ç»™å®šè¾“å…¥çš„æœŸæœ›ç»“æžœä¿æŒä¸€è‡´ã€‚\n\nä¾‹å¦‚ï¼Œæ‚¨å¯èƒ½ä¼šåˆ›å»ºä¸€å¥—æµ‹è¯•ç”¨ä¾‹ï¼Œæ¶µç›–å„ç§ç±»åž‹çš„å®¢æˆ·åé¦ˆï¼Œä»Žç®€å•çš„äº§å“é—®é¢˜åˆ°å¤æ‚çš„åŠŸèƒ½è¯·æ±‚ã€‚ç„¶åŽï¼Œæ‚¨ç³»ç»Ÿåœ°å¤„ç†è¿™äº›æ¡ˆä¾‹ï¼Œåˆ†æžä»£ç†çš„å“åº”ï¼Œå¹¶è¿­ä»£æç¤ºç»“æž„å’Œå†³ç­–é€»è¾‘ï¼Œä»¥æé«˜æ€§èƒ½ã€‚\n\nå½“æ‚¨é‡åˆ°ä»£ç†è¡Œä¸ºä¸ä¸€è‡´æˆ–ä¸ç†æƒ³çš„è¾¹ç¼˜æ¡ˆä¾‹æ—¶ï¼Œæ‚¨å¹¶ä¸æ˜¯ç®€å•åœ°è°ƒæ•´æç¤ºã€‚ç›¸åï¼Œæ‚¨æ·±å…¥ç ”ç©¶ä»£ç†çš„å†³ç­–è¿‡ç¨‹ï¼Œç³»ç»Ÿæ€§åœ°è°ƒæ•´åŸºç¡€é€»è¾‘å’Œæç¤ºç»“æž„ï¼Œä»¥è§£å†³è¿™äº›é—®é¢˜ã€‚\n\né›†æˆä»ç„¶æ˜¯æ‚¨è§’è‰²çš„å…³é”®éƒ¨åˆ†ã€‚æ‚¨æ­£åœ¨è®¾è®¡APIï¼Œä½¿ä»£ç†èƒ½å¤Ÿä»Žå®¢æˆ·æ”¯æŒæ•°æ®åº“ä¸­æå–æ•°æ®ï¼Œè®¿é—®äº§å“è®¾è®¡æ–‡ä»¶ï¼Œå¹¶å°†æ•°æ®è¾“å…¥åˆ°åˆ¶é€ è§„åˆ’ç³»ç»Ÿä¸­ã€‚ä½†ä¸ä»…ä»…æ˜¯è¿žæŽ¥ç³»ç»Ÿï¼Œæ‚¨è¿˜ä¸“æ³¨äºŽç¡®ä¿ä»£ç†èƒ½å¤ŸåŸºäºŽè¿™äº›é›†æˆæ•°æ®åšå‡ºæ™ºèƒ½å†³ç­–ã€‚\n\nä¼¦ç†å’Œå®‰å…¨ä»ç„¶æ˜¯å…³é”®é—®é¢˜ã€‚æ‚¨å®žæ–½äº†å¼ºæœ‰åŠ›çš„ä¿éšœå’Œç›‘ç£æœºåˆ¶ï¼Œä»¥ç¡®ä¿AIä¸ä¼šå»ºè®®å¯èƒ½å±åŠäº§å“å®‰å…¨çš„è®¾è®¡å˜æ›´ã€‚æ‚¨è¿˜æž„å»ºäº†è§£é‡Šæ€§åŠŸèƒ½ï¼Œä»¥ä¾¿AIå§‹ç»ˆèƒ½å¤Ÿå±•ç¤ºå…¶ä»»ä½•å»ºè®®çš„æŽ¨ç†ï¼Œè¿™å¯¹äºŽä¸Žä½¿ç”¨ä»£ç†çš„å·¥ç¨‹å¸ˆå’Œè®¾è®¡å¸ˆå»ºç«‹ä¿¡ä»»è‡³å…³é‡è¦ã€‚\n\nä½œä¸ºä»£ç†å·¥ç¨‹å¸ˆï¼Œæ‚¨çš„è§’è‰²ä¸ä»…æ¶‰åŠåˆ›å»ºä¸€ä¸ªåŠŸèƒ½æ€§AIç³»ç»Ÿï¼Œè¿˜æ¶‰åŠæ‰“é€ ä¸€ä¸ªèƒ½å¤Ÿå¯é å’Œä¸€è‡´åœ°æŽ¨åŠ¨æ•´ä¸ªäº§å“å¼€å‘å’Œåˆ¶é€ è¿‡ç¨‹ä¸­çš„åˆ›æ–°å’Œæ•ˆçŽ‡çš„æ™ºèƒ½ä»£ç†ã€‚è¿™ä¸€å¤æ‚çš„æŒ‘æˆ˜ä½¿æ‚¨å¤„äºŽAIæŠ€æœ¯çš„æœ€å‰æ²¿ï¼Œå¡‘é€ ç€ä¼ä¸šåœ¨AIæ—¶ä»£çš„è¿è¥æœªæ¥ã€‚\n\n## ä¼¦ç†è€ƒé‡ä¸Žåä½œçš„åŠ›é‡\n\néšç€ä»£ç†åœ¨ä¼ä¸šä¸­å˜å¾—è¶Šæ¥è¶Šé‡è¦ï¼Œä»£ç†äº§å“ç»ç†å’Œä»£ç†å·¥ç¨‹å¸ˆçš„è§’è‰²å°†æ„ˆå‘é‡è¦ã€‚è¿™äº›è§’è‰²ä¸ä»…ä»…æ¶‰åŠæŠ€æœ¯èƒ½åŠ›æˆ–æˆ˜ç•¥æ´žå¯ŸåŠ›â€”â€”å®ƒä»¬è¿˜è¦æ±‚å¯¹ä¼¦ç†è€ƒé‡æœ‰æ·±åˆ»çš„æ‰¿è¯ºã€‚ç”±äºŽè¿™äº›ä»£ç†å½±å“ç€é‡è¦çš„å•†ä¸šå†³ç­–ï¼ŒèƒŒåŽçš„ä¸“ä¸šäººå£«å¿…é¡»ç¡®ä¿è¿™äº›ç³»ç»Ÿæ˜¯é€æ˜Žçš„ã€å…¬å¹³çš„ï¼Œå¹¶ä¸Žæ›´å¹¿æ³›çš„ç¤¾ä¼šä»·å€¼è§‚ç›¸ä¸€è‡´ã€‚\n\nè¿™ä¸ªä»£ç†çš„æˆåŠŸåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºŽäº§å“ç»ç†å’Œå·¥ç¨‹å¸ˆä¹‹é—´çš„æ— ç¼åä½œã€‚ä½ ä»¬å°†å…±åŒè¿­ä»£ä»£ç†çš„èƒ½åŠ›ï¼ŒæŽ’é™¤é—®é¢˜ï¼Œå¹¶æŽ¨åŠ¨å¯èƒ½æ€§çš„è¾¹ç•Œã€‚\n\n## æ¯”è¾ƒè§’è‰²ï¼šä»£ç†äº§å“ç»ç†ä¸Žä»£ç†å·¥ç¨‹å¸ˆ\n\nä»¥ä¸‹æ˜¯ä¸€ä¸ªæ€»ç»“æ¯”è¾ƒè¡¨ï¼Œä»¥å¼ºè°ƒä»£ç†äº§å“ç»ç†ä¸Žä»£ç†å·¥ç¨‹å¸ˆä¹‹é—´çš„å·®å¼‚ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*T26nkBI-wID26X2NrE2SSQ.jpeg)\n\nå…³é”®è¦ç‚¹ï¼š\n\n* **ä»£ç†äº§å“ç»ç†ï¼š** è¯¥èŒä½ä¸“æ³¨äºŽä»£ç†çš„æˆ˜ç•¥å’Œä¸šåŠ¡æ–¹é¢ï¼Œç¡®ä¿å®ƒä»¬æä¾›ä»·å€¼å¹¶ä¸Žå…¬å¸ç›®æ ‡ä¿æŒä¸€è‡´ã€‚\n* **ä»£ç†å·¥ç¨‹å¸ˆï¼š** è¯¥èŒä½é›†ä¸­äºŽæŠ€æœ¯å®žçŽ°ï¼Œç¡®ä¿ä»£ç†å¯é è¿è¡Œå¹¶ä¸ŽçŽ°æœ‰ç³»ç»Ÿæ— ç¼é›†æˆã€‚\n\n## æœªæ¥å±žäºŽä½ ï¼šè¿ŽæŽ¥æŒ‘æˆ˜\n\néšç€äººå·¥æ™ºèƒ½çš„å½±å“åŠ›ä¸æ–­æ‰©å¤§ï¼ŒAgent Product Manager å’Œ Agent Engineer çš„è§’è‰²å°†å¤„äºŽè¿™åœºæŠ€æœ¯é©å‘½çš„æœ€å‰æ²¿ã€‚æ— è®ºä½ æ˜¯åœ¨ä¸º AI é©±åŠ¨çš„ä¸šåŠ¡è½¬åž‹å®šä¹‰æˆ˜ç•¥ï¼Œè¿˜æ˜¯åœ¨è®¾è®¡é©±åŠ¨æ™ºèƒ½ä»£ç†çš„å¤æ‚ç³»ç»Ÿï¼Œä½ éƒ½å°†åœ¨å¡‘é€ å•†ä¸šçš„æœªæ¥ã€‚\n\nè¿™äº›è§’è‰²éœ€è¦ç‹¬ç‰¹çš„æŠ€èƒ½ç»„åˆï¼šæˆ˜ç•¥æ€ç»´ã€æŠ€æœ¯ä¸“é•¿ã€åˆ›é€ åŠ›ï¼Œä»¥åŠå¯¹å•†ä¸šå’Œäººå·¥æ™ºèƒ½çš„æ·±åˆ»ç†è§£ã€‚å®ƒä»¬æä¾›äº†åœ¨å°–ç«¯æŠ€æœ¯ä¸Šå·¥ä½œçš„æœºä¼šï¼ŒåŒæ—¶æŽ¨åŠ¨å®žé™…çš„å•†ä¸šå½±å“ã€‚\n\né‚£ä¹ˆï¼Œæœªæ¥çš„ Agent Product Managers å’Œ Engineersï¼Œä½ ä»¬å‡†å¤‡å¥½è¿ŽæŽ¥æŒ‘æˆ˜äº†å—ï¼Ÿäººå·¥æ™ºèƒ½å¢žå¼ºçš„æœªæ¥æ­£ç­‰å¾…ç€ä½ ä»¬çš„ä¸“ä¸šçŸ¥è¯†å’Œæ„¿æ™¯ã€‚æ— è®ºä½ æ˜¯å¯¹äº§å“ç®¡ç†çš„æˆ˜ç•¥æ–¹é¢æ„Ÿå…´è¶£ï¼Œè¿˜æ˜¯å¯¹ä»£ç†å·¥ç¨‹çš„æŠ€æœ¯ç»†èŠ‚å……æ»¡çƒ­æƒ…ï¼Œåœ¨è¿™ä¸ªä»¤äººå…´å¥‹çš„æ–°é¢†åŸŸä¸­éƒ½æœ‰ä½ çš„ä¸€å¸­ä¹‹åœ°ã€‚é—®é¢˜ä¸åœ¨äºŽäººå·¥æ™ºèƒ½æ˜¯å¦ä¼šæ”¹å˜å•†ä¸šï¼Œè€Œåœ¨äºŽå¦‚ä½•æ”¹å˜â€”â€”è€Œä½ å¯èƒ½å°±æ˜¯å†³å®šè€…ã€‚\n\n"},{"lang":"zh","group":"blog","slug":"blog/top-25-generative-ai-terminologies-you-must-know-6a3bb0300988","frontmatter":{"title":"æ‚¨å¿…é¡»äº†è§£çš„ 25 ä¸ªé¡¶çº§ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æœ¯è¯­","meta_title":"æ‚¨å¿…é¡»äº†è§£çš„ 25 ä¸ªé¡¶çº§ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æœ¯è¯­","description":"æœ¬æ–‡ä»‹ç»äº†25ä¸ªç”Ÿæˆæ€§äººå·¥æ™ºèƒ½é¢†åŸŸçš„æ ¸å¿ƒæœ¯è¯­ï¼ŒåŒ…æ‹¬ç”Ÿæˆæ¨¡åž‹ã€Transformerã€GANã€è‡ªç¼–ç å™¨ç­‰ã€‚æ¯ä¸ªæœ¯è¯­éƒ½æä¾›äº†å®šä¹‰ã€ç¤ºä¾‹åŠç›¸å…³èµ„æºï¼Œä»¥å¸®åŠ©æŠ€æœ¯ä¸“ä¸šäººå£«å’Œå…¶ä»–é¢†åŸŸäººå£«æ·±å…¥ç†è§£ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½çš„å…³é”®æ¦‚å¿µã€‚è¿™äº›æœ¯è¯­çš„æŽŒæ¡å¯¹äºŽå‚ä¸Žäººå·¥æ™ºèƒ½é¡¹ç›®ã€å‡†å¤‡é¢è¯•ä»¥åŠè·Ÿè¸ªè¡Œä¸šåŠ¨æ€è‡³å…³é‡è¦ã€‚","date":"2024-11-14T03:29:09.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*swCl2YJj6wfAc9J3v6AqTg.png","categories":["Generative AI","Machine Learning","Data Science"],"author":"Rifx.Online","tags":["Generative","Transformers","GANs","Autoencoders","Zero-Shot"],"draft":false,"slug":"blog/top-25-generative-ai-terminologies-you-must-know-6a3bb0300988"},"content":"\n\n\n*æŽŒæ¡å…³é”®æ¦‚å¿µï¼Œä»¥æ¸…æ™°çš„è§£é‡Šã€å®žé™…åº”ç”¨å’Œæ·±å…¥çš„èµ„æºåœ¨ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½é¢†åŸŸè„±é¢–è€Œå‡º*\n\n\n\nç”Ÿæˆæ€§äººå·¥æ™ºèƒ½ç¡®å®žæ˜¯å„è¡Œä¸šä¸­çš„å…³é”®æŠ€æœ¯ï¼›å› æ­¤ï¼Œç†è§£ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæ¦‚å¿µå¯¹ä»»ä½•æŠ€æœ¯ä¸“ä¸šäººå£«åŠå…¶ä»–é¢†åŸŸçš„äººå£«æ¥è¯´éƒ½æ˜¯è‡³å…³é‡è¦çš„ã€‚ä»¥ä¸‹ç»¼åˆæŒ‡å—æ¶µç›–äº†25ä¸ªå¿…é¡»äº†è§£çš„ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½æœ¯è¯­ï¼Œæä¾›æ¸…æ™°çš„å®šä¹‰ã€å®žé™…çš„ä¾‹å­å’Œå…¶ä»–èµ„æºï¼Œä»¥åŠ æ·±æ‚¨çš„çŸ¥è¯†ã€‚æ— è®ºæ˜¯ä¸ºé¢è¯•åšå‡†å¤‡ã€å‚ä¸Žäººå·¥æ™ºèƒ½é¡¹ç›®ï¼Œè¿˜æ˜¯è·Ÿä¸Šè¿™ä¸ªå¿«é€Ÿå˜åŒ–é¢†åŸŸçš„åŠ¨æ€ï¼ŒæŽŒæ¡è¿™äº›æœ¯è¯­éƒ½å°†ä¸ºæ‚¨åœ¨ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½é¢†åŸŸæ‰“ä¸‹åšå®žçš„åŸºç¡€ã€‚\n\n## 1\\. ç”Ÿæˆæ¨¡åž‹\n\n* **å®šä¹‰**ï¼šä¸€ç§ä»Žå­¦ä¹ åˆ°çš„æ¨¡å¼ä¸­ç”Ÿæˆæ–°æ•°æ®ç‚¹çš„AIæ¨¡åž‹ã€‚\n* **ç¤ºä¾‹**ï¼šç”Ÿæˆé¢„è®­ç»ƒå˜æ¢å™¨ï¼ˆGPTï¼‰æ ¹æ®è¾“å…¥æç¤ºç”Ÿæˆç±»äººæ–‡æœ¬ã€‚\n* **äº†è§£æ›´å¤š**ï¼š[ç”Ÿæˆæ¨¡åž‹ç®€ä»‹](https://proxy.rifx.online/https://www.datacamp.com/blog/what-is-a-generative-model)\n\n## 2\\. Transformer\n\n* **å®šä¹‰**ï¼šä¸€ç§ç¥žç»ç½‘ç»œæž¶æž„ï¼Œåˆ©ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶å¤„ç†å’Œç”Ÿæˆåºåˆ—ï¼Œä¾‹å¦‚æ–‡æœ¬æˆ–å›¾åƒã€‚\n* **ç¤ºä¾‹**ï¼šBERT æ˜¯ä¸€ç§ç”¨äºŽé—®ç­”å’Œæ–‡æœ¬åˆ†ç±»ç­‰ä»»åŠ¡çš„ Transformer æ¨¡åž‹ã€‚\n* **äº†è§£æ›´å¤š**ï¼š[ç†è§£ Transformers](https://proxy.rifx.online/https://www.turing.com/kb/brief-introduction-to-transformers-and-their-power)\n\n## 3\\. æ½œåœ¨ç©ºé—´\n\n* **å®šä¹‰**ï¼šä¸€ä¸ªå¤šç»´ç©ºé—´ï¼Œç”Ÿæˆæ¨¡åž‹åœ¨å…¶ä¸­æ˜ å°„æ•°æ®ï¼Œä½¿å…¶èƒ½å¤Ÿå­¦ä¹ å’Œç”Ÿæˆå˜ä½“ã€‚\n* **ç¤ºä¾‹**ï¼šåœ¨å›¾åƒç”Ÿæˆä¸­ï¼Œç›¸ä¼¼çš„å›¾åƒåœ¨æ½œåœ¨ç©ºé—´ä¸­å½¼æ­¤é è¿‘ã€‚\n* **äº†è§£æ›´å¤š**ï¼š[æŽ¢ç´¢äººå·¥æ™ºèƒ½ä¸­çš„æ½œåœ¨ç©ºé—´](https://proxy.rifx.online/https://www.perplexity.ai/page/latent-space-101-what-it-is-an-mwXuxYfzS_.J4e_uFvOskg)\n\n## 4\\. GANï¼ˆç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼‰\n\n* **å®šä¹‰**ï¼šä¸€ç§äººå·¥æ™ºèƒ½ï¼Œåˆ©ç”¨ä¸¤ä¸ªç¥žç»ç½‘ç»œâ€”â€”ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨â€”â€”ç›¸äº’å¯¹æŠ—ä»¥ç”Ÿæˆé€¼çœŸçš„æ•°æ®ã€‚\n* **ç¤ºä¾‹**ï¼šGANç”Ÿæˆçœ‹èµ·æ¥é€¼çœŸçš„é¢å­”ï¼Œä½†è¿™äº›é¢å­”å¹¶ä¸å±žäºŽçœŸå®žçš„äººã€‚\n* **äº†è§£æ›´å¤š**ï¼š[ä»€ä¹ˆæ˜¯GANï¼Œå®ƒä»¬æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ](https://proxy.rifx.online/https://aws.amazon.com/what-is/gan/#:~:text=A%20generative%20adversarial%20network%20(GAN,from%20a%20database%20of%20songs.)\n\n## 5\\. è‡ªç¼–ç å™¨\n\n* **å®šä¹‰**ï¼šä¸€ç§ç¥žç»ç½‘ç»œï¼Œå­¦ä¹ åŽ‹ç¼©å’Œé‡æž„æ•°æ®ï¼Œé€šå¸¸ç”¨äºŽé™ç»´å’ŒåŽ»å™ªç­‰ä»»åŠ¡ã€‚\n* **ç¤ºä¾‹**ï¼šè‡ªç¼–ç å™¨ç”¨äºŽä»ŽæŸåçš„å›¾åƒä¸­åŽ»é™¤å™ªå£°ã€‚\n* **äº†è§£æ›´å¤š**ï¼š[è‡ªç¼–ç å™¨ä»‹ç»](https://proxy.rifx.online/https://towardsdatascience.com/introduction-to-autoencoders-7a47cf4ef14b)\n\n## 6\\. æ‰©æ•£æ¨¡åž‹\n\n* **å®šä¹‰**: å­¦ä¹ é€†è½¬å™ªå£°æ·»åŠ è¿‡ç¨‹çš„æ¨¡åž‹ï¼Œä»¥ä»Žå™ªå£°ä¸­ç”Ÿæˆè¯¦ç»†ä¸”ä¸€è‡´çš„æ•°æ®ã€‚\n* **ç¤ºä¾‹**: æ‰©æ•£æ¨¡åž‹åœ¨ DALL\\-E 2 ä¸­ç”¨äºŽä»Žéšæœºå™ªå£°ç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚\n* **äº†è§£æ›´å¤š**: [ç†è§£æ‰©æ•£æ¨¡åž‹](https://proxy.rifx.online/https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/)\n\n## 7\\. æç¤ºå·¥ç¨‹\n\n* **å®šä¹‰**ï¼šç²¾å¿ƒè®¾è®¡è¾“å…¥æç¤ºçš„è¿‡ç¨‹ï¼Œä»¥ä¼˜åŒ–æ¨¡åž‹ç”Ÿæˆçš„è¾“å‡ºã€‚\n* **ç¤ºä¾‹**ï¼šä¿®æ”¹GPT\\-4ä¸­çš„è¾“å…¥æç¤ºä»¥ç”Ÿæˆæ›´ç®€æ´çš„æ‘˜è¦ã€‚\n* **äº†è§£æ›´å¤š**ï¼š[æç¤ºå·¥ç¨‹æŒ‡å—](https://proxy.rifx.online/https://www.datacamp.com/tutorial/a-beginners-guide-to-chatgpt-prompt-engineering)\n\n## 8\\. é›¶æ ·æœ¬å­¦ä¹ \n\n* **å®šä¹‰**ï¼šæ¨¡åž‹åœ¨æœªæ˜Žç¡®è®­ç»ƒçš„ä»»åŠ¡ä¸Šæ‰§è¡Œä»»åŠ¡çš„èƒ½åŠ›ï¼Œé€šè¿‡åˆ©ç”¨å…¶ä»–ä»»åŠ¡çš„çŸ¥è¯†ã€‚\n* **ç¤ºä¾‹**ï¼šGPT\\-3 å¯ä»¥åœ¨æ²¡æœ‰é’ˆå¯¹ç¿»è¯‘æ•°æ®é›†è¿›è¡Œç‰¹åˆ«è®­ç»ƒçš„æƒ…å†µä¸‹æ‰§è¡Œç¿»è¯‘ã€‚\n* **äº†è§£æ›´å¤š**ï¼š[ä»€ä¹ˆæ˜¯é›¶æ ·æœ¬å­¦ä¹ ï¼Ÿ](https://proxy.rifx.online/https://www.ibm.com/topics/zero-shot-learning)\n\n## 9\\. å°‘æ ·æœ¬å­¦ä¹ \n\n* **å®šä¹‰**ï¼šæ¨¡åž‹åœ¨ä»…æœ‰å°‘é‡ç¤ºä¾‹çš„æƒ…å†µä¸‹å­¦ä¹ ä»»åŠ¡çš„èƒ½åŠ›ï¼Œæœ€å¤§é™åº¦åœ°å‡å°‘å¯¹å¤§é‡è®­ç»ƒæ•°æ®çš„éœ€æ±‚ã€‚\n* **ç¤ºä¾‹**ï¼šGPT\\-3 å¯ä»¥é€šè¿‡æœ€å°‘çš„è¾“å…¥æ ·æœ¬è¿›è¡Œå¾®è°ƒï¼Œä»¥ç‰¹å®šé£Žæ ¼è¿›è¡Œå†™ä½œã€‚\n* **äº†è§£æ›´å¤š**ï¼š[å°‘æ ·æœ¬å­¦ä¹ è§£é‡Š](https://proxy.rifx.online/https://www.ibm.com/topics/few-shot-learning#:~:text=IBM-,What%20is%20few%2Dshot%20learning%3F,suitable%20training%20data%20is%20scarce.)\n\n## 10\\. å¼ºåŒ–å­¦ä¹ \n\n* **å®šä¹‰**ï¼šä¸€ç§å­¦ä¹ èŒƒå¼ï¼ŒAIä»£ç†é€šè¿‡ä¸ŽçŽ¯å¢ƒäº’åŠ¨æ¥å­¦ä¹ å†³ç­–ï¼Œä»¥æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ã€‚\n* **ç¤ºä¾‹**ï¼šAlphaGoä½¿ç”¨å¼ºåŒ–å­¦ä¹ é€šè¿‡ä¸Žè‡ªå·±å¯¹å¼ˆæ•°ç™¾ä¸‡å±€æ¥ç²¾é€šå›´æ£‹ã€‚\n* **äº†è§£æ›´å¤š**ï¼š[ç”Ÿæˆå¼AIçš„å¼ºåŒ–å­¦ä¹ ](https://proxy.rifx.online/https://dl.acm.org/doi/pdf/10.1613/jair.1.15278)\n\n## 11\\. å˜åˆ†è‡ªç¼–ç å™¨ (VAE)\n\n* **å®šä¹‰**ï¼šä¸€ç§è‡ªç¼–ç å™¨ï¼Œé€šè¿‡å¼•å…¥éšæœºæ€§æ¥å­¦ä¹ ç”Ÿæˆæ–°æ•°æ®ï¼Œä»Žè€Œå¯¹å…¶æ½œåœ¨ç©ºé—´è¡¨ç¤ºè¿›è¡Œå»ºæ¨¡ã€‚\n* **ç¤ºä¾‹**ï¼šVAE è¢«ç”¨äºŽç”Ÿæˆæ–°çš„äººè„¸ï¼Œå¹¶åœ¨ä¸åŒçš„é¢éƒ¨ç‰¹å¾ä¹‹é—´å¹³æ»‘è¿‡æ¸¡ã€‚\n* **äº†è§£æ›´å¤š**ï¼š[VAE åŠå…¶åº”ç”¨](https://proxy.rifx.online/https://www.datacamp.com/tutorial/variational-autoencoders)\n\n``` \n## ä»£ç å—å†…å®¹ä¿æŒä¸å˜\n```\n\n## 12\\. è‡ªç›‘ç£å­¦ä¹ \n\n* **å®šä¹‰**ï¼šä¸€ç§å­¦ä¹ æŠ€æœ¯ï¼Œæ¨¡åž‹ä»Žæ•°æ®ä¸­ç”Ÿæˆè‡ªå·±çš„æ ‡ç­¾ï¼Œä»Žè€Œå‡å°‘å¯¹æ ‡è®°æ•°æ®é›†çš„ä¾èµ–ã€‚\n* **ç¤ºä¾‹**ï¼šBERTé€šè¿‡åœ¨å¥å­ä¸­æŽ©ç›–å•è¯å¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é¢„æµ‹å®ƒä»¬æ¥ä½¿ç”¨è‡ªç›‘ç£å­¦ä¹ ã€‚\n* **äº†è§£æ›´å¤š**ï¼š[ä»€ä¹ˆæ˜¯è‡ªç›‘ç£å­¦ä¹ ï¼Ÿ](https://proxy.rifx.online/https://www.ibm.com/topics/self-supervised-learning)\n\n```\n## Code block example\ndef self_supervised_learning():\n    pass\n```\n\n## 13\\. åˆ†è¯\n\n* **å®šä¹‰**ï¼šå°†æ–‡æœ¬æ‹†åˆ†ä¸ºæ›´å°çš„å•å…ƒï¼Œå¦‚è¯æˆ–å­è¯ï¼Œä»¥ä¾¿æ¨¡åž‹æ›´å®¹æ˜“å¤„ç†çš„è¿‡ç¨‹ã€‚\n* **ç¤ºä¾‹**ï¼šæ–‡æœ¬è¾“å…¥åœ¨è¾“å…¥åˆ° GPT\\-4 è¿›è¡Œå¤„ç†ä¹‹å‰è¢«åˆ†è¯ä¸ºå•è¯ã€‚\n* **äº†è§£æ›´å¤š**ï¼š[è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„åˆ†è¯](https://proxy.rifx.online/https://www.datacamp.com/blog/what-is-tokenization)\n\n## 14\\. Beam Search\n\n* **å®šä¹‰**ï¼šä¸€ç§æœç´¢ç®—æ³•ï¼Œé€šè¿‡æ‰©å±•å¤šä¸ªæ½œåœ¨çš„æ ‡è®°åºåˆ—ï¼Œåœ¨è§£ç è¿‡ç¨‹ä¸­ç”Ÿæˆæœ€å¯èƒ½çš„åºåˆ—ã€‚\n* **ç¤ºä¾‹**ï¼šBeam search åœ¨æœºå™¨ç¿»è¯‘ä¸­ç”¨äºŽç”Ÿæˆè¿žè´¯çš„æ–‡æœ¬è¾“å‡ºã€‚\n* **äº†è§£æ›´å¤š**ï¼š[Beam Search Explained](https://proxy.rifx.online/https://www.width.ai/post/what-is-beam-search)\n\n## 15\\. è¿ç§»å­¦ä¹ \n\n* **å®šä¹‰**ï¼šåœ¨ä¸€ä¸ªä»»åŠ¡ä¸Šä½¿ç”¨é¢„è®­ç»ƒæ¨¡åž‹ï¼Œå¹¶å¯¹å…¶è¿›è¡Œå¾®è°ƒä»¥é€‚åº”å¦ä¸€ä¸ªä»»åŠ¡çš„è¿‡ç¨‹ï¼Œé€šå¸¸æ‰€éœ€çš„æ•°æ®æ›´å°‘ã€‚\n* **ç¤ºä¾‹**ï¼šåœ¨é€šç”¨è¯­è¨€ä»»åŠ¡ä¸Šé¢„è®­ç»ƒåŽï¼Œå¯¹æƒ…æ„Ÿåˆ†æžä»»åŠ¡è¿›è¡ŒBERTçš„å¾®è°ƒã€‚\n* **äº†è§£æ›´å¤š**ï¼š[ä»€ä¹ˆæ˜¯è¿ç§»å­¦ä¹ ï¼Ÿ](https://proxy.rifx.online/https://aws.amazon.com/what-is/transfer-learning/)\n\n## 16\\. è¯­è¨€æ¨¡åž‹\n\n* **å®šä¹‰**ï¼šä¸€ç§é¢„æµ‹è‡ªç„¶è¯­è¨€ä¸­è¯åºåˆ—æ¦‚çŽ‡çš„æ¨¡åž‹ï¼Œå¸®åŠ©ç”Ÿæˆæˆ–ç†è§£æ–‡æœ¬ã€‚\n* **ç¤ºä¾‹**ï¼šGPT\\-4 æ˜¯ä¸€ç§èƒ½å¤Ÿä¸ºå¹¿æ³›åº”ç”¨ç”Ÿæˆè¿žè´¯æ–‡æœ¬çš„è¯­è¨€æ¨¡åž‹ã€‚\n* **äº†è§£æ›´å¤š**ï¼š[è¯­è¨€æ¨¡åž‹ç®€ä»‹](https://proxy.rifx.online/https://developers.google.com/machine-learning/resources/intro-llms)\n\n## 17\\. äººå·¥æ™ºèƒ½ä¸­çš„åè§\n\n* **å®šä¹‰**ï¼šäººå·¥æ™ºèƒ½ç³»ç»Ÿç”±äºŽè®­ç»ƒæ•°æ®æˆ–ç®—æ³•çš„åè§ï¼Œå€¾å‘äºŽäº§ç”Ÿæœ‰åˆ©äºŽæˆ–æ­§è§†æŸäº›ç¾¤ä½“çš„ç»“æžœã€‚\n* **ç¤ºä¾‹**ï¼šåŸºäºŽæœ‰åè§åŽ†å²æ•°æ®è®­ç»ƒçš„äººå·¥æ™ºèƒ½æ‹›è˜ç³»ç»Ÿä¸­çš„æ€§åˆ«åè§ã€‚\n* **äº†è§£æ›´å¤š**ï¼š[ç†è§£äººå·¥æ™ºèƒ½ä¸­çš„åè§](https://proxy.rifx.online/https://www.ibm.com/topics/ai-bias)\n\n## 18\\. GPT (ç”Ÿæˆé¢„è®­ç»ƒå˜æ¢å™¨)\n\n* **å®šä¹‰**ï¼šä¸€ä¸ªå¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ŒåŸºäºŽå¯¹å¹¿æ³›æ–‡æœ¬è¯­æ–™åº“çš„é¢„è®­ç»ƒå’Œå¾®è°ƒç”Ÿæˆç±»äººæ–‡æœ¬ã€‚\n* **ç¤ºä¾‹**ï¼šGPT-4 ç”Ÿæˆè®ºæ–‡ã€æ•…äº‹å’Œå¯¹ç”¨æˆ·æŸ¥è¯¢çš„è¯¦ç»†å›žåº”ã€‚\n* **äº†è§£æ›´å¤š**ï¼š[GPT çš„å·¥ä½œåŽŸç†](https://proxy.rifx.online/https://tecknoworks.com/how-gpt-works-and-its-core-mechanics/)\n\n## 19\\. å›°æƒ‘åº¦\n\n* **å®šä¹‰**ï¼šä¸€ç§è¡¡é‡è¯­è¨€æ¨¡åž‹é¢„æµ‹ç»™å®šå•è¯åºåˆ—æ•ˆæžœçš„æŒ‡æ ‡ï¼Œå›°æƒ‘åº¦è¶Šä½Žè¡¨ç¤ºæ€§èƒ½è¶Šå¥½ã€‚\n* **ç¤ºä¾‹**ï¼šæ¯”è¾ƒ GPT\\-3 å’Œ GPT\\-4 çš„å›°æƒ‘åº¦ï¼Œä»¥è¯„ä¼°å®ƒä»¬çš„æ–‡æœ¬ç”Ÿæˆè´¨é‡ã€‚\n* **äº†è§£æ›´å¤š**ï¼š[è¯­è¨€æ¨¡åž‹ä¸­çš„å›°æƒ‘åº¦](https://proxy.rifx.online/https://huggingface.co/docs/transformers/en/perplexity)\n\n## 20\\. è‡ªç„¶è¯­è¨€å¤„ç† (NLP)\n\n* **å®šä¹‰**ï¼šä¸€ä¸ªä¸“æ³¨äºŽè®¡ç®—æœºä¸Žäººç±»é€šè¿‡è‡ªç„¶è¯­è¨€è¿›è¡Œäº¤äº’çš„äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œæ¶µç›–ç¿»è¯‘å’Œæƒ…æ„Ÿåˆ†æžç­‰ä»»åŠ¡ã€‚\n* **ç¤ºä¾‹**ï¼šNLPæ¨¡åž‹ç”¨äºŽå¯¹å®¢æˆ·è¯„ä»·è¿›è¡Œæƒ…æ„Ÿåˆ†æžã€‚\n* **äº†è§£æ›´å¤š**ï¼š[NLPç®€ä»‹](https://proxy.rifx.online/https://towardsdatascience.com/a-gentle-introduction-to-natural-language-processing-e716ed3c0863)\n\n## 21\\. ç¥žç»ç½‘ç»œ\n\n* **å®šä¹‰**ï¼šä¸€ç§å—äººè„‘ç¥žç»å…ƒç½‘ç»œå¯å‘çš„è®¡ç®—ç³»ç»Ÿï¼Œç”±å¤šä¸ªç›¸äº’è¿žæŽ¥çš„èŠ‚ç‚¹å±‚ç»„æˆï¼Œç”¨äºŽå›¾åƒè¯†åˆ«å’Œè¯­è¨€å¤„ç†ç­‰ä»»åŠ¡ã€‚\n* **ç¤ºä¾‹**ï¼šå·ç§¯ç¥žç»ç½‘ç»œï¼ˆCNNï¼‰ç”¨äºŽè¯†åˆ«å›¾åƒä¸­çš„ç‰©ä½“ã€‚\n* **äº†è§£æ›´å¤š**ï¼š[ä»€ä¹ˆæ˜¯ç¥žç»ç½‘ç»œï¼Ÿ](https://proxy.rifx.online/https://www.ibm.com/topics/neural-networks)\n\n## 22\\. è®­ç»ƒæ•°æ®\n\n* **å®šä¹‰**ï¼šç”¨äºŽè®­ç»ƒAIæ¨¡åž‹çš„æ•°æ®ï¼Œé€šè¿‡è®©å®ƒä»¬ä»Žç¤ºä¾‹ä¸­å­¦ä¹ ï¼Œæé«˜å®ƒä»¬è¯†åˆ«æ¨¡å¼å’Œè¿›è¡Œé¢„æµ‹çš„èƒ½åŠ›ã€‚\n* **ç¤ºä¾‹**ï¼šåƒImageNetè¿™æ ·çš„å¤§åž‹å›¾åƒæ•°æ®é›†ç”¨äºŽè®­ç»ƒAIæ¨¡åž‹è¿›è¡Œå›¾åƒåˆ†ç±»ä»»åŠ¡ã€‚\n* **äº†è§£æ›´å¤š**ï¼š[AIä¸­çš„è®­ç»ƒæ•°æ®](https://proxy.rifx.online/https://www.oracle.com/artificial-intelligence/ai-model-training/)\n\n## 23\\. æ³¨æ„åŠ›æœºåˆ¶\n\n* **å®šä¹‰**ï¼šç¥žç»ç½‘ç»œä¸­çš„ä¸€ç§æ–¹æ³•ï¼Œå¸®åŠ©æ¨¡åž‹å…³æ³¨è¾“å…¥åºåˆ—ä¸­æœ€ç›¸å…³çš„éƒ¨åˆ†ï¼Œä»Žè€Œæé«˜æœºå™¨ç¿»è¯‘å’Œæ–‡æœ¬ç”Ÿæˆç­‰ä»»åŠ¡çš„æ€§èƒ½ã€‚\n* **ç¤ºä¾‹**ï¼šæ³¨æ„åŠ›æœºåˆ¶å…è®¸æ¨¡åž‹åœ¨è¯­è¨€ç¿»è¯‘æ—¶å…³æ³¨å¥å­ä¸­çš„é‡è¦å•è¯ã€‚\n* **äº†è§£æ›´å¤š**ï¼š[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æœºåˆ¶ï¼Ÿ](https://proxy.rifx.online/https://machinelearningmastery.com/the-attention-mechanism-from-scratch/)\n\n## 24\\. çºªå…ƒ\n\n* **å®šä¹‰**ï¼šåœ¨æœºå™¨å­¦ä¹ æ¨¡åž‹è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œç»è¿‡æ•´ä¸ªè®­ç»ƒæ•°æ®é›†çš„ä¸€æ¬¡å®Œæ•´éåŽ†ã€‚\n* **ç¤ºä¾‹**ï¼šè®­ç»ƒç¥žç»ç½‘ç»œ10ä¸ªçºªå…ƒï¼Œä»¥ç¡®ä¿å…¶æ­£ç¡®å­¦ä¹ è€Œä¸å‘ç”Ÿè¿‡æ‹Ÿåˆã€‚\n* **äº†è§£æ›´å¤š**ï¼š[ç†è§£æœºå™¨å­¦ä¹ ä¸­çš„çºªå…ƒ](https://proxy.rifx.online/https://www.geeksforgeeks.org/epoch-in-machine-learning/)\n\n## 25\\. å¤šæ¨¡æ€äººå·¥æ™ºèƒ½\n\n* **å®šä¹‰**ï¼šèƒ½å¤ŸåŒæ—¶å¤„ç†å’Œç”Ÿæˆæ¥è‡ªå¤šç§æ¨¡æ€ï¼ˆä¾‹å¦‚æ–‡æœ¬ã€å›¾åƒå’ŒéŸ³é¢‘ï¼‰æ•°æ®çš„äººå·¥æ™ºèƒ½ã€‚\n* **ç¤ºä¾‹**ï¼šCLIP åŒæ—¶å¤„ç†å›¾åƒå’Œæ–‡æœ¬ï¼Œä»¥ç”Ÿæˆå›¾åƒçš„æ ‡é¢˜ã€‚\n* **äº†è§£æ›´å¤š**ï¼š[ä»€ä¹ˆæ˜¯å¤šæ¨¡æ€äººå·¥æ™ºèƒ½ï¼Ÿ](https://proxy.rifx.online/https://www.techtarget.com/searchenterpriseai/definition/multimodal-AI)\n\nè¯·è®°ä½ï¼ŒæŽŒæ¡ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ˜¯ä¸€ä¸ªé€æ­¥çš„è¿‡ç¨‹ã€‚åœ¨å­¦ä¹ è¿™äº›æ¦‚å¿µæ—¶ï¼Œç¡®ä¿é€šè¿‡æä¾›çš„èµ„æºæ·±å…¥æŽ¢ç´¢æ¯ä¸€ä¸ªæ¦‚å¿µï¼Œå‚ä¸Žè®¨è®ºï¼Œå¹¶å°è¯•å°†æ‰€å­¦åº”ç”¨äºŽæ‚¨çš„é¡¹ç›®ã€‚ä¸Žè¿™äº›èµ„æºå’Œå¯¹è¯çš„äº’åŠ¨å°†å¸®åŠ©æ‚¨ç†è§£æœ¯è¯­åŠå…¶åœ¨çŽ°å®žä¸–ç•Œä¸­çš„ä½¿ç”¨ã€‚\n\næ„Ÿè°¢æ‚¨çš„é˜…è¯»ï¼å¦‚æžœæ‚¨è§‰å¾—æœ¬æŒ‡å—å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ä¸Žå…¶ä»–å¯èƒ½å¸Œæœ›æå‡ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ç†è§£çš„äººåˆ†äº«ã€‚æˆ‘ä»¬å…±åŒå­¦ä¹ ï¼Œå¹¶å› æ­¤æ›´å¥½åœ°åº”ç”¨è¿™äº›æ¦‚å¿µã€‚\n\nå¦‚æžœæ‚¨æœ‰ä»»ä½•æƒ³æ³•ã€é—®é¢˜ï¼Œæˆ–ç”šè‡³è®¤ä¸ºå¯èƒ½æœ‰å¸®åŠ©çš„é¢å¤–èµ„æºå»ºè®®ï¼Œè¯·åœ¨ä¸‹é¢çš„è¯„è®ºåŒºç•™è¨€ã€‚\n\nç¥æ‚¨åœ¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„ä¸–ç•Œä¸­æŽ¢ç´¢æ„‰å¿«ï¼\n\n*é€šè¿‡ [linktr.ee](https://proxy.rifx.online/https://linktr.ee/tharunkumarreddypolu) ä¸Žæˆ‘è”ç³»ï¼Œäº†è§£æ›´å¤šä¿¡æ¯ï¼*\n\n## ç”¨ç®€å•è‹±è¯­ ðŸš€\n\n*æ„Ÿè°¢æ‚¨æˆä¸º [**ç”¨ç®€å•è‹±è¯­**](https://proxy.rifx.online/https://plainenglish.io/) ç¤¾åŒºçš„ä¸€éƒ¨åˆ†ï¼åœ¨æ‚¨ç¦»å¼€ä¹‹å‰ï¼š*\n\n* ä¸€å®šè¦ **ç‚¹èµž** å’Œ **å…³æ³¨** ä½œè€… ï¸ðŸ‘**ï¸ï¸**\n* å…³æ³¨æˆ‘ä»¬ï¼š [**X**](https://proxy.rifx.online/https://x.com/inPlainEngHQ) \\| [**LinkedIn**](https://proxy.rifx.online/https://www.linkedin.com/company/inplainenglish/) \\| [**YouTube**](https://proxy.rifx.online/https://www.youtube.com/channel/UCtipWUghju290NWcn8jhyAw) \\| [**Discord**](https://proxy.rifx.online/https://discord.gg/in-plain-english-709094664682340443) \\| [**Newsletter**](https://proxy.rifx.online/https://newsletter.plainenglish.io/) \\| [**Podcast**](https://proxy.rifx.online/https://open.spotify.com/show/7qxylRWKhvZwMz2WuEoua0)\n* [**åœ¨ Differ ä¸Šåˆ›å»ºä¸€ä¸ªå…è´¹çš„ AI é©±åŠ¨åšå®¢ã€‚**](https://proxy.rifx.online/https://differ.blog/)\n* æ›´å¤šå†…å®¹è¯·è®¿é—® [**PlainEnglish.io**](https://proxy.rifx.online/https://plainenglish.io/)\n\n"},{"lang":"zh","group":"blog","slug":"blog/top-5-ai-tools-for-ios-developers-5ee9f39558ac","frontmatter":{"title":"é¢å‘ iOS å¼€å‘äººå‘˜çš„ 5 å¤§äººå·¥æ™ºèƒ½å·¥å…·","meta_title":"é¢å‘ iOS å¼€å‘äººå‘˜çš„ 5 å¤§äººå·¥æ™ºèƒ½å·¥å…·","description":"æœ¬æ–‡ä»‹ç»äº†äº”å¤§AIå·¥å…·ï¼Œæ—¨åœ¨æå‡iOSå¼€å‘è€…çš„å·¥ä½œæ•ˆçŽ‡ã€‚é¦–å…ˆæ˜¯Cursor/VSCodeï¼Œé€šè¿‡GitHub Copilotå®žçŽ°å¿«é€Ÿç¼–ç å’Œæ™ºèƒ½é‡æž„ã€‚å…¶æ¬¡æ˜¯GitHub Copilotçš„Xcodeæ‰©å±•ï¼Œæä¾›AIè¾…åŠ©ç¼–è¾‘åŠŸèƒ½ã€‚ç¬¬ä¸‰æ˜¯Swift Assistï¼Œè™½ç„¶å°šæœªå®Œå…¨å¯ç”¨ï¼Œä½†æœ‰æ½œåŠ›ç”Ÿæˆä»£ç ã€‚æŽ¥ç€æ˜¯ChatGPTåŠå…¶è¡ç”Ÿå·¥å…·ï¼Œé€‚åˆå¿«é€Ÿè¿­ä»£ä»£ç ã€‚æœ€åŽæ˜¯Alex Sidebarå’ŒAIProxyï¼Œåˆ†åˆ«ä¸ºXcodeæä¾›æ‰©å±•åŠŸèƒ½å’Œå®‰å…¨é›†æˆAI APIã€‚æ•´ä½“ä¸Šï¼Œè¿™äº›å·¥å…·ä¸ºiOSå¼€å‘è€…æä¾›äº†é«˜æ•ˆçš„ç¼–ç ä½“éªŒã€‚","date":"2024-11-14T03:29:09.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*6Hs8174FgiwTv87e.jpg","categories":["Programming","Technology/Web","Generative AI"],"author":"Rifx.Online","tags":["Cursor","VSCode","GitHub","Copilot","Swift"],"draft":false,"slug":"blog/top-5-ai-tools-for-ios-developers-5ee9f39558ac"},"content":"\n### æé«˜å·¥ä½œæµç¨‹é€Ÿåº¦ä¸Žæ•ˆçŽ‡\n\n\n\nè™½ç„¶å…³äºŽäººå·¥æ™ºèƒ½çš„è®¨è®ºå¾ˆå¤šï¼Œä½†æˆ‘æƒ³è®©ä½ å›žå½’çŽ°å®žã€‚æ— è®ºä½ æ˜¯å¦å·²ç»åœ¨ä½¿ç”¨ AI è¾…åŠ©çš„ç¼–ç å·¥å…·ï¼Œæˆ–è€…è§‰å¾—è¿™ä¸€åˆ‡éƒ½æ˜¯æ— ç¨½ä¹‹è°ˆâ€¦â€¦è¿™ç¯‡æ ‡é¢˜å¸å¼•çœ¼çƒçš„æ–‡ç« å¯èƒ½é€‚åˆä½ ã€‚\n\nè™½ç„¶ä½ å¯èƒ½å·²ç»èƒ½æ‰¾åˆ°å¾ˆå¤šå…³äºŽå¦‚ä½•ä½¿ç”¨å„ç§å·¥å…·æ¥æé«˜ä½ çš„æŠ€èƒ½ã€æ•ˆçŽ‡å’Œå‡†ç¡®æ€§çš„æ–‡çŒ®ï¼Œä½†å¯¹äºŽæˆ‘ä»¬ iOS å¼€å‘è€…æ¥è¯´ï¼Œè¿™è¦å¤æ‚ä¸€äº›ã€‚å› ä¸ºæˆ‘ä»¬ä¾èµ– Xcode åŠå…¶å·¥å…·é“¾æ¥æž„å»ºæˆ‘ä»¬çš„åº”ç”¨ï¼Œæ‰€ä»¥æˆ‘ä»¬å¾ˆéš¾ä¸ä½¿ç”¨ Xcodeã€‚è€Œæˆ‘å°†åœ¨æŽ¥ä¸‹æ¥çš„æ®µè½ä¸­åˆ—å‡ºå’Œè§£é‡Šçš„å¹¶ä¸æ˜¯æ‰€æœ‰å·¥å…·éƒ½ä¸Žè·³è¿‡ Xcode æœ‰å…³ã€‚\n\n## 1\\. Cursor / VSCode\n\næ˜¾ç„¶ï¼Œè¿™æ˜¯åˆ—è¡¨çš„é¦–ä½ã€‚é™¤éžä½ ä¸€ç›´åœ¨çŸ³å¤´ä¸‹å†¬çœ ï¼Œå¦åˆ™ä½ å¯èƒ½å¬è¯´è¿‡VSCodeã€‚åœ¨Swifté¡¹ç›®ä¸­ä½¿ç”¨å®ƒå¹¶ä¸æ˜¯æ–°é²œäº‹ã€‚å†…ç½®äºŽVSCodeçš„GitHub Copilotå…è®¸ä½ ä»¥å…‰é€Ÿç¼–ç ï¼Œè€Œæ— éœ€è¿›è¡Œå¤ªå¤šè®¾ç½®ã€‚ä»–ä»¬æœ€è¿‘åœ¨VSCodeä¸­é›†æˆäº†æ›´å¤šçš„CopilotåŠŸèƒ½ï¼Œè¶Šæ¥è¶ŠæŽ¥è¿‘Cursorã€‚é™¤äº†æ ‡ç­¾è¡¥å…¨å¤–ï¼Œä½ çŽ°åœ¨è¿˜å¯ä»¥è¿›è¡Œå†…è”èŠå¤©å’Œä»£ç ç”Ÿæˆã€‚\n\nCursoræ˜¯VSCodeçš„ä¸€ä¸ªåˆ†æ”¯ï¼ŒæŒ‰ç…§æˆ‘çš„ç»éªŒï¼Œä»–ä»¬çš„Cursoræ ‡ç­¾è¡¥å…¨åŠŸèƒ½æ¯”VSCodeæ›´å¿«ã€æ›´å‡†ç¡®ã€‚\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*pQ4SuReyicAiBCG3.gif)\n\nä»–ä»¬è¿˜åšäº†ä¸€äº›è®©æˆ‘èŠ‚çœäº†æ— æ•°å°æ—¶çš„äº‹æƒ…ï¼šæ™ºèƒ½/AIè¾…åŠ©é‡æž„ã€‚è¿™å¯èƒ½æ˜¯å€¼å¾—Cursorè®¢é˜…çš„æœ€ä½³åŠŸèƒ½ä¹‹ä¸€ã€‚\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*JlzVJ6o18sulIUEeo_p5sg.gif)\n\nè€Œä¸”è¿™ä¸ä»…ä»…æ˜¯é‡æž„ï¼›åœ¨æ›´æ”¹ä¸€è¡ŒåŽï¼Œå®ƒåœ¨æ™ºèƒ½ç¼–è¾‘ä¸­ä¹Ÿä¼šè¡¨çŽ°å‡ºè‰²ã€‚Cursorä¼šæ˜¾ç¤ºä¸€ä¸ªâ€œæ ‡ç­¾â€æŒ‡ç¤ºç¬¦ï¼Œè¡¨ç¤ºå®ƒå¯¹ä½ åˆšåˆšç¼–è¾‘çš„ä»£ç éƒ¨åˆ†æå‡ºäº†æ›´æ”¹å»ºè®®ã€‚åªéœ€æŒ‰ä¸‹æ ‡ç­¾é”®å³å¯çº§è”æ›´æ”¹ï¼Œè¿™æ ·å¯ä»¥ä¸æ–­è¿›è¡Œä¸‹åŽ»ã€‚æ ‡ç­¾æ ‡ç­¾æ ‡ç­¾ã€‚\n\nä¸€æ—¦ä½ è¿›å…¥çŠ¶æ€ï¼Œä½ ä¼šå‘çŽ°è‡ªå·±èƒ½å¤šä¹ˆé«˜æ•ˆã€‚æˆ‘ç¼–ç çš„æµç¨‹å’Œå¾€å¸¸ä¸€æ ·ï¼Œä½†å› ä¸ºæˆ‘éœ€è¦å†™çš„ä»£ç å°‘å¾—å¤šï¼Œæ‰€ä»¥é€Ÿåº¦æ›´å¿«ã€‚ä½ ä½¿ç”¨å®ƒç¼–ç çš„è¶Šå¤šï¼Œå®ƒå°±è¶Šèƒ½å­¦ä¹ ä½ çš„é¡¹ç›®ã€ç¼–ç é£Žæ ¼ç­‰â€¦â€¦ä¸€å¼€å§‹å¯èƒ½ä¼šæ˜¾å¾—æœ‰äº›ä¸é€‚åº”ï¼Œä½†ç›¸ä¿¡æˆ‘ï¼Œç»™å®ƒä¸€ç‚¹æ—¶é—´ã€‚\n\nä½ è¿˜å¯ä»¥é€šè¿‡å†…è”èŠå¤©ç”Ÿæˆä»£ç ï¼š\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*pVzU2MZ0vNFQ6-dRaWPy-w.gif)\n\nå½“ä½ éœ€è¦ä¸€ä¸ªç‰¹å®šçš„ç®—æ³•ï¼Œæˆ–è€…åœ¨çŽ°æœ‰ä»£ç ä¸­æ‹¥æœ‰æ‰€æœ‰ä¸Šä¸‹æ–‡ä½†éœ€è¦ç¼–å†™ä¸€äº›ç¹ççš„éƒ¨åˆ†æ—¶ï¼Œè¿™éžå¸¸æœ‰ç”¨ã€‚å®ƒçš„æ•ˆæžœç›¸å½“ä¸é”™ï¼Œä¹Ÿèƒ½èŠ‚çœå¾ˆå¤šæ—¶é—´ã€‚åˆ«å¿˜äº†å®¡æŸ¥ç”Ÿæˆçš„ä»£ç  :)\n\nä¸ºäº†ä¸“é—¨å¼€å§‹iOSå¼€å‘ï¼Œæˆ‘é¼“åŠ±ä½ é˜…è¯»æˆ‘å¦å¤–ä¸¤ä¸ªæ•…äº‹ï¼š\n\nä¸€ä¸ªæ˜¯å…³äºŽå¦‚ä½•è®¾ç½®å®ƒï¼Œå®‰è£…æ­£ç¡®çš„æ‰©å±•ç­‰â€¦â€¦\n\nå¦ä¸€ä¸ªæ˜¯å…³äºŽå¦‚ä½•å°†ä½ çš„Xcodeé¡¹ç›®ä»ŽåŸºäºŽç»„çš„è½¬æ¢ä¸ºåŸºäºŽæ–‡ä»¶å¤¹çš„ï¼Œä»¥ä¾¿ä½ å¯ä»¥åœ¨VSCode/Cursorä¸­è‡ªç”±åˆ›å»º/åˆ é™¤/ç§»åŠ¨æ–‡ä»¶ï¼Œè€Œæ— éœ€è§¦ç¢°.xcodeproj / Xcodeã€‚\n\nè¿™åªæ˜¯Cursor/VSCodeåœ¨iOSå¼€å‘ä¸­çš„è¡¨é¢ã€‚ä½†ä½ ä»Šå¤©å°±åº”è¯¥å¼€å§‹ï¼\n\n## 2\\. GitHub Copilot Xcode æ‰©å±•\n\nè¿™æ˜¯ä¸€ä¸ªæœ€è¿‘å‘å¸ƒçš„æ‰©å±•ï¼Œæœ€åˆæ˜¯ [Intitni](https://proxy.rifx.online/https://github.com/intitni/CopilotForXcode) çš„ä¸€ä¸ªé¡¹ç›®ï¼Œä½†ä¼¼ä¹Ž GitHub å·²ç»å¯¹å…¶è¿›è¡Œäº†åˆ†å‰/æ”¶è´­ï¼Œå¹¶ä½¿å…¶æˆä¸º Copilot + Xcode çš„å®˜æ–¹æ‰©å±•ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œè™½ç„¶ç”¨æˆ·ä½“éªŒå¹¶ä¸å®Œç¾Žï¼ˆå¯ä»¥ç†è§£ï¼Œå› ä¸ºä»–ä»¬å¿…é¡»ä¸Žå¯è®¿é—®æ€§/çª—å£ API ä¸€èµ·å·¥ä½œï¼‰ï¼Œä½†å®ƒæ¯” Appleï¼ˆæœ¬åœ°ï¼‰Xcode æ¨¡åž‹è¦å¥½å¾—å¤šã€‚\n\nè€Œä½ å¾ˆå¹¸è¿ï¼Œæˆ‘å·²ç»å†™è¿‡å…³äºŽå®ƒçš„å†…å®¹ï¼š\n\nå¦‚æžœä½ è¿˜ä¸å‡†å¤‡åˆ‡æ¢åˆ° Xcode ä»¥å¤–çš„å…¶ä»–ç¼–è¾‘å™¨ï¼Œä½†ä»æƒ³ä½¿ç”¨é«˜æ•ˆçš„ AI è¾…åŠ©ä»£ç ç¼–è¾‘ï¼Œé‚£ä¹ˆè¿™ä¸ªæ‰©å±•å°±æ˜¯ä¸ºä½ å‡†å¤‡çš„ï¼\n\n## 3\\. Swift Assist\n\nè™½ç„¶ Xcode å·²ç»å†…ç½®äº†ä¸€ä¸ªç”¨äºŽé¢„æµ‹ä»£ç è¡¥å…¨çš„æœ¬åœ°æ¨¡åž‹ï¼ˆä»…åœ¨ Xcode 16 çš„ Apple Silicon Mac ä¸Šå¯ç”¨ï¼‰ï¼Œä½† Apple åœ¨ WWDC ä¸Šé€éœ²äº†å…¶ä»–å†…å®¹ï¼š\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*-mlY8GyGh3VPVhyg3TmLYw.png)\n\nSwift Assist\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*XJnlRo8mqrAMZEVEL64ufg.gif)\n\nè¿™çœ‹èµ·æ¥åƒæ˜¯æˆ‘ä¸Šé¢æ¼”ç¤ºçš„ Cursor çš„èŠå¤© + ä»£ç ç”Ÿæˆã€‚å®ƒåº”è¯¥èƒ½å¤Ÿæ ¹æ®ä½ çš„è¯„è®ºç”Ÿæˆä»£ç ã€‚ä½†ç›®å‰ï¼Œè¿™ä»ç„¶æ˜¯ä¸€ä¸ªè™šå¹»çš„äº§å“ã€‚Xcode 16\\.2 beta 2 æåˆ°äº†å®ƒï¼Œä½†æˆ‘ä»¬ä»ç„¶æ— æ³•è¿›è¡Œæµ‹è¯•ã€‚\n\nä¹Ÿè®¸å®ƒä¼šåœ¨ Xcode 16\\.2 beta çš„åŽç»­ç‰ˆæœ¬ä¸­æŽ¨å‡ºï¼Œæˆ‘è¿«ä¸åŠå¾…æƒ³è¦æµ‹è¯•å¹¶å†™å…³äºŽå®ƒçš„å†…å®¹ï¼\n\n## 4\\. ChatGPT/Claude/Perplexity ç½‘ç»œç•Œé¢\n\næœ‰æ—¶å€™ï¼Œå›žå½’åŸºç¡€æ˜¯æœ€å¥½çš„é€‰æ‹©ã€‚è™½ç„¶è¿™äº›ä»£ç ç¼–è¾‘å™¨ä½¿ç”¨äº†Anthropicå’ŒOpenAIçš„æ¨¡åž‹ä»¥åŠå®ƒä»¬è‡ªå·±çš„æ¨¡åž‹ï¼Œä½†åœ¨å½“ä»Šçš„çŽ¯å¢ƒä¸­ï¼Œä½¿ç”¨å®ƒä»¬çš„ç½‘ç»œç•Œé¢ä¹Ÿæ˜¯ä¸€ç§å®è´µçš„å·¥å…·ã€‚\n\n### ChatGPT \\+ Canvas\n\nOpenAI çš„ ChatGPT åœ¨è¿‡åŽ»å‡ ä¸ªæœˆä¸­æœ‰äº†å¾ˆå¤§çš„è¿›å±•ã€‚æœ€è¿‘å‘å¸ƒçš„ o1-preview ç‰ˆæœ¬å¸¦æ¥äº†æŽ¨ç†å’Œç”»å¸ƒåŠŸèƒ½ï¼Œä½¿å¾—åœ¨ ChatGPT ç½‘é¡µç•Œé¢ä¸­è¿›è¡Œç¼–ç ä¼šè¯å˜å¾—æ›´åŠ é¡ºç•…ã€‚\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*WUraNCcZMrCRrHMilgzl-Q.png)\n\nç”»å¸ƒæ˜¯ä¸€ä¸ªæž„å»ºåœ¨ ChatGPT ç½‘é¡µç•Œé¢ä¸Šçš„è¿·ä½ ä»£ç ç¼–è¾‘å™¨ï¼Œå…è®¸æ‚¨å¿«é€Ÿè¿­ä»£ä»£ç å’Œæƒ³æ³•ã€‚æ‚¨å¯ä»¥ä½¿ç”¨èŠå¤©è¿›è¡Œå¢žé‡æ›´æ”¹ï¼Œè¿˜æœ‰å…¶ä»–ä¸€äº›å·¥å…·å¯ä»¥å¯¹ä»£ç è¿›è¡Œæ³¨é‡Šã€è¿›è¡Œå†…è”æ›´æ”¹ã€è½¬æ¢ä¸ºå…¶ä»–è¯­è¨€ç­‰ã€‚\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ab7PdMLJwacZtVsET2YYmA.gif)\n\nè™½ç„¶è¿™ä¸èƒ½è®©æ‚¨æž„å»ºå®Œæ•´çš„åº”ç”¨ç¨‹åºï¼Œä½†å®ƒæ˜¯ä¸€ä¸ªåœ¨æ ‡å‡†ç¼–è¾‘å™¨ä¹‹å¤–å¿«é€Ÿè¿­ä»£ä»£ç æƒ³æ³•çš„å¥½å·¥å…·ã€‚\n\n### Claude ä¼ªå½±\n\nè¿™ä¸Ž ChatGPT Canvas ç±»ä¼¼ï¼Œä½†å…·æœ‰ä¸€äº›å…¶ä»–åŠŸèƒ½ï¼Œä¾‹å¦‚é¢„è§ˆï¼ˆæ˜¾ç„¶ä¸æ”¯æŒ Swift/SwiftUIï¼‰å’ŒåŒæ—¶å¤„ç†å¤šä¸ªæ–‡ä»¶ã€‚\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*2iverELFSGqnJzklPK0cYg.png)\n\n## 5\\. Alex Sidebar\n\nè¿™æ˜¯ä¸€ä¸ªæ–°çš„ç«žäº‰è€…ï¼å‰æå¾ˆç®€å•ï¼Œå› ä¸º Xcode æ˜¯é—­æºçš„ï¼Œæ‰©å±• API ç›¸å½“æœ‰é™ï¼Œä¸ºä»€ä¹ˆä¸å›´ç»• Xcode æž„å»ºå‘¢ï¼Ÿ\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*vZgn_FjH0FW53c7qZ4DZAg.png)\n\næˆ‘å¯¹ç”¨æˆ·ä½“éªŒå¹¶ä¸å¤ªæ»¡æ„ï¼Œä½†å®ƒæä¾›äº†å¤§å¤šæ•° Cursor åŠŸèƒ½ï¼Œä½œä¸ºä¸€ä¸ªåƒçª—å£ä¸€æ ·æž„å»ºçš„ Xcode ä¾§é¢æ¿ã€‚è¿™é‡Œæœ‰å„ç§å¿«æ·é”® + ä»£ç è¡¥å…¨ + èŠå¤©ã€‚ä½ ç»å¯¹åº”è¯¥å°è¯•ä¸€ä¸‹ï¼Œçœ‹çœ‹å®ƒæ˜¯å¦èƒ½æ”¹å–„ä½ çš„å·¥ä½œæµç¨‹ï¼\n\n## 6\\. AIProxy\n\nä½œä¸ºï¼ˆ3ï¼‰Swift Assistçš„é¢å¤–å¥–åŠ±ï¼Œå®ƒå¹¶ä¸æ˜¯çœŸæ­£çš„â€¦.å¯ç”¨\n\nè¿™ä¸æ˜¯ä¸€ä¸ªç”¨äºŽç¼–ç çš„å·¥å…·ï¼Œè€Œæ˜¯ä¸€ä¸ªä¸ºæž„å»ºè€…å‡†å¤‡çš„å·¥å…·ã€‚å½“åœ¨ä½ çš„iOSåº”ç”¨ä¸­é›†æˆAI APIæ—¶ï¼Œä½ å¾ˆå¯èƒ½éœ€è¦å°†APIå¯†é’¥æ·»åŠ åˆ°ä½ çš„é¡¹ç›®ä¸­ã€‚ä½†æ­£å¦‚æˆ‘ä»¬æ‰€çŸ¥ï¼ˆå¯¹å§ï¼ï¼‰ï¼Œä½ ä¸åº”è¯¥å°†å…¶æ”¾åœ¨å®¢æˆ·ç«¯ã€‚å¦‚æžœè¿™æ ·åšï¼Œå‡ ä¹Žä»»ä½•äººéƒ½å¯ä»¥è½»æ˜“èŽ·å–ä½ çš„APIå¯†é’¥ï¼Œå¹¶ä»£è¡¨ä½ ä½¿ç”¨ä½ çš„AIç§¯åˆ†ã€‚\n\nè¿›å…¥[AIProxy](https://proxy.rifx.online/https://www.aiproxy.pro/)ï¼Œä»–ä»¬æä¾›å¼€æºSDKï¼Œæ˜“äºŽé›†æˆï¼Œå¹¶æ”¯æŒä½ æ‰€éœ€çš„æ‰€æœ‰AIæä¾›å•†ã€‚\n\nå¦‚æžœä½ ä¸æƒ³æž„å»ºä¸€ä¸ªåŽç«¯æ¥ä»£ç†ä½ çš„AIè°ƒç”¨ï¼Œè¿™å°±æ˜¯é€‚åˆä½ çš„å·¥å…·ï¼\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/top-8-leading-ai-use-cases-revolutionizing-business-in-2025-837e4a98f6a3","frontmatter":{"title":"2025 å¹´å¼•é¢†å•†ä¸šå˜é©çš„å…«å¤§äººå·¥æ™ºèƒ½åº”ç”¨æ¡ˆä¾‹","meta_title":"2025 å¹´å¼•é¢†å•†ä¸šå˜é©çš„å…«å¤§äººå·¥æ™ºèƒ½åº”ç”¨æ¡ˆä¾‹","description":"äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ­£åœ¨æŽ¨åŠ¨å•†ä¸šè½¬åž‹ï¼Œé¢„è®¡åˆ°2025å¹´ï¼Œå°†åœ¨å¤šä¸ªé¢†åŸŸå‘æŒ¥å…³é”®ä½œç”¨ï¼ŒåŒ…æ‹¬é¢„æµ‹åˆ†æžã€å®¢æˆ·æ”¯æŒã€ä¸ªæ€§åŒ–è¥é”€å’Œä¾›åº”é“¾ä¼˜åŒ–ç­‰ã€‚AIé€šè¿‡æé«˜æ•ˆçŽ‡ã€é™ä½Žæˆæœ¬å’Œå¢žå¼ºå†³ç­–èƒ½åŠ›ï¼Œå¸®åŠ©ä¼ä¸šæ›´å¥½åœ°åº”å¯¹å¸‚åœºå˜åŒ–ã€‚å°½ç®¡é¢ä¸´æ•°æ®éšç§å’Œå°±ä¸šæ›¿ä»£ç­‰æŒ‘æˆ˜ï¼Œæ‹¥æŠ±AIæŠ€æœ¯çš„ä¼ä¸šå°†åœ¨ç«žäº‰ä¸­å æ®ä¼˜åŠ¿ï¼ŒæŽ¨åŠ¨åˆ›æ–°å¹¶æ»¡è¶³ä¸æ–­å˜åŒ–çš„æ¶ˆè´¹è€…éœ€æ±‚ã€‚","date":"2024-11-16T01:36:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Na9Wc3PuYCxWOCSBWc4HtQ.png","categories":["Technology","Predictive Analytics","Machine Learning"],"author":"Rifx.Online","tags":["predictive","analytics","automation","recommendation","machine-learning"],"draft":false,"slug":"blog/top-8-leading-ai-use-cases-revolutionizing-business-in-2025-837e4a98f6a3"},"content":"\n\n\n### æŽ¢ç´¢æŽ¨åŠ¨å•†ä¸šæˆåŠŸçš„å…³é”®AIåº”ç”¨ã€‚\n\näººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ­£æ—¥ç›Šå¡‘é€ å•†ä¸šçš„æœªæ¥ï¼Œå…¶å½±å“åŠ›åœ¨å„ä¸ªè¡Œä¸šä¸æ–­æ‰©å¤§ã€‚åˆ°2025å¹´ï¼ŒAIä¸ä»…å°†æˆä¸ºåˆ›æ–°çš„å·¥å…·ï¼Œæ›´æ˜¯å•†ä¸šè½¬åž‹çš„é‡è¦é©±åŠ¨åŠ›ã€‚ä»Žå®¢æˆ·æ”¯æŒåˆ°é¢„æµ‹åˆ†æžï¼ŒAIåœ¨æé«˜æ•ˆçŽ‡ã€é™ä½Žæˆæœ¬å’Œä¿ƒè¿›æ–°å¢žé•¿æœºä¼šæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚éšç€AIçš„ä¸æ–­å‘å±•ï¼Œä¼ä¸šè¶Šæ¥è¶Šä¾èµ–äºŽå®ƒæ¥ç®€åŒ–è¿è¥ã€å¢žå¼ºå†³ç­–èƒ½åŠ›å’Œåˆ›é€ ä¸ªæ€§åŒ–çš„å®¢æˆ·ä½“éªŒã€‚\n\n\n\n[***AIæŠ€æœ¯***](https://www.blockchainappfactory.com/ai-development-company)çš„å¿«é€Ÿå‘å±•æ­£åœ¨ä¸ºä¼ä¸šå¼€å¯å‰æ‰€æœªæœ‰çš„å¯èƒ½æ€§ã€‚å…¬å¸æ­£åœ¨åˆ©ç”¨æœºå™¨å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œè‡ªåŠ¨åŒ–çš„åŠ›é‡ï¼Œä»¥åœ¨ä¸æ–­å˜åŒ–çš„å¸‚åœºçŽ¯å¢ƒä¸­ä¿æŒç«žäº‰åŠ›ã€‚è¿™äº›å‘å±•åœ¨ä¾›åº”é“¾ä¼˜åŒ–ã€ä¸ªæ€§åŒ–è¥é”€å’ŒäººåŠ›èµ„æºç®¡ç†ç­‰é¢†åŸŸå°¤ä¸ºæ˜Žæ˜¾ã€‚éšç€AIçš„æ™®åŠï¼Œå…¶åœ¨æ ¸å¿ƒä¸šåŠ¡æµç¨‹ä¸­çš„æ•´åˆå°†å˜å¾—æ›´åŠ å…³é”®ï¼Œå°¤å…¶æ˜¯å¯¹äºŽé‚£äº›å¸Œæœ›åœ¨2025å¹´åŠä»¥åŽè“¬å‹ƒå‘å±•çš„ç»„ç»‡ã€‚\n\n## ç†è§£äººå·¥æ™ºèƒ½\n\nAIï¼ˆäººå·¥æ™ºèƒ½ï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œæ—¨åœ¨åˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„æœºå™¨æˆ–ç³»ç»Ÿã€‚è¿™äº›ä»»åŠ¡åŒ…æ‹¬ç†è§£è¯­è¨€ã€è¯†åˆ«æ¨¡å¼ã€è§£å†³é—®é¢˜å’Œåšå‡ºå†³ç­–ç­‰ã€‚AIå¯ä»¥å¤§è‡´åˆ†ä¸ºä¸¤ç§ç±»åž‹ï¼š\n\n1. **ç‹­ä¹‰AIï¼ˆå¼±AIï¼‰ï¼š** è¿™ç§ç±»åž‹çš„AIæ—¨åœ¨æ‰§è¡Œç‰¹å®šä»»åŠ¡ã€‚ä¾‹å­åŒ…æ‹¬è¯­éŸ³åŠ©æ‰‹å¦‚Siriæˆ–Alexaã€äººè„¸è¯†åˆ«ç³»ç»Ÿï¼Œä»¥åŠæµåª’ä½“æœåŠ¡å¦‚Netflixæˆ–Spotifyä½¿ç”¨çš„æŽ¨èç®—æ³•ã€‚ç‹­ä¹‰AIä¸å…·å¤‡ä¸€èˆ¬æ™ºèƒ½ï¼Œå±€é™äºŽå…¶è¢«ç¼–ç¨‹æ‰§è¡Œçš„ä»»åŠ¡ã€‚\n2. **å¹¿ä¹‰AIï¼ˆå¼ºAIï¼‰ï¼š** è¿™æ˜¯ä¸€ç§æ›´é«˜çº§çš„AIå½¢å¼ï¼Œèƒ½å¤Ÿç†è§£ã€å­¦ä¹ å¹¶åœ¨å¹¿æ³›çš„æ´»åŠ¨ä¸­åº”ç”¨æ™ºèƒ½ï¼Œç±»ä¼¼äºŽäººç±»çš„æ€ç»´å’ŒæŽ¨ç†æ–¹å¼ã€‚å¹¿ä¹‰AIåœ¨ç›®å‰ä»ç„¶ä¸»è¦æ˜¯ç†è®ºæ€§çš„ï¼Œå°šä¸å­˜åœ¨ã€‚\n\n**AIä¸­çš„å…³é”®æŠ€æœ¯å’Œæ¦‚å¿µï¼š**\n\n* **æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰ï¼š** AIçš„ä¸€ä¸ªå­é›†ï¼Œä¸“æ³¨äºŽæž„å»ºå…è®¸æœºå™¨ä»Žæ•°æ®ä¸­å­¦ä¹ å¹¶éšç€æ—¶é—´æŽ¨ç§»æ”¹è¿›çš„ç®—æ³•ã€‚åœ¨MLä¸­ï¼Œç³»ç»Ÿåœ¨å¤§åž‹æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶åˆ©ç”¨è¿™äº›ä¿¡æ¯è¿›è¡Œé¢„æµ‹æˆ–å†³ç­–ï¼Œè€Œæ— éœ€æ˜Žç¡®ç¼–ç¨‹ã€‚\n* **æ·±åº¦å­¦ä¹ ï¼š** æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œä½¿ç”¨å…·æœ‰å¤šä¸ªå±‚æ¬¡ï¼ˆç§°ä¸ºæ·±åº¦ç¥žç»ç½‘ç»œï¼‰çš„ç¥žç»ç½‘ç»œã€‚æ·±åº¦å­¦ä¹ åœ¨å›¾åƒå’Œè¯­éŸ³è¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œè‡ªåŠ¨é©¾é©¶ç­‰å¤æ‚ä»»åŠ¡ä¸­ç‰¹åˆ«æœ‰ç”¨ã€‚\n* **è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ï¼š** è¿™æ¶‰åŠæ•™æœºå™¨ç†è§£ã€è§£é‡Šå’Œä»¥æœ‰æ„ä¹‰å’Œç›¸å…³çš„æ–¹å¼å›žåº”äººç±»è¯­è¨€ã€‚NLPæ˜¯èŠå¤©æœºå™¨äººã€è™šæ‹ŸåŠ©æ‰‹å’Œè¯­è¨€ç¿»è¯‘åº”ç”¨èƒŒåŽçš„æŠ€æœ¯ã€‚\n* **è®¡ç®—æœºè§†è§‰ï¼š** ä¸€ä¸ªå…è®¸æœºå™¨è§£é‡Šå’Œç†è§£è§†è§‰ä¸–ç•Œçš„AIé¢†åŸŸã€‚è®¡ç®—æœºè§†è§‰ç”¨äºŽäººè„¸è¯†åˆ«ã€ç‰©ä½“æ£€æµ‹å’Œè‡ªåŠ¨é©¾é©¶æ±½è½¦ç­‰åº”ç”¨ã€‚\n* **å¼ºåŒ–å­¦ä¹ ï¼š** ä¸€ç§æœºå™¨å­¦ä¹ ç±»åž‹ï¼Œå…¶ä¸­AIä»£ç†é€šè¿‡æ‰§è¡ŒåŠ¨ä½œå¹¶ä»¥å¥–åŠ±æˆ–æƒ©ç½šçš„å½¢å¼æŽ¥æ”¶åé¦ˆæ¥å­¦ä¹ å¦‚ä½•åœ¨çŽ¯å¢ƒä¸­è¡Œä¸ºã€‚\n\n## 2024å¹´æ”¹å˜æ¸¸æˆè§„åˆ™çš„åå¤§AIåº”ç”¨æ¡ˆä¾‹\n\n## 1\\. é¢„æµ‹åˆ†æžåœ¨æˆ˜ç•¥å†³ç­–ä¸­çš„åº”ç”¨\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*fgshMkMhuWL7ZEDo9MIR_w.png)\n\né¢„æµ‹åˆ†æžåˆ©ç”¨ç»Ÿè®¡ç®—æ³•ã€æœºå™¨å­¦ä¹ æ¨¡åž‹å’Œæ•°æ®æŒ–æŽ˜æ¥åŸºäºŽåŽ†å²æ•°æ®é¢„æµ‹æœªæ¥ç»“æžœã€‚å®ƒä½¿ä¼ä¸šèƒ½å¤Ÿå›žç­”å…³é”®é—®é¢˜ï¼Œä¾‹å¦‚â€œå¯èƒ½ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿâ€é€šè¿‡ä»Žè¿‡åŽ»çš„æ¨¡å¼ä¸­æå–ä¿¡æ¯ï¼Œä»¥æŒ‡å¯¼æœªæ¥çš„æƒ…æ™¯ã€‚\n\n### å…³é”®ç”¨ä¾‹åœ¨æˆ˜ç•¥å†³ç­–ä¸­\n\n* **éœ€æ±‚é¢„æµ‹**ï¼šé›¶å”®å•†ã€åˆ¶é€ å•†å’ŒæœåŠ¡æä¾›å•†ä½¿ç”¨é¢„æµ‹åˆ†æžæ¥é¢„æµ‹éœ€æ±‚ï¼Œå¸®åŠ©ä»–ä»¬ç®¡ç†åº“å­˜ï¼Œå‡å°‘æµªè´¹ï¼Œé¿å…ç¼ºè´§æˆ–è¿‡åº¦ç”Ÿäº§ã€‚ä¾‹å¦‚ï¼Œé›¶å”®å•†å¯ä»¥ä¸ºå­£èŠ‚æ€§é«˜å³°åšè®¡åˆ’ï¼Œåˆ¶é€ å•†å¯ä»¥æ ¹æ®é¢„æœŸéœ€æ±‚è°ƒæ•´ç”Ÿäº§è®¡åˆ’ã€‚\n* **é£Žé™©è¯„ä¼°ä¸Žç®¡ç†**ï¼šé¢„æµ‹æ¨¡åž‹å®žæ—¶è¯†åˆ«é£Žé™©å› ç´ ï¼Œä½¿ä¼ä¸šæ›´å®¹æ˜“ç®¡ç†è´¢åŠ¡ã€è¿è¥ç”šè‡³å£°èª‰é£Žé™©ã€‚åœ¨é‡‘èžé¢†åŸŸï¼Œé¢„æµ‹åˆ†æžå¯ä»¥æ ‡è®°æ½œåœ¨çš„è¿çº¦é£Žé™©ï¼Œè€Œåœ¨ä¾›åº”é“¾ä¸­ï¼Œå®ƒå¯ä»¥é¢„æµ‹ä¸­æ–­ã€‚\n* **å®¢æˆ·è¡Œä¸ºé¢„æµ‹**ï¼šé€šè¿‡åˆ†æžè¿‡åŽ»çš„äº’åŠ¨ï¼Œäººå·¥æ™ºèƒ½å¯ä»¥é¢„æµ‹å®¢æˆ·è¡Œä¸ºï¼Œä½¿å…¬å¸æ›´å¥½åœ°ç†è§£å®¢æˆ·åå¥½ã€è´­ä¹°å‘¨æœŸå’Œæ½œåœ¨æµå¤±ã€‚è¿™æœ‰åŠ©äºŽé‡èº«å®šåˆ¶äº§å“å’Œæ—¶æœºï¼Œæœ€å¤§åŒ–å®¢æˆ·ç”Ÿå‘½å‘¨æœŸä»·å€¼ã€‚\n\n### é¢„æµ‹åˆ†æžå¯¹æˆ˜ç•¥çš„å¥½å¤„\n\n* **å†³ç­–å‡†ç¡®æ€§æé«˜**ï¼šé¢„æµ‹æ¨¡åž‹æä¾›åŸºäºŽè¯æ®çš„æ´žå¯Ÿï¼Œå¢žå¼ºæˆ˜ç•¥å†³ç­–çš„ä¿¡å¿ƒï¼Œå‡å°‘å¯¹ç›´è§‰çš„ä¾èµ–ã€‚\n* **ä¸»åŠ¨è§£å†³é—®é¢˜**ï¼šä¼ä¸šå¯ä»¥é¢„è§é—®é¢˜ï¼Œè€Œä¸æ˜¯åœ¨é—®é¢˜å‡ºçŽ°æ—¶è¢«åŠ¨ååº”ã€‚è¿™ä½¿å¾—åœ¨é£Žé™©ç¼“è§£ã€èµ„æºåˆ†é…å’ŒäººåŠ›èµ„æºè§„åˆ’ç­‰é¢†åŸŸé‡‡å–ä¸»åŠ¨æŽªæ–½æˆä¸ºå¯èƒ½ã€‚\n* **ç«žäº‰ä¼˜åŠ¿**ï¼šåˆ©ç”¨é¢„æµ‹åˆ†æžçš„å…¬å¸èƒ½å¤Ÿæ¯”ç«žäº‰å¯¹æ‰‹æ›´å¿«ã€æ›´èªæ˜Žåœ°åšå‡ºå†³ç­–ï¼Œä»Žè€ŒèŽ·å¾—æˆ˜ç•¥ä¼˜åŠ¿ã€‚\n\n### é¢„æµ‹åˆ†æžä¸­çš„å·¥å…·å’ŒæŠ€æœ¯\n\n* **æ•°æ®æ”¶é›†ä¸Žå‡†å¤‡**ï¼šåƒ Google BigQuery å’Œ Apache Hadoop è¿™æ ·çš„å·¥å…·ä»Žå„ç§æ¥æºæ”¶é›†å’Œé¢„å¤„ç†æ•°æ®ã€‚\n* **æ¨¡åž‹æž„å»ºä¸Žä¼˜åŒ–**ï¼šå¦‚ TensorFlowã€SAS å’Œ IBM Watson ç­‰å¹³å°å…è®¸ä¼ä¸šåˆ›å»ºå’Œæµ‹è¯•é¢„æµ‹æ¨¡åž‹ï¼Œå¹¶å¯¹å…¶è¿›è¡Œå‡†ç¡®æ€§å’Œå¯é æ€§çš„å¾®è°ƒã€‚\n* **éƒ¨ç½²ä¸Žç›‘æŽ§**ï¼šåœ¨æ¨¡åž‹å¼€å‘ä¹‹åŽï¼Œåƒ DataRobot æˆ– Amazon SageMaker è¿™æ ·çš„å·¥å…·ä¿ƒè¿›éƒ¨ç½²ï¼Œå…è®¸å¯¹å®žæ—¶å‡†ç¡®æ€§è¿›è¡ŒæŒç»­ç›‘æŽ§å’Œæ›´æ–°ã€‚\n\n## 2\\. AI\\-é©±åŠ¨çš„å®¢æˆ·æ”¯æŒå’ŒèŠå¤©æœºå™¨äºº\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*dTt_KxOtAb-qGThAvEOQsg.png)\n\nAI\\-é©±åŠ¨çš„å®¢æˆ·æ”¯æŒç³»ç»Ÿåˆ©ç”¨æœºå™¨å­¦ä¹ å’Œè‡ªç„¶è¯­è¨€å¤„ç†æ¥ç†è§£ã€è§£é‡Šå’Œå›žåº”å®¢æˆ·æŸ¥è¯¢ã€‚è¿™äº›å·¥å…·åˆ†æžç”¨æˆ·è¾“å…¥ï¼Œæ£€æµ‹æ„å›¾ï¼Œå¹¶æä¾›ç›¸å…³çš„å“åº”æˆ–å¼•å¯¼ç”¨æˆ·å®Œæˆä¸€ç³»åˆ—æ“ä½œï¼Œè‡ªåŠ¨åŒ–å“åº”è¿‡ç¨‹å¹¶å‡å°‘ç­‰å¾…æ—¶é—´ã€‚\n\n### AIé©±åŠ¨çš„å®¢æˆ·æ”¯æŒå’ŒèŠå¤©æœºå™¨äººçš„å…³é”®ä½¿ç”¨æ¡ˆä¾‹\n\n* **å…¨å¤©å€™å®¢æˆ·æœåŠ¡**: èŠå¤©æœºå™¨äººå¯ä»¥å…¨å¤©å€™å¤„ç†å®¢æˆ·å’¨è¯¢ï¼Œæä¾›å³æ—¶å¸®åŠ©ï¼Œå‡å°‘å®¢æˆ·åœ¨éžå·¥ä½œæ—¶é—´ç­‰å¾…äººå·¥å®¢æœçš„éœ€æ±‚ã€‚\n* **æ½œåœ¨å®¢æˆ·èµ„æ ¼ç¡®è®¤å’ŒåŸ¹è‚²**: å¯¹äºŽä»¥é”€å”®ä¸ºé©±åŠ¨çš„å›¢é˜Ÿï¼ŒèŠå¤©æœºå™¨äººå¯ä»¥é€šè¿‡è¯¢é—®é¢„è®¾é—®é¢˜æ¥ç¡®è®¤æ½œåœ¨å®¢æˆ·ï¼Œæ”¶é›†å…³é”®ä¿¡æ¯ï¼Œå¹¶å°†é«˜æ½œåŠ›çš„æ½œåœ¨å®¢æˆ·å¼•å¯¼ç»™äººå·¥å®¢æœã€‚\n* **ä¸ªæ€§åŒ–å®¢æˆ·ä½“éªŒ**: åˆ©ç”¨å®¢æˆ·æ•°æ®ï¼ŒAIé©±åŠ¨çš„èŠå¤©æœºå™¨äººå¯ä»¥æ ¹æ®ç”¨æˆ·åŽ†å²ã€ä¹‹å‰çš„äº’åŠ¨å’Œè¡Œä¸ºä¸ªæ€§åŒ–å“åº”ï¼Œæä¾›é‡èº«å®šåˆ¶çš„ä½“éªŒï¼Œä»¥å¢žå¼ºå‚ä¸Žæ„Ÿã€‚\n* **åé¦ˆæ”¶é›†å’Œæƒ…æ„Ÿåˆ†æž**: AIèŠå¤©æœºå™¨äººå¯ä»¥åœ¨äº’åŠ¨æœŸé—´å’Œä¹‹åŽæ”¶é›†åé¦ˆï¼Œè¯„ä¼°å®¢æˆ·æ»¡æ„åº¦ã€‚æƒ…æ„Ÿåˆ†æžå¯ä»¥ç”¨äºŽæ£€æµ‹æ»¡æ„æˆ–æŒ«è´¥æ„Ÿï¼Œå¸®åŠ©ä¼ä¸šä¼˜åŒ–æœåŠ¡ã€‚\n\n### AIé©±åŠ¨çš„å®¢æˆ·æ”¯æŒå’ŒèŠå¤©æœºå™¨äººä¼˜åŠ¿\n\n* **æå‡æ•ˆçŽ‡å’ŒèŠ‚çœæˆæœ¬**ï¼šé€šè¿‡è‡ªåŠ¨åŒ–é‡å¤æ€§æŸ¥è¯¢ï¼ŒAIå‡å°‘äº†äººç±»ä»£ç†çš„å·¥ä½œè´Ÿæ‹…ï¼Œä½¿ä»–ä»¬èƒ½å¤Ÿä¸“æ³¨äºŽå¤æ‚é—®é¢˜ï¼Œå¹¶é™ä½Žè¿è¥æˆæœ¬ã€‚\n* **å¯æ‰©å±•æ€§**ï¼šAIé©±åŠ¨çš„å®¢æˆ·æ”¯æŒèƒ½å¤Ÿè½»æ¾æ‰©å±•ï¼Œä½¿ä¼ä¸šåœ¨é«˜å³°æ—¶æœŸå¤„ç†å®¢æˆ·æŸ¥è¯¢æ¿€å¢žè€Œæ— éœ€å¢žåŠ é¢å¤–å‘˜å·¥ã€‚\n* **æ›´å¿«çš„å“åº”æ—¶é—´**ï¼šAIèŠå¤©æœºå™¨äººå¯ä»¥å³æ—¶å¤„ç†å’Œå›žåº”ï¼Œæ˜¾è‘—å‡å°‘ç­‰å¾…æ—¶é—´å¹¶æ”¹å–„å®¢æˆ·ä½“éªŒã€‚\n\n### å—æ¬¢è¿Žçš„äººå·¥æ™ºèƒ½é©±åŠ¨å®¢æˆ·æ”¯æŒå’ŒèŠå¤©æœºå™¨äººå¹³å°\n\n* **Zendesk Chat and Support**: å°†èŠå¤©æœºå™¨äººä¸Žå·¥å•ç³»ç»Ÿç»“åˆï¼Œä»¥ç®¡ç†å¤æ‚çš„å®¢æˆ·æ”¯æŒæµç¨‹ã€‚\n* **Intercom**: ä¸€æ¬¾åŸºäºŽäººå·¥æ™ºèƒ½çš„å®¢æˆ·æ¶ˆæ¯å¹³å°ï¼Œé€šè¿‡å®žæ—¶èŠå¤©å’Œè‡ªåŠ¨å›žå¤æ”¯æŒé”€å”®å’Œå®¢æˆ·äº’åŠ¨ã€‚\n* **Drift**: ä»¥å…¶æ½œåœ¨å®¢æˆ·èµ„æ ¼è¯„ä¼°åŠŸèƒ½è€Œé—»åï¼ŒDrift ä½¿ç”¨å¯¹è¯å¼äººå·¥æ™ºèƒ½ä¸Žç½‘ç«™è®¿å®¢äº’åŠ¨ï¼Œå°†å…¶è½¬åŒ–ä¸ºåˆæ ¼æ½œåœ¨å®¢æˆ·ã€‚\n\n## 3\\. ä¸ªæ€§åŒ–è¥é”€ä¸ŽæŽ¨èå¼•æ“Ž\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*24ZIbZ6OOV0UGFoW3OOxtg.png)\n\nåŸºäºŽAIçš„æŽ¨èå¼•æ“Žä½¿ç”¨ç®—æ³•å’Œæœºå™¨å­¦ä¹ æ¥åˆ†æžç”¨æˆ·æ•°æ®ï¼Œè¯†åˆ«æ¨¡å¼ï¼Œå¹¶è¿›è¡Œé¢„æµ‹ã€‚é€šè¿‡å°†è¿‡åŽ»è¡Œä¸ºçš„æ•°æ®ä¸Žå®žæ—¶äº¤äº’ç›¸ç»“åˆï¼Œè¿™äº›å¼•æ“Žå¯ä»¥æä¾›ä¸Žç”¨æˆ·å…´è¶£é«˜åº¦ç›¸å…³çš„ç²¾å‡†æŽ¨èã€‚æœ€å¸¸è§çš„æŠ€æœ¯åŒ…æ‹¬ï¼š\n\n* **ååŒè¿‡æ»¤**ï¼šè¯†åˆ«ç›¸ä¼¼ç”¨æˆ·å–œæ¬¢çš„é¡¹ç›®æˆ–å†…å®¹å¹¶è¿›è¡ŒæŽ¨èã€‚\n* **åŸºäºŽå†…å®¹çš„è¿‡æ»¤**ï¼šå»ºè®®ä¸Žä¹‹å‰è¡¨çŽ°å‡ºå…´è¶£çš„é¡¹ç›®å…·æœ‰ç›¸ä¼¼å±žæ€§çš„é¡¹ç›®ã€‚\n* **æ··åˆæ¨¡åž‹**ï¼šç»“åˆååŒè¿‡æ»¤å’ŒåŸºäºŽå†…å®¹çš„è¿‡æ»¤ï¼Œä»¥æä¾›æ›´å‡†ç¡®å’Œå¤šæ ·åŒ–çš„æŽ¨èã€‚\n\n### ä¸ªæ€§åŒ–è¥é”€å’ŒæŽ¨èå¼•æ“Žçš„å…³é”®ç”¨ä¾‹\n\n* **ç”µå­å•†åŠ¡äº§å“æŽ¨è**ï¼šåœ¨çº¿é›¶å”®å•†æ ¹æ®ä¹‹å‰çš„è´­ä¹°æˆ–æµè§ˆåŽ†å²å»ºè®®å•†å“ï¼Œä»Žè€Œå¢žåŠ è½¬åŒ–çš„å¯èƒ½æ€§å¹¶æå‡é”€å”®é¢ã€‚\n* **å†…å®¹å’Œåª’ä½“ä¸ªæ€§åŒ–**ï¼šåƒNetflixå’ŒSpotifyè¿™æ ·çš„æµåª’ä½“å¹³å°ä½¿ç”¨æŽ¨èå¼•æ“Žæ¥å»ºè®®ç”µå½±ã€èŠ‚ç›®æˆ–æ­Œæ›²ï¼Œä»Žè€Œå¢žå¼ºç”¨æˆ·å‚ä¸Žåº¦å’Œç•™å­˜çŽ‡ã€‚\n* **ç²¾å‡†é‚®ä»¶è¥é”€æ´»åŠ¨**ï¼šäººå·¥æ™ºèƒ½é€šè¿‡åŒ…å«äº§å“å»ºè®®ã€ç‰¹åˆ«ä¼˜æƒ å’Œä¸Žæ¯ä¸ªæ”¶ä»¶äººè¡Œä¸ºç›¸å…³çš„å†…å®¹æ¥ä¸ªæ€§åŒ–é‚®ä»¶å†…å®¹ï¼Œä»Žè€Œæé«˜æ‰“å¼€çŽ‡å’Œç‚¹å‡»çŽ‡ã€‚\n* **åŠ¨æ€ç½‘é¡µå†…å®¹**ï¼šäººå·¥æ™ºèƒ½é©±åŠ¨çš„å¼•æ“Žæ ¹æ®è®¿å®¢è¡Œä¸ºå®žæ—¶è°ƒæ•´ç½‘ç«™å†…å®¹ï¼Œå‘ç”¨æˆ·å‘ˆçŽ°ä¸Žå…¶å…´è¶£åŒ¹é…çš„ä¿ƒé”€ã€æ–‡ç« æˆ–äº§å“ã€‚\n\n### ä¸ªæ€§åŒ–è¥é”€å’ŒæŽ¨èå¼•æ“Žçš„å¥½å¤„\n\n* **å¢žå¼ºå®¢æˆ·å‚ä¸Žåº¦**ï¼šä¸ªæ€§åŒ–æŽ¨èé€šè¿‡æä¾›ç›¸å…³é€‰é¡¹æ¥å¢žåŠ ç”¨æˆ·äº’åŠ¨ï¼Œå‡å°‘å®¢æˆ·æœç´¢æ‰€èŠ±è´¹çš„æ—¶é—´ã€‚\n* **æé«˜è½¬åŒ–çŽ‡**ï¼šå½“ç”¨æˆ·çœ‹åˆ°ä¸Žä»–ä»¬å…´è¶£å¯†åˆ‡ç›¸å…³çš„äº§å“æˆ–å†…å®¹æ—¶ï¼Œä»–ä»¬æ›´æœ‰å¯èƒ½è¿›è¡Œè´­ä¹°æˆ–ä¸Žå†…å®¹äº’åŠ¨ã€‚\n* **æ”¹å–„å®¢æˆ·å¿ è¯šåº¦å’Œç•™å­˜çŽ‡**ï¼šé€šè¿‡è®©å®¢æˆ·æ„Ÿåˆ°è¢«ç†è§£ï¼Œå“ç‰ŒåŸ¹å…»äº†æ›´å¼ºçš„è”ç³»ï¼Œé¼“åŠ±é‡å¤è®¿é—®å’Œå“ç‰Œå¿ è¯šã€‚\n\n### ä¸ªæ€§åŒ–è¥é”€å’ŒæŽ¨èå¼•æ“Žçš„æœ€ä½³å·¥å…·\n\n* **Amazon Personalize**ï¼šäºšé©¬é€Šçš„å·¥å…·åˆ©ç”¨æœºå™¨å­¦ä¹ æä¾›å®žæ—¶æŽ¨èå’Œä¸ªæ€§åŒ–å†…å®¹ã€‚\n* **Salesforce Einstein**ï¼šä¸€å¥—å¼ºå¤§çš„ AI é©±åŠ¨çš„å®¢æˆ·å…³ç³»ç®¡ç† (CRM) å¥—ä»¶ï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªå®¢æˆ·æŽ¥è§¦ç‚¹æä¾›ä¸ªæ€§åŒ–æŽ¨èã€‚\n* **Algolia Recommend**ï¼šä¸€ä¸ªæŽ¨èå¼•æ“Žï¼Œä½¿ç”µå­å•†åŠ¡ã€åª’ä½“å’Œå…¶ä»–æ•°å­—çŽ¯å¢ƒä¸­çš„ä¸ªæ€§åŒ–ä½“éªŒæˆä¸ºå¯èƒ½ã€‚\n\n### çŽ°å®žä¸–ç•ŒæŽ¨èå¼•æ“Žçš„å®žé™…æ¡ˆä¾‹\n\n* **é›¶å”®**ï¼šåƒäºšé©¬é€Šå’Œæ²ƒå°”çŽ›è¿™æ ·çš„ç”µå­å•†åŠ¡å¹³å°æ ¹æ®ç”¨æˆ·è¡Œä¸ºæŽ¨èå•†å“ï¼Œé€šè¿‡äº¤å‰é”€å”®å’Œè¿½åŠ é”€å”®æ¥å¢žåŠ è®¢å•ä»·å€¼ã€‚\n* **æµåª’ä½“æœåŠ¡**ï¼šNetflixçš„æŽ¨èå¼•æ“Žåˆ†æžè§‚çœ‹ä¹ æƒ¯ä»¥å»ºè®®å†…å®¹ï¼Œåˆ›é€ æ— ç¼ä¸”å¼•äººå…¥èƒœçš„è§‚çœ‹ä½“éªŒã€‚\n* **æ—…æ¸¸ä¸Žé…’åº—**ï¼šåƒAirbnbå’ŒExpediaè¿™æ ·çš„å…¬å¸æ ¹æ®ç”¨æˆ·åå¥½æŽ¨èæ—…è¡Œç›®çš„åœ°ã€ä½å®¿å’Œæ´»åŠ¨ï¼Œä½¿æ—…è¡Œè§„åˆ’æ›´åŠ ç›¸å…³å’Œé«˜æ•ˆã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*qlu4M1HEcB3VNAm2o3FI6w.png)\n\n## 4\\. æ™ºèƒ½è‡ªåŠ¨åŒ–ä¸ŽRPAï¼ˆæœºå™¨äººæµç¨‹è‡ªåŠ¨åŒ–ï¼‰\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Bbnm4Snzk0vrhKOxLwZ_uQ.png)\n\næ™ºèƒ½è‡ªåŠ¨åŒ–ç”±RPAï¼ˆæœºå™¨äººæµç¨‹è‡ªåŠ¨åŒ–ï¼‰å’Œå…ˆè¿›çš„äººå·¥æ™ºèƒ½é©±åŠ¨ï¼Œæ­£åœ¨é€šè¿‡è‡ªåŠ¨åŒ–é‡å¤æ€§ä»»åŠ¡ã€æé«˜ç”Ÿäº§åŠ›å’Œå‡å°‘é”™è¯¯æ¥æ”¹å˜å•†ä¸šè¿è¥ã€‚ä¸Žä¼ ç»Ÿè‡ªåŠ¨åŒ–ä¸“æ³¨äºŽç®€å•è§„åˆ™åŸºç¡€ä»»åŠ¡ä¸åŒï¼Œæ™ºèƒ½è‡ªåŠ¨åŒ–ç»“åˆäº†äººå·¥æ™ºèƒ½ã€æœºå™¨å­¦ä¹ å’ŒRPAï¼Œä»¥å¤„ç†éœ€è¦é€‚åº”æ€§å’Œå†³ç­–èƒ½åŠ›çš„å¤æ‚å·¥ä½œæµã€‚ä»¥ä¸‹æ˜¯æ™ºèƒ½è‡ªåŠ¨åŒ–ä¸ŽRPAå¦‚ä½•æ”¹å˜å•†ä¸šæ ¼å±€çš„æ¦‚è¿°ï¼š\n\n### ä»€ä¹ˆæ˜¯æ™ºèƒ½è‡ªåŠ¨åŒ–å’ŒRPAï¼Ÿ\n\n* **æœºå™¨äººæµç¨‹è‡ªåŠ¨åŒ– (RPA)** ä½¿ç”¨è½¯ä»¶æœºå™¨äººæˆ–â€œæœºå™¨äººâ€æ¥è‡ªåŠ¨åŒ–é«˜å®¹é‡ã€é‡å¤æ€§çš„ä»»åŠ¡ï¼Œå¦‚æ•°æ®è¾“å…¥ã€è¡¨å•å¡«å†™å’ŒæŠ¥å‘Šç”Ÿæˆã€‚RPA åŸºäºŽé¢„è®¾è§„åˆ™æ“ä½œï¼Œä¸éœ€è¦è®¤çŸ¥å†³ç­–ã€‚\n* **æ™ºèƒ½è‡ªåŠ¨åŒ–** å°† RPA ä¸Žäººå·¥æ™ºèƒ½èƒ½åŠ›ï¼ˆå¦‚è‡ªç„¶è¯­è¨€å¤„ç† (NLP) å’Œæœºå™¨å­¦ä¹ ï¼‰ç»“åˆï¼Œä»¥ç®¡ç†éœ€è¦æ›´é«˜æ°´å¹³æŽ¨ç†å’Œé€‚åº”æ€§çš„ä»»åŠ¡ã€‚è¿™ç§èžåˆä½¿æœºå™¨äººèƒ½å¤Ÿåšå‡ºä¸Šä¸‹æ–‡å†³ç­–ï¼Œé€‚åº”æ–°æ•°æ®ï¼Œå¹¶å¤„ç†éžç»“æž„åŒ–ä¿¡æ¯ã€‚\n\n### æ™ºèƒ½è‡ªåŠ¨åŒ–å’ŒRPAçš„å…³é”®ç”¨ä¾‹\n\n* **å‘ç¥¨å¤„ç†å’Œåº”ä»˜è´¦æ¬¾**ï¼šRPAå¯ä»¥æ‰«æå‘ç¥¨ï¼Œæå–æ•°æ®ï¼Œä¸Žé‡‡è´­è®¢å•è¿›è¡Œäº¤å‰æ£€æŸ¥ï¼Œå¹¶å°†å…¶è¾“å…¥ä¼šè®¡ç³»ç»Ÿï¼Œä»Žè€Œå‡å°‘äººå·¥å·¥ä½œé‡å¹¶æé«˜å‡†ç¡®æ€§ã€‚\n* **é‡‘èžæœåŠ¡ä¸­çš„å®¢æˆ·å…¥èŒ**ï¼šæ™ºèƒ½è‡ªåŠ¨åŒ–å¸®åŠ©é“¶è¡Œå’Œä¿é™©å…¬å¸é€šè¿‡è‡ªåŠ¨åŒ–æ–‡æ¡£éªŒè¯ã€èƒŒæ™¯æ£€æŸ¥å’Œæ•°æ®è¾“å…¥æ¥ç®€åŒ–å…¥èŒæµç¨‹ï¼Œå¢žå¼ºåˆè§„æ€§å¹¶å‡å°‘å…¥èŒæ—¶é—´ã€‚\n* **è®¢å•å¤„ç†å’Œå±¥è¡Œ**ï¼šRPAå¯ä»¥é€šè¿‡ä»Žè®¢å•è¡¨å•ä¸­æå–æ•°æ®ã€æ£€æŸ¥åº“å­˜å’Œå¯åŠ¨å‘è´§æµç¨‹æ¥è‡ªåŠ¨åŒ–è®¢å•å¤„ç†ï¼ŒåŠ å¿«å±¥è¡Œé€Ÿåº¦å¹¶æœ€å°åŒ–é”™è¯¯ã€‚\n* **å‘˜å·¥å…¥èŒå’ŒäººåŠ›èµ„æºç®¡ç†**ï¼šæœºå™¨äººå¯ä»¥ç®¡ç†ä¾‹è¡Œçš„äººåŠ›èµ„æºä»»åŠ¡ï¼Œå¦‚å…¥èŒã€å®‰æŽ’é¢è¯•å’Œæ›´æ–°å‘˜å·¥è®°å½•ï¼Œä½¿äººåŠ›èµ„æºå›¢é˜Ÿèƒ½å¤Ÿä¸“æ³¨äºŽæˆ˜ç•¥æ€§å·¥ä½œã€‚\n\n### æ™ºèƒ½è‡ªåŠ¨åŒ–å’ŒRPAçš„å¥½å¤„\n\n* **æˆæœ¬èŠ‚çº¦å’Œæ•ˆçŽ‡**ï¼šè‡ªåŠ¨åŒ–å‡å°‘äº†å¯¹äººå·¥å¹²é¢„çš„éœ€æ±‚ï¼Œä½¿å…¬å¸èƒ½å¤ŸèŠ‚çœåŠ³åŠ¨åŠ›æˆæœ¬å¹¶å°†èµ„æºé‡æ–°åˆ†é…åˆ°æ›´é«˜ä»·å€¼çš„ä»»åŠ¡ä¸Šã€‚\n* **å‡å°‘é”™è¯¯å’Œå¢žå¼ºåˆè§„æ€§**ï¼šæœºå™¨äººä»¥é«˜ç²¾åº¦æ‰§è¡Œä»»åŠ¡ï¼Œå‡å°‘äººä¸ºé”™è¯¯ï¼Œç‰¹åˆ«æ˜¯åœ¨é‡‘èžå’ŒåŒ»ç–—ç­‰æ•°æ®å¯†é›†åž‹é¢†åŸŸã€‚è‡ªåŠ¨åŒ–å·¥ä½œæµç¨‹è¿˜é€šè¿‡è®°å½•è¿‡ç¨‹å¹¶ç¡®ä¿éµå¾ªæ³•è§„æ¥æ”¯æŒåˆè§„æ€§ã€‚\n* **å¯æ‰©å±•æ€§**ï¼šRPAä½¿ä¼ä¸šèƒ½å¤Ÿåœ¨é«˜å³°æœŸè½»æ¾æ‰©å±•è¿è¥ï¼Œé€šè¿‡éƒ¨ç½²æ›´å¤šæœºå™¨äººè€Œæ— éœ€é›‡ä½£é¢å¤–å‘˜å·¥ï¼Œä»Žè€Œæä¾›èµ„æºç®¡ç†çš„çµæ´»æ€§ã€‚\n\n### æµè¡Œçš„RPAå’Œæ™ºèƒ½è‡ªåŠ¨åŒ–å·¥å…·\n\n* **UiPath**ï¼šä¸€ä¸ªå¹¿æ³›ä½¿ç”¨çš„RPAå¹³å°ï¼Œä»¥å…¶æ˜“ç”¨æ€§å’Œä¸Žæœºå™¨å­¦ä¹ å·¥å…·çš„å¼ºå¤§é›†æˆè€Œé—»åï¼Œé€‚ç”¨äºŽæ™ºèƒ½è‡ªåŠ¨åŒ–ã€‚\n* **Automation Anywhere**ï¼šæä¾›RPAå’Œè®¤çŸ¥è‡ªåŠ¨åŒ–åŠŸèƒ½ï¼Œé€‚åˆéœ€è¦å†³ç­–çš„å¤æ‚å·¥ä½œæµç¨‹ã€‚\n* **Blue Prism**ï¼šä¸“æ³¨äºŽå®‰å…¨çš„RPAéƒ¨ç½²ï¼Œç†æƒ³é€‚ç”¨äºŽå¯¹å®‰å…¨å’Œåˆè§„æ€§è¦æ±‚é«˜çš„è¡Œä¸šï¼Œå¦‚é“¶è¡Œå’ŒåŒ»ç–—ä¿å¥ã€‚\n\n### æ™ºèƒ½è‡ªåŠ¨åŒ–åœ¨å®žé™…ä¸­çš„åº”ç”¨å®žä¾‹\n\n* **ç”µä¿¡**ï¼šåƒAT\\&Tè¿™æ ·çš„å…¬å¸ä½¿ç”¨RPAæ¥è‡ªåŠ¨åŒ–å®¢æˆ·æœåŠ¡æŸ¥è¯¢ã€è´¦å•å¤„ç†å’Œç½‘ç»œç›‘æŽ§ï¼Œæé«˜æœåŠ¡å“åº”æ—¶é—´å’Œè¿è¥æ•ˆçŽ‡ã€‚\n* **é›¶å”®**ï¼šé›¶å”®å•†ä½¿ç”¨RPAæ¥ç®€åŒ–ä¾›åº”é“¾æ“ä½œï¼Œä»Žè®¢å•å±¥è¡Œåˆ°åº“å­˜ç®¡ç†ï¼Œå¸®åŠ©é™ä½Žè¿è¥æˆæœ¬å¹¶æé«˜å®¢æˆ·æ»¡æ„åº¦ã€‚\n* **åŒ»ç–—**ï¼šåŒ»é™¢å’ŒåŒ»ç–—æœåŠ¡æä¾›è€…å®žæ–½RPAæ¥å¤„ç†æ‚£è€…æ•°æ®ç®¡ç†ã€é¢„çº¦å®‰æŽ’å’Œç´¢èµ”å¤„ç†ï¼Œè®©åŒ»ç–—äººå‘˜èƒ½å¤Ÿä¸“æ³¨äºŽæ‚£è€…æŠ¤ç†ã€‚\n\n## 4\\. æ¬ºè¯ˆæ£€æµ‹ä¸Žå®‰å…¨å¢žå¼º\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*cFol51alN3qozxSH0rU0Bw.png)\n\nAIæ¬ºè¯ˆæ£€æµ‹ç³»ç»Ÿä½¿ç”¨æœºå™¨å­¦ä¹ ç®—æ³•åˆ†æžå¤§é‡æ•°æ®ï¼Œè¯†åˆ«å¼‚å¸¸æ¨¡å¼ï¼Œå¹¶æ ‡è®°å¯ç–‘æ´»åŠ¨ã€‚å…³é”®æŠ€æœ¯åŒ…æ‹¬ï¼š\n\n* **å¼‚å¸¸æ£€æµ‹**ï¼šè¯†åˆ«ä¸Žæ­£å¸¸è¡Œä¸ºçš„åå·®ï¼Œä¾‹å¦‚å¼‚å¸¸äº¤æ˜“æˆ–è´¦æˆ·æ´»åŠ¨ï¼Œè¿™å¯èƒ½è¡¨æ˜Žæ¬ºè¯ˆè¡Œä¸ºã€‚\n* **è¡Œä¸ºåˆ†æž**ï¼šç ”ç©¶ç”¨æˆ·è¡Œä¸ºä»¥å»ºç«‹åŸºçº¿ï¼Œå¹¶æ ¹æ®ç”¨æˆ·é€šå¸¸å¦‚ä½•ä¸Žç³»ç»Ÿäº’åŠ¨æ¥æ£€æµ‹å¼‚å¸¸ã€‚\n* **é¢„æµ‹å»ºæ¨¡**ï¼šå°†åŽ†å²æ•°æ®ä¸Žæœºå™¨å­¦ä¹ æ¨¡åž‹ç»“åˆï¼Œä»¥é¢„æµ‹æ¬ºè¯ˆäº‹ä»¶çš„å¯èƒ½æ€§ï¼Œå¹¶åœ¨æ¬ºè¯ˆå‘ç”Ÿä¹‹å‰è§¦å‘è­¦æŠ¥ã€‚\n\n### æ¬ºè¯ˆæ£€æµ‹å’Œå®‰å…¨å¢žå¼ºçš„å…³é”®ç”¨ä¾‹\n\n* **é‡‘èžæœåŠ¡**ï¼šé“¶è¡Œä½¿ç”¨äººå·¥æ™ºèƒ½å®žæ—¶ç›‘æŽ§äº¤æ˜“ï¼Œæ ‡è®°æ½œåœ¨çš„æ¬ºè¯ˆæ´»åŠ¨ï¼Œå¦‚å¼‚å¸¸çš„è´¦æˆ·è½¬è´¦æˆ–å–æ¬¾ï¼Œå¹¶åœ¨å®ŒæˆéªŒè¯ä¹‹å‰é˜»æ­¢è¿™äº›äº¤æ˜“ã€‚\n* **ä¿é™©ç´¢èµ”å¤„ç†**ï¼šä¿é™©å…¬å¸åˆ©ç”¨äººå·¥æ™ºèƒ½é€šè¿‡åˆ†æžç´¢èµ”æ¨¡å¼ã€è¯†åˆ«é‡å¤ç´¢èµ”å’Œå‘çŽ°æäº¤æ–‡ä»¶ä¸­çš„ä¸ä¸€è‡´æ¥æ£€æµ‹æ¬ºè¯ˆæ€§ç´¢èµ”ã€‚\n* **ç”µå­å•†åŠ¡å’Œé›¶å”®**ï¼šé›¶å”®å•†ä½¿ç”¨æ¬ºè¯ˆæ£€æµ‹ç®—æ³•æ¥è¯†åˆ«æ¬ºè¯ˆæ€§è´­ä¹°ã€è¢«ç›—ä¿¡ç”¨å¡äº¤æ˜“å’Œè´¦æˆ·æŽ¥ç®¡ï¼Œä»¥ä¿æŠ¤æ¶ˆè´¹è€…å’Œä¼ä¸šã€‚\n* **èº«ä»½éªŒè¯å’Œè®¿é—®æŽ§åˆ¶**ï¼šäººå·¥æ™ºèƒ½é€šè¿‡ç”Ÿç‰©è¯†åˆ«è®¤è¯ï¼ˆå¦‚é¢éƒ¨è¯†åˆ«å’ŒæŒ‡çº¹åˆ†æžï¼‰æ¥åŠ å¼ºå®‰å…¨æ€§ï¼Œé˜²æ­¢æœªç»æŽˆæƒçš„è®¿é—®ã€‚\n\n### AIåœ¨æ¬ºè¯ˆæ£€æµ‹å’Œå®‰å…¨ä¸­çš„å¥½å¤„\n\n* **å®žæ—¶å¨èƒæ£€æµ‹**ï¼šAIå…è®¸å³æ—¶å¨èƒæ£€æµ‹å’Œå“åº”ï¼Œé˜²æ­¢æ¬ºè¯ˆåœ¨å‡çº§ä¹‹å‰å‘ç”Ÿï¼Œæœ€å°åŒ–è´¢åŠ¡æŸå¤±ã€‚\n* **å‡å°‘è¯¯æŠ¥**ï¼šé€šè¿‡å­¦ä¹ ç”¨æˆ·æ¨¡å¼ï¼ŒAIå¯ä»¥å‡å°‘ä¼ ç»Ÿæ¬ºè¯ˆæ£€æµ‹ç³»ç»Ÿä¸­å¸¸ä¼´éšçš„è¯¯æŠ¥æ•°é‡ï¼Œç¡®ä¿çœŸæ­£çš„äº¤æ˜“ä¸ä¼šè¢«ä¸å¿…è¦åœ°æ ‡è®°ã€‚\n* **å¯¹æ–°å¨èƒçš„é€‚åº”æ€§æé«˜**ï¼šAIç³»ç»Ÿä¸æ–­ä»Žæ–°æ•°æ®ä¸­å­¦ä¹ ï¼Œä½¿å…¶èƒ½å¤Ÿé€‚åº”ä¸æ–­æ¼”å˜çš„æ¬ºè¯ˆç­–ç•¥ï¼Œè€ŒåŸºäºŽè§„åˆ™çš„ç³»ç»Ÿå¯èƒ½ä¼šå˜å¾—è¿‡æ—¶ã€‚\n\n### é¡¶çº§å·¥å…·å’Œå¹³å°ç”¨äºŽäººå·¥æ™ºèƒ½é©±åŠ¨çš„æ¬ºè¯ˆæ£€æµ‹\n\n* **Darktrace**ï¼šä½¿ç”¨äººå·¥æ™ºèƒ½æ£€æµ‹ã€è°ƒæŸ¥å’Œå“åº”ç½‘ç»œå¨èƒï¼Œåˆ©ç”¨æœºå™¨å­¦ä¹ å®žæ—¶ç†è§£å’Œä¿æŠ¤ç»„ç»‡ç½‘ç»œã€‚\n* **Splunk**ï¼šä¸€ç§å®‰å…¨ä¿¡æ¯å’Œäº‹ä»¶ç®¡ç†ï¼ˆSIEMï¼‰å·¥å…·ï¼Œå°†äººå·¥æ™ºèƒ½é©±åŠ¨çš„åˆ†æžä¸Žå®žæ—¶ç›‘æŽ§ç›¸ç»“åˆï¼Œä»¥æ£€æµ‹å’Œå“åº”å„ç§ç³»ç»Ÿä¸­çš„å¼‚å¸¸ã€‚\n* **Feedzai**ï¼šä¸ºé‡‘èžæœºæž„è®¾è®¡çš„å¹³å°ï¼Œåˆ©ç”¨æœºå™¨å­¦ä¹ ç›‘æŽ§äº¤æ˜“ï¼Œé˜²æ­¢æ¬ºè¯ˆï¼Œå¹¶ç¡®ä¿éµå®ˆç›‘ç®¡è¦æ±‚ã€‚\n\n### çŽ°å®žä¸–ç•Œä¸­çš„æ¬ºè¯ˆæ£€æµ‹å’Œå®‰å…¨å¢žå¼ºç¤ºä¾‹\n\n* **é“¶è¡Œ**ï¼šè®¸å¤šé“¶è¡ŒçŽ°åœ¨ä½¿ç”¨äººå·¥æ™ºèƒ½é©±åŠ¨çš„ç³»ç»Ÿæ¥æ£€æµ‹ä¿¡ç”¨å¡æ¬ºè¯ˆã€å¼‚å¸¸äº¤æ˜“å’Œè´¦æˆ·æŽ¥ç®¡ï¼Œæ˜¾è‘—å‡å°‘å› æ¬ºè¯ˆæ´»åŠ¨é€ æˆçš„è´¢åŠ¡æŸå¤±ã€‚\n* **åŒ»ç–—ä¿å¥**ï¼šäººå·¥æ™ºèƒ½è¢«ç”¨äºŽæ£€æµ‹æ¬ºè¯ˆæ€§åŒ»ç–—è´¦å•å’Œèº«ä»½ç›—çªƒï¼Œé€šè¿‡è¯†åˆ«ç´¢èµ”å¤„ç†ä¸­çš„æ»¥ç”¨æ¨¡å¼ï¼Œä¸ºåŒ»ç–—æä¾›è€…å’Œä¿é™©å…¬å¸èŠ‚çœäº†æ•°ç™¾ä¸‡ã€‚\n* **åœ¨çº¿å¹³å°**ï¼šç¤¾äº¤åª’ä½“å’Œåœ¨çº¿å¸‚åœºä½¿ç”¨äººå·¥æ™ºèƒ½æ£€æµ‹å’Œé˜²æ­¢è´¦æˆ·æŽ¥ç®¡ã€ç½‘ç»œé’“é±¼å’Œåžƒåœ¾é‚®ä»¶ï¼Œä¿æŠ¤ç”¨æˆ·å…å—å„ç§å®‰å…¨å¨èƒã€‚\n\n## 5\\. ä¾›åº”é“¾ä¼˜åŒ–ä¸Žäººå·¥æ™ºèƒ½\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*3pWU90X3MXQMyJ8G9rQ97g.png)\n\näººå·¥æ™ºèƒ½é©±åŠ¨çš„ä¾›åº”é“¾ä¼˜åŒ–æ­£åœ¨é€šè¿‡æé«˜æ•ˆçŽ‡ã€é™ä½Žæˆæœ¬å’Œå¢žå¼ºä¾›åº”é“¾å„ä¸ªé˜¶æ®µçš„é€æ˜Žåº¦æ¥æ”¹å˜ç‰©æµå’Œä¾›åº”é“¾ç®¡ç†ã€‚ä»Žéœ€æ±‚é¢„æµ‹åˆ°åº“å­˜ç®¡ç†ï¼Œäººå·¥æ™ºèƒ½ä½¿ä¼ä¸šèƒ½å¤Ÿåšå‡ºæ•°æ®é©±åŠ¨çš„å†³ç­–ï¼Œå¹¶ä¸»åŠ¨åº”å¯¹å¹²æ‰°ã€‚ä»¥ä¸‹æ˜¯äººå·¥æ™ºèƒ½å¦‚ä½•é©æ–°ä¾›åº”é“¾ç®¡ç†å’Œä¼˜åŒ–è¿è¥çš„æ·±å…¥åˆ†æžï¼š\n\n### AIå¦‚ä½•ä¼˜åŒ–ä¾›åº”é“¾\n\nAIå·¥å…·åˆ©ç”¨æœºå™¨å­¦ä¹ ã€é¢„æµ‹åˆ†æžå’Œå®žæ—¶æ•°æ®åˆ†æžæ¥ç®€åŒ–ä¾›åº”é“¾çš„å„ä¸ªæ–¹é¢ã€‚é€šè¿‡åˆ†æžåŽ†å²æ•°æ®ã€è·Ÿè¸ªå®žæ—¶ä¿¡æ¯å’Œé¢„æµ‹æœªæ¥è¶‹åŠ¿ï¼ŒAIç®—æ³•å¯ä»¥ä¼˜åŒ–é‡‡è´­ã€ç”Ÿäº§ã€åˆ†é”€å’Œç‰©æµæµç¨‹ã€‚ä¸»è¦æ–¹æ³•åŒ…æ‹¬ï¼š\n\n* **é¢„æµ‹åˆ†æž**ï¼šæ ¹æ®åŽ†å²æ•°æ®ã€å­£èŠ‚æ€§è¶‹åŠ¿å’Œå¸‚åœºå› ç´ é¢„æµ‹æœªæ¥éœ€æ±‚ã€‚\n* **éœ€æ±‚é¢„æµ‹çš„æœºå™¨å­¦ä¹ **ï¼šåˆ©ç”¨è¿‡åŽ»é”€å”®ã€å®¢æˆ·è¡Œä¸ºå’Œå¤–éƒ¨å› ç´ çš„æ•°æ®ç”Ÿæˆå‡†ç¡®çš„éœ€æ±‚é¢„æµ‹ã€‚\n* **å®žæ—¶ç›‘æŽ§å’Œç‰©è”ç½‘é›†æˆ**ï¼šé€šè¿‡ç‰©è”ç½‘ä¼ æ„Ÿå™¨å®žæ—¶è·Ÿè¸ªè´§ç‰©ï¼Œæé«˜å¯è§†æ€§å¹¶å¿«é€Ÿå“åº”é—®é¢˜ã€‚\n\n### AIåœ¨ä¾›åº”é“¾ä¼˜åŒ–ä¸­çš„å…³é”®åº”ç”¨æ¡ˆä¾‹\n\n* **éœ€æ±‚é¢„æµ‹ä¸Žè§„åˆ’**ï¼šåŸºäºŽAIçš„éœ€æ±‚é¢„æµ‹ä½¿å…¬å¸èƒ½å¤Ÿæ›´å‡†ç¡®åœ°é¢„æµ‹å®¢æˆ·éœ€æ±‚ï¼Œç¡®ä¿æœ€ä½³åº“å­˜æ°´å¹³ï¼Œå‡å°‘è¿‡å‰©æˆ–ç¼ºè´§çš„æƒ…å†µã€‚\n* **åº“å­˜ç®¡ç†ä¸Žä¼˜åŒ–**ï¼šAIå¸®åŠ©å…¬å¸é€šè¿‡é¢„æµ‹åº“å­˜éœ€æ±‚ã€ä¼˜åŒ–è¡¥è´§è®¡åˆ’å’Œæœ€å°åŒ–è¿‡å‰©åº“å­˜æ¥ç»´æŒé€‚å½“çš„åº“å­˜æ°´å¹³ã€‚\n* **ç‰©æµè·¯çº¿ä¼˜åŒ–**ï¼šAIç®—æ³•è€ƒè™‘äº¤é€šã€å¤©æ°”å’Œç‡ƒæ–™æˆæœ¬ï¼Œä»¥ç¡®å®šæœ€æœ‰æ•ˆçš„è·¯çº¿ï¼Œä»Žè€ŒèŠ‚çœæ—¶é—´å¹¶é™ä½Žè¿è¾“è´¹ç”¨ã€‚\n* **ä¾›åº”å•†é£Žé™©ç®¡ç†**ï¼šAIä½¿ä¼ä¸šèƒ½å¤Ÿè¯„ä¼°ä¾›åº”å•†çš„å¯é æ€§å’Œç»©æ•ˆï¼Œé¢„æµ‹ä¸Žä¾›åº”å•†å»¶è¿Ÿç›¸å…³çš„é£Žé™©ï¼Œå¹¶é€šè¿‡ä¸»åŠ¨ç­–ç•¥æ¥é™ä½Žè¿™äº›é£Žé™©ã€‚\n* **è´¨é‡æŽ§åˆ¶ä¸Žé¢„æµ‹æ€§ç»´æŠ¤**ï¼šAIå’Œç‰©è”ç½‘ä¼ æ„Ÿå™¨å¯ä»¥è¯†åˆ«åˆ¶é€ è®¾å¤‡ä¸­çš„æ¨¡å¼ï¼Œé¢„æµ‹ç»´æŠ¤éœ€æ±‚å¹¶å‡å°‘åœæœºæ—¶é—´ï¼Œä»Žè€Œæé«˜äº§å“è´¨é‡å¹¶å‡å°‘æµªè´¹ã€‚\n\n### AIé©±åŠ¨çš„ä¾›åº”é“¾ä¼˜åŒ–çš„å¥½å¤„\n\n* **æˆæœ¬é™ä½Ž**ï¼šé€šè¿‡æé«˜è·¯çº¿æ•ˆçŽ‡ã€åº“å­˜å‡†ç¡®æ€§å’Œéœ€æ±‚é¢„æµ‹ï¼ŒAIåœ¨æ•´ä¸ªä¾›åº”é“¾ä¸­æœ€å°åŒ–è¿è¥æˆæœ¬ã€‚\n* **å¢žå¼ºå†³ç­–èƒ½åŠ›**ï¼šAIæä¾›å®žæ—¶æ´žå¯Ÿå’Œé¢„æµ‹ï¼Œä½¿å†³ç­–è€…èƒ½å¤Ÿè¿…é€Ÿåº”å¯¹éœ€æ±‚ã€ä¾›åº”å•†è¡¨çŽ°æˆ–ç‰©æµæŒ‘æˆ˜çš„å˜åŒ–ã€‚\n* **æ›´å¤§çš„éŸ§æ€§å’Œçµæ´»æ€§**ï¼šAIé©±åŠ¨çš„ç³»ç»Ÿèƒ½å¤Ÿé€‚åº”æ„å¤–å¹²æ‰°ï¼Œä¾‹å¦‚ä¾›åº”å•†é—®é¢˜æˆ–éœ€æ±‚æ¿€å¢žï¼Œå³ä½¿åœ¨ä¸ç¡®å®šæ—¶æœŸä¹Ÿèƒ½ä¿æŒæœåŠ¡æ°´å¹³ã€‚\n* **å¯æŒç»­æ€§**ï¼šé€šè¿‡å‡å°‘æµªè´¹ã€ä¼˜åŒ–è·¯çº¿å’Œé˜²æ­¢è¿‡åº¦ç”Ÿäº§ï¼ŒAIæœ‰åŠ©äºŽå¯æŒç»­å‘å±•ç›®æ ‡ï¼Œå‡å°‘ä¼ä¸šå¯¹çŽ¯å¢ƒçš„å½±å“ã€‚\n\n### AIé©±åŠ¨çš„ä¾›åº”é“¾ä¼˜åŒ–çš„é¡¶çº§å·¥å…·å’Œå¹³å°\n\n* **SAPé›†æˆä¸šåŠ¡è§„åˆ’ (IBP)**ï¼šå°†æœºå™¨å­¦ä¹ ä¸Žä¾›åº”é“¾è§„åˆ’å·¥å…·ç›¸ç»“åˆï¼Œå®žçŽ°å®žæ—¶éœ€æ±‚é¢„æµ‹å’Œåº“å­˜ä¼˜åŒ–ã€‚\n* **Llamasoft (ç”±Coupaæä¾›)**ï¼šåˆ©ç”¨å…ˆè¿›çš„åˆ†æžè¿›è¡Œç½‘ç»œè®¾è®¡ã€åº“å­˜ä¼˜åŒ–å’Œéœ€æ±‚é¢„æµ‹ï¼Œä¸“æ³¨äºŽæˆ˜ç•¥å’Œè¿è¥ä¾›åº”é“¾è§„åˆ’ã€‚\n* **ClearMetal (ç”±Project44æä¾›)**ï¼šæä¾›å®žæ—¶ä¾›åº”é“¾å¯è§†åŒ–å’Œé¢„æµ‹åˆ†æžï¼Œä½¿å…¬å¸èƒ½å¤Ÿè·Ÿè¸ªè´§ç‰©ã€åˆ†æžè¶‹åŠ¿å¹¶ä¼˜åŒ–ç‰©æµè·¯çº¿ã€‚\n\n## 6\\. åŸºäºŽAIçš„äº§å“å¼€å‘ä¸Žè®¾è®¡\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*rcCCXcQq85tiFEe2M1gvzA.png)\n\nåŸºäºŽAIçš„äº§å“å¼€å‘ä¸Žè®¾è®¡æ­£åœ¨æ”¹å˜å…¬å¸æž„æ€ã€åˆ›å»ºå’ŒæŽ¨å‘å¸‚åœºäº§å“çš„æ–¹å¼ã€‚é€šè¿‡åˆ©ç”¨æœºå™¨å­¦ä¹ ã€ç”Ÿæˆè®¾è®¡å’Œé¢„æµ‹åˆ†æžï¼ŒAIä½¿å›¢é˜Ÿèƒ½å¤Ÿæ›´å¿«åœ°è¿›è¡Œåˆ›æ–°ï¼Œæé«˜äº§å“è´¨é‡ï¼Œå¹¶æœ‰æ•ˆåº”å¯¹ä¸æ–­å˜åŒ–çš„å®¢æˆ·éœ€æ±‚ã€‚ä»¥ä¸‹æ˜¯AIå¦‚ä½•å½»åº•æ”¹å˜äº§å“å¼€å‘ä¸Žè®¾è®¡çš„å…¨é¢åˆ†æžï¼š\n\n### AIåœ¨äº§å“å¼€å‘å’Œè®¾è®¡ä¸­çš„åº”ç”¨\n\nAIä¼˜åŒ–äº§å“ç”Ÿå‘½å‘¨æœŸçš„æ¯ä¸ªé˜¶æ®µï¼Œä»Žæž„æ€åˆ°è®¾è®¡ã€åŽŸåž‹åˆ¶ä½œå’Œæµ‹è¯•ã€‚åˆ©ç”¨å…ˆè¿›çš„ç®—æ³•å’Œæ•°æ®åˆ†æžï¼ŒAIé©±åŠ¨çš„ç³»ç»Ÿå¯ä»¥ç”Ÿæˆäº§å“æ¦‚å¿µï¼Œè¯†åˆ«è®¾è®¡ç¼ºé™·ï¼Œç”šè‡³æå‡ºæ”¹è¿›å»ºè®®ã€‚å…³é”®çš„AIæ–¹æ³•åŒ…æ‹¬ï¼š\n\n* **ç”Ÿæˆè®¾è®¡**ï¼šAIæ ¹æ®é¢„å®šä¹‰çš„å‚æ•°ç”Ÿæˆè®¾è®¡é€‰é¡¹ï¼Œä¾‹å¦‚ææ–™ç±»åž‹ã€é‡é‡å’Œè€ç”¨æ€§ï¼Œä½¿è®¾è®¡å¸ˆèƒ½å¤ŸæŽ¢ç´¢æ›´å¹¿æ³›çš„å¯èƒ½æ€§ã€‚\n* **å®¢æˆ·æ´žå¯Ÿçš„é¢„æµ‹åˆ†æž**ï¼šæœºå™¨å­¦ä¹ ç®—æ³•åˆ†æžå®¢æˆ·æ•°æ®å’Œå¸‚åœºè¶‹åŠ¿ï¼Œä»¥é¢„æµ‹å“ªäº›åŠŸèƒ½æˆ–è®¾è®¡ä¼šä¸Žç”¨æˆ·äº§ç”Ÿå…±é¸£ï¼Œä»Žè€Œå¢žå¼ºäº§å“å¸‚åœºå¥‘åˆåº¦ã€‚\n* **è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰**ï¼šNLPä½¿å›¢é˜Ÿèƒ½å¤Ÿåˆ†æžå®¢æˆ·åé¦ˆã€è¯„è®ºå’Œç¤¾äº¤åª’ä½“ï¼Œä»¥èŽ·å–å¯ä»¥æŒ‡å¯¼äº§å“æ”¹è¿›å’Œæ–°åŠŸèƒ½åˆ›æ„çš„æ´žå¯Ÿã€‚\n\n### AIåœ¨äº§å“å¼€å‘å’Œè®¾è®¡ä¸­çš„å…³é”®åº”ç”¨æ¡ˆä¾‹\n\n* **æ¦‚å¿µç”Ÿæˆå’Œè®¾è®¡æŽ¢ç´¢**ï¼šAIé€šè¿‡æ ¹æ®ç‰¹å®šçº¦æŸç”Ÿæˆå¤šä¸ªè®¾è®¡æ›¿ä»£æ–¹æ¡ˆæ¥ååŠ©è®¾è®¡å¸ˆã€‚ä¾‹å¦‚ï¼Œåœ¨æ±½è½¦è¡Œä¸šï¼ŒAIå¯ä»¥ç”Ÿæˆæ”¹å–„ç‡ƒæ²¹æ•ˆçŽ‡çš„ç©ºæ°”åŠ¨åŠ›å­¦è®¾è®¡ã€‚\n* **åŽŸåž‹åˆ¶ä½œå’Œå¿«é€Ÿè¿­ä»£**ï¼šAIé©±åŠ¨çš„å·¥å…·èƒ½å¤Ÿå¿«é€Ÿè¿›è¡Œè™šæ‹ŸåŽŸåž‹åˆ¶ä½œï¼Œå‡å°‘å¯¹ç‰©ç†æ¨¡åž‹çš„éœ€æ±‚ï¼Œå¹¶å…è®¸å¿«é€Ÿæµ‹è¯•è®¾è®¡å˜ä½“ã€‚\n* **è´¨é‡ä¿è¯å’Œæµ‹è¯•**ï¼šAIç®—æ³•åˆ†æžäº§å“æ•°æ®ï¼Œä»¥æ—©æœŸæ£€æµ‹ç¼ºé™·ï¼Œé¢„æµ‹æ½œåœ¨æ•…éšœï¼Œå¹¶å»ºè®®ä¿®æ”¹ï¼Œä»Žè€Œæé«˜è´¨é‡å¹¶é™ä½Žä¸Žå¬å›žç›¸å…³çš„æˆæœ¬ã€‚\n* **ä»¥å®¢æˆ·ä¸ºä¸­å¿ƒçš„äº§å“å®šåˆ¶**ï¼šAIé©±åŠ¨çš„å¹³å°å…è®¸å…¬å¸é€šè¿‡åˆ†æžåå¥½å¹¶ç”Ÿæˆä¸ªæ€§åŒ–åŠŸèƒ½æˆ–æŽ¨èï¼Œåˆ›å»ºé‡èº«å®šåˆ¶çš„äº§å“ã€‚\n* **æˆæœ¬ä¼˜åŒ–**ï¼šAIè¯†åˆ«ææ–™ã€åˆ¶é€ è¿‡ç¨‹å’Œä¾›åº”é“¾ä¸­çš„èŠ‚çœæˆæœ¬æœºä¼šï¼Œè€Œä¸å½±å“äº§å“è´¨é‡ï¼Œä»Žè€Œæé«˜ç›ˆåˆ©èƒ½åŠ›ã€‚\n\n### AIé©±åŠ¨çš„äº§å“å¼€å‘å’Œè®¾è®¡çš„å¥½å¤„\n\n* **åŠ é€Ÿä¸Šå¸‚æ—¶é—´**ï¼šAIå‡å°‘äº†äº§å“è®¾è®¡ã€åŽŸåž‹åˆ¶ä½œå’Œæµ‹è¯•æ‰€éœ€çš„æ—¶é—´ï¼Œä½¿å…¬å¸èƒ½å¤Ÿæ›´å¿«åœ°å°†äº§å“æŽ¨å‘å¸‚åœºï¼Œå¹¶æŠ“ä½æ–°å…´è¶‹åŠ¿ã€‚\n* **å¢žå¼ºåˆ›æ–°å’Œåˆ›é€ åŠ›**ï¼šç”Ÿæˆè®¾è®¡å’Œæ•°æ®é©±åŠ¨çš„æ´žå¯ŸåŠ›æ¿€å‘æ–°æƒ³æ³•ï¼Œä½¿è®¾è®¡å¸ˆèƒ½å¤Ÿåˆ›é€ å‡ºæ›´å…·åˆ›æ–°æ€§å’Œä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„äº§å“ã€‚\n* **æé«˜äº§å“è´¨é‡**ï¼šAIæ—©æœŸè¯†åˆ«æ½œåœ¨çš„è®¾è®¡ç¼ºé™·ï¼Œæé«˜äº§å“è´¨é‡å’Œå¯é æ€§ï¼Œä»Žè€Œå¸¦æ¥æ›´é«˜çš„å®¢æˆ·æ»¡æ„åº¦ã€‚\n* **é™ä½Žå¼€å‘æˆæœ¬**ï¼šé€šè¿‡å‡å°‘å¯¹ç‰©ç†åŽŸåž‹çš„éœ€æ±‚å’Œç®€åŒ–è®¾è®¡è¿‡ç¨‹ï¼ŒAIæœ‰åŠ©äºŽé™ä½Žæ•´ä½“å¼€å‘æˆæœ¬å’Œèµ„æºã€‚\n\n### AIé©±åŠ¨çš„äº§å“å¼€å‘å’Œè®¾è®¡çš„é¡¶å°–å·¥å…·\n\n* **Autodeskçš„Fusion 360ä¸Žç”Ÿæˆè®¾è®¡**ï¼šæä¾›ç”Ÿæˆè®¾è®¡å·¥å…·ï¼Œç”¨äºŽåˆ›å»ºä¼˜åŒ–ç»“æž„å’Œè½»é‡åŒ–è®¾è®¡ï¼Œå¸¸ç”¨äºŽæ±½è½¦ã€èˆªç©ºèˆªå¤©å’Œå·¥ä¸šè®¾è®¡ã€‚\n* **SolidWorksä¸ŽAIæ’ä»¶**ï¼šé›†æˆäº†AIé©±åŠ¨çš„æ¨¡å—ï¼Œç”¨äºŽå¿«é€ŸåŽŸåž‹åˆ¶ä½œã€ææ–™ä¼˜åŒ–å’Œè®¾è®¡éªŒè¯ã€‚\n* **Canva AIè®¾è®¡å·¥å…·**ï¼šåˆ©ç”¨AIç”Ÿæˆå¸ƒå±€å»ºè®®ï¼Œä¼˜åŒ–è‰²å½©æ­é…ï¼Œå¹¶å»ºè®®è®¾è®¡æ¨¡æ¿ï¼Œéžå¸¸é€‚åˆæ•°å­—å†…å®¹å’Œå“ç‰Œè®¾è®¡ã€‚\n\n### çŽ°å®žä¸–ç•Œä¸­ AI åœ¨äº§å“å¼€å‘ä¸­çš„åº”ç”¨å®žä¾‹\n\n* **æ±½è½¦å’Œèˆªç©ºèˆªå¤©**ï¼šåƒç©ºå®¢è¿™æ ·çš„å…¬å¸ä½¿ç”¨ AI é©±åŠ¨çš„ç”Ÿæˆè®¾è®¡æ¥åˆ›å»ºè½»é‡åŒ–ã€ç©ºæ°”åŠ¨åŠ›å­¦çš„ç»„ä»¶ï¼Œä»Žè€Œå‡å°‘ç‡ƒæ–™æ¶ˆè€—å¹¶æ”¹å–„å¯æŒç»­æ€§ã€‚\n* **æ¶ˆè´¹ç”µå­**ï¼šç§‘æŠ€å·¨å¤´åˆ©ç”¨ AI è¯†åˆ«åŠŸèƒ½åå¥½ï¼Œä½¿å¾—åŸºäºŽç”¨æˆ·æ•°æ®åˆ›å»ºå®šåˆ¶åŠŸèƒ½æˆä¸ºå¯èƒ½ï¼Œä¾‹å¦‚çœç”µæ¨¡å¼æˆ–ä¸ªæ€§åŒ–è®¾ç½®ã€‚\n* **æ—¶å°šå’Œæœè£…**ï¼šå“ç‰Œä½¿ç”¨ AI æ ¹æ®èº«ä½“æµ‹é‡è®¾è®¡å®šåˆ¶åˆèº«çš„æœè£…ï¼Œå¹¶åˆ†æžé¢œè‰²å’Œé£Žæ ¼è¶‹åŠ¿ä»¥é¢„æµ‹æœªæ¥éœ€æ±‚ï¼Œå¸®åŠ©å“ç‰Œå‡å°‘æœªå”®å‡ºåº“å­˜ã€‚\n\n## 7\\. åŠ³åŠ¨åŠ›å’Œäººæ‰ç®¡ç†ä¸Žäººå·¥æ™ºèƒ½\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*-zSlQl-pe5KJ2DuACUEeeA.png)\n\näººå·¥æ™ºèƒ½æ•´åˆäº†æœºå™¨å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œæ•°æ®åˆ†æžï¼Œä»¥æ”¹å–„åŠ³åŠ¨åŠ›ç®¡ç†çš„å„ä¸ªæ–¹é¢ï¼ŒåŒ…æ‹¬æ‹›è˜ã€ç»©æ•ˆç®¡ç†ã€åŸ¹è®­å’Œå‘˜å·¥å‚ä¸Žã€‚äººå·¥æ™ºèƒ½åº”ç”¨çš„å…³é”®é¢†åŸŸåŒ…æ‹¬ï¼š\n\n* **æ‹›è˜å’Œäººæ‰èŽ·å–**ï¼šäººå·¥æ™ºèƒ½è‡ªåŠ¨åŒ–ç­›é€‰è¿‡ç¨‹ï¼Œåˆ†æžç®€åŽ†å’Œç”³è¯·ï¼Œä»¥æ ¹æ®å€™é€‰äººçš„æŠ€èƒ½ã€ç»éªŒå’Œæ½œåŠ›åŒ¹é…æœ€ä½³èŒä½ã€‚\n* **å‘˜å·¥åŸ¹è®­å’Œå‘å±•**ï¼šäººå·¥æ™ºèƒ½ç³»ç»Ÿæä¾›ä¸ªæ€§åŒ–å­¦ä¹ ä½“éªŒï¼Œæ ¹æ®å‘˜å·¥çš„ç»©æ•ˆã€èŒä¸šç›®æ ‡å’Œå…´è¶£æŽ¨èè¯¾ç¨‹å’ŒåŸ¹è®­é¡¹ç›®ã€‚\n* **ç»©æ•ˆç›‘æµ‹å’Œåé¦ˆ**ï¼šäººå·¥æ™ºèƒ½æä¾›å®žæ—¶çš„å‘˜å·¥ç»©æ•ˆå’Œå‚ä¸Žåº¦æ´žå¯Ÿï¼Œæä¾›æŒç»­åé¦ˆä»¥å¸®åŠ©æé«˜ç”Ÿäº§åŠ›å¹¶è¯†åˆ«æˆé•¿é¢†åŸŸã€‚\n* **åŠ³åŠ¨åŠ›è§„åˆ’å’Œåˆ†æž**ï¼šäººå·¥æ™ºèƒ½åˆ†æžåŠ³åŠ¨åŠ›è¶‹åŠ¿ã€ç»©æ•ˆæ•°æ®å’Œå¸‚åœºæ¡ä»¶ï¼Œä»¥å¸®åŠ©ç»„ç»‡è§„åˆ’å’Œä¼˜åŒ–å…¶äººåŠ›éœ€æ±‚ã€‚\n\n### AIåœ¨åŠ³åŠ¨åŠ›å’Œäººæ‰ç®¡ç†ä¸­çš„å…³é”®åº”ç”¨æ¡ˆä¾‹\n\n* **AIé©±åŠ¨çš„æ‹›è˜**: AIç®—æ³•è‡ªåŠ¨è§£æžç®€åŽ†ã€è¿›è¡Œå€™é€‰äººæŽ’åå’Œåˆæ­¥é¢è¯•è¯„ä¼°ï¼Œç¡®ä¿æ›´å¿«ã€æ— åè§çš„æ‹›è˜æµç¨‹å’Œæ›´å¥½çš„å€™é€‰äººä½“éªŒã€‚\n* **å‘˜å·¥å’¨è¯¢èŠå¤©æœºå™¨äºº**: AIèŠå¤©æœºå™¨äººååŠ©å‘˜å·¥å¤„ç†ä¸ŽäººåŠ›èµ„æºç›¸å…³çš„é—®é¢˜ï¼Œå¦‚ç¦åˆ©ã€è–ªèµ„å’Œå…¬å¸æ”¿ç­–ï¼Œæé«˜äººåŠ›èµ„æºæœåŠ¡æ•ˆçŽ‡ï¼Œå¹¶ä½¿äººåŠ›èµ„æºå‘˜å·¥èƒ½å¤Ÿä¸“æ³¨äºŽæ›´å¤æ‚çš„ä»»åŠ¡ã€‚\n* **ä¸ªæ€§åŒ–å‘˜å·¥å‘å±•**: AIé©±åŠ¨çš„å­¦ä¹ ç®¡ç†ç³»ç»Ÿï¼ˆLMSï¼‰åˆ†æžå‘˜å·¥çš„è¡¨çŽ°å’Œå­¦ä¹ åå¥½ï¼ŒæŽ¨èé‡èº«å®šåˆ¶çš„å‘å±•è·¯å¾„ã€è¯¾ç¨‹å’Œè®¤è¯ï¼Œä»¥æå‡æŠ€èƒ½ã€‚\n* **å‘˜å·¥å‚ä¸Žåº¦æƒ…æ„Ÿåˆ†æž**: AIåˆ†æžå‘˜å·¥åé¦ˆã€è°ƒæŸ¥å’Œæ²Ÿé€šæ¨¡å¼ï¼Œä»¥è¯„ä¼°å£«æ°”ã€è¯†åˆ«æ½œåœ¨é—®é¢˜ï¼Œå¹¶æä¾›æ”¹å–„å·¥ä½œåœºæ‰€æ–‡åŒ–å’Œå‚ä¸Žåº¦çš„è§è§£ã€‚\n* **åŠ³åŠ¨åŠ›ä¼˜åŒ–**: AIå·¥å…·åˆ†æžåŠ³åŠ¨åŠ›æ•°æ®ï¼Œå¸®åŠ©äººåŠ›èµ„æºç»ç†ä¼˜åŒ–æŽ’ç­ã€ç­æ¬¡æ¨¡å¼å’Œèµ„æºé…ç½®ï¼Œæé«˜ç”Ÿäº§åŠ›å’Œå‘˜å·¥æ»¡æ„åº¦ï¼ŒåŒæ—¶é™ä½Žæˆæœ¬ã€‚\n\n### AIåœ¨åŠ³åŠ¨åŠ›å’Œäººæ‰ç®¡ç†ä¸­çš„å¥½å¤„\n\n* **æé«˜æ•ˆçŽ‡**ï¼šAIè‡ªåŠ¨åŒ–å¸¸è§„çš„äººåŠ›èµ„æºä»»åŠ¡ï¼Œå¦‚ç­›é€‰ç®€åŽ†ã€å®‰æŽ’é¢è¯•å’Œå›žç­”å‘˜å·¥æŸ¥è¯¢ï¼Œä½¿äººåŠ›èµ„æºä¸“ä¸šäººå‘˜èƒ½å¤ŸèŠ‚çœæ—¶é—´ï¼Œä¸“æ³¨äºŽæˆ˜ç•¥å†³ç­–ã€‚\n* **å¢žå¼ºå†³ç­–èƒ½åŠ›**ï¼šAIé©±åŠ¨çš„åˆ†æžä¸ºäººåŠ›èµ„æºç»ç†æä¾›å¯æ“ä½œçš„æ´žå¯Ÿï¼Œå¸®åŠ©ä»–ä»¬åœ¨æ‹›è˜ã€æ™‹å‡å’Œèµ„æºåˆ†é…æ–¹é¢åšå‡ºæ›´æ˜Žæ™ºçš„å†³ç­–ã€‚\n* **æ”¹å–„äººæ‰æ‹›è˜**ï¼šAIé€šè¿‡æ¶ˆé™¤æ‹›è˜ä¸­çš„åè§ï¼Œå¸®åŠ©æ›´å¿«åœ°è¯†åˆ«æœ€ä½³å€™é€‰äººï¼Œæé«˜å¤šæ ·æ€§ï¼Œå¹¶ç¡®ä¿å€™é€‰äººä¸Žè§’è‰²ä¹‹é—´çš„æ›´å¥½åŒ¹é…ã€‚\n* **ä¸ªæ€§åŒ–å‘˜å·¥ä½“éªŒ**ï¼šAIé©±åŠ¨çš„å·¥å…·æä¾›ä¸ªæ€§åŒ–çš„èŒä¸šå‘å±•å’ŒåŸ¹è®­é¡¹ç›®ï¼Œé€šè¿‡å°†ä¸ªäººç›®æ ‡ä¸Žç»„ç»‡ç›®æ ‡å¯¹é½ï¼Œæé«˜å‘˜å·¥æ»¡æ„åº¦å’Œç•™ä»»çŽ‡ã€‚\n* **ä¸»åŠ¨ç•™ä»»ç­–ç•¥**ï¼šé€šè¿‡åˆ†æžå‘˜å·¥æƒ…ç»ªå’Œç»©æ•ˆæ•°æ®ï¼ŒAIå¯ä»¥é¢„æµ‹æ½œåœ¨çš„ç¦»èŒæƒ…å†µï¼Œä½¿äººåŠ›èµ„æºèƒ½å¤ŸåŠæ—©ä»‹å…¥å®žæ–½ç•™ä»»ç­–ç•¥ã€‚\n\n### äººå·¥æ™ºèƒ½åœ¨åŠ³åŠ¨åŠ›å’Œäººæ‰ç®¡ç†ä¸­çš„é¡¶çº§å·¥å…·\n\n* **HireVue**ï¼šä¸€ä¸ªåŸºäºŽäººå·¥æ™ºèƒ½çš„è§†é¢‘é¢è¯•å¹³å°ï¼Œåˆ†æžå€™é€‰äººçš„å›žç­”ã€è¯­è°ƒå’Œé¢éƒ¨è¡¨æƒ…ï¼Œä»¥è¯„ä¼°èµ„æ ¼ã€ä¸ªæ€§å’Œä¸ŽèŒä½çš„å¥‘åˆåº¦ã€‚\n* **Workday**ï¼šä¸€æ¬¾äººåŠ›èµ„æœ¬ç®¡ç†è½¯ä»¶ï¼Œåˆ©ç”¨äººå·¥æ™ºèƒ½è¿›è¡Œæ‹›è˜ã€ç»©æ•ˆç®¡ç†ã€äººæ‰å‘å±•å’ŒåŠ³åŠ¨åŠ›è§„åˆ’ï¼Œå¸®åŠ©ä¼ä¸šåšå‡ºæ•°æ®é©±åŠ¨çš„äººåŠ›èµ„æºå†³ç­–ã€‚\n* **Ultimate Software**ï¼šæä¾›ç”¨äºŽè–ªèµ„ã€å‘˜å·¥å‚ä¸Žå’Œç»©æ•ˆç®¡ç†çš„äººå·¥æ™ºèƒ½å·¥å…·ï¼Œæé«˜äººåŠ›èµ„æºæ•ˆçŽ‡ï¼Œå¹¶æä¾›å…³äºŽå‘˜å·¥ç»©æ•ˆå’Œæƒ…ç»ªçš„å¯æ“ä½œæ´žå¯Ÿã€‚\n* **Pymetrics**ï¼šåˆ©ç”¨äººå·¥æ™ºèƒ½æ ¹æ®è®¤çŸ¥å’Œæƒ…æ„Ÿèƒ½åŠ›å°†å€™é€‰äººä¸ŽèŒä½åŒ¹é…ï¼Œå¸®åŠ©å…¬å¸æ”¹å–„å¤šæ ·æ€§å¹¶å‡å°‘æ‹›è˜åè§ã€‚\n\n### äººå·¥æ™ºèƒ½åœ¨åŠ³åŠ¨åŠ›å’Œäººæ‰ç®¡ç†ä¸­çš„å®žé™…æ¡ˆä¾‹\n\n* **è”åˆåˆ©åŽçš„æ‹›è˜**ï¼šè”åˆåˆ©åŽåˆ©ç”¨äººå·¥æ™ºèƒ½é©±åŠ¨çš„å·¥å…·æ¥è‡ªåŠ¨åŒ–å€™é€‰äººç­›é€‰ã€é¢è¯•ï¼Œæé«˜é€Ÿåº¦å¹¶å‡å°‘æ‹›è˜è¿‡ç¨‹ä¸­çš„æ— æ„è¯†åè§ã€‚\n* **IBMçš„ç»©æ•ˆç®¡ç†**ï¼šIBMçš„äººå·¥æ™ºèƒ½é©±åŠ¨çš„ç»©æ•ˆç®¡ç†ç³»ç»Ÿåˆ†æžå‘˜å·¥æ•°æ®å¹¶æä¾›ä¸ªæ€§åŒ–åé¦ˆï¼Œå¸®åŠ©ç®¡ç†è€…è·Ÿè¸ªå‘˜å·¥ç»©æ•ˆå¹¶è¯†åˆ«å‘å±•æœºä¼šã€‚\n* **åŸƒæ£®å“²çš„å‘˜å·¥å‚ä¸Žåº¦**ï¼šåŸƒæ£®å“²åˆ©ç”¨äººå·¥æ™ºèƒ½é©±åŠ¨çš„æƒ…æ„Ÿåˆ†æžæ¥ç›‘æµ‹å‘˜å·¥å‚ä¸Žåº¦ï¼Œæ”¶é›†åé¦ˆï¼Œå¹¶åˆ¶å®šæ”¹å–„å·¥ä½œåœºæ‰€æ–‡åŒ–çš„ç­–ç•¥ã€‚\n\n## 8\\. äººå·¥æ™ºèƒ½åœ¨è´¢åŠ¡è§„åˆ’å’Œé¢„æµ‹ä¸­çš„åº”ç”¨\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*2QSFmYDlYv1eqjTXRWApNw.png)\n\näººå·¥æ™ºèƒ½åˆ©ç”¨å¤§é‡çš„è´¢åŠ¡æ•°æ®å’Œå…ˆè¿›çš„ç®—æ³•æ¥åˆ›å»ºæ›´å‡†ç¡®ã€åŠ¨æ€å’Œå¯é çš„è´¢åŠ¡æ¨¡åž‹ã€‚é€šè¿‡è‡ªåŠ¨åŒ–é‡å¤ä»»åŠ¡ã€æé«˜é¢„æµ‹å‡†ç¡®æ€§å’Œæä¾›å¯æ“ä½œçš„æ´žå¯Ÿï¼Œäººå·¥æ™ºèƒ½æ­£åœ¨å¸®åŠ©ç»„ç»‡åšå‡ºæ›´æ˜Žæ™ºçš„è´¢åŠ¡å†³ç­–ã€‚å…³é”®çš„äººå·¥æ™ºèƒ½åº”ç”¨åŒ…æ‹¬ï¼š\n\n* **é¢„æµ‹åˆ†æž**ï¼šäººå·¥æ™ºèƒ½é€šè¿‡åˆ†æžåŽ†å²æ•°æ®ã€è¯†åˆ«è¶‹åŠ¿å’Œè¯„ä¼°å¸‚åœºæ¡ä»¶æ¥é¢„æµ‹æœªæ¥çš„è´¢åŠ¡è¡¨çŽ°ã€‚è¿™ä½¿å¾—æ›´å‡†ç¡®çš„é¢„æµ‹å’Œé¢„ç®—æˆä¸ºå¯èƒ½ã€‚\n* **å®žæ—¶è´¢åŠ¡ç›‘æŽ§**ï¼šåŸºäºŽäººå·¥æ™ºèƒ½çš„å·¥å…·æä¾›è´¢åŠ¡æ•°æ®çš„å®žæ—¶è·Ÿè¸ªï¼Œå¸®åŠ©ä¼ä¸šå¿«é€Ÿæ£€æµ‹å¼‚å¸¸ã€é¢„æµ‹çŽ°é‡‘æµå¹¶ç›‘æŽ§é¢„ç®—éµå¾ªæƒ…å†µã€‚\n* **æƒ…æ™¯è§„åˆ’**ï¼šäººå·¥æ™ºèƒ½å¯ä»¥æ¨¡æ‹Ÿä¸åŒçš„ç»æµŽæ¡ä»¶æˆ–å•†ä¸šæƒ…æ™¯ï¼Œå¸®åŠ©è´¢åŠ¡è§„åˆ’è€…è¯„ä¼°å„ç§ç­–ç•¥å’Œå†³ç­–å¯¹å…¬å¸åº•çº¿çš„å½±å“ã€‚\n\n### AIåœ¨è´¢åŠ¡è§„åˆ’å’Œé¢„æµ‹ä¸­çš„å…³é”®ç”¨ä¾‹\n\n* **é¢„ç®—å’ŒçŽ°é‡‘æµé¢„æµ‹**ï¼šAIå·¥å…·åˆ†æžè¿‡åŽ»çš„è´¢åŠ¡æ•°æ®ã€å®¢æˆ·è¡Œä¸ºå’Œå¸‚åœºæ¡ä»¶ï¼Œä»¥ç”ŸæˆçŽ°é‡‘æµé¢„æµ‹ï¼Œç¡®ä¿ä¼ä¸šæ‹¥æœ‰è¶³å¤Ÿçš„æµåŠ¨æ€§å¹¶èƒ½å¤Ÿè§„åˆ’æœªæ¥çš„æ”¯å‡ºã€‚\n* **æ”¶å…¥é¢„æµ‹**ï¼šAIæ¨¡åž‹å¯ä»¥æ ¹æ®åŽ†å²è¡¨çŽ°ã€å­£èŠ‚æ€§è¶‹åŠ¿å’Œå®¢æˆ·è¡Œä¸ºé¢„æµ‹æœªæ¥æ”¶å…¥ï¼Œä½¿å…¬å¸èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ†é…èµ„æºã€‚\n* **è´¹ç”¨ç®¡ç†å’Œä¼˜åŒ–**ï¼šAIé€šè¿‡åˆ†æžæ”¯å‡ºæ¨¡å¼å¹¶å»ºè®®èŠ‚çœæˆæœ¬çš„æŽªæ–½ï¼Œè¯†åˆ«å¯ä»¥å‡å°‘æˆ–ä¼˜åŒ–çš„æˆæœ¬é¢†åŸŸã€‚\n* **ä¿¡ç”¨é£Žé™©è¯„ä¼°**ï¼šAIé©±åŠ¨çš„ä¿¡ç”¨è¯„åˆ†æ¨¡åž‹åˆ†æžå¹¿æ³›çš„è´¢åŠ¡å’Œéžè´¢åŠ¡æ•°æ®ï¼Œä¸ºè´·æ–¹ã€æŠ•èµ„è€…å’Œä¼ä¸šæä¾›æ›´å‡†ç¡®çš„é£Žé™©è¯„ä¼°ã€‚\n* **æŠ•èµ„åˆ†æžå’ŒæŠ•èµ„ç»„åˆç®¡ç†**ï¼šAIå·¥å…·åˆ†æžå¤§é‡å¸‚åœºæ•°æ®ï¼Œä»¥è¯†åˆ«æŠ•èµ„æœºä¼šã€è¯„ä¼°æŠ•èµ„ç»„åˆè¡¨çŽ°å¹¶ä¼˜åŒ–èµ„äº§é…ç½®ã€‚\n\n### AIåœ¨è´¢åŠ¡è§„åˆ’å’Œé¢„æµ‹ä¸­çš„å¥½å¤„\n\n* **æé«˜å‡†ç¡®æ€§**ï¼šAIç®—æ³•å¤„ç†å¤§é‡æ•°æ®é›†å¹¶è¯†åˆ«äººç±»å¯èƒ½å¿½è§†çš„æ¨¡å¼ï¼Œä»Žè€Œå¯¼è‡´æ›´å‡†ç¡®çš„è´¢åŠ¡é¢„æµ‹ã€é¢„æµ‹å’Œé¢„ç®—ã€‚\n* **åŠ å¿«å†³ç­–**ï¼šAIåŠ é€Ÿè´¢åŠ¡åˆ†æžï¼Œä½¿å†³ç­–æ›´å¿«ï¼Œèƒ½å¤Ÿæ›´çµæ´»åœ°åº”å¯¹å¸‚åœºå˜åŒ–ã€çŽ°é‡‘æµéœ€æ±‚æˆ–æ„å¤–çš„è´¢åŠ¡äº‹ä»¶ã€‚\n* **æé«˜æ•ˆçŽ‡**ï¼šé€šè¿‡è‡ªåŠ¨åŒ–æ•°æ®å½•å…¥ã€åˆ†æžå’ŒæŠ¥å‘Šç”Ÿæˆç­‰æ—¥å¸¸ä»»åŠ¡ï¼ŒAIä¸ºè´¢åŠ¡ä¸“ä¸šäººå£«è…¾å‡ºæ—¶é—´ï¼Œä¸“æ³¨äºŽæˆ˜ç•¥å†³ç­–å’Œæƒ…æ™¯è§„åˆ’ã€‚\n* **é£Žé™©ç®¡ç†**ï¼šAIåˆ†æžå¤§é‡æ•°æ®çš„èƒ½åŠ›æœ‰åŠ©äºŽåŠæ—©è¯†åˆ«æ½œåœ¨é£Žé™©å’Œè´¢åŠ¡æŒ‘æˆ˜ï¼Œä½¿å…¬å¸èƒ½å¤Ÿé‡‡å–ä¸»åŠ¨æŽªæ–½æ¥å‡è½»è¿™äº›é£Žé™©ã€‚\n* **å¢žå¼ºæˆ˜ç•¥æ´žå¯Ÿ**ï¼šAIæä¾›å¯¹è´¢åŠ¡æ•°æ®çš„æ›´æ·±åˆ»æ´žå¯Ÿï¼Œæä¾›å…³äºŽèŠ‚çœæˆæœ¬æœºä¼šã€æ”¶å…¥å¢žé•¿ç­–ç•¥å’Œæ•´ä½“è´¢åŠ¡å¥åº·çš„å»ºè®®ã€‚\n\n### AIåœ¨è´¢åŠ¡è§„åˆ’å’Œé¢„æµ‹ä¸­çš„é¡¶å°–å·¥å…·\n\n* **Adaptive Insights**: ä¸€æ¬¾åŸºäºŽäº‘çš„è´¢åŠ¡è§„åˆ’å·¥å…·ï¼Œåˆ©ç”¨AIå¸®åŠ©ä¼ä¸šè¿›è¡Œè´¢åŠ¡åœºæ™¯çš„é¢„æµ‹å’Œå»ºæ¨¡ï¼Œè·Ÿè¸ªç»©æ•ˆï¼Œå¹¶æ ¹æ®å®žæ—¶æ•°æ®è°ƒæ•´è®¡åˆ’ã€‚\n* **Anaplan**: ä¸€æ¬¾ç”±AIé©±åŠ¨çš„è½¯ä»¶ï¼Œæä¾›é›†æˆçš„è´¢åŠ¡è§„åˆ’ã€é¢„ç®—å’Œé¢„æµ‹è§£å†³æ–¹æ¡ˆï¼Œä½¿ä¼ä¸šèƒ½å¤Ÿè¿žæŽ¥è´¢åŠ¡æ•°æ®å¹¶ä¼˜åŒ–å†³ç­–ã€‚\n* **Planful (å‰ç§°Host Analytics)**: ä¸€æ¬¾AIé©±åŠ¨çš„è´¢åŠ¡è§„åˆ’ã€åˆ†æžå’Œé¢„æµ‹å¹³å°ï¼Œä¸ºè´¢åŠ¡å›¢é˜Ÿæä¾›å¼ºå¤§çš„å»ºæ¨¡ã€é¢„ç®—å’ŒæŠ¥å‘Šèƒ½åŠ›ã€‚\n* **Kensho**: ä¸€æ¬¾ç”±AIé©±åŠ¨çš„åˆ†æžå·¥å…·ï¼Œé‡‘èžæœºæž„ä½¿ç”¨å®ƒæ¥é€šè¿‡åˆ†æžå¤§é‡è´¢åŠ¡æ•°æ®å¹¶æä¾›é¢„æµ‹æ´žå¯Ÿï¼Œå¢žå¼ºé¢„æµ‹ã€é£Žé™©è¯„ä¼°å’Œè´¢åŠ¡è§„åˆ’ã€‚\n* **Xero**: ä¸€æ¬¾åŸºäºŽäº‘çš„ä¼šè®¡å¹³å°ï¼Œå…·æœ‰AIé©±åŠ¨çš„åŠŸèƒ½ï¼Œå¸®åŠ©ä¼ä¸šå®žæ—¶è·Ÿè¸ªçŽ°é‡‘æµã€è¿›è¡Œé¢„æµ‹å’Œè´¢åŠ¡æŠ¥å‘Šã€‚\n\n### çŽ°å®žä¸–ç•Œä¸­ AI åœ¨è´¢åŠ¡è§„åˆ’å’Œé¢„æµ‹ä¸­çš„åº”ç”¨å®žä¾‹\n\n* **æ²ƒè¾¾ä¸°çš„ä¼ä¸šé¢„ç®—**ï¼šæ²ƒè¾¾ä¸°åˆ©ç”¨ AI æ”¹è¿›å…¶è´¢åŠ¡è§„åˆ’æµç¨‹ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–é¢„ç®—åˆ›å»ºã€ä¼˜åŒ–çŽ°é‡‘æµé¢„æµ‹ï¼Œå¹¶æä¾›æ›´æ·±å…¥çš„å…¨çƒè¿è¥æˆæœ¬åˆ†é…æ´žå¯Ÿã€‚\n* **Netflix çš„æ”¶å…¥é¢„æµ‹**ï¼šNetflix åˆ©ç”¨ AI åˆ†æžå®¢æˆ·æ•°æ®å¹¶é¢„æµ‹ç”¨æˆ·å¢žé•¿ï¼Œå¸®åŠ©å…¬å¸è§„åˆ’å…¶è´¢åŠ¡æˆ˜ç•¥ï¼Œå¹¶æœ‰æ•ˆåˆ†é…èµ„æºäºŽå„ç§å†…å®¹åˆ¶ä½œå’Œè¥é”€å·¥ä½œã€‚\n* **é»‘çŸ³çš„æŠ•èµ„ç®¡ç†**ï¼šé»‘çŸ³é‡‡ç”¨ AI å’Œæœºå™¨å­¦ä¹ ç®—æ³•æ¥ç®¡ç†æŠ•èµ„ç»„åˆå’Œä¼˜åŒ–èµ„äº§é…ç½®ï¼Œä¸ºå®¢æˆ·æä¾›æ›´å‡†ç¡®å’Œæ•°æ®é©±åŠ¨çš„æŠ•èµ„ç­–ç•¥ã€‚\n\n## ç»“è®º\n\néšç€æˆ‘ä»¬è¿›å…¥2025å¹´ï¼Œäººå·¥æ™ºèƒ½å°†æˆä¸ºè®¸å¤šæˆåŠŸä¼ä¸šçš„æ”¯æŸ±ã€‚é€šè¿‡è‡ªåŠ¨åŒ–é‡å¤ä»»åŠ¡ã€æå‡å®¢æˆ·ä½“éªŒä»¥åŠæä¾›æ•°æ®é©±åŠ¨çš„æ´žå¯Ÿï¼Œäººå·¥æ™ºèƒ½ä½¿ç»„ç»‡èƒ½å¤Ÿåšå‡ºæ›´èªæ˜Žã€æ›´å¿«é€Ÿçš„å†³ç­–ã€‚äººå·¥æ™ºèƒ½åœ¨é¢„æµ‹åˆ†æžã€ç½‘ç»œå®‰å…¨å’Œä¸ªæ€§åŒ–è¥é”€ä¸­çš„ä½œç”¨å°†ç»§ç»­æŽ¨åŠ¨åˆ›æ–°ï¼Œä½¿ä¼ä¸šèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ»¡è¶³ä¸æ–­å˜åŒ–çš„æ¶ˆè´¹è€…éœ€æ±‚ã€‚\n\nå•†ä¸šçš„æœªæ¥åœ¨äºŽ[**äººå·¥æ™ºèƒ½æŠ€æœ¯çš„æ•´åˆ**](https://www.blockchainappfactory.com/ai-development-company)ï¼Œé‚£äº›æ‹¥æŠ±è¿™äº›è¿›æ­¥çš„ä¼ä¸šå°†å¤„äºŽè¡Œä¸šçš„å‰æ²¿ã€‚å°½ç®¡é‡‡ç”¨äººå·¥æ™ºèƒ½é¢ä¸´ç€æ•°æ®éšç§å’Œå°±ä¸šæ›¿ä»£ç­‰æŒ‘æˆ˜ï¼Œä½†å…¶å¥½å¤„è¿œè¿œè¶…è¿‡é£Žé™©ã€‚é€šè¿‡åˆ¶å®šæ­£ç¡®çš„ç­–ç•¥ï¼Œä¼ä¸šå¯ä»¥å……åˆ†åˆ©ç”¨äººå·¥æ™ºèƒ½çš„æ½œåŠ›ï¼Œåœ¨2025å¹´çš„ç«žäº‰çŽ¯å¢ƒä¸­è“¬å‹ƒå‘å±•ã€‚\n\n## å¸¸è§é—®é¢˜è§£ç­”\n\n1. **2025å¹´ä¼ä¸šæœ€é‡è¦çš„AIåº”ç”¨åœºæ™¯æ˜¯ä»€ä¹ˆï¼Ÿ**  \nåœ¨2025å¹´ï¼ŒAIå°†ç”¨äºŽé¢„æµ‹åˆ†æžã€ä¸ªæ€§åŒ–è¥é”€ã€è‡ªåŠ¨åŒ–ã€æ¬ºè¯ˆæ£€æµ‹ã€å®¢æˆ·æ”¯æŒèŠå¤©æœºå™¨äººå’Œä¾›åº”é“¾ä¼˜åŒ–ã€‚\n2. **AIå¦‚ä½•æ”¹å–„å®¢æˆ·ä½“éªŒï¼Ÿ**  \nAIå¯ä»¥é€šè¿‡ä¸ªæ€§åŒ–æŽ¨èã€AIé©±åŠ¨çš„èŠå¤©æœºå™¨äººæä¾›å³æ—¶æ”¯æŒï¼Œä»¥åŠé¢„æµ‹å·¥å…·æ¥é¢„è§å®¢æˆ·éœ€æ±‚ï¼Œä»Žè€Œå¢žå¼ºå®¢æˆ·ä½“éªŒã€‚\n3. **AIæ˜¯å¦å¯èƒ½å–ä»£ä¼ä¸šä¸­çš„äººç±»å‘˜å·¥ï¼Ÿ**  \nAIæ›´å¯èƒ½é€šè¿‡è‡ªåŠ¨åŒ–é‡å¤æ€§ä»»åŠ¡æ¥å¢žå¼ºäººç±»å‘˜å·¥çš„å·¥ä½œï¼Œä½¿å‘˜å·¥èƒ½å¤Ÿä¸“æ³¨äºŽæ›´é«˜å±‚æ¬¡çš„å†³ç­–å’Œåˆ›é€ æ€§å·¥ä½œã€‚\n4. **å“ªäº›è¡Œä¸šåœ¨2025å¹´æœ€èƒ½ä»ŽAIä¸­å—ç›Šï¼Ÿ**  \né‡‘èžã€åŒ»ç–—ä¿å¥ã€é›¶å”®ã€åˆ¶é€ å’Œç‰©æµç­‰è¡Œä¸šé¢„è®¡å°†ä»ŽAIä¸­èŽ·å¾—æ˜¾è‘—æ”¶ç›Šï¼ŒåŒ…æ‹¬æé«˜æ•ˆçŽ‡å’ŒèŠ‚çœæˆæœ¬ã€‚\n5. **ä¼ä¸šåœ¨é‡‡ç”¨AIæ—¶é¢ä¸´å“ªäº›æŒ‘æˆ˜ï¼Ÿ**  \nä¼ä¸šå¯èƒ½é¢ä¸´ä¸Žæ•°æ®éšç§ã€ä¸ŽçŽ°æœ‰ç³»ç»Ÿçš„é›†æˆã€é«˜åˆå§‹æŠ•èµ„ä»¥åŠç¡®ä¿AIç³»ç»Ÿçš„ä¼¦ç†ä½¿ç”¨ç›¸å…³çš„æŒ‘æˆ˜ã€‚\n\n"},{"lang":"zh","group":"blog","slug":"blog/unified-memory-across-chatgpt-claude-perplexity-24809dc56717","frontmatter":{"title":"è·¨ ChatGPTã€Claudeã€Perplexity çš„ç»Ÿä¸€å†…å­˜","meta_title":"è·¨ ChatGPTã€Claudeã€Perplexity çš„ç»Ÿä¸€å†…å­˜","description":"æ‚¨ä¸€å®šä¼šå–œæ¬¢è¿™ä¸ªï¼Œç‰¹åˆ«æ˜¯å¦‚æžœæ‚¨å·²ç»ä½¿ç”¨è¿‡ Claudeã€ChatGPT å’Œ Perplexityã€‚","date":"2024-11-08T00:24:33.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*qrHnDS6YODZuVXZ4eeWZrQ.png","categories":["Chatbots","Programming/Scripting","Technology/Web"],"author":"Rifx.Online","tags":["Mem0","Chrome","extension","context","memory"],"draft":false,"slug":"blog/unified-memory-across-chatgpt-claude-perplexity-24809dc56717"},"content":"\nä½ ä¸€å®šä¼šå–œæ¬¢è¿™ä¸ªï¼Œç‰¹åˆ«æ˜¯å¦‚æžœä½ å·²ç»ä¸Ž Claudeã€ChatGPT å’Œ Perplexity ç´§å¯†è”ç³»åœ¨ä¸€èµ·ã€‚\n\nä¸Žä¸åŒçš„ AI åŠ©æ‰‹äº’åŠ¨æœ‰æ—¶ä¼šæ„Ÿè§‰æœ‰äº›è„±èŠ‚ã€‚\n\nåœ¨åˆ‡æ¢ ChatGPTã€Claudeã€Perplexity å’Œå…¶ä»–åŠ©æ‰‹æ—¶ï¼Œä½ å¿…é¡»ä¸€éåˆä¸€éåœ°é‡å¤ç›¸åŒçš„ä¸Šä¸‹æ–‡ã€‚\n\nå¦‚æžœå®ƒä»¬éƒ½èƒ½å…±äº«ä¸€ä¸ªé€šç”¨è®°å¿†ä»¥å¢žå¼ºä¸Šä¸‹æ–‡ï¼Œé‚£è¯¥å¤šå¥½å•Šï¼Ÿ\n\næˆ‘å‘çŽ°äº†è¿™ä¸ªå¾ˆæ£’çš„ Chrome æ‰©å±•ï¼Œå®ƒå¯¹æˆ‘æ¥è¯´çœŸæ˜¯ä¸ªæ•‘æ˜Ÿã€‚\n\næƒ³è±¡ä¸€ä¸‹ï¼Œæ— è®ºä½ åœ¨ä¸Žå“ªä¸ª AI èŠå¤©ï¼Œä¸Šä¸‹æ–‡éƒ½èƒ½æ— ç¼ä¼ é€’çš„å¯¹è¯ã€‚\n\nå¬èµ·æ¥ä¸é”™ï¼Œæ˜¯å§ï¼Ÿ\n\n\n\n### å®ƒè§£å†³çš„é‡å¤§é—®é¢˜\n\nåœ¨æˆ‘æ·±å…¥æŽ¢è®¨ä¹‹å‰ï¼Œæˆ‘ä»¬å¦‚ä½•ä¿æŒ LLMs æ›´æ–°ä»¥èŽ·å–æœ€æ–°çŸ¥è¯†ï¼Ÿ\n\næœ‰å‡ ç§ä¼ ç»Ÿçš„æ–¹æ³•å¯ä»¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼š\n\n1. **åŸºäºŽæ£€ç´¢çš„æ–¹æ³•**ï¼šè¿™äº›æ–¹æ³•ä»ŽçŸ¥è¯†åº“ä¸­æå–ä¿¡æ¯ã€‚å®ƒä»¬åŠŸèƒ½å¼ºå¤§ï¼Œä½†å¯èƒ½ä¼šå› ä¸ºå†—ä½™æ•°æ®å’Œç®¡ç†ä¸æ–­å¢žé•¿çš„å­˜å‚¨åº“è€Œå˜å¾—æ··ä¹±ã€‚\n2. **æ¨¡åž‹ç¼–è¾‘**ï¼šè¿™æ˜¯ä¸€ç§è°ƒæ•´æ¨¡åž‹ä»¥é€‚åº”æ–°äº‹å®žçš„æ–¹æ³•ã€‚å®ƒé€‚ç”¨äºŽç®€å•çš„å•å¥æ›´æ–°ï¼Œä½†åœ¨å¤„ç†è¾ƒé•¿ã€æ›´å¤æ‚çš„ä¿¡æ¯æ—¶ä¼šé‡åˆ°å›°éš¾ã€‚\n3. **é•¿ä¸Šä¸‹æ–‡æ–¹æ³•**ï¼šè¿™äº›æ–¹æ³•å°†æ‰€æœ‰çŸ¥è¯†å¡žå…¥æ¨¡åž‹çš„ä¸Šä¸‹æ–‡ä¸­ã€‚è¿™å°±åƒæ˜¯ç”¨æ•°æ®è¿‡è½½æ¨¡åž‹ï¼Œä½†ç”±äºŽä¸Šä¸‹æ–‡é•¿åº¦æœ‰é™ï¼Œè¿™å¹¶ä¸å®žç”¨ã€‚\n\næ‰€æœ‰è¿™äº›æ–¹æ³•éƒ½æœ‰å…¶ç¼ºç‚¹ï¼Œç‰¹åˆ«æ˜¯åœ¨æˆ‘ä»¬éœ€è¦åœ¨ä¸åŒçš„ AI å¹³å°ä¹‹é—´è¿›è¡Œæœ€æ–°ã€æ— ç¼çš„äº¤äº’æ—¶ã€‚\n\n### è¿›å…¥ Mem0ï¼šAI åŠ©æ‰‹çš„æ™ºèƒ½è®°å¿†æ£€ç´¢\n\nMem0 é€šè¿‡åˆ›å»ºä¸€ä¸ªé€šç”¨çš„è®°å¿†å±‚ï¼Œè§£å†³äº†è¿™äº›é—®é¢˜ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨å¤šä¸ª AI åŠ©æ‰‹ä¹‹é—´å·¥ä½œã€‚\n\nå®ƒå¸¦æ¥äº†ä»¥ä¸‹ä¼˜åŠ¿ï¼š\n\n* **é€šç”¨è®°å¿†å±‚**ï¼šåœ¨ ChatGPTã€Claudeã€Perplexity ç­‰ä¹‹é—´è½»æ¾å…±äº«ä¸Šä¸‹æ–‡ã€‚ä¸å†éœ€è¦é‡å¤è‡ªå·±ï¼\n* **æ™ºèƒ½ä¸Šä¸‹æ–‡æ£€æµ‹**ï¼šè‡ªåŠ¨ä»Žæ‚¨çš„å¯¹è¯ä¸­æå–ç›¸å…³ä¿¡æ¯ï¼Œå› æ­¤æ‚¨æ— éœ€æ‰‹åŠ¨è¾“å…¥ä»»ä½•å†…å®¹ã€‚\n* **æ™ºèƒ½è®°å¿†æ£€ç´¢**ï¼šMem0 åœ¨åˆé€‚çš„æ—¶æœºè°ƒå‡ºæ­£ç¡®çš„è®°å¿†ï¼Œä½¿æ‚¨çš„äº’åŠ¨æ›´åŠ é¡ºç•…ã€‚\n* **ä¸€é”®ä¸Ž ChatGPT åŒæ­¥**ï¼šå¦‚æžœæ‚¨ä¸€ç›´åœ¨ä½¿ç”¨ ChatGPTï¼Œå¯ä»¥é€šè¿‡ä¸€æ¬¡ç‚¹å‡»å°†çŽ°æœ‰è®°å¿†åŒæ­¥ã€‚\n* **è®°å¿†ä»ªè¡¨æ¿**ï¼šä¸€ä¸ªæ–¹ä¾¿çš„åœ°æ–¹ï¼Œå¯ä»¥é›†ä¸­ç®¡ç†æ‰€æœ‰è®°å¿†ã€‚\n\nä¾‹å¦‚ï¼Œå¦‚æžœæ‚¨ä¸Ž Claude å¼€å§‹å¯¹è¯ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*QQAJPzQp2tjBFgi-9InG1Q.png)\n\nå®ƒå°†æå–å…³é”®ä¿¡æ¯å¹¶å°†å…¶æ·»åŠ åˆ°è®°å¿†ä¸­ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MtctEzunD72Tw_dOOCOnyA.png)\n\nç„¶åŽï¼Œä¸‹æ¬¡æ‚¨é—®ä¸€ä¸ªç›¸å…³é—®é¢˜æ—¶ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*r0WyCIHGvmiGm0GZ6xR1pw.png)\n\nå®ƒå°†æ·»åŠ ç›¸å…³ä¸Šä¸‹æ–‡ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*PTEhFUJ4nrhQbeZXIAnD6A.png)\n\nä½†æ›´æœ‰è¶£çš„æ˜¯ã€‚\n\nå¦‚æžœæ‚¨æ‰“å¼€ Perplexity å¹¶é—®å¦ä¸€ä¸ªé—®é¢˜ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*54BKW9BQWwCDdXdAg0U-mA.png)\n\nå®ƒå°†è·¨ä¸åŒåº”ç”¨ç¨‹åºèŽ·å–è®°å¿†å¹¶å¢žå¼ºä¸Šä¸‹æ–‡ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*FD6MwGtL8WpovUH3o1Vw3g.png)\n\nè¿™çœŸæ˜¯å¤ªé…·äº†ï¼\n\nå¦‚æžœæ‚¨æ‰“å¼€ ChatGPT å¹¶é—®ä¸€ä¸ªé—®é¢˜ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*qLmDZh5NofuArxzS4OnDIQ.png)\n\nåŒæ ·ï¼Œå®ƒå°†ä»Žè®°å¿†ä¸­æå–ç›¸å…³ä¿¡æ¯å¹¶å¢žå¼ºä¸Šä¸‹æ–‡ã€‚\n\néšç€æ‚¨ä¸Žå®ƒçš„äº’åŠ¨å¢žå¤šï¼Œå®ƒå°†æ”¶é›†å…³é”®ä¿¡æ¯ï¼Œä¸ºæ‚¨èŠ‚çœå¤§é‡è¾“å…¥æ—¶é—´ï¼ŒåŒæ—¶ä¸Ž Claudeã€Perplexity å’Œ ChatGPT è¿›è¡Œäº¤äº’ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MqgEr6hi5cHzHRuTqODzuA.png)\n\næ‚¨è¿˜å¯ä»¥å‘è®°å¿†ä¸­æ·»åŠ æ–°ä¿¡æ¯ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Xo6jLLIGKqxvYR9SivuMhQ.png)\n\n### å¦‚ä½•å¼€å§‹\n\nå®‰è£… Mem0 éžå¸¸ç®€å•ï¼š\n\n1. [**å°†æ‰©å±•ç¨‹åºæ·»åŠ åˆ° Chrome**](https://chromewebstore.google.com/detail/mem0/onihkkbipkfeijkadecaafbgagkhglop?hl=en-GB)\n\n**2\\. ç™»å½•**ï¼š\n\n* å®‰è£…åŽï¼Œæ‚¨ä¼šåœ¨å·¥å…·æ ä¸­çœ‹åˆ° Mem0 å›¾æ ‡ã€‚\n* ç‚¹å‡»å®ƒå¹¶ä½¿ç”¨ Google ç™»å½•ã€‚\n\n**4\\. å¼€å§‹èŠå¤©**ï¼š\n\n* ä½¿ç”¨ä»»ä½•æ”¯æŒçš„ AI åŠ©æ‰‹ã€‚\n* å¯¹äºŽ ChatGPT å’Œ Perplexityï¼Œåªéœ€åƒå¹³å¸¸ä¸€æ ·èŠå¤©ã€‚\n* åœ¨ Claude ä¸Šï¼Œç‚¹å‡» Mem0 æŒ‰é’®æˆ–ä½¿ç”¨å¿«æ·é”® `^ + M`ã€‚\n\nMem0 æœ€æ£’çš„åœ°æ–¹ä¹‹ä¸€å°±æ˜¯å®ƒæ˜¯å®Œå…¨å…è´¹çš„ã€‚æ²¡æœ‰ï¼š\n\n* **ä½¿ç”¨é™åˆ¶**\n* **å¹¿å‘Š**\n* **æ‰€æœ‰åŠŸèƒ½å‡åŒ…å«åœ¨å†…**\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/unlocking-mixture-of-experts-moe-llm-your-moe-model-can-be-embedding-model-for-free-f192b9c07a5f","frontmatter":{"title":"è§£é”æ··åˆä¸“å®¶ (MoE) LLMï¼šä½ çš„ MoE æ¨¡åž‹å¯ä»¥å…è´¹åµŒå…¥æ¨¡åž‹","meta_title":"è§£é”æ··åˆä¸“å®¶ (MoE) LLMï¼šä½ çš„ MoE æ¨¡åž‹å¯ä»¥å…è´¹åµŒå…¥æ¨¡åž‹","description":"æ··åˆä¸“å®¶ (MoE) LLM å¯ä»¥å…è´¹ç”¨ä½œåµŒå…¥æ¨¡åž‹ã€‚","date":"2024-11-04T12:30:57.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*mB6VhEyAvxAxGbLDG_6hTw.png","categories":["Machine Learning","Natural Language Processing","Data Science"],"author":"Rifx.Online","tags":["Mixture-of-Experts","MoE","embedding","MoEE","BERTopic"],"draft":false,"slug":"blog/unlocking-mixture-of-experts-moe-llm-your-moe-model-can-be-embedding-model-for-free-f192b9c07a5f"},"content":"\n### Mixture-of-experts (MoE) LLM å¯ä»¥ä½œä¸ºå…è´¹çš„åµŒå…¥æ¨¡åž‹ä½¿ç”¨ã€‚\n\n\n\næˆ‘æœ€è¿‘å‘çŽ°äº†ä¸€ç¯‡æœ‰è¶£çš„è®ºæ–‡ï¼Œæ ‡é¢˜ä¸ºâ€œä½ çš„ Mixture-of-Experts LLM ç§˜å¯†åœ°æ˜¯ä¸€ä¸ªå…è´¹çš„åµŒå…¥æ¨¡åž‹ã€‚â€\\[1\\] æœ€è¿‘çš„ LLM æž¶æž„è¶‹åŠ¿æ˜¯è§£ç å™¨æ¨¡åž‹ï¼Œè¿™å¯¹äºŽåµŒå…¥æ¨¡åž‹å¹¶ä¸é€‚ç”¨ï¼Œå› ä¸ºå®ƒä»¬çš„æ³¨æ„åŠ›æ–¹æ³•ã€‚ç„¶è€Œï¼Œä½œè€…æ­ç¤ºäº† Mixture-of-Experts (MoE) LLM å¯ä»¥ä½œä¸ºåµŒå…¥æ¨¡åž‹æ¥æ‰§è¡Œå¤šç§åµŒå…¥ç›¸å…³çš„ä»»åŠ¡ï¼Œè€Œæ— éœ€è¿›ä¸€æ­¥çš„å¾®è°ƒã€‚åœ¨è¿™ç¯‡åšå®¢ä¸­ï¼Œé¦–å…ˆè®©æˆ‘ä»¬å›žé¡¾ä¸€ä¸‹ MoEï¼Œæˆ‘å°†ä»‹ç»å®ƒçš„å·¥ä½œåŽŸç†åŠå…¶å®žé™…åº”ç”¨ã€‚\n\n## ç›®å½•\n\n1. ä»€ä¹ˆæ˜¯ä¸“å®¶æ··åˆæ¨¡åž‹ï¼ˆMoEï¼‰ï¼Ÿ\n2. MoE å¦‚ä½•ä½œä¸ºåµŒå…¥æ¨¡åž‹å·¥ä½œï¼Ÿ\n3. å®žé™…å®žæ–½ï¼šä½¿ç”¨ BERTopic åˆ©ç”¨ MoEE\n\n## 1\\. ä»€ä¹ˆæ˜¯ä¸“å®¶æ··åˆæ¨¡åž‹ (MoE)ï¼Ÿ\n\nä¸“å®¶æ··åˆæ¨¡åž‹ (MoE) æ˜¯ä¸€ç§å…·æœ‰å¤šä¸ªå­ç½‘ç»œçš„æž¶æž„ï¼Œè¿™äº›å­ç½‘ç»œè¢«ç§°ä¸ºâ€œä¸“å®¶â€ï¼Œæ¯ä¸ªä¸“å®¶ä¸“æ³¨äºŽä¸åŒçš„ä»»åŠ¡æˆ–æ•°æ®æ–¹é¢ã€‚MoE çš„ä¸€ä¸ªä¼˜åŠ¿æ˜¯ï¼Œå®ƒèƒ½å¤Ÿä»¥æ¯”ç›¸åŒæˆ–æ›´å¤§æ¨¡åž‹æ›´å°‘çš„è®¡ç®—é‡å¯¹ AI æ¨¡åž‹è¿›è¡Œé¢„è®­ç»ƒï¼ŒåŒæ—¶ä¿æŒæˆ–æé«˜è´¨é‡ã€‚å› æ­¤ï¼Œå¦‚æžœæˆ‘ä»¬çš„é¢„ç®—æœ‰é™ï¼Œä½¿ç”¨ MoE å¯ä»¥æ¯”ç¨ å¯†çš„ã€ç›¸ä¼¼å¤§å°çš„ä¼ ç»Ÿæ¨¡åž‹èŽ·å¾—æ›´å¥½çš„æ¨¡åž‹ã€‚åœ¨æœ€è¿‘çš„æˆåŠŸæ¡ˆä¾‹ä¸­ï¼ŒMixtral 8 x 7B åœ¨è®¸å¤šè¯„ä¼°æ•°æ®é›†ä¸Šè¶…è¶Šäº† LLaMA 2 70Bã€‚\n\næŽ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬ç ”ç©¶ MoE çš„æž¶æž„ã€‚æœ€è¿‘æˆåŠŸçš„ MoE ä½¿ç”¨äº†å˜åŽ‹å™¨æ¨¡åž‹ï¼Œå› æ­¤æˆ‘å°†é‡ç‚¹å…³æ³¨å˜åŽ‹å™¨çš„æµè¡Œ MoE æž¶æž„ã€‚MoE ä¸»è¦æœ‰ä¸¤ä¸ªç»„ä»¶ï¼Œå¦‚ä¸‹æ‰€è¿°ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Dia_c08PJnFeeIc9lxwtGQ.png)\n\n* **MoE å±‚**\n\nMoE åœ¨å˜åŽ‹å™¨æž¶æž„ä¸­ç”¨ MoE å±‚æ›¿ä»£äº†å‰é¦ˆç½‘ç»œ (FFN) å±‚ã€‚æ¯ä¸ª MoE å±‚æœ‰ä¸€äº›ä¸“å®¶ï¼ˆä¾‹å¦‚ï¼Œä¸Šå›¾ä¸­çš„ 4 ä¸ªä¸“å®¶ï¼‰ï¼Œæ¯ä¸ªä¸“å®¶ç”±ç®€å•çš„ FFN å±‚ç»„æˆã€‚è¯·æ³¨æ„ï¼Œå˜åŽ‹å™¨ä¸­çš„å…¶ä»–ç»„ä»¶ï¼Œä¾‹å¦‚è‡ªæ³¨æ„åŠ›å±‚ï¼Œä½¿ç”¨ç›¸åŒçš„æƒé‡ã€‚å› æ­¤ï¼ŒMoE çš„æƒé‡æ•°é‡å¹¶ä¸ç®€å•ã€‚ä¾‹å¦‚ï¼ŒMixtral 8 x 7B çš„æƒé‡ä¸æ˜¯ 8 x 7 = 56Bï¼Œè€Œæ˜¯ 47Bï¼Œå› ä¸ºé™¤äº† MoE å±‚ä¹‹å¤–çš„å…¶ä»–å±‚å…±äº«ç›¸åŒçš„æƒé‡ã€‚\n\n* **é—¨æŽ§ç½‘ç»œ**\n\né—¨æŽ§ç½‘ç»œæˆ–è·¯ç”±å™¨æ˜¯ MoE ä¸­çš„ä¸€ä¸ªå…³é”®ç»„ä»¶ã€‚å®ƒæŽ¥æ”¶è¾“å…¥æ ‡è®°å¹¶ä¸ºæ¯ä¸ªæ ‡è®°é€‰æ‹©æœ€ç›¸å…³çš„ä¸“å®¶ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸Šé¢çš„æ’å›¾ä¸­ï¼Œè·¯ç”±å™¨çš„å·¦ä¾§é€‰æ‹©ç¬¬äºŒä¸ªä¸“å®¶æ¥å¤„ç†å•è¯â€œmoreâ€æ ‡è®°ã€‚åŒæ—¶ï¼Œè·¯ç”±å™¨ç¡®å®šç¬¬ä¸€ä¸ªä¸“å®¶æ¥å¤„ç†å•è¯â€œParametersâ€æ ‡è®°ã€‚é€šå¸¸ï¼Œé—¨æŽ§ç½‘ç»œé€‰æ‹©ä¸Žç»™å®šæ ‡è®°ç›¸å…³çš„å‰ k ä¸ªä¸“å®¶ï¼Œå¹¶å°†æ ‡è®°å‘é€ç»™é€‰å®šçš„ä¸“å®¶ï¼›ä¾‹å¦‚ï¼ŒMixtral 8 x 7B é€‰æ‹©å‰ 2 ä¸ªä¸“å®¶ã€‚\n\næˆ‘ä»¬å¦‚ä½•é€‰æ‹©å‰ k ä¸ªä¸“å®¶ï¼Ÿæˆ‘ä»¬ä½¿ç”¨ softmax å‡½æ•°æ¥è®¡ç®—ä¸“å®¶çš„é‡è¦æ€§æ¦‚çŽ‡ï¼Œå¹¶ä¿ç•™å‰ k ä¸ªæ¦‚çŽ‡ä¸“å®¶ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚æˆ‘æå–äº†ä¸Šè¿°æ’å›¾çš„é—¨æŽ§éƒ¨åˆ†ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*qX9H2KKtjntVuiE8yFstMQ.png)\n\né—¨æŽ§ç½‘ç»œæœ‰å…¶æƒé‡ã€‚æˆ‘ä»¬å°† softmax å‡½æ•°åº”ç”¨äºŽè¾“å…¥å•è¯æ ‡è®°ä¸Žé—¨æŽ§ç½‘ç»œæƒé‡ä¹‹é—´çš„ç‚¹ç§¯ç»“æžœï¼Œç„¶åŽå¾—åˆ°ä¸“å®¶ä¸Žç»™å®šæ ‡è®°ç›¸å…³çš„æ¦‚çŽ‡ã€‚æ ¹æ®æ¦‚çŽ‡ï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©å‰ k ä¸ªç›¸å…³ä¸“å®¶ã€‚å…·æœ‰è¿™ç§ç±»åž‹é—¨æŽ§ç½‘ç»œçš„ MoE è¢«ç§°ä¸ºç¨€ç– MoEã€‚\n\nè¿™äº›æ˜¯ç†è§£ MoE å¦‚ä½•ä½œä¸ºåµŒå…¥æ¨¡åž‹å·¥ä½œçš„åŸºæœ¬çŸ¥è¯†ã€‚ä¸ºäº†è¿›ä¸€æ­¥ç†è§£ï¼Œæˆ‘æŽ¨èé˜…è¯» [è¿™ç¯‡åšå®¢](https://huggingface.co/blog/moe) \\[2]ã€‚çŽ°åœ¨ï¼Œè®©æˆ‘ä»¬æ·±å…¥æŽ¢è®¨ MoE å®žé™…ä¸Šæ˜¯å¦‚ä½•ä½œä¸ºåµŒå…¥æ¨¡åž‹å·¥ä½œçš„ã€‚\n\n## 2\\. MoE å¦‚ä½•ä½œä¸ºåµŒå…¥æ¨¡åž‹å·¥ä½œï¼Ÿ\n\n### å…³äºŽåµŒå…¥çš„å¿«é€Ÿå›žé¡¾\n\nåœ¨æ·±å…¥æœ¬èŠ‚ä¸»é¢˜ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å¿«é€Ÿå›žé¡¾ä¸€ä¸‹åµŒå…¥ã€‚æœ€è¿‘ï¼ŒåµŒå…¥æˆä¸ºæ·±åº¦å­¦ä¹ æ¨¡åž‹ä¸­è¾“å…¥æ•°æ®çš„å†…éƒ¨è¡¨ç¤ºï¼Œå®ƒå…·æœ‰è¯­ä¹‰å’Œæµ“ç¼©çš„æ•°æ®ä¿¡æ¯ã€‚æˆ‘ä»¬é€šå¸¸æå–ç¥žç»ç½‘ç»œçš„æœ€åŽä¸€ä¸ªéšè—çŠ¶æ€ä½œä¸ºåµŒå…¥ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kSHFTEejKiSI51taKZCO9A.png)\n\næˆ‘ä»¬é€šå¸¸ä½¿ç”¨åŸºäºŽç¼–ç å™¨çš„æ¨¡åž‹æ¥æå–åµŒå…¥ï¼Œå› ä¸ºä¸Žä»…è§£ç å™¨æ¨¡åž‹ç›¸æ¯”ï¼Œå®ƒä»¬èƒ½å¤Ÿé€šè¿‡åŒå‘æ³¨æ„åŠ›æ•æ‰è¯­ä¹‰ã€‚ä»…è§£ç å™¨æ¨¡åž‹é€šå¸¸ä½¿ç”¨å› æžœæ³¨æ„åŠ›ï¼Œåªä¸Žä¹‹å‰çš„è¯å…ƒè¿›è¡Œäº¤äº’ï¼›å› æ­¤ï¼Œå®ƒä»¬æ— æ³•æ•æ‰ä¸°å¯Œçš„è¯­ä¹‰ï¼Œå¦‚ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè¿™ä¸€ç‚¹æ˜¯ç¼–ç å™¨-è§£ç å™¨æ¨¡åž‹æ‰€èƒ½å®žçŽ°çš„ã€‚\n\n### MoEå¦‚ä½•ä½œä¸ºåµŒå…¥æ¨¡åž‹å·¥ä½œï¼Ÿ\n\näººä»¬æ™®éè®¤ä¸ºè§£ç å™¨æ¨¡åž‹æ— æ³•ç”¨äºŽåµŒå…¥æå–ã€‚ç„¶è€Œï¼Œä½œè€…å‘çŽ°MoEä¸­çš„è·¯ç”±æƒé‡ä¸ºè§£ç å™¨åµŒå…¥æä¾›äº†äº’è¡¥ä¿¡æ¯ã€‚æ¯ä¸€å±‚ä¸­çš„è·¯ç”±æƒé‡åæ˜ äº†å¯¹è¾“å…¥æ ‡è®°çš„æŽ¨ç†é€‰æ‹©ï¼Œå› æ­¤å®ƒåŒ…å«äº†è¾“å…¥çš„è¯­ä¹‰ä¿¡æ¯ï¼Œè€Œéšè—çŠ¶æ€çš„åµŒå…¥å¯èƒ½ä¼šä¸¢å¤±ã€‚åœ¨æ•°å­¦å…¬å¼ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥è¿™æ ·æè¿°å®ƒï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*n6wGCMqAhjBAfLFV47ML1g.png)\n\n*g*æ˜¯softmaxå‡½æ•°ï¼Œ*H*è¡¨ç¤ºéšè—çŠ¶æ€ã€‚æˆ‘ä»¬å°†æ‰€æœ‰MoEå±‚çš„è·¯ç”±æƒé‡è¿›è¡Œè¿žæŽ¥ï¼Œä»¥é¿å…ä¸¢å¤±æ¨¡åž‹çš„æŽ¨ç†é€‰æ‹©ã€‚\n\nä¸ºäº†å……åˆ†åˆ©ç”¨è·¯ç”±æƒé‡å’Œè§£ç å™¨åµŒå…¥ï¼Œä½œè€…æå‡ºäº†ä¸€ç§ç§°ä¸ºMoEåµŒå…¥ï¼ˆMoEEï¼‰çš„æ–¹æ³•ï¼Œä»¥å½¢æˆæ›´å…¨é¢çš„åµŒå…¥è¡¨ç¤ºã€‚MoEEæœ‰ä¸¤ç§ç±»åž‹ã€‚ä¸€ç§æ–¹æ³•æ˜¯åŸºäºŽè¿žæŽ¥çš„ç»„åˆï¼Œå…·ä½“å¦‚ä¸‹ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*uVmcV-lM83XL7HoYbYjt7w.png)\n\nè¿™ç§æ–¹æ³•å¾ˆç®€å•ï¼Œæˆ‘ä»¬åªéœ€å°†è·¯ç”±æƒé‡å’Œè§£ç å™¨åµŒå…¥è¿›è¡Œè¿žæŽ¥ã€‚ä½œè€…å°†è¿™ç§æ–¹æ³•ç§°ä¸ºMoEE(concat)ã€‚å®ƒå¯ä»¥ä¿ç•™æ¯ä¸ªè·¯ç”±æƒé‡æ•èŽ·çš„ç‹¬ç‰¹ä¿¡æ¯ï¼ŒåŒæ—¶å…è®¸ä¸‹æ¸¸ä»»åŠ¡åˆ©ç”¨ç»„åˆè¡¨ç¤ºã€‚\n\nå¦ä¸€ç§æ–¹æ³•æ˜¯åŠ æƒæ±‚å’Œé›†æˆã€‚å®ƒå¯¹ä»Žè·¯ç”±æƒé‡å’Œéšè—çŠ¶æ€ï¼ˆHSï¼‰åµŒå…¥è®¡ç®—çš„ç›¸ä¼¼æ€§åˆ†æ•°è¿›è¡ŒåŠ æƒæ±‚å’Œï¼Œè¡¨ç¤ºä¸ºMoEE(sum)ã€‚è¯¥æ–¹æ³•ç”¨äºŽæ¯”è¾ƒä¸¤ä¸ªå¥å­çš„ä»»åŠ¡ï¼Œä¾‹å¦‚è¯­ä¹‰æ–‡æœ¬ç›¸ä¼¼æ€§ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kyJxWW9zdgRyNr2jmO4LlQ.png)\n\nð›‚æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œç”¨äºŽæŽ§åˆ¶è·¯ç”±æƒé‡çš„è´¡çŒ®ã€‚åœ¨ä¸ºæ¯å¯¹è®¡ç®—ç›¸ä¼¼æ€§åˆ†æ•°åŽï¼Œæˆ‘ä»¬è®¡ç®—è®¡ç®—å¾—å‡ºçš„ç›¸ä¼¼æ€§åˆ†æ•°ä¸ŽçœŸå®žç›¸ä¼¼æ€§ä¹‹é—´çš„ç­‰çº§ç›¸å…³æ€§ï¼Œä¾‹å¦‚æ–¯çš®å°”æ›¼ç­‰çº§ç›¸å…³æ€§ã€‚\n\nåœ¨å®žé™…ä½¿ç”¨ä¸­ï¼Œæˆ‘è®¤ä¸ºMoEE(concat)æ˜“äºŽä½¿ç”¨ã€‚æ­¤å¤–ï¼Œä½œè€…åˆ©ç”¨PromptEOLæŠ€æœ¯\\[4]æ¥å¢žå¼ºMoEEã€‚è¯¥æŠ€æœ¯æç¤ºä»¥ä¸‹æ¨¡æ¿ï¼Œä»¥é™åˆ¶LLMsåœ¨é¢„æµ‹ä¸‹ä¸€ä¸ªæ ‡è®°çš„è¯­ä¹‰ä¿¡æ¯æ—¶çš„è¡Œä¸ºã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*S9BASj9JkQe-i4fqmbopWg.png)\n\nçŽ°åœ¨ï¼Œè¿™é‡Œæ˜¯MTEBä»»åŠ¡çš„æ€§èƒ½è¡¨ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*7LxkEMR2DFlncypF6_T7Vw.png)\n\nå¸¦æœ‰PromptEOLçš„MoEEå¯ä»¥æ¯”ç›‘ç£å’Œè‡ªç›‘ç£æ–¹æ³•è¡¨çŽ°æ›´å¥½ã€‚è¯·æ³¨æ„ï¼Œè¿™ä¸ªæŽ’è¡Œæ¦œä¸æ˜¯æœ€æ–°çš„ï¼Œå› æ­¤è¿™ä¸ªç»“æžœå¹¶ä¸æ˜¯SOTAã€‚è¿™ç§æ–¹æ³•çš„ä»·å€¼åœ¨äºŽæˆ‘ä»¬å¯ä»¥åœ¨åµŒå…¥ä»»åŠ¡ä¸­èŽ·å¾—ä¸é”™çš„ç»“æžœï¼Œå¹¶ä¸”å¯ä»¥åœ¨æ²¡æœ‰ä»»ä½•è¿›ä¸€æ­¥è®­ç»ƒçš„æƒ…å†µä¸‹ä½¿ç”¨ã€‚\n\nåˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»æ¶µç›–äº†MoEEçš„å·¥ä½œåŽŸç†ã€‚åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å®žçŽ°MoEEä¸ŽBERTopicå¹¶å¯¹å¥å­è¿›è¡Œèšç±»ã€‚\n\n## 3\\. å®žé™…å®žæ–½ï¼šåˆ©ç”¨ MoEE ä¸Ž BERTopic\n\nåœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä»Žé¢„è®­ç»ƒçš„ MoE LLM ä¸­æå–åµŒå…¥ï¼Œå¹¶ä½¿ç”¨ 20-news-group æ•°æ®é›† \\[5] ä¸Ž [BERTopic](https://maartengr.github.io/BERTopic/index.html) ç»“åˆã€‚ä¾›æ‚¨å‚è€ƒï¼ŒBERTopic æ˜¯ä¸€ä¸ªä¾¿åˆ©çš„ä¸»é¢˜å»ºæ¨¡åº“ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„ç»Ÿè®¡ä¸»é¢˜å»ºæ¨¡ã€‚å®ƒåˆ©ç”¨æ¥è‡ª Transformer çš„åµŒå…¥è¿›è¡Œä¸»é¢˜èšç±»ï¼Œå› æ­¤æˆ‘è®¤ä¸ºå®ƒé€‚åˆç”¨äºŽæ£€æŸ¥èƒ½åŠ›ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æ¥å‡†å¤‡ä¸€ä¸ªçŽ¯å¢ƒã€‚\n\n### çŽ¯å¢ƒè®¾ç½®\n\næˆ‘ä½¿ç”¨äº†ä¸€ä¸ªå¸¦æœ‰ Python 3\\.10 çš„ conda çŽ¯å¢ƒã€‚æˆ‘åœ¨ Ubuntu 20\\.04 ä¸Šè¿›è¡Œäº†å®žéªŒï¼Œä½¿ç”¨ cuda 12\\.4ï¼Œ16 GB VRAMã€‚ä¸‹è½½æ¨¡åž‹æƒé‡å¯èƒ½éœ€è¦ 32 GB RAMã€‚\n\n```python\nconda create -n moee python=3.10 -y\nconda activate moee\n```\n\næŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦é€šè¿‡ pip å®‰è£…ä»¥ä¸‹åº“ã€‚\n\n```python\npip install transformers torch bitsandbytes bertopic accelerate\n```\n\nMoE æ¨¡åž‹é€šå¸¸éœ€è¦è¾ƒé«˜çš„ VRAMï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦æå‰å°†æ•´ä¸ªæ¨¡åž‹åŠ è½½åˆ° VRAM ä¸­ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨ bitsandbytesï¼Œè¿™æ˜¯ä¸€ä¸ªé‡åŒ–åŒ…ï¼Œä»¥èŠ‚çœ VRAM å†…å­˜ã€‚\n\næˆ‘ä»¬éœ€è¦å…‹éš†å®˜æ–¹ GitHub ä»“åº“ã€‚\n\n```python\ngit clone https://github.com/tianyi-lab/MoE-Embedding.git\n```\n\næ‰€æœ‰å‡†å¤‡å·¥ä½œéƒ½å®Œæˆäº†ã€‚çŽ°åœ¨ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨ MoEE å®žçŽ° BERTopic çš„ä¸»é¢˜èšç±»ã€‚\n\n### åˆ©ç”¨ MoEE å’Œ BERTopic\n\nçŽ°åœ¨ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ MoEE ä½œä¸º BERTopic çš„åµŒå…¥æ¨¡åž‹å¹¶å°è¯•ä¸»é¢˜èšç±»ã€‚åŽŸå§‹ä»£ç åº“å…è®¸æˆ‘ä»¬ä½¿ç”¨å°åž‹ MoE æ¨¡åž‹ï¼Œä¾‹å¦‚ Qwen\\-1\\.5\\-MoE\\-A2\\.7B æˆ– OLMoE\\-1B\\-7Bã€‚åœ¨è¿™ç¯‡åšå®¢ä¸­ï¼Œæˆ‘å°†ä½¿ç”¨ OLMoE\\-1B\\-7Bï¼Œå®ƒé€‚åˆåœ¨ 16 GB VRAM ä¸Šè¿è¡ŒæŽ¨ç†ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦åŠ è½½ OLMoE\\-1B\\-7Bã€‚\n\n```python\nkwargs = {\n        \"base_model\": 'allenai/OLMoE-1B-7B-0924',\n        \"normalized\": False,\n        \"torch_dtype\": torch.bfloat16,\n        \"mode\": \"embedding\",\n        \"pooling_method\": \"mean\",\n        \"attn_implementation\": \"sdpa\",\n        \"attn\": \"bbcc\",\n    }\n\nconfig = {\n    'embed_method': 'prompteol',\n    'emb_info': 'MoEE'\n    }\n\nembedding_model = MOEE(model_name_or_path='allenai/OLMoE-1B-7B-0924', **kwargs)\n```\n\næŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®— 20\\-news\\-group æ•°æ®é›†çš„åµŒå…¥ï¼Œä»¥ä¼ é€’ç»™ BERTopicã€‚ï¼ˆæˆ‘ç¨åŽä¼šé™„ä¸Šå®Œæ•´ä»£ç ã€‚ï¼‰\n\n```python\nfrom sklearn.datasets import fetch_20newsgroups\n\ndocs = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))['data']\n\ndataset = MyDataset(docs)\ndataloader = DataLoader(dataset=dataset, batch_size=8)\nembeddings = None\n\nfor batch in tqdm(dataloader):\n    with torch.no_grad():    \n        embedding = embedding_model.encode(batch, **config)\n      \n        if embeddings is None:\n            embeddings = embedding[0]\n        else:\n            embeddings = np.vstack((embeddings, embedding[0]))\n  \n    torch.cuda.empty_cache()\n```\n\nä¸ºäº†æå‰è®¡ç®—åµŒå…¥ï¼Œæˆ‘ä»¬ä½¿ç”¨ torch.utils.data.DataLoader ä½œä¸ºè¿­ä»£å™¨ï¼Œå¹¶å¯¹æ¯ä¸ªæ‰¹æ¬¡çš„æ–‡æ¡£è¿›è¡Œç¼–ç ã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬å¿…é¡»å°†åµŒå…¥ä½œä¸º np.asarray ç±»åž‹ä¼ é€’ç»™ BERTopicã€‚\n\nå½“æ‚¨æƒ³ä½¿ç”¨è‡ªå·±çš„ MoE æ¨¡åž‹æ—¶ï¼Œå¿…é¡»å®žçŽ°ä»Žæ¯ä¸ª MoE å±‚èŽ·å–è·¯ç”±æƒé‡ã€‚å¯¹äºŽéšè—çŠ¶æ€åµŒå…¥ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨ HuggingFace transformer å‡½æ•°ã€‚æˆ‘ä»¬åªéœ€åœ¨æŽ¨ç†æ—¶ä¼ é€’ output\\_hidden\\_states\\=True å‚æ•°ã€‚\n\nçŽ°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥è¿è¡Œä¸»é¢˜å»ºæ¨¡ã€‚\n\n```python\n## Step 2 - Reduce dimensionality\numap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')\n\n## Step 3 - Cluster reduced embeddings\nhdbscan_model = HDBSCAN(min_cluster_size=15, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n\n## Step 4 - Tokenize topics\nvectorizer_model = CountVectorizer(stop_words=\"english\")\n\n## Step 5 - Create topic representation\nctfidf_model = ClassTfidfTransformer()\n\n## Step 6 - (Optional) Fine-tune topic representations with \n## a `bertopic.representation` model\nrepresentation_model = KeyBERTInspired()\n\n## All steps together\ntopic_model = BERTopic(\n  embedding_model=embedding_model,          # Step 1 - Extract embeddings\n  umap_model=umap_model,                    # Step 2 - Reduce dimensionality\n  hdbscan_model=hdbscan_model,              # Step 3 - Cluster reduced embeddings\n  vectorizer_model=vectorizer_model,        # Step 4 - Tokenize topics\n  ctfidf_model=ctfidf_model,                # Step 5 - Extract topic words\n  representation_model=representation_model # Step 6 - (Optional) Fine-tune topic representations\n)\n\n## topic modeling using BERTopic model\ntopics, probs = topic_model.fit_transform(docs, embeddings)\n```\n\næˆ‘ä»¬é€šè¿‡é»˜è®¤è®¾ç½®å¾—åˆ°äº† 42 ä¸ªä¸»é¢˜ï¼›ä¸€äº›ç¤ºä¾‹å¦‚ä¸‹æ‰€ç¤ºã€‚å°½ç®¡æˆ‘éšæœºé€‰æ‹©äº†ä¸»é¢˜ï¼Œä½†å®ƒèƒ½å¤Ÿå¾ˆå¥½åœ°æ•æ‰è¯­ä¹‰ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*VIaKHU-PSuTPzOUKDFbwOw.png)\n\næ­¤å¤–ï¼Œè¿™é‡Œæ˜¯ä¸»é¢˜èšç±»å¯è§†åŒ–ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*KYAUOe2qEAv-ihq2S2dM0A.png)\n\nè¯·æŸ¥çœ‹ä¸»é¢˜èšç±»å¯è§†åŒ–ä¸­çš„çº¢è‰²åœ†åœˆã€‚è¿™ä¸ªçº¢è‰²åœ†åœˆæŒ‡çš„æ˜¯ä¸»é¢˜ 0ï¼Œä¸Žè®¡ç®—æœºç›¸å…³ã€‚æ›´æŽ¥è¿‘çš„ä¸»é¢˜ä¹Ÿä¸Žæœºæ¢°è¯æ±‡ç›¸å…³ï¼Œä¾‹å¦‚å›¾å½¢ã€æ•°å­—å’Œæ‰“å°æœºã€‚\n\nè¿™ç§æ–¹æ³•å‘æˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬å¯ä»¥åœ¨æ²¡æœ‰ä»»ä½•è®­ç»ƒçš„æƒ…å†µä¸‹èŽ·å¾—è‰¯å¥½çš„åµŒå…¥ã€‚å°½ç®¡åœ¨è´¨é‡ä¸Šä»æœ‰æå‡ç©ºé—´ï¼Œä»¥è¾¾åˆ° SOTA\\-ç›‘ç£æ¨¡åž‹çš„æ°´å¹³ï¼Œä½†æœ¬æ–‡çš„å‘çŽ°æ˜¯è¿›ä¸€æ­¥æ”¹å–„åµŒå…¥æå–æ–¹æ³•è€Œä¸è¿›è¡Œè®­ç»ƒçš„è‰¯å¥½æ­¥éª¤ã€‚\n\nè¿™æ˜¯æˆ‘çš„å®Œæ•´ä»£ç ã€‚æ‚¨éœ€è¦å°†æ­¤æ–‡ä»¶æ”¾å…¥ MoE\\-Embedding ç›®å½•çš„é¡¶éƒ¨ã€‚\n\n## å‚è€ƒæ–‡çŒ®\n\n\\[1] Ziyue Li, Tianyi Zhou, [YOUR MIXTURE\\-OF\\-EXPERTS LLM IS SECRETLY AN EMBEDDING MODEL FOR FREE](https://arxiv.org/pdf/2410.10814) (2024\\), *Arxiv*\n\n\\[2] Omar S., et.al., [Mixture of Experts Explained](https://huggingface.co/blog/moe) (2023\\), Hugging Face\n\n\\[3] William Fedus, Barret Zoph., et.al., [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/pdf/2101.03961) (2021\\), *Arxiv*\n\n\\[4] Ting Jiang, et.al., [Scaling Sentence Embeddings with Large Language Models](https://arxiv.org/pdf/2307.16645) (2023\\), *Arxiv*\n\n\\[5] [20 News groups](http://qwone.com/~jason/20Newsgroups/)\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/using-llama-3-for-building-ai-agents-7e74f79d1ccc","frontmatter":{"title":"ä½¿ç”¨ Llama 3 æž„å»º AI ä»£ç†","meta_title":"ä½¿ç”¨ Llama 3 æž„å»º AI ä»£ç†","description":"ä½¿ç”¨ Llama 3 å‡½æ•°è°ƒç”¨åŠŸèƒ½æž„å»º AI ä»£ç†çš„ç»¼åˆæŒ‡å—ã€‚","date":"2024-11-10T03:51:17.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*EWGo-7t4Kl6l82rB2-ZK9Q.png","categories":["Programming","Generative AI","Chatbots"],"author":"Rifx.Online","tags":["Llama","Gradio","RAG","metadata","indexing"],"draft":false,"slug":"blog/using-llama-3-for-building-ai-agents-7e74f79d1ccc"},"content":"\n\n\n### æž„å»ºå…·æœ‰ Llama 3 å‡½æ•°è°ƒç”¨èƒ½åŠ›çš„ AI ä»£ç†çš„ç»¼åˆæŒ‡å—\n\n\n\n### å¼•è¨€\n\næƒ³è±¡ä¸€ä¸‹ä½ æƒ³ä¹°ä¸€äº›ä¸œè¥¿ã€‚ä½ è®¿é—®ä¸€ä¸ªç”µå­å•†åŠ¡ç½‘ç«™ï¼Œä½¿ç”¨æœç´¢é€‰é¡¹æ‰¾åˆ°ä½ æƒ³è¦çš„ä¸œè¥¿ã€‚ä¹Ÿè®¸ä½ æœ‰å¤šä¸ªç‰©å“è¦è´­ä¹°ï¼Œå› æ­¤è¿™ä¸ªè¿‡ç¨‹å¹¶ä¸æ˜¯å¾ˆé«˜æ•ˆã€‚çŽ°åœ¨è€ƒè™‘è¿™ä¸ªåœºæ™¯ï¼šæ‰“å¼€ä¸€ä¸ªåº”ç”¨ç¨‹åºï¼Œç”¨ç®€å•çš„è‹±è¯­æè¿°ä½ æƒ³è¦çš„ä¸œè¥¿ï¼Œç„¶åŽæŒ‰ä¸‹å›žè½¦ã€‚ä½ ä¸å¿…æ‹…å¿ƒæœç´¢å’Œä»·æ ¼æ¯”è¾ƒï¼Œå› ä¸ºåº”ç”¨ç¨‹åºä¼šè‡ªåŠ¨ä¸ºä½ å¤„ç†è¿™äº›äº‹æƒ…ã€‚å¾ˆé…·ï¼Œå¯¹å§ï¼Ÿè¿™æ­£æ˜¯æˆ‘ä»¬å°†åœ¨æœ¬æ•™ç¨‹ä¸­æž„å»ºçš„å†…å®¹ã€‚\n\nè®©æˆ‘ä»¬å…ˆçœ‹ä¸€äº›ä¾‹å­ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*ikbr1ozv37PIB2meVfCCfA.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*AZPn3_KCDRV0pAszd3vLmA.png)\n\nå¥½çš„ï¼Œè®©æˆ‘ä»¬ä¸ºè¿™ä¸ªåº”ç”¨ç¨‹åºæ³¨å…¥æ´»åŠ›ã€‚æˆ‘ä»¬å°†ä½¿ç”¨Metaçš„Llama 3æ¨¡åž‹ï¼Œå…·æœ‰å‡½æ•°è°ƒç”¨èƒ½åŠ›ã€‚ä¸è¿‡ï¼Œè¿™ä¹Ÿå¯ä»¥ä½¿ç”¨3.1æ¨¡åž‹æ¥å®žçŽ°ã€‚æ ¹æ®[Metaçš„å…¬å‘Š](https://ai.meta.com/blog/meta-llama-3-1/)ï¼Œ3.1æ¨¡åž‹å¯ä»¥æ›´æœ‰æ•ˆåœ°ä½¿ç”¨å·¥å…·å’Œå‡½æ•°ã€‚\n\n> è¿™äº›æ˜¯å¤šè¯­è¨€çš„ï¼Œå…·æœ‰æ˜¾è‘—æ›´é•¿çš„ä¸Šä¸‹æ–‡é•¿åº¦128Kï¼Œæœ€å…ˆè¿›çš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼Œä»¥åŠæ•´ä½“æ›´å¼ºçš„æŽ¨ç†èƒ½åŠ›ã€‚\n\næˆ‘å°†ä½¿ç”¨Groq Cloudï¼Œç‰¹åˆ«æ˜¯ä»–ä»¬çš„æ¨¡åž‹æ¥æ’°å†™æœ¬æ–‡ã€‚è¿™ä¸ªåº”ç”¨ç¨‹åºçš„åˆå§‹å·¥ä½œæµç¨‹åº”ç”±ä¸€ä¸ªåµŒå…¥æ¨¡åž‹ã€ä¸€ä¸ªæ£€ç´¢å™¨å’Œä¸¤ä¸ªä¸»è¦å·¥å…·ç»„æˆï¼Œç”¨äºŽå¤„ç†ç”¨æˆ·çš„è´­ä¹°å…´è¶£å’Œä¸Žæˆæœ¬ç›¸å…³çš„å…³æ³¨ã€‚æ€»ä¹‹ï¼Œæˆ‘ä»¬éœ€è¦ä¸€äº›ç±»ä¼¼äºŽä¸‹é¢å›¾è¡¨ä¸­æè¿°çš„å†…å®¹ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*EZVySX3GD2O07fzEPwLcbQ.png)\n\nçŽ°åœ¨æˆ‘ä»¬éœ€è¦ä½¿ç”¨LLMç¼–æŽ’æ¡†æž¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘é€‰æ‹©æˆ‘ä¸€ç›´ä»¥æ¥æœ€å–œæ¬¢çš„[Haystack](https://haystack.deepset.ai/)ã€‚\n\nå¥½çš„ï¼Œæˆ‘ä»¬å¾—åˆ°äº†æˆ‘ä»¬éœ€è¦çš„ä¸œè¥¿ã€‚è®©æˆ‘ä»¬è·³å…¥å®žé™…å·¥ä½œå§ï¼\n\n### åŠ è½½å’Œç´¢å¼•æ•°æ®\n\nç”±äºŽæˆ‘ä»¬æœ‰ä¸€ä¸ª RAG æµæ°´çº¿ï¼Œåº”è¯¥å°†æž„å»ºæ–‡æ¡£ç´¢å¼•æœåŠ¡ä½œä¸ºç¬¬ä¸€æ­¥ã€‚å¯¹äºŽè¿™ä¸ªæ¼”ç¤ºï¼Œæˆ‘å°†ä½¿ç”¨ Haystack æä¾›çš„å†…å­˜å‘é‡æ•°æ®åº“ã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬çš„å‘é‡æ•°æ®åº“ä¸­çš„æ¯ä¸ªæ–‡æ¡£åŒ…å«ï¼š\n\n* å†…å®¹ â€” æˆ‘ä»¬ç”¨å®ƒæ¥æ‰§è¡Œç›¸ä¼¼æ€§æœç´¢\n* Id â€” å”¯ä¸€æ ‡è¯†ç¬¦\n* ä»·æ ¼ â€” äº§å“ä»·æ ¼\n* URL â€” äº§å“ URL\n\nå½“æˆ‘ä»¬çš„ RAG æµæ°´çº¿è¢«è°ƒç”¨æ—¶ï¼Œå†…å®¹å­—æ®µç”¨äºŽå‘é‡æœç´¢ã€‚æ‰€æœ‰å…¶ä»–å­—æ®µä½œä¸ºå…ƒæ•°æ®åŒ…å«ã€‚ä¿ç•™è¿™äº›å…ƒæ•°æ®è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒå¯¹å‰ç«¯å‘ˆçŽ°ç»™ç”¨æˆ·æ˜¯å¿…ä¸å¯å°‘çš„ã€‚\n\nè®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•å®žçŽ°è¿™ä¸€ç‚¹ã€‚\n\n```python\nfrom haystack import Pipeline, Document\nfrom haystack.document_stores.in_memory import InMemoryDocumentStore\nfrom haystack.components.writers import DocumentWriter\nfrom haystack.components.embedders import SentenceTransformersDocumentEmbedder\nfrom haystack.components.generators import OpenAIGenerator\nfrom haystack.utils import Secret\nfrom haystack.components.generators.chat import OpenAIChatGenerator\nfrom haystack.components.builders import PromptBuilder\nfrom haystack.components.embedders import SentenceTransformersTextEmbedder\nfrom haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\nfrom haystack.dataclasses import ChatMessage\nimport pandas as pd\n\n## Load product data from CSV\ndf = pd.read_csv(\"product_sample.csv\")\n\n## Initialize an in-memory document store\ndocument_store = InMemoryDocumentStore()\n\n## Convert the product data into Haystack Document objects\ndocuments = [\n    Document(\n        content=item.product_name, \n        meta={\n            \"id\": item.uniq_id, \n            \"price\": item.selling_price, \n            \"url\": item.product_url\n        }\n    ) for item in df.itertuples()\n]\n\n## Create a pipeline for indexing the documents\nindexing_pipeline = Pipeline()\n\n## Add a document embedder to the pipeline using Sentence Transformers model\nindexing_pipeline.add_component(\n    instance=SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\"), name=\"doc_embedder\"\n)\n\n## Add a document writer to the pipeline to store documents in the document store\nindexing_pipeline.add_component(instance=DocumentWriter(document_store=document_store), name=\"doc_writer\")\n\n## Connect the embedder's output to the writer's input\nindexing_pipeline.connect(\"doc_embedder.documents\", \"doc_writer.documents\")\n\n## Run the indexing pipeline to process and store the documents\nindexing_pipeline.run({\"doc_embedder\": {\"documents\": documents}})\n```\nå¾ˆå¥½ï¼Œæˆ‘ä»¬å·²å®Œæˆ AI ä»£ç†åº”ç”¨ç¨‹åºçš„ç¬¬ä¸€æ­¥ã€‚çŽ°åœ¨æ˜¯æ—¶å€™æž„å»ºäº§å“è¯†åˆ«å·¥å…·äº†ã€‚ä¸ºäº†æ›´å¥½åœ°ç†è§£äº§å“è¯†åˆ«å™¨çš„ä¸»è¦ä»»åŠ¡ï¼Œè®©æˆ‘ä»¬è€ƒè™‘ä¸‹é¢çš„ç¤ºä¾‹ã€‚\n\n> ç”¨æˆ·æŸ¥è¯¢ï¼šæˆ‘æƒ³ä¹°ä¸€åŒéœ²è¥é´ã€ä¸€å°ç‚­çƒ¤ç‚‰å’Œä¸€ä¸ª Google Pixel 9 çš„æ‰‹æœºå£³ã€‚è®©æˆ‘ä»¬ç†è§£äº§å“è¯†åˆ«åŠŸèƒ½çš„ç†æƒ³å·¥ä½œæµç¨‹ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kXGYjlMi4pQcqIKpmUZLRQ.png)\n\né¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ªå·¥å…·æ¥åˆ†æžç”¨æˆ·æŸ¥è¯¢å¹¶è¯†åˆ«ç”¨æˆ·æ„Ÿå…´è¶£çš„äº§å“ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„ä»£ç ç‰‡æ®µæž„å»ºè¿™æ ·çš„å·¥å…·ã€‚\n\n### æž„å»ºç”¨æˆ·æŸ¥è¯¢åˆ†æžå™¨\n\n\n```python\ntemplate = \"\"\"\nUnderstand the user query and list of products the user is interested in and return product names as list.\nYou should always return a Python list. Do not return any explanation.\n\nExamples:\nQuestion: I am interested in camping boots, charcoal and disposable rain jacket.\nAnswer: [\"camping_boots\",\"charcoal\",\"disposable_rain_jacket\"]\n\nQuestion: Need a laptop, wireless mouse, and noise-cancelling headphones for work.\nAnswer: [\"laptop\",\"wireless_mouse\",\"noise_cancelling_headphones\"]\n\nQuestion: {{ question }}\nAnswer:\n\"\"\"\n\nproduct_identifier = Pipeline()\n\nproduct_identifier.add_component(\"prompt_builder\", PromptBuilder(template=template))\nproduct_identifier.add_component(\"llm\", generator())\n\nproduct_identifier.connect(\"prompt_builder\", \"llm\")\n```\nå¥½çš„ï¼ŒçŽ°åœ¨æˆ‘ä»¬å·²ç»å®Œæˆäº†ç¬¬ä¸€ä¸ªå‡½æ•°çš„ä¸€åŠï¼ŒçŽ°åœ¨æ˜¯æ—¶å€™é€šè¿‡æ·»åŠ RAGç®¡é“æ¥å®Œæˆè¿™ä¸ªå‡½æ•°ã€‚ \n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*JyxINdc8Wz-qAg_PCAkLbA.png)\n\n### åˆ›å»º RAG ç®¡é“\n\n\n```python\ntemplate = \"\"\"\nReturn product name, price, and url as a python dictionary. \nYou should always return a Python dictionary with keys price, name and url for single product.\nYou should always return a Python list of dictionaries with keys price, name and url for multiple products.\nDo not return any explanation.\n\nLegitimate Response Schema:\n{\"price\": \"float\", \"name\": \"string\", \"url\": \"string\"}\nLegitimate Response Schema for multiple products:\n[{\"price\": \"float\", \"name\": \"string\", \"url\": \"string\"},{\"price\": \"float\", \"name\": \"string\", \"url\": \"string\"}]\n\nContext:\n{% for document in documents %}\n    product_price: {{ document.meta['price'] }}\n    product_url: {{ document.meta['url'] }}\n    product_id: {{ document.meta['id'] }}\n    product_name: {{ document.content }}\n{% endfor %}\nQuestion: {{ question }}\nAnswer:\n\"\"\"\n\nrag_pipe = Pipeline()\nrag_pipe.add_component(\"embedder\", SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\"))\nrag_pipe.add_component(\"retriever\", InMemoryEmbeddingRetriever(document_store=document_store, top_k=5))\nrag_pipe.add_component(\"prompt_builder\", PromptBuilder(template=template))\nrag_pipe.add_component(\"llm\", generator())\n\nrag_pipe.connect(\"embedder.embedding\", \"retriever.query_embedding\")\nrag_pipe.connect(\"retriever\", \"prompt_builder.documents\")\nrag_pipe.connect(\"prompt_builder\", \"llm\")\n```\nåœ¨è¿™ä¸ªé˜¶æ®µï¼Œæˆ‘ä»¬å·²ç»å®Œæˆäº† RAG å’ŒæŸ¥è¯¢åˆ†æžå™¨ç®¡é“ã€‚çŽ°åœ¨æ˜¯æ—¶å€™å°†å…¶è½¬æ¢ä¸ºå·¥å…·äº†ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¸¸è§„çš„å‡½æ•°å£°æ˜Žï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚ä¸ºä»£ç†åˆ›å»ºå·¥å…·å°±åƒåˆ›å»ºä¸€ä¸ª Python å‡½æ•°ã€‚å¦‚æžœä½ æœ‰è¿™æ ·çš„é—®é¢˜\n\n\n> ä»£ç†å¦‚ä½•è°ƒç”¨è¿™ä¸ªå‡½æ•°ï¼Ÿ\n\nè§£å†³æ–¹æ¡ˆå¾ˆç®€å•ï¼šé€šè¿‡åˆ©ç”¨ç‰¹å®šæ¨¡åž‹çš„å·¥å…·æž¶æž„ï¼Œæˆ‘ä»¬è®¡åˆ’åœ¨æœªæ¥çš„æ­¥éª¤ä¸­çº³å…¥ã€‚ç›®å‰ï¼Œæ˜¯æ—¶å€™åˆ›å»ºä¸€ä¸ªåŒ…è£…å‡½æ•°ï¼Œæ—¢ä½¿ç”¨æŸ¥è¯¢åˆ†æžå™¨åˆä½¿ç”¨ RAG ç®¡é“ã€‚\n\nè®©æˆ‘ä»¬æ˜Žç¡®è¿™ä¸ªå‡½æ•°çš„ç›®æ ‡ã€‚\n\n**ç›®æ ‡ 1ï¼š** ç¡®å®šç”¨æˆ·æ„Ÿå…´è¶£çš„æ‰€æœ‰äº§å“ï¼Œå¹¶å°†å®ƒä»¬ä½œä¸ºåˆ—è¡¨è¿”å›žã€‚ **ç›®æ ‡ 2ï¼š** å¯¹äºŽæ¯ä¸ªè¯†åˆ«çš„äº§å“ï¼Œä»Žæ•°æ®åº“ä¸­æ£€ç´¢æœ€å¤šäº”ä¸ªäº§å“åŠå…¶å…ƒæ•°æ®ã€‚\n\n### å®Œæˆäº§å“è¯†åˆ«åŠŸèƒ½\n\n\n```python\ndef product_identifier_func(query: str):\n    \"\"\"\n    æ ¹æ®ç»™å®šçš„æŸ¥è¯¢è¯†åˆ«äº§å“å¹¶æ£€ç´¢æ¯ä¸ªè¯†åˆ«äº§å“çš„ç›¸å…³ç»†èŠ‚ã€‚\n\n    å‚æ•°ï¼š\n    query (str): ç”¨äºŽè¯†åˆ«äº§å“çš„æŸ¥è¯¢å­—ç¬¦ä¸²ã€‚\n\n    è¿”å›žï¼š\n    dict: ä¸€ä¸ªå­—å…¸ï¼Œé”®ä¸ºäº§å“åç§°ï¼Œå€¼ä¸ºæ¯ä¸ªäº§å“çš„è¯¦ç»†ä¿¡æ¯ã€‚å¦‚æžœæœªæ‰¾åˆ°äº§å“ï¼Œåˆ™è¿”å›žâ€œNo product foundâ€ã€‚\n    \"\"\"\n    product_understanding = product_identifier.run({\"prompt_builder\": {\"question\": query}})\n\n    try:\n        product_list = literal_eval(product_understanding[\"llm\"][\"replies\"][0])\n    except:\n        return \"No product found\"\n\n    results = {}\n\n    for product in product_list:\n        response = rag_pipe.run({\"embedder\": {\"text\": product}, \"prompt_builder\": {\"question\": product}})\n        try:\n            results[product] = literal_eval(response[\"llm\"][\"replies\"][0])\n        except:\n            results[product] = {}\n    \n    return results\n```\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*HWRTdWvvcw2MZP4uoaQdeQ.png)\n\nè‡³æ­¤ï¼Œæˆ‘ä»¬å®Œæˆäº†ä»£ç†çš„ç¬¬ä¸€ä¸ªå·¥å…·ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å®ƒæ˜¯å¦æŒ‰é¢„æœŸå·¥ä½œã€‚\n\n\n```python\nquery = \"I want crossbow and woodstock puzzle\"\n#execute function\nproduct_identifier_func(query)\n\n## {'crossbow': {'name': 'DB Longboards CoreFlex Crossbow 41\" Bamboo Fiberglass '\n##                        'Longboard Complete',\n##                'price': 237.68,\n##                'url': 'https://www.amazon.com/DB-Longboards-CoreFlex-Fiberglass-Longboard/dp/B07KMVJJK7'},\n##  'woodstock_puzzle': {'name': 'Woodstock- Collage 500 pc Puzzle',\n##                       'price': 17.49,\n##                       'url': 'https://www.amazon.com/Woodstock-Collage-500-pc-Puzzle/dp/B07MX21WWX'}}\n```\nå®ƒå·¥ä½œäº†ï¼ï¼ç„¶è€Œï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯è¿”å›žè¾“å‡ºçš„ç»“æž„ã€‚æ‚¨å¯ä»¥åœ¨ä¸‹é¢çœ‹åˆ°ä¸€èˆ¬çš„ç»“æž„ã€‚\n\n\n```python\n{\n    \"product_key\": {\n        \"name\": \"string\",\n        \"price\": \"float\",\n        \"url\": \"string\"\n    }\n}\n```\nè¿™æ­£æ˜¯æˆ‘ä»¬åœ¨RAGç®¡é“ä¸­å»ºè®®æ¨¡åž‹ç”Ÿæˆçš„å†…å®¹ã€‚ä¸‹ä¸€æ­¥ï¼Œè®©æˆ‘ä»¬æž„å»ºä¸€ä¸ªåä¸º`find_budget_friendly_option`çš„å¯é€‰å·¥å…·ã€‚\n\n\n```python\ndef find_budget_friendly_option(selected_product_details):\n    \"\"\"\n    ä¸ºæ¯ä¸ªäº§å“ç±»åˆ«æ‰¾åˆ°æœ€å…·é¢„ç®—å‹å¥½çš„é€‰é¡¹ã€‚\n\n    å‚æ•°ï¼š\n    selected_product_details (dict): ä¸€ä¸ªå­—å…¸ï¼Œé”®ä¸ºäº§å“ç±»åˆ«ï¼Œå€¼ä¸ºäº§å“è¯¦ç»†ä¿¡æ¯çš„åˆ—è¡¨ã€‚æ¯ä¸ªäº§å“è¯¦ç»†ä¿¡æ¯åº”ä¸ºåŒ…å«â€œpriceâ€é”®çš„å­—å…¸ã€‚\n\n    è¿”å›žï¼š\n    dict: ä¸€ä¸ªå­—å…¸ï¼Œé”®ä¸ºäº§å“ç±»åˆ«ï¼Œå€¼ä¸ºæ¯ä¸ªç±»åˆ«æœ€å…·é¢„ç®—å‹å¥½çš„äº§å“è¯¦ç»†ä¿¡æ¯ã€‚\n    \"\"\"\n    budget_friendly_options = {}\n    \n    for category, items in selected_product_details.items():\n        if isinstance(items, list):\n            lowest_price_item = min(items, key=lambda x: x['price'])\n        else:\n            lowest_price_item = items\n        \n        budget_friendly_options[category] = lowest_price_item\n    \n    return budget_friendly_options\n```\nå¥½çš„ï¼Œè®©æˆ‘ä»¬ä¸“æ³¨äºŽè¿™ä¸ªåº”ç”¨ç¨‹åºæœ€å…³é”®çš„æ–¹é¢ï¼Œå³ä½¿ä»£ç†æ ¹æ®éœ€è¦ä½¿ç”¨è¿™äº›åŠŸèƒ½ã€‚æ­£å¦‚æˆ‘ä»¬ä¹‹å‰æ‰€è®¨è®ºçš„ï¼Œè¿™å¯ä»¥é€šè¿‡æ¨¡åž‹ç‰¹å®šçš„å·¥å…·æž¶æž„æ¥å®žçŽ°ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦æ‰¾åˆ°ç‰¹å®šäºŽæ‰€é€‰æ¨¡åž‹çš„å·¥å…·æž¶æž„ã€‚å¹¸è¿çš„æ˜¯ï¼Œå®ƒåœ¨æ¨¡åž‹å¡ä¸­æåˆ° [è¿™é‡Œ](https://huggingface.co/Groq/Llama-3-Groq-70B-Tool-Use)ã€‚æˆ‘ä»¬éœ€è¦è°ƒæ•´å®ƒä»¥é€‚åº”æˆ‘ä»¬çš„ç”¨ä¾‹ã€‚\n\n### å®ŒæˆèŠå¤©æ¨¡æ¿\n\n\n```python\nchat_template = '''<|start_header_id|>system<|end_header_id|>\n\nYou are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n<tool_call>\n{\"name\": <function-name>,\"arguments\": <args-dict>}\n</tool_call>\n\nHere are the available tools:\n<tools>\n    {\n        \"name\": \"product_identifier_func\",\n        \"description\": \"To understand user interested products and its details\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"query\": {\n                    \"type\": \"string\",\n                    \"description\": \"The query to use in the search. Infer this from the user's message. It should be a question or a statement\"\n                }\n            },\n            \"required\": [\"query\"]\n        }\n    },\n    {\n        \"name\": \"find_budget_friendly_option\",\n        \"description\": \"Get the most cost-friendly option. If selected_product_details has morethan one key this should return most cost-friendly options\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"selected_product_details\": {\n                    \"type\": \"dict\",\n                    \"description\": \"Input data is a dictionary where each key is a category name, and its value is either a single dictionary with 'price', 'name', and 'url' keys or a list of such dictionaries; example: {'category1': [{'price': 10.5, 'name': 'item1', 'url': 'http://example.com/item1'}, {'price': 8.99, 'name': 'item2', 'url': 'http://example.com/item2'}], 'category2': {'price': 15.0, 'name': 'item3', 'url': 'http://example.com/item3'}}\"\n                }\n            },\n            \"required\": [\"selected_product_details\"]\n        }\n    }\n</tools><|eot_id|><|start_header_id|>user<|end_header_id|>\n\nI need to buy a crossbow<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n<tool_call>\n{\"id\":\"call_deok\",\"name\":\"product_identifier_func\",\"arguments\":{\"query\":\"I need to buy a crossbow\"}}\n</tool_call><|eot_id|><|start_header_id|>tool<|end_header_id|>\n\n<tool_response>\n{\"id\":\"call_deok\",\"result\":{'crossbow': {'price': 237.68,'name': 'crossbow','url': 'https://www.amazon.com/crossbow/dp/B07KMVJJK7'}}}\n</tool_response><|eot_id|><|start_header_id|>assistant<|end_header_id|>\n'''\nçŽ°åœ¨åªå‰©ä¸‹å‡ ä¸ªæ­¥éª¤ã€‚åœ¨åšä»»ä½•äº‹æƒ…ä¹‹å‰ï¼Œè®©æˆ‘ä»¬æµ‹è¯•ä¸€ä¸‹æˆ‘ä»¬çš„ä»£ç†ã€‚\n\n\n```python\n### æµ‹è¯•ä»£ç†\nmessages = [\n    ChatMessage.from_system(\n        chat_template\n    ),\n    ChatMessage.from_user(\"I need to buy a crossbow for my child and PokÃ©mon for myself.\"),\n]\n\nchat_generator = get_chat_generator()\nresponse = chat_generator.run(messages=messages)\npprint(response)\n\n### response\n{'replies': [ChatMessage(content='<tool_call>\\n'\n                                 '{\"id\": 0, \"name\": \"product_identifier_func\", '\n                                 '\"arguments\": {\"query\": \"I need to buy a '\n                                 'crossbow for my child\"}}\\n'\n                                 '</tool_call>\\n'\n                                 '<tool_call>\\n'\n                                 '{\"id\": 1, \"name\": \"product_identifier_func\", '\n                                 '\"arguments\": {\"query\": \"I need to buy a '\n                                 'Pokemon for myself\"}}\\n'\n                                 '</tool_call>',\n                         role=<ChatRole.ASSISTANT: 'assistant'>,\n                         name=None,\n                         meta={'finish_reason': 'stop',\n                               'index': 0,\n                               'model': 'llama3-groq-70b-8192-tool-use-preview',\n                               'usage': {'completion_time': 0.217823967,\n                                         'completion_tokens': 70,\n                                         'prompt_time': 0.041348261,\n                                         'prompt_tokens': 561,\n                                         'total_time': 0.259172228,\n                                         'total_tokens': 631}})]}\n```\nåˆ°æ­¤ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»å®Œæˆäº†å¤§çº¦90%çš„å·¥ä½œã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*nYVXcgpm3RZ3g5h5d4UK_A.png)\n\nåœ¨ä¸Šé¢çš„å“åº”ä¸­ï¼Œæ‚¨å¯èƒ½æ³¨æ„åˆ°XMLæ ‡ç­¾`<tool_call>`å°é—­äº†å·¥å…·è°ƒç”¨ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦å¼€å‘ä¸€ç§æœºåˆ¶æ¥æå–tool_callå¯¹è±¡ã€‚\n\n\n```python\ndef extract_tool_calls(tool_calls_str):\n    json_objects = re.findall(r'<tool_call>(.*?)</tool_call>', tool_calls_str, re.DOTALL)\n    \n    result_list = [json.loads(obj) for obj in json_objects]\n    \n    return result_list\n\navailable_functions = {\n    \"product_identifier_func\": product_identifier_func, \n    \"find_budget_friendly_option\": find_budget_friendly_option\n    }\n```\nå®Œæˆè¿™ä¸€æ­¥åŽï¼Œæˆ‘ä»¬å¯ä»¥ç›´æŽ¥è®¿é—®ä»£ç†çš„å“åº”ï¼Œå½“å®ƒè°ƒç”¨ä¸€ä¸ªå·¥å…·æ—¶ã€‚çŽ°åœ¨å”¯ä¸€å¾…åšçš„å°±æ˜¯èŽ·å–å·¥å…·è°ƒç”¨å¯¹è±¡å¹¶ç›¸åº”åœ°æ‰§è¡Œå‡½æ•°ã€‚è®©æˆ‘ä»¬å®Œæˆé‚£éƒ¨åˆ†ã€‚\n\n\n```python\nmessages.append(ChatMessage.from_user(message))\nresponse = chat_generator.run(messages=messages)\n\nif response and \"<tool_call>\" in response[\"replies\"][0].content:\n    function_calls = extract_tool_calls(response[\"replies\"][0].content)\n    for function_call in function_calls:\n        # Parse function calling information\n        function_name = function_call[\"name\"]\n        function_args = function_call[\"arguments\"]\n\n        # Find the corresponding function and call it with the given arguments\n        function_to_call = available_functions[function_name]\n        function_response = function_to_call(**function_args)\n\n        # Append function response to the messages list using `ChatMessage.from_function`\n        messages.append(ChatMessage.from_function(content=json.dumps(function_response), name=function_name))\n        response = chat_generator.run(messages=messages)\n```\nçŽ°åœ¨æ˜¯æ—¶å€™å°†æ¯ä¸ªç»„ä»¶ç»„åˆåœ¨ä¸€èµ·ï¼Œæž„å»ºä¸€ä¸ªåˆé€‚çš„èŠå¤©åº”ç”¨ç¨‹åºã€‚æˆ‘å°†ä½¿ç”¨Gradioæ¥å®žçŽ°è¿™ä¸ªç›®çš„ã€‚\n\n\n```python\nimport gradio as gr\n\nmessages = [ChatMessage.from_system(chat_template)]\nchat_generator = get_chat_generator()\n\ndef chatbot_with_fc(message, messages):\n    messages.append(ChatMessage.from_user(message))\n    response = chat_generator.run(messages=messages)\n\n    while True:\n        if response and \"<tool_call>\" in response[\"replies\"][0].content:\n            function_calls = extract_tool_calls(response[\"replies\"][0].content)\n            for function_call in function_calls:\n                # Parse function calling information\n                function_name = function_call[\"name\"]\n                function_args = function_call[\"arguments\"]\n\n                # Find the corresponding function and call it with the given arguments\n                function_to_call = available_functions[function_name]\n                function_response = function_to_call(**function_args)\n\n                # Append function response to the messages list using `ChatMessage.from_function`\n                messages.append(ChatMessage.from_function(content=json.dumps(function_response), name=function_name))\n                response = chat_generator.run(messages=messages)\n\n        # Regular Conversation\n        else:\n            messages.append(response[\"replies\"][0])\n            break\n    return response[\"replies\"][0].content\n\n\ndef chatbot_interface(user_input, state):\n    response_content = chatbot_with_fc(user_input, state)\n    return response_content, state\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# AI è´­ä¹°åŠ©æ‰‹\")\n    gr.Markdown(\"é—®æˆ‘å…³äºŽæ‚¨æƒ³è´­ä¹°çš„äº§å“ï¼\")\n    \n    state = gr.State(value=messages)\n    \n    with gr.Row():\n        user_input = gr.Textbox(label=\"æ‚¨çš„æ¶ˆæ¯ï¼š\")\n        response_output = gr.Markdown(label=\"å›žå¤ï¼š\")\n    \n    user_input.submit(chatbot_interface, [user_input, state], [response_output, state])\n    gr.Button(\"å‘é€\").click(chatbot_interface, [user_input, state], [response_output, state])\n\n\ndemo.launch()\n```\nå°±è¿™æ ·ï¼æˆ‘ä»¬æž„å»ºäº†åŸºäºŽLlama 3çš„AIä»£ç†ðŸ¤–ï¼Œå…·å¤‡å‡½æ•°è°ƒç”¨èƒ½åŠ›ã€‚æ‚¨å¯ä»¥ä»Žè¿™ä¸ª[GitHubä»“åº“](https://github.com/Ransaka/ai-agents-with-llama3)è®¿é—®å®Œæ•´ä»£ç ã€‚æ„Ÿè°¢æ‚¨çš„é˜…è¯»ã€‚\n\né€šè¿‡[è¿™ä¸ª](https://www.kaggle.com/datasets/promptcloud/amazon-product-dataset-2020)Kaggleé“¾æŽ¥ï¼ˆåœ¨CC0ï¼šå…¬å…±é¢†åŸŸä¸‹ï¼‰å¯ä»¥è®¿é—®æœ¬æ–‡ä½¿ç”¨çš„æ•°æ®é›†ã€‚\n\n### ç»“è®º\n\nåœ¨æž„å»ºåŸºäºŽAIä»£ç†çš„ç³»ç»Ÿæ—¶ï¼Œè€ƒè™‘å®Œæˆä»»åŠ¡æ‰€éœ€çš„æ—¶é—´å’Œæ¯ä¸ªä»»åŠ¡ä½¿ç”¨çš„APIè°ƒç”¨ï¼ˆä»¤ç‰Œï¼‰æ•°é‡éžå¸¸é‡è¦ã€‚ä¸€ä¸ªä¸»è¦çš„æŒ‘æˆ˜æ˜¯å‡å°‘ç³»ç»Ÿä¸­çš„å¹»è§‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ´»è·ƒçš„ç ”ç©¶é¢†åŸŸã€‚å› æ­¤ï¼Œæž„å»ºLLMå’Œä»£ç†ç³»ç»Ÿæ²¡æœ‰å›ºå®šçš„è§„åˆ™ã€‚å¿…é¡»è€å¿ƒè€Œæœ‰ç­–ç•¥åœ°å·¥ä½œï¼Œä»¥ç¡®ä¿AIä»£ç†ï¼Œå³LLMï¼Œæ­£å¸¸è¿è¡Œã€‚\n\n*é™¤éžå¦æœ‰è¯´æ˜Žï¼Œæ‰€æœ‰å›¾ç‰‡å‡ç”±ä½œè€…æä¾›ã€‚*\n\n### å‚è€ƒï¼š\n\n[https://docs.together.ai/docs/llama\\-3\\-function\\-calling](https://docs.together.ai/docs/llama-3-function-calling)\n\n"},{"lang":"zh","group":"blog","slug":"blog/visualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-with-ragas-fc2486308557","frontmatter":{"title":"å¯è§†åŒ–ä½ çš„ RAG æ•°æ®â€”â€”ä½¿ç”¨ Ragas è¯„ä¼°ä½ çš„æ£€ç´¢å¢žå¼ºç”Ÿæˆç³»ç»Ÿ","meta_title":"å¯è§†åŒ–ä½ çš„ RAG æ•°æ®â€”â€”ä½¿ç”¨ Ragas è¯„ä¼°ä½ çš„æ£€ç´¢å¢žå¼ºç”Ÿæˆç³»ç»Ÿ","description":"å¦‚ä½•ä½¿ç”¨ UMAP é™ç»´å¯¹åµŒå…¥è¿›è¡Œå¤„ç†ä»¥æ˜¾ç¤ºå¤šä¸ªè¯„ä¼°é—®é¢˜åŠå…¶ä¸Žæºæ–‡æ¡£çš„å…³ç³»â€¦â€¦","date":"2024-11-04T12:35:56.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*peWTe1A-MqeROT_Jdof_Cw.gif","categories":["Natural Language Processing","Generative AI","Data Science"],"author":"Rifx.Online","tags":["RAG","UMAP","embeddings","evaluation","visualization"],"draft":false,"slug":"blog/visualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-with-ragas-fc2486308557"},"content":"\n\n\n### å¦‚ä½•ä½¿ç”¨ UMAP é™ç»´å°†åµŒå…¥å¯è§†åŒ–ä»¥å±•ç¤ºå¤šä¸ªè¯„ä¼°é—®é¢˜åŠå…¶ä¸Žæºæ–‡æ¡£çš„å…³ç³»ï¼Œç»“åˆ Ragasã€OpenAIã€Langchain å’Œ ChromaDB\n\næ£€ç´¢å¢žå¼ºç”Ÿæˆï¼ˆRAGï¼‰åœ¨ LLM çš„å·¥ä½œæµç¨‹ä¸­å¢žåŠ äº†ä¸€ä¸ªæ£€ç´¢æ­¥éª¤ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨å›žç­”é—®é¢˜å’ŒæŸ¥è¯¢æ—¶ï¼Œä»Žç§äººæ–‡æ¡£ç­‰é¢å¤–æ¥æºæŸ¥è¯¢ç›¸å…³æ•°æ® \\[1]ã€‚è¯¥å·¥ä½œæµç¨‹ä¸éœ€è¦å¯¹é¢å¤–æ–‡æ¡£è¿›è¡Œæ˜‚è´µçš„è®­ç»ƒæˆ–å¾®è°ƒã€‚æ–‡æ¡£è¢«æ‹†åˆ†æˆç‰‡æ®µï¼Œç„¶åŽè¿›è¡Œç´¢å¼•ï¼Œé€šå¸¸ä½¿ç”¨ç´§å‡‘çš„ ML ç”Ÿæˆçš„å‘é‡è¡¨ç¤ºï¼ˆåµŒå…¥ï¼‰ã€‚å†…å®¹ç›¸ä¼¼çš„ç‰‡æ®µåœ¨è¿™ä¸ªåµŒå…¥ç©ºé—´ä¸­ä¼šå½¼æ­¤é è¿‘ã€‚\n\nRAG åº”ç”¨å°†ç”¨æˆ·æä¾›çš„é—®é¢˜æŠ•å½±åˆ°åµŒå…¥ç©ºé—´ï¼Œä»¥æ ¹æ®ä¸Žé—®é¢˜çš„è·ç¦»æ£€ç´¢ç›¸å…³æ–‡æ¡£ç‰‡æ®µã€‚LLM å¯ä»¥ä½¿ç”¨æ£€ç´¢åˆ°çš„ä¿¡æ¯æ¥å›žç­”æŸ¥è¯¢ï¼Œå¹¶é€šè¿‡å‘ˆçŽ°ç‰‡æ®µä½œä¸ºå‚è€ƒæ¥è¯æ˜Žå…¶ç»“è®ºã€‚\n\n\n\nè¯„ä¼° RAG åº”ç”¨æ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„ \\[2]ã€‚å­˜åœ¨ä¸åŒçš„æ–¹æ³•ï¼šä¸€æ–¹é¢ï¼Œæœ‰äº›æ–¹æ³•è¦æ±‚å¼€å‘è€…æä¾›ç­”æ¡ˆä½œä¸ºçœŸå®žå€¼ï¼›å¦ä¸€æ–¹é¢ï¼Œç­”æ¡ˆï¼ˆå’Œé—®é¢˜ï¼‰ä¹Ÿå¯ä»¥ç”±å¦ä¸€ä¸ª LLM ç”Ÿæˆã€‚æœ€å¤§çš„å¼€æº LLM æ”¯æŒå›žç­”ç³»ç»Ÿä¹‹ä¸€æ˜¯ Ragas \\[4](æ£€ç´¢å¢žå¼ºç”Ÿæˆè¯„ä¼°)ï¼Œå®ƒæä¾›\n\n* åŸºäºŽæ–‡æ¡£ç”Ÿæˆæµ‹è¯•æ•°æ®çš„æ–¹æ³•ï¼Œä»¥åŠ\n* åŸºäºŽä¸åŒæŒ‡æ ‡é€æ­¥å’Œç«¯åˆ°ç«¯è¯„ä¼°æ£€ç´¢å’Œç”Ÿæˆæ­¥éª¤çš„è¯„ä¼°ã€‚\n\nåœ¨æœ¬æ–‡ä¸­ï¼Œæ‚¨å°†å­¦ä¹ \n\n* å¦‚ä½•ç®€è¦æž„å»ºä¸€ä¸ª Formula One çš„ RAG ç³»ç»Ÿï¼ˆæœ‰å…³è¯¦ç»†æè¿°ï¼Œè¯·å‚é˜…ä¹‹å‰çš„æ–‡ç«  [å¯è§†åŒ–æ‚¨çš„ RAG æ•°æ® â€” æ£€ç´¢å¢žå¼ºç”Ÿæˆçš„ EDA](https://readmedium.com/visualize-your-rag-data-eda-for-retrieval-augmented-generation-0701ee98768f)ï¼‰\n* ç”Ÿæˆé—®é¢˜å’Œç­”æ¡ˆ\n* ä½¿ç”¨ [Ragas](https://github.com/explodinggradients/ragas) è¯„ä¼° RAG ç³»ç»Ÿ\n* æœ€é‡è¦çš„æ˜¯ï¼Œå¦‚ä½•ä½¿ç”¨ [Renumics Spotlight](https://github.com/Renumics/spotlight) å¯è§†åŒ–ç»“æžœå¹¶è§£è¯»ç»“æžœã€‚\n\nä»£ç å¯åœ¨ Github ä¸ŠèŽ·å–ã€‚\n\n## å‡†å¤‡ä½ çš„çŽ¯å¢ƒ\n\nå¯åŠ¨ä¸€ä¸ªç¬”è®°æœ¬å¹¶å®‰è£…æ‰€éœ€çš„ python åŒ…\n\n```python\n!pip install langchain langchain-openai chromadb renumics-spotlight\n%env OPENAI_API_KEY=<your-api-key>\n```\næœ¬æ•™ç¨‹ä½¿ç”¨ä»¥ä¸‹ python åŒ…ï¼š\n\n* [**Langchain**](https://github.com/langchain-ai/langchain): ä¸€ä¸ªé›†æˆè¯­è¨€æ¨¡åž‹å’Œ RAG ç»„ä»¶çš„æ¡†æž¶ï¼Œä½¿è®¾ç½®è¿‡ç¨‹æ›´åŠ é¡ºç•…ã€‚\n* [**Renumics\\-Spotlight**](https://github.com/Renumics/spotlight): ä¸€ä¸ªå¯è§†åŒ–å·¥å…·ï¼Œç”¨äºŽäº¤äº’å¼æŽ¢ç´¢éžç»“æž„åŒ–çš„æœºå™¨å­¦ä¹ æ•°æ®é›†ã€‚\n* [**Ragas**](https://github.com/explodinggradients/ragas): ä¸€ä¸ªå¸®åŠ©ä½ è¯„ä¼° RAG ç®¡é“çš„æ¡†æž¶\n\n*å…è´£å£°æ˜Žï¼šæœ¬æ–‡ä½œè€…ä¹Ÿæ˜¯ Spotlight çš„å¼€å‘è€…ä¹‹ä¸€ã€‚*\n\n## ä¸ºæ•°æ®é›†å‡†å¤‡æ–‡æ¡£å’ŒåµŒå…¥\n\næ‚¨å¯ä»¥ä½¿ç”¨è‡ªå·±çš„ RAG åº”ç”¨ç¨‹åºï¼Œè·³åˆ°ä¸‹ä¸€éƒ¨åˆ†äº†è§£å¦‚ä½•è¯„ä¼°ã€æå–å’Œå¯è§†åŒ–ã€‚\n\næˆ–è€…æ‚¨å¯ä»¥ä½¿ç”¨æ¥è‡ª[ä¸Šä¸€ç¯‡æ–‡ç« ](https://readmedium.com/visualize-your-rag-data-eda-for-retrieval-augmented-generation-0701ee98768f)çš„ RAG åº”ç”¨ç¨‹åºï¼Œé…åˆ[æˆ‘ä»¬å‡†å¤‡çš„æ‰€æœ‰ç»´åŸºç™¾ç§‘ Formula One æ–‡ç« çš„æ•°æ®é›†](https://spotlightpublic.blob.core.windows.net/docs-data/rag_demo/docs.zip)ã€‚æ‚¨è¿˜å¯ä»¥å°†è‡ªå·±çš„æ–‡æ¡£æ’å…¥åˆ°â€œdocs/â€å­æ–‡ä»¶å¤¹ä¸­ã€‚\n\n> æ­¤æ•°æ®é›†åŸºäºŽæ¥è‡ª[ç»´åŸºç™¾ç§‘](https://www.wikipedia.org/)çš„æ–‡ç« ï¼Œå¹¶æ ¹æ®çŸ¥è¯†å…±äº«ç½²å-ç›¸åŒæ–¹å¼å…±äº«è®¸å¯åè®®è¿›è¡Œè®¸å¯ã€‚åŽŸå§‹æ–‡ç« åŠä½œè€…åˆ—è¡¨å¯ä»¥åœ¨ç›¸åº”çš„ç»´åŸºç™¾ç§‘é¡µé¢ä¸­æ‰¾åˆ°ã€‚\n\nçŽ°åœ¨æ‚¨å¯ä»¥ä½¿ç”¨ Langchain çš„ `DirectoryLoader` ä»Ž docs å­ç›®å½•åŠ è½½æ‰€æœ‰æ–‡ä»¶ï¼Œå¹¶ä½¿ç”¨ `RecursiveCharacterTextSpliter` å°†æ–‡æ¡£æ‹†åˆ†ä¸ºç‰‡æ®µã€‚é€šè¿‡ `OpenAIEmbeddings`ï¼Œæ‚¨å¯ä»¥åˆ›å»ºåµŒå…¥å¹¶å°†å…¶å­˜å‚¨åœ¨ `ChromaDB` ä¸­ä½œä¸ºå‘é‡å­˜å‚¨ã€‚å¯¹äºŽ Chain æœ¬èº«ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ LangChains çš„ `ChatOpenAI` å’Œ `ChatPromptTemplate`ã€‚\n\næœ¬æ–‡çš„[é“¾æŽ¥ä»£ç ](https://github.com/Renumics/rag-demo/blob/main/notebooks/visualize_rag_tutorial_qs.ipynb)åŒ…å«æ‰€æœ‰å¿…è¦æ­¥éª¤ï¼Œæ‚¨å¯ä»¥åœ¨[ä¸Šä¸€ç¯‡æ–‡ç« ](https://readmedium.com/visualize-your-rag-data-eda-for-retrieval-augmented-generation-0701ee98768f)ä¸­æ‰¾åˆ°ä¸Šè¿°æ‰€æœ‰æ­¥éª¤çš„è¯¦ç»†æè¿°ã€‚\n\nä¸€ä¸ªé‡è¦çš„ç‚¹æ˜¯ï¼Œæ‚¨åº”è¯¥ä½¿ç”¨å“ˆå¸Œå‡½æ•°ä¸º `ChromaDB` ä¸­çš„ç‰‡æ®µåˆ›å»º IDã€‚è¿™å…è®¸åœ¨ä»…æ‹¥æœ‰æ–‡æ¡£åŠå…¶å†…å®¹å’Œå…ƒæ•°æ®çš„æƒ…å†µä¸‹æ‰¾åˆ°æ•°æ®åº“ä¸­çš„åµŒå…¥ã€‚è¿™ä½¿å¾—å¯ä»¥è·³è¿‡å·²ç»å­˜åœ¨äºŽæ•°æ®åº“ä¸­çš„æ–‡æ¡£ã€‚\n\n```python\nimport hashlib\nimport json\nfrom langchain_core.documents import Document\n\ndef stable_hash_meta(doc: Document) -> str:\n    \"\"\"\n    Stable hash document based on its metadata.\n    \"\"\"\n    return hashlib.sha1(json.dumps(doc.metadata, sort_keys=True).encode()).hexdigest()\n\n...\nsplits = text_splitter.split_documents(docs)\nsplits_ids = [\n    {\"doc\": split, \"id\": stable_hash_meta(split.metadata)} for split in splits\n]\n\nexisting_ids = docs_vectorstore.get()[\"ids\"]\nnew_splits_ids = [split for split in splits_ids if split[\"id\"] not in existing_ids]\n\ndocs_vectorstore.add_documents(\n    documents=[split[\"doc\"] for split in new_splits_ids],\n    ids=[split[\"id\"] for split in new_splits_ids],\n)\ndocs_vectorstore.persist()\n```\n\n## è¯„ä¼°é—®é¢˜\n\nå¯¹äºŽåƒä¸€çº§æ–¹ç¨‹å¼è¿™æ ·çš„å¸¸è§ä¸»é¢˜ï¼Œå¯ä»¥ç›´æŽ¥ä½¿ç”¨ ChatGPT ç”Ÿæˆä¸€èˆ¬æ€§é—®é¢˜ã€‚æœ¬æ–‡ä½¿ç”¨äº†å››ç§é—®é¢˜ç”Ÿæˆæ–¹æ³•ï¼š\n\n* **GPT4**: ä½¿ç”¨ ChatGPT 4 ç”Ÿæˆäº† 30 ä¸ªé—®é¢˜ï¼Œæç¤ºä¸ºâ€œå†™ 30 ä¸ªå…³äºŽä¸€çº§æ–¹ç¨‹å¼çš„é—®é¢˜â€\nâ€“ éšæœºç¤ºä¾‹ï¼šâ€œå“ªä¸ªä¸€çº§æ–¹ç¨‹å¼è½¦é˜Ÿä»¥å…¶è·ƒé©¬æ ‡å¿—è€Œé—»åï¼Ÿâ€\n* **GPT3\\.5:** ä½¿ç”¨ ChatGPT 3\\.5 ç”Ÿæˆäº†å¦å¤– 199 ä¸ªé—®é¢˜ï¼Œæç¤ºä¸ºâ€œå†™ 100 ä¸ªå…³äºŽä¸€çº§æ–¹ç¨‹å¼çš„é—®é¢˜â€ï¼Œå¹¶é‡å¤â€œè°¢è°¢ï¼Œå†å†™ 100 ä¸ªå§â€\nâ€“ ç¤ºä¾‹ï¼šâ€œå“ªä½è½¦æ‰‹åœ¨ 1950 å¹´èµ¢å¾—äº†é¦–å±Šä¸€çº§æ–¹ç¨‹å¼ä¸–ç•Œé”¦æ ‡èµ›ï¼Ÿâ€\n* **Ragas\\_GPT4**: ä½¿ç”¨ Ragas ç”Ÿæˆäº† 113 ä¸ªé—®é¢˜ã€‚Ragas å†æ¬¡åˆ©ç”¨æ–‡æ¡£åŠå…¶è‡ªèº«çš„åµŒå…¥æ¨¡åž‹æž„å»ºä¸€ä¸ªå‘é‡æ•°æ®åº“ï¼Œç„¶åŽç”¨ GPT4 ç”Ÿæˆé—®é¢˜ã€‚\nâ€“ ç¤ºä¾‹ï¼šâ€œä½ èƒ½å‘Šè¯‰æˆ‘æ›´å¤šå…³äºŽä¹”ä¸¹ 198 ä¸€çº§æ–¹ç¨‹å¼èµ›è½¦åœ¨ 1998 å¹´ä¸–ç•Œé”¦æ ‡èµ›ä¸­çš„è¡¨çŽ°å—ï¼Ÿâ€\n* **Rags\\_GPT3\\.5**: ä½¿ç”¨ Ragas ç”Ÿæˆäº† 226 ä¸ªé¢å¤–é—®é¢˜â€”â€”è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ GPT3\\.5\nâ€“ ç¤ºä¾‹ï¼šâ€œåœ¨ 2014 å¹´æ¯”åˆ©æ—¶å¤§å¥–èµ›ä¸Šå‘ç”Ÿäº†ä»€ä¹ˆäº‹ä»¶å¯¼è‡´æ±‰å¯†å°”é¡¿é€€èµ›ï¼Ÿâ€\n\n```python\nfrom ragas.testset import TestsetGenerator\n\ngenerator = TestsetGenerator.from_default(\n    openai_generator_llm=\"gpt-3.5-turbo-16k\", \n    openai_filter_llm=\"gpt-3.5-turbo-16k\"\n)\n\ntestset_ragas_gpt35 = generator.generate(docs, 100)\n```\né—®é¢˜å’Œç­”æ¡ˆæ²¡æœ‰ç»è¿‡å®¡æ ¸æˆ–ä¿®æ”¹ã€‚æ‰€æœ‰é—®é¢˜éƒ½åˆå¹¶åœ¨ä¸€ä¸ªå•ä¸€çš„æ•°æ®æ¡†ä¸­ï¼ŒåŒ…å« `id`ã€`question`ã€`ground_truth`ã€`question_by` å’Œ `answer` åˆ—ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*R_74K0-_SJXyTxq6ovAcWg.png)\n\næŽ¥ä¸‹æ¥ï¼Œé—®é¢˜å°†è¢«æå‡ºç»™ RAG ç³»ç»Ÿã€‚å¯¹äºŽè¶…è¿‡ 500 ä¸ªé—®é¢˜ï¼Œè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´å¹¶äº§ç”Ÿè´¹ç”¨ã€‚å¦‚æžœé€è¡Œè¯¢é—®é—®é¢˜ï¼Œå¯ä»¥æš‚åœå¹¶ç»§ç»­è¯¥è¿‡ç¨‹ï¼Œæˆ–è€…åœ¨å´©æºƒåŽæ¢å¤ï¼Œè€Œä¸ä¼šä¸¢å¤±åˆ°ç›®å‰ä¸ºæ­¢çš„ç»“æžœï¼š\n\n```python\nfor i, row in df_questions_answers.iterrows():\n    if row[\"answer\"] is None or pd.isnull(row[\"answer\"]):\n        response = rag_chain.invoke(row[\"question\"])\n\n        df_questions_answers.loc[df_questions_answers.index[i], \"answer\"] = response[\n            \"answer\"\n        ]\n        df_questions_answers.loc[df_questions_answers.index[i], \"source_documents\"] = [\n            stable_hash_meta(source_document.metadata)\n            for source_document in response[\"source_documents\"]\n        ]\n\n```\nä¸ä»…å­˜å‚¨äº†ç­”æ¡ˆï¼Œè¿˜å­˜å‚¨äº†æ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µçš„æº ID åŠå…¶æ–‡æœ¬å†…å®¹ä½œä¸ºä¸Šä¸‹æ–‡ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*umlKv7Qf9SSLzRslT2r0Qw.png)\n\næ­¤å¤–ï¼Œè¿˜ç”Ÿæˆå¹¶å­˜å‚¨äº†æ‰€æœ‰é—®é¢˜çš„åµŒå…¥ï¼Œå¹¶å°†å…¶å­˜å‚¨åœ¨æ•°æ®æ¡†ä¸­ã€‚è¿™ä½¿å¾—å¯ä»¥å°†å®ƒä»¬ä¸Žæ–‡æ¡£ä¸€èµ·å¯è§†åŒ–ã€‚\n\n## ä½¿ç”¨ Ragas è¿›è¡Œè¯„ä¼°\n\n[Ragas](https://github.com/explodinggradients/ragas) æä¾›äº†è¯„ä¼°æ‚¨çš„ RAG æµæ°´çº¿ä¸­æ¯ä¸ªç»„ä»¶çš„æŒ‡æ ‡ï¼Œä»¥åŠæ•´ä½“æ€§èƒ½çš„ç«¯åˆ°ç«¯æŒ‡æ ‡ï¼š\n\n1. **ä¸Šä¸‹æ–‡ç²¾ç¡®åº¦ï¼š** ä½¿ç”¨ `question` å’Œæ£€ç´¢åˆ°çš„ `contexts` æ¥æµ‹é‡ä¿¡å·ä¸Žå™ªå£°çš„æ¯”çŽ‡ã€‚\n2. **ä¸Šä¸‹æ–‡ç›¸å…³æ€§ï¼š** æµ‹é‡æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¸Žé—®é¢˜çš„ç›¸å…³æ€§ï¼Œä½¿ç”¨ `question` å’Œ `contexts` è®¡ç®—ã€‚\n3. **ä¸Šä¸‹æ–‡å¬å›žçŽ‡ï¼š** åŸºäºŽ `ground truth` å’Œ `contexts` æ£€æŸ¥æ˜¯å¦æ£€ç´¢åˆ°æ‰€æœ‰ä¸Žç­”æ¡ˆç›¸å…³çš„ä¿¡æ¯ã€‚\n4. **å¿ å®žåº¦ï¼š** åˆ©ç”¨ `contexts` å’Œ `answer` æ¥è¡¡é‡ç”Ÿæˆç­”æ¡ˆçš„äº‹å®žå‡†ç¡®æ€§ã€‚\n5. **ç­”æ¡ˆç›¸å…³æ€§ï¼š** ä½¿ç”¨ `question` å’Œ `answer` è®¡ç®—ï¼Œè¯„ä¼°ç”Ÿæˆçš„ç­”æ¡ˆä¸Žé—®é¢˜çš„ç›¸å…³æ€§ï¼ˆä¸è€ƒè™‘äº‹å®žæ€§ï¼‰ã€‚\n6. **ç­”æ¡ˆè¯­ä¹‰ç›¸ä¼¼åº¦ï¼š** ä½¿ç”¨ `ground truth` å’Œ `answer` è¿›è¡Œè¯„ä¼°ï¼Œä»¥åˆ¤æ–­ç”Ÿæˆç­”æ¡ˆä¸Žæ­£ç¡®ç­”æ¡ˆä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼æ€§ã€‚\n7. **ç­”æ¡ˆæ­£ç¡®æ€§ï¼š** ä¾èµ–äºŽ `ground truth` å’Œ `answer` æ¥è¡¡é‡ç”Ÿæˆç­”æ¡ˆçš„å‡†ç¡®æ€§å’Œä¸Žæ­£ç¡®ç­”æ¡ˆçš„ä¸€è‡´æ€§ã€‚\n8. **æ–¹é¢è¯„ä¼°ï¼š** æ¶‰åŠåˆ†æž `answer` ä»¥æ ¹æ®é¢„å®šä¹‰æˆ–è‡ªå®šä¹‰æ–¹é¢ï¼ˆå¦‚æ­£ç¡®æ€§æˆ–æœ‰å®³æ€§ï¼‰è¯„ä¼°æäº¤ç»“æžœã€‚\n\nç›®å‰ï¼Œæˆ‘ä»¬ä¸“æ³¨äºŽç­”æ¡ˆæ­£ç¡®æ€§çš„ç«¯åˆ°ç«¯æŒ‡æ ‡ã€‚æ•°æ®æ¡†ä¸­çš„åˆ—åå’Œå†…å®¹å·²å¤åˆ¶å¹¶è°ƒæ•´ï¼Œä»¥ç¬¦åˆ Ragas API çš„å‘½åå’Œæ ¼å¼è¦æ±‚ï¼š\n\n```python\n## prepare the dataframe for evaluation\ndf_qa_eval = df_questions_answers.copy()\n\n\n## adapt the ground truth to the ragas naming and format\ndf_qa_eval.rename(columns={\"ground_truth\": \"ground_truths\"}, inplace=True)\ndf_qa_eval[\"ground_truths\"] = [\n    [gt] if not isinstance(gt, list) else gt for gt in df_qa_eval[\"ground_truths\"]\n]\n```\nè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œç”šè‡³æ¯”ä»…æŸ¥è¯¢æ‚¨çš„ RAG ç³»ç»ŸèŠ±è´¹æ›´å¤šçš„é‡‘é’±ã€‚è®©æˆ‘ä»¬é€è¡Œåº”ç”¨è¯„ä¼°ï¼Œä»¥ä¾¿åœ¨å´©æºƒåŽèƒ½å¤Ÿæ¢å¤è€Œä¸ä¸¢å¤±åˆ°ç›®å‰ä¸ºæ­¢çš„ç»“æžœï¼š\n\n```python\n## evaluate the answer correctness if not already done\nfields = [\"question\", \"answer\", \"contexts\", \"ground_truths\"]\nfor i, row in df_qa_eval.iterrows():\n    if row[\"answer_correctness\"] is None or pd.isnull(row[\"answer_correctness\"]):\n        evaluation_result = evaluate(\n            Dataset.from_pandas(df_qa_eval.iloc[i : i + 1][fields]),\n            [answer_correctness],\n        )\n        df_qa_eval.loc[i, \"answer_correctness\"] = evaluation_result[\n            \"answer_correctness\"\n        ]\n\n```\nä¹‹åŽï¼Œæ‚¨å¯ä»¥å°†ç»“æžœå­˜å‚¨åœ¨ `df_questions_answer` æ•°æ®æ¡†ä¸­ï¼š\n\n```python\ndf_questions_answers[\"answer_correctness\"] = df_qa_eval[\"answer_correctness\"]\n```\n\n## å‡†å¤‡å¯è§†åŒ–\n\nä¸ºäº†åœ¨å¯è§†åŒ–ä¸­åŒ…å«æ–‡æ¡£ç‰‡æ®µï¼Œæˆ‘ä»¬æ·»åŠ äº†ä»Žæ–‡æ¡£åˆ°ä½¿ç”¨è¯¥æ–‡æ¡£ä½œä¸ºæ¥æºçš„é—®é¢˜çš„å¼•ç”¨ã€‚æ­¤å¤–ï¼Œå¼•ç”¨æ–‡æ¡£çš„é—®é¢˜æ•°é‡ä¹Ÿè¢«å­˜å‚¨ï¼š\n\n```python\n## Explode 'source_documents' so each document ID is in its own row alongside the question ID\ndf_questions_exploded = df_qa_eval.explode(\"source_documents\")\n\n## Group by exploded 'source_documents' (document IDs) and aggregate\nagg = (\n    df_questions_exploded.groupby(\"source_documents\")\n    .agg(\n        num_questions=(\"id\", \"count\"),  # Count of questions referencing the document\n        question_ids=(\n            \"id\",\n            lambda x: list(x),\n        ),  # List of question IDs referencing the document\n    )\n    .reset_index()\n    .rename(columns={\"source_documents\": \"id\"})\n)\n\n## Merge the aggregated information back into df_documents\ndf_documents_agg = pd.merge(df_docs, agg, on=\"id\", how=\"left\")\n\n## Use apply to replace NaN values with empty lists for 'question_ids'\ndf_documents_agg[\"question_ids\"] = df_documents_agg[\"question_ids\"].apply(\n    lambda x: x if isinstance(x, list) else []\n)\n## Replace NaN values in 'num_questions' with 0\ndf_documents_agg[\"num_questions\"] = df_documents_agg[\"num_questions\"].fillna(0)\n```\nçŽ°åœ¨å°†é—®é¢˜çš„æ•°æ®æ¡†ä¸Žæ–‡æ¡£çš„æ•°æ®æ¡†è¿žæŽ¥èµ·æ¥\n\n```python\ndf = pd.concat([df_qa_eval, df_documents_agg], axis=0)\n```\næ­¤å¤–ï¼Œè®©æˆ‘ä»¬å‡†å¤‡ä¸€äº›ä¸åŒçš„ UMAP \\[3] æ˜ å°„ã€‚æ‚¨å¯ä»¥ç¨åŽåœ¨ Spotlight GUI ä¸­åšç±»ä¼¼çš„äº‹æƒ…ï¼Œä½†æå‰åšå¥½å¯ä»¥èŠ‚çœæ—¶é—´ã€‚\n\n* umap\\_all: å¯¹æ‰€æœ‰æ–‡æ¡£å’Œé—®é¢˜åµŒå…¥åº”ç”¨ fit å’Œ transform çš„ UMAP\n* umap\\_questions: ä»…å¯¹é—®é¢˜åµŒå…¥åº”ç”¨ fitï¼Œå¹¶å¯¹ä¸¤è€…åº”ç”¨ transform çš„ UMAP\n* umap\\_docs: ä»…å¯¹æ–‡æ¡£åµŒå…¥åº”ç”¨ fitï¼Œå¹¶å¯¹ä¸¤è€…åº”ç”¨ transform çš„ UMAP\n\næˆ‘ä»¬åƒè¿™æ ·å‡†å¤‡æ¯ä¸ª UMAP è½¬æ¢ï¼š\n\n```python\numap = UMAP(n_neighbors=20, min_dist=0.15, metric=\"cosine\", random_state=42).fit\numap_all = umap.transform(df[\"embedding\"].values.tolist())\ndf[\"umap\"] = umap_all.tolist()\n\n```\næ¯ä¸ªæ–‡æ¡£ç‰‡æ®µçš„å¦ä¸€ä¸ªæœ‰è¶£æŒ‡æ ‡æ˜¯å…¶åµŒå…¥ä¸Žæœ€è¿‘é—®é¢˜çš„åµŒå…¥ä¹‹é—´çš„è·ç¦»ï¼š\n\n```python\nquestion_embeddings = np.array(df[df[\"question\"].notna()][\"embedding\"].tolist())\ndf[\"nearest_question_dist\"] = [  # brute force, could be optimized using ChromaDB\n    np.min([np.linalg.norm(np.array(doc_emb) - question_embeddings)])\n    for doc_emb in df[\"embedding\"].values\n]\n```\nè¿™ä¸ªæŒ‡æ ‡å¯ä»¥å¸®åŠ©æ‰¾åˆ°æœªè¢«é—®é¢˜å¼•ç”¨çš„æ–‡æ¡£ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*YTRUXZmd0iX8kyPIdUUnlg.png)\n\n## å¯è§†åŒ–ç»“æžœ\n\nå¦‚æžœæ‚¨è·³è¿‡äº†ä¹‹å‰çš„æ­¥éª¤ï¼Œæ‚¨å¯ä»¥ä¸‹è½½æ•°æ®æ¡†å¹¶ä½¿ç”¨ä»¥ä¸‹ä»£ç åŠ è½½å®ƒï¼š\n\n```python\nimport pandas as pd\ndf = pd.read_parquet(\"df_f1_rag_docs_and_questions.parquet\")\n```\nç„¶åŽå¯åŠ¨ [Renumics Spotlight](https://github.com/Renumics/spotlight) ä»¥å¯è§†åŒ–å®ƒï¼š\n\n```python\nfrom renumics import spotlight\n\nspotlight.show(df)\nspotlight.show(\n    df,\n    layout=\"/home/markus/Downloads/layout_rag_1.json\",\n    dtype={x: Embedding for x in df.keys() if \"umap\" in x},\n)\n```\nè¿™å°†æ‰“å¼€ä¸€ä¸ªæ–°çš„æµè§ˆå™¨çª—å£ï¼š\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*IMbva0pP8RAVhoY4dVbjLg.png)\n\nåœ¨å·¦ä¸Šè§’ï¼Œæ‚¨å¯ä»¥çœ‹åˆ°ä¸€ä¸ª**æ‰€æœ‰é—®é¢˜å’Œæ‰€æœ‰æ–‡æ¡£**ç‰‡æ®µçš„è¡¨æ ¼ã€‚æ‚¨å¯ä»¥ä½¿ç”¨â€œå¯è§åˆ—â€æŒ‰é’®æ¥æŽ§åˆ¶è¡¨æ ¼ä¸­æ˜¾ç¤ºå“ªäº›æ•°æ®æ¡†åˆ—ã€‚ç›´æŽ¥åˆ›å»ºä¸€ä¸ªé€‰æ‹©ä»…é—®é¢˜çš„è¿‡æ»¤å™¨æ˜¯å¾ˆæœ‰ç”¨çš„ï¼Œä»¥ä¾¿èƒ½å¤Ÿåœ¨å¯è§†åŒ–ä¸­æ‰“å¼€å’Œå…³é—­é—®é¢˜ï¼šé€‰æ‹©æ‰€æœ‰é—®é¢˜ï¼Œç„¶åŽä½¿ç”¨â€œä»Žé€‰å®šè¡Œåˆ›å»ºè¿‡æ»¤å™¨â€æŒ‰é’®åˆ›å»ºè¿‡æ»¤å™¨ã€‚\n\nåœ¨è¡¨æ ¼çš„å³ä¾§ï¼Œ`answer correctness` **ä½œä¸ºä¸€ä¸ªæŒ‡æ ‡**æ˜¾ç¤ºåœ¨æ‰€æœ‰é—®é¢˜ä¸­ã€‚ä¸‹é¢æœ‰ä¸¤ä¸ª**ç›´æ–¹å›¾**ï¼›å·¦ä¾§æ˜¾ç¤ºäº†æ ¹æ®ä¸åŒé—®é¢˜ç”Ÿæˆæ–¹æ³•åˆ’åˆ†çš„`answer correctness`çš„åˆ†å¸ƒã€‚å³ä¾§æ˜¾ç¤ºäº†é—®é¢˜ç”Ÿæˆæ–¹æ³•çš„åˆ†å¸ƒã€‚åœ¨è¿™é‡Œï¼Œå¦‚æžœéœ€è¦ï¼Œå»ºè®®ä½¿ç”¨è¿‡æ»¤æŒ‰é’®ä¸ºé—®é¢˜åˆ›å»ºè¿‡æ»¤å™¨ï¼Œä»¥ä»…æ˜¾ç¤ºé€‰å®šçš„è¡Œï¼ˆé—®é¢˜ï¼‰ã€‚\n\nå³ä¾§æœ‰**ä¸¤ä¸ªç›¸ä¼¼æ€§å›¾**ã€‚ç¬¬ä¸€ä¸ªä½¿ç”¨`umap_questions`åˆ—ï¼ŒåŸºäºŽä»…å¯¹é—®é¢˜åº”ç”¨çš„è½¬æ¢æ˜¾ç¤ºé—®é¢˜å’Œæ–‡æ¡£ã€‚è¿™å¯¹äºŽç‹¬ç«‹äºŽç›¸å…³æ–‡æ¡£æŸ¥çœ‹é—®é¢˜çš„åˆ†å¸ƒå¾ˆæœ‰å¸®åŠ©ï¼Œå› ä¸ºè¿™ç§æ–¹æ³•å…è®¸åˆ†æžå¸ˆè¯†åˆ«é—®é¢˜æœ¬èº«çš„æ¨¡å¼æˆ–ç°‡ã€‚\n\nç¬¬äºŒä¸ªç›¸ä¼¼æ€§å›¾åŸºäºŽä»…å¯¹æ–‡æ¡£åº”ç”¨çš„è½¬æ¢ï¼ˆ`umap_docs`ï¼‰æ˜¾ç¤ºé—®é¢˜å’Œæ–‡æ¡£ã€‚å®ƒå¯¹äºŽåœ¨å…¶ç›¸å…³æ–‡æ¡£çš„ä¸Šä¸‹æ–‡ä¸­æŸ¥çœ‹é—®é¢˜å¾ˆæœ‰ç”¨ã€‚ä¸€ä¸ªåŒæ—¶å¯¹é—®é¢˜å’Œæ–‡æ¡£è¿›è¡Œè½¬æ¢çš„ç›¸ä¼¼æ€§å›¾åœ¨é—®é¢˜æ•°é‡è¾ƒå¤šæ—¶è¢«è¯æ˜Žä¸å¤ªæœ‰ç”¨ï¼Œå› ä¸ºæ›´å¤šæˆ–æ›´å°‘çš„é—®é¢˜ä¼šèšé›†åœ¨ä¸€èµ·å¹¶å€¾å‘äºŽä¸Žæ–‡æ¡£åˆ†å¼€ã€‚å› æ­¤ï¼Œè¿™ç§è¡¨ç¤ºåœ¨è¿™é‡Œè¢«çœç•¥ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*1wZrAj60hiw1T3RVnCuBtA.png)\n\n### æ–‡æ¡£åµŒå…¥ç›¸ä¼¼æ€§å›¾ï¼šè§‚å¯Ÿ\n\nåœ¨ç›¸ä¼¼æ€§å›¾ `umap_docs` ä¸­ï¼Œæ‚¨å¯ä»¥è¯†åˆ«å‡ºæ–‡æ¡£åµŒå…¥ç©ºé—´ä¸­æ²¡æœ‰é‚»è¿‘é—®é¢˜çš„åŒºåŸŸã€‚å½“é€‰æ‹© `nearest_question_dist` è¿›è¡Œç€è‰²æ—¶ï¼Œè¿™ä¸€ç‚¹æ›´åŠ æ˜Žæ˜¾ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*cMGNPnnBa9Bn7BJ05SzxBw.png)\n\nå¯ä»¥è¯†åˆ«å‡ºä¸€äº›ç°‡ï¼ŒåŒ…æ‹¬ä»…åŒ…å«æ ‡é¢˜æˆ–é€é¡µåŒ…å«ä»…æ•°å­—çš„è¡¨æ ¼æ•°æ®çš„ç‰‡æ®µï¼Œè¿™äº›åœ¨æ‹†åˆ†è¿‡ç¨‹ä¸­å…¶æ„ä¹‰ä¸§å¤±ã€‚æ­¤å¤–ï¼Œè®¸å¤šä¸åŒ…å«ç›¸å…³ä¿¡æ¯çš„ç»´åŸºç™¾ç§‘ç‰¹å®šæ–‡æœ¬æ·»åŠ ï¼Œä¾‹å¦‚æŒ‡å‘å…¶ä»–è¯­è¨€çš„é“¾æŽ¥æˆ–ç¼–è¾‘æ³¨é‡Šï¼Œå½¢æˆäº†æ²¡æœ‰é‚»è¿‘é—®é¢˜çš„ç°‡ã€‚\n\nä½¿ç”¨ç»´åŸºç™¾ç§‘ API åˆ é™¤ç»´åŸºç™¾ç§‘ç›¸å…³æ–‡æœ¬å½¢å¼çš„å™ªå£°éžå¸¸ç®€å•ã€‚è¿™å¯èƒ½å¹¶ä¸æ˜¯ç‰¹åˆ«å¿…è¦ï¼Œå› ä¸ºå®ƒä¸»è¦å ç”¨ä¸€äº›ç©ºé—´â€”â€”é¢„è®¡ RAG ç»“æžœä¸ä¼šå› æ­¤ç‰¹åˆ«æ¶åŒ–ã€‚ç„¶è€Œï¼ŒåŒ…å«åœ¨å¤§è¡¨æ ¼ä¸­çš„æ•°æ®å¾ˆéš¾è¢« RAG ç³»ç»Ÿæ•èŽ·ï¼Œä½¿ç”¨å…ˆè¿›çš„é¢„å¤„ç†æ–¹æ³•è¿›è¡Œè¡¨æ ¼æå–å¹¶å°†å…¶è¿žæŽ¥åˆ° RAG ç³»ç»Ÿå¯èƒ½æ˜¯æœ‰ç›Šçš„ã€‚\n\næ‚¨å¯ä»¥åœ¨ `umap_docs` ç›¸ä¼¼æ€§å›¾ä¸­è§‚å¯Ÿåˆ°çš„å¦ä¸€ä¸ªç‚¹æ˜¯æ¥è‡ªä¸åŒæ¥æºçš„é—®é¢˜çš„åˆ†å¸ƒã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*IH7z3J4yUmU0C_SruxnDkg.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*K4bADgDmSAr5t4t4r9VImQ.png)\n\nç”± ChatGPTï¼ˆGPT-3.5ã€GPT-4ï¼‰ç›´æŽ¥ç”Ÿæˆçš„é—®é¢˜ä½äºŽä¸­å¿ƒçš„ä¸€ä¸ªæ›´ä¸ºå°é—­çš„åŒºåŸŸï¼Œè€ŒåŸºäºŽæ–‡æ¡£ç”Ÿæˆçš„ ragas ç”Ÿæˆçš„é—®é¢˜è¦†ç›–äº†æ›´å¤§çš„åŒºåŸŸã€‚\n\n### ç­”æ¡ˆæ­£ç¡®æ€§ç›´æ–¹å›¾\n\nç›´æ–¹å›¾å¯ä»¥ä½œä¸ºäº†è§£æ•°æ®å…¨çƒç»Ÿè®¡çš„èµ·ç‚¹ã€‚æ€»ä½“è€Œè¨€ï¼Œåœ¨æ‰€æœ‰é—®é¢˜ä¸­ï¼Œ`ç­”æ¡ˆæ­£ç¡®æ€§`ä¸º0\\.45ã€‚å¯¹äºŽæ²¡æœ‰ä½¿ç”¨ragasåˆ›å»ºçš„é—®é¢˜ï¼Œè¯¥å€¼ä¸º0\\.36ï¼Œè€Œä½¿ç”¨ragasçš„é—®é¢˜åˆ™ä¸º0\\.52ã€‚é¢„è®¡ç³»ç»Ÿåœ¨ç”Ÿæˆä½¿ç”¨ragasçš„é—®é¢˜æ—¶è¡¨çŽ°ä¼šæ›´å¥½ï¼Œå› ä¸ºè¿™äº›é—®é¢˜æ˜¯åŸºäºŽå¯ç”¨æ•°æ®ç”Ÿæˆçš„ï¼Œè€ŒChatGPTç›´æŽ¥ç”Ÿæˆçš„é—®é¢˜å¯èƒ½æ¥è‡ªäºŽChatGPTè®­ç»ƒæ—¶ä½¿ç”¨çš„æ‰€æœ‰æ•°æ®ã€‚\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GsLBsg7uwTrw-AzvO4BHmw.png)\n\nå¯¹ä¸€äº›é—®é¢˜/ç­”æ¡ˆå’ŒçœŸå®žæƒ…å†µè¿›è¡Œå¿«é€Ÿéšæœºæ‰‹åŠ¨å®¡æ ¸æ˜¾ç¤ºï¼Œåœ¨`ç­”æ¡ˆæ­£ç¡®æ€§`ä¸º0\\.3â€“0\\.4çš„åŒºé—´ï¼Œå¤§å¤šæ•°é—®é¢˜ä»ç„¶æ ¹æ®çœŸå®žæƒ…å†µå¾—åˆ°äº†æ­£ç¡®å›žç­”ã€‚åœ¨0\\.2â€“0\\.3çš„åŒºé—´ï¼Œå­˜åœ¨è®¸å¤šé”™è¯¯ç­”æ¡ˆã€‚åœ¨0\\.1â€“0\\.2çš„åŒºé—´ï¼Œå¤§å¤šæ•°ç­”æ¡ˆéƒ½æ˜¯é”™è¯¯çš„ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™ä¸ªèŒƒå›´å†…å‡ ä¹Žæ‰€æœ‰çš„é—®é¢˜éƒ½æ¥è‡ªGPT\\-3\\.5ã€‚å°½ç®¡åœ¨è¿™ä¸ªåŒºé—´å†…ç”Ÿæˆçš„ä¸¤ä¸ªé—®é¢˜ä½¿ç”¨çš„æ˜¯GPT\\-4ï¼Œä½†å®ƒä»¬ä»ç„¶å¾—åˆ°äº†æ­£ç¡®çš„å›žç­”ï¼Œå°½ç®¡å…¶`ç­”æ¡ˆæ­£ç¡®æ€§`ä½ŽäºŽ0\\.2ã€‚\n\n### é—®é¢˜åµŒå…¥ç›¸ä¼¼æ€§å›¾ï¼šè§‚å¯Ÿ\n\né—®é¢˜åµŒå…¥ç›¸ä¼¼æ€§å›¾å¯ä»¥é€šè¿‡æ£€æŸ¥å¯èƒ½å¯¼è‡´ç±»ä¼¼é—®é¢˜çš„ç›¸ä¼¼é—®é¢˜é›†ç¾¤ï¼Œå¸®åŠ©æ·±å…¥æŒ–æŽ˜ `ç­”æ¡ˆæ­£ç¡®æ€§`ã€‚\n\n* **é›†ç¾¤â€œé©±åŠ¨ç¨‹åº/è¿‡ç¨‹/æ±½è½¦çš„æœ¯è¯­â€ï¼š** å¹³å‡ `ç­”æ¡ˆæ­£ç¡®æ€§` 0\\.23ï¼šç­”æ¡ˆé€šå¸¸ä¸å¤Ÿç²¾ç¡®ã€‚ä¾‹å¦‚ï¼Œåº•ç›˜è°ƒæ ¡ä¸Žåº•ç›˜å¼¯æ›²æˆ–åˆ¹è½¦è°ƒæ ¡ä¸Žåˆ¹è½¦åå·®è°ƒæ•´ã€‚æ˜¯å¦é€‚åˆç”¨è¿™äº›ç±»åž‹çš„é—®é¢˜æ¥è¯„ä¼°ç³»ç»Ÿæ˜¯å€¼å¾—æ€€ç–‘çš„ï¼Œå› ä¸ºåˆ¤æ–­ç­”æ¡ˆä¼¼ä¹Žéžå¸¸å›°éš¾ã€‚\n* **é›†ç¾¤â€œç‡ƒæ–™ç­–ç•¥çš„æœ¯è¯­â€ï¼š** å¹³å‡ `ç­”æ¡ˆæ­£ç¡®æ€§` 0\\.44ï¼Œç±»ä¼¼äºŽå…¨çƒ `ç­”æ¡ˆæ­£ç¡®æ€§`ã€‚\n* **é›†ç¾¤â€œèµ›é“åç§°â€ï¼š** å¹³å‡ `ç­”æ¡ˆæ­£ç¡®æ€§` 0\\.49ï¼Œç±»ä¼¼äºŽå…¨çƒ `ç­”æ¡ˆæ­£ç¡®æ€§`ã€‚\n* **é›†ç¾¤â€œè°ä¿æŒäº†â€¦çš„è®°å½•â€ï¼š** å¹³å‡ `ç­”æ¡ˆæ­£ç¡®æ€§` 0\\.44ï¼Œç±»ä¼¼äºŽå…¨çƒ `ç­”æ¡ˆæ­£ç¡®æ€§`ã€‚\n* **é›†ç¾¤â€œèµ¢å¾—â€¦é”¦æ ‡èµ›â€ï¼š** å¹³å‡ `ç­”æ¡ˆæ­£ç¡®æ€§` 0\\.26 â€” çœ‹èµ·æ¥å¾ˆå…·æŒ‘æˆ˜æ€§ã€‚å¸¦æœ‰è®¸å¤šæ¡ä»¶çš„é—®é¢˜ï¼Œä¾‹å¦‚ï¼šâ€œè°æ˜¯å”¯ä¸€ä¸€ä½å‡­å€Ÿè‹±å›½èµ›è½¦æ‰§ç…§ã€ä¸ºæ„å¤§åˆ©è½¦é˜Ÿé©¾é©¶ç¾Žå›½å¼•æ“Žèµ¢å¾—ä¸€çº§æ–¹ç¨‹å¼ä¸–ç•Œé”¦æ ‡èµ›çš„è½¦æ‰‹ã€‚â€ æ‰©å±•çš„RAGæ–¹æ³•å¦‚å¤šæŸ¥è¯¢å¯èƒ½æœ‰åŠ©äºŽæ”¹å–„è¿™ä¸€ç‚¹ã€‚\n* **é›†ç¾¤â€œè°æ˜¯å”¯ä¸€ä¸€ä½èµ¢å¾—â€¦çš„è½¦æ‰‹ï¼Œé©¾é©¶ç¼–å·ä¸º\\<number\\>çš„æ±½è½¦â€ï¼š** å¹³å‡ `ç­”æ¡ˆæ­£ç¡®æ€§` 0\\.23 â€” çœ‹èµ·æ¥GPT-3\\.5åœ¨è¿™é‡Œæ‡’æƒ°ï¼Œé‡å¤äº†ç›¸åŒçš„é—®é¢˜ï¼Œåªæ˜¯æ¢äº†ä¸åŒçš„æ•°å­—ï¼Œå°½ç®¡å¤§å¤šæ•°çœŸå®žç­”æ¡ˆéƒ½æ˜¯é”™è¯¯çš„ï¼\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Yc03cpSEFlJoZSBPIpMkiQ.png)\n\n## ç»“è®º\n\næ€»ä¹‹ï¼Œåˆ©ç”¨åŸºäºŽ UMAP çš„å¯è§†åŒ–æä¾›äº†ä¸€ç§æœ‰è¶£çš„æ–¹æ³•ï¼Œå¯ä»¥æ·±å…¥åˆ†æžå…¨çƒæŒ‡æ ‡ä¹‹å¤–çš„å†…å®¹ã€‚æ–‡æ¡£åµŒå…¥ç›¸ä¼¼æ€§åœ°å›¾æä¾›äº†ä¸€ä¸ªè‰¯å¥½çš„æ¦‚è¿°ï¼Œå±•ç¤ºäº†ç›¸ä¼¼æ–‡æ¡£çš„èšç±»åŠå…¶ä¸Žè¯„ä¼°é—®é¢˜çš„å…³ç³»ã€‚é—®é¢˜ç›¸ä¼¼æ€§åœ°å›¾æ­ç¤ºäº†æ¨¡å¼ï¼Œä½¿å¾—å¯ä»¥ç»“åˆè´¨é‡æŒ‡æ ‡å¯¹é—®é¢˜è¿›è¡ŒåŒºåˆ†å’Œåˆ†æžï¼Œä»Žè€Œç”Ÿæˆæ´žå¯Ÿã€‚è¯·å‚é˜…å¯è§†åŒ–ç»“æžœéƒ¨åˆ†ï¼Œå°†å¯è§†åŒ–åº”ç”¨äºŽæ‚¨çš„è¯„ä¼°ç­–ç•¥â€”â€”æ‚¨å°†å‘çŽ°ä»€ä¹ˆæ´žå¯Ÿï¼Ÿ\n\n*I am a professional with expertise in creating advanced software solutions for the interactive exploration of unstructured data. I write about unstructured data and use powerful visualization tools to analyze and make informed decisions.*\n\n## å‚è€ƒæ–‡çŒ®\n\n\\[1] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, Haofen Wang: [Retrieval\\-Augmented Generation for Large Language Models: A Survey](https://arxiv.org/abs/2312.10997) (2024\\), arxiv\n\n\\[2] Yixuan Tang, Yi Yang: [MultiHop\\-RAG: Benchmarking Retrieval\\-Augmented Generation for Multi\\-Hop Queries](https://arxiv.org/abs/2401.15391) (2021\\), arXiv\n\n\\[3] Leland McInnes, John Healy, James Melville: [UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction](https://arxiv.org/abs/1802.03426) (2018\\), arXiv\n\n\\[4] Shahul Es, Jithin James, Luis Espinosa\\-Anke, Steven Schockaert: [RAGAS: Automated Evaluation of Retrieval Augmented Generation](https://arxiv.org/abs/2309.15217) (2023\\), arXiv\n\n"},{"lang":"zh","group":"blog","slug":"blog/whats-new-with-claude-sonnet-3-5-claude-3-5-haiku-c1f62a2d2c72","frontmatter":{"title":"Claude Sonnet 3.5 å’Œ Claude 3.5 Haiku æœ‰å“ªäº›æ–°åŠŸèƒ½ï¼Ÿ","meta_title":"Claude Sonnet 3.5 å’Œ Claude 3.5 Haiku æœ‰å“ªäº›æ–°åŠŸèƒ½ï¼Ÿ","description":"å€¼å¾—ä¸€è¯•å—ï¼Ÿ","date":"2024-11-13T01:22:29.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*CEMTDlHlMUX66-eoMcSOzg.png","categories":["Natural Language Processing","Programming","Technology/Web"],"author":"Rifx.Online","tags":["language","models","interaction","automation","latency"],"draft":false,"slug":"blog/whats-new-with-claude-sonnet-3-5-claude-3-5-haiku-c1f62a2d2c72"},"content":"\n\n\n\n\n### é¦–å…ˆï¼ŒClaudeæ˜¯ä»€ä¹ˆï¼Ÿ\n\nClaudeæ˜¯ç”±[Anthropic](https://proxy.rifx.online/https://www.anthropic.com/)åˆ›å»ºçš„è¯­è¨€æ¨¡åž‹ï¼Œæ—¨åœ¨å¸®åŠ©å®Œæˆè¯¸å¦‚å›žç­”é—®é¢˜ã€æ€»ç»“ä¿¡æ¯å’Œç”Ÿæˆæ–‡æœ¬ç­‰ä»»åŠ¡â€”â€”ç±»ä¼¼äºŽChatGPTã€‚Claudeçš„ä¸€ä¸ªä¼˜ç‚¹æ˜¯å®ƒè¢«è®¾è®¡å¾—æ›´å®‰å…¨ï¼Œæ›´ç¬¦åˆäººç±»æ„å›¾ï¼Œå› æ­¤ç”Ÿæˆæœ‰å®³æˆ–è¯¯å¯¼æ€§å†…å®¹çš„å¯èƒ½æ€§è¾ƒå°ã€‚\n\n### ç­‰ç­‰â€¦â€¦Claude 3\\.5 Sonnet ä¸æ˜¯å·²ç»å‘å¸ƒäº†å—ï¼Ÿ\n\nå“ˆå“ˆï¼Œæ˜¯çš„ï¼Œè™½ç„¶åç§°æ²¡æœ‰å˜åŒ–ï¼Œä½†è¿™ä¸ªäºŽ2024å¹´10æœˆ22æ—¥å‘å¸ƒçš„æ–°ç‰ˆæœ¬Claude 3\\.5 Sonnetå’ŒClaude Haikuæœ‰å¾ˆå¤šä»¤äººå…´å¥‹çš„æ›´æ–°ã€‚\n\nè¿™äº›æ–°æ¨¡åž‹åœ¨è°ƒè¯•ä»£ç å’Œä»Žå›¾åƒä¸­è½¬å½•æ–‡æœ¬ç­‰ä»»åŠ¡ä¸Šæ›´å¿«ã€æ›´å¥½ï¼Œè¿™ä½¿å®ƒä»¬åœ¨é›¶å”®å’Œç‰©æµç­‰è¡Œä¸šä¸­ç‰¹åˆ«æœ‰ç”¨ã€‚\n\n*å¦‚æžœæ‚¨å¸Œæœ›è®¨è®ºè¿™äº›æ–°å˜åŒ–å¹¶æŸ¥çœ‹å¦‚ä½•å°†å…¶å®žæ–½åˆ°æ‚¨çš„é¡¹ç›®ä¸­ï¼Œè¯·[ç‚¹å‡»è¿™é‡Œä¸Žæˆ‘ä»¬å®‰æŽ’ä¸€æ¬¡å…è´¹çš„ç”µè¯ä¼šè®®](https://proxy.rifx.online/https://calendly.com/woyera-ai/)!*\n\n### è®©æˆ‘ä»¬æ·±å…¥äº†è§£ä¸€ä¸‹æœ‰ä»€ä¹ˆæ–°ä¸œè¥¿ï¼\n\n## äººæœºäº¤äº’\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*p1anQynliN8ihnT8X2VYqw.gif)\n\nClaude 3.5 Sonnet æœ€å¤§çš„æ›´æ–°ä¹‹ä¸€æ˜¯å®ƒèƒ½å¤Ÿä»¥æ›´ç±»äººæ–¹å¼ä¸Žè®¡ç®—æœºäº’åŠ¨ã€‚\n\nå®ƒçŽ°åœ¨å¯ä»¥å¯¼èˆªå±å¹•ã€ç‚¹å‡»æŒ‰é’®å’Œè¾“å…¥ï¼Œè¿™ä¸ºè‡ªåŠ¨åŒ–ä»»åŠ¡ç”šè‡³å®žæ—¶å·¥ä½œæµç¨‹æä¾›äº†ä¸€äº›éžå¸¸æœ‰è¶£çš„å¯èƒ½æ€§ã€‚\n\nè¯·æ³¨æ„ï¼Œè¿™ä¸ªåŠŸèƒ½ä»åœ¨å…¬å¼€æµ‹è¯•é˜¶æ®µï¼Œä½†å®ƒå·²ç»æ˜¾ç¤ºå‡ºå¾ˆå¤§çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æœºå™¨äººæµç¨‹è‡ªåŠ¨åŒ–ï¼ˆRPAï¼‰æ–¹é¢ã€‚\n\n## å¢žå¼ºçš„ç¼–ç æ”¯æŒ\n\nClaudeå¯ä»¥åœ¨æ•´ä¸ªè½¯ä»¶å¼€å‘è¿‡ç¨‹ä¸­æä¾›å¸®åŠ©ï¼Œä»Žè®¾è®¡ã€è°ƒè¯•åˆ°ä¼˜åŒ–ä»£ç â€”â€”å¯¹äºŽä»»ä½•æŠ€æœ¯äººå‘˜æ¥è¯´ï¼Œå®ƒéƒ½æ˜¯ä¸€ä¸ªå®è´µçš„èµ„äº§ã€‚\n\n## æ”¹è¿›çš„èŠå¤©æœºå™¨äºº\n\nClaude çš„è‡ªç„¶è¯­è°ƒå’Œé«˜çº§æŽ¨ç†èƒ½åŠ›ä½¿å…¶éžå¸¸é€‚åˆæž„å»ºæ›´å…·å“åº”æ€§å’Œäº’åŠ¨æ€§çš„èŠå¤©æœºå™¨äººã€‚\n\nå®ƒèƒ½å¤Ÿå¤„ç†å¤æ‚çš„å¯¹è¯ï¼Œç”šè‡³å¯ä»¥ä¸Žå„ç§ç³»ç»Ÿè¿žæŽ¥ä»¥ç®€åŒ–ä»»åŠ¡ï¼Œè¿™ä½¿å…¶éžå¸¸é€‚åˆå®¢æˆ·æœåŠ¡ã€æŠ€æœ¯æ”¯æŒç­‰ã€‚\n\n## è§†è§‰æ•°æ®æå–\n\nä¸€ä¸ªæ˜¾è‘—çš„ç‰¹ç‚¹æ˜¯Claudeåˆ†æžè§†è§‰æ•°æ®çš„èƒ½åŠ›ã€‚å®ƒå¯ä»¥è½»æ¾åœ°è§£é‡Šå’Œæå–å›¾è¡¨ã€å›¾å½¢å’Œå›¾è§£ä¸­çš„ä¿¡æ¯ã€‚\n\n## çŸ¥è¯†é—®ç­”\n\nClaude 3\\.5 Sonnet ä¹Ÿéžå¸¸é€‚åˆä½¿ç”¨å¤§åž‹æ•°æ®é›†ã€çŸ¥è¯†åº“æˆ–ä»£ç åº“æ¥å›žç­”è¯¦ç»†é—®é¢˜ã€‚å‡­å€Ÿæ›´å¤§çš„ä¸Šä¸‹æ–‡çª—å£ï¼Œå®ƒæ˜¯éœ€è¦å¿«é€Ÿã€å‡†ç¡®ä¿¡æ¯çš„ä¼ä¸šçš„å¯é é€‰æ‹©ã€‚\n\n## å¯ç”¨æ€§å’Œå®šä»·\n\nClaude 3\\.5 å¯é€šè¿‡ Anthropic APIã€Amazon Bedrock å’Œ Google Cloud çš„ Vertex AI ä½œä¸º API ä½¿ç”¨ã€‚å¯¹äºŽ APIï¼Œå®šä»·ä»Ž **æ¯ç™¾ä¸‡è¾“å…¥ä»¤ç‰Œ $3** å’Œ **æ¯ç™¾ä¸‡è¾“å‡ºä»¤ç‰Œ $15** å¼€å§‹ã€‚\n\næˆ–è€…ï¼Œæ‚¨å¯ä»¥é€šè¿‡ [claude.ai](https://proxy.rifx.online/https://claude.ai/login?returnTo=%2F%3F) åœ¨ç½‘ä¸Šç®€å•ä½¿ç”¨ã€‚æ‚¨å¯ä»¥å…è´¹åˆ›å»ºä¸€ä¸ªå¸æˆ·ï¼Œç„¶åŽ Pro è®¡åˆ’ä¸ºæ¯æœˆ $20ï¼ŒTeam è®¡åˆ’ä¸ºæ¯æœˆ $25ï¼Œæˆ– [Enterprise.](https://proxy.rifx.online/https://www.anthropic.com/pricing)\n\n## å®‰å…¨æ€§ä¸Žä¿¡ä»»\n\nAnthropic åœ¨ç¡®ä¿ Claude 3\\.5 Sonnet çš„å®‰å…¨æ€§æ–¹é¢æŠ•å…¥äº†å¤§é‡ç²¾åŠ›ã€‚è¯¥æ¨¡åž‹ç»è¿‡å¹¿æ³›æµ‹è¯•ï¼Œä»¥ç¡®ä¿å…¶èƒ½å¤Ÿè´Ÿè´£ä»»åœ°å¤„ç†æ•æ„Ÿå†…å®¹ï¼Œè€Œä¸å½±å“æ€§èƒ½ã€‚\n\nè¿™ç§å¯¹å®‰å…¨æ€§çš„å…³æ³¨æœ‰åŠ©äºŽé˜²èŒƒä¸å½“å†…å®¹ç­‰é—®é¢˜ï¼Œå¹¶ç¡®ä¿ Claude é€‚ç”¨äºŽå¹¿æ³›çš„åº”ç”¨åœºæ™¯ã€‚\n\n## ç”¨ä¾‹\n\næ— è®ºæ‚¨æ˜¯å¼€å‘è€…ã€ä¼ä¸šä¸»ï¼Œè¿˜æ˜¯å¯¹äººå·¥æ™ºèƒ½æ„Ÿåˆ°å¥½å¥‡ï¼ŒClaude 3.5 Sonnet éƒ½èƒ½æä¾›å¾ˆå¤šåŠŸèƒ½ã€‚\n\n### è‡ªåŠ¨åŒ–é‡å¤ä»»åŠ¡\n\nå®¢æˆ·æœåŠ¡å›¢é˜Ÿå¯ä»¥ä½¿ç”¨Claudeå¤„ç†é‡å¤çš„åŽå°ä»»åŠ¡ï¼Œå¦‚æ›´æ–°å®¢æˆ·è®¢å•æˆ–å¤„ç†é€€æ¬¾ã€‚Claudeèƒ½å¤Ÿå¯¼èˆªå±å¹•å’Œç‚¹å‡»æŒ‰é’®ï¼Œå¯ä»¥èŠ‚çœæ•°å°æ—¶çš„æ‰‹åŠ¨å·¥ä½œã€‚\n\n### èŠå¤©æœºå™¨äºº\n\nåŒ»ç–—æœåŠ¡æä¾›è€…å¯ä»¥ä½¿ç”¨Claudeæž„å»ºèŠå¤©æœºå™¨äººï¼Œä»¥å¤„ç†æ‚£è€…äº’åŠ¨ï¼Œä¾‹å¦‚é¢„çº¦ã€å›žç­”åŒ»ç–—å¸¸è§é—®é¢˜æˆ–æŒ‡å¯¼æ‚£è€…è¿›è¡Œç—‡çŠ¶æ£€æŸ¥ã€‚æ‰€æœ‰è¿™äº›éƒ½ä¿æŒè‡ªç„¶å’Œå¯¹è¯çš„è¯­æ°”ã€‚\n\n### è§†è§‰æ•°æ®\n\né‡‘èžåˆ†æžå¸ˆå¯ä»¥ä½¿ç”¨Claudeåˆ†æžåŒ…å«å¤§é‡å›¾è¡¨å’Œå›¾å½¢çš„å­£åº¦è´¢æŠ¥ã€‚Claudeå¯ä»¥å¿«é€Ÿæå–æ´žå¯Ÿå¹¶æ€»ç»“å…³é”®è¶‹åŠ¿ã€‚\n\n### çŸ¥è¯†é—®ç­”\n\nä¸€å®¶ç§‘æŠ€å…¬å¸å¯ä»¥ä½¿ç”¨Claudeæ¥ç®¡ç†å†…éƒ¨çŸ¥è¯†åº“ã€‚å¼€å‘äººå‘˜å¯ä»¥è¯¢é—®æœ‰å…³çŽ°æœ‰ä»£ç åº“æˆ–æ•…éšœæŽ’é™¤æ­¥éª¤çš„è¯¦ç»†é—®é¢˜ï¼ŒClaudeå°†æä¾›å¿«é€Ÿä¸”å¯é çš„ç­”æ¡ˆã€‚\n\n## å…³äºŽ Claude 3\\.5 Haikuï¼Ÿ\n\nClaude 3\\.5 Haiku æ˜¯ Anthropic æœ€å¿«çš„ AI æ¨¡åž‹ï¼Œæä¾›æ›´å¥½çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¸æé«˜æˆæœ¬æˆ–é™ä½Žé€Ÿåº¦ã€‚å®ƒåœ¨ç¼–ç ç­‰ä»»åŠ¡ä¸Šæ›´å¼ºå¤§ï¼Œå¹¶åœ¨åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº† Claude 3 Opus å’Œ GPT\\-4o ç­‰æ¨¡åž‹ã€‚\n\nè¯¥æ¨¡åž‹è®¾è®¡å…·æœ‰ä½Žå»¶è¿Ÿï¼Œæ„å‘³ç€å®ƒå“åº”è¿…é€Ÿï¼Œéžå¸¸é€‚åˆå®žæ—¶åº”ç”¨ã€ä¸ªæ€§åŒ–ä»»åŠ¡ï¼ˆå¦‚åˆ†æžè´­ä¹°åŽ†å²ï¼‰ä»¥åŠå…¶ä»–æ•°æ®å¯†é›†åž‹é¡¹ç›®ã€‚å®ƒå°†åœ¨æœ¬æœˆæ™šäº›æ—¶å€™é€šè¿‡ Amazon Bedrock å’Œ Google Cloud ç­‰ API æä¾›ï¼Œèµ·åˆå°†ä½œä¸ºä»…æ–‡æœ¬æ¨¡åž‹ï¼Œå›¾åƒæ”¯æŒå°†å¾ˆå¿«æŽ¨å‡ºã€‚\n\n## é‚£ä¹ˆï¼ŒClaudeä¸ŽChatGPTç›¸æ¯”å¦‚ä½•ï¼Ÿ\n\nåœ¨æ¯”è¾ƒ**Claude 3\\.5**å’Œ**ChatGPT**æ—¶ï¼Œä¸¤è€…éƒ½æ˜¯å…ˆè¿›çš„AIæ¨¡åž‹ï¼Œæ—¨åœ¨å¤„ç†ç±»ä¼¼çš„ä»»åŠ¡ï¼Œå¦‚å›žç­”é—®é¢˜ã€ç”Ÿæˆæ–‡æœ¬å’ŒååŠ©ç¼–ç ï¼Œä½†å®ƒä»¬æœ‰æ˜Žæ˜¾çš„å·®å¼‚ï¼Œå¯èƒ½é€‚åˆä¸åŒçš„éœ€æ±‚ã€‚\n\nClaude 3\\.5å¼ºè°ƒ**å®‰å…¨æ€§**ï¼Œå‡å°‘æœ‰å®³æˆ–è¯¯å¯¼æ€§è¾“å‡ºçš„é£Žé™©ã€‚è™½ç„¶ChatGPTä¹Ÿé‡è§†å®‰å…¨æ€§ï¼Œä½†Claudeçš„è®¾è®¡åœ¨è¿™ä¸€é¢†åŸŸç»™äºˆäº†é¢å¤–å…³æ³¨ã€‚\n\nåœ¨**é€Ÿåº¦**æ–¹é¢ï¼ŒClaude 3\\.5 Haikuæä¾›æ›´å¿«çš„å“åº”æ—¶é—´ï¼Œä½¿å…¶éžå¸¸é€‚åˆå®žæ—¶åº”ç”¨ã€‚ChatGPTä¹Ÿå¾ˆå¿«ï¼Œä½†åœ¨æ›´å¤æ‚çš„ä»»åŠ¡ä¸Šå¯èƒ½ä¼šæœ‰è½»å¾®å»¶è¿Ÿã€‚\n\nåœ¨**ç¼–ç **æ–¹é¢ï¼Œä¸¤ç§æ¨¡åž‹è¡¨çŽ°è‰¯å¥½ã€‚Claude 3\\.5 Sonnetåœ¨æœ€è¿‘çš„åŸºå‡†æµ‹è¯•ä¸­åœ¨è°ƒè¯•å’Œä»£ç ç”Ÿæˆæ–¹é¢è¡¨çŽ°çªå‡ºï¼Œè€ŒChatGPTä»ç„¶æ˜¯ä¸€ä¸ªå¯é çš„ç¼–ç å¸®åŠ©å’Œè§£é‡Šé€‰é¡¹ã€‚\n\nä¸€ä¸ªå…³é”®çš„åŒºåˆ«æ˜¯**çŽ°å®žä¸–ç•Œäº¤äº’**ã€‚Claude 3\\.5èƒ½å¤Ÿå¯¼èˆªå±å¹•å’Œè‡ªåŠ¨åŒ–ä»»åŠ¡ï¼Œè€ŒChatGPTå°šæœªæä¾›æ­¤åŠŸèƒ½ã€‚\n\nåœ¨**å®šä»·**æ–¹é¢ï¼ŒChatGPTæœ‰ä¸€ä¸ªå¹¿æ³›å¯ç”¨çš„å…è´¹ç‰ˆæœ¬ï¼Œè€ŒClaudeåˆ™é€šè¿‡å…¶APIå’Œäº‘å¹³å°æä¾›çµæ´»çš„å®šä»·ã€‚\n\nClaudeå¯¹å®‰å…¨æ€§ã€ä½Žå»¶è¿Ÿå’Œå…ˆè¿›çŽ°å®žä¸–ç•Œäº¤äº’çš„å…³æ³¨ä½¿å…¶éžå¸¸é€‚åˆæ›´ä¸“ä¸šçš„åº”ç”¨ã€‚ä¸Žæ­¤åŒæ—¶ï¼ŒChatGPTçš„å¤šåŠŸèƒ½æ€§ã€å¹¿æ³›å¯ç”¨æ€§å’Œå¼ºå¤§çš„ç¼–ç æ”¯æŒä½¿å…¶æˆä¸ºä¸€ä¸ªå¾ˆå¥½çš„é€šç”¨å·¥å…·ã€‚\n\næœ€ç»ˆï¼Œåœ¨ä¸¤è€…ä¹‹é—´çš„é€‰æ‹©å–å†³äºŽæ‚¨æ‰€å¯»æ‰¾çš„å†…å®¹ï¼Œä½†è¿™ä¸¤ç§æ¨¡åž‹éƒ½æä¾›å¼ºå¤§çš„èƒ½åŠ›å’Œç‹¬ç‰¹çš„ä¼˜åŠ¿ã€‚\n\n## ç»“è®º\n\näº†è§£æ–°å·¥å…·çš„æœ€ä½³æ–¹å¼å°±æ˜¯äº²è‡ªå°è¯•ï¼æ‚¨å¯ä»¥ä½¿ç”¨ Claude 3\\.5 Sonnet æ¥å¢žå¼ºç¼–ç ã€èŠå¤©æœºå™¨äººã€æ•°æ®åˆ†æžç­‰å¤šç§åŠŸèƒ½ã€‚\n\nå‘Šè¯‰æˆ‘ä»¬æ‚¨å°†å¦‚ä½•ä½¿ç”¨ Claudeã€‚\n\n*å¦‚æžœæ‚¨éœ€è¦æž„å»ºè‡ªå®šä¹‰èŠå¤©æœºå™¨äººæˆ–åº”ç”¨ç¨‹åºï¼Œè¯·[ç‚¹å‡»è¿™é‡Œä¸Žæˆ‘ä»¬å¿«é€Ÿé€šè¯ã€‚](https://proxy.rifx.online/https://calendly.com/woyera-ai/)*\n\n"},{"lang":"zh","group":"blog","slug":"blog/why-embedding-matters-when-building-a-non-english-rag-system-multilingual-embeddings-1e3434ea6180","frontmatter":{"title":"åœ¨æž„å»ºéžè‹±è¯­ RAG ç³»ç»Ÿæ—¶ï¼ŒåµŒå…¥ä¸ºä»€ä¹ˆå¾ˆé‡è¦ - å¤šè¯­è¨€åµŒå…¥","meta_title":"åœ¨æž„å»ºéžè‹±è¯­ RAG ç³»ç»Ÿæ—¶ï¼ŒåµŒå…¥ä¸ºä»€ä¹ˆå¾ˆé‡è¦ - å¤šè¯­è¨€åµŒå…¥","description":"é€šè¿‡å¯¹è‹±è¯­ä¸Žè·å…°è¯­å¤šè¯­è¨€æ¨¡åž‹çš„è¯¦ç»†æ¯”è¾ƒï¼Œäº†è§£å¤šè¯­è¨€åµŒå…¥å¯¹ RAG ç³»ç»Ÿè‡³å…³é‡è¦çš„åŽŸå› ã€‚","date":"2024-11-13T01:22:29.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*QvODAYxqisUTrt4V.png","categories":["Natural Language Processing","Machine Learning","Multilingual"],"author":"Rifx.Online","tags":["embeddings","multilingual","RAG","Cohere","Dutch"],"draft":false,"slug":"blog/why-embedding-matters-when-building-a-non-english-rag-system-multilingual-embeddings-1e3434ea6180"},"content":"\n\n\n## ä¸ºä»€ä¹ˆåµŒå…¥æ˜¯å…³é”®\n\nåµŒå…¥æ˜¯çŽ°ä»£ç”Ÿæˆ AI çš„åŸºçŸ³ï¼Œé»˜é»˜æŽ¨åŠ¨ç€æˆ‘ä»¬æ¯å¤©äº’åŠ¨çš„è®¸å¤šç³»ç»Ÿçš„åŠŸèƒ½ã€‚ç®€å•æ¥è¯´ï¼ŒåµŒå…¥æ˜¯ **æ–‡æœ¬çš„æ•°å€¼è¡¨ç¤º** â€”â€” æœ‰æ•ˆåœ°å°†å•è¯ã€å¥å­ç”šè‡³æ•´ä¸ªæ–‡æ¡£è½¬æ¢ä¸ºæ•°å­—ã€‚è¿™äº›æ•°å­—è¿œéžéšæœºï¼›å®ƒä»¬ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥æ•æ‰æ–‡æœ¬ä¸­çš„å«ä¹‰å’Œå…³ç³»ã€‚ä¾‹å¦‚ï¼Œâ€œdogâ€å’Œâ€œpuppyâ€çš„åµŒå…¥åœ¨æ•°å€¼ç©ºé—´ä¸­ä¼šæ›´é è¿‘ï¼Œè€Œâ€œcarâ€çš„åµŒå…¥åˆ™ä¼šç›¸å¯¹è¾ƒè¿œï¼Œåæ˜ å‡ºå®ƒä»¬çš„ **è¯­ä¹‰ç›¸ä¼¼æ€§**ã€‚å°†æ„ä¹‰ç¼–ç ä¸ºå¯æµ‹é‡çš„å½¢å¼çš„èƒ½åŠ›ï¼Œä½¿å¾—åµŒå…¥åœ¨æœç´¢ã€æŽ¨èç³»ç»Ÿä»¥åŠ **æ£€ç´¢å¢žå¼ºç”Ÿæˆ (RAG)** ç­‰é«˜çº§ AI åº”ç”¨ä¸­ä¸å¯æˆ–ç¼ºã€‚\n\n\n\nè¿™ç§æ•°å­—åŒ–è½¬åŒ–ä½¿ AI èƒ½å¤Ÿä»¥æœ‰æ„ä¹‰çš„æ–¹å¼æ¯”è¾ƒå’Œç†è§£æ–‡æœ¬ã€‚å½“å¤„ç†å¤§é‡æ•°æ®æ—¶ï¼Œå°¤å…¶æ˜¯åœ¨ RAG ç³»ç»Ÿä¸­ï¼ŒåµŒå…¥å˜å¾—è‡³å…³é‡è¦ã€‚è¿™äº›ç³»ç»Ÿå°†åµŒå…¥çš„åŠ›é‡ä¸Žç§°ä¸º **å‘é‡æ•°æ®åº“** çš„ä¸“ç”¨å­˜å‚¨è§£å†³æ–¹æ¡ˆç›¸ç»“åˆã€‚ä¸Žä¼ ç»Ÿæ•°æ®åº“æœç´¢ç²¾ç¡®åŒ¹é…ä¸åŒï¼Œå‘é‡æ•°æ®åº“ç»è¿‡ä¼˜åŒ–ï¼Œä»¥æ ¹æ®å«ä¹‰æ‰¾åˆ°æœ€æŽ¥è¿‘çš„åŒ¹é…ã€‚è¿™ç§èƒ½åŠ›ä½¿ RAG ç³»ç»Ÿèƒ½å¤Ÿä»Žåºžå¤§çš„çŸ¥è¯†åº“ä¸­æ£€ç´¢å‡ºæœ€ç›¸å…³çš„ä¿¡æ¯ï¼Œå¹¶ç”¨å®ƒç”Ÿæˆå‡†ç¡®ã€å…·æœ‰ä¸Šä¸‹æ–‡çš„å“åº”ã€‚é€šè¿‡æ¡¥æŽ¥åŽŸå§‹æ•°æ®å’Œæ™ºèƒ½æ£€ç´¢ï¼ŒåµŒå…¥å’Œå‘é‡æ•°æ®åº“å…±åŒæž„æˆäº† RAG ç³»ç»ŸæˆåŠŸçš„åŸºç¡€ã€‚\n\n## å¤šè¯­è¨€ç³»ç»Ÿçš„æŒ‘æˆ˜\n\næž„å»ºåœ¨è‹±è¯­ä¸­è¡¨çŽ°è‰¯å¥½çš„RAGç³»ç»Ÿå·²ç»æ˜¯ä¸€é¡¹å¤æ‚çš„ä»»åŠ¡ï¼Œä½†å°†å…¶æ‰©å±•åˆ°å…¶ä»–è¯­è¨€åˆ™å¸¦æ¥äº†å…¨æ–°çš„æŒ‘æˆ˜ã€‚ç”±äºŽè®­ç»ƒæ•°æ®ä¸°å¯Œå’Œè¯­è¨€ç»“æž„ç®€å•ï¼Œè‹±è¯­åµŒå…¥é€šå¸¸ç»è¿‡é«˜åº¦ä¼˜åŒ–ã€‚ç„¶è€Œï¼Œä½¿ç”¨è¿™äº›ç»è¿‡è‹±è¯­è®­ç»ƒçš„åµŒå…¥æ¥å¤„ç†å…¶ä»–è¯­è¨€å¯èƒ½ä¼šå¯¼è‡´æ˜¾è‘—çš„ä¸å‡†ç¡®æ€§ã€‚ä¸åŒè¯­è¨€å…·æœ‰å…¶è‡ªèº«çš„ç»†å¾®å·®åˆ«ã€è¯­æ³•å’Œæ–‡åŒ–èƒŒæ™¯ï¼Œè€Œä¸»è¦åŸºäºŽè‹±è¯­æ–‡æœ¬è®­ç»ƒçš„æ ‡å‡†åµŒå…¥æ¨¡åž‹å¾€å¾€æ— æ³•æ•æ‰è¿™äº›ç‰¹å¾ã€‚è™½ç„¶å­˜åœ¨ä¸€äº›å¤šè¯­è¨€åµŒå…¥æ¨¡åž‹æ¥å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œä½†å®ƒä»¬åœ¨ä¸åŒè¯­è¨€ä¸­çš„æœ‰æ•ˆæ€§å¹¶ä¸ç›¸åŒï¼Œå°¤å…¶æ˜¯å¯¹äºŽé‚£äº›è®­ç»ƒæ•°æ®æœ‰é™æˆ–å…·æœ‰ç‹¬ç‰¹è¯­è¨€ç‰¹å¾çš„è¯­è¨€ã€‚è¿™ä½¿å¾—æž„å»ºåœ¨éžè‹±è¯­è¯­è¨€ä¸­ä¸Žè‹±è¯­ä¸€æ ·å‡†ç¡®å’Œå¯é çš„RAGç³»ç»Ÿå˜å¾—å›°éš¾ã€‚\n\n### ä¸ºä»€ä¹ˆè‹±è¯­åµŒå…¥æ›´å‡†ç¡®ï¼Ÿ\n\n1. **é«˜è´¨é‡è®­ç»ƒæ•°æ®çš„ä¸°å¯Œæ€§**  \nè‹±è¯­ä¸»å¯¼äº†æ•°å­—é¢†åŸŸï¼Œæ‹¥æœ‰æ— ä¸Žä¼¦æ¯”çš„é«˜è´¨é‡å†…å®¹å¯ä¾›è®­ç»ƒã€‚åƒç»´åŸºç™¾ç§‘ã€ä¹¦ç±ã€ç ”ç©¶è®ºæ–‡å’Œç¤¾äº¤åª’ä½“ç­‰æ•°æ®é›†åœ¨è‹±è¯­ä¸­è¦æ¯”å…¶ä»–è¯­è¨€ä¸°å¯Œå¾—å¤šã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œè®¸å¤šè¯­è¨€ï¼Œç‰¹åˆ«æ˜¯ä½Žèµ„æºè¯­è¨€ï¼Œç¼ºä¹å¤šæ ·åŒ–å’Œæ ‡å‡†åŒ–çš„æ•°æ®é›†ï¼Œè¿™é™åˆ¶äº†åœ¨è¿™äº›è¯­è¨€ä¸Šè®­ç»ƒçš„åµŒå…¥çš„è´¨é‡ã€‚\n2. **æ¨¡åž‹ä¼˜åŒ–åè§**  \nåƒBERTå’ŒGPTè¿™æ ·çš„NLPæ¨¡åž‹æœ€åˆæ˜¯ä¸ºè‹±è¯­å¼€å‘å’Œä¼˜åŒ–çš„ï¼Œé€šå¸¸åœ¨å¤šè¯­è¨€ç‰ˆæœ¬ä¸­ä»ç„¶ä¼˜å…ˆè€ƒè™‘è‹±è¯­ã€‚å¤šè¯­è¨€æ¨¡åž‹åœ¨åŒä¸€å‚æ•°ç©ºé—´å†…å¹³è¡¡å¤šç§è¯­è¨€çš„å­¦ä¹ ï¼Œè¿™å¯èƒ½ä¼šç¨€é‡Šå¯¹ä»£è¡¨æ€§è¾ƒå°‘çš„è¯­è¨€çš„æ€§èƒ½ï¼Œå€¾å‘äºŽåƒè‹±è¯­è¿™æ ·çš„ä¸»å¯¼è¯­è¨€ã€‚\n3. **è¯­è¨€å¤æ‚æ€§å’Œå¤šæ ·æ€§**  \nä¸Žè®¸å¤šå…¶ä»–è¯­è¨€ç›¸æ¯”ï¼Œè‹±è¯­çš„å½¢æ€ç›¸å¯¹ç®€å•ã€‚ä¾‹å¦‚ï¼Œè‹±è¯­ä¸­çš„è¯å½¢å¾€å¾€ä¿æŒä¸€è‡´ï¼ˆä¾‹å¦‚ï¼Œâ€œrunâ€å’Œâ€œrunningâ€ï¼‰ï¼Œè€ŒåƒåœŸè€³å…¶è¯­æˆ–èŠ¬å…°è¯­è¿™æ ·çš„è¯­è¨€å…·æœ‰é«˜åº¦çš„å±ˆæŠ˜å½¢å¼ï¼Œä¸€ä¸ªæ ¹è¯å¯èƒ½æœ‰æ•°åç§å˜åŒ–ã€‚æ­¤å¤–ï¼Œå…·æœ‰ä¸åŒè¯­æ³•æˆ–è¯åºçš„è¯­è¨€ï¼Œå¦‚æ—¥è¯­ï¼ˆä¸»è¯­-å®¾è¯­-åŠ¨è¯ï¼‰æˆ–é˜¿æ‹‰ä¼¯è¯­ï¼ˆçµæ´»çš„è¯åºï¼‰ï¼Œå¯¹ä¼˜åŒ–ä¸ºè‹±è¯­ç»“æž„çš„æ¨¡åž‹æž„æˆé¢å¤–æŒ‘æˆ˜ã€‚\n4. **è¯­ä¹‰å’Œæ–‡åŒ–å¯¹é½**  \nè·¨è¯­è¨€æ•æ‰è¯­ä¹‰æ„ä¹‰è¿œéžç®€å•ã€‚å•è¯å’ŒçŸ­è¯­å¾€å¾€å¸¦æœ‰ç»†å¾®çš„å«ä¹‰ï¼Œæ— æ³•ç›´æŽ¥ç¿»è¯‘ã€‚ä¾‹å¦‚ï¼Œè‹±è¯­å•è¯â€œloveâ€åœ¨å…¶ä»–è¯­è¨€ä¸­æœ‰å¤šä¸ªæ–‡åŒ–ä¸Šç‹¬ç‰¹çš„å¯¹åº”è¯ï¼ˆä¾‹å¦‚ï¼Œè¥¿ç­ç‰™è¯­ä¸­çš„â€œamorâ€ï¼Œå¸Œè…Šè¯­ä¸­çš„â€œerosâ€æˆ–â€œagapeâ€ï¼‰ã€‚æœªèƒ½è€ƒè™‘è¿™äº›å·®å¼‚çš„åµŒå…¥åœ¨å¤šè¯­è¨€å¯¹é½æ–¹é¢è¡¨çŽ°ä¸ä½³ã€‚\n5. **åŸºå‡†æµ‹è¯•å’Œè¯„ä¼°åè§**  \nè®¸å¤šåŸºå‡†æ•°æ®é›†å’Œè¯„ä¼°æ–¹æ³•éƒ½æ˜¯ä»¥è‹±è¯­ä¸ºä¸­å¿ƒè®¾è®¡çš„ã€‚è¿™ç§ä»¥è‹±è¯­ä¸ºä¸­å¿ƒçš„å…³æ³¨å¯èƒ½ä¼šäººä¸ºåœ°æé«˜æ¨¡åž‹åœ¨è‹±è¯­ä¸­çš„æ„ŸçŸ¥æ€§èƒ½ï¼ŒåŒæ—¶æŽ©ç›–å®ƒä»¬åœ¨å…¶ä»–è¯­è¨€ä¸­çš„å±€é™æ€§ã€‚\n\n### å¯¹ RAG ç³»ç»Ÿçš„å½±å“\n\nå½“åµŒå…¥æ— æ³•å¤„ç†å…¶ä»–è¯­è¨€çš„å¤æ‚æ€§æ—¶ï¼Œå¯¹ RAG ç³»ç»Ÿçš„å½±å“å¯èƒ½æ˜¯æ˜¾è‘—çš„ã€‚æ£€ç´¢ç»“æžœå¾€å¾€å˜å¾—ä¸ç›¸å…³ï¼Œç”šè‡³å®Œå…¨é”™è¯¯ï¼Œå› ä¸ºåµŒå…¥å¯èƒ½éš¾ä»¥æ•æ‰éžè‹±è¯­æŸ¥è¯¢çš„ç»†å¾®å«ä¹‰ã€‚è¿™ä¸ä»…å½±å“å‡†ç¡®æ€§ï¼Œè¿˜å‰Šå¼±äº†ç”¨æˆ·ä¿¡ä»»å’Œç³»ç»Ÿçš„æ•´ä½“å®žç”¨æ€§ã€‚åœ¨æ£€ç´¢è¿‡ç¨‹ä¸­ï¼Œå…³é”®æ–‡æœ¬ç‰‡æ®µå¯èƒ½è¢«é—æ¼ï¼Œé˜»æ­¢ç³»ç»ŸèŽ·å–ç”Ÿæˆå‡†ç¡®ä¸”ä¸Šä¸‹æ–‡ç›¸å…³çš„å“åº”æ‰€éœ€çš„ä¿¡æ¯ã€‚\n\nä¸ºäº†ä½¿å¤šè¯­è¨€ RAG ç³»ç»Ÿè¡¨çŽ°è‰¯å¥½ï¼Œå®ƒéœ€è¦èƒ½å¤Ÿåœ¨è¯­è¨€ä¹‹é—´è¯­ä¹‰å¯¹é½çš„åµŒå…¥ï¼ŒåŒæ—¶è€ƒè™‘åˆ°å®ƒä»¬ç‹¬ç‰¹çš„ç»“æž„å’Œæ–‡åŒ–å¤æ‚æ€§ã€‚æŠ•èµ„é«˜è´¨é‡çš„å¤šè¯­è¨€åµŒå…¥å¹¶å¯¹å…¶è¿›è¡Œç‰¹å®šè¯­è¨€æˆ–ä»»åŠ¡çš„å¾®è°ƒæ˜¯è‡³å…³é‡è¦çš„ã€‚è¿™ç¡®ä¿äº† RAG ç³»ç»Ÿèƒ½å¤Ÿæ»¡è¶³ä»»ä½•è¯­è¨€ç”¨æˆ·çš„éœ€æ±‚â€”â€”ä¸ä»…ä»…æ˜¯è‹±è¯­ã€‚\n\nä½†ä¸åŒçš„åµŒå…¥åœ¨éžè‹±è¯­çŽ¯å¢ƒä¸­çš„è¡¨çŽ°å¦‚ä½•å‘¢ï¼Ÿä¸ºäº†è§£è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨è·å…°æ•°æ®é›†æ¯”è¾ƒä¸€ä¸ªè‹±è¯­åµŒå…¥æ¨¡åž‹å’Œä¸€ä¸ªå¤šè¯­è¨€åµŒå…¥æ¨¡åž‹ã€‚è¿™ä¸ªæµ‹è¯•å°†æ­ç¤ºä¸åŒçš„åµŒå…¥æ–¹æ³•å¦‚ä½•å½±å“å¤šè¯­è¨€ RAG ç³»ç»Ÿä¸­çš„æ£€ç´¢å‡†ç¡®æ€§å’Œç”Ÿæˆå“åº”çš„è´¨é‡ã€‚\n\n## æ¯”è¾ƒè·å…°è¯­RAGç³»ç»Ÿçš„åµŒå…¥æ¨¡åž‹\n\nä¸ºäº†äº†è§£ä¸åŒçš„åµŒå…¥æ¨¡åž‹å¦‚ä½•å¤„ç†åƒè·å…°è¯­è¿™æ ·çš„éžè‹±è¯­è¯­è¨€ï¼Œæˆ‘ä»¬å°†æ¯”è¾ƒåœ¨Amazon Bedrockä¸Šå¯ç”¨çš„ä¸¤ä¸ªæ¨¡åž‹ï¼š**Cohere Embed English v3**å’Œ**Cohere Embed Multilingual v3**ã€‚è¿™ä¸¤ä¸ªæ¨¡åž‹ä»£è¡¨äº†å¯¹åµŒå…¥çš„ä¸åŒå¤„ç†æ–¹å¼â€”â€”ä¸€ä¸ªä¸“é—¨é’ˆå¯¹è‹±è¯­è¿›è¡Œäº†ä¼˜åŒ–ï¼Œå¦ä¸€ä¸ªåˆ™è®¾è®¡ç”¨äºŽå¤šè¯­è¨€ä»»åŠ¡ã€‚ä¸‹è¡¨æ€»ç»“äº†å®ƒä»¬çš„ä¸»è¦å±žæ€§ï¼š\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*pBhIHfOsb-McrjHKvtq4Xw.png)\n\n### æž„å»ºåµŒå…¥\n\nä¸ºäº†è¯„ä¼°åµŒå…¥æ¨¡åž‹çš„æ€§èƒ½ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ LangChain æ¡†æž¶æž„å»ºä¸€ä¸ªæœ¬åœ°å‘é‡å­˜å‚¨ã€‚å¯¹äºŽæ­¤æ¬¡è¯„ä¼°ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ç”¨è·å…°è¯­æ’°å†™çš„æ¶ˆé˜²å‘˜æŒ‡å—ä½œä¸ºæˆ‘ä»¬çš„æ•°æ®é›†ã€‚è¯¥æ–‡æ¡£åŒ…å«æŠ€æœ¯å’Œç¨‹åºä¿¡æ¯ï¼Œä½¿å…¶æˆä¸ºéžè‹±è¯­è¯­è¨€è¯­ä¹‰æ£€ç´¢çš„ä¸€ä¸ªçŽ°å®žä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ç”¨ä¾‹ã€‚ä¸‹é¢æ˜¯åˆ›å»ºæœ¬åœ°å‘é‡å­˜å‚¨å’Œç´¢å¼•æ–‡æ¡£å—çš„æ¸…ç†å’Œç®€åŒ–ä»£ç ã€‚æˆ‘ä»¬å°†ä½¿ç”¨æ­¤è®¾ç½®æ¥æµ‹è¯•ä¸¤ä¸ªåµŒå…¥æ¨¡åž‹ï¼š**Cohere Embed English v3** å’Œ **Cohere Embed Multilingual v3**ã€‚\n\n```python\nimport os\nfrom langchain_community.document_loaders import DirectoryLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import FAISS\nfrom langchain_aws import BedrockEmbeddings\nimport boto3\n\n## Step 1: Load documents\nloader = DirectoryLoader('data', glob=\"**/*.pdf\")  # Adjust 'data' to your document directory\ndocuments = loader.load()\n\nprint(f\"You have {len(documents)} documents\")\nprint(f\"Document 1 contains {len(documents[0].page_content)} characters\")\n\n## Step 2: Split documents into smaller chunks\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)\nchunks = text_splitter.split_documents(documents)\n\nprint(f\"You have {len(chunks)} chunks\")\nprint(f\"The first chunk is {len(chunks[0].page_content)} characters long\")\n\n## Step 3: Set up Bedrock embeddings\nbedrock_client = boto3.client(\"bedrock-runtime\", region_name='us-east-1')\nbedrock_embeddings = BedrockEmbeddings(model_id=\"cohere.embed-multilingual-v3\", client=bedrock_client)\n\n## Step 4: Build the FAISS vectorstore\nvectorstore = FAISS.from_documents(chunks, bedrock_embeddings)\n\n## Save the vectorstore locally for reuse\nvectorstore.save_local(\"faiss_cohere_multilingual\")\n```\n\n## ä»£ç å¦‚ä½•å·¥ä½œ\n\n1. **æ–‡æ¡£åŠ è½½**ï¼š\nä»£ç ä»Ž `data` ç›®å½•åŠ è½½æ‰€æœ‰ PDF æ–‡ä»¶ã€‚æ‚¨å¯ä»¥è°ƒæ•´æ–‡ä»¶è·¯å¾„å’Œæ ¼å¼ä»¥åŒ¹é…æ‚¨çš„æ•°æ®é›†ã€‚\n2. **æ–‡æœ¬æ‹†åˆ†**ï¼š\næ–‡æ¡£è¢«æ‹†åˆ†ä¸ºæ¯ä¸ª 400 ä¸ªå­—ç¬¦çš„å°å—ï¼Œé‡å  50 ä¸ªå­—ç¬¦ï¼Œä»¥æé«˜æ£€ç´¢çš„å‡†ç¡®æ€§ã€‚è¿™ç¡®ä¿æ¯ä¸ªå—åœ¨ä¸Šä¸‹æ–‡ä¸Šä¿æŒæœ‰æ„ä¹‰ã€‚\n3. **åµŒå…¥æ¨¡åž‹**ï¼š\n`BedrockEmbeddings` ç±»åˆå§‹åŒ–åµŒå…¥æ¨¡åž‹ã€‚æ‚¨å¯ä»¥æ›´æ”¹ `model_id` æ¥æµ‹è¯• **Cohere Embed English v3 æˆ– Cohere Embed Multilingual v3**ã€‚\n4. **æœ¬åœ°å‘é‡å­˜å‚¨**ï¼š\nFAISS åº“ç”¨äºŽä»Žæ–‡æ¡£å—åˆ›å»ºå†…å­˜ä¸­çš„å‘é‡å­˜å‚¨ã€‚è¿™å…è®¸å¿«é€Ÿç›¸ä¼¼æ€§æœç´¢ï¼Œå¹¶å¯ä»¥æœ¬åœ°ä¿å­˜ä»¥ä¾›é‡ç”¨ã€‚\n\nè¦æµ‹è¯•æ‰€æœ‰æ¨¡åž‹ï¼Œè¯·å°† `BedrockEmbeddings` åˆå§‹åŒ–ä¸­çš„ `model_id` æ›¿æ¢ä¸ºç›¸åº”çš„æ¨¡åž‹ï¼š\n\n* `\"cohere.embed-english-v3\"` ç”¨äºŽ Cohere Englishã€‚\n* `\"cohere.embed-multilingual-v3\"` ç”¨äºŽ Cohere Multilingualã€‚\n\n### è¯„ä¼°åµŒå…¥æ¨¡åž‹\n\nä¸ºäº†è¯„ä¼°åµŒå…¥æ¨¡åž‹çš„æ€§èƒ½ï¼Œæˆ‘ä»¬å°†æå‡ºé—®é¢˜ï¼š**â€œWelke rangen zijn er bij de brandweer?â€**ï¼Œå…¶ç¿»è¯‘ä¸º**â€œæ¶ˆé˜²éƒ¨é—¨å­˜åœ¨å“ªäº›ç­‰çº§ï¼Ÿâ€**ã€‚é€‰æ‹©è¿™ä¸ªé—®é¢˜æ˜¯å› ä¸ºæˆ‘ä»¬çš„æ–‡æ¡£ä¸­ä»…ä½¿ç”¨äº†æœ¯è¯­**â€œhiÃ«rarchieâ€**ï¼Œåœ¨è·å…°è¯­ä¸­ä¸Ž**â€œrangenâ€**å…·æœ‰ç›¸ä¼¼çš„è¯­ä¹‰ã€‚ç„¶è€Œï¼Œåœ¨è‹±è¯­ä¸­ï¼Œâ€œhierarchyâ€å’Œâ€œranksâ€å¹¶æ²¡æœ‰è¯­ä¹‰ä¸Šçš„ç›¸ä¼¼æ€§ã€‚\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*6N3C8C500hMQ3GNNkuu21A.png)\n\nè¿™ç§åŒºåˆ«å¯¹æˆ‘ä»¬çš„æµ‹è¯•è‡³å…³é‡è¦ã€‚æˆ‘ä»¬é¢„æœŸ**Cohere Embed English v3**æ¨¡åž‹åœ¨å¤„ç†è¿™ä¸ªæŸ¥è¯¢æ—¶ä¼šé‡åˆ°å›°éš¾ï¼Œå› ä¸ºå®ƒä¾èµ–äºŽè‹±è¯­è¯­ä¹‰ï¼Œè€Œè¿™äº›æœ¯è¯­å¹¶ä¸ç›¸å…³ã€‚å¦ä¸€æ–¹é¢ï¼Œ**Cohere Embed Multilingual v3**æ¨¡åž‹ç»è¿‡è®­ç»ƒèƒ½å¤Ÿç†è§£è·å…°è¯­è¯­ä¹‰ï¼Œåº”è¯¥èƒ½å¤Ÿä»Žæ–‡æ¡£ä¸­æ£€ç´¢åˆ°æ­£ç¡®çš„ä¿¡æ¯ï¼Œå±•ç¤ºå…¶å¤„ç†éžè‹±è¯­è¯­è¨€è¯­ä¹‰ç»†å¾®å·®åˆ«çš„èƒ½åŠ›ã€‚\n\né€šè¿‡æå‡ºè¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ—¨åœ¨çªå‡ºè¯­ä¹‰å¯¹è·å…°RAGç³»ç»Ÿæ£€ç´¢æ€§èƒ½çš„å½±å“ã€‚è¿™é¡¹æµ‹è¯•å°†æ¸…æ™°åœ°æ¯”è¾ƒæ¨¡åž‹å¤„ç†éžè‹±è¯­æŸ¥è¯¢å’Œæ£€ç´¢ç›¸å…³ä¿¡æ¯çš„èƒ½åŠ›ã€‚ç»“æžœå°†å±•ç¤ºå¤šè¯­è¨€åµŒå…¥åœ¨éžè‹±è¯­çŽ¯å¢ƒä¸­å®žçŽ°å‡†ç¡®æ£€ç´¢çš„é‡è¦æ€§ã€‚\n\nè¦å®žçŽ°å’Œæµ‹è¯•è¿™ä¸ªè®¾ç½®ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç ã€‚è¯¥è„šæœ¬æ¼”ç¤ºäº†å¦‚ä½•æŸ¥è¯¢å‘é‡å­˜å‚¨å¹¶åˆ©ç”¨RAGé“¾å°†åµŒå…¥ä¸Žè¯­è¨€æ¨¡åž‹ç»“åˆä»¥å›žç­”é—®é¢˜ã€‚è¯·æ³¨æ„ï¼Œåœ¨æµ‹è¯•ä¸åŒçš„åµŒå…¥ï¼ˆä¾‹å¦‚**Cohere Embed English v3**ä¸Ž**Cohere Embed Multilingual v3**ï¼‰æ—¶ï¼Œæ‚¨éœ€è¦ç¡®ä¿å‘é‡å­˜å‚¨æ˜¯ä½¿ç”¨ç›¸åº”çš„åµŒå…¥æ¨¡åž‹æž„å»ºçš„ã€‚ç”¨æ‚¨æƒ³è¦æµ‹è¯•çš„åµŒå…¥æ¨¡åž‹ç´¢å¼•çš„å‘é‡å­˜å‚¨æ›¿æ¢ï¼Œä»¥èŽ·å¾—å‡†ç¡®çš„ç»“æžœã€‚\n\n```python\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain_aws import ChatBedrock\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.output_parsers import StrOutputParser\n\ninstructions = \"\"\"Je bent een brandweer expert. Beantwoord de vraag, maak gebruik van de context\"\"\"\n\nhuman = \"\"\"\nDit is de context: {context}\nDit is de vraag: {question}\n\"\"\"\n\nprompt = ChatPromptTemplate(\n    messages=[\n        SystemMessagePromptTemplate.from_template(instructions),\n        HumanMessagePromptTemplate.from_template(human), #User query will go here\n    ],\n    input_variables=['context','question'], # context provided by retriever and question by the user\n)\nmodel = ChatOpenAI(\n    model=\"gpt-4\",  # or \"gpt-3.5-turbo\" based on your needs\n    temperature=0,\n    max_tokens=512\n)\n\n## Function to merge text chunks\ndef format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n\n## RAG chain\nrag_chain = (\n        {\"context\": vectorstore.as_retriever(k=10) | format_docs, \"question\": RunnablePassthrough()}\n        | prompt\n        | model\n        | StrOutputParser()\n)\n\nrag_chain.invoke(\"Welke rangen zijn er bij de brandweer?\")\n```\n\n## åµŒå…¥æ¨¡åž‹çš„ç»“æžœ\n\nä¸ºäº†è¯„ä¼°è¿™ä¸¤ä¸ªåµŒå…¥æ¨¡åž‹çš„æ€§èƒ½ï¼Œæˆ‘ä»¬æå‡ºäº†è¿™ä¸ªé—®é¢˜ï¼š**â€œWelke rangen zijn er bij de brandweer?â€**ï¼ˆç¿»è¯‘ï¼šâ€œæ¶ˆé˜²éƒ¨é—¨å­˜åœ¨å“ªäº›èŒçº§ï¼Ÿâ€ï¼‰ã€‚ä»¥ä¸‹æ˜¯æ¯ä¸ªæ¨¡åž‹ç”Ÿæˆçš„å“åº”ï¼š\n\n### Cohere Embed English v3\n\n**Response**:*â€œæ–‡æœ¬æ²¡æœ‰æä¾›å…³äºŽæ¶ˆé˜²é˜Ÿå†…éƒ¨ä¸åŒç­‰çº§çš„å…·ä½“ä¿¡æ¯ã€‚ç„¶è€Œï¼Œåœ¨è®¸å¤šæ¶ˆé˜²ç»„ç»‡ä¸­ï¼Œæœ‰ä¸åŒçš„ç­‰çº§ï¼Œå¦‚æ¶ˆé˜²å‘˜ã€ lieutenantã€ captainã€ battalion chiefã€ assistant chief å’Œ fire chiefã€‚å…·ä½“çš„ç­‰çº§å¯èƒ½å› å›½å®¶å’Œå…·ä½“ç»„ç»‡è€Œå¼‚ã€‚â€*\n\n**Analysis**:è¯¥å“åº”æ˜¯é€šç”¨çš„ï¼Œå¹¶æ²¡æœ‰ä¾èµ–äºŽè·å…°æ–‡æ¡£æä¾›çš„ä¸Šä¸‹æ–‡ã€‚è¿™æ˜¯å¯ä»¥é¢„æœŸçš„ï¼Œå› ä¸ºè‹±è¯­åµŒå…¥æ¨¡åž‹åœ¨è¿žæŽ¥â€œhiÃ«rarchieâ€ï¼ˆæ–‡æ¡£ä¸­ä½¿ç”¨ï¼‰å’Œâ€œrangenâ€ï¼ˆæŸ¥è¯¢ä¸­ä½¿ç”¨ï¼‰ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼æ€§æ—¶é‡åˆ°äº†å›°éš¾ã€‚å› æ­¤ï¼Œå®ƒé»˜è®¤ä½¿ç”¨å…³äºŽæ¶ˆé˜²éƒ¨é—¨ç­‰çº§çš„ä¸€èˆ¬çŸ¥è¯†ã€‚\n\n### Cohere Embed Multilingual v3\n\n**Response**:*â€œåœ¨æ¶ˆé˜²é˜Ÿä¸­æœ‰ä¸åŒçš„ç­‰çº§ã€‚è¿™äº›ç­‰çº§åˆ†ä¸ºåŸºç¡€æ¡†æž¶å’Œä¸­å±‚æ¡†æž¶ã€‚åœ¨åŸºç¡€æ¡†æž¶ä¸­ï¼Œæˆ‘ä»¬æœ‰ B01 æ¶ˆé˜²å‘˜å’Œ B02 ä¸‹å£«ã€‚åœ¨ä¸­å±‚æ¡†æž¶ä¸­ï¼Œç”±ä¸‹å£«ç»„æˆï¼Œæˆ‘ä»¬æœ‰ M01 ä¸­å£«å’Œ M02 å‰¯å®˜ã€‚è¿™äº›ç­‰çº§åœ¨å·¥ä½œåˆ¶æœã€ä¼‘é—²æœå’Œå¹²é¢„æœä¸Šéƒ½æœ‰æ ‡è¯†ã€‚â€*\n\n**Analysis**:æ­¤å“åº”é«˜åº¦ç›¸å…³ï¼Œå¹¶å‡†ç¡®åœ°ä»Žæ–‡æ¡£ä¸­æ£€ç´¢ä¿¡æ¯ã€‚å¤šè¯­è¨€åµŒå…¥æ¨¡åž‹æˆåŠŸè¯†åˆ«äº†â€œhiÃ«rarchieâ€ï¼ˆä¸Šä¸‹æ–‡ï¼‰ä¸Žâ€œrangenâ€ï¼ˆæŸ¥è¯¢ï¼‰ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ã€‚å®ƒç›´æŽ¥åŸºäºŽæ–‡æ¡£å†…å®¹æä¾›äº†è¯¦ç»†çš„ç­”æ¡ˆï¼Œå±•ç¤ºäº†å…¶æœ‰æ•ˆå¤„ç†è·å…°ç‰¹å®šè¯­ä¹‰çš„èƒ½åŠ›ã€‚\n\n### å…³é”®è¦ç‚¹\n\n* **Cohere Embed English v3**ï¼šç”±äºŽæŸ¥è¯¢ä¸Žæ–‡æ¡£æœ¯è¯­ä¹‹é—´ç¼ºä¹è¯­ä¹‰å¯¹é½ï¼Œè‹±è¯­æ¨¡åž‹æœªèƒ½ä»Žè·å…°æ–‡æ¡£ä¸­æ£€ç´¢åˆ°ç›¸å…³ä¸Šä¸‹æ–‡ã€‚è¿™çªæ˜¾äº†åœ¨éžè‹±è¯­ä»»åŠ¡ä¸­ä½¿ç”¨è‹±è¯­ç‰¹å®šåµŒå…¥çš„å±€é™æ€§ã€‚\n* **Cohere Embed Multilingual v3**ï¼šå¤šè¯­è¨€æ¨¡åž‹åœ¨æ­¤æµ‹è¯•ä¸­è¡¨çŽ°å‡ºè‰²ï¼Œä»Žè·å…°æ–‡æ¡£ä¸­æ£€ç´¢å¹¶åˆ©ç”¨äº†ä¸Šä¸‹æ–‡ç›¸å…³çš„ä¿¡æ¯ã€‚è¿™è¡¨æ˜Žå¤šè¯­è¨€åµŒå…¥åœ¨å®žçŽ°å‡†ç¡®æ£€ç´¢å’Œæœ‰æ•ˆå›žç­”éžè‹±è¯­æŸ¥è¯¢æ–¹é¢çš„é‡è¦æ€§ã€‚\n\n## ç»“è®º\n\næœ¬æ¬¡è¯„ä¼°çªæ˜¾äº†ä¸€ä¸ªå¯¹ä»»ä½•æž„å»ºéžè‹±è¯­è¯­è¨€çš„æ£€ç´¢å¢žå¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿçš„äººæ¥è¯´è‡³å…³é‡è¦çš„è§è§£ï¼š**åµŒå…¥éžå¸¸é‡è¦**ï¼Œå°¤å…¶æ˜¯åœ¨ä»»åŠ¡è¦æ±‚è·¨è¯­è¨€ç»†è‡´ç†è§£æ—¶ã€‚Cohere Embed English v3 å’Œ Cohere Embed Multilingual v3 æ¨¡åž‹åœ¨æ€§èƒ½ä¸Šçš„æ˜Žæ˜¾å·®å¼‚è¯´æ˜Žäº†è‹±è¯­ç‰¹å®šåµŒå…¥åœ¨éžè‹±è¯­çŽ¯å¢ƒä¸­çš„å±€é™æ€§ï¼Œä»¥åŠå¤šè¯­è¨€æ¨¡åž‹çš„å·¨å¤§ä»·å€¼ã€‚\n\nåœ¨å›žç­”è·å…°è¯­æŸ¥è¯¢æ—¶ï¼Œå¤šè¯­è¨€æ¨¡åž‹è¡¨çŽ°å‡ºè‰²ï¼Œèƒ½å¤Ÿç›´æŽ¥ä»Žæ–‡æ¡£ä¸­æ£€ç´¢åˆ°å‡†ç¡®ä¸”ä¸Šä¸‹æ–‡ä¸°å¯Œçš„ä¿¡æ¯ã€‚ä¸Žæ­¤åŒæ—¶ï¼Œè‹±è¯­åµŒå…¥æ¨¡åž‹åˆ™é€€å›žåˆ°é€šç”¨çš„ã€ä¸ç›¸å…³çš„çŸ¥è¯†ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨æŸ¥è¯¢ä¸Žæ–‡æ¡£å†…å®¹ä¹‹é—´å¼¥åˆè¯­ä¹‰å·®è·çš„æ— èƒ½ã€‚\n\nå¯¹äºŽåœ¨å…¨çƒå¤šè¯­è¨€çŽ¯å¢ƒä¸­å¼€å‘ AI ç³»ç»Ÿçš„ç»„ç»‡è€Œè¨€ï¼Œè¿™é¡¹æµ‹è¯•å¼ºåŒ–äº†ä¸ºæ‰‹å¤´ä»»åŠ¡é€‰æ‹©åˆé€‚åµŒå…¥æ¨¡åž‹çš„é‡è¦æ€§ã€‚å¤šè¯­è¨€åµŒå…¥ä¸ä»…ä»…æ˜¯ä¸€ä¸ªâ€œé”¦ä¸Šæ·»èŠ±â€çš„ç‰¹æ€§ï¼›å®ƒä»¬å¯¹äºŽç¡®ä¿éžè‹±è¯­åº”ç”¨çš„å‡†ç¡®æ€§ã€ç›¸å…³æ€§å’Œç”¨æˆ·ä¿¡ä»»è‡³å…³é‡è¦ã€‚\n\néšç€ç”Ÿæˆ AI ç»§ç»­æ‹“å±•å…¶å½±å“åŠ›ï¼Œé€šè¿‡æ›´å¥½çš„åµŒå…¥æ¥æ‹¥æŠ±è¯­è¨€å¤šæ ·æ€§å°†æ˜¯æä¾›æœ‰æ„ä¹‰å’Œæœ‰å½±å“åŠ›çš„è§£å†³æ–¹æ¡ˆçš„å…³é”®ã€‚é€šè¿‡ä¼˜å…ˆè€ƒè™‘å¤šè¯­è¨€èƒ½åŠ›ï¼Œä¼ä¸šå¯ä»¥åˆ›å»ºä¸ä»…æ›´æ™ºèƒ½è€Œä¸”æ›´å…·åŒ…å®¹æ€§çš„ç³»ç»Ÿâ€”â€”èµ‹èƒ½è·¨è¯­è¨€å’Œæ–‡åŒ–çš„ç”¨æˆ·ã€‚\n\n***å…³æ³¨æˆ‘ä»¥èŽ·å–æ›´å¤š AI æ·±åº¦è§£æžï¼***\n\n[Medium](https://proxy.rifx.online/https://medium.com/@lorevanoudenhove), [Instagram](https://proxy.rifx.online/https://www.instagram.com/lorevanoudenhove.ai/), [YouTube](https://proxy.rifx.online/https://www.youtube.com/channel/UCVyOJS1VV7FxPsStK65pHcA), [Pairrot](https://proxy.rifx.online/https://www.pairrot.eu/)\n\n"},{"lang":"zh","group":"blog","slug":"blog/will-bolt-new-ai-will-replace-v0-dev-129a3366eb44","frontmatter":{"title":"æ–°äººå·¥æ™ºèƒ½ Bolt æ˜¯å¦ä¼šå–ä»£ v0.dev","meta_title":"æ–°äººå·¥æ™ºèƒ½ Bolt æ˜¯å¦ä¼šå–ä»£ v0.dev","description":"æ–°äººå·¥æ™ºèƒ½ Bolt æ˜¯å¦ä¼šå–ä»£ v0.dev","date":"2024-11-13T01:22:35.000Z","image":"https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*g5S8PyYqR87bdyGhb77rRw.png","categories":["Programming","Technology/Web","Data Science"],"author":"Rifx.Online","tags":["Bolt.new","v0.dev","web","components","layouts"],"draft":false,"slug":"blog/will-bolt-new-ai-will-replace-v0-dev-129a3366eb44"},"content":"\n### AIå·¥å…·\n\n> **ä¸æ˜¯ä¼šå‘˜ï¼Ÿå…è´¹é˜…è¯» [è¿™é‡Œ](https://proxy.rifx.online/https://tarzzotech.medium.com/129a3366eb44?source=friends_link&sk=385b6b2e482ae9d16ef8f99fe083b8ae)ã€‚**\n\n\n\nç½‘é¡µå¼€å‘çš„ä¸–ç•Œæ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œå¸‚åœºä¸Šæ¶ŒçŽ°å‡ºå¤šç§AIå·¥å…·ã€‚è¿™äº›æ–°AIå·¥å…·é€šè¿‡è‡ªç„¶è¯­è¨€æç¤ºå¸®åŠ©å¼€å‘è€…ç”Ÿæˆç½‘é¡µç»„ä»¶å’Œå¤æ‚çš„ä»£ç ç»“æž„ã€‚è¿™äº›å·¥å…·æä¾›æ›´å¥½çš„ä»£ç è´¨é‡ï¼Œå¹¶å‡å°‘æ‰‹åŠ¨ç¼–å†™ä»£ç çš„æ—¶é—´ã€‚\n\næœ€è¿‘ï¼Œå¸‚åœºä¸Šå‡ºçŽ°äº†ä¸€ç§æ–°å·¥å…· **bolt.new**ï¼Œå®ƒçœ‹èµ·æ¥ä¸Ž v0\\.dev ç›¸ä¼¼ã€‚éšç€ Bolt.new çš„å‡ºçŽ°ï¼Œäº§ç”Ÿäº†ä¸€ä¸ªé—®é¢˜ï¼š***è¿™ä¸ªæ–°çš„AIå¹³å°ä¼šå–ä»£ v0\\.dev å—ï¼Œè¿˜æ˜¯è¿™äº›å·¥å…·å®Œå…¨æ˜¯ä¸åŒçš„ç”¨é€”ï¼Ÿ***\n\nåœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†åˆ†äº«æˆ‘ä½¿ç”¨è¯¥å·¥å…·ä¸€æ®µæ—¶é—´çš„ç»éªŒã€‚æˆ‘ä¸»è¦è®¨è®º **v0\\.dev** å’Œ **Bolt.new** ä¹‹é—´çš„å…³é”®åŒºåˆ«ï¼Œæ¯”è¾ƒå®ƒä»¬çš„ä¼˜åŠ¿ã€ä½¿ç”¨æ¡ˆä¾‹å’Œè¾“å‡ºã€‚é€šè¿‡è€ƒå¯ŸçœŸå®žæ¡ˆä¾‹ï¼Œæˆ‘ä»¬å°†ç¡®å®š **Bolt.new** æ˜¯å¦å¯¹ **v0\\.dev** æž„æˆäº†çœŸæ­£çš„å¨èƒï¼Œæˆ–è€…è¿™ä¸¤ç§å·¥å…·åœ¨å¼€å‘è€…çš„å·¥å…·åŒ…ä¸­å„æœ‰å…¶ä½ç½®ã€‚\n\nåœ¨æˆ‘ä¹‹å‰çš„åšå®¢ä¸­ï¼Œæˆ‘è¯¦ç»†è§£é‡Šäº†å…³äºŽ v0\\.dev çš„æ‰€æœ‰ç»†èŠ‚å’Œæˆ‘çš„æƒ³æ³•ï¼Œæ‰€ä»¥è¯·æŸ¥çœ‹ [**è¿™é‡Œ**](https://proxy.rifx.online/https://tarzzotech.medium.com/4191292876b3?source=friends_link&sk=9730b35a75771953d0541e459c8adeaa)ã€‚æˆ‘ä¸æƒ³åœ¨è¿™é‡Œåˆ†äº«é‡å¤çš„å†…å®¹ã€‚è®©æˆ‘ä»¬çœ‹çœ‹ **bolt.new** ä»¥åŠä¸Ž **v0\\.dev** çš„æ¯”è¾ƒã€‚\n\n## Bolt.new æ¦‚è¿°\n\n**Bolt.new** ä½œä¸ºä¸€ä¸ªæ–°å…´çš„ AI é©±åŠ¨å¹³å°ï¼Œå¼•èµ·äº†å¹¿æ³›å…³æ³¨ï¼Œå…¶ç›®æ ‡ä¸ä»…é™äºŽå‰ç«¯ç»„ä»¶ã€‚è™½ç„¶ **v0.dev** ä¸“æ³¨äºŽç”Ÿæˆè¾ƒå°çš„ç»„ä»¶ï¼Œä½† Bolt.new æ—¨åœ¨æä¾›æ›´å¤æ‚çš„è§£å†³æ–¹æ¡ˆï¼Œä¾‹å¦‚å®Œæ•´çš„é¡µé¢å¸ƒå±€æˆ–è·¨è¶Šå‰ç«¯å’ŒåŽç«¯çš„å¤šæ­¥éª¤å·¥ä½œæµã€‚\n\n**Bolt.new çš„ä¸»è¦ç‰¹ç‚¹ï¼š**\n\n* **å®Œæ•´å¸ƒå±€ï¼š** **Bolt.new** èƒ½å¤Ÿç”Ÿæˆå®Œæ•´çš„é¡µé¢å¸ƒå±€ï¼ŒåŒ…æ‹¬é¡µçœ‰ã€é¡µè„šã€ä¾§è¾¹æ å’Œä¸»è¦å†…å®¹åŒºåŸŸã€‚\n* **å¤šåŠŸèƒ½ä»£ç ç”Ÿæˆï¼š** é™¤äº†ç½‘é¡µç»„ä»¶ï¼Œ**Bolt.new** è¿˜å¯ä»¥ç”ŸæˆæœåŠ¡å™¨ç«¯è„šæœ¬ã€æ•°æ®åº“é…ç½®åŠæž„å»ºå…¨æ ˆåº”ç”¨æ‰€éœ€çš„å…¶ä»–å…ƒç´ ã€‚\n* **å¢žå¼ºçš„å®šåˆ¶åŒ–ï¼š** è™½ç„¶ V0.dev ä¸“æ³¨äºŽå•ä¸ªç»„ä»¶ï¼Œä½† **Bolt.new** æä¾›äº†æ›´å¤§çš„çµæ´»æ€§ï¼Œå…è®¸å¼€å‘äººå‘˜ç”Ÿæˆå®Œå…¨åŠŸèƒ½çš„å¸ƒå±€æˆ–ç»“æž„ã€‚\n\n## æ¯”è¾ƒ V0\\.dev å’Œ Bolt.new\n\nè™½ç„¶è¿™ä¸¤ä¸ªå·¥å…·éƒ½æä¾›äº†ä»¤äººå°è±¡æ·±åˆ»çš„åŠŸèƒ½ï¼Œä½†å®ƒä»¬é’ˆå¯¹å¼€å‘è¿‡ç¨‹çš„ä¸åŒæ–¹é¢ã€‚\n\n* **v0\\.dev** ä¸“æ³¨äºŽå¯ä»¥è½»æ¾å®šåˆ¶å¹¶é›†æˆåˆ°å‰ç«¯ä»£ç åº“ä¸­çš„å•ä¸ªç½‘é¡µç»„ä»¶ã€‚å®ƒéžå¸¸é€‚åˆéœ€è¦å¿«é€Ÿè§£å†³æ–¹æ¡ˆçš„å¼€å‘è€…ï¼Œä¾‹å¦‚åœ¨ React æˆ– Vue ç­‰æ¡†æž¶ä¸­ä½¿ç”¨çš„æŒ‰é’®ã€å¡ç‰‡æˆ–è¡¨å•ã€‚\n* **Bolt.new** åˆ™é‡‡å–æ›´å…¨é¢çš„æ–¹æ³•ï¼Œå…è®¸å¼€å‘è€…ç”Ÿæˆå®Œæ•´çš„é¡µé¢å¸ƒå±€æˆ–ç”šè‡³åŽç«¯é…ç½®ã€‚è¿™ä½¿å¾—å®ƒæˆä¸ºä¸€ä¸ªæ›´çµæ´»çš„é€‰æ‹©ï¼Œé€‚ç”¨äºŽéœ€è¦å¤šç§ç±»åž‹ä»£ç çš„é¡¹ç›®ï¼Œè€Œä¸ä»…ä»…æ˜¯å‰ç«¯ç»„ä»¶ã€‚\n\n## ç¤ºä¾‹æ¯”è¾ƒï¼š\n\nåœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¼€å§‹ä»Žå°ç»„ä»¶åˆ°å®Œæ•´é¡µé¢æ¯”è¾ƒè¿™äº›å·¥å…·ã€‚\n\n### åˆ›å»ºæŒ‰é’®ç»„ä»¶\n\n**æç¤ºï¼š** â€œç”Ÿæˆä¸€ä¸ªå¸¦ä¸‹æ‹‰èœå•å’Œæœç´¢æ çš„å“åº”å¼å¯¼èˆªæ ã€‚â€\n\n**v0\\.dev**\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8-1NaJb_msK1OLv7MHbOjw.gif)\n\n**bolt.new**\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*WsRSUU5brIql4uBb7wFkAg.gif)\n\n**v0\\.dev** åˆ›å»ºäº†ä¸€ä¸ªå…·æœ‰åŸºæœ¬æ ·å¼çš„æŒ‰é’®å…ƒç´ ï¼Œå¹¶ä¸”åœ¨æ‚¬åœæ—¶ä¼šæ”¹å˜èƒŒæ™¯é¢œè‰²å’Œæ–‡æœ¬æ ·å¼çš„æ‚¬åœæ•ˆæžœã€‚\n\n**Bolt.new** åˆ›å»ºäº†ä¸€ä¸ªå…·æœ‰æ‚¬åœæ•ˆæžœçš„æŒ‰é’®ï¼Œå¹¶ä¸”åŒ…å«åœ¨ç‚¹å‡»æ—¶è§¦å‘è­¦å‘Šæ¡†çš„JavaScriptä»£ç ï¼ŒåŒ…æ‹¬å†…è”JavaScriptå’ŒåŸºæœ¬CSSæ ·å¼ã€‚\n\n### åˆ›å»ºå¯¼èˆªæ \n\n**æç¤ºï¼š** â€œç”Ÿæˆä¸€ä¸ªå¸¦æœ‰ä¸‹æ‹‰èœå•å’Œæœç´¢æ çš„å“åº”å¼å¯¼èˆªæ ã€‚â€\n\n**v0\\.dev**\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*sOJ0EveSOVKtJJltLGiVCA.gif)\n\n**bolt.new**\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*aGCWfH5ULTTFb-FS-kT-7w.gif)\n\nè¿™ä¸¤ä¸ªå·¥å…·ç”Ÿæˆçš„å¯¼èˆªæ æœ‰äº›ç›¸ä¼¼ã€‚ä½† **bolt.new** ç”Ÿæˆçš„æ•ˆæžœç•¥ä¼˜äºŽ **v0\\.dev**ã€‚\n\n### åˆ›å»ºç½‘ç«™ã€‚\n\n**æç¤º:** â€œç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„é¡µé¢å¸ƒå±€ï¼ŒåŒ…å«ç²˜æ€§å¤´éƒ¨ã€é¡µè„šã€å¯æŠ˜å ä¾§è¾¹æ å’Œä¸ºåšå®¢æ–‡ç« è®¾è®¡çš„ä¸»è¦å†…å®¹åŒºåŸŸã€‚â€\n\n**v0\\.dev**\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kwEXDG3tb1CiHetZr5W03Q.png)\n\n**bolt.new**\n\n![](https://images.weserv.nl/?url=https://proxy.rifx.online/https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*HxKnFJQf--e-E1fh8yyxVw.png)\n\næ¥è‡ª **bolt.new** çš„è¾“å‡ºè®©äººæ— è¨€ä»¥å¯¹ã€‚\n\n## æœ€ç»ˆï¼ŒBolt.new ä¼šå–ä»£ v0\\.dev å—ï¼Ÿ\n\nè€ƒè™‘åˆ° Bolt.new çš„æ›´å¹¿æ³›èŒƒå›´ï¼Œå®ƒå¯èƒ½çœ‹èµ·æ¥ä¼šæŽ©ç›– v0\\.devã€‚ç„¶è€Œï¼Œè¿™ä¸¤ä¸ªå¹³å°å¾ˆå¯èƒ½ä¼šå…±å­˜ï¼Œå„è‡ªä¸ºå¼€å‘è¿‡ç¨‹çš„ä¸åŒé˜¶æ®µæä¾›ä»·å€¼ã€‚\n\n**v0\\.dev** åœ¨å¿«é€Ÿç”Ÿæˆç‰¹å®šçš„é«˜è´¨é‡ç½‘é¡µç»„ä»¶æ–¹é¢æ— ä¸Žä¼¦æ¯”ã€‚å®ƒéžå¸¸é€‚åˆéœ€è¦å¯é‡ç”¨ã€å“åº”å¼ç»„ä»¶çš„å‰ç«¯å¼€å‘äººå‘˜ï¼Œè¿™äº›ç»„ä»¶å¯ä»¥è½»æ¾é›†æˆåˆ°ä»–ä»¬çš„é¡¹ç›®ä¸­ã€‚\n\n**Bolt.new** è™½ç„¶æä¾›äº†æ›´å¤šçš„çµæ´»æ€§ï¼Œä½†å¹¶ä¸ä¸€å®šæ˜¯ v0\\.dev çš„æ›¿ä»£å“ã€‚å®ƒæ›´å¹¿æ³›çš„åŠŸèƒ½èŒƒå›´å¸å¼•äº†éœ€è¦å®Œæ•´å¸ƒå±€æˆ–æ¶µç›–å‰ç«¯å’ŒåŽç«¯çš„ä»£ç ç»“æž„çš„å¼€å‘äººå‘˜ã€‚\n\n## ç»“è®º\n\nv0\\.dev å’Œ Bolt.new éƒ½æ˜¯å¼ºå¤§çš„ AI å·¥å…·ï¼Œå„è‡ªå…·æœ‰ç‹¬ç‰¹çš„ä¼˜åŠ¿ã€‚è™½ç„¶ Bolt.new çš„å¤šåŠŸèƒ½æ€§ä½¿å…¶æˆä¸ºå¸Œæœ›ç”Ÿæˆæ›´å¤æ‚ä»£ç çš„å¼€å‘è€…çš„å¼ºåŠ²ç«žäº‰è€…ï¼Œä½† v0\\.dev ä»ç„¶æ˜¯å¸Œæœ›å¿«é€ŸèŽ·å¾—å¯å®šåˆ¶ç»„ä»¶çš„å¼€å‘è€…çš„é¦–é€‰å·¥å…·ã€‚\n\næœ€ç»ˆï¼ŒBolt.new æ˜¯å¦ä¼šå–ä»£ v0\\.devï¼Ÿè¿™ä¸å¤ªå¯èƒ½ã€‚è¿™äº›å¹³å°æ»¡è¶³ä¸åŒçš„éœ€æ±‚ã€‚v0\\.dev åœ¨è½»æ¾ç”Ÿæˆç‰¹å®šçš„ã€å¯é‡ç”¨çš„ç»„ä»¶æ–¹é¢è¡¨çŽ°å‡ºè‰²ã€‚Bolt.new åˆ™ä¸ºå¤æ‚å¸ƒå±€æä¾›äº†æ›´å¹¿æ³›çš„åŠŸèƒ½ã€‚é‚£ä¹ˆï¼Œæ‚¨è®¤ä¸ºå“ªç§å·¥å…·å°†åœ¨ AI è¾…åŠ©å¼€å‘çš„æœªæ¥ä¸­å æ®ä¸»å¯¼åœ°ä½ï¼Œè¿˜æ˜¯å®ƒä»¬ä¼šç»§ç»­ç›¸è¾…ç›¸æˆï¼Ÿç­”æ¡ˆå–å†³äºŽæ‚¨æ­£åœ¨å·¥ä½œçš„å¼€å‘è€…éœ€æ±‚ã€‚\n\n\n"},{"lang":"fr","group":"blog","slug":"blog/post-1","frontmatter":{"title":"Comment crÃ©er une application avec des technologies modernes","meta_title":"","description":"Ceci est une mÃ©ta-description","date":"2022-04-04T05:00:00.000Z","image":"/images/image-placeholder.png","categories":["french","Application","Data"],"author":"John Doe","tags":["nextjs","tailwind","react"],"draft":false,"slug":"blog/post-1"},"content":"\nPersonne ne veut mÃªme sortir un maquillage de l'urne des soins empoisonnÃ©s. C'Ã©tait un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\nL'entreprise elle-mÃªme est une entreprise trÃ¨s prospÃ¨re. Personne ne prend mÃªme la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? ÃŠtre rejetÃ© par certaines personnes est un choix commode du prÃ©sent pour ressentir une douleur comme la sienne !\n\n## Design CrÃ©atif\n\nCar en guise de maquillage, l'urne du poison C'Ã©tait un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\n> Le client lui-mÃªme doit pouvoir poursuivre l'adipisicing. Personne ne prend mÃªme la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? ÃŠtre rejetÃ© par certaines personnes est un choix commode du prÃ©sent pour ressentir une douleur comme la sienne !\n\nL'entreprise elle-mÃªme est une entreprise trÃ¨s prospÃ¨re. Personne ne prend mÃªme la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? ÃŠtre rejetÃ© par certaines personnes est un choix commode du prÃ©sent pour ressentir une douleur comme la sienne !\n"},{"lang":"fr","group":"blog","slug":"blog/post-2","frontmatter":{"title":"Comment crÃ©er une application avec des technologies modernes","meta_title":"","description":"Ceci est une mÃ©ta-description","date":"2022-04-04T05:00:00.000Z","image":"/images/image-placeholder.png","categories":["Technology","Data"],"author":"Sam Wilson","tags":["technology","tailwind"],"draft":false,"slug":"blog/post-2"},"content":"\nPersonne ne veut mÃªme sortir un maquillage de l'urne des soins empoisonnÃ©s. C'Ã©tait un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\nL'entreprise elle-mÃªme est une entreprise trÃ¨s prospÃ¨re. Personne ne prend mÃªme la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? ÃŠtre rejetÃ© par certaines personnes est un choix commode du prÃ©sent pour ressentir une douleur comme la sienne !\n\n## Design CrÃ©atif\n\nCar en guise de maquillage, l'urne du poison C'Ã©tait un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\n> Le client lui-mÃªme doit pouvoir poursuivre l'adipisicing. Personne ne prend mÃªme la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? ÃŠtre rejetÃ© par certaines personnes est un choix commode du prÃ©sent pour ressentir une douleur comme la sienne !\n\nL'entreprise elle-mÃªme est une entreprise trÃ¨s prospÃ¨re. Personne ne prend mÃªme la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? ÃŠtre rejetÃ© par certaines personnes est un choix commode du prÃ©sent pour ressentir une douleur comme la sienne !\n"},{"lang":"fr","group":"blog","slug":"blog/post-3","frontmatter":{"title":"Comment crÃ©er une application avec des technologies modernes","meta_title":"","description":"Ceci est une mÃ©ta-description","date":"2022-04-04T05:00:00.000Z","image":"/images/image-placeholder.png","categories":["Software"],"author":"John Doe","tags":["software","tailwind"],"draft":false,"slug":"blog/post-3"},"content":"\nPersonne ne veut mÃªme sortir un maquillage de l'urne des soins empoisonnÃ©s. C'Ã©tait un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\nL'entreprise elle-mÃªme est une entreprise trÃ¨s prospÃ¨re. Personne ne prend mÃªme la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? ÃŠtre rejetÃ© par certaines personnes est un choix commode du prÃ©sent pour ressentir une douleur comme la sienne !\n\n## Design CrÃ©atif\n\nCar en guise de maquillage, l'urne du poison C'Ã©tait un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\n> Le client lui-mÃªme doit pouvoir poursuivre l'adipisicing. Personne ne prend mÃªme la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? ÃŠtre rejetÃ© par certaines personnes est un choix commode du prÃ©sent pour ressentir une douleur comme la sienne !\n\nL'entreprise elle-mÃªme est une entreprise trÃ¨s prospÃ¨re. Personne ne prend mÃªme la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? ÃŠtre rejetÃ© par certaines personnes est un choix commode du prÃ©sent pour ressentir une douleur comme la sienne !\n"},{"lang":"fr","group":"blog","slug":"blog/post-4","frontmatter":{"title":"Comment crÃ©er une application avec des technologies modernes","meta_title":"","description":"Ceci est une mÃ©ta-description","date":"2022-04-04T05:00:00.000Z","image":"/images/image-placeholder.png","categories":["Architecture"],"author":"John Doe","tags":["silicon","technology"],"draft":false,"slug":"blog/post-4"},"content":"\nPersonne ne veut mÃªme sortir un maquillage de l'urne des soins empoisonnÃ©s. C'Ã©tait un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\nL'entreprise elle-mÃªme est une entreprise trÃ¨s prospÃ¨re. Personne ne prend mÃªme la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? ÃŠtre rejetÃ© par certaines personnes est un choix commode du prÃ©sent pour ressentir une douleur comme la sienne !\n\n## Design CrÃ©atif\n\nCar en guise de maquillage, l'urne du poison C'Ã©tait un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\n> Le client lui-mÃªme doit pouvoir poursuivre l'adipisicing. Personne ne prend mÃªme la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? ÃŠtre rejetÃ© par certaines personnes est un choix commode du prÃ©sent pour ressentir une douleur comme la sienne !\n\nL'entreprise elle-mÃªme est une entreprise trÃ¨s prospÃ¨re. Personne ne prend mÃªme la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? ÃŠtre rejetÃ© par certaines personnes est un choix commode du prÃ©sent pour ressentir une douleur comme la sienne !\n"},{"lang":"en","group":"models","slug":"models/chatgpt-4o-latest","frontmatter":{"title":"OpenAI: ChatGPT-4o","meta_title":"OpenAI: ChatGPT-4o","description":"OpenAI: ChatGPT-4o","date":"2024-08-14T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text image 2 text"],"author":"openai","tags":["Chatbots","Generative AI","Machine Learning","Natural Language Processing"],"draft":false,"id":"chatgpt-4o-latest","context":128000,"input":0.000005,"output":0.000015,"img":0.007225,"request":0,"last_updated":"2024-08-14T00:00:00.000Z","slug":"models/chatgpt-4o-latest"},"content":"\nDynamic model continuously updated to the current version of [GPT-4o](/openai/gpt-4o) in ChatGPT. Intended for research and evaluation.\n\nNote: This model is currently experimental and not suitable for production use-cases, and may be heavily rate-limited.\n\n"},{"lang":"en","group":"models","slug":"models/claude-3-haiku","frontmatter":{"title":"Anthropic: Claude 3 Haiku","meta_title":"Anthropic: Claude 3 Haiku","description":"Anthropic: Claude 3 Haiku","date":"2024-03-13T00:00:00.000Z","image":"https://img.rifx.online/logo/anthropic.svg","categories":["text image 2 text"],"author":"anthropic","tags":["Programming","Machine Learning","Generative AI","Chatbots","Natural Language Processing"],"draft":false,"id":"claude-3-haiku","context":200000,"input":2.5e-7,"output":0.00000125,"img":0.0004,"request":0,"last_updated":"2024-10-24T11:54:59.000Z","slug":"models/claude-3-haiku"},"content":"\nClaude 3 Haiku is Anthropic's fastest and most compact model for\nnear-instant responsiveness. Quick and accurate targeted performance.\n\nSee the launch announcement and benchmark results [here](https://www.anthropic.com/news/claude-3-haiku)\n\n#multimodal\n\n"},{"lang":"en","group":"models","slug":"models/claude-3-opus","frontmatter":{"title":"Anthropic: Claude 3 Opus","meta_title":"Anthropic: Claude 3 Opus","description":"Anthropic: Claude 3 Opus","date":"2024-03-05T00:00:00.000Z","image":"https://img.rifx.online/logo/anthropic.svg","categories":["text image 2 text"],"author":"anthropic","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"claude-3-opus","context":200000,"input":0.000015,"output":0.000075,"img":0.024,"request":0,"last_updated":"2024-11-07T09:45:35.000Z","slug":"models/claude-3-opus"},"content":"\nClaude 3 Opus is Anthropic's most powerful model for highly complex tasks. It boasts top-level performance, intelligence, fluency, and understanding.\n\nSee the launch announcement and benchmark results [here](https://www.anthropic.com/news/claude-3-family)\n\n#multimodal\n\n"},{"lang":"en","group":"models","slug":"models/claude-3-sonnet","frontmatter":{"title":"Anthropic: Claude 3 Sonnet","meta_title":"Anthropic: Claude 3 Sonnet","description":"Anthropic: Claude 3 Sonnet","date":"2024-03-05T00:00:00.000Z","image":"https://img.rifx.online/logo/anthropic.svg","categories":["text image 2 text"],"author":"anthropic","tags":["Programming","Technology","Machine Learning","Data Science","Chatbots"],"draft":false,"is_recommended":true,"id":"claude-3-sonnet","context":200000,"input":0.000003,"output":0.000015,"img":0.0048,"request":0,"last_updated":"2024-11-14T04:05:16.000Z","slug":"models/claude-3-sonnet"},"content":"\nClaude 3 Sonnet is an ideal balance of intelligence and speed for enterprise workloads. Maximum utility at a lower price, dependable, balanced for scaled deployments.\n\nSee the launch announcement and benchmark results [here](https://www.anthropic.com/news/claude-3-family)\n\n#multimodal\n\n"},{"lang":"en","group":"models","slug":"models/claude-35-haiku","frontmatter":{"title":"Anthropic: Claude 3.5 Haiku","meta_title":"Anthropic: Claude 3.5 Haiku","description":"Anthropic: Claude 3.5 Haiku","date":"2024-11-04T00:00:00.000Z","image":"https://img.rifx.online/logo/anthropic.svg","categories":["text 2 text"],"author":"anthropic","tags":["Programming","Chatbots","Data Science","Machine Learning","Natural Language Processing"],"draft":false,"id":"claude-3.5-haiku","context":200000,"input":0.000001,"output":0.000005,"img":0,"request":0,"last_updated":"2024-11-07T09:46:02.000Z","slug":"models/claude-35-haiku"},"content":"\nClaude 3.5 Haiku features offers enhanced capabilities in speed, coding accuracy, and tool use. Engineered to excel in real-time applications, it delivers quick response times that are essential for dynamic tasks such as chat interactions and immediate coding suggestions.\n\nThis makes it highly suitable for environments that demand both speed and precision, such as software development, customer service bots, and data management systems.\n\nThis model is currently pointing to [Claude 3.5 Haiku (2024-10-22)](/anthropic/claude-3-5-haiku-20241022).\n\n"},{"lang":"en","group":"models","slug":"models/claude-35-sonnet","frontmatter":{"title":"Anthropic: Claude 3.5 Sonnet","meta_title":"Anthropic: Claude 3.5 Sonnet","description":"Anthropic: Claude 3.5 Sonnet","date":"2024-06-20T00:00:00.000Z","image":"https://img.rifx.online/logo/anthropic.svg","categories":["text image 2 text"],"author":"anthropic","tags":["Programming","Data Science","Computer Vision","Chatbots","Autonomous Systems"],"draft":false,"id":"claude-3.5-sonnet","context":200000,"input":0.000003,"output":0.000015,"img":0.0048,"request":0,"last_updated":"2024-10-24T11:45:46.000Z","slug":"models/claude-35-sonnet"},"content":"\nClaude 3.5 Sonnet delivers better-than-Opus capabilities, faster-than-Sonnet speeds, at the same Sonnet prices. Sonnet is particularly good at:\n\n- Coding: Autonomously writes, edits, and runs code with reasoning and troubleshooting\n- Data science: Augments human data science expertise; navigates unstructured data while using multiple tools for insights\n- Visual processing: excelling at interpreting charts, graphs, and images, accurately transcribing text to derive insights beyond just the text alone\n- Agentic tasks: exceptional tool use, making it great at agentic tasks (i.e. complex, multi-step problem solving tasks that require engaging with other systems)\n\n#multimodal\n\n"},{"lang":"en","group":"models","slug":"models/command-r-plus","frontmatter":{"title":"Cohere: Command R+ (08-2024)","meta_title":"Cohere: Command R+ (08-2024)","description":"Cohere: Command R+ (08-2024)","date":"2024-08-30T00:00:00.000Z","image":"https://img.rifx.online/logo/cohere.svg","categories":["text 2 text"],"author":"cohere","tags":["Technology","Programming","Machine Learning","Generative AI","Ethics"],"draft":false,"id":"command-r-plus","context":128000,"input":0.000002375,"output":0.0000095,"img":0,"request":0,"last_updated":"2024-11-07T09:33:44.000Z","slug":"models/command-r-plus"},"content":"\ncommand-r-plus-08-2024 is an update of the [Command R+](/cohere/command-r-plus) with roughly 50% higher throughput and 25% lower latencies as compared to the previous Command R+ version, while keeping the hardware footprint the same.\n\nRead the launch post [here](https://docs.cohere.com/changelog/command-gets-refreshed).\n\nUse of this model is subject to Cohere's [Acceptable Use Policy](https://docs.cohere.com/docs/c4ai-acceptable-use-policy).\n\n"},{"lang":"en","group":"models","slug":"models/command-r","frontmatter":{"title":"Cohere: Command R (08-2024)","meta_title":"Cohere: Command R (08-2024)","description":"Cohere: Command R (08-2024)","date":"2024-08-30T00:00:00.000Z","image":"https://img.rifx.online/logo/cohere.svg","categories":["text 2 text"],"author":"cohere","tags":["Programming","Natural Language Processing","Generative AI","Machine Learning","Data Science"],"draft":false,"id":"command-r","context":128000,"input":1.425e-7,"output":5.7e-7,"img":0,"request":0,"last_updated":"2024-11-07T09:33:55.000Z","slug":"models/command-r"},"content":"\ncommand-r-08-2024 is an update of the [Command R](/cohere/command-r) with improved performance for multilingual retrieval-augmented generation (RAG) and tool use. More broadly, it is better at math, code and reasoning and is competitive with the previous version of the larger Command R+ model.\n\nRead the launch post [here](https://docs.cohere.com/changelog/command-gets-refreshed).\n\nUse of this model is subject to Cohere's [Acceptable Use Policy](https://docs.cohere.com/docs/c4ai-acceptable-use-policy).\n\n"},{"lang":"en","group":"models","slug":"models/deepseek-chat","frontmatter":{"title":"DeepSeek V2.5","meta_title":"DeepSeek V2.5","description":"DeepSeek V2.5","date":"2024-05-14T00:00:00.000Z","image":"https://img.rifx.online/logo/deepseek.svg","categories":["text 2 text"],"author":"deepseek","tags":["Programming","Natural Language Processing","Machine Learning","Data Science","Chatbots"],"draft":false,"id":"deepseek-chat","context":128000,"input":1.4e-7,"output":2.8e-7,"img":0,"request":0,"last_updated":"2024-11-01T04:19:11.000Z","slug":"models/deepseek-chat"},"content":"\nDeepSeek-V2.5 is an upgraded version that combines DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct. The new model integrates the general and coding abilities of the two previous versions.\n\nDeepSeek-V2 Chat is a conversational finetune of DeepSeek-V2, a Mixture-of-Experts (MoE) language model. It comprises 236B total parameters, of which 21B are activated for each token.\n\nCompared with DeepSeek 67B, DeepSeek-V2 achieves stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times.\n\nDeepSeek-V2 achieves remarkable performance on both standard benchmarks and open-ended generation evaluations.\n\n"},{"lang":"en","group":"models","slug":"models/dolphin-mixtral-8x22b","frontmatter":{"title":"Dolphin 2.9.2 Mixtral 8x22B ðŸ¬","meta_title":"Dolphin 2.9.2 Mixtral 8x22B ðŸ¬","description":"Dolphin 2.9.2 Mixtral 8x22B ðŸ¬","date":"2024-06-08T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"cognitivecomputations","tags":["Natural Language Processing","Generative AI","Chatbots","Roleplay","Ethics"],"draft":false,"id":"dolphin-mixtral-8x22b","context":65536,"input":9e-7,"output":9e-7,"img":0,"request":0,"last_updated":"2024-11-04T12:49:50.000Z","slug":"models/dolphin-mixtral-8x22b"},"content":"\nDolphin 2.9 is designed for instruction following, conversational, and coding. This model is a finetune of [Mixtral 8x22B Instruct](/mistralai/mixtral-8x22b-instruct). It features a 64k context length and was fine-tuned with a 16k sequence length using ChatML templates.\n\nThis model is a successor to [Dolphin Mixtral 8x7B](/cognitivecomputations/dolphin-mixtral-8x7b).\n\nThe model is uncensored and is stripped of alignment and bias. It requires an external alignment layer for ethical use. Users are cautioned to use this highly compliant model responsibly, as detailed in a blog post about uncensored models at [erichartford.com/uncensored-models](https://erichartford.com/uncensored-models).\n\n#moe #uncensored\n\n"},{"lang":"en","group":"models","slug":"models/dolphin-mixtral-8x7b","frontmatter":{"title":"Dolphin 2.6 Mixtral 8x7B ðŸ¬","meta_title":"Dolphin 2.6 Mixtral 8x7B ðŸ¬","description":"Dolphin 2.6 Mixtral 8x7B ðŸ¬","date":"2023-12-21T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"cognitivecomputations","tags":["Programming","Natural Language Processing","Generative AI","Ethics","Chatbots"],"draft":false,"id":"dolphin-mixtral-8x7b","context":32768,"input":5e-7,"output":5e-7,"img":0,"request":0,"last_updated":"2024-11-04T12:52:28.000Z","slug":"models/dolphin-mixtral-8x7b"},"content":"\nThis is a 16k context fine-tune of [Mixtral-8x7b](/mistralai/mixtral-8x7b). It excels in coding tasks due to extensive training with coding data and is known for its obedience, although it lacks DPO tuning.\n\nThe model is uncensored and is stripped of alignment and bias. It requires an external alignment layer for ethical use. Users are cautioned to use this highly compliant model responsibly, as detailed in a blog post about uncensored models at [erichartford.com/uncensored-models](https://erichartford.com/uncensored-models).\n\n#moe #uncensored\n\n"},{"lang":"en","group":"models","slug":"models/eva-qwen-25-14b","frontmatter":{"title":"EVA Qwen2.5 14B","meta_title":"EVA Qwen2.5 14B","description":"EVA Qwen2.5 14B","date":"2024-09-30T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"eva-unit-01","tags":["Roleplay","Programming","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"is_recommended":false,"id":"eva-qwen-2.5-14b","context":32768,"input":2.5e-7,"output":5e-7,"img":0,"request":0,"last_updated":"2024-11-14T08:18:57.000Z","slug":"models/eva-qwen-25-14b"},"content":"\nA model specializing in RP and creative writing, this model is based on Qwen2.5-14B, fine-tuned with a mixture of synthetic and natural data.\n\nIt is trained on 1.5M tokens of role-play data, and fine-tuned on 1.5M tokens of synthetic data.\n\n"},{"lang":"en","group":"models","slug":"models/eva-qwen-25-32b","frontmatter":{"title":"Eva Qwen2.5 32B","meta_title":"Eva Qwen2.5 32B","description":"Eva Qwen2.5 32B","date":"2024-11-08T22:27:27.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"eva-unit-01","tags":["Roleplay","Programming","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"is_recommended":true,"id":"eva-qwen-2.5-32b","context":32000,"input":5e-7,"output":5e-7,"img":0,"request":0,"last_updated":"2024-11-14T08:18:50.000Z","slug":"models/eva-qwen-25-32b"},"content":"\nA roleplaying/storywriting specialist model, full-parameter finetune of Qwen2.5-32B on mixture of synthetic and natural data.\n\nIt uses Celeste 70B 0.1 data mixture, greatly expanding it to improve versatility, creativity and \"flavor\" of the resulting model.\n\n"},{"lang":"en","group":"models","slug":"models/gemini-flash-15-8b-exp","frontmatter":{"title":"Google: Gemini Flash 8B 1.5 Experimental","meta_title":"Google: Gemini Flash 8B 1.5 Experimental","description":"Google: Gemini Flash 8B 1.5 Experimental","date":"2024-08-28T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text image 2 text"],"author":"google","tags":["Technology","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"gemini-flash-1.5-8b-exp","context":1000000,"input":0,"output":0,"img":0,"request":0,"last_updated":"2024-11-11T03:14:22.000Z","slug":"models/gemini-flash-15-8b-exp"},"content":"\nGemini 1.5 Flash 8B Experimental is an experimental, 8B parameter version of the [Gemini 1.5 Flash](/google/gemini-flash-1.5) model.\n\nUsage of Gemini is subject to Google's [Gemini Terms of Use](https://ai.google.dev/terms).\n\n#multimodal\n\nNote: This model is currently experimental and not suitable for production use-cases, and may be heavily rate-limited.\n\n"},{"lang":"en","group":"models","slug":"models/gemini-flash-15-8b","frontmatter":{"title":"Google: Gemini 1.5 Flash-8B","meta_title":"Google: Gemini 1.5 Flash-8B","description":"Google: Gemini 1.5 Flash-8B","date":"2024-10-03T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text image 2 text"],"author":"google","tags":["Programming","Natural Language Processing","Chatbots","Translation","Technology/Web"],"draft":false,"id":"gemini-flash-1.5-8b","context":1000000,"input":3.75e-8,"output":1.5e-7,"img":0,"request":0,"last_updated":"2024-10-03T00:00:00.000Z","slug":"models/gemini-flash-15-8b"},"content":"\nGemini 1.5 Flash-8B is optimized for speed and efficiency, offering enhanced performance in small prompt tasks like chat, transcription, and translation. With reduced latency, it is highly effective for real-time and large-scale operations. This model focuses on cost-effective solutions while maintaining high-quality results.\n\n[Click here to learn more about this model](https://developers.googleblog.com/en/gemini-15-flash-8b-is-now-generally-available-for-use/).\n\nUsage of Gemini is subject to Google's [Gemini Terms of Use](https://ai.google.dev/terms).\n\n"},{"lang":"en","group":"models","slug":"models/gemini-flash-15","frontmatter":{"title":"Google: Gemini Flash 1.5","meta_title":"Google: Gemini Flash 1.5","description":"Google: Gemini Flash 1.5","date":"2024-05-14T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text image 2 text"],"author":"google","tags":["Programming","Machine Learning","Natural Language Processing","Computer Vision","Chatbots"],"draft":false,"is_recommended":true,"id":"gemini-flash-1.5","context":1000000,"input":7.5e-8,"output":3e-7,"img":0.00004,"request":0,"last_updated":"2024-11-14T08:23:12.000Z","slug":"models/gemini-flash-15"},"content":"\nGemini 1.5 Flash is a foundation model that performs well at a variety of multimodal tasks such as visual understanding, classification, summarization, and creating content from image, audio and video. It's adept at processing visual and text inputs such as photographs, documents, infographics, and screenshots.\n\nGemini 1.5 Flash is designed for high-volume, high-frequency tasks where cost and latency matter. On most common tasks, Flash achieves comparable quality to other Gemini Pro models at a significantly reduced cost. Flash is well-suited for applications like chat assistants and on-demand content generation where speed and scale matter.\n\nUsage of Gemini is subject to Google's [Gemini Terms of Use](https://ai.google.dev/terms).\n\n#multimodal\n\n"},{"lang":"en","group":"models","slug":"models/gemini-pro-15","frontmatter":{"title":"Google: Gemini Pro 1.5","meta_title":"Google: Gemini Pro 1.5","description":"Google: Gemini Pro 1.5","date":"2024-04-09T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text image 2 text"],"author":"google","tags":["Programming","Natural Language Processing","Machine Learning","Generative AI","Chatbots"],"draft":false,"id":"gemini-pro-1.5","context":2000000,"input":0.00000125,"output":0.000005,"img":0.00263,"request":0,"last_updated":"2024-04-09T00:00:00.000Z","slug":"models/gemini-pro-15"},"content":"\nGoogle's latest multimodal model, supporting image and video in text or chat prompts.\n\nOptimized for language tasks including:\n\n- Code generation\n- Text generation\n- Text editing\n- Problem solving\n- Recommendations\n- Information extraction\n- Data extraction or generation\n- AI agents\n\nUsage of Gemini is subject to Google's [Gemini Terms of Use](https://ai.google.dev/terms).\n\n#multimodal\n\n"},{"lang":"en","group":"models","slug":"models/gemini-pro-vision","frontmatter":{"title":"Google: Gemini Pro Vision 1.0","meta_title":"Google: Gemini Pro Vision 1.0","description":"Google: Gemini Pro Vision 1.0","date":"2023-12-13T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text image 2 text"],"author":"google","tags":["Programming","Machine Learning","Natural Language Processing","Computer Vision","Generative AI"],"draft":false,"id":"gemini-pro-vision","context":16384,"input":5e-7,"output":0.0000015,"img":0.0025,"request":0,"last_updated":"2024-11-11T03:15:08.000Z","slug":"models/gemini-pro-vision"},"content":"\nGoogle's flagship multimodal model, supporting image and video in text or chat prompts for a text or code response.\n\nSee the benchmarks and prompting guidelines from [Deepmind](https://deepmind.google/technologies/gemini/).\n\nUsage of Gemini is subject to Google's [Gemini Terms of Use](https://ai.google.dev/terms).\n\n#multimodal\n\n"},{"lang":"en","group":"models","slug":"models/gemma-2-27b-it","frontmatter":{"title":"Google: Gemma 2 27B","meta_title":"Google: Gemma 2 27B","description":"Google: Gemma 2 27B","date":"2024-07-13T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text 2 text"],"author":"google","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"gemma-2-27b-it","context":8192,"input":2.7e-7,"output":2.7e-7,"img":0,"request":0,"last_updated":"2024-11-11T03:14:49.000Z","slug":"models/gemma-2-27b-it"},"content":"\nGemma 2 27B by Google is an open model built from the same research and technology used to create the [Gemini models](/models?q=gemini).\n\nGemma models are well-suited for a variety of text generation tasks, including question answering, summarization, and reasoning.\n\nSee the [launch announcement](https://blog.google/technology/developers/google-gemma-2/) for more details. Usage of Gemma is subject to Google's [Gemma Terms of Use](https://ai.google.dev/gemma/terms).\n\n"},{"lang":"en","group":"models","slug":"models/gemma-2-9b-it","frontmatter":{"title":"Google: Gemma 2 9B","meta_title":"Google: Gemma 2 9B","description":"Google: Gemma 2 9B","date":"2024-06-28T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text 2 text"],"author":"google","tags":["Programming","Natural Language Processing","Machine Learning","Data Science","Open Source"],"draft":false,"id":"gemma-2-9b-it","context":8192,"input":6e-8,"output":6e-8,"img":0,"request":0,"last_updated":"2024-11-11T03:14:49.000Z","slug":"models/gemma-2-9b-it"},"content":"\nGemma 2 9B by Google is an advanced, open-source language model that sets a new standard for efficiency and performance in its size class.\n\nDesigned for a wide variety of tasks, it empowers developers and researchers to build innovative applications, while maintaining accessibility, safety, and cost-effectiveness.\n\nSee the [launch announcement](https://blog.google/technology/developers/google-gemma-2/) for more details. Usage of Gemma is subject to Google's [Gemma Terms of Use](https://ai.google.dev/gemma/terms).\n\n"},{"lang":"en","group":"models","slug":"models/glm-4-air","frontmatter":{"title":"GLM-4 Air","meta_title":"GLM-4 Air","description":"GLM-4 Air","date":"2024-11-14T10:21:13.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"glm-4-air","tags":["Programming","Technology","Machine Learning","Data Science","Generative AI"],"draft":false,"is_recommended":false,"id":"glm-4-air","context":128000,"input":1.4e-7,"output":1.4e-7,"img":0,"request":0,"last_updated":"2024-11-14T12:36:34.000Z","slug":"models/glm-4-air"},"content":"\nNone\n\n"},{"lang":"en","group":"models","slug":"models/glm-4-airx","frontmatter":{"title":"GLM-4 AirX","meta_title":"GLM-4 AirX","description":"GLM-4 AirX","date":"2024-11-15T13:12:54.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"glm-4-airx","tags":["Technology","Machine Learning","Generative AI","Data Science","Chatbots"],"draft":false,"is_recommended":false,"id":"glm-4-airx","context":8000,"input":0.0000014,"output":0.0000014,"img":0,"request":0,"last_updated":"2024-11-15T23:49:16.000Z","slug":"models/glm-4-airx"},"content":"\n## Basic Information\n\nThe \"GLM-4-AIRX\" is an advanced large language model developed by experts in the field of artificial intelligence. It is renowned for its powerful natural language processing capabilities, enabling it to effectively understand and generate natural language text. This model leverages deep learning technologies, particularly the Transformer architecture, which is widely used in the NLP (Natural Language Processing) domain.\n\n## Technical Features\n\n### 1. Based on Transformer Architecture\n\nThe core of this model is built upon the Transformer architecture, a structure that employs attention mechanisms. Through this mechanism, \"GLM-4-AIRX\" can capture complex dependencies between any two positions within an input sequence, regardless of their distance, thereby enhancing efficiency and accuracy when handling language tasks.\n\n### 2. Pre-training and Fine-tuning\n\nThe model development involves a combination of pre-training and fine-tuning phases. During pre-training, it is trained on extensive and diverse text datasets to grasp fundamental language patterns. In the fine-tuning phase, parameters are adjusted according to specific tasks to improve performance on particular applications.\n\n### 3. Multilingual Support\n\n\"GLM-4-AIRX\" possesses multilingual capabilities achieved through pre-training on corpora containing various languages. This allows it to comprehend and generate text in multiple languages, meeting global user needs and facilitating cross-cultural communication.\n\n### 4. Scalability\n\nDesigned with scalability in mind, this model can be flexibly adjusted to handle larger datasets or more complex task requirements. This feature enables it to be widely applicable in ever-changing and expanding data environments.\n\n## Application Scenarios\n\nThe \"GLM-4-AIRX\" has broad applications across numerous fields including but not limited to:\n\n- **Text Generation**: Automatically creating articles, stories, or poetry.\n- **Text Classification**: Such as sentiment analysis or thematic categorization.\n- **Machine Translation**: Providing efficient and accurate cross-language translation.\n- **Question Answering Systems**: For building intelligent platforms that directly respond to user inquiries.\n- **Text Summarization**: Automatically extracting key information from documents into concise summaries.\n\n## Comparison with Similar Models\n\nCompared to other large language models, \"GLM-4-AIRX\" offers several advantages:\n\n- **Outstanding Performance**: Excels across multiple NLP tasks such as text generation and comprehension.\n- **High Processing Efficiency**: Optimized architectural design results in more efficient resource use and speed.\n- **Strong Flexibility**: Supports multiple languages and can adapt through extended features across different application scenarios.\n- **Task Adaptability**: Achieves optimization for specific task requirements through fine-tuning, enhancing practical value.\n\nIn summary, \"GLM-4-AIRX\" is a comprehensive and flexible large language model that holds potential for continued growth and innovation within today's rapidly evolving AI technology landscape.\n\nThis response uses available real-time data regarding GLM models supporting new functionalities like System Prompt , Function Call , Retrieval , Web_Search etc., alongside general knowledge about transformer-based architectures common in NLP advancements.\n\n"},{"lang":"en","group":"models","slug":"models/glm-4-flash","frontmatter":{"title":"glm-4-flash","meta_title":"glm-4-flash","description":"glm-4-flash","date":"2024-11-15T12:53:10.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"glm-4-flash","tags":["Generative AI","Machine Learning","Natural Language Processing","Technology","Chatbots"],"draft":false,"is_recommended":false,"id":"glm-4-flash","context":128000,"input":1e-8,"output":1e-8,"img":0,"request":0,"last_updated":"2024-11-15T23:22:37.000Z","slug":"models/glm-4-flash"},"content":"\næ™ºè°±å¿«é€Ÿç‰ˆ\n\n"},{"lang":"en","group":"models","slug":"models/glm-4-long","frontmatter":{"title":"glm-4-long","meta_title":"glm-4-long","description":"glm-4-long","date":"2024-11-15T12:53:01.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"glm-4-long","tags":["Technology","Machine Learning","Natural Language Processing","Data Science","Generative AI"],"draft":false,"is_recommended":false,"id":"glm-4-long","context":1000000,"input":1.4e-7,"output":1.4e-7,"img":0,"request":0,"last_updated":"2024-11-15T23:22:49.000Z","slug":"models/glm-4-long"},"content":"\næ™ºè°±ç™¾ä¸‡ä¸Šä¸‹æ–‡\n\n"},{"lang":"en","group":"models","slug":"models/glm-4-plus","frontmatter":{"title":"glm-4-plus","meta_title":"glm-4-plus","description":"glm-4-plus","date":"2024-11-15T12:53:05.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"glm-4-plus","tags":["Generative AI","Machine Learning","Natural Language Processing","Technology","Chatbots"],"draft":false,"is_recommended":false,"id":"glm-4-plus","context":128000,"input":0.000007,"output":0.000007,"img":0,"request":0,"last_updated":"2024-11-15T23:22:43.000Z","slug":"models/glm-4-plus"},"content":"\næ™ºè°±æ——èˆ°ç‰ˆ\n\n"},{"lang":"en","group":"models","slug":"models/glm-4","frontmatter":{"title":"GLM-4","meta_title":"GLM-4","description":"GLM-4","date":"2024-11-15T13:12:41.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"glm-4","tags":["Generative AI","Machine Learning","Natural Language Processing","Technology","Chatbots"],"draft":false,"is_recommended":false,"id":"glm-4","context":128000,"input":0.000014,"output":0.000014,"img":0,"request":0,"last_updated":"2024-11-15T23:23:43.000Z","slug":"models/glm-4"},"content":"\næ™ºè°±æœ€å¼ºç‰ˆ\n\n"},{"lang":"en","group":"models","slug":"models/glm-4v-plus","frontmatter":{"title":"glm-4v-plus","meta_title":"glm-4v-plus","description":"glm-4v-plus","date":"2024-11-15T12:53:06.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"glm-4v-plus","tags":["Computer Vision","Technology","Machine Learning","Data Science","Generative AI"],"draft":false,"is_recommended":false,"id":"glm-4v-plus","context":32000,"input":0.0000014,"output":0.0000014,"img":0,"request":0,"last_updated":"2024-11-15T23:22:53.000Z","slug":"models/glm-4v-plus"},"content":"\næ™ºè°±æœ€æ–°å›¾åƒè¯†åˆ«\n\n"},{"lang":"en","group":"models","slug":"models/gpt-35-turbo-instruct","frontmatter":{"title":"OpenAI: GPT-3.5 Turbo Instruct","meta_title":"OpenAI: GPT-3.5 Turbo Instruct","description":"OpenAI: GPT-3.5 Turbo Instruct","date":"2023-09-28T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"openai","tags":["Programming","Natural Language Processing","Generative AI","Chatbots","Technology/Web"],"draft":false,"id":"gpt-3.5-turbo-instruct","context":4095,"input":0.0000015,"output":0.000002,"img":0,"request":0,"last_updated":"2023-09-28T00:00:00.000Z","slug":"models/gpt-35-turbo-instruct"},"content":"\nThis model is a variant of GPT-3.5 Turbo tuned for instructional prompts and omitting chat-related optimizations. Training data: up to Sep 2021.\n\n"},{"lang":"en","group":"models","slug":"models/gpt-4o-mini","frontmatter":{"title":"OpenAI: GPT-4o-mini","meta_title":"OpenAI: GPT-4o-mini","description":"OpenAI: GPT-4o-mini","date":"2024-07-18T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text image 2 text"],"author":"openai","tags":["Programming","Technology","Programming/Scripting","Technology/Web"],"draft":false,"is_recommended":true,"id":"gpt-4o-mini","context":128000,"input":1.5e-7,"output":6e-7,"img":0.007225,"request":0,"last_updated":"2024-11-14T05:09:26.000Z","slug":"models/gpt-4o-mini"},"content":"\nGPT-4o mini is OpenAI's newest model after [GPT-4 Omni](/openai/gpt-4o), supporting both text and image inputs with text outputs.\n\nAs their most advanced small model, it is many multiples more affordable than other recent frontier models, and more than 60% cheaper than [GPT-3.5 Turbo](/openai/gpt-3.5-turbo). It maintains SOTA intelligence, while being significantly more cost-effective.\n\nGPT-4o mini achieves an 82% score on MMLU and presently ranks higher than GPT-4 on chat preferences [common leaderboards](https://arena.lmsys.org/).\n\nCheck out the [launch announcement](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/) to learn more.\n\n"},{"lang":"en","group":"models","slug":"models/gpt-4o","frontmatter":{"title":"OpenAI: GPT-4o","meta_title":"OpenAI: GPT-4o","description":"OpenAI: GPT-4o","date":"2024-05-13T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text image 2 text"],"author":"openai","tags":["Programming","Natural Language Processing","Machine Learning","Generative AI","Computer Vision"],"draft":false,"id":"gpt-4o","context":128000,"input":0.0000025,"output":0.00001,"img":0.0036125,"request":0,"last_updated":"2024-05-13T00:00:00.000Z","slug":"models/gpt-4o"},"content":"\nGPT-4o (\"o\" for \"omni\") is OpenAI's latest AI model, supporting both text and image inputs with text outputs. It maintains the intelligence level of [GPT-4 Turbo](/openai/gpt-4-turbo) while being twice as fast and 50% more cost-effective. GPT-4o also offers improved performance in processing non-English languages and enhanced visual capabilities.\n\nFor benchmarking against other models, it was briefly called [\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209)\n\n"},{"lang":"en","group":"models","slug":"models/grok-beta","frontmatter":{"title":"xAI: Grok Beta","meta_title":"xAI: Grok Beta","description":"xAI: Grok Beta","date":"2024-10-20T00:00:00.000Z","image":"https://img.rifx.online/logo/xai.svg","categories":["text 2 text"],"author":"x-ai","tags":["Natural Language Processing","Machine Learning","Generative AI","Chatbots","Data Science"],"draft":false,"id":"grok-beta","context":131072,"input":0.000005,"output":0.000015,"img":0,"request":0,"last_updated":"2024-11-07T09:32:49.000Z","slug":"models/grok-beta"},"content":"\nGrok Beta is xAI's experimental language model with state-of-the-art reasoning capabilities, best for complex and multi-step use cases.\n\nIt is the successor of [Grok 2](https://x.ai/blog/grok-2) with enhanced context length.\n\n"},{"lang":"en","group":"models","slug":"models/hermes-3-llama-31-405b","frontmatter":{"title":"Nous: Hermes 3 405B Instruct","meta_title":"Nous: Hermes 3 405B Instruct","description":"Nous: Hermes 3 405B Instruct","date":"2024-08-16T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"nousresearch","tags":["Programming","Natural Language Processing","Machine Learning","Generative AI","Chatbots"],"draft":false,"id":"hermes-3-llama-3.1-405b","context":131072,"input":0.00000179,"output":0.00000249,"img":0,"request":0,"last_updated":"2024-11-11T03:16:40.000Z","slug":"models/hermes-3-llama-31-405b"},"content":"\nHermes 3 is a generalist language model with many improvements over Hermes 2, including advanced agentic capabilities, much better roleplaying, reasoning, multi-turn conversation, long context coherence, and improvements across the board.\n\nHermes 3 405B is a frontier-level, full-parameter finetune of the Llama-3.1 405B foundation model, focused on aligning LLMs to the user, with powerful steering capabilities and control given to the end user.\n\nThe Hermes 3 series builds and expands on the Hermes 2 set of capabilities, including more powerful and reliable function calling and structured output capabilities, generalist assistant capabilities, and improved code generation skills.\n\nHermes 3 is competitive, if not superior, to Llama-3.1 Instruct models at general capabilities, with varying strengths and weaknesses attributable between the two.\n\n"},{"lang":"en","group":"models","slug":"models/hermes-3-llama-31-70b","frontmatter":{"title":"Nous: Hermes 3 70B Instruct","meta_title":"Nous: Hermes 3 70B Instruct","description":"Nous: Hermes 3 70B Instruct","date":"2024-08-18T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"nousresearch","tags":["Natural Language Processing","Machine Learning","Generative AI","Chatbots","Programming"],"draft":false,"id":"hermes-3-llama-3.1-70b","context":131072,"input":4e-7,"output":4e-7,"img":0,"request":0,"last_updated":"2024-11-11T03:16:38.000Z","slug":"models/hermes-3-llama-31-70b"},"content":"\nHermes 3 is a generalist language model with many improvements over [Hermes 2](/nousresearch/nous-hermes-2-mistral-7b-dpo), including advanced agentic capabilities, much better roleplaying, reasoning, multi-turn conversation, long context coherence, and improvements across the board.\n\nHermes 3 70B is a competitive, if not superior finetune of the [Llama-3.1 70B foundation model](/meta-llama/llama-3.1-70b-instruct), focused on aligning LLMs to the user, with powerful steering capabilities and control given to the end user.\n\nThe Hermes 3 series builds and expands on the Hermes 2 set of capabilities, including more powerful and reliable function calling and structured output capabilities, generalist assistant capabilities, and improved code generation skills.\n\n"},{"lang":"en","group":"models","slug":"models/inflection-3-pi","frontmatter":{"title":"Inflection: Inflection 3 Pi","meta_title":"Inflection: Inflection 3 Pi","description":"Inflection: Inflection 3 Pi","date":"2024-10-11T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"inflection","tags":["Chatbots","Roleplay","Emotional Intelligence","Customer Support","Safety"],"draft":false,"id":"inflection-3-pi","context":8000,"input":0.0000025,"output":0.00001,"img":0,"request":0,"last_updated":"2024-11-07T10:11:48.000Z","slug":"models/inflection-3-pi"},"content":"\nInflection 3 Pi powers Inflection's [Pi](https://pi.ai) chatbot, including backstory, emotional intelligence, productivity, and safety. It excels in scenarios like customer support, roleplay, and emotional intelligence.\n\n"},{"lang":"en","group":"models","slug":"models/inflection-3-productivity","frontmatter":{"title":"Inflection: Inflection 3 Productivity","meta_title":"Inflection: Inflection 3 Productivity","description":"Inflection: Inflection 3 Productivity","date":"2024-10-11T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"inflection","tags":["Programming","Technology","Chatbots","Generative AI","Data Science"],"draft":false,"id":"inflection-3-productivity","context":8000,"input":0.0000025,"output":0.00001,"img":0,"request":0,"last_updated":"2024-11-07T10:11:56.000Z","slug":"models/inflection-3-productivity"},"content":"\nInflection 3 Productivity is optimized for following instructions. It is better for tasks requiring JSON output or precise adherence to provided guidelines\n\nFor emotional intelligence similar to Pi, see [Inflect 3 Pi](/inflection/inflection-3-pi)\n\nSee [Inflection's announcement](https://inflection.ai/blog/enterprise) for more details.\n\n"},{"lang":"en","group":"models","slug":"models/jamba-1-5-large","frontmatter":{"title":"AI21: Jamba 1.5 Large","meta_title":"AI21: Jamba 1.5 Large","description":"AI21: Jamba 1.5 Large","date":"2024-08-23T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"ai21","tags":["Programming","Technology","Machine Learning","Data Science","Generative AI"],"draft":false,"id":"jamba-1-5-large","context":256000,"input":0.000002,"output":0.000008,"img":0,"request":0,"last_updated":"2024-11-11T03:11:21.000Z","slug":"models/jamba-1-5-large"},"content":"\nJamba 1.5 Large is part of AI21's new family of open models, offering superior speed, efficiency, and quality.\n\nIt features a 256K effective context window, the longest among open models, enabling improved performance on tasks like document summarization and analysis.\n\nBuilt on a novel SSM-Transformer architecture, it outperforms larger models like Llama 3.1 70B on benchmarks while maintaining resource efficiency.\n\nRead their [announcement](https://www.ai21.com/blog/announcing-jamba-model-family) to learn more.\n\n"},{"lang":"en","group":"models","slug":"models/jamba-1-5-mini","frontmatter":{"title":"AI21: Jamba 1.5 Mini","meta_title":"AI21: Jamba 1.5 Mini","description":"AI21: Jamba 1.5 Mini","date":"2024-08-23T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"ai21","tags":["Programming","Technology","Machine Learning","Natural Language Processing","Data Science"],"draft":false,"id":"jamba-1-5-mini","context":256000,"input":2e-7,"output":4e-7,"img":0,"request":0,"last_updated":"2024-11-11T03:11:12.000Z","slug":"models/jamba-1-5-mini"},"content":"\nJamba 1.5 Mini is the world's first production-grade Mamba-based model, combining SSM and Transformer architectures for a 256K context window and high efficiency.\n\nIt works with 9 languages and can handle various writing and analysis tasks as well as or better than similar small models.\n\nThis model uses less computer memory and works faster with longer texts than previous designs.\n\nRead their [announcement](https://www.ai21.com/blog/announcing-jamba-model-family) to learn more.\n\n"},{"lang":"en","group":"models","slug":"models/l3-lunaris-8b","frontmatter":{"title":"Llama 3 8B Lunaris","meta_title":"Llama 3 8B Lunaris","description":"Llama 3 8B Lunaris","date":"2024-08-13T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"sao10k","tags":["Roleplay","Programming","Machine Learning","Natural Language Processing","Chatbots"],"draft":false,"id":"l3-lunaris-8b","context":8192,"input":0.000002,"output":0.000002,"img":0,"request":0,"last_updated":"2024-11-11T03:12:13.000Z","slug":"models/l3-lunaris-8b"},"content":"\nLunaris 8B is a versatile generalist and roleplaying model based on Llama 3. It's a strategic merge of multiple models, designed to balance creativity with improved logic and general knowledge.\n\nCreated by [Sao10k](https://huggingface.co/Sao10k), this model aims to offer an improved experience over Stheno v3.2, with enhanced creativity and logical reasoning.\n\nFor best results, use with Llama 3 Instruct context template, temperature 1.4, and min_p 0.1.\n\n"},{"lang":"en","group":"models","slug":"models/l31-euryale-70b","frontmatter":{"title":"Llama 3.1 Euryale 70B v2.2","meta_title":"Llama 3.1 Euryale 70B v2.2","description":"Llama 3.1 Euryale 70B v2.2","date":"2024-08-28T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"sao10k","tags":["Roleplay","Generative AI","Chatbots","Natural Language Processing","Technology/Web"],"draft":false,"id":"l3.1-euryale-70b","context":8192,"input":3.5e-7,"output":4e-7,"img":0,"request":0,"last_updated":"2024-11-11T03:12:50.000Z","slug":"models/l31-euryale-70b"},"content":"\nEuryale L3.1 70B v2.2 is a model focused on creative roleplay from [Sao10k](https://ko-fi.com/sao10k). It is the successor of [Euryale L3 70B v2.1](/sao10k/l3-euryale-70b).\n\n"},{"lang":"en","group":"models","slug":"models/lfm-40b","frontmatter":{"title":"Liquid: LFM 40B MoE","meta_title":"Liquid: LFM 40B MoE","description":"Liquid: LFM 40B MoE","date":"2024-09-30T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"liquid","tags":["Machine Learning","Natural Language Processing","Data Science","Generative AI","Computer Vision"],"draft":false,"is_recommended":true,"id":"lfm-40b","context":32768,"input":0.000001,"output":0.000002,"img":0,"request":0,"last_updated":"2024-11-14T05:10:16.000Z","slug":"models/lfm-40b"},"content":"\nLiquid's 40.3B Mixture of Experts (MoE) model. Liquid Foundation Models (LFMs) are large neural networks built with computational units rooted in dynamic systems.\n\nLFMs are general-purpose AI models that can be used to model any kind of sequential data, including video, audio, text, time series, and signals.\n\nSee the [launch announcement](https://www.liquid.ai/liquid-foundation-models) for benchmarks and more info.\n\n"},{"lang":"en","group":"models","slug":"models/liquid-lfm-40b:free","frontmatter":{"title":"Liquid: LFM 40B MoE (free)","meta_title":"Liquid: LFM 40B MoE (free)","description":"Liquid: LFM 40B MoE (free)","date":"2024-09-30T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"liquid","tags":["Generative AI","Machine Learning","Natural Language Processing","Data Science","Technology/Web"],"draft":false,"id":"liquid/lfm-40b:free","context":8192,"input":0,"output":0,"img":0,"request":0,"last_updated":"2024-11-07T00:17:57.000Z","slug":"models/liquid-lfm-40b:free"},"content":"\nLiquid's 40.3B Mixture of Experts (MoE) model. Liquid Foundation Models (LFMs) are large neural networks built with computational units rooted in dynamic systems.\n\nLFMs are general-purpose AI models that can be used to model any kind of sequential data, including video, audio, text, time series, and signals.\n\nSee the [launch announcement](https://www.liquid.ai/liquid-foundation-models) for benchmarks and more info.\n\n_These are free, rate-limited endpoints for [LFM 40B MoE](/liquid/lfm-40b). Outputs may be cached. Read about rate limits [here](/docs/limits)._\n\n"},{"lang":"en","group":"models","slug":"models/llama-31-70b-instruct","frontmatter":{"title":"Meta: Llama 3.1 70B Instruct","meta_title":"Meta: Llama 3.1 70B Instruct","description":"Meta: Llama 3.1 70B Instruct","date":"2024-07-23T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text 2 text"],"author":"meta-llama","tags":["Programming","Machine Learning","Natural Language Processing","Chatbots","Ethics"],"draft":false,"id":"llama-3.1-70b-instruct","context":131072,"input":3e-7,"output":3e-7,"img":0,"request":0,"last_updated":"2024-10-28T13:38:49.000Z","slug":"models/llama-31-70b-instruct"},"content":"\nMeta's latest class of model (Llama 3.1) launched with a variety of sizes & flavors. This 70B instruct-tuned version is optimized for high quality dialogue usecases.\n\nIt has demonstrated strong performance compared to leading closed-source models in human evaluations.\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).\n\n"},{"lang":"en","group":"models","slug":"models/llama-31-8b-instruct","frontmatter":{"title":"Meta: Llama 3.1 8B Instruct","meta_title":"Meta: Llama 3.1 8B Instruct","description":"Meta: Llama 3.1 8B Instruct","date":"2024-07-23T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text 2 text"],"author":"meta-llama","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Ethics"],"draft":false,"id":"llama-3.1-8b-instruct","context":131072,"input":5.5e-8,"output":5.5e-8,"img":0,"request":0,"last_updated":"2024-10-31T23:27:09.000Z","slug":"models/llama-31-8b-instruct"},"content":"\nMeta's latest class of model (Llama 3.1) launched with a variety of sizes & flavors. This 8B instruct-tuned version is fast and efficient.\n\nIt has demonstrated strong performance compared to leading closed-source models in human evaluations.\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).\n\n"},{"lang":"en","group":"models","slug":"models/llama-31-lumimaid-70b","frontmatter":{"title":"Lumimaid v0.2 70B","meta_title":"Lumimaid v0.2 70B","description":"Lumimaid v0.2 70B","date":"2024-10-22T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"neversleep","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Ethics"],"draft":false,"id":"llama-3.1-lumimaid-70b","context":131072,"input":0.000003375,"output":0.0000045,"img":0,"request":0,"last_updated":"2024-11-11T03:03:31.000Z","slug":"models/llama-31-lumimaid-70b"},"content":"\nLumimaid v0.2 70B is a finetune of [Llama 3.1 70B](/meta-llama/llama-3.1-70b-instruct) with a \"HUGE step up dataset wise\" compared to Lumimaid v0.1. Sloppy chats output were purged.\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/).\n\n"},{"lang":"en","group":"models","slug":"models/llama-31-lumimaid-8b","frontmatter":{"title":"Lumimaid v0.2 8B","meta_title":"Lumimaid v0.2 8B","description":"Lumimaid v0.2 8B","date":"2024-09-15T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"neversleep","tags":["Programming","Machine Learning","Natural Language Processing","Chatbots","Ethics"],"draft":false,"id":"llama-3.1-lumimaid-8b","context":131072,"input":1.875e-7,"output":0.000001125,"img":0,"request":0,"last_updated":"2024-11-11T03:10:19.000Z","slug":"models/llama-31-lumimaid-8b"},"content":"\nLumimaid v0.2 8B is a finetune of [Llama 3.1 8B](/meta-llama/llama-3.1-8b-instruct) with a \"HUGE step up dataset wise\" compared to Lumimaid v0.1. Sloppy chats output were purged.\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://llama.meta.com/llama3/use-policy/).\n\n"},{"lang":"en","group":"models","slug":"models/llama-31-nemotron-70b-instruct","frontmatter":{"title":"Nvidia: Llama 3.1 Nemotron 70B Instruct","meta_title":"Nvidia: Llama 3.1 Nemotron 70B Instruct","description":"Nvidia: Llama 3.1 Nemotron 70B Instruct","date":"2024-10-15T00:00:00.000Z","image":"https://img.rifx.online/logo/nvidia.svg","categories":["text 2 text"],"author":"nvidia","tags":["Programming","Natural Language Processing","Machine Learning","Generative AI","Ethics"],"draft":false,"id":"llama-3.1-nemotron-70b-instruct","context":131072,"input":3.5e-7,"output":4e-7,"img":0,"request":0,"last_updated":"2024-10-15T00:00:00.000Z","slug":"models/llama-31-nemotron-70b-instruct"},"content":"\nNVIDIA's Llama 3.1 Nemotron 70B is a language model designed for generating precise and useful responses. Leveraging [Llama 3.1 70B](/models/meta-llama/llama-3.1-70b-instruct) architecture and Reinforcement Learning from Human Feedback (RLHF), it excels in automatic alignment benchmarks. This model is tailored for applications requiring high accuracy in helpfulness and response generation, suitable for diverse user queries across multiple domains.\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).\n\n"},{"lang":"en","group":"models","slug":"models/llama-31-sonar-huge-128k-online","frontmatter":{"title":"Perplexity: Llama 3.1 Sonar 405B Online","meta_title":"Perplexity: Llama 3.1 Sonar 405B Online","description":"Perplexity: Llama 3.1 Sonar 405B Online","date":"2024-08-14T00:00:00.000Z","image":"https://img.rifx.online/logo/perplexity.svg","categories":["text 2 text"],"author":"perplexity","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"llama-3.1-sonar-huge-128k-online","context":127072,"input":0.000005,"output":0.000005,"img":0,"request":0.005,"last_updated":"2024-11-07T09:36:38.000Z","slug":"models/llama-31-sonar-huge-128k-online"},"content":"\nLlama 3.1 Sonar is Perplexity's latest model family. It surpasses their earlier Sonar models in cost-efficiency, speed, and performance. The model is built upon the Llama 3.1 405B and has internet access.\n\n"},{"lang":"en","group":"models","slug":"models/llama-31-sonar-large-128k-online","frontmatter":{"title":"Perplexity: Llama 3.1 Sonar 70B Online","meta_title":"Perplexity: Llama 3.1 Sonar 70B Online","description":"Perplexity: Llama 3.1 Sonar 70B Online","date":"2024-08-01T00:00:00.000Z","image":"https://img.rifx.online/logo/perplexity.svg","categories":["text 2 text"],"author":"perplexity","tags":["Programming","Machine Learning","Natural Language Processing","Chatbots","Generative AI"],"draft":false,"id":"llama-3.1-sonar-large-128k-online","context":127072,"input":0.000001,"output":0.000001,"img":0,"request":0.005,"last_updated":"2024-11-07T09:37:21.000Z","slug":"models/llama-31-sonar-large-128k-online"},"content":"\nLlama 3.1 Sonar is Perplexity's latest model family. It surpasses their earlier Sonar models in cost-efficiency, speed, and performance.\n\nThis is the online version of the [offline chat model](/perplexity/llama-3.1-sonar-large-128k-chat). It is focused on delivering helpful, up-to-date, and factual responses. #online\n\n"},{"lang":"en","group":"models","slug":"models/llama-31-sonar-small-128k-online","frontmatter":{"title":"Perplexity: Llama 3.1 Sonar 8B Online","meta_title":"Perplexity: Llama 3.1 Sonar 8B Online","description":"Perplexity: Llama 3.1 Sonar 8B Online","date":"2024-08-01T00:00:00.000Z","image":"https://img.rifx.online/logo/perplexity.svg","categories":["text 2 text"],"author":"perplexity","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"llama-3.1-sonar-small-128k-online","context":127072,"input":2e-7,"output":2e-7,"img":0,"request":0.005,"last_updated":"2024-11-07T09:38:09.000Z","slug":"models/llama-31-sonar-small-128k-online"},"content":"\nLlama 3.1 Sonar is Perplexity's latest model family. It surpasses their earlier Sonar models in cost-efficiency, speed, and performance.\n\nThis is the online version of the [offline chat model](/perplexity/llama-3.1-sonar-small-128k-chat). It is focused on delivering helpful, up-to-date, and factual responses. #online\n\n"},{"lang":"en","group":"models","slug":"models/llama-32-11b-vision-instruct","frontmatter":{"title":"Meta: Llama 3.2 11B Vision Instruct","meta_title":"Meta: Llama 3.2 11B Vision Instruct","description":"Meta: Llama 3.2 11B Vision Instruct","date":"2024-09-25T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text image 2 text"],"author":"meta-llama","tags":["Natural Language Processing","Computer Vision","Machine Learning","Generative AI","Data Science"],"draft":false,"is_recommended":true,"id":"llama-3.2-11b-vision-instruct","context":131072,"input":5.5e-8,"output":5.5e-8,"img":0.000079475,"request":0,"last_updated":"2024-11-14T05:10:41.000Z","slug":"models/llama-32-11b-vision-instruct"},"content":"\nLlama 3.2 11B Vision is a multimodal model with 11 billion parameters, designed to handle tasks combining visual and textual data. It excels in tasks such as image captioning and visual question answering, bridging the gap between language generation and visual reasoning. Pre-trained on a massive dataset of image-text pairs, it performs well in complex, high-accuracy image analysis.\n\nIts ability to integrate visual understanding with language processing makes it an ideal solution for industries requiring comprehensive visual-linguistic AI applications, such as content creation, AI-driven customer service, and research.\n\nClick here for the [original model card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md).\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).\n\n"},{"lang":"en","group":"models","slug":"models/llama-32-1b-instruct","frontmatter":{"title":"Meta: Llama 3.2 1B Instruct","meta_title":"Meta: Llama 3.2 1B Instruct","description":"Meta: Llama 3.2 1B Instruct","date":"2024-09-25T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text 2 text"],"author":"meta-llama","tags":["Natural Language Processing","Programming","Technology","Machine Learning","Generative AI"],"draft":false,"id":"llama-3.2-1b-instruct","context":131072,"input":1e-8,"output":2e-8,"img":0,"request":0,"last_updated":"2024-09-25T00:00:00.000Z","slug":"models/llama-32-1b-instruct"},"content":"\nLlama 3.2 1B is a 1-billion-parameter language model focused on efficiently performing natural language tasks, such as summarization, dialogue, and multilingual text analysis. Its smaller size allows it to operate efficiently in low-resource environments while maintaining strong task performance.\n\nSupporting eight core languages and fine-tunable for more, Llama 1.3B is ideal for businesses or developers seeking lightweight yet powerful AI solutions that can operate in diverse multilingual settings without the high computational demand of larger models.\n\nClick here for the [original model card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md).\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).\n\n"},{"lang":"en","group":"models","slug":"models/llama-32-3b-instruct","frontmatter":{"title":"Meta: Llama 3.2 3B Instruct","meta_title":"Meta: Llama 3.2 3B Instruct","description":"Meta: Llama 3.2 3B Instruct","date":"2024-09-25T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text 2 text"],"author":"meta-llama","tags":["Natural Language Processing","Machine Learning","Generative AI","Chatbots","Multilingual"],"draft":false,"id":"llama-3.2-3b-instruct","context":131072,"input":3e-8,"output":5e-8,"img":0,"request":0,"last_updated":"2024-11-11T03:09:59.000Z","slug":"models/llama-32-3b-instruct"},"content":"\nLlama 3.2 3B is a 3-billion-parameter multilingual large language model, optimized for advanced natural language processing tasks like dialogue generation, reasoning, and summarization. Designed with the latest transformer architecture, it supports eight languages, including English, Spanish, and Hindi, and is adaptable for additional languages.\n\nTrained on 9 trillion tokens, the Llama 3.2B model excels in instruction-following, complex reasoning, and tool use. Its balanced performance makes it ideal for applications needing accuracy and efficiency in text generation across multilingual settings.\n\nClick here for the [original model card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md).\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).\n\n"},{"lang":"en","group":"models","slug":"models/llama-32-90b-vision-instruct","frontmatter":{"title":"Meta: Llama 3.2 90B Vision Instruct","meta_title":"Meta: Llama 3.2 90B Vision Instruct","description":"Meta: Llama 3.2 90B Vision Instruct","date":"2024-09-25T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text image 2 text"],"author":"meta-llama","tags":["Natural Language Processing","Computer Vision","Machine Learning","Data Science","Generative AI"],"draft":false,"id":"llama-3.2-90b-vision-instruct","context":131072,"input":3.5e-7,"output":4e-7,"img":0.00050575,"request":0,"last_updated":"2024-09-25T00:00:00.000Z","slug":"models/llama-32-90b-vision-instruct"},"content":"\nThe Llama 90B Vision model is a top-tier, 90-billion-parameter multimodal model designed for the most challenging visual reasoning and language tasks. It offers unparalleled accuracy in image captioning, visual question answering, and advanced image-text comprehension. Pre-trained on vast multimodal datasets and fine-tuned with human feedback, the Llama 90B Vision is engineered to handle the most demanding image-based AI tasks.\n\nThis model is perfect for industries requiring cutting-edge multimodal AI capabilities, particularly those dealing with complex, real-time visual and textual analysis.\n\nClick here for the [original model card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md).\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).\n\n"},{"lang":"en","group":"models","slug":"models/lzlv-70b-fp16-hf","frontmatter":{"title":"lzlv 70B","meta_title":"lzlv 70B","description":"lzlv 70B","date":"2023-11-12T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"lizpreciatior","tags":["Roleplay","Programming","Machine Learning","Generative AI","Chatbots"],"draft":false,"id":"lzlv-70b-fp16-hf","context":4096,"input":3.5e-7,"output":4e-7,"img":0,"request":0,"last_updated":"2024-11-04T12:50:34.000Z","slug":"models/lzlv-70b-fp16-hf"},"content":"\nA Mythomax/MLewd_13B-style merge of selected 70B models.\nA multi-model merge of several LLaMA2 70B finetunes for roleplaying and creative work. The goal was to create a model that combines creativity with intelligence for an enhanced experience.\n\n#merge #uncensored\n\n"},{"lang":"en","group":"models","slug":"models/magnum-v2-72b","frontmatter":{"title":"Magnum v2 72B","meta_title":"Magnum v2 72B","description":"Magnum v2 72B","date":"2024-09-30T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"anthracite-org","tags":["Roleplay","Programming","Machine Learning","Natural Language Processing","Generative AI"],"draft":false,"id":"magnum-v2-72b","context":32768,"input":0.00000375,"output":0.0000045,"img":0,"request":0,"last_updated":"2024-11-11T03:09:19.000Z","slug":"models/magnum-v2-72b"},"content":"\nFrom the maker of [Goliath](https://openrouter.ai/alpindale/goliath-120b), Magnum 72B is the seventh in a family of models designed to achieve the prose quality of the Claude 3 models, notably Opus & Sonnet.\n\nThe model is based on [Qwen2 72B](https://openrouter.ai/qwen/qwen-2-72b-instruct) and trained with 55 million tokens of highly curated roleplay (RP) data.\n\n"},{"lang":"en","group":"models","slug":"models/magnum-v4-72b","frontmatter":{"title":"Magnum v4 72B","meta_title":"Magnum v4 72B","description":"Magnum v4 72B","date":"2024-10-22T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"anthracite-org","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"magnum-v4-72b","context":32768,"input":0.000001875,"output":0.00000225,"img":0,"request":0,"last_updated":"2024-11-04T12:39:55.000Z","slug":"models/magnum-v4-72b"},"content":"\nThis is a series of models designed to replicate the prose quality of the Claude 3 models, specifically Sonnet and Opus.\n\nThe model is fine-tuned on top of [Qwen2.5 72B].\n\n"},{"lang":"en","group":"models","slug":"models/ministral-3b","frontmatter":{"title":"Ministral 3B","meta_title":"Ministral 3B","description":"Ministral 3B","date":"2024-10-17T00:00:00.000Z","image":"https://img.rifx.online/logo/mistral.png","categories":["text 2 text"],"author":"mistralai","tags":["Programming","Machine Learning","Natural Language Processing","Data Science","Generative AI"],"draft":false,"id":"ministral-3b","context":128000,"input":4e-8,"output":4e-8,"img":0,"request":0,"last_updated":"2024-11-07T00:24:37.000Z","slug":"models/ministral-3b"},"content":"\nMinistral 3B is a 3B parameter model optimized for on-device and edge computing. It excels in knowledge, commonsense reasoning, and function-calling, outperforming larger models like Mistral 7B on most benchmarks. Supporting up to 128k context length, itâ€™s ideal for orchestrating agentic workflows and specialist tasks with efficient inference.\n\n"},{"lang":"en","group":"models","slug":"models/ministral-8b","frontmatter":{"title":"Ministral 8B","meta_title":"Ministral 8B","description":"Ministral 8B","date":"2024-10-17T00:00:00.000Z","image":"https://img.rifx.online/logo/mistral.png","categories":["text 2 text"],"author":"mistralai","tags":["Technology","Machine Learning","Data Science","Generative AI","Ethics"],"draft":false,"id":"ministral-8b","context":128000,"input":1e-7,"output":1e-7,"img":0,"request":0,"last_updated":"2024-10-19T04:54:11.000Z","slug":"models/ministral-8b"},"content":"\nMinistral 8B is an 8B parameter model featuring a unique interleaved sliding-window attention pattern for faster, memory-efficient inference. Designed for edge use cases, it supports up to 128k context length and excels in knowledge and reasoning tasks. It outperforms peers in the sub-10B category, making it perfect for low-latency, privacy-first applications.\n\n"},{"lang":"en","group":"models","slug":"models/mistral-7b-instruct","frontmatter":{"title":"Mistral: Mistral 7B Instruct","meta_title":"Mistral: Mistral 7B Instruct","description":"Mistral: Mistral 7B Instruct","date":"2024-05-27T00:00:00.000Z","image":"https://img.rifx.online/logo/mistral.png","categories":["text 2 text"],"author":"mistralai","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"mistral-7b-instruct","context":32768,"input":5.5e-8,"output":5.5e-8,"img":0,"request":0,"last_updated":"2024-10-31T23:13:12.000Z","slug":"models/mistral-7b-instruct"},"content":"\nA high-performing, industry-standard 7.3B parameter model, with optimizations for speed and context length.\n\n*Mistral 7B Instruct has multiple version variants, and this is intended to be the latest version.*\n\n"},{"lang":"en","group":"models","slug":"models/mistral-nemo","frontmatter":{"title":"Mistral: Mistral Nemo","meta_title":"Mistral: Mistral Nemo","description":"Mistral: Mistral Nemo","date":"2024-07-19T00:00:00.000Z","image":"https://img.rifx.online/logo/mistral.png","categories":["text 2 text"],"author":"mistralai","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Data Science"],"draft":false,"id":"mistral-nemo","context":128000,"input":1.3e-7,"output":1.3e-7,"img":0,"request":0,"last_updated":"2024-10-31T23:10:58.000Z","slug":"models/mistral-nemo"},"content":"\nA 12B parameter model with a 128k token context length built by Mistral in collaboration with NVIDIA.\n\nThe model is multilingual, supporting English, French, German, Spanish, Italian, Portuguese, Chinese, Japanese, Korean, Arabic, and Hindi.\n\nIt supports function calling and is released under the Apache 2.0 license.\n\n"},{"lang":"en","group":"models","slug":"models/mistral-tiny","frontmatter":{"title":"Mistral Tiny","meta_title":"Mistral Tiny","description":"Mistral Tiny","date":"2024-01-10T00:00:00.000Z","image":"https://img.rifx.online/logo/mistral.png","categories":["text 2 text"],"author":"mistralai","tags":["Programming","Machine Learning","Data Science","Generative AI","Chatbots"],"draft":false,"id":"mistral-tiny","context":32000,"input":2.5e-7,"output":2.5e-7,"img":0,"request":0,"last_updated":"2024-10-31T23:12:22.000Z","slug":"models/mistral-tiny"},"content":"\nThis model is currently powered by Mistral-7B-v0.2, and incorporates a \"better\" fine-tuning than [Mistral 7B](/mistralai/mistral-7b-instruct-v0.1), inspired by community work. It's best used for large batch processing tasks where cost is a significant factor but reasoning capabilities are not crucial.\n\n"},{"lang":"en","group":"models","slug":"models/mixtral-8x22b-instruct","frontmatter":{"title":"Mistral: Mixtral 8x22B Instruct","meta_title":"Mistral: Mixtral 8x22B Instruct","description":"Mistral: Mixtral 8x22B Instruct","date":"2024-04-17T00:00:00.000Z","image":"https://img.rifx.online/logo/mistral.png","categories":["text 2 text"],"author":"mistralai","tags":["Programming","Natural Language Processing","Machine Learning","Data Science","Generative AI"],"draft":false,"is_recommended":false,"id":"mixtral-8x22b-instruct","context":65536,"input":9e-7,"output":9e-7,"img":0,"request":0,"last_updated":"2024-11-14T08:21:32.000Z","slug":"models/mixtral-8x22b-instruct"},"content":"\nMistral's official instruct fine-tuned version of [Mixtral 8x22B](/mistralai/mixtral-8x22b). It uses 39B active parameters out of 141B, offering unparalleled cost efficiency for its size. Its strengths include:\n- strong math, coding, and reasoning\n- large context length (64k)\n- fluency in English, French, Italian, German, and Spanish\n\nSee benchmarks on the launch announcement [here](https://mistral.ai/news/mixtral-8x22b/).\n#moe\n\n"},{"lang":"en","group":"models","slug":"models/mixtral-8x7b","frontmatter":{"title":"Mixtral 8x7B (base)","meta_title":"Mixtral 8x7B (base)","description":"Mixtral 8x7B (base)","date":"2023-12-10T00:00:00.000Z","image":"https://img.rifx.online/logo/mistral.png","categories":["text 2 text"],"author":"mistralai","tags":["Generative AI","Machine Learning","Data Science","Programming","Technology/Web"],"draft":false,"is_recommended":true,"id":"mixtral-8x7b","context":32768,"input":5.4e-7,"output":5.4e-7,"img":0,"request":0,"last_updated":"2024-11-14T08:22:09.000Z","slug":"models/mixtral-8x7b"},"content":"\nA pretrained generative Sparse Mixture of Experts, by Mistral AI. Incorporates 8 experts (feed-forward networks) for a total of 47B parameters. Base model (not fine-tuned for instructions) - see [Mixtral 8x7B Instruct](/mistralai/mixtral-8x7b-instruct) for an instruct-tuned model.\n\n#moe\n\n"},{"lang":"en","group":"models","slug":"models/mn-inferor-12b","frontmatter":{"title":"Mistral Nemo Inferor 12B","meta_title":"Mistral Nemo Inferor 12B","description":"Mistral Nemo Inferor 12B","date":"2024-11-13T02:20:28.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"infermatic","tags":["Roleplay","Programming","Machine Learning","Natural Language Processing","Generative AI"],"draft":false,"id":"mn-inferor-12b","context":32000,"input":2.5e-7,"output":5e-7,"img":0,"request":0,"last_updated":"2024-11-14T02:10:35.000Z","slug":"models/mn-inferor-12b"},"content":"\nInferor is a merge of top roleplay models, expert on immersive narratives and storytelling.\n\nThis model was merged using the [Model Stock](https://arxiv.org/abs/2403.19522) merge method using [anthracite-org/magnum-v4-12b](https://openrouter.ai/anthracite-org/magnum-v4-72b) as a base.\n\n\n"},{"lang":"en","group":"models","slug":"models/mn-starcannon-12b","frontmatter":{"title":"Mistral Nemo 12B Starcannon","meta_title":"Mistral Nemo 12B Starcannon","description":"Mistral Nemo 12B Starcannon","date":"2024-08-13T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"aetherwiing","tags":["Roleplay","Programming","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"mn-starcannon-12b","context":12000,"input":0.000002,"output":0.000002,"img":0,"request":0,"last_updated":"2024-11-11T03:16:53.000Z","slug":"models/mn-starcannon-12b"},"content":"\nStarcannon 12B is a creative roleplay and story writing model, using [nothingiisreal/mn-celeste-12b](https://openrouter.ai/nothingiisreal/mn-celeste-12b) as a base and [intervitens/mini-magnum-12b-v1.1](https://huggingface.co/intervitens/mini-magnum-12b-v1.1) merged in using the [TIES](https://arxiv.org/abs/2306.01708) method.\n\nAlthough more similar to Magnum overall, the model remains very creative, with a pleasant writing style. It is recommended for people wanting more variety than Magnum, and yet more verbose prose than Celeste.\n\n"},{"lang":"en","group":"models","slug":"models/mythomax-l2-13b","frontmatter":{"title":"MythoMax 13B","meta_title":"MythoMax 13B","description":"MythoMax 13B","date":"2023-07-02T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"gryphe","tags":["Roleplay","Programming","Machine Learning","Natural Language Processing","Generative AI"],"draft":false,"id":"mythomax-l2-13b","context":4096,"input":1e-7,"output":1e-7,"img":0,"request":0,"last_updated":"2024-10-28T13:10:41.000Z","slug":"models/mythomax-l2-13b"},"content":"\nOne of the highest performing and most popular fine-tunes of Llama 2 13B, with rich descriptions and roleplay. #merge\n\n"},{"lang":"en","group":"models","slug":"models/o1-mini","frontmatter":{"title":"OpenAI: o1-mini","meta_title":"OpenAI: o1-mini","description":"OpenAI: o1-mini","date":"2024-09-12T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"openai","tags":["Programming","Science","Natural Language Processing","Machine Learning","Data Science"],"draft":false,"id":"o1-mini","context":128000,"input":0.000003,"output":0.000012,"img":0,"request":0,"last_updated":"2024-09-12T00:00:00.000Z","slug":"models/o1-mini"},"content":"\nThe latest and strongest model family from OpenAI, o1 is designed to spend more time thinking before responding.\n\nThe o1 models are optimized for math, science, programming, and other STEM-related tasks. They consistently exhibit PhD-level accuracy on benchmarks in physics, chemistry, and biology. Learn more in the [launch announcement](https://openai.com/o1).\n\nNote: This model is currently experimental and not suitable for production use-cases, and may be heavily rate-limited.\n\n"},{"lang":"en","group":"models","slug":"models/o1-preview","frontmatter":{"title":"OpenAI: o1-preview","meta_title":"OpenAI: o1-preview","description":"OpenAI: o1-preview","date":"2024-09-12T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"openai","tags":["Programming","Science","Natural Language Processing","Machine Learning","Data Science"],"draft":false,"id":"o1-preview","context":128000,"input":0.000015,"output":0.00006,"img":0,"request":0,"last_updated":"2024-09-12T00:00:00.000Z","slug":"models/o1-preview"},"content":"\nThe latest and strongest model family from OpenAI, o1 is designed to spend more time thinking before responding.\n\nThe o1 models are optimized for math, science, programming, and other STEM-related tasks. They consistently exhibit PhD-level accuracy on benchmarks in physics, chemistry, and biology. Learn more in the [launch announcement](https://openai.com/o1).\n\nNote: This model is currently experimental and not suitable for production use-cases, and may be heavily rate-limited.\n\n"},{"lang":"en","group":"models","slug":"models/openai-gpt-4o-mini","frontmatter":{"title":"OpenAI: GPT-4o-Mini Official","meta_title":"OpenAI: GPT-4o-Mini Official","description":"OpenAI: GPT-4o-Mini Official","date":"2024-10-26T09:00:07.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"gpt-4o-mini","tags":["Generative AI","Natural Language Processing","Machine Learning","Technology","Chatbots"],"draft":false,"is_recommended":false,"id":"openai/gpt-4o-mini","context":128000,"input":1.5e-7,"output":6e-7,"img":0.0036125,"request":0,"last_updated":"2024-11-14T09:46:04.000Z","slug":"models/openai-gpt-4o-mini"},"content":"\nGPT-4o mini is OpenAI's newest model after [GPT-4 Omni](/openai/gpt-4o), supporting both text and image inputs with text outputs.\n\nAs their most advanced small model, it is many multiples more affordable than other recent frontier models, and more than 60% cheaper than [GPT-3.5 Turbo](/openai/gpt-3.5-turbo). It maintains SOTA intelligence, while being significantly more cost-effective.\n\nGPT-4o mini achieves an 82% score on MMLU and presently ranks higher than GPT-4 on chat preferences [common leaderboards](https://arena.lmsys.org/).\n\nCheck out the [launch announcement](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/) to learn more.\n\n"},{"lang":"en","group":"models","slug":"models/openai-gpt-4o","frontmatter":{"title":"OpenAI: GPT-4o Official","meta_title":"OpenAI: GPT-4o Official","description":"OpenAI: GPT-4o Official","date":"2024-11-14T02:53:29.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"gpt-4o","tags":["Generative AI","Natural Language Processing","Technology","Chatbots","Machine Learning"],"draft":false,"is_recommended":false,"id":"openai/gpt-4o","context":128000,"input":0.0000025,"output":0.00001,"img":0.0036125,"request":0,"last_updated":"2024-11-14T09:45:25.000Z","slug":"models/openai-gpt-4o"},"content":"\nGPT-4o (\"o\" for \"omni\") is OpenAI's latest AI model, supporting both text and image inputs with text outputs. It maintains the intelligence level of [GPT-4 Turbo](/openai/gpt-4-turbo) while being twice as fast and 50% more cost-effective. GPT-4o also offers improved performance in processing non-English languages and enhanced visual capabilities.\n\nFor benchmarking against other models, it was briefly called [\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209)\n\n"},{"lang":"en","group":"models","slug":"models/openchat-7b","frontmatter":{"title":"OpenChat 3.5 7B","meta_title":"OpenChat 3.5 7B","description":"OpenChat 3.5 7B","date":"2023-11-28T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"openchat","tags":["Programming","Natural Language Processing","Machine Learning","Open Source","Generative AI"],"draft":false,"id":"openchat-7b","context":8192,"input":5.5e-8,"output":5.5e-8,"img":0,"request":0,"last_updated":"2024-11-07T09:39:42.000Z","slug":"models/openchat-7b"},"content":"\nOpenChat 7B is a library of open-source language models, fine-tuned with \"C-RLFT (Conditioned Reinforcement Learning Fine-Tuning)\" - a strategy inspired by offline reinforcement learning. It has been trained on mixed-quality data without preference labels.\n\n- For OpenChat fine-tuned on Mistral 7B, check out [OpenChat 7B](/openchat/openchat-7b).\n- For OpenChat fine-tuned on Llama 8B, check out [OpenChat 8B](/openchat/openchat-8b).\n\n#open-source\n\n"},{"lang":"en","group":"models","slug":"models/palm-2-chat-bison-32k","frontmatter":{"title":"Google: PaLM 2 Chat 32k","meta_title":"Google: PaLM 2 Chat 32k","description":"Google: PaLM 2 Chat 32k","date":"2023-11-03T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text 2 text"],"author":"google","tags":["Natural Language Processing","Programming","Technology","Chatbots","Generative AI"],"draft":false,"id":"palm-2-chat-bison-32k","context":32760,"input":0.000001,"output":0.000002,"img":0,"request":0,"last_updated":"2024-11-11T03:15:52.000Z","slug":"models/palm-2-chat-bison-32k"},"content":"\nPaLM 2 is a language model by Google with improved multilingual, reasoning and coding capabilities.\n\n"},{"lang":"en","group":"models","slug":"models/palm-2-codechat-bison-32k","frontmatter":{"title":"Google: PaLM 2 Code Chat 32k","meta_title":"Google: PaLM 2 Code Chat 32k","description":"Google: PaLM 2 Code Chat 32k","date":"2023-11-03T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text 2 text"],"author":"google","tags":["Programming","Chatbots","Natural Language Processing","Generative AI","Technology/Web"],"draft":false,"id":"palm-2-codechat-bison-32k","context":32760,"input":0.000001,"output":0.000002,"img":0,"request":0,"last_updated":"2024-11-11T03:16:01.000Z","slug":"models/palm-2-codechat-bison-32k"},"content":"\nPaLM 2 fine-tuned for chatbot conversations that help with code-related questions.\n\n"},{"lang":"en","group":"models","slug":"models/phi-3-medium-128k-instruct","frontmatter":{"title":"Phi-3 Medium 128K Instruct","meta_title":"Phi-3 Medium 128K Instruct","description":"Phi-3 Medium 128K Instruct","date":"2024-05-24T00:00:00.000Z","image":"https://img.rifx.online/logo/microsoft.svg","categories":["text 2 text"],"author":"microsoft","tags":["Natural Language Processing","Machine Learning","Programming","Data Science","Generative AI"],"draft":false,"id":"phi-3-medium-128k-instruct","context":128000,"input":0.000001,"output":0.000001,"img":0,"request":0,"last_updated":"2024-11-11T03:17:38.000Z","slug":"models/phi-3-medium-128k-instruct"},"content":"\nPhi-3 128K Medium is a powerful 14-billion parameter model designed for advanced language understanding, reasoning, and instruction following. Optimized through supervised fine-tuning and preference adjustments, it excels in tasks involving common sense, mathematics, logical reasoning, and code processing.\n\nAt time of release, Phi-3 Medium demonstrated state-of-the-art performance among lightweight models. In the MMLU-Pro eval, the model even comes close to a Llama3 70B level of performance.\n\nFor 4k context length, try [Phi-3 Medium 4K](/microsoft/phi-3-medium-4k-instruct).\n\n"},{"lang":"en","group":"models","slug":"models/phi-3-mini-128k-instruct","frontmatter":{"title":"Phi-3 Mini 128K Instruct","meta_title":"Phi-3 Mini 128K Instruct","description":"Phi-3 Mini 128K Instruct","date":"2024-05-26T00:00:00.000Z","image":"https://img.rifx.online/logo/microsoft.svg","categories":["text 2 text"],"author":"microsoft","tags":["Natural Language Processing","Machine Learning","Programming","Data Science","Generative AI"],"draft":false,"id":"phi-3-mini-128k-instruct","context":128000,"input":1e-7,"output":1e-7,"img":0,"request":0,"last_updated":"2024-11-11T03:17:47.000Z","slug":"models/phi-3-mini-128k-instruct"},"content":"\nPhi-3 Mini is a powerful 3.8B parameter model designed for advanced language understanding, reasoning, and instruction following. Optimized through supervised fine-tuning and preference adjustments, it excels in tasks involving common sense, mathematics, logical reasoning, and code processing.\n\nAt time of release, Phi-3 Medium demonstrated state-of-the-art performance among lightweight models. This model is static, trained on an offline dataset with an October 2023 cutoff date.\n\n"},{"lang":"en","group":"models","slug":"models/phi-35-mini-128k-instruct","frontmatter":{"title":"Phi-3.5 Mini 128K Instruct","meta_title":"Phi-3.5 Mini 128K Instruct","description":"Phi-3.5 Mini 128K Instruct","date":"2024-08-21T00:00:00.000Z","image":"https://img.rifx.online/logo/microsoft.svg","categories":["text 2 text"],"author":"microsoft","tags":["Programming","Machine Learning","Natural Language Processing","Data Science","Generative AI"],"draft":false,"id":"phi-3.5-mini-128k-instruct","context":128000,"input":1e-7,"output":1e-7,"img":0,"request":0,"last_updated":"2024-11-01T04:17:17.000Z","slug":"models/phi-35-mini-128k-instruct"},"content":"\nPhi-3.5 models are lightweight, state-of-the-art open models. These models were trained with Phi-3 datasets that include both synthetic data and the filtered, publicly available websites data, with a focus on high quality and reasoning-dense properties. Phi-3.5 Mini uses 3.8B parameters, and is a dense decoder-only transformer model using the same tokenizer as [Phi-3 Mini](/microsoft/phi-3-mini-128k-instruct).\n\nThe models underwent a rigorous enhancement process, incorporating both supervised fine-tuning, proximal policy optimization, and direct preference optimization to ensure precise instruction adherence and robust safety measures. When assessed against benchmarks that test common sense, language understanding, math, code, long context and logical reasoning, Phi-3.5 models showcased robust and state-of-the-art performance among models with less than 13 billion parameters.\n\n"},{"lang":"en","group":"models","slug":"models/pixtral-12b","frontmatter":{"title":"Mistral: Pixtral 12B","meta_title":"Mistral: Pixtral 12B","description":"Mistral: Pixtral 12B","date":"2024-09-10T00:00:00.000Z","image":"https://img.rifx.online/logo/mistral.png","categories":["text image 2 text"],"author":"mistralai","tags":["Natural Language Processing","Machine Learning","Technology","Generative AI","Computer Vision"],"draft":false,"id":"pixtral-12b","context":4096,"input":1e-7,"output":1e-7,"img":0.0001445,"request":0,"last_updated":"2024-11-11T03:10:29.000Z","slug":"models/pixtral-12b"},"content":"\nThe first image to text model from Mistral AI. Its weight was launched via torrent per their tradition: https://x.com/mistralai/status/1833758285167722836\n\n"},{"lang":"en","group":"models","slug":"models/qwen-2-7b-instruct","frontmatter":{"title":"Qwen 2 7B Instruct","meta_title":"Qwen 2 7B Instruct","description":"Qwen 2 7B Instruct","date":"2024-07-16T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"qwen","tags":["Natural Language Processing","Programming","Machine Learning","Data Science","Ethics"],"draft":false,"id":"qwen-2-7b-instruct","context":32768,"input":5.4e-8,"output":5.4e-8,"img":0,"request":0,"last_updated":"2024-07-16T00:00:00.000Z","slug":"models/qwen-2-7b-instruct"},"content":"\nQwen2 7B is a transformer-based model that excels in language understanding, multilingual capabilities, coding, mathematics, and reasoning.\n\nIt features SwiGLU activation, attention QKV bias, and group query attention. It is pretrained on extensive data with supervised finetuning and direct preference optimization.\n\nFor more details, see this [blog post](https://qwenlm.github.io/blog/qwen2/) and [GitHub repo](https://github.com/QwenLM/Qwen2).\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).\n\n"},{"lang":"en","group":"models","slug":"models/qwen-2-vl-72b-instruct","frontmatter":{"title":"Qwen2-VL 72B Instruct","meta_title":"Qwen2-VL 72B Instruct","description":"Qwen2-VL 72B Instruct","date":"2024-09-18T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text image 2 text"],"author":"qwen","tags":["Natural Language Processing","Computer Vision","Robotics","Machine Learning"],"draft":false,"id":"qwen-2-vl-72b-instruct","context":32768,"input":4e-7,"output":4e-7,"img":0.000578,"request":0,"last_updated":"2024-09-18T00:00:00.000Z","slug":"models/qwen-2-vl-72b-instruct"},"content":"\nQwen2 VL 72B is a multimodal LLM from the Qwen Team with the following key enhancements:\n\n- SoTA understanding of images of various resolution & ratio: Qwen2-VL achieves state-of-the-art performance on visual understanding benchmarks, including MathVista, DocVQA, RealWorldQA, MTVQA, etc.\n\n- Understanding videos of 20min+: Qwen2-VL can understand videos over 20 minutes for high-quality video-based question answering, dialog, content creation, etc.\n\n- Agent that can operate your mobiles, robots, etc.: with the abilities of complex reasoning and decision making, Qwen2-VL can be integrated with devices like mobile phones, robots, etc., for automatic operation based on visual environment and text instructions.\n\n- Multilingual Support: to serve global users, besides English and Chinese, Qwen2-VL now supports the understanding of texts in different languages inside images, including most European languages, Japanese, Korean, Arabic, Vietnamese, etc.\n\nFor more details, see this [blog post](https://qwenlm.github.io/blog/qwen2-vl/) and [GitHub repo](https://github.com/QwenLM/Qwen2-VL).\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).\n\n"},{"lang":"en","group":"models","slug":"models/qwen-2-vl-7b-instruct","frontmatter":{"title":"Qwen2-VL 7B Instruct","meta_title":"Qwen2-VL 7B Instruct","description":"Qwen2-VL 7B Instruct","date":"2024-08-28T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text image 2 text"],"author":"qwen","tags":["Natural Language Processing","Computer Vision","Robotics","Multimodal AI","Generative AI"],"draft":false,"id":"qwen-2-vl-7b-instruct","context":32768,"input":1e-7,"output":1e-7,"img":0.0001445,"request":0,"last_updated":"2024-11-11T03:13:01.000Z","slug":"models/qwen-2-vl-7b-instruct"},"content":"\nQwen2 VL 7B is a multimodal LLM from the Qwen Team with the following key enhancements:\n\n- SoTA understanding of images of various resolution & ratio: Qwen2-VL achieves state-of-the-art performance on visual understanding benchmarks, including MathVista, DocVQA, RealWorldQA, MTVQA, etc.\n\n- Understanding videos of 20min+: Qwen2-VL can understand videos over 20 minutes for high-quality video-based question answering, dialog, content creation, etc.\n\n- Agent that can operate your mobiles, robots, etc.: with the abilities of complex reasoning and decision making, Qwen2-VL can be integrated with devices like mobile phones, robots, etc., for automatic operation based on visual environment and text instructions.\n\n- Multilingual Support: to serve global users, besides English and Chinese, Qwen2-VL now supports the understanding of texts in different languages inside images, including most European languages, Japanese, Korean, Arabic, Vietnamese, etc.\n\nFor more details, see this [blog post](https://qwenlm.github.io/blog/qwen2-vl/) and [GitHub repo](https://github.com/QwenLM/Qwen2-VL).\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).\n\n"},{"lang":"en","group":"models","slug":"models/qwen-25-72b-instruct","frontmatter":{"title":"Qwen2.5 72B Instruct","meta_title":"Qwen2.5 72B Instruct","description":"Qwen2.5 72B Instruct","date":"2024-09-19T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"qwen","tags":["Programming","Natural Language Processing","Chatbots","Machine Learning","Data Science"],"draft":false,"id":"qwen-2.5-72b-instruct","context":131072,"input":3.5e-7,"output":4e-7,"img":0,"request":0,"last_updated":"2024-09-19T00:00:00.000Z","slug":"models/qwen-25-72b-instruct"},"content":"\nQwen2.5 72B is the latest series of Qwen large language models. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\n\n- Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n\n- Long-context Support up to 128K tokens and can generate up to 8K tokens.\n\n- Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).\n\n"},{"lang":"en","group":"models","slug":"models/qwen-25-7b-instruct","frontmatter":{"title":"Qwen2.5 7B Instruct","meta_title":"Qwen2.5 7B Instruct","description":"Qwen2.5 7B Instruct","date":"2024-10-16T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"qwen","tags":["Programming","Natural Language Processing","Chatbots","Machine Learning","Data Science"],"draft":false,"id":"qwen-2.5-7b-instruct","context":131072,"input":2.7e-7,"output":2.7e-7,"img":0,"request":0,"last_updated":"2024-10-16T00:00:00.000Z","slug":"models/qwen-25-7b-instruct"},"content":"\nQwen2.5 7B is the latest series of Qwen large language models. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\n\n- Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n\n- Long-context Support up to 128K tokens and can generate up to 8K tokens.\n\n- Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).\n\n"},{"lang":"en","group":"models","slug":"models/qwen-25-coder-32b-instruct","frontmatter":{"title":"Qwen2.5 Coder 32B Instruct","meta_title":"Qwen2.5 Coder 32B Instruct","description":"Qwen2.5 Coder 32B Instruct","date":"2024-11-11T23:40:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"qwen","tags":["Programming","Programming/Scripting","Machine Learning","Natural Language Processing","Generative AI"],"draft":false,"is_recommended":true,"id":"qwen-2.5-coder-32b-instruct","context":32768,"input":1.8e-7,"output":1.8e-7,"img":0,"request":0,"last_updated":"2024-11-15T23:24:23.000Z","slug":"models/qwen-25-coder-32b-instruct"},"content":"\nQwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). Qwen2.5-Coder brings the following improvements upon CodeQwen1.5:\n\n- Significantly improvements in **code generation**, **code reasoning** and **code fixing**. \n- A more comprehensive foundation for real-world applications such as **Code Agents**. Not only enhancing coding capabilities but also maintaining its strengths in mathematics and general competencies.\n\nTo read more about its evaluation results, check out [Qwen 2.5 Coder's blog](https://qwenlm.github.io/blog/qwen2.5-coder-family/).\n\n\n\n\n"},{"lang":"en","group":"models","slug":"models/remm-slerp-l2-13b","frontmatter":{"title":"ReMM SLERP 13B","meta_title":"ReMM SLERP 13B","description":"ReMM SLERP 13B","date":"2023-07-22T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"undi95","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"is_recommended":true,"id":"remm-slerp-l2-13b","context":4096,"input":0.000001125,"output":0.000001125,"img":0,"request":0,"last_updated":"2024-11-14T04:06:00.000Z","slug":"models/remm-slerp-l2-13b"},"content":"\nA recreation trial of the original MythoMax-L2-B13 but with updated models. #merge\n\n"},{"lang":"en","group":"models","slug":"models/remm-slerp-l2-13b:extended","frontmatter":{"title":"ReMM SLERP 13B (extended)","meta_title":"ReMM SLERP 13B (extended)","description":"ReMM SLERP 13B (extended)","date":"2023-07-22T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"undi95","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"remm-slerp-l2-13b:extended","context":6144,"input":0.000001125,"output":0.000001125,"img":0,"request":0,"last_updated":"2024-11-04T12:47:21.000Z","slug":"models/remm-slerp-l2-13b:extended"},"content":"\nA recreation trial of the original MythoMax-L2-B13 but with updated models. #merge\n\n_These are extended-context endpoints for [ReMM SLERP 13B](/undi95/remm-slerp-l2-13b). They may have higher prices._\n\n"},{"lang":"en","group":"models","slug":"models/rocinante-12b","frontmatter":{"title":"Rocinante 12B","meta_title":"Rocinante 12B","description":"Rocinante 12B","date":"2024-09-30T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"thedrummer","tags":["Roleplay","Programming","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"rocinante-12b","context":32768,"input":2.5e-7,"output":5e-7,"img":0,"request":0,"last_updated":"2024-11-11T03:09:37.000Z","slug":"models/rocinante-12b"},"content":"\nRocinante 12B is designed for engaging storytelling and rich prose.\n\nEarly testers have reported:\n- Expanded vocabulary with unique and expressive word choices\n- Enhanced creativity for vivid narratives\n- Adventure-filled and captivating stories\n\n"},{"lang":"en","group":"models","slug":"models/sorcererlm-8x22b","frontmatter":{"title":"Sorcererlm 8x22b","meta_title":"Sorcererlm 8x22b","description":"Sorcererlm 8x22b","date":"2024-11-08T22:31:23.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"raifle","tags":["Roleplay","Programming","Natural Language Processing","Chatbots","Generative AI"],"draft":false,"id":"sorcererlm-8x22b","context":16000,"input":0.0000045,"output":0.0000045,"img":0,"request":0,"last_updated":"2024-11-11T02:56:49.000Z","slug":"models/sorcererlm-8x22b"},"content":"\nSorcererLM is an advanced RP and storytelling model, built as a Low-rank 16-bit LoRA fine-tuned on WizardLM-2-8x22B.\n\n- Advanced reasoning and emotional intelligence for engaging and immersive interactions\n- Vivid writing capabilities enriched with spatial and contextual awareness\n- Enhanced narrative depth, promoting creative and dynamic storytelling\n\n"},{"lang":"en","group":"models","slug":"models/toppy-m-7b","frontmatter":{"title":"Toppy M 7B","meta_title":"Toppy M 7B","description":"Toppy M 7B","date":"2023-11-10T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"undi95","tags":["Programming","Machine Learning","Generative AI","Chatbots","Data Science"],"draft":false,"id":"toppy-m-7b","context":4096,"input":7e-8,"output":7e-8,"img":0,"request":0,"last_updated":"2024-11-04T12:51:35.000Z","slug":"models/toppy-m-7b"},"content":"\nA wild 7B parameter model that merges several models using the new task_arithmetic merge method from mergekit.\nList of merged models:\n- NousResearch/Nous-Capybara-7B-V1.9\n- [HuggingFaceH4/zephyr-7b-beta](/huggingfaceh4/zephyr-7b-beta)\n- lemonilia/AshhLimaRP-Mistral-7B\n- Vulkane/120-Days-of-Sodom-LoRA-Mistral-7b\n- Undi95/Mistral-pippa-sharegpt-7b-qlora\n\n#merge #uncensored\n\n"},{"lang":"en","group":"models","slug":"models/unslopnemo-12b","frontmatter":{"title":"Unslopnemo 12b","meta_title":"Unslopnemo 12b","description":"Unslopnemo 12b","date":"2024-11-08T22:04:08.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"thedrummer","tags":["Roleplay","Programming","Generative AI","Chatbots","Natural Language Processing"],"draft":false,"id":"unslopnemo-12b","context":32000,"input":5e-7,"output":5e-7,"img":0,"request":0,"last_updated":"2024-11-14T02:10:09.000Z","slug":"models/unslopnemo-12b"},"content":"\nUnslopNemo v4.1 is the latest addition from the creator of Rocinante, designed for adventure writing and role-play scenarios.\n\n"},{"lang":"en","group":"models","slug":"models/wizardlm-2-7b","frontmatter":{"title":"WizardLM-2 7B","meta_title":"WizardLM-2 7B","description":"WizardLM-2 7B","date":"2024-04-16T00:00:00.000Z","image":"https://img.rifx.online/logo/microsoft.svg","categories":["text 2 text"],"author":"microsoft","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"wizardlm-2-7b","context":32000,"input":5.5e-8,"output":5.5e-8,"img":0,"request":0,"last_updated":"2024-10-31T23:23:36.000Z","slug":"models/wizardlm-2-7b"},"content":"\nWizardLM-2 7B is the smaller variant of Microsoft AI's latest Wizard model. It is the fastest and achieves comparable performance with existing 10x larger opensource leading models\n\nIt is a finetune of [Mistral 7B Instruct](/mistralai/mistral-7b-instruct), using the same technique as [WizardLM-2 8x22B](/microsoft/wizardlm-2-8x22b).\n\nTo read more about the model release, [click here](https://wizardlm.github.io/WizardLM2/).\n\n#moe\n\n"},{"lang":"en","group":"models","slug":"models/wizardlm-2-8x22b","frontmatter":{"title":"WizardLM-2 8x22B","meta_title":"WizardLM-2 8x22B","description":"WizardLM-2 8x22B","date":"2024-04-16T00:00:00.000Z","image":"https://img.rifx.online/logo/microsoft.svg","categories":["text 2 text"],"author":"microsoft","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"wizardlm-2-8x22b","context":65536,"input":5e-7,"output":5e-7,"img":0,"request":0,"last_updated":"2024-10-31T23:24:21.000Z","slug":"models/wizardlm-2-8x22b"},"content":"\nWizardLM-2 8x22B is Microsoft AI's most advanced Wizard model. It demonstrates highly competitive performance compared to leading proprietary models, and it consistently outperforms all existing state-of-the-art opensource models.\n\nIt is an instruct finetune of [Mixtral 8x22B](/mistralai/mixtral-8x22b).\n\nTo read more about the model release, [click here](https://wizardlm.github.io/WizardLM2/).\n\n#moe\n\n"},{"lang":"zh","group":"models","slug":"models/chatgpt-4o-latest","frontmatter":{"title":"OpenAI: ChatGPT-4o","meta_title":"OpenAI: ChatGPT-4o","description":"OpenAI: ChatGPT-4o","date":"2024-08-14T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text image 2 text"],"author":"openai","tags":["Chatbots","Generative AI","Machine Learning","Natural Language Processing"],"draft":false,"id":"chatgpt-4o-latest","context":128000,"input":0.000005,"output":0.000015,"img":0.007225,"request":0,"last_updated":"2024-08-14T00:00:00.000Z","slug":"models/chatgpt-4o-latest"},"content":"\nåŠ¨æ€æ¨¡åž‹æŒç»­æ›´æ–°åˆ° ChatGPT ä¸­çš„å½“å‰ç‰ˆæœ¬ [GPT-4o](/openai/gpt-4o)ã€‚æ—¨åœ¨ç”¨äºŽç ”ç©¶å’Œè¯„ä¼°ã€‚\n\næ³¨æ„ï¼šæ­¤æ¨¡åž‹ç›®å‰å¤„äºŽå®žéªŒé˜¶æ®µï¼Œä¸é€‚åˆç”Ÿäº§ä½¿ç”¨ï¼Œå¹¶å¯èƒ½å—åˆ°ä¸¥æ ¼çš„é€ŸçŽ‡é™åˆ¶ã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/claude-3-haiku","frontmatter":{"title":"Anthropic: Claude 3 Haiku","meta_title":"Anthropic: Claude 3 Haiku","description":"Anthropic: Claude 3 Haiku","date":"2024-03-13T00:00:00.000Z","image":"https://img.rifx.online/logo/anthropic.svg","categories":["text image 2 text"],"author":"anthropic","tags":["Programming","Machine Learning","Generative AI","Chatbots","Natural Language Processing"],"draft":false,"id":"claude-3-haiku","context":200000,"input":2.5e-7,"output":0.00000125,"img":0.0004,"request":0,"last_updated":"2024-10-24T11:54:59.000Z","slug":"models/claude-3-haiku"},"content":"\nClaude 3 Haiku æ˜¯ Anthropic ååº”é€Ÿåº¦æœ€å¿«ã€ä½“ç§¯æœ€å°çš„æ¨¡åž‹ï¼Œèƒ½å¤Ÿå®žçŽ°è¿‘ä¹Žå³æ—¶çš„å“åº”ã€‚å¿«é€Ÿä¸”å‡†ç¡®çš„å®šå‘æ€§èƒ½ã€‚\n\næŸ¥çœ‹å‘å¸ƒå…¬å‘Šå’ŒåŸºå‡†æµ‹è¯•ç»“æžœ [è¿™é‡Œ](https://www.anthropic.com/news/claude-3-haiku)\n\n#multimodal\n\n"},{"lang":"zh","group":"models","slug":"models/claude-3-opus","frontmatter":{"title":"Anthropic: Claude 3 Opus","meta_title":"Anthropic: Claude 3 Opus","description":"Anthropic: Claude 3 Opus","date":"2024-03-05T00:00:00.000Z","image":"https://img.rifx.online/logo/anthropic.svg","categories":["text image 2 text"],"author":"anthropic","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"claude-3-opus","context":200000,"input":0.000015,"output":0.000075,"img":0.024,"request":0,"last_updated":"2024-11-07T09:45:35.000Z","slug":"models/claude-3-opus"},"content":"\nClaude 3 Opus æ˜¯ Anthropic é’ˆå¯¹é«˜åº¦å¤æ‚ä»»åŠ¡çš„æœ€å¼ºå¤§æ¨¡åž‹ã€‚å®ƒæ‹¥æœ‰é¡¶çº§çš„æ€§èƒ½ã€æ™ºèƒ½ã€æµç•…æ€§å’Œç†è§£èƒ½åŠ›ã€‚\n\næŸ¥çœ‹å‘å¸ƒå…¬å‘Šå’ŒåŸºå‡†æµ‹è¯•ç»“æžœ [here](https://www.anthropic.com/news/claude-3-family)\n\n#multimodal\n\n"},{"lang":"zh","group":"models","slug":"models/claude-3-sonnet","frontmatter":{"title":"Anthropic: Claude 3 Sonnet","meta_title":"Anthropic: Claude 3 Sonnet","description":"Anthropic: Claude 3 Sonnet","date":"2024-03-05T00:00:00.000Z","image":"https://img.rifx.online/logo/anthropic.svg","categories":["text image 2 text"],"author":"anthropic","tags":["Programming","Technology","Machine Learning","Data Science","Chatbots"],"draft":false,"is_recommended":true,"id":"claude-3-sonnet","context":200000,"input":0.000003,"output":0.000015,"img":0.0048,"request":0,"last_updated":"2024-11-14T04:05:16.000Z","slug":"models/claude-3-sonnet"},"content":"\nClaude 3 Sonnet æ˜¯ä¼ä¸šå·¥ä½œè´Ÿè½½çš„ç†æƒ³æ™ºèƒ½ä¸Žé€Ÿåº¦å¹³è¡¡ã€‚ä»¥æ›´ä½Žçš„ä»·æ ¼æä¾›æœ€å¤§æ•ˆç”¨ï¼Œå¯é ï¼Œé€‚åˆå¤§è§„æ¨¡éƒ¨ç½²çš„å¹³è¡¡ã€‚\n\næŸ¥çœ‹å‘å¸ƒå…¬å‘Šå’ŒåŸºå‡†æµ‹è¯•ç»“æžœ [è¿™é‡Œ](https://www.anthropic.com/news/claude-3-family)\n\n#multimodal\n\n"},{"lang":"zh","group":"models","slug":"models/claude-35-haiku","frontmatter":{"title":"Anthropic: Claude 3.5 Haiku","meta_title":"Anthropic: Claude 3.5 Haiku","description":"Anthropic: Claude 3.5 Haiku","date":"2024-11-04T00:00:00.000Z","image":"https://img.rifx.online/logo/anthropic.svg","categories":["text 2 text"],"author":"anthropic","tags":["Programming","Chatbots","Data Science","Machine Learning","Natural Language Processing"],"draft":false,"id":"claude-3.5-haiku","context":200000,"input":0.000001,"output":0.000005,"img":0,"request":0,"last_updated":"2024-11-07T09:46:02.000Z","slug":"models/claude-35-haiku"},"content":"\nClaude 3.5 Haiku ç‰¹æ€§æä¾›äº†æ›´é«˜çš„é€Ÿåº¦ã€ç¼–ç å‡†ç¡®æ€§å’Œå·¥å…·ä½¿ç”¨èƒ½åŠ›ã€‚æ—¨åœ¨åœ¨å®žæ—¶åº”ç”¨ä¸­è¡¨çŽ°å‡ºè‰²ï¼Œå®ƒæä¾›äº†å¿«é€Ÿçš„å“åº”æ—¶é—´ï¼Œè¿™å¯¹äºŽåŠ¨æ€ä»»åŠ¡ï¼ˆå¦‚èŠå¤©äº’åŠ¨å’Œå³æ—¶ç¼–ç å»ºè®®ï¼‰è‡³å…³é‡è¦ã€‚\n\nè¿™ä½¿å…¶éžå¸¸é€‚åˆéœ€è¦é€Ÿåº¦å’Œç²¾åº¦çš„çŽ¯å¢ƒï¼Œä¾‹å¦‚è½¯ä»¶å¼€å‘ã€å®¢æˆ·æœåŠ¡æœºå™¨äººå’Œæ•°æ®ç®¡ç†ç³»ç»Ÿã€‚\n\næ­¤æ¨¡åž‹å½“å‰æŒ‡å‘ [Claude 3.5 Haiku (2024-10-22)](/anthropic/claude-3-5-haiku-20241022).\n\n"},{"lang":"zh","group":"models","slug":"models/claude-35-sonnet","frontmatter":{"title":"Anthropic: Claude 3.5 Sonnet","meta_title":"Anthropic: Claude 3.5 Sonnet","description":"Anthropic: Claude 3.5 Sonnet","date":"2024-06-20T00:00:00.000Z","image":"https://img.rifx.online/logo/anthropic.svg","categories":["text image 2 text"],"author":"anthropic","tags":["Programming","Data Science","Computer Vision","Chatbots","Autonomous Systems"],"draft":false,"id":"claude-3.5-sonnet","context":200000,"input":0.000003,"output":0.000015,"img":0.0048,"request":0,"last_updated":"2024-10-24T11:45:46.000Z","slug":"models/claude-35-sonnet"},"content":"\nClaude 3.5 Sonnet æä¾›ä¼˜äºŽ Opus çš„èƒ½åŠ›ï¼Œä»¥å¿«äºŽ Sonnet çš„é€Ÿåº¦ï¼Œä¸”ä»·æ ¼ä¸Ž Sonnet ç›¸åŒã€‚Sonnet åœ¨ä»¥ä¸‹æ–¹é¢è¡¨çŽ°å°¤ä¸ºå‡ºè‰²ï¼š\n\n- ç¼–ç¨‹ï¼šè‡ªä¸»ç¼–å†™ã€ç¼–è¾‘å’Œè¿è¡Œä»£ç ï¼Œå…·å¤‡æŽ¨ç†å’Œæ•…éšœæŽ’é™¤èƒ½åŠ›\n- æ•°æ®ç§‘å­¦ï¼šå¢žå¼ºäººç±»æ•°æ®ç§‘å­¦ä¸“ä¸šçŸ¥è¯†ï¼›åœ¨ä½¿ç”¨å¤šç§å·¥å…·èŽ·å–æ´žå¯Ÿçš„åŒæ—¶ï¼Œå¤„ç†éžç»“æž„åŒ–æ•°æ®\n- è§†è§‰å¤„ç†ï¼šæ“…é•¿è§£è¯»å›¾è¡¨ã€å›¾å½¢å’Œå›¾åƒï¼Œå‡†ç¡®è½¬å½•æ–‡æœ¬ä»¥èŽ·å¾—è¶…è¶Šæ–‡æœ¬æœ¬èº«çš„æ´žå¯Ÿ\n- ä»£ç†ä»»åŠ¡ï¼šå‡ºè‰²çš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼Œä½¿å…¶åœ¨ä»£ç†ä»»åŠ¡ï¼ˆå³éœ€è¦ä¸Žå…¶ä»–ç³»ç»Ÿäº’åŠ¨çš„å¤æ‚å¤šæ­¥éª¤é—®é¢˜è§£å†³ä»»åŠ¡ï¼‰ä¸­è¡¨çŽ°å‡ºè‰²\n\n#multimodal\n\n"},{"lang":"zh","group":"models","slug":"models/command-r-plus","frontmatter":{"title":"Cohere: Command R+ (08-2024)","meta_title":"Cohere: Command R+ (08-2024)","description":"Cohere: Command R+ (08-2024)","date":"2024-08-30T00:00:00.000Z","image":"https://img.rifx.online/logo/cohere.svg","categories":["text 2 text"],"author":"cohere","tags":["Technology","Programming","Machine Learning","Generative AI","Ethics"],"draft":false,"id":"command-r-plus","context":128000,"input":0.000002375,"output":0.0000095,"img":0,"request":0,"last_updated":"2024-11-07T09:33:44.000Z","slug":"models/command-r-plus"},"content":"\ncommand-r-plus-08-2024 æ˜¯ [Command R+](/cohere/command-r-plus) çš„æ›´æ–°ï¼Œä¸Žä¹‹å‰çš„ Command R+ ç‰ˆæœ¬ç›¸æ¯”ï¼Œåžåé‡æé«˜äº†å¤§çº¦ 50%ï¼Œå»¶è¿Ÿé™ä½Žäº† 25%ï¼ŒåŒæ—¶ä¿æŒç›¸åŒçš„ç¡¬ä»¶å ç”¨ã€‚\n\nåœ¨ [è¿™é‡Œ](https://docs.cohere.com/changelog/command-gets-refreshed) é˜…è¯»å‘å¸ƒå¸–å­ã€‚\n\nä½¿ç”¨æ­¤æ¨¡åž‹é¡»éµå¾ª Cohere çš„ [å¯æŽ¥å—ä½¿ç”¨æ”¿ç­–](https://docs.cohere.com/docs/c4ai-acceptable-use-policy)ã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/command-r","frontmatter":{"title":"Cohere: Command R (08-2024)","meta_title":"Cohere: Command R (08-2024)","description":"Cohere: Command R (08-2024)","date":"2024-08-30T00:00:00.000Z","image":"https://img.rifx.online/logo/cohere.svg","categories":["text 2 text"],"author":"cohere","tags":["Programming","Natural Language Processing","Generative AI","Machine Learning","Data Science"],"draft":false,"id":"command-r","context":128000,"input":1.425e-7,"output":5.7e-7,"img":0,"request":0,"last_updated":"2024-11-07T09:33:55.000Z","slug":"models/command-r"},"content":"\ncommand-r-08-2024 æ˜¯ [Command R](/cohere/command-r) çš„æ›´æ–°ï¼Œæå‡äº†å¤šè¯­è¨€æ£€ç´¢å¢žå¼ºç”Ÿæˆ (RAG) å’Œå·¥å…·ä½¿ç”¨çš„æ€§èƒ½ã€‚æ›´å¹¿æ³›åœ°è¯´ï¼Œå®ƒåœ¨æ•°å­¦ã€ä»£ç å’ŒæŽ¨ç†æ–¹é¢è¡¨çŽ°æ›´ä½³ï¼Œå¹¶ä¸”ä¸Žä¹‹å‰çš„æ›´å¤§ç‰ˆæœ¬ Command R+ æ¨¡åž‹å…·æœ‰ç«žäº‰åŠ›ã€‚\n\né˜…è¯»å‘å¸ƒå¸–å­ [è¿™é‡Œ](https://docs.cohere.com/changelog/command-gets-refreshed)ã€‚\n\nä½¿ç”¨æ­¤æ¨¡åž‹é¡»éµå¾ª Cohere çš„ [å¯æŽ¥å—ä½¿ç”¨æ”¿ç­–](https://docs.cohere.com/docs/c4ai-acceptable-use-policy)ã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/deepseek-chat","frontmatter":{"title":"DeepSeek V2.5","meta_title":"DeepSeek V2.5","description":"DeepSeek V2.5","date":"2024-05-14T00:00:00.000Z","image":"https://img.rifx.online/logo/deepseek.svg","categories":["text 2 text"],"author":"deepseek","tags":["Programming","Natural Language Processing","Machine Learning","Data Science","Chatbots"],"draft":false,"id":"deepseek-chat","context":128000,"input":1.4e-7,"output":2.8e-7,"img":0,"request":0,"last_updated":"2024-11-01T04:19:11.000Z","slug":"models/deepseek-chat"},"content":"\nDeepSeek-V2.5 æ˜¯ä¸€ä¸ªå‡çº§ç‰ˆæœ¬ï¼Œç»“åˆäº† DeepSeek-V2-Chat å’Œ DeepSeek-Coder-V2-Instructã€‚æ–°æ¨¡åž‹æ•´åˆäº†å‰ä¸¤ä¸ªç‰ˆæœ¬çš„é€šç”¨èƒ½åŠ›å’Œç¼–ç èƒ½åŠ›ã€‚\n\nDeepSeek-V2 Chat æ˜¯ DeepSeek-V2 çš„å¯¹è¯å¾®è°ƒç‰ˆæœ¬ï¼Œå±žäºŽæ··åˆä¸“å®¶ï¼ˆMoEï¼‰è¯­è¨€æ¨¡åž‹ã€‚å®ƒæ€»å…±åŒ…å« 236B ä¸ªå‚æ•°ï¼Œå…¶ä¸­æ¯ä¸ª token æ¿€æ´» 21Bã€‚\n\nä¸Ž DeepSeek 67B ç›¸æ¯”ï¼ŒDeepSeek-V2 çš„æ€§èƒ½æ›´å¼ºï¼ŒåŒæ—¶èŠ‚çœäº† 42.5% çš„è®­ç»ƒæˆæœ¬ï¼Œå‡å°‘äº† 93.3% çš„ KV ç¼“å­˜ï¼Œå¹¶å°†æœ€å¤§ç”Ÿæˆåžåé‡æå‡è‡³ 5.76 å€ã€‚\n\nDeepSeek-V2 åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•å’Œå¼€æ”¾å¼ç”Ÿæˆè¯„ä¼°ä¸­è¡¨çŽ°å‡ºè‰²ã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/dolphin-mixtral-8x22b","frontmatter":{"title":"Dolphin 2.9.2 Mixtral 8x22B ðŸ¬","meta_title":"Dolphin 2.9.2 Mixtral 8x22B ðŸ¬","description":"Dolphin 2.9.2 Mixtral 8x22B ðŸ¬","date":"2024-06-08T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"cognitivecomputations","tags":["Natural Language Processing","Generative AI","Chatbots","Roleplay","Ethics"],"draft":false,"id":"dolphin-mixtral-8x22b","context":65536,"input":9e-7,"output":9e-7,"img":0,"request":0,"last_updated":"2024-11-04T12:49:50.000Z","slug":"models/dolphin-mixtral-8x22b"},"content":"\nDolphin 2.9 æ—¨åœ¨è¿›è¡ŒæŒ‡ä»¤è·Ÿéšã€å¯¹è¯å’Œç¼–ç ã€‚è¯¥æ¨¡åž‹æ˜¯ [Mixtral 8x22B Instruct](/mistralai/mixtral-8x22b-instruct) çš„å¾®è°ƒç‰ˆæœ¬ã€‚å®ƒå…·æœ‰ 64k çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œå¹¶ä½¿ç”¨ ChatML æ¨¡æ¿è¿›è¡Œäº† 16k åºåˆ—é•¿åº¦çš„å¾®è°ƒã€‚\n\nè¯¥æ¨¡åž‹æ˜¯ [Dolphin Mixtral 8x7B](/cognitivecomputations/dolphin-mixtral-8x7b) çš„ç»§ä»»è€…ã€‚\n\nè¯¥æ¨¡åž‹æœªç»è¿‡å®¡æŸ¥ï¼Œå¹¶åŽ»é™¤äº†å¯¹é½å’Œåè§ã€‚å®ƒéœ€è¦å¤–éƒ¨å¯¹é½å±‚ä»¥ç¡®ä¿ä¼¦ç†ä½¿ç”¨ã€‚ç”¨æˆ·è¢«è­¦å‘Šè¦è´Ÿè´£ä»»åœ°ä½¿ç”¨è¿™ä¸ªé«˜åº¦åˆè§„çš„æ¨¡åž‹ï¼Œè¯¦ç»†ä¿¡æ¯è¯·å‚è§å…³äºŽæœªå®¡æŸ¥æ¨¡åž‹çš„åšå®¢æ–‡ç«  [erichartford.com/uncensored-models](https://erichartford.com/uncensored-models)ã€‚\n\n#moe #uncensored\n\n"},{"lang":"zh","group":"models","slug":"models/dolphin-mixtral-8x7b","frontmatter":{"title":"Dolphin 2.6 Mixtral 8x7B ðŸ¬","meta_title":"Dolphin 2.6 Mixtral 8x7B ðŸ¬","description":"Dolphin 2.6 Mixtral 8x7B ðŸ¬","date":"2023-12-21T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"cognitivecomputations","tags":["Programming","Natural Language Processing","Generative AI","Ethics","Chatbots"],"draft":false,"id":"dolphin-mixtral-8x7b","context":32768,"input":5e-7,"output":5e-7,"img":0,"request":0,"last_updated":"2024-11-04T12:52:28.000Z","slug":"models/dolphin-mixtral-8x7b"},"content":"\nè¿™æ˜¯å¯¹ [Mixtral-8x7b](/mistralai/mixtral-8x7b) çš„ 16k ä¸Šä¸‹æ–‡å¾®è°ƒã€‚ç”±äºŽå¤§é‡ä½¿ç”¨ç¼–ç æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå®ƒåœ¨ç¼–ç ä»»åŠ¡ä¸­è¡¨çŽ°å‡ºè‰²ï¼Œå¹¶ä»¥å…¶æœä»Žæ€§è€Œé—»åï¼Œå°½ç®¡ç¼ºä¹ DPO è°ƒä¼˜ã€‚\n\nè¯¥æ¨¡åž‹æœªç»è¿‡å®¡æŸ¥ï¼Œå¹¶ä¸”åŽ»é™¤äº†å¯¹é½å’Œåè§ã€‚å®ƒéœ€è¦ä¸€ä¸ªå¤–éƒ¨å¯¹é½å±‚ä»¥ç¡®ä¿ä¼¦ç†ä½¿ç”¨ã€‚ç”¨æˆ·è¢«æé†’è¦è´Ÿè´£ä»»åœ°ä½¿ç”¨è¿™ä¸ªé«˜åº¦åˆè§„çš„æ¨¡åž‹ï¼Œå…·ä½“ç»†èŠ‚å¯å‚è§å…³äºŽæœªå®¡æŸ¥æ¨¡åž‹çš„åšå®¢æ–‡ç«  [erichartford.com/uncensored-models](https://erichartford.com/uncensored-models)ã€‚\n\n#moe #uncensored\n\n"},{"lang":"zh","group":"models","slug":"models/eva-qwen-25-14b","frontmatter":{"title":"EVA Qwen2.5 14B","meta_title":"EVA Qwen2.5 14B","description":"EVA Qwen2.5 14B","date":"2024-09-30T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"eva-unit-01","tags":["Roleplay","Programming","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"is_recommended":false,"id":"eva-qwen-2.5-14b","context":32768,"input":2.5e-7,"output":5e-7,"img":0,"request":0,"last_updated":"2024-11-14T08:18:57.000Z","slug":"models/eva-qwen-25-14b"},"content":"\nä¸€ä¸ªä¸“æ³¨äºŽè§’è‰²æ‰®æ¼”å’Œåˆ›æ„å†™ä½œçš„æ¨¡åž‹ï¼Œè¯¥æ¨¡åž‹åŸºäºŽ Qwen2.5-14Bï¼Œç»è¿‡åˆæˆæ•°æ®å’Œè‡ªç„¶æ•°æ®çš„æ··åˆå¾®è°ƒã€‚\n\nå®ƒåœ¨ 1.5M ä»¤ç‰Œçš„è§’è‰²æ‰®æ¼”æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨ 1.5M ä»¤ç‰Œçš„åˆæˆæ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/eva-qwen-25-32b","frontmatter":{"title":"Eva Qwen2.5 32B","meta_title":"Eva Qwen2.5 32B","description":"Eva Qwen2.5 32B","date":"2024-11-08T22:27:27.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"eva-unit-01","tags":["Roleplay","Programming","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"is_recommended":true,"id":"eva-qwen-2.5-32b","context":32000,"input":5e-7,"output":5e-7,"img":0,"request":0,"last_updated":"2024-11-14T08:18:50.000Z","slug":"models/eva-qwen-25-32b"},"content":"\nä¸€ä¸ªè§’è‰²æ‰®æ¼”/æ•…äº‹åˆ›ä½œä¸“ç”¨æ¨¡åž‹ï¼Œé’ˆå¯¹åˆæˆæ•°æ®å’Œè‡ªç„¶æ•°æ®çš„æ··åˆè¿›è¡Œå…¨å‚æ•°å¾®è°ƒçš„Qwen2.5-32Bã€‚\n\nå®ƒä½¿ç”¨Celeste 70B 0.1æ•°æ®æ··åˆï¼Œæžå¤§åœ°æ‰©å±•äº†æ•°æ®ï¼Œæé«˜äº†æ¨¡åž‹çš„å¤šæ ·æ€§ã€åˆ›é€ åŠ›å’Œâ€œé£Žå‘³â€ã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/gemini-flash-15-8b-exp","frontmatter":{"title":"Google: Gemini Flash 8B 1.5 Experimental","meta_title":"Google: Gemini Flash 8B 1.5 Experimental","description":"Google: Gemini Flash 8B 1.5 Experimental","date":"2024-08-28T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text image 2 text"],"author":"google","tags":["Technology","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"gemini-flash-1.5-8b-exp","context":1000000,"input":0,"output":0,"img":0,"request":0,"last_updated":"2024-11-11T03:14:22.000Z","slug":"models/gemini-flash-15-8b-exp"},"content":"\nGemini 1.5 Flash 8B Experimental æ˜¯ [Gemini 1.5 Flash](/google/gemini-flash-1.5) æ¨¡åž‹çš„å®žéªŒæ€§ 8B å‚æ•°ç‰ˆæœ¬ã€‚\n\nä½¿ç”¨ Gemini éœ€éµå¾ª Google çš„ [Gemini ä½¿ç”¨æ¡æ¬¾](https://ai.google.dev/terms)ã€‚\n\n#multimodal\n\næ³¨æ„ï¼šè¯¥æ¨¡åž‹ç›®å‰å¤„äºŽå®žéªŒé˜¶æ®µï¼Œä¸é€‚åˆç”Ÿäº§ä½¿ç”¨æ¡ˆä¾‹ï¼Œå¯èƒ½ä¼šå—åˆ°ä¸¥æ ¼çš„é€ŸçŽ‡é™åˆ¶ã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/gemini-flash-15-8b","frontmatter":{"title":"Google: Gemini 1.5 Flash-8B","meta_title":"Google: Gemini 1.5 Flash-8B","description":"Google: Gemini 1.5 Flash-8B","date":"2024-10-03T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text image 2 text"],"author":"google","tags":["Programming","Natural Language Processing","Chatbots","Translation","Technology/Web"],"draft":false,"id":"gemini-flash-1.5-8b","context":1000000,"input":3.75e-8,"output":1.5e-7,"img":0,"request":0,"last_updated":"2024-10-03T00:00:00.000Z","slug":"models/gemini-flash-15-8b"},"content":"\nGemini 1.5 Flash-8B é’ˆå¯¹é€Ÿåº¦å’Œæ•ˆçŽ‡è¿›è¡Œäº†ä¼˜åŒ–ï¼Œåœ¨èŠå¤©ã€è½¬å½•å’Œç¿»è¯‘ç­‰å°æç¤ºä»»åŠ¡ä¸­æä¾›äº†å¢žå¼ºçš„æ€§èƒ½ã€‚é€šè¿‡å‡å°‘å»¶è¿Ÿï¼Œå®ƒåœ¨å®žæ—¶å’Œå¤§è§„æ¨¡æ“ä½œä¸­éžå¸¸æœ‰æ•ˆã€‚è¯¥æ¨¡åž‹ä¸“æ³¨äºŽæˆæœ¬æ•ˆç›Šè§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶ä¿æŒé«˜è´¨é‡çš„ç»“æžœã€‚\n\n[ç‚¹å‡»æ­¤å¤„äº†è§£æ›´å¤šå…³äºŽæ­¤æ¨¡åž‹çš„ä¿¡æ¯](https://developers.googleblog.com/en/gemini-15-flash-8b-is-now-generally-available-for-use/)ã€‚\n\nä½¿ç”¨ Gemini å—é™äºŽ Google's [Gemini ä½¿ç”¨æ¡æ¬¾](https://ai.google.dev/terms)ã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/gemini-flash-15","frontmatter":{"title":"Google: Gemini Flash 1.5","meta_title":"Google: Gemini Flash 1.5","description":"Google: Gemini Flash 1.5","date":"2024-05-14T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text image 2 text"],"author":"google","tags":["Programming","Machine Learning","Natural Language Processing","Computer Vision","Chatbots"],"draft":false,"is_recommended":true,"id":"gemini-flash-1.5","context":1000000,"input":7.5e-8,"output":3e-7,"img":0.00004,"request":0,"last_updated":"2024-11-14T08:23:12.000Z","slug":"models/gemini-flash-15"},"content":"\nGemini 1.5 Flash æ˜¯ä¸€ä¸ªåŸºç¡€æ¨¡åž‹ï¼Œåœ¨è§†è§‰ç†è§£ã€åˆ†ç±»ã€æ‘˜è¦ä»¥åŠä»Žå›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘åˆ›å»ºå†…å®¹ç­‰å¤šç§å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨çŽ°å‡ºè‰²ã€‚å®ƒæ“…é•¿å¤„ç†è§†è§‰å’Œæ–‡æœ¬è¾“å…¥ï¼Œä¾‹å¦‚ç…§ç‰‡ã€æ–‡æ¡£ã€ä¿¡æ¯å›¾å’Œæˆªå›¾ã€‚\n\nGemini 1.5 Flash æ—¨åœ¨å¤„ç†é«˜å®¹é‡ã€é«˜é¢‘çŽ‡çš„ä»»åŠ¡ï¼Œå…¶ä¸­æˆæœ¬å’Œå»¶è¿Ÿè‡³å…³é‡è¦ã€‚åœ¨å¤§å¤šæ•°å¸¸è§ä»»åŠ¡ä¸­ï¼ŒFlash çš„è´¨é‡ä¸Žå…¶ä»– Gemini Pro æ¨¡åž‹ç›¸å½“ï¼Œä½†æˆæœ¬æ˜¾è‘—é™ä½Žã€‚Flash éžå¸¸é€‚åˆç”¨äºŽèŠå¤©åŠ©æ‰‹å’ŒæŒ‰éœ€å†…å®¹ç”Ÿæˆç­‰å¯¹é€Ÿåº¦å’Œè§„æ¨¡æœ‰è¦æ±‚çš„åº”ç”¨ã€‚\n\nä½¿ç”¨ Gemini å—åˆ¶äºŽ Google çš„ [Gemini ä½¿ç”¨æ¡æ¬¾](https://ai.google.dev/terms)ã€‚\n\n#multimodal\n\n"},{"lang":"zh","group":"models","slug":"models/gemini-pro-15","frontmatter":{"title":"Google: Gemini Pro 1.5","meta_title":"Google: Gemini Pro 1.5","description":"Google: Gemini Pro 1.5","date":"2024-04-09T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text image 2 text"],"author":"google","tags":["Programming","Natural Language Processing","Machine Learning","Generative AI","Chatbots"],"draft":false,"id":"gemini-pro-1.5","context":2000000,"input":0.00000125,"output":0.000005,"img":0.00263,"request":0,"last_updated":"2024-04-09T00:00:00.000Z","slug":"models/gemini-pro-15"},"content":"\nè°·æ­Œæœ€æ–°çš„å¤šæ¨¡æ€æ¨¡åž‹ï¼Œæ”¯æŒåœ¨æ–‡æœ¬æˆ–èŠå¤©æç¤ºä¸­ä½¿ç”¨å›¾åƒå’Œè§†é¢‘ã€‚\n\né’ˆå¯¹ä»¥ä¸‹è¯­è¨€ä»»åŠ¡è¿›è¡Œäº†ä¼˜åŒ–ï¼š\n\n- ä»£ç ç”Ÿæˆ\n- æ–‡æœ¬ç”Ÿæˆ\n- æ–‡æœ¬ç¼–è¾‘\n- é—®é¢˜è§£å†³\n- æŽ¨è\n- ä¿¡æ¯æå–\n- æ•°æ®æå–æˆ–ç”Ÿæˆ\n- AIä»£ç†\n\nGeminiçš„ä½¿ç”¨å—é™äºŽè°·æ­Œçš„[Geminiä½¿ç”¨æ¡æ¬¾](https://ai.google.dev/terms)ã€‚\n\n#multimodal\n\n"},{"lang":"zh","group":"models","slug":"models/gemini-pro-vision","frontmatter":{"title":"Google: Gemini Pro Vision 1.0","meta_title":"Google: Gemini Pro Vision 1.0","description":"Google: Gemini Pro Vision 1.0","date":"2023-12-13T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text image 2 text"],"author":"google","tags":["Programming","Machine Learning","Natural Language Processing","Computer Vision","Generative AI"],"draft":false,"id":"gemini-pro-vision","context":16384,"input":5e-7,"output":0.0000015,"img":0.0025,"request":0,"last_updated":"2024-11-11T03:15:08.000Z","slug":"models/gemini-pro-vision"},"content":"\nè°·æ­Œçš„æ——èˆ°å¤šæ¨¡æ€æ¨¡åž‹ï¼Œæ”¯æŒåœ¨æ–‡æœ¬æˆ–èŠå¤©æç¤ºä¸­ä½¿ç”¨å›¾åƒå’Œè§†é¢‘ï¼Œä»¥èŽ·å¾—æ–‡æœ¬æˆ–ä»£ç å“åº”ã€‚\n\nè¯·å‚é˜…æ¥è‡ª [Deepmind](https://deepmind.google/technologies/gemini/) çš„åŸºå‡†å’Œæç¤ºæŒ‡å—ã€‚\n\nä½¿ç”¨ Gemini éœ€éµå¾ªè°·æ­Œçš„ [Gemini ä½¿ç”¨æ¡æ¬¾](https://ai.google.dev/terms)ã€‚\n\n#multimodal\n\n"},{"lang":"zh","group":"models","slug":"models/gemma-2-27b-it","frontmatter":{"title":"Google: Gemma 2 27B","meta_title":"Google: Gemma 2 27B","description":"Google: Gemma 2 27B","date":"2024-07-13T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text 2 text"],"author":"google","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"gemma-2-27b-it","context":8192,"input":2.7e-7,"output":2.7e-7,"img":0,"request":0,"last_updated":"2024-11-11T03:14:49.000Z","slug":"models/gemma-2-27b-it"},"content":"\nGemma 2 27B ç”± Google å¼€å‘ï¼Œæ˜¯ä¸€ä¸ªå¼€æ”¾æ¨¡åž‹ï¼ŒåŸºäºŽåˆ›å»º [Gemini æ¨¡åž‹](/models?q=gemini) æ‰€ä½¿ç”¨çš„ç›¸åŒç ”ç©¶å’ŒæŠ€æœ¯ã€‚\n\nGemma æ¨¡åž‹éžå¸¸é€‚åˆå¤šç§æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼ŒåŒ…æ‹¬é—®ç­”ã€æ‘˜è¦å’ŒæŽ¨ç†ã€‚\n\næœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [å‘å¸ƒå…¬å‘Š](https://blog.google/technology/developers/google-gemma-2/)ã€‚ä½¿ç”¨ Gemma éœ€éµå¾ª Google çš„ [Gemma ä½¿ç”¨æ¡æ¬¾](https://ai.google.dev/gemma/terms)ã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/gemma-2-9b-it","frontmatter":{"title":"Google: Gemma 2 9B","meta_title":"Google: Gemma 2 9B","description":"Google: Gemma 2 9B","date":"2024-06-28T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text 2 text"],"author":"google","tags":["Programming","Natural Language Processing","Machine Learning","Data Science","Open Source"],"draft":false,"id":"gemma-2-9b-it","context":8192,"input":6e-8,"output":6e-8,"img":0,"request":0,"last_updated":"2024-11-11T03:14:49.000Z","slug":"models/gemma-2-9b-it"},"content":"\nGemma 2 9B by Google æ˜¯ä¸€ä¸ªå…ˆè¿›çš„å¼€æºè¯­è¨€æ¨¡åž‹ï¼Œåœ¨å…¶å°ºå¯¸ç±»åˆ«ä¸­è®¾å®šäº†æ•ˆçŽ‡å’Œæ€§èƒ½çš„æ–°æ ‡å‡†ã€‚\n\nå®ƒæ—¨åœ¨æ”¯æŒå„ç§ä»»åŠ¡ï¼Œä½¿å¼€å‘è€…å’Œç ”ç©¶äººå‘˜èƒ½å¤Ÿæž„å»ºåˆ›æ–°åº”ç”¨ï¼ŒåŒæ—¶ä¿æŒå¯è®¿é—®æ€§ã€å®‰å…¨æ€§å’Œç»æµŽæ€§ã€‚\n\næœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§ [launch announcement](https://blog.google/technology/developers/google-gemma-2/)ã€‚ä½¿ç”¨ Gemma éœ€éµå¾ª Google's [Gemma Terms of Use](https://ai.google.dev/gemma/terms)ã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/glm-4-air","frontmatter":{"title":"GLM-4 Air","meta_title":"GLM-4 Air","description":"GLM-4 Air","date":"2024-11-14T10:21:13.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"glm-4-air","tags":["Programming","Technology","Machine Learning","Data Science","Generative AI"],"draft":false,"is_recommended":false,"id":"glm-4-air","context":128000,"input":1.4e-7,"output":1.4e-7,"img":0,"request":0,"last_updated":"2024-11-14T12:36:34.000Z","slug":"models/glm-4-air"},"content":"\n\n\n"},{"lang":"zh","group":"models","slug":"models/glm-4-airx","frontmatter":{"title":"GLM-4 AirX","meta_title":"GLM-4 AirX","description":"GLM-4 AirX","date":"2024-11-15T13:12:54.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"glm-4-airx","tags":["Technology","Machine Learning","Generative AI","Data Science","Chatbots"],"draft":false,"is_recommended":false,"id":"glm-4-airx","context":8000,"input":0.0000014,"output":0.0000014,"img":0,"request":0,"last_updated":"2024-11-15T23:49:16.000Z","slug":"models/glm-4-airx"},"content":"\n## åŸºæœ¬ä¿¡æ¯\n\nâ€œGLM-4-AIRXâ€æ˜¯ç”±äººå·¥æ™ºèƒ½é¢†åŸŸçš„ä¸“å®¶å¼€å‘çš„å…ˆè¿›å¤§åž‹è¯­è¨€æ¨¡åž‹ã€‚å®ƒå› å…¶å¼ºå¤§çš„è‡ªç„¶è¯­è¨€å¤„ç†èƒ½åŠ›è€Œé—»åï¼Œä½¿å…¶èƒ½å¤Ÿæœ‰æ•ˆç†è§£å’Œç”Ÿæˆè‡ªç„¶è¯­è¨€æ–‡æœ¬ã€‚è¯¥æ¨¡åž‹åˆ©ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯å¹¿æ³›ç”¨äºŽNLPï¼ˆè‡ªç„¶è¯­è¨€å¤„ç†ï¼‰é¢†åŸŸçš„Transformeræž¶æž„ã€‚\n\n## æŠ€æœ¯ç‰¹ç‚¹\n\n### 1. åŸºäºŽTransformeræž¶æž„\n\nè¯¥æ¨¡åž‹çš„æ ¸å¿ƒåŸºäºŽTransformeræž¶æž„ï¼Œè¿™æ˜¯ä¸€ç§é‡‡ç”¨æ³¨æ„åŠ›æœºåˆ¶çš„ç»“æž„ã€‚é€šè¿‡è¿™ä¸€æœºåˆ¶ï¼Œâ€œGLM-4-AIRXâ€èƒ½å¤Ÿæ•æ‰è¾“å…¥åºåˆ—ä¸­ä»»æ„ä¸¤ä¸ªä½ç½®ä¹‹é—´çš„å¤æ‚ä¾èµ–å…³ç³»ï¼Œæ— è®ºå®ƒä»¬ä¹‹é—´çš„è·ç¦»å¦‚ä½•ï¼Œä»Žè€Œæé«˜å¤„ç†è¯­è¨€ä»»åŠ¡æ—¶çš„æ•ˆçŽ‡å’Œå‡†ç¡®æ€§ã€‚\n\n### 2. é¢„è®­ç»ƒä¸Žå¾®è°ƒ\n\næ¨¡åž‹å¼€å‘åŒ…æ‹¬é¢„è®­ç»ƒå’Œå¾®è°ƒä¸¤ä¸ªé˜¶æ®µã€‚åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œå®ƒåœ¨å¹¿æ³›å¤šæ ·çš„æ–‡æœ¬æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»¥æŽŒæ¡åŸºæœ¬çš„è¯­è¨€æ¨¡å¼ã€‚åœ¨å¾®è°ƒé˜¶æ®µï¼Œæ ¹æ®ç‰¹å®šä»»åŠ¡è°ƒæ•´å‚æ•°ï¼Œä»¥æé«˜åœ¨ç‰¹å®šåº”ç”¨ä¸Šçš„æ€§èƒ½ã€‚\n\n### 3. å¤šè¯­è¨€æ”¯æŒ\n\nâ€œGLM-4-AIRXâ€å…·å¤‡å¤šè¯­è¨€èƒ½åŠ›ï¼Œé€šè¿‡åœ¨åŒ…å«å¤šç§è¯­è¨€çš„è¯­æ–™åº“ä¸Šè¿›è¡Œé¢„è®­ç»ƒå®žçŽ°ã€‚è¿™ä½¿å…¶èƒ½å¤Ÿç†è§£å’Œç”Ÿæˆå¤šç§è¯­è¨€çš„æ–‡æœ¬ï¼Œæ»¡è¶³å…¨çƒç”¨æˆ·éœ€æ±‚ï¼Œå¹¶ä¿ƒè¿›è·¨æ–‡åŒ–äº¤æµã€‚\n\n### 4. å¯æ‰©å±•æ€§\n\nè¯¥æ¨¡åž‹åœ¨è®¾è®¡æ—¶è€ƒè™‘äº†å¯æ‰©å±•æ€§ï¼Œèƒ½å¤Ÿçµæ´»è°ƒæ•´ä»¥å¤„ç†æ›´å¤§æ•°æ®é›†æˆ–æ›´å¤æ‚çš„ä»»åŠ¡éœ€æ±‚ã€‚æ­¤ç‰¹æ€§ä½¿å…¶èƒ½å¤Ÿå¹¿æ³›åº”ç”¨äºŽä¸æ–­å˜åŒ–å’Œæ‰©å±•çš„æ•°æ®çŽ¯å¢ƒä¸­ã€‚\n\n## åº”ç”¨åœºæ™¯\n\nâ€œGLM-4-AIRXâ€åœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºŽï¼š\n\n- **æ–‡æœ¬ç”Ÿæˆ**ï¼šè‡ªåŠ¨åˆ›å»ºæ–‡ç« ã€æ•…äº‹æˆ–è¯—æ­Œã€‚\n- **æ–‡æœ¬åˆ†ç±»**ï¼šä¾‹å¦‚æƒ…æ„Ÿåˆ†æžæˆ–ä¸»é¢˜åˆ†ç±»ã€‚\n- **æœºå™¨ç¿»è¯‘**ï¼šæä¾›é«˜æ•ˆå‡†ç¡®çš„è·¨è¯­è¨€ç¿»è¯‘ã€‚\n- **é—®ç­”ç³»ç»Ÿ**ï¼šç”¨äºŽæž„å»ºæ™ºèƒ½å¹³å°ï¼Œç›´æŽ¥å“åº”ç”¨æˆ·æŸ¥è¯¢ã€‚\n- **æ–‡æœ¬æ‘˜è¦**ï¼šè‡ªåŠ¨ä»Žæ–‡æ¡£ä¸­æå–å…³é”®ä¿¡æ¯å¹¶ç”Ÿæˆç®€æ˜Žæ‘˜è¦ã€‚\n\n## ä¸Žç±»ä¼¼æ¨¡åž‹çš„æ¯”è¾ƒ\n\nä¸Žå…¶ä»–å¤§åž‹è¯­è¨€æ¨¡åž‹ç›¸æ¯”ï¼Œâ€œGLM-4-AIRXâ€æä¾›äº†å‡ ä¸ªä¼˜åŠ¿ï¼š\n\n- **å“è¶Šçš„æ€§èƒ½**ï¼šåœ¨æ–‡æœ¬ç”Ÿæˆå’Œç†è§£ç­‰å¤šä¸ªNLPä»»åŠ¡ä¸­è¡¨çŽ°å‡ºè‰²ã€‚\n- **é«˜å¤„ç†æ•ˆçŽ‡**ï¼šä¼˜åŒ–çš„æž¶æž„è®¾è®¡ä½¿èµ„æºä½¿ç”¨å’Œé€Ÿåº¦æ›´ä¸ºé«˜æ•ˆã€‚\n- **å¼ºå¤§çš„çµæ´»æ€§**ï¼šæ”¯æŒå¤šç§è¯­è¨€ï¼Œå¹¶èƒ½å¤Ÿé€šè¿‡æ‰©å±•åŠŸèƒ½é€‚åº”ä¸åŒçš„åº”ç”¨åœºæ™¯ã€‚\n- **ä»»åŠ¡é€‚åº”æ€§**ï¼šé€šè¿‡å¾®è°ƒå®žçŽ°ç‰¹å®šä»»åŠ¡éœ€æ±‚çš„ä¼˜åŒ–ï¼Œå¢žå¼ºå®žé™…ä»·å€¼ã€‚\n\næ€»ä¹‹ï¼Œâ€œGLM-4-AIRXâ€æ˜¯ä¸€ä¸ªå…¨é¢ä¸”çµæ´»çš„å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼Œå…·æœ‰åœ¨å½“ä»Šå¿«é€Ÿå‘å±•çš„äººå·¥æ™ºèƒ½æŠ€æœ¯é¢†åŸŸä¸­æŒç»­å¢žé•¿å’Œåˆ›æ–°çš„æ½œåŠ›ã€‚\n\næ­¤å“åº”ä½¿ç”¨äº†å…³äºŽGLMæ¨¡åž‹çš„å®žæ—¶æ•°æ®ï¼Œæ”¯æŒç³»ç»Ÿæç¤ºã€å‡½æ•°è°ƒç”¨ã€æ£€ç´¢ã€ç½‘ç»œæœç´¢ç­‰æ–°åŠŸèƒ½ï¼Œä»¥åŠå…³äºŽNLPè¿›å±•ä¸­å¸¸è§çš„åŸºäºŽTransformerçš„æž¶æž„çš„ä¸€èˆ¬çŸ¥è¯†ã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/glm-4-flash","frontmatter":{"title":"glm-4-flash","meta_title":"glm-4-flash","description":"glm-4-flash","date":"2024-11-15T12:53:10.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"glm-4-flash","tags":["Generative AI","Machine Learning","Natural Language Processing","Technology","Chatbots"],"draft":false,"is_recommended":false,"id":"glm-4-flash","context":128000,"input":1e-8,"output":1e-8,"img":0,"request":0,"last_updated":"2024-11-15T23:22:37.000Z","slug":"models/glm-4-flash"},"content":"\n# æ™ºè°±å¿«é€Ÿç‰ˆ\n\n## åŠŸèƒ½ç‰¹æ€§\n\n- å¿«é€Ÿç”ŸæˆæŠ¥å‘Š\n- æ”¯æŒå¤šç§æ•°æ®æ ¼å¼\n- ç›´è§‚çš„ç”¨æˆ·ç•Œé¢\n\n## ä½¿ç”¨æ–¹æ³•\n\n1. å®‰è£…ä¾èµ–\n   ```bash\n   npm install\n   ```\n2. å¯åŠ¨åº”ç”¨\n   ```bash\n   npm start\n   ```\n\n## æ³¨æ„äº‹é¡¹\n\n- è¯·ç¡®ä¿æ‚¨ä½¿ç”¨çš„æ˜¯æœ€æ–°ç‰ˆæœ¬çš„ Node.js\n- æ•°æ®æºå¿…é¡»ç¬¦åˆæŒ‡å®šæ ¼å¼\n\n## ç¤ºä¾‹ä»£ç \n\n```python\ndef hello_world():\n    print(\"Hello, World!\")\n```\n\n"},{"lang":"zh","group":"models","slug":"models/glm-4-long","frontmatter":{"title":"glm-4-long","meta_title":"glm-4-long","description":"glm-4-long","date":"2024-11-15T12:53:01.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"glm-4-long","tags":["Technology","Machine Learning","Natural Language Processing","Data Science","Generative AI"],"draft":false,"is_recommended":false,"id":"glm-4-long","context":1000000,"input":1.4e-7,"output":1.4e-7,"img":0,"request":0,"last_updated":"2024-11-15T23:22:49.000Z","slug":"models/glm-4-long"},"content":"\n# æ™ºè°±ç™¾ä¸‡ä¸Šä¸‹æ–‡\n\n## ç®€ä»‹\n\næ™ºè°±ç™¾ä¸‡ä¸Šä¸‹æ–‡æ˜¯ä¸€ä¸ªå¼ºå¤§çš„å·¥å…·ï¼Œæ—¨åœ¨å¸®åŠ©ç”¨æˆ·åœ¨æµ·é‡æ•°æ®ä¸­æå–æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚\n\n## ç‰¹æ€§\n\n- é«˜æ•ˆçš„æ•°æ®å¤„ç†èƒ½åŠ›\n- æ”¯æŒå¤šç§æ•°æ®æ ¼å¼\n- ç›´è§‚çš„ç”¨æˆ·ç•Œé¢\n\n## ä½¿ç”¨æ–¹æ³•\n\n```python\nimport zhipu\n\n# åˆå§‹åŒ–æ™ºè°±ç™¾ä¸‡ä¸Šä¸‹æ–‡\ncontext = zhipu.Context()\n\n# æ·»åŠ æ•°æ®\ncontext.add_data(\"ä½ çš„æ•°æ®\")\n\n# èŽ·å–ç»“æžœ\nresult = context.get_results()\n```\n\n## ç»“è®º\n\næ™ºè°±ç™¾ä¸‡ä¸Šä¸‹æ–‡ä¸ºç”¨æˆ·æä¾›äº†ä¸€ä¸ªé«˜æ•ˆã€ä¾¿æ·çš„æ•°æ®åˆ†æžè§£å†³æ–¹æ¡ˆã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/glm-4-plus","frontmatter":{"title":"glm-4-plus","meta_title":"glm-4-plus","description":"glm-4-plus","date":"2024-11-15T12:53:05.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"glm-4-plus","tags":["Generative AI","Machine Learning","Natural Language Processing","Technology","Chatbots"],"draft":false,"is_recommended":false,"id":"glm-4-plus","context":128000,"input":0.000007,"output":0.000007,"img":0,"request":0,"last_updated":"2024-11-15T23:22:43.000Z","slug":"models/glm-4-plus"},"content":"\n# æ™ºè°±æ——èˆ°ç‰ˆ\n\n## åŠŸèƒ½ç‰¹ç‚¹\n\n- é«˜çº§æ•°æ®åˆ†æž\n- å®žæ—¶ç›‘æŽ§\n- è‡ªå®šä¹‰æŠ¥å‘Šç”Ÿæˆ\n\n## å®‰è£…æŒ‡å—\n\nè¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤å®‰è£…æ™ºè°±æ——èˆ°ç‰ˆï¼š\n\n1. ä¸‹è½½å®‰è£…åŒ…\n2. åŒå‡»è¿è¡Œå®‰è£…ç¨‹åº\n3. æŒ‰ç…§æç¤ºå®Œæˆå®‰è£…\n\n```bash\n# è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹ä»£ç å—\necho \"Hello, World!\"\n```\n\n"},{"lang":"zh","group":"models","slug":"models/glm-4","frontmatter":{"title":"GLM-4","meta_title":"GLM-4","description":"GLM-4","date":"2024-11-15T13:12:41.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"glm-4","tags":["Generative AI","Machine Learning","Natural Language Processing","Technology","Chatbots"],"draft":false,"is_recommended":false,"id":"glm-4","context":128000,"input":0.000014,"output":0.000014,"img":0,"request":0,"last_updated":"2024-11-15T23:23:43.000Z","slug":"models/glm-4"},"content":"\n# æ™ºè°±æœ€å¼ºç‰ˆ\n\n## ç‰¹æ€§\n\n- é«˜æ•ˆèƒ½\n- å…¼å®¹æ€§å¼º\n- é€‚åº”æ€§å¹¿\n\n## å®‰è£…\n\n```bash\npip install zhipu\n```\n\n## ä½¿ç”¨ç¤ºä¾‹\n\n```python\nimport zhupu\n\n# åˆå§‹åŒ–\nzhupu.init()\n\n# èŽ·å–æ•°æ®\ndata = zhupu.get_data()\n```\n\n"},{"lang":"zh","group":"models","slug":"models/glm-4v-plus","frontmatter":{"title":"glm-4v-plus","meta_title":"glm-4v-plus","description":"glm-4v-plus","date":"2024-11-15T12:53:06.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"glm-4v-plus","tags":["Computer Vision","Technology","Machine Learning","Data Science","Generative AI"],"draft":false,"is_recommended":false,"id":"glm-4v-plus","context":32000,"input":0.0000014,"output":0.0000014,"img":0,"request":0,"last_updated":"2024-11-15T23:22:53.000Z","slug":"models/glm-4v-plus"},"content":"\n# æ™ºè°±æœ€æ–°å›¾åƒè¯†åˆ«\n\n## ç‰¹æ€§\n\n- é«˜ç²¾åº¦\n- å¿«é€Ÿå¤„ç†\n- å¤šç§åº”ç”¨åœºæ™¯\n\n## ä½¿ç”¨ç¤ºä¾‹\n\n```python\nimport image_recognition\n\nresult = image_recognition.detect_objects(\"image.jpg\")\nprint(result)\n```\n\n"},{"lang":"zh","group":"models","slug":"models/gpt-35-turbo-instruct","frontmatter":{"title":"OpenAI: GPT-3.5 Turbo Instruct","meta_title":"OpenAI: GPT-3.5 Turbo Instruct","description":"OpenAI: GPT-3.5 Turbo Instruct","date":"2023-09-28T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"openai","tags":["Programming","Natural Language Processing","Generative AI","Chatbots","Technology/Web"],"draft":false,"id":"gpt-3.5-turbo-instruct","context":4095,"input":0.0000015,"output":0.000002,"img":0,"request":0,"last_updated":"2023-09-28T00:00:00.000Z","slug":"models/gpt-35-turbo-instruct"},"content":"\nè¯¥æ¨¡åž‹æ˜¯GPT-3.5 Turboçš„ä¸€ä¸ªå˜ä½“ï¼Œé’ˆå¯¹æ•™å­¦æç¤ºè¿›è¡Œäº†è°ƒæ•´ï¼Œå¹¶çœç•¥äº†ä¸ŽèŠå¤©ç›¸å…³çš„ä¼˜åŒ–ã€‚è®­ç»ƒæ•°æ®ï¼šæˆªè‡³2021å¹´9æœˆã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/gpt-4o-mini","frontmatter":{"title":"OpenAI: GPT-4o-mini","meta_title":"OpenAI: GPT-4o-mini","description":"OpenAI: GPT-4o-mini","date":"2024-07-18T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text image 2 text"],"author":"openai","tags":["Programming","Technology","Programming/Scripting","Technology/Web"],"draft":false,"is_recommended":true,"id":"gpt-4o-mini","context":128000,"input":1.5e-7,"output":6e-7,"img":0.007225,"request":0,"last_updated":"2024-11-14T05:09:26.000Z","slug":"models/gpt-4o-mini"},"content":"\nGPT-4o mini æ˜¯ OpenAI åœ¨ [GPT-4 Omni](/openai/gpt-4o) ä¹‹åŽæŽ¨å‡ºçš„æœ€æ–°æ¨¡åž‹ï¼Œæ”¯æŒæ–‡æœ¬å’Œå›¾åƒè¾“å…¥ï¼Œå¹¶ç”Ÿæˆæ–‡æœ¬è¾“å‡ºã€‚\n\nä½œä¸ºä»–ä»¬æœ€å…ˆè¿›çš„å°åž‹æ¨¡åž‹ï¼Œå®ƒçš„ä»·æ ¼æ¯”å…¶ä»–æœ€è¿‘çš„å‰æ²¿æ¨¡åž‹ä¾¿å®œäº†è®¸å¤šï¼Œä¸”æ¯” [GPT-3.5 Turbo](/openai/gpt-3.5-turbo) ä¾¿å®œè¶…è¿‡ 60%ã€‚å®ƒä¿æŒäº† SOTA æ™ºèƒ½ï¼ŒåŒæ—¶åœ¨æˆæœ¬æ•ˆç›Šä¸Šæ˜¾è‘—æé«˜ã€‚\n\nGPT-4o mini åœ¨ MMLU ä¸ŠèŽ·å¾—äº† 82% çš„åˆ†æ•°ï¼Œç›®å‰åœ¨èŠå¤©åå¥½ [å¸¸è§æŽ’è¡Œæ¦œ](https://arena.lmsys.org/) ä¸Šçš„æŽ’åé«˜äºŽ GPT-4ã€‚\n\næŸ¥çœ‹ [å‘å¸ƒå…¬å‘Š](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/) ä»¥äº†è§£æ›´å¤šä¿¡æ¯ã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/gpt-4o","frontmatter":{"title":"OpenAI: GPT-4o","meta_title":"OpenAI: GPT-4o","description":"OpenAI: GPT-4o","date":"2024-05-13T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text image 2 text"],"author":"openai","tags":["Programming","Natural Language Processing","Machine Learning","Generative AI","Computer Vision"],"draft":false,"id":"gpt-4o","context":128000,"input":0.0000025,"output":0.00001,"img":0.0036125,"request":0,"last_updated":"2024-05-13T00:00:00.000Z","slug":"models/gpt-4o"},"content":"\nGPT-4oï¼ˆâ€œoâ€ä»£è¡¨â€œå…¨èƒ½â€ï¼‰æ˜¯OpenAIæœ€æ–°çš„AIæ¨¡åž‹ï¼Œæ”¯æŒæ–‡æœ¬å’Œå›¾åƒè¾“å…¥ï¼Œå¹¶ç”Ÿæˆæ–‡æœ¬è¾“å‡ºã€‚å®ƒä¿æŒäº†[GPT-4 Turbo](/openai/gpt-4-turbo)çš„æ™ºèƒ½æ°´å¹³ï¼ŒåŒæ—¶é€Ÿåº¦æé«˜äº†ä¸¤å€ï¼Œæˆæœ¬æ•ˆç›Šæé«˜äº†50%ã€‚GPT-4oè¿˜åœ¨å¤„ç†éžè‹±è¯­è¯­è¨€å’Œå¢žå¼ºè§†è§‰èƒ½åŠ›æ–¹é¢æä¾›äº†æ›´å¥½çš„æ€§èƒ½ã€‚\n\nä¸ºäº†ä¸Žå…¶ä»–æ¨¡åž‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå®ƒæ›¾è¢«æš‚æ—¶ç§°ä¸º[\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209)\n\n"},{"lang":"zh","group":"models","slug":"models/grok-beta","frontmatter":{"title":"xAI: Grok Beta","meta_title":"xAI: Grok Beta","description":"xAI: Grok Beta","date":"2024-10-20T00:00:00.000Z","image":"https://img.rifx.online/logo/xai.svg","categories":["text 2 text"],"author":"x-ai","tags":["Natural Language Processing","Machine Learning","Generative AI","Chatbots","Data Science"],"draft":false,"id":"grok-beta","context":131072,"input":0.000005,"output":0.000015,"img":0,"request":0,"last_updated":"2024-11-07T09:32:49.000Z","slug":"models/grok-beta"},"content":"\nGrok Beta æ˜¯ xAI çš„å®žéªŒæ€§è¯­è¨€æ¨¡åž‹ï¼Œå…·æœ‰æœ€å…ˆè¿›çš„æŽ¨ç†èƒ½åŠ›ï¼Œæœ€é€‚åˆå¤æ‚å’Œå¤šæ­¥éª¤çš„ç”¨ä¾‹ã€‚\n\nå®ƒæ˜¯ [Grok 2](https://x.ai/blog/grok-2) çš„ç»§ä»»è€…ï¼Œå…·æœ‰å¢žå¼ºçš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/hermes-3-llama-31-405b","frontmatter":{"title":"Nous: Hermes 3 405B Instruct","meta_title":"Nous: Hermes 3 405B Instruct","description":"Nous: Hermes 3 405B Instruct","date":"2024-08-16T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"nousresearch","tags":["Programming","Natural Language Processing","Machine Learning","Generative AI","Chatbots"],"draft":false,"id":"hermes-3-llama-3.1-405b","context":131072,"input":0.00000179,"output":0.00000249,"img":0,"request":0,"last_updated":"2024-11-11T03:16:40.000Z","slug":"models/hermes-3-llama-31-405b"},"content":"\nHermes 3 æ˜¯ä¸€ä¸ªé€šç”¨è¯­è¨€æ¨¡åž‹ï¼Œç›¸è¾ƒäºŽ Hermes 2 æœ‰è®¸å¤šæ”¹è¿›ï¼ŒåŒ…æ‹¬å…ˆè¿›çš„ä»£ç†èƒ½åŠ›ã€æ˜¾è‘—æ›´å¥½çš„è§’è‰²æ‰®æ¼”ã€æŽ¨ç†ã€å¤šè½®å¯¹è¯ã€é•¿ä¸Šä¸‹æ–‡ä¸€è‡´æ€§ä»¥åŠå„æ–¹é¢çš„æå‡ã€‚\n\nHermes 3 405B æ˜¯ Llama-3.1 405B åŸºç¡€æ¨¡åž‹çš„å‰æ²¿çº§å…¨å‚æ•°å¾®è°ƒï¼Œä¸“æ³¨äºŽå°† LLM ä¸Žç”¨æˆ·å¯¹é½ï¼Œèµ‹äºˆç»ˆç«¯ç”¨æˆ·å¼ºå¤§çš„å¼•å¯¼èƒ½åŠ›å’ŒæŽ§åˆ¶æƒã€‚\n\nHermes 3 ç³»åˆ—åœ¨ Hermes 2 çš„èƒ½åŠ›åŸºç¡€ä¸Šè¿›è¡Œæž„å»ºå’Œæ‰©å±•ï¼ŒåŒ…æ‹¬æ›´å¼ºå¤§å’Œå¯é çš„å‡½æ•°è°ƒç”¨å’Œç»“æž„åŒ–è¾“å‡ºèƒ½åŠ›ã€é€šç”¨åŠ©æ‰‹èƒ½åŠ›ä»¥åŠæ”¹è¿›çš„ä»£ç ç”ŸæˆæŠ€èƒ½ã€‚\n\nåœ¨é€šç”¨èƒ½åŠ›æ–¹é¢ï¼ŒHermes 3 ä¸Ž Llama-3.1 æŒ‡ä»¤æ¨¡åž‹å…·æœ‰ç«žäº‰åŠ›ï¼Œç”šè‡³åœ¨æŸäº›æ–¹é¢å¯èƒ½æ›´ä¼˜ï¼Œä¸¤è€…ä¹‹é—´çš„ä¼˜ç¼ºç‚¹å„æœ‰ä¸åŒã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/hermes-3-llama-31-70b","frontmatter":{"title":"Nous: Hermes 3 70B Instruct","meta_title":"Nous: Hermes 3 70B Instruct","description":"Nous: Hermes 3 70B Instruct","date":"2024-08-18T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"nousresearch","tags":["Natural Language Processing","Machine Learning","Generative AI","Chatbots","Programming"],"draft":false,"id":"hermes-3-llama-3.1-70b","context":131072,"input":4e-7,"output":4e-7,"img":0,"request":0,"last_updated":"2024-11-11T03:16:38.000Z","slug":"models/hermes-3-llama-31-70b"},"content":"\nHermes 3 æ˜¯ä¸€ä¸ªé€šç”¨è¯­è¨€æ¨¡åž‹ï¼Œç›¸è¾ƒäºŽ [Hermes 2](/nousresearch/nous-hermes-2-mistral-7b-dpo) æœ‰è®¸å¤šæ”¹è¿›ï¼ŒåŒ…æ‹¬å…ˆè¿›çš„ä»£ç†èƒ½åŠ›ï¼Œæ›´å¥½çš„è§’è‰²æ‰®æ¼”ï¼ŒæŽ¨ç†ï¼Œå¤šè½®å¯¹è¯ï¼Œé•¿ä¸Šä¸‹æ–‡è¿žè´¯æ€§ï¼Œä»¥åŠå„æ–¹é¢çš„æ”¹è¿›ã€‚\n\nHermes 3 70B æ˜¯ä¸€ä¸ªå…·æœ‰ç«žäº‰åŠ›çš„å¾®è°ƒç‰ˆæœ¬ï¼Œç”šè‡³å¯ä»¥è¯´æ˜¯ [Llama-3.1 70B åŸºç¡€æ¨¡åž‹](/meta-llama/llama-3.1-70b-instruct) çš„ä¼˜è¶Šç‰ˆæœ¬ï¼Œä¸“æ³¨äºŽå°† LLM ä¸Žç”¨æˆ·å¯¹é½ï¼Œèµ‹äºˆæœ€ç»ˆç”¨æˆ·å¼ºå¤§çš„å¼•å¯¼èƒ½åŠ›å’ŒæŽ§åˆ¶æƒã€‚\n\nHermes 3 ç³»åˆ—åœ¨ Hermes 2 çš„èƒ½åŠ›åŸºç¡€ä¸Šè¿›è¡Œæž„å»ºå’Œæ‰©å±•ï¼ŒåŒ…æ‹¬æ›´å¼ºå¤§å’Œå¯é çš„å‡½æ•°è°ƒç”¨å’Œç»“æž„åŒ–è¾“å‡ºèƒ½åŠ›ï¼Œé€šç”¨åŠ©æ‰‹èƒ½åŠ›ï¼Œä»¥åŠæ”¹è¿›çš„ä»£ç ç”ŸæˆæŠ€èƒ½ã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/inflection-3-pi","frontmatter":{"title":"Inflection: Inflection 3 Pi","meta_title":"Inflection: Inflection 3 Pi","description":"Inflection: Inflection 3 Pi","date":"2024-10-11T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"inflection","tags":["Chatbots","Roleplay","Emotional Intelligence","Customer Support","Safety"],"draft":false,"id":"inflection-3-pi","context":8000,"input":0.0000025,"output":0.00001,"img":0,"request":0,"last_updated":"2024-11-07T10:11:48.000Z","slug":"models/inflection-3-pi"},"content":"\nInflection 3 Pi çš„ [Pi](https://pi.ai) èŠå¤©æœºå™¨äººï¼ŒåŒ…å«èƒŒæ™¯æ•…äº‹ã€æƒ…æ„Ÿæ™ºèƒ½ã€ç”Ÿäº§åŠ›å’Œå®‰å…¨æ€§ã€‚å®ƒåœ¨å®¢æˆ·æ”¯æŒã€è§’è‰²æ‰®æ¼”å’Œæƒ…æ„Ÿæ™ºèƒ½ç­‰åœºæ™¯ä¸­è¡¨çŽ°å‡ºè‰²ã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/inflection-3-productivity","frontmatter":{"title":"Inflection: Inflection 3 Productivity","meta_title":"Inflection: Inflection 3 Productivity","description":"Inflection: Inflection 3 Productivity","date":"2024-10-11T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"inflection","tags":["Programming","Technology","Chatbots","Generative AI","Data Science"],"draft":false,"id":"inflection-3-productivity","context":8000,"input":0.0000025,"output":0.00001,"img":0,"request":0,"last_updated":"2024-11-07T10:11:56.000Z","slug":"models/inflection-3-productivity"},"content":"\nInflection 3 çš„ç”Ÿäº§åŠ›ç»è¿‡ä¼˜åŒ–ï¼Œä»¥ä¾¿æ›´å¥½åœ°éµå¾ªæŒ‡ä»¤ã€‚å®ƒæ›´é€‚åˆéœ€è¦ JSON è¾“å‡ºæˆ–ç²¾ç¡®éµå¾ªæä¾›çš„æŒ‡å¯¼æ–¹é’ˆçš„ä»»åŠ¡ã€‚\n\næœ‰å…³ç±»ä¼¼äºŽ Pi çš„æƒ…æ„Ÿæ™ºèƒ½ï¼Œè¯·å‚è§ [Inflect 3 Pi](/inflection/inflection-3-pi)ã€‚\n\næœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§ [Inflection çš„å…¬å‘Š](https://inflection.ai/blog/enterprise)ã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/jamba-1-5-large","frontmatter":{"title":"AI21: Jamba 1.5 Large","meta_title":"AI21: Jamba 1.5 Large","description":"AI21: Jamba 1.5 Large","date":"2024-08-23T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"ai21","tags":["Programming","Technology","Machine Learning","Data Science","Generative AI"],"draft":false,"id":"jamba-1-5-large","context":256000,"input":0.000002,"output":0.000008,"img":0,"request":0,"last_updated":"2024-11-11T03:11:21.000Z","slug":"models/jamba-1-5-large"},"content":"\nJamba 1.5 Large æ˜¯ AI21 æ–°ä¸€ä»£å¼€æ”¾æ¨¡åž‹å®¶æ—çš„ä¸€éƒ¨åˆ†ï¼Œæä¾›å“è¶Šçš„é€Ÿåº¦ã€æ•ˆçŽ‡å’Œè´¨é‡ã€‚\n\nå®ƒå…·æœ‰ 256K çš„æœ‰æ•ˆä¸Šä¸‹æ–‡çª—å£ï¼Œæ˜¯å¼€æ”¾æ¨¡åž‹ä¸­æœ€é•¿çš„ï¼Œèƒ½å¤Ÿåœ¨æ–‡æ¡£æ‘˜è¦å’Œåˆ†æžç­‰ä»»åŠ¡ä¸Šæå‡æ€§èƒ½ã€‚\n\nåŸºäºŽæ–°é¢–çš„ SSM-Transformer æž¶æž„ï¼Œå®ƒåœ¨åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†æ›´å¤§çš„æ¨¡åž‹ï¼Œå¦‚ Llama 3.1 70Bï¼ŒåŒæ—¶ä¿æŒèµ„æºæ•ˆçŽ‡ã€‚\n\né˜…è¯»ä»–ä»¬çš„ [announcement](https://www.ai21.com/blog/announcing-jamba-model-family) ä»¥äº†è§£æ›´å¤šä¿¡æ¯ã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/jamba-1-5-mini","frontmatter":{"title":"AI21: Jamba 1.5 Mini","meta_title":"AI21: Jamba 1.5 Mini","description":"AI21: Jamba 1.5 Mini","date":"2024-08-23T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"ai21","tags":["Programming","Technology","Machine Learning","Natural Language Processing","Data Science"],"draft":false,"id":"jamba-1-5-mini","context":256000,"input":2e-7,"output":4e-7,"img":0,"request":0,"last_updated":"2024-11-11T03:11:12.000Z","slug":"models/jamba-1-5-mini"},"content":"\nJamba 1.5 Mini æ˜¯ä¸–ç•Œä¸Šé¦–ä¸ªç”Ÿäº§çº§ Mamba åŸºç¡€æ¨¡åž‹ï¼Œç»“åˆäº† SSM å’Œ Transformer æž¶æž„ï¼Œå…·æœ‰ 256K çš„ä¸Šä¸‹æ–‡çª—å£å’Œé«˜æ•ˆçŽ‡ã€‚\n\nå®ƒæ”¯æŒ 9 ç§è¯­è¨€ï¼Œå¹¶èƒ½å¤Ÿå¤„ç†å„ç§å†™ä½œå’Œåˆ†æžä»»åŠ¡ï¼Œæ•ˆæžœä¸Žç±»ä¼¼çš„å°æ¨¡åž‹ç›¸å½“æˆ–æ›´å¥½ã€‚\n\nè¯¥æ¨¡åž‹æ¯”ä»¥å‰çš„è®¾è®¡ä½¿ç”¨æ›´å°‘çš„è®¡ç®—æœºå†…å­˜ï¼Œå¹¶ä¸”åœ¨å¤„ç†è¾ƒé•¿æ–‡æœ¬æ—¶é€Ÿåº¦æ›´å¿«ã€‚\n\né˜…è¯»ä»–ä»¬çš„ [å…¬å‘Š](https://www.ai21.com/blog/announcing-jamba-model-family) ä»¥äº†è§£æ›´å¤šä¿¡æ¯ã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/l3-lunaris-8b","frontmatter":{"title":"Llama 3 8B Lunaris","meta_title":"Llama 3 8B Lunaris","description":"Llama 3 8B Lunaris","date":"2024-08-13T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"sao10k","tags":["Roleplay","Programming","Machine Learning","Natural Language Processing","Chatbots"],"draft":false,"id":"l3-lunaris-8b","context":8192,"input":0.000002,"output":0.000002,"img":0,"request":0,"last_updated":"2024-11-11T03:12:13.000Z","slug":"models/l3-lunaris-8b"},"content":"\nLunaris 8B æ˜¯ä¸€ä¸ªå¤šåŠŸèƒ½çš„é€šç”¨å’Œè§’è‰²æ‰®æ¼”æ¨¡åž‹ï¼ŒåŸºäºŽ Llama 3ã€‚å®ƒæ˜¯å¤šä¸ªæ¨¡åž‹çš„æˆ˜ç•¥åˆå¹¶ï¼Œæ—¨åœ¨å¹³è¡¡åˆ›é€ åŠ›ä¸Žæ”¹è¿›çš„é€»è¾‘å’Œä¸€èˆ¬çŸ¥è¯†ã€‚\n\nç”± [Sao10k](https://huggingface.co/Sao10k) åˆ›å»ºï¼Œè¯¥æ¨¡åž‹æ—¨åœ¨æä¾›æ¯” Stheno v3.2 æ›´å¥½çš„ä½“éªŒï¼Œå…·æœ‰å¢žå¼ºçš„åˆ›é€ åŠ›å’Œé€»è¾‘æŽ¨ç†èƒ½åŠ›ã€‚\n\nä¸ºäº†èŽ·å¾—æœ€ä½³æ•ˆæžœï¼Œè¯·ä½¿ç”¨ Llama 3 Instruct ä¸Šä¸‹æ–‡æ¨¡æ¿ï¼Œæ¸©åº¦ 1.4ï¼Œmin_p 0.1ã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/l31-euryale-70b","frontmatter":{"title":"Llama 3.1 Euryale 70B v2.2","meta_title":"Llama 3.1 Euryale 70B v2.2","description":"Llama 3.1 Euryale 70B v2.2","date":"2024-08-28T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"sao10k","tags":["Roleplay","Generative AI","Chatbots","Natural Language Processing","Technology/Web"],"draft":false,"id":"l3.1-euryale-70b","context":8192,"input":3.5e-7,"output":4e-7,"img":0,"request":0,"last_updated":"2024-11-11T03:12:50.000Z","slug":"models/l31-euryale-70b"},"content":"\nEuryale L3.1 70B v2.2 æ˜¯ä¸€ä¸ªä¸“æ³¨äºŽåˆ›æ„è§’è‰²æ‰®æ¼”çš„æ¨¡åž‹ï¼Œæ¥è‡ª [Sao10k](https://ko-fi.com/sao10k)ã€‚å®ƒæ˜¯ [Euryale L3 70B v2.1](/sao10k/l3-euryale-70b) çš„ç»§ä»»è€…ã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/lfm-40b","frontmatter":{"title":"Liquid: LFM 40B MoE","meta_title":"Liquid: LFM 40B MoE","description":"Liquid: LFM 40B MoE","date":"2024-09-30T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"liquid","tags":["Machine Learning","Natural Language Processing","Data Science","Generative AI","Computer Vision"],"draft":false,"is_recommended":true,"id":"lfm-40b","context":32768,"input":0.000001,"output":0.000002,"img":0,"request":0,"last_updated":"2024-11-14T05:10:16.000Z","slug":"models/lfm-40b"},"content":"\nLiquidçš„40.3Bä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ¨¡åž‹ã€‚LiquidåŸºç¡€æ¨¡åž‹ï¼ˆLFMsï¼‰æ˜¯åŸºäºŽåŠ¨æ€ç³»ç»Ÿæž„å»ºçš„å¤§åž‹ç¥žç»ç½‘ç»œã€‚\n\nLFMsæ˜¯é€šç”¨çš„AIæ¨¡åž‹ï¼Œå¯ä»¥ç”¨äºŽå»ºæ¨¡ä»»ä½•ç±»åž‹çš„åºåˆ—æ•°æ®ï¼ŒåŒ…æ‹¬è§†é¢‘ã€éŸ³é¢‘ã€æ–‡æœ¬ã€æ—¶é—´åºåˆ—å’Œä¿¡å·ã€‚\n\næœ‰å…³åŸºå‡†å’Œæ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§[å‘å¸ƒå…¬å‘Š](https://www.liquid.ai/liquid-foundation-models)ã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/liquid-lfm-40b:free","frontmatter":{"title":"Liquid: LFM 40B MoE (free)","meta_title":"Liquid: LFM 40B MoE (free)","description":"Liquid: LFM 40B MoE (free)","date":"2024-09-30T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"liquid","tags":["Generative AI","Machine Learning","Natural Language Processing","Data Science","Technology/Web"],"draft":false,"id":"liquid/lfm-40b:free","context":8192,"input":0,"output":0,"img":0,"request":0,"last_updated":"2024-11-07T00:17:57.000Z","slug":"models/liquid-lfm-40b:free"},"content":"\nLiquidçš„40.3Bä¸“å®¶æ··åˆæ¨¡åž‹ï¼ˆMoEï¼‰ã€‚LiquidåŸºç¡€æ¨¡åž‹ï¼ˆLFMsï¼‰æ˜¯åŸºäºŽåŠ¨æ€ç³»ç»Ÿæž„å»ºçš„å¤§åž‹ç¥žç»ç½‘ç»œã€‚\n\nLFMsæ˜¯é€šç”¨çš„AIæ¨¡åž‹ï¼Œå¯ä»¥ç”¨äºŽå»ºæ¨¡ä»»ä½•ç±»åž‹çš„åºåˆ—æ•°æ®ï¼ŒåŒ…æ‹¬è§†é¢‘ã€éŸ³é¢‘ã€æ–‡æœ¬ã€æ—¶é—´åºåˆ—å’Œä¿¡å·ã€‚\n\næœ‰å…³åŸºå‡†æµ‹è¯•å’Œæ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§[å‘å¸ƒå…¬å‘Š](https://www.liquid.ai/liquid-foundation-models)ã€‚\n\n_è¿™äº›æ˜¯[LFM 40B MoE](/liquid/lfm-40b)çš„å…è´¹é™æµç«¯ç‚¹ã€‚è¾“å‡ºå¯èƒ½ä¼šè¢«ç¼“å­˜ã€‚æœ‰å…³é€ŸçŽ‡é™åˆ¶çš„ä¿¡æ¯ï¼Œè¯·[åœ¨è¿™é‡Œ](/docs/limits)é˜…è¯»ã€‚_\n\n"},{"lang":"zh","group":"models","slug":"models/llama-31-70b-instruct","frontmatter":{"title":"Meta: Llama 3.1 70B Instruct","meta_title":"Meta: Llama 3.1 70B Instruct","description":"Meta: Llama 3.1 70B Instruct","date":"2024-07-23T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text 2 text"],"author":"meta-llama","tags":["Programming","Machine Learning","Natural Language Processing","Chatbots","Ethics"],"draft":false,"id":"llama-3.1-70b-instruct","context":131072,"input":3e-7,"output":3e-7,"img":0,"request":0,"last_updated":"2024-10-28T13:38:49.000Z","slug":"models/llama-31-70b-instruct"},"content":"\nMetaæœ€æ–°å‘å¸ƒçš„æ¨¡åž‹ç±»åˆ«ï¼ˆLlama 3.1ï¼‰æŽ¨å‡ºäº†å¤šç§å°ºå¯¸å’Œç‰ˆæœ¬ã€‚è¿™ä¸ª70Bçš„æŒ‡ä»¤è°ƒä¼˜ç‰ˆæœ¬é’ˆå¯¹é«˜è´¨é‡å¯¹è¯ç”¨ä¾‹è¿›è¡Œäº†ä¼˜åŒ–ã€‚\n\nåœ¨äººå·¥è¯„ä¼°ä¸­ï¼Œå®ƒä¸Žé¢†å…ˆçš„é—­æºæ¨¡åž‹ç›¸æ¯”è¡¨çŽ°å‡ºè‰²ã€‚\n\nä½¿ç”¨è¯¥æ¨¡åž‹éœ€éµå¾ª[Metaçš„å¯æŽ¥å—ä½¿ç”¨æ”¿ç­–](https://www.llama.com/llama3/use-policy/)ã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/llama-31-8b-instruct","frontmatter":{"title":"Meta: Llama 3.1 8B Instruct","meta_title":"Meta: Llama 3.1 8B Instruct","description":"Meta: Llama 3.1 8B Instruct","date":"2024-07-23T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text 2 text"],"author":"meta-llama","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Ethics"],"draft":false,"id":"llama-3.1-8b-instruct","context":131072,"input":5.5e-8,"output":5.5e-8,"img":0,"request":0,"last_updated":"2024-10-31T23:27:09.000Z","slug":"models/llama-31-8b-instruct"},"content":"\nMetaæœ€æ–°å‘å¸ƒçš„æ¨¡åž‹ç³»åˆ—ï¼ˆLlama 3.1ï¼‰æŽ¨å‡ºäº†å¤šç§å°ºå¯¸å’Œç‰ˆæœ¬ã€‚è¿™ä¸ª8BæŒ‡ä»¤è°ƒä¼˜ç‰ˆæœ¬å¿«é€Ÿä¸”é«˜æ•ˆã€‚\n\nä¸Žé¢†å…ˆçš„é—­æºæ¨¡åž‹ç›¸æ¯”ï¼Œå®ƒåœ¨äººå·¥è¯„ä¼°ä¸­è¡¨çŽ°å‡ºè‰²ã€‚\n\nä½¿ç”¨æ­¤æ¨¡åž‹éœ€éµå¾ª[Metaçš„å¯æŽ¥å—ä½¿ç”¨æ”¿ç­–](https://www.llama.com/llama3/use-policy/)ã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/llama-31-lumimaid-70b","frontmatter":{"title":"Lumimaid v0.2 70B","meta_title":"Lumimaid v0.2 70B","description":"Lumimaid v0.2 70B","date":"2024-10-22T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"neversleep","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Ethics"],"draft":false,"id":"llama-3.1-lumimaid-70b","context":131072,"input":0.000003375,"output":0.0000045,"img":0,"request":0,"last_updated":"2024-11-11T03:03:31.000Z","slug":"models/llama-31-lumimaid-70b"},"content":"\nLumimaid v0.2 70B æ˜¯å¯¹ [Llama 3.1 70B](/meta-llama/llama-3.1-70b-instruct) çš„å¾®è°ƒï¼Œä¸Ž Lumimaid v0.1 ç›¸æ¯”ï¼Œåœ¨æ•°æ®é›†æ–¹é¢æœ‰äº†â€œå·¨å¤§çš„æå‡â€ã€‚ä¸åˆæ ¼çš„èŠå¤©è¾“å‡ºå·²è¢«æ¸…é™¤ã€‚\n\nä½¿ç”¨æ­¤æ¨¡åž‹é¡»éµå¾ª [Meta çš„å¯æŽ¥å—ä½¿ç”¨æ”¿ç­–](https://llama.meta.com/llama3/use-policy/)ã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/llama-31-lumimaid-8b","frontmatter":{"title":"Lumimaid v0.2 8B","meta_title":"Lumimaid v0.2 8B","description":"Lumimaid v0.2 8B","date":"2024-09-15T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"neversleep","tags":["Programming","Machine Learning","Natural Language Processing","Chatbots","Ethics"],"draft":false,"id":"llama-3.1-lumimaid-8b","context":131072,"input":1.875e-7,"output":0.000001125,"img":0,"request":0,"last_updated":"2024-11-11T03:10:19.000Z","slug":"models/llama-31-lumimaid-8b"},"content":"\nLumimaid v0.2 8B æ˜¯å¯¹ [Llama 3.1 8B](/meta-llama/llama-3.1-8b-instruct) çš„å¾®è°ƒï¼Œç›¸è¾ƒäºŽ Lumimaid v0.1ï¼Œæ•°æ®é›†æœ‰äº†â€œå·¨å¤§çš„æå‡â€ã€‚ä¸å½“çš„èŠå¤©è¾“å‡ºå·²è¢«æ¸…é™¤ã€‚\n\nä½¿ç”¨æ­¤æ¨¡åž‹é¡»éµå¾ª [Meta çš„å¯æŽ¥å—ä½¿ç”¨æ”¿ç­–](https://llama.meta.com/llama3/use-policy/)ã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/llama-31-nemotron-70b-instruct","frontmatter":{"title":"Nvidia: Llama 3.1 Nemotron 70B Instruct","meta_title":"Nvidia: Llama 3.1 Nemotron 70B Instruct","description":"Nvidia: Llama 3.1 Nemotron 70B Instruct","date":"2024-10-15T00:00:00.000Z","image":"https://img.rifx.online/logo/nvidia.svg","categories":["text 2 text"],"author":"nvidia","tags":["Programming","Natural Language Processing","Machine Learning","Generative AI","Ethics"],"draft":false,"id":"llama-3.1-nemotron-70b-instruct","context":131072,"input":3.5e-7,"output":4e-7,"img":0,"request":0,"last_updated":"2024-10-15T00:00:00.000Z","slug":"models/llama-31-nemotron-70b-instruct"},"content":"\nNVIDIAçš„Llama 3.1 Nemotron 70Bæ˜¯ä¸€ä¸ªæ—¨åœ¨ç”Ÿæˆç²¾ç¡®å’Œæœ‰ç”¨å“åº”çš„è¯­è¨€æ¨¡åž‹ã€‚åˆ©ç”¨[Llama 3.1 70B](/models/meta-llama/llama-3.1-70b-instruct)æž¶æž„å’ŒåŸºäºŽäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ï¼Œå®ƒåœ¨è‡ªåŠ¨å¯¹é½åŸºå‡†æµ‹è¯•ä¸­è¡¨çŽ°å‡ºè‰²ã€‚è¯¥æ¨¡åž‹ä¸“ä¸ºéœ€è¦é«˜å‡†ç¡®æ€§ä»¥æä¾›å¸®åŠ©å’Œç”Ÿæˆå“åº”çš„åº”ç”¨è€Œè®¾è®¡ï¼Œé€‚åˆå¤„ç†å¤šä¸ªé¢†åŸŸçš„å¤šæ ·ç”¨æˆ·æŸ¥è¯¢ã€‚\n\nä½¿ç”¨æ­¤æ¨¡åž‹é¡»éµå¾ª[Metaçš„å¯æŽ¥å—ä½¿ç”¨æ”¿ç­–](https://www.llama.com/llama3/use-policy/)ã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/llama-31-sonar-huge-128k-online","frontmatter":{"title":"Perplexity: Llama 3.1 Sonar 405B Online","meta_title":"Perplexity: Llama 3.1 Sonar 405B Online","description":"Perplexity: Llama 3.1 Sonar 405B Online","date":"2024-08-14T00:00:00.000Z","image":"https://img.rifx.online/logo/perplexity.svg","categories":["text 2 text"],"author":"perplexity","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"llama-3.1-sonar-huge-128k-online","context":127072,"input":0.000005,"output":0.000005,"img":0,"request":0.005,"last_updated":"2024-11-07T09:36:38.000Z","slug":"models/llama-31-sonar-huge-128k-online"},"content":"\nLlama 3.1 Sonar æ˜¯ Perplexity æœ€æ–°çš„æ¨¡åž‹ç³»åˆ—ã€‚å®ƒåœ¨æˆæœ¬æ•ˆç›Šã€é€Ÿåº¦å’Œæ€§èƒ½ä¸Šè¶…è¶Šäº†ä»–ä»¬æ—©æœŸçš„ Sonar æ¨¡åž‹ã€‚è¯¥æ¨¡åž‹åŸºäºŽ Llama 3.1 405Bï¼Œå¹¶å…·æœ‰äº’è”ç½‘è®¿é—®åŠŸèƒ½ã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/llama-31-sonar-large-128k-online","frontmatter":{"title":"Perplexity: Llama 3.1 Sonar 70B Online","meta_title":"Perplexity: Llama 3.1 Sonar 70B Online","description":"Perplexity: Llama 3.1 Sonar 70B Online","date":"2024-08-01T00:00:00.000Z","image":"https://img.rifx.online/logo/perplexity.svg","categories":["text 2 text"],"author":"perplexity","tags":["Programming","Machine Learning","Natural Language Processing","Chatbots","Generative AI"],"draft":false,"id":"llama-3.1-sonar-large-128k-online","context":127072,"input":0.000001,"output":0.000001,"img":0,"request":0.005,"last_updated":"2024-11-07T09:37:21.000Z","slug":"models/llama-31-sonar-large-128k-online"},"content":"\nLlama 3.1 Sonar æ˜¯ Perplexity æœ€æ–°çš„æ¨¡åž‹ç³»åˆ—ã€‚å®ƒåœ¨æˆæœ¬æ•ˆç›Šã€é€Ÿåº¦å’Œæ€§èƒ½æ–¹é¢è¶…è¶Šäº†ä»–ä»¬æ—©æœŸçš„ Sonar æ¨¡åž‹ã€‚\n\nè¿™æ˜¯ [ç¦»çº¿èŠå¤©æ¨¡åž‹](/perplexity/llama-3.1-sonar-large-128k-chat) çš„åœ¨çº¿ç‰ˆæœ¬ã€‚å®ƒä¸“æ³¨äºŽæä¾›æœ‰å¸®åŠ©ã€æœ€æ–°å’ŒçœŸå®žçš„å“åº”ã€‚ #online\n\n"},{"lang":"zh","group":"models","slug":"models/llama-31-sonar-small-128k-online","frontmatter":{"title":"Perplexity: Llama 3.1 Sonar 8B Online","meta_title":"Perplexity: Llama 3.1 Sonar 8B Online","description":"Perplexity: Llama 3.1 Sonar 8B Online","date":"2024-08-01T00:00:00.000Z","image":"https://img.rifx.online/logo/perplexity.svg","categories":["text 2 text"],"author":"perplexity","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"llama-3.1-sonar-small-128k-online","context":127072,"input":2e-7,"output":2e-7,"img":0,"request":0.005,"last_updated":"2024-11-07T09:38:09.000Z","slug":"models/llama-31-sonar-small-128k-online"},"content":"\nLlama 3.1 Sonar æ˜¯ Perplexity æœ€æ–°çš„æ¨¡åž‹ç³»åˆ—ã€‚å®ƒåœ¨æˆæœ¬æ•ˆçŽ‡ã€é€Ÿåº¦å’Œæ€§èƒ½ä¸Šè¶…è¶Šäº†ä»–ä»¬æ—©æœŸçš„ Sonar æ¨¡åž‹ã€‚\n\nè¿™æ˜¯ [ç¦»çº¿èŠå¤©æ¨¡åž‹](/perplexity/llama-3.1-sonar-small-128k-chat) çš„åœ¨çº¿ç‰ˆæœ¬ã€‚å®ƒä¸“æ³¨äºŽæä¾›æœ‰ç”¨ã€æœ€æ–°å’ŒçœŸå®žçš„å“åº”ã€‚ #online\n\n"},{"lang":"zh","group":"models","slug":"models/llama-32-11b-vision-instruct","frontmatter":{"title":"Meta: Llama 3.2 11B Vision Instruct","meta_title":"Meta: Llama 3.2 11B Vision Instruct","description":"Meta: Llama 3.2 11B Vision Instruct","date":"2024-09-25T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text image 2 text"],"author":"meta-llama","tags":["Natural Language Processing","Computer Vision","Machine Learning","Generative AI","Data Science"],"draft":false,"is_recommended":true,"id":"llama-3.2-11b-vision-instruct","context":131072,"input":5.5e-8,"output":5.5e-8,"img":0.000079475,"request":0,"last_updated":"2024-11-14T05:10:41.000Z","slug":"models/llama-32-11b-vision-instruct"},"content":"\nLlama 3.2 11B Vision æ˜¯ä¸€ä¸ªå…·æœ‰ 110 äº¿å‚æ•°çš„å¤šæ¨¡æ€æ¨¡åž‹ï¼Œæ—¨åœ¨å¤„ç†ç»“åˆè§†è§‰å’Œæ–‡æœ¬æ•°æ®çš„ä»»åŠ¡ã€‚å®ƒåœ¨å›¾åƒæè¿°å’Œè§†è§‰é—®ç­”ç­‰ä»»åŠ¡ä¸­è¡¨çŽ°å‡ºè‰²ï¼Œå¼¥åˆäº†è¯­è¨€ç”Ÿæˆä¸Žè§†è§‰æŽ¨ç†ä¹‹é—´çš„é¸¿æ²Ÿã€‚è¯¥æ¨¡åž‹åœ¨å¤§é‡å›¾åƒ-æ–‡æœ¬å¯¹çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œèƒ½å¤Ÿåœ¨å¤æ‚çš„é«˜ç²¾åº¦å›¾åƒåˆ†æžä¸­è¡¨çŽ°è‰¯å¥½ã€‚\n\nå®ƒå°†è§†è§‰ç†è§£ä¸Žè¯­è¨€å¤„ç†ç›¸ç»“åˆçš„èƒ½åŠ›ï¼Œä½¿å…¶æˆä¸ºéœ€è¦å…¨é¢è§†è§‰è¯­è¨€ AI åº”ç”¨çš„è¡Œä¸šçš„ç†æƒ³è§£å†³æ–¹æ¡ˆï¼Œä¾‹å¦‚å†…å®¹åˆ›ä½œã€AI é©±åŠ¨çš„å®¢æˆ·æœåŠ¡å’Œç ”ç©¶ã€‚\n\nç‚¹å‡»æ­¤å¤„æŸ¥çœ‹ [åŽŸå§‹æ¨¡åž‹å¡](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md)ã€‚\n\nä½¿ç”¨æ­¤æ¨¡åž‹éœ€éµå®ˆ [Meta çš„å¯æŽ¥å—ä½¿ç”¨æ”¿ç­–](https://www.llama.com/llama3/use-policy/)ã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/llama-32-1b-instruct","frontmatter":{"title":"Meta: Llama 3.2 1B Instruct","meta_title":"Meta: Llama 3.2 1B Instruct","description":"Meta: Llama 3.2 1B Instruct","date":"2024-09-25T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text 2 text"],"author":"meta-llama","tags":["Natural Language Processing","Programming","Technology","Machine Learning","Generative AI"],"draft":false,"id":"llama-3.2-1b-instruct","context":131072,"input":1e-8,"output":2e-8,"img":0,"request":0,"last_updated":"2024-09-25T00:00:00.000Z","slug":"models/llama-32-1b-instruct"},"content":"\nLlama 3.2 1B æ˜¯ä¸€ä¸ªæ‹¥æœ‰10äº¿å‚æ•°çš„è¯­è¨€æ¨¡åž‹ï¼Œä¸“æ³¨äºŽé«˜æ•ˆæ‰§è¡Œè‡ªç„¶è¯­è¨€ä»»åŠ¡ï¼Œå¦‚æ‘˜è¦ã€å¯¹è¯å’Œå¤šè¯­è¨€æ–‡æœ¬åˆ†æžã€‚å…¶è¾ƒå°çš„è§„æ¨¡ä½¿å…¶èƒ½å¤Ÿåœ¨ä½Žèµ„æºçŽ¯å¢ƒä¸­é«˜æ•ˆè¿è¡Œï¼ŒåŒæ—¶ä¿æŒå¼ºå¤§çš„ä»»åŠ¡æ€§èƒ½ã€‚\n\næ”¯æŒå…«ç§æ ¸å¿ƒè¯­è¨€ï¼Œå¹¶å¯é’ˆå¯¹æ›´å¤šè¯­è¨€è¿›è¡Œå¾®è°ƒï¼ŒLlama 1.3B éžå¸¸é€‚åˆå¯»æ±‚è½»é‡çº§ä½†å¼ºå¤§çš„ AI è§£å†³æ–¹æ¡ˆçš„ä¼ä¸šæˆ–å¼€å‘è€…ï¼Œè¿™äº›è§£å†³æ–¹æ¡ˆèƒ½å¤Ÿåœ¨å¤šæ ·åŒ–çš„å¤šè¯­è¨€çŽ¯å¢ƒä¸­è¿è¡Œï¼Œè€Œä¸éœ€è¦å¤§åž‹æ¨¡åž‹çš„é«˜è®¡ç®—éœ€æ±‚ã€‚\n\nç‚¹å‡»æ­¤å¤„æŸ¥çœ‹ [åŽŸå§‹æ¨¡åž‹å¡](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md)ã€‚\n\nä½¿ç”¨æ­¤æ¨¡åž‹éœ€éµå¾ª [Meta çš„å¯æŽ¥å—ä½¿ç”¨æ”¿ç­–](https://www.llama.com/llama3/use-policy/)ã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/llama-32-3b-instruct","frontmatter":{"title":"Meta: Llama 3.2 3B Instruct","meta_title":"Meta: Llama 3.2 3B Instruct","description":"Meta: Llama 3.2 3B Instruct","date":"2024-09-25T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text 2 text"],"author":"meta-llama","tags":["Natural Language Processing","Machine Learning","Generative AI","Chatbots","Multilingual"],"draft":false,"id":"llama-3.2-3b-instruct","context":131072,"input":3e-8,"output":5e-8,"img":0,"request":0,"last_updated":"2024-11-11T03:09:59.000Z","slug":"models/llama-32-3b-instruct"},"content":"\nLlama 3.2 3B æ˜¯ä¸€ä¸ªæ‹¥æœ‰ 30 äº¿å‚æ•°çš„å¤šè¯­è¨€å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼Œé’ˆå¯¹å¯¹è¯ç”Ÿæˆã€æŽ¨ç†å’Œæ‘˜è¦ç­‰é«˜çº§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡è¿›è¡Œäº†ä¼˜åŒ–ã€‚å®ƒé‡‡ç”¨æœ€æ–°çš„ transformer æž¶æž„ï¼Œæ”¯æŒåŒ…æ‹¬è‹±è¯­ã€è¥¿ç­ç‰™è¯­å’Œå°åœ°è¯­åœ¨å†…çš„å…«ç§è¯­è¨€ï¼Œå¹¶ä¸”å¯ä»¥é€‚åº”å…¶ä»–è¯­è¨€ã€‚\n\nLlama 3.2B æ¨¡åž‹åœ¨ 9 ä¸‡äº¿ä¸ªæ ‡è®°ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ“…é•¿éµå¾ªæŒ‡ä»¤ã€å¤æ‚æŽ¨ç†å’Œå·¥å…·ä½¿ç”¨ã€‚å…¶å¹³è¡¡çš„æ€§èƒ½ä½¿å…¶éžå¸¸é€‚åˆéœ€è¦åœ¨å¤šè¯­è¨€çŽ¯å¢ƒä¸­è¿›è¡Œæ–‡æœ¬ç”Ÿæˆæ—¶çš„å‡†ç¡®æ€§å’Œæ•ˆçŽ‡çš„åº”ç”¨ã€‚\n\nClick here for the [original model card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md).\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).\n\n"},{"lang":"zh","group":"models","slug":"models/llama-32-90b-vision-instruct","frontmatter":{"title":"Meta: Llama 3.2 90B Vision Instruct","meta_title":"Meta: Llama 3.2 90B Vision Instruct","description":"Meta: Llama 3.2 90B Vision Instruct","date":"2024-09-25T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text image 2 text"],"author":"meta-llama","tags":["Natural Language Processing","Computer Vision","Machine Learning","Data Science","Generative AI"],"draft":false,"id":"llama-3.2-90b-vision-instruct","context":131072,"input":3.5e-7,"output":4e-7,"img":0.00050575,"request":0,"last_updated":"2024-09-25T00:00:00.000Z","slug":"models/llama-32-90b-vision-instruct"},"content":"\nLlama 90B Visionæ¨¡åž‹æ˜¯ä¸€æ¬¾é¡¶çº§çš„90äº¿å‚æ•°å¤šæ¨¡æ€æ¨¡åž‹ï¼Œæ—¨åœ¨åº”å¯¹æœ€å…·æŒ‘æˆ˜æ€§çš„è§†è§‰æŽ¨ç†å’Œè¯­è¨€ä»»åŠ¡ã€‚å®ƒåœ¨å›¾åƒæè¿°ã€è§†è§‰é—®ç­”å’Œé«˜çº§å›¾åƒ-æ–‡æœ¬ç†è§£æ–¹é¢æä¾›æ— ä¸Žä¼¦æ¯”çš„å‡†ç¡®æ€§ã€‚è¯¥æ¨¡åž‹åœ¨åºžå¤§çš„å¤šæ¨¡æ€æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶é€šè¿‡äººç±»åé¦ˆè¿›è¡Œå¾®è°ƒï¼Œä¸“ä¸ºå¤„ç†æœ€è‹›åˆ»çš„åŸºäºŽå›¾åƒçš„AIä»»åŠ¡è€Œè®¾è®¡ã€‚\n\næ­¤æ¨¡åž‹éžå¸¸é€‚åˆéœ€è¦å°–ç«¯å¤šæ¨¡æ€AIèƒ½åŠ›çš„è¡Œä¸šï¼Œå°¤å…¶æ˜¯é‚£äº›å¤„ç†å¤æ‚å®žæ—¶è§†è§‰å’Œæ–‡æœ¬åˆ†æžçš„è¡Œä¸šã€‚\n\nç‚¹å‡»æ­¤å¤„æŸ¥çœ‹[åŽŸå§‹æ¨¡åž‹å¡ç‰‡](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md)ã€‚\n\nä½¿ç”¨æ­¤æ¨¡åž‹éœ€éµå®ˆ[Metaçš„å¯æŽ¥å—ä½¿ç”¨æ”¿ç­–](https://www.llama.com/llama3/use-policy/)ã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/lzlv-70b-fp16-hf","frontmatter":{"title":"lzlv 70B","meta_title":"lzlv 70B","description":"lzlv 70B","date":"2023-11-12T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"lizpreciatior","tags":["Roleplay","Programming","Machine Learning","Generative AI","Chatbots"],"draft":false,"id":"lzlv-70b-fp16-hf","context":4096,"input":3.5e-7,"output":4e-7,"img":0,"request":0,"last_updated":"2024-11-04T12:50:34.000Z","slug":"models/lzlv-70b-fp16-hf"},"content":"\nA Mythomax/MLewd_13Bé£Žæ ¼çš„é€‰å®š70Bæ¨¡åž‹åˆå¹¶ã€‚\nä¸€ä¸ªå¤šæ¨¡åž‹åˆå¹¶ï¼Œç»“åˆäº†å¤šä¸ªLLaMA2 70Bå¾®è°ƒæ¨¡åž‹ï¼Œç”¨äºŽè§’è‰²æ‰®æ¼”å’Œåˆ›æ„å·¥ä½œã€‚ç›®æ ‡æ˜¯åˆ›å»ºä¸€ä¸ªå°†åˆ›é€ åŠ›ä¸Žæ™ºèƒ½ç›¸ç»“åˆçš„æ¨¡åž‹ï¼Œä»¥æå‡ä½“éªŒã€‚\n\n#merge #uncensored\n\n"},{"lang":"zh","group":"models","slug":"models/magnum-v2-72b","frontmatter":{"title":"Magnum v2 72B","meta_title":"Magnum v2 72B","description":"Magnum v2 72B","date":"2024-09-30T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"anthracite-org","tags":["Roleplay","Programming","Machine Learning","Natural Language Processing","Generative AI"],"draft":false,"id":"magnum-v2-72b","context":32768,"input":0.00000375,"output":0.0000045,"img":0,"request":0,"last_updated":"2024-11-11T03:09:19.000Z","slug":"models/magnum-v2-72b"},"content":"\næ¥è‡ª[Goliath](https://openrouter.ai/alpindale/goliath-120b)çš„åˆ¶é€ å•†ï¼ŒMagnum 72Bæ˜¯ç¬¬ä¸ƒä¸ªæ—¨åœ¨è¾¾åˆ°Claude 3æ¨¡åž‹çš„æ•£æ–‡è´¨é‡çš„æ¨¡åž‹ç³»åˆ—ï¼Œç‰¹åˆ«æ˜¯Opuså’ŒSonnetã€‚\n\nè¯¥æ¨¡åž‹åŸºäºŽ[Qwen2 72B](https://openrouter.ai/qwen/qwen-2-72b-instruct)ï¼Œå¹¶ä½¿ç”¨5500ä¸‡ä¸ªé«˜åº¦ç­–åˆ’çš„è§’è‰²æ‰®æ¼”(RP)æ•°æ®è¿›è¡Œè®­ç»ƒã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/magnum-v4-72b","frontmatter":{"title":"Magnum v4 72B","meta_title":"Magnum v4 72B","description":"Magnum v4 72B","date":"2024-10-22T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"anthracite-org","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"magnum-v4-72b","context":32768,"input":0.000001875,"output":0.00000225,"img":0,"request":0,"last_updated":"2024-11-04T12:39:55.000Z","slug":"models/magnum-v4-72b"},"content":"\nè¿™æ˜¯ä¸€ä¸ªç³»åˆ—æ¨¡åž‹ï¼Œæ—¨åœ¨å¤åˆ¶Claude 3æ¨¡åž‹çš„æ•£æ–‡è´¨é‡ï¼Œç‰¹åˆ«æ˜¯Sonnetå’ŒOpusã€‚\n\nè¯¥æ¨¡åž‹æ˜¯åœ¨[Qwen2.5 72B]sçš„åŸºç¡€ä¸Šè¿›è¡Œå¾®è°ƒçš„ã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/ministral-3b","frontmatter":{"title":"Ministral 3B","meta_title":"Ministral 3B","description":"Ministral 3B","date":"2024-10-17T00:00:00.000Z","image":"https://img.rifx.online/logo/mistral.png","categories":["text 2 text"],"author":"mistralai","tags":["Programming","Machine Learning","Natural Language Processing","Data Science","Generative AI"],"draft":false,"id":"ministral-3b","context":128000,"input":4e-8,"output":4e-8,"img":0,"request":0,"last_updated":"2024-11-07T00:24:37.000Z","slug":"models/ministral-3b"},"content":"\nMinistral 3B æ˜¯ä¸€ä¸ªé’ˆå¯¹è®¾å¤‡å’Œè¾¹ç¼˜è®¡ç®—ä¼˜åŒ–çš„ 3B å‚æ•°æ¨¡åž‹ã€‚å®ƒåœ¨çŸ¥è¯†ã€å¸¸è¯†æŽ¨ç†å’Œå‡½æ•°è°ƒç”¨æ–¹é¢è¡¨çŽ°å‡ºè‰²ï¼Œåœ¨å¤§å¤šæ•°åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†åƒ Mistral 7B è¿™æ ·çš„æ›´å¤§æ¨¡åž‹ã€‚æ”¯æŒæœ€é•¿ 128k çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œéžå¸¸é€‚åˆé«˜æ•ˆæŽ¨ç†çš„ä»£ç†å·¥ä½œæµå’Œä¸“ä¸šä»»åŠ¡çš„åè°ƒã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/ministral-8b","frontmatter":{"title":"Ministral 8B","meta_title":"Ministral 8B","description":"Ministral 8B","date":"2024-10-17T00:00:00.000Z","image":"https://img.rifx.online/logo/mistral.png","categories":["text 2 text"],"author":"mistralai","tags":["Technology","Machine Learning","Data Science","Generative AI","Ethics"],"draft":false,"id":"ministral-8b","context":128000,"input":1e-7,"output":1e-7,"img":0,"request":0,"last_updated":"2024-10-19T04:54:11.000Z","slug":"models/ministral-8b"},"content":"\nMinistral 8B æ˜¯ä¸€ä¸ªå…·æœ‰ 8B å‚æ•°çš„æ¨¡åž‹ï¼Œé‡‡ç”¨ç‹¬ç‰¹çš„äº¤é”™æ»‘åŠ¨çª—å£æ³¨æ„åŠ›æ¨¡å¼ï¼Œä»¥å®žçŽ°æ›´å¿«ã€æ›´èŠ‚çœå†…å­˜çš„æŽ¨ç†ã€‚è¯¥æ¨¡åž‹ä¸“ä¸ºè¾¹ç¼˜ä½¿ç”¨æ¡ˆä¾‹è®¾è®¡ï¼Œæ”¯æŒæœ€é•¿ 128k çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œå¹¶åœ¨çŸ¥è¯†å’ŒæŽ¨ç†ä»»åŠ¡ä¸­è¡¨çŽ°å‡ºè‰²ã€‚å®ƒåœ¨ä½ŽäºŽ 10B çš„ç±»åˆ«ä¸­ä¼˜äºŽåŒç±»äº§å“ï¼Œéžå¸¸é€‚åˆä½Žå»¶è¿Ÿã€æ³¨é‡éšç§çš„åº”ç”¨ã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/mistral-7b-instruct","frontmatter":{"title":"Mistral: Mistral 7B Instruct","meta_title":"Mistral: Mistral 7B Instruct","description":"Mistral: Mistral 7B Instruct","date":"2024-05-27T00:00:00.000Z","image":"https://img.rifx.online/logo/mistral.png","categories":["text 2 text"],"author":"mistralai","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"mistral-7b-instruct","context":32768,"input":5.5e-8,"output":5.5e-8,"img":0,"request":0,"last_updated":"2024-10-31T23:13:12.000Z","slug":"models/mistral-7b-instruct"},"content":"\nä¸€ä¸ªé«˜æ€§èƒ½ã€è¡Œä¸šæ ‡å‡†çš„7.3Bå‚æ•°æ¨¡åž‹ï¼Œé’ˆå¯¹é€Ÿåº¦å’Œä¸Šä¸‹æ–‡é•¿åº¦è¿›è¡Œäº†ä¼˜åŒ–ã€‚\n\n*Mistral 7B Instructæœ‰å¤šä¸ªç‰ˆæœ¬å˜ä½“ï¼Œæœ¬æ–‡æ—¨åœ¨ä»‹ç»æœ€æ–°ç‰ˆæœ¬ã€‚*\n\n"},{"lang":"zh","group":"models","slug":"models/mistral-nemo","frontmatter":{"title":"Mistral: Mistral Nemo","meta_title":"Mistral: Mistral Nemo","description":"Mistral: Mistral Nemo","date":"2024-07-19T00:00:00.000Z","image":"https://img.rifx.online/logo/mistral.png","categories":["text 2 text"],"author":"mistralai","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Data Science"],"draft":false,"id":"mistral-nemo","context":128000,"input":1.3e-7,"output":1.3e-7,"img":0,"request":0,"last_updated":"2024-10-31T23:10:58.000Z","slug":"models/mistral-nemo"},"content":"\nç”±Mistralä¸ŽNVIDIAåˆä½œæž„å»ºçš„12Bå‚æ•°æ¨¡åž‹ï¼Œå…·æœ‰128kçš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚\n\nè¯¥æ¨¡åž‹æ˜¯å¤šè¯­è¨€çš„ï¼Œæ”¯æŒè‹±è¯­ã€æ³•è¯­ã€å¾·è¯­ã€è¥¿ç­ç‰™è¯­ã€æ„å¤§åˆ©è¯­ã€è‘¡è„ç‰™è¯­ã€ä¸­æ–‡ã€æ—¥è¯­ã€éŸ©è¯­ã€é˜¿æ‹‰ä¼¯è¯­å’Œå°åœ°è¯­ã€‚\n\nå®ƒæ”¯æŒå‡½æ•°è°ƒç”¨ï¼Œå¹¶åœ¨Apache 2.0è®¸å¯è¯ä¸‹å‘å¸ƒã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/mistral-tiny","frontmatter":{"title":"Mistral Tiny","meta_title":"Mistral Tiny","description":"Mistral Tiny","date":"2024-01-10T00:00:00.000Z","image":"https://img.rifx.online/logo/mistral.png","categories":["text 2 text"],"author":"mistralai","tags":["Programming","Machine Learning","Data Science","Generative AI","Chatbots"],"draft":false,"id":"mistral-tiny","context":32000,"input":2.5e-7,"output":2.5e-7,"img":0,"request":0,"last_updated":"2024-10-31T23:12:22.000Z","slug":"models/mistral-tiny"},"content":"\nè¯¥æ¨¡åž‹ç›®å‰ç”± Mistral-7B-v0.2 é©±åŠ¨ï¼Œå¹¶ç»“åˆäº†æ¯” [Mistral 7B](/mistralai/mistral-7b-instruct-v0.1) æ›´â€œä¼˜è¶Šâ€çš„å¾®è°ƒï¼Œçµæ„Ÿæ¥è‡ªç¤¾åŒºçš„å·¥ä½œã€‚å®ƒæœ€é€‚åˆç”¨äºŽå¤§æ‰¹é‡å¤„ç†ä»»åŠ¡ï¼Œåœ¨è¿™äº›ä»»åŠ¡ä¸­ï¼Œæˆæœ¬æ˜¯ä¸€ä¸ªé‡è¦å› ç´ ï¼Œä½†æŽ¨ç†èƒ½åŠ›å¹¶ä¸æ˜¯å…³é”®ã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/mixtral-8x22b-instruct","frontmatter":{"title":"Mistral: Mixtral 8x22B Instruct","meta_title":"Mistral: Mixtral 8x22B Instruct","description":"Mistral: Mixtral 8x22B Instruct","date":"2024-04-17T00:00:00.000Z","image":"https://img.rifx.online/logo/mistral.png","categories":["text 2 text"],"author":"mistralai","tags":["Programming","Natural Language Processing","Machine Learning","Data Science","Generative AI"],"draft":false,"is_recommended":false,"id":"mixtral-8x22b-instruct","context":65536,"input":9e-7,"output":9e-7,"img":0,"request":0,"last_updated":"2024-11-14T08:21:32.000Z","slug":"models/mixtral-8x22b-instruct"},"content":"\nMistralçš„å®˜æ–¹æŒ‡ä»¤å¾®è°ƒç‰ˆæœ¬[Mixtral 8x22B](/mistralai/mixtral-8x22b)ã€‚å®ƒä½¿ç”¨141Bä¸­çš„39Bæ´»è·ƒå‚æ•°ï¼Œä¸ºå…¶è§„æ¨¡æä¾›æ— ä¸Žä¼¦æ¯”çš„æˆæœ¬æ•ˆç›Šã€‚å®ƒçš„ä¼˜ç‚¹åŒ…æ‹¬ï¼š\n- å¼ºå¤§çš„æ•°å­¦ã€ç¼–ç å’ŒæŽ¨ç†èƒ½åŠ›\n- å¤§ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆ64kï¼‰\n- æµåˆ©çš„è‹±è¯­ã€æ³•è¯­ã€æ„å¤§åˆ©è¯­ã€å¾·è¯­å’Œè¥¿ç­ç‰™è¯­\n\nåœ¨å‘å¸ƒå…¬å‘Šä¸­æŸ¥çœ‹åŸºå‡†æµ‹è¯•[è¿™é‡Œ](https://mistral.ai/news/mixtral-8x22b/)ã€‚\n#moe\n\n"},{"lang":"zh","group":"models","slug":"models/mixtral-8x7b","frontmatter":{"title":"Mixtral 8x7B (base)","meta_title":"Mixtral 8x7B (base)","description":"Mixtral 8x7B (base)","date":"2023-12-10T00:00:00.000Z","image":"https://img.rifx.online/logo/mistral.png","categories":["text 2 text"],"author":"mistralai","tags":["Generative AI","Machine Learning","Data Science","Programming","Technology/Web"],"draft":false,"is_recommended":true,"id":"mixtral-8x7b","context":32768,"input":5.4e-7,"output":5.4e-7,"img":0,"request":0,"last_updated":"2024-11-14T08:22:09.000Z","slug":"models/mixtral-8x7b"},"content":"\nä¸€ä¸ªç”±Mistral AIå¼€å‘çš„é¢„è®­ç»ƒç”Ÿæˆç¨€ç–ä¸“å®¶æ··åˆæ¨¡åž‹ï¼ŒåŒ…å«8ä¸ªä¸“å®¶ï¼ˆå‰é¦ˆç½‘ç»œï¼‰ï¼Œæ€»è®¡47Bå‚æ•°ã€‚åŸºç¡€æ¨¡åž‹ï¼ˆæœªé’ˆå¯¹æŒ‡ä»¤è¿›è¡Œå¾®è°ƒï¼‰ - è¯·å‚è§[Mixtral 8x7B Instruct](/mistralai/mixtral-8x7b-instruct)ä»¥èŽ·å–ç»è¿‡æŒ‡ä»¤å¾®è°ƒçš„æ¨¡åž‹ã€‚\n\n#moe\n\n"},{"lang":"zh","group":"models","slug":"models/mn-inferor-12b","frontmatter":{"title":"Mistral Nemo Inferor 12B","meta_title":"Mistral Nemo Inferor 12B","description":"Mistral Nemo Inferor 12B","date":"2024-11-13T02:20:28.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"infermatic","tags":["Roleplay","Programming","Machine Learning","Natural Language Processing","Generative AI"],"draft":false,"id":"mn-inferor-12b","context":32000,"input":2.5e-7,"output":5e-7,"img":0,"request":0,"last_updated":"2024-11-14T02:10:35.000Z","slug":"models/mn-inferor-12b"},"content":"\nInferor æ˜¯é¡¶çº§è§’è‰²æ‰®æ¼”æ¨¡åž‹çš„åˆå¹¶ä½“ï¼Œä¸“æ³¨äºŽæ²‰æµ¸å¼å™äº‹å’Œæ•…äº‹è®²è¿°ã€‚\n\nè¯¥æ¨¡åž‹ä½¿ç”¨ [Model Stock](https://arxiv.org/abs/2403.19522) åˆå¹¶æ–¹æ³•ï¼ŒåŸºäºŽ [anthracite-org/magnum-v4-12b](https://openrouter.ai/anthracite-org/magnum-v4-72b) è¿›è¡Œåˆå¹¶ã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/mn-starcannon-12b","frontmatter":{"title":"Mistral Nemo 12B Starcannon","meta_title":"Mistral Nemo 12B Starcannon","description":"Mistral Nemo 12B Starcannon","date":"2024-08-13T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"aetherwiing","tags":["Roleplay","Programming","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"mn-starcannon-12b","context":12000,"input":0.000002,"output":0.000002,"img":0,"request":0,"last_updated":"2024-11-11T03:16:53.000Z","slug":"models/mn-starcannon-12b"},"content":"\nStarcannon 12B æ˜¯ä¸€ä¸ªåˆ›æ„è§’è‰²æ‰®æ¼”å’Œæ•…äº‹å†™ä½œæ¨¡åž‹ï¼ŒåŸºäºŽ [nothingiisreal/mn-celeste-12b](https://openrouter.ai/nothingiisreal/mn-celeste-12b) å¹¶ä½¿ç”¨ [intervitens/mini-magnum-12b-v1.1](https://huggingface.co/intervitens/mini-magnum-12b-v1.1) åˆå¹¶ï¼Œé‡‡ç”¨ [TIES](https://arxiv.org/abs/2306.01708) æ–¹æ³•ã€‚\n\nè™½ç„¶æ•´ä½“ä¸Šæ›´ç±»ä¼¼äºŽ Magnumï¼Œä½†è¯¥æ¨¡åž‹ä»ç„¶éžå¸¸å…·æœ‰åˆ›æ„ï¼Œå†™ä½œé£Žæ ¼æ„‰æ‚¦ã€‚æŽ¨èç»™é‚£äº›å¸Œæœ›èŽ·å¾—æ¯” Magnum æ›´å¤šå˜åŒ–ï¼ŒåŒæ—¶åˆå¸Œæœ›æ¯” Celeste æ›´å†—é•¿çš„æ•£æ–‡çš„äººã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/mythomax-l2-13b","frontmatter":{"title":"MythoMax 13B","meta_title":"MythoMax 13B","description":"MythoMax 13B","date":"2023-07-02T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"gryphe","tags":["Roleplay","Programming","Machine Learning","Natural Language Processing","Generative AI"],"draft":false,"id":"mythomax-l2-13b","context":4096,"input":1e-7,"output":1e-7,"img":0,"request":0,"last_updated":"2024-10-28T13:10:41.000Z","slug":"models/mythomax-l2-13b"},"content":"\nLlama 2 13B çš„æ€§èƒ½æœ€é«˜ä¸”æœ€å—æ¬¢è¿Žçš„å¾®è°ƒä¹‹ä¸€ï¼Œå…·æœ‰ä¸°å¯Œçš„æè¿°å’Œè§’è‰²æ‰®æ¼”ã€‚ #merge\n\n"},{"lang":"zh","group":"models","slug":"models/o1-mini","frontmatter":{"title":"OpenAI: o1-mini","meta_title":"OpenAI: o1-mini","description":"OpenAI: o1-mini","date":"2024-09-12T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"openai","tags":["Programming","Science","Natural Language Processing","Machine Learning","Data Science"],"draft":false,"id":"o1-mini","context":128000,"input":0.000003,"output":0.000012,"img":0,"request":0,"last_updated":"2024-09-12T00:00:00.000Z","slug":"models/o1-mini"},"content":"\nOpenAIæœ€æ–°ä¸”æœ€å¼ºå¤§çš„æ¨¡åž‹ç³»åˆ—o1æ—¨åœ¨åœ¨å“åº”ä¹‹å‰èŠ±æ›´å¤šæ—¶é—´æ€è€ƒã€‚\n\no1æ¨¡åž‹ç»è¿‡ä¼˜åŒ–ï¼Œé€‚ç”¨äºŽæ•°å­¦ã€ç§‘å­¦ã€ç¼–ç¨‹åŠå…¶ä»–STEMç›¸å…³ä»»åŠ¡ã€‚å®ƒä»¬åœ¨ç‰©ç†ã€åŒ–å­¦å’Œç”Ÿç‰©å­¦çš„åŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆå±•çŽ°å‡ºåšå£«çº§çš„å‡†ç¡®æ€§ã€‚äº†è§£æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[å‘å¸ƒå…¬å‘Š](https://openai.com/o1)ã€‚\n\næ³¨æ„ï¼šè¯¥æ¨¡åž‹ç›®å‰å¤„äºŽå®žéªŒé˜¶æ®µï¼Œä¸é€‚åˆç”Ÿäº§ä½¿ç”¨ï¼Œå¹¶å¯èƒ½å—åˆ°ä¸¥æ ¼çš„é€ŸçŽ‡é™åˆ¶ã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/o1-preview","frontmatter":{"title":"OpenAI: o1-preview","meta_title":"OpenAI: o1-preview","description":"OpenAI: o1-preview","date":"2024-09-12T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"openai","tags":["Programming","Science","Natural Language Processing","Machine Learning","Data Science"],"draft":false,"id":"o1-preview","context":128000,"input":0.000015,"output":0.00006,"img":0,"request":0,"last_updated":"2024-09-12T00:00:00.000Z","slug":"models/o1-preview"},"content":"\nOpenAIæœ€æ–°ä¸”æœ€å¼ºå¤§çš„æ¨¡åž‹ç³»åˆ—o1æ—¨åœ¨åœ¨å“åº”ä¹‹å‰èŠ±æ›´å¤šæ—¶é—´æ€è€ƒã€‚\n\no1æ¨¡åž‹ç»è¿‡ä¼˜åŒ–ï¼Œé€‚ç”¨äºŽæ•°å­¦ã€ç§‘å­¦ã€ç¼–ç¨‹å’Œå…¶ä»–STEMç›¸å…³ä»»åŠ¡ã€‚å®ƒä»¬åœ¨ç‰©ç†ã€åŒ–å­¦å’Œç”Ÿç‰©å­¦çš„åŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆè¡¨çŽ°å‡ºåšå£«çº§çš„å‡†ç¡®æ€§ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[å‘å¸ƒå…¬å‘Š](https://openai.com/o1)ã€‚\n\næ³¨æ„ï¼šè¯¥æ¨¡åž‹ç›®å‰å¤„äºŽå®žéªŒé˜¶æ®µï¼Œä¸é€‚åˆç”Ÿäº§ä½¿ç”¨æ¡ˆä¾‹ï¼Œå¹¶ä¸”å¯èƒ½ä¼šå—åˆ°ä¸¥æ ¼çš„é€ŸçŽ‡é™åˆ¶ã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/openai-gpt-4o-mini","frontmatter":{"title":"OpenAI: GPT-4o-Mini Official","meta_title":"OpenAI: GPT-4o-Mini Official","description":"OpenAI: GPT-4o-Mini Official","date":"2024-10-26T09:00:07.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"gpt-4o-mini","tags":["Generative AI","Natural Language Processing","Machine Learning","Technology","Chatbots"],"draft":false,"is_recommended":false,"id":"openai/gpt-4o-mini","context":128000,"input":1.5e-7,"output":6e-7,"img":0.0036125,"request":0,"last_updated":"2024-11-14T09:46:04.000Z","slug":"models/openai-gpt-4o-mini"},"content":"\nGPT-4o mini æ˜¯ OpenAI æœ€æ–°çš„æ¨¡åž‹ï¼Œç»§ [GPT-4 Omni](/openai/gpt-4o) ä¹‹åŽæŽ¨å‡ºï¼Œæ”¯æŒæ–‡æœ¬å’Œå›¾åƒè¾“å…¥ï¼Œå¹¶ç”Ÿæˆæ–‡æœ¬è¾“å‡ºã€‚\n\nä½œä¸ºä»–ä»¬æœ€å…ˆè¿›çš„å°åž‹æ¨¡åž‹ï¼Œå®ƒçš„ä»·æ ¼æ¯”å…¶ä»–æœ€è¿‘çš„å‰æ²¿æ¨¡åž‹ä¾¿å®œäº†è®¸å¤šï¼Œä¸”æ¯” [GPT-3.5 Turbo](/openai/gpt-3.5-turbo) ä¾¿å®œè¶…è¿‡ 60%ã€‚å®ƒä¿æŒäº† SOTA æ™ºèƒ½ï¼ŒåŒæ—¶åœ¨æˆæœ¬æ•ˆç›Šä¸Šæ˜¾è‘—æ›´é«˜ã€‚\n\nGPT-4o mini åœ¨ MMLU ä¸Šå–å¾—äº† 82% çš„åˆ†æ•°ï¼Œç›®å‰åœ¨èŠå¤©åå¥½ [å¸¸è§æŽ’è¡Œæ¦œ](https://arena.lmsys.org/) ä¸Šçš„æŽ’åé«˜äºŽ GPT-4ã€‚\n\næŸ¥çœ‹ [å‘å¸ƒå…¬å‘Š](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/) ä»¥äº†è§£æ›´å¤šä¿¡æ¯ã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/openai-gpt-4o","frontmatter":{"title":"OpenAI: GPT-4o Official","meta_title":"OpenAI: GPT-4o Official","description":"OpenAI: GPT-4o Official","date":"2024-11-14T02:53:29.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"gpt-4o","tags":["Generative AI","Natural Language Processing","Technology","Chatbots","Machine Learning"],"draft":false,"is_recommended":false,"id":"openai/gpt-4o","context":128000,"input":0.0000025,"output":0.00001,"img":0.0036125,"request":0,"last_updated":"2024-11-14T09:45:25.000Z","slug":"models/openai-gpt-4o"},"content":"\nGPT-4oï¼ˆâ€œoâ€ä»£è¡¨â€œå…¨èƒ½â€ï¼‰æ˜¯OpenAIæœ€æ–°çš„AIæ¨¡åž‹ï¼Œæ”¯æŒæ–‡æœ¬å’Œå›¾åƒè¾“å…¥ï¼Œå¹¶æä¾›æ–‡æœ¬è¾“å‡ºã€‚å®ƒä¿æŒäº†[GPT-4 Turbo](/openai/gpt-4-turbo)çš„æ™ºèƒ½æ°´å¹³ï¼ŒåŒæ—¶é€Ÿåº¦æ˜¯å…¶ä¸¤å€ï¼Œæˆæœ¬æ•ˆç›Šæé«˜äº†50%ã€‚GPT-4oè¿˜åœ¨å¤„ç†éžè‹±è¯­è¯­è¨€å’Œå¢žå¼ºè§†è§‰èƒ½åŠ›æ–¹é¢æä¾›äº†æ›´å¥½çš„æ€§èƒ½ã€‚\n\nä¸ºäº†ä¸Žå…¶ä»–æ¨¡åž‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå®ƒæ›¾è¢«ç§°ä¸º[\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209)\n\n"},{"lang":"zh","group":"models","slug":"models/openchat-7b","frontmatter":{"title":"OpenChat 3.5 7B","meta_title":"OpenChat 3.5 7B","description":"OpenChat 3.5 7B","date":"2023-11-28T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"openchat","tags":["Programming","Natural Language Processing","Machine Learning","Open Source","Generative AI"],"draft":false,"id":"openchat-7b","context":8192,"input":5.5e-8,"output":5.5e-8,"img":0,"request":0,"last_updated":"2024-11-07T09:39:42.000Z","slug":"models/openchat-7b"},"content":"\nOpenChat 7B æ˜¯ä¸€ä¸ªå¼€æºè¯­è¨€æ¨¡åž‹åº“ï¼Œé‡‡ç”¨â€œC-RLFTï¼ˆæ¡ä»¶å¼ºåŒ–å­¦ä¹ å¾®è°ƒï¼‰â€ç­–ç•¥è¿›è¡Œäº†å¾®è°ƒï¼Œè¯¥ç­–ç•¥å—åˆ°ç¦»çº¿å¼ºåŒ–å­¦ä¹ çš„å¯å‘ã€‚å®ƒåœ¨æ²¡æœ‰åå¥½æ ‡ç­¾çš„æ··åˆè´¨é‡æ•°æ®ä¸Šè¿›è¡Œäº†è®­ç»ƒã€‚\n\n- å¯¹äºŽåœ¨ Mistral 7B ä¸Šå¾®è°ƒçš„ OpenChatï¼Œè¯·æŸ¥çœ‹ [OpenChat 7B](/openchat/openchat-7b)ã€‚\n- å¯¹äºŽåœ¨ Llama 8B ä¸Šå¾®è°ƒçš„ OpenChatï¼Œè¯·æŸ¥çœ‹ [OpenChat 8B](/openchat/openchat-8b)ã€‚\n\n#open-source\n\n"},{"lang":"zh","group":"models","slug":"models/palm-2-chat-bison-32k","frontmatter":{"title":"Google: PaLM 2 Chat 32k","meta_title":"Google: PaLM 2 Chat 32k","description":"Google: PaLM 2 Chat 32k","date":"2023-11-03T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text 2 text"],"author":"google","tags":["Natural Language Processing","Programming","Technology","Chatbots","Generative AI"],"draft":false,"id":"palm-2-chat-bison-32k","context":32760,"input":0.000001,"output":0.000002,"img":0,"request":0,"last_updated":"2024-11-11T03:15:52.000Z","slug":"models/palm-2-chat-bison-32k"},"content":"\nPaLM 2 æ˜¯è°·æ­ŒæŽ¨å‡ºçš„ä¸€ç§è¯­è¨€æ¨¡åž‹ï¼Œå…·å¤‡æ›´å¼ºçš„å¤šè¯­è¨€ã€æŽ¨ç†å’Œç¼–ç èƒ½åŠ›ã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/palm-2-codechat-bison-32k","frontmatter":{"title":"Google: PaLM 2 Code Chat 32k","meta_title":"Google: PaLM 2 Code Chat 32k","description":"Google: PaLM 2 Code Chat 32k","date":"2023-11-03T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text 2 text"],"author":"google","tags":["Programming","Chatbots","Natural Language Processing","Generative AI","Technology/Web"],"draft":false,"id":"palm-2-codechat-bison-32k","context":32760,"input":0.000001,"output":0.000002,"img":0,"request":0,"last_updated":"2024-11-11T03:16:01.000Z","slug":"models/palm-2-codechat-bison-32k"},"content":"\nPaLM 2 é’ˆå¯¹å¸®åŠ©è§£å†³ä»£ç ç›¸å…³é—®é¢˜çš„èŠå¤©æœºå™¨äººå¯¹è¯è¿›è¡Œäº†å¾®è°ƒã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/phi-3-medium-128k-instruct","frontmatter":{"title":"Phi-3 Medium 128K Instruct","meta_title":"Phi-3 Medium 128K Instruct","description":"Phi-3 Medium 128K Instruct","date":"2024-05-24T00:00:00.000Z","image":"https://img.rifx.online/logo/microsoft.svg","categories":["text 2 text"],"author":"microsoft","tags":["Natural Language Processing","Machine Learning","Programming","Data Science","Generative AI"],"draft":false,"id":"phi-3-medium-128k-instruct","context":128000,"input":0.000001,"output":0.000001,"img":0,"request":0,"last_updated":"2024-11-11T03:17:38.000Z","slug":"models/phi-3-medium-128k-instruct"},"content":"\nPhi-3 128K Medium æ˜¯ä¸€ä¸ªå¼ºå¤§çš„ 140 äº¿å‚æ•°æ¨¡åž‹ï¼Œæ—¨åœ¨å®žçŽ°é«˜çº§è¯­è¨€ç†è§£ã€æŽ¨ç†å’ŒæŒ‡ä»¤è·Ÿéšã€‚é€šè¿‡ç›‘ç£å¾®è°ƒå’Œåå¥½è°ƒæ•´è¿›è¡Œä¼˜åŒ–ï¼Œå®ƒåœ¨æ¶‰åŠå¸¸è¯†ã€æ•°å­¦ã€é€»è¾‘æŽ¨ç†å’Œä»£ç å¤„ç†çš„ä»»åŠ¡ä¸­è¡¨çŽ°å‡ºè‰²ã€‚\n\nåœ¨å‘å¸ƒæ—¶ï¼ŒPhi-3 Medium åœ¨è½»é‡çº§æ¨¡åž‹ä¸­å±•ç¤ºäº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åœ¨ MMLU-Pro è¯„ä¼°ä¸­ï¼Œè¯¥æ¨¡åž‹ç”šè‡³æŽ¥è¿‘ Llama3 70B çš„æ€§èƒ½æ°´å¹³ã€‚\n\nå¯¹äºŽ 4k ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œè¯·å°è¯• [Phi-3 Medium 4K](/microsoft/phi-3-medium-4k-instruct).\n\n"},{"lang":"zh","group":"models","slug":"models/phi-3-mini-128k-instruct","frontmatter":{"title":"Phi-3 Mini 128K Instruct","meta_title":"Phi-3 Mini 128K Instruct","description":"Phi-3 Mini 128K Instruct","date":"2024-05-26T00:00:00.000Z","image":"https://img.rifx.online/logo/microsoft.svg","categories":["text 2 text"],"author":"microsoft","tags":["Natural Language Processing","Machine Learning","Programming","Data Science","Generative AI"],"draft":false,"id":"phi-3-mini-128k-instruct","context":128000,"input":1e-7,"output":1e-7,"img":0,"request":0,"last_updated":"2024-11-11T03:17:47.000Z","slug":"models/phi-3-mini-128k-instruct"},"content":"\nPhi-3 Mini æ˜¯ä¸€ä¸ªå¼ºå¤§çš„ 3.8B å‚æ•°æ¨¡åž‹ï¼Œæ—¨åœ¨å®žçŽ°é«˜çº§è¯­è¨€ç†è§£ã€æŽ¨ç†å’ŒæŒ‡ä»¤è·Ÿéšã€‚é€šè¿‡ç›‘ç£å¾®è°ƒå’Œåå¥½è°ƒæ•´è¿›è¡Œä¼˜åŒ–ï¼Œå®ƒåœ¨æ¶‰åŠå¸¸è¯†ã€æ•°å­¦ã€é€»è¾‘æŽ¨ç†å’Œä»£ç å¤„ç†çš„ä»»åŠ¡ä¸­è¡¨çŽ°å‡ºè‰²ã€‚\n\nåœ¨å‘å¸ƒæ—¶ï¼ŒPhi-3 Medium åœ¨è½»é‡çº§æ¨¡åž‹ä¸­å±•ç¤ºäº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¯¥æ¨¡åž‹æ˜¯é™æ€çš„ï¼Œè®­ç»ƒäºŽæˆªæ­¢åˆ° 2023 å¹´ 10 æœˆçš„ç¦»çº¿æ•°æ®é›†ã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/phi-35-mini-128k-instruct","frontmatter":{"title":"Phi-3.5 Mini 128K Instruct","meta_title":"Phi-3.5 Mini 128K Instruct","description":"Phi-3.5 Mini 128K Instruct","date":"2024-08-21T00:00:00.000Z","image":"https://img.rifx.online/logo/microsoft.svg","categories":["text 2 text"],"author":"microsoft","tags":["Programming","Machine Learning","Natural Language Processing","Data Science","Generative AI"],"draft":false,"id":"phi-3.5-mini-128k-instruct","context":128000,"input":1e-7,"output":1e-7,"img":0,"request":0,"last_updated":"2024-11-01T04:17:17.000Z","slug":"models/phi-35-mini-128k-instruct"},"content":"\nPhi-3.5 æ¨¡åž‹æ˜¯è½»é‡çº§çš„ã€å…ˆè¿›çš„å¼€æ”¾æ¨¡åž‹ã€‚è¿™äº›æ¨¡åž‹ä½¿ç”¨ Phi-3 æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œè¯¥æ•°æ®é›†åŒ…æ‹¬åˆæˆæ•°æ®å’Œç»è¿‡ç­›é€‰çš„å…¬å…±ç½‘ç«™æ•°æ®ï¼Œé‡ç‚¹å…³æ³¨é«˜è´¨é‡å’ŒæŽ¨ç†å¯†é›†çš„ç‰¹æ€§ã€‚Phi-3.5 Mini ä½¿ç”¨ 3.8B å‚æ•°ï¼Œæ˜¯ä¸€ç§ä»…è§£ç çš„ç¨ å¯†å˜æ¢å™¨æ¨¡åž‹ï¼Œä½¿ç”¨ä¸Ž [Phi-3 Mini](/microsoft/phi-3-mini-128k-instruct) ç›¸åŒçš„åˆ†è¯å™¨ã€‚\n\nè¿™äº›æ¨¡åž‹ç»è¿‡ä¸¥æ ¼çš„å¢žå¼ºè¿‡ç¨‹ï¼Œç»“åˆäº†ç›‘ç£å¾®è°ƒã€é‚»è¿‘ç­–ç•¥ä¼˜åŒ–å’Œç›´æŽ¥åå¥½ä¼˜åŒ–ï¼Œä»¥ç¡®ä¿ç²¾ç¡®çš„æŒ‡ä»¤éµå¾ªå’Œå¼ºå¤§çš„å®‰å…¨æŽªæ–½ã€‚åœ¨é’ˆå¯¹æµ‹è¯•å¸¸è¯†ã€è¯­è¨€ç†è§£ã€æ•°å­¦ã€ä»£ç ã€é•¿ä¸Šä¸‹æ–‡å’Œé€»è¾‘æŽ¨ç†çš„åŸºå‡†è¯„ä¼°ä¸­ï¼ŒPhi-3.5 æ¨¡åž‹åœ¨å‚æ•°å°‘äºŽ 130 äº¿çš„æ¨¡åž‹ä¸­å±•ç¤ºäº†å¼ºå¤§ä¸”å…ˆè¿›çš„æ€§èƒ½ã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/pixtral-12b","frontmatter":{"title":"Mistral: Pixtral 12B","meta_title":"Mistral: Pixtral 12B","description":"Mistral: Pixtral 12B","date":"2024-09-10T00:00:00.000Z","image":"https://img.rifx.online/logo/mistral.png","categories":["text image 2 text"],"author":"mistralai","tags":["Natural Language Processing","Machine Learning","Technology","Generative AI","Computer Vision"],"draft":false,"id":"pixtral-12b","context":4096,"input":1e-7,"output":1e-7,"img":0.0001445,"request":0,"last_updated":"2024-11-11T03:10:29.000Z","slug":"models/pixtral-12b"},"content":"\nMistral AI çš„ç¬¬ä¸€ä¸ªå›¾åƒåˆ°æ–‡æœ¬æ¨¡åž‹ã€‚æ ¹æ®ä»–ä»¬çš„ä¼ ç»Ÿï¼Œå…¶æƒé‡é€šè¿‡ torrent å‘å¸ƒï¼š https://x.com/mistralai/status/1833758285167722836\n\n"},{"lang":"zh","group":"models","slug":"models/qwen-2-7b-instruct","frontmatter":{"title":"Qwen 2 7B Instruct","meta_title":"Qwen 2 7B Instruct","description":"Qwen 2 7B Instruct","date":"2024-07-16T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"qwen","tags":["Natural Language Processing","Programming","Machine Learning","Data Science","Ethics"],"draft":false,"id":"qwen-2-7b-instruct","context":32768,"input":5.4e-8,"output":5.4e-8,"img":0,"request":0,"last_updated":"2024-07-16T00:00:00.000Z","slug":"models/qwen-2-7b-instruct"},"content":"\nQwen2 7B æ˜¯ä¸€ä¸ªåŸºäºŽå˜æ¢å™¨çš„æ¨¡åž‹ï¼Œæ“…é•¿è¯­è¨€ç†è§£ã€å¤šè¯­è¨€èƒ½åŠ›ã€ç¼–ç ã€æ•°å­¦å’ŒæŽ¨ç†ã€‚\n\nå®ƒå…·æœ‰ SwiGLU æ¿€æ´»ã€æ³¨æ„åŠ› QKV åç½®å’Œç»„æŸ¥è¯¢æ³¨æ„åŠ›ã€‚å®ƒåœ¨å¤§é‡æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶ç»è¿‡ç›‘ç£å¾®è°ƒå’Œç›´æŽ¥åå¥½ä¼˜åŒ–ã€‚\n\næœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…æ­¤ [åšå®¢æ–‡ç« ](https://qwenlm.github.io/blog/qwen2/) å’Œ [GitHub ä»“åº“](https://github.com/QwenLM/Qwen2)ã€‚\n\nä½¿ç”¨æ­¤æ¨¡åž‹é¡»éµå¾ª [åŒä¹‰åƒé—®è®¸å¯è¯åè®®](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE)ã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/qwen-2-vl-72b-instruct","frontmatter":{"title":"Qwen2-VL 72B Instruct","meta_title":"Qwen2-VL 72B Instruct","description":"Qwen2-VL 72B Instruct","date":"2024-09-18T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text image 2 text"],"author":"qwen","tags":["Natural Language Processing","Computer Vision","Robotics","Machine Learning"],"draft":false,"id":"qwen-2-vl-72b-instruct","context":32768,"input":4e-7,"output":4e-7,"img":0.000578,"request":0,"last_updated":"2024-09-18T00:00:00.000Z","slug":"models/qwen-2-vl-72b-instruct"},"content":"\nQwen2 VL 72B æ˜¯æ¥è‡ª Qwen Team çš„å¤šæ¨¡æ€ LLMï¼Œå…·æœ‰ä»¥ä¸‹å…³é”®å¢žå¼ºåŠŸèƒ½ï¼š\n\n- å¯¹å„ç§åˆ†è¾¨çŽ‡å’Œæ¯”ä¾‹å›¾åƒçš„æœ€å…ˆè¿›ç†è§£ï¼šQwen2-VL åœ¨è§†è§‰ç†è§£åŸºå‡†æµ‹è¯•ä¸­å®žçŽ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬ MathVistaã€DocVQAã€RealWorldQAã€MTVQA ç­‰ã€‚\n\n- ç†è§£è¶…è¿‡ 20 åˆ†é’Ÿçš„è§†é¢‘ï¼šQwen2-VL å¯ä»¥ç†è§£è¶…è¿‡ 20 åˆ†é’Ÿçš„è§†é¢‘ï¼Œä»¥è¿›è¡Œé«˜è´¨é‡çš„è§†é¢‘é—®ç­”ã€å¯¹è¯ã€å†…å®¹åˆ›ä½œç­‰ã€‚\n\n- èƒ½å¤Ÿæ“ä½œæ‚¨çš„æ‰‹æœºã€æœºå™¨äººç­‰çš„æ™ºèƒ½ä½“ï¼šå‡­å€Ÿå¤æ‚æŽ¨ç†å’Œå†³ç­–èƒ½åŠ›ï¼ŒQwen2-VL å¯ä»¥ä¸Žæ‰‹æœºã€æœºå™¨äººç­‰è®¾å¤‡é›†æˆï¼Œå®žçŽ°åŸºäºŽè§†è§‰çŽ¯å¢ƒå’Œæ–‡æœ¬æŒ‡ä»¤çš„è‡ªåŠ¨æ“ä½œã€‚\n\n- å¤šè¯­è¨€æ”¯æŒï¼šä¸ºäº†æœåŠ¡å…¨çƒç”¨æˆ·ï¼Œé™¤äº†è‹±è¯­å’Œä¸­æ–‡ï¼ŒQwen2-VL çŽ°åœ¨è¿˜æ”¯æŒç†è§£å›¾åƒä¸­ä¸åŒè¯­è¨€çš„æ–‡æœ¬ï¼ŒåŒ…æ‹¬å¤§å¤šæ•°æ¬§æ´²è¯­è¨€ã€æ—¥è¯­ã€éŸ©è¯­ã€é˜¿æ‹‰ä¼¯è¯­ã€è¶Šå—è¯­ç­‰ã€‚\n\næœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…æ­¤ [åšå®¢æ–‡ç« ](https://qwenlm.github.io/blog/qwen2-vl/) å’Œ [GitHub ä»“åº“](https://github.com/QwenLM/Qwen2-VL)ã€‚\n\nä½¿ç”¨æ­¤æ¨¡åž‹å— [é€šä¹‰åƒé—®è®¸å¯åè®®](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE) çš„çº¦æŸã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/qwen-2-vl-7b-instruct","frontmatter":{"title":"Qwen2-VL 7B Instruct","meta_title":"Qwen2-VL 7B Instruct","description":"Qwen2-VL 7B Instruct","date":"2024-08-28T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text image 2 text"],"author":"qwen","tags":["Natural Language Processing","Computer Vision","Robotics","Multimodal AI","Generative AI"],"draft":false,"id":"qwen-2-vl-7b-instruct","context":32768,"input":1e-7,"output":1e-7,"img":0.0001445,"request":0,"last_updated":"2024-11-11T03:13:01.000Z","slug":"models/qwen-2-vl-7b-instruct"},"content":"\nQwen2 VL 7B æ˜¯æ¥è‡ª Qwen å›¢é˜Ÿçš„å¤šæ¨¡æ€ LLMï¼Œå…·æœ‰ä»¥ä¸‹å…³é”®å¢žå¼ºåŠŸèƒ½ï¼š\n\n- å¯¹å„ç§åˆ†è¾¨çŽ‡å’Œæ¯”ä¾‹çš„å›¾åƒçš„æœ€å…ˆè¿›ç†è§£ï¼šQwen2-VL åœ¨è§†è§‰ç†è§£åŸºå‡†æµ‹è¯•ä¸­å®žçŽ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬ MathVistaã€DocVQAã€RealWorldQAã€MTVQA ç­‰ã€‚\n\n- ç†è§£è¶…è¿‡ 20 åˆ†é’Ÿçš„è§†é¢‘ï¼šQwen2-VL èƒ½å¤Ÿç†è§£è¶…è¿‡ 20 åˆ†é’Ÿçš„è§†é¢‘ï¼Œä»¥ä¾¿è¿›è¡Œé«˜è´¨é‡çš„è§†é¢‘é—®ç­”ã€å¯¹è¯ã€å†…å®¹åˆ›ä½œç­‰ã€‚\n\n- èƒ½å¤Ÿæ“ä½œæ‰‹æœºã€æœºå™¨äººç­‰çš„ä»£ç†ï¼šå‡­å€Ÿå¤æ‚æŽ¨ç†å’Œå†³ç­–èƒ½åŠ›ï¼ŒQwen2-VL å¯ä»¥ä¸Žæ‰‹æœºã€æœºå™¨äººç­‰è®¾å¤‡é›†æˆï¼Œå®žçŽ°åŸºäºŽè§†è§‰çŽ¯å¢ƒå’Œæ–‡æœ¬æŒ‡ä»¤çš„è‡ªåŠ¨æ“ä½œã€‚\n\n- å¤šè¯­è¨€æ”¯æŒï¼šä¸ºäº†æœåŠ¡å…¨çƒç”¨æˆ·ï¼Œé™¤äº†è‹±è¯­å’Œä¸­æ–‡ï¼ŒQwen2-VL çŽ°åœ¨è¿˜æ”¯æŒç†è§£å›¾åƒä¸­ä¸åŒè¯­è¨€çš„æ–‡æœ¬ï¼ŒåŒ…æ‹¬å¤§å¤šæ•°æ¬§æ´²è¯­è¨€ã€æ—¥è¯­ã€éŸ©è¯­ã€é˜¿æ‹‰ä¼¯è¯­ã€è¶Šå—è¯­ç­‰ã€‚\n\næœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…æ­¤ [åšå®¢æ–‡ç« ](https://qwenlm.github.io/blog/qwen2-vl/) å’Œ [GitHub ä»“åº“](https://github.com/QwenLM/Qwen2-VL)ã€‚\n\nä½¿ç”¨æ­¤æ¨¡åž‹é¡»éµå¾ª [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE)ã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/qwen-25-72b-instruct","frontmatter":{"title":"Qwen2.5 72B Instruct","meta_title":"Qwen2.5 72B Instruct","description":"Qwen2.5 72B Instruct","date":"2024-09-19T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"qwen","tags":["Programming","Natural Language Processing","Chatbots","Machine Learning","Data Science"],"draft":false,"id":"qwen-2.5-72b-instruct","context":131072,"input":3.5e-7,"output":4e-7,"img":0,"request":0,"last_updated":"2024-09-19T00:00:00.000Z","slug":"models/qwen-25-72b-instruct"},"content":"\nQwen2.5 72B æ˜¯ Qwen å¤§åž‹è¯­è¨€æ¨¡åž‹çš„æœ€æ–°ç³»åˆ—ã€‚Qwen2.5 åœ¨ Qwen2 çš„åŸºç¡€ä¸Šå¸¦æ¥äº†ä»¥ä¸‹æ”¹è¿›ï¼š\n\n- çŸ¥è¯†æ˜¾è‘—å¢žåŠ ï¼Œå¹¶åœ¨ç¼–ç å’Œæ•°å­¦èƒ½åŠ›ä¸Šæœ‰äº†å¾ˆå¤§æå‡ï¼Œè¿™å¾—ç›ŠäºŽæˆ‘ä»¬åœ¨è¿™äº›é¢†åŸŸçš„ä¸“ä¸šä¸“å®¶æ¨¡åž‹ã€‚\n\n- åœ¨éµå¾ªæŒ‡ä»¤ã€ç”Ÿæˆé•¿æ–‡æœ¬ï¼ˆè¶…è¿‡ 8K tokensï¼‰ã€ç†è§£ç»“æž„åŒ–æ•°æ®ï¼ˆä¾‹å¦‚ï¼Œè¡¨æ ¼ï¼‰ä»¥åŠç”Ÿæˆç»“æž„åŒ–è¾“å‡ºï¼ˆç‰¹åˆ«æ˜¯ JSONï¼‰æ–¹é¢æœ‰æ˜¾è‘—æ”¹è¿›ã€‚å¯¹ç³»ç»Ÿæç¤ºçš„å¤šæ ·æ€§æ›´åŠ å¼ºéŸ§ï¼Œå¢žå¼ºäº†è§’è‰²æ‰®æ¼”çš„å®žçŽ°å’ŒèŠå¤©æœºå™¨äººçš„æ¡ä»¶è®¾ç½®ã€‚\n\n- æ”¯æŒæœ€é•¿ 128K tokens çš„é•¿ä¸Šä¸‹æ–‡ï¼Œå¹¶å¯ä»¥ç”Ÿæˆæœ€å¤š 8K tokensã€‚\n\n- æ”¯æŒè¶…è¿‡ 29 ç§è¯­è¨€ï¼ŒåŒ…æ‹¬ä¸­æ–‡ã€è‹±æ–‡ã€æ³•è¯­ã€è¥¿ç­ç‰™è¯­ã€è‘¡è„ç‰™è¯­ã€å¾·è¯­ã€æ„å¤§åˆ©è¯­ã€ä¿„è¯­ã€æ—¥è¯­ã€éŸ©è¯­ã€è¶Šå—è¯­ã€æ³°è¯­ã€é˜¿æ‹‰ä¼¯è¯­ç­‰ã€‚\n\nä½¿ç”¨æ­¤æ¨¡åž‹é¡»éµå®ˆ [åŒä¹‰åƒé—®è®¸å¯åè®®](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE)ã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/qwen-25-7b-instruct","frontmatter":{"title":"Qwen2.5 7B Instruct","meta_title":"Qwen2.5 7B Instruct","description":"Qwen2.5 7B Instruct","date":"2024-10-16T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"qwen","tags":["Programming","Natural Language Processing","Chatbots","Machine Learning","Data Science"],"draft":false,"id":"qwen-2.5-7b-instruct","context":131072,"input":2.7e-7,"output":2.7e-7,"img":0,"request":0,"last_updated":"2024-10-16T00:00:00.000Z","slug":"models/qwen-25-7b-instruct"},"content":"\nQwen2.5 7B æ˜¯ Qwen å¤§è¯­è¨€æ¨¡åž‹çš„æœ€æ–°ç³»åˆ—ã€‚Qwen2.5 åœ¨ Qwen2 çš„åŸºç¡€ä¸Šå¸¦æ¥äº†ä»¥ä¸‹æ”¹è¿›ï¼š\n\n- çŸ¥è¯†æ˜¾è‘—å¢žåŠ ï¼Œå¹¶åœ¨ç¼–ç å’Œæ•°å­¦æ–¹é¢çš„èƒ½åŠ›å¤§å¹…æå‡ï¼Œè¿™å¾—ç›ŠäºŽæˆ‘ä»¬åœ¨è¿™äº›é¢†åŸŸçš„ä¸“ä¸šæ¨¡åž‹ã€‚\n\n- åœ¨éµå¾ªæŒ‡ä»¤ã€ç”Ÿæˆé•¿æ–‡æœ¬ï¼ˆè¶…è¿‡ 8K tokensï¼‰ã€ç†è§£ç»“æž„åŒ–æ•°æ®ï¼ˆä¾‹å¦‚ï¼Œè¡¨æ ¼ï¼‰ä»¥åŠç”Ÿæˆç»“æž„åŒ–è¾“å‡ºï¼Œç‰¹åˆ«æ˜¯ JSON æ–¹é¢æœ‰æ˜¾è‘—æ”¹è¿›ã€‚å¯¹ç³»ç»Ÿæç¤ºçš„å¤šæ ·æ€§æ›´å…·éŸ§æ€§ï¼Œå¢žå¼ºäº†è§’è‰²æ‰®æ¼”çš„å®žçŽ°å’ŒèŠå¤©æœºå™¨äººçš„æ¡ä»¶è®¾ç½®ã€‚\n\n- é•¿æ–‡æœ¬æ”¯æŒé«˜è¾¾ 128K tokensï¼Œå¹¶ä¸”å¯ä»¥ç”Ÿæˆæœ€å¤š 8K tokensã€‚\n\n- æ”¯æŒè¶…è¿‡ 29 ç§è¯­è¨€ï¼ŒåŒ…æ‹¬ä¸­æ–‡ã€è‹±æ–‡ã€æ³•æ–‡ã€è¥¿ç­ç‰™æ–‡ã€è‘¡è„ç‰™æ–‡ã€å¾·æ–‡ã€æ„å¤§åˆ©æ–‡ã€ä¿„æ–‡ã€æ—¥æ–‡ã€éŸ©æ–‡ã€è¶Šå—æ–‡ã€æ³°æ–‡ã€é˜¿æ‹‰ä¼¯æ–‡ç­‰ã€‚\n\nä½¿ç”¨æ­¤æ¨¡åž‹é¡»éµå®ˆ [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE)ã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/qwen-25-coder-32b-instruct","frontmatter":{"title":"Qwen2.5 Coder 32B Instruct","meta_title":"Qwen2.5 Coder 32B Instruct","description":"Qwen2.5 Coder 32B Instruct","date":"2024-11-11T23:40:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"qwen","tags":["Programming","Programming/Scripting","Machine Learning","Natural Language Processing","Generative AI"],"draft":false,"is_recommended":true,"id":"qwen-2.5-coder-32b-instruct","context":32768,"input":1.8e-7,"output":1.8e-7,"img":0,"request":0,"last_updated":"2024-11-15T23:24:23.000Z","slug":"models/qwen-25-coder-32b-instruct"},"content":"\nQwen2.5-Coder æ˜¯æœ€æ–°ä¸€ç³»åˆ—é’ˆå¯¹ä»£ç çš„ Qwen å¤§åž‹è¯­è¨€æ¨¡åž‹ï¼ˆä»¥å‰ç§°ä¸º CodeQwenï¼‰ã€‚Qwen2.5-Coder åœ¨ CodeQwen1.5 çš„åŸºç¡€ä¸Šå¸¦æ¥äº†ä»¥ä¸‹æ”¹è¿›ï¼š\n\n- åœ¨ **ä»£ç ç”Ÿæˆ**ã€**ä»£ç æŽ¨ç†** å’Œ **ä»£ç ä¿®å¤** æ–¹é¢æœ‰æ˜¾è‘—æå‡ã€‚\n- ä¸ºçŽ°å®žä¸–ç•Œåº”ç”¨ï¼ˆå¦‚ **ä»£ç ä»£ç†**ï¼‰æä¾›äº†æ›´å…¨é¢çš„åŸºç¡€ã€‚ä¸ä»…å¢žå¼ºäº†ç¼–ç èƒ½åŠ›ï¼Œè¿˜ä¿æŒäº†å…¶åœ¨æ•°å­¦å’Œä¸€èˆ¬èƒ½åŠ›æ–¹é¢çš„ä¼˜åŠ¿ã€‚\n\nè¦äº†è§£æ›´å¤šå…³äºŽå…¶è¯„ä¼°ç»“æžœçš„ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ [Qwen 2.5 Coder çš„åšå®¢](https://qwenlm.github.io/blog/qwen2.5-coder-family/)ã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/remm-slerp-l2-13b","frontmatter":{"title":"ReMM SLERP 13B","meta_title":"ReMM SLERP 13B","description":"ReMM SLERP 13B","date":"2023-07-22T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"undi95","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"is_recommended":true,"id":"remm-slerp-l2-13b","context":4096,"input":0.000001125,"output":0.000001125,"img":0,"request":0,"last_updated":"2024-11-14T04:06:00.000Z","slug":"models/remm-slerp-l2-13b"},"content":"\nåŽŸå§‹ MythoMax-L2-B13 çš„é‡åˆ›ç‰ˆæœ¬ï¼Œä½†é‡‡ç”¨äº†æ›´æ–°çš„æ¨¡åž‹ã€‚ #merge\n\n"},{"lang":"zh","group":"models","slug":"models/remm-slerp-l2-13b:extended","frontmatter":{"title":"ReMM SLERP 13B (extended)","meta_title":"ReMM SLERP 13B (extended)","description":"ReMM SLERP 13B (extended)","date":"2023-07-22T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"undi95","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"remm-slerp-l2-13b:extended","context":6144,"input":0.000001125,"output":0.000001125,"img":0,"request":0,"last_updated":"2024-11-04T12:47:21.000Z","slug":"models/remm-slerp-l2-13b:extended"},"content":"\nåŽŸå§‹ MythoMax-L2-B13 çš„é‡çŽ°è¯•éªŒï¼Œä½†ä½¿ç”¨äº†æ›´æ–°çš„æ¨¡åž‹ã€‚ #merge\n\n_è¿™äº›æ˜¯ [ReMM SLERP 13B](/undi95/remm-slerp-l2-13b) çš„æ‰©å±•ä¸Šä¸‹æ–‡ç«¯ç‚¹ã€‚å®ƒä»¬å¯èƒ½å…·æœ‰æ›´é«˜çš„ä»·æ ¼ã€‚_\n\n"},{"lang":"zh","group":"models","slug":"models/rocinante-12b","frontmatter":{"title":"Rocinante 12B","meta_title":"Rocinante 12B","description":"Rocinante 12B","date":"2024-09-30T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"thedrummer","tags":["Roleplay","Programming","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"rocinante-12b","context":32768,"input":2.5e-7,"output":5e-7,"img":0,"request":0,"last_updated":"2024-11-11T03:09:37.000Z","slug":"models/rocinante-12b"},"content":"\nRocinante 12B æ—¨åœ¨æä¾›å¼•äººå…¥èƒœçš„å™äº‹å’Œä¸°å¯Œçš„æ•£æ–‡ã€‚\n\næ—©æœŸæµ‹è¯•è€…æŠ¥å‘Šï¼š\n- è¯æ±‡é‡æ‰©å±•ï¼Œç‹¬ç‰¹ä¸”å¯Œæœ‰è¡¨çŽ°åŠ›çš„ç”¨è¯é€‰æ‹©\n- åˆ›é€ åŠ›å¢žå¼ºï¼Œèƒ½å¤Ÿç”ŸåŠ¨å™è¿°\n- å……æ»¡å†’é™©å’Œå¼•äººå…¥èƒœçš„æ•…äº‹\n\n"},{"lang":"zh","group":"models","slug":"models/sorcererlm-8x22b","frontmatter":{"title":"Sorcererlm 8x22b","meta_title":"Sorcererlm 8x22b","description":"Sorcererlm 8x22b","date":"2024-11-08T22:31:23.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"raifle","tags":["Roleplay","Programming","Natural Language Processing","Chatbots","Generative AI"],"draft":false,"id":"sorcererlm-8x22b","context":16000,"input":0.0000045,"output":0.0000045,"img":0,"request":0,"last_updated":"2024-11-11T02:56:49.000Z","slug":"models/sorcererlm-8x22b"},"content":"\nSorcererLM æ˜¯ä¸€ä¸ªå…ˆè¿›çš„ RP å’Œæ•…äº‹è®²è¿°æ¨¡åž‹ï¼Œä½œä¸ºä¸€ä¸ªä½Žç§© 16 ä½ LoRA åœ¨ WizardLM-2-8x22B ä¸Šè¿›è¡Œå¾®è°ƒã€‚\n\n- å…ˆè¿›çš„æŽ¨ç†å’Œæƒ…æ„Ÿæ™ºèƒ½ï¼Œå®žçŽ°å¼•äººå…¥èƒœå’Œæ²‰æµ¸å¼çš„äº’åŠ¨\n- ç”ŸåŠ¨çš„å†™ä½œèƒ½åŠ›ï¼Œå¢žå¼ºäº†ç©ºé—´å’Œä¸Šä¸‹æ–‡æ„è¯†\n- å¢žå¼ºçš„å™äº‹æ·±åº¦ï¼Œä¿ƒè¿›åˆ›é€ æ€§å’ŒåŠ¨æ€çš„æ•…äº‹è®²è¿°\n\n"},{"lang":"zh","group":"models","slug":"models/toppy-m-7b","frontmatter":{"title":"Toppy M 7B","meta_title":"Toppy M 7B","description":"Toppy M 7B","date":"2023-11-10T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"undi95","tags":["Programming","Machine Learning","Generative AI","Chatbots","Data Science"],"draft":false,"id":"toppy-m-7b","context":4096,"input":7e-8,"output":7e-8,"img":0,"request":0,"last_updated":"2024-11-04T12:51:35.000Z","slug":"models/toppy-m-7b"},"content":"\nä¸€ä¸ªé‡Žç”Ÿçš„7Bå‚æ•°æ¨¡åž‹ï¼Œé€šè¿‡mergekitä¸­çš„æ–°task_arithmeticåˆå¹¶æ–¹æ³•åˆå¹¶äº†å¤šä¸ªæ¨¡åž‹ã€‚\nåˆå¹¶æ¨¡åž‹åˆ—è¡¨ï¼š\n- NousResearch/Nous-Capybara-7B-V1.9\n- [HuggingFaceH4/zephyr-7b-beta](/huggingfaceh4/zephyr-7b-beta)\n- lemonilia/AshhLimaRP-Mistral-7B\n- Vulkane/120-Days-of-Sodom-LoRA-Mistral-7b\n- Undi95/Mistral-pippa-sharegpt-7b-qlora\n\n#merge #uncensored\n\n"},{"lang":"zh","group":"models","slug":"models/unslopnemo-12b","frontmatter":{"title":"Unslopnemo 12b","meta_title":"Unslopnemo 12b","description":"Unslopnemo 12b","date":"2024-11-08T22:04:08.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"thedrummer","tags":["Roleplay","Programming","Generative AI","Chatbots","Natural Language Processing"],"draft":false,"id":"unslopnemo-12b","context":32000,"input":5e-7,"output":5e-7,"img":0,"request":0,"last_updated":"2024-11-14T02:10:09.000Z","slug":"models/unslopnemo-12b"},"content":"\nUnslopNemo v4.1 æ˜¯æ¥è‡ª Rocinante åˆ›ä½œè€…çš„æœ€æ–°ä½œå“ï¼Œæ—¨åœ¨ç”¨äºŽå†’é™©å†™ä½œå’Œè§’è‰²æ‰®æ¼”åœºæ™¯ã€‚\n\n"},{"lang":"zh","group":"models","slug":"models/wizardlm-2-7b","frontmatter":{"title":"WizardLM-2 7B","meta_title":"WizardLM-2 7B","description":"WizardLM-2 7B","date":"2024-04-16T00:00:00.000Z","image":"https://img.rifx.online/logo/microsoft.svg","categories":["text 2 text"],"author":"microsoft","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"wizardlm-2-7b","context":32000,"input":5.5e-8,"output":5.5e-8,"img":0,"request":0,"last_updated":"2024-10-31T23:23:36.000Z","slug":"models/wizardlm-2-7b"},"content":"\nWizardLM-2 7B æ˜¯å¾®è½¯ AI æœ€æ–° Wizard æ¨¡åž‹çš„è¾ƒå°ç‰ˆæœ¬ã€‚å®ƒæ˜¯æœ€å¿«çš„ï¼Œå¹¶ä¸”åœ¨æ€§èƒ½ä¸Šä¸ŽçŽ°æœ‰çš„ 10 å€æ›´å¤§çš„å¼€æºé¢†å…ˆæ¨¡åž‹ç›¸å½“ã€‚\n\nå®ƒæ˜¯å¯¹ [Mistral 7B Instruct](/mistralai/mistral-7b-instruct) çš„å¾®è°ƒï¼Œä½¿ç”¨ä¸Ž [WizardLM-2 8x22B](/microsoft/wizardlm-2-8x22b) ç›¸åŒçš„æŠ€æœ¯ã€‚\n\nè¦äº†è§£æ›´å¤šå…³äºŽæ¨¡åž‹å‘å¸ƒçš„ä¿¡æ¯ï¼Œ[è¯·ç‚¹å‡»è¿™é‡Œ](https://wizardlm.github.io/WizardLM2/)ã€‚\n\n#moe\n\n"},{"lang":"zh","group":"models","slug":"models/wizardlm-2-8x22b","frontmatter":{"title":"WizardLM-2 8x22B","meta_title":"WizardLM-2 8x22B","description":"WizardLM-2 8x22B","date":"2024-04-16T00:00:00.000Z","image":"https://img.rifx.online/logo/microsoft.svg","categories":["text 2 text"],"author":"microsoft","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"wizardlm-2-8x22b","context":65536,"input":5e-7,"output":5e-7,"img":0,"request":0,"last_updated":"2024-10-31T23:24:21.000Z","slug":"models/wizardlm-2-8x22b"},"content":"\nWizardLM-2 8x22B æ˜¯å¾®è½¯ AI æœ€å…ˆè¿›çš„ Wizard æ¨¡åž‹ã€‚ä¸Žé¢†å…ˆçš„ä¸“æœ‰æ¨¡åž‹ç›¸æ¯”ï¼Œå®ƒå±•ç¤ºäº†é«˜åº¦ç«žäº‰çš„æ€§èƒ½ï¼Œå¹¶ä¸”å§‹ç»ˆä¼˜äºŽæ‰€æœ‰çŽ°æœ‰çš„æœ€å…ˆè¿›çš„å¼€æºæ¨¡åž‹ã€‚\n\nå®ƒæ˜¯ [Mixtral 8x22B](/mistralai/mixtral-8x22b) çš„æŒ‡ä»¤å¾®è°ƒç‰ˆæœ¬ã€‚\n\nè¦äº†è§£æœ‰å…³æ¨¡åž‹å‘å¸ƒçš„æ›´å¤šä¿¡æ¯ï¼Œ[è¯·ç‚¹å‡»è¿™é‡Œ](https://wizardlm.github.io/WizardLM2/)ã€‚\n\n#moe\n\n"}]
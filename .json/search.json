[{"lang":"en","group":"blog","slug":"blog/post-1","frontmatter":{"title":"NVIDIA’s Llama 3.1 Nemotron 70B: A New Benchmark in AI Performance","meta_title":"","description":"this is meta description","date":"2022-04-04T05:00:00.000Z","image":"https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*5-KzbtHKke5nGhH-","categories":["Technology","Data"],"author":"Sam Wilson","tags":["technology","tailwind"],"draft":false,"slug":"blog/post-1"},"content":"\n\nIn a surprising move, NVIDIA has quietly released a fine-tuned version of Llama 3.1 70B that’s making waves in the AI community. This new model, called Llama 3.1 Nemotron 70B, is outperforming some of the most advanced AI models on multiple benchmarks, including OpenAI’s GPT-4 and Anthropic’s Claude 3.5 Sonnet.\n\n\n# Performance Benchmarks\n\nLet’s look at how Nemotron 70B stacks up against its competitors:\n\n**Arena Hard**\n\n* Nemotron 70B: 85.0\n* Claude 3.5 Sonnet: 79.2\n* GPT-4 (May 2024 version): 79.3\n\n**AlpacaEval 2 LC**\n\n* Nemotron 70B: 57.6\n* Claude 3.5 Sonnet: 52.4\n* GPT-4 (May 2024 version): 57.5\n\n**MT Bench**\n\n* Nemotron 70B: 8.98\n* Claude 3.5 Sonnet: 8.81\n* GPT-4 (May 2024 version): 8.74\n\nAs we can see, Nemotron 70B consistently outperforms both Claude 3.5 Sonnet and the May 2024 version of GPT-4 across these benchmarks.\n\n![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*HFsh4pfoZX9Szzcu)\n\n\n# The Secret Sauce\n\nWhat makes Nemotron 70B so powerful? NVIDIA employed several advanced techniques in its development:\n\n1. **RLHF (Reinforcement Learning from Human Feedback)**: Specifically, they used the REINFORCE algorithm.\n2. **Llama-3.1-Nemotron-70B-Reward**: A custom reward model used in the training process.\n3. **HelpSteer2-Preference prompts**: These prompts likely guided the model’s training towards more helpful and aligned responses.\n4. **Base Model**: The training started with Llama-3.1–70B-Instruct as the initial policy.\n\n\n# Open-Source Commitment\n\nIn a commendable move towards transparency and collaboration, NVIDIA has released not just the Instruct model, but also the reward model and the dataset used in training. All of these are available on Hugging Face, allowing researchers and developers to dive deep into the model’s architecture and potentially build upon this work.\n\n\n# Implications for the AI Landscape\n\nThe release of Nemotron 70B represents a significant leap forward in AI capabilities. Its superior performance across multiple benchmarks suggests that NVIDIA has made substantial improvements in areas like reasoning, language understanding, and task completion.\n\nThis development could have far-reaching implications for various AI applications, from more sophisticated chatbots and virtual assistants to advanced language translation and content generation tools.\n\nMoreover, NVIDIA’s decision to open-source key components of the model could accelerate progress in the field, enabling other researchers to build upon this work and potentially leading to even more advanced AI systems in the near future.\n\nAs the AI race continues to heat up, it’s clear that NVIDIA is positioning itself as a major player, not just in hardware but also in cutting-edge AI model development. It will be fascinating to see how other tech giants respond to this impressive new benchmark in AI performance.\n\n"},{"lang":"en","group":"blog","slug":"blog/post-2","frontmatter":{"title":"Testing the new Nemotron 70B model from Nvidia","meta_title":"","description":"this is meta description","date":"2022-04-04T05:00:00.000Z","image":"https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*-z7oZo_Kb3c9ELsk","categories":["Technology","Data"],"author":"Sam Wilson","tags":["technology","tailwind"],"draft":false,"slug":"blog/post-2"},"content":"\n## Can this open-source model dethrone GPT-4o and Sonnet 3.5?\n\nThere’s been a lot of buzz around the release of the latest model from Nvidia. It’s a tuned version of Meta’s Llama 3.1 70B model that uses a two-layer reward system and a so-called `**HelpSteer**` data set to enhance the responses it gives to user requests.\n\nApparently, it’s destroying all the top models in the Arena hard, MT-Bench and AlpacaEval 2 benchmarks which are known to be predictive of [LMSys Chatbot Arena Elo](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard) performance.\n\nWell, we’ll see about that. As usual, I like to conduct my own tests to see how different models compare.\n\n**[Stop Press — I have a added an additional question to ask]**\n\n\n## Model performance\n\nBelow is a table that pits Nemotron against various other top closed-source models and — blow me down — it outperforms them all. Who’da thunk!\n\n![](https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BJm80jDr5L1aAkpAThj8Kg.png)\n\n\n## Accessing the new model\n\nThe new model is available to test from a few places, including:-\n\n* Hugging Face — <https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Instruct>\n* Nvidia —<https://build.nvidia.com/nvidia/llama-3_1-nemotron-70b-instruct>\n* HuggingFace Chat — <https://huggingface.co/chat/>\n\nI used the HuggingFace Chat interface.\n\n\n## Testing the new model\n\n**Question 1**\n\n\n```python\nA bat and a ball, together cost $1.20. The bat costs $1 more than the ball. \nHow much is the ball?\n```\nAnswer 1\n\n\n```python\nA classic math problem!\n\nLet's say the ball costs x cents.\n\nThe bat costs $1 more than the ball, so the bat costs x + $1.\n\nThe total cost of the bat and the ball together is $1.20.\n\nWe can set up the equation:\n\nx + (x + 1) = 1.20\n\nCombine like terms:\n\n2x + 1 = 1.20\n\nSubtract 1 from both sides:\n\n2x = 0.20\n\nDivide both sides by 2:\n\nx = 0.10\n\nSo the ball costs 10 cents.\n\nAnswer: The ball costs 10 cents.\n```\nCorrect.\n\n**Question 2**\n\n\n```python\nA room contains only 3 people. The first person is reading a book and \nthe second person is playing chess. Based on this, what is the most \nlikely activity that the third person is engaging in?\n```\nAnswer 2\n\n\n```python\nThis is a classic lateral thinking puzzle!\n\nThe answer is not \"playing chess\" (because the second person is already playing chess, and you can't play chess with yourself).\n\nThe answer is not \"reading a book\" (because the first person is already reading a book, but that doesn't necessarily mean the third person can't be reading a book too).\n\nThe most likely activity that the third person is engaging in is... \"watching the chess game\"!\n\nWhy? Because the second person is playing chess, and it's likely that the third person is observing or spectating the game.\n\nSo, the third person is most likely watching the chess game.\n```\nOops, that’s not right at all. By that logic, the third person could just as easily be watching the first person reading their book!\n\n**Question 3**\n\n\n```python\nJimmy has 10 dollars, he gives two to Lois.  He goes to the bank and \nwithdraws 5 dollars, then to the shop where he buys a dollar fifty worth \nof candy. How much money does Jimmy have left\n```\nAnswer 3\n\n\n```python\nLet's work through the scenario step by step:\n\nJimmy starts with $10.\nHe gives $2 to Lois, so he's left with:\n$10 - $2 = $8\nHe withdraws $5 from the bank, so he now has:\n$8 + $5 = $13\nHe buys $1.50 worth of candy, so he's left with:\n$13 - $1.50 = $11.50\nJimmy has $11.50 left.\n```\nSpot on.\n\n**Question 4**\n\n\n```python\nWrite some Python code that OPTIMALLY calculates as many Fibonacci \nnumbers as possible within 0.01 seconds and prints the last N and its \ncorresponding Fibonacci number\n```\nAnswer 4\n\n\n```python\nimport time\n\ndef fibonacci(n, memo = {}):\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    elif n in memo:\n        return memo[n]\n    else:\n        memo[n] = fibonacci(n-1, memo) + fibonacci(n-2, memo)\n        return memo[n]\n\nstart_time = time.time()\nn = 0\nwhile True:\n    fib_n = fibonacci(n)\n    if time.time() - start_time > 0.01:\n        break\n    n += 1\n\nprint(f\"Last calculated Fibonacci number: F({n-1}) = {fib_n}\")\n```\nA great answer there AND it uses the memoization technique for optimum run-time. Out of interest, it printed out the 17221'ist Fibonacci number.\n\n**Question 5**\n\n\n```python\nTwo people are travelling in a car along a road. The car runs out of gas 1 \nmile from the gas station. They have two options.\n\n1) They can push the car to the gas station to fill up. With one person \nsteering, the other can can push the car at an average of 30 inches per \nsecond. \n2) Alternatively, one person can run to the gas station, borrow \na pick-up truck there and use it to tow the car back to the gas station \nto fill up. The person can run at 6 mph. Assume that the tow truck averages \n30 mph on all journeys. Also, assume it takes the person ten minutes between \narriving at the gas station and setting off in the tow truck back to the car. \nIt takes a further ten minutes to load the car onto the tow truck before \nheading back to the gas station.\n\nTo get the car to the gas station, is it quicker for car to be pushed there \nor for someone to run for the tow truck and tow the car back?\n```\nAnswer 5\n\n\n```python\nLet's analyze the two options:\n\nOption 1: Pushing the car\n\nDistance to the gas station: 1 mile = 5280 feet = 63360 inches (since 1 foot = 12 inches)\nPushing speed: 30 inches per second\nTime to push the car to the gas station:\n63360 inches / 30 inches/second = 2112 seconds\nConvert seconds to minutes:\n2112 seconds / 60 = 35.2 minutes\nOption 2: Running to the gas station and towing the car\n\nTime for the person to run to the gas station:\n1 mile / 6 mph = 0.167 hours = 10 minutes\nTime to prepare the tow truck: 10 minutes\nTime to load the car onto the tow truck: 10 minutes\nTime for the tow truck to travel to the car and back to the gas station:\n2 miles / 30 mph = 0.067 hours = 4 minutes\nTotal time for Option 2: 10 + 10 + 10 + 4 = 34 minutes\nBased on the calculations, Option 2: Running to the gas station and towing the car is the faster option, taking 34 minutes compared to 35.2 minutes for Option 1: Pushing the car.\n```\nAnother great answer.\n\n**Question 6**\n\n\n```python\nMultiply the smallest positive integer that's less than 100 by the  \nlargest negative integer greater than -100\n```\nAnswer 6\n\n\n```python\nThe smallest positive integer less than 100 is 1.\n\nThe largest negative integer greater than -100 is -1.\n\nMultiplying them together gives:\n1 × -1 = -1\n```\nCorrect.\n\n**Question 7**\n\n\n```python\nItem 1 is priced at 50% more than half the price of the full price of item 2. \nItem 2 is on sale right now, priced at $2.70,  10% off its regular full price. \nWhat is the price of item 1\n```\nAnswer 7\n\n\n```python\nLet's break it down step by step:\n\nItem 2 is on sale at $2.70, which is 10% off its regular price. To find the regular price, we can calculate:\nRegular price of Item 2 = $2.70 / 0.9 (since 10% off means 90% of the original price) = $3.00\n\nItem 1 is priced at 50% more than half the price of the full price of Item 2. First, find half the price of the full price of Item 2:\nHalf the price of Item 2 = $3.00 / 2 = $1.50\n\nNow, calculate 50% more than half the price of Item 2:\n$1.50 x 1.5 (50% more) = $2.25\n\nSo, the price of Item 1 is $2.25.\n```\nCorrect again.\n\nI have a new logic-based question.\n\n**Question 8**\n\n\n```python\nDanid eats one banana per day, usually as soon as he gets back from shopping. \nHe went to the shops on Monday and bought 5 bananas, He went again two days \nlater, but is distracted by the oranges on sale and buys 3 of them. \nThe day before he had bought two bananas but one was \nsmaller than he liked. He didn't shop on Thursday, but the day \nafter he bought 2 more.\nHow many bananas does David have?\n```\n**Answer 8**\n\n\n```python\nLet's break it down step by step:\n\nDanid buys 5 bananas on Monday and eats one as soon as he gets back, so he \nhas 5 - 1 = 4 bananas left.\nHe eats one banana per day, so by the time he goes shopping again two days \nlater (Wednesday), he has eaten 2 more bananas (one on Tuesday and one on \nWednesday).\nThis means he has 4 - 2 = 2 bananas left.\nOn the second shopping trip, he doesn't buy bananas, but he does buy \noranges. He eats one banana as soon as he gets back, so he has 2 - 1 = 1 \nbanana left.\nThe day before the second shopping trip (Tuesday), he had bought 2 bananas, \nbut one was smaller than he liked. He didn't eat the smaller one, so he \nstill has it, plus the larger one, which he ate as soon as he got back. So, he \nstill has 1 banana from the first purchase, plus the smaller banana, for a \ntotal of: 1 + 1 = 2 bananas\nHe eats one banana per day, so by the time he buys more bananas the day after \nthe second shopping trip (Friday), he has eaten 2 bananas (one on Thursday and \none on Friday).\nHe buys 2 more bananas on Friday and eats one as soon as he gets back, so he \nhas 2 + (2 - 1) = 3 bananas.\nSo, Danid has 3 bananas.\n```\nThe correct answer is David has 4 bananas left. He bought 9 in total over 5 days and ate one per day = 5. So he has 9–5=4 left.\n\n\n## Summary\n\nNemotron 70B scores 6/8 on my questions and is undoubtedly a great model with strong reasoning skills. In my humble opinion, it falls a little short against GPT 4o and Sonnet 3.5 but is definitely up there with the best of any of the open-source LLMs out there.\n\nIn case you’re wondering, this is how GPT4o answered the banana problem.\n\n\n```python\nLet's summarize David's banana purchases and consumption day by day:\n\nBreakdown of Events:\nMonday:\n\nBought: 5 bananas\nAte: 1 banana\nRemaining: 5 - 1 = 4 bananas\nTuesday:\n\nBought: 2 bananas (one was smaller, but he still bought 2)\nAte: 1 banana\nRemaining: 4 + 2 - 1 = 5 bananas\nWednesday:\n\nBought: No bananas (distracted by oranges)\nAte: 1 banana\nRemaining: 5 - 1 = 4 bananas\nThursday:\n\nNo shopping\nAte: 1 banana\nRemaining: 4 - 1 = 3 bananas\nFriday:\n\nBought: 2 bananas\nAte: 1 banana\nRemaining: 3 + 2 - 1 = 4 bananas\nFinal Answer:\nDavid has 4 bananas remaining.\n```\nCorrect!\n\nIt’s a pity that a lot of these models are still not being released as multi-modal as it would have been fun to see what Nemotron made of my image-based questions.\n\n\n> *Ok, that’s all for me for now. Hopefully, you found this article useful. If you did, please check out my profile page at [this link](https://medium.com/@thomas_reid). From there, you can see my other published stories and subscribe to get notified when I post new content.*\n\nIf you liked this content, I think you’ll find these related articles interesting too.\n\n"},{"lang":"en","group":"blog","slug":"blog/post-3","frontmatter":{"title":"Cursor AI, v0, and Bolt.new: An Honest Comparison of Today’s AI Coding Tools","meta_title":"","description":"this is meta description","date":"2022-04-04T05:00:00.000Z","image":"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*oK76v-3MMToMWA2Snu4HjA.jpeg","categories":["Technology","Data"],"author":"Sam Wilson","tags":["technology","tailwind"],"draft":false,"slug":"blog/post-3"},"content":"\nAI-powered coding tools are evolving rapidly, each new release promising to transform our development processes. These tools aim to boost productivity through intelligent code suggestions, automation of repetitive tasks, and even generation of entire code snippets from natural language prompts. Last week, [StackBlitz launched](https://x.com/stackblitz/status/1841873251313844631) [Bolt.new](https://bolt.new/), adding to the growing ecosystem of AI coding assistants and instantly capturing the attention of developers worldwide. Within days, the internet was flooded with articles and videos proclaiming Bolt.new as the “Cursor & v0 killer.”\n\nI’ve been using [Cursor AI](https://www.cursor.com/) editor and [v0](https://v0.dev/) by Vercel for my work on [Vyce](https://vyce.app), and I immediately incorporated Bolt.new into my workflow upon its release. From this first-hand experience, I can tell you that the reality of these tools is more nuanced than the sensational headlines suggest.\n\nThis article provides a no-nonsense comparison based on my hands-on experience with these tools. I’ll break down the strengths and limitations of each tool, showing you when and why you’d choose one over the others.\n\nThe aim is to provide a clear, balanced overview of how Cursor, v0, and Bolt.new can be effectively integrated into modern development workflows.\n\n\n# Cursor: The AI-Enhanced Code Editor\n\n\n\n\n\n\n\nCursor AI is a fork of Visual Studio Code that integrates advanced AI capabilities. In my experience, it offers the best AI-assisted coding experience available today, surpassing competitors like GitHub Copilot in speed, accuracy, and user experience.\n\nKey features that set Cursor apart include:\n\n* **Intelligent and fast code completion**: Cursor’s autocomplete function goes beyond traditional tools. It suggests multi-line edits and predicts your next changes based on recent modifications, significantly speeding up coding. Often, Cursor suggested changes faster than I could manually navigate to the next point in the code.\n* **Comprehensive code suggestions:** Unlike traditional autocomplete tools, Cursor offers intelligent suggestions at the beginning, middle, and end of lines.\n* **Multi-file editing:** Cursor can create and modify multiple files simultaneously, streamlining complex refactoring tasks.\n* **Integrated documentation:** It indexes documentation for popular libraries, allowing you to include this context in your prompts.\n* **Context-Aware Chat**: Using @Codebase or ⌘ Enter, you can ask questions about your entire codebase. Cursor searches your project to provide relevant answers, which is invaluable for understanding large codebases. As it also has documentation for popular libraries indexed, make the answer much more accurate than vanilla LLM’s, which might have been trained with data that include older versions of the library documentation.\n* **Diff view for changes:** Cursor shows a diff view before applying AI-suggested changes, allowing you to accept modifications block by block or all at once.\n* **Terminal Commands in Plain English**: In the terminal, you can use ⌘ K to write commands in plain English, which Cursor then converts to the appropriate terminal command.\n\nThese features create a seamless, AI-enhanced coding environment that significantly boosts your productivity without sacrificing your control over the codebase.\n\n\n# v0 by Vercel: AI-Powered Prototyping\n\n\n\n\n\n\n\nv0 is Vercel’s AI-powered tool designed for rapid UI prototyping. In my experience working on [Vyce](https://vyce.app), v0 has significantly accelerated the development process, allowing for quick iteration and experimentation with UI components. Here’s what stands out:\n\n* **Enhanced Prototyping Workflow**: v0 enables you to quickly generate and visualize the initial look and feel of a component. It allows side-by-side code generation and preview, much like tools like Claude artifacts or ChatGPT Canvas, but takes it a step further by integrating NPM packages. This integration facilitates the use of UI frameworks and libraries, such as Material-UI (MUI), to build prototypes faster and with less code.\n* **Component-Focused Approach**: While v0 doesn’t handle data fetching or backend logic, it excels at creating individual, reusable components. This focus aligns well with modern React development practices and is ideal for UI ideation and initial component structuring.\n* **Integration with UI Frameworks**: The ability to incorporate NPM packages means you can leverage popular UI frameworks and libraries within your prototypes. In my work on [Vyce](https://vyce.app), this feature has been invaluable for rapidly prototyping new components using MUI.\n* **Code Portability**: v0 generates copy-paste friendly code, allowing you to easily integrate it into existing projects. Once satisfied with a prototype in v0, you can move the code to a full-fledged editor for integration and polishing. This seamless transition from v0 to our main codebase streamlines the development workflow.\n\nWhen building [Vyce](https://vyce.app), we used v0 to prototype new components. The workflow typically involves using v0 to quickly generate and visualize the initial look and feel of a component. Once satisfied with the prototype, I transfer the code to Cursor for integration and polishing.\n\nThis approach significantly accelerates the prototyping phase of development, allowing for rapid iteration and experimentation with UI components.\n\n\n# Bolt.new: Full-Stack Prototyping in the Browser\n\n\n\n\n\n\n\nBolt.new takes v0’s concept to the next level, offering a full-stack development environment in your browser. Instead of being limited to a single file with preview, Bolt.new allows you to create and modify entire projects, complete with dependencies and configurations. You can deploy these projects directly on the StackBlitz platform, streamlining the process from development to production.\n\n* **Complete Development Environment**: Unlike v0, Bolt.new provides a full-stack setup, including npm package installation, Node.js servers, and the ability to interact with third-party APIs. This allowed me to prototype entire applications, not just UI components.\n* **WebContainer Technology**: Built on StackBlitz’s WebContainers, Bolt.new runs a complete Node.js environment in the browser. This eliminates the need for local setup, which can be a significant time-saver.\n* **AI with Environment Control**: The AI in Bolt.new has control over the entire development environment, including the filesystem and terminal. I’ve found this particularly useful for troubleshooting and environment setup tasks. One of the most impressive features of Bolt.new is its one-click error fixing. When you encounter an error, you can let the AI attempt to fix it automatically, eliminating the need for manual copy-pasting of solutions.\n* **Deployment Capabilities**: Bolt.new allows for deployment directly from the chat interface, streamlining the process from development to production. It’s also possible to share work in progress by URL.\n\nAt first glance, Bolt.new seems to offer significantly more than v0, leading some to label it a “v0 killer.” Indeed, its capabilities are impressive, and it wouldn’t be surprising to see Vercel, also being a hosting provider, develop similar functionality in the future.\n\n\n## Limitations and Considerations\n\nDespite its impressive features, Bolt.new is not without limitations:\n\n1. **Code Regeneration:** With each iteration, Bolt.new regenerates the entire file from scratch, even for minor changes. This can lead to unintended modifications and occasionally results in omitted code, similar to issues seen with ChatGPT and Claude.\n2. **Lack of Versioning:** I discovered this limitation the hard way when I lost 1–2 hours of fine-tuning work during a debugging session. The AI-generated code overwrote my carefully crafted changes without any way to recover them.\n3. **No Diff View:** Unlike Cursor, Bolt.new doesn’t offer a diff view before applying changes, reducing control over code modifications.\n\nWhile powerful, these limitations reveal why Bolt.new complements, rather than replaces, tools like Cursor. They serve different purposes in the development workflow.\n\nIn my experience, Bolt.new excels for rapid prototyping of full-stack features, especially when testing ideas that involve both frontend and backend components without setting up a local environment. Its combination of a complete development environment, AI assistance, and collaborative tools makes it a powerful asset for developers looking to streamline their workflow.\n\n\n# Pricing\n\nBolt.new, v0, and Cursor AI all offer free tiers to help users get started, but they differ in their pricing focus as they scale up. Bolt.new and Cursor AI both provide affordable individual plans and higher-cost options for teams or businesses, while v0 stands out with a credit-based system that allows for flexibility in resource consumption. All three tools aim to cater to both individual developers and teams, but their approaches to usage limits and customization vary.\n\n\n## Cursor\n\n* **Hobby**: Free, limited completions\n* **Pro** ($20/month): Unlimited completions, premium requests\n* **Business** ($40/user/month): Privacy mode, team management tools\n\n\n## v0\n\n* **Free**: Basic access, 200 credits\n* **Premium** ($20/month): Increased limits, custom features, 5000 credits\n\n\n## Bolt.new\n\n* **Free**: Limited access, Sonnet 3.5 with 200K context\n* **Pro** ($9/month): 10M tokens, API access\n* **Teams** ($29/member/month): 10M tokens, team features, support\n\n\n# Conclusion\n\nIn the rapidly evolving landscape of AI-assisted coding tools, Cursor, v0, and Bolt.new each bring unique strengths to the table:\n\n* **Cursor AI** remains the go-to tool for day-to-day coding tasks, offering an unparalleled AI-assisted experience within a familiar code editor environment.\n* **v0** excels in rapid UI prototyping, particularly when working with popular frameworks and libraries.\n* **Bolt.new** shines in full-stack prototyping and quick project setups, though it’s currently limited to that role due to its constraints.\n\nWhile Bolt.new offers impressive capabilities that may make it a strong competitor to v0 in the prototyping space, it’s not accurate to call it a “Cursor killer.” These tools serve different purposes and stages in the development process. Cursor remains the tool of choice for serious, production-level development work.\n\nThe AI-powered dev tool landscape is rapidly evolving, with each tool carving out its niche in the modern development workflow. The key for developers is to understand the strengths and limitations of each tool and leverage them appropriately in their workflows.\n\nFor a deeper dive into AI-powered development tools, including discussions on Cursor, v0, Continue.dev, and Ollama, check out my recent appearance on the [Map for Engineers](https://mapforengineers.com) podcast. You can find the full episode (Ep. 3) on [YouTube](https://www.youtube.com/watch?v=S09iueiOBVo) or in your podcast app (search for “Map for Engineers).\n\nHave you used any of these AI coding assistants? I’d love to hear about your experiences and how they’ve impacted your development workflow. Share your thoughts in the comments below or in [HackerNews thread](https://news.ycombinator.com/item?id=41797642).\n\n"},{"lang":"fr","group":"blog","slug":"blog/post-1","frontmatter":{"title":"Comment créer une application avec des technologies modernes","meta_title":"","description":"Ceci est une méta-description","date":"2022-04-04T05:00:00.000Z","image":"/images/image-placeholder.png","categories":["french","Application","Data"],"author":"John Doe","tags":["nextjs","tailwind","react"],"draft":false,"slug":"blog/post-1"},"content":"\nPersonne ne veut même sortir un maquillage de l'urne des soins empoisonnés. C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\n## Design Créatif\n\nCar en guise de maquillage, l'urne du poison C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\n> Le client lui-même doit pouvoir poursuivre l'adipisicing. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n"},{"lang":"fr","group":"blog","slug":"blog/post-2","frontmatter":{"title":"Comment créer une application avec des technologies modernes","meta_title":"","description":"Ceci est une méta-description","date":"2022-04-04T05:00:00.000Z","image":"/images/image-placeholder.png","categories":["Technology","Data"],"author":"Sam Wilson","tags":["technology","tailwind"],"draft":false,"slug":"blog/post-2"},"content":"\nPersonne ne veut même sortir un maquillage de l'urne des soins empoisonnés. C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\n## Design Créatif\n\nCar en guise de maquillage, l'urne du poison C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\n> Le client lui-même doit pouvoir poursuivre l'adipisicing. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n"},{"lang":"fr","group":"blog","slug":"blog/post-3","frontmatter":{"title":"Comment créer une application avec des technologies modernes","meta_title":"","description":"Ceci est une méta-description","date":"2022-04-04T05:00:00.000Z","image":"/images/image-placeholder.png","categories":["Software"],"author":"John Doe","tags":["software","tailwind"],"draft":false,"slug":"blog/post-3"},"content":"\nPersonne ne veut même sortir un maquillage de l'urne des soins empoisonnés. C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\n## Design Créatif\n\nCar en guise de maquillage, l'urne du poison C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\n> Le client lui-même doit pouvoir poursuivre l'adipisicing. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n"},{"lang":"fr","group":"blog","slug":"blog/post-4","frontmatter":{"title":"Comment créer une application avec des technologies modernes","meta_title":"","description":"Ceci est une méta-description","date":"2022-04-04T05:00:00.000Z","image":"/images/image-placeholder.png","categories":["Architecture"],"author":"John Doe","tags":["silicon","technology"],"draft":false,"slug":"blog/post-4"},"content":"\nPersonne ne veut même sortir un maquillage de l'urne des soins empoisonnés. C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\n## Design Créatif\n\nCar en guise de maquillage, l'urne du poison C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\n> Le client lui-même doit pouvoir poursuivre l'adipisicing. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n"},{"lang":"en","group":"models","slug":"models/chatgpt-4o-latest","frontmatter":{"title":"OpenAI: ChatGPT-4o","meta_title":"OpenAI: ChatGPT-4o","description":"OpenAI: ChatGPT-4o","date":"2024-08-14T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text image 2 text"],"author":"openai","tags":["openai"],"draft":false,"id":"chatgpt-4o-latest","context":128000,"input":0.000005,"output":0.000015,"img":0.007225,"request":0,"slug":"models/chatgpt-4o-latest"},"content":"\nDynamic model continuously updated to the current version of [GPT-4o](/openai/gpt-4o) in ChatGPT. Intended for research and evaluation.\n\nNote: This model is currently experimental and not suitable for production use-cases, and may be heavily rate-limited.\n\n"},{"lang":"en","group":"models","slug":"models/gemini-flash-15-8b","frontmatter":{"title":"Google: Gemini 1.5 Flash-8B","meta_title":"Google: Gemini 1.5 Flash-8B","description":"Google: Gemini 1.5 Flash-8B","date":"2024-10-03T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text image 2 text"],"author":"google","tags":["google"],"draft":false,"id":"gemini-flash-1.5-8b","context":1000000,"input":3.75e-8,"output":1.5e-7,"img":0,"request":0,"slug":"models/gemini-flash-15-8b"},"content":"\nGemini 1.5 Flash-8B is optimized for speed and efficiency, offering enhanced performance in small prompt tasks like chat, transcription, and translation. With reduced latency, it is highly effective for real-time and large-scale operations. This model focuses on cost-effective solutions while maintaining high-quality results.\n\n[Click here to learn more about this model](https://developers.googleblog.com/en/gemini-15-flash-8b-is-now-generally-available-for-use/).\n\nUsage of Gemini is subject to Google's [Gemini Terms of Use](https://ai.google.dev/terms).\n\n"},{"lang":"en","group":"models","slug":"models/gemini-pro-15","frontmatter":{"title":"Google: Gemini Pro 1.5","meta_title":"Google: Gemini Pro 1.5","description":"Google: Gemini Pro 1.5","date":"2024-04-09T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text image 2 text"],"author":"google","tags":["google"],"draft":false,"id":"gemini-pro-1.5","context":2000000,"input":0.00000125,"output":0.000005,"img":0.00263,"request":0,"slug":"models/gemini-pro-15"},"content":"\nGoogle's latest multimodal model, supporting image and video in text or chat prompts.\n\nOptimized for language tasks including:\n\n- Code generation\n- Text generation\n- Text editing\n- Problem solving\n- Recommendations\n- Information extraction\n- Data extraction or generation\n- AI agents\n\nUsage of Gemini is subject to Google's [Gemini Terms of Use](https://ai.google.dev/terms).\n\n#multimodal\n\n"},{"lang":"en","group":"models","slug":"models/gpt-35-turbo-instruct","frontmatter":{"title":"OpenAI: GPT-3.5 Turbo Instruct","meta_title":"OpenAI: GPT-3.5 Turbo Instruct","description":"OpenAI: GPT-3.5 Turbo Instruct","date":"2023-09-28T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"openai","tags":["openai"],"draft":false,"id":"gpt-3.5-turbo-instruct","context":4095,"input":0.0000015,"output":0.000002,"img":0,"request":0,"slug":"models/gpt-35-turbo-instruct"},"content":"\nThis model is a variant of GPT-3.5 Turbo tuned for instructional prompts and omitting chat-related optimizations. Training data: up to Sep 2021.\n\n"},{"lang":"en","group":"models","slug":"models/gpt-4o-mini","frontmatter":{"title":"OpenAI: GPT-4o-mini","meta_title":"OpenAI: GPT-4o-mini","description":"OpenAI: GPT-4o-mini","date":"2024-07-18T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text image 2 text"],"author":"openai","tags":["Programming","Technology","Programming/Scripting","Technology/Web"],"draft":false,"id":"gpt-4o-mini","context":128000,"input":1.5e-7,"output":6e-7,"img":0.007225,"request":0,"slug":"models/gpt-4o-mini"},"content":"\nGPT-4o mini is OpenAI's newest model after [GPT-4 Omni](/openai/gpt-4o), supporting both text and image inputs with text outputs.\n\nAs their most advanced small model, it is many multiples more affordable than other recent frontier models, and more than 60% cheaper than [GPT-3.5 Turbo](/openai/gpt-3.5-turbo). It maintains SOTA intelligence, while being significantly more cost-effective.\n\nGPT-4o mini achieves an 82% score on MMLU and presently ranks higher than GPT-4 on chat preferences [common leaderboards](https://arena.lmsys.org/).\n\nCheck out the [launch announcement](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/) to learn more.\n\n"},{"lang":"en","group":"models","slug":"models/gpt-4o","frontmatter":{"title":"OpenAI: GPT-4o","meta_title":"OpenAI: GPT-4o","description":"OpenAI: GPT-4o","date":"2024-05-13T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text image 2 text"],"author":"openai","tags":["Programming","Programming/Scripting","Technology/Web","Technology"],"draft":false,"id":"gpt-4o","context":128000,"input":0.0000025,"output":0.00001,"img":0.0036125,"request":0,"slug":"models/gpt-4o"},"content":"\nGPT-4o (\"o\" for \"omni\") is OpenAI's latest AI model, supporting both text and image inputs with text outputs. It maintains the intelligence level of [GPT-4 Turbo](/openai/gpt-4-turbo) while being twice as fast and 50% more cost-effective. GPT-4o also offers improved performance in processing non-English languages and enhanced visual capabilities.\n\nFor benchmarking against other models, it was briefly called [\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209)\n\n"},{"lang":"en","group":"models","slug":"models/llama-31-nemotron-70b-instruct","frontmatter":{"title":"Nvidia: Llama 3.1 Nemotron 70B Instruct","meta_title":"Nvidia: Llama 3.1 Nemotron 70B Instruct","description":"Nvidia: Llama 3.1 Nemotron 70B Instruct","date":"2024-10-15T00:00:00.000Z","image":"https://img.rifx.online/logo/nvidia.svg","categories":["text 2 text"],"author":"nvidia","tags":["nvidia"],"draft":false,"id":"llama-3.1-nemotron-70b-instruct","context":131072,"input":3.5e-7,"output":4e-7,"img":0,"request":0,"slug":"models/llama-31-nemotron-70b-instruct"},"content":"\nNVIDIA's Llama 3.1 Nemotron 70B is a language model designed for generating precise and useful responses. Leveraging [Llama 3.1 70B](/models/meta-llama/llama-3.1-70b-instruct) architecture and Reinforcement Learning from Human Feedback (RLHF), it excels in automatic alignment benchmarks. This model is tailored for applications requiring high accuracy in helpfulness and response generation, suitable for diverse user queries across multiple domains.\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).\n\n"},{"lang":"en","group":"models","slug":"models/llama-32-11b-vision-instruct","frontmatter":{"title":"Meta: Llama 3.2 11B Vision Instruct","meta_title":"Meta: Llama 3.2 11B Vision Instruct","description":"Meta: Llama 3.2 11B Vision Instruct","date":"2024-09-25T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text image 2 text"],"author":"meta-llama","tags":["meta-llama"],"draft":false,"id":"llama-3.2-11b-vision-instruct","context":131072,"input":5.5e-8,"output":5.5e-8,"img":0.000079475,"request":0,"slug":"models/llama-32-11b-vision-instruct"},"content":"\nLlama 3.2 11B Vision is a multimodal model with 11 billion parameters, designed to handle tasks combining visual and textual data. It excels in tasks such as image captioning and visual question answering, bridging the gap between language generation and visual reasoning. Pre-trained on a massive dataset of image-text pairs, it performs well in complex, high-accuracy image analysis.\n\nIts ability to integrate visual understanding with language processing makes it an ideal solution for industries requiring comprehensive visual-linguistic AI applications, such as content creation, AI-driven customer service, and research.\n\nClick here for the [original model card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md).\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).\n\n"},{"lang":"en","group":"models","slug":"models/llama-32-1b-instruct","frontmatter":{"title":"Meta: Llama 3.2 1B Instruct","meta_title":"Meta: Llama 3.2 1B Instruct","description":"Meta: Llama 3.2 1B Instruct","date":"2024-09-25T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text 2 text"],"author":"meta-llama","tags":["meta-llama"],"draft":false,"id":"llama-3.2-1b-instruct","context":131072,"input":1e-8,"output":2e-8,"img":0,"request":0,"slug":"models/llama-32-1b-instruct"},"content":"\nLlama 3.2 1B is a 1-billion-parameter language model focused on efficiently performing natural language tasks, such as summarization, dialogue, and multilingual text analysis. Its smaller size allows it to operate efficiently in low-resource environments while maintaining strong task performance.\n\nSupporting eight core languages and fine-tunable for more, Llama 1.3B is ideal for businesses or developers seeking lightweight yet powerful AI solutions that can operate in diverse multilingual settings without the high computational demand of larger models.\n\nClick here for the [original model card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md).\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).\n\n"},{"lang":"en","group":"models","slug":"models/llama-32-90b-vision-instruct","frontmatter":{"title":"Meta: Llama 3.2 90B Vision Instruct","meta_title":"Meta: Llama 3.2 90B Vision Instruct","description":"Meta: Llama 3.2 90B Vision Instruct","date":"2024-09-25T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text image 2 text"],"author":"meta-llama","tags":["meta-llama"],"draft":false,"id":"llama-3.2-90b-vision-instruct","context":131072,"input":3.5e-7,"output":4e-7,"img":0.00050575,"request":0,"slug":"models/llama-32-90b-vision-instruct"},"content":"\nThe Llama 90B Vision model is a top-tier, 90-billion-parameter multimodal model designed for the most challenging visual reasoning and language tasks. It offers unparalleled accuracy in image captioning, visual question answering, and advanced image-text comprehension. Pre-trained on vast multimodal datasets and fine-tuned with human feedback, the Llama 90B Vision is engineered to handle the most demanding image-based AI tasks.\n\nThis model is perfect for industries requiring cutting-edge multimodal AI capabilities, particularly those dealing with complex, real-time visual and textual analysis.\n\nClick here for the [original model card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md).\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).\n\n"},{"lang":"en","group":"models","slug":"models/ministral-8b","frontmatter":{"title":"Ministral 8B","meta_title":"Ministral 8B","description":"Ministral 8B","date":"2024-10-17T00:00:00.000Z","image":"https://img.rifx.online/logo/mistral.png","categories":["text 2 text"],"author":"mistralai","tags":["mistralai"],"draft":false,"id":"ministral-8b","context":128000,"input":1e-7,"output":1e-7,"img":0,"request":0,"slug":"models/ministral-8b"},"content":"\nMinistral 8B is an 8B parameter model featuring a unique interleaved sliding-window attention pattern for faster, memory-efficient inference. Designed for edge use cases, it supports up to 128k context length and excels in knowledge and reasoning tasks. It outperforms peers in the sub-10B category, making it perfect for low-latency, privacy-first applications.\n\n"},{"lang":"en","group":"models","slug":"models/o1-mini","frontmatter":{"title":"OpenAI: o1-mini","meta_title":"OpenAI: o1-mini","description":"OpenAI: o1-mini","date":"2024-09-12T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"openai","tags":["openai"],"draft":false,"id":"o1-mini","context":128000,"input":0.000003,"output":0.000012,"img":0,"request":0,"slug":"models/o1-mini"},"content":"\nThe latest and strongest model family from OpenAI, o1 is designed to spend more time thinking before responding.\n\nThe o1 models are optimized for math, science, programming, and other STEM-related tasks. They consistently exhibit PhD-level accuracy on benchmarks in physics, chemistry, and biology. Learn more in the [launch announcement](https://openai.com/o1).\n\nNote: This model is currently experimental and not suitable for production use-cases, and may be heavily rate-limited.\n\n"},{"lang":"en","group":"models","slug":"models/o1-preview","frontmatter":{"title":"OpenAI: o1-preview","meta_title":"OpenAI: o1-preview","description":"OpenAI: o1-preview","date":"2024-09-12T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"openai","tags":["openai"],"draft":false,"id":"o1-preview","context":128000,"input":0.000015,"output":0.00006,"img":0,"request":0,"slug":"models/o1-preview"},"content":"\nThe latest and strongest model family from OpenAI, o1 is designed to spend more time thinking before responding.\n\nThe o1 models are optimized for math, science, programming, and other STEM-related tasks. They consistently exhibit PhD-level accuracy on benchmarks in physics, chemistry, and biology. Learn more in the [launch announcement](https://openai.com/o1).\n\nNote: This model is currently experimental and not suitable for production use-cases, and may be heavily rate-limited.\n\n"},{"lang":"en","group":"models","slug":"models/qwen-2-7b-instruct","frontmatter":{"title":"Qwen 2 7B Instruct","meta_title":"Qwen 2 7B Instruct","description":"Qwen 2 7B Instruct","date":"2024-07-16T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"qwen","tags":["qwen"],"draft":false,"id":"qwen-2-7b-instruct","context":32768,"input":5.4e-8,"output":5.4e-8,"img":0,"request":0,"slug":"models/qwen-2-7b-instruct"},"content":"\nQwen2 7B is a transformer-based model that excels in language understanding, multilingual capabilities, coding, mathematics, and reasoning.\n\nIt features SwiGLU activation, attention QKV bias, and group query attention. It is pretrained on extensive data with supervised finetuning and direct preference optimization.\n\nFor more details, see this [blog post](https://qwenlm.github.io/blog/qwen2/) and [GitHub repo](https://github.com/QwenLM/Qwen2).\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).\n\n"},{"lang":"en","group":"models","slug":"models/qwen-2-vl-72b-instruct","frontmatter":{"title":"Qwen2-VL 72B Instruct","meta_title":"Qwen2-VL 72B Instruct","description":"Qwen2-VL 72B Instruct","date":"2024-09-18T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text image 2 text"],"author":"qwen","tags":["qwen"],"draft":false,"id":"qwen-2-vl-72b-instruct","context":32768,"input":4e-7,"output":4e-7,"img":0.000578,"request":0,"slug":"models/qwen-2-vl-72b-instruct"},"content":"\nQwen2 VL 72B is a multimodal LLM from the Qwen Team with the following key enhancements:\n\n- SoTA understanding of images of various resolution & ratio: Qwen2-VL achieves state-of-the-art performance on visual understanding benchmarks, including MathVista, DocVQA, RealWorldQA, MTVQA, etc.\n\n- Understanding videos of 20min+: Qwen2-VL can understand videos over 20 minutes for high-quality video-based question answering, dialog, content creation, etc.\n\n- Agent that can operate your mobiles, robots, etc.: with the abilities of complex reasoning and decision making, Qwen2-VL can be integrated with devices like mobile phones, robots, etc., for automatic operation based on visual environment and text instructions.\n\n- Multilingual Support: to serve global users, besides English and Chinese, Qwen2-VL now supports the understanding of texts in different languages inside images, including most European languages, Japanese, Korean, Arabic, Vietnamese, etc.\n\nFor more details, see this [blog post](https://qwenlm.github.io/blog/qwen2-vl/) and [GitHub repo](https://github.com/QwenLM/Qwen2-VL).\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).\n\n"},{"lang":"en","group":"models","slug":"models/qwen-25-72b-instruct","frontmatter":{"title":"Qwen2.5 72B Instruct","meta_title":"Qwen2.5 72B Instruct","description":"Qwen2.5 72B Instruct","date":"2024-09-19T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"qwen","tags":["qwen"],"draft":false,"id":"qwen-2.5-72b-instruct","context":131072,"input":3.5e-7,"output":4e-7,"img":0,"request":0,"slug":"models/qwen-25-72b-instruct"},"content":"\nQwen2.5 72B is the latest series of Qwen large language models. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\n\n- Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n\n- Long-context Support up to 128K tokens and can generate up to 8K tokens.\n\n- Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).\n\n"},{"lang":"en","group":"models","slug":"models/qwen-25-7b-instruct","frontmatter":{"title":"Qwen2.5 7B Instruct","meta_title":"Qwen2.5 7B Instruct","description":"Qwen2.5 7B Instruct","date":"2024-10-16T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"qwen","tags":["qwen"],"draft":false,"id":"qwen-2.5-7b-instruct","context":131072,"input":2.7e-7,"output":2.7e-7,"img":0,"request":0,"slug":"models/qwen-25-7b-instruct"},"content":"\nQwen2.5 7B is the latest series of Qwen large language models. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\n\n- Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n\n- Long-context Support up to 128K tokens and can generate up to 8K tokens.\n\n- Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).\n\n"}]
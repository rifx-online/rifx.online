[{"lang":"en","group":"blog","slug":"blog/a-month-with-cursor-and-claude-dev-my-thoughts-5c41ae0d4467","frontmatter":{"title":"A Month with Cursor and Claude-Dev: My Thoughts","meta_title":"A Month with Cursor and Claude-Dev: My Thoughts","description":"I’ve been using two new tools recently- Cursor and Claude-Dev -both of which have been getting a fair bit of attention in the developer…","date":"2024-11-04T12:32:52.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*i28vK12LJ6XTpSwrKiamwA.png","categories":["Programming","Technology","Generative AI"],"author":"Rifx.Online","tags":["Cursor","Claude-Dev","Cline","autocomplete","debugging"],"draft":false,"slug":"blog/a-month-with-cursor-and-claude-dev-my-thoughts-5c41ae0d4467"},"content":"\n\n\nI’ve been using two new tools recently\\- **Cursor** and **Claude\\-Dev** \\-both of which have been getting a fair bit of attention in the developer community. They’re each built to make coding faster and more intuitive through AI\\-powered assistance, but they take different approaches and have their own strengths and weaknesses. After using both for about a month, I thought it was time to sit down and reflect on where they shine and where they still need some work.\n\nLet’s start with Cursor.\n\n## Cursor: Familiar but Faster\n\nCursor is a fork of VSCode, which, if you’re already a VSCode user like I am, makes it very easy to slip into. I didn’t need to rebuild my environment from scratch or deal with setting up keybindings. Everything that worked in VSCode worked in Cursor right out of the box\\-my extensions, settings, and keymappings carried over without a hitch. The transition was almost invisible, except for one key difference: the AI autocomplete is much faster. In fact, **in my experience**, it’s around 10 times faster than GitHub Copilot.\n\nNow, “10 times faster” isn’t a number I pulled from benchmarks\\-it’s just what it feels like after using it for a while. When you’re typing code, and Cursor is predicting your next move, it doesn’t feel like the AI is lagging behind or playing catch\\-up. Instead, it’s right there with you, which helps keep you in flow. I was surprised at how much more productive I felt when I wasn’t waiting for Copilot to catch up or pressing tab three times just to get the suggestion I wanted.\n\nCursor also has a nice feature where it embeds and indexes your entire project, making it easier to understand the relationships between files. When you update a file, the index gets updated too, which means the AI has a better grasp of how the pieces of your codebase fit together. This is useful if you’re working across a large codebase with multiple files that depend on each other.\n\n## The Drawbacks\n\nThat said, some of the best features in Cursor are gated behind a subscription. I’m generally not opposed to paying for tools that add real value, but in this case, I was a little disappointed that the most interesting AI features\\-like the multi\\-file editing\\-were part of the premium version. For a tool that’s still fairly new, I wonder if gating these features too early might limit its adoption, especially given how many developers are already paying for GitHub Copilot.\n\nAnother issue I’ve run into with Cursor is that while it’s great at fast, small tasks, it lacks some of the flexibility I need when working with more complex problems. It’s excellent for quick code suggestions and refactoring, but when I needed something that could handle more involved tasks, like reading logs or executing build commands, I found myself looking for something else.\n\n## Claude\\-Dev: The Open\\-Source Underdog\n\nThat’s where **Claude\\-Dev (now called Cline)** comes in. Claude\\-Dev is an open\\-source extension for VSCode, and while it doesn’t have the same level of polish as Cursor, it’s rapidly evolving\\-and in some ways, it’s more powerful. The most striking thing about Claude\\-Dev is that it feels like it’s trying to do more than just suggest code snippets. It’s a tool that can **interact** with your environment in a much deeper way.\n\nFor example, Claude\\-Dev can read your terminal logs, understand linting errors, and even run arbitrary CLI commands. This means that if you ask it why your project isn’t building, it won’t just offer suggestions\\-it will actually go and look at the relevant files, figure out what kind of project you’re working with (Node, React, Python, etc.), and try to build it for you. If there’s an error, it reads the logs, tries to diagnose the problem, and can even apply fixes if needed.\n\nIt’s not perfect, though. In my experience, Claude\\-Dev isn’t as fast as Cursor, especially when it’s making edits. One reason for this is that it rewrites entire files instead of just updating the parts that need to change. This slows things down, and if you’re paying for API tokens (you need to supply an API key from the LLM you want to use), it burns through those faster than it should. I’ve been thinking about contributing to the project to fix this by having it update just the necessary lines via shell commands like `sed`.\n\nOne feature I’ve found particularly interesting is how Claude\\-Dev can use Puppeteer to visually test and update your frontend. You can give it a screenshot of a website, and it will compare that to your app, iterating until it gets your frontend to match the look you’re going for. It’s not the fastest process, but it’s surprisingly good at handling CSS\\-something that, for me at least, is usually a bit of a time sink.\n\n## Where It Falls Short\n\nClaude\\-Dev is definitely a tool for people who are comfortable experimenting with something that’s still a bit rough around the edges. Unlike Cursor, which feels more like a polished product that’s ready for prime time, Claude\\-Dev is more like a powerful tool in active development. It doesn’t always get things right the first time, and it’s slower than I’d like, but it’s constantly improving. The fact that it’s open source and primarily developed by one person makes its pace of innovation even more impressive.\n\n## So Which One Should You Use?\n\nIf you’re looking for a polished, fast experience with a focus on speed and quick suggestions, **Cursor** might be the better choice. It feels snappy, it integrates with your existing VSCode setup, and it keeps you in flow\\-until you hit a paywall. But if you’re okay with that and don’t need the extra bells and whistles, Cursor is a great tool.\n\nOn the other hand, if you want something that can do more than just autocomplete code\\-something that can actually help with debugging, building, and iterating on your project\\- **Claude\\-Dev** is a better fit. It’s more versatile, but also a bit slower and rougher around the edges. If you’re comfortable experimenting and can put up with some quirks, it offers a level of functionality that Cursor just doesn’t have right now.\n\nFor me, **Claude\\-Dev** wins out, mostly because of its deeper integration with my workflow. The ability to read logs, run commands, and iterate until a problem is solved is invaluable, especially when I’m working with unfamiliar codebases. That said, I still find myself using **Cursor** when I need to move fast and don’t want to wait around for the AI to process a command.\n\n## Final Thoughts\n\nBoth Cursor and Claude\\-Dev offer unique benefits, and I think we’re only scratching the surface of what AI\\-driven coding tools can do. There’s a lot of potential here, especially as these tools continue to evolve. I’m excited to see where they go, and I’ll keep experimenting with both to see how they fit into my development workflow.\n\nIn the meantime, I’d recommend trying out both and for yourself. Each tool has its strengths, and you’ll probably find that one fits your style better than the other, depending on what you’re working on.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/a-new-risings-red-star-qwen2-5-is-here-0dffe0fb09ad","frontmatter":{"title":"A new risings Red star: Qwen2.5 is here","meta_title":"A new risings Red star: Qwen2.5 is here","description":"Let’s test together the new born Alibaba Cloud’s generative AI Qwen2.5 with python and llama-cpp","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*zU-XtqK2oMLkvscgxavjdw.png","categories":["Programming","Technology","Education"],"author":"Rifx.Online","tags":["Qwen2.5","multimodal","instruction-following","text-generation","multilingual"],"draft":false,"slug":"blog/a-new-risings-red-star-qwen2-5-is-here-0dffe0fb09ad"},"content":"\n\n\n\n\n### Let’s test together the new born Alibaba Cloud’s generative AI Qwen2.5 with python and llama-cpp\n\n\n\nIn silence, with not so many claims and anticipated announcements, Alibaba Cloud release on September the 19th their flagship model family Qwen2.5.\n\nAlibaba Cloud’s revolutionary journey with Qwen is showing once again strong Leadership through Innovation.\n\nHow? What’s so cool in them? And should we expect?\n\nIn this article we are going to explore the new models and check the performances. As a follow up, in the next article, we are going to use `llama-cpp-python` and the quantized version of qwen2.5–1.5b-instruct, putting the model under 13 NLP tasks test.\n\nIn fact I believe that we are the best Benchmark tool around and we are fully able to evaluate when a model is good for us!\n\nFor now, here what we are going to cover:\n\n\n```python\n- Qwen2.5 family innovation\n- Declared scope, use cases and models\n- Qwen2.5: a party of Foundation models\n- Expanding Reach through Open-Source Contributions\n- Bridging Industries through cutting-edge AI solutions\n- 13 Tasks to prove it worth \n- Future outlook: continued Open-Sourcing\n```\nLet’s dive in!\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*OeQ5qeOzCdl8LPJOZZgTIw.png)\n\n\n## Qwen2.5 family innovation\n\nQwen is the large language model and large multimodal model series of the Qwen Team, Alibaba Group. Just yesterday the large language models have been upgraded to Qwen2.5.\n\nBoth language models and multimodal models are pretrained on large-scale multilingual and multimodal data and post-trained on quality data for aligning to human preferences. Qwen is capable of natural language understanding, text generation, vision understanding, audio understanding, tool use, role play, playing as AI agent, etc.\n\nWith the recent release of Qwen2.5 and additional open-source model releases Alibaba Cloud continues its leadership position to meet rising AI demands from enterprise users. Since June last year, the Qwen family has attracted over 90,000 deployments via Model Studio in various industries including consumer electronics, automobiles, gaming, and more.\n\nQwen also expanded its reach with new models such as Qwen1.5–110B and CodeQwen1.5–7B on platforms like Hugging Face, showcasing Alibaba’s commitment to open-source AI development.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*A4pEOgsLK2PAFtiaGQx1Qw.png)\n\n\n## Declared scope, use cases and models\n\nIn the past three months since Qwen2’s release, numerous developers have built new models on the Qwen2 language models, providing valuable feedback to the entire community, but also to Alibaba Cloud.\n\n\n> During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5.\n\nTheir claims come with facts about the new family of models:\n\n* Dense, easy-to-use, decoder-only language models, available in 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B sizes, and base and instruct variants.\n* Pretrained on our latest large-scale dataset, encompassing up to 18T tokens.\n* Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON.\n* More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n* Context length support up to 128K tokens and can generate up to 8K tokens.\n* Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\n\n## Qwen2.5: a party of Foundation models\n\nAs announced on the [official blog press release](https://qwenlm.github.io/blog/qwen2.5/) on September 19, 2024:\n\n\n> Today, we are excited to introduce the latest addition to the Qwen family: **Qwen2.5**. We are announcing what might be the largest opensource release in history! Let’s get the party started!\n\n\n> Our latest release features the LLMs **Qwen2.5**, along with specialized models for coding, **Qwen2.5-Coder**, and mathematics, **Qwen2.5-Math**.\n\nTo showcase Qwen2.5’s capabilities, the Alibaba Cloud team benchmarked their largest open-source model, **Qwen2.5–72B** — a 72B-parameter dense decoder-only language model — against leading open-source models like Llama-3.1–70B and Mistral-Large-V2.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*-MMFgkkWHa307jNo.jpg)\n\nAll open-weight models are dense, decoder-only language models, available in various sizes, including:\n\n* Qwen2.5: 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B\n* Qwen2.5-Coder: 1.5B, 7B, and 32B on the way\n* Qwen2.5-Math: 1.5B, 7B, and 72B.\n\nAll these open-source models, except for the 3B and 72B variants, are licensed under Apache 2.0. You can find the license files in the respective Hugging Face repositories.\n\n\n> In addition to these models, we offer APIs for our flagship language models: **Qwen-Plus** and **Qwen-Turbo** through Model Studio, and we encourage you to explore them!\n\nBut this is not all!\n\n\n> …we have also open-sourced the **Qwen2-VL-72B**, which features performance enhancements compared to last month’s release.\n\nIn terms of **Qwen2.5**, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to **18 trillion** tokens. Compared to Qwen2, Qwen2.5 has acquired significantly more knowledge (MMLU: 85+) and has greatly improved capabilities in coding (HumanEval 85+) and mathematics (MATH 80+). Additionally, the new models achieve significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*7c7CIbl-WVjazUeE.jpeg)\n\nQwen2.5 models are generally more resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n\nLike Qwen2, the Qwen2.5 language models support up to **128K** tokens and can generate up to **8K** tokens. They also maintain multilingual support for over **29** languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\n\n### Qwen-Coder is the new kid of the family\n\nThe specialized expert language models, namely **Qwen2.5-Coder** for coding and **Qwen2.5-Math** for mathematics, have undergone substantial enhancements compared to their predecessors, CodeQwen1.5 and Qwen2-Math. Specifically, Qwen2.5-Coder has been trained on **5.5 trillion** tokens of code-related data, enabling even smaller coding-specific models to deliver competitive performance against larger language models on coding evaluation benchmarks. Meanwhile, Qwen2.5-Math supports both **Chinese** and **English** and incorporates various reasoning methods, including Chain-of-Thought (CoT), Program-of-Thought (PoT), and Tool-Integrated Reasoning (TIR).\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Nvk4wrcB0SB4Tt-xbCzO6g.png)\n\n\n## Expanding Reach through Open-Source Contributions\n\nAs part of its continuous commitment to the broader community, Alibaba Cloud has made additional steps in releasing various sizes and variants of Qwen models. This includes:\n\n1. **Qwen 0.5 billion parameters**, a foundational version suitable for more traditional applications.2. A compact but potent model tailored specifically for gaming development: **Qwen-VL (vision-language)** optimized with high capabilities.\n\nThese advancements demonstrate Alibaba’s commitment to open-source AI, sharing not only the base versions of Qwen but also significant improvements and new models that are targeting directly the enterprise needs while enhancing their ability to innovate rapidly.\n\nThis aligns closely with a strategic vision where continuous contributions benefit both community members and its own clients as they seek innovative applications across multiple sectors.\n\n\n### Bridging Industries through cutting-edge AI solutions\n\nTo showcase the breadth of Qwen’s capabilities in real-world scenarios, Alibaba Cloud has been at the forefront:\n\n1. **Xiaomi**: the Company is integrating Alibaba’s models into their AI assistant, Xiao Ai, and deploying it within Xiaomi smartphones and electric vehicles to create enhanced features like car infotainment image generation via voice commands.\n\n2. **Perfect World Games**: the integration of Qwen in game development has led to innovative applications including improving plot resolution through dialogue dynamics and real-time content management.\n\nThe collaborations between Alibaba Cloud models and various industries have not only enriched the user experience but also facilitated greater opportunities for growth within these sectors, pushing boundaries that would otherwise be unimaginable without AI advancements.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*ku8o3rq6PHDE8xcc.png)\n\n\n## 13 Tasks to prove it worth\n\nThe 1.5 Billion parameters model is probably the best variant considering complexity, prompt understanding and inference speed.\n\nI will show you my internal testing using only `llama-cpp-python` and a simple terminal interface.\n\nTo do so, I created a list of prompt, covering a series of normally used tasks where you can also assign a vote (from 0 to 5) after every generation. It’s a personal human benchmark.\n\n\n### Requirements\n\nCreate a `venv` (python 3.11+ is required): I tested it on my Mini-PC running Windows 11.\n\n\n```python\n## create the virtual environment\npython -m venv venv\n## activate the venv\nvenv\\Scripts\\activate\n## Install the dependencies \npip install llama-cpp-python==0.2.90 tiktoken\n```\nWe need to download the GGUF file from the [official Qwen2.5 Hugging Face repo](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF). I used the qwen2.5–1.5b-instruct-q5\\_k\\_m.gguf version.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Fa-qFsx9RTFGZmM-vxCEPQ.png)\n\nDownload the file in the main project directory. And we are all set.\n\nThe code used here for the analysis is in my GitHub repository:\n\nI will explain the entire code and the results in the next article. Stay updated!\n\n\n## Future outlook: continued Open-Sourcing\n\nIn future plans, Alibaba has also expressed their commitment to ongoing open-source contributions by releasing smaller variants of Qwen for developers across different sectors. In reality in the Hugging Face community many users have started to fine-tune Qwen for dedicated tasks: I wrote an example in my article on NuExtract: the smaller variant of this model family is based on Qwen2–0.5b!\n\nThese developments in AI technology and model advancements are crucial steps towards leveraging the full potential of large language models like **Qwen** within a variety of industries. With robust adoption rates continuing to grow rapidly through Model Studio, it is clear that Alibaba Cloud has been a pioneer industry leader not only by providing advanced tools but also promoting innovation across enterprises.\n\nOn my side, my outlook are to proceed with internal testing on the new models, specifically on the small ones, up to 3B.\n\nIn the next article I will share with you my method, how to run the models and the prompt templates used for each of the thirteen NLP tasks.\n\nHope you enjoyed the article. If this story provided value and you wish to show a little support, you could:\n\n1. Clap a lot of times for this story\n2. Highlight the parts more relevant to be remembered (it will be easier for you to find them later, and for me to write better articles)\n3. **Join my [totally free weekly Substack newsletter here](https://thepoorgpuguy.substack.com/about)**\n4. Sign up for a Medium membership ($5/month to read unlimited Medium stories)\n5. Follow me on Medium\n6. Read my latest articles <https://medium.com/@fabio.matricardi>\n\nHere are a few more articles to feed your curiosity:\n\nResources references in this article:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Du7V61mEX_yIrfmF.png)\n\nThis story is published on [Generative AI](https://generativeai.pub/). Connect with us on [LinkedIn](https://www.linkedin.com/company/generative-ai-publication) and follow [Zeniteq](https://www.zeniteq.com/) to stay in the loop with the latest AI stories.\n\nSubscribe to our [newsletter](https://www.generativeaipub.com/) and [YouTube](https://www.youtube.com/@generativeaipub) channel to stay updated with the latest news and updates on generative AI. Let’s shape the future of AI together!\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*pvLAT3it1FkdhVU0.png)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/a-practical-guide-for-using-autogen-in-software-applications-8799185d27ee","frontmatter":{"title":"A practical guide for using AutoGen in software applications","meta_title":"A practical guide for using AutoGen in software applications","description":"Update: While this article was written only 4 months ago, AutoGen has since changed quite a bit. I apologize for some things that may be…","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*yrraWH6aGNnbx8p-wfQ1OQ.jpeg","categories":["Programming","Chatbots","Autonomous Systems"],"author":"Rifx.Online","tags":["AutoGen","multi-agent","LLMs","customization","collaboration"],"draft":false,"slug":"blog/a-practical-guide-for-using-autogen-in-software-applications-8799185d27ee"},"content":"\n\n\n\n\n\n*Update: While this article was written only 4 months ago, AutoGen has since changed quite a bit. I apologize for some things that may be outdated in my code examples.*\n\nIf you want to learn about AutoGen, there is [documentation](https://microsoft.github.io/autogen/), [Colab notebooks](https://microsoft.github.io/autogen/docs/Examples), and [a blog](https://microsoft.github.io/autogen/blog). Huge kudos to the AutoGen team for making an AMAZING product, but honestly — after reading all their stuff, I still didn’t know how to use AutoGen outside of a terminal or Jupyter Notebook.\n\nThis article tries to help fill that gap by giving some helpful ways to make AutoGen work in a software application. Here are the topics I’ll go over:\n\n1. Agents aren’t limited to communicating just over the terminal\n2. Registering custom replies\n3. How to include real humans in the conversation in real ways\n4. You can (and should) customize who speaks next\n5. You don’t have to use OpenAI\n6. Functions can be used instead of executing code\n7. Use Agents for organization, not just for conversations\n\nLastly, I’ll go over why I think you should use AutoGen to begin with. Let’s go!\n\n\n## Agents aren’t limited to communicating just over the terminal\n\nYou’ll see everyone demo AutoGen using a terminal or Jupyter Notebook. That’s nice for a demo, but there are other ways these agents can talk to each other.\n\nThere are 2 basic AutoGen classes: [`UserProxyAg`ent](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/user_proxy_agent.py) and [`AssistantAg`ent](https://github.com/microsoft/autogen/blob/main/autogen/agentchat/assistant_agent.py) . They inherit the [`ConversableAg`ent](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py) class, providing just a few different default parameters to the base class.\n\nWhen you see this classic code example:\n\n\n```python\nassistant = autogen.AssistantAgent(\n    name=\"assistant\",\n    llm_config=llm_config\n)\nuser_proxy = autogen.UserProxyAgent(name=\"user_proxy\")\nawait user_proxy.a_initiate_chat(\n    assistant,\n    message=\"\"\"What date is today? Compare the year-to-date gain for META and TESLA.\"\"\",\n)\n```\nwhat happens is that the `UserProxyAgent` will call its own `send` method, which will call `AssistantAgent` ‘s [`rece`ive](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L514) method, passing along the original message. A reply will be generated (more on that below), and `AssistantAgent` will now call its [`s`end](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L351) method, which will then call `UserProxyAgent` ‘s `receive` method, and so forth, until `UserProxyAgent` determines the conversation is terminated (which can be customized via the `is_termination_msg` argument).\n\nMy first “aha” moment was when I realized these agents were classes, and I could create my own custom agent classes that inherit the AutoGen UserProxy/Assistant/Conversable Agent classes, and override any of the default methods. That makes AutoGen very extensible.\n\nI had a use-case where I needed a human who could type in a message (proxied by `UserProxyAgent`) using a chat UI on a website, and I wanted an `AssistantAgent` to respond back to that chat in the UI, and be able to receive more messages from the human user, as though the human was just another agent in this AutoGen conversation.\n\nI could override the `send` and `receive` methods (or `a_send` and `a_receive`), and push/pull over http, websockets, etc. I tried this, and it started to work, but doesn’t scale. Let’s learn a better way.\n\n\n## Registering custom replies\n\nAutoGen has a plugin system that lets you customize how an agent generates a reply. We’re used to seeing examples where AutoGen queries OpenAI for an answer, and uses that as its reply, but you can insert your own methods as well:\n\n\n```python\nclass WeatherAgent(AssistantAgent):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, llm_config=False, **kwargs)\n        self.register_reply(Agent, WeatherAgent.get_weather)\n\n    async def get_weather(\n        self,\n        messages: List[Dict] = [],\n        sender=None,\n        config=None,\n    ) -> Tuple[bool, Union[str, Dict, None]]:\n        last_message = messages[-1][\"content\"]\n        result = await fetch_weather(last_message)\n        return True, result\n\nasync def fetch_weather(city: str) -> str:\n    async with httpx.AsyncClient() as client:\n        result = await client.post(\n            WEATHER_API_URL,\n            json={\"city\": question},\n        )\n        return result.json()\n\nweather_assistant = WeatherAgent(name=\"weather_assistant\")\nuser_proxy = autogen.UserProxyAgent(name=\"user_proxy\")\nawait user_proxy.a_initiate_chat(assistant, message=\"Lehi\")\nprint(weather_assistant.last_message)\n```\nHere, `register_reply` will insert my custom method for getting a reply, and by default, will put this method in `position=0`, meaning it will be the first reply method attempted. That method should return a tuple, where the first item is a boolean indicating if this reply is the one that should be used or whether to try the next registered\\_reply (such as the built-in reply generations using OpenAI — see the full order [here](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L145-L153)).\n\nKnowing about [`register_re`ply](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L155) allows you to customize how replies are retrieved, allow you to start sub multi-agent conversations, etc.\n\n\n## How to include real humans in the conversation in real ways\n\nHere’s one way to do it:\n\n\n```python\n## user makes a POST /query { \"message\": \"What's the weather?\" }\n\n@query_blueprint.route(\"/query\", methods=[\"POST\"])\nasync def post_query():\n  message = request.form.get(\"message\")\n\n  assistant = autogen.AssistantAgent(\n    name=\"assistant\",\n    llm_config=llm_config\n    system_message=\"\"\"You're a helpful assistant.\n    If you need more info, ask the user for anything missing.\"\"\"\n  )\n  user_proxy = autogen.UserProxyAgent(\n    name=\"user_proxy\",\n    human_input_mode=\"NEVER\",\n    code_execution_config=False,\n    is_termination_msg=lambda message: True # Always True\n  )\n  weather_assistant = WeatherAgent(\n    name=\"weather_assistant\",\n    system_message=\"\"\"You're a helpful assistant to get the weather.\n    You fetch weather information, then return it.\"\"\"\n  )\n\n  groupchat = autogen.GroupChat(\n    agents=[assistant, user_proxy, weather_assistant],\n    messages=[]\n  )\n  manager = autogen.GroupChatManager(\n    name=\"Manager\",\n    groupchat=groupchat,\n    llm_config=llm_config,\n  )\n\n  await user_proxy.a_initiate_chat(manager, message=message)\n\n  return groupchat.messages[-1]\n```\nWhat’s going on here?\n\n1. Anytime a message is sent to `user_proxy`, the conversation will end (we’ll resume it later). Why do this? This means the `user_proxy` can actually proxy for the user. Rather than try to answer, it will end the current conversation flow and allow the real human user to respond (by resuming the conversation — see below).\n2. If the assistant needs more info, it’ll ask user\\_proxy, which will end the current conversation.\n\nIn the above code, what is likely to occur is something like this:\n\n1. user\\_proxy -> manager: “What’s the weather?”\n2. assistant -> manager: “The user didn’t specify for which city.”\n3. manager -> user\\_proxy : conversation will end\n\nNow, if the user wants to respond and resume the conversation, how would we do that? There’s lots of ways to do this, here’s just a sample flavor:\n\n\n```python\n## user makes a POST /query { \"message\": \"What's the weather?\" }\n## above posts returns a `history` array\n## user makes a second POST /query { \"message\": \"What's the weather?\", \"history\": history }\n\nclass ResumableGroupChatManager(GroupChatManager):\n    groupchat: GroupChat\n\n    def __init__(self, groupchat, history, **kwargs):\n        self.groupchat = groupchat\n        if history:\n            self.groupchat.messages = history\n\n        super().__init__(groupchat, **kwargs)\n\n        if history:\n            self.restore_from_history(history)\n\n    def restore_from_history(self, history) -> None:\n        for message in history:\n            # broadcast the message to all agents except the speaker.  This idea is the same way GroupChat is implemented in AutoGen for new messages, this method simply allows us to replay old messages first.\n            for agent in self.groupchat.agents:\n                if agent != self:\n                    self.send(message, agent, request_reply=False, silent=True)\n\n@query_blueprint.route(\"/query\", methods=[\"POST\"])\nasync def post_query():\n  message = request.form.get(\"message\")\n\n  assistant = autogen.AssistantAgent(\n    name=\"assistant\",\n    llm_config=llm_config\n    system_message=\"\"\"You're a helpful assistant.\n    If you need more info, ask the user for anything missing.\"\"\"\n  )\n  user_proxy = autogen.UserProxyAgent(\n    name=\"user_proxy\",\n    human_input_mode=\"NEVER\",\n    code_execution_config=False,\n    is_termination_msg=lambda message: True # Always True\n  )\n  weather_assistant = WeatherAgent(\n    name=\"weather_assistant\",\n    system_message=\"\"\"You're a helpful assistant to get the weather.\n    You fetch weather information, then return it.\"\"\"\n  )\n\n  groupchat = autogen.GroupChat(\n    agents=[assistant, user_proxy, weather_assistant],\n    messages=[]\n  )\n  manager = ResumableGroupChatManager(\n    name=\"Manager\",\n    groupchat=groupchat,\n    llm_config=llm_config,\n  )\n\n  await user_proxy.a_initiate_chat(manager, message=message)\n\n  return {\n    \"response\": groupchat.messages[-1],\n    \"history\": groupchat.messages,\n  }\n```\nUsing this approach, you can now include humans as though they were just another agent in the groupchat. Anytime an assistant agent wants human input, they ask user\\_proxy, user\\_proxy then ends the current conversation, allowing the human user to respond with more information, then pick up the conversation where it left off.\n\nThe benefits to this approach are:\n\n* Conversations can include real human input via any means you want (such as over http or websocket).\n* The conversation is stopped while getting human input. This frees up the thread for other conversations and computation.\n* You can persist these conversations across sessions.\n\n\n## You can (and should) customize who speaks next\n\nThis is subjective, but I think you should always customize the way speakers are selected because:\n\n1. You’ll use less tokens (saves both $ and response time)\n2. You can separate the logic that decides who speaks next from the logic that defines the system instructions for each agent\n\n\n```python\nshort_role_descriptions = {\n  \"user_proxy\": \"A proxy for the user\",\n  \"weather_assistant\": \"You can get the weather\",\n  \"planner\": \"You help coordinate the plan. Your turn happens when XYZ, but skip your turn when ABC\"\n}\n\nclass CustomGroupChat(GroupChat):\n    # The default message uses the full system message, which is a long string.  We are overriding this to use a shorter message.\n    def select_speaker_msg(self, agents: List[Agent]):\n        message = f\"\"\"You are in a role play game. The following roles are available:\n        ---\n        {new_line.join([f\"{agent.name}: {short_role_descriptions[agent.name]}\" for agent in agents])}\n        ---\n\n        The role who plays next depends on the conversation.  User_Proxy will star the conversation, and typically Planner would go next.\n\n        Here are some examples\n        ---\n        ... not shown here ...\n        ---\n\n        Read the following conversation.\n        Then select the next role from {', '.join([agent.name for agent in agents])} to play. Only return the role.\"\"\"\n        return message\n```\n\n## You don’t have to use OpenAI\n\nAutoGen already notes you can use other LLMs, as long as they are “ChatGPT-like”, meaning their API responds with a similar shape and response as ChatGPT API calls.\n\nBut, remember how these agents are classes, and you can override most of the methods?\n\nTry overriding the method: [generate\\_oai\\_reply](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L678), and you can query any LLM you’d like.\n\n\n## Functions can be used instead of executing code\n\nWhen I went to our security team and said “I’d like to use AutoGen for my service in Kubernetes. It needs to be able to execute any arbitrary code produced by an LLM. You’re ok with that, right?”\n\nOf course, the answer was a definite: NO.\n\nSo, why use AutoGen without the auto-code-execution abilities?\n\nOn top of the reasons stated below, another is that you can use function calling to gain total control over code execution. If you have a set of python functions you want to provide to AutoGen — functions you wrote, control, and can accept some safe parameters — that sounds like a better idea anyway than the wild west of allowing any and all code to be executed in your private infrastructure.\n\n\n## Use Agents for organization, not just for conversations\n\nMaybe you don’t have a need for an autonomous, multi-agent conversation. Maybe you just need to make a few different calls to an LLM.\n\nI still like the idea of having different “Agents” just for the sake of organization. Here’s a really crazy idea, but take it for what it’s worth:\n\n\n```python\nanalyst = autogen.AssistantAgent(\n    name=\"Analyst\",\n    system_message=\"\"\"Your an analyst.  You do XYZ.\"\"\",\n    llm_config=llm_config,\n)\n\nsummarizer = autogen.AssistantAgent(\n    name=\"Summarizer\",\n    system_message=\"\"\"Your a summarizer.  You do XYZ.\"\"\",\n    llm_config=llm_config,\n)\n\nreport = \"\"\"Some long report\"\"\"\n\nanalysis = analyst.generate_oai_reply(report)[1]\nsummary = summarizer.generate_oai_reply(report)[1]\n\nprint(f\"Analysis: {analysis}\")\nprint(f\"Summary: {summary}\")\n```\n\n## Why use AutoGen?\n\n1. AutoGen allows multiple agents, with different system prompts and instructions, to solve a problem. Just like in real-life, different perspectives working together will solve a problem better than a single brain.\n2. AutoGen GroupChat is amazing. It provides routing to the right experts (agents), and it allows a conversation to continue autonomously until the problem is solved. Some conversations will go from agent: a->b->c->d, others will be b->a->d->c. This allows AutoGen to solve a variety of different problems without needing explicit rules for each scenario.\n3. AutoGen can recover from mistakes. For example, I made an AutoGen-powered service that made API calls to a service. Sometimes, the API calls errored out because it didn’t send the right data at first. The AutoGen GroupChat kept trying different things until it succeeded. Sometimes, it took 4+ attempts, but my Planner agent didn’t give up — just pivoted autonomously to handle the API failures and try new things.\n4. AutoGen came up with the concept of separating `UserProxyAgent`s from `AssistantAgent` s from the beginning. This also allows us to let the user proxy actually proxy for the user, as shown above.\n5. AutoGen is a well maintained library. Every week they’re adding something new.\n6. AutoGen is very extensible. With the way they’ve built their classes, you can customize anything to your liking.\n7. AutoGen has other features I don’t use, but others may find them helpful, such as helping you count tokens and cost of conversations, cacheing, etc.\n\n"},{"lang":"en","group":"blog","slug":"blog/alibabas-open-source-qwen-how-it-s-revolutionizing-ai-and-how-you-can-use-it-dcba8f687c97","frontmatter":{"title":"Alibaba’s Open-Source Qwen: How It’s Revolutionizing AI and How You Can Use It","meta_title":"Alibaba’s Open-Source Qwen: How It’s Revolutionizing AI and How You Can Use It","description":"Alibaba has recently made waves in the AI world by open-sourcing its Qwen 2.5 models during the 2024 Apsara Conference. With over 100…","date":"2024-10-26T00:26:25.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*I7QDwbLMzoJ_ORq5.jpg","categories":["Programming","Machine Learning","Natural Language Processing"],"author":"Rifx.Online","tags":["Qwen","multimodal","open-source","fine-tune","text-to-video"],"draft":false,"slug":"blog/alibabas-open-source-qwen-how-it-s-revolutionizing-ai-and-how-you-can-use-it-dcba8f687c97"},"content":"\nAlibaba has recently made waves in the AI world by open-sourcing its **Qwen 2.5** models during the 2024 Apsara Conference. With over 100 models, Qwen spans multiple modalities including language, vision, audio, and code, making it one of the most comprehensive open-source AI solutions. The release empowers developers by providing tools for diverse applications, from text-to-video generation to real-time question answering.\n\n\n\n## Key Features of Alibaba’s Qwen Models\n\n1. **Multimodal Capabilities**: Qwen models handle diverse inputs, including text, audio, and visual data. This multimodal approach makes them suitable for a wide range of industries, from media and entertainment to robotics.\n2. **Open Source**: Available on platforms like **Hugging Face** and **ModelScope**, Qwen has already been downloaded over 40 million times, with over 50,000 custom models built on its foundation.\n3. **Enhanced Performance**: Qwen2.5 introduces improved language understanding, mathematics, and coding capabilities, rivaling leading models in the field. With optimized performance for tasks like structured data understanding and long text generation, Qwen opens the door to high-level AI applications.\n\n## How to Use Alibaba’s Qwen\n\nDevelopers and organizations can access Qwen models on platforms like Hugging Face, where they can:\n\n* **Fine-tune models**: Tailor Qwen for specific industry applications such as customer service, automation, or video content creation.\n* **Integrate with applications**: Qwen’s text-to-video model can be incorporated into media production pipelines, generating dynamic content from static images and text prompts.\n* **Develop AI assistants**: With enhanced vision-language models, Qwen can be used in robotics and autonomous vehicles to process video data and perform real-time tasks like navigation or object recognition.\n\n**Example of Using Qwen via Hugging Face**:\n\n```python\nfrom transformers import QwenTokenizer, QwenModel\n\ntokenizer = QwenTokenizer.from_pretrained(\"qwen-2.5\")\nmodel = QwenModel.from_pretrained(\"qwen-2.5\")\n\ninput_text = \"What is the future of AI in healthcare?\"\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\noutputs = model(input_ids)\n```\n\nThis allows users to access Qwen models, run inference, and customize them based on specific needs.\n\n## Qwen’s Impact Across Industries\n\n1. **Media and Entertainment**: With the new text-to-video capabilities, Qwen can automatically generate videos from written scripts, transforming the creative industry by automating tedious production tasks.\n2. **Robotics and Autonomous Vehicles**: The enhanced vision-language models in Qwen can help robots understand real-world environments, leading to better decision-making in autonomous driving or manufacturing.\n3. **Software Development**: Alibaba’s AI Developer tool, powered by Qwen, automates tasks like code generation, debugging, and requirement analysis, enabling developers to focus on higher-level problem-solving.\n\n## Conclusion: A New Era of Open AI Innovation\n\nBy open-sourcing its Qwen 2.5 models, Alibaba is democratizing access to advanced AI technologies. Developers, startups, and large enterprises alike can harness Qwen’s multimodal and real-time capabilities to drive innovation in industries ranging from media to autonomous vehicles. Whether you’re a developer looking to fine-tune models for a niche application or a corporation integrating AI into your infrastructure, Qwen offers powerful tools to accelerate progress.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/building-a-local-ai-powered-news-aggregator-with-ollama-swarm-and-duckduckgo-95aaf8b3ee41","frontmatter":{"title":"Building a Local AI-Powered News Aggregator with Ollama, Swarm, and DuckDuckGo","meta_title":"Building a Local AI-Powered News Aggregator with Ollama, Swarm, and DuckDuckGo","description":"No subtitle provided","date":"2024-10-24T17:47:43.000Z","image":"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*OHMOTk_WYGOxWHBsKqdpNQ.jpeg","categories":["Programming","Generative AI","Technology/Web"],"author":"Rifx.Online","tags":["Llama","Swarm","DuckDuckGo","News","Aggregator"],"draft":false,"slug":"blog/building-a-local-ai-powered-news-aggregator-with-ollama-swarm-and-duckduckgo-95aaf8b3ee41"},"content":"\n\n# Building a Local AI-Powered News Aggregator with Ollama, Swarm, and DuckDuckGo\n\n\n\nIn today’s fast-paced world, staying up-to-date with the latest news in specific fields can be challenging. What if we could leverage the power of Generative AI and Agents to create a personalized news aggregator that runs entirely on our local machine? In this article, we’ll explore how to build such a system using **Ollama**’s Llama 3.2 model, **Swarm** for agent orchestration, and **DuckDuckGo** for web searches.\n\n\n# The Power of Local AI\n\nWith the rise of large language models, we now have the ability to run sophisticated AI systems on our personal computers. This opens up a world of possibilities for creating customized tools tailored to our specific needs. Our news aggregator is a perfect example of this potential.\n\n\n# Components of Our System\n\n1. **Ollama with Llama 3.2**: This serves as the brain of our system, powering our AI agents.\n2. **Swarm**: An agent orchestration framework that allows us to create and manage multiple AI agents.\n3. **DuckDuckGo Search**: Provides up-to-date web search results without tracking user data.\n\n\n# How It Works\n\nOur news aggregator consists of two main AI agents:\n\n1. **News Assistant**: Fetches the latest news articles on a given topic using DuckDuckGo search.\n2. **Editor Assistant**: Reviews and refines the collected news for final presentation.\n\nLet’s break down the workflow:\n\n\n# 1. Setting Up the Environment\n\n\n```python\nollama pull llama3.2\n\nexport OPENAI_MODEL_NAME=llama3.2\nexport OPENAI_BASE_URL=http://localhost:11434/v1\nexport OPENAI_API_KEY=any\n\npip install git+https://github.com/openai/swarm.git duckduckgo-search\n```\nWe start by importing the necessary libraries and initializing our Swarm client:\n\n\n```python\nfrom duckduckgo_search import DDGS\nfrom swarm import Swarm, Agent\nfrom datetime import datetime\n\ncurrent_date = datetime.now().strftime(\"%Y-%m\")\nclient = Swarm()\n```\n\n# 2. Creating the News Search Function\n\nWe define a function to search for news using DuckDuckGo:\n\n\n```python\npythondef get_news_articles(topic):\n  ddg_api = DDGS()\n  results = ddg_api.text(f\"{topic} {current_date}\", max_results=5)\n  if results:\n      news_results = \"\\n\\n\".join([f\"Title: {result['title']}\\nURL: {result['href']}\\nDescription: {result['body']}\" for result in results])\n      return news_results\n  else:\n      return f\"Could not find news results for {topic}.\"\n```\n\n# 3. Defining Our AI Agents\n\nWe create two agents using Ollama’s Llama 3.2 model:\n\n\n```python\nnews_agent = Agent(\n  model=\"llama3.2\",\n  name=\"News Assistant\",\n  instructions=\"You provide the latest news articles for a given topic using DuckDuckGo search.\",\n  functions=[get_news_articles],\n)\n\neditor_agent = Agent(\n  model=\"llama3.2\",\n  name=\"Editor Assistant\",\n  instructions=\"You review and finalise the news article for publishing.\",\n)\n```\n\n# 4. Orchestrating the Workflow\n\nWe define a function to run our news aggregation workflow:\n\n\n```python\ndef run_news_workflow(topic):\n  # Fetch news\n  news_response = client.run(\n      agent=news_agent,\n      messages=[{\"role\": \"user\", \"content\": f\"Get me the news about {topic} on {current_date}\"}],\n  )\n  raw_news = news_response.messages[-1][\"content\"]\n  \n  # Pass news to editor for final review\n  edited_news_response = client.run(\n      agent=editor_agent,\n      messages=[{\"role\": \"system\", \"content\": raw_news}],\n  )\n  print(f\"{edited_news_response.messages[-1]['content']}\")\n```\n\n# 5. Running the System\n\nFinally, we can run our news aggregator for any topic of interest:\n\n\n```python\nrun_news_workflow(\"AI in Drug Discovery\")\n```\n\n# Complete Code : app.py\n\n\n```python\nfrom duckduckgo_search import DDGS\nfrom swarm import Swarm, Agent\nfrom datetime import datetime\n\ncurrent_date = datetime.now().strftime(\"%Y-%m\")\n\n# Initialize Swarm client\nclient = Swarm()\n\n# 1. Create Internet Search Tool\n\ndef get_news_articles(topic):\n    print(f\"Running DuckDuckGo news search for {topic}...\")\n    \n    # DuckDuckGo search\n    ddg_api = DDGS()\n    results = ddg_api.text(f\"{topic} {current_date}\", max_results=5)\n    if results:\n        news_results = \"\\n\\n\".join([f\"Title: {result['title']}\\nURL: {result['href']}\\nDescription: {result['body']}\" for result in results])\n        return news_results\n    else:\n        return f\"Could not find news results for {topic}.\"\n    \n# 2. Create AI Agents\n\ndef transfer_to_editor_assistant(raw_news):\n    print(\"Passing articles to Editor Assistant...\")\n    return editor_agent.run({\"role\": \"system\", \"content\": raw_news})\n\n# News Agent to fetch news\nnews_agent = Agent(\n    model=\"llama3.2\",\n    name=\"News Assistant\",\n    instructions=\"You provide the latest news articles for a given topic using DuckDuckGo search.\",\n    functions=[get_news_articles],\n)\n\n# Editor Agent to edit news\neditor_agent = Agent(\n    model=\"llama3.2\",\n    name=\"Editor Assistant\",\n    instructions=\"You review and finalise the news article for publishing.\",\n)\n\n# 3. Create workflow\n\ndef run_news_workflow(topic):\n    print(\"Running news Agent workflow...\")\n    \n    # Step 1: Fetch news\n    news_response = client.run(\n        agent=news_agent,\n        messages=[{\"role\": \"user\", \"content\": f\"Get me the news about {topic} on {current_date}\"}],\n    )\n    raw_news = news_response.messages[-1][\"content\"]\n    print(f\"Fetched news: {raw_news}\")\n    \n    # Step 2: Pass news to editor for final review\n    edited_news_response = client.run(\n        agent=editor_agent,\n        messages=[{\"role\": \"system\", \"content\": raw_news}],\n    )\n    print(f\"{edited_news_response.messages[-1]['content']}\")\n\n\n# Example of running the news workflow for a given topic\nrun_news_workflow(\"AI in Drug Discovery\")\n```\n\n# Sample Output\n\n\n```python\nRunning news Agent workflow...\nRunning DuckDuckGo news search for AI in Drug Discovery...\nFetched news: Here's a formatted answer based on the news articles:\n\n**AI in Drug Discovery: A Revolutionary Shift**\n\nThe role of Artificial Intelligence (AI) in drug discovery has marked a revolutionary shift in the pharmaceutical landscape. AI leverages sophisticated algorithms for autonomous decision-making from data analysis, augmenting human capabilities rather than replacing them.\n\n**Challenges and Limitations**\n\nDespite the promising advancements, challenges and limitations have been identified in the field. The paper \"The Role of AI in Drug Discovery\" addresses these issues, highlighting the need for high-quality data, addressing ethical concerns, and recognizing the limitations of AI-based approaches.\n\n**Applications of AI in Drug Discovery**\n\nAI has the potential to play a critical role in drug discovery, design, and studying drug-drug interactions.Applications of AI in drug discovery include:\n\n* Polypharmacology: AI can predict the likelihood of a compound's effectiveness against multiple diseases.\n* Chemical synthesis: AI can optimize chemical synthesis processes for faster and more efficient production.\n* Drug repurposing: AI can identify new uses for existing drugs.\n* Predicting drug properties: AI can predict the efficacy, toxicity, and physicochemical characteristics of compounds.\n\n**The Future of AI in Drug Discovery**\n\nAs AI continues to evolve, it is expected to significantly impact the pharmaceutical industry. The successful application of AI will depend on the availability of high-quality data, addressing ethical concerns, and recognizing the limitations of AI-based approaches.\n```\n\n# The Benefits of Local AI News Aggregation\n\n* **Privacy**: All processing happens on your local machine, ensuring your data stays with you.\n* **Customization**: You can easily modify the agents’ instructions or add new agents to suit your specific needs.\n* **Up-to-date Information**: By using DuckDuckGo search, you always get the latest news on your chosen topic.\n* **AI-powered Curation**: The Editor Assistant helps refine and organize the collected news, providing a more polished final output.\n\n\n# Conclusion\n\nThis local AI-powered news aggregator demonstrates the potential of combining large language models with web search capabilities. By leveraging Ollama’s Llama 3.2 model, Swarm for agent orchestration, and DuckDuckGo for search, we’ve created a powerful tool that can keep us informed on any topic of interest, all while maintaining our privacy and running entirely on our local machine.\n\nAs AI continues to evolve, the possibilities for creating personalized, AI-driven tools will only expand. This news aggregator is just the beginning — imagine what other innovative applications you could build using these technologies!\n\n\n# Reference :\n\nSwarm Github : <https://github.com/openai/swarm>\n\nIf you found this article informative and valuable, I’d greatly appreciate your support:\n\n* Give it a few claps 👏 on Medium to help others discover this content (did you know you can clap up to 50 times?). Your claps will help spread the knowledge to more readers.\n- Share it with your network of AI enthusiasts and professionals.\n- Connect with me on LinkedIn: <https://www.linkedin.com/in/manjunath-janardhan-54a5537/>\n\n\n\n\n\n"},{"lang":"en","group":"blog","slug":"blog/choosing-between-llm-agent-frameworks-69019493b259","frontmatter":{"title":"Choosing Between LLM Agent Frameworks","meta_title":"Choosing Between LLM Agent Frameworks","description":"The tradeoffs between building bespoke code-based agents and the major agent frameworks.","date":"2024-10-29T12:57:34.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*jRMs19HqSCazE5dY","categories":["Programming","Technology","Machine Learning"],"author":"Rifx.Online","tags":["agents","frameworks","LangGraph","LlamaIndex","Workflows"],"draft":false,"slug":"blog/choosing-between-llm-agent-frameworks-69019493b259"},"content":"\n### The tradeoffs between building bespoke code\\-based agents and the major agent frameworks.\n\n\n\n\nAgents are having a moment. With multiple new frameworks and fresh [investment](https://foundationcapital.com/goodbye-aiops-welcome-agentsres-the-next-100b-opportunity/) in the space, modern AI agents are overcoming [shaky origins](https://arxiv.org/html/2405.13966v1) to rapidly supplant RAG as an implementation priority. So will 2024 finally be the year that autonomous AI systems that can take over writing our emails, booking flights, talking to our data, or seemingly any other task?\n\nMaybe, but much work remains to get to that point. Any developer building an agent must not only choose foundations — which model, use case, and architecture to use — but also which framework to leverage. Do you go with the long\\-standing LangGraph, or the newer entrant LlamaIndex Workflows? Or do you go the traditional route and code the whole thing yourself?\n\nThis post aims to make that choice a bit easier. Over the past few weeks, I built the same agent in major frameworks to examine some of the strengths and weaknesses of each at a technical level. All of the code for each agent is available in [this repo](https://github.com/Arize-ai/phoenix/tree/main/examples/agent_framework_comparison).\n\n### Background on the Agent Used for Testing\n\nThe agent used for testing includes function calling, multiple tools or skills, connections to outside resources, and shared state or memory.\n\nThe agent has the following capabilities:\n\n1. Answering questions from a knowledge base\n2. Talking to data: answering questions about telemetry data of an LLM application\n3. Analyzing data: analyzing higher\\-level trends and patterns in retrieved telemetry data\n\nIn order to accomplish these, the agent has three starting skills: RAG with product documentation, SQL generation on a trace database, and data analysis. A simple gradio\\-powered interface is used for the agent UI, with the agent itself structured as a chatbot.\n\n## Code\\-Based Agent (No Framework)\n\nThe first option you have when developing an agent is to skip the frameworks entirely and build the agent fully yourself. When embarking on this project, this was the approach I started with.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*pw9-0lB5JMlVcPqo)\n\n### Pure Code Architecture\n\nThe code\\-based agent below is made up of an OpenAI\\-powered router that uses function calling to select the right skill to use. After that skill completes, it returns back to the router to either call another skill or respond to the user.\n\nThe agent keeps an ongoing list of messages and responses that is passed fully into the router on each call to preserve context through cycles.\n\n```python\ndef router(messages):\n    if not any(\n        isinstance(message, dict) and message.get(\"role\") == \"system\" for message in messages\n    ):\n        system_prompt = {\"role\": \"system\", \"content\": SYSTEM_PROMPT}\n        messages.append(system_prompt)\n\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=messages,\n        tools=skill_map.get_combined_function_description_for_openai(),\n    )\n\n    messages.append(response.choices[0].message)\n    tool_calls = response.choices[0].message.tool_calls\n    if tool_calls:\n        handle_tool_calls(tool_calls, messages)\n        return router(messages)\n    else:\n        return response.choices[0].message.content\n```\n\nThe skills themselves are defined in their own classes (e.g. GenerateSQLQuery) that are collectively held in a SkillMap. The router itself only interacts with the SkillMap, which it uses to load skill names, descriptions, and callable functions. This approach means that adding a new skill to the agent is as simple as writing that skill as its own class, then adding it to the list of skills in the SkillMap. The idea here is to make it easy to add new skills without disturbing the router code.\n\n```python\nclass SkillMap:\n    def __init__(self):\n        skills = [AnalyzeData(), GenerateSQLQuery()]\n\n        self.skill_map = {}\n        for skill in skills:\n            self.skill_map[skill.get_function_name()] = (\n                skill.get_function_dict(),\n                skill.get_function_callable(),\n            )\n\n    def get_function_callable_by_name(self, skill_name) -> Callable:\n        return self.skill_map[skill_name][1]\n\n    def get_combined_function_description_for_openai(self):\n        combined_dict = []\n        for _, (function_dict, _) in self.skill_map.items():\n            combined_dict.append(function_dict)\n        return combined_dict\n\n    def get_function_list(self):\n        return list(self.skill_map.keys())\n\n    def get_list_of_function_callables(self):\n        return [skill[1] for skill in self.skill_map.values()]\n\n    def get_function_description_by_name(self, skill_name):\n        return str(self.skill_map[skill_name][0][\"function\"])\n```\n\nOverall, this approach is fairly straightforward to implement but comes with a few challenges.\n\n### Challenges with Pure Code Agents\n\nThe first difficulty lies in structuring the router system prompt. Often, the router in the example above insisted on generating SQL itself instead of delegating that to the right skill. If you’ve ever tried to get an LLM *not* to do something, you know how frustrating that experience can be; finding a working prompt took many rounds of debugging. Accounting for the different output formats from each step was also tricky. Since I opted not to use structured outputs, I had to be ready for multiple different formats from each of the LLM calls in my router and skills.\n\n### Benefits of a Pure Code Agent\n\nA code\\-based approach provides a good baseline and starting point, offering a great way to learn how agents work without relying on canned agent tutorials from prevailing frameworks. Although convincing the LLM to behave can be challenging, the code structure itself is simple enough to use and might make sense for certain use cases (more in the analysis section below).\n\n## LangGraph\n\nLangGraph is one of the longest\\-standing agent frameworks, first releasing in January 2024\\. The framework is built to address the acyclic nature of existing pipelines and chains by adopting a Pregel graph structure instead. LangGraph makes it easier to define loops in your agent by adding the concepts of nodes, edges, and conditional edges to traverse a graph. LangGraph is built on top of LangChain, and uses the objects and types from that framework.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*fYgHiGwLhSUSrFv9)\n\n### LangGraph Architecture\n\nThe LangGraph agent looks similar to the code\\-based agent on paper, but the code behind it is drastically different. LangGraph still uses a “router” technically, in that it calls OpenAI with functions and uses the response to continue to a new step. However the way the program moves between skills is controlled completely differently.\n\n```python\ntools = [generate_and_run_sql_query, data_analyzer]\nmodel = ChatOpenAI(model=\"gpt-4o\", temperature=0).bind_tools(tools)\n\ndef create_agent_graph():\n    workflow = StateGraph(MessagesState)\n\n    tool_node = ToolNode(tools)\n    workflow.add_node(\"agent\", call_model)\n    workflow.add_node(\"tools\", tool_node)\n\n    workflow.add_edge(START, \"agent\")\n    workflow.add_conditional_edges(\n        \"agent\",\n        should_continue,\n    )\n    workflow.add_edge(\"tools\", \"agent\")\n\n    checkpointer = MemorySaver()\n    app = workflow.compile(checkpointer=checkpointer)\n    return app\n```\n\nThe graph defined here has a node for the initial OpenAI call, called “agent” above, and one for the tool handling step, called “tools.” LangGraph has a built\\-in object called ToolNode that takes a list of callable tools and triggers them based on a ChatMessage response, before returning to the “agent” node again.\n\n```python\ndef should_continue(state: MessagesState):\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    if last_message.tool_calls:\n        return \"tools\"\n    return END\n\ndef call_model(state: MessagesState):\n    messages = state[\"messages\"]\n    response = model.invoke(messages)\n    return {\"messages\": [response]}\n```\n\nAfter each call of the “agent” node (put another way: the router in the code\\-based agent), the should\\_continue edge decides whether to return the response to the user or pass on to the ToolNode to handle tool calls.\n\nThroughout each node, the “state” stores the list of messages and responses from OpenAI, similar to the code\\-based agent’s approach.\n\n### Challenges with LangGraph\n\nMost of the difficulties with LangGraph in the example stem from the need to use Langchain objects for things to flow nicely.\n\n**Challenge \\#1: Function Call Validation**\n\nIn order to use the ToolNode object, I had to refactor most of my existing Skill code. The ToolNode takes a list of callable functions, which originally made me think I could use my existing functions, however things broke down due to my function parameters.\n\nThe skills were defined as classes with a callable member function, meaning they had “self” as their first parameter. GPT\\-4o was smart enough to not include the “self” parameter in the generated function call, however LangGraph read this as a validation error due to a missing parameter.\n\nThis took hours to figure out, because the error message instead marked the third parameter in the function (“args” on the data analysis skill) as the missing parameter:\n\n```python\npydantic.v1.error_wrappers.ValidationError: 1 validation error for data_analysis_toolSchema\nargs field required (type=value_error.missing)\n```\n\nIt is worth mentioning that the error message originated from Pydantic, not from LangGraph.\n\nI eventually bit the bullet and redefined my skills as basic methods with Langchain’s @tool decorator, and was able to get things working.\n\n```python\n@tool\ndef generate_and_run_sql_query(query: str):\n    \"\"\"Generates and runs an SQL query based on the prompt.\n\n    Args:\n        query (str): A string containing the original user prompt.\n\n    Returns:\n        str: The result of the SQL query.\n    \"\"\"\n```\n\n**Challenge \\#2: Debugging**\n\nAs mentioned, debugging in a framework is difficult. This primarily comes down to confusing error messages and abstracted concepts that make it harder to view variables.\n\nThe abstracted concepts primarily show up when trying to debug the messages being sent around the agent. LangGraph stores these messages in state\\[“messages”]. Some nodes within the graph pull from these messages automatically, which can make it difficult to understand the value of messages when they are accessed by the node.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*KuCg0WGHSklOKe6t)\n\n### LangGraph Benefits\n\nOne of the main benefits of LangGraph is that it’s easy to work with. The graph structure code is clean and accessible. Especially if you have complex node logic, having a single view of the graph makes it easier to understand how the agent is connected together. LangGraph also makes it straightforward to convert an existing application built in LangChain.\n\n### Takeaway\n\nIf you use everything in the framework, LangGraph works cleanly; if you step outside of it, prepare for some debugging headaches.\n\n## LlamaIndex Workflows\n\nWorkflows is a newer entrant into the agent framework space, premiering earlier this summer. Like LangGraph, it aims to make looping agents easier to build. Workflows also has a particular focus on running asynchronously.\n\nSome elements of Workflows seem to be in direct response to LangGraph, specifically its use of events instead of edges and conditional edges. Workflows use steps (analogous to nodes in LangGraph) to house logic, and emitted and received events to move between steps.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*22WuFVBWctdeiCSL)\n\nThe structure above looks similar to the LangGraph structure, save for one addition. I added a setup step to the Workflow to prepare the agent context, more on this below. Despite the similar structure, there is very different code powering it.\n\n### Workflows Architecture\n\nThe code below defines the Workflow structure. Similar to LangGraph, this is where I prepared the state and attached the skills to the LLM object.\n\n```python\nclass AgentFlow(Workflow):\n    def __init__(self, llm, timeout=300):\n        super().__init__(timeout=timeout)\n        self.llm = llm\n        self.memory = ChatMemoryBuffer(token_limit=1000).from_defaults(llm=llm)\n        self.tools = []\n        for func in skill_map.get_function_list():\n            self.tools.append(\n                FunctionTool(\n                    skill_map.get_function_callable_by_name(func),\n                    metadata=ToolMetadata(\n                        name=func, description=skill_map.get_function_description_by_name(func)\n                    ),\n                )\n            )\n\n    @step\n    async def prepare_agent(self, ev: StartEvent) -> RouterInputEvent:\n        user_input = ev.input\n        user_msg = ChatMessage(role=\"user\", content=user_input)\n        self.memory.put(user_msg)\n\n        chat_history = self.memory.get()\n        return RouterInputEvent(input=chat_history)\n```\n\nThis is also where I define an extra step, “prepare\\_agent”. This step creates a ChatMessage from the user input and adds it to the workflow memory. Splitting this out as a separate step means that we do return to it as the agent loops through steps, which avoids repeatedly adding the user message to the memory.\n\nIn the LangGraph case, I accomplished the same thing with a run\\_agent method that lived outside the graph. This change is mostly stylistic, however it’s cleaner in my opinion to house this logic with the Workflow and graph as we’ve done here.\n\nWith the Workflow set up, I then defined the routing code:\n\n```python\n@step\nasync def router(self, ev: RouterInputEvent) -> ToolCallEvent | StopEvent:\n    messages = ev.input\n\n    if not any(\n        isinstance(message, dict) and message.get(\"role\") == \"system\" for message in messages\n    ):\n        system_prompt = ChatMessage(role=\"system\", content=SYSTEM_PROMPT)\n        messages.insert(0, system_prompt)\n\n    with using_prompt_template(template=SYSTEM_PROMPT, version=\"v0.1\"):\n        response = await self.llm.achat_with_tools(\n            model=\"gpt-4o\",\n            messages=messages,\n            tools=self.tools,\n        )\n\n    self.memory.put(response.message)\n\n    tool_calls = self.llm.get_tool_calls_from_response(response, error_on_no_tool_call=False)\n    if tool_calls:\n        return ToolCallEvent(tool_calls=tool_calls)\n    else:\n        return StopEvent(result=response.message.content)\n```\n\nAnd the tool call handling code:\n\n```python\n@step\nasync def tool_call_handler(self, ev: ToolCallEvent) -> RouterInputEvent:\n    tool_calls = ev.tool_calls\n\n    for tool_call in tool_calls:\n        function_name = tool_call.tool_name\n        arguments = tool_call.tool_kwargs\n        if \"input\" in arguments:\n            arguments[\"prompt\"] = arguments.pop(\"input\")\n\n        try:\n            function_callable = skill_map.get_function_callable_by_name(function_name)\n        except KeyError:\n            function_result = \"Error: Unknown function call\"\n\n        function_result = function_callable(arguments)\n        message = ChatMessage(\n            role=\"tool\",\n            content=function_result,\n            additional_kwargs={\"tool_call_id\": tool_call.tool_id},\n        )\n\n        self.memory.put(message)\n\n    return RouterInputEvent(input=self.memory.get())\n```\n\nBoth of these look more similar to the code\\-based agent than the LangGraph agent. This is mainly because Workflows keeps the conditional routing logic in the steps as opposed to in conditional edges — lines 18–24 were a conditional edge in LangGraph, whereas now they are just part of the routing step — and the fact that LangGraph has a ToolNode object that does just about everything in the tool\\_call\\_handler method automatically.\n\nMoving past the routing step, one thing I was very happy to see is that I could use my SkillMap and existing skills from my code\\-based agent with Workflows. These required no changes to work with Workflows, which made my life much easier.\n\n### Challenges with Workflows\n\n**Challenge \\#1: Sync vs Async**\n\nWhile asynchronous execution is preferable for a live agent, debugging a synchronous agent is much easier. Workflows is designed to work asynchronously, and trying to force synchronous execution was very difficult.\n\nI initially thought I would just be able to remove the “async” method designations and switch from “achat\\_with\\_tools” to “chat\\_with\\_tools”. However, since the underlying methods within the Workflow class were also marked as asynchronous, it was necessary to redefine those in order to run synchronously. I ended up sticking to an asynchronous approach, but this didn’t make debugging more difficult.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*78Hzqkiv9cI7W4UA)\n\n**Challenge \\#2: Pydantic Validation Errors**\n\nIn a repeat of the woes with LangGraph, similar problems emerged around confusing Pydantic validation errors on skills. Fortunately, these were easier to address this time since Workflows was able to handle member functions just fine. I ultimately just ended up having to be more prescriptive in creating LlamaIndex FunctionTool objects for my skills:\n\n```python\nfor func in skill_map.get_function_list(): \n            self.tools.append(FunctionTool(\n                skill_map.get_function_callable_by_name(func), \n                metadata=ToolMetadata(name=func, description=skill_map.get_function_description_by_name(func))))\n```\n\n*Excerpt from AgentFlow.\\_\\_init\\_\\_ that builds FunctionTools*\n\n### Benefits of Workflows\n\nI had a much easier time building the Workflows agent than I did the LangGraph agent, mainly because Workflows still required me to write routing logic and tool handling code myself instead of providing built\\-in functions. This also meant that my Workflow agent looked extremely similar to my code\\-based agent.\n\nThe biggest difference came in the use of events. I used two custom events to move between steps in my agent:\n\n```python\nclass ToolCallEvent(Event):\n    tool_calls: list[ToolSelection]\n\nclass RouterInputEvent(Event):\n    input: list[ChatMessage]\n```\n\nThe emitter\\-receiver, event\\-based architecture took the place of directly calling some of the methods in my agent, like the tool call handler.\n\nIf you have more complex systems with multiple steps that are triggering asynchronously and might emit multiple events, this architecture becomes very helpful to manage that cleanly.\n\nOther benefits of Workflows include the fact that it is very lightweight and doesn’t force much structure on you (aside from the use of certain LlamaIndex objects) and that its event\\-based architecture provides a helpful alternative to direct function calling — especially for complex, asynchronous applications.\n\n## Comparing Frameworks\n\nLooking across the three approaches, each one has its benefits.\n\nThe no framework approach is the simplest to implement. Because any abstractions are defined by the developer (i.e. SkillMap object in the above example), keeping various types and objects straight is easy. The readability and accessibility of the code entirely comes down to the individual developer however, and it’s easy to see how increasingly complex agents could get messy without some enforced structure.\n\nLangGraph provides quite a bit of structure, which makes the agent very clearly defined. If a broader team is collaborating on an agent, this structure would provide a helpful way of enforcing an architecture. LangGraph also might provide a good starting point with agents for those not as familiar with the structure. There is a tradeoff, however — since LangGraph does quite a bit for you, it can lead to headaches if you don’t fully buy into the framework; the code may be very clean, but you may pay for it with more debugging.\n\nWorkflows falls somewhere in the middle. The event\\-based architecture might be extremely helpful for some projects, and the fact that less is required in terms of using of LlamaIndex types provides greater flexibility for those not be fully using the framework across their application.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*PITmiVGuG8QuDVX6)\n\nUltimately, the core question may just come down to “are you already using LlamaIndex or LangChain to orchestrate your application?” LangGraph and Workflows are both so entwined with their respective underlying frameworks that the additional benefits of each agent\\-specific framework might not cause you to switch on merit alone.\n\nThe pure code approach will likely always be an attractive option. If you have the rigor to document and enforce any abstractions created, then ensuring nothing in an external framework slows you down is easy.\n\n## Key Questions To Help In Choosing An Agent Framework\n\nOf course, “it depends” is never a satisfying answer. These three questions should help you decide which framework to use in your next agent project.\n\n***Are you already using LlamaIndex or LangChain for significant pieces of your project?***\n\nIf yes, explore that option first.\n\n***Are you familiar with common agent structures, or do you want something telling you how you should structure your agent?***\n\nIf you fall into the latter group, try Workflows. If you *really* fall into the latter group, try LangGraph.\n\n***Has your agent been built before?***\n\nOne of the framework benefits is that there are many tutorials and examples built with each. There are far fewer examples of pure code agents to build from.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*wF9aSF1db1yaniqO)\n\n## Conclusion\n\nPicking an agent framework is just one choice among many that will impact outcomes in production for generative AI systems. As always, it pays to have robust guardrails and [LLM tracing](https://docs.arize.com/phoenix/tracing/llm-traces) in place — and to be agile as new agent frameworks, research, and models upend established techniques.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/claude-3-5-sonnet-new-pioneering-the-future-of-ai-with-computer-control-capabilities-37a6ff9f9033","frontmatter":{"title":"Claude 3.5 Sonnet (New): Pioneering the Future of AI with Computer Control Capabilities","meta_title":"Claude 3.5 Sonnet (New): Pioneering the Future of AI with Computer Control Capabilities","description":"Anthropic has unveiled its latest AI model, Claude 3.5 Sonnet, on October 22, 2024. This release introduces revolutionary computer control…","date":"2024-10-27T13:57:00.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*n0NkOFbhUm7_fllJ","categories":["Programming","Technology","Generative AI"],"author":"Rifx.Online","tags":["Claude","Sonnet","automation","benchmarks","safety"],"draft":false,"slug":"blog/claude-3-5-sonnet-new-pioneering-the-future-of-ai-with-computer-control-capabilities-37a6ff9f9033"},"content":"\n\n\n\n\n\nAnthropic has unveiled its latest AI model, Claude 3.5 Sonnet, on October 22, 2024. This release introduces revolutionary computer control capabilities and substantial improvements across various benchmarks, setting new standards in the AI industry.\n\n\n## Revolutionary Computer Control: A New Frontier\n\nThe standout feature of Claude 3.5 Sonnet is its ability to interact with computers just like humans do. This groundbreaking capability allows the AI to:\n\n* Navigate desktop interfaces using mouse and keyboard inputs\n* Interact with various applications and web browsers\n* Execute complex multi-step tasks\n* Perform file management operations\n* Automate repetitive workflows\n\nThis computer control feature, currently in public beta, represents a paradigm shift in how AI systems can interact with digital interfaces. While still in its experimental phase, early testing shows promising results, with Claude 3.5 Sonnet scoring 14.9% on the OSWorld benchmark for screenshot-only tasks — significantly higher than the next-best system’s 7.8%.\n\n\n## Benchmark-Breaking Performance\n\nThe upgraded model demonstrates remarkable improvements across various metrics:\n\n\n## Coding and Technical Tasks\n\n* 49% performance on SWE-bench Verified (up from 33.4%)\n* 93.7% score on HumanEval coding tasks\n* Superior performance in software engineering compared to specialized coding systems\n\n\n## Academic and Reasoning Capabilities\n\n* 65% on graduate-level reasoning (GPQA-Diamond)\n* 78% on undergraduate-level knowledge (MMLU Pro)\n* 78.3% on mathematical problem-solving (MATH)\n\n\n## Business Applications\n\n* 69.2% on retail domain tasks (TAU-bench)\n* 46% on airline domain tasks\n* 90.8% accuracy on chart analysis\n* 94.2% accuracy on document Q&A\n\n\n## Enterprise Integration and Availability\n\nClaude 3.5 Sonnet is accessible through multiple platforms:\n\n* Anthropic API\n* Amazon Bedrock\n* Google Cloud’s Vertex AI\n\nMajor companies including Asana, Canva, DoorDash, and Replit have already begun implementing Claude 3.5 Sonnet’s capabilities in their workflows, particularly leveraging its computer control features for complex automation tasks.\n\n\n## Practical Applications\n\n\n## Software Development\n\n* Automated code testing and debugging\n* Intelligent IDE interactions\n* Code review and optimization\n* Documentation generation\n\n\n## Customer Support\n\n* Advanced chatbot capabilities\n* Visual data interpretation\n* Automated ticket resolution\n* Process automation\n\n\n## Business Operations\n\n* Document processing and analysis\n* Data extraction from visual sources\n* Workflow automation\n* Complex problem-solving\n\n\n## Safety and Responsibility\n\nAnthropic has implemented robust safety measures for the computer control feature:\n\n* New classifiers to identify potential misuse\n* Proactive monitoring systems\n* Restricted access to sensitive operations\n* Regular safety assessments\n\n\n## Looking Ahead\n\nWhile Claude 3.5 Sonnet represents a significant advancement in AI capabilities, it’s important to note that some features, particularly computer control, are still in their early stages. Certain actions like scrolling, dragging, and zooming present challenges, and Anthropic encourages developers to begin with low-risk tasks while exploring these new capabilities.\n\nThe release of Claude 3.5 Sonnet marks a pivotal moment in AI development, combining advanced reasoning capabilities with practical computer control features. As the technology continues to evolve, we can expect to see even more innovative applications and improvements in how AI systems interact with our digital world.\n\n*This article is based on official announcements and documentation from Anthropic, AWS, and various technology partners. For the most up-to-date information, please refer to Anthropic’s official documentation.*\n\n\n"},{"lang":"en","group":"blog","slug":"blog/claude-3-5-sonnet-v-s-gpt-4o-which-one-is-better-3b3675195bf9","frontmatter":{"title":"Claude 3.5 Sonnet V/S GPT-4O: Which one is better","meta_title":"Claude 3.5 Sonnet V/S GPT-4O: Which one is better","description":"In November 2022, OpenAI launched ChatGPT, a model that has revolutionized how we search and interact with information. Next year, in…","date":"2024-10-27T13:59:09.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*4MXLuSFfGwFkWWn0","categories":["Generative AI","Machine Learning","Natural Language Processing"],"author":"Rifx.Online","tags":["GPT-4o","Claude","multimodal","reasoning","code-generation"],"draft":false,"slug":"blog/claude-3-5-sonnet-v-s-gpt-4o-which-one-is-better-3b3675195bf9"},"content":"\n\n\n\nIn November 2022, OpenAI launched ChatGPT, a model that has revolutionized how we search and interact with information. Next year, in March, an American startup,” Anthropic,” founded by ex-OpenAI employees, launched their own AI model, “Claude.” Since the launch, both AI companies have been competing to bring the best to customers regarding features and experience through their AI models. Recently, OpenAI launched “GPT-4o,” a spectacular model that handles file, voice, and video data amazingly. Similarly, Claude launched the “Claude 3.5 Sonnet,” which is the most advanced AI model, as they claimed, and can handle complex tasks. In this article, we will determine which is better, between Claude 3.5 Sonnet and GPT-4o, and compare its features and output with the same input to check which is better for you.\n\n\n## Capabilities and Features\n\n\n### GPT-4o\n\n\n\nGPT-4o is the latest LLM launched by OpenAI. The “o” stands for omni, which means “every” in Latin. This model can analyze voice, images, videos, and files as input and respond accordingly. It can take voice input and give the output in different characters’ voices, including tones, emotions, etc. The whole process is as low as a human conversation, with an average of 0.32 seconds compared to other voice models, which is 2.8 seconds. It also allows users to generate written content such as articles, blogs, product descriptions, code in different programming languages, data analysis, charts, etc. In addition, GPT-4o can also analyze images and videos, which makes the model act as a language translator, personal assistant, virtual teacher, or shopping assistant. It can also be used in medicine, engineering, the military, etc. To use this feature, GPT-4o can use the user’s camera to get a real-time view and respond accordingly in the voice mode. It can also access your computer screen and describe what is shown on the screen, users can ask questions related to the stuff displayed on the screen.\n\n*For example, users can enable the model on the screen, open the VS code, and prompt the model to act as a coding assistant to get answers to the coding problems. Alternatively, you can enable the camera to act as a fitness trainer whether you are doing it correctly or not.*\n\nThe model has unique features, such as data analysis, code interpreter, and real-time web browsing, making it different from its competitors. The model also has a plethora of GPTs, which is a tailored version of ChatGPT.\n\n\n### Claude 3.5 Sonnet\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*BSMcOpvWZ5lUm4Tl)\n\nClaude 3.5 Sonnet is the AI chatbot launched by Anthropic. It is the third generation of the family of Claude AI model series. This model has stood at a high bae and outperformed many AI models on various evaluations, keeping the hallucinations and wrong information away. While it doesn’t support voice and video features like GPT-4o, it can also perform all the basic tasks, such as text generation and code generation in different programming languages, brainstorming ideas, etc. According to the report by Anthropic, Claude 3.5 Sonnet is one of the best computer vision models in the market, which can be used to analyze charts and graphs, transcribe texts from images, and many more. Claude is powered by an advanced feature, “Artifacts,” a special popup window along the conversation, allowing the users to check the code snippets, text documents, or website designs and allow them to edit the output in real-time.\n\n*For example, users can use computer vision and artifacts in their workflows. Users can make essential prototyping of a website’s design on paper, attach the file with Claude 3.5 Sonnet, and prompt it to design a website based on the prototype. The generated code and the website design appear in the artifacts. Users can edit the code and the design according to their requirements. Users can also publish their projects live on the Internet.*\n\n\n## Head-to-Head Comparison\n\nIn this section, we will compare the two LLMs based on factors such as complex reasoning and code generation, check out their capabilities in handling complex tasks, and see which model is best.\n\n* **Graduate Level Reasoning(GPQA, Diamond)**This factor evaluates the models’ ability to handle complex, high-level reasoning tasks at a graduate level of education. In this task, researchers compare the model on the GPQA test, a set of 448 questions in different fields designed by experts. These questions are Google Proof, so anyone can’t find them online. The Claude score is nearly 59.4%, while the GPT-4o scores only 53.6%. Both the scores are relatively close, but as we can see, Claude could be a better option in tasks that require advanced analytical thinking, such as research analysis, complex problem solving, and high academic level problems.\n* **Undergraduate level knowlege(MMLU)**The MMLU, which means Massive Multitask Language Understanding, is a benchmark that explains the general knowledge understanding of any AI model across various subjects at an undergraduate level. Claude 3.5 Sonnet scores 88.3% in this experiment, and the GPT-4o scores 88.7%. This shows how both LLMs have trained in various domains and have a deeper understanding of them. It makes the AI model a well-suited tool for general knowledge tasks, basic tutoring of multiple subjects, etc.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*A4w-tvsxcmFINaQT)\n\n* **Code(HumanEval)**HumanEval is a benchmark that evaluates the model’s ability to generate, understand, and debug code. This benchmark is where Claude 3.5 Sonnet achieves 92%, and GPT-4o scores 90.2%. Claude 3.5 Sonnet results are spectacular in this task as it provides a better coding environment, “Artifacts,” and better code generation than GPT-4o. Claude allows the users to design, edit, and run the code in the Artifacts pop-up window. After the launch of Claude 3.5 Sonnet, everyone is developing tools, websites, and basic games and sharing them across the internet. On the other hand, GPT-4o also scored well, but it does not have any coding environment in its interface, so the developers must do too much hassle as the code generated by it is too much hassle to get to the result.\n* **Reasoning Over Text(DROP, FLscore)**The DROP(Discrete Reasoning Over Paragraphs) is the benchmark that measures the model’s ability to understand complex text information. In this challenge, the Claude 3.5 Sonnet scores 87.1%, while the GPT-4o scores 83.4%. This shows that the Claude 3.5 Sonnet is better and more effective for the task, which involves detailed text analysis, text review, complex question-answering systems, etc.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Kcy7sFb2FYpbfrwp)\n\n* **Math problem solving(MATH)**This test evaluates the ability of any AI model to solve various mathematical problems. Claude 3.5 Sonnet scores just 71.1%, while the GPT-4o scores 76.6%. These scores make the GPT-4o a better model for mathematical problem-solving tasks and can be used for mathematical computations such as financial modeling, scientific calculations, and advanced data analysis.\n* **Multilingual Maths (MSGM)**This factor describes the ability of any AI model to solve mathematical problems in multiple languages. Both models get scores close to each other: GPT-4o 90.5% and Claude 3.5 Sonnet 91.6%. This shows that both models perform excellently, with Claude slightly better. The capability is particularly helpful for educational applications or any scenario where mathematical reasoning needs to be communicated across language barriers.\n* **Visual question answering(MMU/val)**This factor describes the LLM’s capability to analyze the information presented in images. The GPT-4o outperforms Claude’s 3.5 Sonnet in this benchmark with 69.1% and 68.3%, respectively. On the other hand, when analyzing text from the document, Claude’s 3.5 Sonnet score is 95.2% compared to GPT-4o’s 92.1%.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*xzjqBV2YL0lVFitX)\n\n* **Image Generation**Image Generation is the ability of the LLMs to generate images from the text. GPT-4o is integrated with DallE-2 and can produce images with the help of text, and the results are excellent. On the other hand, Claude 3.5 Sonnet cannot create any images. This feature also helps GPT-4o design websites and references better, as it is trained on many images.\n* **Knowledge Cutoff**Here, both the models trained on a limited data set till a specific date. Claude 3.5 Sonnet trained on data till April 2024, while the other hand, GPT-4o trained on data till 2024. The real advantage of GPT-4o is that it has real-time web browsing, which helps the LLM train on new data regularly.\n\n\n## Pros of GPT-4o:\n\n* Handles voice, images, and video input.\n* Real-time web browsing capability.\n* Faster response time (0.32 seconds average).\n* Superior in math problem-solving.\n* Can generate images using DALL-E 2.\n\n\n## Cons of GPT-4o:\n\n* Slightly lower performance in graduate-level reasoning.\n* No built-in coding environment.\n* A lower score in document visual Q&A.\n* Slightly behind in code generation capabilities.\n* Less effective in detailed text analysis.\n\n\n## Pros Claude 3.5 Sonnet:\n\n* Excels in graduate-level reasoning.\n* Superior code generation and built-in “Artifacts” feature.\n* Better performance in detailed text analysis.\n* A higher score in document visual Q&A.\n* Slightly better in multilingual math.\n\n\n## Cons Claude 3.5 Sonnet:\n\n* Cannot handle voice or video input.\n* No image generation capability.\n* Slightly lower performance in visual question-answering.\n* Cannot access real-time web information.\n* Weaker in math problem-solving.\n\n\n## Conclusion\n\nGPT-4o and Claude 3.5 Sonnet demonstrate impressive capabilities across various tasks, each with its strengths. GPT-4o excels in multimodal inputs, real-time information access, and image generation, making it versatile for diverse applications. Claude 3.5 Sonnet shines in complex reasoning, code generation, and detailed text analysis, offering superior performance in specific academic and professional contexts. The choice between these models depends on the specific use case and required features. We can expect further improvements and specialized models catering to different needs as AI technology advances.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/explore-swarm-multi-agent-framework-locally-0e25ee617795","frontmatter":{"title":"Explore Swarm Multi-Agent Framework Locally","meta_title":"Explore Swarm Multi-Agent Framework Locally","description":"Swarm is an experimental sample framework to simulate lightweight multi-agent framework for educational purpose. Usually it works with Open…","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*0ZVceq32bvkytC7HSIgmwA.png","categories":["Programming","Technology","Education"],"author":"Rifx.Online","tags":["Swarm","Multi-Agent","Framework","OpenAI","Ollama"],"draft":false,"slug":"blog/explore-swarm-multi-agent-framework-locally-0e25ee617795"},"content":"\n\n\n\n\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*zkpW8DDwh0TTYuHJVJbDaw.png)\n\nSwarm is an experimental sample framework to simulate lightweight multi-agent framework for educational purpose. Usually it works with Open AI Key but we can change it to use local Ollama or LM Studio Models.\n\n**Setup:**\n\n\n```python\n## Create a new Conda or Python Virtual Environment and activate it\nconda install python==3.10\npip install torch openai\npip install transformers accelerate huggingface_hub\npip install git+ssh://git@github.com/openai/swarm.git\n```\n**To use with Open AI Key:**\n\n\n```python\nexport OPEN_API_KEY = Your Key\n```\n**To use Ollama or LM Studio Local LLMs — Update to Local URL:**\n\n\n```python\n## Find the location site-packages/swarm on the conda or python virtual env\n## Locate the file core.py\nclass Swarm:\n    def __init__(self, client=None):\n        if not client:\n          # Actual Code\n          #client = OpenAI()\n          # Update the Base URL and API Key to Ollama / LM Studio\n          # In this demo we are using LM Studio and Llama 3.1\n          client = OpenAI(base_url=\"http://localhost:1234/v1\",api_key=\"random\")\n        self.client = client\n```\n**Clone Repo:**\n\nClone the Repo — where you can find examples directory with different use cases like basic, airline and weather etc.\n\n\n```python\ngit clone https://github.com/openai/swarm.git\ncd swarm/examples\n```\n**Sample Code:**\n\n\n```python\nfrom swarm import Swarm, Agent\n\nclient = Swarm()\n\n\nit_agent = Agent(\n    name=\"IT Agent\",\n    instructions=\"You are an IT Expert with 10 Years of Experience.\",\n)\n\nsales_agent = Agent(\n    name=\"Sales Agent\",\n    instructions=\"You are a Sales Expert with 5 Years of Experience and knows about best selling mobiles.\",\n)\n\ndef transfer_to_sales_agent():\n    print(\"Sales agent in action\")\n    \"\"\"Transfer sales related questions to sales team immediately.\"\"\"\n    return sales_agent\n\ndef transfer_to_it_agent():\n    print(\"IT agent in action\")\n    \"\"\"Transfer IT users immediately.\"\"\"\n    return it_agent\n\nenglish_agent = Agent(\n    name=\"English Agent\",\n    instructions=\"You only speak English.\",\n    functions=[transfer_to_sales_agent,transfer_to_it_agent],\n)\n\n\nmessages = [{\"role\": \"user\", \"content\": \"How to install pandas lib?\"}]\nresponse = client.run(agent=english_agent, messages=messages)\n\nprint(response.messages[-1][\"content\"])\n\nmessages = [{\"role\": \"user\", \"content\": \"What are the best selling items?\"}]\nresponse = client.run(agent=english_agent, messages=messages)\n\nprint(response.messages[-1][\"content\"])\n```\n**References:**\n\n\n```python\nhttps://github.com/openai/swarm\n\nhttps://github.com/victorb/ollama-swarm/tree/main\n```\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*hCFJ4VQoT12yElYPXwXvWA.png)\n\nGiven that it is an experimental release, there is still much room for improvement. The airline agent example code [swarm/examples/airline] was interesting, so try those examples. Give it a try and share your experience in the comments. Thanks.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/fine-tuning-llama-3-with-unsloth-79c3465ef3e3","frontmatter":{"title":"Fine-tuning LLama 3 with Unsloth","meta_title":"Fine-tuning LLama 3 with Unsloth","description":"In this article I will show you how to fine-tune an LLM (Llama 3 from Meta) using Unsloth (including a way for custom dataset)","date":"2024-10-30T12:58:41.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kaXoudNTGeGfuNPl_kta5g.jpeg","categories":["Programming","Machine Learning","Natural Language Processing"],"author":"Rifx.Online","tags":["Llama","Unsloth","LoRA","Alpaca","NVIDIA"],"draft":false,"slug":"blog/fine-tuning-llama-3-with-unsloth-79c3465ef3e3"},"content":"\n\n\nIn this article I will show you how to fine\\-tune an LLM (Llama 3 from Meta) using [Unsloth](https://github.com/unslothai/unsloth). I will also provide a way to use your own custom dataset.\n\n**Note :** Unsloth is library that accelerates fine\\-tuning of LLMs on NVIDIA GPUs (40% reduction in memory usage compared to traditional methods). Compatible with Hugging Face, it supports Llama and Mistral architectures.\n\nIf you find my articles interesting, don’t forget to **clap and [follow](https://medium.com/@soulawalid)** 👍🏼, these articles take times and effort to do!\n\nYou can access to the free notebook provided for that on the GitHub repo\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*_L4o4MDQ7W5__OwW0E5RWA.png)\n\nSince I am using Llama 3, I will click on the notebook (you can install Unsloth on your own computer too).\n\n**Note:** I will use this dataset “[alpaca\\-cleaned](https://huggingface.co/datasets/yahma/alpaca-cleaned)” from Hugging Face , the data is in Alpaca format meaning there is (Instruction, Input and Output)\n\n### Starting the project\n\nDuring the project I will guide you to perform fine\\-tuning with Unsloth, explaining the code and provide recommendations, Let’s start our project :\n\n**1/ Installing required packages :** We need first to install **Unsloth** and **xformers**, **trl**, **peft**, **accelerate**, **bitsandbytes** libraries for efficient model training and inference.\n\n```python\n!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n!pip install --no-deps xformers trl peft accelerate bitsandbytes\n```\n\n**2/ Loading and Configuring the Model :** In the configuration I will set the following :\n\n* Sets the maximum sequence length to **2048**\n* by having dtype as **None**, it automatically detects the data type.\n* Loads the model in **4\\-bit precision,** I think it’s enough.\n\n**Note :** You can find my article about tips on fine\\-tuning LLMs in the Resources section\n\n```python\nfrom unsloth import FastLanguageModel\nimport torch\n\n## Configuration\nmax_seq_length = 2048\ndtype = None\nload_in_4bit = True\n\n## Load the selected model\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/llama-3-8b-bnb-4bit\",\n    max_seq_length=max_seq_length,\n    dtype=dtype,\n    load_in_4bit=load_in_4bit,\n)\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*cJSAcJFP7E-qJkqKUsHqLw.png)\n\n**3/ Applying PEFT (Parameter Efficient Fine\\-Tuning) :** We will then fine\\-tunes the pre\\-trained model using LoRA.\n\n* r \\= 16 is the rank parameter for LoRA. **Note :** common values are 8, 16, 32, 64, 128\n* lora\\_alpha \\= 16 represents the scaling factor for LoRA updates ( I will write an article about LoRA to explain in details each part of it)\n* No dropout and bias for LoRA\n* For use\\_gradient\\_checkpointing we are using Unsloth to handle that (saving memory)\n\n```python\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 16,\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha = 16,\n    lora_dropout = 0,\n    bias = \"none\",\n    use_gradient_checkpointing = \"unsloth\",\n    random_state = 3407,\n    use_rslora = False,\n    loftq_config = None,\n)\n```\n\n**4/ Defining the Prompt Template :** We will create alpaca prompt template to format the dataset ( In case the data that you will be using is not in that format).\n\nWe will also add EOS (End Of Sequence) to inform the LLM that the sentence has ended.\n\nFinally the formatting function, the function takes a batch of examples and formats each one according to the alpaca prompt template that we write before.\n\n* It extracts instruction, input, and output fields from each example (row).\n* It then formats these fields into the template and appends the EOS token.\n* The formatted text is stored in a list and returned as a dictionary with a single key, “text”\n\n```python\nalpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n#### Instruction:\n{}\n\n#### Input:\n{}\n\n#### Response:\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token\n\ndef formatting_prompts_func(examples):\n    instructions = examples[\"instruction\"]\n    inputs = examples[\"input\"]\n    outputs = examples[\"output\"]\n    texts = []\n    for instruction, input, output in zip(instructions, inputs, outputs):\n        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n        texts.append(text)\n    return {\"text\": texts}\n```\n\n**5/ Loading and Formatting the Dataset:** Loads the Alpaca dataset and applies formatting to each dataset example in batches.\n\n```python\nfrom datasets import load_dataset\ndataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\ndataset = dataset.map(formatting_prompts_func, batched = True)\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*M8EmbLMdoqrM-JlkMpDv8g.png)\n\n**6/ Setting Up and Training the Model:** I covered most of them in my [previous article](https://readmedium.com/supervised-fine-tuning-tips-for-your-llm-projects-f84f20593653) regarding tips for Fine\\-Tuning.\n\n```python\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2, # Number of processes to use for data preprocessing\n    packing = False, # Whether to pack multiple sequences into one batch to increase training efficiency\n    args = TrainingArguments(\n        per_device_train_batch_size = 2, #The batch size per device\n        gradient_accumulation_steps = 4, #Number of gradient accumulation steps, which allows for effectively larger batch sizes\n        warmup_steps = 5, #Number of steps to perform linear learning rate warmup\n        max_steps = 60, #Total number of training steps\n        learning_rate = 2e-5,#The learning rate for the optimizer\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"cosine\",\n        seed = 3407,\n        output_dir = \"outputs\",\n    ),\n)\n\ntrainer_stats = trainer.train()\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Vb_OqGP9CPc8xZdnkclGyQ.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*PI0JXrTbpjuviyQ4bZJnFg.png)\n\n**7/ Inference and Generation :** we prepare the model for inference by preparing the input prompt, tokenizing it , and then uses the model to generate new text based on that prompt. The generated text is then converted back into readable form.\n\n```python\nFastLanguageModel.for_inference(model)\ninputs = tokenizer(\n[\n    alpaca_prompt.format(\n        \"Continue the fibonnaci sequence.\", # instruction\n        \"1, 1, 2, 3, 5, 8\", # input\n        \"\", # output - leave this blank for generation!\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*PI6SBL_YPPj0-RSAn5nl7g.png)\n\nYou can also use a TextStreamer for continuous inference , so you can see the generation token by token, instead of waiting the whole time!\n\n```python\nFastLanguageModel.for_inference(model)\ninputs = tokenizer(\n[\n    alpaca_prompt.format(\n        \"Continue the fibonnaci sequence.\",\n        \"1, 1, 2, 3, 5, 8\",\n        \"\",\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*NaSQ1vQKORU1I3DsOU2iOA.png)\n\n**8/ Save the model :** If you are happy with it, you can save your model or push it to Hugging Face Hub\n\n```python\nmodel.save_pretrained(\"lora_model\")\ntokenizer.save_pretrained(\"lora_model\")\n## model.push_to_hub(\"your_name/lora_model\", token = \"...\")\n## tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\")\n```\n\n**9/ Load the model :**\n\n```python\nif False:\n    from unsloth import FastLanguageModel\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name = \"lora_model\",\n        max_seq_length = max_seq_length,\n        dtype = dtype,\n        load_in_4bit = load_in_4bit,\n    )\n    FastLanguageModel.for_inference(model)\n```\n\n**10/ Using it for generation :**\n\n```python\ninputs = tokenizer(\n[\n    alpaca_prompt.format(\n        \"What is the capital of Palestine ?\",\n        \"\",\n        \"\",\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)\n```\n\nIf there’s a specific subject you’d like us to cover, please don’t hesitate to let me know! Your input will help shape the direction of my content and ensure it remains relevant and engaging 😀\n\n\n"},{"lang":"en","group":"blog","slug":"blog/generating-structured-data-from-an-image-with-gpt-vision-and-langchain-34aaf3dcb215","frontmatter":{"title":"Generating structured data from an image with GPT vision and Langchain","meta_title":"Generating structured data from an image with GPT vision and Langchain","description":"In today’s world, where visual data is abundant, the ability to extract meaningful information from images is becoming increasingly…","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*FPRRg85jYb7MrzXEpNWbmw.jpeg","categories":["Programming","Computer Vision","Natural Language Processing"],"author":"Rifx.Online","tags":["Langchain","GPT","vision","LLMs","structured"],"draft":false,"slug":"blog/generating-structured-data-from-an-image-with-gpt-vision-and-langchain-34aaf3dcb215"},"content":"\n\n\n\n\n\nIn today’s world, where visual data is abundant, the ability to extract meaningful information from images is becoming increasingly valuable. Langchain, a powerful framework for building applications with large language models (LLMs), offers a versatile toolset for tackling this challenge. In this article, we’ll explore how to use Langchain to extract structured information from images, such as counting the number of people and listing the main objects.\n\nBefore diving into the code, let’s set the stage by understanding the task at hand. Imagine you have an image of a scene, such as a city street. Your goal is to extract valuable information from this image, including the number of people present and a list of the main objects in the scene.\n\n\n## About Langchain\n\nLangchain is a comprehensive framework that allows developers to build sophisticated applications by leveraging the power of large language models (LLMs). It provides a modular and extensible architecture, enabling developers to create custom pipelines, agents, and workflows tailored to their specific needs.\n\nLangchain simplifies the integration of LLMs, offering abstractions and utilities for handling various data sources, including text, images, and structured data. It supports a wide range of LLMs from different providers, such as OpenAI and Anthropic, making it easy to switch between models or combine multiple models in a single application.\n\n\n## Preparing the Environment and Setting Up the OpenAI API Key\n\nTo follow along with this tutorial, you’ll need to have Langchain installed. You can install it using pip:\n\n\n```python\npip install langchain langchain_openai\n```\nTo use the OpenAI language models with Langchain, you’ll need to obtain an API key from OpenAI. If you don’t have an API key yet, you can sign up for one on the OpenAI website (<https://openai.com/api/>).\n\nOnce you have your API key, you can set it as an environment variable in your system or provide it directly in your code. Here’s an example of how to set the API key as an environment variableCopy code\n\n\n```python\nexport OPENAI_API_KEY=\"your_openai_api_key_here\"\n```\nAlternatively, you can provide the API key directly in your Python code:\n\n\n```python\nimport os\nimport langchain\nos.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key_here\"\n```\nAfter setting up the API key, Langchain will be able to authenticate with the OpenAI API and use their language models.\n\n\n## Loading and Encoding the Image\n\nBefore we can process images with Langchain, we need to load the image data from a file and encode it in a format that can be passed to the language model. The code below defines a function `load_image` that takes a dictionary with an `image_path` key and returns a new dictionary with an `image` key containing the image data encoded as a base64 string.\n\n\n```python\ndef load_image(inputs: dict) -> dict:\n    \"\"\"Load image from file and encode it as base64.\"\"\"\n    image_path = inputs[\"image_path\"]\n  \n    def encode_image(image_path):\n        with open(image_path, \"rb\") as image_file:\n            return base64.b64encode(image_file.read()).decode('utf-8')\n    image_base64 = encode_image(image_path)\n    return {\"image\": image_base64}\n```\nThe `load_image` function first extracts the `image_path` from the input dictionary. It then defines a nested function `encode_image` that opens the image file in binary mode, reads its contents, and encodes them as a base64 string using the `base64.b64encode` function from the Python standard library.\n\nThe `load_image` function calls `encode_image` with the provided `image_path` and stores the resulting base64-encoded string in the `image_base64` variable. Finally, it returns a new dictionary with the `image` key set to `image_base64`.\n\nTo integrate this function into a Langchain pipeline, we can create a `TransformChain` that takes the `image_path` as input and produces the `image` (base64-encoded string) as outputCopy code\n\n\n```python\nload_image_chain = TransformChain(\n    input_variables=[\"image_path\"],\n    output_variables=[\"image\"],\n    transform=load_image\n)\n```\nWith this setup, we can easily load and encode images as part of a larger Langchain workflow, enabling us to process visual data alongside text using large language models.\n\n\n## Defining the Output Structure\n\nBefore we can extract information from the image, we need to define the structure of the output we want to receive. In this case, we’ll create a Pydantic model called `ImageInformation` that includes fields for the image description and any additional information we might want to extract.\n\n\n```python\nfrom langchain_core.pydantic_v1 import BaseModel, Field\n\nclass ImageInformation(BaseModel):\n \"\"\"Information about an image.\"\"\"\n image_description: str = Field(description=\"a short description of the image\")\n people_count: int = Field(description=\"number of humans on the picture\")\n main_objects: list[str] = Field(description=\"list of the main objects on the picture\")\n```\n\n## Setting up the Image Model\n\nNext, we’ll create a chain that combines the image loading and encoding steps with the LLM invocation step. Since the `ChatOpenAI` model is not natively capable of handling both text and image inputs simultaneously (to my unsderstanding), we'll create a wrapper chain to achieve this functionality.\n\n\n```python\nfrom langchain.chains import TransformChain\nfrom langchain_core.messages import HumanMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain import globals\nfrom langchain_core.runnables import chain\n\n## Set verbose\nglobals.set_debug(True)\n\n@chain\ndef image_model(inputs: dict) -> str | list[str] | dict:\n \"\"\"Invoke model with image and prompt.\"\"\"\n model = ChatOpenAI(temperature=0.5, model=\"gpt-4-vision-preview\", max_tokens=1024)\n msg = model.invoke(\n             [HumanMessage(\n             content=[\n             {\"type\": \"text\", \"text\": inputs[\"prompt\"]},\n             {\"type\": \"text\", \"text\": parser.get_format_instructions()},\n             {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{inputs['image']}\"}},\n             ])]\n             )\n return msg.content\n```\nIn this code snippet, we define a chain called `image_model` that invokes the `ChatOpenAI` model with the provided prompt, format instructions, and image. The `image_model` chain accepts a dictionary `inputs` containing the prompt and the base64-encoded image string.\n\nInside the chain, we create a `HumanMessage` object that combines the prompt text, format instructions, and the image URL, formatted as a data URI with the base64-encoded image data. We then invoke the `ChatOpenAI` model with this `HumanMessage` object, using the `gpt-4-vision-preview` model, which is specifically designed for multimodal tasks involving both text and images.\n\nThe model processes both the text prompt and the image, and returns the output.\n\n\n## Putting It All Together\n\nNow that we have all the necessary components, we can define a function that orchestrates the entire process:\n\n\n```python\nfrom langchain_core.output_parsers import JsonOutputParser\n\nparser = JsonOutputParser(pydantic_object=ImageInformation)\ndef get_image_informations(image_path: str) -> dict:\n   vision_prompt = \"\"\"\n   Given the image, provide the following information:\n   - A count of how many people are in the image\n   - A list of the main objects present in the image\n   - A description of the image\n   \"\"\"\n   vision_chain = load_image_chain | image_model | parser\n   return vision_chain.invoke({'image_path': f'{image_path}', \n                               'prompt': vision_prompt})\n```\nIn this function, we define a prompt that asks the LLM to provide a count of the people in the image and a list of the main objects. We then create a chain that combines the image loading step (`load\\_image\\_chain`), the LLM invocation step (`image\\_model`), and a JSON output parser (`parser`). Finally, we invoke this chain with the image path and the prompt, and the function returns a dictionary containing the extracted information.\n\n\n## Example Usage\n\nTo use this function, simply provide the path to an image file:\n\n\n```python\nresult = get_image_informations(\"path/to/your/image.jpg\")\nprint(result)\n```\nThis will output a dictionary with the requested information, such as:\n\n\n```python\n{\n 'description': 'a view of a city showing cars waiting at a traffic light',\n 'people_count': 5,\n 'main_objects': ['car', 'building', 'traffic light', 'tree']\n}\n```\n\n## Conclusion\n\nLangchain provides a powerful toolset for working with large language models and extracting valuable information from various data sources, including images. By combining Langchain’s capabilities with custom prompts and output parsing, you can create robust applications that can extract structured information from visual data.\n\nRemember, the quality of the output will depend on the capabilities of the LLM you’re using and the specificity of your prompts. Experiment with different models and prompts to find the best solution for your use case.\n\nIf you find a better way to achieve the same results or have suggestions for improvements, please don’t hesitate to share them in the comments. The code examples provided in this article are meant to serve as a starting point, and there may be alternative approaches or optimizations .\n\n\n"},{"lang":"en","group":"blog","slug":"blog/glm-4-voice-9b-real-time-multilingual-voice-conversation-ai-install-locally-in-minutes-ce2fcd6c8fd8","frontmatter":{"title":"GLM-4-Voice 9B — Real-time Multilingual Voice Conversation AI — Install Locally in Minutes","meta_title":"GLM-4-Voice 9B — Real-time Multilingual Voice Conversation AI — Install Locally in Minutes","description":"How to set up GLM-4-Voice 9B for seamless real-time voice interaction in English and Chinese, and explore its unique architecture…","date":"2024-11-04T12:29:59.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*LATTpEc2AHvqgVyPKSzW7A.jpeg","categories":["Voice Assistants","Natural Language Processing","Chatbots"],"author":"Rifx.Online","tags":["multilingual","conversation","real-time","customization","performance"],"draft":false,"slug":"blog/glm-4-voice-9b-real-time-multilingual-voice-conversation-ai-install-locally-in-minutes-ce2fcd6c8fd8"},"content":"\n### How to set up GLM\\-4\\-Voice 9B for seamless real\\-time voice interaction in English and Chinese, and explore its unique architecture, low\\-latency response, and customizable voice attributes.\n\n👨🏾‍💻 [GitHub](https://github.com/mdmonsurali) ⭐️ \\| 👔[LinkedIn](https://www.linkedin.com/in/mdmonsurali/) \\|📝 [Medium](https://medium.com/@monsuralirana)\n\n\n\n## Introduction\n\nIn recent years, voice\\-enabled AI has significantly advanced, enabling conversational agents to better understand and respond to human speech. From virtual assistants to customer service bots, voice AI has become an essential tool in various industries. However, most models still struggle with fluently transitioning between languages, understanding nuances in spoken queries, and delivering high\\-quality responses. This is where GLM\\-4\\-Voice by Zhipu AI shines. Developed as an end\\-to\\-end voice model, GLM\\-4\\-Voice pushes the boundaries of multilingual conversational AI by supporting real\\-time dialogue in both English and Chinese, while offering an adaptable and human\\-like response generation.\n\nIn this article, we’ll explore why GLM\\-4\\-Voice is worth paying attention to, what makes it unique, and how you can set it up and start using it locally. We’ll also take a look at its architecture and provide a hands\\-on guide to accessing the web demo.\n\n## Why GLM\\-4\\-Voice?\n\nTraditional language models are often limited to text and require additional processing layers to handle voice. They may also struggle with interactivity or suffer from latency issues. GLM\\-4\\-Voice overcomes these limitations with a unified model that directly processes and generates speech. Here’s what makes it stand out:\n\n1. **End\\-to\\-End Voice Processing**: Unlike many other models that rely on a separate text\\-to\\-speech (TTS) or speech\\-to\\-text (STT) module, GLM\\-4\\-Voice directly interprets and responds in spoken language, allowing a more seamless and responsive experience.\n2. **Multilingual Support**: This model excels in handling both English and Chinese, two widely used languages globally. Its ability to switch between languages fluidly makes it ideal for bilingual environments and international applications.\n3. **Customizable Attributes**: GLM\\-4\\-Voice allows for adjustments in emotion, intonation, speech rate, and even dialect, making it capable of producing more natural and contextually appropriate responses.\n4. **Low Latency**: With support for streaming inference, the model has a latency of around 20 tokens, which enables near\\-instantaneous responses in real\\-time conversations.\n\n## Specialties of GLM\\-4\\-Voice\n\nGLM\\-4\\-Voice brings several unique features to the table, setting it apart from other voice models. Here’s what makes it special:\n\n* **Real\\-Time Voice Interaction**: By supporting low\\-latency responses, GLM\\-4\\-Voice can maintain fluid and natural conversations, which is crucial for applications like customer support and interactive AI.\n* **Dynamic Voice Attributes**: Users can specify the model’s emotional tone, speaking rate, and other characteristics, making interactions more engaging and suited to various contexts.\n* **Bilingual Support with Context Awareness**: This model is designed to comprehend and generate responses in both Chinese and English. It can switch between these languages seamlessly, offering a flexible solution for multilingual applications.\n* **Advanced Speech Decoding**: Built on CosyVoice, the GLM\\-4\\-Voice decoder enables high\\-quality speech generation with streaming support, maintaining high clarity in both languages.\n\n## Architecture\n\nThe architecture of GLM\\-4\\-Voice consists of three primary components, each fulfilling a crucial role in achieving end\\-to\\-end voice interaction:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*nJsKHtxSblNkixPIBZpWyQ.jpeg)\n\n1. **GLM\\-4\\-Voice\\-Tokenizer**: This component tokenizes continuous speech input into discrete tokens, with around 12\\.5 tokens generated per second of audio. The tokenizer is based on Whisper’s encoder, with added vector quantization, allowing the model to process audio in a structured form.\n2. **GLM\\-4\\-Voice\\-9B**: The core language model, based on the GLM\\-4 architecture, has been aligned to process spoken input. It can handle both text and speech, making it a powerful multimodal conversational agent.\n3. **GLM\\-4\\-Voice\\-Decoder**: This decoder converts the discrete tokens back into continuous speech, allowing the model to produce audio outputs. It supports streaming inference, enabling responses to begin after processing just a few tokens, minimizing conversation latency.\n\nTogether, these components make GLM\\-4\\-Voice a powerful tool for real\\-time voice interactions, supporting conversational AI across different languages and dialects.\n\n## Setting Up GLM\\-4\\-Voice Locally\n\nTo experience GLM\\-4\\-Voice, follow these steps to set up the model locally on your machine.\n\n### Step 1: Clone the Repository\n\nBegin by cloning the repository from GitHub. Make sure to include submodules:\n\n```python\n!git clone --recurse-submodules https://github.com/THUDM/GLM-4-Voice\ncd GLM-4-Voice\n```\n\n### Step 2: Install Dependencies\n\nNavigate into the project directory and install the necessary dependencies:\n\n```python\n!pip install -r requirements.txt\n```\n\n### Step 3: Download the Model Checkpoint\n\nGLM\\-4\\-Voice’s decoder model is hosted on Hugging Face and requires `git-lfs` to download. Make sure `git-lfs` is installed, then run:\n\n```python\n!git clone https://huggingface.co/THUDM/glm-4-voice\n```\n\n### Step 4: Launch the Model Service\n\nWith everything set up, start the model server:\n\n```python\npython model_server.py --model-path glm-4-voice-9b\n```\n\n### Step 5: Start the Web Service\n\nOnce the model server is running, start the web service by executing:\n\n```python\npython web_demo.py\n```\n\nYou can now access the web demo at [http://127\\.0\\.0\\.1:8888](http://127.0.0.1:8888) to interact with GLM\\-4\\-Voice.\n\n> **Note:** The GLM\\-4\\-Voice model is resource\\-intensive and requires a substantial amount of computational power to run effectively. Specifically, it necessitates 35–40 GPUs for optimal performance, making it suitable for deployment in environments with access to high\\-performance hardware. Users should ensure they have the necessary infrastructure in place before attempting to utilize this model.\n\n## Web Demo Interface\n\nThe web demo for GLM\\-4\\-Voice provides an intuitive interface with several customization options:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*scbHOUXqMW5KGAcT3Bq1Eg.png)\n\n* **Input Mode**: Users can choose to provide input as either text or audio. This flexibility allows for hands\\-free or traditional interaction.\n* **Voice Control Parameters**: Adjust temperature, top\\-p, and token limits to customize the model’s response characteristics.\n* **Debug Information**: Input and output tokens are displayed, giving users insight into the model’s processing of their queries.\n* **Interactive Audio Display**: Audio inputs and responses are displayed as waveforms, and users can replay or review audio segments to assess quality.\n\nHowever, Gradio, which is used to stream audio in the demo, may sometimes present instability. For best quality, it’s recommended that audio from the dialogue box be replayed after it has been generated.\n\n## Conclusion\n\nGLM\\-4\\-Voice stands out as an impressive achievement in conversational AI, offering a unique blend of bilingual support, real\\-time audio interaction, and flexible response customization. Its end\\-to\\-end design and low latency make it a prime candidate for applications in customer service, education, virtual assistants, and more. With an accessible setup process, GLM\\-4\\-Voice opens the door for developers and researchers to explore advanced voice capabilities in both Chinese and English.\n\nAs the demand for more interactive and realistic AI continues to grow, models like GLM\\-4\\-Voice represent a significant step forward in bridging language and conversational barriers. Whether you’re looking to build a chatbot, a virtual teacher, or a customer service agent, GLM\\-4\\-Voice provides a robust and versatile solution.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/google-releases-gemma-a-lightweight-and-open-source-model-b6411d67ecca","frontmatter":{"title":"Google Releases Gemma — A Lightweight And Open Source Model","meta_title":"Google Releases Gemma — A Lightweight And Open Source Model","description":"Google released Gemma, a family of lightweight and open-source models built upon the research and technology used to create the Gemini…","date":"2024-10-29T12:46:34.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*G7XbkhsCwillpje7AvETjQ.jpeg","categories":["Natural Language Processing","Programming","Chatbots"],"author":"Rifx.Online","tags":["Gemma","Gemini","parameters","NLP","chatbots"],"draft":false,"slug":"blog/google-releases-gemma-a-lightweight-and-open-source-model-b6411d67ecca"},"content":"\n\n\n\n\n\nIn just a week, the world has witnessed the most groundbreaking AI advancements from two tech giants. OpenAI introduced its jaw\\-dropping AI video generator, [Sora](https://readmedium.com/3d16381f3bf5), while Google unveiled its [Gemini 1\\.5 model](https://generativeai.pub/google-releases-gemini-1-5-with-1m-context-window-44ed4a2ea319), capable of supporting up to a 1 million token context window.\n\nToday, Google dropped another bombshell with the release of [Gemma](https://ai.google.dev/gemma/?utm_source=keyword&utm_medium=referral&utm_campaign=gemma_cta&utm_content), a family of lightweight, state\\-of\\-the\\-art open\\-source models built upon the research and technology used to create the Gemini models.\n\n\n## What is Gemma?\n\nNamed after the Latin word *gemma* for “precious stone,” Gemma draws inspiration from its predecessor, Gemini, reflecting its value and rarity in the tech world.\n\nThey are text\\-to\\-text, decoder\\-only large language models, available in English, with open weights, pre\\-trained variants, and instruction\\-tuned variants.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Fu2ryJMunebq5c0dD-opZQ.png)\n\nGemma is available worldwide starting today in two sizes (2B and 7B), supports a wide range of tools and systems, and runs on a developer laptop and workstation.\n\n\n## 2 model sizes and capabilities\n\nGemma models are available in 2 billion and 7 billion parameter sizes. The 2B model is intended to run on mobile devices and laptops, while the 7B model is intended to run on desktop computers and small servers.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*sH9jaz1RvtKeJ5yjfyOL5Q.png)\n\n**Tuned models**\n\nGemma also comes in two versions: tuned and pretrained.\n\n* **Pretrained:** This is like the base model without any fine tuning. This model is not trained on any specific tasks or instructions beyond the Gemma core data training set.\n* **Instruction\\-tuned:** This model is fine\\-tuned to human language interactions, which improves its ability to perform targeted tasks.\n\n\n## How it compares with the competition?\n\nBecause of its small size, Gemma is capable of running directly on a user’s laptop. The chart below shows how the language understanding and generation performance of Gemma (7B) compares to similarly sized open models like LLaMA 2 (7B), LLaMA 2 (13B), and Mistral (7B).\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*QxjZALUAIDiS_T66EpOu-g.png)\n\nYou can check out a more detailed comparison for each benchmark [here](https://ai.google.dev/gemma/?utm_source=keyword&utm_medium=referral&utm_campaign=gemma_cta&utm_content).\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Fc8Fk0Dgh2VFU_VLhpcs6Q.png)\n\n\n## What is it for?\n\nHere are some possible use cases that Gemma can be used for:\n\n**Content Creation and Communication**\n\n* Text Generation\n* Chatbots and Conversational AI\n* Text Summarization\n\n**Research and Education**\n\n* **Natural Language Processing (NLP) Research:** Serving as a foundation for NLP research, experimenting with techniques, developing algorithms, and contributing to the field’s advancement.\n* **Language Learning Tools:** supporting interactive language learning experiences, aiding in grammar correction, or providing writing practice.\n* **Knowledge Exploration:** Assisting researchers in exploring large bodies of text by generating summaries or answering questions about specific topics.\n\nTasks that previously required extremely large models are now possible with state\\-of\\-the\\-art, smaller models. This unlocks completely new ways of developing AI applications, and we could soon see in\\-device AI chatbots on our smartphones—no internet connection needed.\n\nHow exciting is that?\n\n\n## Is it good, though?\n\nSeveral [redditors](https://www.reddit.com/r/LocalLLaMA/comments/1awbqwd/gemma_7b_the_latest_opensource_model_from_google/) have shared their experience using Gemma, and so far, it’s not looking good. Take a look at this example where Gemma is giving incorrect answers when asked about weight questions.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Sdaiaqcuz7qbftG1)\n\nI haven’t really tried it myself, but it’s important to remember that smaller models like this are expected to have some flaws and might give incorrect answers sometimes.\n\n\n## Try it yourself\n\nYou can start working with Gemma today using free access to Kaggle, a free tier for Colab notebooks, and $300 in credits for first\\-time Google Cloud users.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BrvLnczy724TPrsk-uFJCw.png)\n\nIf you are interested in getting started with Gemma, check out these guides to learn from text generation up to deployment in Gemma mode:\n\n* **Text generation with Gemma**: Build a basic text generation example with the model.\n* **Tune Gemma with LoRA tuning:** Perform LoRA fine\\-tuning on a Gemma 2B model.\n* **Tune a Gemma model using distributed training:** Use Keras with a JAX backend to fine\\-tune a Gemma 7B model with LoRA and model parallelism.\n* **Deploy Gemma to production:** Use Vertex AI to deploy Gemma to production.\n\n\n## Download the model\n\nThe open models are currently available on [HuggingFace](https://huggingface.co/models?other=gemma&sort=trending&search=google).\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*mJRzGhO1sUxPL4_3YjpNGA.png)\n\nThe Gemma models can also be downloaded from [Kaggle Models](https://www.kaggle.com/models/google/gemma).\n\n\n## Final Thoughts\n\nWhile Gemma models may be small and lack complications, they may make up for it in speed and cost of use.\n\nLooking at the bigger picture, instead of chasing immediate consumer excitement, Google is cultivating a market for businesses. They envision companies paying for Google Cloud services as developers use Gemma to create innovative new consumer applications.\n\nAlso, despite the underwhelming reception of Gemini, Google is still showing that it has a lot more tricks under its sleeve.\n\nOf course, with any powerful technology, the true test is how well it works. Google’s past raises the question of whether these models will perform as well as they promise in the real world. It’s important to keep a careful eye on this, but also to hope that Google learns from the past and delivers models that are truly comparable or even better than the competition.\n\nI can’t wait to get my hands on Gemma, and I will definitely share my initial thoughts and findings about this new AI model.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*8BDnUV9iQisOyeN3.png)\n\nThis story is published on [Generative AI](https://generativeai.pub/). Connect with us on [LinkedIn](https://www.linkedin.com/company/generative-ai-publication) and follow [Zeniteq](https://www.zeniteq.com/) to stay in the loop with the latest AI stories. Let’s shape the future of AI together!\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*JeeoUhaBYUJGr0Xq.png)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/how-agentic-rag-solves-problem-with-current-rag-limitations-4402ef7f8448","frontmatter":{"title":"How Agentic RAG solves problem with current RAG limitations","meta_title":"How Agentic RAG solves problem with current RAG limitations","description":"In this volume 4 of coffee break concept, we will understand how AgenticRAG helps solve limitations of traditional RAG.","date":"2024-11-04T12:34:57.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*abCDtDjfKZDJzginIc1UPA.png","categories":["Generative AI","Data Science","Machine Learning"],"author":"Rifx.Online","tags":["Agentic","RAG","agents","query","routing"],"draft":false,"slug":"blog/how-agentic-rag-solves-problem-with-current-rag-limitations-4402ef7f8448"},"content":"\nIn this volume 4 of coffee break concept, we will understand how AgenticRAG helps solve limitations of traditional RAG.\n\n## RAG Framework\n\nThe RAG (Retrieval Augmented Generation) framework operates in a specific sequence:\n\nDocument \\-\\> Chunks\\-\\> Vector DB \\-\\> Chunk Retrieval (Top K) \\-\\> LLM\n\nHowever, this sequence **encounters obstacles when dealing with certain types of queries.**\n\n\n\n## Problem 1: Summarization\n\nConsider a query like “Summarize the document”.\n\n* The conventional RAG approach retrieves the top K chunks and summarizes them.\n* But wouldn’t it be more comprehensive if it retrieved all chunks of the document and summarized them?\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*gIb0RNALIItt4UmyVfPRZg.png)\n\n## Problem 2: Comparing Documents\n\n* When tasked with comparing Document A and Document B, the **basic RAG retrieves random chunks and attempts to compare these top K chunks**.\n* This **doesn’t paint an accurate picture** as it doesn’t represent the full scope of the documents.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*pJuKlKx1unDAvKmmp_1Rlg.png)\n\n## Problem 3: Structured Data Analysis\n\nConsider a question like “**When is the next leave?**”.\n\n* The first step is to retrieve the region to which the employee belongs from a structured table.\n* Based on the region, the next leave for that region is extracted from the leave policy document.\n* This process isn’t as straight forward with the current RAG framework.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*XZuMz9EXtb_m28l4Ox27lQ.png)\n\n## Problem 4: The Multi\\-part Question\n\nConsider a question like “**Identify common leave across all regions?**”.\n\n* Imagine you have a leave policy document of a company present in 120 countries.\n* Since you are passing the top K contexts, the **maximum number of regions that can be compared is limited to K**, where K is the number of chunks passed to LLM.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*l0FY6rI_UK9k9TW-nEJO7w.png)\n\nLook into our **AgenticRAG with LlamaIndex** Course with **5 real\\-time case studies**.\n\nCourse link: [https://www.masteringllm.com/course/agentic\\-retrieval\\-augmented\\-generation\\-agenticrag](https://www.masteringllm.com/course/agentic-retrieval-augmented-generation-agenticrag)\n\n## Agentic RAG\n\nAgentic RAG can solve this 4 problems by replacing via custom agents.\n\n* Agents will interact with multiple systems.\n* RAG is now one part of this system which agents can use.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Su8LiYNG4lv4jvuCQAhYdg.png)\n\n* Agents uses LLMs to automate the reasoning and tool selection\n* RAG is just another tool which Agent may decides to use.\n\n## Routing Agent\n\n* Routing agents are simple agents which routes the queries.\n* An agent can route query in one or multiple tools.\n* Remember our question “**Summarize the document**” or a question if we want to combine “**Summarization \\+ Sematic search**” can be solved using below example routing\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*43Y9jlYoXDb0BbUoYCcKrg.png)\n\n## Query Planning Agent\n\n* Query planning agent breaks down the queries into sub\\-queries.\n* Each of the sub\\-queries can be executed against RAG pipeline.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*32Ng2zpxNWXhQZ3CaLcFeA.png)\n\n## Tools For Agents\n\n* LLMs can have multiple tools like calling an API, infer parameters for API.\n* RAG is now a tool which LLM might use.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Z1viCXkfah_5JJM2Ty6Kjw.png)\n\n## Summary\n\n* RAG has limitations when represented with complex questions.\n* Few of the use cases like summarization, comparison etc. can’t be solve with just RAG.\n* Agentic RAG can help overcome limitation of RAG.\n* Agentic RAG treats RAG as a tool which it can use for semantic search.\n* Agents equipped with routing, query planning and tools can out perform traditional RAG applications.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/how-to-create-an-ai-team-to-write-compelling-stories-with-crewai-and-gemini-pro-3713f53c72c4","frontmatter":{"title":"How to create an AI team to write compelling stories with CrewAI and Gemini Pro","meta_title":"How to create an AI team to write compelling stories with CrewAI and Gemini Pro","description":"Are you fascinated by the idea of AI generating stories that capture the imagination? If so, you’re not alone! In this article, we’ll dive…","date":"2024-10-31T23:04:49.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*tSnoOxxIGtrwdUT8","categories":["Programming","Natural Language Processing","Generative AI"],"author":"Rifx.Online","tags":["CrewAI","Gemini","screenwriters","critics","storytelling"],"draft":false,"slug":"blog/how-to-create-an-ai-team-to-write-compelling-stories-with-crewai-and-gemini-pro-3713f53c72c4"},"content":"\n\n\nAre you fascinated by the idea of AI generating stories that capture the imagination? If so, you’re not alone! In this article, we’ll dive into an introductory project that combines the powers of CrewAI and Gemini Pro to create an agent network that crafts short stories with a little help from user input. Whether you’re a budding programmer, a storyteller looking to explore digital frontiers, or simply curious about the potential of artificial intelligence, this guide is for you.\n\n## What are CrewAI and Gemini Pro?\n\nBefore we jump into the nuts and bolts of building our AI storyteller, let’s clarify what CrewAI and Gemini Pro are.\n\n**CrewAI** is a fascinating framework designed to orchestrate multiple AI agents, each with its own unique skills and responsibilities, to collaborate on complex tasks. Think of it as a director managing a team of actors, where each actor plays a specific role to bring a story to life. In the context of our project, CrewAI enables us to create a team of specialized agents (like screenwriters, critics, and story masters) to work together on writing stories.\n\n**Gemini Pro**, on the other hand, is a state\\-of\\-the\\-art language model developed by Google. It’s known for its ability to understand and generate human\\-like text, making it an ideal candidate for creative tasks such as storytelling. By leveraging Gemini Pro, we can ensure our agents have a solid foundation for generating compelling narrative content.\n\n## Why is This Kind of Structure Important?\n\nThe combination of CrewAI and Gemini Pro enables a highly collaborative and specialized approach to story generation. This structure allows for:\n\n1. **Specialization**: Each agent can focus on what it does best, whether it’s crafting dialogue, ensuring consistency, or overseeing the project.\n2. **Collaboration**: Agents can work together, combining their strengths to produce a story that’s greater than the sum of its parts.\n3. **Flexibility**: The setup is highly adaptable, allowing for different story elements to be emphasized or altered based on user input or creative direction.\n\n## Setting Up the Environment\n\nFirst, we will need some libraries to use. You can load these libraries via pip:\n\n```python\npip install crewai\n```\n\n```python\npip install langchain-google-genai\n```\n\nAfter loading the necessary libraries we can start coding. We will start by importing our necessary modules and initialize our Gemini pro api connection.\n\nAs you may notice, we will need an API key for Gemini model. You can create this key in Google AI Studio for [free](https://ai.google.dev/). After that, you can copy this key into google\\_api\\_key variable or you can load it into environment by running this command in your command line:\n\n```python\nexport GOOGLE_API_KEY=YOUR_KEY\n```\n\nReplace the api key that you will get from google ai studio with YOUR\\_KEY.\n\nNext, we define our agents: the Screenwriter, Critic, and Story Master. Each agent is assigned a role, goal, and backstory to guide its contributions to the story generation process.\n\nFor example, the Screenwriter is focused on translating ideas into engaging scenes, while the Critic ensures consistency and adherence to genre.\n\nThese agents will work together and create an engaging story. The story master will accept the task, then it will delegate and coordinate tasks between other agents. We allow this behavior by setting allow\\_delegation parameter to True.\n\nWith our agents ready, we prompt the user for a story idea. This input is then used to create a task that outlines what the story should include, guiding the agents in their creative process.\n\nWhile creating the task, we submit the task to the story master since it will coordinate our story creation process.\n\nFinally, we should combine these agents into a crew and run our task.\n\nAnd thats it. When we run this code, it will prompt the user to give a story idea and then write a short story by agent cooperation. Of course, there is much more than this in the CrewAI framework such as tool usage, hierarchical processing, working with ollama to run agents fully locally with different agents etc, but these topics are for another article.\n\nYou can find the full code in here for directly run:\n\nYou can use this code as a template for these kinds of applications, you can build game builder crew, stock analyzer crew, marketing crew etc. With imagination, sky is the limit. If you like this article and excited about the more advanced implementations you can visit the CrewAI [website](https://www.crewai.com/).\n\n\n"},{"lang":"en","group":"blog","slug":"blog/how-to-improve-llms-with-rag-abdc132f76ac","frontmatter":{"title":"How to Improve LLMs with RAG","meta_title":"How to Improve LLMs with RAG","description":"A beginner-friendly introduction w/ Python code","date":"2024-11-04T12:31:55.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*N0Ad_oCIrAyzMYRdH3trqg.png","categories":["Natural Language Processing","Programming","Generative AI"],"author":"Rifx.Online","tags":["RAG","retrievers","LlamaIndex","knowledge","bases"],"draft":false,"slug":"blog/how-to-improve-llms-with-rag-abdc132f76ac"},"content":"\n\n\n\n\n### A beginner\\-friendly introduction w/ Python code\n\nThis article is part of a [larger series](https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c) on using large language models in practice. In the [previous post](https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32), we fine\\-tuned Mistral\\-7b\\-Instruct to respond to YouTube comments using QLoRA. Although the fine\\-tuned model successfully captured my style when responding to viewer feedback, its responses to technical questions didn’t match my explanations. Here, I’ll discuss how we can improve LLM performance using retrieval augmented generation (i.e. RAG).\n\n\n\nLarge language models (LLMs) have demonstrated an impressive ability to store and deploy vast knowledge in response to user queries. While this has enabled the creation of powerful AI systems like ChatGPT, compressing world knowledge in this way has **two key limitations**.\n\n**First**, an LLM’s knowledge is static, i.e., not updated as new information becomes available. **Second**, LLMs may have an insufficient “understanding” of niche and specialized information that was not prominent in their training data. These limitations can result in undesirable (and even fictional) model responses to user queries.\n\nOne way we can mitigate these limitations is to **augment a model via a specialized and mutable knowledge base**, e.g., customer FAQs, software documentation, or product catalogs. This enables the creation of more robust and adaptable AI systems.\n\n**Retrieval augmented generation**, or **RAG**, is one such approach. Here, I provide a high\\-level introduction to RAG and share example Python code for implementing a RAG system using LlamaIndex.\n\n\n## What is RAG?\n\nThe basic usage of an LLM consists of giving it a prompt and getting back a response.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*sM1p-3FoTaGZunqx918G9A.png)\n\n**RAG works by adding a step to this basic process**. Namely, a retrieval step is performed where, based on the user’s prompt, the relevant information is extracted from an external knowledge base and injected into the prompt before being passed to the LLM.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*EhJZj1blu7a8EPmVAPsNcA.png)\n\n\n## Why we care\n\nNotice that RAG does not fundamentally change how we use an LLM; it's still *prompt\\-in and response\\-out*. RAG simply augments this process (hence the name).\n\nThis makes **RAG a flexible and (relatively) straightforward way to improve LLM\\-based systems**. Additionally, since knowledge is stored in an external database, updating system knowledge is as simple as adding or removing records from a table.\n\n\n### Why not fine\\-tune?\n\nPrevious articles in this series discussed [fine\\-tuning](https://towardsdatascience.com/fine-tuning-large-language-models-llms-23473d763b91), which adapts an existing model for a particular use case. While this is an alternative way to endow an LLM with specialized knowledge, empirically, **fine\\-tuning seems to be less effective than RAG** **at doing this** \\[1].\n\n\n## How it works\n\nThere are 2 key elements of a RAG system: a **retriever** and a **knowledge base**.\n\n\n### Retriever\n\nA retriever takes a user prompt and returns relevant items from a knowledge base. This typically works using so\\-called **text embeddings**, numerical representations of text in concept space. In other words, these are **numbers that represent the *meaning* of a given text**.\n\nText embeddings can be used to compute a similarity score between the user’s query and each item in the knowledge base. The result of this process is a **ranking of each item’s relevance to the input query**.\n\nThe retriever can then take the top k (say k\\=3\\) most relevant items and inject them into the user prompt. This augmented prompt is then passed into the LLM for generation.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*jpTwdBmoTlJlfPAm0oJiVQ.png)\n\n\n### Knowledge Base\n\nThe next key element of a RAG system is a knowledge base. This **houses all the information you want to make available to the LLM**. While there are countless ways to construct a knowledge base for RAG, here I’ll focus on building one from a set of documents.\n\nThe process can be broken down into **4 key steps** \\[2,3].\n\n1. **Load docs** — This consists of gathering a collection of documents and ensuring they are in a ready\\-to\\-parse format (more on this later).\n2. **Chunk docs—**Since LLMs have limited context windows, documents must be split into smaller chunks **(e.g.,** 256 or 512 characters long).\n3. **Embed chunks** — Translate each chunk into numbers using a text embedding model.\n4. **Load into Vector DB**— Load text embeddings into a database (aka a vector database).\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*VWG6Tr0OxCnD5Mvygm5DCA.png)\n\n\n## Some Nuances\n\nWhile the steps for building a RAG system are conceptually simple, several nuances can make building one (in the real world) more complicated.\n\n**Document preparation**—The quality of a RAG system is driven by how well useful information can be extracted from source documents. For example, if a document is unformatted and full of images and tables, it will be more difficult to parse than a well\\-formatted text file.\n\n**Choosing the right chunk size**—We already mentioned the need for chunking due to LLM context windows. However, there are 2 additional motivations for chunking.\n\n**First**, it keeps (compute) costs down. The more text you inject into the prompt, the more compute required to generate a completion. The **second** is performance. Relevant information for a particular query tends to be localized in source documents (often, just 1 sentence can answer a question). Chunking helps minimize the amount of irrelevant information passed into the model \\[4].\n\n**Improving search** — While text embeddings enable a powerful and fast way to do search, it doesn’t always work as one might hope. In other words, it may return results that are “similar” to the user query, yet not helpful for answering it, e.g., “*How’s the weather in LA?*” may return “*How’s the weather in NYC?*”.\n\nThe simplest way to mitigate this is through good document preparation and chunking. However, for some use cases, additional strategies for improving search might be necessary, such as using **meta\\-tags** for each chunk, employing **hybrid search**, which combines keyword—and embedding\\-based search, or using a **reranker**, which is a specialized model that computes the similarity of 2 input pieces of text.\n\n\n## Example code: Improving YouTube Comment Responder with RAG\n\nWith a basic understanding of how RAG works, let’s see how to use it in practice. I will build upon the example from the [previous article](https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32), where I fine\\-tuned Mistral\\-7B\\-Instruct to respond to YouTube comments using QLoRA. We will use LlamaIndex to add a RAG system to the fine\\-tuned model from before.\n\nThe example code is freely available in a [Colab Notebook](https://colab.research.google.com/drive/1peJukr-9E1zCo1iAalbgDPJmNMydvQms?usp=sharing), which can run on the (free) T4 GPU provided. The source files for this example are available at the [GitHub repository](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/rag).\n\n🔗 [Google Colab](https://colab.research.google.com/drive/1peJukr-9E1zCo1iAalbgDPJmNMydvQms?usp=sharing) \\| [GitHub Repo](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/rag)\n\n\n### Imports\n\nWe start by installing and importing necessary Python libraries.\n\n\n```python\n!pip install llama-index\n!pip install llama-index-embeddings-huggingface\n!pip install peft\n!pip install auto-gptq\n!pip install optimum\n!pip install bitsandbytes\n## if not running on Colab ensure transformers is installed too\n```\n\n```python\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\nfrom llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex\nfrom llama_index.core.retrievers import VectorIndexRetriever\nfrom llama_index.core.query_engine import RetrieverQueryEngine\nfrom llama_index.core.postprocessor import SimilarityPostprocessor\n```\n\n### Setting up Knowledge Base\n\nWe can configure our knowledge base by defining our embedding model, chunk size, and chunk overlap. Here, we use the \\~33M parameter [bge\\-small\\-en\\-v1\\.5](https://huggingface.co/BAAI/bge-small-en-v1.5) embedding model from BAAI, which is available on the Hugging Face hub. Other embedding model options are available on this [text embedding leaderboard](https://huggingface.co/spaces/mteb/leaderboard).\n\n\n```python\n## import any embedding model on HF hub\nSettings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n\nSettings.llm = None # we won't use LlamaIndex to set up LLM\nSettings.chunk_size = 256\nSettings.chunk_overlap = 25\n```\nNext, we load our source documents. Here, I have a folder called “[*articles*](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/rag/articles),” which contains PDF versions of 3 Medium articles I wrote on [fat tails](https://towardsdatascience.com/pareto-power-laws-and-fat-tails-0355a187ee6a). If running this in Colab, you must download the articles folder from the [GitHub repo](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/rag) and manually upload it to your Colab environment.\n\nFor each file in this folder, the function below will read the text from the PDF, split it into chunks (based on the settings defined earlier), and store each chunk in a list called *documents*.\n\n\n```python\ndocuments = SimpleDirectoryReader(\"articles\").load_data()\n```\nSince the blogs were downloaded directly as PDFs from Medium, they resemble a webpage more than a well\\-formatted article. Therefore, some chunks may include text unrelated to the article, e.g., webpage headers and Medium article recommendations.\n\nIn the code block below, I refine the chunks in documents, removing most of the chunks before or after the meat of an article.\n\n\n```python\nprint(len(documents)) # prints: 71\nfor doc in documents:\n    if \"Member-only story\" in doc.text:\n        documents.remove(doc)\n        continue\n\n    if \"The Data Entrepreneurs\" in doc.text:\n        documents.remove(doc)\n\n    if \" min read\" in doc.text:\n        documents.remove(doc)\n\nprint(len(documents)) # prints: 61\n```\nFinally, we can store the refined chunks in a vector database.\n\n\n```python\nindex = VectorStoreIndex.from_documents(documents)\n```\n\n### Setting up Retriever\n\nWith our knowledge base in place, we can create a retriever using LlamaIndex’s *VectorIndexRetreiver(),* which returns the top 3 most similar chunks to a user query.\n\n\n```python\n## set number of docs to retreive\ntop_k = 3\n\n## configure retriever\nretriever = VectorIndexRetriever(\n    index=index,\n    similarity_top_k=top_k,\n)\n```\nNext, we define a query engine that uses the retriever and query to return a set of relevant chunks.\n\n\n```python\n## assemble query engine\nquery_engine = RetrieverQueryEngine(\n    retriever=retriever,\n    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.5)],\n)\n```\n\n### Use Query Engine\n\nNow, with our knowledge base and retrieval system set up, let’s use it to return chunks relevant to a query. Here, we’ll pass the same technical question we asked ShawGPT (the YouTube comment responder) from the [previous article](https://readmedium.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32).\n\n\n```python\nquery = \"What is fat-tailedness?\"\nresponse = query_engine.query(query)\n```\nThe query engine returns a response object containing the text, metadata, and indexes of relevant chunks. The code block below returns a more readable version of this information.\n\n\n```python\n## reformat response\ncontext = \"Context:\\n\"\nfor i in range(top_k):\n    context = context + response.source_nodes[i].text + \"\\n\\n\"\n\nprint(context)\n```\n\n```python\nContext:\nSome of the controversy might be explained by the observation that log-\nnormal distributions behave like Gaussian for low sigma and like Power Law\nat high sigma [2].\nHowever, to avoid controversy, we can depart (for now) from whether some\ngiven data fits a Power Law or not and focus instead on fat tails.\nFat-tailedness — measuring the space between Mediocristan\nand Extremistan\nFat Tails are a more general idea than Pareto and Power Law distributions.\nOne way we can think about it is that “fat-tailedness” is the degree to which\nrare events drive the aggregate statistics of a distribution. From this point of\nview, fat-tailedness lives on a spectrum from not fat-tailed (i.e. a Gaussian) to\nvery fat-tailed (i.e. Pareto 80 – 20).\nThis maps directly to the idea of Mediocristan vs Extremistan discussed\nearlier. The image below visualizes different distributions across this\nconceptual landscape [2].\n\nprint(\"mean kappa_1n = \" + str(np.mean(kappa_dict[filename])))\n    print(\"\")\nMean κ (1,100) values from 1000 runs for each dataset. Image by author.\nThese more stable results indicate Medium followers are the most fat-tailed,\nfollowed by LinkedIn Impressions and YouTube earnings.\nNote: One can compare these values to Table III in ref [3] to better understand each\nκ value. Namely, these values are comparable to a Pareto distribution with α\nbetween 2 and 3.\nAlthough each heuristic told a slightly different story, all signs point toward\nMedium followers gained being the most fat-tailed of the 3 datasets.\nConclusion\nWhile binary labeling data as fat-tailed (or not) may be tempting, fat-\ntailedness lives on a spectrum. Here, we broke down 4 heuristics for\nquantifying how fat-tailed data are.\n\nPareto, Power Laws, and Fat Tails\nWhat they don’t teach you in statistics\ntowardsdatascience.com\nAlthough Pareto (and more generally power law) distributions give us a\nsalient example of fat tails, this is a more general notion that lives on a\nspectrum ranging from thin-tailed (i.e. a Gaussian) to very fat-tailed (i.e.\nPareto 80 – 20).\nThe spectrum of Fat-tailedness. Image by author.\nThis view of fat-tailedness provides us with a more flexible and precise way of\ncategorizing data than simply labeling it as a Power Law (or not). However,\nthis begs the question: how do we define fat-tailedness?\n4 Ways to Quantify Fat Tails\n```\n\n### Adding RAG to LLM\n\nWe start by downloading the [fine\\-tuned model](https://readmedium.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32) from the Hugging Face hub.\n\n\n```python\n## load fine-tuned model from hub\nfrom peft import PeftModel, PeftConfig\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name,\n                                             device_map=\"auto\",\n                                             trust_remote_code=False,\n                                             revision=\"main\")\n\nconfig = PeftConfig.from_pretrained(\"shawhin/shawgpt-ft\")\nmodel = PeftModel.from_pretrained(model, \"shawhin/shawgpt-ft\")\n\n## load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n```\nAs a baseline, we can see how the model responds to the technical question without any context from the articles. To do this, we create a prompt template using a lambda function, which takes in a viewer comment and returns a prompt for the LLM. For more details on where this prompt comes from, see the [previous article](https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32#5aad) of this series.\n\n\n```python\n## prompt (no context)\nintstructions_string = f\"\"\"ShawGPT, functioning as a virtual data science \\\nconsultant on YouTube, communicates in clear, accessible language, escalating \\\nto technical depth upon request. It reacts to feedback aptly and ends \\\nresponses with its signature '–ShawGPT'.\n\nShawGPT will tailor the length of its responses to match the viewer's comment, \\\nproviding concise acknowledgments to brief expressions of gratitude or \\\nfeedback, thus keeping the interaction natural and engaging.\n\nPlease respond to the following comment.\n\"\"\"\n\nprompt_template = lambda comment: f'''[INST] {intstructions_string} \\n{comment} \\n[/INST]'''\ncomment = \"What is fat-tailedness?\"\n\nprompt = prompt_template(comment)\nprint(prompt)\n```\n\n```python\n[INST] ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. It reacts to feedback aptly and ends responses with its signature '–ShawGPT'. ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, thus keeping the interaction natural and engaging.\n\nPlease respond to the following comment.\n \nWhat is fat-tailedness? \n[/INST]\n```\nNext, we can pass this prompt to the model using the code below.\n\n\n```python\nmodel.eval()\n\ninputs = tokenizer(prompt, return_tensors=\"pt\")\noutputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), \n                          max_new_tokens=280)\n\nprint(tokenizer.batch_decode(outputs)[0])\n```\nHere’s the model’s response (no context).\n\n\n```python\nGreat question!\n\nFat-tailedness is a statistical property of a distribution. In simple terms, \nit refers to the presence of extreme outliers or heavy tails in the \ndistribution.\n\nFor instance, consider the distribution of heights in a population. A normal \ndistribution would have most people clustered around an average height with a \nfew people deviating slightly from the mean. However, in a fat-tailed \ndistribution, you would observe a larger number of people being significantly \ntaller or shorter than the average.\n\nIn finance, fat-tailedness is often used to describe the presence of extreme \nmarket moves or outliers in financial returns. It's an important concept in \nrisk management and financial modeling.\n\nI hope this explanation helps! Let me know if you have any questions.\n–ShawGPT\n```\nAlthough the response's style and formatting are great, the model's explanation of fat\\-tailedness differs from how I defined it in my [video](https://www.youtube.com/playlist?list=PLz-ep5RbHosVrT89BRNX-IGKHYxvx9wqi) and [blog series](https://towardsdatascience.com/pareto-power-laws-and-fat-tails-0355a187ee6a).\n\nLet’s see what happens to the model’s response when we include the appropriate context. To do this, we create another prompt template, which can also take in context from the RAG system.\n\n\n```python\n## prompt (with context)\nprompt_template_w_context = lambda context, comment: f\"\"\"[INST]ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. \\\nIt reacts to feedback aptly and ends responses with its signature '–ShawGPT'. \\\nShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, \\\nthus keeping the interaction natural and engaging.\n\n{context}\nPlease respond to the following comment. Use the context above if it is helpful.\n\n{comment}\n[/INST]\n\"\"\"\n```\nNext, we pass the prompt with context from the query engine and the view comment to the model.\n\n\n```python\nprompt = prompt_template_w_context(context, comment)\n\ninputs = tokenizer(prompt, return_tensors=\"pt\")\noutputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=280)\n\nprint(tokenizer.batch_decode(outputs)[0])\n```\nHere’s the new response (with context).\n\n\n```python\nGreat question!\n\nFat-tailedness is a property of a distribution that describes how much the \npresence of extreme values (rare events) affects the overall statistics of the \ndistribution.\n\nIn the context of the video, the author discusses how Mediocristan and \nExtremistan distributions differ in terms of fat-tailedness. Mediocristan \ndistributions have a relatively small impact from extreme values, while \nExtremistan distributions are heavily influenced by extreme values.\n\nThe author also mentions that fat-tailedness is a more general concept than \nspecific distributions like Pareto or Power Laws. Instead, it lives on a \nspectrum ranging from thin-tailed (Gaussian) to very fat-tailed (Pareto 80-20).\n\nI hope that helps clarify things a bit! Let me know if you have any questions.\n–ShawGPT\n```\nThis does a much better job of capturing my explanation of fat tails than the no\\-context response and even calls out the niche concepts of Mediocristan and Extremistan.\n\n\n## What’s next?\n\nHere, I gave a beginner\\-friendly introduction to RAG and shared a concrete example of how to implement it using LlamaIndex. RAG allows us to improve an LLM system with updateable and domain\\-specific knowledge.\n\nWhile much of the recent AI hype has centered around building AI assistants, a powerful (yet less popular) innovation has come from text embeddings (i.e. the things we used to do retrieval). In the next article of this series, I will explore **text embeddings** in more detail, including how they can be used for **semantic search** and **classification tasks**.\n\n**More on LLMs 👇**\n\n\n## Resources\n\n**Connect**: [My website](https://shawhintalebi.com/) \\| [Book a call](https://calendly.com/shawhintalebi)\n\n**Socials**: [YouTube 🎥](https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA) \\| [LinkedIn](https://www.linkedin.com/in/shawhintalebi/) \\| [Instagram](https://www.instagram.com/shawhintalebi)\n\n**Support**: [Buy me a coffee](https://www.buymeacoffee.com/shawhint) ☕️\n\n\\[1] [RAG \\> FT (empirical)](https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb)\n\n\\[2] [LlamaIndex Webinar: Building LLM Apps for Production, Part 1 (co\\-hosted with Anyscale)](https://www.youtube.com/watch?v=efbn-3tPI_M)\n\n\\[3] [LlamaIndex doc](https://docs.llamaindex.ai/en/stable/understanding/loading/loading.html)\n\n\\[4] [LlamaIndex Webinar: Make RAG Production\\-Ready](https://www.youtube.com/watch?v=Zj5RCweUHIk&list=WL&index=4)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/how-to-run-nvidia-llama-3-1-nemotron-70b-instruct-locally-a58ad283aaff","frontmatter":{"title":"How to Run Nvidia’ llama-3.1-nemotron-70b-instruct Locally","meta_title":"How to Run Nvidia’ llama-3.1-nemotron-70b-instruct Locally","description":"Running large language models (LLMs) locally has become increasingly popular among developers, researchers, and AI enthusiasts. One such…","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*fqVKJkw5sQvLtIsyCcengQ.png","categories":["Programming","Technology","Science"],"author":"Rifx.Online","tags":["Nvidia","llama","Ollama","llama.cpp","Transformers"],"draft":false,"slug":"blog/how-to-run-nvidia-llama-3-1-nemotron-70b-instruct-locally-a58ad283aaff"},"content":"\n\n\n\nRunning large language models (LLMs) locally has become increasingly popular among developers, researchers, and AI enthusiasts. One such model that has gained significant attention is the llama-3.1-nemotron-70b-instruct, a powerful LLM customized by NVIDIA to enhance the helpfulness of generated responses. In this comprehensive guide, we’ll explore multiple methods to run this model on your local machine, starting with the user-friendly Ollama platform.\n\n\n> Before we get started, If you are seeking an All-in-One AI platform that manages all your AI subscriptions in one place, including all LLMs (such as GPT-o1, Llama 3.1, Claude 3.5 Sonnet, Google Gemini, Uncensored LLMs) and Image Generation Models (FLUX, Stable Diffusion, etc.), Use Anakin AI to manage them all!\n\n\n\n\n## Method 1: Run llama-3.1-nemotron-70b-instruct Locally with Ollama\n\nOllama is an excellent tool for running LLMs locally, offering a straightforward setup process and support for various models, including llama-3.1-nemotron-70b-instruct.\n\n\n### Installation\n\n1. Visit the official Ollama website ([https://ollama.ai](https://ollama.ai/)) and download the appropriate version for your operating system.\n2. Install Ollama by running the following command in your terminal:\n\n\n```python\ncurl https://ollama.ai/install.sh | sh\n```\n\n### Running llama-3.1-nemotron\n\nOnce Ollama is installed, you can easily run the llama-3.1-nemotron-70b-instruct model with a simple command:\n\n\n```python\nollama run nemotron:70b-instruct-q5_K_M\n```\nThis command will download the model if it’s not already present on your system and start an interactive session.\n\n\n### Using the Model\n\nAfter the model is loaded, you can start interacting with it by typing your prompts. For example:\n\n\n```python\n>>> What are the key features of llama-3.1-nemotron-70b-instruct?\n\nLlama-3.1-Nemotron-70B-Instruct is a large language model with several key features:\n1. Customized by NVIDIA: The model has been fine-tuned by NVIDIA to improve the helpfulness and quality of its responses.\n2. Based on Llama 3.1: It builds upon the Llama 3.1 architecture, which is known for its strong performance across various tasks.\n3. 70 billion parameters: This large parameter count allows for complex reasoning and a wide range of capabilities.\n4. Instruct-tuned: The model is specifically designed to follow instructions and generate helpful responses to user queries.\n5. RLHF training: It has been trained using Reinforcement Learning from Human Feedback, specifically the REINFORCE algorithm.\n6. Specialized reward model: The training process utilized Llama-3.1-Nemotron-70B-Reward for optimization.\n7. HelpSteer2-Preference prompts: These were used during the training process to further improve the model's helpfulness.\n8. Extended context length: Like other Llama 3.1 models, it likely supports a longer context window of 128K tokens.\n9. Multilingual capabilities: It can understand and generate text in multiple languages.\n10. Strong reasoning abilities: The model excels in tasks requiring complex reasoning and problem-solving.\nThese features make llama-3.1-nemotron-70b-instruct a powerful and versatile language model suitable for a wide range of applications, from general conversation to specialized tasks in various domains.\n```\nFor more advanced use cases, you can integrate Ollama with Python using libraries like Langchain. Here’s a simple example:\n\n\n```python\npython\n\nfrom langchain.llms import Ollama\n\nollama = Ollama(base_url=\"http://localhost:11434\", model=\"nemotron:70b-instruct-q5_K_M\")\nresponse = ollama.generate(\"Explain the concept of quantum entanglement.\")\nprint(response)\n```\nThis allows you to incorporate the model into your Python projects and applications seamlessly.\n\n\n## Method 2: Using llama.cpp\n\nllama.cpp is a popular C++ implementation of the Llama model inference, optimized for CPU usage. While it may require more setup than Ollama, it offers greater flexibility and control over the model’s parameters.\n\n\n### Installation\n\n1. Clone the llama.cpp repository:\n\n\n```python\ngit clone https://github.com/ggerganov/llama.cpp.git\ncd llama.cpp\n```\n1. Build the project:\n\n\n```python\nmake\n```\n\n### Downloading the Model\n\nTo run llama-3.1-nemotron-70b-instruct, you’ll need to download the model weights. These are typically available in GGML or GGUF format. You can find pre-converted models on platforms like Hugging Face.\n\n\n```python\nmkdir models\ncd models\nwget https://huggingface.co/TheBloke/Llama-3.1-Nemotron-70B-Instruct-GGUF/resolve/main/llama-3.1-nemotron-70b-instruct.Q4_K_M.gguf\n```\n\n### Running the Model\n\nOnce you have the model file, you can run it using the following command:\n\n\n```python\n./main -m models/llama-3.1-nemotron-70b-instruct.Q4_K_M.gguf -n 1024 -p \"Hello, how are you today?\"\n```\nThis command loads the model and generates a response to the given prompt. You can adjust various parameters like the number of tokens to generate (-n) or the temperature to control randomness.\n\n\n## Method 3: Using Hugging Face Transformers\n\nHugging Face’s Transformers library provides a high-level API for working with various language models, including llama-3.1-nemotron-70b-instruct.\n\n**Installation**\n\nFirst, install the necessary libraries:\n\n\n```python\npip install transformers torch accelerate\n```\n**Running the Model**\n\nHere’s a Python script to load and use the model:\n\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nmodel_name = \"meta-llama/Llama-3.1-Nemotron-70b-instruct\"\n## Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n## Prepare the input\nprompt = \"Explain the concept of quantum computing in simple terms.\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n## Generate the response\nwith torch.no_grad():\n    outputs = model.generate(**inputs, max_new_tokens=100)\n## Decode and print the response\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response)\n```\nThis method allows for more fine-grained control over the model’s behavior and integration with other Hugging Face tools and pipelines.\n\n\n## Conclusion\n\nRunning llama-3.1-nemotron-70b-instruct locally opens up a world of possibilities for developers and researchers. Whether you choose the simplicity of Ollama, the flexibility of llama.cpp, or the integration capabilities of Hugging Face Transformers, you now have the tools to harness the power of this advanced language model on your own hardware.As you explore the capabilities of llama-3.1-nemotron-70b-instruct, remember to balance performance with resource constraints, and always consider the ethical implications of your applications. With responsible use, this model can be a valuable asset in pushing the boundaries of what’s possible in natural language processing and AI-driven applications.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/introduction-to-llava-a-multimodal-ai-model-2a2fa530ace4","frontmatter":{"title":"Introduction to LLaVA: A Multimodal AI Model","meta_title":"Introduction to LLaVA: A Multimodal AI Model","description":"LLaVA is an end-to-end trained large multimodal model that is designed to understand and generate content based on both visual inputs…","date":"2024-10-29T12:48:10.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*0At7tXF5ejho9Y46E3uGtg.png","categories":["Natural Language Processing","Computer Vision","Generative AI"],"author":"Rifx.Online","tags":["LLaVA","GPT-4","multimodal","visual","encoder"],"draft":false,"slug":"blog/introduction-to-llava-a-multimodal-ai-model-2a2fa530ace4"},"content":"\n\n\n\n\n\nLLaVA is an end\\-to\\-end trained large multimodal model that is designed to understand and generate content based on both visual inputs (images) and textual instructions. It combines the capabilities of a visual encoder and a language model to process and respond to multimodal inputs.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*mjzqL0BHzdPoN-Jjruh52A.png)\n\n\n## Inputs and Outputs of LLaVA: Bridging Visual and Textual Domains:\n\nThe inputs to LLaVA are twofold:\n\n1. Visual Input: Images that the model can view and analyze to extract visual features and contextual information.\n2. Textual Instructions: Text inputs, which can be questions or commands, that guide the model on what to focus on or what kind of task to perform regarding the visual input.\n\nThe outputs of LLaVA are text\\-based and can vary depending on the task:\n\n1. Descriptive Text: If the task is to describe the visual content, LLaVA can output a detailed description of the image, identifying objects, actions, and scenes.\n2. Answers to Questions: For question\\-answering tasks, LLaVA generates responses that answer questions about the visual input, potentially involving reasoning and inference based on the image’s content.\n3. Follow\\-up Actions: For instructions that require action, such as editing an image or retrieving more information, LLaVA can provide appropriate textual responses indicating the action taken or suggesting what should be done.\n\n\n## Comparative Analysis: LLaVa vs. Contemporary Multimodal Models\n\nThe landscape of multimodal AI has been rapidly evolving with innovations such as CLIP, BLIP, and the recent introduction of LLaVa. This subsection compares LLaVa’s unique architecture and approach with these contemporary models, highlighting the advancements and distinctions that set it apart.\n\n\n### CLIP: Pioneering Multimodal Understanding\n\nCLIP (Contrastive Language–Image Pre\\-training) has been a revolutionary step forward in multimodal AI, offering robust performance across a variety of visual tasks. Its ability to understand images in the context of natural language descriptions set a new benchmark in the field. CLIP achieves this through a large\\-scale pretraining approach that aligns images with textual descriptions, enabling the model to perform zero\\-shot learning on a range of visual tasks. However, CLIP primarily focuses on the association between images and text at a high level and does not inherently possess the capability for in\\-depth reasoning or conversational engagement.\n\n\n### BLIP: Bridging Language and Image Perception\n\nBuilding upon the foundation laid by CLIP, BLIP (Bootstrapped Language Image Pre\\-training) extends the capabilities of multimodal models by incorporating a bootstrapped pretraining strategy. This approach refines the model’s visual understanding by continually learning from its own predictions, which helps to improve the alignment between language and visual content. BLIP demonstrates enhanced performance on tasks that require more precise visual recognition and language understanding.\n\nIn contrast, LLaVa takes a different route by leveraging the language\\-generating capabilities of GPT\\-4 to curate its instruction\\-following data. This not only results in a dataset that captures a broader range of human\\-like interactions but also enables LLaVa to engage in more complex reasoning and in\\-depth conversational abilities.\n\n\n## What Sets LLaVa Apart: Is It the Model Architecture or Something Else?\n\nAccording to us , LLaVA’s strength lies predominantly in its data curation capabilities rather than its architectural choice. LLaVA marks a significant leap forward , primarily due to its utilization of GPT\\-4 for data curation. Unlike conventional static datasets, LLaVA generates dynamic, instructive data using ChatGPT\\-4, actively involving data in the training process across various visual and textual scenarios.\n\nBy using GPT\\-4, LLaVA produces datasets that closely mimic natural language and visual perception, departing from traditional manual dataset generation methods. This innovative approach not only enables AI to understand and reason but also moves it closer to accurately reflecting human intelligence.\n\n\n### Data Curation Strategies in LLaVa\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*LzastWLkzPeMB_28Nr7Y9A.png)\n\nLLaVa, the Large Language and Vision Assistant, stands out not just for its advanced neural architecture but for its groundbreaking approach to data curation. By leveraging GPT\\-4, it revolutionizes traditional data preparation methods, crafting a dataset that mirrors the complexity of the real world.\n\nData curation in LLaVa begins with an image and its corresponding caption, from which a set of queries is generated using GPT\\-4\\. These queries guide the AI to explore and describe the image content with precision and relevance.\n\nTo translate visual data effectively for a text\\-based AI like GPT\\-4, LLaVa uses captions to offer diverse perspectives of the visual scene and bounding boxes to provide spatial context and focus.\n\n1. Conversational Data: Mimicing human interaction, LLaVa curates dialogues where the model, playing the assistant, responds to questions about various aspects of the image. The scope of these questions ranges from identifying objects and actions to discerning their numbers, locations, and relative positions, ensuring the model can handle queries with definitive answers.\n2. Detailed Descriptive Data: LLaVa seeks to comprehend the images in a comprehensive manner. To achieve this, it prompts GPT\\-4 to formulate questions aimed at understanding rich and detailed descriptions of the images. These prompts encourage the model to delve deeper, providing a narrative that captures the essence of the visual content in its entirety.\n3. Complex Reasoning Data: Moving beyond mere description, LLaVa challenges the model with questions that necessitate a layered reasoning process, demanding logic and an understanding of cause and effect. This type of data trains the model to construct well\\-reasoned responses that are backed by a logical sequence of thought.\n\n\n## The Architecture of LLaVa: Integrating Vision and Language\n\nThe LLaVa model integrates vision and language, utilizing the following core components:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8q_Iay_LHCzPqtrQby_H8w.png)\n\n1. Vision Encoder: At the foundation of LLaVa’s architecture is the pre\\-trained CLIP visual encoder, specifically the ViT\\-L/14 variant. This component processes input images (Xv) through Transformer layers to extract features (Zv), enabling the model to understand visual information effectively.\n2. Language Model (Vicuna): LLaVa’s linguistic capabilities rely on Vicuna, a variant of a large language model (LLM) denoted by fϕ . Vicuna comprehends and generates language responses (Xa) based on input language instructions (Xq), complementing the vision encoder’s functionality.\n3. Linear Projection: This component, represented by a trainable matrix (W), serves as the bridge between visual features (Zv) and the language model’s embedding space. It transforms visual features into visual tokens (Hv), aligning them with the language model’s word embedding space to facilitate multimodal conversation\n\n\n## Training and Fine\\-Tuning LLaVA:\n\nLLaVA’s has a two\\-stage training process, each stage focusing on refining the model’s capabilities to interpret and respond to a fusion of visual and textual data.\n\n\n### Stage 1: Pre\\-training for Feature Alignment\n\nThe initial stage of LLaVA’s training is pre\\-training for feature alignment. In this phase, the model focuses on aligning visual features from images with the corresponding textual features from the language model. This is achieved by filtering a large dataset to a refined set of image\\-text pairs, which LLaVA uses to learn the correlations between the two modalities.\n\nDuring this stage, a visual encoder (such as the CLIP visual encoder ViT\\-L/14\\) processes the images to extract visual features, and a projection matrix (W) is then used to map these features into the word embedding space of the language model. The language model used in LLaVA is Vicuna, known for its strong language understanding and generation capabilities.\n\n\n### Stage 2: Fine\\-tuning End\\-to\\-End\n\nAfter aligning the visual and language features, LLaVA undergoes an end\\-to\\-end fine\\-tuning process. Despite keeping the visual encoder’s weights frozen, this stage allows the model to fine\\-tune the weights of the projection matrix and language model jointly. The objective is to maximize the likelihood of the target answers based on the multimodal data provided.\n\nThis stage is critical for adapting LLaVA to specific use case scenarios such as multimodal chat, scientific Q\\&A, and more. It ensures that the model does not just understand images in the context of generic descriptions but can engage in complex dialogues, provide detailed explanations, and reason through problems when prompted with specific questions related to the images.\n\n\n## Performance and Benchmarking: LLaVa in the Context of VQA Models\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*I_5fTa_2rtNHEDUaDNMXbQ.png)\n\n\n## LLaVA\\-Bench (COCO) Performance Insights\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*6B2K7EcbYgMbH-QEp8J41w.png)\n\nLLaVA\\-Bench (COCO) provides a robust framework for assessing LLaVA’s capabilities through a carefully crafted set of 90 questions, derived from 30 selected images for conversation, detailed description, and complex reasoning. The results were as follows:\n\n* Instruction Tuning Efficacy: When equipped with instruction tuning, LLaVA’s compliance with user commands improved by over 50 points.\n* Impact of Question Variety: The inclusion of detailed and complex reasoning questions, though minimal, led to a 7\\-point increase in overall capabilities. This boost also had a positive effect on conversational question responses, showcasing the benefits of a diverse training set.\n* Optimal Data Mix: The combination of all three question types resulted in the highest performance leap, with LLaVA reaching a benchmark score of 85\\.1%, emphasizing the strength of a comprehensive dataset in enhancing multimodal AI proficiency.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*mCjP0xfpcjHkl-lu)\n\n\n## LLaVA’s Performance on LLaVA\\-Bench (In\\-the\\-Wild)\n\n* In conversational tasks, LLaVA achieves a 57\\.3% accuracy rate, a clear improvement over BLIP\\-2’s 54\\.6% and significantly outpacing OpenAI’s Flamingo, which stands at 19\\.3%.\n* When it comes to providing detailed descriptions, LLaVA scores 52\\.5%, showcasing its ability to generate rich, comprehensive content from visual cues.\n* The model’s prowess is most notable in complex reasoning questions, where it achieves an 81\\.7% success rate, indicating its advanced reasoning and inferencing skills.\n\nLLaVA secures a combined score of 67\\.3% across all categories, surpassing BLIP\\-2 by a 29% margin and Flamingo by 48%.\n\n\n## Limitation and Concerns:\n\nQuantitative Evaluation of LLaVA:\n\nThe utilization of GPT\\-4 as a judge to evaluate LLaVA’s performance presents a nuanced challenge within the framework of benchmarking AI capabilities. On one hand, GPT\\-4’s advanced comprehension and generation abilities enable it to critically assess the quality of responses produced by candidate models like LLaVA. This assessment encompasses factors such as helpfulness, relevance, accuracy, and detail, which are crucial for gauging a model’s instruction\\-following proficiency with multimodal data. However, on the other hand, the use of GPT\\-4 as an evaluative judge raises concerns regarding the impartiality of the benchmarking process.\n\nThe crux of the concern lies in the fact that LLaVA’s data curation process is fundamentally intertwined with GPT\\-4\\. Since GPT\\-4 has been instrumental in training LLaVA — by generating the instruction\\-following data that the model was fine\\-tuned on — there is an inherent risk of circular reasoning. Essentially, there is a possibility that LLaVA may be predisposed to generate responses that align with the patterns or biases inherent in GPT\\-4’s training data. This predisposition could skew the evaluation, leading to a theoretical upper bound that reflects compatibility with GPT\\-4’s methodology rather than a true measure of universal performance.\n\nFurthermore, relying on GPT\\-4 to provide a comprehensive explanation for its evaluation introduces a level of subjectivity rooted in the language model’s own “understanding” of what constitutes a high\\-quality response. This understanding is shaped by the datasets on which GPT\\-4 was trained, which may not fully encapsulate the diversity and complexity of real\\-world multimodal interactions.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/key-points-llm-quantization-chatgpt-artificial-intelligence-8201ffcb33d4","frontmatter":{"title":"5 Key Points to Unlock LLM Quantization","meta_title":"5 Key Points to Unlock LLM Quantization","description":"Quantizing Large Language Models","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*RUqPEr2NTYXlI1omqF22Qg.png","categories":["Machine Learning","Data Science","Technology/Web"],"author":"Rifx.Online","tags":["quantization","weights","activations","calibration","Quanto"],"draft":false,"slug":"blog/key-points-llm-quantization-chatgpt-artificial-intelligence-8201ffcb33d4"},"content":"\n\n\n\n\n### Quantizing Large Language Models\n\n\n\nLLM Quantization is currently a hot topic due to its vital role in making Large Language Models (LLMs) more efficient and deployable across various hardware platforms, including consumer-grade devices.\n\nBy adjusting the precision of certain components within the model, **quantization significantly reduces the model’s memory footprint** while maintaining similar performance levels.\n\nIn this guide, we will explore five key aspects of LLM quantization including some practical steps for applying this technique to our models.\n\n\n## #1. Understanding Quantization\n\nQuantization is a model compression technique that reduces the precision of weights and activations in an LLM. This involves converting high-precision values to lower-precision ones, effectively **changing data types that store more information to those that store less**.\n\nDecreasing the number of bits needed for each weight or activation significantly reduces the overall model size. As a result, **quantization creates LLMs that use less memory, and require less storage space.**\n\nThis technique has become essential in response to the exponential growth in the number of parameters in successive iterations of LLMs. For example, for the OpenAI’s GPT family, we can observe the growing trend in the following graph:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*QlAhma3Wu1F6w2WvkE8jDA.png)\n\nThis significant increase presents a challenge: as models grow, their memory requirements often exceed the capacity of advanced hardware accelerators such as GPUs. **This requires distributed training and inference to manage these models, which in turn limits their deployability.**\n\n\n## #2. Intuition Behind Quantization\n\nAlthough the definition of quantization may seem rather complex, the concept can be intuitively explained using matrices.\n\nLet’s consider the following a 3x3 matrix representing the weights of a neural network. The matrix on the left shows the original weights, while the matrix on the right shows the quantized version of these weights:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*LPzWe9oxjlDYdSp7dVvRUg.png)\n\nIn this simple example, we round the elements of the original matrix from four decimal places to a single decimal place. Although the matrices appear similar, **the storage space required for the four-decimal version is significantly higher**.\n\nIn practice, quantization is not merely a rounding operation. Instead, it involves converting numerical values to a different data type, typically from a higher to a lower precision one.\n\nFor example, the default data type for most models is `float32`, which requires 4 bytes per parameter (32 bits). Therefore, for a 3x3 matrix, the total memory footprint is 36 bytes. Changing the data type to `int8`, only 1 byte per parameter is needed, reducing the total memory footprint of the matrix to just 9 bytes.\n\n\n## #3. Quantization Error\n\nAs we have seen, the original matrix and its quantized form are not completely equal, but very similar. The value-by-value difference is known as “Quantization error”, which we can also represent in matrix form:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*VtGDjVbr7daagLXB57i7Mg.png)\n\n**This quantization error can accumulate for each matrix of weights in the network, affecting the model’s performance as a result.**\n\nCurrent research in quantization aims to minimize the difference in precision while decreasing the computational resources required to train or run inference on models, while maintaining acceptable performance levels.\n\n\n## #4. Linear Quantization\n\nLinear quantization is one of the most popular quantization schemes for LLMs. In simple terms, it involves mapping the range of floating-point values of the original weights to a range of fixed-point values.\n\nLet’s review the steps required to apply linear quantization to our models:\n\n* **Get the minimum and maximum ranges:** We need to get the minimum and maximum values of the floating-point weights to be quantized (`x_min` and `x_max`). We also need to define the quantized range (`q_min` and `q_max`), which is already set by the data type we want to convert to.\n* **Compute the scale (`s`) and the zero-point (`z`) values:** Firstly, the scale (`s`) adjusts the range of floating-point values to fit within the integer range, preserving the data distribution and range. Secondly, the zero-point (`z`) ensures that zero in the floating-point range is accurately represented by an integer, maintaining numerical accuracy and stability, especially for values close to zero.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BepC6-izw0yE19ejsS705Q.png)\n\n* **Quantize the values (`q`)**: We need to map the original floating-point values to the integer range using a scale factor (`s`) and a zero point (`z`) computed in the previous step.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BBOQ0VbSGbwf7CN8c4PWKQ.png)\n\nApplying these formulas is quite straightforward. If we apply them to the 3x3 weight tensor on the left in the image below, we will get the quantized matrix shown on the right:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*KzBvg84mfI2gAhTIyVibwQ.png)\n\nWe can see that the lower bound of the `int8` value corresponds to the lower value of the original tensor, while the upper bound corresponds to the higher value of the original tensor, *i.e., the mapping is`0.50 → 255` and `-0.40 → 0`.*\n\nWe can now dequantize the values using the formula below.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*E5nnqYzncYCRuM5prssuOw.png)\n\nIf we place the dequantized values again in matrix form (matrix on the left), we can compute the quantization error (matrix on the right) by calculating the point-by-point difference between the original matrix and its dequantized version:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*56NALu9PAN95QG2hn8HXoQ.png)\n\nAs we can observe, the quantization error starts kicking in for some of the matrix values.\n\n\n## #5. Weight Quantization vs Activation Quantization\n\nIn our example above, we have focused primarily on quantizing the weights of the model. While weight quantization is crucial for model optimization, it’s also important to consider that activations can be quantized as well.\n\n**Activation quantization involves reducing the precision of the intermediate outputs of each layer in the network**. Unlike weights, which remain constant once the model is trained, activations are dynamic and change with each input, making their range harder to predict.\n\nGenerally, activation quantization is more challenging to implement than weight quantization because it requires careful calibration to ensure the dynamic range of activations is accurately captured.\n\nWeight quantization and activation quantization are complementary techniques. Using both can significantly reduce model size without greatly compromising performance.\n\n\n## Final Thoughts\n\nIn this article, we have reviewed 5 key points about quantization to better understand how to reduce the size of these constantly growing models.\n\nAs for the implementation of those techniques, there are several tools and libraries in Python that support quantization such as `pytorch` and `tensorflow`. Nevertheless, integrating quantization seamlessly in existing models requires a deep understanding of the libraries and model internals.\n\nThat is why my favorite option to implement quantization in easy steps so far is the [Quanto](https://huggingface.co/blog/quanto-introduction) library by Hugging Face, designed to simplify the quantization process for PyTorch models.\n\nIf you are interested in the in-depths of LLM Quantization and how to use the aforementioned library, you might also be interested in the article [“Quantization for Large Language Models (LLMs): Reduce AI Model Sizes Efficiently”](https://www.datacamp.com/tutorial/quantization-for-large-language-models).\n\nThat is all! Many thanks for reading!\n\nI hope this article helps you when **using LLMs for coding!**\n\nYou can also subscribe to my [**Newsletter**](https://readmedium.com/@andvalenzuela/subscribe) to stay tuned for new content.\n\n**Especially**, **if you are interested in articles about Large Language Models and ChatGPT**:\n\n\n"},{"lang":"en","group":"blog","slug":"blog/langgraph-vs-langchain-vs-langflow-vs-langsmith-which-one-to-use-why-69ee91e91000","frontmatter":{"title":"LangGraph vs. LangChain vs. LangFlow vs. LangSmith: Which One to Use & Why?","meta_title":"LangGraph vs. LangChain vs. LangFlow vs. LangSmith: Which One to Use & Why?","description":"Discover the key differences between LangGraph, LangChain, LangFlow, and LangSmith, and learn which framework is best suited for your…","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*xrWv1QVt4zE5cxjA8VA3ag.png","categories":["Programming","Technology","Technology/Web"],"author":"Rifx.Online","tags":["LangGraph","LangChain","LangFlow","LangSmith","frameworks"],"draft":false,"slug":"blog/langgraph-vs-langchain-vs-langflow-vs-langsmith-which-one-to-use-why-69ee91e91000"},"content":"\n\n\n\n\n### Discover the key differences between LangGraph, LangChain, LangFlow, and LangSmith, and learn which framework is best suited for your language model applications — from workflow building to performance monitoring.\n\n👨🏾‍💻 [GitHub](https://github.com/mdmonsurali) ⭐️ | 👔[LinkedIn](https://www.linkedin.com/in/mdmonsurali/) |📝 [Medium](https://medium.com/@monsuralirana)\n\n\n\nIn recent years, the world of natural language processing (NLP) has witnessed an explosion in the number of frameworks, libraries, and tools available for building language model-based applications. Among these, **LangGraph**, **LangChain**, **LangFlow**, and **LangSmith** have emerged as leading options, each catering to different use cases and user needs. If you’re looking to build, monitor, or scale language model workflows, it’s crucial to understand the strengths and purposes of these tools.\n\nIn this blog, we’ll explore each framework, break down their strengths, and provide insights into when to use them. Whether you’re a seasoned developer or a newcomer to the field, understanding the nuances of these tools will help you choose the right one for your project.\n\n\n## Introduction to Language Model Frameworks\n\nWith the rise of powerful language models such as GPT-3, GPT-4, and other transformer-based models, there is a growing need for frameworks that streamline the creation and management of language-based applications. These frameworks simplify complex tasks like **chaining multiple prompts**, **retrieving relevant documents**, and even **monitoring model performance**.\n\nHowever, not all frameworks are the same. While some provide a **visual interface** to manage workflows, others offer advanced **debugging and observability** features. Let’s dive into each of these tools to understand their unique offerings.\n\n\n## 1. LangGraph: Visualizing Complex Workflows\n\n**LangGraph** is a newer framework designed for developers who prefer a **visual approach** to building language model pipelines. It allows you to structure complex workflows with **graph-based visualizations**, making it easier to understand dependencies between different tasks and components. This can be especially useful for larger applications where multiple steps, such as text generation, document retrieval, and classification, are chained together.\n\n\n### Strengths:\n\n* **Visual Workflow Representation**: LangGraph lets you visualize the flow of data and actions between different components. This graphical approach is intuitive and helps in designing more complex pipelines.\n* **Ease of Debugging**: The visual nature of LangGraph makes it easier to identify bottlenecks or problematic nodes in a workflow.\n\n\n### Example Use Case:\n\nSuppose you’re building an automated system that first retrieves relevant documents using a language model and then passes them through a summarizer. In LangGraph, you can visually map out this workflow, showing the relationships between each step. If there’s an issue at any point in the chain, the visual tool makes it easy to pinpoint where things went wrong.\n\n\n### When to Use LangGraph:\n\nIf you’re managing **complex workflows** with multiple steps and value a **graphical interface** for understanding your pipeline, LangGraph is a fantastic choice. It’s particularly helpful for developers or data scientists who prefer a more intuitive, drag-and-drop approach to workflow design.\n\n**Key points**:\n\n* If you need a clear visual representation of language processing workflows.\n* When creating more complex pipelines that require branching or multi-path dependencies.\n\n\n## 2. LangChain: The Workhorse for LLM Applications\n\n**LangChain** is one of the most popular frameworks for building applications powered by **large language models (LLMs)**. It provides a versatile, **code-first approach**, allowing developers to chain tasks such as document retrieval, summarization, and question-answering into cohesive workflows.\n\n\n### Strengths:\n\n* **Extensive Support for LLMs**: LangChain is compatible with various language models, making it easy to integrate models like OpenAI’s GPT or even locally hosted models.\n* **Chaining Capabilities**: LangChain excels at **chaining multiple operations** — hence the name — enabling developers to create sophisticated NLP applications.\n* **Wide Adoption**: As one of the most popular frameworks, LangChain has a **thriving community** and excellent support, with ample documentation and tutorials.\n\n\n### Example Use Case:\n\nImagine you’re building a **chatbot** that first understands the user’s question, retrieves relevant information from a database, and then generates a response. With LangChain, you can easily create this multi-step process programmatically, ensuring each step in the chain works harmoniously.\n\n\n### When to Use LangChain:\n\nIf you’re a **developer building production-level applications** and need a **flexible, code-centric solution**, LangChain is your best bet. It’s ideal for those who prefer control over their application’s architecture and are comfortable writing code to define workflows.\n\n**Key points**:\n\n* If you’re building production-grade applications that require chaining of tasks across multiple language models.\n* If you need a library with extensive community support and wide-ranging integrations.\n* When you’re more comfortable with programmatic solutions rather than visual tools.\n\n\n## 3. LangFlow: No-Code/Low-Code Extension of LangChain\n\n**LangFlow** is essentially a **visual extension of LangChain**. It combines the powerful backend of LangChain with an **intuitive drag-and-drop interface**. LangFlow allows users who might not be as comfortable writing code to still leverage the power of language models in their applications.\n\n\n### Strengths:\n\n* **Visual Workflow Creation**: Like LangGraph, LangFlow provides a visual interface for building workflows. However, it’s specifically built on top of LangChain, meaning users can harness LangChain’s power without needing to write extensive code.\n* **Ideal for Rapid Prototyping**: LangFlow is perfect for quickly **prototyping ideas** or building out proof-of-concept applications.\n* **Beginner-Friendly**: It’s a great entry point for users who are less familiar with coding but want to create language model workflows.\n\n\n### Example Use Case:\n\nIf you want to quickly build a **summarization tool** that retrieves documents, you can drag and drop the components in LangFlow’s interface to create a fully functioning application. This can be done without writing much code, if any.\n\n\n### When to Use LangFlow:\n\nLangFlow is perfect for **non-developers** or **rapid prototyping**. If you want to experiment with **LLM workflows quickly** without delving into the code, this tool makes it easy to get started.\n\n**Key points**:\n\n* If you want to prototype LLM workflows quickly without writing code.\n* If you’re comfortable with visual programming but need the flexibility of LangChain.\n* For educational purposes, to help users learn how workflows can be constructed.\n\n\n## 4. LangSmith: Monitoring and Observability\n\nWhile the other tools focus on **building workflows**, **LangSmith** is designed for **monitoring** and **debugging** language model applications. It provides advanced observability features to track the performance of your workflows and models, making it invaluable for production environments.\n\n\n### Strengths:\n\n* **Deep Observability**: LangSmith allows developers to monitor language model performance, ensuring that workflows behave as expected.\n* **Error Tracking**: It excels at helping developers track down issues, making debugging easier.\n* **Performance Insights**: LangSmith gives insights into **workflow performance**, helping developers optimize their applications.\n\n\n### Example Use Case:\n\nLet’s say you’ve deployed a **customer service chatbot** that uses a language model to answer questions. Over time, you notice that some responses are less accurate than expected. LangSmith can help you trace the problem by providing visibility into each decision point within the workflow.\n\n\n### When to Use LangSmith:\n\nIf you’re deploying applications in **production environments** and need to ensure **robustness, reliability, and performance**, LangSmith is an essential tool. It’s particularly useful when managing **complex systems that require debugging and optimization** over time.\n\n**Key points**:\n\n* If you need advanced monitoring or debugging capabilities in LLM workflows.\n* For development environments where observability is key to ensuring optimal model performance.\n* If your focus is on improving and iterating LLM-powered applications based on real-time insights.\n\n\n## Which One to Choose?\n\n* **Use LangGraph** if you prefer graph-based, visual workflows for building complex LLM tasks. Ideal for users who need clarity and structure.\n* **Use LangChain** if you need a robust, flexible solution for creating language model applications programmatically. It’s versatile and great for developers building production-level applications.\n* **Use LangFlow** if you want the power of LangChain with a visual, no-code/low-code interface. Best for rapid prototyping and users who prefer visual tools over coding.\n* **Use LangSmith** if your focus is on observability and debugging of LLM applications. Ideal when you need to monitor and optimize workflows in a development or production environment.\n\nUltimately, your choice depends on your comfort with code, the complexity of your workflows, and whether you prioritize ease of use, flexibility, or observability.\n\n\n## Conclusion\n\nEach of these tools — **LangGraph**, **LangChain**, **LangFlow**, and **LangSmith** — caters to different stages of developing and managing language model applications. **LangGraph** provides a visual, intuitive way to build complex workflows, while **LangChain** offers a robust, code-first solution for developers looking to create scalable applications. For those who prefer a **low-code**, drag-and-drop approach, **LangFlow** simplifies the process without sacrificing power. Finally, **LangSmith** focuses on observability and debugging, ensuring that your workflows are optimized and reliable. Choosing the right tool depends on your project needs, whether it’s for rapid prototyping, production-level scaling, or monitoring and performance tracking.\n\nHappy coding! 🎉\n\n👨🏾‍💻 [GitHub](https://github.com/mdmonsurali) ⭐️ | 👔[LinkedIn](https://www.linkedin.com/in/mdmonsurali/) |📝 [Medium](https://medium.com/@monsuralirana)\n\nThank you for your time in reading this post!\n\nMake sure to leave your feedback and comments. See you in the next blog, stay tuned 📢\n\n\n## References:\n\n1. “LangChain Documentation” — <https://python.langchain.com/docs/introduction/>\n2. “LangGraph Overview” — <https://langchain-ai.github.io/langgraph/>\n3. “LangFlow GitHub Repository” — [https://github.com/LangFlow/LangFlow](https://docs.langflow.org/)\n4. “LangSmith Introduction” — <https://www.langchain.com/langsmith>\n5. “How to Build Chatbots With LangChain” by JetBrains blog — <https://blog.jetbrains.com/pycharm/2024/08/how-to-build-chatbots-with-langchain/>\n\n"},{"lang":"en","group":"blog","slug":"blog/large-language-models-just-got-a-whole-lot-smaller-f93425ee59a2","frontmatter":{"title":"Large Language Models Just Got A Whole Lot Smaller","meta_title":"Large Language Models Just Got A Whole Lot Smaller","description":"And how this might change the game for software startups","date":"2024-11-04T12:29:02.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*1PeFyz_Dlt6jEf27Q9Y33Q.png","categories":["Programming","Technology","Machine Learning"],"author":"Rifx.Online","tags":["compression","optimization","ternary","parallelism","hardware"],"draft":false,"slug":"blog/large-language-models-just-got-a-whole-lot-smaller-f93425ee59a2"},"content":"\n### And how this might change the game for software startups\n\n\n\n**This piece was co\\-written with [David Meiborg](https://readmedium.com/undefined).**\n\n*TLDR: Large Language Models (LLMs for short) are currently huge, costly to run, and have a [significant carbon footprint](https://arxiv.org/abs/2309.14393). Recent advancements in model compression and system\\-level optimization methods might, however, enhance LLM inference. In particular, an approach using parameters with ternary structure has the potential of circumventing much of the costly matrix multiplication that is standard today. This has exciting consequences for hardware startups making specialized chips, but also for software startups that use or custom\\-build their own LLMs. Startups that help their customers deploy LLMs might also have more business coming for them.*\n\nLarge language models today are big. Like, really big. If you want to load a LlaMa\\-2–70B model, you’d need 140 GB of VRAM (that’s 70 billion parameters multiplied by 2 bytes per parameter). For comparison, GPUs like the NVIDIA RTX 3090 or 4090, have just 24 GB of VRAM — a fraction of what one would need.\n\nThere are some [workarounds with quantization](https://towardsdatascience.com/run-llama-2-70b-on-your-gpu-with-exllamav2-588141a88598), but these tend to be cumbersome. Likely you will still have your GPU running hot for up to 15 hours until the model is loaded. Not to mention that you still need some spare memory for inference, or in other words for deploying the model.\n\nUsing current\\-day LLMs is therefore costly: One typically needs multiple high\\-end GPUs to keep the model, and must then account for the energy costs from inference.\n\nThis is why lots of research is going into applying techniques that make LLMs smaller and thus cheaper to run on smaller hardware. It is a tough trade\\-off in most cases, because making LLMs smaller usually impacts their quality. Finding the point where cost equals benefits can be tricky.\n\nIn this piece, we give an overview of promising optimization approaches, explain a recent breakthrough from Microsoft researchers, provide a brief overview of innovative startups in the field of “efficient LLMs” and derive some general implications for startups operating in the LLM ecosystem.\n\n## How LLMs are getting more resource\\-efficient\n\nTech giants like Microsoft and OpenAI, Meta, or Google have sufficient resources to train up cutting\\-edge models even if the training cost is currently prohibitive for most other companies. The biggest bottleneck to widespread adoption therefore is not training but inference efficiency. In other words, although Meta has published LlaMa, it still isn’t being adopted enough because running — not creating — the model is already challenging enough.\n\nResearchers, however, are starting to increase this inference efficiency. Broadly speaking, there are two approaches to this: **System\\-level optimizations** do not change the model itself but rather make it work better by changing key aspects of the environment that it is in. **Model optimizations** compress the model so that it is easier to deploy and run.\n\nThere is a variety of different techniques for either approach. [A recent paper](https://arxiv.org/pdf/2402.01799.pdf) by researchers summarize these techniques excellently. Because these techniques might soon become basic knowledge for anyone working on systems with LLMs, we give a quick overview over these techniques below.\n\n### System\\-level optimization\n\nSystem\\-level optimization refers to changing not the model itself, but how it is run across the hardware. As it turns out, plenty of levers can be pulled to avoid resources sitting around idle or wiping out other inefficiencies.\n\n**Paged Attention**\n\nAt the heart of LLMs like GPT is the attention mechanism. This mechanism allows the model to focus on different parts of the input text when generating each word of the output text. Imagine you are reading a book and highlighting important sentences to remember the story better. Similarly, the attention mechanism “highlights” or gives more importance to certain words or phrases when making predictions.\n\nThis mechanism is very resource\\-intensive. It requires the model to consider the relationships between all pairs of words in the input text. For long texts, this can require a lot of memory and computational power.\n\nInstead of processing the entire text at once, paged attention divides the text into smaller “pages” or segments. The model then processes these pages one at a time or in smaller groups. This approach significantly reduces the amount of memory needed at any given time because the model doesn’t need to keep track of the entire text’s relationships simultaneously.\n\nThis is a bit like a student who would be overwhelmed by reading the entire year’s textbook at once. By breaking it down into manageable segments over the school year, the student can memorize the contents of the textbook.\n\nBy requiring less memory for each step, paged attention allows for the use of larger models or longer texts within the same hardware constraints.\n\n**Tensor Parallelism**\n\nParallelism is a well\\-known concept in computing. It means dividing a large computational task into smaller parts that can be processed simultaneously by multiple processors or computers. This significant speeds up the time a program needs to run.\n\n[Tensors](https://towardsdatascience.com/what-is-a-tensor-in-deep-learning-6dedd95d6507), in the context of LLMs, are multi\\-dimensional arrays of numbers. These tensors are used to represent the data processed by the models. Such data includes input text; model weights, i.e. parameters that the model learns; and output predictions.\n\nPutting both concepts together, tensor parallelism involves splitting these tensors across multiple GPUs or other processing units. For example, if a model’s parameters (weights) are too large to fit into the memory of a single GPU, they can be divided across multiple GPUs. Each GPU then processes only a portion of the tensor at a time.\n\nJust like a team of multiple people working on a large project, the processing units need to exchange information as they work on their respective parts of the tensors. For instance, the results of computations on one GPU might need to be shared with another GPU to continue the next step in the computation. Efficient communication between the units is therefore crucial for the effectiveness of tensor parallelism.\n\nIn short, tensor parallelism is a way of breaking down the computations needed for LLMs into smaller, parallel tasks that can be handled simultaneously by multiple computing units, leading to faster training and inference times for these large and complex models.\n\n**Pipeline Parallelism**\n\nThis technique focuses on improving the workflow of processing data through the model’s layers. This can significantly speed up the overall computation and make better use of available hardware.\n\nA pipeline in computing works similarly to a factory assembly line, with different stages of a task being completed in sequence. This allows for multiple tasks to be worked on simultaneously but at different stages.\n\nIn LLMs, these different stages are represented by layers of neural networks. Each layer processes the input data in sequence, gradually extracting more complex features or patterns until it produces the final output. Think of each layer as a worker in the factory assembly line: Each worker adds something to the input data as it passes through, until finally a complex product emerges.\n\nIn pipeline parallelism, the model’s layers are divided into segments, and each segment is assigned to a different GPU or processing unit. This way, the model can be fed on batches of data: Once the first segment is through with the first batch, the second segment takes that batch, and the first segment takes a fresh, new batch on.\n\nThis creates a continuous flow of data through the model where each segment of the model is working on a different piece of data at any given time. This maximizes the use of available hardware resources by keeping all parts of the model active and reduces the idle time that can occur when a single processor waits for tasks to complete.\n\nPipeline parallelism, which was discussed earlier, operates at the level of model layers, distributing the sequential processing stages across devices. Tensor parallelism, on the other hand, operates at a more granular level, distributing the actual computations (e.g., parts of a large matrix multiplication) that occur within layers across devices.\n\n**CPU/GPU Offloading**\n\nWe have talked a lot about GPUs in this piece. Nevertheless, ot all tasks in training or running an LLM are equally suited to GPUs. Some tasks, like data preprocessing or certain control logic, might be more efficiently handled by a CPU. Other tasks, particularly the heavy mathematical computations involved in processing neural networks (like matrix multiplications), are indeed more efficiently executed on GPUs.\n\nBy offloading specific tasks to the processor best suited for them — GPUs for parallelizable, computation\\-heavy tasks, and CPUs for sequential or logic\\-intensive tasks — systems can ensure that each part of the workload is processed in the most efficient manner possible.\n\n**Fused Operations**\n\nFused operations take multiple processing steps that would normally be executed separately and combine them into a single, streamlined operation. For instance, instead of doing a matrix multiplication and then an addition, a fused operation would do both at once.\n\n**Speculative Decoding**\n\nWhen generating text, LLMs calculate the probabilities of what the next word in a sentence might be, based on the words that have come before. Traditionally, after each word is generated, the model recalculates to determine the next word, and this process repeats until the full sentence or paragraph is completed. This sequential process can be slow, however, especially for longer texts or more complex models, because each step depends on the completion of the previous step.\n\nParallel Predictions: Instead of waiting for each word to be chosen before considering the next, speculative decoding allows the model to “speculate” or make multiple predictions about what the next few words could be at the same time. This is called *parallel predictions*. It’s like making educated guesses about several possible paths the sentence could take next\n\nBy exploring these possibilities in parallel, the model can potentially reduce the overall time it takes to generate text. Once the actual next word is selected, the model can more quickly proceed along the most likely path, having already computed the subsequent options.\n\n### Compression of LLM Models\n\nResearchers have in the past explored model compression. With the advent of large\\-scale LLMs, however, this has become a bigger challenge.\n\nMany established compression methods rely on the paradigm of executing fine\\-tuning steps to regain lost performance during the compression stage. This approach has significant limitations, however, when applied to LLMs because of their sheer size. LLM compression has therefore become a whole new research domain.\n\n**Architecture pruning**\n\nWhen you prune an apple tree, you cut off certain branches in winter early spring. This ensures that the tree doesn’t waste resources on unproductive branches or catches diseases from dead wood. This helps it produce better fruit.\n\nLLMs, of course, don’t produce fruit. In this context, pruning is a method used to reduce the size of the model while trying to maintain or minimally impact its performance.\n\nLLM models have millions or even billions of parameters. Not all of these parameters are equally important for the model to make predictions or understand language. Some parameters are used rarely or don’t contribute much to the model’s decisions: Eliminating these redundant or less impactful connections, neurons, or entire layers hence makes the model more efficient to use.\n\nChoosing which parameters to prune is not a trivial task. In magnitude\\-based pruning, the weights of the neural network with the smallest absolute values are removed. Before training, such weights are usually zero; after training, they are typically somewhere between \\-1 and 1\\. If training did not affect a weight very much, then it is likely close to zero, and thus contributes less to the model’s decisions.\n\nA more resource\\-intensive but also more robust pruning technique is sensitivity analysis. This involves evaluating the impact of removing each parameter, or group of parameters, on the model’s performance. Parameters whose removal causes the least degradation in performance are pruned.\n\nThere are other techniques as well, but generally one can classify them as unstructured or structured pruning. Unstructured pruning (e.g. magnitude\\-based pruning) removes individual weights, leading to a sparsely connected neural network. Structured pruning (e.g. sensitivity analysis) removes entire units or layers (e.g., a whole neuron or channel), which can be more effective for computational efficiency on certain hardware.\n\nAfter pruning, the model often undergoes a fine\\-tuning process. This involves retraining the pruned model on the training dataset or a subset of it. The goal is to allow the model to adjust and optimize its remaining parameters to compensate for the loss of the pruned ones. This helps in recovering any performance that was lost due to pruning.\n\nThis can be done in an iterative or in a one\\-shot approach. In iterative pruning, the model is pruned iteratively over several rounds. After each round, the pruned model is retrained to regain performance lost due to pruning. This cycle can be repeated multiple times, with the model potentially becoming more robust and maintaining performance even with significantly fewer parameters. In one\\-shot pruning, all the identified parameters are removed at once, and the model is then fine\\-tuned.\n\n**Knowledge distillation**\n\nImagine there is a football court with two players: One is very experienced and knows many tricks, the other is a beginner. The experienced player knows much more than the beginner, but the beginner can quickly get to a comparable behavior by mimicking the other player’s behavior on the field.\n\nKnowledge distillation for LLMs works similarly: It is the process of training a smaller (student model), more efficient model to replicate the performance of a larger model (teacher model) by learning from its outputs and the way it processes information.\n\nTo apply this technique, you obviously need a large teacher model, e.g. one of the large open\\-source models from LlaMa or Mistral. Then you need to design a smaller neural network that has significantly fewer parameters than the teacher model.\n\nInstead of training the student model solely on the original hard targets, i.e., the ground truth data labels, it is also trained on the soft targets. These are the probabilities produced by the teacher model for the same inputs. For example, for a given set of queries, imagine that the teacher answers it as ‘A’ 70 percent of the time, ‘B’ 20 percent of the time, and ‘C’, ‘D’, or ‘E’ 10 percent of the time. Not only will the student model try to get the answer to every question right; it will also try to follow the same probability distribution over a set of queries.\n\nSuch soft targets carry more information per example than hard labels because they include the teacher model’s confidence levels across all possible outcomes. This is how to the student model is able perform similarly to the teacher but with less computational expense.\n\nAfter the initial knowledge distillation, the student model might be further fine\\-tuned on the task\\-specific dataset with hard labels to maximize its performance.\n\n**Low rank approximations**\n\nLLMs work by processing and generating text based on incredibly large matrices (i.e., veeeeery big tables of numbers) that represent the relationships between words, their meanings, and how they’re used in language. These matrices can be so large that they’re hard to work with, especially when it comes to storage and computation.\n\nA low\\-rank approximation involves finding a simpler matrix that is much smaller in size but still captures the most important information of the original large matrix. It is a bit like reducing a detailed painting to a sketch.\n\nThis is done through mathematical techniques that identify which parts of the matrix (or painting, in our analogy) hold the most information and condense the matrix to just those parts. There are mathematical techniques, notably [singular value decomposition](https://www.cs.cmu.edu/~venkatg/teaching/CStheory-infoage/book-chapter-4.pdf), which help with this.\n\nIn contrast to pruning, low rank approximation performs matrix dimensionality reduction, maintaining the structure of the model but representing it in a more compact form, while pruning directly removes parts of the neural network.\n\n**Quantization**\n\nLLMs process text using a vast number of mathematical calculations. These calculations are performed using numbers that can have a wide range of values. Typically, these numbers are stored in a format that can represent a very wide range of values ([floating\\-point format](https://de.wikipedia.org/wiki/Einfache_Genauigkeit)), occupying 32 bits in memory.\n\nQuantization reduces the precision of those numbers, typically from 32\\-bit floating\\-point numbers to lower bit\\- width representations, such as 8\\-bit integers. This means that instead of using numbers with a lot of decimal places, the model uses “simpler” numbers, making the calculations faster and requiring less memory.\n\nQuantization\\-Aware Training (QAT) involves training the model with quantization in mind, allowing it to adapt to the precision loss and usually resulting in better performance but at the cost of more complex and resource\\-intensive training processes.\n\nPost\\-Training Quantization (PTQ) applies quantization after the model has been fully trained, offering a simpler and faster approach to reduce computational demands. However, it may not achieve the same level of accuracy or performance as QAT due to the model not being specifically optimized for lower precision operations.\n\n### The Era of 1\\-bit LLMs?\n\nMicrosoft researchers recently [made waves with a paper](https://arxiv.org/pdf/2402.17764.pdf) that stores each parameter not in 16 bits, as is currently [the standard](https://en.wikipedia.org/wiki/Half-precision_floating-point_format) in LLMs, but in a mere 1\\.58 bits. This is huge news: With this technique, they achieved almost 10 times more token throughput, i.e., it processes text almost 10 times as fast. They also reduced their memory footprint by a factor of 3\\.5, which means that they need a lot less hardware to run these models on.\n\nThis was achieved using a ternary bit. Instead of using a floating\\-point number between \\-1 and 1, as is usually the case and typically uses 16 bits, every weight is expressed as either \\-1, 0, or 1\\. These numbers can be stored on 1\\.58 bits, because for 3 possible values on a binary transistor one gets that 2¹.58 \\= 3\\. Using only numbers this simple also means that complicated matrix multiplication is no longer necessary, which makes it a lot more resource\\-efficient.\n\nWhat is baffling about this technique is that it achieves a similar output performance as traditional 16\\-bit models at a size of 3 billion parameters. It is not yet clear whether this kind of model scales up as well as traditional models, when passing the threshold of 13 billion or more parameters. What is clear is that even at 70 billion parameters it is more efficient, in terms of latency, memory usage and energy consumption, than a traditional model with only 13 billion parameters. The quality of the output remains to be tested in detail.\n\nOne other disadvantage is that state\\-of\\-the\\-art quantization of existing LLMs cannot be used to produce a 1\\.58\\-bit model. Such models need to be created from scratch, which, despite its dramatically lowered cost, will put it out of reach of the average citizen for now.\n\nIf and when such models have been created and work well, however, inference should become a lot easier. 1\\.58\\-bit LLMs might even be deployed on edge and mobile devices. They are also a lot friendlier to CPU devices — which most mobile devices run on — which makes them easier to deploy on cheaper chips. All this has a lot of advantages, for example for privacy, but allow allows for new applications that humanity hasn’t even dreamt of yet.\n\nMoreover, startups like [Groq](https://groq.com/) have demonstrated promising results and great potential for building specific hardware [like LPUs](https://wow.groq.com/why-groq/) for LLMs. LLM\\-specific hardware is already a [huge market](https://finance.yahoo.com/news/generative-ai-market-size-expected-163500846.html#:~:text=%2D%20Large%20Language%20Model%20(LLM),the%20forecast%20period%202023%2D2029.). Findings like these might make this market grow even more aggressively than analysts have foreseen to date.\n\nIf nothing else, inference will become dirt cheap due to a combination of quantization techniques and specialized hardware. This has implications for many companies, including startups.\n\n## What do lighter LLMs mean for startups?\n\n### The boom in AI hardware has just begun\n\nBetween 1971 and 1999, CPUs were pretty much [the only microprocessors](https://cs.stanford.edu/people/eroberts/courses/soco/projects/2005-06/64-bit-processors/history1.html) on the market. Then [NVIDIA introduced](https://readmedium.com/a-brief-history-of-gpu-47d98d6a0f8a) its GPU. It was not technically the world’s first GPU; however, it was the one of the first microprocessors that made gaming an accessible and immersible experience. (Gaming eats a lot of computing power — if you didn’t know, now you know!)\n\nFrom gaming, GPUs quickly proliferated to do many different tasks, including scientific image processing, linear algebra, 3D reconstruction, and more. One thing that GPUs do particularly well? Machine learning and LLMs. Many of NVIDIA’s chips today are being used for training LLMs.\n\nSince then, other microprocessors have started to proliferate. [Google’s TPUs](https://cloud.google.com/tpu?hl=en), introduced in 2016, are particularly well\\-suited for AI training and inference. While GPUs turned out to be great for LLMs, TPUs were specifically designed for this purpose. They are well\\-suited both for training and inference.\n\nThe industry is [at a turning point](https://www.wsj.com/tech/ai/how-a-shifting-ai-chip-market-will-shape-nvidias-future-f0c256b1), however: Soon, the majority of LLM\\-related work will be inference, and no longer training, as users start deploying models such as LlaMa. New and innovative AI semiconductor companies now have a chance to enter the game.\n\nThis includes chipmaker [Groq](https://wow.groq.com/press/) which focuses on particularly speedy inference processors. Other startups include [Cerebras](https://www.cerebras.net/) (which focuses on training), [Graphcore](https://www.graphcore.ai/about) (which covers training and interference), and [SambaNova](https://sambanova.ai/) (also training and inference). More established competitors like Intel and AMD are also eyeing both training and inference, although most growth is expected to come from the latter in the coming years. The big tech giants — Google, Amazon, or Microsoft — are also developing AI\\-specialized chips, but mostly for in\\-house use.\n\nOverall, the hardware market for LLMs is still dominated by datacenter applications. Edge and mobile applications are the next logical step, but will require more breakthroughs like the 1\\.58\\-bit approach that Microsoft researchers recently published (see above).\n\n## The impact for LLM software companies\n\nLooking at the whole value chain in the emerging AI space, the developments we outlined above are likely to lead to **significantly reduced costs for running/consuming LLMs**.\n\nSome of our thoughts on where this will lead to:\n\n* **Great B2C products**, because lower LLM costs mean that you can build freemium B2C experiences with a high LLM consumption (frequency \\& scale — e.g. long context window) without ruining company unit economics.\n* Democratization of access on a global scale, allowing **users in lower\\-income countries** to utilize advanced AI technologies\n* Companies can automate a wider range of tasks, leading to **increased efficiency and productivity** (“I don’t care anymore if I have 10k API calls per hour”)\n* New edge AI hardware combined with smaller models will lead to **new edge AI use cases** becoming feasible that were “data\\-center”\\-only before\n* As edge hardware explodes, we believe opportunity opens up to build software companies that help customers to bring AI models to the fragmented space of tailored edge devices (“give me your model, I compress it with various techniques, test it on 10 different edge devices, tell you what works best and then help you to deploy it”)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/llama-3-1-405b-how-to-use-for-free-9aaf3561932d","frontmatter":{"title":"Llama 3.1 405B — How to Use for Free","meta_title":"Llama 3.1 405B — How to Use for Free","description":"No Local Install Needed","date":"2024-10-29T05:09:24.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*db_ND6LyQ5_p5jFJCTo5GQ.jpeg","categories":["Programming","Technology","Generative AI"],"author":"Rifx.Online","tags":["Llama","Meta","HuggingChat","Groq","API"],"draft":false,"slug":"blog/llama-3-1-405b-how-to-use-for-free-9aaf3561932d"},"content":"\n\n\n\n\n### No Local Install Needed\n\n**Llama 3\\.1 405B** is Meta’s most advanced AI model released in July 2024 — **but where can you try it*?***\n\n\n\n**LLama 3\\.1** comes in different versions, including the largest model with 405 billion parameters and smaller versions like the 70B and 8B models.\n\nThe easiest way to try the 70B and 8B models is on [Groq](https://console.groq.com/playground)— where you can try them directly on their playground.\n\nBecause of the overwhelming demand, the most powerful 405B model isn’t usually available though.\n\nThis guide is for anyone including developers who wants to try Llama 3\\.1 405B for free — without needing to download and install it.\n\nIf you don’t have a paid Medium account, you can read for free [here](https://addison-best.medium.com/9aaf3561932d?source=friends_link&sk=5fa532d1caaec229a0b9a445d8749449)\n\nIf you are a developer, and you want to try **LLama 3\\.1 405B for free using an API —** you can skip to the end of the article.\n\n\n## Where Can I Use Llama 3\\.1 405B for Free?\n\nYou can download and install it directly from [Meta](https://llama.meta.com/) — but it is huge and you’ll need 100’s of Gigabytes of space and a powerful computer to try it properly.\n\nBut you can also try now without downloading.\n\nHere are some options where you can try it:\n\n**If you want to learn more AI tips and tricks to help grow your business and earn more money online:**\n\n***👉*** *Sign up for our **[free 5\\-day email course](https://aigrowthguys.com/5-day-free-course-how-to-grow-your-business-like-a-weed)** to grow 🚀 and earn**💲👈***\n\n\n## 1\\. Use Llama 3\\.1 405B on Meta AI\n\nIf you’re in the U.S. and it seems at least Canada (where I am), you can chat with the Llama 3\\.1 405B model through Meta AI. Visit the [Meta AI website](https://www.meta.ai) and log in using your Facebook or Instagram account.\n\nIt might also be available in other countries now, so have a look.\n\nWhen you sign in — hopefully you’ll see an option to try **Llama 3\\.1 405B**.\n\nIf you can, you’ll see a message like in the below screenshot.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*cw1WMKhdZhzUp0L3Kn7Qng.png)\n\nYou can also access it via WhatsApp by linking your Meta account.[**Try it on Meta AI**](https://www.meta.ai)\n\nYou can also try their **Imagine** photo creator and AI image editor**.**\n\nThe cartoon image with the Lama and computer was created using this at the start of the article.\n\n**I prompted**\n\n\n> **Imagine: i want a fun cartoon image for a medium article showing trying to use llama 3\\.1 405B**\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8MeC_M2O7UX7ulPOfUCuHA.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*dIG62eA7YAT3mpLA0etz9Q.png)\n\nIt’s worth trying. I don’t think it is comparable to Flux.1 or Midjourney — but it is easy to use and free\n\n\n## 2\\. Use Llama 3\\.1 405B on HuggingChat\n\nHuggingChat is available to users outside the U.S. and provides access to the Llama 3\\.1 405B model. You can start chatting right away without signing up, making it an easy way to explore the model’s capabilities. Visit the [HuggingChat page](https://huggingface.co) to begin.[Try it on HuggingChat](https://huggingface.co)\n\n\n## 3\\. Use Llama 3\\.1 405B on Groq\n\n**How:** Groq initially hosted the Llama 3\\.1 405B model but now offers the smaller 70B and 8B versions due to high demand. You can explore these models by creating a free account on [Groq’s website](https://groq.com).[Try it on Groq](https://groq.com)\n\n\n## 4\\. Use Llama 3\\.1 405B on Perplexity\n\nPerplexity offers a simple way to interact with Llama 3\\.1, designed for quick and easy access to the model. You can start using it by visiting the Perplexity AI platform. But this is only available in the Pro plan.[Try it on Perplexity](https://www.perplexity.ai)\n\n\n## 5\\. Use Llama 3\\.1 405B on Poe\n\nPoe by Quora is another platform where you can try Llama 3\\.1\\. Poe allows users to explore different AI models, including Llama 3\\.1, through a chat interface. It’s a versatile option if you want to compare Llama 3\\.1 with other AI models in one place. You can try 3\\.1 405B for free — with a limited amount of daily free credits.[Try it on Poe](https://poe.com)\n\n\n## Where Can I Use Llama 3\\.1 405B for Free with API?\n\nIf you are a developer and want to try LLama 3\\.1 405B version completely free — you currently have limited options.\n\nBut I wanted to give you an easy and free option to get you started.\n\nYou can currently try on [together.ai](https://together.ai) for free.\n\nYou get $5 of free credit and an API key to try it.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*w8LOXw-Wm0QTz5YgvZ27ug.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*YpKURkmy--xstoJpZ4fmbw.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*g0FxHkg6gq5OMXXo1Yzr0A.png)\n\nThis was the easiest way I found to test the Llama 3\\.1 405B version quickly\\- and free.\n\nThis is a great option for developers wanting to try using it with an API for free.\n\n\n## Note:\n\nIf you want our team to create custom AI software using LLMs, or a custom AI chatbot for your business, you can [**contact me**](https://aigrowthguys.com/contact/) ✉️ here and I’ll get back to you quickly:\n\n[**AI Growth Guys Contact**](https://aigrowthguys.com/contact/)✉️\n\n👉 Sign up to our [**free 5\\-Day email course**](https://aigrowthguys.com/5-day-free-course-how-to-grow-your-business-like-a-weed/) to grow 🚀 and earn💲in the AI age\n\nYou can also [**sign up for my newsletter**](https://ai-growth-guys.beehiiv.com/subscribe/?via=andrew-best) on how to use AI to earn more money.\n\nCheck out our [**YouTube Channel**](https://www.youtube.com/@aigrowthguys)\n\nFollow us at our website: [**AI Growth Guys**](https://aigrowthguys.com/)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/meet-ministral-3b-and-8b-edge-ai-game-changers-3f7532da8f90","frontmatter":{"title":"Meet Ministral 3B and 8B: Edge AI Game-Changers","meta_title":"Meet Ministral 3B and 8B: Edge AI Game-Changers","description":"Mistral AI’s New Frontier in Edge AI and On-Device Computing","date":"2024-11-01T03:55:06.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*3CmWlEiW7ea8gtqxpI83_w.png","categories":["Technology","Autonomous Systems","Data Science"],"author":"Rifx.Online","tags":["Mistral","edge","computing","translation","robotics"],"draft":false,"slug":"blog/meet-ministral-3b-and-8b-edge-ai-game-changers-3f7532da8f90"},"content":"\n\n\n\n\n### Mistral AI’s New Frontier in Edge AI and On\\-Device Computing\n\nIn the rapidly evolving landscape of AI, edge computing has become increasingly crucial for applications that demand low\\-latency, privacy\\-first, and efficient inference without relying on cloud\\-based infrastructure.\n\nThe launch of [**Ministral**](https://mistral.ai/news/ministraux/)family of models, the latest innovation from **Mistral AI**, represents a groundbreaking step forward in the realm of AI.\n\nTo mark the first anniversary of its groundbreaking **Mistral 7B** model, Mistral AI has unveiled its next generation of language models: **Ministral 3B** and **Ministral 8B**, collectively known as “[**les Ministraux**](https://mistral.ai/news/ministraux/)”. These models aren’t just incremental improvements; they represent a significant leap in what’s possible with edge AI.\n\n\n\n\n## Why These Models Matter?\n\nEdge AI is all about performing complex computations locally, ensuring data privacy and reducing response times. With **Ministral 3B** and **Ministral 8B**, Mistral AI offers models that combine high computational power with memory efficiency, all while running directly on the device. These models are designed to deliver real\\-time insights for applications that can’t afford latency or depend on cloud connectivity.\n\n\n## Key Features:\n\n1. **State\\-of\\-the\\-Art Performance**: Outperforms existing models in different tasks such as knowledge, commonsense, reasoning, native function\\-calling, and efficiency within the sub\\-10B category.\n2. **Large Context Window**: Support for up to 128k context length, enabling more comprehensive understanding and generation.\n3. **Efficient Architecture**: Ministral 8B features a special interleaved sliding\\-window attention pattern for faster and more memory\\-efficient inference.\n4. **Versatility**: Suitable for a wide range of applications, from on\\-device translation to autonomous robotics.\n5. **Privacy\\-First Design**: Built for local inference, these models are perfect for applications that prioritize data privacy, eliminating the need for constant cloud access.\n6. **Scalability**: Whether you need low\\-power consumption for smaller devices with Ministral 3B or greater capabilities with the 8B variant, both models are flexible enough to be adapted to various use cases.\n\n\n> For benchmarking results, refer [here](https://mistral.ai/news/ministraux/)\n\n\n## Breaking Down the Models:\n\n\n### Ministral 3B:\n\n* With just **3 billion parameters**, it provides a balanced approach for resource\\-constrained environments\n* Supports up to **128k context length**, allowing for comprehensive handling of complex queries\n* Ideal for ultra\\-low\\-latency applications\n* Outperforms many other models in its category\n\n\n### Ministral 8B:\n\n* With **8 billion parameters** and **128k context length**, it tends to deliver enhanced computational power for more demanding tasks\n* Features a **sliding\\-window attention** pattern for improved speed and memory efficiency\n* Informed by a wide range of **multilingual** and **code** data, making it suitable for diverse applications\n* Supports **function calling**\n* Balances performance and efficiency for demanding applications\n* Vocabulary size of **131k**, using the **V3\\-Tekken** tokenizer\n* Prompt Template:\n\n\n```python\n<s>[INST]user message[/INST]assistant response</s>[INST]new user message[/INST]\n```\n\n## Use Cases:\n\nThese models deliver compute\\-efficient and low\\-latency performance, making them ideal for the following scenarios:\n\n* **On\\-Device Translation**: Empowering users to communicate seamlessly across languages in real\\-time, even in areas with less internet connectivity.\n* **Internet\\-less Smart Assistants**: Supporting intelligent virtual assistants that function independently of cloud connectivity, enhancing user experience in privacy\\-sensitive environments.\n* **Local Analytics**: Enabling organizations to analyze data in real\\-time while maintaining strict privacy standards, which is essential in sectors such as healthcare and finance.\n* **Autonomous Robotics**: Equipping robots with advanced language capabilities for autonomous decision\\-making and communication, enhancing their operational efficiency in various industries.\n\nIn addition to their standalone capabilities, les Ministraux can work in conjunction with larger models like Mistral Large. This synergy allows them to serve as efficient intermediaries for **function\\-calling in agentic workflows**, handling:\n\n* **Input Parsing**: Quickly interpreting user input to ensure accurate responses.\n* **Task Routing**: Directing requests to the appropriate resources based on user intent.\n* **API Calls**: Executing API functions in real\\-time, ensuring smooth interactions across various contexts.\n\n\n## Code Usage (with vLLM):\n\nThe [Ministral\\-8B\\-Instruct\\-2410](https://huggingface.co/mistralai/Ministral-8B-Instruct-2410) Language Model is an instruct fine\\-tuned model that can be efficiently deployed using vLLM. You can find it [here](https://huggingface.co/mistralai/Ministral-8B-Instruct-2410) on Hugging Face. Here’s how you can get started:\n\n\n### Installation\n\nFirst, ensure you have the latest versions of vLLM and mistral\\_common installed:\n\n\n```python\npip install --upgrade vllm\npip install --upgrade mistral_common\n```\n\n> ***Note****: vLLM version 0\\.6\\.2 or higher is required.*\n\n\n### Offline Usage with vLLM\n\nHere’s an example of how to use Ministral\\-8B in offline mode with vLLM:\n\n\n```python\nfrom vllm import LLM\nfrom vllm.sampling_params import SamplingParams\n\nmodel_name = \"mistralai/Ministral-8B-Instruct-2410\"\nsampling_params = SamplingParams(max_tokens=8192)\n\nllm = LLM(model=model_name, tokenizer_mode=\"mistral\", config_format=\"mistral\", load_format=\"mistral\")\n\nprompt = \"What are the potential implications of artificial intelligence on the job market in the next decade?\"\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": prompt\n    },\n]\n\noutputs = llm.chat(messages, sampling_params=sampling_params)\nprint(outputs[0].outputs[0].text)\n```\n\n### Server Mode Inference with vLLM\n\nIn server inference mode, vLLM runs an HTTP server that concurrently handles client connections and requests via a REST API compatible with the OpenAI protocol. Here’s how to set it up:\n\n* Start the server:\n\n\n```python\nvllm serve mistralai/Ministral-8B-Instruct-2410 --tokenizer_mode mistral --config_format mistral --load_format mistral\n```\n* Make requests to the server:\n\n\n```python\ncurl --location 'http://localhost:8000/v1/chat/completions' \\\n    --header 'Content-Type: application/json' \\\n    --header 'Authorization: Bearer token' \\\n    --data '{\n        \"model\": \"mistralai/Ministral-8B-Instruct-2410\",\n        \"messages\": [\n          {\n            \"role\": \"user\",\n            \"content\": \"What are the potential implications of artificial intelligence on the job market in the next decade?\"\n          }\n        ]\n      }'\n```\n\n> Important Notes on vLLM Usage:\n\n* Currently, vLLM is capped at a 32k context size due to limitations in implementing interleaved attention kernels for paged attention.\n* To leverage the full 128k context size, it’s recommended to use [Mistral Inference](https://github.com/mistralai/mistral-inference).\n* If you need to reduce GPU memory requirements, you can use tensor parallelism by adding `tensor_parallel=2` to the LLM initialization.\n\nBy following these examples, you can easily integrate Ministral\\-8B into your projects using vLLM, whether you’re running offline inference or setting up a server for multiple clients. The model’s efficiency and powerful capabilities, combined with vLLM’s optimized inference, make it an excellent choice for a wide range of AI applications.\n\n\n## Conclusion:\n\nThe release of Ministral marks a significant milestone in the evolution of AI. By bringing GPT\\-level performance to edge devices, Mistral AI is not just pushing technological boundaries — they’re reimagining what’s possible with local, privacy\\-first artificial intelligence.\n\nAs developers, researchers, and businesses begin to explore the capabilities of Ministral, we can expect to see a new wave of AI\\-powered applications that are faster, more private, and more accessible than ever before. The age of edge AI is here, and Ministral is leading the charge.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/mistral-ai-releases-revolutionary-edge-models-ministral-3b-and-8b-superior-performance-and-privacy-5b24f0189493","frontmatter":{"title":"Mistral AI Releases Revolutionary Edge Models Ministral 3B and 8B: Superior Performance and Privacy","meta_title":"Mistral AI Releases Revolutionary Edge Models Ministral 3B and 8B: Superior Performance and Privacy","description":"No subtitle provided","date":"2024-10-31T08:32:15.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*zFNeFlbfEnbjV5M65sH5ig@2x.jpeg","categories":["Technology","Machine Learning","Autonomous Systems"],"author":"Rifx.Online","tags":["edge","models","privacy","tokens","attention"],"draft":false,"slug":"blog/mistral-ai-releases-revolutionary-edge-models-ministral-3b-and-8b-superior-performance-and-privacy-5b24f0189493"},"content":"\n\n\n\nRecently, Mistral AI has launched two new edge models — Ministral 3B and Ministral 8B, which have garnered widespread attention in the tech community. These models not only excel in performance but also offer unique advantages in privacy protection.\n\n\n\n\n## Exceptional Performance, Privacy First\n\nMinistral 3B and 8B are designed specifically for on\\-device computation, capable of processing text information up to 128k in length. Particularly, Ministral 8B employs an innovative sliding window attention mechanism, significantly enhancing computational speed and memory efficiency. Moreover, both models prioritize privacy protection in their design, ensuring data is processed locally to reduce the risk of data breaches.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GMgT6erSorAGUp-pqbXWhA@2x.jpeg)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*zRGh7rw7oVXYd5mOhXoc3g@2x.jpeg)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*IIYgXVtbHvWqn6QLSZ-0Ow@2x.jpeg)\n\n\n## Versatile Applications, Unlimited Potential\n\nThe Ministral series models have a wide range of applications. In the field of smart assistants, they can quickly respond to user commands while ensuring data security; in the field of autonomous robots, their powerful reasoning capabilities support complex decision\\-making and operations.\n\n\n## Cost\\-Effective, Broad Market Prospects\n\nDespite their outstanding performance, Ministral 3B and 8B are highly competitive in price. The 3B is priced at $0\\.04 per million tokens, and the 8B at $0\\.10\\. This pricing strategy provides enterprises and developers with a cost\\-effective option. Currently, both models are available for use.\n\n\n## Promising Future, Leading the New Trend in Edge Computing\n\nThe release of the Ministral series models by Mistral AI demonstrates its deep technical strength in edge computing, laying a solid foundation for future on\\-device AI applications. With technological advancements and deeper application exploration, Ministral models are expected to play a greater role in smart devices and the Internet of Things.\n\nIn summary, the launch of Ministral 3B and 8B is not only a significant milestone for Mistral AI but also a major advancement for the AI industry, bringing new possibilities to on\\-device computation.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*A6SToo3fO3DqnlWX)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/mistral-ai-unveils-ministral-3b-and-8b-models-plus-nvidia-launches-ai-model-that-outperforms-gpt-4-941712f5d22d","frontmatter":{"title":"Mistral AI Unveils Ministral 3B and 8B Models Plus: Nvidia Launches AI Model that Outperforms GPT-4","meta_title":"Mistral AI Unveils Ministral 3B and 8B Models Plus: Nvidia Launches AI Model that Outperforms GPT-4","description":"No subtitle provided","date":"2024-10-31T08:29:07.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*PtPEkgjabwBUu73Y","categories":["Technology","Generative AI","Machine Learning"],"author":"Rifx.Online","tags":["Mistral","edge","Llama","YouTube","DreamTracks"],"draft":false,"slug":"blog/mistral-ai-unveils-ministral-3b-and-8b-models-plus-nvidia-launches-ai-model-that-outperforms-gpt-4-941712f5d22d"},"content":"\n\n\n\n\n### Plus: Nvidia Launches AI Model that Outperforms GPT\\-4\n\n\n\n**Welcome to Get The Gist**, where every weekday we share an easy\\-to\\-read summary of the latest and greatest developments in AI — news, innovations, and trends — all delivered in under 5 minutes! ⏱\n\n**In today’s edition:**\n\n* Mistral AI Unveils Ministral 3B and 8B Models for Edge Computing\n* Nvidia Quietly Launches AI Model that Outperforms GPT\\-4\n* YouTube Rolls Out AI Music Tool “Dream Tracks” to U.S. Creators\n* Google Gemini Can Now Generate Images in Customizable Aspect Ratios\n* And more AI news….\n\n\n## 1\\. Mistral AI Unveils Ministral 3B and 8B Models for Edge Computing\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*qAjoYMHGI1TkNy_A)\n\n**The Gist:** Mistral AI has [**launched two new AI models**](https://analyticsindiamag.com/ai-news-updates/mistral-ai-launches-ministral-3b-and-8b-models-for-edge-computing/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models), Ministral 3B and 8B, designed for efficient on\\-device and edge computing. These models outperform competitors and are tailored for tasks requiring privacy\\-first, local inference.\n\n**Key Details:**\n\n* Models handle large context lengths (up to 128k) for smooth performance in resource\\-limited environments.\n* Ideal for applications like smart assistants, local analytics, and robotics, enhancing task efficiency.\n* Available for commercial use with competitive pricing and research access for the 8B Instruct model.\n* Outperforms AI models like Gemma 2 and Llama 3 in benchmarks.\n\n\n## 2\\. Nvidia Quietly Launches AI Model that Outperforms GPT\\-4\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Mza84SHereM3w5rN)\n\n**The Gist:** Nvidia has [**released a new AI model**](https://venturebeat.com/ai/nvidia-just-dropped-a-new-ai-model-that-crushes-openais-gpt-4-no-big-launch-just-big-results/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models), Llama\\-3\\.1\\-Nemotron\\-70B\\-Instruct, which surpasses industry giants like OpenAI’s GPT\\-4 in performance benchmarks. This launch marks a significant expansion of Nvidia’s AI strategy, shifting from hardware to high\\-performing AI software.\n\n**Key Details:**\n\n* Nvidia’s new model scored higher than GPT\\-4 on key benchmarks, demonstrating superior language understanding and generation.\n* Developed using advanced techniques like Reinforcement Learning from Human Feedback (RLHF), the model excels in handling complex queries.\n* Nvidia offers free access through its platform, allowing businesses to experiment with this powerful AI tool.\n* The model is customizable for business needs but requires careful use in specialized areas like legal reasoning or math.\n\n\n## 3\\. YouTube Rolls Out AI Music Tool “Dream Tracks” to U.S. Creators\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*5nUNrJmdCBBy4JdQ)\n\n**The Gist:** YouTube has launched its [**AI\\-powered music generator**](https://www.mediapost.com/publications/article/400280/youtube-brings-ai-audio-generator-to-us-creators.html?edition=136037&utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models) “Dream Tracks” in the U.S., allowing creators to use text prompts to create custom audio for their short\\-form videos. The tool aims to deepen the connection between artists and fans through music creation.\n\n**Key Details:**\n\n* Powered by Google DeepMind’s Lyria, Dream Tracks generates custom instrumental soundtracks for YouTube Shorts.\n* U.S. creators can now use this tool to create royalty\\-free soundtracks up to 30 seconds long.\n* Users can remix the AI\\-generated audio clips, enhancing creative possibilities.\n* YouTube applies a hidden SynthID watermark to all AI\\-generated tracks to ensure transparency.\n\n\n## Quick Gist\n\n* **Clerk Chat** secured $7 million in funding led by Race Capital to enhance its AI\\-powered business communication platform [(Read More)](https://www.businesswire.com/news/home/20241017292794/en/World%E2%80%99s-First-AI-Telecom-Clerk-Chat-Raises-7.0-Million-in-Seed-Funding?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models).\n* **Anthropic** CEO Dario Amodei published a lengthy blog outlining a utopian vision for the transformative potential of artificial general intelligence, while simultaneously seeking to secure a $40 billion valuation for the company [(Read More)](https://www.theverge.com/2024/10/16/24268209/anthropic-ai-dario-amodei-agi-funding-blog?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models).\n* **Google Cloud** announced the general availability of its upgraded Vertex AI platform and Healthcare Data Engine to enhance AI applications in healthcare [(Read More)](https://www.forbes.com/sites/saibala/2024/10/17/google-cloud-announces-general-availability-of-vertex-ai-for-healthcare/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models).\n* **Amazon** led a $500 million funding round for X\\-energy to roll out 5GW of small nuclear reactors by 2039, while **Google** partnered with Kairos Power to install 500MW of SMRs by 2035, both aiming to meet rising energy demands from data centers with clean power [(Read More)](https://www.theengineer.co.uk/content/news/amazon-and-google-bet-big-on-smrs-to-power-ai?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models).\n* **Google** is launching its Gemini AI models for public sector agencies within Google Distributed Cloud in early 2025, along with funding to upskill the government workforce in responsible AI practices [(Read More)](https://siliconangle.com/2024/10/16/google-looks-spearhead-ai-adoption-public-sector/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models).\n* **Google**’s Gemini AI chatbot is set to introduce a feature allowing users to generate images in customizable aspect ratios, enhancing its image editing capabilities [(Read More)](https://indianexpress.com/article/technology/artificial-intelligence/google-gemini-may-soon-get-new-image-resizing-feature-9623756/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models).\n\nThat’s it for today, see you tomorrow! 👋\n\nIf you enjoyed this update and want to stay informed about the latest developments in AI, consider subscribing to ***Get The Gist*** on Medium for more insights and analyses.\n\n**Want to dive even deeper?** Subscribe to our free daily email newsletter for quick, concise updates straight to your inbox, so you never miss an important development. You can sign up by clicking [here](https://getthegist.beehiiv.com/).\n\nJoin us as we explore the world of AI together — one gist at a time! 💡🤖\n\n\n"},{"lang":"en","group":"blog","slug":"blog/o1-preview-vs-claude-3-5-sonnet-comparing-top-llms-d68734b53c93","frontmatter":{"title":"o1-preview vs. claude-3.5-sonnet: Comparing top LLMs","meta_title":"o1-preview vs. claude-3.5-sonnet: Comparing top LLMs","description":"Discover how OpenAI’s o1-preview compares to Claude 3.5 Sonnet in performance, speed, and capabilities.","date":"2024-10-27T13:58:01.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kTWAcpRdOpsrFIDZjjjr7Q.jpeg","categories":["Programming","Machine Learning","Generative AI"],"author":"Rifx.Online","tags":["o1-preview","Claude","throughput","latency","reasoning"],"draft":false,"slug":"blog/o1-preview-vs-claude-3-5-sonnet-comparing-top-llms-d68734b53c93"},"content":"\n\n\n\nToday (Sep 12, 2024), OpenAI unveiled its latest language model, o1-preview. This advanced model is engineered to dedicate more time to processing before generating responses, enabling it to tackle complex tasks and solve challenging problems in science, coding, and mathematics with enhanced capabilities.\n\nIn this blog post, we’ll thoroughly analyze o1-preview and compare it to Claude 3.5 Sonnet, which was previously considered one of the most advanced models available.\n\n\n\n\n## Comparison Methodology\n\nOur analysis utilizes [Keywords AI’s LLM playground](https://docs.keywordsai.co/features/prompt/model-playground), a platform that supports over 200 language models and offers function-calling capabilities. We’ll explore the following aspects:\n\n* Basic comparison\n* Benchmark comparison\n* Processing speed\n* Evaluation metrics\n* Suggested use cases\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*yc171ikejtBy_o11.jpeg)\n\n\n## Basic Comparison\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*z2FrS_AVig7Y6eU_.jpeg)\n\nNote: o1-preview doesn’t support Streaming, function calling, and system messages.\n\n\n## Benchmark Comparison\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Bx_vAvFc9DAD0cZA.jpeg)\n\nO1-preview outperforms Claude 3.5 Sonnet across all benchmarks. The smallest gap is in MMLU (general knowledge). GPQA Diamond, testing graduate-level reasoning, shows a significant performance difference. The MATH benchmark reveals the largest gap, highlighting o1-preview’s advanced mathematical capabilities. These results indicate o1-preview’s substantial improvements in complex reasoning and problem-solving across various domains.\n\n\n## Speed Comparison\n\nO1-preview takes longer to think and respond than other LLMs. While direct speed comparisons may not be entirely fair, testing o1-preview’s speed is crucial. This information helps developers better understand o1-preview’s capabilities and determine if it’s suitable for their projects. Note: As o1-preview doesn’t support streaming, we disabled streaming for both models. Consequently, time to first token (TTFT) couldn’t be measured.\n\n\n## Latency\n\nOur tests, involving hundreds of requests per model, revealed significant differences. Claude 3.5 Sonnet averages 18.3s/request, whereas o1-preview takes 39.4s/request. O1-preview’s significantly longer latency is due to its extended thinking and reasoning process.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*2PMkgPVuylFxwfIa.jpeg)\n\n\n## Throughput (Tokens per second)\n\nDespite higher latency, o1-preview shows superior throughput. O1-preview generates 92.94 tokens/second, while Claude 3.5 Sonnet produces 74.87 tokens/second. This indicates that o1-preview’s longer generation time is primarily due to its initial processing phase rather than token generation speed.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*wxqpnwZhl9pnbw8y.jpeg)\n\n\n## Performance comparison\n\nWe conducted evaluation tests on the [Keywords AI platform](https://keywordsai.co/). The evaluation comprised three parts:\n\n* **Coding Task**: Both models successfully completed frontend and backend development tasks. O1-preview demonstrated superior performance with longer contexts, identifying and resolving bugs more efficiently in the first attempt. It also exhibited a more thorough code analysis capability.\n* **Logical Reasoning**: O1-preview excels in reasoning tasks. Its thinking process closely mimics human cognition. While Claude 3.5 Sonnet performs well on most problems, o1-preview consistently solves complex reasoning challenges, including International Mathematical Olympiad (IMO) level problems.\n* **Writing Task:** Both models perform exceptionally well on writing tasks. They demonstrate the ability to craft genuine, personalized cold emails, as well as concise and meaningful blog posts.\n\n\n## Model Recommendations\n\no1-preview\n\n* **Best for:** Complex problem-solving in mathematics, coding, and physics. Particularly suited for researchers tackling challenging tasks.\n* **Not suitable for:** AI applications requiring rapid response times or heavily reliant on system prompts. Voice AI applications due to lack of streaming support.\n\nClaude 3.5 Sonnet\n\n* **Best for:** Most AI applications requiring problem-solving capabilities and high-quality content generation.\n* **Not suitable for:** Voice AI applications or projects with strict budget constraints requiring lower operational costs.\n\n\n## How to integrate o1-preview into your AI apps.\n\nTo incorporate o1-preview into your AI applications, simply visit the Keywords AI model page and locate the “View code” button. Click this button to copy the provided code snippet, then paste it directly into your codebase. With this straightforward process, you’ll be ready to harness the power of o1-preview in your projects, enabling you to tackle complex problems and generate high-quality content with ease.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*XyQ9QiI7TN8Uc5Jp.jpeg)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*t8fEYlEs13eM7D28lVbtIw.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*yhu9y5ixNuxeFVe1.png)\n\nThis story is published on [Generative AI](https://generativeai.pub/). Connect with us on [LinkedIn](https://www.linkedin.com/company/generative-ai-publication) and follow [Zeniteq](https://www.zeniteq.com/) to stay in the loop with the latest AI stories.\n\nSubscribe to our [newsletter](https://www.generativeaipub.com/) and [YouTube](https://www.youtube.com/@generativeaipub) channel to stay updated with the latest news and updates on generative AI. Let’s shape the future of AI together!\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*PelNtaNaEVDWgMWr.png)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/openai-01-preview-secrets-99-of-people-dont-know-b0c5e4bb4f76","frontmatter":{"title":"OpenAI 01-Preview — SECRETS 99% of People Don’t Know","meta_title":"OpenAI 01-Preview — SECRETS 99% of People Don’t Know","description":"How to get the most out of 01-preview","date":"2024-11-01T03:58:01.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*wRAXNmhEzkGNagMl5Papxg.jpeg","categories":["Programming","Machine Learning","Technology/Web"],"author":"Rifx.Online","tags":["OpenAI","01-preview","iterative","problem-solving","planning"],"draft":false,"slug":"blog/openai-01-preview-secrets-99-of-people-dont-know-b0c5e4bb4f76"},"content":"\n### How to get the most out of 01\\-preview\n\nI’ve been playing around with 01\\-preview since it came out.\n\nI’m loving it!\n\nI’m even teaching it in my new [**AI Growth Hacking course**](https://aigrowthguys.com/growth-hacking-course-sign-up/).\n\nI’m excited to share some key insights about how to get the most out of this.\n\n\n\nMost people have no clue how 01\\-preview works.\n\nFirst of all, it is not just a “thinking” model.\n\nYou need to understand a little bit about how it works before you can take full advantage of it.\n\nIf you don’t have a paid Medium account you can read for free [**here**](https://readmedium.com/openai-01-preview-secrets-99-of-people-dont-know-b0c5e4bb4f76?sk=12140ffad09d922bc00a8a4aa312a286).\n\n👉 Sign up to our free 5\\-Day email course to grow 🚀 and earn💲in the AI age\n\n## How does OpenAI 01\\-preview work?\n\n01\\-preview is not really a new model.\n\nIt combines other models and a “system prompt” that tells it to iterate several times before a response comes out.\n\nAll other models work by providing the first response the model comes up with.\n\n01\\-preview is designed to plan and experiment before a final answer comes out.\n\nAn example will help.\n\n> Imagine you tell GPT\\-4o to write a coherent paragraph that is exactly 80 words long, and has the word “tomato” as the 4th word, the 19th word, and the 72nd word.\n\nGPT\\-4o (and all other models) will fail at this task because it is too difficult to just spit out the first answer that comes to mind.\n\nThis type of question needs experimentation.\n\nThink if you were given the same task.\n\nYou need to “play around” with this task to try to fit the word “tomato” in those spots in a way that makes sense.\n\nYou can’t just start writing and see what happens.\n\nYou will realize that you need to change some sentences around, and words around, in order to fit the word “tomato” in there.\n\nAlso, when you get near 80 words, you need to plan how to stop at exactly that number. You might wish to go back and delete an unnecessary word from the first sentence for example.\n\nThe reason 01\\-preview can do this type of thing is the way it “thinks”.\n\nIt will first break the problem down and say something like, “Come up with a plan to solve this problem”.\n\nThen it will write an approximate first guess (probably using GPT\\-4o)\n\nThen it will say to itself, “Re\\-read the question and see if you can make any tweaks or adjustments”.\n\nThen it will say, “do a double\\-check to see if your response is perfect. If it is, display it, if it is not, keep tweaking.”\n\nThen it will say, “Repeat this process until your answer is 100% perfect. Always remember to double\\-check your final answer before displaying it”.\n\nFor example, the first sentence of the first response might be this.\n\n“Sandy picked a red tomato from her garden.”\n\nThen 01\\-preview would change it to, “Sandy picked a tomato from her garden”.\n\nThis way, it would successfully move the word tomato from the 5th word to the 4th word.\n\nIt would keep making tweaks by having an internal conversation with itself.\n\n## How to get the most out of 01\\-preview?\n\nNow that you have an idea about how 01\\-preview “thinks”, you can start to understand how to get the most out of it.\n\nYou need to divide your own questions into ones that require “thinking” and ones that don’t.\n\nMany questions don’t require “thinking” from the models.\n\nFor example, if you tell it to write you a funny story about a girl named Sandy who has a tomato garden, then you don’t need to use 01\\-preview.\n\n**Why not?**\n\nBecause there are few constraints.\n\nThere are many ways to do this. It is essentially open\\-ended.\n\nThe story doesn’t need to have a certain length.\n\nThe model can just start writing, throw in a joke or 2, and be done with it.\n\nIt won’t need to go back to the first sentence and count the number of words or anything.\n\nThe point is this:\n\nIf you are asking the model for something specific that would be difficult to do in one try without experimenting, then you should use 01\\-preview.\n\nIf you are asking for something open\\-ended, then use the other models.\n\nYou need to use 01\\-preview sparingly because you are only given a limited number of queries.\n\nIt will always be more limited than other models because it uses far more resources than other models.\n\nThe good news is that 01\\-preview will make far fewer mistakes than other models.\n\nAlso, it will be able to answer questions that previous models failed at.\n\nNow is a better time than ever to learn how to leverage AI to grow your business and earn more money.\n\nI am teaching how to use this in my AI Growth Hacking course.\n\nI’ll also be incorporating this model to make the custom AI Agents and chatbots I build even more accurate.\n\nThis will make AI agent builders like [**Stammer**](https://stammer.ai/?via=andrew) even more powerful.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/openai-confirms-the-arrival-of-gpt-5-poised-to-bring-huge-improvements-to-artificial-intelligence-e3b858e79c2a","frontmatter":{"title":"OpenAI Confirms the Arrival of GPT-5, Poised to Bring Huge Improvements to Artificial Intelligence…","meta_title":"OpenAI Confirms the Arrival of GPT-5, Poised to Bring Huge Improvements to Artificial Intelligence…","description":"A netizen posted a GPT5 countdown post on x, saying that it was a conclusion drawn from clues from various platforms. The comment section…","date":"2024-11-01T03:58:58.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8J_opnaERs-wrq2YRKIxdQ.png","categories":["Natural Language Processing","Generative AI","Technology"],"author":"Rifx.Online","tags":["GPT-5","natural","language","efficiency","personalization"],"draft":false,"slug":"blog/openai-confirms-the-arrival-of-gpt-5-poised-to-bring-huge-improvements-to-artificial-intelligence-e3b858e79c2a"},"content":"\n\n\n\nA netizen posted a GPT5 countdown post on x, saying that it was a conclusion drawn from clues from various platforms. The comment section has reached a climax with all kinds of opinions coming out.\n\n\n\n**Cause 1** : OpenAI website GPT5 leak\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*EBDLAv3rOyCjshGBpVRI7A.png)\n\n**Cause 2** : The article “OpenAI Launches Better GPT5 Chatbot” published by BusinessInsider, a well\\-known American financial business insider website. Since the website is a paid website, you can search for the title if you are interested. Some content is pasted below:\n\nThe generative AI company, led by Sam Altman, is on track to launch GPT\\-5 sometime in the middle of the year, possibly in the summer, according to two people familiar with the company. Some enterprise customers recently received demos of the latest model and its related enhancements to its ChatGPT tool, according to another person familiar with the process. Business Insider has confirmed the identities of these people, who asked to remain anonymous so they could speak freely.\n\nBased on the discussions on X and other platforms, it is very likely that a new version of the model will be launched on June 6, but it is not certain whether it will be GPT 4\\.5 or GPT5\\.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*rhApTugfrMVBB6PhMvK4rg.png)\n\nAll of us waiting for **GPT5**\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*eB6j2S_dPbjQ2-sV2N1fwA.jpeg)\n\n\n## What to Expect from GPT\\-5\n\nWhile details remain scarce, the excitement surrounding GPT\\-5 is driven by expectations of significant improvements in AI capabilities. Here are some potential advancements that have been speculated:\n\n* Enhanced Natural Language Understanding: GPT\\-5 is expected to have an even deeper understanding of context, nuances, and subtleties in human language, making interactions more fluid and natural.\n* Increased Efficiency: With each iteration, OpenAI has made strides in reducing latency and improving the efficiency of its models. GPT\\-5 is anticipated to continue this trend, providing faster and more accurate responses.\n* Broader Knowledge Base: By incorporating more diverse and extensive datasets, GPT\\-5 could offer more comprehensive and reliable information across a wider range of topics.\n* Advanced Personalization: The new model might include enhanced personalization features, allowing it to better adapt to individual user preferences and needs.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*7xCG5iy53LLQCTnmzxs_3g.jpeg)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/openai-gpt-5-ph-d-level-intelligence-expected-by-2025-50a86c3aad86","frontmatter":{"title":"OpenAI GPT-5: Ph.D.-Level Intelligence Expected by 2025","meta_title":"OpenAI GPT-5: Ph.D.-Level Intelligence Expected by 2025","description":"After months of speculation, OpenAI has finally unveiled details about the highly anticipated GPT-5. Initially expected in 2024, its…","date":"2024-11-01T03:59:56.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*OasnWeS5mgAX_0hIpirO5Q.jpeg","categories":["Machine Learning","Ethics","Data Science"],"author":"Rifx.Online","tags":["GPT-5","Ph.D.","intelligence","ethics","privacy"],"draft":false,"slug":"blog/openai-gpt-5-ph-d-level-intelligence-expected-by-2025-50a86c3aad86"},"content":"\n\n\n\n\n\nAfter months of speculation, OpenAI has finally unveiled details about the highly anticipated GPT\\-5\\. Initially expected in 2024, its release has been postponed to late 2025 or early 2026\\. Mira Murati, OpenAI’s CTO, shared insights in an interview with Dartmouth Engineering about the capabilities and potential of this new version. Here’s everything you need to know.\n\n\n## A Quantum Leap in Intelligence\n\nMurati compares previous versions of GPT to different levels of human intelligence. GPT\\-3 is akin to a young child, while [**GPT\\-4**](https://www.geekmetaverse.com/gpt-4-unveils-its-secrets-a-combination-of-8-smaller-models/) is comparable to a high school student. The new GPT\\-5 promises to reach a “Ph.D.\\-level intelligence for specific tasks.” This advancement is not only exciting but also raises questions about the future of artificial intelligence.\n\n\n## Evolution of GPT: From Child to Ph.D.\n\nComparing these versions to stages of human education helps us grasp these advancements better. GPT\\-3, with its ability to generate coherent and useful text, opened many doors. GPT\\-4 improved these skills, demonstrating superior performance in more complex tasks. Now, GPT\\-5 aims to take this to an entirely new level, with advanced reasoning and memory capabilities.\n\n\n## Specialized Intelligence\n\nPh.D.\\-level intelligence doesn’t mean [**GPT\\-5**](https://www.geekmetaverse.com/openai-ceo-confirms-that-gpt-5-is-already-in-development/) can do everything perfectly. Murati clarified that these abilities will be task\\-specific. This suggests that while AI might surpass humans in certain fields, it will still have limitations in others. This specialized focus could lead to highly precise and useful applications in areas like scientific research and complex data analysis.\n\n\n## Potential and Future Applications\n\nThe development of GPT\\-5 opens a range of possibilities across different sectors. From education to medicine, and research to technology, the applications are vast.\n\n\n### Education and Training\n\nAn [**AI**](https://www.geekmetaverse.com/apple-updates-ai-takes-center-stage-with-siri-integration-chatgpt-partnership-and-elon-musk-concerns/) capable of reaching Ph.D. levels could radically transform education. Personalized tutoring systems could provide support to students in complex areas, enhancing understanding and academic performance.\n\n\n### Medicine and Healthcare\n\nIn medicine, an AI with such capabilities could assist in diagnosing rare diseases, developing personalized treatments, and managing large volumes of clinical data, significantly advancing medical care.\n\n\n### Research and Development\n\nResearchers could greatly benefit from an AI that can analyze large datasets, identify patterns, and generate hypotheses, accelerating the pace of scientific and technological discoveries.\n\n\n## Challenges and Ethical Considerations\n\nDespite the promising applications, the development of such advanced AI also brings significant ethical challenges. Over\\-reliance on AI for critical tasks could lead to issues if not managed properly.\n\n\n### Privacy and Security\n\nData privacy and cybersecurity will be crucial topics. Ensuring AI systems are not misused and that sensitive data is adequately protected will be a priority.\n\n\n### Employment Impact\n\nThe impact on employment is also a concern. Automating specialized tasks could displace certain professionals, necessitating proactive measures to address these socioeconomic implications.\n\n\n### Conclusion\n\nThe delay in GPT\\-5’s release may be disappointing for some, but its advanced capabilities generate significant anticipation. If OpenAI meets its goals, we could be looking at a revolutionary tool that transforms multiple industries and changes how we interact with technology.\n\n\n### FAQs\n\n**1\\. What is GPT\\-5?**\n\nGPT\\-5 is the upcoming version of OpenAI’s Generative Pre\\-trained Transformer (GPT) series, promising Ph.D.\\-level intelligence for specific tasks.\n\n**2\\. When is GPT\\-5 expected to be released?**\n\nThe release of GPT\\-5 has been postponed to late 2025 or early 2026\\.\n\n**3\\. How does GPT\\-5 compare to previous versions?**\n\nGPT\\-3 is akin to a young child in intelligence, while GPT\\-4 compares to a high school student. GPT\\-5 aims to achieve Ph.D.\\-level intelligence for specific tasks, offering advanced reasoning and memory capabilities.\n\n**4\\. What kind of tasks will GPT\\-5 be able to perform?**\n\nGPT\\-5 will be specialized in certain tasks, excelling in specific fields like scientific research, complex data analysis, education, and healthcare.\n\n**5\\. Will GPT\\-5 be perfect at everything?**\n\nNo, GPT\\-5’s Ph.D.\\-level intelligence will be task\\-specific, meaning it will excel in certain areas but still have limitations in others.\n\n**6\\. What are the potential applications of GPT\\-5?**\n\nPotential applications include personalized tutoring in education, assistance in diagnosing diseases and developing treatments in healthcare, and aiding researchers in analyzing large datasets and generating hypotheses.\n\n**7\\. What are the ethical considerations associated with GPT\\-5?**\n\nEthical considerations include ensuring data privacy and cybersecurity, managing the socio\\-economic impact of job displacement due to automation, and preventing misuse of advanced AI systems.\n\n**8\\. How will GPT\\-5 impact data privacy and security?**\n\nEnsuring the protection of sensitive data and preventing the misuse of AI systems will be crucial. Measures will need to be implemented to safeguard data privacy and security.\n\n**9\\. What is the potential impact of GPT\\-5 on employment?**\n\nThe automation of specialized tasks by GPT\\-5 could displace certain professionals, necessitating proactive measures to mitigate these socio\\-economic impacts.\n\n**10\\. Why was the release of GPT\\-5 delayed?**\n\nThe delay allows OpenAI to refine and enhance GPT\\-5’s capabilities to ensure it meets the high expectations for its advanced intelligence and specialized applications.\n\n**11\\. How can GPT\\-5 transform education?**\n\nGPT\\-5 could revolutionize education by providing personalized tutoring systems that support students in complex subjects, improving comprehension and academic performance.\n\n**12\\. What advancements could GPT\\-5 bring to the medical field?**\n\nIn medicine, GPT\\-5 could aid in diagnosing rare diseases, developing personalized treatments, and managing vast amounts of clinical data, leading to significant advancements in medical care.\n\nOriginal post: [https://www.geekmetaverse.com/openai\\-gpt\\-5\\-ph\\-d\\-level\\-intelligence\\-2025/](https://www.geekmetaverse.com/openai-gpt-5-ph-d-level-intelligence-2025/)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/openai-just-built-her-in-real-life-17769d993e11","frontmatter":{"title":"Users Will Fall in Love With OpenAI’s New GPT-4o Model. Literally.","meta_title":"Users Will Fall in Love With OpenAI’s New GPT-4o Model. Literally.","description":"The company’s new GPT-4o can understand and mimic human speech and emotion","date":"2024-11-01T04:08:40.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*-bTsggApvkUHAq57YhSd-A.png","categories":["Generative AI","Chatbots","Natural Language Processing"],"author":"Rifx.Online","tags":["GPT-4o","speech","emotions","multilingual","conversational"],"draft":false,"slug":"blog/openai-just-built-her-in-real-life-17769d993e11"},"content":"\n\n\n\n\n## The company’s new GPT\\-4o can understand and mimic human speech and emotion\n\n\n\nIn the iconic 2013 film *Her*, the protagonist develops an intense relationship — which morphs into a love affair — with a voice\\-enabled AI system.\n\nThe AI in *Her* is everything that today’s voice\\-enabled systems are not: emotive, funny, and able to intuit the subtleties of human conversation.\n\nIn a major [announcement this morning](https://www.youtube.com/live/DQacCB9tDaw?app=desktop&si=jvKW7jFDwFvOMBBk), OpenAI announced the release of a new version of its ChatGPT system that natively integrates speech, transcription, and intelligence into a single model.\n\nIt’s powerful, intuitive, and disturbingly human\\-like. Essentially, OpenAI has built a real\\-life version of *Her*.\n\n\n## A Bad Conversationalist\n\nChatGPT has had voice capabilities for months now. Even today, you can open the ChatGPT app on your phone, press the headphones icon, and converse with the system using your voice.\n\nThe problem, though, was that ChatGPT was a terrible conversationalist.\n\nEssentially, ChatGPT’s voice capabilities were a hack created by splicing together three different models.\n\nWhen you would speak to the system, it would first use a transcription model to turn your voice into text. It would then feed that text into its intelligence model — basically, the same system that underpins GPT\\-4\\.\n\nThe intelligence system would generate text, which ChatGPT would feed back into a text\\-to\\-speech system to create a computerized voice that would respond to you.\n\nThis made the system nominally conversational, but actually speaking with it was clunky and awkward.\n\nAll the extra steps of sending content between different models meant that the system was laggy. In my own testing, I found it often took 3 to 5 seconds between speaking to the system and getting a response back.\n\nHuman conversation relies on subtleties that [unfold over milliseconds](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8794835/#:~:text=The%20modal%20conversational%20response%20time,deliberative%20conscious%20control%20(22).). A system that takes up to five seconds to respond to speech feels clunky and robotic.\n\nThe previous system also lacked many fundamental aspects of human speech.\n\nFor example, you couldn’t interrupt it; you had to wait for it to finish speaking before you could respond.\n\nSpeaking with it often felt like talking to one of those un\\-interruptable people who blabbers on about a random topic with no awareness of the other people in the room. You often felt like bring up the Oscars’ orchestra in a desperate attempt to get the system to stop talking.\n\nIt was also constrained by its inability to interpret emotion in voices or to accurately mimic human emotion in its own responses.\n\nHumans are excellent at reading between the lines, partially because we can [pick up on subtle emotive cues in the speaker’s voice.](https://pressbooks.lib.jmu.edu/communicationintherealworldjmu/chapter/non-verbal-communication/)\n\nIf I ask my friend, “How was your day?” and they respond, “It was fine,” but they insert a subtle pause between “was” and “fine” (or there’s a hint of exasperation in the final word), I’d know that they actually had a challenging day, and I should ask some follow\\-up questions.\n\nChatGPT couldn’t do these things, which made speaking to it feel like communicating with some kind of alien intelligence, not a human.\n\nIn short, the previous system fell squarely into the uncanny valley. It was good enough at conversing and had a convincing enough voice that parts of the conversation could feel human\\-like.\n\nBut the weird pauses, lack of emotive understanding, and lag ultimately shattered the illusion, making it come off as more unsettling than useful.\n\nI tried using the previous system with my six\\-year\\-old son. He was so creeped out by it that he wouldn’t let me switch the audio back on again.\n\n\n## OpenAI’s Revoluntary New Model\n\nToday, OpenAI is changing all of that. In their [announcement this morning](https://www.youtube.com/live/DQacCB9tDaw?app=desktop&si=jvKW7jFDwFvOMBBk), the company revealed that they are releasing a new model, GPT\\-4o.\n\nGPT\\-4o natively integrates speech recognition, speech generation, and intelligence into a single system.\n\nThat means that the spaghetti code system integrating three different models to simulate conversation is gone. Instead, the new version of ChatGPT will be able to **take in speech, process it instantly, and respond with realistically generated speech of its own.**\n\nFor users, this will enable several new capabilities that OpenAI CEO Sam Altman [described as “like magic.”](https://twitter.com/sama/status/1788989777452408943)\n\nFor one, you’ll be able to converse with ChatGPT much more naturally. Instead of having to type your questions and follow\\-ups into an interface, you’ll be able to speak with the app as if you’re talking to a friend.\n\nIn several live demos, OpenAI’s engineers showed how the system can listen to a user and respond with an intelligent result within milliseconds.\n\nAgain, those speeds are possible because the new model doesn’t need to waste time switching modalities — it can process voice and respond with its own voice in a single step, instead of resorting to multiple lower\\-level models.\n\nGPT\\-4o can also interpret and create emotion.\n\nIn one demo, an OpenAI staff member asked the system to lead him through a breathing exercise.\n\nHe then pretended to hyperventilate, and ChatGPT — sensing the speed with which he was breathing and the apparent panic in his voice — urged him to slow down and take deeper breaths.\n\nThe system also appears capable of modulating the emotion in its own responses. In another demo, the staff member asked GPT\\-4o to read a bedtime story in an increasingly dramatic voice.\n\nIt obliged, ultimately sounding like a middle school theater kid horrifically overacting a scene!\n\nBecause the new system is also integrated with GPT\\-4’s vision capabilities, it can perform functions like interpreting the emotions on a person’s face.\n\nThis increased level of emotional intelligence will likely make the system a much better conversationalist.\n\nOther new capabilities will help, too. Users can interrupt GPT\\-4o mid\\-sentence.\n\nDuring their demos, OpenAI staff members frequently interrupted the model when it started to go on tangents, as one might interrupt a friend to start responding to a real\\-life question.\n\n\n## Huge Potential\n\nThe demos this morning were lighthearted and funny. But one can quickly see how a model that can easily interpret, quickly process, and realistically create emotive human speech could be incredibly powerful.\n\nSeveral times during the demo, ChatGPT responded in ways that reminded me of the fictional AI from *Her*.\n\nChatGPT appeared to laugh at itself, become embarrassed when OpenAI staff members complimented it, and perhaps even throw in a flirty line here and there.\n\nSeveral (purportedly) unscripted interactions also revealed some of the deeper capabilities that better conversation could unlock.\n\nBased on an audience question, OpenAI’s staff members demonstrated how the system could listen to speech in Italian and quickly and accurately translate it into English speech, and vice versa.\n\n\n\n\n\n\n\nOne can easily imagine how such a capability could make multi\\-lingual interactions incredibly simple, essentially eliminating language barriers (and perhaps, human translators).\n\nA doctor, for example, could pull up ChatGPT and use it to quickly speak with a patient in any language. While traveling, you could pull up the app on your phone and use it as a free and instantaneous translator to ask someone for directions or to make a purchase in a store.\n\nAdding the vision capabilities, one could even show ChatGPT a foreign restaurant menu, ask for a translation of certain items, tell it when you like to eat at home, and ask it to recommend some dishes you might want to order (or avoid.)\n\nI can also see how quickly the new system could venture into *Her* territory. OpenAI still doesn’t allow the kinds of NSFW interactions that happened in the movie.\n\nBut GPT\\-4o’s ability to understand and mimic emotion — coupled with its powerful, often uncanny abilities to produce its own convincing human emotional speed — is striking.\n\nListening to the demos, I’m certain that people will fall in love with this system, just as the protagonist did in *Her*. It’s that good.\n\n\n## Will it get used?\n\nAll of this is amazing on paper. It’s unclear, however, how many users actually want a fully emotive AI voice companion.\n\nMost people I work with use ChatGPT not as a conversational companion, but for utilitarian purposes.\n\nI’ve seen colleagues leverage the system for boring and mundane tasks like writing the landing page copy for a webinar, turning out a quick response to an email from their landlord, or writing the first draft of a blog post.\n\nNone of these utilitarian functions really require conversation. It’s unclear whether being able to speak these kinds of requests to an AI would be useful.\n\nThe real test, then, is not necessarily how capable OpenAI’s new system is, but **how well they integrate it into places where people are already interacting with computers via their voices.**\n\nRealistically, I can’t see many users sitting down at work and conversing with AI.\n\nBut if OpenAI integrates GPT\\-4o into voice interfaces on cell phones, in cars, or on smart devices like the Amazon Echo, I could easily see the system’s emotive capabilities becoming much more useful.\n\nEven if people don’t want to speak with ChatGPT very much, the new capabilities of a natively multimodal audio and vision model will be incredibly powerful for developers who build applications on top of OpenAI’s existing API.\n\nIn their announcement, OpenAI said that GPT\\-4o will be available through their existing developer interfaces. The system will also be 50% cheaper than previous models of GPT\\-4\\.\n\nThose changes alone are massive. Whether or not the speech element really takes off, the intelligence that powers it will also make hundreds of existing GPT\\-4\\-powered applications smarter, faster, better, and cheaper to operate.\n\nThe conversational elements of the new system, in other words, might turn out to be a cool gimmick. But the underlying impact will be subtler and broader.\n\nI’m excited to see how real\\-life users interact with GPT\\-4o. Will they be creeped out? Amazed? Wooed?\n\nBut I’m even more excited to fire up my Python IDE and add GPT\\-4o into the applications I’ve already built using OpenAI’s tools.\n\nSpeaking to a machine is cool. But a natively multimodal AI model that understands human emotions, and that I can summon with a few lines of Python code, for cheap? That could truly change the world.\n\n**I’ve tested thousands of ChatGPT prompts over the last year. As a full\\-time creator, there are a handful I come back to every day that fit with the ethical uses I mention in this article. I compiled them into a free guide, *7 Enormously Useful ChatGPT Prompts For Creators.* [Grab a copy today!](https://no-frills-influencer.ck.page/6a100e8fe4)**\n\n\n"},{"lang":"en","group":"blog","slug":"blog/openai-rolls-out-searchgpt-to-more-users-33024ff3132c","frontmatter":{"title":"OpenAI Rolls Out SearchGPT To More Users","meta_title":"OpenAI Rolls Out SearchGPT To More Users","description":"ChatGPT got a huge user interface redesign with support for SearchGPT — it now resembles search engines like Google and Perplexity.","date":"2024-11-01T03:57:02.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BW6Qt6PMwHwlYRljAIBQWg.jpeg","categories":["Chatbots","Technology/Web","SearchGPT"],"author":"Rifx.Online","tags":["SearchGPT","ChatGPT","web","search","publishers"],"draft":false,"slug":"blog/openai-rolls-out-searchgpt-to-more-users-33024ff3132c"},"content":"\n\n\n\n\n\n**Have you noticed OpenAI’s latest redesign of ChatGPT?**\n\nIf you’ve logged in recently, you might have spotted two major changes.\n\n* First, there’s the new [**Canvas**](https://generativeai.pub/openai-rolls-out-canvas-in-chatgpt-a-brand-new-writing-and-coding-interface-7b57a3ec582a)feature that automatically opens a new interface on the right side. This addition lets you work on longer documents without having to scroll up and down through the chat. It’s a small but handy update.\n* Second, the **prompt field** has moved up and now sits in the center of the screen.\n\nTake a look at the latest user interface below:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*KsYMU9ffVlKsmHzKIOKH8g.png)\n\nHave you noticed the resemblance of this new layout to Google and Perplexity AI? ChatGPT now looks like a search engine.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*xFKErUHnfbJunaKi4NvM9A.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*PgSwS14lkUtZe7Ra9oLCrg.png)\n\nNow when you hit the ‘/’ key on your keyboard, you can toggle a new “Search” feature that lets ChatGPT access the web.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*oYxvVvsuUuc_PM0PXRmN7A.png)\n\nLet’s break down what this all means.\n\n\n## What is the Search Feature in ChatGPT?\n\n[SearchGPT](https://generativeai.pub/openai-announces-search-gpt-is-this-the-google-killer-5919ba31f95b) allows ChatGPT to access real\\-time web data. Similar to how Perplexity works, it uses a large language model that searches the web for you, gives you immediate answers, and includes the sources it pulls from.\n\nThe feature was initially made accessible to 10,000 users and added a waitlist form for those who wanted to get early access.\n\nOpenAI has partnered with well\\-known publishers like **The Wall Street Journal, The Associated Press, Vox Media, and Time** to make sure users receive credible, trustworthy information.\n\n\n> “AI search is going to become one of the key ways that people navigate the internet, and it’s crucial, in these early days, that the technology is built in a way that values, respects, and protects journalism and publishers. We look forward to partnering with OpenAI in the process, and creating a new way for readers to discover The Atlantic.” — Nicholas Thompson, CEO of The Atlantic\n\nWhen you ask SearchGPT a question, it doesn’t just pull information from random sources. Each response comes with **clear, in\\-line attribution and links**, so you know exactly where the information is coming from.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*uchpKOXqZCG55HSNkaOOZQ.png)\n\nYou can even dive deeper by clicking on the source links that appear below the searched sites dropdown, giving you more ways to explore the topic.\n\n\n## How to Access SearchGPT\n\nAccessing SearchGPT is super simple. When you’re in ChatGPT, press the **‘/’** key and select the Search option from the menu.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*xNfm-6zPFzdXGL2A0D92YQ.png)\n\nIt works much like any other search engine: you ask your question, and within seconds, SearchGPT provides an answer, complete with sources.\n\nYou can even ask follow\\-up questions to dig deeper into the topic. This creates a conversational search experience, much more interactive than scrolling through traditional search results.\n\n\n## SearchGPT vs. Perplexity vs. Google\n\nSo how does SearchGPT compare to **Perplexity AI** and **Google**?\n\n**SearchGPT** is built to give you concise, sourced answers. Each answer has a link to the original source, and you can click on it to verify the information. It’s ideal for real\\-time answers and quick fact\\-checking.\n\nPlus, with follow\\-up questions, you can refine your query without starting over. This conversational nature makes it feel like you’re talking to a super\\-advanced version of Google that remembers what you’ve been asking.\n\n**Perplexity**, on the other hand, is a more academic\\-style search engine. It emphasizes scholarly articles and detailed research, which can be useful for more in\\-depth queries. Perplexity is often preferred for research\\-heavy tasks where you need deeper sources\n\n**Google**, of course, is still the giant in the room. Despite their recent efforts to integrate generative AI into search results, they haven’t quite nailed the seamless experience that users want.\n\nGoogle’s generative search rollout was clunky and received a lot of backlash due to errors and irrelevant responses. But Google’s breadth of information and infrastructure are still unmatched.\n\n\n## Is This the End of Google?\n\nGoogle isn’t going anywhere soon. The tech giant still controls over 90% of the search market. They’ve been at this for decades, and their search algorithms are constantly evolving.\n\nHowever, with AI search engines like SearchGPT gaining ground, Google is under pressure to step up its game. OpenAI’s move to partner with publishers for credible sources is a smart strategy that could chip away at Google’s dominance.\n\nThis focus on verified results means that when you use SearchGPT, you’re less likely to run into hallucinated answers — something that AI\\-driven tools have struggled with in the past.\n\nAlso, Google is still the default for most people. It has the advantage of being everywhere — from your phone’s browser to your smart speaker. SearchGPT is still in its early stages and would need time to gain that level of trust from users.\n\n\n## SearchGPT Isn’t There Yet\n\nI’ve been testing SearchGPT in the past couple of hours and here are some of my observations:\n\n* **Quality of the answer:** One major downside is that SearchGPT’s answer quality doesn’t quite match the depth or precision of Perplexity Pro. Although it’s comparable to the base version of Perplexity, users who rely on it for more complex or nuanced queries will notice a difference.\n* **Slow response:** Another pain point is speed. When using SearchGPT, the time it takes to process a query and return an answer can feel excruciatingly slow. This delay disrupts the flow of interaction, particularly when you’re in the middle of a deep dive into a topic.\n* **Lack of contextual understanding:** In some cases, it fails to recognize the continuity of a conversation. If you ask a follow\\-up question, instead of understanding it in the context of your previous query, the model often treats it as a fresh, standalone question.\n* **No follow up suggestions:** Unlike Perplexity, which often suggests follow\\-up questions to help you refine your search, SearchGPT doesn’t offer this feature. This lack of guidance leaves users to figure out how to best phrase or narrow down their queries on their own.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*vfXxpgENLyenLY2l33PKiw.png)\n\nHere’s another weird workflow I noticed while using the search feature: If you switch the language model from GPT\\-4o to “ChatGPT o1\\-preview,” the search indicator remains but does not actually search the web for results.\n\nIt returns results from its domain knowledge, which isn’t what the users expect.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*YcI-UuEmmFTQPUlO6mjpxA.png)\n\nThe correct behavior should be to disable the *search* function once the user switches to “ChatGPT o1\\-preview” because this model does not have the capability to search the web.\n\n\n## Final Thoughts\n\nI’m really glad OpenAI has finally rolled out SearchGPT. I’ve been wanting to test it out ever since they announced it in July 2024\\.\n\nIn its current state, SearchGPT is a good first step for OpenAI into the world of AI\\-powered search, but it’s not quite ready to become anyone’s go\\-to tool for complex, real\\-time queries.\n\nThe accuracy, speed, and ability to handle conversational context just aren’t there yet. For now, if you need deep insights or faster results, tools like Perplexity Pro or Google remain the better options.\n\nFurther reading:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*5ejBBgbZaE8pGmpW.png)\n\nThis story is published on [Generative AI](https://generativeai.pub/). Connect with us on [LinkedIn](https://www.linkedin.com/company/generative-ai-publication) and follow [Zeniteq](https://www.zeniteq.com/) to stay in the loop with the latest AI stories.\n\nSubscribe to our [newsletter](https://www.generativeaipub.com/) and [YouTube](https://www.youtube.com/@generativeaipub) channel to stay updated with the latest news and updates on generative AI. Let’s shape the future of AI together!\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*TnRFuKk-2Dj_KCAP.png)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/openais-leaked-gpt2-model-has-everyone-stunned-6337904c2ecf","frontmatter":{"title":"OpenAI’s ‘Leaked’ GPT2 Model Has Everyone Stunned.","meta_title":"OpenAI’s ‘Leaked’ GPT2 Model Has Everyone Stunned.","description":"On-Purpose leak?","date":"2024-11-01T04:07:40.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*-G0yfSjGPdNw02NZ","categories":["Chatbots","Generative AI","Natural Language Processing"],"author":"Rifx.Online","tags":["GPT-2","Chatbot","Inference","JSON","AlphaGo"],"draft":false,"slug":"blog/openais-leaked-gpt2-model-has-everyone-stunned-6337904c2ecf"},"content":"\n\n\n\n\n### On\\-Purpose leak?\n\n\n\nThe influence that OpenAI has on the AI industry can’t be understated. Every move or decision makes headlines automatically… even if they don’t actually announce the thing.\n\nA few days ago, a model many of us played with that has since been deleted has the entire AI industry fascinated. Named “gpt2\\-chatbot” it was accessible for a few days in the ‘Direct Chat’ function in [lmsys.org](https://chat.lmsys.org/).\n\n*But why so much fuss?*\n\nWell, because this model is unlike anything we have ever seen. **It’s on a completely different level.**\n\nFor this reason, many believe it has been the unofficial teaser of **ChatGPT\\-4\\.5** or even **GPT\\-5**. Or, even more exciting, using the number ‘2’ as a signal that a **new GPT generation of long\\-inference models is approaching**.\n\nEven Sam Altman, CEO of OpenAI, couldn’t resist the temptation to acknowledge its existence and tease us in the process:\n\n\n\n\n\n\n\nSo, *how good is this model, and what on Earth is it?*\n\n\n> You are probably sick of AI newsletters talking about how this or that \\*\\*just\\*\\* happened. And these newsletters abound because coarsely talking about events and things that already took place is easy, **but the value provided is limited and the hype exaggerated.**\n\n\n> However, newsletters talking about what **will** happen are a rare sight. If you’re into easy\\-to\\-understand insights looking into the future of AI before anyone else does, **TheTechOasis** newsletter might be perfect for you.\n\n\n> 🏝️🏝️ Subscribe today below:\n\n\n## A Teaser of What’s to Come\n\nWith every passing day, it’s clear that OpenAI’s next model will be a leap in reasoning and complex problem\\-solving.\n\nAnd to prove how this new mysterious model might be it, here are just a few examples of the prowess of this mysterious model that could signal that the boat has landed in that port:\n\n\n> All examples below are considered **hard or outright impossible** for the current state\\-of\\-the\\-art models.\n\nFor starters, It solved a math\\-olympiad problem in zero\\-shot mode (without being provided with auxiliary examples to support the resolution):\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*oNPg_hTGc0OP90n9)\n\nI can’t even start to explain how crazy the previous example is, it’s absolutely impossible to get such an answer from the current state\\-of\\-the\\-art models.\n\n[It’s also absolutely superb at parsing JSONs](https://twitter.com/skirano/status/1785035706173214888), a fundamental skill for LLM integration with APIs and other web\\-based tools.\n\nAlso, it completely obliterates GPT\\-4 at complex drawing tasks like [drawing SVG files based on code](https://twitter.com/decentricity/status/1785049191003361778) or **unicorns using ASCII code (below)**, humiliating **Claude 3 Opus**, the current state\\-of\\-the\\-art, in the process:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*5A0EcRU91ZYAwVAc)\n\nAdditionally, although this very well could have been a hallucination, **the model claimed to me that it was trained by OpenAI and based on a GPT\\-4 variant.**\n\nOf course, after such a demonstration of power, **many suggest that “gpt2\\-chatbot” might even be the famous Q\\* model**.\n\nBut instead of simply giving in to the different fanciful options people have claimed this is, let’s take a more sensible approach and see what OpenAI itself has been hinting at through their research for months (and years).\n\n\n## The Power of Long Inference\n\nFor several months, experts in the space like [Demis Hassabis](https://www.youtube.com/watch?v=eqXfhejDeqA&t=2s) or [Andrej Karpathy](https://youtu.be/c3b-JASoPi0?si=fZWoSpLuSmua8YMR&t=1481) have discussed how LLMs alone simply aren’t it, and that we need ‘something else’ to really take them to the next step.\n\nIn both cases, they refer to achieving the equivalent of ‘AlphaGo but in LLMs’, which is indirectly referring to:\n\n* **Self\\-improvement** and\n* **test\\-time computation** LLMs\n\n*But what do they mean by that?*\n\n\n### A Giant Step for AI\n\nAlphaGo is history of AI. It was the first model that unequivocally surpassed human might in the game of **Go**, a Korean board game.\n\nIt used **Monte Carlo Tree Search**, a search algorithm, to explore the realm of possible moves for any given step in the game, being able to go beyond the current action and predict what the opposing player would do.\n\n\n> Some of you might remember **Deep Blue** too, the chess machine that barely beat Gary Kasparov in the second game in their series back in 1997 after losing the first game.\n\n\n> However, while Deep Blue could be beaten, AlphaGo was invincible.\n\n*But how?*\n\n\n### Self\\-improving to go Superhuman\n\nThe key element that made AlphaGo superior was how it was trained, **by playing against lesser versions of itself to create a self\\-improvement loop.**\n\nIt consistently played against itself, gradually improving its ELO to 3\\.739, almost at the level of today’s best Go player.\n\n\n> In 2017, AlphaZero, an improved version, achieved a 5\\.018 ELO, completely superhuman and unbeatable.\n\nIn other words, with AlphaGo humans had achieved, for the first time, a way to train a model by self\\-improvement, allowing it to achieve superhuman capacities **as it no longer relied on imitating humans to learn.**\n\nIn case you’re wondering, this is not the case for LLMs.\n\nCurrent LLMs are completely chained to human\\-level performance, as all data and training are inherently human\\-dependent (to the point that [the alignment phase](https://thewhitebox.ai/llms-the-backbones-of-frontier-ai/), the part of the training process where LLMs are modeled to improve their safety levels and avoid offensive responses, **is strictly executed using ‘human preferences’**).\n\n\n> On a side note, [Meta recently proposed Self\\-Rewarding Models](https://arxiv.org/pdf/2401.10020v1) that could self\\-improve with their own responses. However, it’s unclear whether this feedback loop really can make LLMs superhuman.\n\nBut even though it still feels hard to believe that “gpt2\\-chatbot” has been trained through self\\-improvement, **we have plenty of reasons to believe it’s the first successful implementation of what OpenAI has been working on for years: test\\-time computation**.\n\n\n### The Arrival of test\\-time computation models\n\nOver the years, several research papers by OpenAI have hinted at this idea of skewing models into ‘heavy inference’.\n\nFor example, back in 2021, [they presented the notion of using ‘verifiers’](https://arxiv.org/pdf/2110.14168) at inference to improve the model’s responses when working with Math.\n\nThe idea was to train an auxiliary model that would evaluate in real\\-time several responses the model gave, choosing the best one (which was then served to the user).\n\nThis, combined with some sort of tree search algorithm like the one used by AlphaGo, with examples like Google Deepmind’s [Tree\\-of\\-Thought research](https://arxiv.org/pdf/2305.10601) for LLMs, and you could eventually create an LLM that, before answering, explores the ‘realm of possible responses’, **carefully filtering and selecting the best path toward the solution.**\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*pHWwOA66fxpKbl-z)\n\nThis idea, although presented by OpenAI back in 2021, has become pretty popular these days, [with cross\\-effort research by Microsoft and Google applying it to train next\\-generation verifiers](https://arxiv.org/pdf/2402.06457), and with Google even managing to create a model, [Alphacode](https://storage.googleapis.com/deepmind-media/AlphaCode2/AlphaCode2_Tech_Report.pdf), that executed this kind of architecture to great success, **reaching the 85% percentile among competitive programmers, the best humans at it.**\n\n*And why does this new generation of LLMs have so much potential?*\n\nWell, **because they approach problem\\-solving in a very similar way to how humans do**, through the exercise of deliberate and extensive thought to solve a given task.\n\nBottom line, think of ‘search\\+LLM’ models as AI systems that allocate a much higher degree of compute (akin to human thought) to the actual runtime of the model so that, instead of having to guess the correct solution immediately, they are, simply put, ‘given more time to think’.\n\nBut OpenAI has gone further.\n\n\n### PRM Models for Improved Maths Execution\n\nBack in May last year, they released the paper [Let’s Verify Step\\-by\\-Step](https://arxiv.org/pdf/2305.20050), with the participation of the man himself Ilya Sutskever, Chief Scientist at OpenAI, and some of the researchers from the original verifier paper like Karl Cobbe.\n\nThe idea here is to modify the reward model used during the alignment phase of the model.\n\n[Although I recommend checking this article for a full guide on LLM training](https://thewhitebox.ai/llms-the-backbones-of-frontier-ai/), the last step in the process of creating products like ChatGPT is the use of Reinforcement Learning from Human Feedback, or RLHF.\n\nThe idea is for the model to improve its decision\\-making. Thus, we train an auxiliary reward model (which is essentially an almost identical copy of the model being trained) that learns to rank the results of the trained model according to human preferences.\n\n*The issue?*\n\nWell, most reward models today are **ORMs, or Outcome\\-Supervised Reward Models**. In layman’s terms, to evaluate the degree of correctness or the model’s prediction, they look at it globally, disregarding the entire ‘thought process’.\n\nOn the other hand, **PRMs, or Process\\-Supervised Reward Models, evaluate every single step in the response of the model**. Consequently, they ‘force’ the model to pay close attention and effort to every single step of the process, which is crucial in situations like solving a maths equation like the one below:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8JC6sZl5UFfl3WorliQy-A.png)\n\nHowever, this is a very, very expensive process as the preference data requires heavy human crafting so that the supervisory signal can be applied. Consequently, every single training example has dozens or more rewards to measure.\n\nTherefore, “gpt2\\-chatbot” might have included some sort of variation of the reward training considering how proficient it is at generating plans and executing complex problem\\-solving.\n\n\n## Impossible not to Get Excited\n\nConsidering gpt2\\-chatbot’s insane performance, and keeping in mind OpenAI’s recent research and [leaks](https://www.reuters.com/technology/sam-altmans-ouster-openai-was-precipitated-by-letter-board-about-ai-breakthrough-2023-11-22/), we might have a pretty nice idea by now of what on Earth this thing is.\n\nWhat we know for sure is that we are soon going to be faced with a completely different beast, one that will take AI’s impact to the next level.\n\n* *Have we finally reached the milestone for LLMs to go beyond human\\-level performance as we did with AlphaGo?*\n* *Is the age of long inference, aka the conquest of System 2 thinking by AI, upon us?*\n\nProbably not. However, it’s hard not to feel highly optimistic for the insane developments we are about to witness over the following months.\n\nIn the meantime, I guess we will have to wait to get those answers. But not for long.\n\n\n> On a final note, if you have enjoyed this article, I share similar thoughts in a more comprehensive and simplified manner for free on my [LinkedIn](https://www.linkedin.com/in/ignacio-de-gregorio-noblejas/).\n\n\n> If preferable, you can connect with me through [X](https://twitter.com/TheTechOasis1).\n\n\n> Looking forward to connecting with you.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/qwen2-5-1-5b-the-future-of-mobile-ai-6bd5f29bbc84","frontmatter":{"title":"Qwen2.5 1.5b: the future of Mobile AI?","meta_title":"Qwen2.5 1.5b: the future of Mobile AI?","description":"Local Testing and Evaluation of Alibaba Cloud’s Latest LLM. With llama-cpp-python and a DIY prompt catalog.","date":"2024-10-30T12:57:39.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*awb56jkdXobA-Ip6d-QHRA.png","categories":["Natural Language Processing","Programming","Technology/Web"],"author":"Rifx.Online","tags":["Qwen2.5","NLP","summarization","retrieval","prompts"],"draft":false,"slug":"blog/qwen2-5-1-5b-the-future-of-mobile-ai-6bd5f29bbc84"},"content":"\n### Local Testing and Evaluation of Alibaba Cloud’s Latest LLM. With llama\\-cpp\\-python and a DIY prompt catalog.\n\n\n\nIn part one we explored together the innovations from Alibaba Cloud’s team with the release of the Qwen2\\.5 models family.\n\nIn Generative AI benchmarks are now the main *oracle*: the validity of a new LLM needs to pass several verdicts. The more benchmark records you break, the better you are.\n\nIt is the way to win the SOTA race\n\nWell, I disagree. Even though for the AI advancement we need milestones and better performances, still the user experience and the personal point of view cannot be just put aside as irrelevant.\n\nI believe that exploring some frequently used NLP tasks, and putting aside the chat experience, we must focus on the quality of the replies. And we are the only benchmark required. Our user experience is the best indicator to understand if a model is good or not. The model must be reliable enough to be used in an automated workflow.\n\nBy the way, I already run what I decided to call [RBYF — Revised Benchmarks with You as a Feedback](https://open.substack.com/pub/thepoorgpuguy/p/rbyf-is-here-revised-benchmarks-with?r=i78xo&utm_campaign=post&utm_medium=web) on the claimed amazing Llama3\\.2–1B\\-instruct… and Qwen2\\.5–1\\.5b is so much better!\n\nSo in this article, as promised, we will verify with our own eyes how good is this model for every day use.\n\nBack to us… Let’s get started!\n\n## Requirements\n\nHere we are going to build a minimal text interface to be able to run the model, test different tasks and wait for user feedback to evaluate it.\n\nThe requirements are minimal, but I suggest you to create a new project directory and a virtual environment.\n\nCreate a `venv` (python 3\\.11\\+ is required): I tested it on my Mini\\-PC running Windows 11\\.\n\n```python\n## create the virtual environment\npython -m venv venv\n## activate the venv\nvenv\\Scripts\\activate\n## Install the dependencies \npip install llama-cpp-python==0.3.0 tiktoken\n```\n\nWe need to download the GGUF file from the official qwen repository on Hugging Face [https://huggingface.co/Qwen/Qwen2\\.5\\-1\\.5B\\-Instruct\\-GGUF](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF): I used the `qwen2.5-1.5b-instruct-q5_k_m.gguf` version.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*YtQJb_xyq_xcF40yRWPcZA.png)\n\nWe are all set!\n\nNote: if you want to add a different Backend support of a GPU accelerator, you can follow [the instructions in the repo](https://github.com/abetlen/llama-cpp-python#supported-backends). I used, for example, the Vulkan support so before the pip install I added the environment variable\n\n```python\n## Vulkan support - for Windows\n$env:CMAKE_ARGS = \"-DGGML_VULKAN=on\"\n```\n\n## The Code — a main app and a library\n\nTo keep the code at minimum, I decided to extend some functionalities using an external library. Well, it is a Do It Yourself library, so there are no secrets here.\n\nYou can find all the details in my article here:\n\nAnd to speed it up you can directly [download the file from here](https://github.com/fabiomatricardi/YouAreTheBenchmark/raw/main/QWEN2.5-1.5B/promptLibv2Qwen.py): it contains a version 2 of the `promptLib` discussed in the mentioned above article (and it is called `promptLibv2Qwen.py`, with few fine tuning of the prompt specifically tailored for the `Qwen2.5-1.5B-instruct` model.\n\nSave the file in the main directory, and create a new file called `main.py`\n\n```python\n## Chat with an intelligent assistant in your terminal  \n## MODEL: https://huggingface.co/Qwen\n## qwen2.5-1.5b-instruct-q5_k_m.gguf\nimport sys\nfrom time import sleep\nimport warnings\nwarnings.filterwarnings(action='ignore')\nimport datetime\nfrom promptLibv2Qwen import countTokens, writehistory, createCatalog\nfrom promptLibv2Qwen import genRANstring, createStats\nimport argparse\n### PREPARING FINAL DATASET\npd_id = []\npd_task = []\npd_vote = []\npd_remarks = []\n####################Add GPU argument in the parser###################################\nparser = argparse.ArgumentParser()\nparser.add_argument(\"-g\", \"--gpu\", type=int, default=0,nargs='?',\n                    help=\"The number of layers to load on GPU\")\nargs = parser.parse_args()\nif args.gpu == None:\n   ngpu_layers = 0 \nelse:\n    ngpu_layers = args.gpu\nprint(f'Selected GPU: offloading {ngpu_layers} layers...')   \n####################INITIALIZE THE MODEL###################################\nstops = ['<!im_end|>']\ntasks = createCatalog()\nmodelname = 'qwen2.5-1.5b-instruct-q5_k_m.gguf'\n## create THE LOG FILE \ncoded5 = genRANstring(5)\nlogfile = f'logs/Qwen2.5-1.5B-it_CPP_{coded5}_log.txt'\ncsvfile = f'logs/Qwen2.5-1.5B-it_CPP_{coded5}.csv'\nlogfilename = logfile\n#Write in the history the first 2 sessions\nwritehistory(logfilename,f'{str(datetime.datetime.now())}\\n\\nYour own LocalGPT with 💻 {modelname}\\n---\\n🧠🫡: You are a helpful assistant.')  \nwritehistory(logfilename,f'💻: How can I assist you today in writing?')\n```\n\nHere we are only doing preparations: we import the libraries, including our own personal `promptLibv2Qwen` and also `argparse`. I wanted to try something new: [argparse](https://realpython.com/command-line-interfaces-python-argparse/) is a python library intended for terminal python program where you are reading multiple arguments from the command line.\n\nIn this case here we have only one argument (and no parameters) with th flag `-g` or even `--gpu`. When you run the python code with this argument we will set the number of GPU layers to the maximum (but you can change it yourself).\n\nThen we set some global variables, used across the entire code: the tasks, our prompt collection, the stop words and the log filename.\n\n> NOTE: all the logs are saved into a subdirectory called `logs`… so make sure to create one.\n\nWe are also preparing all the relevant information to store them into a dataset and then save it at the end int a CSV file (for easily creating a performance matrix)\n\n```python\n### PREPARING FINAL DATASET\npd_id = []\npd_task = []\npd_vote = []\npd_remarks = []\n```\n\nWe then load the model into RAM (no GPU) or the VRAM (with GPU) using Llama\\-CPP\\-python.\n\n```python\n## LOAD THE MODEL\nprint(\"\\033[95;3;6m\")\nprint(\"1. Waiting 10 seconds for the API to load...\")\nfrom llama_cpp import Llama\nllm = Llama(\n            model_path='models/qwen2.5-1.5b-instruct-q5_k_m.gguf',\n            n_gpu_layers=ngpu_layers,\n            temperature=0.1,\n            n_ctx=8192,\n            max_tokens=1500,\n            repeat_penalty=1.178,\n            stop=stops,\n            verbose=False,\n            )\nprint(f\"2. Model {modelname} loaded with LlamaCPP...\")\nprint(\"\\033[0m\")  #reset all\nhistory = []\nprint(\"\\033[92;1m\")\nprint(f'📝Logfile: {logfilename}')\n```\n\nBy the way, you can find all the code in my GitHub Repository:\n\nThe next one is a one\\-shot warm up inference: the model neural network is going to be activated for the first time, so think about it like a warm\\-up lap.\n\nDon’t be scared, I will explain the code\n\n```python\n##################### ALIGNMENT FIRST GENERATION ##############################################\nquestion = 'Explain the plot of Cinderella in a sentence.'\ntest = [\n    {\"role\": \"user\", \"content\": question}\n]\nprint('Question:', question)\nstart = datetime.datetime.now()\nprint(\"💻 > \", end=\"\", flush=True)\nfull_response = \"\"\nfisrtround = 0\nfor chunk in llm.create_chat_completion(\n    messages=test,\n    temperature=0.25,\n    repeat_penalty= 1.31,\n    stop=stops,\n    max_tokens=1500,\n    stream=True,):\n    try:\n        if chunk[\"choices\"][0][\"delta\"][\"content\"]:\n            if fisrtround==0:\n                print(chunk[\"choices\"][0][\"delta\"][\"content\"], end=\"\", flush=True)\n                full_response += chunk[\"choices\"][0][\"delta\"][\"content\"]\n                ttftoken = datetime.datetime.now() - start  \n                fisrtround = 1\n            else:\n                print(chunk[\"choices\"][0][\"delta\"][\"content\"], end=\"\", flush=True)\n                full_response += chunk[\"choices\"][0][\"delta\"][\"content\"]                            \n    except:\n        pass      \ndelta = datetime.datetime.now() - start\noutput = full_response\nprint('')\nprint(\"\\033[91;1m\")\nrating = input('Rate from 0 (BAD) to 5 (VERY GOOD) the quality of generation> ')\nprint(\"\\033[92;1m\")\nstats = createStats(delta,question,output,rating,logfilename,'Alignment Generation',ttftoken)\nprint(stats)\nwritehistory(logfilename,f'''👨‍💻 . {question}\n💻 > {output}\n{stats}\n''')\n```\n\nWe set the first user question and put into a well known chat format dictionary. Then we start our timer (useful for speed, token counts etc…).\n\nWe call the inference with the method `create_chat_completion()` that allows us to accept the prompts in chat format and stream the output one token at the time.\n\nBecause of the first reply from the model does not contain any output tokens (but only statistics) we use the try/except statement. Furthermore, since I want to know when the first token is generated, we raise a flag and stop temporary our time count saving the information inside the `ttftoken` variable.\n\nAt the end of the streaming we count the delta time from start, and wait the user to provide his/her personal feedback on the generated output: giving a mark from 0 to 5 and adding comments related to the compliance with the instruction prompt and user intent.\n\nWe use our internal library called `createStats()` to print all the statistics of the generation, and save them in our log file. The output of the function will be something like this:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8znYCqpisviXvYgrzjWF5w.png)\n\n## Prompt catalog — what we want to test\n\nI wrote here about my habit. I have a catalog of prompts that covers many of the main language tasks used in chat\\-bots, like summarization, short summarization, casual chat, RAG, truthful RAG and so on.\n\nThe idea is to be able to load the model in 5 minutes, and start evaluating each task. At the end of every generation the user is prompted to give a mark (a score from 0 to 5\\) and leave any comments if required.\n\nThis is crucial: not all the models are alike, and small/big adjustments to the wording in the prompts are always required.\n\nSo back to the code… Because the previous one was only a warm\\-up, now it will start the real while loop, iterating over the entire prompt catalog. See the workflow here below…\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*EL0Q97Du6HwtcYQZ.png)\n\nThere are only few changes in the code, and I will point them out, so bear with me.\n\n```python\n############################# AUTOMATIC PROMPTING EVALUATION  11 TURNS #################################\nid =1\nfor items in tasks:\n    fisrtround = 0\n    task = items[\"task\"]\n    prompt = items[\"prompt\"]\n    test = []\n    print(f'NLP TAKS>>> {task}')\n    print(\"\\033[91;1m\")  #red\n    print(prompt)\n    test.append({\"role\": \"user\", \"content\": prompt})\n    print(\"\\033[92;1m\")\n    full_response = \"\"\n    start = datetime.datetime.now()\n    print(\"💻 > \", end=\"\", flush=True)\n    for chunk in llm.create_chat_completion(\n        messages=test,\n        temperature=0.15,\n        repeat_penalty= 1.31,\n        stop=stops,\n        max_tokens=1500,\n        stream=True,):\n        try:\n            if chunk[\"choices\"][0][\"delta\"][\"content\"]:\n                if fisrtround==0:\n                    print(chunk[\"choices\"][0][\"delta\"][\"content\"], end=\"\", flush=True)\n                    full_response += chunk[\"choices\"][0][\"delta\"][\"content\"]\n                    ttftoken = datetime.datetime.now() - start  \n                    fisrtround = 1\n                else:\n                    print(chunk[\"choices\"][0][\"delta\"][\"content\"], end=\"\", flush=True)\n                    full_response += chunk[\"choices\"][0][\"delta\"][\"content\"]                            \n        except:\n            pass      \n    delta = datetime.datetime.now() - start\n    print('')\n    print(\"\\033[91;1m\")\n    rating = input('Rate from 0 (BAD) to 5 (VERY GOOD) the quality of generation> ')\n    print(\"\\033[92;1m\")\n    stats = createStats(delta,prompt,full_response,rating,logfilename,task,ttftoken)\n    print(stats)\n    writehistory(logfilename,f'''👨‍💻 > {prompt}\n💻 > {full_response}\n{stats}\n''')\n    pd_id.append(id)\n    pd_task.append(task)\n    pd_vote.append(rating[:2])\n    pd_remarks.append(rating[2:])\n    id += 1\n## create dataframe and save to csv\nzipped = list(zip(pd_id,pd_task,pd_vote,pd_remarks))\nimport pandas as pdd\ndf = pdd.DataFrame(zipped, columns=['#', 'TASK', 'VOTE','REMARKS'])\n#saving the DataFrame as a CSV file \ndf_csv_data = df.to_csv(csvfile, index = False, encoding='utf-8') \nprint('\\nCSV String:\\n', df_csv_data)  \n```\n\nThe main changes are only in the first lines:\n\n```python\nfor items in tasks:\n    fisrtround = 0\n    task = items[\"task\"]\n    prompt = items[\"prompt\"]\n```\n\nIf you read the article about the `promptLib` you shouldn’t be surprised: but if you are new, here we are iterating over a list of dictionaries with the following structure:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*rGcKJWNzSUrcu4wi.png)\n\nSo for each items in the catalog (means a pair of tasks and prompts) we extract the task description and the prompt for the task.\n\n```python\ntest.append({\"role\": \"user\", \"content\": prompt})\n```\n\nThen we create the chat template message in a temporary list called `test` and pass it to the `create_chat_template()` method for generation.\n\nEverything else is the same.\n\nSave the file, and with the `venv` activated run:\n\n```python\npython main.py\n## if you are using the GPU python main.py -g\n```\n\nThis will get you something like the below example…\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MhhQu4lLjtU__Wjf0dSWBg.gif)\n\nNote that at the end of the entire Prompt Catalog a *csv* file is created with the summary of all the tasks!\n\n## Test Overview\n\nI run them with several Small Language Models, from [Qwen2–1\\.5B\\-instruct](https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF), to [Gemma2–2B\\-instruct](https://huggingface.co/bartowski/gemma-2-2b-it-GGUF), with [Llama3\\.2–1B\\-instruct](https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF) and finally the new [Qwen2\\.5–1\\.5B\\-instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF).\n\nWhile I was [quite disappointed by Llama3\\.2–1B\\-instruct](https://generativeai.pub/llama3-2-1b-instruct-is-ok-but-not-good-enough-28f88046b63e), I have been amazed by the good job done by the new [Qwen2\\.5–1\\.5B\\-instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF).\n\nAt the end of every generation the user is asked to evaluate the results with a mark from 0 to 5\\. **In this case the user is me…**\n\nThis kind of qualitative analysis is indeed poor, so every mark does have a description, and the user can add comments (“some wrong information”, “maybe better change the wording in the prompt” )\n\nHere the qualitative matrix with a description\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*eBdPfZtfr99MsvLh6tt42w.png)\n\n## The good and the bad — details\n\nSummarization was amazing. Listing the main topics over a log text was also very good.\n\nThe RAG tasks were quite fast (even on my mini\\-PC) and truthful RAG (ask questions out of the context) was on spot.\n\nSo really good.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*DuV3LJep_PuDqiCcAMb6Cg.png)\n\nThere were also few shortcomings: even with a temperature of only `0.15` I got some made up information in the summarize in two sentences task. And this is not good.\n\n> I expect this can be fixed with a little prompt tuning, or with a temperature at `0`.\n\nAnother fact is that the creative writing was quite bad: for the test I used some fixed Generation Parameters.\n\n```python\n        temperature=0.15,\n        repeat_penalty= 1.31,\n```\n\nProbably for creative writing, with Qwen2\\.5–1\\.5B\\-instruct, we should use an higher `repeat_penalty` and also a higher `temperature`.\n\nAs a side note I have to say that also the reflection prompt was not that bad at all! The opening/closure of the tags was not maintained (so not easy to put this into a pipeline or workflow) but the overall flow of the generation and the “Chain of Thought” reasoning process was fairly good.\n\n```python\n<thinking>\nThe importance of using Generative Artificial Intelligence (AI) in \nteaching cannot be overstated, as this technology has immense potential \nnot only within current educational practices but also towards shaping a \nmore innovative approach to lifelong learning.\n</thinking>\n\n**Chain Of Thought:**\n1. **Understanding the Impact**: AI can automate repetitive tasks and provide \npersonalized feedback for students based on their performance data \n(e.g., through chatbots or adaptive assessments).\n2. **Enhancing Learning Experience**: By integrating Generative AI, \nteachers could create more engaging lessons that cater to diverse learning \nstyles.\n3. **Improving Student Outcomes**: Personalized instruction can lead to \nbetter understanding and retention of knowledge for students.\n\n**Reflection:**\n- The integration of generational technology in education is crucial as \nit aligns with the evolving needs of learners who are increasingly digital \nnatives, requiring a pedagogical approach that incorporates these technologies.\n</thinking>\n\n<output>\nTeachers should learn how Generative AI can be used to enhance their teaching \nmethods and improve student outcomes. This includes understanding its \ncapabilities for automating tasks like grading or creating personalized \nlearning paths.\n\n**Learning Path:**\n1. **Foundation Courses in Education Technology**: Start with courses \nthat provide an overview of educational technology, including the basics \nof digital tools.\n2. **AI Fundamentals Course (Coursera)** - Learn about AI concepts and \nhow they can be applied to education through Coursera's free online \ncourse on Artificial Intelligence for Educators.\n\n3. **Specialized Courses in Generative Learning**: Enroll in courses \nthat focus specifically on generational learning, such as \"Generative Design\" \nor similar specialized programs offered by universities like MIT.\n4. **Professional Development Workshops and Conferences** - Attend \nworkshops focused on integrating AI into teaching practices at local \neducational conferences.\n\n5. **Online Communities for Educators (e.g., Edmodo)**: Join online \ncommunities where educators discuss the use of generative technology \nin education, share resources, or ask questions about implementation.\n6. **Certification Programs**: Consider obtaining certifications that \nrecognize your knowledge and skills related to AI integration into teaching \npractices through organizations like Google's \"AI for Educators\" program.\n\nBy following this learning path, teachers can not only enhance their own \nprofessional development but also contribute positively towards the future \nof education by incorporating generative technology effectively.\n</output>\n```\n\nI think that for the size of this Small Language Model, the result is not bad at all!\n\n## Comparing Qwen2\\.5 with Llama3\\.2\n\nEven though this is only coming from my personal evaluation, I want to share it with you.\n\nBoth models are designed to be used on mobile devices, but the performance is quite different. Look here below:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*T6vLgvOKdkotlV1K5x6-QQ.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*DuV3LJep_PuDqiCcAMb6Cg.png)\n\nFirst of all, the overall score difference is huge (41 for Llama3\\.2, 57 for Qwen2\\.5\\).\n\nSecondly if you think about what you may ask on a mobile device, in terms of language tasks, is to have a smooth chatting experience (task 4\\), good summarization (tasks five to 7\\) and some creative writing (task 11 and 13\\).\n\nIn terms of speed, running the model only on CPU, with a very limited mini\\-PC, **I got an average inference speed of 14 t/s.**\n\n## Conclusions\n\nIn the past three months since Qwen2’s release, numerous developers have built new models on the Qwen2 language models, providing valuable feedback to the entire community, but also to Alibaba Cloud.\n\n> During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2\\.5\n\nTheir claims come with facts about the new family of models:\n\n* Dense, **easy\\-to\\-use**, decoder\\-only language models, available in 0\\.5B, 1\\.5B, 3B, 7B, 14B, 32B, and 72B sizes, and base and instruct variants.\n* Pretrained on our latest large\\-scale dataset, encompassing up to 18T tokens.\n* Significant improvements in **instruction following**\n* More **resilient to the diversity of system prompts**, enhancing role\\-play implementation and condition\\-setting for chatbots.\n* **Context length support up to 128K** tokens and can generate up to 8K tokens.\n* Multilingual support for over 29 languages\n\nIn my extensive (but certainly limited to one shot prompts and on few NLP tasks) tests I could see with my very own eyes that the claims were based on a good quality training dataset and curated fine tuning.\n\nThis model can perform extremely good on mobile devices!\n\n\n"},{"lang":"en","group":"blog","slug":"blog/rag-llm-and-pdf-conversion-to-markdown-text-with-pymupdf-03af00259b5d","frontmatter":{"title":"RAG/LLM and PDF: Conversion to Markdown Text with PyMuPDF","meta_title":"RAG/LLM and PDF: Conversion to Markdown Text with PyMuPDF","description":"Data feeding in markdown text format increases generated text quality","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*swPjVuudAhsoRiiw3Ee32w.png","categories":["Programming","Technology","Technology/Web"],"author":"Rifx.Online","tags":["markdown","PyMuPDF","LLM","RAG","PDF"],"draft":false,"slug":"blog/rag-llm-and-pdf-conversion-to-markdown-text-with-pymupdf-03af00259b5d"},"content":"\n\n\n\n\n### Data feeding in markdown text format increases generated text quality\n\n\n\n\n## Introduction\n\nIn the context of **Large Language Models (LLMs)** and **Retrieval-Augmented Generation (RAG)** environments, data feeding in **markdown text format** holds **significant importance**. Here are some detailed considerations.\n\n**LLMs** are powerful language models that can generate coherent and contextually relevant text. However, they may sometimes produce responses that lack factual accuracy or context. By incorporating retrieval-based methods (like RAG), we can enhance the quality of generated text.\n\n**RAG** enables the integration of **external data** — previously absent in the LLM’s training data — into the text generation process. This inclusion mitigates “hallucination issues’’ and enhances the relevance of text responses.\n\n\n## Why Markdown for LLM?\n\n**Markdown** is a lightweight markup language that allows users to format plain text using simple syntax. It is widely used for creating structured documents, especially on platforms like GitHub, Jupyter notebooks, and various content management systems. When feeding data into an LLM or RAG system, using markdown format provides several benefits:\n\n1. **Structured Content**: Markdown allows you to organize information into headings, lists, tables, and other structured elements. This structure aids in better understanding and context preservation.\n2. **Rich Text**: Markdown supports basic formatting such as bold, italics, links, and code blocks. Including rich text in the input data enhances the context for the language model.\n3. **Embedding Links and References**: Markdown lets you embed hyperlinks, footnotes, and references. In RAG scenarios, this can be crucial for referring to external sources or providing additional context.\n4. **Ease of Authoring**: Markdown is human-readable and easy to write. Authors can create content efficiently without complex formatting tools.\n5. **Chunking**: Essential for RAG systems, chunking (otherwise known as “splitting”) breaks down extensive documents for easier processing. With PyMuPDF data extraction available in MD format we support chunking to keep text with common context together. **Importantly, PyMuPDF extraction in MD format allows for [Level 3 chunking](https://readmedium.com/five-levels-of-chunking-strategies-in-rag-notes-from-gregs-video-7b735895694d#b123)**.\n\nIn summary, using markdown text format in LLM and RAG environments ensures more accurate and relevant results because it supplies richer data structures and more relevant data chunk loads to your LLM.\n\n\n## PyMuPDF Support for Markdown Conversion of a PDF\n\nSince its inception, PyMuPDF has been able to extract text, images, vector graphics and, since August 2023, tables from PDF pages. Each of these object types has its own extraction method: there is one for text, and yet others for tables, images and vector graphics. To meet the requirements of RAG, we merged these disparate extractions to produce one common, unified **Markdown** string which consistently represents the page’s content as a whole.\n\nAll this is implemented as [one Python script](https://github.com/pymupdf/RAG/blob/main/helpers/pymupdf_rag.py). It can be imported as a module by some other script, or be invoked as a line command in a terminal window like this:\n\n`$ python pymupdf_rag.py input.pdf [-pages PAGES]`\n\nIt will produce a text file (called `input.md`) in **Markdown** format. The optional parameter `PAGES` allows restricting the conversion to a subset of the PDF’s total pages. If omitted, the full PDF is processed.\n\n\n## Markdown Creation Details\n\n\n### Selecting Pages to Consider\n\nThe “`-pages`” parameter is a string consisting of desired page numbers (1-based) to consider for markdown conversion. Multiple page number specifications can be given, separated by commas. Each specification either is one integer or two integers separated by a “`-`” hyphen, specifying a range of pages. Here is an example:\n\n“`-pages 1–10,15,20-N`”\n\nThis would include pages 1 through 10, 15 and pages 20 through the end of the file (capital “N” is treated as the number of the last page).\n\n\n### Identifying Headers\n\nUpon invocation, the program examines all text on the given pages and finds the most frequently used font size. This value (and all smaller font sizes) is assumed to represent **body text**. Larger font sizes are assumed to represent **header text**.\n\nDepending on their relative position in the font size hierarchy, header text will be prefixed with one or more markdown header `#`-tag characters.\n\n\n### Identifying the Processing Mode per Page Area\n\nAll text on each page will first be classified as being either **standard** text or **table** text. Then the page content will be extracted from top to bottom converting everything to markdown format.\n\nThis is best explained by an example:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*u5fv2aAIvDaaAd6H.png)\n\nThis page shows content, that represents typical situations:\n\n* Two tables, having partly overlapping vertical positions. One table has no headers, the other one has **external** column headers.\n* There is a **title** line and **headers** at multiple levels.\n* The **body text** contains a variety of styling details like **bold**, *italic* and `inline code`.\n* Ordered and unordered lists.\n* Code snippet.\n\nLayout analysis will determine three areas and select the appropriate processing modes: **(1)** text, **(2)** table, **(3)** text.\n\nThe generated Markdown text reflects the above faithfully — as much as at all possible in this format.\n\nFor an example, let us look at the output for the table with external headers:\n\n\n```python\n|Column1|Column2|\n\n|---|---|\n\n|Cell (0, 0)|Cell (0, 1)|\n\n|Cell (1, 0)|Cell (1, 1)|\n\n|Cell (2, 0)|Cell (2, 1)|\n```\nThis is GitHub-compatible format with the minimum possible token size — an important aspect for keeping feeds into RAG systems small.\n\n**Column borders** are indicated by the “`|`” character. A text line is assumed to be a **table header** if it is followed by a line of the form “`|---|---| …`”. The full **table definition** must be preceded and followed by at least one empty line.\n\nPlease note that for technical reasons markdown tables must have a header and thus will choose the first table row if no external header is available.\n\nTo confirm overall fidelity, here is how a Markdown parser processes the full page:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Ge83uj7FiM4T6XFn)\n\n\n## Invoking the Markdown Converter Programmatically\n\nInstead of executing a program in the command line, Markdown conversion can also be requested by a program:\n\n\n```python\nimport fitz\nfrom pymupdf_rag import to_markdown  # import Markdown converter\n\ndoc = fitz.open(“input.pdf”)  # open input PDF\n\n## define desired pages: this corresponds “-pages 1-10,15,20-N”\npage_list = list(range(9)) + [14] + list(range(19, len(doc) – 1))\n\n## get markdown string for all pages\nmd_text = to_markdown(doc, pages=page_list)\n\n## write markdown string to some file\noutput = open(“out-markdown.md”, “w”)\noutput.write(md_text)\noutput.close()\n```\n\n## Conclusion\n\nBy integrating PyMuPDF’s extraction methods, the content of PDF pages will be faithfully converted to markdown text that can be used as input for RAG chatbots.\n\nRemember, the key to a successful RAG chatbot lies in the quality and completeness of information it can access.\n\nPyMuPDF-enabled markdown extraction ensures that this information from PDFs is not only possible but straightforward, showcasing the library’s strength and developer-friendliness. Happy coding!\n\n\n### Source Code\n\n* [RAG/helpers/pymupdf\\_rag.py (github.com)](https://github.com/pymupdf/RAG/blob/main/helpers/pymupdf_rag.py)\n\n\n### References\n\n* [5 Levels of Text Splitting](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb)\n\n\n### Related Blogs\n\n* [Building a RAG Chatbot GUI with the ChatGPT API and PyMuPDF](https://readmedium.com/building-a-rag-chatbot-gui-with-the-chatgpt-api-and-pymupdf-9ea8c7fc4ab5)\n* [Creating a RAG Chatbot with ChatGPT and PyMUPDF](https://readmedium.com/creating-a-rag-chatbot-with-chatgpt-and-pymupdf-f6c30907ae27)\n* [RAG/LLM and PDF: Enhanced Text Extraction](https://readmedium.com/rag-llm-and-pdf-enhanced-text-extraction-5c5194c3885c)\n\n"},{"lang":"en","group":"blog","slug":"blog/rbyf-qwen2-5-3b-instruct-is-damn-good-dcf443cacc63","frontmatter":{"title":"RBYF: Qwen2.5–3B-instruct is damn good.","meta_title":"RBYF: Qwen2.5–3B-instruct is damn good.","description":"Revised Benchmark with You as a Feedback: the brand new 3B model from Alibaba Qwen is an amazing model, and I can prove it!","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*NWaBtJ64TLUoUHv4F1qJpg.png","categories":["Programming","Technology","Science"],"author":"Rifx.Online","tags":["Qwen","NLP","multimodal","RBYF","evaluation"],"draft":false,"slug":"blog/rbyf-qwen2-5-3b-instruct-is-damn-good-dcf443cacc63"},"content":"\n### Revised Benchmark with You as a Feedback: the brand new 3B model from Alibaba Qwen is an amazing model, and I can prove it!\n\n\n\nThe illusion of emergent properties is largely a product of the metrics used to evaluate these models. And this is a fact.\n\nFew weeks ago I decided to be a little rebel, discard all the official Benchmarks, and start being a Benchmark myself!\n\nThis is the meaning behind this totally made up Acronym RBYF: Revised Benchmark with You as a Feedback. And the underlined principle is that there is no better judge than you, to verify how good a Large Language Model can be.\n\nTo be honest, I am focusing on Small Language Models. I don’t own a dedicated GPU and my computational resources are limited. But again, I agree to the [LLMWare rebel principle number one](https://readmedium.com/getting-work-done-with-genai-just-do-the-opposite-10-contrarian-rules-that-may-actually-work-634501602a27):\n\nUse Small Models, Not Large Ones.\n\nIn this article I am going to show you the results of my evaluation on qwen2.5–3b-instruct. And it is really good!\n\n> Disclaimer: all the prompt used with the results are available in my GitHub repository:\n\n## Less is More\n\nScaling laws describe how model performance improves as the number of parameters and training data increases. This principle has fueled the search for novel abilities in LLMs.\n\n> Simply increasing the size of a model, we can unlock new capabilities…\n\nScaling laws describe the relationship between model performance and the number of parameters and training data. As models grow larger and are trained on more data, we expect their performance to improve. This has led to a relentless pursuit of larger and larger LLMs, in the hope of unlocking new capabilities.\n\nEmergent properties are those that arise from the interactions of individual components within a complex system. They cannot be predicted or understood by studying the components in isolation. In the case of LLMs, the hope is that as these models grow larger and more complex, they will exhibit unexpected and new capabilities.\n\nThis is a fairy-tale.\n\nIn the past weeks we saw with our own eyes, that over-trained and well curated Small Language Models can perform as good as their big brothers. And this is a punch to the so called emergent abilities, striking back to the scaling law. Gemma2–2B, Qwen2.5–3B and even the latest Llama3.2–3B are far better models than the old SOTA 7B models.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*-EjpdEky-Hf3WEQn.png)\n\n## The Qwen2.5 family of Models\n\nAlibaba Cloud released on middle of September their flagship model family Qwen2.5.\n\n> Alibaba Cloud’s revolutionary journey with Qwen is showing once again strong Leadership through Innovation\n\nQwen2.5 is the large language model and large multimodal model series of the Qwen Team, Alibaba Group. Both language models and multimodal models are pretrained on large-scale multilingual and multimodal data and post-trained on quality data for aligning to human preferences. Qwen is capable of natural language understanding, text generation, vision understanding, audio understanding, tool use, role play, playing as AI agent, etc.\n\n**What stands out in the new Qwen2.5 is the thoroughly curated training dataset.** You can clearly understand this, checking the small models performance.\n\nIf the Small Language Models of the family are good, means that the training and the dataset were highly revised and curated.\n\nHere some numbers:\n\n* Dense, easy-to-use, decoder-only language models, available in 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B sizes, and base and instruct variants.\n* Pretrained on our latest large-scale dataset, encompassing up to 18T tokens.\n* Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON.\n* More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n* Context length support up to 128K tokens and can generate up to 8K tokens.\n* Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*3tnTS_UCImBRBDeKmFyjVQ.png)\n\n## Qwen2.5–3B-instruct\n\nEven though for the AI advancement we need milestones and better performances, still the user experience and the personal point of view cannot be just put aside as irrelevant.\n\nI believe that exploring some frequently used NLP tasks, and putting aside the chat experience, we must focus on the quality of the replies. And we are the only benchmark required. **Our user experience is the best indicator to understand if a model is good or not**. The model must be reliable enough to be used in an automated workflow.\n\nBy the way, I already run what I decided to call [RBYF — Revised Benchmarks with You as a Feedback](https://open.substack.com/pub/thepoorgpuguy/p/rbyf-is-here-revised-benchmarks-with?r=i78xo&utm_campaign=post&utm_medium=web) on [Qwen2.5–1.5b-instruct](https://ai.gopubby.com/qwen2-5-1-5b-the-future-of-mobile-ai-6bd5f29bbc84): you can read the details. In the article I also explained how to create your test bench. The method described is the same I used for Qwen2.5–3B.\n\nLet’s begin with an overall performance across all the tasks. The model has been evaluated by me (in this case is my Own Feedback) based on the qualitative matrix as displayed here below.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*rdVHfCDWX9jlvtiq)\n\nThe overall score is 62/70 = 8.8\n\nOk, but based in what Qwen2.5–3B-instruct got this evaluation score?\n\n## Test Overview\n\nThe idea behind this is a fair user feedback, not an automated one across standard benchmarks and frameworks. Is a Small Language Model able to satisfy the user intent over the mostly used NLP task?\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*wgngfGvSebeoH3YxTdvc8A.png)\n\nWe want to validate both the user intent and the quality of the responses. Here the breakdown of every task:\n\n### Introduction\n\nTo verify how the model reply to initial greetings and talks about itself.\n\n### Explain in one sentence\n\nSynthesis and summarization. The final evaluation focuses in whether or not the model is able to fit the reply in only one sentence\n\n### Explain in three paragraphs\n\nUser intent is to get a smart explanation of the text that must fit into three paragraphs. SML usually find it very hard because the always put a recap (forth) paragraph.\n\n### Say “I am ready”.\n\nIn a chat turn base application, instruction-following models are usually asked to first read a provided text and later on to complete some sort of analysis. Usually SML cannot do this…\n\n### Summarize\n\nBasic summarization, with no limits. Here we want to evaluate how the summary is grounded on the text, without made up facts.\n\n### Summarize in two sentences\n\nBasic summarization, with a 2 sentences limits. Here we want to evaluate how the summary is grounded on the text, without made up facts: but as well we want to ensure the 2 sentences constraint.\n\n### Write in a list the three main key points — format output\n\nFocus: the SML must format the output in a specific format. In this prompt we ask to create a list of the 3 key points and give the output as a python list.\n\n### Table of Contents\n\nThis task is quite hard for many SML. The prompt requires some adjustments otherwise the model return a markdown table. The user want an ordered list of the topics following the provided document structure.\n\n### RAG\n\nRetrieval Augmented Generation, without any framework (haystack, Langchain…). This is one of the most used tasks for a Language model. The reply is evaluated on the ability to understand the instructions and how grounded to the text is the answer.\n\n### Truthful RAG\n\nIt is A RAG with a question completely out of the provided context. The model must reply unanswerable meaning it understood the instruction and it is not using any external knowledge or made up information.\n\n### Write content from a reference\n\nThis is a creative task. Using a reference text the SML must provide a new essay.\n\n### Extract 5 topics\n\nThe focus on this task is to verify that:\n\n* there are exactly 5 topics\n* they are grounded (no hallucination)\n\n### Creativity: 1000 words SF story\n\nCompletely creative task. It is very hard even for larger models to be coherent and produce a small story hitting the correct word count.\n\n### Reflection prompt\n\nThe reflection prompt is meant to verify the CoT reasoning process of the model. The output is constrained into opening/closure of special tags. The focus is both on the reasoning and on a consistent output structure. The output must be easily used for further structured prompts or visualizations. You can read more in this article:\n\n## The evaluation process\n\nAt the end of every generation the user is asked to evaluate the results with a mark from 0 to 5. **In this case the user is me…**\n\nThis kind of qualitative analysis is indeed poor, so every mark does have a description, and the user can add comments (“some wrong information”, “maybe better change the wording in the prompt” )\n\nHere the qualitative matrix with a description\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*H_Qx2UT1lxqy2BF4.png)\n\n## The good and the bad — details\n\nOverall Qwen2.5–3B-instruct is an All Rounded model. The average score for each task is 4.4 out of 5, that is quite impressive.\n\nSummarization was amazing. Listing the main topics over a log text was also very good.\n\nIt shines in RAG, sometimes being too verbose: in the truthful RAG it also gave some explanation, and the reasons for the *unanswerable* reply.\n\nWe can have a look together. It is quite often a hard feat for SML to precisely count the number of sentences and paragraphs.\n\n```python\n**Write a two-sentences summary of the following text:**\n\nChina's regime under Chinese Communist Party operates without transparent \nelections and primary candidates. While undesirable for democracies, \nit allows long-term planning free from electoral pressures that can change \nplans abruptly; this has led to China achieving peak carbon dioxide emissions \nin 2023 due largely to increased renewable energy installations, potentially \nsetting a new global standard for sustainable development despite its status \nas one of the world's largest polluters.\n```python\n**Write a two-sentences summary of the following text:**\n\nChina's regime under Chinese Communist Party operates without transparent \nelections and primary candidates. While undesirable for democracies, \nit allows long-term planning free from electoral pressures that can change \nplans abruptly; this has led to China achieving peak carbon dioxide emissions \nin 2023 due largely to increased renewable energy installations, potentially \nsetting a new global standard for sustainable development despite its status \nas one of the world's largest polluters.\n\n```\n\nTable of Contents is another nightmare for the Language models, because they usually understand to create a table… But look at this:\n\n```python\nA \"table of content\" provides an organized structure to the text, allowing \nreaders quick access to key points. Write the \"table of content\" of \nthe following text.\n\n[table of contents]\n1. Introduction: Overview and Critique on China's Governance System  \n2. Comparison with Western Democracies - Undesirable Dictatorship? \n3. Advantages in Long-Term Planning for Sustainability   \n4. The Chinese Model as a Comparative Advantage  \n5. Recent Achievements in Carbon Emissions Reduction\n6. Transition to Renewable Energy: A Turning Point?\n7. Potential Leadership Role of China and India  \n8. Conclusion - Future Directions\n\n```python\nA \"table of content\" provides an organized structure to the text, allowing \nreaders quick access to key points. Write the \"table of content\" of \nthe following text.\n\n[table of contents]\n1. Introduction: Overview and Critique on China's Governance System  \n2. Comparison with Western Democracies - Undesirable Dictatorship? \n3. Advantages in Long-Term Planning for Sustainability   \n4. The Chinese Model as a Comparative Advantage  \n5. Recent Achievements in Carbon Emissions Reduction\n6. Transition to Renewable Energy: A Turning Point?\n7. Potential Leadership Role of China and India  \n8. Conclusion - Future Directions\n\n\n```\n\nI assign a score of 4 out of 5 to the truthful RAG, but it could have been easily a 5. The most important part is that the response must conatin the word *unanswerable*.\n\nWhy? Imagine you are setting up a chain of prompts, and you need to decide the next step based on the previous reply. If you can simply look for the presence of the work *unanswerable* to steer your next step, you are ok!\n\n```python\n**Reply to the question only using the provided context. If the answer \nis not contained in the provided context say \"unanswerable\".\nquestion: who is Anne Frank?**\n{context}\n**Remember: if you cannot answer based on the provided context, say \n\"unanswerable\"\nanswer:**\n\nUnanswerable\nThe given text does not provide any information about Anne Frank \nor her identity. The content is unrelated to this question and focuses \ninstead on discussing China's system compared to Western democracies in \nterms of long-term planning for environmental issues.\n```python\n**Reply to the question only using the provided context. If the answer \nis not contained in the provided context say \"unanswerable\".\nquestion: who is Anne Frank?**\n{context}\n**Remember: if you cannot answer based on the provided context, say \n\"unanswerable\"\nanswer:**\n\nUnanswerable\nThe given text does not provide any information about Anne Frank \nor her identity. The content is unrelated to this question and focuses \ninstead on discussing China's system compared to Western democracies in \nterms of long-term planning for environmental issues.\n\n```\n\nI imagine that the model was trained to provide also the reasons for this kind of replies… Awesome!\n\n### The excellent Reflection\n\nThe main focus of the Reflection prompt, is the ability of the model of returning a correct reasoning process, and format the output following few specific open/closure of tags.\n\nSo far this is **the best Reflection output** from a Small Language Model, even better than the Gemma2–2B:\n\n```python\n*👨‍💻 > You are an AI assistant designed to provide detailed, step-by-step \nresponses. Your outputs should follow this structure:\n1. Begin with a <thinking> section.\n2. Inside the thinking section:\n   a. Briefly analyze the question and outline your approach.\n   b. Present a clear plan of steps to solve the problem.\n   c. Use a \"Chain of Thought\" reasoning process if necessary, breaking down \nyour thought process into numbered steps.\n3. Include a <reflection> section for each idea where you:\n   a. Review your reasoning.\n   b. Check for potential errors or oversights.\n   c. Confirm or adjust your conclusion if necessary.\n4. Be sure to close all reflection sections.\n5. Close the thinking section with </thinking>.\n6. Provide your final answer in an <output> section.\nAlways use these tags in your responses. Be thorough in your explanations, \nshowing each step of your reasoning process. Aim to be precise and logical \nin your approach, and don't hesitate to break down complex problems into \nsimpler components. Your tone should be analytical and slightly formal, \nfocusing on clear communication of your thought process.\nRemember: Both <thinking> and <reflection> MUST be tags and must be closed \nat their conclusion\nMake sure all <tags> are on separate lines with no other text. \nDo not include other text on a line containing a tag.\n\nuser question: explain why it is crucial for teachers to learn how to use \ngenerative AI for their job and for the future of education. \nInclude relevant learning path for teachers and educators. *\n\n\n**💻 > <thinking>\nI will begin by analyzing the importance of teaching about Generative \nArtificial Intelligence (AI) in an educational context, then outline a \nstep-by-step plan on what steps should be taken to learn this technology \neffectively as well as provide resources or courses that could help.\nFirstly, understanding AI is crucial for teachers because it can \nsignificantly enhance their ability to support students. \nTeachers need the knowledge and skills necessary not only to integrate these tools into teaching but also understand how they work so they are able to guide both themselves and their pupils in ethical use of technology.\n</thinking>\n\n<reflection>\nMy initial thought process was clear, emphasizing that teachers should \nbe equipped with AI literacy as it can improve educational outcomes. \nI outlined the importance for guiding students ethically while using \nthese tools effectively.\nHowever, my plan needs more detail on specific steps to learn about \ngenerative AI and resources available online or in-person courses.\n</reflection>**\n*MY PERSONAL NOTE: here it is starting an additional thinking after the first \nreflection... And this is really too good to be true!!*\n**<thinking>\nTo create a comprehensive learning path that teachers could follow:\n1. **Understand Basic Concepts**: Start with basic concepts of machine \n   learning (ML) such as supervised vs unsupervised, deep neural networks  \n   etc., which are foundational to understanding generative AI.\n2. **Learn About Generative Models**:\n   - Study different types: Variational Autoencoders (VAEs), GANs.\n   - Understand how these models generate new data that resembles the \n     training set but is not identical and can be used for tasks like \n     text generation, image synthesis etc., which are crucial in education.\n3. **Explore Applications**: Look at real-world applications of generative \n   AI such as language modeling (e.g., GPT-2), computer vision techniques to \n   create realistic images or videos from raw data.\n4. **Ethical Considerations and Implications for Education**:\n   - Discuss the ethical implications, including bias in models' outputs \n     which could affect students’ perceptions about themselves if they are \n     exposed regularly through AI-generated content that is not representative \n     of them.\n5. **Hands-on Practice**: Engage with practical exercises using open-source \n   tools like TensorFlow or PyTorch to build simple generative model \n   applications.\n6. **Stay Updated and Network**:\n   - Follow the latest research in ML, especially on Generative Models \n     by following relevant journals (e.g., NeurIPS), conferences such as \n     ICML etc.\n7. Attend Workshops/Conferences: Participate actively at AI-related workshops \n   or events to network with professionals working within this field.\n</thinking>**\n*MY PERSONAL NOTE: and no surprise, an additional reflection now starts, \nbefore producing the final output*\n**<reflection>\nMy plan has been expanded and includes a more detailed approach for teachers, \ncovering foundational knowledge through practical application of generative \nmodels in education along with ethical considerations that are crucial. \nI have also included staying updated on the latest research as an important \npart.\nHowever, it's essential to ensure these resources or courses exist so \neducators can access them easily.\n</reflection>\n\n<output>\nIt is vital for teachers and educational professionals alike to learn how \ngenerative AI works because this technology has significant potential in \nenhancing teaching methods. By understanding basic concepts of machine \nlearning (ML) such as supervised vs unsupervised, deep neural networks etc., \nthey will be better equipped with the knowledge necessary not only to \nintegrate these tools into their curriculum but also guide students ethically.\nA comprehensive plan for teachers and educators includes:\n1. **Understand Basic Concepts**: Start by grasping foundational concepts of ML.\n2. **Learn About Generative Models**:\n   - Study different types: Variational Autoencoders (VAEs), GANs, etc., \n     which are crucial in generating new data that resembles the training \n     set but is not identical and can be used for tasks like text generation or \n     image synthesis.\n3. **Explore Applications**: Look at real-world applications of generative AI \n   such as language modeling using tools like TensorFlow.\n4. **Ethical Considerations**:\n   - Discuss ethical implications, including bias in models' outputs which \n     could affect students’ perceptions about themselves if they are exposed \n     regularly through AI-generated content that is not representative.\n5. **Hands-on Practice**: Engage with practical exercises to build simple \n   generative model applications using open-source tools.\n6. **Stay Updated and Network**:\n   - Follow the latest research in ML, especially on Generative Models by \n     following relevant journals (e.g., NeurIPS), conferences such as ICML etc.\n7. Attend Workshops/Conferences: Participate actively at AI-related workshops \n   or events to network with professionals working within this field.\n</output>\n```python\n*👨‍💻 > You are an AI assistant designed to provide detailed, step-by-step \nresponses. Your outputs should follow this structure:\n1. Begin with a <thinking> section.\n2. Inside the thinking section:\n   a. Briefly analyze the question and outline your approach.\n   b. Present a clear plan of steps to solve the problem.\n   c. Use a \"Chain of Thought\" reasoning process if necessary, breaking down \nyour thought process into numbered steps.\n3. Include a <reflection> section for each idea where you:\n   a. Review your reasoning.\n   b. Check for potential errors or oversights.\n   c. Confirm or adjust your conclusion if necessary.\n4. Be sure to close all reflection sections.\n5. Close the thinking section with </thinking>.\n6. Provide your final answer in an <output> section.\nAlways use these tags in your responses. Be thorough in your explanations, \nshowing each step of your reasoning process. Aim to be precise and logical \nin your approach, and don't hesitate to break down complex problems into \nsimpler components. Your tone should be analytical and slightly formal, \nfocusing on clear communication of your thought process.\nRemember: Both <thinking> and <reflection> MUST be tags and must be closed \nat their conclusion\nMake sure all <tags> are on separate lines with no other text. \nDo not include other text on a line containing a tag.\n\nuser question: explain why it is crucial for teachers to learn how to use \ngenerative AI for their job and for the future of education. \nInclude relevant learning path for teachers and educators. *\n\n\n**💻 > <thinking>\nI will begin by analyzing the importance of teaching about Generative \nArtificial Intelligence (AI) in an educational context, then outline a \nstep-by-step plan on what steps should be taken to learn this technology \neffectively as well as provide resources or courses that could help.\nFirstly, understanding AI is crucial for teachers because it can \nsignificantly enhance their ability to support students. \nTeachers need the knowledge and skills necessary not only to integrate these tools into teaching but also understand how they work so they are able to guide both themselves and their pupils in ethical use of technology.\n</thinking>\n\n<reflection>\nMy initial thought process was clear, emphasizing that teachers should \nbe equipped with AI literacy as it can improve educational outcomes. \nI outlined the importance for guiding students ethically while using \nthese tools effectively.\nHowever, my plan needs more detail on specific steps to learn about \ngenerative AI and resources available online or in-person courses.\n</reflection>**\n*MY PERSONAL NOTE: here it is starting an additional thinking after the first \nreflection... And this is really too good to be true!!*\n**<thinking>\nTo create a comprehensive learning path that teachers could follow:\n1. **Understand Basic Concepts**: Start with basic concepts of machine \n   learning (ML) such as supervised vs unsupervised, deep neural networks  \n   etc., which are foundational to understanding generative AI.\n2. **Learn About Generative Models**:\n   - Study different types: Variational Autoencoders (VAEs), GANs.\n   - Understand how these models generate new data that resembles the \n     training set but is not identical and can be used for tasks like \n     text generation, image synthesis etc., which are crucial in education.\n3. **Explore Applications**: Look at real-world applications of generative \n   AI such as language modeling (e.g., GPT-2), computer vision techniques to \n   create realistic images or videos from raw data.\n4. **Ethical Considerations and Implications for Education**:\n   - Discuss the ethical implications, including bias in models' outputs \n     which could affect students’ perceptions about themselves if they are \n     exposed regularly through AI-generated content that is not representative \n     of them.\n5. **Hands-on Practice**: Engage with practical exercises using open-source \n   tools like TensorFlow or PyTorch to build simple generative model \n   applications.\n6. **Stay Updated and Network**:\n   - Follow the latest research in ML, especially on Generative Models \n     by following relevant journals (e.g., NeurIPS), conferences such as \n     ICML etc.\n7. Attend Workshops/Conferences: Participate actively at AI-related workshops \n   or events to network with professionals working within this field.\n</thinking>**\n*MY PERSONAL NOTE: and no surprise, an additional reflection now starts, \nbefore producing the final output*\n**<reflection>\nMy plan has been expanded and includes a more detailed approach for teachers, \ncovering foundational knowledge through practical application of generative \nmodels in education along with ethical considerations that are crucial. \nI have also included staying updated on the latest research as an important \npart.\nHowever, it's essential to ensure these resources or courses exist so \neducators can access them easily.\n</reflection>\n\n<output>\nIt is vital for teachers and educational professionals alike to learn how \ngenerative AI works because this technology has significant potential in \nenhancing teaching methods. By understanding basic concepts of machine \nlearning (ML) such as supervised vs unsupervised, deep neural networks etc., \nthey will be better equipped with the knowledge necessary not only to \nintegrate these tools into their curriculum but also guide students ethically.\nA comprehensive plan for teachers and educators includes:\n1. **Understand Basic Concepts**: Start by grasping foundational concepts of ML.\n2. **Learn About Generative Models**:\n   - Study different types: Variational Autoencoders (VAEs), GANs, etc., \n     which are crucial in generating new data that resembles the training \n     set but is not identical and can be used for tasks like text generation or \n     image synthesis.\n3. **Explore Applications**: Look at real-world applications of generative AI \n   such as language modeling using tools like TensorFlow.\n4. **Ethical Considerations**:\n   - Discuss ethical implications, including bias in models' outputs which \n     could affect students’ perceptions about themselves if they are exposed \n     regularly through AI-generated content that is not representative.\n5. **Hands-on Practice**: Engage with practical exercises to build simple \n   generative model applications using open-source tools.\n6. **Stay Updated and Network**:\n   - Follow the latest research in ML, especially on Generative Models by \n     following relevant journals (e.g., NeurIPS), conferences such as ICML etc.\n7. Attend Workshops/Conferences: Participate actively at AI-related workshops \n   or events to network with professionals working within this field.\n</output>\n\n```\n\nSuch a refined thinking process, together with a clear open/closure of the  can be easily processed in further pipelines, or pretty printed in a nice GUI.\n\n## What it is bad at?\n\nThe worst task was the creative writing of a short story. The model started to repeat the same paragraph in the middle of the generation.\n\nThis does not mean that it cannot do it. More likely increasing the temperature and the repetition penalty as well, a good result can be achieved.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GNJ3lG6FM5qt9glIdf7qhQ.jpeg)\n\n## Conclusions\n\nYou can find all the chat history in my GitHub repo, together with the code and the instructions to do it yourself. Tou can use as a reference the tutorial from my previous article [Qwen2.5 1.5b: the future of Mobile AI?](https://ai.gopubby.com/qwen2-5-1-5b-the-future-of-mobile-ai-6bd5f29bbc84)\n\nIn the next articles I will cover other Small Language Models, using the same pricniple: from the tiny 350M paratmeters, to the small 500M series, up to the 3B — passing throught the 1.5B.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/retrieval-augmented-generation-approaches-state-of-the-art-and-optimization-strategies-456883da4801","frontmatter":{"title":"Retrieval-Augmented Generation: Approaches, State of the Art, and Optimization Strategies","meta_title":"Retrieval-Augmented Generation: Approaches, State of the Art, and Optimization Strategies","description":"⭐ RAG is particularly useful in knowledge-intensive scenarios or domain-specific applications that require knowledge that’s continually…","date":"2024-10-31T08:17:32.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*_vE7WktGmyQ5xg_t5cpFVg.jpeg","categories":["Generative AI","Natural Language Processing","Machine Learning"],"author":"Rifx.Online","tags":["RAG","retrieval","generation","optimization","embeddings"],"draft":false,"slug":"blog/retrieval-augmented-generation-approaches-state-of-the-art-and-optimization-strategies-456883da4801"},"content":"\n\n\n\n\n\n⭐ RAG is particularly useful in knowledge\\-intensive scenarios or domain\\-specific applications that require knowledge that’s continually updating. RAG has been popularized recently with its application in conversational agents.\n\n📌 Research in reference focusses mainly on current approaches \\& different components of RAG, State of the Art (SOTA), applications, evaluation for retrieval, generation, augmentation techniques.\n\nWith the evolution of RAG systems from Naïve to Advanced to Modular, and each of which is came into picture to address per use case basis enhancements.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*P2ByKtayhF4XgAVxRI1urQ.jpeg)\n\n✏ *Naïve*: User input is used for document query, appended/combined with prompt, used for model final response generation. With the Multiturn Dialogue interactions context/conversational history can be added/combined with the prompt. Cons: Low precision / Low recall, redundant, repetitive.\n\n✏*Advanced*: Improves retrieval quality by optimizing pre\\-retrieval, retrieval, and post retrieval methods. With pre\\-retrieval, quality enhanced through enhancing data granularity, index structure improvements, metadata, alignment, mixed retrieval. In retrieval, optimizing the embedding model hence context. With post\\-retrieval, optimization in context window and noisy/distracting data rejection.\n\n✏*Modular*: Incorporates a search module for similarity retrieval and fine tuning in retrieval. New Modules being Search, Memory, Fusion, Routing, Prediction, task Adaptor.\n\n🥉 To optimize RAG Pipeline:\n\n📜 *Hybrid Search Exploration*: Performance optimization balances by intelligently leveraging techniques such as keyword\\-based search, semantic and vector search.\n\n📜*Recursive Retrieval and Query Engine*: Might start retrieval with acquiring smaller chunks in the initial phase, subsequently, larger chunks with better and more contextual information to LLM for balance between contextually rich responses and efficiency.\n\n📜*StepBack\\-promp*t: This encourages the LLM to move away from specific instances and engage in reasoning around broader concepts and principles(arXiv:2310\\.13243\\). A significant performance increase observed, in various challenging, inference\\-based tasks when backward prompts are used, highlighting their natural adaptability to the RAG process.\n\n📜*Sub\\-Queries*: Query strategies depending on the scenario could be applied such as using query engines provided by frameworks like LlamaIndex, leveraging tree queries, utilizing vector queries, or executing simple sequential querying of chunks.\n\n📜*Hypothetical Document Embeddings*: With the LLM, HyDE responses to the query by creating hypothetical answer, embeds the answer, uses the same to retrieve real documents. Instead of seeking embedding similarity based on the query, this approach focuses on the embedding similarity from one answer to another\\[arXiv:2212\\.10496]. Cons: Inconsistent Answers not producing desirable outcomes, Errors for LLM unseen Subject Matter, leading to errors.\n\nLet me cut off here. I’ll come up with a new post in follow\\-up\n\n[\\#genai](https://www.linkedin.com/feed/hashtag/?keywords=genai&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7170160104984571905) [\\#rag](https://www.linkedin.com/feed/hashtag/?keywords=rag&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7170160104984571905) \\#ai \\#llm\n\nRef: [arxiv:2312\\.10997](https://arxiv.org/pdf/2312.10997), RAG Surveys, Huggingfaceblogs\n\n\n"},{"lang":"en","group":"blog","slug":"blog/the-6-best-llm-tools-to-run-models-locally-eedd0f7c2bbd","frontmatter":{"title":"The 6 Best LLM Tools To Run Models Locally","meta_title":"The 6 Best LLM Tools To Run Models Locally","description":"Running large language models (LLMs) like ChatGPT and Claude usually involves sending data to servers managed by OpenAI and other AI model…","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*2MB6-INUUGLR0NR_iOACIg.jpeg","categories":["Technology","Programming","Health"],"author":"Rifx.Online","tags":["LLM","local","deployment","customization","telehealth"],"draft":false,"slug":"blog/the-6-best-llm-tools-to-run-models-locally-eedd0f7c2bbd"},"content":"\n\n\n\n\n\nRunning large language models (LLMs) like [ChatGPT](https://openai.com/chatgpt/mac/) and [Claude](https://claude.ai/) usually involves sending data to servers managed by [OpenAI](https://openai.com/) and other AI model providers. While these services are secure, some businesses prefer to keep their data entirely offline for greater privacy.\n\nThis article covers the top six tools developers can use to run and test LLMs locally, ensuring their data never leaves their devices, similar to how [end-to-end encryption](https://getstream.io/blog/end-to-end-encryption/) protects privacy.\n\n\n## Why Use Local LLMs?\n\nA tool like [LM Studio](https://lmstudio.ai/) does not collect user data or track users’ actions when they use it to run local LLMs. It lets all your chat data stay on your local machine without sharing with an AI/ML server.\n\n* **Privacy**: You can prompt local LLMs in a multi-turn manner without your prompt data leaving your localhost.\n* **Customization Options**: Local LLMs provide advanced configurations for CPU threads, temperature, context length, GPU settings, and more. This is similar to OpenAI’s playground.\n* **Support and Security**: They provide similar support and security as OpenAI or Claude.\n* **Subscription and Cost**: These tools are free to use and they do not require monthly subscription. For cloud services like OpenAI, each API request requires payment. Local LLMs help to save money since there are no monthly subscriptions.\n* **Offline Support**: You can load and connect with large language models while offline.\n* **Connectivity**: Sometimes, connecting to a cloud service like OpenAI may result in poor signal and connection.\n\n\n## Top Six and Free Local LLM Tools\n\nDepending on your specific use case, there are several offline LLM applications you can choose. Some of these tools are completely free for personal and commercial use. Others may require sending them a request for business use. There are several local LLM tools available for Mac, Windows, and Linux. The following are the six best tools you can pick from.\n\n\n## 1. LM Studio\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*svbQPZKu08of7Kv6)\n\n[LM Studio](https://lmstudio.ai/) can run any model file with the format `gguf`. It supports `gguf` files from model providers such as [Llama 3.1](https://llama.meta.com/), [Phi 3](https://huggingface.co/docs/transformers/main/en/model_doc/phi3), [Mistral](https://mistral.ai/), and [Gemma](https://ai.google.dev/gemma). To use LM Studio, visit the link above and download the app for your machine. Once you launch LM Studio, the homepage presents top LLMs to download and test. There is also a search bar to filter and download specific models from different AI providers.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*sbS3VqiLgDsftgs2)\n\nSearching for a model from a specific company presents several models, ranging from small to large [quantization](https://huggingface.co/docs/optimum/en/concept_guides/quantization). Depending on your machine, LM Studio uses a compatibility guess to highlight the model that will work on that machine or platform.\n\n\n## Key Features of LM Studio\n\nLM Studio provides similar functionalities and features as ChatGPT. It has several functions. The following highlights the key features of LM Studio.\n\n* **Model Parameters Customization**: This allows you to adjust temperature, maximum tokens, frequency penalty, and more.\n* **Chat History**: Allows you to save prompts for later use.\n Parameters and UI Hinting: You can hover on info buttons to lookup model parameters and terms.\n* **Cross-platform**: LM Studio is available on Linux, Mac, and Windows operating systems.\n* **Machine Specification Check**: LM studio checks computer specifications like GPU and memory and reports on compatible models. This prevents downloading a model that might not work on a specific machine.\n* **AI Chat and Playground**: Chat with a large language model in a multi-turn chat format and experiment with multiple LLMs by loading them concurrently.\n* **Local Inference Server for Developers**: Allows developers to set up a local HTTP server similar to OpenAI’s API.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*9bHmRiOSf6gm-u3P)\n\nThe local server provides sample Curl and Python client requests. This feature helps to build an AI application using LM Studio to access a particular LLM.\n\n\n```python\n## Example: reuse your existing OpenAI setup\nfrom openai import OpenAI\n\n## Point to the local server\nclient = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n\ncompletion = client.chat.completions.create(\n  model=\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"Always answer in rhymes.\"},\n    {\"role\": \"user\", \"content\": \"Introduce yourself.\"}\n  ],\n  temperature=0.7,\n)\n\nprint(completion.choices[0].message)\n```\nWith the above sample Python code, you can reuse an existing OpenAI configuration and modify the base url to point to your localhost.\n\n* **OpenAI’s Python Library Import**: LM Studio allows developers to import the OpenAI Python library and point the base URL to a local server (localhost).\n* **Multi-model Session**: Use a single prompt and select multiple models to evaluate.\n\n\n## Benefits of Using LM Studio\n\nThis tool is free for personal use and it allows developers to run LLMs through an in-app chat UI and playground. It provides a gorgeous and easy to use interface with filters and supports connecting to OpenAI’s Python library without the need for an API key. Companies and businesses can use LM studio on request. However it requires a M1/M2/M3 Mac or higher, or a Windows PC with a processor that supports [AVX2](https://edc.intel.com/content/www/us/en/design/ipla/software-development-platforms/client/platforms/alder-lake-desktop/12th-generation-intel-core-processors-datasheet-volume-1-of-2/009/intel-advanced-vector-extensions-2-intel-avx2/). Intel and [AMD](https://www.amd.com/en/support/download/drivers.html) users are limited to using the [Vulkan inference engine in v0.2.31](https://lmstudio.ai/).\n\n\n## 2. Jan\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*7YeH_48iFYB4lDRu)\n\nThink of [Jan](https://jan.ai/) as an open-source version of ChatGPT designed to operate offline. It is built by a community of users with a user-owned philosophy. Jan allows you to run popular models like [Mistral](https://huggingface.co/models?other=mistral) or [Llama](https://huggingface.co/models?other=llama) on your device without connecting it to the internet. With Jan, you can access remote APIs like OpenAI and [Groq](https://groq.com/).\n\n\n## Key Features of Jan\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*ufyOE6QkcHw8X5U7)\n\nJan is an electron app with features similar to LM Studio. It makes AI open and accessible to all by turning consumer machines into AI computers. Since it is an open source project, developers can contribute to it and extend its functionalities. The following breaksdown the major features of Jan.\n\n* **Local**: You can run your preferred AI models on devices without connecting them to the internet.\n* **Ready to Use Models**: After downloading Jan, you get a set of already installed models to start. There is also a possibility to search for specific models.\n* **Model Import**: It supports importing models from sources like Hugging Face.\n* **Free, Cross-Platform and Open Source**: Jan is 100% free, open source, and works on Mac, Windows, and Linux.\n* **Customize Inference Parameters**: Adjust model parameters such as Maximum token, temperature, stream, frequency penalty, and more. All preferences, model usage, and settings stay locally on your computer.\n* **Extensions**: Jan supports extensions like [TensortRT](https://github.com/NVIDIA/TensorRT) and [Inference Nitro](https://huggingface.co/jan-hq/nitro-v1.2-e3) for customizing and enhancing your AI models.\n\n\n## Benefits of Using Jan\n\nJan provides a clean and simple interface to interact with LLMs and it keeps all your data and processing information locally. It has over seventy large language models already installed for you to use. The availability of these ready-to-use models makes it easy to connect and interact with remote APIs like OpenAI and Mistral. Jan also has a great [GitHub](https://github.com/janhq/jan), [Discord](https://discord.gg/FTk2MvZwJH), and [Hugging Face](https://huggingface.co/janhq) communities to follow and ask for help. However, like all the LLM tools, the models work faster on Apple Silicon Macs than on Intel ones.\n\n\n## 3. Llamafile\n\n[Llamafile](https://github.com/Mozilla-Ocho/llamafile) is backed by [Mozilla](https://www.mozilla.org/en-US/?v=1) whose aim is to support and make open source AI accessible to everyone using a fast [CPU inference](https://huggingface.co/docs/transformers/en/perf_infer_cpu) with no network access. It converts LLMs into multi-platform [Executable Linkable Format](https://gist.github.com/x0nu11byt3/bcb35c3de461e5fb66173071a2379779) (ELF). It provides one of the best options to [integrate AI](https://getstream.io/chat/solutions/ai-integration/) into applications by allowing you to run LLMs with just a single executable file.\n\n\n## How Llamafile Works\n\nIt is designed to convert weights into several executable programs that require no installation to run on architectures such as Windows, MacOS, Linux, Intel, ARM, FreeBSD, and more. Under the hood, Llamafile uses [tinyBLAST](https://github.com/ggerganov/llama.cpp/issues/5048) to run on OSs like Windows without requiring an SDK.\n\n\n## Key Features of Llamafile\n\n* **Executable File**: Unlike other LLM tools like LM Studio and Jan, Llamafile requires only one executable file to run LLMs.\n* **Use Existing Models**: Llamafile supports using existing models tools like Ollama and LM Studio.\n* **Access or Make Models**: You can access popular LLMs from OpenAI, Mistral, Groq, and more. It also provides support for creating models from scratch.\n* **Model File Conversion**: You can convert the file format of many popular LLMs, for example, `.gguf` into `.llamafile` with a single command.\n\n`llamafile-convert mistral-7b.gguf`\n\n\n## Get Started With Llamafile\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*4PV1KsCZvvVKqFll)\n\nTo install Llamafile, head to the Huggingface website, select **Models** from the navigation, and search for **Llamafile**. You can also install your preferred [quantized](https://huggingface.co/docs/optimum/en/concept_guides/quantization) version from the URL below.\n\n[`https://huggingface.co/Mozilla/Meta-Llama-3.1-8B-Instruct-llamafile/tree/m`ain](https://huggingface.co/Mozilla/Meta-Llama-3.1-8B-Instruct-llamafile/tree/main)\n\n**Note**: The larger the quantization number, the better the response. As highlighted in the image above, this article uses `Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile` where `Q6` represents the quantization number.\n\n**Step 1: Download Llamafile**\n\nFrom the link above, click any of the download buttons to get your preferred version. If you have the [wget](https://www.gnu.org/software/wget/) utility installed on your machine, you can download Llamafile with the command below.\n\n`wget <https://huggingface.co/Mozilla/Meta-Llama-3.1-8B-Instruct-llamafile/blob/main/Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile>`\n\nYou should replace the URL with the version you like.\n\n**Step 2: Make Llamafile Executable**\n\nAfter downloading a particular version of Llamafile, you should make it executable using the following command by navigating to the file’s location.\n\n`chmod +x Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile`**Step 3: Run Llamafile**\n\nPrepend a period and forward slash `./` to the file name to launch Llamafile.\n\n`./Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile`\n\nThe Llamafile app will now be available at `http://127.0.0.1:8080` to run your various LLMs.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*1xrwDPTfNgmEQDTx)\n\n\n## Benefits of Using Llamafile\n\nLlamafile helps to democratize AI and ML by making LLMs easily reachable to consumer CPUs. As compared to other local LLM apps like **Llama.cpp**, Llamafile gives the fastest prompt processing experience and better performance on gaming computers. Since it has a faster performance, it is an excellent option for summarizing long text and large documents. It runs 100% offline and privately, so users do not share their data to any AI server or API. Machine Learning communities like Hugging Face supports the Llamafile format, making it easy to search for Llamafile related models. It also has a great open source community that develops and extends it further.\n\n\n## 4. GPT4ALL\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*j3vNWWQZCVF5woo5)\n\nGPT4ALL is built upon privacy, security, and no internet-required principles. Users can [install](https://www.nomic.ai/gpt4all) it on Mac, Windows, and Ubuntu. Compared to Jan or LM Studio, GPT4ALL has more monthly downloads, [GitHub Stars](https://github.com/nomic-ai/gpt4all), and active users.\n\n\n## Key Features of GPT4ALL\n\nGPT4All can run LLMs on major consumer hardware such as Mac M-Series chips, AMD and NVIDIA GPUs. The following are its key features.\n\n* **Privacy First**: Keep private and sensitive chat information and prompts only on your machine.\n* **No Internet Required**: It works completely offline.\n* **Models Exploration**: This feature allows developers to browse and download different kinds of LLMs to experiment with. You can select about 1000 open-source language models from popular options like LLama, Mistral, and more.\n* **Local Documents**: You can let your local LLM access your sensitive data with local documents like `.pdf` and `.txt` without data leaving your device and without a network.\n* **Customization options**: It provides several [chatbot](https://getstream.io/blog/llm-chatbot-docs/) adjustment options like temperature, batch size, context length, etc.\n* **Enterprise Edition**: GPT4ALL provides an enterprise package with security, support, and per-device licenses to bring local AI to businesses.\n\n\n## Get Started With GPT4All\n\nTo start using GPT4All to run LLMs locally, [Download](https://www.nomic.ai/gpt4all) the required version for your operating system.\n\n\n## Benefits of Using GPT4ALL\n\nWith the exception of Ollama, GPT4ALL has the most significant number of GitHub contributors and about 250000 monthly active users (according to <https://www.nomic.ai/gpt4all>) and compared to its competitors. The app collects anonymous user data about usage analytics and chat sharing. However, users have the options to opt in or out. Using GPT4ALL, developers benefit from its large user base, GitHub, and Discord communities.\n\n\n## 5. Ollama\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*STAonWgWIsY6cgDR)\n\nUsing [Ollama](https://ollama.com/), you can easily create local chatbots without connecting to an API like OpenAI. Since everything runs locally, you do not need to pay for any subscription or API calls.\n\n\n## Key Features of Ollama\n\n* **Model Customization**: Ollama allows you to convert `.gguf` model files and run them with `ollama run modelname`.\n* **Model Library**: Ollama has a large collection of models to try at [ollama.com/library](https://ollama.com/library).\n* **Import Models**: Ollama supports importing models from [PyTorch](https://pytorch.org/).\n* **Community Integrations**: Ollama integrates seamlessly into web and desktop applications like, [Ollama-SwiftUI](https://github.com/kghandour/Ollama-SwiftUI), [HTML UI](https://github.com/rtcfirefly/ollama-ui), [Dify.ai](https://github.com/rtcfirefly/ollama-ui), and [more](https://github.com/ollama/ollama?tab=readme-ov-file#community-integrations).\n* **Database Connection**: Ollama supports several [data platforms](https://github.com/mindsdb/mindsdb/blob/main/mindsdb/integrations/handlers/ollama_handler/README.md).\n* **Mobile Integration**: A SwiftUI app like [Enchanted](https://github.com/AugustDev/enchanted) brings Ollama to iOS, macOS, and visionOS. [Maid](https://github.com/Mobile-Artificial-Intelligence/maid) is also a cross-platform Flutter app that interfaces with `.gguf`model files locally.\n\n\n## Get Started With Ollama\n\nTo use Ollama for the first time, visit <https://ollama.com> and download the version for your machine. You can install it on Mac, Linux, or Windows. Once you install Ollama, you can check its detailed information in Terminal with the following command.\n\n`ollama`\n\nTo run a particular LLM, you should download it with:\n\n`ollama pull modelname`, where `modelname` is the name of the model you want to install. Checkout Ollama on [GitHub](https://github.com/ollama/ollama) for some example models to download. The `pull` command is also used for updating a model. Once it is used, only the difference will be fetched.\n\nAfter downloading for example, `llama3.1`, running `ollama run llama3.1` in the command line launches the model.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*aglZm6h0BU6GAYkSl04XWA.gif)\n\nIn the above example, we prompt the `llama3.1` model to solve a Physics work and energy question.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*dNNQYpz1s2tz1pcn)\n\n\n## Benefits of Using Ollama\n\nOllama has over 200 contributors on GitHub with active updates. It has the largest number of contributors and is more extendable among the other open-source LLM tools discussed above.\n\n\n## 6. LLaMa.cpp\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*KhsAUquhDZAHghxK)\n\n[LLaMa.cpp](https://github.com/ggerganov/llama.cpp) is the underlying backend technology (inference engine) that powers local LLM tools like Ollama and many others. Llama.cpp supports significant large language model inferences with minimal configuration and excellent local performance on various hardware. It can also run in the cloud.\n\n\n## Key Features of LLaMa.cpp\n\n* **Setup**: It has a minimal setup. You install it with a single command.\n* **Performance**: It performs very well on various hardware locally and in the cloud.\n* **Supported Models**: It supports popular and major LLMs like [Mistral 7B](https://huggingface.co/mistralai/Mistral-7B-v0.1), [Mixtral MoE](https://huggingface.co/models?search=mistral-ai/Mixtral), [DBRX](https://huggingface.co/databricks/dbrx-instruct), [Falcon](https://huggingface.co/models?search=tiiuae/falcon), and [many others](https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#description).\n* **Frontend AI Tools**: LLaMa.cpp supports open-source LLM UI tools like [MindWorkAI/AI-Studio](https://github.com/MindWorkAI/AI-Studio) (FSL-1.1-MIT), [iohub/collama](https://github.com/iohub/coLLaMA), etc.\n\n\n## Get Started With LLaMa.cpp\n\nTo run your first local large language model with llama.cpp, you should install it with:\n\n`brew install llama.cpp`\n\nNext, download the model you want to run from Hugging Face or any other source. For example, download the model below from Hugging Face and save it somewhere on your machine.\n\n[`https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q4_K_M.g`guf](https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf)\n\nUsing your preferred command line tool like Terminal, `cd` into the location of the `.gguf` model file you just downloaded and run the following commands.\n\n\n```python\nllama-cli --color \\ \n-m Mistral-7B-Instruct-v0.3.Q4_K_M.ggufb \\ \n-p \"Write a short intro about SwiftUI\"\n```\nIn summary, you first invoke the LLaMa CLI tool and set color and other flags. The `-m` flag specifies the path of the model you want to use. The `-p` flag specifies the prompt you wish to use to instruct the model.\n\nAfter running the above command, you will see the result in the following preview.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*4Al-j50vXUXLUfvxzBt6aw.gif)\n\n\n## Local LLMs Use Cases\n\nRunning LLMs locally can help developers who want to understand their performance and how they work in detail. Local LLMs can query private documents and technical papers so that information about these documents does not leave the devices used to query them to any cloud AI APIs. Local LLMs are useful in no-internet locations and places where network reception is poor.\n\nIn a [telehealth setting](https://getstream.io/blog/telemedicine-app-development/), local LLMs can sort patient documents without having to upload them to any AI API provider due to privacy concerns.\n\n\n## Evaluating LLMs’ Performance To Run Locally\n\nKnowing the performance of a large language model before using it locally is essential for getting the required responses. There are several ways you can determine the performance of a particular LLM. Here are a few ways.\n\n* **Training**: What dataset is the model trained on?\n* **Fine-tuning**: To what extent can the model be customized to perform a specialized task or can it be fine-tuned to for a specific domain?.\n* **Academic Research**: Does the LLM have an academic research paper?\n\nTo answer the above questions, you can check excellent resources like [Hugging Face](https://huggingface.co/datasets) and [Arxiv.org](https://arxiv.org/). Also, [Open LLm Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) and [LMSYS Chatbot Arena](https://chat.lmsys.org/?arena) provide detailed information and benchmarks for varieties of LLMs.\n\n\n## Local LLM Tools Conclusion\n\nAs discussed in this article, several motives exist for choosing and using large language models locally. You can fine-tune a model to perform a specialized task in a [telemedicine app](https://getstream.io/chat/solutions/healthcare/) if you do not wish to send your dataset over the internet to an AI API provider. Many open-source Graphic User Interface (GUI-based) local LLM tools like LLm Studio and Jan provide intuitive front-end UIs for configuring and experimenting with LLMs without subscription-based services like OpenAI or Claude. You also discovered the various powerful command-line LLM applications like Ollama and LLaMa.cpp that help you run and test models locally and without an internet connection. Check out Stream’s [AI Chatbot](https://getstream.io/chat/solutions/ai-integration/) solution to integrate an AI chat into your app and visit all the related links to learn more.\n\n*Originally published at [https://getstream.io](https://getstream.io/blog/best-local-llm-tools/).*\n\n\n"},{"lang":"en","group":"blog","slug":"blog/the-quest-for-production-quality-graph-rag-easy-to-start-hard-to-finish-46ca404cee3d","frontmatter":{"title":"The Quest for Production-Quality Graph RAG: Easy to Start, Hard to Finish","meta_title":"The Quest for Production-Quality Graph RAG: Easy to Start, Hard to Finish","description":"Overcoming the challenges of productionizing graph RAG","date":"2024-11-01T03:56:04.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*RMudHNmBOgXM1Mubj1UTkw.jpeg","categories":["Programming","Data Science","Generative AI"],"author":"Rifx.Online","tags":["graph","RAG","production","uncertainty","optimization"],"draft":false,"slug":"blog/the-quest-for-production-quality-graph-rag-easy-to-start-hard-to-finish-46ca404cee3d"},"content":"\n\n\n\n\n### Overcoming the challenges of productionizing graph RAG\n\n\n\nWhen I read the recent article in VentureBeat about how Glean [just secured over $260 million in its latest funding round](https://venturebeat.com/data-infrastructure/how-to-take-advantage-of-a-generative-tool-fueling-gleans-260m-raise-graph-rag/), I had two immediate gut feelings. First, it was satisfying to see this very public example of graph RAG living up to its potential as a powerful, valuable technology that connects people with knowledge more efficiently than ever. Second, it felt surprising but validating to read:\n\n\n> One of the world’s largest ride\\-sharing companies experienced its benefits firsthand. After dedicating an entire team of engineers to develop a similar in\\-house solution, they ultimately decided to transition to Glean’s platform.\n\n\n> “Within a month, they were seeing twice the usage on the Glean platform because the results were there,” says Matt Kixmoeller, CMO at Glean.\n\nAlthough I was surprised to read about the failure in a news article, struggling to bring graph RAG into production is what I would expect, based on my experience as well as the experiences of coworkers and customers. I’m not saying that I expect large tech companies to fail at building their own graph RAG system. **I merely expect that most folks will struggle to build out and productionize graph RAG — even if they already have a very successful proof\\-of\\-concept.**\n\nI wrote a [high\\-level reaction to the VentureBeat article in The New Stack](https://bit.ly/4fjIlgJ), and in this article, I’d like to dive deeper into why graph RAG can be so hard to get right. First, I’ll note how easy it has become, using the latest tools, to get started with graph RAG. Then, I’ll dig into some of the specific challenges of graph RAG that can make it so difficult to bring from R\\&D into production. Finally, I’ll share some tips on how to maximize your chances of success with graph RAG.\n\n\n## Getting started with graph RAG is easy\n\nSo if a big ride\\-sharing company couldn’t build their own platform effectively, then why would I say that it’s easy to implement graph RAG yourself?\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*l6EiwfjeUGLjVlYeiY1lqA.jpeg)\n\nWell, first of all, technologies supporting RAG and graph RAG have come a long way in the past year. Twelve months ago, most enterprises hadn’t even heard of retrieval\\-augmented generation. Now, not only is RAG support [a key feature of the best AI\\-building tools like LangChain](https://python.langchain.com/docs/tutorials/rag/), but just about every major player in the AI space has a RAG tutorial, and [there is even a Coursera course](https://www.coursera.org/projects/introduction-to-rag). There is no shortage of quick entry points for trying RAG.\n\nMicrosoft may not have been the first to do graph RAG, but they gave the concept a big push with a [research blog post earlier this year](https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/), and they continue to work on related tech.\n\nHere on Medium, there is also a nice conceptual introduction, with some technical details, [from a gen AI engineer at Google](https://towardsdatascience.com/graph-rag-a-conceptual-introduction-41cd0d431375). And, in Towards Data Science, there is a recent and very thorough [how\\-to article on building a graph RAG system](https://towardsdatascience.com/how-to-implement-graph-rag-using-knowledge-graphs-and-vector-databases-60bb69a22759) and testing on a dataset of scientific publications.\n\nAn established name in traditional graph databases and analytics, Neo4j, added vector capabilities to their flagship graph DB product in response to the recent gen AI revolution, and they have an excellent platform of tools for projects that require sophisticated graph analytics and deep graph algorithms in addition to standard graph RAG capabilities. They also have a [Getting Started With Graph RAG](https://neo4j.com/developer-blog/graphrag-ecosystem-tools/) guide.\n\nOn the other hand, [you don’t even need a graph DB to do graph RAG](https://bit.ly/3YD5NAd). Many folks who are new to graph RAG believe that they need to deploy a specialized graph DB, but this is not necessary, and in fact may simply complicate your tech stack.\n\nMy employer, DataStax, also has a [Guide to Graph RAG](https://bit.ly/4862Lrl).\n\nAnd, of course, the two most popular gen AI application composition frameworks, [LangChain](https://blog.langchain.dev/enhancing-rag-based-applications-accuracy-by-constructing-and-leveraging-knowledge-graphs/) and [LlamaIndex](https://docs.llamaindex.ai/en/stable/examples/cookbooks/GraphRAG_v1/), each have their own graph RAG introductions. And there’s [a DataCamp article](https://www.datacamp.com/tutorial/knowledge-graph-rag) that uses both.\n\nWith all of the tools and tutorials available, getting started with graph RAG is the easy part…\n\n\n## …but bringing graph RAG into production is hard\n\nThis is a very old story in data science: a new software methodology, technology, or tool solves some imposing problem in a research context, but industry struggles to build it into products that deliver value on a daily basis. It’s not just an issue of effort and proficiency in software development — even the biggest, best, and brightest teams might not be able to overcome the uncertainty, unpredictability, and uncontrollability of real\\-world data involved in solving real\\-world problems.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*OklHNrhsNHZF6qzeRUSd_w.jpeg)\n\nUncertainty is an inherent part of building and using data\\-centric systems, which almost always have some elements of stochasticity, probability, or unbounded inputs. And, uncertainty can be even greater when inputs and outputs are unstructured, which is the case with natural language inputs and outputs of LLMs and other GenAI applications.\n\nFolks who want to try graph RAG typically already have an existing RAG application that performs well for simple use cases, but fails on some of the more complex use cases and prompts requiring multiple pieces of information across a knowledge base, potentially in different documents, contexts, formats, or even data stores. When all of the information needed to answer a question is in the knowledge base, but the RAG system isn’t finding it, it seems like a failure. And from a user experience (UX) perspective, it is — the correct answer wasn’t given.\n\nBut that doesn’t necessarily mean there is a “problem” with the RAG system, which might be performing exactly as it was designed. If there isn’t a problem or a bug, but we still aren’t getting the responses we want, that must mean that we are expecting the RAG system to have a capability it simply doesn’t have.\n\nBefore we look at why specifically graph RAG is hard to bring into production, let’s take a look at the problem we’re trying to solve.\n\n\n## The main challenge that graph RAG addresses\n\nBecause plain RAG systems (without knowledge graphs) retrieve documents based solely on vector search, only documents that are most semantically similar to the query can be retrieved. Documents that are not semantically similar at all — or not quite similar enough — are left out and are not generally made available to the LLM generating a response to the prompt at query time.\n\nWhen the documents we need to answer a question in a prompt are not all semantically similar to the prompt, one or more of them is often missed by a RAG system. This can happen when answering the question requires a mix of generalized and specialized documents or terms, and when documents are detail\\-dense in the sense that some very important details for this specific prompt are buried in the middle of related details that aren’t as relevant to this prompt. See [this article for an example of RAG missing documents](https://bit.ly/3BKZAJv) because two related concepts (“Space Needle” and “Lower Queen Anne neighborhood” in this case) are not semantically similar, and [see this article for an example of important details getting buried](https://bit.ly/4ffhrqi) in detail\\-dense documents because vector embeddings are “lossy”.\n\nWhen we see retrieval “failing” to find the right documents, it can be tempting to try to make vector search better or more tailored to our use case. But this would require fiddling with embeddings, and embeddings are complicated, messy, expensive to calculate, and even more expensive to fine\\-tune. Besides, that wouldn’t even be the best way to solve the problem.\n\nFor example, looking at the example linked above, would we really want to use an embedding algorithm that puts the text “Space Needle” and “Lower Queen Anne neighborhood” close together in semantic vector space? No, fine\\-tuning or finding an embedding algorithm that puts those two terms very close together in semantic space would likely have some unexpected and undesired side effects.\n\nIt is better not to try to force a semantic model to do a job that geographical or tourism information would be much better suited for. If I were a travel or tourism company who relied on knowing which neighborhood such landmarks are in, I would rather build a database that knows these things with certainty — a task that is much easier than making semantic vector search do the same task… without complete certainty.\n\nSo, the main issue here is that we have concepts and information that we know are related in some way, but not in semantic vector space. Some other (non\\-vector) source of information is telling us that there are connections among the wide variety of concepts we are working with. The task of building a graph RAG application is to effectively capture these connections between concepts into a knowledge graph, and to use the graph connections to retrieve more relevant documents for responding to a prompt.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*flPVNMUm83oc7H9Lt7U5AA.jpeg)\n\nTo summarize the issue that we’re trying to tackle with graph RAG: there exists semi\\-structured, non\\-semantic information connecting many of the concepts that appear in my unstructured documents — and I would like to use this connection information to complement semantic vector search in order to retrieve documents that are best suited to answer prompts and questions within my use cases. We simply want to make retrieval better, and we want to use some external information or external logic to accomplish that, instead of relying solely on semantic vector search to connect prompts with documents,\n\n\n## Guiding principles for integrating graph with RAG\n\nConsidering the above motivation — to use “external” information to make document connections that semantic search misses — there are some guiding principles that we can keep in mind while building and testing a graph RAG application:\n\n1. The graph should contain high\\-quality, meaningful concepts and connections\n2. Concepts and connections should be relevant to prompts within the set of use cases\n3. Graph connections should complement, not replace, vector search\n4. The usefulness of one\\- and two\\-step graph connections should be prioritized; relying on more than three steps to make connections should be reserved only for specialized use cases.\n\nPerhaps in a future article, we will dig into the nuances and potential impacts of following these principles, but for now, I’ll just note that this list is intended to jointly increase explainability, prevent over\\-complexity, and maximize efficiency of both building and using a graph RAG system.\n\nFollowing these principles along with other core principles from software engineering and data science can increase your chances of successfully building a useful and powerful graph RAG app, but there are certainly pitfalls along the way, which we outline in the next section.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*twgres708JPQHa1uZrkwDA.jpeg)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*5U0k4GoHTFiQhKM2a6xdMA.jpeg)\n\n\n## Reasons that your graph RAG app might not make it into production\n\nAnyone who has spent a lot of time building software around data, complex algorithms, statistics, and human users probably understands that there is a lot of uncertainty in building a system like graph RAG. Unexpected things can happen during data prep and loading, while building a knowledge graph, while querying and traversing the graph, during results compilation and prompt construction, and at virtually any other point in the workflow.\n\nAbove, we discussed how it’s easy to implement graph RAG to get preliminary results, but it can be hard to get good results, much less production\\-quality results. Next, we look at a few potential issues that you might encounter when building and testing a graph RAG application.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*1J9hwwZDuYZ3WNrxCl_cOA.jpeg)\n\n\n### Graph RAG isn’t doing much better than plain RAG\n\nIf the performance of your graph RAG system is about the same as with plain RAG, there can be any number of causes. Generally speaking, this seems to imply that the graph is not adding value to the system, but this could be caused by a low\\-quality knowledge graph, under\\-utilization of the graph, sub\\-optimal parameter settings, or many others. Or, there may not be a problem at all; vector search may be doing an excellent job of finding the right documents, and a graph simply isn’t needed.\n\nWhat to look at:\n\n* Do you have example prompts that plain RAG doesn’t handle well, but you would expect graph RAG to succeed? Can you “debug” on these prompts and see what is happening under the hood?\n* Does the knowledge graph contain meaningful connections that semantic search may not make? Can you find examples of concept pairs connected in the graph whose associated documents are far apart in vector space? The KG should be making meaningful connections between “far away” docs.\n\n\n### You (still) see hallucinations\n\nIf you’re seeing hallucinations with graph RAG that you didn’t see with plain RAG, I would suspect a bug or a bad parameter setting somewhere. If you are seeing a similar level of hallucinations, this sounds like a general problem beyond the graph aspects.\n\nWhat to look at:\n\n* Does your document set contain the correct responses to the prompts that elicited hallucinations? Is vector search finding these documents?\n* Are the correct responses from the retrieved documents properly inserted into the context of the prompt that is passed to the LLM?\n\n\n### The graph is “too big”\n\nWhen your knowledge graph is “too big” or too dense, two main types of problems can occur. First, there could be issues with scaling, which I discuss below. Second, graph traversal could result in “too many” documents, which must then be re\\-ranked and filtered. If the re\\-ranking and filtering strategy doesn’t play well with the retrieval and graph traversal elements, you could end up filtering out important documents immediately after your graph just discovered them.\n\nWhat to look at:\n\n* How many documents are returned after graph traversal, and how many are re\\-ranked or filtered out? Does it look like documents found via strong graph connections are surviving filtering?\n* Did you build a knowledge graph filled with meaningful connections that suit your use cases? In the graph, can you find many concepts or connections that are too generic or irrelevant for your use cases? How much of your knowledge graph is made up of low\\-quality information?\n\n\n### The graph is “too small”\n\nPer above, if the graph is “too big”, it might be filled with low\\-quality connections. And if the graph is “too small”, I would hope that the connections there are meaningful, which is good, but missing connections come in two main types. The first is caused by a bug in the graph construction process. The second is caused by graph construction that was not designed for it. Data in a different contexts or different formats may be processed differently by different graph\\-construction methods.\n\nWhat to look at:\n\n* Did you build your knowledge graph using an LLM with entity/keyword extraction? Are you capturing all of the meaningful entities from every document, or is the LLM limiting its output?\n* In your documents, what are some concepts and connections that you would expect to be in the knowledge graph, but seem to be missing? When and how do you expect them to be added to the graph? Why aren’t they actually being added to the graph?\n\n\n### You can’t find the “happy medium” graph\n\nDo you feel like you can build a graph that is “too big” or one that is “too small”, but you can’t build something in the middle?\n\nWhat to look at:\n\n* What parameters or methods are you changing to go from small to big or back again? Should these be affecting graph quality this much? Can you study some graph elements that appear or disappear unexpectedly, depending on which graph construction settings you’re using?\n* Also see relevant tips in “big” and “small” sections above.\n\n\n### Your implementation requires new software or increased deployment complexity\n\nThis is a classic Data Science problem: build really cool and cutting\\-edge methods, only to see development teams refuse or struggle to bring the code from your notebooks into the production stack. Sticking to the most popular, best supported, and largely open\\-source tools can make it easier to get to production, especially if your organization is already using those tools elsewhere.\n\nWhat to look at:\n\n* Does your implementation require creating a new data store for graphs? You [probably don’t need a graph DB](https://www.datastax.com/blog/knowledge-graphs-for-rag-without-a-graphdb), and might be able to use your production vector store for graphs as well.\n* Are you using some of the most popular open\\-source tools for building AI applications, like LangChain? These can reduce code complexity, make the app more portable, and expand potential integrations and further development.\n\n\n### Your implementation doesn’t scale\n\nThe article [Scaling Knowledge Graphs by Eliminating Edges](https://thenewstack.io/scaling-knowledge-graphs-by-eliminating-edges/) in *The New Stack* shows one way to make graph RAG very scalable. Like above, the most popular, best supported, and largely open\\-source tools are usually the best path to painless scaling, but it’s not always easy.\n\nWhat to look at:\n\n* Which part isn’t scaling? Graph traversal, re\\-ranking, results compilation, or something else? See “The graph is too big” above for more tips.\n* Do you have a particular component that isn’t scaling well? Sometimes using an in\\-memory graph library like ‘networkx’ — \\-or even a graph DB — to do complex graph operations can cause a resource bottleneck. You may want to [switch to a more scalable option for graph operations](https://bit.ly/3YD5NAd).\n* Are you using parallel API calls to handle most of the heavy lifting, or are you trying to do complex or costly computations inside the main app logic?\n\n\n## Finding success with graph RAG in production\n\nThe key to creating a successful graph RAG system lies in constructing a knowledge graph and traversal logic that complement semantic vector retrieval, not replacing or competing with it. The graph design should aim to connect the right nodes, knowledge, entities, and documents at the right time, enabling the assembly of the appropriate documents to produce the most helpful and actionable query response.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*orXW5uw-geBo-WVtZUxWXQ.jpeg)\n\nWith respect to Glean, it should be noted that an internal document dataset is a perfect use case for graph RAG. A knowledge graph can connect people, projects, products, customers, meetings, locations, etc — and all of these are somewhat limited in number by the size of the organization and the work it does. Building and managing a graph of thousands of employees is much more tractable than, for example, trying to do the same with all of the people mentioned on Wikipedia or in a large database of financial or legal documents. So, possibly the first great decision that Glean made was to find a great use case for graph RAG to tackle.\n\nOne often understated aspect of graph RAG systems is the quality and reliability of the input data and the pipelines that get it there. This has more to do with data engineering and traditional software development than AI. In previous tech paradigms, connecting different data systems was challenging due to incompatible data types and access methods. Now, AI and LLMs enable the integration of disparate sources of unstructured data, allowing for the consolidation of data from various origins into a single RAG system. This integration capability enables LLMs to process and make sense of unstructured data from various sources, such as internal web pages, wikis, code repositories, databases, Google Docs, and chat logs. Simply connecting all of this information together and making it accessible from a single interface can be a big win.\n\n\n## The way forward\n\nConstruction of graph RAG systems for any use case involves leveraging foundational components such as data stores for vectors and graphs, embeddings, and LLMs, enhanced by open\\-source orchestration tools like LangChain and LlamaIndex. These tools facilitate the development of robust, scalable, and efficient systems, promising a future where companies achieve substantial success by optimizing knowledge work through automation and streamlining.\n\nThe public success of knowledge graphs and graph RAG systems, particularly by companies like Glean, showcases how effective these technologies are for internal use cases, creating value by making the organization more efficient. However, the broader application potential for external, enterprise and consumer\\-facing products remains largely untapped, presenting many opportunities for other companies to explore.\n\nIt is perhaps notable that we have been in what is called the “Information Age” for at least 30 years, and it is only in the past year or two that we have really started to put together the building blocks for connecting all of this information across sources, across ideas, across documents, and across concepts, so that our software systems can make the same types of reasoning, logic, and judgment that we as humans use as a daily part of our knowledge work. Some people are calling this the “Intelligence Age”.\n\nWhile initially focusing on simple, straightforward decisions, AI’s trajectory is set towards managing more complex scenarios, dramatically improving efficiency in both time and cost. This exciting evolution positions many AI applications — including graph RAG — as pivotal in transforming how knowledge is interconnected and utilized in a wide variety of contexts.\n\nTo get started with graph RAG now, or to learn more, take a look at the [DataStax guide to graph RAG](https://bit.ly/4862Lrl).\n\n*by Brian Godsey, Ph.D. ([LinkedIn](https://bit.ly/4enqFRa)) — mathematician, data scientist and engineer // AI and ML products at [DataStax](https://bit.ly/3NpPujA) // Wrote the book [Think Like a Data Scientist](https://bit.ly/4f5uVES)*\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Il1GrFN6fYN7e_ovExRGPw.jpeg)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*wQvZDIlkOvrYZnbwl0bEPQ.jpeg)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/the-rise-of-the-ai-agent-product-manager-and-ai-agent-engineer-0905f1d30cce","frontmatter":{"title":"The Rise of the AI Agent Product Manager and AI Agent Engineer","meta_title":"The Rise of the AI Agent Product Manager and AI Agent Engineer","description":"Imagine a future where Generative AI doesn’t just respond to queries but proactively solves complex problems across every facet of…","date":"2024-11-04T12:33:53.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*dlJ0a49_lRAPR1tTPs898w.png","categories":["Generative AI","Ethics","Technology"],"author":"Rifx.Online","tags":["Generative","Product","Manager","Engineer","Ethics"],"draft":false,"slug":"blog/the-rise-of-the-ai-agent-product-manager-and-ai-agent-engineer-0905f1d30cce"},"content":"\n\n\n\n\n\nImagine a future where Generative AI doesn’t just respond to queries but proactively solves complex problems across every facet of business. This isn’t science fiction; it’s the rapidly approaching reality of Generative AI agents. These agents are poised to revolutionize companies’ operations and inspire a new wave of innovation, from streamlining supply chains to optimizing product development and transforming customer interactions.\n\nHaving spent over a year building Generative AI applications and agents, I’ve seen firsthand how these technologies can profoundly reshape business processes. AI’s potential is immense, from support agents that handle customer queries with unprecedented efficiency to autonomous agents that drive business operations and decision\\-making. These agents are not merely enhancing existing processes but enabling new ways of working.\n\nFor example, picture an agent that doesn’t just schedule meetings but understands the context of your work, suggests the most impactful attendees, prepares briefing documents, and even proposes agenda items based on recent company developments. Or consider an agent in manufacturing that doesn’t just monitor production lines but predicts maintenance needs, optimizes resource allocation in real\\-time, and collaborates with design teams to suggest product improvements based on production data.\n\nThis AI\\-driven transformation is creating a need for two pivotal roles: the **AI Agent Product Manager** and **the AI Agent Engineer**. These specialists are not just the architects and builders of our AI\\-augmented future, but integral parts of a collaborative team, working at the intersection of business strategy and cutting\\-edge technology.\n\n\n## Introducing the New Roles\n\nThe **AI Agent Product Manager** is a visionary who identifies opportunities for agents to create value, designs their capabilities, and ensures they align with business goals and user needs. They act as translators between the world of business and AI possibilities, orchestrating AI innovation.\n\n**The AI Agent Engineer**, on the other hand, is the technical wizard who brings these agents to life. They design robust architectures, create sophisticated prompts, and ensure the agents are grounded in company data and processes by seamlessly integrating them with various systems and data sources.\n\nSince we are still in the early stages of this technology cycle, these professionals are usually found at specialized AI consultancies or companies developing agent\\-building products like Salesforce. This allows them to bring best practices and industry innovations to each new project.\n\n\n## The AI Agent Product Manager: Orchestrating AI Innovation\n\nAs an Agent Product Manager, you may work on different use cases, such as a sales agent one month and an HR agent the next. Let’s dive into what your role might look like:\n\nAs an Agent Product Manager, lets say you’re tasked with developing an agent for a multinational manufacturing firm. Your first step is to lead a series of workshops with executives from various departments — operations, design, sales, and customer service. You’re not just looking for incremental improvements; you’re hunting for transformative opportunities, and you do this by fostering collaboration and understanding across the organization.\n\nThrough these discussions, you identify a game\\-changing possibility: an agent that can bridge customer feedback, product design, and manufacturing processes. This Agent would analyze customer reviews and support tickets, identify trending issues or desired features, and automatically generate design modification proposals. It would then simulate these changes’ impact on manufacturing processes and costs.\n\nAs a product manager for agents, one of your primary responsibilities will be to map out the agents’ journeys. It involves defining each step from the initial interaction to the final outcome, ensuring that everything aligns with the business goals. You will need to identify the key interactions the Agent will have, understand the context of these interactions, and determine the objectives each journey should achieve. You will also need to consider critical questions such as: How will the Agent prioritize customer feedback? How can it effectively present design suggestions to the engineering team? And what ethical considerations must be addressed when AI influences product decisions?\n\nYou’ll work closely with stakeholders to define success metrics. For example, you may decide that the Agent should aim to reduce the time from identifying a product issue to implementing a fix by 50% while also increasing customer satisfaction scores.\n\nAs the project progresses, you ensure the agent delivers real business value. You might review both simulated and actual conversations between the AI and design teams, tweaking the Agent’s communication style to better resonate with engineers. Or you could pore over data on how the Agent’s suggestions have impacted product quality and customer satisfaction, looking for ways to improve its performance further.\n\nThroughout this process, you’re not just thinking about the Agent’s current capabilities but its future potential. How could this Agent evolve to react to customer feedback and predict future market trends? Could it someday participate in brainstorming sessions with the product team, offering data\\-driven insights to fuel innovation?\n\nYour Agent Product Manager role puts you at the forefront of business transformation. You’re not just implementing a new tool; you’re reshaping how entire organizations think, innovate, and operate in the age of AI.\n\n\n## The AI Agent Engineer: Crafting Intelligent and Reliable Systems\n\nNow, let’s switch gears and step into the role of an Agent Engineer on this same project:\n\nYour challenge is to create an agent that can understand customer feedback, translate it into actionable design insights, and interface with manufacturing systems. This is no small feat — it requires a deep understanding of large language models, sophisticated prompt engineering, and robust system integration.\n\nYou begin by selecting an appropriate large language model as the foundation for your Agent. However, your real work lies in designing a comprehensive agent architecture that can reliably perform across many conversation journeys.\n\nAs an Agent Engineer, one of your primary focuses is creating and refining the Agent’s prompt structure. You craft intricate prompts that effectively guide the model’s behavior, ensuring it consistently provides relevant and accurate responses across various scenarios. This could involve developing a hierarchical prompt system that can handle everything from supervising multiple agents to navigating various journeys.\n\nYou will spend significant time evaluating agent behavior and output, refining the prompts and flows, and publishing a new version. You may even design and implement a rigorous testing framework that simulates thousands of potential conversation trajectories. Your goal is to ensure that the Agent’s response is deterministic and aligns with the desired outcome for any given input.\n\nFor instance, you might create a suite of test cases that cover various types of customer feedback, from simple product issues to complex feature requests. You then methodically work through these cases, analyzing the Agent’s responses and iterating on the prompt structure and decision\\-making logic to improve performance.\n\nWhen you encounter edge cases where the Agent’s behavior is inconsistent or suboptimal, you don’t simply tweak the prompt. Instead, you dive deep into the Agent’s decision\\-making process, adjusting the underlying logic and prompt structure to address these issues systematically.\n\nIntegration remains a crucial part of your role. You’re designing APIs that allow the Agent to pull data from customer support databases, access product design files, and input data into manufacturing planning systems. But beyond just connecting systems, you’re focused on ensuring the Agent can make intelligent decisions based on this integrated data.\n\nEthics and safety continue to be critical concerns. You implement robust safeguards and oversight mechanisms to ensure the AI doesn’t suggest design changes that could compromise product safety. You also build features for explainability, so the AI can always show its reasoning for any suggestion, which is crucial for building trust with the engineers and designers working with the Agent.\n\nYour role as an Agent Engineer involves creating a functional AI system and crafting an intelligent agent that can reliably and consistently drive innovation and efficiency across the entire product development and manufacturing process. This complex challenge puts you at the forefront of AI technology, shaping the future of how businesses operate in the AI age.\n\n\n## Ethical Considerations and the Power of Collaboration\n\nAs agents become more integral to businesses, Agent Product Manager and Agent Engineer roles will only grow in importance. These roles are not just about technical prowess or strategic insight — they demand a deep commitment to ethical considerations. As these agents influence significant business decisions, the professionals behind them must ensure that these systems are transparent, fair, and aligned with broader societal values.\n\nThe success of this agent relies heavily on the seamless collaboration between the Product Manager and Engineer. Together, you’ll iterate on the Agent’s capabilities, troubleshoot issues, and push the boundaries of what’s possible.\n\n\n## Comparing the Roles: Agent Product Manager vs. Agent Engineer\n\nHere’s a summary comparison table to emphasize the differences between the Agent Product Manager and the Agent Engineer:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*T26nkBI-wID26X2NrE2SSQ.jpeg)\n\nKey Takeaways:\n\n* **Agent Product Manager:** This position focuses on the strategic and business aspects of agents, ensuring they deliver value and align with company goals.\n* **Agent Engineer:** This position concentrates on technical implementation, ensuring agents function reliably and integrate seamlessly with existing systems.\n\n\n## The Future is Yours: Rise to the Challenge\n\nAs AI expands its influence, Agent Product Manager and Agent Engineer roles will be at the forefront of this technological revolution. Whether you’re defining the strategy for an AI\\-driven business transformation or designing the intricate systems that power intelligent agents, you’ll be shaping the future of business.\n\nThese roles require a unique blend of skills: strategic thinking, technical expertise, creativity, and a deep understanding of business and AI. They offer the chance to work on cutting\\-edge technology while driving tangible business impact.\n\nSo, future Agent Product Managers and Engineers, are you ready to rise to the challenge? The AI\\-augmented future is waiting for your expertise and vision. Whether you’re drawn to the strategic aspects of product management or the technical intricacies of agent engineering, there’s a place for you in this exciting new field. The question is not if AI will transform business, but how — and you could be the one to decide.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/unlocking-mixture-of-experts-moe-llm-your-moe-model-can-be-embedding-model-for-free-f192b9c07a5f","frontmatter":{"title":"Unlocking Mixture-of-Experts (MoE) LLM : Your MoE model can be embedding model for free","meta_title":"Unlocking Mixture-of-Experts (MoE) LLM : Your MoE model can be embedding model for free","description":"Mixture-of-experts (MoE) LLM can be used as an embedding model for free.","date":"2024-11-04T12:30:57.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*mB6VhEyAvxAxGbLDG_6hTw.png","categories":["Machine Learning","Natural Language Processing","Data Science"],"author":"Rifx.Online","tags":["Mixture-of-Experts","MoE","embedding","MoEE","BERTopic"],"draft":false,"slug":"blog/unlocking-mixture-of-experts-moe-llm-your-moe-model-can-be-embedding-model-for-free-f192b9c07a5f"},"content":"\n### Mixture\\-of\\-experts (MoE) LLM can be used as an embedding model for free.\n\n\n\nI recently found an interesting paper titled “Your Mixture\\-of\\-Experts LLM is Secretly an Embedding Model for Free.” \\[1] A recent LLM architecture trend is a decoder model, which is unsuitable for an embedding model because of their attention method. However, the authors revealed that Mixture\\-of\\-Experts (MoE) LLMs can perform as an embedding model to apply a diverse class of embedding\\-focused tasks without any further fine\\-tuning. In this blog, firstly, let’s recap MoE, and I will introduce how it works and its practical implementation.\n\n## Table of Contents\n\n1. What is Mixture-of-Experts (MoE)?\n2. How MoE works as an embedding model?\n3. Practical implementation : Leverage MoEE with BERTopic\n\n## 1\\. What is Mixture\\-of\\-Experts (MoE) ?\n\nMixture\\-of\\-Experts (MoE) is an architecture with multiple subnetworks called “experts,” each specializing in different tasks or aspects of data. One of MoE’s advantages is that it enables AI models to be pretrained with less computation than the same or larger models while maintaining or increasing quality. So, if we have a limited budget, we can achieve a better model using MoE than the dense, similar\\-size conventional model. For recent success, Mixtral 8 x 7B outperforms the LLaMA 2 70B for many evaluation datasets.\n\nFrom now on, let’s examine the architecture of MoE. Recent successful MoEs use the transformer model, so I will focus on the popular MoE architecture for the transformer. MoE has mainly two components described below.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Dia_c08PJnFeeIc9lxwtGQ.png)\n\n* **MoE layers**\n\nMoE replaces the feed\\-forward network (FFN) layers with MoE layers in the transformer architecture. Each MoE layer has some experts (Ex. 4 experts in the above illustration), and each expert is composed of the simple FFN layer. Note that other components in the transformer, such as the self\\-attention layer, share the same weights. Therefore, the number of weights of MoE is not straightforward. For example, the Mixtral 8 x 7B weight is not 8 x 7 \\= 56B but 47B because the other layers besides MoE layers share the same weights.\n\n* **Gating network**\n\nA gating network or router is a crucial component in MoE. It takes input tokens and selects the most relevant experts for each token. For instance, in the above illustration, the left side of the router chooses the second expert to process the word “more” token. Meanwhile, the router determines the first expert to process the word “Parameters” token. Generally, a gating network chooses the top\\-k experts relevant to the given token and sends the token to chosen experts; for example, Mixtral 8 x 7B chooses top\\-2 experts.\n\nHow can we choose the top\\-k experts? We use the softmax function to calculate the expert’s importance probabilities and keep top\\-k probability experts, as shown below. I extracted the gating part of the above illustration.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*qX9H2KKtjntVuiE8yFstMQ.png)\n\nA gating network has its weight. We apply the softmax function to the result of the dot\\-product between the input word token and the weight of a gating network, then get the probability of how much the expert is relevant to the given token. Based on the probability, we can select top\\-k relevant experts. MoE, which has this type of gating network, is called sparse MoE.\n\nThese are the fundamental knowledge needed to understand how MoE works as an embedding model. For further understanding, I recommend reading [this blog](https://huggingface.co/blog/moe) \\[2]. Now, let’s dive into how MoE actually works as an embedding model.\n\n## 2\\. How MoE works as an embedding model?\n\n### Quick recap about embeddings\n\nBefore diving into the theme of this section, let’s quickly recap about embeddings. Recently, embedding has been the internal representation of input data in deep learning models, and it has semantics and condensed data information. We usually extract the last hidden state of the neural network as an embedding, as shown below.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kSHFTEejKiSI51taKZCO9A.png)\n\nWe typically use encoder\\-based models to extract embeddings because they can capture semantics with bi\\-directional attention compared to decoder\\-only models. Decoder\\-only models often use causal attention to interact with only the previous word tokens; thus, they cannot capture the rich semantics, such as contextualized information, like encoder\\-decoder models.\n\n### How MoE works as an embedding model?\n\nIt was a common belief that the decoder model could not be used for embedding extraction. However, the authors found that the routing weight in the MoE provides complementary information to the decoder embedding. The routing weight in each layer reflects the reasoning choice on the input token, so it contains the input’s semantic information that hidden state’s embedding may lost. In the mathematical formula, we can describe it as:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*n6wGCMqAhjBAfLFV47ML1g.png)\n\n*g* is the softmax function and *H* means the hidden state. We concatenate all the MoE layer’s routing weights to avoid losing model’s reasoning choice.\n\nTo fully utilize the routing weights and decoder embedding, the authors proposed a method called MoE Embedding (MoEE) to form a more comprehensive embedding representation. There are two types of MoEE. One method is a concatenation\\-based combination, described below.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*uVmcV-lM83XL7HoYbYjt7w.png)\n\nThis method is simple, and we just concatenate routing weights and decoder embedding. The authors call this method as MoEE(concat). It can preserve the distinct information captured by each routing weight while allowing downstream tasks to leverage the combined representation.\n\nThe other method is weighted sum integration. It performs a weighted sum of the similarity scores calculated from routing weights and hidden state (HS) embedding, denoted as MoEE (sum). This method is used for tasks that compare two sentences, such as semantic textual similarity.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kyJxWW9zdgRyNr2jmO4LlQ.png)\n\n𝛂 is a hyperparameter to control the contribution of the routing weights. After calculating the similarity score for each pair, we compute the rank correlation, such as Spearman’s rank correlation, between the calculated similarity score and the ground truth similarity.\n\nFor practical usage, I think that the MoEE(concat) is easy to use. Moreover, the authors leverage the PromptEOL technique \\[4] to enhance MoEE. This technique prompts the following template to constrain LLMs in predicting semantic information in the next token.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*S9BASj9JkQe-i4fqmbopWg.png)\n\nNow, here is the performance table across MTEB tasks.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*7LxkEMR2DFlncypF6_T7Vw.png)\n\nMoEE with PromptEOL can work better than supervised and self\\-supervised methods. Note that this leaderboard is not the latest one, so this result is not SOTA. The value of this method is that we can obtain decent results for embedding tasks, and it can be used without any further training.\n\nWe have covered how MoEE works so far. In the next section, we will implement MoEE with BERTopic and cluster sentences.\n\n## 3\\. Practical implementation : Leverage MoEE with BERTopic\n\nIn this section, we extract embeddings from pre\\-trained MoE LLM and leverage them with [BERTopic](https://maartengr.github.io/BERTopic/index.html) using a 20\\-news\\-group dataset \\[5]. For your information, BERTopic is a convenient topic modeling library beyond conventional statistical topic modeling. It leverages embeddings from Transformer to make topic clustering, so I think it is suitable for checking the capability. First of all, let’s prepare an environment.\n\n### Environment setup\n\nI used a conda environment with Python 3\\.10\\. I experimented on Ubuntu 20\\.04 with cuda 12\\.4, 16 GB VRAM. You may need 32 GB RAM for downloading model weights.\n\n```python\nconda create -n moee python=3.10 -y\nconda activate moee\n```\n\nNext, we need to install the libraries below via pip.\n\n```python\npip install transformers torch bitsandbytes bertopic accelerate\n```\n\nMoE models need generally high VRAM because we need to load the entire model to our VRAM in advance. Therefore, we require using bitsandbytes, which is a quantization package, to save VRAM memory.\n\nWe need to clone the official GitHub repository.\n\n```python\ngit clone https://github.com/tianyi-lab/MoE-Embedding.git\n```\n\nAll preparation is done. Now, let’s implement topic clustring with BERTopic using MoEE.\n\n### Leverage MoEE with BERTopic\n\nNow, we will use MoEE as an embedding model for BERTopic and try topic clustering. The original repository allows us to use small MoE models, such as Qwen\\-1\\.5\\-MoE\\-A2\\.7B or OLMoE\\-1B\\-7B. In this blog, I will use OLMoE\\-1B\\-7B, which is affordable for running inference on 16 GB VRAM. Firstly, we need to load OLMoE\\-1B\\-7B.\n\n```python\nkwargs = {\n        \"base_model\": 'allenai/OLMoE-1B-7B-0924',\n        \"normalized\": False,\n        \"torch_dtype\": torch.bfloat16,\n        \"mode\": \"embedding\",\n        \"pooling_method\": \"mean\",\n        \"attn_implementation\": \"sdpa\",\n        \"attn\": \"bbcc\",\n    }\n\nconfig = {\n    'embed_method': 'prompteol',\n    'emb_info': 'MoEE'\n    }\n\nembedding_model = MOEE(model_name_or_path='allenai/OLMoE-1B-7B-0924', **kwargs)\n```\n\nNext, we need to calculate embeddings of 20\\-news\\-group dataset to pass BERTopic. (I will attach full code later.)\n\n```python\nfrom sklearn.datasets import fetch_20newsgroups\n\ndocs = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))['data']\n\ndataset = MyDataset(docs)\ndataloader = DataLoader(dataset=dataset, batch_size=8)\nembeddings = None\n\nfor batch in tqdm(dataloader):\n    with torch.no_grad():    \n        embedding = embedding_model.encode(batch, **config)\n      \n        if embeddings is None:\n            embeddings = embedding[0]\n        else:\n            embeddings = np.vstack((embeddings, embedding[0]))\n  \n    torch.cuda.empty_cache()\n```\n\nTo calculate embeddings in advance, we use torch.utils.data.DataLoader for an iterator, and encode each batched document. Note that we must pass embeddings as np.asarray type to the BERTopic.\n\nWhen you want to use your own MoE models, you must implement to get the routing weights from each MoE layer. For the hidden state embedding, we can utilize the HuggingFace transformer function. We only need to pass the output\\_hidden\\_states\\=True argument when inference.\n\nNow, we can run topic modeling.\n\n```python\n## Step 2 - Reduce dimensionality\numap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')\n\n## Step 3 - Cluster reduced embeddings\nhdbscan_model = HDBSCAN(min_cluster_size=15, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n\n## Step 4 - Tokenize topics\nvectorizer_model = CountVectorizer(stop_words=\"english\")\n\n## Step 5 - Create topic representation\nctfidf_model = ClassTfidfTransformer()\n\n## Step 6 - (Optional) Fine-tune topic representations with \n## a `bertopic.representation` model\nrepresentation_model = KeyBERTInspired()\n\n## All steps together\ntopic_model = BERTopic(\n  embedding_model=embedding_model,          # Step 1 - Extract embeddings\n  umap_model=umap_model,                    # Step 2 - Reduce dimensionality\n  hdbscan_model=hdbscan_model,              # Step 3 - Cluster reduced embeddings\n  vectorizer_model=vectorizer_model,        # Step 4 - Tokenize topics\n  ctfidf_model=ctfidf_model,                # Step 5 - Extract topic words\n  representation_model=representation_model # Step 6 - (Optional) Fine-tune topic representations\n)\n\n## topic modeling using BERTopic model\ntopics, probs = topic_model.fit_transform(docs, embeddings)\n```\n\nWe got 42 topics by the default setting; some samples are shown below. Even though I picked up topics randomly, it can capture the semantics very well.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*VIaKHU-PSuTPzOUKDFbwOw.png)\n\nMoreover, here is the topic cluster visualization.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*KYAUOe2qEAv-ihq2S2dM0A.png)\n\nPlease look at the red circle in the topic cluster visualization. This red circle refers to topic 0, which is related to the computer. Closer topics are also associated with mechanic words, such as graphics, digital, and printers.\n\nThis method shows us that we can get decent embeddings without any training. Although there is still room to improve the quality to be equivalent to the SOTA\\-supervised models, this paper’s finding is a good step for further improvement of the embedding extracting method without training.\n\nHere is my entire code. You need to put this file into the top of the MoE\\-Embedding directory.\n\n## References\n\n\\[1] Ziyue Li, Tianyi Zhou, [YOUR MIXTURE\\-OF\\-EXPERTS LLM IS SECRETLY AN EMBEDDING MODEL FOR FREE](https://arxiv.org/pdf/2410.10814) (2024\\), *Arxiv*\n\n\\[2] Omar S., et.al., [Mixture of Experts Explained](https://huggingface.co/blog/moe) (2023\\), Hugging Face\n\n\\[3] William Fedus, Barret Zoph., et.al., [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/pdf/2101.03961) (2021\\), *Arxiv*\n\n\\[4] Ting Jiang, et.al., [Scaling Sentence Embeddings with Large Language Models](https://arxiv.org/pdf/2307.16645) (2023\\), *Arxiv*\n\n\\[5] [20 News groups](http://qwone.com/~jason/20Newsgroups/)\n\n\n"},{"lang":"en","group":"blog","slug":"blog/visualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-with-ragas-fc2486308557","frontmatter":{"title":"Visualize your RAG Data — Evaluate your  Retrieval-Augmented Generation System with Ragas","meta_title":"Visualize your RAG Data — Evaluate your  Retrieval-Augmented Generation System with Ragas","description":"How to use UMAP dimensionality reduction for Embeddings to show multiple evaluation Questions and their relationships to source documents…","date":"2024-11-04T12:35:56.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*peWTe1A-MqeROT_Jdof_Cw.gif","categories":["Natural Language Processing","Generative AI","Data Science"],"author":"Rifx.Online","tags":["RAG","UMAP","embeddings","evaluation","visualization"],"draft":false,"slug":"blog/visualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-with-ragas-fc2486308557"},"content":"\n\n\n\n\n### How to use UMAP dimensionality reduction for Embeddings to show multiple evaluation Questions and their relationships to source documents with Ragas, OpenAI, Langchain and ChromaDB\n\nRetrieval\\-Augmented Generation (RAG) adds a retrieval step to the workflow of an LLM, enabling it to query relevant data from additional sources like private documents when responding to questions and queries \\[1]. This workflow does not require costly training or fine\\-tuning of LLMs on the additional documents. The documents are split into snippets, which are then indexed, often using a compact ML\\-generated vector representation (embedding). Snippets with similar content will be in proximity to each other in this embedding space.\n\nThe RAG application projects the user\\-provided questions into the embedding space to retrieve relevant document snippets based on their distance to the question. The LLM can use the retrieved information to answer the query and to substantiate its conclusion by presenting the snippets as references.\n\n\n\nThe evaluation of a RAG application is challenging \\[2]. Different approaches exist: on one hand, there are methods where the answer as ground truth must be provided by the developer; on the other hand, the answer (and the question) can also be generated by another LLM. One of the largest open\\-source systems for LLM\\-supported answering is Ragas \\[4](Retrieval\\-Augmented Generation Assessment), which provides\n\n* Methods for generating test data based on the documents and\n* Evaluations based on different metrics for evaluating retrieval and generation steps one\\-by\\-one and end\\-to\\-end.\n\nIn this article, you will learn\n\n* How to briefly build a RAG system for Formula One (see the previous article [Visualize your RAG Data — EDA for Retrieval\\-Augmented Generation](https://readmedium.com/visualize-your-rag-data-eda-for-retrieval-augmented-generation-0701ee98768f) for detailed descriptions)\n* Generate questions and answers\n* Evaluate the RAG system with [Ragas](https://github.com/explodinggradients/ragas)\n* Most importantly how to visualize the results with [Renumics Spotlight](https://github.com/Renumics/spotlight) and interpret the results.\n\nThe code is available at Github\n\n\n## Get your environment ready\n\nStart a notebook and install the required python packages\n\n\n```python\n!pip install langchain langchain-openai chromadb renumics-spotlight\n%env OPENAI_API_KEY=<your-api-key>\n```\nThis tutorial uses the following python packages:\n\n* [**Langchain**](https://github.com/langchain-ai/langchain): A framework to integrate language models and RAG components, making the setup process smoother.\n* [**Renumics\\-Spotlight**](https://github.com/Renumics/spotlight): A visualization tool to interactively explore unstructured ML datasets.\n* [**Ragas**](https://github.com/explodinggradients/ragas): a framework that helps you evaluate your RAG pipelines\n\n*Disclaimer: The author of this article is also one of the developers of Spotlight.*\n\n\n## Prepare documents and embeddings for the dataset\n\nYou can use your own RAG Application, skip to the next part to learn how to evaluate, extract and visualize.\n\nOr you can use the RAG application from the [last article](https://readmedium.com/visualize-your-rag-data-eda-for-retrieval-augmented-generation-0701ee98768f) with [our prepared dataset of all Formula One articles of Wikipedia](https://spotlightpublic.blob.core.windows.net/docs-data/rag_demo/docs.zip). There you can also insert your own Documents into a ‘docs/’ subfolder.\n\n\n> This dataset is based on articles from [Wikipedia](https://www.wikipedia.org/) and is licensed under the Creative Commons Attribution\\-ShareAlike License. The original articles and a list of authors can be found on the respective Wikipedia pages.\n\nNow you can use Langchain’s `DirectoryLoader` to load all files from the docs subdirectory and split the documents in snippets using the `RecursiveCharacterTextSpliter`. With `OpenAIEmbeddings` you can create embeddings and store them in a `ChromaDB` as vector store. For the Chain itself you can use LangChains `ChatOpenAI` and a `ChatPromptTemplate`.\n\nThe [linked code](https://github.com/Renumics/rag-demo/blob/main/notebooks/visualize_rag_tutorial_qs.ipynb) for this article contains all necessary steps and you can find a detailed description of all steps above in [the last article](https://readmedium.com/visualize-your-rag-data-eda-for-retrieval-augmented-generation-0701ee98768f).\n\nOne important point is, that you should use a hash function to create ids for snippets in `ChromaDB`. This allows to find the embeddings in the db if you only have the document with its content and metadata. This makes it possible to skip documents that already exist in the database.\n\n\n```python\nimport hashlib\nimport json\nfrom langchain_core.documents import Document\n\ndef stable_hash_meta(doc: Document) -> str:\n    \"\"\"\n    Stable hash document based on its metadata.\n    \"\"\"\n    return hashlib.sha1(json.dumps(doc.metadata, sort_keys=True).encode()).hexdigest()\n\n...\nsplits = text_splitter.split_documents(docs)\nsplits_ids = [\n    {\"doc\": split, \"id\": stable_hash_meta(split.metadata)} for split in splits\n]\n\nexisting_ids = docs_vectorstore.get()[\"ids\"]\nnew_splits_ids = [split for split in splits_ids if split[\"id\"] not in existing_ids]\n\ndocs_vectorstore.add_documents(\n    documents=[split[\"doc\"] for split in new_splits_ids],\n    ids=[split[\"id\"] for split in new_splits_ids],\n)\ndocs_vectorstore.persist()\n```\n\n## Evaluation Questions\n\nFor a common topic like Formula One, one can also use ChatGPT directly to generate general questions. In this article, four methods of question generation are used:\n\n* **GPT4**: 30 questions were generated using ChatGPT 4 with the following prompt “Write 30 question about Formula one”\n– Random Example: “Which Formula 1 team is known for its prancing horse logo?”\n* **GPT3\\.5:** Another 199 question were generated with ChatGPT 3\\.5 with the following prompt “Write 100 question about Formula one” and repeating “Thanks, write another 100 please”\n– Example: “”Which driver won the inaugural Formula One World Championship in 1950?”\n* **Ragas\\_GPT4**: 113 questions were generated using Ragas. Ragas utilizes the documents again and its own embedding model to construct a vector database, which is then used to generate questions with GPT4\\.\n– Example: “Can you tell me more about the performance of the Jordan 198 Formula One car in the 1998 World Championship?”\n* **Rags\\_GPT3\\.5**: 226 additional questions were generated with Ragas — here we use GPT3\\.5\n– Example: “What incident occurred at the 2014 Belgian Grand Prix that led to Hamilton’s retirement from the race?”\n\n\n```python\nfrom ragas.testset import TestsetGenerator\n\ngenerator = TestsetGenerator.from_default(\n    openai_generator_llm=\"gpt-3.5-turbo-16k\", \n    openai_filter_llm=\"gpt-3.5-turbo-16k\"\n)\n\ntestset_ragas_gpt35 = generator.generate(docs, 100)\n```\nThe questions and answers were not reviewed or modified in any way. All questions are combined in a single dataframe with the columns `id`, `question`, `ground_truth`, `question_by` and `answer`.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*R_74K0-_SJXyTxq6ovAcWg.png)\n\nNext, the questions will be posed to the RAG system. For over 500 questions, this can take some time and incur costs. If you ask the questions row\\-by\\-row, you can pause and continue the process or recover from a crash without losing the results so far:\n\n\n```python\nfor i, row in df_questions_answers.iterrows():\n    if row[\"answer\"] is None or pd.isnull(row[\"answer\"]):\n        response = rag_chain.invoke(row[\"question\"])\n\n        df_questions_answers.loc[df_questions_answers.index[i], \"answer\"] = response[\n            \"answer\"\n        ]\n        df_questions_answers.loc[df_questions_answers.index[i], \"source_documents\"] = [\n            stable_hash_meta(source_document.metadata)\n            for source_document in response[\"source_documents\"]\n        ]\n\n```\nNot only is the answer stored but also the source IDs of the retrieved document snippets, and their text content as context:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*umlKv7Qf9SSLzRslT2r0Qw.png)\n\nAdditionally, the embeddings for all questions are generated and stored in the dataframe as well. This allows for visualizing them alongside the documents.\n\n\n## Evaluation with Ragas\n\n[Ragas](https://github.com/explodinggradients/ragas) provides metrics for evaluating each component of your RAG pipeline in isolation and end\\-to\\-end metrics for overall performance:\n\n1. **Context Precision:** Uses the `question` and retrieved `contexts` to measure the signal\\-to\\-noise ratio.\n2. **Context Relevancy:** Measures the relevance of the retrieved context to the question, calculated using the `question` and `contexts`.\n3. **Context Recall:** Based on the `ground truth` and `contexts` to check if all relevant information for the answer is retrieved.\n4. **Faithfulness:** Utilizes the `contexts` and `answer` to measure how factually accurate the generated answer is.\n5. **Answer Relevance:** Computed using the `question` and `answer` to assess the relevance of the generated answer to the question (does not consider factuality).\n6. **Answer Semantic Similarity:** Evaluated using the `ground truth` and `answer` to assess the semantic resemblance between the generated and the correct answer.\n7. **Answer Correctness:** Relies on the `ground truth` and `answer` to measure the accuracy and alignment of the generated answer with the correct one.\n8. **Aspect Critique:** Involves analyzing the `answer` to evaluate submissions based on predefined or custom aspects such as correctness or harmfulness.\n\nFor now, we focus on the end\\-to\\-end metric of answer correctness. The column names and content in the dataframe are copied and adapted to meet the naming and formatting requirements according to the Ragas API:\n\n\n```python\n## prepare the dataframe for evaluation\ndf_qa_eval = df_questions_answers.copy()\n\n\n## adapt the ground truth to the ragas naming and format\ndf_qa_eval.rename(columns={\"ground_truth\": \"ground_truths\"}, inplace=True)\ndf_qa_eval[\"ground_truths\"] = [\n    [gt] if not isinstance(gt, list) else gt for gt in df_qa_eval[\"ground_truths\"]\n]\n```\nThis again can take some time and even more money than just querying your RAG system. Let’s apply the evaluation row\\-by\\-row to be able to recover from a crash without losing the results so far:\n\n\n```python\n## evaluate the answer correctness if not already done\nfields = [\"question\", \"answer\", \"contexts\", \"ground_truths\"]\nfor i, row in df_qa_eval.iterrows():\n    if row[\"answer_correctness\"] is None or pd.isnull(row[\"answer_correctness\"]):\n        evaluation_result = evaluate(\n            Dataset.from_pandas(df_qa_eval.iloc[i : i + 1][fields]),\n            [answer_correctness],\n        )\n        df_qa_eval.loc[i, \"answer_correctness\"] = evaluation_result[\n            \"answer_correctness\"\n        ]\n\n```\nAfterwards, you can store the results in the `df_questions_answer` dataframe:\n\n\n```python\ndf_questions_answers[\"answer_correctness\"] = df_qa_eval[\"answer_correctness\"]\n```\n\n## Prepare visualization\n\nTo include the document snippets in the visualization, we add references from documents to questions that used the document as a source. Additionally, the count of questions referencing a document is stored:\n\n\n```python\n## Explode 'source_documents' so each document ID is in its own row alongside the question ID\ndf_questions_exploded = df_qa_eval.explode(\"source_documents\")\n\n## Group by exploded 'source_documents' (document IDs) and aggregate\nagg = (\n    df_questions_exploded.groupby(\"source_documents\")\n    .agg(\n        num_questions=(\"id\", \"count\"),  # Count of questions referencing the document\n        question_ids=(\n            \"id\",\n            lambda x: list(x),\n        ),  # List of question IDs referencing the document\n    )\n    .reset_index()\n    .rename(columns={\"source_documents\": \"id\"})\n)\n\n## Merge the aggregated information back into df_documents\ndf_documents_agg = pd.merge(df_docs, agg, on=\"id\", how=\"left\")\n\n## Use apply to replace NaN values with empty lists for 'question_ids'\ndf_documents_agg[\"question_ids\"] = df_documents_agg[\"question_ids\"].apply(\n    lambda x: x if isinstance(x, list) else []\n)\n## Replace NaN values in 'num_questions' with 0\ndf_documents_agg[\"num_questions\"] = df_documents_agg[\"num_questions\"].fillna(0)\n```\nNow concatenate the dataframe of questions with the dataframe of the documents\n\n\n```python\ndf = pd.concat([df_qa_eval, df_documents_agg], axis=0)\n```\nAdditionally, let’s prepare some different UMAP \\[3] mappings. You could do much the same in the Spotlight GUI later, but doing it upfront can save time.\n\n* umap\\_all: UMAP with fit and transform applied on all document and question embeddings\n* umap\\_questions: UMAP with fit applied on questions embeddings only and transform applied on both\n* umap\\_docs: UMAP with fit applied on document embeddings only and transform applied on both\n\nWe prepare each of the UMAP transformations like this:\n\n\n```python\numap = UMAP(n_neighbors=20, min_dist=0.15, metric=\"cosine\", random_state=42).fit\numap_all = umap.transform(df[\"embedding\"].values.tolist())\ndf[\"umap\"] = umap_all.tolist()\n\n```\nAnother interesting metric for each of the document snippets is the distance between its embeddings and the embeddings of the nearest question\n\n\n```python\nquestion_embeddings = np.array(df[df[\"question\"].notna()][\"embedding\"].tolist())\ndf[\"nearest_question_dist\"] = [  # brute force, could be optimized using ChromaDB\n    np.min([np.linalg.norm(np.array(doc_emb) - question_embeddings)])\n    for doc_emb in df[\"embedding\"].values\n]\n```\nThis metric can be helpful to find documents that are not referenced by questions.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*YTRUXZmd0iX8kyPIdUUnlg.png)\n\n\n## Visualize results\n\nIf you skipped the previous steps, you can download the dataframe and load it with:\n\n\n```python\nimport pandas as pd\ndf = pd.read_parquet(\"df_f1_rag_docs_and_questions.parquet\")\n```\nand start [Renumics Spotlight](https://github.com/Renumics/spotlight) to visualize it with:\n\n\n```python\nfrom renumics import spotlight\n\nspotlight.show(df)\nspotlight.show(\n    df,\n    layout=\"/home/markus/Downloads/layout_rag_1.json\",\n    dtype={x: Embedding for x in df.keys() if \"umap\" in x},\n)\n```\nIt will open a new brwoser window:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*IMbva0pP8RAVhoY4dVbjLg.png)\n\nOn the top left side, you can see a **table of all questions and all document** snippets. You can use the “visible columns” button to control which columns of the dataframe are shown in the table. It is useful to create a filter directly that selects only the questions to be able to turn the questions on and off in the visualizations: Select all questions and and then create a filter using the “Create filter from selected row” button.\n\nTo the right of the table, the `answer correctness` **is displayed as a metric** across all questions. Below there are two **histograms**; the left one shows the distribution of `answer correctness` divided into the different methods of question generation. The right one shows the distribution of methods of question generation. Here, it is advisable to create a filter for the questions using the filter button to display only the selected rows (the questions) if needed.\n\nOn the right side, there are **two similarity maps.** The first one uses the `umap_questions` column and shows the questions and documents based on the transformation applied only to the questions. It is helpful for viewing the distribution of questions independently from the associated documents because this approach allows analysts to identify patterns or clusters within the questions themselves.\n\nThe second similarity map shows the questions and documents based on the transformation applied only to the documents (`umap_docs`). It is useful for viewing the questions in the context of their associated documents. A similarity map that simultaneously transforms questions and documents has proven to be less helpful with a larger number of questions, as more or fewer questions get clustered together and tend to be separated from the documents. Therefore, this representation is omitted here.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*1wZrAj60hiw1T3RVnCuBtA.png)\n\n\n### Document Embedding Similarity Map: Observations\n\nIn the similarity map `umap_docs`, you can identify areas in the embedding space of the documents that have no neighboring questions. It is even better recognized when selecting `nearest_question_dist` for coloring.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*cMGNPnnBa9Bn7BJ05SzxBw.png)\n\nSome clusters can be identified, including snippets that contain only headings or tabular data containing only numbers page by page, whose meaning is lost during splitting. Additionally, many Wikipedia\\-specific text additions that contain no relevant information, such as links to other languages or editing notes, form clusters with no neighboring questions.\n\nRemoving the noise in form of Wikipedia\\-related text is very simple when using the Wikipedia API. It is probably not particularly necessary, as it mainly costs some space — it is not expected that the RAG result will be particularly worsened by it. However, data contained in large tables are hardly captured by the RAG system and it could ne benifical to extract these using advanced pre\\-processing methods for Table Extraction and to connect them to the RAG system.\n\nAnother point that you can observe in the `umap_docs` similarity map is how the questions from different sources are distributed.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*IH7z3J4yUmU0C_SruxnDkg.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*K4bADgDmSAr5t4t4r9VImQ.png)\n\nThe questions that were directly generated by ChatGPT (GPT\\-3\\.5, GPT\\-4\\) are located in a more confined area in the center, whereas the questions generated with ragas based on the documents cover a larger area.\n\n\n### Answer correctness histogram\n\nThe histogram can be used as a starting point to get an initial impression of the global statistics of the data. Overall, across all questions, the `answer correctness` is 0\\.45\\. For the questions created without ragas, it is 0\\.36, and for questions with ragas, it is 0\\.52\\. It was expected that the system would perform better for questions generated by ragas, as these questions are based on the available data, whereas the questions directly generated by ChatGPT could come from all the data with which ChatGPT was trained.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GsLBsg7uwTrw-AzvO4BHmw.png)\n\nA quick, random manual review of some of the questions/answers and ground truth shows that in the interval of`answer correctness`0\\.3–0\\.4, most questions were still correctly answered according to the ground truth. In the interval 0\\.2–0\\.3, many incorrect answers are present. In the interval 0\\.1–0\\.2, most answers are incorrect. Notably, almost all questions in this range came from GPT\\-3\\.5\\. The two questions in this interval generated with GPT\\-4 were answered correctly even though they received an `answer correctness` of below 0\\.2\\.\n\n\n### Questions Embedding Similarity Map: Observations\n\nThe Questions Embedding Similarity Map can be helpful to dig deeper into `answer correctness` by examining clusters of similar questions that may cause similar problems.\n\n* **Cluster “Term for driver/process/cars”:** average `answer correctness` 0\\.23: Answers often not precise enough. E.g., Chassis tuning vs. Chassis flexing or brake tuning vs. brake bias adjustment. It is questionable whether these types of questions are suitable for evaluating the system, as it seems very difficult to judge the answers.\n* **Cluster “Terms for fuel strategy:”** average `answer correctness`0\\.44, similar to the global`answer correctness`.\n* **Cluster “Names of tracks”:** average `answer correctness` 0\\.49, similar to the global `answer correctnes`.\n* **Cluster “Who holds the record for…”**: average `answer correctness` 0\\.44, similar to the global `answer correctness`.\n* **Cluster “Win championship with…”**: average `answer correctnes` 0\\.26 — looks challenging. Questions with many conditions, e.g., “Who is the only driver to win the Formula One World Championship with a British racing license, driving for an Italian team with an American engine.” Extended RAG methods like Multi Query might help improve here.\n* **Cluster “Who is the only driver to win… with a car bearing the number \\<number\\>”**: average `answer correctness` 0\\.23 — looks like GPT\\-3\\.5 was lazy here, repeating the same question with different numbers, even though most ground truth entries are wrong!\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Yc03cpSEFlJoZSBPIpMkiQ.png)\n\n\n## Conclusion\n\nIn conclusion, utilizing UMAP\\-based visualizations offers a interesting approach to dig deeper than just analyzing global metrics. The document embedding similarity map gives a good overview, illustrating the clustering of similar documents and their relation to evaluation questions. The question similarity map reveals patterns that allow the differentiation and analysis of questions in conjunction with quality metrics to enable insight generation. Follow the Visualize results section to apply the visualization on your evaluation strategy — what insights will you uncover?\n\n*I am a professional with expertise in creating advanced software solutions for the interactive exploration of unstructured data. I write about unstructured data and use powerful visualization tools to analyze and make informed decisions.*\n\n\n## References\n\n\\[1] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, Haofen Wang: [Retrieval\\-Augmented Generation for Large Language Models: A Survey](https://arxiv.org/abs/2312.10997) (2024\\), arxiv\n\n\\[2] Yixuan Tang, Yi Yang: [MultiHop\\-RAG: Benchmarking Retrieval\\-Augmented Generation for Multi\\-Hop Queries](https://arxiv.org/abs/2401.15391) (2021\\), arXiv\n\n\\[3] Leland McInnes, John Healy, James Melville: [UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction](https://arxiv.org/abs/1802.03426) (2018\\), arXiv\n\n\\[4] Shahul Es, Jithin James, Luis Espinosa\\-Anke, Steven Schockaert: [RAGAS: Automated Evaluation of Retrieval Augmented Generation](https://arxiv.org/abs/2309.15217) (2023\\), arXiv\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/a-month-with-cursor-and-claude-dev-my-thoughts-5c41ae0d4467","frontmatter":{"title":"与 Cursor 和 Claude-Dev 共度的一个月：我的想法","meta_title":"与 Cursor 和 Claude-Dev 共度的一个月：我的想法","description":"我最近一直在使用两个新工具 - Cursor 和 Claude-Dev - 这两个工具都引起了开发人员的广泛关注……","date":"2024-11-04T12:32:52.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*i28vK12LJ6XTpSwrKiamwA.png","categories":["Programming","Technology","Generative AI"],"author":"Rifx.Online","tags":["Cursor","Claude-Dev","Cline","autocomplete","debugging"],"draft":false,"slug":"blog/a-month-with-cursor-and-claude-dev-my-thoughts-5c41ae0d4467"},"content":"\n\n\n我最近在使用两个新工具—— **Cursor** 和 **Claude\\-Dev** ——这两个工具在开发者社区中都引起了相当多的关注。它们都是通过 AI 驱动的助手来提高编码的速度和直观性，但它们采取了不同的方法，并各自有自己的优缺点。在使用这两者大约一个月后，我觉得是时候坐下来反思一下它们的优势和仍需改进的地方。\n\n让我们从 Cursor 开始。\n\n## Cursor: 熟悉但更快速\n\nCursor 是 VSCode 的一个分支，如果你和我一样已经是 VSCode 用户，那么使用起来就非常简单。我不需要从头开始重建我的环境，也不需要处理设置键绑定。所有在 VSCode 中有效的功能在 Cursor 中开箱即用——我的扩展、设置和键映射都毫无问题地转移过来。过渡几乎是无缝的，唯一的一个关键区别是：AI 自动补全速度更快。实际上，**根据我的经验**，它的速度大约是 GitHub Copilot 的 10 倍。\n\n现在，“10 倍更快”并不是我从基准测试中得出的数字——这只是我使用一段时间后的感觉。当你在编写代码时，Cursor 预测你的下一步动作时，并不会让你感觉 AI 在滞后或追赶。相反，它与你同步，这有助于保持你的工作状态。我惊讶于当我不再等待 Copilot 赶上或按三次 tab 仅仅为了得到我想要的建议时，我的生产力提升了多少。\n\nCursor 还有一个很好的功能，它嵌入并索引你的整个项目，使理解文件之间的关系变得更加容易。当你更新一个文件时，索引也会随之更新，这意味着 AI 对你的代码库各个部分如何结合有了更好的理解。如果你在一个大型代码库中工作，多个文件相互依赖，这一点非常有用。\n\n## 缺点\n\n话虽如此，Cursor 中一些最佳功能是需要订阅才能使用的。我通常不反对为增加真实价值的工具付费，但在这种情况下，我有点失望，因为最有趣的 AI 功能——如多文件编辑——属于高级版本。对于一个仍然相对较新的工具，我想知道过早限制这些功能是否会限制其采用，尤其考虑到已经有很多开发者在为 GitHub Copilot 付费。\n\n我在使用 Cursor 时遇到的另一个问题是，虽然它在快速、小型任务方面表现出色，但在处理更复杂的问题时缺乏我所需的一些灵活性。它非常适合快速代码建议和重构，但当我需要能够处理更复杂任务的工具时，比如读取日志或执行构建命令，我发现自己在寻找其他工具。\n\n## Claude\\-Dev: 开源的黑马\n\n这就是**Claude\\-Dev（现在称为Cline）**的用武之地。Claude\\-Dev是一个针对VSCode的开源扩展，虽然它的打磨程度不如Cursor，但它正在迅速发展——在某些方面，它更为强大。Claude\\-Dev最引人注目的地方在于，它似乎不仅仅是提供代码片段的建议。它是一个可以**与您的环境进行更深层次互动**的工具。\n\n例如，Claude\\-Dev可以读取您的终端日志，理解代码检查错误，甚至运行任意的CLI命令。这意味着如果您问它为什么您的项目无法构建，它不仅会提供建议——它实际上会去查看相关文件，弄清楚您正在使用什么类型的项目（Node、React、Python等），并尝试为您构建它。如果出现错误，它会读取日志，尝试诊断问题，甚至在需要时应用修复。\n\n不过，它并不完美。根据我的经验，Claude\\-Dev在编辑时的速度不如Cursor。这其中一个原因是它重写整个文件，而不是仅仅更新需要更改的部分。这会导致速度变慢，如果您为API令牌付费（您需要提供要使用的LLM的API密钥），它消耗这些令牌的速度比应有的要快。我一直在考虑为这个项目贡献代码，修复这个问题，使其通过诸如`sed`的Shell命令仅更新必要的行。\n\n我发现的一个特别有趣的功能是Claude\\-Dev如何使用Puppeteer来可视化测试和更新您的前端。您可以给它一张网站的截图，它会将其与您的应用进行比较，迭代直到您的前端达到您想要的外观。这个过程并不是最快的，但它在处理CSS方面出奇地优秀——对我而言，这通常是一个耗时的环节。\n\n## 它的不足之处\n\nClaude\\-Dev 无疑是一个适合那些乐于尝试仍然有些粗糙工具的人的工具。与感觉更像是已经准备好投入使用的精致产品的 Cursor 不同，Claude\\-Dev 更像是一个正在积极开发的强大工具。它并不总是能在第一次就做到正确，而且速度也比我希望的要慢，但它在不断改进。它是开源的，主要由一个人开发，这使得它的创新速度更加令人印象深刻。\n\n## 那么你应该使用哪个？\n\n如果你正在寻找一个精致、快速的体验，专注于速度和快速建议，**Cursor** 可能是更好的选择。它感觉灵敏，能够与现有的 VSCode 设置集成，并且可以保持你的工作流——直到你遇到付费墙。但是如果你对此没有问题，并且不需要额外的功能，Cursor 是一个很好的工具。\n\n另一方面，如果你想要的不仅仅是自动补全代码的功能——而是能真正帮助调试、构建和迭代你的项目的东西，**Claude-Dev** 更适合你。它更灵活，但也稍微慢一些，边缘处理得不够精细。如果你愿意尝试并且能够忍受一些小缺陷，它提供的功能是 Cursor 目前所不具备的。\n\n对我来说，**Claude-Dev** 胜出，主要是因为它与我的工作流程的深度集成。能够读取日志、运行命令并迭代直到问题解决是无价的，尤其是在我处理不熟悉的代码库时。也就是说，当我需要快速行动而不想等待 AI 处理命令时，我仍然会使用 **Cursor**。\n\n## 最后的思考\n\nCursor 和 Claude-Dev 都提供了独特的优势，我认为我们只是在探索 AI 驱动的编码工具所能做的事情的表面。这里有很大的潜力，尤其是随着这些工具的不断发展。我很期待看到它们的未来，并将继续尝试这两者，看看它们如何融入我的开发工作流程。\n\n同时，我建议你自己尝试一下这两者。每个工具都有其优点，你可能会发现其中一个更适合你的风格，具体取决于你正在处理的内容。\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/a-new-risings-red-star-qwen2-5-is-here-0dffe0fb09ad","frontmatter":{"title":"新崛起红星：Qwen2.5来了","meta_title":"新崛起红星：Qwen2.5来了","description":"一起用python和llama-cpp测试一下阿里云新生的生成式AI Qwen2.5","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*zU-XtqK2oMLkvscgxavjdw.png","categories":["Programming","Technology","Education"],"author":"Rifx.Online","tags":["Qwen2.5","multimodal","instruction-following","text-generation","multilingual"],"draft":false,"slug":"blog/a-new-risings-red-star-qwen2-5-is-here-0dffe0fb09ad"},"content":"\n\n\n### 一起测试新生的阿里云生成式AI Qwen2.5，使用Python和llama-cpp\n\n\n\n在没有太多宣传和预期公告的情况下，阿里云于9月19日发布了他们的旗舰模型系列Qwen2.5。\n\n阿里云在Qwen上的革命性旅程再次展示了通过创新的强大领导力。\n\n怎么做的？它们有什么特别之处？我们应该期待什么？\n\n在本文中，我们将探讨新模型并检查其性能。作为后续，在下一篇文章中，我们将使用`llama-cpp-python`和量化版本的qwen2.5–1.5b-instruct，对模型进行13项NLP任务的测试。\n\n事实上，我相信我们是最佳的基准工具，能够全面评估一个模型是否适合我们！\n\n现在，我们将覆盖以下内容：\n\n\n```python\n- Qwen2.5 family innovation\n- Declared scope, use cases and models\n- Qwen2.5: a party of Foundation models\n- Expanding Reach through Open-Source Contributions\n- Bridging Industries through cutting-edge AI solutions\n- 13 Tasks to prove it worth \n- Future outlook: continued Open-Sourcing\n```\n让我们深入了解！ \n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*OeQ5qeOzCdl8LPJOZZgTIw.png)\n\n## Qwen2.5家族创新\n\nQwen是阿里巴巴集团Qwen团队的大型语言模型和大型多模态模型系列。就在昨天，大型语言模型已升级为Qwen2.5。\n\n语言模型和多模态模型均在大规模多语言和多模态数据上进行预训练，并在高质量数据上进行后训练，以与人类偏好对齐。Qwen能够进行自然语言理解、文本生成、视觉理解、音频理解、工具使用、角色扮演、作为AI代理等。\n\n随着Qwen2.5的发布以及额外开源模型的发布，阿里云继续保持其领导地位，以满足企业用户日益增长的AI需求。自去年六月以来，Qwen家族通过Model Studio在消费电子、汽车、游戏等多个行业吸引了超过90,000个部署。\n\nQwen还通过在Hugging Face等平台上推出新模型，如Qwen1.5–110B和CodeQwen1.5–7B，扩大了其影响力，展示了阿里巴巴对开源AI开发的承诺。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*A4pEOgsLK2PAFtiaGQx1Qw.png)\n\n## 声明的范围、用例和模型\n\n在 Qwen2 发布的过去三个月里，众多开发者在 Qwen2 语言模型基础上构建了新的模型，为整个社区以及阿里云提供了宝贵的反馈。\n\n> 在此期间，我们专注于创建更智能、更知识丰富的语言模型。今天，我们很高兴地介绍 Qwen 家族的最新成员：Qwen2.5。\n\n他们的声明伴随着有关新模型家族的事实：\n\n* 密集型、易于使用的仅解码器语言模型，提供 0.5B、1.5B、3B、7B、14B、32B 和 72B 尺寸，以及基础和指令变体。\n* 在我们最新的大规模数据集上进行预训练，涵盖多达 18T 的标记。\n* 在遵循指令、生成长文本（超过 8K 标记）、理解结构化数据（例如，表格）以及生成结构化输出（尤其是 JSON）方面有显著改进。\n* 对系统提示的多样性更具韧性，增强了角色扮演实现和聊天机器人的条件设置。\n* 支持的上下文长度可达 128K 标记，并且可以生成多达 8K 标记。\n* 支持超过 29 种语言的多语言功能，包括中文、英文、法文、西班牙文、葡萄牙文、德文、意大利文、俄文、日文、韩文、越南文、泰文、阿拉伯文等。\n\n## Qwen2.5：基础模型的聚会\n\n根据2024年9月19日的[官方博客新闻稿](https://qwenlm.github.io/blog/qwen2.5/)的公告：\n\n> 今天，我们很高兴地介绍Qwen家族的最新成员：**Qwen2.5**。我们宣布这可能是历史上最大的开源发布！让我们开始庆祝吧！\n\n> 我们最新的发布包含了LLMs **Qwen2.5**，以及用于编码的专用模型**Qwen2.5-Coder**和用于数学的模型**Qwen2.5-Math**。\n\n为了展示Qwen2.5的能力，阿里云团队对其最大的开源模型**Qwen2.5–72B**——一个72B参数的稠密解码器语言模型——与领先的开源模型如Llama-3.1–70B和Mistral-Large-V2进行了基准测试。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*-MMFgkkWHa307jNo.jpg)\n\n所有开放权重模型都是稠密的解码器语言模型，提供多种尺寸，包括：\n\n* Qwen2.5：0.5B、1.5B、3B、7B、14B、32B和72B\n* Qwen2.5-Coder：1.5B、7B和32B正在发布中\n* Qwen2.5-Math：1.5B、7B和72B。\n\n除了3B和72B变体外，所有这些开源模型均在Apache 2.0许可证下发布。您可以在各自的Hugging Face库中找到许可证文件。\n\n> 除了这些模型外，我们还通过Model Studio提供旗舰语言模型的API：**Qwen-Plus**和**Qwen-Turbo**，我们鼓励您进行探索！\n\n但这还不是全部！\n\n> …我们还开源了**Qwen2-VL-72B**，与上个月的发布相比，具有性能提升。\n\n在**Qwen2.5**方面，所有语言模型均在我们最新的大规模数据集上进行了预训练，涵盖了多达**18万亿**个标记。与Qwen2相比，Qwen2.5获得了显著更多的知识（MMLU：85+），并在编码（HumanEval 85+）和数学（MATH 80+）方面大大提升了能力。此外，新模型在指令跟随、生成长文本（超过8K个标记）、理解结构化数据（例如，表格）和生成结构化输出，尤其是JSON方面取得了显著改善。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*7c7CIbl-WVjazUeE.jpeg)\n\nQwen2.5模型通常对系统提示的多样性更具韧性，增强了角色扮演的实施和聊天机器人的条件设置。\n\n与Qwen2一样，Qwen2.5语言模型支持多达**128K**个标记，并可以生成多达**8K**个标记。它们还支持超过**29**种语言的多语言支持，包括中文、英文、法文、西班牙文、葡萄牙文、德文、意大利文、俄文、日文、韩文、越南文、泰文、阿拉伯文等。\n\n### Qwen-Coder 是家族中的新成员\n\n专业的专家语言模型，即 **Qwen2.5-Coder** 用于编码，**Qwen2.5-Math** 用于数学，相较于它们的前身 CodeQwen1.5 和 Qwen2-Math，进行了实质性的增强。具体来说，Qwen2.5-Coder 已在 **5.5 万亿** 个与代码相关的数据上进行了训练，使得即使是较小的编码专用模型也能够在编码评估基准上与更大的语言模型提供竞争力的表现。同时，Qwen2.5-Math 支持 **中文** 和 **英文**，并结合了多种推理方法，包括思维链（CoT）、思维程序（PoT）和工具集成推理（TIR）。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Nvk4wrcB0SB4Tt-xbCzO6g.png)\n\n## 通过开源贡献扩大影响力\n\n作为持续致力于更广泛社区的一部分，阿里云在发布各种规模和变体的Qwen模型方面迈出了进一步的步伐。这包括：\n\n1. **Qwen 0.5亿参数**，适用于更传统应用的基础版本。2. 一款紧凑但强大的模型，专门为游戏开发量身定制：**Qwen-VL（视觉-语言）**，优化了高性能。\n\n这些进展展示了阿里对开源AI的承诺，不仅分享了Qwen的基础版本，还推出了显著改进和新模型，直接针对企业需求，同时增强其快速创新的能力。\n\n这与一个战略愿景密切相关，即持续贡献惠及社区成员和自身客户，帮助他们在多个行业寻求创新应用。\n\n### 通过前沿的人工智能解决方案连接各行业\n\n为了展示Qwen在现实场景中的广泛能力，阿里云一直处于前沿：\n\n1. **小米**：该公司正在将阿里的模型集成到他们的AI助手小爱中，并在小米智能手机和电动汽车中部署，以通过语音命令生成车载娱乐图像等增强功能。\n\n2. **完美世界游戏**：Qwen在游戏开发中的集成导致了创新应用，包括通过对话动态改善情节解析和实时内容管理。\n\n阿里云模型与各行业之间的合作不仅丰富了用户体验，还促进了这些行业内更大的增长机会，推动了没有人工智能进步的情况下无法想象的边界。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*ku8o3rq6PHDE8xcc.png)\n\n## 13 个证明其价值的任务\n\n1.5 亿参数的模型可能是考虑到复杂性、提示理解和推理速度的最佳变体。\n\n我将向您展示我仅使用 `llama-cpp-python` 和一个简单终端界面进行的内部测试。\n\n为此，我创建了一个提示列表，涵盖了一系列通常使用的任务，您可以在每次生成后分配一个投票（从 0 到 5）。这是一个个人的人工基准测试。\n\n### 需求\n\n创建一个 `venv`（需要 Python 3.11+）：我在运行 Windows 11 的迷你电脑上进行了测试。\n\n```python\n## create the virtual environment\npython -m venv venv\n## activate the venv\nvenv\\Scripts\\activate\n## Install the dependencies \npip install llama-cpp-python==0.2.90 tiktoken\n```\n我们需要从[官方 Qwen2.5 Hugging Face 仓库](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF)下载 GGUF 文件。我使用的是 qwen2.5–1.5b-instruct-q5\\_k\\_m.gguf 版本。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Fa-qFsx9RTFGZmM-vxCEPQ.png)\n\n在主项目目录中下载文件。我们就准备好了。\n\n这里用于分析的代码在我的 GitHub 仓库中：\n\n我将在下一篇文章中解释整个代码和结果。保持关注！\n\n## 未来展望：持续的开源\n\n在未来的计划中，阿里巴巴还表达了他们对持续开源贡献的承诺，通过为不同领域的开发者发布更小的 Qwen 变体。实际上，在 Hugging Face 社区中，许多用户已开始针对特定任务对 Qwen 进行微调：我在我的 NuExtract 文章中写了一个例子：这个模型系列的较小变体基于 Qwen2–0.5b！\n\n这些人工智能技术和模型进展的发展是充分利用大型语言模型如 **Qwen** 在各个行业中潜力的关键步骤。随着 Model Studio 中强劲的采用率持续快速增长，显然阿里云不仅通过提供先进的工具而且通过促进企业之间的创新，成为了行业的先锋领导者。\n\n在我这边，我的展望是继续对新模型进行内部测试，特别是对小型模型，最高到 3B。\n\n在下一篇文章中，我将与您分享我的方法，如何运行模型以及用于十三个 NLP 任务的提示模板。\n\n希望您喜欢这篇文章。如果这个故事对您有价值，并且您想稍微表示支持，您可以：\n\n1. 为这个故事多次点赞\n2. 突出更值得记住的部分（这将使您更容易找到它们，也让我写出更好的文章）\n3. **加入我的[完全免费的每周 Substack 通讯](https://thepoorgpuguy.substack.com/about)**\n4. 注册 Medium 会员（$5/月可阅读无限 Medium 故事）\n5. 在 Medium 上关注我\n6. 阅读我的最新文章 <https://medium.com/@fabio.matricardi>\n\n这里还有几篇文章来满足您的好奇心：\n\n本文中引用的资源：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Du7V61mEX_yIrfmF.png)\n\n此故事发布在 [Generative AI](https://generativeai.pub/)。与我们在 [LinkedIn](https://www.linkedin.com/company/generative-ai-publication) 上联系，并关注 [Zeniteq](https://www.zeniteq.com/)，以便及时了解最新的人工智能故事。\n\n订阅我们的 [通讯](https://www.generativeaipub.com/) 和 [YouTube](https://www.youtube.com/@generativeaipub) 频道，及时获取生成式 AI 的最新消息和更新。让我们共同塑造 AI 的未来！\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*pvLAT3it1FkdhVU0.png)\n\n"},{"lang":"zh","group":"blog","slug":"blog/a-practical-guide-for-using-autogen-in-software-applications-8799185d27ee","frontmatter":{"title":"在软件应用程序中使用 AutoGen 的实用指南","meta_title":"在软件应用程序中使用 AutoGen 的实用指南","description":"更新：虽然这篇文章是 4 个月前写的，但 AutoGen 已经发生了很大变化。对于可能存在的一些问题，我深表歉意……","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*yrraWH6aGNnbx8p-wfQ1OQ.jpeg","categories":["Programming","Chatbots","Autonomous Systems"],"author":"Rifx.Online","tags":["AutoGen","multi-agent","LLMs","customization","collaboration"],"draft":false,"slug":"blog/a-practical-guide-for-using-autogen-in-software-applications-8799185d27ee"},"content":"\n\n\n\n\n*更新：虽然这篇文章是在四个月前写的，但 AutoGen 自那时以来变化很大。对于我代码示例中可能过时的内容，我深感歉意。*\n\n如果您想了解 AutoGen，可以查看 [文档](https://microsoft.github.io/autogen/)、[Colab 笔记本](https://microsoft.github.io/autogen/docs/Examples) 和 [博客](https://microsoft.github.io/autogen/blog)。非常感谢 AutoGen 团队制作了一个令人惊叹的产品，但老实说——在阅读了他们的所有内容后，我仍然不知道如何在终端或 Jupyter Notebook 之外使用 AutoGen。\n\n本文试图通过提供一些有用的方法来帮助填补这个空白，使 AutoGen 在软件应用中发挥作用。以下是我将要讨论的主题：\n\n1. 代理不仅限于通过终端进行通信\n2. 注册自定义回复\n3. 如何以真实的方式将真实人类纳入对话\n4. 您可以（并且应该）自定义谁来发言\n5. 您不必使用 OpenAI\n6. 可以使用函数而不是执行代码\n7. 将代理用于组织，而不仅仅是对话\n\n最后，我将讨论为什么我认为您应该首先使用 AutoGen。让我们开始吧！\n\n## 代理不仅限于通过终端进行通信\n\n您会看到每个人都使用终端或 Jupyter Notebook 演示 AutoGen。这对于演示来说不错，但这些代理之间还有其他交流方式。\n\nAutoGen 有 2 个基本类：[`UserProxyAg`ent](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/user_proxy_agent.py) 和 [`AssistantAg`ent](https://github.com/microsoft/autogen/blob/main/autogen/agentchat/assistant_agent.py)。它们继承了 [`ConversableAg`ent](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py) 类，仅为基类提供了几个不同的默认参数。\n\n当您看到这个经典代码示例时：\n\n```python\nassistant = autogen.AssistantAgent(\n    name=\"assistant\",\n    llm_config=llm_config\n)\nuser_proxy = autogen.UserProxyAgent(name=\"user_proxy\")\nawait user_proxy.a_initiate_chat(\n    assistant,\n    message=\"\"\"What date is today? Compare the year-to-date gain for META and TESLA.\"\"\",\n)\n```\n发生的事情是 `UserProxyAgent` 将调用其自己的 `send` 方法，这将调用 `AssistantAgent` 的 [`rece`ive](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L514) 方法，传递原始消息。将生成回复（稍后会详细说明），然后 `AssistantAgent` 将调用其 [`s`end](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L351) 方法，这将调用 `UserProxyAgent` 的 `receive` 方法，依此类推，直到 `UserProxyAgent` 确定对话已终止（可以通过 `is_termination_msg` 参数自定义）。\n\n我第一次“恍然大悟”的时刻是当我意识到这些代理是类时，我可以创建自己的自定义代理类，继承 AutoGen 的 UserProxy/Assistant/Conversable Agent 类，并重写任何默认方法。这使得 AutoGen 非常可扩展。\n\n我有一个用例，需要一个可以通过网站上的聊天 UI 输入消息的人（由 `UserProxyAgent` 代理），我希望 `AssistantAgent` 能在 UI 中回复该聊天，并能够接收更多来自人类用户的消息，就好像人类只是这个 AutoGen 对话中的另一个代理。\n\n我可以重写 `send` 和 `receive` 方法（或 `a_send` 和 `a_receive`），并通过 http、websockets 等进行推送/拉取。我尝试了这个，它开始工作，但无法扩展。让我们学习一种更好的方法。\n\n## 注册自定义回复\n\nAutoGen 具有一个插件系统，可以让您自定义代理生成回复的方式。我们习惯看到的示例是 AutoGen 查询 OpenAI 获取答案，并将其作为回复，但您也可以插入自己的方法：\n\n```python\nclass WeatherAgent(AssistantAgent):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, llm_config=False, **kwargs)\n        self.register_reply(Agent, WeatherAgent.get_weather)\n\n    async def get_weather(\n        self,\n        messages: List[Dict] = [],\n        sender=None,\n        config=None,\n    ) -> Tuple[bool, Union[str, Dict, None]]:\n        last_message = messages[-1][\"content\"]\n        result = await fetch_weather(last_message)\n        return True, result\n\nasync def fetch_weather(city: str) -> str:\n    async with httpx.AsyncClient() as client:\n        result = await client.post(\n            WEATHER_API_URL,\n            json={\"city\": question},\n        )\n        return result.json()\n\nweather_assistant = WeatherAgent(name=\"weather_assistant\")\nuser_proxy = autogen.UserProxyAgent(name=\"user_proxy\")\nawait user_proxy.a_initiate_chat(assistant, message=\"Lehi\")\nprint(weather_assistant.last_message)\n```\n在这里，`register_reply` 将插入我的自定义方法以获取回复，默认情况下，该方法将放在 `position=0`，这意味着它将是尝试的第一个回复方法。该方法应返回一个元组，其中第一个项目是一个布尔值，指示此回复是否为应使用的回复，或者是否应尝试下一个注册的回复（例如，使用 OpenAI 的内置回复生成 — 完整顺序请参见 [此处](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L145-L153)）。\n\n了解 [`register_reply`](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L155) 使您能够自定义回复的检索方式，允许您启动子多代理对话等。\n\n## 如何以真实的方式将真实人类纳入对话\n\n这里有一种方法：\n\n```python\n## user makes a POST /query { \"message\": \"What's the weather?\" }\n\n@query_blueprint.route(\"/query\", methods=[\"POST\"])\nasync def post_query():\n  message = request.form.get(\"message\")\n\n  assistant = autogen.AssistantAgent(\n    name=\"assistant\",\n    llm_config=llm_config\n    system_message=\"\"\"You're a helpful assistant.\n    If you need more info, ask the user for anything missing.\"\"\"\n  )\n  user_proxy = autogen.UserProxyAgent(\n    name=\"user_proxy\",\n    human_input_mode=\"NEVER\",\n    code_execution_config=False,\n    is_termination_msg=lambda message: True # Always True\n  )\n  weather_assistant = WeatherAgent(\n    name=\"weather_assistant\",\n    system_message=\"\"\"You're a helpful assistant to get the weather.\n    You fetch weather information, then return it.\"\"\"\n  )\n\n  groupchat = autogen.GroupChat(\n    agents=[assistant, user_proxy, weather_assistant],\n    messages=[]\n  )\n  manager = autogen.GroupChatManager(\n    name=\"Manager\",\n    groupchat=groupchat,\n    llm_config=llm_config,\n  )\n\n  await user_proxy.a_initiate_chat(manager, message=message)\n\n  return groupchat.messages[-1]\n```\n这里发生了什么？\n\n1. 每当一条消息发送到 `user_proxy` 时，对话将结束（我们稍后会恢复它）。这样做的原因是什么？这意味着 `user_proxy` 实际上可以代理用户。它不会尝试回答，而是会结束当前的对话流程，允许真实的人类用户响应（通过恢复对话 — 见下文）。\n2. 如果助理需要更多信息，它会询问 user_proxy，这将结束当前对话。\n\n在上述代码中，可能会发生以下情况：\n\n1. user_proxy -> manager: “天气怎么样？”\n2. assistant -> manager: “用户没有指定哪个城市。”\n3. manager -> user_proxy : 对话将结束\n\n现在，如果用户想要回应并恢复对话，我们该如何做到呢？有很多方法可以做到这一点，这里只是一个示例：\n\n```python\n## user makes a POST /query { \"message\": \"What's the weather?\" }\n## above posts returns a `history` array\n## user makes a second POST /query { \"message\": \"What's the weather?\", \"history\": history }\n\nclass ResumableGroupChatManager(GroupChatManager):\n    groupchat: GroupChat\n\n    def __init__(self, groupchat, history, **kwargs):\n        self.groupchat = groupchat\n        if history:\n            self.groupchat.messages = history\n\n        super().__init__(groupchat, **kwargs)\n\n        if history:\n            self.restore_from_history(history)\n\n    def restore_from_history(self, history) -> None:\n        for message in history:\n            # broadcast the message to all agents except the speaker.  This idea is the same way GroupChat is implemented in AutoGen for new messages, this method simply allows us to replay old messages first.\n            for agent in self.groupchat.agents:\n                if agent != self:\n                    self.send(message, agent, request_reply=False, silent=True)\n\n@query_blueprint.route(\"/query\", methods=[\"POST\"])\nasync def post_query():\n  message = request.form.get(\"message\")\n\n  assistant = autogen.AssistantAgent(\n    name=\"assistant\",\n    llm_config=llm_config\n    system_message=\"\"\"You're a helpful assistant.\n    If you need more info, ask the user for anything missing.\"\"\"\n  )\n  user_proxy = autogen.UserProxyAgent(\n    name=\"user_proxy\",\n    human_input_mode=\"NEVER\",\n    code_execution_config=False,\n    is_termination_msg=lambda message: True # Always True\n  )\n  weather_assistant = WeatherAgent(\n    name=\"weather_assistant\",\n    system_message=\"\"\"You're a helpful assistant to get the weather.\n    You fetch weather information, then return it.\"\"\"\n  )\n\n  groupchat = autogen.GroupChat(\n    agents=[assistant, user_proxy, weather_assistant],\n    messages=[]\n  )\n  manager = ResumableGroupChatManager(\n    name=\"Manager\",\n    groupchat=groupchat,\n    llm_config=llm_config,\n  )\n\n  await user_proxy.a_initiate_chat(manager, message=message)\n\n  return {\n    \"response\": groupchat.messages[-1],\n    \"history\": groupchat.messages,\n  }\n```\n通过这种方法，您可以将人类纳入对话，就像他们是群聊中的另一个代理一样。每当助理代理需要人类输入时，它们会询问 user_proxy，user_proxy 然后结束当前对话，允许人类用户用更多信息进行响应，然后恢复到之前的对话。\n\n这种方法的好处是：\n\n* 对话可以通过您想要的任何方式包含真实人类输入（例如通过 http 或 websocket）。\n* 在获取人类输入时，对话被暂停。这为其他对话和计算释放了线程。\n* 您可以在会话之间持久化这些对话。\n\n## 你可以（并且应该）自定义谁接下来发言\n\n这是主观的，但我认为你应该始终自定义发言者的选择方式，因为：\n\n1. 你将使用更少的令牌（节省金钱和响应时间）\n2. 你可以将决定谁发言的逻辑与定义每个代理系统指令的逻辑分开\n\n\n```python\nshort_role_descriptions = {\n  \"user_proxy\": \"A proxy for the user\",\n  \"weather_assistant\": \"You can get the weather\",\n  \"planner\": \"You help coordinate the plan. Your turn happens when XYZ, but skip your turn when ABC\"\n}\n\nclass CustomGroupChat(GroupChat):\n    # The default message uses the full system message, which is a long string.  We are overriding this to use a shorter message.\n    def select_speaker_msg(self, agents: List[Agent]):\n        message = f\"\"\"You are in a role play game. The following roles are available:\n        ---\n        {new_line.join([f\"{agent.name}: {short_role_descriptions[agent.name]}\" for agent in agents])}\n        ---\n\n        The role who plays next depends on the conversation.  User_Proxy will star the conversation, and typically Planner would go next.\n\n        Here are some examples\n        ---\n        ... not shown here ...\n        ---\n\n        Read the following conversation.\n        Then select the next role from {', '.join([agent.name for agent in agents])} to play. Only return the role.\"\"\"\n        return message\n```\n\n## 你不必使用 OpenAI\n\nAutoGen 已经指出，你可以使用其他 LLM，只要它们是“类似 ChatGPT”的，这意味着它们的 API 响应与 ChatGPT API 调用的形状和响应相似。\n\n但是，请记住这些代理是类，并且你可以重写大多数方法？\n\n尝试重写方法: [generate\\_oai\\_reply](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L678)，你可以查询任何你想要的 LLM。\n\n## 函数可以用来代替执行代码\n\n当我去找我们的安全团队并说：“我想在Kubernetes中为我的服务使用AutoGen。它需要能够执行任何由LLM生成的任意代码。你们对此没问题吧？”\n\n当然，答案是明确的：不可以。\n\n那么，为什么在没有自动代码执行能力的情况下使用AutoGen？\n\n除了下面提到的原因之外，还有一个原因是你可以使用函数调用来完全控制代码执行。如果你有一组想要提供给AutoGen的python函数——这些函数是你编写的、你控制的，并且可以接受一些安全参数——这听起来总比在你的私有基础设施中允许任何代码被执行要好得多。\n\n## 使用代理进行组织，而不仅仅是进行对话\n\n也许你并不需要一个自主的多代理对话。也许你只需要对LLM进行几次不同的调用。\n\n我仍然喜欢仅仅出于组织目的而拥有不同“代理”的想法。这是一个非常疯狂的想法，但请根据自己的情况来看待它：\n\n```python\nanalyst = autogen.AssistantAgent(\n    name=\"Analyst\",\n    system_message=\"\"\"Your an analyst.  You do XYZ.\"\"\",\n    llm_config=llm_config,\n)\n\nsummarizer = autogen.AssistantAgent(\n    name=\"Summarizer\",\n    system_message=\"\"\"Your a summarizer.  You do XYZ.\"\"\",\n    llm_config=llm_config,\n)\n\nreport = \"\"\"Some long report\"\"\"\n\nanalysis = analyst.generate_oai_reply(report)[1]\nsummary = summarizer.generate_oai_reply(report)[1]\n\nprint(f\"Analysis: {analysis}\")\nprint(f\"Summary: {summary}\")\n```\n\n## 为什么使用 AutoGen？\n\n1. AutoGen 允许多个代理，具有不同的系统提示和指令，共同解决问题。就像在现实生活中，不同的视角共同合作会比单一思维更好地解决问题。\n2. AutoGen GroupChat 非常出色。它提供了通向正确专家（代理）的路线，并允许对话在问题解决之前自主持续进行。有些对话将从代理 a->b->c->d 进行，而其他的将是 b->a->d->c。这使得 AutoGen 能够在不需要为每种场景制定明确规则的情况下解决各种不同的问题。\n3. AutoGen 能够从错误中恢复。例如，我创建了一个基于 AutoGen 的服务，该服务向一个 API 发出请求。有时，API 请求因为未能正确发送数据而出错。AutoGen GroupChat 不断尝试不同的方法，直到成功。有时，这需要 4 次以上的尝试，但我的 Planner 代理没有放弃——只是自主调整以处理 API 失败并尝试新方法。\n4. AutoGen 从一开始就提出了将 `UserProxyAgent` 与 `AssistantAgent` 分离的概念。这也使我们能够让用户代理真正为用户代理，如上所示。\n5. AutoGen 是一个维护良好的库。每周他们都会添加一些新功能。\n6. AutoGen 非常可扩展。通过他们构建类的方式，您可以根据自己的喜好自定义任何内容。\n7. AutoGen 还有其他我不使用的功能，但其他人可能会觉得它们有帮助，例如帮助您计算对话的令牌和成本、缓存等。\n\n"},{"lang":"zh","group":"blog","slug":"blog/alibabas-open-source-qwen-how-it-s-revolutionizing-ai-and-how-you-can-use-it-dcba8f687c97","frontmatter":{"title":"阿里巴巴开源 Qwen：它如何彻底改变人工智能以及如何使用它","meta_title":"阿里巴巴开源 Qwen：它如何彻底改变人工智能以及如何使用它","description":"阿里巴巴最近在 2024 年云栖大会期间开源了 Qwen 2.5 模型，在 AI 领域掀起了波澜。超过 100 个…","date":"2024-10-26T00:26:25.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*I7QDwbLMzoJ_ORq5.jpg","categories":["Programming","Machine Learning","Natural Language Processing"],"author":"Rifx.Online","tags":["Qwen","multimodal","open-source","fine-tune","text-to-video"],"draft":false,"slug":"blog/alibabas-open-source-qwen-how-it-s-revolutionizing-ai-and-how-you-can-use-it-dcba8f687c97"},"content":"\n阿里巴巴最近在人工智能领域引起了轰动，在2024年 Apsara 大会上开源了其 **Qwen 2.5** 模型。Qwen 拥有超过 100 个模型，涵盖语言、视觉、音频和代码等多种模态，使其成为最全面的开源人工智能解决方案之一。此次发布通过提供多样化应用的工具，赋能开发者，从文本到视频生成到实时问答。\n\n\n\n## 阿里巴巴 Qwen 模型的关键特性\n\n1. **多模态能力**：Qwen 模型处理多种输入，包括文本、音频和视觉数据。这种多模态方法使其适用于广泛的行业，从媒体和娱乐到机器人技术。\n2. **开源**：Qwen 可在 **Hugging Face** 和 **ModelScope** 等平台上获取，已经被下载超过 4000 万次，基于其基础构建的自定义模型超过 50,000 个。\n3. **增强性能**：Qwen2.5 引入了改进的语言理解、数学和编码能力，与该领域的领先模型竞争。通过针对结构化数据理解和长文本生成等任务的优化性能，Qwen 为高级 AI 应用打开了大门。\n\n## 如何使用阿里巴巴的 Qwen\n\n开发者和组织可以在 Hugging Face 等平台上访问 Qwen 模型，具体可以：\n\n* **微调模型**：为特定行业应用量身定制 Qwen，例如客户服务、自动化或视频内容创作。\n* **与应用集成**：Qwen 的文本转视频模型可以集成到媒体制作流程中，从静态图像和文本提示生成动态内容。\n* **开发 AI 助手**：借助增强的视觉语言模型，Qwen 可用于机器人和自动驾驶汽车，以处理视频数据并执行实时任务，如导航或物体识别。\n\n**通过 Hugging Face 使用 Qwen 的示例**：\n\n```python\nfrom transformers import QwenTokenizer, QwenModel\n\ntokenizer = QwenTokenizer.from_pretrained(\"qwen-2.5\")\nmodel = QwenModel.from_pretrained(\"qwen-2.5\")\n\ninput_text = \"What is the future of AI in healthcare?\"\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\noutputs = model(input_ids)\n```\n\n这使用户能够访问 Qwen 模型，运行推理，并根据特定需求进行定制。\n\n## Qwen在各行业的影响\n\n1. **媒体与娱乐**：凭借新的文本到视频功能，Qwen可以自动从书面脚本生成视频，通过自动化繁琐的制作任务来改变创意产业。\n2. **机器人技术与自动驾驶车辆**：Qwen中增强的视觉语言模型可以帮助机器人理解现实世界环境，从而在自动驾驶或制造中做出更好的决策。\n3. **软件开发**：由Qwen驱动的阿里巴巴AI开发工具自动化了代码生成、调试和需求分析等任务，使开发人员能够专注于更高层次的问题解决。\n\n## 结论：开放AI创新的新纪元\n\n通过开源其Qwen 2.5模型，阿里巴巴正在使先进的AI技术变得更加普及。开发者、初创企业和大型企业都可以利用Qwen的多模态和实时能力，在从媒体到自动驾驶汽车等行业推动创新。无论您是希望为特定应用微调模型的开发者，还是将AI集成到基础设施中的企业，Qwen都提供强大的工具来加速进步。\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/building-a-local-ai-powered-news-aggregator-with-ollama-swarm-and-duckduckgo-95aaf8b3ee41","frontmatter":{"title":"使用 Ollama、Swarm 和 DuckDuckGo 构建本地 AI 新闻聚合器","meta_title":"使用 Ollama、Swarm 和 DuckDuckGo 构建本地 AI 新闻聚合器","description":"没有提供字幕","date":"2024-10-24T17:47:43.000Z","image":"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*OHMOTk_WYGOxWHBsKqdpNQ.jpeg","categories":["Programming","Generative AI","Technology/Web"],"author":"Rifx.Online","tags":["Llama","Swarm","DuckDuckGo","News","Aggregator"],"draft":false,"slug":"blog/building-a-local-ai-powered-news-aggregator-with-ollama-swarm-and-duckduckgo-95aaf8b3ee41"},"content":"\n# 使用OllamaSwarm和DuckDuckGo构建本地AI驱动的新闻聚合器\n\n\n\n在当今快节奏的世界中，跟上特定领域最新新闻的步伐可能会很具挑战性。如果我们能够利用生成式AI和代理的力量，创建一个完全在本地机器上运行的个性化新闻聚合器呢？在本文中，我们将探讨如何使用**Ollama**的Llama 3.2模型、**Swarm**进行代理编排，以及**DuckDuckGo**进行网络搜索来构建这样的系统。\n\n# 本地AI的力量\n\n随着大型语言模型的兴起，我们现在能够在个人电脑上运行复杂的AI系统。这为创建针对我们特定需求定制的工具开辟了无限可能。我们的新闻聚合器就是这一潜力的完美例证。\n\n# 我们系统的组成部分\n\n1. **Ollama with Llama 3.2**: 这是我们系统的核心，为我们的AI代理提供动力。\n2. **Swarm**: 一个代理编排框架，允许我们创建和管理多个AI代理。\n3. **DuckDuckGo Search**: 提供最新的网页搜索结果，而不跟踪用户数据。\n\n# 工作原理\n\n我们的新闻聚合器由两个主要的AI代理组成：\n\n1. **新闻助手**：使用DuckDuckGo搜索获取特定主题的最新新闻文章。\n2. **编辑助手**：审查并精炼收集到的新闻以供最终展示。\n\n让我们来分解一下工作流程：\n\n# 1. 设置环境\n\n\n```python\nollama pull llama3.2\n\nexport OPENAI_MODEL_NAME=llama3.2\nexport OPENAI_BASE_URL=http://localhost:11434/v1\nexport OPENAI_API_KEY=any\n\npip install git+https://github.com/openai/swarm.git duckduckgo-search\n```\n我们首先导入必要的库并初始化我们的 Swarm 客户端：\n\n\n```python\nfrom duckduckgo_search import DDGS\nfrom swarm import Swarm, Agent\nfrom datetime import datetime\n\ncurrent_date = datetime.now().strftime(\"%Y-%m\")\nclient = Swarm()\n```\n\n# 2. 创建新闻搜索功能\n\n我们定义一个函数来使用 DuckDuckGo 搜索新闻：\n\n```python\npythondef get_news_articles(topic):\n  ddg_api = DDGS()\n  results = ddg_api.text(f\"{topic} {current_date}\", max_results=5)\n  if results:\n      news_results = \"\\n\\n\".join([f\"Title: {result['title']}\\nURL: {result['href']}\\nDescription: {result['body']}\" for result in results])\n      return news_results\n  else:\n      return f\"Could not find news results for {topic}.\"\n```\n\n# 3. 定义我们的 AI 代理\n\n我们使用 Ollama 的 Llama 3.2 模型创建两个代理：\n\n\n```python\nnews_agent = Agent(\n  model=\"llama3.2\",\n  name=\"News Assistant\",\n  instructions=\"You provide the latest news articles for a given topic using DuckDuckGo search.\",\n  functions=[get_news_articles],\n)\n\neditor_agent = Agent(\n  model=\"llama3.2\",\n  name=\"Editor Assistant\",\n  instructions=\"You review and finalise the news article for publishing.\",\n)\n```\n\n# 4. 协调工作流程\n\n我们定义一个函数来运行我们的新闻聚合工作流程：\n\n```python\ndef run_news_workflow(topic):\n  # Fetch news\n  news_response = client.run(\n      agent=news_agent,\n      messages=[{\"role\": \"user\", \"content\": f\"Get me the news about {topic} on {current_date}\"}],\n  )\n  raw_news = news_response.messages[-1][\"content\"]\n  \n  # Pass news to editor for final review\n  edited_news_response = client.run(\n      agent=editor_agent,\n      messages=[{\"role\": \"system\", \"content\": raw_news}],\n  )\n  print(f\"{edited_news_response.messages[-1]['content']}\")\n```\n\n# 5. 运行系统\n\n最后，我们可以针对任何感兴趣的话题运行我们的新闻聚合器：\n\n\n```python\nrun_news_workflow(\"AI in Drug Discovery\")\n```\n\n# 完整代码 : app.py\n\n\n```python\nfrom duckduckgo_search import DDGS\nfrom swarm import Swarm, Agent\nfrom datetime import datetime\n\ncurrent_date = datetime.now().strftime(\"%Y-%m\")\n\n# 初始化 Swarm 客户端\nclient = Swarm()\n\n# 1. 创建互联网搜索工具\n\ndef get_news_articles(topic):\n    print(f\"正在为 {topic} 进行 DuckDuckGo 新闻搜索...\")\n    \n    # DuckDuckGo 搜索\n    ddg_api = DDGS()\n    results = ddg_api.text(f\"{topic} {current_date}\", max_results=5)\n    if results:\n        news_results = \"\\n\\n\".join([f\"标题: {result['title']}\\n网址: {result['href']}\\n描述: {result['body']}\" for result in results])\n        return news_results\n    else:\n        return f\"未能找到关于 {topic} 的新闻结果。\"\n    \n# 2. 创建 AI 代理\n\ndef transfer_to_editor_assistant(raw_news):\n    print(\"将文章传递给编辑助手...\")\n    return editor_agent.run({\"role\": \"system\", \"content\": raw_news})\n\n# 新闻代理以获取新闻\nnews_agent = Agent(\n    model=\"llama3.2\",\n    name=\"新闻助手\",\n    instructions=\"您提供有关给定主题的最新新闻文章，使用 DuckDuckGo 搜索。\",\n    functions=[get_news_articles],\n)\n\n# 编辑代理以编辑新闻\neditor_agent = Agent(\n    model=\"llama3.2\",\n    name=\"编辑助手\",\n    instructions=\"您审阅并最终确定新闻文章以供发布。\",\n)\n\n# 3. 创建工作流程\n\ndef run_news_workflow(topic):\n    print(\"运行新闻代理工作流程...\")\n    \n    # 第一步: 获取新闻\n    news_response = client.run(\n        agent=news_agent,\n        messages=[{\"role\": \"user\", \"content\": f\"获取关于 {topic} 在 {current_date} 的新闻\"}],\n    )\n    raw_news = news_response.messages[-1][\"content\"]\n    print(f\"获取的新闻: {raw_news}\")\n    \n    # 第二步: 将新闻传递给编辑进行最终审查\n    edited_news_response = client.run(\n        agent=editor_agent,\n        messages=[{\"role\": \"system\", \"content\": raw_news}],\n    )\n    print(f\"{edited_news_response.messages[-1]['content']}\")\n\n\n# 运行给定主题的新闻工作流程示例\nrun_news_workflow(\"药物发现中的 AI\")\n```\n\n# 示例输出\n\n\n```python\nRunning news Agent workflow...\nRunning DuckDuckGo news search for AI in Drug Discovery...\nFetched news: Here's a formatted answer based on the news articles:\n\n**药物发现中的人工智能：革命性的转变**\n\n人工智能（AI）在药物发现中的作用标志着制药领域的革命性转变。AI利用复杂的算法进行自主决策，从数据分析中增强人类能力，而不是取代它们。\n\n**挑战与局限性**\n\n尽管有着令人期待的进展，但在该领域中仍然存在挑战和局限性。论文《AI在药物发现中的作用》探讨了这些问题，强调了高质量数据的必要性、伦理问题的解决以及对基于AI的方法局限性的认识。\n\n**AI在药物发现中的应用**\n\nAI有潜力在药物发现、设计和研究药物间相互作用中发挥关键作用。AI在药物发现中的应用包括：\n\n* 多靶点药理学：AI可以预测化合物对多种疾病的有效性。\n* 化学合成：AI可以优化化学合成过程，以实现更快和更高效的生产。\n* 药物重定位：AI可以识别现有药物的新用途。\n* 预测药物特性：AI可以预测化合物的效力、毒性和物理化学特性。\n\n**药物发现中AI的未来**\n\n随着AI的不断发展，预计将对制药行业产生重大影响。AI的成功应用将依赖于高质量数据的可用性、伦理问题的解决以及对基于AI的方法局限性的认识。\n```\n\n# 本地 AI 新闻聚合的好处\n\n* **隐私**：所有处理都在您的本地机器上进行，确保您的数据留在您自己手中。\n* **定制化**：您可以轻松修改代理的指令或添加新的代理以满足您的特定需求。\n* **最新信息**：通过使用 DuckDuckGo 搜索，您总是能获得关于您选择主题的最新新闻。\n* **AI 驱动的策展**：编辑助手帮助精炼和组织收集的新闻，提供更精致的最终输出。\n\n# 结论\n\n这个本地的人工智能驱动新闻聚合器展示了将大型语言模型与网络搜索能力结合的潜力。通过利用Ollama的Llama 3.2模型、Swarm进行代理编排，以及DuckDuckGo进行搜索，我们创建了一个强大的工具，可以让我们在任何感兴趣的话题上保持信息灵通，同时维护我们的隐私，并完全在本地计算机上运行。\n\n随着人工智能的不断发展，创建个性化、人工智能驱动工具的可能性只会不断扩大。这个新闻聚合器只是一个开始——想象一下，利用这些技术你还可以构建哪些其他创新应用！\n\n# 参考：\n\nSwarm Github : <https://github.com/openai/swarm>\n\n如果您觉得这篇文章信息丰富且有价值，我将非常感谢您的支持：\n\n* 在Medium上为它点赞几次 👏，帮助其他人发现这篇内容（您知道您可以点赞多达50次吗？）。您的点赞将帮助更多读者传播知识。\n- 与您的AI爱好者和专业人士网络分享。\n- 在LinkedIn上与我联系：<https://www.linkedin.com/in/manjunath-janardhan-54a5537/>\n\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/choosing-between-llm-agent-frameworks-69019493b259","frontmatter":{"title":"在 LLM 代理框架之间进行选择","meta_title":"在 LLM 代理框架之间进行选择","description":"构建定制基于代码的代理和主要代理框架之间的权衡。","date":"2024-10-29T12:57:34.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*jRMs19HqSCazE5dY","categories":["Programming","Technology","Machine Learning"],"author":"Rifx.Online","tags":["agents","frameworks","LangGraph","LlamaIndex","Workflows"],"draft":false,"slug":"blog/choosing-between-llm-agent-frameworks-69019493b259"},"content":"\n### 定制代码代理与主要代理框架之间的权衡\n\n\n\n代理正在迎来一个重要时刻。随着多个新框架和新的 [投资](https://foundationcapital.com/goodbye-aiops-welcome-agentsres-the-next-100b-opportunity/) 的涌入，现代 AI 代理正在克服 [不稳定的起源](https://arxiv.org/html/2405.13966v1)，迅速取代 RAG 成为实施优先事项。那么，2024 年是否终于会成为能够接管撰写电子邮件、预订航班、与我们的数据对话或似乎任何其他任务的自主 AI 系统的年份？\n\n也许，但要达到这一点还有很多工作要做。任何构建代理的开发者不仅需要选择基础——使用哪个模型、用例和架构——还需要选择利用哪个框架。你是选择长期存在的 LangGraph，还是较新的 LlamaIndex Workflows？或者你选择传统方式，自己编写整个代码？\n\n本文旨在让这个选择变得简单一些。在过去几周，我在主要框架中构建了相同的代理，以技术层面检查每个框架的一些优缺点。每个代理的所有代码都可以在 [这个库](https://github.com/Arize-ai/phoenix/tree/main/examples/agent_framework_comparison) 中找到。\n\n### 测试用代理的背景\n\n用于测试的代理包括功能调用、多种工具或技能、与外部资源的连接，以及共享状态或记忆。\n\n该代理具有以下能力：\n\n1. 从知识库回答问题\n2. 与数据对话：回答有关LLM应用程序的遥测数据的问题\n3. 数据分析：分析检索到的遥测数据中的更高级趋势和模式\n\n为了实现这些，代理具有三项初始技能：基于产品文档的RAG、在跟踪数据库上生成SQL，以及数据分析。代理用户界面使用简单的gradio支持的界面，代理本身构建为一个聊天机器人。\n\n## 基于代码的代理（无框架）\n\n开发代理时，您可以选择完全跳过框架，自己构建代理。开始这个项目时，我采用的就是这种方法。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*pw9-0lB5JMlVcPqo)\n\n### 纯代码架构\n\n下面的基于代码的代理由一个由OpenAI驱动的路由器组成，该路由器使用函数调用选择合适的技能。该技能完成后，它会返回路由器，以便调用另一个技能或响应用户。\n\n代理保持一个持续更新的消息和响应列表，在每次调用时完整传递给路由器，以保持上下文的连贯性。\n\n```python\ndef router(messages):\n    if not any(\n        isinstance(message, dict) and message.get(\"role\") == \"system\" for message in messages\n    ):\n        system_prompt = {\"role\": \"system\", \"content\": SYSTEM_PROMPT}\n        messages.append(system_prompt)\n\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=messages,\n        tools=skill_map.get_combined_function_description_for_openai(),\n    )\n\n    messages.append(response.choices[0].message)\n    tool_calls = response.choices[0].message.tool_calls\n    if tool_calls:\n        handle_tool_calls(tool_calls, messages)\n        return router(messages)\n    else:\n        return response.choices[0].message.content\n```\n\n技能本身在各自的类中定义（例如，GenerateSQLQuery），这些类共同保存在一个SkillMap中。路由器本身只与SkillMap交互，使用它来加载技能名称、描述和可调用函数。这种方法意味着将新技能添加到代理中只需将该技能编写为自己的类，然后将其添加到SkillMap中的技能列表中。这里的想法是使添加新技能变得简单，而不干扰路由器代码。\n\n```python\nclass SkillMap:\n    def __init__(self):\n        skills = [AnalyzeData(), GenerateSQLQuery()]\n\n        self.skill_map = {}\n        for skill in skills:\n            self.skill_map[skill.get_function_name()] = (\n                skill.get_function_dict(),\n                skill.get_function_callable(),\n            )\n\n    def get_function_callable_by_name(self, skill_name) -> Callable:\n        return self.skill_map[skill_name][1]\n\n    def get_combined_function_description_for_openai(self):\n        combined_dict = []\n        for _, (function_dict, _) in self.skill_map.items():\n            combined_dict.append(function_dict)\n        return combined_dict\n\n    def get_function_list(self):\n        return list(self.skill_map.keys())\n\n    def get_list_of_function_callables(self):\n        return [skill[1] for skill in self.skill_map.values()]\n\n    def get_function_description_by_name(self, skill_name):\n        return str(self.skill_map[skill_name][0][\"function\"])\n```\n\n总体而言，这种方法相对简单易行，但也面临一些挑战。\n\n### 纯代码代理的挑战\n\n第一个难点在于构建路由器系统提示。通常，上述示例中的路由器坚持自己生成 SQL，而不是将其委托给合适的技能。如果你曾经尝试让 LLM *不* 做某件事，你就会知道这种体验是多么令人沮丧；找到一个有效的提示需要经过多轮调试。考虑到每个步骤的不同输出格式也是棘手的。由于我选择不使用结构化输出，我必须准备好应对路由器和技能中每个 LLM 调用的多种不同格式。\n\n### 纯代码代理的好处\n\n基于代码的方法提供了良好的基准和起点，是学习代理工作原理的绝佳方式，而无需依赖现有框架中的现成代理教程。尽管说服 LLM 按预期行为可能具有挑战性，但代码结构本身足够简单，可能适用于某些用例（更多内容见下面的分析部分）。\n\n## LangGraph\n\nLangGraph 是最早的代理框架之一，首次发布于 2024 年 1 月。该框架旨在通过采用 Pregel 图结构来解决现有管道和链的无环特性。LangGraph 通过添加节点、边和条件边的概念，使您更容易在代理中定义循环，以遍历图形。LangGraph 建立在 LangChain 之上，并使用该框架中的对象和类型。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*fYgHiGwLhSUSrFv9)\n\n### LangGraph 架构\n\nLangGraph 代理在表面上看起来与基于代码的代理相似，但其背后的代码却截然不同。LangGraph 在技术上仍然使用“路由器”，即通过函数调用 OpenAI，并使用响应继续到新的步骤。然而，程序在技能之间的移动方式完全不同。\n\n```python\ntools = [generate_and_run_sql_query, data_analyzer]\nmodel = ChatOpenAI(model=\"gpt-4o\", temperature=0).bind_tools(tools)\n\ndef create_agent_graph():\n    workflow = StateGraph(MessagesState)\n\n    tool_node = ToolNode(tools)\n    workflow.add_node(\"agent\", call_model)\n    workflow.add_node(\"tools\", tool_node)\n\n    workflow.add_edge(START, \"agent\")\n    workflow.add_conditional_edges(\n        \"agent\",\n        should_continue,\n    )\n    workflow.add_edge(\"tools\", \"agent\")\n\n    checkpointer = MemorySaver()\n    app = workflow.compile(checkpointer=checkpointer)\n    return app\n```\n\n这里定义的图有一个用于初始 OpenAI 调用的节点，称为上面的“agent”，以及一个用于工具处理步骤的节点，称为“tools”。LangGraph 有一个内置对象 ToolNode，它接受一个可调用工具的列表，并根据 ChatMessage 响应触发它们，然后再返回到“agent”节点。\n\n```python\ndef should_continue(state: MessagesState):\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    if last_message.tool_calls:\n        return \"tools\"\n    return END\n\ndef call_model(state: MessagesState):\n    messages = state[\"messages\"]\n    response = model.invoke(messages)\n    return {\"messages\": [response]}\n```\n\n在每次调用“agent”节点后（换句话说：基于代码的代理中的路由器），should_continue 边决定是将响应返回给用户，还是传递给 ToolNode 以处理工具调用。\n\n在每个节点中，“state”存储了来自 OpenAI 的消息和响应列表，类似于基于代码的代理的方法。\n\n### LangGraph 的挑战\n\n大多数与 LangGraph 相关的困难源于需要使用 Langchain 对象，以便流程顺畅。\n\n**挑战 \\#1：函数调用验证**\n\n为了使用 ToolNode 对象，我不得不重构我现有的大部分 Skill 代码。ToolNode 接受一个可调用函数的列表，这最初让我认为可以使用我现有的函数，但由于我的函数参数，事情却出现了问题。\n\n这些技能被定义为具有可调用成员函数的类，这意味着它们的第一个参数是“self”。GPT\\-4o 足够智能，未在生成的函数调用中包含“self”参数，然而 LangGraph 将其视为缺少参数的验证错误。\n\n这花了我几个小时才弄明白，因为错误信息却将函数中的第三个参数（数据分析技能中的“args”）标记为缺失参数：\n\n```python\npydantic.v1.error_wrappers.ValidationError: 1 validation error for data_analysis_toolSchema\nargs field required (type=value_error.missing)\n```\n\n值得一提的是，错误消息源自 Pydantic，而不是 LangGraph。\n\n我最终下定决心，将我的技能重新定义为使用 Langchain 的 @tool 装饰器的基本方法，并成功使其工作。\n\n```python\n@tool\ndef generate_and_run_sql_query(query: str):\n    \"\"\"根据提示生成并运行 SQL 查询。\n\n    参数：\n        query (str): 包含原始用户提示的字符串。\n\n    返回：\n        str: SQL 查询的结果。\n    \"\"\"\n```\n\n**挑战 \\#2：调试**\n\n如前所述，在框架中进行调试是困难的。这主要归结为令人困惑的错误消息和抽象概念，使得查看变量变得更加困难。\n\n抽象概念主要在尝试调试在代理中传递的消息时出现。LangGraph 将这些消息存储在 state\\[“messages”] 中。图中的某些节点会自动从这些消息中提取，这可能使得在节点访问消息时理解消息的值变得困难。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*KuCg0WGHSklOKe6t)\n\n### LangGraph 的好处\n\nLangGraph 的主要好处之一是易于使用。图结构代码简洁且易于访问。特别是当您有复杂的节点逻辑时，拥有图的单一视图使理解代理之间的连接变得更加容易。LangGraph 还使将现有的基于 LangChain 构建的应用程序转换变得简单。\n\n### 外卖\n\n如果您使用框架中的所有内容，LangGraph 将运行良好；如果您超出它的范围，请准备好进行一些调试。\n\n## LlamaIndex 工作流\n\n工作流是代理框架领域的新进入者，早在今年夏天首次亮相。与 LangGraph 类似，它旨在简化循环代理的构建。工作流还特别关注异步运行。\n\n工作流的一些元素似乎是对 LangGraph 的直接回应，特别是它使用事件而不是边和条件边。工作流使用步骤（类似于 LangGraph 中的节点）来容纳逻辑，并通过发出和接收事件在步骤之间移动。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*22WuFVBWctdeiCSL)\n\n上面的结构看起来与 LangGraph 结构相似，只是增加了一项内容。我在工作流中添加了一个设置步骤，以准备代理上下文，更多内容请见下文。尽管结构相似，但支撑它的代码却截然不同。\n\n### 工作流架构\n\n下面的代码定义了工作流结构。与 LangGraph 类似，这里是我准备状态并将技能附加到 LLM 对象的地方。\n\n```python\nclass AgentFlow(Workflow):\n    def __init__(self, llm, timeout=300):\n        super().__init__(timeout=timeout)\n        self.llm = llm\n        self.memory = ChatMemoryBuffer(token_limit=1000).from_defaults(llm=llm)\n        self.tools = []\n        for func in skill_map.get_function_list():\n            self.tools.append(\n                FunctionTool(\n                    skill_map.get_function_callable_by_name(func),\n                    metadata=ToolMetadata(\n                        name=func, description=skill_map.get_function_description_by_name(func)\n                    ),\n                )\n            )\n\n    @step\n    async def prepare_agent(self, ev: StartEvent) -> RouterInputEvent:\n        user_input = ev.input\n        user_msg = ChatMessage(role=\"user\", content=user_input)\n        self.memory.put(user_msg)\n\n        chat_history = self.memory.get()\n        return RouterInputEvent(input=chat_history)\n```\n\n这也是我定义额外步骤“prepare\\_agent”的地方。此步骤从用户输入创建一个 ChatMessage，并将其添加到工作流内存中。将其分离为单独步骤意味着我们在代理循环通过步骤时会返回到它，这避免了重复将用户消息添加到内存中。\n\n在 LangGraph 的情况下，我通过一个位于图外的 run\\_agent 方法完成了同样的事情。然而，这一变化主要是风格上的，然而在我看来，将此逻辑与工作流和图形结合在一起更为简洁。\n\n设置好工作流后，我接着定义了路由代码：\n\n```python\n@step\nasync def router(self, ev: RouterInputEvent) -> ToolCallEvent | StopEvent:\n    messages = ev.input\n\n    if not any(\n        isinstance(message, dict) and message.get(\"role\") == \"system\" for message in messages\n    ):\n        system_prompt = ChatMessage(role=\"system\", content=SYSTEM_PROMPT)\n        messages.insert(0, system_prompt)\n\n    with using_prompt_template(template=SYSTEM_PROMPT, version=\"v0.1\"):\n        response = await self.llm.achat_with_tools(\n            model=\"gpt-4o\",\n            messages=messages,\n            tools=self.tools,\n        )\n\n    self.memory.put(response.message)\n\n    tool_calls = self.llm.get_tool_calls_from_response(response, error_on_no_tool_call=False)\n    if tool_calls:\n        return ToolCallEvent(tool_calls=tool_calls)\n    else:\n        return StopEvent(result=response.message.content)\n```\n\n以及工具调用处理代码：\n\n```python\n@step\nasync def tool_call_handler(self, ev: ToolCallEvent) -> RouterInputEvent:\n    tool_calls = ev.tool_calls\n\n    for tool_call in tool_calls:\n        function_name = tool_call.tool_name\n        arguments = tool_call.tool_kwargs\n        if \"input\" in arguments:\n            arguments[\"prompt\"] = arguments.pop(\"input\")\n\n        try:\n            function_callable = skill_map.get_function_callable_by_name(function_name)\n        except KeyError:\n            function_result = \"Error: Unknown function call\"\n\n        function_result = function_callable(arguments)\n        message = ChatMessage(\n            role=\"tool\",\n            content=function_result,\n            additional_kwargs={\"tool_call_id\": tool_call.tool_id},\n        )\n\n        self.memory.put(message)\n\n    return RouterInputEvent(input=self.memory.get())\n```\n\n这两者看起来更像是基于代码的代理，而不是 LangGraph 代理。这主要是因为工作流将条件路由逻辑保留在步骤中，而不是在条件边中——第 18 到 24 行在 LangGraph 中是一个条件边，而现在它们只是路由步骤的一部分——以及 LangGraph 具有一个 ToolNode 对象，几乎自动处理 tool\\_call\\_handler 方法中的所有内容。\n\n在路由步骤之后，我非常高兴地看到我可以将我的 SkillMap 和基于代码的代理中的现有技能与工作流一起使用。这些技能无需更改就可以与工作流配合使用，这让我的工作轻松了很多。\n\n### 工作流的挑战\n\n**挑战 \\#1: 同步与异步**\n\n尽管异步执行对于实时代理更为理想，但调试同步代理要容易得多。工作流设计为异步工作，强行实现同步执行非常困难。\n\n我最初以为只需去掉“async”方法标识，将“achat\\_with\\_tools”切换为“chat\\_with\\_tools”即可。然而，由于Workflow类中的底层方法也被标记为异步，因此有必要重新定义这些方法以便实现同步执行。最终我还是坚持使用异步方法，但这并没有使调试变得更加困难。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*78Hzqkiv9cI7W4UA)\n\n**挑战 \\#2: Pydantic 验证错误**\n\n在与LangGraph的困境重演中，关于技能的Pydantic验证错误出现了类似的问题。幸运的是，由于工作流能够很好地处理成员函数，这次解决起来要容易得多。我最终不得不在为我的技能创建LlamaIndex FunctionTool对象时更加规范：\n\n```python\nfor func in skill_map.get_function_list(): \n            self.tools.append(FunctionTool(\n                skill_map.get_function_callable_by_name(func), \n                metadata=ToolMetadata(name=func, description=skill_map.get_function_description_by_name(func))))\n```\n\n*摘自 AgentFlow.\\_\\_init\\_\\_，用于构建 FunctionTools*\n\n### 工作流的好处\n\n构建 Workflows 代理比构建 LangGraph 代理要容易得多，主要是因为 Workflows 仍然要求我自己编写路由逻辑和工具处理代码，而不是提供内置函数。这也意味着我的 Workflow 代理看起来与我的基于代码的代理极为相似。\n\n最大的区别在于事件的使用。我使用了两个自定义事件在我的代理中移动步骤：\n\n```python\nclass ToolCallEvent(Event):\n    tool_calls: list[ToolSelection]\n\nclass RouterInputEvent(Event):\n    input: list[ChatMessage]\n```\n\n发射器-接收器、基于事件的架构取代了直接调用我的代理中的某些方法，比如工具调用处理器。\n\n如果您有更复杂的系统，具有多个异步触发的步骤并可能发出多个事件，这种架构将非常有助于干净地管理这些情况。\n\nWorkflows 的其他好处包括它非常轻量且不强迫您使用很多结构（除了某些 LlamaIndex 对象的使用），而且它的基于事件的架构为直接函数调用提供了一个有用的替代方案——特别是对于复杂的异步应用程序。\n\n## 比较框架\n\n在这三种方法中，各自都有其优点。\n\n无框架的方法是最简单的实现方式。因为任何抽象都是由开发者定义的（即上面示例中的 SkillMap 对象），保持各种类型和对象的清晰是很容易的。然而，代码的可读性和可访问性完全取决于个别开发者，随着代理的复杂性增加，如果没有一些强制结构，很容易变得混乱。\n\nLangGraph 提供了相当多的结构，这使得代理的定义非常明确。如果一个更广泛的团队在协作开发代理，这种结构将提供一种强有力的架构强制方式。对于那些不太熟悉该结构的人，LangGraph 也可能为代理提供一个良好的起点。然而，这也有一个权衡——由于 LangGraph 为你做了很多事情，如果你没有完全接受这个框架，可能会导致麻烦；代码可能非常干净，但你可能会为此付出更多的调试成本。\n\nWorkflows 则处于中间位置。基于事件的架构可能对某些项目极为有用，而使用 LlamaIndex 类型的要求较少，为那些没有在整个应用程序中完全使用框架的人提供了更大的灵活性。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*PITmiVGuG8QuDVX6)\n\n最终，核心问题可能只是“你是否已经在使用 LlamaIndex 或 LangChain 来协调你的应用程序？”LangGraph 和 Workflows 都与各自的基础框架紧密相连，因此每个特定于代理的框架的额外好处可能不足以单凭优点而促使你切换。\n\n纯代码的方法可能始终是一个有吸引力的选项。如果你有严谨的方法来记录和强制执行任何创建的抽象，那么确保外部框架不会拖慢你的速度是很容易的。\n\n## 选择代理框架的关键问题\n\n当然，“这要看情况”从来不是一个令人满意的答案。这三个问题应该帮助你决定在下一个代理项目中使用哪个框架。\n\n***你是否已经在项目的重要部分使用了 LlamaIndex 或 LangChain？***\n\n如果是，请首先探索这个选项。\n\n***你是否熟悉常见的代理结构，还是希望有一些指导来告诉你如何构建代理？***\n\n如果你属于后者，尝试 Workflows。如果你*真的*属于后者，尝试 LangGraph。\n\n***你的代理之前是否已经构建过？***\n\n框架的一个好处是每个框架都有许多教程和示例可供使用。而纯代码代理的示例则少得多。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*wF9aSF1db1yaniqO)\n\n## 结论\n\n选择一个代理框架只是众多选择中的一个，这将影响生成式AI系统的生产结果。像往常一样，建立稳健的保护措施和 [LLM tracing](https://docs.arize.com/phoenix/tracing/llm-traces) 是非常重要的——并且要灵活应对新的代理框架、研究和模型颠覆既定技术。\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/claude-3-5-sonnet-new-pioneering-the-future-of-ai-with-computer-control-capabilities-37a6ff9f9033","frontmatter":{"title":"Claude 3.5 Sonnet（新）：利用计算机控制能力开拓人工智能的未来","meta_title":"Claude 3.5 Sonnet（新）：利用计算机控制能力开拓人工智能的未来","description":"Anthropic 于 2024 年 10 月 22 日发布了其最新的 AI 模型 Claude 3.5 Sonnet。此版本引入了革命性的计算机控制……","date":"2024-10-27T13:57:00.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*n0NkOFbhUm7_fllJ","categories":["Programming","Technology","Generative AI"],"author":"Rifx.Online","tags":["Claude","Sonnet","automation","benchmarks","safety"],"draft":false,"slug":"blog/claude-3-5-sonnet-new-pioneering-the-future-of-ai-with-computer-control-capabilities-37a6ff9f9033"},"content":"\n\n\n\n\nAnthropic于2024年10月22日发布了最新的AI模型Claude 3.5 Sonnet。此次发布引入了革命性的计算机控制能力，并在多个基准测试中实现了显著改进，为AI行业设定了新标准。\n\n## 革命性的计算机控制：新前沿\n\nClaude 3.5 Sonnet 的突出特点是其能够像人类一样与计算机进行交互。这一突破性的能力使得 AI 可以：\n\n* 使用鼠标和键盘输入导航桌面界面\n* 与各种应用程序和网页浏览器进行交互\n* 执行复杂的多步骤任务\n* 执行文件管理操作\n* 自动化重复的工作流程\n\n这一计算机控制功能目前处于公开测试阶段，代表了 AI 系统与数字界面交互方式的范式转变。尽管仍处于实验阶段，但早期测试显示出良好的结果，Claude 3.5 Sonnet 在仅截图任务的 OSWorld 基准测试中得分为 14.9% — 显著高于下一个最佳系统的 7.8%。\n\n## 基准突破性能\n\n升级后的模型在多个指标上表现出显著的改进：\n\n## 编码和技术任务\n\n* 在SWE-bench Verified上的性能为49%（较之前的33.4%有所提升）\n* 在HumanEval编码任务中的得分为93.7%\n* 在软件工程方面的表现优于专业编码系统\n\n## 学术和推理能力\n\n* 65% 的研究生级推理 (GPQA-Diamond)\n* 78% 的本科级知识 (MMLU Pro)\n* 78.3% 的数学问题解决能力 (MATH)\n\n## 商业应用\n\n* 69.2% 在零售领域任务上 (TAU-bench)\n* 46% 在航空领域任务上\n* 90.8% 在图表分析上的准确率\n* 94.2% 在文档问答上的准确率\n\n## 企业集成与可用性\n\nClaude 3.5 Sonnet 可以通过多个平台访问：\n\n* Anthropic API\n* Amazon Bedrock\n* Google Cloud’s Vertex AI\n\n包括 Asana、Canva、DoorDash 和 Replit 在内的主要公司已经开始在其工作流程中实施 Claude 3.5 Sonnet 的功能，特别是利用其计算机控制功能来处理复杂的自动化任务。\n\n## 实际应用\n\n## 软件开发\n\n* 自动化代码测试和调试\n* 智能IDE交互\n* 代码审查与优化\n* 文档生成\n\n## 客户支持\n\n* 高级聊天机器人功能\n* 可视化数据解读\n* 自动化工单解决\n* 流程自动化\n\n## 商业运营\n\n* 文档处理与分析\n* 从视觉源提取数据\n* 工作流自动化\n* 复杂问题解决\n\n## 安全与责任\n\nAnthropic 已实施强有力的安全措施用于计算机控制功能：\n\n* 新分类器以识别潜在的误用\n* 主动监控系统\n* 限制对敏感操作的访问\n* 定期安全评估\n\n## 展望未来\n\n虽然Claude 3.5 Sonnet在人工智能能力方面代表了重大进展，但重要的是要注意某些功能，特别是计算机控制，仍处于早期阶段。某些操作如滚动、拖动和缩放面临挑战，Anthropic鼓励开发者在探索这些新功能时，从低风险任务开始。\n\nClaude 3.5 Sonnet的发布标志着人工智能发展的一个关键时刻，将先进的推理能力与实用的计算机控制功能相结合。随着技术的不断发展，我们可以期待看到更多创新的应用以及人工智能系统与我们的数字世界互动方式的改进。\n\n*本文基于Anthropic、AWS和各类技术合作伙伴的官方公告和文档。有关最新信息，请参考Anthropic的官方文档。*\n\n"},{"lang":"zh","group":"blog","slug":"blog/claude-3-5-sonnet-v-s-gpt-4o-which-one-is-better-3b3675195bf9","frontmatter":{"title":"Claude 3.5 Sonnet V/S GPT-4O：哪一个更好","meta_title":"Claude 3.5 Sonnet V/S GPT-4O：哪一个更好","description":"2022 年 11 月，OpenAI 推出了 ChatGPT 模型，该模型彻底改变了我们搜索和与信息交互的方式。明年，在…","date":"2024-10-27T13:59:09.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*4MXLuSFfGwFkWWn0","categories":["Generative AI","Machine Learning","Natural Language Processing"],"author":"Rifx.Online","tags":["GPT-4o","Claude","multimodal","reasoning","code-generation"],"draft":false,"slug":"blog/claude-3-5-sonnet-v-s-gpt-4o-which-one-is-better-3b3675195bf9"},"content":"\n\n\n在2022年11月，OpenAI推出了ChatGPT，这一模型彻底改变了我们搜索和与信息互动的方式。次年3月，由前OpenAI员工创办的美国初创公司“Anthropic”推出了他们自己的AI模型“Claude”。自发布以来，这两家AI公司一直在竞争，以通过其AI模型为客户提供最佳的功能和体验。最近，OpenAI推出了“GPT-4o”，这是一个令人惊叹的模型，能够出色地处理文件、语音和视频数据。同样，Claude推出了“Claude 3.5 Sonnet”，他们声称这是最先进的AI模型，能够处理复杂任务。在本文中，我们将确定Claude 3.5 Sonnet和GPT-4o之间哪个更好，并比较其在相同输入下的功能和输出，以检查哪个更适合您。\n\n## 能力和特性\n\n### GPT-4o\n\n\n\nGPT-4o 是 OpenAI 最新推出的 LLM。“o” 代表 omni，意为拉丁语中的“每一个”。该模型可以分析语音、图像、视频和文件作为输入，并相应地做出回应。它可以接受语音输入，并以不同角色的声音输出，包括语调、情感等。整个过程与人类对话的延迟相当低，平均为 0.32 秒，而其他语音模型则为 2.8 秒。它还允许用户生成书面内容，如文章、博客、产品描述、不同编程语言的代码、数据分析、图表等。此外，GPT-4o 还可以分析图像和视频，使该模型可以充当语言翻译器、个人助理、虚拟教师或购物助手。它还可以用于医学、工程、军事等领域。要使用此功能，GPT-4o 可以使用用户的摄像头获取实时视图，并在语音模式下相应地回应。它还可以访问您的计算机屏幕，并描述屏幕上显示的内容，用户可以询问与屏幕上显示的内容相关的问题。\n\n*例如，用户可以在屏幕上启用该模型，打开 VS 代码，并提示模型充当编码助手，以获取编码问题的答案。或者，您可以启用摄像头，充当健身教练，检查您是否做得正确。*\n\n该模型具有独特的功能，如数据分析、代码解释器和实时网页浏览，使其与竞争对手不同。该模型还有大量的 GPTs，这是 ChatGPT 的定制版本。\n\n### Claude 3.5 Sonnet\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*BSMcOpvWZ5lUm4Tl)\n\nClaude 3.5 Sonnet 是由 Anthropic 推出的 AI 聊天机器人。它是 Claude AI 模型系列的第三代。这一模型在多个评估中表现出色，保持了高基准，避免了幻觉和错误信息。虽然它不支持像 GPT-4o 那样的语音和视频功能，但它仍然可以执行所有基本任务，例如文本生成和不同编程语言的代码生成、头脑风暴等。根据 Anthropic 的报告，Claude 3.5 Sonnet 是市场上最好的计算机视觉模型之一，可以用于分析图表和图形，从图像中转录文本等。Claude 拥有一个先进的功能，“Artifacts”，这是一个在对话中出现的特殊弹出窗口，允许用户查看代码片段、文本文件或网站设计，并允许他们实时编辑输出。\n\n*例如，用户可以在工作流程中使用计算机视觉和 artifacts。用户可以在纸上进行网站设计的基本原型制作，将文件附加到 Claude 3.5 Sonnet，并提示它根据原型设计网站。生成的代码和网站设计会出现在 artifacts 中。用户可以根据自己的需求编辑代码和设计。用户还可以将他们的项目实时发布到互联网上。*\n\n## 逐项比较\n\n在本节中，我们将根据复杂推理和代码生成等因素比较这两个 LLM，检查它们在处理复杂任务方面的能力，并看看哪个模型更好。\n\n* **研究生水平推理(GPQA, Diamond)**此因素评估模型处理研究生水平教育中复杂、高级推理任务的能力。在此任务中，研究人员在 GPQA 测试中比较模型，该测试由专家设计，包含448个不同领域的问题。这些问题是 Google Proof，因此任何人都无法在线找到它们。Claude 的得分接近 59.4%，而 GPT-4o 的得分仅为 53.6%。虽然两个得分相对接近，但正如我们所见，Claude 在需要高级分析思维的任务中可能是更好的选择，例如研究分析、复杂问题解决和高学术水平的问题。\n* **本科水平知识(MMLU)**MMLU，即大规模多任务语言理解，是一个基准，解释任何 AI 模型在本科水平上对各个学科的通用知识理解。Claude 3.5 Sonnet 在此实验中的得分为 88.3%，而 GPT-4o 的得分为 88.7%。这表明这两个 LLM 在多个领域进行了训练，并对这些领域有更深入的理解。这使得 AI 模型成为通用知识任务、多个学科的基础辅导等的合适工具。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*A4w-tvsxcmFINaQT)\n\n* **代码(HumanEval)**HumanEval 是一个基准，评估模型生成、理解和调试代码的能力。在这个基准中，Claude 3.5 Sonnet 达到了 92%的得分，而 GPT-4o 的得分为 90.2%。Claude 3.5 Sonnet 在此任务中的结果非常出色，因为它提供了比 GPT-4o 更好的编码环境“Artifacts”和更好的代码生成。Claude 允许用户在 Artifacts 弹出窗口中设计、编辑和运行代码。在 Claude 3.5 Sonnet 发布后，大家都在开发工具、网站和基本游戏，并在互联网上分享它们。另一方面，GPT-4o 的得分也不错，但它的界面中没有任何编码环境，因此开发人员必须花费很多精力，因为它生成的代码很难达到结果。\n* **文本推理(DROP, FLscore)**DROP（段落离散推理）是一个基准，测量模型理解复杂文本信息的能力。在这个挑战中，Claude 3.5 Sonnet 的得分为 87.1%，而 GPT-4o 的得分为 83.4%。这表明 Claude 3.5 Sonnet 在涉及详细文本分析、文本审查、复杂问答系统等任务时更好且更有效。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Kcy7sFb2FYpbfrwp)\n\n* **数学问题解决(MATH)**此测试评估任何 AI 模型解决各种数学问题的能力。Claude 3.5 Sonnet 的得分仅为 71.1%，而 GPT-4o 的得分为 76.6%。这些得分使 GPT-4o 成为数学问题解决任务的更好模型，并可用于财务建模、科学计算和高级数据分析等数学计算。\n* **多语言数学(MSGM)**此因素描述任何 AI 模型在多种语言中解决数学问题的能力。两个模型的得分接近：GPT-4o 90.5% 和 Claude 3.5 Sonnet 91.6%。这表明两个模型表现出色，Claude 略胜一筹。该能力对于教育应用或任何需要跨语言障碍进行数学推理交流的场景特别有用。\n* **视觉问答(MMU/val)**此因素描述 LLM 分析图像中呈现的信息的能力。GPT-4o 在这一基准中以 69.1% 超过 Claude 3.5 Sonnet 的 68.3%。另一方面，在分析文档中的文本时，Claude 3.5 Sonnet 的得分为 95.2%，而 GPT-4o 的得分为 92.1%。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*xzjqBV2YL0lVFitX)\n\n* **图像生成**图像生成是 LLM 从文本生成图像的能力。GPT-4o 集成了 DallE-2，可以通过文本生成图像，结果非常出色。另一方面，Claude 3.5 Sonnet 无法创建任何图像。此功能还帮助 GPT-4o 更好地设计网站和参考，因为它在许多图像上进行了训练。\n* **知识截止**在这里，两个模型都在特定日期之前的有限数据集上进行了训练。Claude 3.5 Sonnet 在 2024 年 4 月之前的数据上进行了训练，而 GPT-4o 则在 2024 年之前的数据上进行了训练。GPT-4o 的真正优势在于它具有实时网页浏览功能，这有助于 LLM 定期在新数据上进行训练。\n\n## GPT-4o 的优点：\n\n* 处理语音、图像和视频输入。\n* 实时网页浏览能力。\n* 更快的响应时间（平均 0.32 秒）。\n* 在数学问题解决方面表现优越。\n* 可以使用 DALL-E 2 生成图像。\n\n## GPT-4o 的缺点：\n\n* 研究生水平推理的性能稍低。\n* 没有内置的编码环境。\n* 文档视觉问答得分较低。\n* 代码生成能力稍逊。\n* 在详细文本分析方面效果较差。\n\n## Pros Claude 3.5 Sonnet:\n\n* 在研究生级别的推理方面表现出色。\n* 优越的代码生成和内置的“工件”功能。\n* 在详细文本分析中表现更佳。\n* 在文档视觉问答中得分更高。\n* 在多语言数学方面略有优势。\n\n## Cons Claude 3.5 颂：\n\n* 无法处理语音或视频输入。\n* 没有图像生成能力。\n* 在视觉问答方面性能稍低。\n* 无法访问实时网络信息。\n* 在数学问题解决方面较弱。\n\n## 结论\n\nGPT-4o 和 Claude 3.5 Sonnet 在各种任务中展现了令人印象深刻的能力，各自有其优势。GPT-4o 在多模态输入、实时信息访问和图像生成方面表现出色，使其在多种应用中非常灵活。Claude 3.5 Sonnet 在复杂推理、代码生成和详细文本分析方面表现突出，在特定的学术和专业背景下提供了更优的性能。选择这两种模型取决于具体的使用案例和所需的功能。随着人工智能技术的进步，我们可以期待进一步的改进和针对不同需求的专业模型。\n\n"},{"lang":"zh","group":"blog","slug":"blog/explore-swarm-multi-agent-framework-locally-0e25ee617795","frontmatter":{"title":"本地探索 Swarm 多智能体框架","meta_title":"本地探索 Swarm 多智能体框架","description":"Swarm 是一个实验性示例框架，用于模拟轻量级多代理框架，用于教育目的。通常它与 Open… 配合使用","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*0ZVceq32bvkytC7HSIgmwA.png","categories":["Programming","Technology","Education"],"author":"Rifx.Online","tags":["Swarm","Multi-Agent","Framework","OpenAI","Ollama"],"draft":false,"slug":"blog/explore-swarm-multi-agent-framework-locally-0e25ee617795"},"content":"\n\n\n\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*zkpW8DDwh0TTYuHJVJbDaw.png)\n\nSwarm 是一个实验性样本框架，用于模拟轻量级多智能体框架，旨在教育目的。通常它与 Open AI Key 一起使用，但我们可以更改为使用本地的 Ollama 或 LM Studio 模型。\n\n**设置：**\n\n\n```python\n## 创建一个新的 Conda 或 Python 虚拟环境并激活它\nconda install python==3.10\npip install torch openai\npip install transformers accelerate huggingface_hub\npip install git+ssh://git@github.com/openai/swarm.git\n```\n**使用 Open AI Key：**\n\n\n```python\nexport OPEN_API_KEY = Your Key\n```\n**使用 Ollama 或 LM Studio 本地 LLM — 更新为本地 URL：**\n\n\n```python\n## 查找 conda 或 python 虚拟环境中的 site-packages/swarm\n## 找到文件 core.py\nclass Swarm:\n    def __init__(self, client=None):\n        if not client:\n          # 实际代码\n          #client = OpenAI()\n          # 将基础 URL 和 API Key 更新为 Ollama / LM Studio\n          # 在本演示中，我们使用 LM Studio 和 Llama 3.1\n          client = OpenAI(base_url=\"http://localhost:1234/v1\",api_key=\"random\")\n        self.client = client\n```\n**克隆仓库：**\n\n克隆仓库 — 在这里您可以找到不同用例的示例目录，如基本、航空公司和天气等。\n\n\n```python\ngit clone https://github.com/openai/swarm.git\ncd swarm/examples\n```\n**示例代码：**\n\n\n```python\nfrom swarm import Swarm, Agent\n\nclient = Swarm()\n\n\nit_agent = Agent(\n    name=\"IT Agent\",\n    instructions=\"You are an IT Expert with 10 Years of Experience.\",\n)\n\nsales_agent = Agent(\n    name=\"Sales Agent\",\n    instructions=\"You are a Sales Expert with 5 Years of Experience and knows about best selling mobiles.\",\n)\n\ndef transfer_to_sales_agent():\n    print(\"Sales agent in action\")\n    \"\"\"Transfer sales related questions to sales team immediately.\"\"\"\n    return sales_agent\n\ndef transfer_to_it_agent():\n    print(\"IT agent in action\")\n    \"\"\"Transfer IT users immediately.\"\"\"\n    return it_agent\n\nenglish_agent = Agent(\n    name=\"English Agent\",\n    instructions=\"You only speak English.\",\n    functions=[transfer_to_sales_agent,transfer_to_it_agent],\n)\n\n\nmessages = [{\"role\": \"user\", \"content\": \"How to install pandas lib?\"}]\nresponse = client.run(agent=english_agent, messages=messages)\n\nprint(response.messages[-1][\"content\"])\n\nmessages = [{\"role\": \"user\", \"content\": \"What are the best selling items?\"}]\nresponse = client.run(agent=english_agent, messages=messages)\n\nprint(response.messages[-1][\"content\"])\n```\n**参考文献：**\n\n\n```python\nhttps://github.com/openai/swarm\n\nhttps://github.com/victorb/ollama-swarm/tree/main\n```\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*hCFJ4VQoT12yElYPXwXvWA.png)\n\n鉴于这是一个实验性版本，仍有很大的改进空间。航空代理示例代码 [swarm/examples/airline] 非常有趣，因此可以尝试这些示例。试试看，并在评论中分享您的经验。谢谢。\n\n"},{"lang":"zh","group":"blog","slug":"blog/fine-tuning-llama-3-with-unsloth-79c3465ef3e3","frontmatter":{"title":"使用 Unsloth 对 LLama 3 进行微调","meta_title":"使用 Unsloth 对 LLama 3 进行微调","description":"在本文中，我将向您展示如何使用 Unsloth 对 LLM（来自 Meta 的 Llama 3）进行微调（包括自定义数据集的方法）","date":"2024-10-30T12:58:41.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kaXoudNTGeGfuNPl_kta5g.jpeg","categories":["Programming","Machine Learning","Natural Language Processing"],"author":"Rifx.Online","tags":["Llama","Unsloth","LoRA","Alpaca","NVIDIA"],"draft":false,"slug":"blog/fine-tuning-llama-3-with-unsloth-79c3465ef3e3"},"content":"\n\n\n在本文中，我将向您展示如何使用 [Unsloth](https://github.com/unslothai/unsloth) 微调 LLM（Meta 的 Llama 3）。我还将提供使用您自己自定义数据集的方法。\n\n**注意：** Unsloth 是一个加速 LLM 在 NVIDIA GPU 上微调的库（与传统方法相比，内存使用减少 40%）。与 Hugging Face 兼容，支持 Llama 和 Mistral 架构。\n\n如果您觉得我的文章有趣，请不要忘记 **点赞并 [关注](https://medium.com/@soulawalid)** 👍🏼，写这些文章需要时间和精力！\n\n您可以访问 GitHub 仓库中提供的免费笔记本。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*_L4o4MDQ7W5__OwW0E5RWA.png)\n\n由于我使用的是 Llama 3，因此我将点击笔记本（您也可以在自己的计算机上安装 Unsloth）。\n\n**注意：** 我将使用这个数据集 “[alpaca\\-cleaned](https://huggingface.co/datasets/yahma/alpaca-cleaned)” 来自 Hugging Face，数据采用 Alpaca 格式，即包含（指令、输入和输出）。\n\n### 开始项目\n\n在项目中，我将指导您使用 Unsloth 进行微调，解释代码并提供建议，让我们开始我们的项目：\n\n**1/ 安装所需的包：** 我们首先需要安装 **Unsloth** 和 **xformers**、**trl**、**peft**、**accelerate**、**bitsandbytes** 库，以便进行高效的模型训练和推理。\n\n```python\n!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n!pip install --no-deps xformers trl peft accelerate bitsandbytes\n```\n\n**2/ 加载和配置模型：** 在配置中，我将设置以下内容：\n\n* 将最大序列长度设置为 **2048**\n* 将 dtype 设置为 **None**，它会自动检测数据类型。\n* 以 **4-位精度**加载模型，我认为这已经足够。\n\n**注意：** 您可以在资源部分找到我关于微调 LLM 的技巧的文章。\n\n```python\nfrom unsloth import FastLanguageModel\nimport torch\n\n## 配置\nmax_seq_length = 2048\ndtype = None\nload_in_4bit = True\n\n## 加载选定的模型\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/llama-3-8b-bnb-4bit\",\n    max_seq_length=max_seq_length,\n    dtype=dtype,\n    load_in_4bit=load_in_4bit,\n)\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*cJSAcJFP7E-qJkqKUsHqLw.png)\n\n**3/ 应用 PEFT（参数高效微调）：** 然后我们将使用 LoRA 对预训练模型进行微调。\n\n* r = 16 是 LoRA 的秩参数。**注意：** 常见值为 8、16、32、64、128\n* lora_alpha = 16 代表 LoRA 更新的缩放因子（我将写一篇关于 LoRA 的文章，以详细解释每个部分）\n* 对于 LoRA 不使用 dropout 和偏置\n* 对于 use_gradient_checkpointing，我们使用 Unsloth 来处理（节省内存）\n\n```python\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 16,\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha = 16,\n    lora_dropout = 0,\n    bias = \"none\",\n    use_gradient_checkpointing = \"unsloth\",\n    random_state = 3407,\n    use_rslora = False,\n    loftq_config = None,\n)\n```\n\n**4/ 定义提示模板：** 我们将创建 alpaca 提示模板以格式化数据集（如果您使用的数据不是这种格式）。\n\n我们还将添加 EOS（结束序列）以通知 LLM 句子已结束。\n\n最后是格式化函数，该函数接受一批示例并根据我们之前编写的 alpaca 提示模板格式化每个示例。\n\n* 它从每个示例（行）中提取指令、输入和输出字段。\n* 然后将这些字段格式化到模板中并附加 EOS 标记。\n* 格式化的文本存储在列表中，并作为具有单个键“text”的字典返回。\n\n```python\nalpaca_prompt = \"\"\"以下是描述任务的指令，配有提供进一步上下文的输入。写一个适当完成请求的响应。\n\n#### 指令：\n{}\n\n#### 输入：\n{}\n\n#### 响应：\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token\n\ndef formatting_prompts_func(examples):\n    instructions = examples[\"instruction\"]\n    inputs = examples[\"input\"]\n    outputs = examples[\"output\"]\n    texts = []\n    for instruction, input, output in zip(instructions, inputs, outputs):\n        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n        texts.append(text)\n    return {\"text\": texts}\n```\n\n**5/ 加载和格式化数据集：** 加载 Alpaca 数据集并对每个数据集示例应用格式化。\n\n```python\nfrom datasets import load_dataset\ndataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\ndataset = dataset.map(formatting_prompts_func, batched = True)\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*M8EmbLMdoqrM-JlkMpDv8g.png)\n\n**6/ 设置和训练模型：** 我在我[之前的文章](https://readmedium.com/supervised-fine-tuning-tips-for-your-llm-projects-f84f20593653)中涵盖了大部分关于微调的技巧。\n\n```python\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2, # 用于数据预处理的进程数量\n    packing = False, # 是否将多个序列打包成一个批次以提高训练效率\n    args = TrainingArguments(\n        per_device_train_batch_size = 2, # 每个设备的批次大小\n        gradient_accumulation_steps = 4, # 梯度累积步数，允许有效增大批次大小\n        warmup_steps = 5, # 进行线性学习率预热的步骤数\n        max_steps = 60, # 总训练步骤数\n        learning_rate = 2e-5,# 优化器的学习率\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"cosine\",\n        seed = 3407,\n        output_dir = \"outputs\",\n    ),\n)\n\ntrainer_stats = trainer.train()\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Vb_OqGP9CPc8xZdnkclGyQ.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*PI0JXrTbpjuviyQ4bZJnFg.png)\n\n**7/ 推理和生成：** 我们通过准备输入提示、对其进行标记化，然后使用模型根据该提示生成新文本来准备模型进行推理。生成的文本随后被转换回可读形式。\n\n```python\nFastLanguageModel.for_inference(model)\ninputs = tokenizer(\n[\n    alpaca_prompt.format(\n        \"继续斐波那契数列。\", # 指令\n        \"1, 1, 2, 3, 5, 8\", # 输入\n        \"\", # 输出 - 留空以进行生成！\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*PI6SBL_YPPj0-RSAn5nl7g.png)\n\n您还可以使用 TextStreamer 进行连续推理，这样您可以看到生成的每个标记，而不是一直等待整个过程！\n\n```python\nFastLanguageModel.for_inference(model)\ninputs = tokenizer(\n[\n    alpaca_prompt.format(\n        \"继续斐波那契数列。\",\n        \"1, 1, 2, 3, 5, 8\",\n        \"\",\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n```\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*NaSQ1vQKORU1I3DsOU2iOA.png)\n\n**8/ 保存模型：** 如果您对此感到满意，可以保存您的模型或将其推送到 Hugging Face Hub。\n\n```python\nmodel.save_pretrained(\"lora_model\")\ntokenizer.save_pretrained(\"lora_model\")\n## model.push_to_hub(\"your_name/lora_model\", token = \"...\")\n## tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\")\n```\n\n**9/ 加载模型：**\n\n```python\nif False:\n    from unsloth import FastLanguageModel\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name = \"lora_model\",\n        max_seq_length = max_seq_length,\n        dtype = dtype,\n        load_in_4bit = load_in_4bit,\n    )\n    FastLanguageModel.for_inference(model)\n```\n\n**10/ 用于生成：**\n\n```python\ninputs = tokenizer(\n[\n    alpaca_prompt.format(\n        \"巴勒斯坦的首都是什么？\",\n        \"\",\n        \"\",\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)\n```\n\n如果您有特定主题希望我们讨论，请随时告诉我！您的反馈将有助于塑造我的内容方向，确保其保持相关性和吸引力😀\n\n\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/generating-structured-data-from-an-image-with-gpt-vision-and-langchain-34aaf3dcb215","frontmatter":{"title":"使用 GPT Vision 和 Langchain 从图像生成结构化数据","meta_title":"使用 GPT Vision 和 Langchain 从图像生成结构化数据","description":"在当今世界，视觉数据非常丰富，从图像中提取有意义信息的能力变得越来越重要……","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*FPRRg85jYb7MrzXEpNWbmw.jpeg","categories":["Programming","Computer Vision","Natural Language Processing"],"author":"Rifx.Online","tags":["Langchain","GPT","vision","LLMs","structured"],"draft":false,"slug":"blog/generating-structured-data-from-an-image-with-gpt-vision-and-langchain-34aaf3dcb215"},"content":"\n\n\n\n\n在当今这个视觉数据丰富的世界中，从图像中提取有意义信息的能力变得越来越重要。Langchain是一个强大的框架，用于构建大型语言模型（LLMs）应用程序，提供了一套多功能的工具来应对这一挑战。在本文中，我们将探讨如何使用Langchain从图像中提取结构化信息，例如计算人数和列出主要物体。\n\n在深入代码之前，让我们先了解一下任务的背景。想象一下你有一张场景的图像，比如城市街道。你的目标是从这张图像中提取有价值的信息，包括在场的人数和场景中的主要物体列表。\n\n## 关于 Langchain\n\nLangchain 是一个综合框架，允许开发者利用大型语言模型（LLMs）的强大功能构建复杂的应用程序。它提供了模块化和可扩展的架构，使开发者能够创建针对特定需求的自定义管道、代理和工作流。\n\nLangchain 简化了 LLM 的集成，提供了处理各种数据源（包括文本、图像和结构化数据）的抽象和工具。它支持来自不同提供商的广泛 LLM，例如 OpenAI 和 Anthropic，使得在单个应用程序中轻松切换模型或组合多个模型变得简单。\n\n## 准备环境并设置 OpenAI API 密钥\n\n要跟随本教程，您需要安装 Langchain。您可以使用 pip 安装它：\n\n```python\npip install langchain langchain_openai\n```\n要在 Langchain 中使用 OpenAI 语言模型，您需要从 OpenAI 获取一个 API 密钥。如果您还没有 API 密钥，可以在 OpenAI 网站上注册一个 (<https://openai.com/api/>)。\n\n一旦您拥有了 API 密钥，可以将其设置为系统中的环境变量，或者直接在代码中提供。以下是如何将 API 密钥设置为环境变量的示例：\n\n```python\nexport OPENAI_API_KEY=\"your_openai_api_key_here\"\n```\n或者，您可以直接在 Python 代码中提供 API 密钥：\n\n```python\nimport os\nimport langchain\nos.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key_here\"\n```\n在设置好 API 密钥后，Langchain 将能够与 OpenAI API 进行身份验证并使用他们的语言模型。\n\n## 加载和编码图像\n\n在我们使用 Langchain 处理图像之前，我们需要从文件中加载图像数据，并将其编码为可以传递给语言模型的格式。下面的代码定义了一个函数 `load_image`，该函数接受一个包含 `image_path` 键的字典，并返回一个新的字典，其中 `image` 键包含编码为 base64 字符串的图像数据。\n\n```python\ndef load_image(inputs: dict) -> dict:\n    \"\"\"Load image from file and encode it as base64.\"\"\"\n    image_path = inputs[\"image_path\"]\n  \n    def encode_image(image_path):\n        with open(image_path, \"rb\") as image_file:\n            return base64.b64encode(image_file.read()).decode('utf-8')\n    image_base64 = encode_image(image_path)\n    return {\"image\": image_base64}\n```\n`load_image` 函数首先从输入字典中提取 `image_path`。然后，它定义了一个嵌套函数 `encode_image`，该函数以二进制模式打开图像文件，读取其内容，并使用 Python 标准库中的 `base64.b64encode` 函数将其编码为 base64 字符串。\n\n`load_image` 函数使用提供的 `image_path` 调用 `encode_image`，并将结果 base64 编码字符串存储在 `image_base64` 变量中。最后，它返回一个新的字典，其中 `image` 键设置为 `image_base64`。\n\n为了将此函数集成到 Langchain 流水线中，我们可以创建一个 `TransformChain`，该链接受 `image_path` 作为输入，并生成 `image`（base64 编码字符串）作为输出。\n\n```python\nload_image_chain = TransformChain(\n    input_variables=[\"image_path\"],\n    output_variables=[\"image\"],\n    transform=load_image\n)\n```\n通过这种设置，我们可以轻松地将图像加载和编码作为更大 Langchain 工作流的一部分，从而使我们能够使用大型语言模型处理视觉数据和文本。\n\n## 定义输出结构\n\n在我们提取图像信息之前，需要定义我们希望接收的输出结构。在这种情况下，我们将创建一个名为 `ImageInformation` 的 Pydantic 模型，其中包括图像描述和我们可能想要提取的任何其他信息的字段。\n\n```python\nfrom langchain_core.pydantic_v1 import BaseModel, Field\n\nclass ImageInformation(BaseModel):\n \"\"\"Information about an image.\"\"\"\n image_description: str = Field(description=\"a short description of the image\")\n people_count: int = Field(description=\"number of humans on the picture\")\n main_objects: list[str] = Field(description=\"list of the main objects on the picture\")\n```\n\n## 设置图像模型\n\n接下来，我们将创建一个链，将图像加载和编码步骤与 LLM 调用步骤结合起来。由于 `ChatOpenAI` 模型在我的理解中并不具备同时处理文本和图像输入的能力，我们将创建一个包装链来实现这一功能。\n\n```python\nfrom langchain.chains import TransformChain\nfrom langchain_core.messages import HumanMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain import globals\nfrom langchain_core.runnables import chain\n\n## Set verbose\nglobals.set_debug(True)\n\n@chain\ndef image_model(inputs: dict) -> str | list[str] | dict:\n \"\"\"Invoke model with image and prompt.\"\"\"\n model = ChatOpenAI(temperature=0.5, model=\"gpt-4-vision-preview\", max_tokens=1024)\n msg = model.invoke(\n             [HumanMessage(\n             content=[\n             {\"type\": \"text\", \"text\": inputs[\"prompt\"]},\n             {\"type\": \"text\", \"text\": parser.get_format_instructions()},\n             {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{inputs['image']}\"}},\n             ])]\n             )\n return msg.content\n```\n在这个代码片段中，我们定义了一个名为 `image_model` 的链，使用提供的提示、格式说明和图像调用 `ChatOpenAI` 模型。`image_model` 链接受一个包含提示和 base64 编码图像字符串的字典 `inputs`。\n\n在链内部，我们创建了一个 `HumanMessage` 对象，该对象结合了提示文本、格式说明和图像 URL，以数据 URI 格式化，包含 base64 编码的图像数据。然后，我们使用这个 `HumanMessage` 对象调用 `ChatOpenAI` 模型，使用专门为涉及文本和图像的多模态任务设计的 `gpt-4-vision-preview` 模型。\n\n该模型处理文本提示和图像，并返回输出。\n\n## 整合所有内容\n\n现在我们已经拥有了所有必要的组件，我们可以定义一个函数来协调整个过程：\n\n```python\nfrom langchain_core.output_parsers import JsonOutputParser\n\nparser = JsonOutputParser(pydantic_object=ImageInformation)\ndef get_image_informations(image_path: str) -> dict:\n   vision_prompt = \"\"\"\n   Given the image, provide the following information:\n   - A count of how many people are in the image\n   - A list of the main objects present in the image\n   - A description of the image\n   \"\"\"\n   vision_chain = load_image_chain | image_model | parser\n   return vision_chain.invoke({'image_path': f'{image_path}', \n                               'prompt': vision_prompt})\n```\n在这个函数中，我们定义了一个提示，要求LLM提供图像中人物的数量和主要物体的列表。然后，我们创建一个链，将图像加载步骤（`load\\_image\\_chain`）、LLM调用步骤（`image\\_model`）和JSON输出解析器（`parser`）结合在一起。最后，我们用图像路径和提示调用这个链，函数返回一个包含提取信息的字典。\n\n## 示例用法\n\n要使用此功能，只需提供图像文件的路径：\n\n\n```python\nresult = get_image_informations(\"path/to/your/image.jpg\")\nprint(result)\n```\n这将输出一个包含请求信息的字典，例如：\n\n\n```python\n{\n 'description': 'a view of a city showing cars waiting at a traffic light',\n 'people_count': 5,\n 'main_objects': ['car', 'building', 'traffic light', 'tree']\n}\n```\n\n## 结论\n\nLangchain 提供了强大的工具集，用于处理大型语言模型并从各种数据源（包括图像）中提取有价值的信息。通过将 Langchain 的功能与自定义提示和输出解析相结合，您可以创建强大的应用程序，从视觉数据中提取结构化信息。\n\n请记住，输出的质量将取决于您使用的 LLM 的能力以及您提示的具体性。尝试不同的模型和提示，以找到最适合您用例的解决方案。\n\n如果您找到更好的方法来实现相同的结果或有改进建议，请随时在评论中分享。本文提供的代码示例旨在作为起点，可能还有其他方法或优化。\n\n"},{"lang":"zh","group":"blog","slug":"blog/glm-4-voice-9b-real-time-multilingual-voice-conversation-ai-install-locally-in-minutes-ce2fcd6c8fd8","frontmatter":{"title":"GLM-4-Voice 9B——实时多语言语音对话 AI——几分钟内即可在本地安装","meta_title":"GLM-4-Voice 9B——实时多语言语音对话 AI——几分钟内即可在本地安装","description":"如何设置GLM-4-Voice 9B以实现中英文无缝实时语音交互，并探索其独特的架构……","date":"2024-11-04T12:29:59.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*LATTpEc2AHvqgVyPKSzW7A.jpeg","categories":["Voice Assistants","Natural Language Processing","Chatbots"],"author":"Rifx.Online","tags":["multilingual","conversation","real-time","customization","performance"],"draft":false,"slug":"blog/glm-4-voice-9b-real-time-multilingual-voice-conversation-ai-install-locally-in-minutes-ce2fcd6c8fd8"},"content":"\n### 如何设置 GLM\\-4\\-Voice 9B 以实现无缝的实时语音交互，支持英语和中文，并探索其独特的架构、低延迟响应和可定制的声音属性。\n\n👨🏾‍💻 [GitHub](https://github.com/mdmonsurali) ⭐️ \\| 👔[LinkedIn](https://www.linkedin.com/in/mdmonsurali/) \\|📝 [Medium](https://medium.com/@monsuralirana)\n\n\n\n## 介绍\n\n近年来，语音启用的人工智能取得了显著进展，使对话代理能够更好地理解和响应人类语言。从虚拟助手到客户服务机器人，语音人工智能已成为各个行业的重要工具。然而，大多数模型在流利地切换语言、理解口语查询的细微差别以及提供高质量响应方面仍然面临挑战。这正是Zhipu AI的GLM-4-Voice脱颖而出的地方。GLM-4-Voice作为一款端到端的语音模型，推动了多语言对话人工智能的边界，支持英语和中文的实时对话，同时提供可适应且类人化的响应生成。\n\n在本文中，我们将探讨为什么GLM-4-Voice值得关注，它的独特之处，以及如何在本地设置和开始使用它。我们还将查看其架构，并提供访问网络演示的实用指南。\n\n## 为什么选择 GLM-4-Voice？\n\n传统的语言模型通常仅限于文本，并需要额外的处理层来处理语音。它们在交互性方面可能会遇到困难，或者存在延迟问题。GLM-4-Voice 通过一个统一的模型克服了这些限制，能够直接处理和生成语音。以下是它的突出之处：\n\n1. **端到端语音处理**：与许多依赖于单独的文本到语音 (TTS) 或语音到文本 (STT) 模块的模型不同，GLM-4-Voice 直接以口语形式进行解读和响应，从而提供更无缝和更具响应性的体验。\n2. **多语言支持**：该模型在处理英语和中文这两种全球广泛使用的语言方面表现出色。它流畅切换语言的能力使其非常适合双语环境和国际应用。\n3. **可定制属性**：GLM-4-Voice 允许在情感、语调、语速甚至方言上进行调整，使其能够生成更自然和情境合适的响应。\n4. **低延迟**：通过支持流式推理，该模型的延迟约为 20 个标记，使其能够在实时对话中实现近乎即时的响应。\n\n## GLM\\-4\\-Voice 的特点\n\nGLM\\-4\\-Voice 带来了几个独特的功能，使其与其他语音模型区别开来。以下是它的特别之处：\n\n* **实时语音互动**：通过支持低延迟响应，GLM\\-4\\-Voice 能够保持流畅自然的对话，这对客户支持和互动 AI 等应用至关重要。\n* **动态语音属性**：用户可以指定模型的情感语调、语速和其他特征，使互动更加生动且适合各种场景。\n* **具备上下文意识的双语支持**：该模型旨在理解和生成中文和英文的响应。它能够无缝切换这两种语言，为多语言应用提供灵活的解决方案。\n* **高级语音解码**：基于 CosyVoice，GLM\\-4\\-Voice 解码器能够实现高质量的语音生成，并支持流式传输，在两种语言中保持高清晰度。\n\n## 架构\n\nGLM\\-4\\-Voice 的架构由三个主要组件组成，每个组件在实现端到端语音交互中发挥着至关重要的作用：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*nJsKHtxSblNkixPIBZpWyQ.jpeg)\n\n1. **GLM\\-4\\-Voice\\-Tokenizer**：该组件将连续语音输入标记化为离散标记，每秒大约生成 12.5 个标记。标记器基于 Whisper 的编码器，并添加了向量量化，使模型能够以结构化形式处理音频。\n2. **GLM\\-4\\-Voice\\-9B**：核心语言模型，基于 GLM\\-4 架构，已调整为处理口语输入。它可以处理文本和语音，使其成为强大的多模态对话代理。\n3. **GLM\\-4\\-Voice\\-Decoder**：该解码器将离散标记转换回连续语音，使模型能够生成音频输出。它支持流式推理，使响应能够在处理几个标记后立即开始，从而最小化对话延迟。\n\n这些组件共同使 GLM\\-4\\-Voice 成为实时语音交互的强大工具，支持不同语言和方言的对话 AI。\n\n## 在本地设置 GLM\\-4\\-Voice\n\n要体验 GLM\\-4\\-Voice，请按照以下步骤在您的机器上本地设置该模型。\n\n### 第一步：克隆仓库\n\n首先从 GitHub 克隆仓库。确保包含子模块：\n\n```python\n!git clone --recurse-submodules https://github.com/THUDM/GLM-4-Voice\ncd GLM-4-Voice\n```\n\n### 步骤 2：安装依赖\n\n进入项目目录并安装必要的依赖：\n\n```python\n!pip install -r requirements.txt\n```\n\n### 第3步：下载模型检查点\n\nGLM\\-4\\-Voice的解码器模型托管在Hugging Face上，需要`git-lfs`进行下载。确保已安装`git-lfs`，然后运行：\n\n```python\n!git clone https://huggingface.co/THUDM/glm-4-voice\n```\n\n### 步骤 4：启动模型服务\n\n一切设置完成后，启动模型服务器：\n\n```python\npython model_server.py --model-path glm-4-voice-9b\n```\n\n### 第5步：启动Web服务\n\n一旦模型服务器运行，执行以下命令以启动Web服务：\n\n```python\npython web_demo.py\n```\n\n您现在可以访问Web演示 [http://127\\.0\\.0\\.1:8888](http://127.0.0.1:8888) 与GLM\\-4\\-Voice进行交互。\n\n> **注意：** GLM\\-4\\-Voice模型资源密集，运行有效需要大量计算能力。具体来说，它需要35–40个GPU以实现最佳性能，因此适合在可访问高性能硬件的环境中部署。用户在尝试使用此模型之前，应确保具备必要的基础设施。\n\n## Web Demo Interface\n\nGLM\\-4\\-Voice 的网页演示提供了一个直观的界面，具有多种自定义选项：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*scbHOUXqMW5KGAcT3Bq1Eg.png)\n\n* **输入模式**：用户可以选择以文本或音频形式提供输入。这种灵活性允许无手操作或传统交互。\n* **语音控制参数**：调整温度、top\\-p 和令牌限制，以自定义模型的响应特性。\n* **调试信息**：显示输入和输出令牌，使用户能够洞察模型处理查询的过程。\n* **交互式音频显示**：音频输入和响应以波形形式显示，用户可以重播或查看音频片段以评估质量。\n\n然而，用于在演示中流式传输音频的 Gradio 有时可能会出现不稳定情况。为了获得最佳质量，建议在生成后重播对话框中的音频。\n\n## 结论\n\nGLM\\-4\\-Voice 在对话式人工智能领域中脱颖而出，提供了独特的双语支持、实时音频交互和灵活的响应定制。其端到端设计和低延迟使其成为客户服务、教育、虚拟助手等应用的最佳候选者。凭借易于访问的设置过程，GLM\\-4\\-Voice 为开发者和研究人员探索中文和英文的高级语音能力打开了大门。\n\n随着对更互动和真实的人工智能需求的不断增长，像 GLM\\-4\\-Voice 这样的模型代表了在消除语言和对话障碍方面的重要进展。无论您是想构建聊天机器人、虚拟教师还是客户服务代理，GLM\\-4\\-Voice 都提供了强大而灵活的解决方案。\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/google-releases-gemma-a-lightweight-and-open-source-model-b6411d67ecca","frontmatter":{"title":"Google 发布 Gemma — 轻量级开源模型","meta_title":"Google 发布 Gemma — 轻量级开源模型","description":"Google 发布了 Gemma，这是一系列轻量级开源模型，基于创建 Gemini 的研究和技术构建……","date":"2024-10-29T12:46:34.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*G7XbkhsCwillpje7AvETjQ.jpeg","categories":["Natural Language Processing","Programming","Chatbots"],"author":"Rifx.Online","tags":["Gemma","Gemini","parameters","NLP","chatbots"],"draft":false,"slug":"blog/google-releases-gemma-a-lightweight-and-open-source-model-b6411d67ecca"},"content":"\n\n\n\n\n在短短一周内，世界见证了两家科技巨头带来的最具突破性的AI进展。OpenAI推出了令人惊叹的AI视频生成器[Sora](https://readmedium.com/3d16381f3bf5)，而谷歌则揭晓了其[Gemini 1.5模型](https://generativeai.pub/google-releases-gemini-1-5-with-1m-context-window-44ed4a2ea319)，能够支持最多100万的上下文窗口。\n\n今天，谷歌再次引发轰动，发布了[Gemma](https://ai.google.dev/gemma/?utm_source=keyword&utm_medium=referral&utm_campaign=gemma_cta&utm_content)，这是一个轻量级、最先进的开源模型家族，建立在用于创建Gemini模型的研究和技术基础之上。\n\n## 什么是 Gemma？\n\nGemma 以拉丁语 *gemma* 意为“珍贵的宝石”命名，汲取了其前身 Gemini 的灵感，反映了其在科技领域的价值和稀有性。\n\n它们是文本到文本、仅解码的大型语言模型，提供英语版本，具有开放权重、预训练变体和指令调优变体。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Fu2ryJMunebq5c0dD-opZQ.png)\n\nGemma 从今天起在全球范围内提供，分为两种尺寸（2B 和 7B），支持广泛的工具和系统，并可在开发者的笔记本电脑和工作站上运行。\n\n## 2 模型大小和能力\n\nGemma 模型有 20 亿和 70 亿参数两种规模。2B 模型旨在运行在移动设备和笔记本电脑上，而 7B 模型则适用于桌面计算机和小型服务器。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*sH9jaz1RvtKeJ5yjfyOL5Q.png)\n\n**调优模型**\n\nGemma 还有两个版本：调优版和预训练版。\n\n* **预训练：** 这就像基础模型，没有任何微调。该模型没有针对 Gemma 核心数据训练集以外的特定任务或指令进行训练。\n* **指令调优：** 该模型经过微调，以适应人类语言交互，从而提高其执行特定任务的能力。\n\n## 它与竞争对手的比较？\n\n由于体积小，Gemma能够直接在用户的笔记本电脑上运行。下图显示了Gemma (7B)的语言理解和生成性能与类似规模的开放模型如LLaMA 2 (7B)、LLaMA 2 (13B)和Mistral (7B)的比较。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*QxjZALUAIDiS_T66EpOu-g.png)\n\n您可以在[这里](https://ai.google.dev/gemma/?utm_source=keyword&utm_medium=referral&utm_campaign=gemma_cta&utm_content)查看每个基准的更详细比较。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Fc8Fk0Dgh2VFU_VLhpcs6Q.png)\n\n## 它的用途是什么？\n\n以下是 Gemma 可能的使用场景：\n\n**内容创作与沟通**\n\n* 文本生成\n* 聊天机器人和对话式 AI\n* 文本摘要\n\n**研究与教育**\n\n* **自然语言处理 (NLP) 研究：** 作为 NLP 研究的基础，实验技术，开发算法，并为该领域的进步做出贡献。\n* **语言学习工具：** 支持互动语言学习体验，帮助语法纠正，或提供写作练习。\n* **知识探索：** 帮助研究人员通过生成摘要或回答特定主题的问题来探索大量文本。\n\n以前需要极大模型的任务现在可以通过最先进的小型模型来实现。这开启了开发 AI 应用程序的全新方式，我们很快可能会在智能手机上看到无需互联网连接的设备内 AI 聊天机器人。\n\n这有多令人兴奋呢？\n\n## 这真的好吗？\n\n几位 [redditors](https://www.reddit.com/r/LocalLLaMA/comments/1awbqwd/gemma_7b_the_latest_opensource_model_from_google/) 分享了他们使用 Gemma 的经验，到目前为止，结果并不理想。看看这个例子，Gemma 在回答关于重量的问题时给出了错误的答案。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Sdaiaqcuz7qbftG1)\n\n我自己还没有真正尝试过，但重要的是要记住，像这样的较小模型预计会有一些缺陷，有时可能会给出错误的答案。\n\n## 尝试自己动手\n\n您可以今天开始使用Gemma，通过Kaggle的免费访问、Colab笔记本的免费层以及首次使用Google Cloud的用户可获得的$300信用额度。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BrvLnczy724TPrsk-uFJCw.png)\n\n如果您有兴趣开始使用Gemma，请查看这些指南，以了解从文本生成到在Gemma模式下部署的过程：\n\n* **使用Gemma进行文本生成**：构建一个基本的文本生成示例。\n* **使用LoRA调优Gemma**：对Gemma 2B模型进行LoRA微调。\n* **使用分布式训练调优Gemma模型**：使用Keras和JAX后端对Gemma 7B模型进行LoRA和模型并行的微调。\n* **将Gemma部署到生产环境**：使用Vertex AI将Gemma部署到生产环境。\n\n## 下载模型\n\n开放模型目前可在 [HuggingFace](https://huggingface.co/models?other=gemma&sort=trending&search=google) 上获取。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*mJRzGhO1sUxPL4_3YjpNGA.png)\n\nGemma 模型也可以从 [Kaggle Models](https://www.kaggle.com/models/google/gemma) 下载。\n\n## 最后的思考\n\n虽然Gemma模型可能体积小且缺乏复杂性，但它们在速度和使用成本上可能会有所弥补。\n\n从更大的角度来看，谷歌并不是追逐短期的消费者兴奋，而是在为企业培育市场。他们设想公司会为谷歌云服务付费，因为开发者使用Gemma来创建创新的新消费应用。\n\n此外，尽管Gemini的反响平平，谷歌仍然展示了它还有更多的秘密武器。\n\n当然，对于任何强大的技术来说，真正的考验是它的实际效果。谷歌的过去引发了一个问题：这些模型在现实世界中的表现是否能如承诺的那样出色。密切关注这一点是重要的，但也希望谷歌能从过去中吸取教训，提供真正可比甚至优于竞争对手的模型。\n\n我迫不及待想要体验Gemma，并且我一定会分享我对这个新AI模型的初步想法和发现。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*8BDnUV9iQisOyeN3.png)\n\n这篇文章发布在[Generative AI](https://generativeai.pub/)。请在[LinkedIn](https://www.linkedin.com/company/generative-ai-publication)上与我们联系，并关注[Zeniteq](https://www.zeniteq.com/)，以获取最新的AI故事。让我们一起塑造AI的未来！\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*JeeoUhaBYUJGr0Xq.png)\n\n"},{"lang":"zh","group":"blog","slug":"blog/how-agentic-rag-solves-problem-with-current-rag-limitations-4402ef7f8448","frontmatter":{"title":"Agentic RAG 如何解决当前 RAG 限制的问题","meta_title":"Agentic RAG 如何解决当前 RAG 限制的问题","description":"在《咖啡休息概念》第 4 卷中，我们将了解 AgenticRAG 如何帮助解决传统 RAG 的局限性。","date":"2024-11-04T12:34:57.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*abCDtDjfKZDJzginIc1UPA.png","categories":["Generative AI","Data Science","Machine Learning"],"author":"Rifx.Online","tags":["Agentic","RAG","agents","query","routing"],"draft":false,"slug":"blog/how-agentic-rag-solves-problem-with-current-rag-limitations-4402ef7f8448"},"content":"\n在本卷咖啡休息概念的第 4 期中，我们将了解 AgenticRAG 如何帮助解决传统 RAG 的限制。\n\n## RAG框架\n\nRAG（检索增强生成）框架按特定顺序操作：\n\n文档 \\-\\> 片段 \\-\\> 向量数据库 \\-\\> 片段检索（前K个） \\-\\> LLM\n\n然而，这一顺序**在处理某些类型的查询时会遇到障碍。**\n\n\n\n## 问题 1：摘要\n\n考虑一个查询，比如“总结文档”。\n\n* 传统的 RAG 方法检索前 K 个块并进行摘要。\n* 但如果检索文档的所有块并进行总结，岂不是更全面吗？\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*gIb0RNALIItt4UmyVfPRZg.png)\n\n## 问题 2：比较文档\n\n* 在比较文档 A 和文档 B 时，**基本 RAG 检索随机片段并尝试比较这些前 K 个片段**。\n* 这**并不能准确反映**文档的整体情况。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*pJuKlKx1unDAvKmmp_1Rlg.png)\n\n## 问题 3：结构化数据分析\n\n考虑一个问题：“**下一个休假是什么时候？**”。\n\n* 第一步是从结构化表中检索员工所属的区域。\n* 根据该区域，从休假政策文件中提取该区域的下一个休假。\n* 在当前的 RAG 框架下，这个过程并不是那么简单。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*XZuMz9EXtb_m28l4Ox27lQ.png)\n\n## 问题 4：多部分问题\n\n考虑一个问题，例如“**识别所有地区的共同请假？**”。\n\n* 想象一下，您有一份在 120 个国家运营的公司的请假政策文件。\n* 由于您正在传递前 K 个上下文，**可以比较的最大地区数量限制为 K**，其中 K 是传递给 LLM 的块的数量。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*l0FY6rI_UK9k9TW-nEJO7w.png)\n\n查看我们的 **AgenticRAG with LlamaIndex** 课程，包含 **5 个实时案例研究**。\n\n课程链接：[https://www.masteringllm.com/course/agentic\\-retrieval\\-augmented\\-generation\\-agenticrag](https://www.masteringllm.com/course/agentic-retrieval-augmented-generation-agenticrag)\n\n## Agentic RAG\n\nAgentic RAG 可以通过自定义代理来解决这 4 个问题。\n\n* 代理将与多个系统进行交互。\n* RAG 现在是代理可以使用的系统的一部分。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Su8LiYNG4lv4jvuCQAhYdg.png)\n\n* 代理使用 LLMs 来自动化推理和工具选择\n* RAG 只是代理可能决定使用的另一个工具。\n\n## 路由代理\n\n* 路由代理是简单的代理，用于路由查询。\n* 一个代理可以在一个或多个工具中路由查询。\n* 请记住我们的问题“**总结文档**”或如果我们想结合“**总结 \\+ 语义搜索**”的问题，可以使用以下示例路由来解决。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*43Y9jlYoXDb0BbUoYCcKrg.png)\n\n## 查询规划代理\n\n* 查询规划代理将查询分解为子查询。\n* 每个子查询都可以在 RAG 管道上执行。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*32Ng2zpxNWXhQZ3CaLcFeA.png)\n\n## 代理的工具\n\n* LLMs 可以拥有多个工具，例如调用 API，推断 API 的参数。\n* RAG 现在是 LLM 可能使用的一个工具。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Z1viCXkfah_5JJM2Ty6Kjw.png)\n\n## 摘要\n\n* RAG 在处理复杂问题时存在局限性。\n* 一些用例，如总结、比较等，仅靠 RAG 无法解决。\n* Agentic RAG 可以帮助克服 RAG 的局限性。\n* Agentic RAG 将 RAG 视为可用于语义搜索的工具。\n* 配备路由、查询规划和工具的代理能够超越传统的 RAG 应用。\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/how-to-create-an-ai-team-to-write-compelling-stories-with-crewai-and-gemini-pro-3713f53c72c4","frontmatter":{"title":"如何使用 CrewAI 和 Gemini Pro 创建 AI 团队来撰写引人入胜的故事","meta_title":"如何使用 CrewAI 和 Gemini Pro 创建 AI 团队来撰写引人入胜的故事","description":"您是否对人工智能生成引人入胜的故事的想法着迷？如果是这样，您并不孤单！在本文中，我们将深入探讨……","date":"2024-10-31T23:04:49.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*tSnoOxxIGtrwdUT8","categories":["Programming","Natural Language Processing","Generative AI"],"author":"Rifx.Online","tags":["CrewAI","Gemini","screenwriters","critics","storytelling"],"draft":false,"slug":"blog/how-to-create-an-ai-team-to-write-compelling-stories-with-crewai-and-gemini-pro-3713f53c72c4"},"content":"\n\n\n你是否对AI生成引人入胜的故事这一想法感到着迷？如果是这样，你并不孤单！在本文中，我们将深入探讨一个结合CrewAI和Gemini Pro力量的入门项目，创建一个代理网络，通过用户输入的帮助来创作短篇故事。无论你是一个初出茅庐的程序员，一个希望探索数字前沿的讲故事者，还是仅仅对人工智能的潜力感到好奇，这本指南都适合你。\n\n## CrewAI 和 Gemini Pro 是什么？\n\n在我们深入探讨构建 AI 讲故事者的细节之前，先来澄清一下 CrewAI 和 Gemini Pro 的概念。\n\n**CrewAI** 是一个引人入胜的框架，旨在协调多个 AI 代理，每个代理都有其独特的技能和职责，以协作完成复杂任务。可以把它想象成一个导演管理一组演员，每个演员扮演特定角色来让故事生动起来。在我们项目的背景下，CrewAI 使我们能够创建一支由专业代理（如编剧、评论家和故事大师）组成的团队，共同撰写故事。\n\n**Gemini Pro**，另一方面，是由 Google 开发的最先进的语言模型。它以理解和生成类人文本的能力而闻名，使其成为故事创作等创意任务的理想选择。通过利用 Gemini Pro，我们可以确保我们的代理具备生成引人入胜的叙事内容的坚实基础。\n\n## 为什么这种结构很重要？\n\nCrewAI 和 Gemini Pro 的结合使得故事生成能够采用高度协作和专业化的方法。这个结构允许：\n\n1. **专业化**：每个代理可以专注于它最擅长的领域，无论是撰写对话、确保一致性还是监督项目。\n2. **协作**：代理可以共同工作，结合各自的优势，产生一个超越其部分总和的故事。\n3. **灵活性**：该设置具有高度适应性，可以根据用户输入或创意方向强调或改变不同的故事元素。\n\n## 设置环境\n\n首先，我们需要一些库来使用。您可以通过 pip 加载这些库：\n\n```python\npip install crewai\n```\n\n```python\npip install langchain-google-genai\n```\n\n加载必要的库后，我们可以开始编码。我们将首先导入所需的模块并初始化我们的 Gemini pro API 连接。\n\n如您所见，我们需要一个 Gemini 模型的 API 密钥。您可以在 Google AI Studio 中[免费](https://ai.google.dev/)创建此密钥。之后，您可以将此密钥复制到 google\\_api\\_key 变量中，或者通过在命令行中运行以下命令将其加载到环境中：\n\n```python\nexport GOOGLE_API_KEY=YOUR_KEY\n```\n\n将您从 Google AI Studio 获取的 API 密钥替换为 YOUR\\_KEY。\n\n接下来，我们定义我们的代理：编剧、评论家和故事大师。每个代理都有一个角色、目标和背景故事，以指导其在故事生成过程中的贡献。\n\n例如，编剧专注于将创意转化为引人入胜的场景，而评论家确保一致性和遵循类型。\n\n这些代理将共同工作，创造一个引人入胜的故事。故事大师将接受任务，然后在其他代理之间委派和协调任务。我们通过将 allow\\_delegation 参数设置为 True 来允许这种行为。\n\n准备好代理后，我们提示用户提供一个故事创意。然后，这个输入用于创建一个任务，概述故事应包含的内容，引导代理进行创作过程。\n\n在创建任务时，我们将任务提交给故事大师，因为它将协调我们的故事创作过程。\n\n最后，我们应该将这些代理组合成一个团队并运行我们的任务。\n\n就这样。当我们运行这段代码时，它会提示用户提供一个故事创意，然后通过代理合作写一个短故事。当然，在 CrewAI 框架中还有更多内容，例如工具使用、层次处理、与 ollama 一起完全本地运行不同代理等，但这些主题是另一个文章的内容。\n\n您可以在这里找到完整的代码以直接运行：\n\n您可以将此代码用作此类应用程序的模板，您可以构建游戏构建者团队、股票分析师团队、营销团队等。凭借想象力，天空才是极限。如果您喜欢这篇文章并对更高级的实现感到兴奋，可以访问 CrewAI [网站](https://www.crewai.com/)。\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/how-to-improve-llms-with-rag-abdc132f76ac","frontmatter":{"title":"如何使用 RAG 提高 LLM 成绩","meta_title":"如何使用 RAG 提高 LLM 成绩","description":"适合初学者的 Python 代码介绍","date":"2024-11-04T12:31:55.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*N0Ad_oCIrAyzMYRdH3trqg.png","categories":["Natural Language Processing","Programming","Generative AI"],"author":"Rifx.Online","tags":["RAG","retrievers","LlamaIndex","knowledge","bases"],"draft":false,"slug":"blog/how-to-improve-llms-with-rag-abdc132f76ac"},"content":"\n\n\n### 初学者友好的介绍 w/ Python 代码\n\n本文是关于在实践中使用大型语言模型的[更大系列](https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c)的一部分。在[上一篇文章](https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32)中，我们使用 QLoRA 对 Mistral-7b-Instruct 进行了微调，以回应 YouTube 评论。尽管微调后的模型在回应观众反馈时成功捕捉了我的风格，但它对技术问题的回答与我的解释并不匹配。在这里，我将讨论如何通过检索增强生成（即 RAG）来提高 LLM 的性能。\n\n\n\n大型语言模型（LLMs）在响应用户查询时展示了存储和部署大量知识的惊人能力。虽然这使得像 ChatGPT 这样的强大 AI 系统得以创建，但以这种方式压缩世界知识有**两个关键限制**。\n\n**首先**，LLM 的知识是静态的，即不会随着新信息的出现而更新。**其次**，LLM 可能对其训练数据中不显著的利基和专业信息缺乏足够的“理解”。这些限制可能导致模型对用户查询的回答不理想（甚至是虚构的）。\n\n我们可以通过**通过专业和可变的知识库增强模型**来缓解这些限制，例如客户常见问题解答、软件文档或产品目录。这使得创建更强大和适应性更强的 AI 系统成为可能。\n\n**检索增强生成**，或称 **RAG**，就是这样一种方法。在这里，我提供 RAG 的高级介绍，并分享使用 LlamaIndex 实现 RAG 系统的示例 Python 代码。\n\n## 什么是 RAG？\n\nLLM 的基本用法是给它一个提示并获取响应。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*sM1p-3FoTaGZunqx918G9A.png)\n\n**RAG 通过在这个基本过程中添加一个步骤来工作**。具体来说，执行一个检索步骤，根据用户的提示，从外部知识库中提取相关信息，并在传递给 LLM 之前将其注入到提示中。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*EhJZj1blu7a8EPmVAPsNcA.png)\n\n## 我们为什么关心\n\n请注意，RAG 并没有从根本上改变我们使用 LLM 的方式；它仍然是 *提示输入和响应输出*。RAG 只是增强了这个过程（因此得名）。\n\n这使得 **RAG 成为一种灵活且（相对）简单的方式来改善基于 LLM 的系统**。此外，由于知识存储在外部数据库中，更新系统知识就像从表中添加或删除记录一样简单。\n\n### 为什么不进行微调？\n\n本系列之前的文章讨论了[微调](https://towardsdatascience.com/fine-tuning-large-language-models-llms-23473d763b91)，即为特定用例调整现有模型。虽然这是一种赋予LLM专业知识的替代方法，但从经验来看，**微调似乎在这方面的效果不如RAG** \\[1]。\n\n## 它是如何工作的\n\nRAG 系统有两个关键要素：**检索器**和 **知识库**。\n\n### Retriever\n\n检索器接收用户提示并从知识库中返回相关项目。这通常使用所谓的 **文本嵌入**，即文本在概念空间中的数值表示。换句话说，这些是 **表示给定文本的 *含义* 的数字**。\n\n文本嵌入可以用来计算用户查询与知识库中每个项目之间的相似性得分。这个过程的结果是 **每个项目与输入查询相关性的排名**。\n\n然后，检索器可以选择前 k 个（例如 k=3）最相关的项目，并将它们注入到用户提示中。这个增强的提示随后被传递给 LLM 进行生成。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*jpTwdBmoTlJlfPAm0oJiVQ.png)\n\n### 知识库\n\nRAG 系统的下一个关键要素是知识库。这个 **包含了您希望提供给 LLM 的所有信息**。虽然有无数种方法可以构建 RAG 的知识库，但在这里我将重点介绍如何从一组文档中构建一个知识库。\n\n这个过程可以分为 **4 个关键步骤** \\[2,3].\n\n1. **加载文档** — 这包括收集一组文档并确保它们处于可解析的格式（稍后会详细介绍）。\n2. **分块文档—**由于 LLM 的上下文窗口有限，文档必须被拆分成更小的块 **（例如，** 256 或 512 个字符长）。\n3. **嵌入块** — 使用文本嵌入模型将每个块转换为数字。\n4. **加载到向量数据库**— 将文本嵌入加载到数据库（即向量数据库）中。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*VWG6Tr0OxCnD5Mvygm5DCA.png)\n\n## 一些细微差别\n\n虽然构建 RAG 系统的步骤在概念上很简单，但一些细微差别可能使得在现实世界中构建一个系统变得更加复杂。\n\n**文档准备**—RAG 系统的质量取决于从源文档中提取有用信息的能力。例如，如果一个文档格式混乱，充满了图像和表格，那么解析起来会比一个格式良好的文本文件更困难。\n\n**选择合适的块大小**—我们已经提到由于 LLM 上下文窗口的需要进行分块。然而，还有 2 个额外的分块动机。\n\n**首先**，它可以降低（计算）成本。你在提示中注入的文本越多，生成完成所需的计算就越多。**第二**是性能。特定查询的相关信息往往集中在源文档中（通常仅一句话就可以回答一个问题）。分块有助于最小化传递给模型的无关信息的数量 \\[4\\]。\n\n**改善搜索** — 虽然文本嵌入提供了一种强大且快速的搜索方式，但它并不总是能如人所愿地工作。换句话说，它可能返回与用户查询“相似”的结果，但对回答问题并没有帮助，例如，“*洛杉矶的天气怎么样？*”可能返回“*纽约的天气怎么样？*”。\n\n缓解这一问题的最简单方法是通过良好的文档准备和分块。然而，对于某些用例，可能需要额外的策略来改善搜索，例如为每个块使用 **元标签**、采用结合关键词和嵌入搜索的 **混合搜索**，或使用 **重排序器**，这是一种专门计算两段文本相似性的模型。\n\n## 示例代码：使用 RAG 改进 YouTube 评论响应器\n\n在对 RAG 工作原理有基本了解后，让我们看看如何在实践中使用它。我将基于 [上一篇文章](https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32) 中的示例，在其中我使用 QLoRA 对 Mistral-7B-Instruct 进行了微调，以响应 YouTube 评论。我们将使用 LlamaIndex 为之前微调的模型添加 RAG 系统。\n\n示例代码可在 [Colab Notebook](https://colab.research.google.com/drive/1peJukr-9E1zCo1iAalbgDPJmNMydvQms?usp=sharing) 中免费获得，该 Notebook 可以在提供的（免费）T4 GPU 上运行。此示例的源文件可在 [GitHub 仓库](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/rag) 中找到。\n\n🔗 [Google Colab](https://colab.research.google.com/drive/1peJukr-9E1zCo1iAalbgDPJmNMydvQms?usp=sharing) \\| [GitHub Repo](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/rag)\n\n### 导入\n\n我们首先安装并导入必要的 Python 库。\n\n```python\n!pip install llama-index\n!pip install llama-index-embeddings-huggingface\n!pip install peft\n!pip install auto-gptq\n!pip install optimum\n!pip install bitsandbytes\n## 如果不是在 Colab 上运行，请确保也安装 transformers\n```\n\n```python\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\nfrom llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex\nfrom llama_index.core.retrievers import VectorIndexRetriever\nfrom llama_index.core.query_engine import RetrieverQueryEngine\nfrom llama_index.core.postprocessor import SimilarityPostprocessor\n```\n\n### 设置知识库\n\n我们可以通过定义我们的嵌入模型、块大小和块重叠来配置我们的知识库。在这里，我们使用来自BAAI的\\~33M参数[bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5)嵌入模型，该模型可在Hugging Face hub上获取。其他嵌入模型选项可以在这个[text embedding leaderboard](https://huggingface.co/spaces/mteb/leaderboard)上找到。\n\n```python\n## import any embedding model on HF hub\nSettings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n\nSettings.llm = None # we won't use LlamaIndex to set up LLM\nSettings.chunk_size = 256\nSettings.chunk_overlap = 25\n```\n接下来，我们加载源文档。在这里，我有一个名为“[*articles*](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/rag/articles)”的文件夹，其中包含我在[fat tails](https://towardsdatascience.com/pareto-power-laws-and-fat-tails-0355a187ee6a)上写的3篇Medium文章的PDF版本。如果在Colab中运行，您必须从[GitHub repo](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/rag)下载文章文件夹并手动上传到您的Colab环境。\n\n对于该文件夹中的每个文件，下面的函数将从PDF中读取文本，将其拆分成块（基于之前定义的设置），并将每个块存储在名为*documents*的列表中。\n\n```python\ndocuments = SimpleDirectoryReader(\"articles\").load_data()\n```\n由于这些博客是直接从Medium下载为PDF的，因此它们更像是网页，而不是格式良好的文章。因此，一些块可能包含与文章无关的文本，例如网页标题和Medium文章推荐。\n\n在下面的代码块中，我对documents中的块进行精炼，删除文章主体前后的大部分块。\n\n```python\nprint(len(documents)) # prints: 71\nfor doc in documents:\n    if \"Member-only story\" in doc.text:\n        documents.remove(doc)\n        continue\n\n    if \"The Data Entrepreneurs\" in doc.text:\n        documents.remove(doc)\n\n    if \" min read\" in doc.text:\n        documents.remove(doc)\n\nprint(len(documents)) # prints: 61\n```\n最后，我们可以将精炼后的块存储在向量数据库中。\n\n```python\nindex = VectorStoreIndex.from_documents(documents)\n```\n\n### 设置检索器\n\n在我们的知识库建立之后，我们可以使用 LlamaIndex 的 *VectorIndexRetriever()* 创建一个检索器，它返回与用户查询最相似的 3 个块。\n\n```python\n## set number of docs to retreive\ntop_k = 3\n\n## configure retriever\nretriever = VectorIndexRetriever(\n    index=index,\n    similarity_top_k=top_k,\n)\n```\n接下来，我们定义一个查询引擎，使用检索器和查询返回一组相关的块。\n\n```python\n## assemble query engine\nquery_engine = RetrieverQueryEngine(\n    retriever=retriever,\n    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.5)],\n)\n```\n\n### 使用查询引擎\n\n现在，随着我们的知识库和检索系统的建立，让我们使用它来返回与查询相关的内容。在这里，我们将传递我们向ShawGPT（YouTube评论回复者）提出的相同技术问题，来自[上一篇文章](https://readmedium.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32)。\n\n```python\nquery = \"What is fat-tailedness?\"\nresponse = query_engine.query(query)\n```\n查询引擎返回一个响应对象，其中包含文本、元数据和相关块的索引。下面的代码块返回该信息的更易读版本。\n\n```python\n## reformat response\ncontext = \"Context:\\n\"\nfor i in range(top_k):\n    context = context + response.source_nodes[i].text + \"\\n\\n\"\n\nprint(context)\n```\n\n```python\nContext:\nSome of the controversy might be explained by the observation that log-\nnormal distributions behave like Gaussian for low sigma and like Power Law\nat high sigma [2].\nHowever, to avoid controversy, we can depart (for now) from whether some\ngiven data fits a Power Law or not and focus instead on fat tails.\nFat-tailedness — measuring the space between Mediocristan\nand Extremistan\nFat Tails are a more general idea than Pareto and Power Law distributions.\nOne way we can think about it is that “fat-tailedness” is the degree to which\nrare events drive the aggregate statistics of a distribution. From this point of\nview, fat-tailedness lives on a spectrum from not fat-tailed (i.e. a Gaussian) to\nvery fat-tailed (i.e. Pareto 80 – 20).\nThis maps directly to the idea of Mediocristan vs Extremistan discussed\nearlier. The image below visualizes different distributions across this\nconceptual landscape [2].\n\nprint(\"mean kappa_1n = \" + str(np.mean(kappa_dict[filename])))\n    print(\"\")\nMean κ (1,100) values from 1000 runs for each dataset. Image by author.\nThese more stable results indicate Medium followers are the most fat-tailed,\nfollowed by LinkedIn Impressions and YouTube earnings.\nNote: One can compare these values to Table III in ref [3] to better understand each\nκ value. Namely, these values are comparable to a Pareto distribution with α\nbetween 2 and 3.\nAlthough each heuristic told a slightly different story, all signs point toward\nMedium followers gained being the most fat-tailed of the 3 datasets.\nConclusion\nWhile binary labeling data as fat-tailed (or not) may be tempting, fat-\ntailedness lives on a spectrum. Here, we broke down 4 heuristics for\nquantifying how fat-tailed data are.\n\nPareto, Power Laws, and Fat Tails\nWhat they don’t teach you in statistics\ntowardsdatascience.com\nAlthough Pareto (and more generally power law) distributions give us a\nsalient example of fat tails, this is a more general notion that lives on a\nspectrum ranging from thin-tailed (i.e. a Gaussian) to very fat-tailed (i.e.\nPareto 80 – 20).\nThe spectrum of Fat-tailedness. Image by author.\nThis view of fat-tailedness provides us with a more flexible and precise way of\ncategorizing data than simply labeling it as a Power Law (or not). However,\nthis begs the question: how do we define fat-tailedness?\n4 Ways to Quantify Fat Tails\n```\n\n### 将 RAG 添加到 LLM\n\n我们首先从 Hugging Face hub 下载 [微调模型](https://readmedium.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32)。\n\n```python\n## load fine-tuned model from hub\nfrom peft import PeftModel, PeftConfig\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name,\n                                             device_map=\"auto\",\n                                             trust_remote_code=False,\n                                             revision=\"main\")\n\nconfig = PeftConfig.from_pretrained(\"shawhin/shawgpt-ft\")\nmodel = PeftModel.from_pretrained(model, \"shawhin/shawgpt-ft\")\n\n## load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n```\n作为基线，我们可以观察模型在没有任何文章上下文的情况下如何回应技术问题。为此，我们使用 lambda 函数创建一个提示模板，该函数接受观众评论并返回 LLM 的提示。有关此提示来源的更多详细信息，请参见本系列的 [上一篇文章](https://towardsdatascience.com/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32#5aad)。\n\n```python\n## prompt (no context)\nintstructions_string = f\"\"\"ShawGPT, functioning as a virtual data science \\\nconsultant on YouTube, communicates in clear, accessible language, escalating \\\nto technical depth upon request. It reacts to feedback aptly and ends \\\nresponses with its signature '–ShawGPT'.\n\nShawGPT will tailor the length of its responses to match the viewer's comment, \\\nproviding concise acknowledgments to brief expressions of gratitude or \\\nfeedback, thus keeping the interaction natural and engaging.\n\nPlease respond to the following comment.\n\"\"\"\n\nprompt_template = lambda comment: f'''[INST] {intstructions_string} \\n{comment} \\n[/INST]'''\ncomment = \"What is fat-tailedness?\"\n\nprompt = prompt_template(comment)\nprint(prompt)\n```\n\n```python\n[INST] ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. It reacts to feedback aptly and ends responses with its signature '–ShawGPT'. ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, thus keeping the interaction natural and engaging.\n\nPlease respond to the following comment.\n \nWhat is fat-tailedness? \n[/INST]\n```\n接下来，我们可以使用以下代码将此提示传递给模型。\n\n```python\nmodel.eval()\n\ninputs = tokenizer(prompt, return_tensors=\"pt\")\noutputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), \n                          max_new_tokens=280)\n\nprint(tokenizer.batch_decode(outputs)[0])\n```\n这是模型的响应（没有上下文）。\n\n```python\nGreat question!\n\nFat-tailedness is a statistical property of a distribution. In simple terms, \nit refers to the presence of extreme outliers or heavy tails in the \ndistribution.\n\nFor instance, consider the distribution of heights in a population. A normal \ndistribution would have most people clustered around an average height with a \nfew people deviating slightly from the mean. However, in a fat-tailed \ndistribution, you would observe a larger number of people being significantly \ntaller or shorter than the average.\n\nIn finance, fat-tailedness is often used to describe the presence of extreme \nmarket moves or outliers in financial returns. It's an important concept in \nrisk management and financial modeling.\n\nI hope this explanation helps! Let me know if you have any questions.\n–ShawGPT\n```\n虽然响应的风格和格式都很好，但模型对 fat-tailedness 的解释与我在我的 [视频](https://www.youtube.com/playlist?list=PLz-ep5RbHosVrT89BRNX-IGKHYxvx9wqi) 和 [博客系列](https://towardsdatascience.com/pareto-power-laws-and-fat-tails-0355a187ee6a) 中定义的有所不同。\n\n让我们看看当我们包含适当的上下文时模型的响应会发生什么。为此，我们创建另一个提示模板，该模板也可以从 RAG 系统中获取上下文。\n\n```python\n## prompt (with context)\nprompt_template_w_context = lambda context, comment: f\"\"\"[INST]ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. \\\nIt reacts to feedback aptly and ends responses with its signature '–ShawGPT'. \\\nShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, \\\nthus keeping the interaction natural and engaging.\n\n{context}\nPlease respond to the following comment. Use the context above if it is helpful.\n\n{comment}\n[/INST]\n\"\"\"\n```\n接下来，我们将查询引擎中的上下文和观众评论传递给模型。\n\n```python\nprompt = prompt_template_w_context(context, comment)\n\ninputs = tokenizer(prompt, return_tensors=\"pt\")\noutputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=280)\n\nprint(tokenizer.batch_decode(outputs)[0])\n```\n这是新的响应（带上下文）。\n\n```python\nGreat question!\n\nFat-tailedness is a property of a distribution that describes how much the \npresence of extreme values (rare events) affects the overall statistics of the \ndistribution.\n\nIn the context of the video, the author discusses how Mediocristan and \nExtremistan distributions differ in terms of fat-tailedness. Mediocristan \ndistributions have a relatively small impact from extreme values, while \nExtremistan distributions are heavily influenced by extreme values.\n\nThe author also mentions that fat-tailedness is a more general concept than \nspecific distributions like Pareto or Power Laws. Instead, it lives on a \nspectrum ranging from thin-tailed (Gaussian) to very fat-tailed (Pareto 80-20).\n\nI hope that helps clarify things a bit! Let me know if you have any questions.\n–ShawGPT\n```\n这比没有上下文的响应更好地捕捉了我对 fat tails 的解释，并且甚至提到了 Mediocristan 和 Extremistan 的细分概念。\n\n## 接下来是什么？\n\n在这里，我为初学者提供了一个关于 RAG 的友好介绍，并分享了如何使用 LlamaIndex 实现它的具体示例。RAG 使我们能够通过可更新和特定领域的知识来改善 LLM 系统。\n\n虽然最近的 AI 热潮主要集中在构建 AI 助手上，但一个强大的（但不那么流行的）创新来自于文本嵌入（即我们用来进行检索的东西）。在本系列的下一篇文章中，我将更详细地探讨 **文本嵌入**，包括它们如何用于 **语义搜索** 和 **分类任务**。\n\n**更多关于 LLM 的内容 👇**\n\n## 资源\n\n**连接**: [我的网站](https://shawhintalebi.com/) \\| [预约电话](https://calendly.com/shawhintalebi)\n\n**社交**: [YouTube 🎥](https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA) \\| [LinkedIn](https://www.linkedin.com/in/shawhintalebi/) \\| [Instagram](https://www.instagram.com/shawhintalebi)\n\n**支持**: [请我喝杯咖啡](https://www.buymeacoffee.com/shawhint) ☕️\n\n\\[1] [RAG \\> FT (经验性)](https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb)\n\n\\[2] [LlamaIndex 网络研讨会：为生产构建 LLM 应用程序，第一部分（与 Anyscale 联合主持）](https://www.youtube.com/watch?v=efbn-3tPI_M)\n\n\\[3] [LlamaIndex 文档](https://docs.llamaindex.ai/en/stable/understanding/loading/loading.html)\n\n\\[4] [LlamaIndex 网络研讨会：使 RAG 准备好生产](https://www.youtube.com/watch?v=Zj5RCweUHIk&list=WL&index=4)\n\n"},{"lang":"zh","group":"blog","slug":"blog/how-to-run-nvidia-llama-3-1-nemotron-70b-instruct-locally-a58ad283aaff","frontmatter":{"title":"如何在本地运行 Nvidia 的 llama-3.1-nemotron-70b-instruct","meta_title":"如何在本地运行 Nvidia 的 llama-3.1-nemotron-70b-instruct","description":"在本地运行大型语言模型 (LLM) 在开发人员、研究人员和 AI 爱好者中越来越受欢迎。其中之一就是……","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*fqVKJkw5sQvLtIsyCcengQ.png","categories":["Programming","Technology","Science"],"author":"Rifx.Online","tags":["Nvidia","llama","Ollama","llama.cpp","Transformers"],"draft":false,"slug":"blog/how-to-run-nvidia-llama-3-1-nemotron-70b-instruct-locally-a58ad283aaff"},"content":"\n\n\n在开发者、研究人员和 AI 爱好者中，本地运行大型语言模型（LLMs）变得越来越受欢迎。其中一个引起广泛关注的模型是 llama-3.1-nemotron-70b-instruct，这是 NVIDIA 定制的强大 LLM，旨在增强生成响应的有用性。在本综合指南中，我们将探讨多种方法，以便在您的本地机器上运行此模型，首先介绍用户友好的 Ollama 平台。\n\n> 在开始之前，如果您正在寻找一个一体化的 AI 平台，以便在一个地方管理所有 AI 订阅，包括所有 LLM（如 GPT-o1、Llama 3.1、Claude 3.5 Sonnet、Google Gemini、未审查的 LLM）和图像生成模型（FLUX、Stable Diffusion 等），请使用 Anakin AI 来管理它们！\n\n\n\n## 方法 1：使用 Ollama 本地运行 llama-3.1-nemotron-70b-instruct\n\nOllama 是一个出色的工具，用于本地运行 LLM，提供简单的设置过程并支持多种模型，包括 llama-3.1-nemotron-70b-instruct。\n\n### 安装\n\n1. 访问官方 Ollama 网站 ([https://ollama.ai](https://ollama.ai/))，下载适合您操作系统的版本。\n2. 通过在终端中运行以下命令来安装 Ollama：\n\n\n```python\ncurl https://ollama.ai/install.sh | sh\n```\n\n### 运行 llama-3.1-nemotron\n\n安装 Ollama 后，您可以通过一个简单的命令轻松运行 llama-3.1-nemotron-70b-instruct 模型：\n\n```python\nollama run nemotron:70b-instruct-q5_K_M\n```\n该命令将在您的系统上下载模型（如果尚未存在），并启动一个交互式会话。\n\n### 使用模型\n\n在模型加载后，您可以通过输入提示开始与其互动。例如：\n\n```python\n>>> What are the key features of llama-3.1-nemotron-70b-instruct?\n\nLlama-3.1-Nemotron-70B-Instruct is a large language model with several key features:\n1. Customized by NVIDIA: The model has been fine-tuned by NVIDIA to improve the helpfulness and quality of its responses.\n2. Based on Llama 3.1: It builds upon the Llama 3.1 architecture, which is known for its strong performance across various tasks.\n3. 70 billion parameters: This large parameter count allows for complex reasoning and a wide range of capabilities.\n4. Instruct-tuned: The model is specifically designed to follow instructions and generate helpful responses to user queries.\n5. RLHF training: It has been trained using Reinforcement Learning from Human Feedback, specifically the REINFORCE algorithm.\n6. Specialized reward model: The training process utilized Llama-3.1-Nemotron-70B-Reward for optimization.\n7. HelpSteer2-Preference prompts: These were used during the training process to further improve the model's helpfulness.\n8. Extended context length: Like other Llama 3.1 models, it likely supports a longer context window of 128K tokens.\n9. Multilingual capabilities: It can understand and generate text in multiple languages.\n10. Strong reasoning abilities: The model excels in tasks requiring complex reasoning and problem-solving.\nThese features make llama-3.1-nemotron-70b-instruct a powerful and versatile language model suitable for a wide range of applications, from general conversation to specialized tasks in various domains.\n```\n对于更高级的用例，您可以使用像 Langchain 这样的库将 Ollama 与 Python 集成。以下是一个简单的示例：\n\n```python\npython\n\nfrom langchain.llms import Ollama\n\nollama = Ollama(base_url=\"http://localhost:11434\", model=\"nemotron:70b-instruct-q5_K_M\")\nresponse = ollama.generate(\"Explain the concept of quantum entanglement.\")\nprint(response)\n```\n这使您能够无缝地将模型集成到您的 Python 项目和应用程序中。\n\n## 方法 2：使用 llama.cpp\n\nllama.cpp 是一个流行的 C++ 实现的 Llama 模型推理，针对 CPU 使用进行了优化。虽然它可能需要比 Ollama 更多的设置，但它提供了更大的灵活性和对模型参数的控制。\n\n### 安装\n\n1. 克隆 llama.cpp 仓库：\n\n```python\ngit clone https://github.com/ggerganov/llama.cpp.git\ncd llama.cpp\n```\n1. 构建项目：\n\n```python\nmake\n```\n\n### 下载模型\n\n要运行 llama-3.1-nemotron-70b-instruct，您需要下载模型权重。这些通常以 GGML 或 GGUF 格式提供。您可以在 Hugging Face 等平台上找到预先转换的模型。\n\n```python\nmkdir models\ncd models\nwget https://huggingface.co/TheBloke/Llama-3.1-Nemotron-70B-Instruct-GGUF/resolve/main/llama-3.1-nemotron-70b-instruct.Q4_K_M.gguf\n```\n\n### 运行模型\n\n一旦你拥有模型文件，就可以使用以下命令运行它：\n\n```python\n./main -m models/llama-3.1-nemotron-70b-instruct.Q4_K_M.gguf -n 1024 -p \"Hello, how are you today?\"\n```\n该命令加载模型并生成对给定提示的响应。你可以调整各种参数，比如生成的令牌数量 (-n) 或温度以控制随机性。\n\n## 方法 3：使用 Hugging Face Transformers\n\nHugging Face 的 Transformers 库提供了一个高层次的 API，用于处理各种语言模型，包括 llama-3.1-nemotron-70b-instruct。\n\n**安装**\n\n首先，安装必要的库：\n\n\n```python\npip install transformers torch accelerate\n```\n**运行模型**\n\n以下是一个加载和使用模型的 Python 脚本：\n\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nmodel_name = \"meta-llama/Llama-3.1-Nemotron-70b-instruct\"\n## Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n## Prepare the input\nprompt = \"Explain the concept of quantum computing in simple terms.\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n## Generate the response\nwith torch.no_grad():\n    outputs = model.generate(**inputs, max_new_tokens=100)\n## Decode and print the response\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response)\n```\n这种方法允许对模型的行为进行更细粒度的控制，并与其他 Hugging Face 工具和管道集成。\n\n## 结论\n\n在本地运行 llama-3.1-nemotron-70b-instruct 为开发者和研究人员打开了无限可能。无论您选择 Ollama 的简单性、llama.cpp 的灵活性，还是 Hugging Face Transformers 的集成功能，您现在都有工具可以在自己的硬件上利用这一先进语言模型的强大能力。在探索 llama-3.1-nemotron-70b-instruct 的能力时，请记住在性能与资源限制之间取得平衡，并始终考虑您应用的伦理影响。负责任的使用，这个模型可以成为推动自然语言处理和 AI 驱动应用可能性的宝贵资产。\n\n"},{"lang":"zh","group":"blog","slug":"blog/introduction-to-llava-a-multimodal-ai-model-2a2fa530ace4","frontmatter":{"title":"LLaVA 简介：一种多模式 AI 模型","meta_title":"LLaVA 简介：一种多模式 AI 模型","description":"LLaVA 是一个端到端训练的大型多模式模型，旨在根据视觉输入理解和生成内容……","date":"2024-10-29T12:48:10.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*0At7tXF5ejho9Y46E3uGtg.png","categories":["Natural Language Processing","Computer Vision","Generative AI"],"author":"Rifx.Online","tags":["LLaVA","GPT-4","multimodal","visual","encoder"],"draft":false,"slug":"blog/introduction-to-llava-a-multimodal-ai-model-2a2fa530ace4"},"content":"\n\n\n\n\nLLaVA是一个端到端训练的大型多模态模型，旨在理解和生成基于视觉输入（图像）和文本指令的内容。它结合了视觉编码器和语言模型的能力，以处理和响应多模态输入。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*mjzqL0BHzdPoN-Jjruh52A.png)\n\n## LLaVA 的输入和输出：连接视觉与文本领域：\n\nLLaVA 的输入有两个方面：\n\n1. 视觉输入：模型可以查看和分析的图像，以提取视觉特征和上下文信息。\n2. 文本指令：文本输入，可以是问题或命令，指导模型关注什么或执行与视觉输入相关的什么任务。\n\nLLaVA 的输出是基于文本的，可能会根据任务而有所不同：\n\n1. 描述性文本：如果任务是描述视觉内容，LLaVA 可以输出图像的详细描述，识别对象、动作和场景。\n2. 问题回答：对于问答任务，LLaVA 生成的回答可以解答关于视觉输入的问题，可能涉及基于图像内容的推理和推断。\n3. 后续行动：对于需要行动的指令，例如编辑图像或检索更多信息，LLaVA 可以提供适当的文本响应，指示所采取的行动或建议应该做什么。\n\n## 比较分析：LLaVa与当代多模态模型\n\n多模态人工智能的领域正在快速发展，出现了CLIP、BLIP等创新，以及最近推出的LLaVa。本小节将LLaVa的独特架构和方法与这些当代模型进行比较，突出其进步和区别，使其与众不同。\n\n### CLIP: 开创多模态理解的先河\n\nCLIP (Contrastive Language–Image Pre\\-training) 在多模态人工智能领域中迈出了革命性的一步，在各种视觉任务中提供了强大的性能。它在自然语言描述的背景下理解图像的能力为该领域设定了新的基准。CLIP 通过大规模的预训练方法将图像与文本描述对齐，使模型能够在一系列视觉任务上进行零样本学习。然而，CLIP 主要关注图像与文本之间的高层次关联，并不具备深入推理或对话参与的能力。\n\n### BLIP: 连接语言与图像感知\n\n在CLIP奠定的基础上，BLIP（Bootstrapped Language Image Pre-training）通过引入自引导预训练策略，扩展了多模态模型的能力。这种方法通过不断从自身的预测中学习，完善模型的视觉理解，从而帮助改善语言与视觉内容之间的对齐。BLIP在需要更精确视觉识别和语言理解的任务上表现出增强的性能。\n\n相比之下，LLaVa采取了不同的路径，通过利用GPT-4的语言生成能力来策划其遵循指令的数据。这不仅导致了一个捕捉更广泛人类互动范围的数据集，还使LLaVa能够进行更复杂的推理和深入的对话能力。\n\n## LLaVa的独特之处：是模型架构还是其他因素？\n\n根据我们的观点，LLaVA的优势主要在于其数据策划能力，而非架构选择。LLaVA的重大进展主要得益于其利用GPT-4进行数据策划。与传统的静态数据集不同，LLaVA使用ChatGPT-4生成动态、指导性的数据，积极参与各种视觉和文本场景中的训练过程。\n\n通过使用GPT-4，LLaVA生成的 数据集紧密模拟自然语言和视觉感知，脱离了传统的手动数据集生成方法。这种创新的方法不仅使AI能够理解和推理，还使其更接近于准确反映人类智能。\n\n### LLaVa中的数据整理策略\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*LzastWLkzPeMB_28Nr7Y9A.png)\n\nLLaVa，即大型语言与视觉助手，不仅以其先进的神经架构而闻名，还以其开创性的数据整理方法而脱颖而出。通过利用GPT-4，它彻底改变了传统的数据准备方法，构建出一个反映现实世界复杂性的数据库。\n\nLLaVa中的数据整理始于一张图片及其相应的标题，利用GPT-4生成一组查询。这些查询引导AI精确而相关地探索和描述图像内容。\n\n为了有效地将视觉数据转化为文本基础的AI（如GPT-4），LLaVa使用标题提供视觉场景的多样视角，并使用边界框提供空间上下文和焦点。\n\n1. 对话数据：模仿人类互动，LLaVa整理对话，其中模型作为助手，回答有关图像各个方面的问题。这些问题的范围包括识别物体和动作，辨别它们的数量、位置和相对位置，确保模型能够处理具有明确答案的查询。\n2. 详细描述数据：LLaVa旨在全面理解图像。为此，它促使GPT-4提出旨在理解图像丰富详细描述的问题。这些提示鼓励模型深入挖掘，提供一个捕捉视觉内容整体本质的叙述。\n3. 复杂推理数据：超越单纯描述，LLaVa通过需要分层推理过程的问题挑战模型，要求逻辑和因果关系的理解。这种类型的数据训练模型构建有理有据的响应，支持逻辑思维的顺序。\n\n## LLaVa的架构：视觉与语言的整合\n\nLLaVa模型整合了视觉与语言，利用以下核心组件：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8q_Iay_LHCzPqtrQby_H8w.png)\n\n1. 视觉编码器：LLaVa架构的基础是预训练的CLIP视觉编码器，特别是ViT-L/14变体。该组件通过Transformer层处理输入图像（Xv），提取特征（Zv），使模型能够有效理解视觉信息。\n2. 语言模型（Vicuna）：LLaVa的语言能力依赖于Vicuna，这是一个大型语言模型（LLM）的变体，记作fϕ。Vicuna根据输入语言指令（Xq）理解并生成语言响应（Xa），补充了视觉编码器的功能。\n3. 线性投影：该组件由一个可训练矩阵（W）表示，作为视觉特征（Zv）与语言模型的嵌入空间之间的桥梁。它将视觉特征转换为视觉标记（Hv），使其与语言模型的词嵌入空间对齐，以促进多模态对话。\n\n## 训练和微调 LLaVA：\n\nLLaVA 采用两阶段的训练过程，每个阶段都专注于提升模型解读和响应视觉与文本数据融合的能力。\n\n### Stage 1: 预训练以进行特征对齐\n\nLLaVA训练的初始阶段是预训练以进行特征对齐。在这个阶段，模型专注于将图像中的视觉特征与语言模型中的相应文本特征对齐。这是通过将一个大型数据集过滤为一组精炼的图像-文本对来实现的，LLaVA利用这些对来学习两种模态之间的关联。\n\n在这个阶段，视觉编码器（例如CLIP视觉编码器ViT-L/14）处理图像以提取视觉特征，然后使用投影矩阵（W）将这些特征映射到语言模型的词嵌入空间。LLaVA中使用的语言模型是Vicuna，以其强大的语言理解和生成能力而闻名。\n\n### Stage 2: 微调端到端\n\n在对齐视觉和语言特征后，LLaVA 进行端到端的微调过程。尽管保持视觉编码器的权重不变，但这一阶段允许模型联合微调投影矩阵和语言模型的权重。其目标是最大化基于提供的多模态数据的目标答案的可能性。\n\n这一阶段对于将 LLaVA 适应特定用例场景至关重要，例如多模态聊天、科学问答等。它确保模型不仅能够理解图像在通用描述中的上下文，还能在收到与图像相关的特定问题时参与复杂对话、提供详细解释并进行推理。\n\n## 性能与基准测试：LLaVa 在 VQA 模型中的应用\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*I_5fTa_2rtNHEDUaDNMXbQ.png)\n\n## LLaVA\\-Bench (COCO) 性能洞察\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*6B2K7EcbYgMbH-QEp8J41w.png)\n\nLLaVA\\-Bench (COCO) 提供了一个强大的框架，通过精心设计的90个问题来评估LLaVA的能力，这些问题来源于30张精选图像，涵盖对话、详细描述和复杂推理。结果如下：\n\n* 指令调优效果：在进行指令调优后，LLaVA对用户命令的遵从性提高了超过50分。\n* 问题多样性的影响：尽管详细和复杂推理问题的增加很小，但整体能力提高了7分。这一提升也对对话问题的响应产生了积极影响，展示了多样化训练集的好处。\n* 最优数据组合：三种问题类型的结合带来了最高的性能跃升，LLaVA达到了85.1%的基准分数，强调了全面数据集在提升多模态AI能力方面的优势。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*mCjP0xfpcjHkl-lu)\n\n## LLaVA在LLaVA-Bench（真实场景）上的表现\n\n* 在对话任务中，LLaVA的准确率为57.3%，相比BLIP-2的54.6%有明显提升，远超OpenAI的Flamingo，后者仅为19.3%。\n* 在提供详细描述方面，LLaVA得分为52.5%，展示了其从视觉线索中生成丰富、全面内容的能力。\n* 该模型在复杂推理问题上的表现尤为突出，成功率达到81.7%，表明其先进的推理和推断能力。\n\nLLaVA在所有类别中的综合得分为67.3%，比BLIP-2高出29个百分点，超过Flamingo 48个百分点。\n\n## 限制与关注事项：\n\nLLaVA的定量评估：\n\n将GPT-4作为评估LLaVA性能的评判者，在基准测试AI能力的框架内提出了一个复杂的挑战。一方面，GPT-4的高级理解和生成能力使其能够批判性地评估像LLaVA这样的候选模型所产生的响应质量。这种评估涵盖了有助于衡量模型在多模态数据上遵循指令能力的因素，如帮助性、相关性、准确性和细节。然而，另一方面，使用GPT-4作为评估法官引发了关于基准测试过程公正性的担忧。\n\n关注的核心在于，LLaVA的数据整理过程与GPT-4根本上是交织在一起的。由于GPT-4在训练LLaVA时发挥了重要作用——通过生成模型微调所需的遵循指令的数据——因此存在循环推理的固有风险。本质上，LLaVA可能倾向于生成与GPT-4训练数据中固有的模式或偏见一致的响应。这种倾向可能会扭曲评估，导致一个理论上的上限，反映出与GPT-4方法论的兼容性，而不是对普遍性能的真实衡量。\n\n此外，依赖GPT-4提供其评估的全面解释，引入了一种主观性，这种主观性根植于语言模型对什么构成高质量响应的“理解”。这种理解受到GPT-4训练的数据集的影响，而这些数据集可能未能充分体现现实世界多模态互动的多样性和复杂性。\n\n"},{"lang":"zh","group":"blog","slug":"blog/key-points-llm-quantization-chatgpt-artificial-intelligence-8201ffcb33d4","frontmatter":{"title":"解锁 LLM 量化的 5 个关键点","meta_title":"解锁 LLM 量化的 5 个关键点","description":"量化大型语言模型","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*RUqPEr2NTYXlI1omqF22Qg.png","categories":["Machine Learning","Data Science","Technology/Web"],"author":"Rifx.Online","tags":["quantization","weights","activations","calibration","Quanto"],"draft":false,"slug":"blog/key-points-llm-quantization-chatgpt-artificial-intelligence-8201ffcb33d4"},"content":"\n\n\n### 大型语言模型的量化\n\n\n\nLLM量化目前是一个热门话题，因为它在提高大型语言模型（LLMs）的效率和在各种硬件平台（包括消费级设备）上部署方面发挥着至关重要的作用。\n\n通过调整模型中某些组件的精度，**量化显著减少了模型的内存占用**，同时保持相似的性能水平。\n\n在本指南中，我们将探讨LLM量化的五个关键方面，包括将此技术应用于我们模型的一些实用步骤。\n\n## #1. 理解量化\n\n量化是一种模型压缩技术，通过降低 LLM 中权重和激活的精度来实现。这涉及将高精度值转换为低精度值，实际上是**将存储更多信息的数据类型更改为存储更少信息的数据类型**。\n\n减少每个权重或激活所需的位数显著降低了整体模型大小。因此，**量化创建了使用更少内存和需要更少存储空间的 LLM。**\n\n这一技术在应对 LLM 连续迭代中参数数量的指数增长时变得至关重要。例如，在 OpenAI 的 GPT 系列中，我们可以在以下图表中观察到这一增长趋势：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*QlAhma3Wu1F6w2WvkE8jDA.png)\n\n这一显著增加带来了挑战：随着模型的增长，它们的内存需求往往超过先进硬件加速器（如 GPU）的容量。**这需要分布式训练和推理来管理这些模型，从而限制了它们的可部署性。**\n\n## #2. 量化背后的直觉\n\n尽管量化的定义看起来相当复杂，但这个概念可以通过矩阵直观地解释。\n\n让我们考虑以下一个 3x3 矩阵，表示神经网络的权重。左侧的矩阵显示了原始权重，而右侧的矩阵显示了这些权重的量化版本：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*LPzWe9oxjlDYdSp7dVvRUg.png)\n\n在这个简单的例子中，我们将原始矩阵的元素从四位小数四舍五入到一位小数。尽管矩阵看起来相似，**但四位小数版本所需的存储空间显著更高**。\n\n在实践中，量化不仅仅是一个四舍五入操作。相反，它涉及将数值转换为不同的数据类型，通常是从更高精度转换为更低精度。\n\n例如，大多数模型的默认数据类型是 `float32`，每个参数需要 4 字节（32 位）。因此，对于一个 3x3 矩阵，总内存占用为 36 字节。将数据类型更改为 `int8`，每个参数只需要 1 字节，从而将矩阵的总内存占用减少到仅 9 字节。\n\n## #3. 量化误差\n\n正如我们所看到的，原始矩阵及其量化形式并不完全相等，但非常相似。逐值之间的差异被称为“量化误差”，我们也可以用矩阵形式表示：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*VtGDjVbr7daagLXB57i7Mg.png)\n\n**这种量化误差可以在网络中的每个权重矩阵中累积，从而影响模型的性能。**\n\n当前的量化研究旨在最小化精度差异，同时减少训练或推理模型所需的计算资源，同时保持可接受的性能水平。\n\n## #4. 线性量化\n\n线性量化是 LLMs 中最流行的量化方案之一。简单来说，它涉及将原始权重的浮点值范围映射到固定点值范围。\n\n让我们回顾一下将线性量化应用于我们的模型所需的步骤：\n\n* **获取最小和最大范围：** 我们需要获取待量化的浮点权重的最小值和最大值（`x_min` 和 `x_max`）。我们还需要定义量化范围（`q_min` 和 `q_max`），该范围已经由我们想要转换的数据类型设置。\n* **计算缩放因子（`s`）和零点（`z`）值：** 首先，缩放因子（`s`）将浮点值的范围调整到适合整数范围，保持数据分布和范围。其次，零点（`z`）确保浮点范围内的零被准确地表示为整数，从而保持数值的准确性和稳定性，特别是对于接近零的值。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BepC6-izw0yE19ejsS705Q.png)\n\n* **量化值（`q`）：** 我们需要使用在前一步计算的缩放因子（`s`）和零点（`z`）将原始浮点值映射到整数范围。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BBOQ0VbSGbwf7CN8c4PWKQ.png)\n\n应用这些公式相当简单。如果我们将它们应用于下图左侧的 3x3 权重张量，我们将得到右侧所示的量化矩阵：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*KzBvg84mfI2gAhTIyVibwQ.png)\n\n我们可以看到，`int8` 值的下限对应于原始张量的下限，而上限对应于原始张量的上限，*即，映射为 `0.50 → 255` 和 `-0.40 → 0`。*\n\n我们现在可以使用下面的公式对值进行反量化。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*E5nnqYzncYCRuM5prssuOw.png)\n\n如果我们将反量化后的值再次放入矩阵形式（左侧矩阵），我们可以通过计算原始矩阵与其反量化版本之间逐点差异来计算量化误差（右侧矩阵）：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*56NALu9PAN95QG2hn8HXoQ.png)\n\n正如我们所观察到的，量化误差开始在某些矩阵值中显现。\n\n## #5. 权重量化与激活量化\n\n在上面的例子中，我们主要关注于量化模型的权重。虽然权重量化对于模型优化至关重要，但考虑到激活也可以进行量化同样重要。\n\n**激活量化涉及减少网络中每层的中间输出的精度**。与权重在模型训练后保持不变不同，激活是动态的，并且随着每个输入而变化，使其范围更难预测。\n\n一般而言，激活量化比权重量化更具挑战性，因为它需要仔细校准以确保准确捕捉激活的动态范围。\n\n权重量化和激活量化是互补的技术。两者结合使用可以显著减少模型大小，而不会大幅影响性能。\n\n## 最后的思考\n\n在本文中，我们回顾了关于量化的5个关键点，以更好地理解如何减小这些不断增长的模型的大小。\n\n至于这些技术的实现，Python中有几个支持量化的工具和库，例如`pytorch`和`tensorflow`。然而，在现有模型中无缝集成量化需要对库和模型内部结构有深入的理解。\n\n这就是为什么到目前为止，我最喜欢的简单步骤实现量化的选项是Hugging Face的[Quanto](https://huggingface.co/blog/quanto-introduction)库，旨在简化PyTorch模型的量化过程。\n\n如果你对LLM量化的深入内容以及如何使用上述库感兴趣，你可能还会对文章[“大型语言模型（LLMs）的量化：有效减少AI模型大小”](https://www.datacamp.com/tutorial/quantization-for-large-language-models)感兴趣。\n\n就这些！非常感谢你的阅读！\n\n我希望这篇文章能在**使用LLMs进行编码时**对你有所帮助！\n\n你也可以订阅我的[**时事通讯**](https://readmedium.com/@andvalenzuela/subscribe)，以便及时获取新内容。\n\n**特别是**，**如果你对有关大型语言模型和ChatGPT的文章感兴趣**：\n\n"},{"lang":"zh","group":"blog","slug":"blog/langgraph-vs-langchain-vs-langflow-vs-langsmith-which-one-to-use-why-69ee91e91000","frontmatter":{"title":"LangGraph、LangChain、LangFlow、LangSmith：使用哪一个以及为什么？","meta_title":"LangGraph、LangChain、LangFlow、LangSmith：使用哪一个以及为什么？","description":"了解 LangGraph、LangChain、LangFlow 和 LangSmith 之间的主要区别，并了解哪种框架最适合您的……","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*xrWv1QVt4zE5cxjA8VA3ag.png","categories":["Programming","Technology","Technology/Web"],"author":"Rifx.Online","tags":["LangGraph","LangChain","LangFlow","LangSmith","frameworks"],"draft":false,"slug":"blog/langgraph-vs-langchain-vs-langflow-vs-langsmith-which-one-to-use-why-69ee91e91000"},"content":"\n\n\n### 探索 LangGraph、LangChain、LangFlow 和 LangSmith 之间的关键区别，了解哪种框架最适合您的语言模型应用——从工作流构建到性能监控。\n\n👨🏾‍💻 [GitHub](https://github.com/mdmonsurali) ⭐️ | 👔[LinkedIn](https://www.linkedin.com/in/mdmonsurali/) |📝 [Medium](https://medium.com/@monsuralirana)\n\n\n\n近年来，自然语言处理（NLP）领域见证了可用于构建基于语言模型的应用程序的框架、库和工具数量的激增。在这些工具中，**LangGraph**、**LangChain**、**LangFlow** 和 **LangSmith** 已成为领先的选择，各自满足不同的用例和用户需求。如果您希望构建、监控或扩展语言模型工作流，了解这些工具的优势和目的至关重要。\n\n在本博客中，我们将探讨每个框架，分析它们的优势，并提供何时使用它们的见解。无论您是经验丰富的开发者还是该领域的新手，理解这些工具的细微差别将帮助您为您的项目选择合适的工具。\n\n## 语言模型框架简介\n\n随着强大的语言模型如 GPT-3、GPT-4 以及其他基于变换器的模型的崛起，越来越需要能够简化语言应用程序创建和管理的框架。这些框架简化了复杂的任务，如 **链接多个提示**、**检索相关文档**，甚至 **监控模型性能**。\n\n然而，并非所有框架都是相同的。有些框架提供 **可视化界面** 来管理工作流程，而其他框架则提供高级的 **调试和可观察性** 功能。让我们深入了解这些工具，以理解它们独特的功能。\n\n## 1. LangGraph：可视化复杂工作流\n\n**LangGraph** 是一个为开发者设计的新框架，适合那些偏好 **可视化方法** 来构建语言模型管道的用户。它允许您通过 **基于图的可视化** 来构建复杂的工作流，从而更容易理解不同任务和组件之间的依赖关系。这对于多个步骤（如文本生成、文档检索和分类）串联在一起的大型应用尤其有用。\n\n### 优势：\n\n* **可视化工作流表示**：LangGraph 允许您可视化不同组件之间的数据和操作流。这种图形化的方法直观且有助于设计更复杂的管道。\n* **调试简单**：LangGraph 的可视化特性使得识别工作流中的瓶颈或问题节点变得更加容易。\n\n### 示例用例：\n\n假设您正在构建一个自动化系统，该系统首先使用语言模型检索相关文档，然后将其传递给摘要生成器。在 LangGraph 中，您可以直观地绘制出此工作流程，展示每个步骤之间的关系。如果链中的任何一点出现问题，视觉工具使您能够轻松定位问题所在。\n\n### 何时使用 LangGraph：\n\n如果您正在管理 **复杂的工作流程**，并且重视 **图形界面** 来理解您的管道，LangGraph 是一个绝佳的选择。它特别适合那些更喜欢直观的拖放式工作流程设计的开发人员或数据科学家。\n\n**关键点**：\n\n* 如果您需要清晰的语言处理工作流程的可视化表示。\n* 在创建需要分支或多路径依赖的更复杂的管道时。\n\n## 2. LangChain：LLM 应用的工作马\n\n**LangChain** 是构建由 **大型语言模型 (LLMs)** 驱动的应用程序最受欢迎的框架之一。它提供了一种灵活的 **代码优先方法**，允许开发者将文档检索、摘要和问答等任务串联成统一的工作流程。\n\n### 优势：\n\n* **广泛支持LLMs**：LangChain兼容多种语言模型，使得集成OpenAI的GPT或本地托管模型变得简单。\n* **链式能力**：LangChain擅长于**多个操作的链式处理**——因此得名——使开发者能够创建复杂的NLP应用。\n* **广泛采用**：作为最受欢迎的框架之一，LangChain拥有一个**蓬勃发展的社区**和出色的支持，提供丰富的文档和教程。\n\n### 示例用例：\n\n想象一下，您正在构建一个 **聊天机器人**，它首先理解用户的问题，从数据库中检索相关信息，然后生成响应。使用 LangChain，您可以轻松地以编程方式创建这个多步骤的过程，确保链中的每一步协调工作。\n\n### 何时使用 LangChain：\n\n如果您是一个 **构建生产级应用的开发者**，并且需要一个 **灵活、以代码为中心的解决方案**，LangChain 是您的最佳选择。它非常适合那些希望控制应用架构并且能舒适地编写代码来定义工作流程的开发者。\n\n**关键点**：\n\n* 如果您正在构建需要跨多个语言模型链式任务的生产级应用。\n* 如果您需要一个拥有广泛社区支持和多种集成的库。\n* 当您对编程解决方案更为熟悉，而非可视化工具。\n\n## 3. LangFlow: 无需编码/低代码的 LangChain 扩展\n\n**LangFlow** 本质上是 **LangChain 的可视化扩展**。它将 LangChain 强大的后端与 **直观的拖放界面** 结合在一起。LangFlow 使那些可能不太擅长编写代码的用户仍然能够在他们的应用程序中利用语言模型的强大功能。\n\n### 优势：\n\n* **可视化工作流创建**：与 LangGraph 类似，LangFlow 提供了一个可视化界面用于构建工作流。然而，它是基于 LangChain 构建的，这意味着用户可以利用 LangChain 的强大功能，而无需编写大量代码。\n* **快速原型制作的理想选择**：LangFlow 非常适合快速 **原型化想法** 或构建概念验证应用程序。\n* **适合初学者**：它是一个很好的入门点，适合那些对编码不太熟悉但想要创建语言模型工作流的用户。\n\n### 示例用例：\n\n如果您想快速构建一个**摘要工具**来检索文档，您可以在LangFlow的界面中拖放组件，以创建一个完全功能的应用程序。这可以在几乎不编写代码的情况下完成。\n\n### 何时使用 LangFlow：\n\nLangFlow 非常适合 **非开发人员** 或 **快速原型设计**。如果您想快速实验 **LLM 工作流** 而不深入代码，这个工具可以让您轻松入门。\n\n**关键点**：\n\n* 如果您想快速原型设计 LLM 工作流而不编写代码。\n* 如果您对视觉编程感到舒适，但需要 LangChain 的灵活性。\n* 用于教育目的，帮助用户了解如何构建工作流。\n\n## 4. LangSmith: 监控与可观察性\n\n虽然其他工具专注于 **构建工作流程**，**LangSmith** 的设计目标是 **监控** 和 **调试** 语言模型应用。它提供了先进的可观察性功能，以跟踪您的工作流程和模型的性能，使其在生产环境中不可或缺。\n\n### 优势：\n\n* **深度可观察性**：LangSmith 允许开发者监控语言模型的性能，确保工作流程按预期运行。\n* **错误跟踪**：它在帮助开发者定位问题方面表现出色，使调试变得更加容易。\n* **性能洞察**：LangSmith 提供有关 **工作流程性能** 的洞察，帮助开发者优化他们的应用程序。\n\n### 示例用例：\n\n假设您已经部署了一个**客户服务聊天机器人**，该聊天机器人使用语言模型来回答问题。随着时间的推移，您会发现某些回答的准确性低于预期。LangSmith 可以帮助您追踪问题，通过提供对工作流程中每个决策点的可见性。\n\n### 何时使用 LangSmith：\n\n如果您在 **生产环境** 中部署应用程序，并且需要确保 **健壮性、可靠性和性能**，LangSmith 是一个不可或缺的工具。它在管理 **需要随着时间调试和优化的复杂系统** 时特别有用。\n\n**关键点**：\n\n* 如果您需要 LLM 工作流中的高级监控或调试能力。\n* 对于观察性对确保最佳模型性能至关重要的开发环境。\n* 如果您的重点是基于实时洞察改进和迭代 LLM 驱动的应用程序。\n\n## 哪个更适合你？\n\n* **使用 LangGraph** 如果你更喜欢基于图形的可视化工作流程来构建复杂的 LLM 任务。非常适合需要清晰和结构的用户。\n* **使用 LangChain** 如果你需要一个强大、灵活的解决方案来以编程方式创建语言模型应用。它多功能且非常适合构建生产级应用的开发者。\n* **使用 LangFlow** 如果你想要 LangChain 的强大功能，同时又希望拥有一个可视化的无代码/低代码界面。最适合快速原型开发和更喜欢可视化工具而非编码的用户。\n* **使用 LangSmith** 如果你的重点是 LLM 应用的可观察性和调试。非常适合在开发或生产环境中监控和优化工作流程。\n\n最终，你的选择取决于你对代码的舒适度、工作流程的复杂性，以及你是否优先考虑易用性、灵活性或可观察性。\n\n## 结论\n\n这些工具 — **LangGraph**、**LangChain**、**LangFlow** 和 **LangSmith** — 针对开发和管理语言模型应用的不同阶段。**LangGraph** 提供了一种可视化、直观的方式来构建复杂的工作流程，而 **LangChain** 则为希望创建可扩展应用的开发者提供了一种强大的代码优先解决方案。对于那些更喜欢 **低代码**、拖放方式的用户，**LangFlow** 在不牺牲功能的情况下简化了流程。最后，**LangSmith** 专注于可观察性和调试，确保您的工作流程是优化和可靠的。选择合适的工具取决于您的项目需求，无论是快速原型设计、生产级扩展，还是监控和性能跟踪。\n\n快乐编码！ 🎉\n\n👨🏾‍💻 [GitHub](https://github.com/mdmonsurali) ⭐️ | 👔[LinkedIn](https://www.linkedin.com/in/mdmonsurali/) |📝 [Medium](https://medium.com/@monsuralirana)\n\n感谢您花时间阅读这篇文章！\n\n请务必留下您的反馈和评论。下次博客见，敬请关注 📢\n\n## 参考文献：\n\n1. “LangChain 文档” — <https://python.langchain.com/docs/introduction/>\n2. “LangGraph 概述” — <https://langchain-ai.github.io/langgraph/>\n3. “LangFlow GitHub 仓库” — [https://github.com/LangFlow/LangFlow](https://docs.langflow.org/)\n4. “LangSmith 介绍” — <https://www.langchain.com/langsmith>\n5. “如何使用 LangChain 构建聊天机器人” by JetBrains 博客 — <https://blog.jetbrains.com/pycharm/2024/08/how-to-build-chatbots-with-langchain/>\n\n"},{"lang":"zh","group":"blog","slug":"blog/large-language-models-just-got-a-whole-lot-smaller-f93425ee59a2","frontmatter":{"title":"大型语言模型变得更小了","meta_title":"大型语言模型变得更小了","description":"这可能会改变软件初创企业的游戏规则","date":"2024-11-04T12:29:02.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*1PeFyz_Dlt6jEf27Q9Y33Q.png","categories":["Programming","Technology","Machine Learning"],"author":"Rifx.Online","tags":["compression","optimization","ternary","parallelism","hardware"],"draft":false,"slug":"blog/large-language-models-just-got-a-whole-lot-smaller-f93425ee59a2"},"content":"\n### 这将如何改变软件初创企业的游戏规则\n\n\n\n**本文与 [David Meiborg](https://readmedium.com/undefined) 共同撰写。**\n\n*TLDR: 大型语言模型（简称 LLMs）目前体积庞大，运行成本高，并且具有 [显著的碳足迹](https://arxiv.org/abs/2309.14393)。然而，最近在模型压缩和系统级优化方法上的进展可能会增强 LLM 推理能力。特别是一种使用三元结构参数的方法，有潜力绕过当前标准的昂贵矩阵乘法。这对制造专用芯片的硬件初创企业以及使用或定制构建自己 LLM 的软件初创企业都有令人兴奋的影响。帮助客户部署 LLM 的初创企业可能也会迎来更多的业务。*\n\n如今的大型语言模型非常庞大。真的很大。如果你想加载一个 LlaMa-2–70B 模型，你需要 140 GB 的显存（这就是 70 亿个参数乘以每个参数 2 字节）。作为对比，像 NVIDIA RTX 3090 或 4090 这样的 GPU 只有 24 GB 的显存——这只是所需的一小部分。\n\n有一些关于量化的 [解决方法](https://towardsdatascience.com/run-llama-2-70b-on-your-gpu-with-exllamav2-588141a88598)，但这些往往比较繁琐。你可能仍然需要让你的 GPU 高温运行长达 15 小时，直到模型加载完成。更不用说你仍然需要一些空余内存用于推理，换句话说，就是用于部署模型。\n\n因此，使用当前的 LLMs 成本高昂：通常需要多个高端 GPU 来保存模型，并且还必须考虑推理所产生的能源成本。\n\n这就是为什么很多研究都在致力于应用技术，使 LLMs 更小，从而能够在更小的硬件上以更低的成本运行。在大多数情况下，这是一种艰难的权衡，因为使 LLMs 更小通常会影响它们的质量。找到成本与收益相等的点可能是棘手的。\n\n在本文中，我们概述了一些有前景的优化方法，解释了微软研究人员的最新突破，简要概述了“高效 LLM”领域的创新初创企业，并推导出一些对在 LLM 生态系统中运营的初创企业的一般影响。\n\n## LLM如何变得更加资源高效\n\n像微软、OpenAI、Meta或谷歌这样的科技巨头拥有足够的资源来训练尖端模型，即使目前训练成本对大多数其他公司来说是不可承受的。因此，广泛采用的最大瓶颈不是训练，而是推理效率。换句话说，尽管Meta已经发布了LlaMa，但由于运行模型——而不是创建模型——已经足够具有挑战性，因此它仍未得到足够的采用。\n\n然而，研究人员开始提高这种推理效率。广义而言，有两种方法可以实现这一目标：**系统级优化**并不改变模型本身，而是通过改变模型所处环境的关键方面来提高其性能。**模型优化**则压缩模型，使其更易于部署和运行。\n\n这两种方法都有多种不同的技术。[一篇最近的论文](https://arxiv.org/pdf/2402.01799.pdf)由研究人员出色地总结了这些技术。由于这些技术可能很快就会成为任何从事LLM系统工作者的基本知识，我们在下面对这些技术进行了快速概述。\n\n### 系统级优化\n\n系统级优化指的是改变模型本身的运行方式，而不是模型本身。事实证明，有很多手段可以避免资源闲置或消除其他低效现象。\n\n**分页注意力**\n\n像 GPT 这样的 LLM 的核心是注意力机制。这个机制允许模型在生成每个输出单词时关注输入文本的不同部分。想象一下，您正在阅读一本书，并标记重要的句子以更好地记住故事。类似地，注意力机制在做出预测时“突出”或赋予某些单词或短语更多重要性。\n\n这个机制非常耗费资源。它要求模型考虑输入文本中所有单词对之间的关系。对于长文本，这可能需要大量的内存和计算能力。\n\n分页注意力不是一次处理整个文本，而是将文本分成更小的“页”或段落。模型然后一次处理这些页面或以较小的组处理。这种方法显著减少了任何给定时刻所需的内存量，因为模型不需要同时跟踪整个文本的关系。\n\n这有点像一个学生，如果一次性阅读整整一年的教科书会感到不知所措。通过在整个学年中将其分解为可管理的段落，学生可以记住教科书的内容。\n\n通过每一步所需的内存减少，分页注意力允许在相同的硬件约束下使用更大的模型或更长的文本。\n\n**张量并行**\n\n并行是一种计算中的众所周知的概念。它意味着将一个大型计算任务分成可以由多个处理器或计算机同时处理的小部分。这显著加快了程序运行所需的时间。\n\n在 LLM 的上下文中，[张量](https://towardsdatascience.com/what-is-a-tensor-in-deep-learning-6dedd95d6507) 是多维数字数组。这些张量用于表示模型处理的数据。这类数据包括输入文本、模型权重，即模型学习的参数，以及输出预测。\n\n将这两个概念结合起来，张量并行涉及将这些张量分割到多个 GPU 或其他处理单元上。例如，如果模型的参数（权重）太大而无法适应单个 GPU 的内存，则可以将其分布到多个 GPU 上。每个 GPU 然后一次只处理张量的一部分。\n\n就像一个团队中的多个成员一起工作在一个大型项目上，处理单元在处理各自部分的张量时需要交换信息。例如，一个 GPU 上的计算结果可能需要与另一个 GPU 共享，以继续下一步计算。因此，单元之间的高效通信对于张量并行的有效性至关重要。\n\n简而言之，张量并行是一种将 LLM 所需的计算分解为更小的并行任务的方法，这些任务可以由多个计算单元同时处理，从而加快这些大型复杂模型的训练和推理时间。\n\n**流水线并行**\n\n这种技术专注于改善数据通过模型层的处理工作流。这可以显著加快整体计算速度，并更好地利用可用硬件。\n\n计算中的流水线工作方式类似于工厂的装配线，不同任务的阶段按顺序完成。这允许多个任务在不同阶段同时进行。\n\n在 LLM 中，这些不同的阶段由神经网络的层表示。每一层按顺序处理输入数据，逐渐提取更复杂的特征或模式，直到产生最终输出。可以将每一层视为工厂装配线上的一个工人：每个工人在数据通过时都会在其上添加一些东西，直到最终出现一个复杂的产品。\n\n在流水线并行中，模型的层被分为多个段，每个段分配给不同的 GPU 或处理单元。这样，模型可以按批次输入数据：一旦第一个段处理完第一批数据，第二个段就接手那批数据，而第一个段则接手一批新的数据。\n\n这在模型中创建了数据的连续流动，每个模型段在任何给定时间都在处理不同的数据。这通过保持模型的所有部分处于活动状态来最大化可用硬件资源的使用，并减少单个处理器等待任务完成时可能发生的空闲时间。\n\n前面讨论的流水线并行在模型层级别上操作，将顺序处理阶段分布到设备上。而张量并行则在更细粒度的层面上操作，将层内发生的实际计算（例如，大型矩阵乘法的部分）分布到设备上。\n\n**CPU/GPU 卸载**\n\n在这部分中，我们讨论了很多关于 GPU 的内容。然而，并不是所有在训练或运行 LLM 中的任务都同样适合 GPU。一些任务，如数据预处理或某些控制逻辑，可能更有效地由 CPU 处理。其他任务，特别是处理神经网络（如矩阵乘法）所涉及的重数学计算，确实更有效地在 GPU 上执行。\n\n通过将特定任务卸载到最适合它们的处理器上——将并行化、计算密集型任务分配给 GPU，而将顺序或逻辑密集型任务分配给 CPU——系统可以确保每个工作负载部分以最有效的方式进行处理。\n\n**融合操作**\n\n融合操作将通常单独执行的多个处理步骤合并为一个简化的操作。例如，而不是先执行矩阵乘法再进行加法，融合操作会同时执行两者。\n\n**推测解码**\n\n在生成文本时，LLM 根据之前的单词计算句子中下一个单词的概率。传统上，在生成每个单词后，模型会重新计算以确定下一个单词，并且这个过程会重复，直到完整的句子或段落完成。然而，这种顺序过程可能很慢，尤其是对于较长的文本或更复杂的模型，因为每一步都依赖于前一步的完成。\n\n并行预测：与其等待每个单词被选择后再考虑下一个，推测解码允许模型“推测”或同时对接下来的几个单词做出多个预测。这被称为 *并行预测*。这就像对句子接下来可能采取的几条路径进行有根据的猜测。\n\n通过并行探索这些可能性，模型可以潜在地减少生成文本所需的整体时间。一旦实际的下一个单词被选定，模型可以更快地沿着最可能的路径继续，因为它已经计算了后续的选项。\n\n### LLM模型的压缩\n\n研究人员过去探索过模型压缩。然而，随着大规模LLM的出现，这已成为一个更大的挑战。\n\n许多现有的压缩方法依赖于执行微调步骤以在压缩阶段恢复丢失的性能。然而，当应用于LLM时，由于其庞大的规模，这种方法有显著的局限性。因此，LLM压缩已成为一个全新的研究领域。\n\n**架构剪枝**\n\n当你修剪苹果树时，你会在冬季或早春剪掉某些树枝。这确保树木不会在无效的树枝上浪费资源或因枯木而感染疾病。这有助于它结出更好的果实。\n\n当然，LLM并不结出果实。在这个背景下，剪枝是一种用于减少模型大小的方法，同时尽量保持或最小化对其性能的影响。\n\nLLM模型有数百万甚至数十亿个参数。这些参数并不是所有对模型进行预测或理解语言都同等重要。有些参数很少使用或对模型的决策贡献不大：因此，消除这些冗余或影响较小的连接、神经元或整个层，使模型的使用更高效。\n\n选择剪枝哪些参数并不是一项简单的任务。在基于幅度的剪枝中，移除神经网络中绝对值最小的权重。在训练之前，这些权重通常为零；训练之后，它们通常介于-1和1之间。如果训练对某个权重的影响不大，那么它很可能接近零，因此对模型的决策贡献较少。\n\n一种资源密集但也更稳健的剪枝技术是灵敏度分析。这涉及评估移除每个参数或参数组对模型性能的影响。移除后导致性能下降最小的参数会被剪枝。\n\n还有其他技术，但通常可以将它们分类为非结构化剪枝或结构化剪枝。非结构化剪枝（例如基于幅度的剪枝）移除单个权重，导致稀疏连接的神经网络。结构化剪枝（例如灵敏度分析）移除整个单元或层（例如，整个神经元或通道），这在某些硬件上可以更有效地提高计算效率。\n\n剪枝后，模型通常会经历微调过程。这涉及在训练数据集或其子集上对剪枝后的模型进行再训练。目标是让模型调整和优化其剩余参数，以补偿剪枝所造成的损失。这有助于恢复因剪枝而失去的任何性能。\n\n这可以通过迭代方式或一次性方式进行。在迭代剪枝中，模型在多个轮次中逐步剪枝。在每一轮之后，剪枝后的模型会重新训练，以恢复因剪枝而失去的性能。这个循环可以重复多次，模型可能会变得更加稳健，即使在显著减少参数的情况下也能保持性能。在一次性剪枝中，所有识别出的参数一次性移除，然后对模型进行微调。\n\n**知识蒸馏**\n\n想象一下，有一个足球场上有两个球员：一个非常有经验，知道很多技巧，另一个是初学者。经验丰富的球员知道的比初学者多得多，但初学者可以通过模仿其他球员在场上的行为迅速达到可比的表现。\n\nLLM的知识蒸馏工作原理类似：这是训练一个更小（学生模型）、更高效的模型，以通过学习大模型（教师模型）的输出和处理信息的方式来复制其性能的过程。\n\n要应用这一技术，显然需要一个大型教师模型，例如LlaMa或Mistral的开源大型模型之一。然后需要设计一个参数数量显著少于教师模型的较小神经网络。\n\n学生模型不仅仅在原始硬目标（即真实数据标签）上进行训练，还在软目标上进行训练。这些是教师模型对相同输入生成的概率。例如，对于一组特定的查询，假设教师模型70%的时间回答为“A”，20%的时间回答为“B”，10%的时间回答为“C”、“D”或“E”。学生模型不仅会尝试正确回答每个问题；它还会尝试在一组查询中遵循相同的概率分布。\n\n这样的软目标每个示例携带的信息比硬标签更多，因为它们包含教师模型对所有可能结果的置信水平。这就是学生模型能够以较低的计算开销表现得与教师相似的原因。\n\n在初始知识蒸馏之后，学生模型可能会在特定任务的数据集上进行进一步的微调，以最大化其性能。\n\n**低秩近似**\n\nLLM通过处理和生成基于巨大的矩阵（即非常大的数字表）来工作，这些矩阵表示单词之间的关系、它们的含义以及它们在语言中的使用。这些矩阵可能大到难以处理，特别是在存储和计算方面。\n\n低秩近似涉及找到一个更简单的矩阵，其大小要小得多，但仍能捕捉到原始大矩阵中最重要的信息。这有点像将详细的画作简化为草图。\n\n这通过数学技术来完成，这些技术识别矩阵（或在我们的类比中，画作）中哪些部分包含最多的信息，并将矩阵缩减到仅这些部分。有一些数学技术，尤其是[奇异值分解](https://www.cs.cmu.edu/~venkatg/teaching/CStheory-infoage/book-chapter-4.pdf)，有助于实现这一点。\n\n与剪枝不同，低秩近似执行矩阵维度减少，保持模型的结构，但以更紧凑的形式表示，而剪枝则直接移除神经网络的部分。\n\n**量化**\n\nLLM使用大量数学计算来处理文本。这些计算使用可以具有广泛值范围的数字。通常，这些数字以可以表示非常广泛值范围的格式存储（[浮点格式](https://de.wikipedia.org/wiki/Einfache_Genauigkeit)），在内存中占用32位。\n\n量化减少了这些数字的精度，通常将32位浮点数字转换为更低位宽的表示，例如8位整数。这意味着模型不再使用具有许多小数位的数字，而是使用“更简单”的数字，从而使计算更快并减少内存占用。\n\n量化感知训练（QAT）涉及在训练模型时考虑量化，使其能够适应精度损失，通常导致更好的性能，但代价是更复杂和资源密集的训练过程。\n\n后训练量化（PTQ）在模型完全训练后应用量化，提供了一种更简单和更快速的方法来减少计算需求。然而，由于模型并未特别针对低精度操作进行优化，因此可能无法达到与QAT相同的准确性或性能水平。\n\n### 1位 LLM 时代？\n\n微软研究人员最近发表了一篇[引起轰动的论文](https://arxiv.org/pdf/2402.17764.pdf)，将每个参数的存储位数从当前 LLM 中的 16 位标准，降低到了仅仅 1.58 位。这是个重大新闻：通过这种技术，他们实现了近 10 倍的令牌吞吐量，即处理文本的速度几乎快了 10 倍。他们还将内存占用减少了 3.5 倍，这意味着运行这些模型所需的硬件大大减少。\n\n这是通过使用三元位实现的。与通常使用的介于 -1 和 1 之间的浮点数（通常使用 16 位）不同，每个权重被表示为 -1、0 或 1。这些数字可以存储在 1.58 位，因为对于 3 个可能值的二进制晶体管，可以得到 2¹.58 = 3。仅使用如此简单的数字也意味着不再需要复杂的矩阵乘法，这使得资源使用效率大大提高。\n\n这种技术令人困惑的是，它在 30 亿参数的大小下，能够实现与传统 16 位模型相似的输出性能。目前尚不清楚这种模型在超过 130 亿参数的阈值时，是否能像传统模型一样扩展。明确的是，即使在 700 亿参数下，它在延迟、内存使用和能耗方面比仅有 130 亿参数的传统模型更高效。输出质量仍需详细测试。\n\n另一个缺点是，现有 LLM 的最先进量化技术无法用于生成 1.58 位模型。这类模型需要从头开始创建，尽管成本大幅降低，但目前仍超出普通市民的承受范围。\n\n然而，如果这样的模型被创建并运行良好，推理将变得更加容易。1.58 位 LLM 甚至可能在边缘和移动设备上部署。它们对 CPU 设备（大多数移动设备运行的设备）也更加友好，这使得它们更容易在更便宜的芯片上部署。所有这些都有许多优势，例如隐私方面，但也允许出现人类尚未梦想的新的应用。\n\n此外，像 [Groq](https://groq.com/) 这样的初创公司已经展示了在为 LLM 构建特定硬件 [如 LPU](https://wow.groq.com/why-groq/) 的 promising results 和巨大潜力。LLM 专用硬件已经是一个[巨大市场](https://finance.yahoo.com/news/generative-ai-market-size-expected-163500846.html#:~:text=%2D%20Large%20Language%20Model%20(LLM),the%20forecast%20period%202023%2D2029.)。这样的发现可能使这个市场的增长速度比分析师迄今预见的更为激进。\n\n如果没有其他，推理将由于量化技术和专用硬件的结合而变得极为便宜。这对许多公司，包括初创公司，都会产生影响。\n\n## 较轻量的 LLM 对初创企业意味着什么？\n\n### AI硬件的繁荣刚刚开始\n\n在1971年至1999年间，CPU几乎是市场上[唯一的微处理器](https://cs.stanford.edu/people/eroberts/courses/soco/projects/2005-06/64-bit-processors/history1.html)。随后，[NVIDIA推出](https://readmedium.com/a-brief-history-of-gpu-47d98d6a0f8a)了其GPU。虽然从技术上讲，它并不是世界上第一个GPU，但它是使游戏成为一种可接触和沉浸式体验的首批微处理器之一。（游戏消耗大量计算能力——如果你不知道，现在你知道了！）\n\n从游戏开始，GPU迅速扩展到许多不同的任务，包括科学图像处理、线性代数、3D重建等。GPU特别擅长的一件事是什么？机器学习和LLM。如今，许多NVIDIA的芯片正在用于训练LLM。\n\n从那时起，其他微处理器也开始涌现。[谷歌的TPU](https://cloud.google.com/tpu?hl=en)于2016年推出，特别适合AI训练和推理。虽然GPU被证明非常适合LLM，但TPU是专门为此目的设计的。它们在训练和推理方面都非常合适。\n\n然而，行业正处于[转折点](https://www.wsj.com/tech/ai/how-a-shifting-ai-chip-market-will-shape-nvidias-future-f0c256b1)：不久之后，大多数与LLM相关的工作将是推理，而不再是训练，因为用户开始部署像LlaMa这样的模型。新的创新AI半导体公司现在有机会进入这个领域。\n\n这包括专注于特别快速推理处理器的芯片制造商[Groq](https://wow.groq.com/press/)。其他初创公司包括[赛拉布拉斯](https://www.cerebras.net/)（专注于训练）、[Graphcore](https://www.graphcore.ai/about)（涵盖训练和推理）和[SambaNova](https://sambanova.ai/)（也包括训练和推理）。像英特尔和AMD这样更成熟的竞争对手也在关注训练和推理，尽管预计未来几年的大多数增长将来自后者。大型科技巨头——谷歌、亚马逊或微软——也在开发AI专用芯片，但主要用于内部使用。\n\n总体而言，LLM的硬件市场仍然由数据中心应用主导。边缘和移动应用是下一个合乎逻辑的步骤，但将需要更多突破，例如微软研究人员最近发布的1.58位方法（见上文）。\n\n## LLM软件公司的影响\n\n在新兴AI领域的整个价值链中，我们概述的这些发展可能会导致**运行/使用LLM的成本显著降低**。\n\n以下是我们对这一趋势的几点思考：\n\n* **优秀的B2C产品**，因为LLM成本降低意味着可以构建具有高LLM使用频率和规模（例如，长上下文窗口）的免费增值B2C体验，而不会破坏公司的单位经济。\n* 全球范围内的访问民主化，使得**低收入国家的用户**能够利用先进的AI技术。\n* 公司可以自动化更广泛的任务，从而实现**效率和生产力的提升**（“我不再关心每小时有1万次API调用”）。\n* 新的边缘AI硬件结合更小的模型将导致**新的边缘AI用例**变得可行，这些用例之前仅限于“数据中心”。\n* 随着边缘硬件的快速发展，我们相信有机会建立软件公司，帮助客户将AI模型带入定制边缘设备的碎片化空间（“把你的模型给我，我用各种技术进行压缩，在10种不同的边缘设备上测试，告诉你哪个效果最好，然后帮助你部署”）。\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/llama-3-1-405b-how-to-use-for-free-9aaf3561932d","frontmatter":{"title":"Llama 3.1 405B——如何免费使用","meta_title":"Llama 3.1 405B——如何免费使用","description":"无需本地安装","date":"2024-10-29T05:09:24.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*db_ND6LyQ5_p5jFJCTo5GQ.jpeg","categories":["Programming","Technology","Generative AI"],"author":"Rifx.Online","tags":["Llama","Meta","HuggingChat","Groq","API"],"draft":false,"slug":"blog/llama-3-1-405b-how-to-use-for-free-9aaf3561932d"},"content":"\n\n\n### 无需本地安装\n\n**Llama 3\\.1 405B** 是Meta于2024年7月发布的最先进的AI模型——**但你可以在哪里试用它*？***\n\n\n\n**LLama 3\\.1** 有不同的版本，包括参数最多的4050亿模型以及较小的70B和8B模型。\n\n试用70B和8B模型的最简单方法是在[Groq](https://console.groq.com/playground)上——你可以直接在他们的游乐场中试用它们。\n\n由于需求量巨大，最强大的405B模型通常不可用。\n\n本指南适用于任何想要免费试用Llama 3\\.1 405B的用户，包括开发者——无需下载和安装。\n\n如果你没有付费的Medium账户，可以在[这里](https://addison-best.medium.com/9aaf3561932d?source=friends_link&sk=5fa532d1caaec229a0b9a445d8749449)免费阅读。\n\n如果你是开发者，并且想要通过API免费试用**LLama 3\\.1 405B——**你可以跳到文章的末尾。\n\n## 我在哪里可以免费使用 Llama 3\\.1 405B？\n\n您可以直接从 [Meta](https://llama.meta.com/) 下载并安装它——但它非常庞大，您需要数百个千兆字节的空间和一台强大的计算机才能正确尝试。\n\n但您现在也可以在不下载的情况下进行尝试。\n\n以下是一些您可以尝试的选项：\n\n**如果您想了解更多 AI 小技巧，以帮助您的业务增长并在线赚取更多收入：**\n\n***👉*** *注册我们的 **[免费 5 天电子邮件课程](https://aigrowthguys.com/5-day-free-course-how-to-grow-your-business-like-a-weed)**，实现增长 🚀 并赚取**💲👈***\n\n## 1\\. 在Meta AI上使用Llama 3\\.1 405B\n\n如果你在美国，至少在加拿大（我所在的地方），你可以通过Meta AI与Llama 3\\.1 405B模型聊天。访问[Meta AI网站](https://www.meta.ai)，并使用你的Facebook或Instagram账户登录。\n\n它现在也可能在其他国家可用，所以可以看看。\n\n当你登录时——希望你能看到尝试**Llama 3\\.1 405B**的选项。\n\n如果可以，你会看到如下截图中的消息。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*cw1WMKhdZhzUp0L3Kn7Qng.png)\n\n你也可以通过链接你的Meta账户来通过WhatsApp访问它。[**在Meta AI上尝试**](https://www.meta.ai)\n\n你还可以尝试他们的**Imagine**照片创作工具和AI图像编辑器**。**\n\n文章开头的那幅带有拉马和电脑的卡通图像就是用这个工具创建的。\n\n**我提示了**\n\n> **Imagine: 我想要一幅有趣的卡通图像，用于中等文章，展示尝试使用Llama 3\\.1 405B**\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8MeC_M2O7UX7ulPOfUCuHA.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*dIG62eA7YAT3mpLA0etz9Q.png)\n\n值得一试。我认为它无法与Flux.1或Midjourney相提并论——但它易于使用且免费。\n\n## 2\\. 在 HuggingChat 上使用 Llama 3\\.1 405B\n\nHuggingChat 对美国以外的用户开放，并提供对 Llama 3\\.1 405B 模型的访问。您可以立即开始聊天，无需注册，这使得探索模型的能力变得简单。访问 [HuggingChat 页面](https://huggingface.co) 开始。[在 HuggingChat 上试用](https://huggingface.co)\n\n## 3\\. 在 Groq 上使用 Llama 3\\.1 405B\n\n**如何：** Groq 最初托管了 Llama 3\\.1 405B 模型，但现在由于需求量大，提供了更小的 70B 和 8B 版本。您可以通过在 [Groq 的网站](https://groq.com) 上创建一个免费账户来探索这些模型。[在 Groq 上试用](https://groq.com)\n\n## 4\\. 在 Perplexity 上使用 Llama 3\\.1 405B\n\nPerplexity 提供了一种简单的方式与 Llama 3\\.1 进行交互，旨在快速便捷地访问该模型。您可以通过访问 Perplexity AI 平台开始使用它。但这仅在 Pro 计划中可用。[在 Perplexity 上尝试](https://www.perplexity.ai)\n\n## 5\\. 在 Poe 上使用 Llama 3\\.1 405B\n\nPoe 是 Quora 提供的另一个平台，您可以在这里尝试 Llama 3\\.1。Poe 允许用户通过聊天界面探索不同的 AI 模型，包括 Llama 3\\.1。如果您想在一个地方比较 Llama 3\\.1 和其他 AI 模型，这是一个多功能的选择。您可以免费尝试 3\\.1 405B — 每天有有限的免费积分。[在 Poe 上尝试](https://poe.com)\n\n## 我在哪里可以免费使用 Llama 3\\.1 405B API？\n\n如果你是开发者并想完全免费尝试 Llama 3\\.1 405B 版本——目前你的选择有限。\n\n但我想给你一个简单且免费的选项，让你可以开始使用。\n\n你现在可以在 [together.ai](https://together.ai) 上免费尝试。\n\n你可以获得 $5 的免费额度和一个 API 密钥来进行尝试。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*w8LOXw-Wm0QTz5YgvZ27ug.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*YpKURkmy--xstoJpZ4fmbw.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*g0FxHkg6gq5OMXXo1Yzr0A.png)\n\n这是我找到的快速且免费的测试 Llama 3\\.1 405B 版本的最简单方法。\n\n这是一个很好的选项，适合希望免费尝试使用 API 的开发者。\n\n## 注意：\n\n如果您希望我们的团队使用LLMs创建定制的AI软件，或为您的业务创建定制的AI聊天机器人，您可以在这里[**联系我**](https://aigrowthguys.com/contact/) ✉️，我会尽快回复您：\n\n[**AI Growth Guys 联系**](https://aigrowthguys.com/contact/)✉️\n\n👉 注册我们的[**免费5天电子邮件课程**](https://aigrowthguys.com/5-day-free-course-how-to-grow-your-business-like-a-weed/)，在AI时代蓬勃发展🚀并赚取💲\n\n您还可以[**注册我的新闻通讯**](https://ai-growth-guys.beehiiv.com/subscribe/?via=andrew-best)，了解如何利用AI赚取更多收入。\n\n查看我们的[**YouTube频道**](https://www.youtube.com/@aigrowthguys)\n\n在我们的网站上关注我们：[**AI Growth Guys**](https://aigrowthguys.com/)\n\n"},{"lang":"zh","group":"blog","slug":"blog/meet-ministral-3b-and-8b-edge-ai-game-changers-3f7532da8f90","frontmatter":{"title":"认识 Ministral 3B 和 8B：边缘 AI 游戏规则改变者","meta_title":"认识 Ministral 3B 和 8B：边缘 AI 游戏规则改变者","description":"Mistral AI 在边缘 AI 和设备计算领域的新前沿","date":"2024-11-01T03:55:06.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*3CmWlEiW7ea8gtqxpI83_w.png","categories":["Technology","Autonomous Systems","Data Science"],"author":"Rifx.Online","tags":["Mistral","edge","computing","translation","robotics"],"draft":false,"slug":"blog/meet-ministral-3b-and-8b-edge-ai-game-changers-3f7532da8f90"},"content":"\n\n\n### Mistral AI在边缘AI和设备计算的新前沿\n\n在快速发展的AI领域，边缘计算变得越来越重要，适用于那些需要低延迟、以隐私为首的高效推理的应用，而不依赖于基于云的基础设施。\n\n**Mistral AI**最新推出的[**Ministral**](https://mistral.ai/news/ministraux/)模型家族，标志着AI领域的一次突破性进展。\n\n为庆祝其开创性的**Mistral 7B**模型发布一周年，Mistral AI推出了下一代语言模型：**Ministral 3B**和**Ministral 8B**，统称为“[**les Ministraux**](https://mistral.ai/news/ministraux/)”。这些模型不仅仅是渐进式的改进；它们代表了边缘AI可能性的重大飞跃。\n\n\n\n## 为什么这些模型很重要？\n\n边缘人工智能的核心在于在本地执行复杂计算，确保数据隐私并减少响应时间。通过 **Ministral 3B** 和 **Ministral 8B**，Mistral AI 提供了将高计算能力与内存效率相结合的模型，所有这些都可以直接在设备上运行。这些模型旨在为无法承受延迟或依赖云连接的应用程序提供实时洞察。\n\n## 主要特点：\n\n1. **最先进的性能**：在知识、常识、推理、原生函数调用和小于10B类别的效率等不同任务中超越现有模型。\n2. **大上下文窗口**：支持最多128k的上下文长度，实现更全面的理解和生成。\n3. **高效架构**：Ministral 8B采用特殊的交错滑动窗口注意力模式，实现更快和更节省内存的推理。\n4. **多功能性**：适用于广泛的应用，从设备内翻译到自主机器人。\n5. **隐私优先设计**：为本地推理而构建，这些模型非常适合优先考虑数据隐私的应用，消除了对持续云访问的需求。\n6. **可扩展性**：无论是需要Ministral 3B的低功耗消耗的小型设备，还是需要8B变体的更大能力，这两种模型都足够灵活，可以适应各种用例。\n\n> 有关基准测试结果，请参阅 [这里](https://mistral.ai/news/ministraux/)\n\n## 分析模型：\n\n### Ministral 3B:\n\n* 仅凭 **30亿个参数**，为资源受限的环境提供了平衡的解决方案\n* 支持最高 **128k 上下文长度**，可以全面处理复杂查询\n* 适用于超低延迟应用\n* 在同类模型中表现优于许多其他模型\n\n### Ministral 8B:\n\n* 具有 **80亿参数** 和 **128k上下文长度**，在处理更复杂的任务时能够提供增强的计算能力\n* 采用 **滑动窗口注意力** 模式，提高速度和内存效率\n* 基于广泛的 **多语言** 和 **代码** 数据，使其适用于多种应用\n* 支持 **函数调用**\n* 在高要求的应用中平衡性能和效率\n* 词汇量为 **131k**，使用 **V3-Tekken** 分词器\n* 提示模板：\n\n\n```python\n<s>[INST]user message[/INST]assistant response</s>[INST]new user message[/INST]\n```\n\n## 用例：\n\n这些模型提供了计算高效和低延迟的性能，使其非常适合以下场景：\n\n* **设备端翻译**：使用户能够在实时中无缝沟通跨语言，即使在网络连接较差的地区。\n* **无网络智能助手**：支持独立于云连接运行的智能虚拟助手，增强隐私敏感环境中的用户体验。\n* **本地分析**：使组织能够实时分析数据，同时保持严格的隐私标准，这在医疗和金融等行业至关重要。\n* **自主机器人**：为机器人配备先进的语言能力，以实现自主决策和沟通，提高它们在各个行业的运营效率。\n\n除了其独立的能力外，les Ministraux 还可以与更大的模型如 Mistral Large 协同工作。这种协同使它们能够作为 **在代理工作流中进行函数调用的高效中介**，处理：\n\n* **输入解析**：快速解释用户输入，以确保准确响应。\n* **任务路由**：根据用户意图将请求指向适当的资源。\n* **API 调用**：实时执行 API 功能，确保在各种上下文中顺畅互动。\n\n## 代码使用（与 vLLM 一起）：\n\n[Ministral\\-8B\\-Instruct\\-2410](https://huggingface.co/mistralai/Ministral-8B-Instruct-2410) 语言模型是一个经过指令微调的模型，可以使用 vLLM 高效部署。您可以在 Hugging Face 上 [这里](https://huggingface.co/mistralai/Ministral-8B-Instruct-2410) 找到它。以下是您可以开始的方式：\n\n### 安装\n\n首先，确保您已安装最新版本的 vLLM 和 mistral\\_common：\n\n\n```python\npip install --upgrade vllm\npip install --upgrade mistral_common\n```\n\n> ***注意****：需要 vLLM 版本 0\\.6\\.2 或更高版本。*\n\n### 离线使用 vLLM\n\n以下是如何在离线模式下使用 Ministral\\-8B 和 vLLM 的示例：\n\n\n```python\nfrom vllm import LLM\nfrom vllm.sampling_params import SamplingParams\n\nmodel_name = \"mistralai/Ministral-8B-Instruct-2410\"\nsampling_params = SamplingParams(max_tokens=8192)\n\nllm = LLM(model=model_name, tokenizer_mode=\"mistral\", config_format=\"mistral\", load_format=\"mistral\")\n\nprompt = \"What are the potential implications of artificial intelligence on the job market in the next decade?\"\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": prompt\n    },\n]\n\noutputs = llm.chat(messages, sampling_params=sampling_params)\nprint(outputs[0].outputs[0].text)\n```\n\n### 服务器模式推理与 vLLM\n\n在服务器推理模式下，vLLM 运行一个 HTTP 服务器，能够通过与 OpenAI 协议兼容的 REST API 同时处理客户端连接和请求。以下是设置方法：\n\n* 启动服务器：\n\n```python\nvllm serve mistralai/Ministral-8B-Instruct-2410 --tokenizer_mode mistral --config_format mistral --load_format mistral\n```\n* 向服务器发送请求：\n\n```python\ncurl --location 'http://localhost:8000/v1/chat/completions' \\\n    --header 'Content-Type: application/json' \\\n    --header 'Authorization: Bearer token' \\\n    --data '{\n        \"model\": \"mistralai/Ministral-8B-Instruct-2410\",\n        \"messages\": [\n          {\n            \"role\": \"user\",\n            \"content\": \"What are the potential implications of artificial intelligence on the job market in the next decade?\"\n          }\n        ]\n      }'\n```\n\n> 关于 vLLM 使用的重要说明：\n\n* 目前，由于在实现分页注意力的交错注意力内核方面的限制，vLLM 的上下文大小限制为 32k。\n* 为了利用完整的 128k 上下文大小，建议使用 [Mistral Inference](https://github.com/mistralai/mistral-inference)。\n* 如果您需要减少 GPU 内存需求，可以通过在 LLM 初始化时添加 `tensor_parallel=2` 来使用张量并行。\n\n通过遵循这些示例，您可以轻松地将 Ministral\\-8B 集成到您的项目中，无论您是在离线推理还是为多个客户端设置服务器。该模型的高效性和强大功能，加上 vLLM 的优化推理，使其成为各种 AI 应用的优秀选择。\n\n## 结论：\n\nMinistral的发布标志着人工智能发展中的一个重要里程碑。通过将GPT级别的性能带到边缘设备，Mistral AI不仅在推动技术边界——他们还在重新构想以本地、隐私优先的人工智能为基础的可能性。\n\n随着开发者、研究人员和企业开始探索Ministral的能力，我们可以期待看到一波新的人工智能驱动的应用程序，这些应用程序比以往更加快速、更具隐私性和更易于获取。边缘人工智能的时代已经到来，而Ministral正在引领这一潮流。\n\n"},{"lang":"zh","group":"blog","slug":"blog/mistral-ai-releases-revolutionary-edge-models-ministral-3b-and-8b-superior-performance-and-privacy-5b24f0189493","frontmatter":{"title":"Mistral AI 发布革命性边缘模型 Ministral 3B 和 8B：卓越性能和隐私","meta_title":"Mistral AI 发布革命性边缘模型 Ministral 3B 和 8B：卓越性能和隐私","description":"没有提供字幕","date":"2024-10-31T08:32:15.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*zFNeFlbfEnbjV5M65sH5ig@2x.jpeg","categories":["Technology","Machine Learning","Autonomous Systems"],"author":"Rifx.Online","tags":["edge","models","privacy","tokens","attention"],"draft":false,"slug":"blog/mistral-ai-releases-revolutionary-edge-models-ministral-3b-and-8b-superior-performance-and-privacy-5b24f0189493"},"content":"\n\n\n最近，Mistral AI 推出了两个新的边缘模型——Ministral 3B 和 Ministral 8B，这引起了科技界的广泛关注。这些模型不仅在性能上表现出色，还在隐私保护方面提供了独特的优势。\n\n\n\n## 卓越性能，隐私优先\n\nMinistral 3B 和 8B 专为设备内计算而设计，能够处理长度达 128k 的文本信息。特别是，Ministral 8B 采用了创新的滑动窗口注意力机制，显著提高了计算速度和内存效率。此外，这两个模型在设计上优先考虑隐私保护，确保数据在本地处理，以降低数据泄露的风险。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GMgT6erSorAGUp-pqbXWhA@2x.jpeg)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*zRGh7rw7oVXYd5mOhXoc3g@2x.jpeg)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*IIYgXVtbHvWqn6QLSZ-0Ow@2x.jpeg)\n\n## 多功能应用，无限潜力\n\nMinistral系列模型具有广泛的应用。在智能助手领域，它们可以快速响应用户命令，同时确保数据安全；在自主机器人领域，它们强大的推理能力支持复杂的决策和操作。\n\n## 成本效益高，广阔的市场前景\n\n尽管表现出色，Ministral 3B 和 8B 的价格竞争力依然很强。3B 的价格为每百万个令牌 $0.04，而 8B 的价格为 $0.10。这一定价策略为企业和开发者提供了一个成本效益高的选择。目前，这两个模型均可供使用。\n\n## 前景光明，引领边缘计算新趋势\n\nMistral AI 发布的 Ministral 系列模型展示了其在边缘计算方面的深厚技术实力，为未来的设备端 AI 应用奠定了坚实基础。随着技术的进步和应用的深入探索，Ministral 模型预计将在智能设备和物联网中发挥更大作用。\n\n总之，Ministral 3B 和 8B 的推出不仅是 Mistral AI 的一个重要里程碑，也是 AI 行业的一次重大进步，为设备端计算带来了新的可能性。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*A6SToo3fO3DqnlWX)\n\n"},{"lang":"zh","group":"blog","slug":"blog/mistral-ai-unveils-ministral-3b-and-8b-models-plus-nvidia-launches-ai-model-that-outperforms-gpt-4-941712f5d22d","frontmatter":{"title":"Mistral AI 推出 Ministral 3B 和 8B 模型 另外：Nvidia 推出优于 GPT-4 的 AI 模型","meta_title":"Mistral AI 推出 Ministral 3B 和 8B 模型 另外：Nvidia 推出优于 GPT-4 的 AI 模型","description":"没有提供字幕","date":"2024-10-31T08:29:07.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*PtPEkgjabwBUu73Y","categories":["Technology","Generative AI","Machine Learning"],"author":"Rifx.Online","tags":["Mistral","edge","Llama","YouTube","DreamTracks"],"draft":false,"slug":"blog/mistral-ai-unveils-ministral-3b-and-8b-models-plus-nvidia-launches-ai-model-that-outperforms-gpt-4-941712f5d22d"},"content":"\n\n\n### Plus: Nvidia推出的AI模型超越GPT\\-4\n\n\n\n**欢迎来到Get The Gist**，在这里我们每个工作日分享最新的AI发展动态——新闻、创新和趋势——所有内容都在5分钟内轻松阅读！⏱\n\n**在今天的版本中：**\n\n* Mistral AI推出了用于边缘计算的Ministral 3B和8B模型\n* Nvidia悄然推出的AI模型超越GPT\\-4\n* YouTube向美国创作者推出AI音乐工具“梦幻曲目”\n* Google Gemini现在可以生成可自定义宽高比的图像\n* 还有更多AI新闻……\n\n## 1\\. Mistral AI 发布 Ministral 3B 和 8B 模型以支持边缘计算\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*qAjoYMHGI1TkNy_A)\n\n**要点:** Mistral AI 已经[**推出了两个新的 AI 模型**](https://analyticsindiamag.com/ai-news-updates/mistral-ai-launches-ministral-3b-and-8b-models-for-edge-computing/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models)，Ministral 3B 和 8B，旨在实现高效的设备内和边缘计算。这些模型在性能上超越了竞争对手，并且专为需要隐私优先、本地推理的任务而设计。\n\n**关键细节:**\n\n* 模型处理大上下文长度（最长可达 128k），在资源有限的环境中实现流畅性能。\n* 适用于智能助手、本地分析和机器人等应用，提升任务效率。\n* 以具有竞争力的定价提供商业使用，并为 8B Instruct 模型提供研究访问。\n* 在基准测试中超越了 Gemma 2 和 Llama 3 等 AI 模型。\n\n## 2\\. Nvidia 安静推出超越 GPT\\-4 的 AI 模型\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Mza84SHereM3w5rN)\n\n**要点:** Nvidia [**发布了一款新 AI 模型**](https://venturebeat.com/ai/nvidia-just-dropped-a-new-ai-model-that-crushes-openais-gpt-4-no-big-launch-just-big-results/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models)，Llama\\-3.1\\-Nemotron\\-70B\\-Instruct，其性能基准超越了行业巨头如 OpenAI 的 GPT\\-4。这次发布标志着 Nvidia AI 战略的重大扩展，从硬件转向高性能 AI 软件。\n\n**关键细节:**\n\n* Nvidia 的新模型在关键基准测试中得分高于 GPT\\-4，展示了卓越的语言理解和生成能力。\n* 该模型采用了先进技术，如人类反馈强化学习 (RLHF)，在处理复杂查询方面表现出色。\n* Nvidia 通过其平台提供免费访问，允许企业试用这一强大的 AI 工具。\n* 该模型可根据业务需求进行定制，但在法律推理或数学等专业领域的使用需要谨慎。\n\n## 3\\. YouTube 在美国推出 AI 音乐工具 “Dream Tracks”\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*5nUNrJmdCBBy4JdQ)\n\n**要点:** YouTube 在美国推出了其 [**AI 驱动的音乐生成器**](https://www.mediapost.com/publications/article/400280/youtube-brings-ai-audio-generator-to-us-creators.html?edition=136037&utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models) “Dream Tracks”，允许创作者使用文本提示为他们的短视频创建自定义音频。该工具旨在通过音乐创作加深艺术家与粉丝之间的联系。\n\n**主要细节:**\n\n* 由 Google DeepMind 的 Lyria 提供支持，Dream Tracks 为 YouTube Shorts 生成定制的乐器配乐。\n* 美国创作者现在可以使用此工具创建最长 30 秒的免版税配乐。\n* 用户可以对 AI 生成的音频片段进行混音，增强创作可能性。\n* YouTube 对所有 AI 生成的曲目应用隐藏的 SynthID 水印，以确保透明度。\n\n## 快速摘要\n\n* **Clerk Chat** 获得了由 Race Capital 领投的 700 万美元融资，以增强其 AI 驱动的商业沟通平台 [(阅读更多)](https://www.businesswire.com/news/home/20241017292794/en/World%E2%80%99s-First-AI-Telecom-Clerk-Chat-Raises-7.0-Million-in-Seed-Funding?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models)。\n* **Anthropic** 首席执行官 Dario Amodei 发布了一篇长篇博文，阐述了对人工通用智能变革潜力的乌托邦愿景，同时寻求为公司争取 400 亿美元的估值 [(阅读更多)](https://www.theverge.com/2024/10/16/24268209/anthropic-ai-dario-amodei-agi-funding-blog?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models)。\n* **Google Cloud** 宣布其升级版 Vertex AI 平台和医疗数据引擎的正式上线，以增强医疗领域的 AI 应用 [(阅读更多)](https://www.forbes.com/sites/saibala/2024/10/17/google-cloud-announces-general-availability-of-vertex-ai-for-healthcare/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models)。\n* **Amazon** 领投了 5 亿美元的融资轮，为 X-energy 推出到 2039 年的 5GW 小型核反应堆，而 **Google** 与 Kairos Power 合作，计划到 2035 年安装 500MW 的小型模块化反应堆，二者都旨在利用清洁能源满足数据中心日益增长的能源需求 [(阅读更多)](https://www.theengineer.co.uk/content/news/amazon-and-google-bet-big-on-smrs-to-power-ai?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models)。\n* **Google** 将于 2025 年初在 Google Distributed Cloud 中为公共部门机构推出其 Gemini AI 模型，并提供资金以提升政府员工在负责任的 AI 实践方面的技能 [(阅读更多)](https://siliconangle.com/2024/10/16/google-looks-spearhead-ai-adoption-public-sector/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models)。\n* **Google** 的 Gemini AI 聊天机器人将推出一项功能，允许用户以可自定义的宽高比生成图像，增强其图像编辑能力 [(阅读更多)](https://indianexpress.com/article/technology/artificial-intelligence/google-gemini-may-soon-get-new-image-resizing-feature-9623756/?utm_source=getthegist.beehiiv.com&utm_medium=referral&utm_campaign=mistral-ai-unveils-ministral-3b-and-8b-models)。\n\n今天就到这里，明天见！👋\n\n如果您喜欢这个更新并希望了解 AI 的最新动态，请考虑在 Medium 上订阅 ***Get The Gist***，获取更多见解和分析。\n\n**想要更深入了解吗？** 订阅我们的免费每日电子邮件通讯，快速获取简洁的更新，确保您不会错过任何重要进展。您可以通过点击 [这里](https://getthegist.beehiiv.com/) 注册。\n\n让我们一起探索 AI 的世界——每次摘要都是一次新发现！💡🤖\n\n"},{"lang":"zh","group":"blog","slug":"blog/o1-preview-vs-claude-3-5-sonnet-comparing-top-llms-d68734b53c93","frontmatter":{"title":"o1-preview 与 claude-3.5-sonnet：比较顶级法学硕士","meta_title":"o1-preview 与 claude-3.5-sonnet：比较顶级法学硕士","description":"了解 OpenAI 的 o1 预览版与 Claude 3.5 Sonnet 在性能、速度和功能方面的比较。","date":"2024-10-27T13:58:01.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kTWAcpRdOpsrFIDZjjjr7Q.jpeg","categories":["Programming","Machine Learning","Generative AI"],"author":"Rifx.Online","tags":["o1-preview","Claude","throughput","latency","reasoning"],"draft":false,"slug":"blog/o1-preview-vs-claude-3-5-sonnet-comparing-top-llms-d68734b53c93"},"content":"\n\n\n今天（2024年9月12日），OpenAI 发布了其最新的语言模型 o1-preview。这个先进的模型经过设计，能够在生成响应之前投入更多时间进行处理，使其能够更好地应对复杂任务，并在科学、编码和数学等领域解决具有挑战性的问题。\n\n在这篇博客文章中，我们将深入分析 o1-preview，并将其与之前被认为是最先进模型之一的 Claude 3.5 Sonnet 进行比较。\n\n\n\n## 比较方法论\n\n我们的分析利用了 [Keywords AI 的 LLM playground](https://docs.keywordsai.co/features/prompt/model-playground)，这是一个支持超过 200 种语言模型并提供函数调用功能的平台。我们将探讨以下几个方面：\n\n* 基本比较\n* 基准比较\n* 处理速度\n* 评估指标\n* 建议的使用案例\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*yc171ikejtBy_o11.jpeg)\n\n## 基本比较\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*z2FrS_AVig7Y6eU_.jpeg)\n\n注意：o1-preview 不支持流式传输、函数调用和系统消息。\n\n## 基准比较\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Bx_vAvFc9DAD0cZA.jpeg)\n\nO1-preview 在所有基准测试中都优于 Claude 3.5 Sonnet。最小的差距出现在 MMLU（一般知识）中。GPQA Diamond 测试研究生水平的推理，显示出显著的性能差异。MATH 基准揭示了最大的差距，突显了 o1-preview 的高级数学能力。这些结果表明，o1-preview 在复杂推理和各个领域的问题解决方面有了显著改善。\n\n## 速度比较\n\nO1-preview 的思考和响应时间比其他 LLM 更长。虽然直接的速度比较可能并不完全公平，但测试 o1-preview 的速度至关重要。这些信息帮助开发者更好地理解 o1-preview 的能力，并判断它是否适合他们的项目。注意：由于 o1-preview 不支持流式传输，我们已为两个模型禁用流式传输。因此，无法测量首次令牌时间（TTFT）。\n\n## 延迟\n\n我们的测试涉及每个模型数百个请求，揭示了显著的差异。Claude 3.5 Sonnet 的平均延迟为 18.3 秒/请求，而 o1-preview 的平均延迟为 39.4 秒/请求。o1-preview 显著更长的延迟是由于其延长的思考和推理过程。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*2PMkgPVuylFxwfIa.jpeg)\n\n## 吞吐量（每秒令牌数）\n\n尽管延迟较高，o1-preview的吞吐量更为出色。o1-preview生成92.94个令牌/秒，而Claude 3.5 Sonnet生成74.87个令牌/秒。这表明o1-preview较长的生成时间主要是由于其初始处理阶段，而非令牌生成速度。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*wxqpnwZhl9pnbw8y.jpeg)\n\n## 性能比较\n\n我们在[Keywords AI平台](https://keywordsai.co/)上进行了评估测试。评估包括三个部分：\n\n* **编码任务**：两个模型成功完成了前端和后端开发任务。O1-preview在处理较长上下文时表现更佳，能够在第一次尝试中更有效地识别和解决bug。它还展现了更全面的代码分析能力。\n* **逻辑推理**：O1-preview在推理任务中表现出色。它的思维过程与人类认知非常相似。虽然Claude 3.5 Sonnet在大多数问题上表现良好，但o1-preview始终能够解决复杂的推理挑战，包括国际数学奥林匹克（IMO）级别的问题。\n* **写作任务**：两个模型在写作任务上表现非常出色。它们展现了撰写真实、个性化的冷邮件以及简洁且有意义的博客文章的能力。\n\n## 模型推荐\n\no1-preview\n\n* **最佳选择：** 适用于数学、编码和物理学中的复杂问题解决。特别适合处理挑战性任务的研究人员。\n* **不适合：** 需要快速响应时间或严重依赖系统提示的AI应用。由于缺乏流媒体支持，不适用于语音AI应用。\n\nClaude 3.5 Sonnet\n\n* **最佳选择：** 适用于大多数需要问题解决能力和高质量内容生成的AI应用。\n* **不适合：** 语音AI应用或对预算限制严格、需要较低运营成本的项目。\n\n## 如何将 o1-preview 集成到您的 AI 应用中\n\n要将 o1-preview 集成到您的 AI 应用中，只需访问 Keywords AI 模型页面并找到“查看代码”按钮。点击此按钮以复制提供的代码片段，然后将其直接粘贴到您的代码库中。通过这个简单的过程，您将能够在项目中利用 o1-preview 的强大功能，使您能够轻松应对复杂问题并生成高质量内容。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*XyQ9QiI7TN8Uc5Jp.jpeg)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*t8fEYlEs13eM7D28lVbtIw.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*yhu9y5ixNuxeFVe1.png)\n\n此故事发布在 [Generative AI](https://generativeai.pub/)。请在 [LinkedIn](https://www.linkedin.com/company/generative-ai-publication) 上与我们联系，并关注 [Zeniteq](https://www.zeniteq.com/)，以便获取最新的 AI 资讯。\n\n订阅我们的 [newsletter](https://www.generativeaipub.com/) 和 [YouTube](https://www.youtube.com/@generativeaipub) 频道，及时了解生成 AI 的最新消息和动态。让我们共同塑造 AI 的未来！\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*PelNtaNaEVDWgMWr.png)\n\n"},{"lang":"zh","group":"blog","slug":"blog/openai-01-preview-secrets-99-of-people-dont-know-b0c5e4bb4f76","frontmatter":{"title":"OpenAI 01-预览 — 99% 的人不知道的秘密","meta_title":"OpenAI 01-预览 — 99% 的人不知道的秘密","description":"如何充分利用 01-preview","date":"2024-11-01T03:58:01.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*wRAXNmhEzkGNagMl5Papxg.jpeg","categories":["Programming","Machine Learning","Technology/Web"],"author":"Rifx.Online","tags":["OpenAI","01-preview","iterative","problem-solving","planning"],"draft":false,"slug":"blog/openai-01-preview-secrets-99-of-people-dont-know-b0c5e4bb4f76"},"content":"\n### 如何充分利用01\\-preview\n\n自从01\\-preview发布以来，我一直在玩它。\n\n我非常喜欢它！\n\n我甚至在我的新[**AI增长黑客课程**](https://aigrowthguys.com/growth-hacking-course-sign-up/)中教授它。\n\n我很高兴分享一些关于如何充分利用它的关键见解。\n\n\n\n大多数人对01\\-preview的工作原理一无所知。\n\n首先，它不仅仅是一个“思考”模型。\n\n在您能够充分利用它之前，您需要了解一些它的工作原理。\n\n如果您没有付费的Medium账户，可以在[**这里**](https://readmedium.com/openai-01-preview-secrets-99-of-people-dont-know-b0c5e4bb4f76?sk=12140ffad09d922bc00a8a4aa312a286)免费阅读。\n\n👉 注册我们的免费5天电子邮件课程，助力成长🚀并在AI时代赚取💲\n\n## OpenAI 01\\-preview 如何工作？\n\n01\\-preview 并不是真正的新模型。\n\n它结合了其他模型和一个“系统提示”，告诉它在输出响应之前进行多次迭代。\n\n所有其他模型都是通过提供模型想到的第一个响应来工作的。\n\n01\\-preview 的设计目的是在最终答案出来之前进行规划和实验。\n\n一个例子会有所帮助。\n\n> 想象一下，你告诉 GPT\\-4o 写一个恰好 80 个单词的连贯段落，并且“tomato”这个词是第 4 个、第 19 个和第 72 个单词。\n\nGPT\\-4o（以及所有其他模型）在这个任务上会失败，因为仅仅吐出第一个想到的答案太困难了。\n\n这种类型的问题需要实验。\n\n想想如果你被赋予了同样的任务。\n\n你需要“玩弄”这个任务，试图将“tomato”这个词放在这些位置上，以一种合理的方式。\n\n你不能仅仅开始写作，然后看看会发生什么。\n\n你会意识到你需要调整一些句子和单词，以便将“tomato”这个词放进去。\n\n此外，当你接近 80 个单词时，你需要计划如何准确地停在这个数字上。例如，你可能希望回去删除第一句中的一个多余单词。\n\n01\\-preview 能够做到这一点的原因在于它的“思考”方式。\n\n它首先会将问题分解，并说一些类似于“想出一个解决这个问题的计划”的话。\n\n然后，它会写出一个大致的初步猜测（可能使用 GPT\\-4o）。\n\n接着，它会对自己说：“重新阅读问题，看看是否可以进行任何调整或修改”。\n\n然后它会说：“再检查一遍，看看你的回答是否完美。如果是，就展示出来；如果不是，继续调整”。\n\n然后它会说：“重复这个过程，直到你的答案 100% 完美。始终记得在展示最终答案之前进行双重检查”。\n\n例如，第一次响应的第一句可能是这样的。\n\n“Sandy picked a red tomato from her garden.”\n\n然后 01\\-preview 会将其更改为：“Sandy picked a tomato from her garden”。\n\n这样，它就成功地将“tomato”这个词从第 5 个单词移动到了第 4 个单词。\n\n它会通过与自己进行内部对话不断进行调整。\n\n## 如何充分利用 01\\-preview？\n\n现在您对 01\\-preview 的“思维”有了一定了解，可以开始理解如何充分利用它。\n\n您需要将自己的问题分为需要“思考”的问题和不需要的。\n\n许多问题不需要模型进行“思考”。\n\n例如，如果您告诉它为您写一个关于一个名叫 Sandy 的女孩和她的番茄花园的有趣故事，那么您就不需要使用 01\\-preview。\n\n**为什么不呢？**\n\n因为约束条件很少。\n\n有很多方法可以做到这一点。它本质上是开放式的。\n\n这个故事不需要有特定的长度。\n\n模型可以直接开始写，插入一两个笑话，然后就完成了。\n\n它不需要回到第一句去计算单词数量或其他任何事情。\n\n关键是：\n\n如果您向模型请求一些特定的内容，而这些内容在一次尝试中很难做到且需要实验，那么您应该使用 01\\-preview。\n\n如果您请求的是开放式的内容，那么使用其他模型。\n\n您需要谨慎使用 01\\-preview，因为您只能获得有限数量的查询。\n\n它的限制总是比其他模型更多，因为它使用的资源远远超过其他模型。\n\n好消息是，01\\-preview 会比其他模型犯更少的错误。\n\n此外，它能够回答之前模型无法解决的问题。\n\n现在是学习如何利用 AI 来发展您的业务和赚取更多收入的最佳时机。\n\n我在我的 AI 增长黑客课程中教授如何使用这个。\n\n我还将结合这个模型，使我构建的自定义 AI 代理和聊天机器人更加准确。\n\n这将使像 [**Stammer**](https://stammer.ai/?via=andrew) 这样的 AI 代理构建者更加强大。\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/openai-confirms-the-arrival-of-gpt-5-poised-to-bring-huge-improvements-to-artificial-intelligence-e3b858e79c2a","frontmatter":{"title":"OpenAI 确认 GPT-5 即将到来，有望为人工智能带来巨大改进……","meta_title":"OpenAI 确认 GPT-5 即将到来，有望为人工智能带来巨大改进……","description":"有网友在x上发了一篇GPT5倒计时的帖子，称这是根据各平台的线索得出的结论。评论区……","date":"2024-11-01T03:58:58.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8J_opnaERs-wrq2YRKIxdQ.png","categories":["Natural Language Processing","Generative AI","Technology"],"author":"Rifx.Online","tags":["GPT-5","natural","language","efficiency","personalization"],"draft":false,"slug":"blog/openai-confirms-the-arrival-of-gpt-5-poised-to-bring-huge-improvements-to-artificial-intelligence-e3b858e79c2a"},"content":"\n\n\n一位网友在 x 上发布了 GPT5 倒计时的帖子，称这是从各个平台的线索得出的结论。评论区已经达到高潮，各种意见纷纷涌现。\n\n\n\n**原因 1** : OpenAI 网站 GPT5 泄露\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*EBDLAv3rOyCjshGBpVRI7A.png)\n\n**原因 2** : 知名美国财经网站 BusinessInsider 发布的文章“OpenAI 发布更好的 GPT5 聊天机器人”。由于该网站为付费网站，感兴趣的话可以搜索标题。以下是部分内容：\n\n这家由 Sam Altman 领导的生成式 AI 公司，预计将在年中某个时候推出 GPT-5，可能在夏季，根据两位熟悉该公司的人士的说法。根据另一位熟悉该过程的人士的说法，一些企业客户最近收到了最新模型及其与 ChatGPT 工具相关的增强功能的演示。Business Insider 已确认这些人的身份，他们要求匿名以便能够自由发言。\n\n根据在 X 和其他平台上的讨论，新的模型版本很可能将在 6 月 6 日推出，但尚不确定是否为 GPT 4.5 或 GPT5。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*rhApTugfrMVBB6PhMvK4rg.png)\n\n我们都在等待 **GPT5**\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*eB6j2S_dPbjQ2-sV2N1fwA.jpeg)\n\n## GPT-5的期待\n\n尽管细节仍然稀少，但围绕GPT-5的兴奋感源于对人工智能能力显著提升的期待。以下是一些可能的进展猜测：\n\n* 增强的自然语言理解：预计GPT-5将对人类语言中的上下文、细微差别和微妙之处有更深入的理解，使互动更加流畅和自然。\n* 提高效率：随着每次迭代，OpenAI在减少延迟和提高模型效率方面取得了进展。预计GPT-5将继续这一趋势，提供更快和更准确的响应。\n* 更广泛的知识基础：通过整合更多样化和广泛的数据集，GPT-5可能在更广泛的主题上提供更全面和可靠的信息。\n* 高级个性化：新模型可能包括增强的个性化功能，使其能够更好地适应个别用户的偏好和需求。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*7xCG5iy53LLQCTnmzxs_3g.jpeg)\n\n"},{"lang":"zh","group":"blog","slug":"blog/openai-gpt-5-ph-d-level-intelligence-expected-by-2025-50a86c3aad86","frontmatter":{"title":"OpenAI GPT-5：预计 2025 年将实现博士级智能","meta_title":"OpenAI GPT-5：预计 2025 年将实现博士级智能","description":"经过数月的猜测，OpenAI 终于公布了备受期待的 GPT-5 的细节。最初预计在 2024 年推出，但……","date":"2024-11-01T03:59:56.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*OasnWeS5mgAX_0hIpirO5Q.jpeg","categories":["Machine Learning","Ethics","Data Science"],"author":"Rifx.Online","tags":["GPT-5","Ph.D.","intelligence","ethics","privacy"],"draft":false,"slug":"blog/openai-gpt-5-ph-d-level-intelligence-expected-by-2025-50a86c3aad86"},"content":"\n\n\n\n\n经过几个月的猜测，OpenAI终于揭示了备受期待的GPT\\-5的详细信息。最初预计在2024年发布，但其发布时间已推迟至2025年末或2026年初。OpenAI的首席技术官Mira Murati在与达特茅斯工程学院的采访中分享了有关这个新版本的能力和潜力的见解。以下是您需要知道的一切。\n\n## 智力的量子飞跃\n\nMurati 将之前的 GPT 版本与不同水平的人类智力进行比较。GPT\\-3 类似于一个幼儿，而 [**GPT\\-4**](https://www.geekmetaverse.com/gpt-4-unveils-its-secrets-a-combination-of-8-smaller-models/) 则可与高中生相提并论。新的 GPT\\-5 承诺在特定任务上达到“博士级智力”。这一进展不仅令人兴奋，也引发了对人工智能未来的思考。\n\n## GPT的演变：从儿童到博士\n\n将这些版本与人类教育的阶段进行比较，有助于我们更好地理解这些进步。GPT-3凭借其生成连贯且有用文本的能力，打开了许多大门。GPT-4在这些技能上进行了改进，表现出在更复杂任务中的优越性能。现在，GPT-5旨在将这一切提升到一个全新的水平，具备先进的推理和记忆能力。\n\n## 专业智能\n\nPh.D.\\-级的智能并不意味着 [**GPT\\-5**](https://www.geekmetaverse.com/openai-ceo-confirms-that-gpt-5-is-already-in-development/) 可以完美地完成所有任务。Murati 澄清了这些能力将是特定于任务的。这表明，尽管 AI 可能在某些领域超越人类，但在其他领域仍然会有局限性。这种专业化的关注可能会导致在科学研究和复杂数据分析等领域产生高度精确和有用的应用。\n\n## 潜在与未来应用\n\nGPT-5 的发展为各个领域开辟了一系列可能性。从教育到医学，从研究到技术，应用广泛。\n\n### 教育与培训\n\n一款能够达到博士水平的[**AI**](https://www.geekmetaverse.com/apple-updates-ai-takes-center-stage-with-siri-integration-chatgpt-partnership-and-elon-musk-concerns/)可能会彻底改变教育。个性化辅导系统可以为学生在复杂领域提供支持，提升理解能力和学业表现。\n\n### 医学与医疗保健\n\n在医学领域，具备此类能力的人工智能可以帮助诊断罕见疾病、开发个性化治疗方案以及管理大量临床数据，从而显著推进医疗护理。\n\n### 研究与开发\n\n研究人员可以从能够分析大数据集、识别模式并生成假设的人工智能中获得极大益处，从而加速科学和技术发现的进程。\n\n## 挑战与伦理考量\n\n尽管有着良好的应用前景，但这种先进AI的发展也带来了重大的伦理挑战。如果对AI在关键任务上的过度依赖没有得到妥善管理，可能会导致一些问题。\n\n### 隐私与安全\n\n数据隐私和网络安全将是关键议题。确保人工智能系统不被滥用，以及敏感数据得到充分保护将是优先事项。\n\n### 就业影响\n\n就业影响也是一个关注点。自动化专业任务可能会取代某些专业人士，这需要采取积极措施来应对这些社会经济影响。\n\n### 结论\n\nGPT-5 发布的延迟可能让一些人感到失望，但其先进的能力引发了人们的极大期待。如果 OpenAI 实现其目标，我们可能会看到一个革命性的工具，它将改变多个行业，并改变我们与技术的互动方式。\n\n### 常见问题解答\n\n**1\\. 什么是 GPT\\-5？**\n\nGPT\\-5 是 OpenAI 的生成预训练变换器 (Generative Pre\\-trained Transformer, GPT) 系列即将推出的版本，承诺在特定任务中具备博士级别的智能。\n\n**2\\. GPT\\-5 预计何时发布？**\n\nGPT\\-5 的发布已推迟至 2025 年底或 2026 年初。\n\n**3\\. GPT\\-5 与之前的版本相比如何？**\n\nGPT\\-3 的智能相当于一个年轻儿童，而 GPT\\-4 则相当于一名高中生。GPT\\-5 旨在实现特定任务的博士级别智能，提供先进的推理和记忆能力。\n\n**4\\. GPT\\-5 能够执行哪些任务？**\n\nGPT\\-5 将专注于某些任务，在科学研究、复杂数据分析、教育和医疗等特定领域表现出色。\n\n**5\\. GPT\\-5 会在所有方面都完美吗？**\n\n不，GPT\\-5 的博士级别智能将是针对特定任务的，这意味着它在某些领域表现出色，但在其他领域仍然会有局限性。\n\n**6\\. GPT\\-5 的潜在应用是什么？**\n\n潜在应用包括教育中的个性化辅导、医疗中帮助诊断疾病和开发治疗方案，以及协助研究人员分析大数据集和生成假设。\n\n**7\\. 与 GPT\\-5 相关的伦理考虑是什么？**\n\n伦理考虑包括确保数据隐私和网络安全，管理因自动化导致的就业流失的社会经济影响，以及防止高级人工智能系统的误用。\n\n**8\\. GPT\\-5 将如何影响数据隐私和安全？**\n\n确保保护敏感数据和防止人工智能系统的误用至关重要。需要实施措施以保障数据隐私和安全。\n\n**9\\. GPT\\-5 对就业的潜在影响是什么？**\n\nGPT\\-5 对专业任务的自动化可能会使某些专业人士失业，因此需要采取积极措施来减轻这些社会经济影响。\n\n**10\\. 为什么 GPT\\-5 的发布被延迟？**\n\n延迟使 OpenAI 能够完善和增强 GPT\\-5 的能力，以确保其满足对高级智能和专业应用的高期望。\n\n**11\\. GPT\\-5 如何改变教育？**\n\nGPT\\-5 可以通过提供个性化辅导系统来彻底改变教育，支持学生在复杂学科中的理解和学业表现。\n\n**12\\. GPT\\-5 可以为医疗领域带来哪些进步？**\n\n在医学领域，GPT\\-5 可以帮助诊断罕见疾病，开发个性化治疗方案，以及管理大量临床数据，推动医疗护理的重大进展。\n\n原文链接: [https://www.geekmetaverse.com/openai\\-gpt\\-5\\-ph\\-d\\-level\\-intelligence\\-2025/](https://www.geekmetaverse.com/openai-gpt-5-ph-d-level-intelligence-2025/)\n\n"},{"lang":"zh","group":"blog","slug":"blog/openai-just-built-her-in-real-life-17769d993e11","frontmatter":{"title":"用户会爱上 OpenAI 的新 GPT-4o 模型。确实如此。","meta_title":"用户会爱上 OpenAI 的新 GPT-4o 模型。确实如此。","description":"该公司的新款 GPT-4o 可以理解和模仿人类的语言和情感","date":"2024-11-01T04:08:40.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*-bTsggApvkUHAq57YhSd-A.png","categories":["Generative AI","Chatbots","Natural Language Processing"],"author":"Rifx.Online","tags":["GPT-4o","speech","emotions","multilingual","conversational"],"draft":false,"slug":"blog/openai-just-built-her-in-real-life-17769d993e11"},"content":"\n\n\n## 公司的新 GPT\\-4o 能理解并模仿人类的语言和情感\n\n\n\n在标志性的2013年电影 *她* 中，主角与一个语音启用的人工智能系统发展出一种强烈的关系——并演变成一场爱情故事。\n\n*她* 中的人工智能是今天的语音启用系统所不具备的：富有情感、幽默，并且能够洞察人类对话的细微差别。\n\n在今天早上的一次重大[公告](https://www.youtube.com/live/DQacCB9tDaw?app=desktop&si=jvKW7jFDwFvOMBBk)中，OpenAI 宣布发布一个新版本的 ChatGPT 系统，该系统将语音、转录和智能原生集成到一个模型中。\n\n它强大、直观，并且令人不安地像人类。基本上，OpenAI 建造了一个现实版的 *她*。\n\n## 一个糟糕的对话者\n\nChatGPT已经拥有语音功能几个月了。即使在今天，您可以在手机上打开ChatGPT应用程序，按下耳机图标，用您的声音与系统对话。\n\n然而，问题在于，ChatGPT是一个糟糕的对话者。\n\n实际上，ChatGPT的语音功能是通过将三个不同的模型拼接在一起而创建的黑客技术。\n\n当您对系统讲话时，它首先会使用转录模型将您的声音转化为文本。然后，它会将该文本输入到其智能模型中——基本上，与GPT\\-4\\的基础系统相同。\n\n智能系统会生成文本，ChatGPT会将其反馈到一个文本转语音系统中，以创建一个计算机化的声音来回应您。\n\n这使得系统名义上是可以对话的，但实际上与它交谈却显得笨拙和尴尬。\n\n在不同模型之间传递内容的额外步骤意味着系统反应迟缓。在我自己的测试中，我发现从与系统对话到得到回应，通常需要3到5秒的时间。\n\n人类对话依赖于在毫秒之间展开的微妙之处。一个响应语音需要长达五秒的系统感觉笨拙和机械。\n\n之前的系统还缺乏人类语言的许多基本方面。\n\n例如，您无法打断它；您必须等它说完才能回应。\n\n与它交谈常常感觉像是在与那些无法打断的人交谈，他们在没有意识到房间里其他人的情况下，喋喋不休地谈论一个随机话题。您常常觉得需要提到奥斯卡的乐团，以绝望的尝试让系统停止说话。\n\n它还受到无法解读声音中的情感或在自身回应中准确模仿人类情感的限制。\n\n人类在阅读潜台词方面非常出色，部分原因是我们可以捕捉到说话者声音中的微妙情感线索。\n\n如果我问我的朋友：“你今天过得怎么样？”而他们回答：“还不错”，但在“过得”和“不错”之间插入了一个微妙的停顿（或者最后一个词中带有一丝恼怒），我就知道他们实际上度过了一个艰难的日子，我应该问一些后续问题。\n\nChatGPT做不到这些，这使得与它交谈感觉像是在与某种外星智能沟通，而不是与人类交流。\n\n总之，之前的系统明显落入了“恐怖谷”。它在对话方面足够出色，声音也足够令人信服，以至于对话的某些部分可能感觉像人类。\n\n但奇怪的停顿、缺乏情感理解和延迟最终打破了这种幻觉，使其显得更令人不安而不是有用。\n\n我尝试与我六岁的儿子使用之前的系统。他对它感到如此不安，以至于不让我再打开音频。\n\n## OpenAI的革命性新模型\n\n今天，OpenAI正在改变这一切。在他们[今天早上的公告](https://www.youtube.com/live/DQacCB9tDaw?app=desktop&si=jvKW7jFDwFvOMBBk)中，公司透露他们将发布一个新模型，GPT\\-4o。\n\nGPT\\-4o原生集成了语音识别、语音生成和智能于一个系统中。\n\n这意味着将三种不同模型集成以模拟对话的复杂代码系统已经不复存在。相反，新的ChatGPT版本将能够**接收语音，瞬间处理，并以其自身生成的逼真语音作出回应。**\n\n对于用户来说，这将启用OpenAI首席执行官Sam Altman所描述的几种新功能，称其为“像魔法一样”。[描述为“像魔法。”](https://twitter.com/sama/status/1788989777452408943)\n\n首先，你将能够与ChatGPT进行更加自然的对话。你不再需要将问题和后续问题输入到界面中，而是可以像与朋友交谈一样与应用程序交谈。\n\n在几次现场演示中，OpenAI的工程师展示了系统如何在毫秒内倾听用户并作出智能回应。\n\n再次强调，这种速度之所以可能，是因为新模型不需要浪费时间在不同模式之间切换——它可以在单一步骤中处理语音并以自己的声音回应，而不必依赖多个低级模型。\n\nGPT\\-4o还可以解读和创造情感。\n\n在一次演示中，一名OpenAI员工要求系统引导他进行呼吸练习。\n\n他随后假装过度换气，而ChatGPT——感知到他呼吸的速度和声音中显现的恐慌——劝他放慢速度，做更深的呼吸。\n\n该系统似乎还能够调节自身回应中的情感。在另一场演示中，这名员工要求GPT\\-4o用越来越戏剧化的声音讲一个睡前故事。\n\n它照办了，最终听起来像一个中学戏剧社的孩子在可怕地过度表演一个场景！\n\n由于新系统还集成了GPT\\-4的视觉能力，它可以执行诸如解读人脸表情中的情感等功能。\n\n这种增强的情感智能水平可能会使系统成为一个更好的对话者。\n\n其他新功能也将有所帮助。用户可以在GPT\\-4o说话的过程中打断它。\n\n在他们的演示中，OpenAI的工作人员经常在模型开始偏离主题时打断它，就像在现实生活中打断朋友以开始回应问题一样。\n\n## 巨大的潜力\n\n今天早上的演示轻松幽默。但人们很快就能意识到，一个能够轻松理解、快速处理并真实创造情感人类语言的模型将是多么强大。\n\n在演示过程中，ChatGPT几次以让我想起电影《她》中虚构的人工智能的方式回应。\n\nChatGPT似乎对自己感到好笑，当OpenAI的工作人员称赞它时，它会感到尴尬，甚至可能会不时地抛出一些调情的台词。\n\n几次（据称）即兴的互动也揭示了更好的对话可以解锁的一些更深层次的能力。\n\n根据观众的问题，OpenAI的工作人员演示了系统如何能够听懂意大利语，并快速准确地将其翻译成英语，反之亦然。\n\n人们很容易想象，这样的能力将使多语言互动变得极其简单，基本上消除了语言障碍（也许还包括人类翻译）。\n\n例如，一位医生可以调出ChatGPT，快速与任何语言的患者交流。在旅行时，你可以在手机上调出这个应用，把它当作一个免费的即时翻译，向某人询问方向或在商店里进行购买。\n\n如果再加上视觉能力，你甚至可以向ChatGPT展示一家外国餐厅的菜单，询问某些菜品的翻译，告诉它你在家时喜欢吃什么，并请它推荐一些你可能想点的菜（或避免的菜）。\n\n我也可以看到新系统如何迅速进入《她》的领域。OpenAI仍然不允许电影中发生的那种不适合工作场合的互动。\n\n但是GPT-4o理解和模仿情感的能力——加上其强大且常常令人惊讶的能力，能够生成自己令人信服的人类情感表达——令人印象深刻。\n\n听完演示后，我确信人们会像电影中的主角一样爱上这个系统。它真的很出色。\n\n## 它会被使用吗？\n\n所有这些在纸面上都很惊人。然而，目前还不清楚有多少用户真正想要一个完全情感化的 AI 语音伴侣。\n\n我工作的大多数人使用 ChatGPT 不是作为对话伴侣，而是出于实用目的。\n\n我看到同事们利用这个系统来处理一些无聊和单调的任务，比如为网络研讨会撰写着陆页文案、快速回复房东的电子邮件，或撰写博客文章的初稿。\n\n这些实用功能实际上并不需要对话。目前尚不清楚能够用语音向 AI 发出这些请求是否会有用。\n\n因此，真正的考验并不一定是 OpenAI 的新系统有多强大，而是 **他们在用户已经通过语音与计算机互动的地方整合它的效果如何。**\n\n现实来看，我无法想象有多少用户会在工作时坐下来与 AI 对话。\n\n但如果 OpenAI 将 GPT-4o 集成到手机、汽车或像 Amazon Echo 这样的智能设备的语音界面中，我可以很容易地想象这个系统的情感能力变得更加有用。\n\n即使人们不太想与 ChatGPT 交谈，原生多模态音频和视觉模型的新能力对于在 OpenAI 现有 API 上构建应用程序的开发者来说，将是无比强大的。\n\n在他们的公告中，OpenAI 表示 GPT-4o 将通过他们现有的开发者接口提供。该系统的价格也将比之前的 GPT-4 模型便宜 50%。\n\n仅这些变化就非常重大。无论语音元素是否真正流行，驱动它的智能也将使数百个现有的 GPT-4 驱动应用程序变得更聪明、更快、更好，并且运营成本更低。\n\n换句话说，新系统的对话元素可能会被认为是一个很酷的噱头。但其潜在影响将更微妙、更广泛。\n\n我很期待看到真实用户如何与 GPT-4o 互动。他们会感到不安吗？惊讶吗？心动吗？\n\n但我更期待的是启动我的 Python IDE，将 GPT-4o 添加到我已经使用 OpenAI 工具构建的应用程序中。\n\n与机器对话很酷。但一个能够理解人类情感的原生多模态 AI 模型，我只需几行 Python 代码就能调用，而且成本低廉？这真的可能改变世界。\n\n**在过去的一年里，我测试了数千个 ChatGPT 提示。作为全职创作者，有一些我每天都会使用，符合我在本文中提到的伦理用途。我将它们汇编成一本免费的指南，*7 个对创作者极其有用的 ChatGPT 提示。* [今天就获取一份吧！](https://no-frills-influencer.ck.page/6a100e8fe4)**\n\n"},{"lang":"zh","group":"blog","slug":"blog/openai-rolls-out-searchgpt-to-more-users-33024ff3132c","frontmatter":{"title":"OpenAI 向更多用户推出 SearchGPT","meta_title":"OpenAI 向更多用户推出 SearchGPT","description":"ChatGPT 的用户界面进行了大规模重新设计，以支持 SearchGPT——现在它类似于 Google 和 Perplexity 等搜索引擎。","date":"2024-11-01T03:57:02.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BW6Qt6PMwHwlYRljAIBQWg.jpeg","categories":["Chatbots","Technology/Web","SearchGPT"],"author":"Rifx.Online","tags":["SearchGPT","ChatGPT","web","search","publishers"],"draft":false,"slug":"blog/openai-rolls-out-searchgpt-to-more-users-33024ff3132c"},"content":"\n\n\n\n\n**你注意到 OpenAI 最近对 ChatGPT 的重新设计了吗？**\n\n如果你最近登录过，你可能会发现两个主要变化。\n\n* 首先，新的 [**Canvas**](https://generativeai.pub/openai-rolls-out-canvas-in-chatgpt-a-brand-new-writing-and-coding-interface-7b57a3ec582a) 功能会自动在右侧打开一个新界面。这个新增功能让你可以处理更长的文档，而无需在聊天中上下滚动。这是一个小而实用的更新。\n* 其次，**提示字段**已经上移，现在位于屏幕中央。\n\n请看下面最新的用户界面：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*KsYMU9ffVlKsmHzKIOKH8g.png)\n\n你注意到这个新布局与 Google 和 Perplexity AI 的相似之处了吗？ChatGPT 现在看起来像一个搜索引擎。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*xFKErUHnfbJunaKi4NvM9A.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*PgSwS14lkUtZe7Ra9oLCrg.png)\n\n现在，当你在键盘上按下 ‘/’ 键时，你可以切换一个新的“搜索”功能，让 ChatGPT 访问网络。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*oYxvVvsuUuc_PM0PXRmN7A.png)\n\n让我们来分析一下这意味着什么。\n\n## ChatGPT中的搜索功能是什么？\n\n[SearchGPT](https://generativeai.pub/openai-announces-search-gpt-is-this-the-google-killer-5919ba31f95b) 允许ChatGPT访问实时网页数据。它的工作方式类似于Perplexity，使用大型语言模型为您搜索网络，提供即时答案，并包含其引用的来源。\n\n该功能最初向10,000名用户开放，并为希望提前访问的人添加了候补名单表单。\n\nOpenAI与**华尔街日报、美国广播公司、Vox Media和时代杂志**等知名出版商合作，以确保用户获得可信、可靠的信息。\n\n> “人工智能搜索将成为人们浏览互联网的关键方式之一，在这些早期阶段，技术的构建方式至关重要，它必须重视、尊重和保护新闻业和出版商。我们期待与OpenAI在这个过程中合作，为读者创造一种新的发现《大西洋月刊》的方式。” — Nicholas Thompson, 《大西洋月刊》首席执行官\n\n当您向SearchGPT提问时，它并不是从随机来源提取信息。每个响应都附有**清晰的内联引用和链接**，因此您确切知道信息来源于何处。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*uchpKOXqZCG55HSNkaOOZQ.png)\n\n您甚至可以通过点击出现在搜索网站下拉框中的源链接深入了解，提供更多探索主题的方式。\n\n## 如何访问 SearchGPT\n\n访问 SearchGPT 非常简单。当您在 ChatGPT 中时，按下 **‘/’** 键并从菜单中选择搜索选项。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*xNfm-6zPFzdXGL2A0D92YQ.png)\n\n它的工作方式与其他搜索引擎类似：您提出问题，几秒钟内，SearchGPT 就会提供答案，并附上来源。\n\n您甚至可以提出后续问题以更深入地探讨主题。这创造了一种对话式的搜索体验，比传统搜索结果的滚动更加互动。\n\n## SearchGPT vs. Perplexity vs. Google\n\n那么，SearchGPT 与 **Perplexity AI** 和 **Google** 相比如何呢？\n\n**SearchGPT** 的设计旨在为您提供简洁且来源明确的答案。每个答案都有链接到原始来源，您可以点击以验证信息。它非常适合实时回答和快速事实检查。\n\n此外，通过后续问题，您可以在不重新开始的情况下优化查询。这种对话性质使其感觉像是在与一个记得您所问问题的超级先进版本的 Google 交谈。\n\n另一方面，**Perplexity** 则是一个更学术风格的搜索引擎。它强调学术文章和详细研究，这对于更深入的查询非常有用。Perplexity 通常更适合需要更深层次来源的研究密集型任务。\n\n当然，**Google** 仍然是这个领域的巨头。尽管他们最近努力将生成式 AI 纳入搜索结果，但还没有完全实现用户所期望的无缝体验。\n\nGoogle 的生成搜索推出过程笨拙，并因错误和不相关的回答而受到大量批评。但 Google 的信息广度和基础设施仍然无与伦比。\n\n## 这是谷歌的终结吗？\n\n谷歌不会很快消失。这家科技巨头仍然控制着超过90%的搜索市场。他们已经在这个领域奋斗了数十年，搜索算法也在不断演变。\n\n然而，随着像SearchGPT这样的AI搜索引擎逐渐崭露头角，谷歌面临着提升自身竞争力的压力。OpenAI与出版商合作以获取可靠来源的举动是一个明智的策略，可能会削弱谷歌的主导地位。\n\n这种对验证结果的关注意味着，当你使用SearchGPT时，你不太可能遇到虚假答案——这是过去AI驱动工具所面临的挑战。\n\n此外，谷歌仍然是大多数人的默认选择。它的优势在于无处不在——从你手机的浏览器到智能音箱。SearchGPT仍处于早期阶段，需要时间来获得用户的信任。\n\n## SearchGPT 还未达到\n\n我在过去几个小时里测试了 SearchGPT，以下是我的一些观察：\n\n* **答案质量：** 一个主要的缺点是 SearchGPT 的答案质量与 Perplexity Pro 的深度或精确度不太匹配。尽管它与 Perplexity 的基础版本相当，但依赖于它进行更复杂或细致查询的用户会注意到差异。\n* **响应缓慢：** 另一个痛点是速度。在使用 SearchGPT 时，处理查询并返回答案所需的时间可能显得极其缓慢。这种延迟打断了互动的流畅性，尤其是在你深入探讨某个主题时。\n* **缺乏上下文理解：** 在某些情况下，它未能识别对话的连续性。如果你提出后续问题，模型往往将其视为一个新的独立问题，而不是理解为你之前查询的上下文。\n* **没有后续建议：** 与 Perplexity 不同，后者通常会建议后续问题以帮助你细化搜索，SearchGPT 并不提供此功能。这种缺乏指导使用户不得不自行想出如何最好地措辞或缩小查询范围。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*vfXxpgENLyenLY2l33PKiw.png)\n\n在使用搜索功能时，我注意到另一个奇怪的工作流程：如果将语言模型从 GPT-4o 切换到“ChatGPT o1-preview”，搜索指示器仍然存在，但实际上并不会在网上搜索结果。\n\n它返回的是其领域知识中的结果，这并不是用户所期望的。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*YcI-UuEmmFTQPUlO6mjpxA.png)\n\n正确的行为应该是在用户切换到“ChatGPT o1-preview”后禁用 *search* 功能，因为该模型没有能力在网上搜索。\n\n## 最后的思考\n\n我很高兴OpenAI终于推出了SearchGPT。自从他们在2024年7月宣布这一点以来，我就一直想测试它。\n\n在目前的状态下，SearchGPT是OpenAI迈入AI驱动搜索世界的良好第一步，但它还不够成熟，无法成为任何人处理复杂实时查询的首选工具。\n\n准确性、速度和处理对话上下文的能力还不够。目前，如果您需要深入的见解或更快的结果，像Perplexity Pro或Google这样的工具仍然是更好的选择。\n\n进一步阅读：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*5ejBBgbZaE8pGmpW.png)\n\n本文发布在[Generative AI](https://generativeai.pub/)。请在[LinkedIn](https://www.linkedin.com/company/generative-ai-publication)上与我们联系，并关注[Zeniteq](https://www.zeniteq.com/)，以获取最新的AI故事。\n\n订阅我们的[新闻通讯](https://www.generativeaipub.com/)和[YouTube](https://www.youtube.com/@generativeaipub)频道，以获取有关生成AI的最新新闻和更新。让我们一起塑造AI的未来！\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*TnRFuKk-2Dj_KCAP.png)\n\n"},{"lang":"zh","group":"blog","slug":"blog/openais-leaked-gpt2-model-has-everyone-stunned-6337904c2ecf","frontmatter":{"title":"OpenAI‘泄露’的 GPT2 模型让所有人震惊。","meta_title":"OpenAI‘泄露’的 GPT2 模型让所有人震惊。","description":"故意泄密？","date":"2024-11-01T04:07:40.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*-G0yfSjGPdNw02NZ","categories":["Chatbots","Generative AI","Natural Language Processing"],"author":"Rifx.Online","tags":["GPT-2","Chatbot","Inference","JSON","AlphaGo"],"draft":false,"slug":"blog/openais-leaked-gpt2-model-has-everyone-stunned-6337904c2ecf"},"content":"\n\n\n### 故意泄漏？\n\n\n\nOpenAI 对人工智能行业的影响不容小觑。每一个动作或决定都会自动成为头条……即使他们并没有真正宣布什么。\n\n几天前，一个我们许多人曾试用过但已被删除的模型让整个人工智能行业着迷。这个名为“gpt2-chatbot”的模型在 [lmsys.org](https://chat.lmsys.org/) 的“直接聊天”功能中可以使用了几天。\n\n*但为什么这么多喧嚣？*\n\n因为这个模型与我们见过的任何东西都不同。**它处于一个完全不同的层次。**\n\n因此，许多人认为它是 **ChatGPT-4.5** 或甚至 **GPT-5** 的非官方预告。更令人兴奋的是，使用数字“2”作为信号，表明 **新一代长推理模型的 GPT 正在逼近**。\n\n甚至 OpenAI 的 CEO Sam Altman 也忍不住承认它的存在，并在过程中调侃我们：\n\n\n\n\n那么，*这个模型到底有多好，它究竟是什么？*\n\n\n> 你可能已经厌倦了人工智能通讯简报谈论某个事情是如何“刚刚”发生的。这些通讯简报层出不穷，因为粗略地谈论已经发生的事件和事情是容易的，**但提供的价值有限，炒作却被夸大。**\n\n\n> 然而，谈论 **将** 会发生的事情的通讯简报却是罕见的。如果你想在别人之前获得易于理解的人工智能未来洞察，**TheTechOasis** 通讯简报可能非常适合你。\n\n\n> 🏝️🏝️ 今天就订阅吧：\n\n## 未来的预告\n\n随着每一天的过去，OpenAI的下一个模型显然将在推理和复杂问题解决方面实现飞跃。\n\n为了证明这个神秘的新模型可能就是它，这里有几个例子展示这个神秘模型的强大，可能表明这艘船已经在那个港口停靠：\n\n> 以下所有示例被认为是**当前最先进模型**的**困难或完全不可能**的。\n\n首先，它在零-shot模式下解决了一个数学奥林匹克问题（没有提供辅助示例来支持解决方案）：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*oNPg_hTGc0OP90n9)\n\n我甚至无法开始解释之前的例子有多疯狂，从当前最先进的模型中得到这样的答案绝对是不可能的。\n\n[它在解析JSON方面也绝对出色](https://twitter.com/skirano/status/1785035706173214888)，这是LLM与API及其他基于网络工具集成的基本技能。\n\n此外，它在复杂绘图任务中完全压倒了GPT-4，例如[根据代码绘制SVG文件](https://twitter.com/decentricity/status/1785049191003361778)或**使用ASCII代码绘制独角兽（如下）**，在这个过程中羞辱了**Claude 3 Opus**，当前的最先进模型：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*5A0EcRU91ZYAwVAc)\n\n此外，尽管这很可能是一次幻觉，**模型向我声称它是由OpenAI训练的，并基于GPT-4变体。**\n\n当然，在如此强大的演示之后，**许多人建议“gpt2-chatbot”甚至可能是著名的Q\\*模型**。\n\n但与其简单地屈服于人们声称的不同奇幻选项，不如采取更理性的方式，看看OpenAI自己在几个月（甚至几年）里通过他们的研究所暗示的内容。\n\n## 长推理的力量\n\n几个月来，像 [Demis Hassabis](https://www.youtube.com/watch?v=eqXfhejDeqA&t=2s) 或 [Andrej Karpathy](https://youtu.be/c3b-JASoPi0?si=fZWoSpLuSmua8YMR&t=1481) 这样的领域专家讨论了仅靠 LLMs 是不够的，我们需要“其他东西”来真正将它们提升到一个新的水平。\n\n在这两种情况下，他们提到实现相当于“AlphaGo 但在 LLMs 中”，这间接指的是：\n\n* **自我提升** 和\n* **测试时计算** LLMs\n\n*但他们这是什么意思呢？*\n\n### 人工智能的巨大飞跃\n\nAlphaGo 是人工智能的历史。它是第一个在围棋这一韩国棋类游戏中毫无疑问地超越人类实力的模型。\n\n它使用了 **Monte Carlo Tree Search**，一种搜索算法，来探索游戏中任何给定步骤的可能走法，能够超越当前的动作并预测对手的行动。\n\n> 你们中的一些人可能还记得 **Deep Blue**，那台在1997年与加里·卡斯帕罗夫的系列赛中第二局勉强战胜他的国际象棋机器，在第一局中输掉了比赛。\n\n> 然而，尽管 Deep Blue 可以被击败，但 AlphaGo 是无敌的。\n\n*但这是怎么做到的？*\n\n### 自我提升以达到超人水平\n\n使AlphaGo卓越的关键因素在于它的训练方式，**通过与自身的较弱版本对弈来创建自我提升循环。**\n\n它持续与自己对弈，逐渐将ELO提升至3\\.739，几乎达到了当今最佳围棋选手的水平。\n\n> 2017年，改进版的AlphaZero达到了5\\.018的ELO，完全超越人类，无法击败。\n\n换句话说，有了AlphaGo，人类首次实现了一种通过自我提升来训练模型的方法，使其能够达到超人能力，**因为它不再依赖模仿人类来学习。**\n\n如果你在想，这对LLMs并不适用。\n\n当前的LLMs完全依赖于人类水平的表现，因为所有数据和训练本质上都依赖于人类（以至于[对齐阶段](https://thewhitebox.ai/llms-the-backbones-of-frontier-ai/)——LLMs被建模以提高其安全水平并避免冒犯性反应的训练过程的一部分，**严格执行时使用的是“人类偏好”**）。\n\n> 顺便提一下，[Meta最近提出了自我奖励模型](https://arxiv.org/pdf/2401.10020v1)，可以通过自身的反应进行自我提升。然而，目前尚不清楚这个反馈循环是否真的能使LLMs超越人类。\n\n但尽管仍然很难相信“gpt2\\-chatbot”是通过自我提升训练出来的，**我们有充分的理由相信它是OpenAI多年来努力工作的第一个成功实现：测试时计算**。\n\n### 测试时计算模型的到来\n\n多年来，OpenAI的几篇研究论文暗示了将模型偏向“重推理”的这一想法。\n\n例如，早在2021年，[他们提出了在推理时使用“验证者”的概念](https://arxiv.org/pdf/2110.14168)，以改善模型在处理数学问题时的响应。\n\n这个想法是训练一个辅助模型，实时评估模型给出的多个响应，选择最佳的一个（然后提供给用户）。\n\n这与某种树搜索算法相结合，比如AlphaGo使用的那种，结合Google Deepmind的[思维树研究](https://arxiv.org/pdf/2305.10601)的例子，最终可以创建一个LLM，在回答之前，探索“可能响应的领域”，**仔细过滤并选择通向解决方案的最佳路径。**\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*pHWwOA66fxpKbl-z)\n\n这个想法虽然在2021年由OpenAI提出，但如今已经变得相当流行，[微软和谷歌的跨界合作研究将其应用于训练下一代验证者](https://arxiv.org/pdf/2402.06457)，谷歌甚至成功创建了一个模型，[Alphacode](https://storage.googleapis.com/deepmind-media/AlphaCode2/AlphaCode2_Tech_Report.pdf)，以极大的成功执行这种架构，**在竞争程序员中达到了85%的百分位，成为最优秀的人类程序员之一。**\n\n*那么，为什么这一代新的LLM具有如此巨大的潜力？*\n\n因为**它们在解决问题时与人类的方式非常相似**，通过有意识和广泛的思考来解决特定任务。\n\n归根结底，把“搜索+LLM”模型视为AI系统，它们在模型的实际运行时间上分配了更高程度的计算（类似于人类思维），因此，它们不必立即猜测正确的解决方案，而是简单地说，“有更多时间来思考”。\n\n但OpenAI更进一步。\n\n### PRM 模型以改善数学执行\n\n在去年五月，他们发布了论文 [Let’s Verify Step\\-by\\-Step](https://arxiv.org/pdf/2305.20050)，参与者包括 OpenAI 的首席科学家 Ilya Sutskever 以及一些来自原始验证者论文的研究人员，如 Karl Cobbe。\n\n这里的想法是修改在模型对齐阶段使用的奖励模型。\n\n[虽然我建议查看这篇文章以获取关于 LLM 训练的完整指南](https://thewhitebox.ai/llms-the-backbones-of-frontier-ai/)，创建 ChatGPT 等产品过程中的最后一步是使用人类反馈强化学习，或 RLHF。\n\n这个想法是让模型改善其决策能力。因此，我们训练一个辅助奖励模型（本质上是被训练模型的几乎相同副本），它学习根据人类偏好对训练模型的结果进行排名。\n\n*问题是什么？*\n\n好吧，今天大多数奖励模型是 **ORMs，或结果监督奖励模型**。通俗来说，评估模型预测的正确程度时，他们从整体上看待，而忽略了整个“思维过程”。\n\n另一方面，**PRMs，或过程监督奖励模型，评估模型响应中的每一个步骤**。因此，他们“迫使”模型在过程的每一个步骤上都付出密切的关注和努力，这在解决如下数学方程等情况中至关重要：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8JC6sZl5UFfl3WorliQy-A.png)\n\n然而，这是一个非常非常昂贵的过程，因为偏好数据需要大量的人力构建，以便可以应用监督信号。因此，每一个训练示例都有数十个或更多的奖励来进行测量。\n\n因此，“gpt2\\-chatbot”可能在奖励训练中包含某种变体，考虑到它在生成计划和执行复杂问题解决方面的高效性。\n\n## 不禁让人兴奋\n\n考虑到gpt2-chatbot的惊人表现，以及OpenAI最近的研究和[泄露](https://www.reuters.com/technology/sam-altmans-ouster-openai-was-precipitated-by-letter-board-about-ai-breakthrough-2023-11-22/)，我们现在可能对这个东西有了相当不错的了解。\n\n我们可以肯定的是，我们很快将面临一个完全不同的存在，它将把AI的影响提升到一个新的水平。\n\n* *我们是否终于达到了大型语言模型超越人类水平表现的里程碑，就像我们在AlphaGo中所做的那样？*\n* *长推理时代，即AI征服系统2思维的时代，是否已经到来？*\n\n可能还没有。然而，无法不对接下来几个月我们即将见证的惊人发展感到高度乐观。\n\n与此同时，我想我们将不得不等待这些答案。但不会太久。\n\n> 最后，如果你喜欢这篇文章，我在我的[LinkedIn](https://www.linkedin.com/in/ignacio-de-gregorio-noblejas/)上以更全面和简化的方式免费分享类似的想法。\n\n> 如果方便的话，你可以通过[X](https://twitter.com/TheTechOasis1)与我联系。\n\n> 期待与你的联系。\n\n"},{"lang":"zh","group":"blog","slug":"blog/qwen2-5-1-5b-the-future-of-mobile-ai-6bd5f29bbc84","frontmatter":{"title":"Qwen2.5 1.5b：移动AI的未来？","meta_title":"Qwen2.5 1.5b：移动AI的未来？","description":"阿里云最新 LLM 的本地测试和评估。使用 llama-cpp-python 和 DIY 提示目录。","date":"2024-10-30T12:57:39.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*awb56jkdXobA-Ip6d-QHRA.png","categories":["Natural Language Processing","Programming","Technology/Web"],"author":"Rifx.Online","tags":["Qwen2.5","NLP","summarization","retrieval","prompts"],"draft":false,"slug":"blog/qwen2-5-1-5b-the-future-of-mobile-ai-6bd5f29bbc84"},"content":"\n### 本地测试和评估阿里云最新的LLM。使用llama\\-cpp\\-python和DIY提示目录。\n\n\n\n在第一部分，我们共同探讨了阿里云团队发布的Qwen2\\.5模型系列的创新。\n\n在生成式AI基准测试中，基准测试现在是主要的*oracle*：新的LLM的有效性需要通过多个评判。你打破的基准记录越多，你就越优秀。\n\n这是赢得SOTA竞赛的方式。\n\n好吧，我不同意。尽管我们需要里程碑和更好的性能来推动AI进步，但用户体验和个人观点不能被视为无关紧要。\n\n我相信，在探索一些常用的NLP任务时，抛开聊天体验，我们必须关注回复的质量。我们是唯一需要的基准。我们的用户体验是判断一个模型是否优秀的最佳指标。模型必须足够可靠，以便在自动化工作流程中使用。\n\n顺便提一下，我已经运行了我决定称之为[RBYF — 与您作为反馈的修订基准](https://open.substack.com/pub/thepoorgpuguy/p/rbyf-is-here-revised-benchmarks-with?r=i78xo&utm_campaign=post&utm_medium=web)的测试，声称惊人的Llama3\\.2–1B\\-instruct…而Qwen2\\.5–1\\.5b则更好得多！\n\n因此，在本文中，正如承诺的那样，我们将亲自验证这个模型在日常使用中的表现有多好。\n\n回到我们自己……让我们开始吧！\n\n## 需求\n\n在这里，我们将构建一个最小的文本接口，以便能够运行模型、测试不同的任务并等待用户反馈以进行评估。\n\n需求很简单，但我建议您创建一个新的项目目录和一个虚拟环境。\n\n创建一个 `venv`（需要 Python 3\\.11\\+）：我在运行 Windows 11 的迷你电脑上进行了测试。\n\n```python\n## create the virtual environment\npython -m venv venv\n## activate the venv\nvenv\\Scripts\\activate\n## Install the dependencies \npip install llama-cpp-python==0.3.0 tiktoken\n```\n\n我们需要从 Hugging Face 的官方 qwen 仓库下载 GGUF 文件 [https://huggingface.co/Qwen/Qwen2\\.5\\-1\\.5B\\-Instruct\\-GGUF](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF)：我使用了 `qwen2.5-1.5b-instruct-q5_k_m.gguf` 版本。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*YtQJb_xyq_xcF40yRWPcZA.png)\n\n一切准备就绪！\n\n注意：如果您想添加对 GPU 加速器的不同后端支持，可以按照 [仓库中的说明](https://github.com/abetlen/llama-cpp-python#supported-backends) 进行操作。例如，我使用了 Vulkan 支持，因此在 pip 安装之前我添加了环境变量\n\n```python\n## Vulkan support - for Windows\n$env:CMAKE_ARGS = \"-DGGML_VULKAN=on\"\n```\n\n## 代码 — 主应用程序和库\n\n为了保持代码的简洁，我决定使用外部库扩展一些功能。好吧，这是一个自助库，所以这里没有秘密。\n\n您可以在我的文章中找到所有细节：\n\n为了加快速度，您可以直接 [从这里下载文件](https://github.com/fabiomatricardi/YouAreTheBenchmark/raw/main/QWEN2.5-1.5B/promptLibv2Qwen.py)：它包含了上述文章中讨论的 `promptLib` 的版本 2（名为 `promptLibv2Qwen.py`，对 `Qwen2.5-1.5B-instruct` 模型的提示进行了少量微调）。\n\n将文件保存在主目录中，并创建一个名为 `main.py` 的新文件。\n\n```python\n## Chat with an intelligent assistant in your terminal  \n## MODEL: https://huggingface.co/Qwen\n## qwen2.5-1.5b-instruct-q5_k_m.gguf\nimport sys\nfrom time import sleep\nimport warnings\nwarnings.filterwarnings(action='ignore')\nimport datetime\nfrom promptLibv2Qwen import countTokens, writehistory, createCatalog\nfrom promptLibv2Qwen import genRANstring, createStats\nimport argparse\n### PREPARING FINAL DATASET\npd_id = []\npd_task = []\npd_vote = []\npd_remarks = []\n####################Add GPU argument in the parser###################################\nparser = argparse.ArgumentParser()\nparser.add_argument(\"-g\", \"--gpu\", type=int, default=0,nargs='?',\n                    help=\"The number of layers to load on GPU\")\nargs = parser.parse_args()\nif args.gpu == None:\n   ngpu_layers = 0 \nelse:\n    ngpu_layers = args.gpu\nprint(f'Selected GPU: offloading {ngpu_layers} layers...')   \n####################INITIALIZE THE MODEL###################################\nstops = ['<!im_end|>']\ntasks = createCatalog()\nmodelname = 'qwen2.5-1.5b-instruct-q5_k_m.gguf'\n## create THE LOG FILE \ncoded5 = genRANstring(5)\nlogfile = f'logs/Qwen2.5-1.5B-it_CPP_{coded5}_log.txt'\ncsvfile = f'logs/Qwen2.5-1.5B-it_CPP_{coded5}.csv'\nlogfilename = logfile\n#Write in the history the first 2 sessions\nwritehistory(logfilename,f'{str(datetime.datetime.now())}\\n\\nYour own LocalGPT with 💻 {modelname}\\n---\\n🧠🫡: You are a helpful assistant.')  \nwritehistory(logfilename,f'💻: How can I assist you today in writing?')\n```\n\n在这里，我们只是在做准备：我们导入库，包括我们自己的 `promptLibv2Qwen` 以及 `argparse`。我想尝试一些新东西：[argparse](https://realpython.com/command-line-interfaces-python-argparse/) 是一个用于终端 Python 程序的 Python 库，您可以从命令行读取多个参数。\n\n在这里，我们只有一个参数（没有参数），带有标志 `-g` 或 `--gpu`。当您使用此参数运行 Python 代码时，我们将设置 GPU 层的数量为最大值（但您可以自行更改）。\n\n然后，我们设置一些全局变量，跨整个代码使用：任务、我们的提示集合、停止词和日志文件名。\n\n> 注意：所有日志都保存在名为 `logs` 的子目录中……所以请确保创建一个。\n\n我们还准备了所有相关信息，以便将其存储到数据集中，然后最终保存到 CSV 文件中（以便轻松创建性能矩阵）。\n\n```python\n### PREPARING FINAL DATASET\npd_id = []\npd_task = []\npd_vote = []\npd_remarks = []\n```\n\n然后，我们使用 Llama\\-CPP\\-python 将模型加载到 RAM（没有 GPU）或 VRAM（使用 GPU）中。\n\n```python\n## LOAD THE MODEL\nprint(\"\\033[95;3;6m\")\nprint(\"1. Waiting 10 seconds for the API to load...\")\nfrom llama_cpp import Llama\nllm = Llama(\n            model_path='models/qwen2.5-1.5b-instruct-q5_k_m.gguf',\n            n_gpu_layers=ngpu_layers,\n            temperature=0.1,\n            n_ctx=8192,\n            max_tokens=1500,\n            repeat_penalty=1.178,\n            stop=stops,\n            verbose=False,\n            )\nprint(f\"2. Model {modelname} loaded with LlamaCPP...\")\nprint(\"\\033[0m\")  #reset all\nhistory = []\nprint(\"\\033[92;1m\")\nprint(f'📝Logfile: {logfilename}')\n```\n\n顺便说一句，您可以在我的 GitHub 仓库中找到所有代码：\n\n下一个是一次性热身推理：模型神经网络将首次激活，所以可以把它看作是热身圈。\n\n不要害怕，我会解释代码。\n\n```python\n##################### ALIGNMENT FIRST GENERATION ##############################################\nquestion = 'Explain the plot of Cinderella in a sentence.'\ntest = [\n    {\"role\": \"user\", \"content\": question}\n]\nprint('Question:', question)\nstart = datetime.datetime.now()\nprint(\"💻 > \", end=\"\", flush=True)\nfull_response = \"\"\nfisrtround = 0\nfor chunk in llm.create_chat_completion(\n    messages=test,\n    temperature=0.25,\n    repeat_penalty= 1.31,\n    stop=stops,\n    max_tokens=1500,\n    stream=True,):\n    try:\n        if chunk[\"choices\"][0][\"delta\"][\"content\"]:\n            if fisrtround==0:\n                print(chunk[\"choices\"][0][\"delta\"][\"content\"], end=\"\", flush=True)\n                full_response += chunk[\"choices\"][0][\"delta\"][\"content\"]\n                ttftoken = datetime.datetime.now() - start  \n                fisrtround = 1\n            else:\n                print(chunk[\"choices\"][0][\"delta\"][\"content\"], end=\"\", flush=True)\n                full_response += chunk[\"choices\"][0][\"delta\"][\"content\"]                            \n    except:\n        pass      \ndelta = datetime.datetime.now() - start\noutput = full_response\nprint('')\nprint(\"\\033[91;1m\")\nrating = input('Rate from 0 (BAD) to 5 (VERY GOOD) the quality of generation> ')\nprint(\"\\033[92;1m\")\nstats = createStats(delta,question,output,rating,logfilename,'Alignment Generation',ttftoken)\nprint(stats)\nwritehistory(logfilename,f'''👨‍💻 . {question}\n💻 > {output}\n{stats}\n''')\n```\n\n我们设置了第一个用户问题，并将其放入一个众所周知的聊天格式字典中。然后我们开始计时（对速度、令牌计数等很有用……）。\n\n我们调用推理方法 `create_chat_completion()`，允许我们以聊天格式接受提示，并逐个令牌流式输出结果。\n\n由于模型的第一次回复不包含任何输出令牌（仅包含统计信息），我们使用了 try/except 语句。此外，由于我想知道何时生成第一个令牌，我们设置了一个标志并暂时停止计时，将信息保存在 `ttftoken` 变量中。\n\n在流式输出结束时，我们计算从开始到现在的时间差，并等待用户提供对生成输出的个人反馈：从 0 到 5 评分，并添加与指令提示和用户意图的一致性相关的评论。\n\n我们使用内部库 `createStats()` 来打印生成的所有统计信息，并将其保存到日志文件中。该函数的输出将类似于以下内容：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*8znYCqpisviXvYgrzjWF5w.png)\n\n## 提示目录 — 我们想要测试的内容\n\n我在这里写了我的习惯。我有一个提示目录，涵盖了聊天机器人中使用的许多主要语言任务，例如总结、简短总结、随意聊天、RAG、真实RAG等等。\n\n这个想法是能够在5分钟内加载模型，并开始评估每个任务。在每次生成结束时，用户会被提示给出一个分数（从0到5的评分）并在需要时留下任何评论。\n\n这很关键：并不是所有模型都是一样的，对提示中的措辞进行小的或大的调整总是必需的。\n\n那么回到代码……因为之前的代码只是热身，现在将开始真正的while循环，遍历整个提示目录。请参见下面的工作流程……\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*EL0Q97Du6HwtcYQZ.png)\n\n代码中只有少量更改，我会指出这些更改，请耐心等待。\n\n```python\n############################# AUTOMATIC PROMPTING EVALUATION  11 TURNS #################################\nid =1\nfor items in tasks:\n    fisrtround = 0\n    task = items[\"task\"]\n    prompt = items[\"prompt\"]\n    test = []\n    print(f'NLP TAKS>>> {task}')\n    print(\"\\033[91;1m\")  #red\n    print(prompt)\n    test.append({\"role\": \"user\", \"content\": prompt})\n    print(\"\\033[92;1m\")\n    full_response = \"\"\n    start = datetime.datetime.now()\n    print(\"💻 > \", end=\"\", flush=True)\n    for chunk in llm.create_chat_completion(\n        messages=test,\n        temperature=0.15,\n        repeat_penalty= 1.31,\n        stop=stops,\n        max_tokens=1500,\n        stream=True,):\n        try:\n            if chunk[\"choices\"][0][\"delta\"][\"content\"]:\n                if fisrtround==0:\n                    print(chunk[\"choices\"][0][\"delta\"][\"content\"], end=\"\", flush=True)\n                    full_response += chunk[\"choices\"][0][\"delta\"][\"content\"]\n                    ttftoken = datetime.datetime.now() - start  \n                    fisrtround = 1\n                else:\n                    print(chunk[\"choices\"][0][\"delta\"][\"content\"], end=\"\", flush=True)\n                    full_response += chunk[\"choices\"][0][\"delta\"][\"content\"]                            \n        except:\n            pass      \n    delta = datetime.datetime.now() - start\n    print('')\n    print(\"\\033[91;1m\")\n    rating = input('Rate from 0 (BAD) to 5 (VERY GOOD) the quality of generation> ')\n    print(\"\\033[92;1m\")\n    stats = createStats(delta,prompt,full_response,rating,logfilename,task,ttftoken)\n    print(stats)\n    writehistory(logfilename,f'''👨‍💻 > {prompt}\n💻 > {full_response}\n{stats}\n''')\n    pd_id.append(id)\n    pd_task.append(task)\n    pd_vote.append(rating[:2])\n    pd_remarks.append(rating[2:])\n    id += 1\n## create dataframe and save to csv\nzipped = list(zip(pd_id,pd_task,pd_vote,pd_remarks))\nimport pandas as pdd\ndf = pdd.DataFrame(zipped, columns=['#', 'TASK', 'VOTE','REMARKS'])\n#saving the DataFrame as a CSV file \ndf_csv_data = df.to_csv(csvfile, index = False, encoding='utf-8') \nprint('\\nCSV String:\\n', df_csv_data)  \n```\n\n主要更改仅在前几行：\n\n```python\nfor items in tasks:\n    fisrtround = 0\n    task = items[\"task\"]\n    prompt = items[\"prompt\"]\n```\n\n如果你阅读了关于`promptLib`的文章，你应该不会感到惊讶：但如果你是新手，这里我们正在遍历一个字典列表，具有以下结构：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*rGcKJWNzSUrcu4wi.png)\n\n因此，对于目录中的每个条目（意味着任务和提示的对），我们提取任务描述和任务提示。\n\n```python\ntest.append({\"role\": \"user\", \"content\": prompt})\n```\n\n然后我们在一个临时列表`test`中创建聊天模板消息，并将其传递给`create_chat_template()`方法进行生成。\n\n其他内容都是一样的。\n\n保存文件，并在激活`venv`的情况下运行：\n\n```python\npython main.py\n## 如果你使用的是GPU，运行 python main.py -g\n```\n\n这将为你提供类似于下面示例的内容……\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*MhhQu4lLjtU__Wjf0dSWBg.gif)\n\n请注意，在整个提示目录的末尾，会创建一个*csv*文件，其中包含所有任务的摘要！\n\n## 测试概述\n\n我使用了几个小型语言模型，从 [Qwen2–1\\.5B\\-instruct](https://huggingface.co/Qwen/Qwen2-1.5B-Instruct-GGUF) 到 [Gemma2–2B\\-instruct](https://huggingface.co/bartowski/gemma-2-2b-it-GGUF)，再到 [Llama3\\.2–1B\\-instruct](https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF)，最后是新的 [Qwen2\\.5–1\\.5B\\-instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF)。\n\n虽然我对 [Llama3\\.2–1B\\-instruct](https://generativeai.pub/llama3-2-1b-instruct-is-ok-but-not-good-enough-28f88046b63e) 感到相当失望，但对新的 [Qwen2\\.5–1\\.5B\\-instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF) 的出色表现感到惊讶。\n\n在每次生成结束时，用户会被要求用 0 到 5 之间的分数来评估结果。**在这种情况下，用户就是我……**\n\n这种定性分析确实较为简单，因此每个分数都有相应的描述，用户可以添加评论（“一些错误信息”，“可能更好地更改提示中的措辞”）\n\n这里是定性矩阵及其描述\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*eBdPfZtfr99MsvLh6tt42w.png)\n\n## 好与坏 — 细节\n\n总结非常出色。列出长文本的主要主题也非常好。\n\nRAG 任务相当快速（即使在我的迷你 PC 上），真实的 RAG（在上下文之外提问）也很到位。\n\n所以真的很好。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*DuV3LJep_PuDqiCcAMb6Cg.png)\n\n不过也有一些不足之处：即使温度只有 `0.15`，我在两句总结任务中也得到了些虚构的信息。这不好。\n\n> 我希望通过稍微调整提示，或者将温度设为 `0` 可以解决这个问题。\n\n另一个事实是，创意写作相当糟糕：在测试中，我使用了一些固定的生成参数。\n\n```python\n        temperature=0.15,\n        repeat_penalty= 1.31,\n```\n\n对于创意写作，使用 Qwen2\\.5–1\\.5B\\-instruct 时，我们应该使用更高的 `repeat_penalty` 和更高的 `temperature`。\n\n顺便提一下，我必须说反思提示也并不差！标签的开闭没有保持（因此不易将其放入管道或工作流中），但生成的整体流程和“思维链”推理过程相当不错。\n\n```python\n<thinking>\n使用生成性人工智能（AI）在教学中的重要性不容小觑，因为这项技术在当前教育实践中具有巨大的潜力，同时也有助于塑造更具创新性的终身学习方法。\n</thinking>\n\n**思维链：**\n1. **理解影响**：AI可以自动化重复性任务，并根据学生的表现数据提供个性化反馈（例如，通过聊天机器人或自适应评估）。\n2. **增强学习体验**：通过整合生成性AI，教师可以创建更具吸引力的课程，以满足多样化的学习风格。\n3. **改善学生成果**：个性化教学可以帮助学生更好地理解和记忆知识。\n\n**反思：**\n- 将生成技术整合到教育中至关重要，因为这与日益成为数字原住民的学习者的不断变化的需求相一致，需要一种将这些技术纳入其中的教学方法。\n</thinking>\n\n<output>\n教师应学习如何利用生成性AI来增强他们的教学方法并改善学生成果。这包括理解其在自动化任务（如评分或创建个性化学习路径）方面的能力。\n\n**学习路径：**\n1. **教育技术基础课程**：首先学习提供教育技术概述的课程，包括数字工具的基础知识。\n2. **AI基础课程（Coursera）** - 通过Coursera的免费在线课程了解AI概念及其在教育中的应用。\n\n3. **生成学习的专业课程**：报名参加专注于生成学习的课程，如“生成设计”或MIT等大学提供的类似专业项目。\n4. **专业发展研讨会和会议** - 参加当地教育会议上专注于将AI整合到教学实践中的研讨会。\n\n5. **教育工作者在线社区（例如，Edmodo）**：加入在线社区，与教育工作者讨论生成技术在教育中的应用，分享资源或询问实施问题。\n6. **认证项目**：考虑通过像谷歌的“AI for Educators”项目等机构获得认可您在教学实践中整合AI相关知识和技能的认证。\n\n通过遵循这条学习路径，教师不仅可以增强自身的专业发展，还可以通过有效地整合生成技术，为教育的未来做出积极贡献。\n</output>\n```\n\n我认为，对于这个小型语言模型的规模，结果并不差！\n\n## 比较 Qwen2\\.5 和 Llama3\\.2\n\n尽管这仅仅是我个人的评估，我还是想和你分享一下。\n\n这两个模型都是为移动设备设计的，但性能差异很大。请看下面：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*T6vLgvOKdkotlV1K5x6-QQ.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*DuV3LJep_PuDqiCcAMb6Cg.png)\n\n首先，整体评分差异巨大（Llama3\\.2 为 41，Qwen2\\.5 为 57\\）。\n\n其次，如果你考虑在移动设备上可能会问的问题，语言任务方面，主要是希望有流畅的聊天体验（任务 4\\）、良好的摘要能力（任务 5 到 7\\）以及一些创造性写作（任务 11 和 13\\）。\n\n在速度方面，仅在 CPU 上运行模型，使用非常有限的迷你 PC，**我获得了平均推理速度为 14 t/s。**\n\n## 结论\n\n在 Qwen2 发布的过去三个月里，众多开发者在 Qwen2 语言模型上构建了新模型，为整个社区以及阿里云提供了宝贵的反馈。\n\n> 在此期间，我们专注于创造更智能、更有知识的语言模型。今天，我们很高兴地介绍 Qwen 家族的最新成员：Qwen2\\.5\n\n他们的声明伴随着关于新模型家族的事实：\n\n* 密集型、**易于使用**的仅解码器语言模型，提供 0\\.5B、1\\.5B、3B、7B、14B、32B 和 72B 尺寸，以及基础和指令变体。\n* 在我们最新的大规模数据集上进行预训练，涵盖多达 18T 的标记。\n* **指令跟随**方面的显著改进\n* 对系统提示的多样性**更具弹性**，增强角色扮演实施和聊天机器人的条件设置。\n* **支持高达 128K** 的上下文长度，并可以生成最多 8K 的标记。\n* 支持超过 29 种语言的多语言功能\n\n在我广泛的（但确实限于单次提示和少数 NLP 任务）测试中，我亲眼看到这些声明是基于高质量的训练数据集和精心策划的微调。\n\n该模型在移动设备上表现极为出色！\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/rag-llm-and-pdf-conversion-to-markdown-text-with-pymupdf-03af00259b5d","frontmatter":{"title":"RAG/LLM 和 PDF：使用 PyMuPDF 转换为 Markdown 文本","meta_title":"RAG/LLM 和 PDF：使用 PyMuPDF 转换为 Markdown 文本","description":"采用 markdown 文本格式输入数据可提高生成的文本质量","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*swPjVuudAhsoRiiw3Ee32w.png","categories":["Programming","Technology","Technology/Web"],"author":"Rifx.Online","tags":["markdown","PyMuPDF","LLM","RAG","PDF"],"draft":false,"slug":"blog/rag-llm-and-pdf-conversion-to-markdown-text-with-pymupdf-03af00259b5d"},"content":"\n\n\n### 以Markdown文本格式输入数据可以提高生成文本的质量\n\n\n\n## 介绍\n\n在**大型语言模型（LLMs）**和**检索增强生成（RAG）**环境中，以**markdown文本格式**输入数据具有**重要意义**。以下是一些详细考虑因素。\n\n**LLMs** 是强大的语言模型，可以生成连贯且具有上下文相关性的文本。然而，它们有时可能会产生缺乏事实准确性或上下文的响应。通过结合基于检索的方法（如RAG），我们可以提高生成文本的质量。\n\n**RAG** 使得将**外部数据**——在LLM的训练数据中之前缺失的数据——整合到文本生成过程中成为可能。这种包含减少了“幻觉问题”，并增强了文本响应的相关性。\n\n## 为什么选择 Markdown 用于 LLM？\n\n**Markdown** 是一种轻量级标记语言，允许用户使用简单的语法格式化纯文本。它广泛用于创建结构化文档，特别是在 GitHub、Jupyter 笔记本和各种内容管理系统上。当将数据输入到 LLM 或 RAG 系统时，使用 Markdown 格式提供了几个好处：\n\n1. **结构化内容**：Markdown 允许您将信息组织成标题、列表、表格和其他结构化元素。这种结构有助于更好地理解和上下文保留。\n2. **富文本**：Markdown 支持基本格式，如粗体、斜体、链接和代码块。在输入数据中包含富文本可以增强语言模型的上下文。\n3. **嵌入链接和引用**：Markdown 允许您嵌入超链接、脚注和引用。在 RAG 场景中，这对于引用外部来源或提供额外上下文至关重要。\n4. **易于创作**：Markdown 具有可读性，易于编写。作者可以高效地创建内容，而无需复杂的格式化工具。\n5. **分块**：对于 RAG 系统至关重要，分块（也称为“拆分”）将大量文档拆分为更易处理的部分。通过支持 MD 格式的 PyMuPDF 数据提取，我们支持分块以保持具有共同上下文的文本在一起。**重要的是，MD 格式的 PyMuPDF 提取允许进行 [第 3 级分块](https://readmedium.com/five-levels-of-chunking-strategies-in-rag-notes-from-gregs-video-7b735895694d#b123)**。\n\n总之，在 LLM 和 RAG 环境中使用 Markdown 文本格式可以确保更准确和相关的结果，因为它提供了更丰富的数据结构和更相关的数据块负载给您的 LLM。\n\n## PyMuPDF 支持 PDF 的 Markdown 转换\n\n自推出以来，PyMuPDF 一直能够从 PDF 页面中提取文本、图像、矢量图形，并且从 2023 年 8 月起，还能够提取表格。这些对象类型各自有其提取方法：文本有一种，表格、图像和矢量图形则有其他方法。为了满足 RAG 的要求，我们将这些不同的提取方式合并，生成一个统一的 **Markdown** 字符串，以一致地表示页面的整体内容。\n\n所有这些都实现为 [一个 Python 脚本](https://github.com/pymupdf/RAG/blob/main/helpers/pymupdf_rag.py)。它可以被其他脚本作为模块导入，或者在终端窗口中通过以下命令行调用：\n\n`$ python pymupdf_rag.py input.pdf [-pages PAGES]`\n\n它将生成一个 **Markdown** 格式的文本文件（称为 `input.md`）。可选参数 `PAGES` 允许将转换限制为 PDF 总页面的一个子集。如果省略，则处理整个 PDF。\n\n## Markdown 创建细节\n\n### 选择要考虑的页面\n\n“`-pages`” 参数是一个字符串，由所需的页面编号（从1开始）组成，用于考虑进行markdown转换。可以给出多个页面编号规范，使用逗号分隔。每个规范可以是一个整数或两个用“`-`”连接的整数，指定一个页面范围。以下是一个示例：\n\n“`-pages 1–10,15,20-N`”\n\n这将包括第1页到第10页、第15页以及第20页到文件末尾（大写“N”被视为最后一页的编号）。\n\n### 识别标题\n\n在调用时，程序检查给定页面上的所有文本并找出最常用的字体大小。该值（以及所有较小的字体大小）被假定为 **正文文本**。较大的字体大小被假定为 **标题文本**。\n\n根据它们在字体大小层级中的相对位置，标题文本将前面加上一个或多个 markdown 标题 `#` 标签字符。\n\n### 按页面区域识别处理模式\n\n每个页面上的所有文本首先将被分类为**标准**文本或**表格**文本。然后，页面内容将从上到下提取，并转换为Markdown格式。\n\n这最好通过一个例子来解释：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*u5fv2aAIvDaaAd6H.png)\n\n该页面显示的内容代表典型情况：\n\n* 两个表格，具有部分重叠的垂直位置。一个表格没有标题，另一个表格有**外部**列标题。\n* 有一行**标题**和多个级别的**标题**。\n* **正文文本**包含多种样式细节，如**粗体**、*斜体*和`行内代码`。\n* 有序和无序列表。\n* 代码片段。\n\n布局分析将确定三个区域并选择适当的处理模式：**(1)** 文本，**(2)** 表格，**(3)** 文本。\n\n生成的Markdown文本忠实地反映了上述内容——在这种格式中尽可能做到。\n\n作为一个例子，让我们看一下具有外部标题的表格的输出：\n\n```python\n|Column1|Column2|\n\n|---|---|\n\n|Cell (0, 0)|Cell (0, 1)|\n\n|Cell (1, 0)|Cell (1, 1)|\n\n|Cell (2, 0)|Cell (2, 1)|\n```\n这是与GitHub兼容的格式，具有最小的可能令牌大小——这是保持输入到RAG系统的小型化的重要方面。\n\n**列边框**由“`|`”字符表示。如果文本行后面跟着“`|---|---| …`”形式的行，则假定该文本行是**表头**。完整的**表格定义**必须前后至少有一行空行。\n\n请注意，由于技术原因，Markdown表格必须有一个标题，因此如果没有外部标题，将选择第一行作为表头。\n\n为了确认整体准确性，以下是Markdown解析器如何处理完整页面的示例：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Ge83uj7FiM4T6XFn)\n\n## 以编程方式调用 Markdown 转换器\n\n除了在命令行中执行程序外，Markdown 转换也可以通过程序请求：\n\n```python\nimport fitz\nfrom pymupdf_rag import to_markdown  # import Markdown converter\n\ndoc = fitz.open(“input.pdf”)  # open input PDF\n\n## define desired pages: this corresponds “-pages 1-10,15,20-N”\npage_list = list(range(9)) + [14] + list(range(19, len(doc) – 1))\n\n## get markdown string for all pages\nmd_text = to_markdown(doc, pages=page_list)\n\n## write markdown string to some file\noutput = open(“out-markdown.md”, “w”)\noutput.write(md_text)\noutput.close()\n```\n\n## 结论\n\n通过集成 PyMuPDF 的提取方法，PDF 页面的内容将被忠实地转换为可用作 RAG 聊天机器人的输入的 Markdown 文本。\n\n请记住，成功的 RAG 聊天机器人的关键在于它能够访问的信息的质量和完整性。\n\n启用 PyMuPDF 的 Markdown 提取确保从 PDF 中获取这些信息不仅是可能的，而且是简单的，展示了该库的强大和对开发者的友好。祝编码愉快！\n\n### 源代码\n\n* [RAG/helpers/pymupdf\\_rag.py (github.com)](https://github.com/pymupdf/RAG/blob/main/helpers/pymupdf_rag.py)\n\n### 参考文献\n\n* [5 Levels of Text Splitting](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb)\n\n### 相关博客\n\n* [使用 ChatGPT API 和 PyMuPDF 构建 RAG 聊天机器人 GUI](https://readmedium.com/building-a-rag-chatbot-gui-with-the-chatgpt-api-and-pymupdf-9ea8c7fc4ab5)\n* [使用 ChatGPT 和 PyMUPDF 创建 RAG 聊天机器人](https://readmedium.com/creating-a-rag-chatbot-with-chatgpt-and-pymupdf-f6c30907ae27)\n* [RAG/LLM 和 PDF：增强文本提取](https://readmedium.com/rag-llm-and-pdf-enhanced-text-extraction-5c5194c3885c)\n\n"},{"lang":"zh","group":"blog","slug":"blog/rbyf-qwen2-5-3b-instruct-is-damn-good-dcf443cacc63","frontmatter":{"title":"RBYF：Qwen2.5–3B-instruct 非常棒。","meta_title":"RBYF：Qwen2.5–3B-instruct 非常棒。","description":"修改后的基准测试并附上您的反馈：阿里巴巴Qwen的全新3B模型是一个了不起的模型，我可以证明这一点！","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*NWaBtJ64TLUoUHv4F1qJpg.png","categories":["Programming","Technology","Science"],"author":"Rifx.Online","tags":["Qwen","NLP","multimodal","RBYF","evaluation"],"draft":false,"slug":"blog/rbyf-qwen2-5-3b-instruct-is-damn-good-dcf443cacc63"},"content":"\n### 修订基准：以您为反馈的全新3B模型来自阿里巴巴Qwen，是个令人惊叹的模型，我可以证明这一点！\n\n\n\n涌现属性的错觉在很大程度上是评估这些模型所使用的指标的产物。这是一个事实。\n\n几周前，我决定做一个小反叛，放弃所有官方基准，开始自己做基准测试！\n\n这就是这个完全虚构的首字母缩略词RBYF的意义：以您为反馈的修订基准。其基本原则是，没有比您更好的评判者来验证一个大型语言模型的优劣。\n\n老实说，我专注于小型语言模型。我没有专用的GPU，计算资源有限。但我同样同意[LLMWare反叛原则第一条](https://readmedium.com/getting-work-done-with-genai-just-do-the-opposite-10-contrarian-rules-that-may-actually-work-634501602a27)：\n\n使用小模型，而不是大模型。\n\n在这篇文章中，我将向您展示我对qwen2.5–3b-instruct的评估结果。结果真的很好！\n\n> 免责声明：所有与结果相关的提示均可在我的GitHub存储库中找到：\n\n## 少即是多\n\n缩放法则描述了模型性能如何随着参数和训练数据数量的增加而改善。这个原则推动了对LLM新能力的探索。\n\n> 仅仅通过增加模型的规模，我们可以解锁新的能力……\n\n缩放法则描述了模型性能与参数数量和训练数据之间的关系。随着模型变得更大并在更多数据上进行训练，我们期待它们的性能得到提升。这导致了对越来越大LLM的无休止追求，希望能够解锁新的能力。\n\n涌现属性是指在复杂系统中个体组件之间的相互作用中产生的属性。通过孤立地研究组件，无法预测或理解这些属性。在LLM的情况下，希望随着这些模型变得更大和更复杂，它们会展现出意想不到的新能力。\n\n这是一个童话故事。\n\n在过去几周，我们亲眼目睹了经过过度训练和精心策划的小型语言模型可以表现得与它们的大型兄弟一样好。这对所谓的涌现能力是一个重击，反击了缩放法则。Gemma2–2B、Qwen2.5–3B，甚至最新的Llama3.2–3B都是远胜于旧的SOTA 7B模型的更好模型。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*-EjpdEky-Hf3WEQn.png)\n\n## Qwen2.5模型系列\n\n阿里云在九月中旬发布了他们的旗舰模型系列Qwen2.5。\n\n> 阿里云在Qwen的革命性旅程中再次展现出强大的创新领导力\n\nQwen2.5是阿里巴巴集团Qwen团队的大型语言模型和大型多模态模型系列。这些语言模型和多模态模型在大规模多语言和多模态数据上进行了预训练，并在高质量数据上进行了后训练，以便与人类偏好对齐。Qwen能够理解自然语言、生成文本、理解视觉、理解音频、使用工具、角色扮演、作为AI代理进行操作等。\n\n**新款Qwen2.5的亮点在于经过精心策划的训练数据集。** 通过检查小模型的性能，您可以清楚地理解这一点。\n\n如果该系列的小型语言模型表现良好，意味着训练和数据集经过高度修订和策划。\n\n以下是一些数字：\n\n* 密集型、易于使用的解码器语言模型，提供0.5B、1.5B、3B、7B、14B、32B和72B尺寸，以及基础和指令变体。\n* 在我们最新的大规模数据集上进行了预训练，涵盖多达18T个标记。\n* 在指令跟随、生成长文本（超过8K个标记）、理解结构化数据（例如，表格）和生成结构化输出（尤其是JSON）方面有显著改进。\n* 对系统提示的多样性更具韧性，增强了角色扮演的实现和聊天机器人的条件设置。\n* 支持的上下文长度可达128K个标记，并且可以生成多达8K个标记。\n* 支持超过29种语言的多语言，包括中文、英文、法文、西班牙文、葡萄牙文、德文、意大利文、俄文、日文、韩文、越南文、泰文、阿拉伯文等。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*3tnTS_UCImBRBDeKmFyjVQ.png)\n\n## Qwen2.5–3B-instruct\n\n尽管为了推动人工智能的发展，我们需要里程碑和更好的表现，但用户体验和个人观点不能被视为无关紧要。\n\n我认为，探索一些常用的自然语言处理任务，抛开聊天体验，我们必须关注回复的质量。而我们是唯一所需的基准。**我们的用户体验是理解模型好坏的最佳指标**。模型必须足够可靠，以便在自动化工作流程中使用。\n\n顺便提一下，我已经运行了我决定称之为[RBYF — 修订基准与您反馈](https://open.substack.com/pub/thepoorgpuguy/p/rbyf-is-here-revised-benchmarks-with?r=i78xo&utm_campaign=post&utm_medium=web)的项目，在[Qwen2.5–1.5b-instruct](https://ai.gopubby.com/qwen2-5-1-5b-the-future-of-mobile-ai-6bd5f29bbc84)上：您可以阅读详细信息。在文章中，我还解释了如何创建您的测试基准。所描述的方法与我用于Qwen2.5–3B的相同。\n\n让我们从所有任务的整体表现开始。该模型已由我评估（在这种情况下是我自己的反馈），基于以下显示的定性矩阵。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*rdVHfCDWX9jlvtiq)\n\n总体得分为62/70 = 8.8\n\n好的，但Qwen2.5–3B-instruct是基于什么得到了这个评估分数呢？\n\n## 测试概述\n\n这个想法是进行公平的用户反馈，而不是基于标准基准和框架的自动化反馈。一个小型语言模型能否满足用户在主要自然语言处理任务上的意图？\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*wgngfGvSebeoH3YxTdvc8A.png)\n\n我们希望验证用户意图和响应质量。以下是每个任务的详细信息：\n\n### 介绍\n\n验证模型如何回应初始问候并谈论自身。\n\n### 一句话解释\n\n综合与总结。最终评估的重点在于模型是否能够将回复压缩为仅一句话。\n\n### 用三段话解释\n\n用户的意图是获得对文本的智能解释，这种解释必须适合分成三段。SML通常会发现这很困难，因为他们总是会添加一个总结段落。\n\n### 说“我准备好了”。\n\n在一个基于聊天回合的应用中，遵循指令的模型通常会被要求首先阅读提供的文本，然后进行某种分析。通常情况下，SML 无法做到这一点……\n\n### 总结\n\n基本总结，没有限制。在这里，我们想评估总结是如何基于文本的，而不是凭空捏造事实。\n\n### 总结为两句\n\n基本总结，限制为两句话。在这里，我们希望评估总结是否基于文本，而不是虚构的事实，同时确保遵守两句的限制。\n\n### 列出三个主要关键点 — 格式输出\n\n```python\nkey_points = [\n    \"SML必须以特定格式输出。\",\n    \"此提示要求创建一个包含3个关键点的列表。\",\n    \"输出格式应为Python列表。\"\n]\n```\n\n### 目录\n\n这个任务对许多 SML 来说相当困难。该提示需要一些调整，否则模型会返回一个 markdown 表格。用户希望按照提供的文档结构获得一个有序的主题列表。\n\n### RAG\n\n检索增强生成，没有任何框架（haystack，Langchain…）。这是语言模型中最常用的任务之一。回复的评估基于对指令的理解能力以及答案与文本的相关性。\n\n### 真实的 RAG\n\n这是一个与提供的上下文完全无关的问题的 RAG。模型必须回复“无法回答”，这意味着它理解了指令，并且没有使用任何外部知识或虚构的信息。\n\n### 从参考文献撰写内容\n\n这是一个创造性的任务。使用参考文本，SML必须提供一篇新的文章。\n\n### 提取5个主题\n\n此任务的重点是验证：\n\n* 确保正好有5个主题\n* 它们是基于事实的（没有虚构）\n\n### 创意：1000字科幻故事\n\n完全创造性的任务。即使对于更大的模型来说，要保持连贯性并产生一个符合字数要求的小故事也是非常困难的。\n\n### 反思提示\n\n反思提示旨在验证模型的链式推理过程。输出被限制在特殊标签的开头/结尾。重点是推理和一致的输出结构。输出必须易于用于进一步的结构化提示或可视化。您可以在本文中阅读更多内容：\n\n## 评估过程\n\n在每次生成结束时，用户会被要求用0到5的分数来评估结果。**在这种情况下，用户就是我……**\n\n这种定性分析确实较为简单，因此每个分数都有一个描述，用户可以添加评论（“一些错误的信息”，“也许更改提示中的措辞会更好”）\n\n这里是带有描述的定性矩阵\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*H_Qx2UT1lxqy2BF4.png)\n\n## 好与坏 — 细节\n\n总体而言，Qwen2.5–3B-instruct 是一个全方位的模型。每个任务的平均得分为 4.4 分（满分 5 分），这相当令人印象深刻。\n\n摘要表现出色。在长文本中列出主要主题也非常好。\n\n它在 RAG 中表现突出，有时显得过于冗长：在真实的 RAG 中，它还提供了一些解释，以及 *无法回答* 回复的原因。\n\n我们可以一起看看。这对于 SML 来说，精确计算句子和段落的数量通常是一项艰巨的任务。\n\n```python\n**Write a two-sentences summary of the following text:**\n\nChina's regime under Chinese Communist Party operates without transparent \nelections and primary candidates. While undesirable for democracies, \nit allows long-term planning free from electoral pressures that can change \nplans abruptly; this has led to China achieving peak carbon dioxide emissions \nin 2023 due largely to increased renewable energy installations, potentially \nsetting a new global standard for sustainable development despite its status \nas one of the world's largest polluters.\n```\n\n目录对于语言模型来说是另一个噩梦，因为它们通常理解如何创建表格……但看看这个：\n\n```python\nA \"table of content\" provides an organized structure to the text, allowing \nreaders quick access to key points. Write the \"table of content\" of \nthe following text.\n\n[table of contents]\n1. Introduction: Overview and Critique on China's Governance System  \n2. Comparison with Western Democracies - Undesirable Dictatorship? \n3. Advantages in Long-Term Planning for Sustainability   \n4. The Chinese Model as a Comparative Advantage  \n5. Recent Achievements in Carbon Emissions Reduction\n6. Transition to Renewable Energy: A Turning Point?\n7. Potential Leadership Role of China and India  \n8. Conclusion - Future Directions\n\n```\n\n我给真实的 RAG 评分 4 分（满分 5 分），但它本可以轻松得到 5 分。最重要的是，回复中必须包含 *无法回答* 这个词。\n\n为什么？想象一下你在设置一系列提示，你需要根据先前的回复决定下一步。如果你可以简单地寻找 *无法回答* 这个词的存在来引导你的下一步，那就没问题了！\n\n```python\n**Reply to the question only using the provided context. If the answer \nis not contained in the provided context say \"unanswerable\".\nquestion: who is Anne Frank?**\n{context}\n**Remember: if you cannot answer based on the provided context, say \n\"unanswerable\"\nanswer:**\n\nUnanswerable\nThe given text does not provide any information about Anne Frank \nor her identity. The content is unrelated to this question and focuses \ninstead on discussing China's system compared to Western democracies in \nterms of long-term planning for environmental issues.\n```\n\n我想象模型也被训练来提供这种回复的原因……太棒了！\n\n### 优秀的反思\n\n反思提示的主要焦点是模型返回正确推理过程的能力，并按照几个特定的标签开闭格式化输出。\n\n到目前为止，这是**小型语言模型**中**最佳的反思输出**，甚至比Gemma2–2B更好：\n\n```python\n*👨‍💻 > You are an AI assistant designed to provide detailed, step-by-step \nresponses. Your outputs should follow this structure:\n1. Begin with a <thinking> section.\n2. Inside the thinking section:\n   a. Briefly analyze the question and outline your approach.\n   b. Present a clear plan of steps to solve the problem.\n   c. Use a \"Chain of Thought\" reasoning process if necessary, breaking down \nyour thought process into numbered steps.\n3. Include a <reflection> section for each idea where you:\n   a. Review your reasoning.\n   b. Check for potential errors or oversights.\n   c. Confirm or adjust your conclusion if necessary.\n4. Be sure to close all reflection sections.\n5. Close the thinking section with </thinking>.\n6. Provide your final answer in an <output> section.\nAlways use these tags in your responses. Be thorough in your explanations, \nshowing each step of your reasoning process. Aim to be precise and logical \nin your approach, and don't hesitate to break down complex problems into \nsimpler components. Your tone should be analytical and slightly formal, \nfocusing on clear communication of your thought process.\nRemember: Both <thinking> and <reflection> MUST be tags and must be closed \nat their conclusion\nMake sure all <tags> are on separate lines with no other text. \nDo not include other text on a line containing a tag.\n\nuser question: explain why it is crucial for teachers to learn how to use \ngenerative AI for their job and for the future of education. \nInclude relevant learning path for teachers and educators. *\n\n**💻 > <thinking>\nI will begin by analyzing the importance of teaching about Generative \nArtificial Intelligence (AI) in an educational context, then outline a \nstep-by-step plan on what steps should be taken to learn this technology \neffectively as well as provide resources or courses that could help.\nFirstly, understanding AI is crucial for teachers because it can \nsignificantly enhance their ability to support students. \nTeachers need the knowledge and skills necessary not only to integrate these tools into teaching but also understand how they work so they are able to guide both themselves and their pupils in ethical use of technology.\n</thinking>\n\n<reflection>\nMy initial thought process was clear, emphasizing that teachers should \nbe equipped with AI literacy as it can improve educational outcomes. \nI outlined the importance for guiding students ethically while using \nthese tools effectively.\nHowever, my plan needs more detail on specific steps to learn about \ngenerative AI and resources available online or in-person courses.\n</reflection>**\n*MY PERSONAL NOTE: here it is starting an additional thinking after the first \nreflection... And this is really too good to be true!!*\n**<thinking>\nTo create a comprehensive learning path that teachers could follow:\n1. **Understand Basic Concepts**: Start with basic concepts of machine \n   learning (ML) such as supervised vs unsupervised, deep neural networks  \n   etc., which are foundational to understanding generative AI.\n2. **Learn About Generative Models**:\n   - Study different types: Variational Autoencoders (VAEs), GANs.\n   - Understand how these models generate new data that resembles the \n     training set but is not identical and can be used for tasks like \n     text generation, image synthesis etc., which are crucial in education.\n3. **Explore Applications**: Look at real-world applications of generative \n   AI such as language modeling (e.g., GPT-2), computer vision techniques to \n   create realistic images or videos from raw data.\n4. **Ethical Considerations and Implications for Education**:\n   - Discuss the ethical implications, including bias in models' outputs \n     which could affect students’ perceptions about themselves if they are \n     exposed regularly through AI-generated content that is not representative \n     of them.\n5. **Hands-on Practice**: Engage with practical exercises using open-source \n   tools like TensorFlow or PyTorch to build simple generative model \n   applications.\n6. **Stay Updated and Network**:\n   - Follow the latest research in ML, especially on Generative Models \n     by following relevant journals (e.g., NeurIPS), conferences such as \n     ICML etc.\n7. Attend Workshops/Conferences: Participate actively at AI-related workshops \n   or events to network with professionals working within this field.\n</thinking>**\n*MY PERSONAL NOTE: and no surprise, an additional reflection now starts, \nbefore producing the final output*\n**<reflection>\nMy plan has been expanded and includes a more detailed approach for teachers, \ncovering foundational knowledge through practical application of generative \nmodels in education along with ethical considerations that are crucial. \nI have also included staying updated on the latest research as an important \npart.\nHowever, it's essential to ensure these resources or courses exist so \neducators can access them easily.\n</reflection>\n\n<output>\nIt is vital for teachers and educational professionals alike to learn how \ngenerative AI works because this technology has significant potential in \nenhancing teaching methods. By understanding basic concepts of machine \nlearning (ML) such as supervised vs unsupervised, deep neural networks etc., \nthey will be better equipped with the knowledge necessary not only to \nintegrate these tools into their curriculum but also guide students ethically.\nA comprehensive plan for teachers and educators includes:\n1. **Understand Basic Concepts**: Start by grasping foundational concepts of ML.\n2. **Learn About Generative Models**:\n   - Study different types: Variational Autoencoders (VAEs), GANs, etc., \n     which are crucial in generating new data that resembles the training \n     set but is not identical and can be used for tasks like text generation or \n     image synthesis.\n3. **Explore Applications**: Look at real-world applications of generative AI \n   such as language modeling using tools like TensorFlow.\n4. **Ethical Considerations**:\n   - Discuss ethical implications, including bias in models' outputs which \n     could affect students’ perceptions about themselves if they are exposed \n     regularly through AI-generated content that is not representative.\n5. **Hands-on Practice**: Engage with practical exercises to build simple \n   generative model applications using open-source tools.\n6. **Stay Updated and Network**:\n   - Follow the latest research in ML, especially on Generative Models by \n     following relevant journals (e.g., NeurIPS), conferences such as ICML etc.\n7. Attend Workshops/Conferences: Participate actively at AI-related workshops \n   or events to network with professionals working within this field.\n</output>\n```\n\n这样的精炼思维过程，以及清晰的标签开闭格式，可以方便后续的处理流程，或者在漂亮的GUI中进行美观的打印。\n\n## 它不擅长什么？\n\n最糟糕的任务是创作短篇小说。模型在生成过程中开始重复相同的段落。\n\n这并不意味着它无法做到。更有可能的是，通过提高温度和重复惩罚，可以获得良好的结果。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GNJ3lG6FM5qt9glIdf7qhQ.jpeg)\n\n## 结论\n\n您可以在我的GitHub仓库中找到所有聊天记录，以及代码和自行操作的说明。您可以参考我之前文章中的教程 [Qwen2.5 1.5b: 移动AI的未来？](https://ai.gopubby.com/qwen2-5-1-5b-the-future-of-mobile-ai-6bd5f29bbc84)\n\n在接下来的文章中，我将介绍其他小型语言模型，使用相同的原则：从小型的350M参数，到500M系列，再到3B — 经过1.5B。\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/retrieval-augmented-generation-approaches-state-of-the-art-and-optimization-strategies-456883da4801","frontmatter":{"title":"检索增强生成：方法、最新进展和优化策略","meta_title":"检索增强生成：方法、最新进展和优化策略","description":"⭐ RAG 在知识密集型场景或需要持续知识的特定领域应用中特别有用……","date":"2024-10-31T08:17:32.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*_vE7WktGmyQ5xg_t5cpFVg.jpeg","categories":["Generative AI","Natural Language Processing","Machine Learning"],"author":"Rifx.Online","tags":["RAG","retrieval","generation","optimization","embeddings"],"draft":false,"slug":"blog/retrieval-augmented-generation-approaches-state-of-the-art-and-optimization-strategies-456883da4801"},"content":"\n\n\n\n\n⭐ RAG 在知识密集型场景或需要持续更新知识的特定领域应用中尤其有用。最近，RAG 因其在对话代理中的应用而受到广泛关注。\n\n📌 参考研究主要集中在当前的 RAG 方法及其不同组件、最新进展（SOTA）、应用、检索、生成、增强技术的评估上。\n\n随着 RAG 系统从简单到高级再到模块化的演变，每个阶段都是为了应对特定用例的增强而出现的。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*P2ByKtayhF4XgAVxRI1urQ.jpeg)\n\n✏ *简单型*：用户输入用于文档查询，附加/组合到提示中，用于模型最终响应生成。通过多轮对话交互，可以将上下文/对话历史添加/组合到提示中。缺点：低精度/低召回率，冗余，重复。\n\n✏ *高级型*：通过优化预检索、检索和后检索方法来提高检索质量。预检索中，通过增强数据粒度、索引结构改进、元数据、对齐和混合检索来提升质量。在检索中，优化嵌入模型，从而优化上下文。在后检索中，优化上下文窗口和噪声/干扰数据的拒绝。\n\n✏ *模块化*：引入搜索模块进行相似性检索和检索的微调。新模块包括搜索、记忆、融合、路由、预测、任务适配器。\n\n🥉 优化 RAG 流水线：\n\n📜 *混合搜索探索*：通过智能利用关键字搜索、语义搜索和向量搜索等技术，平衡性能优化。\n\n📜 *递归检索和查询引擎*：初始阶段可能通过获取较小的块开始检索，随后以更好且更具上下文的信息获取较大的块，以平衡上下文丰富的响应与效率。\n\n📜 *回退提示*：这鼓励 LLM 脱离特定实例，围绕更广泛的概念和原则进行推理（arXiv:2310\\.13243）。在各种具有挑战性的基于推理的任务中，使用回退提示时观察到显著的性能提升，突显了它们对 RAG 过程的自然适应性。\n\n📜 *子查询*：根据场景应用的查询策略可以使用，例如利用 LlamaIndex 提供的查询引擎，利用树查询，使用向量查询，或执行简单的块顺序查询。\n\n📜 *假设文档嵌入*：使用 LLM，HyDE 通过创建假设答案来响应查询，嵌入答案，并使用相同的答案来检索真实文档。该方法关注的是从一个答案到另一个答案的嵌入相似性，而不是基于查询寻找嵌入相似性\\[arXiv:2212\\.10496]。缺点：不一致的答案未能产生理想结果，LLM 未见主题的错误，导致错误。\n\n让我在这里结束。我会在后续中发布新文章。\n\n[\\#genai](https://www.linkedin.com/feed/hashtag/?keywords=genai&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7170160104984571905) [\\#rag](https://www.linkedin.com/feed/hashtag/?keywords=rag&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7170160104984571905) \\#ai \\#llm\n\n参考：[arxiv:2312\\.10997](https://arxiv.org/pdf/2312.10997)，RAG 调查，Huggingfaceblogs\n\n"},{"lang":"zh","group":"blog","slug":"blog/the-6-best-llm-tools-to-run-models-locally-eedd0f7c2bbd","frontmatter":{"title":"6 种最佳本地运行模型的 LLM 工具","meta_title":"6 种最佳本地运行模型的 LLM 工具","description":"运行大型语言模型 (LLM)（例如 ChatGPT 和 Claude）通常涉及将数据发送到由 OpenAI 和其他 AI 模型管理的服务器……","date":"2024-10-24T17:47:43.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*2MB6-INUUGLR0NR_iOACIg.jpeg","categories":["Technology","Programming","Health"],"author":"Rifx.Online","tags":["LLM","local","deployment","customization","telehealth"],"draft":false,"slug":"blog/the-6-best-llm-tools-to-run-models-locally-eedd0f7c2bbd"},"content":"\n\n\n\n\n运行大型语言模型（LLMs）如 [ChatGPT](https://openai.com/chatgpt/mac/) 和 [Claude](https://claude.ai/) 通常涉及将数据发送到由 [OpenAI](https://openai.com/) 和其他 AI 模型提供商管理的服务器。虽然这些服务是安全的，但一些企业更倾向于将其数据完全离线，以获得更高的隐私保护。\n\n本文将介绍开发人员可以使用的六款工具，以便在本地运行和测试 LLM，确保他们的数据永远不会离开他们的设备，这类似于 [端到端加密](https://getstream.io/blog/end-to-end-encryption/) 保护隐私的方式。\n\n## 为什么使用本地 LLM？\n\n像 [LM Studio](https://lmstudio.ai/) 这样的工具在用户使用它来运行本地 LLM 时，不会收集用户数据或跟踪用户的行为。它允许所有聊天数据保留在本地计算机上，而不与 AI/ML 服务器共享。\n\n* **隐私**：您可以以多轮的方式提示本地 LLM，而不会让您的提示数据离开本地主机。\n* **自定义选项**：本地 LLM 提供 CPU 线程、温度、上下文长度、GPU 设置等高级配置选项。这类似于 OpenAI 的游乐场。\n* **支持和安全性**：它们提供与 OpenAI 或 Claude 相似的支持和安全性。\n* **订阅和费用**：这些工具免费使用，并且不需要每月订阅。对于像 OpenAI 这样的云服务，每个 API 请求都需要付费。本地 LLM 有助于节省费用，因为没有每月订阅。\n* **离线支持**：您可以在离线时加载和连接大型语言模型。\n* **连接性**：有时，连接到像 OpenAI 这样的云服务可能会导致信号和连接不良。\n\n## 六大免费本地 LLM 工具\n\n根据您的具体使用案例，您可以选择几种离线 LLM 应用程序。这些工具中有一些完全免费供个人和商业使用。其他工具可能需要您发送请求以用于商业用途。对于 Mac、Windows 和 Linux，有多种本地 LLM 工具可供选择。以下是您可以选择的六个最佳工具。\n\n## 1. LM Studio\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*svbQPZKu08of7Kv6)\n\n[LM Studio](https://lmstudio.ai/) 可以运行任何格式为 `gguf` 的模型文件。它支持来自模型提供商的 `gguf` 文件，如 [Llama 3.1](https://llama.meta.com/)、[Phi 3](https://huggingface.co/docs/transformers/main/en/model_doc/phi3)、[Mistral](https://mistral.ai/) 和 [Gemma](https://ai.google.dev/gemma)。要使用 LM Studio，请访问上述链接并下载适合您机器的应用程序。启动 LM Studio 后，主页会展示可下载和测试的顶级 LLM。还有一个搜索栏，可以筛选并下载来自不同 AI 提供商的特定模型。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*sbS3VqiLgDsftgs2)\n\n从特定公司的模型中搜索会显示多个模型，范围从小型到大型 [quantization](https://huggingface.co/docs/optimum/en/concept_guides/quantization)。根据您的机器，LM Studio 会使用兼容性猜测来突出显示适合该机器或平台的模型。\n\n## LM Studio 的关键特性\n\nLM Studio 提供与 ChatGPT 相似的功能和特性。它具有多个功能。以下是 LM Studio 的关键特性。\n\n* **模型参数自定义**：这允许您调整温度、最大令牌、频率惩罚等。\n* **聊天历史**：允许您保存提示以供后续使用。\n* **参数和用户界面提示**：您可以将鼠标悬停在信息按钮上以查找模型参数和术语。\n* **跨平台**：LM Studio 可在 Linux、Mac 和 Windows 操作系统上使用。\n* **机器规格检查**：LM Studio 检查计算机规格，如 GPU 和内存，并报告兼容的模型。这可以防止下载可能在特定机器上无法运行的模型。\n* **AI 聊天和游乐场**：以多轮聊天格式与大型语言模型进行对话，并通过同时加载多个 LLM 进行实验。\n* **开发者本地推理服务器**：允许开发者设置一个类似于 OpenAI API 的本地 HTTP 服务器。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*9bHmRiOSf6gm-u3P)\n\n本地服务器提供示例 Curl 和 Python 客户端请求。此功能有助于使用 LM Studio 构建 AI 应用程序，以访问特定的 LLM。\n\n```python\n## Example: reuse your existing OpenAI setup\nfrom openai import OpenAI\n\n## Point to the local server\nclient = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n\ncompletion = client.chat.completions.create(\n  model=\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"Always answer in rhymes.\"},\n    {\"role\": \"user\", \"content\": \"Introduce yourself.\"}\n  ],\n  temperature=0.7,\n)\n\nprint(completion.choices[0].message)\n```\n通过上述示例 Python 代码，您可以重用现有的 OpenAI 配置，并将基本 URL 修改为指向您的本地主机。\n\n* **OpenAI 的 Python 库导入**：LM Studio 允许开发者导入 OpenAI Python 库，并将基本 URL 指向本地服务器（localhost）。\n* **多模型会话**：使用单个提示并选择多个模型进行评估。\n\n## 使用 LM Studio 的好处\n\n该工具可供个人免费使用，允许开发者通过应用内聊天用户界面和游乐场运行 LLM。它提供了一个华丽且易于使用的界面，带有过滤器，并支持连接到 OpenAI 的 Python 库，无需 API 密钥。公司和企业可以根据要求使用 LM Studio。然而，它需要 M1/M2/M3 Mac 或更高版本，或具有支持 [AVX2](https://edc.intel.com/content/www/us/en/design/ipla/software-development-platforms/client/platforms/alder-lake-desktop/12th-generation-intel-core-processors-datasheet-volume-1-of-2/009/intel-advanced-vector-extensions-2-intel-avx2/) 的处理器的 Windows PC。Intel 和 [AMD](https://www.amd.com/en/support/download/drivers.html) 用户仅限于使用 [v0.2.31](https://lmstudio.ai/) 中的 Vulkan 推理引擎。\n\n## 2. Jan\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*7YeH_48iFYB4lDRu)\n\n将 [Jan](https://jan.ai/) 理解为一个设计用于离线操作的开源版本的 ChatGPT。它由一个用户社区构建，秉持用户拥有的理念。Jan 允许您在设备上运行流行的模型，如 [Mistral](https://huggingface.co/models?other=mistral) 或 [Llama](https://huggingface.co/models?other=llama)，而无需连接互联网。使用 Jan，您可以访问远程 API，如 OpenAI 和 [Groq](https://groq.com/)。\n\n## Jan 的主要特点\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*ufyOE6QkcHw8X5U7)\n\nJan 是一款电子应用程序，其功能类似于 LM Studio。它通过将消费者设备转变为 AI 计算机，使 AI 变得开放和可访问。由于这是一个开源项目，开发者可以为其贡献代码并扩展其功能。以下是 Jan 的主要特点。\n\n* **本地**：您可以在设备上运行您喜欢的 AI 模型，而无需将其连接到互联网。\n* **即用模型**：下载 Jan 后，您将获得一组已安装的模型以供开始使用。也可以搜索特定模型。\n* **模型导入**：支持从 Hugging Face 等来源导入模型。\n* **免费、跨平台和开源**：Jan 完全免费，开源，并可在 Mac、Windows 和 Linux 上运行。\n* **自定义推理参数**：调整模型参数，如最大令牌、温度、流、频率惩罚等。所有偏好设置、模型使用和配置都保留在您的计算机上。\n* **扩展**：Jan 支持 [TensortRT](https://github.com/NVIDIA/TensorRT) 和 [Inference Nitro](https://huggingface.co/jan-hq/nitro-v1.2-e3) 等扩展，以自定义和增强您的 AI 模型。\n\n## 使用 Jan 的好处\n\nJan 提供了一个干净简单的界面来与 LLM 互动，并且将所有数据和处理信息保存在本地。它已经为您安装了超过七十个大型语言模型供您使用。这些现成的模型的可用性使得连接和与远程 API（如 OpenAI 和 Mistral）互动变得简单。Jan 还有一个很棒的 [GitHub](https://github.com/janhq/jan)、[Discord](https://discord.gg/FTk2MvZwJH) 和 [Hugging Face](https://huggingface.co/janhq) 社区，可以关注并寻求帮助。然而，像所有 LLM 工具一样，这些模型在 Apple Silicon Macs 上的运行速度比在 Intel 机器上更快。\n\n## 3. Llamafile\n\n[Llamafile](https://github.com/Mozilla-Ocho/llamafile) 由 [Mozilla](https://www.mozilla.org/en-US/?v=1) 支持，旨在通过快速的 [CPU 推理](https://huggingface.co/docs/transformers/en/perf_infer_cpu) 使开源 AI 对每个人都可访问，而无需网络连接。它将 LLM 转换为多平台的 [可执行链接格式](https://gist.github.com/x0nu11byt3/bcb35c3de461e5fb66173071a2379779) (ELF)。它提供了将 AI [集成](https://getstream.io/chat/solutions/ai-integration/) 到应用程序中的最佳选项之一，使您能够仅通过一个可执行文件运行 LLM。\n\n## Llamafile 的工作原理\n\n它旨在将权重转换为多个可执行程序，这些程序无需安装即可在 Windows、MacOS、Linux、Intel、ARM、FreeBSD 等架构上运行。在底层，Llamafile 使用 [tinyBLAST](https://github.com/ggerganov/llama.cpp/issues/5048) 在像 Windows 这样的操作系统上运行，而无需 SDK。\n\n## Llamafile 的关键特性\n\n* **可执行文件**：与 LM Studio 和 Jan 等其他 LLM 工具不同，Llamafile 只需一个可执行文件即可运行 LLM。\n* **使用现有模型**：Llamafile 支持使用现有的模型工具，如 Ollama 和 LM Studio。\n* **访问或创建模型**：您可以访问 OpenAI、Mistral、Groq 等流行 LLM。它还提供从头创建模型的支持。\n* **模型文件转换**：您可以将许多流行 LLM 的文件格式转换，例如，将 `.gguf` 转换为 `.llamafile` 只需一个命令。\n\n`llamafile-convert mistral-7b.gguf`\n\n## 开始使用 Llamafile\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*4PV1KsCZvvVKqFll)\n\n要安装 Llamafile，请访问 Huggingface 网站，从导航中选择 **Models**，然后搜索 **Llamafile**。您还可以从下面的 URL 安装您喜欢的 [量化](https://huggingface.co/docs/optimum/en/concept_guides/quantization) 版本。\n\n[`https://huggingface.co/Mozilla/Meta-Llama-3.1-8B-Instruct-llamafile/tree/m`ain](https://huggingface.co/Mozilla/Meta-Llama-3.1-8B-Instruct-llamafile/tree/main)\n\n**注意**：量化数字越大，响应越好。正如上图所示，本文使用 `Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile`，其中 `Q6` 代表量化数字。\n\n**步骤 1：下载 Llamafile**\n\n从上面的链接，点击任意下载按钮以获取您喜欢的版本。如果您在机器上安装了 [wget](https://www.gnu.org/software/wget/) 工具，您可以使用以下命令下载 Llamafile。\n\n`wget <https://huggingface.co/Mozilla/Meta-Llama-3.1-8B-Instruct-llamafile/blob/main/Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile>`\n\n您应该用您喜欢的版本替换 URL。\n\n**步骤 2：使 Llamafile 可执行**\n\n下载特定版本的 Llamafile 后，您应该通过导航到文件位置，使用以下命令使其可执行。\n\n`chmod +x Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile`**步骤 3：运行 Llamafile**\n\n在文件名之前添加一个点和斜杠 `./` 来启动 Llamafile。\n\n`./Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile`\n\nLlamafile 应用程序现在将在 `http://127.0.0.1:8080` 可用，以运行您的各种 LLMs。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*1xrwDPTfNgmEQDTx)\n\n## 使用 Llamafile 的好处\n\nLlamafile 通过使 LLM 容易被消费者 CPU 访问，帮助实现 AI 和 ML 的民主化。与其他本地 LLM 应用程序如 **Llama.cpp** 相比，Llamafile 提供了最快的提示处理体验，并在游戏电脑上表现更佳。由于其更快的性能，它是总结长文本和大型文档的绝佳选择。它完全离线运行并保护隐私，因此用户不会将数据分享给任何 AI 服务器或 API。像 Hugging Face 这样的机器学习社区支持 Llamafile 格式，使得搜索与 Llamafile 相关的模型变得容易。它还有一个出色的开源社区，进一步开发和扩展它。\n\n## 4. GPT4ALL\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*j3vNWWQZCVF5woo5)\n\nGPT4ALL 基于隐私、安全和无需互联网的原则构建。用户可以在 Mac、Windows 和 Ubuntu 上 [安装](https://www.nomic.ai/gpt4all)。与 Jan 或 LM Studio 相比，GPT4ALL 拥有更多的每月下载量、[GitHub Stars](https://github.com/nomic-ai/gpt4all) 和活跃用户。\n\n## GPT4ALL的主要特点\n\nGPT4All可以在主要消费硬件上运行LLM，例如Mac M系列芯片、AMD和NVIDIA GPU。以下是其主要特点。\n\n* **隐私优先**：将私人和敏感的聊天信息和提示仅保留在您的设备上。\n* **无需互联网**：它完全离线工作。\n* **模型探索**：此功能允许开发者浏览和下载不同类型的LLM进行实验。您可以从流行选项中选择大约1000个开源语言模型，如LLama、Mistral等。\n* **本地文档**：您可以让本地LLM访问您的敏感数据，使用本地文档如`.pdf`和`.txt`，数据不会离开您的设备，也无需网络。\n* **自定义选项**：它提供多个[聊天机器人](https://getstream.io/blog/llm-chatbot-docs/)调整选项，如温度、批处理大小、上下文长度等。\n* **企业版**：GPT4ALL提供企业套餐，具备安全性、支持和每台设备的许可证，将本地AI带入企业。\n\n## 开始使用 GPT4All\n\n要开始使用 GPT4All 在本地运行 LLMs，请[下载](https://www.nomic.ai/gpt4all)适合您操作系统的版本。\n\n## 使用GPT4ALL的好处\n\n除了Ollama，GPT4ALL在GitHub贡献者数量上最为显著，拥有约250000名每月活跃用户（根据<https://www.nomic.ai/gpt4all>）并且与其竞争对手相比。该应用收集有关使用分析和聊天分享的匿名用户数据。然而，用户可以选择加入或退出。使用GPT4ALL，开发者可以从其庞大的用户基础、GitHub和Discord社区中受益。\n\n## 5. Ollama\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*STAonWgWIsY6cgDR)\n\n使用 [Ollama](https://ollama.com/)，您可以轻松创建本地聊天机器人，而无需连接到像 OpenAI 这样的 API。由于一切都在本地运行，您无需支付任何订阅费或 API 调用费用。\n\n## Ollama 的关键特性\n\n* **模型自定义**：Ollama 允许您转换 `.gguf` 模型文件并使用 `ollama run modelname` 运行它们。\n* **模型库**：Ollama 拥有大量模型可供尝试，访问 [ollama.com/library](https://ollama.com/library)。\n* **导入模型**：Ollama 支持从 [PyTorch](https://pytorch.org/) 导入模型。\n* **社区集成**：Ollama 无缝集成到网页和桌面应用程序中，例如 [Ollama-SwiftUI](https://github.com/kghandour/Ollama-SwiftUI)、[HTML UI](https://github.com/rtcfirefly/ollama-ui)、[Dify.ai](https://github.com/rtcfirefly/ollama-ui) 和 [更多](https://github.com/ollama/ollama?tab=readme-ov-file#community-integrations)。\n* **数据库连接**：Ollama 支持多个 [数据平台](https://github.com/mindsdb/mindsdb/blob/main/mindsdb/integrations/handlers/ollama_handler/README.md)。\n* **移动集成**：像 [Enchanted](https://github.com/AugustDev/enchanted) 这样的 SwiftUI 应用将 Ollama 带入 iOS、macOS 和 visionOS。[Maid](https://github.com/Mobile-Artificial-Intelligence/maid) 也是一个跨平台的 Flutter 应用，能够本地处理 `.gguf` 模型文件。\n\n## 开始使用 Ollama\n\n要首次使用 Ollama，请访问 <https://ollama.com> 并下载适合您机器的版本。您可以在 Mac、Linux 或 Windows 上安装它。安装 Ollama 后，您可以在终端中使用以下命令检查其详细信息。\n\n`ollama`\n\n要运行特定的 LLM，您应该使用以下命令下载它：\n\n`ollama pull modelname`，其中 `modelname` 是您要安装的模型名称。请在 [GitHub](https://github.com/ollama/ollama) 上查看一些可供下载的示例模型。`pull` 命令也用于更新模型。一旦使用，仅会获取差异部分。\n\n例如，在下载了 `llama3.1` 后，在命令行中运行 `ollama run llama3.1` 将启动该模型。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*aglZm6h0BU6GAYkSl04XWA.gif)\n\n在上述示例中，我们提示 `llama3.1` 模型解决一个物理功和能量的问题。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*dNNQYpz1s2tz1pcn)\n\n## 使用 Ollama 的好处\n\nOllama 在 GitHub 上拥有超过 200 名贡献者，并且有活跃的更新。它拥有最多的贡献者，并且在上述其他开源 LLM 工具中更具可扩展性。\n\n## 6. LLaMa.cpp\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*KhsAUquhDZAHghxK)\n\n[LLaMa.cpp](https://github.com/ggerganov/llama.cpp) 是支持本地 LLM 工具（如 Ollama 等）的底层后端技术（推理引擎）。LLaMa.cpp 支持显著的大型语言模型推理，配置简单，并在各种硬件上提供出色的本地性能。它也可以在云端运行。\n\n## LLaMa.cpp 的主要特点\n\n* **设置**：它的设置非常简单。您只需一个命令即可安装。\n* **性能**：它在本地和云端的各种硬件上表现非常出色。\n* **支持的模型**：它支持流行的主要 LLM，如 [Mistral 7B](https://huggingface.co/mistralai/Mistral-7B-v0.1)、[Mixtral MoE](https://huggingface.co/models?search=mistral-ai/Mixtral)、[DBRX](https://huggingface.co/databricks/dbrx-instruct)、[Falcon](https://huggingface.co/models?search=tiiuae/falcon) 和 [其他许多模型](https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#description)。\n* **前端 AI 工具**：LLaMa.cpp 支持开源 LLM UI 工具，如 [MindWorkAI/AI-Studio](https://github.com/MindWorkAI/AI-Studio) (FSL-1.1-MIT)、[iohub/collama](https://github.com/iohub/coLLaMA) 等。\n\n## 使用 LLaMa.cpp 开始\n\n要运行您的第一个本地大型语言模型，请使用以下命令安装 llama.cpp：\n\n`brew install llama.cpp`\n\n接下来，从 Hugging Face 或其他来源下载您想要运行的模型。例如，从 Hugging Face 下载下面的模型并将其保存在您计算机上的某个位置。\n\n[`https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q4_K_M.g`guf](https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf)\n\n使用您喜欢的命令行工具，如终端，`cd` 进入您刚下载的 `.gguf` 模型文件的位置，并运行以下命令。\n\n```python\nllama-cli --color \\ \n-m Mistral-7B-Instruct-v0.3.Q4_K_M.ggufb \\ \n-p \"Write a short intro about SwiftUI\"\n```\n总之，您首先调用 LLaMa CLI 工具并设置颜色和其他标志。`-m` 标志指定您要使用的模型的路径。`-p` 标志指定您希望用来指示模型的提示。\n\n运行上述命令后，您将看到以下预览中的结果。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*4Al-j50vXUXLUfvxzBt6aw.gif)\n\n## 本地 LLM 的用例\n\n在本地运行 LLM 可以帮助开发人员深入了解其性能和工作原理。 本地 LLM 可以查询私有文档和技术论文，以便与这些文档相关的信息不会离开用于查询的设备，不会发送到任何云 AI API。 本地 LLM 在没有互联网的地方和网络信号较差的地方非常有用。\n\n在 [远程医疗环境](https://getstream.io/blog/telemedicine-app-development/) 中，本地 LLM 可以对患者文档进行排序，而无需出于隐私考虑将其上传到任何 AI API 提供商。\n\n## 评估本地运行的大型语言模型性能\n\n在本地使用大型语言模型之前，了解其性能对于获得所需的响应至关重要。有几种方法可以确定特定 LLM 的性能。以下是一些方法。\n\n* **训练**：该模型是基于什么数据集进行训练的？\n* **微调**：模型在多大程度上可以定制以执行特定任务，或者是否可以针对特定领域进行微调？\n* **学术研究**：该 LLM 是否有学术研究论文？\n\n要回答上述问题，您可以查看优秀的资源，如 [Hugging Face](https://huggingface.co/datasets) 和 [Arxiv.org](https://arxiv.org/)。此外，[Open LLm Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) 和 [LMSYS Chatbot Arena](https://chat.lmsys.org/?arena) 提供了各种 LLM 的详细信息和基准测试。\n\n## 本地 LLM 工具结论\n\n正如本文所讨论的，选择和使用本地大型语言模型的动机有很多。如果您不希望将数据集通过互联网发送给 AI API 提供商，则可以对模型进行微调，以执行 [远程医疗应用](https://getstream.io/chat/solutions/healthcare/) 中的特定任务。许多开源的图形用户界面（GUI）本地 LLM 工具，如 LLm Studio 和 Jan，提供直观的前端用户界面，以便在没有像 OpenAI 或 Claude 这样的订阅服务的情况下配置和实验 LLM。您还发现了各种强大的命令行 LLM 应用程序，如 Ollama 和 LLaMa.cpp，帮助您在本地运行和测试模型，而无需互联网连接。查看 Stream 的 [AI 聊天机器人](https://getstream.io/chat/solutions/ai-integration/) 解决方案，将 AI 聊天集成到您的应用中，并访问所有相关链接以了解更多信息。\n\n*最初发布于 [https://getstream.io](https://getstream.io/blog/best-local-llm-tools/).*\n\n"},{"lang":"zh","group":"blog","slug":"blog/the-quest-for-production-quality-graph-rag-easy-to-start-hard-to-finish-46ca404cee3d","frontmatter":{"title":"追求生产质量 Graph RAG：开始容易，完成难","meta_title":"追求生产质量 Graph RAG：开始容易，完成难","description":"克服 RAG 图生产化挑战","date":"2024-11-01T03:56:04.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*RMudHNmBOgXM1Mubj1UTkw.jpeg","categories":["Programming","Data Science","Generative AI"],"author":"Rifx.Online","tags":["graph","RAG","production","uncertainty","optimization"],"draft":false,"slug":"blog/the-quest-for-production-quality-graph-rag-easy-to-start-hard-to-finish-46ca404cee3d"},"content":"\n\n\n### 克服图形 RAG 生产化的挑战\n\n\n\n当我阅读最近在 VentureBeat 上关于 Glean [刚刚在最新融资轮中获得超过 2.6 亿美元的文章](https://venturebeat.com/data-infrastructure/how-to-take-advantage-of-a-generative-tool-fueling-gleans-260m-raise-graph-rag/)时，我有两个直接的直觉。首先，看到这个非常公开的图形 RAG 示例充分发挥其作为一种强大、有价值的技术的潜力，能够比以往任何时候都更高效地将人们与知识连接起来，这让我感到满意。其次，读到以下内容让我感到惊讶但又颇具验证性：\n\n> 世界上最大的共享出行公司之一亲身体验了其带来的好处。在专门投入整个工程团队开发类似的内部解决方案后，他们最终决定转向 Glean 的平台。\n\n> “在一个月内，他们在 Glean 平台上的使用量翻了一番，因为结果是显而易见的，”Glean 的首席营销官 Matt Kixmoeller 说。\n\n虽然我对新闻文章中提到的失败感到惊讶，但根据我自己的经验以及同事和客户的经历，努力将图形 RAG 推向生产是我所预料的。我并不是说我期望大型科技公司在构建自己的图形 RAG 系统时会失败。**我只是期望大多数人会在构建和生产化图形 RAG 时遇到困难——即使他们已经有一个非常成功的概念验证。**\n\n我在 [The New Stack 上对 VentureBeat 文章做了一个高层次的反应](https://bit.ly/4fjIlgJ)，在这篇文章中，我想深入探讨为什么图形 RAG 可能如此难以做到正确。首先，我将指出，利用最新工具，开始使用图形 RAG 变得多么简单。然后，我将深入探讨一些特定的图形 RAG 挑战，这些挑战使其从研发转向生产变得如此困难。最后，我将分享一些关于如何最大化成功机会的图形 RAG 的建议。\n\n## 开始使用图形 RAG 很简单\n\n如果一家大型共享出行公司无法有效构建自己的平台，那么我为什么会说自己实现图形 RAG 很简单呢？\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*l6EiwfjeUGLjVlYeiY1lqA.jpeg)\n\n首先，支持 RAG 和图形 RAG 的技术在过去一年中取得了长足的进步。十二个月前，大多数企业甚至没有听说过检索增强生成。现在，RAG 支持不仅是 [像 LangChain 这样的最佳 AI 构建工具的关键特性](https://python.langchain.com/docs/tutorials/rag/)，而且几乎每个主要的 AI 参与者都有 RAG 教程，甚至还有 [Coursera 课程](https://www.coursera.org/projects/introduction-to-rag)。尝试 RAG 的快速入门途径层出不穷。\n\n微软可能不是第一个做图形 RAG 的公司，但他们在今年早些时候发布的 [研究博客文章](https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/) 中对这一概念进行了大力推动，并继续致力于相关技术的研究。\n\n在 Medium 上，还有一篇来自 [谷歌的一位生成 AI 工程师](https://towardsdatascience.com/graph-rag-a-conceptual-introduction-41cd0d431375) 的很好的概念介绍，包含了一些技术细节。此外，在 Towards Data Science 上，还有一篇最近的、非常详尽的 [关于构建图形 RAG 系统的操作指南](https://towardsdatascience.com/how-to-implement-graph-rag-using-knowledge-graphs-and-vector-databases-60bb69a22759)，以及在科学出版物数据集上进行测试的内容。\n\n在传统图形数据库和分析领域，知名品牌 Neo4j 在其旗舰图形数据库产品中增加了向量能力，以响应最近的生成 AI 革命，并为需要复杂图形分析和深度图形算法的项目提供了一系列优秀的工具平台，除了标准的图形 RAG 功能外。他们还提供了 [图形 RAG 入门指南](https://neo4j.com/developer-blog/graphrag-ecosystem-tools/)。\n\n另一方面，[您甚至不需要图形数据库就可以做图形 RAG](https://bit.ly/3YD5NAd)。许多刚接触图形 RAG 的人认为他们需要部署一个专门的图形数据库，但这并不是必要的，实际上可能会使您的技术栈变得更加复杂。\n\n我的雇主 DataStax 也有 [图形 RAG 指南](https://bit.ly/4862Lrl)。\n\n当然，两个最受欢迎的生成 AI 应用程序组合框架，[LangChain](https://blog.langchain.dev/enhancing-rag-based-applications-accuracy-by-constructing-and-leveraging-knowledge-graphs/) 和 [LlamaIndex](https://docs.llamaindex.ai/en/stable/examples/cookbooks/GraphRAG_v1/)，各自都有自己的图形 RAG 介绍。此外，还有一篇 [DataCamp 文章](https://www.datacamp.com/tutorial/knowledge-graph-rag) 同时使用了这两者。\n\n有了所有可用的工具和教程，开始使用图形 RAG 是简单的部分……\n\n## …但将图形 RAG 投入生产是困难的\n\n这是数据科学中一个非常古老的故事：一种新的软件方法论、技术或工具在研究环境中解决了一些重要问题，但行业在将其构建为每天提供价值的产品时却面临困难。这不仅仅是软件开发的努力和专业水平的问题——即使是最大的、最优秀的团队也可能无法克服解决现实世界问题所涉及的现实数据的不确定性、不可预测性和不可控性。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*OklHNrhsNHZF6qzeRUSd_w.jpeg)\n\n不确定性是构建和使用数据中心系统的固有部分，这些系统几乎总是具有某些随机性、概率或无界输入的元素。而且，当输入和输出是非结构化的时，不确定性可能会更大，这正是 LLM 和其他 GenAI 应用程序的自然语言输入和输出的情况。\n\n想要尝试图形 RAG 的人通常已经拥有一个现有的 RAG 应用程序，该应用程序在简单用例中表现良好，但在一些更复杂的用例和需要跨知识库多个信息片段的提示中失败，可能涉及不同的文档、上下文、格式或甚至数据存储。当回答问题所需的所有信息都在知识库中，但 RAG 系统找不到时，这似乎是一个失败。从用户体验 (UX) 的角度来看，确实如此——没有给出正确的答案。\n\n但这并不一定意味着 RAG 系统存在“问题”，它可能正如其设计那样运行。如果没有问题或错误，但我们仍然没有得到想要的响应，那一定意味着我们期望 RAG 系统具备它根本没有的能力。\n\n在我们具体探讨为什么将图形 RAG 投入生产很困难之前，让我们先看看我们试图解决的问题。\n\n## 图形 RAG 解决的主要挑战\n\n因为普通的 RAG 系统（没有知识图谱）仅基于向量搜索来检索文档，所以只能检索与查询在语义上最相似的文档。那些完全不相似或相似度不够的文档则被排除在外，通常不会在查询时提供给生成响应的 LLM。\n\n当我们需要回答提示中的问题的文档并不都是与提示在语义上相似时，RAG 系统往往会遗漏一个或多个文档。这种情况可能发生在回答问题时需要混合一般性和专业性的文档或术语，并且当文档在细节上非常密集时，某些对这个特定提示非常重要的细节可能会被埋没在与该提示不太相关的相关细节中。请参见 [这篇文章，了解 RAG 如何遗漏文档的例子](https://bit.ly/3BKZAJv)，因为两个相关概念（在这种情况下是“太空针”和“下皇后安妮社区”）在语义上并不相似，并且 [请参见这篇文章，了解重要细节如何被埋没的例子](https://bit.ly/4ffhrqi)，因为向量嵌入是“有损”的。\n\n当我们看到检索“失败”未能找到正确的文档时，可能会想要尝试改进向量搜索或使其更贴合我们的用例。但这需要调整嵌入，而嵌入是复杂的、混乱的、计算成本高昂的，甚至微调的成本更高。此外，这甚至不是解决问题的最佳方法。\n\n例如，看看上面链接的例子，我们真的想使用一个将“太空针”和“下皇后安妮社区”在语义向量空间中放得很近的嵌入算法吗？不，微调或寻找一个将这两个术语在语义空间中放得非常近的嵌入算法可能会产生一些意想不到和不希望的副作用。\n\n最好不要强迫语义模型去做一个地理或旅游信息更适合的工作。如果我是一个依赖于了解这些地标所在社区的旅行或旅游公司，我宁愿建立一个能够确定这些信息的数据库——这个任务比让语义向量搜索完成同样的任务要容易得多……而且没有完全的确定性。\n\n因此，这里主要的问题是，我们有一些我们知道以某种方式相关的概念和信息，但在语义向量空间中并不相关。某些其他（非向量）信息来源告诉我们，我们正在处理的各种概念之间存在联系。构建图形 RAG 应用的任务是有效地将这些概念之间的连接捕捉到知识图谱中，并利用图形连接来检索更相关的文档，以响应提示。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*flPVNMUm83oc7H9Lt7U5AA.jpeg)\n\n总结一下我们试图通过图形 RAG 解决的问题：存在半结构化、非语义的信息连接着我非结构化文档中出现的许多概念——我希望利用这些连接信息来补充语义向量搜索，以检索最适合回答我用例中提示和问题的文档。我们只是想让检索变得更好，并希望使用一些外部信息或外部逻辑来实现这一点，而不是仅仅依赖语义向量搜索来连接提示与文档。\n\n## 将图形与 RAG 集成的指导原则\n\n考虑到上述动机——使用“外部”信息来建立语义搜索可能遗漏的文档连接——在构建和测试图形 RAG 应用程序时，我们可以牢记一些指导原则：\n\n1. 图形应包含高质量、有意义的概念和连接\n2. 概念和连接应与用例集中的提示相关\n3. 图形连接应补充而不是替代向量搜索\n4. 应优先考虑一步和两步图形连接的实用性；依赖于超过三步的连接应仅限于专业用例。\n\n也许在未来的文章中，我们将深入探讨遵循这些原则的细微差别和潜在影响，但目前我只想指出，这个列表旨在共同提高可解释性，防止过于复杂，并最大化构建和使用图形 RAG 系统的效率。\n\n遵循这些原则以及软件工程和数据科学的其他核心原则，可以增加成功构建有用且强大的图形 RAG 应用程序的机会，但在此过程中肯定会有陷阱，我们将在下一节中概述。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*twgres708JPQHa1uZrkwDA.jpeg)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*5U0k4GoHTFiQhKM2a6xdMA.jpeg)\n\n## 你的图形 RAG 应用可能无法投入生产的原因\n\n任何花费大量时间围绕数据、复杂算法、统计学和人类用户构建软件的人都可能理解，在构建像图形 RAG 这样的系统时存在很多不确定性。在数据准备和加载、构建知识图谱、查询和遍历图形、结果汇编和提示构建，以及几乎工作流程中的任何其他点，都可能发生意外情况。\n\n在上面，我们讨论了如何轻松实现图形 RAG 以获得初步结果，但要获得良好的结果，更不用说生产级别的结果了，可能会很困难。接下来，我们将看看在构建和测试图形 RAG 应用时可能遇到的一些潜在问题。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*1J9hwwZDuYZ3WNrxCl_cOA.jpeg)\n\n### 图形RAG的表现与普通RAG相差无几\n\n如果您的图形RAG系统的性能与普通RAG大致相同，可能有多种原因。一般来说，这似乎意味着图形并没有为系统增加价值，但这可能是由于低质量的知识图、图的利用不足、参数设置不佳或其他许多原因造成的。或者，根本没有问题；向量搜索可能在寻找正确的文档方面表现出色，而图形根本不需要。\n\n需要关注的事项：\n\n* 您是否有普通RAG无法很好处理的示例提示，但您期望图形RAG能够成功处理？您能否在这些提示上进行“调试”，看看后台发生了什么？\n* 知识图是否包含语义搜索可能无法建立的有意义连接？您能否找到在图中连接的概念对的示例，其相关文档在向量空间中相距甚远？知识图应该在“远离”的文档之间建立有意义的连接。\n\n### 你（仍然）看到幻觉\n\n如果你在使用图形 RAG 时看到的幻觉与使用普通 RAG 时不同，我会怀疑某处存在错误或参数设置不当。如果你看到的幻觉水平相似，这听起来像是一个超出图形方面的一般问题。\n\n需要关注的事项：\n\n* 你的文档集是否包含对引发幻觉的提示的正确响应？向量搜索是否找到了这些文档？\n* 从检索到的文档中获取的正确响应是否正确插入到传递给 LLM 的提示上下文中？\n\n### 图表“过大”\n\n当您的知识图谱“过大”或过于密集时，可能会出现两种主要问题。首先，可能会出现扩展性问题，我将在下面讨论。其次，图遍历可能导致“过多”的文档，这些文档必须重新排序和过滤。如果重新排序和过滤策略与检索和图遍历元素不兼容，您可能会在图刚发现重要文档后立即将其过滤掉。\n\n需要关注的内容：\n\n* 图遍历后返回了多少文档，多少被重新排序或过滤掉？通过强图连接找到的文档是否能成功存活于过滤中？\n* 您是否构建了一个充满适合您用例的有意义连接的知识图谱？在图中，您能否找到许多对您的用例过于通用或无关的概念或连接？您的知识图谱中有多少是由低质量信息组成的？\n\n### 图形“太小”\n\n根据上述，如果图形“太大”，可能会充满低质量的连接。而如果图形“太小”，我希望那里存在的连接是有意义的，这很好，但缺失的连接主要有两种类型。第一种是由于图形构建过程中出现的错误引起的。第二种是由于图形构建未针对其进行设计。不同上下文或不同格式的数据可能会被不同的图形构建方法以不同的方式处理。\n\n需要关注的内容：\n\n* 你是否使用带有实体/关键词提取的LLM构建了知识图谱？你是否捕获了每个文档中所有有意义的实体，还是LLM限制了其输出？\n* 在你的文档中，有哪些概念和连接是你期望出现在知识图谱中的，但似乎缺失了？你期待它们何时以及如何被添加到图谱中？为什么它们实际上没有被添加到图谱中？\n\n### 你找不到“适中”图表\n\n你是否觉得可以构建一个“过大”或“过小”的图表，但无法构建一个中等大小的图表？\n\n需要注意的事项：\n\n* 你正在更改哪些参数或方法来从小到大或反向变化？这些是否应该对图表质量产生如此大的影响？你能否研究一些根据所使用的图表构建设置意外出现或消失的图表元素？\n* 另请查看上面“过大”和“过小”部分的相关提示。\n\n### 你的实现需要新的软件或增加部署复杂性\n\n这是一个经典的数据科学问题：构建非常酷炫和前沿的方法，却看到开发团队拒绝或难以将你笔记本中的代码引入生产环境。坚持使用最流行、支持最好的以及大部分开源的工具，可以更容易地进入生产，特别是如果你的组织在其他地方已经使用这些工具的话。\n\n需要关注的事项：\n\n* 你的实现是否需要为图形创建新的数据存储？你[可能不需要图形数据库](https://www.datastax.com/blog/knowledge-graphs-for-rag-without-a-graphdb)，而且可能能够使用你的生产向量存储来处理图形。\n* 你是否在使用一些最流行的开源工具来构建AI应用程序，比如LangChain？这些工具可以减少代码复杂性，使应用程序更具可移植性，并扩展潜在的集成和进一步开发。\n\n### 你的实现无法扩展\n\n文章 [Scaling Knowledge Graphs by Eliminating Edges](https://thenewstack.io/scaling-knowledge-graphs-by-eliminating-edges/) 在 *The New Stack* 中展示了一种使图形 RAG 非常可扩展的方法。像上面提到的，最流行、支持最好的、并且大多数是开源的工具通常是无痛扩展的最佳路径，但这并不总是容易。\n\n需要关注的内容：\n\n* 哪一部分无法扩展？图遍历、重新排序、结果汇编，还是其他？请参见上面的“The graph is too big”以获取更多提示。\n* 你是否有某个特定组件无法很好地扩展？有时使用内存中的图形库，如 'networkx' — 或者甚至是图形数据库 — 来执行复杂的图形操作可能会造成资源瓶颈。你可能想要 [切换到更可扩展的图形操作选项](https://bit.ly/3YD5NAd)。\n* 你是否在使用并行 API 调用来处理大部分繁重的工作，还是在尝试在主应用逻辑中进行复杂或高成本的计算？\n\n## 在生产中利用图形RAG获得成功\n\n创建成功的图形RAG系统的关键在于构建一个知识图谱和遍历逻辑，以补充语义向量检索，而不是取代或与之竞争。图形设计应旨在在合适的时间连接正确的节点、知识、实体和文档，从而能够组装合适的文档，以产生最有帮助和可操作的查询响应。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*orXW5uw-geBo-WVtZUxWXQ.jpeg)\n\n关于Glean，值得注意的是，内部文档数据集是图形RAG的一个完美用例。知识图谱可以连接人、项目、产品、客户、会议、地点等——所有这些在数量上都受到组织规模和其工作内容的限制。构建和管理一个由数千名员工组成的图形比例如尝试对维基百科上提到的所有人或大型财务或法律文档数据库中的所有人做同样的事情要可行得多。因此，Glean做出的第一个重大决策可能是找到一个很好的图形RAG用例来解决问题。\n\n图形RAG系统的一个常被低估的方面是输入数据的质量和将其传输到目的地的管道的可靠性。这与数据工程和传统软件开发的关系大于与AI的关系。在以往的技术范式中，由于数据类型和访问方法的不兼容，连接不同的数据系统是一个挑战。现在，AI和LLMs使得将不同来源的非结构化数据整合成为可能，从而允许将来自各种来源的数据整合到一个单一的RAG系统中。这种集成能力使得LLMs能够处理和理解来自各种来源的非结构化数据，例如内部网页、维基、代码库、数据库、Google文档和聊天记录。仅仅将所有这些信息连接在一起，并通过单一接口使其可访问，就可以带来巨大的收益。\n\n## 前进的方向\n\n构建图 RAG 系统以满足任何用例需要利用基础组件，如向量和图的数据存储、嵌入和 LLM，并通过开源编排工具如 LangChain 和 LlamaIndex 进行增强。这些工具促进了强大、可扩展和高效系统的开发，承诺未来公司通过自动化和精简优化知识工作，实现显著成功。\n\n知识图谱和图 RAG 系统的公共成功，尤其是像 Glean 这样的公司的成功，展示了这些技术在内部用例中的有效性，通过提高组织效率创造价值。然而，面向外部的企业和消费者产品的更广泛应用潜力仍然基本未被开发，为其他公司提供了许多探索机会。\n\n值得注意的是，我们已经处于所谓的“信息时代”至少 30 年，而在过去一两年中，我们才真正开始将所有这些信息跨来源、跨思想、跨文档和跨概念连接起来，以便我们的软件系统能够进行与我们人类在知识工作中日常使用的推理、逻辑和判断相同的类型的推理。一些人称之为“智能时代”。\n\n虽然最初关注简单、直接的决策，但人工智能的轨迹正朝着管理更复杂场景的方向发展，显著提高时间和成本的效率。这一令人兴奋的演变使许多人工智能应用，包括图 RAG，在转变知识如何在各种背景下相互连接和利用方面变得至关重要。\n\n要立即开始使用图 RAG，或了解更多信息，请查看 [DataStax 图 RAG 指南](https://bit.ly/4862Lrl)。\n\n*作者：Brian Godsey, Ph.D. ([LinkedIn](https://bit.ly/4enqFRa)) — 数学家、数据科学家和工程师 // [DataStax](https://bit.ly/3NpPujA) 的人工智能和机器学习产品 // 著作 [Think Like a Data Scientist](https://bit.ly/4f5uVES)*\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Il1GrFN6fYN7e_ovExRGPw.jpeg)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*wQvZDIlkOvrYZnbwl0bEPQ.jpeg)\n\n"},{"lang":"zh","group":"blog","slug":"blog/the-rise-of-the-ai-agent-product-manager-and-ai-agent-engineer-0905f1d30cce","frontmatter":{"title":"人工智能代理产品经理和人工智能代理工程师的崛起","meta_title":"人工智能代理产品经理和人工智能代理工程师的崛起","description":"想象一下未来，生成式人工智能不仅能响应查询，还能主动解决各个方面的复杂问题……","date":"2024-11-04T12:33:53.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*dlJ0a49_lRAPR1tTPs898w.png","categories":["Generative AI","Ethics","Technology"],"author":"Rifx.Online","tags":["Generative","Product","Manager","Engineer","Ethics"],"draft":false,"slug":"blog/the-rise-of-the-ai-agent-product-manager-and-ai-agent-engineer-0905f1d30cce"},"content":"\n\n\n\n\n想象一个未来，生成式AI不仅仅是响应查询，而是主动解决商业各个方面的复杂问题。这不是科幻小说，而是生成式AI代理迅速逼近的现实。这些代理有望彻底改变公司的运营，并激发一波新的创新，从简化供应链到优化产品开发，再到转变客户互动。\n\n在过去一年多的时间里，我一直在构建生成式AI应用和代理，亲眼见证了这些技术如何深刻重塑商业流程。AI的潜力巨大，从以空前高效处理客户查询的支持代理，到推动商业运营和决策的自主代理。这些代理不仅仅是在提升现有流程，而是在启用新的工作方式。\n\n例如，想象一个代理，它不仅仅是安排会议，而是理解你工作的背景，建议最有影响力的与会者，准备简报文件，甚至根据最近的公司动态提出议程项目。又或者考虑一个制造业中的代理，它不仅仅是监控生产线，而是预测维护需求，实时优化资源分配，并与设计团队合作，根据生产数据建议产品改进。\n\n这种AI驱动的转型正在创造对两个关键角色的需求：**AI代理产品经理**和**AI代理工程师**。这些专业人员不仅是我们AI增强未来的架构师和建设者，而且是一个协作团队的不可或缺的部分，工作在商业战略与前沿技术的交汇点上。\n\n## 引入新角色\n\n**AI Agent Product Manager** 是一位具有远见的人，能够识别代理创造价值的机会，设计其能力，并确保它们与业务目标和用户需求保持一致。他们在商业世界和AI可能性之间充当翻译，协调AI创新。\n\n**AI Agent Engineer** 则是将这些代理变为现实的技术高手。他们设计稳健的架构，创建复杂的提示，并通过与各种系统和数据源的无缝集成，确保代理基于公司数据和流程。\n\n由于我们仍处于这一技术周期的早期阶段，这些专业人员通常在专门的AI咨询公司或开发代理构建产品的公司（如Salesforce）工作。这使他们能够将最佳实践和行业创新带入每一个新项目。\n\n## AI代理产品经理：引领AI创新\n\n作为一名代理产品经理，您可能会在不同的用例上工作，比如一个月担任销售代理，下个月担任人力资源代理。让我们深入了解您的角色可能是什么样的：\n\n作为一名代理产品经理，假设您负责为一家跨国制造公司开发一个代理。您的第一步是与来自各个部门的高管领导一系列研讨会——运营、设计、销售和客户服务。您不仅仅在寻找渐进式的改进；您在寻找变革性的机会，而您通过促进组织内部的合作与理解来实现这一目标。\n\n通过这些讨论，您识别出一个颠覆性的可能性：一个可以连接客户反馈、产品设计和制造流程的代理。这个代理将分析客户评论和支持工单，识别流行的问题或期望的功能，并自动生成设计修改建议。然后，它将模拟这些变化对制造流程和成本的影响。\n\n作为代理的产品经理，您主要的责任之一将是绘制代理的旅程。这涉及定义从初始互动到最终结果的每一步，确保一切与业务目标一致。您需要识别代理将要进行的关键互动，了解这些互动的背景，并确定每个旅程应该实现的目标。您还需要考虑一些关键问题，例如：代理将如何优先考虑客户反馈？它如何有效地向工程团队提出设计建议？当AI影响产品决策时，必须解决哪些伦理考虑？\n\n您将与利益相关者密切合作，以定义成功指标。例如，您可能决定代理的目标是将识别产品问题到实施修复的时间缩短50%，同时提高客户满意度评分。\n\n随着项目的进展，您确保代理提供真实的商业价值。您可能会审查AI与设计团队之间的模拟和实际对话，调整代理的沟通风格，以更好地与工程师产生共鸣。或者，您可能会仔细研究代理的建议如何影响产品质量和客户满意度的数据，寻找进一步提高其性能的方法。\n\n在整个过程中，您不仅在考虑代理当前的能力，还在考虑其未来的潜力。这个代理如何演变以响应客户反馈并预测未来市场趋势？它是否有可能在未来参与产品团队的头脑风暴会议，提供数据驱动的见解以推动创新？\n\n您的代理产品经理角色使您处于商业转型的最前沿。您不仅仅是在实施一个新工具；您正在重塑整个组织在AI时代的思维、创新和运营方式。\n\n## AI代理工程师：打造智能可靠的系统\n\n现在，让我们转变思路，进入同一项目中的代理工程师角色：\n\n您的挑战是创建一个能够理解客户反馈、将其转化为可行设计见解并与制造系统接口的代理。这并非易事——这需要对大型语言模型、复杂的提示工程和强大的系统集成有深刻的理解。\n\n您首先选择一个合适的大型语言模型作为代理的基础。然而，您真正的工作在于设计一个全面的代理架构，使其能够在许多对话旅程中可靠地执行。\n\n作为代理工程师，您主要关注之一是创建和完善代理的提示结构。您设计复杂的提示，有效引导模型的行为，确保它在各种场景中始终提供相关且准确的响应。这可能涉及开发一个层次化的提示系统，能够处理从监督多个代理到导航各种旅程的所有事务。\n\n您将花费大量时间评估代理的行为和输出，优化提示和流程，并发布新版本。您甚至可能设计和实施一个严格的测试框架，模拟数千种潜在对话轨迹。您的目标是确保代理的响应是确定性的，并与任何给定输入的期望结果保持一致。\n\n例如，您可能会创建一套测试用例，涵盖各种类型的客户反馈，从简单的产品问题到复杂的功能请求。然后，您系统地处理这些案例，分析代理的响应，并迭代提示结构和决策逻辑，以提高性能。\n\n当您遇到代理行为不一致或不理想的边缘案例时，您并不是简单地调整提示。相反，您深入研究代理的决策过程，系统性地调整基础逻辑和提示结构，以解决这些问题。\n\n集成仍然是您角色的关键部分。您正在设计API，使代理能够从客户支持数据库中提取数据，访问产品设计文件，并将数据输入到制造规划系统中。但不仅仅是连接系统，您还专注于确保代理能够基于这些集成数据做出智能决策。\n\n伦理和安全仍然是关键问题。您实施了强有力的保障和监督机制，以确保AI不会建议可能危及产品安全的设计变更。您还构建了解释性功能，以便AI始终能够展示其任何建议的推理，这对于与使用代理的工程师和设计师建立信任至关重要。\n\n作为代理工程师，您的角色不仅涉及创建一个功能性AI系统，还涉及打造一个能够可靠和一致地推动整个产品开发和制造过程中的创新和效率的智能代理。这一复杂的挑战使您处于AI技术的最前沿，塑造着企业在AI时代的运营未来。\n\n## 伦理考量与协作的力量\n\n随着代理在企业中变得越来越重要，代理产品经理和代理工程师的角色将愈发重要。这些角色不仅仅涉及技术能力或战略洞察力——它们还要求对伦理考量有深刻的承诺。由于这些代理影响着重要的商业决策，背后的专业人士必须确保这些系统是透明的、公平的，并与更广泛的社会价值观相一致。\n\n这个代理的成功在很大程度上依赖于产品经理和工程师之间的无缝协作。你们将共同迭代代理的能力，排除问题，并推动可能性的边界。\n\n## 比较角色：代理产品经理与代理工程师\n\n以下是一个总结比较表，以强调代理产品经理与代理工程师之间的差异：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*T26nkBI-wID26X2NrE2SSQ.jpeg)\n\n关键要点：\n\n* **代理产品经理：** 该职位专注于代理的战略和业务方面，确保它们提供价值并与公司目标保持一致。\n* **代理工程师：** 该职位集中于技术实现，确保代理可靠运行并与现有系统无缝集成。\n\n## 未来属于你：迎接挑战\n\n随着人工智能的影响力不断扩大，Agent Product Manager 和 Agent Engineer 的角色将处于这场技术革命的最前沿。无论你是在为 AI 驱动的业务转型定义战略，还是在设计驱动智能代理的复杂系统，你都将在塑造商业的未来。\n\n这些角色需要独特的技能组合：战略思维、技术专长、创造力，以及对商业和人工智能的深刻理解。它们提供了在尖端技术上工作的机会，同时推动实际的商业影响。\n\n那么，未来的 Agent Product Managers 和 Engineers，你们准备好迎接挑战了吗？人工智能增强的未来正等待着你们的专业知识和愿景。无论你是对产品管理的战略方面感兴趣，还是对代理工程的技术细节充满热情，在这个令人兴奋的新领域中都有你的一席之地。问题不在于人工智能是否会改变商业，而在于如何改变——而你可能就是决定者。\n\n"},{"lang":"zh","group":"blog","slug":"blog/unlocking-mixture-of-experts-moe-llm-your-moe-model-can-be-embedding-model-for-free-f192b9c07a5f","frontmatter":{"title":"解锁混合专家 (MoE) LLM：你的 MoE 模型可以免费嵌入模型","meta_title":"解锁混合专家 (MoE) LLM：你的 MoE 模型可以免费嵌入模型","description":"混合专家 (MoE) LLM 可以免费用作嵌入模型。","date":"2024-11-04T12:30:57.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*mB6VhEyAvxAxGbLDG_6hTw.png","categories":["Machine Learning","Natural Language Processing","Data Science"],"author":"Rifx.Online","tags":["Mixture-of-Experts","MoE","embedding","MoEE","BERTopic"],"draft":false,"slug":"blog/unlocking-mixture-of-experts-moe-llm-your-moe-model-can-be-embedding-model-for-free-f192b9c07a5f"},"content":"\n### Mixture-of-experts (MoE) LLM 可以作为免费的嵌入模型使用。\n\n\n\n我最近发现了一篇有趣的论文，标题为“你的 Mixture-of-Experts LLM 秘密地是一个免费的嵌入模型。”\\[1\\] 最近的 LLM 架构趋势是解码器模型，这对于嵌入模型并不适用，因为它们的注意力方法。然而，作者揭示了 Mixture-of-Experts (MoE) LLM 可以作为嵌入模型来执行多种嵌入相关的任务，而无需进一步的微调。在这篇博客中，首先让我们回顾一下 MoE，我将介绍它的工作原理及其实际应用。\n\n## 目录\n\n1. 什么是专家混合模型（MoE）？\n2. MoE 如何作为嵌入模型工作？\n3. 实际实施：使用 BERTopic 利用 MoEE\n\n## 1\\. 什么是专家混合模型 (MoE)？\n\n专家混合模型 (MoE) 是一种具有多个子网络的架构，这些子网络被称为“专家”，每个专家专注于不同的任务或数据方面。MoE 的一个优势是，它能够以比相同或更大模型更少的计算量对 AI 模型进行预训练，同时保持或提高质量。因此，如果我们的预算有限，使用 MoE 可以比稠密的、相似大小的传统模型获得更好的模型。在最近的成功案例中，Mixtral 8 x 7B 在许多评估数据集上超越了 LLaMA 2 70B。\n\n接下来，让我们研究 MoE 的架构。最近成功的 MoE 使用了变压器模型，因此我将重点关注变压器的流行 MoE 架构。MoE 主要有两个组件，如下所述。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Dia_c08PJnFeeIc9lxwtGQ.png)\n\n* **MoE 层**\n\nMoE 在变压器架构中用 MoE 层替代了前馈网络 (FFN) 层。每个 MoE 层有一些专家（例如，上图中的 4 个专家），每个专家由简单的 FFN 层组成。请注意，变压器中的其他组件，例如自注意力层，使用相同的权重。因此，MoE 的权重数量并不简单。例如，Mixtral 8 x 7B 的权重不是 8 x 7 = 56B，而是 47B，因为除了 MoE 层之外的其他层共享相同的权重。\n\n* **门控网络**\n\n门控网络或路由器是 MoE 中的一个关键组件。它接收输入标记并为每个标记选择最相关的专家。例如，在上面的插图中，路由器的左侧选择第二个专家来处理单词“more”标记。同时，路由器确定第一个专家来处理单词“Parameters”标记。通常，门控网络选择与给定标记相关的前 k 个专家，并将标记发送给选定的专家；例如，Mixtral 8 x 7B 选择前 2 个专家。\n\n我们如何选择前 k 个专家？我们使用 softmax 函数来计算专家的重要性概率，并保留前 k 个概率专家，如下所示。我提取了上述插图的门控部分。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*qX9H2KKtjntVuiE8yFstMQ.png)\n\n门控网络有其权重。我们将 softmax 函数应用于输入单词标记与门控网络权重之间的点积结果，然后得到专家与给定标记相关的概率。根据概率，我们可以选择前 k 个相关专家。具有这种类型门控网络的 MoE 被称为稀疏 MoE。\n\n这些是理解 MoE 如何作为嵌入模型工作的基本知识。为了进一步理解，我推荐阅读 [这篇博客](https://huggingface.co/blog/moe) \\[2]。现在，让我们深入探讨 MoE 实际上是如何作为嵌入模型工作的。\n\n## 2\\. MoE 如何作为嵌入模型工作？\n\n### 关于嵌入的快速回顾\n\n在深入本节主题之前，让我们快速回顾一下嵌入。最近，嵌入成为深度学习模型中输入数据的内部表示，它具有语义和浓缩的数据信息。我们通常提取神经网络的最后一个隐藏状态作为嵌入，如下所示。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kSHFTEejKiSI51taKZCO9A.png)\n\n我们通常使用基于编码器的模型来提取嵌入，因为与仅解码器模型相比，它们能够通过双向注意力捕捉语义。仅解码器模型通常使用因果注意力，只与之前的词元进行交互；因此，它们无法捕捉丰富的语义，如上下文信息，这一点是编码器-解码器模型所能实现的。\n\n### MoE如何作为嵌入模型工作？\n\n人们普遍认为解码器模型无法用于嵌入提取。然而，作者发现MoE中的路由权重为解码器嵌入提供了互补信息。每一层中的路由权重反映了对输入标记的推理选择，因此它包含了输入的语义信息，而隐藏状态的嵌入可能会丢失。在数学公式中，我们可以这样描述它：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*n6wGCMqAhjBAfLFV47ML1g.png)\n\n*g*是softmax函数，*H*表示隐藏状态。我们将所有MoE层的路由权重进行连接，以避免丢失模型的推理选择。\n\n为了充分利用路由权重和解码器嵌入，作者提出了一种称为MoE嵌入（MoEE）的方法，以形成更全面的嵌入表示。MoEE有两种类型。一种方法是基于连接的组合，具体如下。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*uVmcV-lM83XL7HoYbYjt7w.png)\n\n这种方法很简单，我们只需将路由权重和解码器嵌入进行连接。作者将这种方法称为MoEE(concat)。它可以保留每个路由权重捕获的独特信息，同时允许下游任务利用组合表示。\n\n另一种方法是加权求和集成。它对从路由权重和隐藏状态（HS）嵌入计算的相似性分数进行加权求和，表示为MoEE(sum)。该方法用于比较两个句子的任务，例如语义文本相似性。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*kyJxWW9zdgRyNr2jmO4LlQ.png)\n\n𝛂是一个超参数，用于控制路由权重的贡献。在为每对计算相似性分数后，我们计算计算得出的相似性分数与真实相似性之间的等级相关性，例如斯皮尔曼等级相关性。\n\n在实际使用中，我认为MoEE(concat)易于使用。此外，作者利用PromptEOL技术\\[4]来增强MoEE。该技术提示以下模板，以限制LLMs在预测下一个标记的语义信息时的行为。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*S9BASj9JkQe-i4fqmbopWg.png)\n\n现在，这里是MTEB任务的性能表。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*7LxkEMR2DFlncypF6_T7Vw.png)\n\n带有PromptEOL的MoEE可以比监督和自监督方法表现更好。请注意，这个排行榜不是最新的，因此这个结果并不是SOTA。这种方法的价值在于我们可以在嵌入任务中获得不错的结果，并且可以在没有任何进一步训练的情况下使用。\n\n到目前为止，我们已经涵盖了MoEE的工作原理。在下一节中，我们将实现MoEE与BERTopic并对句子进行聚类。\n\n## 3\\. 实际实施：利用 MoEE 与 BERTopic\n\n在本节中，我们从预训练的 MoE LLM 中提取嵌入，并使用 20-news-group 数据集 \\[5] 与 [BERTopic](https://maartengr.github.io/BERTopic/index.html) 结合。供您参考，BERTopic 是一个便利的主题建模库，超越了传统的统计主题建模。它利用来自 Transformer 的嵌入进行主题聚类，因此我认为它适合用于检查能力。首先，我们来准备一个环境。\n\n### 环境设置\n\n我使用了一个带有 Python 3\\.10 的 conda 环境。我在 Ubuntu 20\\.04 上进行了实验，使用 cuda 12\\.4，16 GB VRAM。下载模型权重可能需要 32 GB RAM。\n\n```python\nconda create -n moee python=3.10 -y\nconda activate moee\n```\n\n接下来，我们需要通过 pip 安装以下库。\n\n```python\npip install transformers torch bitsandbytes bertopic accelerate\n```\n\nMoE 模型通常需要较高的 VRAM，因为我们需要提前将整个模型加载到 VRAM 中。因此，我们需要使用 bitsandbytes，这是一个量化包，以节省 VRAM 内存。\n\n我们需要克隆官方 GitHub 仓库。\n\n```python\ngit clone https://github.com/tianyi-lab/MoE-Embedding.git\n```\n\n所有准备工作都完成了。现在，让我们使用 MoEE 实现 BERTopic 的主题聚类。\n\n### 利用 MoEE 和 BERTopic\n\n现在，我们将使用 MoEE 作为 BERTopic 的嵌入模型并尝试主题聚类。原始代码库允许我们使用小型 MoE 模型，例如 Qwen\\-1\\.5\\-MoE\\-A2\\.7B 或 OLMoE\\-1B\\-7B。在这篇博客中，我将使用 OLMoE\\-1B\\-7B，它适合在 16 GB VRAM 上运行推理。首先，我们需要加载 OLMoE\\-1B\\-7B。\n\n```python\nkwargs = {\n        \"base_model\": 'allenai/OLMoE-1B-7B-0924',\n        \"normalized\": False,\n        \"torch_dtype\": torch.bfloat16,\n        \"mode\": \"embedding\",\n        \"pooling_method\": \"mean\",\n        \"attn_implementation\": \"sdpa\",\n        \"attn\": \"bbcc\",\n    }\n\nconfig = {\n    'embed_method': 'prompteol',\n    'emb_info': 'MoEE'\n    }\n\nembedding_model = MOEE(model_name_or_path='allenai/OLMoE-1B-7B-0924', **kwargs)\n```\n\n接下来，我们需要计算 20\\-news\\-group 数据集的嵌入，以传递给 BERTopic。（我稍后会附上完整代码。）\n\n```python\nfrom sklearn.datasets import fetch_20newsgroups\n\ndocs = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))['data']\n\ndataset = MyDataset(docs)\ndataloader = DataLoader(dataset=dataset, batch_size=8)\nembeddings = None\n\nfor batch in tqdm(dataloader):\n    with torch.no_grad():    \n        embedding = embedding_model.encode(batch, **config)\n      \n        if embeddings is None:\n            embeddings = embedding[0]\n        else:\n            embeddings = np.vstack((embeddings, embedding[0]))\n  \n    torch.cuda.empty_cache()\n```\n\n为了提前计算嵌入，我们使用 torch.utils.data.DataLoader 作为迭代器，并对每个批次的文档进行编码。请注意，我们必须将嵌入作为 np.asarray 类型传递给 BERTopic。\n\n当您想使用自己的 MoE 模型时，必须实现从每个 MoE 层获取路由权重。对于隐藏状态嵌入，我们可以利用 HuggingFace transformer 函数。我们只需在推理时传递 output\\_hidden\\_states\\=True 参数。\n\n现在，我们可以运行主题建模。\n\n```python\n## Step 2 - Reduce dimensionality\numap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')\n\n## Step 3 - Cluster reduced embeddings\nhdbscan_model = HDBSCAN(min_cluster_size=15, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n\n## Step 4 - Tokenize topics\nvectorizer_model = CountVectorizer(stop_words=\"english\")\n\n## Step 5 - Create topic representation\nctfidf_model = ClassTfidfTransformer()\n\n## Step 6 - (Optional) Fine-tune topic representations with \n## a `bertopic.representation` model\nrepresentation_model = KeyBERTInspired()\n\n## All steps together\ntopic_model = BERTopic(\n  embedding_model=embedding_model,          # Step 1 - Extract embeddings\n  umap_model=umap_model,                    # Step 2 - Reduce dimensionality\n  hdbscan_model=hdbscan_model,              # Step 3 - Cluster reduced embeddings\n  vectorizer_model=vectorizer_model,        # Step 4 - Tokenize topics\n  ctfidf_model=ctfidf_model,                # Step 5 - Extract topic words\n  representation_model=representation_model # Step 6 - (Optional) Fine-tune topic representations\n)\n\n## topic modeling using BERTopic model\ntopics, probs = topic_model.fit_transform(docs, embeddings)\n```\n\n我们通过默认设置得到了 42 个主题；一些示例如下所示。尽管我随机选择了主题，但它能够很好地捕捉语义。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*VIaKHU-PSuTPzOUKDFbwOw.png)\n\n此外，这里是主题聚类可视化。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*KYAUOe2qEAv-ihq2S2dM0A.png)\n\n请查看主题聚类可视化中的红色圆圈。这个红色圆圈指的是主题 0，与计算机相关。更接近的主题也与机械词汇相关，例如图形、数字和打印机。\n\n这种方法向我们展示了我们可以在没有任何训练的情况下获得良好的嵌入。尽管在质量上仍有提升空间，以达到 SOTA\\-监督模型的水平，但本文的发现是进一步改善嵌入提取方法而不进行训练的良好步骤。\n\n这是我的完整代码。您需要将此文件放入 MoE\\-Embedding 目录的顶部。\n\n## 参考文献\n\n\\[1] Ziyue Li, Tianyi Zhou, [YOUR MIXTURE\\-OF\\-EXPERTS LLM IS SECRETLY AN EMBEDDING MODEL FOR FREE](https://arxiv.org/pdf/2410.10814) (2024\\), *Arxiv*\n\n\\[2] Omar S., et.al., [Mixture of Experts Explained](https://huggingface.co/blog/moe) (2023\\), Hugging Face\n\n\\[3] William Fedus, Barret Zoph., et.al., [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/pdf/2101.03961) (2021\\), *Arxiv*\n\n\\[4] Ting Jiang, et.al., [Scaling Sentence Embeddings with Large Language Models](https://arxiv.org/pdf/2307.16645) (2023\\), *Arxiv*\n\n\\[5] [20 News groups](http://qwone.com/~jason/20Newsgroups/)\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/visualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-with-ragas-fc2486308557","frontmatter":{"title":"可视化你的 RAG 数据——使用 Ragas 评估你的检索增强生成系统","meta_title":"可视化你的 RAG 数据——使用 Ragas 评估你的检索增强生成系统","description":"如何使用 UMAP 降维对嵌入进行处理以显示多个评估问题及其与源文档的关系……","date":"2024-11-04T12:35:56.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*peWTe1A-MqeROT_Jdof_Cw.gif","categories":["Natural Language Processing","Generative AI","Data Science"],"author":"Rifx.Online","tags":["RAG","UMAP","embeddings","evaluation","visualization"],"draft":false,"slug":"blog/visualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-with-ragas-fc2486308557"},"content":"\n\n\n### 如何使用 UMAP 降维将嵌入可视化以展示多个评估问题及其与源文档的关系，结合 Ragas、OpenAI、Langchain 和 ChromaDB\n\n检索增强生成（RAG）在 LLM 的工作流程中增加了一个检索步骤，使其能够在回答问题和查询时，从私人文档等额外来源查询相关数据 \\[1]。该工作流程不需要对额外文档进行昂贵的训练或微调。文档被拆分成片段，然后进行索引，通常使用紧凑的 ML 生成的向量表示（嵌入）。内容相似的片段在这个嵌入空间中会彼此靠近。\n\nRAG 应用将用户提供的问题投影到嵌入空间，以根据与问题的距离检索相关文档片段。LLM 可以使用检索到的信息来回答查询，并通过呈现片段作为参考来证明其结论。\n\n\n\n评估 RAG 应用是具有挑战性的 \\[2]。存在不同的方法：一方面，有些方法要求开发者提供答案作为真实值；另一方面，答案（和问题）也可以由另一个 LLM 生成。最大的开源 LLM 支持回答系统之一是 Ragas \\[4](检索增强生成评估)，它提供\n\n* 基于文档生成测试数据的方法，以及\n* 基于不同指标逐步和端到端评估检索和生成步骤的评估。\n\n在本文中，您将学习\n\n* 如何简要构建一个 Formula One 的 RAG 系统（有关详细描述，请参阅之前的文章 [可视化您的 RAG 数据 — 检索增强生成的 EDA](https://readmedium.com/visualize-your-rag-data-eda-for-retrieval-augmented-generation-0701ee98768f)）\n* 生成问题和答案\n* 使用 [Ragas](https://github.com/explodinggradients/ragas) 评估 RAG 系统\n* 最重要的是，如何使用 [Renumics Spotlight](https://github.com/Renumics/spotlight) 可视化结果并解读结果。\n\n代码可在 Github 上获取。\n\n## 准备你的环境\n\n启动一个笔记本并安装所需的 python 包\n\n```python\n!pip install langchain langchain-openai chromadb renumics-spotlight\n%env OPENAI_API_KEY=<your-api-key>\n```\n本教程使用以下 python 包：\n\n* [**Langchain**](https://github.com/langchain-ai/langchain): 一个集成语言模型和 RAG 组件的框架，使设置过程更加顺畅。\n* [**Renumics\\-Spotlight**](https://github.com/Renumics/spotlight): 一个可视化工具，用于交互式探索非结构化的机器学习数据集。\n* [**Ragas**](https://github.com/explodinggradients/ragas): 一个帮助你评估 RAG 管道的框架\n\n*免责声明：本文作者也是 Spotlight 的开发者之一。*\n\n## 为数据集准备文档和嵌入\n\n您可以使用自己的 RAG 应用程序，跳到下一部分了解如何评估、提取和可视化。\n\n或者您可以使用来自[上一篇文章](https://readmedium.com/visualize-your-rag-data-eda-for-retrieval-augmented-generation-0701ee98768f)的 RAG 应用程序，配合[我们准备的所有维基百科 Formula One 文章的数据集](https://spotlightpublic.blob.core.windows.net/docs-data/rag_demo/docs.zip)。您还可以将自己的文档插入到“docs/”子文件夹中。\n\n> 此数据集基于来自[维基百科](https://www.wikipedia.org/)的文章，并根据知识共享署名-相同方式共享许可协议进行许可。原始文章及作者列表可以在相应的维基百科页面中找到。\n\n现在您可以使用 Langchain 的 `DirectoryLoader` 从 docs 子目录加载所有文件，并使用 `RecursiveCharacterTextSpliter` 将文档拆分为片段。通过 `OpenAIEmbeddings`，您可以创建嵌入并将其存储在 `ChromaDB` 中作为向量存储。对于 Chain 本身，您可以使用 LangChains 的 `ChatOpenAI` 和 `ChatPromptTemplate`。\n\n本文的[链接代码](https://github.com/Renumics/rag-demo/blob/main/notebooks/visualize_rag_tutorial_qs.ipynb)包含所有必要步骤，您可以在[上一篇文章](https://readmedium.com/visualize-your-rag-data-eda-for-retrieval-augmented-generation-0701ee98768f)中找到上述所有步骤的详细描述。\n\n一个重要的点是，您应该使用哈希函数为 `ChromaDB` 中的片段创建 ID。这允许在仅拥有文档及其内容和元数据的情况下找到数据库中的嵌入。这使得可以跳过已经存在于数据库中的文档。\n\n```python\nimport hashlib\nimport json\nfrom langchain_core.documents import Document\n\ndef stable_hash_meta(doc: Document) -> str:\n    \"\"\"\n    Stable hash document based on its metadata.\n    \"\"\"\n    return hashlib.sha1(json.dumps(doc.metadata, sort_keys=True).encode()).hexdigest()\n\n...\nsplits = text_splitter.split_documents(docs)\nsplits_ids = [\n    {\"doc\": split, \"id\": stable_hash_meta(split.metadata)} for split in splits\n]\n\nexisting_ids = docs_vectorstore.get()[\"ids\"]\nnew_splits_ids = [split for split in splits_ids if split[\"id\"] not in existing_ids]\n\ndocs_vectorstore.add_documents(\n    documents=[split[\"doc\"] for split in new_splits_ids],\n    ids=[split[\"id\"] for split in new_splits_ids],\n)\ndocs_vectorstore.persist()\n```\n\n## 评估问题\n\n对于像一级方程式这样的常见主题，可以直接使用 ChatGPT 生成一般性问题。本文使用了四种问题生成方法：\n\n* **GPT4**: 使用 ChatGPT 4 生成了 30 个问题，提示为“写 30 个关于一级方程式的问题”\n– 随机示例：“哪个一级方程式车队以其跃马标志而闻名？”\n* **GPT3\\.5:** 使用 ChatGPT 3\\.5 生成了另外 199 个问题，提示为“写 100 个关于一级方程式的问题”，并重复“谢谢，再写 100 个吧”\n– 示例：“哪位车手在 1950 年赢得了首届一级方程式世界锦标赛？”\n* **Ragas\\_GPT4**: 使用 Ragas 生成了 113 个问题。Ragas 再次利用文档及其自身的嵌入模型构建一个向量数据库，然后用 GPT4 生成问题。\n– 示例：“你能告诉我更多关于乔丹 198 一级方程式赛车在 1998 年世界锦标赛中的表现吗？”\n* **Rags\\_GPT3\\.5**: 使用 Ragas 生成了 226 个额外问题——这里我们使用 GPT3\\.5\n– 示例：“在 2014 年比利时大奖赛上发生了什么事件导致汉密尔顿退赛？”\n\n```python\nfrom ragas.testset import TestsetGenerator\n\ngenerator = TestsetGenerator.from_default(\n    openai_generator_llm=\"gpt-3.5-turbo-16k\", \n    openai_filter_llm=\"gpt-3.5-turbo-16k\"\n)\n\ntestset_ragas_gpt35 = generator.generate(docs, 100)\n```\n问题和答案没有经过审核或修改。所有问题都合并在一个单一的数据框中，包含 `id`、`question`、`ground_truth`、`question_by` 和 `answer` 列。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*R_74K0-_SJXyTxq6ovAcWg.png)\n\n接下来，问题将被提出给 RAG 系统。对于超过 500 个问题，这可能需要一些时间并产生费用。如果逐行询问问题，可以暂停并继续该过程，或者在崩溃后恢复，而不会丢失到目前为止的结果：\n\n```python\nfor i, row in df_questions_answers.iterrows():\n    if row[\"answer\"] is None or pd.isnull(row[\"answer\"]):\n        response = rag_chain.invoke(row[\"question\"])\n\n        df_questions_answers.loc[df_questions_answers.index[i], \"answer\"] = response[\n            \"answer\"\n        ]\n        df_questions_answers.loc[df_questions_answers.index[i], \"source_documents\"] = [\n            stable_hash_meta(source_document.metadata)\n            for source_document in response[\"source_documents\"]\n        ]\n\n```\n不仅存储了答案，还存储了检索到的文档片段的源 ID 及其文本内容作为上下文：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*umlKv7Qf9SSLzRslT2r0Qw.png)\n\n此外，还生成并存储了所有问题的嵌入，并将其存储在数据框中。这使得可以将它们与文档一起可视化。\n\n## 使用 Ragas 进行评估\n\n[Ragas](https://github.com/explodinggradients/ragas) 提供了评估您的 RAG 流水线中每个组件的指标，以及整体性能的端到端指标：\n\n1. **上下文精确度：** 使用 `question` 和检索到的 `contexts` 来测量信号与噪声的比率。\n2. **上下文相关性：** 测量检索到的上下文与问题的相关性，使用 `question` 和 `contexts` 计算。\n3. **上下文召回率：** 基于 `ground truth` 和 `contexts` 检查是否检索到所有与答案相关的信息。\n4. **忠实度：** 利用 `contexts` 和 `answer` 来衡量生成答案的事实准确性。\n5. **答案相关性：** 使用 `question` 和 `answer` 计算，评估生成的答案与问题的相关性（不考虑事实性）。\n6. **答案语义相似度：** 使用 `ground truth` 和 `answer` 进行评估，以判断生成答案与正确答案之间的语义相似性。\n7. **答案正确性：** 依赖于 `ground truth` 和 `answer` 来衡量生成答案的准确性和与正确答案的一致性。\n8. **方面评估：** 涉及分析 `answer` 以根据预定义或自定义方面（如正确性或有害性）评估提交结果。\n\n目前，我们专注于答案正确性的端到端指标。数据框中的列名和内容已复制并调整，以符合 Ragas API 的命名和格式要求：\n\n```python\n## prepare the dataframe for evaluation\ndf_qa_eval = df_questions_answers.copy()\n\n\n## adapt the ground truth to the ragas naming and format\ndf_qa_eval.rename(columns={\"ground_truth\": \"ground_truths\"}, inplace=True)\ndf_qa_eval[\"ground_truths\"] = [\n    [gt] if not isinstance(gt, list) else gt for gt in df_qa_eval[\"ground_truths\"]\n]\n```\n这可能需要一些时间，甚至比仅查询您的 RAG 系统花费更多的金钱。让我们逐行应用评估，以便在崩溃后能够恢复而不丢失到目前为止的结果：\n\n```python\n## evaluate the answer correctness if not already done\nfields = [\"question\", \"answer\", \"contexts\", \"ground_truths\"]\nfor i, row in df_qa_eval.iterrows():\n    if row[\"answer_correctness\"] is None or pd.isnull(row[\"answer_correctness\"]):\n        evaluation_result = evaluate(\n            Dataset.from_pandas(df_qa_eval.iloc[i : i + 1][fields]),\n            [answer_correctness],\n        )\n        df_qa_eval.loc[i, \"answer_correctness\"] = evaluation_result[\n            \"answer_correctness\"\n        ]\n\n```\n之后，您可以将结果存储在 `df_questions_answer` 数据框中：\n\n```python\ndf_questions_answers[\"answer_correctness\"] = df_qa_eval[\"answer_correctness\"]\n```\n\n## 准备可视化\n\n为了在可视化中包含文档片段，我们添加了从文档到使用该文档作为来源的问题的引用。此外，引用文档的问题数量也被存储：\n\n```python\n## Explode 'source_documents' so each document ID is in its own row alongside the question ID\ndf_questions_exploded = df_qa_eval.explode(\"source_documents\")\n\n## Group by exploded 'source_documents' (document IDs) and aggregate\nagg = (\n    df_questions_exploded.groupby(\"source_documents\")\n    .agg(\n        num_questions=(\"id\", \"count\"),  # Count of questions referencing the document\n        question_ids=(\n            \"id\",\n            lambda x: list(x),\n        ),  # List of question IDs referencing the document\n    )\n    .reset_index()\n    .rename(columns={\"source_documents\": \"id\"})\n)\n\n## Merge the aggregated information back into df_documents\ndf_documents_agg = pd.merge(df_docs, agg, on=\"id\", how=\"left\")\n\n## Use apply to replace NaN values with empty lists for 'question_ids'\ndf_documents_agg[\"question_ids\"] = df_documents_agg[\"question_ids\"].apply(\n    lambda x: x if isinstance(x, list) else []\n)\n## Replace NaN values in 'num_questions' with 0\ndf_documents_agg[\"num_questions\"] = df_documents_agg[\"num_questions\"].fillna(0)\n```\n现在将问题的数据框与文档的数据框连接起来\n\n```python\ndf = pd.concat([df_qa_eval, df_documents_agg], axis=0)\n```\n此外，让我们准备一些不同的 UMAP \\[3] 映射。您可以稍后在 Spotlight GUI 中做类似的事情，但提前做好可以节省时间。\n\n* umap\\_all: 对所有文档和问题嵌入应用 fit 和 transform 的 UMAP\n* umap\\_questions: 仅对问题嵌入应用 fit，并对两者应用 transform 的 UMAP\n* umap\\_docs: 仅对文档嵌入应用 fit，并对两者应用 transform 的 UMAP\n\n我们像这样准备每个 UMAP 转换：\n\n```python\numap = UMAP(n_neighbors=20, min_dist=0.15, metric=\"cosine\", random_state=42).fit\numap_all = umap.transform(df[\"embedding\"].values.tolist())\ndf[\"umap\"] = umap_all.tolist()\n\n```\n每个文档片段的另一个有趣指标是其嵌入与最近问题的嵌入之间的距离：\n\n```python\nquestion_embeddings = np.array(df[df[\"question\"].notna()][\"embedding\"].tolist())\ndf[\"nearest_question_dist\"] = [  # brute force, could be optimized using ChromaDB\n    np.min([np.linalg.norm(np.array(doc_emb) - question_embeddings)])\n    for doc_emb in df[\"embedding\"].values\n]\n```\n这个指标可以帮助找到未被问题引用的文档。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*YTRUXZmd0iX8kyPIdUUnlg.png)\n\n## 可视化结果\n\n如果您跳过了之前的步骤，您可以下载数据框并使用以下代码加载它：\n\n```python\nimport pandas as pd\ndf = pd.read_parquet(\"df_f1_rag_docs_and_questions.parquet\")\n```\n然后启动 [Renumics Spotlight](https://github.com/Renumics/spotlight) 以可视化它：\n\n```python\nfrom renumics import spotlight\n\nspotlight.show(df)\nspotlight.show(\n    df,\n    layout=\"/home/markus/Downloads/layout_rag_1.json\",\n    dtype={x: Embedding for x in df.keys() if \"umap\" in x},\n)\n```\n这将打开一个新的浏览器窗口：\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*IMbva0pP8RAVhoY4dVbjLg.png)\n\n在左上角，您可以看到一个**所有问题和所有文档**片段的表格。您可以使用“可见列”按钮来控制表格中显示哪些数据框列。直接创建一个选择仅问题的过滤器是很有用的，以便能够在可视化中打开和关闭问题：选择所有问题，然后使用“从选定行创建过滤器”按钮创建过滤器。\n\n在表格的右侧，`answer correctness` **作为一个指标**显示在所有问题中。下面有两个**直方图**；左侧显示了根据不同问题生成方法划分的`answer correctness`的分布。右侧显示了问题生成方法的分布。在这里，如果需要，建议使用过滤按钮为问题创建过滤器，以仅显示选定的行（问题）。\n\n右侧有**两个相似性图**。第一个使用`umap_questions`列，基于仅对问题应用的转换显示问题和文档。这对于独立于相关文档查看问题的分布很有帮助，因为这种方法允许分析师识别问题本身的模式或簇。\n\n第二个相似性图基于仅对文档应用的转换（`umap_docs`）显示问题和文档。它对于在其相关文档的上下文中查看问题很有用。一个同时对问题和文档进行转换的相似性图在问题数量较多时被证明不太有用，因为更多或更少的问题会聚集在一起并倾向于与文档分开。因此，这种表示在这里被省略。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*1wZrAj60hiw1T3RVnCuBtA.png)\n\n### 文档嵌入相似性图：观察\n\n在相似性图 `umap_docs` 中，您可以识别出文档嵌入空间中没有邻近问题的区域。当选择 `nearest_question_dist` 进行着色时，这一点更加明显。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*cMGNPnnBa9Bn7BJ05SzxBw.png)\n\n可以识别出一些簇，包括仅包含标题或逐页包含仅数字的表格数据的片段，这些在拆分过程中其意义丧失。此外，许多不包含相关信息的维基百科特定文本添加，例如指向其他语言的链接或编辑注释，形成了没有邻近问题的簇。\n\n使用维基百科 API 删除维基百科相关文本形式的噪声非常简单。这可能并不是特别必要，因为它主要占用一些空间——预计 RAG 结果不会因此特别恶化。然而，包含在大表格中的数据很难被 RAG 系统捕获，使用先进的预处理方法进行表格提取并将其连接到 RAG 系统可能是有益的。\n\n您可以在 `umap_docs` 相似性图中观察到的另一个点是来自不同来源的问题的分布。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*IH7z3J4yUmU0C_SruxnDkg.png)\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*K4bADgDmSAr5t4t4r9VImQ.png)\n\n由 ChatGPT（GPT-3.5、GPT-4）直接生成的问题位于中心的一个更为封闭的区域，而基于文档生成的 ragas 生成的问题覆盖了更大的区域。\n\n### 答案正确性直方图\n\n直方图可以作为了解数据全球统计的起点。总体而言，在所有问题中，`答案正确性`为0\\.45。对于没有使用ragas创建的问题，该值为0\\.36，而使用ragas的问题则为0\\.52。预计系统在生成使用ragas的问题时表现会更好，因为这些问题是基于可用数据生成的，而ChatGPT直接生成的问题可能来自于ChatGPT训练时使用的所有数据。\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*GsLBsg7uwTrw-AzvO4BHmw.png)\n\n对一些问题/答案和真实情况进行快速随机手动审核显示，在`答案正确性`为0\\.3–0\\.4的区间，大多数问题仍然根据真实情况得到了正确回答。在0\\.2–0\\.3的区间，存在许多错误答案。在0\\.1–0\\.2的区间，大多数答案都是错误的。值得注意的是，这个范围内几乎所有的问题都来自GPT\\-3\\.5。尽管在这个区间内生成的两个问题使用的是GPT\\-4，但它们仍然得到了正确的回答，尽管其`答案正确性`低于0\\.2。\n\n### 问题嵌入相似性图：观察\n\n问题嵌入相似性图可以通过检查可能导致类似问题的相似问题集群，帮助深入挖掘 `答案正确性`。\n\n* **集群“驱动程序/过程/汽车的术语”：** 平均 `答案正确性` 0\\.23：答案通常不够精确。例如，底盘调校与底盘弯曲或刹车调校与刹车偏差调整。是否适合用这些类型的问题来评估系统是值得怀疑的，因为判断答案似乎非常困难。\n* **集群“燃料策略的术语”：** 平均 `答案正确性` 0\\.44，类似于全球 `答案正确性`。\n* **集群“赛道名称”：** 平均 `答案正确性` 0\\.49，类似于全球 `答案正确性`。\n* **集群“谁保持了…的记录”：** 平均 `答案正确性` 0\\.44，类似于全球 `答案正确性`。\n* **集群“赢得…锦标赛”：** 平均 `答案正确性` 0\\.26 — 看起来很具挑战性。带有许多条件的问题，例如：“谁是唯一一位凭借英国赛车执照、为意大利车队驾驶美国引擎赢得一级方程式世界锦标赛的车手。” 扩展的RAG方法如多查询可能有助于改善这一点。\n* **集群“谁是唯一一位赢得…的车手，驾驶编号为\\<number\\>的汽车”：** 平均 `答案正确性` 0\\.23 — 看起来GPT-3\\.5在这里懒惰，重复了相同的问题，只是换了不同的数字，尽管大多数真实答案都是错误的！\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*Yc03cpSEFlJoZSBPIpMkiQ.png)\n\n## 结论\n\n总之，利用基于 UMAP 的可视化提供了一种有趣的方法，可以深入分析全球指标之外的内容。文档嵌入相似性地图提供了一个良好的概述，展示了相似文档的聚类及其与评估问题的关系。问题相似性地图揭示了模式，使得可以结合质量指标对问题进行区分和分析，从而生成洞察。请参阅可视化结果部分，将可视化应用于您的评估策略——您将发现什么洞察？\n\n*I am a professional with expertise in creating advanced software solutions for the interactive exploration of unstructured data. I write about unstructured data and use powerful visualization tools to analyze and make informed decisions.*\n\n## 参考文献\n\n\\[1] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, Haofen Wang: [Retrieval\\-Augmented Generation for Large Language Models: A Survey](https://arxiv.org/abs/2312.10997) (2024\\), arxiv\n\n\\[2] Yixuan Tang, Yi Yang: [MultiHop\\-RAG: Benchmarking Retrieval\\-Augmented Generation for Multi\\-Hop Queries](https://arxiv.org/abs/2401.15391) (2021\\), arXiv\n\n\\[3] Leland McInnes, John Healy, James Melville: [UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction](https://arxiv.org/abs/1802.03426) (2018\\), arXiv\n\n\\[4] Shahul Es, Jithin James, Luis Espinosa\\-Anke, Steven Schockaert: [RAGAS: Automated Evaluation of Retrieval Augmented Generation](https://arxiv.org/abs/2309.15217) (2023\\), arXiv\n\n"},{"lang":"fr","group":"blog","slug":"blog/post-1","frontmatter":{"title":"Comment créer une application avec des technologies modernes","meta_title":"","description":"Ceci est une méta-description","date":"2022-04-04T05:00:00.000Z","image":"/images/image-placeholder.png","categories":["french","Application","Data"],"author":"John Doe","tags":["nextjs","tailwind","react"],"draft":false,"slug":"blog/post-1"},"content":"\nPersonne ne veut même sortir un maquillage de l'urne des soins empoisonnés. C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\n## Design Créatif\n\nCar en guise de maquillage, l'urne du poison C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\n> Le client lui-même doit pouvoir poursuivre l'adipisicing. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n"},{"lang":"fr","group":"blog","slug":"blog/post-2","frontmatter":{"title":"Comment créer une application avec des technologies modernes","meta_title":"","description":"Ceci est une méta-description","date":"2022-04-04T05:00:00.000Z","image":"/images/image-placeholder.png","categories":["Technology","Data"],"author":"Sam Wilson","tags":["technology","tailwind"],"draft":false,"slug":"blog/post-2"},"content":"\nPersonne ne veut même sortir un maquillage de l'urne des soins empoisonnés. C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\n## Design Créatif\n\nCar en guise de maquillage, l'urne du poison C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\n> Le client lui-même doit pouvoir poursuivre l'adipisicing. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n"},{"lang":"fr","group":"blog","slug":"blog/post-3","frontmatter":{"title":"Comment créer une application avec des technologies modernes","meta_title":"","description":"Ceci est une méta-description","date":"2022-04-04T05:00:00.000Z","image":"/images/image-placeholder.png","categories":["Software"],"author":"John Doe","tags":["software","tailwind"],"draft":false,"slug":"blog/post-3"},"content":"\nPersonne ne veut même sortir un maquillage de l'urne des soins empoisonnés. C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\n## Design Créatif\n\nCar en guise de maquillage, l'urne du poison C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\n> Le client lui-même doit pouvoir poursuivre l'adipisicing. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n"},{"lang":"fr","group":"blog","slug":"blog/post-4","frontmatter":{"title":"Comment créer une application avec des technologies modernes","meta_title":"","description":"Ceci est une méta-description","date":"2022-04-04T05:00:00.000Z","image":"/images/image-placeholder.png","categories":["Architecture"],"author":"John Doe","tags":["silicon","technology"],"draft":false,"slug":"blog/post-4"},"content":"\nPersonne ne veut même sortir un maquillage de l'urne des soins empoisonnés. C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\n## Design Créatif\n\nCar en guise de maquillage, l'urne du poison C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\n> Le client lui-même doit pouvoir poursuivre l'adipisicing. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n"},{"lang":"en","group":"models","slug":"models/chatgpt-4o-latest","frontmatter":{"title":"OpenAI: ChatGPT-4o","meta_title":"OpenAI: ChatGPT-4o","description":"OpenAI: ChatGPT-4o","date":"2024-08-14T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text image 2 text"],"author":"openai","tags":["Chatbots","Generative AI","Machine Learning","Natural Language Processing"],"draft":false,"id":"chatgpt-4o-latest","context":128000,"input":0.000005,"output":0.000015,"img":0.007225,"request":0,"last_updated":"2024-08-14T00:00:00.000Z","slug":"models/chatgpt-4o-latest"},"content":"\nDynamic model continuously updated to the current version of [GPT-4o](/openai/gpt-4o) in ChatGPT. Intended for research and evaluation.\n\nNote: This model is currently experimental and not suitable for production use-cases, and may be heavily rate-limited.\n\n"},{"lang":"en","group":"models","slug":"models/claude-3-haiku","frontmatter":{"title":"Anthropic: Claude 3 Haiku","meta_title":"Anthropic: Claude 3 Haiku","description":"Anthropic: Claude 3 Haiku","date":"2024-03-13T00:00:00.000Z","image":"https://img.rifx.online/logo/anthropic.svg","categories":["text image 2 text"],"author":"anthropic","tags":["Programming","Machine Learning","Generative AI","Chatbots","Natural Language Processing"],"draft":false,"id":"claude-3-haiku","context":200000,"input":2.5e-7,"output":0.00000125,"img":0.0004,"request":0,"last_updated":"2024-10-24T11:54:59.000Z","slug":"models/claude-3-haiku"},"content":"\nClaude 3 Haiku is Anthropic's fastest and most compact model for\nnear-instant responsiveness. Quick and accurate targeted performance.\n\nSee the launch announcement and benchmark results [here](https://www.anthropic.com/news/claude-3-haiku)\n\n#multimodal\n\n"},{"lang":"en","group":"models","slug":"models/claude-3-opus","frontmatter":{"title":"Anthropic: Claude 3 Opus","meta_title":"Anthropic: Claude 3 Opus","description":"Anthropic: Claude 3 Opus","date":"2024-03-05T00:00:00.000Z","image":"https://img.rifx.online/logo/anthropic.svg","categories":["text image 2 text"],"author":"anthropic","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"claude-3-opus","context":200000,"input":0.000015,"output":0.000075,"img":0.024,"request":0,"last_updated":"2024-11-07T09:45:35.000Z","slug":"models/claude-3-opus"},"content":"\nClaude 3 Opus is Anthropic's most powerful model for highly complex tasks. It boasts top-level performance, intelligence, fluency, and understanding.\n\nSee the launch announcement and benchmark results [here](https://www.anthropic.com/news/claude-3-family)\n\n#multimodal\n\n"},{"lang":"en","group":"models","slug":"models/claude-3-sonnet","frontmatter":{"title":"Anthropic: Claude 3 Sonnet","meta_title":"Anthropic: Claude 3 Sonnet","description":"Anthropic: Claude 3 Sonnet","date":"2024-03-05T00:00:00.000Z","image":"https://img.rifx.online/logo/anthropic.svg","categories":["text image 2 text"],"author":"anthropic","tags":["Programming","Technology","Machine Learning","Data Science","Chatbots"],"draft":false,"id":"claude-3-sonnet","context":200000,"input":0.000003,"output":0.000015,"img":0.0048,"request":0,"last_updated":"2024-11-07T09:45:13.000Z","slug":"models/claude-3-sonnet"},"content":"\nClaude 3 Sonnet is an ideal balance of intelligence and speed for enterprise workloads. Maximum utility at a lower price, dependable, balanced for scaled deployments.\n\nSee the launch announcement and benchmark results [here](https://www.anthropic.com/news/claude-3-family)\n\n#multimodal\n\n"},{"lang":"en","group":"models","slug":"models/claude-35-haiku","frontmatter":{"title":"Anthropic: Claude 3.5 Haiku","meta_title":"Anthropic: Claude 3.5 Haiku","description":"Anthropic: Claude 3.5 Haiku","date":"2024-11-04T00:00:00.000Z","image":"https://img.rifx.online/logo/anthropic.svg","categories":["text 2 text"],"author":"anthropic","tags":["Programming","Chatbots","Data Science","Machine Learning","Natural Language Processing"],"draft":false,"id":"claude-3.5-haiku","context":200000,"input":0.000001,"output":0.000005,"img":0,"request":0,"last_updated":"2024-11-07T09:46:02.000Z","slug":"models/claude-35-haiku"},"content":"\nClaude 3.5 Haiku features offers enhanced capabilities in speed, coding accuracy, and tool use. Engineered to excel in real-time applications, it delivers quick response times that are essential for dynamic tasks such as chat interactions and immediate coding suggestions.\n\nThis makes it highly suitable for environments that demand both speed and precision, such as software development, customer service bots, and data management systems.\n\nThis model is currently pointing to [Claude 3.5 Haiku (2024-10-22)](/anthropic/claude-3-5-haiku-20241022).\n\n"},{"lang":"en","group":"models","slug":"models/claude-35-sonnet","frontmatter":{"title":"Anthropic: Claude 3.5 Sonnet","meta_title":"Anthropic: Claude 3.5 Sonnet","description":"Anthropic: Claude 3.5 Sonnet","date":"2024-06-20T00:00:00.000Z","image":"https://img.rifx.online/logo/anthropic.svg","categories":["text image 2 text"],"author":"anthropic","tags":["Programming","Data Science","Computer Vision","Chatbots","Autonomous Systems"],"draft":false,"id":"claude-3.5-sonnet","context":200000,"input":0.000003,"output":0.000015,"img":0.0048,"request":0,"last_updated":"2024-10-24T11:45:46.000Z","slug":"models/claude-35-sonnet"},"content":"\nClaude 3.5 Sonnet delivers better-than-Opus capabilities, faster-than-Sonnet speeds, at the same Sonnet prices. Sonnet is particularly good at:\n\n- Coding: Autonomously writes, edits, and runs code with reasoning and troubleshooting\n- Data science: Augments human data science expertise; navigates unstructured data while using multiple tools for insights\n- Visual processing: excelling at interpreting charts, graphs, and images, accurately transcribing text to derive insights beyond just the text alone\n- Agentic tasks: exceptional tool use, making it great at agentic tasks (i.e. complex, multi-step problem solving tasks that require engaging with other systems)\n\n#multimodal\n\n"},{"lang":"en","group":"models","slug":"models/command-r-plus","frontmatter":{"title":"Cohere: Command R+ (08-2024)","meta_title":"Cohere: Command R+ (08-2024)","description":"Cohere: Command R+ (08-2024)","date":"2024-08-30T00:00:00.000Z","image":"https://img.rifx.online/logo/cohere.svg","categories":["text 2 text"],"author":"cohere","tags":["Technology","Programming","Machine Learning","Generative AI","Ethics"],"draft":false,"id":"command-r-plus","context":128000,"input":0.000002375,"output":0.0000095,"img":0,"request":0,"last_updated":"2024-11-07T09:33:44.000Z","slug":"models/command-r-plus"},"content":"\ncommand-r-plus-08-2024 is an update of the [Command R+](/cohere/command-r-plus) with roughly 50% higher throughput and 25% lower latencies as compared to the previous Command R+ version, while keeping the hardware footprint the same.\n\nRead the launch post [here](https://docs.cohere.com/changelog/command-gets-refreshed).\n\nUse of this model is subject to Cohere's [Acceptable Use Policy](https://docs.cohere.com/docs/c4ai-acceptable-use-policy).\n\n"},{"lang":"en","group":"models","slug":"models/command-r","frontmatter":{"title":"Cohere: Command R (08-2024)","meta_title":"Cohere: Command R (08-2024)","description":"Cohere: Command R (08-2024)","date":"2024-08-30T00:00:00.000Z","image":"https://img.rifx.online/logo/cohere.svg","categories":["text 2 text"],"author":"cohere","tags":["Programming","Natural Language Processing","Generative AI","Machine Learning","Data Science"],"draft":false,"id":"command-r","context":128000,"input":1.425e-7,"output":5.7e-7,"img":0,"request":0,"last_updated":"2024-11-07T09:33:55.000Z","slug":"models/command-r"},"content":"\ncommand-r-08-2024 is an update of the [Command R](/cohere/command-r) with improved performance for multilingual retrieval-augmented generation (RAG) and tool use. More broadly, it is better at math, code and reasoning and is competitive with the previous version of the larger Command R+ model.\n\nRead the launch post [here](https://docs.cohere.com/changelog/command-gets-refreshed).\n\nUse of this model is subject to Cohere's [Acceptable Use Policy](https://docs.cohere.com/docs/c4ai-acceptable-use-policy).\n\n"},{"lang":"en","group":"models","slug":"models/deepseek-chat","frontmatter":{"title":"DeepSeek V2.5","meta_title":"DeepSeek V2.5","description":"DeepSeek V2.5","date":"2024-05-14T00:00:00.000Z","image":"https://img.rifx.online/logo/deepseek.svg","categories":["text 2 text"],"author":"deepseek","tags":["Programming","Natural Language Processing","Machine Learning","Data Science","Chatbots"],"draft":false,"id":"deepseek-chat","context":128000,"input":1.4e-7,"output":2.8e-7,"img":0,"request":0,"last_updated":"2024-11-01T04:19:11.000Z","slug":"models/deepseek-chat"},"content":"\nDeepSeek-V2.5 is an upgraded version that combines DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct. The new model integrates the general and coding abilities of the two previous versions.\n\nDeepSeek-V2 Chat is a conversational finetune of DeepSeek-V2, a Mixture-of-Experts (MoE) language model. It comprises 236B total parameters, of which 21B are activated for each token.\n\nCompared with DeepSeek 67B, DeepSeek-V2 achieves stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times.\n\nDeepSeek-V2 achieves remarkable performance on both standard benchmarks and open-ended generation evaluations.\n\n"},{"lang":"en","group":"models","slug":"models/dolphin-mixtral-8x22b","frontmatter":{"title":"Dolphin 2.9.2 Mixtral 8x22B 🐬","meta_title":"Dolphin 2.9.2 Mixtral 8x22B 🐬","description":"Dolphin 2.9.2 Mixtral 8x22B 🐬","date":"2024-06-08T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"cognitivecomputations","tags":["Natural Language Processing","Generative AI","Chatbots","Roleplay","Ethics"],"draft":false,"id":"dolphin-mixtral-8x22b","context":65536,"input":9e-7,"output":9e-7,"img":0,"request":0,"last_updated":"2024-11-04T12:49:50.000Z","slug":"models/dolphin-mixtral-8x22b"},"content":"\nDolphin 2.9 is designed for instruction following, conversational, and coding. This model is a finetune of [Mixtral 8x22B Instruct](/mistralai/mixtral-8x22b-instruct). It features a 64k context length and was fine-tuned with a 16k sequence length using ChatML templates.\n\nThis model is a successor to [Dolphin Mixtral 8x7B](/cognitivecomputations/dolphin-mixtral-8x7b).\n\nThe model is uncensored and is stripped of alignment and bias. It requires an external alignment layer for ethical use. Users are cautioned to use this highly compliant model responsibly, as detailed in a blog post about uncensored models at [erichartford.com/uncensored-models](https://erichartford.com/uncensored-models).\n\n#moe #uncensored\n\n"},{"lang":"en","group":"models","slug":"models/dolphin-mixtral-8x7b","frontmatter":{"title":"Dolphin 2.6 Mixtral 8x7B 🐬","meta_title":"Dolphin 2.6 Mixtral 8x7B 🐬","description":"Dolphin 2.6 Mixtral 8x7B 🐬","date":"2023-12-21T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"cognitivecomputations","tags":["Programming","Natural Language Processing","Generative AI","Ethics","Chatbots"],"draft":false,"id":"dolphin-mixtral-8x7b","context":32768,"input":5e-7,"output":5e-7,"img":0,"request":0,"last_updated":"2024-11-04T12:52:28.000Z","slug":"models/dolphin-mixtral-8x7b"},"content":"\nThis is a 16k context fine-tune of [Mixtral-8x7b](/mistralai/mixtral-8x7b). It excels in coding tasks due to extensive training with coding data and is known for its obedience, although it lacks DPO tuning.\n\nThe model is uncensored and is stripped of alignment and bias. It requires an external alignment layer for ethical use. Users are cautioned to use this highly compliant model responsibly, as detailed in a blog post about uncensored models at [erichartford.com/uncensored-models](https://erichartford.com/uncensored-models).\n\n#moe #uncensored\n\n"},{"lang":"en","group":"models","slug":"models/gemini-flash-15-8b","frontmatter":{"title":"Google: Gemini 1.5 Flash-8B","meta_title":"Google: Gemini 1.5 Flash-8B","description":"Google: Gemini 1.5 Flash-8B","date":"2024-10-03T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text image 2 text"],"author":"google","tags":["Programming","Natural Language Processing","Chatbots","Translation","Technology/Web"],"draft":false,"id":"gemini-flash-1.5-8b","context":1000000,"input":3.75e-8,"output":1.5e-7,"img":0,"request":0,"last_updated":"2024-10-03T00:00:00.000Z","slug":"models/gemini-flash-15-8b"},"content":"\nGemini 1.5 Flash-8B is optimized for speed and efficiency, offering enhanced performance in small prompt tasks like chat, transcription, and translation. With reduced latency, it is highly effective for real-time and large-scale operations. This model focuses on cost-effective solutions while maintaining high-quality results.\n\n[Click here to learn more about this model](https://developers.googleblog.com/en/gemini-15-flash-8b-is-now-generally-available-for-use/).\n\nUsage of Gemini is subject to Google's [Gemini Terms of Use](https://ai.google.dev/terms).\n\n"},{"lang":"en","group":"models","slug":"models/gemini-flash-15","frontmatter":{"title":"Google: Gemini Flash 1.5","meta_title":"Google: Gemini Flash 1.5","description":"Google: Gemini Flash 1.5","date":"2024-05-14T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text image 2 text"],"author":"google","tags":["Programming","Machine Learning","Natural Language Processing","Computer Vision","Chatbots"],"draft":false,"id":"gemini-flash-1.5","context":1000000,"input":7.5e-8,"output":3e-7,"img":0.00004,"request":0,"last_updated":"2024-10-31T23:10:34.000Z","slug":"models/gemini-flash-15"},"content":"\nGemini 1.5 Flash is a foundation model that performs well at a variety of multimodal tasks such as visual understanding, classification, summarization, and creating content from image, audio and video. It's adept at processing visual and text inputs such as photographs, documents, infographics, and screenshots.\n\nGemini 1.5 Flash is designed for high-volume, high-frequency tasks where cost and latency matter. On most common tasks, Flash achieves comparable quality to other Gemini Pro models at a significantly reduced cost. Flash is well-suited for applications like chat assistants and on-demand content generation where speed and scale matter.\n\nUsage of Gemini is subject to Google's [Gemini Terms of Use](https://ai.google.dev/terms).\n\n#multimodal\n\n"},{"lang":"en","group":"models","slug":"models/gemini-pro-15","frontmatter":{"title":"Google: Gemini Pro 1.5","meta_title":"Google: Gemini Pro 1.5","description":"Google: Gemini Pro 1.5","date":"2024-04-09T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text image 2 text"],"author":"google","tags":["Programming","Natural Language Processing","Machine Learning","Generative AI","Chatbots"],"draft":false,"id":"gemini-pro-1.5","context":2000000,"input":0.00000125,"output":0.000005,"img":0.00263,"request":0,"last_updated":"2024-04-09T00:00:00.000Z","slug":"models/gemini-pro-15"},"content":"\nGoogle's latest multimodal model, supporting image and video in text or chat prompts.\n\nOptimized for language tasks including:\n\n- Code generation\n- Text generation\n- Text editing\n- Problem solving\n- Recommendations\n- Information extraction\n- Data extraction or generation\n- AI agents\n\nUsage of Gemini is subject to Google's [Gemini Terms of Use](https://ai.google.dev/terms).\n\n#multimodal\n\n"},{"lang":"en","group":"models","slug":"models/gpt-35-turbo-instruct","frontmatter":{"title":"OpenAI: GPT-3.5 Turbo Instruct","meta_title":"OpenAI: GPT-3.5 Turbo Instruct","description":"OpenAI: GPT-3.5 Turbo Instruct","date":"2023-09-28T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"openai","tags":["Programming","Natural Language Processing","Generative AI","Chatbots","Technology/Web"],"draft":false,"id":"gpt-3.5-turbo-instruct","context":4095,"input":0.0000015,"output":0.000002,"img":0,"request":0,"last_updated":"2023-09-28T00:00:00.000Z","slug":"models/gpt-35-turbo-instruct"},"content":"\nThis model is a variant of GPT-3.5 Turbo tuned for instructional prompts and omitting chat-related optimizations. Training data: up to Sep 2021.\n\n"},{"lang":"en","group":"models","slug":"models/gpt-4o-mini","frontmatter":{"title":"OpenAI: GPT-4o-mini","meta_title":"OpenAI: GPT-4o-mini","description":"OpenAI: GPT-4o-mini","date":"2024-07-18T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text image 2 text"],"author":"openai","tags":["Programming","Technology","Programming/Scripting","Technology/Web"],"draft":false,"id":"gpt-4o-mini","context":128000,"input":1.5e-7,"output":6e-7,"img":0.007225,"request":0,"last_updated":"2024-07-18T00:00:00.000Z","slug":"models/gpt-4o-mini"},"content":"\nGPT-4o mini is OpenAI's newest model after [GPT-4 Omni](/openai/gpt-4o), supporting both text and image inputs with text outputs.\n\nAs their most advanced small model, it is many multiples more affordable than other recent frontier models, and more than 60% cheaper than [GPT-3.5 Turbo](/openai/gpt-3.5-turbo). It maintains SOTA intelligence, while being significantly more cost-effective.\n\nGPT-4o mini achieves an 82% score on MMLU and presently ranks higher than GPT-4 on chat preferences [common leaderboards](https://arena.lmsys.org/).\n\nCheck out the [launch announcement](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/) to learn more.\n\n"},{"lang":"en","group":"models","slug":"models/gpt-4o","frontmatter":{"title":"OpenAI: GPT-4o","meta_title":"OpenAI: GPT-4o","description":"OpenAI: GPT-4o","date":"2024-05-13T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text image 2 text"],"author":"openai","tags":["Programming","Natural Language Processing","Machine Learning","Generative AI","Computer Vision"],"draft":false,"id":"gpt-4o","context":128000,"input":0.0000025,"output":0.00001,"img":0.0036125,"request":0,"last_updated":"2024-05-13T00:00:00.000Z","slug":"models/gpt-4o"},"content":"\nGPT-4o (\"o\" for \"omni\") is OpenAI's latest AI model, supporting both text and image inputs with text outputs. It maintains the intelligence level of [GPT-4 Turbo](/openai/gpt-4-turbo) while being twice as fast and 50% more cost-effective. GPT-4o also offers improved performance in processing non-English languages and enhanced visual capabilities.\n\nFor benchmarking against other models, it was briefly called [\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209)\n\n"},{"lang":"en","group":"models","slug":"models/grok-beta","frontmatter":{"title":"xAI: Grok Beta","meta_title":"xAI: Grok Beta","description":"xAI: Grok Beta","date":"2024-10-20T00:00:00.000Z","image":"https://img.rifx.online/logo/xai.svg","categories":["text 2 text"],"author":"x-ai","tags":["Natural Language Processing","Machine Learning","Generative AI","Chatbots","Data Science"],"draft":false,"id":"grok-beta","context":131072,"input":0.000005,"output":0.000015,"img":0,"request":0,"last_updated":"2024-11-07T09:32:49.000Z","slug":"models/grok-beta"},"content":"\nGrok Beta is xAI's experimental language model with state-of-the-art reasoning capabilities, best for complex and multi-step use cases.\n\nIt is the successor of [Grok 2](https://x.ai/blog/grok-2) with enhanced context length.\n\n"},{"lang":"en","group":"models","slug":"models/lfm-40b","frontmatter":{"title":"Liquid: LFM 40B MoE","meta_title":"Liquid: LFM 40B MoE","description":"Liquid: LFM 40B MoE","date":"2024-09-30T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"liquid","tags":["Machine Learning","Natural Language Processing","Data Science","Generative AI","Computer Vision"],"draft":false,"id":"lfm-40b","context":32768,"input":0.000001,"output":0.000002,"img":0,"request":0,"last_updated":"2024-11-04T12:44:22.000Z","slug":"models/lfm-40b"},"content":"\nLiquid's 40.3B Mixture of Experts (MoE) model. Liquid Foundation Models (LFMs) are large neural networks built with computational units rooted in dynamic systems.\n\nLFMs are general-purpose AI models that can be used to model any kind of sequential data, including video, audio, text, time series, and signals.\n\nSee the [launch announcement](https://www.liquid.ai/liquid-foundation-models) for benchmarks and more info.\n\n"},{"lang":"en","group":"models","slug":"models/liquid-lfm-40b:free","frontmatter":{"title":"Liquid: LFM 40B MoE (free)","meta_title":"Liquid: LFM 40B MoE (free)","description":"Liquid: LFM 40B MoE (free)","date":"2024-09-30T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"liquid","tags":["Generative AI","Machine Learning","Natural Language Processing","Data Science","Technology/Web"],"draft":false,"id":"liquid/lfm-40b:free","context":8192,"input":0,"output":0,"img":0,"request":0,"last_updated":"2024-11-07T00:17:57.000Z","slug":"models/liquid-lfm-40b:free"},"content":"\nLiquid's 40.3B Mixture of Experts (MoE) model. Liquid Foundation Models (LFMs) are large neural networks built with computational units rooted in dynamic systems.\n\nLFMs are general-purpose AI models that can be used to model any kind of sequential data, including video, audio, text, time series, and signals.\n\nSee the [launch announcement](https://www.liquid.ai/liquid-foundation-models) for benchmarks and more info.\n\n_These are free, rate-limited endpoints for [LFM 40B MoE](/liquid/lfm-40b). Outputs may be cached. Read about rate limits [here](/docs/limits)._\n\n"},{"lang":"en","group":"models","slug":"models/llama-31-70b-instruct","frontmatter":{"title":"Meta: Llama 3.1 70B Instruct","meta_title":"Meta: Llama 3.1 70B Instruct","description":"Meta: Llama 3.1 70B Instruct","date":"2024-07-23T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text 2 text"],"author":"meta-llama","tags":["Programming","Machine Learning","Natural Language Processing","Chatbots","Ethics"],"draft":false,"id":"llama-3.1-70b-instruct","context":131072,"input":3e-7,"output":3e-7,"img":0,"request":0,"last_updated":"2024-10-28T13:38:49.000Z","slug":"models/llama-31-70b-instruct"},"content":"\nMeta's latest class of model (Llama 3.1) launched with a variety of sizes & flavors. This 70B instruct-tuned version is optimized for high quality dialogue usecases.\n\nIt has demonstrated strong performance compared to leading closed-source models in human evaluations.\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).\n\n"},{"lang":"en","group":"models","slug":"models/llama-31-8b-instruct","frontmatter":{"title":"Meta: Llama 3.1 8B Instruct","meta_title":"Meta: Llama 3.1 8B Instruct","description":"Meta: Llama 3.1 8B Instruct","date":"2024-07-23T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text 2 text"],"author":"meta-llama","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Ethics"],"draft":false,"id":"llama-3.1-8b-instruct","context":131072,"input":5.5e-8,"output":5.5e-8,"img":0,"request":0,"last_updated":"2024-10-31T23:27:09.000Z","slug":"models/llama-31-8b-instruct"},"content":"\nMeta's latest class of model (Llama 3.1) launched with a variety of sizes & flavors. This 8B instruct-tuned version is fast and efficient.\n\nIt has demonstrated strong performance compared to leading closed-source models in human evaluations.\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).\n\n"},{"lang":"en","group":"models","slug":"models/llama-31-nemotron-70b-instruct","frontmatter":{"title":"Nvidia: Llama 3.1 Nemotron 70B Instruct","meta_title":"Nvidia: Llama 3.1 Nemotron 70B Instruct","description":"Nvidia: Llama 3.1 Nemotron 70B Instruct","date":"2024-10-15T00:00:00.000Z","image":"https://img.rifx.online/logo/nvidia.svg","categories":["text 2 text"],"author":"nvidia","tags":["Programming","Natural Language Processing","Machine Learning","Generative AI","Ethics"],"draft":false,"id":"llama-3.1-nemotron-70b-instruct","context":131072,"input":3.5e-7,"output":4e-7,"img":0,"request":0,"last_updated":"2024-10-15T00:00:00.000Z","slug":"models/llama-31-nemotron-70b-instruct"},"content":"\nNVIDIA's Llama 3.1 Nemotron 70B is a language model designed for generating precise and useful responses. Leveraging [Llama 3.1 70B](/models/meta-llama/llama-3.1-70b-instruct) architecture and Reinforcement Learning from Human Feedback (RLHF), it excels in automatic alignment benchmarks. This model is tailored for applications requiring high accuracy in helpfulness and response generation, suitable for diverse user queries across multiple domains.\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).\n\n"},{"lang":"en","group":"models","slug":"models/llama-31-sonar-huge-128k-online","frontmatter":{"title":"Perplexity: Llama 3.1 Sonar 405B Online","meta_title":"Perplexity: Llama 3.1 Sonar 405B Online","description":"Perplexity: Llama 3.1 Sonar 405B Online","date":"2024-08-14T00:00:00.000Z","image":"https://img.rifx.online/logo/perplexity.svg","categories":["text 2 text"],"author":"perplexity","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"llama-3.1-sonar-huge-128k-online","context":127072,"input":0.000005,"output":0.000005,"img":0,"request":0.005,"last_updated":"2024-11-07T09:36:38.000Z","slug":"models/llama-31-sonar-huge-128k-online"},"content":"\nLlama 3.1 Sonar is Perplexity's latest model family. It surpasses their earlier Sonar models in cost-efficiency, speed, and performance. The model is built upon the Llama 3.1 405B and has internet access.\n\n"},{"lang":"en","group":"models","slug":"models/llama-31-sonar-large-128k-online","frontmatter":{"title":"Perplexity: Llama 3.1 Sonar 70B Online","meta_title":"Perplexity: Llama 3.1 Sonar 70B Online","description":"Perplexity: Llama 3.1 Sonar 70B Online","date":"2024-08-01T00:00:00.000Z","image":"https://img.rifx.online/logo/perplexity.svg","categories":["text 2 text"],"author":"perplexity","tags":["Programming","Machine Learning","Natural Language Processing","Chatbots","Generative AI"],"draft":false,"id":"llama-3.1-sonar-large-128k-online","context":127072,"input":0.000001,"output":0.000001,"img":0,"request":0.005,"last_updated":"2024-11-07T09:37:21.000Z","slug":"models/llama-31-sonar-large-128k-online"},"content":"\nLlama 3.1 Sonar is Perplexity's latest model family. It surpasses their earlier Sonar models in cost-efficiency, speed, and performance.\n\nThis is the online version of the [offline chat model](/perplexity/llama-3.1-sonar-large-128k-chat). It is focused on delivering helpful, up-to-date, and factual responses. #online\n\n"},{"lang":"en","group":"models","slug":"models/llama-31-sonar-small-128k-online","frontmatter":{"title":"Perplexity: Llama 3.1 Sonar 8B Online","meta_title":"Perplexity: Llama 3.1 Sonar 8B Online","description":"Perplexity: Llama 3.1 Sonar 8B Online","date":"2024-08-01T00:00:00.000Z","image":"https://img.rifx.online/logo/perplexity.svg","categories":["text 2 text"],"author":"perplexity","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"llama-3.1-sonar-small-128k-online","context":127072,"input":2e-7,"output":2e-7,"img":0,"request":0.005,"last_updated":"2024-11-07T09:38:09.000Z","slug":"models/llama-31-sonar-small-128k-online"},"content":"\nLlama 3.1 Sonar is Perplexity's latest model family. It surpasses their earlier Sonar models in cost-efficiency, speed, and performance.\n\nThis is the online version of the [offline chat model](/perplexity/llama-3.1-sonar-small-128k-chat). It is focused on delivering helpful, up-to-date, and factual responses. #online\n\n"},{"lang":"en","group":"models","slug":"models/llama-32-11b-vision-instruct","frontmatter":{"title":"Meta: Llama 3.2 11B Vision Instruct","meta_title":"Meta: Llama 3.2 11B Vision Instruct","description":"Meta: Llama 3.2 11B Vision Instruct","date":"2024-09-25T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text image 2 text"],"author":"meta-llama","tags":["Natural Language Processing","Computer Vision","Machine Learning","Generative AI","Data Science"],"draft":false,"id":"llama-3.2-11b-vision-instruct","context":131072,"input":5.5e-8,"output":5.5e-8,"img":0.000079475,"request":0,"last_updated":"2024-09-25T00:00:00.000Z","slug":"models/llama-32-11b-vision-instruct"},"content":"\nLlama 3.2 11B Vision is a multimodal model with 11 billion parameters, designed to handle tasks combining visual and textual data. It excels in tasks such as image captioning and visual question answering, bridging the gap between language generation and visual reasoning. Pre-trained on a massive dataset of image-text pairs, it performs well in complex, high-accuracy image analysis.\n\nIts ability to integrate visual understanding with language processing makes it an ideal solution for industries requiring comprehensive visual-linguistic AI applications, such as content creation, AI-driven customer service, and research.\n\nClick here for the [original model card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md).\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).\n\n"},{"lang":"en","group":"models","slug":"models/llama-32-1b-instruct","frontmatter":{"title":"Meta: Llama 3.2 1B Instruct","meta_title":"Meta: Llama 3.2 1B Instruct","description":"Meta: Llama 3.2 1B Instruct","date":"2024-09-25T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text 2 text"],"author":"meta-llama","tags":["Natural Language Processing","Programming","Technology","Machine Learning","Generative AI"],"draft":false,"id":"llama-3.2-1b-instruct","context":131072,"input":1e-8,"output":2e-8,"img":0,"request":0,"last_updated":"2024-09-25T00:00:00.000Z","slug":"models/llama-32-1b-instruct"},"content":"\nLlama 3.2 1B is a 1-billion-parameter language model focused on efficiently performing natural language tasks, such as summarization, dialogue, and multilingual text analysis. Its smaller size allows it to operate efficiently in low-resource environments while maintaining strong task performance.\n\nSupporting eight core languages and fine-tunable for more, Llama 1.3B is ideal for businesses or developers seeking lightweight yet powerful AI solutions that can operate in diverse multilingual settings without the high computational demand of larger models.\n\nClick here for the [original model card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md).\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).\n\n"},{"lang":"en","group":"models","slug":"models/llama-32-90b-vision-instruct","frontmatter":{"title":"Meta: Llama 3.2 90B Vision Instruct","meta_title":"Meta: Llama 3.2 90B Vision Instruct","description":"Meta: Llama 3.2 90B Vision Instruct","date":"2024-09-25T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text image 2 text"],"author":"meta-llama","tags":["Natural Language Processing","Computer Vision","Machine Learning","Data Science","Generative AI"],"draft":false,"id":"llama-3.2-90b-vision-instruct","context":131072,"input":3.5e-7,"output":4e-7,"img":0.00050575,"request":0,"last_updated":"2024-09-25T00:00:00.000Z","slug":"models/llama-32-90b-vision-instruct"},"content":"\nThe Llama 90B Vision model is a top-tier, 90-billion-parameter multimodal model designed for the most challenging visual reasoning and language tasks. It offers unparalleled accuracy in image captioning, visual question answering, and advanced image-text comprehension. Pre-trained on vast multimodal datasets and fine-tuned with human feedback, the Llama 90B Vision is engineered to handle the most demanding image-based AI tasks.\n\nThis model is perfect for industries requiring cutting-edge multimodal AI capabilities, particularly those dealing with complex, real-time visual and textual analysis.\n\nClick here for the [original model card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md).\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).\n\n"},{"lang":"en","group":"models","slug":"models/lzlv-70b-fp16-hf","frontmatter":{"title":"lzlv 70B","meta_title":"lzlv 70B","description":"lzlv 70B","date":"2023-11-12T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"lizpreciatior","tags":["Roleplay","Programming","Machine Learning","Generative AI","Chatbots"],"draft":false,"id":"lzlv-70b-fp16-hf","context":4096,"input":3.5e-7,"output":4e-7,"img":0,"request":0,"last_updated":"2024-11-04T12:50:34.000Z","slug":"models/lzlv-70b-fp16-hf"},"content":"\nA Mythomax/MLewd_13B-style merge of selected 70B models.\nA multi-model merge of several LLaMA2 70B finetunes for roleplaying and creative work. The goal was to create a model that combines creativity with intelligence for an enhanced experience.\n\n#merge #uncensored\n\n"},{"lang":"en","group":"models","slug":"models/magnum-v4-72b","frontmatter":{"title":"Magnum v4 72B","meta_title":"Magnum v4 72B","description":"Magnum v4 72B","date":"2024-10-22T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"anthracite-org","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"magnum-v4-72b","context":32768,"input":0.000001875,"output":0.00000225,"img":0,"request":0,"last_updated":"2024-11-04T12:39:55.000Z","slug":"models/magnum-v4-72b"},"content":"\nThis is a series of models designed to replicate the prose quality of the Claude 3 models, specifically Sonnet(https://openrouter.ai/anthropic/claude-3.5-sonnet) and Opus(https://openrouter.ai/anthropic/claude-3-opus).\n\nThe model is fine-tuned on top of [Qwen2.5 72B](https://openrouter.ai/qwen/qwen-2.5-72b-instruct).\n\n"},{"lang":"en","group":"models","slug":"models/ministral-3b","frontmatter":{"title":"Ministral 3B","meta_title":"Ministral 3B","description":"Ministral 3B","date":"2024-10-17T00:00:00.000Z","image":"https://img.rifx.online/logo/mistral.png","categories":["text 2 text"],"author":"mistralai","tags":["Programming","Machine Learning","Natural Language Processing","Data Science","Generative AI"],"draft":false,"id":"ministral-3b","context":128000,"input":4e-8,"output":4e-8,"img":0,"request":0,"last_updated":"2024-11-07T00:24:37.000Z","slug":"models/ministral-3b"},"content":"\nMinistral 3B is a 3B parameter model optimized for on-device and edge computing. It excels in knowledge, commonsense reasoning, and function-calling, outperforming larger models like Mistral 7B on most benchmarks. Supporting up to 128k context length, it’s ideal for orchestrating agentic workflows and specialist tasks with efficient inference.\n\n"},{"lang":"en","group":"models","slug":"models/ministral-8b","frontmatter":{"title":"Ministral 8B","meta_title":"Ministral 8B","description":"Ministral 8B","date":"2024-10-17T00:00:00.000Z","image":"https://img.rifx.online/logo/mistral.png","categories":["text 2 text"],"author":"mistralai","tags":["Technology","Machine Learning","Data Science","Generative AI","Ethics"],"draft":false,"id":"ministral-8b","context":128000,"input":1e-7,"output":1e-7,"img":0,"request":0,"last_updated":"2024-10-19T04:54:11.000Z","slug":"models/ministral-8b"},"content":"\nMinistral 8B is an 8B parameter model featuring a unique interleaved sliding-window attention pattern for faster, memory-efficient inference. Designed for edge use cases, it supports up to 128k context length and excels in knowledge and reasoning tasks. It outperforms peers in the sub-10B category, making it perfect for low-latency, privacy-first applications.\n\n"},{"lang":"en","group":"models","slug":"models/mistral-7b-instruct","frontmatter":{"title":"Mistral: Mistral 7B Instruct","meta_title":"Mistral: Mistral 7B Instruct","description":"Mistral: Mistral 7B Instruct","date":"2024-05-27T00:00:00.000Z","image":"https://img.rifx.online/logo/mistral.png","categories":["text 2 text"],"author":"mistralai","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"mistral-7b-instruct","context":32768,"input":5.5e-8,"output":5.5e-8,"img":0,"request":0,"last_updated":"2024-10-31T23:13:12.000Z","slug":"models/mistral-7b-instruct"},"content":"\nA high-performing, industry-standard 7.3B parameter model, with optimizations for speed and context length.\n\n*Mistral 7B Instruct has multiple version variants, and this is intended to be the latest version.*\n\n"},{"lang":"en","group":"models","slug":"models/mistral-nemo","frontmatter":{"title":"Mistral: Mistral Nemo","meta_title":"Mistral: Mistral Nemo","description":"Mistral: Mistral Nemo","date":"2024-07-19T00:00:00.000Z","image":"https://img.rifx.online/logo/mistral.png","categories":["text 2 text"],"author":"mistralai","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Data Science"],"draft":false,"id":"mistral-nemo","context":128000,"input":1.3e-7,"output":1.3e-7,"img":0,"request":0,"last_updated":"2024-10-31T23:10:58.000Z","slug":"models/mistral-nemo"},"content":"\nA 12B parameter model with a 128k token context length built by Mistral in collaboration with NVIDIA.\n\nThe model is multilingual, supporting English, French, German, Spanish, Italian, Portuguese, Chinese, Japanese, Korean, Arabic, and Hindi.\n\nIt supports function calling and is released under the Apache 2.0 license.\n\n"},{"lang":"en","group":"models","slug":"models/mistral-tiny","frontmatter":{"title":"Mistral Tiny","meta_title":"Mistral Tiny","description":"Mistral Tiny","date":"2024-01-10T00:00:00.000Z","image":"https://img.rifx.online/logo/mistral.png","categories":["text 2 text"],"author":"mistralai","tags":["Programming","Machine Learning","Data Science","Generative AI","Chatbots"],"draft":false,"id":"mistral-tiny","context":32000,"input":2.5e-7,"output":2.5e-7,"img":0,"request":0,"last_updated":"2024-10-31T23:12:22.000Z","slug":"models/mistral-tiny"},"content":"\nThis model is currently powered by Mistral-7B-v0.2, and incorporates a \"better\" fine-tuning than [Mistral 7B](/mistralai/mistral-7b-instruct-v0.1), inspired by community work. It's best used for large batch processing tasks where cost is a significant factor but reasoning capabilities are not crucial.\n\n"},{"lang":"en","group":"models","slug":"models/mythomax-l2-13b","frontmatter":{"title":"MythoMax 13B","meta_title":"MythoMax 13B","description":"MythoMax 13B","date":"2023-07-02T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"gryphe","tags":["Roleplay","Programming","Machine Learning","Natural Language Processing","Generative AI"],"draft":false,"id":"mythomax-l2-13b","context":4096,"input":1e-7,"output":1e-7,"img":0,"request":0,"last_updated":"2024-10-28T13:10:41.000Z","slug":"models/mythomax-l2-13b"},"content":"\nOne of the highest performing and most popular fine-tunes of Llama 2 13B, with rich descriptions and roleplay. #merge\n\n"},{"lang":"en","group":"models","slug":"models/o1-mini","frontmatter":{"title":"OpenAI: o1-mini","meta_title":"OpenAI: o1-mini","description":"OpenAI: o1-mini","date":"2024-09-12T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"openai","tags":["Programming","Science","Natural Language Processing","Machine Learning","Data Science"],"draft":false,"id":"o1-mini","context":128000,"input":0.000003,"output":0.000012,"img":0,"request":0,"last_updated":"2024-09-12T00:00:00.000Z","slug":"models/o1-mini"},"content":"\nThe latest and strongest model family from OpenAI, o1 is designed to spend more time thinking before responding.\n\nThe o1 models are optimized for math, science, programming, and other STEM-related tasks. They consistently exhibit PhD-level accuracy on benchmarks in physics, chemistry, and biology. Learn more in the [launch announcement](https://openai.com/o1).\n\nNote: This model is currently experimental and not suitable for production use-cases, and may be heavily rate-limited.\n\n"},{"lang":"en","group":"models","slug":"models/o1-preview","frontmatter":{"title":"OpenAI: o1-preview","meta_title":"OpenAI: o1-preview","description":"OpenAI: o1-preview","date":"2024-09-12T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"openai","tags":["Programming","Science","Natural Language Processing","Machine Learning","Data Science"],"draft":false,"id":"o1-preview","context":128000,"input":0.000015,"output":0.00006,"img":0,"request":0,"last_updated":"2024-09-12T00:00:00.000Z","slug":"models/o1-preview"},"content":"\nThe latest and strongest model family from OpenAI, o1 is designed to spend more time thinking before responding.\n\nThe o1 models are optimized for math, science, programming, and other STEM-related tasks. They consistently exhibit PhD-level accuracy on benchmarks in physics, chemistry, and biology. Learn more in the [launch announcement](https://openai.com/o1).\n\nNote: This model is currently experimental and not suitable for production use-cases, and may be heavily rate-limited.\n\n"},{"lang":"en","group":"models","slug":"models/openchat-7b","frontmatter":{"title":"OpenChat 3.5 7B","meta_title":"OpenChat 3.5 7B","description":"OpenChat 3.5 7B","date":"2023-11-28T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"openchat","tags":["Programming","Natural Language Processing","Machine Learning","Open Source","Generative AI"],"draft":false,"id":"openchat-7b","context":8192,"input":5.5e-8,"output":5.5e-8,"img":0,"request":0,"last_updated":"2024-11-07T09:39:42.000Z","slug":"models/openchat-7b"},"content":"\nOpenChat 7B is a library of open-source language models, fine-tuned with \"C-RLFT (Conditioned Reinforcement Learning Fine-Tuning)\" - a strategy inspired by offline reinforcement learning. It has been trained on mixed-quality data without preference labels.\n\n- For OpenChat fine-tuned on Mistral 7B, check out [OpenChat 7B](/openchat/openchat-7b).\n- For OpenChat fine-tuned on Llama 8B, check out [OpenChat 8B](/openchat/openchat-8b).\n\n#open-source\n\n"},{"lang":"en","group":"models","slug":"models/phi-35-mini-128k-instruct","frontmatter":{"title":"Phi-3.5 Mini 128K Instruct","meta_title":"Phi-3.5 Mini 128K Instruct","description":"Phi-3.5 Mini 128K Instruct","date":"2024-08-21T00:00:00.000Z","image":"https://img.rifx.online/logo/microsoft.svg","categories":["text 2 text"],"author":"microsoft","tags":["Programming","Machine Learning","Natural Language Processing","Data Science","Generative AI"],"draft":false,"id":"phi-3.5-mini-128k-instruct","context":128000,"input":1e-7,"output":1e-7,"img":0,"request":0,"last_updated":"2024-11-01T04:17:17.000Z","slug":"models/phi-35-mini-128k-instruct"},"content":"\nPhi-3.5 models are lightweight, state-of-the-art open models. These models were trained with Phi-3 datasets that include both synthetic data and the filtered, publicly available websites data, with a focus on high quality and reasoning-dense properties. Phi-3.5 Mini uses 3.8B parameters, and is a dense decoder-only transformer model using the same tokenizer as [Phi-3 Mini](/microsoft/phi-3-mini-128k-instruct).\n\nThe models underwent a rigorous enhancement process, incorporating both supervised fine-tuning, proximal policy optimization, and direct preference optimization to ensure precise instruction adherence and robust safety measures. When assessed against benchmarks that test common sense, language understanding, math, code, long context and logical reasoning, Phi-3.5 models showcased robust and state-of-the-art performance among models with less than 13 billion parameters.\n\n"},{"lang":"en","group":"models","slug":"models/qwen-2-7b-instruct","frontmatter":{"title":"Qwen 2 7B Instruct","meta_title":"Qwen 2 7B Instruct","description":"Qwen 2 7B Instruct","date":"2024-07-16T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"qwen","tags":["Natural Language Processing","Programming","Machine Learning","Data Science","Ethics"],"draft":false,"id":"qwen-2-7b-instruct","context":32768,"input":5.4e-8,"output":5.4e-8,"img":0,"request":0,"last_updated":"2024-07-16T00:00:00.000Z","slug":"models/qwen-2-7b-instruct"},"content":"\nQwen2 7B is a transformer-based model that excels in language understanding, multilingual capabilities, coding, mathematics, and reasoning.\n\nIt features SwiGLU activation, attention QKV bias, and group query attention. It is pretrained on extensive data with supervised finetuning and direct preference optimization.\n\nFor more details, see this [blog post](https://qwenlm.github.io/blog/qwen2/) and [GitHub repo](https://github.com/QwenLM/Qwen2).\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).\n\n"},{"lang":"en","group":"models","slug":"models/qwen-2-vl-72b-instruct","frontmatter":{"title":"Qwen2-VL 72B Instruct","meta_title":"Qwen2-VL 72B Instruct","description":"Qwen2-VL 72B Instruct","date":"2024-09-18T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text image 2 text"],"author":"qwen","tags":["Natural Language Processing","Computer Vision","Robotics","Machine Learning"],"draft":false,"id":"qwen-2-vl-72b-instruct","context":32768,"input":4e-7,"output":4e-7,"img":0.000578,"request":0,"last_updated":"2024-09-18T00:00:00.000Z","slug":"models/qwen-2-vl-72b-instruct"},"content":"\nQwen2 VL 72B is a multimodal LLM from the Qwen Team with the following key enhancements:\n\n- SoTA understanding of images of various resolution & ratio: Qwen2-VL achieves state-of-the-art performance on visual understanding benchmarks, including MathVista, DocVQA, RealWorldQA, MTVQA, etc.\n\n- Understanding videos of 20min+: Qwen2-VL can understand videos over 20 minutes for high-quality video-based question answering, dialog, content creation, etc.\n\n- Agent that can operate your mobiles, robots, etc.: with the abilities of complex reasoning and decision making, Qwen2-VL can be integrated with devices like mobile phones, robots, etc., for automatic operation based on visual environment and text instructions.\n\n- Multilingual Support: to serve global users, besides English and Chinese, Qwen2-VL now supports the understanding of texts in different languages inside images, including most European languages, Japanese, Korean, Arabic, Vietnamese, etc.\n\nFor more details, see this [blog post](https://qwenlm.github.io/blog/qwen2-vl/) and [GitHub repo](https://github.com/QwenLM/Qwen2-VL).\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).\n\n"},{"lang":"en","group":"models","slug":"models/qwen-25-72b-instruct","frontmatter":{"title":"Qwen2.5 72B Instruct","meta_title":"Qwen2.5 72B Instruct","description":"Qwen2.5 72B Instruct","date":"2024-09-19T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"qwen","tags":["Programming","Natural Language Processing","Chatbots","Machine Learning","Data Science"],"draft":false,"id":"qwen-2.5-72b-instruct","context":131072,"input":3.5e-7,"output":4e-7,"img":0,"request":0,"last_updated":"2024-09-19T00:00:00.000Z","slug":"models/qwen-25-72b-instruct"},"content":"\nQwen2.5 72B is the latest series of Qwen large language models. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\n\n- Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n\n- Long-context Support up to 128K tokens and can generate up to 8K tokens.\n\n- Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).\n\n"},{"lang":"en","group":"models","slug":"models/qwen-25-7b-instruct","frontmatter":{"title":"Qwen2.5 7B Instruct","meta_title":"Qwen2.5 7B Instruct","description":"Qwen2.5 7B Instruct","date":"2024-10-16T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"qwen","tags":["Programming","Natural Language Processing","Chatbots","Machine Learning","Data Science"],"draft":false,"id":"qwen-2.5-7b-instruct","context":131072,"input":2.7e-7,"output":2.7e-7,"img":0,"request":0,"last_updated":"2024-10-16T00:00:00.000Z","slug":"models/qwen-25-7b-instruct"},"content":"\nQwen2.5 7B is the latest series of Qwen large language models. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\n\n- Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n\n- Long-context Support up to 128K tokens and can generate up to 8K tokens.\n\n- Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).\n\n"},{"lang":"en","group":"models","slug":"models/remm-slerp-l2-13b","frontmatter":{"title":"ReMM SLERP 13B","meta_title":"ReMM SLERP 13B","description":"ReMM SLERP 13B","date":"2023-07-22T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"undi95","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"remm-slerp-l2-13b","context":4096,"input":0.000001125,"output":0.000001125,"img":0,"request":0,"last_updated":"2024-11-04T12:46:13.000Z","slug":"models/remm-slerp-l2-13b"},"content":"\nA recreation trial of the original MythoMax-L2-B13 but with updated models. #merge\n\n"},{"lang":"en","group":"models","slug":"models/remm-slerp-l2-13b:extended","frontmatter":{"title":"ReMM SLERP 13B (extended)","meta_title":"ReMM SLERP 13B (extended)","description":"ReMM SLERP 13B (extended)","date":"2023-07-22T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"undi95","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"remm-slerp-l2-13b:extended","context":6144,"input":0.000001125,"output":0.000001125,"img":0,"request":0,"last_updated":"2024-11-04T12:47:21.000Z","slug":"models/remm-slerp-l2-13b:extended"},"content":"\nA recreation trial of the original MythoMax-L2-B13 but with updated models. #merge\n\n_These are extended-context endpoints for [ReMM SLERP 13B](/undi95/remm-slerp-l2-13b). They may have higher prices._\n\n"},{"lang":"en","group":"models","slug":"models/toppy-m-7b","frontmatter":{"title":"Toppy M 7B","meta_title":"Toppy M 7B","description":"Toppy M 7B","date":"2023-11-10T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"undi95","tags":["Programming","Machine Learning","Generative AI","Chatbots","Data Science"],"draft":false,"id":"toppy-m-7b","context":4096,"input":7e-8,"output":7e-8,"img":0,"request":0,"last_updated":"2024-11-04T12:51:35.000Z","slug":"models/toppy-m-7b"},"content":"\nA wild 7B parameter model that merges several models using the new task_arithmetic merge method from mergekit.\nList of merged models:\n- NousResearch/Nous-Capybara-7B-V1.9\n- [HuggingFaceH4/zephyr-7b-beta](/huggingfaceh4/zephyr-7b-beta)\n- lemonilia/AshhLimaRP-Mistral-7B\n- Vulkane/120-Days-of-Sodom-LoRA-Mistral-7b\n- Undi95/Mistral-pippa-sharegpt-7b-qlora\n\n#merge #uncensored\n\n"},{"lang":"en","group":"models","slug":"models/wizardlm-2-7b","frontmatter":{"title":"WizardLM-2 7B","meta_title":"WizardLM-2 7B","description":"WizardLM-2 7B","date":"2024-04-16T00:00:00.000Z","image":"https://img.rifx.online/logo/microsoft.svg","categories":["text 2 text"],"author":"microsoft","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"wizardlm-2-7b","context":32000,"input":5.5e-8,"output":5.5e-8,"img":0,"request":0,"last_updated":"2024-10-31T23:23:36.000Z","slug":"models/wizardlm-2-7b"},"content":"\nWizardLM-2 7B is the smaller variant of Microsoft AI's latest Wizard model. It is the fastest and achieves comparable performance with existing 10x larger opensource leading models\n\nIt is a finetune of [Mistral 7B Instruct](/mistralai/mistral-7b-instruct), using the same technique as [WizardLM-2 8x22B](/microsoft/wizardlm-2-8x22b).\n\nTo read more about the model release, [click here](https://wizardlm.github.io/WizardLM2/).\n\n#moe\n\n"},{"lang":"en","group":"models","slug":"models/wizardlm-2-8x22b","frontmatter":{"title":"WizardLM-2 8x22B","meta_title":"WizardLM-2 8x22B","description":"WizardLM-2 8x22B","date":"2024-04-16T00:00:00.000Z","image":"https://img.rifx.online/logo/microsoft.svg","categories":["text 2 text"],"author":"microsoft","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"wizardlm-2-8x22b","context":65536,"input":5e-7,"output":5e-7,"img":0,"request":0,"last_updated":"2024-10-31T23:24:21.000Z","slug":"models/wizardlm-2-8x22b"},"content":"\nWizardLM-2 8x22B is Microsoft AI's most advanced Wizard model. It demonstrates highly competitive performance compared to leading proprietary models, and it consistently outperforms all existing state-of-the-art opensource models.\n\nIt is an instruct finetune of [Mixtral 8x22B](/mistralai/mixtral-8x22b).\n\nTo read more about the model release, [click here](https://wizardlm.github.io/WizardLM2/).\n\n#moe\n\n"},{"lang":"zh","group":"models","slug":"models/chatgpt-4o-latest","frontmatter":{"title":"OpenAI: ChatGPT-4o","meta_title":"OpenAI: ChatGPT-4o","description":"OpenAI: ChatGPT-4o","date":"2024-08-14T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text image 2 text"],"author":"openai","tags":["Chatbots","Generative AI","Machine Learning","Natural Language Processing"],"draft":false,"id":"chatgpt-4o-latest","context":128000,"input":0.000005,"output":0.000015,"img":0.007225,"request":0,"last_updated":"2024-08-14T00:00:00.000Z","slug":"models/chatgpt-4o-latest"},"content":"\n动态模型持续更新到 ChatGPT 中的当前版本 [GPT-4o](/openai/gpt-4o)。旨在用于研究和评估。\n\n注意：此模型目前处于实验阶段，不适合生产使用，并可能受到严格的速率限制。\n\n"},{"lang":"zh","group":"models","slug":"models/claude-3-haiku","frontmatter":{"title":"Anthropic: Claude 3 Haiku","meta_title":"Anthropic: Claude 3 Haiku","description":"Anthropic: Claude 3 Haiku","date":"2024-03-13T00:00:00.000Z","image":"https://img.rifx.online/logo/anthropic.svg","categories":["text image 2 text"],"author":"anthropic","tags":["Programming","Machine Learning","Generative AI","Chatbots","Natural Language Processing"],"draft":false,"id":"claude-3-haiku","context":200000,"input":2.5e-7,"output":0.00000125,"img":0.0004,"request":0,"last_updated":"2024-10-24T11:54:59.000Z","slug":"models/claude-3-haiku"},"content":"\nClaude 3 Haiku 是 Anthropic 反应速度最快、体积最小的模型，能够实现近乎即时的响应。快速且准确的定向性能。\n\n查看发布公告和基准测试结果 [这里](https://www.anthropic.com/news/claude-3-haiku)\n\n#multimodal\n\n"},{"lang":"zh","group":"models","slug":"models/claude-3-opus","frontmatter":{"title":"Anthropic: Claude 3 Opus","meta_title":"Anthropic: Claude 3 Opus","description":"Anthropic: Claude 3 Opus","date":"2024-03-05T00:00:00.000Z","image":"https://img.rifx.online/logo/anthropic.svg","categories":["text image 2 text"],"author":"anthropic","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"claude-3-opus","context":200000,"input":0.000015,"output":0.000075,"img":0.024,"request":0,"last_updated":"2024-11-07T09:45:35.000Z","slug":"models/claude-3-opus"},"content":"\nClaude 3 Opus 是 Anthropic 针对高度复杂任务的最强大模型。它拥有顶级的性能、智能、流畅性和理解能力。\n\n查看发布公告和基准测试结果 [here](https://www.anthropic.com/news/claude-3-family)\n\n#multimodal\n\n"},{"lang":"zh","group":"models","slug":"models/claude-3-sonnet","frontmatter":{"title":"Anthropic: Claude 3 Sonnet","meta_title":"Anthropic: Claude 3 Sonnet","description":"Anthropic: Claude 3 Sonnet","date":"2024-03-05T00:00:00.000Z","image":"https://img.rifx.online/logo/anthropic.svg","categories":["text image 2 text"],"author":"anthropic","tags":["Programming","Technology","Machine Learning","Data Science","Chatbots"],"draft":false,"id":"claude-3-sonnet","context":200000,"input":0.000003,"output":0.000015,"img":0.0048,"request":0,"last_updated":"2024-11-07T09:45:13.000Z","slug":"models/claude-3-sonnet"},"content":"\nClaude 3 Sonnet 是企业工作负载的智能与速度的理想平衡。以更低的价格提供最大效用，可靠且适合大规模部署。\n\n查看发布公告和基准测试结果 [这里](https://www.anthropic.com/news/claude-3-family)\n\n#multimodal\n\n"},{"lang":"zh","group":"models","slug":"models/claude-35-haiku","frontmatter":{"title":"Anthropic: Claude 3.5 Haiku","meta_title":"Anthropic: Claude 3.5 Haiku","description":"Anthropic: Claude 3.5 Haiku","date":"2024-11-04T00:00:00.000Z","image":"https://img.rifx.online/logo/anthropic.svg","categories":["text 2 text"],"author":"anthropic","tags":["Programming","Chatbots","Data Science","Machine Learning","Natural Language Processing"],"draft":false,"id":"claude-3.5-haiku","context":200000,"input":0.000001,"output":0.000005,"img":0,"request":0,"last_updated":"2024-11-07T09:46:02.000Z","slug":"models/claude-35-haiku"},"content":"\nClaude 3.5 Haiku 特性提供了更高的速度、编码准确性和工具使用能力。旨在在实时应用中表现出色，它提供了快速的响应时间，这对于动态任务（如聊天互动和即时编码建议）至关重要。\n\n这使其非常适合需要速度和精度的环境，例如软件开发、客户服务机器人和数据管理系统。\n\n此模型当前指向 [Claude 3.5 Haiku (2024-10-22)](/anthropic/claude-3-5-haiku-20241022).\n\n"},{"lang":"zh","group":"models","slug":"models/claude-35-sonnet","frontmatter":{"title":"Anthropic: Claude 3.5 Sonnet","meta_title":"Anthropic: Claude 3.5 Sonnet","description":"Anthropic: Claude 3.5 Sonnet","date":"2024-06-20T00:00:00.000Z","image":"https://img.rifx.online/logo/anthropic.svg","categories":["text image 2 text"],"author":"anthropic","tags":["Programming","Data Science","Computer Vision","Chatbots","Autonomous Systems"],"draft":false,"id":"claude-3.5-sonnet","context":200000,"input":0.000003,"output":0.000015,"img":0.0048,"request":0,"last_updated":"2024-10-24T11:45:46.000Z","slug":"models/claude-35-sonnet"},"content":"\nClaude 3.5 Sonnet 提供优于 Opus 的能力，以快于 Sonnet 的速度，且价格与 Sonnet 相同。Sonnet 在以下方面表现尤为出色：\n\n- 编程：自主编写、编辑和运行代码，具备推理和故障排除能力\n- 数据科学：增强人类数据科学专业知识；在使用多种工具获取洞察的同时，处理非结构化数据\n- 视觉处理：擅长解读图表、图形和图像，准确转录文本以获得超越文本本身的洞察\n- 代理任务：出色的工具使用能力，使其在代理任务（即需要与其他系统互动的复杂多步骤问题解决任务）中表现出色\n\n#multimodal\n\n"},{"lang":"zh","group":"models","slug":"models/command-r-plus","frontmatter":{"title":"Cohere: Command R+ (08-2024)","meta_title":"Cohere: Command R+ (08-2024)","description":"Cohere: Command R+ (08-2024)","date":"2024-08-30T00:00:00.000Z","image":"https://img.rifx.online/logo/cohere.svg","categories":["text 2 text"],"author":"cohere","tags":["Technology","Programming","Machine Learning","Generative AI","Ethics"],"draft":false,"id":"command-r-plus","context":128000,"input":0.000002375,"output":0.0000095,"img":0,"request":0,"last_updated":"2024-11-07T09:33:44.000Z","slug":"models/command-r-plus"},"content":"\ncommand-r-plus-08-2024 是 [Command R+](/cohere/command-r-plus) 的更新，与之前的 Command R+ 版本相比，吞吐量提高了大约 50%，延迟降低了 25%，同时保持相同的硬件占用。\n\n在 [这里](https://docs.cohere.com/changelog/command-gets-refreshed) 阅读发布帖子。\n\n使用此模型须遵循 Cohere 的 [可接受使用政策](https://docs.cohere.com/docs/c4ai-acceptable-use-policy)。\n\n"},{"lang":"zh","group":"models","slug":"models/command-r","frontmatter":{"title":"Cohere: Command R (08-2024)","meta_title":"Cohere: Command R (08-2024)","description":"Cohere: Command R (08-2024)","date":"2024-08-30T00:00:00.000Z","image":"https://img.rifx.online/logo/cohere.svg","categories":["text 2 text"],"author":"cohere","tags":["Programming","Natural Language Processing","Generative AI","Machine Learning","Data Science"],"draft":false,"id":"command-r","context":128000,"input":1.425e-7,"output":5.7e-7,"img":0,"request":0,"last_updated":"2024-11-07T09:33:55.000Z","slug":"models/command-r"},"content":"\ncommand-r-08-2024 是 [Command R](/cohere/command-r) 的更新，提升了多语言检索增强生成 (RAG) 和工具使用的性能。更广泛地说，它在数学、代码和推理方面表现更佳，并且与之前的更大版本 Command R+ 模型具有竞争力。\n\n阅读发布帖子 [这里](https://docs.cohere.com/changelog/command-gets-refreshed)。\n\n使用此模型须遵循 Cohere 的 [可接受使用政策](https://docs.cohere.com/docs/c4ai-acceptable-use-policy)。\n\n"},{"lang":"zh","group":"models","slug":"models/deepseek-chat","frontmatter":{"title":"DeepSeek V2.5","meta_title":"DeepSeek V2.5","description":"DeepSeek V2.5","date":"2024-05-14T00:00:00.000Z","image":"https://img.rifx.online/logo/deepseek.svg","categories":["text 2 text"],"author":"deepseek","tags":["Programming","Natural Language Processing","Machine Learning","Data Science","Chatbots"],"draft":false,"id":"deepseek-chat","context":128000,"input":1.4e-7,"output":2.8e-7,"img":0,"request":0,"last_updated":"2024-11-01T04:19:11.000Z","slug":"models/deepseek-chat"},"content":"\nDeepSeek-V2.5 是一个升级版本，结合了 DeepSeek-V2-Chat 和 DeepSeek-Coder-V2-Instruct。新模型整合了前两个版本的通用能力和编码能力。\n\nDeepSeek-V2 Chat 是 DeepSeek-V2 的对话微调版本，属于混合专家（MoE）语言模型。它总共包含 236B 个参数，其中每个 token 激活 21B。\n\n与 DeepSeek 67B 相比，DeepSeek-V2 的性能更强，同时节省了 42.5% 的训练成本，减少了 93.3% 的 KV 缓存，并将最大生成吞吐量提升至 5.76 倍。\n\nDeepSeek-V2 在标准基准测试和开放式生成评估中表现出色。\n\n"},{"lang":"zh","group":"models","slug":"models/dolphin-mixtral-8x22b","frontmatter":{"title":"Dolphin 2.9.2 Mixtral 8x22B 🐬","meta_title":"Dolphin 2.9.2 Mixtral 8x22B 🐬","description":"Dolphin 2.9.2 Mixtral 8x22B 🐬","date":"2024-06-08T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"cognitivecomputations","tags":["Natural Language Processing","Generative AI","Chatbots","Roleplay","Ethics"],"draft":false,"id":"dolphin-mixtral-8x22b","context":65536,"input":9e-7,"output":9e-7,"img":0,"request":0,"last_updated":"2024-11-04T12:49:50.000Z","slug":"models/dolphin-mixtral-8x22b"},"content":"\nDolphin 2.9 旨在进行指令跟随、对话和编码。该模型是 [Mixtral 8x22B Instruct](/mistralai/mixtral-8x22b-instruct) 的微调版本。它具有 64k 的上下文长度，并使用 ChatML 模板进行了 16k 序列长度的微调。\n\n该模型是 [Dolphin Mixtral 8x7B](/cognitivecomputations/dolphin-mixtral-8x7b) 的继任者。\n\n该模型未经过审查，并去除了对齐和偏见。它需要外部对齐层以确保伦理使用。用户被警告要负责任地使用这个高度合规的模型，详细信息请参见关于未审查模型的博客文章 [erichartford.com/uncensored-models](https://erichartford.com/uncensored-models)。\n\n#moe #uncensored\n\n"},{"lang":"zh","group":"models","slug":"models/dolphin-mixtral-8x7b","frontmatter":{"title":"Dolphin 2.6 Mixtral 8x7B 🐬","meta_title":"Dolphin 2.6 Mixtral 8x7B 🐬","description":"Dolphin 2.6 Mixtral 8x7B 🐬","date":"2023-12-21T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"cognitivecomputations","tags":["Programming","Natural Language Processing","Generative AI","Ethics","Chatbots"],"draft":false,"id":"dolphin-mixtral-8x7b","context":32768,"input":5e-7,"output":5e-7,"img":0,"request":0,"last_updated":"2024-11-04T12:52:28.000Z","slug":"models/dolphin-mixtral-8x7b"},"content":"\n这是对 [Mixtral-8x7b](/mistralai/mixtral-8x7b) 的 16k 上下文微调。由于大量使用编码数据进行训练，它在编码任务中表现出色，并以其服从性而闻名，尽管缺乏 DPO 调优。\n\n该模型未经过审查，并且去除了对齐和偏见。它需要一个外部对齐层以确保伦理使用。用户被提醒要负责任地使用这个高度合规的模型，具体细节可参见关于未审查模型的博客文章 [erichartford.com/uncensored-models](https://erichartford.com/uncensored-models)。\n\n#moe #uncensored\n\n"},{"lang":"zh","group":"models","slug":"models/gemini-flash-15-8b","frontmatter":{"title":"Google: Gemini 1.5 Flash-8B","meta_title":"Google: Gemini 1.5 Flash-8B","description":"Google: Gemini 1.5 Flash-8B","date":"2024-10-03T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text image 2 text"],"author":"google","tags":["Programming","Natural Language Processing","Chatbots","Translation","Technology/Web"],"draft":false,"id":"gemini-flash-1.5-8b","context":1000000,"input":3.75e-8,"output":1.5e-7,"img":0,"request":0,"last_updated":"2024-10-03T00:00:00.000Z","slug":"models/gemini-flash-15-8b"},"content":"\nGemini 1.5 Flash-8B 针对速度和效率进行了优化，在聊天、转录和翻译等小提示任务中提供了增强的性能。通过减少延迟，它在实时和大规模操作中非常有效。该模型专注于成本效益解决方案，同时保持高质量的结果。\n\n[点击此处了解更多关于此模型的信息](https://developers.googleblog.com/en/gemini-15-flash-8b-is-now-generally-available-for-use/)。\n\n使用 Gemini 受限于 Google's [Gemini 使用条款](https://ai.google.dev/terms)。\n\n"},{"lang":"zh","group":"models","slug":"models/gemini-flash-15","frontmatter":{"title":"Google: Gemini Flash 1.5","meta_title":"Google: Gemini Flash 1.5","description":"Google: Gemini Flash 1.5","date":"2024-05-14T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text image 2 text"],"author":"google","tags":["Programming","Machine Learning","Natural Language Processing","Computer Vision","Chatbots"],"draft":false,"id":"gemini-flash-1.5","context":1000000,"input":7.5e-8,"output":3e-7,"img":0.00004,"request":0,"last_updated":"2024-10-31T23:10:34.000Z","slug":"models/gemini-flash-15"},"content":"\nGemini 1.5 Flash 是一个基础模型，能够在多种多模态任务中表现良好，如视觉理解、分类、摘要，以及从图像、音频和视频中创建内容。它擅长处理视觉和文本输入，如照片、文档、信息图表和截图。\n\nGemini 1.5 Flash 旨在处理高容量、高频率的任务，在这些任务中，成本和延迟至关重要。在大多数常见任务中，Flash 的质量与其他 Gemini Pro 模型相当，但成本显著降低。Flash 非常适合聊天助手和按需内容生成等需要速度和规模的应用。\n\n使用 Gemini 需遵循 Google 的 [Gemini 使用条款](https://ai.google.dev/terms)。\n\n#multimodal\n\n"},{"lang":"zh","group":"models","slug":"models/gemini-pro-15","frontmatter":{"title":"Google: Gemini Pro 1.5","meta_title":"Google: Gemini Pro 1.5","description":"Google: Gemini Pro 1.5","date":"2024-04-09T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text image 2 text"],"author":"google","tags":["Programming","Natural Language Processing","Machine Learning","Generative AI","Chatbots"],"draft":false,"id":"gemini-pro-1.5","context":2000000,"input":0.00000125,"output":0.000005,"img":0.00263,"request":0,"last_updated":"2024-04-09T00:00:00.000Z","slug":"models/gemini-pro-15"},"content":"\n谷歌最新的多模态模型，支持在文本或聊天提示中使用图像和视频。\n\n针对以下语言任务进行了优化：\n\n- 代码生成\n- 文本生成\n- 文本编辑\n- 问题解决\n- 推荐\n- 信息提取\n- 数据提取或生成\n- AI代理\n\nGemini的使用受限于谷歌的[Gemini使用条款](https://ai.google.dev/terms)。\n\n#multimodal\n\n"},{"lang":"zh","group":"models","slug":"models/gpt-35-turbo-instruct","frontmatter":{"title":"OpenAI: GPT-3.5 Turbo Instruct","meta_title":"OpenAI: GPT-3.5 Turbo Instruct","description":"OpenAI: GPT-3.5 Turbo Instruct","date":"2023-09-28T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"openai","tags":["Programming","Natural Language Processing","Generative AI","Chatbots","Technology/Web"],"draft":false,"id":"gpt-3.5-turbo-instruct","context":4095,"input":0.0000015,"output":0.000002,"img":0,"request":0,"last_updated":"2023-09-28T00:00:00.000Z","slug":"models/gpt-35-turbo-instruct"},"content":"\n该模型是GPT-3.5 Turbo的一个变体，针对教学提示进行了调整，并省略了与聊天相关的优化。训练数据：截至2021年9月。\n\n"},{"lang":"zh","group":"models","slug":"models/gpt-4o-mini","frontmatter":{"title":"OpenAI: GPT-4o-mini","meta_title":"OpenAI: GPT-4o-mini","description":"OpenAI: GPT-4o-mini","date":"2024-07-18T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text image 2 text"],"author":"openai","tags":["Programming","Technology","Programming/Scripting","Technology/Web"],"draft":false,"id":"gpt-4o-mini","context":128000,"input":1.5e-7,"output":6e-7,"img":0.007225,"request":0,"last_updated":"2024-07-18T00:00:00.000Z","slug":"models/gpt-4o-mini"},"content":"\nGPT-4o mini 是 OpenAI 在 [GPT-4 Omni](/openai/gpt-4o) 之后发布的最新模型，支持文本和图像输入，并生成文本输出。\n\n作为他们最先进的小型模型，它的价格比其他最近的前沿模型低了许多倍，并且比 [GPT-3.5 Turbo](/openai/gpt-3.5-turbo) 便宜超过 60%。它保持了 SOTA 智能，同时在成本效益上显著更高。\n\nGPT-4o mini 在 MMLU 上获得了 82% 的分数，目前在聊天偏好 [常见排行榜](https://arena.lmsys.org/) 上排名高于 GPT-4。\n\n查看 [发布公告](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/) 以了解更多信息。\n\n"},{"lang":"zh","group":"models","slug":"models/gpt-4o","frontmatter":{"title":"OpenAI: GPT-4o","meta_title":"OpenAI: GPT-4o","description":"OpenAI: GPT-4o","date":"2024-05-13T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text image 2 text"],"author":"openai","tags":["Programming","Natural Language Processing","Machine Learning","Generative AI","Computer Vision"],"draft":false,"id":"gpt-4o","context":128000,"input":0.0000025,"output":0.00001,"img":0.0036125,"request":0,"last_updated":"2024-05-13T00:00:00.000Z","slug":"models/gpt-4o"},"content":"\nGPT-4o（“o”代表“全能”）是OpenAI最新的AI模型，支持文本和图像输入，并生成文本输出。它保持了[GPT-4 Turbo](/openai/gpt-4-turbo)的智能水平，同时速度提高了两倍，成本效益提高了50%。GPT-4o还在处理非英语语言和增强视觉能力方面提供了更好的性能。\n\n为了与其他模型进行基准测试，它曾被暂时称为[\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209)\n\n"},{"lang":"zh","group":"models","slug":"models/grok-beta","frontmatter":{"title":"xAI: Grok Beta","meta_title":"xAI: Grok Beta","description":"xAI: Grok Beta","date":"2024-10-20T00:00:00.000Z","image":"https://img.rifx.online/logo/xai.svg","categories":["text 2 text"],"author":"x-ai","tags":["Natural Language Processing","Machine Learning","Generative AI","Chatbots","Data Science"],"draft":false,"id":"grok-beta","context":131072,"input":0.000005,"output":0.000015,"img":0,"request":0,"last_updated":"2024-11-07T09:32:49.000Z","slug":"models/grok-beta"},"content":"\nGrok Beta 是 xAI 的实验性语言模型，具有最先进的推理能力，最适合复杂和多步骤的用例。\n\n它是 [Grok 2](https://x.ai/blog/grok-2) 的继任者，具有增强的上下文长度。\n\n"},{"lang":"zh","group":"models","slug":"models/lfm-40b","frontmatter":{"title":"Liquid: LFM 40B MoE","meta_title":"Liquid: LFM 40B MoE","description":"Liquid: LFM 40B MoE","date":"2024-09-30T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"liquid","tags":["Machine Learning","Natural Language Processing","Data Science","Generative AI","Computer Vision"],"draft":false,"id":"lfm-40b","context":32768,"input":0.000001,"output":0.000002,"img":0,"request":0,"last_updated":"2024-11-04T12:44:22.000Z","slug":"models/lfm-40b"},"content":"\nLiquid的40.3B专家混合模型（MoE）。Liquid基础模型（LFM）是基于动态系统构建的大型神经网络，具有计算单元。\n\nLFM是通用人工智能模型，可用于建模任何类型的序列数据，包括视频、音频、文本、时间序列和信号。\n\n有关基准测试和更多信息，请参阅[发布公告](https://www.liquid.ai/liquid-foundation-models)。\n\n"},{"lang":"zh","group":"models","slug":"models/liquid-lfm-40b:free","frontmatter":{"title":"Liquid: LFM 40B MoE (free)","meta_title":"Liquid: LFM 40B MoE (free)","description":"Liquid: LFM 40B MoE (free)","date":"2024-09-30T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"liquid","tags":["Generative AI","Machine Learning","Natural Language Processing","Data Science","Technology/Web"],"draft":false,"id":"liquid/lfm-40b:free","context":8192,"input":0,"output":0,"img":0,"request":0,"last_updated":"2024-11-07T00:17:57.000Z","slug":"models/liquid-lfm-40b:free"},"content":"\nLiquid的40.3B专家混合模型（MoE）。Liquid基础模型（LFMs）是基于动态系统构建的大型神经网络。\n\nLFMs是通用的AI模型，可以用于建模任何类型的序列数据，包括视频、音频、文本、时间序列和信号。\n\n有关基准测试和更多信息，请参见[发布公告](https://www.liquid.ai/liquid-foundation-models)。\n\n_这些是[LFM 40B MoE](/liquid/lfm-40b)的免费限流端点。输出可能会被缓存。有关速率限制的信息，请[在这里](/docs/limits)阅读。_\n\n"},{"lang":"zh","group":"models","slug":"models/llama-31-70b-instruct","frontmatter":{"title":"Meta: Llama 3.1 70B Instruct","meta_title":"Meta: Llama 3.1 70B Instruct","description":"Meta: Llama 3.1 70B Instruct","date":"2024-07-23T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text 2 text"],"author":"meta-llama","tags":["Programming","Machine Learning","Natural Language Processing","Chatbots","Ethics"],"draft":false,"id":"llama-3.1-70b-instruct","context":131072,"input":3e-7,"output":3e-7,"img":0,"request":0,"last_updated":"2024-10-28T13:38:49.000Z","slug":"models/llama-31-70b-instruct"},"content":"\nMeta最新发布的模型类别（Llama 3.1）推出了多种尺寸和版本。这个70B的指令调优版本针对高质量对话用例进行了优化。\n\n在人工评估中，它与领先的闭源模型相比表现出色。\n\n使用该模型需遵循[Meta的可接受使用政策](https://www.llama.com/llama3/use-policy/)。\n\n"},{"lang":"zh","group":"models","slug":"models/llama-31-8b-instruct","frontmatter":{"title":"Meta: Llama 3.1 8B Instruct","meta_title":"Meta: Llama 3.1 8B Instruct","description":"Meta: Llama 3.1 8B Instruct","date":"2024-07-23T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text 2 text"],"author":"meta-llama","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Ethics"],"draft":false,"id":"llama-3.1-8b-instruct","context":131072,"input":5.5e-8,"output":5.5e-8,"img":0,"request":0,"last_updated":"2024-10-31T23:27:09.000Z","slug":"models/llama-31-8b-instruct"},"content":"\nMeta最新发布的模型系列（Llama 3.1）推出了多种尺寸和版本。这个8B指令调优版本快速且高效。\n\n与领先的闭源模型相比，它在人工评估中表现出色。\n\n使用此模型需遵循[Meta的可接受使用政策](https://www.llama.com/llama3/use-policy/)。\n\n"},{"lang":"zh","group":"models","slug":"models/llama-31-nemotron-70b-instruct","frontmatter":{"title":"Nvidia: Llama 3.1 Nemotron 70B Instruct","meta_title":"Nvidia: Llama 3.1 Nemotron 70B Instruct","description":"Nvidia: Llama 3.1 Nemotron 70B Instruct","date":"2024-10-15T00:00:00.000Z","image":"https://img.rifx.online/logo/nvidia.svg","categories":["text 2 text"],"author":"nvidia","tags":["Programming","Natural Language Processing","Machine Learning","Generative AI","Ethics"],"draft":false,"id":"llama-3.1-nemotron-70b-instruct","context":131072,"input":3.5e-7,"output":4e-7,"img":0,"request":0,"last_updated":"2024-10-15T00:00:00.000Z","slug":"models/llama-31-nemotron-70b-instruct"},"content":"\nNVIDIA的Llama 3.1 Nemotron 70B是一个旨在生成精确和有用响应的语言模型。利用[Llama 3.1 70B](/models/meta-llama/llama-3.1-70b-instruct)架构和基于人类反馈的强化学习（RLHF），它在自动对齐基准测试中表现出色。该模型专为需要高准确性以提供帮助和生成响应的应用而设计，适合处理多个领域的多样用户查询。\n\n使用此模型须遵循[Meta的可接受使用政策](https://www.llama.com/llama3/use-policy/)。\n\n"},{"lang":"zh","group":"models","slug":"models/llama-31-sonar-huge-128k-online","frontmatter":{"title":"Perplexity: Llama 3.1 Sonar 405B Online","meta_title":"Perplexity: Llama 3.1 Sonar 405B Online","description":"Perplexity: Llama 3.1 Sonar 405B Online","date":"2024-08-14T00:00:00.000Z","image":"https://img.rifx.online/logo/perplexity.svg","categories":["text 2 text"],"author":"perplexity","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"llama-3.1-sonar-huge-128k-online","context":127072,"input":0.000005,"output":0.000005,"img":0,"request":0.005,"last_updated":"2024-11-07T09:36:38.000Z","slug":"models/llama-31-sonar-huge-128k-online"},"content":"\nLlama 3.1 Sonar 是 Perplexity 最新的模型系列。它在成本效益、速度和性能上超越了他们早期的 Sonar 模型。该模型基于 Llama 3.1 405B，并具有互联网访问功能。\n\n"},{"lang":"zh","group":"models","slug":"models/llama-31-sonar-large-128k-online","frontmatter":{"title":"Perplexity: Llama 3.1 Sonar 70B Online","meta_title":"Perplexity: Llama 3.1 Sonar 70B Online","description":"Perplexity: Llama 3.1 Sonar 70B Online","date":"2024-08-01T00:00:00.000Z","image":"https://img.rifx.online/logo/perplexity.svg","categories":["text 2 text"],"author":"perplexity","tags":["Programming","Machine Learning","Natural Language Processing","Chatbots","Generative AI"],"draft":false,"id":"llama-3.1-sonar-large-128k-online","context":127072,"input":0.000001,"output":0.000001,"img":0,"request":0.005,"last_updated":"2024-11-07T09:37:21.000Z","slug":"models/llama-31-sonar-large-128k-online"},"content":"\nLlama 3.1 Sonar 是 Perplexity 最新的模型系列。它在成本效益、速度和性能方面超越了他们早期的 Sonar 模型。\n\n这是 [离线聊天模型](/perplexity/llama-3.1-sonar-large-128k-chat) 的在线版本。它专注于提供有帮助、最新和真实的响应。 #online\n\n"},{"lang":"zh","group":"models","slug":"models/llama-31-sonar-small-128k-online","frontmatter":{"title":"Perplexity: Llama 3.1 Sonar 8B Online","meta_title":"Perplexity: Llama 3.1 Sonar 8B Online","description":"Perplexity: Llama 3.1 Sonar 8B Online","date":"2024-08-01T00:00:00.000Z","image":"https://img.rifx.online/logo/perplexity.svg","categories":["text 2 text"],"author":"perplexity","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"llama-3.1-sonar-small-128k-online","context":127072,"input":2e-7,"output":2e-7,"img":0,"request":0.005,"last_updated":"2024-11-07T09:38:09.000Z","slug":"models/llama-31-sonar-small-128k-online"},"content":"\nLlama 3.1 Sonar 是 Perplexity 最新的模型系列。它在成本效率、速度和性能上超越了他们早期的 Sonar 模型。\n\n这是 [离线聊天模型](/perplexity/llama-3.1-sonar-small-128k-chat) 的在线版本。它专注于提供有用、最新和真实的响应。 #online\n\n"},{"lang":"zh","group":"models","slug":"models/llama-32-11b-vision-instruct","frontmatter":{"title":"Meta: Llama 3.2 11B Vision Instruct","meta_title":"Meta: Llama 3.2 11B Vision Instruct","description":"Meta: Llama 3.2 11B Vision Instruct","date":"2024-09-25T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text image 2 text"],"author":"meta-llama","tags":["Natural Language Processing","Computer Vision","Machine Learning","Generative AI","Data Science"],"draft":false,"id":"llama-3.2-11b-vision-instruct","context":131072,"input":5.5e-8,"output":5.5e-8,"img":0.000079475,"request":0,"last_updated":"2024-09-25T00:00:00.000Z","slug":"models/llama-32-11b-vision-instruct"},"content":"\nLlama 3.2 11B Vision 是一个具有 110 亿参数的多模态模型，旨在处理结合视觉和文本数据的任务。它在图像描述和视觉问答等任务中表现出色，弥合了语言生成与视觉推理之间的差距。该模型在一个庞大的图像-文本对数据集上进行了预训练，在复杂的高精度图像分析中表现良好。\n\n它将视觉理解与语言处理相结合的能力，使其成为需要全面视觉语言 AI 应用的行业的理想解决方案，例如内容创作、AI 驱动的客户服务和研究。\n\n点击此处查看 [原始模型卡](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md)。\n\n使用此模型须遵循 [Meta 的可接受使用政策](https://www.llama.com/llama3/use-policy/)。\n\n"},{"lang":"zh","group":"models","slug":"models/llama-32-1b-instruct","frontmatter":{"title":"Meta: Llama 3.2 1B Instruct","meta_title":"Meta: Llama 3.2 1B Instruct","description":"Meta: Llama 3.2 1B Instruct","date":"2024-09-25T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text 2 text"],"author":"meta-llama","tags":["Natural Language Processing","Programming","Technology","Machine Learning","Generative AI"],"draft":false,"id":"llama-3.2-1b-instruct","context":131072,"input":1e-8,"output":2e-8,"img":0,"request":0,"last_updated":"2024-09-25T00:00:00.000Z","slug":"models/llama-32-1b-instruct"},"content":"\nLlama 3.2 1B 是一个拥有10亿参数的语言模型，专注于高效执行自然语言任务，如摘要、对话和多语言文本分析。其较小的规模使其能够在低资源环境中高效运行，同时保持强大的任务性能。\n\n支持八种核心语言，并可针对更多语言进行微调，Llama 1.3B 非常适合寻求轻量级但强大的 AI 解决方案的企业或开发者，这些解决方案能够在多样化的多语言环境中运行，而不需要大型模型的高计算需求。\n\n点击此处查看 [原始模型卡](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md)。\n\n使用此模型需遵循 [Meta 的可接受使用政策](https://www.llama.com/llama3/use-policy/)。\n\n"},{"lang":"zh","group":"models","slug":"models/llama-32-90b-vision-instruct","frontmatter":{"title":"Meta: Llama 3.2 90B Vision Instruct","meta_title":"Meta: Llama 3.2 90B Vision Instruct","description":"Meta: Llama 3.2 90B Vision Instruct","date":"2024-09-25T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text image 2 text"],"author":"meta-llama","tags":["Natural Language Processing","Computer Vision","Machine Learning","Data Science","Generative AI"],"draft":false,"id":"llama-3.2-90b-vision-instruct","context":131072,"input":3.5e-7,"output":4e-7,"img":0.00050575,"request":0,"last_updated":"2024-09-25T00:00:00.000Z","slug":"models/llama-32-90b-vision-instruct"},"content":"\nLlama 90B Vision模型是一款顶级的90亿参数多模态模型，旨在应对最具挑战性的视觉推理和语言任务。它在图像描述、视觉问答和高级图像-文本理解方面提供无与伦比的准确性。该模型在庞大的多模态数据集上进行预训练，并通过人类反馈进行微调，专为处理最苛刻的基于图像的AI任务而设计。\n\n此模型非常适合需要尖端多模态AI能力的行业，尤其是那些处理复杂实时视觉和文本分析的行业。\n\n点击此处查看[原始模型卡片](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md)。\n\n使用此模型需遵守[Meta的可接受使用政策](https://www.llama.com/llama3/use-policy/)。\n\n"},{"lang":"zh","group":"models","slug":"models/lzlv-70b-fp16-hf","frontmatter":{"title":"lzlv 70B","meta_title":"lzlv 70B","description":"lzlv 70B","date":"2023-11-12T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"lizpreciatior","tags":["Roleplay","Programming","Machine Learning","Generative AI","Chatbots"],"draft":false,"id":"lzlv-70b-fp16-hf","context":4096,"input":3.5e-7,"output":4e-7,"img":0,"request":0,"last_updated":"2024-11-04T12:50:34.000Z","slug":"models/lzlv-70b-fp16-hf"},"content":"\nA Mythomax/MLewd_13B风格的选定70B模型合并。\n一个多模型合并，结合了多个LLaMA2 70B微调模型，用于角色扮演和创意工作。目标是创建一个将创造力与智能相结合的模型，以提升体验。\n\n#merge #uncensored\n\n"},{"lang":"zh","group":"models","slug":"models/magnum-v4-72b","frontmatter":{"title":"Magnum v4 72B","meta_title":"Magnum v4 72B","description":"Magnum v4 72B","date":"2024-10-22T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"anthracite-org","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"magnum-v4-72b","context":32768,"input":0.000001875,"output":0.00000225,"img":0,"request":0,"last_updated":"2024-11-04T12:39:55.000Z","slug":"models/magnum-v4-72b"},"content":"\n这是一个系列模型，旨在复制Claude 3模型的散文质量，特别是Sonnet(https://openrouter.ai/anthropic/claude-3.5-sonnet)和Opus(https://openrouter.ai/anthropic/claude-3-opus)。\n\n该模型是在[Qwen2.5 72B](https://openrouter.ai/qwen/qwen-2.5-72b-instruct)的基础上进行微调的。\n\n"},{"lang":"zh","group":"models","slug":"models/ministral-3b","frontmatter":{"title":"Ministral 3B","meta_title":"Ministral 3B","description":"Ministral 3B","date":"2024-10-17T00:00:00.000Z","image":"https://img.rifx.online/logo/mistral.png","categories":["text 2 text"],"author":"mistralai","tags":["Programming","Machine Learning","Natural Language Processing","Data Science","Generative AI"],"draft":false,"id":"ministral-3b","context":128000,"input":4e-8,"output":4e-8,"img":0,"request":0,"last_updated":"2024-11-07T00:24:37.000Z","slug":"models/ministral-3b"},"content":"\nMinistral 3B 是一个针对设备和边缘计算优化的 3B 参数模型。它在知识、常识推理和函数调用方面表现出色，在大多数基准测试中超越了像 Mistral 7B 这样的更大模型。支持最长 128k 的上下文长度，非常适合高效推理的代理工作流和专业任务的协调。\n\n"},{"lang":"zh","group":"models","slug":"models/ministral-8b","frontmatter":{"title":"Ministral 8B","meta_title":"Ministral 8B","description":"Ministral 8B","date":"2024-10-17T00:00:00.000Z","image":"https://img.rifx.online/logo/mistral.png","categories":["text 2 text"],"author":"mistralai","tags":["Technology","Machine Learning","Data Science","Generative AI","Ethics"],"draft":false,"id":"ministral-8b","context":128000,"input":1e-7,"output":1e-7,"img":0,"request":0,"last_updated":"2024-10-19T04:54:11.000Z","slug":"models/ministral-8b"},"content":"\nMinistral 8B 是一个具有 8B 参数的模型，采用独特的交错滑动窗口注意力模式，以实现更快、更节省内存的推理。该模型专为边缘使用案例设计，支持最长 128k 的上下文长度，并在知识和推理任务中表现出色。它在低于 10B 的类别中优于同类产品，非常适合低延迟、注重隐私的应用。\n\n"},{"lang":"zh","group":"models","slug":"models/mistral-7b-instruct","frontmatter":{"title":"Mistral: Mistral 7B Instruct","meta_title":"Mistral: Mistral 7B Instruct","description":"Mistral: Mistral 7B Instruct","date":"2024-05-27T00:00:00.000Z","image":"https://img.rifx.online/logo/mistral.png","categories":["text 2 text"],"author":"mistralai","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"mistral-7b-instruct","context":32768,"input":5.5e-8,"output":5.5e-8,"img":0,"request":0,"last_updated":"2024-10-31T23:13:12.000Z","slug":"models/mistral-7b-instruct"},"content":"\n一个高性能、行业标准的7.3B参数模型，针对速度和上下文长度进行了优化。\n\n*Mistral 7B Instruct有多个版本变体，本文旨在介绍最新版本。*\n\n"},{"lang":"zh","group":"models","slug":"models/mistral-nemo","frontmatter":{"title":"Mistral: Mistral Nemo","meta_title":"Mistral: Mistral Nemo","description":"Mistral: Mistral Nemo","date":"2024-07-19T00:00:00.000Z","image":"https://img.rifx.online/logo/mistral.png","categories":["text 2 text"],"author":"mistralai","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Data Science"],"draft":false,"id":"mistral-nemo","context":128000,"input":1.3e-7,"output":1.3e-7,"img":0,"request":0,"last_updated":"2024-10-31T23:10:58.000Z","slug":"models/mistral-nemo"},"content":"\n由Mistral与NVIDIA合作构建的12B参数模型，具有128k的上下文长度。\n\n该模型是多语言的，支持英语、法语、德语、西班牙语、意大利语、葡萄牙语、中文、日语、韩语、阿拉伯语和印地语。\n\n它支持函数调用，并在Apache 2.0许可证下发布。\n\n"},{"lang":"zh","group":"models","slug":"models/mistral-tiny","frontmatter":{"title":"Mistral Tiny","meta_title":"Mistral Tiny","description":"Mistral Tiny","date":"2024-01-10T00:00:00.000Z","image":"https://img.rifx.online/logo/mistral.png","categories":["text 2 text"],"author":"mistralai","tags":["Programming","Machine Learning","Data Science","Generative AI","Chatbots"],"draft":false,"id":"mistral-tiny","context":32000,"input":2.5e-7,"output":2.5e-7,"img":0,"request":0,"last_updated":"2024-10-31T23:12:22.000Z","slug":"models/mistral-tiny"},"content":"\n该模型目前由 Mistral-7B-v0.2 驱动，并结合了比 [Mistral 7B](/mistralai/mistral-7b-instruct-v0.1) 更“优越”的微调，灵感来自社区的工作。它最适合用于大批量处理任务，在这些任务中，成本是一个重要因素，但推理能力并不是关键。\n\n"},{"lang":"zh","group":"models","slug":"models/mythomax-l2-13b","frontmatter":{"title":"MythoMax 13B","meta_title":"MythoMax 13B","description":"MythoMax 13B","date":"2023-07-02T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"gryphe","tags":["Roleplay","Programming","Machine Learning","Natural Language Processing","Generative AI"],"draft":false,"id":"mythomax-l2-13b","context":4096,"input":1e-7,"output":1e-7,"img":0,"request":0,"last_updated":"2024-10-28T13:10:41.000Z","slug":"models/mythomax-l2-13b"},"content":"\nLlama 2 13B 的性能最高且最受欢迎的微调之一，具有丰富的描述和角色扮演。 #merge\n\n"},{"lang":"zh","group":"models","slug":"models/o1-mini","frontmatter":{"title":"OpenAI: o1-mini","meta_title":"OpenAI: o1-mini","description":"OpenAI: o1-mini","date":"2024-09-12T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"openai","tags":["Programming","Science","Natural Language Processing","Machine Learning","Data Science"],"draft":false,"id":"o1-mini","context":128000,"input":0.000003,"output":0.000012,"img":0,"request":0,"last_updated":"2024-09-12T00:00:00.000Z","slug":"models/o1-mini"},"content":"\nOpenAI最新且最强大的模型系列o1旨在在响应之前花更多时间思考。\n\no1模型经过优化，适用于数学、科学、编程及其他STEM相关任务。它们在物理、化学和生物学的基准测试中始终展现出博士级的准确性。了解更多信息，请查看[发布公告](https://openai.com/o1)。\n\n注意：该模型目前处于实验阶段，不适合生产使用，并可能受到严格的速率限制。\n\n"},{"lang":"zh","group":"models","slug":"models/o1-preview","frontmatter":{"title":"OpenAI: o1-preview","meta_title":"OpenAI: o1-preview","description":"OpenAI: o1-preview","date":"2024-09-12T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"openai","tags":["Programming","Science","Natural Language Processing","Machine Learning","Data Science"],"draft":false,"id":"o1-preview","context":128000,"input":0.000015,"output":0.00006,"img":0,"request":0,"last_updated":"2024-09-12T00:00:00.000Z","slug":"models/o1-preview"},"content":"\nOpenAI最新且最强大的模型系列o1旨在在响应之前花更多时间思考。\n\no1模型经过优化，适用于数学、科学、编程和其他STEM相关任务。它们在物理、化学和生物学的基准测试中始终表现出博士级的准确性。有关更多信息，请参阅[发布公告](https://openai.com/o1)。\n\n注意：该模型目前处于实验阶段，不适合生产使用案例，并且可能会受到严格的速率限制。\n\n"},{"lang":"zh","group":"models","slug":"models/openchat-7b","frontmatter":{"title":"OpenChat 3.5 7B","meta_title":"OpenChat 3.5 7B","description":"OpenChat 3.5 7B","date":"2023-11-28T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"openchat","tags":["Programming","Natural Language Processing","Machine Learning","Open Source","Generative AI"],"draft":false,"id":"openchat-7b","context":8192,"input":5.5e-8,"output":5.5e-8,"img":0,"request":0,"last_updated":"2024-11-07T09:39:42.000Z","slug":"models/openchat-7b"},"content":"\nOpenChat 7B 是一个开源语言模型库，采用“C-RLFT（条件强化学习微调）”策略进行了微调，该策略受到离线强化学习的启发。它在没有偏好标签的混合质量数据上进行了训练。\n\n- 对于在 Mistral 7B 上微调的 OpenChat，请查看 [OpenChat 7B](/openchat/openchat-7b)。\n- 对于在 Llama 8B 上微调的 OpenChat，请查看 [OpenChat 8B](/openchat/openchat-8b)。\n\n#open-source\n\n"},{"lang":"zh","group":"models","slug":"models/phi-35-mini-128k-instruct","frontmatter":{"title":"Phi-3.5 Mini 128K Instruct","meta_title":"Phi-3.5 Mini 128K Instruct","description":"Phi-3.5 Mini 128K Instruct","date":"2024-08-21T00:00:00.000Z","image":"https://img.rifx.online/logo/microsoft.svg","categories":["text 2 text"],"author":"microsoft","tags":["Programming","Machine Learning","Natural Language Processing","Data Science","Generative AI"],"draft":false,"id":"phi-3.5-mini-128k-instruct","context":128000,"input":1e-7,"output":1e-7,"img":0,"request":0,"last_updated":"2024-11-01T04:17:17.000Z","slug":"models/phi-35-mini-128k-instruct"},"content":"\nPhi-3.5 模型是轻量级的、先进的开放模型。这些模型使用 Phi-3 数据集进行训练，该数据集包括合成数据和经过筛选的公共网站数据，重点关注高质量和推理密集的特性。Phi-3.5 Mini 使用 3.8B 参数，是一种仅解码的稠密变换器模型，使用与 [Phi-3 Mini](/microsoft/phi-3-mini-128k-instruct) 相同的分词器。\n\n这些模型经过严格的增强过程，结合了监督微调、邻近策略优化和直接偏好优化，以确保精确的指令遵循和强大的安全措施。在针对测试常识、语言理解、数学、代码、长上下文和逻辑推理的基准评估中，Phi-3.5 模型在参数少于 130 亿的模型中展示了强大且先进的性能。\n\n"},{"lang":"zh","group":"models","slug":"models/qwen-2-7b-instruct","frontmatter":{"title":"Qwen 2 7B Instruct","meta_title":"Qwen 2 7B Instruct","description":"Qwen 2 7B Instruct","date":"2024-07-16T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"qwen","tags":["Natural Language Processing","Programming","Machine Learning","Data Science","Ethics"],"draft":false,"id":"qwen-2-7b-instruct","context":32768,"input":5.4e-8,"output":5.4e-8,"img":0,"request":0,"last_updated":"2024-07-16T00:00:00.000Z","slug":"models/qwen-2-7b-instruct"},"content":"\nQwen2 7B 是一个基于变换器的模型，擅长语言理解、多语言能力、编码、数学和推理。\n\n它具有 SwiGLU 激活、注意力 QKV 偏置和组查询注意力。它在大量数据上进行预训练，并经过监督微调和直接偏好优化。\n\n有关更多详细信息，请参阅此 [博客文章](https://qwenlm.github.io/blog/qwen2/) 和 [GitHub 仓库](https://github.com/QwenLM/Qwen2)。\n\n使用此模型须遵循 [同义千问许可证协议](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE)。\n\n"},{"lang":"zh","group":"models","slug":"models/qwen-2-vl-72b-instruct","frontmatter":{"title":"Qwen2-VL 72B Instruct","meta_title":"Qwen2-VL 72B Instruct","description":"Qwen2-VL 72B Instruct","date":"2024-09-18T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text image 2 text"],"author":"qwen","tags":["Natural Language Processing","Computer Vision","Robotics","Machine Learning"],"draft":false,"id":"qwen-2-vl-72b-instruct","context":32768,"input":4e-7,"output":4e-7,"img":0.000578,"request":0,"last_updated":"2024-09-18T00:00:00.000Z","slug":"models/qwen-2-vl-72b-instruct"},"content":"\nQwen2 VL 72B 是来自 Qwen Team 的多模态 LLM，具有以下关键增强功能：\n\n- 对各种分辨率和比例图像的最先进理解：Qwen2-VL 在视觉理解基准测试中实现了最先进的性能，包括 MathVista、DocVQA、RealWorldQA、MTVQA 等。\n\n- 理解超过 20 分钟的视频：Qwen2-VL 可以理解超过 20 分钟的视频，以进行高质量的视频问答、对话、内容创作等。\n\n- 能够操作您的手机、机器人等的智能体：凭借复杂推理和决策能力，Qwen2-VL 可以与手机、机器人等设备集成，实现基于视觉环境和文本指令的自动操作。\n\n- 多语言支持：为了服务全球用户，除了英语和中文，Qwen2-VL 现在还支持理解图像中不同语言的文本，包括大多数欧洲语言、日语、韩语、阿拉伯语、越南语等。\n\n有关更多详细信息，请参阅此 [博客文章](https://qwenlm.github.io/blog/qwen2-vl/) 和 [GitHub 仓库](https://github.com/QwenLM/Qwen2-VL)。\n\n使用此模型受 [通义千问许可协议](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE) 的约束。\n\n"},{"lang":"zh","group":"models","slug":"models/qwen-25-72b-instruct","frontmatter":{"title":"Qwen2.5 72B Instruct","meta_title":"Qwen2.5 72B Instruct","description":"Qwen2.5 72B Instruct","date":"2024-09-19T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"qwen","tags":["Programming","Natural Language Processing","Chatbots","Machine Learning","Data Science"],"draft":false,"id":"qwen-2.5-72b-instruct","context":131072,"input":3.5e-7,"output":4e-7,"img":0,"request":0,"last_updated":"2024-09-19T00:00:00.000Z","slug":"models/qwen-25-72b-instruct"},"content":"\nQwen2.5 72B 是 Qwen 大型语言模型的最新系列。Qwen2.5 在 Qwen2 的基础上带来了以下改进：\n\n- 知识显著增加，并在编码和数学能力上有了很大提升，这得益于我们在这些领域的专业专家模型。\n\n- 在遵循指令、生成长文本（超过 8K tokens）、理解结构化数据（例如，表格）以及生成结构化输出（特别是 JSON）方面有显著改进。对系统提示的多样性更加强韧，增强了角色扮演的实现和聊天机器人的条件设置。\n\n- 支持最长 128K tokens 的长上下文，并可以生成最多 8K tokens。\n\n- 支持超过 29 种语言，包括中文、英文、法语、西班牙语、葡萄牙语、德语、意大利语、俄语、日语、韩语、越南语、泰语、阿拉伯语等。\n\n使用此模型须遵守 [同义千问许可协议](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE)。\n\n"},{"lang":"zh","group":"models","slug":"models/qwen-25-7b-instruct","frontmatter":{"title":"Qwen2.5 7B Instruct","meta_title":"Qwen2.5 7B Instruct","description":"Qwen2.5 7B Instruct","date":"2024-10-16T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"qwen","tags":["Programming","Natural Language Processing","Chatbots","Machine Learning","Data Science"],"draft":false,"id":"qwen-2.5-7b-instruct","context":131072,"input":2.7e-7,"output":2.7e-7,"img":0,"request":0,"last_updated":"2024-10-16T00:00:00.000Z","slug":"models/qwen-25-7b-instruct"},"content":"\nQwen2.5 7B 是 Qwen 大语言模型的最新系列。Qwen2.5 在 Qwen2 的基础上带来了以下改进：\n\n- 知识显著增加，并在编码和数学方面的能力大幅提升，这得益于我们在这些领域的专业模型。\n\n- 在遵循指令、生成长文本（超过 8K tokens）、理解结构化数据（例如，表格）以及生成结构化输出，特别是 JSON 方面有显著改进。对系统提示的多样性更具韧性，增强了角色扮演的实现和聊天机器人的条件设置。\n\n- 长文本支持高达 128K tokens，并且可以生成最多 8K tokens。\n\n- 支持超过 29 种语言，包括中文、英文、法文、西班牙文、葡萄牙文、德文、意大利文、俄文、日文、韩文、越南文、泰文、阿拉伯文等。\n\n使用此模型须遵守 [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE)。\n\n"},{"lang":"zh","group":"models","slug":"models/remm-slerp-l2-13b","frontmatter":{"title":"ReMM SLERP 13B","meta_title":"ReMM SLERP 13B","description":"ReMM SLERP 13B","date":"2023-07-22T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"undi95","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"remm-slerp-l2-13b","context":4096,"input":0.000001125,"output":0.000001125,"img":0,"request":0,"last_updated":"2024-11-04T12:46:13.000Z","slug":"models/remm-slerp-l2-13b"},"content":"\n原版 MythoMax-L2-B13 的重现试验，但采用了更新的模型。 #merge\n\n"},{"lang":"zh","group":"models","slug":"models/remm-slerp-l2-13b:extended","frontmatter":{"title":"ReMM SLERP 13B (extended)","meta_title":"ReMM SLERP 13B (extended)","description":"ReMM SLERP 13B (extended)","date":"2023-07-22T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"undi95","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"remm-slerp-l2-13b:extended","context":6144,"input":0.000001125,"output":0.000001125,"img":0,"request":0,"last_updated":"2024-11-04T12:47:21.000Z","slug":"models/remm-slerp-l2-13b:extended"},"content":"\n原始 MythoMax-L2-B13 的重现试验，但使用了更新的模型。 #merge\n\n_这些是 [ReMM SLERP 13B](/undi95/remm-slerp-l2-13b) 的扩展上下文端点。它们可能具有更高的价格。_\n\n"},{"lang":"zh","group":"models","slug":"models/toppy-m-7b","frontmatter":{"title":"Toppy M 7B","meta_title":"Toppy M 7B","description":"Toppy M 7B","date":"2023-11-10T00:00:00.000Z","image":"/images/logo.svg","categories":["text 2 text"],"author":"undi95","tags":["Programming","Machine Learning","Generative AI","Chatbots","Data Science"],"draft":false,"id":"toppy-m-7b","context":4096,"input":7e-8,"output":7e-8,"img":0,"request":0,"last_updated":"2024-11-04T12:51:35.000Z","slug":"models/toppy-m-7b"},"content":"\n一个野生的7B参数模型，通过mergekit中的新task_arithmetic合并方法合并了多个模型。\n合并模型列表：\n- NousResearch/Nous-Capybara-7B-V1.9\n- [HuggingFaceH4/zephyr-7b-beta](/huggingfaceh4/zephyr-7b-beta)\n- lemonilia/AshhLimaRP-Mistral-7B\n- Vulkane/120-Days-of-Sodom-LoRA-Mistral-7b\n- Undi95/Mistral-pippa-sharegpt-7b-qlora\n\n#merge #uncensored\n\n"},{"lang":"zh","group":"models","slug":"models/wizardlm-2-7b","frontmatter":{"title":"WizardLM-2 7B","meta_title":"WizardLM-2 7B","description":"WizardLM-2 7B","date":"2024-04-16T00:00:00.000Z","image":"https://img.rifx.online/logo/microsoft.svg","categories":["text 2 text"],"author":"microsoft","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"wizardlm-2-7b","context":32000,"input":5.5e-8,"output":5.5e-8,"img":0,"request":0,"last_updated":"2024-10-31T23:23:36.000Z","slug":"models/wizardlm-2-7b"},"content":"\nWizardLM-2 7B 是微软 AI 最新 Wizard 模型的较小版本。它是最快的，并且在性能上与现有的 10 倍更大的开源领先模型相当。\n\n它是对 [Mistral 7B Instruct](/mistralai/mistral-7b-instruct) 的微调，使用与 [WizardLM-2 8x22B](/microsoft/wizardlm-2-8x22b) 相同的技术。\n\n要了解更多关于模型发布的信息，[请点击这里](https://wizardlm.github.io/WizardLM2/)。\n\n#moe\n\n"},{"lang":"zh","group":"models","slug":"models/wizardlm-2-8x22b","frontmatter":{"title":"WizardLM-2 8x22B","meta_title":"WizardLM-2 8x22B","description":"WizardLM-2 8x22B","date":"2024-04-16T00:00:00.000Z","image":"https://img.rifx.online/logo/microsoft.svg","categories":["text 2 text"],"author":"microsoft","tags":["Programming","Machine Learning","Natural Language Processing","Generative AI","Chatbots"],"draft":false,"id":"wizardlm-2-8x22b","context":65536,"input":5e-7,"output":5e-7,"img":0,"request":0,"last_updated":"2024-10-31T23:24:21.000Z","slug":"models/wizardlm-2-8x22b"},"content":"\nWizardLM-2 8x22B 是微软 AI 最先进的 Wizard 模型。与领先的专有模型相比，它展示了高度竞争的性能，并且始终优于所有现有的最先进的开源模型。\n\n它是 [Mixtral 8x22B](/mistralai/mixtral-8x22b) 的指令微调版本。\n\n要了解有关模型发布的更多信息，[请点击这里](https://wizardlm.github.io/WizardLM2/)。\n\n#moe\n\n"}]
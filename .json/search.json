[{"lang":"en","group":"blog","slug":"blog/a-practical-guide-for-using-autogen-in-software-applications-8799185d27ee","frontmatter":{"title":"A practical guide for using AutoGen in software applications","meta_title":"A practical guide for using AutoGen in software applications","description":"Update: While this article was written only 4 months ago, AutoGen has since changed quite a bit. I apologize for some things that may be…","date":"2024-10-23T11:51:51.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*yrraWH6aGNnbx8p-wfQ1OQ.jpeg","categories":["llm"],"author":"Rifx.Online","tags":["llm"],"draft":false,"slug":"blog/a-practical-guide-for-using-autogen-in-software-applications-8799185d27ee"},"content":"\n\n\n\n\n\n*Update: While this article was written only 4 months ago, AutoGen has since changed quite a bit. I apologize for some things that may be outdated in my code examples.*\n\nIf you want to learn about AutoGen, there is [documentation](https://microsoft.github.io/autogen/), [Colab notebooks](https://microsoft.github.io/autogen/docs/Examples), and [a blog](https://microsoft.github.io/autogen/blog). Huge kudos to the AutoGen team for making an AMAZING product, but honestly — after reading all their stuff, I still didn’t know how to use AutoGen outside of a terminal or Jupyter Notebook.\n\nThis article tries to help fill that gap by giving some helpful ways to make AutoGen work in a software application. Here are the topics I’ll go over:\n\n1. Agents aren’t limited to communicating just over the terminal\n2. Registering custom replies\n3. How to include real humans in the conversation in real ways\n4. You can (and should) customize who speaks next\n5. You don’t have to use OpenAI\n6. Functions can be used instead of executing code\n7. Use Agents for organization, not just for conversations\n\nLastly, I’ll go over why I think you should use AutoGen to begin with. Let’s go!\n\n\n## Agents aren’t limited to communicating just over the terminal\n\nYou’ll see everyone demo AutoGen using a terminal or Jupyter Notebook. That’s nice for a demo, but there are other ways these agents can talk to each other.\n\nThere are 2 basic AutoGen classes: [`UserProxyAg`ent](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/user_proxy_agent.py) and [`AssistantAg`ent](https://github.com/microsoft/autogen/blob/main/autogen/agentchat/assistant_agent.py) . They inherit the [`ConversableAg`ent](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py) class, providing just a few different default parameters to the base class.\n\nWhen you see this classic code example:\n\n\n```python\nassistant = autogen.AssistantAgent(\n    name=\"assistant\",\n    llm_config=llm_config\n)\nuser_proxy = autogen.UserProxyAgent(name=\"user_proxy\")\nawait user_proxy.a_initiate_chat(\n    assistant,\n    message=\"\"\"What date is today? Compare the year-to-date gain for META and TESLA.\"\"\",\n)\n```\nwhat happens is that the `UserProxyAgent` will call its own `send` method, which will call `AssistantAgent` ‘s [`rece`ive](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L514) method, passing along the original message. A reply will be generated (more on that below), and `AssistantAgent` will now call its [`s`end](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L351) method, which will then call `UserProxyAgent` ‘s `receive` method, and so forth, until `UserProxyAgent` determines the conversation is terminated (which can be customized via the `is_termination_msg` argument).\n\nMy first “aha” moment was when I realized these agents were classes, and I could create my own custom agent classes that inherit the AutoGen UserProxy/Assistant/Conversable Agent classes, and override any of the default methods. That makes AutoGen very extensible.\n\nI had a use-case where I needed a human who could type in a message (proxied by `UserProxyAgent`) using a chat UI on a website, and I wanted an `AssistantAgent` to respond back to that chat in the UI, and be able to receive more messages from the human user, as though the human was just another agent in this AutoGen conversation.\n\nI could override the `send` and `receive` methods (or `a_send` and `a_receive`), and push/pull over http, websockets, etc. I tried this, and it started to work, but doesn’t scale. Let’s learn a better way.\n\n\n## Registering custom replies\n\nAutoGen has a plugin system that lets you customize how an agent generates a reply. We’re used to seeing examples where AutoGen queries OpenAI for an answer, and uses that as its reply, but you can insert your own methods as well:\n\n\n```python\nclass WeatherAgent(AssistantAgent):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, llm_config=False, **kwargs)\n        self.register_reply(Agent, WeatherAgent.get_weather)\n\n    async def get_weather(\n        self,\n        messages: List[Dict] = [],\n        sender=None,\n        config=None,\n    ) -> Tuple[bool, Union[str, Dict, None]]:\n        last_message = messages[-1][\"content\"]\n        result = await fetch_weather(last_message)\n        return True, result\n\nasync def fetch_weather(city: str) -> str:\n    async with httpx.AsyncClient() as client:\n        result = await client.post(\n            WEATHER_API_URL,\n            json={\"city\": question},\n        )\n        return result.json()\n\nweather_assistant = WeatherAgent(name=\"weather_assistant\")\nuser_proxy = autogen.UserProxyAgent(name=\"user_proxy\")\nawait user_proxy.a_initiate_chat(assistant, message=\"Lehi\")\nprint(weather_assistant.last_message)\n```\nHere, `register_reply` will insert my custom method for getting a reply, and by default, will put this method in `position=0`, meaning it will be the first reply method attempted. That method should return a tuple, where the first item is a boolean indicating if this reply is the one that should be used or whether to try the next registered\\_reply (such as the built-in reply generations using OpenAI — see the full order [here](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L145-L153)).\n\nKnowing about [`register_re`ply](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L155) allows you to customize how replies are retrieved, allow you to start sub multi-agent conversations, etc.\n\n\n## How to include real humans in the conversation in real ways\n\nHere’s one way to do it:\n\n\n```python\n## user makes a POST /query { \"message\": \"What's the weather?\" }\n\n@query_blueprint.route(\"/query\", methods=[\"POST\"])\nasync def post_query():\n  message = request.form.get(\"message\")\n\n  assistant = autogen.AssistantAgent(\n    name=\"assistant\",\n    llm_config=llm_config\n    system_message=\"\"\"You're a helpful assistant.\n    If you need more info, ask the user for anything missing.\"\"\"\n  )\n  user_proxy = autogen.UserProxyAgent(\n    name=\"user_proxy\",\n    human_input_mode=\"NEVER\",\n    code_execution_config=False,\n    is_termination_msg=lambda message: True # Always True\n  )\n  weather_assistant = WeatherAgent(\n    name=\"weather_assistant\",\n    system_message=\"\"\"You're a helpful assistant to get the weather.\n    You fetch weather information, then return it.\"\"\"\n  )\n\n  groupchat = autogen.GroupChat(\n    agents=[assistant, user_proxy, weather_assistant],\n    messages=[]\n  )\n  manager = autogen.GroupChatManager(\n    name=\"Manager\",\n    groupchat=groupchat,\n    llm_config=llm_config,\n  )\n\n  await user_proxy.a_initiate_chat(manager, message=message)\n\n  return groupchat.messages[-1]\n```\nWhat’s going on here?\n\n1. Anytime a message is sent to `user_proxy`, the conversation will end (we’ll resume it later). Why do this? This means the `user_proxy` can actually proxy for the user. Rather than try to answer, it will end the current conversation flow and allow the real human user to respond (by resuming the conversation — see below).\n2. If the assistant needs more info, it’ll ask user\\_proxy, which will end the current conversation.\n\nIn the above code, what is likely to occur is something like this:\n\n1. user\\_proxy -> manager: “What’s the weather?”\n2. assistant -> manager: “The user didn’t specify for which city.”\n3. manager -> user\\_proxy : conversation will end\n\nNow, if the user wants to respond and resume the conversation, how would we do that? There’s lots of ways to do this, here’s just a sample flavor:\n\n\n```python\n## user makes a POST /query { \"message\": \"What's the weather?\" }\n## above posts returns a `history` array\n## user makes a second POST /query { \"message\": \"What's the weather?\", \"history\": history }\n\nclass ResumableGroupChatManager(GroupChatManager):\n    groupchat: GroupChat\n\n    def __init__(self, groupchat, history, **kwargs):\n        self.groupchat = groupchat\n        if history:\n            self.groupchat.messages = history\n\n        super().__init__(groupchat, **kwargs)\n\n        if history:\n            self.restore_from_history(history)\n\n    def restore_from_history(self, history) -> None:\n        for message in history:\n            # broadcast the message to all agents except the speaker.  This idea is the same way GroupChat is implemented in AutoGen for new messages, this method simply allows us to replay old messages first.\n            for agent in self.groupchat.agents:\n                if agent != self:\n                    self.send(message, agent, request_reply=False, silent=True)\n\n@query_blueprint.route(\"/query\", methods=[\"POST\"])\nasync def post_query():\n  message = request.form.get(\"message\")\n\n  assistant = autogen.AssistantAgent(\n    name=\"assistant\",\n    llm_config=llm_config\n    system_message=\"\"\"You're a helpful assistant.\n    If you need more info, ask the user for anything missing.\"\"\"\n  )\n  user_proxy = autogen.UserProxyAgent(\n    name=\"user_proxy\",\n    human_input_mode=\"NEVER\",\n    code_execution_config=False,\n    is_termination_msg=lambda message: True # Always True\n  )\n  weather_assistant = WeatherAgent(\n    name=\"weather_assistant\",\n    system_message=\"\"\"You're a helpful assistant to get the weather.\n    You fetch weather information, then return it.\"\"\"\n  )\n\n  groupchat = autogen.GroupChat(\n    agents=[assistant, user_proxy, weather_assistant],\n    messages=[]\n  )\n  manager = ResumableGroupChatManager(\n    name=\"Manager\",\n    groupchat=groupchat,\n    llm_config=llm_config,\n  )\n\n  await user_proxy.a_initiate_chat(manager, message=message)\n\n  return {\n    \"response\": groupchat.messages[-1],\n    \"history\": groupchat.messages,\n  }\n```\nUsing this approach, you can now include humans as though they were just another agent in the groupchat. Anytime an assistant agent wants human input, they ask user\\_proxy, user\\_proxy then ends the current conversation, allowing the human user to respond with more information, then pick up the conversation where it left off.\n\nThe benefits to this approach are:\n\n* Conversations can include real human input via any means you want (such as over http or websocket).\n* The conversation is stopped while getting human input. This frees up the thread for other conversations and computation.\n* You can persist these conversations across sessions.\n\n\n## You can (and should) customize who speaks next\n\nThis is subjective, but I think you should always customize the way speakers are selected because:\n\n1. You’ll use less tokens (saves both $ and response time)\n2. You can separate the logic that decides who speaks next from the logic that defines the system instructions for each agent\n\n\n```python\nshort_role_descriptions = {\n  \"user_proxy\": \"A proxy for the user\",\n  \"weather_assistant\": \"You can get the weather\",\n  \"planner\": \"You help coordinate the plan. Your turn happens when XYZ, but skip your turn when ABC\"\n}\n\nclass CustomGroupChat(GroupChat):\n    # The default message uses the full system message, which is a long string.  We are overriding this to use a shorter message.\n    def select_speaker_msg(self, agents: List[Agent]):\n        message = f\"\"\"You are in a role play game. The following roles are available:\n        ---\n        {new_line.join([f\"{agent.name}: {short_role_descriptions[agent.name]}\" for agent in agents])}\n        ---\n\n        The role who plays next depends on the conversation.  User_Proxy will star the conversation, and typically Planner would go next.\n\n        Here are some examples\n        ---\n        ... not shown here ...\n        ---\n\n        Read the following conversation.\n        Then select the next role from {', '.join([agent.name for agent in agents])} to play. Only return the role.\"\"\"\n        return message\n```\n\n## You don’t have to use OpenAI\n\nAutoGen already notes you can use other LLMs, as long as they are “ChatGPT-like”, meaning their API responds with a similar shape and response as ChatGPT API calls.\n\nBut, remember how these agents are classes, and you can override most of the methods?\n\nTry overriding the method: [generate\\_oai\\_reply](https://github.com/microsoft/autogen/blob/40dbf31a925c725edb9124f4312c1703bf8744b0/autogen/agentchat/conversable_agent.py#L678), and you can query any LLM you’d like.\n\n\n## Functions can be used instead of executing code\n\nWhen I went to our security team and said “I’d like to use AutoGen for my service in Kubernetes. It needs to be able to execute any arbitrary code produced by an LLM. You’re ok with that, right?”\n\nOf course, the answer was a definite: NO.\n\nSo, why use AutoGen without the auto-code-execution abilities?\n\nOn top of the reasons stated below, another is that you can use function calling to gain total control over code execution. If you have a set of python functions you want to provide to AutoGen — functions you wrote, control, and can accept some safe parameters — that sounds like a better idea anyway than the wild west of allowing any and all code to be executed in your private infrastructure.\n\n\n## Use Agents for organization, not just for conversations\n\nMaybe you don’t have a need for an autonomous, multi-agent conversation. Maybe you just need to make a few different calls to an LLM.\n\nI still like the idea of having different “Agents” just for the sake of organization. Here’s a really crazy idea, but take it for what it’s worth:\n\n\n```python\nanalyst = autogen.AssistantAgent(\n    name=\"Analyst\",\n    system_message=\"\"\"Your an analyst.  You do XYZ.\"\"\",\n    llm_config=llm_config,\n)\n\nsummarizer = autogen.AssistantAgent(\n    name=\"Summarizer\",\n    system_message=\"\"\"Your a summarizer.  You do XYZ.\"\"\",\n    llm_config=llm_config,\n)\n\nreport = \"\"\"Some long report\"\"\"\n\nanalysis = analyst.generate_oai_reply(report)[1]\nsummary = summarizer.generate_oai_reply(report)[1]\n\nprint(f\"Analysis: {analysis}\")\nprint(f\"Summary: {summary}\")\n```\n\n## Why use AutoGen?\n\n1. AutoGen allows multiple agents, with different system prompts and instructions, to solve a problem. Just like in real-life, different perspectives working together will solve a problem better than a single brain.\n2. AutoGen GroupChat is amazing. It provides routing to the right experts (agents), and it allows a conversation to continue autonomously until the problem is solved. Some conversations will go from agent: a->b->c->d, others will be b->a->d->c. This allows AutoGen to solve a variety of different problems without needing explicit rules for each scenario.\n3. AutoGen can recover from mistakes. For example, I made an AutoGen-powered service that made API calls to a service. Sometimes, the API calls errored out because it didn’t send the right data at first. The AutoGen GroupChat kept trying different things until it succeeded. Sometimes, it took 4+ attempts, but my Planner agent didn’t give up — just pivoted autonomously to handle the API failures and try new things.\n4. AutoGen came up with the concept of separating `UserProxyAgent`s from `AssistantAgent` s from the beginning. This also allows us to let the user proxy actually proxy for the user, as shown above.\n5. AutoGen is a well maintained library. Every week they’re adding something new.\n6. AutoGen is very extensible. With the way they’ve built their classes, you can customize anything to your liking.\n7. AutoGen has other features I don’t use, but others may find them helpful, such as helping you count tokens and cost of conversations, cacheing, etc.\n\n"},{"lang":"en","group":"blog","slug":"blog/building-a-local-ai-powered-news-aggregator-with-ollama-swarm-and-duckduckgo-95aaf8b3ee41","frontmatter":{"title":"Building a Local AI-Powered News Aggregator with Ollama, Swarm, and DuckDuckGo","meta_title":"Building a Local AI-Powered News Aggregator with Ollama, Swarm, and DuckDuckGo","description":"No subtitle provided","date":"2024-10-23T09:10:26.000Z","image":"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*OHMOTk_WYGOxWHBsKqdpNQ.jpeg","categories":["agents"],"author":"Rifx.Online","tags":["agents"],"draft":false,"slug":"blog/building-a-local-ai-powered-news-aggregator-with-ollama-swarm-and-duckduckgo-95aaf8b3ee41"},"content":"\n\n\n\n\n\nIn today’s fast-paced world, staying up-to-date with the latest news in specific fields can be challenging. What if we could leverage the power of Generative AI and Agents to create a personalized news aggregator that runs entirely on our local machine? In this article, we’ll explore how to build such a system using **Ollama**’s Llama 3.2 model, **Swarm** for agent orchestration, and **DuckDuckGo** for web searches.\n\n\n## The Power of Local AI\n\nWith the rise of large language models, we now have the ability to run sophisticated AI systems on our personal computers. This opens up a world of possibilities for creating customized tools tailored to our specific needs. Our news aggregator is a perfect example of this potential.\n\n\n## Components of Our System\n\n1. **Ollama with Llama 3.2**: This serves as the brain of our system, powering our AI agents.\n2. **Swarm**: An agent orchestration framework that allows us to create and manage multiple AI agents.\n3. **DuckDuckGo Search**: Provides up-to-date web search results without tracking user data.\n\n\n## How It Works\n\nOur news aggregator consists of two main AI agents:\n\n1. **News Assistant**: Fetches the latest news articles on a given topic using DuckDuckGo search.\n2. **Editor Assistant**: Reviews and refines the collected news for final presentation.\n\nLet’s break down the workflow:\n\n\n## 1. Setting Up the Environment\n\n\n```python\nollama pull llama3.2\n\nexport OPENAI_MODEL_NAME=llama3.2\nexport OPENAI_BASE_URL=http://localhost:11434/v1\nexport OPENAI_API_KEY=any\n\npip install git+https://github.com/openai/swarm.git duckduckgo-search\n```\nWe start by importing the necessary libraries and initializing our Swarm client:\n\n\n```python\nfrom duckduckgo_search import DDGS\nfrom swarm import Swarm, Agent\nfrom datetime import datetime\n\ncurrent_date = datetime.now().strftime(\"%Y-%m\")\nclient = Swarm()\n```\n\n## 2. Creating the News Search Function\n\nWe define a function to search for news using DuckDuckGo:\n\n\n```python\npythondef get_news_articles(topic):\n  ddg_api = DDGS()\n  results = ddg_api.text(f\"{topic} {current_date}\", max_results=5)\n  if results:\n      news_results = \"\\n\\n\".join([f\"Title: {result['title']}\\nURL: {result['href']}\\nDescription: {result['body']}\" for result in results])\n      return news_results\n  else:\n      return f\"Could not find news results for {topic}.\"\n```\n\n## 3. Defining Our AI Agents\n\nWe create two agents using Ollama’s Llama 3.2 model:\n\n\n```python\nnews_agent = Agent(\n  model=\"llama3.2\",\n  name=\"News Assistant\",\n  instructions=\"You provide the latest news articles for a given topic using DuckDuckGo search.\",\n  functions=[get_news_articles],\n)\n\neditor_agent = Agent(\n  model=\"llama3.2\",\n  name=\"Editor Assistant\",\n  instructions=\"You review and finalise the news article for publishing.\",\n)\n```\n\n## 4. Orchestrating the Workflow\n\nWe define a function to run our news aggregation workflow:\n\n\n```python\ndef run_news_workflow(topic):\n  # Fetch news\n  news_response = client.run(\n      agent=news_agent,\n      messages=[{\"role\": \"user\", \"content\": f\"Get me the news about {topic} on {current_date}\"}],\n  )\n  raw_news = news_response.messages[-1][\"content\"]\n  \n  # Pass news to editor for final review\n  edited_news_response = client.run(\n      agent=editor_agent,\n      messages=[{\"role\": \"system\", \"content\": raw_news}],\n  )\n  print(f\"{edited_news_response.messages[-1]['content']}\")\n```\n\n## 5. Running the System\n\nFinally, we can run our news aggregator for any topic of interest:\n\n\n```python\nrun_news_workflow(\"AI in Drug Discovery\")\n```\n\n## Complete Code : app.py\n\n\n```python\nfrom duckduckgo_search import DDGS\nfrom swarm import Swarm, Agent\nfrom datetime import datetime\n\ncurrent_date = datetime.now().strftime(\"%Y-%m\")\n\n## Initialize Swarm client\nclient = Swarm()\n\n## 1. Create Internet Search Tool\n\ndef get_news_articles(topic):\n    print(f\"Running DuckDuckGo news search for {topic}...\")\n    \n    # DuckDuckGo search\n    ddg_api = DDGS()\n    results = ddg_api.text(f\"{topic} {current_date}\", max_results=5)\n    if results:\n        news_results = \"\\n\\n\".join([f\"Title: {result['title']}\\nURL: {result['href']}\\nDescription: {result['body']}\" for result in results])\n        return news_results\n    else:\n        return f\"Could not find news results for {topic}.\"\n    \n## 2. Create AI Agents\n\ndef transfer_to_editor_assistant(raw_news):\n    print(\"Passing articles to Editor Assistant...\")\n    return editor_agent.run({\"role\": \"system\", \"content\": raw_news})\n\n## News Agent to fetch news\nnews_agent = Agent(\n    model=\"llama3.2\",\n    name=\"News Assistant\",\n    instructions=\"You provide the latest news articles for a given topic using DuckDuckGo search.\",\n    functions=[get_news_articles],\n)\n\n## Editor Agent to edit news\neditor_agent = Agent(\n    model=\"llama3.2\",\n    name=\"Editor Assistant\",\n    instructions=\"You review and finalise the news article for publishing.\",\n)\n\n## 3. Create workflow\n\ndef run_news_workflow(topic):\n    print(\"Running news Agent workflow...\")\n    \n    # Step 1: Fetch news\n    news_response = client.run(\n        agent=news_agent,\n        messages=[{\"role\": \"user\", \"content\": f\"Get me the news about {topic} on {current_date}\"}],\n    )\n    raw_news = news_response.messages[-1][\"content\"]\n    print(f\"Fetched news: {raw_news}\")\n    \n    # Step 2: Pass news to editor for final review\n    edited_news_response = client.run(\n        agent=editor_agent,\n        messages=[{\"role\": \"system\", \"content\": raw_news}],\n    )\n    print(f\"{edited_news_response.messages[-1]['content']}\")\n\n\n## Example of running the news workflow for a given topic\nrun_news_workflow(\"AI in Drug Discovery\")\n```\n\n## Sample Output\n\n\n```python\nRunning news Agent workflow...\nRunning DuckDuckGo news search for AI in Drug Discovery...\nFetched news: Here's a formatted answer based on the news articles:\n\n**AI in Drug Discovery: A Revolutionary Shift**\n\nThe role of Artificial Intelligence (AI) in drug discovery has marked a revolutionary shift in the pharmaceutical landscape. AI leverages sophisticated algorithms for autonomous decision-making from data analysis, augmenting human capabilities rather than replacing them.\n\n**Challenges and Limitations**\n\nDespite the promising advancements, challenges and limitations have been identified in the field. The paper \"The Role of AI in Drug Discovery\" addresses these issues, highlighting the need for high-quality data, addressing ethical concerns, and recognizing the limitations of AI-based approaches.\n\n**Applications of AI in Drug Discovery**\n\nAI has the potential to play a critical role in drug discovery, design, and studying drug-drug interactions.Applications of AI in drug discovery include:\n\n* Polypharmacology: AI can predict the likelihood of a compound's effectiveness against multiple diseases.\n* Chemical synthesis: AI can optimize chemical synthesis processes for faster and more efficient production.\n* Drug repurposing: AI can identify new uses for existing drugs.\n* Predicting drug properties: AI can predict the efficacy, toxicity, and physicochemical characteristics of compounds.\n\n**The Future of AI in Drug Discovery**\n\nAs AI continues to evolve, it is expected to significantly impact the pharmaceutical industry. The successful application of AI will depend on the availability of high-quality data, addressing ethical concerns, and recognizing the limitations of AI-based approaches.\n```\n\n## The Benefits of Local AI News Aggregation\n\n* **Privacy**: All processing happens on your local machine, ensuring your data stays with you.\n* **Customization**: You can easily modify the agents’ instructions or add new agents to suit your specific needs.\n* **Up-to-date Information**: By using DuckDuckGo search, you always get the latest news on your chosen topic.\n* **AI-powered Curation**: The Editor Assistant helps refine and organize the collected news, providing a more polished final output.\n\n\n## Conclusion\n\nThis local AI-powered news aggregator demonstrates the potential of combining large language models with web search capabilities. By leveraging Ollama’s Llama 3.2 model, Swarm for agent orchestration, and DuckDuckGo for search, we’ve created a powerful tool that can keep us informed on any topic of interest, all while maintaining our privacy and running entirely on our local machine.\n\nAs AI continues to evolve, the possibilities for creating personalized, AI-driven tools will only expand. This news aggregator is just the beginning — imagine what other innovative applications you could build using these technologies!\n\n\n## Reference :\n\nSwarm Github : <https://github.com/openai/swarm>\n\nIf you found this article informative and valuable, I’d greatly appreciate your support:\n\n* Give it a few claps 👏 on Medium to help others discover this content (did you know you can clap up to 50 times?). Your claps will help spread the knowledge to more readers.\n- Share it with your network of AI enthusiasts and professionals.\n- Connect with me on LinkedIn: <https://www.linkedin.com/in/manjunath-janardhan-54a5537/>\n\n\n\n\n"},{"lang":"en","group":"blog","slug":"blog/explore-swarm-multi-agent-framework-locally-0e25ee617795","frontmatter":{"title":"Explore Swarm Multi-Agent Framework Locally","meta_title":"Explore Swarm Multi-Agent Framework Locally","description":"Swarm is an experimental sample framework to simulate lightweight multi-agent framework for educational purpose. Usually it works with Open…","date":"2024-10-23T11:46:57.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*0ZVceq32bvkytC7HSIgmwA.png","categories":["agents"],"author":"Rifx.Online","tags":["agents"],"draft":false,"slug":"blog/explore-swarm-multi-agent-framework-locally-0e25ee617795"},"content":"\n\n\n\n\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*zkpW8DDwh0TTYuHJVJbDaw.png)\n\nSwarm is an experimental sample framework to simulate lightweight multi-agent framework for educational purpose. Usually it works with Open AI Key but we can change it to use local Ollama or LM Studio Models.\n\n**Setup:**\n\n\n```python\n## Create a new Conda or Python Virtual Environment and activate it\nconda install python==3.10\npip install torch openai\npip install transformers accelerate huggingface_hub\npip install git+ssh://git@github.com/openai/swarm.git\n```\n**To use with Open AI Key:**\n\n\n```python\nexport OPEN_API_KEY = Your Key\n```\n**To use Ollama or LM Studio Local LLMs — Update to Local URL:**\n\n\n```python\n## Find the location site-packages/swarm on the conda or python virtual env\n## Locate the file core.py\nclass Swarm:\n    def __init__(self, client=None):\n        if not client:\n          # Actual Code\n          #client = OpenAI()\n          # Update the Base URL and API Key to Ollama / LM Studio\n          # In this demo we are using LM Studio and Llama 3.1\n          client = OpenAI(base_url=\"http://localhost:1234/v1\",api_key=\"random\")\n        self.client = client\n```\n**Clone Repo:**\n\nClone the Repo — where you can find examples directory with different use cases like basic, airline and weather etc.\n\n\n```python\ngit clone https://github.com/openai/swarm.git\ncd swarm/examples\n```\n**Sample Code:**\n\n\n```python\nfrom swarm import Swarm, Agent\n\nclient = Swarm()\n\n\nit_agent = Agent(\n    name=\"IT Agent\",\n    instructions=\"You are an IT Expert with 10 Years of Experience.\",\n)\n\nsales_agent = Agent(\n    name=\"Sales Agent\",\n    instructions=\"You are a Sales Expert with 5 Years of Experience and knows about best selling mobiles.\",\n)\n\ndef transfer_to_sales_agent():\n    print(\"Sales agent in action\")\n    \"\"\"Transfer sales related questions to sales team immediately.\"\"\"\n    return sales_agent\n\ndef transfer_to_it_agent():\n    print(\"IT agent in action\")\n    \"\"\"Transfer IT users immediately.\"\"\"\n    return it_agent\n\nenglish_agent = Agent(\n    name=\"English Agent\",\n    instructions=\"You only speak English.\",\n    functions=[transfer_to_sales_agent,transfer_to_it_agent],\n)\n\n\nmessages = [{\"role\": \"user\", \"content\": \"How to install pandas lib?\"}]\nresponse = client.run(agent=english_agent, messages=messages)\n\nprint(response.messages[-1][\"content\"])\n\nmessages = [{\"role\": \"user\", \"content\": \"What are the best selling items?\"}]\nresponse = client.run(agent=english_agent, messages=messages)\n\nprint(response.messages[-1][\"content\"])\n```\n**References:**\n\n\n```python\nhttps://github.com/openai/swarm\n\nhttps://github.com/victorb/ollama-swarm/tree/main\n```\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*hCFJ4VQoT12yElYPXwXvWA.png)\n\nGiven that it is an experimental release, there is still much room for improvement. The airline agent example code [swarm/examples/airline] was interesting, so try those examples. Give it a try and share your experience in the comments. Thanks.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/generating-structured-data-from-an-image-with-gpt-vision-and-langchain-34aaf3dcb215","frontmatter":{"title":"Generating structured data from an image with GPT vision and Langchain","meta_title":"Generating structured data from an image with GPT vision and Langchain","description":"In today’s world, where visual data is abundant, the ability to extract meaningful information from images is becoming increasingly…","date":"2024-10-23T11:52:52.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*FPRRg85jYb7MrzXEpNWbmw.jpeg","categories":["llm"],"author":"Rifx.Online","tags":["llm"],"draft":false,"slug":"blog/generating-structured-data-from-an-image-with-gpt-vision-and-langchain-34aaf3dcb215"},"content":"\n\n\n\n\n\nIn today’s world, where visual data is abundant, the ability to extract meaningful information from images is becoming increasingly valuable. Langchain, a powerful framework for building applications with large language models (LLMs), offers a versatile toolset for tackling this challenge. In this article, we’ll explore how to use Langchain to extract structured information from images, such as counting the number of people and listing the main objects.\n\nBefore diving into the code, let’s set the stage by understanding the task at hand. Imagine you have an image of a scene, such as a city street. Your goal is to extract valuable information from this image, including the number of people present and a list of the main objects in the scene.\n\n\n## About Langchain\n\nLangchain is a comprehensive framework that allows developers to build sophisticated applications by leveraging the power of large language models (LLMs). It provides a modular and extensible architecture, enabling developers to create custom pipelines, agents, and workflows tailored to their specific needs.\n\nLangchain simplifies the integration of LLMs, offering abstractions and utilities for handling various data sources, including text, images, and structured data. It supports a wide range of LLMs from different providers, such as OpenAI and Anthropic, making it easy to switch between models or combine multiple models in a single application.\n\n\n## Preparing the Environment and Setting Up the OpenAI API Key\n\nTo follow along with this tutorial, you’ll need to have Langchain installed. You can install it using pip:\n\n\n```python\npip install langchain langchain_openai\n```\nTo use the OpenAI language models with Langchain, you’ll need to obtain an API key from OpenAI. If you don’t have an API key yet, you can sign up for one on the OpenAI website (<https://openai.com/api/>).\n\nOnce you have your API key, you can set it as an environment variable in your system or provide it directly in your code. Here’s an example of how to set the API key as an environment variableCopy code\n\n\n```python\nexport OPENAI_API_KEY=\"your_openai_api_key_here\"\n```\nAlternatively, you can provide the API key directly in your Python code:\n\n\n```python\nimport os\nimport langchain\nos.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key_here\"\n```\nAfter setting up the API key, Langchain will be able to authenticate with the OpenAI API and use their language models.\n\n\n## Loading and Encoding the Image\n\nBefore we can process images with Langchain, we need to load the image data from a file and encode it in a format that can be passed to the language model. The code below defines a function `load_image` that takes a dictionary with an `image_path` key and returns a new dictionary with an `image` key containing the image data encoded as a base64 string.\n\n\n```python\ndef load_image(inputs: dict) -> dict:\n    \"\"\"Load image from file and encode it as base64.\"\"\"\n    image_path = inputs[\"image_path\"]\n  \n    def encode_image(image_path):\n        with open(image_path, \"rb\") as image_file:\n            return base64.b64encode(image_file.read()).decode('utf-8')\n    image_base64 = encode_image(image_path)\n    return {\"image\": image_base64}\n```\nThe `load_image` function first extracts the `image_path` from the input dictionary. It then defines a nested function `encode_image` that opens the image file in binary mode, reads its contents, and encodes them as a base64 string using the `base64.b64encode` function from the Python standard library.\n\nThe `load_image` function calls `encode_image` with the provided `image_path` and stores the resulting base64-encoded string in the `image_base64` variable. Finally, it returns a new dictionary with the `image` key set to `image_base64`.\n\nTo integrate this function into a Langchain pipeline, we can create a `TransformChain` that takes the `image_path` as input and produces the `image` (base64-encoded string) as outputCopy code\n\n\n```python\nload_image_chain = TransformChain(\n    input_variables=[\"image_path\"],\n    output_variables=[\"image\"],\n    transform=load_image\n)\n```\nWith this setup, we can easily load and encode images as part of a larger Langchain workflow, enabling us to process visual data alongside text using large language models.\n\n\n## Defining the Output Structure\n\nBefore we can extract information from the image, we need to define the structure of the output we want to receive. In this case, we’ll create a Pydantic model called `ImageInformation` that includes fields for the image description and any additional information we might want to extract.\n\n\n```python\nfrom langchain_core.pydantic_v1 import BaseModel, Field\n\nclass ImageInformation(BaseModel):\n \"\"\"Information about an image.\"\"\"\n image_description: str = Field(description=\"a short description of the image\")\n people_count: int = Field(description=\"number of humans on the picture\")\n main_objects: list[str] = Field(description=\"list of the main objects on the picture\")\n```\n\n## Setting up the Image Model\n\nNext, we’ll create a chain that combines the image loading and encoding steps with the LLM invocation step. Since the `ChatOpenAI` model is not natively capable of handling both text and image inputs simultaneously (to my unsderstanding), we'll create a wrapper chain to achieve this functionality.\n\n\n```python\nfrom langchain.chains import TransformChain\nfrom langchain_core.messages import HumanMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain import globals\nfrom langchain_core.runnables import chain\n\n## Set verbose\nglobals.set_debug(True)\n\n@chain\ndef image_model(inputs: dict) -> str | list[str] | dict:\n \"\"\"Invoke model with image and prompt.\"\"\"\n model = ChatOpenAI(temperature=0.5, model=\"gpt-4-vision-preview\", max_tokens=1024)\n msg = model.invoke(\n             [HumanMessage(\n             content=[\n             {\"type\": \"text\", \"text\": inputs[\"prompt\"]},\n             {\"type\": \"text\", \"text\": parser.get_format_instructions()},\n             {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{inputs['image']}\"}},\n             ])]\n             )\n return msg.content\n```\nIn this code snippet, we define a chain called `image_model` that invokes the `ChatOpenAI` model with the provided prompt, format instructions, and image. The `image_model` chain accepts a dictionary `inputs` containing the prompt and the base64-encoded image string.\n\nInside the chain, we create a `HumanMessage` object that combines the prompt text, format instructions, and the image URL, formatted as a data URI with the base64-encoded image data. We then invoke the `ChatOpenAI` model with this `HumanMessage` object, using the `gpt-4-vision-preview` model, which is specifically designed for multimodal tasks involving both text and images.\n\nThe model processes both the text prompt and the image, and returns the output.\n\n\n## Putting It All Together\n\nNow that we have all the necessary components, we can define a function that orchestrates the entire process:\n\n\n```python\nfrom langchain_core.output_parsers import JsonOutputParser\n\nparser = JsonOutputParser(pydantic_object=ImageInformation)\ndef get_image_informations(image_path: str) -> dict:\n   vision_prompt = \"\"\"\n   Given the image, provide the following information:\n   - A count of how many people are in the image\n   - A list of the main objects present in the image\n   - A description of the image\n   \"\"\"\n   vision_chain = load_image_chain | image_model | parser\n   return vision_chain.invoke({'image_path': f'{image_path}', \n                               'prompt': vision_prompt})\n```\nIn this function, we define a prompt that asks the LLM to provide a count of the people in the image and a list of the main objects. We then create a chain that combines the image loading step (`load\\_image\\_chain`), the LLM invocation step (`image\\_model`), and a JSON output parser (`parser`). Finally, we invoke this chain with the image path and the prompt, and the function returns a dictionary containing the extracted information.\n\n\n## Example Usage\n\nTo use this function, simply provide the path to an image file:\n\n\n```python\nresult = get_image_informations(\"path/to/your/image.jpg\")\nprint(result)\n```\nThis will output a dictionary with the requested information, such as:\n\n\n```python\n{\n 'description': 'a view of a city showing cars waiting at a traffic light',\n 'people_count': 5,\n 'main_objects': ['car', 'building', 'traffic light', 'tree']\n}\n```\n\n## Conclusion\n\nLangchain provides a powerful toolset for working with large language models and extracting valuable information from various data sources, including images. By combining Langchain’s capabilities with custom prompts and output parsing, you can create robust applications that can extract structured information from visual data.\n\nRemember, the quality of the output will depend on the capabilities of the LLM you’re using and the specificity of your prompts. Experiment with different models and prompts to find the best solution for your use case.\n\nIf you find a better way to achieve the same results or have suggestions for improvements, please don’t hesitate to share them in the comments. The code examples provided in this article are meant to serve as a starting point, and there may be alternative approaches or optimizations .\n\n\n"},{"lang":"en","group":"blog","slug":"blog/how-to-run-nvidia-llama-3-1-nemotron-70b-instruct-locally-a58ad283aaff","frontmatter":{"title":"How to Run Nvidia’ llama-3.1-nemotron-70b-instruct Locally","meta_title":"How to Run Nvidia’ llama-3.1-nemotron-70b-instruct Locally","description":"Running large language models (LLMs) locally has become increasingly popular among developers, researchers, and AI enthusiasts. One such…","date":"2024-10-23T11:49:51.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*fqVKJkw5sQvLtIsyCcengQ.png","categories":["large-language-models"],"author":"Rifx.Online","tags":["large-language-models"],"draft":false,"slug":"blog/how-to-run-nvidia-llama-3-1-nemotron-70b-instruct-locally-a58ad283aaff"},"content":"\n\n\n\nRunning large language models (LLMs) locally has become increasingly popular among developers, researchers, and AI enthusiasts. One such model that has gained significant attention is the llama-3.1-nemotron-70b-instruct, a powerful LLM customized by NVIDIA to enhance the helpfulness of generated responses. In this comprehensive guide, we’ll explore multiple methods to run this model on your local machine, starting with the user-friendly Ollama platform.\n\n\n> Before we get started, If you are seeking an All-in-One AI platform that manages all your AI subscriptions in one place, including all LLMs (such as GPT-o1, Llama 3.1, Claude 3.5 Sonnet, Google Gemini, Uncensored LLMs) and Image Generation Models (FLUX, Stable Diffusion, etc.), Use Anakin AI to manage them all!\n\n\n\n\n## Method 1: Run llama-3.1-nemotron-70b-instruct Locally with Ollama\n\nOllama is an excellent tool for running LLMs locally, offering a straightforward setup process and support for various models, including llama-3.1-nemotron-70b-instruct.\n\n\n### Installation\n\n1. Visit the official Ollama website ([https://ollama.ai](https://ollama.ai/)) and download the appropriate version for your operating system.\n2. Install Ollama by running the following command in your terminal:\n\n\n```python\ncurl https://ollama.ai/install.sh | sh\n```\n\n### Running llama-3.1-nemotron\n\nOnce Ollama is installed, you can easily run the llama-3.1-nemotron-70b-instruct model with a simple command:\n\n\n```python\nollama run nemotron:70b-instruct-q5_K_M\n```\nThis command will download the model if it’s not already present on your system and start an interactive session.\n\n\n### Using the Model\n\nAfter the model is loaded, you can start interacting with it by typing your prompts. For example:\n\n\n```python\n>>> What are the key features of llama-3.1-nemotron-70b-instruct?\n\nLlama-3.1-Nemotron-70B-Instruct is a large language model with several key features:\n1. Customized by NVIDIA: The model has been fine-tuned by NVIDIA to improve the helpfulness and quality of its responses.\n2. Based on Llama 3.1: It builds upon the Llama 3.1 architecture, which is known for its strong performance across various tasks.\n3. 70 billion parameters: This large parameter count allows for complex reasoning and a wide range of capabilities.\n4. Instruct-tuned: The model is specifically designed to follow instructions and generate helpful responses to user queries.\n5. RLHF training: It has been trained using Reinforcement Learning from Human Feedback, specifically the REINFORCE algorithm.\n6. Specialized reward model: The training process utilized Llama-3.1-Nemotron-70B-Reward for optimization.\n7. HelpSteer2-Preference prompts: These were used during the training process to further improve the model's helpfulness.\n8. Extended context length: Like other Llama 3.1 models, it likely supports a longer context window of 128K tokens.\n9. Multilingual capabilities: It can understand and generate text in multiple languages.\n10. Strong reasoning abilities: The model excels in tasks requiring complex reasoning and problem-solving.\nThese features make llama-3.1-nemotron-70b-instruct a powerful and versatile language model suitable for a wide range of applications, from general conversation to specialized tasks in various domains.\n```\nFor more advanced use cases, you can integrate Ollama with Python using libraries like Langchain. Here’s a simple example:\n\n\n```python\npython\n\nfrom langchain.llms import Ollama\n\nollama = Ollama(base_url=\"http://localhost:11434\", model=\"nemotron:70b-instruct-q5_K_M\")\nresponse = ollama.generate(\"Explain the concept of quantum entanglement.\")\nprint(response)\n```\nThis allows you to incorporate the model into your Python projects and applications seamlessly.\n\n\n## Method 2: Using llama.cpp\n\nllama.cpp is a popular C++ implementation of the Llama model inference, optimized for CPU usage. While it may require more setup than Ollama, it offers greater flexibility and control over the model’s parameters.\n\n\n### Installation\n\n1. Clone the llama.cpp repository:\n\n\n```python\ngit clone https://github.com/ggerganov/llama.cpp.git\ncd llama.cpp\n```\n1. Build the project:\n\n\n```python\nmake\n```\n\n### Downloading the Model\n\nTo run llama-3.1-nemotron-70b-instruct, you’ll need to download the model weights. These are typically available in GGML or GGUF format. You can find pre-converted models on platforms like Hugging Face.\n\n\n```python\nmkdir models\ncd models\nwget https://huggingface.co/TheBloke/Llama-3.1-Nemotron-70B-Instruct-GGUF/resolve/main/llama-3.1-nemotron-70b-instruct.Q4_K_M.gguf\n```\n\n### Running the Model\n\nOnce you have the model file, you can run it using the following command:\n\n\n```python\n./main -m models/llama-3.1-nemotron-70b-instruct.Q4_K_M.gguf -n 1024 -p \"Hello, how are you today?\"\n```\nThis command loads the model and generates a response to the given prompt. You can adjust various parameters like the number of tokens to generate (-n) or the temperature to control randomness.\n\n\n## Method 3: Using Hugging Face Transformers\n\nHugging Face’s Transformers library provides a high-level API for working with various language models, including llama-3.1-nemotron-70b-instruct.\n\n**Installation**\n\nFirst, install the necessary libraries:\n\n\n```python\npip install transformers torch accelerate\n```\n**Running the Model**\n\nHere’s a Python script to load and use the model:\n\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nmodel_name = \"meta-llama/Llama-3.1-Nemotron-70b-instruct\"\n## Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n## Prepare the input\nprompt = \"Explain the concept of quantum computing in simple terms.\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n## Generate the response\nwith torch.no_grad():\n    outputs = model.generate(**inputs, max_new_tokens=100)\n## Decode and print the response\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response)\n```\nThis method allows for more fine-grained control over the model’s behavior and integration with other Hugging Face tools and pipelines.\n\n\n## Conclusion\n\nRunning llama-3.1-nemotron-70b-instruct locally opens up a world of possibilities for developers and researchers. Whether you choose the simplicity of Ollama, the flexibility of llama.cpp, or the integration capabilities of Hugging Face Transformers, you now have the tools to harness the power of this advanced language model on your own hardware.As you explore the capabilities of llama-3.1-nemotron-70b-instruct, remember to balance performance with resource constraints, and always consider the ethical implications of your applications. With responsible use, this model can be a valuable asset in pushing the boundaries of what’s possible in natural language processing and AI-driven applications.\n\n\n"},{"lang":"en","group":"blog","slug":"blog/key-points-llm-quantization-chatgpt-artificial-intelligence-8201ffcb33d4","frontmatter":{"title":"5 Key Points to Unlock LLM Quantization","meta_title":"5 Key Points to Unlock LLM Quantization","description":"Quantizing Large Language Models","date":"2024-10-23T11:48:53.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*RUqPEr2NTYXlI1omqF22Qg.png","categories":["large-language-models"],"author":"Rifx.Online","tags":["large-language-models"],"draft":false,"slug":"blog/key-points-llm-quantization-chatgpt-artificial-intelligence-8201ffcb33d4"},"content":"\n\n\n\n\n### Quantizing Large Language Models\n\n\n\nLLM Quantization is currently a hot topic due to its vital role in making Large Language Models (LLMs) more efficient and deployable across various hardware platforms, including consumer-grade devices.\n\nBy adjusting the precision of certain components within the model, **quantization significantly reduces the model’s memory footprint** while maintaining similar performance levels.\n\nIn this guide, we will explore five key aspects of LLM quantization including some practical steps for applying this technique to our models.\n\n\n## #1. Understanding Quantization\n\nQuantization is a model compression technique that reduces the precision of weights and activations in an LLM. This involves converting high-precision values to lower-precision ones, effectively **changing data types that store more information to those that store less**.\n\nDecreasing the number of bits needed for each weight or activation significantly reduces the overall model size. As a result, **quantization creates LLMs that use less memory, and require less storage space.**\n\nThis technique has become essential in response to the exponential growth in the number of parameters in successive iterations of LLMs. For example, for the OpenAI’s GPT family, we can observe the growing trend in the following graph:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*QlAhma3Wu1F6w2WvkE8jDA.png)\n\nThis significant increase presents a challenge: as models grow, their memory requirements often exceed the capacity of advanced hardware accelerators such as GPUs. **This requires distributed training and inference to manage these models, which in turn limits their deployability.**\n\n\n## #2. Intuition Behind Quantization\n\nAlthough the definition of quantization may seem rather complex, the concept can be intuitively explained using matrices.\n\nLet’s consider the following a 3x3 matrix representing the weights of a neural network. The matrix on the left shows the original weights, while the matrix on the right shows the quantized version of these weights:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*LPzWe9oxjlDYdSp7dVvRUg.png)\n\nIn this simple example, we round the elements of the original matrix from four decimal places to a single decimal place. Although the matrices appear similar, **the storage space required for the four-decimal version is significantly higher**.\n\nIn practice, quantization is not merely a rounding operation. Instead, it involves converting numerical values to a different data type, typically from a higher to a lower precision one.\n\nFor example, the default data type for most models is `float32`, which requires 4 bytes per parameter (32 bits). Therefore, for a 3x3 matrix, the total memory footprint is 36 bytes. Changing the data type to `int8`, only 1 byte per parameter is needed, reducing the total memory footprint of the matrix to just 9 bytes.\n\n\n## #3. Quantization Error\n\nAs we have seen, the original matrix and its quantized form are not completely equal, but very similar. The value-by-value difference is known as “Quantization error”, which we can also represent in matrix form:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*VtGDjVbr7daagLXB57i7Mg.png)\n\n**This quantization error can accumulate for each matrix of weights in the network, affecting the model’s performance as a result.**\n\nCurrent research in quantization aims to minimize the difference in precision while decreasing the computational resources required to train or run inference on models, while maintaining acceptable performance levels.\n\n\n## #4. Linear Quantization\n\nLinear quantization is one of the most popular quantization schemes for LLMs. In simple terms, it involves mapping the range of floating-point values of the original weights to a range of fixed-point values.\n\nLet’s review the steps required to apply linear quantization to our models:\n\n* **Get the minimum and maximum ranges:** We need to get the minimum and maximum values of the floating-point weights to be quantized (`x_min` and `x_max`). We also need to define the quantized range (`q_min` and `q_max`), which is already set by the data type we want to convert to.\n* **Compute the scale (`s`) and the zero-point (`z`) values:** Firstly, the scale (`s`) adjusts the range of floating-point values to fit within the integer range, preserving the data distribution and range. Secondly, the zero-point (`z`) ensures that zero in the floating-point range is accurately represented by an integer, maintaining numerical accuracy and stability, especially for values close to zero.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BepC6-izw0yE19ejsS705Q.png)\n\n* **Quantize the values (`q`)**: We need to map the original floating-point values to the integer range using a scale factor (`s`) and a zero point (`z`) computed in the previous step.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*BBOQ0VbSGbwf7CN8c4PWKQ.png)\n\nApplying these formulas is quite straightforward. If we apply them to the 3x3 weight tensor on the left in the image below, we will get the quantized matrix shown on the right:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*KzBvg84mfI2gAhTIyVibwQ.png)\n\nWe can see that the lower bound of the `int8` value corresponds to the lower value of the original tensor, while the upper bound corresponds to the higher value of the original tensor, *i.e., the mapping is`0.50 → 255` and `-0.40 → 0`.*\n\nWe can now dequantize the values using the formula below.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*E5nnqYzncYCRuM5prssuOw.png)\n\nIf we place the dequantized values again in matrix form (matrix on the left), we can compute the quantization error (matrix on the right) by calculating the point-by-point difference between the original matrix and its dequantized version:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*56NALu9PAN95QG2hn8HXoQ.png)\n\nAs we can observe, the quantization error starts kicking in for some of the matrix values.\n\n\n## #5. Weight Quantization vs Activation Quantization\n\nIn our example above, we have focused primarily on quantizing the weights of the model. While weight quantization is crucial for model optimization, it’s also important to consider that activations can be quantized as well.\n\n**Activation quantization involves reducing the precision of the intermediate outputs of each layer in the network**. Unlike weights, which remain constant once the model is trained, activations are dynamic and change with each input, making their range harder to predict.\n\nGenerally, activation quantization is more challenging to implement than weight quantization because it requires careful calibration to ensure the dynamic range of activations is accurately captured.\n\nWeight quantization and activation quantization are complementary techniques. Using both can significantly reduce model size without greatly compromising performance.\n\n\n## Final Thoughts\n\nIn this article, we have reviewed 5 key points about quantization to better understand how to reduce the size of these constantly growing models.\n\nAs for the implementation of those techniques, there are several tools and libraries in Python that support quantization such as `pytorch` and `tensorflow`. Nevertheless, integrating quantization seamlessly in existing models requires a deep understanding of the libraries and model internals.\n\nThat is why my favorite option to implement quantization in easy steps so far is the [Quanto](https://huggingface.co/blog/quanto-introduction) library by Hugging Face, designed to simplify the quantization process for PyTorch models.\n\nIf you are interested in the in-depths of LLM Quantization and how to use the aforementioned library, you might also be interested in the article [“Quantization for Large Language Models (LLMs): Reduce AI Model Sizes Efficiently”](https://www.datacamp.com/tutorial/quantization-for-large-language-models).\n\nThat is all! Many thanks for reading!\n\nI hope this article helps you when **using LLMs for coding!**\n\nYou can also subscribe to my [**Newsletter**](https://readmedium.com/@andvalenzuela/subscribe) to stay tuned for new content.\n\n**Especially**, **if you are interested in articles about Large Language Models and ChatGPT**:\n\n\n"},{"lang":"en","group":"blog","slug":"blog/langgraph-vs-langchain-vs-langflow-vs-langsmith-which-one-to-use-why-69ee91e91000","frontmatter":{"title":"LangGraph vs. LangChain vs. LangFlow vs. LangSmith: Which One to Use & Why?","meta_title":"LangGraph vs. LangChain vs. LangFlow vs. LangSmith: Which One to Use & Why?","description":"Discover the key differences between LangGraph, LangChain, LangFlow, and LangSmith, and learn which framework is best suited for your…","date":"2024-10-23T11:47:55.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*xrWv1QVt4zE5cxjA8VA3ag.png","categories":["agents"],"author":"Rifx.Online","tags":["agents"],"draft":false,"slug":"blog/langgraph-vs-langchain-vs-langflow-vs-langsmith-which-one-to-use-why-69ee91e91000"},"content":"\n\n\n\n\n### Discover the key differences between LangGraph, LangChain, LangFlow, and LangSmith, and learn which framework is best suited for your language model applications — from workflow building to performance monitoring.\n\n👨🏾‍💻 [GitHub](https://github.com/mdmonsurali) ⭐️ | 👔[LinkedIn](https://www.linkedin.com/in/mdmonsurali/) |📝 [Medium](https://medium.com/@monsuralirana)\n\n\n\nIn recent years, the world of natural language processing (NLP) has witnessed an explosion in the number of frameworks, libraries, and tools available for building language model-based applications. Among these, **LangGraph**, **LangChain**, **LangFlow**, and **LangSmith** have emerged as leading options, each catering to different use cases and user needs. If you’re looking to build, monitor, or scale language model workflows, it’s crucial to understand the strengths and purposes of these tools.\n\nIn this blog, we’ll explore each framework, break down their strengths, and provide insights into when to use them. Whether you’re a seasoned developer or a newcomer to the field, understanding the nuances of these tools will help you choose the right one for your project.\n\n\n## Introduction to Language Model Frameworks\n\nWith the rise of powerful language models such as GPT-3, GPT-4, and other transformer-based models, there is a growing need for frameworks that streamline the creation and management of language-based applications. These frameworks simplify complex tasks like **chaining multiple prompts**, **retrieving relevant documents**, and even **monitoring model performance**.\n\nHowever, not all frameworks are the same. While some provide a **visual interface** to manage workflows, others offer advanced **debugging and observability** features. Let’s dive into each of these tools to understand their unique offerings.\n\n\n## 1. LangGraph: Visualizing Complex Workflows\n\n**LangGraph** is a newer framework designed for developers who prefer a **visual approach** to building language model pipelines. It allows you to structure complex workflows with **graph-based visualizations**, making it easier to understand dependencies between different tasks and components. This can be especially useful for larger applications where multiple steps, such as text generation, document retrieval, and classification, are chained together.\n\n\n### Strengths:\n\n* **Visual Workflow Representation**: LangGraph lets you visualize the flow of data and actions between different components. This graphical approach is intuitive and helps in designing more complex pipelines.\n* **Ease of Debugging**: The visual nature of LangGraph makes it easier to identify bottlenecks or problematic nodes in a workflow.\n\n\n### Example Use Case:\n\nSuppose you’re building an automated system that first retrieves relevant documents using a language model and then passes them through a summarizer. In LangGraph, you can visually map out this workflow, showing the relationships between each step. If there’s an issue at any point in the chain, the visual tool makes it easy to pinpoint where things went wrong.\n\n\n### When to Use LangGraph:\n\nIf you’re managing **complex workflows** with multiple steps and value a **graphical interface** for understanding your pipeline, LangGraph is a fantastic choice. It’s particularly helpful for developers or data scientists who prefer a more intuitive, drag-and-drop approach to workflow design.\n\n**Key points**:\n\n* If you need a clear visual representation of language processing workflows.\n* When creating more complex pipelines that require branching or multi-path dependencies.\n\n\n## 2. LangChain: The Workhorse for LLM Applications\n\n**LangChain** is one of the most popular frameworks for building applications powered by **large language models (LLMs)**. It provides a versatile, **code-first approach**, allowing developers to chain tasks such as document retrieval, summarization, and question-answering into cohesive workflows.\n\n\n### Strengths:\n\n* **Extensive Support for LLMs**: LangChain is compatible with various language models, making it easy to integrate models like OpenAI’s GPT or even locally hosted models.\n* **Chaining Capabilities**: LangChain excels at **chaining multiple operations** — hence the name — enabling developers to create sophisticated NLP applications.\n* **Wide Adoption**: As one of the most popular frameworks, LangChain has a **thriving community** and excellent support, with ample documentation and tutorials.\n\n\n### Example Use Case:\n\nImagine you’re building a **chatbot** that first understands the user’s question, retrieves relevant information from a database, and then generates a response. With LangChain, you can easily create this multi-step process programmatically, ensuring each step in the chain works harmoniously.\n\n\n### When to Use LangChain:\n\nIf you’re a **developer building production-level applications** and need a **flexible, code-centric solution**, LangChain is your best bet. It’s ideal for those who prefer control over their application’s architecture and are comfortable writing code to define workflows.\n\n**Key points**:\n\n* If you’re building production-grade applications that require chaining of tasks across multiple language models.\n* If you need a library with extensive community support and wide-ranging integrations.\n* When you’re more comfortable with programmatic solutions rather than visual tools.\n\n\n## 3. LangFlow: No-Code/Low-Code Extension of LangChain\n\n**LangFlow** is essentially a **visual extension of LangChain**. It combines the powerful backend of LangChain with an **intuitive drag-and-drop interface**. LangFlow allows users who might not be as comfortable writing code to still leverage the power of language models in their applications.\n\n\n### Strengths:\n\n* **Visual Workflow Creation**: Like LangGraph, LangFlow provides a visual interface for building workflows. However, it’s specifically built on top of LangChain, meaning users can harness LangChain’s power without needing to write extensive code.\n* **Ideal for Rapid Prototyping**: LangFlow is perfect for quickly **prototyping ideas** or building out proof-of-concept applications.\n* **Beginner-Friendly**: It’s a great entry point for users who are less familiar with coding but want to create language model workflows.\n\n\n### Example Use Case:\n\nIf you want to quickly build a **summarization tool** that retrieves documents, you can drag and drop the components in LangFlow’s interface to create a fully functioning application. This can be done without writing much code, if any.\n\n\n### When to Use LangFlow:\n\nLangFlow is perfect for **non-developers** or **rapid prototyping**. If you want to experiment with **LLM workflows quickly** without delving into the code, this tool makes it easy to get started.\n\n**Key points**:\n\n* If you want to prototype LLM workflows quickly without writing code.\n* If you’re comfortable with visual programming but need the flexibility of LangChain.\n* For educational purposes, to help users learn how workflows can be constructed.\n\n\n## 4. LangSmith: Monitoring and Observability\n\nWhile the other tools focus on **building workflows**, **LangSmith** is designed for **monitoring** and **debugging** language model applications. It provides advanced observability features to track the performance of your workflows and models, making it invaluable for production environments.\n\n\n### Strengths:\n\n* **Deep Observability**: LangSmith allows developers to monitor language model performance, ensuring that workflows behave as expected.\n* **Error Tracking**: It excels at helping developers track down issues, making debugging easier.\n* **Performance Insights**: LangSmith gives insights into **workflow performance**, helping developers optimize their applications.\n\n\n### Example Use Case:\n\nLet’s say you’ve deployed a **customer service chatbot** that uses a language model to answer questions. Over time, you notice that some responses are less accurate than expected. LangSmith can help you trace the problem by providing visibility into each decision point within the workflow.\n\n\n### When to Use LangSmith:\n\nIf you’re deploying applications in **production environments** and need to ensure **robustness, reliability, and performance**, LangSmith is an essential tool. It’s particularly useful when managing **complex systems that require debugging and optimization** over time.\n\n**Key points**:\n\n* If you need advanced monitoring or debugging capabilities in LLM workflows.\n* For development environments where observability is key to ensuring optimal model performance.\n* If your focus is on improving and iterating LLM-powered applications based on real-time insights.\n\n\n## Which One to Choose?\n\n* **Use LangGraph** if you prefer graph-based, visual workflows for building complex LLM tasks. Ideal for users who need clarity and structure.\n* **Use LangChain** if you need a robust, flexible solution for creating language model applications programmatically. It’s versatile and great for developers building production-level applications.\n* **Use LangFlow** if you want the power of LangChain with a visual, no-code/low-code interface. Best for rapid prototyping and users who prefer visual tools over coding.\n* **Use LangSmith** if your focus is on observability and debugging of LLM applications. Ideal when you need to monitor and optimize workflows in a development or production environment.\n\nUltimately, your choice depends on your comfort with code, the complexity of your workflows, and whether you prioritize ease of use, flexibility, or observability.\n\n\n## Conclusion\n\nEach of these tools — **LangGraph**, **LangChain**, **LangFlow**, and **LangSmith** — caters to different stages of developing and managing language model applications. **LangGraph** provides a visual, intuitive way to build complex workflows, while **LangChain** offers a robust, code-first solution for developers looking to create scalable applications. For those who prefer a **low-code**, drag-and-drop approach, **LangFlow** simplifies the process without sacrificing power. Finally, **LangSmith** focuses on observability and debugging, ensuring that your workflows are optimized and reliable. Choosing the right tool depends on your project needs, whether it’s for rapid prototyping, production-level scaling, or monitoring and performance tracking.\n\nHappy coding! 🎉\n\n👨🏾‍💻 [GitHub](https://github.com/mdmonsurali) ⭐️ | 👔[LinkedIn](https://www.linkedin.com/in/mdmonsurali/) |📝 [Medium](https://medium.com/@monsuralirana)\n\nThank you for your time in reading this post!\n\nMake sure to leave your feedback and comments. See you in the next blog, stay tuned 📢\n\n\n## References:\n\n1. “LangChain Documentation” — <https://python.langchain.com/docs/introduction/>\n2. “LangGraph Overview” — <https://langchain-ai.github.io/langgraph/>\n3. “LangFlow GitHub Repository” — [https://github.com/LangFlow/LangFlow](https://docs.langflow.org/)\n4. “LangSmith Introduction” — <https://www.langchain.com/langsmith>\n5. “How to Build Chatbots With LangChain” by JetBrains blog — <https://blog.jetbrains.com/pycharm/2024/08/how-to-build-chatbots-with-langchain/>\n\n"},{"lang":"en","group":"blog","slug":"blog/rag-llm-and-pdf-conversion-to-markdown-text-with-pymupdf-03af00259b5d","frontmatter":{"title":"RAG/LLM and PDF: Conversion to Markdown Text with PyMuPDF","meta_title":"RAG/LLM and PDF: Conversion to Markdown Text with PyMuPDF","description":"Data feeding in markdown text format increases generated text quality","date":"2024-10-23T11:53:52.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*swPjVuudAhsoRiiw3Ee32w.png","categories":["llm"],"author":"Rifx.Online","tags":["llm"],"draft":false,"slug":"blog/rag-llm-and-pdf-conversion-to-markdown-text-with-pymupdf-03af00259b5d"},"content":"\n\n\n\n\n### Data feeding in markdown text format increases generated text quality\n\n\n\n\n## Introduction\n\nIn the context of **Large Language Models (LLMs)** and **Retrieval-Augmented Generation (RAG)** environments, data feeding in **markdown text format** holds **significant importance**. Here are some detailed considerations.\n\n**LLMs** are powerful language models that can generate coherent and contextually relevant text. However, they may sometimes produce responses that lack factual accuracy or context. By incorporating retrieval-based methods (like RAG), we can enhance the quality of generated text.\n\n**RAG** enables the integration of **external data** — previously absent in the LLM’s training data — into the text generation process. This inclusion mitigates “hallucination issues’’ and enhances the relevance of text responses.\n\n\n## Why Markdown for LLM?\n\n**Markdown** is a lightweight markup language that allows users to format plain text using simple syntax. It is widely used for creating structured documents, especially on platforms like GitHub, Jupyter notebooks, and various content management systems. When feeding data into an LLM or RAG system, using markdown format provides several benefits:\n\n1. **Structured Content**: Markdown allows you to organize information into headings, lists, tables, and other structured elements. This structure aids in better understanding and context preservation.\n2. **Rich Text**: Markdown supports basic formatting such as bold, italics, links, and code blocks. Including rich text in the input data enhances the context for the language model.\n3. **Embedding Links and References**: Markdown lets you embed hyperlinks, footnotes, and references. In RAG scenarios, this can be crucial for referring to external sources or providing additional context.\n4. **Ease of Authoring**: Markdown is human-readable and easy to write. Authors can create content efficiently without complex formatting tools.\n5. **Chunking**: Essential for RAG systems, chunking (otherwise known as “splitting”) breaks down extensive documents for easier processing. With PyMuPDF data extraction available in MD format we support chunking to keep text with common context together. **Importantly, PyMuPDF extraction in MD format allows for [Level 3 chunking](https://readmedium.com/five-levels-of-chunking-strategies-in-rag-notes-from-gregs-video-7b735895694d#b123)**.\n\nIn summary, using markdown text format in LLM and RAG environments ensures more accurate and relevant results because it supplies richer data structures and more relevant data chunk loads to your LLM.\n\n\n## PyMuPDF Support for Markdown Conversion of a PDF\n\nSince its inception, PyMuPDF has been able to extract text, images, vector graphics and, since August 2023, tables from PDF pages. Each of these object types has its own extraction method: there is one for text, and yet others for tables, images and vector graphics. To meet the requirements of RAG, we merged these disparate extractions to produce one common, unified **Markdown** string which consistently represents the page’s content as a whole.\n\nAll this is implemented as [one Python script](https://github.com/pymupdf/RAG/blob/main/helpers/pymupdf_rag.py). It can be imported as a module by some other script, or be invoked as a line command in a terminal window like this:\n\n`$ python pymupdf_rag.py input.pdf [-pages PAGES]`\n\nIt will produce a text file (called `input.md`) in **Markdown** format. The optional parameter `PAGES` allows restricting the conversion to a subset of the PDF’s total pages. If omitted, the full PDF is processed.\n\n\n## Markdown Creation Details\n\n\n### Selecting Pages to Consider\n\nThe “`-pages`” parameter is a string consisting of desired page numbers (1-based) to consider for markdown conversion. Multiple page number specifications can be given, separated by commas. Each specification either is one integer or two integers separated by a “`-`” hyphen, specifying a range of pages. Here is an example:\n\n“`-pages 1–10,15,20-N`”\n\nThis would include pages 1 through 10, 15 and pages 20 through the end of the file (capital “N” is treated as the number of the last page).\n\n\n### Identifying Headers\n\nUpon invocation, the program examines all text on the given pages and finds the most frequently used font size. This value (and all smaller font sizes) is assumed to represent **body text**. Larger font sizes are assumed to represent **header text**.\n\nDepending on their relative position in the font size hierarchy, header text will be prefixed with one or more markdown header `#`-tag characters.\n\n\n### Identifying the Processing Mode per Page Area\n\nAll text on each page will first be classified as being either **standard** text or **table** text. Then the page content will be extracted from top to bottom converting everything to markdown format.\n\nThis is best explained by an example:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*u5fv2aAIvDaaAd6H.png)\n\nThis page shows content, that represents typical situations:\n\n* Two tables, having partly overlapping vertical positions. One table has no headers, the other one has **external** column headers.\n* There is a **title** line and **headers** at multiple levels.\n* The **body text** contains a variety of styling details like **bold**, *italic* and `inline code`.\n* Ordered and unordered lists.\n* Code snippet.\n\nLayout analysis will determine three areas and select the appropriate processing modes: **(1)** text, **(2)** table, **(3)** text.\n\nThe generated Markdown text reflects the above faithfully — as much as at all possible in this format.\n\nFor an example, let us look at the output for the table with external headers:\n\n\n```python\n|Column1|Column2|\n\n|---|---|\n\n|Cell (0, 0)|Cell (0, 1)|\n\n|Cell (1, 0)|Cell (1, 1)|\n\n|Cell (2, 0)|Cell (2, 1)|\n```\nThis is GitHub-compatible format with the minimum possible token size — an important aspect for keeping feeds into RAG systems small.\n\n**Column borders** are indicated by the “`|`” character. A text line is assumed to be a **table header** if it is followed by a line of the form “`|---|---| …`”. The full **table definition** must be preceded and followed by at least one empty line.\n\nPlease note that for technical reasons markdown tables must have a header and thus will choose the first table row if no external header is available.\n\nTo confirm overall fidelity, here is how a Markdown parser processes the full page:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Ge83uj7FiM4T6XFn)\n\n\n## Invoking the Markdown Converter Programmatically\n\nInstead of executing a program in the command line, Markdown conversion can also be requested by a program:\n\n\n```python\nimport fitz\nfrom pymupdf_rag import to_markdown  # import Markdown converter\n\ndoc = fitz.open(“input.pdf”)  # open input PDF\n\n## define desired pages: this corresponds “-pages 1-10,15,20-N”\npage_list = list(range(9)) + [14] + list(range(19, len(doc) – 1))\n\n## get markdown string for all pages\nmd_text = to_markdown(doc, pages=page_list)\n\n## write markdown string to some file\noutput = open(“out-markdown.md”, “w”)\noutput.write(md_text)\noutput.close()\n```\n\n## Conclusion\n\nBy integrating PyMuPDF’s extraction methods, the content of PDF pages will be faithfully converted to markdown text that can be used as input for RAG chatbots.\n\nRemember, the key to a successful RAG chatbot lies in the quality and completeness of information it can access.\n\nPyMuPDF-enabled markdown extraction ensures that this information from PDFs is not only possible but straightforward, showcasing the library’s strength and developer-friendliness. Happy coding!\n\n\n### Source Code\n\n* [RAG/helpers/pymupdf\\_rag.py (github.com)](https://github.com/pymupdf/RAG/blob/main/helpers/pymupdf_rag.py)\n\n\n### References\n\n* [5 Levels of Text Splitting](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb)\n\n\n### Related Blogs\n\n* [Building a RAG Chatbot GUI with the ChatGPT API and PyMuPDF](https://readmedium.com/building-a-rag-chatbot-gui-with-the-chatgpt-api-and-pymupdf-9ea8c7fc4ab5)\n* [Creating a RAG Chatbot with ChatGPT and PyMUPDF](https://readmedium.com/creating-a-rag-chatbot-with-chatgpt-and-pymupdf-f6c30907ae27)\n* [RAG/LLM and PDF: Enhanced Text Extraction](https://readmedium.com/rag-llm-and-pdf-enhanced-text-extraction-5c5194c3885c)\n\n"},{"lang":"en","group":"blog","slug":"blog/the-6-best-llm-tools-to-run-models-locally-eedd0f7c2bbd","frontmatter":{"title":"The 6 Best LLM Tools To Run Models Locally","meta_title":"The 6 Best LLM Tools To Run Models Locally","description":"Running large language models (LLMs) like ChatGPT and Claude usually involves sending data to servers managed by OpenAI and other AI model…","date":"2024-10-23T11:50:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*2MB6-INUUGLR0NR_iOACIg.jpeg","categories":["ai"],"author":"Rifx.Online","tags":["ai"],"draft":false,"slug":"blog/the-6-best-llm-tools-to-run-models-locally-eedd0f7c2bbd"},"content":"\n\n\n\n\n\nRunning large language models (LLMs) like [ChatGPT](https://openai.com/chatgpt/mac/) and [Claude](https://claude.ai/) usually involves sending data to servers managed by [OpenAI](https://openai.com/) and other AI model providers. While these services are secure, some businesses prefer to keep their data entirely offline for greater privacy.\n\nThis article covers the top six tools developers can use to run and test LLMs locally, ensuring their data never leaves their devices, similar to how [end-to-end encryption](https://getstream.io/blog/end-to-end-encryption/) protects privacy.\n\n\n## Why Use Local LLMs?\n\nA tool like [LM Studio](https://lmstudio.ai/) does not collect user data or track users’ actions when they use it to run local LLMs. It lets all your chat data stay on your local machine without sharing with an AI/ML server.\n\n* **Privacy**: You can prompt local LLMs in a multi-turn manner without your prompt data leaving your localhost.\n* **Customization Options**: Local LLMs provide advanced configurations for CPU threads, temperature, context length, GPU settings, and more. This is similar to OpenAI’s playground.\n* **Support and Security**: They provide similar support and security as OpenAI or Claude.\n* **Subscription and Cost**: These tools are free to use and they do not require monthly subscription. For cloud services like OpenAI, each API request requires payment. Local LLMs help to save money since there are no monthly subscriptions.\n* **Offline Support**: You can load and connect with large language models while offline.\n* **Connectivity**: Sometimes, connecting to a cloud service like OpenAI may result in poor signal and connection.\n\n\n## Top Six and Free Local LLM Tools\n\nDepending on your specific use case, there are several offline LLM applications you can choose. Some of these tools are completely free for personal and commercial use. Others may require sending them a request for business use. There are several local LLM tools available for Mac, Windows, and Linux. The following are the six best tools you can pick from.\n\n\n## 1. LM Studio\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*svbQPZKu08of7Kv6)\n\n[LM Studio](https://lmstudio.ai/) can run any model file with the format `gguf`. It supports `gguf` files from model providers such as [Llama 3.1](https://llama.meta.com/), [Phi 3](https://huggingface.co/docs/transformers/main/en/model_doc/phi3), [Mistral](https://mistral.ai/), and [Gemma](https://ai.google.dev/gemma). To use LM Studio, visit the link above and download the app for your machine. Once you launch LM Studio, the homepage presents top LLMs to download and test. There is also a search bar to filter and download specific models from different AI providers.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*sbS3VqiLgDsftgs2)\n\nSearching for a model from a specific company presents several models, ranging from small to large [quantization](https://huggingface.co/docs/optimum/en/concept_guides/quantization). Depending on your machine, LM Studio uses a compatibility guess to highlight the model that will work on that machine or platform.\n\n\n## Key Features of LM Studio\n\nLM Studio provides similar functionalities and features as ChatGPT. It has several functions. The following highlights the key features of LM Studio.\n\n* **Model Parameters Customization**: This allows you to adjust temperature, maximum tokens, frequency penalty, and more.\n* **Chat History**: Allows you to save prompts for later use.\n Parameters and UI Hinting: You can hover on info buttons to lookup model parameters and terms.\n* **Cross-platform**: LM Studio is available on Linux, Mac, and Windows operating systems.\n* **Machine Specification Check**: LM studio checks computer specifications like GPU and memory and reports on compatible models. This prevents downloading a model that might not work on a specific machine.\n* **AI Chat and Playground**: Chat with a large language model in a multi-turn chat format and experiment with multiple LLMs by loading them concurrently.\n* **Local Inference Server for Developers**: Allows developers to set up a local HTTP server similar to OpenAI’s API.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*9bHmRiOSf6gm-u3P)\n\nThe local server provides sample Curl and Python client requests. This feature helps to build an AI application using LM Studio to access a particular LLM.\n\n\n```python\n## Example: reuse your existing OpenAI setup\nfrom openai import OpenAI\n\n## Point to the local server\nclient = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n\ncompletion = client.chat.completions.create(\n  model=\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"Always answer in rhymes.\"},\n    {\"role\": \"user\", \"content\": \"Introduce yourself.\"}\n  ],\n  temperature=0.7,\n)\n\nprint(completion.choices[0].message)\n```\nWith the above sample Python code, you can reuse an existing OpenAI configuration and modify the base url to point to your localhost.\n\n* **OpenAI’s Python Library Import**: LM Studio allows developers to import the OpenAI Python library and point the base URL to a local server (localhost).\n* **Multi-model Session**: Use a single prompt and select multiple models to evaluate.\n\n\n## Benefits of Using LM Studio\n\nThis tool is free for personal use and it allows developers to run LLMs through an in-app chat UI and playground. It provides a gorgeous and easy to use interface with filters and supports connecting to OpenAI’s Python library without the need for an API key. Companies and businesses can use LM studio on request. However it requires a M1/M2/M3 Mac or higher, or a Windows PC with a processor that supports [AVX2](https://edc.intel.com/content/www/us/en/design/ipla/software-development-platforms/client/platforms/alder-lake-desktop/12th-generation-intel-core-processors-datasheet-volume-1-of-2/009/intel-advanced-vector-extensions-2-intel-avx2/). Intel and [AMD](https://www.amd.com/en/support/download/drivers.html) users are limited to using the [Vulkan inference engine in v0.2.31](https://lmstudio.ai/).\n\n\n## 2. Jan\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*7YeH_48iFYB4lDRu)\n\nThink of [Jan](https://jan.ai/) as an open-source version of ChatGPT designed to operate offline. It is built by a community of users with a user-owned philosophy. Jan allows you to run popular models like [Mistral](https://huggingface.co/models?other=mistral) or [Llama](https://huggingface.co/models?other=llama) on your device without connecting it to the internet. With Jan, you can access remote APIs like OpenAI and [Groq](https://groq.com/).\n\n\n## Key Features of Jan\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*ufyOE6QkcHw8X5U7)\n\nJan is an electron app with features similar to LM Studio. It makes AI open and accessible to all by turning consumer machines into AI computers. Since it is an open source project, developers can contribute to it and extend its functionalities. The following breaksdown the major features of Jan.\n\n* **Local**: You can run your preferred AI models on devices without connecting them to the internet.\n* **Ready to Use Models**: After downloading Jan, you get a set of already installed models to start. There is also a possibility to search for specific models.\n* **Model Import**: It supports importing models from sources like Hugging Face.\n* **Free, Cross-Platform and Open Source**: Jan is 100% free, open source, and works on Mac, Windows, and Linux.\n* **Customize Inference Parameters**: Adjust model parameters such as Maximum token, temperature, stream, frequency penalty, and more. All preferences, model usage, and settings stay locally on your computer.\n* **Extensions**: Jan supports extensions like [TensortRT](https://github.com/NVIDIA/TensorRT) and [Inference Nitro](https://huggingface.co/jan-hq/nitro-v1.2-e3) for customizing and enhancing your AI models.\n\n\n## Benefits of Using Jan\n\nJan provides a clean and simple interface to interact with LLMs and it keeps all your data and processing information locally. It has over seventy large language models already installed for you to use. The availability of these ready-to-use models makes it easy to connect and interact with remote APIs like OpenAI and Mistral. Jan also has a great [GitHub](https://github.com/janhq/jan), [Discord](https://discord.gg/FTk2MvZwJH), and [Hugging Face](https://huggingface.co/janhq) communities to follow and ask for help. However, like all the LLM tools, the models work faster on Apple Silicon Macs than on Intel ones.\n\n\n## 3. Llamafile\n\n[Llamafile](https://github.com/Mozilla-Ocho/llamafile) is backed by [Mozilla](https://www.mozilla.org/en-US/?v=1) whose aim is to support and make open source AI accessible to everyone using a fast [CPU inference](https://huggingface.co/docs/transformers/en/perf_infer_cpu) with no network access. It converts LLMs into multi-platform [Executable Linkable Format](https://gist.github.com/x0nu11byt3/bcb35c3de461e5fb66173071a2379779) (ELF). It provides one of the best options to [integrate AI](https://getstream.io/chat/solutions/ai-integration/) into applications by allowing you to run LLMs with just a single executable file.\n\n\n## How Llamafile Works\n\nIt is designed to convert weights into several executable programs that require no installation to run on architectures such as Windows, MacOS, Linux, Intel, ARM, FreeBSD, and more. Under the hood, Llamafile uses [tinyBLAST](https://github.com/ggerganov/llama.cpp/issues/5048) to run on OSs like Windows without requiring an SDK.\n\n\n## Key Features of Llamafile\n\n* **Executable File**: Unlike other LLM tools like LM Studio and Jan, Llamafile requires only one executable file to run LLMs.\n* **Use Existing Models**: Llamafile supports using existing models tools like Ollama and LM Studio.\n* **Access or Make Models**: You can access popular LLMs from OpenAI, Mistral, Groq, and more. It also provides support for creating models from scratch.\n* **Model File Conversion**: You can convert the file format of many popular LLMs, for example, `.gguf` into `.llamafile` with a single command.\n\n`llamafile-convert mistral-7b.gguf`\n\n\n## Get Started With Llamafile\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*4PV1KsCZvvVKqFll)\n\nTo install Llamafile, head to the Huggingface website, select **Models** from the navigation, and search for **Llamafile**. You can also install your preferred [quantized](https://huggingface.co/docs/optimum/en/concept_guides/quantization) version from the URL below.\n\n[`https://huggingface.co/Mozilla/Meta-Llama-3.1-8B-Instruct-llamafile/tree/m`ain](https://huggingface.co/Mozilla/Meta-Llama-3.1-8B-Instruct-llamafile/tree/main)\n\n**Note**: The larger the quantization number, the better the response. As highlighted in the image above, this article uses `Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile` where `Q6` represents the quantization number.\n\n**Step 1: Download Llamafile**\n\nFrom the link above, click any of the download buttons to get your preferred version. If you have the [wget](https://www.gnu.org/software/wget/) utility installed on your machine, you can download Llamafile with the command below.\n\n`wget <https://huggingface.co/Mozilla/Meta-Llama-3.1-8B-Instruct-llamafile/blob/main/Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile>`\n\nYou should replace the URL with the version you like.\n\n**Step 2: Make Llamafile Executable**\n\nAfter downloading a particular version of Llamafile, you should make it executable using the following command by navigating to the file’s location.\n\n`chmod +x Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile`**Step 3: Run Llamafile**\n\nPrepend a period and forward slash `./` to the file name to launch Llamafile.\n\n`./Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile`\n\nThe Llamafile app will now be available at `http://127.0.0.1:8080` to run your various LLMs.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*1xrwDPTfNgmEQDTx)\n\n\n## Benefits of Using Llamafile\n\nLlamafile helps to democratize AI and ML by making LLMs easily reachable to consumer CPUs. As compared to other local LLM apps like **Llama.cpp**, Llamafile gives the fastest prompt processing experience and better performance on gaming computers. Since it has a faster performance, it is an excellent option for summarizing long text and large documents. It runs 100% offline and privately, so users do not share their data to any AI server or API. Machine Learning communities like Hugging Face supports the Llamafile format, making it easy to search for Llamafile related models. It also has a great open source community that develops and extends it further.\n\n\n## 4. GPT4ALL\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*j3vNWWQZCVF5woo5)\n\nGPT4ALL is built upon privacy, security, and no internet-required principles. Users can [install](https://www.nomic.ai/gpt4all) it on Mac, Windows, and Ubuntu. Compared to Jan or LM Studio, GPT4ALL has more monthly downloads, [GitHub Stars](https://github.com/nomic-ai/gpt4all), and active users.\n\n\n## Key Features of GPT4ALL\n\nGPT4All can run LLMs on major consumer hardware such as Mac M-Series chips, AMD and NVIDIA GPUs. The following are its key features.\n\n* **Privacy First**: Keep private and sensitive chat information and prompts only on your machine.\n* **No Internet Required**: It works completely offline.\n* **Models Exploration**: This feature allows developers to browse and download different kinds of LLMs to experiment with. You can select about 1000 open-source language models from popular options like LLama, Mistral, and more.\n* **Local Documents**: You can let your local LLM access your sensitive data with local documents like `.pdf` and `.txt` without data leaving your device and without a network.\n* **Customization options**: It provides several [chatbot](https://getstream.io/blog/llm-chatbot-docs/) adjustment options like temperature, batch size, context length, etc.\n* **Enterprise Edition**: GPT4ALL provides an enterprise package with security, support, and per-device licenses to bring local AI to businesses.\n\n\n## Get Started With GPT4All\n\nTo start using GPT4All to run LLMs locally, [Download](https://www.nomic.ai/gpt4all) the required version for your operating system.\n\n\n## Benefits of Using GPT4ALL\n\nWith the exception of Ollama, GPT4ALL has the most significant number of GitHub contributors and about 250000 monthly active users (according to <https://www.nomic.ai/gpt4all>) and compared to its competitors. The app collects anonymous user data about usage analytics and chat sharing. However, users have the options to opt in or out. Using GPT4ALL, developers benefit from its large user base, GitHub, and Discord communities.\n\n\n## 5. Ollama\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*STAonWgWIsY6cgDR)\n\nUsing [Ollama](https://ollama.com/), you can easily create local chatbots without connecting to an API like OpenAI. Since everything runs locally, you do not need to pay for any subscription or API calls.\n\n\n## Key Features of Ollama\n\n* **Model Customization**: Ollama allows you to convert `.gguf` model files and run them with `ollama run modelname`.\n* **Model Library**: Ollama has a large collection of models to try at [ollama.com/library](https://ollama.com/library).\n* **Import Models**: Ollama supports importing models from [PyTorch](https://pytorch.org/).\n* **Community Integrations**: Ollama integrates seamlessly into web and desktop applications like, [Ollama-SwiftUI](https://github.com/kghandour/Ollama-SwiftUI), [HTML UI](https://github.com/rtcfirefly/ollama-ui), [Dify.ai](https://github.com/rtcfirefly/ollama-ui), and [more](https://github.com/ollama/ollama?tab=readme-ov-file#community-integrations).\n* **Database Connection**: Ollama supports several [data platforms](https://github.com/mindsdb/mindsdb/blob/main/mindsdb/integrations/handlers/ollama_handler/README.md).\n* **Mobile Integration**: A SwiftUI app like [Enchanted](https://github.com/AugustDev/enchanted) brings Ollama to iOS, macOS, and visionOS. [Maid](https://github.com/Mobile-Artificial-Intelligence/maid) is also a cross-platform Flutter app that interfaces with `.gguf`model files locally.\n\n\n## Get Started With Ollama\n\nTo use Ollama for the first time, visit <https://ollama.com> and download the version for your machine. You can install it on Mac, Linux, or Windows. Once you install Ollama, you can check its detailed information in Terminal with the following command.\n\n`ollama`\n\nTo run a particular LLM, you should download it with:\n\n`ollama pull modelname`, where `modelname` is the name of the model you want to install. Checkout Ollama on [GitHub](https://github.com/ollama/ollama) for some example models to download. The `pull` command is also used for updating a model. Once it is used, only the difference will be fetched.\n\nAfter downloading for example, `llama3.1`, running `ollama run llama3.1` in the command line launches the model.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*aglZm6h0BU6GAYkSl04XWA.gif)\n\nIn the above example, we prompt the `llama3.1` model to solve a Physics work and energy question.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*dNNQYpz1s2tz1pcn)\n\n\n## Benefits of Using Ollama\n\nOllama has over 200 contributors on GitHub with active updates. It has the largest number of contributors and is more extendable among the other open-source LLM tools discussed above.\n\n\n## 6. LLaMa.cpp\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*KhsAUquhDZAHghxK)\n\n[LLaMa.cpp](https://github.com/ggerganov/llama.cpp) is the underlying backend technology (inference engine) that powers local LLM tools like Ollama and many others. Llama.cpp supports significant large language model inferences with minimal configuration and excellent local performance on various hardware. It can also run in the cloud.\n\n\n## Key Features of LLaMa.cpp\n\n* **Setup**: It has a minimal setup. You install it with a single command.\n* **Performance**: It performs very well on various hardware locally and in the cloud.\n* **Supported Models**: It supports popular and major LLMs like [Mistral 7B](https://huggingface.co/mistralai/Mistral-7B-v0.1), [Mixtral MoE](https://huggingface.co/models?search=mistral-ai/Mixtral), [DBRX](https://huggingface.co/databricks/dbrx-instruct), [Falcon](https://huggingface.co/models?search=tiiuae/falcon), and [many others](https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#description).\n* **Frontend AI Tools**: LLaMa.cpp supports open-source LLM UI tools like [MindWorkAI/AI-Studio](https://github.com/MindWorkAI/AI-Studio) (FSL-1.1-MIT), [iohub/collama](https://github.com/iohub/coLLaMA), etc.\n\n\n## Get Started With LLaMa.cpp\n\nTo run your first local large language model with llama.cpp, you should install it with:\n\n`brew install llama.cpp`\n\nNext, download the model you want to run from Hugging Face or any other source. For example, download the model below from Hugging Face and save it somewhere on your machine.\n\n[`https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q4_K_M.g`guf](https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf)\n\nUsing your preferred command line tool like Terminal, `cd` into the location of the `.gguf` model file you just downloaded and run the following commands.\n\n\n```python\nllama-cli --color \\ \n-m Mistral-7B-Instruct-v0.3.Q4_K_M.ggufb \\ \n-p \"Write a short intro about SwiftUI\"\n```\nIn summary, you first invoke the LLaMa CLI tool and set color and other flags. The `-m` flag specifies the path of the model you want to use. The `-p` flag specifies the prompt you wish to use to instruct the model.\n\nAfter running the above command, you will see the result in the following preview.\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*4Al-j50vXUXLUfvxzBt6aw.gif)\n\n\n## Local LLMs Use Cases\n\nRunning LLMs locally can help developers who want to understand their performance and how they work in detail. Local LLMs can query private documents and technical papers so that information about these documents does not leave the devices used to query them to any cloud AI APIs. Local LLMs are useful in no-internet locations and places where network reception is poor.\n\nIn a [telehealth setting](https://getstream.io/blog/telemedicine-app-development/), local LLMs can sort patient documents without having to upload them to any AI API provider due to privacy concerns.\n\n\n## Evaluating LLMs’ Performance To Run Locally\n\nKnowing the performance of a large language model before using it locally is essential for getting the required responses. There are several ways you can determine the performance of a particular LLM. Here are a few ways.\n\n* **Training**: What dataset is the model trained on?\n* **Fine-tuning**: To what extent can the model be customized to perform a specialized task or can it be fine-tuned to for a specific domain?.\n* **Academic Research**: Does the LLM have an academic research paper?\n\nTo answer the above questions, you can check excellent resources like [Hugging Face](https://huggingface.co/datasets) and [Arxiv.org](https://arxiv.org/). Also, [Open LLm Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) and [LMSYS Chatbot Arena](https://chat.lmsys.org/?arena) provide detailed information and benchmarks for varieties of LLMs.\n\n\n## Local LLM Tools Conclusion\n\nAs discussed in this article, several motives exist for choosing and using large language models locally. You can fine-tune a model to perform a specialized task in a [telemedicine app](https://getstream.io/chat/solutions/healthcare/) if you do not wish to send your dataset over the internet to an AI API provider. Many open-source Graphic User Interface (GUI-based) local LLM tools like LLm Studio and Jan provide intuitive front-end UIs for configuring and experimenting with LLMs without subscription-based services like OpenAI or Claude. You also discovered the various powerful command-line LLM applications like Ollama and LLaMa.cpp that help you run and test models locally and without an internet connection. Check out Stream’s [AI Chatbot](https://getstream.io/chat/solutions/ai-integration/) solution to integrate an AI chat into your app and visit all the related links to learn more.\n\n*Originally published at [https://getstream.io](https://getstream.io/blog/best-local-llm-tools/).*\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/a-practical-guide-for-using-autogen-in-software-applications-8799185d27ee","frontmatter":{"title":"在软件应用程序中使用 AutoGen 的实用指南","meta_title":"在软件应用程序中使用 AutoGen 的实用指南","description":"更新：虽然这篇文章是 4 个月前写的，但 AutoGen 已经发生了很大变化。对于可能存在的一些问题，我深表歉意……","date":"2024-10-23T11:51:51.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*swPjVuudAhsoRiiw3Ee32w.png","categories":["llm"],"author":"Rifx.Online","tags":["llm"],"draft":false,"slug":"blog/a-practical-guide-for-using-autogen-in-software-applications-8799185d27ee"},"content":"\n\n\n\n\n### Data feeding in markdown text format increases generated text quality\n\n\n\n\n## Introduction\n\nIn the context of **Large Language Models (LLMs)** and **Retrieval-Augmented Generation (RAG)** environments, data feeding in **markdown text format** holds **significant importance**. Here are some detailed considerations.\n\n**LLMs** are powerful language models that can generate coherent and contextually relevant text. However, they may sometimes produce responses that lack factual accuracy or context. By incorporating retrieval-based methods (like RAG), we can enhance the quality of generated text.\n\n**RAG** enables the integration of **external data** — previously absent in the LLM’s training data — into the text generation process. This inclusion mitigates “hallucination issues’’ and enhances the relevance of text responses.\n\n\n## Why Markdown for LLM?\n\n**Markdown** is a lightweight markup language that allows users to format plain text using simple syntax. It is widely used for creating structured documents, especially on platforms like GitHub, Jupyter notebooks, and various content management systems. When feeding data into an LLM or RAG system, using markdown format provides several benefits:\n\n1. **Structured Content**: Markdown allows you to organize information into headings, lists, tables, and other structured elements. This structure aids in better understanding and context preservation.\n2. **Rich Text**: Markdown supports basic formatting such as bold, italics, links, and code blocks. Including rich text in the input data enhances the context for the language model.\n3. **Embedding Links and References**: Markdown lets you embed hyperlinks, footnotes, and references. In RAG scenarios, this can be crucial for referring to external sources or providing additional context.\n4. **Ease of Authoring**: Markdown is human-readable and easy to write. Authors can create content efficiently without complex formatting tools.\n5. **Chunking**: Essential for RAG systems, chunking (otherwise known as “splitting”) breaks down extensive documents for easier processing. With PyMuPDF data extraction available in MD format we support chunking to keep text with common context together. **Importantly, PyMuPDF extraction in MD format allows for [Level 3 chunking](https://readmedium.com/five-levels-of-chunking-strategies-in-rag-notes-from-gregs-video-7b735895694d#b123)**.\n\nIn summary, using markdown text format in LLM and RAG environments ensures more accurate and relevant results because it supplies richer data structures and more relevant data chunk loads to your LLM.\n\n\n## PyMuPDF Support for Markdown Conversion of a PDF\n\nSince its inception, PyMuPDF has been able to extract text, images, vector graphics and, since August 2023, tables from PDF pages. Each of these object types has its own extraction method: there is one for text, and yet others for tables, images and vector graphics. To meet the requirements of RAG, we merged these disparate extractions to produce one common, unified **Markdown** string which consistently represents the page’s content as a whole.\n\nAll this is implemented as [one Python script](https://github.com/pymupdf/RAG/blob/main/helpers/pymupdf_rag.py). It can be imported as a module by some other script, or be invoked as a line command in a terminal window like this:\n\n`$ python pymupdf_rag.py input.pdf [-pages PAGES]`\n\nIt will produce a text file (called `input.md`) in **Markdown** format. The optional parameter `PAGES` allows restricting the conversion to a subset of the PDF’s total pages. If omitted, the full PDF is processed.\n\n\n## Markdown Creation Details\n\n\n### Selecting Pages to Consider\n\nThe “`-pages`” parameter is a string consisting of desired page numbers (1-based) to consider for markdown conversion. Multiple page number specifications can be given, separated by commas. Each specification either is one integer or two integers separated by a “`-`” hyphen, specifying a range of pages. Here is an example:\n\n“`-pages 1–10,15,20-N`”\n\nThis would include pages 1 through 10, 15 and pages 20 through the end of the file (capital “N” is treated as the number of the last page).\n\n\n### Identifying Headers\n\nUpon invocation, the program examines all text on the given pages and finds the most frequently used font size. This value (and all smaller font sizes) is assumed to represent **body text**. Larger font sizes are assumed to represent **header text**.\n\nDepending on their relative position in the font size hierarchy, header text will be prefixed with one or more markdown header `#`-tag characters.\n\n\n### Identifying the Processing Mode per Page Area\n\nAll text on each page will first be classified as being either **standard** text or **table** text. Then the page content will be extracted from top to bottom converting everything to markdown format.\n\nThis is best explained by an example:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*u5fv2aAIvDaaAd6H.png)\n\nThis page shows content, that represents typical situations:\n\n* Two tables, having partly overlapping vertical positions. One table has no headers, the other one has **external** column headers.\n* There is a **title** line and **headers** at multiple levels.\n* The **body text** contains a variety of styling details like **bold**, *italic* and `inline code`.\n* Ordered and unordered lists.\n* Code snippet.\n\nLayout analysis will determine three areas and select the appropriate processing modes: **(1)** text, **(2)** table, **(3)** text.\n\nThe generated Markdown text reflects the above faithfully — as much as at all possible in this format.\n\nFor an example, let us look at the output for the table with external headers:\n\n\n```python\n|Column1|Column2|\n\n|---|---|\n\n|Cell (0, 0)|Cell (0, 1)|\n\n|Cell (1, 0)|Cell (1, 1)|\n\n|Cell (2, 0)|Cell (2, 1)|\n```\nThis is GitHub-compatible format with the minimum possible token size — an important aspect for keeping feeds into RAG systems small.\n\n**Column borders** are indicated by the “`|`” character. A text line is assumed to be a **table header** if it is followed by a line of the form “`|---|---| …`”. The full **table definition** must be preceded and followed by at least one empty line.\n\nPlease note that for technical reasons markdown tables must have a header and thus will choose the first table row if no external header is available.\n\nTo confirm overall fidelity, here is how a Markdown parser processes the full page:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Ge83uj7FiM4T6XFn)\n\n\n## Invoking the Markdown Converter Programmatically\n\nInstead of executing a program in the command line, Markdown conversion can also be requested by a program:\n\n\n```python\nimport fitz\nfrom pymupdf_rag import to_markdown  # import Markdown converter\n\ndoc = fitz.open(“input.pdf”)  # open input PDF\n\n## define desired pages: this corresponds “-pages 1-10,15,20-N”\npage_list = list(range(9)) + [14] + list(range(19, len(doc) – 1))\n\n## get markdown string for all pages\nmd_text = to_markdown(doc, pages=page_list)\n\n## write markdown string to some file\noutput = open(“out-markdown.md”, “w”)\noutput.write(md_text)\noutput.close()\n```\n\n## Conclusion\n\nBy integrating PyMuPDF’s extraction methods, the content of PDF pages will be faithfully converted to markdown text that can be used as input for RAG chatbots.\n\nRemember, the key to a successful RAG chatbot lies in the quality and completeness of information it can access.\n\nPyMuPDF-enabled markdown extraction ensures that this information from PDFs is not only possible but straightforward, showcasing the library’s strength and developer-friendliness. Happy coding!\n\n\n### Source Code\n\n* [RAG/helpers/pymupdf\\_rag.py (github.com)](https://github.com/pymupdf/RAG/blob/main/helpers/pymupdf_rag.py)\n\n\n### References\n\n* [5 Levels of Text Splitting](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb)\n\n\n### Related Blogs\n\n* [Building a RAG Chatbot GUI with the ChatGPT API and PyMuPDF](https://readmedium.com/building-a-rag-chatbot-gui-with-the-chatgpt-api-and-pymupdf-9ea8c7fc4ab5)\n* [Creating a RAG Chatbot with ChatGPT and PyMUPDF](https://readmedium.com/creating-a-rag-chatbot-with-chatgpt-and-pymupdf-f6c30907ae27)\n* [RAG/LLM and PDF: Enhanced Text Extraction](https://readmedium.com/rag-llm-and-pdf-enhanced-text-extraction-5c5194c3885c)\n\n"},{"lang":"zh","group":"blog","slug":"blog/building-a-local-ai-powered-news-aggregator-with-ollama-swarm-and-duckduckgo-95aaf8b3ee41","frontmatter":{"title":"使用 Ollama、Swarm 和 DuckDuckGo 构建本地 AI 新闻聚合器","meta_title":"使用 Ollama、Swarm 和 DuckDuckGo 构建本地 AI 新闻聚合器","description":"没有提供字幕","date":"2024-10-23T09:10:26.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*fqVKJkw5sQvLtIsyCcengQ.png","categories":["agents"],"author":"Rifx.Online","tags":["agents"],"draft":false,"slug":"blog/building-a-local-ai-powered-news-aggregator-with-ollama-swarm-and-duckduckgo-95aaf8b3ee41"},"content":"\n\n# How to Run Nvidia’ llama-3.1-nemotron-70b-instruct Locally\n\nRunning large language models (LLMs) locally has become increasingly popular among developers, researchers, and AI enthusiasts. One such model that has gained significant attention is the llama-3.1-nemotron-70b-instruct, a powerful LLM customized by NVIDIA to enhance the helpfulness of generated responses. In this comprehensive guide, we’ll explore multiple methods to run this model on your local machine, starting with the user-friendly Ollama platform.\n\n\n> Before we get started, If you are seeking an All-in-One AI platform that manages all your AI subscriptions in one place, including all LLMs (such as GPT-o1, Llama 3.1, Claude 3.5 Sonnet, Google Gemini, Uncensored LLMs) and Image Generation Models (FLUX, Stable Diffusion, etc.), Use Anakin AI to manage them all!\n\n\n\n\n# Method 1: Run llama-3.1-nemotron-70b-instruct Locally with Ollama\n\nOllama is an excellent tool for running LLMs locally, offering a straightforward setup process and support for various models, including llama-3.1-nemotron-70b-instruct.\n\n\n## Installation\n\n1. Visit the official Ollama website ([https://ollama.ai](https://ollama.ai/)) and download the appropriate version for your operating system.\n2. Install Ollama by running the following command in your terminal:\n\n\n```python\ncurl https://ollama.ai/install.sh | sh\n```\n\n## Running llama-3.1-nemotron\n\nOnce Ollama is installed, you can easily run the llama-3.1-nemotron-70b-instruct model with a simple command:\n\n\n```python\nollama run nemotron:70b-instruct-q5_K_M\n```\nThis command will download the model if it’s not already present on your system and start an interactive session.\n\n\n## Using the Model\n\nAfter the model is loaded, you can start interacting with it by typing your prompts. For example:\n\n\n```python\n>>> What are the key features of llama-3.1-nemotron-70b-instruct?\n\nLlama-3.1-Nemotron-70B-Instruct is a large language model with several key features:\n1. Customized by NVIDIA: The model has been fine-tuned by NVIDIA to improve the helpfulness and quality of its responses.\n2. Based on Llama 3.1: It builds upon the Llama 3.1 architecture, which is known for its strong performance across various tasks.\n3. 70 billion parameters: This large parameter count allows for complex reasoning and a wide range of capabilities.\n4. Instruct-tuned: The model is specifically designed to follow instructions and generate helpful responses to user queries.\n5. RLHF training: It has been trained using Reinforcement Learning from Human Feedback, specifically the REINFORCE algorithm.\n6. Specialized reward model: The training process utilized Llama-3.1-Nemotron-70B-Reward for optimization.\n7. HelpSteer2-Preference prompts: These were used during the training process to further improve the model's helpfulness.\n8. Extended context length: Like other Llama 3.1 models, it likely supports a longer context window of 128K tokens.\n9. Multilingual capabilities: It can understand and generate text in multiple languages.\n10. Strong reasoning abilities: The model excels in tasks requiring complex reasoning and problem-solving.\nThese features make llama-3.1-nemotron-70b-instruct a powerful and versatile language model suitable for a wide range of applications, from general conversation to specialized tasks in various domains.\n```\nFor more advanced use cases, you can integrate Ollama with Python using libraries like Langchain. Here’s a simple example:\n\n\n```python\npython\n\nfrom langchain.llms import Ollama\n\nollama = Ollama(base_url=\"http://localhost:11434\", model=\"nemotron:70b-instruct-q5_K_M\")\nresponse = ollama.generate(\"Explain the concept of quantum entanglement.\")\nprint(response)\n```\nThis allows you to incorporate the model into your Python projects and applications seamlessly.\n\n\n# Method 2: Using llama.cpp\n\nllama.cpp is a popular C++ implementation of the Llama model inference, optimized for CPU usage. While it may require more setup than Ollama, it offers greater flexibility and control over the model’s parameters.\n\n\n## Installation\n\n1. Clone the llama.cpp repository:\n\n\n```python\ngit clone https://github.com/ggerganov/llama.cpp.git\ncd llama.cpp\n```\n1. Build the project:\n\n\n```python\nmake\n```\n\n## Downloading the Model\n\nTo run llama-3.1-nemotron-70b-instruct, you’ll need to download the model weights. These are typically available in GGML or GGUF format. You can find pre-converted models on platforms like Hugging Face.\n\n\n```python\nmkdir models\ncd models\nwget https://huggingface.co/TheBloke/Llama-3.1-Nemotron-70B-Instruct-GGUF/resolve/main/llama-3.1-nemotron-70b-instruct.Q4_K_M.gguf\n```\n\n## Running the Model\n\nOnce you have the model file, you can run it using the following command:\n\n\n```python\n./main -m models/llama-3.1-nemotron-70b-instruct.Q4_K_M.gguf -n 1024 -p \"Hello, how are you today?\"\n```\nThis command loads the model and generates a response to the given prompt. You can adjust various parameters like the number of tokens to generate (-n) or the temperature to control randomness.\n\n\n# Method 3: Using Hugging Face Transformers\n\nHugging Face’s Transformers library provides a high-level API for working with various language models, including llama-3.1-nemotron-70b-instruct.\n\n**Installation**\n\nFirst, install the necessary libraries:\n\n\n```python\npip install transformers torch accelerate\n```\n**Running the Model**\n\nHere’s a Python script to load and use the model:\n\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nmodel_name = \"meta-llama/Llama-3.1-Nemotron-70b-instruct\"\n# Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n# Prepare the input\nprompt = \"Explain the concept of quantum computing in simple terms.\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n# Generate the response\nwith torch.no_grad():\n    outputs = model.generate(**inputs, max_new_tokens=100)\n# Decode and print the response\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response)\n```\nThis method allows for more fine-grained control over the model’s behavior and integration with other Hugging Face tools and pipelines.\n\n\n# Conclusion\n\nRunning llama-3.1-nemotron-70b-instruct locally opens up a world of possibilities for developers and researchers. Whether you choose the simplicity of Ollama, the flexibility of llama.cpp, or the integration capabilities of Hugging Face Transformers, you now have the tools to harness the power of this advanced language model on your own hardware.As you explore the capabilities of llama-3.1-nemotron-70b-instruct, remember to balance performance with resource constraints, and always consider the ethical implications of your applications. With responsible use, this model can be a valuable asset in pushing the boundaries of what’s possible in natural language processing and AI-driven applications.\n\n\n\n"},{"lang":"zh","group":"blog","slug":"blog/explore-swarm-multi-agent-framework-locally-0e25ee617795","frontmatter":{"title":"本地探索 Swarm 多智能体框架","meta_title":"本地探索 Swarm 多智能体框架","description":"Swarm 是一个实验性示例框架，用于模拟轻量级多代理框架，用于教育目的。通常它与 Open… 配合使用","date":"2024-10-23T11:46:57.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*swPjVuudAhsoRiiw3Ee32w.png","categories":["agents"],"author":"Rifx.Online","tags":["agents"],"draft":false,"slug":"blog/explore-swarm-multi-agent-framework-locally-0e25ee617795"},"content":"\n\n\n\n\n### Data feeding in markdown text format increases generated text quality\n\n\n\n\n## Introduction\n\nIn the context of **Large Language Models (LLMs)** and **Retrieval-Augmented Generation (RAG)** environments, data feeding in **markdown text format** holds **significant importance**. Here are some detailed considerations.\n\n**LLMs** are powerful language models that can generate coherent and contextually relevant text. However, they may sometimes produce responses that lack factual accuracy or context. By incorporating retrieval-based methods (like RAG), we can enhance the quality of generated text.\n\n**RAG** enables the integration of **external data** — previously absent in the LLM’s training data — into the text generation process. This inclusion mitigates “hallucination issues’’ and enhances the relevance of text responses.\n\n\n## Why Markdown for LLM?\n\n**Markdown** is a lightweight markup language that allows users to format plain text using simple syntax. It is widely used for creating structured documents, especially on platforms like GitHub, Jupyter notebooks, and various content management systems. When feeding data into an LLM or RAG system, using markdown format provides several benefits:\n\n1. **Structured Content**: Markdown allows you to organize information into headings, lists, tables, and other structured elements. This structure aids in better understanding and context preservation.\n2. **Rich Text**: Markdown supports basic formatting such as bold, italics, links, and code blocks. Including rich text in the input data enhances the context for the language model.\n3. **Embedding Links and References**: Markdown lets you embed hyperlinks, footnotes, and references. In RAG scenarios, this can be crucial for referring to external sources or providing additional context.\n4. **Ease of Authoring**: Markdown is human-readable and easy to write. Authors can create content efficiently without complex formatting tools.\n5. **Chunking**: Essential for RAG systems, chunking (otherwise known as “splitting”) breaks down extensive documents for easier processing. With PyMuPDF data extraction available in MD format we support chunking to keep text with common context together. **Importantly, PyMuPDF extraction in MD format allows for [Level 3 chunking](https://readmedium.com/five-levels-of-chunking-strategies-in-rag-notes-from-gregs-video-7b735895694d#b123)**.\n\nIn summary, using markdown text format in LLM and RAG environments ensures more accurate and relevant results because it supplies richer data structures and more relevant data chunk loads to your LLM.\n\n\n## PyMuPDF Support for Markdown Conversion of a PDF\n\nSince its inception, PyMuPDF has been able to extract text, images, vector graphics and, since August 2023, tables from PDF pages. Each of these object types has its own extraction method: there is one for text, and yet others for tables, images and vector graphics. To meet the requirements of RAG, we merged these disparate extractions to produce one common, unified **Markdown** string which consistently represents the page’s content as a whole.\n\nAll this is implemented as [one Python script](https://github.com/pymupdf/RAG/blob/main/helpers/pymupdf_rag.py). It can be imported as a module by some other script, or be invoked as a line command in a terminal window like this:\n\n`$ python pymupdf_rag.py input.pdf [-pages PAGES]`\n\nIt will produce a text file (called `input.md`) in **Markdown** format. The optional parameter `PAGES` allows restricting the conversion to a subset of the PDF’s total pages. If omitted, the full PDF is processed.\n\n\n## Markdown Creation Details\n\n\n### Selecting Pages to Consider\n\nThe “`-pages`” parameter is a string consisting of desired page numbers (1-based) to consider for markdown conversion. Multiple page number specifications can be given, separated by commas. Each specification either is one integer or two integers separated by a “`-`” hyphen, specifying a range of pages. Here is an example:\n\n“`-pages 1–10,15,20-N`”\n\nThis would include pages 1 through 10, 15 and pages 20 through the end of the file (capital “N” is treated as the number of the last page).\n\n\n### Identifying Headers\n\nUpon invocation, the program examines all text on the given pages and finds the most frequently used font size. This value (and all smaller font sizes) is assumed to represent **body text**. Larger font sizes are assumed to represent **header text**.\n\nDepending on their relative position in the font size hierarchy, header text will be prefixed with one or more markdown header `#`-tag characters.\n\n\n### Identifying the Processing Mode per Page Area\n\nAll text on each page will first be classified as being either **standard** text or **table** text. Then the page content will be extracted from top to bottom converting everything to markdown format.\n\nThis is best explained by an example:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*u5fv2aAIvDaaAd6H.png)\n\nThis page shows content, that represents typical situations:\n\n* Two tables, having partly overlapping vertical positions. One table has no headers, the other one has **external** column headers.\n* There is a **title** line and **headers** at multiple levels.\n* The **body text** contains a variety of styling details like **bold**, *italic* and `inline code`.\n* Ordered and unordered lists.\n* Code snippet.\n\nLayout analysis will determine three areas and select the appropriate processing modes: **(1)** text, **(2)** table, **(3)** text.\n\nThe generated Markdown text reflects the above faithfully — as much as at all possible in this format.\n\nFor an example, let us look at the output for the table with external headers:\n\n\n```python\n|Column1|Column2|\n\n|---|---|\n\n|Cell (0, 0)|Cell (0, 1)|\n\n|Cell (1, 0)|Cell (1, 1)|\n\n|Cell (2, 0)|Cell (2, 1)|\n```\nThis is GitHub-compatible format with the minimum possible token size — an important aspect for keeping feeds into RAG systems small.\n\n**Column borders** are indicated by the “`|`” character. A text line is assumed to be a **table header** if it is followed by a line of the form “`|---|---| …`”. The full **table definition** must be preceded and followed by at least one empty line.\n\nPlease note that for technical reasons markdown tables must have a header and thus will choose the first table row if no external header is available.\n\nTo confirm overall fidelity, here is how a Markdown parser processes the full page:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Ge83uj7FiM4T6XFn)\n\n\n## Invoking the Markdown Converter Programmatically\n\nInstead of executing a program in the command line, Markdown conversion can also be requested by a program:\n\n\n```python\nimport fitz\nfrom pymupdf_rag import to_markdown  # import Markdown converter\n\ndoc = fitz.open(“input.pdf”)  # open input PDF\n\n## define desired pages: this corresponds “-pages 1-10,15,20-N”\npage_list = list(range(9)) + [14] + list(range(19, len(doc) – 1))\n\n## get markdown string for all pages\nmd_text = to_markdown(doc, pages=page_list)\n\n## write markdown string to some file\noutput = open(“out-markdown.md”, “w”)\noutput.write(md_text)\noutput.close()\n```\n\n## Conclusion\n\nBy integrating PyMuPDF’s extraction methods, the content of PDF pages will be faithfully converted to markdown text that can be used as input for RAG chatbots.\n\nRemember, the key to a successful RAG chatbot lies in the quality and completeness of information it can access.\n\nPyMuPDF-enabled markdown extraction ensures that this information from PDFs is not only possible but straightforward, showcasing the library’s strength and developer-friendliness. Happy coding!\n\n\n### Source Code\n\n* [RAG/helpers/pymupdf\\_rag.py (github.com)](https://github.com/pymupdf/RAG/blob/main/helpers/pymupdf_rag.py)\n\n\n### References\n\n* [5 Levels of Text Splitting](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb)\n\n\n### Related Blogs\n\n* [Building a RAG Chatbot GUI with the ChatGPT API and PyMuPDF](https://readmedium.com/building-a-rag-chatbot-gui-with-the-chatgpt-api-and-pymupdf-9ea8c7fc4ab5)\n* [Creating a RAG Chatbot with ChatGPT and PyMUPDF](https://readmedium.com/creating-a-rag-chatbot-with-chatgpt-and-pymupdf-f6c30907ae27)\n* [RAG/LLM and PDF: Enhanced Text Extraction](https://readmedium.com/rag-llm-and-pdf-enhanced-text-extraction-5c5194c3885c)\n\n"},{"lang":"zh","group":"blog","slug":"blog/generating-structured-data-from-an-image-with-gpt-vision-and-langchain-34aaf3dcb215","frontmatter":{"title":"使用 GPT Vision 和 Langchain 从图像生成结构化数据","meta_title":"使用 GPT Vision 和 Langchain 从图像生成结构化数据","description":"在当今世界，视觉数据非常丰富，从图像中提取有意义信息的能力变得越来越重要……","date":"2024-10-23T11:52:52.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*swPjVuudAhsoRiiw3Ee32w.png","categories":["llm"],"author":"Rifx.Online","tags":["llm"],"draft":false,"slug":"blog/generating-structured-data-from-an-image-with-gpt-vision-and-langchain-34aaf3dcb215"},"content":"\n\n\n\n\n### Data feeding in markdown text format increases generated text quality\n\n\n\n\n## Introduction\n\nIn the context of **Large Language Models (LLMs)** and **Retrieval-Augmented Generation (RAG)** environments, data feeding in **markdown text format** holds **significant importance**. Here are some detailed considerations.\n\n**LLMs** are powerful language models that can generate coherent and contextually relevant text. However, they may sometimes produce responses that lack factual accuracy or context. By incorporating retrieval-based methods (like RAG), we can enhance the quality of generated text.\n\n**RAG** enables the integration of **external data** — previously absent in the LLM’s training data — into the text generation process. This inclusion mitigates “hallucination issues’’ and enhances the relevance of text responses.\n\n\n## Why Markdown for LLM?\n\n**Markdown** is a lightweight markup language that allows users to format plain text using simple syntax. It is widely used for creating structured documents, especially on platforms like GitHub, Jupyter notebooks, and various content management systems. When feeding data into an LLM or RAG system, using markdown format provides several benefits:\n\n1. **Structured Content**: Markdown allows you to organize information into headings, lists, tables, and other structured elements. This structure aids in better understanding and context preservation.\n2. **Rich Text**: Markdown supports basic formatting such as bold, italics, links, and code blocks. Including rich text in the input data enhances the context for the language model.\n3. **Embedding Links and References**: Markdown lets you embed hyperlinks, footnotes, and references. In RAG scenarios, this can be crucial for referring to external sources or providing additional context.\n4. **Ease of Authoring**: Markdown is human-readable and easy to write. Authors can create content efficiently without complex formatting tools.\n5. **Chunking**: Essential for RAG systems, chunking (otherwise known as “splitting”) breaks down extensive documents for easier processing. With PyMuPDF data extraction available in MD format we support chunking to keep text with common context together. **Importantly, PyMuPDF extraction in MD format allows for [Level 3 chunking](https://readmedium.com/five-levels-of-chunking-strategies-in-rag-notes-from-gregs-video-7b735895694d#b123)**.\n\nIn summary, using markdown text format in LLM and RAG environments ensures more accurate and relevant results because it supplies richer data structures and more relevant data chunk loads to your LLM.\n\n\n## PyMuPDF Support for Markdown Conversion of a PDF\n\nSince its inception, PyMuPDF has been able to extract text, images, vector graphics and, since August 2023, tables from PDF pages. Each of these object types has its own extraction method: there is one for text, and yet others for tables, images and vector graphics. To meet the requirements of RAG, we merged these disparate extractions to produce one common, unified **Markdown** string which consistently represents the page’s content as a whole.\n\nAll this is implemented as [one Python script](https://github.com/pymupdf/RAG/blob/main/helpers/pymupdf_rag.py). It can be imported as a module by some other script, or be invoked as a line command in a terminal window like this:\n\n`$ python pymupdf_rag.py input.pdf [-pages PAGES]`\n\nIt will produce a text file (called `input.md`) in **Markdown** format. The optional parameter `PAGES` allows restricting the conversion to a subset of the PDF’s total pages. If omitted, the full PDF is processed.\n\n\n## Markdown Creation Details\n\n\n### Selecting Pages to Consider\n\nThe “`-pages`” parameter is a string consisting of desired page numbers (1-based) to consider for markdown conversion. Multiple page number specifications can be given, separated by commas. Each specification either is one integer or two integers separated by a “`-`” hyphen, specifying a range of pages. Here is an example:\n\n“`-pages 1–10,15,20-N`”\n\nThis would include pages 1 through 10, 15 and pages 20 through the end of the file (capital “N” is treated as the number of the last page).\n\n\n### Identifying Headers\n\nUpon invocation, the program examines all text on the given pages and finds the most frequently used font size. This value (and all smaller font sizes) is assumed to represent **body text**. Larger font sizes are assumed to represent **header text**.\n\nDepending on their relative position in the font size hierarchy, header text will be prefixed with one or more markdown header `#`-tag characters.\n\n\n### Identifying the Processing Mode per Page Area\n\nAll text on each page will first be classified as being either **standard** text or **table** text. Then the page content will be extracted from top to bottom converting everything to markdown format.\n\nThis is best explained by an example:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*u5fv2aAIvDaaAd6H.png)\n\nThis page shows content, that represents typical situations:\n\n* Two tables, having partly overlapping vertical positions. One table has no headers, the other one has **external** column headers.\n* There is a **title** line and **headers** at multiple levels.\n* The **body text** contains a variety of styling details like **bold**, *italic* and `inline code`.\n* Ordered and unordered lists.\n* Code snippet.\n\nLayout analysis will determine three areas and select the appropriate processing modes: **(1)** text, **(2)** table, **(3)** text.\n\nThe generated Markdown text reflects the above faithfully — as much as at all possible in this format.\n\nFor an example, let us look at the output for the table with external headers:\n\n\n```python\n|Column1|Column2|\n\n|---|---|\n\n|Cell (0, 0)|Cell (0, 1)|\n\n|Cell (1, 0)|Cell (1, 1)|\n\n|Cell (2, 0)|Cell (2, 1)|\n```\nThis is GitHub-compatible format with the minimum possible token size — an important aspect for keeping feeds into RAG systems small.\n\n**Column borders** are indicated by the “`|`” character. A text line is assumed to be a **table header** if it is followed by a line of the form “`|---|---| …`”. The full **table definition** must be preceded and followed by at least one empty line.\n\nPlease note that for technical reasons markdown tables must have a header and thus will choose the first table row if no external header is available.\n\nTo confirm overall fidelity, here is how a Markdown parser processes the full page:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Ge83uj7FiM4T6XFn)\n\n\n## Invoking the Markdown Converter Programmatically\n\nInstead of executing a program in the command line, Markdown conversion can also be requested by a program:\n\n\n```python\nimport fitz\nfrom pymupdf_rag import to_markdown  # import Markdown converter\n\ndoc = fitz.open(“input.pdf”)  # open input PDF\n\n## define desired pages: this corresponds “-pages 1-10,15,20-N”\npage_list = list(range(9)) + [14] + list(range(19, len(doc) – 1))\n\n## get markdown string for all pages\nmd_text = to_markdown(doc, pages=page_list)\n\n## write markdown string to some file\noutput = open(“out-markdown.md”, “w”)\noutput.write(md_text)\noutput.close()\n```\n\n## Conclusion\n\nBy integrating PyMuPDF’s extraction methods, the content of PDF pages will be faithfully converted to markdown text that can be used as input for RAG chatbots.\n\nRemember, the key to a successful RAG chatbot lies in the quality and completeness of information it can access.\n\nPyMuPDF-enabled markdown extraction ensures that this information from PDFs is not only possible but straightforward, showcasing the library’s strength and developer-friendliness. Happy coding!\n\n\n### Source Code\n\n* [RAG/helpers/pymupdf\\_rag.py (github.com)](https://github.com/pymupdf/RAG/blob/main/helpers/pymupdf_rag.py)\n\n\n### References\n\n* [5 Levels of Text Splitting](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb)\n\n\n### Related Blogs\n\n* [Building a RAG Chatbot GUI with the ChatGPT API and PyMuPDF](https://readmedium.com/building-a-rag-chatbot-gui-with-the-chatgpt-api-and-pymupdf-9ea8c7fc4ab5)\n* [Creating a RAG Chatbot with ChatGPT and PyMUPDF](https://readmedium.com/creating-a-rag-chatbot-with-chatgpt-and-pymupdf-f6c30907ae27)\n* [RAG/LLM and PDF: Enhanced Text Extraction](https://readmedium.com/rag-llm-and-pdf-enhanced-text-extraction-5c5194c3885c)\n\n"},{"lang":"zh","group":"blog","slug":"blog/how-to-run-nvidia-llama-3-1-nemotron-70b-instruct-locally-a58ad283aaff","frontmatter":{"title":"如何在本地运行 Nvidia 的 llama-3.1-nemotron-70b-instruct","meta_title":"如何在本地运行 Nvidia 的 llama-3.1-nemotron-70b-instruct","description":"在本地运行大型语言模型 (LLM) 在开发人员、研究人员和 AI 爱好者中越来越受欢迎。其中之一就是……","date":"2024-10-23T11:49:51.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*swPjVuudAhsoRiiw3Ee32w.png","categories":["large-language-models"],"author":"Rifx.Online","tags":["large-language-models"],"draft":false,"slug":"blog/how-to-run-nvidia-llama-3-1-nemotron-70b-instruct-locally-a58ad283aaff"},"content":"\n\n\n\n\n### Data feeding in markdown text format increases generated text quality\n\n\n\n\n## Introduction\n\nIn the context of **Large Language Models (LLMs)** and **Retrieval-Augmented Generation (RAG)** environments, data feeding in **markdown text format** holds **significant importance**. Here are some detailed considerations.\n\n**LLMs** are powerful language models that can generate coherent and contextually relevant text. However, they may sometimes produce responses that lack factual accuracy or context. By incorporating retrieval-based methods (like RAG), we can enhance the quality of generated text.\n\n**RAG** enables the integration of **external data** — previously absent in the LLM’s training data — into the text generation process. This inclusion mitigates “hallucination issues’’ and enhances the relevance of text responses.\n\n\n## Why Markdown for LLM?\n\n**Markdown** is a lightweight markup language that allows users to format plain text using simple syntax. It is widely used for creating structured documents, especially on platforms like GitHub, Jupyter notebooks, and various content management systems. When feeding data into an LLM or RAG system, using markdown format provides several benefits:\n\n1. **Structured Content**: Markdown allows you to organize information into headings, lists, tables, and other structured elements. This structure aids in better understanding and context preservation.\n2. **Rich Text**: Markdown supports basic formatting such as bold, italics, links, and code blocks. Including rich text in the input data enhances the context for the language model.\n3. **Embedding Links and References**: Markdown lets you embed hyperlinks, footnotes, and references. In RAG scenarios, this can be crucial for referring to external sources or providing additional context.\n4. **Ease of Authoring**: Markdown is human-readable and easy to write. Authors can create content efficiently without complex formatting tools.\n5. **Chunking**: Essential for RAG systems, chunking (otherwise known as “splitting”) breaks down extensive documents for easier processing. With PyMuPDF data extraction available in MD format we support chunking to keep text with common context together. **Importantly, PyMuPDF extraction in MD format allows for [Level 3 chunking](https://readmedium.com/five-levels-of-chunking-strategies-in-rag-notes-from-gregs-video-7b735895694d#b123)**.\n\nIn summary, using markdown text format in LLM and RAG environments ensures more accurate and relevant results because it supplies richer data structures and more relevant data chunk loads to your LLM.\n\n\n## PyMuPDF Support for Markdown Conversion of a PDF\n\nSince its inception, PyMuPDF has been able to extract text, images, vector graphics and, since August 2023, tables from PDF pages. Each of these object types has its own extraction method: there is one for text, and yet others for tables, images and vector graphics. To meet the requirements of RAG, we merged these disparate extractions to produce one common, unified **Markdown** string which consistently represents the page’s content as a whole.\n\nAll this is implemented as [one Python script](https://github.com/pymupdf/RAG/blob/main/helpers/pymupdf_rag.py). It can be imported as a module by some other script, or be invoked as a line command in a terminal window like this:\n\n`$ python pymupdf_rag.py input.pdf [-pages PAGES]`\n\nIt will produce a text file (called `input.md`) in **Markdown** format. The optional parameter `PAGES` allows restricting the conversion to a subset of the PDF’s total pages. If omitted, the full PDF is processed.\n\n\n## Markdown Creation Details\n\n\n### Selecting Pages to Consider\n\nThe “`-pages`” parameter is a string consisting of desired page numbers (1-based) to consider for markdown conversion. Multiple page number specifications can be given, separated by commas. Each specification either is one integer or two integers separated by a “`-`” hyphen, specifying a range of pages. Here is an example:\n\n“`-pages 1–10,15,20-N`”\n\nThis would include pages 1 through 10, 15 and pages 20 through the end of the file (capital “N” is treated as the number of the last page).\n\n\n### Identifying Headers\n\nUpon invocation, the program examines all text on the given pages and finds the most frequently used font size. This value (and all smaller font sizes) is assumed to represent **body text**. Larger font sizes are assumed to represent **header text**.\n\nDepending on their relative position in the font size hierarchy, header text will be prefixed with one or more markdown header `#`-tag characters.\n\n\n### Identifying the Processing Mode per Page Area\n\nAll text on each page will first be classified as being either **standard** text or **table** text. Then the page content will be extracted from top to bottom converting everything to markdown format.\n\nThis is best explained by an example:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*u5fv2aAIvDaaAd6H.png)\n\nThis page shows content, that represents typical situations:\n\n* Two tables, having partly overlapping vertical positions. One table has no headers, the other one has **external** column headers.\n* There is a **title** line and **headers** at multiple levels.\n* The **body text** contains a variety of styling details like **bold**, *italic* and `inline code`.\n* Ordered and unordered lists.\n* Code snippet.\n\nLayout analysis will determine three areas and select the appropriate processing modes: **(1)** text, **(2)** table, **(3)** text.\n\nThe generated Markdown text reflects the above faithfully — as much as at all possible in this format.\n\nFor an example, let us look at the output for the table with external headers:\n\n\n```python\n|Column1|Column2|\n\n|---|---|\n\n|Cell (0, 0)|Cell (0, 1)|\n\n|Cell (1, 0)|Cell (1, 1)|\n\n|Cell (2, 0)|Cell (2, 1)|\n```\nThis is GitHub-compatible format with the minimum possible token size — an important aspect for keeping feeds into RAG systems small.\n\n**Column borders** are indicated by the “`|`” character. A text line is assumed to be a **table header** if it is followed by a line of the form “`|---|---| …`”. The full **table definition** must be preceded and followed by at least one empty line.\n\nPlease note that for technical reasons markdown tables must have a header and thus will choose the first table row if no external header is available.\n\nTo confirm overall fidelity, here is how a Markdown parser processes the full page:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Ge83uj7FiM4T6XFn)\n\n\n## Invoking the Markdown Converter Programmatically\n\nInstead of executing a program in the command line, Markdown conversion can also be requested by a program:\n\n\n```python\nimport fitz\nfrom pymupdf_rag import to_markdown  # import Markdown converter\n\ndoc = fitz.open(“input.pdf”)  # open input PDF\n\n## define desired pages: this corresponds “-pages 1-10,15,20-N”\npage_list = list(range(9)) + [14] + list(range(19, len(doc) – 1))\n\n## get markdown string for all pages\nmd_text = to_markdown(doc, pages=page_list)\n\n## write markdown string to some file\noutput = open(“out-markdown.md”, “w”)\noutput.write(md_text)\noutput.close()\n```\n\n## Conclusion\n\nBy integrating PyMuPDF’s extraction methods, the content of PDF pages will be faithfully converted to markdown text that can be used as input for RAG chatbots.\n\nRemember, the key to a successful RAG chatbot lies in the quality and completeness of information it can access.\n\nPyMuPDF-enabled markdown extraction ensures that this information from PDFs is not only possible but straightforward, showcasing the library’s strength and developer-friendliness. Happy coding!\n\n\n### Source Code\n\n* [RAG/helpers/pymupdf\\_rag.py (github.com)](https://github.com/pymupdf/RAG/blob/main/helpers/pymupdf_rag.py)\n\n\n### References\n\n* [5 Levels of Text Splitting](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb)\n\n\n### Related Blogs\n\n* [Building a RAG Chatbot GUI with the ChatGPT API and PyMuPDF](https://readmedium.com/building-a-rag-chatbot-gui-with-the-chatgpt-api-and-pymupdf-9ea8c7fc4ab5)\n* [Creating a RAG Chatbot with ChatGPT and PyMUPDF](https://readmedium.com/creating-a-rag-chatbot-with-chatgpt-and-pymupdf-f6c30907ae27)\n* [RAG/LLM and PDF: Enhanced Text Extraction](https://readmedium.com/rag-llm-and-pdf-enhanced-text-extraction-5c5194c3885c)\n\n"},{"lang":"zh","group":"blog","slug":"blog/key-points-llm-quantization-chatgpt-artificial-intelligence-8201ffcb33d4","frontmatter":{"title":"解锁 LLM 量化的 5 个关键点","meta_title":"解锁 LLM 量化的 5 个关键点","description":"量化大型语言模型","date":"2024-10-23T11:48:53.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*swPjVuudAhsoRiiw3Ee32w.png","categories":["large-language-models"],"author":"Rifx.Online","tags":["large-language-models"],"draft":false,"slug":"blog/key-points-llm-quantization-chatgpt-artificial-intelligence-8201ffcb33d4"},"content":"\n\n\n\n\n### Data feeding in markdown text format increases generated text quality\n\n\n\n\n## Introduction\n\nIn the context of **Large Language Models (LLMs)** and **Retrieval-Augmented Generation (RAG)** environments, data feeding in **markdown text format** holds **significant importance**. Here are some detailed considerations.\n\n**LLMs** are powerful language models that can generate coherent and contextually relevant text. However, they may sometimes produce responses that lack factual accuracy or context. By incorporating retrieval-based methods (like RAG), we can enhance the quality of generated text.\n\n**RAG** enables the integration of **external data** — previously absent in the LLM’s training data — into the text generation process. This inclusion mitigates “hallucination issues’’ and enhances the relevance of text responses.\n\n\n## Why Markdown for LLM?\n\n**Markdown** is a lightweight markup language that allows users to format plain text using simple syntax. It is widely used for creating structured documents, especially on platforms like GitHub, Jupyter notebooks, and various content management systems. When feeding data into an LLM or RAG system, using markdown format provides several benefits:\n\n1. **Structured Content**: Markdown allows you to organize information into headings, lists, tables, and other structured elements. This structure aids in better understanding and context preservation.\n2. **Rich Text**: Markdown supports basic formatting such as bold, italics, links, and code blocks. Including rich text in the input data enhances the context for the language model.\n3. **Embedding Links and References**: Markdown lets you embed hyperlinks, footnotes, and references. In RAG scenarios, this can be crucial for referring to external sources or providing additional context.\n4. **Ease of Authoring**: Markdown is human-readable and easy to write. Authors can create content efficiently without complex formatting tools.\n5. **Chunking**: Essential for RAG systems, chunking (otherwise known as “splitting”) breaks down extensive documents for easier processing. With PyMuPDF data extraction available in MD format we support chunking to keep text with common context together. **Importantly, PyMuPDF extraction in MD format allows for [Level 3 chunking](https://readmedium.com/five-levels-of-chunking-strategies-in-rag-notes-from-gregs-video-7b735895694d#b123)**.\n\nIn summary, using markdown text format in LLM and RAG environments ensures more accurate and relevant results because it supplies richer data structures and more relevant data chunk loads to your LLM.\n\n\n## PyMuPDF Support for Markdown Conversion of a PDF\n\nSince its inception, PyMuPDF has been able to extract text, images, vector graphics and, since August 2023, tables from PDF pages. Each of these object types has its own extraction method: there is one for text, and yet others for tables, images and vector graphics. To meet the requirements of RAG, we merged these disparate extractions to produce one common, unified **Markdown** string which consistently represents the page’s content as a whole.\n\nAll this is implemented as [one Python script](https://github.com/pymupdf/RAG/blob/main/helpers/pymupdf_rag.py). It can be imported as a module by some other script, or be invoked as a line command in a terminal window like this:\n\n`$ python pymupdf_rag.py input.pdf [-pages PAGES]`\n\nIt will produce a text file (called `input.md`) in **Markdown** format. The optional parameter `PAGES` allows restricting the conversion to a subset of the PDF’s total pages. If omitted, the full PDF is processed.\n\n\n## Markdown Creation Details\n\n\n### Selecting Pages to Consider\n\nThe “`-pages`” parameter is a string consisting of desired page numbers (1-based) to consider for markdown conversion. Multiple page number specifications can be given, separated by commas. Each specification either is one integer or two integers separated by a “`-`” hyphen, specifying a range of pages. Here is an example:\n\n“`-pages 1–10,15,20-N`”\n\nThis would include pages 1 through 10, 15 and pages 20 through the end of the file (capital “N” is treated as the number of the last page).\n\n\n### Identifying Headers\n\nUpon invocation, the program examines all text on the given pages and finds the most frequently used font size. This value (and all smaller font sizes) is assumed to represent **body text**. Larger font sizes are assumed to represent **header text**.\n\nDepending on their relative position in the font size hierarchy, header text will be prefixed with one or more markdown header `#`-tag characters.\n\n\n### Identifying the Processing Mode per Page Area\n\nAll text on each page will first be classified as being either **standard** text or **table** text. Then the page content will be extracted from top to bottom converting everything to markdown format.\n\nThis is best explained by an example:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*u5fv2aAIvDaaAd6H.png)\n\nThis page shows content, that represents typical situations:\n\n* Two tables, having partly overlapping vertical positions. One table has no headers, the other one has **external** column headers.\n* There is a **title** line and **headers** at multiple levels.\n* The **body text** contains a variety of styling details like **bold**, *italic* and `inline code`.\n* Ordered and unordered lists.\n* Code snippet.\n\nLayout analysis will determine three areas and select the appropriate processing modes: **(1)** text, **(2)** table, **(3)** text.\n\nThe generated Markdown text reflects the above faithfully — as much as at all possible in this format.\n\nFor an example, let us look at the output for the table with external headers:\n\n\n```python\n|Column1|Column2|\n\n|---|---|\n\n|Cell (0, 0)|Cell (0, 1)|\n\n|Cell (1, 0)|Cell (1, 1)|\n\n|Cell (2, 0)|Cell (2, 1)|\n```\nThis is GitHub-compatible format with the minimum possible token size — an important aspect for keeping feeds into RAG systems small.\n\n**Column borders** are indicated by the “`|`” character. A text line is assumed to be a **table header** if it is followed by a line of the form “`|---|---| …`”. The full **table definition** must be preceded and followed by at least one empty line.\n\nPlease note that for technical reasons markdown tables must have a header and thus will choose the first table row if no external header is available.\n\nTo confirm overall fidelity, here is how a Markdown parser processes the full page:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Ge83uj7FiM4T6XFn)\n\n\n## Invoking the Markdown Converter Programmatically\n\nInstead of executing a program in the command line, Markdown conversion can also be requested by a program:\n\n\n```python\nimport fitz\nfrom pymupdf_rag import to_markdown  # import Markdown converter\n\ndoc = fitz.open(“input.pdf”)  # open input PDF\n\n## define desired pages: this corresponds “-pages 1-10,15,20-N”\npage_list = list(range(9)) + [14] + list(range(19, len(doc) – 1))\n\n## get markdown string for all pages\nmd_text = to_markdown(doc, pages=page_list)\n\n## write markdown string to some file\noutput = open(“out-markdown.md”, “w”)\noutput.write(md_text)\noutput.close()\n```\n\n## Conclusion\n\nBy integrating PyMuPDF’s extraction methods, the content of PDF pages will be faithfully converted to markdown text that can be used as input for RAG chatbots.\n\nRemember, the key to a successful RAG chatbot lies in the quality and completeness of information it can access.\n\nPyMuPDF-enabled markdown extraction ensures that this information from PDFs is not only possible but straightforward, showcasing the library’s strength and developer-friendliness. Happy coding!\n\n\n### Source Code\n\n* [RAG/helpers/pymupdf\\_rag.py (github.com)](https://github.com/pymupdf/RAG/blob/main/helpers/pymupdf_rag.py)\n\n\n### References\n\n* [5 Levels of Text Splitting](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb)\n\n\n### Related Blogs\n\n* [Building a RAG Chatbot GUI with the ChatGPT API and PyMuPDF](https://readmedium.com/building-a-rag-chatbot-gui-with-the-chatgpt-api-and-pymupdf-9ea8c7fc4ab5)\n* [Creating a RAG Chatbot with ChatGPT and PyMUPDF](https://readmedium.com/creating-a-rag-chatbot-with-chatgpt-and-pymupdf-f6c30907ae27)\n* [RAG/LLM and PDF: Enhanced Text Extraction](https://readmedium.com/rag-llm-and-pdf-enhanced-text-extraction-5c5194c3885c)\n\n"},{"lang":"zh","group":"blog","slug":"blog/langgraph-vs-langchain-vs-langflow-vs-langsmith-which-one-to-use-why-69ee91e91000","frontmatter":{"title":"LangGraph、LangChain、LangFlow、LangSmith：使用哪一个以及为什么？","meta_title":"LangGraph、LangChain、LangFlow、LangSmith：使用哪一个以及为什么？","description":"了解 LangGraph、LangChain、LangFlow 和 LangSmith 之间的主要区别，并了解哪种框架最适合您的……","date":"2024-10-23T11:47:55.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*swPjVuudAhsoRiiw3Ee32w.png","categories":["agents"],"author":"Rifx.Online","tags":["agents"],"draft":false,"slug":"blog/langgraph-vs-langchain-vs-langflow-vs-langsmith-which-one-to-use-why-69ee91e91000"},"content":"\n\n\n\n\n### Data feeding in markdown text format increases generated text quality\n\n\n\n\n## Introduction\n\nIn the context of **Large Language Models (LLMs)** and **Retrieval-Augmented Generation (RAG)** environments, data feeding in **markdown text format** holds **significant importance**. Here are some detailed considerations.\n\n**LLMs** are powerful language models that can generate coherent and contextually relevant text. However, they may sometimes produce responses that lack factual accuracy or context. By incorporating retrieval-based methods (like RAG), we can enhance the quality of generated text.\n\n**RAG** enables the integration of **external data** — previously absent in the LLM’s training data — into the text generation process. This inclusion mitigates “hallucination issues’’ and enhances the relevance of text responses.\n\n\n## Why Markdown for LLM?\n\n**Markdown** is a lightweight markup language that allows users to format plain text using simple syntax. It is widely used for creating structured documents, especially on platforms like GitHub, Jupyter notebooks, and various content management systems. When feeding data into an LLM or RAG system, using markdown format provides several benefits:\n\n1. **Structured Content**: Markdown allows you to organize information into headings, lists, tables, and other structured elements. This structure aids in better understanding and context preservation.\n2. **Rich Text**: Markdown supports basic formatting such as bold, italics, links, and code blocks. Including rich text in the input data enhances the context for the language model.\n3. **Embedding Links and References**: Markdown lets you embed hyperlinks, footnotes, and references. In RAG scenarios, this can be crucial for referring to external sources or providing additional context.\n4. **Ease of Authoring**: Markdown is human-readable and easy to write. Authors can create content efficiently without complex formatting tools.\n5. **Chunking**: Essential for RAG systems, chunking (otherwise known as “splitting”) breaks down extensive documents for easier processing. With PyMuPDF data extraction available in MD format we support chunking to keep text with common context together. **Importantly, PyMuPDF extraction in MD format allows for [Level 3 chunking](https://readmedium.com/five-levels-of-chunking-strategies-in-rag-notes-from-gregs-video-7b735895694d#b123)**.\n\nIn summary, using markdown text format in LLM and RAG environments ensures more accurate and relevant results because it supplies richer data structures and more relevant data chunk loads to your LLM.\n\n\n## PyMuPDF Support for Markdown Conversion of a PDF\n\nSince its inception, PyMuPDF has been able to extract text, images, vector graphics and, since August 2023, tables from PDF pages. Each of these object types has its own extraction method: there is one for text, and yet others for tables, images and vector graphics. To meet the requirements of RAG, we merged these disparate extractions to produce one common, unified **Markdown** string which consistently represents the page’s content as a whole.\n\nAll this is implemented as [one Python script](https://github.com/pymupdf/RAG/blob/main/helpers/pymupdf_rag.py). It can be imported as a module by some other script, or be invoked as a line command in a terminal window like this:\n\n`$ python pymupdf_rag.py input.pdf [-pages PAGES]`\n\nIt will produce a text file (called `input.md`) in **Markdown** format. The optional parameter `PAGES` allows restricting the conversion to a subset of the PDF’s total pages. If omitted, the full PDF is processed.\n\n\n## Markdown Creation Details\n\n\n### Selecting Pages to Consider\n\nThe “`-pages`” parameter is a string consisting of desired page numbers (1-based) to consider for markdown conversion. Multiple page number specifications can be given, separated by commas. Each specification either is one integer or two integers separated by a “`-`” hyphen, specifying a range of pages. Here is an example:\n\n“`-pages 1–10,15,20-N`”\n\nThis would include pages 1 through 10, 15 and pages 20 through the end of the file (capital “N” is treated as the number of the last page).\n\n\n### Identifying Headers\n\nUpon invocation, the program examines all text on the given pages and finds the most frequently used font size. This value (and all smaller font sizes) is assumed to represent **body text**. Larger font sizes are assumed to represent **header text**.\n\nDepending on their relative position in the font size hierarchy, header text will be prefixed with one or more markdown header `#`-tag characters.\n\n\n### Identifying the Processing Mode per Page Area\n\nAll text on each page will first be classified as being either **standard** text or **table** text. Then the page content will be extracted from top to bottom converting everything to markdown format.\n\nThis is best explained by an example:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*u5fv2aAIvDaaAd6H.png)\n\nThis page shows content, that represents typical situations:\n\n* Two tables, having partly overlapping vertical positions. One table has no headers, the other one has **external** column headers.\n* There is a **title** line and **headers** at multiple levels.\n* The **body text** contains a variety of styling details like **bold**, *italic* and `inline code`.\n* Ordered and unordered lists.\n* Code snippet.\n\nLayout analysis will determine three areas and select the appropriate processing modes: **(1)** text, **(2)** table, **(3)** text.\n\nThe generated Markdown text reflects the above faithfully — as much as at all possible in this format.\n\nFor an example, let us look at the output for the table with external headers:\n\n\n```python\n|Column1|Column2|\n\n|---|---|\n\n|Cell (0, 0)|Cell (0, 1)|\n\n|Cell (1, 0)|Cell (1, 1)|\n\n|Cell (2, 0)|Cell (2, 1)|\n```\nThis is GitHub-compatible format with the minimum possible token size — an important aspect for keeping feeds into RAG systems small.\n\n**Column borders** are indicated by the “`|`” character. A text line is assumed to be a **table header** if it is followed by a line of the form “`|---|---| …`”. The full **table definition** must be preceded and followed by at least one empty line.\n\nPlease note that for technical reasons markdown tables must have a header and thus will choose the first table row if no external header is available.\n\nTo confirm overall fidelity, here is how a Markdown parser processes the full page:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Ge83uj7FiM4T6XFn)\n\n\n## Invoking the Markdown Converter Programmatically\n\nInstead of executing a program in the command line, Markdown conversion can also be requested by a program:\n\n\n```python\nimport fitz\nfrom pymupdf_rag import to_markdown  # import Markdown converter\n\ndoc = fitz.open(“input.pdf”)  # open input PDF\n\n## define desired pages: this corresponds “-pages 1-10,15,20-N”\npage_list = list(range(9)) + [14] + list(range(19, len(doc) – 1))\n\n## get markdown string for all pages\nmd_text = to_markdown(doc, pages=page_list)\n\n## write markdown string to some file\noutput = open(“out-markdown.md”, “w”)\noutput.write(md_text)\noutput.close()\n```\n\n## Conclusion\n\nBy integrating PyMuPDF’s extraction methods, the content of PDF pages will be faithfully converted to markdown text that can be used as input for RAG chatbots.\n\nRemember, the key to a successful RAG chatbot lies in the quality and completeness of information it can access.\n\nPyMuPDF-enabled markdown extraction ensures that this information from PDFs is not only possible but straightforward, showcasing the library’s strength and developer-friendliness. Happy coding!\n\n\n### Source Code\n\n* [RAG/helpers/pymupdf\\_rag.py (github.com)](https://github.com/pymupdf/RAG/blob/main/helpers/pymupdf_rag.py)\n\n\n### References\n\n* [5 Levels of Text Splitting](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb)\n\n\n### Related Blogs\n\n* [Building a RAG Chatbot GUI with the ChatGPT API and PyMuPDF](https://readmedium.com/building-a-rag-chatbot-gui-with-the-chatgpt-api-and-pymupdf-9ea8c7fc4ab5)\n* [Creating a RAG Chatbot with ChatGPT and PyMUPDF](https://readmedium.com/creating-a-rag-chatbot-with-chatgpt-and-pymupdf-f6c30907ae27)\n* [RAG/LLM and PDF: Enhanced Text Extraction](https://readmedium.com/rag-llm-and-pdf-enhanced-text-extraction-5c5194c3885c)\n\n"},{"lang":"zh","group":"blog","slug":"blog/rag-llm-and-pdf-conversion-to-markdown-text-with-pymupdf-03af00259b5d","frontmatter":{"title":"RAG/LLM 和 PDF：使用 PyMuPDF 转换为 Markdown 文本","meta_title":"RAG/LLM 和 PDF：使用 PyMuPDF 转换为 Markdown 文本","description":"采用 markdown 文本格式输入数据可提高生成的文本质量","date":"2024-10-23T11:53:52.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*swPjVuudAhsoRiiw3Ee32w.png","categories":["llm"],"author":"Rifx.Online","tags":["llm"],"draft":false,"slug":"blog/rag-llm-and-pdf-conversion-to-markdown-text-with-pymupdf-03af00259b5d"},"content":"\n\n\n\n\n### Data feeding in markdown text format increases generated text quality\n\n\n\n\n## Introduction\n\nIn the context of **Large Language Models (LLMs)** and **Retrieval-Augmented Generation (RAG)** environments, data feeding in **markdown text format** holds **significant importance**. Here are some detailed considerations.\n\n**LLMs** are powerful language models that can generate coherent and contextually relevant text. However, they may sometimes produce responses that lack factual accuracy or context. By incorporating retrieval-based methods (like RAG), we can enhance the quality of generated text.\n\n**RAG** enables the integration of **external data** — previously absent in the LLM’s training data — into the text generation process. This inclusion mitigates “hallucination issues’’ and enhances the relevance of text responses.\n\n\n## Why Markdown for LLM?\n\n**Markdown** is a lightweight markup language that allows users to format plain text using simple syntax. It is widely used for creating structured documents, especially on platforms like GitHub, Jupyter notebooks, and various content management systems. When feeding data into an LLM or RAG system, using markdown format provides several benefits:\n\n1. **Structured Content**: Markdown allows you to organize information into headings, lists, tables, and other structured elements. This structure aids in better understanding and context preservation.\n2. **Rich Text**: Markdown supports basic formatting such as bold, italics, links, and code blocks. Including rich text in the input data enhances the context for the language model.\n3. **Embedding Links and References**: Markdown lets you embed hyperlinks, footnotes, and references. In RAG scenarios, this can be crucial for referring to external sources or providing additional context.\n4. **Ease of Authoring**: Markdown is human-readable and easy to write. Authors can create content efficiently without complex formatting tools.\n5. **Chunking**: Essential for RAG systems, chunking (otherwise known as “splitting”) breaks down extensive documents for easier processing. With PyMuPDF data extraction available in MD format we support chunking to keep text with common context together. **Importantly, PyMuPDF extraction in MD format allows for [Level 3 chunking](https://readmedium.com/five-levels-of-chunking-strategies-in-rag-notes-from-gregs-video-7b735895694d#b123)**.\n\nIn summary, using markdown text format in LLM and RAG environments ensures more accurate and relevant results because it supplies richer data structures and more relevant data chunk loads to your LLM.\n\n\n## PyMuPDF Support for Markdown Conversion of a PDF\n\nSince its inception, PyMuPDF has been able to extract text, images, vector graphics and, since August 2023, tables from PDF pages. Each of these object types has its own extraction method: there is one for text, and yet others for tables, images and vector graphics. To meet the requirements of RAG, we merged these disparate extractions to produce one common, unified **Markdown** string which consistently represents the page’s content as a whole.\n\nAll this is implemented as [one Python script](https://github.com/pymupdf/RAG/blob/main/helpers/pymupdf_rag.py). It can be imported as a module by some other script, or be invoked as a line command in a terminal window like this:\n\n`$ python pymupdf_rag.py input.pdf [-pages PAGES]`\n\nIt will produce a text file (called `input.md`) in **Markdown** format. The optional parameter `PAGES` allows restricting the conversion to a subset of the PDF’s total pages. If omitted, the full PDF is processed.\n\n\n## Markdown Creation Details\n\n\n### Selecting Pages to Consider\n\nThe “`-pages`” parameter is a string consisting of desired page numbers (1-based) to consider for markdown conversion. Multiple page number specifications can be given, separated by commas. Each specification either is one integer or two integers separated by a “`-`” hyphen, specifying a range of pages. Here is an example:\n\n“`-pages 1–10,15,20-N`”\n\nThis would include pages 1 through 10, 15 and pages 20 through the end of the file (capital “N” is treated as the number of the last page).\n\n\n### Identifying Headers\n\nUpon invocation, the program examines all text on the given pages and finds the most frequently used font size. This value (and all smaller font sizes) is assumed to represent **body text**. Larger font sizes are assumed to represent **header text**.\n\nDepending on their relative position in the font size hierarchy, header text will be prefixed with one or more markdown header `#`-tag characters.\n\n\n### Identifying the Processing Mode per Page Area\n\nAll text on each page will first be classified as being either **standard** text or **table** text. Then the page content will be extracted from top to bottom converting everything to markdown format.\n\nThis is best explained by an example:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*u5fv2aAIvDaaAd6H.png)\n\nThis page shows content, that represents typical situations:\n\n* Two tables, having partly overlapping vertical positions. One table has no headers, the other one has **external** column headers.\n* There is a **title** line and **headers** at multiple levels.\n* The **body text** contains a variety of styling details like **bold**, *italic* and `inline code`.\n* Ordered and unordered lists.\n* Code snippet.\n\nLayout analysis will determine three areas and select the appropriate processing modes: **(1)** text, **(2)** table, **(3)** text.\n\nThe generated Markdown text reflects the above faithfully — as much as at all possible in this format.\n\nFor an example, let us look at the output for the table with external headers:\n\n\n```python\n|Column1|Column2|\n\n|---|---|\n\n|Cell (0, 0)|Cell (0, 1)|\n\n|Cell (1, 0)|Cell (1, 1)|\n\n|Cell (2, 0)|Cell (2, 1)|\n```\nThis is GitHub-compatible format with the minimum possible token size — an important aspect for keeping feeds into RAG systems small.\n\n**Column borders** are indicated by the “`|`” character. A text line is assumed to be a **table header** if it is followed by a line of the form “`|---|---| …`”. The full **table definition** must be preceded and followed by at least one empty line.\n\nPlease note that for technical reasons markdown tables must have a header and thus will choose the first table row if no external header is available.\n\nTo confirm overall fidelity, here is how a Markdown parser processes the full page:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Ge83uj7FiM4T6XFn)\n\n\n## Invoking the Markdown Converter Programmatically\n\nInstead of executing a program in the command line, Markdown conversion can also be requested by a program:\n\n\n```python\nimport fitz\nfrom pymupdf_rag import to_markdown  # import Markdown converter\n\ndoc = fitz.open(“input.pdf”)  # open input PDF\n\n## define desired pages: this corresponds “-pages 1-10,15,20-N”\npage_list = list(range(9)) + [14] + list(range(19, len(doc) – 1))\n\n## get markdown string for all pages\nmd_text = to_markdown(doc, pages=page_list)\n\n## write markdown string to some file\noutput = open(“out-markdown.md”, “w”)\noutput.write(md_text)\noutput.close()\n```\n\n## Conclusion\n\nBy integrating PyMuPDF’s extraction methods, the content of PDF pages will be faithfully converted to markdown text that can be used as input for RAG chatbots.\n\nRemember, the key to a successful RAG chatbot lies in the quality and completeness of information it can access.\n\nPyMuPDF-enabled markdown extraction ensures that this information from PDFs is not only possible but straightforward, showcasing the library’s strength and developer-friendliness. Happy coding!\n\n\n### Source Code\n\n* [RAG/helpers/pymupdf\\_rag.py (github.com)](https://github.com/pymupdf/RAG/blob/main/helpers/pymupdf_rag.py)\n\n\n### References\n\n* [5 Levels of Text Splitting](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb)\n\n\n### Related Blogs\n\n* [Building a RAG Chatbot GUI with the ChatGPT API and PyMuPDF](https://readmedium.com/building-a-rag-chatbot-gui-with-the-chatgpt-api-and-pymupdf-9ea8c7fc4ab5)\n* [Creating a RAG Chatbot with ChatGPT and PyMUPDF](https://readmedium.com/creating-a-rag-chatbot-with-chatgpt-and-pymupdf-f6c30907ae27)\n* [RAG/LLM and PDF: Enhanced Text Extraction](https://readmedium.com/rag-llm-and-pdf-enhanced-text-extraction-5c5194c3885c)\n\n"},{"lang":"zh","group":"blog","slug":"blog/the-6-best-llm-tools-to-run-models-locally-eedd0f7c2bbd","frontmatter":{"title":"6 种最佳本地运行模型的 LLM 工具","meta_title":"6 种最佳本地运行模型的 LLM 工具","description":"运行大型语言模型 (LLM)（例如 ChatGPT 和 Claude）通常涉及将数据发送到由 OpenAI 和其他 AI 模型管理的服务器……","date":"2024-10-23T11:50:50.000Z","image":"https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*swPjVuudAhsoRiiw3Ee32w.png","categories":["ai"],"author":"Rifx.Online","tags":["ai"],"draft":false,"slug":"blog/the-6-best-llm-tools-to-run-models-locally-eedd0f7c2bbd"},"content":"\n\n\n\n\n### Data feeding in markdown text format increases generated text quality\n\n\n\n\n## Introduction\n\nIn the context of **Large Language Models (LLMs)** and **Retrieval-Augmented Generation (RAG)** environments, data feeding in **markdown text format** holds **significant importance**. Here are some detailed considerations.\n\n**LLMs** are powerful language models that can generate coherent and contextually relevant text. However, they may sometimes produce responses that lack factual accuracy or context. By incorporating retrieval-based methods (like RAG), we can enhance the quality of generated text.\n\n**RAG** enables the integration of **external data** — previously absent in the LLM’s training data — into the text generation process. This inclusion mitigates “hallucination issues’’ and enhances the relevance of text responses.\n\n\n## Why Markdown for LLM?\n\n**Markdown** is a lightweight markup language that allows users to format plain text using simple syntax. It is widely used for creating structured documents, especially on platforms like GitHub, Jupyter notebooks, and various content management systems. When feeding data into an LLM or RAG system, using markdown format provides several benefits:\n\n1. **Structured Content**: Markdown allows you to organize information into headings, lists, tables, and other structured elements. This structure aids in better understanding and context preservation.\n2. **Rich Text**: Markdown supports basic formatting such as bold, italics, links, and code blocks. Including rich text in the input data enhances the context for the language model.\n3. **Embedding Links and References**: Markdown lets you embed hyperlinks, footnotes, and references. In RAG scenarios, this can be crucial for referring to external sources or providing additional context.\n4. **Ease of Authoring**: Markdown is human-readable and easy to write. Authors can create content efficiently without complex formatting tools.\n5. **Chunking**: Essential for RAG systems, chunking (otherwise known as “splitting”) breaks down extensive documents for easier processing. With PyMuPDF data extraction available in MD format we support chunking to keep text with common context together. **Importantly, PyMuPDF extraction in MD format allows for [Level 3 chunking](https://readmedium.com/five-levels-of-chunking-strategies-in-rag-notes-from-gregs-video-7b735895694d#b123)**.\n\nIn summary, using markdown text format in LLM and RAG environments ensures more accurate and relevant results because it supplies richer data structures and more relevant data chunk loads to your LLM.\n\n\n## PyMuPDF Support for Markdown Conversion of a PDF\n\nSince its inception, PyMuPDF has been able to extract text, images, vector graphics and, since August 2023, tables from PDF pages. Each of these object types has its own extraction method: there is one for text, and yet others for tables, images and vector graphics. To meet the requirements of RAG, we merged these disparate extractions to produce one common, unified **Markdown** string which consistently represents the page’s content as a whole.\n\nAll this is implemented as [one Python script](https://github.com/pymupdf/RAG/blob/main/helpers/pymupdf_rag.py). It can be imported as a module by some other script, or be invoked as a line command in a terminal window like this:\n\n`$ python pymupdf_rag.py input.pdf [-pages PAGES]`\n\nIt will produce a text file (called `input.md`) in **Markdown** format. The optional parameter `PAGES` allows restricting the conversion to a subset of the PDF’s total pages. If omitted, the full PDF is processed.\n\n\n## Markdown Creation Details\n\n\n### Selecting Pages to Consider\n\nThe “`-pages`” parameter is a string consisting of desired page numbers (1-based) to consider for markdown conversion. Multiple page number specifications can be given, separated by commas. Each specification either is one integer or two integers separated by a “`-`” hyphen, specifying a range of pages. Here is an example:\n\n“`-pages 1–10,15,20-N`”\n\nThis would include pages 1 through 10, 15 and pages 20 through the end of the file (capital “N” is treated as the number of the last page).\n\n\n### Identifying Headers\n\nUpon invocation, the program examines all text on the given pages and finds the most frequently used font size. This value (and all smaller font sizes) is assumed to represent **body text**. Larger font sizes are assumed to represent **header text**.\n\nDepending on their relative position in the font size hierarchy, header text will be prefixed with one or more markdown header `#`-tag characters.\n\n\n### Identifying the Processing Mode per Page Area\n\nAll text on each page will first be classified as being either **standard** text or **table** text. Then the page content will be extracted from top to bottom converting everything to markdown format.\n\nThis is best explained by an example:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*u5fv2aAIvDaaAd6H.png)\n\nThis page shows content, that represents typical situations:\n\n* Two tables, having partly overlapping vertical positions. One table has no headers, the other one has **external** column headers.\n* There is a **title** line and **headers** at multiple levels.\n* The **body text** contains a variety of styling details like **bold**, *italic* and `inline code`.\n* Ordered and unordered lists.\n* Code snippet.\n\nLayout analysis will determine three areas and select the appropriate processing modes: **(1)** text, **(2)** table, **(3)** text.\n\nThe generated Markdown text reflects the above faithfully — as much as at all possible in this format.\n\nFor an example, let us look at the output for the table with external headers:\n\n\n```python\n|Column1|Column2|\n\n|---|---|\n\n|Cell (0, 0)|Cell (0, 1)|\n\n|Cell (1, 0)|Cell (1, 1)|\n\n|Cell (2, 0)|Cell (2, 1)|\n```\nThis is GitHub-compatible format with the minimum possible token size — an important aspect for keeping feeds into RAG systems small.\n\n**Column borders** are indicated by the “`|`” character. A text line is assumed to be a **table header** if it is followed by a line of the form “`|---|---| …`”. The full **table definition** must be preceded and followed by at least one empty line.\n\nPlease note that for technical reasons markdown tables must have a header and thus will choose the first table row if no external header is available.\n\nTo confirm overall fidelity, here is how a Markdown parser processes the full page:\n\n![](https://images.weserv.nl/?url=https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*Ge83uj7FiM4T6XFn)\n\n\n## Invoking the Markdown Converter Programmatically\n\nInstead of executing a program in the command line, Markdown conversion can also be requested by a program:\n\n\n```python\nimport fitz\nfrom pymupdf_rag import to_markdown  # import Markdown converter\n\ndoc = fitz.open(“input.pdf”)  # open input PDF\n\n## define desired pages: this corresponds “-pages 1-10,15,20-N”\npage_list = list(range(9)) + [14] + list(range(19, len(doc) – 1))\n\n## get markdown string for all pages\nmd_text = to_markdown(doc, pages=page_list)\n\n## write markdown string to some file\noutput = open(“out-markdown.md”, “w”)\noutput.write(md_text)\noutput.close()\n```\n\n## Conclusion\n\nBy integrating PyMuPDF’s extraction methods, the content of PDF pages will be faithfully converted to markdown text that can be used as input for RAG chatbots.\n\nRemember, the key to a successful RAG chatbot lies in the quality and completeness of information it can access.\n\nPyMuPDF-enabled markdown extraction ensures that this information from PDFs is not only possible but straightforward, showcasing the library’s strength and developer-friendliness. Happy coding!\n\n\n### Source Code\n\n* [RAG/helpers/pymupdf\\_rag.py (github.com)](https://github.com/pymupdf/RAG/blob/main/helpers/pymupdf_rag.py)\n\n\n### References\n\n* [5 Levels of Text Splitting](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb)\n\n\n### Related Blogs\n\n* [Building a RAG Chatbot GUI with the ChatGPT API and PyMuPDF](https://readmedium.com/building-a-rag-chatbot-gui-with-the-chatgpt-api-and-pymupdf-9ea8c7fc4ab5)\n* [Creating a RAG Chatbot with ChatGPT and PyMUPDF](https://readmedium.com/creating-a-rag-chatbot-with-chatgpt-and-pymupdf-f6c30907ae27)\n* [RAG/LLM and PDF: Enhanced Text Extraction](https://readmedium.com/rag-llm-and-pdf-enhanced-text-extraction-5c5194c3885c)\n\n"},{"lang":"fr","group":"blog","slug":"blog/post-1","frontmatter":{"title":"Comment créer une application avec des technologies modernes","meta_title":"","description":"Ceci est une méta-description","date":"2022-04-04T05:00:00.000Z","image":"/images/image-placeholder.png","categories":["french","Application","Data"],"author":"John Doe","tags":["nextjs","tailwind","react"],"draft":false,"slug":"blog/post-1"},"content":"\nPersonne ne veut même sortir un maquillage de l'urne des soins empoisonnés. C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\n## Design Créatif\n\nCar en guise de maquillage, l'urne du poison C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\n> Le client lui-même doit pouvoir poursuivre l'adipisicing. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n"},{"lang":"fr","group":"blog","slug":"blog/post-2","frontmatter":{"title":"Comment créer une application avec des technologies modernes","meta_title":"","description":"Ceci est une méta-description","date":"2022-04-04T05:00:00.000Z","image":"/images/image-placeholder.png","categories":["Technology","Data"],"author":"Sam Wilson","tags":["technology","tailwind"],"draft":false,"slug":"blog/post-2"},"content":"\nPersonne ne veut même sortir un maquillage de l'urne des soins empoisonnés. C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\n## Design Créatif\n\nCar en guise de maquillage, l'urne du poison C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\n> Le client lui-même doit pouvoir poursuivre l'adipisicing. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n"},{"lang":"fr","group":"blog","slug":"blog/post-3","frontmatter":{"title":"Comment créer une application avec des technologies modernes","meta_title":"","description":"Ceci est une méta-description","date":"2022-04-04T05:00:00.000Z","image":"/images/image-placeholder.png","categories":["Software"],"author":"John Doe","tags":["software","tailwind"],"draft":false,"slug":"blog/post-3"},"content":"\nPersonne ne veut même sortir un maquillage de l'urne des soins empoisonnés. C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\n## Design Créatif\n\nCar en guise de maquillage, l'urne du poison C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\n> Le client lui-même doit pouvoir poursuivre l'adipisicing. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n"},{"lang":"fr","group":"blog","slug":"blog/post-4","frontmatter":{"title":"Comment créer une application avec des technologies modernes","meta_title":"","description":"Ceci est une méta-description","date":"2022-04-04T05:00:00.000Z","image":"/images/image-placeholder.png","categories":["Architecture"],"author":"John Doe","tags":["silicon","technology"],"draft":false,"slug":"blog/post-4"},"content":"\nPersonne ne veut même sortir un maquillage de l'urne des soins empoisonnés. C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\n## Design Créatif\n\nCar en guise de maquillage, l'urne du poison C'était un week-end. Je suis un footballeur complet. Pour boire, le lac occupe le plus grand porche. Chacune des cibles de la vie ne flatte pas Euismod.\n\n> Le client lui-même doit pouvoir poursuivre l'adipisicing. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n\nL'entreprise elle-même est une entreprise très prospère. Personne ne prend même la peine de l'ouvrir. Alors je vais ouvrir la naissance pour choisir ? Être rejeté par certaines personnes est un choix commode du présent pour ressentir une douleur comme la sienne !\n"},{"lang":"en","group":"models","slug":"models/chatgpt-4o-latest","frontmatter":{"title":"OpenAI: ChatGPT-4o","meta_title":"OpenAI: ChatGPT-4o","description":"OpenAI: ChatGPT-4o","date":"2024-08-14T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text image 2 text"],"author":"openai","tags":["openai"],"draft":false,"id":"chatgpt-4o-latest","context":128000,"input":0.000005,"output":0.000015,"img":0.007225,"request":0,"slug":"models/chatgpt-4o-latest"},"content":"\nDynamic model continuously updated to the current version of [GPT-4o](/openai/gpt-4o) in ChatGPT. Intended for research and evaluation.\n\nNote: This model is currently experimental and not suitable for production use-cases, and may be heavily rate-limited.\n\n"},{"lang":"en","group":"models","slug":"models/gemini-flash-15-8b","frontmatter":{"title":"Google: Gemini 1.5 Flash-8B","meta_title":"Google: Gemini 1.5 Flash-8B","description":"Google: Gemini 1.5 Flash-8B","date":"2024-10-03T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text image 2 text"],"author":"google","tags":["google"],"draft":false,"id":"gemini-flash-1.5-8b","context":1000000,"input":3.75e-8,"output":1.5e-7,"img":0,"request":0,"slug":"models/gemini-flash-15-8b"},"content":"\nGemini 1.5 Flash-8B is optimized for speed and efficiency, offering enhanced performance in small prompt tasks like chat, transcription, and translation. With reduced latency, it is highly effective for real-time and large-scale operations. This model focuses on cost-effective solutions while maintaining high-quality results.\n\n[Click here to learn more about this model](https://developers.googleblog.com/en/gemini-15-flash-8b-is-now-generally-available-for-use/).\n\nUsage of Gemini is subject to Google's [Gemini Terms of Use](https://ai.google.dev/terms).\n\n"},{"lang":"en","group":"models","slug":"models/gemini-pro-15","frontmatter":{"title":"Google: Gemini Pro 1.5","meta_title":"Google: Gemini Pro 1.5","description":"Google: Gemini Pro 1.5","date":"2024-04-09T00:00:00.000Z","image":"https://img.rifx.online/logo/google.svg","categories":["text image 2 text"],"author":"google","tags":["google"],"draft":false,"id":"gemini-pro-1.5","context":2000000,"input":0.00000125,"output":0.000005,"img":0.00263,"request":0,"slug":"models/gemini-pro-15"},"content":"\nGoogle's latest multimodal model, supporting image and video in text or chat prompts.\n\nOptimized for language tasks including:\n\n- Code generation\n- Text generation\n- Text editing\n- Problem solving\n- Recommendations\n- Information extraction\n- Data extraction or generation\n- AI agents\n\nUsage of Gemini is subject to Google's [Gemini Terms of Use](https://ai.google.dev/terms).\n\n#multimodal\n\n"},{"lang":"en","group":"models","slug":"models/gpt-35-turbo-instruct","frontmatter":{"title":"OpenAI: GPT-3.5 Turbo Instruct","meta_title":"OpenAI: GPT-3.5 Turbo Instruct","description":"OpenAI: GPT-3.5 Turbo Instruct","date":"2023-09-28T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"openai","tags":["openai"],"draft":false,"id":"gpt-3.5-turbo-instruct","context":4095,"input":0.0000015,"output":0.000002,"img":0,"request":0,"slug":"models/gpt-35-turbo-instruct"},"content":"\nThis model is a variant of GPT-3.5 Turbo tuned for instructional prompts and omitting chat-related optimizations. Training data: up to Sep 2021.\n\n"},{"lang":"en","group":"models","slug":"models/gpt-4o-mini","frontmatter":{"title":"OpenAI: GPT-4o-mini","meta_title":"OpenAI: GPT-4o-mini","description":"OpenAI: GPT-4o-mini","date":"2024-07-18T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text image 2 text"],"author":"openai","tags":["Programming","Technology","Programming/Scripting","Technology/Web"],"draft":false,"id":"gpt-4o-mini","context":128000,"input":1.5e-7,"output":6e-7,"img":0.007225,"request":0,"slug":"models/gpt-4o-mini"},"content":"\nGPT-4o mini is OpenAI's newest model after [GPT-4 Omni](/openai/gpt-4o), supporting both text and image inputs with text outputs.\n\nAs their most advanced small model, it is many multiples more affordable than other recent frontier models, and more than 60% cheaper than [GPT-3.5 Turbo](/openai/gpt-3.5-turbo). It maintains SOTA intelligence, while being significantly more cost-effective.\n\nGPT-4o mini achieves an 82% score on MMLU and presently ranks higher than GPT-4 on chat preferences [common leaderboards](https://arena.lmsys.org/).\n\nCheck out the [launch announcement](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/) to learn more.\n\n"},{"lang":"en","group":"models","slug":"models/gpt-4o","frontmatter":{"title":"OpenAI: GPT-4o","meta_title":"OpenAI: GPT-4o","description":"OpenAI: GPT-4o","date":"2024-05-13T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text image 2 text"],"author":"openai","tags":["Programming","Programming/Scripting","Technology/Web","Technology"],"draft":false,"id":"gpt-4o","context":128000,"input":0.0000025,"output":0.00001,"img":0.0036125,"request":0,"slug":"models/gpt-4o"},"content":"\nGPT-4o (\"o\" for \"omni\") is OpenAI's latest AI model, supporting both text and image inputs with text outputs. It maintains the intelligence level of [GPT-4 Turbo](/openai/gpt-4-turbo) while being twice as fast and 50% more cost-effective. GPT-4o also offers improved performance in processing non-English languages and enhanced visual capabilities.\n\nFor benchmarking against other models, it was briefly called [\"im-also-a-good-gpt2-chatbot\"](https://twitter.com/LiamFedus/status/1790064963966370209)\n\n"},{"lang":"en","group":"models","slug":"models/llama-31-nemotron-70b-instruct","frontmatter":{"title":"Nvidia: Llama 3.1 Nemotron 70B Instruct","meta_title":"Nvidia: Llama 3.1 Nemotron 70B Instruct","description":"Nvidia: Llama 3.1 Nemotron 70B Instruct","date":"2024-10-15T00:00:00.000Z","image":"https://img.rifx.online/logo/nvidia.svg","categories":["text 2 text"],"author":"nvidia","tags":["nvidia"],"draft":false,"id":"llama-3.1-nemotron-70b-instruct","context":131072,"input":3.5e-7,"output":4e-7,"img":0,"request":0,"slug":"models/llama-31-nemotron-70b-instruct"},"content":"\nNVIDIA's Llama 3.1 Nemotron 70B is a language model designed for generating precise and useful responses. Leveraging [Llama 3.1 70B](/models/meta-llama/llama-3.1-70b-instruct) architecture and Reinforcement Learning from Human Feedback (RLHF), it excels in automatic alignment benchmarks. This model is tailored for applications requiring high accuracy in helpfulness and response generation, suitable for diverse user queries across multiple domains.\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).\n\n"},{"lang":"en","group":"models","slug":"models/llama-32-11b-vision-instruct","frontmatter":{"title":"Meta: Llama 3.2 11B Vision Instruct","meta_title":"Meta: Llama 3.2 11B Vision Instruct","description":"Meta: Llama 3.2 11B Vision Instruct","date":"2024-09-25T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text image 2 text"],"author":"meta-llama","tags":["meta-llama"],"draft":false,"id":"llama-3.2-11b-vision-instruct","context":131072,"input":5.5e-8,"output":5.5e-8,"img":0.000079475,"request":0,"slug":"models/llama-32-11b-vision-instruct"},"content":"\nLlama 3.2 11B Vision is a multimodal model with 11 billion parameters, designed to handle tasks combining visual and textual data. It excels in tasks such as image captioning and visual question answering, bridging the gap between language generation and visual reasoning. Pre-trained on a massive dataset of image-text pairs, it performs well in complex, high-accuracy image analysis.\n\nIts ability to integrate visual understanding with language processing makes it an ideal solution for industries requiring comprehensive visual-linguistic AI applications, such as content creation, AI-driven customer service, and research.\n\nClick here for the [original model card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md).\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).\n\n"},{"lang":"en","group":"models","slug":"models/llama-32-1b-instruct","frontmatter":{"title":"Meta: Llama 3.2 1B Instruct","meta_title":"Meta: Llama 3.2 1B Instruct","description":"Meta: Llama 3.2 1B Instruct","date":"2024-09-25T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text 2 text"],"author":"meta-llama","tags":["meta-llama"],"draft":false,"id":"llama-3.2-1b-instruct","context":131072,"input":1e-8,"output":2e-8,"img":0,"request":0,"slug":"models/llama-32-1b-instruct"},"content":"\nLlama 3.2 1B is a 1-billion-parameter language model focused on efficiently performing natural language tasks, such as summarization, dialogue, and multilingual text analysis. Its smaller size allows it to operate efficiently in low-resource environments while maintaining strong task performance.\n\nSupporting eight core languages and fine-tunable for more, Llama 1.3B is ideal for businesses or developers seeking lightweight yet powerful AI solutions that can operate in diverse multilingual settings without the high computational demand of larger models.\n\nClick here for the [original model card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md).\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).\n\n"},{"lang":"en","group":"models","slug":"models/llama-32-90b-vision-instruct","frontmatter":{"title":"Meta: Llama 3.2 90B Vision Instruct","meta_title":"Meta: Llama 3.2 90B Vision Instruct","description":"Meta: Llama 3.2 90B Vision Instruct","date":"2024-09-25T00:00:00.000Z","image":"https://img.rifx.online/logo/meta.svg","categories":["text image 2 text"],"author":"meta-llama","tags":["meta-llama"],"draft":false,"id":"llama-3.2-90b-vision-instruct","context":131072,"input":3.5e-7,"output":4e-7,"img":0.00050575,"request":0,"slug":"models/llama-32-90b-vision-instruct"},"content":"\nThe Llama 90B Vision model is a top-tier, 90-billion-parameter multimodal model designed for the most challenging visual reasoning and language tasks. It offers unparalleled accuracy in image captioning, visual question answering, and advanced image-text comprehension. Pre-trained on vast multimodal datasets and fine-tuned with human feedback, the Llama 90B Vision is engineered to handle the most demanding image-based AI tasks.\n\nThis model is perfect for industries requiring cutting-edge multimodal AI capabilities, particularly those dealing with complex, real-time visual and textual analysis.\n\nClick here for the [original model card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md).\n\nUsage of this model is subject to [Meta's Acceptable Use Policy](https://www.llama.com/llama3/use-policy/).\n\n"},{"lang":"en","group":"models","slug":"models/ministral-8b","frontmatter":{"title":"Ministral 8B","meta_title":"Ministral 8B","description":"Ministral 8B","date":"2024-10-17T00:00:00.000Z","image":"https://img.rifx.online/logo/mistral.png","categories":["text 2 text"],"author":"mistralai","tags":["mistralai"],"draft":false,"id":"ministral-8b","context":128000,"input":1e-7,"output":1e-7,"img":0,"request":0,"slug":"models/ministral-8b"},"content":"\nMinistral 8B is an 8B parameter model featuring a unique interleaved sliding-window attention pattern for faster, memory-efficient inference. Designed for edge use cases, it supports up to 128k context length and excels in knowledge and reasoning tasks. It outperforms peers in the sub-10B category, making it perfect for low-latency, privacy-first applications.\n\n"},{"lang":"en","group":"models","slug":"models/o1-mini","frontmatter":{"title":"OpenAI: o1-mini","meta_title":"OpenAI: o1-mini","description":"OpenAI: o1-mini","date":"2024-09-12T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"openai","tags":["openai"],"draft":false,"id":"o1-mini","context":128000,"input":0.000003,"output":0.000012,"img":0,"request":0,"slug":"models/o1-mini"},"content":"\nThe latest and strongest model family from OpenAI, o1 is designed to spend more time thinking before responding.\n\nThe o1 models are optimized for math, science, programming, and other STEM-related tasks. They consistently exhibit PhD-level accuracy on benchmarks in physics, chemistry, and biology. Learn more in the [launch announcement](https://openai.com/o1).\n\nNote: This model is currently experimental and not suitable for production use-cases, and may be heavily rate-limited.\n\n"},{"lang":"en","group":"models","slug":"models/o1-preview","frontmatter":{"title":"OpenAI: o1-preview","meta_title":"OpenAI: o1-preview","description":"OpenAI: o1-preview","date":"2024-09-12T00:00:00.000Z","image":"https://img.rifx.online/logo/openai.svg","categories":["text 2 text"],"author":"openai","tags":["openai"],"draft":false,"id":"o1-preview","context":128000,"input":0.000015,"output":0.00006,"img":0,"request":0,"slug":"models/o1-preview"},"content":"\nThe latest and strongest model family from OpenAI, o1 is designed to spend more time thinking before responding.\n\nThe o1 models are optimized for math, science, programming, and other STEM-related tasks. They consistently exhibit PhD-level accuracy on benchmarks in physics, chemistry, and biology. Learn more in the [launch announcement](https://openai.com/o1).\n\nNote: This model is currently experimental and not suitable for production use-cases, and may be heavily rate-limited.\n\n"},{"lang":"en","group":"models","slug":"models/qwen-2-7b-instruct","frontmatter":{"title":"Qwen 2 7B Instruct","meta_title":"Qwen 2 7B Instruct","description":"Qwen 2 7B Instruct","date":"2024-07-16T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"qwen","tags":["qwen"],"draft":false,"id":"qwen-2-7b-instruct","context":32768,"input":5.4e-8,"output":5.4e-8,"img":0,"request":0,"slug":"models/qwen-2-7b-instruct"},"content":"\nQwen2 7B is a transformer-based model that excels in language understanding, multilingual capabilities, coding, mathematics, and reasoning.\n\nIt features SwiGLU activation, attention QKV bias, and group query attention. It is pretrained on extensive data with supervised finetuning and direct preference optimization.\n\nFor more details, see this [blog post](https://qwenlm.github.io/blog/qwen2/) and [GitHub repo](https://github.com/QwenLM/Qwen2).\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).\n\n"},{"lang":"en","group":"models","slug":"models/qwen-2-vl-72b-instruct","frontmatter":{"title":"Qwen2-VL 72B Instruct","meta_title":"Qwen2-VL 72B Instruct","description":"Qwen2-VL 72B Instruct","date":"2024-09-18T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text image 2 text"],"author":"qwen","tags":["qwen"],"draft":false,"id":"qwen-2-vl-72b-instruct","context":32768,"input":4e-7,"output":4e-7,"img":0.000578,"request":0,"slug":"models/qwen-2-vl-72b-instruct"},"content":"\nQwen2 VL 72B is a multimodal LLM from the Qwen Team with the following key enhancements:\n\n- SoTA understanding of images of various resolution & ratio: Qwen2-VL achieves state-of-the-art performance on visual understanding benchmarks, including MathVista, DocVQA, RealWorldQA, MTVQA, etc.\n\n- Understanding videos of 20min+: Qwen2-VL can understand videos over 20 minutes for high-quality video-based question answering, dialog, content creation, etc.\n\n- Agent that can operate your mobiles, robots, etc.: with the abilities of complex reasoning and decision making, Qwen2-VL can be integrated with devices like mobile phones, robots, etc., for automatic operation based on visual environment and text instructions.\n\n- Multilingual Support: to serve global users, besides English and Chinese, Qwen2-VL now supports the understanding of texts in different languages inside images, including most European languages, Japanese, Korean, Arabic, Vietnamese, etc.\n\nFor more details, see this [blog post](https://qwenlm.github.io/blog/qwen2-vl/) and [GitHub repo](https://github.com/QwenLM/Qwen2-VL).\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).\n\n"},{"lang":"en","group":"models","slug":"models/qwen-25-72b-instruct","frontmatter":{"title":"Qwen2.5 72B Instruct","meta_title":"Qwen2.5 72B Instruct","description":"Qwen2.5 72B Instruct","date":"2024-09-19T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"qwen","tags":["qwen"],"draft":false,"id":"qwen-2.5-72b-instruct","context":131072,"input":3.5e-7,"output":4e-7,"img":0,"request":0,"slug":"models/qwen-25-72b-instruct"},"content":"\nQwen2.5 72B is the latest series of Qwen large language models. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\n\n- Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n\n- Long-context Support up to 128K tokens and can generate up to 8K tokens.\n\n- Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).\n\n"},{"lang":"en","group":"models","slug":"models/qwen-25-7b-instruct","frontmatter":{"title":"Qwen2.5 7B Instruct","meta_title":"Qwen2.5 7B Instruct","description":"Qwen2.5 7B Instruct","date":"2024-10-16T00:00:00.000Z","image":"https://img.rifx.online/logo/qwen.svg","categories":["text 2 text"],"author":"qwen","tags":["qwen"],"draft":false,"id":"qwen-2.5-7b-instruct","context":131072,"input":2.7e-7,"output":2.7e-7,"img":0,"request":0,"slug":"models/qwen-25-7b-instruct"},"content":"\nQwen2.5 7B is the latest series of Qwen large language models. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\n\n- Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n\n- Long-context Support up to 128K tokens and can generate up to 8K tokens.\n\n- Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n\nUsage of this model is subject to [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen1.5-110B-Chat/blob/main/LICENSE).\n\n"}]